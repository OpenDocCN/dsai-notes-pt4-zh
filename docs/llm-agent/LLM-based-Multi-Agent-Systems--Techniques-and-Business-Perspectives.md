<!--yml
category: 未分类
date: 2025-01-11 11:54:53
-->

# LLM-based Multi-Agent Systems: Techniques and Business Perspectives

> 来源：[https://arxiv.org/html/2411.14033/](https://arxiv.org/html/2411.14033/)

Yingxuan Yang [zoeyyx@sjtu.edu.cn](mailto:zoeyyx@sjtu.edu.cn) Shanghai Jiao Tong UniversityShanghaiChina ,  Qiuying Peng [qypeng.ustc@gmail.com](mailto:qypeng.ustc@gmail.com) OPPO Research InstituteShenzhenChina ,  Jun Wang [junwang.lu@gmail.com](mailto:junwang.lu@gmail.com) OPPO Research InstituteShenzhenChina ,  Ying Wen [ying.wen@sjtu.edu.cn](mailto:ying.wen@sjtu.edu.cn) Shanghai Jiao Tong University & SIIShanghaiChina  and  Weinan Zhang [wnzhang@sjtu.edu.cn](mailto:wnzhang@sjtu.edu.cn) Shanghai Jiao Tong University & SIIShanghaiChina

###### Abstract.

In the era of (multi-modal) large language models, most operational processes can be reformulated and reproduced using LLM agents. The LLM agents can perceive, control, and get feedback from the environment so as to accomplish the given tasks in an autonomous manner. Besides the environment-interaction property, the LLM agents can call various external tools to ease the task completion process. The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs. As a natural trend of development, the tools for calling are becoming autonomous agents, thus the full intelligent system turns out to be a LLM-based Multi-Agent System (LaMAS). Compared to the previous single-LLM-agent system, LaMAS has the advantages of i) dynamic task decomposition and organic specialization, ii) higher flexibility for system changing, iii) proprietary data preserving for each participating entity, and iv) feasibility of monetization for each entity. This paper discusses the technical and business landscapes of LaMAS. To support the ecosystem of LaMAS, we provide a preliminary version of such LaMAS protocol considering technical requirements, data privacy, and business incentives. As such, LaMAS would be a practical solution to achieve artificial collective intelligence in the near future.

LLM-based Multi-Agent System, large language model, data privacy, monetization

## 1\. Background and Trend

The development of Large Language Models (LLMs) (OpenAI et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib45)) marks a key advancement in artificial intelligence. These models have transformed from simple text processors to sophisticated systems capable of reasoning, understanding multimodal inputs, and making autonomous decisions (Wang et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib66)). Such developments have enabled the emergence of AI agents powered by LLMs¹¹1For presentation brevity, in this paper, the multi-modal LLM concept (Caffagni et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib8)) is merged into the LLM concept., which can adapt to diverse tasks, comprehend context, and interact with their environments autonomously (Zhang et al., [2024c](https://arxiv.org/html/2411.14033v2#bib.bib77); Zhou et al., [2024b](https://arxiv.org/html/2411.14033v2#bib.bib79)).

A critical transition in LLM capabilities is their evolution from passive tools that merely respond to commands to active agents capable of independent decision-making and action-taking (Schick et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib55); Lin et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib37)). Initially, LLMs were primarily used for single-purpose tasks, such as text generation or analysis. However, recent advances have equipped them to interact with graphical user interfaces (GUIs) and perform complex operations such as web browsing, app navigation, and system control (Zhang et al., [2024b](https://arxiv.org/html/2411.14033v2#bib.bib76)). Beyond these capabilities, modern LLMs have transformed into autonomous agents that dynamically select and use tools based on contextual requirements. This evolution highlights their dual nature: they not only utilize tools but can also function as tools within modular systems, enabling the creation of multi-agent architectures where agents collaborate to solve complex problems.

The rise of LLM-based Multi-Agent Systems (LaMAS) marks a significant leap in AI applications (Chen et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib12); Liu et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib39)). Although such systems may require greater computational resources compared to single-agent approaches, they offer crucial advantages that justify this trade-off: inherent fault tolerance through agent redundancy, natural task decomposition without explicit workflow design, and organic specialization in complex problem-solving. When one agent fails in a multi-agent system, others can seamlessly continue operations, providing robust reliability that centralized systems cannot match. Moreover, while single-agent systems demand careful orchestration of execution workflows for each task type, multi-agent systems naturally emerge with collaborative specialization patterns, allowing each agent to focus on its core competencies within the larger system architecture.

Recognizing these benefits, researchers have developed LaMAS frameworks to enable complex task collaboration (Guo et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib24); Chen et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib12); Liu et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib39); Fourney et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib19); Ghafarollahi and Buehler, [2024](https://arxiv.org/html/2411.14033v2#bib.bib22); Chen et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib11); Qian et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib49); Hong et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib27); Wu et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib70); Xie et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib72); Li et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib35); Gao et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib21); Zhuge et al., [[n.d.]](https://arxiv.org/html/2411.14033v2#bib.bib81); Trirat et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib64); Fu et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib20); Li et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib33); Chan et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib9)). Beyond traditional paradigms like SaaS, PaaS, and IaaS, LaMAS introduces a novel approach by seamlessly integrating intelligent agents into cloud ecosystems. This framework supports the deployment of specialized agents capable of collaboration while maintaining data privacy and security. Furthermore, it establishes a marketplace for agent monetization, allowing users to customize and combine agent services according to their needs. The system architecture emphasizes modular design, standardized communication protocols, and robust security measures, fostering sustainable innovation.

Incentivization via Monetization Mechanisms. Just like the Internet applications are highly incentivized to connect to the Internet, the agents in a LaMAS are also highly incentivized based on monetization mechanism design. First, the experience data generated from interacting within a LaMAS is crucial for training well-functional agents. In LaMAS, the agents receive the task instructions from upstream agents, perform inner-agent reasoning and tool usage, send task instructions to downstream agents and acquire their returned information, and obtain the final task accomplishment results. Such experience data is more valuable and of a larger volume than a single agent just connecting to the users. Second, similar to Internet monetization via online advertising, there will be a monetization mechanism over the LaMAS. Specifically, for each accomplished task that is assigned with a business value, e.g., the user books a hotel or purchases an item, there will be a promotion fee from the merchant provided to the engaged team of agents in the LaMAS and the credit allocation mechanism can be built by the participation or the essential contribution to the task accomplishment. As such, the entity behind each agent has the essential motivation to build a highly intelligent agent connecting to the LaMAS.

Entity’s Responsibility based on Agent Intelligence. For the Cable or 4/5G Internet, each entity behind an Internet service, e.g., the company, institute, or team, is responsible for maintaining a stable function running and connection to the Internet. If its server crashes or the connection is disabled, the other services depending on its service will be highly influenced, thus the entity should take charge of the influence it makes. Analogously, in the LaMAS ecosystem, the entity behind each agent has the responsibility to make the ecosystem run smoothly and intelligently. First, inherited from the Internet services, each agent needs to support a stable function running and connection to the Internet. Second, more importantly, the intelligence provided by the agent must meet or exceed predefined standards, as the low intelligence offered by an agent would possibly make the whole LaMAS less functional for accomplishing intelligence tasks.

In this paper, we show our perspectives on LaMAS by discussing its technical and business landscapes. We will show the key AI technical aspects, including system architectures, collaboration protocols, agent training methods, and business aspects, including data privacy-preserving and monetization via traffic and intelligence. With these analyses, it is worth expecting that LaMAS will form a new technical business paradigm in the coming few years.

## 2\. Key AI Technical Aspects

### 2.1\. Architecture of LLM Agents

The architecture of LLM-based AI agents consists of several interrelated components essential for autonomous operation and intelligent interaction. At its core, this architecture is designed to effectively process inputs, maintain contextual relevance, make informed decisions, and generate appropriate responses.

The interaction wrapper serves as the principal interface through which the agent interacts with its environment and other agents. This component manages the flow of incoming and outgoing communications. It adapts to various input modalities, and standardizes them for internal processing. The interaction wrapper implements protocol-specific adaptations to ensure seamless integration with various communication standards. This approach preserves the internal consistency of the agent’s operations.

Memory management is pivotal to the architecture. It includes both short-term working memory and long-term episodic storage. The short-term memory buffer retains immediate context and recent interactions, facilitating conversational coherence. Meanwhile, the long-term memory system archives significant experiences and learned patterns, enabling the agent to adapt its responses based on historical interactions and enhancing decision-making capabilities in contextually rich scenarios.

The cognitive functionality of the architecture is currently underpinned by Chain-of-Thought (CoT) reasoning (Wei et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib67); Yao et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib75)). This structured reasoning framework decomposes complex tasks into manageable logical steps, thereby facilitating clarity and thoroughness in problem-solving. The CoT mechanism enables the agent to articulate intermediate reasoning states, verify logical consistency, and engage in self-correction through a systematic analysis of its reasoning processes.

Additionally, to enhance the agent’s operational capacity beyond natural language processing, the tool integration framework is necessary (Schick et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib55); Patil et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib46); Lin et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib37)). This subsystem discovers and registers tools, maps of parameters between natural language commands and tool APIs, monitors execution, handles errors, and interprets results. It ensures the effective integration of external functionalities into its decision-making processes.

The architecture also features a sophisticated routing mechanism that governs the connections with neighboring agents. This component is instrumental in facilitating dynamic neighbor discovery, making capability-based routing decisions, balancing loads across the agent network, and enforcing policy-based access control. Such networking capabilities are vital for ensuring efficient communication and collaboration within multi-agent systems.

Furthermore, the architecture incorporates feedback loops that enable continuous learning and adaptation. These loops facilitate the processing of interaction outcomes, allowing the agent to update its internal models and refine its decision-making strategies based on experiential learning. The integration of these architectural elements not only establishes a robust foundation for the autonomous operation of LaMAS but also significantly enhances their collaborative capabilities within multi-agent systems.

![Refer to caption](img/befde2e249bf5d2e436a19baf49fbc96.png)

Figure 1\. Illustration of LaMAS.

### 2.2\. Mechanisms and Architectures of LaMAS

As a multi-agent system, the design of mechanisms and architectures of a LaMAS is crucial for its success. Roughly, according to the coordination form of a MAS, there are three major architectures of a LaMAS.

First, for fully centralized architectures, the whole system has full control of the engaged agents, which is a very high requirement, then centralized training with centralized execution methods can be used, and the agents will act with high coordination. In practice, such methods can be applied only when the agents are the applications developed over an OS-liked platform and grant the data and control access to the platform.

Second, for decentralized architectures with global credit allocation, the whole system cannot fully control the engaged agents but can allocate credit to each one for each accomplished task, then centralized training with centralized execution methods can be applied. This is much more practical since each agent (and the entity behind it) does not have to grant the data or control access to the platform. Also, the platform can still incentivize the engaged agents to improve their collaboration performance by allocating credit to the team.

Third, for fully decentralized architecture, i.e., there is no access to data or control for each engaged agent and no credit allocation for the platform, the agents in the system will need to find their own way to collaborate with others and improve themselves. In such a case, the mechanism design will be much important from the beginning.

### 2.3\. Protocols of Agent Interaction

The LLM-based Multi-Agent System (LaMAS) framework necessitates sophisticated interaction protocols to facilitate effective agent collaboration. These protocols must bridge the gap between traditional structured formats and natural language understanding, addressing unique challenges posed by LLM-based agents’ probabilistic decision-making and emergent capabilities (Marro et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib42)).

Core Challenges and Key Issues. The development of LaMAS protocols presents several fundamental challenges in protocol effectiveness measurement, behavioral diversity optimization, and non-transitive interaction management.

![[Uncaptioned image]](img/e5a6d06325b5e839d43d592f4096bffb.png)

From these challenges, we identify 3 critical issues in protocol design. First, LaMAS requires a layered protocol architecture to manage diverse agent interactions efficiently, enabling dynamic protocol selection based on task and agent capabilities (Wooldridge, [2009](https://arxiv.org/html/2411.14033v2#bib.bib69)). Second, as system scale increases, traditional protocols face limitations in managing communication overhead and maintaining consistency, necessitating innovative approaches to protocol design (Shoham and Leyton-Brown, [2009](https://arxiv.org/html/2411.14033v2#bib.bib59)). Third, LaMAS leverages the strengths of LLM agents in language understanding and contextual interpretation. Protocols should leverage capabilities, such as handling ambiguous commands or enabling real-time negotiation to clarify information (Marro et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib42)).

Core Protocol Framework. We propose a comprehensive protocol framework consisting of five essential components: the instruction processing protocol, the message exchange protocol, the consensus formation protocol, the credit allocation protocol and the experience management protocol.

![Refer to caption](img/e59f6795cc1477344dcb8dbbb1c53068.png)

Figure 2\. Protocol Hierarchy.

The Instruction Processing Protocol standardizes the interpretation of user instructions through structured parsing mechanisms and context-aware processing pipelines (Aziz, [2010](https://arxiv.org/html/2411.14033v2#bib.bib4)). This protocol implements sophisticated disambiguation techniques to handle uncertain or incomplete instructions, maintaining consistency across multiple interaction rounds.

The Message Exchange Protocol establishes the foundation for inter-agent communication through standardized message formats and adaptive transmission mechanisms (Shoham and Leyton-Brown, [2009](https://arxiv.org/html/2411.14033v2#bib.bib59); Zhou et al., [2024a](https://arxiv.org/html/2411.14033v2#bib.bib80)). This protocol dynamically switches between synchronous and asynchronous modes based on task requirements and system load, implementing priority-based routing algorithms to optimize message delivery under varying conditions.

The Consensus Formation Protocol implements distributed decision-making mechanisms through a combination of voting systems and negotiation frameworks (Amirkhani and Barshooi, [2022](https://arxiv.org/html/2411.14033v2#bib.bib3)). This protocol adapts consensus thresholds dynamically based on task criticality and system state, ensuring robust decision-making while maintaining system responsiveness. When proposals conflict, agents can resolve disagreements through negotiation, ensuring that progress is still made despite differences in opinion. Voting protocols allow agents to express preferences and reach a decision even when full consensus is not achievable, thus preventing deadlocks and ensuring that tasks continue to progress.

The Credit Allocation Protocol addresses the challenge of fair contribution assessment through multi-level propagation mechanisms. Agents that participate in tasks receive corresponding credit based on their contributions (Zhou et al., [2024a](https://arxiv.org/html/2411.14033v2#bib.bib80)). By implementing task-specific metrics and performance-based distribution algorithms, this protocol ensures equitable reward allocation while incentivizing collaborative behavior.

The Experience Management Protocol facilitates collective learning through structured logging and pattern extraction mechanisms (Zhang et al., [2024a](https://arxiv.org/html/2411.14033v2#bib.bib78)). Each agent logs its experiences and learning outcomes during task execution. These experience records may include successes, failures, the effectiveness of strategies used, and interactions with other agents. This protocol implements cross-agent knowledge sharing algorithms, enabling systematic improvement of system performance through accumulated experience.

The effectiveness of LaMAS depends on the seamless integration of these protocols, as illustrated in Figure [2](https://arxiv.org/html/2411.14033v2#S2.F2 "Figure 2 ‣ 2.3\. Protocols of Agent Interaction ‣ 2\. Key AI Technical Aspects ‣ LLM-based Multi-Agent Systems: Techniques and Business Perspectives"). The hierarchical organization enables dynamic protocol selection and efficient resource utilization, while maintaining system scalability.

### 2.4\. Agent Training Methods

In LaMAS, each agent has the incentive to improve itself in order to get more credits assigned to them from the platform. In terms of “agent training”, we mean the methods of improving the agent’s performance, including tuning-free methods and parameter-tuning methods.

Tuning-free Methods. In the field of LLM agents, tuning-free methods are strategies to improve performance without modifying model parameters. These methods are beneficial when direct parameter tuning is costly or impractical. Key tuning-free methods include:

*   •

    Prompt Engineering: Prompt engineering involves designing specific input prompts to elicit desired responses from LLM agents. By carefully structuring prompts, agents can align more closely with task requirements without any parameter adjustments (Brown et al., [2020](https://arxiv.org/html/2411.14033v2#bib.bib5)).

*   •

    Few-Shot Learning: Few-shot learning provides limited examples within the prompt, helping the agent understand new tasks. In zero-shot learning, models tackle tasks solely through natural language instructions, leveraging pre-trained knowledge. Both approaches enable flexibility and adaptability in multi-agent environments (Radford et al., [2021](https://arxiv.org/html/2411.14033v2#bib.bib50); Yao et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib75); Zhou et al., [2024b](https://arxiv.org/html/2411.14033v2#bib.bib79)).

*   •

    External Tool Utilization: Agents can enhance their capabilities by interacting with external tools or APIs. Using external resources, such as databases or calculators, allows agents to perform complex tasks without additional model training (Schick et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib55); Patil et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib46)).

These tuning-free methods are particularly valuable in LaMAS. They enable agents to adapt quickly and work together on complex tasks in dynamic environments, supporting smooth collaboration with minimal computational costs.

Parameter-tuning Methods. To directly tune the parameters of the LLMs behind each agent, the alignment methods and multi-agent reinforcement learning methods can be used. First, the alignment methods for tuning LLMs are generally based on supervised learning loss based on the target output or the preference of the experts. Directly fitting the experts’ output corresponds to the behavioral cloning methods in agent imitation learning (Pomerleau, [1991](https://arxiv.org/html/2411.14033v2#bib.bib48)), while training over the experts’ preference pairs improves the agent’s policy in a learning-to-rank manner (Rafailov et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib51)). Such a kind of method has not been much utilized in a multi-agent task as the alignment target is not clearly formulated in such a scenario. Second, multi-agent reinforcement learning (MARL) is a key method for training agent policy in a multi-agent system, which formulates the task as a multi-agent sequential decision-making problem (Busoniu et al., [2008](https://arxiv.org/html/2411.14033v2#bib.bib7)). Here, we mainly consider cooperative MARL methods as, in general, the agents are organized to pursue team success, i.e., to fulfill the user’s task. According to the form of agent coordination in training and execution, MARL methods can be divided into three major categories, namely i) centralized training with centralized execution (Sukhbaatar et al., [2016](https://arxiv.org/html/2411.14033v2#bib.bib61); Wen et al., [2022](https://arxiv.org/html/2411.14033v2#bib.bib68)), ii) centralized training with decentralized execution (Lowe et al., [2017](https://arxiv.org/html/2411.14033v2#bib.bib41); Rashid et al., [2020](https://arxiv.org/html/2411.14033v2#bib.bib53)), and iii) decentralized training and execution (Tan, [1993](https://arxiv.org/html/2411.14033v2#bib.bib62); Tian et al., [2019](https://arxiv.org/html/2411.14033v2#bib.bib63)).

### 2.5\. Attacks and Defenses in LaMAS

As LaMAS systems handle sensitive data and critical operations, their security is a top concern. The distributed nature of LaMAS introduces unique vulnerabilities beyond those of single-LLM systems. Malicious actors can target not only individual agents but also exploit inter-agent communications and collective decision-making processes. This section examines the attacks against LaMAS and the defense mechanisms, with a focus on their implications in multi-agent systems.

Attack Surface and Vulnerabilities. LaMAS face three main types of attacks.

First, prompt injection attacks manipulate input prompts to trick models into generating harmful responses. These attacks are particularly dangerous in LaMAS, where compromised agents can propagate malicious prompts across the system. Recent work (Zou et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib82)) show how slight changes in input phrasing can bypass defenses, while research (Liu et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib40)) demonstrate how system prompts can be modified using escape characters and context omission.

Second, memory and data poisoning attacks target the knowledge bases that agents use for decision-making. In LaMAS, poisoned data can affect multiple agents simultaneously. Research shows how contaminated knowledge bases in retrieval-augmented generation (RAG) systems can cause cascading errors throughout the agent network (Xue et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib73)). Other study highlights how poisoned training samples with specific triggers can compromise fine-tuned agents, impacting system reliability (Yan et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib74)).

Third, model inversion and extraction attacks aim to reconstruct training data or extract model details through targeted queries. Analysis indicates that these attacks are particularly effective in LaMAS, where attackers responses from multi-agents to enhance extraction efficiency (Morris et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib43)). The risk of data leakage is especially high for systems handling sensitive personal or commercial data, as shown by recent work on prompt-based vulnerabilities (Sha and Zhang, [2024](https://arxiv.org/html/2411.14033v2#bib.bib57)).

Defense Mechanisms and Future Directions. Several defense strategies have been proposed to counter these attacks, each addressing specific vulnerabilities in LaMAS environments.

Input sanitization techniques, such as prompt randomization and query encapsulation, help neutralize prompt injection attacks. One work demonstrate the effectiveness of these methods in LaMAS contexts, though they may introduce communication overhead (Robey et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib54)). An improved approach involves adaptive delimiter strategies that maintain communication efficiency (Hines et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib26)).

Perplexity-based filtering is another promising defense. Several researches show that monitoring model perplexity can detect adversarial prompts without compromising model utility (Alon and Kamfonas, [2023](https://arxiv.org/html/2411.14033v2#bib.bib2); Jain et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib29)). In LaMAS, this method can be enhanced by cross-validating perplexity scores across agents, though careful calibration is required to avoid false positives during legitimate interactions.

Adversarially robust fine-tuning has also proven useful for enhancing LaMAS security. A dual-model approach, where adversarial samples are generated and validated during training, offers significant benefits (O’Neill et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib44)). Further optimization focus on balancing robustness and utility, making these techniques particularly valuable for LaMAS by enabling system-wide application while preserving agent specialization (Xhonneux et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib71)).

Despite these advances, challenges remain in securing LaMAS. Current defenses often struggle with the dynamic nature of agent interactions, where complex communication patterns can trigger false positives. Additionally, the computational overhead of comprehensive security measures can affect system performance, requiring a balance between security and efficiency.

Looking ahead, several research directions are crucial. First, there is a need for standardized security evaluation frameworks for LaMAS that account for individual agent vulnerabilities and system-wide risks. Second, developing lightweight security measures that maintain communication efficiency is an open challenge. Finally, adaptive defense mechanisms that evolve with emerging threats will be essential for long-term security.

To ensure the safe deployment of LaMAS, future research should focus on a holistic approach that combines robust model architectures, effective training procedures, and dynamic defense mechanisms. This will be critical for maintaining public trust as LaMAS systems continue to grow in complexity and impact.

## 3\. Key Business Aspects

Drawing from our research on LaMAS, we present our vision of its business implications across three critical dimensions: privacy preservation, traffic monetization, and intelligence monetization. We anticipate how LaMAS could reshape business paradigms and explore the potential trajectories of its commercial applications.

### 3.1\. Privacy Preservation in LaMAS

The rise of LaMAS introduces new privacy challenges that go beyond those of traditional multi-agent systems. Unlike conventional agents, which exchange structured data, LLM agents handle rich, contextual information that may contain sensitive data embedded in natural language conversations, reasoning processes, and knowledge representations. Privacy preservation in LaMAS is critical because these systems process natural language data, which can inadvertently leak sensitive information through semantic connections and implicit knowledge representations.

Privacy-Preserving Challenges. We identify three levels of privacy concerns that require systematic analysis and novel solutions:

*   •

    At the semantic level, the challenge lies in LLMs’ natural language processing, which may inadvertently reveal sensitive information through contextual associations and semantic connections. Traditional privacy mechanisms, designed for structured data, are insufficient for handling such complex information, especially against attacks that exploit semantic vulnerabilities.

*   •

    At the agent interaction level, the continuous exchange of information between agents introduces privacy risks. Sensitive information can be exposed not only through direct content but also through behavioral patterns and response characteristics. Additionally, maintaining conversation history and context windows in each agent creates persistent vulnerabilities over time.

*   •

    At the system architecture level, the distributed nature of LaMAS complicates the enforcement of privacy guarantees across all components while maintaining system efficiency. The dynamic interactions between agents and their evolving knowledge further challenge the implementation of robust privacy protections.

Privacy-Preserving Technologies. Recent research has explored various technologies to address privacy challenges, ranging from cryptographic techniques to system-level solutions.

At the foundational level, Homomorphic Encryption (HE) (Cheon et al., [2017](https://arxiv.org/html/2411.14033v2#bib.bib13)) enables secure computation on encrypted data, supporting private agent-to-agent communication and inference. While promising in privacy-preserving machine learning (Gilad-Bachrach et al., [2016](https://arxiv.org/html/2411.14033v2#bib.bib23)) and secure data sharing (Chen et al., [2017](https://arxiv.org/html/2411.14033v2#bib.bib10)), HE’s computational complexity remains a significant challenge in LaMAS.

Secure Multi-Party Computation (SMPC) (Lindell, [2020](https://arxiv.org/html/2411.14033v2#bib.bib38)) enables secure collaborative computation among multiple agents. SMPC has been used in privacy-preserving data analysis (Li and Xu, [2019](https://arxiv.org/html/2411.14033v2#bib.bib34)) and collaborative learning (Knott et al., [2021](https://arxiv.org/html/2411.14033v2#bib.bib31)) in traditional multi-agent systems, but scaling this technology to large LaMAS remains known.

For system-level protection, Trusted Execution Environments (TEEs) (Costan, [2016](https://arxiv.org/html/2411.14033v2#bib.bib15)) provide hardware-based security guarantees. Technologies such as Intel SGX (Costan, [2016](https://arxiv.org/html/2411.14033v2#bib.bib15)), ARM TrustZone (Pinto and Santos, [2019](https://arxiv.org/html/2411.14033v2#bib.bib47)), and AMD SEV (Sev-Snp, [2020](https://arxiv.org/html/2411.14033v2#bib.bib56)) create secure enclaves for sensitive computations. However, integrating TEEs into LaMAS requires careful consideration of security and performance trade-offs.

Additionally, Differential Privacy (DP) (Dwork, [2006](https://arxiv.org/html/2411.14033v2#bib.bib16)) offers mathematical methods for privacy-preserving data analysis. While effective for protecting sensitive information in collaborative tasks, DP faces unique challenges in natural language processing, such as managing privacy budgets and preserving utility.

Research Directions and Open Challenges. Advancing privacy preservation in LaMAS involves addressing several key issues. First, existing privacy metrics do not fully capture the complexities of semantic information leakage in natural language processing. There is a need for LaMAS-specific privacy frameworks that account for both direct and indirect information flows in semantic spaces. Second, integrating privacy-preserving technologies across LaMAS requires a unified approach that combines data protection, secure computation, and communication security. Performance optimization and scalability remain major hurdles, especially as agent networks expand, and maintaining privacy while enabling efficient collaboration is critical. Future research should focus on creating comprehensive privacy frameworks tailored to LaMAS. This includes standardizing privacy protocols, developing efficient implementations, and establishing evaluation metrics to ensure the practical deployment of privacy-preserving LaMAS systems.

### 3.2\. Traffic Monetization

Traffic Monetization in LaMAS involves generating commercial value by managing user traffic and optimizing ads, using the strengths of various agents. This includes improving traffic flow, boosting click-through rates (CTR), and increasing conversion rates (CVR) (Kumar and Reinartz, [2016](https://arxiv.org/html/2411.14033v2#bib.bib32)). LaMAS leverages each agent’s capabilities to enhance user engagement and make advertising strategies more effective. The system also ensures fair and transparent revenue allocation. This section explores Traffic Monetization in LaMAS, covering business scenarios, revenue models, profit allocation, and the roles of application agents.

![Refer to caption](img/957b894e2f7559decb72b0e2ed889bfd.png)

Figure 3\. Traffic Monetization.

Business Scenarios and Revenue Generation. In LaMAS, agents analyze user behaviors and preferences to optimize traffic management and deploy targeted advertisements. Through real-time data analytics, agents identify the most relevant ads to engage users, thereby enhancing click-through rates (CTR) and conversion rates (CVR). This approach builds user profiles and uses intelligent recommendation systems to personalize ads, boosting engagement and driving purchases. Revenue in LaMAS mainly comes from advertising, using Cost Per Click (CPC) and Cost Per Action (CPA) models (Kannan and Li, [2017](https://arxiv.org/html/2411.14033v2#bib.bib30)). In the CPC model, advertisers pay based on clicks, with agents earning commissions based on their contribution to traffic management and ad effectiveness. In the CPA model, payments are made for completed purchases, rewarding agents who drive conversions with a higher share of the revenue. In addition to advertising, income can be generated through user subscriptions for premium features or personalized services within specific applications, such as advanced analytics dashboards or exclusive access to specialized tools. These monetization strategies work together, with agents optimizing traffic flow, user engagement, and ad targeting to drive overall profitability.

Profit Allocation Mechanisms. Converting revenue into profits that are fairly distributed among agents is key to Traffic Monetization. The process starts by assessing each agent’s contribution to traffic generation, ad clicks, and conversions, using metrics like CTR and CVR to quantify individual impact. To ensure fairness, LaMAS may use blockchain-based smart contracts to automate the distribution process, minimizing bias and human error. Additionally, a scoring system rates agents based on their performance, including factors like user feedback and engagement. Agents with higher scores receive a larger share of revenue, incentivizing better performance and continuous optimization. Attribution methods, such as the Shapley Value, ensure profits are allocated based on each agent’s contribution to the system (Shapley, [1953](https://arxiv.org/html/2411.14033v2#bib.bib58)). Dynamic adjustment mechanisms allow real-time updates to revenue shares based on agent performance and market conditions, ensuring fair compensation for optimizing traffic and ads. Incorporating metrics like CPM (Cost Per Mille) adds a more nuanced approach to revenue allocation, offering a deeper understanding of ad performance beyond just clicks and conversions (Brynjolfsson and McAfee, [2014](https://arxiv.org/html/2411.14033v2#bib.bib6)).

Roles of Application Agents. Different types of application agents play distinct yet interrelated roles in Traffic Monetization. Advertising agents manage and deploy advertisements, using data analytics to optimize ad performance and increase user engagement. They select the best ad placements and adjust strategies based on real-time data and user behavior. Data analysis agents analyze user behavior, providing insights that help advertising agents refine ad strategies and improve content. These agents help create accurate user profiles and identify emerging trends to make advertising more effective. Transaction agents handle user purchases, ensuring smooth transactions and tracking conversions. They link sales performance to specific ads, helping advertisers improve future campaigns and boost conversion rates (CVR). Subscription agents manage premium services, offering personalized features and contributing additional revenue streams. These agents also contribute to long-term user engagement by enhancing retention and loyalty, supporting sustained revenue growth.

Traffic Monetization in LaMAS creates a system where revenue is generated and shared through the collaboration of different agents. By improving traffic management, optimizing ads, and ensuring fair distribution, LaMAS makes sure all agents benefit fairly. Future research should focus on improving attribution models like the Shapley Value to make profit allocation even fairer. Adding metrics like CPM will also help allocate revenue more accurately and improve monetization strategies.

### 3.3\. Intelligence Monetization

Intelligence Monetization in LLM-based Multi-Agent Systems represents a significant evolution in AI commercialization by leveraging the collaborative capabilities of specialized agents. Unlike traditional single-model paradigms, multi-agent systems enable dynamic interactions among specialized agents, each designed to address specific tasks, thereby facilitating the creation of more versatile and robust intelligence solutions. This paradigm has shown promise in various applications, as exemplified by Microsoft’s Copilot Studio Platform, launched on November 19, 2024\. The platform supports an ecosystem of over 1,800 large models and offers open APIs and integration tools, enabling enterprises to incorporate agent technology into workflows and applications for enhanced customization and scalability (VentureBeat, [2024](https://arxiv.org/html/2411.14033v2#bib.bib65)).

Revenue Generation through Data-Driven Services. A key revenue model in Intelligence Monetization within LaMAS is the sale of data-driven services (Sjödin et al., [2021](https://arxiv.org/html/2411.14033v2#bib.bib60); Faroukhi et al., [2020](https://arxiv.org/html/2411.14033v2#bib.bib18); Hajipour et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib25); VentureBeat, [2024](https://arxiv.org/html/2411.14033v2#bib.bib65)). Specialized agents analyze distinct datasets—such as consumer preferences, product usage, and market trends—to generate actionable insights. These insights are delivered in the form of reports, forecasts, or tailored recommendations that businesses can purchase. For instance, one agent may provide personalized user behavior insights to enhance marketing, while another offers market trend analysis to guide strategic planning. By integrating specialized agents, LaMAS offers a broad range of insights, which can be monetized through subscription models or one-time reports. In practice, successful implementations like OpenAI’s GPT-4 API (OpenAI et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib45)) have demonstrated how multiple specialized models can work in concert, with distinct agents handling different aspects of the intelligence pipeline. These include specialized agents for data processing and preprocessing, deep pattern recognition and insight generation, recommendation transformation, and platform integration.

Innovative Licensing and Agent Marketplaces. LaMAS also introduces novel licensing approaches that move beyond traditional software licensing. One of the most prominent is the Agent-as-a-Service (AaaS) (Rajaei, [2024](https://arxiv.org/html/2411.14033v2#bib.bib52)), as seen in Google Cloud’s AutoML, which enables dynamic agent deployment based on computational needs, with usage-based pricing and automatic scaling (Cloud, [2024](https://arxiv.org/html/2411.14033v2#bib.bib14)). Complementing this will be the emergence of agent marketplace platforms, creating ecosystems for third-party agent development and deployment, as demonstrated by Hugging Face’s model hub adapted for deployment of LLMs (Face, [2024](https://arxiv.org/html/2411.14033v2#bib.bib17)). Furthermore, hybrid deployment architectures will become increasingly popular, combining on-premise agent deployment for sensitive operations with cloud-based agents for scalable tasks, as seen in IBM’s Watson services which use distributed agent architecture (IBM, [2024](https://arxiv.org/html/2411.14033v2#bib.bib28)).

Intelligence Monetization within LaMAS offers a sustainable revenue framework by harnessing the collective intelligence of specialized agents. Looking ahead, as demand for AI-driven insights grows, LaMAS’s role in delivering scalable, actionable intelligence will continue to expand. Future developments should focus on enhancing agent collaboration for real-time insights and adapting to emerging business models and industries, offering tailored solutions to meet specific needs.

### 3.4\. Integration of 3 Business Aspects

The three key business aspects of LaMAS form an interconnected framework that drives both commercial success and ethical operation. Data privacy ensures trust and compliance while allowing secure data usage. This foundation supports traffic monetization by generating user engagement data while adhering to privacy regulations. Intelligence monetization transforms these privacy-preserved interactions into actionable insights and services. As LaMAS evolves, maintaining a balance between privacy, commercial success, and technological progress will be critical for long-term sustainability. Future development should focus on strengthening these connections while adapting to evolving privacy regulations and market demands.

![Refer to caption](img/59d5022557f9bef6a7a98cf37c386b96.png)

Figure 4\. Architectures of LaMAS.

## 4\. Case Study

Building on the technical foundations and business considerations of LaMAS, we now delve into real-world implementations to illustrate how these theoretical frameworks are realized in practice. Through carefully selected case studies, we explore how different architectural choices influence system efficiency, data privacy, and monetization capabilities. These examples not only validate our theoretical insights but also shed light on the challenges and opportunities inherent in deploying LaMAS solutions.

### 4.1\. Architectures in LaMAS

The implementation of LaMAS in real-world applications reveals various architectural patterns, each designed to address specific operational requirements and constraints.

![Refer to caption](img/92289e6ec0f27c6d870977f0c6676167.png)

Figure 5\. Centralized Architecture of LaMAS.

![Refer to caption](img/9ac863bc8fabe4f0228a4c4fcd797d1b.png)

Figure 6\. Centralized data handling of LaMAS.

As we analyze current LaMAS deployments, we observe that architectural choices significantly influence system capabilities, from privacy protection to operational efficiency. Figure [4](https://arxiv.org/html/2411.14033v2#S3.F4 "Figure 4 ‣ 3.4\. Integration of 3 Business Aspects ‣ 3\. Key Business Aspects ‣ LLM-based Multi-Agent Systems: Techniques and Business Perspectives") illustrates four fundamental patterns that have emerged in practice:

*   •

    Star Architecture: In this structure, a central agent coordinates communication with all other agents (Qian et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib49); Hong et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib27); Wu et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib70); Xie et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib72); Fu et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib20)). This centralized control model works well when one agent is responsible for task distribution and overall orchestration.

*   •

    Ring Architecture: Agents are arranged in a circular configuration, each communicating with its predecessor and successor (Chan et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib9); Liang et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib36)). This decentralized structure supports sequential task processing, ensuring each agent has a specific role in the task pipeline.

*   •

    Graph Architecture: This network allows for a fully interconnected system where each agent can communicate directly with any other agent (Zhuge et al., [[n.d.]](https://arxiv.org/html/2411.14033v2#bib.bib81); Chen et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib11)).This architecture creates a fully or non-fully interconnected network where each agent can communicate with their neighbors. It provides maximum flexibility and redundancy, allowing multiple communication pathways to support complex interactions.

*   •

    Bus Architecture: This structure uses a fixed workflow or Standard Operating Procedure (SOP), where tasks are sent to a central bus, which then distributes them to the appropriate agents or processes (Li et al., [2023](https://arxiv.org/html/2411.14033v2#bib.bib35); Gao et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib21); Trirat et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib64); Li et al., [2024](https://arxiv.org/html/2411.14033v2#bib.bib33)). The bus ensures a clear input-output mechanism and a structured flow of tasks in a sequential manner.

### 4.2\. A Decentralized Star Architecture in LaMAS

Figure [5](https://arxiv.org/html/2411.14033v2#S4.F5 "Figure 5 ‣ 4.1\. Architectures in LaMAS ‣ 4\. Case Study ‣ LLM-based Multi-Agent Systems: Techniques and Business Perspectives") and Figure [6](https://arxiv.org/html/2411.14033v2#S4.F6 "Figure 6 ‣ 4.1\. Architectures in LaMAS ‣ 4\. Case Study ‣ LLM-based Multi-Agent Systems: Techniques and Business Perspectives") present our first case study of LaMAS implementation in a music service scenario. The system uses a centralized architecture where several agents, including a Personal Agent, an Orchestrator Agent, and a Song Agent, collaborate to process user’s music playback requests. In this setup, the Orchestrator Agent acts as a central hub, managing communication and coordinating tasks among the agents.

However, this centralized approach means that all agents must send their data, including sensitive user information, through the Orchestrator Agent to complete tasks. While efficient for coordination, this design creates potential privacy and security risks since user data passes through multiple agents.

![Refer to caption](img/5ea1846594ac348068f75c7c40942553.png)

Figure 7\. Decentralized Star Architecture of LaMAS.

![Refer to caption](img/41ce0605d8752353cf02c79e3b3fa9b2.png)

Figure 8\. Decentralized data handling of LaMAS.

To address these privacy concerns, we propose a modified decentralized Star Architecture, illustrated in Figure [7](https://arxiv.org/html/2411.14033v2#S4.F7 "Figure 7 ‣ 4.2\. A Decentralized Star Architecture in LaMAS ‣ 4\. Case Study ‣ LLM-based Multi-Agent Systems: Techniques and Business Perspectives") and Figure [8](https://arxiv.org/html/2411.14033v2#S4.F8 "Figure 8 ‣ 4.2\. A Decentralized Star Architecture in LaMAS ‣ 4\. Case Study ‣ LLM-based Multi-Agent Systems: Techniques and Business Perspectives") through a travel booking scenario. In this new design, the Orchestrator Agent still coordinates tasks but avoids directly handling sensitive data. Instead, specialized agents, like the Navigation Agent or Ticket Agent, process their tasks independently and interact directly with user data when needed. This setup reduces privacy risks while maintaining system efficiency.

In the decentralized architecture, the Orchestrator Agent focuses on breaking down user instructions into smaller tasks and deciding the order of task execution. It stays uninvolved in sensitive data processing and only reconnects when tasks are completed or additional coordination is required. Each specialized agent handles its specific tasks within its own data domain, ensuring privacy and security.

A fair credit allocation system, as shown in Figure [6](https://arxiv.org/html/2411.14033v2#S4.F6 "Figure 6 ‣ 4.1\. Architectures in LaMAS ‣ 4\. Case Study ‣ LLM-based Multi-Agent Systems: Techniques and Business Perspectives") and Figure [8](https://arxiv.org/html/2411.14033v2#S4.F8 "Figure 8 ‣ 4.2\. A Decentralized Star Architecture in LaMAS ‣ 4\. Case Study ‣ LLM-based Multi-Agent Systems: Techniques and Business Perspectives"), managed by the Transaction Logger, ensures that all agents receive appropriate rewards based on the tasks they complete and the resources they use. This credit allocation approach improves data protection and system security while keeping operations efficient.

## 5\. Conclusion & Future

In this paper, we provide our analysis on the future development of LLM-based Multi-Agent Systems (LaMAS) from the perspectives of techniques and business. Technically, compared to the traditional single-LLM-agent systems, LaMAS has a higher potential for overall performance and system flexibility; and commercially, LaMAS brings the feasibility of proprietary data preservability and monetization through traffic and intelligence, which essentially incentivizes various entities to contribute to the whole ecosystem. Several effective protocols for multi-agent communication and collaboration are developed in progress, which will drive the implementation of the LaMAS ecosystem towards achieving artificial collective intelligence in the near future.

## References

*   (1)
*   Alon and Kamfonas (2023) Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks with perplexity. *arXiv preprint arXiv:2308.14132* (2023).
*   Amirkhani and Barshooi (2022) Abdollah Amirkhani and Amir Hossein Barshooi. 2022. Consensus in multi-agent systems: a review. *Artif. Intell. Rev.* 55, 5 (June 2022), 3897–3935. [https://doi.org/10.1007/s10462-021-10097-x](https://doi.org/10.1007/s10462-021-10097-x)
*   Aziz (2010) Haris Aziz. 2010. Multiagent systems: algorithmic, game-theoretic, and logical foundations by Y. Shoham and K. Leyton-Brown Cambridge University Press, 2008. *SIGACT News* 41, 1 (March 2010), 34–37. [https://doi.org/10.1145/1753171.1753181](https://doi.org/10.1145/1753171.1753181)
*   Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, et al. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]
*   Brynjolfsson and McAfee (2014) E. Brynjolfsson and A. McAfee. 2014. The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies. *Journal of Advertising Research* (2014).
*   Busoniu et al. (2008) Lucian Busoniu, Robert Babuska, and Bart De Schutter. 2008. A comprehensive survey of multiagent reinforcement learning. *IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)* 38, 2 (2008), 156–172.
*   Caffagni et al. (2024) Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. 2024. The (r) evolution of multimodal large language models: A survey. *arXiv preprint arXiv:2402.12451* (2024).
*   Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. arXiv:2308.07201 [cs.CL] [https://arxiv.org/abs/2308.07201](https://arxiv.org/abs/2308.07201)
*   Chen et al. (2017) Hao Chen, Kim Laine, and Peter Rindal. 2017. Fast private set intersection from homomorphic encryption. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security*. 1243–1255.
*   Chen et al. (2023) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. 2023. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. *arXiv preprint arXiv:2308.10848* 2, 4 (2023), 5.
*   Chen et al. (2024) Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2024. Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence. arXiv:2407.07061 [cs.CL] [https://arxiv.org/abs/2407.07061](https://arxiv.org/abs/2407.07061)
*   Cheon et al. (2017) Jung Hee Cheon, Andrey Kim, Miran Kim, and Yongsoo Song. 2017. Homomorphic encryption for arithmetic of approximate numbers. In *Advances in Cryptology–ASIACRYPT 2017: 23rd International Conference on the Theory and Applications of Cryptology and Information Security, Hong Kong, China, December 3-7, 2017, Proceedings, Part I 23*. Springer, 409–437.
*   Cloud (2024) Google Cloud. 2024. AutoML - Google Cloud. [https://cloud.google.com/automl/](https://cloud.google.com/automl/) Accessed: 2024-11-12.
*   Costan (2016) Victor Costan. 2016. Intel SGX explained. *IACR Cryptol, EPrint Arch* (2016).
*   Dwork (2006) Cynthia Dwork. 2006. Differential privacy. In *International colloquium on automata, languages, and programming*. Springer, 1–12.
*   Face (2024) Hugging Face. 2024. Hugging Face Model Hub. [https://huggingface.co/models](https://huggingface.co/models) Accessed: 2024-11-12.
*   Faroukhi et al. (2020) Abou Zakaria Faroukhi, Imane El Alaoui, Youssef Gahi, and Aouatif Amine. 2020. Big data monetization throughout Big Data Value Chain: a comprehensive review. *Journal of Big Data* 7 (2020), 1–22.
*   Fourney et al. (2024) Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. 2024. Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks. arXiv:2411.04468 [cs.AI] [https://arxiv.org/abs/2411.04468](https://arxiv.org/abs/2411.04468)
*   Fu et al. (2024) Dayuan Fu, Biqing Qi, Yihuai Gao, Che Jiang, Guanting Dong, and Bowen Zhou. 2024. MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making. arXiv:2409.16686 [cs.AI] [https://arxiv.org/abs/2409.16686](https://arxiv.org/abs/2409.16686)
*   Gao et al. (2024) Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, Liuyi Yao, Hongyi Peng, Zeyu Zhang, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024. AgentScope: A Flexible yet Robust Multi-Agent Platform. arXiv:2402.14034 [cs.MA] [https://arxiv.org/abs/2402.14034](https://arxiv.org/abs/2402.14034)
*   Ghafarollahi and Buehler (2024) Alireza Ghafarollahi and Markus J. Buehler. 2024. SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning. arXiv:2409.05556 [cs.AI] [https://arxiv.org/abs/2409.05556](https://arxiv.org/abs/2409.05556)
*   Gilad-Bachrach et al. (2016) Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. 2016. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In *International conference on machine learning*. PMLR, 201–210.
*   Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large Language Model based Multi-Agents: A Survey of Progress and Challenges. arXiv:2402.01680 [cs.CL] [https://arxiv.org/abs/2402.01680](https://arxiv.org/abs/2402.01680)
*   Hajipour et al. (2023) Vahid Hajipour, Siavash Hekmat, and Mohammad Amini. 2023. A value-oriented Artificial Intelligence-as-a-Service business plan using integrated tools and services. *Decision Analytics Journal* 8 (2023), 100302. [https://doi.org/10.1016/j.dajour.2023.100302](https://doi.org/10.1016/j.dajour.2023.100302)
*   Hines et al. (2024) Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. 2024. Defending Against Indirect Prompt Injection Attacks With Spotlighting. *arXiv preprint arXiv:2403.14720* (2024).
*   Hong et al. (2024) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2024. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. In *The Twelfth International Conference on Learning Representations*. [https://openreview.net/forum?id=VtmBAGCN7o](https://openreview.net/forum?id=VtmBAGCN7o)
*   IBM (2024) IBM. 2024. watsonx Assistant. [https://www.ibm.com/cn-zh/products/watsonx-assistant](https://www.ibm.com/cn-zh/products/watsonx-assistant) Accessed: 2024-11-12.
*   Jain et al. (2023) Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. *arXiv preprint arXiv:2309.00614* (2023).
*   Kannan and Li (2017) P. K. Kannan and H. Li. 2017. Digital Advertising: A Review and Future Research Directions. *Journal of Advertising* (2017).
*   Knott et al. (2021) Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim, and Laurens van der Maaten. 2021. Crypten: Secure multi-party computation meets machine learning. *Advances in Neural Information Processing Systems* 34 (2021), 4961–4973.
*   Kumar and Reinartz (2016) V. Kumar and W. Reinartz. 2016. *Creating Enduring Customer Value*. Wharton School Press.
*   Li et al. (2024) Ao Li, Yuexiang Xie, Songze Li, Fugee Tsung, Bolin Ding, and Yaliang Li. 2024. Agent-Oriented Planning in Multi-Agent Systems. arXiv:2410.02189 [cs.AI] [https://arxiv.org/abs/2410.02189](https://arxiv.org/abs/2410.02189)
*   Li and Xu (2019) Yi Li and Wei Xu. 2019. PrivPy: General and scalable privacy-preserving data mining. In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*. 1299–1307.
*   Li et al. (2023) Yuan Li, Yixuan Zhang, and Lichao Sun. 2023. MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents. arXiv:2310.06500 [cs.AI] [https://arxiv.org/abs/2310.06500](https://arxiv.org/abs/2310.06500)
*   Liang et al. (2024) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. arXiv:2305.19118 [cs.CL] [https://arxiv.org/abs/2305.19118](https://arxiv.org/abs/2305.19118)
*   Lin et al. (2024) Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, and Weinan Zhang. 2024. Hammer: Robust Function-Calling for On-Device Language Models via Function Masking. arXiv:2410.04587 [cs.LG] [https://arxiv.org/abs/2410.04587](https://arxiv.org/abs/2410.04587)
*   Lindell (2020) Yehuda Lindell. 2020. Secure multiparty computation. *Commun. ACM* 64, 1 (2020), 86–96.
*   Liu et al. (2024) Wei Liu, Chenxi Wang, Yifei Wang, Zihao Xie, Rennai Qiu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, and Chen Qian. 2024. Autonomous Agents for Collaborative Task under Information Asymmetry. arXiv:2406.14928 [cs.AI] [https://arxiv.org/abs/2406.14928](https://arxiv.org/abs/2406.14928)
*   Liu et al. (2023) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, et al. 2023. Prompt Injection attack against LLM-integrated Applications. *arXiv preprint arXiv:2306.05499* (2023).
*   Lowe et al. (2017) Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. 2017. Multi-agent actor-critic for mixed cooperative-competitive environments. *Advances in neural information processing systems* 30 (2017).
*   Marro et al. (2024) Samuele Marro, Emanuele La Malfa, Jesse Wright, Guohao Li, Nigel Shadbolt, Michael Wooldridge, and Philip Torr. 2024. A Scalable Communication Protocol for Networks of Large Language Models. arXiv:2410.11905 [cs.AI] [https://arxiv.org/abs/2410.11905](https://arxiv.org/abs/2410.11905)
*   Morris et al. (2023) John X Morris, Wenting Zhao, Justin T Chiu, Vitaly Shmatikov, and Alexander M Rush. 2023. Language model inversion. *arXiv preprint arXiv:2311.13647* (2023).
*   O’Neill et al. (2023) Charles O’Neill, Jack Miller, Ioana Ciuca, Yuan-Sen Ting, and Thang Bui. 2023. Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content. *arXiv preprint arXiv:2308.13768* (2023).
*   OpenAI et al. (2024) OpenAI, Josh Achi@ARTICLE7414384, author=Agiwal, Mamta and Roy, Abhishek and Saxena, Navrati, journal=IEEE Communications Surveys & Tutorials, title=Next Generation 5G Wireless Networks: A Comprehensive Survey, year=2016, volume=18, number=3, pages=1617-1655, keywords=5G mobile communication;Wireless communication;Computer architecture;Microprocessors;MIMO;Streaming media;5G;mm-wave;beamforming;channel model;C-RAN;SDN;HetNets;massive MIMO;SDMA;IDMA;D2D;M2M;IoT;QoE;SON;sustainability;field trials, doi=10.1109/COMST.2016.2532458 am, Steven Adler, Sandhini Agarwal, et al. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)
*   Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large Language Model Connected with Massive APIs. arXiv:2305.15334 [cs.CL] [https://arxiv.org/abs/2305.15334](https://arxiv.org/abs/2305.15334)
*   Pinto and Santos (2019) Sandro Pinto and Nuno Santos. 2019. Demystifying arm trustzone: A comprehensive survey. *ACM computing surveys (CSUR)* 51, 6 (2019), 1–36.
*   Pomerleau (1991) Dean A Pomerleau. 1991. Efficient training of artificial neural networks for autonomous navigation. *Neural computation* 3, 1 (1991), 88–97.
*   Qian et al. (2023) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ChatDev: Communicative Agents for Software Development. *arXiv preprint arXiv:2307.07924* (2023). [https://arxiv.org/abs/2307.07924](https://arxiv.org/abs/2307.07924)
*   Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, et al. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020 [cs.CL]
*   Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. *Advances in Neural Information Processing Systems* 36 (2024).
*   Rajaei (2024) Saman Rajaei. 2024. Multi-Agent-as-a-Service — A Senior Engineer’s Overview. [https://towardsdatascience.com/multi-agent-as-a-service-a-senior-engineers-overview-fc759f5bbcfa](https://towardsdatascience.com/multi-agent-as-a-service-a-senior-engineers-overview-fc759f5bbcfa).
*   Rashid et al. (2020) Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. 2020. Monotonic value function factorisation for deep multi-agent reinforcement learning. *Journal of Machine Learning Research* 21, 178 (2020), 1–51.
*   Robey et al. (2023) Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. 2023. Smoothllm: Defending large language models against jailbreaking attacks. *arXiv preprint arXiv:2310.03684* (2023).
*   Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. arXiv:2302.04761 [cs.CL] [https://arxiv.org/abs/2302.04761](https://arxiv.org/abs/2302.04761)
*   Sev-Snp (2020) AMD Sev-Snp. 2020. Strengthening VM isolation with integrity protection and more. *White Paper, January* 53 (2020), 1450–1465.
*   Sha and Zhang (2024) Zeyang Sha and Yang Zhang. 2024. Prompt stealing attacks against large language models. *arXiv preprint arXiv:2402.12959* (2024).
*   Shapley (1953) Lloyd Shapley. 1953. A Value for n-Person Games. In *Contributions to the Theory of Games*. Vol. II. Princeton University Press, 307–317.
*   Shoham and Leyton-Brown (2009) Yoav Shoham and Kevin Leyton-Brown. 2009. Multiagent systems: Algorithmic, game-theoretic, and logical foundations.
*   Sjödin et al. (2021) David Sjödin, Vinit Parida, Maximilian Palmié, and Joakim Wincent. 2021. How AI capabilities enable business model innovation: Scaling AI through co-evolutionary processes and feedback loops. *Journal of Business Research* 134 (2021), 574–587. [https://doi.org/10.1016/j.jbusres.2021.05.009](https://doi.org/10.1016/j.jbusres.2021.05.009)
*   Sukhbaatar et al. (2016) Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learning multiagent communication with backpropagation. *Advances in neural information processing systems* 29 (2016).
*   Tan (1993) Ming Tan. 1993. Multi-agent reinforcement learning: Independent vs. cooperative agents. In *Proceedings of the tenth international conference on machine learning*. 330–337.
*   Tian et al. (2019) Zheng Tian, Ying Wen, Zhichen Gong, Faiz Punakkath, Shihao Zou, and Jun Wang. 2019. A regularized opponent model with maximum entropy objective. In *Proceedings of the 28th International Joint Conference on Artificial Intelligence*. 602–608.
*   Trirat et al. (2024) Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. 2024. AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML. arXiv:2410.02958 [cs.LG] [https://arxiv.org/abs/2410.02958](https://arxiv.org/abs/2410.02958)
*   VentureBeat (2024) VentureBeat. 2024. Microsoft quietly assembles the largest AI agent ecosystem — and no one else is close. *VentureBeat* (2024). [https://venturebeat.com/ai/microsoft-quietly-assembles-the-largest-ai-agent-ecosystem-and-no-one-else-is-close/](https://venturebeat.com/ai/microsoft-quietly-assembles-the-largest-ai-agent-ecosystem-and-no-one-else-is-close/) Accessed: 2024-11-21.
*   Wang et al. (2024) Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel M Ni, et al. 2024. OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models. *arXiv preprint arXiv:2410.09671* (2024).
*   Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL] [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)
*   Wen et al. (2022) Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang. 2022. Multi-agent reinforcement learning is a sequence modeling problem. *Advances in Neural Information Processing Systems* 35 (2022), 16509–16521.
*   Wooldridge (2009) Michael Wooldridge. 2009. An Introduction to MultiAgent Systems.
*   Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. *arXiv preprint arXiv:2308.08155* (2023).
*   Xhonneux et al. (2024) Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, and Leo Schwinn. 2024. Efficient adversarial training in llms with continuous attacks. *arXiv preprint arXiv:2405.15589* (2024).
*   Xie et al. (2023) Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. 2023. Openagents: An open platform for language agents in the wild. *arXiv preprint arXiv:2310.10634* (2023).
*   Xue et al. (2024) Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, and Qian Lou. 2024. BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models. *arXiv preprint arXiv:2406.00083* (2024).
*   Yan et al. (2024) Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. 2024. Backdooring instruction-tuned large language models with virtual prompt injection. In *Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)*. 6065–6086.
*   Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs.CL] [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)
*   Zhang et al. (2024b) Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. 2024b. Large Language Model-Brained GUI Agents: A Survey. arXiv:2411.18279 [cs.AI] [https://arxiv.org/abs/2411.18279](https://arxiv.org/abs/2411.18279)
*   Zhang et al. (2024c) Weinan Zhang, Junwei Liao, Ning Li, and Kounianhua Du. 2024c. Agentic Information Retrieval. arXiv:2410.09713 [cs.IR] [https://arxiv.org/abs/2410.09713](https://arxiv.org/abs/2410.09713)
*   Zhang et al. (2024a) Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024a. A Survey on the Memory Mechanism of Large Language Model based Agents. arXiv:2404.13501 [cs.AI] [https://arxiv.org/abs/2404.13501](https://arxiv.org/abs/2404.13501)
*   Zhou et al. (2024b) Ruiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen, Wenhao Wang, Chunling Xi, Guoqiang Xu, Yong Yu, and Weinan Zhang. 2024b. TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision. In *Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval* (Washington DC, USA) *(SIGIR ’24)*. Association for Computing Machinery, New York, NY, USA, 3–13. [https://doi.org/10.1145/3626772.3657788](https://doi.org/10.1145/3626772.3657788)
*   Zhou et al. (2024a) Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. 2024a. Symbolic Learning Enables Self-Evolving Agents. arXiv:2406.18532 [cs.CL] [https://arxiv.org/abs/2406.18532](https://arxiv.org/abs/2406.18532)
*   Zhuge et al. ([n.d.]) Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. [n.d.]. GPTSwarm: Language Agents as Optimizable Graphs. In *Forty-first International Conference on Machine Learning*.
*   Zou et al. (2023) Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. *arXiv preprint arXiv:2307.15043* (2023).