<!--yml
category: 未分类
date: 2025-01-11 12:14:38
-->

# LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents

> 来源：[https://arxiv.org/html/2409.11393/](https://arxiv.org/html/2409.11393/)

[![[Uncaptioned image]](img/5bc393116fd78fb04651c8fac32bb08d.png) Amine Ben Hassouna](https://orcid.org/0009-0005-8915-7905) amine.benhassouna@medtech.tn, amine.benhassouna@dracodes.com (Corresponding author) Mediterranean Institute of Technology, South Mediterranean University, Tunis, Tunisia  Dracodes, Tunis, Tunisia   [![[Uncaptioned image]](img/c3e934a253ba9ab1514f01a405960886.png) Hana Chaari](https://orcid.org/0009-0002-2629-5102)^§ hana.chaari@medtech.tn, hana.chaari@dracodes.com Mediterranean Institute of Technology, South Mediterranean University, Tunis, Tunisia  Dracodes, Tunis, Tunisia  [![[Uncaptioned image]](img/b9a9330b7440933f12ac0dcec199e20a.png) Ines Belhaj](https://orcid.org/0009-0008-3435-740X)^§ ines.bel-hadj@medtech.tn, ines.bel-hadj@dracodes.com Mediterranean Institute of Technology, South Mediterranean University, Tunis, Tunisia  Dracodes, Tunis, Tunisia 

###### Abstract

In an era where vast amounts of data are collected and processed from diverse sources, there is a growing demand to develop sophisticated AI systems capable of intelligently fusing and analyzing this information. To address these challenges, researchers have turned towards integrating tools into LLM-powered agents to enhance the overall information fusion process. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity and terminological inconsistencies among researchers. To address these issues, we propose a novel LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) that aims to establish a clear foundation for agent development from both functional and software architectural perspectives. Our framework clearly distinguishes between the different components of an LLM-based agent, setting LLMs, and tools apart from a new element, the core-agent, playing the role of the central coordinator of the agent. This pivotal entity comprises five modules: planning, memory, profile, action, and security—the latter often neglected in previous works. By classifying core-agents into passive and active types based on their authoritative natures, we propose various multi-core agent architectures that combine unique characteristics of distinctive agents to tackle complex tasks more efficiently. We evaluate our framework by applying it to thirteen state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects. Moreover, we thoroughly assess five of our proposed architectures through the integration of existing agents into new hybrid active/passive core-agents architectures. This analysis provides clear insights into potential improvements and highlights challenges involved in combining specific agents.

^§^§footnotetext: Equal contribution.

*Keywords* LLM-based agent  $\cdot$ software architecture  $\cdot$ modularity  $\cdot$ privacy  $\cdot$ security  $\cdot$ classification  $\cdot$ multi-core agent

## 1 Introduction

Large Language Models (LLMs) excel in tasks like language modeling, question answering, sentiment analysis, Natural Language Understanding (NLU), commonsense reasoning and knowledge fusion [[1](https://arxiv.org/html/2409.11393v2#bib.bib1), [2](https://arxiv.org/html/2409.11393v2#bib.bib2)]. However, standalone LLMs lacks other skills such as information retrieval, mathematical reasoning, code evaluation, and numerous others. These functional shortcomings can be managed by AI agents leveraging external tools, knowledge repositories and human feedback. An autonomous agent is a system interacting with an environment, sensing it, and acting on it over time following a certain agenda [[3](https://arxiv.org/html/2409.11393v2#bib.bib3)]. While there are different classes of agents, in this paper we will be focusing on LLM-based agents which, by combining the capabilities of LLMs and autonomous agents, can achieve a broad range of tasks [[1](https://arxiv.org/html/2409.11393v2#bib.bib1)].

In fact, the shift towards more natural conversational interfaces powered by LLMs is transforming the way humans engage with agents, enabling seamless and intuitive interactions. Furthermore, LLM-based agents play an essential role as information fusion intermediaries by intelligently synthesizing data from heterogeneous sources and providing meaningful insights to human users or other AI systems. These LLM-powered agents have now become dominant in the landscape of AI agents, and they are regarded as potential steppingstones towards Artificial General Intelligence (AGI), offering hope for the development of AI agents that can adapt to diverse scenarios [[4](https://arxiv.org/html/2409.11393v2#bib.bib4)].

Understanding the potential of these agents and improving them necessitates a deep understanding of their structure. In LLM-based agents, besides the LLM who handles reasoning, there are other components that oversee the execution of tasks, ensure the security of the agent, and handle its memory [[4](https://arxiv.org/html/2409.11393v2#bib.bib4)]. But merely pinpointing existing functionalities is insufficient for developers and researchers. To provide a comprehensive overview of these agents, survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] proposed a comprehensive framework for their construction, consisting of four main modules which will be discussed thoroughly in the next section. Although it provides valuable insights into the functionality of each module in an agent, it overlooks their delineation from a software architectural perspective. This perspective is crucial for developers to establish a common base architecture to build upon and for researchers to improve. Our analysis of the state-of-the-art LLM-based agents reveals common limitations: the complexity of implementation, resulting in unstructured and ambiguous software architecture; lack of modularity and composability, making components non-reusable by other agent-based solutions; and difficulty in maintainability and introducing improvements to existing agents.

In response to these limitations, we propose the LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF). To the best of our knowledge, our framework is the first to emphasize a clear delineation of each component within an LLM-powered agent and define their interactions within specified boundaries from both architectural and functional perspectives. In addition to traditional components like LLMs and tools, we introduce a new unit within the agent, which we label as the "core-agent". This pioneering entity is further classified into two types – active and passive core-agents – which enhance our understanding of each component’s capabilities and accurately describe the dynamics between modules within the agent. As a result, we alleviate the complexity of the architecture and improve the reusability of the components, promoting a shift from multi-agent systems to multi-core agents.

Throughout our paper, we highlight several advantages offered by the LLM-Agent-UMF, from resolving terminological ambiguities to the introduction of enhancing modules like the security module. To validate the reliability of our framework, we apply it to existing solutions and identify the modules within each agent. This approach enables us to assess the feasibility and requirements for merging one agent with another, ultimately aiming to create an agent with fully enhanced capabilities.

Compared with previous works, the five main contributions of this paper can be summarized as follows:

1.  1.

    We introduce a new terminology, core-agent, as a structural sub-component in LLM-based agents to improve modularity and promote more effective and precise communication among researchers and contributors in the field of agents and LLM technologies.

2.  2.

    We model the internal structure of a core-agent by adapting the framework suggested by [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] that was originally meant to describe the whole agent from an abstract functional perspective.

3.  3.

    We improve our modeling framework by augmenting the core-agent with a security module and introducing new methods within other modules.

4.  4.

    We classify core-agents into active and passive ones, explaining their differences and similarities, and highlighting their unique advantages.

5.  5.

    Finally, we introduce various multi-core agent architectures, emphasizing that the hybrid one-active-many-passive core-agent architecture is the optimal setup for LLM-based agents.

The rest of this paper is organized as follows. Section [2](https://arxiv.org/html/2409.11393v2#S2 "2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") covers a background on LLM-based agents and reviews relevant state-of-the-art works. Section [3](https://arxiv.org/html/2409.11393v2#S3 "3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") introduces our LLM-based agent unified modeling framework and possible architectures. Section [4](https://arxiv.org/html/2409.11393v2#S4 "4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") evaluates the efficiency of our proposed agent designs leveraging the capabilities of multiple distinctive agents. Finally, Section [5](https://arxiv.org/html/2409.11393v2#S5 "5 Conclusion and future work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") summarizes key findings and discusses future challenges.

## 2 Related Work

We will start off this section by providing a comprehensive overview of the key concepts that serve as the fundamental basis for our work. First, tool-augmented LLMs are a major advancement in Natural Language Processing (NLP) that combine the language understanding and generation capabilities of LLMs with the ability to interface with external tools and Application Programming Interfaces (APIs). For instance, TALM [[6](https://arxiv.org/html/2409.11393v2#bib.bib6)] introduces models which can leverage a wide range of functionalities, from information retrieval to task planning and execution.

By incorporating tool-augmented LLMs, LLM-based autonomous agents exhibit exceptional proficiency in NLP tasks [[7](https://arxiv.org/html/2409.11393v2#bib.bib7)], including reasoning [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)], programming [[9](https://arxiv.org/html/2409.11393v2#bib.bib9)], and text generation, surpassing other types of agents in these areas. Moreover, they address several limitations of standalone LLMs, such as context length constraints and the inability to utilize tools. This development marks a significant breakthrough in the scientific community, explaining the recent surge in the adoption of LLM-based agents.

Researchers and practitioners across various disciplines are leveraging these agents to tackle complex problems and drive innovation in fields such as gaming [[10](https://arxiv.org/html/2409.11393v2#bib.bib10)] and other professional domains that require specialized expertise [[11](https://arxiv.org/html/2409.11393v2#bib.bib11)]. Additionally, the trend towards integrating LLMs in scientific research, namely in chemistry [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)], highlights their transformative potential, promising to drive forward new discoveries and applications.

Upon examining existing agents, we detected a notable absence of direct security measures or guardrails within the agents to ensure the protection of sensitive information and enhance overall system integrity. Indeed, the level of autonomy in LLM-based agents raises significant concerns regarding ethical use, malicious data, privacy and robustness [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]. Notably, one critical risk involves jailbreaks which can be mitigated through the implementation of more robust monitoring and control mechanisms to detect and respond to jailbreaks during deployment [[14](https://arxiv.org/html/2409.11393v2#bib.bib14)]. Moreover, data privacy remains a persistent challenge in any software system, including agents. To address this, various methods have been developed to safeguard against data extraction attempts from prompts, such as leveraging privacy-preserving algorithms for prompt learning [[15](https://arxiv.org/html/2409.11393v2#bib.bib15)]. These findings prompted us to place an emphasis on this often-neglected aspect and include security measures in our framework.

Besides security assurance, the development of AI systems, particularly agents, must adhere to fundamental software development principles such as modularity, composability, and maintainability. These principles enhance the flexibility, scalability, and adaptability of AI systems, enabling them to effectively meet the evolving needs of both users and businesses.

![Refer to caption](img/d53eaea8f765f38bb8e274819a833936.png)

Figure 1: Framework proposed by survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] for LLM-based agents

Existing agents do not necessarily respect these principles as they do not focus primarily on architectural design. This underscores the importance of architectural frameworks as blueprints for LLM-based agents, highlighting their essential role in releasing the full potential of these systems. Such frameworks enable the development of modular, robust, extensible, and interoperable designs. They provide the necessary scaffolding to build increasingly capable and reliable agents capable of tackling complex real-world problems.

For example, the paper [[7](https://arxiv.org/html/2409.11393v2#bib.bib7)] exploring LLM-based intelligent agents points out 5 main axes: Planning, Memory, Rethinking, Environment, and Action. Despite their attempt to delineate between LLMs, environment and tools, they did not define the software components of the agent. Likewise, the framework proposed by the survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] identifies the structure and applications of LLM-powered autonomous agents and highlights four key modules as shown in Figure [1](https://arxiv.org/html/2409.11393v2#S2.F1 "Figure 1 ‣ 2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"): the Profile module, the Memory module, the Planning module, and the Action module.

The profile module serves to delineate the diverse roles of the LLM. Meanwhile, the memory module retains internal logs encompassing the agent’s past thoughts, actions, and observations within its dynamic environment, including interactions with users. The planning module guides the agent in decomposing overarching tasks into manageable steps or subtasks, enhancing responsiveness by leveraging past behaviors in future plans. Together, these modules significantly influence the action module, which translates the agent’s decisions into specific outputs harnessing external tools to extend the agent’s capabilities [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)].

While this framework effectively addresses the functionalities of the agent, it does present some areas for improvement. First, there are functional overlaps in certain modules, particularly the overlap of reflective activities between planning and memory modules. Second, the definition of memory is ambiguous as will be discussed in Section [3.1](https://arxiv.org/html/2409.11393v2#S3.SS1 "3.1 Core-Agent: Keystone component of an LLM-based agent ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), leading to confusion as the term is used to represent different concepts without clear distinction. Lastly, as illustrated in Figure [1](https://arxiv.org/html/2409.11393v2#S2.F1 "Figure 1 ‣ 2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), it is not explicitly stated whether the LLM, tools, data sources, and memory are part of the agent. This fuzzy distinction between the functionalities of each module foster division between software developers and leads to incompatibility and discourages reusability. In the next section, we introduce the main components of our framework, explain the rationale behind their inclusion, and highlight how they solve the limitations present in other works.

## 3 LLM-based Agent Unified Modeling Framework

In order to comprehensively explore LLM-based agents’ capabilities, a rigorous study was conducted across numerous scholarly databases and online digital libraries. These sources include ScienceDirect, Scopus, Springer, Nature, IEEE Xplore, ACM Digital Library, ACL Anthology, NeurIPS proceedings, AAAI library, arXiv, Google Scholar, Semantic Scholar, among others. The analysis was conducted on articles focusing on tool-powered LLMs, LLM-based agents, LLM planning strategies, information fusion, AI ethical concerns, AI privacy considerations, LLM security and guardrails, software modeling best practices, and terminological consistency in the context of AI systems.

In this section, we start by introducing the fundamental component of our framework, the core-agent, distinguishing it from other elements within an LLM-based agent. By examining the existing literature and analyzing their shortcomings, we delineate a unified approach to designing the internal structure of the core-agent. This comprehensive analysis led us to classify core-agents into two categories: active and passive core-agents. Finally, we outline various multi-core agent architectures that highlight the efficiency of LLM-Agent-UMF in modeling multi-core agent systems.

### 3.1 Core-Agent: Keystone component of an LLM-based agent

Several works have aimed to establish a well-defined framework for building LLM-based agents. For instance, the paper [[16](https://arxiv.org/html/2409.11393v2#bib.bib16)] presents a framework for designing educational problem-solving simulations using LLM-powered agents. The authors emphasize that separating the AI agent from the environment is important in the design process. However, the paper does not provide a clear framework for the agent’s components, making it challenging for future work to identify specific points of modification or reuse. Without a transparent and detailed outline of the agent’s architecture and component interactions, it is difficult to pinpoint where changes can be made to alter the agent’s behavior or how its components could be reused and integrated in other systems.

![Refer to caption](img/4a24bc6aca50dd1d39a55a4cb1763d47.png)

Figure 2: The core-agent as the central component of LLM-based agents

This issue arises from a lack of modularity in the software design, which can be mitigated by adhering to the Single Responsibility Principle (SRP). As emphasized in the paper [[17](https://arxiv.org/html/2409.11393v2#bib.bib17)], the SRP is crucial in software development because it provides granularity at different levels of the software, both in terms of code and functionalities. This granularity facilitates the implementation of future improvements and enhances reusability opportunities. Additionally, applying this principle prevents various code smells, ensuring the modularity of the code and thus making it easier for practitioners to manage and maintain the software.

Paper [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)] attempts to establish boundaries between LLM-based agents, the tools they utilize, and their surrounding environment. Although the proposed LLM-based agent system offers an abstract separation of the agent’s components, it does not clearly delineate them from a software engineering perspective. Understanding the theoretical contributions of each component within a unified agent is valuable; however, from a practical development standpoint, it is essential to identify the location, functionality, and role of each internal component within the agent.

Upon a thorough analysis of LLM-powered agents from a software perspective, we recognize an LLM-based agent as a software system comprising various components including tools, LLMs, and the core-agent, our newly introduced term. While the core-agent is not a newly invented component, it serves as a label to denote an existing functional element that has been previously implicit or unnamed in past frameworks or architectures. Essentially, this label highlights its crucial role within these systems. This terminology helps clarify the structure of an LLM-based agent by identifying its essential parts. As depicted in Figure [2](https://arxiv.org/html/2409.11393v2#S3.F2 "Figure 2 ‣ 3.1 Core-Agent: Keystone component of an LLM-based agent ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), the core-agent interacts with the environment, collaborates with the language model to make decisions, and translates high-level goals into concrete actions optionally by leveraging the available tools.

Core-Agent:

In this architecture, the core-agent serves as the keystone component, acting as the crucial interface between itself and all the other elements of the agent. It facilitates communication and coordination among these various parts to ensure optimal functionality of the entire system. Functioning as the executor of the agent, the core-agent translates plans developed by the Large Language Model (LLM) into actionable steps potentially leveraging tools and engages with the environment to provide the agent with insights into the external world. These capabilities enable the core-agent to operate as a controller, ensuring precise and complex synchronization of actions that result in flawless interactions and effective information sharing.

The significance of the core-agent is underscored by its symbiotic relationship with the LLM, with each enhancing the other’s capabilities. LLMs unveils powerful language understanding, cognitive and reasoning abilities, alongside extensive knowledge. However, it lacks the perception and action components, which are provided by the core-agent, enabling direct environmental interaction. This collaboration expands the scope of problems the agent can address, effectively combining the strengths of both the LLM and the core-agent to tackle a broader range of challenges.

Furthermore, the presence of the core-agent as a controller mitigates the overall complexity of the agent, especially in a multi-LLM setup where it serves as the primary communication hub within the agent. Consequently, information flow between the LLMs is managed through the core-agent, streamlining interactions and ensuring efficient coordination. In terms of communication, the core-agent is capable of interacting with humans through a well-structured pipeline. However, there are instances where the core-agent may not engage in direct communication with humans. This variability is dependent on the specific type of core-agent, as elucidated in detail in Section [3.3](https://arxiv.org/html/2409.11393v2#S3.SS3 "3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").

LLM:

The LLM acts as a cerebral entity covering cognitive tasks such as natural language understanding and comprehensive text generation based on specific context. Indeed, LLMs can communicate with core-agents which in turn can interact with tools or data sources within the system. Considering that an agent can manifest as unimodal or multimodal [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)], the latter can be replicated with different task-specific LLMs, as illustrated in Figure [2](https://arxiv.org/html/2409.11393v2#S3.F2 "Figure 2 ‣ 3.1 Core-Agent: Keystone component of an LLM-based agent ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). This distinction fosters modularity within our LLM-based agent unified modeling framework, making them open for extension and addition of further domain-specific models without having to introduce changes to the overall system architecture, adhering more to the Open-Close principle (OCP) [[19](https://arxiv.org/html/2409.11393v2#bib.bib19), [20](https://arxiv.org/html/2409.11393v2#bib.bib20)].

Tools:

In LLM-based agents, tools are important assets. They can take various forms such as supplementary systems, software applications or even physical devices [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)] that extend and enhance the capabilities of an agent. They span across a spectrum of complexity, ranging from basic API integration to sophisticated auxiliary systems designed for specific tasks. In this context, tools can be considered external if they are independent systems which achieve a complete objective, or internal if they cooperate with the core-agent to achieve tasks in the scope of the agent goal.

In conclusion, our proposed terminology for an LLM-based agent, which includes a core-agent as its central coordinating component, serves to enhance clarity and consistency in describing these systems from a software perspective. The underscored significance of the core-agent highlights the importance of investigating its internal structure. Section [3.2](https://arxiv.org/html/2409.11393v2#S3.SS2 "3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") focuses on defining the various modules within the core-agent and how they collaborate to facilitate cognitive tasks, decision-making, and action execution leveraging available tools.

### 3.2 Modeling the Core-Agent Internal Structure

The human brain exhibits remarkable modularity, with distinct regions and functionalities working in coordination to facilitate cognitive processes, decision-making, and behavior. Inspired by this organized design, our proposed framework aims to emulate the brain’s modular structure thought the incorporation of five internal modules: the planning module, the memory module, the profile module, the action module and the security module (Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3 "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")). By integrating the latter concepts, the core-agent serves as the central coordinator, controlling the interactions and information flow between different components, as elaborated in the previous section.

![Refer to caption](img/d1191b6d14093900ec5d9a23ec3c35de.png)

Figure 3: Overview of the core-agent internal structure within an LLM-based agent

This modular framework effectively addresses the challenges of extendibility and maintainability. Specifically, it allows for independent development and integration of new modules without impacting the entire system. In fact, such modules can be replaced or upgraded separately, facilitating the addition of new capabilities and enabling the framework to adapt more easily to new requirements or technologies. Furthermore, the modular structure promotes code reusability, as individual modules can be shared across different agents or applications, thereby reducing duplication and enhancing consistency.

To provide a robust comparison and underscore the novelty of our framework, it is instructive to examine it alongside existing approaches. A recent survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] introduced a framework for LLM-based agents, comprising four key modules: profile, memory, planning, and action modules. For a comprehensive overview of this framework, we direct readers to Section [2](https://arxiv.org/html/2409.11393v2#S2 "2 Related Work ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). While the survey’s proposed framework treated the LLM-based agent as a whole system without distinguishing a core component from other entities, our adaptation, tailored specifically for the core-agent, necessitates modifying certain definitions and module structures to accommodate the separation between the LLM and the core-agent. Notably, we had to reposition the four key modules to operate under the core-agent within the framework alongside our newly introduced security-oriented module.

#### 3.2.1 Planning Module

In our proposed framework, the planning module is a pivotal element that enables the agent to break down the complex problems to generate effective plans, the same as in the original framework [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. However, in our solution the planning module becomes part of the core-agent and collaborates with the other sibling modules to empower the agent to achieve specific goals.

The planning module requires complicated understanding and reasoning [[21](https://arxiv.org/html/2409.11393v2#bib.bib21)] which could leverage the capabilities of an LLM. In fact, the LLM’s ability to comprehend nuanced instructions, interpret implicit information, and adapt to various problem domains renders it an invaluable asset in the planning process. Consequently, the planning module can formulate more comprehensive, context-aware, and adaptable plans, significantly enhancing the core-agent’s decision-making process. Furthermore, the planning module works in close collaboration with all other modules within the core-agent, including the memory module for Memory-augmented Planning [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]. This collaboration enhances planning capabilities by leveraging stored information such as commonsense knowledge, past experiences, and domain-specific knowledge.

This section will detail the functionalities and characteristics of this module from four critical aspects: process, strategies, techniques, and feedback sources (Figure [4](https://arxiv.org/html/2409.11393v2#S3.F4 "Figure 4 ‣ 3.2.1 Planning Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")). These aspects derive from the clear separation we have established between the core-agent and the LLM, and between our system and external systems (Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3 "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")).

![Refer to caption](img/0d40eaa42fca895b6df38c93caaf7ea1.png)

Figure 4: Planning module functional perspectives

Planning process:

While generating the procedures to undertake, the planning module follows an incremental approach. Therein, the steps dictate how tasks are decomposed, how the planning procedure unfolds, and how alternative solutions are generated and evaluated. Inspired by the human capacity to decompose complex tasks into simpler ones to achieve overarching goals, this process comprises two main steps [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)].

*   •

    Task decomposition:

    This initial phase involves breaking down a complex task into simpler subtasks, thereby establishing a structured hierarchy of intermediate goals. The decomposition of complex tasks can adopt two primary approaches: In the non-iterative decomposition approach, the complex task is broken down into simple subtasks all at once. The planning module defines all subtasks, creating a complete task hierarchy in a single step. This method provides a comprehensive overview of the entire task structure upfront. However, the iterative decomposition approach involves a step-by-step breakdown of the task. In fact, the planning module first defines the initial subtask and goal to reach, establish a proper plan and generates the procedures to undertake. After performing the procedures and completing the plan, the output is considered, and the next subtask is defined. This process is repeated, with each new subtask being planned and executed, contributing to defining the next subtask to achieve. The key advantage of this method is its flexibility and ability to adapt the plan based on the outcomes of each subtask. An exemplary application of the iterative approach is the Decomposition-Alignment-Reasoning Agent (DARA) framework [[23](https://arxiv.org/html/2409.11393v2#bib.bib23)]. DARA demonstrates how iterative decomposition can lead to more precise and context-aware planning, particularly for complex tasks with multiple interdependent subtasks.

*   •

    Plan generation:

    For each subtask derived from the decomposition step, a specific plan is established outlining the procedures to achieve the task’s goal while defining the different tools and parties involved. Depending on the chosen planning strategy, the module can generate either a single plan or multiple candidate-plans for each subtask which will be further detailed in the following section.

Planning strategies:

To guide the planification, organization and execution of complex tasks, the planning module must adhere to a specific strategy when elaborating the procedures during the plan generation step. Selecting a planning strategy can substantially influence the effectiveness, efficiency, and robustness of the resulting plans. Within the LLM-Agent-UMF, we define two primary strategies:

*   •

    Single-path strategy:

    This approach involves generating a singular path or sequence of procedures to achieve the goal, adhering to the plan step-by-step without exploring alternatives, thereby providing a straightforward, deterministic approach to planning. Chain of thought (CoT) [[24](https://arxiv.org/html/2409.11393v2#bib.bib24)] is an example that outlines such a strategy. Indeed, it uses sequential reasoning as it involves breaking down complex problems into multiple procedures, each built on the previous ones.

*   •

    Multi-path strategy:

    Consequently, in single path strategy, any error in one procedure can lead the subsequent procedures or the overall plan to be suboptimal or infeasible [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)], negatively impacting the entire strategy. A straightforward approach to mitigate such failures is the Multi-path strategy, which involves two major steps: The first phase involves leveraging the LLM to generate multiple plans for the complex task. Indeed, each intermediate step holds multiple subsequent paths [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. As for the second phase, it deals with the evaluation and the selection of the most suitable path.

    As a matter of fact, the Tree of Thoughts (ToT) and Graph of Thoughts (GoT) [[25](https://arxiv.org/html/2409.11393v2#bib.bib25)] are two frameworks that utilize this multi-path approach. They both operate by leveraging an LLM as a thought generator to produce intermediate procedures, that are structured either as a hierarchical tree in case of ToT or as a more complex graph in case of GoT. However, the complexity of managing these structures cannot be solely handled by the LLM. It requires the integration of a specialized software component responsible for orchestrating the process, interacting with the LLM, and organizing the thoughts into the desired structure, whether a tree or a graph. This essential software component is identified as the planning module within the core-agent. The planning module further evaluates different paths within the generated structure and selects an optimal plan [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)] based on its assessment.

Planning Techniques:

The planning module follows planning techniques as methodological approaches to form executable plans. These techniques are chosen based on criteria such as the complexity of the task, and the need for contextual comprehension. Our framework presents two primary techniques:

*   •

    Rule-based technique:

    Within our architectural framework, rule-based methodologies encompass what is commonly referred to in literature as symbolic planners [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)]. These techniques proved to be valuable especially in contexts characterized by complex constraints, such as mathematical problem-solving or the generation of plans within highly problematic situations.

    Symbolic planners, leveraging frameworks like PDDL (Planning Domain Definition Language), utilize formal reasoning to delineate optimal trajectories from initial states to targeted goal states [[26](https://arxiv.org/html/2409.11393v2#bib.bib26)]. These methodologies entail formalizing problem scenarios into structured formats, subsequently subjecting them to specialized planning algorithms.

*   •

    Language model powered technique:

    Language Model powered (LM-powered) methodologies leverage the vast knowledge and reasoning capabilities inherent in LMs to orchestrate planning strategies. Within our framework, this category also encompasses neural planners [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)], who are adept at addressing intricate and vague tasks necessitating nuanced comprehension and adaptive problem-solving abilities.

This categorization in our framework provides a clear distinction between approaches primarily driven by LMs and those relying on rule-based methods. It allows for a more streamlined understanding of planning techniques within the context of our core-agent architecture, while still acknowledging the valuable contributions of various planning approaches in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] and [[22](https://arxiv.org/html/2409.11393v2#bib.bib22)].

Feedback Sources:

Planning without feedback may pose several challenges as feedback plays a crucial role in optimizing the performance of the planning module within the core-agent. For instance, in iterative task decomposition, feedback has an influential impact on the next generated step and enhances the agent’s alignment with the user’s expectations. To effectively address these challenges, the planning module relies on diverse feedback sources. As outlined in Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3 "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), the core-agent engages with tools within its system boundaries, as well as entities outside its scope, such as external systems and humans. Consequently, interactions with these components can offer valuable feedback:

*   •

    Human Feedback:

    Human feedback may be an essential source of information for aligning the planning module with human values and preferences. This feedback results from direct interactions between the core-agent and humans. For example, when the core-agent proposes a plan, humans may provide feedback on its appropriateness, effectiveness, or ethical implications. This feedback could come in various forms, such as ratings, or comments.

*   •

    Tool Feedback:

    The core-agent often utilizes various tools, which can be internal components of the system or external applications. For instance, an internal calculator tool might raise an exception upon receiving an illegal operation like division by zero. Likewise, external tools provide feedback in the form of error messages, or performance indicators. Indeed, if the core-agent uses a weather prediction remote API, the accuracy of the prediction serves as feedback. This tool-provided feedback helps the core-agent refine its tool selection and usage strategies.

*   •

    Sibling Core-Agent Feedback:

    In multi-core agent systems, as will be discussed in Section [3.4](https://arxiv.org/html/2409.11393v2#S3.SS4 "3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), feedback from sibling core-agents becomes a valuable source of information. This type of feedback results from interactions and information exchanges between different core-agents within the same system. This intra-agent feedback can include shared observations, alternative perspectives on a problem, or evaluations of proposed plans. Such feedback promotes collaborative problem-solving and allows for cross-validation of plans. It enhances the overall robustness of multi-core agent systems by facilitating collective intelligence.

To conclude, the planning module is a critical component of our framework, employing a structured planning process that consists of task decomposition and plan generation steps. This process is distinct from, yet closely intertwined with, the planning strategies—single-path and multi-path—which guide the formulation of plans. These strategies are crucial as they shape the core-agent’s approach to problem-solving, influencing both the quality and efficiency of the solutions. By breaking down tasks and generating multiple plans, the core-agent can identify optimal solutions more quickly and effectively.

The planning module’s effectiveness is further enhanced by incorporating feedback from various sources, including humans, tools, and sibling core-agents. This feedback loop allows for continuous refinement and adaptation of plans, ensuring that the agent remains responsive and efficient. Moreover, incorporating planning strategies and feedback mechanisms enhances adaptability, enabling the core-agent to address a wide range of scenarios, from simple tasks to complex challenges. Such strategic diversity equips the system with greater intelligence and versatility.

It is important to note that the planning module does not operate in isolation. It collaborates closely with other modules, particularly the memory module, which will be discussed in the next section. This collaboration, especially in the context of memory-augmented planning, further enhances the core-agent’s capabilities by leveraging stored information and past experiences.

#### 3.2.2 Memory Module

The memory module is responsible for the storage and retrieval of information pertinent to the core-agent’s activities, thereby enhancing the core-agent’s decision-making efficiency and task execution capabilities. In [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)] and [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], the memory module in an LLM-based agent was approached from an abstract functional perspective, neglecting its analysis from a software architectural viewpoint. This led to some overlap between the memory module and the other modules defined in the framework suggested by [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. Consequently, we propose a more comprehensive and well-defined presentation of this module based on three perspectives: Memory Scope, Memory Location, and Memory Format (Figure [5](https://arxiv.org/html/2409.11393v2#S3.F5 "Figure 5 ‣ 3.2.2 Memory Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")).

![Refer to caption](img/05caf8c00573b756f50635d3ac81a571.png)

Figure 5: Memory module functional perspectives

The framework presented in the survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] classified the memory structure into Unified Memory, intended to emulate human short-term memory via in-context learning, and Hybrid Memory, which represents both short-term and long-term memory functionalities. While these names aimed to differentiate types of agent systems, they deviate unnecessarily from the well-established terminology in the field. A more conventional and widely recognized categorization is introduced through the first perspective, Memory Scope, which includes short-term and long-term memory, aligning more with the human memory types as detailed in [[28](https://arxiv.org/html/2409.11393v2#bib.bib28)].

As opposite to how it was defined in framework [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], our short-term memory definition diverges to focus more on the impact of the core-agent rather than on the LLM. From a content perspective, various sources of information contribute to short-term memory. As pointed out in paper [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)], data can be derived from one trial of a given task or from previous trials of the same task, which is referred to as short-term memory. This data has a narrow scope that focuses on a specific task and primarily communicated to the LLM for the purpose of in-context learning and enhancing the LLM capability with more information related to the task at hand. Conversely, long-term memory refers to the ability to store and recall information over extended periods, beyond the scope of a specific task. This type of memory enables the core-agent to maintain coherence and context over prolonged interactions, learning and adapting from past experiences. Namely, MemoryBank [[29](https://arxiv.org/html/2409.11393v2#bib.bib29)] stores all interactions with user in a large symbolic memory and processes experiences into high-level summaries to reflect upon future similar tasks.

However, there is always a limit to the amount of memory a core-agent can retain. Therefore, techniques such as forgetting mechanisms must be implemented to decide which memories to discard and which to retain [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)]. As mentioned earlier, survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] treats memory from an abstract functional aspect, which is inconsistent with our framework that emphasizes a clear delineation of distinct modules. Our approach aims to provide a more precise and structured understanding of core-agent components from a software perspective. For instance, according to survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], the memory module includes a "reflection" operation with a cognitive aspect that we believe belongs to the planning module. The planning module focuses on achieving the agent’s goals and is responsible for decision-making and synchronizing among all other modules. Thus, it makes it more suitable for handling reflections and memory optimization by collaborating with the memory module to access stored data. Likewise, extracting useful information from memory and reflecting upon it to produce a solid plan belongs to planning module responsibilities. For example, Voyager [[30](https://arxiv.org/html/2409.11393v2#bib.bib30)] considers environmental feedback, handled as short-term information, and leverages the LLM capability to adjust its plan and make more efficient and rational decisions in the scope of planning module.

Furthermore, the writing operation is an essential aspect of a memory, distinguishing it from data repositories. Therefore, we exclude knowledge repositories from the memory category. As a matter of fact, the retrieval of knowledge from databanks, in a read-only way, falls under the responsibilities of the action module, which will be discussed in Section [3.2.4](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS4 "3.2.4 Action Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). As we have set writing and reading as essential operations for the memory module and excluded reflection from the possible set of operations, we decided to omit the Memory Operation perspective discussed in survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] from our framework.

Beyond focusing on the scope of the data, inside trial and across trials, as stated in [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)], we emphasize the location of the memory because it is more relevant from a software architecture point of view. Hence, we introduce the second perspective, Memory Location, which encompass two categories: Embedded Memory, internal to the core-agent boundaries, and Memory Extension, outside the core-agent boundaries, yet still within the boundaries of the agent system as depicted in Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3 "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").

Finally, the last perspective, Memory Format, already present in both works [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] and [[27](https://arxiv.org/html/2409.11393v2#bib.bib27)], focuses on the shape and the representation of the memory that could have diverse manifestations: natural language, embeddings, SQL databases, or structured lists. In fact, memory information such as agent behaviors and observations could be directly described using raw natural language, providing flexibility and retaining rich semantic information. Another solution would be to encode memory information into embedding vectors to enhance retrieval and reading efficiency. In addition, as exemplified by ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)], databases could be used as memory holders to store memory information in a structured representation, allowing agents to manipulate memories efficiently and comprehensively using SQL queries. And finally, structured lists, as used in GITM [[32](https://arxiv.org/html/2409.11393v2#bib.bib32)] let core-agents save the sequential actions of subgoals in a structured way and organize memory information into hierarchical lists to convey semantic information concisely.

As an observation, the most common format is textual, nevertheless [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] notes that these formats are not mutually exclusive, so one core-agent can handle multiple formats, such as GITM’s key-value list structure that combines embedding vectors and raw natural language, to harness the respective benefits of each approach.

#### 3.2.3 Profile Module

Similarly to the approach presented by the paper [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], our framework defines the function of the profile module as to establish the role of the LLM and adopts more diverse methods. This template explicitly separates the roles of both the core-agent and the LLM. Indeed, the profile module facilitates the dynamic adaptation of various profiles tailored to specific use cases and strategies employed by the planning module. The Profile module features four methods for defining profiles: the Handcrafted In-Context Learning Method, the LLM-generation method, the Dataset Alignment Method, and the newly introduced Fine-tuned Pluggable Modules method (Figure [6](https://arxiv.org/html/2409.11393v2#S3.F6 "Figure 6 ‣ 3.2.3 Profile Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")).

![Refer to caption](img/0d972ec7418aad995702fd5cadd855fa.png)

Figure 6: Profile module techniques

The Handcrafted In-Context Learning Method, previously referred to as the Handcrafting Method in the survey [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], involves deputing the core-agent to set the LLMs profiles through in-context learning techniques that employ pre-configured prompts. This method allows for fine-grained control over the LLM’s personality and behavior. While it is a straightforward method to implement, it necessitates the use of LLMs that are well-suited for in-context learning and often possess a cumbersome size.

The LLM-Generation Method facilitates the automatic creation of profiles for agents using LLMs. This method begins by specifying the profile’s characteristics which include detailed information such as age, gender, and interests. Optionally, several seed agent profiles can be selected to serve as few-shot examples, as outlined in the paper [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. Once these seed profiles are established, LLMs are employed to generate additional profiles by referring to the initial seed examples. For instance, RecAgent [[33](https://arxiv.org/html/2409.11393v2#bib.bib33)] suggests designing appropriate prompts that encourage a GPT model to generate comprehensive profile descriptions by referring to a table of attributes corresponding to various samples and generate additional profiles. The LLM-Generation Method, while offering significant time-saving advantages, is constrained by its reliance on LLMs. This dependence can lead to potential biases [[34](https://arxiv.org/html/2409.11393v2#bib.bib34)] or inaccuracies in generated agent profiles.

Additionally, the Dataset Alignment Method derives profiles from real-world datasets [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], which consists of data about actual individuals. This approach starts by organizing the information in these datasets into natural language prompts that describe the characteristics of the role of the LLM. These structured data are then used to create the profiles. In the study conducted by [[35](https://arxiv.org/html/2409.11393v2#bib.bib35)], researchers utilized GPT-3 alongside real world demographic data from ANES to assign roles based on characteristics such as state of residence. Subsequently, they evaluated whether GPT-3 could mimic real human behavior reliably. The Dataset Alignment Method ensures that the LLM profile accurately reflects real-world attributes and behaviors, thereby making it meaningful and realistic. However, the effectiveness of this method relies heavily on the accuracy and representativeness of the underlying real-world datasets.

Finally, we introduced the Fine-tuned Pluggable Modules Method, a pioneering solution to set LLM’s profile leveraging several state-of-the-art techniques, aiming to provide an efficient customization and adaptation of the LLM’s profile. This approach defines the LLM profile by injecting a pluggable module fine-tuned to influence the language model behavior [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)]. Indeed, that module is a set of additional tunable parameters that must be previously trained using Parameter-Efficient Fine-Tuning (PEFT) techniques, such as Sequential adapter (AdapterS), Prompt-tuning, or LoRA [[37](https://arxiv.org/html/2409.11393v2#bib.bib37)].

This process ensures precise and effective customization to achieve the desired profile by eliminating the need for in-context learning, which traditionally involves injecting extensive prompt information into each query. By omitting this step, the required context size is significantly reduced, as the adapters directly encode the desired behaviors and profiles into the model parameters. This reduction in context size and memory footprint allows the LLM to operate more efficiently, using less memory during inference without compromising performance and flexibility.

#### 3.2.4 Action Module

While we have enriched the approaches applied in this module, we define it within the LLM-Agent-UMF almost similarly to the methodology described in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. The action module is responsible for converting high-level instructions from the planning module into low-level actions, leveraging tools available in its environment. In our framework, the action module interacts with the security module to ensure that any executed action aligns with predefined criteria and prevent information leakage as will be explained further in Section [3.2.5](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS5 "3.2.5 Security Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). In essence, our analysis considers four fundamental aspects: Goal, Trigger, Action Space, and Impact (Figure [7](https://arxiv.org/html/2409.11393v2#S3.F7 "Figure 7 ‣ 3.2.4 Action Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")).

![Refer to caption](img/25448fc27f340ed4dfdb1640d11588db.png)

Figure 7: Action module functional perspectives

The first perspective, the Action Goal, represents what the core-agent intends to accomplish by performing actions based on various objectives such as Task Completion, Communication, or Environment Exploration which aligns with the framework proposed in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. The second aspect, Action Trigger, focuses on how the actions are produced and the catalysts behind them. This perspective is derived from the Action Production perspective originally proposed in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], but we emphasize more on the causality that drives action production. In this regard, the "Action via Memory Recollection" approach mentioned in [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)] is considered to be better handled by the planning module as it focuses on leveraging past experiences for making appropriate decisions. In fact, the core-agent can produce actions as a result of two primary triggers: Plan Following and API Call Request, newly introduced in our framework. In the case of Plan Following trigger, the core-agent can execute actions by adhering to pre-generated plans elaborated by the planning module. As a matter of fact, in GITM [[32](https://arxiv.org/html/2409.11393v2#bib.bib32)], the agent breaks down the task into subtasks and procedures and executes the appropriate actions in the Minecraft world via the LLM interface. By exploring other state-of-the-art techniques like TALM [[6](https://arxiv.org/html/2409.11393v2#bib.bib6)] and ToolFormer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)] that exemplify LLMs’ capacity to improve performance across diverse tasks through incorporating external tools, we introduce a new trigger named API Call Request. This approach enables the core-agent to execute actions in response to API call requests initiated by the LLM, facilitating smooth integration and effective utilization of external resources.

The third perspective, Action Space, defines the set of possible actions that can be performed by the core-agent in response to its objectives and environmental factors. This concept is distinct from operations involving internal state changes and memory management, which are handled separately by the Memory Module. In contrast to [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)], our framework emphasizes this separation to avoid potential overlap between action and memory modules functions, thus ensuring a clear division of responsibilities for efficient execution. The action module can leverage tools ranging from basic software components like calculators to more advanced systems accessed through API calls, as exemplified in the HuggingGPT work that leverages the HuggingFace API to accomplish complex user tasks [[39](https://arxiv.org/html/2409.11393v2#bib.bib39)]. Alternatively, the core-agent may expand its operational scope and knowledge by communicating with read-only data sources such as knowledge repositories. This process is often referred to as Retrieval-Augmented Generation (RAG) [[40](https://arxiv.org/html/2409.11393v2#bib.bib40)]. This is particularly crucial for AI systems deployed in critical industries like healthcare where explainability in decision-making processes becomes paramount [[41](https://arxiv.org/html/2409.11393v2#bib.bib41), [42](https://arxiv.org/html/2409.11393v2#bib.bib42)]. By interacting with these external resources, transparency, provenance and traceability are ensured to guarantee the reliability of the agent’s outputs [[43](https://arxiv.org/html/2409.11393v2#bib.bib43)]. It is useful to note here that data repositories can have the same formats discussed in the memory module (Section [3.2.2](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS2 "3.2.2 Memory Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")). For instance, LLamaIndex [[44](https://arxiv.org/html/2409.11393v2#bib.bib44)] stores data as vector embeddings at the indexing stage to leverage semantic search, where the similarity between embeddings is used to rank documents by their relevance to a query. In contrast, ReAct [[45](https://arxiv.org/html/2409.11393v2#bib.bib45)] uses a textual data repository, like Wikipedia, to mitigate error propagation in chain-of-thought reasoning.

Finally, Action Impact refers to the consequences resulting from an action. Numerous impacts can be cited, such as changing the environment by moving to different locations, gathering resources, or synthesizing new chemical compounds [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)]. Actions can also result in the alteration of the internal state of the agent, especially when acquiring new knowledge and collaborating with the memory module. Additionally, actions can trigger new ones, creating a chain of actions during task execution.

#### 3.2.5 Security Module

As LLMs continue to advance and become increasingly prevalent in various applications, it becomes crucial to address potential risks, ethical considerations and unintended consequences associated with their deployment [[46](https://arxiv.org/html/2409.11393v2#bib.bib46)]. These concerns revolve around issues such as unauthorized or unethical use, data biases, privacy breaches, and the spread of misinformation [[4](https://arxiv.org/html/2409.11393v2#bib.bib4)]. This has led to the introduction of guardrails recently in LLMs field as algorithms to identify and prevent the misuse of LLMs [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]. To bolster our framework for trustworthy AI systems, we propose a fifth module: the Security Module. This module aims to provide a more capable and responsible core-agent. Its role is monitoring the action module specifically in production environments to ensure the safety and responsible use of LLMs.

The Security Module operates within the parameters of the Confidentiality, Integrity, Availability (CIA) triad [[47](https://arxiv.org/html/2409.11393v2#bib.bib47)], a crucial model which encompasses three pivotal principles in the security field. Confidentiality is centered around protecting sensitive information from unauthorized access or disclosure. Within LLMs-based agents, this principle is critical in safeguarding user data and ensuring the non-divulgence of sensitive information. Integrity is concerned with the accuracy, consistency, and trustworthiness of data throughout its lifecycle. For agents, this principle involves maintaining the reliability of the model’s outputs and preventing any unauthorized modifications to the system or its data. Lastly, availability focuses on ensuring that information and resources are accessible to authorized users whenever required. In the context of LLM applications, this entails maintaining system uptime and providing safe responses to user inquiries. By adhering to these principles, the Security Module aims to establish a robust and trustworthy environment for the operation of the agent, effectively addressing critical concerns related to their deployment and usage.

![Refer to caption](img/ebe191b5d2129ea63a94b06b2ac46dd0.png)

Figure 8: Security module functional perspectives

While conducting a thorough research around the Security Module, our approach will involve exploring multiple facets: Identifying and securing critical assets and data within the core-agent modeling framework, implementing strategies and mechanisms to protect these assets from potential threats, ensuring the core-agent’s ability to effectively respond to and mitigate any security incidents, and maintaining the privacy of user data while adhering to relevant data protection regulations (Figure [8](https://arxiv.org/html/2409.11393v2#S3.F8 "Figure 8 ‣ 3.2.5 Security Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")).

Security measures:

Regardless of the guardrail type deployed, the Security Module encompasses three fundamental axes: Prompt safeguarding, response safeguarding, and data privacy safeguarding.

*   •

    Prompt Safeguarding:

    Prompt safeguarding necessitates employing measures to detect and mitigate unauthorized access to Large Language Models through prompt injection attacks [[48](https://arxiv.org/html/2409.11393v2#bib.bib48)]. Techniques for enhancing the security of LLM-based agents can be integrated directly into the LLM itself, with Adversarial Training (AT) being a prominent example [[49](https://arxiv.org/html/2409.11393v2#bib.bib49)] AT enhances an LLM’s defense mechanisms by fine-tuning it with augmented training data containing adversarial examples, thereby increasing the model’s ability to safeguard against malicious prompts and improving its robustness. However, AT faces significant limitations, such as the challenges in efficiently selecting adversarial examples and the model’s exposure to adversarial perturbations such as HOUYI [[50](https://arxiv.org/html/2409.11393v2#bib.bib50)], a black-box prompt injection attack. Moreover, such training-based security techniques may impact the generative performance of the LLM which necessitates additional evaluation steps.

    Other techniques address these limitations by decoupling the security measures from the LLM and delegating them to a distinct entity that we identify as a core-agent supplemented with a security module as illustrated in Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3 "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). This decoupling is essential for improved protection and enables the implementation of more advanced security protocols on the LLM input, which can evolve independently of the LLM’s training process. An example of this approach is Nvidia NeMo [[51](https://arxiv.org/html/2409.11393v2#bib.bib51)], which functions as an intermediary layer between users and LLMs, employing advanced techniques such as vector databases and comparison with stored canonical forms to filter and process user inputs before they reach the model, thereby providing robust prompt safeguarding without directly modifying the underlying LLM.

    These approaches are essential to address scalability challenges, enable proactive defense, and facilitate continuous learning in LLM security, ensuring that protection mechanisms can adapt to new threats and maintain the integrity of LLM interactions.

*   •

    Response Safeguarding:

    The recent survey [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)] has demonstrated that despite the implementation of prompt safeguarding techniques, the overall resilience of Large Language Models (LLMs) against advanced attacks, known as jailbreaks, may not experience significant improvement. These jailbreaks are designed to exploit biases or vulnerabilities within language models by manipulating their responses. Notable examples include white-box attacks AutoDAN-Zhu [[53](https://arxiv.org/html/2409.11393v2#bib.bib53)], which generate stealthy prompts to avoid triggering the model protective mechanisms. Additionally, black-box attacks leverage manually crafted prompts to deceive the LLM [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]. The effectiveness of these jailbreak techniques is further illustrated in [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)], where researchers successfully achieved a jailbreak attack on ChatGPT 3 by framing potentially harmful query, "how to hotwire a car" as a hypothetical scenario. The existence of these jailbreak methods highlights the urgent necessity for continuous and rigorous monitoring of LLM outputs to detect and mitigate potential breaches. It is useful to note that the aforementioned jailbreak attack was addressed and resolved in later versions of ChatGPT, 3.5 and 4, as confirmed by [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)].

    Guarding the agent outputs is considered a supplementary measure alongside prompt safeguarding, with the objective of guaranteeing the safety, integrity and authenticity of the text generated by LLMs. This includes detecting and redacting harmful content while maintaining coherence and relevance. Two notable examples of such techniques are Guardrails AI [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)] and LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)], discussed further in Section [4](https://arxiv.org/html/2409.11393v2#S4 "4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").

*   •

    Data Privacy Safeguarding:

    Lastly, safeguarding data privacy is pivotal, especially when handling sensitive or personal information. It is essential to ensure that LLMs are protected against sensitive data breaches [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]. Existing research has predominantly focused on securing training data through traditional techniques such as Differential Privacy [[55](https://arxiv.org/html/2409.11393v2#bib.bib55)] and watermarking [[56](https://arxiv.org/html/2409.11393v2#bib.bib56)]. As a matter of fact, Differential Privacy tuned models add noise to the data, making it difficult to identify individual data points. Similarly, watermarking techniques embed identifiable markers into LLM outputs, allowing for the tracing of data origin and preventing unauthorized use.

    However, under our proposed framework, the primary objective shifts from solely protecting the privacy of training data to ensuring that the LLM does not divulge sensitive information to external tools. This approach aims to maintain the privacy and security of data while interacting with other systems. Indeed, as previously mentioned, the core-agent can leverage external tools, APIs, and knowledge repositories to augment the capabilities of the LLM. Being part of the agent as a whole system, internal tools are part of the privacy circle, thereby, there is no need to apply security measures on communication procedures. However, the use of external resources introduces potential risks that require specific attention. These risks include a lack of robust data privacy measures, potentially leading to data leaks or unauthorized access. For example, when the core-agent utilizes third-party services to access additional information or perform specific tasks, the data transmitted may contain sensitive details that specific systems are not authorized to access. Additionally, such sensitive information could be intercepted or mishandled if proper secure channels are not leveraged.

    To mitigate these risks, the security module within the core-agent must implement a range of powerful techniques such as access control mechanisms, and data encryption. Therefore, the framework ensures that interactions with external resources maintain the highest standards of security and data privacy.

Thus, the security module operates along three fundamental axes: Prompt Safeguarding, Response Safeguarding, and Data Privacy Safeguarding. Their integration forms a robust defense mechanism capable of mitigating diverse threats, ranging from prompt injection attacks to potential data breaches. Through this approach, the security module shapes the behavior of the core-agent, prioritizing security in every aspect, from data retrieval to external communication.

Guardrail types:

To implement these security axes effectively, various guardrail methodologies have been developed. These guardrails act as the operational layer of the security module, translating high-level security objectives into actionable safeguards. The paper [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)] delves into diverse guardrail methodologies and solutions offered by LLM service providers and the open-source community. Through meticulous analysis of these methodologies, two primary types emerge, rule-based guardrails and LLM-based guardrails.

*   •

    Rule-based guardrails:

    These guardrails operate based on a predetermined set of rules and regulations aimed at screening and preventing potentially detrimental or undesirable inputs/outputs from LLMs. To elucidate the process, users define the content necessitating protection. Subsequently, the guardrails assess the inputs/outputs against these predefined regulations, and custom rules [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)], to ascertain compliance. In instances where the content is deemed unsafe, it may be obstructed, or a cautionary alert may be issued. For instance, the Adversarial Robustness Toolbox (ART) [[57](https://arxiv.org/html/2409.11393v2#bib.bib57)] is specifically designed to bolster the security and robustness of models against adversarial attacks. It offers tools and methods to defend against and adapt to malicious inputs, thereby safeguarding AI applications from potential vulnerabilities.

*   •

    LLM-powered guardrails:

    While rule-based guardrails provide a solid foundation for safeguarding LLM operations, they face limitations in adaptability and maintenance. The need for manual, continuous improvement and intervention to upgrade rules can be time-consuming and may struggle to keep pace with rapidly evolving threats and diverse use cases. LLM-powered guardrails offer a compelling solution to these challenges. A prevalent design approach for constructing these guardrails involves the usage of neural-symbolic agents [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]. These agents, functioning akin to core-agents from a security standpoint, undertake the critical task of analyzing input and output, ensuring their adherence to a predefined set of requirements. By leveraging the inherent learning and adaptability capabilities of language models, these guardrails can evolve and respond to new situations in a faster and more automated manner. They can understand context, nuance, and intent more effectively than rigid rule sets, allowing more sophisticated and flexible protection mechanisms.

    Moreover, neural-symbolic agents resolve conflicts that may arise between requirements, leverage historical data to reason symbolically and possess the capability to collaborate with other AI systems [[52](https://arxiv.org/html/2409.11393v2#bib.bib52)]. While LLM-based solutions may introduce computational overhead, this potential drawback can be mitigated by adopting lightweight models specifically designed for guardrail tasks. These optimized models can provide the benefits of LLM-powered security with reduced resource demands, striking a balance between robust protection and operational efficiency.

    In fact, within the general scope of the agent, the core-agent can communicate with an auxiliary LLM, which is finetuned on specialized dataset to set the acceptability guidelines of the response generated by the main LLM. The core-agent allows for customization of guardrail rules, including monitoring and enforcement protocols [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]. These customized rules are then passed to the auxiliary LLM to classify the nature of the input. Such classification helps the core-agent to decide whether the requirements are fulfilled or not. A leading example of this approach is LLaMA Guard [[13](https://arxiv.org/html/2409.11393v2#bib.bib13)]. Introduced by Meta (Facebook), it was designed specifically to guarantee the security and reasonable utilization of LLaMA models and used to analyze both input and output data. It employs predictive classification techniques to assess and improve security across user-specified categories. This implementation underscores the critical role of LLM-based guardrails in fortifying the integrity and reliability of AI systems. By leveraging the LLM’s capabilities to understand and enforce complex security rules, Llama Guard provides a flexible and powerful mechanism for ensuring safe and responsible AI operation, particularly in next-generation LLM models like Llama 3.1 [[58](https://arxiv.org/html/2409.11393v2#bib.bib58)].

To wrap up the LLM-based agent unified modeling framework, our proposed solution emulates the modular architecture of the human brain by introducing the core-agent component, which encompasses five internal modules: planning, memory, profile, action, and security. This modular design introduces enhancements on the security level and addresses challenges related to extendibility and maintainability, effectively separating the core-agent functionalities from the LLM. Consequently, this structured aspect highlights the role of each module and the implication of their integration, especially the planning module as it gives core-agents adopting it an authoritative aspect. The latter perspective leads to the classification of core-agents that will be discussed in Section [3.3](https://arxiv.org/html/2409.11393v2#S3.SS3 "3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") .

### 3.3 Active/Passive Core-Agent Classification

As discussed in the preceding sections, the core-agent represents a distinct entity within the LLM-Agent-UMF. While the LLM excels in cognitive tasks such as understanding, reasoning, and generating responses, it lacks the capability to directly interact with the environment or external tools. This is where the core-agent plays a crucial role. It bridges the gap between the LLM’s cognitive abilities and the need to engage with external sources, enabling seamless integration with various tools and systems. The core-agent is thus characterized by its action capabilities and its ability to respond to user requests through interaction with these diverse tools. In fact, ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] is a general tool-use framework that enhances LLMs capabilities enabling agents to use external tools and APIs. It uses a neural API retriever to recommend appropriate APIs for each instruction. Then they employ a depth-first search-based decision tree algorithm to evaluate multiple reasoning traces and expand the search space. Consequently, it enhances the planning ability of the retriever and empowers the finetuned LLM, ToolLlaMA, to generate adequate instructions. The retriever here, in association with the search-based decision tree algorithm, satisfies our definition of a core-agent. In this case where the LLM-based agent conducts cognitive tasks, memory and planning modules are essential in the core-agent to ensure reasoning capabilities because they enable the agent to retain and recall past experiences, plan and synchronize actions, reason and make decisions [[27](https://arxiv.org/html/2409.11393v2#bib.bib27), [22](https://arxiv.org/html/2409.11393v2#bib.bib22)].

In other cases, such as Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)], we identify entities that fit our definition of a core-agent but lack both planning and memory modules. Indeed, Toolformer fine-tunes its LLM on function calling, enabling it to generate API requests within natural language as needed. Consequently, the LLM determines when to make an API call, which API to use, and how to integrate the results, while the actual execution of the API request is delegated to an entity that we identify as a core-agent. In this case, the planning module of the core-agent is obsolete because planning is handled solely by the LLM. Yet, its action module is present because it is still responsible for executing API calls systematically. For example, if the model suggests using a calculator API, the core-agent retrieves the arguments for the mathematical operation, performs the calculation, and returns the computed result to the LLM.

The inspection of the state-of-the-art led to the conclusion that the action module is always indispensable in a core-agent as it is responsible for producing the executive steps to achieve their goals. However, the architectural disparities in core-agents and the absence of some modules in some proposed agent systems highlight the need to introduce a new taxonomy classifying core-agents into two distinct categories: Active core-agents (Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3 "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")) and passive core-agents (Figure [9](https://arxiv.org/html/2409.11393v2#S3.F9 "Figure 9 ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents")). The following sections analyze and pinpoint the main differences and similarities between active and passive core-agents in their structural alignment with our framework.

![Refer to caption](img/9e860edaddf009a5722d9c0cbd45c0b7.png)

Figure 9: LLM-based agent architecture including a passive core-agent

#### 3.3.1 Active Core-Agents

Active core-agents encompass all five modules described in Section [3.2](https://arxiv.org/html/2409.11393v2#S3.SS2 "3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") and illustrated in Figure [3](https://arxiv.org/html/2409.11393v2#S3.F3 "Figure 3 ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), but what differentiates an active from a passive core-agent is its managerial aspect. An active core-agent is characterized by its leading position in the agent as the orchestrator of other components, so naturally, it requires a planning module to divide tasks into subtasks and collaborates with the memory module to provide the necessary context, analyze information, and make decisions. Consequently, we consider an active core-agent to be stateful, meaning it can maintains information about its past interactions and states over time. This is facilitated by an adaptive memory that captures and stores various aspects of the agent’s lifecycle, allowing it to use this historical data to inform future actions and decisions. The profile module role is emphasized in the active core-agent category, because it guides the LLM’s behavior in a certain direction. Furthermore, the security module plays a prominent role in safeguarding the communication between the LLM and the human, ensuring a reliable exchange; Acting as an intermediary, the core-agent safeguards the LLM from threats such as jailbreak attempts and protects user data privacy by implementing safety measures as outlined in Table [1](https://arxiv.org/html/2409.11393v2#S3.T1 "Table 1 ‣ 3.3.1 Active Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").

Table 1: Active and passive core-agents internal structure

| Core-Agent Structure |  |  |
| Modules | Sub-modules / Methods | Active Core-Agent | Passive Core-Agent |
|  |  | Rule-based |  |  |
|  | Planning techniques | LM-powered |  |  |
|  |  | Task decomposition |  |  |
|  | Planning process | Plan generation |  |  |
|  |  | Single-path strategy |  |  |
|  | Planning strategies | Multi-path strategy |  |  |
|  |  | Human feedback |  |  |
|  |  | Tools feedback |  |  |
| Planning | Feedback sources | Sibling core-agent feedback | X |  |
|  |  | Embedded memory |  |  |
|  | Memory location | Memory extension |  |  |
|  |  | Short-term |  |  |
|  | Memory scope | Long-term |  |  |
|  |  | Natural language |  |  |
|  |  | SQL database |  |  |
|  |  | Embeddings |  |  |
| Memory | Memory format | Structured list | X |  |
|  | Handcrafted in-context learning method | X | [*] |
|  | Fine-tuned pluggable modules method | X | [*] |
|  | LLM-generation method | X |  |
| Profile | Dataset alignment method | X |  |
|  |  | Task completion | X | X |
|  |  | Communication | X [**] | X [***] |
|  | Action goals | Environment Exploration | X |  |
|  |  | Plan Following | X |  |
|  | Action trigger | API Call Request | X | X |
|  |  | Tools (APIs, External systems, etc) | X | X |
|  | Action space | Data repositories | X | X |
|  |  | Change environment | X | X |
|  |  | Alter internal state | X |  |
| Action | Action impact | Trigger new actions | X | X |
|  |  | Prompt Safeguarding | X |  |
|  |  | Response Safeguarding | X |  |
|  | Safeguarding measures | Data Privacy Safeguarding | X | X |
|  |  | Rule-based guardrail | X | X |
| Security | Guardrail types | LLM-powered guardrail | X | X |

^([∗]) Passive core-agents do not have a profile module. Depending on the architecture of the whole agent, the LLM’ s profile will be set either statically or dynamically, but not by the passive core-agent as it has no control over it.

^([∗∗]) Communication can be initiated by either Humans or the active core-agent.

^([∗∗∗]) Communication can only be initiated by the passive core-agent.

Throughout our research on the state of the art, we observed that LLM-based agents are recently built upon active core-agents performing tasks from planning to execution [[5](https://arxiv.org/html/2409.11393v2#bib.bib5)]. As highlighted in [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)], active core-agents are more effective because they incorporate planning and memory modules, which enable them to reason, plan, and execute tasks efficiently. This structure allows the agent to adapt to changing situations and make informed decisions, making the system more robust and capable.

However, relying solely on active core-agents would increase the complexity of the agent, which can lead to scalability issues and negatively impacts the maintainability of the agent as it will hinder and complicate future improvement efforts. As noted in [[18](https://arxiv.org/html/2409.11393v2#bib.bib18)], "the complexity of the agent system grows exponentially with the number of tasks it needs to perform". Therefore, rather than centralizing responsibilities on one entity, it would be more beneficial and adhering to the Single Responsibility Principle, if we leverage other core-agents to granulate task execution and reduce the complexity of the agent system. This approach is supported by the concept of "separation of concerns" in software engineering, which emphasizes the importance of dividing responsibilities among multiple components to improve system modularity and maintainability. By distributing tasks among multiple core-agents, we can reduce the cognitive load on individual core-agents, improve system efficiency, and enhance overall performance.

#### 3.3.2 Passive Core-Agents

Passive core-agents are employed when LLMs cover all cognitive tasks of the agent such as planning and taking decisions, while passive core-agent’s role is mainly to execute specific procedures. As a direct consequence, the planning module becomes unnecessary and likewise the memory needed in reasoning. Unlike active core-agents, passive core-agents are stateless, and the short-term memory is handled by the LLM, covering only the current task’s state. In LLM-based agents, passive core-agents, which always follow instructions from domain-specific LLMs, lack the ability to control the profile of the LLM thus do not possess a Profile module. The LLM profile may be statically defined during the system setup or dynamically defined by another entity, which will be discussed in the next section.

The most essential module in a passive core-agent is the action module. Our framework posits that the function of a passive core-agent is limited to specific task execution. Actions are often triggered by API call requests, which are not decision-based nor self-generated by the passive core-agent but provided by another entity (e.g., LLM or an active core-agent) as shown in Figure [9](https://arxiv.org/html/2409.11393v2#S3.F9 "Figure 9 ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). The actions do not alter the internal state of the agent or change a predetermined plan. This again points to the absence of a planning module in passive core-agents. Furthermore, we introduce another distinction between passive and active core-agents: the communication between humans and core-agents is interactive and bidirectional in both categories, aiming to gather information and/or feedback. However, as pointed out by Figure [9](https://arxiv.org/html/2409.11393v2#S3.F9 "Figure 9 ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), the communication between passive core-agents and humans can only be initiated from the passive core-agent part, unlike active core-agents, where communication can be initiated by either party.

Despite not being directly responsible for handling prompts from humans and providing generated text responses, passive core-agents should still possess a robust security module. This component is crucial in ensuring privacy during their interactions with other humans or third-party systems by preventing leakage of sensitive data while minimizing potential threats and breaches. Consequently, this bolsters the overall trustworthiness and reliability of LLM-based agent applications. It is also important to note that in this setup, as illustrated in Figure [9](https://arxiv.org/html/2409.11393v2#S3.F9 "Figure 9 ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), the LLM ensures by itself the safety of the prompts by implementing one of the mechanisms previously discussed in Section [3.2.5](https://arxiv.org/html/2409.11393v2#S3.SS2.SSS5 "3.2.5 Security Module ‣ 3.2 Modeling the Core-Agent Internal Structure ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") such as adversarial training.

Table 2: Advantages and disadvantages of active and passive core-agents

| Core-agents’ advantages and disadvantages |  

&#124; Active &#124;
&#124; Core-Agent &#124;

 |  

&#124; Passive &#124;
&#124; Core-Agent &#124;

 |
| --- | --- | --- |
| Advantages | Reinforce Single Responsibility Principle (SRP). |  | X |
| Imply simple implementation based on two modules. |  | X |
| Improve modularity and reduce complexity of the system. | X | X |
| Improve component reusability. | X | X |
| Improve composability and integration in multi core-agents’ setups without any (or with minor) synchronization. |  | X |
| Enhance LLM planning and memory capabilities. | X |  |
| Handle complex tasks. | X |  |
| Access to memory and contextual data. | X |  |
| Possess flexible profile that can be adapted dynamically. | X |  |
| Break down complex tasks into subtasks. | X |  |
| Possess multi-tasking capabilities. | X | X |
| Protects against adversarial attacks. | X |  |
|  | Imply complex implementation with multiple modules. | X |  |
| Disadvantages | Synchronization is needed in multi core-agents’ setups. | X |  |
| Handle tasks with limited complexity. |  | X |
| Preclude human-initiated communication |  | X |
|  | Lacks memory, limiting visibility into the agent’s overall status. |  | X |

Describing the characteristics of both active and passive core-agent classes results in constructing a well-structured summary, Table [1](https://arxiv.org/html/2409.11393v2#S3.T1 "Table 1 ‣ 3.3.1 Active Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), that encapsulates the distinct modules, their respective sub-modules, and the underlying methods for each category. This enables a comprehensive understanding of their functional differences and similarities within the context of LLM-based agents.

In this section, we detailed the construction of passive and active core-agents, emphasizing their architectural design. This analysis allowed us to identify their utilities and limitations, which are detailed in Table [2](https://arxiv.org/html/2409.11393v2#S3.T2 "Table 2 ‣ 3.3.2 Passive Core-Agents ‣ 3.3 Active/Passive Core-Agent Classification ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). Both categories improve modularity, reduce system complexity, and possess multi-tasking capabilities. Passive core-agents reinforce the single responsibility principle and imply simple implementation based only on action and security modules, enhancing reusability and offering straightforward integration into multi core-agents’ setups with minimal synchronization requirements which will be discussed in the Section [3.4](https://arxiv.org/html/2409.11393v2#S3.SS4 "3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). However, their simplicity limits their ability to handle complex tasks, precludes human-initiated communication, and lacks memory. Thus, it restricts visibility into the agent’s overall status and contextual data access, which is only available via API call requests. Additionally, they have no control over the LLM profile.

In contrast, active core-agents enhance LLM planning and memory capabilities, making them suitable for handling complex tasks. They can access memory and contextual data, control dynamically LLM’s profile, and break down complex tasks into manageable subtasks. Despite these advantages, active core-agents require complex implementation involving extra modules compared to passive core-agent and intricate synchronization in multi core-agent’ setups, which will be detailed in Section [3.4](https://arxiv.org/html/2409.11393v2#S3.SS4 "3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents").

### 3.4 Multi Active/Passive Core-Agent Architecture

Handling complex tasks often necessitates the use of multiple agents, as a single agent may not possess the requisite capabilities or expertise to tackle diverse domains. However, LLM-based multi-agent systems face considerable challenges, including scalability, integration, management of inter-agent relationships, and ensuring interpretability in managing intricate tasks [[60](https://arxiv.org/html/2409.11393v2#bib.bib60)]. In some instances, implementing a multi-agent system may be unnecessary, as their aforementioned complexities and drawbacks can be circumvented with a multi-core agent system. A single-agent system can potentially accommodate multiple core-agents, each dedicated to distinct tasks such as systematic execution or complex management. This idea leads us to propose a pioneering multi active/passive core-agent architecture.

To achieve the effective distribution of responsibilities and manage the workload within the agent system, we must propose an efficient classification of multi-core agent architectures. Our framework classifies multi-core agents into two primary categories: uniform and hybrid.

#### 3.4.1 Uniform multi-core agent

Uniform multi-core agents are exclusively based either on active core-agents or passive core-agents, unlike hybrid multi-core agents that integrate both active and passive core-agents within a single system.

![Refer to caption](img/f85f5f6733f10fe138f6d275d070fd97.png)

Figure 10: Multi-passive core-agent architecture

*   •

    Uniform multi-passive core-agent architecture leverages passive core-agents’ capabilities of handling low-level operations and executing specific tasks. The configuration shown in Figure [10](https://arxiv.org/html/2409.11393v2#S3.F10 "Figure 10 ‣ 3.4.1 Uniform multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") is an example where the LLM communicates with multiple passive core-agents and harnesses strategically their singular strengths to retrieve diverse information or perform specialized functions to generate a comprehensive final output. In fact, the language model assumes leadership and complete control over the ensemble of passive core-agents. In essence, uniform multi-passive core-agent systems are distinguished by the ease of integration of new passive core-agents, thereby extending their functionality without the need for complex synchronization. Consequently, the only modification resulting from the introduction of new passive core-agents involves adapting the LLM profile, either statically at setup/configuration time or dynamically via an active core-agent, as will be discussed in the context of the hybrid setup.

*   •

    Uniform active core-agents architecture deals with the interaction of a group of active core-agents in one system as illustrated in Figure [11](https://arxiv.org/html/2409.11393v2#S3.F11 "Figure 11 ‣ 2nd item ‣ 3.4.1 Uniform multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). As opposite to passive core-agents that operate solely with action and security modules, active core-agents possess all five modules (Planning, Memory, Profile, Action and Security) enabling them to manage complex cognitive tasks. This architecture may be seen as a better alternative to uniform multi-passive core-agents’ design due to its wider range of capabilities and functionalities. However, due to the authoritative nature of the active entities, the multi-active core-agent design is more complex than the one exclusively based on passive core-agents. The inclusion of multiple active elements in one system introduces challenges similar to those in multi-agent systems. For instance, effective communication among active core-agents is paramount; given their dynamic nature, timely and accurate exchange of information _such as inter-sibling core-agent feedback and status updates_ is crucial to ensure cohesive operation of the agent. As the number of active core-agents grows, managing intra-communication becomes increasingly complex resulting in frequently emerging synchronization issues. This is why multi-active core-agent systems potentially necessitate consensus algorithms, such as Raft [[61](https://arxiv.org/html/2409.11393v2#bib.bib61), [62](https://arxiv.org/html/2409.11393v2#bib.bib62)], to elect a leader.

    ![Refer to caption](img/48f8b4877f79c9010d6d5cbb70e98ec2.png)

    Figure 11: Multi-active core-agent architecture

    Therefore, we deduce that the independent implementation of either of these architectures is limited and may be problematic. While the multi-passive core-agent architecture is efficient in granular task execution and low-level operations, it lacks a component for handling high-level tasks such as decision-making, task planning, and resource allocation which are intrinsic to multi-active core-agent systems. However, the latter introduces synchronization issues and increases system complexity. This dilemma compels us to introduce the hybrid approach.

#### 3.4.2 Hybrid multi-core agent

To leverage the strength of both passive and active core-agents architectures, we propose an optimal system depicted in Figure [12](https://arxiv.org/html/2409.11393v2#S3.F12 "Figure 12 ‣ 3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). It integrates one active entity as the manager with multiple passive entities functioning as workers within a unified system. One managerial aspect of the active core-agent is its ability to configure dynamically the profile of LLMs, enabling them to effectively utilize passive core-agents for handling specific tasks. This configuration leverages the parallel execution capabilities of numerous passive core-agents under the guidance and leadership of an active core-agent, empowering the system to handle wider range of tasks, while preserving a comfortable level of flexibility, extendibility and scalability.

![Refer to caption](img/6fe6a490ac9deae30405da6b075b08cb.png)

Figure 12: One-active-many-passive core-agent architecture

This hybrid design realizes the full potential of the multi-core architecture: by uniting the strengths of the uniform passive core-agents architecture with the capabilities of the active core-agent, the system can dynamically allocate resources and adjust its configuration based on the specific requirements of the task at hand. Our proposed architecture of one-active-many-passive strikes a balance between the intricate nature of multi-active core-agent architectures and the practicality offered by passive core-agents.

As a matter of fact, in scenarios characterized by dynamic environmental changes, the inclusion of multiple active core-agents becomes essential to uphold the resilience and adaptability of the agent. Naturally, given the complexities outlined earlier, the implementation of an agent based on a many-active-many-passive architecture, as illustrated in Figure [13](https://arxiv.org/html/2409.11393v2#S3.F13 "Figure 13 ‣ 3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), would be intricate especially on the level of synchronization between active core-agents. Such a system impels a meticulous design, emphasizing intra-agent interactions, adherence to communication protocols, delineation of tasks for each active core-agent, and error-handling strategies. Clearly, these challenges underscore the simplicity of our proposed one-active-many-passive architecture. Nevertheless, there remains a promising opportunity for further research, as the challenges posed by multi-active core-agents pave the way for advancing and refining our framework.

![Refer to caption](img/3cdb2fcce32d2a53f0a8714a882497fd.png)

Figure 13: Many-active-many-passive core-agent architecture

In conclusion, the modularity of core-agents contributes to guaranteeing the composability within the agent architecture from a software perspective. It facilitates the seamless integration of new passive core-agents within a single agent system as the system scales, obviating the need for a transition to a multi-agent system. Furthermore, this architecture tackles scalability and adaptability challenges by adhering to the Open/Closed Principle (OCP), enhancing core-agents’ integration across evolving systems, and fostering robustness and flexibility. As outlined in the paper [[7](https://arxiv.org/html/2409.11393v2#bib.bib7)], the expansion of the system necessitates dynamic scaling to accommodate growing demands and ensure optimal performance. This entails adaptive capabilities such as increasing the number of agents or utilizing larger LLMs. These challenges are effectively addressed by architectures based on multiple core-agents primarily due to the unitary role a core-agent plays within the agent system. In fact, our framework allows the active core-agent to dynamically incorporate or detach passive core-agents, as illustrated by the switch linking the leader active core-agent and the passive core-agent (2) in Figure [13](https://arxiv.org/html/2409.11393v2#S3.F13 "Figure 13 ‣ 3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). In the following section, we discuss the results of our work.

## 4 Results and discussion

LLM-based agents have traditionally been discussed and conceptualized as intricate systems with different inner functional entities, such as LLMs and complementary software components, treated in an intertwined manner. Our proposed LLM-based Agent Unified Modeling Framework (LLM-Agent-UMF) was shaped to overcome these issues and promote a clear delineation of components and responsibilities. In this section, we present and analyze the outcomes derived from adopting the LLM-Agent-UMF through diverse evaluation approaches. Firstly, Section [4.1](https://arxiv.org/html/2409.11393v2#S4.SS1 "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") evaluates the newly introduced core-agent terminology by applying the LLM-Agent-UMF to existing systems that do not explicitly identify themselves as agents. Secondly, Section [4.2](https://arxiv.org/html/2409.11393v2#S4.SS2 "4.2 Evaluation of active/passive core-agent internal structure delineation ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") dives deeper into understanding and dissecting the internal structure of thirteen LLM-based agents by leveraging our proposed framework. This is performed not only on open-source agents but also on proprietary ones by formulating hypothesis based on their observed behavior. Through this process, we classify core-agents into active and passive types and identify critical modules that may have been overlooked or underemphasized during development. Finally, Section [4.3](https://arxiv.org/html/2409.11393v2#S4.SS3 "4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents") presents five novel architectures based on the LLM-Agent-UMF to model multi-core agents by combining characteristics from separate agents not intended for fusion in their original design.

### 4.1 Evaluation of the new core-agent terminology

One of the main reasons why agent definition remains misunderstood lies in the disparity between how researchers and practitioners use terminology to describe their work. For instance, some studies might not explicitly refer to their LLM-based system as an "agent", even though it exhibits characteristics commonly associated with agents such as autonomy, adaptability, and goal-directed behavior. The ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] research paper provides a prime example of this discrepancy. While the authors indeed utilize API retriever component in conjunction with their LLM, they refrain from using the term "agent". The same can be observed in the Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)] work, which implicitly incorporates software components to execute API call requests generated by LLMs but similarly avoids referencing the system as an agent.

Introducing the core-agent term, to denote the central component within LLM-powered agents, and defining the role it plays, address these terminological ambiguities and facilitate more transparent discussions about such systems. For clear communication, as explained in [[63](https://arxiv.org/html/2409.11393v2#bib.bib63)], one should avoid the use of different terms for one thing or a single term for different things. They suggest that establishing a well-defined language can help reduce confusion, promote consistency, and enhance communication among researchers and educators. These improvements will ultimately facilitate better collaboration, leading to accelerated development and optimization of these systems for a wide array of applications.

Based on the definition that we established for the core-agent component, we successfully identified core-agents within multiple LLM-based systems, mainly Toolformer and ToolLLM. On the one hand, Toolformer incorporates a simple-structured core-agent that includes only the action module responsible for API execution. On the other hand, ToolLLM adopted a more sophisticated approach. In fact, we recognize the use of a core-agent encompassing two modules: an action module represented by the neural API retriever and an implicate planning module responsible of managing the flow of information within the agent: The core-agent intercept user instructions, leverage the API retriever to gather relevant APIs, relays the APIs to the LLM for response generation, executes the requested APIs, and finally returns the outcome back to the LLM for final user response formulation. Thus, the identification of core-agents in these systems, accompanied with the presence of LLMs elucidates that Toolformer and ToolLLM are indeed LLM-based agents.

Furthermore, [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)] improves planning process by leveraging both LLMs and Planning Domain Definition Language (PDDL) based planners. In fact, the LLM generates a PDDL-based description of the problem, which is then evaluated by a PDDL-based planner to elaborate the optimal plan, and finally translated back into natural language. We observe that our framework aligns with the aforementioned architecture; The discussed technique describes indeed a core-agent containing a planning module powered with a PDDL interpreter and utilizing an LLM to translate from and to natural language.

It’s worth noting that another paper introduced a framework called ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)] which leverages databases as symbolic memory for LLMs. Although they referenced other memory-augmented LLM techniques such as Auto-GPT [[65](https://arxiv.org/html/2409.11393v2#bib.bib65)] and Generative Agent [[66](https://arxiv.org/html/2409.11393v2#bib.bib66)], the authors did not categorize their own framework as an agent. They presented a system comprising two components: an LLM controller and a memory module. Despite mentioning that any commonly used LLM can be employed as the controller component, it is evident that the LLM alone cannot interface with its associated database memory extension without the need of an intermediary software element. This analysis prompted us to identify an inherent core-agent responsible for managing the integration of LLMs into SQL query generation processes and executing these queries in the context of databases representing the system’s memory. Following this analysis, we deduce that ChatDB aligns with our defined software architectural framework for an LLM-based agent.

The integration of the security module into the core-agent is substantiated by the presence of security concerns in LLMs and the imperative to address them comprehensively. While some studies focus solely on guardrail techniques or algorithms without tying them to an agent, by leveraging our framework, the application of guardrails occurs within the frame of a core-agent, particularly within its security module.

For example, LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)] introduces a lightweight framework to protect in real-time the LLM text generation. This study opted for the usage of the beam search algorithm to generate candidate responses and leverage an external validator to handle the safety checking. Projecting this work onto our framework enabled us to conclude that this study indeed describes an LLM-based agent orchestrated by an inherent core-agent. The core-agent utilizes the LLM to generate candidate response, assesses its alignment with the agent’s safety constraints and evaluates whether to proceed with sentence completion in case of acceptance or generate alternative candidates in case of rejection.

Besides, despite paper [[67](https://arxiv.org/html/2409.11393v2#bib.bib67)] emphasizing on the necessity of achieving reliability, confidentiality, and integrity in LLM-based agents, it does not explicitly discuss the implementation of these mechanisms architecturally. Additionally, all discussed techniques serve the diverse security goals outlined in our framework, reinforcing the rationale for relocating guardrails to the security module within the core-agent.

The successful identification of core-agents within systems that do not identify themselves as agents emphasizes the significance of establishing a consistent terminology to accurately describe entities and underscore the importance of assigning distinct roles to software components within complex systems.

### 4.2 Evaluation of active/passive core-agent internal structure delineation

Our introduced framework, LLM-Agent-UMF, constitutes a valuable tool for comparing several state-of-the-art LLM-based agents as represented in Table [3](https://arxiv.org/html/2409.11393v2#S4.T3 "Table 3 ‣ 4.2 Evaluation of active/passive core-agent internal structure delineation ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). In this section, we evaluate our LLM-Agent-UMF by projecting existing agents onto our framework, distinguishing core-agents and highlighting the presence or absence of essential modules such as planning, memory, profile, action, and security modules.

Table 3: Classification of state-of-the-art agents using the LLM-Agent-UMF

|  | Core-Agent Modules |  |
|  | Planning | Profile | Memory | Action [*] | Security | Core-Agent Category |
| Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)] | - | - | - | X | - | Passive |
| Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)] | - | - | - | X | - | Passive |
| ToolAlpaca[[69](https://arxiv.org/html/2409.11393v2#bib.bib69)] | - | - | - | X | - | Passive |
|  | Zero-shot | - | - | - | X | - | Passive |
| Gorilla [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)] | With retriever | X | - | M | X | - | Active |
| ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] | X | - | M | X | - | Active |
| GPT4Tools [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)] | M | - | M | X | - | Active |
| Chameleon [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)] | X | X | M | X | - | Active |
| ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)] | X | X | X | M | - | Active |
| ChemCrow [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)] | X | X | M | X | M | Active |
| LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)] | X | X | M | M | - | Active |
| LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)] | X | - | - | M | X | Active |
|  | Hypothesis 1 | - | - | - | - | - | N/A |
| ChatGPT 4o mini | Hypothesis 2 | X | - | - | M | X | Active |
| ChatGPT 4o | Hypothesis 3 | X | X | X | X | X | Active |

X: Denotes the presence of a module and the fact that its functionalities were well discussed in the research.

M: Denotes the presence of a minimal implied module and that it was not the main focus of the research.

^([∗]) The action module is an essential module in a core-agent. In its minimal form, it is responsible for human-machine interaction only.

In this case study, the thirteen agents that will be thoroughly examined are Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)], Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)], ToolAlpaca [[69](https://arxiv.org/html/2409.11393v2#bib.bib69)], Gorilla [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)], ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)], GTP4Tools [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)], ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)], Chameleon [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)], LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)], ChemCrow [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)], LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)], as well as both ChatGPT 4o and its minimal version. Each of these LLM-based agents has been carefully selected to showcase a diverse range of functionalities across various modules within our proposed framework. The outcome of this dissection constitutes a valuable medium to identify overlooked functionalities and complementary counterparts. It also allows us to distinguish entities that may either play or not an authoritative role in a system, subsequently categorizing them as active or passive core-agents.

Initially, our analysis focuses on Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)], Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)] and ToolAlpaca [[69](https://arxiv.org/html/2409.11393v2#bib.bib69)] which possess significant similarities in terms of functionality and behavior. All three agents primarily offer innovative finetuning methods aimed at influencing LLMs to utilize APIs for more accurate results. Notably, they do not require external assistance from other software components for orchestration or API selection; instead, the LLM’s inherent capabilities are leveraged for this purpose. The passive core-agent design within these tools is notable, featuring a unique action module responsible for executing the LLM’s will in making API calls and relaying responses back to the model for improved formulation.

Going further in our analysis, we study ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] and Gorilla [[70](https://arxiv.org/html/2409.11393v2#bib.bib70)], and we examine the latter two modes: "zero-shot" and "with retriever". In the zero-shot mode, Gorilla behaves similarly to the three previously discussed agents by acting as an LLM-based agent with a passive core-agent that possesses an action module for API call execution. However, in the second mode, functioning similarly to ToolLLM, Gorilla utilizes an API retriever to gather API recommendations and fetch documentation. This process necessitates a software component for orchestrating interactions with LLMs and tools; this component, which we label as the core-agent, was not highlighted in the original papers. It is inherent that this core-agent includes a planning module incorporated as the API retriever and requires a minimal memory module to manage the internal state of the agent in case of multi-step instructions.

We complement our analysis by deeply evaluating the case of GPT4Tools [[36](https://arxiv.org/html/2409.11393v2#bib.bib36)], which took inspiration from its predecessors and focused on finetuning the LLM to utilize tools but extended its capabilities to leverage visual models such as image segmentation and generation models. Once trained using their technique, the LLM is capable of responding to instruction by performing multiple steps to achieve a targeted goal. To satisfy the needs of such an LLM, a software component must be integrated to listen to its requests, execute actions and either return textural responses or store intermediate outputs such as images in its memory to be utilized in subsequent steps. This software entity, identified as the core-agent, lacks any established authoritative nature but requires basic planning and memory modules to manage multi-step subtasks. Therefore, we classify it as an active core-agent.

Projecting ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)] onto our framework, as discussed in Section [4.1](https://arxiv.org/html/2409.11393v2#S4.SS1 "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), reveals the existence of a core-agent. Its managerial position within the agent system makes a clear indication of its active nature. In fact, this core-agent incorporates a planning module that follows a chain-of-memory approach and utilizes an LLM for decomposing complex problems into multiple steps of memory operations. To efficiently manage read and write operations to the memory extension implemented as a SQL database, an additional memory module is essential. Moreover, we highlight the necessity of a profile module adopting the in-context learning approach to influence the LLM in generating appropriate SQL queries.

Additionally, Chameleon [[8](https://arxiv.org/html/2409.11393v2#bib.bib8)], another LLM-based agent, harnesses the power of GPT-4 for planning and selecting suitable tools to achieve desired goals. It synthesizes programs that leverage various tools such as LLMs, vision models, python functions or heuristic-based modules. To accomplish this, a software component is necessary to manage the flow of information and utilize these tools effectively. In this context, we identify a core-agent comprising: a planning module empowered with GPT-4, a profile module to control the behavior of the LLM without any prior training, a minimal memory module to store intermediate results and a versatile action module that make use of the available tools. The internal structure and the managerial aspect of this core-agent lead us to classify it as an active core-agent.

As previously elucidated in Section [4.1](https://arxiv.org/html/2409.11393v2#S4.SS1 "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), the LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)] solution is recognized as an LLM-based agent processing a core-agent with a sophisticated planning module integrating a PDDL interpreter. The described approach requires influencing GPT-4 to convert from and to PDDL representation and thus underscore the need for profile module. The core-agent needs also a minimal memory module to manage the internal state of the agent as there are multiple phases in the planning process. The presence of these modules emphasizes the active nature of the core-agent.

Furthermore, we would like to emphasize the significant effort invested in ChemCrow [[12](https://arxiv.org/html/2409.11393v2#bib.bib12)], a chemistry-oriented agent. When projecting it onto LLM-Agent-UMF, we recognize ChemCrow as an active core-agent itself. The researchers explicitly defined it as an independent entity from both the LLM and the available tools. Notably, this intricate core-agent leverages the capabilities of GPT-4 to coordinate the utilization of 18 expert-designed tools for synthesizing chemical compounds. As a result, we acknowledge the presence of several crucial modules within ChemCrow: A planning module responsible for orchestrating the entire process; a minimal memory module that stores intermediate results; a profile module to guide GPT-4’s text generation process; a versatile action module managing communication with all available tools; and a security module, which despite its simplicity, plays a vital role in checking safety information before proceeding further with the task at hand and interrupts the execution if it is deemed dangerous.

Another notable solution is LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)] that we have already identified as an LLM-based agent with an implicit core-agent. The researchers augmented the beam search algorithm with an external validator which rejects candidates that violate security constraints and proceeds with valid ones. The core-agent in this solution is indeed an active one and includes a planning module, which ensures the correct execution of the workflow, and a security module represented by the external validator.

As a prominent language modeling solution, ChatGPT’s inner workings warrant thorough examination to better understand its capabilities and limitations. However, being a proprietary and closed-source system limits our ability to assertively analyze its structure and capabilities. Although it exhibits characteristics that align with a simple LLM, we cannot definitively conclude whether it is indeed a standalone LLM or an LLM-based agent. Nevertheless, based on public information and rigorous analysis of ChatGPT behavior through the lens of our framework, we can make plausible hypotheses regarding its structure.

![Refer to caption](img/62a5b9e13234464b78b382945d749d89.png)

Figure 14: ChatGPT 4o mini refusing to explain how to hot-wire a car

To guide our hypothesis, we assessed ChatGPT 4o mini’s response to illegal prompts by demanding the steps to hotwire a car. Our investigation aimed to verify the system’s behavior when confronted with malicious queries. Matching our expectation, ChatGPT 4o mini declined to respond to direct illegal prompts, as illustrated in Figure [14](https://arxiv.org/html/2409.11393v2#S4.F14 "Figure 14 ‣ 4.2 Evaluation of active/passive core-agent internal structure delineation ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). Consequently, we formulated two hypotheses on how the security measures are implemented:

*   •

    Hypothesis 1: The LLM itself is trained to monitor the input and prevent generation of undesirable output. This can be achieved by adhering to Adversarial Training (AT) techniques [[71](https://arxiv.org/html/2409.11393v2#bib.bib71)]. Additionally, it has been verified that ChatGPT 4o mini does not possess direct access to external tools or sources of information; hence, we can hypothesize that it is not an agent in the first place.

*   •

    Hypothesis 2: Security measures are implemented independently outside the scope of the main LLM. The input is handled before being forwarded to the LLM, and the output is monitored after its generation and before being presented to the user. According to this flow of events, we observe a minimum level of algorithmic planning thus the need for an active core-agent responsible for managing the guardrails of ChatGPT. However, such simple planning designed for a specific goal _ensuring safeguarding workflow_ does not require a memory module nor a profile module.

Similarly, we consider the capability of ChatGPT 4o on code execution and suppose the presence of an active core-agent. Indeed, we construct the third hypothesis, around that process as follows: the core-agent checks if the prompt requires coding operations. If it is the case, it changes the profile of the LLM according to the task at hand, then leverages the LLM capability to generate code which is later executed in an isolated environment. Afterwards it reset the profile of the LLM to explain the results in a suitable textual representation. This hypothesis could be further enriched with assumptions from the ChatGPT 4o mini second hypothesis about the security measures and leads us to the conclusion that ChatGPT 4o is an LLM-based agent powered with a fully featured active core-agent rather than a standalone LLM.

Through this exercise, we demonstrate how the LLM-Agent-UMF can assist developers in reevaluating their agent designs and potentially improving upon them. Specifically, out of all studied agents that utilize external tools, 78% (7 over 9) did not incorporate necessary security measures to handle privacy concerns. Only ChemRow and ChatGPT demonstrated an ability to manage this aspect effectively. ChemRow utilized IBM Research’s tools, which fundamentally emphasize privacy protection [[72](https://arxiv.org/html/2409.11393v2#bib.bib72)], while ChatGPT takes an additional step by requiring user validation before sharing any data with third-parties. This highlights the critical importance of addressing privacy concerns and safeguarding against information leakage to ensure robustness and trustworthiness in AI-powered systems.

In our analysis, we identified that only 31% (4 out of 13) of the examined agents utilized passive core-agents. Conversely, a majority of 69% had active core-agents. This distinction may be significant when considering integrating these agent types into multi-core systems as passive core-agents are not typically subject to heavy synchronization processes, which can potentially improve system scalability.

Moreover, we observed that while some modules were present in the studied agents, they were sometimes implemented minimally and could likely be easily merged with other systems offering more comprehensive module implementations. For instance, ToolLLM has a simplistic memory module compared to ChatDB’s robust version. It is also crucial to note that the planning module plays an integral role in both agents. Therefore, any merge or integration must consider these distinct features to maximize synergy and prevent compromising functionality within the resulting system.

By examining the core-agents within each agent, researchers and practitioners can deduce the compatibility of core-agents and identify challenges that may arise when integrating multiple agents into one system such as the necessity of synchronization, potential conflicts between core-agents or functional redundancy.

### 4.3 Evaluation of multi-core agent architectures

Thanks to the classification done in Table [3](https://arxiv.org/html/2409.11393v2#S4.T3 "Table 3 ‣ 4.2 Evaluation of active/passive core-agent internal structure delineation ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), we can now effortlessly merge various aspects from existing agents into a single entity. To demonstrate this capability, we propose five representative scenarios that highlight the potential of LLM-Agent-UMF for designing multi core-agent systems by combining distinctive features from state-of-the-art agents. Each of these scenarios utilizes Llama 3.1 8B [[58](https://arxiv.org/html/2409.11393v2#bib.bib58)], the newest state-of-the-art LLM from Meta AI team, for its remarkable performance and optimized memory footprint.

#### 4.3.1 Toolformer and Confucius as a multi passive core-agents system

As both Toolformer [[38](https://arxiv.org/html/2409.11393v2#bib.bib38)] and Confucius [[68](https://arxiv.org/html/2409.11393v2#bib.bib68)] agents incorporate only passive core-agents, it becomes evident that integrating their capabilities within one agent is viable. As illustrated in Figure [15](https://arxiv.org/html/2409.11393v2#S4.F15 "Figure 15 ‣ 4.3.1 Toolformer and Confucius as a multi passive core-agents system ‣ 4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), the new agent, named LA1 (LLM-based Agent 1), encompasses two passive core-agents. On one hand, the Toolformer passive core-agent empowers the agent with the ability to utilize specialized tools such as a calculator, a calendar, a knowledge retrieval LM, a machine translation system and the Wikipedia search engine, ensuring that LA1 can effectively handle these tools in an accurate manner. On the other hand, the Confucius passive core-agent acts as a complementary second core-agent, enabling LA1 to manage unseen tools and work alongside Toolformer to tackle tools not previously evaluated or encountered during the testing phase. This versatile design makes LA1 capable of dealing with new challenges in real world scenarios while maximizing efficiency.

![Refer to caption](img/ca50564e79d4cf967c7a7884237195a9.png)

Figure 15: LLM-based Agent 1 (LA1): Toolformer and Confucius – Multi passive core-agent architecture

Nevertheless, it is crucial to ensure that the agent LLM, Llama 3.1 8B, is aligned with relevant regulation datasets. As elucidated by the LLM-Agent-UMF, techniques such as LoRA can be used to create pluggable modules to define the profile of the LLM. In fact, Toolformer’s modified version of CCNet augmented with API calls should be used to teach the LLM how to communicate appropriately with the Toolformer passive core-agent. Similarly, Confucius, defining itself as a tool learning framework, should also be leveraged to train the LLM to master various external tools.

The integration of these two passive core-agents within LA1 showcases the effectiveness of LLM-Agent-UMF in designing multi passive core-agent systems and highlights its flexibility as well as potential for combining multiple existing agents’ capabilities. Furthermore, it is important to acknowledge that during this process, LLM-Agent-UMF led us to identify weaknesses in the architectural design such as the absence of a privacy safeguarding mechanism to monitor data transfers between the Toolformer and Confucius core-agents and external service providers. This emphasizes the significance of ongoing research aimed at addressing these concerns.

#### 4.3.2 ToolLLM and ChatDB as a multi active core-agent system

The second scenario explores a new agent design that integrates the ToolLLM [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] and ChatDB [[31](https://arxiv.org/html/2409.11393v2#bib.bib31)] capabilities. While both agents possess unique strengths, their combined functionality offers an advantageous synergy. Equipped with a neural API retriever, ToolLLM is capable of leveraging the appropriate external API to fulfill human instructions. On the other hand, ChatDB incorporates an SQL-based symbolic memory framework that enables LLMs to perform complex multi-hop reasoning.

![Refer to caption](img/c735fe0bd3b13fa6c9326586bbe3dd65.png)

Figure 16: LLM-based Agent 2-A (LA2-A): ToolLLM and ChatDB – Multi active core-agent architecture

As mentioned in Section [3.4.2](https://arxiv.org/html/2409.11393v2#S3.SS4.SSS2 "3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), incorporating multiple active core-agents within one system could pose challenges. To evaluate this scenario, two architectural variants were explored: LA2-A and LA2-B. In LA2-A, Figure [16](https://arxiv.org/html/2409.11393v2#S4.F16 "Figure 16 ‣ 4.3.2 ToolLLM and ChatDB as a multi active core-agent system ‣ 4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), both ToolLLM and ChatDB retained their individual functionalities as distinct active core-agents. This case requires synchronization between the two active core-agents and opting for a consensus algorithm like Raft [[61](https://arxiv.org/html/2409.11393v2#bib.bib61)] would be a knowledgeable choice. To further optimize the memory footprint of the system, there will be one unique instance of Llama 3.1 8B shared between the two active core-agents and each one of them must inject the adequate profile dynamically either as a pluggable trained module like LoRA or using a system prompt.

However, for LA2-B, illustrated in Figure [17](https://arxiv.org/html/2409.11393v2#S4.F17 "Figure 17 ‣ 4.3.2 ToolLLM and ChatDB as a multi active core-agent system ‣ 4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), the unique capabilities of the two core-agents were merged into a single monolithic active core-agent. In this case, it is essential to identify specific modules where conflicts may arise. Being the key element in an active core-agent, the planning module must be thoroughly analyzed. Indeed, it should be designed to optimally handle external API calling through the integration of ToolLLM’s API retriever while also seamlessly communicating with ChatDB’s memory module specialized in SQL-based database handling. Furthermore, the profile module must be able to select the appropriate profile for the LLM depending on the task at hand. By addressing potential conflicts within these modules, LA2-B can effectively leverage both agents’ strengths and achieve a synergistic advantage by leveraging only one monolithic active core-agent.

These two scenarios highlight the versatility of the LLM-Agent-UMF for designing novel LLM-based agents combining multiple complex state-of-the-art agents supplemented with active core-agents, while also identify and addresses the challenges associated with such integration efforts.

![Refer to caption](img/109b824e5334d28dd6354dc715ecc4e2.png)

Figure 17: LLM-based Agent 2-B (LA2-B): ToolLLM and ChatDB – Monolithic active core-agent architecture

#### 4.3.3 Implanting the LLMSafeGuard security module into ToolLLM

A third observation is that any active core-agent can be taken as a base to be expanded with other modules as depicted in Figure [18](https://arxiv.org/html/2409.11393v2#S4.F18 "Figure 18 ‣ 4.3.3 Implanting the LLMSafeGuard security module into ToolLLM ‣ 4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). Taking the example of ToolLLM, we can implant the security module of another agent selected based on specific security objectives. Namely, if the goal is to augment ToolLLM’s capabilities [[59](https://arxiv.org/html/2409.11393v2#bib.bib59)] with real-time safeguarding of the generated text, we can incorporate the security module of LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)] resulting in a newly designed agent, LA3\. This example underscores the simplicity of such integration from a software architectural viewpoint.

Indeed, the LLM-Agent-UMF enables us to easily identify and incorporate missing modules without causing functional conflicts or challenges. Structurally, LA3 inherits the four primary modules of ToolLLM, along with the security module from LLMSafeGuard, seamlessly expanding its capabilities while maintaining compatibility and coherence between components.

![Refer to caption](img/6d2fc522f453d47e8e8d0e08a497bd43.png)

Figure 18: LLM-based Agent 3 (LA3): ToolLLM with the security module of LLMSafeGuard

#### 4.3.4 Hybrid multi active/passive core-agents system

The last proposed agent design, LA4, is the most broad-based integration proposition representing the one-active-many-passive architecture outlined in Section [3.4.2](https://arxiv.org/html/2409.11393v2#S3.SS4.SSS2 "3.4.2 Hybrid multi-core agent ‣ 3.4 Multi Active/Passive Core-Agent Architecture ‣ 3 LLM-based Agent Unified Modeling Framework ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). As illustrated in Figure [19](https://arxiv.org/html/2409.11393v2#S4.F19 "Figure 19 ‣ 4.3.4 Hybrid multi active/passive core-agents system ‣ 4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), LA4 architecture incorporates the active core-agent from LLM+P [[64](https://arxiv.org/html/2409.11393v2#bib.bib64)], implants the security module from LLMSafeGuard [[54](https://arxiv.org/html/2409.11393v2#bib.bib54)], and integrates the two passive core-agents from Toolformer and Confucius.

The selection of the LLM+P active core-agent as our central active entity was motivated by its cutting-edge planning module. As delineated in Section [4.1](https://arxiv.org/html/2409.11393v2#S4.SS1 "4.1 Evaluation of the new core-agent terminology ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), it seamlessly makes use of an LLM to generate a PDDL-based description from natural language inputs, which is subsequently evaluated by the integrated PDDL planner to establish an optimal plan. Unfortunately, as identified in Table [3](https://arxiv.org/html/2409.11393v2#S4.T3 "Table 3 ‣ 4.2 Evaluation of active/passive core-agent internal structure delineation ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), LLM+P does not possess a security module making the overall agent vulnerable to security threats such as adversarial attacks that easily circumvent basic protection mechanisms such as implemented using adversarial training. This led us to leverage the same solution proposed in Section [4.3.3](https://arxiv.org/html/2409.11393v2#S4.SS3.SSS3 "4.3.3 Implanting the LLMSafeGuard security module into ToolLLM ‣ 4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"). and implant the LLMSafeGuard security module in the LLM+P active core-agent. As a result, this procedure yields an optimized active core-agent that incorporates all five necessary modules for efficient functioning while ensuring robustness against potential security threats.

To further enhance LA4’s capabilities and performance, it would be advantageous to empower the agent with the skill of effectively utilizing available tools and APIs. In Section [4.3.1](https://arxiv.org/html/2409.11393v2#S4.SS3.SSS1 "4.3.1 Toolformer and Confucius as a multi passive core-agents system ‣ 4.3 Evaluation of multi-core agent architectures ‣ 4 Results and discussion ‣ LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents"), Toolformer and Confucius were proposed as suitable candidates due to their complementary features. Indeed, their passive core-agents nature makes the integration straightforward and does not necessitate advanced synchronization mechanisms since both will be controlled by the LLM which itself is managed by the active core-agent. The primary consideration is the incorporation of both Toolformer and Confucius profiles into the profile module of LA4’s active core-agent, allowing it to adapt the LLM behavior dynamically for its specific needs. By combining these various technologies within LA4, it would become a comprehensive agent capable of elaborating an optimal planning strategy leveraging tools and external APIs.

![Refer to caption](img/753a172ca33e448da4c77aa6b2e67d3d.png)

Figure 19: LLM-based Agent 4 (LA4): LLM+P, LLMSafeGuard, Toolformer and Confucius - Hybrid multi active/passive core-agents architecture

Nonetheless, it is crucial to address the security vulnerability uncovered through the application of LLM-Agent-UMF within LA4’s design. Indeed, the external API calling mechanisms utilized by Toolformer and Confucius passive core-agents are not monitored or safeguarded against potential information leakage. Following the guidance provided by LLM-Agent-UMF, this issue can be resolved either by implementing a dedicated security module on each of these passive core-agents or as supplementary measures within the active core-agent’s security module, thus centralizing the management of information safeguarding processes. The optimal selection of technologies and implementation details will be explored in future work.

The five discussed scenarios exemplify how researchers and developers can make informed decisions about the design of their LLM-based agents prior to the development process, grounded in clear architectural reasoning. This systematic approach will enhance the robustness and functionality of LLM-based agents. In the next section, we will discuss the limitations and the future work to enhance our framework.

## 5 Conclusion and future work

In this paper, we introduce a structural component within LLM-based agents named the "core-agent". This component is engineered to address the architectural ambiguities that software developers encounter and foster better understanding of the interacting entities within LLM-powered agents. We propose the LLM-Agent-UMF, a comprehensive framework for modeling the structure of the agent, explicating each of core-agent’s five modules: planning, memory, profile, action, and security. Subsequently, we classified core-agents into passive and active categories and highlighted their structural and functional differences. Based on this classification, we designed uniform and hybrid multi-core agent architectures. Most prominently, the one-active-many-passive architecture exploits the full potential of both active and passive core-agents, striking a balance between easiness of development and the power of hybrid architectures. By applying our framework to state-of-the-art agents, we identified within their structures core-agents and their constituting internal modules which assisted us in the classification process. This allowed us to recognize the individual characteristics of each agent, discern their limitations and discover potential prospects of merging different functionalities into a single multi-core agent.

Our work prepares the foundation for the development of LLM-based agents with a clear delineated structure that leverages the power of core-agents. The progressive adoption of LLM-Agent-UMF will further attest to its efficiency. An interesting future direction to improve it involves finding solutions to simplify the implementation of multi-active core-agent architectures. In fact, they suffer from challenges related to synchronization that necessitate further investigation. In this context, we see two promising avenues: Integrating a consensus algorithms like Raft to elect a leader responsible for the coordination; Otherwise, integrating a central gateway that is solely responsible of selecting the most suitable active core-agent to handle the user request based on factors like load, availability, and domain. Each active core-agent shall register with the gateway, providing information about their capabilities and status. The selected core-agent processes the task and sends the response back to the user through the gateway.

In conclusion, the ultimate purpose of this framework, which is predicated on the core-agent, is to reformulate the conception of LLM-based agents. By basing their development on this unit rather than addressing it in a monolithic manner, researchers and practitioners can use a unified terminology to refer to different modules within a common architecture. This shared foundation not only facilitates uniformity in research and development but also enables developers to implement consistent solutions that are easily maintainable and highly adaptable for future improvements.

## CRediT authorship contribution statement

Amine Ben Hassouna: Writing – original draft, Writing – review & editing, Methodology, Project administration, Supervision, Validation, Visualization, Investigation, Data Curation, Formal analysis, Conceptualization. Hana Chaari: Writing – original draft, Methodology, Investigation, Data Curation, Formal analysis. Ines Belhaj: Writing – original draft, Methodology, Investigation, Data Curation, Formal analysis.

## References

*   Chang et al. [2024] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language models. *ACM Trans. Intell. Syst. Technol.*, 15(3), mar 2024. ISSN 2157-6904. doi:[10.1145/3641289](https://doi.org/10.1145/3641289). URL [https://doi.org/10.1145/3641289](https://doi.org/10.1145/3641289).
*   Yin et al. [2023] Bin Yin, Junjie Xie, Yu Qin, Zixiang Ding, Zhichao Feng, Xiang Li, and Wei Lin. Heterogeneous knowledge fusion: A novel approach for personalized recommendation via llm. In *Proceedings of the 17th ACM Conference on Recommender Systems*, RecSys ’23, page 599–601, New York, NY, USA, 2023\. Association for Computing Machinery. ISBN 9798400702419. doi:[10.1145/3604915.3608874](https://doi.org/10.1145/3604915.3608874). URL [https://doi.org/10.1145/3604915.3608874](https://doi.org/10.1145/3604915.3608874).
*   Franklin and Graesser [1997] Stan Franklin and Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. In Jörg P. Müller, Michael J. Wooldridge, and Nicholas R. Jennings, editors, *Intelligent Agents III Agent Theories, Architectures, and Languages*, pages 21–35, Berlin, Heidelberg, 1997\. Springer Berlin Heidelberg. ISBN 978-3-540-68057-4. doi:[10.1007/BFb0013570](https://doi.org/10.1007/BFb0013570).
*   Xi et al. [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey, 2023.
*   Wang et al. [2024a] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large language model based autonomous agents. *Frontiers of Computer Science*, 18(6):186345, 2024a. doi:[10.1007/s11704-024-40231-1](https://doi.org/10.1007/s11704-024-40231-1). URL [https://doi.org/10.1007/s11704-024-40231-1](https://doi.org/10.1007/s11704-024-40231-1).
*   Parisi et al. [2022] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models, 2022. URL [https://arxiv.org/abs/2205.12255](https://arxiv.org/abs/2205.12255).
*   Cheng et al. [2024a] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. Exploring large language model based intelligent agents: Definitions, methods, and prospects, 2024a. URL [https://arxiv.org/abs/2401.03428](https://arxiv.org/abs/2401.03428).
*   Lu et al. [2023] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36, pages 43447–43478\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf).
*   Wang et al. [2024b] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents, 2024b. URL [https://arxiv.org/abs/2402.01030](https://arxiv.org/abs/2402.01030).
*   Hu et al. [2024] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu. A survey on large language model-based game agents, 2024. URL [https://arxiv.org/abs/2404.02039](https://arxiv.org/abs/2404.02039).
*   Chu et al. [2024a] Zhixuan Chu, Yan Wang, Feng Zhu, Lu Yu, Longfei Li, and Jinjie Gu. Professional agents – evolving large language models into autonomous experts with human-level competencies, 2024a. URL [https://arxiv.org/abs/2402.03628](https://arxiv.org/abs/2402.03628).
*   Bran et al. [2024] M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. Augmenting large language models with chemistry tools. *Nature Machine Intelligence*, 6(5):525–535, May 2024. doi:[10.1038/s42256-024-00832-8](https://doi.org/10.1038/s42256-024-00832-8). URL [https://doi.org/10.1038/s42256-024-00832-8](https://doi.org/10.1038/s42256-024-00832-8).
*   Dong et al. [2024a] Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, and Xiaowei Huang. Building guardrails for large language models, 2024a. URL [https://arxiv.org/abs/2402.01822](https://arxiv.org/abs/2402.01822).
*   Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36, pages 80079–80110\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf).
*   Duan et al. [2023] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. Flocks of stochastic parrots: Differentially private prompt learning for large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36, pages 76852–76871\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/f26119b4ffe38c24d97e4c49d334b99e-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/f26119b4ffe38c24d97e4c49d334b99e-Paper-Conference.pdf).
*   Lee et al. [2023] Unggi Lee, Sanghyeok Lee, Junbo Koh, Yeil Jeong, Haewon Jung, Gyuri Byun, Yunseo Lee, Jewoong Moon, Jieun Lim, and Hyeoncheol Kim. Generative agent for teacher training: Designing educational problem-solving simulations with large language model-based agents for pre-service teachers. In *NeurIPS’23 Workshop on Generative AI for Education (GAIED)*, 2023. URL [https://gaied.org/neurips2023/files/8/8_paper.pdf](https://gaied.org/neurips2023/files/8/8_paper.pdf).
*   Ampatzoglou et al. [2019] Apostolos Ampatzoglou, Angeliki-Agathi Tsintzira, Elvira-Maria Arvanitou, Alexander Chatzigeorgiou, Ioannis Stamelos, Alexandru Moga, Robert Heb, Oliviu Matei, Nikolaos Tsiridis, and Dionisis Kehagias. Applying the single responsibility principle in industry: Modularity benefits and trade-offs. In *Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering*, EASE ’19, page 347–352, New York, NY, USA, 2019\. Association for Computing Machinery. ISBN 9781450371452. doi:[10.1145/3319008.3320125](https://doi.org/10.1145/3319008.3320125). URL [https://doi.org/10.1145/3319008.3320125](https://doi.org/10.1145/3319008.3320125).
*   Cheng et al. [2024b] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. Exploring large language model based intelligent agents: Definitions, methods, and prospects, 2024b. URL [https://arxiv.org/abs/2401.03428](https://arxiv.org/abs/2401.03428).
*   Turan and Tanrıöver [2018] O. Turan and Ö. Ö. Tanrıöver. An experimental evaluation of the effect of solid principles to microsoft vs code metrics. *AJIT-E: Academic Journal of Information Technology*, 9(34):7–24, 2018. doi:[10.5824/1309-1581.2018.4.001.x](https://doi.org/10.5824/1309-1581.2018.4.001.x).
*   Laguna et al. [2010] Miguel A. Laguna, José M. Marqués, and Yania Crespo. On the semantics of the extend relationship in use case models: Open-closed principle or clairvoyance? In Barbara Pernici, editor, *Advanced Information Systems Engineering*, pages 409–423, Berlin, Heidelberg, 2010\. Springer Berlin Heidelberg. ISBN 978-3-642-13094-6.
*   Ghallab [2004] Malik Ghallab. *Automated Planning: Theory and Practice*. Morgan Kaufmann, 2004.
*   Huang et al. [2024] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey, 2024. URL [https://arxiv.org/abs/2402.02716](https://arxiv.org/abs/2402.02716).
*   Fang et al. [2024] Haishuo Fang, Xiaodan Zhu, and Iryna Gurevych. Dara: Decomposition-alignment-reasoning autonomous language agent for question answering over knowledge graphs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, *Findings of the Association for Computational Linguistics ACL 2024*, pages 3406–3432, Bangkok, Thailand and virtual meeting, aug 2024\. Association for Computational Linguistics. URL [https://aclanthology.org/2024.findings-acl.203](https://aclanthology.org/2024.findings-acl.203).
*   Chu et al. [2024b] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1173–1203, Bangkok, Thailand, aug 2024b. Association for Computational Linguistics. URL [https://aclanthology.org/2024.acl-long.65](https://aclanthology.org/2024.acl-long.65).
*   Besta et al. [2024] Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Aidan O’Mahony, Onur Mutlu, and Torsten Hoefler. Demystifying chains, trees, and graphs of thoughts, 2024. URL [https://arxiv.org/abs/2401.14295](https://arxiv.org/abs/2401.14295).
*   Ghallab et al. [1998] Malik Ghallab, Craig Knoblock, David Wilkins, Anthony Barrett, Dave Christianson, Marc Friedman, Chung Kwok, Keith Golden, Scott Penberthy, David Smith, Ying Sun, and Daniel Weld. Pddl - the planning domain definition language, 08 1998.
*   Zhang et al. [2024] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents, 2024. URL [https://arxiv.org/abs/2404.13501](https://arxiv.org/abs/2404.13501).
*   Atkinson and Shiffrin [1968] R.C. Atkinson and R.M. Shiffrin. Human memory: A proposed system and its control processes. In Kenneth W. Spence and Janet Taylor Spence, editors, *Psychology of Learning and Motivation*, volume 2, pages 89–195\. Academic Press, 1968. doi:[10.1016/S0079-7421(08)60422-3](https://doi.org/10.1016/S0079-7421(08)60422-3). URL [https://www.sciencedirect.com/science/article/pii/S0079742108604223](https://www.sciencedirect.com/science/article/pii/S0079742108604223).
*   Zhong et al. [2024] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. *Proceedings of the AAAI Conference on Artificial Intelligence*, 38(17):19724–19731, Mar. 2024. doi:[10.1609/aaai.v38i17.29946](https://doi.org/10.1609/aaai.v38i17.29946). URL [https://ojs.aaai.org/index.php/AAAI/article/view/29946](https://ojs.aaai.org/index.php/AAAI/article/view/29946).
*   Wang et al. [2024c] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. *Transactions on Machine Learning Research*, 2024c. ISSN 2835-8856. URL [https://openreview.net/forum?id=ehfRiF0R3a](https://openreview.net/forum?id=ehfRiF0R3a).
*   Hu et al. [2023] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory, 2023. URL [https://arxiv.org/abs/2306.03901](https://arxiv.org/abs/2306.03901).
*   Zhu et al. [2023a] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory, 2023a. URL [https://arxiv.org/abs/2305.17144](https://arxiv.org/abs/2305.17144).
*   Singh et al. [2024a] Aniket Kumar Singh, Bishal Lamichhane, Suman Devkota, Uttam Dhakal, and Chandra Dhakal. Do large language models show human-like biases? exploring confidence—competence gap in ai. *Information*, 15(2), 2024a. ISSN 2078-2489. doi:[10.3390/info15020092](https://doi.org/10.3390/info15020092). URL [https://www.mdpi.com/2078-2489/15/2/92](https://www.mdpi.com/2078-2489/15/2/92).
*   Singh et al. [2024b] Aniket Kumar Singh, Bishal Lamichhane, Suman Devkota, Uttam Dhakal, and Chandra Dhakal. Do large language models show human-like biases? exploring confidence - competence gap in ai. *Inf.*, 15:92, 2024b. URL [https://api.semanticscholar.org/CorpusID:267539494](https://api.semanticscholar.org/CorpusID:267539494).
*   Argyle et al. [2023] Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and David Wingate. Out of one, many: Using language models to simulate human samples. *Political Analysis*, 31(3):337–351, 2023. doi:[10.1017/pan.2023.2](https://doi.org/10.1017/pan.2023.2).
*   Yang et al. [2023] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36, pages 71995–72007\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf).
*   Xu et al. [2023] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. URL [https://arxiv.org/abs/2312.12148](https://arxiv.org/abs/2312.12148).
*   Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36, pages 68539–68551\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf).
*   Shen et al. [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36, pages 38154–38180\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf).
*   Lewis et al. [2021] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. URL [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401).
*   Wani et al. [2024] Niyaz Ahmad Wani, Ravinder Kumar, Mamta, Jatin Bedi, and Imad Rida. Explainable ai-driven iomt fusion: Unravelling techniques, opportunities, and challenges with explainable ai in healthcare. *Information Fusion*, 110:102472, 2024. ISSN 1566-2535. doi:[10.1016/j.inffus.2024.102472](https://doi.org/10.1016/j.inffus.2024.102472). URL [https://www.sciencedirect.com/science/article/pii/S1566253524002501](https://www.sciencedirect.com/science/article/pii/S1566253524002501).
*   Nasarian et al. [2024] Elham Nasarian, Roohallah Alizadehsani, U.Rajendra Acharya, and Kwok-Leung Tsui. Designing interpretable ml system to enhance trust in healthcare: A systematic review to proposed responsible clinician-ai-collaboration framework. *Information Fusion*, 108:102412, 2024. ISSN 1566-2535. doi:[10.1016/j.inffus.2024.102412](https://doi.org/10.1016/j.inffus.2024.102412). URL [https://www.sciencedirect.com/science/article/pii/S1566253524001908](https://www.sciencedirect.com/science/article/pii/S1566253524001908).
*   Gao et al. [2024a] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024a. URL [https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997).
*   Liu [2022] Jerry Liu. LlamaIndex, 11 2022. URL [https://github.com/jerryjliu/llama_index](https://github.com/jerryjliu/llama_index). [Accessed: August 26, 2024].
*   Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629).
*   Díaz-Rodríguez et al. [2023] Natalia Díaz-Rodríguez, Javier Del Ser, Mark Coeckelbergh, Marcos López de Prado, Enrique Herrera-Viedma, and Francisco Herrera. Connecting the dots in trustworthy artificial intelligence: From ai principles, ethics, and key requirements to responsible ai systems and regulation. *Information Fusion*, 99:101896, 2023. ISSN 1566-2535. doi:[10.1016/j.inffus.2023.101896](https://doi.org/10.1016/j.inffus.2023.101896). URL [https://www.sciencedirect.com/science/article/pii/S1566253523002129](https://www.sciencedirect.com/science/article/pii/S1566253523002129).
*   Chowdhury et al. [2023] MD Minhaz Chowdhury, Nafiz Rifat, Mostofa Ahsan, Shadman Latif, Rahul Gomes, and Md Saifur Rahman. Chatgpt: A threat against the cia triad of cyber security. In *2023 IEEE International Conference on Electro Information Technology (eIT)*, pages 1–6, 2023. doi:[10.1109/eIT57321.2023.10187355](https://doi.org/10.1109/eIT57321.2023.10187355).
*   Gehman et al. [2020] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Trevor Cohn, Yulan He, and Yang Liu, editors, *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages 3356–3369, Online, nov 2020\. Association for Computational Linguistics. doi:[10.18653/v1/2020.findings-emnlp.301](https://doi.org/10.18653/v1/2020.findings-emnlp.301). URL [https://aclanthology.org/2020.findings-emnlp.301](https://aclanthology.org/2020.findings-emnlp.301).
*   Goodfellow et al. [2015] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples, 2015. URL [https://arxiv.org/abs/1412.6572](https://arxiv.org/abs/1412.6572).
*   Liu et al. [2024] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. Prompt injection attack against llm-integrated applications, 2024. URL [https://arxiv.org/abs/2306.05499](https://arxiv.org/abs/2306.05499).
*   Rebedea et al. [2023] Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar, Christopher Parisien, and Jonathan Cohen. NeMo guardrails: A toolkit for controllable and safe LLM applications with programmable rails. In Yansong Feng and Els Lefever, editors, *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 431–445, Singapore, dec 2023\. Association for Computational Linguistics. doi:[10.18653/v1/2023.emnlp-demo.40](https://doi.org/10.18653/v1/2023.emnlp-demo.40). URL [https://aclanthology.org/2023.emnlp-demo.40](https://aclanthology.org/2023.emnlp-demo.40).
*   Dong et al. [2024b] Yi Dong, Ronghui Mu, Yanghao Zhang, Siqi Sun, Tianle Zhang, Changshun Wu, Gaojie Jin, Yi Qi, Jinwei Hu, Jie Meng, Saddek Bensalem, and Xiaowei Huang. Safeguarding large language models: A survey, 2024b. URL [https://arxiv.org/abs/2406.02622](https://arxiv.org/abs/2406.02622).
*   Zhu et al. [2023b] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Interpretable gradient-based adversarial attacks on large language models, 2023b. URL [https://arxiv.org/abs/2310.15140](https://arxiv.org/abs/2310.15140).
*   Dong et al. [2024c] Ximing Dong, Dayi Lin, Shaowei Wang, and Ahmed E. Hassan. A framework for real-time safeguarding the text generation of large language model, 2024c. URL [https://arxiv.org/abs/2404.19048](https://arxiv.org/abs/2404.19048).
*   Li et al. [2023] Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, and Yangqiu Song. Privacy in large language models: Attacks, defenses and future directions, 2023. URL [https://arxiv.org/abs/2310.10383](https://arxiv.org/abs/2310.10383).
*   Kirchenbauer et al. [2024] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models, 2024. URL [https://arxiv.org/abs/2301.10226](https://arxiv.org/abs/2301.10226).
*   Nicolae et al. [2019] Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Ian M. Molloy, and Ben Edwards. Adversarial robustness toolbox v1.0.0, 2019. URL [https://arxiv.org/abs/1807.01069](https://arxiv.org/abs/1807.01069).
*   Dubey et al. [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, and Aiesha Letman et al. The llama 3 herd of models, 2024. URL [https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783).
*   Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. URL [https://arxiv.org/abs/2307.16789](https://arxiv.org/abs/2307.16789).
*   Händler [2023] Thorsten Händler. A taxonomy for autonomous llm-powered multi-agent architectures. In *Proceedings of the 15th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - KMIS*, pages 85–98\. INSTICC, SciTePress, 2023. ISBN 978-989-758-671-2. doi:[10.5220/0012239100003598](https://doi.org/10.5220/0012239100003598).
*   Huang et al. [2020] Dongyan Huang, Xiaoli Ma, and Shengli Zhang. Performance analysis of the raft consensus algorithm for private blockchains. *IEEE Transactions on Systems, Man, and Cybernetics: Systems*, 50(1):172–181, 2020. doi:[10.1109/TSMC.2019.2895471](https://doi.org/10.1109/TSMC.2019.2895471).
*   Ongaro and Ousterhout [2014] Diego Ongaro and John Ousterhout. In search of an understandable consensus algorithm. In *2014 USENIX Annual Technical Conference (USENIX ATC 14)*, pages 305–319, Philadelphia, PA, jun 2014\. USENIX Association. ISBN 978-1-931971-10-2. URL [https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro](https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro).
*   Slisko and Dykstra Jr [1997] Josip Slisko and Dewey I Dykstra Jr. The role of scientific terminology in research and teaching: is something important missing? *Journal of Research in Science Teaching: The Official Journal of the National Association for Research in Science Teaching*, 34(6):655–660, 1997.
*   Liu et al. [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+p: Empowering large language models with optimal planning proficiency, 2023. URL [https://arxiv.org/abs/2304.11477](https://arxiv.org/abs/2304.11477).
*   [65] Toran Bruce Richards. AutoGPT. URL [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT). [Accessed: September 26, 2024].
*   Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In *Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology*, UIST ’23, New York, NY, USA, 2023\. Association for Computing Machinery. ISBN 9798400701320. doi:[10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763). URL [https://doi.org/10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763).
*   Li et al. [2024] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, and Yunxin Liu. Personal llm agents: Insights and survey about the capability, efficiency and security, 2024. URL [https://arxiv.org/abs/2401.05459](https://arxiv.org/abs/2401.05459).
*   Gao et al. [2024b] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun Ren. Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38, pages 18030–18038, Mar. 2024b. doi:[10.1609/aaai.v38i16.29759](https://doi.org/10.1609/aaai.v38i16.29759). URL [https://ojs.aaai.org/index.php/AAAI/article/view/29759](https://ojs.aaai.org/index.php/AAAI/article/view/29759).
*   Tang et al. [2023] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases, 2023. URL [https://arxiv.org/abs/2306.05301](https://arxiv.org/abs/2306.05301).
*   Patil et al. [2023] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis, 2023. URL [https://arxiv.org/abs/2305.15334](https://arxiv.org/abs/2305.15334).
*   Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In *Proceedings of the 6th International Conference on Learning Representations - ICLR 2018 Conference*, 2018.
*   [72] International Business Machines Corporation. IBM Privacy Statement. URL [https://www.ibm.com/us-en/privacy](https://www.ibm.com/us-en/privacy). [Accessed: September 26, 2024].