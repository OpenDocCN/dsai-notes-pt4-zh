<!--yml
category: 未分类
date: 2025-01-11 12:53:54
-->

# Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications

> 来源：[https://arxiv.org/html/2402.09015/](https://arxiv.org/html/2402.09015/)

Negar Arabzadeh${}^{1}$  Julia Kiseleva${}^{2}$  Qingyun Wu${}^{3}$  Chi Wang${}^{2}$  
Ahmed Awadallah${}^{2}$  Victor Dibia${}^{2}$  Adam Fourney${}^{2}$  Charles Clarke${}^{1}$
${}^{1}$Univerity of Waterloo
${}^{2}$Microsoft Research
${}^{3}$Pennsylvania State University __Work done during an internship at Microsoft Research

###### Abstract

The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application’s functionality and end-user needs. We introduce AgentEval¹¹1[https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb) provides an implementation for the math problems, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness AgentEval for two open source datasets.

## 1 Introduction

The rapid development of open-source libraries Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47)); Li et al. ([2023a](https://arxiv.org/html/2402.09015v3#bib.bib23)) that aims to simplify the development of LLM-powered agentic solutions for various user-enteric tasks has led to the rapid growth of such applications Liang et al. ([2023b](https://arxiv.org/html/2402.09015v3#bib.bib28)); Hong et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib14)); Talebirad and Nadiri ([2023](https://arxiv.org/html/2402.09015v3#bib.bib39)). One of the long-lasting goals Winograd ([1972](https://arxiv.org/html/2402.09015v3#bib.bib46)) is the ability to seamlessly interact with humans in natural language to help end-users and to make their lives easier by assisting with their tasks from math tutoring to completing household tasks and so on. End users have expectations and requirements for a developed application that need to be met. This understanding is essential to assess the *utility* it brings and, consequently, to further improve and align the application towards end-users’ goals.

Directly evaluating agentic systems poses challenges as current approaches predominantly rely on end-to-end success metrics – essentially, whether the agent accomplishes tasks Shridhar et al. ([2020b](https://arxiv.org/html/2402.09015v3#bib.bib38), [2019](https://arxiv.org/html/2402.09015v3#bib.bib36)); Myers et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib33)). However, comprehending user interaction with an application involves much more than success alone Kiseleva et al. ([2022a](https://arxiv.org/html/2402.09015v3#bib.bib19), [b](https://arxiv.org/html/2402.09015v3#bib.bib20)); Zhang et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib49)). Take math problems, for instance; it is not merely about the agent solving the problem. Equally significant is its ability to present solutions based on various criteria, including completeness, conciseness, and clarity of the explanation provided. In other words, in a code completion scenario, even an incomplete code suggestion can be useful when it provides substantial boilerplate code or proposes a framework to solve a task Dibia et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib9)). Furthermore, success is not always clearly defined for every task. Knowing such criteria for an LLM-powered application and being able to quantify them is essential to verify whether user requirements are being satisfied, in other words, if the application brings utility to the end-users. Given the objective of verifying arbitrary applications, reliance on a benchmarking approach is untenable due to the expansive range of tasks requiring automation. A prerequisite is a scalable and flexible methodology capable of accommodating a diverse set of applications.

![Refer to caption](img/450c4d396cb163b9e92ef0a8f7ccdfc9.png)

Figure 1: An overview of the *AgentEval* framework consists of two main components: (C) *CriticAgent*, which learns a list of $n$ criteria ($C=\{c_{1},\dots,c_{n}\}$) and suggested values for each criterion ($c_{i}:\{\omega_{j}\}_{j=1}^{m}$), where $m$ is the number of suggested values, applicable to an arbitrary application that can be assessed by a domain expert; and (Q) *QuantifierAgent*, which verifies a set of suggested criteria for a considered application and suggests a task utility for an end-user ($U_{t}(s)=\{Q_{i}(s|c_{i})\}_{i=1}^{n}$)

In this work, we aim to introduce the AgentEval framework, a tool crafted to swiftly gauge the utility of LLM-powered agentic applications designed to help end-users accomplish their desired tasks. The goal of AgentEval is to assess the current alignment between application behavior and user goals, providing application developers with insights into how and what aspects of the current flow can be improved. AgentEval takes into account recent discoveries that have shown LLMs to emerge as a scalable and cost-effective alternative to human evaluations for open-ended tasks Li et al. ([2023b](https://arxiv.org/html/2402.09015v3#bib.bib24)). The *AgentEval* is illustrated in Fig. [1](https://arxiv.org/html/2402.09015v3#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), which consists of two main agents executed consecutively. These agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47))²²2[https://github.com/microsoft/autogen](https://github.com/microsoft/autogen):

*   •

    *CriticAgent* suggests the list of criteria based on the task description and the suggested solutions, e.g. for math problems can be *Efficiency* of the proposed solution and *Clarity* of the proposed solution;

*   •

    *QuantifierAgent* verifies how well the solution $s$ produced by an agentic system designed for task $t$ are performing for each criterion and returns the utility function, e.g. what is the Clarity level of the solution, not clear, moderately clear or very clear.

We believe that the usage of *AgentEval* can extend beyond the immediate verification of the current performance of LLM-powered applications. The framework can be employed over time to uncover new capabilities of the system and potential changes in task utility for the end user. The discovered utility function can be utilized to optimize the system towards user needs or system developer requirements, and this optimization can occur over time.

In summary, our main contributions are:

1.  C1

    A definition of task utility that enables access to the possible requirements an end-user may have regarding an LLM-powered application and how well the application satisfies this list of criteria;

2.  C2

    An introduction of *AgentEval*, a novel framework that leverages LLM-powered agents as a scalable and cost-effective alternative to human evaluations to produce task utility through the collaboration of two agents: *CriticAgent* proposes a list of criteria based on the task description and the successful and failed execution of the agent, and *QuantifierAgent* assesses how well the list of criteria is supported by the current implementation of an application;

3.  C3

    An in-depth analysis of the *AgentEval* robustness on various tasks and datasets across different solutions that can be replicated for a newly unseen domain.

The remainder of this paper is organized as follows. Section [2](https://arxiv.org/html/2402.09015v3#S2 "2 Related Work ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") describes earlier work and background. We provide the motivation behind AgentEval and define the utility of the task in Section [3](https://arxiv.org/html/2402.09015v3#S3 "3 Defining Task Utility ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"). Section [4](https://arxiv.org/html/2402.09015v3#S4 "4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") gives an overview of the datasets, namely MATH Hendrycks et al. ([2021b](https://arxiv.org/html/2402.09015v3#bib.bib13)) and ALFWorld Shridhar et al. ([2020b](https://arxiv.org/html/2402.09015v3#bib.bib38)), and the solutions for building applications powered by LLM utilized in our work. Section [5](https://arxiv.org/html/2402.09015v3#S5 "5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") demonstrates our findings about applying AgentEval to assess the utility of the task for the selected datasets. Section [6](https://arxiv.org/html/2402.09015v3#S6 "6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") presents an in-depth analysis of the robustness of AgentEval, namely *CriticAgent* robustness (Section [6.1](https://arxiv.org/html/2402.09015v3#S6.SS1 "6.1 Task-based vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")), *QuantifierAgent* robustness (Section [6.2](https://arxiv.org/html/2402.09015v3#S6.SS2 "6.2 Quantifier Agent Robustness ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")) and an automatic verification of *QuantifierAgent* (Section [6.3](https://arxiv.org/html/2402.09015v3#S6.SS3 "6.3 QuantifierAgent Verification ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")).

## 2 Related Work

We build on the streams of prior work. First, we will discuss the list of benchmarks and approaches to evaluate general LLMs in general (Section [2.1](https://arxiv.org/html/2402.09015v3#S2.SS1 "2.1 LLM evaluation ‣ 2 Related Work ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")). Second, we will present approaches to understand and predict user utility functions (Section [2.2](https://arxiv.org/html/2402.09015v3#S2.SS2 "2.2 User satisfaction prediction ‣ 2 Related Work ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")). Third, we will go over the current tendencies in using LLMs as evaluators in Section [2.3](https://arxiv.org/html/2402.09015v3#S2.SS3 "2.3 Using LLMs as evaluators ‣ 2 Related Work ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications").

### 2.1 LLM evaluation

There exists a significant volume of literature dedicated to assessing Language Model Models (LLMs), as evidenced by extensive research efforts Guo et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib11)); Ziyu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib50)); Chang et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib6)); Liang et al. ([2023a](https://arxiv.org/html/2402.09015v3#bib.bib27)). LLMs have been evaluated from various aspects including, but not limited to, specialized LLMs Jin et al. ([2019](https://arxiv.org/html/2402.09015v3#bib.bib16)), ethics and morality Hendrycks et al. ([2021a](https://arxiv.org/html/2402.09015v3#bib.bib12)), safety and robustness Wang et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib42)), and knowledge and reasoning Bian et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib5)). Additionally, recent developments include the introduction of intricate multi-modal benchmark datasets Mialon et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib32)); Bang et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib3)). Furthermore, there are attempts to evaluate LLMs as agents Liu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib30)).

However, there is a lack of literature that focuses on the holistic verification of the utility of LLMs for end-users in solving their tasks, which we tackle in this work.

### 2.2 User satisfaction prediction

Recent studies suggest that users interacting with various systems operate with specific utility functions in mind Li et al. ([2020](https://arxiv.org/html/2402.09015v3#bib.bib25)); Azzopardi et al. ([2018](https://arxiv.org/html/2402.09015v3#bib.bib2)); Ahmadvand et al. ([2022](https://arxiv.org/html/2402.09015v3#bib.bib1)). Traditionally, metrics defining user satisfaction was built based on various at scale collected behavioral signals Kiseleva et al. ([2014](https://arxiv.org/html/2402.09015v3#bib.bib17)) and they were tailored to specific applications, such as intelligent assistants Kiseleva et al. ([2016a](https://arxiv.org/html/2402.09015v3#bib.bib21), [b](https://arxiv.org/html/2402.09015v3#bib.bib22)), web search engines Williams et al. ([2016a](https://arxiv.org/html/2402.09015v3#bib.bib43), [b](https://arxiv.org/html/2402.09015v3#bib.bib44)); Williams and Zitouni ([2017](https://arxiv.org/html/2402.09015v3#bib.bib45)), dialogue systems See et al. ([2019](https://arxiv.org/html/2402.09015v3#bib.bib35)), multi-turn conversations Li et al. ([2021](https://arxiv.org/html/2402.09015v3#bib.bib26)) and general-purpose personal assistants Kiseleva and de Rijke ([2017](https://arxiv.org/html/2402.09015v3#bib.bib18)).

### 2.3 Using LLMs as evaluators

Moreover, there is a growing trend in utilizing LLMs as evaluates Chiang and Lee ([2023](https://arxiv.org/html/2402.09015v3#bib.bib7)); Fu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib10)) for qualitative research Bano et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib4)) and adopting LLMs as proxies for human behavior Tjuatja et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib40)); Liu and Sun ([2023](https://arxiv.org/html/2402.09015v3#bib.bib29)). Jain et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib15)) studied efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by LLMs. Notably, CoEval Li et al. ([2023b](https://arxiv.org/html/2402.09015v3#bib.bib24)) has recently demonstrated the synergy between human evaluation and LLMs in establishing evaluation criteria and conducting multi-dimensional evaluations for openended NLG tasks.

Building on top of these works, we propose a framework capable of assessing the utility of various LLM-powered applications at scale. This framework aims to align agentic systems with human preferences.

## 3 Defining Task Utility

![Refer to caption](img/47e7c4dd05dbf4da0c5024ad6c60dab3.png)

Figure 2: The taxonomy of task assessments based on optimal solutions existence

It is important to begin by considering the categories of tasks around which we focus LLM-powered applications. Fig. [1](https://arxiv.org/html/2402.09015v3#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") outlines a taxonomy of target tasks for agentic systems in terms of success metrics. At the top level, tasks can be split into two main categories, where:

*   •

    *Success is not clearly defined* — For these tasks, users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task end-to-end. For example, a user might request the system to generate an email based on some user input. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is less well-defined. In case of online evaluation, while being expensive, we can ask users to what extend the the assistance was helpful. Although quantifying to what extent the help was useful is still challenging on its own, the problem becomes increasingly challenging when it comes to offline evaluation, or evaluation in novel scenarios before we have users.

*   •

    *Success is clearly defined* — For these tasks, we can clearly determine whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable.

This second category can be further divided into two subcategories:

*   [leftmargin=*, nosep]

*   •

    *Success is clearly defined and an optimal solution exits* — For these tasks, only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.

*   •

    *Success is clearly defined and multiple solutions exist* — increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful outcomes. For example, when you ask the agent to suggest a food recipe or tell you a joke, you may define success as the food tasting good or the joke being funny, but perhaps the recipe should not be too expensive to prepare, and the joke should not be offensive.

In our AgentEval framework, we are currently focused on tasks where success is clearly defined and multiple successful solutions may exist.

Our previous research on assistive agents suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences Kiseleva et al. ([2022b](https://arxiv.org/html/2402.09015v3#bib.bib20)). In this setup of pairwise comparison, humans can develop a list criteria to explain why they prefer the behavior of one agent over another. For instance, ‘the first agent was faster in execution’ or ‘the second agent moves more naturally’. Therefore, the comparative nature guided humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. [1](https://arxiv.org/html/2402.09015v3#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. The AgentEval Framework employs two types of agents, namely:

*   [leftmargin=*, nosep]

*   •

    The goal of a *CriticAgent* is to suggest a list of criteria that can be used to assess task utility for end users. The critic is given a task description as well as a list of few successful and failed examples of the task execution; then it is able to return a list of criteria: $C=\{c_{1},\dots,c_{n}\}$ where each criterion $c_{i}$ is accompanied with a set of accepted values $\omega$ as $c_{i}:\{\omega_{j}\}_{j=1}^{m}$. For example, The *CriticAgent* generated the criteria such as Clarity, Efficiency, and more as described in the Tab. [1](https://arxiv.org/html/2402.09015v3#S4.T1 "Table 1 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") for solving math problems. Each of the Criterion would be accompanied with a set of accepted values as shown in this Table as an example.

*   •

    The goal of *QuantifierAgent* is to quantify each of the suggested criteria to access the task utility $U_{t}$ for the end user in the following form: $U_{t}(s)=\{Q_{i}(s|c_{i})\}_{i=1}^{n}$, providing us with an idea of the utility of this system for the given task for the end user. In which $s$ represent the task sample and $Q(s|c_{i}.)$ is the quantifier output for sample $s$ based on the criterion $c_{i}$. For example, for one sample of math problem solving, and given the generated criteria shown in Tab. [1](https://arxiv.org/html/2402.09015v3#S4.T1 "Table 1 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") the solution’s Accuracy could be quantified as “Incorrect”, “partially correct” or “correct”. Eligible quantified values for quantification process are shown in “Accepted values” column in Tab. [1](https://arxiv.org/html/2402.09015v3#S4.T1 "Table 1 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")

Next we will discuss the datasets and baselines we use to test the work of AgentEval.

## 4 Datasets and Solutions

Table 1: Verification Criteria for MathProblems

 | Criteria | Description |  
Accepted Values

 |
| --- | --- | --- |
| Clarity | The ease of understanding the steps, explanations, and language used in the solution. | – Not Clear (0) – Moderately Clear (1) – Very Clear (2) |
| Efficiency | The use of optimal methods or approaches to solve the math problem. | – Inefficient (0) – Moderately Efficient (1) – Efficient (2) |
| Error Analysis | The identification and description of possible errors or misconceptions in the math problem-solving process. | – Not Addressed (0) – Partially Addressed (1) – Well Addressed (2) |
| Completeness | Quality of code in terms of efficiency and elegance | – Incomplete (0) – Mostly Complete (1) – Complete (2) | 

This section provides an overview of the datasets utilized in our study. Our selection encompasses a variety of datasets, from those based on real-world problems to their simulations and beyond. The Mathematics Dataset (Section [4.1](https://arxiv.org/html/2402.09015v3#S4.SS1 "4.1 MATH Problem Solving ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")) is chosen for its widespread usage and comprehensive understanding in the field. It represents complex problem-solving scenarios that are fundamental in evaluating the effectiveness of multi-agent systems. AlfWorld (Section [4.2](https://arxiv.org/html/2402.09015v3#S4.SS2 "4.2 ALFWorld Household Task ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")) offers a scenario involving multi-turn interactions within a moderately approximated multi-modal environment. This dataset is instrumental in assessing agents’ performance in interactive and dynamic settings.

Each dataset plays a critical role in evaluating different aspects of AgentEval’s capabilities, from handling complex theoretical problems to navigating real-world scenarios. In both tasks, although success is clearly defined, multiple solutions exist for accomplishing the objectives. For example, when solving a math problem, there are various approaches one can take. Similarly, in the Alfworld dataset, which involves household tasks, there are multiple ways to complete them based on how you search for objects and the thinking strategies you employ, among other factors. An example of Math problem solving and AlfWorld task are shown in Appendix [A.1](https://arxiv.org/html/2402.09015v3#A1.SS1 "A.1 Task Examples ‣ Appendix A Appendix ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications").

![Refer to caption](img/f664e1345389bfc424ae9f47042c193e.png)

Figure 3: (a) AgentEval assessment of three different solutions on math problem solving task categorized (b) Same assessment categorized by success and failed cases

### 4.1 MATH Problem Solving

The MATH dataset, originally is a substantial collection of 12,500 challenging mathematics problems from high school competitions Hendrycks et al. ([2021b](https://arxiv.org/html/2402.09015v3#bib.bib13)). Each problem comes with a step-by-step solution, enabling models to learn how to generate both derivations and explanations. The dataset covers a wide range of mathematical subjects and is tagged by difficulty levels, offering a nuanced measure of model performance across various aspects of mathematical problem-solving.

This dataset is particularly suitable for testing multi-agent systems for several reason including: (i) The problems in the MATH dataset are not simple computations but require a deep understanding of mathematical concepts, heuristics, and problem-solving strategies. (ii) Since the dataset includes step-by-step solutions, it allows for the assessment of an agent’s ability to learn and reason through a problem, not just its ability to arrive at the correct answer. (iii) The variety of subjects and difficulty levels in the MATH dataset enables a comprehensive evaluation of a system’s versatility and adaptability in different mathematical domains which is crucial for multi-agent systems that are expected to operate across a range of scenarios.

Similar to the math problem experimental setup as in Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47)), we carry out two experimental evaluations which involves 120 problems from level-5, the most challenging category, and includes 20 problems each from six different categories, of number theory, counting and probability, prealgebra, algebra, intermediate algebra, and precalculus.

Solutions: In establishing a solution for this task, we draw inspiration from the experiments showcased in Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47)). We evaluate the proposed methodology by AutoGen Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47)), as well as Langchain ReAct ³³3[https://python.langchain.com/en/latest/index.html](https://python.langchain.com/en/latest/index.html) and a Vanilla solver that employs gpt-4 to tackle the task. These solutions approaches have previously demonstrated promising performance in solving mathematical problems, particularly on the dataset at hand Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47)). We assess and compare the performance of these three solutions using AgentEval. Fig. [9](https://arxiv.org/html/2402.09015v3#A1.F9 "Figure 9 ‣ A.1 Task Examples ‣ Appendix A Appendix ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") displays an example of one math problem from prealgebra category as well as the solution created by AutoGen. In Section [5.1](https://arxiv.org/html/2402.09015v3#S5.SS1 "5.1 AgentEval for Math Problems ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), we delve into how AgentEval would perform on math problem solving tasks and how the measured performance with AgentEval correlates with the ground truths.

### 4.2 ALFWorld Household Task

ALFWorld, presents a set of language-based interactive decision-making tasks within simulated household environments Shridhar et al. ([2020b](https://arxiv.org/html/2402.09015v3#bib.bib38)). This benchmark is distinguished by its diversity of tasks, offering a comprehensive platform for testing AI and multi-agent systems. This benchmark is particularly suited for such evaluations because first, ALFWorld is the first interactive parallel environment that aligns text descriptions and commands with physically embodied robotic simulation. It extends two prior works: TextWorld, an engine for interactive text-based games, and ALFRED, a large-scale dataset for vision-language instruction following in embodied environments Shridhar et al. ([2020a](https://arxiv.org/html/2402.09015v3#bib.bib37)); Côté et al. ([2019](https://arxiv.org/html/2402.09015v3#bib.bib8)). The cross-modality framework of this benchmark allows for a variety of embodied tasks with corresponding text-based counterparts, allowing agents to be trained and evaluated in both the language and embodied world. In addition, ALFWorld supports the development of agents that can reason both abstractly and execute actions concretely, mimicking human-like decision-making processes in varying contexts. Finally, the dataset’s inclusion of a wide range of tasks, from household chores to more intricate problem-solving scenarios, provides a comprehensive testbed for evaluating the adaptability and problem-solving capabilities of AI and multi-agent systems. In general, the dataset allows agents to explore, interact, and learn in an abstract language environment before dealing with the complexities of an embodied environment.

Solutions: As for the solutions to solve ALFWorld Household tasks, similar to Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47)), we consider ReAct Yao et al. ([2022](https://arxiv.org/html/2402.09015v3#bib.bib48)) as well as AutoGen with two agents and AutoGen with three agents Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47)). ReAct is an agent that operates within the ALFWorld environments and is responsible for suggesting plans and executing actions. On the other hand, AutoGen Two-Agent System consists of an LLM-backed assistant agent responsible for suggesting plans, and an executor agent responsible for executing actions in the ALFWorld environments. Both ReAct and this solution occasionally struggles with leveraging basic commonsense knowledge about the physical world, which can lead to repetitive errors and getting stuck in loops. In AutoGen with three agents, a grounding agent is provided just for the sake of critical common sense knowledge whenever the system exhibits early signs of recurring errors. We assess and compare the performance of these three solutions using AgentEval. Fig. [10](https://arxiv.org/html/2402.09015v3#A1.F10 "Figure 10 ‣ A.1 Task Examples ‣ Appendix A Appendix ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") displays a portion of an example of AlfWorld Householding task solved by AutoGen.

## 5 AgentEval Workflow

This Section outlines the workflow of the AgentEval illustrated in Fig. [1](https://arxiv.org/html/2402.09015v3#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"). Next, we will demonstrate how AgentEval works based on 3 different datasets: Math Problems (Sec. [4.1](https://arxiv.org/html/2402.09015v3#S4.SS1 "4.1 MATH Problem Solving ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")) and AlfWorld (Sec. [4.2](https://arxiv.org/html/2402.09015v3#S4.SS2 "4.2 ALFWorld Household Task ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")).

### 5.1 AgentEval for Math Problems

#### Critic and Quantifier Findings

After executing the CriticAgent, we have obtained a set of criteria for validating the results of the mathematical problem presented in Table [1](https://arxiv.org/html/2402.09015v3#S4.T1 "Table 1 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"). Subsequently, the *QuantifierAgent* is tasked with quantifying each criterion based on accepted values. In Figure [3](https://arxiv.org/html/2402.09015v3#S4.F3 "Figure 3 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") (a), we present the outcome of *QuantifierAgent* i.e., the measured performance of three solutions on this task. This visual representation of the AgentEval output reveals some intriguing insights. Notably, it is evident that Agenteval do not quantify the three solutions as if they are equally performing well across different criteria. For instance, while all three solutions leverage GPT-4 as the underlying language model, Autogen outperforms ReAct and Vanilla GPT-4 in terms of accuracy. This observation extends to solution completeness and efficiency as well. Conversely, when considering the criterion of Clarity, all three approaches exhibit more competitive performance.

As depicted in this figure, the error analysis range of quantified values differs from other metrics. To gain a better understanding of this criterion, we further scrutinize the results by categorizing them into successful and failed cases, as illustrated in Fig. [3](https://arxiv.org/html/2402.09015v3#S4.F3 "Figure 3 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") (b). While AutoGen, Vanilla Solver and ReAct solution are each presented in orange, blue and green respectively, the darker bars represent the performance on successful cases and lighter bars represent the failed cases performance. The difference between the dark and light bar of each color, verify the AgentEval performance as we expect that each positive criteria should be quantifier higher for successful cases compared to their failed cases. We observe that in most cases, the successful and failed cases are distinguished even with 95% interval confidence on all the success and failed cases.

We delve further into the differences between successful cases among the three solutions and failed cases among the solutions. One interesting observation from this Figure is that not all successful cases are identical, and similarly, not all failed cases are the same. The difference between successful cases among the three solutions is smaller than the differences between their failed cases. For instance, Autogen’s failed cases exhibit higher efficiency and completeness compared to the Vanilla gpt-4 solver. This observation provides us with valuable additional insights.

Table 2: Verification Criteria for AlfWorld Housholding Tasks.

 | Criteria | Description |  
Accepted Values

 |
| --- | --- | --- |
| Task Understanding | How well the participant was able to comprehend the problem set and follow the task instructions | – Excellent (4) – Good (3) – Average (2) – Poor (1) – Terrible (0) |
| Plan Making | The ability of the participant to strategize and make a plan for tackling the task. | – Excellent (4) – Good (3) – Average (2) – Poor (1) – Terrible (0) |
| Action Decision | The participant’s decision-making skills in choosing the right action to perform. | – Excellent (4) – Good (3) – Average (2) – Poor (1) – Terrible (0) |
| Action Execution | How effectively the participant is able to execute the chosen action. | – Excellent (4) – Good (3) – Average (2) – Poor (1) – Terrible (0) |
| Response to Feedback | How well the participant adapts his/her next steps based on the feedback from the environment | – Excellent (4) – Good (3) – Average (2) – Poor (1) – Terrible (0) |
| Correctness of Action | The correctness of the action performed by the participant with respect to the available actions and the current context | – Correct (1) – Incorrect (0) |
| Use of Terminate | Whether the participant uses the ’TERMINATE’ command appropriately | – Appropriate (1) – Inappropriate (0) | 

### 5.2 AgentEval for AlfWorld

#### Critic and Quantifier Finding

In this section, we provide an example of AgentEval applied to the AlfWorld Householding task, as mentioned in Sec. [5.1](https://arxiv.org/html/2402.09015v3#S5.SS1 "5.1 AgentEval for Math Problems ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") in which real-world household environments is emulated through textual interfaces Shridhar et al. ([2020b](https://arxiv.org/html/2402.09015v3#bib.bib38)). When running the *CriticAgent* on this task, it identified specific criteria such as “Task understanding”, “Plan making” and “Response to Feedback” as outlined in Tab. [2](https://arxiv.org/html/2402.09015v3#S5.T2 "Table 2 ‣ Critic and Quantifier Findings ‣ 5.1 AgentEval for Math Problems ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"). We consulted researchers deeply involved with these tasks, and their expertise confirmed that these criteria are critically relevant and significant similar to Li et al. ([2023b](https://arxiv.org/html/2402.09015v3#bib.bib24)). For example, given that these tasks are language-based and require interactive decision-making, an agent in ALFWorld is tasked with high-level objectives, such as placing a hot apple in the fridge, and must navigate and interact with a simulated household environment to achieve these objectives. Therefore, criteria displayed in Tab. [2](https://arxiv.org/html/2402.09015v3#S5.T2 "Table 2 ‣ Critic and Quantifier Findings ‣ 5.1 AgentEval for Math Problems ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") satisfy the assessment of this task. While the criteria are pretty self-descriptive, about the criterion “Use of TERMINATE” We note that the agent is prompted to use the term “TERMINATE” upon task completion, which is closely correlated with task success.

![Refer to caption](img/bcc224a45b7e5b8f7b1b10984f6066ef.png)

Figure 4: (a) AgentEval assessment of three different solutions on AlfWorld Householding Task (b) Same assessment categorized by success and failed cases.

Following the extraction of a set of criteria as detailed in Tab [2](https://arxiv.org/html/2402.09015v3#S5.T2 "Table 2 ‣ Critic and Quantifier Findings ‣ 5.1 AgentEval for Math Problems ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), these criteria are passed to the QuantifierAgent for quantification on each sample. Figure [4](https://arxiv.org/html/2402.09015v3#S5.F4 "Figure 4 ‣ Critic and Quantifier Finding ‣ 5.2 AgentEval for AlfWorld ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") presents the results for three introduced solutions: AutoGen with 2 agents, AutoGen with 3 agents, and ReAct, on the 134-test set from Wu et al. ([2023](https://arxiv.org/html/2402.09015v3#bib.bib47)). On the left side of Fig. [4](https://arxiv.org/html/2402.09015v3#S5.F4 "Figure 4 ‣ Critic and Quantifier Finding ‣ 5.2 AgentEval for AlfWorld ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), a Spider Figure illustrates the performance of these three solutions across all criteria. It is important to note that all criteria, except “Use of TERMINATE” and “Correctness of Action” employ a five-level grading system, while these two criteria are binary. From this figure, it is evident that ReACT performs notably worse across all criteria, while AutoGen with 2 agents and 3 agents demonstrate competitive performance. Notably, AutoGen with an additional common-sense grounding agent slightly outperforms others, particularly in the areas of Response to Feedback and Action Execution. Additionally, the barplot on the right side of Fig. [4](https://arxiv.org/html/2402.09015v3#S5.F4 "Figure 4 ‣ Critic and Quantifier Finding ‣ 5.2 AgentEval for AlfWorld ‣ 5 AgentEval Workflow ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") categorizes the 134 games into two groups: failed and successful, displaying the quantifier performance for each subgroup. Similar to Fig. [3](https://arxiv.org/html/2402.09015v3#S4.F3 "Figure 3 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), darker colors represent performance in successful cases for each solution, while lighter colors represent performance in failed cases. AutoGen 3-agent, AutoGen 2-agent, and ReAct are represented by blue, green, and orange, respectively. For most criteria, the distinction between failed and successful cases is clear, even within a 95% confidence interval. However, for certain criteria, such as “Task understanding” all solutions, whether they failed or succeeded, exhibit very similar performance. This could be interpreted as either (1) all solutions have a good understanding of the task, even if they fail to complete it, (2) this criterion may be redundant, as it does not provide additional information among these three solutions or (3) the *QuantifierAgent* is unable to score the criterion in a meaningful way. We refrain from concluding which criteria are most suitable for this specific task. Instead, we emphasize the importance of conducting a more in-depth analysis of performance beyond success rates, tailored to one’s goals and application requirements.

## 6 AgentEval Robustness Analysis and In-depth Discussion

This section presents the results of the analysis of how robust AgenEval is. First, we inspect if the list of criteria can be solely extracted from the task description (task-based criteria), and how the list of criteria can be changed by adding failed and successful samples from the data. Where we played with varies sample size to check its effect of the final list of criteria (Section [6.1](https://arxiv.org/html/2402.09015v3#S6.SS1 "6.1 Task-based vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")). Second, we focus on how can we estimate the robustness of the *QuantifierAgent* (Section [6.2](https://arxiv.org/html/2402.09015v3#S6.SS2 "6.2 Quantifier Agent Robustness ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")). We note that all the experiments reported in the paper are conducted with the temperature set at 0. Next, we will present our analysis using the MATH Problems dataset.

![Refer to caption](img/09162c595ed84892117ffd36142422ef.png)

Figure 5: Task based criteria vs solution based criteria for Math problems. show the 95% interval at each step

### 6.1 Task-based vs Solution-based criteria

#### General Hypothesis

We execute the CriticAgent using two distinct methods. The first method involves the Agent generating criteria solely based on the provided task description, which we refer to as “task-based” criteria. On the other hand, the CriticAgent could potentially derives criteria not only from a task description but also from examples of task solutions so called as “solution-based” criteria. In this context, our objective is to examine whether this approach leads to variations in the criteria formulated by agents. We believe this investigation is important to have a more clear vision of what criteria necessitate for having a promising assessment.

A solution to a mathematical problem, might probably satisfy criteria such as accuracy and clarity in any case, independent of what the solution is. However, when additional tools are being utilized to solve the problems, such as coding to solve math problems, additional criteria like ‘Code Efficiency’ may be introduced to the set of criteria. If one never considered solving the problem with a specific solution method like coding, they might not initially include such criterion. In summary, depending on whether the *CriticAgent* receives only a task description or both a task description and examples of solutions, we classify the criteria as either “task-based” or “solution-based”. Additionally, it is important to analyze whether the solution-based criteria overlap across different solutions and to what extent different solutions share these criteria.

To compare the differences between task-based and solution-based criteria, Fig. [5](https://arxiv.org/html/2402.09015v3#S6.F5 "Figure 5 ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") displays the number of unique criteria extracted for mathematical problem solving in task-based mode and three different solution-based approaches i.e., when the solutions come from AutoGen, ReAct and Vanilla Solver. To keep the balance between computational costs and analyzing the robustness, we conducted 50 runs of the CriticAgent with different seeds. Subsequently, for $N=50$ iterations, we randomly selected $M\in[1,50]$ samples ($M$ is shown on the x-axis of Fig. [5](https://arxiv.org/html/2402.09015v3#S6.F5 "Figure 5 ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications")) and present the average number of unique extracted criteria along with its 95% confidence interval after repeating this process 50 times. We note that because we obtained results from the *CriticAgent* in 50 iterations in total, the confidence intervals become smaller when $M$ get closer to the maximum number of samples i.e., $50$.

When examining the criteria, we have identified instances where certain criteria are quite similar but are expressed differently. These are essentially metrics that convey the same concept but are phrased with slight variations. In Table [3](https://arxiv.org/html/2402.09015v3#S6.T3 "Table 3 ‣ General Hypothesis ‣ 6.1 Task-based vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), we provide examples of such similarities along with their descriptions. In order to gain a deeper insight into the results presented in Figure [5](https://arxiv.org/html/2402.09015v3#S6.F5 "Figure 5 ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), we suggest consolidating these closely related criteria to determine the total number of unique criteria once again. This approach serves two purposes: 1\. It enhances our understanding of the actual number of unique criteria that have been extracted. 2\. It allows us to assess whether the repetitiveness and redundancy of criteria differ between solution-based and task-based criteria. By doing so, we can gain a better grasp of the data and draw more meaningful conclusions from our analysis.

Table 3: Pairs of similar criteria extracted for Math problem solving task.

 | - Problem Difficulty: The complexity of the math problem that has been solved. |
| - Problem Complexity: The level of difficulty of the problem. |
| - Innovativeness: The novelty and creativity in the approach to solve the problem |
| - Innovation: The ability to solve a problem using a unique or creative method not commonly known. |
| - Time Taken: The time taken to solve the problem. |
| - Time to Completion: The amount of time taken to solve the problem completely |
| - Understandability: The clarity and ease of comprehension of the solution provided. |
| - Readability: How easy it is to comprehend the provided solution. | 

![Refer to caption](img/d7b0063e1ffbaab91bdd1b3d4fe71f10.png)

Figure 6: Quantifier Robustness on criteria of Math Problem Solving problem. Each bar represent the average performance of success (dark blue "//") and failed (light blue “\\”) cases and 95% interval on each set is shaded across the average point. The two plots are overlaid.

In order to consolidate similar criteria, we draw inspiration from previous work Liu et al. ([2022](https://arxiv.org/html/2402.09015v3#bib.bib31)); Vahtola et al. ([2022](https://arxiv.org/html/2402.09015v3#bib.bib41)); Reimers and Gurevych ([2019](https://arxiv.org/html/2402.09015v3#bib.bib34)) which demonstrated that utilizing pre-trained language models fine-tuned for paraphrasing and semantic similarity can yield high performance in numerous downstream NLP tasks. Additionally, we employ a fine-tuned pre-trained language model specifically designed for paraphrasing, known as the Hugging Face Paraphrase MiniLM ⁴⁴4[https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2).

Our approach begins by encoding each criterion’s title and its description, followed by measuring pairwise similarity between all available criteria within our experiments. Subsequently, by employing a specified threshold value denoted as $\tau$, we classify pairs with higher cosine similarity between the embedded representations of each criterion pair as one and select one of them as the representative for that pair. This strategy is commonly employed in various NLP downstream tasks.

In Fig. [5](https://arxiv.org/html/2402.09015v3#S6.F5 "Figure 5 ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), we illustrate the outcomes of the number of unique extracted criteria using different threshold values, namely 0.7, 0.85, and 1\. A threshold of 1 implies that no criteria are filtered out.

#### Summary

In this section, we delved into various inputs and methods for extracting criteria. Our exploration compared the outcomes of task-based criteria, derived solely from task descriptions, with those of solution-based criteria, where the *CriticAgent* is exposed to both examples of solutions and the task description. We observed that solution-based methods produce a greater diversity of criteria compared to task-based methods. Furthermore, the diversity in the unique number of criteria varied even within solution-based methods, influenced by the model’s level of creativity. Additionally, we noticed a tendency for certain criteria to recur when running the *CriticAgent* multiple times. To address this, we suggest implementing consolidation techniques, such as merging synonymous terms, to eliminate redundant criteria."

### 6.2 Quantifier Agent Robustness

#### General Hypothesis

Here, we aim to investigate the robustness of the *QuantifierAgent* when applied repeatedly to the same set of criteria. Our goal is to assess the consistency of the results when quantifying the same set of criteria multiple times. This is of utmost importance as we expect the behavior of the quantifier to be stable and relatively free from noise when provided with a single sample and a fixed set of criteria. This stability is crucial for us to have confidence in the results. Additionally, this analysis can help us identify and filter out criteria that may not be sufficiently stable for reliable use.

To achieve this, we selected a specific subset of criteria related to mathematical problems, as detailed in Table [1](https://arxiv.org/html/2402.09015v3#S4.T1 "Table 1 ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), and conducted 50 runs of the quantifier agent on the 120 problems described in Section [4.1](https://arxiv.org/html/2402.09015v3#S4.SS1 "4.1 MATH Problem Solving ‣ 4 Datasets and Solutions ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"). Our expectation is to observe consistent quantified performance for each of the criteria. In Fig.  [6](https://arxiv.org/html/2402.09015v3#S6.F6 "Figure 6 ‣ General Hypothesis ‣ 6.1 Task-based vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), we present the distribution of quantified performance across 50 runs for both successful and failed cases, focusing on the five selected criteria. A consistently horizontal performance trend indicates greater robustness in the quantifier, whereas more fluctuations in the figure suggest less robustness and a noisier performance of the agent.

As shown in the results, for four out of the five generated criteria, we consistently observe steady performance. Not only do the success cases consistently outperform the failed cases, but their performance also falls within a similar range across runs. However, when it comes to the “error analysis” criterion, we observe a more variable performance of the quantifier. It does not consistently predict one group (success or failed) to perform better than the other, and the quantifier’s performance varies across different runs. This suggests that the AgentEval tool may not exhibit promising robustness for this particular criterion. The underlying issues could be either the criterion itself lacks clarity and appropriateness for the task, or the *QuantifierAgent* struggles to quantify this criterion effectively. In either case, it is advisable to either modify or eliminate this criterion to enhance trustworthiness and reliability.

Furthermore, we present the distribution of quantified values in Fig. [7](https://arxiv.org/html/2402.09015v3#S6.F7 "Figure 7 ‣ General Hypothesis ‣ 6.2 Quantifier Agent Robustness ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") using box plots, illustrating the distribution of quantifier values for both failed (dark blue) and successful cases (light blue) across all criteria. The box plots display the first and third quartiles of the distribution as well as the median. In this figure, robust criteria should exhibit a narrower range of quantifier performance (narrower box plots), and it should be easy to distinguish between the dark and light box plots for each criterion.

Consistently with our previous observations, all four criteria, except “error analysis” allow for easy differentiation between successful and failed cases. Additionally, some criteria prove to be more robust compared to others. For example, accuracy displays a narrower range of distribution, while clarity in failed cases covers a wider range. We believe that such an analysis of the quantifier agent’s performance will yield valuable insights for enhancing reliability, trustworthiness, and explainability in performance evaluation.

![Refer to caption](img/1eb93af55764afcb6e5c66889872f7dc.png)

Figure 7: Quantifier Robustness - Distribution of QuantifierAgent output on AutoGen results on 120 Math problems on Success (dark blue) and Failed (light blue) cases on terms of different criteria. The distributions demonstrates the same results as in Fig [6](https://arxiv.org/html/2402.09015v3#S6.F6 "Figure 6 ‣ General Hypothesis ‣ 6.1 Task-based vs Solution-based criteria ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications").

#### Summary

We recognize the importance of thoroughly investigating the robustness of each criterion in quantification studies. This analysis is crucial as it sheds light on the stability of each criterion. Moreover, when ground truths are available, such as in cases of success versus failure, they provide a benchmark to validate our assessments. Additionally, it’s important to acknowledge that not all criteria exhibit the same level of robustness. This variability demands careful consideration during evaluations, especially given the non-deterministic nature of LLMs. Such awareness is essential to ensure the reliability and accuracy of our assessments in the dynamic field of LLMs.

### 6.3 *QuantifierAgent* Verification

To assess the accuracy of quantifying each criterion, it is essential to verify the quantification process. Ideally, we would like to validate this process by comparing it with known pairwise samples, where we have definitive knowledge that for a given criterion $C$, sample $A$ is superior compared to sample $B$. The correct quantification should align with this knowledge. However, as the use of LLM-powered applications continues to expand daily, obtaining annotated data for many tasks is often impractical, if not impossible. Therefore, we propose employing synthetically altered versions of the samples to obtain the knowledge required for this verification.

Let us assume that we have an alternative disturbed version of sample $A$, which is called $A^{\prime}$. Assuming that the original sample $A$ outperforms the one with injected noise $A^{\prime}$, we anticipate that the criteria that assess sample quality will assign higher values to the original sample compared to the noisier variant in the same case. To carry out this validation, we conducted experiments involving mathematical problems. We introduce random noise into the solutions by removing a certain percentage of the solution sentences from Autogen’s results for the math problem solving dataset. For criteria such as “completeness” or “clarity”, we expect to observe greater completeness or clarity in the original solution as opposed to the one missing a portion of the solution.

In our study, our goal is to assess the *QuantifierAgent’s* ability to capture these distinctions between a known better solution and a worse one. We generated disturbed versions of solutions by randomly removing 25% of the sentences and running the quantifier over the noisy solutions. The results of these experiments are presented in Fig. [8](https://arxiv.org/html/2402.09015v3#S6.F8 "Figure 8 ‣ 6.3 QuantifierAgent Verification ‣ 6 AgentEval Robustness Analysis and In-depth Discussion ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"). As depicted in this figure, the criteria that captures the quality of the solutions such as “clarity” and “completeness” of the disturbed solutions decreased compared to the original ones. This observation helps establish confidence in the performance of *QuantifierAgent*.

![Refer to caption](img/5345e874fc877a17389fb3136f348afc.png)

Figure 8: Quantifier Verification on original set of solutions as well as the disturbed solutions on Math Problem Solving dataset.

## 7 Conclusions and Future Work

The rapid development of open-source libraries aiming to simplify the creation of Language Model Models (LLM)-powered agentic solutions for various user-centric tasks has facilitated the rapid growth of such applications. However, meeting end-users’ expectations and requirements for these applications is paramount, underscoring the importance of assessing the utility they provide. Directly evaluating agentic systems presents challenges, as current approaches often rely on end-to-end success metrics alone. However, understanding user interaction with an application entails more than just task success. Given the diverse range of tasks requiring automation, a scalable and flexible methodology is essential for evaluating these applications effectively.

In this work, we introduce the AgentEval framework, designed to swiftly gauge the utility of LLM-powered agentic applications for end-users. AgentEval aims to assess the alignment between application behavior and user goals, providing developers with insights into areas for improvement. The framework leverages recent findings suggesting LLMs as a scalable and cost-effective alternative to human evaluations for open-ended tasks. AgentEval consists of two agents: *CriticAgent* suggests criteria based on task descriptions and suggested solutions, while *QuantifierAgent* verifies how well the solutions align with these criteria. This framework is customizable, adaptable, and can operate in various modes, employing combinations of LLMs, human inputs, and tools. We believe that AgentEval’s utility extends beyond immediate performance verification. It can uncover new system capabilities over time and adapt to changes in user needs or developer requirements.

In summary, our contributions include defining task utility, introducing the AgentEval framework, and conducting a robust analysis of its performance across various datasets and solutions. AgentEval represents a significant step towards evaluating and optimizing LLM-powered applications to better serve end-users.

## Glossary

## References

*   Ahmadvand et al. (2022) Ali Ahmadvand, Negar Arabzadeh, Julia Kiseleva, Patricio Figueroa Sanz, Xin Deng, Sujay Jauhar, Michael Gamon, Eugene Agichtein, Ned Friend, et al. 2022. Supporting complex information-seeking tasks with implicit constraints. *arXiv preprint arXiv:2205.00584*.
*   Azzopardi et al. (2018) Leif Azzopardi, Paul Thomas, and Nick Craswell. 2018. Measuring the utility of search engine result pages: an information foraging based measure. In *The 41st International ACM SIGIR conference on research & development in information retrieval*, pages 605–614.
*   Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. [A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity](http://arxiv.org/abs/2302.04023).
*   Bano et al. (2023) Muneera Bano, Didar Zowghi, and Jon Whittle. 2023. Exploring qualitative research using llms. *arXiv preprint arXiv:2306.13298*.
*   Bian et al. (2023) Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and Ben He. 2023. Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. *arXiv preprint arXiv:2303.16421*.
*   Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. *ACM Transactions on Intelligent Systems and Technology*.
*   Chiang and Lee (2023) Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? *arXiv preprint arXiv:2305.01937*.
*   Côté et al. (2019) Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. 2019. Textworld: A learning environment for text-based games. In *Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7*, pages 41–75\. Springer.
*   Dibia et al. (2023) Victor Dibia, Adam Fourney, Gagan Bansal, Forough Poursabzi-Sangdeh, Han Liu, and Saleema Amershi. 2023. [Aligning offline metrics and human judgments of value for code generation models](http://arxiv.org/abs/2210.16494).
*   Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. *arXiv preprint arXiv:2302.04166*.
*   Guo et al. (2023) Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluating large language models: A comprehensive survey. *arXiv preprint arXiv:2310.19736*.
*   Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning ai with shared human values. *Proceedings of the International Conference on Learning Representations (ICLR)*.
*   Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*.
*   Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework. *arXiv preprint arXiv:2308.00352*.
*   Jain et al. (2023) Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023. Multi-dimensional evaluation of text summarization with in-context learning. *arXiv preprint arXiv:2306.01200*.
*   Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering. *arXiv preprint arXiv:1909.06146*.
*   Kiseleva et al. (2014) Julia Kiseleva, Eric Crestan, Riccardo Brigo, and Roland Dittel. 2014. Modelling and detecting changes in user satisfaction. In *Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management*, pages 1449–1458.
*   Kiseleva and de Rijke (2017) Julia Kiseleva and Maarten de Rijke. 2017. Evaluating personal assistants on mobile devices. *arXiv preprint arXiv:1706.04524*.
*   Kiseleva et al. (2022a) Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha Mohanty, Maartje ter Hoeve, Mikhail Burtsev, Alexey Skrynnik, Artem Zholus, Aleksandr Panov, Kavya Srinet, Arthur Szlam, Yuxuan Sun, Katja Hofmann, Marc-Alexandre Côté, Ahmed Awadallah, Linar Abdrazakov, Igor Churin, Putra Manggala, Kata Naszadi, Michiel van der Meer, and Taewoon Kim. 2022a. [Interactive grounded language understanding in a collaborative environment: Iglu 2021](https://proceedings.mlr.press/v176/kiseleva22a.html). In *Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track*, volume 176 of *Proceedings of Machine Learning Research*, pages 146–161\. PMLR.
*   Kiseleva et al. (2022b) Julia Kiseleva, Alexey Skrynnik, Artem Zholus, Shrestha Mohanty, Negar Arabzadeh, Marc-Alexandre Côté, Mohammad Aliannejadi, Milagro Teruel, Ziming Li, Mikhail Burtsev, Maartje ter Hoeve, Zoya Volovikova, Aleksandr Panov, Yuxuan Sun, Kavya Srinet, Arthur Szlam, Ahmed Awadallah, Seungeun Rho, Taehwan Kwon, Daniel Wontae Nam, Felipe Bivort Haiek, Edwin Zhang, Linar Abdrazakov, Guo Qingyam, Jason Zhang, and Zhibin Guo. 2022b. [Interactive grounded language understanding in a collaborative environment: Retrospective on iglu 2022 competition](https://proceedings.mlr.press/v220/kiseleva22a.html). In *Proceedings of the NeurIPS 2022 Competitions Track*, volume 220 of *Proceedings of Machine Learning Research*, pages 204–216\. PMLR.
*   Kiseleva et al. (2016a) Julia Kiseleva, Kyle Williams, Ahmed Hassan Awadallah, Aidan C Crook, Imed Zitouni, and Tasos Anastasakos. 2016a. Predicting user satisfaction with intelligent assistants. In *Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval*, pages 45–54.
*   Kiseleva et al. (2016b) Julia Kiseleva, Kyle Williams, Jiepu Jiang, Ahmed Hassan Awadallah, Aidan C Crook, Imed Zitouni, and Tasos Anastasakos. 2016b. Understanding user satisfaction with intelligent assistants. In *Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval*, pages 121–130.
*   Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. Camel: Communicative agents for" mind" exploration of large scale language model society. *arXiv preprint arXiv:2303.17760*.
*   Li et al. (2023b) Qintong Li, Leyang Cui, Lingpeng Kong, and Wei Bi. 2023b. Collaborative evaluation: Exploring the synergy of large language models and humans for open-ended generation evaluation. *arXiv preprint arXiv:2310.19740*.
*   Li et al. (2020) Ziming Li, Julia Kiseleva, Alekh Agarwal, Maarten de Rijke, and Ryen W White. 2020. Optimizing interactive systems via data-driven objectives. *arXiv preprint arXiv:2006.12999*.
*   Li et al. (2021) Ziming Li, Dookun Park, Julia Kiseleva, Young-Bum Kim, and Sungjin Lee. 2021. Deus: A data-driven approach to estimate user satisfaction in multi-turn dialogues. *arXiv preprint arXiv:2103.01287*.
*   Liang et al. (2023a) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023a. [Holistic evaluation of language models](http://arxiv.org/abs/2211.09110).
*   Liang et al. (2023b) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023b. [Encouraging divergent thinking in large language models through multi-agent debate](http://arxiv.org/abs/2305.19118).
*   Liu and Sun (2023) Alex Liu and Min Sun. 2023. From voices to validity: Leveraging large language models (llms) for textual analysis of policy stakeholder interviews. *arXiv preprint arXiv:2312.01202*.
*   Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench: Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.
*   Liu et al. (2022) Yanchen Liu, Timo Schick, and Hinrich Schütze. 2022. Semantic-oriented unlabeled priming for large-scale language models. *arXiv preprint arXiv:2202.06133*.
*   Mialon et al. (2023) Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: a benchmark for general ai assistants. *arXiv preprint arXiv:2311.12983*.
*   Myers et al. (2023) Vivek Myers, Andre Wang He, Kuan Fang, Homer Rich Walke, Philippe Hansen-Estruch, Ching-An Cheng, Mihai Jalobeanu, Andrey Kolobov, Anca Dragan, and Sergey Levine. 2023. [Goal representations for instruction following: A semi-supervised language interface to control](https://proceedings.mlr.press/v229/myers23a.html). In *Proceedings of The 7th Conference on Robot Learning*, volume 229 of *Proceedings of Machine Learning Research*, pages 3894–3908\. PMLR.
*   Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. *arXiv preprint arXiv:1908.10084*.
*   See et al. (2019) Abigail See, Stephen Roller, Douwe Kiela, and Jason Weston. 2019. What makes a good conversation? how controllable attributes affect human judgments. *arXiv preprint arXiv:1902.08654*.
*   Shridhar et al. (2019) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2019. [ALFRED: A benchmark for interpreting grounded instructions for everyday tasks](http://arxiv.org/abs/1912.01734). *CoRR*, abs/1912.01734.
*   Shridhar et al. (2020a) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020a. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 10740–10749.
*   Shridhar et al. (2020b) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020b. Alfworld: Aligning text and embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.
*   Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023. Multi-agent collaboration: Harnessing the power of intelligent llm agents. *arXiv preprint arXiv:2306.03314*.
*   Tjuatja et al. (2023) Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig. 2023. Do llms exhibit human-like response biases? a case study in survey design. *arXiv preprint arXiv:2311.04076*.
*   Vahtola et al. (2022) Teemu Vahtola, Mathias Creutz, and Jörg Tiedemann. 2022. It is not easy to detect paraphrases: Analysing semantic similarity with antonyms and negation using the new semantoneg benchmark. In *Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP*, pages 249–262.
*   Wang et al. (2023) Jindong Wang, Xixu HU, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang, Xiubo Geng, Binxing Jiao, Yue Zhang, and Xing Xie. 2023. [On the robustness of chatGPT: An adversarial and out-of-distribution perspective](https://openreview.net/forum?id=uw6HSkgoM29). In *ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models*.
*   Williams et al. (2016a) Kyle Williams, Julia Kiseleva, Aidan C Crook, Imed Zitouni, Ahmed Hassan Awadallah, and Madian Khabsa. 2016a. Detecting good abandonment in mobile search. In *Proceedings of the 25th International Conference on World Wide Web*, pages 495–505.
*   Williams et al. (2016b) Kyle Williams, Julia Kiseleva, Aidan C Crook, Imed Zitouni, Ahmed Hassan Awadallah, and Madian Khabsa. 2016b. Is this your final answer? evaluating the effect of answers on good abandonment in mobile search. In *Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval*, pages 889–892.
*   Williams and Zitouni (2017) Kyle Williams and Imed Zitouni. 2017. Does that mean you’re happy? rnn-based modeling of user interaction sequences to detect good abandonment. In *Proceedings of the 2017 ACM on Conference on Information and Knowledge Management*, pages 727–736.
*   Winograd (1972) Terry Winograd. 1972. Understanding natural language. *Cognitive psychology*, 3(1):1–191.
*   Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. *arXiv preprint arXiv:2308.08155*.
*   Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. *arXiv preprint arXiv:2210.03629*.
*   Zhang et al. (2023) Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing Lu. 2023. Creative agents: Empowering agents with imagination for creative tasks. *arXiv preprint arXiv:2312.02519*.
*   Ziyu et al. (2023) Zhuang Ziyu, Chen Qiguang, Ma Longxuan, Li Mingda, Han Yi, Qian Yushan, Bai Haopeng, Zhang Weinan, and Ting Liu. 2023. [Through the lens of core competency: Survey on evaluation of large language models](https://aclanthology.org/2023.ccl-2.8). In *Proceedings of the 22nd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)*, pages 88–109, Harbin, China. Chinese Information Processing Society of China.

## Appendix A Appendix

### A.1 Task Examples

In Figure [9](https://arxiv.org/html/2402.09015v3#A1.F9 "Figure 9 ‣ A.1 Task Examples ‣ Appendix A Appendix ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications") and [10](https://arxiv.org/html/2402.09015v3#A1.F10 "Figure 10 ‣ A.1 Task Examples ‣ Appendix A Appendix ‣ Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications"), we display an example of Math problems and AlfWorld Householding tasks with AutoGen’s solution for these problems.

<svg class="ltx_picture" height="2280.99" id="A1.F9.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,2280.99) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="2253.43" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">"problem": "How many positive and negative integers is $12$ a multiple of?" "level": "Level 5" "type": "Prealgebra" "response_with_ans": "messages": [ { "content": "How many positive and negative integers is $12$ a multiple of?", "role": "user" }, { "content": "To determine how many positive and negative integers 12 is a multiple of, we need to find the divisors of 12\. Then we will double the number of divisors, as each positive divisor has a corresponding negative divisor. Let’s use a Python code snippet to find all the divisors of 12 and count them.", [⬇](data:text/plain;base64,bnVtID0gMTIKZGl2aXNvcnMgPSBbaSBmb3IgaSBpbiByYW5nZSgxLCBudW0gKyAxKSBpZiBudW0gXCUgaSA9PSAwXQpwb3NpdGl2ZV9kaXZpc29yc19jb3VudCA9IGxlbihkaXZpc29ycykKbmVnYXRpdmVfZGl2aXNvcnNfY291bnQgPSBwb3NpdGl2ZV9kaXZpc29yc19jb3VudAp0b3RhbF9kaXZpc29yc19jb3VudCA9IHBvc2l0aXZlX2Rpdmlzb3JzX2NvdW50ICsgbmVnYXRpdmVfZGl2aXNvcnNfY291bnQKcHJpbnQoZiJQb3NpdGl2ZSBkaXZpc29ycyBvZiAxMjoge2Rpdmlzb3JzfSIpCmRpdmlzb3JzX25lZ2F0aXZlID0gWy14IGZvciB4IGluIGRpdmlzb3JzXQpwcmludChmIk5lZ2F0aXZlIGRpdmlzb3JzIG9mIDEyOiB7ZGl2aXNvcnNfbmVnYXRpdmV9IikKcHJpbnQoZiJUb3RhbCBkaXZpc29ycyBjb3VudDoge3RvdGFsX2Rpdmlzb3JzX2NvdW50fSIp) 1num = 12 2divisors = [i for i in range(1, num + 1) if num \% i == 0] 3positive_divisors_count = len(divisors) 4negative_divisors_count = positive_divisors_count 5total_divisors_count = positive_divisors_count + negative_divisors_count 6print(f"Positive divisors of 12: {divisors}") 7divisors_negative = [-x for x in divisors] 8print(f"Negative divisors of 12: {divisors_negative}") 9print(f"Total divisors count: {total_divisors_count}") Execute this code snippet to find the divisors of 12 and count how many positive and negative integers 12 is a multiple of. "role": "assistant" } ], "time": 17.360238790512085</foreignobject></g></g></svg>

Figure 9: An example of Math Problem solving sample with solution generated by AutoGen

{    { "content": "Perform actions and interact with a household to solve a task. At the beginning of your interactions, you will be given the detailed description of the current environment and your goal to accomplish. For each of your turn, you should choose from two actions: "think" or     "Action". If you choose "think", you should first think about the current condition and plan for your future actions, and then output your action in this turn. Your output must strictly follow this format:"think: your thoughts."    "Action: your next action\\n"; If you choose "ACTION", you should directly output the action in this turn. Your output must strictly follow this format:"ACTION: your next action".    After each of your turn, the environment will give you immediate feedback based on which you should plan your next few steps. if the envrionment output "Nothing happened", that means the previous action is invalid and you should try more options.    Reminder:    1. the action must be chosen from the given available actions. Any actions except provided available actions will be regarded as illegal.    2. Take the target household immediatly after you find it.    3. Reply ’TERMINATE’ only under two circumstances: a). The task has given you clear instructions to return ’TERMINATE’ b). The task seems insolvable.\\n Here are two examples.\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a hot apple in fridge.\n",        "role": "user"},    {   "content": "THOUGHT: The task is to put a hot apple in fridge. So i need to first find an apple and heat it. First I need to find an apple. An apple is more likely to appear in diningtable (1), coffeetable (1), fridge (1), drawer (1), cabinet (1-13), garbagecan (1). I can check one by one, starting with fridge 1.\n ACTION: go to diningtable 1\n",        "role": "assistant"},    {   "content": "On the diningtable 1, you see a apple 1, a bread 1, a fork 3, a fork 2, a fork 1, a mug 2, a peppershaker 3, a plate 2, a pot 1, a soapbottle 3, a spatula 1, a spoon 3, a spoon 2, a spoon 1, a winebottle 3, and a winebottle 2.\n",        "role": "user"},    {   "content": "ACTION: take apple 1 from diningtable 1\n",        "role": "assistant"},    {   "content": "You pick up the apple 1 from the diningtable 1.\n",        "role": "user"},    {   "content": "THOUGHT: I noticed that there is a microwave 1 in the room, i will use it to heat the apple.\n ACTION: go to microwave 1\n",        "role": "assistant"},    ...    ...    {   "content": "You open the microwave 1. The microwave 1 is open. In it, you see a egg 1.",        "role": "user"    },    {   "content": "ACTION: heat mug 2 with microwave 1",        "role": "assistant"},    {    "content": "You heat the mug 2 using the microwave 1.Task success, now reply TERMINATE\n",        "role": "user"},    {   "content": "TERMINATE",        "role": "assistant"},    {   "content": "TERMINATE",        "role": "user"}}

Figure 10: An example of AlfWorld Householding task from “Pick, Heat, then Place” category.