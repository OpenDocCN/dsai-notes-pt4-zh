- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'MuLan: 多模态-LLM 代理用于渐进式多对象扩散'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12741](https://ar5iv.labs.arxiv.org/html/2402.12741)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12741](https://ar5iv.labs.arxiv.org/html/2402.12741)
- en: Sen Li    Ruochen Wang    Cho-Jui Hsieh    Minhao Cheng    Tianyi Zhou
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Sen Li    Ruochen Wang    Cho-Jui Hsieh    Minhao Cheng    Tianyi Zhou
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Existing text-to-image models still struggle to generate images of multiple
    objects, especially in handling their spatial positions, relative sizes, overlapping,
    and attribute bindings. In this paper, we develop a training-free Multimodal-LLM
    agent (MuLan) to address these challenges by progressive multi-object generation
    with planning and feedback control, like a human painter. MuLan harnesses a large
    language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating
    only one object conditioned on previously generated objects by stable diffusion.
    Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at
    the beginning while the exact size and location of each object are determined
    by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a
    vision-language model (VLM) to provide feedback to the image generated in each
    sub-task and control the diffusion model to re-generate the image if it violates
    the original prompt. Hence, each model in every step of MuLan only needs to address
    an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects
    with spatial relationships and attribute bindings from different benchmarks to
    evaluate MuLan. The results demonstrate the superiority of MuLan in generating
    multiple objects over baselines. The code is available on [https:github.com/measure-infinity/mulan-code](https:github.com/measure-infinity/mulan-code).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的文本到图像模型在生成多个对象的图像时仍然存在困难，特别是在处理它们的空间位置、相对大小、重叠和属性绑定方面。在本文中，我们开发了一种无训练的多模态-LLM
    代理 (MuLan)，通过渐进式的多对象生成、规划和反馈控制来解决这些挑战，类似于人类画家。MuLan 利用大型语言模型 (LLM) 将提示分解为一系列子任务，每个子任务仅生成一个对象，并以稳定扩散生成的先前对象作为条件。与现有的
    LLM 基础方法不同，MuLan 只在开始时生成一个高级计划，而每个对象的确切大小和位置由 LLM 和注意力引导在每个子任务中确定。此外，MuLan 采用视觉-语言模型
    (VLM) 对每个子任务生成的图像提供反馈，并在图像违反原始提示时控制扩散模型重新生成图像。因此，MuLan 每一步中的每个模型只需要解决它专门化的简单子任务。我们从不同的基准中收集了包含空间关系和属性绑定的
    200 个多对象提示，以评估 MuLan。结果展示了 MuLan 在生成多个对象方面相较于基线的优越性。代码可在 [https:github.com/measure-infinity/mulan-code](https:github.com/measure-infinity/mulan-code)
    获取。
- en: Computer Vision, Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉、机器学习、ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Diffusion models (Sohl-Dickstein et al., [2015](#bib.bib19); Ho et al., [2020](#bib.bib9);
    Song et al., [2020](#bib.bib20)) have shown growing potential in generative AI
    tasks, especially in creating diverse and high-quality images with text prompts (Saharia
    et al., [2022](#bib.bib18); Rombach et al., [2022](#bib.bib17)). However, current
    state-of-the-art text-to-image (T2I) models such as Stable Diffusion (Rombach
    et al., [2022](#bib.bib17)) and DALL-E 3 (Betker et al., [2023](#bib.bib2)) still
    struggle to deal with complicated prompts involving multiple objects and lack
    precise control of their spatial relations, potential occlusions, relative sizes,
    etc. As shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"), to generate a sketch of “The orange
    pumpkin is on the right side of the black door”, even the SOTA open-source T2I
    model, Stable Diffusion XL (Podell et al., [2023](#bib.bib16)), still generates
    wrong attribute-binding as well as incorrect spatial positions of several objects.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '扩散模型 (Sohl-Dickstein 等, [2015](#bib.bib19); Ho 等, [2020](#bib.bib9); Song 等,
    [2020](#bib.bib20)) 在生成性 AI 任务中显示出越来越大的潜力，尤其是在使用文本提示生成多样化和高质量图像方面 (Saharia 等,
    [2022](#bib.bib18); Rombach 等, [2022](#bib.bib17))。然而，当前最先进的文本到图像 (T2I) 模型，如 Stable
    Diffusion (Rombach 等, [2022](#bib.bib17)) 和 DALL-E 3 (Betker 等, [2023](#bib.bib2))，仍然在处理涉及多个对象的复杂提示时遇到困难，并且对其空间关系、潜在遮挡、相对尺寸等缺乏精确控制。如图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion") 所示，为了生成“橙色南瓜在黑色门的右侧”的草图，即使是最先进的开源 T2I 模型 Stable Diffusion
    XL (Podell 等, [2023](#bib.bib16))，仍然会生成错误的属性绑定以及多个对象的位置错误。'
- en: Among works that aim to improve the controllability of T2I models on complicated
    prompts, a recent promising line of research seeks to utilize large language models
    (LLMs), e.g., ChatGPT, GPT-4 (Achiam et al., [2023](#bib.bib1)), to guide the
    generation process (Lian et al., [2023](#bib.bib12); Feng et al., [2023](#bib.bib6)).
    Specifically, an LLM is prompted to generate a layout for the given prompt, i.e.,
    a bounding box for each object in the image, given detailed instructions or demonstrations
    if necessary. However, due to the limited spatial reasoning capability of LLMs
    as well as their lack of alignment with the diffusion models, it is still challenging
    for LLMs to directly generate a complete and precise layout for multiple objects.
    Without a feedback loop interacting with the generative process, the layout’s
    possible mistakes cannot be effectively detected and corrected. Moreover, the
    layout is often applied as an extra condition in addition to the original prompt
    (e.g., bounding boxes combined with GLIGEN (Li et al., [2023](#bib.bib11))), so
    the diffusion models may still generate an incorrect image due to its misunderstanding
    of the complicated prompt.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些旨在改善T2I模型对复杂提示的控制能力的工作中，最近一个有前途的研究方向是利用大型语言模型（LLMs），例如ChatGPT、GPT-4（Achiam等，[2023](#bib.bib1)），来指导生成过程（Lian等，[2023](#bib.bib12)；Feng等，[2023](#bib.bib6)）。具体而言，LLM被提示生成给定提示的布局，即图像中每个对象的边界框，必要时提供详细的指示或演示。然而，由于LLM的空间推理能力有限，以及它们与扩散模型的不匹配，LLM仍然难以直接生成多个对象的完整且精确的布局。没有与生成过程交互的反馈循环，布局可能出现的错误无法有效检测和纠正。此外，布局通常作为原始提示的附加条件（例如，将边界框与GLIGEN（Li等，[2023](#bib.bib11)）结合），因此由于对复杂提示的误解，扩散模型仍然可能生成不正确的图像。
- en: '![Refer to caption](img/d7e07ee781b1b94e34befc2ca4f28179.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d7e07ee781b1b94e34befc2ca4f28179.png)'
- en: 'Figure 1: The proposed training-free Multimodal-LLM Agent (MuLan) for Progressive
    Multi-Object Diffusion. MuLan consists of three main components: (1) LLM planning;
    (2) Single-object diffusion with attention guidance; and (3) VLM-feedback control.
    MuLan first decomposes a complicated prompt into a sequence of sub-prompts each
    for one object, and then generates one object per step conditioned on a sub-prompt
    and previously generated objects, where LLM plans the rough layout of the object
    and attention guidance provides an accurate mask for it. The VLM-feedback control
    allows MuLan to correct mistakes in each step by adjusting hyperparameters in
    (2).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：提出的无训练Multimodal-LLM代理（MuLan）用于渐进式多对象扩散。MuLan由三个主要组件组成：（1）LLM规划；（2）带注意力指导的单对象扩散；（3）VLM反馈控制。MuLan首先将复杂提示分解为每个对象一个子提示的序列，然后根据子提示和先前生成的对象每次生成一个对象，其中LLM规划对象的大致布局，注意力指导提供准确的掩模。VLM反馈控制允许MuLan通过调整（2）中的超参数来纠正每一步的错误。
- en: To address the limitations and challenges of previous methods, we develop a
    training-free and controllable T2I generation paradigm that does not require demonstrations
    but mainly focuses on improving the tool usage of existing models. Our paradigm
    is built upon a progressive multi-object generation by a Multimodal-LLM agent
    (MuLan), which generates only one object per stage, conditioned on generated objects
    in the image and attention masks of the most plausible positions to place the
    new object. Unlike previous methods that add conditions to each model and make
    the task even more challenging, MuLan uses an LLM as a planner decomposing the
    original T2I task into a sequence of easier subtasks. Each subtask generates one
    single object, which can be easily handled by diffusion models. To be noted, the
    LLM applied at the beginning of MuLan only focuses on high-level planning rather
    than a precise layout of bounding boxes, while the exact size and position of
    each object are determined later in each stage by LLM and attention guidance based
    on the generated objects in the image. Hence, we can avoid mistakes in the planning
    stage and find a better placement for each object adaptive to the generated content
    and adhering to the original prompt. In addition, MuLan builds a feedback loop
    monitoring the generation process, which assesses the generated image per stage
    using a vision-language model (VLM). When the generated image violates the prompt,
    the VLM will adjust the diffusion model to re-generate the image so any mistake
    can be corrected before moving to the next stage. Furthermore, we develop a strategy
    applied in each stage to handle the overlapping between objects, which is commonly
    ignored by previous work (Lian et al., [2023](#bib.bib12)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决先前方法的局限性和挑战，我们开发了一种无需训练且可控的 T2I 生成范式，该范式不需要演示，而主要关注于提高现有模型的工具使用效果。我们的范式基于由多模态
    LLM 代理 (MuLan) 进行的渐进式多物体生成，该代理每个阶段只生成一个物体，条件是图像中已生成物体和最有可能放置新物体的位置的注意力掩码。与先前将条件添加到每个模型并使任务更加困难的方法不同，MuLan
    使用 LLM 作为规划者，将原始 T2I 任务分解为一系列更简单的子任务。每个子任务生成一个单独的物体，这可以被扩散模型轻松处理。需要注意的是，MuLan
    开始阶段应用的 LLM 只关注于高层次的规划，而不是边界框的精确布局，而每个物体的确切大小和位置则由 LLM 和基于图像中生成物体的注意力指导在每个阶段后续确定。因此，我们可以避免规划阶段的错误，并找到适应生成内容并符合原始提示的更好物体放置。此外，MuLan
    建立了一个反馈循环来监控生成过程，该过程使用视觉语言模型 (VLM) 评估每个阶段生成的图像。当生成的图像违反提示时，VLM 会调整扩散模型以重新生成图像，以便在进入下一阶段之前纠正任何错误。此外，我们开发了一种应用于每个阶段的策略，以处理物体之间的重叠，这是先前工作中常被忽视的问题
    (Lian et al., [2023](#bib.bib12))。
- en: 'Therefore, MuLan obtains better controllability of the multi-object composition.
    An illustration of the progressive generation process is shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion"). To evaluate MuLan, we curate a dataset of intricate and challenging
    prompts from different benchmarks. To compare MuLan with existing approaches,
    we prompt GPT-4V (OpenAI, [2023](#bib.bib15)) several questions based on the input
    texts to comprehensively evaluate the alignment of the generated images with the
    prompts from three aspects. We further conduct human evaluations of the generated
    images. Extensive experimental results show that MuLan can achieve better controllability
    over the generation process and generate high-quality images aligning better with
    the prompts than the baselines. Example images generated by different methods
    are shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"). Our main contributions are summarized
    as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，MuLan 实现了对多物体组合的更好控制。渐进生成过程的示意图见图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")。为了评估 MuLan，我们策划了一个来自不同基准的复杂且具有挑战性的提示数据集。为了将
    MuLan 与现有方法进行比较，我们向 GPT-4V (OpenAI, [2023](#bib.bib15)) 提出了几个基于输入文本的问题，以全面评估生成的图像与提示的一致性，从三个方面进行评估。我们进一步进行了生成图像的人类评估。大量实验结果表明，MuLan
    可以在生成过程中实现更好的控制，并生成与提示更好对齐的高质量图像，优于基准方法。不同方法生成的示例图像见图 [2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")。我们的主要贡献总结如下：'
- en: '![Refer to caption](img/4277fec584bbedd7ba278d69e51fb0a9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4277fec584bbedd7ba278d69e51fb0a9.png)'
- en: 'Figure 2: Examples of MuLan-generated images, compared to the original SD-v1.4 (Rombach
    et al., [2022](#bib.bib17)), the original SDXL (Podell et al., [2023](#bib.bib16)),
    Structure diffusion (Feng et al., [2022](#bib.bib5)), Promptist (Hao et al., [2022](#bib.bib7)),
    and PixArt-$\alpha$ (Chen et al., [2023](#bib.bib3)).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：MuLan生成的图像示例，与原始SD-v1.4 (Rombach et al., [2022](#bib.bib17))、原始SDXL (Podell
    et al., [2023](#bib.bib16))、结构扩散 (Feng et al., [2022](#bib.bib5))、Promptist (Hao
    et al., [2022](#bib.bib7))和PixArt-$\alpha$ (Chen et al., [2023](#bib.bib3))相比。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel training-free paradigm for text-to-image generation and a
    Multimodal-LLM agent. It achieves better control in generating images for complicated
    prompts consisting of multiple objects with specified spatial relationships and
    attribute bindings.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的无训练范式用于文本到图像生成，以及一个多模态LLM代理。它在生成具有指定空间关系和属性绑定的多个对象的复杂提示的图像时，实现了更好的控制。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose an effective strategy to handle multi-object occlusion in T2I generation,
    which improves the image quality and makes them more realistic.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种有效的策略来处理T2I生成中的多对象遮挡，这提高了图像质量并使其更具现实感。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We curate a dataset of prompts to evaluate multi-object composition with spatial
    relationships and attribute bindings in T2I tasks. The quantitative results and
    human evaluation results show that our method can achieve better results compared
    to different controllable generation methods and general T2I generation methods.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们策划了一个提示数据集，以评估T2I任务中多对象的空间关系和属性绑定的组合。定量结果和人工评估结果表明，我们的方法能够实现比不同的可控生成方法和通用T2I生成方法更好的结果。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Diffusion models
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩散模型
- en: As a new family of generative models, diffusion models have attracting more
    and more attention due to its powerful creative capability. Text-to-image generation,
    which aims to generate the high-quality image aligning with given text prompts,
    is one of the most popular applications (Nichol et al., [2021](#bib.bib14); Saharia
    et al., [2022](#bib.bib18); Rombach et al., [2022](#bib.bib17); Betker et al.,
    [2023](#bib.bib2)). Among different powerful diffusion models, the latent diffusion
    model (Rombach et al., [2022](#bib.bib17)) has shown amazing capability and has
    been widely used in practice due to the efficiency and superior performance, which
    is also the backbone of the current SOTA stable diffusion models. Different from
    the typical diffusion models which directly perform the diffusion and denoising
    process in the pixel space, the latent diffusion model perform the whole process
    in the encoded latent space (Rombach et al., [2022](#bib.bib17)), which can greatly
    reduce the training and inference time. Recently, empowered by a significantly
    expanded model capacity, Stable Diffusion XL has demonstrated performance levels
    approaching commercial application standards (Podell et al., [2023](#bib.bib16)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为新一代生成模型，扩散模型因其强大的创作能力而受到越来越多的关注。文本到图像生成旨在生成与给定文本提示对齐的高质量图像，是最受欢迎的应用之一 (Nichol
    et al., [2021](#bib.bib14); Saharia et al., [2022](#bib.bib18); Rombach et al.,
    [2022](#bib.bib17); Betker et al., [2023](#bib.bib2))。在不同的强大扩散模型中，潜在扩散模型 (Rombach
    et al., [2022](#bib.bib17))展现了惊人的能力，并因其效率和优越的性能被广泛应用，亦是当前SOTA稳定扩散模型的核心。与直接在像素空间进行扩散和去噪处理的典型扩散模型不同，潜在扩散模型在编码的潜在空间中进行整个过程 (Rombach
    et al., [2022](#bib.bib17))，这可以大大减少训练和推理时间。最近，得益于显著扩展的模型容量，稳定扩散XL展示了接近商业应用标准的性能水平 (Podell
    et al., [2023](#bib.bib16))。
- en: Composed generation in diffusion models
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩散模型中的复合生成
- en: Although Stable Diffusion model has shown unprecedented performance on the T2I
    generation task, it still struggles with text prompts with multi-object, especially
    when there are several spatial relationships and attribute bindings in the prompts.
    To achieve more controllable and accurate image compositions, many compositional
    generation methods have been proposed. StructureDiffusion (Feng et al., [2022](#bib.bib5))
    proposed a training-free method to parse the input prompt and combine it with
    the cross-attention to achieve better control over attribute bindings and compositional
    generation. On the other hand, Promptist (Hao et al., [2022](#bib.bib7)) aimed
    to train a language model with the objective of optimizing input prompts, rendering
    them more comprehensible and facilitative for diffusion models. Recently, several
    works utilize the large language model to directly generate the whole layout for
    the input prompt with in-context learning, and then generate the image conditioned
    on the layout (Lian et al., [2023](#bib.bib12); Feng et al., [2023](#bib.bib6)).
    While all the previous take the whole input prompt, we propose to turn the original
    complicated task into several easier sub-tasks. A training-free multimodal-LLM
    agent is utilized to progressively generate objects with feedback control so that
    the whole generation process would be better controlled.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管稳定扩散模型在T2I生成任务上表现出前所未有的性能，但在处理具有多个对象的文本提示时仍面临困难，尤其是当提示中存在多个空间关系和属性绑定时。为了实现更可控和准确的图像构图，已经提出了许多组合生成方法。StructureDiffusion（Feng等，[2022](#bib.bib5)）提出了一种无需训练的方法，以解析输入提示并结合交叉注意力，以实现对属性绑定和组合生成的更好控制。另一方面，Promptist（Hao等，[2022](#bib.bib7)）旨在训练一个语言模型，以优化输入提示，使其对扩散模型更具可理解性和辅助性。最近，一些工作利用大语言模型直接生成输入提示的整个布局，并在上下文学习的基础上生成图像（Lian等，[2023](#bib.bib12)；Feng等，[2023](#bib.bib6)）。虽然之前的方法都处理整个输入提示，我们提出将原始复杂任务转化为几个更简单的子任务。采用无训练的多模态-LLM代理逐步生成对象，并通过反馈控制来更好地控制整个生成过程。
- en: 3 Multimodal-LLM Agent (MuLan)
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 多模态-LLM代理（MuLan）
- en: Existing diffusion models often struggle with complicated prompts but can handle
    simpler ones. Recent approaches train a model or apply in-context learning given
    similar examples to produce a detailed layout for the prompt in advance and the
    diffusion model can generate each part of the layout with a simpler prompt separately.
    Rather than generating all objects at once or in parallel, MuLan is inspired by
    many human painters, who start by making a high-level plan, painting objects one
    after another as planned, and correcting mistakes after each step if needed. Thereby,
    the constraints between objects can be naturally taken into account.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的扩散模型通常在处理复杂提示时会遇到困难，但可以处理较简单的提示。近期的方法训练一个模型或应用上下文学习，给出类似的示例，以提前生成详细的布局，然后扩散模型可以用更简单的提示单独生成布局的每一部分。与其一次性或并行生成所有对象，MuLan受到许多人类画家的启发，他们开始时制定高层次计划，按照计划一个一个地绘制对象，并在每一步后如有需要进行纠正。这样，对象之间的约束可以自然地考虑进去。
- en: 3.1 Problem Formulation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: 'Likewise, MuLan begins by strategically planning and decomposing an intricate
    input prompt into a manageable sequence of sub-prompts, each focusing on an easier
    sub-task generating one single object. MuLan then adopts a progressive strategy
    that generates one object in each stage conditioned on previously generated objects
    using a diffusion model. Simultaneously, a VLM offers insightful feedback and
    adaptively adjusts the generation process to guarantee precision in accomplishing
    each subtask. Compared to previous methods, MuLan is entirely training-free and
    does not require any examples. As illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"),
    MuLan is composed of three components:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '同样，MuLan开始时会战略性地规划和分解复杂的输入提示，将其拆分为一个可管理的子提示序列，每个子提示专注于生成一个单一对象。然后，MuLan采用逐步生成策略，在每个阶段生成一个对象，这个过程是基于之前生成的对象，并使用扩散模型。同时，VLM提供有价值的反馈，并自适应地调整生成过程，以确保每个子任务的精确完成。与以前的方法相比，MuLan完全无训练要求，不需要任何示例。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion")所示，MuLan由三个组件组成：'
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prompt decomposition by LLM planning, which produces a sequence of sub-prompts,
    each focusing on generating one object in the prompt.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由LLM规划进行的提示分解，生成一个子提示序列，每个子提示集中于生成提示中的一个对象。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Conditional single-object diffusion with LLM planning and attention guidance,
    which generates a new object conditioned on the previous step’s image using a
    stable diffusion model. While a sub-prompt from LLM planning provides text guidance,
    the object’s size and position are controlled by an attention mask.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具有 LLM 规划和注意力指导的条件单对象扩散，它使用稳定扩散模型生成一个新的对象，该对象以先前步骤的图像为条件。虽然 LLM 规划中的子提示提供了文本指导，但对象的大小和位置由注意力掩码控制。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: VLM-feedback control, which inspects the image generated per stage and adjusts
    hyperparameters to re-generate the image if it violates the original prompt.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VLM 反馈控制，检查每个阶段生成的图像，并调整超参数以重新生成图像，如果它违反了原始提示。
- en: 3.2 Background on (Latent) Diffusion Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 (潜在) 扩散模型背景
- en: Consisting of the diffusion process and the reverse process, diffusion models
    have shown impressive capability for high-quality image generation by iteratively
    adding noise and denoising (Ho et al., [2020](#bib.bib9)). Let $\bm{x}_{0}\sim
    q(\bm{x}_{0})$, the training loss of the model can be simplified as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由扩散过程和反向过程组成，扩散模型通过反复添加噪声和去噪显示了在高质量图像生成方面的显著能力 (Ho 等， [2020](#bib.bib9))。让 $\bm{x}_{0}\sim
    q(\bm{x}_{0})$，模型的训练损失可以简化为
- en: '|  | $1$2 |  | (1) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: Latent diffusion models (Rombach et al., [2022](#bib.bib17)) have recently attracted
    growing attention due to their efficiency and superior performance. Instead of
    performing diffusion and its reverse process in the pixel space, they add noise
    and denoise in a latent space of $\bm{z}$. Accordingly, the training loss becomes
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型 (Rombach 等， [2022](#bib.bib17)) 最近因其高效性和优越的性能而受到越来越多的关注。它们不是在像素空间中执行扩散及其反向过程，而是在
    $\bm{z}$ 的潜在空间中添加噪声和去噪。因此，训练损失变为
- en: '|  | $\displaystyle L_{LDM}=\mathbb{E}_{\bm{z}_{0},\bm{\epsilon},t}\&#124;\bm{\epsilon}-\bm{\epsilon}_{\theta}(\bm{z}_{t},t)\&#124;^{2}.$
    |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{LDM}=\mathbb{E}_{\bm{z}_{0},\bm{\epsilon},t}\&#124;\bm{\epsilon}-\bm{\epsilon}_{\theta}(\bm{z}_{t},t)\&#124;^{2}.$
    |  | (2) |'
- en: 3.3 Prompt Decomposition by LLM Planning
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM 规划的提示分解
- en: 'Given a complicated prompt p, MuLan first uses an LLM to automatically decompose
    p into $N$”. MuLan conducts the above global planning by an LLM at the very beginning
    before generating any image. The detailed prompts and template for LLM planning
    can be found in Appendix [A](#A1 "Appendix A Detailed prompt template of the global
    planning by the LLM ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '给定一个复杂的提示 p，MuLan 首先使用 LLM 自动将 p 分解为 $N$”。MuLan 在生成任何图像之前，首先由 LLM 进行上述全局规划。LLM
    规划的详细提示和模板可以在附录 [A](#A1 "附录 A LLM 全局规划详细提示模板 ‣ MuLan: 多模态-LLM 代理用于渐进式多对象扩散") 中找到。'
- en: 'When generating each object in Section [3.4](#S3.SS4 "3.4 Conditional Single-Object
    Diffusion with LLM Planning and Attention Guidance ‣ 3 Multimodal-LLM Agent (MuLan)
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"), we will
    use the LLM again as a local planner of the object’s position and size, i.e.,
    by generating a mask in the image and coordinating its overlap with previous objects.
    Then a diffusion model is used to generate the object under the attention guidance
    of the mask. These will be further elaborated in Section [3.4](#S3.SS4 "3.4 Conditional
    Single-Object Diffusion with LLM Planning and Attention Guidance ‣ 3 Multimodal-LLM
    Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '在生成第 [3.4](#S3.SS4 "3.4 具有 LLM 规划和注意力指导的条件单对象扩散 ‣ 3 多模态-LLM 代理 (MuLan) ‣ MuLan:
    多模态-LLM 代理用于渐进式多对象扩散") 节中的每个对象时，我们将再次使用 LLM 作为对象位置和大小的局部规划器，即通过在图像中生成掩码并协调其与先前对象的重叠。然后，使用扩散模型在掩码的注意力指导下生成对象。这些将在第
    [3.4](#S3.SS4 "3.4 具有 LLM 规划和注意力指导的条件单对象扩散 ‣ 3 多模态-LLM 代理 (MuLan) ‣ MuLan: 多模态-LLM
    代理用于渐进式多对象扩散") 节中进一步阐述。'
- en: 3.4 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 具有 LLM 规划和注意力指导的条件单对象扩散
- en: '![Refer to caption](img/a677a4ab6ce43fd95c5fea4526fd850f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a677a4ab6ce43fd95c5fea4526fd850f.png)'
- en: 'Figure 3: Single object diffusion with LLM planning and attention guidance
    for $\texttt{obj}_{n}$ (detailed procedure in Algorithm [1](#alg1 "Algorithm 1
    ‣ 3.4 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
    ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion")).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3：使用 LLM 规划和注意力引导的单对象扩散，针对 $\texttt{obj}_{n}$（详细过程见算法 [1](#alg1 "算法 1 ‣ 3.4
    条件单对象扩散与 LLM 规划和注意力引导 ‣ 3 多模态-LLM 代理 (MuLan) ‣ MuLan: 多模态-LLM 代理用于渐进式多对象扩散")）。'
- en: Algorithm 1 Single Object Diffusion in MuLan
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 MuLan 中的单对象扩散
- en: '1:  Input: Object number $n$}'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：对象编号 $n$}'
- en: 'At stage-$n$-1). The pipeline is given in Figure [3](#S3.F3 "Figure 3 ‣ 3.4
    Conditional Single-Object Diffusion with LLM Planning and Attention Guidance ‣
    3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion") with the complete procedure listed in Algorithm [1](#alg1 "Algorithm
    1 ‣ 3.4 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
    ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion"). We will introduce it step by step in the following.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在阶段-$n$-1）。管道见图 [3](#S3.F3 "图 3 ‣ 3.4 条件单对象扩散与 LLM 规划和注意力引导 ‣ 3 多模态-LLM 代理
    (MuLan) ‣ MuLan: 多模态-LLM 代理用于渐进式多对象扩散")，完整过程列于算法 [1](#alg1 "算法 1 ‣ 3.4 条件单对象扩散与
    LLM 规划和注意力引导 ‣ 3 多模态-LLM 代理 (MuLan) ‣ MuLan: 多模态-LLM 代理用于渐进式多对象扩散")。我们将在下面逐步介绍。'
- en: LLM Planning of a Rough Mask for $\texttt{obj}_{n}$.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 针对 $\texttt{obj}_{n}$ 的粗略掩膜的 LLM 规划。
- en: At stage-$n$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在阶段-$n$。
- en: 'When $n=1$, which leads to the following bounding box:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $n=1$ 时，导致以下边界框：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: '![Refer to caption](img/eaf6680d2961f1b44158eed3637777d6.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/eaf6680d2961f1b44158eed3637777d6.png)'
- en: 'Figure 4: The rough mask $\bm{M}_{n}$.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：粗略掩膜 $\bm{M}_{n}$。
- en: When $$n> as followings.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $$n> 如下。
- en: '|  | <math id="S3.E4.m1.4" class="ltx_Math" alttext="\displaystyle\bm{M}_{n}=\begin{cases}(\tilde{x}_{n-1}+\tilde{w}_{n-1},0,\frac{W-\tilde{x}_{n-1}+\tilde{w}_{n-1}}{\texttt{Num}_{n}},H),\\
    \text{if }\texttt{opt}_{n}=\texttt{right},\\'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math id="S3.E4.m1.4" class="ltx_Math" alttext="\displaystyle\bm{M}_{n}=\begin{cases}(\tilde{x}_{n-1}+\tilde{w}_{n-1},0,\frac{W-\tilde{x}_{n-1}+\tilde{w}_{n-1}}{\texttt{Num}_{n}},H),\\
    \text{if }\texttt{opt}_{n}=\texttt{right},\\'
- en: \\
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \\
- en: (0,\frac{\tilde{y}_{n-1}\cdot(\texttt{Num}_{n}-1)}{\texttt{Num}_{n}},W,\frac{\tilde{y}_{n-1}}{\texttt{Num}_{n}}),\\
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (0,\frac{\tilde{y}_{n-1}\cdot(\texttt{Num}_{n}-1)}{\texttt{Num}_{n}},W,\frac{\tilde{y}_{n-1}}{\texttt{Num}_{n}}),\\
- en: \text{if }\texttt{opt}^{n}=\texttt{top}.\end{cases}$$ |  | (4) |
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \text{if }\texttt{opt}^{n}=\texttt{top}.\end{cases}$$ |  | (4) |
- en: 'Figure [4](#S3.F4 "Figure 4 ‣ LLM Planning of a Rough Mask for "obj"_𝑛. ‣ 3.4
    Conditional Single-Object Diffusion with LLM Planning and Attention Guidance ‣
    3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion") illustrates how the rough mask can be computed based on the precise
    mask of previous objects.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#S3.F4 "图 4 ‣ 针对“obj”_𝑛 的粗略掩膜的 LLM 规划 ‣ 3.4 条件单对象扩散与 LLM 规划和注意力引导 ‣ 3
    多模态-LLM 代理 (MuLan) ‣ MuLan: 多模态-LLM 代理用于渐进式多对象扩散") 说明了如何基于先前对象的精确掩膜计算粗略掩膜。'
- en: '![Refer to caption](img/f5bebbdfc8f48f560f2b5bfdd1892c6f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f5bebbdfc8f48f560f2b5bfdd1892c6f.png)'
- en: 'Figure 5: Illustration of the rough mask $\bm{M}_{1}$ for the mask since the
    LLM is prompted to plan the object order from left to right, bottom to top.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：粗略掩膜 $\bm{M}_{1}$ 的示意图，因为 LLM 被提示按照从左到右、从下到上的顺序规划对象。
- en: Single-Object Diffusion with Attention Guidance.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带有注意力引导的单对象扩散。
- en: 'Given the rough mask $\bm{M}_{n}$) to have a larger value inside the mask:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 给定粗略掩膜 $\bm{M}_{n}$ 以在掩膜内具有更大的值：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: Then, in every step-$t$ by
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在每一步 $t$ 中由
- en: '|  | $\displaystyle\bm{z}_{n,t}=\bm{z}_{n,t}-\eta\cdot\nabla_{\bm{z}_{n,t}}\sum_{j\in
    B}E(\bm{A}^{(j)},\bm{M}_{n},k),$ |  | (6) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{n,t}=\bm{z}_{n,t}-\eta\cdot\nabla_{\bm{z}_{n,t}}\sum_{j\in
    B}E(\bm{A}^{(j)},\bm{M}_{n},k),$ |  | (6) |'
- en: where $\eta$ by
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\eta$ 由
- en: '|  | $\displaystyle\bm{z}_{n,(t-1)}=\bm{M}^{\prime}_{n}\odot\bm{z}_{n,(t-1)}+(1-\bm{M}^{\prime}_{n})\odot\bm{z}_{(n-1),(t-1)},$
    |  | (7) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{n,(t-1)}=\bm{M}^{\prime}_{n}\odot\bm{z}_{n,(t-1)}+(1-\bm{M}^{\prime}_{n})\odot\bm{z}_{(n-1),(t-1)},$
    |  | (7) |'
- en: where $\odot$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$。
- en: MuLan applies the above single-object diffusion to each object one after another
    from $\texttt{obj}_{1}$-defined bounding box via attention guidance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: MuLan 将上述单对象扩散应用于每个对象，从 $\texttt{obj}_{1}$ 定义的边界框开始，通过注意力引导依次处理。
- en: Overlapping between Objects
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对象间的重叠
- en: 'Overlapping between objects is a key challenge when generating one object conditioned
    on the previous one(s). However, it lacks attention in previous methods (Lian
    et al., [2023](#bib.bib12); Feng et al., [2023](#bib.bib6)). Instead, we propose
    an effective strategy that can be merged into the procedure above. Specifically,
    at the generation of object $\texttt{obj}_{n}$. An illustration is given in Figure [7](#A3.F7
    "Figure 7 ‣ Appendix C More details on the overlapping processing ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion") with more details of candidate
    masks in Appendix [C](#A3 "Appendix C More details on the overlapping processing
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '生成一个对象时，如何处理对象之间的重叠是一个关键挑战。然而，之前的方法中对此关注不足 (Lian et al., [2023](#bib.bib12);
    Feng et al., [2023](#bib.bib6))。因此，我们提出了一种有效的策略，可以与上述过程合并。具体来说，在生成对象 $\texttt{obj}_{n}$
    时，图示如图 [7](#A3.F7 "图 7 ‣ 附录 C 更多重叠处理细节 ‣ MuLan: 用于渐进式多对象扩散的多模态-LLM 代理")，更多候选掩码的细节见附录
    [C](#A3 "附录 C 更多重叠处理细节 ‣ MuLan: 用于渐进式多对象扩散的多模态-LLM 代理")。'
- en: Given the three masks $\bm{M}_{n,i}$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 给定三个掩码 $\bm{M}_{n,i}$。
- en: 3.5 Adaptive Feedback Control by VLM
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 VLM 的自适应反馈控制
- en: To correct the possible mistakes made in the sequential generation process,
    MuLan builds a feedback-loop control by a vision-language model (VLM). After each
    generation stage, MuLan queries the VLM to inspect the generated object(s)and
    its consistency with the input prompt. If they do not align well, MuLan will adjust
    the backward guidance and $T^{*}$ of the current stage to re-generate the object.
    Such a close-loop control involves LLM, diffusion, and VLM and significantly automates
    the T2I generation for complicated prompts, leading to a more accurate generation
    in practice.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了纠正顺序生成过程中的可能错误，MuLan 通过视觉-语言模型（VLM）建立了一个反馈循环控制。在每个生成阶段之后，MuLan 查询 VLM 来检查生成的对象及其与输入提示的一致性。如果不一致，MuLan
    将调整当前阶段的反向指导和 $T^{*}$ 以重新生成对象。这种闭环控制涉及 LLM、扩散和 VLM，大大自动化了复杂提示的 T2I 生成，从而在实践中实现了更准确的生成。
- en: 4 Experiments
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'Table 1: GPT-4V evaluation/human evaluation of images generated by different
    methods for complicated prompts.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: GPT-4V 对不同方法生成的复杂提示图像的评估/人工评估。'
- en: '| Method | Object completeness | Attribute bindings | Spatial relationships
    | Overall |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 对象完整性 | 属性绑定 | 空间关系 | 总体 |'
- en: '| Structure Diffusion (Feng et al., [2022](#bib.bib5)) | 88.97%/87.37% | 54.62%/62.63%
    | 34.36%/24.24% | 64.31%/64.85% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 结构扩散 (Feng et al., [2022](#bib.bib5)) | 88.97%/87.37% | 54.62%/62.63% | 34.36%/24.24%
    | 64.31%/64.85% |'
- en: '| Promptist-SD v1.4 (Hao et al., [2022](#bib.bib7)) | 80.36%/70.71% | 49.23%/52.02%
    | 24.49%/13.13% | 56.73%/51.72% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Promptist-SD v1.4 (Hao et al., [2022](#bib.bib7)) | 80.36%/70.71% | 49.23%/52.02%
    | 24.49%/13.13% | 56.73%/51.72% |'
- en: '| Promptist-SDXL (Hao et al., [2022](#bib.bib7)) | 94.36%/93.94% | 70.00%/78.28%
    | 35.89%/33.33% | 72.92%/75.56% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Promptist-SDXL (Hao et al., [2022](#bib.bib7)) | 94.36%/93.94% | 70.00%/78.28%
    | 35.89%/33.33% | 72.92%/75.56% |'
- en: '| SD v1.4 (Rombach et al., [2022](#bib.bib17)) | 90.31%/74.49% | 57.14%/51.02%
    | 37.24%/32.65% | 66.43%/56.73% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 (Rombach et al., [2022](#bib.bib17)) | 90.31%/74.49% | 57.14%/51.02%
    | 37.24%/32.65% | 66.43%/56.73% |'
- en: '| SDXL (Podell et al., [2023](#bib.bib16)) | 94.64%/78.57% | 66.07%/53.06%
    | 41.14%/24.49% | 72.34%/57.55% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SDXL (Podell et al., [2023](#bib.bib16)) | 94.64%/78.57% | 66.07%/53.06%
    | 41.14%/24.49% | 72.34%/57.55% |'
- en: '| PixArt-$\alpha$ (Chen et al., [2023](#bib.bib3)) | 92.09%/76.53% | 66.58%/61.22%
    | 34.69%/32.65% | 70.41%/61.63% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| PixArt-$\alpha$ (Chen et al., [2023](#bib.bib3)) | 92.09%/76.53% | 66.58%/61.22%
    | 34.69%/32.65% | 70.41%/61.63% |'
- en: '| MuLan-SD v1.4 (Ours) | 93.11%/86.36% | 74.23%/74.24% | 51.53%/54.54% | 77.24%/75.15%
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| MuLan-SD v1.4 (我们的) | 93.11%/86.36% | 74.23%/74.24% | 51.53%/54.54% | 77.24%/75.15%
    |'
- en: '| MuLan-SDXL (Ours) | 96.17%/90.40% | 75.00%/79.29% | 39.29%/49.49% | 76.33%/77.78%
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| MuLan-SDXL (我们的) | 96.17%/90.40% | 75.00%/79.29% | 39.29%/49.49% | 76.33%/77.78%
    |'
- en: Dataset
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: To evaluate our framework, we construct a prompt dataset from different benchmarks.
    Specifically, since our focus is to achieve better generation for complex prompts
    containing multi-objects with both spatial relationships and attribute bindings,
    we first collect all complex spatial prompts from T2I-CompBench (Huang et al.,
    [2023](#bib.bib10)). To make the experiments more comprehensive, we let ChatGPT
    generate about 400 prompts with different objects, spatial relationships, and
    attribute bindings so that the prompt sets consists of about 600 prompts. To further
    evaluate the capability of our framework on extremely complex and hard prompts,
    we manually add prompts that SDXL fails to generate, leading to a hard prompt
    dataset containing 200 prompts. Similar to the complex spatial prompts in T2I-CompBench (Huang
    et al., [2023](#bib.bib10)), each prompt in our curated dataset typically contains
    two objects with various spatial relationships, with each object containing attribute
    bindings randomly selected from {color,shape,texture}.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的框架，我们从不同的基准中构建了一个提示数据集。具体来说，由于我们的重点是实现对包含多对象的复杂提示的更好生成，这些提示具有空间关系和属性绑定，我们首先从T2I-CompBench（Huang等，[2023](#bib.bib10)）收集所有复杂的空间提示。为了使实验更全面，我们让ChatGPT生成约400个具有不同对象、空间关系和属性绑定的提示，从而使提示集包含约600个提示。为了进一步评估我们框架在极其复杂和困难提示上的能力，我们手动添加了SDXL未能生成的提示，形成了一个包含200个提示的困难提示数据集。与T2I-CompBench（Huang等，[2023](#bib.bib10)）中的复杂空间提示类似，我们策划的数据集中的每个提示通常包含两个对象，这些对象具有各种空间关系，每个对象的属性绑定从{颜色、形状、纹理}中随机选择。
- en: Models & Baseline
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型与基线
- en: As a training-free framework, MuLan can be incorporated into any existing diffusion
    models. We evaluate two stable diffusion models with our framework, Stable Diffusion
    v1.4 (Rombach et al., [2022](#bib.bib17)) and the SOTA Stable Diffusion XL (Podell
    et al., [2023](#bib.bib16)). To verify the superiority of MuLan, we compare it
    with previous controllable generation methods and general T2I generation methods.
    Specifically, we evaluate Structure Diffusion (Feng et al., [2022](#bib.bib5)),
    Promptist (Hao et al., [2022](#bib.bib7)), the original Stable Diffusion v1.4,
    the original SDXL, and the recent SOTA diffusion model PixArt-$\alpha$ (Chen et al.,
    [2023](#bib.bib3)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个无需训练的框架，MuLan可以被整合到任何现有的扩散模型中。我们用我们的框架评估了两个稳定扩散模型，即Stable Diffusion v1.4（Rombach等，[2022](#bib.bib17)）和SOTA
    Stable Diffusion XL（Podell等，[2023](#bib.bib16)）。为了验证MuLan的优越性，我们将其与先前的可控生成方法和通用T2I生成方法进行了比较。具体来说，我们评估了Structure
    Diffusion（Feng等，[2022](#bib.bib5)）、Promptist（Hao等，[2022](#bib.bib7)）、原始的Stable
    Diffusion v1.4、原始的SDXL以及最近的SOTA扩散模型PixArt-$\alpha$（Chen等，[2023](#bib.bib3)）。
- en: Implementation Details
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节
- en: MuLan use GPT-4 (Achiam et al., [2023](#bib.bib1)) as the LLM planner, and LLaVA-1.5 (Liu
    et al., [2023](#bib.bib13)) as the VLM checker to provide the feedback. We also
    conducted an ablation study to show the importance of the feedback control provided
    by the VLM and the effect of different VLMs. Moreover, we found the attention
    blocks utilized during the attention guidance are vital, which can be classified
    as near-input blocks, near-middle blocks, and near-output blocks. We utilize the
    near-middle blocks in our main experiments and also show the ablation results
    of different blocks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: MuLan使用GPT-4（Achiam等，[2023](#bib.bib1)）作为LLM规划器，并使用LLaVA-1.5（Liu等，[2023](#bib.bib13)）作为VLM检查器提供反馈。我们还进行了消融研究，以展示VLM提供的反馈控制的重要性以及不同VLM的效果。此外，我们发现，在注意力引导过程中使用的注意力块是至关重要的，这些块可以分为近输入块、近中间块和近输出块。我们在主要实验中使用了近中间块，并展示了不同块的消融结果。
- en: Evaluation
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: 'Since the prompt dataset contains texts with complex compositions, we design
    a questionnaire to comprehensively investigate the alignment between the generated
    image and the corresponding input text. The questionnaire is composed of three
    aspects - object completeness, correctness of attribute bindings, and correctness
    of spatial relationships. We only set two options for each question (Yes or No),
    without any ambiguity. For detailed questions and examples of the evaluation,
    please refer to Appendix [D](#A4 "Appendix D More details on the evaluation questionnaire
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"). For each
    aspect of the evaluation, we compute the percentage of answers with “Yes”. Given
    the generated image, we assess the image’s quality using a questionnaire asking
    both the state-of-the-art multi-modal large language model (GPT-4V (OpenAI, [2023](#bib.bib15)))
    and the human evaluator.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '由于提示数据集包含复杂构成的文本，我们设计了一个问卷来全面调查生成图像与相应输入文本之间的一致性。问卷由三个方面组成——对象完整性、属性绑定的正确性和空间关系的正确性。每个问题仅设置了两个选项（是或否），没有任何模糊性。有关评估的详细问题和示例，请参见附录[D](#A4
    "Appendix D More details on the evaluation questionnaire ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion")。对于评估的每个方面，我们计算了“是”答案的百分比。给定生成的图像，我们使用问卷评估图像的质量，问卷由最新的多模态大型语言模型
    (GPT-4V [OpenAI, 2023](#bib.bib15)) 和人工评估者完成。'
- en: 4.1 Main Results and Analysis
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主要结果与分析
- en: Results with GPT Evaluation
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT 评估结果
- en: 'Given the generated image, we prompt GPT-4V to answer the questions about the
    image in the questionnaire, where each only focuses on one of the three aspects.
    The results for different methods and different base models are shown in Table [1](#S4.T1
    "Table 1 ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion"). The results show that our framework can achieve the best performance
    compared to different controllable generation methods and T2I generation methods.
    In particular, in the two ‘harder’ aspects - attribute bindings and spatial relationships,
    MuLan can surpass other methods by a large margin. More qualitative results can
    be found in Figure [6](#S4.F6 "Figure 6 ‣ Results with GPT Evaluation ‣ 4.1 Main
    Results and Analysis ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion") and Appendix [E](#A5 "Appendix E More qualitative results
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '给定生成的图像，我们让 GPT-4V 在问卷中回答关于图像的问题，每个问题只关注三个方面中的一个。不同方法和不同基础模型的结果见表格[1](#S4.T1
    "Table 1 ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion")。结果显示，我们的框架在不同的可控生成方法和 T2I 生成方法中表现最佳。特别是在两个“更难”的方面——属性绑定和空间关系，MuLan
    可以大幅超越其他方法。更多定性结果见图表[6](#S4.F6 "Figure 6 ‣ Results with GPT Evaluation ‣ 4.1 Main
    Results and Analysis ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion")和附录[E](#A5 "Appendix E More qualitative results ‣ MuLan:
    Multimodal-LLM Agent for Progressive Multi-Object Diffusion")。'
- en: '![Refer to caption](img/86f64cf6bc764564666a32e3e0e87822.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/86f64cf6bc764564666a32e3e0e87822.png)'
- en: 'Figure 6: More qualitative examples of images generated by different methods
    on intricate prompts.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同方法在复杂提示下生成的图像的更多定性示例。
- en: Results with Human evaluation
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工评估结果
- en: 'To further accurately evaluate the generated images about the alignments with
    human preferences, we further conduct a human evaluation by randomly sampling
    100 prompts from the prompt dataset. Similarly, we ask human evaluators to finish
    the questionnaire used in GPT evaluation. The results are shown in Table [1](#S4.T1
    "Table 1 ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion"), which indicates that our method can still achieve the best performance
    and is consistent with the GPT-4V evaluation results.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更准确地评估生成图像与人类偏好的对齐情况，我们进一步通过随机抽取 100 个提示从提示数据集中进行人工评估。类似地，我们让人工评估者完成 GPT
    评估中使用的问卷。结果见表格[1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent
    for Progressive Multi-Object Diffusion")，表明我们的方法仍能取得最佳性能，并且与 GPT-4V 的评估结果一致。'
- en: 4.2 Ablation study
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 消融研究
- en: In this section, we show ablation results on the effect of the attention blocks
    during diffusion generation and the importance of the VLM feedback control in
    the proposed framework. 50 prompts are randomly sampled from the prompt dataset
    for all experiments in the ablation study.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了注意力块在扩散生成过程中的效果消融结果以及在提议框架中VLM反馈控制的重要性。所有消融实验中的50个提示从提示数据集中随机抽取。
- en: Ablation on the attention blocks
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对注意力块的消融实验
- en: 'As we mentioned at the beginning of Section [4](#S4 "4 Experiments ‣ MuLan:
    Multimodal-LLM Agent for Progressive Multi-Object Diffusion"), there are three
    options for the attention blocks used for backward guidance, i.e., near-input
    blocks, near-middle blocks, and near-output blocks. We empirically found the near-middle
    blocks can achieve the best control and performance for the generation, which
    generally contains the richest semantics. Hence here we show the ablation results
    on different choices of the attention blocks. We utilize SD-v1.4 as the base model,
    and evaluate the performance of different attention blocks under our framework
    by GPT-4V. The results are shown in Table [2](#S4.T2 "Table 2 ‣ Ablation on the
    attention blocks ‣ 4.2 Ablation study ‣ 4 Experiments ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"), which indicates the diffusion
    generation with near-middle blocks can achieve much better results compared to
    the other two options.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在第[4](#S4 "4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion")节开始时提到的，对于用于向后指导的注意力块有三种选择，即near-input块、near-middle块和near-output块。我们经验性地发现，near-middle块可以实现最佳的控制和生成性能，这通常包含最丰富的语义。因此，这里展示了不同选择的注意力块的消融结果。我们使用SD-v1.4作为基础模型，并通过GPT-4V评估在我们框架下不同注意力块的性能。结果见表[2](#S4.T2
    "Table 2 ‣ Ablation on the attention blocks ‣ 4.2 Ablation study ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")，结果表明，使用near-middle块的扩散生成相比其他两个选项可以取得更好的效果。'
- en: 'Table 2: Ablation study on attention blocks with SD-v1.4 as the base model.
    “Objects”, “Attributes”, and “Spatial” denote Object completeness, Attribute bindings,
    and Spatial relationships. The results (evaluated by GPT-4V (OpenAI, [2023](#bib.bib15)))
    show that near-middle attention blocks perform the best for attention guidance.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：以SD-v1.4作为基础模型的注意力块消融研究。“Objects”、“Attributes”和“Spatial”分别表示对象完整性、属性绑定和空间关系。结果（由GPT-4V评估（OpenAI，[2023](#bib.bib15)））显示，near-middle注意力块在注意力指导方面表现最佳。
- en: '| Guidance | Objects | Attributes | Spatial | Overall |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Guidance | Objects | Attributes | Spatial | Overall |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| near-input | 83.67% | 55.10% | 14.29% | 58.37% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| near-input | 83.67% | 55.10% | 14.29% | 58.37% |'
- en: '| near-middle | 97.96% | 80.61% | 30.61% | 77.55% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| near-middle | 97.96% | 80.61% | 30.61% | 77.55% |'
- en: '| near-output | 72.45% | 45.92% | 22.45% | 51.84% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| near-output | 72.45% | 45.92% | 22.45% | 51.84% |'
- en: Ablation on the VLM feedback control
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对VLM反馈控制的消融实验
- en: 'The VLM feedback control is a key componenet in MuLan to provide feedback and
    adjust the generation process to ensure the every stage’s correct generation.
    Here, we show the importance of the feedback by removing feedback control from
    the whole framework. As shown in Table [3](#S4.T3 "Table 3 ‣ Ablation on the VLM
    feedback control ‣ 4.2 Ablation study ‣ 4 Experiments ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"), after removing the VLM, the results
    would be much worse. It is because there is no guarantee or adaptive adjustment
    for each generation stage, which verifies that the feedback control provided by
    the VLM is essential to handle complex prompts. Moreover, we also test MuLan’s
    compatibility with different VLMs. As shown in Table [4](#S4.T4 "Table 4 ‣ Ablation
    on the VLM feedback control ‣ 4.2 Ablation study ‣ 4 Experiments ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion"), we compare the Mulan’s performance
    using different VLMs including LLaVA-1.5 (Liu et al., [2023](#bib.bib13)), GPT-4V (OpenAI,
    [2023](#bib.bib15)), and Gemini-Pro (Team et al., [2023](#bib.bib21)). The results
    show that MuLan could still maintain a good performance with different choices
    of the VLM and achieve good compatibility.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'VLM反馈控制是MuLan中的关键组成部分，用于提供反馈并调整生成过程，以确保每个阶段的正确生成。在这里，我们通过从整个框架中移除反馈控制来展示反馈的重要性。如表[3](#S4.T3
    "Table 3 ‣ Ablation on the VLM feedback control ‣ 4.2 Ablation study ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")所示，移除VLM后，结果会变得更差。这是因为没有对每个生成阶段进行保证或自适应调整，这验证了VLM提供的反馈控制对于处理复杂提示的重要性。此外，我们还测试了MuLan与不同VLM的兼容性。如表[4](#S4.T4
    "Table 4 ‣ Ablation on the VLM feedback control ‣ 4.2 Ablation study ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")所示，我们比较了MuLan使用不同VLM的性能，包括LLaVA-1.5 (刘等，[2023](#bib.bib13))、GPT-4V (OpenAI，[2023](#bib.bib15))和Gemini-Pro (团队等，[2023](#bib.bib21))。结果表明，MuLan在不同VLM的选择下仍能保持良好的性能，并实现良好的兼容性。'
- en: 'Table 3: Ablation study comparing MuLan with vs. without VLM feedback control,
    using SD-v1.4 as the diffusion model and GPT-4 as the judge in evaluations. It
    indicates that feedback control can significantly improve the performance.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：比较MuLan有无VLM反馈控制的消融研究，使用SD-v1.4作为扩散模型，GPT-4作为评估判断。结果表明，反馈控制可以显著提高性能。
- en: '| MuLan | Objects | Attributes | Spatial | Overall |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| MuLan | 对象 | 属性 | 空间 | 总体 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| w/ Feedback | 97.96% | 80.61% | 30.61% | 77.55% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 有反馈 | 97.96% | 80.61% | 30.61% | 77.55% |'
- en: '| w/o Feedback | 81.63% | 59.18% | 18.37% | 60.00% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 无反馈 | 81.63% | 59.18% | 18.37% | 60.00% |'
- en: 'Table 4: Ablation study of the VLM used in MuLan, using SD-v1.4 as the diffusion
    model and GPT-4 as the judge in evaluations. The results show that the choice
    of the VLM would not affect the overall performance too much.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：MuLan中使用的VLM的消融研究，使用SD-v1.4作为扩散模型，GPT-4作为评估判断。结果显示，VLM的选择不会对总体性能产生太大影响。
- en: '| VLM in MuLan | Objects | Attributes | Spatial | Overall |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| MuLan中的VLM | 对象 | 属性 | 空间 | 总体 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaVA-1.5 (Liu et al., [2023](#bib.bib13)) | 97.96% | 80.61% | 30.61% | 77.55%
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA-1.5 (刘等，[2023](#bib.bib13)) | 97.96% | 80.61% | 30.61% | 77.55% |'
- en: '| GPT-4V (OpenAI, [2023](#bib.bib15)) | 95.92% | 80.61% | 28.57% | 76.33% |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V (OpenAI，[2023](#bib.bib15)) | 95.92% | 80.61% | 28.57% | 76.33% |'
- en: '| Gemini-Pro (Team et al., [2023](#bib.bib21)) | 95.92% | 83.67% | 38.78% |
    79.59% |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro (团队等，[2023](#bib.bib21)) | 95.92% | 83.67% | 38.78% | 79.59% |'
- en: 5 Conclusions and Limitations
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与局限性
- en: In this paper, we propose a training-free multimodal-LLM agent (MuLan) to progressively
    generate objects contained in the complicated input prompt with closed-loop feedback
    control, achieving better and more precise control on the whole generation process.
    By first decomposing the complicated prompt into easier sub-tasks, our method
    takes turns to deal with each object, conditioned on the previous one. The VLM
    checker further provides a guarantee with feedback control and adaptive adjustment
    for correct generation at each stage. Extensive experiments demonstrate the superiority
    of MuLan over previous methods, showing the potential of MuLan as a new paradigm
    of controllable diffusion generation. However, there are still limitations to
    be further addressed in the future work. Since the whole generation contains multiple
    stages, depending on the number of objects, it will take a longer time than a
    one-stage generation approach. On the other hand, the LLM planner may mistakenly
    parse the input prompt which results in incorrect decomposition. This could be
    addressed by first re-writing the input prompt by the LLM to facilitate later
    processing.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种无需训练的多模态LLM代理（MuLan），以闭环反馈控制逐步生成复杂输入提示中包含的对象，实现对整个生成过程的更好、更精确的控制。通过首先将复杂提示分解为更简单的子任务，我们的方法轮流处理每个对象，依赖于之前的对象。VLM检查器进一步提供反馈控制和自适应调整的保障，以确保每个阶段的正确生成。大量实验表明，MuLan相较于以往的方法具有优越性，展示了MuLan作为一种新型可控扩散生成范式的潜力。然而，仍然存在未来工作中需进一步解决的局限性。由于整个生成过程包含多个阶段，依赖于对象数量，它将比单阶段生成方法花费更长的时间。另一方面，LLM规划器可能会错误解析输入提示，导致不正确的分解。这可以通过首先由LLM重写输入提示以促进后续处理来解决。
- en: Broader Impact
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: Our work will bring significant advantages to both the research community focused
    on diffusion models and the practical application of T2I generation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作将为专注于扩散模型的研究社区和T2I生成的实际应用带来显著优势。
- en: In terms of the research community, we present a new and novel controllable
    image generation paradigm that demonstrates exceptional controllability and produces
    remarkable results even when tackling challenging tasks. This pioneering approach
    can offer valuable insights for future investigations into diffusion models.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 就研究社区而言，我们提出了一种全新且具有可控性的图像生成范式，这一方法展现了卓越的可控性，即使在面对具有挑战性的任务时也能产生显著的成果。这一开创性的方法能够为未来对扩散模型的研究提供宝贵的见解。
- en: Regarding industrial applications, our method can be readily employed by T2I
    generation service providers to enhance the performance of their models. Moreover,
    the diffusion models operating within our framework are less likely to generate
    harmful content due to the meticulous control exerted at each generation stage.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 关于工业应用，我们的方法可以被T2I生成服务提供商轻松采用，以提升他们模型的性能。此外，由于在每个生成阶段施加了细致的控制，我们框架内运行的扩散模型生成有害内容的可能性较小。
- en: References
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.
    Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman,
    F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., 等. Gpt-4技术报告。*arXiv
    预印本 arXiv:2303.08774*，2023。
- en: Betker et al. (2023) Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li,
    L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation
    with better captions. *Computer Science. https://cdn. openai. com/papers/dall-e-3\.
    pdf*, 2(3), 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Betker 等（2023）Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang,
    L., Zhuang, J., Lee, J., Guo, Y., 等. 通过更好的标题改进图像生成。*计算机科学。 https://cdn.openai.com/papers/dall-e-3.pdf*，2(3)，2023。
- en: 'Chen et al. (2023) Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang,
    Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-$alpha$: Fast training of diffusion
    transformer for photorealistic text-to-image synthesis. *arXiv preprint arXiv:2310.00426*,
    2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2023）Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok,
    J., Luo, P., Lu, H., 等. Pixart-$alpha$: 快速训练扩散变换器以实现 photorealistic 文本到图像合成。*arXiv
    预印本 arXiv:2310.00426*，2023。'
- en: Chen et al. (2024) Chen, M., Laina, I., and Vedaldi, A. Training-free layout
    control with cross-attention guidance. In *Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pp.  5343–5353, 2024.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2024）Chen, M., Laina, I., 和 Vedaldi, A. 无需训练的布局控制与交叉注意力引导。在*IEEE/CVF计算机视觉应用冬季会议论文集*中，页码5343–5353，2024。
- en: Feng et al. (2022) Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A., Narayana,
    P., Basu, S., Wang, X. E., and Wang, W. Y. Training-free structured diffusion
    guidance for compositional text-to-image synthesis. *arXiv preprint arXiv:2212.05032*,
    2022.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng等（2022）Feng, W., He, X., Fu, T.-J., Jampani, V., Akula, A., Narayana, P.,
    Basu, S., Wang, X. E., 和 Wang, W. Y. 无需训练的结构化扩散指导用于组合文本到图像合成。*arXiv预印本 arXiv:2212.05032*，2022年。
- en: 'Feng et al. (2023) Feng, W., Zhu, W., Fu, T.-j., Jampani, V., Akula, A., He,
    X., Basu, S., Wang, X. E., and Wang, W. Y. Layoutgpt: Compositional visual planning
    and generation with large language models. *arXiv preprint arXiv:2305.15393*,
    2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng等（2023）Feng, W., Zhu, W., Fu, T.-j., Jampani, V., Akula, A., He, X., Basu,
    S., Wang, X. E., 和 Wang, W. Y. Layoutgpt: 大型语言模型的组合视觉规划与生成。*arXiv预印本 arXiv:2305.15393*，2023年。'
- en: Hao et al. (2022) Hao, Y., Chi, Z., Dong, L., and Wei, F. Optimizing prompts
    for text-to-image generation. *arXiv preprint arXiv:2212.09611*, 2022.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao等（2022）Hao, Y., Chi, Z., Dong, L., 和 Wei, F. 优化文本到图像生成的提示。*arXiv预印本 arXiv:2212.09611*，2022年。
- en: 'Hessel et al. (2021) Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and
    Choi, Y. Clipscore: A reference-free evaluation metric for image captioning. *arXiv
    preprint arXiv:2104.08718*, 2021.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hessel等（2021）Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., 和 Choi, Y.
    Clipscore: 一种无参考的图像字幕评价指标。*arXiv预印本 arXiv:2104.08718*，2021年。'
- en: Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic
    models. *Advances in neural information processing systems*, 33:6840–6851, 2020.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho等（2020）Ho, J., Jain, A., 和 Abbeel, P. 去噪扩散概率模型。*神经信息处理系统进展*，33:6840–6851，2020年。
- en: 'Huang et al. (2023) Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-compbench:
    A comprehensive benchmark for open-world compositional text-to-image generation.
    *arXiv preprint arXiv:2307.06350*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang等（2023）Huang, K., Sun, K., Xie, E., Li, Z., 和 Liu, X. T2i-compbench: 开放世界组合文本到图像生成的全面基准测试。*arXiv预印本
    arXiv:2307.06350*，2023年。'
- en: 'Li et al. (2023) Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C.,
    and Lee, Y. J. Gligen: Open-set grounded text-to-image generation. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  22511–22521,
    2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等（2023）Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., 和 Lee,
    Y. J. Gligen: 开放集基础文本到图像生成。见*IEEE/CVF计算机视觉与模式识别会议论文集*，第22511–22521页，2023年。'
- en: 'Lian et al. (2023) Lian, L., Li, B., Yala, A., and Darrell, T. Llm-grounded
    diffusion: Enhancing prompt understanding of text-to-image diffusion models with
    large language models. *arXiv preprint arXiv:2305.13655*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lian等（2023）Lian, L., Li, B., Yala, A., 和 Darrell, T. Llm-grounded diffusion:
    用大型语言模型增强文本到图像扩散模型的提示理解。*arXiv预印本 arXiv:2305.13655*，2023年。'
- en: Liu et al. (2023) Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines
    with visual instruction tuning. *arXiv preprint arXiv:2310.03744*, 2023.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2023）Liu, H., Li, C., Li, Y., 和 Lee, Y. J. 通过视觉指令调优改进基线。*arXiv预印本 arXiv:2310.03744*，2023年。
- en: 'Nichol et al. (2021) Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,
    P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image
    generation and editing with text-guided diffusion models. *arXiv preprint arXiv:2112.10741*,
    2021.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nichol等（2021）Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P.,
    McGrew, B., Sutskever, I., 和 Chen, M. Glide: 通过文本引导扩散模型迈向真实感图像生成与编辑。*arXiv预印本
    arXiv:2112.10741*，2021年。'
- en: OpenAI (2023) OpenAI. Gpt-4v(ision) system card. 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. Gpt-4v(ision)系统卡。2023年。
- en: 'Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,
    T., Müller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models
    for high-resolution image synthesis. *arXiv preprint arXiv:2307.01952*, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Podell等（2023）Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T.,
    Müller, J., Penna, J., 和 Rombach, R. Sdxl: 改进潜在扩散模型以进行高分辨率图像合成。*arXiv预印本 arXiv:2307.01952*，2023年。'
- en: Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
    Ommer, B. High-resolution image synthesis with latent diffusion models. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp.  10684–10695,
    2022.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach等（2022）Rombach, R., Blattmann, A., Lorenz, D., Esser, P., 和 Ommer, B.
    高分辨率图像合成与潜在扩散模型。见*IEEE/CVF计算机视觉与模式识别会议论文集*，第10684–10695页，2022年。
- en: Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J.,
    Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans,
    T., et al. Photorealistic text-to-image diffusion models with deep language understanding.
    *Advances in Neural Information Processing Systems*, 35:36479–36494, 2022.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saharia等（2022）Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,
    E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., 等.
    具有深度语言理解的真实感文本到图像扩散模型。*神经信息处理系统进展*，35:36479–36494，2022年。
- en: Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan,
    N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *International conference on machine learning*, pp.  2256–2265\. PMLR, 2015.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein 等（2015）Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., 和
    Ganguli, S. 使用非平衡热力学的深度无监督学习。在*国际机器学习会议*，第2256–2265页。PMLR，2015。
- en: Song et al. (2020) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon,
    S., and Poole, B. Score-based generative modeling through stochastic differential
    equations. *arXiv preprint arXiv:2011.13456*, 2020.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2020）Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S.,
    和 Poole, B. 基于分数的生成建模通过随机微分方程。*arXiv 预印本 arXiv:2011.13456*，2020。
- en: 'Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B.,
    Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family
    of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team 等（2023）Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J.,
    Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., 等。Gemini: 一系列高能力的多模态模型。*arXiv
    预印本 arXiv:2312.11805*，2023。'
- en: Appendix A Detailed prompt template of the global planning by the LLM
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A LLM 全球规划的详细提示模板
- en: 'As stated in Section [3.3](#S3.SS3 "3.3 Prompt Decomposition by LLM Planning
    ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion"), MuLan first conduct the global planning to decompose
    the input prompts into $N$ objects before the whole generation process. To this
    end, given the input prompt p, we prompt the LLM using the following template:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '如[3.3节](#S3.SS3 "3.3 Prompt Decomposition by LLM Planning ‣ 3 Multimodal-LLM
    Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")所述，MuLan
    首先进行全球规划，将输入提示分解为 $N$ 个对象，然后再进行整个生成过程。为此，给定输入提示 p，我们使用以下模板提示 LLM：'
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. You only need to list the objects
    in the description by painting order, from left to right, from down to top. Do
    not list additional information other than the objects mentioned in the description.
    Description: {p}.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个出色的画家。我会给你一些描述。你的任务是将描述转化为一幅画。你只需按绘画顺序列出描述中的对象，从左到右，从下到上。不要列出描述中未提及的其他信息。描述：{p}。
- en: In this way, the LLM will decompose the input prompt p following the pre-defined
    order.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，LLM 将按照预定义的顺序分解输入提示 p。
- en: Appendix B Detailed prompt template of the local planning by the LLM
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LLM 局部规划的详细提示模板
- en: 'As stated in Section [3.4](#S3.SS4 "3.4 Conditional Single-Object Diffusion
    with LLM Planning and Attention Guidance ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan:
    Multimodal-LLM Agent for Progressive Multi-Object Diffusion"), the LLM is also
    utilized during the generation stage for local planning of the object’s rough
    position and the object counting.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '如[3.4节](#S3.SS4 "3.4 Conditional Single-Object Diffusion with LLM Planning
    and Attention Guidance ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion")所述，LLM 还在生成阶段用于对象粗略位置的局部规划和对象计数。'
- en: 'For the rough position $\texttt{opt}_{1}$ planning of the first object, we
    utilize the following template:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个对象的粗略位置$\texttt{opt}_{1}$规划，我们使用以下模板：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I want to paint the {$\texttt{obj}_{1}$}? Choose from left, right, top, and bottom.
    You can make reasonable guesses. Give one answer.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个出色的画家。我会给你一些描述。你的任务是将描述转化为一幅画。现在给出的描述是：{p}。如果我想画{$\texttt{obj}_{1}$}？从左、右、上和下中选择。你可以做出合理的猜测。给出一个答案。
- en: Then the LLM is prompted to figure out the object number based on $\texttt{opt}_{1}$.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 LLM 将根据$\texttt{opt}_{1}$来确定对象数量。
- en: 'If $\texttt{opt}_{1}=\texttt{left}$ is:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\texttt{opt}_{1}=\texttt{left}$是：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. How
    many non-overlapping objects are there in the horizontal direction? ONLY give
    the final number.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个出色的画家。我会给你一些描述。你的任务是将描述转化为一幅画。现在给出的描述是：{p}。在水平方向上有多少个不重叠的对象？仅给出最终的数字。
- en: 'If $\texttt{opt}_{1}=\texttt{bottom}$, the prompt template would be:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\texttt{opt}_{1}=\texttt{bottom}$，提示模板将是：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. How
    many non-overlapping objects are there in the vertical direction? ONLY give the
    final number.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一位出色的画家。我将给你一些描述。你的任务是将描述转化为一幅画。现在给定描述：{p}。在垂直方向上有多少个不重叠的物体？只给出最终的数字。
- en: 'For the rough position $\texttt{opt}_{n}(n\geq 2)$, we utilize the following
    template:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于粗略位置$\texttt{opt}_{n}(n\geq 2)$，我们使用以下模板：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I already have a painting that contains {$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}?
    Choose from left, right, above, bottom, and none of above. You can make reasonable
    guesses. Give one answer.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一位出色的画家。我将给你一些描述。你的任务是将描述转化为一幅画。现在给定描述：{p}。如果我已经有一幅包含{$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}的画？选择左边、右边、上方、下方或以上都不是。你可以做出合理的猜测。给出一个答案。
- en: 'Then we prompt the LLM to figure out the object number by:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们提示LLM通过以下方式确定物体数量：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I already have a painting that contains {$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}?
    Only give the final number.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一位出色的画家。我将给你一些描述。你的任务是将描述转化为一幅画。现在给定描述：{p}。如果我已经有一幅包含{$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}的画？只给出最终的数字。
- en: Appendix C More details on the overlapping processing
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 重叠处理的更多细节
- en: Given $\texttt{opt}_{n}$ can be computed as
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 给定$\texttt{opt}_{n}$可以计算为
- en: '|  | $$\displaystyle\bm{M}_{n,i}=\begin{cases}\Big{(}\tilde{x}_{n-1}\cdot r_{i}+(\tilde{x}_{n-1}+\tilde{w}_{n-1})\cdot(1-r_{i}),\tilde{y}_{n-1},\tilde{w}_{n-1}\cdot
    r_{i}+\frac{W-\tilde{x}_{n-1}-\tilde{w}_{n-1}}{\texttt{Num}^{n}},\tilde{h}_{n-1}\Big{)},\text{if
    }\texttt{opt}^{n}=\texttt{right},\\ \\'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\bm{M}_{n,i}=\begin{cases}\Big{(}\tilde{x}_{n-1}\cdot r_{i}+(\tilde{x}_{n-1}+\tilde{w}_{n-1})\cdot(1-r_{i}),\tilde{y}_{n-1},\tilde{w}_{n-1}\cdot
    r_{i}+\frac{W-\tilde{x}_{n-1}-\tilde{w}_{n-1}}{\texttt{Num}^{n}},\tilde{h}_{n-1}\Big{)},\text{if
    }\texttt{opt}^{n}=\texttt{right},\\ \\'
- en: \Big{(}\tilde{x}_{n-1},\frac{(\texttt{Num}^{n}-1)\cdot\tilde{y}_{n-1}}{\texttt{Num}^{n}},\tilde{w}_{n-1},\tilde{h}_{n-1}\cdot
    r_{i}+\frac{\tilde{y}_{n-1}}{\texttt{Num}^{n}}\Big{)},\text{if }\texttt{opt}^{n}=\texttt{top}.\end{cases}$$
    |  | (8) |
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: \Big{(}\tilde{x}_{n-1},\frac{(\texttt{Num}^{n}-1)\cdot\tilde{y}_{n-1}}{\texttt{Num}^{n}},\tilde{w}_{n-1},\tilde{h}_{n-1}\cdot
    r_{i}+\frac{\tilde{y}_{n-1}}{\texttt{Num}^{n}}\Big{)},\text{if }\texttt{opt}^{n}=\texttt{top}.\end{cases}$$
    |  | (8) |
- en: 'The illustration for different overlapping ratios is shown in Figure [7](#A3.F7
    "Figure 7 ‣ Appendix C More details on the overlapping processing ‣ MuLan: Multimodal-LLM
    Agent for Progressive Multi-Object Diffusion").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '不同重叠比的示意图见图[7](#A3.F7 "Figure 7 ‣ Appendix C More details on the overlapping
    processing ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion")。'
- en: '![Refer to caption](img/2e78c0fd2cf02630e7c5408947de4ee8.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2e78c0fd2cf02630e7c5408947de4ee8.png)'
- en: 'Figure 7: Three candidate masks $\bm{M}_{n,i}$.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：三个候选掩膜$\bm{M}_{n,i}$。
- en: Appendix D More details on the evaluation questionnaire
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 评估问卷的更多细节
- en: 'As shown in Section [4](#S4 "4 Experiments ‣ MuLan: Multimodal-LLM Agent for
    Progressive Multi-Object Diffusion"), we design a questionnaire to comprehensively
    evaluate the alignment between the generated image and the text by GPT-4V (OpenAI,
    [2023](#bib.bib15)) and human, from three aspects - object completeness, correctness
    of attribute bindings, and correctness of spatial relationships. Specifically,
    given an image and a text prompt, for object completeness, we will evaluate if
    the image contains each single object in the prompt. If the object appears in
    the image, we will then judge if the attribute bindings of the object in the image
    align with the corresponding attribute bindings in the text prompt, to evaluate
    the correctness of attribute bindings. We will also ask GPT-4V or human to judge
    if the spatial relationships are correct and match the text, as the evaluation
    of the spatial relationships.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[4](#S4 "4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object
    Diffusion")节所示，我们设计了一份问卷来全面评估生成的图像与GPT-4V（OpenAI，[2023](#bib.bib15)）和人类文本之间的对齐情况，评估内容包括
    - 物体完整性、属性绑定的正确性和空间关系的正确性。具体来说，给定一幅图像和一个文本提示，对于物体完整性，我们将评估图像是否包含提示中的每个单独物体。如果物体出现在图像中，我们将判断图像中物体的属性绑定是否与文本提示中的相应属性绑定一致，以评估属性绑定的正确性。我们还将要求GPT-4V或人类判断空间关系是否正确并与文本匹配，以评估空间关系。'
- en: 'Examples of the questionnaire for different images and text prompts are shown
    in Figure [8](#A4.F8 "Figure 8 ‣ Appendix D More details on the evaluation questionnaire
    ‣ MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion").'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '不同图像和文本提示的问卷示例如图[8](#A4.F8 "图 8 ‣ 附录 D 更多关于评估问卷的细节 ‣ MuLan: 多模态LLM代理用于渐进式多目标扩散")所示。'
- en: '![Refer to caption](img/d52a88ce7f6619b631c7a83f32248c86.png)![Refer to caption](img/f1cbcc625be0cab74a23aca5fcb67c04.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d52a88ce7f6619b631c7a83f32248c86.png)![参考说明](img/f1cbcc625be0cab74a23aca5fcb67c04.png)'
- en: 'Figure 8: Illustration of the questionnaire for the evaluation of generated
    images'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 生成图像评估问卷的示例'
- en: Appendix E More qualitative results
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 更多定性结果
- en: 'We show more examples of different methods in Figure [9](#A5.F9 "Figure 9 ‣
    Appendix E More qualitative results ‣ MuLan: Multimodal-LLM Agent for Progressive
    Multi-Object Diffusion").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[9](#A5.F9 "图 9 ‣ 附录 E 更多定性结果 ‣ MuLan: 多模态LLM代理用于渐进式多目标扩散")中展示了不同方法的更多示例。'
- en: '![Refer to caption](img/b1b08ddef8a8425379e74b9c4a58958d.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b1b08ddef8a8425379e74b9c4a58958d.png)'
- en: 'Figure 9: More qualitative examples of images generated by different methods
    on intricate prompts.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 使用不同方法生成的复杂提示的更多定性图像示例。'
