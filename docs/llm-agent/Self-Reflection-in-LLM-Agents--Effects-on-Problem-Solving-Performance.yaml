- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:46:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:46:36'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Self-Reflection in LLM Agents: Effects on Problem-Solving Performance'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM代理中的自我反思：对问题解决表现的影响
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06682](https://ar5iv.labs.arxiv.org/html/2405.06682)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06682](https://ar5iv.labs.arxiv.org/html/2405.06682)
- en: Matthew Renze
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 马修·伦泽
- en: Johns Hopkins University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰斯·霍普金斯大学
- en: mrenze1@jhu.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: mrenze1@jhu.edu
- en: '&Erhan Guven'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&厄尔汉·古文'
- en: Johns Hopkins University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰斯·霍普金斯大学
- en: eguven2@jhu.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: eguven2@jhu.edu
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this study, we investigated the effects of self-reflection in large language
    models (LLMs) on problem-solving performance. We instructed nine popular LLMs
    to answer a series of multiple-choice questions to provide a performance baseline.
    For each incorrectly answered question, we instructed eight types of self-reflecting
    LLM agents to reflect on their mistakes and provide themselves with guidance to
    improve problem-solving. Then, using this guidance, each self-reflecting agent
    attempted to re-answer the same questions. Our results indicate that LLM agents
    are able to significantly improve their problem-solving performance through self-reflection
    ($p<0.001$). In addition, we compared the various types of self-reflection to
    determine their individual contribution to performance. All code and data are
    available on GitHub at [https://github.com/matthewrenze/self-reflection](https://github.com/matthewrenze/self-reflection)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们调查了自我反思在大型语言模型（LLMs）中的作用对问题解决表现的影响。我们指示九个流行的LLMs回答一系列多项选择题，以提供表现基准。对于每个回答错误的问题，我们指示八种自我反思的LLM代理对其错误进行反思，并为自己提供改进问题解决的指导。然后，利用这些指导，每个自我反思的代理尝试重新回答相同的问题。我们的结果表明，LLM代理通过自我反思能够显著提高其问题解决表现（$p<0.001$）。此外，我们还比较了各种类型的自我反思，以确定它们对表现的个体贡献。所有代码和数据均可在GitHub上获取，网址为[https://github.com/matthewrenze/self-reflection](https://github.com/matthewrenze/self-reflection)
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 1.1 Background
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 背景
- en: Self-reflection is a process in which a person thinks about their thoughts,
    feelings, and behaviors. In the context of problem-solving, self-reflection allows
    us to inspect the thought process leading to our solution. This type of self-reflection
    aims to avoid making similar errors when confronted with similar problems in the
    future.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自我反思是一个人思考自己的思想、感受和行为的过程。在问题解决的背景下，自我反思使我们能够检查导致我们解决方案的思维过程。这种自我反思旨在避免在遇到类似问题时犯相似的错误。
- en: Like humans, large language model (LLM) agents can be instructed to produce
    a chain of thought (CoT) before answering a question. CoT prompting has been shown
    to significantly improve LLM performance on a variety of problem-solving tasks
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]. However, LLMs still often make
    errors in their CoT due to logic errors, mathematical errors, hallucination, etc.
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类类似，大型语言模型（LLM）代理可以被指示在回答问题之前产生一系列思维过程（CoT）。CoT提示已被证明能够显著提高LLM在各种问题解决任务中的表现
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]。然而，由于逻辑错误、数学错误、幻觉等原因，LLM在其CoT中仍然经常出现错误
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]。
- en: Also similar to humans, LLM agents can be instructed to reflect on their own
    CoT. This allows them to identify errors, explain the cause of these errors, and
    generate advice to avoid making similar types of errors in the future [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类类似，LLM代理也可以被指示对其自身的CoT进行反思。这使得它们能够识别错误，解释这些错误的原因，并生成避免在未来犯类似错误的建议 [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)]。
- en: Our research investigates the use of self-reflection in LLM agents to improve
    their problem-solving capabilities.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究调查了在LLM代理中使用自我反思来提高其问题解决能力。
- en: 1.2 Prior Literature
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 相关文献
- en: Over the past few years, we’ve seen the emergence of AI agents based on LLM
    architectures [[16](#bib.bib16), [17](#bib.bib17)]. These agents have demonstrated
    impressive capabilities in solving multi-step problems [[18](#bib.bib18), [19](#bib.bib19),
    [10](#bib.bib10)]. In addition, they’ve been observed successfully using tools,
    including web browsers, search engines, code interpreters, etc. [[20](#bib.bib20),
    [19](#bib.bib19), [10](#bib.bib10), [21](#bib.bib21)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，我们见证了基于LLM架构的AI代理的出现[[16](#bib.bib16), [17](#bib.bib17)]。这些代理在解决多步骤问题方面表现出了令人印象深刻的能力[[18](#bib.bib18),
    [19](#bib.bib19), [10](#bib.bib10)]。此外，它们被观察到成功使用了包括网页浏览器、搜索引擎、代码解释器等工具[[20](#bib.bib20),
    [19](#bib.bib19), [10](#bib.bib10), [21](#bib.bib21)]。
- en: However, these LLM agents have several imitations. They have limited knowledge,
    make errors in reasoning, hallucinate output, and get stuck in unproductive loops
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些LLM代理有几个局限性。它们的知识有限，推理错误，产生虚假的输出，并陷入无效的循环[[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]。
- en: To improve their performance, we can provide them with a series of cognitive
    capabilities. For example, we can provide them with a CoT [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)], access to external memory [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)], and the ability to learn from feedback [[18](#bib.bib18),
    [10](#bib.bib10), [19](#bib.bib19)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升它们的表现，我们可以为它们提供一系列认知能力。例如，我们可以提供给它们CoT[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]、访问外部记忆[[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]以及从反馈中学习的能力[[18](#bib.bib18),
    [10](#bib.bib10), [19](#bib.bib19)]。
- en: Learning from feedback can be decomposed into several components. These components
    include the source of the feedback, the type of feedback, and the strategy used
    to learn from feedback [[11](#bib.bib11)]. There are two sources of feedback (e.g.,
    internal or external feedback) and two main types of feedback (e.g., scalar values
    or natural language) [[11](#bib.bib11), [12](#bib.bib12)].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从反馈中学习可以被分解为几个组成部分。这些组成部分包括反馈的来源、反馈的类型以及用于从反馈中学习的策略[[11](#bib.bib11)]。反馈有两种来源（例如，内部反馈或外部反馈）和两种主要类型（例如，标量值或自然语言）[[11](#bib.bib11),
    [12](#bib.bib12)]。
- en: There are also several strategies for learning from feedback. These strategies
    depend on where they occur in the LLM’s output-generation process. They can occur
    at model-training time, output-generation time, or after the output has been generated.
    Within each of these three phases, there are various techniques available (e.g.,
    model fine-tuning, output re-ranking, and self-correction) [[11](#bib.bib11)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从反馈中学习也有几种策略。这些策略取决于它们发生在LLM输出生成过程中的位置。它们可以发生在模型训练时、输出生成时或输出生成后。在这三个阶段中，每个阶段都有各种技术（例如，模型微调、输出重新排序和自我修正）[[11](#bib.bib11)]。
- en: In terms of learning from self-correction, various methods are currently being
    investigated. These include iterative refinement, multi-model debate, and self-reflection
    [[11](#bib.bib11)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在自我修正学习方面，目前正在研究各种方法。这些方法包括迭代优化、多模型辩论和自我反思[[11](#bib.bib11)]。
- en: Self-reflection in LLM agents is a meta-cognitive strategy also known as introspection
    [[13](#bib.bib13), [14](#bib.bib14)]. Some research studies have indicated that
    LLMs using self-reflection are able to identify and correct their mistakes [[12](#bib.bib12),
    [10](#bib.bib10), [8](#bib.bib8), [15](#bib.bib15)]. Others have indicated that
    LLMs cannot identify errors in their reasoning; regardless, they still may be
    able to correct them with external feedback [[7](#bib.bib7), [26](#bib.bib26)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM代理中，自我反思是一种元认知策略，也称为内省[[13](#bib.bib13), [14](#bib.bib14)]。一些研究表明，使用自我反思的LLM能够识别并纠正其错误[[12](#bib.bib12),
    [10](#bib.bib10), [8](#bib.bib8), [15](#bib.bib15)]。另一些研究则表明，LLM无法识别其推理中的错误；不过，它们可能仍然能够通过外部反馈进行纠正[[7](#bib.bib7),
    [26](#bib.bib26)]。
- en: 1.3 Contribution
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 贡献
- en: Our research builds upon the prior literature by determining which aspects of
    self-reflection are most beneficial in improving an LLM agent’s performance on
    problem-solving tasks. It decomposes the process of self-reflection into several
    components and identifies how each component contributes to the agent’s overall
    increase in performance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究基于以往的文献，确定哪些自我反思的方面在提高LLM代理解决问题任务的表现方面最为有利。它将自我反思的过程分解为几个组成部分，并识别每个组成部分如何促进代理整体性能的提升。
- en: In addition, it provides insight into which types of LLMs and problem domains
    benefit most from each type of self-reflection. These include LLMs like GPT-4,
    Llama 2 70B, and Gemini 1.5 Pro. It also includes various problem domains such
    as math, science, medicine, etc.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还提供了哪些类型的LLM和问题领域最能从每种类型的自我反思中受益的见解。这些包括像GPT-4、Llama 2 70B和Gemini 1.5 Pro这样的LLM。它还包括数学、科学、医学等各种问题领域。
- en: This information is useful to AI engineers attempting to build LLM agents with
    self-reflection capabilities. In addition, it is valuable to AI researchers studying
    metacognition in LLM agents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息对那些试图构建具有自我反思能力的LLM代理的AI工程师非常有用。此外，它对研究LLM代理中元认知的AI研究人员也具有重要价值。
- en: 2 Methods
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 2.1 Data
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据
- en: Our test dataset consists of a set of multiple-choice question-and-answer (MCQA)
    problems derived from popular LLM benchmarks. These benchmarks include ARC, AGIEval,
    HellaSwag, MedMCQA, etc. [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试数据集由一组多项选择问答（MCQA）问题组成，这些问题来自流行的LLM基准测试。这些基准测试包括ARC、AGIEval、HellaSwag、MedMCQA等。[[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)]。
- en: We preprocessed and converted these datasets into a standardized format. Then,
    we randomly selected 100 questions from each of the ten datasets to create a multi-domain
    exam with 1,000 problems.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这些数据集进行了预处理和转换，标准化为统一的格式。然后，我们从十个数据集中随机选择了100个问题，创建了一个包含1,000道题目的多领域考试。
- en: 'For a complete list of the source problem sets used to create the MCQA exam,
    see Table [1](#S2.T1 "Table 1 ‣ 2.1 Data ‣ 2 Methods ‣ Self-Reflection in LLM
    Agents: Effects on Problem-Solving Performance"). For a sample of an MCQA problem,
    see Figure [5](#A2.F5 "Figure 5 ‣ Appendix B Data ‣ Self-Reflection in LLM Agents:
    Effects on Problem-Solving Performance") in the appendix.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '关于用于创建MCQA考试的源问题集的完整列表，请参见表格[1](#S2.T1 "Table 1 ‣ 2.1 Data ‣ 2 Methods ‣ Self-Reflection
    in LLM Agents: Effects on Problem-Solving Performance")。有关MCQA问题的示例，请参见附录中的图[5](#A2.F5
    "Figure 5 ‣ Appendix B Data ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving
    Performance")。'
- en: 'Table 1: Problem sets used to create the 1,000-question multi-domain MCQA exam.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表格1：用于创建1,000道题目的多领域MCQA考试的问题集。
- en: '| Problem Set | Benchmark | Domain | Questions | License | Source |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 问题集 | 基准 | 领域 | 问题数 | 许可证 | 来源 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ARC Challenge Test | ARC | Science | 1,173 | CC BY-SA | [[27](#bib.bib27)]
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| ARC Challenge Test | ARC | 科学 | 1,173 | CC BY-SA | [[27](#bib.bib27)] |'
- en: '| AQUA-RAT | AGI Eval | Math | 254 | Apache v2.0 | [[30](#bib.bib30)] |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| AQUA-RAT | AGI Eval | 数学 | 254 | Apache v2.0 | [[30](#bib.bib30)] |'
- en: '| Hellaswag Val | Hellaswag | Common Sense Reasoning | 10,042 | MIT | [[28](#bib.bib28)]
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Hellaswag Val | Hellaswag | 常识推理 | 10,042 | MIT | [[28](#bib.bib28)] |'
- en: '| LogiQA (English) | AGI Eval | Logic | 651 | GitHub | [[30](#bib.bib30), [31](#bib.bib31)]
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| LogiQA (英语) | AGI Eval | 逻辑 | 651 | GitHub | [[30](#bib.bib30), [31](#bib.bib31)]
    |'
- en: '| LSAT-AR | AGI Eval | Law (Analytic Reasoning) | 230 | MIT | [[30](#bib.bib30),
    [32](#bib.bib32)] |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LSAT-AR | AGI Eval | 法律（分析推理） | 230 | MIT | [[30](#bib.bib30), [32](#bib.bib32)]
    |'
- en: '| LSAT-LR | AGI Eval | Law (Logical Reasoning) | 510 | MIT | [[30](#bib.bib30),
    [32](#bib.bib32)] |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| LSAT-LR | AGI Eval | 法律（逻辑推理） | 510 | MIT | [[30](#bib.bib30), [32](#bib.bib32)]
    |'
- en: '| LSAT-RC | AGI Eval | Law (Reading Comprehension) | 260 | MIT | [[30](#bib.bib30),
    [32](#bib.bib32)] |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| LSAT-RC | AGI Eval | 法律（阅读理解） | 260 | MIT | [[30](#bib.bib30), [32](#bib.bib32)]
    |'
- en: '| MedMCQA Valid | MedMCQA | Medicine | 6,150 | MIT | [[29](#bib.bib29)] |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| MedMCQA Valid | MedMCQA | 医学 | 6,150 | MIT | [[29](#bib.bib29)] |'
- en: '| SAT-English | AGI Eval | English | 206 | MIT | [[30](#bib.bib30)] |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| SAT-英语 | AGI Eval | 英语 | 206 | MIT | [[30](#bib.bib30)] |'
- en: '| SAT-Math | AGI Eval | Math | 220 | MIT | [[30](#bib.bib30)] |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| SAT-数学 | AGI Eval | 数学 | 220 | MIT | [[30](#bib.bib30)] |'
- en: 'Note: The GitHub repository for LogiQA does not include a license file. However,
    both the paper and readme.md file states that "The dataset is freely available."'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：LogiQA的GitHub存储库不包括许可证文件。然而，论文和readme.md文件中都指出“数据集是免费提供的。”
- en: 2.2 Models
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 模型
- en: We evaluated our agents using nine popular LLMs, including GPT-4, Llama 2 70B,
    Google Gemini, etc. [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47)]. All models were accessed via cloud-based APIs hosted by Microsoft,
    Anthropic, and Google.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用包括GPT-4、Llama 2 70B、Google Gemini等在内的九种流行LLM对我们的代理进行了评估。[[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)]。所有模型都通过微软、Anthropic和Google托管的云API进行访问。
- en: Each of these LLMs has its own unique strengths and weaknesses. For example,
    LLMs like GPT-4, Gemini 1.5 Pro, and Claude Opus are powerful LLMs with a large
    number of parameters [[44](#bib.bib44), [40](#bib.bib40), [34](#bib.bib34)]. However,
    they have a significantly higher cost per token than smaller models like GPT-3.5
    and Llama 2 7B [[42](#bib.bib42), [46](#bib.bib46)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些LLM各有其独特的优缺点。例如，像GPT-4、Gemini 1.5 Pro和Claude Opus这样的LLM是功能强大的LLM，具有大量参数[[44](#bib.bib44),
    [40](#bib.bib40), [34](#bib.bib34)]。然而，它们的每个令牌的成本显著高于像GPT-3.5和Llama 2 7B这样的较小模型[[42](#bib.bib42),
    [46](#bib.bib46)]。
- en: 'For a complete list of LLMs used in our experiment, see Table [2](#S2.T2 "Table
    2 ‣ 2.2 Models ‣ 2 Methods ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving
    Performance").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有关我们实验中使用的LLM的完整列表，请参见表[2](#S2.T2 "表 2 ‣ 2.2 模型 ‣ 2 方法 ‣ LLM代理中的自我反思：对问题解决性能的影响")。
- en: 'Table 2: LLMs used in the experiment.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：实验中使用的LLM。
- en: '| Name | Vendor | Released | License | Source |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 供应商 | 发布日期 | 许可 | 来源 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Claude 3 Opus | Anthropic | 2024-03-04 | Closed | [[33](#bib.bib33), [34](#bib.bib34)]
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Opus | Anthropic | 2024-03-04 | Closed | [[33](#bib.bib33), [34](#bib.bib34)]
    |'
- en: '| Command R+ | Cohere | 2024-04-04 | Open | [[35](#bib.bib35), [36](#bib.bib36)]
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Command R+ | Cohere | 2024-04-04 | Open | [[35](#bib.bib35), [36](#bib.bib36)]
    |'
- en: '| Gemini 1.0 Pro | Google | 2023-12-06 | Closed | [[37](#bib.bib37), [38](#bib.bib38)]
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Gemini 1.0 Pro | Google | 2023-12-06 | Closed | [[37](#bib.bib37), [38](#bib.bib38)]
    |'
- en: '| Gemini 1.5 Pro (Preview) | Google | 2024-02-15 | Closed | [[39](#bib.bib39),
    [40](#bib.bib40)] |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Gemini 1.5 Pro (Preview) | Google | 2024-02-15 | Closed | [[39](#bib.bib39),
    [40](#bib.bib40)] |'
- en: '| GPT-3.5 Turbo | OpenAI | 2022-11-30 | Closed | [[41](#bib.bib41), [42](#bib.bib42)]
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 Turbo | OpenAI | 2022-11-30 | Closed | [[41](#bib.bib41), [42](#bib.bib42)]
    |'
- en: '| GPT-4 | OpenAI | 2023-03-14 | Closed | [[43](#bib.bib43), [44](#bib.bib44)]
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | OpenAI | 2023-03-14 | Closed | [[43](#bib.bib43), [44](#bib.bib44)]
    |'
- en: '| Llama 2 7B Chat | Meta | 2023-07-18 | Open | [[45](#bib.bib45), [46](#bib.bib46)]
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 7B Chat | Meta | 2023-07-18 | Open | [[45](#bib.bib45), [46](#bib.bib46)]
    |'
- en: '| Llama 2 70B Chat | Meta | 2023-07-18 | Open | [[45](#bib.bib45), [46](#bib.bib46)]
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 70B Chat | Meta | 2023-07-18 | Open | [[45](#bib.bib45), [46](#bib.bib46)]
    |'
- en: '| Mistral Large | Mistral AI | 2024-02-26 | Closed | [[47](#bib.bib47)] |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Mistral Large | Mistral AI | 2024-02-26 | Closed | [[47](#bib.bib47)] |'
- en: 2.3 Agents
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 代理
- en: We investigated eight types of self-reflecting LLM agents. These agents reflect
    upon their own CoT and then generate self-reflections to use when attempting to
    re-answer questions. Each of these agents uses a unique type of self-reflection
    to assist it. We also included a single non-reflecting (i.e., Baseline) agent
    as our control.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了八种自我反思LLM代理。这些代理对自己的CoT进行反思，然后生成自我反思，以便在尝试重新回答问题时使用。每种代理都使用一种独特的自我反思来辅助其工作。我们还包括了一个没有自我反思能力（即基线）的代理作为对照。
- en: 'Listed below are the various types of agents and the type of self-reflection
    they generate and use to re-answer questions:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下列列出了各种类型的代理及其生成和使用的自我反思类型：
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Baseline - no self-reflection capabilities.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基线 - 无自我反思能力。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Retry - informed that it answered incorrectly and simply tries again.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重试 - 被告知回答错误后，简单地重新尝试。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Keywords - a list of keywords for each type of error.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键词 - 每种错误类型的关键词列表。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Advice - a list of general advice for improvement.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 建议 - 改进的一般建议列表。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Explanation - an explanation of why it made an error.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解释 - 解释为什么出现了错误。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Instructions - an ordered list of instructions for how to solve the problem.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指令 - 解决问题的有序指令列表。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Solution - a step-by-step solution to the problem.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解决方案 - 问题的逐步解决方案。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Composite - all six types of self-reflections.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合 - 六种自我反思类型的综合。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unredacted - all six types without the answers redacted
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未编辑 - 六种类型的原始数据，答案未被编辑
- en: 'The Baseline agent is our control for the experiment and a lower bound for
    the scores. It informs us how well the base model answers the question without
    using any self-reflection. The Baseline agent used standard prompt-engineering
    techniques, including domain expertise, CoT, conciseness, and few-shot prompting
    [[48](#bib.bib48), [49](#bib.bib49), [1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)].
    The sampling temperature was set to 0.0 for all LLMs to improve reproducibility
    [[50](#bib.bib50)]. See Figure [6](#A2.F6 "Figure 6 ‣ Appendix B Data ‣ Self-Reflection
    in LLM Agents: Effects on Problem-Solving Performance") in the appendix for an
    example of the Baseline answer prompt.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '基线代理人是我们实验的对照组，也是分数的下限。它告知我们基本模型在不使用任何自我反思的情况下，回答问题的效果。基线代理人使用了标准的提示工程技术，包括领域专业知识、链式思维、简洁性和少量提示[[48](#bib.bib48),
    [49](#bib.bib49), [1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]。所有LLM的采样温度设为0.0，以提高可重复性[[50](#bib.bib50)]。有关基线答案提示的示例，请参见附录中的图[6](#A2.F6
    "Figure 6 ‣ Appendix B Data ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving
    Performance")。'
- en: 'The self-reflecting agents used the same prompt-engineering techniques as the
    Baseline agent to re-answer questions. However, they also reflected upon their
    mistakes before attempting to re-answer. While re-answering, the self-reflection
    was injected into the re-answer prompt to allow the agent to learn from its mistakes.
    See Figures [7](#A2.F7 "Figure 7 ‣ Appendix B Data ‣ Self-Reflection in LLM Agents:
    Effects on Problem-Solving Performance") and [8](#A2.F8 "Figure 8 ‣ Appendix B
    Data ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance")
    in the appendix for examples of the self-reflection prompt and the re-answer prompt.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '自我反思代理人使用了与基线代理人相同的提示工程技术来重新回答问题。然而，他们在尝试重新回答之前，还会反思自己的错误。在重新回答时，自我反思会被注入到重新回答的提示中，以便代理人能从错误中学习。有关自我反思提示和重新回答提示的示例，请参见附录中的图[7](#A2.F7
    "Figure 7 ‣ Appendix B Data ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving
    Performance")和[8](#A2.F8 "Figure 8 ‣ Appendix B Data ‣ Self-Reflection in LLM
    Agents: Effects on Problem-Solving Performance")。'
- en: We redacted all of the answer labels (e.g., "A", "B", "C") and answer descriptions
    (e.g., "Baltimore", "Des Moines", "Las Vegas") from the agents’ self-reflections.
    However, the Unredacted agent retains this information. This agent is only used
    to provide an upper bound for the scores. Essentially, the Unredacted agent tells
    us how accurately the LLM could answer the questions when given the correct answer
    in its self-reflection.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们删除了所有答案标签（例如，“A”，“B”，“C”）和答案描述（例如，“巴尔的摩”，“得梅因”，“拉斯维加斯”）在代理人的自我反思中。然而，未编辑的代理人保留了这些信息。该代理人仅用于提供分数的上限。实际上，未编辑的代理人告诉我们，当在自我反思中给出正确答案时，LLM
    能多准确地回答问题。
- en: 2.4 Process
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 过程
- en: 'First, the Baseline agent answered all 1,000 questions. If a question was answered
    correctly, it was added to the Baseline agent’s score. If it was answered incorrectly,
    it was added to a queue of incorrectly answered questions to be reflected upon
    (see Figure [1](#S2.F1.1 "Figure 1 ‣ 2.4 Process ‣ 2 Methods ‣ Self-Reflection
    in LLM Agents: Effects on Problem-Solving Performance")).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，基线代理人回答了所有1000个问题。如果问题回答正确，则将其计入基线代理人的分数。如果回答错误，则将其添加到需要反思的错误问题队列中（见图[1](#S2.F1.1
    "Figure 1 ‣ 2.4 Process ‣ 2 Methods ‣ Self-Reflection in LLM Agents: Effects on
    Problem-Solving Performance")）。'
- en: Next, for each incorrectly answered question, the self-reflecting agents reflected
    upon the problem, their incorrect solution, and the correct answer. Using the
    correct answer as an external feedback signal, they each generated one of the
    eight types of self-reflection feedback described above.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对于每个回答错误的问题，自我反思的代理人会反思问题、他们的错误解决方案和正确答案。利用正确答案作为外部反馈信号，他们各自生成上述八种类型的自我反思反馈中的一种。
- en: Then, a find-and-replace operation was performed on the text of each self-reflection
    to redact the answer labels and answer descriptions. For example, we replaced
    answer labels (e.g., "A", "B", "C") and answer descriptions (e.g., "Baltimore",
    "Des Moines", "Las Vegas") with the text "[REDACTED]".¹¹1The process we used to
    redact answer labels and descriptions was greedy. It often redacting additional
    text that did not leak the answer. However, we felt it necessary to err on the
    side of caution by eliminating any possible answer leakage. This was done to all
    of the self-reflecting agents, except for the Unredacted agent, to prevent answer
    leakage in the self-reflections.²²2It is important to note that the self-reflections
    generated by the Explanation, Instructions, and Solution agents indirectly leak
    information about the correct answer without directly specifying the correct or
    incorrect answers. However, they generated this information on their own based
    on nothing more than being provided the correct answer during the self-reflection
    process.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对每个自我反思的文本进行查找和替换操作，以删除答案标签和答案描述。例如，我们将答案标签（例如，“A”，“B”，“C”）和答案描述（例如，“巴尔的摩”，“得梅因”，“拉斯维加斯”）替换为文本“[已编辑]”。¹¹1我们用来编辑答案标签和描述的过程是贪婪的。它通常会编辑掉额外的文本，这些文本不会泄露答案。然而，我们认为有必要谨慎处理，通过消除任何可能的答案泄露。对所有自我反思的代理（除了未编辑的代理）进行了这一处理，以防止自我反思中的答案泄露。²²2值得注意的是，Explanation、Instructions
    和 Solution 代理生成的自我反思间接泄露了有关正确答案的信息，而没有直接指定正确或错误的答案。然而，它们根据自我反思过程中提供的正确答案自主生成了这些信息。
- en: Finally, for each incorrectly answered question, the self-reflecting agents
    used their specific self-reflection text to assist them in re-answering the question.
    We calculated the scores for all agents and compared them to the Baseline agent
    for analysis.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于每个错误回答的问题，自我反思的代理使用其特定的自我反思文本来帮助他们重新回答问题。我们计算了所有代理的得分，并将其与基线代理进行比较分析。
- en: 'While LLM agents typically operate over a series of iterative steps, the code
    for this experiment was implemented as batch operations to save time and cost.
    So, each step in the self-reflection process occurred in one of four batch phases
    described above. Conceptually, the experiment represented virtual multi-step agents.
    However, the technical implementation of the experiment was actually a series
    of batch operations (see Algorithm [1](#alg1 "Algorithm 1 ‣ Figure 1 ‣ 2.4 Process
    ‣ 2 Methods ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance")).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LLM 代理通常通过一系列迭代步骤进行操作，但本实验的代码作为批处理操作实现，以节省时间和成本。因此，自我反思过程中的每一步都发生在上述四个批处理阶段中的一个。从概念上讲，这个实验代表了虚拟的多步骤代理。然而，实验的技术实现实际上是一系列的批处理操作（参见算法
    [1](#alg1 "算法 1 ‣ 图 1 ‣ 2.4 过程 ‣ 2 方法 ‣ LLM 代理中的自我反思：对问题解决性能的影响")）。
- en: '![Refer to caption](img/a139eaa788150aa2cdd4733574cc76e8.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a139eaa788150aa2cdd4733574cc76e8.png)'
- en: 'Figure 1: Diagram of the self-reflection experiment.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：自我反思实验的示意图。
- en: Algorithm 1 Self-reflection Experiment (Batch)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 自我反思实验（批处理）
- en: 1:for each model, exam, and problem do2:     Create the answer prompt3:     Answer
    the question4:     if the answer is incorrect then5:         Add the problem to
    the incorrect list6:     end if7:end for8:Calculate the Baseline agent scores9:10:for each
    model, exam, and problem do11:     Reflect upon the incorrect solution12:     Generate
    the self-reflections13:     if not the Unredacted agent then14:         Redact
    the answers15:     end if16:     Separate the reflections by type17:end for18:19:for each
    model, agent, exam, and problem do20:     Create the re-answer prompt21:     Inject
    the agent’s reflection22:     Re-answer the question23:end for24:Calculate the
    reflected agent scores
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1:对于每个模型、考试和问题执行2:     创建答案提示3:     回答问题4:     如果答案不正确则5:         将问题添加到错误列表中6:     结束 if7:结束 for8:计算基线代理得分9:10:对于每个模型、考试和问题执行11:     反思错误的解决方案12:     生成自我反思13:     如果不是未编辑代理则14:         编辑答案15:     结束 if16:     按类型分离反思17:结束 for18:19:对于每个模型、代理、考试和问题执行20:     创建重新回答提示21:     注入代理的反思22:     重新回答问题23:结束 for24:计算反思代理得分
- en: 2.5 Metrics
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 指标
- en: We used correct-answer accuracy as our primary metric to measure the performance
    of the agents. We divided the number of correctly answered questions by the total
    number of questions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用正确答案的准确性作为主要指标来衡量代理的性能。我们将正确回答的问题数除以总问题数。
- en: However, to reduce the cost of running our experiment, we did not have the self-reflecting
    agents re-answer all of the questions that were correctly answered by the Baseline
    agent. Rather, the self-reflecting agents only re-answered the incorrectly answered
    questions. We then added the self-reflecting agent’s correct re-answer score to
    the Baseline agent’s score to create a new total score for the self-reflecting
    agent.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了降低实验成本，我们没有让自我反思代理重新回答所有基线代理正确回答的问题。相反，自我反思代理仅重新回答了错误回答的问题。然后，我们将自我反思代理的正确重答分数加到基线代理的分数中，以创建自我反思代理的新总分。
- en: 'The calculations for accuracy used in our experiment are listed in Equation(s)
    [1](#S2.E1 "In 2.5 Metrics ‣ 2 Methods ‣ Self-Reflection in LLM Agents: Effects
    on Problem-Solving Performance"). In these equations, the subscript [base] refers
    to the Baseline agent’s correct-answer score, and the subscript [ref] is the reflection
    agent’s correct re-answer score.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '我们实验中用于准确度计算的公式列在方程 [1](#S2.E1 "In 2.5 Metrics ‣ 2 Methods ‣ Self-Reflection
    in LLM Agents: Effects on Problem-Solving Performance") 中。在这些方程中，下标 [base] 指代基线代理的正确答案分数，下标
    [ref] 指代反思代理的正确重答分数。'
- en: '|  | $\displaystyle\text{Accuracy}_{\text{base}}$ |  | (1) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Accuracy}_{\text{base}}$ |  | (1) |'
- en: 2.6 Analysis
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 分析
- en: When comparing the scores of the self-reflecting agents to the Baseline agent,
    we performed the McNemar test to determine statistical significance and report
    p-values. This test was specifically chosen because our analysis compared two
    series of binary outcomes (i.e., correct or incorrect answers). These outcomes
    were paired question-by-question across both the Baseline agent and self-reflecting
    agent being compared.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较自我反思代理与基线代理的分数时，我们进行了 McNemar 检验以确定统计显著性并报告 p 值。选择此检验是因为我们的分析比较了两个系列的二元结果（即正确或错误的答案）。这些结果在基线代理和自我反思代理之间一一配对。
- en: The McNemar test compares the number of discordant pairs in the two sets of
    pair-wise outcomes. To compute the test statistic, we create a $2\times 2$ contains
    correct-incorrect answer pairs (which, in our case, will always be zero) [[51](#bib.bib51)].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: McNemar 检验比较两个配对结果集中不一致对的数量。为了计算检验统计量，我们创建了一个 $2\times 2$ 表格，其中包含正确-错误的答案对（在我们的例子中，这些对的数量将始终为零）
    [[51](#bib.bib51)]。
- en: 'The McNemar’s test statistic is calculated as:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: McNemar 检验统计量的计算公式为：
- en: '|  | $$\chi^{2}=\frac{(b-c)^{2}}{b+c}\quad\text{where }b\text{ and }c\text{
    are the discordant pairs in }\left[\begin{array}[]{cc}a&amp;b\\ c&amp;d\\'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\chi^{2}=\frac{(b-c)^{2}}{b+c}\quad\text{where }b\text{ and }c\text{
    are the discordant pairs in }\left[\begin{array}[]{cc}a&amp;b\\ c&amp;d\\'
- en: \end{array}\right]$$ |  |
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right]$$ |  |
- en: 3 Results
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 结果
- en: 3.1 Performance by Agent
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 代理性能
- en: 'Our analysis revealed that agents using various types of self-reflection outperformed
    our Baseline agent. The increase in performance was statistically significant
    ($p<0.001$) for all types of self-reflection across all LLMs. We can use GPT-4
    as an example case. In Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Performance by Agent
    ‣ 3 Results ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance"),
    we can see that all types of self-reflection improve the accuracy of the agent
    in solving MCQA problems. See Table [3](#A1.T3 "Table 3 ‣ Appendix A Results ‣
    Self-Reflection in LLM Agents: Effects on Problem-Solving Performance") in the
    appendix for a numerical analysis of the results for GPT-4.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的分析揭示了使用各种自我反思类型的代理优于我们的基线代理。所有自我反思类型的性能提高在统计上都是显著的（$p<0.001$）。我们可以以 GPT-4
    为例。在图 [2](#S3.F2 "Figure 2 ‣ 3.1 Performance by Agent ‣ 3 Results ‣ Self-Reflection
    in LLM Agents: Effects on Problem-Solving Performance") 中，我们可以看到所有自我反思类型都提高了代理解决
    MCQA 问题的准确性。有关 GPT-4 结果的数值分析，请参见附录中的表 [3](#A1.T3 "Table 3 ‣ Appendix A Results
    ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance")。'
- en: '![Refer to caption](img/0d4a267d4c4a050a77f794dc19b4a38a.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0d4a267d4c4a050a77f794dc19b4a38a.png)'
- en: 'Figure 2: All self-reflection types improved the accuracy of GPT-4 agents.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：所有自我反思类型都提高了 GPT-4 代理的准确性。
- en: 3.2 Performance by Model
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模型性能
- en: 'In terms of performance by model, every LLM that we tested demonstrated similar
    increases in accuracy across all self-reflection types. In all cases, the improvement
    in performance was statistically significant ($p<0.001$). See Figure [3](#S3.F3
    "Figure 3 ‣ 3.2 Performance by Model ‣ 3 Results ‣ Self-Reflection in LLM Agents:
    Effects on Problem-Solving Performance") for a plot of accuracy by model and agent.
    See Table [4](#A1.T4 "Table 4 ‣ Appendix A Results ‣ Self-Reflection in LLM Agents:
    Effects on Problem-Solving Performance") for a numerical analysis of accuracy
    across all models.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 就模型性能而言，我们测试的每个LLM在所有自我反思类型中都表现出了类似的准确性提升。在所有情况下，性能的提升都具有统计学意义（$p<0.001$）。有关模型和代理的准确性图，请参见图[3](#S3.F3
    "图 3 ‣ 3.2 按模型性能 ‣ 3 结果 ‣ LLM代理中的自我反思：对问题解决性能的影响")。有关所有模型准确性的数值分析，请参见表[4](#A1.T4
    "表 4 ‣ 附录 A 结果 ‣ LLM代理中的自我反思：对问题解决性能的影响")。
- en: '![Refer to caption](img/836be5c7bc33ad9dec69f159f1c373e4.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/836be5c7bc33ad9dec69f159f1c373e4.png)'
- en: 'Figure 3: All LLMs we tested showed a similar pattern of improvement across
    self-reflection agents.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：我们测试的所有LLM在自我反思代理方面显示了类似的改进模式。
- en: 3.3 Performance by Exam
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 按考试性能
- en: 'In terms of performance by exam, we saw that self-reflection significantly
    increased performance for some problem domains. However, other problem domains
    were less affected. For example, we saw the largest improvement on the LSAT-AR
    (Analytical Reasoning) exam. Other exams, like the SAT English exam, had much
    smaller effects. See Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Performance by Exam ‣ 3
    Results ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving Performance")
    for a plot of accuracy by exam and agent for GPT-4\. See Table [5](#A1.T5 "Table
    5 ‣ Appendix A Results ‣ Self-Reflection in LLM Agents: Effects on Problem-Solving
    Performance") in the appendix for a numerical analysis of the results.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 就考试性能而言，我们看到自我反思在某些问题领域显著提高了表现。然而，其他问题领域受影响较小。例如，我们在LSAT-AR（分析推理）考试中看到最大的改进。其他考试，如SAT英语考试，效果要小得多。有关GPT-4的考试和代理准确性的图表，请参见图[4](#S3.F4
    "图 4 ‣ 3.3 按考试性能 ‣ 3 结果 ‣ LLM代理中的自我反思：对问题解决性能的影响")。有关结果的数值分析，请参见附录中的表[5](#A1.T5
    "表 5 ‣ 附录 A 结果 ‣ LLM代理中的自我反思：对问题解决性能的影响")。
- en: '![Refer to caption](img/6354e0070805e1e1d22fab20790200c3.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6354e0070805e1e1d22fab20790200c3.png)'
- en: 'Figure 4: The increase in performance from self-reflection was larger for some
    exams and smaller for others.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：自我反思带来的性能提升在某些考试中较大，而在其他考试中较小。
- en: 4 Discussion
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: 4.1 Interpretation
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 解释
- en: Based on these results, all types of self-reflection improve the performance
    of LLM agents. In addition, these effects were observed across every LLM we tested.
    Self-reflections that contain more information (e.g., Instructions, Explanation,
    and Solution) outperform types of self-reflection with limited information (e.g.,
    Retry, Keywords, and Advice).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些结果，所有类型的自我反思都提高了大型语言模型（LLM）代理的表现。此外，我们测试的每种LLM中都观察到了这些效果。包含更多信息的自我反思（例如，指令、解释和解决方案）比信息有限的自我反思类型（例如，重试、关键词和建议）表现更佳。
- en: The difference in accuracy between the self-reflecting agents and the Unredacted
    agent demonstrated that we were effectively eliminating direct answer leakage
    from the self-reflections. However, the structure of feedback generated by the
    Instruction, Explanation, Solution, and Composite agents clearly provides indirect
    guidance toward the correct answer without directly giving the answer away.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 自我反思代理与未编辑代理之间的准确性差异表明，我们有效地消除了自我反思中的直接答案泄漏。然而，由指令、解释、解决方案和综合代理生成的反馈结构明显提供了间接的正确答案指导，而不是直接给出答案。
- en: Interestingly, the Retry agent significantly improved performance across all
    LLMs. As a result, it appears that even the mere knowledge that the agent previously
    made a mistake improves the agent’s performance while re-answering the question.
    We hypothesize that this is either the result of the agent being more diligent
    in its second attempt or choosing the second most likely answer based on its re-answer
    CoT. Further investigation will be required to answer this question.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，重试代理显著提高了所有LLM的性能。因此，即使只是知道代理之前犯过错误，也能提高代理在重新回答问题时的表现。我们假设这要么是因为代理在第二次尝试中更加认真，要么是基于其重新回答的思路选择了第二可能的答案。需要进一步的研究来解答这个问题。
- en: 4.2 Limitations
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 限制
- en: First, the LLM agent we created for this experiment only solved a single-step
    problem. The real value in LLM agents is their ability to solve complex multi-step
    problems by iteratively choosing actions that lead them toward their goal. As
    a result, this experiment does not fully demonstrate the potential of self-reflecting
    LLM agents.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为此实验创建的LLM代理只解决了单步问题。LLM代理的真正价值在于它们通过迭代选择行动来解决复杂的多步骤问题。因此，这个实验并没有完全展示自我反思LLM代理的潜力。
- en: Second, API response errors may have introduced a small amount of error into
    our results. API errors typically occurred when content-safety filters were triggered
    by the questions being asked. In most cases, this may have amounted to an error
    in reporting an agent’s accuracy of less than 1%. However, in the case of the
    Gemini 1.0 Pro and the Mistral Large models, this error could be as high as 2.8%.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，API响应错误可能在我们的结果中引入了一些误差。API错误通常发生在内容安全过滤器被提出的问题触发时。在大多数情况下，这可能导致报告代理准确率的误差不到1%。然而，在Gemini
    1.0 Pro和Mistral Large模型的情况下，这种误差可能高达2.8%。
- en: Third, the top-performing LLMs scored above 90% accuracy for most exams. As
    a result, the increase in scores for the top exams was compressed near the upper
    limit of 100% (i.e., a perfect score). This compression effect makes it difficult
    to assess the performance increase. As a result, our analysis would benefit from
    exams with a higher level of difficulty.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，表现最好的LLM在大多数考试中的准确率超过了90%。因此，顶级考试的分数提升被压缩在接近100%的上限（即满分）附近。这种压缩效应使得评估表现提升变得困难。因此，我们的分析将从更高难度的考试中受益。
- en: Finally, for all models and agents, the LSAT-AR (Analytical Reasoning) exam
    was the most difficult and also the most benefited by self-reflection. This large
    increase in performance from a single exam had the potential to skew the aggregate
    results across all exams. Using a set of exams with more uniform difficulty would
    eliminate this skewness.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于所有模型和代理，LSAT-AR（分析推理）考试是最难的，也是通过自我反思受益最大的。单次考试中的这种表现提升有可能影响所有考试的总体结果。使用一组难度更为均匀的考试将消除这种偏差。
- en: 4.3 Implications
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 含义
- en: Our research builds upon previous research in LLM agents and self-reflecting
    agents.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究建立在之前关于LLM代理和自我反思代理的研究基础上。
- en: It has practical implications for AI engineers who are building agentic LLM
    systems. Agents that can self-reflect on their own mistakes based on error signals
    from the environment can learn to avoid similar mistakes in the future. This will
    also help prevent the common issue of agents getting stuck in unproductive loops
    because they continue repeating the same mistake indefinitely.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这对构建自主LLM系统的AI工程师具有实际意义。能够基于环境中的错误信号对自身错误进行自我反思的代理可以学会避免未来类似的错误。这也有助于防止代理陷入无效循环，因为它们不断重复相同的错误。
- en: In addition, our research has theoretical implications for AI researchers studying
    meta-cognition in LLMs. If LLMs are able to self-reflect on their own CoT, other
    similar metacognitive processes may also be leveraged to improve their performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的研究对研究LLM元认知的AI研究人员具有理论意义。如果LLM能够对自己的**CoT**进行自我反思，其他类似的元认知过程也可能被利用来提高其表现。
- en: 4.4 Future Research
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 未来研究
- en: First, we recommend repeating this experiment using a more complex set of problems.
    Using problems as difficult or more difficult than the LSAT-AR exam would better
    reflect the performance improvement from self-reflection by avoiding compression
    of the scores around 100% accuracy.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们建议重复进行更复杂问题集的实验。使用难度与LSAT-AR考试相当或更高的问题将更好地反映自我反思带来的表现提升，从而避免分数接近100%时的压缩效应。
- en: Second, we recommend performing an experiment using multi-step problems. This
    would allow the agents to receive feedback from their environment after each step
    to use as external signals for error correction. It would also demonstrate the
    potential of self-reflection on long-horizon problems.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们建议进行使用多步骤问题的实验。这将使代理能够在每一步之后从环境中获得反馈，作为外部信号进行错误修正。这也将展示在长期问题上的自我反思的潜力。
- en: Third, we recommend repeating this experiment while providing the agents with
    access to external tools. This would allow us to see how error signals from the
    tools benefit self-reflection. For example, we could observe how an agent adapts
    to compiler errors from a Python interpreter or low-rank search results from a
    search engine.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们建议在提供外部工具访问权限的情况下重复这一实验。这将使我们能够观察工具的错误信号如何有助于自我反思。例如，我们可以观察代理如何适应Python解释器中的编译错误或搜索引擎中的低排名搜索结果。
- en: Fourth, we recommend repeating this experiment with agents the possess external
    memory. Having an agent answer the same questions based on self-reflection is
    only beneficial from an experimental standpoint. Real-world agents need to store
    self-reflections and retrieve them (using Retrieval Augmented Generation) when
    encountering similar but not necessarily identical problems.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，我们建议重复这一实验，并为代理提供外部记忆。仅从实验角度来看，让代理基于自我反思回答相同的问题是有益的。现实世界中的代理需要存储自我反思，并在遇到类似但不完全相同的问题时检索这些反思（使用检索增强生成）。
- en: Finally, we recommend a survey of self-reflection across a wider set of LLMs,
    agent types, and problem domains. This would help us better characterize the effects
    of self-reflection and provide further empirical evidence for the potential benefits
    of self-reflecting LLM agents.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们建议对更广泛的LLM、代理类型和问题领域进行自我反思的调查。这将帮助我们更好地描述自我反思的效果，并提供进一步的实证证据，证明自我反思LLM代理的潜在好处。
- en: 5 Conclusion
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this study, we investigated the effects of self-reflection in LLM agents
    on problem-solving tasks. Our results indicate that LLMs are able to reflect upon
    their own CoT and produce guidance that can significantly improve problem-solving
    performance. These performance improvements were observed across multiple LLMs,
    self-reflection types, and problem domains. This research has practical implications
    for AI engineers building agentic AI systems as well as theoretical implications
    for AI researchers studying meta-cognition in LLMs.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们调查了LLM代理在解决问题任务中的自我反思效果。我们的结果表明，LLM能够反思其自身的CoT，并生成可以显著改善问题解决表现的指导。这些表现改善在多种LLM、自我反思类型和问题领域中均有观察到。这项研究对构建代理性AI系统的AI工程师具有实际意义，也对研究LLM元认知的AI研究人员具有理论意义。
- en: 6 Acknowledgements
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 致谢
- en: Funding for this research was provided by [Microsoft](https://www.microsoft.com/)
    and [Renze AI Research Institute](https://renzeai.org/).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的资金由 [Microsoft](https://www.microsoft.com/) 和 [Renze AI Research Institute](https://renzeai.org/)
    提供。
- en: References
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language
    models are zero-shot reasoners,” in *Advances in Neural Information Processing
    Systems*, vol. 35, 5 2022, pp. 22 199–22 213\. [Online]. Available: [https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, 和 Y. Iwasawa, “大型语言模型是零-shot 推理者，”
    *《神经信息处理系统进展》*，第35卷，2022年5月，第22 199–22 213页。 [在线]. 可用链接: [https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)'
- en: '[2] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le,
    and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,”
    *arXiv*, 1 2022\. [Online]. Available: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q.
    Le, 和 D. Zhou, “链式思维提示引发大型语言模型的推理，” *arXiv*，2022年1月。 [在线]. 可用链接: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
- en: '[3] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba,
    “Large language models are human-level prompt engineers,” *The Eleventh International
    Conference on Learning Representations*, 11 2023\. [Online]. Available: [https://arxiv.org/abs/2211.01910](https://arxiv.org/abs/2211.01910)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, 和 J. Ba,
    “大型语言模型是人类级别的提示工程师，” *第十一届国际学习表征会议*，2023年11月。 [在线]. 可用链接: [https://arxiv.org/abs/2211.01910](https://arxiv.org/abs/2211.01910)'
- en: '[4] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. Bang, D. Chen,
    H. S. Chan, W. Dai, A. Madotto, and P. Fung, “Survey of hallucination in natural
    language generation,” *ACM Computing Surveys*, vol. 55, 2 2022\. [Online]. Available:
    [http://dx.doi.org/10.1145/3571730](http://dx.doi.org/10.1145/3571730)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. Bang, D. Chen,
    H. S. Chan, W. Dai, A. Madotto, 和 P. Fung, “自然语言生成中的幻觉调查，” *《ACM计算机调查》*，第55卷，2022年2月。
    [在线]. 可用链接: [http://dx.doi.org/10.1145/3571730](http://dx.doi.org/10.1145/3571730)'
- en: '[5] M. U. Hadi, qasem al tashi, R. Qureshi, A. Shah, amgad muneer, M. Irfan,
    A. Zafar, M. B. Shaikh, N. Akhtar, J. Wu, S. Mirjalili, Q. Al-Tashi, and A. Muneer,
    “A survey on large language models: Applications, challenges, limitations, and
    practical usage,” *Authorea Preprints*, 10 2023\. [Online]. Available: [https://doi.org/10.36227/techrxiv.23589741.v1](https://doi.org/10.36227/techrxiv.23589741.v1)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. U. Hadi, qasem al tashi, R. Qureshi, A. Shah, amgad muneer, M. Irfan,
    A. Zafar, M. B. Shaikh, N. Akhtar, J. Wu, S. Mirjalili, Q. Al-Tashi, 和 A. Muneer，“关于大语言模型的调查：应用、挑战、局限性和实际使用，”
    *Authorea Preprints*，2023年10月。[在线]. 可用: [https://doi.org/10.36227/techrxiv.23589741.v1](https://doi.org/10.36227/techrxiv.23589741.v1)'
- en: '[6] A. Payandeh, D. Pluth, J. Hosier, X. Xiao, and V. K. Gurbani, “How susceptible
    are llms to logical fallacies?” *arXiv*, 8 2023\. [Online]. Available: [https://arxiv.org/abs/2308.09853v1](https://arxiv.org/abs/2308.09853v1)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Payandeh, D. Pluth, J. Hosier, X. Xiao, 和 V. K. Gurbani，“大语言模型对逻辑谬误的易感性如何？”
    *arXiv*，2023年8月。[在线]. 可用: [https://arxiv.org/abs/2308.09853v1](https://arxiv.org/abs/2308.09853v1)'
- en: '[7] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou,
    “Large language models cannot self-correct reasoning yet,” *arXiv*, 10 2023\.
    [Online]. Available: [https://arxiv.org/abs/2310.01798](https://arxiv.org/abs/2310.01798)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, 和 D. Zhou，“大语言模型尚不能自我纠正推理，”
    *arXiv*，2023年10月。[在线]. 可用: [https://arxiv.org/abs/2310.01798](https://arxiv.org/abs/2310.01798)'
- en: '[8] Z. Ji, T. Yu, Y. Xu, N. Lee, E. Ishii, and P. Fung, “Towards mitigating
    hallucination in large language models via self-reflection,” *arXiv*, 10 2023\.
    [Online]. Available: [https://arxiv.org/abs/2310.06271](https://arxiv.org/abs/2310.06271)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Z. Ji, T. Yu, Y. Xu, N. Lee, E. Ishii, 和 P. Fung，“旨在通过自我反思缓解大语言模型中的幻觉，”
    *arXiv*，2023年10月。[在线]. 可用: [https://arxiv.org/abs/2310.06271](https://arxiv.org/abs/2310.06271)'
- en: '[9] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain,
    and J. Gao, “Large language models: A survey,” *arXiv*, 2 2024\. [Online]. Available:
    [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain,
    和 J. Gao，“大语言模型：一项调查，” *arXiv*，2024年2月。[在线]. 可用: [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196)'
- en: '[10] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao,
    “Reflexion: Language agents with verbal reinforcement learning,” *arXiv*, 3 2023\.
    [Online]. Available: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, 和 S. Yao，“Reflexion:
    带有言语强化学习的语言代理，” *arXiv*，2023年3月。[在线]. 可用: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)'
- en: '[11] L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, and W. Y. Wang, “Automatically
    correcting large language models: Surveying the landscape of diverse self-correction
    strategies,” *arXiv*, 8 2023\. [Online]. Available: [https://arxiv.org/abs/2308.03188](https://arxiv.org/abs/2308.03188)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] L. Pan, M. Saxon, W. Xu, D. Nathani, X. Wang, 和 W. Y. Wang，“自动修正大语言模型：调查多样化自我修正策略的现状，”
    *arXiv*，2023年8月。[在线]. 可用: [https://arxiv.org/abs/2308.03188](https://arxiv.org/abs/2308.03188)'
- en: '[12] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon,
    N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck,
    A. Yazdanbakhsh, and P. Clark, “Self-refine: Iterative refinement with self-feedback,”
    *arXiv*, 3 2023\. [Online]. Available: [https://arxiv.org/abs/2303.17651v2](https://arxiv.org/abs/2303.17651v2)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U.
    Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann,
    S. Welleck, A. Yazdanbakhsh, 和 P. Clark，“Self-refine: 具有自我反馈的迭代精炼，” *arXiv*，2023年3月。[在线].
    可用: [https://arxiv.org/abs/2303.17651v2](https://arxiv.org/abs/2303.17651v2)'
- en: '[13] J. Toy, J. MacAdam, and P. Tabor, “Metacognition is all you need? using
    introspection in generative agents to improve goal-directed behavior,” *arXiv*,
    1 2024\. [Online]. Available: [https://arxiv.org/abs/2401.10910](https://arxiv.org/abs/2401.10910)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Toy, J. MacAdam, 和 P. Tabor，“元认知就是你所需要的？利用生成代理中的内省改善目标导向行为，” *arXiv*，2024年1月。[在线].
    可用: [https://arxiv.org/abs/2401.10910](https://arxiv.org/abs/2401.10910)'
- en: '[14] Y. Wang and Y. Zhao, “Metacognitive prompting improves understanding in
    large language models,” *arXiv*, 8 2023\. [Online]. Available: [https://arxiv.org/abs/2308.05342](https://arxiv.org/abs/2308.05342)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Wang 和 Y. Zhao，“元认知提示提升大语言模型的理解，” *arXiv*，2023年8月。[在线]. 可用: [https://arxiv.org/abs/2308.05342](https://arxiv.org/abs/2308.05342)'
- en: '[15] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning
    to retrieve, generate, and critique through self-reflection,” *arXiv*, 10 2023\.
    [Online]. Available: [https://arxiv.org/abs/2310.11511v1](https://arxiv.org/abs/2310.11511v1)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Asai, Z. Wu, Y. Wang, A. Sil, 和 H. Hajishirzi，“Self-rag: 通过自我反思学习检索、生成和批判，”
    *arXiv*，2023年10月。[在线]. 可用: [https://arxiv.org/abs/2310.11511v1](https://arxiv.org/abs/2310.11511v1)'
- en: '[16] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, A. Anandkumar,
    U. Austin, and U. Madison, “Voyager: An open-ended embodied agent with large language
    models,” *arXiv*, 5 2023\. [Online]. Available: [https://arxiv.org/abs/2305.16291v2](https://arxiv.org/abs/2305.16291v2)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, A. Anandkumar,
    U. Austin, 和 U. Madison, “Voyager：一个开放式的具身代理，配备大型语言模型”，*arXiv*，2023年5月\. [在线].
    可用链接：[https://arxiv.org/abs/2305.16291v2](https://arxiv.org/abs/2305.16291v2)'
- en: '[17] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin,
    E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou,
    X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu,
    X. Huang, and T. Gui, “The rise and potential of large language model based agents:
    A survey,” *arXiv*, 9 2023\. [Online]. Available: [https://arxiv.org/abs/2309.07864v3](https://arxiv.org/abs/2309.07864v3)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S.
    Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang,
    Y. Zou, X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng,
    X. Qiu, X. Huang, 和 T. Gui, “大型语言模型基于代理的崛起与潜力：综述”，*arXiv*，2023年9月\. [在线]. 可用链接：[https://arxiv.org/abs/2309.07864v3](https://arxiv.org/abs/2309.07864v3)'
- en: '[18] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,
    “React: Synergizing reasoning and acting in language models,” *arXiv*, 10 2022\.
    [Online]. Available: [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, 和 Y. Cao, “React：在语言模型中协同推理与行动”，*arXiv*，2022年10月\.
    [在线]. 可用链接：[https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)'
- en: '[19] N. Miao, Y. W. Teh, and T. Rainforth, “Selfcheck: Using llms to zero-shot
    check their own step-by-step reasoning,” *arXiv*, 8 2023\. [Online]. Available:
    [https://arxiv.org/abs/2308.00436v3](https://arxiv.org/abs/2308.00436v3)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] N. Miao, Y. W. Teh, 和 T. Rainforth, “Selfcheck：利用大型语言模型零-shot检查其自身的逐步推理”，*arXiv*，2023年8月\.
    [在线]. 可用链接：[https://arxiv.org/abs/2308.00436v3](https://arxiv.org/abs/2308.00436v3)'
- en: '[20] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain,
    V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button,
    M. Knight, B. Chess, and J. S. Openai, “Webgpt: Browser-assisted question-answering
    with human feedback,” *arXiv*, 12 2021\. [Online]. Available: [https://arxiv.org/abs/2112.09332v3](https://arxiv.org/abs/2112.09332v3)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S.
    Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K.
    Button, M. Knight, B. Chess, 和 J. S. Openai, “Webgpt：带有人工反馈的浏览器辅助问答”，*arXiv*，2021年12月\.
    [在线]. 可用链接：[https://arxiv.org/abs/2112.09332v3](https://arxiv.org/abs/2112.09332v3)'
- en: '[21] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer,
    N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves
    to use tools,” *arXiv*, 2 2023\. [Online]. Available: [https://arxiv.org/abs/2302.04761v1](https://arxiv.org/abs/2302.04761v1)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer,
    N. Cancedda, 和 T. Scialom, “Toolformer：语言模型可以自我教会使用工具”，*arXiv*，2023年2月\. [在线].
    可用链接：[https://arxiv.org/abs/2302.04761v1](https://arxiv.org/abs/2302.04761v1)'
- en: '[22] P. Lewis and et al., “Retrieval-augmented generation for knowledge-intensive
    nlp tasks,” *arXiv*, 5 2020\. [Online]. Available: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] P. Lewis 等人, “针对知识密集型自然语言处理任务的检索增强生成”，*arXiv*，2020年5月\. [在线]. 可用链接：[https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)'
- en: '[23] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang,
    and H. Wang, “Retrieval-augmented generation for large language models: A survey,”
    *arXiv*, 12 2023\. [Online]. Available: [https://arxiv.org/abs/2312.10997v5](https://arxiv.org/abs/2312.10997v5)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang,
    和 H. Wang, “针对大型语言模型的检索增强生成：综述”，*arXiv*，2023年12月\. [在线]. 可用链接：[https://arxiv.org/abs/2312.10997v5](https://arxiv.org/abs/2312.10997v5)'
- en: '[24] W. Zhong, L. Guo, Q. Gao, H. Ye, and Y. Wang, “Memorybank: Enhancing large
    language models with long-term memory,” *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 38, pp. 19 724–19 731, 5 2023\. [Online]. Available:
    [https://arxiv.org/abs/2305.10250v3](https://arxiv.org/abs/2305.10250v3)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] W. Zhong, L. Guo, Q. Gao, H. Ye, 和 Y. Wang, “Memorybank：增强大型语言模型的长期记忆”，*AAAI人工智能会议论文集*，第38卷，第19,724–19,731页，2023年5月\.
    [在线]. 可用链接：[https://arxiv.org/abs/2305.10250v3](https://arxiv.org/abs/2305.10250v3)'
- en: '[25] Z. Wang, A. Liu, H. Lin, J. Li, X. Ma, and Y. Liang, “Rat: Retrieval augmented
    thoughts elicit context-aware reasoning in long-horizon generation,” *arXiv*,
    3 2024\. [Online]. Available: [https://arxiv.org/abs/2403.05313](https://arxiv.org/abs/2403.05313)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Z. Wang, A. Liu, H. Lin, J. Li, X. Ma, 和 Y. Liang, “Rat：检索增强思想引发长远生成中的上下文感知推理”，*arXiv*，2024年3月\.
    [在线]. 可用链接：[https://arxiv.org/abs/2403.05313](https://arxiv.org/abs/2403.05313)'
- en: '[26] G. Tyen, H. Mansoor, V. Cărbune, P. Chen, and T. Mak, “Llms cannot find
    reasoning errors, but can correct them!” *arXiv*, 11 2023\. [Online]. Available:
    [https://arxiv.org/abs/2311.08516v2](https://arxiv.org/abs/2311.08516v2)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] G. Tyen, H. Mansoor, V. Cărbune, P. Chen, 和 T. Mak, “LLMs 无法发现推理错误，但可以纠正它们！”
    *arXiv*，2023年11月\. [在线]. 可用链接: [https://arxiv.org/abs/2311.08516v2](https://arxiv.org/abs/2311.08516v2)'
- en: '[27] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    and O. Tafjord, “Think you have solved question answering? try arc, the ai2 reasoning
    challenge,” *ArXiv*, 3 2018\. [Online]. Available: [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
    和 O. Tafjord, “认为你已经解决了问答问题？试试 ARC，AI2 推理挑战，” *ArXiv*，2018年3月\. [在线]. 可用链接: [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457)'
- en: '[28] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:
    Can a machine really finish your sentence?” in *Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics*, 2019\. [Online]. Available:
    [https://arxiv.org/abs/1905.07830](https://arxiv.org/abs/1905.07830)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, 和 Y. Choi, “Hellaswag: 机器真的能完成你的句子吗？”
    在 *第57届计算语言学协会年会论文集*，2019\. [在线]. 可用链接: [https://arxiv.org/abs/1905.07830](https://arxiv.org/abs/1905.07830)'
- en: '[29] A. Pal, L. K. Umapathi, and M. Sankarasubbu, “Medmcqa: A large-scale multi-subject
    multi-choice dataset for medical domain question answering,” in *Proceedings of
    the Conference on Health, Inference, and Learning*.   PMLR, 2022, pp. 248–260\.
    [Online]. Available: [https://proceedings.mlr.press/v174/pal22a.html](https://proceedings.mlr.press/v174/pal22a.html)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. Pal, L. K. Umapathi, 和 M. Sankarasubbu, “Medmcqa: 一个大规模的多主题多选数据集，用于医学领域问答，”
    在 *健康、推理与学习会议论文集*。 PMLR, 2022, 第 248–260 页\. [在线]. 可用链接: [https://proceedings.mlr.press/v174/pal22a.html](https://proceedings.mlr.press/v174/pal22a.html)'
- en: '[30] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen,
    and N. Duan, “Agieval: A human-centric benchmark for evaluating foundation models,”
    *ArXiv*, 4 2023\. [Online]. Available: [https://arxiv.org/abs/2304.06364](https://arxiv.org/abs/2304.06364)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen,
    和 N. Duan, “Agieval: 一个以人为本的基准，用于评估基础模型，” *ArXiv*，2023年4月\. [在线]. 可用链接: [https://arxiv.org/abs/2304.06364](https://arxiv.org/abs/2304.06364)'
- en: '[31] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, “Logiqa: A challenge
    dataset for machine reading comprehension with logical reasoning,” in *International
    Joint Conference on Artificial Intelligence*, 2020\. [Online]. Available: [https://arxiv.org/abs/2007.08124](https://arxiv.org/abs/2007.08124)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, 和 Y. Zhang, “Logiqa: 一个用于机器阅读理解与逻辑推理的挑战数据集，”
    在 *国际人工智能联合会议*，2020\. [在线]. 可用链接: [https://arxiv.org/abs/2007.08124](https://arxiv.org/abs/2007.08124)'
- en: '[32] S. Wang, Z. Liu, W. Zhong, M. Zhou, Z. Wei, Z. Chen, and N. Duan, “From
    lsat: The progress and challenges of complex reasoning,” *IEEE/ACM Transactions
    on Audio, Speech and Language Processing*, vol. 30, pp. 2201–2216, 8 2021\. [Online].
    Available: [https://doi.org/10.1109/TASLP.2022.3164218](https://doi.org/10.1109/TASLP.2022.3164218)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. Wang, Z. Liu, W. Zhong, M. Zhou, Z. Wei, Z. Chen, 和 N. Duan, “来自 LSAT:
    复杂推理的进展与挑战，” *IEEE/ACM 音频、语音与语言处理汇刊*，第 30 卷，第 2201–2216 页，2021年8月\. [在线]. 可用链接:
    [https://doi.org/10.1109/TASLP.2022.3164218](https://doi.org/10.1109/TASLP.2022.3164218)'
- en: '[33] Anthropic, “Introducing the next generation of claude  anthropic,” 2024\.
    [Online]. Available: [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Anthropic, “介绍下一代 Claude  Anthropic，” 2024\. [在线]. 可用链接: [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)'
- en: '[34] ——, “The claude 3 model family: Opus, sonnet, haiku,” 2024\. [Online].
    Available: [https://www.anthropic.com/claude-3-model-card](https://www.anthropic.com/claude-3-model-card)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] ——, “Claude 3 模型家族: Opus, sonnet, haiku，” 2024\. [在线]. 可用链接: [https://www.anthropic.com/claude-3-model-card](https://www.anthropic.com/claude-3-model-card)'
- en: '[35] Cohere, “Command r+,” 2024\. [Online]. Available: [https://docs.cohere.com/docs/command-r-plus](https://docs.cohere.com/docs/command-r-plus)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Cohere, “Command r+，” 2024\. [在线]. 可用链接: [https://docs.cohere.com/docs/command-r-plus](https://docs.cohere.com/docs/command-r-plus)'
- en: '[36] ——, “Model card for c4ai command r+,” 2024\. [Online]. Available: [https://huggingface.co/CohereForAI/c4ai-command-r-plus](https://huggingface.co/CohereForAI/c4ai-command-r-plus)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] ——, “c4ai command r+ 的模型卡，” 2024\. [在线]. 可用链接: [https://huggingface.co/CohereForAI/c4ai-command-r-plus](https://huggingface.co/CohereForAI/c4ai-command-r-plus)'
- en: '[37] S. Pichai and D. Hassabis, “Introducing gemini: Google’s most capable
    ai model yet,” 2023\. [Online]. Available: [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Pichai 和 D. Hassabis, “介绍 Gemini: 谷歌迄今为止最强大的 AI 模型，” 2023。 [在线]. 可用链接:
    [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/)'
- en: '[38] Gemini-Team, “Gemini: A family of highly capable multimodal models,” *arXiv*,
    12 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Gemini-Team, “Gemini: 一系列高能力的多模态模型，” *arXiv*，2023年12月。'
- en: '[39] S. Pichai and D. Hassabis, “Introducing gemini 1.5, google’s next-generation
    ai model,” 2024\. [Online]. Available: [https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Pichai 和 D. Hassabis, “介绍 Gemini 1.5，谷歌的下一代 AI 模型，” 2024。 [在线]. 可用链接:
    [https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)'
- en: '[40] Gemini-Team, “Gemini 1.5: Unlocking multimodal understanding across millions
    of tokens of context,” 2024\. [Online]. Available: [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Gemini-Team, “Gemini 1.5: 解锁跨越数百万个上下文令牌的多模态理解，” 2024。 [在线]. 可用链接: [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530)'
- en: '[41] OpenAI, “Introducing chatgpt,” 11 2022\. [Online]. Available: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] OpenAI, “介绍 ChatGPT，” 2022年11月。 [在线]. 可用链接: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)'
- en: '[42] ——, “Models - openai api.” [Online]. Available: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] ——, “模型 - OpenAI API。” [在线]. 可用链接: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
- en: '[43] ——, “Gpt-4,” 3 2023\. [Online]. Available: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] ——, “GPT-4，” 2023年3月。 [在线]. 可用链接: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
- en: '[44] ——, “Gpt-4 technical report,” *arXiv*, 3 2023\. [Online]. Available: [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] ——, “GPT-4 技术报告，” *arXiv*，2023年3月。 [在线]. 可用链接: [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)'
- en: '[45] Meta, “Meta and microsoft introduce the next generation of llama | meta,”
    2023\. [Online]. Available: [https://about.meta.com/news/2023/07/llama-2/](https://about.meta.com/news/2023/07/llama-2/)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Meta, “Meta 和微软介绍下一代 Llama | Meta，” 2023。 [在线]. 可用链接: [https://about.meta.com/news/2023/07/llama-2/](https://about.meta.com/news/2023/07/llama-2/)'
- en: '[46] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen,
    G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami,
    N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa,
    I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich,
    Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton,
    J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian,
    X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov,
    Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov,
    and T. Scialom, “Llama 2: Open foundation and fine-tuned chat models,” *arXiv*,
    7 2023\. [Online]. Available: [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer,
    M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao,
    V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V.
    Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril,
    J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog,
    Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva,
    E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X.
    Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez,
    R. Stojnic, S. Edunov, 和 T. Scialom, “Llama 2: 开放的基础和微调的聊天模型，” *arXiv*，2023年7月。
    [在线]. 可用链接: [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288)'
- en: '[47] Mistral-AI-Team, “Au large | mistral ai | frontier ai in your hands,”
    2024\. [Online]. Available: [https://mistral.ai/news/mistral-large/](https://mistral.ai/news/mistral-large/)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Mistral-AI-Team, “Au large | Mistral AI | 手中的前沿 AI，” 2024。 [在线]. 可用链接:
    [https://mistral.ai/news/mistral-large/](https://mistral.ai/news/mistral-large/)'
- en: '[48] S. M. Bsharat, A. Myrzakhan, and Z. Shen, “Principled instructions are
    all you need for questioning llama-1/2, gpt-3.5/4,” *arXiv*, 12 2023\. [Online].
    Available: [https://arxiv.org/abs/2312.16171](https://arxiv.org/abs/2312.16171)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] S. M. Bsharat, A. Myrzakhan, 和 Z. Shen, “有原则的指令是你对 Llama-1/2、GPT-3.5/4
    进行提问所需的全部，” *arXiv*，2023年12月。 [在线]. 可用链接: [https://arxiv.org/abs/2312.16171](https://arxiv.org/abs/2312.16171)'
- en: '[49] M. Renze and E. Guven, “The benefits of a concise chain of thought on
    problem-solving in large language models,” *arXiv*, 1 2024\. [Online]. Available:
    [https://arxiv.org/abs/2401.05618v1](https://arxiv.org/abs/2401.05618v1)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Renze 和 E. Guven, “简洁思维链对大型语言模型问题解决的好处，” *arXiv*, 2024年1月\. [在线]. 可用：
    [https://arxiv.org/abs/2401.05618v1](https://arxiv.org/abs/2401.05618v1)'
- en: '[50] ——, “The effect of sampling temperature on problem solving in large language
    models,” *arXiv*, 2 2024\. [Online]. Available: [https://arxiv.org/abs/2402.05201v1](https://arxiv.org/abs/2402.05201v1)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] ——, “采样温度对大型语言模型问题解决的影响，” *arXiv*, 2024年2月\. [在线]. 可用： [https://arxiv.org/abs/2402.05201v1](https://arxiv.org/abs/2402.05201v1)'
- en: '[51] Q. McNemar, “Note on the sampling error of the difference between correlated
    proportions or percentages,” *Psychometrika*, vol. 12, pp. 153–157, 6 1947\. [Online].
    Available: [https://doi.org/10.1007/BF02295996](https://doi.org/10.1007/BF02295996)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Q. McNemar, “相关比例或百分比差异的采样误差说明，” *Psychometrika*, 第12卷，第153–157页，1947年6月\.
    [在线]. 可用： [https://doi.org/10.1007/BF02295996](https://doi.org/10.1007/BF02295996)'
- en: Appendix A Results
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 结果
- en: '| Agent Name | Accuracy | Difference | Test Statistic | p-value |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 代理名称 | 准确性 | 差异 | 测试统计量 | p值 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Baseline | 0.786 | N/A | N/A | N/A |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 0.786 | 不适用 | 不适用 | 不适用 |'
- en: '| Retry | 0.827 | 0.041 | 39.024 | $<0.001$ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 重试 | 0.827 | 0.041 | 39.024 | $<0.001$ |'
- en: '| Keywords | 0.832 | 0.046 | 44.022 | $<0.001$ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 关键词 | 0.832 | 0.046 | 44.022 | $<0.001$ |'
- en: '| Advice | 0.840 | 0.054 | 52.019 | $<0.001$ |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 建议 | 0.840 | 0.054 | 52.019 | $<0.001$ |'
- en: '| Instructions | 0.849 | 0.063 | 61.016 | $<0.001$ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 说明 | 0.849 | 0.063 | 61.016 | $<0.001$ |'
- en: '| Explanation | 0.876 | 0.090 | 88.011 | $<0.001$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 0.876 | 0.090 | 88.011 | $<0.001$ |'
- en: '| Solution | 0.925 | 0.139 | 137.007 | $<0.001$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 解决方案 | 0.925 | 0.139 | 137.007 | $<0.001$ |'
- en: '| Composite | 0.932 | 0.146 | 144.007 | $<0.001$ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 复合 | 0.932 | 0.146 | 144.007 | $<0.001$ |'
- en: '| Unredacted | 0.971 | 0.185 | 183.005 | $<0.001$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 未删减 | 0.971 | 0.185 | 183.005 | $<0.001$ |'
- en: 'Table 3: Comparison of accuracy by agent for GPT-4'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：GPT-4按代理比较的准确性
- en: '| Model Name | Baseline | Retry | Keywords | Advice | Instruction | Explanation
    | Solution | Composite | Unredacted |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 基线 | 重试 | 关键词 | 建议 | 说明 | 解释 | 解决方案 | 复合 | 未删减 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Claude 3 Opus | 0.792 | 0.849 | 0.855 | 0.852 | 0.853 | 0.908 | 0.939 | 0.947
    | 0.971 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Opus | 0.792 | 0.849 | 0.855 | 0.852 | 0.853 | 0.908 | 0.939 | 0.947
    | 0.971 |'
- en: '| Cohere Command R+ | 0.641 | 0.745 | 0.77 | 0.733 | 0.798 | 0.77 | 0.843 |
    0.874 | 0.937 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Cohere Command R+ | 0.641 | 0.745 | 0.77 | 0.733 | 0.798 | 0.77 | 0.843 |
    0.874 | 0.937 |'
- en: '| Gemini 1.0 Pro | 0.617 | 0.724 | 0.734 | 0.724 | 0.725 | 0.748 | 0.763 |
    0.774 | 0.881 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Gemini 1.0 Pro | 0.617 | 0.724 | 0.734 | 0.724 | 0.725 | 0.748 | 0.763 |
    0.774 | 0.881 |'
- en: '| Gemini 1.5 Pro | 0.751 | 0.813 | 0.807 | 0.824 | 0.804 | 0.812 | 0.818 |
    0.815 | 0.972 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Gemini 1.5 Pro | 0.751 | 0.813 | 0.807 | 0.824 | 0.804 | 0.812 | 0.818 |
    0.815 | 0.972 |'
- en: '| GPT-3.5 Turbo | 0.596 | 0.686 | 0.691 | 0.704 | 0.706 | 0.802 | 0.831 | 0.827
    | 0.904 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 Turbo | 0.596 | 0.686 | 0.691 | 0.704 | 0.706 | 0.802 | 0.831 | 0.827
    | 0.904 |'
- en: '| GPT-4 | 0.786 | 0.827 | 0.832 | 0.840 | 0.849 | 0.876 | 0.925 | 0.932 | 0.971
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 0.786 | 0.827 | 0.832 | 0.840 | 0.849 | 0.876 | 0.925 | 0.932 | 0.971
    |'
- en: '| Llama 2 70b | 0.376 | 0.481 | 0.564 | 0.591 | 0.575 | 0.655 | 0.600 | 0.672
    | 0.837 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 70b | 0.376 | 0.481 | 0.564 | 0.591 | 0.575 | 0.655 | 0.600 | 0.672
    | 0.837 |'
- en: '| Llama 2 7b | 0.297 | 0.372 | 0.364 | 0.374 | 0.377 | 0.457 | 0.413 | 0.427
    | 0.495 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 7b | 0.297 | 0.372 | 0.364 | 0.374 | 0.377 | 0.457 | 0.413 | 0.427
    | 0.495 |'
- en: '| Mistral Large | 0.723 | 0.769 | 0.796 | 0.802 | 0.803 | 0.825 | 0.889 | 0.896
    | 0.922 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Mistral Large | 0.723 | 0.769 | 0.796 | 0.802 | 0.803 | 0.825 | 0.889 | 0.896
    | 0.922 |'
- en: 'Table 4: Accuracy by model and agent'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：按模型和代理的准确性
- en: '| Agent Title | AQUA-RAT | ARC | Hellaswag | LSAT-AR | LSAT-LR | LSAT-RC |
    LogiQA | MedMCQA | SAT-English | SAT-Math |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 代理标题 | AQUA-RAT | ARC | Hellaswag | LSAT-AR | LSAT-LR | LSAT-RC | LogiQA
    | MedMCQA | SAT-English | SAT-Math |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Baseline | 0.79 | 0.95 | 0.89 | 0.33 | 0.83 | 0.85 | 0.62 | 0.77 | 0.93 |
    0.90 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 0.79 | 0.95 | 0.89 | 0.33 | 0.83 | 0.85 | 0.62 | 0.77 | 0.93 | 0.90
    |'
- en: '| Retry | 0.83 | 0.96 | 0.92 | 0.45 | 0.84 | 0.86 | 0.68 | 0.79 | 0.95 | 0.99
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 重试 | 0.83 | 0.96 | 0.92 | 0.45 | 0.84 | 0.86 | 0.68 | 0.79 | 0.95 | 0.99
    |'
- en: '| Keywords | 0.85 | 0.97 | 0.91 | 0.45 | 0.85 | 0.88 | 0.69 | 0.81 | 0.94 |
    0.97 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 关键词 | 0.85 | 0.97 | 0.91 | 0.45 | 0.85 | 0.88 | 0.69 | 0.81 | 0.94 | 0.97
    |'
- en: '| Advice | 0.87 | 0.98 | 0.92 | 0.45 | 0.84 | 0.88 | 0.71 | 0.84 | 0.94 | 0.97
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 建议 | 0.87 | 0.98 | 0.92 | 0.45 | 0.84 | 0.88 | 0.71 | 0.84 | 0.94 | 0.97
    |'
- en: '| Instructions | 0.86 | 0.98 | 0.93 | 0.48 | 0.86 | 0.88 | 0.71 | 0.88 | 0.95
    | 0.96 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 说明 | 0.86 | 0.98 | 0.93 | 0.48 | 0.86 | 0.88 | 0.71 | 0.88 | 0.95 | 0.96
    |'
- en: '| Explanation | 0.86 | 0.99 | 0.93 | 0.53 | 0.91 | 0.93 | 0.76 | 0.92 | 0.96
    | 0.97 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 0.86 | 0.99 | 0.93 | 0.53 | 0.91 | 0.93 | 0.76 | 0.92 | 0.96 | 0.97
    |'
- en: '| Solution | 0.88 | 1.00 | 0.96 | 0.76 | 0.94 | 0.95 | 0.87 | 0.94 | 0.97 |
    0.98 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 解答 | 0.88 | 1.00 | 0.96 | 0.76 | 0.94 | 0.95 | 0.87 | 0.94 | 0.97 | 0.98
    |'
- en: '| Composite | 0.87 | 1.00 | 0.99 | 0.72 | 0.99 | 0.96 | 0.88 | 0.95 | 0.98
    | 0.98 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 复合 | 0.87 | 1.00 | 0.99 | 0.72 | 0.99 | 0.96 | 0.88 | 0.95 | 0.98 | 0.98
    |'
- en: '| Unredacted | 0.91 | 1.00 | 0.99 | 0.92 | 0.99 | 0.98 | 0.95 | 0.99 | 0.99
    | 0.99 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 未修改 | 0.91 | 1.00 | 0.99 | 0.92 | 0.99 | 0.98 | 0.95 | 0.99 | 0.99 | 0.99
    |'
- en: 'Table 5: Accuracy by agent and exam for GPT-4'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：GPT-4 的代理和考试准确率
- en: Appendix B Data
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 数据
- en: '[PRE0]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Figure 5: Sample of an MCQA problem in JSON-L format – with whitespace added
    for readability.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：JSON-L 格式的 MCQA 问题示例——为了提高可读性，添加了空白。
- en: '[PRE1]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Figure 6: Sample of the answer prompt used by the baseline agent to solve MCQA
    problems.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基准代理用于解决 MCQA 问题的回答提示示例。
- en: '[PRE2]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Figure 7: Sample of an MCQA problem in JSON-L format – with whitespace added
    for readability.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：JSON-L 格式的 MCQA 问题示例——为了提高可读性，添加了空白。
- en: '[PRE3]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Figure 8: Sample of the re-answer prompt used by the self-reflecting agents.
    The system prompt, example problem, and example solution are identical to the
    answer prompt and thus omitted for clarity.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：自我反思代理使用的重新回答提示示例。系统提示、示例问题和示例解答与回答提示相同，因此为了清晰起见，省略了这些部分。
