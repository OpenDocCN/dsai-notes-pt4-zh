- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:46:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:46:32'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AIOS 编译器：LLM 作为自然语言编程和 AI 代理流程编程的解释器
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06907](https://ar5iv.labs.arxiv.org/html/2405.06907)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06907](https://ar5iv.labs.arxiv.org/html/2405.06907)
- en: Shuyuan Xu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shuyuan Xu
- en: Rutgers University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: shuyuan.xu@rutgers.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: shuyuan.xu@rutgers.edu
- en: 'Zelong Li ¹¹footnotemark: 1'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zelong Li ¹¹脚注标记: 1'
- en: Rutgers University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: zelong.li@rutgers.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: zelong.li@rutgers.edu
- en: Kai Mei
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kai Mei
- en: Rutgers University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: kai.mei@rutgers.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: kai.mei@rutgers.edu
- en: Yongfeng Zhang
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Yongfeng Zhang
- en: Rutgers University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: yongfeng.zhang@rutgers.edu Both authors contributed equally to this work.Corresponding
    author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: yongfeng.zhang@rutgers.edu 两位作者对这项工作贡献相等。通讯作者
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Since their inception, programming languages have trended towards greater readability
    and lower barriers for programmers. Following this trend, natural language can
    be a promising type of programming language that provides great flexibility and
    usability and helps towards the democracy of programming. However, the inherent
    vagueness, ambiguity, and verbosity of natural language pose significant challenges
    in developing an interpreter that can accurately understand the programming logic
    and execute instructions written in natural language. Fortunately, recent advancements
    in Large Language Models (LLMs) have demonstrated remarkable proficiency in interpreting
    complex natural language. Inspired by this, we develop a novel system for Code
    Representation and Execution (CoRE), which employs LLM as interpreter to interpret
    and execute natural language programs (NLPg). The proposed system unifies natural
    language programming, pseudo-code programming, and flow programming under the
    same representation for constructing language agents, while LLM serves as the
    interpreter to interpret and execute the agent programs. In this paper, we begin
    with defining the programming syntax that structures natural language instructions
    logically. During the execution, we incorporate external memory to minimize redundancy.
    Furthermore, we equip the designed interpreter with the capability to invoke external
    tools, compensating for the limitations of LLM in specialized domains or when
    accessing real-time information. This work is open-source at [https://github.com/agiresearch/CoRE](https://github.com/agiresearch/CoRE),
    [https://github.com/agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI),
    and [https://github.com/agiresearch/AIOS](https://github.com/agiresearch/AIOS).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自编程语言诞生以来，它们逐渐趋向于更高的可读性和更低的程序员门槛。遵循这一趋势，自然语言可能是一种有前景的编程语言类型，它提供了极大的灵活性和可用性，并有助于编程的民主化。然而，自然语言固有的模糊性、歧义性和冗长性在开发能够准确理解编程逻辑并执行自然语言编写指令的解释器时带来了显著挑战。幸运的是，近期在大规模语言模型（LLM）领域的进展已展示了在解释复杂自然语言方面的卓越能力。受到此启发，我们开发了一种新的代码表示与执行系统（CoRE），该系统利用LLM作为解释器来解释和执行自然语言程序（NLPg）。所提出的系统将自然语言编程、伪代码编程和流程编程统一为构建语言代理的相同表示形式，同时LLM作为解释器来解释和执行代理程序。在本文中，我们首先定义了逻辑结构化自然语言指令的编程语法。在执行过程中，我们引入了外部记忆以减少冗余。此外，我们为设计的解释器配备了调用外部工具的能力，以弥补LLM在专门领域或访问实时信息时的局限性。这项工作开源于
    [https://github.com/agiresearch/CoRE](https://github.com/agiresearch/CoRE)、[https://github.com/agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI)
    和 [https://github.com/agiresearch/AIOS](https://github.com/agiresearch/AIOS)。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/2c5a6aaf59f8898c31659aa93b9514be.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2c5a6aaf59f8898c31659aa93b9514be.png)'
- en: 'Figure 1: In our CoRE system, we design the CoRE language to unify natural
    language programming, pseudo-code programming, and flow programming in the same
    syntax representative. We use the program for OpenAGI [[14](#bib.bib14)] platform
    as an example.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 在我们的 CoRE 系统中，我们设计了 CoRE 语言，以在相同的语法表示中统一自然语言编程、伪代码编程和流程编程。我们以 OpenAGI
    [[14](#bib.bib14)] 平台的程序为例。'
- en: Programming is crucial for computers as it enables them to execute specific
    tasks based on a predefined set of instructions. It allows us to utilize logical
    algorithms to enable computers to solve problems. Programming has evolved significantly
    since its inception, with new technologies and innovations driving its growth.
    Initially, programming languages were based on binary machine language, such as
    punched cards, which can be directly executed by the machine. However, machine
    language was hardly readable to humans. Subsequently, low-level programming languages,
    such as assembly language, use mnemonic instructions and operands to represent
    machine code, which enhances the readability [[18](#bib.bib18)]. However, due
    to the requirement of controlling memory locations and registers, assembly language
    still has a high entry barrier for programmers. With the design of high-level
    programming languages like C/C++, Java and Python, coding has become more user-friendly
    and efficient. They offer programmers a more productive and accessible approach,
    leading to increased participation in programming and software development. Consequently,
    programming languages are becoming more integrated into everyday life.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 编程对计算机至关重要，因为它使计算机能够根据预定义的一组指令执行特定任务。它允许我们利用逻辑算法使计算机解决问题。自其诞生以来，编程经历了显著的演变，新技术和创新推动了其发展。最初，编程语言基于二进制机器语言，如打孔卡片，这些卡片可以直接由机器执行。然而，机器语言对人类几乎不可读。随后，低级编程语言，如汇编语言，使用助记符指令和操作数来表示机器代码，从而提高了可读性[[18](#bib.bib18)]。然而，由于需要控制内存位置和寄存器，汇编语言对程序员仍有很高的入门门槛。随着C/C++、Java和Python等高级编程语言的设计，编码变得更加用户友好和高效。这些语言为程序员提供了更具生产力和更易于访问的方法，导致编程和软件开发的参与度增加。因此，编程语言越来越融入日常生活中。
- en: From the history of programming languages, we can observe a clear trend toward
    increased usability, readability, and democracy of programming. Following this
    trend, natural language can be a desirable choice for coding due to its accessibility,
    readability, and minimal training requirements for programmers. However, the application
    of natural language programming presents challenges due to the inherent vagueness,
    ambiguity, and verbosity of natural language. The recently emerged Large Language
    Models (LLMs) serve as a solution to this challenge due to their extraordinary
    capability in language understanding [[38](#bib.bib38), [8](#bib.bib8)], tool
    use and function calling [[14](#bib.bib14), [42](#bib.bib42)], as well as interacting
    with human or environments [[43](#bib.bib43), [12](#bib.bib12)]. In this work,
    we propose a novel system for Code Representation and Execution (CoRE), which
    takes LLM as the interpreter to interpret and execute the instructions in natural
    language, enabling agent programming in natural language.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程语言的历史中，我们可以观察到一个明确的趋势，即编程的可用性、可读性和民主化不断提高。遵循这一趋势，自然语言由于其可访问性、可读性和对程序员的培训要求较少，可以成为编码的理想选择。然而，由于自然语言固有的模糊性、歧义性和冗长性，自然语言编程的应用面临挑战。近期出现的大型语言模型（LLMs）因其卓越的语言理解能力[[38](#bib.bib38),
    [8](#bib.bib8)]、工具使用和函数调用能力[[14](#bib.bib14), [42](#bib.bib42)]，以及与人类或环境的互动能力[[43](#bib.bib43),
    [12](#bib.bib12)]，成为解决这一挑战的方案。在这项工作中，我们提出了一种新型的代码表示与执行系统（CoRE），该系统将LLM作为解释器来解释和执行自然语言中的指令，从而实现自然语言中的代理编程。
- en: 'CoRE can be used for natural language programming, pseudo-code programming,
    and flow programming, as the three forms of agent programs unify into our CoRE
    language, as shown by the example in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents"). In the realm of programming, the fundamental task
    involves designing and developing logically structured instructions to address
    specific problems. Natural language programming offers a method where instructions
    are formulated in everyday language, making the code intuitive and accessible.
    When we structure all natural language instructions in a logical way, it inherently
    mirrors the essence of pseudo-code programming. Pseudo-code, by design, simplifies
    the coding process by stripping down syntax complexities and focusing on the algorithmic
    logic for easy understanding. Therefore, when the instructions are expressed in
    natural language, the structured instructions can be identified as pseudo-code.
    Moreover, pseudo-code shares a direct relationship with flow programming, as it
    essentially represents the algorithm’s logic that can seamlessly be visualized
    as a workflow. Workflow, in turn, provides a graphical representation of the step-by-step
    execution of programs, emphasizing the decision-making visualization process and
    the flow of control across the program.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'CoRE 可以用于自然语言编程、伪代码编程和流程编程，因为这三种形式的代理程序统一为我们的 CoRE 语言，如图 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents") 中的示例所示。在编程领域，基本任务涉及设计和开发逻辑结构化的指令以解决特定问题。自然语言编程提供了一种方法，其中指令以日常语言编写，使代码直观易懂。当我们以逻辑方式构建所有自然语言指令时，它本质上反映了伪代码编程的本质。伪代码通过简化编码过程，去除语法复杂性，并专注于算法逻辑，从而使理解变得容易。因此，当指令以自然语言表达时，结构化的指令可以被视为伪代码。此外，伪代码与流程编程直接相关，因为它本质上表示了可以无缝可视化为工作流的算法逻辑。工作流反过来提供了程序逐步执行的图形表示，强调决策可视化过程和程序控制流。'
- en: 'We face several significant challenges when designing the novel system for
    natural language programming with LLM as an interpreter. First, how to represent
    the logic of the program using natural language instructions. To tackle this issue,
    we design a set of programming syntax to logically structure natural language
    instructions, and unify the natural language programming, pseudo-code programming,
    and flow programming in the same representation. Second, given that the programs
    consist of step-by-step instructions, it is crucial to make sure that each step
    is executed according to its corresponding instruction. To ensure precise execution
    of the instructions in natural language for each step, we design two additional
    components: one for retrieving information from memory, and the other for invoking
    external tools. Considering the LLM’s limitation on the number of input tokens
    (context window size), including all runtime information in the input prompt is
    impractical. To address this problem, we store a large volume of intermediate
    results in temporary memory, retrieving relevant information as needed in subsequent
    steps [[48](#bib.bib48), [34](#bib.bib34), [27](#bib.bib27), [4](#bib.bib4)].
    Besides, while LLMs excel at processing textual information, they often fall short
    in tasks that require domain-specific knowledge or up-to-date information [[15](#bib.bib15)].
    To mitigate these limitations, we enable the LLM to utilize external tools to
    solve the problems [[14](#bib.bib14), [42](#bib.bib42), [30](#bib.bib30), [2](#bib.bib2)].
    Finally, when executing the natural language program, incorrectly determining
    the next step can lead to different final results. We solve this problem by demanding
    the LLM interpreter to evaluate the current results so as to identify the most
    suitable subsequent step. The overall execution pipeline of CoRE is depicted in
    Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents").'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计用于自然语言编程的全新系统时，我们面临几个重大挑战，其中 LLM 作为解释器。首先，如何使用自然语言指令表示程序逻辑。为了解决这个问题，我们设计了一套编程语法来逻辑地结构化自然语言指令，并在相同的表示中统一自然语言编程、伪代码编程和流程编程。其次，鉴于程序由逐步指令组成，确保每一步按其对应指令执行至关重要。为确保每一步的自然语言指令精准执行，我们设计了两个额外的组件：一个用于从记忆中检索信息，另一个用于调用外部工具。考虑到
    LLM 对输入 token 数量（上下文窗口大小）的限制，将所有运行时信息包含在输入提示中是不切实际的。为了解决这个问题，我们在临时内存中存储大量中间结果，并在后续步骤中根据需要检索相关信息[[48](#bib.bib48),
    [34](#bib.bib34), [27](#bib.bib27), [4](#bib.bib4)]。此外，尽管 LLM 擅长处理文本信息，但在需要领域特定知识或最新信息的任务中往往表现不足[[15](#bib.bib15)]。为缓解这些限制，我们使
    LLM 能够利用外部工具来解决问题[[14](#bib.bib14), [42](#bib.bib42), [30](#bib.bib30), [2](#bib.bib2)]。最后，在执行自然语言程序时，错误确定下一步可能导致不同的最终结果。我们通过要求
    LLM 解释器评估当前结果以确定最合适的后续步骤来解决这个问题。CoRE 的整体执行流程如图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ AIOS 编译器：LLM
    作为自然语言编程和 AI 代理流程编程的解释器") 所示。
- en: '![Refer to caption](img/4f521cb6d72ec1df2e6f205c7cb0016b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4f521cb6d72ec1df2e6f205c7cb0016b.png)'
- en: 'Figure 2: An example showing how the CoRE system executes one step.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：一个示例展示了 CoRE 系统如何执行一步操作。
- en: 'In summary, the key contributions of the work are listed as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，工作的关键贡献如下：
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design a CoRE language that unifies natural language programming, pseudo-code
    programming and flow programming. The CoRE language logically structures natural
    language instructions.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种 CoRE 语言，统一了自然语言编程、伪代码编程和流程编程。CoRE 语言逻辑上结构化自然语言指令。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose the CoRE system, which utilizes Large Language Model (LLM) as an
    interpreter to interpret and execute instructions step-by-step. During execution,
    the LLM follows the instructions and leverages both information retrieval and
    external tools to enhance its effectiveness.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 CoRE 系统，该系统利用大型语言模型（LLM）作为解释器来逐步解释和执行指令。在执行过程中，LLM 遵循指令，并利用信息检索和外部工具来提升其效果。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We verify the effectiveness and efficiency of our system based on public benchmark
    datasets. Specifically, we employ our proposed system for agent task solving based
    on natural language programs, showcasing its practical capabilities.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们基于公共基准数据集验证了系统的有效性和效率。具体来说，我们使用我们提出的系统来解决基于自然语言程序的代理任务，展示其实际能力。
- en: 'In the following part of this paper, we first provide the related work in Section
    [2](#S2 "2 Related Work ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents"). In Section [3](#S3 "3 The CoRE
    System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and
    Flow Programming of AI Agents") we present the CoRE framework and how the framework
    can be applied to LLM agents. We provide the experimental results in Section [4](#S4
    "4 Experiments ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents"), and conclude the work together with future
    directions in Section [5](#S5 "5 Conclusions and Future Work ‣ AIOS Compiler:
    LLM as Interpreter for Natural Language Programming and Flow Programming of AI
    Agents").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文的后续部分，我们首先在[第 2 节](#S2 "2 Related Work ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents")中提供相关工作。在[第
    3 节](#S3 "3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents")中，我们介绍 CoRE 框架以及如何将该框架应用于 LLM 代理。我们在[第
    4 节](#S4 "4 Experiments ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents")中提供实验结果，并在[第 5 节](#S5 "5 Conclusions
    and Future Work ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents")中总结工作及未来方向。'
- en: 2 Related Work
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Natural Language Programming
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 自然语言编程
- en: Research in natural language programming [[17](#bib.bib17), [5](#bib.bib5),
    [46](#bib.bib46), [28](#bib.bib28), [10](#bib.bib10), [13](#bib.bib13)] primarily
    focus on addressing the ambiguity in translating natural language into programming
    language statements. [Heidorn](#bib.bib17) [[17](#bib.bib17)] proposes to adopt
    heuristic NLP encoding and decoding rules to develop an automatic programming
    system that can accept natural language dialogues. [Vadas and Curran](#bib.bib46)
    [[46](#bib.bib46)] introduce a prototype system that can translate certain English
    instructions into executable Python code using Combinatory Categorial Grammar
    (CCG) parser, which uses unrestricted syntax to cover a wide range of user instruction
    semantics. [Mihalcea et al.](#bib.bib35) [[35](#bib.bib35)] implement a procedural
    natural language programming system to convert natural language to programming
    language. Early natural language programming techniques are restricted in extensibility
    by the need to create domain-specific languages (DSLs). To avoid the problems
    of repeatedly designing new DSLs, [Desai et al.](#bib.bib10) [[10](#bib.bib10)]
    propose a general generative framework for constructing a program that takes natural
    language input and produces the expressions in the target DSL. Further, [Ernst](#bib.bib13)
    [[13](#bib.bib13)] leverages neural networks, i.e., the recurrent neural networks
    (RNN), to convert English specifications of file system operations into corresponding
    bash commands.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言编程的研究[[17](#bib.bib17), [5](#bib.bib5), [46](#bib.bib46), [28](#bib.bib28),
    [10](#bib.bib10), [13](#bib.bib13)] 主要集中在解决将自然语言翻译成编程语言语句中的歧义问题。[Heidorn](#bib.bib17)
    [[17](#bib.bib17)] 提议采用启发式 NLP 编码和解码规则来开发一个能够接受自然语言对话的自动编程系统。[Vadas 和 Curran](#bib.bib46)
    [[46](#bib.bib46)] 介绍了一个原型系统，该系统可以使用组合范畴文法 (CCG) 解析器将某些英语指令翻译成可执行的 Python 代码，该解析器使用不受限制的语法来覆盖广泛的用户指令语义。[Mihalcea
    等](#bib.bib35) [[35](#bib.bib35)] 实现了一个过程性自然语言编程系统，将自然语言转换为编程语言。早期的自然语言编程技术由于需要创建特定领域语言
    (DSL) 而限制了其可扩展性。为了避免反复设计新 DSL 的问题，[Desai 等](#bib.bib10) [[10](#bib.bib10)] 提出了一个通用生成框架，用于构建一个接受自然语言输入并生成目标
    DSL 表达式的程序。此外，[Ernst](#bib.bib13) [[13](#bib.bib13)] 利用神经网络，即递归神经网络 (RNN)，将文件系统操作的英语规范转换为相应的
    bash 命令。
- en: 2.2 Large Language Models and AI Agents for Problem Solving
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大型语言模型和 AI 代理用于问题解决
- en: Large Language Models (LLMs) have emerged as powerful tools for problem solving,
    encompassing tasks in reasoning, planning, and code generation. LLM reasoning
    typically involves decomposing a complex task into a sequence of steps, also known
    as a reasoning chain [[49](#bib.bib49)]. Prominent approaches in LLM reasoning
    include Chain-of-Thought (CoT) and its derivatives [[49](#bib.bib49), [26](#bib.bib26)].
    To further improve the reasoning ability of LLM, several work has been proposed.
    The Self-consistency method [[47](#bib.bib47)] samples multiple reasoning paths
    and selects the most consistent outcome by voting. Additionally, classical data
    structures like trees and graphs are utilized to enhance reasoning efficiency
    and accuracy in fewer steps [[52](#bib.bib52), [3](#bib.bib3)]. Apart from reasoning,
    planning is also an important task that can be used to solve problems. LLM Planning
    involves generating a series of actions to achieve the predefined goals [[16](#bib.bib16)].
    Recent advancements include direct prompting of LLMs for planning tasks, showing
    promising results [[20](#bib.bib20), [45](#bib.bib45), [11](#bib.bib11)]. Finite
    state machines have been integrated into LLM to enhance the planning ability [[29](#bib.bib29),
    [51](#bib.bib51)]. ReAct [[53](#bib.bib53)] proposes to leverage external tools
    like search engine to enhance the LLM planning. Besides, considering the powerful
    ability of LLM in programming, recent work propose to generate programming code
    to solve problems [[32](#bib.bib32), [22](#bib.bib22), [31](#bib.bib31), [6](#bib.bib6),
    [23](#bib.bib23), [40](#bib.bib40), [36](#bib.bib36)]. Furthermore, the “self-reflection”
    mechanism [[33](#bib.bib33), [39](#bib.bib39), [44](#bib.bib44)] enables LLMs
    to critique their own outputs, significantly enhancing performance in tasks such
    as reasoning [[3](#bib.bib3)] and code generation [[7](#bib.bib7)]. In contrast
    to existing methods that directly use LLMs for generating solutions, the proposed
    CoRE system utilizes LLMs as interpreters, executing solutions designed by humans
    to address complex questions. This approach leverages human creativity in solution
    design, coupled with LLM’s ability, to enhance problem-solving capabilities in
    natural language programming contexts.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已成为解决问题的强大工具，涵盖了推理、规划和代码生成等任务。LLM 推理通常涉及将复杂任务分解为一系列步骤，也称为推理链 [[49](#bib.bib49)]。LLM
    推理中的突出方法包括 Chain-of-Thought（CoT）及其衍生方法 [[49](#bib.bib49), [26](#bib.bib26)]。为了进一步提高
    LLM 的推理能力，已经提出了几项工作。自洽性方法 [[47](#bib.bib47)] 通过投票选择最一致的结果，并对多个推理路径进行采样。此外，经典数据结构如树和图被用于提高推理效率和准确性
    [[52](#bib.bib52), [3](#bib.bib3)]。除了推理，规划也是一个可以用来解决问题的重要任务。LLM 规划涉及生成一系列行动以实现预定目标
    [[16](#bib.bib16)]。最近的进展包括对 LLM 进行直接提示以进行规划任务，显示出有前景的结果 [[20](#bib.bib20), [45](#bib.bib45),
    [11](#bib.bib11)]。有限状态机已被集成到 LLM 中以增强规划能力 [[29](#bib.bib29), [51](#bib.bib51)]。ReAct
    [[53](#bib.bib53)] 提出利用搜索引擎等外部工具来增强 LLM 规划。此外，考虑到 LLM 在编程中的强大能力，最近的工作提出生成编程代码以解决问题
    [[32](#bib.bib32), [22](#bib.bib22), [31](#bib.bib31), [6](#bib.bib6), [23](#bib.bib23),
    [40](#bib.bib40), [36](#bib.bib36)]。此外，“自我反思”机制 [[33](#bib.bib33), [39](#bib.bib39),
    [44](#bib.bib44)] 使 LLM 能够批判自己的输出，显著提高了在推理 [[3](#bib.bib3)] 和代码生成 [[7](#bib.bib7)]
    等任务中的表现。与现有的直接使用 LLM 生成解决方案的方法相比，提议的 CoRE 系统利用 LLM 作为解释器，执行由人类设计的解决方案以解决复杂问题。这种方法结合了人类在解决方案设计中的创造力和
    LLM 的能力，以提高在自然语言编程环境中的问题解决能力。
- en: '![Refer to caption](img/ef8bf251abb6c21bf49116f3966e4d4a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ef8bf251abb6c21bf49116f3966e4d4a.png)'
- en: 'Figure 3: An overview of the CoRE LLM interpreter system.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: CoRE LLM 解释器系统概述。'
- en: 3 The CoRE System
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CoRE 系统
- en: In this section, we will introduce how we define the natural language programming
    syntax and how to use LLM as an interpreter to interpret and execute natural language
    programs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何定义自然语言编程语法以及如何使用 LLM 作为解释器来解释和执行自然语言程序。
- en: 3.1 CoRE Language Syntax
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 CoRE 语言语法
- en: 'To organize natural language instructions, we define the basic structural representation
    for each step, which consists of four components. An example can be found in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for
    Natural Language Programming and Flow Programming of AI Agents").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了组织自然语言指令，我们定义了每个步骤的基本结构表示，该表示由四个组件组成。一个示例可以在图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ AIOS
    编译器：LLM 作为自然语言编程和 AI 代理的流程编程解释器") 中找到。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Name: Each step in the program is uniquely identified by a step name.
    This identifier is analogous to function identifiers in traditional programming
    languages, which facilitates navigation and reference within the program structure,
    ensuring that each operation within the program can be distinctly addressed and
    accessed.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤名称：程序中的每个步骤都通过步骤名称唯一标识。这个标识符类似于传统编程语言中的函数标识符，有助于在程序结构中导航和引用，确保每个程序中的操作可以被明确地处理和访问。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Type: The step type categorizes the nature of the operation being performed
    in each step, analogous to control structures in conventional programming. We
    define three primary step types:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤类型：步骤类型对每个步骤中执行的操作性质进行分类，类似于传统编程中的控制结构。我们定义了三种主要的步骤类型：
- en: –
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Process: Akin to a procedural statement in traditional programming, this step
    type executes a specific operation and transitions to the next specified step.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过程：类似于传统编程中的过程语句，这种步骤类型执行特定操作并过渡到下一个指定步骤。
- en: –
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Decision: Corresponding to conditional statements (e.g., “if-else”), this step
    involves branching the program flow based on evaluated conditions, leading to
    multiple potential paths.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 决策：对应于条件语句（例如，“if-else”），这一步根据评估的条件分支程序流，导致多个潜在路径。
- en: –
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Terminal: Similar to the “end” or “return” statement, this step marks the conclusion
    of the program, indicating that no further steps are to be executed.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 终端：类似于“end”或“return”语句，这一步标志着程序的结束，表示不再执行进一步的步骤。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Instruction: The step instruction explicates the task to be conducted
    at a step. This component is integral as it provides the instruction and content
    for execution, paralleling the statement block in traditional programming languages.
    By demonstrating operations in natural language, NLPg lowers the barrier to programming,
    making it more readable for non-expert programmers.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤指令：步骤指令阐明了在步骤中要执行的任务。这个组件至关重要，因为它提供了执行的指令和内容，与传统编程语言中的语句块相似。通过以自然语言展示操作，NLPg
    降低了编程的门槛，使其对非专业程序员更易读。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Connection: Step connections define the progression from one step to another,
    establishing the flow of the program execution. In process steps, a single subsequent
    step is specified. In decision steps, multiple pathways are delineated based on
    conditions. Terminal steps, by definition, do not lead to any future steps, indicating
    the end of program execution.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤连接：步骤连接定义了从一个步骤到另一个步骤的进展，建立了程序执行的流程。在过程步骤中，指定了一个后续步骤。在决策步骤中，根据条件划分了多个路径。终端步骤，按定义，不会引导到任何未来步骤，标志着程序执行的结束。
- en: 'For each step in the program, the above four components are separated by “:::”
    (as illustrated in the CoRE language in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents")). Other special tokens can also be used to separate
    different components.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于程序中的每个步骤，上述四个组件用“:::”（如图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ AIOS 编译器：LLM 作为自然语言编程和 AI
    代理流编程的解释器")）分隔。也可以使用其他特殊标记来分隔不同的组件。
- en: '![Refer to caption](img/83df2ac177fb5aba7e0603621082d0dc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83df2ac177fb5aba7e0603621082d0dc.png)'
- en: 'Figure 4: An example showing how the CoRE system retrieves relevant information.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：一个示例展示了 CoRE 系统如何检索相关信息。
- en: 'In programming languages, there are three basic control constructs in programming
    [[9](#bib.bib9), [41](#bib.bib41)]: sequence, selection and iteration. These three
    basic constructs can be easily designed within the CoRE language.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程语言中，编程有三种基本控制结构 [[9](#bib.bib9), [41](#bib.bib41)]：顺序、选择和迭代。这三种基本结构可以很容易地在
    CoRE 语言中设计。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sequence: Sequence in programming is the execution of statements in a linear
    order, with each statement leading to the next. In the CoRE framework, this construct
    is designed by setting the “Step Connection” to point to the subsequent step.
    Each step operates under the Process type until the sequence concludes.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顺序：编程中的顺序是以线性顺序执行语句，每个语句引导到下一个。在 CoRE 框架中，这个结构通过将“步骤连接”设置为指向后续步骤来设计。每个步骤在顺序结束之前都以过程类型运行。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Selection: Selection in programming languages facilitates conditional branching,
    allowing the program to execute different sequences of steps based on specific
    conditions. This is implemented using the Decision step type where the “Step Connection”
    part explicitly outlines multiple potential paths. Each branch is defined by a
    condition stated within the “Step Connection” part, guiding the program flow to
    various steps depending on the conditions.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择：编程语言中的选择促进了条件分支，使程序能够根据特定条件执行不同的步骤序列。这通过决策步骤类型实现，其中“步骤连接”部分明确列出了多个潜在路径。每个分支由“步骤连接”部分中说明的条件定义，根据条件引导程序流向不同的步骤。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Iteration: Iteration involves repeating a set of operations until a certain
    condition is met, akin to loops in conventional programming. In the CoRE framework,
    we utilize a step with the Decision type to assess whether the loop condition
    has been fulfilled. At the end of one loop cycle, the “Step Connection” is configured
    to point back to the previous Decision step, thereby enabling the continuation
    of the loop.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代：迭代涉及重复一组操作，直到满足某个条件，类似于传统编程中的循环。在CoRE框架中，我们使用一个决策类型的步骤来评估循环条件是否已满足。在一个循环周期结束时，“步骤连接”被配置为指向先前的决策步骤，从而实现循环的继续。
- en: 3.2 LLM as Interpreter
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLM作为解释器
- en: 'In this section, we will discuss how the CoRE system utilizes a Large Language
    Model (LLM) as an interpreter to execute programs written in the CoRE language.
    We will demonstrate the execution of a single step within the CoRE system, which
    is illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Large Language Models and
    AI Agents for Problem Solving ‣ 2 Related Work ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents"). More specifically,
    the system executes a single step in four procedures. First of all, the interpreter
    determines the useful information to execute the current step. Then the interpreter
    will integrate all relevant information to construct the prompt. Based on the
    generated prompt, the interpreter will generate response and may utilize tools
    to execute the current step. Finally, after executing the current step, the interpreter
    will determine the next step based on step type and execution results. We will
    introduce the four parts in details.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们将讨论CoRE系统如何利用大型语言模型（LLM）作为解释器来执行用CoRE语言编写的程序。我们将演示CoRE系统中单步执行的过程，如图[3](#S2.F3
    "Figure 3 ‣ 2.2 Large Language Models and AI Agents for Problem Solving ‣ 2 Related
    Work ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and
    Flow Programming of AI Agents")所示。具体来说，系统在四个程序中执行一个步骤。首先，解释器确定执行当前步骤所需的信息。然后，解释器将整合所有相关信息以构建提示。根据生成的提示，解释器将生成响应，并可能利用工具执行当前步骤。最后，在执行当前步骤后，解释器将根据步骤类型和执行结果确定下一步。我们将详细介绍这四个部分。'
- en: 3.2.1 Observation Retrieval from Memory
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 从内存中检索观察
- en: 'This initial procedure is critical since it sets the stage for the entire execution
    process of the current step. Figure [4](#S3.F4 "Figure 4 ‣ 3.1 CoRE Language Syntax
    ‣ 3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents") shows an example. The system’s memory serves
    as a repository of all prior observations related to the program, where the observation
    represents the results of tool execution, such as search results. During this
    phase, the interpreter scans the memory to identify records that are relevant
    to the current instruction. This selective retrieval ensures that the interpreter’s
    decisions are informed by accurate and contextually relevant data, which is crucial
    for the successful execution of the program.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '这个初始过程至关重要，因为它为当前步骤的整个执行过程奠定了基础。图[4](#S3.F4 "Figure 4 ‣ 3.1 CoRE Language Syntax
    ‣ 3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents")展示了一个示例。系统的内存作为所有与程序相关的先前观察的存储库，其中观察代表工具执行的结果，如搜索结果。在这个阶段，解释器扫描内存以识别与当前指令相关的记录。这种选择性检索确保了解释器的决策基于准确且具有上下文相关的数据，这对于程序的成功执行至关重要。'
- en: '![Refer to caption](img/11b695f56fb432a75407286203b85b0b.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/11b695f56fb432a75407286203b85b0b.png)'
- en: 'Figure 5: An example showing how the CoRE system analyze the output from the
    LLM interpreter.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：一个示例展示了CoRE系统如何分析LLM解释器的输出。
- en: 3.2.2 Input Prompt Construction
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 输入提示构建
- en: 'Constructing the prompt is essentially about synthesizing the information into
    a comprehensive and coherent query that the LLM can understand and respond to
    effectively. This involves combining multiple information into a single, structured
    prompt that guides the LLM towards generating the most appropriate and contextually
    relevant response. In the CoRE system, the interpreter constructs a detailed prompt
    with four elements:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 构建提示本质上是将信息综合成一个全面且连贯的查询，以便 LLM 能够理解并有效响应。这涉及将多个信息结合成一个结构化的提示，指导 LLM 生成最合适且与上下文相关的响应。在
    CoRE 系统中，解释器构建了一个包含四个元素的详细提示：
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task Description: The query that defines the entire program, acting as the
    primary input to guide the system’s operations.'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务描述：定义整个程序的查询，作为指导系统操作的主要输入。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Current Progress: Summarizes the previous steps including what has been done
    or decided, helping maintain a narrative flow.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前进度：总结之前的步骤，包括已完成或已决定的内容，帮助保持叙事流畅。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Observation: This part may not be included in every step. When relevant information
    is retrieved from the memory by the interpreter, it is incorporated here.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察：这一部分可能不会出现在每一步中。当解释器从记忆中检索相关信息时，会将其纳入这里。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Current Instruction: Specifies the action to be taken in natural language,
    directing the interpreter on how to proceed in the current step.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前指令：用自然语言指定要执行的操作，指导解释器如何在当前步骤中进行。
- en: 3.2.3 Output Analysis
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 输出分析
- en: 'While the LLM can generate direct responses, complex tasks may require capabilities
    beyond its immediate scope. Incorporating the use of specialized tools when necessary
    extends the LLM’s capabilities, allowing the system to handle a broader range
    of tasks effectively. A demonstrative example of the execution process is shown
    in Figure [5](#S3.F5 "Figure 5 ‣ 3.2.1 Observation Retrieval from Memory ‣ 3.2
    LLM as Interpreter ‣ 3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for
    Natural Language Programming and Flow Programming of AI Agents"). In the CoRE
    system, the interpreter will make a decision about if or not to employ specialized
    tools based on the LLM’s initial response and the demands of the task at hand,
    which ensures that the system remains highly functional and versatile, actively
    solving problems rather than merely processing the language prompt for the current
    step. Specifically, if tool usage is warranted, the system will select the suitable
    tool, configure it with the necessary parameters, execute it, and integrate the
    output into the ongoing process.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LLM 可以生成直接响应，但复杂任务可能需要超出其即时范围的能力。在必要时，结合使用专门工具可以扩展 LLM 的能力，使系统能更有效地处理更广泛的任务。图
    [5](#S3.F5 "图 5 ‣ 3.2.1 从记忆中检索观察 ‣ 3.2 LLM 作为解释器 ‣ 3 CoRE 系统 ‣ AIOS 编译器：LLM 作为自然语言编程和
    AI 代理流程编程的解释器") 显示了执行过程的示范性示例。在 CoRE 系统中，解释器将根据 LLM 的初步响应和任务要求决定是否使用专门工具，这确保了系统保持高度功能性和多样性，积极解决问题，而不仅仅是处理当前步骤的语言提示。具体而言，如果需要使用工具，系统将选择合适的工具，配置必要的参数，执行工具，并将输出集成到正在进行的过程中。
- en: 3.2.4 Branching Analysis
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 分支分析
- en: '![Refer to caption](img/eb570220bb754c763f4e54d30a5d1817.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb570220bb754c763f4e54d30a5d1817.png)'
- en: 'Figure 6: An example showing how the CoRE system determines the next step in
    the flow.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：一个示例展示了 CoRE 系统如何确定流程中的下一步。
- en: 'Determining the appropriate next step in the program is critical, especially
    in multi-branch scenarios where different outcomes can lead to different subsequent
    actions. Figure [6](#S3.F6 "Figure 6 ‣ 3.2.4 Branching Analysis ‣ 3.2 LLM as Interpreter
    ‣ 3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents") shows an example. In the CoRE language interpreter,
    the Decision steps indicate multiple branches with the corresponding conditions.
    The interpreter uses LLM to decide if the prompt satisfies the natural language
    described branching condition or not and which next step to take. This adaptive
    approach allows the system to navigate through decision points effectively, ensuring
    logical progression toward the program’s goals.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '确定程序中的适当下一步至关重要，特别是在多分支场景中，不同的结果可能导致不同的后续操作。图 [6](#S3.F6 "Figure 6 ‣ 3.2.4
    Branching Analysis ‣ 3.2 LLM as Interpreter ‣ 3 The CoRE System ‣ AIOS Compiler:
    LLM as Interpreter for Natural Language Programming and Flow Programming of AI
    Agents") 显示了一个示例。在 CoRE 语言解释器中，决策步骤指示具有相应条件的多个分支。解释器使用 LLM 来决定提示是否满足自然语言描述的分支条件，以及采取哪个下一步。这种自适应的方法使系统能够有效地导航决策点，确保逻辑地向程序的目标推进。'
- en: 4 Experiments
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Backbone Large Language Model (LLM)
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基础大语言模型（LLM）
- en: 'We conduct experiments on both closed-source and open-source LLMs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在闭源和开源的 LLM 上进行实验：
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-4 [[37](#bib.bib37)] (Closed-source) is a generative pre-trained transformer
    of OpenAI. In this work, we use the GPT-4-1106-preview version.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4 [[37](#bib.bib37)]（闭源）是 OpenAI 的生成式预训练变换器。在这项工作中，我们使用的是 GPT-4-1106-preview
    版本。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixtral-8x7B [[21](#bib.bib21)] (Open-source) is a pre-trained generative Sparse
    Mixture of Experts with 46.7 billion parameters.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mixtral-8x7B [[21](#bib.bib21)]（开源）是一个预训练的稀疏专家混合模型，具有 467 亿个参数。
- en: 4.2 Planning Schema of LLMs
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LLM 的规划方案
- en: 'We adopt the following LLM-based agent planning schema:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用以下基于 LLM 的代理规划方案：
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Zero-shot Learning (Zero) directly inputs the query to the LLM.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Zero-shot Learning (Zero) 直接将查询输入到 LLM 中。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Chain-of-Thought (CoT) [[49](#bib.bib49)] induces the LLM to generate a coherent
    language sequence that serves as a meaningful intermediate step bridging the input
    query and the output answer.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Chain-of-Thought (CoT) [[49](#bib.bib49)] 使 LLM 生成一个连贯的语言序列，作为连接输入查询和输出答案的有意义的中间步骤。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Few-shot Learning (Few) presents a set of high-quality demonstrations in the
    prompt, each consisting of both input and desired output on the target task.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Few-shot Learning (Few) 在提示中展示了一组高质量的示例，每个示例都包含目标任务的输入和期望输出。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CoRE is our natural language programming method with LLM as an interpreter.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoRE 是我们的自然语言编程方法，以 LLM 作为解释器。
- en: 4.3 Benchmark Datasets
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基准数据集
- en: 'We conduct experiments on a benchmark dataset, OpenAGI [[14](#bib.bib14)].
    The OpenAGI benchmark tasks are categorized based on their output type and ground-truth
    label type (Task 1, 2, and 3). Then, based on different task types, different
    metrics are employed to gauge the performance: CLIP Score [[19](#bib.bib19)],
    assessing the similarity between text and image, is utilized for Text-to-Image
    tasks; BERT Score [[55](#bib.bib55)], evaluating text generation with BERT, is
    applied when both data labels and the expected outputs are texts; and ViT Score
    [[50](#bib.bib50)] gauges the similarity between the image label and image output.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在基准数据集 OpenAGI [[14](#bib.bib14)] 上进行实验。OpenAGI 基准任务根据其输出类型和真实标签类型（任务 1、2
    和 3）进行分类。然后，根据不同的任务类型，采用不同的指标来评估性能：CLIP Score [[19](#bib.bib19)] 评估文本和图像之间的相似性，用于文本到图像任务；BERT
    Score [[55](#bib.bib55)] 评估使用 BERT 的文本生成，当数据标签和期望输出都是文本时应用；ViT Score [[50](#bib.bib50)]
    评估图像标签和图像输出之间的相似性。
- en: 4.4 Implementation Details
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实施细节
- en: Our framework and all baselines are implemented by PyTorch, an open-source library.
    We follow the implementation setting of the OpenAGI platform [[14](#bib.bib14)]
    for Zero-shot and few-shot learnings. We leverage the DSPy framework [[24](#bib.bib24),
    [25](#bib.bib25)] to apply the CoT strategy to the OpenAGI platform. We also tried
    Program-of-Thought [[6](#bib.bib6)] and ReAct [[54](#bib.bib54)] strategies on
    the OpenAGI platform. However, the ReAct strategy requires text observation, which
    is unsuitable for our OpenAGI task since some observations are in image format,
    and Program-of-Thought cannot generate executable codes. Thus, we did not include
    them as the baselines.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架和所有基准均使用开源库 PyTorch 实现。我们遵循 OpenAGI 平台 [[14](#bib.bib14)] 的零样本和少样本学习的实现设置。我们利用
    DSPy 框架 [[24](#bib.bib24), [25](#bib.bib25)] 将 CoT 策略应用于 OpenAGI 平台。我们还尝试了 Program-of-Thought
    [[6](#bib.bib6)] 和 ReAct [[54](#bib.bib54)] 策略，但 ReAct 策略需要文本观察，这不适合我们的 OpenAGI
    任务，因为一些观察是图像格式，且 Program-of-Thought 无法生成可执行代码。因此，我们没有将它们作为基准。
- en: 4.5 Experimental Analysis
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 实验分析
- en: 'The experiment results on the OpenAGI benchmark are shown in Table [1](#S4.T1
    "Table 1 ‣ 4.5 Experimental Analysis ‣ 4 Experiments ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents"). Each row
    stands for a type of task, each column represents the planning schema of an LLM
    interpreter, and every four columns are the results of the same LLM interpreter.
    From the results, we can see that our CoRE planning schema is better on average
    performance than any baseline under both Mixtral and GPT-4 as the interpreters.
    When using Mixtral as the interpreter, CoRE outperforms Zero-shot and CoT under
    each type of task, and is better than Few-shot learning on Task 2 and average
    score, though worse on Task 3 and slightly worse on Task 1\. When using GPT-4
    as the interpreter, CoT, Few-shot has similar performance on Task 1 and Task 3,
    while on Task 2 and average score, CoRE is still the best. It may be worth noting
    that it is unfair to compare CoRE with Few-shot learning since we do not directly
    provide the output format and output example in the prompt. However, even without
    using such examples, the CoRE planning strategy is still better than the Few-shot
    strategy on average. We also find that even for the same CoRE program, the system
    may perform differently when using different LLM as interpreters, which means
    that the performance of natural language programming depends on the natural language
    understanding ability of the LLM interpreter.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'OpenAGI 基准上的实验结果见表 [1](#S4.T1 "Table 1 ‣ 4.5 Experimental Analysis ‣ 4 Experiments
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents")。每一行代表一种任务类型，每一列代表一个 LLM 解释器的规划方案，每四列是同一 LLM 解释器的结果。从结果可以看出，我们的
    CoRE 规划方案在 Mixtral 和 GPT-4 作为解释器的情况下，平均表现优于任何基准。当使用 Mixtral 作为解释器时，CoRE 在每种任务类型下都优于
    Zero-shot 和 CoT，并且在任务 2 和平均分数上优于少样本学习，尽管在任务 3 和任务 1 上表现稍逊。当使用 GPT-4 作为解释器时，CoT
    和少样本在任务 1 和任务 3 上表现相似，而在任务 2 和平均分数上，CoRE 仍然是最好的。值得注意的是，由于我们没有在提示中直接提供输出格式和输出示例，因此将
    CoRE 与少样本学习进行比较是不公平的。然而，即使没有使用这些示例，CoRE 规划策略在平均水平上仍优于少样本策略。我们还发现，即使是相同的 CoRE 程序，当使用不同的
    LLM 作为解释器时，系统可能表现不同，这意味着自然语言编程的性能依赖于 LLM 解释器的自然语言理解能力。'
- en: '| Metrics / Task | Mixtral (open source) as LLM interpreter | GPT-4 (closed-source)
    as LLM interpreter |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 指标 / 任务 | 作为 LLM 解释器的 Mixtral（开源） | 作为 LLM 解释器的 GPT-4（闭源） |'
- en: '| Zero | CoT | Few | CoRE (Ours) | Zero | CoT | Few | CoRE (Ours) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Zero | CoT | Few | CoRE（我们的） | Zero | CoT | Few | CoRE（我们的） |'
- en: '| Task 1 (CLIP Score) | 0.0 | 0.0 | $0.1839$ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 任务 1（CLIP 分数） | 0.0 | 0.0 | $0.1839$ |'
- en: '| Task 2 (BERT Score) | 0.1092 | 0.1987 | 0.0687 | $0.2593$ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 任务 2（BERT 分数） | 0.1092 | 0.1987 | 0.0687 | $0.2593$ |'
- en: '| Task 3 (ViT Score) | 0.1949 | 0.1562 | $0.5501$ | 0.6611 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 任务 3（ViT 分数） | 0.1949 | 0.1562 | $0.5501$ | 0.6611 |'
- en: '| Average over tasks | 0.1206 | 0.1736 | 0.1887 | $0.2483$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 任务平均 | 0.1206 | 0.1736 | 0.1887 | $0.2483$ |'
- en: '| % of Valid Plans | 23.08 | 38.46 | 46.15 | $56.92$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 有效计划的百分比 | 23.08 | 38.46 | 46.15 | $56.92$ |'
- en: 'Table 1: OpenAGI [[14](#bib.bib14)] benchmark task performances under different
    settings. Zero is for Zero-shot Learning, Few is for Few-shot Learning. The boldface
    numbers denote the highest score under each task type using the same LLM.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：OpenAGI [[14](#bib.bib14)] 基准任务在不同设置下的表现。Zero 代表零样本学习，Few 代表少样本学习。粗体数字表示在每种任务类型下使用相同
    LLM 的最高分数。
- en: 5 Conclusions and Future Work
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: In this study, we introduce a novel system, CoRE, for Code Representation and
    Execution. CoRE is designed to bridge natural language programming, pseudo-code,
    and flow programming through the development of a unified CoRE language for the
    construction of AI Agents. CoRE leverages natural language as the programming
    interface, which lowers the programming barrier and advocates the democracy of
    programming, so that even ordinary users can create their AI Agents. Our system
    leverages Large Language Models (LLMs) as interpreters to process and execute
    natural language instructions. Throughout execution, the interpreter dynamically
    retrieves necessary information, utilizes appropriate external tools, and navigates
    through instructions based on previous outputs. The experimental outcomes validate
    the efficacy of the CoRE system in natural language programming.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们介绍了一个新颖的系统 CoRE，用于代码表示和执行。CoRE 旨在通过开发统一的 CoRE 语言来桥接自然语言编程、伪代码和流程编程，以构建
    AI 代理。CoRE 利用自然语言作为编程接口，从而降低编程门槛，并倡导编程的民主化，使得普通用户也能创建他们的 AI 代理。我们的系统利用大型语言模型（LLMs）作为解释器来处理和执行自然语言指令。在执行过程中，解释器动态检索必要的信息，使用适当的外部工具，并根据先前的输出导航指令。实验结果验证了
    CoRE 系统在自然语言编程中的有效性。
- en: While CoRE demonstrates promising results, it currently relies on manually crafted
    programs, which may introduce inefficiencies due to the inherent ambiguities of
    natural language. To address this, future research could explore the development
    of automated systems for generating natural language programming instructions.
    This automation would help standardize instruction clarity and precision, potentially
    improving system performance. Additionally, a future direction is to expand CoRE’s
    language support to facilitate international use and implement real-time debugging
    features to aid in education and assist novice programmers, further broadening
    the system’s utility and accessibility.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CoRE 展示了有希望的结果，但它目前依赖于手工制作的程序，这可能由于自然语言固有的模糊性而引入低效。为了解决这个问题，未来的研究可以探索开发自动化系统来生成自然语言编程指令。这种自动化将有助于标准化指令的清晰度和精确度，从而可能改善系统性能。此外，未来的方向是扩展
    CoRE 的语言支持，以促进国际使用，并实现实时调试功能，以帮助教育和协助初学程序员，进一步扩大系统的实用性和可达性。
- en: References
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1]'
- en: 'Ahn et al. [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
    Karol Hausman, et al. 2022. Do as i can, not as i say: Grounding language in robotic
    affordances. *arXiv preprint arXiv:2204.01691* (2022).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等 [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar
    Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
    Hausman 等。2022。做我能做的，而不是我说的：将语言与机器人可操作性结合。*arXiv 预印本 arXiv:2204.01691* (2022)。
- en: 'Besta et al. [2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski,
    Piotr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with
    large language models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 38\. 17682–17690.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 等 [2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski,
    Piotr Nyczyk 等。2024。思维图谱：使用大型语言模型解决复杂问题。发表于 *AAAI 人工智能会议论文集*，第 38 卷。17682–17690。
- en: Borgeaud et al. [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by
    retrieving from trillions of tokens. In *International conference on machine learning*.
    PMLR, 2206–2240.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud 等 [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark 等。2022。通过从数万亿个标记中检索来改进语言模型。发表于 *国际机器学习会议*。PMLR,
    2206–2240。
- en: Bruckman and Edwards [1999] Amy Bruckman and Elizabeth Edwards. 1999. Should
    we leverage natural-language knowledge? An analysis of user errors in a natural-language-style
    programming language. In *Proceedings of the SIGCHI conference on Human Factors
    in Computing Systems*. 207–214.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruckman 和 Edwards [1999] Amy Bruckman 和 Elizabeth Edwards。1999。我们应该利用自然语言知识吗？对自然语言风格编程语言中的用户错误的分析。发表于
    *SIGCHI 人机因素会议论文集*。207–214。
- en: 'Chen et al. [2023b] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    2023b. Program of Thoughts Prompting: Disentangling Computation from Reasoning
    for Numerical Reasoning Tasks. *Transactions on Machine Learning Research* (2023).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023b] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    2023b. 思维提示的程序：为数值推理任务解开计算与推理的纠缠。*机器学习研究交易* (2023)。
- en: Chen et al. [2023a] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023a. Teaching large language models to self-debug. *arXiv preprint arXiv:2304.05128*
    (2023).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023a] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023a. 教授大型语言模型自我调试。*arXiv 预印本 arXiv:2304.05128* (2023)。
- en: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,
    et al. 2024. Scaling instruction-finetuned language models. *Journal of Machine
    Learning Research* 25, 70 (2024), 1–53.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,
    等. 2024. 扩展指令微调语言模型。*机器学习研究期刊* 25, 70 (2024), 1–53。
- en: Dahl et al. [1972] Ole-Johan Dahl, Edsger Wybe Dijkstra, and Charles Antony Richard
    Hoare. 1972. *Structured programming*. Academic Press Ltd.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dahl et al. [1972] Ole-Johan Dahl, Edsger Wybe Dijkstra, and Charles Antony
    Richard Hoare. 1972. *结构化编程*。Academic Press Ltd.
- en: Desai et al. [2016] Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain,
    Amey Karkare, Mark Marron, and Subhajit Roy. 2016. Program synthesis using natural
    language. In *Proceedings of the 38th International Conference on Software Engineering*.
    345–356.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Desai et al. [2016] Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain,
    Amey Karkare, Mark Marron, and Subhajit Roy. 2016. 使用自然语言进行程序合成。在 *第38届国际软件工程会议论文集*。345–356。
- en: Ding et al. [2023] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. 2023.
    Task and motion planning with large language models for object rearrangement.
    In *2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.
    IEEE, 2086–2092.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. [2023] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. 2023.
    利用大型语言模型进行任务和运动规划以实现物体重新排列。在 *2023 IEEE/RSJ 国际智能机器人与系统大会 (IROS)*。IEEE, 2086–2092。
- en: 'Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. *arXiv
    preprint arXiv:2303.03378* (2023).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,
    等. 2023. Palm-e：一种具身的多模态语言模型。*arXiv 预印本 arXiv:2303.03378* (2023)。
- en: 'Ernst [2017] Michael D Ernst. 2017. Natural language is a programming language:
    Applying natural language processing to software development. In *2nd Summit on
    Advances in Programming Languages (SNAPL 2017)*. Schloss-Dagstuhl-Leibniz Zentrum
    für Informatik.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ernst [2017] Michael D Ernst. 2017. 自然语言是一种编程语言：将自然语言处理应用于软件开发。在 *第二届编程语言进展峰会
    (SNAPL 2017)*。Schloss-Dagstuhl-Leibniz 计算机科学中心。
- en: 'Ge et al. [2023a] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, and Yongfeng Zhang. 2023a. OpenAGI: When LLM Meets Domain
    Experts. *In Advances in Neural Information Processing Systems (NeurIPS)* (2023).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2023a] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, and Yongfeng Zhang. 2023a. OpenAGI：当大型语言模型遇上领域专家。在 *神经信息处理系统进展
    (NeurIPS)* (2023)。
- en: 'Ge et al. [2023b] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan,
    and Yongfeng Zhang. 2023b. LLM as OS, Agents as Apps: Envisioning AIOS, Agents
    and the AIOS-Agent Ecosystem. *arXiv e-prints* (2023), arXiv–2312.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2023b] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan,
    and Yongfeng Zhang. 2023b. 大型语言模型作为操作系统，代理作为应用：展望AIOS、代理和AIOS-代理生态系统。*arXiv 电子预印本*
    (2023), arXiv–2312。
- en: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning
    with world model. *arXiv preprint arXiv:2305.14992* (2023).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. 2023. 使用语言模型进行推理即使用世界模型进行规划。*arXiv 预印本 arXiv:2305.14992*
    (2023)。
- en: 'Heidorn [1976] George E Heidorn. 1976. Automatic programming through natural
    language dialogue: A survey. *IBM Journal of research and development* 20, 4 (1976),
    302–313.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidorn [1976] George E Heidorn. 1976. 通过自然语言对话进行自动编程：一项综述。*IBM 研究与开发杂志* 20,
    4 (1976), 302–313。
- en: 'Hennessy and Patterson [2011] John L Hennessy and David A Patterson. 2011.
    *Computer architecture: a quantitative approach*. Elsevier.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hennessy and Patterson [2011] John L Hennessy 和 David A Patterson. 2011. *计算机架构：一种定量方法*。Elsevier。
- en: 'Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. CLIPScore: 一种无参考图像字幕评价指标。'
- en: 'Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    2022. Inner monologue: Embodied reasoning through planning with language models.
    *arXiv preprint arXiv:2207.05608* (2022).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et
    al. 2022. 内在独白：通过语言模型进行规划的具身推理。*arXiv preprint arXiv:2207.05608* (2022)。
- en: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. *arXiv preprint
    arXiv:2401.04088* (2024).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. 2024. 专家混合。*arXiv preprint arXiv:2401.04088*
    (2024)。
- en: 'Jojic et al. [2023] Ana Jojic, Zhen Wang, and Nebojsa Jojic. 2023. Gpt is becoming
    a turing machine: Here are some ways to program it. *arXiv preprint arXiv:2303.14310*
    (2023).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jojic et al. [2023] Ana Jojic, Zhen Wang, and Nebojsa Jojic. 2023. GPT正成为图灵机：以下是一些编程方法。*arXiv
    preprint arXiv:2303.14310* (2023)。
- en: 'Josifoski et al. [2023] Martin Josifoski, Lars Klein, Maxime Peyrard, Yifei
    Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, and
    Robert West. 2023. Flows: Building blocks of reasoning and collaborating ai. *arXiv
    preprint arXiv:2308.01285* (2023).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Josifoski et al. [2023] Martin Josifoski, Lars Klein, Maxime Peyrard, Yifei
    Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, and
    Robert West. 2023. Flows: 推理和协作AI的构建模块。*arXiv preprint arXiv:2308.01285* (2023)。'
- en: 'Khattab et al. [2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-Search-Predict:
    Composing Retrieval and Language Models for Knowledge-Intensive NLP. *arXiv preprint
    arXiv:2212.14024* (2022).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khattab et al. [2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-Search-Predict:
    组合检索和语言模型以处理知识密集型NLP。*arXiv preprint arXiv:2212.14024* (2022)。'
- en: 'Khattab et al. [2023] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan
    Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T.
    Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023.
    DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.
    *arXiv preprint arXiv:2310.03714* (2023).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khattab et al. [2023] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan
    Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas
    T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts.
    2023. DSPy: 将声明式语言模型调用编译为自我改进的管道。*arXiv preprint arXiv:2310.03714* (2023)。'
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems* 35 (2022), 22199–22213.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. 大型语言模型是零-shot推理器。*Advances in Neural Information
    Processing Systems* 35 (2022), 22199–22213。
- en: Lewis et al. [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems* 33 (2020), 9459–9474.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. 用于知识密集型NLP任务的检索增强生成。*Advances in Neural Information
    Processing Systems* 33 (2020), 9459–9474。
- en: 'Li and Hovy [2015] Jiwei Li and Eduard Hovy. 2015. The NLP engine: A universal
    turing machine for nlp. *arXiv preprint arXiv:1503.00168* (2015).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Hovy [2015] Jiwei Li and Eduard Hovy. 2015. NLP引擎：NLP的通用图灵机。*arXiv preprint
    arXiv:1503.00168* (2015)。
- en: 'Li et al. [2024] Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and Yongfeng Zhang.
    2024. Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents. *arXiv:2402.00798* (2024).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2024] Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and Yongfeng Zhang.
    2024. Formal-LLM: 将形式语言与自然语言整合用于可控的LLM基础代理。*arXiv:2402.00798* (2024)。'
- en: 'Liang et al. [2023] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia,
    Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. 2023. Taskmatrix. ai:
    Completing tasks by connecting foundation models with millions of apis. *arXiv
    preprint arXiv:2303.16434* (2023).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang et al. [2023] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia,
    Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, 等。2023。Taskmatrix.ai: 通过将基础模型与数百万个
    API 连接来完成任务。*arXiv 预印本 arXiv:2303.16434*（2023年）。'
- en: 'Liu et al. [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. 2023. Llm+ p: Empowering large language models
    with optimal planning proficiency. *arXiv preprint arXiv:2304.11477* (2023).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, 和 Peter Stone。2023。Llm+ p: 赋予大型语言模型优化规划能力。*arXiv 预印本 arXiv:2304.11477*（2023年）。'
- en: Lyu et al. [2023] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-of-thought
    reasoning. *arXiv preprint arXiv:2301.13379* (2023).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu et al. [2023] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, 和 Chris Callison-Burch。2023。忠实的连锁推理。*arXiv 预印本
    arXiv:2301.13379*（2023年）。
- en: 'Madaan et al. [2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in
    Neural Information Processing Systems* 36 (2024).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan et al. [2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    等。2024。Self-refine: 自反馈的迭代优化。*神经信息处理系统进展* 36（2024年）。'
- en: 'Mei et al. [2024] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge,
    and Yongfeng Zhang. 2024. AIOS: LLM Agent Operating System. *arXiv* (2024).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mei et al. [2024] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge,
    和 Yongfeng Zhang。2024。AIOS: LLM 代理操作系统。*arXiv*（2024年）。'
- en: 'Mihalcea et al. [2006] Rada Mihalcea, Hugo Liu, and Henry Lieberman. 2006.
    NLP (natural language processing) for NLP (natural language programming). In *Computational
    Linguistics and Intelligent Text Processing: 7th International Conference, CICLing
    2006, Mexico City, Mexico, February 19-25, 2006\. Proceedings 7*. Springer, 319–330.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mihalcea et al. [2006] Rada Mihalcea, Hugo Liu, 和 Henry Lieberman。2006。NLP（自然语言处理）用于NLP（自然语言编程）。在*计算语言学与智能文本处理:
    第七届国际会议，CICLing 2006，墨西哥城，墨西哥，2006年2月19-25日\. 会议录第7卷*。Springer，319–330。'
- en: 'Nijkamp et al. [2022] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp et al. [2022] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, 和 Caiming Xiong。2022。Codegen: 用于多轮程序合成的开源大型语言模型。*arXiv
    预印本 arXiv:2203.13474*（2022年）。'
- en: OpenAI [2023] Josh et al OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] Josh 等 OpenAI。2023。GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems* 35 (2022), 27730–27744.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, 等。2022。训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展* 35（2022年），27730–27744。
- en: 'Paul et al. [2023] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback
    on intermediate representations. *arXiv preprint arXiv:2304.01904* (2023).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paul et al. [2023] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, 和 Boi Faltings。2023。Refiner: 中间表示的推理反馈。*arXiv 预印本
    arXiv:2304.01904*（2023年）。'
- en: 'Poesia et al. [2022] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari,
    Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable
    code generation from pre-trained language models. *arXiv preprint arXiv:2201.11227*
    (2022).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Poesia et al. [2022] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari,
    Gustavo Soares, Christopher Meek, 和 Sumit Gulwani。2022。Synchromesh: 从预训练语言模型中可靠地生成代码。*arXiv
    预印本 arXiv:2201.11227*（2022年）。'
- en: Prather [1997] Ronald E Prather. 1997. Regular expressions for program computations.
    *The American mathematical monthly* 104, 2 (1997), 120–130.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prather [1997] Ronald E Prather。1997。程序计算中的正则表达式。*美国数学月刊* 104, 2（1997年），120–130。
- en: 'Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789* (2023).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等人 [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
    Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian 等. 2023. Toolllm: 使大型语言模型掌握16000+现实世界API。*arXiv
    预印本 arXiv:2307.16789* (2023)。'
- en: 'Ross et al. [2023] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael
    Muller, and Justin D Weisz. 2023. The programmer’s assistant: Conversational interaction
    with a large language model for software development. In *Proceedings of the 28th
    International Conference on Intelligent User Interfaces*. 491–514.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ross 等人 [2023] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller,
    和 Justin D Weisz. 2023. 程序员助手: 与大型语言模型进行的软件开发对话交互。在 *第28届国际智能用户界面会议论文集* 中。491–514。'
- en: 'Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*
    (2023).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人 [2023] Noah Shinn, Beck Labash, 和 Ashwin Gopinath. 2023. Reflexion:
    一个具有动态记忆和自我反思的自主代理。*arXiv 预印本 arXiv:2303.11366* (2023)。'
- en: 'Singh et al. [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.
    Progprompt: Generating situated robot task plans using large language models.
    In *2023 IEEE International Conference on Robotics and Automation (ICRA)*. IEEE,
    11523–11530.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 等人 [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, 和 Animesh Garg. 2023.
    Progprompt: 使用大型语言模型生成情境化机器人任务计划。在 *2023 IEEE 国际机器人与自动化会议 (ICRA)* 上。IEEE，11523–11530。'
- en: Vadas and Curran [2005] David Vadas and James R Curran. 2005. Programming with
    unrestricted natural language. In *Proceedings of the Australasian Language Technology
    Workshop 2005*. 191–199.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vadas 和 Curran [2005] David Vadas 和 James R Curran. 2005. 使用不受限制的自然语言编程。在 *2005年澳大利亚语言技术研讨会论文集*
    中。191–199。
- en: Wang et al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*
    (2022).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
    Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2022. 自我一致性改善语言模型中的思维链推理。*arXiv 预印本
    arXiv:2203.11171* (2022)。
- en: Wang et al. [2023] Yubo Wang, Xueguang Ma, and Wenhu Chen. 2023. Augmenting
    black-box llms with medical textbooks for clinical question answering. *arXiv
    preprint arXiv:2309.02233* (2023).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023] Yubo Wang, Xueguang Ma, 和 Wenhu Chen. 2023. 使用医学教科书增强黑箱LLMs以进行临床问题回答。*arXiv
    预印本 arXiv:2309.02233* (2023)。
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems* 35 (2022), 24824–24837.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou 等. 2022. 思维链提示激发大型语言模型中的推理。*神经信息处理系统进展* 35 (2022)，24824–24837。
- en: 'Wu et al. [2020] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao
    Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter
    Vajda. 2020. Visual Transformers: Token-based Image Representation and Processing
    for Computer Vision. arXiv:2006.03677 [cs.CV]'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 [2020] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang,
    Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, 和 Peter Vajda.
    2020. 视觉变换器: 基于标记的图像表示和计算机视觉处理。arXiv:2006.03677 [cs.CV]'
- en: 'Wu et al. [2024] Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, and Qingyun
    Wu. 2024. StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows.
    *arXiv preprint arXiv:2403.11322* (2024).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 [2024] Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, 和 Qingyun Wu.
    2024. StateFlow: 通过状态驱动工作流提升LLM任务解决能力。*arXiv 预印本 arXiv:2403.11322* (2024)。'
- en: 'Yao et al. [2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*
    36 (2024).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 2024. 思维树: 使用大型语言模型进行深思熟虑的问题解决。*神经信息处理系统进展* 36
    (2024)。'
- en: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629* (2022).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan, 和 Yuan Cao. 2022. React: 在语言模型中协同推理和行动。*arXiv 预印本 arXiv:2210.03629*
    (2022)。'
- en: 'Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人 [2023] Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik Narasimhan
    和 Yuan Cao。2023年。ReAct：在语言模型中协同推理与行动。发表于 *国际学习表征会议（ICLR）*。
- en: 'Zhang et al. [2020] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人 [2020] Tianyi Zhang、Varsha Kishore、Felix Wu、Kilian Q. Weinberger 和 Yoav
    Artzi。2020年。BERTScore：使用BERT评估文本生成。
