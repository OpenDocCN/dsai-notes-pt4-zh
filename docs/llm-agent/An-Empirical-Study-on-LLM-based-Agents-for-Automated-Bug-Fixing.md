<!--yml
category: 未分类
date: 2025-01-11 11:55:39
-->

# An Empirical Study on LLM-based Agents for Automated Bug Fixing

> 来源：[https://arxiv.org/html/2411.10213/](https://arxiv.org/html/2411.10213/)

Xiangxin Meng [mengxiangxin.1219@bytedance.com](mailto:mengxiangxin.1219@bytedance.com) BytedanceBeijingChina ,  Zexiong Ma [mazexiong@bytedance.com](mailto:mazexiong@bytedance.com) Bytedance & Peking UniversityBeijingChina ,  Pengfei Gao [gaopengfei.se@bytedance.com](mailto:gaopengfei.se@bytedance.com) BytedanceBeijingChina  and  Chao Peng [pengchao.x@bytedance.com](mailto:pengchao.x@bytedance.com) BytedanceBeijingChina

###### Abstract.

Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent and non-agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine seven proprietary and open-source systems on the SWE-bench Lite benchmark for automated bug fixing. We first assess each system’s overall performance, noting instances solvable by all or none of these sytems, and explore why some instances are uniquely solved by specific system types. We also compare fault localization accuracy at file and line levels and evaluate bug reproduction capabilities, identifying instances solvable only through dynamic reproduction. Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.

## 1\. Introduction

Large Language Models (LLMs) (Zhao et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib61)) are advanced machine learning models trained on vast amounts of textual data, capable of understanding and generating human-like text. LLM-based Agents (Xi et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib49)) are systems that utilize large language models to interact with the environment and accomplish specific tasks. Recently, LLM-based Agents have demonstrated significant influence in automated bug fixing in code repositories (Kang et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib15); Zhang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib60); Yang et al., [2024a](https://arxiv.org/html/2411.10213v1#bib.bib53)). Thanks to the powerful natural language processing capabilities of LLMs, these Agents can efficiently understand and analyze source code and its associated natural language descriptions, such as user-submitted issue descriptions and code comments. Additionally, through dynamic interaction with local environments (e.g., via terminal), LLM-based Agents can retrieve useful information from the code repository, perform code editing and execution, and iterate and validate repair results, thereby improving the accuracy and efficiency of bug fixes. This combination of LLM and environmental feedback has made automated bug fixing more efficient and feasible than ever before, providing revolutionary new tools for software maintenance and development.

Researchers from both the industry (Liu et al., [2024a](https://arxiv.org/html/2411.10213v1#bib.bib27); hon, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib2); gru, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib3); Ma et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib29)) and academia (Zhang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib60); Tao et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib40)) have developed LLM-based Agent systems to locate and fix bugs in code repositories. To evaluate the fault localization and repair capabilities of LLMs and various Agent systems, Jimenez et.al (Jimenez et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib14)) proposed the evaluation datasets SWE-bench, with derived versions SWE-bench Lite (a subset of the full benchmark), and SWE-bench Verified (human annotated subset of SWE-bench published recently). These datasets contain real bugs from code repositories and can verify the correctness of the patches generated by Agents through unit tests. Recently, these datasets have become the most influential benchmarks in the field of automated bug fixing, attracting both academic and industrial participants to compete on the SWE-bench Lite leaderboard ¹¹1[https://www.swebench.com/](https://www.swebench.com/), with new submissions typically every one or a half week.

However, no work has systematically analyzed the fault localization and repair capabilities of LLM-based Agents or the performance differences among various tools within these Agent systems. Regarding the SWE-bench Lite dataset itself, due to the quality of issue descriptions and the complexity of the logical dependencies related to the defects, some instances in the benchmark are easier for Agents to fix, while others are more difficult (Xia et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib50)). As for the design of the systems, different designs exhibit different planning, reasoning, and problem solving capabilities, i.e. some systems adopting static approaches (gru, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib3)) and others adopting dynamic approaches (Zhang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib60)). We have also observed significant differences in the sets of cases that each system can solve. Therefore, analyzing the solving capabilities of LLM-based Agents on specific instances can not only help us better understand the current performance of Agents but also provide comparative insights to inspire future research directions.

We collected the four most outstanding commercial systems (i.e. MarsCode Agent (Liu et al., [2024a](https://arxiv.org/html/2411.10213v1#bib.bib27)), Honeycomb (hon, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib2)), Gru (gru, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib3)), Alibaba Lingma Agent (Ma et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib29))) and the three most excellent open-source systems (i.e. AutoCodeRover (Zhang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib60)), Agentless + RepoGraph (Ouyang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib36)), Agentless (Xia et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib50))) with top performances on the SWE-bench Lite Leaderboard and conducted a comprehensive analysis of the performance differences of each system. First, we evaluated the overall performance of LLM-based Agents in bug fixing, including statistics on the instances that all seven systems can solve and those that none can solve, and analyzed the reasons behind these results. We also explored why some instances can only be solved by Agent systems while others can only be solved by non-Agent systems. Next, we investigated the performance differences in fault localization among different systems and their causes, compiling file-level and line-level localization accuracy rates. Finally, we analyzed the impact of bug reproduction on bug fixing, and the common characteristics of instances that can only be solved through dynamic reproduction.

Through data analysis, we have summarized several insights.To improve bug fixing, it is essential to enhance the model’s reasoning ability, enabling it to accurately identify bug-related information within an issue and reduce noise interference. For multiple potential repair locations, the model should leverage its reasoning capabilities to determine the most relevant location. From the Agentic flow perspective, there should be a strong focus on the quality of the issue and attention to multiple suspicious locations in the stack trace. The design should include mechanisms to verify the completeness of patches and consider their global impact. Mechanisms should also be implemented to mitigate the randomness of the model’s output or effectively utilize its diversity. In error localization, achieving line-level accuracy is more critical than file-level, due to the larger discovery space, necessitating finer-grained results. During the reproduction process, ensuring the correctness of reproductions is crucial, as incorrect reproductions can result in the failure of the entire solving process.

Novelty and Contribution To the best of knowledge, this is the first work to:

1.  (1)

    Study the effectiveness of LLM-based Agents in automatic bug fixing for code repositories

2.  (2)

    Examine the effectiveness of different LLM-based Agents in Fault Localization and analyze the reasons for their differences

3.  (3)

    Investigate the impact of bug reproduction on bug fixing of LLM-based Agents

4.  (4)

    Summarize the current issues and future research directions for LLM-based Agents in bug fixing

Paper Organization The remainder of this paper is organized as follows: Section [2](https://arxiv.org/html/2411.10213v1#S2 "2\. Background ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") explains the background. Section [3](https://arxiv.org/html/2411.10213v1#S3 "3\. Study Design ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") describes the study design. Section [4](https://arxiv.org/html/2411.10213v1#S4 "4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") presents the analysis results and findings. Section [5](https://arxiv.org/html/2411.10213v1#S5 "5\. Discussion ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") discusses the analysis results and findings. Section [6](https://arxiv.org/html/2411.10213v1#S6 "6\. Threats to Validity ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") reports the threats to validity. Section [7](https://arxiv.org/html/2411.10213v1#S7 "7\. Related Work ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") discusses related work, and Section [8](https://arxiv.org/html/2411.10213v1#S8 "8\. Conclusion ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") concludes the paper.

## 2\. Background

In this section, we first introduce SWE-bench Lite and then we introduce the leading LLM-based bug fixing systems.

### 2.1\. SWE-bench Lite

SWE-Bench (Jimenez et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib14)) is a comprehensive benchmark designed to evaluate LLMs on complex real-world software engineering tasks sourced from GitHub issues and corresponding pull requests across 12 popular Python repositories. This benchmark addresses the limitations of existing coding benchmarks such as HumanEval (Chen et al., [2021](https://arxiv.org/html/2411.10213v1#bib.bib7)) by presenting tasks that require models to understand and coordinate changes across large codebases involving multiple functions and files. The benchmark includes 2,294 task instances and emphasizes the need for models to interact with execution environments and handle long contexts, showcasing the challenges that real-world software engineering problems pose to current LLMs. Their evaluations reveal that even the best-performing models at the time of publication, such as Claude 2, achieve a success rate of only 1.96%, highlighting significant room for improvement.

As the computational demands and high difficulty of SWE-bench which comprises 2,294 issue-commit pairs across 12 Python repositories, the authors of SWE-bench introduces SWE-bench Lite ²²2[https://www.swebench.com/lite.html](https://www.swebench.com/lite.html), which includes 300 more manageable and self-contained instances focused on functional bug fixes, covering 11 of the original 12 repositories. It retains the diversity of SWE-bench but is easier to evaluate. The selection criteria is shown below:

1.  (1)

    Removed instances with images, external links, commit SHAs, or references.

2.  (2)

    Excluded problem statements under 40 words.

3.  (3)

    Excluded instances editing more than one file or with more than three edit hunks.

4.  (4)

    Excluded instances creating/removing files or with error message checks.

5.  (5)

    Sampled final 300 test and 23 development instances from remaining ones.

### 2.2\. Leading LLM-based Bug Fixing Systems

LLM-based Bug Fixing Systems are systems built on Large Language Models (LLMs) that can automatically edit code repositories to fix bugs based on issue reports. Bug fixing is a highly resource-intensive task in software development, requiring developers to reproduce the bugs reported in issue reports, precisely locate defective code snippets within large code repositories, understand the cause of errors, and implement fixes. Automating bug fixing has long attracted widespread attention in both academia and industry. Given the strong logical reasoning and coding capabilities demonstrated by LLMs, numerous works have explored the development of automated bug fixing tools based on LLMs. In this paper, we study seven leading LLM-based Bug Fixing Systems (four commercial systems (Liu et al., [2024a](https://arxiv.org/html/2411.10213v1#bib.bib27); hon, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib2); gru, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib3); Ma et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib29)) and three open source systems (Zhang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib60); Xia et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib50); Ouyang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib36))), comparing their differences in system design and performance in automated bug fixing, analyzing the shortcomings and limitations of existing systems, and providing direction for future work in building adaptive, high-reliability automated bug fixing systems.

Table  [1](https://arxiv.org/html/2411.10213v1#S2.T1 "Table 1 ‣ 2.2\. Leading LLM-based Bug Fixing Systems ‣ 2\. Background ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") shows the leading LLM-based bug fixing systems on SWE-bench Lite. We classified the fault localization techniques adopted by different systems from three perspectives: (1) the utilization of LLMs, (2) the utilization of CodeGraph, and (3) the utilization of software analysis techniques. We also compared whether different systems use reproduction for patch validation and analyzed the patch generation format. Blow this section, we will introduce these systems from a technical perspective:

MarsCode Agent (Liu et al., [2024a](https://arxiv.org/html/2411.10213v1#bib.bib27)),developed by ByteDance, is a bug fixing system that combines code knowledge graphs, software analysis techniques, and LLMs. It uses a Reproducer Agent to automatically reproduce the bugs described in the issue reports. The system constructs a code knowledge graph for the code repository and utilizes graph reasoning, software analysis techniques, and the reasoning capabilities of LLMs to achieve fine-grained defect localization. Marscode Agent generates candidate patches using LLMs and selects the final patch from these candidates based on the reproduction scripts. It has demonstrated outstanding performance in bug fixing tasks, achieving the highest issue resolution rate on the SWE-bench Lite leaderboard.

Honeycomb (hon, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib2)), an agent-based bug fixing system developed by Honeycomb. It provides tools for file viewing and editing to the LLM, enabling it to attempt to construct reproduction scripts for all issues. The system calls these tools for defect localization and generates patches in the format of $(Line_{b},Line_{e},Replace)$. The tool design in Honeycomb is relatively simple, streamlining the decision-making process for LLM tool usage, but it may result in the system being unable to obtain more accurate localization information. Honeycomb successfully resolved 38.33% of the issues in the SWE-bench Lite dataset.

Gru (gru, [[n. d.]](https://arxiv.org/html/2411.10213v1#bib.bib3)), a workflow-based bug fixing system developed by Gru. It first uses an LLM to select files related to the issue, then the LLM makes decisions on which files to change and how to change them. It generates patches in the format of $(Line_{b},Line_{e},Replace)$, and after the LLM reviews the patches, they are applied to the repository. Gru does not include a reproduction module and successfully resolved 35.67% of the issues in the SWE-bench Lite dataset.

Alibaba Lingma Agent (Ma et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib29)), a bug fixing system developed by Alibaba that combines code knowledge graphs with LLMs. It constructs a code knowledge graph for the code repository and utilizes LLMs to perform Monte Carlo Tree Search based on issue information to locate code snippets related to the issue throughout the entire repository. This approach effectively alleviates the issue of short context supported by LLMs but places high demands on the reasoning capabilities of the LLMs. It generates patches in Search/Replace format and, for some issues, also validates patches using reproduction scripts. Alibaba Lingma Agent successfully resolved 33.00% of the issues in the SWE-bench Lite dataset.

AutoCodeRover (Zhang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib60)), a bug fixing system that combines LLMs with software analysis techniques. It uses Spectrum-based Fault Localization (SBFL) for defect localization, providing functions potentially related to the issue to the LLM, which then generates patches for bug fixing. AutoCodeRover’s localization process offers greater interpretability; however, it requires executing failed unit tests, and its localization effectiveness depends on the quality of the test cases. In practical development, it may struggle to locate defects accurately if tests are missing. AutoCodeRover successfully resolved 30.67% of the issues in the SWE-bench Lite dataset.

Agentless (Xia et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib50)), a two-stage, workflow-based bug fixing system. It directly uses an LLM to sequentially perform file, function, and line localization. First, it lists the repository’s file structure, allowing the LLM to select the files most relevant to the issue based on the project’s structure. Then, it lists the file framework (including class names, function declarations, etc.) and uses the LLM to identify the functions most relevant to the issue. The model then locates the specific lines that need modification based on the function content. Once the required lines are identified, Agentless provides the context of these lines as input to the model, which generates multiple candidate patches in Search/Replace format. The final patch is selected using a majority voting strategy. Agentless achieves automated bug fixing using a straightforward workflow approach; however, due to the lack of an autonomous decision-making process and missing information in the localization phase, its localization performance is limited. Agentless successfully resolved 27.33% of the issues in the SWE-bench Lite dataset.

Agentless + RepoGraph (Ouyang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib36)), a bug fixing system that combines code knowledge graphs with the Agentless approach. It constructs a code knowledge graph for the code repository and provides a graph-based retrieval interface for the LLM, using GRAG (Hu et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib12)) to retrieve classes/functions related to the issue from the graph. Then, Agentless generates patches based on the localized code snippets, addressing the information loss issue in Agentless’s localization phase. However, it performs poorly on issues that are brief and lack code identifiers, resulting in a gap compared to commercial agent systems. RepoGraph + Agentless successfully resolved 29.67% of the issues in the SWE-bench Lite dataset.

As shown in Table [1](https://arxiv.org/html/2411.10213v1#S2.T1 "Table 1 ‣ 2.2\. Leading LLM-based Bug Fixing Systems ‣ 2\. Background ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing"), Marscode Agent, which combines three localization strategies and utilizes reproduction for patch validation, achieved the state-of-the-art bug fixing performance. In this paper, we will analyze the localization accuracy of different localization strategies and the impact of reproduction on the final patch generation accuracy, providing guidance for building more reliable bug fixing systems in future work.

Table 1. Leading LLM-based Bug Fixing Systems on SWE-bench Lite.

 | System Name | Type | %Resolved | Fault Localization | Reproduction | Patch Generation Format |
| Using LLMs | CodeGraph | Software Analysis |
| MarsCode Agent | Commercial | 39.33 | ✓ | ✓ | ✓ | ✓ | Search/Replace |
| Honeycomb | 38.33 | ✓ | $\times$ | $\times$ | ✓ | ($Line_{b}$, $Line_{e}$, Replace) |
| Gru | 35.67 | ✓ | $\times$ | $\times$ | $\times$ | ($Line_{b}$, $Line_{e}$, Replace) |
| Alibaba Lingma Agent | 33.00 | ✓ | ✓ | $\times$ | ✓ | Search/Replace |
| AutoCodeRover | Open Source | 30.67 | ✓ | $\times$ | ✓ | ✓ | Search/Replace |
| Agentless + RepoGraph | 29.67 | ✓ | ✓ | $\times$ | $\times$ | Search/Replace |
| Agentless | 27.33 | ✓ | $\times$ | $\times$ | $\times$ | Search/Replace | 

## 3\. Study Design

In this section, we first introduce the research questions and then introduce the data collection.

### 3.1\. Research Questions

RQ1\. Effectiveness of Systems: How does the LLM-based Agent currently perform in automatic bug fixing in code repositories?

Motivation: In the SWE-bench Lite leaderboard, the solution rates of various systems vary significantly, and there are substantial differences in the instances that each system can and cannot solve. This discrepancy is usually due to the quality of issue description and the design of the systems themselves. When an issue description is of sufficiently high quality, we expect an LLM-based Agent to be able to resolve it. Therefore, it is necessary to analyze why certain instances with high-quality issue description are not successfully fixed by the Agent, while some instances with low-quality issue description are resolved. Additionally, there are significant differences in the implementation of Agent-based and non-Agent systems, and it is worth investigating the differences in their resolution capabilities.

Approach: We will analyze the differences in the instances solved by various systems, showing how many instances are resolved by all systems and how many instances are not solved by any system. Then, based on the criteria proposed by Agentless[] for evaluating issue descriptions, we will score the quality of the issue descriptions for each instance, where higher scores indicate higher-quality issue descriptions. Subsequently, we will investigate why many high-scoring issues cannot be resolved by any system, while some low-scoring issues can be resolved by all tools. Additionally, we will examine the characteristics of instances that can be resolved by all Agent systems but not by non-Agent systems, as well as those instances that can be resolved by all non-Agent systems but not by Agent systems.

RQ2\. Effectiveness of FL: How do different systems perform in Fault Localization and what are the reasons for their differences?

Motivation: Fault localization is a crucial step in bug fixing, as the more accurately the fault is localized, the higher the probability of successfully fixing the bug. Therefore, we need to investigate the differences in fault localization effectiveness among different systems.

Approach: Based on the ground truth, we will compile statistics on the proportion of successfully localized faulty files and the proportion of successfully localized faulty lines for each system in each SWE-bench Lite instance.

RQ3\. Effectiveness of Reproduction: How Bug Reproduction in Different Systems Affects Bug Fixing Performance?

Motivation: Bug reproduction is an important step in bug fixing and an essential part of dynamic debugging. Its role is reflected in two aspects. First, the error messages from running the bug reproduction script can be used for fault localization. Second, the bug reproduction script can be used to validate the final generated patch. The higher the quality of the bug reproduction script, the more accurate information it can provide to the Agent, increasing the probability of successfully fixing the bug. Therefore, we need to investigate the impact of bug reproduction on bug fixing.

Approach: We will compile statistics on the adoption rate of the reproduction scripts generated by each system, providing a comparison of the impact of bug reproduction on bug fixing. Additionally, we will analyze the cases that can only be solved with the involvement of reproduction scripts, ans the cases that bug reproduction negatively impact bug fixing.

### 3.2\. Data Collection

In RQ1, we design a scoring system based on the five metrics and corresponding candidate values provided by Agentless, allowing us to evaluate the quality of different issue sets across multiple dimensions. In RQ2, we conduct a reverse analysis of the patches generated by different tools, thereby offering an unbiased evaluation of each tool’s performance in fault localization.In RQ3, to determine the use of reproduction by different systems from their trajectories, we first identify Agentless, RepoGraph+Agentless, and Gru as systems that do not support reproduction, based on keyword matching for "reproduce" and manual analysis. Then, for the remaining four systems, we utilize different heuristic rules to identify the construction of reproduction scripts.

## 4\. Analysis & Results

We will sequentially present the analysis results and insights for RQ1 to RQ3.

### 4.1\. RQ1: Effectiveness of Systems

We analyzed the versions of cases that each of the seven tools can solve individually, as well as the differences between the cases that each tool can resolve, as shown in Figure [1](https://arxiv.org/html/2411.10213v1#S4.F1 "Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing")-(a). The histogram at the top of the figure shows the number of cases each tool can resolve in SWE-bench Lite. Specifically, ranked from lowest to highest, these are Agentless, Agentless+RepoGraph, AutoCodeRover, Alibaba Lingma Agent, Gru, Honeycomb, and MarsCode Agent, resolving 82, 89, 92, 99, 107, 115, and 118 cases, respectively. MarsCode Agent performs the best, achieving a 43.9% performance improvement over Agentless and addressing 39.3% of the total 300 cases in SWE-bench Lite. Compared to the popular APR benchmark Defects4J over the past decade, SWE-bench Lite introduces stricter usage protocols, prohibiting participants from leveraging dynamic evaluation results generated by closely related failing test cases as feedback information for filtering patches. This test case set can only be utilized as a quality standard once the patch generation process has concluded. In this context, many error localization methods based on dynamic test execution information—such as spectrum-based and mutation-based error localization methods—cannot be used, adding further complexity to problem detection and resolution. This strict protocol undoubtedly aligns more closely with real-world development scenarios, where repair tools must rely almost solely on issues raised by users and the current state of the code repository to devise solutions. Against this backdrop, MarsCode’s ability to address 39.3% of cases underscores its advanced capabilities and utility in real-world development environments.

![Refer to caption](img/cd9ed158bc19b8cb7e46e25f337a0e4d.png)

(a) Issue Resolve Task

![Refer to caption](img/fd304e6eea2ee2c228e36e4d77577a5d.png)

(b) File-level FL Task

![Refer to caption](img/beeb80a091dafa0466630e60ab9467d5.png)

(c) Line-level FL Task

Figure 1. Analysis of Performance of State-of-the-art Techniques on Issue Resolve and Fault Localization Tasks, where FL indicates Fault Localization.

Table 2. Issue Quality Metrics (REs indicates Reproducible Examples, and NL indicates Natutal Language).

| Metrics | Descriptions of Metrics | Candidates | Scores |
| --- | --- | --- | --- |
| Quality of Reproducible Examples | This Metric indicates whether each issue description contains sufficient information to perform the desired task. | Contains REs | 10.00 |
|  |  | Contains Partial REs | 6.67 |
|  |  | Info in NL | 3.33 |
|  |  | Not Enough Info | 0.00 |
| Quality of Resolve Solutions | This metric indicates whether the solutions or steps to solve the problem are already provided in the issue description. | Exact Patch | 10.00 |
|  |  | Complete Steps in NL | 7.50 |
|  |  | Some Steps in NL | 5.00 |
|  |  | No Solution | 2.50 |
|  |  | Misleading Information | 0.00 |
| Quality of Bug Locations (File/Function/Line) | This metric indicates whether the issue description contains the correct bug location information. | Stacktrace | 10.00 |
|  |  | Keyword | 6.67 |
|  |  | Natural Language | 3.33 |
|  |  | None Information | 0.00 |

In Figure [1](https://arxiv.org/html/2411.10213v1#S4.F1 "Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing")-(a), the histogram beneath the tool names (referred to as Part-I) presents the case versions that different tool combinations can address. Each row indicates the number of case versions solvable by the tools marked with black dots but not by those marked with gray dots. For instance, in the first row, only MarsCode is marked with a black dot, while the other six tools are marked with gray dots, indicating that MarsCode can solve a unique set of 9 cases that none of the other six tools can handle. Similarly, the seventh row demonstrates that MarsCode and Honeycomb together can solve 5 cases that the other five tools cannot address. The final row shows that all seven tools collectively can solve 36 cases. The following sections will analyze the statistical findings presented in this figure from several perspectives.

Analysis of Case Solvability. Among the 300 cases in SWE-bench Lite, 168 cases are solvable by at least one of the seven tools (representing the sum of all values in Part-II of Figure [1](https://arxiv.org/html/2411.10213v1#S4.F1 "Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing")-(a)), while 132 cases remain unsolved by any tool (not displayed in the figure and constituting the complement of the 168 solvable cases). Furthermore, 36 cases can be solved by all seven tools (represented by the last row in Part-II). We hypothesize that the issue descriptions for these 36 universally solvable cases are generally of higher quality, whereas the 132 cases that none of the tools can resolve likely exhibit lower-quality issue descriptions. To validate this hypothesis, we conducted a significance analysis of issue quality differences using five metrics provided by the issue quality analysis report in Agentless, as detailed in Table [2](https://arxiv.org/html/2411.10213v1#S4.T2 "Table 2 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") (with bug location represented by three metrics: file, function, and line). For each metric, Agentless provided candidate values that represent the completeness of information, which we listed in descending order of completeness in the third column. We then assigned scores on a 10-point scale, uniformly distributing scores among candidate values to facilitate further analysis. For example, for the Quality of Reproducible Examples metric, we assigned a score of 10 points to "Contain REs" (highest information completeness), 6.67 points to "Contain Partial REs" (moderate completeness), 3.33 points to "Info in NL", and 0 points to "Not enough Info". While the number of candidates varied across metrics, we ensured that all metric scores ranged from 0 to 10, allowing comparability across metrics. Notably, for the Quality of Resolve Solutions metric, we positioned the "Misleading" candidate (where the issue description contains misleading information) at the lowest rank, as we consider it more detrimental than "No Solution." It is important to note that the indicators for each case have already been manually analyzed in the open-source artifacts of Agentless. Our approach builds on this by assigning scores to each candidate value to facilitate further analysis of issue quality.

To examine the validity of our hypothesis, we calculated the mean values for the five indicators across two case sets: the set of 132 cases unsolved by any tool (referred to as the "No-one-resolve Set") and the set of 36 cases solvable by all tools (referred to as the "All-resolve Set"). These results are shown in Table [3](https://arxiv.org/html/2411.10213v1#S4.T3 "Table 3 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing"). For each indicator, a higher score indicates that the corresponding issue description provides more complete and detailed information. It is evident that the scores for all five indicators are consistently higher in the All-resolve Set compared to the No-one-resolve Set. This suggests that issue quality significantly influences the effectiveness of resolution methods, underscoring the importance of crafting clear and comprehensive issue descriptions to improve resolution rates from the outset.

Examining the performance details for each indicator across both case sets, the most notable difference is observed in the Quality of Line-level Location metric, where the average score in the All-resolve Set is 27.8 times that of the No-one-resolve Set. This indicates that, at least within the SWE-bench Lite dataset, providing line-level location information markedly enhances the resolution rate for repair tools. The second largest difference appears in Quality of Solution in Description, with an average score difference of 2.13 times between the two sets, indicating that suggesting positive modifications based on observed symptoms can also substantially aid in improving tool solvability. This is followed by function-level and file-level location information, with score differences of 1.66 times and 1.47 times, respectively, suggesting that as the granularity of the bug location becomes coarser, its benefit to repair tools diminishes—a finding that aligns with intuitive expectations. Lastly, Quality of Reproducible Examples shows only a 1.1-time score difference between the two sets, suggesting that the completeness of this indicator has a comparatively minor impact on enhancing the solvability of repair tools.

In the above analysis, we validated our hypothesis by calculating differences across five metrics that represent issue quality in the All-resolve Set and No-one-resolve Set, showing a clear disparity in issue quality between cases that can be resolved and those that cannot. This reflects an overall trend, though some outliers may exist. On one hand, we observed several high-scoring cases in the No-one-resolve Set; we aim to investigate why, despite relatively complete information in these issues, no tools were able to resolve them. On the other hand, we also identified some low-scoring cases within the All-resolve Set, which may further highlight the capability of repair tools to independently find and gather necessary information. To this end, we collected the top 5 cases with the highest average scores across the five metrics in the No-one-resolve Set, and the 5 cases with the lowest average scores in the All-resolve Set, as shown in Table [3](https://arxiv.org/html/2411.10213v1#S4.T3 "Table 3 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing").

Table 3. Issue Quality Analysis for All-resolve Cases and No-one-resolve Cases.

| Case Sets |  

&#124; Quality of &#124;
&#124; Reproducible &#124;
&#124; Examples &#124;

 |  

&#124; Quality of &#124;
&#124; Resolve &#124;
&#124; Solutions &#124;

 |  

&#124; Quality of &#124;
&#124; File-level &#124;
&#124; Locations &#124;

 |  

&#124; Quality of &#124;
&#124; Func-level &#124;
&#124; Locations &#124;

 |  

&#124; Quality of &#124;
&#124; Line-level &#124;
&#124; Locations &#124;

 |  

&#124; Avg &#124;
&#124; Scores &#124;

 |
| --- | --- | --- | --- | --- | --- | --- |
|  

&#124; No-one-resolve &#124;
&#124; Set (132 cases) &#124;

 | 6.46 | 2.67 | 2.95 | 1.67 | 0.05 | 2.76 |
|  

&#124; All-resolve Set &#124;
&#124; (36 cases) &#124;

 | 7.22 | 5.69 | 4.35 | 2.78 | 1.39 | 4.29 |
|  

&#124; All-resolve Set / &#124;
&#124; No-one-resolve Set &#124;

 | 110% | 213% | 147% | 166% | 2780% | 155% |

Table 4. Scores of 5 top-ranked cases in No-one-resolve Set and 5 bottom-ranked cases in All-resolve Set.

| No-one-resolve Set |
| Case IDs |  

&#124; Scores of &#124;
&#124; Reproducible &#124;
&#124; Examples &#124;

 |  

&#124; Scores of &#124;
&#124; Resolve &#124;
&#124; Solutions &#124;

 |  

&#124; Scores of &#124;
&#124; File-level &#124;
&#124; Locations &#124;

 |  

&#124; Scores of &#124;
&#124; Func-level &#124;
&#124; Locations &#124;

 |  

&#124; Scores of &#124;
&#124; Line-level &#124;
&#124; Locations &#124;

 |  

&#124; Avg &#124;
&#124; Scores &#124;

 |
| scikit-learn-25747 | 10 | 2.5 | 10 | 10 | 0 | 6.5 |
| seaborn-3407 | 10 | 2.5 | 10 | 10 | 0 | 6.5 |
| django-15202 | 6.67 | 5 | 10 | 10 | 0 | 6.33 |
| scikit-learn-10508 | 10 | 5 | 10 | 6.67 | 0 | 6.33 |
| django-13660 | 3.33 | 7.5 | 10 | 10 | 0 | 6.17 |
| All-resolve Set |
| django-12497 | 3.33 | 7.5 | 0 | 0 | 0 | 2.17 |
| django-11133 | 3.33 | 2.5 | 6.67 | 0 | 0 | 2.5 |
| pytest-7432 | 3.33 | 2.5 | 3.33 | 3.33 | 0 | 2.5 |
| pytest-5692 | 3.33 | 2.5 | 6.67 | 0 | 0 | 2.5 |
| django-16046 | 6.67 | 2.5 | 3.33 | 0 | 0 | 2.5 |

For each case, we examined the execution trajectories generated by seven tools to analyze the reasons they could or could not resolve the respective cases. Due to space limitations, we present only the more commonly observed findings across the seven methods:

scikit-learn-25747: Most methods successfully extracted relevant files, functions, and even fine-grained code snippets containing the error line as a set of suspicious elements for fault localization. This is largely because the issue provided a high-quality description with file- and function-level localization, scoring a 10\. However, the main reason for the failure to resolve this case lies in the excessive number of suspicious elements identified by the fault localization module (as each call node in the stack trace tends to be included). The actual error location is deeply embedded within the stack trace, making it challenging to prioritize among the top-ranked elements. This issue tests the model’s ability to infer the relationship between the observed symptoms and the root cause of the error, where the model’s reasoning capabilities remain limited. Additionally, the issue title references a code element associated with the observed error symptom (i.e., the entry point in the stack trace). As the title is a summary of the issue, it naturally draws more attention from the model, further reinforcing the impression that this element caused the error. Meanwhile, in the subsequent repair phase, tools are constrained by the time and cost associated with large-model calls to generate repair suggestions. Resources may be fully exhausted on the first suspicious location, thus limiting opportunities to address subsequent ones.

seaborn-3407: The reason it remains unresolved is nearly identical to that of scikit-learn-25747.

django-15202: The reason it remains unresolved is nearly identical to that of scikit-learn-25747.

scikit-learn-10508: This issue requires consistent modifications at two locations to be resolved. We observed that many tools successfully identified and repaired one location while neglecting the other. In fact, the two required modification points are in close proximity, within the same function and only about 10 lines apart. However, due to current limitations in assessing the completeness of fixes, tools tend to favor generating an ostensibly reasonable solution. Once it passes their designed validation processes—such as using a self-reflective agent to critique the patch’s plausibility or generating plausible reproduction scripts to verify the model-generated patch—the tools often finalize the patch and conclude the resolution process. At present, models demonstrate limited capacity for assessing the completeness of patches in response to issues. Semantic relationships between multiple related modifications that may be overlooked during patch generation are also challenging to detect in subsequent self-reflection phases.

django-13660: The primary reason this issue remains unresolved is due to a misleading solution provided within the issue description. Specifically, the issue description suggests the following fix: “exec should be passed a dictionary containing a minimal set of globals. This can be done by just passing a new, empty dictionary as the second argument of exec.” Many tools adhered to this recommendation, generating patches based on it, primarily modifying “exec(options[’command’])” to “exec(options[’command’], )”, which aligns with the solution suggested in the issue. However, the actual patch replaces the statement with “exec(options[’command’], globals())”, which differs in code semantics from the suggested fix (as shown in the following code box). In the Agentless report, the Quality of Resolve Solution metric received a score of “Complete steps in NL” (7.5 points in Table [4](https://arxiv.org/html/2411.10213v1#S4.T4 "Table 4 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing")) rather than a “Misleading” score (0 points). We speculate that this may have been an oversight in manual analysis.

<svg class="ltx_picture" height="6003.72" id="S4.SS1.p13.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,6003.72) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 5985.52)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Comparison of the Generated Patch Candidate and the Human-written Patch.</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="5954.02" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><svg class="ltx_picture" height="2966.56" id="S4.SS1.p13.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,2966.56) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 2948.35)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">The Patch Candidate Generated by Gru</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="2916.85" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">[⬇](data:text/plain;base64,ZGlmZiAtLWdpdCBhL2RqYW5nby9jb3JlL21hbmFnZW1lbnQvY29tbWFuZHMvc2hlbGwucHkgYi9kamFuZ28vY29yZS9tYW5hZ2VtZW50L2NvbW1hbmRzL3NoZWxsLnB5Ci0tLSBhL2RqYW5nby9jb3JlL21hbmFnZW1lbnQvY29tbWFuZHMvc2hlbGwucHkKKysrIGIvZGphbmdvL2NvcmUvbWFuYWdlbWVudC9jb21tYW5kcy9zaGVsbC5weQpAQCAtODQsMTMgKzg0LDEzIEBAIGRlZiBweXRob24oc2VsZiwgb3B0aW9ucyk6CiAgICAgZGVmIGhhbmRsZShzZWxmLCAqKm9wdGlvbnMpOgogICAgICAgICAjIEV4ZWN1dGUgdGhlIGNvbW1hbmQgYW5kIGV4aXQuCiAgICAgICAgIGlmIG9wdGlvbnNbJ2NvbW1hbmQnXToKLSAgICAgICAgICAgIGV4ZWMob3B0aW9uc1snY29tbWFuZCddKQorICAgICAgICAgICAgZXhlYyhvcHRpb25zWydjb21tYW5kJ10sIHt9KQogICAgICAgICAgICAgcmV0dXJuCi4uLg==) 1diff  --git  a/django/core/management/commands/shell.py  b/django/core/management/commands/shell.py 2---  a/django/core/management/commands/shell.py 3+++  b/django/core/management/commands/shell.py 4@@  -84,13  +84,13  @@  def  python(self,  options): 5  def  handle(self,  **options): 6  #  Execute  the  command  and  exit. 7  if  options[’command’]: 8-  exec(options[’command’]) 9+  exec(options[’command’],  {}) 10  return 11...</foreignobject></g></g></svg> <svg class="ltx_picture" height="2984.7" id="S4.SS1.p13.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,2984.7) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 2964.95)"><foreignobject height="13.84" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">The Human-written Patch (Ground Truth)</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="2933.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">[⬇](data:text/plain;base64,ZGlmZiAtLWdpdCBhL2RqYW5nby9jb3JlL21hbmFnZW1lbnQvY29tbWFuZHMvc2hlbGwucHkgYi9kamFuZ28vY29yZS9tYW5hZ2VtZW50L2NvbW1hbmRzL3NoZWxsLnB5Ci0tLSBhL2RqYW5nby9jb3JlL21hbmFnZW1lbnQvY29tbWFuZHMvc2hlbGwucHkKKysrIGIvZGphbmdvL2NvcmUvbWFuYWdlbWVudC9jb21tYW5kcy9zaGVsbC5weQpAQCAtODQsMTMgKzg0LDEzIEBAIGRlZiBweXRob24oc2VsZiwgb3B0aW9ucyk6CiAgICAgZGVmIGhhbmRsZShzZWxmLCAqKm9wdGlvbnMpOgogICAgICAgICAjIEV4ZWN1dGUgdGhlIGNvbW1hbmQgYW5kIGV4aXQuCiAgICAgICAgIGlmIG9wdGlvbnNbJ2NvbW1hbmQnXToKLSAgICAgICAgICAgIGV4ZWMob3B0aW9uc1snY29tbWFuZCddKQorICAgICAgICAgICAgZXhlYyhvcHRpb25zWydjb21tYW5kJ10sIGdsb2JhbHMoKSkKICAgICAgICAgICAgIHJldHVybgouLi4=) 1diff  --git  a/django/core/management/commands/shell.py  b/django/core/management/commands/shell.py 2---  a/django/core/management/commands/shell.py 3+++  b/django/core/management/commands/shell.py 4@@  -84,13  +84,13  @@  def  python(self,  options): 5  def  handle(self,  **options): 6  #  Execute  the  command  and  exit. 7  if  options[’command’]: 8-  exec(options[’command’]) 9+  exec(options[’command’],  globals()) 10  return 11...</foreignobject></g></g></svg></foreignobject></g></g></svg><svg class="ltx_picture" height="156.09" id="S4.SS1.p14.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,156.09) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Insights(1)-(2): Overall, we derive two key insights from the above case analyses: (1) Enhancing the large model or agent’s understanding of root causes is crucial. The model’s understanding should extend beyond the location where the issue occurs (i.e., the symptoms) and include deeper reasoning about the relationship between multiple suspicious locations in the issue title or stack trace and the root cause. This would help prioritize the root cause location more effectively. (2) Improving the large model or agent’s ability to generate patches for related locations and to verify patch completeness offers a feasible approach to further enhance issue resolution effectiveness from a holistic perspective.</foreignobject></g></g></svg>

The above analysis pertains to the five highest-scoring cases within the No-one-resolve Set. Next, we will analyze the five lowest-scoring cases within the All-resolve Set.

django-12497: Although this case’s issue description does not provide any granular bug location information, it does offer a high-quality solution. Repair tools, based on this solution, progressively localized the bug down to the file, function, and line levels, ultimately reusing the suggested solution to generate a correct patch. In relation to the conclusions drawn from Table [3](https://arxiv.org/html/2411.10213v1#S4.T3 "Table 3 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing"), as the impact of the completeness of these five metrics on issue resolution varies, cases with an equivalent total score but concentrated on more relevant metrics (e.g., line-level location and resolve solution) are likely to be easier to resolve.

django-11133: The patch candidate provided is in a different location within the same file as the manually crafted patch, but it also passes the failed tests. Thus, after the issue provided high-quality file-level bug localization information (6.67 points), the absence of function- and line-level location information does not appear to be critical. However, it is also important to note that passing failed tests does not always equate to generating a semantically correct patch, as discussed in Section  [6](https://arxiv.org/html/2411.10213v1#S6 "6\. Threats to Validity ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing").

pytest-7432: The patch candidate is situated in a different location within the same function as the manually created patch, but it too passes the failed tests. After providing partial file- and function-level localization information (each scoring 3.33 points), the lack of line-level information does not seem to be a significant limitation.

pytest-5692: Although the issue only provides file-level bug location information, repair tools effectively refined the location through iterative understanding of the issue, eventually pinpointing the bug’s line-level location and producing a correct patch. This indicates that the model and agent possess strong capabilities for identifying relevant information, capable of comprehending the problem described in natural language and extracting high-relevance code segments through multiple rounds of iteration.

django-16046: The reasons for its resolution are nearly identical to those of django-11133.

<svg class="ltx_picture" height="172.69" id="S4.SS1.p21.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,172.69) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="145.13" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Insights(3)-(5): Overall, we derive three insights from the above case analyses: (3) Due to the varying impact of the completeness of these five metrics on issue resolution, cases with the same total score but concentrated in more relevant metrics (e.g., line-level location and resolve solution) tend to be easier to resolve. (4) The manually crafted patch may not be the only solution to the issue; multiple resolution strategies may exist. The model may attempt to generate a semantically correct or partially correct patch based on the suspicious locations already highlighted in the issue. (5) Large models and agents demonstrate strong capabilities for discovering relevant information. They can deeply analyze the natural language issue description, iteratively extracting high-relevance code segments over multiple interactions.</foreignobject></g></g></svg>

As shown in Figure [1](https://arxiv.org/html/2411.10213v1#S4.F1 "Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing"), we found significant differences between Agent and non-Agent systems in solving cases. Therefore, we analyzed the differences between cases that can only be solved by the Agent system and those that can only be solved by the non-Agent system. We analyzed two cases that could only be solved by Agentless, specifically django-11422 and sympy-24909. In django-11422, the Agent system identified three locations to make repairs, but bypassed the correct repair location in the golden patch, iter_all_python_module_files(), during the fix. On the other hand, Agentless only selected the location of the golden patch during localization, hence it correctly fixed the issue. The following figure shows the response of the two systems when selecting repair locations.

<svg class="ltx_picture" height="292.7" id="S4.SS1.p23.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,292.7) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 274.5)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Comparison of MarsCode Agent and Agentless</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="243" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><svg class="ltx_picture" height="132.72" id="S4.SS1.p23.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,132.72) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 114.52)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">MarsCode Agent</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="83.02" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Thank you for providing the iter_all_python_module_files() implementation. It appears that this function only iterates through loaded Python modules, which means manage.py is not included since it’s not imported as a module. Thought: The issue is that manage.py is not being added to the list of watched files. We need to explicitly add it to the extra_files set in the reloader. Let’s modify the runserver command to add manage.py to the watched files.</foreignobject></g></g></svg> <svg class="ltx_picture" height="95.21" id="S4.SS1.p23.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,95.21) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 77)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Agentless</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">To fix this, we need to ensure that the manage.py file is included in the list of files to be watched. We can do this by modifying the iter_all_python_module_files function to include the manage.py file explicitly.</foreignobject></g></g></svg></foreignobject></g></g></svg>

In sympy-24909, we also observed that both Agent and non-Agent systems identified the correct faulty location, the function Prefix.__mul__. However, the LLM was not confident about how to fix the bug. Because the Agent system lacked sampling, it directly generated an incorrect patch. On the other hand, Agentless performed 42 samples and chose the patch that appeared most frequently, resulting in the correct modification.

The instance django-12453 is an example that all Agent systems can solve but Agentless cannot. We observed that the Agentless patch is very close to the ground truth, only missing the import of the relevant module. The Agent, through multiple rounds of dialogue and tool invocation, can detect the missing dependencies, whereas Agentless merely generates the code block and cannot compensate for the missing dependencies.

<svg class="ltx_picture" height="106.28" id="S4.SS1.p26.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,106.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Insights(6)-(8): (6) The Agent system may fail to complete the repair due to an inability to select the correct repair location among multiple candidates. (7) Additionally, the lack of sampling in the Agent system leads to low diversity in the results, causing failure in repairs. (8) Non-Agent systems, on the other hand, may produce syntactically incorrect patches because they lack multi-turn dialogues and tool invocation.</foreignobject></g></g></svg>

### 4.2\. RQ2: Effectiveness of FL

Fault localization is a critical step in resolving issues; only when the faulty code element is accurately identified can the model generate a semantically correct patch. In this RQ, we examine the effectiveness of seven methods in fault localization. As some tools’ run trajectories lack explicit fault localization results or provide insufficient localization granularity—offering bug location information only at the file level, without finer-grained function or line-level details—we use the modified files and line information in the final submitted patch as a reference to fairly assess each tool’s localization capability. Specifically, we downloaded the final patch set submitted for each tool’s benchmark on the SWE-bench Lite website, then parsed the formatted patch files. From the perspective of the files pre-modification (pre-patch code files), we recorded the files and lines that were modified. We applied the same processing to the manually crafted patches (ground truth).

After obtaining the above processed data, we created Figure [1(b)](https://arxiv.org/html/2411.10213v1#S4.F1.sf2 "In Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") and Figure [1(c)](https://arxiv.org/html/2411.10213v1#S4.F1.sf3 "In Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") to illustrate the hit rate of fault localization at the file and line levels, respectively. The meaning of the different modules in the figures is similar to Figure [1(a)](https://arxiv.org/html/2411.10213v1#S4.F1.sf1 "In Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing"), except that the latter is oriented towards the issue resolve task. For the file-level localization task, since cases in SWE-bench Lite only modify one file (the ground truth file), but repair tools are not limited to a single file in generating patches, we consider it a file hit if the patch file set contains the ground truth file. For line-level localization, we define a hit as an overlap between the lines modified in the patch and the ground truth line.

Based on this definition of a hit, we investigated the localization performance of the seven tools. As shown in Figure [1(a)](https://arxiv.org/html/2411.10213v1#S4.F1.sf1 "In Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing"), MarsCode Agent performs best on the file-level localization task, successfully localizing 239 cases. It is followed by Alibaba Lingma Agent, Gru, Agentless+RepoGraph, AutoCodeRover, Honeycomb, and finally Agentless, which successfully localized 206 cases. MarsCode Agent shows a 16.0% improvement over Agentless. As the highest and lowest-ranking tools in the issue resolve task, MarsCode Agent and Agentless demonstrate a smaller performance gap of 43.9% in the issue resolve task, which is much larger than their difference in file-level localization performance. Additionally, Agentless+RepoGraph, another representative of non-agent systems, ranks fourth in file-level fault localization but sixth in the issue resolve task. These comparisons indicate that the file-level localization task is generally simpler than the subsequent patch generation task. Moreover, agent-based repair methods like MarsCode Agent show more strength in subsequent patch generation optimization.

However, we encountered an unexpected result: Honeycomb, the second-best tool in the issue resolve task (with minimal gap from MarsCode), performs poorly in file-level localization, ranking sixth, with only two more hits than the lowest-ranking Agentless. To investigate this further, we examined the tools’ performance in line-level localization. As shown in Figure [1(c)](https://arxiv.org/html/2411.10213v1#S4.F1.sf3 "In Figure 1 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing"), MarsCode remains the best method, while Honeycomb ranks a close second, trailing by only two hits. This suggests that although Honeycomb performs poorly in file-level localization, it has a relatively high hit rate for faulty lines within the located file (137/208 = 65.9%). Therefore, strong line-level localization performance is the main reason Honeycomb resolves a larger number of issues. This further suggests that line-level localization results are more closely associated with the subsequent patch generation process than file-level results. Improving fine-grained line-level localization may thus be key to enhancing end-to-end repair effectiveness, aligning with our conclusions based on Table [3](https://arxiv.org/html/2411.10213v1#S4.T3 "Table 3 ‣ 4.1\. RQ1: Effectiveness of Systems ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") (i.e., the line-level location metric shows the most significant difference between the All-resolve Set and No-one-resolve Set).

Overall, three key insights can be derived from the above analysis: (1) Existing methods have achieved relatively strong performance in file-level fault localization, but there remains room for improvement in line-level localization tasks. (2) Finer-grained line-level localization results are more closely correlated with end-to-end issue resolution. When erroneous code lines can be accurately identified by a tool, the generation of a correct patch becomes more feasible.

<svg class="ltx_picture" height="120.19" id="S4.SS2.p6.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,120.19) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="92.63" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Insights(9)-(10): Overall, two key insights can be derived from the above analysis: (9) Existing methods have achieved relatively strong performance in file-level fault localization, but there remains room for improvement in line-level localization tasks. (10) Finer-grained line-level localization results are more closely correlated with end-to-end issue resolution. When erroneous code lines can be accurately identified by a tool, the generation of a correct patch becomes more feasible.</foreignobject></g></g></svg>

### 4.3\. RQ3: Effectiveness of Reproduction

Table 5. Reproduction Statistics of Leading LLM-based Bug Fixing Systems on SWE-Bench Lite. #Resolved refers to the number of successfully resolved issues, #(Reproduced&Resolved) refers to the number of issues that were successfully resolved and involved bug reproduction during the resolution. %(Reproduced&Resolved) refers to #(Reproduced&Resolved) / #Resolved.

 | System Name | Type | Reproducer | #Resolved | #(Reproduced&Resolved) | %(Reproduced&Resolved) |
| --- | --- | --- | --- | --- | --- |
| MarsCode Agent | Commercial | ✓ | 118 | 83 | 70.3 |
| Honeycomb | ✓ | 115 | 115 | 100 |
| Gru | $\times$ | 107 | 0 | 0 |
| Alibaba Lingma Agent | ✓ | 99 | 12 | 12.1 |
| AutoCodeRover | Open Source | ✓ | 92 | 39 | 42.4 |
| Agentless + RepoGraph | $\times$ | 89 | 0 | 0 |
| Agentless | $\times$ | 82 | 0 | 0 | 

We analyzed the trajectories of the seven systems evaluated on SWE-Bench-Lite, examining the use of reproduction in different systems and analyzing some typical cases to gain a deeper understanding of the impact of reproduction on bug fix success rates, as well as the challenges reproduction faces in current systems. Table [5](https://arxiv.org/html/2411.10213v1#S4.T5 "Table 5 ‣ 4.3\. RQ3: Effectiveness of Reproduction ‣ 4\. Analysis & Results ‣ An Empirical Study on LLM-based Agents for Automated Bug Fixing") presents the statistics on reproduction usage across different systems. The systems Gru, Agentless, and Agentless+RepoGraph do not include a reproduction module, while Marscode Agent, Honeycomb, Alibaba Lingma Agent, and AutoCodeRover all employ a reproduction module, and in the successfully fixed bugs, they perform bug reproduction for 70.3%, 100%, 12.1%, and 42.4% of the issues, respectively. Marscode Agent, Alibaba Lingma Agent, and AutoCodeRover decide whether to perform bug reproduction based on the issue, opting for static analysis for bugs that are difficult to reproduce. Honeycomb, on the other hand, requires the LLM to attempt bug reproduction for all issues.

Through an analysis and comparison of the trajectories of different systems, we found that reproduction can provide additional information for defect localization when issue information is lacking, and it helps verify the accuracy of generated candidate patches. Our trajectory analysis revealed that among the 168 issues resolved by at least one system, 24 issues could only be solved using reproduction. These 24 instances generally had brief textual descriptions that only briefly outlined the bug conditions, making defect localization challenging based on the text alone. Of these, 20 instances included code snippets in the issue description to help understand the bug. For example, in sympy-15346, the issue contains only a brief textual description followed by an example of the bug. Locating the defect based solely on the text was challenging. Marscode Agent implemented a bug reproduction script based on the example in the issue and gradually identified the defect location based on the reproduction results, eventually generating the correct patch. Notably, during trajectory analysis, we observed that Marscode Agent initially struggled to pinpoint the defect location when running the reproduction script. It then attempted to print intermediate variables at different points in the script, and it successfully identified the defect location by analyzing the outputs during execution. This further demonstrates the advantage of the autonomy of agent-based systems in bug fixing.

Although reproduction could provide additional information and verify the correctness of candidate patches, it does not always improve the success rate of bug fixing. For instance, in django-11422, the "Autoreloader" mentioned in the issue description directly points to the file $autoreload.py$. By using information such as "track changes" from the issue, Agentless accurately located the code snippet needing modification and generated the correct patch. However, systems like Marscode Agent and Honeycomb, which performed bug reproduction, was distracted by the reproduction information. The reproduction diverted the LLM’s attention away from the issue description, leading to incorrect localization of the defect cause and ultimately failing to produce the correct patch.

<svg class="ltx_picture" height="89.67" id="S4.SS3.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,89.67) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Insights(11)-(12): (11) Reproduction can provide additional information for defect localization when issue information is lacking and help verify the accuracy of generated candidate patches. (12) When the issue description is already clear and precise, reproduction may mislead the LLM’s judgment, reducing its focus on the issue description.</foreignobject></g></g></svg>

## 5\. Discussion

### 5.1\. Large Language Model

From the LLM perspective, it is necessary to further enhance the model’s reasoning ability so that it can accurately identify information related to the bug within the issue, thereby reducing the interference of noise. Additionally, for multiple potential repair locations, the model should utilize its reasoning capability to select the location most relevant to the issue.

### 5.2\. Agentic Flow

From the Agentic flow perspective, agents should especially focus on the quality of the issue and pay attention to multiple suspicious locations in the stack trace. The Agentic flow design should include mechanisms to check the completeness of patches and consider the global impact of the fixes. During the use of the model, mechanisms should be established to either avoid the randomness of the model’s output or make full use of the diversity in the model’s output.

In fault localization, the accuracy of line-level localization is more important than file-level, as the discovery space at the line level is larger, necessitating finer-grained localization results. During the reproduction process, it is crucial to strengthen the determination of the correctness of the reproduction, as an incorrect reproduction can lead to the failure of the entire solving process.

## 6\. Threats to Validity

Fail-to-Pass Tests: SWE-bench uses Fail-to-Pass (F2P) tests to verify the correctness of generated patches. However, F2P tests may not be comprehensive, allowing a patch to pass F2P and be deemed correct without fully addressing the user’s issue. This is a common problem in the field of APR as well as in LLM evaluation based on unit tests (Liu et al., [2024b](https://arxiv.org/html/2411.10213v1#bib.bib26)). In this context, we assume that a patch is correct as long as it passes the F2P test cases. We also call for contributions from the academic community to improve the test cases in the SWE-bench evaluation dataset to make the evaluation results more reliable.

Uncertainty of LLM: The output of LLMs is stochastic, leading to a probabilistic nature for whether an instance is solved. In this work, we directly analyzed the patches and trajectories submitted by various systems, assuming that the results submitted to SWE-bench represent the best performance of the Agent. Furthermore, conducting multiple experiments for each system to eliminate stochasticity is impractical in terms of both cost and accessibility.

## 7\. Related Work

In this section, we discuss basic concepts of large language models and their application on software engineering tasks, especially for fault localization and automated program repair. We also discuss recent advances in LLM-based agents for software engineering.

### 7.1\. Large Language Models

Large language models (LLMs) are highly advanced pre-trained language models. These models undergo initial unsupervised training on vast amounts of corpus, followed by fine-tuning for specific tasks to enhance performance. In natural language processing (NLP), LLMs have been extensively applied to various tasks such as machine translation (Wang et al., [2023d](https://arxiv.org/html/2411.10213v1#bib.bib43); Zhang et al., [2023b](https://arxiv.org/html/2411.10213v1#bib.bib57)), text summarization (Zhang et al., [2023c](https://arxiv.org/html/2411.10213v1#bib.bib59)), and classification (Mayer et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib31)).

Language models are classified into three categories based on their architecture: encoder-only models (Feng et al., [2020](https://arxiv.org/html/2411.10213v1#bib.bib10)), decoder-only models (Nijkamp et al., [2022](https://arxiv.org/html/2411.10213v1#bib.bib35)), and encoder-decoder models (Tian et al., [2022](https://arxiv.org/html/2411.10213v1#bib.bib41)). Most existing LLMs for code utilize the transformer architecture’s encoders, known for their exceptional learning capabilities and scalability. Regardless of their architecture, most models can be fine-tuned with task-specific data to enhance performance (Lin et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib24)).

Large language models (LLMs) have become a promising choice for various software engineering tasks due to their impressive performance in both code generation and understanding (Yang et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib54)). Researchers and developers have applied LLMs to several software engineering tasks, such as program synthesis (Liu et al., [2024b](https://arxiv.org/html/2411.10213v1#bib.bib26); Zhu-Tian et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib63); Wang et al., [2023b](https://arxiv.org/html/2411.10213v1#bib.bib42), [c](https://arxiv.org/html/2411.10213v1#bib.bib45), [a](https://arxiv.org/html/2411.10213v1#bib.bib44)), code translation (Yu et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib56); Yang et al., [2024b](https://arxiv.org/html/2411.10213v1#bib.bib55)), program repair (Lin et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib25); Jiang et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib13); Xia et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib51)), fault detection and localization (Du et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib9); Qin et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib38)), incident analysis (Chen et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib8); Ahmed et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib6)), code summarization (Geng et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib11)) and testing (Sun et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib39)). For example, Codex (Chen et al., [2021](https://arxiv.org/html/2411.10213v1#bib.bib7)), StarCoder (Lozhkov et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib28)), and DeepSeek-Coder (Zhu et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib62)) are notable code-specific LLMs developed through extensive training on large datasets of open-source code snippets. Additionally, instruction-following code-specific LLMs such as DeepSeek-Coder-Instruct (Zhu et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib62)) and Magicoder (Wei et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib47)) have been created using instruction-tuning methods to enhance their utility in coding tasks.

### 7.2\. Fault Localization

Fault localization (FL) (Wong et al., [2016](https://arxiv.org/html/2411.10213v1#bib.bib48)) techniques aim to discover and analyze the location and causes of faults, which can be categorized into dynamic and static approaches. Dynamic FL techniques, such as spectrum-based fault localization (SBFL) (Abreu et al., [2007](https://arxiv.org/html/2411.10213v1#bib.bib5), [2009](https://arxiv.org/html/2411.10213v1#bib.bib4)) and mutation-based fault localization (MBFL) (Papadakis and Le Traon, [2015](https://arxiv.org/html/2411.10213v1#bib.bib37)), analyze the dynamic execution information of a program to determine fault locations, though they are resource-intensive. Static FL techniques (Mao et al., [2014](https://arxiv.org/html/2411.10213v1#bib.bib30)) determine fault locations through semantic or syntactic analysis at the bug report or source code level, offering fast detection with low resource consumption. Advanced FL techniques, such as multiple fault localization (MFL) and combined dynamic and static methods, have emerged to guide APR tools in finding and fixing more errors (Xiao et al., [2021](https://arxiv.org/html/2411.10213v1#bib.bib52); Kim et al., [2019](https://arxiv.org/html/2411.10213v1#bib.bib16); Neelofar et al., [2017](https://arxiv.org/html/2411.10213v1#bib.bib33)).

### 7.3\. Automated Program Repair

Automated program repair (APR) (Le Goues et al., [2019](https://arxiv.org/html/2411.10213v1#bib.bib20)) has attracted significant attention over the past decade. APR techniques aim to generate patches for buggy programs to pass given test suites. These techniques can be categorized into search-based (Li et al., [2022b](https://arxiv.org/html/2411.10213v1#bib.bib21); Mehne et al., [2018](https://arxiv.org/html/2411.10213v1#bib.bib32)), semantics-based (Le et al., [2017](https://arxiv.org/html/2411.10213v1#bib.bib17); Nguyen et al., [2013](https://arxiv.org/html/2411.10213v1#bib.bib34); Le et al., [2016](https://arxiv.org/html/2411.10213v1#bib.bib18)), and pattern/learning-based approaches (Li et al., [2020](https://arxiv.org/html/2411.10213v1#bib.bib22), [2022a](https://arxiv.org/html/2411.10213v1#bib.bib23); Zhang et al., [2023a](https://arxiv.org/html/2411.10213v1#bib.bib58)). Search-based APR techniques like GenProg (Le Goues et al., [2011](https://arxiv.org/html/2411.10213v1#bib.bib19)) use predefined code mutation operators to generate patches, while semantics-based APR techniques generate patches by solving repair constraints based on test suite specifications. Learning-based APR techniques, such as those utilizing deep learning models, train on large code repositories to predict correct patches. Recent work has shown the use of LLMs for APR, often focusing on constructing APR-specific prompts to guide LLMs in generating patches for buggy program statements (Xia et al., [2023](https://arxiv.org/html/2411.10213v1#bib.bib51)).

### 7.4\. Agents for Software Development

The emergence and popularity of agent-based frameworks have led to the development of agent-based approaches for solving software engineering tasks. Devin and its open-source counterpart OpenDevin (Wang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib46)) are among the first end-to-end LLM agent-based frameworks. These frameworks use agents for planning based on user requirements and enable agents to iteratively perform tasks using tools like file editors, terminals, and web search engines. SWE-agent (Yang et al., [2024a](https://arxiv.org/html/2411.10213v1#bib.bib53)), for example, designs a custom agent-computer interface (ACI) allowing LLM agents to interact with the repository environment through actions such as reading, editing files, and running bash commands. AutoCodeRover (Zhang et al., [2024](https://arxiv.org/html/2411.10213v1#bib.bib60)) provides LLM agents with specific APIs to effectively identify locations needing modification to resolve issues. Numerous other agent-based approaches have been developed, both in open-source and commercial products.

## 8\. Conclusion

In this paper, we analyzed the top 4 commercial systems and the top 3 open-source systems on SWE-bench Lite. We conducted detailed analyses of the performance of LLM-based Agents in automatic bug fixing for code repositories, the performance of different systems in Fault Localization, and their performance in Reproduction. The analysis results indicate that to further enhance the capabilities of LLM-based Agents in bug fixing, future efforts should focus on improving the reasoning ability of LLMs. Additionally, attention should be given to the Agentic flow design, considering the quality of issues, stack traces, and the correctness of reproductions.

## Data availability

All raw data comes from the official SWE-bench experiment repository: https://github.com/swe-bench/experiments/

## References

*   (1)
*   hon ([n. d.]) [n. d.]. [https://honeycomb.sh/](https://honeycomb.sh/).
*   gru ([n. d.]) [n. d.]. [https://gru.ai/](https://gru.ai/).
*   Abreu et al. (2009) Rui Abreu, Peter Zoeteweij, Rob Golsteijn, and Arjan JC Van Gemund. 2009. A practical evaluation of spectrum-based fault localization. *Journal of Systems and Software* 82, 11 (2009), 1780–1792.
*   Abreu et al. (2007) Rui Abreu, Peter Zoeteweij, and Arjan JC Van Gemund. 2007. On the accuracy of spectrum-based fault localization. In *Testing: Academic and industrial conference practice and research techniques-MUTATION (TAICPART-MUTATION 2007)*. IEEE, 89–98.
*   Ahmed et al. (2023) Toufique Ahmed, Supriyo Ghosh, Chetan Bansal, Thomas Zimmermann, Xuchao Zhang, and Saravan Rajmohan. 2023. Recommending root-cause and mitigation steps for cloud incidents using large language models. In *2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)*. IEEE, 1737–1749.
*   Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374* (2021).
*   Chen et al. (2024) Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao, Xuedong Gao, Hao Fan, Ming Wen, et al. 2024. Automatic root cause analysis via large language models for cloud incidents. In *Proceedings of the Nineteenth European Conference on Computer Systems*. 674–688.
*   Du et al. (2024) Xiaohu Du, Ming Wen, Jiahao Zhu, Zifan Xie, Bin Ji, Huijun Liu, Xuanhua Shi, and Hai Jin. 2024. Generalization-Enhanced Code Vulnerability Detection via Multi-Task Instruction Fine-Tuning. *arXiv preprint arXiv:2406.03718* (2024).
*   Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. *arXiv preprint arXiv:2002.08155* (2020).
*   Geng et al. (2024) Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao. 2024. Large language models are few-shot summarizers: Multi-intent comment generation via in-context learning. In *Proceedings of the 46th IEEE/ACM International Conference on Software Engineering*. 1–13.
*   Hu et al. (2024) Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. 2024. GRAG: Graph Retrieval-Augmented Generation. *arXiv preprint arXiv:2405.16506* (2024).
*   Jiang et al. (2023) Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact of code language models on automated program repair. In *2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)*. IEEE, 1430–1442.
*   Jimenez et al. (2023) Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? *arXiv preprint arXiv:2310.06770* (2023).
*   Kang et al. (2023) Sungmin Kang, Bei Chen, Shin Yoo, and Jian-Guang Lou. 2023. Explainable automated debugging via large language model-driven scientific debugging. *arXiv preprint arXiv:2304.02195* (2023).
*   Kim et al. (2019) Yunho Kim, Seokhyeon Mun, Shin Yoo, and Moonzoo Kim. 2019. Precise learn-to-rank fault localization using dynamic and static features of target programs. *ACM Transactions on Software Engineering and Methodology (TOSEM)* 28, 4 (2019), 1–34.
*   Le et al. (2017) Xuan-Bach D Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser. 2017. JFIX: semantics-based repair of Java programs via symbolic PathFinder. In *Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis*. 376–379.
*   Le et al. (2016) Xuan-Bach D Le, David Lo, and Claire Le Goues. 2016. Empirical study on synthesis engines for semantics-based program repair. In *2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)*. IEEE, 423–427.
*   Le Goues et al. (2011) Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2011. Genprog: A generic method for automatic software repair. *Ieee transactions on software engineering* 38, 1 (2011), 54–72.
*   Le Goues et al. (2019) Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated program repair. *Commun. ACM* 62, 12 (2019), 56–65.
*   Li et al. (2022b) Dongcheng Li, W Eric Wong, Mingyong Jian, Yi Geng, and Matthew Chau. 2022b. Improving search-based automatic program repair with Neural Machine Translation. *IEEE Access* 10 (2022), 51167–51175.
*   Li et al. (2020) Yi Li, Shaohua Wang, and Tien N Nguyen. 2020. Dlfix: Context-based code transformation learning for automated program repair. In *Proceedings of the ACM/IEEE 42nd international conference on software engineering*. 602–614.
*   Li et al. (2022a) Yi Li, Shaohua Wang, and Tien N Nguyen. 2022a. Dear: A novel deep learning-based approach for automated program repair. In *Proceedings of the 44th international conference on software engineering*. 511–523.
*   Lin et al. (2023) Bo Lin, Shangwen Wang, Zhongxin Liu, Yepang Liu, Xin Xia, and Xiaoguang Mao. 2023. Cct5: A code-change-oriented pre-trained model. In *Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering*. 1509–1521.
*   Lin et al. (2024) Bo Lin, Shangwen Wang, Ming Wen, Liqian Chen, and Xiaoguang Mao. 2024. One Size Does Not Fit All: Multi-granularity Patch Generation for Better Automated Program Repair. In *Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis*.
*   Liu et al. (2024b) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024b. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. *Advances in Neural Information Processing Systems* 36 (2024).
*   Liu et al. (2024a) Yizhou Liu, Pengfei Gao, Xinchen Wang, Chao Peng, and Zhao Zhang. 2024a. MarsCode Agent: AI-native Automated Bug Fixing. *arXiv preprint arXiv:2409.00899* (2024).
*   Lozhkov et al. (2024) Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. *arXiv preprint arXiv:2402.19173* (2024).
*   Ma et al. (2024) Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. 2024. How to Understand Whole Software Repository? arXiv:2406.01422 [cs.SE] [https://arxiv.org/abs/2406.01422](https://arxiv.org/abs/2406.01422)
*   Mao et al. (2014) Xiaoguang Mao, Yan Lei, Ziying Dai, Yuhua Qi, and Chengsong Wang. 2014. Slice-based statistical fault localization. *Journal of Systems and Software* 89 (2014), 51–62.
*   Mayer et al. (2023) Christian WF Mayer, Sabrina Ludwig, and Steffen Brandt. 2023. Prompt text classifications with transformer models! An exemplary introduction to prompt-based learning with large language models. *Journal of Research on Technology in Education* 55, 1 (2023), 125–141.
*   Mehne et al. (2018) Ben Mehne, Hiroaki Yoshida, Mukul R Prasad, Koushik Sen, Divya Gopinath, and Sarfraz Khurshid. 2018. Accelerating search-based program repair. In *2018 IEEE 11th international conference on software testing, verification and validation (ICST)*. IEEE, 227–238.
*   Neelofar et al. (2017) Neelofar Neelofar, Lee Naish, Jason Lee, and Kotagiri Ramamohanarao. 2017. Improving spectral-based fault localization using static analysis. *Software: Practice and Experience* 47, 11 (2017), 1633–1655.
*   Nguyen et al. (2013) Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. 2013. Semfix: Program repair via semantic analysis. In *2013 35th International Conference on Software Engineering (ICSE)*. IEEE, 772–781.
*   Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. *arXiv preprint arXiv:2203.13474* (2022).
*   Ouyang et al. (2024) Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. 2024. RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph. *arXiv preprint arXiv:2410.14684* (2024).
*   Papadakis and Le Traon (2015) Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL: mutation-based fault localization. *Software Testing, Verification and Reliability* 25, 5-7 (2015), 605–628.
*   Qin et al. (2024) Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, and Xiaoguang Mao. 2024. AgentFL: Scaling LLM-based Fault Localization to Project-Level Context. *arXiv preprint arXiv:2403.16362* (2024).
*   Sun et al. (2023) Maolin Sun, Yibiao Yang, Yang Wang, Ming Wen, Haoxiang Jia, and Yuming Zhou. 2023. SMT solver validation empowered by large pre-trained language models. In *2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)*. IEEE, 1288–1300.
*   Tao et al. (2024) Wei Tao, Yucheng Zhou, Wenqiang Zhang, and Yu Cheng. 2024. MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution. *arXiv preprint arXiv:2403.17927* (2024).
*   Tian et al. (2022) Zhao Tian, Junjie Chen, Qihao Zhu, Junjie Yang, and Lingming Zhang. 2022. Learning to construct better mutation faults. In *Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering*. 1–13.
*   Wang et al. (2023b) Chaozheng Wang, Junhao Hu, Cuiyun Gao, Yu Jin, Tao Xie, Hailiang Huang, Zhenyu Lei, and Yuetang Deng. 2023b. How practitioners expect code completion?. In *Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering*. 1294–1306.
*   Wang et al. (2023d) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023d. Document-level machine translation with large language models. *arXiv preprint arXiv:2304.02210* (2023).
*   Wang et al. (2023a) Shangwen Wang, Mingyang Geng, Bo Lin, Zhensu Sun, Ming Wen, Yepang Liu, Li Li, Tegawendé F Bissyandé, and Xiaoguang Mao. 2023a. Natural language to code: How far are we?. In *Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering*. 375–387.
*   Wang et al. (2023c) Shangwen Wang, Bo Lin, Zhensu Sun, Ming Wen, Yepang Liu, Yan Lei, and Xiaoguang Mao. 2023c. Two birds with one stone: Boosting code generation and code search via a generative adversarial network. *Proceedings of the ACM on Programming Languages* 7, OOPSLA2 (2023), 486–515.
*   Wang et al. (2024) Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. 2024. OpenDevin: An Open Platform for AI Software Developers as Generalist Agents. *arXiv preprint arXiv:2407.16741* (2024).
*   Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need. *arXiv preprint arXiv:2312.02120* (2023).
*   Wong et al. (2016) W Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A survey on software fault localization. *IEEE Transactions on Software Engineering* 42, 8 (2016), 707–740.
*   Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864* (2023).
*   Xia et al. (2024) Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying LLM-based Software Engineering Agents. *arXiv preprint arXiv:2407.01489* (2024).
*   Xia et al. (2023) Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated program repair in the era of large pre-trained language models. In *2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)*. IEEE, 1482–1494.
*   Xiao et al. (2021) Xi Xiao, Yuqing Pan, Bin Zhang, Guangwu Hu, Qing Li, and Runiu Lu. 2021. ALBFL: A novel neural ranking model for software fault localization via combining static and dynamic features. *Information and Software Technology* 139 (2021), 106653.
*   Yang et al. (2024a) John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024a. Swe-agent: Agent-computer interfaces enable automated software engineering. *arXiv preprint arXiv:2405.15793* (2024).
*   Yang et al. (2023) Kang Yang, Xinjun Mao, Shangwen Wang, Tanghaoran Zhang, Bo Lin, Yanlin Wang, Yihao Qin, Zhang Zhang, and Xiaoguang Mao. 2023. Enhancing Code Intelligence Tasks with ChatGPT. *arXiv preprint arXiv:2312.15202* (2023).
*   Yang et al. (2024b) Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024b. Exploring and unleashing the power of large language models in automated code translation. *Proceedings of the ACM on Software Engineering* 1, FSE (2024), 1585–1608.
*   Yu et al. (2024) Zeliang Yu, Ming Wen, Xiaochen Guo, and Hai Jin. 2024. Maltracker: A Fine-Grained NPM Malware Tracker Copiloted by LLM-Enhanced Dataset. In *Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis*.
*   Zhang et al. (2023b) Biao Zhang, Barry Haddow, and Alexandra Birch. 2023b. Prompting large language model for machine translation: A case study. *arXiv preprint arXiv:2301.07069* (2023).
*   Zhang et al. (2023a) Quanjun Zhang, Chunrong Fang, Yuxiang Ma, Weisong Sun, and Zhenyu Chen. 2023a. A survey of learning-based automated program repair. *ACM Transactions on Software Engineering and Methodology* 33, 2 (2023), 1–69.
*   Zhang et al. (2023c) Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023c. Benchmarking large language models for news summarization. *arXiv preprint arXiv:2301.13848* (2023).
*   Zhang et al. (2024) Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. Autocoderover: Autonomous program improvement. *arXiv preprint arXiv:2404.05427* (2024).
*   Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. *arXiv preprint arXiv:2303.18223* (2023).
*   Zhu et al. (2024) Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. 2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. *arXiv preprint arXiv:2406.11931* (2024).
*   Zhu-Tian et al. (2024) Chen Zhu-Tian, Zeyu Xiong, Xiaoshuo Yao, and Elena Glassman. 2024. Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches. *arXiv preprint arXiv:2405.03998* (2024).