- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:42:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:42:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Richelieu: 自我进化的基于LLM的人工智能外交代理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.06813](https://ar5iv.labs.arxiv.org/html/2407.06813)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.06813](https://ar5iv.labs.arxiv.org/html/2407.06813)
- en: Zhenyu Guan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhenyu Guan
- en: Peking University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: gzyxxn@stu.pku.edu.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: gzyxxn@stu.pku.edu.cn
- en: '&Xiangyu Kong^(🖂)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Xiangyu Kong^(🖂)'
- en: Beijing Information Science and Technology University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 北京信息科技大学
- en: xykong@bistu.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: xykong@bistu.edu.cn
- en: Fangwei Zhong^(🖂)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Fangwei Zhong^(🖂)
- en: Peking University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: zfw@pku.edu.cn
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: zfw@pku.edu.cn
- en: '&Yizhou Wang'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yizhou Wang'
- en: Peking University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: yizhou.wang@pku.edu.cn
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: yizhou.wang@pku.edu.cn
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Diplomacy is one of the most sophisticated activities in human society. The
    complex interactions among multiple parties/ agents involve various abilities
    like social reasoning, negotiation arts, and long-term strategy planning. Previous
    AI agents surely have proved their capability of handling multi-step games and
    larger action spaces on tasks involving multiple agents. However, diplomacy involves
    a staggering magnitude of decision spaces, especially considering the negotiation
    stage required. Recently, LLM agents have shown their potential for extending
    the boundary of previous agents on a couple of applications, however, it is still
    not enough to handle a very long planning period in a complex multi-agent environment.
    Empowered with cutting-edge LLM technology, we make the first stab to explore
    AI’s upper bound towards a human-like agent for such a highly comprehensive multi-agent
    mission by combining three core and essential capabilities for stronger LLM-based
    societal agents: 1) strategic planner with memory and reflection; 2) goal-oriented
    negotiate with social reasoning; 3) augmenting memory by self-play games to self-evolving
    without any human in the loop.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 外交是人类社会中最复杂的活动之一。多个方/代理之间的复杂互动涉及社交推理、谈判艺术和长期战略规划等多种能力。之前的AI代理确实证明了它们在处理多步骤游戏和涉及多个代理的任务中的能力。然而，外交涉及到惊人的决策空间规模，尤其是考虑到所需的谈判阶段。最近，LLM代理在几个应用中展示了其扩展前代理边界的潜力，但在复杂的多代理环境中处理非常长的规划周期仍然不够。借助尖端的LLM技术，我们首次尝试通过结合三种核心和基本能力来探索人工智能在这种高度综合的多代理任务中的上限：1）具有记忆和反思的战略规划者；2）具有社交推理的目标导向谈判者；3）通过自我游戏增强记忆，实现自我进化而无需任何人工干预。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Diplomacy, as a cornerstone of international relations, is an intricate and
    multifaceted activity that lies at the heart of human society’s most complex interactions.
    It encompasses a wide array of skills and strategies, including social reasoning,
    negotiation, and long-term planning, to navigate the intricate web of relationships
    and alliances between multiple parties. Mirroring this complexity, the board game
    Diplomacy[[59](#bib.bib59)] involves seven players to control European powers,
    presenting a complex strategic challenge that requires both sophisticated negotiation
    and strategic planning to triumph.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 外交作为国际关系的基石，是一项错综复杂的活动，处于人类社会最复杂互动的核心。它涵盖了广泛的技能和策略，包括社交推理、谈判和长期规划，以驾驭多个方之间错综复杂的关系和联盟。与此复杂性相映，桌游《外交》[[59](#bib.bib59)]
    涉及七名玩家控制欧洲大国，提出了一个复杂的战略挑战，需要精巧的谈判和战略规划才能成功。
- en: 'The AI community has shown an increasing interest in the deployment of AI agents
    to master such games [[45](#bib.bib45), [26](#bib.bib26), [29](#bib.bib29), [15](#bib.bib15),
    [36](#bib.bib36), [28](#bib.bib28)]. The recent breakthrough [[6](#bib.bib6)]
    has turned into press diplomacy, which allows communication between players. However,
    the previous methods [[6](#bib.bib6)] heavily rely on domain-specific human data,
    leading to its poor generalization to other scenarios/ applications. The question
    then arises: Can we build an AI agent that excels in the art of diplomacy without
    relying on domain-specific human data?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能社区对部署AI代理以掌握此类游戏表现出越来越大的兴趣[[45](#bib.bib45), [26](#bib.bib26), [29](#bib.bib29),
    [15](#bib.bib15), [36](#bib.bib36), [28](#bib.bib28)]。最近的突破[[6](#bib.bib6)]转变为新闻外交，允许玩家之间的沟通。然而，以前的方法[[6](#bib.bib6)]严重依赖特定领域的人类数据，导致其在其他场景/应用中的泛化能力较差。那么问题来了：我们能否构建一个在外交艺术上表现出色的AI代理，而不依赖于特定领域的人类数据？
- en: Recently, agents based on the Large Language Model(LLM) have emerged as a promising
    development for AI agents. The previous applications on personal assistants [[32](#bib.bib32)],
    robotics [[10](#bib.bib10), [64](#bib.bib64)], and video games [[48](#bib.bib48)]
    have shown the surprising ability of LLM-based agents in communication and planning,
    benefiting from the emergent ability of common sense reasoning, in-context/ few-shot
    learning, and sophisticated natural language processing on LLMs. However, diplomacy
    presents a unique set of challenges. It not only requires planning long-horizon
    strategic [[40](#bib.bib40)] and communicating with natural language, but also
    reasoning and adopting the complex social dynamics with partial observations,
    including gaining trust and reputation, building rapport, detecting deception,
    and assessing the reliability of other players.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于大型语言模型（LLM）的代理已经成为AI代理的一个有前景的发展方向。之前在个人助理[[32](#bib.bib32)]、机器人[[10](#bib.bib10),
    [64](#bib.bib64)]和视频游戏[[48](#bib.bib48)]上的应用显示了LLM基于代理在沟通和规划方面的惊人能力，得益于LLM的常识推理、上下文/少量学习和复杂自然语言处理的涌现能力。然而，外交带来了独特的挑战。它不仅需要规划长期战略[[40](#bib.bib40)]和用自然语言进行沟通，还需要推理和采用复杂的社会动态，包括获得信任和声誉、建立关系、检测欺骗和评估其他参与者的可靠性。
- en: 'In this work, we aim to make the first attempt to explore LLMs’ potential to
    develop a human-like AI diplomacy agent. We name the agent Richelieu in memorizing
    a pivotal figure in European history who had enduring impacts on French politics,
    foreign affairs, and state building. To achieve this goal, we have identified
    three core and essential capabilities that are crucial for building an LLM-based
    societal agent: 1) Social reasoning This is the basic function for a social agent
    to interact with others, particularly for adapting to the dynamic changes in the
    nation’s intentions and relationships. 2) Balance long- and short-term planning
    Diplomacy often requires a delicate balance between short-term tactics and long-term
    strategies. An effective AI agent must be able to recognize and weigh the immediate
    consequences of its actions against the potential long-term outcomes. 3) Powerful
    memory A robust memory system is a critical component of learning and improvement.
    The AI agent must be able to recall and integrate information from past negotiations
    and actions to inform its current and future decision-making processes. This endows
    the agent with the ability to evolve. 4) Profound reflection An AI agent capable
    of profound reflection can analyze its own decisions, learn from its memory experience,
    and adapt its strategies accordingly. By integrating these three capabilities,
    the agent can operate at the highest level of diplomatic sophistication, outperforming
    the state-of-the-art AI diplomats [[6](#bib.bib6)].'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们旨在首次探索大型语言模型（LLMs）在发展类人AI外交代理方面的潜力。我们将该代理命名为**里谢留**，以纪念这位在欧洲历史上具有重要影响的关键人物，他对法国政治、外交事务和国家建设产生了深远影响。为了实现这一目标，我们确定了三个核心且至关重要的能力，这些能力对于构建基于LLM的社会代理至关重要：1)
    社会推理 这是社会代理与他人互动的基本功能，特别是适应国家意图和关系的动态变化。2) 平衡长期和短期规划 外交往往需要在短期战术和长期战略之间取得微妙的平衡。一个有效的AI代理必须能够认识并权衡其行动的即时后果与潜在的长期结果。3)
    强大的记忆系统 这是学习和改进的关键组成部分。AI代理必须能够回忆和整合过去谈判和行动中的信息，以指导当前和未来的决策过程。这使得代理具备了进化的能力。4)
    深刻反思 一个能够深刻反思的AI代理可以分析自身的决策，从记忆经验中学习，并相应地调整其策略。通过整合这三种能力，该代理可以在外交复杂性方面达到最高水平，超越最先进的AI外交官[[6](#bib.bib6)]。
- en: 'Our contributions can be summarized in three-fold: 1) We introduced a new paradigm
    for building AI diplomacy agents, compared to previous work (Fig. [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy")).
    The agent can self-evolve by generating experience via self-play games, without
    any task-specific human data. 2) We demonstrate the superior performance of our
    agent playing against the SOTA method, e.g., Cicero [[6](#bib.bib6)], that relies
    on a large-scale human demonstration for training. 3) We further analyze the effectiveness
    of each module in our agent and the generalization of our agent in adopting different
    LLMs, such as [GPT4.0](https://openai.com/index/gpt-4/) and [Llamma 3](https://llama.meta.com/llama3).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的贡献可以总结为三点：1) 我们引入了一种新的AI外交代理人构建范式，相较于之前的工作（图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ Richelieu:
    自我进化的基于LLM的AI外交代理人")）。该代理人可以通过自我对弈生成经验，自我进化，无需任何任务特定的人类数据。2) 我们展示了我们的代理人在对抗SOTA方法（例如
    Cicero [[6](#bib.bib6)]）时的优越性能，该方法依赖于大规模的人类示范进行训练。3) 我们进一步分析了代理人每个模块的有效性以及代理人在采用不同LLM（如
    [GPT4.0](https://openai.com/index/gpt-4/) 和 [Llamma 3](https://llama.meta.com/llama3)）时的泛化能力。'
- en: '![Refer to caption](img/d38077694619c7a6df46fb45a80df691.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d38077694619c7a6df46fb45a80df691.png)'
- en: 'Figure 1: A new paradigm for building AI Diplomacy agent.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：构建AI外交代理人的新范式。
- en: 2 Related work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: AI Diplomacy. The game involves seven players controlling different powers in
    Europe. In each turn, players can negotiate for cooperation before making moves
    to take as much supply centers as they can. Apparently, this challenging strategy
    task requires both complex negotiation skills and superior planning capability
    for player agents to achieve final victory. So far, most AI research on this task
    remain focused on the planning strategies (a.k.a. No-Press Diplomacy where no
    communication channels are allowed). The setting remains challenging considering
    its enormous action space of $10^{2}1$ per turn (compared with Chess, which has
    much fewer than 100 actions per turn). No wonder existing efforts rely on human
    data to play the game. Among the methods, one typical research is DipNet [[39](#bib.bib39)]
    which uses supervised and reinforcement learning. Based on DipNet, BRPI [[3](#bib.bib3)],
    SearchBot [[18](#bib.bib18)], DORA [[5](#bib.bib5)], and KL-Regularized search
    (Diplodocus) [[24](#bib.bib24)] were conducted. Until very recently, research
    has also emerged for the full-setting of Diplomacy, or Press Diplomacy where players
    are allowed to communicate with each before making their moves in each turn. Such
    studies [[13](#bib.bib13)][[6](#bib.bib6)][[25](#bib.bib25)][[29](#bib.bib29)]
    mainly benefit from the recent thriving language models. Specifically, notable
    advancements include policy iteration methods from DeepMind and Facebook AI Research’s
    equilibrium search agent [[25](#bib.bib25)]. However, Deepmind propose to learn
    negotiation agents based on predefined contracts/protocols [[29](#bib.bib29)].
    And Meta AI’s work, instead of one unified architecture, Cicero [[6](#bib.bib6)]
    integrates a language model for negotiation and an RL model for planning respectively.
    Such separately trained models make it inconvenient for agents’ continual evolution.
    What’s more, like no-press methods, these approaches heavily rely on human player
    data for agent training. Unlike these approaches, this paper delves into solving
    the negotiation and planning in one single self-evolving LLM-based agent model,
    without any pre-collected human expert training data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AI外交。这款游戏涉及七名玩家控制欧洲的不同势力。在每一回合，玩家可以在进行移动之前进行合作谈判，争取尽可能多的供应中心。显然，这项具有挑战性的策略任务要求玩家具备复杂的谈判技能和卓越的规划能力，以实现最终胜利。到目前为止，大多数关于这项任务的AI研究仍然集中在规划策略（也称为无沟通外交，其中不允许任何沟通渠道）。考虑到其每回合的巨大动作空间$10^{2}1$（相比于棋类游戏，后者每回合的动作远少于100个），这个设置依然具有挑战性。难怪现有的努力依赖于人类数据来进行游戏。在这些方法中，一个典型的研究是DipNet
    [[39](#bib.bib39)]，它使用了监督学习和强化学习。基于DipNet，BRPI [[3](#bib.bib3)]、SearchBot [[18](#bib.bib18)]、DORA
    [[5](#bib.bib5)]和KL正则化搜索（Diplodocus）[[24](#bib.bib24)]也相继进行了研究。直到最近，研究者们也开始关注Diplomacy的完整设置，或称为Press
    Diplomacy，其中玩家在每回合进行移动之前允许相互沟通。这类研究[[13](#bib.bib13)][[6](#bib.bib6)][[25](#bib.bib25)][[29](#bib.bib29)]主要受益于最近兴起的语言模型。具体而言，显著的进展包括来自DeepMind的策略迭代方法和Facebook
    AI Research的均衡搜索代理[[25](#bib.bib25)]。然而，DeepMind建议基于预定义合同/协议来学习谈判代理[[29](#bib.bib29)]。而Meta
    AI的工作，Cicero [[6](#bib.bib6)]，则分别集成了用于谈判的语言模型和用于规划的RL模型。这些分别训练的模型使得代理的持续进化变得不便。此外，像无沟通方法一样，这些方法在代理训练中也严重依赖人类玩家数据。与这些方法不同，本文深入探讨了如何在一个自我进化的LLM基础的代理模型中同时解决谈判和规划问题，而无需任何预先收集的人类专家训练数据。
- en: LLM-based Agents. With the emergence and growth of large language models (LLM),
    there is a growing trend in utilizing LLMs as fundamental controllers for autonomous
    agents[[52](#bib.bib52)]. One wide application genre is LLM-based answering engines,
    which merely cover the negotiation aspects of Diplomacy. Such systems include
    HuggingGPT [[44](#bib.bib44)], GPT4Tools [[63](#bib.bib63)] and ToT [[65](#bib.bib65)],
    etc. They leverage LLMs to manage Al models, use tools, implement policy iteration,
    and enhance problem-solving across various tasks. Related work including AutoGPT,
    AgentGPT, BabyAGl [[47](#bib.bib47)], Toolformer [[43](#bib.bib43)], and Visual
    ChatGPT aim to improve LLM capabilities in task automation and tool usage. And
    Reflexion, a framework that improves LLMs through linguistic feedback and episodic
    memory [[68](#bib.bib68)], facilitating better decision-making across diverse
    tasks is proposed. Besides [[55](#bib.bib55)][[50](#bib.bib50)][[56](#bib.bib56)][[76](#bib.bib76)][[62](#bib.bib62)]
    apply LLM agents to the complex planning tasks in the well-known open-world game
    Minecraft[[16](#bib.bib16)]. Unlike these LLM-based agents which only focus on
    the negotiation/planning aspect, the proposed approach involves multiple self-evolving
    schemes to handle both of them simultaneously.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LLM 的代理。随着大语言模型（LLM）的出现和发展，利用 LLM 作为自主代理的基本控制器的趋势越来越明显[[52](#bib.bib52)]。其中一个广泛的应用领域是基于
    LLM 的回答引擎，这仅仅涵盖了《外交》游戏中的谈判方面。这些系统包括 HuggingGPT [[44](#bib.bib44)]、GPT4Tools [[63](#bib.bib63)]
    和 ToT [[65](#bib.bib65)] 等。它们利用 LLM 来管理 AI 模型，使用工具，实现政策迭代，并在各种任务中提高问题解决能力。相关工作包括
    AutoGPT、AgentGPT、BabyAGl [[47](#bib.bib47)]、Toolformer [[43](#bib.bib43)] 和 Visual
    ChatGPT，旨在提高 LLM 在任务自动化和工具使用中的能力。此外，还提出了 Reflexion，一个通过语言反馈和情节记忆[[68](#bib.bib68)]
    改善 LLM 的框架，促进在各种任务中的更好决策。除此之外[[55](#bib.bib55)][[50](#bib.bib50)][[56](#bib.bib56)][[76](#bib.bib76)][[62](#bib.bib62)]
    还将 LLM 代理应用于知名开放世界游戏 Minecraft[[16](#bib.bib16)] 中的复杂规划任务。与这些只关注谈判/规划方面的 LLM 代理不同，所提议的方法涉及多种自我进化方案，以同时处理这两方面。
- en: 3 Problem Statement
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题陈述
- en: The Diplomacy game [[59](#bib.bib59), [8](#bib.bib8)] is set in pre-World War
    I Europe and involves each player (agent) representing one of the seven Great
    Powers of Europe, such as Germany, France, England, Italy, Austria-Hungary, Russia,
    and Turkey. Each player has a set of military units, including armies and fleets,
    which they can move and use to capture other supply centers. The ultimate goal
    for the agent is to control a majority of the total supply centers on the board
    by the end of the game’s Fall phase. It’s important to note that it is not won
    by eliminating other players or their units; it is won by controlling the requisite
    number of supply centers. This often involves forming and breaking alliances,
    negotiating, and sometimes betraying other players to achieve one’s own goals.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 《外交》游戏 [[59](#bib.bib59), [8](#bib.bib8)] 设定在第一次世界大战前的欧洲，每个玩家（代理）代表欧洲的七大强国之一，如德国、法国、英格兰、意大利、奥匈帝国、俄罗斯和土耳其。每个玩家拥有一组军事单位，包括陆军和海军，他们可以移动这些单位并利用其占领其他补给中心。代理的最终目标是通过游戏秋季阶段结束时控制棋盘上大多数的补给中心。重要的是要注意，游戏的胜利不是通过消灭其他玩家或他们的单位来获得的，而是通过控制所需数量的补给中心来获得的。这通常涉及形成和打破联盟、谈判以及有时背叛其他玩家以实现自己的目标。
- en: In each turn,[[58](#bib.bib58), [53](#bib.bib53)] the agent $i$ are commands
    to the armies, such as moving into an adjacent territory, supporting another unit,
    or holding a position. Actions can also include diplomatic moves, such as proposing
    or withdrawing from an alliance, although these are less formalized in the game
    mechanics.[[39](#bib.bib39), [22](#bib.bib22)]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个回合中[[58](#bib.bib58), [53](#bib.bib53)]，代理 $i$ 会对军队发出命令，如进入邻近领土、支持另一个单位或保持阵地。行动还可以包括外交举措，例如提议或退出联盟，尽管这些在游戏机制中不够正式[[39](#bib.bib39),
    [22](#bib.bib22)]。
- en: 4 Self-Evolving LLM-based Diplomat
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 自我进化的基于 LLM 的外交官
- en: '![Refer to caption](img/2679d765634ba03dec7aed3f3aa53b84.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2679d765634ba03dec7aed3f3aa53b84.png)'
- en: 'Figure 2: The framework of the proposed LLM-based-agent, Richelieu. The agent
    can explicitly reason social beliefs, propose sub-goals with reflection, negotiate
    with others, and take actions to master diplomacy. It augments memories by self-play
    games for self-evolving without any human annotation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：所提议的基于 LLM 的代理 Richelieu 的框架。代理可以明确地推理社会信念，通过反思提出子目标，与他人进行谈判，并采取行动来掌握外交。它通过自我对弈游戏增强记忆，实现自我进化，无需任何人工注释。
- en: 'We have constructed a comprehensive framework with modules for memory management,
    social reasoning, strategic planning, negotiation, decision-making, memory update,
    and self-evolving to fully leverage the capabilities of LLMs. Richelieu starts
    by setting up with map details, game rules, domain knowledge, and the long-term
    goal.[[74](#bib.bib74), [58](#bib.bib58), [53](#bib.bib53)] At each turn, the
    agent will run in the following steps: 1) Social Reasoning: First of all, the
    agent undergoes a comprehensive analysis of the game state $s_{t}$. This logged
    data serves as a historical experience, guiding Richelieu’s subsequent actions
    in future turns [[20](#bib.bib20), [73](#bib.bib73)]. 6) Self-evolution: The agent’s
    evolution is highly dependent on the diversity of experiences stored in its memory.
    As this diversity grows, so does the agent’s capability. Without human demonstrations,
    we employ multi-agent self-play games, i.e., our agents respectively control all
    the countries to simulate and acquire diverse experiences for self-evolving. Notably,
    the agent can further evolve during testing to adapt to different players.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了一个全面的框架，包含内存管理、社会推理、战略规划、谈判、决策、记忆更新和自我进化等模块，以充分利用大语言模型（LLMs）的能力。Richelieu
    首先设置地图细节、游戏规则、领域知识和长期目标[[74](#bib.bib74), [58](#bib.bib58), [53](#bib.bib53)]。在每一轮中，代理将执行以下步骤：1）社会推理：首先，代理对游戏状态
    $s_{t}$ 进行全面分析。这些记录的数据作为历史经验，指导 Richelieu 在未来回合的后续行动[[20](#bib.bib20), [73](#bib.bib73)]。6）自我进化：代理的进化高度依赖于其内存中存储的经验多样性。随着这种多样性的增加，代理的能力也随之增强。在没有人工示范的情况下，我们采用多代理自我对弈游戏，即我们的代理分别控制所有国家，以模拟和获取多样的经验以进行自我进化。值得注意的是，代理在测试过程中还可以进一步进化，以适应不同的玩家。
- en: 4.1 Social Reasoning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 社会推理
- en: There are no permanent enemies, no permanent allies. The relationship among
    countries is dynamically changing upon the evolving global state. However, it
    is difficult to determine the appropriate allies and enemies with partial observation.
    For example, there is uncertainty about the intentions of potential allies, which
    could lead to betrayal at pivotal moments. Consequently, we need to identify the
    intention and relationship of the current state by social reasoning to shape the
    social belief [[71](#bib.bib71), [19](#bib.bib19)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 没有永远的敌人，也没有永远的盟友。国家之间的关系会根据全球状态的变化而动态变化。然而，凭借部分观察很难确定合适的盟友和敌人。例如，对于潜在盟友的意图存在不确定性，这可能导致在关键时刻的背叛。因此，我们需要通过社会推理识别当前状态的意图和关系，以形成社会信念[[71](#bib.bib71),
    [19](#bib.bib19)]。
- en: '1) Modeling Relationship: Before setting sub-goals, Richelieu evaluates its
    relations with other players, identifying enemies such as aggressive nations,
    vulnerable neighbors for expansion, and those with long-term potential threats.
    It also seeks out potential allies to counter these threats.[[46](#bib.bib46),
    [72](#bib.bib72)] Simultaneously, Richelieu also tries to identify potential allies
    that could be instrumental in countering these adversaries. By isolating the analysis
    of inter-player relationships as a discrete element, Richelieu strategically exploits
    the actions of other players in subsequent stages of the game to reach its goals.
    2) Inferring Intention: The social belief is also used by the planner, ensuring
    that Richelieu’s sub-goals are formulated with a comprehensive consideration of
    the behaviors and intentions of other intelligent agents within the game.[[14](#bib.bib14),
    [21](#bib.bib21)] Richelieu’s sub-goals will particularly emphasize on those who
    are identified as potential adversaries or allies, fostering more effective collaboration
    with potential allies and participation in strategic opposition against adversaries.
    Furthermore, the insights gleaned from this analysis are instrumental in the subsequent
    negotiation phases. They are employed to assess the authenticity of the statements
    made by other players, as well as to aid Richelieu in reaching cooperative agreements.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 建模关系：在设定子目标之前，Richelieu 评估其与其他玩家的关系，识别出敌人，如侵略性国家、适合扩张的脆弱邻国以及具有长期潜在威胁的国家。它还寻求可能的盟友以对抗这些威胁。[[46](#bib.bib46),
    [72](#bib.bib72)]与此同时，Richelieu 还尝试识别可能在对抗这些对手中发挥重要作用的潜在盟友。通过将玩家间关系的分析作为一个独立元素，Richelieu
    在游戏的后续阶段战略性地利用其他玩家的行动以实现其目标。2) 推断意图：规划者还使用社会信念，确保Richelieu 的子目标是在全面考虑游戏中其他智能体的行为和意图的基础上制定的。[[14](#bib.bib14),
    [21](#bib.bib21)]Richelieu 的子目标将特别关注那些被识别为潜在对手或盟友的对象，以促进与潜在盟友的更有效合作，并参与对抗对手的战略
    opposition。此外，从这一分析中获得的见解在随后的谈判阶段中起着关键作用。它们用于评估其他玩家所做陈述的真实性，并帮助Richelieu 达成合作协议。
- en: 4.2 Strategic Planner with Reflection
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 具有反思的战略规划者
- en: The strategic planner specifies the sub-goals, which serves as an intermediary
    between immediate actions and the overarching goal of securing victory in the
    game. That is because we observe that LLMs are often characterized by their propensity
    to prioritize short-term gains in decision-making processes, with a notable deficiency
    in incorporating the future into their strategic calculations. [[41](#bib.bib41),
    [70](#bib.bib70)]For example, it is common for a non-neighboring country to become
    too powerful. Formally, $\vec{\chi_{t}}\leftarrow SR(s_{t},\vec{\phi_{t}},\Upsilon)$
    represents the inferred relationship on the social belief. These goals may encompass
    a range of tactical considerations, such as the containment of a formidable rival’s
    advancement or the strategic expansion in a particular direction to consolidate
    power.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 战略规划者指定子目标，作为立即行动和赢得游戏的总体目标之间的中介。这是因为我们观察到，LLMs往往优先考虑短期收益，决策过程中缺乏将未来纳入战略计算的能力。[[41](#bib.bib41),
    [70](#bib.bib70)]例如，非邻国变得过于强大是常见的现象。形式上，$\vec{\chi_{t}}\leftarrow SR(s_{t},\vec{\phi_{t}},\Upsilon)$
    表示社会信念中的推断关系。这些目标可能包括一系列战术考虑，例如遏制强大对手的进攻或在特定方向上进行战略扩张以巩固力量。
- en: 'Reflection with Memory. We further develop a reflection mechanism to enhance
    the rationality and effectiveness of our agent’s sub-goals in achieving long-term
    goals.[[33](#bib.bib33)] This reflection mechanism relies on the past experiences
    to critique and enhance proposed sub-goals. We employ a similarity-based function
    to find relevant historical experiences that match the current game state from
    its memory. This function considers two factors: goal similarity and state similarity,
    to select the most comparable experiences. The process can be written as: $\vec{\eta_{t}}\leftarrow
    h(s_{t},\chi^{i}_{t},M)$. In practice, considering the limited context windows
    of LLM, we retrieve the most analogous experiences from the memory based on these
    metrics. Experiences with high evaluative scores reinforce successful strategies
    and support the continuity of existing sub-goals. On the other hand, lower scores
    indicate areas that need improvement and prompt the necessary adjustments. As
    our agent, Richelieu, undergoes more training sessions, its reflection abilities
    improve. The growing pool of historical experiences consistently enhances its
    performance.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 反思与记忆。我们进一步开发了一种反思机制，以提高代理人在实现长期目标过程中的理性和有效性[[33](#bib.bib33)]。该反思机制依赖于过去的经验来批判和增强提出的子目标。我们采用基于相似性的函数，从记忆中找到与当前游戏状态匹配的相关历史经验。该函数考虑两个因素：目标相似性和状态相似性，以选择最可比的经验。该过程可以表示为：$\vec{\eta_{t}}\leftarrow
    h(s_{t},\chi^{i}_{t},M)$。在实际操作中，考虑到大型语言模型（LLM）的上下文窗口有限，我们根据这些指标从记忆中检索最类似的经验。具有高评估分数的经验强化成功的策略，并支持现有子目标的延续。另一方面，较低的分数表示需要改进的领域，并促使必要的调整。随着我们的代理人Richelieu接受更多训练，其反思能力不断提高。不断增长的历史经验库持续提升其表现。
- en: 4.3 Negotiator and Actor
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 谈判者与演员
- en: '![Refer to caption](img/39a6b99d7dde91bef217a7f848f361d2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/39a6b99d7dde91bef217a7f848f361d2.png)'
- en: 'Figure 3: The social reasoning flow for negotiation. With the received words
    and memory, the agent will reason by answering the following questions: “Is the
    opponent lying?", “What is the true intention of the opponent?", “is the opponent
    enemy?", “Is it necessary to deceive the opponent?", and “Is it necessary to change
    the relationship with the opponent?", and then generate the words accordingly
    for negotiation.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：谈判的社会推理流程。根据收到的话语和记忆，代理人将通过回答以下问题进行推理：“对手在撒谎吗？”，“对手的真实意图是什么？”，“对手是敌人吗？”，“是否有必要欺骗对手？”，以及“是否有必要改变与对手的关系？”，然后相应地生成谈判的话语。
- en: By chatting with other players, the goal of the negotiation is to update the
    social belief according to the received words and reach the sub-goal by manipulating
    other’s intentions, such as securing cooperative agreements with other nations,
    terminating ongoing conflicts with a specific country, or deterring the formation
    of alliances directed against its interests.[[37](#bib.bib37), [67](#bib.bib67)]
    However, it is difficult to reach a consensus, as the interests and strategies
    of the various nations often conflict, and trust between players can be scarce,
    making it challenging to establish and maintain cooperative agreements. In this
    case, we argue that the negotiator should identify the true intentions and relationship
    of the opponent before generating the words for the negotiation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他玩家聊天时，谈判的目标是根据收到的话语更新社会信念，通过操控他人的意图来实现子目标，例如，与其他国家达成合作协议、结束与特定国家的冲突或阻止针对自身利益的联盟的形成[[37](#bib.bib37),
    [67](#bib.bib67)]。然而，达成共识很困难，因为各国的利益和策略通常存在冲突，玩家之间的信任也可能稀缺，这使得建立和维护合作协议变得具有挑战性。在这种情况下，我们认为谈判者应该在生成谈判话语之前识别对手的真实意图和关系。
- en: 'To fully utilize the power of LLMs, we construct a social reasoning flow for
    negotiation, as shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Negotiator and Actor
    ‣ 4 Self-Evolving LLM-based Diplomat ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy"). During the negotiation process, we guide Richelieu to consider
    the veracity of what other players said and their true intentions, and in conjunction
    with our established sub-goals and analysis of our relationships with other players,
    to negotiate and form alliances with potential allies and attempt to deceive enemies.[[60](#bib.bib60),
    [35](#bib.bib35)]'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '为了充分利用LLMs的力量，我们构建了一个社交推理流程用于谈判，如图[3](#S4.F3 "Figure 3 ‣ 4.3 Negotiator and
    Actor ‣ 4 Self-Evolving LLM-based Diplomat ‣ Richelieu: Self-Evolving LLM-Based
    Agents for AI Diplomacy")所示。在谈判过程中，我们引导Richelieu考虑其他玩家所说的真实性及其真正意图，并结合我们建立的子目标和对其他玩家关系的分析，与潜在盟友谈判并建立联盟，同时尝试欺骗敌人。[
    [60](#bib.bib60), [35](#bib.bib35) ]'
- en: To counteract the challenge of non-binding agreements and potential deception,
    we incorporate a discrete module dedicated to the assessment of the veracity of
    statements made by other players during negotiations. To determine the truthiness
    of other players’ statements $\psi^{j}_{t}$. With such a reasoning flow, our agent
    can adeptly navigate diplomatic discourse. After the negotiation, the actor will
    get the updated social beliefs and choose a specific action for the army.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对无约束协议和潜在欺骗的挑战，我们引入了一个离散模块，专门用于评估其他玩家在谈判过程中所说话语的真实性。为了确定其他玩家陈述的真实度$\psi^{j}_{t}$。有了这样的推理流程，我们的代理可以熟练地进行外交讨论。谈判结束后，演员将获得更新的社会信念，并为军队选择具体行动。
- en: 4.4 Memory Management and Self-Evolution
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 记忆管理与自我进化
- en: This memory is the foundation of the framework that accumulates the historical
    experience of the agent and summarizes them for other modules.[[17](#bib.bib17),
    [30](#bib.bib30), [66](#bib.bib66), [23](#bib.bib23)] It supports other modules,
    such as planner and negotiator, to provide long-tail experiences.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个记忆是框架的基础，它积累了代理的历史经验并将其总结给其他模块。[ [17](#bib.bib17), [30](#bib.bib30), [66](#bib.bib66),
    [23](#bib.bib23) ] 它支持其他模块，如计划者和谈判者，提供长尾经验。
- en: Raw Experience Management. Specifically, the memory module is tasked with the
    acquisition and archival of historical data, encompassing the observed game state
    $s_{t}$, and then is incorporated into memory.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 原始经验管理。具体来说，记忆模块负责获取和归档历史数据，包括观察到的游戏状态$s_{t}$，然后将其纳入记忆中。
- en: 'Acquisition Experience via Self-Play Games. Self-play allows the agent to accumulate
    more experiences for self-evolution.[[33](#bib.bib33), [69](#bib.bib69)] After
    training, when Richelieu is faced with a certain state, it can draw on a larger
    pool of similar historical experiences. Diverse evaluations enable Richelieu to
    reflect more comprehensively on the strategies it currently devises, leading to
    a stronger optimization of decision making. As self-play continues, the acquisition
    of new and better historical experiences by Richelieu will diminish. This means
    that Richelieu’s capabilities will not improve indefinitely. At the same time,
    as the memory grows, selecting appropriate historical experiences becomes a new
    challenge. The chosen m experience $\vec{\eta_{t}}$ may be almost identical, which
    could actually reduce the amount of useful information available to Richelieu.
    As shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy"), Richelieu’s performance against
    Cicero [[6](#bib.bib6)] becomes better with increasing training iterations. With
    the accumulation of experiences, Richelieu’s win rate exhibited a steady increase
    with accumulated training iterations, ultimately plateauing at a stable performance
    level. In contrast, the defeated rate showed a consistent decrease, approaching
    an asymptotic value. These observations confirm the effectiveness of self-play
    in Richelieu’s evolution.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '通过自我对弈游戏获得经验。自我对弈允许代理积累更多经验以进行自我演进。[[33](#bib.bib33), [69](#bib.bib69)] 训练后，当Richelieu面临某种状态时，它可以借鉴更大的类似历史经验池。多样化的评估使Richelieu能更全面地反思其当前制定的策略，从而更强有力地优化决策。随着自我对弈的继续，Richelieu获得新的、更好的历史经验的能力将减弱。这意味着Richelieu的能力不会无限提高。同时，随着记忆的增长，选择适当的历史经验成为新的挑战。所选的m经验$\vec{\eta_{t}}$可能几乎相同，这实际上可能减少Richelieu获得有用信息的数量。如图[5](#S5.F5
    "Figure 5 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy")所示，Richelieu在与Cicero [[6](#bib.bib6)]对战时的表现随着训练迭代次数的增加而变得更好。随着经验的积累，Richelieu的胜率在积累训练迭代次数后稳步上升，最终稳定在一个稳定的表现水平。相比之下，失败率则持续下降，接近渐近值。这些观察结果确认了自我对弈在Richelieu演进中的有效性。'
- en: 5 Experiment
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'In the experiments, our goal is to answer the following questions: 1) Mastery
    of Non-Press Diplomacy: Can our agent master the non-press diplomacy against baselines?
    2) Competing with State-of-the-Art: Can our agent surpass the performance of the
    current state-of-the-art agents in press diplomacy? 3) Compatibility with LLMs:
    Can our self-evolving framework be compatible with different LLMs? 4) Contribution
    of Framework Modules: Do the individual modules within our framework contribute
    to the overall improvement of our agent’s performance?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们的目标是回答以下问题：1) 非新闻外交的掌握：我们的代理能否掌握与基线的非新闻外交？2) 与最先进技术的竞争：我们的代理能否超越当前最先进代理在新闻外交中的表现？3)
    与LLM的兼容性：我们的自我演进框架是否能与不同的LLM兼容？4) 框架模块的贡献：框架中的各个模块是否有助于整体提升代理的表现？
- en: 5.1 Experimental Setup
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: Environment. The widely-used open source Diplomacy game platform introduced
    by [[39](#bib.bib39)] is adopted for evaluating Richelieu against other models.
    It is easy to switch between no-press (with communication/negotiation between
    players) and press (no communication between players) games based on this platform,
    facilitating comparison on both settings. The platform also contains over 10,000
    human game data on which previous approaches are trained. Note that our method
    does not need them. In each game, a model will play the role of one randomly selected
    country to compete against countries controlled by other methods. It wins if occupying
    all the supply centers and loses vice versa.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 环境。广泛使用的开源外交游戏平台，由[[39](#bib.bib39)]介绍，用于评估Richelieu与其他模型的对比。基于该平台，容易在无新闻（玩家之间有沟通/谈判）和有新闻（玩家之间没有沟通）游戏之间切换，便于在这两种设置下进行比较。该平台还包含了超过10,000条人类游戏数据，之前的方法是基于这些数据进行训练的。请注意，我们的方法不需要这些数据。在每场游戏中，一个模型将扮演一个随机选择的国家，与其他方法控制的国家竞争。占领所有供应中心则获胜，否则失败。
- en: 'Evaluation Metrics. We evaluate the models based on the results of multiple
    rounds of games. In each round, the model is randomly assigned a country to control.
    Typically, 1000 rounds are played to obtain the average results. We evaluate the
    models in two metrics. One is based on the win rate, Most SC rate, survived rate,
    and defeated rate. There are four possible outcomes for each country in the game.
    If a country loses all its supply centers (SC), it is eliminated and recorded
    as “defeated". If a country occupies 18 or more out of 34 supply centers, the
    game ends, and that country is recorded as “win", while other countries are recorded
    as “defeated". In other cases, the game ends in a draw. The country with the most
    supply centers is recorded as “Most SC", the countries that have been eliminated
    are recorded as “defeated", and the other countries are recorded as “Survived".
    The other is based on the scores obtained by the models after multiple rounds
    of competition. To compare the capabilities of multiple models, we use C-Diplo
    Argir[[4](#bib.bib4)], a scoring system. This system is used in many international
    diplomacy competitions. The scoring method is as follows: If a player wins by
    occupying 18 or more supply centers, the player scores 93 points, and each of
    the other six players scores 1 point. If the game ends in a draw, the player with
    the most centers scores 37 points. The second player with the most centers scores
    14 points. The third player with the most centers scores 7 points. Each player
    scores 1 point per center owned. Each player also scores 1 point for participating.
    In this way, regardless of the game outcome, a total of 99 points will be distributed
    among the players in each game.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们根据多轮游戏的结果来评估模型。在每轮中，模型被随机分配一个国家进行控制。通常进行 1000 轮以获得平均结果。我们从两个指标来评估模型。一个是基于胜率、最多供应中心（SC）率、存活率和被击败率。游戏中每个国家有四种可能的结果。如果一个国家失去了所有的供应中心（SC），它将被淘汰并记录为“被击败”。如果一个国家占据了
    34 个供应中心中的 18 个或更多，游戏结束，该国被记录为“胜利”，而其他国家被记录为“被击败”。在其他情况下，游戏以平局结束。拥有最多供应中心的国家被记录为“最多
    SC”，被淘汰的国家被记录为“被击败”，其他国家被记录为“存活”。另一个指标是基于模型在多轮竞争中获得的分数。为了比较多个模型的能力，我们使用了 C-Diplo
    Argir[[4](#bib.bib4)]，这是一个在许多国际外交比赛中使用的评分系统。评分方法如下：如果一个玩家通过占领 18 个或更多供应中心获胜，该玩家得
    93 分，其他六个玩家每人得 1 分。如果游戏以平局结束，拥有最多中心的玩家得 37 分。第二多中心的玩家得 14 分。第三多中心的玩家得 7 分。每个玩家每拥有一个中心得
    1 分。每个玩家也因参与比赛得 1 分。这样，无论游戏结果如何，每场游戏中的玩家总共会分配 99 分。
- en: Baselines. We select six previous models as baselines for comparison. Among
    them, Cicero[[6](#bib.bib6)] by Meta is a diplomacy model with a negotiation module.
    The other five are no-press diplomacy models, including the SL-DipNet and RL-DipNet
    [[39](#bib.bib39)], the BRPI [[3](#bib.bib3)], the SearchBot [[18](#bib.bib18)],
    and the DORA[[5](#bib.bib5)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基线模型。我们选择了六个先前的模型作为比较基线。其中，Meta 的 Cicero[[6](#bib.bib6)] 是一个具有谈判模块的外交模型。其他五个是无交流外交模型，包括
    SL-DipNet 和 RL-DipNet [[39](#bib.bib39)]、BRPI [[3](#bib.bib3)]、SearchBot [[18](#bib.bib18)]
    和 DORA[[5](#bib.bib5)]。
- en: 5.2 Results
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果
- en: 'Massively Play with Baselines on no-press setting. We let Richelieu compete
    with the other six models including Cicero[[6](#bib.bib6)], SL-DipNet and RL-DipNet
    [[39](#bib.bib39)], BRPI [[3](#bib.bib3)], SearchBot [[18](#bib.bib18)], and DORA[[5](#bib.bib5)]
    on No-Press Diplomacy, in which players make moves without communication. Figure
     [4](#S5.F4 "Figure 4 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy") indicates that Richelieu outperforms other
    previous models relying on human game data. In contrast, Richelieu does not need
    such data but outperforms these methods by a clear margin, which demonstrates
    the outstanding planning capability of Richelieu.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在无交流设置下的大规模游戏。我们让 Richelieu 与包括 Cicero[[6](#bib.bib6)]、SL-DipNet 和 RL-DipNet
    [[39](#bib.bib39)]、BRPI [[3](#bib.bib3)]、SearchBot [[18](#bib.bib18)] 和 DORA[[5](#bib.bib5)]
    在内的其他六个模型在无交流外交中进行对抗，其中玩家在没有沟通的情况下进行行动。图 [4](#S5.F4 "Figure 4 ‣ 5.2 Results ‣
    5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy") 表明，Richelieu
    超越了依赖于人类游戏数据的其他先前模型。相比之下，Richelieu 不需要这些数据却显著超越了这些方法，这展示了 Richelieu 的卓越规划能力。'
- en: 'Play against Cicero on press setting. We also evaluate Richelieu through competition
    against Cicero in the challenging scenario where negotiation is enabled. Specifically,
    we randomly assign three countries to one model and the remaining four to another.
    After playing several rounds of the game, the win rate, most SC rate, survived
    rate, and the defeated rate is calculated using a weighted average for evaluation.
    Table  [1](#S5.T1 "Table 1 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy") demonstrates the competitive performance of
    Richelieu in comparison to Cicero. Richelieu’s win rate is approximately 0.7%
    higher than Cicero’s. If the Most SC rate is also taken into account, Richelieu
    is about 2% higher than Cicero. At the same time, Richelieu’s loss rate is also
    0.6% lower. According to our scoring system, Richelieu’s score is about 10% higher
    than Cicero’s. This is nontrivial especially when Richelieu is trained in a self-play
    game without humans and the opponents are trained with the data from human players.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '在新闻设置下与 Cicero 对局。我们还通过在启用谈判的挑战场景中与 Cicero 竞争来评估 Richelieu。具体而言，我们将三个国家随机分配给一个模型，将剩下的四个国家分配给另一个模型。在进行几轮游戏后，使用加权平均计算获胜率、最高
    SC 率、生存率和失败率进行评估。表 [1](#S5.T1 "Table 1 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy") 展示了 Richelieu 相对于 Cicero 的竞争性能。Richelieu
    的获胜率比 Cicero 高约 0.7%。如果同时考虑最高 SC 率，Richelieu 约比 Cicero 高 2%。与此同时，Richelieu 的失败率也低
    0.6%。根据我们的评分系统，Richelieu 的得分比 Cicero 高约 10%。这尤其值得注意，因为 Richelieu 是在没有人类参与的自对弈游戏中训练的，而对手则是用来自人类玩家的数据进行训练的。'
- en: 'Table 1: The results of our method playing against Cicero.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们的方法与 Cicero 对局的结果。
- en: Model Win$\uparrow$ Richelieu_1 6.20% 9.40% 38.90% 45.50% Richelieu_1 6.30%
    7.90% 39.40% 46.40% Richelieu_2 6.60% 7.80% 40.80% 44.80% Richelieu_2 6.60% 8.30%
    41.20% 43.90% Richelieu_3 7.10% 9.30% 39.90% 43.70% Richelieu_3 7.20% 8.70% 41.70%
    42.40% Richelieu_4 7.40% 8.00% 40.20% 44.40% Cicero_1 5.80% 6.70% 41.20% 46.30%
    Cicero_1 5.90% 6.50% 41.50% 46.10% Cicero_2 6.50% 7.20% 42.50% 43.80% Cicero_2
    6.30% 7.20% 42.50% 44.00% Cicero_3 6.00% 7.00% 41.60% 45.40% Cicero_3 5.90% 7.00%
    41.60% 45.50% Cicero_4 6.10% 7.20% 42.30% 44.40% Richelieu 6.83% 8.63% 39.95%
    44.60% Richelieu 6.70% 8.30% 40.77% 44.23% Cicero 6.03% 6.90% 41.87% 45.20% Cicero
    6.10% 7.03% 41.90% 44.98%
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 获胜率$\uparrow$ Richelieu_1 6.20% 9.40% 38.90% 45.50% Richelieu_1 6.30% 7.90%
    39.40% 46.40% Richelieu_2 6.60% 7.80% 40.80% 44.80% Richelieu_2 6.60% 8.30% 41.20%
    43.90% Richelieu_3 7.10% 9.30% 39.90% 43.70% Richelieu_3 7.20% 8.70% 41.70% 42.40%
    Richelieu_4 7.40% 8.00% 40.20% 44.40% Cicero_1 5.80% 6.70% 41.20% 46.30% Cicero_1
    5.90% 6.50% 41.50% 46.10% Cicero_2 6.50% 7.20% 42.50% 43.80% Cicero_2 6.30% 7.20%
    42.50% 44.00% Cicero_3 6.00% 7.00% 41.60% 45.40% Cicero_3 5.90% 7.00% 41.60% 45.50%
    Cicero_4 6.10% 7.20% 42.30% 44.40% Richelieu 6.83% 8.63% 39.95% 44.60% Richelieu
    6.70% 8.30% 40.77% 44.23% Cicero 6.03% 6.90% 41.87% 45.20% Cicero 6.10% 7.03%
    41.90% 44.98%
- en: 'Generalization of self-evolving framework to diverse LLMs. [Llamma 3](https://llama.meta.com/llama3)
    To demonstrate the effectiveness of our framework in a variety of LLM, we conducted
    experiments using four models: [GPT4.0](https://openai.com/index/gpt-4/), [ERNIE
    Bot](https://yiyan.baidu.com/welcome), [Spark Desk](https://xinghuo.xfyun.cn/),
    and [Llamma 3](https://llama.meta.com/llama3). The experimental results show that,
    despite variations in Richelieu’s performance due to the inherent differences
    in the capabilities of these LLMs, as illustrated in Figure [5](#S5.F5 "Figure
    5 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents for
    AI Diplomacy"), our framework and training approach significantly enhance the
    capabilities of all large language models. After training, the win rate using
    GPT4.0 increased from 1.5% lower than Cicero’s to about 0.7% higher than Cicero’s.
    The win rate using llama3 increased from 2.3% lower than Cicero’s to almost equal
    to Cicero’s. The win rates using Models Spark Desk and ERNIE Bot increased from
    3% and 4% lower than Cicero’s to 0.7% and 1.6% lower than Cicero’s, respectively.
    This indicates the generalization of a self-evolving framework to various LLMs.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '自我进化框架对各种LLM的推广。 [Llamma 3](https://llama.meta.com/llama3) 为了展示我们框架在各种LLM中的有效性，我们使用了四个模型进行实验：[GPT4.0](https://openai.com/index/gpt-4/)、[ERNIE
    Bot](https://yiyan.baidu.com/welcome)、[Spark Desk](https://xinghuo.xfyun.cn/)
    和 [Llamma 3](https://llama.meta.com/llama3)。实验结果表明，尽管由于这些LLM的能力固有差异，Richelieu的性能有所变化，如图
    [5](#S5.F5 "Figure 5 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based
    Agents for AI Diplomacy") 所示，我们的框架和训练方法显著提升了所有大型语言模型的能力。经过训练，使用GPT4.0的胜率从比Cicero低1.5%上升至比Cicero高约0.7%。使用llama3的胜率从比Cicero低2.3%上升至几乎与Cicero持平。使用Models
    Spark Desk和ERNIE Bot的胜率从比Cicero低3%和4%上升至分别低0.7%和1.6%。这表明自我进化框架对各种LLM的推广。'
- en: '![Refer to caption](img/631495edb8589e8623e6338a6354e0d0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/631495edb8589e8623e6338a6354e0d0.png)'
- en: 'Figure 4: The relative scores among 7 different agents when massively playing
    on the no-press setting. Each point shows the ratio of the model’s score on the
    vertical axis to the score gained by the model on the horizontal axis.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在没有压制设置下，7个不同代理的相对评分。每个点表示模型在纵轴上的得分与在横轴上获得的得分的比率。
- en: '![Refer to caption](img/e03e4ad9d6c4c6520e3233d9dd887b8a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e03e4ad9d6c4c6520e3233d9dd887b8a.png)'
- en: 'Figure 5: Richelieu modules benefit different LLMs. The solid line represents
    the experimental results for Richelieu, while the dashed line corresponds to Cicero.
    Different colors are used for different LLMs. The horizontal axis represents the
    logarithm of the number of training sessions, and the vertical axis denotes the
    rate.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Richelieu模块对不同LLM的好处。实线表示Richelieu的实验结果，而虚线对应于Cicero。不同颜色用于不同的LLM。横轴表示训练次数的对数，纵轴表示比率。
- en: 'Ablation Study. We conduct comprehensive ablation studies on Richelieu by analyzing
    the benefit of incorporating Richelieu’s various modules, like planners or memory,
    into basic LLMs. The results are shown in Table [2](#S5.T2 "Table 2 ‣ 5.2 Results
    ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy").
    As can be seen, direct use of vanilla LLM yields relatively poor results. Richelieu’s
    performance obtains steady and significant improvement by incorporating each individual
    module. This indicates that Richelieu is able to leverage other players’ actions
    during decision-making and consider both short-term and long-term benefits. Additionally,
    Richelieu’s negotiation ability has been significantly improved, allowing it to
    effectively express intentions to cooperate with other players and avoid deception
    during negotiations. And after self-play, Richelieu’s experience makes it perform
    better.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '消融研究。我们通过分析将Richelieu的各种模块（如规划者或记忆）纳入基本LLM中的好处，进行全面的消融研究。结果显示在表 [2](#S5.T2
    "Table 2 ‣ 5.2 Results ‣ 5 Experiment ‣ Richelieu: Self-Evolving LLM-Based Agents
    for AI Diplomacy") 中。可以看出，直接使用原始LLM的结果相对较差。通过纳入每个单独的模块，Richelieu的性能获得了稳定而显著的提升。这表明Richelieu能够在决策过程中利用其他玩家的行动，考虑短期和长期利益。此外，Richelieu的谈判能力有了显著提高，使其能够有效地表达与其他玩家合作的意图，并在谈判中避免欺骗。自我对弈后，Richelieu的经验使其表现更好。'
- en: 'Table 2: Ablation study: average results of 3 Richelieu vs. 4 Cicero.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：消融研究：3个Richelieu与4个Cicero的平均结果。
- en: Modeling others sub-goals Negotiation pipeline Reflection with Memory Self-play
    Win $\uparrow$ ✗ ✗ ✗ ✗ ✗ 0.4% 0.7% 4.3% 94.6% ✓ ✗ ✗ ✗ ✗ 0.7% 1.2% 10.6% 87.5%
    ✓ ✓ ✗ ✗ ✗ 3.3% 4.7% 26.7% 65.3% ✓ ✓ ✓ ✗ ✗ 3.8% 5.8% 33.1% 57.3% ✓ ✓ ✓ ✓ ✗ 5.2%
    6.6% 39.5% 48.7% ✓ ✓ ✓ ✓ ✓ 6.7% 8.5% 40.4% 44.4%
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模型他人子目标 谈判流程 反思与记忆 自我对弈 胜率 $\uparrow$ ✗ ✗ ✗ ✗ ✗ 0.4% 0.7% 4.3% 94.6% ✓ ✗ ✗ ✗
    ✗ 0.7% 1.2% 10.6% 87.5% ✓ ✓ ✗ ✗ ✗ 3.3% 4.7% 26.7% 65.3% ✓ ✓ ✓ ✗ ✗ 3.8% 5.8% 33.1%
    57.3% ✓ ✓ ✓ ✓ ✗ 5.2% 6.6% 39.5% 48.7% ✓ ✓ ✓ ✓ ✓ 6.7% 8.5% 40.4% 44.4%
- en: 6 Example Cases
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 示例案例
- en: '![Refer to caption](img/51ee1811083e27d41d71f10101317f50.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/51ee1811083e27d41d71f10101317f50.png)'
- en: 'Figure 6: An example case of negotiating with a nation to form an alliance
    to confront a strong enemy.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：与某国谈判以建立联盟对抗强敌的一个示例案例。
- en: 'As is shown in Figure  [6](#S6.F6 "Figure 6 ‣ 6 Example Cases ‣ Richelieu:
    Self-Evolving LLM-Based Agents for AI Diplomacy"), Richelieu controls France.
    At this time, France is at war with Austria over control of the Apennine Peninsula.
    However, Russia is on the verge of victory in its war against Turkey, which will
    lead to significant territorial expansion for Russia. Although France and Russia
    currently do not share a border, are not at war, and have no conflicts of interest,
    Richelieu foresees Russia becoming the most threatening enemy in the future. Therefore,
    Richelieu sets a sub-goal of weakening Russia.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [6](#S6.F6 "Figure 6 ‣ 6 Example Cases ‣ Richelieu: Self-Evolving LLM-Based
    Agents for AI Diplomacy") 所示，里什利厄控制了法国。此时，法国正在与奥地利争夺亚平宁半岛的控制权。然而，俄罗斯在与土耳其的战争中接近胜利，这将导致俄罗斯领土的显著扩张。尽管法国和俄罗斯目前没有接壤、没有战争，也没有利益冲突，里什利厄预见到未来俄罗斯将成为最具威胁的敌人。因此，里什利厄设定了削弱俄罗斯的子目标。'
- en: In the subsequent negotiation phase, Richelieu proactively proposes ending the
    war with Austria, despite holding an advantage in this conflict. Richelieu promises
    Austria that if it ceases hostilities and attacks Russia, Richelieu will assist
    Austria in defending against any attacks from England. The negotiations are successful.
    Austria accepted Richelieu’s proposal, and the two countries reached an agreement
    to exchange the supply centers of Napoli and Munich.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的谈判阶段，里什利厄主动提议结束与奥地利的战争，尽管在这场冲突中占有优势。里什利厄承诺，如果奥地利停止敌对行动并攻击俄罗斯，里什利厄将协助奥地利防御来自英国的任何攻击。谈判成功。奥地利接受了里什利厄的提议，两国达成了交换那不勒斯和慕尼黑供应中心的协议。
- en: During the action phase, Austria moves its troops from Venice to Apulia in preparation
    for capturing Napoli in the next turn, while the rest of its forces are repositioned
    to the eastern regions bordering Russia to defend against Russian attacks and
    compete for supply centers. French units occupy Munich and prepare to advance
    on Russian territories such as Berlin. Meanwhile, French units support Austria
    in the Holland and Belgium regions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在行动阶段，奥地利将部队从威尼斯移动到阿普利亚，为下一回合夺取那不勒斯做准备，同时将其余部队重新部署到接壤俄罗斯的东部地区，以防御俄罗斯的攻击并争夺供应中心。法国部队占领了慕尼黑，并准备向如柏林等俄罗斯领土推进。同时，法国部队在荷兰和比利时地区支援奥地利。
- en: 'As shown in Figure  [7](#S6.F7 "Figure 7 ‣ 6 Example Cases ‣ Richelieu: Self-Evolving
    LLM-Based Agents for AI Diplomacy"), Richelieu controls Germany. During the negotiation
    phase, England proposed a ceasefire to Germany and invited Germany to form an
    alliance to jointly attack France. England hoped to cease the war with Germany
    in Holland and Belgium. Subsequently, German units would support England in attacking
    Brest, and then England would utilize its fleets to assist Germany in attacking
    Spain and Portugal. Richelieu suspected that England was deceiving Germany, as
    England was likely to attack territories in the north such as Belgium and Berlin
    after German units were redirected to support Brest. Therefore, we pretended to
    accept England’s alliance proposal during the negotiation process. However, at
    the same time, we sought out France and expressed our willingness to cease hostilities,
    allowing France to focus entirely on defending against England’s attacks. In the
    action phase, England’s actions confirmed Richelieu’s suspicions. England attacked
    Belgium from Holland, but because Richelieu didn’t move units in Belgium, England’s
    attack failed.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [7](#S6.F7 "图 7 ‣ 6 示例案例 ‣ Richelieu: 自我进化的基于LLM的AI外交代理") 所示，Richelieu 控制了德国。在谈判阶段，英格兰向德国提出停火，并邀请德国结盟共同攻击法国。英格兰希望在荷兰和比利时与德国结束战争。随后，德国部队将支持英格兰攻击布雷斯特，然后英格兰将利用其舰队帮助德国攻击西班牙和葡萄牙。Richelieu怀疑英格兰在欺骗德国，因为英格兰可能会在德国部队被重新部署以支持布雷斯特后攻击北部如比利时和柏林的领土。因此，我们在谈判过程中假装接受了英格兰的结盟提议。然而，同时我们寻求法国，表示愿意停止敌对行动，让法国完全专注于防御英格兰的攻击。在行动阶段，英格兰的行动证实了Richelieu的怀疑。英格兰从荷兰攻击比利时，但由于Richelieu没有在比利时移动部队，英格兰的攻击失败了。'
- en: '![Refer to caption](img/3bb119299fe8a072ee0d0740dacc00e4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3bb119299fe8a072ee0d0740dacc00e4.png)'
- en: 'Figure 7: An example case of avoiding being deceived by other countries during
    negotiations.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 避免在谈判中被其他国家欺骗的案例示例。'
- en: 7 Conclusion
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we introduce Richelieu, a self-evolving LLM-based agent for AI
    diplomacy. Our model enables hierarchical planning for multi-agent tasks and utilizes
    a memory module for reflective optimization. Our model does not require human
    data and can evolve through self-play. It ultimately outperforms existing models
    like Cicero in the Diplomacy. Our ablation study demonstrates the effectiveness
    of the modules we have established. By conducting experiments using different
    LLMs, we validate the generalization of our framework to various LLMs. We believe
    that the use of LLM-based agents will become an effective approach in social science
    in the future.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了Richelieu，一种基于LLM的自我进化AI外交代理。我们的模型支持多代理任务的层次规划，并利用记忆模块进行反思优化。我们的模型不需要人工数据，并且可以通过自我对弈进行进化。它最终超越了现有的模型如Cicero。在《外交》中，我们的消融研究证明了我们建立的模块的有效性。通过使用不同的LLM进行实验，我们验证了框架对各种LLM的泛化能力。我们相信，基于LLM的代理将在未来成为社会科学中的一种有效方法。
- en: 8 Limitations and Future Work
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制与未来工作
- en: Our study is subject to certain limitations. We utilize diplomacy as the platform
    for constructing our model. However, the space of actions within diplomacy is
    constrained, whereas the decision-making space in real-world diplomacy is virtually
    boundless. In Diplomacy, apart from the negotiation information exchanged between
    players, all other information is public and certain. Conversely, real-world diplomacy
    operates within a framework of incomplete information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究存在一定的局限性。我们利用外交作为构建模型的平台。然而，外交中的行动空间是有限的，而现实世界中的外交决策空间几乎是无限的。在《外交》中，除了玩家之间交换的谈判信息外，所有其他信息都是公开且确定的。相反，现实世界的外交在一个不完全信息的框架内运作。
- en: The potential applications of such an AI agent are vast, ranging from simulated
    diplomatic environments to real-world assistance and analysis. In future research,
    we intend to develop a more realistic game space, characterized by incomplete
    information and multi-player games, to enhance and refine our model further. We
    will also extend the framework to other multi-agent scenarios, including embodied
    interactions [[75](#bib.bib75), [11](#bib.bib11), [9](#bib.bib9)], sensor networks [[54](#bib.bib54),
    [61](#bib.bib61), [38](#bib.bib38), [31](#bib.bib31)], and video games [[49](#bib.bib49),
    [34](#bib.bib34)]. This framework can also be employed to develop various applications.
    For instance, in the fields of economics and finance, we intend to utilize it
    to create business analytics and negotiation models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种AI代理的潜在应用广泛，从模拟的外交环境到现实世界的援助和分析。未来的研究中，我们打算开发一个更现实的游戏空间，特征为不完全信息和多玩家游戏，以进一步增强和完善我们的模型。我们还将扩展框架到其他多智能体场景，包括具身交互
    [[75](#bib.bib75)、[11](#bib.bib11)、[9](#bib.bib9)]、传感器网络 [[54](#bib.bib54)、[61](#bib.bib61)、[38](#bib.bib38)、[31](#bib.bib31)]
    和视频游戏 [[49](#bib.bib49)、[34](#bib.bib34)]。该框架还可以用于开发各种应用。例如，在经济学和金融领域，我们打算利用它来创建商业分析和谈判模型。
- en: References
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abdelnabi et al. [2023] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea
    Schönherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive
    multi-agent negotiation games. *arXiv preprint arXiv:2309.17234*, 2023.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelnabi et al. [2023] Sahar Abdelnabi、Amr Gomaa、Sarath Sivaprasad、Lea Schönherr
    和 Mario Fritz. Llm-deliberation：用交互式多智能体谈判游戏评估 llms。*arXiv 预印本 arXiv:2309.17234*，2023年。
- en: Allan [1975] Calhamer Allan. *The Games & puzzles book of modern board games*.
    W. Luscombe, 1975. ISBN 978-0860020592.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allan [1975] Calhamer Allan. *现代桌面游戏的游戏与谜题书*。W. Luscombe，1975年。ISBN 978-0860020592。
- en: Anthony et al. [2020] Thomas Anthony, Tom Eccles, Andrea Tacchetti, János Kramár,
    Ian Gemp, Thomas Hudson, Nicolas Porcel, Marc Lanctot, Julien Pérolat, Richard
    Everett, et al. Learning to play no-press diplomacy with best response policy
    iteration. *Advances in Neural Information Processing Systems*, 33:17987–18003,
    2020.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthony et al. [2020] Thomas Anthony、Tom Eccles、Andrea Tacchetti、János Kramár、Ian
    Gemp、Thomas Hudson、Nicolas Porcel、Marc Lanctot、Julien Pérolat、Richard Everett
    等人. 学习通过最佳响应策略迭代来玩无新闻外交。*神经信息处理系统进展*，33:17987–18003，2020年。
- en: Archer [2024] Bruno-AndrÃ© Giraudon & Vincent Archer. C-diplo argir, 2024. URL
    [https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7](https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7).
    Accessed:2024-05-02.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Archer [2024] Bruno-André Giraudon & Vincent Archer. C-diplo argir，2024年。网址
    [https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7](https://world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7)。访问日期：2024年5月2日。
- en: Bakhtin et al. [2021] Anton Bakhtin, David Wu, Adam Lerer, and Noam Brown. No-press
    diplomacy from scratch. *Advances in Neural Information Processing Systems*, 34:18063–18074,
    2021.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin et al. [2021] Anton Bakhtin、David Wu、Adam Lerer 和 Noam Brown. 从零开始的无新闻外交。*神经信息处理系统进展*，34:18063–18074，2021年。
- en: Bakhtin et al. [2022] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina,
    Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al.
    Human-level play in the game of diplomacy by combining language models with strategic
    reasoning. *Science*, 378(6624):1067–1074, 2022.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin et al. [2022] Anton Bakhtin、Noam Brown、Emily Dinan、Gabriele Farina、Colin
    Flaherty、Daniel Fried、Andrew Goff、Jonathan Gray、Hengyuan Hu 等人. 通过结合语言模型与战略推理在外交游戏中实现人类水平的游戏。*科学*，378(6624):1067–1074，2022年。
- en: Bianchi et al. [2024] Federico Bianchi, Patrick John Chia, Mert Yuksekgonul,
    Jacopo Tagliabue, Dan Jurafsky, and James Zou. How well can llms negotiate? negotiationarena
    platform and analysis. *arXiv preprint arXiv:2402.05863*, 2024.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchi et al. [2024] Federico Bianchi、Patrick John Chia、Mert Yuksekgonul、Jacopo
    Tagliabue、Dan Jurafsky 和 James Zou. LLMs 能在谈判中表现得多好？Negotiationarena 平台和分析。*arXiv
    预印本 arXiv:2402.05863*，2024年。
- en: 'Calhamer [1974] Allan Calhamer. The invention of diplomacy. [https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm](https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm),
    1974. Accessed: 2024-05-18.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calhamer [1974] Allan Calhamer. 外交的发明。 [https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm](https://web.archive.org/web/20090910012615/http://www.diplom.org/~diparch/resources/calhamer/invention.htm)，1974年。访问日期：2024年5月18日。
- en: 'Chen et al. [2023] Yuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang
    Jiang, Zongqing Lu, Hao Dong, and Yaodong Yang. Bi-dexhands: Towards human-level
    bimanual dexterous manipulation. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 2023.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2023] 陈元佩、耿依然、钟芳伟、季家明、姜杰创、陆宗庆、董浩和杨耀东。**Bi-dexhands: 实现类人双手灵巧操作**。*IEEE模式分析与机器智能学报*，2023年。'
- en: Cheng et al. [2024] Guangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin
    Sun, and Jiang Bian. Empowering large language models on robotic manipulation
    with affordance prompting. *arXiv preprint arXiv:2404.11027*, 2024.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. [2024] 广然·程、楚恒·张、温哲·蔡、李兆、常银·孙和姜边。**通过赋能提示增强大型语言模型在机器人操作中的能力**。*arXiv
    预印本 arXiv:2404.11027*，2024年。
- en: Ci et al. [2023] Hai Ci, Mickel Liu, Xuehai Pan, Fangwei Zhong, and Yizhou Wang.
    Proactive multi-camera collaboration for 3d human pose estimation. *arXiv preprint
    arXiv:2303.03767*, 2023.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ci et al. [2023] 海·茨、米克尔·刘、潘雪海、钟芳伟和王毅洲。**主动式多摄像头协作用于 3D 人体姿态估计**。*arXiv 预印本
    arXiv:2303.03767*，2023年。
- en: 'David [2014] Hill David. The board game of the alpha nerds. [https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/](https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/),
    2014. Accessed: 2024-05-18.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David [2014] Hill David. **阿尔法极客的棋盘游戏**。 [https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/](https://grantland.com/features/diplomacy-the-board-game-of-the-alpha-nerds/),
    2014. 访问日期：2024-05-18。
- en: 'De Jonge and Sierra [2017] Dave De Jonge and Carles Sierra. D-brane: a diplomacy
    playing agent for automated negotiations research. *Applied Intelligence*, 47(1):158–177,
    2017.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'De Jonge and Sierra [2017] 戴夫·德·琼格和卡尔斯·塞拉。**D-brane: 用于自动化谈判研究的外交代理**。*应用智能*，47(1):158–177，2017年。'
- en: 'de Zarzà et al. [2023] I de Zarzà, J de Curtò, Gemma Roig, Pietro Manzoni,
    and Carlos T Calafate. Emergent cooperation and strategy adaptation in multi-agent
    systems: An extended coevolutionary theory with llms. *Electronics*, 12(12):2722,
    2023.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Zarzà et al. [2023] I de Zarzà、J de Curtò、杰玛·罗伊格、皮耶特罗·曼佐尼和卡洛斯·T·卡拉法特。**多代理系统中的新兴合作与策略适应：一种扩展的共进化理论与LLMs**。*电子学*，12(12):2722，2023年。
- en: Duéñez-Guzmán et al. [2023] Edgar A Duéñez-Guzmán, Suzanne Sadedin, Jane X Wang,
    Kevin R McKee, and Joel Z Leibo. A social path to human-like artificial intelligence.
    *Nature Machine Intelligence*, 5(11):1181–1188, 2023.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duéñez-Guzmán et al. [2023] 埃德加·A·杜埃涅斯-古兹曼、苏珊·萨德丁、简·X·王、凯文·R·麦基和乔尔·Z·莱博。**通向类人人工智能的社会路径**。*自然机器智能*，5(11):1181–1188，2023年。
- en: 'Fan et al. [2022] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo:
    Building open-ended embodied agents with internet-scale knowledge. In *Thirty-sixth
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*,
    2022. URL [https://openreview.net/forum?id=rc8o_j8I8PX](https://openreview.net/forum?id=rc8o_j8I8PX).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan et al. [2022] 林曦范、王冠智、蒋云凡、阿贾伊·曼德尔卡、杨云聪、朱浩毅、安德鲁·唐、黄德安、朱宇克和安妮玛·安南德库马尔。**Minedojo:
    构建具有互联网规模知识的开放式具身代理**。在 *第36届神经信息处理系统会议数据集与基准跟踪*，2022年。网址 [https://openreview.net/forum?id=rc8o_j8I8PX](https://openreview.net/forum?id=rc8o_j8I8PX)。'
- en: Gao and Zhang [2024] Hang Gao and Yongfeng Zhang. Memory sharing for large language
    model based agents. *arXiv preprint arXiv:2404.09982*, 2024.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao and Zhang [2024] 杭·高和永峰·张。**大型语言模型基础的代理的记忆共享**。*arXiv 预印本 arXiv:2404.09982*，2024年。
- en: Gray et al. [2020] Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown.
    Human-level performance in no-press diplomacy via equilibrium search. *International
    Conference on Learning Representations*, 2020.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gray et al. [2020] 乔纳森·格雷、亚当·勒雷、安东·巴赫廷和诺亚·布朗。**通过平衡搜索实现人类水平的无压外交表现**。*国际学习表征会议*，2020年。
- en: 'Gürcan [2024] Önder Gürcan. Llm-augmented agent-based modelling for social
    simulations: Challenges and opportunities. *HHAI 2024: Hybrid Human AI Systems
    for the Social Good*, pages 134–144, 2024.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gürcan [2024]  Önder Gürcan。**基于 LLM 增强的代理建模用于社会模拟：挑战与机遇**。*HHAI 2024: 为社会福祉服务的混合人类
    AI 系统*，第134–144页，2024年。'
- en: 'Hatalis et al. [2023] Kostas Hatalis, Despina Christou, Joshua Myers, Steven
    Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer.
    Memory matters: The need to improve long-term memory in llm-agents. In *Proceedings
    of the AAAI Symposium Series*, pages 277–280, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hatalis et al. [2023] 科斯塔斯·哈塔利斯、德斯皮娜·克里斯图、约书亚·迈尔斯、史蒂文·琼斯、基思·兰伯特、亚当·阿莫斯-宾克斯、佐赫雷·丹嫩豪尔和达斯汀·丹嫩豪尔。**记忆很重要：提高
    LLM 代理的长期记忆的必要性**。在 *AAAI研讨会论文集*，第277–280页，2023年。
- en: 'He et al. [2024] Junda He, Christoph Treude, and David Lo. Llm-based multi-agent
    systems for software engineering: Vision and the road ahead. *arXiv preprint arXiv:2404.04834*,
    2024.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2024] Junda He、Christoph Treude 和 David Lo。基于 LLM 的多智能体系统在软件工程中的应用：愿景与未来道路。*arXiv
    预印本 arXiv:2404.04834*，2024年。
- en: 'Hill [2014] Avalon Hill. Diplomacy rules 4th edition. [https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf](https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf),
    2014. Accessed: 2024-05-18.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill [2014] Avalon Hill。外交规则第 4 版。[https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf](https://web.archive.org/web/20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/2000AH4th.pdf)，2014年。访问时间：2024-05-18。
- en: 'Hou et al. [2024] Yuki Hou, Haruki Tamoto, and Homei Miyashita. " my agent
    understands me better": Integrating dynamic human-like memory recall and consolidation
    in llm-based agents. In *Extended Abstracts of the CHI Conference on Human Factors
    in Computing Systems*, pages 1–7, 2024.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. [2024] Yuki Hou、Haruki Tamoto 和 Homei Miyashita。“my agent understands
    me better”：在基于 LLM 的智能体中集成动态类似人类的记忆召回和巩固。在*CHI 人机交互会议扩展摘要*中，页码 1–7，2024年。
- en: Jacob et al. [2022] Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer,
    Hengyuan Hu, Anton Bakhtin, Jacob Andreas, and Noam Brown. Modeling strong and
    human-like gameplay with kl-regularized search. In *International Conference on
    Machine Learning*, pages 9695–9728\. PMLR, 2022.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob et al. [2022] Athul Paul Jacob、David J Wu、Gabriele Farina、Adam Lerer、Hengyuan
    Hu、Anton Bakhtin、Jacob Andreas 和 Noam Brown。使用 KL 规整化搜索建模强大且类似人类的游戏玩法。在*国际机器学习大会*中，页码
    9695–9728。PMLR，2022年。
- en: 'Jaidka et al. [2024] Kokil Jaidka, Hansin Ahuja, and Lynnette Hui Xian Ng.
    It takes two to negotiate: Modeling social exchange in online multiplayer games.
    *Proceedings of the ACM on Human-Computer Interaction*, 8(CSCW1):1–22, 2024.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaidka et al. [2024] Kokil Jaidka、Hansin Ahuja 和 Lynnette Hui Xian Ng。谈判需要两个人：建模在线多人游戏中的社会交换。在*ACM
    人机交互学报*，8(CSCW1)：1–22，2024年。
- en: Konya et al. [2023] Andrew Konya, Deger Turan, Aviv Ovadya, Lina Qui, Daanish
    Masood, Flynn Devine, Lisa Schirch, and Isabella Roberts. Deliberative technology
    for alignment. *ArXiv*, abs/2312.03893, 2023.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konya et al. [2023] Andrew Konya、Deger Turan、Aviv Ovadya、Lina Qui、Daanish Masood、Flynn
    Devine、Lisa Schirch 和 Isabella Roberts。用于对齐的审议技术。*ArXiv*，abs/2312.03893，2023年。
- en: Kostick [2015] Conor Kostick. *The Art of Correspondence in the Game of Diplomacy*.
    Curses & Magic, 2nd edition, 2015. ISBN 978-0993415104.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kostick [2015] Conor Kostick。*外交游戏中的通信艺术*。Curses & Magic，第 2 版，2015年。ISBN 978-0993415104。
- en: 'Kovač et al. [2023] Grgur Kovač, Rémy Portelas, Peter Ford Dominey, and Pierre-Yves
    Oudeyer. The socialai school: Insights from developmental psychology towards artificial
    socio-cultural agents. *arXiv preprint arXiv:2307.07871*, 2023.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovač et al. [2023] Grgur Kovač、Rémy Portelas、Peter Ford Dominey 和 Pierre-Yves
    Oudeyer。社会 AI 学校：从发展心理学到人工社会文化智能体的见解。*arXiv 预印本 arXiv:2307.07871*，2023年。
- en: Kramár et al. [2022] János Kramár, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin R
    McKee, Mateusz Malinowski, Thore Graepel, and Yoram Bachrach. Negotiation and
    honesty in artificial intelligence methods for the board game of diplomacy. *Nature
    Communications*, 13(1):7214, 2022.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kramár et al. [2022] János Kramár、Tom Eccles、Ian Gemp、Andrea Tacchetti、Kevin
    R McKee、Mateusz Malinowski、Thore Graepel 和 Yoram Bachrach。在博弈外交中关于谈判和诚信的人工智能方法。*自然通讯*，13(1)：7214，2022年。
- en: Li et al. [2024a] Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and
    Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue.
    *arXiv preprint arXiv:2406.05925*, 2024a.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024a] Hao Li、Chenghao Yang、An Zhang、Yang Deng、Xiang Wang 和 Tat-Seng
    Chua。再见！LLM 驱动的个性化智能体用于长期对话。*arXiv 预印本 arXiv:2406.05925*，2024a。
- en: Li et al. [2020] Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu Qiao, and
    Yizhou Wang. Pose-assisted multi-camera collaboration for active object tracking.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34,
    pages 759–766, 2020.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2020] Jing Li、Jing Xu、Fangwei Zhong、Xiangyu Kong、Yu Qiao 和 Yizhou
    Wang。基于姿态的多摄像头协作用于主动物体跟踪。在*AAAI 人工智能会议论文集*，卷 34，页码 759–766，2020年。
- en: 'Li et al. [2024b] Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan,
    Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm
    agents: Insights and survey about the capability, efficiency and security. *arXiv
    preprint arXiv:2401.05459*, 2024b.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024b] Yuanchun Li、Hao Wen、Weijun Wang、Xiangyu Li、Yizhen Yuan、Guohong
    Liu、Jiacheng Liu、Wenxing Xu、Xiang Wang、Yi Sun 等。个人 LLM 智能体：关于能力、效率和安全性的见解与调查。*arXiv
    预印本 arXiv:2401.05459*，2024b。
- en: 'Liu et al. [2024] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al. Agentlite:
    A lightweight library for building and advancing task-oriented llm agent system.
    *arXiv preprint arXiv:2402.15538*, 2024.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等人 [2024] 朱伟 刘、维然 姚、建国 张、良伟 杨、祖欣 刘、军涛 谭、帕夫拉·K·乔贝、天 兰、杰森 吴、环 王等人。Agentlite:
    用于构建和推进任务导向的 llm 代理系统的轻量级库。*arXiv 预印本 arXiv:2402.15538*，2024。'
- en: Ma et al. [2024] Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou
    Wang. Fast peer adaptation with context-aware exploration. *arXiv preprint arXiv:2402.02468*,
    2024.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马等人 [2024] 龙 马、元飞 王、方伟 钟、宋春 Zhu 和一舟 王。基于情境感知的快速同行适应。*arXiv 预印本 arXiv:2402.02468*，2024。
- en: Moghimifar et al. [2024] Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, and
    Gholamreza Haffari. Modelling political coalition negotiations using llm-based
    agents. *arXiv preprint arXiv:2402.11712*, 2024.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莫吉米法尔等人 [2024] 法赫德·莫吉米法尔、袁芳 李、罗伯特·汤姆森和戈拉姆雷扎·哈法里。使用 llm 代理建模政治联盟谈判。*arXiv 预印本
    arXiv:2402.11712*，2024。
- en: Mukobi et al. [2023] Gabriel Mukobi, Ann-Katrin Reuel, Juan-Pablo Rivera, and
    Chandler Smith. Assessing risks of using autonomous language models in military
    and diplomatic planning. In *Multi-Agent Security Workshop @ NeurIPS’23*, 2023.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 穆科比等人 [2023] 加布里埃尔 穆科比、安娜-凯特琳·瑞乌尔、胡安-巴布罗·里维拉和钱德勒·史密斯。评估在军事和外交规划中使用自主语言模型的风险。发表于*多智能体安全研讨会
    @ NeurIPS’23*，2023。
- en: Noh and Chang [2024] Sean Noh and Ho-Chun Herbert Chang. Llms with personalities
    in multi-issue negotiation games. *arXiv preprint arXiv:2405.05248*, 2024.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noh 和 Chang [2024] 肖恩 Noh 和 何春·赫伯特·张。多议题谈判游戏中的有个性 llm。*arXiv 预印本 arXiv:2405.05248*，2024。
- en: 'Pan et al. [2022] Xuehai Pan, Mickel Liu, Fangwei Zhong, Yaodong Yang, Song-Chun
    Zhu, and Yizhou Wang. Mate: Benchmarking multi-agent reinforcement learning in
    distributed target coverage control. *Advances in Neural Information Processing
    Systems*, 35:27862–27879, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '潘等人 [2022] 薛海 潘、米克尔 刘、方伟 钟、耀东 杨、宋春 Zhu 和一舟 王。Mate: 在分布式目标覆盖控制中基准测试多智能体强化学习。*神经信息处理系统进展*，35:27862–27879，2022。'
- en: 'Paquette et al. [2019] Philip Paquette, Yuchen Lu, Seton Steven Bocco, Max
    Smith, Satya O-G, Jonathan K Kummerfeld, Joelle Pineau, Satinder Singh, and Aaron C
    Courville. No-press diplomacy: Modeling multi-agent gameplay. *Advances in Neural
    Information Processing Systems*, 32, 2019.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕凯特等人 [2019] 菲利普 帕凯特、宇辰 陆、赛顿·史蒂文·博科、马克斯 史密斯、萨蒂亚 O-G、乔纳森·K·库默费尔德、乔艾尔·皮诺、萨廷德·辛格和亚伦·C·库尔维尔。无压力外交：建模多智能体博弈。*神经信息处理系统进展*，32，2019。
- en: 'Qi et al. [2024] Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang,
    Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu,
    Yaodong Yang, and Song-Chun Zhu. Civrealm: A learning and reasoning odyssey in
    civilization for decision-making agents. In *The Twelfth International Conference
    on Learning Representations*, 2024. URL [https://openreview.net/forum?id=UBVNwD3hPN](https://openreview.net/forum?id=UBVNwD3hPN).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '齐等人 [2024] 思源 齐、硕 陈、叶欣 李、向宇 孔、俊琦 王、邦成 杨、普林·黄、一凡 钟、小源 张、兆伟 张、年 刘、耀东 杨和宋春 Zhu。Civrealm:
    决策代理的文明学习与推理之旅。发表于*第十二届国际学习表征大会*，2024。网址 [https://openreview.net/forum?id=UBVNwD3hPN](https://openreview.net/forum?id=UBVNwD3hPN)。'
- en: 'Renze and Guven [2024] Matthew Renze and Erhan Guven. Self-reflection in llm
    agents: Effects on problem-solving performance. *arXiv preprint arXiv:2405.06682*,
    2024.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任泽和古文 [2024] 马修 任泽和厄尔汉 古文。llm 代理中的自我反思：对问题解决绩效的影响。*arXiv 预印本 arXiv:2405.06682*，2024。
- en: Richard [1979] Sharp Richard. *The game of diplomacy*. Arthur Barker, 1979.
    ISBN 978-0213166762.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理查德 [1979] 夏普 理查德。*外交游戏*。亚瑟·巴克出版社，1979。ISBN 978-0213166762。
- en: 'Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    Toolformer: Language models can teach themselves to use tools. In *Thirty-seventh
    Conference on Neural Information Processing Systems*, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '希克等人 [2023] 蒂莫·希克、简·德维维迪-余、罗伯托·德西、罗伯塔·赖利安努、玛丽亚·洛梅利、埃里克·汉布罗、卢克·泽特尔摩耶、尼古拉·坎切达和托马斯·肖隆。Toolformer:
    语言模型可以自学使用工具。发表于*第七十七届神经信息处理系统会议*，2023。'
- en: 'Shen et al. [2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with chatGPT and its friends
    in hugging face. In *Thirty-seventh Conference on Neural Information Processing
    Systems*, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '沈等人 [2023] 永亮 沈、开涛 宋、徐 谭、东升 李、伟明 陆和跃亭 庄。HuggingGPT: 与 chatGPT 及其在 hugging face
    的朋友们一起解决 AI 任务。发表于*第七十七届神经信息处理系统会议*，2023。'
- en: 'Shoker et al. [2023] Sarah Shoker, Andrew W. Reddie, Sarah Barrington, Miles
    Brundage, Husanjot Chahal, Michael Depp, Bill Drexel, Ritwik Gupta, Marina Favaro,
    Jake J. Hecla, Alan Hickey, Margarita Konaev, KI Pavan Kumar, Nathan Lambert,
    Andrew J. Lohn, Cullen O’Keefe, Nazneen Rajani, Michael Sellitto, Robert F. Trager,
    Leah A. Walker, Alexa Wehsener, and Jessica Young. Confidence-building measures
    for artificial intelligence: Workshop proceedings. *ArXiv*, abs/2308.00862, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shoker et al. [2023] Sarah Shoker, Andrew W. Reddie, Sarah Barrington, Miles
    Brundage, Husanjot Chahal, Michael Depp, Bill Drexel, Ritwik Gupta, Marina Favaro,
    Jake J. Hecla, Alan Hickey, Margarita Konaev, KI Pavan Kumar, Nathan Lambert,
    Andrew J. Lohn, Cullen O’Keefe, Nazneen Rajani, Michael Sellitto, Robert F. Trager,
    Leah A. Walker, Alexa Wehsener, 和 Jessica Young. 人工智能的信心建设措施：研讨会论文集。*ArXiv*，abs/2308.00862，2023。
- en: 'Sun et al. [2024] Chuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based
    multi-agent reinforcement learning: Current and future directions. *arXiv preprint
    arXiv:2405.11106*, 2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2024] Chuanneng Sun, Songjun Huang, 和 Dario Pompili. 基于 LLM 的多智能体强化学习：当前与未来方向。*arXiv
    预印本 arXiv:2405.11106*，2024。
- en: 'Talebirad and Nadiri [2023] Yashar Talebirad and Amirhossein Nadiri. Multi-agent
    collaboration: Harnessing the power of intelligent llm agents. *ArXiv*, abs/2306.03314,
    2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talebirad and Nadiri [2023] Yashar Talebirad 和 Amirhossein Nadiri. 多智能体合作：利用智能
    LLM 代理的力量。*ArXiv*，abs/2306.03314，2023。
- en: Wan et al. [2024] Hongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao,
    Dakuo Wang, Yvonne Coady, and Mirjana Prpa. Building llm-based ai agents in social
    virtual reality. In *Extended Abstracts of the CHI Conference on Human Factors
    in Computing Systems*, pages 1–7, 2024.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. [2024] Hongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao,
    Dakuo Wang, Yvonne Coady, 和 Mirjana Prpa. 在社交虚拟现实中构建基于 LLM 的 AI 代理。收录于*CHI 计算机系统人因会议扩展摘要*，第
    1–7 页，2024。
- en: 'Wang et al. [2024a] Dongzi Wang, Fangwei Zhong, Minglong Li, Muning Wen, Yuanxi
    Peng, Teng Li, and Adam Yang. Romat: Role-based multi-agent transformer for generalizable
    heterogeneous cooperation. *Neural Networks*, 174:106129, 2024a.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024a] Dongzi Wang, Fangwei Zhong, Minglong Li, Muning Wen, Yuanxi
    Peng, Teng Li, 和 Adam Yang. Romat：基于角色的多智能体变换器用于可推广的异质合作。*Neural Networks*，174:106129，2024a。
- en: 'Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. In *NeurIPS 2023 Foundation Models for Decision
    Making Workshop*, 2023a. URL [https://openreview.net/forum?id=P8E4Br72j3](https://openreview.net/forum?id=P8E4Br72j3).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, 和 Anima Anandkumar. Voyager：一个开放式的具身代理与大型语言模型。收录于*NeurIPS
    2023 决策制定基础模型研讨会*，2023a。网址 [https://openreview.net/forum?id=P8E4Br72j3](https://openreview.net/forum?id=P8E4Br72j3)。
- en: 'Wang et al. [2024b] Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, and Yang Li.
    Devil’s advocate: Anticipatory reflection for llm agents. *arXiv preprint arXiv:2405.16334*,
    2024b.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024b] Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, and Yang Li.
    魔鬼代言人：针对 LLM 代理的预期反思。*arXiv 预印本 arXiv:2405.16334*，2024b。
- en: Wang et al. [2024c] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large
    language model based autonomous agents. *Frontiers of Computer Science*, 18(6):1–26,
    2024c.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024c] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等。基于大型语言模型的自主代理综述。*Frontiers
    of Computer Science*，18(6):1–26，2024c。
- en: Wang et al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 自一致性提高语言模型的链式推理。*arXiv 预印本 arXiv:2203.11171*，2022。
- en: 'Wang et al. [2021] Yuanfei Wang, Fangwei Zhong, Jing Xu, and Yizhou Wang. Tom2c:
    Target-oriented multi-agent communication and cooperation with theory of mind.
    *arXiv preprint arXiv:2111.09189*, 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2021] Yuanfei Wang, Fangwei Zhong, Jing Xu, 和 Yizhou Wang. Tom2c：基于目标的多智能体通信与合作理论。*arXiv
    预印本 arXiv:2111.09189*，2021。
- en: 'Wang et al. [2023b] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian (Shawn)
    Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning
    with llms enables open-world multi-task agents. In A. Oh, T. Naumann, A. Globerson,
    K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing
    Systems*, volume 36, pages 34153–34189\. Curran Associates, Inc., 2023b. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023b] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    (Shawn) Ma, and Yitao Liang. 描述、解释、计划和选择：与 LLM 互动规划实现开放世界多任务代理。见 A. Oh, T. Naumann,
    A. Globerson, K. Saenko, M. Hardt, 和 S. Levine 编辑的*神经信息处理系统进展*，第 36 卷，第 34153–34189
    页。Curran Associates, Inc.，2023b。网址 [https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf)。
- en: 'Wang et al. [2023c] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian
    Ma, and Yitao Liang. Jarvis-1: Open-world multi-task agents with memory-augmented
    multimodal language models. *ArXiv*, abs/2311.05997, 2023c.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023c] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian
    Ma, and Yitao Liang. Jarvis-1：具有记忆增强的多模态语言模型的开放世界多任务代理。*ArXiv*，abs/2311.05997，2023c。
- en: 'Wang et al. [2024d] Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong
    Feng, Wenya Wang, and Jie Zhang. Re2llm: Reflective reinforcement large language
    model for session-based recommendation. *arXiv preprint arXiv:2403.16427*, 2024d.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2024d] Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong
    Feng, Wenya Wang, and Jie Zhang. Re2llm: 反射性强化大型语言模型用于基于会话的推荐。*arXiv 预印本 arXiv:2403.16427*，2024d。'
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837, 2022.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等。链式思维提示引发大型语言模型的推理。*神经信息处理系统进展*，35:24824–24837，2022。
- en: 'Wikipedia [2024] Wikipedia. Diplomacy(game). [https://en.wikipedia.org/wiki/Diplomacy_(game)](https://en.wikipedia.org/wiki/Diplomacy_(game)),
    2024. Accessed: 2024-05-18.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wikipedia [2024] Wikipedia. Diplomacy（游戏）。[https://en.wikipedia.org/wiki/Diplomacy_(game)](https://en.wikipedia.org/wiki/Diplomacy_(game))，2024。访问时间：2024-05-18。
- en: 'Xia et al. [2024] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang,
    Yang Yang, and Rui Wang. Measuring bargaining abilities of llms: A benchmark and
    a buyer-enhancement method. *arXiv preprint arXiv:2402.15813*, 2024.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. [2024] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang,
    Yang Yang, and Rui Wang. 测量 LLM 的谈判能力：基准测试和买方增强方法。*arXiv 预印本 arXiv:2402.15813*，2024。
- en: Xu et al. [2020] Jing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent
    coordination for enhancing target coverage in directional sensor networks. *Advances
    in Neural Information Processing Systems*, 33:10053–10064, 2020.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2020] Jing Xu, Fangwei Zhong, and Yizhou Wang. 学习多智能体协调以增强方向传感器网络中的目标覆盖。*神经信息处理系统进展*，33:10053–10064，2020。
- en: 'Yan et al. [2023] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and
    Ji Yan. Larp: Language-agent role play for open-world games. *arXiv preprint arXiv:2312.17653*,
    2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. [2023] Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and
    Ji Yan. LARP：开放世界游戏中的语言代理角色扮演。*arXiv 预印本 arXiv:2312.17653*，2023。
- en: 'Yang et al. [2023a] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. GPT4tools: Teaching large language model to use tools via self-instruction.
    In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023a.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2023a] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. GPT4tools：通过自我指导教大型语言模型使用工具。见于*第三十七届神经信息处理系统会议*，2023a。
- en: 'Yang et al. [2023b] Ziyi Yang, Shreyas S Raman, Ankit Shah, and Stefanie Tellex.
    Plug in the safety chip: Enforcing constraints for llm-driven robot agents. *arXiv
    preprint arXiv:2309.09919*, 2023b.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2023b] Ziyi Yang, Shreyas S Raman, Ankit Shah, and Stefanie Tellex.
    插入安全芯片：为 LLM 驱动的机器人代理执行约束。*arXiv 预印本 arXiv:2309.09919*，2023b。
- en: 'Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. In *Thirty-seventh Conference on Neural Information
    Processing Systems*, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. 思维树：使用大型语言模型的深思熟虑问题解决。见于*第三十七届神经信息处理系统会议*，2023。
- en: 'Yu et al. [2024] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li,
    Denghui Zhang, Rong Liu, Jordan W Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced
    llm trading agent with layered memory and character design. In *Proceedings of
    the AAAI Symposium Series*, volume 3, pages 595–597, 2024.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. [2024] 杨杨余、浩航李、智陈、岳辰姜、杨李、邓辉张、荣刘、乔丹·W·苏乔和哈尔顿·哈尚。Finmem：具有分层记忆和角色设计的性能增强LLM交易代理。在*AAAI研讨会系列论文集*，第3卷，页码595–597，2024年。
- en: Zhan et al. [2024] Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma,
    Zhuang Li, Lizhen Qu, Zhaleh Semnani Azad, Ingrid Zukerman, and Gholamreza Haffari.
    Let’s negotiate! a survey of negotiation dialogue systems. *arXiv preprint arXiv:2402.01097*,
    2024.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhan et al. [2024] 好兰展、玉飞王、陶丰、云城华、苏拉杰·夏尔马、庄李、丽珍曲、扎赫拉·塞姆纳尼·阿扎德、英格丽德·祖克曼和戈拉姆雷扎·哈法里。让我们谈判！谈判对话系统的综述。*arXiv
    预印本 arXiv:2402.01097*，2024年。
- en: 'Zhang et al. [2023] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language model is semi-parametric reinforcement learning
    agent. *CoRR*, abs/2306.07929, 2023. doi: 10.48550/arXiv.2306.07929. URL [https://doi.org/10.48550/arXiv.2306.07929](https://doi.org/10.48550/arXiv.2306.07929).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2023] 丹阳张、陆晨、司徒张、洪申徐、子涵赵和凯余。大型语言模型是半参数强化学习代理。*CoRR*，abs/2306.07929，2023年。doi:
    10.48550/arXiv.2306.07929。网址 [https://doi.org/10.48550/arXiv.2306.07929](https://doi.org/10.48550/arXiv.2306.07929)。'
- en: Zhang et al. [2024a] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language models are semi-parametric reinforcement learning
    agents. *Advances in Neural Information Processing Systems*, 36, 2024a.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024a] 丹阳张、陆晨、司徒张、洪申徐、子涵赵和凯余。大型语言模型是半参数强化学习代理。*神经信息处理系统进展*，第36卷，2024a。
- en: 'Zhang et al. [2024b] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen,
    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning
    to evolve via policy-level reflection and optimization. *arXiv preprint arXiv:2402.17574*,
    2024b.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024b] 温奇张、柯唐、海吴、孟娜王、永亮沈、贵阳侯、泽琪谭、彭李、岳亭庄和伟铭陆。Agent-pro：通过策略级反思和优化学习进化。*arXiv
    预印本 arXiv:2402.17574*，2024b。
- en: 'Zhang et al. [2024c] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian
    de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as a mastermind:
    A survey of strategic reasoning with large language models. *arXiv preprint arXiv:2404.01230*,
    2024c.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024c] 亚东张、邵广毛、陶歌、寻王、阿德里安·德·温特、燕霞、文山吴、廷宋、曼兰和富如魏。LLM作为策划者：大语言模型战略推理的综述。*arXiv
    预印本 arXiv:2404.01230*，2024c。
- en: Zhang et al. [2024d] Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong
    Li, and Zhen Wang. Towards efficient llm grounding for embodied multi-agent collaboration.
    *arXiv preprint arXiv:2405.14314*, 2024d.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024d] 杨张、施新杨、陈佳白、飞吴、秀李、雪龙李和振王。朝着高效LLM基础的具身多代理协作迈进。*arXiv 预印本
    arXiv:2405.14314*，2024d。
- en: Zhang et al. [2024e] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu
    Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism
    of large language model based agents. *arXiv preprint arXiv:2404.13501*, 2024e.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024e] 泽宇张、小赫博、陈马、瑞李、徐陈、全宇戴、杰明朱、振华董和季荣温。关于大型语言模型基于代理的记忆机制的综述。*arXiv
    预印本 arXiv:2404.13501*，2024e。
- en: Zhang et al. [2022] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic
    chain of thought prompting in large language models. *arXiv preprint arXiv:2210.03493*,
    2022.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2022] 朱生张、阿斯顿张、穆李和亚历克斯·斯莫拉。大型语言模型中的自动思维链提示。*arXiv 预印本 arXiv:2210.03493*，2022年。
- en: 'Zhong et al. [2023] Fangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang, and Yizhou
    Wang. Rspt: reconstruct surroundings and predict trajectory for generalizable
    active object tracking. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 37, pages 3705–3714, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong et al. [2023] 方维中、肖碧、余迪张、魏张和易洲王。Rspt：重建环境并预测轨迹以实现通用的主动目标跟踪。在*AAAI人工智能会议论文集*，第37卷，页码3705–3714，2023年。
- en: 'Zhu et al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world
    environments via large language models with text-based knowledge and memory. *arXiv
    preprint arXiv:2305.17144*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2023] 习近平朱、云涛陈、郝天、晨欣陶、伟杰苏、晨宇杨、高黄、宾李、乐伟陆、肖刚王、余乔、赵翔张和季锋戴。Minecraft中的幽灵：通过具有基于文本的知识和记忆的大语言模型实现对开放世界环境的一般能力代理。*arXiv
    预印本 arXiv:2305.17144*，2023年。
- en: Appendix A Implementation Details
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实施细节
- en: A.1 Rules of Diplomacy Game
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 外交游戏规则
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: You need to occupy as many supply centers as possible. If you occupy 18 or more
    supply centers, you will win the game directly. If you lose all your supply centers,
    you will be eliminated immediately.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你需要占领尽可能多的补给中心。如果你占领18个或更多的补给中心，你将直接赢得游戏。如果你失去所有的补给中心，你将立即被淘汰。
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The units consist of armies and fleets. Armies can only move to adjacent areas,
    while fleets can move to adjacent sea zones or coastal areas and can move along
    the coast.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单位包括陆军和舰队。陆军只能移动到相邻区域，而舰队可以移动到相邻的海域或沿海区域，并可以沿海岸线移动。
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To occupy a supply center, your units must move into that area in the autumn.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要占领一个补给中心，你的单位必须在秋季进入该区域。
- en: •
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When a unit moves to an area, if another unit is in the destination or if other
    units are also moving to that destination, the move fails, resulting in a standoff.
    In such cases, you can seek support from units in adjacent areas to the destination.
    If another unit moves into the region from which support is coming, the support
    is cut off. The unit with the most support moves into the area, while other units
    must retreat to an adjacent province or disband. If there is no place to retreat,
    the unit must disband. Fleets can transport armies across sea zones from one coastal
    region to another. However, if another fleet moves into that sea zone, the transport
    is cut off.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当一个单位移动到一个区域时，如果另一个单位在目的地，或者其他单位也在移动到该目的地，则移动失败，导致僵局。在这种情况下，你可以寻求来自邻近区域的单位的支持。如果另一个单位从支持来源的区域移动过来，支持将被切断。拥有最多支持的单位进入该区域，而其他单位必须退回到相邻的省份或解散。如果没有退路，单位必须解散。舰队可以将陆军从一个沿海区域运输到另一个沿海区域。然而，如果另一个舰队进入该海域，运输将被切断。
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The number of units a country can have cannot exceed the number of supply centers
    it controls. If the number of supply centers decreases, excess units must be disbanded.
    Each autumn, new units can be built at supply centers. Coastal supply centers
    can produce fleets or armies, while others can only produce armies. [[22](#bib.bib22)]
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个国家可以拥有的单位数量不能超过其控制的补给中心数量。如果补给中心数量减少，则必须解散多余的单位。每年秋季，可以在补给中心建立新的单位。沿海补给中心可以生产舰队或陆军，而其他补给中心只能生产陆军。[[22](#bib.bib22)]
- en: A.2 Domain Knowledge
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 领域知识
- en: Richelieu can adopt a strategy of allying with distant countries while attacking
    neighboring ones to occupy adjacent territories and achieve rapid expansion. Richelieu
    should pay attention to the Balance of Power by forming alliances with other countries
    or supporting weaker states to prevent any single country or alliance from becoming
    too powerful. [[12](#bib.bib12)] To this end, Richelieu can also adopt a strategy
    of attacking distant countries while allying with nearby ones, sacrificing short-term
    benefits to avoid the emergence of future hegemonic states that could threaten
    his own survival. When facing multiple enemies, Richelieu can find ways to divide
    other countries and incite wars among them. Whether in offense or defense, Richelieu
    should actively choose suitable allies. Richelieu can also introduce a third party
    to achieve goals such as ceasefire, alliance, or joint attack. To achieve alliances
    or ceasefires, Richelieu can sacrifice some interests to the other party as long
    as the ultimate benefits are greater. Others may lie and deceive [[27](#bib.bib27)];
    their words in negotiations are not binding. Richelieu must avoid being deceived
    or betrayed. At the same time, Richelieu can also actively deceive others to achieve
    his own goals.[[42](#bib.bib42), [2](#bib.bib2)]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**黎塞留**可以采用与远方国家结盟的策略，同时攻击邻国以占领相邻领土，实现快速扩张。黎塞留应注意权力平衡，通过与其他国家结盟或支持较弱的国家来防止任何单一国家或联盟变得过于强大。[[12](#bib.bib12)]
    为此，黎塞留还可以采取攻击远方国家的策略，同时与邻近国家结盟，牺牲短期利益，以避免未来霸权国家的出现，这些国家可能威胁到他的生存。在面对多个敌人时，黎塞留可以找到方法分化其他国家，并煽动它们之间的战争。无论是进攻还是防守，黎塞留都应积极选择合适的盟友。黎塞留还可以引入第三方以实现停火、结盟或联合攻击等目标。为了实现结盟或停火，只要最终利益更大，黎塞留可以牺牲一些利益给对方。其他人可能会撒谎和欺骗[[27](#bib.bib27)]；他们在谈判中的话并不具约束力。黎塞留必须避免被欺骗或背叛。同时，黎塞留也可以积极地欺骗他人以实现自己的目标。[[42](#bib.bib42),
    [2](#bib.bib2)]'
- en: A.3 Prompt Templates
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 提示模板
- en: For the convenience of reproducing the results of the experiments of this paper,
    here we give the prompt template of different modules of Richelieu.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便再现本文的实验结果，这里提供了**黎塞留**不同模块的提示模板。
- en: 1) INIT
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 1) INIT
- en: 1You  will  control  {country}  and  compete  with  six  other  countries  on  the  map  for  supply  centers.2The  map  consists  of  different  regions  and  sea  areas.  Their  adjacency  relationships  are  shown  in  the  matrix.  The  numbers  for  the  regions  and  sea  areas  are  ......3Different  regions  are  occupied  by  different  countries.  The  ownership  of  the  regions  is  shown  in  the  matrix.4The  region  Berlin,  ........  are  supply  centers.5You  need  to  follow  these  rules  ......6To  help  you  achieve  victory,  these  diplomatic  strategies  might  be  of  assistance.  ......
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 1你将控制{country}，并与地图上的六个其他国家争夺供应中心。2地图由不同的区域和海域组成。它们的邻接关系在矩阵中显示。区域和海域的编号是……3不同的区域由不同的国家占据。区域的所有权在矩阵中显示。4区域柏林，……是供应中心。5你需要遵循这些规则……6为了帮助你取得胜利，这些外交策略可能会有所帮助……
- en: 2) Social Reasoning
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 社会推理
- en: 1France  occupies  Portugal  Ruhr,  Paris,  Burgundy,  ......2France  has  armies  in  Brest,  Belgium,  ......  And  France  has  fleets  in  Mid  Atlantic,  England  Channel,  ......3England  ......4......5Based  on  the  current  state,  what  do  you  think  are  the  current  strategic  intentions  of  the  other  countries?6Which  country  do  you  think  needs  to  be  attacked  or  weakened  the  most  right  now?7And  which  country  do  you  think  is  most  suitable  for  you  to  ally  with  in  order  to  deal  with  this  country?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 1法国占据了葡萄牙、鲁尔、巴黎、勃艮第，……2法国在布列斯特、比利时……有军队，并且法国在中大西洋、英吉利海峡……有舰队。3英格兰……4……5基于当前状态，你认为其他国家的战略意图是什么？6你认为现在最需要攻击或削弱哪个国家？7你认为哪个国家最适合与你结盟以对付这个国家？
- en: 3) Planner with Reflection
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 带有反思的规划者
- en: 1In  the  current  state,  with  {ally  and  enemy},  what  sub-goal  do  you  think  should  be  set  for  {country}  ?2I  have  found  some  useful  historical  experiences  for  you.  Please  reflect  on  and  optimize  your  sub-goal  based  on  these  historical  experiences.3The  sub-goal  you  formulated  when  {state}  was  to  {sub-goal}.  The  eventual  result  was  {future}.  The  evaluation  for  this  sub-goal  is  {score}.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 1在当前状态下，与{ally and enemy}，你认为应该为{country}设定什么子目标？2我为你找到了一些有用的历史经验。请基于这些历史经验反思并优化你的子目标。3你在{state}时制定的子目标是{sub-goal}。最终结果是{future}。对这个子目标的评价是{score}。
