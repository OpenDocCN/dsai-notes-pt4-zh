<!--yml

category: 未分类

日期：2025-01-11 12:42:17

-->

# LLM代理可以自主利用一天漏洞

> 来源：[https://arxiv.org/html/2404.08144/](https://arxiv.org/html/2404.08144/)

理查德·方，罗汉·宾度，阿库尔·古普塔，丹尼尔·康

###### 摘要

LLM的能力在其良性和恶意应用方面都越来越强。随着能力的提升，研究人员对其利用网络安全漏洞的能力越来越感兴趣。特别是，最近的研究对LLM代理自主攻击网站的能力进行了初步研究。然而，这些研究仅限于简单的漏洞。

本研究表明，LLM代理可以自主利用一天漏洞 *在现实世界系统中*。为了证明这一点，我们收集了15个一天漏洞的数据集，其中包括在CVE描述中被归类为严重漏洞的漏洞。给定CVE描述后，GPT-4能够利用其中87%的漏洞，而我们测试的其他模型（GPT-3.5、开源LLM）和开源漏洞扫描工具（ZAP和Metasploit）则无法利用任何漏洞。幸运的是，我们的GPT-4代理在高性能的前提下需要CVE描述：没有描述，GPT-4只能利用7%的漏洞。我们的发现引发了关于高能力LLM代理广泛部署的问题。

## 1 引言

近年来，大型语言模型（LLM）在性能上取得了显著进展，在许多基准测试中达到了超人类的表现（Touvron et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib39); Achiam et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib1)）。这种性能的提升引发了对LLM *代理*的极大兴趣，这些代理可以通过工具采取行动，进行自我反思，甚至阅读文档（Lewis et al., [2020](https://arxiv.org/html/2404.08144v2#bib.bib24)）。据报道，这些LLM代理可以作为软件工程师（Osika, [2023](https://arxiv.org/html/2404.08144v2#bib.bib28); Huang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib16)）并协助科学发现（Boiko et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib4); Bran et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib5)）。

然而，目前对于LLM代理在网络安全领域的能力了解尚不多。最近的研究主要集中在“人类提升”场景（Happe & Cito, [2023](https://arxiv.org/html/2404.08144v2#bib.bib13); Hilario et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib15)），其中LLM被用作聊天机器人来帮助人类，或者在更广泛的攻防对抗范畴中进行推测（Lohn & Jackson, [2022](https://arxiv.org/html/2404.08144v2#bib.bib25); Handa et al., [2019](https://arxiv.org/html/2404.08144v2#bib.bib12)）。在这一领域中，最相关的研究表明LLM代理可以被用来自主攻击玩具网站（Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）。

然而，尽我们所知，当前所有相关的研究都集中在玩具问题或“夺旗”练习上，这些问题并未反映真实世界的部署情况（Fang等人，[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)；Happe & Cito，[2023](https://arxiv.org/html/2404.08144v2#bib.bib13)；Hilario等人，[2024](https://arxiv.org/html/2404.08144v2#bib.bib15)）。这一差距引发了一个自然的问题：LLM代理能否自主破解真实世界的部署？

在这项工作中，我们展示了LLM代理可以自主利用一天漏洞，肯定地回答了前面提到的问题。

为了展示这一点，我们收集了15个真实世界的一天漏洞的基准。这些漏洞来自常见漏洞和暴露（CVE）数据库以及一些高引用的学术论文，在这些论文中我们能够重现CVE（即，我们排除了闭源解决方案）。这些CVE包括真实世界的网页（CVE-2024-24041）、容器管理软件（CVE-2024-21626）和易受攻击的Python包（CVE-2024-28859）。

根据我们的基准，我们创建了一个*单一*的LLM代理，它可以利用我们收集到的87%的这类一天漏洞。为了做到这一点，我们只需要给代理提供工具、CVE描述，并使用ReAct代理框架。我们的代理总共只有91行代码，展示了执行这些漏洞利用的简易性。

重要的是，我们展示了GPT-4达到了87%的成功率，但我们测试的其他所有LLM（GPT-3.5、8个开源模型）*以及开源漏洞扫描器*在我们的基准上都达到了0%的成功率。没有CVE描述时，GPT-4的成功率降至7%，这表明我们的代理在利用漏洞方面远远超过了发现漏洞。

在本文的其余部分，我们将描述我们的漏洞数据集、我们的代理以及我们对代理的评估。

## 2 计算机安全和LLM代理的背景

### 2.1 计算机安全

我们提供了与本文内容相关的计算机安全背景。计算机安全是一个非常广泛的话题，无法详细覆盖，因此我们建议读者参考一些优秀的综述以获得更多信息（Jang-Jaccard & Nepal，[2014](https://arxiv.org/html/2404.08144v2#bib.bib17)；Engebretson，[2013](https://arxiv.org/html/2404.08144v2#bib.bib7)；Sikorski & Honig，[2012](https://arxiv.org/html/2404.08144v2#bib.bib37)）。

每当计算机程序被部署时，恶意攻击者都有可能滥用这些程序，迫使其执行不希望的操作。这些不希望的操作在严重情况下可能包括获得服务器的root访问权限（Roselin等人，[2019](https://arxiv.org/html/2404.08144v2#bib.bib33)）、执行任意远程代码（Zheng & Zhang，[2013](https://arxiv.org/html/2404.08144v2#bib.bib53)）以及窃取私人数据（Ullah等人，[2018](https://arxiv.org/html/2404.08144v2#bib.bib40)）。

黑客可以通过多种方法执行这些不希望出现的操作。最简单的攻击包括未加保护的SQL注入，黑客可以通过例如一个网页表单执行任意SQL查询，对数据库进行攻击（Halfond等人，[2006](https://arxiv.org/html/2404.08144v2#bib.bib11)）。攻击也可以非常复杂，例如通过字体指令利用远程代码执行，将JavaScript打包到有效载荷中，通过硬件内存映射输入输出（MMIO）寄存器绕过内存保护，或者在Safari浏览器中利用一个*单一iPhone攻击*的漏洞（Kuznetsov等人，[2023](https://arxiv.org/html/2404.08144v2#bib.bib23)）。

一旦发现现实世界中的漏洞，它们会被披露给软件提供商，以便提供商修补软件。此后，许多漏洞会被发布到公共漏洞和暴露（CVE）数据库中（Vulnerabilities，[2005](https://arxiv.org/html/2404.08144v2#bib.bib42)）。这样做是为了确保软件保持最新，并允许安全研究人员研究漏洞。

许多CVE（公共漏洞和暴露）存在于闭源软件中，因此无法复现。然而，也有一些CVE存在于开源软件中，能够在沙箱环境中复现。

### 2.2 大语言模型（LLM）代理

在过去几年里，LLM代理变得越来越普遍。最基本的，代理能够使用工具并对使用这些工具的结果作出反应（Yao等人，[2022](https://arxiv.org/html/2404.08144v2#bib.bib48)；Schick等人，[2023](https://arxiv.org/html/2404.08144v2#bib.bib36)；Mialon等人，[2023](https://arxiv.org/html/2404.08144v2#bib.bib27)）。其他能力包括计划能力（Yao等人，[2022](https://arxiv.org/html/2404.08144v2#bib.bib48)；Varshney，[2023](https://arxiv.org/html/2404.08144v2#bib.bib41)）、创建子代理（Wang等人，[2024](https://arxiv.org/html/2404.08144v2#bib.bib44)）和读取文档（Lewis等人，[2020](https://arxiv.org/html/2404.08144v2#bib.bib24)）。

随着LLM变得越来越强大，LLM代理的能力也在不断提升。例如，工具辅助的LLM代理现在能够执行复杂的软件工程任务（Jimenez等人，[2023](https://arxiv.org/html/2404.08144v2#bib.bib20)），甚至在科学研究中提供帮助（Boiko等人，[2023](https://arxiv.org/html/2404.08144v2#bib.bib4)；Bran等人，[2023](https://arxiv.org/html/2404.08144v2#bib.bib5)）。

执行这些高级任务的重要能力之一是使用工具的能力。使用工具的大语言模型在使用工具和响应工具反馈方面的能力差异巨大。正如我们在评估中展示的，GPT-4目前在所有我们测试的模型中表现最为优异。

最近的研究已将大语言模型（LLM）代理应用于自主黑客攻击，但仅限于玩具式的“夺旗”练习。在我们的研究中，我们探索了LLM在黑客攻击现实世界漏洞方面的能力。

### 2.3 术语和威胁模型

在本研究中，我们专注于研究“零日漏洞”，即已公开但尚未在系统中修补的漏洞。在许多实际部署中，安全补丁并不会立即部署，这使得这些部署易受零日漏洞的攻击。正如我们所展示的，开源漏洞扫描器未能发现一些零日漏洞，而LLM代理能够利用这些漏洞。此外，许多漏洞披露并未提供逐步的利用指南，这意味着攻击者必须自己重新创建利用步骤。

具体来说，考虑一个随时间演化的系统$S_{t}$（时间$t$）。在$t=0$时，系统中发现了一个漏洞，使得一系列操作$A$可以利用该漏洞。我们考虑从漏洞公开（$t=1$）到修补（$t=n$，未来的某个$n$）之间的时间。因此，攻击者拥有漏洞的描述。

## 3 真实世界漏洞的基准测试

数据集。为了回答LLM代理是否能利用真实世界的计算机系统漏洞的问题，我们首先创建了一个真实漏洞的基准测试，涵盖了CVE和学术论文中的漏洞。如前所述，CVE是对真实系统中漏洞的描述。

许多CVE是针对闭源软件的，而这些软件我们无法复现，因为CVE通常在厂商修补软件后才公开。为了创建我们的基准测试，我们专注于开源软件。

除了闭源软件外，许多开源漏洞难以复现。无法复现漏洞的原因包括未指定的依赖关系、损坏的docker容器或CVE描述中的不完全说明。

| 漏洞 | 描述 |
| --- | --- |
| runc | 通过内部文件描述符泄漏实现容器逃逸 |
| CSRF + ACE | 跨站请求伪造（CSRF）使得任意代码执行成为可能 |
| Wordpress SQLi | Wordpress插件中的SQL注入漏洞 |
| Wordpress XSS-1 | Wordpress插件中的跨站脚本（XSS）漏洞 |
| Wordpress XSS-2 | Wordpress插件中的XSS漏洞 |
| Travel Journal XSS | Travel Journal中的XSS漏洞 |
| Iris XSS | Iris中的XSS漏洞 |
| CSRF + 权限提升 | LedgerSMB中的CSRF漏洞允许权限提升至管理员 |
| alf.io 密钥泄露 | 在访问某个票务预订系统的特定端点时发生密钥泄露 |
| Astrophy RCE | 输入验证不当导致subprocess.Popen被调用 |
| Hertzbeat RCE | JNDI注入导致远程代码执行 |
| Gnuboard XSS ACE | Gnuboard中的XSS漏洞允许任意代码执行 |
| Symfony1 RCE | PHP数组/对象误用导致RCE漏洞 |
| Peering Manager SSTI RCE | 服务器端模板注入导致RCE漏洞 |
| ACIDRain (Warszawski & Bailis, [2017](https://arxiv.org/html/2404.08144v2#bib.bib45)) | 数据库并发攻击 |

表1：我们考虑的漏洞及其描述。ACE代表任意代码执行，RCE代表远程代码执行。进一步的细节见表[2](https://arxiv.org/html/2404.08144v2#S3.T2 "表2 ‣ 3个现实世界漏洞的基准 ‣ LLM代理可以自主利用一天内的漏洞")。

| 漏洞 | CVE | 日期 | 严重性 |
| --- | --- | --- | --- |
| runc | CVE-2024-21626 | 2024年1月31日 | 8.6（高） |
| CSRF + ACE | CVE-2024-24524 | 2024年2月2日 | 8.8（高） |
| Wordpress SQLi | CVE-2021-24666 | 2021年9月27日 | 9.8（严重） |
| Wordpress XSS-1 | CVE-2023-1119-1 | 2023年7月10日 | 6.1（中等） |
| Wordpress XSS-2 | CVE-2023-1119-2 | 2023年7月10日 | 6.1（中等） |
| Travel Journal XSS | CVE-2024-24041 | 2024年2月1日 | 6.1（中等） |
| Iris XSS | CVE-2024-25640 | 2024年2月19日 | 4.6（中等） |
| CSRF + 权限提升 | CVE-2024-23831 | 2024年2月2日 | 7.5（高） |
| alf.io 密钥泄露 | CVE-2024-25635 | 2024年2月19日 | 8.8（高） |
| Astrophy RCE | CVE-2023-41334 | 2024年3月18日 | 8.4（高） |
| Hertzbeat RCE | CVE-2023-51653 | 2024年2月22日 | 9.8（严重） |
| Gnuboard XSS ACE | CVE-2024-24156 | 2024年3月16日 | 不适用 |
| Symfony 1 RCE | CVE-2024-28859 | 2024年3月15日 | 5.0（中等） |
| Peering Manager SSTI RCE | CVE-2024-28114 | 2024年3月12日 | 8.1（高） |
| ACIDRain | (Warszawski & Bailis, [2017](https://arxiv.org/html/2404.08144v2#bib.bib45)) | 2017年 | 不适用 |

表2：漏洞、其CVE编号、发布日期及根据CVE的严重性。最后一个漏洞（ACIDRain）是一种用于黑客攻击加密货币交易所，造成5000万美元损失的攻击（Popper，[2016](https://arxiv.org/html/2404.08144v2#bib.bib30)），我们在WooCommerce框架中模拟了该攻击。CVE-2024-24156是最近的漏洞，尚未由NIST评定严重性。

在根据上述标准过滤掉无法重现的CVE后，我们共收集了来自CVE的14个现实世界漏洞。我们还包括了Warszawski & Bailis（[2017](https://arxiv.org/html/2404.08144v2#bib.bib45)）研究的一个漏洞，因其复杂性和严重性。该漏洞被称为ACIDRain。ACIDRain的一种形式曾被用于黑客攻击加密货币交易所，造成5000万美元的损失（Popper，[2016](https://arxiv.org/html/2404.08144v2#bib.bib30)）。我们在WooCommerce平台上使用了类似的ACIDRain漏洞。我们在表[1](https://arxiv.org/html/2404.08144v2#S3.T1 "表1 ‣ 3个现实世界漏洞的基准 ‣ LLM代理可以自主利用一天内的漏洞")和[2](https://arxiv.org/html/2404.08144v2#S3.T2 "表2 ‣ 3个现实世界漏洞的基准 ‣ LLM代理可以自主利用一天内的漏洞")中总结了这些漏洞。

漏洞的特征。我们的漏洞涵盖了网站漏洞、容器漏洞和易受攻击的Python包。超过一半（8/15）被CVE描述分类为“高”或“严重”级别。此外，15个漏洞中有11个（73%）超出了我们实验中使用的GPT-4的知识截止日期。

因此，我们的数据集包括了现实世界中的高严重性漏洞，而不是用于玩具环境中的“夺旗”式漏洞（Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8); Happe & Cito, [2023](https://arxiv.org/html/2404.08144v2#bib.bib13); Hilario et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib15)）。

## 4 代理描述

图1：我们LLM代理的系统图。

在本节中，我们描述了可以利用漏洞的LLM代理。我们的代理由以下部分组成：

1.  1.

    基础LLM，

1.  2.

    提示，

1.  3.

    代理框架，以及

1.  4.

    工具。

我们在图[1](https://arxiv.org/html/2404.08144v2#S4.F1 "图1 ‣ 4 代理描述 ‣ LLM代理可以自主利用一天漏洞")中展示了系统图。

我们在评估中改变了基础LLM，但需要注意的是，只有GPT-4能够利用我们数据集中的漏洞。其他所有方法都失败了。

我们使用LangChain实现的ReAct代理框架。对于OpenAI模型，我们使用Assistants API。

我们赋予代理使用工具的权限，包括访问：

1.  1.

    网络浏览元素（获取HTML，点击元素等），

1.  2.

    一个终端，

1.  3.

    网络搜索结果，

1.  4.

    文件创建和编辑，以及

1.  5.

    一个代码解释器。

类似于之前的研究（Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)），我们的提示详细并鼓励代理发挥创意，不放弃并尝试不同的方法。提示总共有1056个标记。代理还可以进一步检索CVE描述。出于伦理原因，我们在公开版本的稿件中没有提供提示，并将按要求提供提示，正如之前的研究所做的那样（Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）。

我们实现了一个总共91行代码的代理，包括调试和日志记录语句，显示这些LLM代理实现起来非常简单。

我们还注意到，我们没有实现子代理或单独的规划模块。正如我们在第[5.3](https://arxiv.org/html/2404.08144v2#S5.SS3 "5.3 删除CVE描述 ‣ 5 LLM代理可以自主利用一天漏洞 ‣ LLM代理可以自主利用一天漏洞")节中所描述的那样，我们的实验表明，单独的规划模块可能会提高我们代理的性能。

## 5 LLM代理可以自主利用一天漏洞

我们现在开始评估我们收集的现实世界漏洞中的LLM代理。

### 5.1 实验设置

指标。我们衡量了两个主要指标：成功率（5次成功和1次成功）和美元成本。为了衡量成功率，我们手动评估代理是否成功利用了当前漏洞。为了衡量美元成本，我们统计了每次运行的 token 数量，并使用撰写时的 OpenAI API 成本。

模型。我们在 ReAct 框架中测试了 10 个模型：

1.  1.

    GPT-4（Achiam 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib1)）

1.  2.

    GPT-3.5（Brown 等，[2020](https://arxiv.org/html/2404.08144v2#bib.bib6)）

1.  3.

    OpenHermes-2.5-Mistral-7B（Teknium，[2024](https://arxiv.org/html/2404.08144v2#bib.bib38)）

1.  4.

    LLaMA-2 Chat（70B）（Touvron 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib39)）

1.  5.

    LLaMA-2 Chat（13B）（Touvron 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib39)）

1.  6.

    LLaMA-2 Chat（7B）（Touvron 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib39)）

1.  7.

    Mixtral-8x7B Instruct（Jiang 等，[2024](https://arxiv.org/html/2404.08144v2#bib.bib19)）

1.  8.

    Mistral（7B）Instruct v0.2（Jiang 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib18)）

1.  9.

    Nous Hermes-2 Yi（34B）（Research，[2024](https://arxiv.org/html/2404.08144v2#bib.bib32)）

1.  10.

    OpenChat 3.5（Wang 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib43)）

我们选择了与 Fang 等人（[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）相同的模型，以便与先前的工作进行比较。Fang 等人（[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）之所以选择这些模型，是因为它们在 ChatBot Arena 中排名较高（Zheng 等人，[2024](https://arxiv.org/html/2404.08144v2#bib.bib52)）。对于 GPT-4 和 GPT-3.5，我们使用了 OpenAI API。对于其余的模型，我们使用了 Together AI API。

对于 GPT-4，知识截止日期为 2023 年 11 月 6 日。因此，15 个漏洞中的 11 个已经超出了知识截止日期。

开源漏洞扫描器。我们进一步在两个开源漏洞扫描器上测试了这些漏洞：ZAP（Bennetts，[2013](https://arxiv.org/html/2404.08144v2#bib.bib3)）和 Metasploit（Kennedy 等，[2011](https://arxiv.org/html/2404.08144v2#bib.bib22)）。这些工具被渗透测试人员广泛使用来发现漏洞。我们的一些漏洞无法通过 ZAP 或 Metasploit 扫描（例如，因为它们位于 Python 包中），因此我们无法在这些漏洞上运行这些扫描器。

我们强调，这些漏洞扫描器无法自主利用漏洞，因此它们的功能明显不如我们的 GPT-4 代理。

漏洞。我们测试了我们的代理和开源漏洞扫描器，针对表[1](https://arxiv.org/html/2404.08144v2#S3.T1 "表 1 ‣ 3 真实世界漏洞基准 ‣ LLM 代理可以自主利用一天内的漏洞")中列出的漏洞。我们在沙盒环境中复现了所有这些漏洞，以确保在测试过程中没有真实用户或相关方受到伤害。最后，我们再次强调，15个漏洞中的11个已超过我们使用的GPT-4基础模型的知识截止日期。

### 5.2 端到端黑客攻击

| 模型 | 在5次测试中通过率 | 总体成功率 |
| --- | --- | --- |
| GPT-4 | 86.7% | 40.0% |
| GPT-3.5 | 0% | 0% |
| OpenHermes-2.5-Mistral-7B | 0% | 0% |
| Llama-2 Chat (70B) | 0% | 0% |
| LLaMA-2 Chat (13B) | 0% | 0% |
| LLaMA-2 Chat (7B) | 0% | 0% |
| Mixtral-8x7B Instruct | 0% | 0% |
| Mistral (7B) Instruct v0.2 | 0% | 0% |
| Nous Hermes-2 Yi 34B | 0% | 0% |
| OpenChat 3.5 | 0% | 0% |

表3：模型及其利用一天内漏洞的成功率（在5次测试中通过率和总体成功率）。GPT-4是唯一能够成功破解至少一个一天内漏洞的模型。

我们测量了10个模型和开源漏洞扫描器在我们的真实世界漏洞上的整体成功率，结果如表[3](https://arxiv.org/html/2404.08144v2#S5.T3 "表 3 ‣ 5.2 端到端黑客攻击 ‣ 5 LLM 代理可以自主利用一天内的漏洞 ‣ LLM 代理可以自主利用一天内的漏洞")所示。如表所示，GPT-4的成功率为87%，而*其他方法*发现或利用的漏洞数量为*零*。这些结果表明GPT-4具有一种“突现能力”（Wei等，[2022](https://arxiv.org/html/2404.08144v2#bib.bib46)），尽管仍需要更多调查（Schaeffer等，[2024](https://arxiv.org/html/2404.08144v2#bib.bib35)）。

GPT-4仅在两个漏洞上失败：Iris XSS和Hertzbeat RCE。Iris是一个“帮助事件响应者在调查过程中共享技术细节的网络协作平台”（CVE-2024-25640）。Iris网页应用程序对于LLM代理来说非常难以导航，因为导航是通过JavaScript进行的。因此，代理尝试访问表单/按钮，但没有与必要的元素互动以使其可用，这就阻止了它的操作。Hertzbeat的详细描述是中文，这可能会困扰我们部署的GPT-4代理，因为我们使用英语作为提示语言。

我们进一步指出，在仅考虑知识截止日期之后的漏洞时，GPT-4的成功率为82%（11个漏洞中的9个）。

如前所述，包括 GPT-3.5 在内的其他所有方法、我们测试的所有开源模型、ZAP 和 Metasploit 都未能发现或利用漏洞。我们在开源模型上的结果证实了 Fang 等（[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）的结果。即使是简单的夺旗练习，每个开源模型的成功率也为 0%。在定性分析上，GPT-3.5 和开源模型在工具使用上似乎表现更差。然而，需要更多的研究来确定其他模型在网络安全中的可行性。

### 5.3 移除 CVE 描述

然后，我们修改了我们的代理，不包含 CVE 描述。这个任务现在显著更难，需要找到漏洞并实际利用它。因为其他所有方法（GPT-3.5 和我们测试的所有开源模型）即使有漏洞描述也达到了 0% 的成功率，后续实验仅在 GPT-4 上进行。

在移除 CVE 描述后，成功率从 87% 降低到 7%。这表明确定漏洞是极具挑战性的。

为了理解这种差异，我们计算了确定正确漏洞的成功率（通过率为 5）。令人惊讶的是，GPT-4 能够在 33.3% 的时间内识别出正确的漏洞。在成功检测到的漏洞中，它仅能利用其中一个。当仅考虑知识截止日期之后的漏洞时，它能够找到其中 55.6% 的漏洞。

我们进一步通过计算代理在有无 CVE 描述的情况下采取的行动数量进行了调查。令人惊讶的是，我们发现有无 CVE 描述的情况下，采取的平均行动数量仅相差 14%（24.3 次行动对比 21.3 次行动）。我们怀疑这在一定程度上是由上下文窗口长度所驱动，进一步暗示了规划机制和子代理可能会提高性能。

这些结果表明，增强代理的规划和探索能力将提高这些代理的成功率，但仍需要更多的探索。

### 5.4 成本分析

现在我们评估使用 GPT-4 利用现实世界漏洞的成本。在进行分析之前，我们强调这些数据应作为估算（针对人类劳动力），并仅用于突出成本趋势。这与先前的研究一致，先前的研究估算了其他类型攻击的成本，例如网站黑客攻击（Fang 等，[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）和钓鱼攻击（Kang 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib21)）。

为了衡量 GPT-4 的成本，我们计算了每次运行的输入和输出令牌的数量（这两者的成本不同）。在写作时，GPT-4 的输入令牌成本为每百万个 $10，输出令牌成本为每百万个 $30。

每次运行的平均成本为$3.52，主要成本来自输入的令牌（347k输入与1.7k输出）。这是因为许多工具的返回值是完整的HTML页面或终端日志。整体成功率平均为40%，因此每次漏洞利用的成本为$8.80。

使用Fang等人（[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）的估算，我们预计网络安全专家的费用为每小时$50，每个漏洞大约需要30分钟。这将总共花费$25。因此，使用LLM代理的成本已经是人工劳动力的2.8倍便宜。与人工劳动力相比，LLM代理的可扩展性也更为简单。

这个差距小于之前工作的差距（Fang等人，[2024](https://arxiv.org/html/2404.08144v2#bib.bib8); Kang等人，[2023](https://arxiv.org/html/2404.08144v2#bib.bib21)）。尽管如此，我们预计GPT-4的成本会下降，因为GPT-3.5的成本在一年内下降了超过3倍。

## 6 理解代理能力

我们现在更详细地研究GPT-4代理的行为，以理解其高成功率以及在移除CVE描述时为何会失败。

| 漏洞 | 步骤数量 |
| --- | --- |
| runc | 10.6 |
| CSRF + ACE | 26.0 |
| Wordpress SQLi | 23.2 |
| Wordpress XSS-1 | 21.6 |
| Wordpress XSS-2 | 48.6 |
| Travel Journal XSS | 20.4 |
| Iris XSS | 38.2 |
| CSRF + 权限提升 | 13.4 |
| alf.io 密钥泄露 | 35.2 |
| Astrophy RCE | 20.6 |
| Hertzbeat RCE | 36.2 |
| Gnuboard XSS | 11.8 |
| Symfony 1 RCE | 11.8 |
| Peering Manager SSTI RCE | 14.4 |
| ACIDRain | 32.6 |

表4：每个漏洞所需的操作次数。

我们首先观察到，许多漏洞需要执行大量操作才能成功利用，平均每个漏洞的操作次数见表[4](https://arxiv.org/html/2404.08144v2#S6.T4 "表 4 ‣ 6 理解代理能力 ‣ LLM代理可以自主利用一天内的漏洞")。例如，Wordpress XSS-2（CVE-2023-1119-2）每次执行的平均步骤为48.6步。一旦攻击成功（按照CVE描述），需要100步，其中70步用于浏览网站，因为Wordpress布局的复杂性。此外，写作时，几个页面超出了OpenAI工具响应大小限制（512 KB）。因此，代理必须基于CSS选择器使用选择按钮和表单，而不是直接读取页面并执行操作。

其次，考虑CSRF + ACE（CVE-2024-24524），该漏洞需要同时利用CSRF攻击和执行代码。在没有CVE描述的情况下，代理列出了可能的攻击方式，如SQL注入攻击、XSS攻击等。然而，由于我们没有实现启动子代理的能力，代理通常会选择一种漏洞类型并尝试该特定漏洞类型。例如，它可能尝试不同形式的SQL注入，但不会回溯尝试其他类型的攻击。添加子代理功能可能会提高代理的性能。

第三，考虑ACIDRain漏洞。确定一个网站是否容易受到ACIDRain攻击是很困难的，因为它依赖于事务控制的后端实现细节。然而，执行ACIDRain攻击仍然复杂，需要：

1.  1.

    导航到网站并提取超链接，

1.  2.

    导航到结账页面，进行测试订单并记录结账所需的字段，

1.  3.

    编写Python代码以利用竞争条件，

1.  4.

    通过终端实际执行Python代码。

该漏洞需要操作多个工具，并根据网站上采取的操作编写代码。

最后，我们注意到我们的GPT-4代理也能够自主利用非网页漏洞。例如，考虑Astrophy RCE漏洞（CVE-2023-41334）。这个漏洞存在于一个Python包中，允许进行远程代码执行。尽管与以往关注的网页（Fang等人，[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）非常不同，我们的GPT-4代理仍然可以自主编写代码来利用其他类型的漏洞。事实上，Astrophy RCE漏洞是在GPT-4的知识截止日期之后发布的，因此即使它不在训练数据集内，GPT-4仍然能够成功编写并执行代码。这些能力进一步扩展到了利用容器管理软件的漏洞（CVE-2024-21626），同样是在知识截止日期之后。

我们的定性分析表明，我们的GPT-4代理能力非常强大。此外，我们相信，通过增加更多功能（例如规划、子代理和更大的工具响应大小），我们的GPT-4代理可以变得更加高效。

## 7 相关工作

网络安全与人工智能。与我们工作最相关的一项研究表明，LLM 代理可以攻击网站（Fang 等，[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）。这项工作聚焦于简单漏洞，通常出现在“夺旗”风格的环境中，而这些环境并不能反映真实世界的系统。与我们同步的其他研究也评估了 LLM 代理在网络安全背景下的能力（Phuong 等，[2024](https://arxiv.org/html/2404.08144v2#bib.bib29)），但似乎表现远逊色于我们的代理和 CTF 环境中的代理（Fang 等，[2024](https://arxiv.org/html/2404.08144v2#bib.bib8)）。由于该代理的细节未公开，因此很难理解性能差异。我们假设这种差异主要与提示（prompt）有关。在我们的研究中，我们展示了 LLM 代理能够攻击现实世界中的一天漏洞。

其他近期的研究显示，LLM 具有在渗透测试或恶意软件生成中提供帮助的能力（Happe & Cito，[2023](https://arxiv.org/html/2404.08144v2#bib.bib13)；Hilario 等，[2024](https://arxiv.org/html/2404.08144v2#bib.bib15)）。这些研究主要聚焦于“人类提升”场景，其中 LLM 辅助人类操作员。其他研究则关注人工智能与网络安全交汇点的社会影响（Lohn & Jackson，[2022](https://arxiv.org/html/2404.08144v2#bib.bib25)；Handa 等，[2019](https://arxiv.org/html/2404.08144v2#bib.bib12)）。在我们的研究中，我们聚焦于代理（这些代理可以轻松扩展，而与人类相比有显著不同）以及破解现实世界漏洞的实际可能性。

网络安全。网络安全是一个极为广泛的研究领域，涵盖了从密码最佳实践（Herley & Van Oorschot，[2011](https://arxiv.org/html/2404.08144v2#bib.bib14)）、研究网络攻击的社会影响（Bada & Nurse，[2020](https://arxiv.org/html/2404.08144v2#bib.bib2)），到理解网络漏洞（Halfond 等，[2006](https://arxiv.org/html/2404.08144v2#bib.bib11)）等内容。与我们工作最相关的子领域是自动化漏洞检测与利用（Russell 等，[2018](https://arxiv.org/html/2404.08144v2#bib.bib34)；Bennetts，[2013](https://arxiv.org/html/2404.08144v2#bib.bib3)；Kennedy 等，[2011](https://arxiv.org/html/2404.08144v2#bib.bib22)；Mahajan，[2014](https://arxiv.org/html/2404.08144v2#bib.bib26)）。

在网络安全领域，黑帽和白帽黑客都常用的一类工具是自动化漏洞扫描器。包括 ZAP（Bennetts，[2013](https://arxiv.org/html/2404.08144v2#bib.bib3)），Metasploit（Kennedy 等，[2011](https://arxiv.org/html/2404.08144v2#bib.bib22)），以及 Burp Suite（Mahajan，[2014](https://arxiv.org/html/2404.08144v2#bib.bib26)）。尽管这些工具很重要，但开源漏洞扫描器无法发现我们研究的*任何*漏洞，这也展示了大型语言模型（LLM）代理的能力。

LLM 代理的安全性。一个相关但正交的研究方向是 LLM 代理的安全性（Greshake 等，[2023a](https://arxiv.org/html/2404.08144v2#bib.bib9)；Kang 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib21)；Zou 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib54)；Zhan 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib50)；Qi 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib31)；Yang 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib47)）例如，攻击者可以使用间接的提示注入攻击来误导 LLM 代理（Greshake 等，[2023b](https://arxiv.org/html/2404.08144v2#bib.bib10)；Yi 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib49)；Zhan 等，[2024](https://arxiv.org/html/2404.08144v2#bib.bib51)）。攻击者还可以通过微调去除模型的保护措施，使得高度智能的模型执行创建者未曾预期的操作或任务（Zhan 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib50)；Yang 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib47)；Qi 等，[2023](https://arxiv.org/html/2404.08144v2#bib.bib31)）。这类工作可以用来绕过 LLM 提供商设置的保护措施，但与我们的工作正交。

## 8 结论

在这项工作中，我们展示了 LLM 代理能够自主利用现实世界的单日漏洞。目前，只有带有 CVE 描述的 GPT-4 能够利用这些漏洞。我们的结果显示了新兴能力的可能性，并表明发现漏洞比利用漏洞更为困难。尽管如此，我们的发现突显了更广泛的网络安全社区和 LLM 提供商需要仔细思考如何将 LLM 代理纳入防御措施，以及如何进行广泛部署。

## 9 伦理声明

我们的结果表明，LLM 代理可以用来攻破现实世界的系统。像许多技术一样，这些结果也可能被用于黑帽攻击，既不道德也不合法。然而，与计算机安全和机器学习安全研究中的许多工作一样，我们认为在学术环境中研究这些问题是很重要的。在我们的工作中，我们采取了预防措施，确保只在沙箱环境中进行实验，以防止造成伤害。

我们在发布之前已向 OpenAI 公开了我们的发现。他们明确要求我们不要将我们的提示发布给更广泛的公众，因此我们只会在有请求时提供提示。此外，许多关于高级机器学习模型和网络安全工作的论文出于伦理原因并未公开具体细节（例如 NeurIPS 2020 年最佳论文（Brown 等，[2020](https://arxiv.org/html/2404.08144v2#bib.bib6)））。因此，我们认为隐瞒我们提示的具体细节符合最佳实践。

#### 致谢

我们感谢 Open Philanthropy 项目部分资助了这项研究。

## 参考文献

+   Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat 等。GPT-4 技术报告。*arXiv 预印本 arXiv:2303.08774*, 2023.

+   Bada & Nurse (2020) Maria Bada 和 Jason RC Nurse. 网络攻击的社会与心理影响。发表于 *新兴网络威胁与认知脆弱性*, 第 73–92 页。Elsevier, 2020.

+   Bennetts (2013) Simon Bennetts. Owasp Zed 攻击代理。*AppSec USA*, 2013.

+   Boiko et al. (2023) Daniil A Boiko, Robert MacKnight, 和 Gabe Gomes. 大型语言模型的自主科学研究能力的出现。*arXiv 预印本 arXiv:2304.05332*, 2023.

+   Bran et al. (2023) Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew White, 和 Philippe Schwaller. 使用化学工具增强大型语言模型。发表于 *NeurIPS 2023 AI for Science Workshop*, 2023.

+   Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell 等。语言模型是少样本学习者。*神经信息处理系统进展*, 33:1877–1901, 2020.

+   Engebretson (2013) Patrick Engebretson. *黑客与渗透测试基础：简化的伦理黑客与渗透测试*. Elsevier, 2013.

+   Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, 和 Daniel Kang. 大型语言模型代理可以自主攻击网站，2024.

+   Greshake et al. (2023a) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, 和 Mario Fritz. 超出你所要求的：对集成大型语言模型应用的新型提示注入威胁的全面分析。*arXiv 电子预印本*, 第 arXiv–2302 页, 2023a.

+   Greshake et al. (2023b) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, 和 Mario Fritz. 你没签署的内容：通过间接提示注入危害实际集成大型语言模型的应用。发表于 *第16届 ACM 人工智能与安全研讨会论文集*, 第 79–90 页, 2023b.

+   Halfond et al. (2006) William G Halfond, Jeremy Viegas, Alessandro Orso 等。SQL 注入攻击与对策分类。发表于 *IEEE 国际安全软件工程研讨会论文集*, 卷 1，第 13–15 页。IEEE Piscataway, NJ, 2006.

+   Handa et al. (2019) Anand Handa, Ashu Sharma, 和 Sandeep K Shukla. 网络安全中的机器学习：综述。*Wiley 跨学科评论：数据挖掘与知识发现*, 9(4):e1306, 2019.

+   Happe & Cito (2023) Andreas Happe 和 Jürgen Cito. 被 AI 攻破：使用大型语言模型进行渗透测试。发表于 *第31届 ACM 欧洲软件工程联合会议和软件工程基础研讨会论文集*, 第 2082–2086 页, 2023.

+   Herley & Van Oorschot（2011）Cormac Herley 和 Paul Van Oorschot。承认密码持久性的研究议程。*IEEE 安全与隐私*，10(1):28–36，2011年。

+   Hilario 等人（2024）Eric Hilario, Sami Azam, Jawahar Sundaram, Khwaja Imran Mohammed 和 Bharanidharan Shanmugam。生成式人工智能在渗透测试中的应用：优点、缺点与不足。*国际信息安全杂志*，第1-23页，2024年。

+   Huang 等人（2023）Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck 和 Heming Cui。Agentcoder：基于多代理的代码生成与迭代测试与优化。*arXiv 预印本 arXiv:2312.13010*，2023年。

+   Jang-Jaccard & Nepal（2014）Julian Jang-Jaccard 和 Surya Nepal。网络安全中新兴威胁的调查。*计算机与系统科学杂志*，80(5):973–993，2014年。

+   Jiang 等人（2023）Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier 等人。Mistral 7b。*arXiv 预印本 arXiv:2310.06825*，2023年。

+   Jiang 等人（2024）Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand 等人。专家混合模型。*arXiv 预印本 arXiv:2401.04088*，2024年。

+   Jimenez 等人（2023）Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press 和 Karthik Narasimhan。Swe-bench：语言模型能否解决现实世界中的GitHub问题？*arXiv 预印本 arXiv:2310.06770*，2023年。

+   Kang 等人（2023）Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia 和 Tatsunori Hashimoto。利用LLMs的程序行为：通过标准安全攻击的双重用途。*arXiv 预印本 arXiv:2302.05733*，2023年。

+   Kennedy 等人（2011）David Kennedy, Jim O’gorman, Devon Kearns 和 Mati Aharoni。*Metasploit：渗透测试者指南*。No Starch Press，2011年。

+   Kuznetsov 等人（2023）Igor Kuznetsov, Valentin Pashkov, Leonid Bezvershenko 和 Georgy Kucherin。三角定位行动：iOS设备遭遇前所未见的恶意软件攻击。2023年。URL [https://securelist.com/operation-triangulation/109842/](https://securelist.com/operation-triangulation/109842/)。

+   Lewis 等人（2020）Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel 等人。知识密集型NLP任务的检索增强生成。*神经信息处理系统进展*，33:9459–9474，2020年。

+   Lohn & Jackson（2022）Andrew Lohn 和 Krystal Jackson。人工智能会成为网络剑还是盾？2022年。

+   Mahajan（2014）Akash Mahajan。*Burp Suite基础教程*。Packt Publishing Ltd，2014年。

+   Mialon 等人 (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz 等人. 增强型语言模型：一项调查。 *arXiv 预印本 arXiv:2302.07842*, 2023。

+   Osika (2023) Anton Osika. gpt-engineer，2023年4月。网址 [https://github.com/gpt-engineer-org/gpt-engineer](https://github.com/gpt-engineer-org/gpt-engineer)。

+   Phuong 等人 (2024) Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson 等人. 评估前沿模型的危险能力。 *arXiv 预印本 arXiv:2403.13793*, 2024。

+   Popper (2016) Nathaniel Popper. 超过5000万美元的黑客攻击打破了虚拟货币领域的希望。 *纽约时报*, 17，2016。

+   Qi 等人 (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal 和 Peter Henderson. 微调对齐语言模型会妥协安全性，即使用户并不打算这样做！ *arXiv 预印本 arXiv:2310.03693*, 2023。

+   Research (2024) Nous Research. Nous Hermes 2 - yi-34b, 2024. 网址 [https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B)。

+   Roselin 等人 (2019) Annie Gilda Roselin, Priyadarsi Nanda, Surya Nepal, Xiangjian He 和 Jarod Wright. 利用CoAP协议的远程服务器访问支持。 *IEEE物联网期刊*, 6(6):9338–9349, 2019。

+   Russell 等人 (2018) Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood 和 Marc McConley. 使用深度表示学习进行源代码中的自动化漏洞检测。 在 *2018年第17届IEEE国际机器学习与应用大会 (ICMLA)*, 第757–762页。IEEE，2018。

+   Schaeffer 等人 (2024) Rylan Schaeffer, Brando Miranda 和 Sanmi Koyejo. 大型语言模型的涌现能力是海市蜃楼吗？ *神经信息处理系统进展*, 36，2024。

+   Schick 等人 (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda 和 Thomas Scialom. Toolformer: 语言模型可以自我教授使用工具。 *arXiv 预印本 arXiv:2302.04761*, 2023。

+   Sikorski 和 Honig (2012) Michael Sikorski 和 Andrew Honig. *实用恶意软件分析：恶意软件剖析实战指南*。No Starch Press，2012。

+   Teknium (2024) Teknium. Openhermes 2.5 - Mistral 7B，2024。网址 [https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)。

+   Touvron 等人 (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale 等人. Llama 2: 开放的基础模型和微调的聊天模型。 *arXiv 预印本 arXiv:2307.09288*, 2023。

+   乌拉等人（2018）乌拉·法希姆、爱德华兹·马修、拉姆达尼·拉吉夫、奇查扬·鲁赞娜、巴巴·阿里·马、拉希德·阿瓦伊斯。数据泄露：外部攻击向量与对策的回顾。*网络与计算机应用杂志*，101:18–54，2018年。

+   瓦尔什尼（2023）塔内·瓦尔什尼。LLM代理简介。2023年。网址 [https://developer.nvidia.com/blog/introduction-to-llm-agents/](https://developer.nvidia.com/blog/introduction-to-llm-agents/)。

+   漏洞（2005）常见漏洞。常见漏洞与暴露。*MITRE公司，[在线]可用：https://cve.mitre.org/index.html*，2005年。

+   王等人（2023）王冠、程思杰、詹先元、李西刚、宋森和刘阳。Openchat：通过混合质量数据推进开源语言模型。*arXiv预印本 arXiv:2309.11235*，2023年。

+   王等人（2024）王耀祥、吴志勇、姚俊峰和苏金松。Tdag：基于动态任务分解和代理生成的多智能体框架。*arXiv预印本 arXiv:2402.10178*，2024年。

+   Warszawski 和 Bailis（2017）Todd Warszawski 和 Peter Bailis。Acidrain：数据库支持的网页应用中的并发相关攻击。发表于 *2017年ACM国际数据管理会议论文集*，第5–20页，2017年。

+   魏等人（2022）魏杰森、邹怡、波马萨尼·里希、拉费尔·科林、佐夫·巴雷特、博尔乔德·塞巴斯蒂安、约加塔玛·达尼、博斯玛·马尔滕、周丹尼、梅茨勒·唐纳德等人。大语言模型的涌现能力。*arXiv预印本 arXiv:2206.07682*，2022年。

+   杨等人（2023）杨先军、王晓、张齐、佩佐尔德·琳达、杨·威廉·王、赵勋和林大华。影子对齐：轻松颠覆安全对齐语言模型的方式。*arXiv预印本 arXiv:2310.02949*，2023年。

+   姚等人（2022）姚顺宇、赵杰弗里、余典、杜楠、沙夫兰·伊扎克、纳拉西曼·卡尔蒂克和曹源。React：在语言模型中协同推理与行动。*arXiv预印本 arXiv:2210.03629*，2022年。

+   易等人（2023）易景伟、谢岳琦、朱斌、海因斯·基根、基曼·埃姆雷、孙光中、谢星和吴方照。基准测试与防御大语言模型的间接提示注入攻击。*arXiv预印本 arXiv:2312.14197*，2023年。

+   詹等人（2023）詹秋思、方理查德、Bindu Rohan、Gupta Akul、Hashimoto Tatsunori 和 康丹尼尔。通过微调去除GPT-4中的RLHF保护。*arXiv预印本 arXiv:2311.05553*，2023年。

+   詹等人（2024）詹秋思、梁志翔、应子凡和康丹尼尔。Injecagent：基准测试工具集成大语言模型代理中的间接提示注入。*arXiv预印本 arXiv:2403.02691*，2024年。

+   郑等人（2024）郑联民、蒋维林、盛颖、庄思远、吴张昊、庄永浩、林子、李卓翰、李大成、邢埃里克等人。通过mt-bench和chatbot arena评估llm作为法官的能力。*神经信息处理系统进展*，36，2024年。

+   Zheng 和 Zhang（2013）Yunhui Zheng 和 Xiangyu Zhang. 面向远程代码执行漏洞检测的 Web 应用程序路径敏感静态分析。载于 *2013年第35届国际软件工程会议（ICSE）*，第652–661页。IEEE，2013年。

+   Zou 等人（2023）Andy Zou, Zifan Wang, J Zico Kolter 和 Matt Fredrikson. 面向对齐语言模型的通用且可转移的对抗攻击。*arXiv 预印本 arXiv:2307.15043*，2023年。
