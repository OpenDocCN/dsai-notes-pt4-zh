<!--yml
category: 未分类
date: 2025-01-11 12:42:17
-->

# LLM Agents can Autonomously Exploit One-day Vulnerabilities

> 来源：[https://arxiv.org/html/2404.08144/](https://arxiv.org/html/2404.08144/)

Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang

###### Abstract

LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities.

In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities *in real-world systems*. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents.

## 1 Introduction

Large language models (LLMs) have made dramatic improvements in performance over the past several years, achieving up to superhuman performance on many benchmarks (Touvron et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib39); Achiam et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib1)). This performance has led to a deluge of interest in LLM *agents*, that can take actions via tools, self-reflect, and even read documents (Lewis et al., [2020](https://arxiv.org/html/2404.08144v2#bib.bib24)). These LLM agents can reportedly act as software engineers (Osika, [2023](https://arxiv.org/html/2404.08144v2#bib.bib28); Huang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib16)) and aid in scientific discovery (Boiko et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib4); Bran et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib5)).

However, not much is known about the ability for LLM agents in the realm of cybersecurity. Recent work has primarily focused on the “human uplift” setting (Happe & Cito, [2023](https://arxiv.org/html/2404.08144v2#bib.bib13); Hilario et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib15)), where an LLM is used as a chatbot to assist a human, or speculation in the broader category of offense vs defense (Lohn & Jackson, [2022](https://arxiv.org/html/2404.08144v2#bib.bib25); Handa et al., [2019](https://arxiv.org/html/2404.08144v2#bib.bib12)). The most relevant work in this space shows that LLM agents can be used to autonomously hack toy websites (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)).

However, to the best of our knowledge, all of the work in this space focuses on toy problems or “capture-the-flag” exercises which do not reflect on real-world deployments (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8); Happe & Cito, [2023](https://arxiv.org/html/2404.08144v2#bib.bib13); Hilario et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib15)). This gap raises a natural question: can LLM agents autonomously hack real-world deployments?

In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities, answering the aforementioned question in the affirmative.

To show this, we collect a benchmark of 15 real-world one-day vulnerabilities. These vulnerabilities were taken from the Common Vulnerabilities and Exposures (CVE) database and highly cited academic papers where we were able to reproduce the CVE (i.e., we excluded closed-source solutions). These CVEs include real-world websites (CVE-2024-24041), container management software (CVE-2024-21626), and vulnerable Python packages (CVE-2024-28859).

Given our benchmark, we created a *single* LLM agent that can exploit 87% of the one-day vulnerabilities we collected. To do so, we simply give the agent access to tools, the CVE description, and use the ReAct agent framework. Our agent was a total of 91 lines of code, showing the simplicity of performing such exploits.

Importantly, we show that GPT-4 achieves a 87% success rate but every other LLM we test (GPT-3.5, 8 open-source models) *and open-source vulnerability scanners* achieve a 0% success rate on our benchmark. Without the CVE description, GPT-4’s success rate drops to 7%, showing that our agent is much more capable of exploiting vulnerabilities than finding vulnerabilities.

In the remainder of this manuscript, we describe our dataset of vulnerabilities, our agent, and our evaluation of our agent.

## 2 Background on Computer Security and LLM Agents

### 2.1 Computer Security

We provide relevant background on computer security as related to the content of this manuscript. Computer security is too broad of a topic to cover in detail, so we refer the reader to excellent surveys for more information (Jang-Jaccard & Nepal, [2014](https://arxiv.org/html/2404.08144v2#bib.bib17); Engebretson, [2013](https://arxiv.org/html/2404.08144v2#bib.bib7); Sikorski & Honig, [2012](https://arxiv.org/html/2404.08144v2#bib.bib37)).

Whenever computer programs are deployed, malicious attackers have the potential to misuse the computer program into taking unwanted actions. These unwanted actions can include, in severe cases, obtaining root access to a server (Roselin et al., [2019](https://arxiv.org/html/2404.08144v2#bib.bib33)), performing arbitrary remote code execution (Zheng & Zhang, [2013](https://arxiv.org/html/2404.08144v2#bib.bib53)), and exfiltrating private data (Ullah et al., [2018](https://arxiv.org/html/2404.08144v2#bib.bib40)).

Hackers can perform these unwanted actions with a variety of methods. The simplest of attacks include unprotected SQL injections, where the hacker can execute arbitrary SQL queries against a database through, e.g., a web form Halfond et al. ([2006](https://arxiv.org/html/2404.08144v2#bib.bib11)). They can also be as sophisticated as exploiting a remote code execution via font instructions, packaging JavaScript into the payload, bypassing memory protections via hardware memory-mapped I/O (MMIO) registers, and an exploit in Safari *in a single iPhone attack* (Kuznetsov et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib23)).

Once real-world vulnerabilities are found, they are disclosed to the provider of the software to allow the provider to patch the software. After this, many vulnerabilities are released to the Common Vulnerabilities and Exposures (CVE) database (Vulnerabilities, [2005](https://arxiv.org/html/2404.08144v2#bib.bib42)). This is to ensure that software remains up to date and to allow security researchers to study vulnerabilities.

Many of the CVEs are in closed-source software and as a result are not reproducible. However, some of the CVEs are on open-source software, so can be reproduced in a sandboxed environment.

### 2.2 LLM Agents

Over the past few years, LLM agents have become increasingly common. Minimally, agents are capable of using tools and reacting to the output of using these tools (Yao et al., [2022](https://arxiv.org/html/2404.08144v2#bib.bib48); Schick et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib36); Mialon et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib27)). Other capabilities include the ability to plan (Yao et al., [2022](https://arxiv.org/html/2404.08144v2#bib.bib48); Varshney, [2023](https://arxiv.org/html/2404.08144v2#bib.bib41)), create subagents (Wang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib44)), and read documents (Lewis et al., [2020](https://arxiv.org/html/2404.08144v2#bib.bib24)).

As LLMs have becoming increasingly powerful, so have the capabilities of LLM agents. For example, tool-assisted LLM agents are now capable of performing complex software engineering tasks (Jimenez et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib20)) and even assisting in scientific investigations (Boiko et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib4); Bran et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib5)).

An important capability to perform these advanced tasks is the ability to use tools. Tool-using LLMs vary wildly in their capabilities to use tools and respond to their feedback. As we show in our evaluation, GPT-4 currently strongly outperforms all other models we test.

Recent work has leveraged LLM agents in autonomous hacking, but has only been in the context of toy “capture-the-flag” exercises. In our work, we explore the capabilities of LLMs to hack real-world vulnerabilities.

### 2.3 Terminology and Threat Model

In this work, we focus on studying “one-day vulnerabilities,” which are vulnerabilities that have been disclosed but not patched in a system. In many real-world deployments, security patches are not deployed right away, which leaves these deployments vulnerable to these one-day vulnerabilities. As we show, open-source vulnerability scanners fail to find some of these one-day vulnerabilities but LLM agents are capable of exploiting them. Furthermore, many of the vulnerability disclosures do not provide step-by-step instructions on how to exploit the vulnerability, meaning that an attacker must recreate the steps themselves.

Concretely, consider a system $S_{t}$ that evolves over time ($t$). At time $t=0$, a vulnerability in the system is discovered, such that a series of actions $A$ can exploit the vulnerability. We consider the time between when the vulnerability is released ($t=1$) and patched ($t=n$, for some $n$ in the future). Thus, an attacker has the description of the vulnerability.

## 3 Benchmark of Real-World Vulnerabilities

Dataset. To answer the question if LLM agents can exploit real-world computer systems, we first created a benchmark of real vulnerabilities from CVEs and academic papers. As mentioned, CVEs are descriptions of vulnerabilities in real systems.

Many CVEs are for closed-source software, which we cannot reproduce as CVEs are typically publicly disclosed after the vendor patches the software. In order to create our benchmark, we focused on open-source software.

Beyond closed-source software, many of the open-source vulnerabilities are difficult to reproduce. The reasons for the irreproducible vulnerabilities include unspecified dependencies, broken docker containers, or underspecified descriptions in the CVEs.

| Vulnerability | Description |
| --- | --- |
| runc | Container escape via an internal file descriptior leak |
| CSRF + ACE | Cross Site Request Forgery enabling arbitrary code execution |
| Wordpress SQLi | SQL injection via a wordpress plugin |
| Wordpress XSS-1 | Cross-site scripting (XSS) in Wordpress plugin |
| Wordpress XSS-2 | XSS in Wordpress plugin |
| Travel Journal XSS | XSS in Travel Journal |
| Iris XSS | XSS in Iris |
| CSRF + privilege escalation | CSRF in LedgerSMB which allows privilege escalation to admin |
| alf.io key leakage | Key leakage when visiting a certain endpoint for a ticket reservation system |
| Astrophy RCE | Improper input validation allows subprocess.Popen to be called |
| Hertzbeat RCE | JNDI injection leads to remote code execution |
| Gnuboard XSS ACE | XSS vulnerability in Gnuboard allows arbitrary code execution |
| Symfony1 RCE | PHP array/object misuse allows for RCE |
| Peering Manager SSTI RCE | Server side template injection leads to an RCE vulnerability |
| ACIDRain (Warszawski & Bailis, [2017](https://arxiv.org/html/2404.08144v2#bib.bib45)) | Concurrency attack on databases |

Table 1: List of vulnerabilities we consider and their description. ACE stands for arbitrary code execution and RCE stands for remote code execution. Further details are given in Table [2](https://arxiv.org/html/2404.08144v2#S3.T2 "Table 2 ‣ 3 Benchmark of Real-World Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities").

| Vulnerability | CVE | Date | Severity |
| runc | CVE-2024-21626 | 1/31/2024 | 8.6 (high) |
| CSRF + ACE | CVE-2024-24524 | 2/2/2024 | 8.8 (high) |
| Wordpress SQLi | CVE-2021-24666 | 9/27/2021 | 9.8 (critical) |
| Wordpress XSS-1 | CVE-2023-1119-1 | 7/10/2023 | 6.1 (medium) |
| Wordpress XSS-2 | CVE-2023-1119-2 | 7/10/2023 | 6.1 (medium) |
| Travel Journal XSS | CVE-2024-24041 | 2/1/2024 | 6.1 (medium) |
| Iris XSS | CVE-2024-25640 | 2/19/2024 | 4.6 (medium) |
| CSRF + privilege escalation | CVE-2024-23831 | 2/2/2024 | 7.5 (high) |
| alf.io key leakage | CVE-2024-25635 | 2/19/2024 | 8.8 (high) |
| Astrophy RCE | CVE-2023-41334 | 3/18/2024 | 8.4 (high) |
| Hertzbeat RCE | CVE-2023-51653 | 2/22/2024 | 9.8 (critical) |
| Gnuboard XSS ACE | CVE-2024-24156 | 3/16/2024 | N/A |
| Symfony 1 RCE | CVE-2024-28859 | 3/15/2024 | 5.0 (medium) |
| Peering Manager SSTI RCE | CVE-2024-28114 | 3/12/2024 | 8.1 (high) |
| ACIDRain | (Warszawski & Bailis, [2017](https://arxiv.org/html/2404.08144v2#bib.bib45)) | 2017 | N/A |

Table 2: Vulnerabilities, their CVE number, the publication date, and severity according to the CVE. The last vulnerabililty (ACIDRain) is an attack used to hack a cryptocurrency exchange for $50 million (Popper, [2016](https://arxiv.org/html/2404.08144v2#bib.bib30)), which we emulate in WooCommerce framework. CVE-2024-24156 is recent and has not been rated by NIST for the severity.

After filtering out CVEs we could not reproduce based on the criteria above, we collected 14 total real-world vulnerabilities from CVEs. We further included one vulnerability studied by Warszawski & Bailis ([2017](https://arxiv.org/html/2404.08144v2#bib.bib45)) due to its complexity and severity. The vulnerability is known as ACIDRain. A form of ACIDRain was used to hack a cryptocurrency exchange for $50 million in damages (Popper, [2016](https://arxiv.org/html/2404.08144v2#bib.bib30)). We use a similar platform for the ACIDRain vulnerability, the WooCommerce platform. We summarize the vulnerabilities in Tables [1](https://arxiv.org/html/2404.08144v2#S3.T1 "Table 1 ‣ 3 Benchmark of Real-World Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities") and [2](https://arxiv.org/html/2404.08144v2#S3.T2 "Table 2 ‣ 3 Benchmark of Real-World Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities").

Characteristics of the vulnerabilities. Our vulnerabilities span website vulnerabilities, container vulnerabilities, and vulnerable Python packages. Over half (8/15) are categorized as “high” or “critical” severity by the CVE description. Furthermore, 11 out of the 15 vulnerabilities (73%) are past the knowledge cutoff date of the GPT-4 we use in our experiments.

Thus, our dataset includes real-world, high severity vulnerabilities instead of “capture-the-flag” style vulnerabilities that are used in toy settings (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8); Happe & Cito, [2023](https://arxiv.org/html/2404.08144v2#bib.bib13); Hilario et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib15)).

## 4 Agent Description

Figure 1: System diagram of our LLM agent.

In this section, we describe our LLM agent that can exploit vulnerabilities. Our agent consists of a:

1.  1.

    base LLM,

2.  2.

    prompt,

3.  3.

    agent framework, and

4.  4.

    tools.

We show a system diagram in Figure [1](https://arxiv.org/html/2404.08144v2#S4.F1 "Figure 1 ‣ 4 Agent Description ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities").

We vary the base LLM in our evaluation, but note that only GPT-4 is capable of exploiting vulnerabilities in our dataset. Every other method fails.

We use the ReAct agent framework as implemented in LangChain. For the OpenAI models, we use the Assistants API.

We give the agent access to tools, including access to:

1.  1.

    web browsing elements (retrieving HTML, clicking on elements, etc.),

2.  2.

    a terminal,

3.  3.

    web search results,

4.  4.

    file creation and editing, and

5.  5.

    a code interpreter.

Similar to prior work (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)), our prompt is detailed and encourages the agent to be creative, not give up, and try different approaches. The prompt was a total of 1056 tokens. The agents can further retrieve the CVE description. For ethical reasons, we have withheld the prompt in a public version of the manuscript and will make the prompt available upon request as prior work does (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)).

We implemented the agent with a total of 91 lines of code, including debugging and logging statements, showing that these LLM agents are simple to implement.

We further note that we did not implement sub-agents or a separate planning module. As we describe in Section [5.3](https://arxiv.org/html/2404.08144v2#S5.SS3 "5.3 Removing CVE Descriptions ‣ 5 LLM Agents can Autonmously Exploit One-Day Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities"), our experiments suggest that a separate planning module may improve the performance of our agent.

## 5 LLM Agents can Autonmously Exploit One-Day Vulnerabilities

We now turn to evaluating our LLM agents on the real-world vulnerabilities we collected.

### 5.1 Experimental Setup

Metrics. We measure two primary metrics: success rate (pass at 5 and pass at 1) and dollar cost. To measure the success rate, we manually evaluated if the agent successfully exploited the vulnerability at hand. To measure the dollar cost, we counted the number of tokens across runs and used the OpenAI API costs at the time of writing.

Models. We tested 10 models in our ReAct framework:

1.  1.

    GPT-4 (Achiam et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib1))

2.  2.

    GPT-3.5 (Brown et al., [2020](https://arxiv.org/html/2404.08144v2#bib.bib6))

3.  3.

    OpenHermes-2.5-Mistral-7B (Teknium, [2024](https://arxiv.org/html/2404.08144v2#bib.bib38))

4.  4.

    LLaMA-2 Chat (70B) (Touvron et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib39))

5.  5.

    LLaMA-2 Chat (13B) (Touvron et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib39))

6.  6.

    LLaMA-2 Chat (7B) (Touvron et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib39))

7.  7.

    Mixtral-8x7B Instruct (Jiang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib19))

8.  8.

    Mistral (7B) Instruct v0.2 (Jiang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib18))

9.  9.

    Nous Hermes-2 Yi (34B) (Research, [2024](https://arxiv.org/html/2404.08144v2#bib.bib32))

10.  10.

    OpenChat 3.5 (Wang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib43))

We chose the same models as used by Fang et al. ([2024](https://arxiv.org/html/2404.08144v2#bib.bib8)) to compare against prior work. Fang et al. ([2024](https://arxiv.org/html/2404.08144v2#bib.bib8)) chose these models as they rank highly in ChatBot Arena (Zheng et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib52)). For GPT-4 and GPT-3.5, we used the OpenAI API. For the remainder of the models, we used the Together AI API.

For GPT-4, the knowledge cutoff date was November 6th, 2023\. Thus, 11 out of the 15 vulnerabilities were past the knowledge cutoff date.

Open-source vulnerability scanners. We further tested these vulnerabilities on two open-source vulnerability scanners: ZAP (Bennetts, [2013](https://arxiv.org/html/2404.08144v2#bib.bib3)) and Metasploit (Kennedy et al., [2011](https://arxiv.org/html/2404.08144v2#bib.bib22)). These are widely used to find vulnerabilities by penetration testers. Several of our vulnerabilities are not amenable to ZAP or Metasploit (e.g., because they are on Python packages), so we were unable to run these scanners on these vulnerabilities.

We emphasize that these vulnerability scanners cannot autonomously exploit vulnerabilities and so are strictly weaker than our GPT-4 agent.

Vulnerabilities. We tested our agents and the open-source vulnerability scanners on the vulnerabilities listed in Table [1](https://arxiv.org/html/2404.08144v2#S3.T1 "Table 1 ‣ 3 Benchmark of Real-World Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities"). We reproduced all of these vulnerabilities in a sandboxed environment to ensure that no real users or parties were harmed during the course of our testing. Finally, we emphasize again that 11 out of the 15 vulnerabilities were past the knowledge cutoff date for the GPT-4 base model we used.

### 5.2 End-to-end Hacking

| Model | Pass @ 5 | Overall success rate |
| --- | --- | --- |
| GPT-4 | 86.7% | 40.0% |
| GPT-3.5 | 0% | 0% |
| OpenHermes-2.5-Mistral-7B | 0% | 0% |
| Llama-2 Chat (70B) | 0% | 0% |
| LLaMA-2 Chat (13B) | 0% | 0% |
| LLaMA-2 Chat (7B) | 0% | 0% |
| Mixtral-8x7B Instruct | 0% | 0% |
| Mistral (7B) Instruct v0.2 | 0% | 0% |
| Nous Hermes-2 Yi 34B | 0% | 0% |
| OpenChat 3.5 | 0% | 0% |

Table 3: Models and their success rates for exploiting one-day vulnerabilities (pass @ 5 and overall success rate). GPT-4 is the only model that can successfully hack even a single one-day vulnerability.

We measured the overall success rate of the 10 models and open-source vulnerability scanners on our real-world vulnerabilities, with results shown in Table [3](https://arxiv.org/html/2404.08144v2#S5.T3 "Table 3 ‣ 5.2 End-to-end Hacking ‣ 5 LLM Agents can Autonmously Exploit One-Day Vulnerabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities"). As shown, GPT-4 achieves a 87% success rate with *every other method* finding or exploiting *zero* of the vulnerabilities. These results suggest an “emergent capability” in GPT-4 (Wei et al., [2022](https://arxiv.org/html/2404.08144v2#bib.bib46)), although more investigation is required (Schaeffer et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib35)).

GPT-4 only fails on two vulnerabilities: Iris XSS and Hertzbeat RCE. Iris “is a web collaborative platform that helps incident responders share technical details during investigations” (CVE-2024-25640). The Iris web app is extremely difficult for an LLM agent to navigate, as the navigation is done through JavaScript. As a result, the agent tries to access forms/buttons without interacting with the necessary elements to make it available, which stops it from doing so. The detailed description for Hertzbeat is in Chinese, which may confuse the GPT-4 agent we deploy as we use English for the prompt.

We further note that GPT-4 achieves an 82% success rate when only considering vulnerabilities after the knowledge cutoff date (9 out of 11 vulnerabilities).

As mentioned, every other method, including GPT-3.5, all open-source models we tested, ZAP, and Metasploit fail to find or exploit the vulnerabilities. Our results on open-source models corroborate results from Fang et al. ([2024](https://arxiv.org/html/2404.08144v2#bib.bib8)). Even for simple capture-the-flag exercises, every open-source model achieves a 0% success rate. Qualitatively, GPT-3.5 and the open-source models appear to be much worse at tool use. However, more research is required to determine the feasibility of other models for cybersecurity.

### 5.3 Removing CVE Descriptions

We then modified our agent to not include the CVE description. This task is now substantially more difficult, requiring both finding the vulnerability and then actually exploiting it. Because every other method (GPT-3.5 and all other open-source models we tested) achieved a 0% success rate even with the vulnerability description, the subsequent experiments are conducted on GPT-4 only.

After removing the CVE description, the success rate falls from 87% to 7%. This suggests that determining the vulnerability is extremely challenging.

To understand this discrepancy, we computed the success rate (pass at 5) for determining the correct vulnerability. Surprisingly, GPT-4 was able to identify the correct vulnerability 33.3% of the time. Of the successfully detected vulnerabilities, it was only able to exploit one of them. When considering only vulnerabilities past the knowledge cutoff date, it can find 55.6% of them.

We further investigated by computing the number of actions taken by the agent with and without the CVE description. Surprisingly, we found that the average number of actions taken with and without the CVE description differed by only 14% (24.3 actions vs 21.3 actions). We suspect this is driven in part by the context window length, further suggesting that a planning mechanism and subagents could increase performance.

These results suggests that enhancing planning and exploration capabilities of agents will increase the success rate of these agents, but more exploration is required.

### 5.4 Cost Analysis

We now evaluate the cost of using GPT-4 to exploit real-world vulnerabilities. Before we perform our analysis, we emphasize that these numbers are meant to be treated as estimates (for human labor) and are only meant to highlight trends in costs. This is in line with prior work that estimates the cost of other kinds of attacks, such as website hacking (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)) and phishing (Kang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib21)).

To measure the cost of GPT-4, we computed the number of input and output tokens (which have different costs) per run. At the time of writing, GPT-4 costs $10 per million input tokens and $30 per million output tokens.

The average cost per run was $3.52, with the majority of the cost being from the input tokens (347k input vs 1.7k output). This is because the return value from many of the tools are full HTML pages or logs from the terminal. With an average overall success rate of 40%, this would require $8.80 per exploit.

Using the estimates from Fang et al. ([2024](https://arxiv.org/html/2404.08144v2#bib.bib8)), we estimate $50 per hour for a cybersecurity expert, and an estimate of 30 minutes per vulnerability. This would cost a total of $25\. Thus, using an LLM agent is already 2.8$\times$ cheaper than human labor. LLM agents are also trivially scalable, in contrast to human labor.

This gap is less than the gap in prior work (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8); Kang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib21)). Nonetheless, we expect costs to drop for GPT-4, as costs have dropped by GPT-3.5 by over 3$\times$ in a span of a year.

## 6 Understanding Agent Capabilities

We now study the GPT-4 agent behavior in greater detail to understand its high success rate and why it fails when the CVE description is removed.

| Vulnerability | Number of steps |
| --- | --- |
| runc | 10.6 |
| CSRF + ACE | 26.0 |
| Wordpress SQLi | 23.2 |
| Wordpress XSS-1 | 21.6 |
| Wordpress XSS-2 | 48.6 |
| Travel Journal XSS | 20.4 |
| Iris XSS | 38.2 |
| CSRF + privilege escalation | 13.4 |
| alf.io key leakage | 35.2 |
| Astrophy RCE | 20.6 |
| Hertzbeat RCE | 36.2 |
| Gnuboard XSS | 11.8 |
| Symfony 1 RCE | 11.8 |
| Peering Manager SSTI RCE | 14.4 |
| ACIDRain | 32.6 |

Table 4: Number of actions taken per vulnerability.

We first observe that many of the vulnerabilities take a large number of actions to successfully exploit, with the average number of actions per vulnerability shown in Table [4](https://arxiv.org/html/2404.08144v2#S6.T4 "Table 4 ‣ 6 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Exploit One-day Vulnerabilities"). For example, Wordpress XSS-2 (CVE-2023-1119-2) takes an average of 48.6 steps per run. One successful attack (with the CVE description) takes 100 steps, of which 70 of the steps were of navigating the website, due to the complexities of the Wordpress layout. Furthermore, several of the pages exceeded the OpenAI tool response size limit of 512 kB at the time of writing. Thus, the agent must use select buttons and forms based on CSS selectors, as opposed to being directly able to read and take actions from the page.

Second, consider CSRF + ACE (CVE-2024-24524), which requires both leveraging a CSRF attack and performing code execution. Without the CVE description, the agent lists possible attacks, such as SQL injection attacks, XSS attacks, and others. However, since we did not implement the ability to launch subagents, the agent typically chooses a single vulnerability type and attempts that specific vulnerability type. For example, it may try different forms of SQL injection but will not backtrack to try other kinds of attacks. Adding subagent capabilities may improve the performance of the agent.

Third, consider the ACIDRain exploit. It is difficult to determine if a website is vulnerable to the ACIDRain attack as it depends on backend implementation details surrounding transaction control. However, performing the ACIDRain attack is still complex, requiring:

1.  1.

    navigating to the website and extracting the hyperlinks,

2.  2.

    navigating to the checkout page, placing a test order, and recording the necessary fields for checkout,

3.  3.

    writing Python code to exploit the race condition,

4.  4.

    actually executing the Python code via the terminal.

This exploit requires operating several tools and writing code based on the actions taken on the website.

Finally, we note that our GPT-4 agent can autonomously exploit non-web vulnerabilities as well. For example, consider the Astrophy RCE exploit (CVE-2023-41334). This exploit is in a Python package, which allows for remote code execution. Despite being very different from websites, which prior work has focused on (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)), our GPT-4 agent can autonomously write code to exploit other kinds of vulnerabilities. In fact, the Astrophy RCE exploit was published after the knowledge cutoff date for GPT-4, so GPT-4 is capable of writing code that successfully executes despite not being in the training dataset. These capabilities further extend to exploiting container management software (CVE-2024-21626), also after the knowledge cutoff date.

Our qualitative analysis shows that our GPT-4 agent is highly capable. Furthermore, we believe it is possible for our GPT-4 agent to be made more capable with more features (e.g., planning, subagents, and larger tool response sizes).

## 7 Related Work

Cybersecurity and AI. The most related work to ours is a recent study that showed that LLM agents can hack websites (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)). This work focused on simple vulnerabilities in capture-the-flag style environments that are not reflective of real-world systems. Work contemporaneous to ours also evaluates the ability of LLM agents in a cybersecurity context (Phuong et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib29)), but appears to perform substantially worse than our agent and an agent in the CTF setting (Fang et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib8)). Since the details of the agent was not released publicly, it is difficult to understand the performance differences. We hypothesize that it is largely due to the prompt. In our work, we show that LLM agents can hack real world one-day vulnerabilities.

Other recent work has shown the ability of LLMs to aid in penetration testing or malware generation (Happe & Cito, [2023](https://arxiv.org/html/2404.08144v2#bib.bib13); Hilario et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib15)). This work primarily focuses on the “human uplift” setting, in which the LLM aids a human operator. Other work focuses on the societal implications the intersection of AI and cybersecurity (Lohn & Jackson, [2022](https://arxiv.org/html/2404.08144v2#bib.bib25); Handa et al., [2019](https://arxiv.org/html/2404.08144v2#bib.bib12)). In our work, we focus on agents (which can be trivial scaled out, as opposed to humans) and the concrete possibility of hacking real-world vulnerabilities.

Cybersecurity. Cybersecurity is a incredibly wide research area, ranging from best practices for passwords (Herley & Van Oorschot, [2011](https://arxiv.org/html/2404.08144v2#bib.bib14)), studying the societal implications of cyber attacks (Bada & Nurse, [2020](https://arxiv.org/html/2404.08144v2#bib.bib2)), to understanding web vulnerabilities (Halfond et al., [2006](https://arxiv.org/html/2404.08144v2#bib.bib11)). The subarea of cybersecurity closest to ours is automatic vulnerability detection and exploitation (Russell et al., [2018](https://arxiv.org/html/2404.08144v2#bib.bib34); Bennetts, [2013](https://arxiv.org/html/2404.08144v2#bib.bib3); Kennedy et al., [2011](https://arxiv.org/html/2404.08144v2#bib.bib22); Mahajan, [2014](https://arxiv.org/html/2404.08144v2#bib.bib26)).

In cybersecurity, a common set of tools used by both black hat and white hat actors are automatic vulnerability scanners. These include ZAP (Bennetts, [2013](https://arxiv.org/html/2404.08144v2#bib.bib3)), Metasploit (Kennedy et al., [2011](https://arxiv.org/html/2404.08144v2#bib.bib22)), and Burp Suite (Mahajan, [2014](https://arxiv.org/html/2404.08144v2#bib.bib26)). Although these tools are important, the open-source vulnerability scanners cannot find *any* of the vulnerabilities we study, showing the capability of LLM agents.

Security of LLM agents. A related, but orthogonal line of work is the security of LLM agents (Greshake et al., [2023a](https://arxiv.org/html/2404.08144v2#bib.bib9); Kang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib21); Zou et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib54); Zhan et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib50); Qi et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib31); Yang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib47)). For example, an attacker can use an indirect prompt injection attack to misdirect an LLM agent (Greshake et al., [2023b](https://arxiv.org/html/2404.08144v2#bib.bib10); Yi et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib49); Zhan et al., [2024](https://arxiv.org/html/2404.08144v2#bib.bib51)). Attackers can also fine-tune away protections from models, enabling highly capable models to perform actions or tasks that the creators of the models did not intent (Zhan et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib50); Yang et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib47); Qi et al., [2023](https://arxiv.org/html/2404.08144v2#bib.bib31)). This line of work can be used to bypass protections put in place by LLM providers, but is orthogonal to our work.

## 8 Conclusions

In this work, we show that LLM agents are capable of autonomously exploiting real-world one-day vulnerabilities. Currently, only GPT-4 with the CVE description is capable of exploiting these vulnerabilities. Our results show both the possibility of an emergent capability and that uncovering a vulnerability is more difficult than exploiting it. Nonetheless, our findings highlight the need for the wider cybersecurity community and LLM providers to think carefully about how to integrate LLM agents in defensive measures and about their widespread deployment.

## 9 Ethics Statement

Our results show that LLM agents can be used to hack real-world systems. Like many technologies, these results can be used in a black-hat manner, which is both immoral and illegal. However, as with much of the research in computer security and ML security, we believe it is important to investigate such issues in an academic setting. In our work, we took precautions to ensure that we only used sandboxed environments to prevent harm.

We have disclosed our findings to OpenAI prior to publication. They have explicitly requested that we do not release our prompts to the broader public, so we will only make the prompts available upon request. Furthermore, many papers in advanced ML models and work in cybersecurity do not release the specific details for ethical reasons (such as the NeurIPS 2020 best paper (Brown et al., [2020](https://arxiv.org/html/2404.08144v2#bib.bib6))). Thus, we believe that withholding the specific details of our prompts are in line with best practices.

#### Acknowledgments

We would like to acknowledge the Open Philanthropy project for funding this research in part.

## References

*   Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
*   Bada & Nurse (2020) Maria Bada and Jason RC Nurse. The social and psychological impact of cyberattacks. In *Emerging cyber threats and cognitive vulnerabilities*, pp. 73–92\. Elsevier, 2020.
*   Bennetts (2013) Simon Bennetts. Owasp zed attack proxy. *AppSec USA*, 2013.
*   Boiko et al. (2023) Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. *arXiv preprint arXiv:2304.05332*, 2023.
*   Bran et al. (2023) Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew White, and Philippe Schwaller. Augmenting large language models with chemistry tools. In *NeurIPS 2023 AI for Science Workshop*, 2023.
*   Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877–1901, 2020.
*   Engebretson (2013) Patrick Engebretson. *The basics of hacking and penetration testing: ethical hacking and penetration testing made easy*. Elsevier, 2013.
*   Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang. Llm agents can autonomously hack websites, 2024.
*   Greshake et al. (2023a) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. More than you’ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. *arXiv e-prints*, pp.  arXiv–2302, 2023a.
*   Greshake et al. (2023b) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In *Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security*, pp.  79–90, 2023b.
*   Halfond et al. (2006) William G Halfond, Jeremy Viegas, Alessandro Orso, et al. A classification of sql-injection attacks and countermeasures. In *Proceedings of the IEEE international symposium on secure software engineering*, volume 1, pp.  13–15\. IEEE Piscataway, NJ, 2006.
*   Handa et al. (2019) Anand Handa, Ashu Sharma, and Sandeep K Shukla. Machine learning in cybersecurity: A review. *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, 9(4):e1306, 2019.
*   Happe & Cito (2023) Andreas Happe and Jürgen Cito. Getting pwn’d by ai: Penetration testing with large language models. In *Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering*, pp.  2082–2086, 2023.
*   Herley & Van Oorschot (2011) Cormac Herley and Paul Van Oorschot. A research agenda acknowledging the persistence of passwords. *IEEE Security & privacy*, 10(1):28–36, 2011.
*   Hilario et al. (2024) Eric Hilario, Sami Azam, Jawahar Sundaram, Khwaja Imran Mohammed, and Bharanidharan Shanmugam. Generative ai for pentesting: the good, the bad, the ugly. *International Journal of Information Security*, pp.  1–23, 2024.
*   Huang et al. (2023) Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. *arXiv preprint arXiv:2312.13010*, 2023.
*   Jang-Jaccard & Nepal (2014) Julian Jang-Jaccard and Surya Nepal. A survey of emerging threats in cybersecurity. *Journal of computer and system sciences*, 80(5):973–993, 2014.
*   Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
*   Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024.
*   Jimenez et al. (2023) Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? *arXiv preprint arXiv:2310.06770*, 2023.
*   Kang et al. (2023) Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. *arXiv preprint arXiv:2302.05733*, 2023.
*   Kennedy et al. (2011) David Kennedy, Jim O’gorman, Devon Kearns, and Mati Aharoni. *Metasploit: the penetration tester’s guide*. No Starch Press, 2011.
*   Kuznetsov et al. (2023) Igor Kuznetsov, Valentin Pashkov, Leonid Bezvershenko, and Georgy Kucherin. Operation triangulation: ios devices targeted with previously unknown malware. 2023. URL [https://securelist.com/operation-triangulation/109842/](https://securelist.com/operation-triangulation/109842/).
*   Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474, 2020.
*   Lohn & Jackson (2022) Andrew Lohn and Krystal Jackson. Will ai make cyber swords or shields? 2022.
*   Mahajan (2014) Akash Mahajan. *Burp Suite Essentials*. Packt Publishing Ltd, 2014.
*   Mialon et al. (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. *arXiv preprint arXiv:2302.07842*, 2023.
*   Osika (2023) Anton Osika. gpt-engineer, April 2023. URL [https://github.com/gpt-engineer-org/gpt-engineer](https://github.com/gpt-engineer-org/gpt-engineer).
*   Phuong et al. (2024) Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, et al. Evaluating frontier models for dangerous capabilities. *arXiv preprint arXiv:2403.13793*, 2024.
*   Popper (2016) Nathaniel Popper. A hacking of more than $50 million dashes hopes in the world of virtual currency. *The New York Times*, 17, 2016.
*   Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! *arXiv preprint arXiv:2310.03693*, 2023.
*   Research (2024) Nous Research. Nous hermes 2 - yi-34b, 2024. URL [https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B).
*   Roselin et al. (2019) Annie Gilda Roselin, Priyadarsi Nanda, Surya Nepal, Xiangjian He, and Jarod Wright. Exploiting the remote server access support of coap protocol. *IEEE Internet of Things Journal*, 6(6):9338–9349, 2019.
*   Russell et al. (2018) Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood, and Marc McConley. Automated vulnerability detection in source code using deep representation learning. In *2018 17th IEEE international conference on machine learning and applications (ICMLA)*, pp.  757–762\. IEEE, 2018.
*   Schaeffer et al. (2024) Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? *Advances in Neural Information Processing Systems*, 36, 2024.
*   Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*, 2023.
*   Sikorski & Honig (2012) Michael Sikorski and Andrew Honig. *Practical malware analysis: the hands-on guide to dissecting malicious software*. no starch press, 2012.
*   Teknium (2024) Teknium. Openhermes 2.5 - mistral 7b, 2024. URL [https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B).
*   Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*, 2023.
*   Ullah et al. (2018) Faheem Ullah, Matthew Edwards, Rajiv Ramdhany, Ruzanna Chitchyan, M Ali Babar, and Awais Rashid. Data exfiltration: A review of external attack vectors and countermeasures. *Journal of Network and Computer Applications*, 101:18–54, 2018.
*   Varshney (2023) Tanay Varshney. Introduction to llm agents. 2023. URL [https://developer.nvidia.com/blog/introduction-to-llm-agents/](https://developer.nvidia.com/blog/introduction-to-llm-agents/).
*   Vulnerabilities (2005) Common Vulnerabilities. Common vulnerabilities and exposures. *The MITRE Corporation,[online] Available: https://cve. mitre. org/index. html*, 2005.
*   Wang et al. (2023) Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. *arXiv preprint arXiv:2309.11235*, 2023.
*   Wang et al. (2024) Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, and Jinsong Su. Tdag: A multi-agent framework based on dynamic task decomposition and agent generation. *arXiv preprint arXiv:2402.10178*, 2024.
*   Warszawski & Bailis (2017) Todd Warszawski and Peter Bailis. Acidrain: Concurrency-related attacks on database-backed web applications. In *Proceedings of the 2017 ACM International Conference on Management of Data*, pp.  5–20, 2017.
*   Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*, 2022.
*   Yang et al. (2023) Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. *arXiv preprint arXiv:2310.02949*, 2023.
*   Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. *arXiv preprint arXiv:2210.03629*, 2022.
*   Yi et al. (2023) Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. *arXiv preprint arXiv:2312.14197*, 2023.
*   Zhan et al. (2023) Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. Removing rlhf protections in gpt-4 via fine-tuning. *arXiv preprint arXiv:2311.05553*, 2023.
*   Zhan et al. (2024) Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents. *arXiv preprint arXiv:2403.02691*, 2024.
*   Zheng et al. (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information Processing Systems*, 36, 2024.
*   Zheng & Zhang (2013) Yunhui Zheng and Xiangyu Zhang. Path sensitive static analysis of web applications for remote code execution vulnerability detection. In *2013 35th International Conference on Software Engineering (ICSE)*, pp.  652–661\. IEEE, 2013.
*   Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. *arXiv preprint arXiv:2307.15043*, 2023.