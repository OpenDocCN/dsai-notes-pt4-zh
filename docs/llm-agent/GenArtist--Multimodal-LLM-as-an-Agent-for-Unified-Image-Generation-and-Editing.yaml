- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:42:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:42:18'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GenArtist: 多模态LLM作为统一图像生成和编辑的代理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.05600](https://ar5iv.labs.arxiv.org/html/2407.05600)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.05600](https://ar5iv.labs.arxiv.org/html/2407.05600)
- en: Zhenyu Wang¹         Aoxue Li²         Zhenguo Li²         Xihui Liu³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhenyu Wang¹         Aoxue Li²         Zhenguo Li²         Xihui Liu³
- en: ¹ Tsinghua University    ² Noah’s Ark Lab, Huawei    ³ The University of Hong
    Kong
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 清华大学    ² 华为诺亚方舟实验室    ³ 香港大学
- en: wangzy20@mails.tsinghua.edu.cn, lax@pku.edu.cn,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: wangzy20@mails.tsinghua.edu.cn, lax@pku.edu.cn,
- en: Li.Zhenguo@huawei.com, xihuiliu@eee.hku.hk
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Li.Zhenguo@huawei.com, xihuiliu@eee.hku.hk
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract
- en: 'Despite the success achieved by existing image generation and editing methods,
    current models still struggle with complex problems including intricate text prompts,
    and the absence of verification and self-correction mechanisms makes the generated
    images unreliable. Meanwhile, a single model tends to specialize in particular
    tasks and possess the corresponding capabilities, making it inadequate for fulfilling
    all user requirements. We propose GenArtist, a *unified* image generation and
    editing system, coordinated by a multimodal large language model (MLLM) agent.
    We integrate a comprehensive range of existing models into the tool library and
    utilize the agent for tool selection and execution. For a complex problem, the
    MLLM agent decomposes it into simpler sub-problems and constructs a tree structure
    to systematically plan the procedure of generation, editing, and self-correction
    with step-by-step verification. By automatically generating missing position-related
    inputs and incorporating position information, the appropriate tool can be effectively
    employed to address each sub-problem. Experiments demonstrate that GenArtist can
    perform various generation and editing tasks, achieving state-of-the-art performance
    and surpassing existing models such as SDXL and DALL-E 3, as can be seen in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ GenArtist: Multimodal LLM as an Agent for Unified
    Image Generation and Editing"). Project page is [https://zhenyuw16.github.io/GenArtist_page/](https://zhenyuw16.github.io/GenArtist_page/).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管现有的图像生成和编辑方法已经取得了一定的成功，但当前的模型仍然面临复杂问题的挑战，包括复杂的文本提示，并且缺乏验证和自我纠正机制使得生成的图像不可靠。同时，单一模型通常倾向于专注于特定任务，并具备相应的能力，这使其难以满足所有用户需求。我们提出了GenArtist，一个*统一*的图像生成和编辑系统，由一个多模态大语言模型（MLLM）代理协调。我们将一系列现有模型集成到工具库中，并利用该代理进行工具选择和执行。对于复杂问题，MLLM代理将其分解为更简单的子问题，并构建一个树状结构以系统地规划生成、编辑和自我纠正的过程，并进行逐步验证。通过自动生成缺失的位置相关输入并结合位置信息，可以有效地使用适当的工具来解决每个子问题。实验表明，GenArtist能够执行各种生成和编辑任务，达到最先进的性能，并超越现有模型如SDXL和DALL-E
    3，具体可见于图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ GenArtist: Multimodal LLM as
    an Agent for Unified Image Generation and Editing")。项目页面为 [https://zhenyuw16.github.io/GenArtist_page/](https://zhenyuw16.github.io/GenArtist_page/)。'
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 Introduction
- en: '![Refer to caption](img/206a8647a886e8edc0b79f00c053bb71.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/206a8647a886e8edc0b79f00c053bb71.png)'
- en: 'Figure 1: Visualized examples from GenArtist. It can accomplish various tasks,
    achieving unified generation and editing. For text-to-image generation, it obtains
    greater accuracy compared to existing models like SDXL and DALL-E 3\. For image
    editing, it also excels in complex editing tasks.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 1: 来自GenArtist的可视化示例。它能够完成各种任务，实现统一的生成和编辑。在文本到图像生成方面，它比现有模型如SDXL和DALL-E
    3获得更高的准确性。在图像编辑方面，它在复杂编辑任务中也表现优异。'
- en: With the recent advancements in diffusion models [ho2020denoising](#bib.bib16)
    ; [dhariwal2021diffusion](#bib.bib10) , image generation and editing methods have
    rapidly progressed. Current improvements in image generation and editing can be
    broadly categorized into two tendencies. The first [ramesh2022hierarchical](#bib.bib39)
    ; [rombach2022high](#bib.bib40) ; [podell2023sdxl](#bib.bib36) ; [chen2023pixart](#bib.bib6)
    ; [betker2023improving](#bib.bib1) involves training from scratch using more advanced
    model architectures [rombach2022high](#bib.bib40) ; [peebles2023scalable](#bib.bib35)
    and larger-scale datasets, thereby scaling up existing models to achieve a more
    general generation or editing capability. These methods can usually enhance the
    overall controllability and quality of image generation. The second is primarily
    about finetuning or additionally designing pre-trained large-scale image generation
    models on specific datasets to extend their capability [ruiz2023dreambooth](#bib.bib41)
    ; [li2023blip](#bib.bib22) ; [chen2023textdiffuser](#bib.bib4) or enhance their
    performance on certain tasks [lian2023llm](#bib.bib24) ; [huang2023t2i](#bib.bib17)
    . These methods are usually task-specific and can demonstrate advantageous results
    on some particular tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着扩散模型的最新进展 [ho2020denoising](#bib.bib16) ; [dhariwal2021diffusion](#bib.bib10)，图像生成和编辑方法迅速发展。目前图像生成和编辑的改进可以大致分为两种趋势。第一种
    [ramesh2022hierarchical](#bib.bib39) ; [rombach2022high](#bib.bib40) ; [podell2023sdxl](#bib.bib36)
    ; [chen2023pixart](#bib.bib6) ; [betker2023improving](#bib.bib1) 涉及使用更先进的模型架构
    [rombach2022high](#bib.bib40) ; [peebles2023scalable](#bib.bib35) 和大规模数据集从头训练，从而扩大现有模型，实现更通用的生成或编辑能力。这些方法通常可以提高图像生成的整体可控性和质量。第二种主要是关于在特定数据集上微调或额外设计预训练的大规模图像生成模型，以扩展其能力
    [ruiz2023dreambooth](#bib.bib41) ; [li2023blip](#bib.bib22) ; [chen2023textdiffuser](#bib.bib4)
    或提高其在某些任务上的性能 [lian2023llm](#bib.bib24) ; [huang2023t2i](#bib.bib17)。这些方法通常是特定任务的，能够在某些特定任务上展示出优越的结果。
- en: 'Despite this, current image generation or editing methods are still imperfect
    and confront some urgent challenges on the way to building a human-desired system:
    1) The demand for image generation and editing is highly diverse and variable,
    like various requirements for objects and backgrounds, numerous demands about
    various operations in text prompts or instructions. Meanwhile, different models
    often possess different strengths and focus. General models may be weaker than
    some finetuned models in certain aspects, but they can exhibit better performance
    in out-of-distribution data. Therefore, it is nearly impossible for a well-trained
    model to meet all human requirements, and the use of only a single model is often
    sub-optimal. 2) Models still struggle with complex problems, such as lengthy and
    intricate sentences in text-to-image tasks or complicated instructions with multiple
    steps in editing tasks. Scaling up or finetuning models can alleviate this issue.
    However, since texts are highly variable, flexible, and can be easy to combine,
    there are always complex problems that a trained model cannot effectively handle.
    3) Although meticulously designed, models still inevitably encounter some failure
    cases. Generated images sometimes fail to accurately correspond to the content
    of user prompts. Existing models lack the ability to autonomously assess the correctness
    of generated images, not to mention self-correcting them, making generated images
    unreliable. What we truly desire, therefore, should be *a unified image generation
    and editing system*, which can satisfy nearly all human requirements while producing
    reliable image results.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，目前的图像生成或编辑方法仍然不完善，并且面临一些迫切的挑战：1) 对图像生成和编辑的需求高度多样化和变化，例如对对象和背景的各种要求，文本提示或指令中对各种操作的众多需求。同时，不同的模型往往具有不同的优势和侧重点。通用模型在某些方面可能不如一些微调模型，但在处理分布外数据时表现可能更好。因此，一个训练良好的模型几乎不可能满足所有人类需求，仅使用单一模型通常效果不佳。2)
    模型仍然面临复杂问题，例如文本到图像任务中的长句和复杂句子，或编辑任务中的多步骤复杂指令。扩大或微调模型可以缓解这个问题。然而，由于文本高度可变、灵活且容易组合，总是存在训练模型无法有效处理的复杂问题。3)
    尽管设计精良，模型仍不可避免地遇到一些失败案例。生成的图像有时无法准确对应用户提示的内容。现有模型缺乏自主评估生成图像正确性的能力，更不用说自我纠正，这使得生成的图像不可靠。因此，我们真正渴望的应该是*一个统一的图像生成和编辑系统*，它可以满足几乎所有人类需求，同时生成可靠的图像结果。
- en: 'In this paper, we propose a unified image generation and editing system called
    GenArtist to address the above challenges. Our fundamental idea is to utilize
    a multimodal large language model (MLLM) as an AI agent, which acts as an "artist"
    and "draws" images according to user instructions. Specifically, in response to
    user instructions, the agent will analyze the user requirements, decompose complex
    problems, and conduct planning comprehensively to formulate the specific solutions.
    Then, it executes image generation or editing operations by invoking external
    tools to meet the user demands. After images are obtained, it finally performs
    verification and correction on the generated results to further ensure the accuracy
    of the generated images. The core mechanisms of the agent are:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种统一的图像生成和编辑系统，称为 GenArtist，以应对上述挑战。我们的基本思想是利用多模态大语言模型（MLLM）作为 AI
    代理，它充当“艺术家”并根据用户指令“绘制”图像。具体而言，代理会分析用户需求，分解复杂问题，并进行全面规划以制定具体解决方案。然后，通过调用外部工具执行图像生成或编辑操作以满足用户需求。获得图像后，它最终对生成结果进行验证和修正，以进一步确保生成图像的准确性。代理的核心机制包括：
- en: Decomposition of intricate text prompts. The MLLM agent first decomposes the
    complex problems into several simple sub-problems. For complicated text prompts
    in generation tasks, it extracts single-object concepts and necessary background
    elements. For complex instructions in editing tasks, it breaks down intricate
    operations into several simple single editing actions. The decomposition of complex
    problems significantly improves the reliability of model execution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂文本提示的分解。MLLM代理首先将复杂问题分解为几个简单的子问题。对于生成任务中的复杂文本提示，它提取单一对象概念和必要的背景元素。对于编辑任务中的复杂指令，它将复杂操作拆解为几个简单的单一编辑动作。复杂问题的分解显著提高了模型执行的可靠性。
- en: Planning tree with step-by-step verification. After decomposition, we construct
    a tree structure to plan the execution of sub-tasks. Each operation is a node
    in the tree, with subsequent operations as its child nodes, and different tools
    for the same action are its sibling nodes. Each node is followed by verification
    to ensure that its operation can be executed correctly. Then, both generation,
    editing, and self-correction mechanisms can be incorporated. Through this planning
    tree, the proceeding of the system can be considered as a traversal process and
    the whole system can be coordinated.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 规划树与逐步验证。在分解后，我们构建树形结构以规划子任务的执行。每个操作都是树中的一个节点，随后的操作作为其子节点，不同工具用于相同操作则作为其兄弟节点。每个节点后都进行验证，以确保其操作能够正确执行。然后，可以将生成、编辑和自我修正机制纳入其中。通过这一规划树，系统的推进可以视为一个遍历过程，从而协调整个系统。
- en: Position-aware tool execution. Most of object-level tools require position-related
    inputs, like the position of the object to be manipulated. These necessary inputs
    may not be provided by the user. Existing MLLMs are also position-insensitive,
    and cannot provide accurate positional guidance. We thus introduce a set of auxiliary
    tools to automatically complete these position-related inputs, and incorporate
    position information for the MLLM agent through detection models for tool execution.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 位置感知工具执行。大多数对象级工具需要位置相关输入，比如被操作对象的位置。这些必要的输入可能未由用户提供。现有的 MLLM 也对位置不敏感，无法提供准确的位置指导。因此，我们引入了一套辅助工具来自动完成这些位置相关的输入，并通过检测模型将位置信息整合到
    MLLM 代理中，以进行工具执行。
- en: 'Our main contributions can be summarized as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献可以总结如下：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose GenArtist, a unified image generation and editing system. The MLLM
    agent serves as the "brain" to coordinate and manage the entire process. To the
    best of our knowledge, this is the first unified system that encompasses the vast
    majority of existing generation and editing tasks.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 GenArtist，一个统一的图像生成和编辑系统。MLLM 代理作为“脑部”来协调和管理整个过程。根据我们所知，这是第一个涵盖绝大多数现有生成和编辑任务的统一系统。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Through viewing the operations as nodes and constructing the planning tree,
    our MLLM agent can schedule for generation and editing tasks, and automatically
    verify and self-correct generated images. This significantly enhances the controllability
    of user instructions over images.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过将操作视为节点并构建规划树，我们的 MLLM 代理可以安排生成和编辑任务，并自动验证和自我修正生成的图像。这显著增强了用户对图像的指令控制能力。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: By incorporating position information into the integrated tool library and employing
    auxiliary tools for providing missing position-related inputs, the agent performs
    tool selection and invokes the most suitable tool, providing a unified interface
    for various tasks in generation and editing.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过将位置信息整合到工具库中，并使用辅助工具提供缺失的位置信息，代理能够进行工具选择并调用最合适的工具，为生成和编辑中的各种任务提供统一的接口。
- en: 'Extensive experiments demonstrate the effectiveness of our GenArtist. It achieves
    more than 7% improvement compared to DALL-E 3 [betker2023improving](#bib.bib1)
    on T2I-CompBench [huang2023t2i](#bib.bib17) , a comprehensive benchmark for open-world
    compositional T2I generation, and also obtains the state-of-the-art performance
    on the image editing benchmark MagicBrush [zhang2024magicbrush](#bib.bib60) .
    As can be seen in the visualized examples in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing"),
    GenArtist well serves as a unified image generation and editing system.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '大量实验证明了我们的 GenArtist 的有效性。与 DALL-E 3 [betker2023improving](#bib.bib1) 相比，在
    T2I-CompBench [huang2023t2i](#bib.bib17) 这个综合性的开放世界组合 T2I 生成基准上，取得了超过 7% 的改进，并在图像编辑基准
    MagicBrush [zhang2024magicbrush](#bib.bib60) 上获得了最先进的性能。如图 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation
    and Editing") 中的可视化示例所示，GenArtist 作为统一的图像生成和编辑系统表现良好。'
- en: 2 Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Image generation and editing. With the development of diffusion models [dhariwal2021diffusion](#bib.bib10)
    ; [ho2020denoising](#bib.bib16) , both image generation and editing have achieved
    remarkable success. Many general text-to-image generation [rombach2022high](#bib.bib40)
    ; [saharia2022photorealistic](#bib.bib42) ; [podell2023sdxl](#bib.bib36) ; [chen2023pixart](#bib.bib6)
    and editing methods [brooks2023instructpix2pix](#bib.bib2) ; [zhang2024magicbrush](#bib.bib60)
    ; [sheynin2023emu](#bib.bib44) ; [geng2023instructdiffusion](#bib.bib15) have
    been proposed and achieved high-quality generated images. Based on these general
    models, many methods conduct finetuning or design additional modules for some
    specialized tasks, like customized image generation [ruiz2023dreambooth](#bib.bib41)
    ; [kumari2023multi](#bib.bib20) ; [li2023blip](#bib.bib22) ; [liu2023cones](#bib.bib29)
    , image generation with text rendering [chen2024textdiffuser](#bib.bib5) ; [chen2023textdiffuser](#bib.bib4)
    , exemplar-based image editing [yang2023paint](#bib.bib55) ; [chen2023anydoor](#bib.bib8)
    , image generation that focuses on persons [xiao2023fastcomposer](#bib.bib52)
    . Meanwhile, some methods aim to improve the controllability of texts over images.
    For example, ControlNet [zhang2023adding](#bib.bib61) controls Stable Diffusion
    with various conditioning inputs like Canny edges,  [voynov2023sketch](#bib.bib49)
    adopts sketch images for conditions, and layout-to-image methods [li2023gligen](#bib.bib23)
    ; [xie2023boxdiff](#bib.bib53) ; [lian2023llm](#bib.bib24) ; [chen2024training](#bib.bib7)
    synthesize images according to the given bounding boxes of objects. Despite the
    success, these methods still focus on specific tasks, thus unable to support unified
    image generation and editing.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成和编辑。随着扩散模型的进展 [dhariwal2021diffusion](#bib.bib10) ; [ho2020denoising](#bib.bib16)
    ，图像生成和编辑都取得了显著成功。许多通用的文本到图像生成 [rombach2022high](#bib.bib40) ; [saharia2022photorealistic](#bib.bib42)
    ; [podell2023sdxl](#bib.bib36) ; [chen2023pixart](#bib.bib6) 和编辑方法 [brooks2023instructpix2pix](#bib.bib2)
    ; [zhang2024magicbrush](#bib.bib60) ; [sheynin2023emu](#bib.bib44) ; [geng2023instructdiffusion](#bib.bib15)
    已被提出，并实现了高质量的生成图像。基于这些通用模型，许多方法进行微调或设计附加模块以应对某些专业任务，如定制化图像生成 [ruiz2023dreambooth](#bib.bib41)
    ; [kumari2023multi](#bib.bib20) ; [li2023blip](#bib.bib22) ; [liu2023cones](#bib.bib29)
    ，带文本渲染的图像生成 [chen2024textdiffuser](#bib.bib5) ; [chen2023textdiffuser](#bib.bib4)
    ，基于示例的图像编辑 [yang2023paint](#bib.bib55) ; [chen2023anydoor](#bib.bib8) ，以及关注人物的图像生成
    [xiao2023fastcomposer](#bib.bib52) 。与此同时，一些方法旨在提高文本对图像的可控性。例如，ControlNet [zhang2023adding](#bib.bib61)
    控制稳定扩散，接受各种条件输入，如 Canny 边缘，[voynov2023sketch](#bib.bib49) 采用素描图像作为条件，而布局到图像的方法
    [li2023gligen](#bib.bib23) ; [xie2023boxdiff](#bib.bib53) ; [lian2023llm](#bib.bib24)
    ; [chen2024training](#bib.bib7) 根据给定的对象边界框合成图像。尽管取得了成功，这些方法仍然专注于特定任务，因此无法支持统一的图像生成和编辑。
- en: AI agent. Large language models (LLMs), like ChatGPT, Llama [touvron2023llama](#bib.bib47)
    ; [touvron2023llama2](#bib.bib48) , have demonstrated impressive capability in
    natural language processing. The involvement of vision ability for multimodal
    large language models (MLLMS), like LLaVA [liu2024visual](#bib.bib25) , Claude,
    GPT-4 [gpt42023](#bib.bib33) , further enables the models to process visual data.
    Recently, LLMs begin to be adopted as agents for executing complex tasks. These
    works [yang2023auto](#bib.bib56) ; [shen2023hugginggpt](#bib.bib43) ; [liu2023internchat](#bib.bib28)
    apply LLMs to learn to use tools for tasks like visual interaction, speech processing,
    software development [qian2023communicative](#bib.bib37) , gaming [meta2022human](#bib.bib11)
    , APP use [yang2023appagent](#bib.bib59) or math [xin2023lego](#bib.bib54) . Recently,
    the idea of AI agents has also begun to be applied to image generation related
    tasks. For example, [lian2023llm](#bib.bib24) ; [feng2024layoutgpt](#bib.bib13)
    design scene layout with LLMs, [wu2023self](#bib.bib51) utilizes LLMs to assist
    self-correcting, [wang2024divide](#bib.bib50) ; [yang2024mastering](#bib.bib58)
    target at MLLMs in complex text-to-image generation problems, and [qin2024diffusiongpt](#bib.bib38)
    leverages LLM for model selection in the text-to-image generation task.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: AI 代理。大型语言模型（LLMs），如 ChatGPT、Llama [touvron2023llama](#bib.bib47) ; [touvron2023llama2](#bib.bib48)
    ，在自然语言处理方面表现出色。多模态大型语言模型（MLLMS）如 LLaVA [liu2024visual](#bib.bib25) ，Claude，GPT-4 [gpt42023](#bib.bib33)
    ，进一步使模型能够处理视觉数据。最近，LLMs 开始被用作执行复杂任务的代理。这些工作 [yang2023auto](#bib.bib56) ; [shen2023hugginggpt](#bib.bib43)
    ; [liu2023internchat](#bib.bib28) 应用 LLMs 学习使用工具来处理视觉互动、语音处理、软件开发 [qian2023communicative](#bib.bib37)
    、游戏 [meta2022human](#bib.bib11) 、应用程序使用 [yang2023appagent](#bib.bib59) 或数学 [xin2023lego](#bib.bib54)
    等任务。最近，AI 代理的概念也开始应用于图像生成相关任务。例如，[lian2023llm](#bib.bib24) ; [feng2024layoutgpt](#bib.bib13)
    使用 LLMs 设计场景布局，[wu2023self](#bib.bib51) 利用 LLMs 辅助自我纠正，[wang2024divide](#bib.bib50)
    ; [yang2024mastering](#bib.bib58) 针对复杂的文本到图像生成问题研究 MLLMs，[qin2024diffusiongpt](#bib.bib38)
    则利用 LLM 进行文本到图像生成任务中的模型选择。
- en: '![Refer to caption](img/23070cc08a994351a0afd15a78001e5c.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/23070cc08a994351a0afd15a78001e5c.png)'
- en: 'Figure 2: The overview of our GenArtist. The MLLM agent is responsible for
    decomposing problems and planning using a tree structure, then invoking tools
    to address the issues. Employing the agent as the "brain" effectively realizes
    a unified generation and editing system.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们的 GenArtist 概述。MLLM 代理负责分解问题并使用树结构进行规划，然后调用工具来解决问题。将代理作为“大脑”有效地实现了统一的生成和编辑系统。
- en: 3 Method
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'The overview of our GenArtist is illustrated in Fig. [2](#S2.F2 "Figure 2 ‣
    2 Related Work ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation
    and Editing"). The MLLM agent coordinates the whole system. Its primary responsibilities
    center around decomposing the complicated tasks and constructing the planning
    tree with step-by-step verification for image generation, editing, and self-correction.
    It invokes tools from an image generation tool library and an editing tool library
    to execute the specific operations, and an auxiliary tool library serves to provide
    missing position-related values.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 GenArtist 概述如图 [2](#S2.F2 "图 2 ‣ 2 相关工作 ‣ GenArtist：作为统一图像生成和编辑的多模态 LLM
    代理") 所示。MLLM 代理协调整个系统。它的主要职责包括将复杂任务分解，并构建带有逐步验证的规划树，用于图像生成、编辑和自我纠正。它从图像生成工具库和编辑工具库中调用工具执行具体操作，辅助工具库则提供缺失的位置相关值。
- en: 3.1 Planning Tree with Step-by-Step Verification
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 带有逐步验证的规划树
- en: Decomposition. When it comes to complicated prompt inputs, existing methods
    usually cannot understand all requirements, which hurts the controllability and
    reliability of model results. The MLLM agent thus first decomposes the complex
    problems. For generation tasks, it decomposes both object and background information
    according to the text prompts. It extracts the discrete objects embedded within
    the text prompts, along with their associated attributes. For background information,
    it mainly analyzes the overall scene and image style required by the input texts.
    For editing tasks, It decomposes complex editing operations into several specific
    actions, such as add, move, remove, into simple editing instructions. After decomposition,
    the simpler operations can be relatively easier to address, which thus improves
    the reliability of model execution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 分解。对于复杂的提示输入，现有方法通常无法理解所有要求，这会影响模型结果的可控性和可靠性。因此，MLLM代理首先对复杂问题进行分解。对于生成任务，它根据文本提示分解对象和背景信息。它提取文本提示中嵌入的离散对象及其相关属性。对于背景信息，它主要分析输入文本所需的整体场景和图像风格。对于编辑任务，它将复杂的编辑操作分解为几个具体的操作，如添加、移动、删除，形成简单的编辑指令。经过分解后，更简单的操作相对容易处理，从而提高了模型执行的可靠性。
- en: 'Tree construction. After decomposition, we organize all operations into a structure
    of tree for planning. Such a tree primarily consists of three types of nodes:
    initial nodes, generation nodes, and editing nodes. The initial node serves as
    the root of the tree, marking the beginning of the system. Generation nodes are
    about image generation using tools from the generation tool library, while editing
    nodes are about performing a single editing operation using the corresponding
    tools from the editing tool library. For pure image editing tasks, the generation
    nodes will be absent.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 树结构构建。经过分解后，我们将所有操作组织成树状结构进行规划。这种树主要由三种类型的节点组成：初始节点、生成节点和编辑节点。初始节点作为树的根节点，标志着系统的开始。生成节点涉及使用生成工具库中的工具进行图像生成，而编辑节点涉及使用编辑工具库中的相应工具进行单一编辑操作。对于纯图像编辑任务，生成节点将不存在。
- en: 'In practice, as the correctness of generated images cannot be guaranteed, we
    introduce the self-correction mechanism to assess and rectify the results of generation.
    Each generation node thus has a sub-tree consisting entirely of editing nodes
    for self-correction. After the tools in the generation nodes are invoked and verification
    is conducted, this sub-tree will be adaptively generated by the MLLM agent. Specifically,
    after verification, we instruct the MLLM agent to devise a series of corresponding
    editing operations to correct the images. Take the example in Fig. [2](#S2.F2
    "Figure 2 ‣ 2 Related Work ‣ GenArtist: Multimodal LLM as an Agent for Unified
    Image Generation and Editing") for example, editing actions including "add a black
    bicycle", "edit the color of the scooter to blue", "add a bird" should be conducted.
    These operations are organized into a tree structure to be the sub-tree of the
    generation node, allowing for specific planning of self-correction.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '实际操作中，由于生成图像的正确性无法保证，我们引入了自我修正机制来评估和纠正生成结果。每个生成节点都有一个完全由编辑节点组成的子树用于自我修正。在生成节点中的工具被调用并进行验证后，这个子树将由MLLM代理自适应生成。具体而言，经过验证后，我们指示MLLM代理制定一系列相应的编辑操作来纠正图像。例如，在图示[2](#S2.F2
    "图2 ‣ 2 相关工作 ‣ GenArtist: 多模态LLM作为统一图像生成和编辑的代理")中，应该执行包括“添加一辆黑色自行车”、“将滑板车的颜色改为蓝色”、“添加一只鸟”等编辑操作。这些操作被组织成一个树结构，成为生成节点的子树，从而实现具体的自我修正规划。'
- en: '![Refer to caption](img/33b10e66534ecd61f1268eec7ca5a73f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/33b10e66534ecd61f1268eec7ca5a73f.png)'
- en: 'Figure 3: Illustration of the tree for planning. The sub-tree of the "alternative
    generation tool" node will be adaptively generated after verification, and the
    sub-tree of the "instruction" node is the same as the left.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：规划树的示意图。 “替代生成工具”节点的子树将在验证后自适应生成，而“指令”节点的子树与左侧相同。
- en: 'Each generation or editing action corresponds to a node in the tree, with its
    subsequent operations as its child nodes. This construction initially forms a
    "chain", enabling a planning chain. Then, we note that we can usually utilize
    different tools to address the same problem. For example, for adding an object
    into the image, we can employ a tool specifically designed for object addition
    or instruction-based editing models by translating the adding operation into text
    instructions. Similarly, for attribute editing, we can use attribute editing models
    or utilize replacement or instruction-based editing models. Moreover, numerous
    generation tools can achieve text-to-image generation, and varying the random
    seeds can also produce different outputs. We consider these nodes as siblings,
    all serving as child nodes of their parent nodes. They also share the same sub-tree,
    containing subsequent editing operations. The tool selected by the MLLM agent
    will be placed as the optimal child node and positioned on the far left. In this
    way, we establish the structure of the tree. An illustration example for the Fig. [2](#S2.F2
    "Figure 2 ‣ 2 Related Work ‣ GenArtist: Multimodal LLM as an Agent for Unified
    Image Generation and Editing") case is provided in Fig. [3](#S3.F3 "Figure 3 ‣
    3.1 Planning Tree with Step-by-Step Verification ‣ 3 Method ‣ GenArtist: Multimodal
    LLM as an Agent for Unified Image Generation and Editing") (we omit some sub-trees
    with identical structures or adaptively generated after generation nodes, and
    some nodes about varying random seeds for simplicity).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '每次生成或编辑操作对应树中的一个节点，其后续操作作为子节点。这个构建最初形成了一个“链”，以实现规划链。然后，我们注意到通常可以利用不同的工具来解决相同的问题。例如，对于向图像中添加对象，我们可以使用专门设计的对象添加工具或通过将添加操作转化为文本指令的基于指令的编辑模型。类似地，对于属性编辑，我们可以使用属性编辑模型或利用替换或基于指令的编辑模型。此外，许多生成工具可以实现文本到图像的生成，改变随机种子也能产生不同的输出。我们将这些节点视为兄弟节点，它们都作为其父节点的子节点。它们还共享相同的子树，包含后续编辑操作。由MLLM代理选择的工具将被放置为最优的子节点，并且位于最左侧。通过这种方式，我们建立了树的结构。关于图.
    [2](#S2.F2 "图 2 ‣ 2 相关工作 ‣ GenArtist: 多模态 LLM 作为统一图像生成和编辑的代理") 的示例图见图. [3](#S3.F3
    "图 3 ‣ 3.1 带逐步验证的规划树 ‣ 3 方法 ‣ GenArtist: 多模态 LLM 作为统一图像生成和编辑的代理")（我们省略了一些具有相同结构的子树或在生成节点之后自适应生成的子树，以及一些关于随机种子的节点以简化说明）。'
- en: Planning. Once the tree is established, planning for self-correction or the
    whole system can be viewed as the pre-order traversal of the structure. For a
    particular node, its corresponding tool is invoked to conduct the operation, followed
    by verification to determine whether the editing is successful. If successful,
    the process proceeds to its leftmost child node for subsequent operations, and
    its sibling nodes are deleted. If unsuccessful, the process backtracks to its
    sibling nodes, and its sub-tree is removed. This process continues until the generated
    image is correct, *i.e.*, when a node at the lowest level successfully executes.
    We can also limit the branching factor or the number of nodes of the tree for
    early termination, and require the agent to return the most accurate image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 规划。一旦树形结构建立起来，规划自我修正或整个系统可以被视为结构的前序遍历。对于特定节点，调用其相应的工具进行操作，然后进行验证以确定编辑是否成功。如果成功，过程将继续到其最左边的子节点进行后续操作，并删除其兄弟节点。如果不成功，过程会回溯到其兄弟节点，并移除其子树。这个过程将持续进行，直到生成的图像正确，*即*，当最低层级的节点成功执行时。我们还可以限制树的分支因子或节点数量以实现提前终止，并要求代理返回最准确的图像。
- en: Verification. As described above, the verification mechanism plays a crucial
    role both in tree construction and the execution process. Through the multimodal
    perception capability of the MLLM, the agent verifies the correctness of the generated
    images. The main aspect of verification involves the objects contained in the
    text, together with their own attributes like their color, shape, texture, the
    positions of the objects, the relationship among these different objects. Besides,
    the background, scene, overall style and the aesthetic quality of generated images
    are also considered. Since the perception ability of existing models tends to
    be superior to the generative ability, employing such verification allows for
    effectively assessing the correctness of generated images.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 验证。如上所述，验证机制在树构建和执行过程中都起着至关重要的作用。通过 MLLM 的多模态感知能力，代理验证生成图像的正确性。验证的主要方面包括文本中包含的对象及其自身属性，如颜色、形状、纹理、对象的位置、不同对象之间的关系。此外，还考虑了生成图像的背景、场景、整体风格和美学质量。由于现有模型的感知能力往往优于生成能力，采用这种验证可以有效评估生成图像的正确性。
- en: 3.2 Tool Library
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 工具库
- en: 'After constructing the planning tree, the agent proceeds to execute each node
    by calling external tools, ultimately solving the problem. We first introduce
    the tools used in GenArtist. The primary tools that the MLLM agent utilizes can
    be generally divided into the image generation tool library and the editing tool
    library. The specific tools we utilize currently are listed in Tab. [1](#S3.T1
    "Table 1 ‣ 3.2 Tool Library ‣ 3 Method ‣ GenArtist: Multimodal LLM as an Agent
    for Unified Image Generation and Editing"), and some new tools can be seamlessly
    added, allowing for the expansion of the tool library. To assist the subsequent
    tool selection, we need to convey information to the MLLM agent about the specific
    task performed by the tool, its required inputs, and its characteristics and advantages.
    The prompts for introducing tools consist of the following parts specifically:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '构建规划树后，代理会通过调用外部工具来执行每个节点，最终解决问题。我们首先介绍在 GenArtist 中使用的工具。MLLM 代理使用的主要工具通常分为图像生成工具库和编辑工具库。我们当前使用的具体工具列在表 [1](#S3.T1
    "Table 1 ‣ 3.2 Tool Library ‣ 3 Method ‣ GenArtist: Multimodal LLM as an Agent
    for Unified Image Generation and Editing")中，且可以无缝添加一些新工具，从而扩展工具库。为了协助后续的工具选择，我们需要向
    MLLM 代理传达工具所执行的具体任务、所需输入以及其特点和优势。介绍工具的提示包括以下几个部分：'
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The tool skill and name. It briefly describes the tool-related task and its
    name, as listed in Tab. [1](#S3.T1 "Table 1 ‣ 3.2 Tool Library ‣ 3 Method ‣ GenArtist:
    Multimodal LLM as an Agent for Unified Image Generation and Editing"), such as
    (text-to-image, SDXL), (canny-to-image, ControlNet), (object removal, LaMa). It
    serves as a unique identifier, enabling the agent to differentiate the utilized
    tools.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '工具技能和名称。简要描述与工具相关的任务及其名称，如表 [1](#S3.T1 "Table 1 ‣ 3.2 Tool Library ‣ 3 Method
    ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing")中列出的（text-to-image，SDXL）、（canny-to-image，ControlNet）、（object
    removal，LaMa）。它作为唯一标识符，使代理能够区分所使用的工具。'
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The tool required inputs. It pertains to the specific inputs required for the
    execution of the tool. For example, text-to-image models require "text" as input
    for generation, customization models also need "subject images" for personalized
    generation. Most of object-level editing tools demand instructions about "object
    name" and "object position".
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具所需输入。涉及执行工具所需的具体输入。例如，text-to-image 模型需要“文本”作为生成输入，自定义模型还需要“主题图像”用于个性化生成。大多数对象级编辑工具需要关于“对象名称”和“对象位置”的指令。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The tool characteristic and advantage. It primarily provides a more detailed
    introduction of the tool, including its specific characteristics, serving as a
    key reference for the agent during tool selection. For example, SDXL can be a
    general text-to-image generation model, LMD usually controls scene layout strictly
    and is suitable for compositional text-to-image generation, where text prompts
    usually contain multiple objects, BoxDiff controls scene layout relatively loosely,
    TextDiffuser is specially designed for image generation with text rendering.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具的特性和优势。它主要提供了对工具的更详细介绍，包括其具体特性，作为代理在工具选择过程中的关键参考。例如，SDXL 可以是一个通用的文本到图像生成模型，LMD
    通常严格控制场景布局，适用于复杂的文本到图像生成，其中文本提示通常包含多个对象，BoxDiff 相对宽松地控制场景布局，TextDiffuser 专门设计用于带有文本渲染的图像生成。
- en: 'Table 1: GenArtist utilized tool library, including the tool names and their
    skills. The main tools are from the generation tool library and the editing tool
    library. The following models represent all the tools used in our current version,
    while new models can be seamlessly added.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：GenArtist 使用的工具库，包括工具名称及其技能。主要工具来自生成工具库和编辑工具库。以下模型代表了我们当前版本中使用的所有工具，而新模型可以无缝添加。
- en: '|        Generation Tools | Editing Tools |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|        生成工具 | 编辑工具 |'
- en: '| skill | tool | skill | tool |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 技能 | 工具 | 技能 | 工具 |'
- en: '| text-to-image | SDXL [podell2023sdxl](#bib.bib36) |  |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 文本到图像 | SDXL [podell2023sdxl](#bib.bib36) |  |  |'
- en: '| text-to-image | PixArt-$\alpha$ [chen2023pixart](#bib.bib6) | object addition
    | AnyDoor [chen2023anydoor](#bib.bib8) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 文本到图像 | PixArt-$\alpha$ [chen2023pixart](#bib.bib6) | 对象添加 | AnyDoor [chen2023anydoor](#bib.bib8)
    |'
- en: '| image-to-image | Stable Diffusion v2 [rombach2022high](#bib.bib40) | object
    removal | LaMa [suvorov2022resolution](#bib.bib46) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 图像到图像 | Stable Diffusion v2 [rombach2022high](#bib.bib40) | 对象移除 | LaMa [suvorov2022resolution](#bib.bib46)
    |'
- en: '| layout-to-image | LMD [lian2023llm](#bib.bib24) | object replacement | AnyDoor [chen2023anydoor](#bib.bib8)
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 布局到图像 | LMD [lian2023llm](#bib.bib24) | 对象替换 | AnyDoor [chen2023anydoor](#bib.bib8)
    |'
- en: '| layout-to-image | BoxDiff [xie2023boxdiff](#bib.bib53) | attribute editing
    | DiffEdit [couairon2022diffedit](#bib.bib9) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 布局到图像 | BoxDiff [xie2023boxdiff](#bib.bib53) | 属性编辑 | DiffEdit [couairon2022diffedit](#bib.bib9)
    |'
- en: '| single-object customization | BLIP-Diffusion [li2023blip](#bib.bib22) | instruction-based
    | MagicBrush [zhang2024magicbrush](#bib.bib60) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 单对象定制 | BLIP-Diffusion [li2023blip](#bib.bib22) | 基于指令 | MagicBrush [zhang2024magicbrush](#bib.bib60)
    |'
- en: '| multi-object customization | $\lambda$-ECLIPSE [patel2024lambda](#bib.bib34)
    | dragging (detail) | DragDiffusion [shi2023dragdiffusion](#bib.bib45) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 多对象定制 | $\lambda$-ECLIPSE [patel2024lambda](#bib.bib34) | 拖动（详细） | DragDiffusion [shi2023dragdiffusion](#bib.bib45)
    |'
- en: '| super-resolution | SDXL [podell2023sdxl](#bib.bib36) | dragging (object)
    | DragonDiffusion [mou2023dragondiffusion](#bib.bib32) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 超分辨率 | SDXL [podell2023sdxl](#bib.bib36) | 拖动（对象） | DragonDiffusion [mou2023dragondiffusion](#bib.bib32)
    |'
- en: '| image with texts | TextDiffuser [chen2023textdiffuser](#bib.bib4) | style
    transfer | InST [zhang2023inversion](#bib.bib63) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 带文本的图像 | TextDiffuser [chen2023textdiffuser](#bib.bib4) | 风格转移 | InST [zhang2023inversion](#bib.bib63)
    |'
- en: '| {canny, depth …}-to-image | ControlNet [zhang2023adding](#bib.bib61) |  |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| {canny, depth …}-到图像 | ControlNet [zhang2023adding](#bib.bib61) |  |  |'
- en: '|   |  |  |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: 3.3 Position-Aware Tool Execution
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 位置感知工具执行
- en: 'With tool libraries, the MLLM agent will further perform tool selection and
    execution to utilize the suitable tool for fulfilling the image generation or
    editing task. Before tool execution, we compensate for the deficiency of position
    information in user inputs and the MLLM agent through two designs:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 利用工具库，MLLM 代理将进一步执行工具选择和操作，以利用合适的工具完成图像生成或编辑任务。在工具执行之前，我们通过两个设计弥补了用户输入和 MLLM
    代理中位置信息的不足：
- en: 'Position-related input compensation. In practice, it is common to encounter
    scenes where the agent selects a suitable tool but some necessary user inputs
    are missing. These user inputs are mostly related to positions. For example, for
    some complex text prompts where multiple objects exist, the layout-to-image tool
    can be suitable. However, users may not necessarily provide the scene layouts
    and usually only text prompts are provided. In such cases, due to the absence
    of some necessary inputs, these suitable tools cannot be directly invoked. We
    therefore introduce the auxiliary tool library to provide these position-related
    missing inputs. This auxiliary tool library mainly contains: 1) localization models
    like object detection [liu2023grounding](#bib.bib27) or segmentation [kirillov2023segment](#bib.bib19)
    models, to provide position information of objects for some object-level editing
    tools; 2) the preprocessors of ControlNet [zhang2023adding](#bib.bib61) like the
    pose estimator, canny edge map extractor, depth map extractor; 3) some LLM-implemented
    tools, like the scene layout generator [lian2023llm](#bib.bib24) ; [feng2024layoutgpt](#bib.bib13)
    . The MLLM agent can invoke these auxiliary tools automatically if necessary,
    to guarantee that the most suitable tool to address the user instruction can be
    utilized, rather than solely relying on user-provided inputs to select tools.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与位置相关的输入补偿。在实践中，常常会遇到代理选择合适工具但一些必要的用户输入缺失的情况。这些用户输入大多与位置相关。例如，对于一些存在多个对象的复杂文本提示，布局到图像工具可能适用。然而，用户可能不会提供场景布局，通常只提供文本提示。在这种情况下，由于缺少一些必要的输入，这些合适的工具不能直接调用。因此，我们引入了辅助工具库来提供这些与位置相关的缺失输入。这个辅助工具库主要包括：1)
    本地化模型，如目标检测 [liu2023grounding](#bib.bib27)或分割 [kirillov2023segment](#bib.bib19)模型，为一些对象级编辑工具提供对象位置的信息；2)
    ControlNet [zhang2023adding](#bib.bib61)的预处理器，如姿态估计器、canny边缘图提取器、深度图提取器；3) 一些LLM实现的工具，如场景布局生成器 [lian2023llm](#bib.bib24)；[feng2024layoutgpt](#bib.bib13)。如果有必要，MLLM代理可以自动调用这些辅助工具，以确保能够使用最合适的工具来处理用户指令，而不是仅依靠用户提供的输入来选择工具。
- en: Position information introduction. Existing MLLMs primarily focus on text comprehension
    and holistic image perception, with relatively limited attention to precise position
    information within images. MLLMs can easily determine whether objects exist in
    the image, but sometimes struggle with discerning spatial relationships between
    objects, such as whether a specific object is to the left or right of another.
    It is also more challenging for these MLLMs to provide accurate guidance for tools
    that require position-related inputs, such as object-level editing tools. To address
    this, we employ an object detector on the input images, and include the detected
    objects along with their bounding boxes as part of the prompt, to provide a spatial
    reference for the MLLM agent. In this way, the agent can effectively determine
    the positions within the image where certain tools should operate.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 位置信息介绍。现有的MLLMs主要关注文本理解和整体图像感知，对图像中的精确位置信息关注较少。MLLMs可以轻松判断图像中是否存在对象，但有时难以辨别对象之间的空间关系，例如一个特定的对象是否在另一个对象的左侧或右侧。这些MLLMs对于需要位置相关输入的工具，如对象级编辑工具，提供准确指导也更具挑战性。为了解决这个问题，我们在输入图像上使用目标检测器，并将检测到的对象及其边界框包含在提示中，为MLLM代理提供空间参考。这样，代理可以有效地确定图像中某些工具应操作的位置。
- en: 'The prompts for the agent to conduct tool selection thus mainly consist of
    the following parts:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 代理进行工具选择的提示主要由以下部分组成：
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Task instruction. Its main purpose is to clarify the task of the agent, *i.e.*,
    tool selection within a unified generation and editing system. Simultaneously,
    it takes user instructions as input and specifies the output format. We request
    the agent to output in the format of {"tool_name":tools, "input":inputs} and annotate
    missing inputs with the pre-defined specified identifier.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务指令。其主要目的是明确代理的任务，*即*，在统一生成和编辑系统中的工具选择。同时，它将用户指令作为输入，并指定输出格式。我们要求代理以{"tool_name":tools,
    "input":inputs}的格式输出，并用预定义的指定标识符注释缺失的输入。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tool introductions. We input the description of each tool into the agent in
    the format as described earlier. The detailed information about the tools will
    serve as the crucial references for the tool selection process. We also state
    that the primary criterion for tool selection is the suitability of the tool,
    rather than the content of given inputs, since missing inputs can be generated
    automatically.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具介绍。我们将每个工具的描述输入到代理中，格式如前所述。有关工具的详细信息将作为工具选择过程中的关键参考。我们还指出，工具选择的主要标准是工具的适用性，而不是给定输入的内容，因为缺失的输入可以自动生成。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Position information. The outputs from the object detector are utilized and
    provided to the MLLM agent to compensate for the lack of position information.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置信息。来自对象检测器的输出被利用并提供给MLLM代理，以补偿位置信息的缺乏。
- en: 'In summary, the basic steps for tool execution are as follows: First, determine
    whether the task pertains to image generation or editing. Next, conduct tool selection
    according to the instructions and the characteristics of the tools, and output
    in the required format. Finally, for missing inputs which are necessary for the
    selected tools, utilize auxiliary tools to complete them. Upon completing these
    steps, the agent will be able to correctly execute the appropriate tools, thereby
    initially meeting the requirements of users. The integration, selection, and execution
    of diverse tools significantly facilitate the development of a unified image generation
    and editing system.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，工具执行的基本步骤如下：首先，确定任务是否涉及图像生成或编辑。接下来，根据指示和工具的特性进行工具选择，并按要求格式输出。最后，对于所选工具所需的缺失输入，使用辅助工具完成。完成这些步骤后，代理将能够正确执行适当的工具，从而初步满足用户需求。多样化工具的集成、选择和执行极大地促进了统一图像生成和编辑系统的发展。
- en: 4 Experiments
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'Table 2: Quantitative Comparison on T2I-CompBench with existing text-to-image
    generation models and compositional methods. Our method demonstrates superior
    compositional generation ability in both attribute binding, object relationships,
    and complex compositions. We use the officially updated code for evaluation, which
    updates the noun phrase number. Consequently, some metric values for certain methods
    may be lower than those reported in their original papers.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：与现有文本到图像生成模型和组合方法在T2I-CompBench上的定量比较。我们的方法在属性绑定、对象关系和复杂组合方面表现出优越的组合生成能力。我们使用官方更新的代码进行评估，该更新调整了名词短语的数量。因此，某些方法的指标值可能低于其原始论文中报告的值。
- en: '|      Model | Attribute Binding | Object Relationship | Complex$\uparrow$
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|      模型 | 属性绑定 | 对象关系 | 复杂$\uparrow$ |'
- en: '| Color $\uparrow$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 颜色 $\uparrow$ |'
- en: '| Stable Diffusion v1.4 [rombach2022high](#bib.bib40) | 0.3765 | 0.3576 | 0.4156
    | 0.1246 | 0.3079 | 0.3080 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 稳定扩散 v1.4 [rombach2022high](#bib.bib40) | 0.3765 | 0.3576 | 0.4156 | 0.1246
    | 0.3079 | 0.3080 |'
- en: '| Stable Diffusion v2 [rombach2022high](#bib.bib40) | 0.5065 | 0.4221 | 0.4922
    | 0.1342 | 0.3096 | 0.3386 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 稳定扩散 v2 [rombach2022high](#bib.bib40) | 0.5065 | 0.4221 | 0.4922 | 0.1342
    | 0.3096 | 0.3386 |'
- en: '| DALL-E 2 [ramesh2022hierarchical](#bib.bib39) | 0.5750 | 0.5464 | 0.6374
    | 0.1283 | 0.3043 | 0.3696 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| DALL-E 2 [ramesh2022hierarchical](#bib.bib39) | 0.5750 | 0.5464 | 0.6374
    | 0.1283 | 0.3043 | 0.3696 |'
- en: '| Composable Diffusion [liu2022compositional](#bib.bib26) | 0.4063 | 0.3299
    | 0.3645 | 0.0800 | 0.2980 | 0.2898 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 可组合扩散 [liu2022compositional](#bib.bib26) | 0.4063 | 0.3299 | 0.3645 | 0.0800
    | 0.2980 | 0.2898 |'
- en: '| StructureDiffusion [feng2023trainingfree](#bib.bib12) | 0.4990 | 0.4218 |
    0.4900 | 0.1386 | 0.3111 | 0.3355 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| StructureDiffusion [feng2023trainingfree](#bib.bib12) | 0.4990 | 0.4218 |
    0.4900 | 0.1386 | 0.3111 | 0.3355 |'
- en: '| Attn-Exct [chefer2023attend](#bib.bib3) | 0.6400 | 0.4517 | 0.5963 | 0.1455
    | 0.3109 | 0.3401 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Attn-Exct [chefer2023attend](#bib.bib3) | 0.6400 | 0.4517 | 0.5963 | 0.1455
    | 0.3109 | 0.3401 |'
- en: '| GORS [huang2023t2i](#bib.bib17) | 0.6603 | 0.4785 | 0.6287 | 0.1815 | 0.3193
    | 0.3328 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| GORS [huang2023t2i](#bib.bib17) | 0.6603 | 0.4785 | 0.6287 | 0.1815 | 0.3193
    | 0.3328 |'
- en: '| SDXL [podell2023sdxl](#bib.bib36) | 0.5879 | 0.4687 | 0.5299 | 0.2133 | 0.3119
    | 0.3237 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SDXL [podell2023sdxl](#bib.bib36) | 0.5879 | 0.4687 | 0.5299 | 0.2133 | 0.3119
    | 0.3237 |'
- en: '| PixArt-$\alpha$ [chen2023pixart](#bib.bib6) | 0.6690 | 0.4927 | 0.6477 |
    0.2064 | 0.3197 | 0.3433 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| PixArt-$\alpha$ [chen2023pixart](#bib.bib6) | 0.6690 | 0.4927 | 0.6477 |
    0.2064 | 0.3197 | 0.3433 |'
- en: '| CompAgent [wang2024divide](#bib.bib50) | 0.7760 | 0.6105 | 0.7008 | 0.4837
    | 0.3212 | 0.3972 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| CompAgent [wang2024divide](#bib.bib50) | 0.7760 | 0.6105 | 0.7008 | 0.4837
    | 0.3212 | 0.3972 |'
- en: '| DALL-E 3 [betker2023improving](#bib.bib1) | 0.7785 | 0.6205 | 0.7036 | 0.2865
    | 0.3003 | 0.3773 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| DALL-E 3 [betker2023improving](#bib.bib1) | 0.7785 | 0.6205 | 0.7036 | 0.2865
    | 0.3003 | 0.3773 |'
- en: '| GenArtist (ours) | 0.8482 | 0.6948 | 0.7709 | 0.5437 | 0.3346 | 0.4499 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GenArtist (我们的方法) | 0.8482 | 0.6948 | 0.7709 | 0.5437 | 0.3346 | 0.4499 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: In this section, we demonstrate the effectiveness of our GenArtist and its unified
    ability through extensive experiments in image generation and editing. For image
    generation, we mainly conduct quantitative comparisons on the recent T2I-CompBench
    benchmark [huang2023t2i](#bib.bib17) . It is mainly about image generation with
    complex text prompts, involving multiple objects together with their own attributes
    or relationships. For image editing, we mainly conduct comparisons on the MagicBrush
    benchmark [zhang2024magicbrush](#bib.bib60) , which involves multiple types of
    text instructions, both single-turn and multi-turn dialogs for image editing.
    We choose GPT-4 [gpt42023](#bib.bib33) as our MLLM agent. In quantitative comparative
    experiments, we constrain the editing tree to be a binary tree.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过广泛的图像生成和编辑实验展示了我们 GenArtist 的有效性及其统一能力。对于图像生成，我们主要在近期的 T2I-CompBench
    基准上进行定量比较 [huang2023t2i](#bib.bib17)。该基准主要涉及复杂文本提示的图像生成，涉及多个对象及其自身属性或关系。对于图像编辑，我们主要在
    MagicBrush 基准 [zhang2024magicbrush](#bib.bib60) 上进行比较，该基准涉及多种类型的文本指令，包括单轮和多轮对话的图像编辑。我们选择了
    GPT-4 [gpt42023](#bib.bib33) 作为我们的 MLLM 代理。在定量比较实验中，我们将编辑树限制为二叉树。
- en: 4.1 Comparison with Image Generation Methods
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 与图像生成方法的比较
- en: 'We list the quantitative metric results of our GenArtist in Tab. [2](#S4.T2
    "Table 2 ‣ 4 Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified Image
    Generation and Editing") and compare with existing state-of-the-art text-to-image
    synthesis methods. It can be seen that our GenArtist consistently achieves better
    performance on all sub-categories. This demonstrates that for the text-to-image
    generation task, our system effectively achieves better control over text-to-image
    correspondence and higher accuracy in generated images, especially in the case
    of complicated text prompts. It can be observed that based on Stable Diffusion,
    both scaling-up models such as SDXL, PixArt-$\alpha$, and those methods specifically
    designed for this context like Attn-Exct, GORS, can achieve higher accuracy. In
    contrast, our approach, by integrating various models as tools, effectively harnesses
    the strengths of these two categories of methods. Additionally, the self-correction
    mechanism further ensures the accuracy of the generated images. Compared to the
    current state-of-the-art model DALL-E 3, our method achieves nearly a 7% improvement
    in attribute binding, and a more than 20% improvement in spatial relationships,
    partly due to the inclusion of position-sensitive tools and the input of position
    information during tool selection. Compared to CompAgent, a method that also employs
    an AI agent for compositional text-to-image generation, GenArtist achieves a 6%
    improvement on average, partially because our system encompasses a more comprehensive
    framework for both generation and self-correction. The capability in image generation
    of our unified system can thus be demonstrated.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表 [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ GenArtist: Multimodal LLM as an
    Agent for Unified Image Generation and Editing") 中列出了 GenArtist 的定量度量结果，并与现有的最先进的文本到图像合成方法进行了比较。可以看出，我们的
    GenArtist 在所有子类别中都能 consistently 实现更好的性能。这表明，对于文本到图像生成任务，我们的系统有效地实现了对文本到图像对应的更好控制，并且生成图像的准确性更高，尤其是在复杂文本提示的情况下。可以观察到，基于
    Stable Diffusion 的模型，如 SDXL、PixArt-$\alpha$，以及专为此情境设计的方法，如 Attn-Exct、GORS，都能实现更高的准确性。相比之下，我们的方法通过将各种模型作为工具进行整合，有效地利用了这两类方法的优势。此外，自我校正机制进一步确保了生成图像的准确性。与当前最先进的模型
    DALL-E 3 相比，我们的方法在属性绑定上实现了近 7% 的改善，在空间关系上实现了超过 20% 的提升，部分原因是引入了位置敏感工具和在工具选择过程中输入了位置信息。与也使用
    AI 代理进行组合文本到图像生成的 CompAgent 相比，GenArtist 平均提高了 6%，部分原因是我们的系统涵盖了更全面的生成和自我校正框架。因此，可以展示我们统一系统在图像生成方面的能力。'
- en: 4.2 Comparison with Image Editing Methods
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 与图像编辑方法的比较
- en: 'Table 3: Quantitative Comparison on MagicBrush with existing image editing
    methods. Multi-turn setting evaluates images that iteratively edited on the previous
    source images in edit sessions.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：MagicBrush 与现有图像编辑方法的定量比较。多轮设置评估在编辑会话中对先前源图像进行迭代编辑的图像。
- en: '|   Settinigs | Methods | L1$\downarrow$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|   设置 | 方法 | L1$\downarrow$ |'
- en: '| Single-turn | Null Text Inversion [mokady2023null](#bib.bib31) | 0.0749 |
    0.0197 | 0.8827 | 0.8206 | 0.2737 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 单轮 | Null Text Inversion [mokady2023null](#bib.bib31) | 0.0749 | 0.0197 |
    0.8827 | 0.8206 | 0.2737 |'
- en: '| HIVE [zhang2023hive](#bib.bib62) | 0.1092 | 0.0341 | 0.8519 | 0.7500 | 0.2752
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| HIVE [zhang2023hive](#bib.bib62) | 0.1092 | 0.0341 | 0.8519 | 0.7500 | 0.2752
    |'
- en: '| InstructPix2Pix [brooks2023instructpix2pix](#bib.bib2) | 0.1122 | 0.0371
    | 0.8524 | 0.7428 | 0.2764 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| InstructPix2Pix [brooks2023instructpix2pix](#bib.bib2) | 0.1122 | 0.0371
    | 0.8524 | 0.7428 | 0.2764 |'
- en: '| MagicBrush [zhang2024magicbrush](#bib.bib60) | 0.0625 | 0.0203 | 0.9332 |
    0.8987 | 0.2781 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MagicBrush [zhang2024magicbrush](#bib.bib60) | 0.0625 | 0.0203 | 0.9332 |
    0.8987 | 0.2781 |'
- en: '| SmartEdit [huang2023smartedit](#bib.bib18) | 0.0810 | - | 0.9140 | 0.8150
    | 0.3050 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SmartEdit [huang2023smartedit](#bib.bib18) | 0.0810 | - | 0.9140 | 0.8150
    | 0.3050 |'
- en: '| GenArtist (ours) | 0.0536 | 0.0147 | 0.9403 | 0.9131 | 0.3129 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GenArtist (ours) | 0.0536 | 0.0147 | 0.9403 | 0.9131 | 0.3129 |'
- en: '| Multi-turn | Null Text Inversion [mokady2023null](#bib.bib31) | 0.1057 |
    0.0335 | 0.8468 | 0.7529 | 0.2710 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 多轮 | Null Text Inversion [mokady2023null](#bib.bib31) | 0.1057 | 0.0335 |
    0.8468 | 0.7529 | 0.2710 |'
- en: '| HIVE [zhang2023hive](#bib.bib62) | 0.1521 | 0.0557 | 0.8004 | 0.6463 | 0.2673
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| HIVE [zhang2023hive](#bib.bib62) | 0.1521 | 0.0557 | 0.8004 | 0.6463 | 0.2673
    |'
- en: '| InstructPix2Pix [brooks2023instructpix2pix](#bib.bib2) | 0.1584 | 0.0598
    | 0.7924 | 0.6177 | 0.2726 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| InstructPix2Pix [brooks2023instructpix2pix](#bib.bib2) | 0.1584 | 0.0598
    | 0.7924 | 0.6177 | 0.2726 |'
- en: '| MagicBrush [zhang2024magicbrush](#bib.bib60) | 0.0964 | 0.0353 | 0.8924 |
    0.8273 | 0.2754 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| MagicBrush [zhang2024magicbrush](#bib.bib60) | 0.0964 | 0.0353 | 0.8924 |
    0.8273 | 0.2754 |'
- en: '| GenArtist (ours) | 0.0858 | 0.0298 | 0.9071 | 0.8492 | 0.3067 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GenArtist (ours) | 0.0858 | 0.0298 | 0.9071 | 0.8492 | 0.3067 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: 'We then list the comparative quantitative comparisons on the image editing
    benchmark MagicBrush in Tab. [3](#S4.T3 "Table 3 ‣ 4.2 Comparison with Image Editing
    Methods ‣ 4 Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified Image
    Generation and Editing"). Our GenArtist also achieves superior editing results,
    no matter in the single-turn or multi-turn setting, compared to both previous
    global description-guided methods like Null Text Inversion and instruction-guided
    methods like InstrctPix2Pix and MagicBrush. The main reason is that editing operations
    are highly diverse, and it’s challenging for a single model to achieve excellent
    performance across all these diverse editing operations. In contrast, our method
    can leverage the strengths of different models comprehensively. Additionally,
    the planning tree can effectively consider scenarios where model execution fails,
    making editing results more reliable and accurate. The capability in image editing
    of our unified system can thus be demonstrated.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，我们在表格[3](#S4.T3 "Table 3 ‣ 4.2 Comparison with Image Editing Methods ‣ 4
    Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation
    and Editing")中列出了图像编辑基准 MagicBrush 的比较定量数据。我们的 GenArtist 也在单轮和多轮设置中都实现了优越的编辑结果，相比于以往的全局描述指导方法如
    Null Text Inversion 以及指令指导方法如 InstrctPix2Pix 和 MagicBrush。主要原因在于编辑操作种类繁多，很难有单一模型在所有这些多样化的编辑操作中都表现出色。相比之下，我们的方法能够全面利用不同模型的优势。此外，规划树能够有效考虑模型执行失败的情况，使编辑结果更加可靠和准确。因此，我们统一系统在图像编辑方面的能力得到了体现。'
- en: 4.3 Ablation Study
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: 'Table 4: Ablation Study on T2I-CompBench. The upper section is about relevant
    tools from the generation tool library, then we study the tool selection and planning
    mechanisms respectively.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：T2I-CompBench上的消融研究。上部分涉及生成工具库中的相关工具，下部分我们分别研究了工具选择和规划机制。
- en: '|      Method | Attribute Binding | Object Relationship | Complex$\uparrow$
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|      方法 | 属性绑定 | 对象关系 | 复杂度$\uparrow$ |'
- en: '| Color $\uparrow$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 颜色 $\uparrow$ |'
- en: '| Stable Diffusion v2 [rombach2022high](#bib.bib40) | 0.5065 | 0.4221 | 0.4922
    | 0.1342 | 0.3096 | 0.3386 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Stable Diffusion v2 [rombach2022high](#bib.bib40) | 0.5065 | 0.4221 | 0.4922
    | 0.1342 | 0.3096 | 0.3386 |'
- en: '| LMD [lian2023llm](#bib.bib24) | 0.5736 | 0.5334 | 0.5227 | 0.2704 | 0.3073
    | 0.3083 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LMD [lian2023llm](#bib.bib24) | 0.5736 | 0.5334 | 0.5227 | 0.2704 | 0.3073
    | 0.3083 |'
- en: '| BoxDiff [xie2023boxdiff](#bib.bib53) | 0.6374 | 0.4869 | 0.6100 | 0.2625
    | 0.3158 | 0.3457 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| BoxDiff [xie2023boxdiff](#bib.bib53) | 0.6374 | 0.4869 | 0.6100 | 0.2625
    | 0.3158 | 0.3457 |'
- en: '| $\lambda$-ECLIPSE [patel2024lambda](#bib.bib34) | 0.4581 | 0.4420 | 0.5084
    | 0.1285 | 0.2922 | 0.3131 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda$-ECLIPSE [patel2024lambda](#bib.bib34) | 0.4581 | 0.4420 | 0.5084
    | 0.1285 | 0.2922 | 0.3131 |'
- en: '| SDXL [podell2023sdxl](#bib.bib36) | 0.5879 | 0.4687 | 0.5299 | 0.2133 | 0.3119
    | 0.3237 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| SDXL [podell2023sdxl](#bib.bib36) | 0.5879 | 0.4687 | 0.5299 | 0.2133 | 0.3119
    | 0.3237 |'
- en: '| PixArt-$\alpha$ [chen2023pixart](#bib.bib6) | 0.6690 | 0.4927 | 0.6477 |
    0.2064 | 0.3197 | 0.3433 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| PixArt-$\alpha$ [chen2023pixart](#bib.bib6) | 0.6690 | 0.4927 | 0.6477 |
    0.2064 | 0.3197 | 0.3433 |'
- en: '| + tool selection | 0.7028 | 0.5764 | 0.6883 | 0.4305 | 0.3187 | 0.3739 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| + 工具选择 | 0.7028 | 0.5764 | 0.6883 | 0.4305 | 0.3187 | 0.3739 |'
- en: '| + planning chain | 0.7509 | 0.6045 | 0.7192 | 0.4787 | 0.3216 | 0.4095 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| + 规划链 | 0.7509 | 0.6045 | 0.7192 | 0.4787 | 0.3216 | 0.4095 |'
- en: '| + planning tree | 0.8482 | 0.6948 | 0.7709 | 0.5437 | 0.3346 | 0.4499 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| + 规划树 | 0.8482 | 0.6948 | 0.7709 | 0.5437 | 0.3346 | 0.4499 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: 'We finally conduct the ablation study on the T2I-CompBench benchmark and list
    the results in Tab. [4](#S4.T4 "Table 4 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣
    GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing").
    We present the results of Stable Diffusion as a reference. The top section includes
    various tools relevant to the task, including text-to-image, layout-to-image,
    and customized generation methods. It can be observed that through scaling up
    or additional design, these tools have generally achieved better results than
    Stable Diffusion. After tool selection by the MLLM agent, the quantitative metrics
    outperform all these tools. This demonstrates that the agent can effectively choose
    appropriate tools based on the content of text prompts, thus achieving superior
    performance compared to all these tools. If we use a chain structure for planning
    to further correct the images, we achieve an average improvement of 3%, demonstrating
    the necessity of verification and correction of erroneous results. Furthermore,
    by utilizing a tree structure, we can further consider and handle cases where
    the editing tool fails, resulting in even more reliable output results. Such an
    ablation study illustrates the necessity of integrating multiple models as tools
    and utilizing tree structure for planning. The reasonableness of our agent-centric
    system designs can also be demonstrated.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '我们最终在T2I-CompBench基准上进行消融研究，并在表[4](#S4.T4 "Table 4 ‣ 4.3 Ablation Study ‣ 4
    Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation
    and Editing")中列出结果。我们展示了Stable Diffusion的结果作为参考。顶部部分包含与任务相关的各种工具，包括文本到图像、布局到图像以及定制生成方法。可以观察到，通过扩展或额外设计，这些工具通常取得了比Stable
    Diffusion更好的结果。在MLLM代理选择工具后，定量指标超越了所有这些工具。这表明代理可以有效地根据文本提示的内容选择合适的工具，从而比所有这些工具实现更优的性能。如果我们使用链结构进行规划以进一步修正图像，我们实现了平均3%的改善，展示了验证和修正错误结果的必要性。此外，通过利用树结构，我们可以进一步考虑和处理编辑工具失败的情况，产生更可靠的输出结果。这样的消融研究说明了将多个模型集成为工具和利用树结构进行规划的必要性。我们的代理中心系统设计的合理性也得到了证明。'
- en: '![Refer to caption](img/223589188f87915187299ca2082c7c94.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/223589188f87915187299ca2082c7c94.png)'
- en: 'Figure 4: Visualization of the planning tree for image generation tasks.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：图像生成任务的规划树可视化。
- en: '![Refer to caption](img/143b701e5ec68a3b2303ab3e66e98f84.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/143b701e5ec68a3b2303ab3e66e98f84.png)'
- en: 'Figure 5: Visualization of the planning tree for image editing tasks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：图像编辑任务的规划树可视化。
- en: 'We further list some visualized generation examples in Fig. [4](#S4.F4 "Figure
    4 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ GenArtist: Multimodal LLM as an Agent
    for Unified Image Generation and Editing") to illustrate our planning tree and
    how the system proceeds. In the first example, as the text prompts contain multiple
    objects, the agent chooses the LMD tool for generation. However, there are still
    some errors in the image. The agent first attempts to use the attribute editing
    tool to change the leftmost sheep to white, but it fails. The agent further attempts
    to modify the color using the replace tool, but after replacement, the size of
    the sheep becomes too small and not very noticeable. The agent then chooses to
    remove the black sheep and then adds a white sheep, successfully achieving the
    same effect as editing color. Finally, the agent uses the object addition tool
    to add a goat on the right side, ensuring that the image accurately matches the
    text prompt in the end. In the second example, due to the lack of clarity of the
    hair in the BoxDiff generated image, the editing tools cannot edit so that the
    hair correctly matches the description of "long black hair". Therefore, the agent
    invokes another generation tool to guarantee the final image is correct. Some
    image editing examples are also provided in Fig. [5](#S4.F5 "Figure 5 ‣ 4.3 Ablation
    Study ‣ 4 Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified Image
    Generation and Editing").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步在图 Fig. [4](#S4.F4 "图4 ‣ 4.3 消融研究 ‣ 4 实验 ‣ GenArtist: 多模态LLM作为统一图像生成和编辑的代理")
    中列出了一些可视化的生成示例，以说明我们的规划树和系统的运行方式。在第一个示例中，由于文本提示包含多个对象，代理选择了LMD工具进行生成。然而，图像中仍然存在一些错误。代理首先尝试使用属性编辑工具将最左侧的绵羊改为白色，但失败了。代理进一步尝试使用替换工具修改颜色，但替换后，绵羊的大小变得过小，不太显眼。随后，代理选择移除黑绵羊，然后添加白绵羊，成功实现了与编辑颜色相同的效果。最后，代理使用对象添加工具在右侧添加了一只山羊，确保图像最终准确匹配文本提示。在第二个示例中，由于BoxDiff生成图像中的头发不够清晰，编辑工具无法将头发正确匹配为“长黑发”的描述。因此，代理调用了另一个生成工具以确保最终图像正确。一些图像编辑示例也在图
    Fig. [5](#S4.F5 "图5 ‣ 4.3 消融研究 ‣ 4 实验 ‣ GenArtist: 多模态LLM作为统一图像生成和编辑的代理") 中提供。'
- en: 5 Conclusion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose GenArtist, a unified image generation and editing
    system coordinated by a MLLM agent. By decomposing input problems, employing the
    tree structure for planning and invoking external tools for execution, the MLLM
    agent acts as the "brain" to generate high-fidelity and accurate images for various
    tasks. Extensive experiments demonstrate that GenArtist well addresses complex
    problems in image generation and editing, and achieves state-of-the-art performance
    compared to existing methods. Its ability in a wide range of generation tasks
    also validates its unified capacity. We believe our approach of leveraging the
    agent to achieve a unified image generation and editing system with enhanced controllability
    can provide valuable insights for future research, and we consider it an important
    step toward the future of autonomous agents.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了GenArtist，一个由MLLM代理协调的统一图像生成和编辑系统。通过分解输入问题，使用树结构进行规划以及调用外部工具执行，MLLM代理作为“大脑”生成高保真和准确的图像以满足各种任务。广泛的实验表明，GenArtist有效解决了图像生成和编辑中的复杂问题，并且与现有方法相比，取得了**最先进**的性能。其在各种生成任务中的能力也验证了其统一能力。我们相信，我们利用代理实现具有增强控制能力的统一图像生成和编辑系统的方法，可以为未来的研究提供有价值的见解，我们认为这是迈向未来自主代理的重要一步。
- en: References
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li,
    Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation
    with better captions. Computer Science. https://cdn. openai.com/papers/dall-e-3.pdf,
    2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li,
    Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, 等等。通过更好的字幕改善图像生成。计算机科学。https://cdn.openai.com/papers/dall-e-3.pdf,
    2023。'
- en: '[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning
    to follow image editing instructions. In CVPR, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Tim Brooks, Aleksander Holynski, 和 Alexei A Efros. Instructpix2pix: 学习跟随图像编辑指令。在CVPR,
    2023。'
- en: '[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or.
    Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion
    models. TOG, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, 和 Daniel Cohen-Or. Attend-and-excite:
    基于注意力的语义指导用于文本到图像扩散模型。TOG, 2023。'
- en: '[4] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei.
    Textdiffuser-2: Unleashing the power of language models for text rendering. arXiv
    preprint arXiv:2311.16465, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen 和 Furu Wei。Textdiffuser-2：释放语言模型在文本渲染中的力量。arXiv
    预印本 arXiv:2311.16465，2023年。'
- en: '[5] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei.
    Textdiffuser: Diffusion models as text painters. In NeurIPS, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen 和 Furu Wei。Textdiffuser：扩散模型作为文本画家。在
    NeurIPS，2023年。'
- en: '[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao
    Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-$\alpha$: Fast training
    of diffusion transformer for photorealistic text-to-image synthesis. In ICLR,
    2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao
    Wang, James Kwok, Ping Luo, Huchuan Lu 等人。Pixart-$\alpha$：用于照片级真实感文本到图像合成的扩散变换器的快速训练。在
    ICLR，2024年。'
- en: '[7] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control
    with cross-attention guidance. In WACV, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Minghao Chen, Iro Laina 和 Andrea Vedaldi。无训练的布局控制与交叉注意力指导。在 WACV，2024年。'
- en: '[8] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang
    Zhao. Anydoor: Zero-shot object-level image customization. In CVPR, 2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao 和 Hengshuang Zhao。Anydoor：零样本对象级图像定制。在
    CVPR，2024年。'
- en: '[9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit:
    Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427,
    2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk 和 Matthieu Cord。Diffedit：基于扩散的语义图像编辑与掩模引导。arXiv
    预印本 arXiv:2210.11427，2022年。'
- en: '[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on
    image synthesis. In NeurIPS, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Prafulla Dhariwal 和 Alexander Nichol。扩散模型在图像合成上超越 GANs。在 NeurIPS，2021年。'
- en: '[11] Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam
    Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff,
    Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by
    combining language models with strategic reasoning. Science, 2022.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Meta 基础人工智能研究外交团队 (FAIR)†，Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele
    Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu
    等人。通过结合语言模型与战略推理在外交游戏中实现人类水平的游戏表现。《科学》，2022年。'
- en: '[12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna
    Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured
    diffusion guidance for compositional text-to-image synthesis. In ICLR, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna
    Narayana, Sugato Basu, Xin Eric Wang 和 William Yang Wang。无训练结构扩散指导用于组合文本到图像合成。在
    ICLR，2023年。'
- en: '[13] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai
    He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional
    visual planning and generation with large language models. In NeurIPS, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai
    He, Sugato Basu, Xin Eric Wang 和 William Yang Wang。Layoutgpt：使用大型语言模型进行组合视觉规划和生成。在
    NeurIPS，2023年。'
- en: '[14] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and
    Zhe Gan. Guiding instruction-based image editing via multimodal large language
    models. In ICLR, 2024.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang 和 Zhe
    Gan。通过多模态大型语言模型引导基于指令的图像编辑。在 ICLR，2024年。'
- en: '[15] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang,
    Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, et al. Instructdiffusion: A generalist
    modeling interface for vision tasks. In CVPR, 2024.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang,
    Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen 等人。Instructdiffusion：面向视觉任务的通用建模接口。在
    CVPR，2024年。'
- en: '[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models. In NeurIPS, 2020.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Jonathan Ho, Ajay Jain 和 Pieter Abbeel。去噪扩散概率模型。在 NeurIPS，2020年。'
- en: '[17] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench:
    A comprehensive benchmark for open-world compositional text-to-image generation.
    In NeurIPS, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li 和 Xihui Liu。T2i-compbench：用于开放世界组合文本到图像生成的综合基准。在
    NeurIPS，2023年。'
- en: '[18] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao
    Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring
    complex instruction-based image editing with multimodal large language models.
    In CVPR, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao
    Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang 等人。Smartedit：利用多模态大型语言模型探索复杂指令基础的图像编辑。在
    CVPR，2024年。'
- en: '[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
    Segment anything. In ICCV, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo，等。Segment
    anything。在 ICCV，2023。'
- en: '[20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan
    Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, 和 Jun-Yan
    Zhu. 多概念定制的文本到图像扩散。在 CVPR，2023。'
- en: '[21] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail
    Doshi. Playground v2.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, 和 Suhail
    Doshi. Playground v2。'
- en: '[22] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject
    representation for controllable text-to-image generation and editing. In NeurIPS,
    2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Dongxu Li, Junnan Li, 和 Steven CH Hoi. Blip-diffusion：用于可控文本到图像生成和编辑的预训练主题表示。在
    NeurIPS，2023。'
- en: '[23] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng
    Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation.
    In CVPR, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng
    Gao, Chunyuan Li, 和 Yong Jae Lee. Gligen：开放集基础的文本到图像生成。在 CVPR，2023。'
- en: '[24] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion:
    Enhancing prompt understanding of text-to-image diffusion models with large language
    models. arXiv preprint arXiv:2305.13655, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Long Lian, Boyi Li, Adam Yala, 和 Trevor Darrell. Llm-grounded diffusion：通过大语言模型增强对文本到图像扩散模型的提示理解。arXiv
    预印本 arXiv:2305.13655，2023。'
- en: '[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. In NeurIPS, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee. 视觉指令调整。在 NeurIPS，2023。'
- en: '[26] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum.
    Compositional visual generation with composable diffusion models. In ECCV, 2022.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, 和 Joshua B Tenenbaum.
    具有可组合扩散模型的组合视觉生成。在 ECCV，2022。'
- en: '[27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang,
    Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino
    with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang,
    Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu，等。Grounding dino：将 dino 与基础预训练结合用于开放集目标检测。arXiv
    预印本 arXiv:2303.05499，2023。'
- en: '[28] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen,
    Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, et al. Internchat: Solving
    vision-centric tasks by interacting with chatbots beyond language. arXiv preprint
    arXiv:2305.05662, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen,
    Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu，等。Internchat：通过与聊天机器人互动解决以视觉为中心的任务。arXiv
    预印本 arXiv:2305.05662，2023。'
- en: '[29] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng,
    Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis
    with multiple subjects. arXiv preprint arXiv:2305.19327, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng,
    Yu Liu, Deli Zhao, Jingren Zhou, 和 Yang Cao. Cones 2：具有多个主体的可定制图像合成。arXiv 预印本
    arXiv:2305.19327，2023。'
- en: '[30] Midjourney. Midjourney, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Midjourney. Midjourney，2023。'
- en: '[31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.
    Null-text inversion for editing real images using guided diffusion models. In
    CVPR, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, 和 Daniel Cohen-Or.
    使用引导扩散模型编辑真实图像的空文本反转。在 CVPR，2023。'
- en: '[32] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion:
    Enabling drag-style manipulation on diffusion models. In ICLR, 2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, 和 Jian Zhang. Dragondiffusion：在扩散模型上实现拖动风格操作。在
    ICLR，2024。'
- en: '[33] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] OpenAI. Gpt-4 技术报告。arXiv 预印本 arXiv:2303.08774，2023。'
- en: '[34] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou Yang. $\lambda$-eclipse:
    Multi-concept personalized text-to-image diffusion models by leveraging clip latent
    space. arXiv preprint arXiv:2402.05195, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Maitreya Patel, Sangmin Jung, Chitta Baral, 和 Yezhou Yang. $\lambda$-eclipse：通过利用
    clip 潜在空间的多概念个性化文本到图像扩散模型。arXiv 预印本 arXiv:2402.05195，2024。'
- en: '[35] William Peebles and Saining Xie. Scalable diffusion models with transformers.
    In ICCV, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] William Peebles 和 Saining Xie. 使用变换器的可扩展扩散模型。在 ICCV，2023。'
- en: '[36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn,
    Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models
    for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn,
    Jonas Müller, Joe Penna, 和 Robin Rombach。Sdxl: 改进潜在扩散模型以实现高分辨率图像合成。arXiv 预印本 arXiv:2307.01952,
    2023。'
- en: '[37] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, 和 Maosong Sun。用于软件开发的交流代理。arXiv 预印本 arXiv:2307.07924, 2023。'
- en: '[38] Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng
    Xiao, Rui Wang, and Shilei Wen. Diffusiongpt: Llm-driven text-to-image generation
    system. arXiv preprint arXiv:2401.10061, 2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng
    Xiao, Rui Wang, 和 Shilei Wen。Diffusiongpt: 基于 LLM 的文本到图像生成系统。arXiv 预印本 arXiv:2401.10061,
    2024。'
- en: '[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
    Hierarchical text-conditional image generation with clip latents. arXiv preprint
    arXiv:2204.06125, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, 和 Mark Chen。基于
    CLIP 潜在变量的分层文本条件图像生成。arXiv 预印本 arXiv:2204.06125, 2022。'
- en: '[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In CVPR,
    2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn
    Ommer。利用潜在扩散模型进行高分辨率图像合成。发表于 CVPR, 2022。'
- en: '[41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein,
    and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
    generation. In CVPR, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein,
    和 Kfir Aberman。Dreambooth: 针对主题驱动生成的文本到图像扩散模型微调。发表于 CVPR, 2023。'
- en: '[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
    Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
    et al. Photorealistic text-to-image diffusion models with deep language understanding.
    In NeurIPS, 2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
    L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans
    等。具有深度语言理解的逼真文本到图像扩散模型。发表于 NeurIPS, 2022。'
- en: '[43] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
    Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
    In NeurIPS, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, 和 Yueting
    Zhuang。Hugginggpt: 使用 chatgpt 和其在 huggingface 的朋友解决 AI 任务。发表于 NeurIPS, 2023。'
- en: '[44] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar,
    Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via
    recognition and generation tasks. arXiv preprint arXiv:2311.10089, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar,
    Oron Ashual, Devi Parikh, 和 Yaniv Taigman。Emu edit: 通过识别和生成任务进行精确图像编辑。arXiv 预印本
    arXiv:2311.10089, 2023。'
- en: '[45] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and
    Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based
    image editing. In CVPR, 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, 和 Song
    Bai。Dragdiffusion: 利用扩散模型进行交互式基于点的图像编辑。发表于 CVPR, 2024。'
- en: '[46] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova,
    Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park,
    and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions.
    In WACV, 2022.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova,
    Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park,
    和 Victor Lempitsky。使用傅里叶卷积进行分辨率鲁棒的大面积修补。发表于 WACV, 2022。'
- en: '[47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等。Llama: 开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971, 2023。'
- en: '[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等。Llama 2: 开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288, 2023。'
- en: '[49] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image
    diffusion models. In SIGGRAPH, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Andrey Voynov, Kfir Aberman, 和 Daniel Cohen-Or。基于草图指导的文本到图像扩散模型。发表于 SIGGRAPH,
    2023。'
- en: '[50] Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang, Xihui Liu, and Zhenguo
    Li. Divide and conquer: Language models can plan and self-correct for compositional
    text-to-image generation. arXiv preprint arXiv:2401.15688, 2024.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 王振宇, 谢恩泽, 李傲雪, 王钟道, 刘希辉, 和 李正国. 分而治之: 语言模型可以规划和自我纠正以生成组合文本到图像. arXiv 预印本
    arXiv:2401.15688, 2024.'
- en: '[51] Tsung-Han Wu, Long Lian, Joseph E Gonzalez, Boyi Li, and Trevor Darrell.
    Self-correcting llm-controlled diffusion models. In CVPR, 2024.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 吴宗汉, 连龙, 约瑟夫·E·冈萨雷斯, 李博艺, 和 特雷弗·达雷尔. 自我纠正的 llm 控制扩散模型. 在 CVPR, 2024.'
- en: '[52] Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo Durand, and Song
    Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention.
    arXiv preprint arXiv:2305.10431, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 肖光轩, 尹天玮, 威廉·T·弗里曼, 弗雷多·杜朗, 和 韩松. Fastcomposer: 无需调优的多主题图像生成与局部注意力. arXiv
    预印本 arXiv:2305.10431, 2023.'
- en: '[53] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng
    Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free
    box-constrained diffusion. In ICCV, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 谢金恒, 李跃翔, 黄雅文, 刘浩哲, 张文天, 郑业峰, 和 Mike Zheng Shou. Boxdiff: 无需训练的箱约束扩散的文本到图像合成.
    在 ICCV, 2023.'
- en: '[54] Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing
    Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et al. Lego-prover: Neural theorem
    proving with growing libraries. In ICLR, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 辛华建, 王海明, 郑川扬, 李琳, 刘正英, 曹青兴, 黄寅雅, 熊静, 石涵, 谢恩泽, 等. Lego-prover: 带有增长库的神经定理证明.
    在 ICLR, 2023.'
- en: '[55] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun,
    Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion
    models. In CVPR, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 杨斌欣, 顾书阳, 张博, 张婷, 陈雪津, 孙晓燕, 陈栋, 和 文芳. 按例绘制: 基于示例的扩散模型图像编辑. 在 CVPR, 2023.'
- en: '[56] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making:
    Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 杨辉, 许福, 和 赫云中. Auto-gpt用于在线决策: 基准测试和附加意见. arXiv 预印本 arXiv:2306.02224,
    2023.'
- en: '[57] Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming
    Cai, Wentao Zhang, and Bin Cui. Improving diffusion-based image synthesis with
    context prediction. In NeurIPS, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 杨玲, 刘静伟, 洪申达, 张志龙, 黄志林, 蔡哲明, 张文涛, 和 崔斌. 通过上下文预测改善基于扩散的图像合成. 在 NeurIPS,
    2023.'
- en: '[58] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin
    Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating
    with multimodal llms. In ICML, 2024.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 杨玲, 余兆辰, 孟晨琳, 徐敏凯, 斯特凡诺·厄尔蒙, 和 崔斌. 精通文本到图像扩散: 使用多模态 llms 的重新描述、规划和生成.
    在 ICML, 2024.'
- en: '[59] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and
    Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771,
    2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 赵阳, 刘佳璇, 韩宇成, 陈鑫, 黄泽标, 傅斌, 和 俞刚. Appagent: 作为智能手机用户的多模态智能体. arXiv 预印本
    arXiv:2312.13771, 2023.'
- en: '[60] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually
    annotated dataset for instruction-guided image editing. In NeurIPS, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 张凯, 莫灵博, 陈文虎, 孙欢, 和 苏宇. Magicbrush: 一个用于指令引导图像编辑的人工注释数据集. 在 NeurIPS, 2023.'
- en: '[61] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control
    to text-to-image diffusion models. In ICCV, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 张吕敏, 饶安逸, 和 曼尼什·阿格拉瓦拉. 为文本到图像扩散模型添加条件控制. 在 ICCV, 2023.'
- en: '[62] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan
    Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human
    feedback for instructional visual editing. In CVPR, 2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 张舒, 杨欣怡, 冯宜浩, 秦灿, 陈家驰, 于宁, 陈泽元, 王欢, 西尔维奥·萨瓦雷斯, 斯特凡诺·厄尔蒙, 等. Hive: 利用人类反馈进行指令性视觉编辑.
    在 CVPR, 2024.'
- en: '[63] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming
    Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models.
    In CVPR, 2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 张玉欣, 黄妮莎, 唐凡, 黄海滨, 马崇阳, 董伟明, 和 徐长生. 基于反转的风格迁移与扩散模型. 在 CVPR, 2023.'
- en: Appendix
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: In the appendix, we primarily include more quantitative comparisons, along with
    additional visual results, to more comprehensively compare with existing state-of-the-art
    methods.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '-   附录中，我们主要包含了更多的定量比较以及附加的视觉结果，以更全面地与现有最先进的方法进行比较。'
- en: Appendix A More Quantitative Experiments
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 更多定量实验
- en: 'Table 5: Quantitative Comparison on T2I-CompBench with existing text-to-image
    generation models and compositional methods. The metric here is from the officially
    old-version code.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '-   表 5: 在 T2I-CompBench 上与现有文本到图像生成模型和组合方法的定量比较。这里的指标来自于官方旧版代码。'
- en: '|      Model | Attribute Binding | Object Relationship | Complex$\uparrow$
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|      模型 | 属性绑定 | 对象关系 | 复杂度$\uparrow$ |'
- en: '| Color $\uparrow$ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 颜色 $\uparrow$ |'
- en: '| SDXL [podell2023sdxl](#bib.bib36) | 0.6369 | 0.5408 | 0.5637 | 0.2032 | 0.3110
    | 0.4091 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| SDXL [podell2023sdxl](#bib.bib36) | 0.6369 | 0.5408 | 0.5637 | 0.2032 | 0.3110
    | 0.4091 |'
- en: '| PixArt-$\alpha$ [chen2023pixart](#bib.bib6) | 0.6886 | 0.5582 | 0.7044 |
    0.2082 | 0.3179 | 0.4117 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| PixArt-$\alpha$ [chen2023pixart](#bib.bib6) | 0.6886 | 0.5582 | 0.7044 |
    0.2082 | 0.3179 | 0.4117 |'
- en: '| ConPreDiff [yang2024improving](#bib.bib57) | 0.7019 | 0.5637 | 0.7021 | 0.2362
    | 0.3195 | 0.4184 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| ConPreDiff [yang2024improving](#bib.bib57) | 0.7019 | 0.5637 | 0.7021 | 0.2362
    | 0.3195 | 0.4184 |'
- en: '| DALL-E 3 [betker2023improving](#bib.bib1) | 0.8110 | 0.6750 | 0.8070 | -
    | - | - |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| DALL-E 3 [betker2023improving](#bib.bib1) | 0.8110 | 0.6750 | 0.8070 | -
    | - | - |'
- en: '| CompAgent [wang2024divide](#bib.bib50) | 0.8488 | 0.7233 | 0.7916 | 0.4837
    | 0.3212 | 0.4863 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CompAgent [wang2024divide](#bib.bib50) | 0.8488 | 0.7233 | 0.7916 | 0.4837
    | 0.3212 | 0.4863 |'
- en: '| RPG [yang2024mastering](#bib.bib58) | 0.8335 | 0.6801 | 0.8129 | 0.4547 |
    0.3462 | 0.5408 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| RPG [yang2024mastering](#bib.bib58) | 0.8335 | 0.6801 | 0.8129 | 0.4547 |
    0.3462 | 0.5408 |'
- en: '| GenArtist (ours) | 0.8837 | 0.8014 | 0.8771 | 0.5437 | 0.3346 | 0.5514 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| GenArtist (我们的) | 0.8837 | 0.8014 | 0.8771 | 0.5437 | 0.3346 | 0.5514 |'
- en: '|   |  |  |  |  |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |'
- en: 'Considering that many existing methods on T2I-CompBench report results based
    on the official old version evaluation code, here we utilize the same old version
    evaluation method and list the results in Tab. [5](#A1.T5 "Table 5 ‣ Appendix
    A More Quantitative Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified
    Image Generation and Editing"). It can be observed that the performance improvement
    keeps consistently under this metric. Compared to the current state-of-the-art
    text-to-image method, DALL-E 3, our approach achieves over a 7% improvement in
    attribute binding. For shape-related attributes, the improvement is even up to
    12.64%. Additionally, compared to RPG, which also utilizes MLLM to assist image
    generation, our method demonstrates an over 5% enhancement. This is because our
    GenArtist incorporates MLLM for step-by-step verification and the corresponding
    planning, thereby better ensuring the correctness of the images. This quantitative
    comparison more comprehensively demonstrates the effectiveness of our method.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到许多现有方法在 T2I-CompBench 上报告的结果基于官方旧版评估代码，这里我们使用相同的旧版评估方法，并在表格[5](#A1.T5 "表
    5 ‣ 附录 A 更多定量实验 ‣ GenArtist: 多模态 LLM 作为统一图像生成和编辑的代理")中列出了结果。可以观察到，性能改进在这个指标下保持一致。与当前最先进的文本到图像方法
    DALL-E 3 相比，我们的方法在属性绑定方面提高了超过 7%。对于与形状相关的属性，改进甚至达到 12.64%。此外，与同样利用 MLLM 来协助图像生成的
    RPG 相比，我们的方法展示了超过 5% 的提升。这是因为我们的 GenArtist 结合了 MLLM 进行逐步验证和相应的规划，从而更好地确保了图像的正确性。这一定量比较更全面地展示了我们方法的有效性。'
- en: Appendix B More Qualitative Experiments
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 更多定性实验
- en: In this section, we provide more visual analyses to further illustrate our GenArtist
    and to compare it more thoroughly with existing methods.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了更多的视觉分析，以进一步说明我们的 GenArtist，并与现有方法进行更全面的比较。
- en: '![Refer to caption](img/8520f95d08ad6328054e2ffd8cc60c65.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8520f95d08ad6328054e2ffd8cc60c65.png)'
- en: 'Figure 6: Qualitative comparison with existing state-of-the-art methods for
    image generation tasks. We compare our GenArtist with SOTA text-to-image models
    including SDXL [podell2023sdxl](#bib.bib36) , LMD+ [lian2023llm](#bib.bib24) ,
    RPG [yang2024mastering](#bib.bib58) , PixArt-$\alpha$ [chen2023pixart](#bib.bib6)
    , Playground [playground](#bib.bib21) , Midjourney [Midjourney](#bib.bib30) ,
    DALL-E 3 [betker2023improving](#bib.bib1) .'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：与现有最先进方法在图像生成任务中的定性比较。我们将我们的 GenArtist 与包括 SDXL [podell2023sdxl](#bib.bib36)、LMD+
    [lian2023llm](#bib.bib24)、RPG [yang2024mastering](#bib.bib58)、PixArt-$\alpha$
    [chen2023pixart](#bib.bib6)、Playground [playground](#bib.bib21)、Midjourney [Midjourney](#bib.bib30)、DALL-E
    3 [betker2023improving](#bib.bib1) 在内的 SOTA 文本到图像模型进行了比较。
- en: '![Refer to caption](img/1ddc3c27a99730aa13cab2f50a73f119.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1ddc3c27a99730aa13cab2f50a73f119.png)'
- en: 'Figure 7: Qualitative comparison with existing state-of-the-art methods for
    image editing tasks. We compare our GenArtist with SOTA image editing models including
    InstructPix2Pix [brooks2023instructpix2pix](#bib.bib2) , MagicBrush [zhang2024magicbrush](#bib.bib60)
    , MGIE [fu2023guiding](#bib.bib14) , InstructDiffusion [geng2023instructdiffusion](#bib.bib15)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：与现有最先进方法在图像编辑任务中的定性比较。我们将我们的 GenArtist 与包括 InstructPix2Pix [brooks2023instructpix2pix](#bib.bib2)、MagicBrush
    [zhang2024magicbrush](#bib.bib60)、MGIE [fu2023guiding](#bib.bib14)、InstructDiffusion
    [geng2023instructdiffusion](#bib.bib15) 在内的 SOTA 图像编辑模型进行了比较。
- en: '![Refer to caption](img/3c73bbacf67c10fc8b7a7243e26b101b.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3c73bbacf67c10fc8b7a7243e26b101b.png)'
- en: 'Figure 8: Visualized results of GenArtist about various tasks and user instructions.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: GenArtist 关于各种任务和用户指令的可视化结果。'
- en: '![Refer to caption](img/7825bc4b4f3785ece790289f37b38b34.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7825bc4b4f3785ece790289f37b38b34.png)'
- en: 'Figure 9: Visualization of the step-by-step process for image generation tasks.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 图像生成任务的逐步过程可视化。'
- en: '![Refer to caption](img/356ba1a0cde52b85e445d237e1f46455.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/356ba1a0cde52b85e445d237e1f46455.png)'
- en: 'Figure 10: Visualization of the step-by-step process for image editing tasks.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 图像编辑任务的逐步过程可视化。'
- en: 'Comparative visualized results on image generation. We first present visual
    comparisons with existing methods in Fig. [6](#A2.F6 "Figure 6 ‣ Appendix B More
    Qualitative Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified Image
    Generation and Editing"). It can be observed that our GenArtist achieves superior
    results in multiple aspects: 1) *Attribute Binding*. For example, in the fourth
    example, there are strict requirements for the clothes and pants each person is
    wearing. Such numerous requirements are challenging for existing methods to meet.
    In this case, GenArtist can continuously verify and edit to ensure all these requirements
    are correctly satisfied. 2) *Numeric Accuracy*. In the second example, detailed
    quantity requirements are given for various objects. Our method can gradually
    achieve the correct quantities through addition and removal operations. In contrast,
    even though methods like LMD+ can meet numeric accuracy, they struggle to maintain
    the accuracy of other aspects, such as the atmosphere of the image. 3) *Position
    Accuracy*. By position-aware tool execution, better position-related accuracy
    can be guaranteed. In the first example, although DALL-E 3 can correctly predict
    many other aspects, it fails to accurately place the book on the left floor, which
    our method can achieve. 4) *Complex Relationships*, like the complex requirements
    for the relationship between the panda and bamboo in the fifth example. 5) *Other
    Diverse Requirements*. By integrating various tools, GenArtist effectively leverages
    the strengths of different tools to meet diverse requirements, the ability that
    a single model lacks. For instance, the text requirements in the third example
    are better handled by our method. Such visualized results strongly demonstrate
    the effectiveness of our method in image generation.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '图像生成的比较可视化结果。我们首先在图 [6](#A2.F6 "图 6 ‣ 附录 B 更多定性实验 ‣ GenArtist: 统一图像生成和编辑的多模态
    LLM 代理") 中呈现与现有方法的视觉比较。可以观察到，我们的 GenArtist 在多个方面取得了优越的结果：1) *属性绑定*。例如，在第四个例子中，对每个人穿的衣服和裤子有严格要求。这些众多要求对于现有方法来说难以满足。在这种情况下，GenArtist
    可以不断验证和编辑以确保所有这些要求得到正确满足。2) *数值准确性*。在第二个例子中，对各种对象给出了详细的数量要求。我们的方法可以通过添加和移除操作逐渐达到正确的数量。相比之下，尽管像
    LMD+ 这样的 方法可以满足数值准确性，但它们难以保持其他方面的准确性，例如图像的氛围。3) *位置准确性*。通过位置感知工具的执行，可以保证更好的位置相关准确性。在第一个例子中，尽管
    DALL-E 3 可以正确预测许多其他方面，但它未能准确地将书本放在左侧地板上，而我们的 方法则可以做到这一点。4) *复杂关系*，如第五个例子中熊猫和竹子之间的复杂关系要求。5)
    *其他多样化要求*。通过整合各种工具，GenArtist 有效地利用了不同工具的优势来满足多样化的要求，这是单一模型所缺乏的。例如，第三个例子中的文本要求由我们的方法更好地处理。这些可视化结果强有力地展示了我们方法在图像生成中的有效性。'
- en: 'Comparative visualized results on image editing. We further present comparisons
    with existing image editing methods in Fig. [7](#A2.F7 "Figure 7 ‣ Appendix B
    More Qualitative Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified
    Image Generation and Editing"). GenArtist shows superior performance in several
    aspects: 1) *Highly Specific Editing Instructions*. For instance, in the first
    example, only a particular pizza needs to be modified, while the second example
    requires changes to the color and placement of a vase. Existing methods often
    struggle to satisfy such specific requirements. 2) *Reasoning-based Instructions*.
    The third example requires the model to autonomously determine which person needs
    to be removed. Because of the reasoning capability of the MLLM agent, our method
    can accurately make this determination. In contrast, even MGIE, which also uses
    MLLM assistance, fails to make the correct modification. 3) *Instructions with
    Excessive Requirements*. The fourth example requires different modifications to
    both birds, which existing methods struggle to achieve. 4) *Multi-step Instructions.*
    The fifth example involves complex instructions including multiple operations.
    The MLLM agent can decompose the problem into multiple single-step operations,
    simplifying complex tasks. 5) *Diverse Operations*. It can be seen that our method
    excels in various editing operations, such as addition, removal, and attribute
    editing, due to the integration of different tools. These comparisons strongly
    demonstrate the effectiveness of our method in image editing.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '图像编辑的对比可视化结果。我们进一步展示了与现有图像编辑方法的比较，见图 [7](#A2.F7 "Figure 7 ‣ Appendix B More
    Qualitative Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified Image
    Generation and Editing")。GenArtist 在多个方面表现出色：1) *高度特定的编辑指令*。例如，在第一个示例中，只需修改特定的比萨饼，而第二个示例要求更改花瓶的颜色和位置。现有方法常常难以满足如此具体的要求。2)
    *基于推理的指令*。第三个示例要求模型自主确定需要移除哪个人。由于 MLLM 代理的推理能力，我们的方法可以准确做出这个判断。相比之下，即使是同样使用 MLLM
    协助的 MGIE 也未能做出正确的修改。3) *要求过多的指令*。第四个示例要求对两只鸟进行不同的修改，而现有方法难以实现。4) *多步骤指令*。第五个示例涉及包括多个操作在内的复杂指令。MLLM
    代理可以将问题分解成多个单步骤操作，从而简化复杂任务。5) *多样化操作*。可以看出，由于整合了不同的工具，我们的方法在添加、删除和属性编辑等各种编辑操作中表现优异。这些比较强烈证明了我们在图像编辑方面的方法的有效性。'
- en: 'Visualized results about various tasks and user instructions. To demonstrate
    that our GenArtist can meet a wide range of user requirements, we provide visual
    examples in Fig. [8](#A2.F8 "Figure 8 ‣ Appendix B More Qualitative Experiments
    ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing").
    As can be seen, because of the integration of various tools, our framework can
    efficiently address these diverse requirements. For instance, it can generate
    images with a layout or pose similar to a given image, as well as customization-related
    generation. Through the use of multiple generation and editing tools, our method
    also achieves greater control, such as representing more objects and more complex
    relationships between objects in customization generation. These visualization
    examples strongly illustrate the necessity of employing an agent for image generation
    and demonstrate that our approach effectively accomplishes the goal of unified
    image generation and editing.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '关于各种任务和用户指令的可视化结果。为了展示我们的 GenArtist 如何满足广泛的用户需求，我们在图 [8](#A2.F8 "Figure 8 ‣
    Appendix B More Qualitative Experiments ‣ GenArtist: Multimodal LLM as an Agent
    for Unified Image Generation and Editing")中提供了可视化示例。可以看出，由于各种工具的集成，我们的框架能够高效地满足这些多样化的需求。例如，它可以生成与给定图像布局或姿势类似的图像，以及与定制相关的生成。通过使用多个生成和编辑工具，我们的方法还实现了更大的控制，例如在定制生成中表现出更多的对象及对象之间更复杂的关系。这些可视化示例强烈表明，采用图像生成代理的必要性，并展示了我们的方法有效地实现了统一的图像生成和编辑目标。'
- en: 'Visualization for the step-by-step process. Finally, we present our step-by-step
    visualized results in Fig. [9](#A2.F9 "Figure 9 ‣ Appendix B More Qualitative
    Experiments ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation
    and Editing") and Fig. [10](#A2.F10 "Figure 10 ‣ Appendix B More Qualitative Experiments
    ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing").
    For image generation, our method initially utilizes the most suitable tool to
    generate the initial image. If the image quality is too low or cannot be corrected
    after some modification operations, additional tools are invoked to continue generation.
    Further, for parts of the image that do not meet the text requirements, editing
    tools are continuously called to make modifications until the image correctly
    matches the text. For image editing, our method effectively decomposes the input
    problem and iteratively utilizes different tools to make step-by-step modifications
    until the image is correctly edited. This visualization clearly demonstrates the
    process, from decomposition and planning tree with step-by-step verification,
    to the final tool execution.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤的可视化过程。最后，我们在图 Fig. [9](#A2.F9 "Figure 9 ‣ Appendix B More Qualitative Experiments
    ‣ GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing")
    和图 Fig. [10](#A2.F10 "Figure 10 ‣ Appendix B More Qualitative Experiments ‣ GenArtist:
    Multimodal LLM as an Agent for Unified Image Generation and Editing") 中展示了我们的逐步可视化结果。对于图像生成，我们的方法最初使用最合适的工具生成初始图像。如果图像质量过低或经过一些修改操作后无法修正，则会调用其他工具继续生成。此外，对于不符合文本要求的图像部分，会持续调用编辑工具进行修改，直到图像正确匹配文本。对于图像编辑，我们的方法有效地将输入问题分解，并迭代使用不同的工具逐步进行修改，直到图像被正确编辑。这种可视化清晰地展示了从分解和规划树到最终工具执行的整个过程。'
- en: Limitation and Potential Negative Social Impacts. Our method employs an MLLM
    agent as the core for the entire system operations. Therefore, the method effectiveness
    depends on the performance of the MLLM used. Current MLLMs, such as GPT-4, are
    capable of meeting most basic requirements. For tasks that exceed the capability
    of GPT-4, our method may fail. Additionally, the misuse of image generation or
    editing could potentially lead to negative social impacts.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 限制与潜在的负面社会影响。我们的方法将 MLLM 代理作为整个系统操作的核心。因此，方法的有效性依赖于所使用的 MLLM 的性能。目前的 MLLM，例如
    GPT-4，能够满足大多数基本要求。对于超出 GPT-4 能力的任务，我们的方法可能会失败。此外，图像生成或编辑的误用可能会导致负面的社会影响。
