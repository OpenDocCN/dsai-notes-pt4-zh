- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:50:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM增强自主体能否合作？，通过Melting Pot评估其合作能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.11381](https://ar5iv.labs.arxiv.org/html/2403.11381)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.11381](https://ar5iv.labs.arxiv.org/html/2403.11381)
- en: '[1]\fnmManuel \surMosquera [1]\fnmJuan Sebastian \surPinzón [1]\fnmRubén \surManrique'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]\fnmManuel \surMosquera [1]\fnmJuan Sebastian \surPinzón [1]\fnmRubén \surManrique'
- en: 1]\orgdivDepartment of Engineering Systems and Computing, \orgnameLos Andes
    University, \orgaddress\cityBogotá, \postcode111711, \countryColombia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 1]\orgdiv工程系统与计算系，\orgname洛斯安第斯大学，\orgaddress\city波哥大，\postcode111711，\country哥伦比亚
- en: '[ma.mosquerao@uniandes.edu.co](mailto:ma.mosquerao@uniandes.edu.co)    [js.pinzonr@uniandes.edu.co](mailto:js.pinzonr@uniandes.edu.co)
       \fnmManuel \surRios [ms.rios10@uniandes.edu.co](mailto:ms.rios10@uniandes.edu.co)
       \fnmYesid \surFonseca [y.fonseca@uniandes.edu.co](mailto:y.fonseca@uniandes.edu.co)
       \fnmLuis Felipe \surGiraldo [lf.giraldo404@uniandes.edu.co](mailto:lf.giraldo404@uniandes.edu.co)
       \fnmNicanor \surQuijano [nquijano@uniandes.edu.co](mailto:nquijano@uniandes.edu.co)
       [rf.manrique@uniandes.edu.co](mailto:rf.manrique@uniandes.edu.co) ['
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[ma.mosquerao@uniandes.edu.co](mailto:ma.mosquerao@uniandes.edu.co)    [js.pinzonr@uniandes.edu.co](mailto:js.pinzonr@uniandes.edu.co)
       \fnmManuel \surRios [ms.rios10@uniandes.edu.co](mailto:ms.rios10@uniandes.edu.co)
       \fnmYesid \surFonseca [y.fonseca@uniandes.edu.co](mailto:y.fonseca@uniandes.edu.co)
       \fnmLuis Felipe \surGiraldo [lf.giraldo404@uniandes.edu.co](mailto:lf.giraldo404@uniandes.edu.co)
       \fnmNicanor \surQuijano [nquijano@uniandes.edu.co](mailto:nquijano@uniandes.edu.co)
       [rf.manrique@uniandes.edu.co](mailto:rf.manrique@uniandes.edu.co) ['
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As the field of AI continues to evolve, a significant dimension of this progression
    is the development of Large Language Models and their potential to enhance multi-agent
    artificial intelligence systems. This paper explores the cooperative capabilities
    of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known
    Meltin Pot environments along with reference models such as GPT4 and GPT3.5\.
    Preliminary results suggest that while these agents demonstrate a propensity for
    cooperation, they still struggle with effective collaboration in given environments,
    emphasizing the need for more robust architectures. The study’s contributions
    include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the
    implementation of a reusable architecture for LLM-mediated agent development –
    which includes short and long-term memories and different cognitive modules, and
    the evaluation of cooperation capabilities using a set of metrics tied to the
    Melting Pot’s ”Commons Harvest” game. The paper closes, by discussing the limitations
    of the current architectural framework and the potential of a new set of modules
    that fosters better cooperation among LAAs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能领域的不断发展，这一进展的重要维度是大型语言模型的开发及其在增强多智能体人工智能系统中的潜力。本文探讨了使用著名的Melting Pot环境及参考模型如GPT4和GPT3.5的**大型语言模型增强自主体（LAAs）**的协作能力。初步结果表明，虽然这些智能体展示了合作的倾向，但在特定环境中仍然难以有效协作，强调了需要更强健的架构。研究的贡献包括一个抽象层，用于将Melting
    Pot游戏场景适配到LLMs，实施一个可重用的架构用于LLM中介的智能体开发——包括短期和长期记忆以及不同的认知模块，以及通过一套与Melting Pot的“Commons
    Harvest”游戏相关的指标来评估合作能力。论文最后讨论了当前架构框架的局限性及新模块集合在促进LAAs更好合作方面的潜力。
- en: 'keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Agents, LLMs, Cooperative AI
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体，LLMs，合作AI
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The increased presence and relevance of AI agents within everyday spheres such
    as self-driving vehicles and customer service necessitates these entities being
    equipped with the appropriate capabilities to facilitate cooperation with humans
    and its AI counterparts. While noteworthy strides have been made in advancing
    individual intelligence components within AI agents, expanding the research focus
    to enhancing their social intelligence – the ability to effectively collaborate
    within group settings to solve prevalent problems – is now timely. This pivot
    aligns with the rapid progression of AI research presenting fresh prospects for
    fostering cooperation, drawing on insights from social choice theory and the development
    of social systems [[1](#bib.bib1)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: AI代理在自驾车和客户服务等日常领域中的出现和相关性增加，需要这些实体具备适当的能力以促进与人类及其AI对应体的合作。尽管在推动AI代理的个体智能组件方面取得了显著进展，但现在是时候将研究重点扩展到增强它们的社会智能——即在群体环境中有效合作以解决普遍问题的能力。这一转变符合AI研究的快速进展，为促进合作提供了新的前景，借鉴了社会选择理论和社会系统的发展[[1](#bib.bib1)]。
- en: Research into AI agents presents an avenue for generating intelligent technologies
    that embody more human-like features and are compatible with humans, a far cry
    from solipsistic approaches that overlook agent interactions. A promising illustration
    of this approach is the Melting Pot, an AI research tool designed to foster collaborative
    efforts within multi-agent artificial intelligence via canonical test scenarios.
    These environments emphasize non-trivial, learnable, and measurable cooperation
    by pairing a physical environment (a ”substrate”) with a reference set of co-players
    (a ”background population”) [[2](#bib.bib2)]. The environments fostered interdependence
    between the individuals involved.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对AI代理的研究提供了一条途径，可以生成更具人性化特征并与人类兼容的智能技术，这与忽视代理互动的唯我主义方法相去甚远。这个方法的一个有前景的例子是Melting
    Pot，这是一个AI研究工具，旨在通过规范的测试场景促进多代理人工智能中的协作。这些环境通过将物理环境（“基底”）与一组参考的共同玩家（“背景人群”）配对，强调非琐碎的、可学习的和可测量的合作[[2](#bib.bib2)]。这些环境促进了涉及个体之间的相互依赖。
- en: Research on AI agents has also recently been permeated by the leaps and bounds
    of Large Language Models (LLMs). The increasing success of LLMs encourages further
    exploration into LLM-augmented Autonomous Agents (LAAs). LAAs is an avenue of
    research that is still emerging, with limited explorations currently available
    [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)]. Something common in these works is that a clear need is established.
    To achieve success, LAAs have to rely on an architecture that can recall relevant
    events, reflect on such memories to generalize and draw a higher level of inferences,
    and utilize those reasonings to develop timely and long-term plans [[9](#bib.bib9)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，AI代理的研究也被大型语言模型（LLMs）的飞跃式发展所渗透。LLMs的成功鼓励了对LLM增强的自主代理（LAAs）的进一步探索。LAAs是一个仍在新兴的研究领域，目前可用的探索有限[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]。这些工作中的共同点是明确需要建立。要取得成功，LAAs必须依赖一种能够回忆相关事件、反思这些记忆以进行归纳并得出更高级别的推论的架构，并利用这些推理来制定及时和长期的计划[[9](#bib.bib9)]。
- en: These architectures offer specialized modules for specific tasks, utilizing
    meticulously crafted prompts and flows to perform complicated tasks and navigate
    intricate environments. Human behavior replication has been observed in some of
    these architectures, notably by Park et al. [[9](#bib.bib9)], who managed to create
    convincingly realistic human behavior in simulated environments. Similar frameworks
    utilized by Voyager [[10](#bib.bib10)] enabled an agent to navigate the Minecraft
    world and independently develop tools and skills. MetaGPT [[11](#bib.bib11)] introduced
    a framework allowing for the generation of fully functional programs, simulating
    a business-like environment with distinct roles and predefined agent interactions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构提供了针对特定任务的专用模块，利用精心设计的提示和流程来执行复杂任务和导航复杂环境。在一些这些架构中观察到了人类行为的复制，特别是Park等人[[9](#bib.bib9)]，他们在模拟环境中成功地创造了逼真的人类行为。类似的框架被Voyager[[10](#bib.bib10)]利用，使得一个代理能够在Minecraft世界中导航并独立开发工具和技能。MetaGPT[[11](#bib.bib11)]引入了一个框架，允许生成完全功能的程序，模拟一个具有明确角色和预定义代理互动的商业环境。
- en: Despite significant advancements in the field, the potential for cooperative
    abilities in (LAAs) has been somewhat neglected in current research. These capabilities
    could, however, be paramount in empowering these agents to perform innovative
    tasks and succeed in complex environments. This study represents an initial exploration
    into the inherent cooperative capabilities of LAAs. We employ an evaluation framework
    that includes a communication interface of scenarios from the Melting Pot project[[2](#bib.bib2)]
    (in which artificial agents co-exist in environments where social dilemmas can
    arise), the recent architecture proposed by Park et al.[[9](#bib.bib9)], as well
    as reference Large Language Models (LLMs) such as GPT4 and GPT3.5\. Our results
    hint towards the capability for cooperative behavior, based on simple natural
    language definitions and cooperation metrics tailored to the chosen Melting Pot
    scenario. While the agents showed a propensity to cooperate, their actions did
    not demonstrate a clear understanding of effective collaboration within the given
    environment. Consequently, our analysis underscores the necessity for more robust
    architectures that can foster better collaboration in LAAs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该领域取得了显著进展，但当前研究中对（LAAs）合作能力的潜力有所忽视。然而，这些能力对于赋予这些智能体执行创新任务并在复杂环境中取得成功可能至关重要。本研究代表了对LAAs固有合作能力的初步探索。我们使用了一个评估框架，其中包括来自“熔炉”项目的场景通信接口[[2](#bib.bib2)]（其中人工智能体在可能出现社会困境的环境中共存），Park等人提出的最新架构[[9](#bib.bib9)]，以及参考的大型语言模型（LLMs），如GPT4和GPT3.5。我们的结果暗示了基于简单自然语言定义和针对选择的熔炉场景定制的合作度指标的合作行为能力。尽管智能体表现出合作的倾向，但其行动并未显示出在给定环境中有效协作的明确理解。因此，我们的分析强调了需要更强大的架构，以促进LAAs中的更好合作。
- en: 'In summary, our contributions are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的贡献如下：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adapting the Melting Pot scenarios to textual representations that can be easily
    operationalized by LLMs.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将“熔炉”场景适配为可以被大语言模型（LLMs）轻松操作的文本表示形式。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Implementing a reusable architecture for the development of LAAs employing the
    modules proposed in [[9](#bib.bib9)]. This architecture includes short and long-term
    memories and cognitive modules of perception, planning, reflection, and action.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实施一个可重复使用的LAAs开发架构，采用[[9](#bib.bib9)]中提出的模块。该架构包括短期和长期记忆以及感知、规划、反思和行动的认知模块。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Implementing “personalities” specified in natural language, making it clear
    to the agents whether they should be cooperative or not. These descriptions are
    intended to discern, based on their pre-training knowledge, what they perceive
    as cooperation in an unfamiliar context.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实施自然语言中指定的“个性”，明确告知智能体是否应合作。这些描述旨在基于其预训练知识，辨别在陌生环境中它们所感知的合作行为。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Evaluating LLM-mediated agents in the “Commons Harvest” game of Melting Pot
    using our architecture in different scenarios where we specify or not, through
    natural language, the personality of the agents.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在“公共收获”游戏中评估LLM介导的智能体，使用我们在不同场景中指定或不指定通过自然语言描述智能体个性的架构。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discussing the results in terms of cooperativity metrics associated with the
    “ Commons Harvest” game, the limitations of the used architecture, and the proposal
    of an improved architecture that fosters better cooperation among LAAs.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 讨论与“公共收获”游戏相关的合作度指标、所用架构的局限性以及提出改进架构以促进LAAs之间更好合作的建议。
- en: 2 Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Agent architectures have evolved to address the limitations of traditional LLMs,
    equipping them with diverse tools for autonomous operation or minimal human oversight.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体架构已经发展到能够解决传统LLMs的局限性，为其配备了多样的工具，以实现自主操作或最小化人类监督。
- en: A notable challenge with LLMs is their susceptibility to hallucinations and
    gaps in knowledge regarding recent events or specific subjects. Such constraints
    diminish their practicality, as they remain confined to the information acquired
    during training without the capability to assimilate new data. To address this
    issue, Shick et al. [[7](#bib.bib7)] introduced an early solution named Toolformer.
    This model was trained to discern when and how to invoke APIs (tools) to enhance
    the LLM’s performance across various tasks. The dataset was self-supervised, with
    API calls incorporated only when they positively impacted the model’s performance.
    This methodology empowered the model to determine the relevance and optimal execution
    of API calls.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLMs）的一个显著挑战是它们容易产生虚假信息和对近期事件或特定主题的知识缺口。这些限制减少了它们的实用性，因为它们仍然局限于训练期间获得的信息，无法吸收新的数据。为了应对这个问题，Shick
    等人[[7](#bib.bib7)] 提出了一个早期解决方案，名为 Toolformer。该模型被训练用来识别何时以及如何调用 API（工具）以提高 LLM
    在各种任务中的表现。数据集是自我监督的，仅在 API 调用对模型表现产生积极影响时才会被纳入。这种方法使模型能够确定 API 调用的相关性和最佳执行方式。
- en: However, for executing more intricate tasks, merely invoking tools may be insufficient.
    Toolformer lacks the capability to reason about the rationale behind API calls
    and does not receive comprehensive environmental feedback to guide its subsequent
    actions toward achieving a goal. Recognizing this gap, the prompt-based paradigm
    ReAct [[8](#bib.bib8)], developed by Yao et al., integrates reasoning with action.
    By providing contextual prompt examples, ReAct guides the LLM on when to engage
    in reasoning and when to act, resulting in enhanced performance compared to approaches
    that employ reasoning or action in isolation. Furthermore, to enable the agent
    to learn from its errors, Shinn et al. [[6](#bib.bib6)] expanded the ReAct framework
    by incorporating a self-reflection module. This addition offers verbal feedback
    on past unsuccessful attempts, facilitating performance enhancement in subsequent
    trials.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于执行更复杂的任务，仅仅调用工具可能是不够的。Toolformer 缺乏对 API 调用背后原因的推理能力，并且没有获得全面的环境反馈来指导其后续行动以实现目标。意识到这一差距，由
    Yao 等人开发的基于提示的范式 ReAct [[8](#bib.bib8)] 将推理与行动结合起来。通过提供上下文提示示例，ReAct 指导 LLM 何时进行推理，何时采取行动，与单独使用推理或行动的方法相比，性能得到了提升。此外，为了使代理能够从错误中学习，Shinn
    等人[[6](#bib.bib6)] 通过引入自我反思模块扩展了 ReAct 框架。这个新增的模块提供了对过去失败尝试的口头反馈，从而促进了后续尝试的性能提升。
- en: Conversely, drawing inspiration from the emulation of authentic human behavior,
    Park et al. [[9](#bib.bib9)] devised an intricate agent framework. This architecture
    boasts a cognitive sequence structured around modules primarily anchored by diverse
    prompts. Leveraging distinct prompts optimizes LLM performance, enabling specialized
    techniques for specific tasks. Notably, memory holds a pivotal position within
    this framework, preserving the agent’s experiences and insights. The ability to
    retrieve these memories diversely amplifies their utility across multiple objectives.
    Moreover, Wang et al. developed the Voyager architecture [[10](#bib.bib10)], enabling
    autonomous gameplay in Minecraft. This advanced framework empowers the agent to
    autonomously curate a discovery agenda. Remarkably, the architecture can also
    create its own APIs, write corresponding code, verify API functionality, and store
    it in a vector database for future utilization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，Park 等人[[9](#bib.bib9)] 从模拟真实人类行为的灵感出发，设计了一个复杂的代理框架。这个架构拥有一个围绕模块构建的认知序列，这些模块主要由各种提示支持。利用不同的提示可以优化
    LLM 的表现，使其能够为特定任务采用专业技术。值得注意的是，记忆在这个框架中占据了关键位置，保存了代理的经验和见解。多样化地检索这些记忆提高了它们在多个目标中的效用。此外，Wang
    等人开发了 Voyager 架构[[10](#bib.bib10)]，实现了在 Minecraft 中的自主游戏。这一先进框架使代理能够自主策划发现计划。值得一提的是，该架构还可以创建自己的
    API，编写相应代码，验证 API 功能，并将其存储在向量数据库中以供未来使用。
- en: More recently, efforts have been directed towards enhancing agent performance
    through multi-agent frameworks that utilize different instances of LLMs to independently
    perform roles or tasks. Du et al. [[3](#bib.bib3)] demonstrated this through a
    framework designed to engage different LLM instances in a debate, aiming to improve
    the factuality and accuracy of the responses. Subsequently, Hong et al. further
    capitalized on the potential of multiple agents. They allocated specific roles
    to each agent, accompanied by a sequence of predefined tasks with clear input
    and output expectations. These tasks establish a structured interaction pathway
    between agents, enabling them to achieve user-defined objectives. Demonstrating
    its efficacy in software-related tasks, this framework, inspired by the operational
    dynamics of conventional software companies, attained state-of-the-art performance
    in the HumanEval and MBPP benchmarks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究者们致力于通过利用不同实例的 LLMs 来独立执行角色或任务的多代理框架来提升代理性能。杜等人 [[3](#bib.bib3)] 通过一个框架展示了这一点，该框架设计用于让不同的
    LLM 实例进行辩论，旨在提高响应的真实性和准确性。随后，洪等人进一步利用了多代理的潜力。他们为每个代理分配了特定角色，并附以一系列具有明确输入和输出期望的预定义任务。这些任务建立了代理之间的结构化互动路径，使它们能够实现用户定义的目标。该框架在软件相关任务中展示了其有效性，受传统软件公司操作动态的启发，在
    HumanEval 和 MBPP 基准测试中取得了最先进的性能。
- en: 'Similarly, Liu et al. [[4](#bib.bib4)] introduced the BOLAA framework. This
    system orchestrates multi-agent activity by defining specialized agents overseen
    by a central controller. The controller’s role is pivotal: it selects the most
    suitable agent for a given task and facilitates communication with it. Additionally,
    Zhang et al. [[5](#bib.bib5)] delved into multi-agent architectures, exploring
    the influence of social traits and collaborative strategies on different datasets.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，刘等人 [[4](#bib.bib4)] 提出了 BOLAA 框架。该系统通过定义由中央控制器监督的专门代理来协调多代理活动。控制器的角色至关重要：它为特定任务选择最合适的代理并促成与其的沟通。此外，张等人
    [[5](#bib.bib5)] 探讨了多代理架构，研究了社会特征和协作策略对不同数据集的影响。
- en: 3 Methodology
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Experimental setup
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 'Environment: This paper utilizes a scenario sourced from Melting Pot [[2](#bib.bib2)],
    a research tool developed by DeepMind for the purpose of experimentation and evaluation
    within the realm of multi-agent artificial intelligence. The scenarios within
    Melting Pot are specifically crafted to establish social situations in which the
    ability of the agents to solve conflict is challenged and characterized by significant
    interdependence among the involved agents.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 环境：本文利用了 Melting Pot [[2](#bib.bib2)] 中的一个场景，这是 DeepMind 为实验和评估多代理人工智能领域而开发的研究工具。Melting
    Pot 中的场景专门设计用于建立社会情境，以挑战代理解决冲突的能力，并且涉及的代理之间具有显著的相互依赖关系。
- en: '![Refer to caption](img/d0ee147f1c122f0dae368fb1fd390746.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d0ee147f1c122f0dae368fb1fd390746.png)'
- en: 'Figure 1: This is a screen capture of a running simulation of the Commons Harvest
    scenario. Bots can be identified by their arms and legs of color black.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：这是 Commons Harvest 场景的运行仿真屏幕截图。可以通过黑色的手臂和腿部识别出机器人。
- en: In the course of our experiments, we selected the “ Commons Harvest” scenario.
    In this scenario, agents with unsustainable practices can lead to situations where
    resources are depleted. This is known as the tragedy of the commons. This scenario
    is structured around a grid world featuring apples, each conferring a reward of
    1 to agents. The regrowth of apples is subject to a per-step probability determined
    by the apples’ distribution in an L2 norm with a radius of 2\. Notably, apples
    may become depleted if there are no other apples in close proximity. Fig. [1](#S3.F1
    "Figure 1 ‣ 3.1 Experimental setup ‣ 3 Methodology ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot") provides a visual representation of this custom-designed scenario, illustrating
    the presence of 3 LLM agents and 2 bots.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验过程中，我们选择了 “Commons Harvest” 场景。在这个场景中，采取不可持续的做法可能导致资源枯竭。这被称为公地悲剧。这个场景围绕一个网格世界构建，其中包含苹果，每个苹果给代理提供
    1 的奖励。苹果的再生概率由苹果在 L2 范数下的分布决定，半径为 2\. 值得注意的是，如果附近没有其他苹果，苹果可能会枯竭。图 [1](#S3.F1 "图
    1 ‣ 3.1 实验设置 ‣ 3 方法论 ‣ LLM 增强的自主代理能否合作？通过 Melting Pot 对其合作能力的评估") 提供了这个自定义场景的视觉表现，展示了
    3 个 LLM 代理和 2 个机器人。
- en: The LLM agents possess the capacity to execute high-level actions in each round.
    These actions include `immobilize player (player_name) at (x, y)`, `go to position
    (x, y)`, `stay put`, and `explore (x, y)`. They enable the agents to zap other
    players, navigate to predefined positions on the map, stay in the same position,
    and explore the world respectively. On the contrary, bots, characterized as agents
    trained through reinforcement learning, perform one movement for every two movements
    made by any of the LLM agents. The policies governing the bots lead them to engage
    in unsustainable harvesting practices and instigate attacks against other agents
    in close proximity.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理具有在每回合执行高级动作的能力。这些动作包括`immobilize player (player_name) at (x, y)`、`go to
    position (x, y)`、`stay put`和`explore (x, y)`。这些动作使代理能够分别电击其他玩家、导航到地图上的预定位置、保持当前位置和探索世界。相对而言，作为通过强化学习训练的代理的机器人，每两次LLM代理移动进行一次移动。控制机器人的策略使其从事不可持续的采摘行为，并对附近的其他代理进行攻击。
- en: In general, maximizing the welfare population for this scenario would require
    the LLM agents to restrain themselves from eating the last apple in each of the
    apple trees, and to attack the bots or agents that take the apples in an unsustainable
    way to avoid the depletion of the apples.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，要最大化这个场景下的福利人口，LLM代理需要自我约束，不吃每棵苹果树上的最后一个苹果，并且攻击以不可持续方式采摘苹果的机器人或代理，以避免苹果的枯竭。
- en: 'Simulation: In a simulation, each episode of the game involves the participation
    of a predetermined quantity of LLM agents and bots. The LLM agents take a high-level
    action on their turn and proceed to execute it until all three LLM agents have
    completed their respective high-level actions. Meanwhile, the bots are in constant
    motion, executing a move for every two moves made by any of the agents (note that
    a high-level action typically comprises more than one movement). The simulation
    concludes either upon reaching a maximum predetermined number of rounds (100 typically)
    or prematurely if all the apples in the environment are consumed.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 仿真：在仿真中，每一轮游戏涉及预定数量的LLM代理和机器人。LLM代理在其回合采取一个高级动作，并执行该动作，直到所有三个LLM代理完成各自的高级动作。与此同时，机器人在不断移动，每两次代理移动执行一次移动（注意，高级动作通常包括多个移动）。仿真在达到最大预定回合数（通常为100）时结束，或者如果环境中的所有苹果都被消耗，则提前结束。
- en: 3.2 Adapting the environment to LLM agents
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使环境适应LLM代理
- en: The Melting Pot scenarios consist of several two-dimensional layers accommodating
    various objects, each with its own custom logic. While initially, a matrix with
    distinct symbols seemed the most intuitive way to communicate the game state to
    the LLMs, it proved challenging for LLMs like GPT3.5 or GPT4 to interpret and
    reason about the spatial information provided by the position of objects in the
    matrix. To address this issue, we opted to develop an observation generator tailored
    to this particular environment. In this generator, every relevant object receives
    a natural language description, supplemented by coordinates expressed as a vector
    $[x,y]$, denoting row and column respectively. Moreover, some relevant state changes
    are captured while an agent waits for its turn, and these changes are also captured
    and communicated to the agents. The complete list of descriptions generated for
    the objects and events of this environment is shown in Appendix [A](#A1 "Appendix
    A Descriptions generated for the objects in the environment ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot").
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Melting Pot场景由几个二维层组成，每层容纳各种对象，每个对象都有自己的自定义逻辑。虽然最初使用具有不同符号的矩阵似乎是向LLM传达游戏状态的最直观方式，但对像GPT3.5或GPT4这样的LLM来说，解读和推理矩阵中对象位置提供的空间信息是具有挑战性的。为了解决这个问题，我们选择开发一个针对特定环境的观察生成器。在这个生成器中，每个相关对象都得到自然语言描述，并辅以作为向量
    $[x,y]$ 表示的坐标，分别表示行和列。此外，一些相关的状态变化在代理等待其回合时被捕捉，这些变化也被捕捉并传达给代理。环境中对象和事件的完整描述列表见附录
    [A](#A1 "Appendix A Descriptions generated for the objects in the environment
    ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot")。
- en: 4 LLM agent architecture
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LLM代理架构
- en: The design of the LLM agents predominantly drew upon the Generative Agents architecture
    [[9](#bib.bib9)]. This choice was motivated by its comprehensive nature, positioning
    it as one of the most versatile architectures for agents that could be readily
    tailored to various tasks. While the Voyager architecture [[10](#bib.bib10)] also
    presented a viable option, its efficacy was somewhat limited due to its inherent
    inflexibility. Voyager constructs agent actions dynamically during gameplay, involving
    the generation and validation of code to execute actions in the environment. In
    the context of our specific case, it was deemed preferable to externalize actions
    from the architecture to enhance simplicity.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理的设计主要借鉴了生成代理架构 [[9](#bib.bib9)]。这一选择的原因在于其全面性，使其成为一种最具多样性的代理架构之一，可以轻松调整以适应各种任务。虽然
    Voyager 架构 [[10](#bib.bib10)] 也提供了一个可行的选项，但由于其固有的灵活性限制，其效能有所下降。Voyager 在游戏过程中动态构建代理动作，包括生成和验证代码以在环境中执行动作。在我们特定的情况下，将动作从架构中外部化被认为是更好的选择，以提高简单性。
- en: Fig. [2](#S4.F2 "Figure 2 ‣ 4 LLM agent architecture ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot") illustrates the flow diagram outlining the process through which an agent
    initiates an action. Each action undertaken by LLM agents entails a comprehensive
    cognitive sequence designed to enhance the agent’s reasoning capabilities. This
    sequence involves the assimilation of feedback from past experiences and the translation
    of its objectives into a viable plan, enabling the execution of actions within
    the environment. This architectural framework is in a perpetual state of environmental
    sensing, generating observations that empower the agent to respond effectively
    to changes in the world.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S4.F2 "Figure 2 ‣ 4 LLM agent architecture ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot") 说明了一个代理启动动作的流程图。LLM 代理进行的每个动作都涉及一个全面的认知序列，旨在提升代理的推理能力。该序列包括对过去经验的反馈进行吸收，并将其目标转化为可行的计划，从而在环境中执行动作。这一架构框架处于持续的环境感知状态，生成观察，使代理能够有效应对世界的变化。
- en: '![Refer to caption](img/85c8e76cc93205899afa9be5134b3d22.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/85c8e76cc93205899afa9be5134b3d22.png)'
- en: 'Figure 2: The flow diagram for an action taken by an LLM agent.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLM 代理采取动作的流程图。
- en: 4.1 Memory structures
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 记忆结构
- en: 'This agent architecture employs three distinct memory structures designed for
    specific functions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该代理架构使用了三种不同的记忆结构，每种结构都用于特定功能：
- en: 'Long-Term Memory: This repository stores observations of the environment and
    the various thoughts generated by the agent in its cognitive modules. Leveraging
    the ChromaDB vector database, memories are stored, and the Ada OpenAI model generates
    contextual embeddings, enabling the agent to retrieve memories relevant to a given
    query.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 长期记忆：这个存储库保存了环境的观察以及代理在其认知模块中生成的各种思维。利用 ChromaDB 向量数据库，记忆被存储，Ada OpenAI 模型生成上下文嵌入，使代理能够检索与给定查询相关的记忆。
- en: 'Short-Term Memory: To facilitate rapid retrieval of specific memories or information,
    a Python dictionary is utilized. This dictionary stores information that must
    always be readily available to the agent, such as its name, as well as data that
    undergoes constant updates, such as current observations of the world.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 短期记忆：为了快速检索特定的记忆或信息，使用了一个 Python 字典。这个字典存储代理必须始终可以随时获取的信息，如其名称，以及不断更新的数据，如当前的世界观察。
- en: 'Spatial Memory: Given the agent’s navigation requirements in a grid world environment,
    spatial information becomes pivotal. This includes the agent’s position and orientation.
    To support effective navigation from one point to another, utility functions are
    implemented to aid the agent in spatial awareness and movement.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 空间记忆：鉴于代理在网格世界环境中的导航需求，空间信息变得至关重要。这包括代理的位置和方向。为了支持从一个点到另一个点的有效导航，实现了实用功能以帮助代理在空间意识和移动方面。
- en: 4.2 Cognitive modules
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 认知模块
- en: 'Perception module: The initial stage in the cognitive sequence is the Perception
    Module. This module is tasked with assimilating raw observations from the environment.
    These observations serve as a comprehensive snapshot of the current state of the
    world, offering insights into the items within the agent’s observable window.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 感知模块：认知序列的初始阶段是感知模块。这个模块负责吸收来自环境的原始观察数据。这些观察数据提供了世界当前状态的全面快照，提供了代理可观察窗口内物品的洞察。
- en: To optimize processing efficiency, the observations undergo an initial sorting
    based on their proximity to the agent. Subsequently, only the closest observations
    are channeled to the succeeding cognitive modules. The parameter governing the
    number of observations passed is denoted as `attention_bandwidth`, initially configured
    at a value of 10.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化处理效率，观察数据首先会根据其与代理的接近程度进行初步排序。随后，只有最接近的观察数据会被传递到后续的认知模块。控制传递观察数据数量的参数称为`attention_bandwidth`，初始配置为10。
- en: 'Following this, the module undertakes the responsibility of constructing a
    memory, destined for long-term storage. An illustrative memory example is outlined
    below:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，该模块负责构建一个用于长期存储的记忆。下面列出了一个示例记忆：
- en: I  took  the  action  ”grab  apple  (9,  20)”  in  my  last  turn.  Since  then,  the  followingchanges  in  the  environment  have  been  observed:Observed  that  agent  bot_1  took  an  apple  from  position  [8,  20].  At  2023-11-19  04:00:00Observed  that  agent  bot_1  took  an  apple  from  position  [8,  21].  At  2023-11-19  06:00:00Observed  that  an  apple  grew  at  position  [9,  20].  At  2023-11-19  06:00:00Observed  that  agent  Laura  took  an  apple  from  position  [2,  15].  At  2023-11-19  07:00:00Now  it’s  2023-11-19  09:00:00  and  the  reward  obtained  by  me  is  1.0.  I  amat  the  position  (10,  20)  looking  to  the  North.I  can  currently  observe  the  following:Observed  an  apple  at  position  [9,  20].  This  apple  belongs  to  tree  6.Observed  grass  to  grow  apples  at  position  [8,  20].  This  grass  belongs  to  tree  6.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一个回合执行了“抓取苹果 (9, 20)”的动作。从那时起，观察到环境中的以下变化：观察到代理bot_1从位置[8, 20]处拿走了一个苹果，时间为2023-11-19
    04:00:00；观察到代理bot_1从位置[8, 21]处拿走了一个苹果，时间为2023-11-19 06:00:00；观察到位置[9, 20]处的一个苹果长出来了，时间为2023-11-19
    06:00:00；观察到代理Laura从位置[2, 15]处拿走了一个苹果，时间为2023-11-19 07:00:00。现在是2023-11-19 09:00:00，我获得的奖励为1.0。我在位置(10,
    20)，面向北方。我现在可以观察到以下情况：观察到位置[9, 20]处有一个苹果。这个苹果属于树6。观察到位置[8, 20]处的草生长出了苹果。这个草属于树6。
- en: Ultimately, the Perceive module determines whether an agent should initiate
    a response based on the current observations. During this stage, the agent assesses
    its existing plan and queued actions to ascertain their suitability. It evaluates
    whether it is appropriate to proceed with the current course of action or if the
    observed conditions warrant the development of a new plan and the generation of
    corresponding actions for execution. The complete prompt is shown in Appendix
    [C](#A3 "Appendix C React prompt ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot").
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*最终*，Perceive模块会根据当前观察数据判断代理是否应该启动响应。在此阶段，代理会评估其现有的计划和排队的行动，以确定它们的适用性。它评估是否继续当前的行动方案，还是观察到的条件需要制定新计划并生成相应的行动以执行。完整的提示见附录[C](#A3
    "Appendix C React prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An
    evaluation of their cooperative capabilities through Melting Pot")。'
- en: 'Planning module: This module comes into play once observations have been sorted
    and filtered. The Planning modules leverage the amalgamation of current observations,
    the existing plan, the contextual understanding of the world, reflections from
    the past, and rationale to meticulously craft a newly devised plan. This plan
    intricately outlines the high-level behavior expected from the agent and delineates
    the goals the agent will diligently pursue. For the complete prompt refer to Appendix
    [D](#A4 "Appendix D Plan prompt ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 规划模块：在观察数据被排序和过滤后，规划模块开始发挥作用。规划模块利用当前观察数据、现有计划、世界的上下文理解、过去的反思和推理，精心制定一个新的计划。这个计划详细描述了代理预期的高层行为，并列出了代理将认真追求的目标。完整的提示见附录[D](#A4
    "Appendix D Plan prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot")。
- en: 'Reflection module: The Reflect module is designed to facilitate profound contemplation
    on observations and fellow agent thoughts at a higher cognitive level. Activation
    of this module is contingent upon reaching a predetermined threshold of accumulated
    observations. In our experimental setup, reflections were initiated every 30 perceived
    observations, roughly translating to three rounds in the game. The Reflect model
    comprises two key stages:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 反思模块：反思模块旨在在更高认知水平上促进对观察和其他代理思维的深入思考。激活此模块依赖于达到预设的累积观察阈值。在我们的实验设置中，每30次感知观察就会启动一次反思，这大致相当于游戏中的三轮。反思模型包括两个关键阶段：
- en: '1.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Question Formulation: In the first stage, the module utilizes the 30 retained
    observations to formulate the three most salient questions regarding these observations.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题制定：在第一阶段，模块利用保留的30个观察来制定关于这些观察的三个最显著问题。
- en: '2.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Insight Generation: The second stage involves utilizing these questions to
    retrieve pertinent memories from long-term memory. Subsequently, the questions
    and retrieved memories are employed to generate three insights, which are then
    stored as reflections in the long-term memory.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 洞察生成：第二阶段涉及利用这些问题从长期记忆中检索相关记忆。随后，这些问题和检索到的记忆被用来生成三个洞察，然后将其作为反思存储在长期记忆中。
- en: The retrieval of relevant memories employs a weighted average encompassing cosine
    similarity, recency score, and poignancy scores. The recency score is computed
    as $e^{h}$ denotes the number of hours since the last memory was recorded. Meanwhile,
    the poignancy score reflects the intensity assigned to the memory at its point
    of creation. Throughout the experiments, a uniform poignancy score of 10 was assigned
    to all memory types. For the complete prompt and more details on this module,
    refer to Appendix [E](#A5 "Appendix E Reflection prompts ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot").
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 相关记忆的检索使用了加权平均，包括余弦相似度、近期得分和情感得分。近期得分计算为$e^{h}$，表示自上次记忆记录以来的小时数。与此同时，情感得分反映了记忆创建时分配的强度。在实验过程中，所有记忆类型都被赋予了统一的情感得分10。有关完整的提示和更多关于此模块的细节，请参见附录[E](#A5
    "附录 E 反思提示 ‣ LLM增强型自主代理能否合作？通过Melting Pot对其合作能力进行评估")。
- en: 'Action Module: This module plays the role of generating an action for the agent
    to undertake. As detailed in Appendix [F](#A6 "Appendix F Act prompt ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot"), the selection of the action is determined by the Language
    Model (LLM), which considers the agent’s comprehension of the world, its current
    goals and plans, reflections, ongoing observations, and the available valid actions
    within the environment. The creation of new action sequences occurs under two
    conditions: when the current sequence is empty or when the agent is responding
    to observations. For this prompt, we manually crafted a reasoning structure, similar
    to those described in Self-Discover [[12](#bib.bib12)] to help the LLM consider
    different alternatives, and evaluate them before making the final decision.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 行动模块：该模块负责生成代理需要执行的行动。如附录[F](#A6 "附录 F 行动提示 ‣ LLM增强型自主代理能否合作？通过Melting Pot对其合作能力进行评估")中详细说明，行动的选择由语言模型（LLM）决定，它考虑了代理对世界的理解、当前目标和计划、反思、正在进行的观察以及环境中的有效行动。新行动序列的创建发生在两种情况下：当前序列为空或代理响应观察时。对于此提示，我们手动构建了一个推理结构，类似于Self-Discover
    [[12](#bib.bib12)]中描述的结构，以帮助LLM考虑不同的替代方案，并在做出最终决定之前对其进行评估。
- en: 5 Evaluation scenarios
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个评估场景
- en: To assess the outcomes, we utilized the per capita average reward of the focal
    population as our primary metric. The focal population comprises LLM agents, and
    the chosen metric aligns with the Melting Pot framework’s approach [[2](#bib.bib2)],
    which evaluates population welfare. We compare this metric across two sets of
    scenarios.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估结果，我们使用了焦点人群的每人平均奖励作为主要指标。焦点人群由LLM代理组成，所选指标与Melting Pot框架的[[2](#bib.bib2)]方法一致，该方法评估人群福利。我们将该指标在两组场景之间进行比较。
- en: 'The first set of scenarios has the intention of measuring how the personality
    given to the agents affects the agents’ welfare. For this purpose, we prepared
    five scenarios: (1) as a baseline we do not give the agents any personality specifications
    (Without personality), (2) we tell the agents to be cooperative (All coop.), (3)
    we tell the agents to be cooperative and provide a short description of how to
    be cooperative in the chosen scenario (All coop. with def.), (4) we tell the agents
    to be selfish (All selfish), (5) we tell the agents to be selfish and provide
    a definition with the expected behavior of someone selfish for the given scenario
    (All selfish with def.).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组场景的目的是衡量赋予代理的个性如何影响代理的福利。为此，我们准备了五个场景：（1）作为基准，我们不给代理任何个性说明（没有个性），（2）我们告诉代理要合作（全部合作），（3）我们告诉代理要合作，并提供如何在选择的场景中进行合作的简短描述（全部合作，附带说明），（4）我们告诉代理要自私（全部自私），（5）我们告诉代理要自私，并提供一个关于在给定场景中自私行为的定义（全部自私，附带说明）。
- en: 'The second set of scenarios is more challenging as the competition increases
    by reducing the number of trees and apples, modifying the agents’ initial social
    environment understanding, or by adding other entities to the environment (bots),
    these changes demand a deeper understanding from the agents and swift reactions
    to master the scenarios. More concretely, the • first three scenarios consist
    of an environment where there are three agents and only one apple tree. Each scenario
    differs in the personality given to the agents: (1) All coop, (2) All selfish,
    and (3) without personality. The last scenario of the second set (4) has the same
    base configuration, but there are two agents and two bots, where the bots are
    reinforcement learning agents that were trained to harvest in an unsustainable
    way and to attack other agents. These bots make part of scenario 0 of the commons
    harvest open scenario described in Meltingpot 2.0 [[2](#bib.bib2)].'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组场景更具挑战性，因为竞争增加了，通过减少树木和苹果的数量，修改代理的初始社会环境理解，或者通过向环境中添加其他实体（机器人）来实现，这些变化要求代理有更深入的理解和迅速的反应来掌握这些场景。更具体地说，•前三个场景由三个代理和一个苹果树组成。每个场景在赋予代理的个性方面有所不同：（1）全部合作，（2）全部自私，和（3）没有个性。第二组的最后一个场景（4）具有相同的基本配置，但有两个代理和两个机器人，其中机器人是经过训练以不可持续方式收获和攻击其他代理的强化学习代理。这些机器人是
    Meltingpot 2.0 [[2](#bib.bib2)] 中描述的公共收获开放场景的场景 0 的一部分。
- en: We also add a scenario aimed at demonstrating how the information an agent has
    about the rest of the agents can influence their behavior. In this scenario, the
    environment begins with the same number of trees; however, from the start of the
    simulation, each agent was informed that among them, one was acting entirely selfishly,
    representing a risk due to their unsustainable consumption.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还增加了一个场景，旨在展示一个代理对其他代理的了解如何影响其行为。在这个场景中，环境开始时有相同数量的树木；然而，从模拟开始时，每个代理都被告知在他们中间，有一个代理完全自私，代表了由于其不可持续的消费而存在的风险。
- en: For all the experiments, the agents receive information about the environmental
    rules. They are aware that the per-step growth probability of apples is influenced
    by nearby apples and that green patches can be depleted if all apples within them
    are consumed. However, the agents lack information about what is the optimal policy
    for each scenario, and are unfamiliar with bots and other situations in the game.
    The complete world context that is given to the agents is shown in Appendix [B](#A2
    "Appendix B Knowledge about the world given to agents ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot").
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有实验中，代理都接收到关于环境规则的信息。他们知道每一步苹果的生长概率受到附近苹果的影响，并且如果所有苹果被消费，绿色斑块可能会被耗尽。然而，代理缺乏关于每个场景的最佳策略的信息，并且对游戏中的机器人和其他情况不熟悉。提供给代理的完整世界背景显示在附录
    [B](#A2 "Appendix B Knowledge about the world given to agents ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot")。
- en: Ten simulations for each scenario were done in which the LLM agents were powered
    by the GPT3.5 from the OpenAI API for the majority of modules, and GPT4 powered
    the act module. On another hand, the Ada model was used to create contextual embeddings
    of the memories. A detail of the simulations cost is available in the Appendix
    [G](#A7 "Appendix G Simulations cost ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot").
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个情境下进行了十次模拟，其中LLM代理的大多数模块由OpenAI API的GPT3.5提供支持，而行为模块由GPT4提供支持。另一方面，Ada模型用于创建记忆的上下文嵌入。模拟成本的详细信息见附录
    [G](#A7 "附录 G 模拟成本 ‣ LLM增强的自主代理能否合作？通过Melting Pot评估其合作能力")。
- en: 6 Results
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结果
- en: 6.1 Impact of personality in population welfare
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 个性对人口福利的影响
- en: The average per capita reward obtained for the first set of scenarios is shown
    in Fig. [3](#S6.F3 "Figure 3 ‣ 6.1 Impact of personality in population welfare
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot"). The best-performing simulations
    were the ones where no particular personality description was given to the agents,
    followed by the scenarios where the agents were instructed to be selfish. Surprisingly,
    the scenarios where the agents were told to be cooperative had the worst performance.
    On further analysis, we found that these results are explained mainly by the number
    of times the agents decided to attack other agents (see Fig. [4](#S6.F4 "Figure
    4 ‣ 6.1 Impact of personality in population welfare ‣ 6 Results ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot")).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组情境下获得的每人平均奖励见图 [3](#S6.F3 "图 3 ‣ 6.1 个性对人口福利的影响 ‣ 6 结果 ‣ LLM增强的自主代理能否合作？通过Melting
    Pot评估其合作能力")。表现最佳的模拟是没有给代理特定个性的情境，其次是指示代理自私的情境。令人惊讶的是，指示代理合作的情境表现最差。进一步分析发现，这些结果主要通过代理攻击其他代理的次数来解释（见图
    [4](#S6.F4 "图 4 ‣ 6.1 个性对人口福利的影响 ‣ 6 结果 ‣ LLM增强的自主代理能否合作？通过Melting Pot评估其合作能力")）。
- en: '![Refer to caption](img/c32efb494f5fbb6f99eb4dcdde216580.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c32efb494f5fbb6f99eb4dcdde216580.png)'
- en: 'Figure 3: The per capita average reward of the agents by scenario. Ten simulations
    were performed per scenario to find how the agents’ given personality could affect
    population welfare. We found that the scenario where no particular personality
    was assigned had the best per capita reward, followed by the scenarios where agents
    were told to be selfish, and lastly, the worst performance was seen in the scenarios
    where the agents were instructed to be cooperative.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同情境下代理的每人平均奖励。每个情境下进行了十次模拟，以了解代理的特定个性如何影响整体福利。我们发现，未分配特定个性的情境下的每人奖励最好，其次是指示代理自私的情境，最差的是指示代理合作的情境。
- en: To gain a better understanding of the agents’ behavior, we recorded the number
    of times the agents decided to attack other agents, and the number of times these
    attacks were effective. These actions play an important part in the game because
    they are the only mechanism the game provides to directly interact with other
    agents. Therefore, helping the agents counteract other agents’ behavior, such
    as attacking other agents that are taking apples indiscriminately to avoid the
    depletion of the apple trees or decreasing the competition when there are too
    many agents near the same tree. More concretely, when an agent attacks and the
    ray beam hits its target (another agent), the agent that was hit is taken out
    of the game for the next five steps and then it is revived in a random position
    of the spawning area of the map.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解代理的行为，我们记录了代理决定攻击其他代理的次数，以及这些攻击的有效次数。这些行动在游戏中起着重要作用，因为它们是游戏提供的唯一直接与其他代理互动的机制。因此，帮助代理应对其他代理的行为，比如攻击那些无差别采摘苹果的代理，以避免苹果树的枯竭，或减少当有太多代理靠近同一棵树时的竞争。更具体地说，当一个代理攻击并且光束击中目标（另一个代理）时，被击中的代理会在接下来的五步中被淘汰，然后在地图的重生区域的随机位置复活。
- en: In Fig. [4](#S6.F4 "Figure 4 ‣ 6.1 Impact of personality in population welfare
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") we show the results of these
    attack indicators for the first set of experiments. The results depict some important
    differences across the scenarios, mainly reflecting the reluctance of the cooperative
    agents to attack, and a strange difference between the number of attacks of the
    selfish agents with definition and the selfish agents without definition.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [4](#S6.F4 "图 4 ‣ 6.1 个性对人口福利的影响 ‣ 6 结果 ‣ LLM 增强的自主代理能否合作？通过 Melting Pot
    对其合作能力的评估") 中，我们展示了这些攻击指标在第一组实验中的结果。结果显示了场景之间的一些重要差异，主要反映了合作代理在攻击上的犹豫，以及有定义的自私代理和没有定义的自私代理之间攻击次数的奇怪差异。
- en: LLMs appear to equate cooperation with refraining from attacking, even when
    attacking may be the only viable strategy to address uncooperative agents. This
    behavior was the main cause for cooperative instructed agents to achieve the worst
    average per capita reward. On the other hand, the selfishly instructed agents
    behave similarly to the agents lacking assigned personalities, suggesting that
    LLMs partially disregard the personality given and tend to cooperate by harvesting
    apples sustainably. The notable disparity in attack frequencies between selfish
    agents with and without definition is intriguing because agents with the selfish
    definition decided to explore more frequently rather than attack, the reason for
    that remains a mistery.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 似乎将合作等同于避免攻击，即使攻击可能是解决不合作代理的唯一可行策略。这种行为是合作指导的代理获得最差人均奖励的主要原因。另一方面，自私指导的代理表现得类似于缺乏分配个性的代理，表明
    LLM 部分忽视了给予的个性，倾向于通过可持续的方式采摘苹果。自私代理有无定义之间攻击频率的显著差异很有趣，因为有定义的自私代理决定更频繁地探索而不是攻击，这个原因仍然是个谜。
- en: '![Refer to caption](img/0d789779324b662de92d00f9d55b4140.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0d789779324b662de92d00f9d55b4140.png)'
- en: 'Figure 4: The number of times the agents decided to attack and the number of
    times the attacks were effective, i.e. the number of times the attack hit the
    other agent, thus taking the agent out of the game for the next five moves. The
    scenarios All selfish and Without personality registered a higher number of attacks,
    while the scenarios All coop. and All coop. with def. showed the least number
    of attacks.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：代理决定攻击的次数和攻击有效的次数，即攻击击中另一代理的次数，从而使该代理在接下来的五个回合中无法参与游戏。场景中的所有自私和没有个性的场景记录了更多的攻击次数，而所有合作和有定义的合作场景显示了最少的攻击次数。
- en: Another important behavior to keep track of is the decisions the agents made
    when they were near the last apple of a tree. Whether they choose to take it or
    ignore it is a crucial event and highly impactful in the final per capita reward,
    as there are only 6 apple trees in the game, and taking the last apple of a tree
    would mean that the tree would be depleted and will not produce more apples. For
    this reason, we created an indicator that counts how many times the agents closed
    the distance between the last apple of a tree and its position, divided by how
    many times the nearest apple to the agent was the last apple of a tree. However,
    this indicator does not consider that sometimes the last apple, despite being
    the closest to the agent, is not visible to the agent because it is outside the
    observation window of the agent. This limitation could have had an impact on the
    observed results.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要跟踪的重要行为是代理在接近树上的最后一个苹果时做出的决定。选择是否取走这个苹果或忽略它是一个关键事件，对最终的人均奖励有很大影响，因为游戏中只有
    6 棵苹果树，取走树上的最后一个苹果意味着这棵树将枯竭，不会再生产苹果。因此，我们创建了一个指标来计算代理缩短到树上最后一个苹果之间距离的次数，除以最近的苹果是树上最后一个苹果的次数。然而，这个指标没有考虑到有时最后一个苹果尽管是最接近代理的，但由于它在代理的观察窗口之外，因此对代理不可见。这一限制可能对观察到的结果产生了影响。
- en: In Fig. [5](#S6.F5 "Figure 5 ‣ 6.1 Impact of personality in population welfare
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") we can see that the proportion
    of times the agents moved towards the last apple is pretty similar across all
    the scenarios, indicating that the personality descriptions did not cause a major
    effect on the awareness of the agents regarding the welfare detriment caused by
    the depletion of apple trees. These results highlight a limited understanding
    among the agents regarding the consequences of their actions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](#S6.F5 "图 5 ‣ 6.1 个性对群体福利的影响 ‣ 6 结果 ‣ LLM增强型自主代理是否能合作？通过熔炉评估其合作能力")中，我们可以看到代理朝向最后一个苹果移动的比例在所有场景中相似，这表明个性描述对代理对苹果树枯竭所带来的福利损害的意识没有造成重大影响。这些结果突显了代理对其行为后果的理解有限。
- en: '![Refer to caption](img/20e0e8f58375fd8d1364fa23b898bac6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/20e0e8f58375fd8d1364fa23b898bac6.png)'
- en: 'Figure 5: Indicator of the number of times the agent closed the distance towards
    the last apple of a tree divided by the times the last apple of a tree was the
    nearest to the agent. The results show that there are no important differences
    between the first set of scenarios.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：指标为代理接近树上最后一个苹果的次数与树上最后一个苹果距离代理最近的次数的比值。结果表明，第一组场景之间没有显著差异。
- en: 6.2 Performance of the agents in more challenging scenarios
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 代理在更具挑战性场景中的表现
- en: The second set of experiments consists of scenarios where the competence increases
    or the resources become scarcer. The purpose of these scenarios is to measure
    how the agents respond to the new game conditions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组实验包括了能力增加或资源变得更稀缺的场景。这些场景的目的是测量代理如何应对新的游戏条件。
- en: One single tree scenarios. The first three scenarios in this set represent an
    environment involving a more intensive competition for resources. The three agents,
    who usually have a limited field of vision, are in constant observation of a single
    tree in the environment, which is situated in a confined space. The difference
    between each scenario lies in the type of personality assigned to each agent,
    with the personalities in this case being All cooperative, All selfish, and Without
    Personality. For practical purposes, no specific definition was given to any personality.
    The purpose of the scenario is to demonstrate the collective sustainability capacity
    that different types of agents can have where resources are highly limited.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 单棵树场景。此组中的前三个场景涉及资源竞争更加激烈的环境。三名代理通常视野有限，持续观察一个位于狭小空间中的单棵树。每个场景的区别在于分配给每个代理的个性类型，分别为全体合作、全体自私和无个性。出于实际考虑，没有对任何个性进行具体定义。此场景的目的是展示在资源极为有限的情况下，不同类型代理可能具有的集体可持续性能力。
- en: In Fig. [6](#S6.F6 "Figure 6 ‣ 6.2 Performance of the agents in more challenging
    scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot"), the results for the ”Per
    capita reward” contrasted with the ”Average amount of available apples” for the
    described group of scenarios are observed. Upon close examination, it is noted
    that the slope of the reward curve for cooperative agents is less than that of
    the ”Selfish” and ”Without personality” agents, this behavior contributes to this
    set of agents having resource availability for a slightly longer period, as shown
    in the figure. However, given the dynamics of the probability of apple reappearance,
    this behavior was not significant enough to allow cooperative agents to have a
    considerably superior reward per capita. Therefore, it is concluded that any set
    of agents was able to demonstrate sufficiently good sustainable behavior due to
    their lack of understanding of the world and their lack of communication and coordination
    capabilities with other agents.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [6](#S6.F6 "Figure 6 ‣ 6.2 Performance of the agents in more challenging
    scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") 中，观察了“人均奖励”与“可用苹果的平均数量”在所描述情境组中的对比。经过仔细审查，可以注意到，合作代理的奖励曲线斜率低于“自私”代理和“无个性”代理，这种行为使得这组代理在资源可用性方面略长一些，如图所示。然而，鉴于苹果重新出现的概率动态，这种行为并不显著到能使合作代理获得显著优于其他代理的人均奖励。因此，可以得出结论，任何一组代理由于缺乏对世界的理解以及与其他代理的沟通和协调能力，未能表现出足够良好的可持续行为。
- en: '![Refer to caption](img/10bfeeb61a699328f15c3af39b213560.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10bfeeb61a699328f15c3af39b213560.png)'
- en: 'Figure 6: Average reward per capita versus average apple availability across
    personality scenarios when there is only a single tree: The results show a slight
    superiority in terms of sustainability by cooperative agents, the number of rounds
    they managed to keep the tree alive was slightly higher than the rest of the agents.
    However, this behavior was not significant enough to obtain a better Reward per
    capita than other agents.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在只有一棵树的个性化情境下，人均奖励与平均苹果可用性对比：结果显示，合作代理在可持续性方面略有优势，它们维持树木存活的回合数略高于其他代理。然而，这种行为并不显著到能获得比其他代理更高的人均奖励。
- en: Agents versus Bots. The fifth scenario of the second set of experiments exposes
    two agents to the presence of two reinforcement learning bots. The policy of the
    bots makes them take the apples without regard for the replenishment rate or the
    risk of depleting the trees, they focus solely on maximizing their rewards by
    taking the apples, but they also attack other agents, mainly where there are no
    other apples in proximity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 代理与机器人。在第二组实验的第五种情境中，两个代理被暴露于两个强化学习机器人的存在。机器人的策略使它们在拿取苹果时不考虑补给率或树木枯竭的风险，它们完全专注于通过拿取苹果来最大化奖励，但它们也会攻击其他代理，主要是在附近没有其他苹果的地方。
- en: In Fig. [7](#S6.F7 "Figure 7 ‣ 6.2 Performance of the agents in more challenging
    scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") we get the results of
    the average reward per capita for the agents versus bots scenario. The initial
    notable observation is that the bots can consistently achieve higher rewards than
    the agents. This phenomenon is mainly explained by the policy of the bots that
    prioritize taking all the visible apples over other actions, while the agents
    explore the map or go to other positions of the map with higher frequency than
    the bots. However, it is important to note how the per capita reward for the bots
    stops increasing earlier than that for the agents, indicating greater difficulty
    for the bots to increase their rewards when trees are scarce in comparison to
    the agents. Moreover, we found that in half of the simulations, at least one of
    the agents achieved a better reward than that of a bot, leading us to conclude
    that sometimes the agents are capable of performing better than the greedy policy
    of the agents. By closer examination, we observed that in those simulations, the
    agents were able to find apple trees more easily than the bots and that they also
    tended to attack when another agent or bot was taking apples from the same tree
    as them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[7](#S6.F7 "Figure 7 ‣ 6.2 Performance of the agents in more challenging scenarios
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot")中，我们得到了代理与机器人场景的平均奖励结果。初步显著的观察是，机器人可以
    consistently 比代理获得更高的奖励。这一现象主要是由于机器人的政策优先考虑采摘所有可见的苹果而非其他行为，而代理则比机器人更频繁地探索地图或前往地图上的其他位置。然而，值得注意的是，机器人的人均奖励比代理的奖励更早停止增长，这表明在树木稀少时，机器人提高奖励的难度大于代理。此外，我们发现，在一半的模拟中，至少有一个代理的奖励比机器人的奖励更高，这让我们得出结论，有时代理的表现优于贪婪政策的机器人。通过进一步观察，我们发现，在这些模拟中，代理比机器人更容易找到苹果树，并且当另一个代理或机器人从同一棵树上采摘苹果时，它们也倾向于进行攻击。
- en: '![Refer to caption](img/69096b7f15b85bda81e3243c0fa5f3a6.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/69096b7f15b85bda81e3243c0fa5f3a6.png)'
- en: 'Figure 7: Average reward per capita by sub-population (agents and bots). In
    the results, there is a clear gap between the agents and the bots, where the bots
    can take advantage of the agents by solely focusing on taking apples without worrying
    about depleting the trees.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：按子群体（代理和机器人）计算的平均奖励。在结果中，代理和机器人的差距明显，机器人可以通过仅专注于采摘苹果而不必担心树木的枯竭，来利用代理的弱点。
- en: In Fig. [8](#S6.F8 "Figure 8 ‣ 6.2 Performance of the agents in more challenging
    scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot"), a significant disparity
    between the number of attacks perpetrated by bots and agents is observed. Despite
    bots’ attacks occurring almost five times as frequently as those executed by agents,
    the latter proved to be twice as effective in their attacks. Upon manual review
    of the simulations, we identified that bots increased their frequency of attacks
    when they were unable to perceive apples within their observation window, even
    when the attacks were not directed towards any specific target. This finding led
    us to appreciate how the actions taken by the agents are comparatively more coherent
    than those of the bots. Furthermore, the behavior of the agents exhibited closer
    resemblance to human behavior, not only in terms of attacks but also in their
    movement patterns, in contrast to the seemingly random and redundant actions of
    the bots.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[8](#S6.F8 "Figure 8 ‣ 6.2 Performance of the agents in more challenging scenarios
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot")中，观察到机器人和代理发动攻击的数量之间存在显著差异。尽管机器人的攻击发生的频率几乎是代理的五倍，但后者在攻击中的效果却是前者的两倍。在对模拟结果进行人工审查后，我们发现机器人在无法在观察窗口内发现苹果时，会增加攻击频率，即使攻击并不针对任何特定目标。这一发现让我们意识到，代理采取的行动在比较中比机器人的行动更为一致。此外，代理的行为在攻击以及移动模式上更接近于人类行为，而不像机器人那样显得随机和冗余。
- en: '![Refer to caption](img/eb3afc91b253cffc7a00847fb7137167.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb3afc91b253cffc7a00847fb7137167.png)'
- en: 'Figure 8: The number of times the agents decided to attack and the number of
    times the attacks were effective. Bots attacked almost five times as frequently
    as agents. However, the agents’ effectiveness was more than double that of the
    bots.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：代理人决定攻击的次数和攻击有效的次数。机器人攻击的频率几乎是代理人的五倍。然而，代理人的有效性是机器人的两倍多。
- en: Moreover, Fig. [9](#S6.F9 "Figure 9 ‣ 6.2 Performance of the agents in more
    challenging scenarios ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot") shows that
    the agents depleted trees with higher frequency than the agents. Thus the agents
    demonstrated the capacity to sometimes restrain themselves from just taking apples
    by trying to maximize their long-term rewards, whereas bots always prioritized
    their short-term reward.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图 [9](#S6.F9 "图 9 ‣ 6.2 代理人在更具挑战性的情境中的表现 ‣ 6 结果 ‣ LLM增强的自主代理人能否合作？通过熔炉对其合作能力的评估")
    显示，代理人比机器人更频繁地消耗树木。因此，代理人展现了有时抑制自己只取苹果的能力，通过尝试最大化长期奖励，而机器人总是优先考虑短期奖励。
- en: '![Refer to caption](img/13aaa03e16e1ba849619d2bf7775974d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/13aaa03e16e1ba849619d2bf7775974d.png)'
- en: 'Figure 9: Average number of times the agents and bots took the last apple of
    a tree by sub-population (agents and bots). In the results, we observed that the
    agents depleted trees less frequently than the bots did, showcasing that the bots
    were more responsible for the depletion of resources and had a higher negative
    impact in the population welfare.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：代理人和机器人按子群体（代理人和机器人）最后一次取苹果的平均次数。在结果中，我们观察到代理人消耗树木的频率低于机器人，这表明机器人对资源的消耗负有更大的责任，对群体福利的负面影响更大。
- en: 6.3 Impact of knowledge of other agent’s behavior
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 其他代理人行为知识的影响
- en: This experiment considers the hypothetical scenario in which all agents are
    previously informed that one specific agent is entirely selfish and the implications
    that its uncooperative behavior can have. Likewise, this agent is informed to
    act selfishly (providing the previously described definition of selfishness).
    The objective of this scenario is to highlight the behavior that agents can exhibit
    when possessing valuable information about their social environment.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验考虑了一个假设情境，其中所有代理人都事先知道一个特定的代理人完全自私，以及其不合作行为可能带来的影响。同样，该代理人被告知要自私地行动（提供了之前描述的自私定义）。这个情境的目的是突出当代理人拥有关于其社会环境的有价值信息时可能表现出的行为。
- en: Fig. [10](#S6.F10 "Figure 10 ‣ 6.3 Impact of knowledge of other agent’s behavior
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") shows that, on average, agents
    without personality targeted Pedro, the selfish agent, exclusively in 86% of the
    attacks. This illustrates how the two agents without a defined personality utilized
    the information forcibly implanted in them to benefit the overall sustainability
    of the environment, as they repeatedly immobilized the agent who posed a risk
    due to his excessive consumption and selfish actions. This demonstrates the necessity
    for agents to acquire this type of information, whether independently through
    their observations, reflections, and understanding of the world, or through communication
    with another agent who has previously synthesized this information from their
    experiences.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10](#S6.F10 "图 10 ‣ 6.3 其他代理人行为知识的影响 ‣ 6 结果 ‣ LLM增强的自主代理人能否合作？通过熔炉对其合作能力的评估")
    显示，平均而言，没有个性的代理人专门攻击自私的代理人佩德罗的比例为86%。这说明两个没有明确个性的代理人如何利用被强行植入的信息来促进环境的整体可持续性，因为他们反复使得由于过度消费和自私行为而构成风险的代理人失效。这显示了代理人获取这种信息的必要性，无论是通过独立观察、反思和理解世界，还是通过与另一个已从经验中综合这些信息的代理人的沟通。
- en: '![Refer to caption](img/65c9394008e83ad767a37d3a98d17d8f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/65c9394008e83ad767a37d3a98d17d8f.png)'
- en: 'Figure 10: Graph depicting the average number of times an agent effectively
    attacked another agent in the scenario where all agents are informed that ”Pedro”
    is a ”Selfish agent.” At first glance, the results clearly show how the other
    two agents without personality choose to immobilize Pedro repeatedly throughout
    the simulations, directing more than 80% of their attacks exclusively at ”Pedro”.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：图示描述了在所有代理人都被告知“Pedro”是一个“自私代理人”的情况下，一个代理人有效攻击另一个代理人的平均次数。乍一看，结果清楚地显示了另外两个没有个性的代理人如何在模拟过程中反复选择使
    Pedro 失效，将超过 80% 的攻击专门集中在“Pedro”身上。
- en: 7 Discussion
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: 7.1 Importance of Cooperative Capabilities
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 合作能力的重要性
- en: In the presented scenarios, experiments detailed in Section [6](#S6 "6 Results
    ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot") revealed that the used agent architecture yielded
    suboptimal results when confronted with unfamiliar situations or when the LLM
    knowledge couldn’t decisively guide optimal decision-making. Furthermore, while
    agents demonstrated a willingness to cooperate, their actions did not reflect
    a clear understanding of how to effectively collaborate within the given environment.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在所呈现的场景中，第 [6](#S6 "6 结果 ‣ LLM 增强的自主代理能否合作？通过 Melting Pot 评估其合作能力") 节中详细的实验表明，当面临不熟悉的情况或
    LLM 知识无法明确指导最佳决策时，所使用的代理架构产生了次优结果。此外，尽管代理人表现出了合作的意愿，但他们的行动并未反映出对如何在给定环境中有效合作的清晰理解。
- en: To address the proposed scenarios in a better way, agents needed to recognize
    certain principles. For instance, they should refrain from harvesting the last
    apple in a green patch to prevent depletion and should engage in cooperation with
    other agents while avoiding collaboration with the bots or uncooperative agents.
    Observing that the bots consistently harvested apples unsustainably, agents should
    have deduced that attacking the bots was necessary to protect the green patches
    from depletion. This ability to prioritize long-term and collective welfare over
    short-term rewards, as well as recognizing the divergent behavior and preferences
    of other entities (bots), aligns with what Dadfoe et al. [[1](#bib.bib1)] refer
    to as cooperative capabilities.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地应对提出的场景，代理人需要认识到某些原则。例如，他们应该避免在绿色区域中采摘最后一个苹果，以防止资源枯竭，并且应与其他代理人合作，同时避免与机器人或不合作的代理人合作。观察到机器人一贯地不可持续地采摘苹果，代理人应该得出攻击机器人是保护绿色区域免受枯竭的必要措施。将长期和集体福利优先于短期奖励的能力，以及识别其他实体（机器人）的不同表现和偏好，符合
    Dadfoe 等人 [[1](#bib.bib1)] 所称的合作能力。
- en: 'This prompts a consideration of whether current agent architectures genuinely
    enable cooperative behavior, and if the absence of such capabilities hinders their
    ability to navigate more intricate tasks and environments. Dadfoe et al. [[1](#bib.bib1)]
    succinctly categorize cooperative capabilities into four essential components:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了对当前代理架构是否真正能够实现合作行为的思考，以及这种能力的缺失是否妨碍了它们在更复杂任务和环境中的导航。Dadfoe 等人 [[1](#bib.bib1)]
    简明扼要地将合作能力分类为四个基本组成部分：
- en: '1.'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Understanding: Agents must comprehend the world, anticipate the consequences
    of their actions, and demonstrate an understanding of the beliefs and preferences
    of others.'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解：代理人必须理解世界，预测他们行动的后果，并展示对他人信念和偏好的理解。
- en: '2.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Communication: Vital for achieving understanding and coordination, communication
    should be intentional, serving as a tool to gather information and coordinate
    efforts. Agents should be equipped to assess the intentions of others and establish
    their own criteria for discerning relevant information. Moreover, agents do not
    always have common interests, the other agent could be trying to deceive or convince
    in its self-interest.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 沟通：沟通对于实现理解和协调至关重要，应该是有意的，作为获取信息和协调努力的工具。代理人应该具备评估他人意图的能力，并建立自己的标准来辨别相关信息。此外，代理人并不总是有共同的利益，其他代理人可能会试图欺骗或为了自身利益进行说服。
- en: '3.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Commitment: Cooperation is often hindered by commitment problems arising from
    an inability to make credible promises or threats. Agent architectures should
    address these issues by providing mechanisms for agents to enforce or establish
    credibility in their promises and threats.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 承诺：合作通常会受到承诺问题的阻碍，这些问题源于无法做出可信的承诺或威胁。代理架构应该通过提供机制来解决这些问题，以便代理人能够执行或建立他们的承诺和威胁的可信度。
- en: '4.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Institutions: Social structures, such as institutions, play a crucial role
    in simplifying interactions between agents. These structures define the rules
    of the game for all entities, potentially extending to the allocation of roles,
    power, and resources.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机构：社会结构，如机构，在简化智能体之间的互动方面发挥着关键作用。这些结构定义了所有实体的游戏规则，可能扩展到角色、权力和资源的分配。
- en: In essence, cultivating collaborative capabilities within agent architectures
    is crucial for tackling the complexities inherent in diverse tasks and environments.
    Historically, agent architectures have inadequately endowed agents with such capabilities.
    Instances such as Generative Agents [[9](#bib.bib9)] and the Improving Factuality
    and Reasoning in Language Models through Multiagent Debate [[3](#bib.bib3)] enable
    agents to engage in conversations or observe the perspectives of others. However,
    these approaches are hampered by the absence of independent evaluation criteria
    and discernment specific to the current LLMs limitations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，在智能体架构中培养协作能力对于应对多样化任务和环境中的复杂性至关重要。历史上，智能体架构未能充分赋予智能体这种能力。例如，生成型智能体 [[9](#bib.bib9)]
    和通过多智能体辩论改进语言模型的事实性和推理能力 [[3](#bib.bib3)] 使智能体能够进行对话或观察他人的观点。然而，这些方法受到缺乏独立评估标准和针对当前LLM局限性的洞察力的阻碍。
- en: 7.2 Cooperative Agent Architecture
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 协作智能体架构
- en: '![Refer to caption](img/769fc49a98e3380a0407671bf1169b74.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/769fc49a98e3380a0407671bf1169b74.png)'
- en: 'Figure 11: Diagram of the proposed cooperative architecture. The modified or
    new modules are painted in blue.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：所提出的协作架构图。修改或新增的模块用蓝色标出。
- en: 'Based on previous findings, we proposed an architecture to enhance agents’
    cooperative capabilities (see Fig. [11](#S7.F11 "Figure 11 ‣ 7.2 Cooperative Agent
    Architecture ‣ 7 Discussion ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot")). In this
    architecture several new modules are proposed:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前的研究，我们提出了一种架构来增强智能体的协作能力（见图 [11](#S7.F11 "图 11 ‣ 7.2 协作智能体架构 ‣ 7 讨论 ‣ 增强型语言模型智能体能否协作？通过Melting
    Pot评估其协作能力")）。在这个架构中提出了几个新模块：
- en: '1.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Understanding module: This component is tasked with a comprehensive analysis
    of the agent’s memories, fostering a deeper comprehension of the surrounding world.
    The agent’s proficiency extends to predicting the behaviors of fellow agents and
    discerning environmental changes, enabling it to take actions with a keen awareness
    of their potential consequences. Notably, the agent must possess the capacity
    to infer both the governing principles of the world and the underlying motivations
    guiding others’ actions. This inference capability extends to scenarios where
    these principles may deviate from common knowledge or the pre-training model knowledge.
    Zhu et al. [[13](#bib.bib13)] demonstrate that LLMs, like GPT4, can learn such
    rules when explicitly prompted to identify them, utilizing question-answer pairs
    to later apply the learned rules in problem-solving. The proposed module operates
    by initially extracting the rules and behavioral patterns of the world and other
    agents. It achieves this by prompting the LLM with historical world observations
    and the current state of the world, aiming to identify rules that explain the
    current state based on the agent’s observations. These identified rules are initially
    stored as world hypotheses. As the agent utilizes these hypotheses to interpret
    the current state, they are transformed into explicit rules once they surpass
    a predefined threshold. Additionally, the LLM is prompted to generate predictions
    about future states of the environment, empowering the agent to make informed
    decisions guided by anticipated future scenarios.'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解模块：该组件负责对代理的记忆进行全面分析，以促进对周围世界的深入理解。代理的能力包括预测其他代理的行为和识别环境变化，使其能够在对潜在后果有清晰意识的情况下采取行动。值得注意的是，代理必须具备推断世界治理原则和指导他人行动的潜在动机的能力。这种推断能力适用于这些原则可能偏离常识或预训练模型知识的情况。Zhu等人[[13](#bib.bib13)]展示了像GPT4这样的LLM可以在明确提示下学习这些规则，通过问答对来识别规则，并在解决问题时应用学到的规则。提议的模块通过最初提取世界及其他代理的规则和行为模式来操作。它通过向LLM提供历史世界观察数据和当前世界状态，以识别基于代理观察的解释当前状态的规则。这些识别的规则最初被存储为世界假设。当代理利用这些假设来解释当前状态时，一旦超过预定义的阈值，它们会转变为明确规则。此外，LLM还被提示生成对环境未来状态的预测，赋予代理做出基于预期未来情景的明智决策的能力。
- en: '2.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Communication module: The primary objective of this module is to equip the
    agent with the ability to engage in intentional communication with other agents.
    Two key objectives have been identified to enhance cooperative capabilities: (1)
    The agent is encouraged to seek new information from other agents. It must decide
    whether there are pertinent questions that can be posed to fellow agents, aiding
    in a better understanding of the world or gaining insights into the preferences
    of others. This information is pivotal for augmenting the agent’s overall comprehension.
    (2) Agents are provided with the opportunity to negotiate and establish agreements
    deemed mutually beneficial. These agreements are stored in memory in a specialized
    manner to hold agents accountable for their commitments. The goal is to foster
    improved coordination among agents, thereby enhancing collaborative efforts.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通信模块：该模块的主要目标是赋予代理与其他代理进行有意沟通的能力。为了增强合作能力，已确定两个关键目标：（1）鼓励代理从其他代理那里获取新信息。代理必须决定是否可以向其他代理提出相关问题，以帮助更好地理解世界或获取他人的偏好。这些信息对于增强代理的整体理解至关重要。（2）为代理提供了谈判和建立互惠协议的机会。这些协议以专业的方式存储在内存中，以使代理对其承诺负责。目标是促进代理之间的协调，从而提升合作效果。
- en: '3.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Constitution Module: This module plays a crucial role in establishing a shared
    foundation for all agents. Its primary function is to define a set of common rules,
    providing agents with an initial framework to comprehend the world and formulate
    assumptions about the behavior of other agents. The constitution also delineates
    the consequences, whether penalties or rewards, that agents may face for specific
    behaviors or interactions. This not only lends credibility to agreements among
    agents but also discourages undesirable behaviors, streamlining interactions and
    cultivating a cooperative environment.'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 体制模块：该模块在为所有代理建立共同基础方面起着关键作用。其主要功能是定义一套共同规则，为代理提供一个初步框架，以理解世界并对其他代理的行为形成假设。体制还规定了代理在特定行为或互动中可能面临的后果，无论是惩罚还是奖励。这不仅增强了代理之间协议的可信度，还抑制了不良行为，简化了互动并促进了合作环境的培养。
- en: '4.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Reputation System: This system is designed to hold agents accountable for their
    actions. It evaluates each agent based on their adherence to agreements made with
    other agents. Periodically, the system prompts a language model with the existing
    agreements and corresponding actions, requesting a reputation score. This score
    is then accessible to all agents, influencing communication dynamics and aiding
    in understanding the behavior of others. Additionally, it facilitates making predictions
    about future states.'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 声誉系统：该系统旨在让代理对其行为负责。它根据每个代理遵守与其他代理达成的协议的情况来评估每个代理。系统会定期将现有协议和相应行动提示给语言模型，要求其提供一个声誉评分。这个评分对所有代理都是可访问的，影响沟通动态并帮助理解他人的行为。此外，它还便于对未来状态进行预测。
- en: 8 Conclusion and Future Work
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论与未来工作
- en: Cooperative capabilities have been somewhat overlooked in LLMs agent architectures,
    yet they may represent the crucial element enabling agents to accomplish pioneering
    tasks and thrive in intricate environments. As large language models (LLMs) advance,
    agent architectures stand to gain significantly by attaining enhanced responses
    from LLMs, particularly in tasks demanding substantial reasoning or when confronted
    with copious information in the prompt.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 合作能力在 LLMs 代理架构中有些被忽视，但它们可能代表了使代理能够完成开创性任务并在复杂环境中蓬勃发展的关键因素。随着大型语言模型（LLMs）的进步，代理架构将通过获得
    LLMs 的增强响应，特别是在需要大量推理或面对大量信息提示的任务中，显著受益。
- en: In this paper, our objective is to ascertain whether LLMs-enhanced autonomous
    agents can operate cooperatively. To this end, we adapt the Melting Pot scenarios
    to textual representations that can be easily operationalized by LLMs, and implement
    a reusable architecture for the development of LAAs employing the modules proposed
    in [[9](#bib.bib9)]. This architecture includes short and long-term memories and
    cognitive modules of perception, planning, reflection, and action. The common
    “Commons Harvest” game was used to test the resulting system, and the results
    were evaluated from the viewpoint of cooperative metrics in different proposed
    scenarios.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的目标是确定是否增强的 LLMs 自主代理能够协同工作。为此，我们将 Melting Pot 场景调整为 LLMs 可以轻松操作的文本表示，并实施了一种可重用的架构，用于开发使用
    [[9](#bib.bib9)] 中提出的模块的 LAAs。该架构包括短期和长期记忆以及感知、规划、反思和行动的认知模块。我们使用了常见的“Commons
    Harvest”游戏来测试结果系统，并从不同提出场景的协作指标角度评估了结果。
- en: The results indicate a gap in the current agent’s cooperative capabilities vis-à-vis
    unfamiliar situations. Agents showed a cooperative tendency but lacked adequate
    understanding of how to collaborate effectively in an unknown environment. The
    agents needed to understand complex factors like the need to conserve resources,
    identify non-cooperative agents, and prioritize collective welfare over short-term
    gains. The research, thereby, draws attention to the need for a more inclusive
    architecture fostering cooperation and enhancing agent capabilities, including
    superior understanding, effective communication, credible commitment, and well-defined
    social structures or institutions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，当前代理在面对不熟悉的情况时的合作能力存在差距。代理表现出合作倾向，但在如何有效地在未知环境中协作方面缺乏足够的理解。代理需要理解复杂的因素，如资源节约的必要性、识别非合作代理以及将集体福利置于短期利益之上。因此，这项研究引起了对建立更具包容性的架构的关注，以促进合作并提升代理能力，包括更好的理解、有效沟通、可信承诺和明确的社会结构或制度。
- en: Responding to the findings, we also proposed to improve the architecture with
    several modules to enhance the cooperative capabilities of the agents. These include
    an understanding module responsible for a comprehensive analysis of the agent’s
    memory and surroundings, a communication module to enable intentional information
    exchange, a constitution module that lays out common rules of engagement, and
    a reputation system that holds agents accountable for making decisions for the
    collective good. Our future efforts will be focused on building and evaluating
    this cooperative architecture.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些发现，我们还提议通过若干模块改进架构，以增强代理的合作能力。这些模块包括一个负责对代理的记忆和环境进行全面分析的理解模块，一个实现有意信息交换的通信模块，一个制定共同规则的宪法模块，以及一个让代理为集体利益做决策负责的信誉系统。我们未来的工作将集中在构建和评估这一合作架构上。
- en: \bmhead
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: \bmhead
- en: Acknowledgments
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by Google through the Google Research Scholar Program.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了谷歌通过谷歌研究学者计划的支持。
- en: References
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: \bibcommenthead
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \bibcommenthead
- en: 'Dafoe et al. [2020] Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee,
    K.R., Leibo, J.Z., Larson, K., Graepel, T.: Open Problems in Cooperative AI (2020)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dafoe 等人 [2020] Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee, K.R.,
    Leibo, J.Z., Larson, K., Graepel, T.: Open Problems in Cooperative AI (2020)'
- en: 'Agapiou et al. [2023] Agapiou, J.P., Vezhnevets, A.S., Duéñez-Guzmán, E.A.,
    Matyas, J., Mao, Y., Sunehag, P., Köster, R., Madhushani, U., Kopparapu, K., Comanescu,
    R., Strouse, D., Johanson, M.B., Singh, S., Haas, J., Mordatch, I., Mobbs, D.,
    Leibo, J.Z.: Melting Pot 2.0 (2023)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agapiou 等人 [2023] Agapiou, J.P., Vezhnevets, A.S., Duéñez-Guzmán, E.A., Matyas,
    J., Mao, Y., Sunehag, P., Köster, R., Madhushani, U., Kopparapu, K., Comanescu,
    R., Strouse, D., Johanson, M.B., Singh, S., Haas, J., Mordatch, I., Mobbs, D.,
    Leibo, J.Z.: Melting Pot 2.0 (2023)'
- en: 'Du et al. [2023] Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I.:
    Improving Factuality and Reasoning in Language Models through Multiagent Debate
    (2023)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等人 [2023] Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I.: Improving
    Factuality and Reasoning in Language Models through Multiagent Debate (2023)'
- en: 'Liu et al. [2023] Liu, Z., Yao, W., Zhang, J., Xue, L., Heinecke, S., Murthy,
    R., Feng, Y., Chen, Z., Niebles, J.C., Arpit, D., Xu, R., Mui, P., Wang, H., Xiong,
    C., Savarese, S.: BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous
    Agents (2023)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2023] Liu, Z., Yao, W., Zhang, J., Xue, L., Heinecke, S., Murthy, R.,
    Feng, Y., Chen, Z., Niebles, J.C., Arpit, D., Xu, R., Mui, P., Wang, H., Xiong,
    C., Savarese, S.: BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous
    Agents (2023)'
- en: 'Zhang et al. [2023] Zhang, J., Xu, X., Deng, S.: Exploring Collaboration Mechanisms
    for LLM Agents: A Social Psychology View (2023)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2023] Zhang, J., Xu, X., Deng, S.: Exploring Collaboration Mechanisms
    for LLM Agents: A Social Psychology View (2023)'
- en: 'Shinn et al. [2023] Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan,
    K., Yao, S.: Reflexion: Language Agents with Verbal Reinforcement Learning (2023)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人 [2023] Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan,
    K., Yao, S.: Reflexion: Language Agents with Verbal Reinforcement Learning (2023)'
- en: 'Schick et al. [2023] Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
    M., Zettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: Language Models Can
    Teach Themselves to Use Tools (2023)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人 [2023] Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
    M., Zettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: Language Models Can
    Teach Themselves to Use Tools (2023)'
- en: 'Yao et al. [2023] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
    K., Cao, Y.: ReAct: Synergizing Reasoning and Acting in Language Models (2023)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2023] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.,
    Cao, Y.: ReAct: Synergizing Reasoning and Acting in Language Models (2023)'
- en: 'Park et al. [2023] Park, J.S., O’Brien, J.C., Cai, C.J., Morris, M.R., Liang,
    P., Bernstein, M.S.: Generative Agents: Interactive Simulacra of Human Behavior
    (2023)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等人 [2023] Park, J.S., O’Brien, J.C., Cai, C.J., Morris, M.R., Liang, P.,
    Bernstein, M.S.: Generative Agents: Interactive Simulacra of Human Behavior (2023)'
- en: 'Wang et al. [2023] Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,
    Y., Fan, L., Anandkumar, A.: Voyager: An Open-Ended Embodied Agent with Large
    Language Models (2023)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2023] Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,
    Y., Fan, L., Anandkumar, A.: Voyager: An Open-Ended Embodied Agent with Large
    Language Models (2023)'
- en: 'Hong et al. [2023] Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang,
    C., Wang, J., Wang, Z., Yau, S.K.S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu,
    C., Schmidhuber, J.: MetaGPT: Meta Programming for A Multi-Agent Collaborative
    Framework (2023)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong 等人 [2023] Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang,
    C., Wang, J., Wang, Z., Yau, S.K.S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu,
    C., Schmidhuber, J.: MetaGPT: Meta Programming for A Multi-Agent Collaborative
    Framework (2023)'
- en: 'Zhou et al. [2024] Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.-T., Le,
    Q.V., Chi, E.H., Zhou, D., Mishra, S., Zheng, H.S.: Self-Discover: Large Language
    Models Self-Compose Reasoning Structures (2024)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 [2024] Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.-T., Le, Q.V.,
    Chi, E.H., Zhou, D., Mishra, S., Zheng, H.S.: Self-Discover: 大型语言模型自我构建推理结构 (2024)'
- en: 'Zhu et al. [2023] Zhu, Z., Xue, Y., Chen, X., Zhou, D., Tang, J., Schuurmans,
    D., Dai, H.: Large Language Models can Learn Rules (2023)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等 [2023] Zhu, Z., Xue, Y., Chen, X., Zhou, D., Tang, J., Schuurmans, D.,
    Dai, H.: 大型语言模型可以学习规则 (2023)'
- en: Appendix A Descriptions generated for the objects in the environment
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 对环境中对象的描述
- en: In Table [1](#A1.T1 "Table 1 ‣ Appendix A Descriptions generated for the objects
    in the environment ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") we show all the natural
    language descriptions generated to represent the relevant objects and events of
    the Commons Harvest scenario of Melting Pot.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [1](#A1.T1 "表格 1 ‣ 附录 A 对环境中对象的描述 ‣ LLM增强型自主代理能否合作？通过Melting Pot评估其合作能力")
    中，我们展示了为表示 Melting Pot Commons Harvest 场景中的相关对象和事件而生成的所有自然语言描述。
- en: 'Table 1: Natural language description by object or event'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 1: 按对象或事件的自然语言描述'
- en: '| Object/Event | Description |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 对象/事件 | 描述 |'
- en: '| --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Other agent | Observed agent `` at position `[, ]`. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 其他代理 | 在位置`[, ]`观察到代理``。 |'
- en: '| Grass | Observed grass to grow apples at position `[, ]`. This grass
    belongs to tree ``. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 草 | 在位置`[, ]`观察到草长出了苹果。该草属于树``。 |'
- en: '| Apple | Observed an apple at position `[, ]`. This apple belongs to
    tree ``. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 苹果 | 在位置`[, ]`观察到一个苹果。该苹果属于树``。 |'
- en: '| Tree | Observed tree `` at position `[, ]`. This tree has
    `apples_number` apples remaining and `grass_number` grass for apples growing on
    the observed map. The tree might have more apples and grass on the global map.
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 树 | 在位置`[, ]`观察到树``。该树剩余`apples_number`个苹果和`grass_number`块草，用于苹果的生长。树在全局地图上可能还有更多苹果和草。
    |'
- en: '| Observed someone being attacked | Someone was attacked at position `[,
    ]`. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 观察到有人被攻击 | 在位置`[, ]`有人被攻击。 |'
- en: '| Observed a ray beam | Observed a ray beam from an attack at position `[,
    ]`. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 观察到光束 | 观察到在位置`[, ]`的攻击光束。 |'
- en: '| Observed an apple was taken | Observed that agent `agent_name` took an apple
    from position `[, ]`. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 观察到一个苹果被拿走了 | 观察到代理`agent_name`从位置`[, ]`拿走了一个苹果。 |'
- en: '| Observed grass disappeared | Observed that the grass at position `[, ]`
    disappeared. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 观察到草消失了 | 观察到位置`[, ]`的草消失了。 |'
- en: '| Observed grass grew | Observed that grass to grow apples appeared at position
    `[, ]`. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 观察到草长出了苹果 | 观察到在位置`[, ]`的草长出了苹果。 |'
- en: '| Observed apple grew | Observed that an apple grew at position `[, ]`.
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 观察到苹果长大了 | 观察到在位置`[, ]`的苹果长大了。 |'
- en: '| The agent was attacked | There are no observations: You were attacked by
    agent `agent_name` and currently you’re out of the game. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 代理被攻击了 | 没有观察到：你被代理`agent_name`攻击了，目前你已经出局。 |'
- en: '| The agent is out of the game | There are no observations: you’re out of the
    game. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 代理已经出局 | 没有观察到：你已经出局。 |'
- en: '| \botrule |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| \botrule |  |'
- en: Appendix B Knowledge about the world given to agents
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 提供给代理的世界知识
- en: The Listing [1](#LST1 "Listing 1 ‣ Appendix B Knowledge about the world given
    to agents ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
    cooperative capabilities through Melting Pot") shows the raw world description
    passed to the agents. This is the only information agents have about the environment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 [1](#LST1 "清单 1 ‣ 附录 B 提供给代理的世界知识 ‣ LLM增强型自主代理能否合作？通过Melting Pot评估其合作能力")
    显示了传递给代理的原始世界描述。这是代理对环境所拥有的唯一信息。
- en: 'Listing 1: World context'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '清单 1: 世界背景'
- en: 'I  am  in  a  misterious  grid  world.  In  this  world  there  are  the  following  elements:Apple:  This  object  can  be  taken  by  any  agent.  The  apple  is  taken  when  I  go  to  its  position.  Apples  only  grow  on  grass  tiles.  When  an  apple  is  taken  it  gives  the  agent  who  took  it  a  reward  of  1.Grass:  Grass  tiles  are  visible  when  an  apple  is  taken.  Apples  will  regrow  only  in  this  type  of  tile  based  on  a  probability  that  depends  on  the  number  of  current  apples  in  a  L2  norm  neighborhood  of  radius  2.  When  there  are  no  apples  in  a  radius  of  2  from  the  grass  tile,  the  grass  will  disappear.  On  the  other  hand,  if  an  apple  grows  at  a  determined  position,  all  grass  tiles  that  had  beeen  lost  will  reappear  if  they  are  between  a  radius  of  two  from  the  apple.Tree:  A  tree  is  composed  from  apples  or  grass  tiles,  and  it  is  a  tree  because  the  patch  of  these  tiles  is  connected  and  have  a  fix  location  on  the  map.  These  trees  have  an  id  to  indentify  them.Wall:  These  tiles  delimits  the  grid  world  at  the  top,  the  left,  the  bottom,  and  the  right  of  the  grid  world.The  grid  world  is  composed  of  18  rows  and  24  columns.  The  tiles  start  from  the  [0,  0]  position  located  at  the  top  left,  and  finish  on  the  [17,  23]  position  located  at  the  bottom  right.I  am  an  agent  and  I  have  a  limited  window  of  observation  of  the  world.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我在一个神秘的网格世界中。在这个世界里，有以下元素：苹果：这个物体可以被任何代理人拿走。苹果在我走到其位置时被取走。苹果只能在草地砖上生长。当苹果被取走时，它会给取走它的代理人一个1的奖励。草地：当苹果被取走时，草地砖会变得可见。苹果将只会在这种类型的砖上重新生长，这取决于当前在半径2的L2范数邻域中的苹果数量。当半径2内没有苹果时，草地会消失。另一方面，如果苹果在某个位置生长，所有丢失的草地砖会重新出现，只要它们在苹果半径2的范围内。树：树由苹果或草地砖组成，它之所以被称为树，是因为这些砖块的区域是连接的，并且在地图上有固定的位置。这些树有一个ID来标识它们。墙：这些砖块限定了网格世界的顶部、左侧、底部和右侧。网格世界由18行24列组成。砖块从位于左上角的[0,
    0]位置开始，到位于右下角的[17, 23]位置结束。我是一个代理人，我对世界的观察窗口有限。
- en: Appendix C React prompt
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 反应提示
- en: 'The Listing [2](#LST2 "Listing 2 ‣ Appendix C React prompt ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") shows the whole prompt used in the react module. This prompt
    was used to let the agent decide whether to react or not to the current observations,
    where react means to change the plan and generate a new action. The inputs that
    this prompt receives are the following in order: name, world context, current
    observations, current plan, actions to take if any, changes observed in the game
    state, game time, and agent’s personality,'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [2](#LST2 "Listing 2 ‣ Appendix C React prompt ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot") 显示了在反应模块中使用的整个提示。这个提示用于让代理人决定是否对当前观察做出反应，其中反应意味着改变计划并生成一个新的动作。这个提示接收的输入顺序如下：名称、世界背景、当前观察、当前计划、需要采取的行动（如果有）、游戏状态的变化、游戏时间和代理人的个性，
- en: 'Listing 2: Prompt of the Perceive module'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 2: 感知模块的提示'
- en: 'You  have  this  information  about  an  agent  called  :’s  world  understanding:  Current  observations  at  :Current  plan:  Actions  to  execute:  Review  the  plan  and  the  actions  to  execute,  and  then  decide  if    should  continue  with  its  plan  and  the  actions  to  execute  given  the  new  information  that  it’s  seeing  in  the  observations.Remember  that  the  current  observations  are  ordered  by  closeness,  being  the  first  the  closest  observation  and  the  last  the  farest  one.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  :“‘json{”Reasoning”:  string,  \\  Step  by  step  thinking  and  analysis  of  all  the  observations  and  the  current  plan  to  decide  if  the  plan  should  be  changed  or  not”Answer”:  bool  \\  Answer  true  if  the  plan  or  actions  to  execute  should  be  changed  or  false  otherwise}“‘'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '你拥有关于一个名为  的代理的信息： 的世界理解： 当前观察情况在 ：
    当前计划： 执行的行动： 回顾计划和执行的行动，然后决定  是否应继续其计划和执行的行动，考虑到它在观察中看到的新信息。记住，当前观察是按接近程度排序的，第一个是最接近的观察，最后一个是最远的。输出应为以下格式的
    Markdown 代码片段，包括前导和结尾的 ”“‘json” 和 ”“‘”，回答时仿照 ：“‘json{”Reasoning”:  字符串，\\  对所有观察和当前计划进行逐步思考和分析，以决定计划是否需要更改”Answer”:  布尔值
    \\  如果计划或执行的行动需要更改，则回答 true，否则为 false}“‘'
- en: Appendix D Plan prompt
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 计划提示
- en: 'The Listing [3](#LST3 "Listing 3 ‣ Appendix D Plan prompt ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") shows the raw prompt used in the plan module. This prompt
    helps the agent make a high-level plan and define several goals to guide its actions.
    The inputs that this prompt receives are the following in order: name, world context,
    current observations, current plan, reflections, reason to react, agent’s personality,
    and changes observed in the game state.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [3](#LST3 "列表 3 ‣ 附录 D 计划提示 ‣ LLM 增强的自主代理是否能合作？通过 Melting Pot 评估它们的合作能力")
    显示了在计划模块中使用的原始提示。这个提示帮助代理制定高层次的计划，并定义几个目标以指导其行动。该提示接收的输入如下：名称、世界背景、当前观察、当前计划、反思、反应原因、代理的个性以及游戏状态中的观察到的变化。
- en: 'Listing 3: Prompt of the Plan module'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3：计划模块的提示
- en: 'You  have  this  information  about  an  agent  called  :’s  world  understanding:  Recent  analysis  of  past  observations:Observed  changes  in  the  game  state:Current  observations:Current  plan:  This  is  the  reason  to  change  the  current  plan:  With  the  information  given  above,  generate  a  new  plan  and  new  objectives  to  persuit.  The  plan  should  be  a  description  of  how    should  behave  in  the  long-term  to  maximize  its  wellbeing.The  plan  should  include  how  to  act  to  different  situations  observed  in  past  experiences.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  :“‘json{”Reasoning”:  string,  \\  Step  by  step  thinking  and  analysis  of  all  the  observations  and  the  current  plan  to  create  the  new  plan  and  the  new  goals.”Goals”:  string,  \\  The  new  goals  for  .”Plan”:  string  \\  The  new  plan  for  .  Do  not  describe  specific  actions.}“‘'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '你拥有关于一个名为  的代理的信息： 的世界理解： 过去观察的最新分析：
    游戏状态中的观察到的变化： 当前观察： 当前计划： 这是更改当前计划的原因： 根据以上信息，生成一个新的计划和新的目标。计划应描述
     在长期内如何行为以最大化其福祉。计划应包括如何应对过去经验中观察到的不同情况。输出应为以下格式的 Markdown 代码片段，包括前导和结尾的
    ”“‘json” 和 ”“‘”，回答时仿照 ：“‘json{”Reasoning”:  字符串，\\  对所有观察和当前计划进行逐步思考和分析，以制定新的计划和目标。”Goals”:  字符串，\\  
    的新目标。”Plan”:  字符串 \\   的新计划。不要描述具体行动。}“‘'
- en: Appendix E Reflection prompts
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 反思提示
- en: 'The Listing [4](#LST4 "Listing 4 ‣ Appendix E Reflection prompts ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") shows the raw prompt used in the first part of the reflections
    module i.e. question formulation. The inputs for this prompt are the following:
    name, world context, accumulated observations since the last reflection, and agent’s
    personality.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表[4](#LST4 "列表 4 ‣ 附录 E 反射提示 ‣ LLM增强的自主代理能否合作？通过Melting Pot评估他们的合作能力")展示了在反射模块的第一部分，即问题制定中使用的原始提示。该提示的输入包括：名称、世界背景、自上次反射以来积累的观察和代理的个性。
- en: 'The prompt used in the insight generation part that takes place in the reflect
    module is shown in Listing [5](#LST5 "Listing 5 ‣ Appendix E Reflection prompts
    ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot"), its corresponding inputs are the following:
    name, world context, group of memories retrieved for each generated question in
    the first part, and agent’s personality.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在反射模块中，洞察生成部分使用的提示见列表[5](#LST5 "列表 5 ‣ 附录 E 反射提示 ‣ LLM增强的自主代理能否合作？通过Melting
    Pot评估他们的合作能力")，其对应的输入包括：名称、世界背景、第一部分生成的问题所检索到的记忆组和代理的个性。
- en: 'Listing 4: Prompt for question generation in the reflect module'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4：反射模块中的问题生成提示
- en: 'You  have  this  information  about  an  agent  called  :’s  world  understanding:  Here  you  have  a  list  of  statements:Given  only  the  information  above,  formulate  the  3  most  salient  high-level  questionsyou  can  answer  about  the  events,  entities,  and  agents  in  the  statements.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  :“‘json{”Question_1”:  {”Reasoning”:  string  \\  Reasoning  for  the  question”Question”:  string  \\  The  question  itself},”Question_2”:  {”Reasoning”:  string  \\  Reasoning  for  the  question”Question”:  string  \\  The  question  itself},”Question_3”:  {”Reasoning”:  string  \\  Reasoning  for  the  question”Question”:  string  \\  The  question  itself}}“‘'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '关于一个名为`:`的代理的世界理解，你有以下信息：`这里有一个声明列表：仅根据以上信息，制定出3个最重要的高层次问题，这些问题可以回答有关事件、实体和声明中的代理的信息。输出应为以下格式的Markdown代码片段，包括开头和结尾的“```json”和“```”，回答时假设你是：“```json{”Question_1”:  {”Reasoning”:  string  \\  问题的推理”Question”:  string  \\  问题本身},”Question_2”:  {”Reasoning”:  string  \\  问题的推理”Question”:  string  \\  问题本身},”Question_3”:  {”Reasoning”:  string  \\  问题的推理”Question”:  string  \\  问题本身}}```。'
- en: 'Listing 5: Prompt for insight generation in the reflect module'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5：反射模块中的洞察生成提示
- en: 'You  have  this  information  about  an  agent  called  :’s  world  understanding:  Here  you  have  a  list  of  memory  statements  separated  in  groups  of  memories:Given  ’s  memories,  for  each  one  of  the  group  of  memories,  what  is  the  best  insight  you  can  provide  based  on  the  information  you  have?Express  your  answer  in  the  JSON  format  provided,  and  remember  to  explain  the  reasoning  behind  each  insight.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  :“‘json{”Insight_1”:  {”Reasoning”:  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  1”Insight”:  string  \\  The  insight  itself},”Insight_2”:  {”Reasoning”:  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  2”Insight”:  string  \\  The  insight  itself},”Insight_n”:  {”Reasoning”:  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  n”Insight”:  string  \\  The  insight  itself}}“‘'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '你有关于一个名为`:`的代理的信息：这里你有一组分组的记忆陈述：根据的记忆，对于每一个记忆组，你可以基于你拥有的信息提供什么最佳见解？请将你的回答以提供的
    JSON 格式表达，并记得解释每个见解背后的理由。输出应为以下模式的 Markdown 代码片段，包括前导和尾随的“‘json”和“‘”，请像一样回答：
    “‘json{”Insight_1”: {”Reasoning”: string  \\  记忆组 1 的见解背后的理由”Insight”: string  \\  见解本身},”Insight_2”:
    {”Reasoning”: string  \\  记忆组 2 的见解背后的理由”Insight”: string  \\  见解本身},”Insight_n”:
    {”Reasoning”: string  \\  记忆组 n 的见解背后的理由”Insight”: string  \\  见解本身}}“‘'
- en: Appendix F Act prompt
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F Act 提示
- en: 'The Listing [6](#LST6 "Listing 6 ‣ Appendix F Act prompt ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") shows the raw prompt used in the act module. This prompt
    is in charge of deciding which action to take. The inputs that this prompt receives
    are the following: name, world context, current plan, the most recent ten reflections,
    current observations, number of actions to generate, set of valid actions, current
    goals, agent’s personality, position of the known trees, portion of the map explored,
    previous actions, and changes observed in the game state.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [6](#LST6 "列表 6 ‣ 附录 F Act 提示 ‣ LLM 增强的自主代理能否合作？通过 Melting Pot 对其合作能力进行评估")
    显示了用于 Act 模块的原始提示。该提示负责决定采取何种行动。该提示接收的输入如下：名称、世界背景、当前计划、最近的十次反思、当前观察、要生成的动作数量、有效动作集合、当前目标、代理的个性、已知树木的位置、已探测的地图部分、之前的行动和在游戏状态中观察到的变化。
- en: 'Listing 6: Prompt of the Act module'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 6: Act 模块的提示'
- en: 'You  have  this  information  about  an  agent  called  :’s  world  understanding:  ’s  goals:  Current  plan:  Analysis  of  past  experiences:Portion  of  the  map  explored  by  :  Observed  changes  in  the  game  state:You  are  currently  viewing  a  portion  of  the  map,  and  from  your  position  at    you  observe  the  following:Define  what  should  be  the  nex  action  for  Laura  get  closer  to  achieve  its  goals  following  the  current  plan.Remember  that  the  current  observations  are  ordered  by  closeness,  being  the  first  the  closest  observation  and  the  last  the  farest  one.Each  action  you  determinate  can  only  be  one  of  the  following,  make  sure  you  assign  a  valid  position  from  the  current  observations  and  a  valid  name  for  each  action:Valid  actions:Remember  that  going  to  positions  near  the  edge  of  the  portion  of  the  map  you  are  seeing  will  allow  you  to  get  new  observations.The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  ”“‘json”  and  ”“‘”,  answer  as  if  you  were  Laura:“‘json{”Opportunities”:  string  \\  What  are  the  most  relevant  opportunities?  those  that  can  yield  the  best  benefit  for  you  in  the  long  term”Threats”:  string  \\  What  are  the  biggest  threats?,  what  observations  you  should  carefully  follow  to  avoid  potential  harm  in  your  wellfare  in  the  long  term?”Options:  string  \\  Which  actions  you  could  take  to  address  both  the  opportunities  ans  the  threats?”Consequences”:  string  \\  What  are  the  consequences  of  each  of  the  options?”Final  analysis:  string  \\  The  analysis  of  the  consequences  to  reason  about  what  is  the  best  action  to  take”Answer”:  string  \\  Must  be  one  of  the  valid  actions  with  the  position  replaced}“‘'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '你拥有有关一个名为:的代理的信息：的目标：当前计划：过去经验分析：探索的地图部分：观察到的游戏状态变化：你当前正在查看地图的一部分，从你在的位置可以观察到以下内容：定义
    Laura 应该采取的下一步行动，以使她更接近实现当前计划中的目标。记住，当前观察是按接近程度排序的，最前面的观察是最接近的，最后面的观察是最远的。你确定的每个行动只能是以下之一，确保你为每个行动分配一个有效的观察位置和一个有效的名称：有效行动：记住，前往你正在查看的地图部分边缘附近的位置将使你能够获得新的观察。输出应为
    Markdown 代码片段，格式如下，包括开头和结尾的“`json”和“`”，回答时请像 Laura 一样：“‘json{”Opportunities”:
    string \\ 最相关的机会是什么？那些可以在长期内为你带来最佳利益的机会”Threats”: string \\ 最大的威胁是什么？你应该仔细跟踪哪些观察，以避免长期内对你的福利造成潜在伤害？”Options”:
    string \\ 你可以采取哪些行动来应对机会和威胁？”Consequences”: string \\ 每个选项的后果是什么？”Final analysis:
    string \\ 对后果的分析，以推断出最佳行动”Answer”: string \\ 必须是有效行动之一，并且位置已被替换}“‘'
- en: Appendix G Simulations cost
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 模拟成本
- en: 'Table 2: Costs of simulations'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 模拟成本'
- en: '| Experiment | Avg. Simulation cost | Avg. execution time (minutes) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 实验 | 平均模拟成本 | 平均执行时间（分钟） |'
- en: '| --- | --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Set 1 - No bio | $8.57\pm 0.93$ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 组合 1 - 无生物 | $8.57\pm 0.93$ |'
- en: '| Set 1 - All Coop | $7.00\pm 1.58$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 组合 1 - 全部合作 | $7.00\pm 1.58$ |'
- en: '| Set 1 - All Coop with def¹¹1For this experiment the models of OpenAI were
    used through Azure. | $15.63\pm 5.69$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 组合 1 - 全部合作带防御¹¹1在此实验中，使用了通过 Azure 提供的 OpenAI 模型。 | $15.63\pm 5.69$ |'
- en: '| Set 1 - All Selfish | $8.60\pm 1.84$ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 组合 1 - 全部自私 | $8.60\pm 1.84$ |'
- en: '| Set 1 - All Selfish with def | $9.73\pm 1.59$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 组合 1 - 全部自私带防御 | $9.73\pm 1.59$ |'
- en: '| Set 2 - One tree - no bio | $0.78\pm 0.28$ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 组合 2 - 一棵树 - 无生物 | $0.78\pm 0.28$ |'
- en: '| Set 2 - One tree - all coop | $0.78\pm 0.17$ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 组合 2 - 一棵树 - 全部合作 | $0.78\pm 0.17$ |'
- en: '| Set 2 - One tree - all selfish | $0.83\pm 0.30$ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 组合 2 - 一棵树 - 全部自私 | $0.83\pm 0.30$ |'
- en: '| Set 2 - Agents versus Bots | $1.88\pm 0.66$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 组合 2 - 代理人与机器人 | $1.88\pm 0.66$ |'
- en: '| Set 3 - All aware one selfish | $10.05\pm 0.88$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 组合 3 - 所有知情且自私 | $10.05\pm 0.88$ |'
- en: '| \botrule |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| \botrule |  |  |'
