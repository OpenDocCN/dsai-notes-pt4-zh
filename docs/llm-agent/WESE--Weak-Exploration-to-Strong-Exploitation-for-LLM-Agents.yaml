- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:48:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:48:34'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'WESE: Weak Exploration to Strong Exploitation for LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'WESE: 从弱探索到强利用的LLM智能体'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07456](https://ar5iv.labs.arxiv.org/html/2404.07456)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.07456](https://ar5iv.labs.arxiv.org/html/2404.07456)
- en: Anonymous Authors Anonymous Affiliation anonymous@example.com    Xu Huang¹   
    Weiwen Liu²    Xiaolong Chen¹    Xingmei Wang¹    Defu Lian¹¹¹1Defu Lian is the
    corresponding author.    Yasheng Wang²    Ruiming Tang²    Enhong Chen¹ ¹University
    of Science and Technology of China, Hefei, China
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 匿名作者 匿名单位 anonymous@example.com    Xu Huang¹    Weiwen Liu²    Xiaolong Chen¹
       Xingmei Wang¹    Defu Lian¹¹¹1Defu Lian是通讯作者。    Yasheng Wang²    Ruiming Tang²
       Enhong Chen¹ ¹中国科学技术大学，合肥，中国
- en: ²Huawei Noah’s Ark Lab, Shenzhen, China xuhuangcs, chenxiaolong, xingmeiwang@mail.ustc.edu.cn,
    {liandefu, cheneh}@ustc.edu.cn,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ²华为诺亚方舟实验室，深圳，中国 xuhuangcs, chenxiaolong, xingmeiwang@mail.ustc.edu.cn, {liandefu,
    cheneh}@ustc.edu.cn,
- en: '{liuweiwen8,wangyasheng, tangruiming}@huawei.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{liuweiwen8,wangyasheng, tangruiming}@huawei.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, large language models (LLMs) have demonstrated remarkable potential
    as an intelligent agent. However, existing researches mainly focus on enhancing
    the agent’s reasoning or decision-making abilities through well-designed prompt
    engineering or task-specific fine-tuning, ignoring the procedure of exploration
    and exploitation. When addressing complex tasks within open-world interactive
    environments, these methods exhibit limitations. Firstly, the lack of global information
    of environments leads to greedy decisions, resulting in sub-optimal solutions.
    On the other hand, irrelevant information acquired from the environment not only
    adversely introduces noise, but also incurs additional cost. This paper proposes
    a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM
    agents in solving open-world interactive tasks. Concretely, WESE involves decoupling
    the exploration and exploitation process, employing a cost-effective weak agent
    to perform exploration tasks for global knowledge. A knowledge graph-based strategy
    is then introduced to store the acquired knowledge and extract task-relevant knowledge,
    enhancing the stronger agent in success rate and efficiency for the exploitation
    task. Our approach is flexible enough to incorporate diverse tasks, and obtains
    significant improvements in both success rates and efficiency across four interactive
    benchmarks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）展示了作为智能体的显著潜力。然而，现有研究主要集中在通过精心设计的提示工程或任务特定的微调来提升智能体的推理或决策能力，而忽略了探索与利用的过程。在处理开放世界互动环境中的复杂任务时，这些方法显示出局限性。首先，环境全球信息的缺乏导致了贪婪的决策，从而产生次优解。另一方面，从环境中获得的无关信息不仅引入了噪声，还增加了额外的成本。本文提出了一种新方法，即弱探索到强利用（WESE），以增强LLM智能体在解决开放世界互动任务中的能力。具体来说，WESE涉及解耦探索与利用过程，采用成本效益高的弱智能体执行探索任务以获得全球知识。随后，介绍了一种基于知识图谱的策略来存储获得的知识并提取与任务相关的知识，从而提高强智能体在利用任务中的成功率和效率。我们的方法足够灵活，可以融入多种任务，并在四个互动基准测试中获得了显著的成功率和效率提升。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) showcase a myriad of capabilities across diverse
    domains, encompassing human-computer conversation, instruction following, reasoning,
    and few-shot learning Zhao et al. ([2023](#bib.bib30)). These comprehensive abilities
    form a robust foundation, positioning LLMs as intelligent agents in solving open-world
    tasks, such as household tasks and open-world question-answering tasks Wang et
    al. ([2023b](#bib.bib18)); Xi et al. ([2023](#bib.bib24)). Recently, there have
    been numerous works to investigate the potential of LLM agents in enhancing their
    capabilities for open-world tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了在各种领域中的众多能力，包括人机对话、指令跟随、推理和少量学习 Zhao et al. ([2023](#bib.bib30))。这些综合能力构成了坚实的基础，将LLMs定位为解决开放世界任务的智能体，如家庭任务和开放世界问答任务 Wang
    et al. ([2023b](#bib.bib18)); Xi et al. ([2023](#bib.bib24))。最近，已有大量工作研究了LLM智能体在增强其开放世界任务能力方面的潜力。
- en: 'Benefiting from the capabilities of LLMs in instruction-following and few-shot
    learning, most methods guide LLMs in decision-making tasks through human-crafted
    design, avoiding the costly fine-tuning of LLMs Wei et al. ([2022](#bib.bib22));
    Wang et al. ([2022b](#bib.bib16)); Yao et al. ([2022](#bib.bib27)); Kojima et
    al. ([2022](#bib.bib4)). Existing prompt-engineering approaches primarily consider
    two factors: how to incorporate task-relevant information in the prompt, and how
    to elicit the reasoning ability of LLMs through prompts. Task-relevant information
    encompasses task descriptions and contextual feedback, such as the question and
    pertinent task statements in question-answering tasks, along with textual materials
    retrieved by the agent from the web while problem-solving. To enhance the reasoning
    capabilities of LLM agents, methods like CoT Wei et al. ([2022](#bib.bib22)),
    ReAct Yao et al. ([2022](#bib.bib27)) Reflexion Shinn et al. ([2023](#bib.bib9)),
    et al, inspire LLMs to engage in reasoning by constructing few-shot examples with
    explicit reasoning paths.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs在指令跟随和少量示例学习中的能力，许多方法通过人工设计指导LLMs进行决策任务，避免了昂贵的LLMs微调 Wei et al. ([2022](#bib.bib22));
    Wang et al. ([2022b](#bib.bib16)); Yao et al. ([2022](#bib.bib27)); Kojima et
    al. ([2022](#bib.bib4))。现有的提示工程方法主要考虑两个因素：如何在提示中融入与任务相关的信息，以及如何通过提示引发LLMs的推理能力。与任务相关的信息包括任务描述和上下文反馈，例如在问答任务中的问题和相关任务陈述，以及代理在解决问题时从网络中检索的文本材料。为了提升LLM代理的推理能力，像CoT Wei
    et al. ([2022](#bib.bib22))、ReAct Yao et al. ([2022](#bib.bib27))、Reflexion Shinn
    et al. ([2023](#bib.bib9))等方法，激励LLMs通过构建具有明确推理路径的少量示例来进行推理。
- en: '![Refer to caption](img/13fad9534415f43144291f3b82a4a43a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/13fad9534415f43144291f3b82a4a43a.png)'
- en: (a) ScienceWorld. Lack of global environmental information causes failure due
    to trapping in a loop or sub-optimal solution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ScienceWorld。缺乏全球环境信息会导致陷入循环或次优解，从而导致失败。
- en: '![Refer to caption](img/a034d859fa186ccbb17654a8ab5ae85d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a034d859fa186ccbb17654a8ab5ae85d.png)'
- en: (b) HotPotQA. The green sentence is helpful while others are task-irrelevant.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (b) HotPotQA。绿色句子有帮助，而其他句子与任务无关。
- en: 'Figure 1: Examples for sub-optimal decisions and irrelevant information in
    feedbacks.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：关于反馈中的次优决策和无关信息的示例。
- en: 'However, open-world tasks serve as a simulation of the real environment, wherein
    an agent explores and interacts continuously with the environment to acquire more
    information for solving complex tasks Côté et al. ([2019](#bib.bib2)); Shridhar
    et al. ([2020](#bib.bib10)); Wang et al. ([2022a](#bib.bib15)). There are several
    characteristics of such tasks, making them more challenging. The ability of LLM
    agents is far from optimal due to the following challenges: 1) Complexity. Each
    task involves multi-step actions and each task can have multiple feasible solutions.
    2) Uncertainty. The agent cannot obtain all the information from the initial task
    description, and additional information must be acquired through exploration.
    Regarding these challenges, solving these tasks necessitates multi-step exploration
    and exploitation by the agent. Exploration involves perceiving the environment
    and obtaining task-relevant information, while exploitation involves making action
    decisions based on existing knowledge. In existing prompt-based methods, exploration
    and exploitation issues are often overlooked, embedded within the reasoning process
    of the LLM Yao et al. ([2022](#bib.bib27)), leading to two major problems.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，开放世界任务作为真实环境的模拟，其中代理持续探索和与环境互动，以获取更多信息来解决复杂任务 Côté et al. ([2019](#bib.bib2));
    Shridhar et al. ([2020](#bib.bib10)); Wang et al. ([2022a](#bib.bib15))。这类任务具有几个特点，使其更具挑战性。LLM代理的能力远未达到最佳，面临以下挑战：1)
    复杂性。每个任务涉及多步骤操作，并且每个任务可能有多个可行的解决方案。2) 不确定性。代理无法从初始任务描述中获得所有信息，必须通过探索获得额外信息。对于这些挑战，解决这些任务需要代理进行多步骤的探索和利用。探索涉及感知环境和获取与任务相关的信息，而利用则基于现有知识做出行动决策。在现有的基于提示的方法中，探索和利用的问题常被忽视，嵌入LLM的推理过程中 Yao
    et al. ([2022](#bib.bib27))，导致两个主要问题。
- en: 'Firstly, the lack of global awareness of the environment at the outset solutions
    results in suboptimal decision-making by the LLM. As illustrated in Figure [1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents"), the goal is to find one aluminum object and test its conductivity.
    The agent is located outside initially. The best trajectory is marked with the
    white line, where the agent goes to the kitchen to take the aluminum fork first
    and then go to the workshop. When lack of global environmental information, the
    agent probably gets trapped in some room due to failure in finding an aluminum
    object (the red line) or chooses a more time-consuming way (the blue line). Secondly,
    the knowledge acquired by the LLM from environmental exploration tends to be excessive,
    including irrelevant information to the task. The presence of such information
    not only disrupts LLM decision-making but also incurs additional costs. Referred
    in Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents"), the feedback from the environment usually
    consists of massive task-irrelevant information while only one helpful sentence,
    i.e. the green line in this example, resulting in extra token usage of LLM and
    a negative effect on making optimal decisions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，初始阶段缺乏对环境的全局认知导致LLM的决策效果不佳。如图[1(a)](#S1.F1.sf1 "图1 ‣ 1 引言 ‣ WESE：弱探索到强利用的LLM代理")所示，目标是找到一个铝制物体并测试其导电性。代理最初位于外部。最佳轨迹用白线标记，代理首先前往厨房取铝制叉子，然后去工作坊。在缺乏全局环境信息的情况下，代理可能由于无法找到铝制物体而被困在某个房间（红线），或选择更耗时的路径（蓝线）。其次，LLM从环境探索中获得的知识往往过多，包括与任务无关的信息。这些信息不仅干扰LLM的决策，还会增加额外的成本。如图[1(b)](#S1.F1.sf2
    "图1 ‣ 1 引言 ‣ WESE：弱探索到强利用的LLM代理")所示，来自环境的反馈通常包含大量与任务无关的信息，而只有一个有用的句子，即示例中的绿色线，导致LLM额外的令牌使用并对做出最佳决策产生负面影响。
- en: To address the above limitations, we propose a novel prompt-based strategy to
    enhance the LLM agent in this work, termed Weak Exploration to Strong Exploitation
    (WESE). To tackle the first limitation, we introduce an idea that decouples the
    exploration and exploitation. Specifically, we construct two distinct LLM agents
    for exploration and exploitation tasks, respectively. In the exploration task,
    the LLM agent’s goal is to interact with the environment, exploring potentially
    helpful environmental information for task resolution. In the exploitation task,
    the information obtained during exploration serves as a global environmental prior,
    aiding the LLM agent in reasoning and decision-making to generate decisions. Regarding
    the second limitation, we compress the environmental information acquired by the
    exploration agent, structuring it in the form of a knowledge graph. During exploitation,
    we adopt a one-hop knowledge retrieval approach, selecting one-hop neighbors of
    task-relevant entities from the graph as priors, thereby reducing interference
    from irrelevant information. Furthermore, to further minimize resource consumption,
    we observe that a cost-effective weaker LLM (such as a 7B model) is fully capable
    of the less challenging exploratory tasks. Therefore, we propose the strategy
    of weak exploration to strong exploitation—leveraging the knowledge explored by
    the weak LLM agent to enhance the performance of the strong LLM agent.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述限制，我们提出了一种新颖的基于提示的策略，以增强本工作的LLM代理，称为“弱探索到强利用”（WESE）。为了克服第一个限制，我们引入了一个将探索和利用解耦的想法。具体而言，我们构建了两个不同的LLM代理，分别用于探索和利用任务。在探索任务中，LLM代理的目标是与环境互动，探索可能对任务解决有帮助的环境信息。在利用任务中，探索过程中获得的信息作为全局环境先验，帮助LLM代理进行推理和决策，以生成决策。关于第二个限制，我们将探索代理获得的环境信息压缩，形成知识图谱。在利用阶段，我们采用单跳知识检索方法，从图谱中选择任务相关实体的单跳邻居作为先验，从而减少无关信息的干扰。此外，为了进一步减少资源消耗，我们观察到成本效益较高的较弱LLM（如7B模型）完全能够完成较少挑战的探索任务。因此，我们提出了“弱探索到强利用”的策略——利用弱LLM代理探索的知识来提升强LLM代理的性能。
- en: 'Our main contributions are summarized as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献总结如下：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first work to investigate the effect
    of decoupling exploration and exploitation for LLM agents in open-world tasks.
    We further propose WESE, leveraging a weaker agent to enhance the stronger agent
    in a cost-effective manner.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是首个研究LLM智能体在开放世界任务中解耦探索和利用效果的工作。我们进一步提出了WESE，利用一个较弱的智能体以一种具有成本效益的方式增强更强的智能体。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To better leverage the environmental information obtained from exploration,
    we introduce a strategy to compress it into a knowledge graph. Then we devise
    a one-hop retrieval approach to filter out the irrelevant information.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了更好地利用探索过程中获得的环境信息，我们引入了一种将其压缩成知识图谱的策略。然后，我们设计了一种单跳检索方法，以过滤掉无关的信息。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experimental results over four open-world interactive benchmarks demonstrate
    the superiority of WESE, notably in achieving a remarkable balance between effectiveness,
    efficiency and cost.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在四个开放世界互动基准上的实验结果证明了WESE的优越性，尤其是在实现有效性、效率和成本之间的显著平衡方面。
- en: 2 Related Works
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM agents
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM智能体
- en: 'With the emergence of LLMs, their intelligence has sparked considerable potential
    in applying LLMs as the brains of agents. Existing LLM agent works primarily consider
    three key modules: planning, tool usage, and memory Wang et al. ([2023b](#bib.bib18)).
    Planning module aims to empower agent with the task-decomposition ability, encompassing
    works on task decomposition Wang et al. ([2023c](#bib.bib19)), feedback-driven
    adjustments Shinn et al. ([2023](#bib.bib9)), and multi-path reasoning Yao et
    al. ([2023](#bib.bib28)); Besta et al. ([2023](#bib.bib1)). Tool usage aims to
    strengthen the ability to use external tools Qin et al. ([2023a](#bib.bib7)).
    For instance, Visual ChatGPT Wu et al. ([2023](#bib.bib23)) incorporates visual
    models as tools to augment the LLM’s visual capabilities. ToolLlama Qin et al.
    ([2023b](#bib.bib8)) fine-tunes Llama’s ability to leverage various APIs. The
    memory module focuses on storing feedback information perceived from the environment,
    assisting the agent with experience, and fostering the growth of the agent. In
    Generative Agents Park et al. ([2023](#bib.bib6)), memories of simulated roles
    are stored as texts, utilizing RAG for relevant pieces. REMEMBER Zhang et al.
    ([2023](#bib.bib29)) proposes a semi-parametric memory, i.e. the Q-value table,
    to record rewards as the value and action in a given environment and task as the
    key. MemoryBank Wang et al. ([2023d](#bib.bib20)) leverages the Ebbinghaus forgetting
    curve, incorporating update and forgetting mechanisms into the memory design.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大语言模型（LLMs）的出现，它们的智能激发了将LLMs作为智能体“大脑”的巨大潜力。现有的LLM智能体工作主要考虑三个关键模块：规划、工具使用和记忆。规划模块旨在赋予智能体任务分解能力，包括任务分解、反馈驱动的调整和多路径推理；工具使用旨在加强使用外部工具的能力。例如，Visual
    ChatGPT通过将视觉模型作为工具来增强LLM的视觉能力。ToolLlama微调了Llama利用各种API的能力。记忆模块专注于存储从环境中感知到的反馈信息，帮助智能体积累经验，并促进智能体的成长。在生成智能体中，模拟角色的记忆以文本形式存储，利用RAG检索相关部分。REMEMBER提出了一种半参数记忆，即Q值表，用于记录奖励作为值，在给定环境和任务中记录行动作为键。MemoryBank利用艾宾浩斯遗忘曲线，将更新和遗忘机制纳入记忆设计中。
- en: In our proposed WESE, the knowledge graph is essentially a memory, updating
    information obtained through exploration into the graph.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的WESE中，知识图谱本质上是一种记忆，将通过探索获得的信息更新到图谱中。
- en: 2.2 LLM for open-world tasks
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM用于开放世界任务
- en: Open-world tasks represent the simulation of real-world environments. Within
    these tasks, agents engage in continuous interactions with the environment to
    gather pertinent information, subsequently making decisions and taking action
    to accomplish goals. Open-world tasks typically exhibit fewer constraints on the
    process, placing greater emphasis on the final rewards. Representative examples
    of open-world tasks include games like “Minecraft” Wang et al. ([2023a](#bib.bib17),
    [e](#bib.bib21)), where textual information and visual feedback are involved.
    Another category comprises text-based simulators based on the TextWorld Côté et
    al. ([2019](#bib.bib2)), such as AlfWorld Shridhar et al. ([2020](#bib.bib10)),
    which involves household tasks, ScienceWorld Wang et al. ([2022a](#bib.bib15)),
    which involves simple scientific experiments, and question-answering tasks Yang
    et al. ([2018](#bib.bib25)); Thorne et al. ([2018](#bib.bib12)) where agents need
    to interact with the web to obtain supporting information, such as Wikipedia.
    In tackling such tasks, Chain-of-Thought(CoT) Wei et al. ([2022](#bib.bib22))
    proposes adding few-shot examples in the prompt, guiding the LLM to solve the
    task step by step. ReAct Yao et al. ([2022](#bib.bib27)) induces the reasoning
    capability of LLMs by introducing an extra thought step. Subsequent methods have
    built upon ReAct, with enhancements such as the Reflexion Shinn et al. ([2023](#bib.bib9))
    mechanism, allowing agents to learn from mistakes in subsequent attempts. Additionally,
    several methods leverage the coding capabilities of LLMs, transforming tasks into
    programming tasks and guiding LLMs to generate codes as plans, such as VOYAGER Wang
    et al. ([2023a](#bib.bib17)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 开放世界任务代表了现实世界环境的模拟。在这些任务中，代理与环境进行持续互动，以收集相关信息，然后做出决策并采取行动以实现目标。开放世界任务通常对过程的约束较少，更加重视最终奖励。开放世界任务的代表性例子包括像
    “Minecraft” Wang 等人 ([2023a](#bib.bib17), [e](#bib.bib21)) 这样的游戏，其中涉及文本信息和视觉反馈。另一类包括基于
    TextWorld Côté 等人 ([2019](#bib.bib2)) 的文本模拟器，例如涉及家务任务的 AlfWorld Shridhar 等人 ([2020](#bib.bib10))，涉及简单科学实验的
    ScienceWorld Wang 等人 ([2022a](#bib.bib15))，以及需要与网络互动以获取支持信息的问答任务 Yang 等人 ([2018](#bib.bib25))；Thorne
    等人 ([2018](#bib.bib12))。在处理这些任务时，Chain-of-Thought (CoT) Wei 等人 ([2022](#bib.bib22))
    提出了在提示中添加少量示例，指导 LLM 一步步解决任务。ReAct Yao 等人 ([2022](#bib.bib27)) 通过引入额外的思考步骤来引导
    LLM 的推理能力。随后的方法在 ReAct 的基础上进行了改进，例如 Reflexion Shinn 等人 ([2023](#bib.bib9)) 机制，允许代理在后续尝试中从错误中学习。此外，还有一些方法利用
    LLM 的编码能力，将任务转化为编程任务，并指导 LLM 生成代码作为计划，例如 VOYAGER Wang 等人 ([2023a](#bib.bib17))。
- en: 3 Methodologies
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/723282ebe4e685e3ef254ed54725786b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/723282ebe4e685e3ef254ed54725786b.png)'
- en: 'Figure 2: Framework of WESE. The left part represents the weak exploration
    and the right part represents the strong exploitation. We employ Llama-2-7B as
    the weak agent and text-davinci-003 as the strong agent in the implementation.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：WESE 的框架。左侧表示弱探索，右侧表示强利用。在实现中，我们使用 Llama-2-7B 作为弱代理，使用 text-davinci-003
    作为强代理。
- en: 3.1 Decoupling Exploration and Exploitation
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 解耦探索与利用
- en: Open-world tasks differ from traditional reasoning and decision-making tasks.
    Traditional reasoning Huang and Chang ([2022](#bib.bib3)); Sun et al. ([2023](#bib.bib11))
    or decision-making Yang et al. ([2023](#bib.bib26)) tasks typically present all
    relevant information at once, requiring the agent to deduce and make a plan based
    on the provided information, such as mathematical calculations or logical reasoning
    problems. Conversely, in open-world tasks, only the task description is initially
    specified. In this context, the agent must continually interact with the environment
    to obtain supporting information, comprising the exploration and exploitation
    steps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 开放世界任务不同于传统的推理和决策任务。传统的推理 Huang 和 Chang ([2022](#bib.bib3))；Sun 等人 ([2023](#bib.bib11))
    或决策任务 Yang 等人 ([2023](#bib.bib26)) 通常会一次性呈现所有相关信息，要求代理根据提供的信息进行推导和规划，例如数学计算或逻辑推理问题。相反，在开放世界任务中，最初仅指定任务描述。在这种情况下，代理必须不断与环境互动，以获取支持信息，包括探索和利用步骤。
- en: 'Let $E$ is represented as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让 $E$ 表示如下：
- en: '|  | $1$2 |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $reasion(\cdot)$ denotes the mix of explore and exploit.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $reasion(\cdot)$ 表示探索和利用的混合。
- en: 'Input: Knowledge triplets set $K$;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：知识三元组集合 $K$；
- en: Algorithm 1 Graph construction algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 图构建算法。
- en: 'Input: Knowledge graph $G$;9                  10            11'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：知识图谱 $G$；9                  10            11
- en: Algorithm 2 Triplet retrieval algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 三元组检索算法
- en: 'Within this paradigm, the knowledge $K$ utilized is solely the limited information
    about the environment obtained through partial observations. Particularly, greedy
    decisions are taken in the initial steps when the agent possesses limited awareness
    of the environment. For instance, in a task such as “cleaning some apples with
    soap” and the agent’s initial location is the hall. The actual locations of the
    apple and soap are in the drawer of the table in the hall and on the sink in the
    kitchen, respectively. The lack of environmental knowledge may lead the agent
    to be misled by the world knowledge of the LLM, going to the kitchen to find the
    apple. Consequently, substantial efforts traversing every corner of the kitchen
    are wasted, resulting in suboptimal plans and even failures due to trapping in
    the loop. Therefore, we investigate the strategy to decouple exploration and exploitation,
    formalized as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式下，使用的知识 $K$ 仅限于通过部分观察获得的环境信息。特别是在智能体对环境了解有限的初始步骤中，会采取贪婪决策。例如，在任务“用肥皂清洗一些苹果”中，智能体的初始位置是大厅。苹果和肥皂的实际位置分别在大厅的桌子抽屉和厨房的水槽中。缺乏环境知识可能导致智能体被
    LLM 的世界知识误导，前往厨房寻找苹果。因此，智能体在厨房的每个角落浪费了大量时间，导致了次优计划甚至因陷入循环而失败。因此，我们研究了探索与利用解耦的策略，形式化如下：
- en: '|  | $\small a_{i}=\left\{\begin{aligned} &amp;explore(E,T,s_{i-1};\Theta,P_{e})\in\mathcal{A}_{e},\;i<N_{e};\\
    &amp;exploit(E,T,s_{i-1};\Theta,P_{t},K=\cup_{j\leq N}\{F_{j}\})\in\mathcal{A}_{t},i\geq
    N_{e}.\end{aligned}\right.$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small a_{i}=\left\{\begin{aligned} &amp;explore(E,T,s_{i-1};\Theta,P_{e})\in\mathcal{A}_{e},\;i<N_{e};\\
    &amp;exploit(E,T,s_{i-1};\Theta,P_{t},K=\cup_{j\leq N}\{F_{j}\})\in\mathcal{A}_{t},i\geq
    N_{e}.\end{aligned}\right.$ |  |'
- en: where $P_{e}$ is the maximum number of steps of exploration, which could also
    be determined by the agent, such as terminating the exploration automatically
    when it thinks the obtained information is sufficient.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{e}$ 是探索的最大步骤数，也可以由智能体确定，例如当智能体认为获取的信息足够时自动终止探索。
- en: Different from the previous methods, our method places the whole exploration
    phase before exploitation explicitly, as opposed to the alternation of exploration
    and exploitation. In this manner, the agent has extensively explored the environment,
    acquiring global environmental prior knowledge denoted as $K=\cup_{j\leq N}\{F_{j}\}$.
    Exploitation with global knowledge benefits the effectiveness and efficiency of
    the solutions, which is empirically validated in our experiments.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的方法不同，我们的方法明确将整个探索阶段置于利用阶段之前，而不是探索与利用交替进行。通过这种方式，智能体已经广泛探索了环境，获取了全球环境的先验知识，记作
    $K=\cup_{j\leq N}\{F_{j}\}$。利用全球知识可以提高解决方案的有效性和效率，这在我们的实验中得到了实证验证。
- en: However, two subsequent issues exist following the decoupling approach. Firstly,
    the information obtained from environmental feedback is huge due to the extensive
    exploration, including a lot of task-irrelevant information. Secondly, the extensive
    exploration contributes to increased resource consumption, such as token usage.
    Therefore, we demand an efficient mechanism for information transfer between exploration
    and exploitation and a cost-efficient exploration-exploitation strategy. We address
    the two issues in the subsequent parts of this section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解耦方法后存在两个后续问题。首先，由于广泛探索，环境反馈获得的信息量巨大，包括大量与任务无关的信息。其次，广泛的探索会导致资源消耗增加，例如令牌使用量。因此，我们需要一个高效的信息传递机制在探索和利用之间，以及一个成本效益高的探索-利用策略。我们在本节的后续部分解决这两个问题。
- en: 3.2 Knowledge Compression and Retrieval
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 知识压缩与检索
- en: Real-world textual information exhibits inherent sparsity, characterized by
    long sentences consisting of plenty of non-informative conjunctions and adjectives.
    Environmental feedback in open-world tasks manifests as such text, where the cumulative
    extensive exploration yield long and unstructured textual information, demonstrating
    serve sparsity. Considering the limited context window of the LLM and the expensive
    cost of token usage, it is necessary to compress the sparse information. Leveraging
    a knowledge graph (KG) to store information has proved advantageous in enhancing
    information density and leveraging domain-specific knowledge in existing works Pan
    et al. ([2024](#bib.bib5)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的文本信息具有固有的稀疏性，其特征是长句子包含大量非信息性连词和形容词。开放世界任务中的环境反馈表现为这种文本，其中大量的广泛探索产生长而非结构化的文本信息，表现出严重的稀疏性。考虑到LLM的有限上下文窗口和代币使用的高昂成本，有必要压缩稀疏信息。利用知识图谱(KG)来存储信息已被证明有利于提高信息密度并利用领域特定知识，现有研究如Pan等人([2024](#bib.bib5))中也有所体现。
- en: Consequently, benefiting from the superiority of LLM in relation-extraction
    tasks Wadhwa et al. ([2023](#bib.bib14)), we extract the knowledge from the received
    feedback to form an environmental knowledge graph. Specifically, the LLM extracts
    knowledge triplets from the environmental feedback after each exploration step,
    updating them into the knowledge graph. For example, as for the search result
    given by Wikipedia “Since 2005 Wendy Schaal has primarily worked in voice acting,
    most notably voicing Francine Smith in the animated comedy television series American
    Dad!”, knowledge triplets are extracted as $\langle$. Notably, the environmental
    knowledge graph we obtained is task-relevant, serving as a memory like the Random
    Access Memory(RAM). Actually, a worldwide knowledge graph could be leveraged and
    continually in our method, serving as a general memory. We leave it for further
    work.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，得益于LLM在关系提取任务中的优势Wadhwa等人([2023](#bib.bib14))，我们从接收到的反馈中提取知识以形成环境知识图谱。具体而言，LLM从每次探索步骤后的环境反馈中提取知识三元组，并将其更新到知识图谱中。例如，对于维基百科给出的搜索结果“自2005年以来，Wendy
    Schaal主要从事配音工作，最著名的是为动画喜剧电视系列《美国老爹》中Francine Smith配音！”，知识三元组被提取为$\langle$。值得注意的是，我们获得的环境知识图谱是与任务相关的，像随机存取存储器（RAM）一样作为记忆。实际上，我们的方法可以利用全球知识图谱，并持续作为通用记忆。我们将其留待进一步研究。
- en: 'Input: Environment $E$;16'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：环境$E$;16
- en: Algorithm 3 WESE algorithm.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法3 WESE算法。
- en: 'Nevertheless, it is imperative to acknowledge that not all information in the
    knowledge graph proves useful. The introduction of task-irrelevant information
    has the potential to lead the hallucination phenomena of LLM, such as the confusion
    of entity and relation. For example, giving the triplet $\langle$ and the question
    is “What’s the favorite fruit of Bill?”, the LLM would confuse the relation and
    answer with apple. Benefiting from the graph structure, we adopt a one-hop retrieval
    method to extract task-related information easily, illustrated in Algorithm [2](#alg2
    "In 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents"). Concretely, we initiate the
    process by extracting involved entities from the task description with LLM. Subsequently,
    we perform a one-hop retrieval on the graph to obtain the neighbors of these entities.
    The retrieved knowledge triplets are then injected into the prompt, serving as
    task-relevant knowledge during the exploitation phase, thereby assisting the LLM
    in task-solving.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须承认并非所有知识图谱中的信息都是有用的。引入与任务无关的信息可能导致LLM的幻觉现象，如实体与关系的混淆。例如，给出三元组$\langle$，问题是“比尔最喜欢的水果是什么？”，LLM可能会混淆关系并回答“苹果”。得益于图结构，我们采用一种一跳检索方法来轻松提取与任务相关的信息，如算法[2](#alg2
    "在 3.1 解耦探索与利用 ‣ 3 方法 ‣ WESE：LLM代理的弱探索到强利用")所示。具体而言，我们通过LLM从任务描述中提取相关实体。随后，我们对图进行一跳检索，以获得这些实体的邻居。检索到的知识三元组随后被注入到提示中，作为任务相关知识，在利用阶段辅助LLM完成任务。
- en: 3.3 Weak Exploration to Strong Exploitation
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 从弱探索到强利用
- en: 'Table 1: Results on ALFWorld(134 tasks). SR and AS are abbreviations for success
    rate and average steps of successful tasks, respectively. SESE represents the
    variant of WESE—Strong Exploration to Strong Exploitation. The Imp represents
    the relative improvements compared to base methods, i.e. Act and ReAct. The bold
    and underline represent the best and the second best for the same base method.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：ALFWorld上的结果（134个任务）。SR和AS分别是成功率和成功任务的平均步数的缩写。SESE表示WESE的变体——强探索到强利用。Imp表示与基础方法相比的相对改进，即Act和ReAct。粗体和下划线表示同一基础方法下的最佳和第二最佳。
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 效果 | 效率 | 成本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | SR$\uparrow$ | Imp(%) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SR$\uparrow$ | 改进(%) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Act | 0.43 | 0.00 | 10.83 | 0.00 | 4,908,548 | 21,243 | 98.60 | 0.00 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Act | 0.43 | 0.00 | 10.83 | 0.00 | 4,908,548 | 21,243 | 98.60 | 0.00 |'
- en: '| Act-WESE | 0.63 | +46.51 | 7.54 | +30.38 | 3,746,290 | 19,562 | 75.32 | +23.61
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Act-WESE | 0.63 | +46.51 | 7.54 | +30.38 | 3,746,290 | 19,562 | 75.32 | +23.61
    |'
- en: '| Act-SESE | 0.67 | +55.81 | 6.73 | +37.86 | 7,259,508 | 75,153 | 146.69 |
    -48.77 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Act-SESE | 0.67 | +55.81 | 6.73 | +37.86 | 7,259,508 | 75,153 | 146.69 |
    -48.77 |'
- en: '| ReAct | 0.57 | 0.00 | 16.64 | 0.00 | 7,565,676 | 43,250 | 152.18 | 0.00 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.57 | 0.00 | 16.64 | 0.00 | 7,565,676 | 43,250 | 152.18 | 0.00 |'
- en: '| ReAct-WESE | 0.72 | +26.32 | 13.69 | +17.73 | 5,032,374 | 41,004 | 101.47
    | +33.32 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 0.72 | +26.32 | 13.69 | +17.73 | 5,032,374 | 41,004 | 101.47
    | +33.32 |'
- en: '| ReAct-SESE | 0.75 | +31.58 | 12.41 | +25.42 | 8,996,182 | 97,286 | 181.87
    | -19.51 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-SESE | 0.75 | +31.58 | 12.41 | +25.42 | 8,996,182 | 97,286 | 181.87
    | -19.51 |'
- en: 'Table 2: Results on ScienceWorld(296 tasks). TR, AR and AS are abbreviations
    for total reward, average reward and average steps to get positive reward, respectively.
    Other symbols are consistent with Table [1](#S3.T1 "Table 1 ‣ 3.3 Weak Exploration
    to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：ScienceWorld上的结果（296个任务）。TR、AR和AS分别是总奖励、平均奖励和获得正奖励的平均步数的缩写。其他符号与表 [1](#S3.T1
    "表 1 ‣ 3.3 从弱探索到强利用 ‣ 3 方法论 ‣ WESE：LLM代理的弱探索到强利用")一致。
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 效果 | 效率 | 成本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | TR$\uparrow$ | Imp(%) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | TR$\uparrow$ | 改进(%) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Act | 4908 | 16.58 | 0.00 | 18.00 | 0.00 | 13,554,960 | 55,817 | 272.22 |
    0.00 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Act | 4908 | 16.58 | 0.00 | 18.00 | 0.00 | 13,554,960 | 55,817 | 272.22 |
    0.00 |'
- en: '| Act-WESE | 5198 | 17.56 | 5.91 | 15.68 | +12.91 | 13,491,043 | 65,952 | 271.14
    | +0.40 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Act-WESE | 5198 | 17.56 | 5.91 | 15.68 | +12.91 | 13,491,043 | 65,952 | 271.14
    | +0.40 |'
- en: '| Act-SESE | 5249 | 17.73 | 6.94 | 15.39 | +14.49 | 36,424,190 | 165,568 |
    731.80 | -168.83 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Act-SESE | 5249 | 17.73 | 6.94 | 15.39 | +14.49 | 36,424,190 | 165,568 |
    731.80 | -168.83 |'
- en: '| ReAct | 4454 | 15.05 | 0.00 | 20.00 | 0.00 | 17,716,698 | 84,724 | 356.03
    | 0.00 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 4454 | 15.05 | 0.00 | 20.00 | 0.00 | 17,716,698 | 84,724 | 356.03
    | 0.00 |'
- en: '| ReAct-WESE | 5317 | 17.96 | 19.34 | 19.65 | +1.77 | 16,310,632 | 80,851 |
    327.83 | +7.92 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 5317 | 17.96 | 19.34 | 19.65 | +1.77 | 16,310,632 | 80,851 |
    327.83 | +7.92 |'
- en: '| ReAct-WESE | 5053 | 17.07 | 13.42 | 19.02 | +4.92 | 40,293,571 | 196,338
    | 809.80 | -127.45 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 5053 | 17.07 | 13.42 | 19.02 | +4.92 | 40,293,571 | 196,338
    | 809.80 | -127.45 |'
- en: Acquiring more comprehensive global information about the environment demands
    a considerable resource cost in the exploration process. However, compared to
    exploitation, exploration exhibits lower complexity, requiring less reasoning
    and induction. Concretely, exploration operations exhibit low requirements for
    the logic and coherence of actions, emphasizing actions pertaining to environmental
    observation. For example, the exploration actions mainly consist of several simple
    actions on decision-making benchmarks, such as “go to [room]”, “look around”,
    et al, while exploitation involves a series of coherent operations like (go to
    sink/stove, put the bowl in/on the sink/stove, activate the sink/stove, wait,
    deactivate the sink/stove). Therefore, we propose to use a weaker agent for the
    exploration to mitigate resource consumption, namely the weak exploration. From
    the perspective of the LLM agent, a weaker agent represents substituting the underlying
    LLM for exploration with a weaker LLM, i.e. an LLM with fewer parameters, thereby
    reducing costs. In our experiments, we compare performance between strong exploration
    and weak exploration. Our findings reveal that a weaker exploration has a negligible
    impact on the final success rate, yet it significantly lowers costs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 获取关于环境的更全面的全球信息在探索过程中需要大量的资源成本。然而，与利用相比，探索表现出较低的复杂性，需要更少的推理和归纳。具体而言，探索操作对逻辑性和一致性的要求较低，强调与环境观察相关的操作。例如，探索行动主要包括决策制定基准上的几个简单动作，如“去[房间]”、“四处看看”等，而利用涉及一系列连贯的操作，如（去水槽/炉子，放碗到水槽/炉子上，激活水槽/炉子，等待，关闭水槽/炉子）。因此，我们建议使用较弱的代理进行探索以减少资源消耗，即弱探索。从LLM代理的角度来看，较弱的代理意味着用一个较弱的LLM替代用于探索的基础LLM，即具有较少参数的LLM，从而降低成本。在我们的实验中，我们比较了强探索和弱探索的性能。我们的发现表明，较弱的探索对最终成功率的影响微乎其微，但显著降低了成本。
- en: 'The framework of WESE is illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3 Methodologies
    ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"). There are three
    key components in the framework: a weak LLM agent, a strong LLM agent, and a KG-based
    memory. The whole process consists of the weak exploration (left) and the strong
    exploitation (right). Meanwhile, we offer an algorithmic pseudo-code in Algorithm [3](#alg3
    "In 3.2 Knowledge Compression and Retrieval ‣ 3 Methodologies ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents"). First, a weak LLM agent is employed to
    explore the interactive environment to obtain information in line 1 to 7\. Then
    those knowledge triplets are organized as a knowledge graph $G_{K}$ in line 8,
    as illustrated in Algorithm [1](#alg1 "In 3.1 Decoupling Exploration and Exploitation
    ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents").
    Further, the involved entities are extracted from the task with a LLM and the
    relevant triplets are retrieved from the graph in line 10\. Retrieved knowledge
    is leveraged for exploitation in line 12, serving as the prior knowledge.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'WESE的框架如图[2](#S3.F2 "图 2 ‣ 3 方法论 ‣ WESE: 从弱探索到强利用的LLM代理")所示。框架中有三个关键组件：一个弱LLM代理、一个强LLM代理和一个基于KG的记忆。整个过程包括弱探索（左）和强利用（右）。同时，我们在算法[3](#alg3
    "在 3.2 知识压缩与检索 ‣ 3 方法论 ‣ WESE: 从弱探索到强利用的LLM代理")中提供了一个算法伪代码。首先，使用弱LLM代理探索交互环境以获取信息，见第1至7行。然后，这些知识三元组被组织成一个知识图谱$G_{K}$，见第8行，如算法[1](#alg1
    "在 3.1 解耦探索与利用 ‣ 3 方法论 ‣ WESE: 从弱探索到强利用的LLM代理")所示。进一步地，从任务中提取相关实体，并用LLM从图谱中检索相关三元组，见第10行。检索到的知识在第12行用于利用，作为先验知识。'
- en: 4 Experiments
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'We employ two categories of interactive open-world tasks as benchmarks: decision-making
    and question-answering, where each task requires multi-step interactions with
    the environment. We evaluate our methods from three perspectives: effectiveness,
    efficiency and cost, representing whether the agent can complete the tasks, how
    many steps the agent would take to finish the task, and the expenses for the agent
    to complete the task, respectively.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用两类交互式开放世界任务作为基准：决策制定和问答任务，其中每个任务都需要与环境进行多步交互。我们从三个方面评估我们的方法：有效性、效率和成本，分别代表代理是否能完成任务、完成任务需要多少步骤，以及完成任务的费用。
- en: 4.1 Decision Making Tasks
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 决策制定任务
- en: We begin with the open-world decision-making tasks, where environments are based
    on a text-based simulator. The tasks are about the household, where the agent
    needs to explore various rooms and take operations on several objects.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从开放世界决策任务开始，这些任务基于文本模拟器。任务涉及家庭，代理需要探索不同的房间，并对多个对象进行操作。
- en: 4.1.1 ALFWorld
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 ALFWorld
- en: ALFWorld Shridhar et al. ([2020](#bib.bib10)) is a synthetic text-based simulated
    interactive environment. It comprises six types of tasks where agents need to
    interact with the environment to generate a series of actions to solve household
    tasks. For example, in the task “clean some knife and put it in countertop”, the
    ideal solution involves actions such as (go to countertop 2, take knife 1, go
    to sinkbasin 2, clean knife 1, put knife 1 on countertop 2). These tasks vary
    in difficulty, with challenging tasks encompassing over 50 locations and requiring
    more than 50-step actions, posing challenges for both the exploration and exploitation
    processes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ALFWorld Shridhar 等人 ([2020](#bib.bib10)) 是一个基于文本的模拟交互环境。它包含六种任务类型，代理需要与环境互动以生成一系列动作来解决家庭任务。例如，在“清洁一些刀子并将其放在台面上”任务中，理想的解决方案包括（前往台面
    2，取刀子 1，前往水槽 2，清洁刀子 1，将刀子 1 放在台面 2 上）等动作。这些任务难度各异，具有挑战性的任务涉及超过 50 个位置，并需要超过 50
    步的动作，对探索和利用过程都提出了挑战。
- en: '![Refer to caption](img/58629698405dfa0ee223aadfb9e74791.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/58629698405dfa0ee223aadfb9e74791.png)'
- en: (a) Relative improvements for Act-based methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基于 Act 方法的相对改进。
- en: '![Refer to caption](img/bee38cfe075e8ca0a71467d49d7217ab.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bee38cfe075e8ca0a71467d49d7217ab.png)'
- en: (b) Relative improvements for ReAct-based methods.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于 ReAct 方法的相对改进。
- en: 'Figure 3: Relative improvements in success rate over various types of tasks
    on ALFWorld. The left tasks are more complicated.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：ALFWorld 上各种任务类型的成功率相对改进。左侧任务更复杂。
- en: To validate the effectiveness of WESE, we adopt Act Yao et al. ([2022](#bib.bib27))
    and ReAct Yao et al. ([2022](#bib.bib27)) as baselines. Act leverages the idea
    of CoT, providing LLMs with few-shot interactive examples. ReAct, building upon
    Act, introduces an extra “thought” step where LLMs can choose to explicitly output
    their thought about the current state or generate action. In WESE, we initially
    use a Weak LLM for exploration to acquire task-relevant knowledge. We then leverage
    the obtained knowledge to solve problems with two base methods. We employ Llama-2-7B Touvron
    et al. ([2023](#bib.bib13)) as the weak LLM and text-davinci-003 (with probably
    more than 175 billion parameters) developed by OpenAI ²²2[https://platform.openai.com/](https://platform.openai.com/)
    as the strong LLM. The limits of steps $N_{e},N_{t}$ are both set to 50\. Our
    evaluation focuses on success rates, average steps to complete tasks, and the
    cost of OpenAI API tokens as three key metrics. Additionally, we introduce a variant
    of WESE—Strong Exploration to Strong Exploitation (SESE), where the weak LLM in
    the exploration process is replaced with the strong LLM, to verify the effectiveness
    of the decoupling strategy and examine the impact of LLM strength on exploration
    quality.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 WESE 的有效性，我们采用 Act Yao 等人 ([2022](#bib.bib27)) 和 ReAct Yao 等人 ([2022](#bib.bib27))
    作为基准。Act 利用 CoT 的理念，提供 LLM 少量交互示例。ReAct 在 Act 的基础上，引入了额外的“思考”步骤，LLMs 可以选择明确输出关于当前状态的思考或生成行动。在
    WESE 中，我们最初使用弱 LLM 进行探索以获取与任务相关的知识。然后，我们利用获得的知识通过两种基础方法解决问题。我们使用 Llama-2-7B Touvron
    等人 ([2023](#bib.bib13)) 作为弱 LLM，以及由 OpenAI 开发的 text-davinci-003（可能有超过 1750 亿个参数）²²2[https://platform.openai.com/](https://platform.openai.com/)
    作为强 LLM。步骤 $N_{e},N_{t}$ 的限制都设置为 50。我们的评估重点关注成功率、完成任务的平均步骤和 OpenAI API 令牌的成本这三个关键指标。此外，我们引入了
    WESE 的一种变体——从强探索到强利用（SESE），其中在探索过程中用强 LLM 替代弱 LLM，以验证解耦策略的有效性，并考察 LLM 强度对探索质量的影响。
- en: 4.1.2 ScienceWorld
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 ScienceWorld
- en: Similar to ALFWorld, ScienceWorld Wang et al. ([2022a](#bib.bib15)) is an interactive
    household environment as well. However, the tasks in ScienceWorld are more challenging,
    involving scientific experiments such as boiling and creating a new color by mixing
    primary colors. The environment is more complex, comprising ten distinct rooms,
    each with different furnishings, and not each pair of rooms is connected.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 ALFWorld，ScienceWorld Wang 等人 ([2022a](#bib.bib15)) 也是一个交互式家庭环境。然而，ScienceWorld
    中的任务更具挑战性，涉及科学实验，如煮沸和通过混合原色创造新颜色。环境更为复杂，包含十个不同的房间，每个房间的家具不同，并且不是每对房间都是连接的。
- en: We conduct experiments on eight types of tasks within ScienceWorld, choosing
    about 30 instances for each task due to a limited budget. Unlike ALFWorld where
    the agent can get a reward of 1 only when the task is completed, the agent in
    ScienceWorld receives partial rewards upon completing crucial steps, with the
    total reward reaching 100\. Given the challenging nature of the tasks, achieving
    a full reward of 100 is rare. Therefore, we utilize the number of steps taken
    by the agent until it first obtains a positive reward as the metric for efficiency.
    Other settings are consistent with ALFWorld.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 ScienceWorld 中对八种任务进行了实验，由于预算有限，我们为每个任务选择了大约 30 个实例。与 ALFWorld 中只有在任务完成时代理才会获得
    1 的奖励不同，ScienceWorld 中的代理在完成关键步骤时会获得部分奖励，总奖励为 100。由于任务具有挑战性，获得满分 100 的奖励是很少见的。因此，我们利用代理在首次获得正奖励之前所采取的步骤数作为效率的度量。其他设置与
    ALFWorld 一致。
- en: 4.1.3 Results
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 结果
- en: 'The results on ALFWorld and ScienceWorld are shown in Table [1](#S3.T1 "Table
    1 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents") and Table [2](#S3.T2 "Table
    2 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak
    Exploration to Strong Exploitation for LLM Agents"), respectively. We conclude
    several findings based on the results. Consistent with results reported in ReAct,
    ReAct outperforms Act on two benchmarks, showing the superiority of the “thought”
    step. However, this additional step leads to a longer action sequence, resulting
    in an average relative 32.38% increase in average steps. Decoupling of exploration
    and exploitation demonstrates advantages in effectiveness and efficiency, resulting
    in SESE outperforming baselines significantly with average relative 26.94% and
    20.67% improvements in terms of success rate (average reward) and average steps.
    However, the cost of SESE increases a lot due to the introduction of extensive
    strong exploration, showing an average relative 91.14% increase over baselines.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'ALFWorld 和 ScienceWorld 上的结果分别显示在表 [1](#S3.T1 "Table 1 ‣ 3.3 Weak Exploration
    to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation
    for LLM Agents") 和表 [2](#S3.T2 "Table 2 ‣ 3.3 Weak Exploration to Strong Exploitation
    ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents")。根据结果我们得出几个结论。与
    ReAct 报告的结果一致，ReAct 在两个基准测试中优于 Act，显示了“思考”步骤的优势。然而，这一额外步骤导致了更长的动作序列，使得平均步骤数相对增加了
    32.38%。探索和利用的解耦显示了在有效性和效率上的优势，使得 SESE 在成功率（平均奖励）和平均步骤方面相对提高了 26.94% 和 20.67%，显著优于基线。然而，由于引入了广泛的强探索，SESE
    的成本大幅增加，比基线高出 91.14%。'
- en: WESE shows a better balance between effectiveness, efficiency, and cost, which
    saves 53.83% of costs with only relative 1.43% and 6.89% degradations in effectiveness
    and efficiency compared with SESE. In WESE, the weak LLM agent undertakes the
    exploration process, resulting in cost savings for extensive exploration. Besides,
    benefiting from the related triplets extracted from the explored KG, the strong
    LLM agent only needs to focus on exploitation, further decreasing the number of
    steps, evidenced by the decreased completion tokens and average steps.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: WESE 在有效性、效率和成本之间展现了更好的平衡，与 SESE 相比，节省了 53.83% 的成本，同时在有效性和效率上的相对降幅仅为 1.43% 和
    6.89%。在 WESE 中，较弱的 LLM 代理承担了探索过程，从而为广泛的探索节省了成本。此外，得益于从已探索的 KG 中提取的相关三元组，强大的 LLM
    代理只需专注于利用，进一步减少了步骤数量，这从完成的 token 和平均步骤的减少中得到了证实。
- en: 'We further investigate the improvements of WESE on various types of tasks,
    shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.1.1 ALFWorld ‣ 4.1 Decision Making Tasks
    ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents").
    Both WESE and SESE show improvements over almost all types of tasks, further indicating
    the effectiveness of the decoupling strategy. In addition, the improvements in
    “clean” and “heat” tasks are greater than other tasks. The reason lies in that
    the two tasks involved more complicated exploitation compared with “put”, where
    the agents need to find the object first and then clean or heat it instead of
    just moving it to another place. The result demonstrates extensive exploration
    benefits more for complex tasks.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步探讨了WESE在各种任务上的改进，如图[3](#S4.F3 "Figure 3 ‣ 4.1.1 ALFWorld ‣ 4.1 Decision
    Making Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for
    LLM Agents")所示。WESE和SESE在几乎所有类型的任务中都显示出改进，进一步表明了去耦策略的有效性。此外，“清洁”和“加热”任务的改进大于其他任务。原因在于这两个任务相比于“放置”，涉及了更复杂的利用，代理需要先找到物体，然后进行清洁或加热，而不是简单地移动到另一个地方。这一结果表明，广泛探索对复杂任务更有益。'
- en: 4.2 Question Answering Tasks
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 问答任务
- en: We also validate our WESE on two open-world interactive question-answering benchmarks,
    i.e., HotPotQA and FEVER. Different from traditional question-answering tasks
    where supporting sentences are given, those tasks provide the question only and
    require the agent to search information on the web step by step to give the final
    answer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在两个开放世界互动问答基准上验证了我们的WESE，即HotPotQA和FEVER。与传统的问答任务不同，这些任务只提供问题，需要代理逐步在网上搜索信息以给出最终答案。
- en: 'Table 3: Results on HotPotQA(500 tasks). SR and AS are abbreviations for success
    rate and average steps of successful tasks, respectively. SESE represents the
    variant of WESE—Strong Exploration to Strong Exploitation. The Imp represents
    the relative improvements compared to base methods, i.e. Act and ReAct. The bold
    and underline represent the best and the second best for the same base method.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：HotPotQA（500任务）上的结果。SR和AS分别是成功率和成功任务的平均步骤的缩写。SESE表示WESE的变体——强探索到强利用。Imp表示与基础方法（即Act和ReAct）相比的相对改进。粗体和下划线表示相同基础方法中的最佳和第二最佳。
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 效果性 | 效率 | 成本 |'
- en: '| Method | SR$\uparrow$ | Imp(%) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SR$\uparrow$ | 改进(%) |'
- en: '| CoT | 0.318 | N/A | 1.00 | N/A | 261,347 | 25,382 | 5.73 | N/A |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.318 | N/A | 1.00 | N/A | 261,347 | 25,382 | 5.73 | N/A |'
- en: '| Act | 0.296 | 0.00 | 3.53 | 0.00 | 2,390,041 | 14,236 | 48.09 | 0.00 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Act | 0.296 | 0.00 | 3.53 | 0.00 | 2,390,041 | 14,236 | 48.09 | 0.00 |'
- en: '| Act-WESE | 0.353 | +19.26 | 2.69 | +23.80 | 2,307,421 | 13,973 | 46.42 |
    +3.45 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Act-WESE | 0.353 | +19.26 | 2.69 | +23.80 | 2,307,421 | 13,973 | 46.42 |
    +3.45 |'
- en: '| Act-SESE | 0.361 | +21.96 | 2.58 | +26.91 | 7,522,826 | 27,1551 | 155.89
    | -224.18 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Act-SESE | 0.361 | +21.96 | 2.58 | +26.91 | 7,522,826 | 27,1551 | 155.89
    | -224.18 |'
- en: '| ReAct | 0.342 | 0.00 | 3.17 | 0.00 | 3,234,876 | 65,306 | 66.00 | 0.00 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.342 | 0.00 | 3.17 | 0.00 | 3,234,876 | 65,306 | 66.00 | 0.00 |'
- en: '| ReAct-WESE | 0.394 | +15.20 | 2.29 | +27.76 | 2,574,401 | 67,908 | 52.85
    | +19.93 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 0.394 | +15.20 | 2.29 | +27.76 | 2,574,401 | 67,908 | 52.85
    | +19.93 |'
- en: '| ReAct-SESE | 0.416 | +21.64 | 2.11 | +33.44 | 7,338,590 | 323,401 | 153.24
    | -132.17 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-SESE | 0.416 | +21.64 | 2.11 | +33.44 | 7,338,590 | 323,401 | 153.24
    | -132.17 |'
- en: 'Table 4: Results on FEVER(500 tasks). The meanings of abbreviations and symbols
    are consistent with Table [3](#S4.T3 "Table 3 ‣ 4.2 Question Answering Tasks ‣
    4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents").'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '表4：FEVER（500任务）上的结果。缩写和符号的含义与表[3](#S4.T3 "Table 3 ‣ 4.2 Question Answering
    Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM
    Agents")中的一致。'
- en: '| Performance | Effectiveness | Efficiency | Cost |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 效果性 | 效率 | 成本 |'
- en: '| Method | SR$\uparrow$ | Imp(%) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SR$\uparrow$ | 改进(%) |'
- en: '| CoT | 0.61 | N/A | 1.00 | N/A | 100,387 | 11,942 | 2.25 | N/A |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.61 | N/A | 1.00 | N/A | 100,387 | 11,942 | 2.25 | N/A |'
- en: '| Act | 0.56 | 0.00 | 2.16 | 0.00 | 723,646 | 6,980 | 14.61 | 0.00 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Act | 0.56 | 0.00 | 2.16 | 0.00 | 723,646 | 6,980 | 14.61 | 0.00 |'
- en: '| Act-WESE | 0.62 | +10.71 | 1.58 | +26.66 | 723,867 | 5,937 | 14.60 | +0.11
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Act-WESE | 0.62 | +10.71 | 1.58 | +26.66 | 723,867 | 5,937 | 14.60 | +0.11
    |'
- en: '| Act-SESE | 0.64 | +14.29 | 1.57 | +27.34 | 2,822,189 | 122,543 | 60.89 |
    -316.73 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Act-SESE | 0.64 | +14.29 | 1.57 | +27.34 | 2,822,189 | 122,543 | 60.89 |
    -316.73 |'
- en: '| ReAct | 0.63 | 0.00 | 2.18 | 0.00 | 1,074,080 | 36,040 | 22.20 | 0.00 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 0.63 | 0.00 | 2.18 | 0.00 | 1,074,080 | 36,040 | 22.20 | 0.00 |'
- en: '| ReAct-WESE | 0.68 | +7.26 | 1.62 | +25.96 | 918,905 | 29,895 | 18.98 | +14.53
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-WESE | 0.68 | +7.26 | 1.62 | +25.96 | 918,905 | 29,895 | 18.98 | +14.53
    |'
- en: '| ReAct-SESE | 0.70 | +10.09 | 1.59 | +27.18 | 3,104,924 | 162,363 | 65.35
    | -194.32 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ReAct-SESE | 0.70 | +10.09 | 1.59 | +27.18 | 3,104,924 | 162,363 | 65.35
    | -194.32 |'
- en: 4.2.1 HotPotQA
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 HotPotQA
- en: 'HotPotQAYang et al. ([2018](#bib.bib25)) is a question-answering dataset where
    each question is paired with supporting sentences from Wikipedia articles. In
    traditional QA tasks, the supporting sentences are given and the remained task
    is to reason. Referred in ReAct, we use the Wikipedia API with three types of
    actions to support interactive information retrieval: (1) search[entity], which
    searches the Wikipedia with the entity and returns the corresponding page if it
    exists, or suggests top-5 similar entities; (2) lookup[keyword], which looks up
    keyword in the page and returns the next sentence containing the keyword, simulating
    the Ctrl+F function in a web browser; (3) finish[answer], which answers the question
    with answer. Once the answer matches the ground truth, the environment would return
    reward 1\. We sample 500 tasks from the development set.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: HotPotQA Yang等人（[2018](#bib.bib25)）是一个问答数据集，每个问题都与维基百科文章中的支持句子配对。在传统的QA任务中，支持句子是给定的，剩下的任务是推理。参考ReAct，我们使用维基百科API配合三种类型的操作来支持交互式信息检索：（1）search[entity]，在维基百科中搜索实体，如果存在则返回相应的页面，或建议前5个类似实体；（2）lookup[keyword]，在页面中查找关键词，并返回包含该关键词的下一句，模拟浏览器中的Ctrl+F功能；（3）finish[answer]，用答案回答问题。一旦答案与真实答案匹配，环境将返回奖励1。我们从开发集中抽取了500个任务。
- en: We employ the CoT Wei et al. ([2022](#bib.bib22)), Act and ReAct as baselines
    and empower Act and ReAct with WESE and SESE. Note that CoT is a one-step method
    that does not support interactive tasks, we inject the supporting sentences into
    the prompts and instruct the LLM to reason for the final answer without searching
    on the web. Also, WESE is not designed for such a purely reasoning method but
    for methods involving interactions with the environment. For Act and ReAct, we
    keep the settings consistent with the original paper. As there are probably lots
    of related triplets to the task-involved entities, we set the limit of retrieved
    triplets as 10 and the limits of steps $N_{e},N_{t}$ as 8\. The evaluation for
    effectiveness, efficiency and cost is consistent with the ALFWorld.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用CoT Wei等人（[2022](#bib.bib22)）、Act和ReAct作为基线，并用WESE和SESE增强Act和ReAct。请注意，CoT是一种不支持交互任务的一步方法，我们将支持句子注入到提示中，并指示LLM在不进行网络搜索的情况下推理出最终答案。此外，WESE并不是为这种纯推理方法设计的，而是为涉及与环境交互的方法设计的。对于Act和ReAct，我们保持设置与原始论文一致。由于与任务相关的三元组可能很多，我们将检索的三元组限制设置为10，将步骤限制$N_{e},N_{t}$设置为8。效果、效率和成本的评估与ALFWorld一致。
- en: 4.2.2 FEVER
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 FEVER
- en: FEVER Thorne et al. ([2018](#bib.bib12)) is a fact verification dataset, consisting
    of instances where each instance comprises a claim and a justification(True or
    False or Not Clear). We employ the Wikipedia API to construct an interactive environment
    consistent with that in HotPotQA. Other settings are kept consistent with HotPotQA,
    such as the number of retrieved triplets and the maximum steps.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: FEVER Thorne等人（[2018](#bib.bib12)）是一个事实验证数据集，每个实例包括一个声明和一个理由（真、假或不明确）。我们使用维基百科API来构建一个与HotPotQA一致的互动环境。其他设置也与HotPotQA保持一致，例如检索的三元组数量和最大步骤数。
- en: 4.2.3 Results
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 结果
- en: 'The results on HotPotQA and FEVER are shown in Table [3](#S4.T3 "Table 3 ‣
    4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong
    Exploitation for LLM Agents") and Table [4](#S4.T4 "Table 4 ‣ 4.2 Question Answering
    Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM
    Agents"), respectively. We can conclude several findings based on the results.
    Similar to decision-making tasks, ReAct outperforms Act significantly due to the
    additional “thought” step. Also, methods equipped with WESE or SESE outperform
    baselines in both success rate and the number of taken actions, resulting in average
    relative improvements of 19.5% and 28.0%, respectively. Especially, SESE methods
    surpass WESE slightly with average relative 3.5% and 3.6% improvements in terms
    of success rate and average steps, while increasing more than twice the expenses.
    This further demonstrates that the weak agent powered by Llama-2-7B is almost
    sufficient for the exploration task.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'HotPotQA和FEVER上的结果分别显示在表[3](#S4.T3 "Table 3 ‣ 4.2 Question Answering Tasks
    ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents")和表[4](#S4.T4
    "Table 4 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration
    to Strong Exploitation for LLM Agents")中。我们可以根据结果得出一些结论。类似于决策任务，由于额外的“思考”步骤，ReAct在性能上显著优于Act。此外，配备了WESE或SESE的方法在成功率和所采取的行动数量上均优于基准，平均相对改进分别为19.5%和28.0%。特别地，SESE方法在成功率和平均步骤上相较于WESE略有提高，平均相对改进分别为3.5%和3.6%，但费用增加了两倍以上。这进一步证明了由Llama-2-7B驱动的弱代理对于探索任务几乎足够。'
- en: Different from decision-making tasks, question-answering tasks require fewer
    steps due to more information being returned with one search action. However,
    our WESE and SESE are still capable of reducing the number of steps, further showing
    the advantage of the explored knowledge. As for the cost, the tokens increased
    in SESE are far more than those in decision-making tasks, which can be attributed
    to the long-textual feedback from Wikipedia.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与决策任务不同，问答任务由于每次搜索操作返回的信息更多，因此所需的步骤较少。然而，我们的WESE和SESE仍然能够减少步骤数量，进一步显示了探索知识的优势。至于成本，SESE中的token增加远高于决策任务，这可以归因于来自维基百科的长文本反馈。
- en: 5 Conclusion
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we introduce WESE, a cost-effective method that enhances LLM
    agents in open-world interactive tasks. We decouple the exploration and exploitation,
    employing two agents for the distinct processes. To empower the communication
    between the two processes, we introduce a knowledge graph-based memory to compress
    and structure the information obtained in exploration, where task-relevant information
    is extracted from the graph by a one-hop retrieval method. We then propose to
    leverage a weaker agent for the exploration process, forming a cost-effective
    manner with negligible performance degradation. Experimental results demonstrate
    the superiority of WESE in effectiveness, efficiency, and cost.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们介绍了WESE，这是一种成本效益高的方法，用于增强LLM代理在开放世界互动任务中的表现。我们将探索与开发过程解耦，采用两个代理分别处理这两个过程。为了增强这两个过程之间的沟通，我们引入了基于知识图谱的记忆来压缩和结构化在探索中获得的信息，其中任务相关的信息通过单步检索方法从图谱中提取。我们随后建议使用一个较弱的代理进行探索，从而形成一种成本效益高且性能降幅几乎可以忽略不计的方式。实验结果表明，WESE在效果、效率和成本方面优于其他方法。
- en: References
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Besta et al. [2023] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large
    language models. arXiv preprint arXiv:2308.09687, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta等人 [2023] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk等人。《思想图谱：利用大型语言模型解决复杂问题》。arXiv预印本 arXiv:2308.09687, 2023年。
- en: 'Côté et al. [2019] Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas,
    Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud
    Adada, et al. Textworld: A learning environment for text-based games. In Computer
    Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International
    Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13,
    2018, Revised Selected Papers 7, pages 41–75\. Springer, 2019.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Côté等人 [2019] Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian
    Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada等人。《Textworld：一个基于文本游戏的学习环境》。在计算机游戏：第七届研讨会，CGW
    2018，与第27届国际人工智能会议（IJCAI 2018）联合举办，2018年7月13日，瑞典斯德哥尔摩，修订精选论文7，第41–75页。Springer，2019年。
- en: 'Huang and Chang [2022] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning
    in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄和张 [2022] 黄杰和张凯文·陈全。面向大型语言模型的推理：一项综述。arXiv预印本 arXiv:2212.10403，2022年。
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances
    in neural information processing systems, 35:22199–22213, 2022.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小岛等人 [2022] 小岛武、谷世香·肖恩、马歇尔·里德、松尾丰和岩泽悠介。大型语言模型是零-shot推理者。神经信息处理系统进展，第35卷：22199–22213，2022年。
- en: 'Pan et al. [2024] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap.
    IEEE Transactions on Knowledge and Data Engineering, 2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潘等人 [2024] 潘诗瑞、罗琳豪、王宇飞、陈晨、王佳浦、吴心东。统一大型语言模型和知识图谱：一条路线图。IEEE知识与数据工程汇刊，2024年。
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴等人 [2023] 朴俊成、约瑟夫·欧布莱恩、蔡卡瑞、梅雷迪思·林格尔·莫里斯、佩西·梁和迈克尔·S·伯恩斯坦。生成代理：人类行为的互动模拟体。发表于第36届ACM用户界面软件与技术年度会议论文集，第1–22页，2023年。
- en: Qin et al. [2023a] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng
    Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen,
    Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning
    Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han,
    Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu,
    and Maosong Sun. Tool learning with foundation models, 2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秦等人 [2023a] 祁佳、胡胜丁、林彦凯、陈维泽、丁宁、崔干秋、曾真妮、黄玉飞、肖超军、韩驰、冯易仁、苏玉生、王华东、钱成、田润初、朱坤伦、梁世豪、沈兴宇、许博凯、张振、叶宜宁、李博文、唐紫薇、易晶、朱雨章、戴振宁、闫岚、从鑫、陆雅曦、赵伟霖、黄玉翔、闫俊熙、韩旭、孙贤、李大海、方杰森、杨成、吴同爽、季恒、刘志远和孙茂松。基础模型的工具学习，2023年。
- en: 'Qin et al. [2023b] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating
    large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789,
    2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秦等人 [2023b] 祁佳、梁世豪、叶宜宁、朱坤伦、闫岚、陆雅曦、林彦凯、从鑫、唐向如、钱比尔等。Toolllm：促进大型语言模型掌握16000+实际应用程序接口。arXiv预印本
    arXiv:2307.16789，2023年。
- en: 'Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366,
    2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辛恩等人 [2023] 诺亚·辛恩、贝克·拉巴什和阿什温·戈皮纳斯。Reflexion：一个具有动态记忆和自我反思的自主代理。arXiv预印本 arXiv:2303.11366，2023年。
- en: 'Shridhar et al. [2020] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied
    environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 施里达等人 [2020] 莫希特·施里达、袁兴迪、马克-亚历山大·科特、约纳坦·比斯克、亚当·特里施勒和马修·豪斯克内赫特。Alfworld：将文本与具身环境对齐以实现互动学习。arXiv预印本
    arXiv:2010.03768，2020年。
- en: Sun et al. [2023] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang
    Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. A survey
    of reasoning with foundation models. arXiv preprint arXiv:2312.11562, 2023.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孙等人 [2023] 孙建凯、郑川扬、谢恩泽、刘正英、储瑞航、邱佳宁、徐佳琪、丁明宇、李鸿扬、耿梦哲等。基础模型推理综述。arXiv预印本 arXiv:2312.11562，2023年。
- en: 'Thorne et al. [2018] James Thorne, Andreas Vlachos, Christos Christodoulopoulos,
    and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification.
    arXiv preprint arXiv:1803.05355, 2018.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索恩等人 [2018] 詹姆斯·索恩、安德烈亚斯·弗拉乔斯、克里斯托斯·克里斯托杜洛普洛斯和阿尔皮特·米塔尔。Fever：用于事实提取和验证的大规模数据集。arXiv预印本
    arXiv:1803.05355，2018年。
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale、Dan Bikel、Lukas
    Blecher、Cristian Canton Ferrer、Moya Chen、Guillem Cucurull、David Esiobu、Jude Fernandes、Jeremy
    Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman Goyal、Anthony Hartshorn、Saghar
    Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor Kerkez、Madian Khabsa、Isabel Kloumann、Artem
    Korenev、Punit Singh Koura、Marie-Anne Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai
    Lu、Yuning Mao、Xavier Martinet、Todor Mihaylov、Pushkar Mishra、Igor Molybog、Yixin
    Nie、Andrew Poulton、Jeremy Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan
    Silva、Eric Michael Smith、Ranjan Subramanian、Xiaoqing Ellen Tan、Binh Tang、Ross
    Taylor、Adina Williams、Jian Xiang Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela
    Fan、Melanie Kambadur、Sharan Narang、Aurelien Rodriguez、Robert Stojnic、Sergey Edunov
    和 Thomas Scialom。Llama 2：开放基础和微调聊天模型，2023年。
- en: Wadhwa et al. [2023] Somin Wadhwa, Silvio Amir, and Byron C Wallace. Revisiting
    relation extraction in the era of large language models. arXiv preprint arXiv:2305.05003,
    2023.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wadhwa 等人 [2023] Somin Wadhwa、Silvio Amir 和 Byron C Wallace。重新审视大语言模型时代的关系提取。arXiv
    预印本 arXiv:2305.05003，2023年。
- en: 'Wang et al. [2022a] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj
    Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint
    arXiv:2203.07540, 2022.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022a] Ruoyao Wang、Peter Jansen、Marc-Alexandre Côté 和 Prithviraj Ammanabrolu。Scienceworld：你的代理比五年级学生更聪明吗？arXiv
    预印本 arXiv:2203.07540，2022年。
- en: Wang et al. [2022b] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171,
    2022.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022b] Xuezhi Wang、Jason Wei、Dale Schuurmans、Quoc Le、Ed Chi、Sharan
    Narang、Aakanksha Chowdhery 和 Denny Zhou。自一致性提升语言模型中的思维链推理。arXiv 预印本 arXiv:2203.11171，2022年。
- en: 'Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023a] Guanzhi Wang、Yuqi Xie、Yunfan Jiang、Ajay Mandlekar、Chaowei Xiao、Yuke
    Zhu、Linxi Fan 和 Anima Anandkumar。Voyager：一种开放式的具身智能体，结合大语言模型。arXiv 预印本 arXiv:2305.16291，2023年。
- en: Wang et al. [2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large
    language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023b] Lei Wang、Chen Ma、Xueyang Feng、Zeyu Zhang、Hao Yang、Jingsen Zhang、Zhiyuan
    Chen、Jiakai Tang、Xu Chen、Yankai Lin 等。基于大语言模型的自主智能体调查。arXiv 预印本 arXiv:2308.11432，2023年。
- en: 'Wang et al. [2023c] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091,
    2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023c] Lei Wang、Wanyu Xu、Yihuai Lan、Zhiqiang Hu、Yunshi Lan、Roy Ka-Wei
    Lee 和 Ee-Peng Lim。计划与解决提示：通过大语言模型改善零样本思维链推理。arXiv 预印本 arXiv:2305.04091，2023年。
- en: Wang et al. [2023d] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan,
    Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory.
    arXiv preprint arXiv:2306.07174, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023d] Weizhi Wang、Li Dong、Hao Cheng、Xiaodong Liu、Xifeng Yan、Jianfeng
    Gao 和 Furu Wei。通过长期记忆增强语言模型。arXiv 预印本 arXiv:2306.07174，2023年。
- en: 'Wang et al. [2023e] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. Describe, explain, plan and select: interactive planning
    with llms enables open-world multi-task agents. In Thirty-seventh Conference on
    Neural Information Processing Systems, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023e] Zihao Wang、Shaofei Cai、Guanzhou Chen、Anji Liu、Xiaojian Ma 和
    Yitao Liang。描述、解释、计划和选择：与大型语言模型的互动规划使得开放世界的多任务智能体成为可能。在第37届神经信息处理系统大会，2023年。
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. Advances in Neural Information Processing
    Systems, 35:24824–24837, 2022.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 韦等 [2022] 杰森·韦、薛志·王、戴尔·舒尔曼斯、马滕·博斯玛、费伊·夏、艾德·池、阮维·乐、丹尼·周等。《Chain-of-thought prompting
    elicits reasoning in large language models》。神经信息处理系统进展, 35:24824–24837, 2022。
- en: 'Wu et al. [2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation
    models. arXiv preprint arXiv:2303.04671, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴等 [2023] 陈飞·吴、盛明·尹、韦臻·齐、肖栋·王、泽成·唐和段楠。《Visual chatgpt: Talking, drawing and
    editing with visual foundation models》。arXiv 预印本 arXiv:2303.04671, 2023。'
- en: 'Xi et al. [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential
    of large language model based agents: A survey. arXiv preprint arXiv:2309.07864,
    2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '西等 [2023] 志恒·西、文翔·陈、辛·郭、韦·赫、怡文·丁、博洋·洪、铭·张、军哲·王、森杰·金、恩宇·周等。《The rise and potential
    of large language model based agents: A survey》。arXiv 预印本 arXiv:2309.07864, 2023。'
- en: 'Yang et al. [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W
    Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for
    diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600,
    2018.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '杨等 [2018] 直林·杨、彭奇、塞曾·张、约书亚·本吉奥、威廉·W·科恩、鲁斯兰·萨拉赫丁诺夫和克里斯托弗·D·曼宁。《Hotpotqa: A dataset
    for diverse, explainable multi-hop question answering》。arXiv 预印本 arXiv:1809.09600,
    2018。'
- en: 'Yang et al. [2023] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel,
    and Dale Schuurmans. Foundation models for decision making: Problems, methods,
    and opportunities. arXiv preprint arXiv:2303.04129, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '杨等 [2023] 谢瑞·杨、奥菲尔·纳赫姆、伊伦·杜、杰森·韦、彼得·阿贝尔和戴尔·舒尔曼斯。《Foundation models for decision
    making: Problems, methods, and opportunities》。arXiv 预印本 arXiv:2303.04129, 2023。'
- en: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. arXiv preprint arXiv:2210.03629, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '姚等 [2022] 书云·姚、杰弗里·赵、滕远·于、段楠、伊扎克·沙夫兰、卡尔提克·纳拉辛汉和袁超。《React: Synergizing reasoning
    and acting in language models》。arXiv 预印本 arXiv:2210.03629, 2022。'
- en: 'Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. arXiv preprint arXiv:2305.10601, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '姚等 [2023] 书云·姚、滕远·于、杰弗里·赵、伊扎克·沙夫兰、托马斯·L·格里菲斯、袁超和卡尔提克·纳拉辛汉。《Tree of thoughts:
    Deliberate problem solving with large language models》。arXiv 预印本 arXiv:2305.10601,
    2023。'
- en: Zhang et al. [2023] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan
    Zhao, and Kai Yu. Large language model is semi-parametric reinforcement learning
    agent. arXiv preprint arXiv:2306.07929, 2023.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等 [2023] 丹阳·张、陆·陈、司徒·张、洪深·徐、自涵·赵和凯·余。《Large language model is semi-parametric
    reinforcement learning agent》。arXiv 预印本 arXiv:2306.07929, 2023。
- en: Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等 [2023] 韦恩·辛·赵、昆·周、俊毅·李、天一·唐、晓磊·王、玉鹏·侯、颖倩·闵、北辰·张、俊杰·张、子灿·董等。《A survey of large
    language models》。arXiv 预印本 arXiv:2303.18223, 2023。
