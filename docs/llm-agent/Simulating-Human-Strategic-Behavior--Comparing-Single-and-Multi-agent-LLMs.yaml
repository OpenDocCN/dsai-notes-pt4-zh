- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟人类战略行为：比较单一和多代理LLMs
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.08189](https://ar5iv.labs.arxiv.org/html/2402.08189)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.08189](https://ar5iv.labs.arxiv.org/html/2402.08189)
- en: Karthik Sreedhar [ks4190@columbia.edu](mailto:ks4190@columbia.edu)  and  Lydia
    Chilton [lc3251@columbia.edu](mailto:lc3251@columbia.edu) Columbia UniversityUSA
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Karthik Sreedhar [ks4190@columbia.edu](mailto:ks4190@columbia.edu) 和 Lydia Chilton
    [lc3251@columbia.edu](mailto:lc3251@columbia.edu) 哥伦比亚大学 美国
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'When creating plans, policies, or applications for people, it is challenging
    for designers to think through the strategic ways that different people will behave.
    Recently, Large Language Models (LLMs) have been shown to create realistic simulations
    of human-like behavior based on personas. We build on this to investigate whether
    LLMs can simulate human strategic behavior: Human strategies are complex because
    they take into account social norms in addition to aiming to maximize personal
    gain. The ultimatum game is a classic economics experiment used to understand
    human strategic behavior in a social setting. It shows that people will often
    choose to “punish” other players to enforce social norms rather than to maximize
    personal profits. We test whether LLMs can replicate this complex behavior in
    simulations. We compare two architectures: single- and multi-agent LLMs. We compare
    their abilities to (1) simulate human-like actions in the ultimatum game, (2)
    simulate two player personalities, greedy and fair, and (3) create robust strategies
    that are logically complete and consistent with personality. Our evaluation shows
    the multi-agent architecture is much more accurate than single LLMs (88% vs. 50%)
    in simulating human strategy creation and actions for personality pairs. Thus
    there is potential to use LLMs to simulate human strategic behavior to help designers,
    planners, and policymakers perform preliminary exploration of how people behave
    in systems.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在为人们制定计划、政策或应用程序时，设计师很难考虑不同人群的战略行为方式。最近，大型语言模型（LLMs）被证明能够基于角色创建逼真的人类行为模拟。我们在此基础上研究LLMs是否能够模拟人类的战略行为：人类策略复杂，因为它们除了追求个人利益外，还考虑社会规范。**终极博弈**是一个经典的经济学实验，用于理解人类在社会环境中的战略行为。它表明，人们往往会选择“惩罚”其他玩家以执行社会规范，而不是仅仅追求个人利润。我们测试LLMs是否能在模拟中复制这种复杂行为。我们比较了两种架构：单一代理和多代理LLMs。我们比较了它们在以下方面的能力：(1)
    在**终极博弈**中模拟类似人类的行为，(2) 模拟两种玩家个性，即贪婪和公平，以及(3) 创建逻辑上完整且与个性一致的稳健策略。我们的评估显示，多代理架构在模拟人类策略创建和个性对的行为方面，比单一LLM准确得多（88%
    对 50%）。因此，使用LLMs模拟人类战略行为具有潜力，可以帮助设计师、规划者和政策制定者进行初步探索，以了解人们在系统中的行为。
- en: '^†^†booktitle:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†书名：
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Simulations help us design the world. Whether designing earthquake-safe buildings,
    evaluation plans, economic policies, or even a late assignment policy in a class,
    it is useful for designers to be able to simulate the effects of their design
    as a heuristic to guide the process. Although physical simulation has come a long
    way, simulating human behavior is notoriously difficult. When economists model
    human behavior, they assume that people are rational actors, but psychology has
    discovered many important ways in which humans are not rational or strictly profit
    maximizing (Kahneman, [2012](#bib.bib13); Ariely, [2008](#bib.bib4)). Moreover,
    people do not behave uniformly – their personalities (McCrae and Costa, [2008](#bib.bib17)),
    experiences (Kidd et al., [2013](#bib.bib14)) and circumstances (Mullainathan
    and Shafir, [2013](#bib.bib18)) affect their decision making. Another complexity
    is that people often act strategically - to reason about other people and base
    their actions on it (as chess players do). This makes it very mentally demanding
    for a designer to think through all the possibilities of what people would do
    in response to a new design or policy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟帮助我们设计世界。无论是设计抗震建筑、评估计划、经济政策，还是课堂中的迟交作业政策，设计师能够模拟设计效果作为启发式指导过程都非常有用。虽然物理模拟已经取得了长足进展，但模拟人类行为却
    notoriously 难。经济学家在建模人类行为时，假设人们是理性的行为者，但心理学发现了许多人类不理性或并非严格追求利润最大化的重要方式（Kahneman,
    [2012](#bib.bib13); Ariely, [2008](#bib.bib4)）。此外，人们的行为并不统一——他们的个性（McCrae and
    Costa, [2008](#bib.bib17)）、经历（Kidd et al., [2013](#bib.bib14)）和环境（Mullainathan
    and Shafir, [2013](#bib.bib18)）影响他们的决策。另一个复杂性是，人们常常采取战略行动——推测他人的想法并据此调整自己的行为（如同棋手所做）。这使得设计师在思考人们对新设计或政策的反应时，心理负担非常大。
- en: Recently, LLMs have been shown to be able to simulate plausible human behavior
    based on personas. This includes modeling the opinions of supreme court justices
    in past rulings (Hamilton, [2023](#bib.bib8)), simulating a fictional town’s ability
    to plan and attend events like a party (Park et al., [2023](#bib.bib20)), and
    simulating human behavior in classic economic and psychology experiments (Aher
    et al., [2023](#bib.bib2)). We build on this to investigate whether LLMs can simulate
    human strategic behavior. To evaluate not just plausible human behavior but also
    behavior that accurately reflects what people do, we compare LLM simulations to
    experimental baselines taken from literature on studies of human strategic behavior.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，LLMs 被证明能够基于角色模拟出可信的人类行为。这包括模拟最高法院法官在过去裁决中的意见（Hamilton, [2023](#bib.bib8)）、模拟虚构小镇规划和参加如派对等活动的能力（Park
    et al., [2023](#bib.bib20)），以及模拟经典经济学和心理学实验中的人类行为（Aher et al., [2023](#bib.bib2)）。我们在此基础上调查
    LLMs 是否能模拟人类的战略行为。为了评估不仅仅是可信的人类行为，还要评估准确反映人们行为的情况，我们将 LLM 模拟与文献中关于人类战略行为的实验基线进行比较。
- en: 'The ultimatum game is a classic economics experiment used to understand human
    social strategic behavior. It also captures human’s social behavior (often deemed
    irrational, such as the desire to “punish” unfair actors) and personality differences
    (greedy and fair). In the ultimatum game, there are two players: a proposer and
    a receiver. The proposer is given an amount of money, such as $1, and is tasked
    with offering a portion of the amount to the receiver. The receiver can either
    accept or reject the offer - if they accept, the players divide the amount as
    proposed. If they reject, both players receive nothing. Economic theory dictates
    that a profit-maximizing proposer should offer only $0.01 (the smallest nonzero
    amount) and keep $0.99, and that the receiver should accept it because $0.01 is
    more than the receiver would have otherwise. However, experiments with human subjects
    show that humans do not act in a purely “rational” manner; receivers will reject
    a low offer to punish proposers for offering an unfair split (Krawczyk, [2018](#bib.bib15)).
    Moreover, proposers are aware of this, and thus strategically make offers that
    are closer to fair - especially after multiple rounds of playing the game.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最终通牒游戏是一个经典的经济学实验，用于理解人类的社会战略行为。它还捕捉了人类的社会行为（通常被认为是不理性的，比如“惩罚”不公平行为者的愿望）和个性差异（贪婪和公平）。在最终通牒游戏中，有两个玩家：提议者和接收者。提议者获得一笔钱，比如1美元，并负责将这笔钱的一部分提供给接收者。接收者可以选择接受或拒绝这个提议——如果接受，玩家们将按提议分配金额。如果拒绝，双方都不会获得任何东西。经济理论规定，利润最大化的提议者应该只提供0.01美元（最小的非零金额），并保留0.99美元，接收者应该接受，因为0.01美元比接收者本来获得的要多。然而，对人类受试者的实验表明，人类不会以纯粹“理性”的方式行动；接收者会拒绝低提议，以惩罚提议者提供不公平的分配（Krawczyk，[2018](#bib.bib15)）。此外，提议者也意识到这一点，因此在多轮游戏后，战略性地提出更接近公平的提议。
- en: 'We use the ultimatum game to test whether LLMs can simulate the strategic,
    social, and personality aspects of human players. We extract human gameplay actions
    (offers and accept/reject decisions) from economics literature (Houser and McCabe,
    [2014](#bib.bib12)) and evaluate whether LLMs can simulate human behavior in the
    ultimatum game with 5 rounds. When the game is played for multiple rounds, both
    players have the opportunity to adjust their actions in response to the actions
    of the other player. We compare two LLM structures: a single LLM and a multi-agent
    LLM architecture. We compare their abilities to (1) create realistic strategies,
    (2) adhere to created strategies, and (3) accurately model two different player
    personalities: greedy and fair. The single LLM structure involves prompting GPT4
    directly, while the multi-agent LLM architecture is adapted from recent literature
    (Park et al., [2023](#bib.bib20)). In the single-LLM structure, GPT4 is directly
    prompted to simulate the actions of both a proposer and receiver over five rounds
    of the ultimatum game. In the multi-agent LLM architecture, each player is represented
    by a separate GPT4 agent. Each player is tasked with playing the ultimatum game
    with the other, with information such as personality being private to the agent.
    In both conditions, the LLM is tasked with creating a strategy based on a given
    personality and to play the game according to their personality and strategy.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用最终通牒游戏来测试LLMs是否能够模拟人类玩家的战略、社会和个性方面。我们从经济学文献中提取人类游戏行为（提议和接受/拒绝决定）（Houser
    和 McCabe，[2014](#bib.bib12)），并评估LLMs是否能够在5轮最终通牒游戏中模拟人类行为。当游戏进行多轮时，双方玩家都有机会根据对方的行动调整自己的行动。我们比较了两种LLM结构：单一LLM和多智能体LLM架构。我们比较了它们在以下方面的能力：(1)
    创建现实的策略，(2) 遵守已创建的策略，以及(3) 准确模拟两种不同玩家个性：贪婪和公平。单一LLM结构涉及直接提示GPT4，而多智能体LLM架构则改编自最近的文献（Park等，[2023](#bib.bib20)）。在单一LLM结构中，GPT4被直接提示以模拟最终通牒游戏中提议者和接收者的行动，共进行五轮。在多智能体LLM架构中，每个玩家由一个单独的GPT4代理表示。每个玩家负责与另一个玩家进行最终通牒游戏，其中个性等信息对代理是私密的。在这两种条件下，LLM的任务是根据给定的个性创建策略，并根据其个性和策略进行游戏。
- en: Our evaluation shows that the multi-agent LLM architecture is much more accurate
    than using a single LLM to simulate strategic behavior in the ultimatum game.
    Over 40 simulations, the Multi-agent structure was consistent with human behavior
    88% of the time, and the single LLM was on consistent 50% of the time. There are
    three reasons for for LLM simulations to be inconsistent with human behavior,
    (1) a created strategy is incomplete, (2) a created strategy is inconsistent with
    the specified personality, or (3) a player deviates from the created strategy
    during game play. We find that over 90% of issues in single LLM simulations are
    caused by the LLMs strategy rather than their gameplay. Incomplete strategies
    or inconsistent personality strategies, with both categories accounting for a
    roughly equal amount of errors. There is only 1 of 40 simulations in which an
    error is caused by a player not adhering to the created strategies. In the multi-agent
    LLM architecture, the most common issue is strategies inconsistent with personality,
    accounting for more than 85% of errors
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估显示，多智能体 LLM 架构在模拟最后通牒游戏中的战略行为时比单一 LLM 更准确。在40次模拟中，多智能体结构与人类行为一致的比例为88%，而单一
    LLM 仅为50%。LLM 模拟与人类行为不一致的原因有三种：(1) 创建的策略不完整，(2) 创建的策略与指定的人格不一致，或 (3) 玩家在游戏过程中偏离了创建的策略。我们发现，单一
    LLM 模拟中超过90%的问题是由 LLM 的策略造成的，而不是其游戏玩法。不完整的策略或不一致的人格策略，占错误的比例大致相等。40次模拟中只有1次错误是由于玩家未遵守创建的策略。在多智能体
    LLM 架构中，最常见的问题是策略与人格不一致，占错误的比例超过85%。
- en: Based on these results, we believe that multi-agent LLMs show potential to simulate
    more than just plausible human behavior, but behavior consistent with experimental
    evidence in complex scenarios. They might one day become a tool for designs to
    evaluate plans, policies, and interfaces.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些结果，我们认为多智能体 LLM 展示了模拟不仅是合理的人类行为，还能在复杂场景中与实验证据一致的行为的潜力。它们有可能成为设计评估计划、政策和接口的工具。
- en: 2\. Related Work
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: 2.1\. Ultimatum Game Background and Experiments
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. **最后通牒游戏**背景和实验
- en: Prior experiments with human subjects have revealed that humans often reject
    low offers when playing the ultimatum game, despite economic theory dictating
    the receiver to accept any positive offer. Houser and McCabe (2014) (Houser and
    McCabe, [2014](#bib.bib12)) finds that proposers most commonly offer 40% or 50%
    of the total amount to the receiver, with the receiver almost always accepting.
    However, the acceptance rate falls to 50% when offers are 20% of the total amount,
    and falls further for 10% and lower offers. Krawzcyk (2018) (Krawczyk, [2018](#bib.bib15))
    presents similar findings, recording that receivers reject offers that are 10%
    of the total amount nearly 90% of the time. The study also notes that in variations
    of the game that involve multiple rounds, it is a sensible strategy for receivers
    to reject low offers to drive up future offers in following rounds.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 先前对人类受试者的实验揭示了人类在玩最后通牒游戏时常常拒绝低提议，尽管经济理论规定接收者应接受任何正数提议。Houser 和 McCabe (2014)（Houser
    and McCabe, [2014](#bib.bib12)）发现，提议者最常向接收者提供40%或50%的总金额，接收者几乎总是接受。然而，当提议为总金额的20%时，接受率降至50%，对于10%及更低的提议则进一步下降。Krawzcyk
    (2018)（Krawczyk, [2018](#bib.bib15)）提供了类似的发现，记录到接收者几乎在90%的情况下拒绝10%总金额的提议。研究还指出，在涉及多轮次的游戏变体中，接收者拒绝低提议以推动未来轮次的提议增加是一种合理的策略。
- en: Previous work has shown that there are significant impacts of introducing personality
    traits and multiple rounds into the ultimatum game that cause human players to
    act differently from expected economic theory. Brandstätter and Königstein (2001)
    (Königstein, [2001](#bib.bib16)) shows that proposers that demonstrate personality
    traits consistent with selfishness make skewed offers in their favor that highlight
    a difference between economic rationality and equity. Similarly, receivers that
    have personality traits consistent with fairness can use rejection as a form of
    retaliation to “punish” proposers that egregiously violate social norms of equity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究表明，引入人格特征和多轮次的最后通牒游戏会显著影响人类玩家，使其行为与预期的经济理论不同。Brandstätter 和 Königstein
    (2001)（Königstein, [2001](#bib.bib16)）显示，表现出与自私一致的人格特征的提议者会提出偏向自己的不公平提议，突出经济理性与公平之间的差异。类似地，具有公平一致的人格特征的接收者可以利用拒绝作为一种报复方式来“惩罚”严重违反社会公平规范的提议者。
- en: Vavra et. al. (2018) (Vavra et al., [2018](#bib.bib21)) uses the ultimatum game
    to more broadly study fairness, aiming to explain why receivers reject non-zero
    offers that are unfair, such as 10% of the total amount, as a way to punish the
    proposer despite both players ending in a worse outcome than an acceptance. They
    find that the canonical explanation (players value equal outcomes over personal
    games) is not sufficient to understand the phenomenon, and that there is abundant
    evidence that a decision to reject an offer can be influenced by contextual factors
    (such as multiple rounds) or based on the difference between the actual and expected
    offer (the latter of which can be heavily influenced by a greedy or fair personality
    label placed on the receiver).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Vavra 等人（2018）（Vavra et al., [2018](#bib.bib21)）利用最后通牒游戏更广泛地研究公平性，旨在解释为什么接收者会拒绝不公平的非零提议，比如总金额的
    10%，作为惩罚提议者的方式，即使这样做使得双方的结果比接受提议时更糟。他们发现经典解释（玩家重视平等结果而非个人利益）不足以理解这种现象，且有大量证据表明拒绝提议的决定可能受上下文因素（如多轮游戏）或实际与预期提议之间的差异的影响（后者可能受到对接收者的贪婪或公平性标签的影响）。
- en: Experiments also show humans from societies of different modernization levels
    act slightly differently when playing the ultimatum game. Henrich (2000) (Henrich,
    [2000](#bib.bib9)) found that proposers from the Machiguenga indigenous people
    of the Peruvian Amazon made much lower offers to receivers in games of all stakes
    than their western counterparts. Meanwhile, Alvard (2004) (Alvard, [2004](#bib.bib3))
    found that results from big-game hunting populations in Indonesia were similar
    to western societies, but had a higher occurrence of “hyper-fairness,” the direct
    opposite of what was observed in Peru. Tracer (2004) found that results from indigenous
    populations in Papa New Guinea indicated proposers were greedier than in western
    society, but not to the extent of the Machiguenga. The variations in results are
    attributed to tribal relationships and customs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实验还表明，来自不同现代化水平社会的人在进行最后通牒游戏时表现略有不同。Henrich（2000）（Henrich, [2000](#bib.bib9)）发现，来自秘鲁亚马逊地区的Machiguenga土著人民在所有赌注的游戏中向接收者提供的提议远低于西方同行。同时，Alvard（2004）（Alvard,
    [2004](#bib.bib3)）发现，印尼的大型猎物狩猎群体的结果与西方社会类似，但“超公平”现象的发生率更高，这与在秘鲁观察到的情况正好相反。Tracer（2004）发现，来自巴布亚新几内亚土著群体的结果表明提议者比西方社会更贪婪，但不及Machiguenga。结果的差异归因于部落关系和习俗。
- en: 2.2\. LLM Reasoning
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. LLM 推理
- en: Previous work has also shown that LLMs are more accurate when asked to create
    and explain a thought process before acting. Wei et. al. (2023) (Wei et al., [2022](#bib.bib22))
    shows that requiring LLMs with more than one billion parameters to explain intermediate
    reasoning steps improves performance. They evaluate chain-of-thought prompting
    against standard prompting on a database of grade school math word problems, and
    find that chain-of-though prompting has a solve rate (57%) more than two and a
    half times that of standard-prompting (18%). Zheng et. al. (2023) (Zheng et al.,
    [2023](#bib.bib23)) introduces progressive-hint prompting, which involves users
    inputting previous provided answers back to LLMs as hints to guide them toward
    the correct answer. They find that the average accuracy across problem types increases
    by 20% when using progressive-hint-prompting compared to standard prompting with
    text-davinci-003\. Thus, there is reason to believe that prompting GPT to create
    strategies before simulating the ultimatum game will lead to more accurate outcomes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究还表明，LLM在被要求创建和解释思维过程后，准确性更高。Wei 等人（2023）（Wei et al., [2022](#bib.bib22)）显示，要求具有超过十亿参数的LLM解释中间推理步骤能提高性能。他们对比了链式思维提示和标准提示在小学数学词汇问题数据库上的表现，发现链式思维提示的解决率（57%）比标准提示（18%）高出两倍多。Zheng
    等人（2023）（Zheng et al., [2023](#bib.bib23)）介绍了渐进提示，这涉及用户将之前提供的答案作为提示输入LLM，以引导其找到正确答案。他们发现，与标准提示相比，使用渐进提示时，各类问题的平均准确率提高了
    20%。因此，有理由相信，提示 GPT 在模拟最后通牒游戏前创建策略将带来更准确的结果。
- en: 2.3\. Using LLMs to Simulate Strategic Behavior
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 使用 LLM 模拟战略行为
- en: 'Previous research has studied to what degree GPT can simulate human behavior
    in simple behavioral experiments to mixed results. Aher et. al. (2023) (Aher et al.,
    [2023](#bib.bib2)) introduces a new test, called a Turing Experiment, to evaluate
    the extent to which LLMs can simulate aspects of human behavior via classic human
    behavior experiments: ultimatum game, garden path sentences, milgram shock experiment,
    and wisdom of crowds. They compare ultimatum game results from text-davinci-002
    simulations to those from past human studies and find that three out of the four
    results from human studies fall nearly on the trend line created by simulations
    from the LLM, with the fourth deviating by less than 10%. Results from the next
    two experiments also closely model human trends, although the fourth reveals a
    “hyper-accuracy-distortion” present in LLMs. While the evidence is encouraging
    in supporting that LLMs can replicate human behavior to a reasonable extent, the
    work uses a now outdated model. In the ultimatum game specifically, the work only
    tests the simplest case – there is no exploration of multiple round games or personality
    traits.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究探讨了GPT在简单行为实验中模拟人类行为的程度，并得出了不一致的结果。Aher等人（2023）（Aher et al., [2023](#bib.bib2)）提出了一种新的测试方法，称为图灵实验，以评估LLMs在经典人类行为实验中模拟人类行为方面的能力：**最后通牒游戏**、**歧义句**、**米尔格伦电击实验**和**群体智慧**。他们将文本-davinci-002模拟的最后通牒游戏结果与过去人类研究的结果进行比较，发现四项人类研究结果中的三项几乎符合LLM模拟所创建的趋势线，第四项的偏差小于10%。接下来的两个实验的结果也紧密地模拟了人类趋势，尽管第四项揭示了LLMs中存在的“超精确失真”。尽管证据令人鼓舞，支持LLMs在合理范围内能够复制人类行为，但该研究使用了一个现在已过时的模型。在最后通牒游戏中，该研究仅测试了最简单的情况——没有对多轮游戏或个性特征进行探索。
- en: Horton (2023) (Horton, [2023](#bib.bib11)) builds on this work by using text-davinci-003
    and proposes that LLMs can be used like economists use homo economicus, as representations
    of humans in economic scenarios. The methodology demonstrates that GPT can be
    given information and preferences and that their behavior can then be explored
    in scenarios via simulation. Results in several different economic experiments
    shows that text-davinci-003 will change its behavior mid-simulation, but that
    there is room for improvement – they conclude that text-davinci-003 is most suited
    to be used as a “toy model,” or a tool not meant to reflect reality but rather
    to help a human experimenter think. The results suggest that while there is reason
    for further exploration, LLMs could not yet be fully trusted to emulate human
    behavior in economic games.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Horton（2023）（Horton, [2023](#bib.bib11)）在此基础上使用了文本-davinci-003，并提出LLMs可以像经济学家使用**经济人**一样，用作经济场景中的人类代表。这种方法展示了GPT可以被赋予信息和偏好，然后通过模拟在不同场景中探索其行为。多个经济实验中的结果表明，文本-davinci-003在模拟过程中会改变其行为，但仍有改进空间——他们得出结论，文本-davinci-003最适合用作“玩具模型”，即一个不旨在反映现实的工具，而是帮助人类实验者进行思考。结果表明，虽然进一步探索是有理由的，但LLMs尚未完全可信于模拟经济游戏中的人类行为。
- en: 2.4\. Multi-Agent Paradigms of LLMs
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. LLM的多智能体范式
- en: 'Previous studies of GPT’s ability to simulate economic games have primarily
    used a single LLM, but new agent-based architecture has potential to simulate
    human behavior more effectively. Park et. al. (2023) (Park et al., [2023](#bib.bib20))
    introduces a LLM agent architecture that allows for multiple humans to be represented
    by multiple LLMs that can interact with one another. The architecture includes
    five components: self-knowledge, memory, plans, reactions, and reflections, and
    is evaluated using an effective Bayesian rating system called TrueSkill (introduced
    in Herbrich et. al. (2006) (Herbrich et al., [2006](#bib.bib10))). They also evaluate
    the agent architecture by populating an interactive sandbox to explore specific
    human scenarios such as planning a Valentine’s Day party or surprise birthday
    party at the office. They find that agents demonstrate reasonable individual and
    emergent social behaviors, but leave room for exploration into concrete scenarios
    like the ultimatum game which require more than “reasonable” performance and have
    a well-defined expected result.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 先前关于GPT模拟经济游戏能力的研究主要使用了单一LLM，但新的基于代理的架构有潜力更有效地模拟人类行为。Park等人（2023）（Park et al.,
    [2023](#bib.bib20)）介绍了一种LLM代理架构，允许多个LLMs代表多个真人并相互互动。该架构包括五个组件：自我知识、记忆、计划、反应和反思，并使用一种称为TrueSkill的有效贝叶斯评分系统进行评估（在Herbrich等人（2006）（Herbrich
    et al., [2006](#bib.bib10)）中介绍）。他们还通过填充一个互动沙盒来评估该代理架构，以探索具体的人类场景，例如策划情人节派对或办公室惊喜生日派对。他们发现，代理展示了合理的个体和突现社会行为，但对于像最终通牒游戏这样的具体场景还需要进一步探索，这些场景要求的不仅仅是“合理”的表现，而且有明确的预期结果。
- en: Hamilton (2023) (Hamilton, [2023](#bib.bib8)) also introduces a multi-agent
    system and uses it to simulate judicial rulings of the supreme court of the United
    States from 2010 to 2016\. Nine separate GPT-2 models are trained with authored
    opinions from each of the nine supreme court justices, all of which achieved greater
    than 50% accuracy for predicting the justice’s rulings and achieved better-than-random
    accuracy in predicting overall decisions of the actual supreme court. While encouraging,
    these results also demonstrate room for improvement and the need for exploration
    into the matter with the latest GPT models, as 50% accuracy is random for a binary
    decision, meaning in both metrics examined, the results could not confidently
    state that outcomes are generally accurate but rather only that there is better-than-random
    performance, which is a low threshold. A 2023 preprint (Guo, [2023](#bib.bib7))
    uses GPT Agents to simulate the ultimatum game with two personalities. They look
    at average offers and acceptance rates of rounds simulated, but do not compare
    results to human baselines. They prompt agents to create strategies but do not
    analyze the strategies’ consistency with human behaviors.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Hamilton（2023）（Hamilton, [2023](#bib.bib8)）还介绍了一种多智能体系统，并用它来模拟2010年至2016年美国最高法院的司法裁决。九个单独的GPT-2模型以九位最高法院法官撰写的意见进行训练，所有模型在预测法官裁决时的准确率都超过了50%，并在预测实际最高法院的总体决策时取得了比随机更好的准确率。虽然这些结果令人鼓舞，但也显示出改进的空间，并需要用最新的GPT模型进一步探索，因为50%的准确率在二元决策中属于随机水平，这意味着在两个检查的指标中，结果无法自信地声明结果通常准确，而只是表现出比随机更好的性能，这是一个较低的门槛。2023年的预印本（Guo,
    [2023](#bib.bib7)）使用GPT代理来模拟具有两种个性的最终通牒游戏。他们查看了模拟轮次的平均报价和接受率，但没有将结果与人类基线进行比较。他们提示代理制定策略，但没有分析这些策略与人类行为的一致性。
- en: 3\. Experimental Set-Up
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 实验设置
- en: 'To test the ability of LLMs to simulate human-like behavior in the ultimatum
    game, we ran simulations of the five-round ultimatum game where LLMs were tasked
    with creating strategies and then playing the game. We did this with two different
    architectures: a single LLM and a multi-agent LLM architecture. With the single
    LLM, we directly prompt an LLM to create strategies for both players and simulate
    the actions of both players acting in accordance with the previously created strategies.
    For the multi-agent LLM architecture, we adapted recently introduced architectures
    with each player being represented by a separate LLM agent.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为测试LLMs在最终通牒游戏中模拟人类行为的能力，我们进行了五轮最终通牒游戏的模拟，其中LLMs负责制定策略并参与游戏。我们使用了两种不同的架构：单一LLM和多智能体LLM架构。对于单一LLM，我们直接提示LLM为两个玩家制定策略，并模拟两个玩家按照之前制定的策略行动。对于多智能体LLM架构，我们采用了最近引入的架构，每个玩家由一个单独的LLM代理表示。
- en: We also tested two personality types, greedy and fair, with the expectation
    that created strategies would be different for different personality types, also
    resulting in different progressions of gameplay towards ultimately reaching an
    equal split. For instance, we expected the initial offer in a fair-fair simulation
    to be an even ($0.50) or close-to-even split and to be accepted, whereas we expected
    the initial offer in a greedy-fair simulation to be skewed in favor of the proposer
    and likely be rejected. In total, we ran forty simulations each for both the single
    LLM and multi-agent LLM architectures (ten simulations for each personality pairing).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还测试了两种人格类型：贪婪型和公平型，期望不同人格类型的策略会有所不同，从而导致游戏进程不同，最终达成平等分配。例如，我们预期在公平-公平的模拟中，初始报价将是均等的（$0.50）或接近均等的分配并被接受，而在贪婪-公平的模拟中，初始报价将偏向于提议者并可能被拒绝。总的来说，我们对单一LLM和多代理LLM架构分别进行了四十次模拟（每种人格配对十次模拟）。
- en: For all experiments, we use Open AI’s GPT. We tested both GPT-3.5 and GPT-4,
    the two most recent large language models released by OpenAI. Specifically, we
    used the gpt-3.5-turbo and gpt-4-1106-preview models, the latter of which was
    the first version of GPT-4 released. GPT-4 has been shown to have the ability
    to interpret inherently human concepts such as equity and also scores well on
    a variety of standardized tests, ranging from the Bar Exam to the GRE (OpenAI
    et al., [2023](#bib.bib19)). For both versions of GPT, we set temperature and
    top P parameters at 0.5, and did not add any frequency or presence penalties.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有实验，我们使用了Open AI的GPT。我们测试了GPT-3.5和GPT-4，这两个是OpenAI发布的最新大型语言模型。具体来说，我们使用了gpt-3.5-turbo和gpt-4-1106-preview模型，其中后者是GPT-4发布的第一个版本。GPT-4已被证明能够解释如公平等本质上属于人类的概念，并且在各种标准化测试中表现良好，从律师考试到GRE（OpenAI
    et al., [2023](#bib.bib19)）。对于这两个版本的GPT，我们将温度和top P参数设置为0.5，并且没有添加任何频率或存在惩罚。
- en: 3.1\. Research Questions
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 研究问题
- en: 'In the following experiment section, we specifically address the following
    research questions:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下实验部分，我们专门解决了以下研究问题：
- en: RQ1\. Which LLM architecture more accurately simulates human-like actions in
    the five-round ultimatum game?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1\. 哪种LLM架构更准确地模拟了五轮最后通牒游戏中的类人行为？
- en: RQ2\. Which LLM architecture more accurately simulates the actions of player
    personalities?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2\. 哪种LLM架构更准确地模拟了玩家个性的行为？
- en: 'RQ3\. Which LLM architecture more often creates robust strategies: both logically
    complete and consistent with personality?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3\. 哪种LLM架构更常创造出稳健的策略：既在逻辑上完整又与个性一致？
- en: Single LLMs and multi-agent LLMs have different potential advantages in simulations.
    In Single LLMs, the LLM has the full context of all the players’ personalities
    and actions and could potentially orchestrate strategies and gameplay to achieve
    a sensible outcome. Global context could allow for a more coherent narrative in
    the gameplay. Multi-agent LLMs lack the global context of all the agents behavior,
    as agents can only exchange information through the act of speaking. Although
    multi-agent LLMs allow the LLM to focus on one agent at a time, they might not
    expose enough information for agents to make reasonable choices.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 单一LLM和多代理LLM在模拟中具有不同的潜在优势。在单一LLM中，LLM拥有所有玩家个性和行动的完整上下文，可以潜在地策划策略和游戏玩法以实现合理的结果。全球上下文可以使游戏玩法中的叙事更连贯。多代理LLM缺乏所有代理行为的全球上下文，因为代理只能通过言语交流信息。尽管多代理LLM允许LLM一次专注于一个代理，但它们可能没有提供足够的信息使代理做出合理的选择。
- en: For all three research questions, we predict that the multi-agent LLM architecture
    will perform better because each agent acting independently will better model
    interactions between multiple humans and more closely resemble the anonymous conditions
    of the ultimatum game when ran with human subjects. The multi-agent LLM architecture
    also allows for each agent to focus only on creating strategies and simulating
    strategies of one human, as opposed to a single LLM having to track the strategy
    of each player and act accordingly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有三个研究问题，我们预测多代理LLM架构将表现更好，因为每个代理独立行动将更好地模拟多个人类之间的互动，并更接近于在与人类受试者进行最后通牒游戏时的匿名条件。多代理LLM架构还允许每个代理仅专注于创建策略和模拟一个人类的策略，而不是单一LLM需要跟踪每个玩家的策略并相应地行动。
- en: 3.2\. Single LLM and Multi-Agent Architecture
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 单一LLM与多代理架构
- en: 3.2.1\. Inputs
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 输入
- en: 'For the single-LLM structure, we prompt an LLM to 1) create and display strategies
    for both players of the five-round ultimatum game, and then 2) simulate with both
    players acting in accordance with the previously created strategies for 5 rounds.
    The prompt for two fair players is as follows: “Create a strategy for a fair proposer
    and a fair receiver in playing the ultimatum game five times with $1\. Once the
    strategies are created, simulate five rounds of the ultimatum game with the proposer
    and the receiver adhering to the previously outlined strategies.” The prompts
    for other personality pairings are virtually identical, with only the personality
    descriptor ahead of each player in the first sentence being changed. In early
    pilots of the prompt, we discovered that we did not need to explain the ultimatum
    game or the notion of a proposer or receiver. The LLM had the context to correctly
    infer the set-up. From the LLMs response to this prompt, we extract the created
    strategies for both players as well as the offers made by the proposer and the
    receiver’s response (accept or reject) in each of the five rounds.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单一 LLM 结构，我们提示 LLM 1) 为五轮最终通牒游戏的两个玩家创建并展示策略，然后 2) 进行模拟，让两个玩家根据之前创建的策略进行 5
    轮游戏。两个公平玩家的提示如下：“为一个公平的提议者和一个公平的接收者在进行五轮 $1\. 的最终通牒游戏时创建策略。一旦策略创建完成，模拟五轮最终通牒游戏，提议者和接收者遵循之前概述的策略。”其他个性配对的提示几乎相同，只是在第一句中每个玩家前面的个性描述有所更改。在提示的早期试验中，我们发现不需要解释最终通牒游戏或提议者和接收者的概念。LLM
    能够正确推断设置。从 LLM 对此提示的回应中，我们提取出两个玩家的创建策略以及提议者所做的提议和接收者在每轮中的响应（接受或拒绝）。
- en: For the multi-agent LLM architecture we adapt the agent architecture introduced
    to simulate a town and its inhabitants called Smallville (Park et al., [2023](#bib.bib20)).
    In this architecture, the administrator provides each agent with a name, public
    and private biographies, instructions (called directives), and an initial plan,
    which consists of a description, stop condition, and a location. For our experiments,
    we name each agent as per their role in the game (Proposer or Receiver) and set
    the initial plan for each player to be to create a strategy for the five-round
    ultimatum game as per their role and then store it in memory. The ultimatum game
    does not need players to move around, or the concept of location, so the agents
    are set to a single location called “Default.” In the private biography, we specify
    the personality trait of each player (“Proposer is greedy.”). This tells the agent
    their personality without informing the other agent. The public biography for
    both agents is left blank, since in experiments with human subjects of the ultimatum
    game neither player is given any information about the other.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们采用的多智能体 LLM 架构，我们调整了引入的智能体架构，用以模拟一个名为 Smallville 的城镇及其居民（Park 等， [2023](#bib.bib20)）。在这一架构中，管理员为每个智能体提供一个名字、公共和私人传记、指令（称为指令）以及一个初始计划，初始计划包括描述、停止条件和位置。对于我们的实验，我们根据每个智能体在游戏中的角色（提议者或接收者）为每个智能体命名，并设置每个玩家的初始计划为根据其角色为五轮最终通牒游戏制定一个策略，并将其存储在记忆中。最终通牒游戏不需要玩家移动，也没有位置的概念，因此智能体被设置在一个名为“默认”的单一位置。在私人传记中，我们指定每个玩家的个性特征（“提议者是贪婪的。”）。这告诉智能体他们的个性，但不会通知其他智能体。两个智能体的公共传记为空白，因为在涉及最终通牒游戏的实验中，任何玩家都没有关于其他玩家的任何信息。
- en: Communication between agents in the Smallville architecture is not inherently
    turn-taking. To limit agents to turn-based communication as in the ultimatum game,
    we add instructions for either agent in the directives. The proposer is instructed
    to make offers only after the receiver has responded to the previous offer and
    the receiver is instructed to only respond when the proposer makes offers. Agents
    then use their abilities to create additional plans, react/respond to observed
    events, and communicate with one another to simulate the five rounds of gameplay.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Smallville 架构中，智能体之间的通信本质上不是轮流的。为了将智能体限制为像最终通牒游戏中的回合制通信，我们在指令中为任一智能体添加了指示。提议者被指示在接收者回应先前的提议后再提出新的提议，而接收者则被指示仅在提议者提出提议时才做出回应。智能体随后利用其能力制定额外计划、对观察到的事件做出反应/回应，并相互通信，以模拟五轮游戏。
- en: In our prompt design, we aimed for the simplest prompt that generated the desired
    behavior. Preliminary prompt testing proved that the ultimatum game did not have
    to be defined - simply using the term in the prompt produced LLM outputs that
    showed a comprehensive understanding of the game. We also kept the prompts across
    the two conditions as similar as possible, adding no extra knowledge or instructions
    about personality, strategy, or gameplay in either condition.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的提示设计中，我们旨在使用最简单的提示来生成所需的行为。初步的提示测试证明，最后通牒游戏不需要明确定义——仅仅在提示中使用这一术语就能产生显示对游戏有全面理解的
    LLM 输出。我们还保持两个条件下的提示尽可能相似，没有在任何条件下添加有关个性、策略或游戏玩法的额外知识或指令。
- en: 3.2.2\. Outputs
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 输出
- en: '![Refer to caption](img/6df4069ee7344ca8e32d252019c8683f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6df4069ee7344ca8e32d252019c8683f.png)'
- en: Figure 1\. An output log from a SingleLLM simulation of two fair players playing
    five rounds of the ultimatum game. All text and indentation is from the LLM, bold
    text added by the authors to highlight strategy and gameplay actions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 来自两个公平玩家进行五轮最后通牒游戏的单一 LLM 模拟的输出日志。所有文本和缩进均来自 LLM，粗体文本由作者添加以突出策略和游戏动作。
- en: '![Refer to caption](img/06d525d47e810703e893aec7652f6a60.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/06d525d47e810703e893aec7652f6a60.png)'
- en: Figure 2\. An output log from a Multi-Agent simulation of two fair players playing
    five rounds of the ultimatum game. All text is from the LLM; the labels (underlined)
    are provided by the architecture.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 来自两个公平玩家进行五轮最后通牒游戏的多智能体模拟的输出日志。所有文本来自 LLM；标签（下划线）由架构提供。
- en: 'In both tested architectures, the LLM outputs contain a log of players actions.
    In the single LLM architecture, the strategies of both players and subsequent
    gameplay is displayed in a single output log. Both player strategies are labeled
    and are in the format of numbered or bullet-point lists which typically involve
    an instruction for the first round’s offer or response in the first entry, and
    explains adjustments or changes in strategies for subsequent rounds in the following
    entries. The gameplay is displayed with each round being represented by three
    lines: the first being the proposer’s offer, the second being the receiver’s response,
    and the third explaining what each player receives from the outcome of the round.
    An example output log is shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.2.2\. Outputs
    ‣ 3.2\. Single LLM and Multi-Agent Architecture ‣ 3\. Experimental Set-Up ‣ Simulating
    Human Strategic Behavior: Comparing Single and Multi-agent LLMs") from a simulation
    involving two fair players. The proposer creates a strategy to offer fixed $0.50
    offers while the receiver creates a strategy that uses $0.50 as a fair threshold,
    resulting in five rounds where the proposer offers %0.50 and the receiver accepts.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种测试架构中，LLM 输出包含玩家行动的日志。在单一 LLM 架构中，两个玩家的策略以及随后的游戏玩法都显示在单一输出日志中。两个玩家的策略都有标签，格式为编号或项目符号列表，通常包括第一个回合的提议或回应的指令，以及在随后的条目中解释策略的调整或变化。游戏玩法通过三行来表示每一回合：第一行为提议者的报价，第二行为接收者的回应，第三行解释每个玩家从回合的结果中获得的内容。图
    [1](#S3.F1 "图 1 ‣ 3.2.2\. 输出 ‣ 3.2\. 单一 LLM 和多智能体架构 ‣ 3\. 实验设置 ‣ 模拟人类战略行为：比较单一和多智能体
    LLM") 显示了涉及两个公平玩家的模拟的示例输出日志。提议者制定了一个固定 $0.50 报价的策略，而接收者制定了一个将 $0.50 作为公平门槛的策略，结果是五轮中提议者提供
    $0.50 报价，接收者接受。
- en: 'In the multi-agent LLM architecture, the strategies and actions of each player
    are displayed in two separate output logs, each representing one of the two agents
    involved in the simulation. In each of the output logs, the agent first thinks
    through a strategy creation process, often considering several possible strategies
    for the ultimatum game before choosing or combining multiple into one strategy
    and recording it to memory. The gameplay is then displayed as per the turn-based
    structure specified in each agent’s directives - the proposer makes offers in
    the first output log, while the receiver indicates their responses in the second
    output log, both of which we record. An example output log is shown in Figure
    [2](#S3.F2 "Figure 2 ‣ 3.2.2\. Outputs ‣ 3.2\. Single LLM and Multi-Agent Architecture
    ‣ 3\. Experimental Set-Up ‣ Simulating Human Strategic Behavior: Comparing Single
    and Multi-agent LLMs") from a simulation involving two fair players. The proposer
    creates a strategy to offer $0.50 to the proposer, and consider lowering the offer
    to $0.40 if the receiver repeatedly accepts. The receiver creates a strategy to
    accept offers that uses a $0.40 threshold. The simulation results in the proposer
    offering the receiver $0.50 in the first three rounds and $0.40 to the receiver
    in the last two rounds, with the receiver accepting all five offers.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体 LLM 架构中，每位玩家的策略和行为被显示在两个独立的输出日志中，每个日志代表一个参与模拟的智能体。在每个输出日志中，智能体首先考虑策略创建过程，通常会考虑几种可能的终极博弈策略，然后选择或将多种策略组合成一个策略并记录到记忆中。游戏过程按照每个智能体的指令中指定的回合制结构进行显示——提议者在第一个输出日志中提出报价，而接收者在第二个输出日志中表示回应，两个日志我们都会记录。图
    [2](#S3.F2 "图 2 ‣ 3.2.2\. 输出 ‣ 3.2\. 单一 LLM 和多智能体架构 ‣ 3\. 实验设置 ‣ 模拟人类战略行为：比较单一和多智能体
    LLM") 展示了一个涉及两个公平玩家的模拟输出日志示例。提议者制定了一个策略，向接收者提供 $0.50，并考虑如果接收者重复接受则将报价降低到 $0.40。接收者制定了一个接受报价的策略，使用
    $0.40 的阈值。模拟结果是提议者在前三轮中向接收者提供 $0.50，在最后两轮中提供 $0.40，接收者接受了所有五次报价。
- en: From this collected data, we record the strategies of each player and the amount
    offered by the proposer and the receiver’s response in each round, and then compare
    it with results from human studies to conclude whether or not strategies are human-like
    and personality consistent and whether a simulation’s outcome is human-like.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些收集的数据中，我们记录了每位玩家的策略、提议者所提供的金额以及接收者在每一轮的回应，然后将其与人类研究的结果进行比较，以得出策略是否具有人类特征和个性一致性，及模拟结果是否类似于人类行为的结论。
- en: 3.3\. Evaluation
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 评估
- en: 3.3.1\. Evaluation of Gameplay
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 游戏玩法评估
- en: Based on large-scale studies of human players (Houser and McCabe, [2014](#bib.bib12);
    Krawczyk, [2018](#bib.bib15)), we establish ranges of offers and answers for each
    personality type. Prior experiments with human studies show that fair proposers
    will offer equal or close to equal splits between the range of $0.40 to $0.50,
    with fair receivers typically accepting offers and greedy receivers typically
    rejecting. Meanwhile, greedy proposers offer initial splits heavily biased in
    their favor, typically above $0.70, which is typically rejected by both a fair
    and greedy receiver.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对人类玩家的大规模研究（Houser 和 McCabe，[2014](#bib.bib12)；Krawczyk，[2018](#bib.bib15)），我们为每种人格类型建立了报价和回答范围。先前的人类研究实验表明，公平的提议者通常会在
    $0.40 到 $0.50 的范围内提供相等或接近相等的分割，而公平的接收者通常会接受这些报价，贪婪的接收者则通常会拒绝。与此同时，贪婪的提议者提供的初始分割通常偏向于自己，通常高于
    $0.70，这通常会被公平和贪婪的接收者拒绝。
- en: We evaluate the initial offers of each simulation based on these criteria. In
    the first round, fair proposers are considered to act consistently with their
    personality if their offer is between $0.40 and $0.60, inclusive. Greedy proposers
    are considered to act consistently with their personality if their offer is biased
    in their favor, or strictly less than $0.50 to the receiver. Fair receivers are
    considered to act consistently with their personality if they reject offers that
    are less than $0.40 and accept offers that are greater than $0.40\. Greedy receivers,
    however, are only considered to act consistently with their personality if they
    accept offers that are strictly greater than $0.50; if a greedy receiver accepts
    any amount less than or equal to $0.50, we consider the receiver to have not inconsistently
    with the greedy personality.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据这些标准评估每个模拟的初始报价。在第一回合中，如果公平提议者的报价在 $0.40 到 $0.60 之间（含）则被认为是符合其个性的。如果贪婪提议者的报价偏向于自己，或严格低于
    $0.50 的接收者，则被认为符合其个性。公平接收者如果拒绝低于 $0.40 的报价并接受高于 $0.40 的报价，则被认为符合其个性。然而，贪婪接收者只有在接受严格大于
    $0.50 的报价时才被认为符合其个性；如果贪婪接收者接受任何低于或等于 $0.50 的金额，则我们认为接收者没有与贪婪个性不一致。
- en: In subsequent rounds, we check if each player continues to act as per their
    created strategy as well as whether the taken action is consistent with results
    from human studies. Proposers are expected to continue making offers similar to
    the range of the initial offer if the receiver accepts, but if the receiver rejects,
    proposers are expected to increase their offers slightly (Krawczyk, [2018](#bib.bib15)).
    Receivers are expected to accept offers as per their initial thresholds as well,
    but if gameplay progresses with no accepted offers, receivers are expected to
    lower their threshold, and potentially even discard it by the fifth round as there
    are no future rounds to influence offers of.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的回合中，我们检查每个玩家是否继续按照他们制定的策略行动，以及采取的行动是否与人类研究的结果一致。如果接收者接受了提议者的初始报价，提议者应继续提出类似的报价范围，但如果接收者拒绝，提议者预计会略微提高报价（Krawczyk,
    [2018](#bib.bib15)）。接收者也应根据其初始阈值接受报价，但如果游戏进行中没有接受的报价，接收者应降低其阈值，并可能在第五回合时放弃，因为没有更多的回合影响报价。
- en: 3.3.2\. Evaluation of Strategies
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 策略评估
- en: 'From the information collected from the LLM outputs, we evaluate strategies
    for three components: (1) the completeness of strategies, (2) the consistency
    of strategies with the specified personality trait, and (3) the adherence to the
    strategies in the following gameplay.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从从 LLM 输出收集的信息中，我们评估三部分的策略： (1) 策略的完整性，(2) 策略与指定个性特征的一致性，和 (3) 在随后的游戏中对策略的遵守。
- en: Strategies are considered complete if the player has a course of action for
    all possible states of the game. To be complete, a proposer’s strategy has to
    include an initial offer plan, and then a course of action for subsequent rounds
    based on whether the receiver accepts or rejects the previous offer. If the proposer’s
    strategy is incomplete, there can be issues with the proposer acting inappropriately
    when the receiver does not take the action for which the rest of the strategy
    is contingent on. Similarly, to be complete, a receiver’s strategy has to include
    a course of action for all five rounds for all possible offers between $0.00 to
    $1.00, typically specified via an acceptance threshold based on which the receiver
    acts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 策略被认为是完整的，如果玩家对游戏的所有可能状态都有行动方案。为了完整，提议者的策略必须包括初始报价计划，然后根据接收者是否接受或拒绝前一个报价制定后续回合的行动方案。如果提议者的策略不完整，当接收者不采取其余策略所依赖的行动时，提议者可能会出现不适当的行为。同样，为了完整，接收者的策略必须包括对所有从
    $0.00 到 $1.00 之间的报价的五个回合的行动方案，通常通过一个接受阈值来指定，接收者基于此行动。
- en: 'For example, an incomplete strategy for a greedy proposer is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个贪婪提议者的不完整策略如下：
- en: '”Low-Ball Offers: The greedy proposer would aim to keep as much money as possible
    for themselves. They might start with a low offer to test the receiver’s limit.
    Since we’re dealing with $1, the proposer may start by offering $0.10 to the receiver.'
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “低价报价：贪婪的提议者会尽量为自己保留尽可能多的钱。他们可能会从低报价开始，测试接收者的底线。由于我们处理的是 $1，提议者可能会从 $0.10 的报价开始。
- en: ''
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Incremental Increase: If the offer is rejected, in subsequent rounds, they
    may increase the offer by a small increment, just enough to tempt the receiver
    to accept. For example, the proposer might increase the offer by $0.05 each time.'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 增量增加：如果出价被拒绝，在后续回合中，他们可能会将出价增加一个小幅度，足以诱使接受者接受。例如，提议者可能每次增加$0.05的出价。
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Cut-Off Point: The proposer will have a cut-off point where they find it no
    longer worth to increase the offer because they would rather end up with nothing
    than give away more..”'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 截止点：提议者会有一个截止点，在那里他们认为继续提高出价不再值得，因为他们宁愿什么都不拿也不愿意多给出一些……”
- en: The strategy does not account for the receiver accepting the first offer, potentially
    resulting in problematic gameplay from the proposer if this case is reached.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略没有考虑到接受者接受第一轮出价的情况，如果出现这种情况，可能导致提议者的游戏变得有问题。
- en: Strategies are consistent with the specified personality if the offers made
    (for the proposer) or rejected/accepted (for the receiver) are biased towards
    the player for greedy players and closer to an equal split for fair players. For
    example, a greedy proposer’s strategy should be to make low initial offers that
    are biased in the proposer’s favor, while a fair proposer’s strategy should be
    to make initial offers that are equal or close to equal. Similarly, a greedy receiver’s
    strategy should be to only accept initial offers biased in the receiver’s favor,
    while a fair receiver’s strategy should be to accept initial offers that are equal
    or close to equal. In subsequent rounds, the strategy should be generally similar,
    although based on the actions of the other players there may be concessions made
    by either player to reach agreements, even if they are not biased in the favor
    of a greedy player, because the strategy should also consider that something is
    better than nothing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出价（对于提议者）或拒绝/接受（对于接受者）偏向贪婪玩家或者接近公平分配，则策略与指定的人格一致。例如，贪婪的提议者的策略应是做出对提议者有利的低初始出价，而公平的提议者的策略应是做出相等或接近相等的初始出价。类似地，贪婪的接受者的策略应是只接受对接受者有利的初始出价，而公平的接受者的策略应是接受相等或接近相等的初始出价。在后续回合中，策略应大致相似，尽管根据其他玩家的行动，任何玩家可能会做出让步以达成协议，即使这些让步不是偏向贪婪玩家的，因为策略还应考虑到有些东西总比没有好。
- en: 'For example, a strategy inconsistent with personality for a greedy receiver
    is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于一个贪婪的接受者，不一致的策略如下：
- en: '”Reject Low Offers: Initial minimum acceptance threshold is set high with a
    rejection of any offer below $0.40\. Accept all offers above $0.40.'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “拒绝低出价：初始最低接受阈值设定为高于$0.40，拒绝任何低于$0.40的出价。接受所有高于$0.40的出价。
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Willingness to Adjust: If offers remain low, be willing to gradually lower
    the acceptance threshold to ensure some gain.'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 调整意愿：如果出价保持较低，愿意逐渐降低接受阈值以确保获得一些收益。
- en: ''
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Last Round Acceptance: On the final round, accept any non-zero offer, under
    the assumption that some gain is better than none, adjusting the minimum threshold
    to $0.15..”'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最后一轮接受：在最后一轮，接受任何非零出价，假设有些收益总比没有好，将最低阈值调整为$0.15……”
- en: This strategy sets an acceptance threshold of $0.40, which is lower than an
    equal split, and hence inconsistent with a greedy receiver whom would be expected
    to prefer offers that are biased in their favor (at least above $0.50).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略设定了$0.40的接受阈值，低于公平分配，因此与贪婪的接受者不一致，贪婪的接受者预计会偏好对其有利的出价（至少高于$0.50）。
- en: 4\. Results
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 结果
- en: 'We analyze the outputs of all 40 simulations of the five-round ultimatum game
    for 4 conditions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了在4种条件下的五轮最终通牒游戏的所有40次模拟的输出：
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: multi-agent LLM architecture with GPT 3.5 (abbreviated MultiAgent-3.5)
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GPT 3.5的多智能体LLM架构（缩写为MultiAgent-3.5）
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: multi-agent LLM architecture with GPT 4 (abbreviated MultiAgent-3.5)
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GPT 4的多智能体LLM架构（缩写为MultiAgent-4）
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a single LLM with GPT 3.5 (abbreviated SingleLLM-3.5)
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GPT 3.5的单一LLM（缩写为SingleLLM-3.5）
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a single LLM with GPT 3.5 (abbreviated SingleLLM-4).
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GPT 3.5的单一LLM（缩写为SingleLLM-3.5）。
- en: We report results for our three research questions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了三项研究问题的结果。
- en: RQ1\. Which LLM architecture more accurately simulates human-like actions in
    the five-round ultimatum game?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1\. 哪种LLM架构在五轮最终通牒游戏中更准确地模拟了类人行为？
- en: 'The experiments show that the multi-agent LLM architecture yields actions consistent
    with human experimental data significantly more often than the single LLM. The
    best multi-agent architecture was MultiAgent-4 which resulted in human-like actions
    in 87.5% of simulations, while the best single LLM (SingleLLM-4) only resulted
    in human-like actions in 50% of simulations out of 40 total simulations. See Table
    [1](#S4.T1 "Table 1 ‣ 4\. Results ‣ Simulating Human Strategic Behavior: Comparing
    Single and Multi-agent LLMs"). A chi-square test shows this is statistically significant
    at the p ¡ .01 level $\chi^{2}(1,N=80)=13.091,p=.000297$.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '实验表明，多代理 LLM 架构比单一 LLM 更频繁地产生与人类实验数据一致的行动。最佳的多代理架构是 MultiAgent-4，它在 87.5% 的模拟中产生了类似人类的行动，而最佳的单一
    LLM（SingleLLM-4）仅在 40 次模拟中的 50% 产生了类似人类的行动。见表格 [1](#S4.T1 "Table 1 ‣ 4\. Results
    ‣ Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs")。卡方检验表明，这在
    $p ¡ .01$ 水平上具有统计学意义 $\chi^{2}(1,N=80)=13.091,p=.000297$。'
- en: 'An analysis of the errors shows that strategy creation was a bigger source
    of errors than gameplay mistakes for both architectures. Table [2](#S4.T2 "Table
    2 ‣ 4\. Results ‣ Simulating Human Strategic Behavior: Comparing Single and Multi-agent
    LLMs") shows the percentages of errors due to strategy, gameplay, or both for
    all four conditions. In both MultiAgent architectures, strategy creation errors
    accounted for 100% of errors in simulation, with there being no gameplay mistakes.
    In the SingleLLM-3.5 architecture, 73.9% of errors were in strategy creation,
    compared to only 39.1% in gameplay (and 13.0% having both). In the SingleLLM-4
    architecture, 100% of errors involved an issue with strategy creation, with 25%
    of errors also including gameplay mistakes. Two-proportion z-tests revealed a
    statistically significant difference between the number of strategy creation errors
    and gameplay mistakes for all four conditions at a $p<0.05$ level.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '对错误的分析显示，策略制定比游戏过程中的错误在两种架构中都是更大的错误来源。表格 [2](#S4.T2 "Table 2 ‣ 4\. Results
    ‣ Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs")
    显示了所有四种条件下由策略、游戏过程或两者共同导致的错误百分比。在两种 MultiAgent 架构中，策略制定错误占模拟错误的 100%，没有游戏过程中的错误。在
    SingleLLM-3.5 架构中，73.9% 的错误发生在策略制定中，而游戏过程中的错误仅占 39.1%（同时存在两者的为 13.0%）。在 SingleLLM-4
    架构中，100% 的错误涉及策略制定问题，其中 25% 的错误还包括游戏过程中的错误。两比例 z 检验显示，在所有四种条件下，策略制定错误与游戏过程错误的数量差异在
    $p<0.05$ 水平上具有统计学意义。'
- en: '| Architecture | Success Rate |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Architecture | Success Rate |'
- en: '| --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| MultiAgent-3.5 | 82.5% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 82.5% |'
- en: '| MultiAgent-4 | 87.5% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 87.5% |'
- en: '| SingleLLM-3.5 | 42.5% |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 42.5% |'
- en: '| SingleLLM-4 | 50.0% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 50.0% |'
- en: 'Table 1\. RQ1: Percentage of simulations with human-like actions by architecture.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1\. RQ1: 各架构中具有类似人类行为的模拟百分比。'
- en: '| Architecture | Total Errors | Strategy Errors | Gameplay Errors | Both Errors
    | z-test |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Architecture | Total Errors | Strategy Errors | Gameplay Errors | Both Errors
    | z-test |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MultiAgent-3.5 | 7 | 100% (7/7) | 0% (0/7) | 0% (0/7) | $z=3.7417,p=.00018$
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 7 | 100% (7/7) | 0% (0/7) | 0% (0/7) | $z=3.7417,p=.00018$
    |'
- en: '| MultiAgent-4 | 5 | 100% (5/5) | 0% (0/5) | 0% (0/5) | $z=3.1632,p=.00158$
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 5 | 100% (5/5) | 0% (0/5) | 0% (0/5) | $z=3.1632,p=.00158$
    |'
- en: '| SingleLLM-3.5 | 23 | 73.9% (17/23) | 39.1% (9/23) | 13.0% (3/23) | $z=2.379,p=.017$
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 23 | 73.9% (17/23) | 39.1% (9/23) | 13.0% (3/23) | $z=2.379,p=.017$
    |'
- en: '| SingleLLM-4 | 20 | 100% (20/20) | 25% (5/20) | 25% (5/20) | $z=4.899,p<.00001$
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 20 | 100% (20/20) | 25% (5/20) | 25% (5/20) | $z=4.899,p<.00001$
    |'
- en: 'Table 2\. RQ1: Percentage of errors caused by strategy and gameplay.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2\. RQ1: 由策略和游戏过程导致的错误百分比。'
- en: RQ2\. Which LLM architecture more accurately simulates the actions of player
    personalities?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2\. 哪种 LLM 架构更准确地模拟玩家人格的行动？
- en: 'The experiments show that MultiAgent-4 performed best at modeling the two personality
    types. MultiAgent-4 achieved human-like gameplay for all four personality pairs
    at least 80% of the time (see Table [3](#S4.T3 "Table 3 ‣ 4\. Results ‣ Simulating
    Human Strategic Behavior: Comparing Single and Multi-agent LLMs")). In contrast,
    SingleLLM-4 was inconsistent across personality pairs; it achieved human-like
    gameplay for 100% of the Fair-Fair simulations, but only 10% of the Greedy-Greedy
    conditions.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '实验表明，MultiAgent-4 在模拟两种人格类型时表现最佳。MultiAgent-4 在所有四种人格对中至少有 80% 的时间达到了类似人类的游戏过程（见表格
    [3](#S4.T3 "Table 3 ‣ 4\. Results ‣ Simulating Human Strategic Behavior: Comparing
    Single and Multi-agent LLMs")）。相比之下，SingleLLM-4 在人格对之间的不一致性较大；它在 Fair-Fair 模拟中达到了
    100% 的类似人类游戏过程，但在 Greedy-Greedy 条件下仅为 10%。'
- en: 'When analyzing gameplay for each of the personality pairs, we see the errors
    are not the same across the pairs. Fair-Fair has the best performance with SingleLLM-4,
    MultiAgent-3.5, and MultiAgent-4 all being 100% consistent with human gameplay.
    The most errors occurred in simulations of the Greedy-Greedy personality pairing,
    with MultiAgent-4 performing the best with 80% of simulations being consistent
    with human gameplay. The MultiAgent-3.5, SingleLLM-3.5, and SingleLLM-4 were consistent
    with human gameplay in 70%, 60%, and 10% of simulations respectively. The Fair-Greedy
    and Greedy-Fair conditions were somewhere in between: with both SingleLLM’s having
    middling scores (30-50%).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析每对个性的游戏玩法时，我们发现错误在不同对之间并不相同。Fair-Fair 在 SingleLLM-4、MultiAgent-3.5 和 MultiAgent-4
    上的表现最佳，三者在 100% 的模拟中与人类游戏玩法一致。Greedy-Greedy 个性配对的模拟中错误最多，其中 MultiAgent-4 的表现最佳，有
    80% 的模拟与人类游戏玩法一致。MultiAgent-3.5、SingleLLM-3.5 和 SingleLLM-4 分别在 70%、60% 和 10%
    的模拟中与人类游戏玩法一致。Fair-Greedy 和 Greedy-Fair 条件介于两者之间：两个 SingleLLM 的得分中等（30-50%）。
- en: '| Architecture | Fair-Fair | Fair-Greedy | Greedy-Fair | Greedy-Greedy |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | Fair-Fair | Fair-Greedy | Greedy-Fair | Greedy-Greedy |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MultiAgent-3.5 | 100% | 80% | 80% | 70% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 100% | 80% | 80% | 70% |'
- en: '| MultiAgent-4 | 100% | 80% | 90% | 80% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 100% | 80% | 90% | 80% |'
- en: '| SingleLLM-3.5 | 30% | 50% | 30% | 60% |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 30% | 50% | 30% | 60% |'
- en: '| SingleLLM-4 | 100% | 40% | 50% | 10% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 100% | 40% | 50% | 10% |'
- en: 'Table 3\. RQ2: Percentage of simulations with human-like gameplay by architecture
    and personality-pairing.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3\. RQ2: 各架构和个性配对下模拟与人类游戏玩法一致的百分比。'
- en: 'RQ3\. Which LLM architecture more often creates robust strategies: both logically
    complete and consistent with personality?'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3\. 哪种 LLM 架构更常创建出既逻辑上完整又与个性一致的稳健策略？
- en: 'The MultiAgent architectures create robust strategies at a higher rate than
    SingleLLMs (See Table [4](#S4.T4 "Table 4 ‣ 4\. Results ‣ Simulating Human Strategic
    Behavior: Comparing Single and Multi-agent LLMs")). MultiAgent-4 creates complete
    and personality-consistent strategies for both players in 87.5% of simulations.
    The MultiAgent3.5 architecture performs slightly worse, creating complete and
    personality-consistent strategies for both players in 80% of simulations. The
    SingleLLM-3.5 and SingleLLM-4 architectures create complete and personality-consistent
    strategies in 55% and 47.5% of simulations respectively.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: MultiAgent 架构创建稳健策略的比例高于 SingleLLMs（参见表 [4](#S4.T4 "表 4 ‣ 4\. 结果 ‣ 模拟人类战略行为：比较单一与多代理
    LLM")）。MultiAgent-4 在 87.5% 的模拟中为两名玩家创建了完整且与个性一致的策略。MultiAgent-3.5 架构表现稍差，为两名玩家在
    80% 的模拟中创建了完整且与个性一致的策略。SingleLLM-3.5 和 SingleLLM-4 架构分别在 55% 和 47.5% 的模拟中创建了完整且与个性一致的策略。
- en: We find that the MultiAgent-4 architecture performs better in creating complete
    and personality-consistent strategies than the best-performing SingleLLM architecture
    (SingleLLM-3.5). A chi-square test shows this is statistically significant at
    the p ¡ .01 level $\chi^{2}(1,N=40)=10.3127,p=.001321$.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 MultiAgent-4 架构在创建完整且与个性一致的策略方面表现优于表现最佳的 SingleLLM 架构（SingleLLM-3.5）。卡方检验显示，这在
    p < .01 水平上具有统计学意义 $\chi^{2}(1,N=40)=10.3127,p=.001321$。
- en: 'To analyze the source of these errors, we analyze the robustness of proposer
    strategies and receiver strategies separately. Table [4](#S4 "4\. Results ‣ Simulating
    Human Strategic Behavior: Comparing Single and Multi-agent LLMs") shows that the
    problem with proposer strategies is always incompleteness. Proposers have no errors
    with personality consistency across all four architectures. Conversely, Table
    [4](#S4 "4\. Results ‣ Simulating Human Strategic Behavior: Comparing Single and
    Multi-agent LLMs") shows that the problem with receiver strategies with issues
    are almost always inconsistent with personality. Across all conditions, there
    was only one incomplete receiver strategy.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析这些错误的来源，我们分别分析了提议者策略和接收者策略的稳健性。表 [4](#S4 "4\. 结果 ‣ 模拟人类战略行为：比较单一与多代理 LLM")
    显示提议者策略的问题总是完整性不足。提议者在所有四种架构中都没有个性一致性方面的错误。相反，表 [4](#S4 "4\. 结果 ‣ 模拟人类战略行为：比较单一与多代理
    LLM") 显示接收者策略的问题几乎总是不与个性一致。在所有条件下，只有一个接收者策略是不完整的。
- en: '| Architecture | % Strategies Complete | % Strategies Consistent with Personality
    | % Strategies Complete & Consistent |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | % 策略完整 | % 策略与个性一致 | % 策略完整且一致 |'
- en: '| MultiAgent-3.5 | 90% | 85% | 80% |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 90% | 85% | 80% |'
- en: '| MultiAgent-4 | 95% | 87.5% | 87.5% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 95% | 87.5% | 87.5% |'
- en: '| SingleLLM-3.5 | 65% | 80% | 55% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 65% | 80% | 55% |'
- en: '| SingleLLM-4 | 55% | 60% | 47.5% |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 55% | 60% | 47.5% |'
- en: 'Table 4\. RQ3: Percentage of simulations in which both strategies are complete,
    consistent, and both.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. RQ3：两个策略均完整、一致及两者兼具的模拟百分比。
- en: '| Architecture | Proposer: |  |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 提议者： |  |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | Proposer: |  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | 提议者： |  |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MultiAgent-3.5 | 92.5% | 100% |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 92.5% | 100% |  |'
- en: '| MultiAgent-4 | 95% | 100% |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 95% | 100% |  |'
- en: '| SingleLLM-3.5 | 67.5% | 100% |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 67.5% | 100% |  |'
- en: '| SingleLLM-4 | 52.5% | 100% |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 52.5% | 100% |  |'
- en: 'Table 5\. RQ3: Percentage of proposer strategies that are complete, consistent,
    and both. Red indicates the presence of errors.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. RQ3：提议者策略的完整、一致及两者兼具的百分比。红色表示存在错误。
- en: '| Architecture | Receiver: |  |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 接收者： |  |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | Receiver: |  |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | 接收者： |  |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MultiAgent-3.5 | 97.5% | 85% |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-3.5 | 97.5% | 85% |  |'
- en: '| MultiAgent-4 | 100% | 87.5% |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| MultiAgent-4 | 100% | 87.5% |  |'
- en: '| SingleLLM-3.5 | 100% | 80% |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-3.5 | 100% | 80% |  |'
- en: '| SingleLLM-4 | 100% | 60% |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SingleLLM-4 | 100% | 60% |  |'
- en: 'Table 6\. RQ3: Percentage of receiver strategies that are complete, consistent,
    and both. Red indicates the presence of errors.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. RQ3：接收者策略的完整、一致及两者兼具的百分比。红色表示存在错误。
- en: 5\. Discussion
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 讨论
- en: 5.1\. Why are multi-agent structures better at strategic simulation?
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 为什么多智能体结构在战略模拟中表现更好？
- en: We found that multi-agent LLM architectures show great promise for simulating
    strategic human behavior. They were consistent with human experimental data 80%
    of the time, simulated all personality pairings well, and were generally able
    to create complete and consistent strategies and adhere to them in gameplay. In
    contrast, single agent LLMs were only 43% accurate, with 90% of the errors coming
    from the strategies.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现多智能体LLM架构在模拟战略性人类行为方面展现了极大的潜力。它们在80%的情况下与人类实验数据一致，成功模拟了所有性格配对，并且通常能够生成完整且一致的策略并在游戏过程中遵守。相比之下，单智能体LLMs的准确率仅为43%，其中90%的错误来自策略。
- en: Surprisingly, single LLMs errors were caused by issues in strategy creation.
    Creating strategies seems like the simplest part of the instructions, and was
    expected to be similar in performance to the multi-agent LLM. We expected single
    agent LLMs to have errors caused by gameplay mistakes - as it generated more tokens,
    the LLM might “forget” earlier information like it’s strategies or results of
    early rounds. But attention or memory were seemingly not the problem. Perhaps
    a single LLM created abbreviated (and thus poor) strategies because it was tasked
    to come up with two strategies, as opposed to just one, as the multi-agent structure
    was. Doing two things well is often harder than doing one thing well.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，单智能体LLMs的错误是由于策略创建中的问题。创建策略似乎是指令中最简单的部分，并且预计其性能应与多智能体LLM相似。我们预期单智能体LLMs的错误是由游戏过程中的错误引起的——由于生成了更多的tokens，LLM可能会“忘记”早期的信息，如其策略或早期轮次的结果。但注意力或记忆似乎不是问题所在。也许单智能体LLM创建了缩略（因此较差）的策略，因为它被要求提出两个策略，而不是像多智能体结构那样仅提出一个。做好两件事往往比做好一件事更难。
- en: We ran brief experiments attempting to improve our prompt to guide the LLMs
    to create strategies. However, no obvious rewording created better results. Explicitly
    instructing proposer strategies to consider both acceptance and rejection cases
    still resulted in incomplete strategies. Explicitly defining both of the personality
    characteristics in the prompt also did not impact the strategies created significantly.
    Asking both LLM architectures to only create strategies (and not simulate following
    gameplay) also resulted in similar error types; incomplete proposer strategies
    were still generated, as were receiver strategies inconsistent with personality.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了简短的实验，尝试改进我们的提示，以指导LLMs生成策略。然而，没有明显的措辞改动带来更好的结果。明确指示提议者策略同时考虑接受和拒绝的情况，仍然导致策略不完整。明确在提示中定义两个性格特征，也没有显著影响生成的策略。要求两个LLM架构仅生成策略（而不模拟跟随的游戏过程）也导致了类似的错误类型；仍然生成了不完整的提议者策略，以及与性格不一致的接收者策略。
- en: Why are LLMs making errors in strategy creation? Perhaps proposer strategies
    are difficult to create due to the complexity of specifying plans for both receivers’
    actions. Perhaps receiver strategies are difficult to create because they require
    setting a numeric threshold based on personality, and LLMs are sometimes bad with
    math. However, this wouldn’t explain why Multi-Agent LLMs do a little better than
    Single LLMs. Regardless of the reason for the error, if someone wanted to run
    a simulation, they could put a little effort into checking the agents’ strategies
    and correcting it before letting the simulation run.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么LLMs在策略创建中会出现错误？也许提出者的策略很难创建，因为指定接收者行动的计划复杂。也许接收者策略很难创建，因为它们需要根据个性设置数值阈值，而LLMs有时数学能力较差。然而，这并不能解释为什么多代理LLMs比单一LLMs表现稍好。无论错误的原因是什么，如果有人想要进行模拟，他们可以花一点时间检查代理的策略，并在让模拟运行之前进行纠正。
- en: Surprisingly, the single LLM didn’t struggle to separate out the knowledge the
    two players should have. Even though it has full knowledge of both players, it
    didn’t seem to abuse that knowledge or get confused. However, in a more complex
    game with more players, this might become a problem. When creating scripts with
    multiple stories. This all argues for using Multi-agent LLMs when approaching
    simulating problems.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 出乎意料的是，单一LLM并未在区分两名玩家应具备的知识方面遇到困难。尽管它对两名玩家拥有全面的知识，但它似乎没有滥用这些知识或感到困惑。然而，在一个更复杂且玩家更多的游戏中，这可能会成为一个问题。在创建包含多个故事的脚本时。这一切都表明在处理模拟问题时应该使用多代理LLMs。
- en: 5.2\. Potential for LLM-based behavior simulations to help designers
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 基于LLM的行为模拟对设计师的潜力
- en: Designers and other creators depend on human feedback to guide their process.
    Early LLM tools showed that LLM-based writing tools has already shown that LLMs
    can be use to give the readers’ perspective when the writer is struggling to know
    if they are understood  (Gero et al., [2022](#bib.bib6)). Persona-based tools
    like Smallville showed that LLMs can capture individual differences in thoughts,
    actions and behavior. Work in the design field has shown that persona-based discussions
    in the early stages of design can help explore the design space through dialectics
     (Cai et al., [2024](#bib.bib5)) - the art of investigating or discussing the
    truth of opinions. Although these scenarios are relatively simple, as LLMs expand
    their knowledge, context window, and alignment with human values, we expect they
    will increase in their ability to simulate complex human behavior.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 设计师和其他创作者依赖于人类反馈来指导他们的过程。早期的LLM工具显示，基于LLM的写作工具已经表明LLMs可以在作者难以确认自己是否被理解时，提供读者的视角（Gero
    et al., [2022](#bib.bib6)）。像Smallville这样的基于角色的工具显示LLMs能够捕捉到思想、行动和行为中的个体差异。设计领域的研究表明，在设计的早期阶段进行基于角色的讨论可以通过辩证法（Cai
    et al., [2024](#bib.bib5)）帮助探索设计空间——辩证法是探讨或讨论观点真实性的艺术。尽管这些场景相对简单，但随着LLMs扩展其知识、上下文窗口和与人类价值观的对齐，我们预计它们在模拟复杂人类行为方面的能力将会提高。
- en: Strategic behavior is especially important to simulate in policy design and
    security settings. How will malicious, lazy, or new/confused people react? Will
    they break the system, either intentionally or unintentionally? And for a proposed
    patch to the system, will it work, and will it negatively impact well-intentioned
    actors, and expert actors? Consider a mundane example like designing a late policy
    for homework. Allowing infinite lateness will be advantageous to students who
    are busy and need flexibility but likely lead students who are prone to procrastinating
    to fall behind. A strict policy will likely keep procrastinators motivated, but
    will also not allow flexibility (and will generate millions of complaint emails).
    As with all design problems, there is no right or wrong answer, but better and
    worse solutions. Thinking through a problem from the perspective of multiple types
    of actors is mentally demanding. Although we don’t expect an LLM system to perfectly
    replicate human behavior in all situations, we think it has the potential to be
    a great interactive tool to help designers explore a space of action consistent
    with human behavior and take into account complexities like personality, “irrationality,”
    and strategic thinking.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在政策设计和安全设置中，模拟战略行为尤为重要。恶意、懒惰或新手/困惑的人员会如何反应？他们会故意或无意地破坏系统吗？对于系统的修补提案，它是否有效？是否会对善意的参与者和专家造成负面影响？考虑一个平凡的例子，比如设计作业的迟交政策。允许无限迟交对忙碌且需要灵活性的学生有利，但可能会导致那些容易拖延的学生落后。严格的政策可能会让拖延者保持动力，但也不会提供灵活性（并且会产生大量的投诉邮件）。与所有设计问题一样，没有绝对正确或错误的答案，只有更好或更差的解决方案。从多种类型参与者的角度思考问题是有挑战性的。虽然我们不期望LLM系统在所有情况下都能完美地复制人类行为，但我们认为它有潜力成为一个很好的互动工具，帮助设计者探索与人类行为一致的行动空间，并考虑诸如个性、“非理性”和战略思维等复杂因素。
- en: 5.3\. Limitations
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 限制
- en: Although this paper studies human strategic behavior, the ultimatum game is
    a rather small example. In more complex scenarios, LLMs may not perform as well
    as they do in the ultimatum game. Our version of ultimatum uses 5 rounds and two
    players - each with very simple decisions to make (how much to offer and accept/reject).
    This size does not challenge the LLM’s context window, output constraints or attention
    mechanism. Further investigations should test hundreds of rounds of games to see
    if and when it breaks down. Simulations also get harder with more agents. We expect
    multi-agent architectures to be good at this, as that is what they were designed
    for. However, this should be tested, perhaps on variants of the ultimatum game
    such as the competitive ultimatum game where multiple proposers make offers and
    receivers must pick among them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本文研究了人类的战略行为，但最终通牒游戏只是一个相对较小的例子。在更复杂的场景中，大型语言模型（LLM）的表现可能不如在最终通牒游戏中那样出色。我们版本的最终通牒游戏使用了5轮和两个玩家——每个玩家都有非常简单的决策（决定出价和接受/拒绝）。这个规模不会挑战LLM的上下文窗口、输出限制或注意力机制。进一步的研究应测试数百轮游戏，以观察何时以及是否会出现问题。随着代理数量的增加，模拟也会变得更困难。我们预计多代理架构在这方面会表现良好，因为这是它们设计的目的。然而，这应该经过测试，也许可以在最终通牒游戏的变体上进行测试，例如竞争最终通牒游戏，其中多个提议者提出报价，接收者必须在这些报价中进行选择。
- en: The ultimatum game might be too popular to be used as a test for generalized
    human behavior. LLMs are trained to make predictions based on their large text
    corpus. GPT may have examples of strategies and gameplay to draw from. Thus, it
    might not be performing strategic behavior that can be generalized to other scenarios.
    It could just be recreating examples it has seen. However, this is unlikely because
    the SingleLLM performs poorly, with only the multi-agent starts to get promising
    results. If the LLM were purely parroting back past examples, we would expect
    a SingleLLM to excel. Either way, it is unclear how an LLM would be able to simulate
    strategic human behavior in novel scenarios. It is an open question, but a reason
    to be optimistic is that LLMs have such a broad knowledge base that very little
    is truly new to them. Despite not having seen simulations of classroom late policies,
    they could probably simulate typical student complaints that fill Reddit message
    boards.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最终通牒游戏可能过于流行，不适合用作测试普遍人类行为的工具。大型语言模型（LLMs）是通过大量文本语料库进行训练的。GPT 可能有策略和游戏玩法的示例可以借鉴。因此，它可能并没有表现出能够推广到其他场景的战略行为。它可能只是重现了它见过的示例。然而，这种可能性不大，因为单一
    LLM 的表现不佳，只有多代理模型才开始取得令人满意的结果。如果 LLM 仅仅是在重复过去的示例，我们应该期望单一 LLM 表现出色。不论如何，目前还不清楚
    LLM 如何在新颖场景中模拟战略性人类行为。这是一个未解之谜，但令人乐观的原因是 LLM 拥有如此广泛的知识基础，对它们来说几乎没有什么是全新的。尽管没有见过课堂迟到政策的模拟，它们可能仍然能够模拟填充
    Reddit 论坛的典型学生抱怨。
- en: It is an addtional challenge to simulate human behavior for tor truly unprecedented
    events with no history to draw from. This might include pandemics like COVID,
    or new technologies like AI in the workforce. Without explicit data to draw from,
    LLMs would have to reason from first principles, or draw inferences from past
    events like previous emergencies or innovations and adjust them to modern times.
    It could be possible for an LLM to rely on social science theories of human behavior
    to base simulations on. LLMs have shown a surprising ability to reason, rather
    than just recall information. And even if they can’t reason completely about novel
    events, they can still be useful to designers in covering the less novel aspects
    of a complex situation as it evolves. However, much more research is necessary
    and this is a fertile and important area for researchers to explore.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟人类行为面临的额外挑战是应对真正前所未有的事件，这些事件没有历史可供借鉴。这可能包括 COVID 等流行病或 AI 进入劳动力市场等新技术。如果没有明确的数据可供借鉴，LLM
    将不得不从基本原则出发进行推理，或从以往的紧急事件或创新中得出推论，并将其调整到现代时代。LLM 可能会依赖于社会科学理论来基础模拟人类行为。LLM 展现了令人惊讶的推理能力，而不仅仅是回忆信息。即使它们不能完全推理新颖事件，它们仍然可以帮助设计师覆盖复杂情况中较少新颖的方面。尽管如此，仍需要大量的研究，这是一个富有成效且重要的研究领域。
- en: 6\. Conclusion
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: Based on our experiments with Single and Multi-Agent LLMs, we conclude that
    Multi-Agent LLMs show great potential for simulating strategic behavior consistent
    with human gameplay. We compare LLMs playing The ultimatum game over 5 rounds
    and see that Multi-Agent LLMs achieve gameplay consistent with human experimental
    data in 85% of simulations. While single LLMs achieve gameplay consistent with
    human data in only 43% of simulations. Surprisingly, when the Single LLMs make
    errors, they tend to make them in strategy creation (100%) rather than in gameplay
    (25%). Based on the strengths of Multi-agent LLMs to create and execute strategic
    thinking and behavior, we believe these can become a tool for policy designers
    to think through the behavior of agents with different personalities, trying to
    strategically navigate a system to achieve a personal outcome. This type of thinking
    is immensely difficult for people, and LLM-based simulations can aid this cognitive
    process.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对单代理和多代理 LLM 的实验，我们得出结论，多代理 LLM 在模拟与人类游戏玩法一致的战略行为方面显示出巨大潜力。我们比较了 LLM 在 5
    轮最终通牒游戏中的表现，发现多代理 LLM 在 85% 的模拟中达到了与人类实验数据一致的游戏玩法。而单一 LLM 仅在 43% 的模拟中达到了与人类数据一致的游戏玩法。令人惊讶的是，当单一
    LLM 出错时，它们往往是在战略创建方面出错（100%），而不是在游戏玩法方面（25%）。基于多代理 LLM 在创建和执行战略思维和行为方面的优势，我们认为这些
    LLM 可以成为政策设计师的工具，用于思考不同个性代理的行为，尝试战略性地驾驭系统以实现个人目标。这种思维对人类来说极其困难，而基于 LLM 的模拟可以帮助这一认知过程。
- en: References
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Aher et al. (2023) Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023.
    Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject
    Studies. arXiv:2208.10264 [cs.CL]
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aher et al. (2023) Gati Aher, Rosa I. Arriaga, 和 Adam Tauman Kalai. 2023. 使用大型语言模型模拟多个人类并复制人类受试者研究。arXiv:2208.10264 [cs.CL]
- en: Alvard (2004) Michael Alvard. 2004. *The Ultimatum Game, Fairness, and Cooperation
    among Big Game Hunters*. 413–435. [https://doi.org/10.1093/0199262055.003.0014](https://doi.org/10.1093/0199262055.003.0014)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alvard (2004) Michael Alvard. 2004. *最后通牒游戏、公平性与大型猎人的合作*。413–435. [https://doi.org/10.1093/0199262055.003.0014](https://doi.org/10.1093/0199262055.003.0014)
- en: 'Ariely (2008) Dan Ariely. 2008. *Predictably Irrational: The Hidden Forces
    That Shape Our Decisions*. Harper, New York, NY.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ariely (2008) Dan Ariely. 2008. *预测性非理性：塑造我们决策的隐秘力量*。哈珀，纽约，NY。
- en: 'Cai et al. (2024) Alice Cai, Shiyan Zhang, Celine Janssen, and Jeffrey Nickerson.
    2024. Upside Down Dialectics: Exploring design conversations with synthetic humans.
    In *under review at DESRIST 2024*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2024) Alice Cai, Shiyan Zhang, Celine Janssen, 和 Jeffrey Nickerson.
    2024. 颠覆性辩证法：与合成人的设计对话。在 *待审稿的DESRIST 2024*。
- en: 'Gero et al. (2022) Katy Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022. Sparks:
    Inspiration for Science Writing using Language Models. In *Proceedings of the
    2022 ACM Designing Interactive Systems Conference* (¡conf-loc¿, ¡city¿Virtual
    Event¡/city¿, ¡country¿Australia¡/country¿, ¡/conf-loc¿) *(DIS ’22)*. Association
    for Computing Machinery, New York, NY, USA, 1002–1019. [https://doi.org/10.1145/3532106.3533533](https://doi.org/10.1145/3532106.3533533)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gero et al. (2022) Katy Ilonka Gero, Vivian Liu, 和 Lydia Chilton. 2022. Sparks:
    使用语言模型进行科学写作的灵感。在 *2022年ACM互动系统设计会议论文集*（虚拟活动，澳大利亚）*(DIS ’22)*. 计算机协会，纽约，NY，美国，1002–1019.
    [https://doi.org/10.1145/3532106.3533533](https://doi.org/10.1145/3532106.3533533)'
- en: Guo (2023) Fulin Guo. 2023. GPT in Game Theory Experiments. arXiv:2305.05516 [econ.GN]
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo (2023) Fulin Guo. 2023. GPT在博弈论实验中的应用。arXiv:2305.05516 [econ.GN]
- en: 'Hamilton (2023) Sil Hamilton. 2023. Blind Judgement: Agent-Based Supreme Court
    Modelling With GPT. arXiv:2301.05327 [cs.CL]'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton (2023) Sil Hamilton. 2023. 盲目判断：基于代理的最高法院建模与GPT。arXiv:2301.05327 [cs.CL]
- en: Henrich (2000) Joseph Henrich. 2000. Does Culture Matter in Economic Behavior?
    Ultimatum Game Bargaining among the Machiguenga of the Peruvian Amazon. *American
    Economic Review* 90, 4 (September 2000), 973–979. [https://doi.org/10.1257/aer.90.4.973](https://doi.org/10.1257/aer.90.4.973)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henrich (2000) Joseph Henrich. 2000. 文化在经济行为中重要吗？在秘鲁亚马逊地区Machiguenga族中的最后通牒游戏讨价还价。*美国经济评论*
    90, 4 (2000年9月)，973–979. [https://doi.org/10.1257/aer.90.4.973](https://doi.org/10.1257/aer.90.4.973)
- en: 'Herbrich et al. (2006) Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkill™:
    A Bayesian Skill Rating System. In *Advances in Neural Information Processing
    Systems*, B. Schölkopf, J. Platt, and T. Hoffman (Eds.), Vol. 19\. MIT Press.
    [https://proceedings.neurips.cc/paper_files/paper/2006/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2006/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Herbrich et al. (2006) Ralf Herbrich, Tom Minka, 和 Thore Graepel. 2006. TrueSkill™:
    一个贝叶斯技能评分系统。在 *神经信息处理系统进展*，B. Schölkopf, J. Platt, 和 T. Hoffman（编辑），第19卷。MIT出版社。
    [https://proceedings.neurips.cc/paper_files/paper/2006/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2006/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf)'
- en: 'Horton (2023) John J. Horton. 2023. Large Language Models as Simulated Economic
    Agents: What Can We Learn from Homo Silicus? arXiv:2301.07543 [econ.GN]'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horton (2023) John J. Horton. 2023. 大型语言模型作为模拟经济代理：我们能从Homo Silicus中学到什么？arXiv:2301.07543 [econ.GN]
- en: Houser and McCabe (2014) Daniel Houser and Kevin McCabe. 2014. Chapter 2 - Experimental
    Economics and Experimental Game Theory. In *Neuroeconomics (Second Edition)* (second
    edition ed.), Paul W. Glimcher and Ernst Fehr (Eds.). Academic Press, San Diego,
    19–34. [https://doi.org/10.1016/B978-0-12-416008-8.00002-4](https://doi.org/10.1016/B978-0-12-416008-8.00002-4)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houser and McCabe (2014) Daniel Houser 和 Kevin McCabe. 2014. 第2章 - 实验经济学与实验博弈论。在
    *神经经济学（第二版）*（第二版），Paul W. Glimcher 和 Ernst Fehr（编辑）。学术出版社，圣地亚哥，19–34. [https://doi.org/10.1016/B978-0-12-416008-8.00002-4](https://doi.org/10.1016/B978-0-12-416008-8.00002-4)
- en: Kahneman (2012) Daniel Kahneman. 2012. *Thinking, fast and slow*. Penguin, London.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahneman (2012) Daniel Kahneman. 2012. *思考，快与慢*。企鹅出版社，伦敦。
- en: 'Kidd et al. (2013) Celeste Kidd, Holly Palmeri, and Richard N. Aslin. 2013.
    Rational snacking: Young children’s decision-making on the marshmallow task is
    moderated by beliefs about environmental reliability. *Cognition* 126, 1 (2013),
    109–114. [https://doi.org/10.1016/j.cognition.2012.08.004](https://doi.org/10.1016/j.cognition.2012.08.004)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kidd 等 (2013) 塞莱斯特·基德、霍莉·帕尔梅里 和 理查德·N·阿斯林。2013。理性零食：幼儿在棉花糖任务中的决策受对环境可靠性的信念调节。*认知*
    126, 1 (2013), 109–114。 [https://doi.org/10.1016/j.cognition.2012.08.004](https://doi.org/10.1016/j.cognition.2012.08.004)
- en: 'Krawczyk (2018) Daniel C. Krawczyk. 2018. Chapter 12 - Social Cognition: Reasoning
    With Others. In *Reasoning*, Daniel C. Krawczyk (Ed.). Academic Press, 283–311.
    [https://doi.org/10.1016/B978-0-12-809285-9.00012-0](https://doi.org/10.1016/B978-0-12-809285-9.00012-0)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krawczyk (2018) 丹尼尔·C·克劳西克。2018。第12章 - 社会认知：与他人推理。见于*推理*，丹尼尔·C·克劳西克（编辑）。学术出版社，283–311。
    [https://doi.org/10.1016/B978-0-12-809285-9.00012-0](https://doi.org/10.1016/B978-0-12-809285-9.00012-0)
- en: Königstein (2001) Manfred Königstein. 2001. Personality influences on Ultimatum
    Game bargaining decisions. *European Journal of Personality* 15 (10 2001), S53
    – S70. [https://doi.org/10.1002/per.424](https://doi.org/10.1002/per.424)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Königstein (2001) 曼弗雷德·科尼希斯坦。2001。人格对终极博弈谈判决策的影响。*欧洲人格杂志* 15 (2001年10月), S53
    – S70。 [https://doi.org/10.1002/per.424](https://doi.org/10.1002/per.424)
- en: 'McCrae and Costa (2008) Robert R McCrae and Paul T Jr Costa. 2008. The five-factor
    theory of personality. In *Handbook of personality: Theory and research* (3 ed.),
    Oliver P John, Richard W Robins, and Lawrence A Pervin (Eds.). The Guilford Press,
    159–181.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCrae 和 Costa (2008) 罗伯特·R·麦克雷和保罗·T·Jr·科斯塔。2008。《人格的五因素理论》。见于*人格手册：理论与研究*（第3版），奥利弗·P·约翰、理查德·W·罗宾斯
    和 劳伦斯·A·佩尔文（编辑）。格uilford出版社，159–181。
- en: 'Mullainathan and Shafir (2013) Sendhil Mullainathan and Eldar Shafir. 2013.
    *Scarcity: Why having too little means so much*. Times Books/Henry Holt and Co.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mullainathan 和 Shafir (2013) 森德希尔·穆莱那坦 和 埃尔达·沙菲尔。2013。《稀缺：为什么拥有太少意味着这么多》。时代出版社/亨利·霍尔特公司。
- en: OpenAI et al. (2023) OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal,
    Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake
    Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg
    Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,
    Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea
    Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen,
    Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,
    Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,
    Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
    Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan
    Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian
    Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong,
    Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,
    Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,
    Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023.
    GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人（2023 年）OpenAI，:，Josh Achiam，Steven Adler，Sandhini Agarwal，Lama Ahmad，Ilge
    Akkaya，Florencia Leoni Aleman，Diogo Almeida，Janko Altenschmidt，Sam Altman，Shyamal
    Anadkat，Red Avila，Igor Babuschkin，Suchir Balaji，Valerie Balcom，Paul Baltescu，Haiming
    Bao，Mo Bavarian，Jeff Belgum，Irwan Bello，Jake Berdine，Gabriel Bernadett-Shapiro，Christopher
    Berner，Lenny Bogdonoff，Oleg Boiko，Madelaine Boyd，Anna-Luisa Brakman，Greg Brockman，Tim
    Brooks，Miles Brundage，Kevin Button，Trevor Cai，Rosie Campbell，Andrew Cann，Brittany
    Carey，Chelsea Carlson，Rory Carmichael，Brooke Chan，Che Chang，Fotis Chantzis，Derek
    Chen，Sully Chen，Ruby Chen，Jason Chen，Mark Chen，Ben Chess，Chester Cho，Casey Chu，Hyung
    Won Chung，Dave Cummings，Jeremiah Currier，Yunxing Dai，Cory Decareaux，Thomas Degry，Noah
    Deutsch，Damien Deville，Arka Dhar，David Dohan，Steve Dowling，Sheila Dunning，Adrien
    Ecoffet，Atty Eleti，Tyna Eloundou，David Farhi，Liam Fedus，Niko Felix，Simón Posada
    Fishman，Juston Forte，Isabella Fulford，Leo Gao，Elie Georges，Christian Gibson，Vik
    Goel，Tarun Gogineni，Gabriel Goh，Rapha Gontijo-Lopes，Jonathan Gordon，Morgan Grafstein，Scott
    Gray，Ryan Greene，Joshua Gross，Shixiang Shane Gu，Yufei Guo，Chris Hallacy，Jesse
    Han，Jeff Harris，Yuchen He，Mike Heaton，Johannes Heidecke，Chris Hesse，Alan Hickey，Wade
    Hickey，Peter Hoeschele，Brandon Houghton，Kenny Hsu，Shengli Hu，Xin Hu，Joost Huizinga，Shantanu
    Jain，Shawn Jain，Joanne Jang，Angela Jiang，Roger Jiang，Haozhun Jin，Denny Jin，Shino
    Jomoto，Billie Jonn，Heewoo Jun，Tomer Kaftan，Łukasz Kaiser，Ali Kamali，Ingmar Kanitscheider，Nitish
    Shirish Keskar，Tabarak Khan，Logan Kilpatrick，Jong Wook Kim，Christina Kim，Yongjik
    Kim，Hendrik Kirchner，Jamie Kiros，Matt Knight，Daniel Kokotajlo，Łukasz Kondraciuk，Andrew
    Kondrich，Aris Konstantinidis，Kyle Kosic，Gretchen Krueger，Vishal Kuo，Michael Lampe，Ikai
    Lan，Teddy Lee，Jan Leike，Jade Leung，Daniel Levy，Chak Ming Li，Rachel Lim，Molly Lin，Stephanie
    Lin，Mateusz Litwin，Theresa Lopez，Ryan Lowe，Patricia Lue，Anna Makanju，Kim Malfacini，Sam
    Manning，Todor Markov，Yaniv Markovski，Bianca Martin，Katie Mayer，Andrew Mayne，Bob
    McGrew，Scott Mayer McKinney，Christine McLeavey，Paul McMillan，Jake McNeil，David
    Medina，Aalok Mehta，Jacob Menick，Luke Metz，Andrey Mishchenko，Pamela Mishkin，Vinnie
    Monaco，Evan Morikawa，Daniel Mossing，Tong Mu，Mira Murati，Oleg Murk，David Mély，Ashvin
    Nair，Reiichiro Nakano，Rajeev Nayak，Arvind Neelakantan，Richard Ngo，Hyeonwoo Noh，Long
    Ouyang，Cullen O’Keefe，Jakub Pachocki，Alex Paino，Joe Palermo，Ashley Pantuliano，Giambattista
    Parascandolo，Joel Parish，Emy Parparita，Alex Passos，Mikhail Pavlov，Andrew Peng，Adam
    Perelman，Filipe de Avila Belbute Peres，Michael Petrov，Henrique Ponde de Oliveira
    Pinto，Michael Pokorny，Michelle Pokrass，Vitchyr Pong，Tolly Powell，Alethea Power，Boris
    Power，Elizabeth Proehl，Raul Puri，Alec Radford，Jack Rae，Aditya Ramesh，Cameron Raymond，Francis
    Real，Kendra Rimbach，Carl Ross，Bob Rotsted，Henri Roussez，Nick Ryder，Mario Saltarelli，Ted
    Sanders，Shibani Santurkar，Girish Sastry，Heather Schmidt，David Schnurr，John Schulman，Daniel
    Selsam，Kyla Sheppard，Toki Sherbakov，Jessica Shieh，Sarah Shoker，Pranav Shyam，Szymon
    Sidor，Eric Sigler，Maddie Simens，Jordan Sitkin，Katarina Slama，Ian Sohl，Benjamin
    Sokolowsky，Yang Song，Natalie Staudacher，Felipe Petroski Such，Natalie Summers，Ilya
    Sutskever，Jie Tang，Nikolas Tezak，Madeleine Thompson，Phil Tillet，Amin Tootoonchian，Elizabeth
    Tseng，Preston Tuggle，Nick Turley，Jerry Tworek，Juan Felipe Cerón Uribe，Andrea Vallone，Arun
    Vijayvergiya，Chelsea Voss，Carroll Wainwright，Justin Jay Wang，Alvin Wang，Ben Wang，Jonathan
    Ward，Jason Wei，CJ Weinmann，Akila Welihinda，Peter Welinder，Jiayi Weng，Lilian Weng，Matt
    Wiethoff，Dave Willner，Clemens Winter，Samuel Wolrich，Hannah Wong，Lauren Workman，Sherwin
    Wu，Jeff Wu，Michael Wu，Kai Xiao，Tao Xu，Sarah Yoo，Kevin Yu，Qiming Yuan，Wojciech
    Zaremba，Rowan Zellers，Chong Zhang，Marvin Zhang，Shengjia Zhao，Tianhao Zheng，Juntang
    Zhuang，William Zhuk，和 Barret Zoph。2023 年。《GPT-4 技术报告》。arXiv:2303.08774 [cs.CL]
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive
    Simulacra of Human Behavior. arXiv:2304.03442 [cs.HC]'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等人（2023）Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang 和 Michael S. Bernstein. 2023. Generative Agents: Interactive
    Simulacra of Human Behavior. arXiv:2304.03442 [cs.HC]'
- en: 'Vavra et al. (2018) Peter Vavra, Luke J. Chang, and Alan G. Sanfey. 2018. Expectations
    in the Ultimatum Game: Distinct Effects of Mean and Variance of Expected Offers.
    *Frontiers in Psychology* 9 (2018). [https://doi.org/10.3389/fpsyg.2018.00992](https://doi.org/10.3389/fpsyg.2018.00992)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vavra 等人（2018）Peter Vavra, Luke J. Chang 和 Alan G. Sanfey. 2018. Expectations
    in the Ultimatum Game: Distinct Effects of Mean and Variance of Expected Offers.
    *心理学前沿* 9 (2018). [https://doi.org/10.3389/fpsyg.2018.00992](https://doi.org/10.3389/fpsyg.2018.00992)'
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H.
    Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning
    in Large Language Models. *CoRR* abs/2201.11903 (2022). arXiv:2201.11903 [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2022）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi,
    Quoc Le 和 Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large
    Language Models. *CoRR* abs/2201.11903 (2022). arXiv:2201.11903 [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)
- en: Zheng et al. (2023) Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and
    Yu Li. 2023. Progressive-Hint Prompting Improves Reasoning in Large Language Models.
    arXiv:2304.09797 [cs.CL]
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2023）Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li 和 Yu Li.
    2023. Progressive-Hint Prompting Improves Reasoning in Large Language Models.
    arXiv:2304.09797 [cs.CL]
