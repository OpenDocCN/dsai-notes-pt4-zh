- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:50:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:50:39'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Reframe Anything: LLM Agent for Open World Video Reframing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重构任何视频：开放世界视频重构的LLM代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.06070](https://ar5iv.labs.arxiv.org/html/2403.06070)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.06070](https://ar5iv.labs.arxiv.org/html/2403.06070)
- en: '(eccv) Package eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’,
    which is *not* recommended for camera-ready version'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '(eccv) Package eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’,
    which is *not* recommended for camera-ready version'
- en: '¹¹institutetext: Opus AI Research ²²institutetext: Southeast University ³³institutetext:
    National University of Singapore'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹institutetext: Opus AI Research ²²institutetext: Southeast University ³³institutetext:
    National University of Singapore'
- en: '³³email: {gavin.cao, henry.chi, vito.zhu, lirian.su, jay.wu}@opus.pro, yongliangwu@seu.edu.cn,
    weiheng_chi@u.nus.eduJiawang Cao Equal contribution.11    Yongliang Wu^($\star$)
    11    Ziyue Su 11    Jay Wu 11'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '³³email: {gavin.cao, henry.chi, vito.zhu, lirian.su, jay.wu}@opus.pro, yongliangwu@seu.edu.cn,
    weiheng_chi@u.nus.eduJiawang Cao Equal contribution.11    Yongliang Wu^($\star$)
    11    Ziyue Su 11    Jay Wu 11'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The proliferation of mobile devices and social media has revolutionized content
    dissemination, with short-form video becoming increasingly prevalent. This shift
    has introduced the challenge of video reframing to fit various screen aspect ratios,
    a process that highlights the most compelling parts of a video. Traditionally,
    video reframing is a manual, time-consuming task requiring professional expertise,
    which incurs high production costs. A potential solution is to adopt some machine
    learning models, such as video salient object detection, to automate the process.
    However, these methods often lack generalizability due to their reliance on specific
    training data. The advent of powerful large language models (LLMs) open new avenues
    for AI capabilities. Building on this, we introduce Reframe Any Video Agent (RAVA),
    a LLM-based agent that leverages visual foundation models and human instructions
    to restructure visual content for video reframing. RAVA operates in three stages:
    perception, where it interprets user instructions and video content; planning,
    where it determines aspect ratios and reframing strategies; and execution, where
    it invokes the editing tools to produce the final video. Our experiments validate
    the effectiveness of RAVA in video salient object detection and real-world reframing
    tasks, demonstrating its potential as a tool for AI-powered video editing.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 移动设备和社交媒体的普及彻底改变了内容传播方式，短视频变得越来越普遍。这一变化带来了将视频重构以适应各种屏幕宽高比的挑战，这个过程突出了视频中最吸引人的部分。传统的视频重构是一个手动、耗时的任务，需要专业的技能，且生产成本高。一个可能的解决方案是采用一些机器学习模型，如视频显著性对象检测，来自动化这个过程。然而，由于这些方法依赖于特定的训练数据，它们往往缺乏通用性。强大的大型语言模型（LLMs）的出现为AI能力开辟了新的途径。在此基础上，我们介绍了Reframe
    Any Video Agent（RAVA），一个基于LLM的代理，利用视觉基础模型和人工指令来重构视觉内容以实现视频重构。RAVA操作分为三个阶段：感知阶段，解释用户指令和视频内容；规划阶段，确定宽高比和重构策略；执行阶段，调用编辑工具生成最终视频。我们的实验验证了RAVA在视频显著性对象检测和现实世界重构任务中的有效性，展示了其作为AI驱动的视频编辑工具的潜力。
- en: 'Keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '关键词:'
- en: Video Reframing LLM Agent Open World
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 视频重构 LLM代理 开放世界
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The short-form video has emerged as a novel and swiftly expanding mode of content
    dissemination under the rapid evolution of social media and handheld mobile devices [[5](#bib.bib5)].
    Traditional video aspect ratios no longer cater to the convenience of social media
    platform viewing due to varying screen proportions. Consequently, the challenge
    of reconstructing original videos to accommodate different aspect ratios has become
    a burgeoning demand in the field of video editing. The process often involves
    identifying and focusing on the most captivating or crucial elements within the
    current frame. Additionally, from an artistic standpoint, it sometimes necessitates
    zooming in on a specific area of the scene. This technique is referred to as video
    reframing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 短视频在社交媒体和移动设备迅猛发展的背景下，成为了一种新颖且快速扩展的内容传播模式[[5](#bib.bib5)]。由于屏幕比例的不同，传统的视频宽高比已不再适应社交媒体平台的观看需求。因此，重构原始视频以适应不同宽高比的挑战在视频编辑领域成为了一个日益增长的需求。这个过程通常涉及在当前画面中识别和聚焦最吸引人或最关键的元素。此外，从艺术角度看，有时还需要放大场景中的特定区域。这一技术被称为视频重构。
- en: 'Manual video reframing is a labor-intensive and time-consuming task that demands
    the expertise of professional editors. Creating high-quality videos typically
    involves skilled individuals, which in turn escalates the overall cost of production.
    Therefore, some machine learning researchers have started to investigate the possibility
    of automating video reframing. A practical strategy for this involves the application
    of video saliency detection [[41](#bib.bib41), [14](#bib.bib14)] which focuses
    on identifying the most salient, or attention-grabbing, region within a video.
    For example, Christel et al. [[4](#bib.bib4)] utilize a bottom-up visual model
    to generate a saliency map of the current frame and edit the scene to focus on
    these key areas. The aforementioned methods, while partially useful, do not always
    ensure that the parts they extract are complete, which can hinder their use in
    real-world scenarios. In an attempt to address this limitation, research in video
    salient object detection make significant strides. This line of work focuses on
    segmenting the most visually striking objects in a video frame. By doing so, it
    facilitates a more accurate determination of the full scope of the segmented area.
    However, the effectiveness of these models is often compromised by their dependence
    on specific domains of training data. This dependency can limit their generalizability
    and negatively impact their performance and interpretability in diverse applications.
    Furthermore, taking into account that different viewers have varying interests
    in specific parts of a video and that users have diverse requirements for editing,
    as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Reframe Anything:
    LLM Agent for Open World Video Reframing"), it is necessary to design a framework
    capable of flexibly performing video reframing according to user instructions.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '手动视频重新构图是一个劳动密集型且耗时的任务，需要专业编辑的技能。制作高质量的视频通常涉及熟练的人员，这会提高整体制作成本。因此，一些机器学习研究人员开始探讨自动化视频重新构图的可能性。一种实际策略是应用视频显著性检测[[41](#bib.bib41),
    [14](#bib.bib14)]，该方法侧重于识别视频中最显著或最吸引注意的区域。例如，Christel 等人[[4](#bib.bib4)]利用自下而上的视觉模型生成当前帧的显著性图，并编辑场景以关注这些关键区域。虽然上述方法部分有效，但它们提取的部分并不总是完整的，这可能会限制其在实际场景中的使用。为了应对这一局限，视频显著性目标检测研究取得了重大进展。这一研究方向专注于分割视频帧中最具视觉冲击力的物体。通过这样做，可以更准确地确定分割区域的完整范围。然而，这些模型的有效性往往受到对特定领域训练数据的依赖的影响。这种依赖性可能限制了它们的通用性，并对其在不同应用中的性能和可解释性产生负面影响。此外，考虑到不同观众对视频中不同部分的兴趣和用户对编辑的多样化需求，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Reframe Anything: LLM Agent for Open World Video
    Reframing")所示，设计一个能够根据用户指令灵活进行视频重新构图的框架是必要的。'
- en: '![Refer to caption](img/878bc7935976d81dfe9d11a4b611b53e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/878bc7935976d81dfe9d11a4b611b53e.png)'
- en: 'Figure 1: The overview of open world video reframing task. Even within the
    same video, different viewers may focus on different subjects of interest. Therefore,
    it is essential to implement video reframing based on user instructions to achieve
    specific objectives.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 开放世界视频重新构图任务的概述。即使在同一视频中，不同观众可能会关注不同的兴趣点。因此，基于用户指令实施视频重新构图以实现特定目标是至关重要的。'
- en: Recently, the development of powerful large language models (LLMs), such as
    ChatGPT [[25](#bib.bib25)] and GPT-4 [[26](#bib.bib26)], further propel the advancement
    of artificial intelligence. These models demonstrate a formidable capability for
    understanding and generating human language, including the ability to perceive
    and comprehend visual content through textual descriptions of images. For instance,
    they can convey the coordinates of objects within an image, grasping the essence
    of the visual scene, without the need for a direct vision encoder [[21](#bib.bib21)].
    The research community is swiftly moving towards utilizing LLMs as agents that
    can perform complex cognitive functions, which include perception, planning, and
    action execution. Pioneering developments in this area include systems like TaskMatrix [[20](#bib.bib20)],
    AutoGPT [[44](#bib.bib44)], and MetaGPT [[11](#bib.bib11)]. Adding to this innovation,
    the advent of multimodal LLMs such as GPT-4V [[27](#bib.bib27)] usher in a new
    era where LLMs can directly sense visual content. Consequently, some works such
    as AppAgent [[45](#bib.bib45)] and MobileAgent [[40](#bib.bib40)] take this further
    by creating agents that can navigate and control any smartphone application within
    a mobile operating system.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，强大的大型语言模型（LLMs）如 ChatGPT [[25](#bib.bib25)] 和 GPT-4 [[26](#bib.bib26)] 进一步推动了人工智能的发展。这些模型展现了理解和生成自然语言的强大能力，包括通过图像的文本描述感知和理解视觉内容。例如，它们可以传达图像中物体的坐标，抓住视觉场景的本质，而无需直接的视觉编码器
    [[21](#bib.bib21)]。研究界迅速朝着利用 LLMs 执行复杂认知功能的方向发展，包括感知、规划和行动执行。该领域的开创性进展包括 TaskMatrix
    [[20](#bib.bib20)]、AutoGPT [[44](#bib.bib44)] 和 MetaGPT [[11](#bib.bib11)] 等系统。随着多模态
    LLMs 如 GPT-4V [[27](#bib.bib27)] 的出现，开启了 LLMs 直接感知视觉内容的新纪元。因此，像 AppAgent [[45](#bib.bib45)]
    和 MobileAgent [[40](#bib.bib40)] 这样的作品更进一步，创建了能够在移动操作系统中导航和控制任何智能手机应用的代理。
- en: 'Inspired by these explorations into LLMs as agents, in this paper, we introduce
    Reframe Any Video Agent (RAVA), a LLM-based agent designed to execute video reframing
    tasks flexibly based on human instructions. The overall framework can be divided
    into three main stages: perception, planning, and execution. In the perception
    phase, RAVA employs language learning to interpret user directives and video understanding
    to dissect scenes, identify pivotal objects, and generate textual scene descriptions.
    This grasp of content and context informs the subsequent planning stage, where
    the agent meticulously determines aspect ratios, prioritizes object importance,
    configures dynamic layouts, and devises a visual effect strategy that aligns with
    the narrative and user preferences. Finally, in the execution phase, RAVA translates
    the intricate plan into action, orchestrating the reframing with precision, applying
    effects, and arranging content according to a structured execution blueprint,
    all while allowing for feedback loops to refine the output. This comprehensive,
    three-stage process executed by RAVA ensures that videos are not only adapted
    to new formats but also resonate with the intended audience, amplifying the impact
    of content on various platforms.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这些 LLMs 作为代理的探索启发，本文介绍了 Reframe Any Video Agent (RAVA)，一个基于 LLM 的代理，旨在根据人类指令灵活地执行视频重构任务。总体框架可以分为三个主要阶段：感知、规划和执行。在感知阶段，RAVA
    采用语言学习来解读用户指令，运用视频理解来剖析场景、识别关键物体并生成文本场景描述。这种内容和背景的把握为随后的规划阶段提供了依据，在这一阶段，代理仔细确定长宽比，优先考虑物体的重要性，配置动态布局，并制定与叙事和用户偏好相符合的视觉效果策略。最后，在执行阶段，RAVA
    将复杂的计划转化为行动，精准地组织重构，应用效果，并按照结构化的执行蓝图排列内容，同时允许反馈循环来优化输出。这一由 RAVA 执行的全面三阶段过程确保了视频不仅适应新格式，还能与目标受众产生共鸣，放大内容在各种平台上的影响力。
- en: Furthermore, to validate the effectiveness of RAVA, we embark on experiments
    from two dimensions. First, we apply it to the classic computer vision task of
    video salient object detection to verify its ability to accurately execute human
    instructions and its capability for scene comprehension. Second, we employ it
    in real-world video reframing tasks to determine its proficiency in completing
    this practically valuable task. Both quantitative results and user studies highlight
    the advantages of RAVA.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了验证RAVA的有效性，我们从两个维度展开实验。首先，我们将其应用于经典计算机视觉任务——视频显著物体检测，以验证其准确执行人类指令的能力以及场景理解能力。其次，我们将其应用于现实世界的视频重新构图任务，以确定其在完成这一实际有价值任务方面的熟练程度。定量结果和用户研究都突显了RAVA的优势。
- en: 'Our main contributions can be summarized as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献可以总结如下：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce RAVA, a LLM-based agent that is adept at performing video reframing
    tasks in accordance with human directives.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了RAVA，一个基于LLM的代理，擅长根据人类指令执行视频重新构图任务。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Through a carefully crafted perception, planning, and execution framework, RAVA
    is able to effectively utilize the power of existing foundational models to carry
    out human instructions accurately.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过精心设计的感知、规划和执行框架，RAVA能够有效利用现有基础模型的力量，准确执行人类指令。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: By conducting thorough experiments on video salient object detection and real-world
    video reframing cases, we validate the strengths of RAVA, showcasing its promise
    in the field of AI-powered video editing.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过对视频显著物体检测和现实世界视频重新构图案例的全面实验，我们验证了RAVA的优势，展示了其在人工智能视频编辑领域的潜力。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Video Editing
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 视频编辑
- en: Efforts in the field of movie analysis have made significant strides, especially
    in the realm of Audio-Visual Event (AVE) Localization. This particular task requires
    the identification and precise localization of various events within a video [[38](#bib.bib38),
    [9](#bib.bib9)]. Such advancements are beneficial to video editors, as they can
    simplify the editing workflow [[34](#bib.bib34)]. It’s important to note, however,
    that these studies do not provide a means for direct video editing. In addition
    to this line of research, some scholars have directly incorporated machine learning
    techniques into video editing. Argaw et al. introduce a benchmark suite specifically
    designed for video editing tasks, which includes but is not limited to, visual
    effects. This suite also facilitates the automatic organization of footage and
    provides assistance in video assembly [[3](#bib.bib3)]. Furthermore, Rao et al.
    present another benchmark aimed at selecting the best camera angle from multiple
    options, a crucial element in the production of television shows [[32](#bib.bib32)].
    Despite these advancements, current methods do not address the challenge of video
    reframing, which involves highlighting and focusing on the most compelling segments
    of a video. The task of Video Salient Object Detection could potentially resolve
    this issue [[12](#bib.bib12), [46](#bib.bib46), [37](#bib.bib37)]. However, the
    effectiveness of these methods is hampered by their reliance on specific training
    datasets, which limits their generalizability in diverse real-world settings and
    affects their interpretability.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在电影分析领域的努力取得了显著进展，特别是在视听事件（AVE）定位方面。这项任务需要在视频中识别和精确定位各种事件[[38](#bib.bib38),
    [9](#bib.bib9)]。这些进展对视频编辑人员非常有利，因为它们可以简化编辑工作流程[[34](#bib.bib34)]。然而，值得注意的是，这些研究并未提供直接的视频编辑手段。除了这一研究方向外，一些学者直接将机器学习技术应用于视频编辑。Argaw等人介绍了一个专门为视频编辑任务设计的基准套件，包括但不限于视觉效果。该套件还促进了素材的自动组织，并在视频组装中提供了帮助[[3](#bib.bib3)]。此外，Rao等人提出了另一个基准，旨在从多个选项中选择最佳镜头角度，这是电视节目制作中的关键要素[[32](#bib.bib32)]。尽管有这些进展，当前的方法仍未解决视频重新构图的挑战，即突出和关注视频中最引人注目的片段。视频显著物体检测任务有可能解决这一问题[[12](#bib.bib12),
    [46](#bib.bib46), [37](#bib.bib37)]。然而，这些方法的有效性受到其依赖特定训练数据集的限制，这限制了它们在多样化现实世界环境中的适用性，并影响了其可解释性。
- en: 2.2 Open Vocabulary Segmentation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 开放词汇分割
- en: Open Vocabulary Segmentation aims to segment images into meaningful regions
    without being constrained by a predefined set of categories. This approach significantly
    diverges from traditional segmentation methods [[30](#bib.bib30), [18](#bib.bib18),
    [42](#bib.bib42)], which rely on a fixed vocabulary of labels, thus limiting their
    ability to generalize to novel or unseen objects. Seminal works like CLIP [[31](#bib.bib31)]
    and ALIGN [[13](#bib.bib13)] enable segmentation models to identify and categorize
    a diverse array of unseen objects by leveraging natural language descriptions.
    Building on these foundations, LSeg [[19](#bib.bib19)] trains an image encoder
    to create pixel embeddings and uses CLIP [[31](#bib.bib31)] text embeddings as
    the per-pixel classifier. To make use of cheap image-level supervision, OpenSeg [[10](#bib.bib10)]
    employs weakly-supervised grounding loss and random word dropout to foster alignment
    between words and image regions. Although significant progress has been made,
    this field still faces numerous challenges and a scarcity of training data to
    address them. To cope with it, SAM [[16](#bib.bib16)] introduces a pre-trained
    promptable foundation model for image segmentation, showcasing remarkable improvements
    in segmentation performance and adaptability. Recently, HQ-SAM [[15](#bib.bib15)]
    demonstrates an architecture that closely integrates and utilizes the existing
    knowledge within the SAM structure. This approach enables the production of higher-quality
    masks while maintaining zero-shot capabilities. Studies like MedSAM [[22](#bib.bib22)]
    also highlight the significant potential of SAM in the field of medicine.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 开放词汇分割旨在将图像分割成有意义的区域，而不受预定义类别集的限制。这种方法与传统的分割方法显著不同[[30](#bib.bib30), [18](#bib.bib18),
    [42](#bib.bib42)]，传统方法依赖于固定的标签词汇，因此限制了它们对新颖或未见对象的泛化能力。像CLIP[[31](#bib.bib31)]和ALIGN[[13](#bib.bib13)]这样的开创性工作使得分割模型能够通过利用自然语言描述来识别和分类各种未见对象。在这些基础上，LSeg[[19](#bib.bib19)]训练图像编码器以创建像素嵌入，并使用CLIP[[31](#bib.bib31)]文本嵌入作为每个像素的分类器。为了利用低成本的图像级监督，OpenSeg[[10](#bib.bib10)]采用弱监督的基础损失和随机词丢弃来促进词汇与图像区域之间的对齐。尽管取得了显著进展，但该领域仍面临许多挑战以及训练数据的稀缺。为应对这些问题，SAM[[16](#bib.bib16)]引入了一种预训练的可提示基础模型用于图像分割，展示了在分割性能和适应性方面的显著改进。最近，HQ-SAM[[15](#bib.bib15)]展示了一种紧密集成并利用SAM结构中现有知识的架构。这种方法在保持零样本能力的同时，实现了更高质量的掩码。诸如MedSAM[[22](#bib.bib22)]的研究也突显了SAM在医学领域的巨大潜力。
- en: 2.3 LLM Agent
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLM 代理
- en: In recent times, we’ve seen the emergence of valuable frameworks such as AutoGPT [[44](#bib.bib44)],
    MetaGPT [[11](#bib.bib11)], and HuggingGPT [[35](#bib.bib35)], which symbolize
    the trend towards the swift integration of Large Language Models (LLMs) for performing
    intricate tasks. The development of multimodal LLMs, as referenced in works like
    Flamingo [[2](#bib.bib2)], Multimodal [[8](#bib.bib8)], and AudioLM [[36](#bib.bib36)],
    expand the application spectrum of LLMs by enabling them to process diverse inputs
    such as text, images, audio, and video. This advancement allows models to directly
    handle multimodal inputs, moving beyond systems like TaskMatrix [[20](#bib.bib20)],
    which depend on several base models to convert visual information into linguistic
    forms through image captioning or object recognition. Capitalizing on the sophisticated
    perceptual abilities of these models, innovative approaches such as AppAgent [[45](#bib.bib45)],
    MobileAgent [[40](#bib.bib40)], and VisualWebArena [[17](#bib.bib17)] are designed
    to precisely interact with mobile applications and execute web-based tasks. While
    there is a surge in LLM agent research, the domain of video editing is relatively
    untapped. LAVE [[39](#bib.bib39)] serves as an agent for video editing, but its
    functions are limited to following user-defined goals. Our proposed research,
    however, aims to delve deeper into the potential of LLMs to enhance automated
    video reframing capabilities.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们看到了一些有价值的框架，如AutoGPT [[44](#bib.bib44)]、MetaGPT [[11](#bib.bib11)]和HuggingGPT [[35](#bib.bib35)]，这些框架代表了大语言模型（LLMs）快速整合以执行复杂任务的趋势。多模态LLMs的开发，如Flamingo [[2](#bib.bib2)]、Multimodal [[8](#bib.bib8)]和AudioLM [[36](#bib.bib36)]，通过使模型能够处理文本、图像、音频和视频等多种输入，扩大了LLMs的应用范围。这一进展使得模型能够直接处理多模态输入，超越了如TaskMatrix [[20](#bib.bib20)]等依赖多个基础模型通过图像字幕或对象识别将视觉信息转换为语言形式的系统。利用这些模型的复杂感知能力，创新的方法如AppAgent [[45](#bib.bib45)]、MobileAgent [[40](#bib.bib40)]和VisualWebArena [[17](#bib.bib17)]被设计为精确地与移动应用程序互动并执行基于网络的任务。尽管LLM代理研究正在兴起，但视频编辑领域仍然相对未开发。LAVE [[39](#bib.bib39)]作为视频编辑的代理，但其功能仅限于遵循用户定义的目标。然而，我们提出的研究旨在更深入地挖掘LLMs在增强自动化视频重新构框能力方面的潜力。
- en: 3 Reframe Any Video Agent
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 重新构框任何视频代理
- en: '![Refer to caption](img/405ab81b68da51d923508f0c722caba4.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/405ab81b68da51d923508f0c722caba4.png)'
- en: 'Figure 2: The overall workflow of our proposed Reframe Any Video Agent (RAVA).
    RAVA is capable of receiving user dialogue inputs with a language user interface
    (LUI) tailored for reframing tasks, and invokes the grounding function to retrieve
    object information within the video, then reframes the video automatically following
    the request from the users.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们提出的“重新构框任何视频代理（RAVA）”的整体工作流程。RAVA能够接收用户对话输入，使用为重新构框任务量身定制的语言用户界面（LUI），并调用基础功能以检索视频中的对象信息，然后根据用户的请求自动重新构框视频。
- en: 'We present Reframe Any Video Agent (RAVA) for the task of re-framing real-world
    videos, which also incorporates an LLM-based method supporting Language User Interface
    (LUI). Our method undergoes testing in an open-world setting, where scenes in
    videos might feature unseen objects. It is designed to robustly identify every
    object within a scene and discern which is of importance. Subsequently, it reframes
    the original video frames into varying aspect ratios, adapting to the specifications
    demanded by different social media applications or platforms. Furthermore, our
    method supports visual effects in two distinct scenarios: within individual scenes
    (in-scene) and between consecutive scenes (trans-scene). The entire workflow for
    the video reframing task can be accomplished automatically, specifically encompassing
    the following three key steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了“重新构框任何视频代理（RAVA）”，用于现实世界视频的重新构框任务，它还结合了基于LLM的方法，支持语言用户界面（LUI）。我们的方法在开放世界环境中进行了测试，其中视频中的场景可能包含未见过的对象。它被设计为能够稳健地识别场景中的每个对象，并区分哪些对象是重要的。随后，它将原始视频帧重新构框为不同的宽高比，以适应不同社交媒体应用或平台的规格。此外，我们的方法在两个不同场景中支持视觉效果：个别场景内（in-scene）和连续场景之间（trans-scene）。视频重新构框任务的整个工作流程可以自动完成，具体包括以下三个关键步骤：
- en: Object Grounding. Specifically, for a given original video, there will exist
    $M$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对象定位。具体而言，对于给定的原始视频，将存在$M$。
- en: Layout Setting. In scenarios where multiple important objects are present, such
    as in a dialog scene, it is also necessary to determine the layout $\mathbf{L}_{k}\in\{1,2,3,\ldots,N\}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 布局设置。在存在多个重要对象的场景中，如对话场景，还需要确定布局 $\mathbf{L}_{k}\in\{1,2,3,\ldots,N\}$。
- en: 'Effect Adding. Following this, our approach involves adding specific visual
    effects based on the content of the video. We introduce two types of visual effects:
    one that is applied within the current scene $\mathbf{S}_{k}$, such as fading
    in and out.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 效果添加。接下来，我们的方法包括根据视频内容添加特定的视觉效果。我们引入了两种类型的视觉效果：一种是在当前场景 $\mathbf{S}_{k}$ 中应用的效果，如淡入和淡出。
- en: 'Figure [2](#S3.F2 "Figure 2 ‣ 3 Reframe Any Video Agent ‣ Reframe Anything:
    LLM Agent for Open World Video Reframing") demonstrates RAVA’s workflow, three
    phases automate the task of reframing the video, adapting it to various aspect
    ratios required by different social media platforms or specifications, and enhancing
    viewer engagement through strategic layout decisions and visual effects.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2](#S3.F2 "Figure 2 ‣ 3 Reframe Any Video Agent ‣ Reframe Anything: LLM
    Agent for Open World Video Reframing") 展示了RAVA的工作流程，三个阶段自动完成视频的重新构框，将其适配到不同社交媒体平台或规范所需的各种宽高比，并通过战略性布局决策和视觉效果来增强观众的参与度。'
- en: 3.1 Perception
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 感知
- en: 'For the perception phase of the agent, it can be divided into two segments:
    language learning and video understanding. Language learning focuses on comprehending
    the focal points of user interest, while video understanding centers on deciphering
    the content within the video frames.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理的感知阶段，它可以分为两个部分：语言学习和视频理解。语言学习专注于理解用户兴趣的重点，而视频理解则侧重于解读视频画面中的内容。
- en: Dialogue is an aspect where LLMs excel. By configuring prompts, the agent can
    comprehend the search objectives of the user. This process can be viewed as a
    structuring of LUI input information. Specifically, we initiate the process by
    providing RAVA with a video and user interest. This context can include both the
    human-generated text query and the information retrieved from tools, as detailed
    subsequently. We also enable the LLM to output the video topic along with structured
    targets, which are then incorporated into the planning phase.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对话是LLM擅长的一个方面。通过配置提示，代理能够理解用户的搜索目标。这个过程可以视为LUI输入信息的结构化。具体来说，我们通过向RAVA提供视频和用户兴趣来启动这个过程。这个上下文可以包括人类生成的文本查询以及随后从工具中检索的信息。我们还使LLM能够输出视频主题和结构化目标，然后将其纳入规划阶段。
- en: Inspired by films and scripts, we have adopted the shot detection approach to
    understand the entire video at the scene level. The specific methodology involves
    the use of scenedetect¹¹1https://www.scenedetect.com/ to transform the original
    video into a combination of multiple scenes $\{\mathbf{S}_{1},\ldots,\mathbf{S}_{M}\}$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 受到电影和剧本的启发，我们采用了镜头检测方法来在场景级别理解整个视频。具体方法包括使用scenedetect¹¹1https://www.scenedetect.com/
    将原始视频转换为多个场景的组合 $\{\mathbf{S}_{1},\ldots,\mathbf{S}_{M}\}$。
- en: Understanding the video scenes necessitates the use of tools. Firstly, the RAM[[47](#bib.bib47)]
    identifies all objects within the scenes. Subsequently, we employ SAM [[16](#bib.bib16)]
    and Grounded-SAM [[33](#bib.bib33)] to extract the masks of all objects and locations.
    CLIP[[31](#bib.bib31)] is utilized to acquire captions for each object. $\{\mathbf{O}_{1},\ldots,\mathbf{O}_{N}\}$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 理解视频场景需要使用工具。首先，RAM[[47](#bib.bib47)] 识别场景中的所有对象。随后，我们使用SAM [[16](#bib.bib16)]
    和 Grounded-SAM [[33](#bib.bib33)] 提取所有对象和位置的掩模。CLIP[[31](#bib.bib31)] 用于获取每个对象的标题。$\{\mathbf{O}_{1},\ldots,\mathbf{O}_{N}\}$。
- en: 'Ultimately, the visual semantics of each video scene are transformed into a
    textual description. For instance, as demonstrated in Figure [2](#S3.F2 "Figure
    2 ‣ 3 Reframe Any Video Agent ‣ Reframe Anything: LLM Agent for Open World Video
    Reframing"), a scene in the video is described as "Scene-1:Object-1: a boy standing
    in…". This methodology allows for a comprehensive understanding and reframing
    of video content through an LLM agent by combining language comprehension and
    video analysis techniques.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '最终，每个视频场景的视觉语义会被转化为文本描述。例如，如图 [2](#S3.F2 "Figure 2 ‣ 3 Reframe Any Video Agent
    ‣ Reframe Anything: LLM Agent for Open World Video Reframing") 所示，视频中的一个场景被描述为“Scene-1:Object-1:
    a boy standing in…”。这种方法通过结合语言理解和视频分析技术，允许通过LLM代理对视频内容进行全面理解和重新构框。'
- en: 3.2 Planning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 规划
- en: Following perception, which provides a textual description and comprehension
    of the video content, the planning phase is pivotal as it involves devising a
    comprehensive strategy for reframing the video. This strategy should accommodate
    different aspect ratios, highlight imperative objects, and integrate visual effects
    in a cohesive manner that enhances user engagement.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在感知阶段提供文本描述和视频内容理解之后，规划阶段至关重要，因为它涉及制定一个全面的重构视频策略。该策略应适应不同的纵横比，突出重要对象，并以一种增强用户参与感的方式整合视觉效果。
- en: 'In this section, we detail the algorithms and methodologies employed for planning
    in the Reframe Any Video Agent (RAVA). With the insights gained from the perception
    phase, the planning phase consists of the following components:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了在 Reframe Any Video Agent (RAVA) 中用于规划的算法和方法。通过感知阶段获得的见解，规划阶段包括以下几个组成部分：
- en: Aspect Ratio Determination. A critical part of planning is determining the desired
    aspect ratios for the output videos. We consider user preferences, platform requirements,
    and the context of the scenes. A dynamic decision-making process chooses an optimal
    aspect ratio for each scene to ensure the visual content is delivered effectively.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 纵横比确定。规划的一个关键部分是确定输出视频的期望纵横比。我们考虑用户偏好、平台要求以及场景的背景。动态决策过程选择每个场景的最佳纵横比，以确保视觉内容的有效传达。
- en: Object Importance Hierarchy. The multiple objects identified in the perception
    phase need to be prioritized based on their significance relative to the context
    of the scene and user interest. Employing the language comprehension abilities
    of the LLM, we construct an importance hierarchy to aid in selection and layout
    decisions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对象重要性层次。感知阶段识别的多个对象需要根据其相对于场景背景和用户兴趣的重要性进行优先排序。利用 LLM 的语言理解能力，我们构建了一个重要性层次，以帮助选择和布局决策。
- en: 'Dynamic Layout Configuration. Based on the significance and spatial arrangement
    of objects in a scene, we must plan for a layout that maximizes visual appeal
    and narrative coherence. As demonstrated in Figure [3](#S3.F3 "Figure 3 ‣ 3.3
    Execution ‣ 3 Reframe Any Video Agent ‣ Reframe Anything: LLM Agent for Open World
    Video Reframing"), dynamic layout configurations consider dialogue exchanges,
    object interactions, and scene transitions to determine how the objects should
    be framed.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '动态布局配置。根据场景中对象的重要性和空间排列，我们必须规划一个布局，以最大化视觉吸引力和叙事连贯性。如图 [3](#S3.F3 "图 3 ‣ 3.3
    执行 ‣ 3 Reframe Any Video Agent ‣ Reframe Anything: LLM Agent for Open World Video
    Reframing") 所示，动态布局配置考虑了对话交换、对象交互和场景过渡，以确定对象的框架方式。'
- en: Visual Effect Strategy. With visual effect preferences acquired from the perception
    phase, a plan is formulated to apply in-scene and trans-scene effects in a way
    that complements the narrative flow. The challenge lies not only in deciding which
    effects to use but also in determining their intensity and timing to maximize
    impact without distracting from the core content.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉效果策略。根据感知阶段获得的视觉效果偏好，制定计划以在场景内外应用效果，使其与叙事流相辅相成。挑战不仅在于决定使用哪些效果，还在于确定它们的强度和时机，以最大化影响力而不分散核心内容的注意力。
- en: Execution Blueprint. The culmination of the planning phase is an execution blueprint,
    which is a structured set of instructions ready to be parsed for execution. The
    blueprint encapsulates aspect ratios, object arrangement, effect schematics, and
    other relevant parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 执行蓝图。规划阶段的最终成果是执行蓝图，这是一个结构化的指令集，准备好进行解析执行。蓝图包含了纵横比、对象排列、效果示意图和其他相关参数。
- en: Agent Feedback Loop. An optional feedback loop allows the LLM to refine the
    planning based on a review of preliminary reframing results. This review process
    involves generating a low-resolution quick preview of the reframe and running
    it through the LLM for appraisal against the user objectives.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 代理反馈循环。一个可选的反馈循环允许 LLM 基于初步重构结果的审查来改进规划。此审查过程涉及生成重构的低分辨率快速预览，并通过 LLM 进行评估，以确保符合用户目标。
- en: The planning phase incorporates algorithms for scene understanding to create
    a storyboard-like sequence of actions. This storyboard guides the execution phase,
    ensuring a seamless transition from a conceptual model to the practical implementation
    of video reframing. Each scene is treated as a separate module, with transitions
    planned to ensure a cohesive final product that aligns with the intentions of
    user and maximizes viewer engagement. RAVA’s planning phase synthesizes all available
    data into a coherent plan, readying the system for precise execution.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 规划阶段结合了场景理解算法以创建类似故事板的动作序列。这个故事板指导执行阶段，确保从概念模型到视频重新框架的实际实施之间的无缝过渡。每个场景被视为一个独立的模块，过渡经过规划，以确保最终产品的连贯性，符合用户的意图，并最大限度地提高观众的参与度。RAVA的规划阶段将所有可用数据综合成一个连贯的计划，为精确执行做好准备。
- en: 3.3 Execution
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 执行
- en: '![Refer to caption](img/7800266578ad64fa72ecd97953fe1407.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7800266578ad64fa72ecd97953fe1407.png)'
- en: 'Figure 3: The video editing tools in RAVA are described as follows: The first
    line is about ‘Layout settings’, where ‘L’ determines the number of selected objects
    in the video. The second line is ‘Effect In-scene’, represents the visual effects
    within a scene, divided into ‘Zoom in’ and ‘Zoom out’. The third line is ‘Effect
    Trans-scene’, indicates the visual effects for scene transitions, divided into
    ‘Fade in’ and ‘Fade out’.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：RAVA中的视频编辑工具描述如下：第一行是‘布局设置’，其中‘L’决定视频中选定的对象数量。第二行是‘效果场景’，表示场景中的视觉效果，分为‘放大’和‘缩小’。第三行是‘效果过渡’，指示场景过渡的视觉效果，分为‘渐入’和‘渐出’。
- en: The agent then proceeds to carry out the reframe task according to the plan
    generated before. During the execution phase, regular expression matching is employed
    to extract structured execution steps from the plan. These structured texts correspond
    to specific executable functions. We generate a JSON file for each video represented
    as $\{\mathbf{S}_{1},\ldots,\mathbf{S}_{M}\}$, This paper presents several simple
    implementations of special effects and argues that our framework is extensible.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代理程序根据之前生成的计划执行重新框架任务。在执行阶段，使用正则表达式匹配从计划中提取结构化执行步骤。这些结构化文本对应于特定的可执行功能。我们为每个视频生成一个表示为$\{\mathbf{S}_{1},\ldots,\mathbf{S}_{M}\}$的JSON文件。本文展示了几种简单的特效实现，并论证了我们的框架具有扩展性。
- en: For instance, if the layout is set to 2, it will choose the two objects as the
    main subjects of the video and arrange them vertically. If the in-scene visual
    effects $\mathbf{E}_{in}$, a transition effect will also be inserted at the end
    of the current scene.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果布局设置为2，它将选择两个对象作为视频的主要主题，并将其垂直排列。如果场景中的视觉效果$\mathbf{E}_{in}$，也将在当前场景结束时插入过渡效果。
- en: 4 Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: To validate the effectiveness of the Reframe Any Video Agent (RAVA) we proposed,
    we conduct evaluations through two principal tasks. In the first task, we employ
    the model to address a significant challenge in the field of video understanding,
    namely video salient object detection. This involves segmenting the most visually
    prominent objects as perceived by human vision. In the second task, we apply the
    model to tackle the task of video reframing. This process involves adjusting the
    video frame to focus on the most important elements, enhancing the overall composition
    and storytelling aspect of the visual content.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们提出的Reframe Any Video Agent (RAVA)的有效性，我们通过两个主要任务进行评估。在第一个任务中，我们使用该模型解决视频理解领域中的一个重大挑战，即视频显著物体检测。这涉及到将人眼视觉感知到的最显著对象进行分割。在第二个任务中，我们应用该模型来解决视频重新框架的任务。这个过程涉及调整视频帧，以聚焦于最重要的元素，提升视觉内容的整体构图和讲故事的方面。
- en: 'Table 1: Our experimental results for Video Salient Object Detection task.
    The abbreviation ‘SD’ denotes ‘Scene Detection’.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们在视频显著物体检测任务中的实验结果。缩写‘SD’表示‘场景检测’。
- en: '| Method | SD | DAVIS[16] | FBMS |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SD | DAVIS[16] | FBMS |'
- en: '| --- | --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $\alpha_{1}$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha_{1}$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| UPL | 5 | 5 | .0390 | .8025 | .9183 | .8426 | .0850 | .6651 | .8513 | .7439
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| UPL | 5 | 5 | .0390 | .8025 | .9183 | .8426 | .0850 | .6651 | .8513 | .7439
    |'
- en: '| A2S-v2 | .0663 | .4858 | .5786 | .5817 | .0851 | .6444 | .8366 | .7004 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| A2S-v2 | .0663 | .4858 | .5786 | .5817 | .0851 | .6444 | .8366 | .7004 |'
- en: '| Ours | .0501 | .7025 | .8219 | .7795 | .1015 | .5721 | .7532 | .6643 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | .0501 | .7025 | .8219 | .7795 | .1015 | .5721 | .7532 | .6643 |'
- en: '| UPL | 5 | 30 | .0367 | .8127 | .9275 | .8481 | .0844 | .6673 | .8458 | .7527
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| UPL | 5 | 30 | .0367 | .8127 | .9275 | .8481 | .0844 | .6673 | .8458 | .7527
    |'
- en: '| A2S-v2 | .0638 | .5046 | .5929 | .5926 | .0832 | .6406 | .8288 | .7054 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| A2S-v2 | .0638 | .5046 | .5929 | .5926 | .0832 | .6406 | .8288 | .7054 |'
- en: '| Ours | .0419 | .6727 | .8177 | .7680 | .1148 | .5446 | .7131 | .6422 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | .0419 | .6727 | .8177 | .7680 | .1148 | .5446 | .7131 | .6422 |'
- en: '| UPL | 10 | 5 | .0381 | .8009 | .9210 | .8361 | .0848 | .6670 | .8615 | .7373
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| UPL | 10 | 5 | .0381 | .8009 | .9210 | .8361 | .0848 | .6670 | .8615 | .7373
    |'
- en: '| A2S-v2 | .0640 | .4907 | .5723 | .5836 | .0900 | .6299 | .8446 | .6836 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| A2S-v2 | .0640 | .4907 | .5723 | .5836 | .0900 | .6299 | .8446 | .6836 |'
- en: '| Ours | .0506 | .7126 | .8256 | .7804 | .1313 | .5128 | .6776 | .6089 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | .0506 | .7126 | .8256 | .7804 | .1313 | .5128 | .6776 | .6089 |'
- en: 4.1 Video Salient Object Detection
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 视频显著目标检测
- en: 4.1.1 Datasets.
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 数据集。
- en: Our approach is scrutinized through the lens of two widely used datasets, DAVIS[16]
    [[29](#bib.bib29)] and FBMS [[24](#bib.bib24)]. The former consists of 50 videos,
    amounting to 3,455 annotated frames. The latter dataset incorporates 33 supplementary
    video sequences, collectively characterized by 720 annotated frames.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法通过两个广泛使用的数据集进行检验，DAVIS[16] [[29](#bib.bib29)] 和 FBMS [[24](#bib.bib24)]。前者包含50个视频，总计3,455个标注帧。后者的数据集包含33个附加的视频序列，总共具有720个标注帧。
- en: 4.1.2 Metrics.
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 指标。
- en: Four widely used evaluation metrics are employed to assess the performance,
    including Mean Absolute Error (MAE) [[28](#bib.bib28)], F-measure ($F_{\beta}$) [[6](#bib.bib6)].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 四种广泛使用的评估指标被用来评估性能，包括平均绝对误差（MAE）[[28](#bib.bib28)]，F-measure（$F_{\beta}$）[[6](#bib.bib6]]。
- en: 4.1.3 Settings.
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 设置。
- en: We composite the frames into videos at 30 frames per second. Then, The scene
    detection method is applied for video splitting. To minimize the possibility of
    a scene detection result of 0 and to reduce the number of scenes detected under
    each category, we choose the parameter combination of a lower threshold called
    $\alpha_{1}$ in the perception phase generates only a caption and a mask. This
    is because over-adjusting the frame can lead to an undesirable jittery effect,
    which can negatively impact the overall viewing experience.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将帧合成视频，每秒30帧。然后，应用场景检测方法进行视频切分。为了最小化场景检测结果为0的可能性，并减少每个类别下检测到的场景数量，我们选择了一个较低阈值的参数组合，称为感知阶段的$\alpha_{1}$，它仅生成一个说明和一个掩模。这是因为过度调整帧可能导致不良的抖动效果，这会对整体观看体验产生负面影响。
- en: 4.1.4 Results.
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 结果。
- en: '![Refer to caption](img/fa2300dcdcec20d376bc8736e338b36b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa2300dcdcec20d376bc8736e338b36b.png)'
- en: 'Figure 4: The qualitative results on two video salient object detection datasets
    in comparison with two state-of-the-art methods. The findings indicate that RAVA
    copes well even in the presence of occlusions and distractions, and at times,
    it even surpasses the results of human annotations.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：与两种最先进方法相比的两种视频显著目标检测数据集上的定性结果。结果表明，RAVA即使在存在遮挡和干扰的情况下也表现良好，有时甚至超过了人工标注的结果。
- en: 'We compare RAVA with the current state-of-the-art video salient object detection
    methods, namely UPL [[43](#bib.bib43)] and A2S-v2 [[48](#bib.bib48)]. As can be
    seen in Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ Reframe Anything: LLM Agent
    for Open World Video Reframing"), under various settings of scene detection parameters,
    our approach consistently achieves competitive outcomes. It is noteworthy that
    our framework is not specifically designed for the task of video salient object
    detection. The fact that it achieves competitive results is sufficient to demonstrate
    the effectiveness of RAVA. To further analyze our results, we present a subset
    of visualized results in Figure [4](#S4.F4 "Figure 4 ‣ 4.1.4 Results. ‣ 4.1 Video
    Salient Object Detection ‣ 4 Experiments ‣ Reframe Anything: LLM Agent for Open
    World Video Reframing"):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将RAVA与当前最先进的视频显著目标检测方法进行比较，即UPL [[43](#bib.bib43)] 和 A2S-v2 [[48](#bib.bib48]]。如表[1](#S4.T1
    "表 1 ‣ 4 实验 ‣ Reframe Anything: LLM Agent for Open World Video Reframing")所示，在不同的场景检测参数设置下，我们的方法始终取得了竞争性的结果。值得注意的是，我们的框架并不是专门为视频显著目标检测任务设计的。能够取得竞争性的结果已经足以证明RAVA的有效性。为了进一步分析我们的结果，我们在图[4](#S4.F4
    "图 4 ‣ 4.1.4 结果 ‣ 4.1 视频显著目标检测 ‣ 4 实验 ‣ Reframe Anything: LLM Agent for Open World
    Video Reframing")中展示了一部分可视化结果：'
- en: (a) RAVA is capable of precisely segmenting the entirety of the Blackswan instance;
    however, A2S-v2 fails to provide a complete mask, and UPL erroneously segments
    a part of the background as if it were part of the Blackswan.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RAVA能够精确分割出整个BlackSwan实例；然而，A2S-v2未能提供完整的掩模，而UPL错误地将背景的一部分分割为BlackSwan的一部分。
- en: (b) RAVA maintains accurate segmentation even when objects are occluded, whereas
    A2S-v2, despite attempting to address occlusions, incorrectly segments parts of
    the occluding item; UPL, on the other hand, overlooks the occlusion altogether.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (b) RAVA即使在物体被遮挡的情况下也能保持准确的分割，而A2S-v2尽管尝试解决遮挡问题，但错误地分割了遮挡物的一部分；而UPL则完全忽视了遮挡。
- en: (c) When confronted with the distraction of other objects in the scene, both
    A2S-v2 and UPL fail to respond accurately, while RAVA can precisely identify the
    salient object.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 当面对场景中其他物体的干扰时，A2S-v2和UPL都未能准确响应，而RAVA可以准确识别显著物体。
- en: (d) This example better demonstrates the robust capability of RAVA, which, through
    a powerful segmentation model, can perceive and achieve results that are more
    accurate than human annotations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 这个例子更好地展示了RAVA的强大能力，通过强大的分割模型，它可以感知并获得比人工标注更准确的结果。
- en: 4.1.5 Bad Cases.
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 不佳案例。
- en: '![Refer to caption](img/37c7f9ed7496a8556acfac595e16fa63.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37c7f9ed7496a8556acfac595e16fa63.png)'
- en: 'Figure 5: The cases where using RAVA might fail include situations (a) and
    (b), where it struggles to effectively handle composite objects and clusters of
    closely situated objects. However, in cases (c) and (d), despite the differences
    in perception of salient objects, we consider the segmentation results to be acceptable.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 使用RAVA可能失败的情况包括（a）和（b），在这些情况下，它在处理复合物体和紧密相邻的物体群时表现不佳。然而，在（c）和（d）中，尽管对显著物体的感知存在差异，我们认为分割结果是可以接受的。'
- en: 'Even though RAVA exhibits commendable performance across most scenarios, there
    are still some bad cases. We believe these instances contribute to its inability
    to reach state-of-the-art status in the task of video salient object detection.
    These results are illustrated in Figure [5](#S4.F5 "Figure 5 ‣ 4.1.5 Bad Cases.
    ‣ 4.1 Video Salient Object Detection ‣ 4 Experiments ‣ Reframe Anything: LLM Agent
    for Open World Video Reframing"):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管RAVA在大多数场景中表现出色，但仍存在一些不佳的情况。我们认为这些实例导致其在视频显著对象检测任务中无法达到最先进的水平。这些结果在图 [5](#S4.F5
    "图 5 ‣ 4.1.5 不佳案例。 ‣ 4.1 视频显著对象检测 ‣ 4 实验 ‣ Reframe Anything: LLM Agent for Open
    World Video Reframing") 中进行了说明：'
- en: (a) In this case, RAVA can segment the person riding the bicycle with relative
    precision, but it fails to recognize that the bicycle and the person constitute
    a single instance, therefore not achieving an optimal result.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在这种情况下，RAVA能够相对准确地分割骑自行车的人，但未能识别出自行车和人构成一个单一实例，因此未能取得最佳结果。
- en: (b) When multiple objects close to each other that should be considered as one
    instance, RAVA struggles to include the other objects within the segmentation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 当多个接近的物体应该被视为一个实例时，RAVA在分割中很难包含其他物体。
- en: (c) RAVA segments the black cat in the image as the salient object, while ignoring
    the two birds that are farther away. We believe this result is reasonable, as
    perspectives on what constitutes a salient object can vary. This process still
    demonstrates the visual understanding of RAVA.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (c) RAVA将图像中的黑猫分割为显著物体，同时忽略了距离较远的两只鸟。我们认为这个结果是合理的，因为对显著物体的定义可能有所不同。这个过程仍然展示了RAVA的视觉理解能力。
- en: (d) Although the GT provided in this image depicts the person and the horse
    as a single entity, the fences are also prominently marked in blue, making it
    reasonable to consider the fences as the salient objects. This scenario is similarly
    observed in the mask produced by the UPL, which segments out both the fences and
    the horse.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 尽管这张图像中的GT将人和马描绘为一个整体，但围栏也被显著标记为蓝色，因此可以合理地将围栏视为显著对象。这种情况在UPL生成的掩模中也有类似的观察结果，UPL将围栏和马都分割了出来。
- en: 4.1.6 Ablation Study.
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6 消融研究。
- en: To further evaluate the effectiveness of RAVA in a single-language modality,
    we substituted the LLM with GPT-4 [[26](#bib.bib26)], which lacks multimodal perception
    capabilities. Apart from omitting visual inputs in the perception, we maintain
    all other settings unchanged.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为进一步评估RAVA在单一语言模式下的有效性，我们用不具备多模态感知能力的GPT-4 [[26](#bib.bib26)] 替代了LLM。除了在感知中省略视觉输入外，我们保持所有其他设置不变。
- en: 'Table.[2](#S4.T2 "Table 2 ‣ 4.1.6 Ablation Study. ‣ 4.1 Video Salient Object
    Detection ‣ 4 Experiments ‣ Reframe Anything: LLM Agent for Open World Video Reframing")
    reveals that even in the absence of direct visual information, GPT-4 is still
    capable of delivering commendable performance on two datasets based on the textual
    descriptions provided. This underscores the robust transferability of RAVA. However,
    considering the existing gap, it is necessary to use a LLM with strong visual
    perception capabilities.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表格。[2](#S4.T2 "表格 2 ‣ 4.1.6 消融研究。 ‣ 4.1 视频显著目标检测 ‣ 4 实验 ‣ 重新框架任何内容：面向开放世界的视频重新框架的LLM代理")
    显示，即使在缺乏直接视觉信息的情况下，GPT-4 仍然能够在两个数据集上根据提供的文本描述展现出可圈可点的性能。这突显了RAVA的强大迁移能力。然而，考虑到现有的差距，有必要使用具有强大视觉感知能力的LLM。
- en: 'Table 2: Our experimental results for Video Salient Object Detection task on
    single-modality LLM. The abbreviation ‘SD’ denotes ‘Scene Detection’.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：我们在单一模态LLM上的视频显著目标检测任务的实验结果。缩写‘SD’表示‘场景检测’。
- en: '| LLM | SD | DAVIS[16] | FBMS |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LLM | SD | DAVIS[16] | FBMS |'
- en: '| --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $\alpha_{1}$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha_{1}$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4 | 5 | 5 | .0831 | .6497 | .7885 | .7395 | .1294 | .4953 | .6943 | .6125
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 5 | 5 | .0831 | .6497 | .7885 | .7395 | .1294 | .4953 | .6943 | .6125
    |'
- en: '| 5 | 30 | .0548 | .6163 | .7930 | .7422 | .1642 | .4856 | .6670 | .5915 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 30 | .0548 | .6163 | .7930 | .7422 | .1642 | .4856 | .6670 | .5915 |'
- en: '| 10 | 5 | .0690 | .6432 | .7913 | .7454 | .1307 | .4931 | .6684 | .6069 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 5 | .0690 | .6432 | .7913 | .7454 | .1307 | .4931 | .6684 | .6069 |'
- en: 4.2 Video Reframing
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 视频重新框架
- en: 4.2.1 Settings.
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 设置。
- en: 'To assess the video editing capabilities of RAVA in the wild, a user study
    is conducted with 12 participants. Edited versions of 5 videos are created using
    three reframe methodologies in addition to RAVA:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估RAVA在实际环境中的视频编辑能力，进行了一个包括12名参与者的用户研究。使用三种重新框架方法以及RAVA创建了5个视频的编辑版本：
- en: •
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Editor: This method involves a professional video editor (experience >3 years)
    who manually reframe the videos.'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编辑器：该方法涉及一个专业的视频编辑师（经验 >3 年）手动重新框架视频。
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adobe: This method is based on the results obtained by ordinary users utilizing
    the reframe tool in Adobe Premiere Pro to adjust the videos, following the instructions²²2https://helpx.adobe.com/premiere-pro/using/auto-reframe.html.'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Adobe：该方法基于普通用户使用Adobe Premiere Pro中的重新框架工具调整视频所获得的结果，遵循说明²²2https://helpx.adobe.com/premiere-pro/using/auto-reframe.html。
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Center Cut: This method selects the center point of the video, maintaining
    a 9:16 aspect ratio, with the width unchanged.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中心裁剪：该方法选择视频的中心点，保持9:16的纵横比，宽度不变。
- en: 'To minimize the impact of the video itself and to maintain an element of unbiased
    evaluation by users, the open caption tool³³3https://www.opus.pro/tools/opusclip-captions
    is employed to add captions for each video. After watching the original video,
    each participant views the 4 edited versions in a random sequence. The participants
    review all reframed videos, and they are unaware of the editing methods employed
    for each video. This arrangement led to a comprehensive experimental design involving
    5 (number of videos) × 12 (users) × 4 (editing strategies). Users are required
    to compare the reframed version of a video with the original and provide a rating
    on a scale from 0 to 5 for each of the attributes. These attributes were inspired
    by studies on video re-positioning[[23](#bib.bib23)], and it’s important to note
    that, although video reframing and video repositioning differ technically, both
    aim to direct the attention of the viewer to the focal scene events within given
    rendering constraints. Thus, some of the questions used to assess methods of video
    re-positioning are also applicable to video editing. Our attributes of interest
    include:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化视频本身的影响，并保持用户评价的无偏性，使用了开放字幕工具³³3https://www.opus.pro/tools/opusclip-captions
    为每个视频添加字幕。在观看原始视频后，每位参与者以随机顺序观看4个编辑版本。参与者查看所有重新框架的视频，而对每个视频所采用的编辑方法一无所知。这种安排导致了一个全面的实验设计，包括5（视频数量）
    × 12（用户） × 4（编辑策略）。用户需要将视频的重新框架版本与原始视频进行比较，并对每个属性进行0到5的评分。这些属性受到视频重新定位研究[[23](#bib.bib23)]的启发，需要注意的是，尽管视频重新框架和视频重新定位在技术上有所不同，但两者都旨在在给定的渲染约束内将观众的注意力引导到重点场景事件上。因此，用于评估视频重新定位方法的一些问题也适用于视频编辑。我们关注的属性包括：
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Content Preservation
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内容保留
- en: –
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: (Relevance) How well does the reframed video maintain the key elements of the
    original content?
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (相关性) 重新构图的视频在多大程度上保留了原始内容的关键元素？
- en: –
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: (Completeness) Are important parts of the scene, especially the main subjects,
    preserved in the reframed version?
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (完整性) 重要的场景部分，特别是主要主题，在重新构图版本中是否得到保留？
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Continuity and Consistency
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连续性和一致性
- en: –
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: (Continuity) Does the sequence of shots follow a logical temporal order without
    visually jarring jumps?
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (连续性) 镜头的顺序是否按照逻辑时间顺序排列，没有出现视觉上的突兀跳跃？
- en: –
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: (Consistency) Does the reframing maintain the original style and mood of the
    video?
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (一致性) 重新构图是否保持了视频的原始风格和氛围？
- en: •
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: User Experience
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户体验
- en: –
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: (Satisfaction) Overall, how satisfied are the users with the reframed video?
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (满意度) 用户对重新构图的视频整体满意度如何？
- en: –
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: (Usability) Overall, are you willing to post this video on your social platform?
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (可用性) 总体而言，你愿意将这个视频发布到你的社交平台吗？
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Technical Quality
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术质量
- en: –
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: (Resolution and Clarity) Is the video crisp and clear, without degradation from
    cropping or zooming?
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (分辨率和清晰度) 视频是否清晰明了，没有因为裁剪或缩放而降质？
- en: –
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: (Stability) Does the video maintain stability, without introducing shakiness
    due to reframing?
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (稳定性) 视频是否保持稳定，没有因为重新构图而引入抖动？
- en: '![Refer to caption](img/b35e1ba607ebcfd2ced462401c55682d.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b35e1ba607ebcfd2ced462401c55682d.png)'
- en: (a) CP
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CP
- en: '![Refer to caption](img/7e93c1da7414b56a8039af324bd687cc.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7e93c1da7414b56a8039af324bd687cc.png)'
- en: (b) CC
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (b) CC
- en: '![Refer to caption](img/d4b38e603100ae8121abf761d4286907.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d4b38e603100ae8121abf761d4286907.png)'
- en: (c) UE
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (c) UE
- en: '![Refer to caption](img/f9f740bcb74673c5cc2fdff3e3da3b01.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f9f740bcb74673c5cc2fdff3e3da3b01.png)'
- en: (d) TQ
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: (d) TQ
- en: 'Figure 6: Overall scores of the individual attributes: Content Preservation
    (CP), Continuity and Consistency (CC), User Experience (UE), and Technical Quality
    (TQ).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 各个属性的总体评分：内容保留 (CP)、连续性和一致性 (CC)、用户体验 (UE) 和技术质量 (TQ)。'
- en: 4.2.2 Results.
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 结果。
- en: 'As seen in the results presented in Figure [6](#S4.F6 "Figure 6 ‣ 4.2.1 Settings.
    ‣ 4.2 Video Reframing ‣ 4 Experiments ‣ Reframe Anything: LLM Agent for Open World
    Video Reframing") and outlined in our experimental data, the traditional editing
    method, referred to as ‘Editor’, received the highest overall mean score of $4.34$
    in Content Preservation, reflecting its limited ability to identify and maintain
    critical video elements.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[6](#S4.F6 "Figure 6 ‣ 4.2.1 Settings. ‣ 4.2 Video Reframing ‣ 4 Experiments
    ‣ Reframe Anything: LLM Agent for Open World Video Reframing")中所示以及我们实验数据中所概述的那样，传统的编辑方法，即“编辑器”，在内容保留方面获得了$4.34$的最高总体平均分，这反映了其在识别和保留关键视频元素方面的局限性。'
- en: The variability in scores, as indicated by the interquartile range in the boxplot,
    further substantiates the need for an intelligent and context-aware re-framing
    technique. Future work could explore enhancing the object identification and importance
    determination algorithms of RAVA to further close the gap between automated and
    professional video editing tools.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如箱线图中所示的评分变异性进一步证实了需要一种智能且具有上下文感知的重新构图技术。未来的工作可以探索增强RAVA的物体识别和重要性确定算法，以进一步缩小自动化与专业视频编辑工具之间的差距。
- en: Besides, we strongly recommend that readers view the edited video included in
    the supplementary materials for a more intuitive understanding.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们强烈建议读者查看补充材料中包含的编辑视频，以获得更直观的理解。
- en: 5 Conclusion
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present a groundbreaking approach in the realm of video reframing by introducing
    Reframe Any Video Agent (RAVA), a sophisticated agent powered by large language
    models (LLMs) that excels in executing video reframing tasks based on human instructions.
    Through a meticulous three-stage process encompassing perception, planning, and
    execution, RAVA showcases its prowess in understanding user directives, dissecting
    scenes, prioritizing objects, configuring layouts, and applying visual effects—all
    while ensuring alignment with narrative and user preferences. Our experiments,
    spanning from traditional computer vision tasks to real-world video reframing
    scenarios, demonstrate the efficacy and promise of RAVA in the domain of AI-driven
    video editing. By validating the capabilities of RAVA through quantitative results
    and user studies, we establish its potential as a transformative tool for enhancing
    content creation and resonating with diverse audiences across various platforms.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种开创性的视频重新构框方法，通过引入Reframe Any Video Agent（RAVA），这是一个由大型语言模型（LLMs）驱动的复杂代理，擅长根据人类指令执行视频重新构框任务。通过一个细致的三阶段过程，包括感知、规划和执行，RAVA展示了其在理解用户指令、解剖场景、优先排序对象、配置布局和应用视觉效果方面的能力，同时确保与叙事和用户偏好保持一致。我们的实验涵盖了从传统计算机视觉任务到现实世界视频重新构框场景，展示了RAVA在AI驱动的视频编辑领域的有效性和前景。通过定量结果和用户研究验证RAVA的能力，我们确立了其作为增强内容创作和与不同平台上的各种观众产生共鸣的变革性工具的潜力。
- en: 5.0.1 Limitations.
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.0.1 限制。
- en: The main constraint of our study stems from its dependence on the underlying
    performance of foundation models. Utilizing a more sophisticated visual model
    may lead to enhanced results. Additionally, exploring how to extend the agent
    to perform editing across the video timeline, thereby achieving the condensation
    of longer content into shorter formats, represents a promising direction for development.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的主要限制来自于对基础模型性能的依赖。使用更复杂的视觉模型可能会获得更好的结果。此外，探索如何扩展代理在视频时间轴上进行编辑，从而将较长内容压缩成较短格式，代表了一个有前景的发展方向。
- en: 5.0.2 Social Impact.
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.0.2 社会影响。
- en: The research presented in this paper involves the utilization of publicly available
    video content sourced from YouTube. We have ensured that all video materials employed
    in our study adhere to the platform’s terms of service regarding non-commercial,
    research-based usage.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文中涉及的研究使用了来自YouTube的公开视频内容。我们确保所有在研究中使用的视频材料符合平台关于非商业、研究用途的服务条款。
- en: References
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Achanta, R., Hemami, S., Estrada, F., Susstrunk, S.: Frequency-tuned salient
    region detection. In: 2009 IEEE conference on computer vision and pattern recognition.
    pp. 1597–1604\. IEEE (2009)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Achanta, R., Hemami, S., Estrada, F., Susstrunk, S.：频率调谐显著区域检测。发表于：2009年IEEE计算机视觉与模式识别会议。第1597–1604页。IEEE（2009）'
- en: '[2] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
    K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language
    model for few-shot learning. Advances in Neural Information Processing Systems
    35, 23716–23736 (2022)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
    K., Mensch, A., Millican, K., Reynolds, M., 等：Flamingo：用于少量学习的视觉语言模型。神经信息处理系统进展35，23716–23736（2022）'
- en: '[3] Argaw, D.M., Heilbron, F.C., Lee, J.Y., Woodson, M., Kweon, I.S.: The anatomy
    of video editing: a dataset and benchmark suite for ai-assisted video editing.
    In: European Conference on Computer Vision. pp. 201–218\. Springer (2022)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Argaw, D.M., Heilbron, F.C., Lee, J.Y., Woodson, M., Kweon, I.S.：视频编辑的解剖学：用于AI辅助视频编辑的数据集和基准套件。发表于：欧洲计算机视觉会议。第201–218页。Springer（2022）'
- en: '[4] Chamaret, C., Le Meur, O.: Attention-based video reframing: Validation
    using eye-tracking. In: 2008 19th International Conference on Pattern Recognition.
    pp. 1–4\. IEEE (2008)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Chamaret, C., Le Meur, O.：基于注意力的视频重新构框：使用眼动追踪验证。发表于：2008年第19届国际模式识别大会。第1–4页。IEEE（2008）'
- en: '[5] Cochrane, T.: Mobile social media as a catalyst for pedagogical change.
    In: EdMedia+ innovate learning. pp. 2187–2200\. Association for the Advancement
    of Computing in Education (AACE) (2014)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Cochrane, T.：移动社交媒体作为教育变革的催化剂。发表于：EdMedia+创新学习。第2187–2200页。教育计算机协会（AACE）（2014）'
- en: '[6] Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.: Structure-measure:
    A new way to evaluate foreground maps. In: Proceedings of the IEEE international
    conference on computer vision. pp. 4548–4557 (2017)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.：结构测量：评估前景图的新方法。发表于：IEEE国际计算机视觉会议论文集。第4548–4557页（2017）'
- en: '[7] Fan, D.P., Gong, C., Cao, Y., Ren, B., Cheng, M.M., Borji, A.: Enhanced-alignment
    measure for binary foreground map evaluation. In: Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence. International Joint
    Conferences on Artificial Intelligence Organization (2018)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Fan, D.P., Gong, C., Cao, Y., Ren, B., Cheng, M.M., Borji, A.：用于二进制前景图评估的增强对齐度量。见：第二十七届国际联合人工智能会议论文集。国际联合人工智能组织（2018）'
- en: '[8] Furuta, H., Nachum, O., Lee, K.H., Matsuo, Y., Gu, S.S., Gur, I.: Multimodal
    web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854
    (2023)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Furuta, H., Nachum, O., Lee, K.H., Matsuo, Y., Gu, S.S., Gur, I.：基于指令微调的基础模型的多模态网页导航。arXiv预印本
    arXiv:2305.11854（2023）'
- en: '[9] Geng, T., Wang, T., Duan, J., Cong, R., Zheng, F.: Dense-localizing audio-visual
    events in untrimmed videos: A large-scale benchmark and baseline. In: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22942–22951
    (2023)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Geng, T., Wang, T., Duan, J., Cong, R., Zheng, F.：密集定位未裁剪视频中的音频-视觉事件：一个大规模基准和基线。见：IEEE/CVF计算机视觉与模式识别会议论文集。第22942–22951页（2023）'
- en: '[10] Ghiasi, G., Gu, X., Cui, Y., Lin, T.Y.: Scaling open-vocabulary image
    segmentation with image-level labels. In: European Conference on Computer Vision.
    pp. 540–557\. Springer (2022)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ghiasi, G., Gu, X., Cui, Y., Lin, T.Y.：利用图像级标签扩展开放词汇图像分割。见：欧洲计算机视觉会议。第540–557页。Springer（2022）'
- en: '[11] Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z.,
    Yau, S.K.S., Lin, Z., Zhou, L., et al.: Metagpt: Meta programming for multi-agent
    collaborative framework. arXiv preprint arXiv:2308.00352 (2023)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z.,
    Yau, S.K.S., Lin, Z., Zhou, L., 等：Metagpt：多智能体协作框架的元编程。arXiv预印本 arXiv:2308.00352（2023）'
- en: '[12] Hu, F., Palazzo, S., Salanitri, F.P., Bellitto, G., Moradi, M., Spampinato,
    C., McGuinness, K.: Tinyhd: Efficient video saliency prediction with heterogeneous
    decoders using hierarchical maps distillation. In: Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision. pp. 2051–2060 (2023)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Hu, F., Palazzo, S., Salanitri, F.P., Bellitto, G., Moradi, M., Spampinato,
    C., McGuinness, K.：Tinyhd：利用层次映射蒸馏的异构解码器进行高效视频显著性预测。见：IEEE/CVF计算机视觉应用冬季会议论文集。第2051–2060页（2023）'
- en: '[13] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.,
    Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation
    learning with noisy text supervision. In: International conference on machine
    learning. pp. 4904–4916\. PMLR (2021)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.,
    Sung, Y.H., Li, Z., Duerig, T.：利用噪声文本监督扩展视觉和视觉语言表征学习。见：国际机器学习会议。第4904–4916页。PMLR（2021）'
- en: '[14] Jiang, L., Xu, M., Liu, T., Qiao, M., Wang, Z.: Deepvs: A deep learning
    based video saliency prediction approach. In: Proceedings of the european conference
    on computer vision (eccv). pp. 602–617 (2018)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jiang, L., Xu, M., Liu, T., Qiao, M., Wang, Z.：Deepvs：一种基于深度学习的视频显著性预测方法。见：欧洲计算机视觉会议（eccv）论文集。第602–617页（2018）'
- en: '[15] Ke, L., Ye, M., Danelljan, M., Tai, Y.W., Tang, C.K., Yu, F., et al.:
    Segment anything in high quality. Advances in Neural Information Processing Systems
    36 (2024)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Ke, L., Ye, M., Danelljan, M., Tai, Y.W., Tang, C.K., Yu, F., 等：高质量分割任何东西。神经信息处理系统进展
    36（2024）'
- en: '[16] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L.,
    Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment
    anything (2023)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L.,
    Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.：分割任何东西（2023）'
- en: '[17] Koh, J.Y., Lo, R., Jang, L., Duvvur, V., Lim, M.C., Huang, P.Y., Neubig,
    G., Zhou, S., Salakhutdinov, R., Fried, D.: Visualwebarena: Evaluating multimodal
    agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649 (2024)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Koh, J.Y., Lo, R., Jang, L., Duvvur, V., Lim, M.C., Huang, P.Y., Neubig,
    G., Zhou, S., Salakhutdinov, R., Fried, D.：Visualwebarena：在现实视觉网页任务上评估多模态代理。arXiv预印本
    arXiv:2401.13649（2024）'
- en: '[18] Lambert, J., Liu, Z., Sener, O., Hays, J., Koltun, V.: Mseg: A composite
    dataset for multi-domain semantic segmentation. In: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition. pp. 2879–2888 (2020)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Lambert, J., Liu, Z., Sener, O., Hays, J., Koltun, V.：《Mseg：用于多领域语义分割的复合数据集》。见：IEEE/CVF计算机视觉与模式识别会议论文集。第2879–2888页（2020）'
- en: '[19] Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.: Language-driven
    semantic segmentation (2022)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.：语言驱动的语义分割（2022）'
- en: '[20] Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S.,
    Ji, L., Mao, S., et al.: Taskmatrix. ai: Completing tasks by connecting foundation
    models with millions of apis. arXiv preprint arXiv:2303.16434 (2023)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S.,
    Ji, L., Mao, S., et al.: Taskmatrix.ai: 通过连接基础模型与数百万个 API 完成任务。arXiv 预印本 arXiv:2303.16434
    (2023)'
- en: '[21] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances
    in neural information processing systems 36 (2024)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Liu, H., Li, C., Wu, Q., Lee, Y.J.: 可视化指令调整。神经信息处理系统进展 36 (2024)'
- en: '[22] Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in
    medical images. Nature Communications 15(1),  654 (2024)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: 在医学图像中分割任何东西。自然通讯
    15(1), 654 (2024)'
- en: '[23] Moorthy, K.B., Kumar, M., Subramanian, R., Gandhi, V.: Gazed–gaze-guided
    cinematic editing of wide-angle monocular video recordings. In: Proceedings of
    the 2020 CHI Conference on Human Factors in Computing Systems. pp. 1–11 (2020)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Moorthy, K.B., Kumar, M., Subramanian, R., Gandhi, V.: Gazed–基于注视的宽角单目视频录制的电影编辑。在:
    2020年计算机系统人因会议论文集。第1–11页 (2020)'
- en: '[24] Ochs, P., Malik, J., Brox, T.: Segmentation of moving objects by long
    term video analysis. IEEE transactions on pattern analysis and machine intelligence
    36(6), 1187–1200 (2013)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Ochs, P., Malik, J., Brox, T.: 通过长期视频分析进行运动物体分割。IEEE 图案分析与机器智能学报 36(6),
    1187–1200 (2013)'
- en: '[25] OpenAI: Chatgpt. [https://openai.com/research/chatgpt](https://openai.com/research/chatgpt)
    (2021)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] OpenAI: Chatgpt. [https://openai.com/research/chatgpt](https://openai.com/research/chatgpt)
    (2021)'
- en: '[26] OpenAI: Gpt-4\. [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)
    (2023)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] OpenAI: Gpt-4\. [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)
    (2023)'
- en: '[27] OpenAI: Gpt-4v. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card)
    (2023)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] OpenAI: Gpt-4v. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card)
    (2023)'
- en: '[28] Perazzi, F., Krähenbühl, P., Pritch, Y., Hornung, A.: Saliency filters:
    Contrast based filtering for salient region detection. In: 2012 IEEE conference
    on computer vision and pattern recognition. pp. 733–740\. IEEE (2012)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Perazzi, F., Krähenbühl, P., Pritch, Y., Hornung, A.: 显著性滤波器: 基于对比度的显著区域检测滤波。在:
    2012 IEEE 计算机视觉与模式识别大会。第733–740页。IEEE (2012)'
- en: '[29] Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M.,
    Sorkine-Hornung, A.: A benchmark dataset and evaluation methodology for video
    object segmentation. In: Proceedings of the IEEE conference on computer vision
    and pattern recognition. pp. 724–732 (2016)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M.,
    Sorkine-Hornung, A.: 视频对象分割的基准数据集和评估方法。在: IEEE 计算机视觉与模式识别会议论文集。第724–732页 (2016)'
- en: '[30] Pinheiro, P.O., Collobert, R.: From image-level to pixel-level labeling
    with convolutional networks. In: Proceedings of the IEEE conference on computer
    vision and pattern recognition. pp. 1713–1721 (2015)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Pinheiro, P.O., Collobert, R.: 从图像级到像素级标注的卷积网络。在: IEEE 计算机视觉与模式识别会议论文集。第1713–1721页
    (2015)'
- en: '[31] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning
    transferable visual models from natural language supervision (2021)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: 从自然语言监督中学习可迁移的视觉模型
    (2021)'
- en: '[32] Rao, A., Jiang, X., Wang, S., Guo, Y., Liu, Z., Dai, B., Pang, L., Wu,
    X., Lin, D., Jin, L.: Temporal and contextual transformer for multi-camera editing
    of tv shows. arXiv preprint arXiv:2210.08737 (2022)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Rao, A., Jiang, X., Wang, S., Guo, Y., Liu, Z., Dai, B., Pang, L., Wu,
    X., Lin, D., Jin, L.: 用于电视节目的多摄像头编辑的时间和上下文变换器。arXiv 预印本 arXiv:2210.08737 (2022)'
- en: '[33] Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang,
    X., Chen, Y., Yan, F., et al.: Grounded sam: Assembling open-world models for
    diverse visual tasks. arXiv preprint arXiv:2401.14159 (2024)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang,
    X., Chen, Y., Yan, F., et al.: Grounded sam: 组装开放世界模型以应对多样的视觉任务。arXiv 预印本 arXiv:2401.14159
    (2024)'
- en: '[34] Serrano, A., Sitzmann, V., Ruiz-Borau, J., Wetzstein, G., Gutierrez, D.,
    Masia, B.: Movie editing and cognitive event segmentation in virtual reality video.
    ACM Transactions on Graphics (TOG) 36(4), 1–12 (2017)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Serrano, A., Sitzmann, V., Ruiz-Borau, J., Wetzstein, G., Gutierrez, D.,
    Masia, B.: 虚拟现实视频中的电影编辑和认知事件分割。ACM 图形学交易 (TOG) 36(4), 1–12 (2017)'
- en: '[35] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving
    ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information
    Processing Systems 36 (2024)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.：Hugginggpt：利用ChatGPT及其在**Hugging
    Face**的朋友解决AI任务。神经信息处理系统进展 36 (2024)'
- en: '[36] Shu, F., Zhang, L., Jiang, H., Xie, C.: Audio-visual llm for video understanding.
    arXiv preprint arXiv:2312.06720 (2023)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Shu, F., Zhang, L., Jiang, H., Xie, C.：音频-视觉LLM用于视频理解。arXiv预印本 arXiv:2312.06720
    (2023)'
- en: '[37] Su, Y., Deng, J., Sun, R., Lin, G., Su, H., Wu, Q.: A unified transformer
    framework for group-based segmentation: Co-segmentation, co-saliency detection
    and video salient object detection. IEEE Transactions on Multimedia (2023)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Su, Y., Deng, J., Sun, R., Lin, G., Su, H., Wu, Q.：一个统一的变换器框架用于基于组的分割：共同分割、共同显著性检测和视频显著目标检测。**IEEE多媒体学报**
    (2023)'
- en: '[38] Tian, Y., Shi, J., Li, B., Duan, Z., Xu, C.: Audio-visual event localization
    in unconstrained videos. In: Proceedings of the European conference on computer
    vision (ECCV). pp. 247–263 (2018)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Tian, Y., Shi, J., Li, B., Duan, Z., Xu, C.：音频-视觉事件定位于无约束视频中。发表于：**欧洲计算机视觉会议**
    (ECCV) 论文集。第247–263页 (2018)'
- en: '[39] Wang, B., Li, Y., Lv, Z., Xia, H., Xu, Y., Sodhi, R.: Lave: Llm-powered
    agent assistance and language augmentation for video editing. arXiv preprint arXiv:2402.10294
    (2024)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Wang, B., Li, Y., Lv, Z., Xia, H., Xu, Y., Sodhi, R.：Lave：用于视频编辑的**LLM**驱动的代理辅助和语言增强。arXiv预印本
    arXiv:2402.10294 (2024)'
- en: '[40] Wang, J., Xu, H., Ye, J., Yan, M., Shen, W., Zhang, J., Huang, F., Sang,
    J.: Mobile-agent: Autonomous multi-modal mobile device agent with visual perception.
    arXiv preprint arXiv:2401.16158 (2024)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Wang, J., Xu, H., Ye, J., Yan, M., Shen, W., Zhang, J., Huang, F., Sang,
    J.：Mobile-agent：具有视觉感知的自主多模态移动设备代理。arXiv预印本 arXiv:2401.16158 (2024)'
- en: '[41] Wang, W., Shen, J., Xie, J., Cheng, M.M., Ling, H., Borji, A.: Revisiting
    video saliency prediction in the deep learning era. IEEE transactions on pattern
    analysis and machine intelligence 43(1), 220–237 (2019)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Wang, W., Shen, J., Xie, J., Cheng, M.M., Ling, H., Borji, A.：在深度学习时代重新审视视频显著性预测。**IEEE模式分析与机器智能学报**
    43(1)，220–237 (2019)'
- en: '[42] Xian, Y., Choudhury, S., He, Y., Schiele, B., Akata, Z.: Semantic projection
    network for zero-and few-label semantic segmentation. In: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition. pp. 8256–8265 (2019)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Xian, Y., Choudhury, S., He, Y., Schiele, B., Akata, Z.：用于零样本和少量标签语义分割的语义投影网络。发表于：**IEEE/CVF计算机视觉与模式识别大会**论文集。第8256–8265页
    (2019)'
- en: '[43] Yan, P., Wu, Z., Liu, M., Zeng, K., Lin, L., Li, G.: Unsupervised domain
    adaptive salient object detection through uncertainty-aware pseudo-label learning.
    In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp.
    3000–3008 (2022)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Yan, P., Wu, Z., Liu, M., Zeng, K., Lin, L., Li, G.：通过不确定性感知伪标签学习进行无监督领域自适应显著目标检测。发表于：**AAAI人工智能会议**论文集。第36卷，第3000–3008页
    (2022)'
- en: '[44] Yang, H., Yue, S., He, Y.: Auto-gpt for online decision making: Benchmarks
    and additional opinions. arXiv preprint arXiv:2306.02224 (2023)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Yang, H., Yue, S., He, Y.：Auto-gpt用于在线决策：基准测试和额外意见。arXiv预印本 arXiv:2306.02224
    (2023)'
- en: '[45] Yang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B., Yu, G.: Appagent:
    Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771 (2023)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Yang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B., Yu, G.：Appagent：作为智能手机用户的多模态代理。arXiv预印本
    arXiv:2312.13771 (2023)'
- en: '[46] Yuan, Y., Wang, Y., Wang, L., Zhao, X., Lu, H., Wang, Y., Su, W., Zhang,
    L.: Isomer: Isomerous transformer for zero-shot video object segmentation. In:
    Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 966–976
    (2023)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Yuan, Y., Wang, Y., Wang, L., Zhao, X., Lu, H., Wang, Y., Su, W., Zhang,
    L.：Isomer：用于零样本视频目标分割的同分异构体变换器。发表于：**IEEE/CVF计算机视觉国际会议**论文集。第966–976页 (2023)'
- en: '[47] Zhang, Y., Huang, X., Ma, J., Li, Z., Luo, Z., Xie, Y., Qin, Y., Luo,
    T., Li, Y., Liu, S., et al.: Recognize anything: A strong image tagging model.
    arXiv preprint arXiv:2306.03514 (2023)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Zhang, Y., Huang, X., Ma, J., Li, Z., Luo, Z., Xie, Y., Qin, Y., Luo,
    T., Li, Y., Liu, S., 等：Recognize anything：一个强大的图像标记模型。arXiv预印本 arXiv:2306.03514
    (2023)'
- en: '[48] Zhou, H., Qiao, B., Yang, L., Lai, J., Xie, X.: Texture-guided saliency
    distilling for unsupervised salient object detection. In: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition. pp. 7257–7267 (2023)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Zhou, H., Qiao, B., Yang, L., Lai, J., Xie, X.：基于纹理引导的显著性提取用于无监督显著目标检测。发表于：**IEEE/CVF计算机视觉与模式识别大会**论文集。第7257–7267页
    (2023)'
