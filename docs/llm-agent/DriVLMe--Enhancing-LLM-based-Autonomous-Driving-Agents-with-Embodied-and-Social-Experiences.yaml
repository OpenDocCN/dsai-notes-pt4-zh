- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:45:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social
    Experiences'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DriVLMe：利用具身和社会经验提升基于大语言模型的自动驾驶代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.03008](https://ar5iv.labs.arxiv.org/html/2406.03008)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.03008](https://ar5iv.labs.arxiv.org/html/2406.03008)
- en: Yidong Huang¹ Jacob Sansom¹ Ziqiao Ma¹  Felix Gervits² Joyce Chai¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 黄怡东¹ 雅各布·桑索姆¹ 马紫桥¹  费利克斯·杰维茨² 乔伊斯·柴¹
- en: ¹University of Michigan  ²Army Research Lab
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹密歇根大学  ²美国陆军研究实验室
- en: '[https://sled-group.github.io/driVLMe/](https://sled-group.github.io/driVLMe/)
    Correspondence, contact: marstin@umich.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://sled-group.github.io/driVLMe/](https://sled-group.github.io/driVLMe/)
    通讯联系：marstin@umich.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advancements in foundation models (FMs) have unlocked new prospects in
    autonomous driving, yet the experimental settings of these studies are preliminary,
    over-simplified, and fail to capture the complexity of real-world driving scenarios
    in human environments. It remains under-explored whether FM agents can handle
    long-horizon navigation tasks with free-from dialogue and deal with unexpected
    situations caused by environmental dynamics or task changes. To explore the capabilities
    and boundaries of FMs faced with the challenges above, we introduce DriVLMe, a
    video-language-model-based agent to facilitate natural and effective communication
    between humans and autonomous vehicles that perceive the environment and navigate.
    We develop DriVLMe from both embodied experiences in a simulated environment and
    social experiences from real human dialogue. While DriVLMe demonstrates competitive
    performance in both open-loop benchmarks and closed-loop human studies, we reveal
    several limitations and challenges, including unacceptable inference time, imbalanced
    training data, limited visual understanding, challenges with multi-turn interactions,
    simplified language generation from robotic experiences, and difficulties in handling
    on-the-fly unexpected situations like environmental dynamics and task changes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型（FMs）的最新进展为自动驾驶开辟了新的前景，但这些研究的实验设置仍处于初步阶段，过于简化，未能捕捉人类环境中真实驾驶情境的复杂性。目前尚未充分探讨基础模型代理是否能处理无对话的长期导航任务，并应对由于环境动态或任务变化造成的意外情况。为了探索基础模型面对上述挑战的能力和局限性，我们引入了DriVLMe，这是一种基于视频语言模型的代理，旨在促进人类与能够感知环境并进行导航的自动驾驶车辆之间的自然有效的沟通。我们从模拟环境中的具身经验和真实人类对话中的社会经验两个方面开发了DriVLMe。虽然DriVLMe在开放环路基准测试和闭环人类研究中表现出竞争力，但我们揭示了几个局限性和挑战，包括不可接受的推理时间、不平衡的训练数据、有限的视觉理解、多人轮次交互的挑战、机器人经验生成的简化语言以及处理环境动态和任务变化等即时意外情况的困难。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Autonomous driving (AD) has made remarkable progress in recent years, bringing
    us closer to a future where vehicles can function as our social robot partners
    that navigate roads safely and efficiently with minimal human intervention [[44](#bib.bib44),
    [58](#bib.bib58)]. As these AD agents start to enter our everyday lives, techniques
    to enable effective human-agent dialogue and collaboration become important. The
    ability to communicate with humans through natural language dialogue plays a crucial
    role in ensuring passenger safety, recovering from unexpected situations, gaining
    trustworthiness, and enhancing the overall driving experience [[62](#bib.bib62),
    [27](#bib.bib27)]. In traditional autonomous driving systems and in-vehicle dialogue
    systems, rule-based approaches [[37](#bib.bib37), [2](#bib.bib2), [43](#bib.bib43)]
    have been employed to interpret human instructions and generate appropriate responses.
    However, these systems often struggle to handle the complexity and variability
    of natural language, leading to limited functionality and sub-optimal performance.
    Recently, the paradigm has shifted to data-driven learning-based approaches [[20](#bib.bib20),
    [15](#bib.bib15), [18](#bib.bib18), [6](#bib.bib6)], which offer language-based
    interpretability and promising results in short-horizon tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶（AD）近年来取得了显著进展，使我们更接近于一个未来，在这个未来中，车辆能够作为我们的社交机器人伙伴，安全高效地行驶，且人类干预最小[[44](#bib.bib44),
    [58](#bib.bib58)]。随着这些AD代理开始进入我们的日常生活，使人类与代理之间有效对话和协作的技术变得重要。通过自然语言对话与人类沟通的能力在确保乘客安全、应对意外情况、赢得信任以及提升整体驾驶体验中发挥着关键作用[[62](#bib.bib62),
    [27](#bib.bib27)]。在传统的自动驾驶系统和车载对话系统中，已经采用了基于规则的方法[[37](#bib.bib37), [2](#bib.bib2),
    [43](#bib.bib43)]来解读人类指令并生成适当的响应。然而，这些系统常常难以处理自然语言的复杂性和多样性，导致功能有限和性能不佳。最近，范式已转向数据驱动的学习方法[[20](#bib.bib20),
    [15](#bib.bib15), [18](#bib.bib18), [6](#bib.bib6)]，这些方法提供了基于语言的可解释性，并在短期任务中取得了令人鼓舞的结果。
- en: Advances in foundation models (FMs) like Large Language Models (LLMs) have opened
    up new opportunities, as they demonstrate the ability to perform step-by-step
    reasoning [[60](#bib.bib60)], to understand multimodal data [[71](#bib.bib71),
    [68](#bib.bib68)], to learn from embodied experiences [[33](#bib.bib33), [63](#bib.bib63)],
    and to use external tools [[42](#bib.bib42)]. An increasing number of efforts [[61](#bib.bib61),
    [47](#bib.bib47), [52](#bib.bib52), [64](#bib.bib64), [19](#bib.bib19), [26](#bib.bib26),
    [45](#bib.bib45)] have demonstrated the potential of FMs in the field of autonomous
    driving. However, the experimental setups of these works are preliminary and simplified,
    compared to the real driving scenarios in human environments. One common limitation
    is the lack of an ability to handle long-horizon navigation tasks. Trained on
    simple action-level natural language instructions, these models perform well on
    short-horizon tasks like turn or overtake but fail to understand goal-level instructions
    that require route planning and map knowledge. Also, these systems only focus
    on following individual instructions in a single turn of interaction. Realistic
    interactions with human passengers often involve free-form dialogue, especially
    for collaboratively handling unexpected situations, e.g., those caused by sensor
    limitations, environmental dynamics, or task changes. Without modeling the interaction
    context, these models may fall short of understanding nuanced dialogue and providing
    appropriate responses in human-vehicle interactions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型（FMs）的进展，如大型语言模型（LLMs），开辟了新的机会，因为它们展示了逐步推理的能力[[60](#bib.bib60)]，理解多模态数据[[71](#bib.bib71),
    [68](#bib.bib68)]，从具身体验中学习[[33](#bib.bib33), [63](#bib.bib63)]，以及使用外部工具[[42](#bib.bib42)]。越来越多的努力[[61](#bib.bib61),
    [47](#bib.bib47), [52](#bib.bib52), [64](#bib.bib64), [19](#bib.bib19), [26](#bib.bib26),
    [45](#bib.bib45)]展示了FMs在自动驾驶领域的潜力。然而，这些工作的实验设置相较于人类环境中的真实驾驶场景仍然是初步和简化的。一个常见的局限性是缺乏处理长时间导航任务的能力。这些模型在简单的动作级自然语言指令上表现良好，如转弯或超车，但在需要路线规划和地图知识的目标级指令上却无法理解。此外，这些系统只关注单次交互中的单个指令。与人类乘客的现实互动通常涉及自由形式的对话，特别是在协作处理意外情况时，例如由传感器限制、环境动态或任务变化引起的情况。没有建模交互背景，这些模型可能无法理解细微的对话并在人与车辆交互中提供适当的响应。
- en: To explore the capabilities and boundaries of FMs faced with the challenges
    above, we introduce DriVLMe, a novel video-language-model-based AD agent to facilitate
    natural and effective communication between humans and autonomous vehicles that
    perceive the environment and navigate. Motivated by Hu and Shu [[16](#bib.bib16)],
    our goal is to enhance a language model backend as world and agent models. We
    develop DriVLMe by learning from both embodied experiences in a simulated environment
    and social experiences from real human dialogue. Unlike previous works that only
    focus on open-loop benchmark evaluation using non-interactive datasets such as
    nuScenes [[4](#bib.bib4)] and BDD [[67](#bib.bib67)], we present both open-loop
    and closed-loop experiments in a simulated environment (i.e., CARLA [[10](#bib.bib10)]).
    For open-loop evaluations, we leverage the Situated Dialogue Navigation (SDN) [[27](#bib.bib27)]
    and the BDD-X [[21](#bib.bib21)] benchmarks to assess DriVLMe’s performance in
    generating dialogue responses and physical actions. Our experimental results have
    shown that DriVLMe significantly outperforms previous baselines on SDN by a large
    margin and competes with baselines trained with LLM-augmented data. We further
    conduct closed-loop pilot studies in the CARLA simulation environment. DriVLMe
    is engaged in dialogue to follow language instructions from human subjects in
    the CARLA environment. Our preliminary findings have demonstrated some promising
    abilities of DriVLMe in navigation and re-planning, and on the other hand also
    revealed several limitations including unacceptable inference time, imbalanced
    training data, and low image input resolution. It remains a challenge to support
    multi-turn interactions and language generation from robotic experiences. We hope
    this paper offers a comprehensive perspective view of the strengths and weaknesses
    of foundation models as AD agents, highlighting areas that need future enhancement.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索面对上述挑战时FMs的能力和界限，我们引入了DriVLMe，这是一种新型的视频语言模型基础的自动驾驶（AD）代理，旨在促进人类与感知环境和导航的自动驾驶车辆之间自然且有效的沟通。受到Hu和Shu[[16](#bib.bib16)]的启发，我们的目标是将语言模型后端增强为世界和代理模型。我们通过在模拟环境中的具体现实经验和来自真实人类对话的社会经验来开发DriVLMe。与之前仅关注使用非交互式数据集（如nuScenes[[4](#bib.bib4)]和BDD[[67](#bib.bib67)]）进行开环基准评估的工作不同，我们在模拟环境中（即CARLA[[10](#bib.bib10)]）展示了开环和闭环实验。对于开环评估，我们利用Situative
    Dialogue Navigation (SDN)[[27](#bib.bib27)]和BDD-X[[21](#bib.bib21)]基准来评估DriVLMe在生成对话响应和物理动作方面的表现。我们的实验结果表明，DriVLMe在SDN上显著超越了之前的基线，并与使用LLM增强数据训练的基线进行竞争。我们进一步在CARLA模拟环境中进行闭环初步研究。DriVLMe在CARLA环境中参与对话，以跟随来自人类受试者的语言指令。我们的初步发现展示了DriVLMe在导航和重新规划方面的一些有希望的能力，但另一方面也揭示了包括不可接受的推理时间、不平衡的训练数据和低图像输入分辨率在内的若干局限性。支持多轮交互和基于机器人经验的语言生成仍然是一个挑战。我们希望本文提供了对基础模型作为AD代理的优缺点的全面视角，突显了需要未来改进的领域。
- en: 2 Related Work
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: '![Refer to caption](img/17e00a3915da1f33ca2309dddcdaac63.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/17e00a3915da1f33ca2309dddcdaac63.png)'
- en: 'Figure 1: Overview of the DriVLMe model architecture. DriVLMe is a multimodal
    Large Language Model that consists of (1) A video tokenizer that tokenize the
    input visual history from the CARLA [[10](#bib.bib10)] simulator using a frozen
    CLIP encoder and a linear projection layer, (2) A route planner, a tool designed
    to assist the LLM in finding the shortest path from the agent’s current location
    to another landmark specified by the LLM. (3) The base large language model, which
    receives input in the form of video representations, situated dialogue instructions,
    history of physical actions, and the output planned route from the route planner.
    It predicts dialogue responses to human inputs and physical actions that interact
    with the simulator.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：DriVLMe模型架构概述。DriVLMe是一个多模态大语言模型，由以下部分组成：(1) 一个视频标记器，它使用冻结的CLIP编码器和线性投影层对来自CARLA[[10](#bib.bib10)]模拟器的输入视觉历史进行标记；(2)
    一个路线规划器，它是一个工具，旨在帮助LLM找到从代理当前位置到LLM指定的另一个地标的最短路径；(3) 基础大语言模型，它以视频表示、情境对话指令、物理动作历史以及来自路线规划器的输出规划路线的形式接收输入。它预测对人类输入的对话响应和与模拟器交互的物理动作。
- en: 2.1 Foundation Models for Autonomous Driving
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基础模型在自动驾驶中的应用
- en: Recent research has explored the potential of LLMs in autonomous driving, e.g.,
    by prompt engineering on off-the-shelf LLMs to obtain the driving decisions from
    textual descriptions of the surrounding environment [[46](#bib.bib46), [45](#bib.bib45),
    [61](#bib.bib61)], or by fine-tuning LLMs to predict the next action or plan future
    trajectories [[5](#bib.bib5), [30](#bib.bib30)]. To develop multimodal systems,
    both real and simulated driving videos have been utilized for instruction tuning [[49](#bib.bib49)].
    For example, DriveGPT4 [[64](#bib.bib64)] and RAG-Driver [[69](#bib.bib69)] fine-tuned
    multimodal LLMs on real-world driving videos to predict future throttle and steering
    angles. DriveMLM [[59](#bib.bib59)] and LMDrive [[47](#bib.bib47)] adopted camera
    data and ego-vehicle states from the CARLA simulator. We refer to recent surveys
    and position papers for detailed reviews [[24](#bib.bib24), [7](#bib.bib7), [12](#bib.bib12),
    [65](#bib.bib65)]. We note that the experimental setups in these efforts are preliminary
    and simplified, compared to the real driving scenarios in human environments.
    First, these prior approaches were restricted to single human instructions (or
    even no language input), limiting performance on longer-horizon tasks with back-and-forth
    dialogue and higher-fidelity navigation goals. Furthermore, these prior models
    only focus on using LLMs to predict physical actions and give explanations, ignoring
    their potential to initiate dialogue and generate language responses from robotic
    experiences. Finally, none of these setups consider unexpected situations caused
    by sensor limitations, environmental dynamics, or plan changes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究探索了大型语言模型（LLMs）在自动驾驶中的潜力，例如，通过对现成LLMs进行提示工程以从周围环境的文本描述中获取驾驶决策[[46](#bib.bib46),
    [45](#bib.bib45), [61](#bib.bib61)]，或通过微调LLMs来预测下一步行动或规划未来轨迹[[5](#bib.bib5), [30](#bib.bib30)]。为了开发多模态系统，已利用真实和模拟驾驶视频进行指令调整[[49](#bib.bib49)]。例如，DriveGPT4[[64](#bib.bib64)]和RAG-Driver[[69](#bib.bib69)]在真实世界驾驶视频上微调了多模态LLMs，以预测未来的油门和转向角度。DriveMLM[[59](#bib.bib59)]和LMDrive[[47](#bib.bib47)]采用了来自CARLA模拟器的摄像头数据和自车状态。我们参考了最近的调查和立场论文以获得详细的回顾[[24](#bib.bib24),
    [7](#bib.bib7), [12](#bib.bib12), [65](#bib.bib65)]。我们注意到，与人类环境中的真实驾驶场景相比，这些努力中的实验设置是初步和简化的。首先，这些先前的方法限制于单一的人类指令（甚至没有语言输入），这限制了它们在长时间对话和高保真导航目标上的表现。此外，这些先前的模型只专注于使用LLMs预测物理行动和给出解释，忽略了它们启动对话和从机器人经验中生成语言响应的潜力。最后，这些设置都没有考虑传感器限制、环境动态或计划变更导致的意外情况。
- en: 2.2 Language-guided Autonomous Driving and Outdoor Vision-Language Navigation
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 语言引导的自动驾驶和户外视觉-语言导航
- en: Situated human-vehicle communication has been extensively studied in the form
    of spoken language, and this line of work dates back to early resources including
    several multilingual [[54](#bib.bib54)] and multimodal [[22](#bib.bib22), [9](#bib.bib9)]
    speech corpora. Recently, vision-and-language navigation (VLN) tasks require an
    agent to navigate in a 3D environment based on natural-language instructions and
    egocentric camera observations, with some efforts in the outdoor scenarios [[55](#bib.bib55),
    [23](#bib.bib23)]. They consider the world as a discrete graph while agents navigate
    toward the goal by moving among nodes. Thanks to open-world autonomous driving
    simulators [[10](#bib.bib10), [72](#bib.bib72), [57](#bib.bib57)], recent work
    bridges the gap between discrete model prediction and continuous closed-loop control.
    Various language-guided autonomous driving experiments and datasets [[50](#bib.bib50),
    [41](#bib.bib41), [27](#bib.bib27)] have been developed based on these simulators.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 人车通信的场景研究已经广泛涉及口语语言，这项工作可以追溯到早期资源，包括一些多语言[[54](#bib.bib54)]和多模态[[22](#bib.bib22),
    [9](#bib.bib9)]语料库。最近，视觉与语言导航（VLN）任务要求代理根据自然语言指令和自我中心的摄像机观测在3D环境中导航，且在户外场景中有一些努力[[55](#bib.bib55),
    [23](#bib.bib23)]。它们将世界视为一个离散图，代理通过在节点间移动来朝着目标前进。由于开放世界自动驾驶模拟器[[10](#bib.bib10),
    [72](#bib.bib72), [57](#bib.bib57)]，最近的工作弥合了离散模型预测和连续闭环控制之间的差距。基于这些模拟器，已经开发了各种语言引导的自动驾驶实验和数据集[[50](#bib.bib50),
    [41](#bib.bib41), [27](#bib.bib27)]。
- en: 2.3 Dialogue-guided Robotic Agents
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 对话引导的机器人代理
- en: Dialogue-guided agents for improving human-robot interaction have gained significant
    attention [[31](#bib.bib31), [32](#bib.bib32)]. Efforts in this field have ranged
    from enabling robots to adjust their plans in real-time based on human dialogue [[48](#bib.bib48),
    [8](#bib.bib8)], to seeking additional hints [[51](#bib.bib51), [36](#bib.bib36)],
    or to ask for direct human collaboration [[34](#bib.bib34)] for task completion.
    The advances of LLMs have infused new potential into these studies [[13](#bib.bib13),
    [66](#bib.bib66)]. For instance, InnerMonologue [[17](#bib.bib17)] investigates
    the use of LLMs for generating internal dialogue to assist in completing human-oriented
    tasks, while PromptCraft [[40](#bib.bib40)] explores precise prompt engineering
    to enhance the communication skills of robots. These developments underscore the
    pivotal role of foundation models as building blocks of agents to foster more
    effective human-robot collaboration.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在改善人机互动的对话引导代理受到了显著关注 [[31](#bib.bib31), [32](#bib.bib32)]。该领域的努力包括使机器人能够实时调整计划以基于人类对话 [[48](#bib.bib48),
    [8](#bib.bib8)]，寻求额外提示 [[51](#bib.bib51), [36](#bib.bib36)]，或请求直接的人类协作 [[34](#bib.bib34)]
    来完成任务。大型语言模型的进步为这些研究注入了新的潜力 [[13](#bib.bib13), [66](#bib.bib66)]。例如，InnerMonologue [[17](#bib.bib17)]
    研究了使用大型语言模型生成内部对话以协助完成以人为导向的任务，而 PromptCraft [[40](#bib.bib40)] 探索了精确的提示工程以增强机器人沟通能力。这些发展突显了基础模型作为代理构建块以促进更有效的人机协作的关键作用。
- en: 3 Dorothie & Situated Dialogue Navigation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 Dorothie & 情境对话导航
- en: We set up our experiment in CARLA [[10](#bib.bib10)], a driving simulator for
    autonomous vehicles, and use the DOROTHIE framework [[27](#bib.bib27)] built upon
    it, which supports human-agent dialogue and various forms of unexpected situations.
    In this work, we adopt the problem definition and data from the Situated Dialogue
    Navigation (SDN) benchmark in [[27](#bib.bib27)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 CARLA [[10](#bib.bib10)] 中设置了实验，这是一种用于自动驾驶车辆的驾驶模拟器，并使用了建立在其基础上的 DOROTHIE
    框架 [[27](#bib.bib27)]，该框架支持人机对话和各种突发情况。在这项工作中，我们采用了来自[[27](#bib.bib27)]的情境对话导航
    (SDN) 基准的定义和数据。
- en: 3.1 Overview
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: The SDN benchmark is designed to assess the agent’s capability in generating
    dialogue responses and physical navigation actions according to the perceptual
    and dialogue history. SDN is collected from human-human interactions in Wizard-of-Oz
    (WoZ) studies, consisting of over 8,000 utterances and 18.7 hours of control streams.
    In the WoZ study, a human participant engages with what they believe to be an
    autonomous driving agent to accomplish various navigation tasks. Behind the scenes,
    the actions of this agent are operated by a human wizard. This setup ensures that
    the participant’s interactions with the agent are natural and synchronized. During
    the interaction, there is also an adversarial wizard who creates unexpected situations
    on the fly. This adversarial wizard changes environmental dynamics as well as
    current goals and plans by using language instructions and manipulating road conditions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: SDN 基准旨在评估代理根据感知和对话历史生成对话响应和物理导航动作的能力。SDN 来自于 Wizard-of-Oz (WoZ) 研究中的人际互动，包括超过
    8,000 个发言和 18.7 小时的控制流。在 WoZ 研究中，一个人类参与者与他们认为是自动驾驶代理的系统互动，以完成各种导航任务。在幕后，这个代理的动作由一个人类巫师操作。这种设置确保了参与者与代理的互动自然且同步。在互动过程中，还有一个对抗性巫师实时制造突发情况。这个对抗性巫师通过使用语言指令和操控道路条件来改变环境动态以及当前目标和计划。
- en: 3.2 Problem Definitions
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 问题定义
- en: 'At time $t$, the agent is provided with a perceptual observation and a human
    language input, aggregated into the following model input:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间 $t$，代理提供了一个感知观察和一个人类语言输入，汇总成以下模型输入：
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Map knowledge. A graph-structured topology $M$.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 地图知识。一个图结构拓扑 $M$。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Perceptual history. A sequence of RGB images $V=\{V_{0},V_{1},\cdots,V_{t-1}\}$
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 感知历史。RGB 图像序列 $V=\{V_{0},V_{1},\cdots,V_{t-1}\}$
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dialogue history. The dialogue utterances from the human ($U_{t,\mathrm{HUM}}$).
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话历史。来自人类的对话发言 ($U_{t,\mathrm{HUM}}$)。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Action history. The action history includes a sequence of previous actions
    $A_{t}=\{a_{0},a_{1},\cdots,a_{t-1}\}$. More details about physical action definitions
    are in Table [1](#S3.T1 "Table 1 ‣ 3.2 Problem Definitions ‣ 3 Dorothie & Situated
    Dialogue Navigation ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents with
    Embodied and Social Experiences").'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '动作历史。动作历史包括之前动作的序列 $A_{t}=\{a_{0},a_{1},\cdots,a_{t-1}\}$。有关物理动作定义的更多细节见表 [1](#S3.T1
    "Table 1 ‣ 3.2 Problem Definitions ‣ 3 Dorothie & Situated Dialogue Navigation
    ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social
    Experiences")。'
- en: '| Physical Actions | Args | Descriptions |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 物理动作 | 参数 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LaneFollow | - | Default behaviour, follow the current lane. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| LaneFollow | - | 默认行为，跟随当前车道。 |'
- en: '| LaneSwitch | Direction | Switch to a neighboring lane. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| LaneSwitch | 方向 | 切换到邻近车道。 |'
- en: '| JTurn | Direction | Turn to a connecting road at a junction. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| JTurn | 方向 | 在交叉路口转入连接的道路。 |'
- en: '| UTurn | - | Make a U-turn to the opposite direction. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| UTurn | - | 进行 U 型转弯到相反方向。 |'
- en: '| Stop | - | Brake the vehicle manually. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Stop | - | 手动刹车。 |'
- en: '| Start | - | Start the vehicle manually. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Start | - | 手动启动车辆。 |'
- en: '| SpeedChange | Speed ($\pm$5) | Change the desired cruise speed by 5 km/h.
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| SpeedChange | 速度 ($\pm$5) | 将期望巡航速度更改 5 km/h。 |'
- en: '| LightChange | Light State (On/Off) | Change the front light state. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| LightChange | 光状态（开/关） | 更改前灯状态。 |'
- en: 'Table 1: The high-levels action space in the SDN benchmark.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：SDN 基准中的高级动作空间。
- en: The goal of the agent is to navigate to a sequence of landmarks on the map following
    the dialogue instructions from the human partner. To guarantee coherence in future
    dialogues and unforeseen events, the tasks are defined in a teacher-forcing manner.
    This means that during data collection, the model is always presented with the
    actual action history $A_{t}$, rather than model-predicted actions during inference.
    The model is evaluated against the action and dialogue decisions of the human
    wizard. We particularly consider two sub-problems.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的目标是根据人类伙伴的对话指令导航到地图上的一系列地标。为了保证未来对话和不可预见事件的连贯性，任务以教师强制方式定义。这意味着在数据收集期间，模型始终会看到实际的动作历史
    $A_{t}$，而不是推断期间模型预测的动作。模型的评估是基于人类专家的动作和对话决策。我们特别考虑两个子问题。
- en: The Dialogue Response for Navigation (RfN) task.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话响应导航 (RfN) 任务。
- en: The RfN task evaluates the agent’s performance in generating an adequate response
    in driving-related communication. At time stamp $\tau$. Instead of predicting
    only the dialogue move, we task the agent to generate the natural language.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: RfN 任务评估代理在驾驶相关沟通中生成适当回应的表现。在时间戳 $\tau$。我们要求代理生成自然语言，而不仅仅是预测对话动作。
- en: The Navigation from Dialogue (NfD) task.
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从对话中导航 (NfD) 任务。
- en: The NfD task evaluates the agent’s performance in following human instructions
    from dialogue. At time stamp $\tau$, the agent is required to predict this physical
    action.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: NfD 任务评估代理在对话中遵循人类指令的表现。在时间戳 $\tau$，代理需要预测这一物理动作。
- en: 4 Method
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: 4.1 Model Architecture
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型架构
- en: 'Our DriVLMe agent is a large video-language model consisting of three parts:
    a video tokenizer, a route planning module, and a large language model backbone.
    The overview architecture of DriVLMe is visualized in Figure [1](#S2.F1 "Figure
    1 ‣ 2 Related Work ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents with
    Embodied and Social Experiences").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的 DriVLMe 代理是一个大型视频语言模型，由三个部分组成：视频分词器、路线规划模块和大型语言模型主干。DriVLMe 的概览架构在图 [1](#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents
    with Embodied and Social Experiences") 中可视化。'
- en: Video Tokenizer.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 视频分词器。
- en: At time $t$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间 $t$。
- en: LLM Backbone.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM 主干。
- en: 'The LLM decoder is the core module that processes the input video and translates
    the dialogue instructions into lower-level decisions. Motivated by Video-ChatGPT [[29](#bib.bib29)],
    we adopt Vicuna-7B (v1.1) [[53](#bib.bib53)] as the LLM decoder. Motivated by
    the tool-using capability of LLMs, we introduce a planning framework for environmental
    understanding with the detailed prompts shown in Figure [2](#S4.F2 "Figure 2 ‣
    Route Planning Module. ‣ 4.1 Model Architecture ‣ 4 Method ‣ DriVLMe: Enhancing
    LLM-based Autonomous Driving Agents with Embodied and Social Experiences").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM解码器是处理输入视频并将对话指令转换为较低级别决策的核心模块。受Video-ChatGPT[[29](#bib.bib29)]的启发，我们采用了Vicuna-7B
    (v1.1)[[53](#bib.bib53)]作为LLM解码器。受到LLMs工具使用能力的启发，我们引入了一个环境理解的规划框架，详细提示如图[2](#S4.F2
    "图2 ‣ 路线规划模块 ‣ 4.1 模型架构 ‣ 4 方法 ‣ DriVLMe: 通过具身和社会经验提升基于LLM的自动驾驶代理")所示。'
- en: Route Planning Module.
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 路线规划模块。
- en: To enable symbolic planning for long-horizon goals, we introduce a route planner
    to incorporate the graph knowledge in the map $M$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现长期目标的符号规划，我们引入了一个路线规划器，以将图形知识融入到地图$M$中。
- en: (Video)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (视频)
- en: '(System Message): You are DriVLMe. You are responsible for safely piloting
    a car according to the instructions of a passenger. You must communicate with
    the passenger and make high-level decisions regarding the current navigational
    goals.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '(系统消息): 你是DriVLMe。你负责根据乘客的指示安全驾驶汽车。你必须与乘客沟通，并就当前导航目标做出高级决策。'
- en: '(Prompt): Describe what you see.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '(提示): 描述你看到的。'
- en: '(LLM, Description): I can see a car in front of me. I can only switch left
    lane…'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '(LLM, 描述): 我能看到前面有一辆车。我只能切换到左车道…'
- en: (Dialogue & Action History)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (对话与行动历史)
- en: '(Route Planning Instruction): You have a planning tool that you can plan your
    path to the destination. You can call it by plan(destination), and it will return
    you a plan to get to your destination. If you don’t have a destination in your
    mind, you can return plan(None).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '(路线规划指令): 你有一个规划工具可以规划通往目的地的路径。你可以通过plan(destination)调用它，它会返回一个到达目的地的计划。如果你没有明确的目的地，可以返回plan(None)。'
- en: '(LLM, Planning): plan(ikea)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '(LLM, 规划): plan(ikea)'
- en: '(Route Planner): [left, straight, …]'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '(路线规划器): [左转, 直行, …]'
- en: '(Prompt): You can select a new navigational action and reply to the passenger.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '(提示): 你可以选择新的导航行动并回复乘客。'
- en: '(LLM, Action): SwitchLane'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '(LLM, 行动): SwitchLane'
- en: '(LLM, Dialogue): “Ok, I will go to IKEA.”'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '(LLM, 对话): “好的，我会去IKEA。”'
- en: 'Figure 2: Example of system message and interaction between user and DriVLMe
    system. The system message is an overview of the task the agent is required to
    accomplish. Given the video and the observation history, the agent is required
    to first describe the surrounding environment, then call the planner API to plan
    a route to the predicted goal, and make a decision at last. The output of the
    LLM is highlighted.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '图2: 系统消息和用户与DriVLMe系统之间的交互示例。系统消息是对代理需要完成任务的概述。根据视频和观察历史，代理需要首先描述周围环境，然后调用规划器API以规划通往预测目标的路线，最后做出决策。LLM的输出被突出显示。'
- en: 4.2 Instruction Tuning
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 指令调整
- en: 'Motivated by Hu and Shu [[16](#bib.bib16)], our goal is to enhance a language
    model’s competence as a world model and agent model by learning from embodied
    experiences and social interactions. The training process of DriVLMe consists
    of two stages: (1) the general video instruction tuning stage, focused on aligning
    the LLM and the video tokenizer using large-scale driving videos, and (2) the
    social and embodied instruction tuning stage, focused on training the LLM on the
    conversational data collected from real human-human dialogue and episodes of embodied
    experiences in a simulator.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 受 Hu 和 Shu [[16](#bib.bib16)] 的启发，我们的目标是通过从具象体验和社会互动中学习来增强语言模型作为世界模型和代理模型的能力。DriVLMe
    的训练过程包括两个阶段：（1）一般视频指令调整阶段，重点是使用大规模驾驶视频对齐 LLM 和视频标记器；（2）社会和具象指令调整阶段，重点是基于从真实人际对话和模拟器中的具象体验收集的对话数据来训练
    LLM。
- en: 4.2.1 Domain Video Instruction Tuning
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 域视频指令调整
- en: Following the practice of Video-ChatGPT [[29](#bib.bib29)], we initialize the
    projection layer directly from LLaVA-7B (lightening v1.1) [[25](#bib.bib25)].
    We adopt 50k video-text pairs from the BDD-X dataset [[21](#bib.bib21)] for the
    driving domain tuning. The pre-training images are collected from real driving
    videos and textual annotations of the environmental description and action explanations.
    We freeze the CLIP encoder and the LLM decoder, and train the projection layer
    only.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参考 Video-ChatGPT 的做法 [[29](#bib.bib29)]，我们直接从 LLaVA-7B (lightening v1.1) [[25](#bib.bib25)]
    初始化投影层。我们采用了来自 BDD-X 数据集 [[21](#bib.bib21)] 的 50k 视频-文本对用于驾驶领域的调整。预训练图像采集自真实驾驶视频及环境描述和动作解释的文本注释。我们冻结了
    CLIP 编码器和 LLM 解码器，仅训练投影层。
- en: 4.2.2 Social Instruction Tuning
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 社会指令调整
- en: At this stage, we used LoRA [[14](#bib.bib14)] to fine-tune the LLM in addition
    to the projector. We train the model on the whole training set of the SDN dataset,
    which has 13k video-dialogue pairs, including human-vehicle dialogues and long-term
    goals for planners. At each datapoint $\tau$ is an argument (e.g., left). We aim
    for the agent to learn how to plan in alignment with human intentions, which involves
    creating a sequence of primitive actions based on the goal and dialogue history,
    particularly when there’s a change in the goal or plan. We manually annotate plan
    changes based on the car’s trajectory and the current dialogue. While there could
    be several valid paths from the current location to the goal, we manually selected
    the routes that the vehicle took during the recording. These annotated plans serve
    as a part of the video-instruction data pairs for training, facilitating more
    effective learning of the planner as a tool.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，除了投影器外，我们还使用 LoRA [[14](#bib.bib14)] 对 LLM 进行微调。我们在包含 13k 视频-对话对的 SDN 数据集的整个训练集上训练模型，这些对话包括人-车对话和规划者的长期目标。在每个数据点
    $\tau$ 是一个参数（例如，左）。我们的目标是让代理学会如何根据目标和对话历史进行规划，特别是在目标或计划发生变化时。这些标注的计划作为视频-指令数据对的一部分，用于训练，从而更有效地学习规划者作为工具。
- en: 4.2.3 Embodied Instruction Tuning
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 具象指令调整
- en: Besides the original dialogue data, we developed a data generation pipeline
    to obtain paired data of embodied perception and descriptions from the simulator.
    We replay the training sessions in the SDN benchmark to obtain the egocentric
    perception, record the environmental factors such as weather and nearby objects,
    and then fill these details into language descriptions using templates.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始对话数据外，我们开发了一个数据生成管道，从模拟器中获得具象感知和描述的配对数据。我们重播 SDN 基准测试中的训练会话，以获取自我中心感知，记录环境因素如天气和附近的物体，然后使用模板将这些细节填入语言描述中。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distance to Road End: We compute the distance to the road’s end by subtracting
    the current waypoint’s $s$ value is defined according to the OpenDrive 1.4 standard [[11](#bib.bib11)].'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 距离路段末端的距离：我们通过减去当前路径点的 $s$ 值来计算道路的末端距离，该值根据 OpenDrive 1.4 标准 [[11](#bib.bib11)]
    定义。
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lane Information: We note the lane number the car was in, counting from the
    left, and record whether the car could switch to the adjacent left or right lanes.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 车道信息：我们记录了车辆所在的车道编号，从左侧开始计数，并记录车辆是否可以切换到相邻的左侧或右侧车道。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Object in Front: We identify the object directly in front of the vehicle from
    the ground truth obtained from the simulation, and compute the distance to it.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前方物体：我们从模拟中获得的地面真值中识别出车辆正前方的物体，并计算与其距离。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Traffic Sign Visibility: We record all visible traffic signs (e.g., traffic
    lights, stop signs, speed limit signs), along with the information they displayed
    (red/green for lights, posted speed limits), and their distances from the vehicle.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交通标志可见性：我们记录所有可见的交通标志（例如，交通灯、停止标志、限速标志），以及它们显示的信息（灯光的红色/绿色、公布的速度限制），和它们距离车辆的距离。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Weather Conditions: We record the current weather conditions that could impact
    the vehicle’s control.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 天气条件：我们记录可能影响车辆控制的当前天气条件。
- en: 'The text templates used to verbalize the embodied experiences are available
    in Appendix [8.1](#Sx2.SS1 "8.1 Language Templates for Verbalizing the Embodied
    Experiences. ‣ Appendix ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents
    with Embodied and Social Experiences").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '用于将体现经验转化为语言的文本模板见附录 [8.1](#Sx2.SS1 "8.1 Language Templates for Verbalizing
    the Embodied Experiences. ‣ Appendix ‣ DriVLMe: Enhancing LLM-based Autonomous
    Driving Agents with Embodied and Social Experiences")。'
- en: 4.2.4 Hyper-parameters.
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 超参数。
- en: The input resolution of the video is set as $224\times 224$ and a batch size
    of 4. We fine-tune the LLM with LoRA [[14](#bib.bib14)] and ZeRO [[39](#bib.bib39)].
    The training epoch is 2 and the batch size is 1\. For the LoRA configuration,
    we set rank to 128 and alpha to 256.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 视频的输入分辨率设置为 $224\times 224$，批量大小为 4。我们使用 LoRA [[14](#bib.bib14)] 和 ZeRO [[39](#bib.bib39)]
    对 LLM 进行微调。训练轮数为 2，批量大小为 1。对于 LoRA 配置，我们将秩设置为 128，alpha 设置为 256。
- en: 5 Open-loop Evaluation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 开环评估
- en: 5.1 SDN Benchmark
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 SDN 基准
- en: For the open-loop evaluation, we tested the model on the test split of the SDN
    benchmark. The test set has two subsets, seen and unseen, where seen data points
    adopt either CARLA map Town01, Town03, or Town05 as the environment (which appeared
    in the training set). The unseen data points are from Town02, which is a relatively
    simple town map that was held out from training.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开环评估，我们在 SDN 基准的测试集上测试了模型。测试集有两个子集，已见和未见，其中已见数据点采用 CARLA 地图 Town01、Town03
    或 Town05 作为环境（这些环境出现在训练集中）。未见数据点来自 Town02，这是一个相对简单的城镇地图，未用于训练。
- en: 5.2 Evaluation Metrics
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评价指标
- en: 'We evaluate our model on two tasks, RfN and NfD. The NfD task necessitates
    the agent’s prediction of the physical action $\langle p,\alpha\rangle$ as defined
    in SDN. To evaluate the natural language dialogue output, we consider additional
    language generation metrics: CIDEr [[56](#bib.bib56)], BERTScore [[70](#bib.bib70)],
    and METEOR [[3](#bib.bib3)].'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个任务上评估了我们的模型，RfN 和 NfD。NfD 任务需要代理预测物理动作 $\langle p,\alpha\rangle$，如 SDN
    所定义。为了评估自然语言对话输出，我们考虑了额外的语言生成指标：CIDEr [[56](#bib.bib56)]，BERTScore [[70](#bib.bib70)]，和
    METEOR [[3](#bib.bib3)]。
- en: '| Model | NfD | RfN |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Model | NfD | RfN |'
- en: '| --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Act$\uparrow$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Act$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Seen Environments |  |  |  |  |  |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Seen Environments |  |  |  |  |  |  |'
- en: '| TOTO | 41.2 | 36.0 | 40.9 | - | - | - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| TOTO | 41.2 | 36.0 | 40.9 | - | - | - |'
- en: '| GPT-4 | 53.0 | 44.2 | 11.0 | 0.06 | 0.48 | 0.09 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 53.0 | 44.2 | 11.0 | 0.06 | 0.48 | 0.09 |'
- en: '| GPT-4V | 52.0 | 29.4 | 6.5 | 0.07 | 0.54 | 0.11 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | 52.0 | 29.4 | 6.5 | 0.07 | 0.54 | 0.11 |'
- en: '| \cdashline1-7 DriveVLM | 70.4 | 71.3 | 61.4 | 0.43 | 0.76 | 0.37 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-7 DriveVLM | 70.4 | 71.3 | 61.4 | 0.43 | 0.76 | 0.37 |'
- en: '| DriVLMe (-social) | 68.7 | 69.0 | 19.1 | 0.17 | 0.60 | 0.13 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-social) | 68.7 | 69.0 | 19.1 | 0.17 | 0.60 | 0.13 |'
- en: '| DriVLMe (-embodied) | 68.4 | 67.7 | 62.7 | 0.45 | 0.76 | 0.37 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-embodied) | 68.4 | 67.7 | 62.7 | 0.45 | 0.76 | 0.37 |'
- en: '| DriVLMe (-domain) | 62.4 | 70.7 | 60.9 | 0.35 | 0.75 | 0.18 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-domain) | 62.4 | 70.7 | 60.9 | 0.35 | 0.75 | 0.18 |'
- en: '| DriVLMe (-video) | 60.3 | 72.5 | 42.7 | 0.33 | 0.69 | 0.26 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-video) | 60.3 | 72.5 | 42.7 | 0.33 | 0.69 | 0.26 |'
- en: '| DriVLMe (-planner) | 57.6 | 52.0 | 21.3 | 0.19 | 0.61 | 0.12 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-planner) | 57.6 | 52.0 | 21.3 | 0.19 | 0.61 | 0.12 |'
- en: '| Unseen Environment |  |  |  |  |  |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Unseen Environment |  |  |  |  |  |  |'
- en: '| TOTO | 45.8 | 41.1 | 31.0 | - | - | - |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| TOTO | 45.8 | 41.1 | 31.0 | - | - | - |'
- en: '| GPT-4 | 67.5 | 61.3 | 14.5 | 0.05 | 0.47 | 0.08 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 67.5 | 61.3 | 14.5 | 0.05 | 0.47 | 0.08 |'
- en: '| GPT-4V | 63.5 | 51.6 | 7.5 | 0.07 | 0.53 | 0.13 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | 63.5 | 51.6 | 7.5 | 0.07 | 0.53 | 0.13 |'
- en: '| \cdashline1-7 DriveVLM | 70.8 | 71.3 | 68.5 | 0.55 | 0.81 | 0.43 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-7 DriveVLM | 70.8 | 71.3 | 68.5 | 0.55 | 0.81 | 0.43 |'
- en: '| DriVLMe (-social) | 69.8 | 66.8 | 26.9 | 0.25 | 0.64 | 0.16 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-social) | 69.8 | 66.8 | 26.9 | 0.25 | 0.64 | 0.16 |'
- en: '| DriVLMe (-embodied) | 72.9 | 68.0 | 66.7 | 0.52 | 0.79 | 0.42 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-embodied) | 72.9 | 68.0 | 66.7 | 0.52 | 0.79 | 0.42 |'
- en: '| DriVLMe (-domain) | 65.9 | 70.8 | 65.3 | 0.48 | 0.78 | 0.38 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-domain) | 65.9 | 70.8 | 65.3 | 0.48 | 0.78 | 0.38 |'
- en: '| DriVLMe (-video) | 62.6 | 68.6 | 46.5 | 0.41 | 0.73 | 0.31 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-video) | 62.6 | 68.6 | 46.5 | 0.41 | 0.73 | 0.31 |'
- en: '| DriVLMe (-planner) | 58.2 | 59.1 | 23.7 | 0.22 | 0.63 | 0.13 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe (-planner) | 58.2 | 59.1 | 23.7 | 0.22 | 0.63 | 0.13 |'
- en: 'Table 2: Results of open-loop evaluation on the SDN test set. The seen sessions
    are from CARLA map Town01, Town03, and Town05, while unseen sessions are from
    CARLA map Town02\. The NfD task measures the agent’s ability to navigate according
    to human instruction and the RfN task measures the agent’s ability to respond
    to humans in a situated dialogue, M stands for METEOR.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：SDN测试集的开环评估结果。已见会话来自CARLA地图Town01、Town03和Town05，而未见会话来自CARLA地图Town02。NfD任务衡量代理根据人类指令导航的能力，而RfN任务衡量代理在情境对话中回应人类的能力，M代表METEOR。
- en: 5.3 Baselines
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 基线
- en: Expert Baseline.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 专家基线。
- en: We compared our model with TOTO [[27](#bib.bib27)], a baseline model implemented
    with an episodic transformer. Since the TOTO model does not have a text decoder
    and thus cannot generate dialogue, we only recorded the dialogue move prediction
    accuracy of TOTO.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的模型与TOTO [[27](#bib.bib27)]进行了比较，TOTO是一个使用情节变压器实现的基线模型。由于TOTO模型没有文本解码器，因此无法生成对话，我们只记录了TOTO的对话移动预测准确率。
- en: Generalist Baselines.
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通用基线。
- en: 'The GPT-4 [[1](#bib.bib1)] and GPT-4V [[35](#bib.bib35)] models are generalist
    LLMs we consider.¹¹1We use the OpenAI gpt-4-0125-preview and gpt-4-vision-preview
    models, respectively. Due to computational constraints, rather than test both
    models on the entirety of the SDN test set, we chose to randomly sample data points
    from four strata: seen RfN, unseen RfN, seen NfD, and unseen NfD. To evaluate
    each model on one of these strata, we randomly sampled 200 data points and fed
    them into a custom prompting infrastructure similar to the structure in Table [2](#S4.F2
    "Figure 2 ‣ Route Planning Module. ‣ 4.1 Model Architecture ‣ 4 Method ‣ DriVLMe:
    Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences").
    For the vision-enabled model (GPT-4V), we prepended an image $V_{t-1}$ as the
    current visual input. To help the LLMs better understand the output format, we
    explain each option in the decision-making prompt. The prompt engineering details
    are in Appendix [8.2](#Sx2.SS2 "8.2 Prompt Engineering for GPT-4 Baseline ‣ Appendix
    ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social
    Experiences").'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 [[1](#bib.bib1)] 和 GPT-4V [[35](#bib.bib35)] 模型是我们考虑的通用LLM。¹¹1我们分别使用了OpenAI的gpt-4-0125-preview和gpt-4-vision-preview模型。由于计算限制，我们没有在整个SDN测试集上测试这两个模型，而是选择从四个层次中随机抽样数据点：已见RfN、未见RfN、已见NfD和未见NfD。为了在这些层次之一上评估每个模型，我们随机抽取了200个数据点，并将其输入到类似于表[2](#S4.F2
    "图 2 ‣ 路径规划模块 ‣ 4.1 模型架构 ‣ 4 方法 ‣ DriVLMe：利用具身和社会经验增强基于LLM的自主驾驶代理")结构的自定义提示基础设施中。对于启用了视觉的模型（GPT-4V），我们在当前视觉输入之前添加了一张图像$V_{t-1}$。为了帮助LLM更好地理解输出格式，我们解释了决策提示中的每个选项。提示工程细节见附录[8.2](#Sx2.SS2
    "8.2 GPT-4基线的提示工程 ‣ 附录 ‣ DriVLMe：利用具身和社会经验增强基于LLM的自主驾驶代理")。
- en: 5.4 Main Results
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 主要结果
- en: 'As shown in Table [2](#S5.T2 "Table 2 ‣ 5.2 Evaluation Metrics ‣ 5 Open-loop
    Evaluation ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied
    and Social Experiences"), our DriveVLMe model significantly outperforms the baseline
    models across most metrics, except for the physical action accuracy in the NfD
    task for the unseen map. This discrepancy may be attributed to the unfamiliarity
    with the unseen Town02, though it is topographically simpler. Overall, DriVLMe
    can predict more precise decisions and give better responses in the situated dialogue.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S5.T2 "表 2 ‣ 5.2 评估指标 ‣ 5 开环评估 ‣ DriVLMe：利用具身和社会经验增强基于LLM的自主驾驶代理")所示，我们的DriveVLMe模型在大多数指标上显著超越了基线模型，除了在未见地图上的NfD任务中的物理动作准确性。这一差异可能归因于对未见Town02的陌生，尽管它在地形上更简单。总体而言，DriVLMe能够做出更精确的决策，并在情境对话中给出更好的回应。
- en: '![Refer to caption](img/9022650d9a5df4fd80b1888b1363185e.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9022650d9a5df4fd80b1888b1363185e.png)'
- en: 'Figure 3: Examples of closed-loop evaluation of DriVLMe in CARLA, following
    action-level natural language instructions.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：DriVLMe在CARLA中的闭环评估示例，遵循行动级自然语言指令。
- en: '![Refer to caption](img/e47421be540aa63c06adccf060ee9e0b.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e47421be540aa63c06adccf060ee9e0b.png)'
- en: 'Figure 4: Example of a closed-loop evaluation session: The initial goal of
    the session is set to Shell, which is later changed to KFC during the course of
    the evaluation. The yellow solid line represents the path taken by the agent and
    the yellow dotted line represents the route planned by the planner. We took eight
    checkpoints in the whole evaluation session and recorded the input dialogue, goal
    prediction, dialogue response and the physical action taken for each checkpoint.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：闭环评估会话示例：会话的初始目标设置为Shell，在评估过程中后更改为KFC。黄色实线表示智能体所走的路径，黄色虚线表示规划模块规划的路线。我们在整个评估会话中设置了八个检查点，并记录了每个检查点的输入对话、目标预测、对话响应和采取的实际行动。
- en: 5.5 Ablation Studies
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 消融研究
- en: To assess the effectiveness of various data and components in developing DriVLMe,
    we conducted an ablation study. We evaluated the model performance by systematically
    removing specific training data and components to observe their impact on the
    model’s ability to generate dialogue responses and predict physical actions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估在开发DriVLMe过程中各种数据和组件的有效性，我们进行了消融研究。通过系统性地去除特定训练数据和组件来观察它们对模型生成对话响应和预测实际行动能力的影响。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Social Data (-social): We removed the human-vehicle dialogue data used for
    social instruction tuning.'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 社会数据 (-social)：我们去除了用于社会指令调整的人车对话数据。
- en: •
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Embodied Data (-embodied): We removed the simulated data used for embodied
    instruction tuning.'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具身数据 (-embodied)：我们去除了用于具身指令调整的模拟数据。
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Domain Data (-domain): We removed the BDD-X data used for domain-general instruction
    tuning.'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域数据 (-domain)：我们去除了用于领域通用指令调整的BDD-X数据。
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Video Input (-video): We removed the video processing component from DriVLMe
    and evaluated its performance without visual information.'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频输入 (-video)：我们去除了DriVLMe中的视频处理组件，并评估了在没有视觉信息的情况下的性能。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Planner Module (-planner): We removed the planner module responsible for route
    planning in DriVLMe. This experiment aimed to assess the impact of proactive route
    planning on the model’s navigation capabilities.'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规划模块 (-planner)：我们去除了负责DriVLMe中路线规划的规划模块。该实验旨在评估主动路线规划对模型导航能力的影响。
- en: 'As shown in Table [2](#S5.T2 "Table 2 ‣ 5.2 Evaluation Metrics ‣ 5 Open-loop
    Evaluation ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied
    and Social Experiences"), removing the video input and the planner module both
    decrease the performance of the model on the RfN tasks on all metrics, indicating
    the contribution of both models on response generation. A similar decrease in
    NfD performance is observed, while the impact of removing the planner is significant,
    suggesting that the route planner module greatly contributes to the next action
    prediction. Data ablation studies show that social experiences significantly enhance
    response generation. We observed that embodied experiences mainly aid the model
    in predicting actions unrelated to route planning, such as lane switching. Consequently,
    this was less beneficial in the unseen Town02, where lane switching is not necessary.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[2](#S5.T2 "Table 2 ‣ 5.2 Evaluation Metrics ‣ 5 Open-loop Evaluation ‣ DriVLMe:
    Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences")所示，去除视频输入和规划模块都会降低模型在所有指标上的RfN任务性能，表明这两个模型在响应生成中的贡献。观察到NfD性能有类似的下降，而去除规划模块的影响显著，表明路线规划模块对下一步行动预测有很大贡献。数据消融研究显示，社会经验显著增强了响应生成。我们观察到，具身经验主要帮助模型预测与路线规划无关的动作，如车道变换。因此，这在未见过的Town02中效果较差，因为那里不需要车道变换。'
- en: 5.6 Evaluation on Realworld Benchmark
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 实际世界基准评估
- en: 'We also explore whether DriVLMe can transition from simulated evaluations to
    benchmarks involving real driving scenarios. We utilize the BDD-X [[21](#bib.bib21)]
    benchmark, which offers video clips recorded by vehicle-mounted cameras along
    with language interpretations and control signals. We fine-tune the DriVLMe model
    with LoRA for another 9 epochs on the BDD-X training set, using a learning rate
    of $5e^{-5}$ with both LoRA rank and alpha set to 256. As indicated in Table [3](#S5.T3
    "Table 3 ‣ 5.6 Evaluation on Realworld Benchmark ‣ 5 Open-loop Evaluation ‣ DriVLMe:
    Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences"),
    DriVLMe successfully adapts to real-world driving scenarios beyond merely navigating
    in a simulated environment. It outperforms the ADAPT [[18](#bib.bib18)] baseline
    and achieves comparable performance to the state-of-the-art DriveGPT4 [[64](#bib.bib64)]
    baseline, surpassing several metrics, without relying on ChatGPT-augmented data
    as adopted in DriveGPT4.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还探讨了 DriVLMe 是否能够从模拟评估过渡到涉及真实驾驶场景的基准测试。我们利用了 BDD-X [[21](#bib.bib21)] 基准，该基准提供了由车载摄像头录制的视频片段以及语言解释和控制信号。我们使用
    LoRA 对 DriVLMe 模型进行了额外 9 轮的微调，学习率为 $5e^{-5}$，LoRA 排名和 alpha 都设置为 256。如表 [3](#S5.T3
    "Table 3 ‣ 5.6 Evaluation on Realworld Benchmark ‣ 5 Open-loop Evaluation ‣ DriVLMe:
    Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences")
    所示，DriVLMe 成功适应了真实世界的驾驶场景，不仅仅是在模拟环境中导航。它超越了 ADAPT [[18](#bib.bib18)] 基准，并且在多个指标上超过了最先进的
    DriveGPT4 [[64](#bib.bib64)] 基准，而不依赖于 DriveGPT4 中采用的 ChatGPT 增强数据。'
- en: '| Model | Description | Justification | Full |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 描述 | 解释 | 完整 |  |'
- en: '| C$\uparrow$ |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| C$\uparrow$ |  |'
- en: '| ADAPT | 219.35 | 33.42 | 61.83 | 94.62 | 9.95 | 32.01 | 93.66 | 17.76 | 44.32
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| ADAPT | 219.35 | 33.42 | 61.83 | 94.62 | 9.95 | 32.01 | 93.66 | 17.76 | 44.32
    |  |'
- en: '| DriveGPT4 | 254.62 | 35.99 | 63.97 | 101.55 | 10.84 | 31.91 | 102.71 | 19.00
    | 45.10 |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| DriveGPT4 | 254.62 | 35.99 | 63.97 | 101.55 | 10.84 | 31.91 | 102.71 | 19.00
    | 45.10 |  |'
- en: '| DriVLMe | 227.05 | 33.39 | 61.02 | 132.17 | 13.39 | 33.18 | 114.16 | 19.59
    | 44.83 |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe | 227.05 | 33.39 | 61.02 | 132.17 | 13.39 | 33.18 | 114.16 | 19.59
    | 44.83 |  |'
- en: '| Model | Speed | Turning Angle |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 速度 | 转向角度 |'
- en: '| E$\downarrow$ |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| E$\downarrow$ |'
- en: '| ADAPT | 3.02 | 9.56 | 24.77 | 37.07 | 90.39 | 11.98 | 27.93 | 66.83 | 75.13
    | 89.45 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ADAPT | 3.02 | 9.56 | 24.77 | 37.07 | 90.39 | 11.98 | 27.93 | 66.83 | 75.13
    | 89.45 |'
- en: '| DriveGPT4 | 1.30 | 30.09 | 60.88 | 79.92 | 98.44 | 8.98 | 59.23 | 72.89 |
    79.59 | 95.32 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| DriveGPT4 | 1.30 | 30.09 | 60.88 | 79.92 | 98.44 | 8.98 | 59.23 | 72.89 |
    79.59 | 95.32 |'
- en: '| DriVLMe | 1.59 | 22.76 | 50.55 | 70.80 | 99.20 | 33.54 | 61.38 | 70.70 |
    76.21 | 91.55 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| DriVLMe | 1.59 | 22.76 | 50.55 | 70.80 | 99.20 | 33.54 | 61.38 | 70.70 |
    76.21 | 91.55 |'
- en: 'Table 3: Results of open-loop evaluation on the BDD-X test set. We provide
    evaluation results on action description, action justification, full-text generation
    and control signal prediction. C stands for CIDEr; B4 stands for BLEU4; R stands
    for ROUGE; E stands for Root Mean Square Error (RMSE).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：BDD-X 测试集的开环评估结果。我们提供了动作描述、动作解释、全文生成和控制信号预测的评估结果。C 代表 CIDEr；B4 代表 BLEU4；R
    代表 ROUGE；E 代表均方根误差（RMSE）。
- en: 6 Closed-loop Evaluation
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 闭环评估
- en: For the closed-loop evaluation, we developed a human-in-the-loop simulation
    protocol in CARLA based on the simulator developed in DOROTHIE for human studies.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于闭环评估，我们在 CARLA 中开发了一种基于 DOROTHIE 开发的模拟器的人体在环模拟协议，用于人类研究。
- en: 6.1 Experimental Design
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 实验设计
- en: 'We designed our closed-loop experiment to assess the adaptability and robustness
    of our autonomous driving system under various dynamic scenarios. The experiment
    was conducted in Town01 and Town02, including both seen and unseen maps. A human
    subject instructed the DriVLMe agent to navigate to a preset goal by giving natural
    language instructions following the storyboard, and the agent attempted to follow
    these instructions, autonomously navigate in the environment, and communicate
    with the human subject. To comprehensively evaluate the system’s performance,
    we test the model with different settings as specific in the storyboards below:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了闭环实验来评估我们自动驾驶系统在各种动态场景下的适应性和鲁棒性。实验在 Town01 和 Town02 中进行，包括已见和未见的地图。一名人工主体通过自然语言指令按照故事板指示
    DriVLMe 代理导航到预设目标，代理尝试按照这些指令行驶，在环境中自主导航，并与人工主体进行沟通。为了全面评估系统性能，我们在下面的故事板中测试了不同设置的模型：
- en: •
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Long-horizon v.s. Short-horizon Instructions: Users instruct the agent with
    either long-horizon instructions, involving higher-level navigational goals (e.g.,
    “go to the KFC”), or short-horizon instructions (e.g., “turn right at the next
    intersection”) asking for immediate maneuvers.'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长期目标与短期目标指令：用户可以用长期目标指令（例如，“前往KFC”）或短期目标指令（例如，“在下一个交叉口右转”）来指导代理，长期目标涉及较高级的导航目标，而短期目标请求立即的操作。
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Weather Change: A sudden weather change (e.g., rain) is triggered during driving.'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 天气变化：在驾驶过程中触发突然的天气变化（例如，降雨）。
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Goal Change: The human user asks for a change of goal to let the agent replan
    the route. The human user first instructs the agent to navigate to an initial
    goal and then updates it.'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标变化：用户请求更改目标，以便代理重新规划路线。用户首先指示代理导航到初始目标，然后进行更新。
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Obstacle Addition: An obstacle is placed in front of the agent to force a stop
    or lane change.'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 障碍物添加：在代理前方放置障碍物以迫使其停下或变道。
- en: 6.2 Connecting DriVLMe to Simulation
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 连接 DriVLMe 与模拟
- en: Throughout 20 pilot studies with real human subjects, agents’ interactions with
    the simulator formed a closed-loop control mechanism. We used a local motion planner
    to translate the physical actions back into throttle and steering control. Due
    to the LLM inference rate, we limited the LLM to interact with the environment
    at a frequency of 2 Hz, and provided the model with the whole interaction history
    $H_{t}$ to prompt the model. For the evaluation, we used whether the final goal
    was achieved as the metric and recorded the failure cases for analysis.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在与真实人类对象进行的20次试点研究中，代理与模拟器的互动形成了一个闭环控制机制。我们使用了本地运动规划器将物理动作转化为油门和方向控制。由于LLM推理速度的限制，我们将LLM的环境互动频率限制为2
    Hz，并向模型提供了整个互动历史 $H_{t}$ 以进行提示。对于评估，我们使用最终目标是否达成作为指标，并记录失败案例以进行分析。
- en: 6.3 Main Results
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 主要结果
- en: 'The outcomes of our experimental investigations provide compelling evidence
    regarding the efficacy and robustness of our proposed DriVLMe model in autonomous
    driving dialogue tasks, with 6 successful sessions out of 20 tests. As can be
    seen in Figure [3](#S5.F3 "Figure 3 ‣ 5.4 Main Results ‣ 5 Open-loop Evaluation
    ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social
    Experiences"), we find that the DriVLMe model is capable of following simple human
    instructions and performing the physical actions as requested, in line with previous
    studies on foundation model agents for autonomous driving. Surprisingly, we find
    that DriVLMe can effectively call the route planner API for reliable graph planning
    and re-planning, demonstrating LLMs’ tool use capabilities. The model is also
    robust under weather changes during the session. Still, these successful sessions
    are limited to cases when there is one single long-horizon goal or only one change
    of goal. We observe challenges with multi-turn interactions with multiple short-horizon
    instructions. DriVLMe also faces difficulties in handling unexpected situations
    and changes to environmental dynamics. Lastly, the simplified language generation
    from robotic experiences has triggered concerns about trustworthiness as raised
    by human subjects. Figure [4](#S5.F4 "Figure 4 ‣ 5.4 Main Results ‣ 5 Open-loop
    Evaluation ‣ DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied
    and Social Experiences") shows an example of our session with a goal change instruction.
    We find that the agent can react to goal changes and plan turns according to the
    plan given by the route planner tool. However, we encountered two failure cases
    during the experiment. First, the agent failed to stop when the car in front suddenly
    stopped (timestamp 7). Second, the agent failed to predict a turn at the last
    intersection, causing the agent to stall at the intersection (as marked on the
    map). We present the video demonstration for additional details and discuss the
    limitations of foundation model agents in the following section.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的实验调查结果提供了有力证据，证明我们提出的DriVLMe模型在自动驾驶对话任务中的有效性和鲁棒性，20次测试中有6次成功。如图[3](#S5.F3
    "图 3 ‣ 5.4 主要结果 ‣ 5 开环评估 ‣ DriVLMe: 通过具身和社交经验增强基于LLM的自动驾驶智能体")所示，我们发现DriVLMe模型能够按照简单的人类指令执行物理动作，与之前的自动驾驶基础模型智能体研究一致。令人惊讶的是，我们发现DriVLMe能够有效调用路线规划API进行可靠的图规划和重新规划，展示了LLMs的工具使用能力。模型在会话期间对天气变化也表现出鲁棒性。然而，这些成功的会话仅限于单一长远目标或仅有一个目标变化的情况。我们观察到在多轮交互和多个短期指令的情况下存在挑战。DriVLMe在处理意外情况和环境动态变化方面也面临困难。最后，来自机器人经验的简化语言生成引发了人类受试者对可信度的担忧。图[4](#S5.F4
    "图 4 ‣ 5.4 主要结果 ‣ 5 开环评估 ‣ DriVLMe: 通过具身和社交经验增强基于LLM的自动驾驶智能体")展示了一个目标变更指令的会话示例。我们发现智能体能够响应目标变化，并根据路线规划工具给出的计划进行转弯。然而，我们在实验中遇到了两个失败案例。首先，当前方车辆突然停车时，智能体未能及时停车（时间戳7）。其次，智能体未能预测最后一个交叉路口的转弯，导致智能体在交叉路口停滞（如地图所示）。我们提供了视频演示以供详细了解，并在接下来的部分讨论了基础模型智能体的局限性。'
- en: 7 Limitations and Future Work
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 局限性和未来工作
- en: Our pilot studies revealed several failure cases and technical challenges for
    LLM-based AD agents, outlined as follows.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初步研究揭示了基于LLM的AD智能体的若干失败案例和技术挑战，具体如下。
- en: Imbalanced Embodied Experiences.
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不平衡的具身经验。
- en: An inherent challenge in autonomous driving tasks lies in the imbalance of training
    data, where the majority of data points are routine actions like lane following
    or maintaining a safe distance from the preceding vehicle. This imbalance can
    lead to model biases, particularly towards predicting more frequent actions while
    failing to predict actions like stop. Addressing this issue requires introducing
    robust data augmentation in embodied experiences, sampling strategies, or domain-specific
    knowledge into the training process to ensure comprehensive model training across
    diverse driving scenarios.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶任务中的一个固有挑战在于训练数据的不平衡，大多数数据点是像车道跟随或保持安全距离这样的常规动作。这种不平衡可能导致模型偏差，特别是倾向于预测更频繁的动作而无法预测诸如停车等动作。解决这个问题需要在具身经验、采样策略或领域特定知识中引入强健的数据增强，以确保模型在各种驾驶场景下的全面训练。
- en: Limited World Modeling and Visual Understanding.
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 有限的世界建模和视觉理解。
- en: Our experiment revealed instances where the visual encoder failed to capture
    critical world states due to low image input resolution, such as the color of
    traffic lights or the interpretation of traffic signs. The absence of optical
    character recognition (OCR) capabilities further exacerbates the risk of misinterpreting
    traffic signs and thus breaking traffic rules. Future efforts could explore techniques
    to enhance image resolution, integrate OCR functionalities, or incorporate complementary
    sensor modalities to enrich perception and improve overall world modeling performance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验揭示了视觉编码器由于图像输入分辨率低而未能捕捉到关键世界状态的实例，例如交通灯的颜色或交通标志的解读。缺乏光学字符识别（OCR）能力进一步加剧了误解交通标志的风险，从而违反交通规则。未来的努力可以探索增强图像分辨率、整合OCR功能或引入互补传感器模态的技术，以丰富感知并改善整体世界建模性能。
- en: Unexpected Situations and World Dynamics.
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 意外情况与世界动态。
- en: Our closed-loop experiment results on unexpected situations like encountering
    an obstacle have revealed limitations in the LLM agent’s ability to effectively
    address out-of-distribution corner cases. Such cases are common in real-world
    driving scenarios, highlighting the need for enhanced capabilities in LLM-based
    autonomous driving agents to handle unforeseen circumstances. One potential direction
    for the future is to enable agents to learn from in-the-wild driving video/data
    and develop a better world model. Alternatively, allowing large language models
    to proactively seek human help in unforeseen circumstances could also help.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在遇到障碍等意外情况的闭环实验结果揭示了LLM代理在有效处理分布外极端情况方面的局限性。这些情况在实际驾驶场景中很常见，突显了LLM基础的自动驾驶代理需要提升处理意外情况的能力。未来的一个潜在方向是使代理能够从实际驾驶视频/数据中学习，并发展更好的世界模型。另一个可能的方向是允许大型语言模型在意外情况下主动寻求人工帮助。
- en: Language Generation from Embodied Experiences.
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 来自具身体验的语言生成。
- en: Furthermore, our investigation revealed that the language generated by our model
    tends to be oversimplified, primarily consisting of straightforward responses
    to human instructions or simplistic yes/no replies. Additionally, the model cannot
    initiate a dialogue with a human instructor, e.g., requesting additional advice
    or low-level instructions. Future work should focus on enhancing the model’s conversational
    initiative, enabling self-motivated dialogue.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的调查揭示了我们的模型生成的语言往往过于简单，主要是对人类指令的直接响应或简单的“是/否”回答。此外，该模型无法主动与人类导师发起对话，例如，要求额外的建议或低级别的指示。未来的工作应关注提升模型的对话主动性，实现自我激励的对话。
- en: Multi-turn Interactions and Instruction Following.
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多轮互动与指令跟随。
- en: Our closed-loop experiments also suggest the challenges of multi-turn interactions
    and instruction following. As the conversation goes on, the agent occasionally
    fails to retain previous long-horizon instructions, leading to wrong goal predictions
    and subsequent disruptions to the planning route. This issue underscores the critical
    importance of memory retention and context awareness in maintaining an agent model,
    particularly in situations where extensive dialogue exchange happens. Addressing
    these challenges through the implementation of memory-based mechanisms within
    LLM architectures or adding some memory modules in the autonomous driving agent
    framework could significantly enhance the agent’s ability to follow complex instructions
    in a complex environment that needs lots of human-agent collaboration.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的闭环实验还表明多轮互动和指令跟随的挑战。随着对话的进行，代理偶尔未能保留之前的长时间指令，导致错误的目标预测和后续规划路线的中断。这个问题突显了在维护代理模型中记忆保持和上下文意识的关键重要性，特别是在发生大量对话交换的情况下。通过在LLM架构中实施基于记忆的机制或在自动驾驶代理框架中添加一些记忆模块来解决这些挑战，可能会显著提高代理在复杂环境中跟随复杂指令的能力，这需要大量的人机合作。
- en: Limited Theory of Mind and Trust-worthiness.
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 有限的心智理论和可信度。
- en: Another critical limitation observed in our study is the absence of a situated
    Theory of Mind (ToM) [[28](#bib.bib28)] in the autonomous agent. At times, the
    agent misinterprets the instructor’s intentions, mistakenly perceiving low-level
    instructions as cues to abandon the previously provided long-horizon instruction
    and predict the goal incorrectly. The agent fails to recognize that the instruction
    may simply be specifying details within the ongoing long-horizon instructions.
    This highlights the need for autonomous driving agents with a nuanced understanding
    of the instructor’s intentions and context, enabling better agent modeling for
    their interaction partners, thus, gaining trust from humans.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中观察到的另一个关键限制是自主代理缺乏情境理论（Theory of Mind, ToM）[[28](#bib.bib28)]。有时，代理错误地解读了讲师的意图，错误地将低级指令视为放弃之前提供的长期指令的信号，并错误地预测目标。代理未能认识到这些指令可能只是对正在进行的长期指令中的细节进行说明。这突显了对具有细致理解讲师意图和上下文的自主驾驶代理的需求，从而更好地建模与其交互的伙伴，并获得人类的信任。
- en: Unacceptable Inference Time.
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不可接受的推理时间。
- en: Our model’s single inference time takes approximately 5 seconds, which significantly
    exceeds the interval between two decision points, posing a substantial challenge
    in real-world scenarios where rapid decision-making is imperative. While this
    delay is avoidable in a simulated environment through step-by-step simulation,
    addressing this inference time disparity is crucial for practical deployment.
    Future research directions may focus on distilling the model, leveraging hardware
    acceleration, or implementing efficient inference strategies to mitigate this
    bottleneck. This also raises a research problem of balancing the length of the
    Chain-of-Thought reasoning to reduce the inference time while keeping a comparable
    performance in task accomplishment.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的单次推理时间约为5秒，这远远超过了两个决策点之间的间隔，这在需要快速决策的实际场景中提出了重大挑战。虽然在模拟环境中通过逐步模拟可以避免这种延迟，但解决这一推理时间差异对于实际部署至关重要。未来的研究方向可以集中在精简模型、利用硬件加速或实施高效推理策略，以减轻这一瓶颈。这也提出了一个研究问题，即平衡思维链推理的长度，以减少推理时间，同时保持任务完成的相当性能。
- en: 8 Conclusion
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this work, we presented DriVLMe, an LLM-based autonomous driving agent that
    leverages both embodied experiences in a simulated environment and social experiences
    in real human dialogue. The egocentric perception and conversational interaction
    empower DriVLMe to engage in meaningful dialogues with human passengers while
    navigating complex driving environments. Through empirical evaluations, we demonstrated
    the effectiveness and versatility of DriVLMe in autonomous driving dialogue tasks,
    showcasing significant improvements in both physical action prediction and dialogue
    response generation metrics. Our findings have demonstrated the potential of DriVLMe
    in enabling human-agent communication and autonomous driving, and on the other
    hand, reveal ed several key limitations and challenges. of foundation models as
    AD agents, highlighting areas that need future enhancement.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了DriVLMe，这是一种基于LLM的自主驾驶代理，利用了在模拟环境中的具身经验和在真实人类对话中的社会经验。自我中心的感知和对话互动使DriVLMe能够在复杂的驾驶环境中与人类乘客进行有意义的对话。通过实证评估，我们展示了DriVLMe在自主驾驶对话任务中的有效性和多样性，显示出在物理行动预测和对话响应生成指标方面的显著改善。我们的研究结果展示了DriVLMe在促进人机沟通和自主驾驶方面的潜力，同时也揭示了作为AD代理的基础模型的一些关键限制和挑战，突显了需要未来改进的领域。
- en: Acknowledgment
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the Automotive Research Center (ARC) at the University
    of Michigan and NSF IIS1949634. The authors would like to thank the reviewers
    for their valuable feedback.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了密歇根大学汽车研究中心（ARC）和NSF IIS1949634的支持。作者感谢评审员的宝贵反馈。
- en: References
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等人 [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等人。Gpt-4技术报告。*arXiv预印本 arXiv:2303.08774*, 2023。
- en: Baca et al. [2003] Julie Baca, Feng Zheng, Hualin Gao, and Joseph Picone. Dialog
    systems for automotive environments. In *INTERSPEECH*, pages 1929–1932, 2003.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baca 等 [2003] Julie Baca, Feng Zheng, Hualin Gao, 和 Joseph Picone. 汽车环境中的对话系统。见
    *INTERSPEECH*，第1929–1932页，2003年。
- en: 'Banerjee and Lavie [2005] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic
    metric for mt evaluation with improved correlation with human judgments. In *Proceedings
    of the acl workshop on intrinsic and extrinsic evaluation measures for machine
    translation and/or summarization*, pages 65–72, 2005.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Banerjee 和 Lavie [2005] Satanjeev Banerjee 和 Alon Lavie. Meteor: 一种自动化指标，用于机器翻译评估，与人类判断的相关性得到改善。见
    *ACL 研讨会关于机器翻译和/或摘要的内在和外在评估措施的会议论文集*，第65–72页，2005年。'
- en: 'Caesar et al. [2020] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,
    Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar
    Beijbom. nuscenes: A multimodal dataset for autonomous driving. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    11621–11631, 2020.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caesar 等 [2020] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice
    Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, 和 Oscar Beijbom.
    nuscenes: 一个用于自动驾驶的多模态数据集。见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第11621–11631页，2020年。'
- en: 'Chen et al. [2023a] Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund,
    Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with
    llms: Fusing object-level vector modality for explainable autonomous driving.
    *arXiv preprint arXiv:2310.01957*, 2023a.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2023a] Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew
    James Willmott, Danny Birch, Daniel Maund, 和 Jamie Shotton. 利用 LLMS 驾驶: 融合物体级向量模态实现可解释的自动驾驶。*arXiv
    预印本 arXiv:2310.01957*，2023年。'
- en: 'Chen et al. [2023b] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas
    Geiger, and Hongyang Li. End-to-end autonomous driving: Challenges and frontiers.
    *arXiv preprint arXiv:2306.16927*, 2023b.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2023b] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas
    Geiger, 和 Hongyang Li. 端到端自动驾驶: 挑战与前沿。*arXiv 预印本 arXiv:2306.16927*，2023年。'
- en: Cui et al. [2024] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao
    Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. A survey on
    multimodal large language models for autonomous driving. In *Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision*, pages 958–979,
    2024.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等 [2024] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang,
    Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao 等。多模态大语言模型在自动驾驶中的综述。见 *IEEE/CVF
    计算机视觉应用冬季会议论文集*，第958–979页，2024年。
- en: 'Cui et al. [2023] Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar,
    Percy Liang, and Dorsa Sadigh. No, to the right: Online language corrections for
    robotic manipulation via shared autonomy. In *Proceedings of the 2023 ACM/IEEE
    International Conference on Human-Robot Interaction*, pages 93–101, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cui 等 [2023] Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar,
    Percy Liang, 和 Dorsa Sadigh. 不，往右: 通过共享自主进行机器人的在线语言修正。见 *2023年 ACM/IEEE 国际人机交互会议论文集*，第93–101页，2023年。'
- en: 'Deruyttere et al. [2019] Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic,
    Luc Van Gool, and Marie-Francine Moens. Talk2car: Taking control of your self-driving
    car. *arXiv preprint arXiv:1909.10838*, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deruyttere 等 [2019] Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic,
    Luc Van Gool, 和 Marie-Francine Moens. Talk2car: 控制你的自动驾驶汽车。*arXiv 预印本 arXiv:1909.10838*，2019年。'
- en: 'Dosovitskiy et al. [2017] Alexey Dosovitskiy, German Ros, Felipe Codevilla,
    Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In
    *Conference on robot learning*, pages 1–16\. PMLR, 2017.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dosovitskiy 等 [2017] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio
    Lopez, 和 Vladlen Koltun. Carla: 一个开放的城市驾驶模拟器。见 *机器人学习会议*，第1–16页，PMLR，2017年。'
- en: Dupuis and Grezlikowski [2006] Marius Dupuis and Han Grezlikowski. Opendrive®-an
    open standard for the description of roads in driving simulations. In *Proceedings
    of the Driving Simulation Conference*, pages 25–36, 2006.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dupuis 和 Grezlikowski [2006] Marius Dupuis 和 Han Grezlikowski. Opendrive®-用于驾驶仿真的道路描述的开放标准。见
    *驾驶模拟会议论文集*，第25–36页，2006年。
- en: Gao et al. [2024] Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, and Yiqing
    Shen. A survey for foundation models in autonomous driving. *arXiv preprint arXiv:2402.01105*,
    2024.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 [2024] Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, 和 Yiqing Shen.
    基础模型在自动驾驶中的综述。*arXiv 预印本 arXiv:2402.01105*，2024年。
- en: 'Gu et al. [2023] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy
    Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty
    Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for
    perception and planning. *arXiv preprint arXiv:2309.16650*, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. [2023] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy
    Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty
    Ellis, Rama Chellappa, 等. Conceptgraphs：用于感知和规划的开放词汇3D场景图。*arXiv 预印本 arXiv:2309.16650*，2023。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora：大型语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*，2021。
- en: Hu et al. [2023] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou
    Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented
    autonomous driving. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 17853–17862, 2023.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. [2023] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou
    Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, 等. 面向规划的自主驾驶。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，第17853–17862页，2023。
- en: 'Hu and Shu [2023] Zhiting Hu and Tianmin Shu. Language models, agent models,
    and world models: The law for machine reasoning and planning. *arXiv preprint
    arXiv:2312.05230*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 和 Shu [2023] Zhiting Hu 和 Tianmin Shu. 语言模型、代理模型和世界模型：机器推理和规划的规律。*arXiv 预印本
    arXiv:2312.05230*，2023。
- en: 'Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan James Richard Tompson, Igor Mordatch, Yevgen
    Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine,
    Karol Hausman, and Brian Andrew Ichter. Innermonologue: Embodied reasoning through
    planning with language models. 2022. CoRL 2022 (to appear).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan James Richard Tompson, Igor Mordatch, Yevgen
    Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine,
    Karol Hausman, 和 Brian Andrew Ichter. Innermonologue：通过语言模型进行的具身推理规划。2022年。CoRL
    2022（待出现）。
- en: 'Jin et al. [2023a] Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong
    Zhang, Yuhang Zheng, Guyue Zhou, and Jingjing Liu. Adapt: Action-aware driving
    caption transformer. In *2023 IEEE International Conference on Robotics and Automation
    (ICRA)*, pages 7554–7561\. IEEE, 2023a.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin et al. [2023a] Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong
    Zhang, Yuhang Zheng, Guyue Zhou, 和 Jingjing Liu. Adapt: 行动感知驱动描述转换器。发表于*2023 IEEE国际机器人与自动化大会（ICRA）*，第7554–7561页。IEEE，2023a。'
- en: 'Jin et al. [2023b] Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin,
    Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, and Jiangtao Gong. Surrealdriver:
    Designing generative driver agent simulation framework in urban contexts based
    on large language model. *arXiv preprint arXiv:2309.13193*, 2023b.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. [2023b] Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin,
    Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, 和 Jiangtao Gong. Surrealdriver：基于大型语言模型的城市环境下生成式驾驶员代理模拟框架设计。*arXiv
    预印本 arXiv:2309.13193*，2023b。
- en: Kendall et al. [2019] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur,
    Daniele Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning
    to drive in a day. In *2019 international conference on robotics and automation
    (ICRA)*, pages 8248–8254\. IEEE, 2019.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall et al. [2019] Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur,
    Daniele Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, 和 Amar Shah. 一天内学会驾驶。发表于*2019年国际机器人与自动化大会（ICRA）*，第8248–8254页。IEEE，2019。
- en: Kim et al. [2018] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and
    Zeynep Akata. Textual explanations for self-driving vehicles. *Proceedings of
    the European Conference on Computer Vision (ECCV)*, 2018.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2018] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, 和 Zeynep
    Akata. 自驾驶车辆的文本解释。*欧洲计算机视觉大会（ECCV）论文集*，2018。
- en: 'Lee et al. [2004] Bowon Lee, Mark Hasegawa-Johnson, Camille Goudeseune, Suketu
    Kamdar, Sarah Borys, Ming Liu, and Thomas Huang. Avicar: Audio-visual speech corpus
    in a car environment. In *Eighth International Conference on Spoken Language Processing*,
    2004.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2004] Bowon Lee, Mark Hasegawa-Johnson, Camille Goudeseune, Suketu
    Kamdar, Sarah Borys, Ming Liu, 和 Thomas Huang. Avicar：车载环境中的视听语音语料库。发表于*第八届国际语音语言处理会议*，2004。
- en: 'Li et al. [2024] Jialu Li, Aishwarya Padmakumar, Gaurav Sukhatme, and Mohit
    Bansal. Vln-video: Utilizing driving videos for outdoor vision-and-language navigation.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, 2024.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024] Jialu Li, Aishwarya Padmakumar, Gaurav Sukhatme, 和 Mohit Bansal.
    Vln-video：利用驾驶视频进行户外视觉与语言导航。发表于*AAAI人工智能大会论文集*，2024。
- en: Li et al. [2023] Xin Li, Yeqi Bai, Pinlong Cai, Licheng Wen, Daocheng Fu, Bo
    Zhang, Xuemeng Yang, Xinyu Cai, Tao Ma, Jianfei Guo, et al. Towards knowledge-driven
    autonomous driving. *arXiv preprint arXiv:2312.04316*, 2023.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023] Xin Li, Yeqi Bai, Pinlong Cai, Licheng Wen, Daocheng Fu, Bo
    Zhang, Xuemeng Yang, Xinyu Cai, Tao Ma, Jianfei Guo, 等人。走向知识驱动的自动驾驶。*arXiv 预印本
    arXiv:2312.04316*，2023年。
- en: Liu et al. [2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual
    instruction tuning. 2023.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023] Haotian Liu, Chunyuan Li, Qingyang Wu 和 Yong Jae Lee。视觉指令调优。2023年。
- en: 'Ma et al. [2023a] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei
    Xiao. Dolphins: Multimodal language model for driving. *arXiv preprint arXiv:2312.00438*,
    2023a.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2023a] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone 和 Chaowei
    Xiao。Dolphins：用于驾驶的多模态语言模型。*arXiv 预印本 arXiv:2312.00438*，2023年。
- en: 'Ma et al. [2022] Ziqiao Ma, Benjamin VanDerPloeg, Cristian-Paul Bara, Yidong
    Huang, Eui-In Kim, Felix Gervits, Matthew Marge, and Joyce Chai. DOROTHIE: Spoken
    dialogue for handling unexpected situations in interactive autonomous driving
    agents. In *Findings of the Association for Computational Linguistics: EMNLP 2022*,
    pages 4800–4822, Abu Dhabi, United Arab Emirates, 2022.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2022] Ziqiao Ma, Benjamin VanDerPloeg, Cristian-Paul Bara, Yidong
    Huang, Eui-In Kim, Felix Gervits, Matthew Marge 和 Joyce Chai。DOROTHIE：处理互动自动驾驶代理中意外情况的语音对话。发表于
    *计算语言学协会的发现：EMNLP 2022*，第4800–4822页，阿布扎比，阿联酋，2022年。
- en: 'Ma et al. [2023b] Ziqiao Ma, Jacob Sansom, Run Peng, and Joyce Chai. Towards
    a holistic landscape of situated theory of mind in large language models. In *Findings
    of the Association for Computational Linguistics: EMNLP 2023*, pages 1011–1031,
    2023b.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2023b] Ziqiao Ma, Jacob Sansom, Run Peng 和 Joyce Chai。走向大语言模型中的全面情境理论。发表于
    *计算语言学协会的发现：EMNLP 2023*，第1011–1031页，2023年。
- en: 'Maaz et al. [2024] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz
    Khan. Video-chatgpt: Towards detailed video understanding via large vision and
    language models. In *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*, 2024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maaz et al. [2024] Muhammad Maaz, Hanoona Rasheed, Salman Khan 和 Fahad Shahbaz
    Khan。Video-chatgpt：通过大规模视觉和语言模型实现详细的视频理解。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2024年。
- en: 'Mao et al. [2023] Jiageng Mao, Yuxi Qian, Junjie Ye, Hang Zhao, and Yue Wang.
    Gpt-driver: Learning to drive with gpt. In *NeurIPS 2023 Foundation Models for
    Decision Making Workshop*, 2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao et al. [2023] Jiageng Mao, Yuxi Qian, Junjie Ye, Hang Zhao 和 Yue Wang。Gpt-driver：利用gpt学习驾驶。在
    *NeurIPS 2023 决策制定基础模型研讨会*，2023年。
- en: 'Marge et al. [2022] Matthew Marge, Carol Espy-Wilson, Nigel G Ward, Abeer Alwan,
    Yoav Artzi, Mohit Bansal, Gil Blankenship, Joyce Chai, Hal Daumé III, et al. Spoken
    language interaction with robots: Recommendations for future research. *Computer
    Speech & Language*, 71:101255, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marge et al. [2022] Matthew Marge, Carol Espy-Wilson, Nigel G Ward, Abeer Alwan,
    Yoav Artzi, Mohit Bansal, Gil Blankenship, Joyce Chai, Hal Daumé III 等人。与机器人进行口语语言互动：对未来研究的建议。*计算机语音与语言*，71：101255，2022年。
- en: Minato et al. [2023] Takashi Minato, Ryuichiro Higashinaka, Kurima Sakai, Tomo
    Funayama, Hiromitsu Nishizaki, and Takayuki Nagai. Design of a competition specifically
    for spoken dialogue with a humanoid robot. *Advanced Robotics*, 37(21):1349–1363,
    2023.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minato et al. [2023] Takashi Minato, Ryuichiro Higashinaka, Kurima Sakai, Tomo
    Funayama, Hiromitsu Nishizaki 和 Takayuki Nagai。专为人形机器人语音对话设计的竞赛。*先进机器人学*，37(21)：1349–1363，2023年。
- en: 'Mu et al. [2023] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding,
    Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language
    pre-training via embodied chain of thought. In *Advances in Neural Information
    Processing Systems*, 2023.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu et al. [2023] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding,
    Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao 和 Ping Luo。Embodiedgpt：通过具身思维链进行视觉-语言预训练。在
    *神经信息处理系统进展*，2023年。
- en: Nguyen et al. [2022] Khanh X Nguyen, Yonatan Bisk, and Hal Daumé Iii. A framework
    for learning to request rich and contextually useful information from humans.
    In *International Conference on Machine Learning*, pages 16553–16568\. PMLR, 2022.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. [2022] Khanh X Nguyen, Yonatan Bisk 和 Hal Daumé III。一个用于学习从人类那里请求丰富且具有上下文信息的框架。在
    *国际机器学习会议*，第16553–16568页。PMLR，2022年。
- en: OpenAI [2023] OpenAI. Gpt-4v(ision) system card, 2023.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI。Gpt-4v(ision) 系统卡，2023年。
- en: 'Padmakumar et al. [2022] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava,
    Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan
    Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. In *AAAI*,
    2022.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Padmakumar et al. [2022] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava,
    Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan
    Tur 和 Dilek Hakkani-Tur。Teach：任务驱动的具身代理对话系统。在 *AAAI*，2022年。
- en: Pellom et al. [2001] Bryan Pellom, Wayne Ward, John Hansen, Ronald Cole, Kadri
    Hacioglu, Jianping Zhang, Xiuyang Yu, and Sameer Pradhan. University of colorado
    dialogue systems for travel and navigation. In *Proceedings of the first international
    conference on Human language technology research*, 2001.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pellom 等人 [2001] Bryan Pellom, Wayne Ward, John Hansen, Ronald Cole, Kadri Hacioglu,
    Jianping Zhang, Xiuyang Yu 和 Sameer Pradhan. 科罗拉多大学的旅行和导航对话系统。在*第一届国际人类语言技术研究会议*上，2001年。
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark 等人. 从自然语言监督中学习可转移的视觉模型。在*国际机器学习会议*上，页码 8748–8763。PMLR，2021。
- en: 'Rajbhandari et al. [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*, pages 1–16\. IEEE, 2020.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari 等人 [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase 和 Yuxiong
    He. Zero: 训练万亿参数模型的内存优化。在*SC20: 高性能计算、网络、存储和分析国际会议*上，页码 1–16。IEEE，2020。'
- en: 'Ren et al. [2023] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh,
    Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al.
    Robots that ask for help: Uncertainty alignment for large language model planners.
    In *Conference on Robot Learning*, pages 661–682\. PMLR, 2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 [2023] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen
    Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley 等人. 请求帮助的机器人：针对大型语言模型规划器的误差对齐。在*机器人学习会议*上，页码
    661–682。PMLR，2023。
- en: Roh et al. [2020] Junha Roh, Chris Paxton, Andrzej Pronobis, Ali Farhadi, and
    Dieter Fox. Conditional driving from natural language instructions. In *Proceedings
    of the Conference on Robot Learning*, pages 540–551, 2020.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roh 等人 [2020] Junha Roh, Chris Paxton, Andrzej Pronobis, Ali Farhadi 和 Dieter
    Fox. 从自然语言指令中进行条件驾驶。在*机器人学习会议*上，页码 540–551，2020。
- en: 'Schick et al. [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    Toolformer: Language models can teach themselves to use tools. In *Advances in
    Neural Information Processing Systems*, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人 [2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda 和 Thomas Scialom.
    Toolformer: 语言模型可以自学使用工具。在*神经信息处理系统进展*上，2023。'
- en: Schwarting et al. [2018] Wilko Schwarting, Javier Alonso-Mora, and Daniela Rus.
    Planning and decision-making for autonomous vehicles. *Annual Review of Control,
    Robotics, and Autonomous Systems*, 1:187–210, 2018.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwarting 等人 [2018] Wilko Schwarting, Javier Alonso-Mora 和 Daniela Rus. 自动驾驶车辆的规划与决策。*控制、机器人及自主系统年评*，1:187–210，2018。
- en: Schwarting et al. [2019] Wilko Schwarting, Alyssa Pierson, Javier Alonso-Mora,
    Sertac Karaman, and Daniela Rus. Social behavior for autonomous vehicles. *Proceedings
    of the National Academy of Sciences*, 116(50):24972–24978, 2019.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwarting 等人 [2019] Wilko Schwarting, Alyssa Pierson, Javier Alonso-Mora, Sertac
    Karaman 和 Daniela Rus. 自动驾驶车辆的社会行为。*国家科学院院刊*，116(50):24972–24978，2019。
- en: 'Sha et al. [2023] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping
    Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, and Mingyu Ding. Languagempc:
    Large language models as decision makers for autonomous driving. *arXiv preprint
    arXiv:2310.03026*, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sha 等人 [2023] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo,
    Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan 和 Mingyu Ding. Languagempc: 大型语言模型作为自动驾驶决策者。*arXiv
    预印本 arXiv:2310.03026*，2023。'
- en: 'Shah et al. [2023] Dhruv Shah, Błażej Osiński, Sergey Levine, et al. Lm-nav:
    Robotic navigation with large pre-trained models of language, vision, and action.
    In *Conference on robot learning*, pages 492–504\. PMLR, 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shah 等人 [2023] Dhruv Shah, Błażej Osiński, Sergey Levine 等人. Lm-nav: 使用大型预训练语言、视觉和行动模型的机器人导航。在*机器人学习会议*上，页码
    492–504。PMLR，2023。'
- en: 'Shao et al. [2023] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander, Yu
    Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language
    models. *arXiv preprint arXiv:2312.07488*, 2023.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao 等人 [2023] Hao Sha, Yuxuan Hu, Letian Wang, Steven L Waslander, Yu Liu
    和 Hongsheng Li. Lmdrive: 利用大型语言模型进行闭环端到端驾驶。*arXiv 预印本 arXiv:2312.07488*，2023。'
- en: Sharma et al. [2022] Pratyusha Sharma, Balakumar Sundaralingam, Valts Blukis,
    Chris Paxton, Tucker Hermans, Antonio Torralba, Jacob Andreas, and Dieter Fox.
    Correcting robot plans with natural language feedback. *arXiv preprint arXiv:2204.05186*,
    2022.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 [2022] Pratyusha Sharma、Balakumar Sundaralingam、Valts Blukis、Chris
    Paxton、Tucker Hermans、Antonio Torralba、Jacob Andreas 和 Dieter Fox。使用自然语言反馈纠正机器人计划。*arXiv
    预印本 arXiv:2204.05186*，2022年。
- en: 'Sima et al. [2023] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue
    Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving
    with graph visual question answering. *arXiv preprint arXiv:2312.14150*, 2023.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sima 等人 [2023] Chonghao Sima、Katrin Renz、Kashyap Chitta、Li Chen、Hanxue Zhang、Chengen
    Xie、Ping Luo、Andreas Geiger 和 Hongyang Li。Drivelm：通过图形视觉问答进行驾驶。*arXiv 预印本 arXiv:2312.14150*，2023年。
- en: 'Sriram et al. [2019] NN Sriram, Tirth Maniar, Jayaganesh Kalyanasundaram, Vineet
    Gandhi, Brojeshwar Bhowmick, and K Madhava Krishna. Talk to the vehicle: Language
    conditioned autonomous navigation of self driving cars. In *2019 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, pages 5284–5290\. IEEE,
    2019.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sriram 等人 [2019] NN Sriram、Tirth Maniar、Jayaganesh Kalyanasundaram、Vineet Gandhi、Brojeshwar
    Bhowmick 和 K Madhava Krishna。与车辆对话：基于语言的自主导航自驾车。在*2019 IEEE/RSJ 国际智能机器人与系统大会 (IROS)*，页码
    5284–5290。IEEE，2019年。
- en: Thomason et al. [2020] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke
    Zettlemoyer. Vision-and-dialog navigation. In *Conference on Robot Learning*,
    pages 394–406\. PMLR, 2020.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thomason 等人 [2020] Jesse Thomason、Michael Murray、Maya Cakmak 和 Luke Zettlemoyer。视觉与对话导航。在*机器人学习会议*，页码
    394–406。PMLR，2020年。
- en: 'Tian et al. [2024] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu,
    Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence
    of autonomous driving and large vision-language models. *arXiv preprint arXiv:2402.12289*,
    2024.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等人 [2024] Xiaoyu Tian、Junru Gu、Bailin Li、Yicheng Liu、Chenxu Hu、Yang Wang、Kun
    Zhan、Peng Jia、Xianpeng Lang 和 Hang Zhao。Drivevlm：自动驾驶与大型视觉语言模型的融合。*arXiv 预印本 arXiv:2402.12289*，2024年。
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023] Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。
- en: 'van den Heuvel et al. [1999] Henk van den Heuvel, Jérôme Boudy, Robrecht Comeyne,
    Stephan Euler, Asunción Moreno, and Gaël Richard. The speechdat-car multilingual
    speech databases for in-car applications: some first validation results. In *EUROSPEECH*,
    1999.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van den Heuvel 等人 [1999] Henk van den Heuvel、Jérôme Boudy、Robrecht Comeyne、Stephan
    Euler、Asunción Moreno 和 Gaël Richard。Speechdat-car 多语言语音数据库用于车载应用：一些初步验证结果。在*EUROSPEECH*，1999年。
- en: 'Vasudevan et al. [2021] Arun Balajee Vasudevan, Dengxin Dai, and Luc Van Gool.
    Talk2nav: Long-range vision-and-language navigation with dual attention and spatial
    memory. *International Journal of Computer Vision*, 129(1):246–266, 2021.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vasudevan 等人 [2021] Arun Balajee Vasudevan、Dengxin Dai 和 Luc Van Gool。Talk2nav：长距离视觉与语言导航，具有双重注意力和空间记忆。*《计算机视觉国际期刊》*，129(1)：246–266，2021年。
- en: 'Vedantam et al. [2015] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    Cider: Consensus-based image description evaluation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 4566–4575, 2015.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vedantam 等人 [2015] Ramakrishna Vedantam、C Lawrence Zitnick 和 Devi Parikh。Cider：基于共识的图像描述评估。在*《IEEE计算机视觉与模式识别会议论文集》*，页码
    4566–4575，2015年。
- en: 'Vinitsky et al. [2022] Eugene Vinitsky, Nathan Lichtlé, Xiaomeng Yang, Brandon
    Amos, and Jakob Foerster. Nocturne: a scalable driving benchmark for bringing
    multi-agent learning one step closer to the real world. *Advances in Neural Information
    Processing Systems*, 35:3962–3974, 2022.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinitsky 等人 [2022] Eugene Vinitsky、Nathan Lichtlé、Xiaomeng Yang、Brandon Amos
    和 Jakob Foerster。Nocturne：一个可扩展的驾驶基准，将多智能体学习更进一步接近现实世界。*《神经信息处理系统进展》*，35：3962–3974，2022年。
- en: 'Wang et al. [2022] Wenshuo Wang, Letian Wang, Chengyuan Zhang, Changliu Liu,
    Lijun Sun, et al. Social interactions for autonomous driving: A review and perspectives.
    *Foundations and Trends® in Robotics*, 10(3-4):198–376, 2022.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] Wenshuo Wang、Letian Wang、Chengyuan Zhang、Changliu Liu、Lijun Sun
    等。自动驾驶中的社交互动：回顾与展望。*《机器人学基础与趋势》*，10(3-4)：198–376，2022年。
- en: 'Wang et al. [2023] Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan
    Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, et al. Drivemlm: Aligning
    multi-modal large language models with behavioral planning states for autonomous
    driving. *arXiv preprint arXiv:2312.09245*, 2023.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2023] **王文海**、**谢江伟**、**胡川阳**、**邹浩铭**、**范剑南**、**童文文**、**文杨**、**吴思磊**、**邓汉铭**
    等。Drivemlm: 将多模态大语言模型与自主驾驶的行为规划状态对齐。*arXiv 预印本 arXiv:2312.09245*，2023年。'
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837, 2022.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人 [2022] **魏杰森**、**王学之**、**戴尔·舒尔曼斯**、**马滕·博斯玛**、**夏飞**、**埃德·奇**、**阮玮龙**、**丹尼·周**
    等。链式思维提示在大语言模型中引发推理。*神经信息处理系统进展*，35:24824–24837，2022年。
- en: 'Wen et al. [2023] Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong
    Cai, Min Dou, Botian Shi, Liang He, and Yu Qiao. Dilu: A knowledge-driven approach
    to autonomous driving with large language models. *arXiv preprint arXiv:2309.16292*,
    2023.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文等人 [2023] **温立诚**、**傅道诚**、**李欣**、**蔡欣宇**、**马涛**、**蔡品龙**、**窦敏**、**施博天**、**何亮**
    和 **姚昱**。Dilu: 一种基于知识的大语言模型自主驾驶方法。*arXiv 预印本 arXiv:2309.16292*，2023年。'
- en: 'Weng et al. [2016] Fuliang Weng, Pongtep Angkititrakul, Elizabeth E Shriberg,
    Larry Heck, Stanley Peters, and John HL Hansen. Conversational in-vehicle dialog
    systems: The past, present, and future. *IEEE Signal Processing Magazine*, 33(6):49–60,
    2016.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翁等人 [2016] **翁福良**、**龚贴**、**伊丽莎白·E·施里伯格**、**拉里·赫克**、**斯坦利·彼得斯** 和 **约翰·HL·汉森**。对话式车载对话系统：过去、现在和未来。*IEEE信号处理杂志*，33(6):49–60，2016年。
- en: 'Xiang et al. [2023] Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang,
    Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences
    enhance language models. *Advances in neural information processing systems*,
    36, 2023.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向等人 [2023] **向建南**、**陶天华**、**顾毅**、**舒天敏**、**王子瑞**、**杨子超** 和 **胡志亭**。语言模型遇见世界模型：具身体验增强语言模型。*神经信息处理系统进展*，36，2023年。
- en: 'Xu et al. [2023] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kenneth KY
    Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous
    driving via large language model. *arXiv preprint arXiv:2310.01412*, 2023.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '徐等人 [2023] **徐振华**、**张钰佳**、**谢恩泽**、**赵震**、**郭永**、**肯尼斯·KY·黄**、**李正国** 和 **赵恒霜**。Drivegpt4:
    通过大语言模型进行可解释的端到端自主驾驶。*arXiv 预印本 arXiv:2310.01412*，2023年。'
- en: 'Yan et al. [2024] Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao
    Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, et al. Forging vision
    foundation models for autonomous driving: Challenges, methodologies, and opportunities.
    *arXiv preprint arXiv:2401.08045*, 2024.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 闫等人 [2024] **闫旭**、**张海铭**、**蔡英杰**、**郭靖明**、**邱伟超**、**高彬**、**周凯强**、**赵岳**、**金欢**、**高建涛**
    等。锻造视觉基础模型用于自主驾驶：挑战、方法论和机遇。*arXiv 预印本 arXiv:2401.08045*，2024年。
- en: 'Yang et al. [2023] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan,
    Madhavan Iyengar, David F Fouhey, and Joyce Chai. Llm-grounder: Open-vocabulary
    3d visual grounding with large language model as an agent. *arXiv preprint arXiv:2309.12311*,
    2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '杨等人 [2023] **杨佳宁**、**陈旭伟**、**钱胜义**、**尼基尔·马丹**、**马达万·艾扬**、**大卫·F·富赫** 和 **乔伊斯·蔡**。Llm-grounder:
    使用大语言模型作为代理的开放词汇3D视觉定位。*arXiv 预印本 arXiv:2309.12311*，2023年。'
- en: 'Yu et al. [2020] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen,
    Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving
    dataset for heterogeneous multitask learning. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, pages 2636–2645, 2020.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '于等人 [2020] **于菲舍尔**、**陈浩峰**、**王欣**、**谢文琪**、**陈颖颖**、**刘芳辰**、**瓦希什特·马达万** 和 **特雷弗·达雷尔**。Bdd100k:
    一个用于异质多任务学习的多样化驾驶数据集。见 *IEEE/CVF计算机视觉与模式识别会议论文集*，第2636–2645页，2020年。'
- en: 'Yu et al. [2024] Shoubin Yu, Jaehong Yoon, and Mohit Bansal. Crema: Multimodal
    compositional video reasoning via efficient modular adaptation and fusion. *arXiv
    preprint arXiv:2402.05889*, 2024.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '于等人 [2024] **石彬·于**、**崔青龙** 和 **莫希特·班萨尔**。Crema: 通过高效的模块适应和融合进行多模态组合视频推理。*arXiv
    预印本 arXiv:2402.05889*，2024年。'
- en: 'Yuan et al. [2024] Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul
    Newman, Lars Kunze, and Matthew Gadd. Rag-driver: Generalisable driving explanations
    with retrieval-augmented in-context learning in multi-modal large language model.
    *arXiv preprint arXiv:2402.10828*, 2024.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '袁等人 [2024] **袁建浩**、**孙硕阳**、**丹尼尔·欧梅扎**、**博·赵**、**保罗·纽曼**、**拉尔斯·昆泽** 和 **马修·加德**。Rag-driver:
    通过检索增强的上下文学习在多模态大语言模型中的可泛化驾驶解释。*arXiv 预印本 arXiv:2402.10828*，2024年。'
- en: 'Zhang et al. [2019] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. In *International
    Conference on Learning Representations*, 2019.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等人 [2019] 张天一、瓦尔莎·基肖尔、费利克斯·吴、基利安·Q·温伯格、约瓦·阿尔茨。Bertscore: 用 BERT 评估文本生成。发表于
    *国际学习表征会议*，2019年。'
- en: 'Zhang et al. [2024] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah,
    Qiaozi Gao, and Joyce Chai. Groundhog: Grounding large language models to holistic
    segmentation. In *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*, 2024.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等人 [2024] 张艺池、马子乔、高晓峰、苏海拉·沙基亚、郝乔子、柴乔伊。Groundhog: 将大型语言模型用于整体分割。发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2024年。'
- en: 'Zhou et al. [2020] Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David
    Rusu, Jiayu Miao, Weinan Zhang, Montgomery Alban, Iman Fadakar, Zheng Chen, et al.
    Smarts: Scalable multi-agent reinforcement learning training school for autonomous
    driving. *arXiv preprint arXiv:2010.09776*, 2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '周等人 [2020] 周明、罗军、朱利安·维莱拉、姚东洋、大卫·鲁苏、苗佳宇、张伟南、蒙哥马利·阿尔班、伊曼·法达卡尔、郑晨等人。Smarts: 可扩展的多智能体强化学习培训系统用于自动驾驶。*arXiv
    预印本 arXiv:2010.09776*，2020年。'
- en: Appendix
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 8.1 Language Templates for Verbalizing the Embodied Experiences.
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 语言模板用于表达体现的经验。
- en: 'With the data about the surrounding environment, we use templates to generate
    synthetic data as the caption of the input video:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 使用关于周围环境的数据，我们使用模板生成合成数据作为输入视频的标题：
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distance and Turning Decisions: For the distance to the road end, we generated
    different outputs based on the distance recorded. When the distance is larger
    than 10, we used the prompt “I am far from the end of the road. I don’t need to
    make a decision for turning now.” When the distance is larger than 5 while smaller
    than 10, we used the prompt “I am near the end of the road. I don’t need to make
    a decision for turning now.” When the distance is smaller than 5, we used the
    prompt: “I am at the end of the road, I need to stop if there is a red light,
    or make a decision to turn left, turn right, or go straight now.”'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 距离和转弯决策：对于到达道路尽头的距离，我们根据记录的距离生成不同的输出。当距离大于 10 时，我们使用提示 “我距离道路尽头很远。我现在不需要做转弯决定。”
    当距离大于 5 小于 10 时，我们使用提示 “我靠近道路尽头。我现在不需要做转弯决定。” 当距离小于 5 时，我们使用提示：“我在道路尽头，如果有红灯我需要停车，或者现在做出左转、右转或直行的决定。”
- en: •
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lane and Lane Switching Decisions: For the lane information, we used the prompt
    “I’m on the {lane_number} lane from the left of the road”, and based on whether
    a lane change is affordable, we chose from the 4 prompts: “I’m not able to change
    lane”, “I’m only able to change to the right lane”, “I’m only able to change to
    the left lane”, “I’m able to change to both right and left lane.”'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 车道和车道切换决策：对于车道信息，我们使用提示 “我在道路左侧的 {lane_number} 车道”，并根据是否可以进行车道变更，我们选择以下 4 个提示中的一个：“我不能变更车道”，“我只能变更到右侧车道”，“我只能变更到左侧车道”，“我可以变更到左右两个车道。”
- en: •
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Object and Stop Decisions: For each object in front, we used the template “There
    is a obstacle {object_type} in front of me, the distance is {distance}.” For the
    object type, we used the object class in CARLA (e.g. vehicle, pedestrian, traffic
    sign).'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对象和停止决策：对于前方的每个对象，我们使用模板 “前方有一个 {object_type}，距离是 {distance}。” 对于对象类型，我们使用 CARLA
    中的对象类别（例如，车辆、行人、交通标志）。
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Signs and Stop Decisions: For each traffic sign in front, we used the template
    “There is a {sign_name} that is {distance} meters from me, showing {state}.” The
    sign_name is the name of the sign while the state is the information the sign
    displayed (e.g., red/green for lights, posted speed limits).'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标志和停止决策：对于前方的每个交通标志，我们使用模板 “前方有一个 {sign_name}，距离我 {distance} 米，显示 {state}。”
    sign_name 是标志的名称，而 state 是标志显示的信息（例如，红灯/绿灯，限速标志）。
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Weather: For the weather, we straightly described that using the template “It’s
    {weather}.”'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 天气：对于天气，我们直接描述为使用模板 “天气是 {weather}。”
- en: 8.2 Prompt Engineering for GPT-4 Baseline
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 GPT-4 基准的提示工程
- en: 'Each prompt template we used for the GPT-4 baseline consists of the following
    components in order:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为 GPT-4 基准使用的每个提示模板包含以下组件：
- en: '1.'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Image: For the vision-enabled model only (GPT-4V, not GPT-4), we prepended
    an image of the third-person driver view.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像：仅适用于启用视觉的模型（GPT-4V，而非 GPT-4），我们在前面加上了第三人称驾驶员视角的图像。
- en: '2.'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Header: Informs GPT that it must act as a Chauffeur, piloting a car while talking
    with its passenger.'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Header: 通知 GPT 它必须充当司机，在驾驶汽车的同时与乘客交谈。'
- en: '3.'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Dialogue History: Turn-by-turn record of the conversation between passenger
    and driver prior to the time of prompting.'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话历史：在提示时间之前，乘客与司机之间的逐轮对话记录。
- en: '4.'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Current Map: A text-based representation displaying the map along with landmarks,
    street names, and the vehicle location'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前地图：一个基于文本的表示，显示地图及地标、街道名称和车辆位置。
- en: '5.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Physical Action History: Turn-by-turn record of the previous physical actions
    taken by the driver.'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物理行动历史：司机先前采取的逐轮物理行动记录。
- en: '6.'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Planner: Asks GPT to call a planning module using the form plan(landmark).
    If GPT both uses this API correctly and selects the correct landmark, the planning
    module provides the plan (a sequence of turns at each intersection).'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规划器：要求 GPT 调用规划模块，使用格式 plan(landmark)。如果 GPT 正确使用此 API 并选择正确的地标，规划模块将提供计划（每个交叉口的转向序列）。
- en: '7.'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Question 1: For NfD, this segment asks GPT a multiple-choice navigational question.
    For RfN, it asks GPT what type of dialogue it would like to output.'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题 1：对于 NfD，该部分向 GPT 提出一个多项选择导航问题。对于 RfN，它询问 GPT 希望输出哪种类型的对话。
- en: '8.'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Question 2: For NfD, if the correct action takes an argument (e.g., for turning,
    the argument is a direction), this segment asks for the argument in a multiple-choice
    format. For RfN, this segment asks for the natural language dialogue. For question
    2, we utilize teacher forcing, providing the GPT model with the correct answer
    to question 1 even if it is answered incorrectly.'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题 2：对于 NfD，如果正确的动作需要一个参数（例如，对于转弯，参数是方向），该部分会以多项选择的形式询问参数。对于 RfN，该部分询问自然语言对话。对于问题
    2，我们使用教师强制，向 GPT 模型提供问题 1 的正确答案，即使其回答不正确。
- en: 8.3 Ethics Statement
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 伦理声明
- en: The institution’s Institutional Review Board (IRB) considered this project exempt
    from ongoing review, registered under eResearch ID HUM00205133. The SDN and BDD-X
    datasets contain human-generated contents. Our use of both datasets is in compliance
    with their licenses and exclusively for research purposes.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 机构的机构审查委员会（IRB）认为该项目免于持续审查，注册在 eResearch ID HUM00205133 下。SDN 和 BDD-X 数据集包含人类生成的内容。我们对这两个数据集的使用符合其许可要求，并且仅用于研究目的。
