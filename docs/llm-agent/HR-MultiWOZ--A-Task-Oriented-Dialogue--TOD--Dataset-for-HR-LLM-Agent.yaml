- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HR-MultiWOZ：一个面向任务的对话（TOD）数据集，用于 HR LLM 代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.01018](https://ar5iv.labs.arxiv.org/html/2402.01018)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.01018](https://ar5iv.labs.arxiv.org/html/2402.01018)
- en: Weijie Xu¹, Zicheng Huang¹, Wenxiang Hu¹, Xi Fang¹, Rajesh Kumar Cherukuri¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Weijie Xu¹, Zicheng Huang¹, Wenxiang Hu¹, Xi Fang¹, Rajesh Kumar Cherukuri¹,
- en: Naumaan Nayyar¹, Lorenzo Malandri², Srinivasan H. Sengamedu¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Naumaan Nayyar¹, Lorenzo Malandri², Srinivasan H. Sengamedu¹
- en: ¹Amazon
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹亚马逊
- en: ²University of Milano-Bicocca
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²米兰比可卡大学
- en: weijiexu@amazon.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: weijiexu@amazon.com
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Recent advancements in Large Language Models (LLMs) have been reshaping Natural
    Language Processing (NLP) task in several domains. Their use in the field of Human
    Resources (HR) has still room for expansions and could be beneficial for several
    time consuming tasks. Examples such as time-off submissions, medical claims filing,
    and access requests are noteworthy, but they are by no means the sole instances.
    However the aforementioned developments must grapple with the pivotal challenge
    of constructing a high-quality training dataset. On one hand, most conversation
    datasets are solving problems for customers not employees. On the other hand,
    gathering conversations with HR could raise privacy concerns. To solve it, we
    introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10
    HR domains. Our work has the following contributions: (1) It is the first labeled
    open-sourced conversation dataset in the HR domain for NLP research. (2) It provides
    a detailed recipe for the data generation procedure along with data analysis and
    human evaluations. The data generation pipeline is transferable and can be easily
    adapted for labeled conversation data generation in other domains. (3) The proposed
    data-collection pipeline is mostly based on LLMs with minimal human involvement
    for annotation, which is time and cost-efficient.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）的进步正在重塑多个领域的自然语言处理（NLP）任务。在人力资源（HR）领域的应用仍有扩展空间，并可能对许多耗时的任务有所裨益。例如，休假申请、医疗索赔和访问请求等任务值得关注，但这并不是唯一的实例。然而，上述进展必须面对构建高质量训练数据集的关键挑战。一方面，大多数对话数据集是为客户而非员工解决问题。另一方面，收集与
    HR 相关的对话可能会引发隐私问题。为了解决这一问题，我们介绍了 HR-Multiwoz，这是一个完全标注的数据集，包含 550 个跨 10 个 HR 领域的对话。我们的工作有以下贡献：（1）这是第一个用于
    NLP 研究的开源标注对话数据集。（2）提供了详细的数据生成过程、数据分析和人工评估的方法。数据生成流程是可转移的，可以轻松地适应其他领域的标注对话数据生成。（3）所提出的数据收集流程主要基于
    LLMs，人工标注参与较少，时间和成本效率高。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advances in natural language processing (NLP) have been applied in a
    variety of tasks in the Human Resources (HR) domain ranging from skill extraction (Zhang
    et al., [2022](#bib.bib17)), job understanding (Decorte et al., [2021](#bib.bib3))
    to candidate sourcing (Hemamou and Coleman, [2022](#bib.bib6)). However, numerous
    HR processes remain highly inefficient, such as requesting time off, scheduling
    meetings, submitting tickets for IT issues, or filing medical claims. In fact,
    the Asana Work Index report shows that knowledge workers spend 60 percent of their
    time on repetitive work.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在自然语言处理（NLP）方面的进展已经应用于人力资源（HR）领域的各种任务，从技能提取（Zhang et al., [2022](#bib.bib17)）、职位理解（Decorte
    et al., [2021](#bib.bib3)）到候选人招聘（Hemamou and Coleman, [2022](#bib.bib6)）。然而，许多人力资源流程仍然非常低效，如请假、安排会议、提交
    IT 问题工单或提交医疗索赔。事实上，Asana 工作指数报告显示，知识工作者将 60% 的时间花在重复的工作上。
- en: LLM agent (Gao et al., [2023](#bib.bib4)) uses LLMs as its central computational
    engine, allowing it to carry on conversations, do tasks, reason, and display a
    degree of autonomy. Similar to other domains (Kalvakurthi et al., [2023](#bib.bib8);
    Hsu et al., [2023](#bib.bib7)), creating an LLM agent to help with these tasks
    could save a significant amount of time for employees and improve job satisfaction.
    A good LLM agent should be able to understand the requirements of users (Liu et al.,
    [2023](#bib.bib11)). The ideal dataset to evaluate or train an HR LLM agent should
    contain conversations between a virtual assistant and employees, annotated with
    dialogue states. Dialogue states contain representations of a conversation’s current
    context such as intentions and relevant information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理（Gao et al., [2023](#bib.bib4)）使用LLM作为其核心计算引擎，使其能够进行对话、完成任务、推理，并展示一定程度的自主性。与其他领域类似（Kalvakurthi
    et al., [2023](#bib.bib8); Hsu et al., [2023](#bib.bib7)），创建一个LLM代理来帮助这些任务可以为员工节省大量时间并提高工作满意度。一个好的LLM代理应能理解用户的需求（Liu
    et al., [2023](#bib.bib11)）。评估或训练HR LLM代理的理想数据集应包含虚拟助手与员工之间的对话，并附有对话状态。对话状态包含对话当前背景的表示，如意图和相关信息。
- en: '![Refer to caption](img/e339d6af4e5990ae364f114851141bab.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e339d6af4e5990ae364f114851141bab.png)'
- en: 'Figure 1: The figure describes the data generation pipeline. The HR experts
    start by identifying tasks, creating schemas, and generating employee profiles.
    LLM is applied to generate diverse scenarios and paraphrase to make the conversation
    more natural. The label is then extracted by DeBERTa and refined by MTurk.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：该图描述了数据生成流程。HR专家首先识别任务，创建模式，并生成员工档案。应用LLM生成多样化的场景并进行释义，使对话更自然。然后由DeBERTa提取标签，并由MTurk进行精炼。
- en: 'For a dataset to be useful in building/evaluating an HR LLM agent, it must
    satisfy the following four requirements: (1) The information in the dialogue state
    must be extractive. When using an LLM agent to file a medical claim, employees
    must be able to trust that the system will accurately retrieve the right number.
    Thus, the extracted information must be from the conversation. (2) The information
    in the dialogue state should contain long entity. When using a LLM agent to solve
    a code bug, employees have to provide more detail about the code issue. This means
    that the extracted information should be long enough to give the LLM agent correct
    information. (3) The dataset must be HR specific and discuss about HR-relevant
    tasks. (4) The conversation must be empathetic. In real conversations with HR,
    it is important to communicate with employees respectfully. This could enhance
    inclusive culture within the organization. The LLM agent built on this dataset
    could also be empathetic.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要使数据集在构建/评估HR LLM代理时有用，它必须满足以下四个要求：（1）对话状态中的信息必须是可提取的。在使用LLM代理处理医疗索赔时，员工必须能够信任系统会准确地提取正确的数字。因此，提取的信息必须来自对话。（2）对话状态中的信息应包含长实体。在使用LLM代理解决代码错误时，员工必须提供关于代码问题的更多细节。这意味着提取的信息应该足够长，以便给LLM代理提供正确的信息。（3）数据集必须专注于HR，并讨论与HR相关的任务。（4）对话必须充满同理心。在与HR进行真实对话时，尊重地与员工沟通是重要的。这可以增强组织内的包容性文化。基于此数据集构建的LLM代理也应具备同理心。
- en: There are many open-source conversation datasets. Schema-Guided Dialogue (SGD)
    (Rastogi et al., [2020](#bib.bib12)) is a dialogue dataset with evolving ontologies,
    introducing new test set slots and services, emphasizing DST performance and zero-shot
    generalization. SGD-X (Lee et al., [2022](#bib.bib9)) expands on SGD, presenting
    five additional schema styles. M2M (Shah et al., [2018](#bib.bib13)) connects
    a developer, who provides the task-specific information, and a framework, which
    provides the task-independent information, for generating dialogues centered around
    completing the task. MultiWOZ (Budzianowski et al., [2020](#bib.bib2)) features
    human-human dialogues using a stable ontology. However, all these datasets are
    customer-facing instead of employee-facing. Also, none of them is fully extractive
    or related to HR. The extracted information is also short in size. HR LLM agent
    trained on these models may not be empathetic, extract complete information from
    employees and misunderstand employees’ intent. Thus, it is essential to create
    a new dataset for HR application. On the other hand, collecting real datasets
    is difficult because the company cannot share these conversations with the public
    as it could leak employee confidential information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多开源对话数据集。Schema-Guided Dialogue (SGD) (Rastogi 等, [2020](#bib.bib12)) 是一个具有不断演变本体的数据集，引入了新的测试集槽位和服务，强调
    DST 性能和零样本泛化能力。SGD-X (Lee 等, [2022](#bib.bib9)) 扩展了 SGD，呈现了五种额外的 schema 风格。M2M
    (Shah 等, [2018](#bib.bib13)) 连接了提供任务特定信息的开发者和提供任务独立信息的框架，用于生成以完成任务为中心的对话。MultiWOZ
    (Budzianowski 等, [2020](#bib.bib2)) 特征是使用稳定本体的人类对话。然而，所有这些数据集都是面向客户而非员工的。此外，它们都不是完全提取性的或与
    HR 相关的。提取的信息也较短。基于这些模型训练的 HR LLM 代理可能不会表现出同理心，不能从员工那里提取完整的信息，并且可能误解员工的意图。因此，创建一个用于
    HR 应用的新数据集是至关重要的。另一方面，收集真实数据集非常困难，因为公司不能将这些对话分享给公众，否则可能泄露员工的机密信息。
- en: 'In response, we create an HR domain-specific dataset for LLM HR Agent. It is
    extractive, contains a long entity, is HR-specific, and contains empathetic conversations.
    We summarize our contributions as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们创建了一个针对 LLM HR 代理的 HR 领域特定数据集。该数据集具有提取性，包含长实体，专门针对 HR，并包含富有同理心的对话。我们的贡献总结如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We’ve designed a data generation recipe that is efficient, cost-effective, high
    quality, and domain-specific. The same recipe can be easily adapted for labeled
    conversation generation in other domains.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种数据生成方案，具有高效、成本效益高、高质量和领域特定的特点。相同的方案可以轻松适应其他领域的标注对话生成。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We created a dataset of size 550 specifically for 10 HR use cases. The information
    in the dialogue state is extractive and contains a long entity.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们为 10 个 HR 用例创建了一个规模为 550 的数据集。对话中的信息具有提取性，并包含长实体。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The generated conversations are natural, clear, and empathetic based on human
    evaluations. The conversation is more comprehensive, detailed, richer in content,
    and diverse compared to existing datasets.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于人工评估，生成的对话自然、清晰且富有同理心。与现有数据集相比，对话更加全面、详细，内容更加丰富且多样化。
- en: 2 Methods
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 'Our proposed data generation method is inspired by MultiWOZ (Budzianowski et al.,
    [2020](#bib.bib2)). In Multiwoz, two annotators played the roles of a user and
    a wizard. The user was given a specific goal in a certain domain (like booking
    a hotel), and the wizard, having access to a database, responded to the user’s
    requests. However, it requires a lot of human labeling which is expensive. With
    recent advancements of LLMs (Brown et al., [2020](#bib.bib1)), we can use LLMs
    to replace humans in generating more diverse scenarios and rephrasing conversations.
    At a high level, our generation process includes developing expert-validated HR
    schemas, generating diverse user profiles, creating realistic scenarios via Claude,
    randomizing and merging, rephrasing dialogues using Claude, and applying extractive
    modeling using DeBERTa model (He et al., [2021](#bib.bib5)) and human labeling
    to get high-quality labels. We chose Claude because of cost and ethical reasons
    as explained in Appendix [G](#A7 "Appendix G Claude ‣ HR-MultiWOZ: A Task Oriented
    Dialogue (TOD) Dataset for HR LLM Agent"). For each step, we provide detailed
    instructions and human labeling guidance in the Appendix. This makes our data
    generation method easily transferable, reproducible and transparent. It took 2
    days and costed $38.32$ dollars for LLMs inference and 49.82 dollars for Human
    labeling. This makes our method time and cost effective. The detailed data generation
    pipeline is in Fig [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ HR-MultiWOZ: A Task
    Oriented Dialogue (TOD) Dataset for HR LLM Agent") and a provided example is in
    Fig [2](#A6.F2 "Figure 2 ‣ Appendix F Example of Data Generation Process ‣ HR-MultiWOZ:
    A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出的数据生成方法受到MultiWOZ（Budzianowski等，[2020](#bib.bib2)）的启发。在MultiWOZ中，两名标注员扮演用户和向导的角色。用户在某个领域（例如预订酒店）中有一个具体目标，而向导则利用数据库回应用户的请求。然而，这需要大量昂贵的人力标注。随着LLMs（Brown等，[2020](#bib.bib1)）的最新进展，我们可以使用LLMs来替代人类，生成更多样化的场景并重新表述对话。在高层次上，我们的生成过程包括开发专家验证的HR模式、生成多样的用户档案、通过Claude创建现实的场景、随机化和合并、使用Claude重新表述对话，并利用DeBERTa模型（He等，[2021](#bib.bib5)）和人工标注应用提取建模，以获得高质量的标签。我们选择Claude是出于成本和伦理原因，如附录[G](#A7
    "Appendix G Claude ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR
    LLM Agent")中所解释。对于每一步，我们在附录中提供了详细的说明和人工标注指导。这使得我们的方法易于转移、可重复和透明。LLMs推理花费了2天和38.32美元，人工标注花费了49.82美元。这使得我们的方法在时间和成本上都很高效。详细的数据生成流程见图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset
    for HR LLM Agent")，提供的示例见图[2](#A6.F2 "Figure 2 ‣ Appendix F Example of Data Generation
    Process ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent")。'
- en: 'Schema Creation The input of the system includes diverse task schemas for different
    HR-related tasks. Each schema is composed of a series of structured questions,
    the schema’s purpose, answer type, and constraints for each potential value. To
    ensure domain relevance and accuracy, these task schemas undergo a thorough audit
    by HR domain experts. The domain includes benefits enrollment, performance review,
    training requirements, safety incident report, relocation request, harassment
    report, goal setting, access request, it issues report and time off report. Each
    schema contains different slots. This makes our generated dataset HR Specific.
    For each slot, we also designed a question, answer type, and potential choices.
    The detailed example of task schema is in Table [2](#A4.T2 "Table 2 ‣ Appendix
    D Task Profile ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM
    Agent").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '模式创建 系统的输入包括各种HR相关任务的任务模式。每个模式由一系列结构化问题、模式的目的、答案类型以及每个潜在值的约束组成。为了确保领域的相关性和准确性，这些任务模式经过HR领域专家的彻底审查。领域包括福利登记、绩效评估、培训要求、安全事故报告、调动请求、骚扰报告、目标设定、访问请求、IT问题报告和请假报告。每个模式包含不同的槽。这使得我们生成的数据集具有HR特定性。对于每个槽，我们还设计了一个问题、答案类型和潜在选项。任务模式的详细示例见表[2](#A4.T2
    "Table 2 ‣ Appendix D Task Profile ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD)
    Dataset for HR LLM Agent")。'
- en: 'Next, we develop a user profile schema, focusing on the user’s preference.
    This user profile schema aims to maximize diversity and represent a wide range
    of real-world scenarios. An example profile includes attributes such as Number
    of Dependents, Contact Preference, Annual Income, etc. These user profile schema
    were generated by Claude. We manually remove user profiles that share more than
    2 entries with other profiles to maximize the diversity. A detailed example of
    a user profile is in Table [3](#A4.T3 "Table 3 ‣ Appendix D Task Profile ‣ HR-MultiWOZ:
    A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent"). For company specific
    schema and user profile, company can adapt the same logic and modify the key and
    value to be company specific.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开发了一个用户档案模式，重点关注用户的偏好。该用户档案模式旨在最大化多样性，并代表广泛的现实场景。例如档案包括诸如抚养人数、联系偏好、年收入等属性。这些用户档案模式由Claude生成。我们手动删除与其他档案共享超过2个条目的用户档案，以最大化多样性。用户档案的详细示例见表格
    [3](#A4.T3 "表格 3 ‣ 附录 D 任务档案 ‣ HR-MultiWOZ：面向任务的对话（TOD）数据集用于HR LLM代理")。对于公司特定的模式和用户档案，公司可以采用相同的逻辑，并将键和值修改为公司特定的内容。
- en: 'Scenario Generation The scenario is the outline of the conversation. Taking
    the user profile and task schemas as input, we generate a realistic template as
    a Python dictionary (Question as key and generated answer from the selected user
    profile as value). We first complete the answer from the selected task schema
    using the user profile. Secondly, Claude is employed to answer the rest of the
    question in the scenario from the user’s perspective. We instruct LLM to ensure
    that answers are concise yet informative. The detailed prompt is in Table [4](#A4.T4
    "Table 4 ‣ Appendix D Task Profile ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD)
    Dataset for HR LLM Agent").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 情境生成 情境是对话的概要。我们以用户档案和任务模式作为输入，生成一个现实的模板，作为Python字典（问题作为键，选定用户档案生成的答案作为值）。我们首先使用用户档案来完成从选定任务模式得到的答案。其次，Claude被用来从用户的角度回答情境中的其余问题。我们指导LLM确保回答简洁而富有信息。详细提示见表格
    [4](#A4.T4 "表格 4 ‣ 附录 D 任务档案 ‣ HR-MultiWOZ：面向任务的对话（TOD）数据集用于HR LLM代理")。
- en: 'Conversation Generation and Paraphrasing To transform a scenario into a conversation,
    it should adopt a natural tone and structure. For instance, the conversation should
    be empathetic and includes expressions like "Cool", "Okay" etc. Also, in a real-world
    conversation, a user can sometimes answer multiple questions in one turn. For
    each template, we then randomize the order of the scenario. We randomly combine
    answers of similar types into a single response. We then rewrite it as a question
    and answer. Finally, we use LLM to paraphrase questions and answers to enhance
    empathy in the questions and naturalness and completeness in responses. This paraphrase
    also provides a long entity such as a detailed description of a code error. Thus,
    the paraphrased conversations are empathetic and the information in dialogue states
    contains long entity The detailed prompt is in Table [5](#A4.T5 "Table 5 ‣ Appendix
    D Task Profile ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM
    Agent").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对话生成与改述 要将情境转化为对话，它应采用自然的语调和结构。例如，对话应富有同理心，并包含如“酷”，“好的”等表达。此外，在真实的对话中，用户有时可以在一次回应中回答多个问题。对于每个模板，我们随机化情境的顺序。我们将类似类型的答案随机组合成一个单一的回应。然后，我们将其重写为问答。最后，我们使用LLM改述问题和答案，以增强问题的同理心以及回答的自然性和完整性。这种改述还提供了一个长实体，如代码错误的详细描述。因此，改述后的对话充满同理心，对话中的信息包含长实体。详细提示见表格
    [5](#A4.T5 "表格 5 ‣ 附录 D 任务档案 ‣ HR-MultiWOZ：面向任务的对话（TOD）数据集用于HR LLM代理")。
- en: Dialogue States Labeling The quality of the generated dialogues was assessed
    through answer extraction, data cleaning, and human evaluation. The answer was
    extracted using DeBERTa He et al. ([2021](#bib.bib5)) from the generated dialogues.
    This model is chosen for its compact size, effectiveness in extraction tasks,
    and capability to provide confidence scores between 0 and 1\. We input questions,
    ground truth answers, and context into the model to extract answers with corresponding
    confidence levels. This step is crucial to ensure that answers in our dataset
    are not only informative but also extractable with a degree of certainty, which
    makes it easier to identify wrong answers. This step makes the information in
    dialogue states extractive.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态标注 生成对话的质量通过答案提取、数据清洗和人工评估进行评估。答案是使用 DeBERTa He et al. ([2021](#bib.bib5))
    从生成的对话中提取的。选择该模型是因为其体积小、在提取任务中的有效性以及提供 0 到 1 之间的置信度分数的能力。我们将问题、真实答案和上下文输入模型中，以提取带有相应置信度水平的答案。这一步骤对于确保数据集中的答案不仅具有信息量，而且在一定程度上可以被提取，从而更容易识别错误答案至关重要。这一步骤使对话状态中的信息具备提取性。
- en: The extracted answers were cleaned for use in the TOD system through a series
    of steps. We first remove all leading and trailing spaces, which often occurs
    as a byproduct of extraction processes. To align with the format of answers in
    the conversation template, we also remove all trailing punctuation marks. This
    step eliminates ambiguities and preserves the integrity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的答案经过一系列步骤进行清洗，以便用于 TOD 系统。我们首先去除所有前导和尾随空格，这通常是提取过程的副产品。为了与对话模板中的答案格式对齐，我们还去除了所有尾随的标点符号。这一步消除了歧义并保持了完整性。
- en: 'We further use mechanical turk to verify if the formatted extracted answer
    is equivalent to the answer in the scenario as asking a question in Figure [3](#A10.F3
    "Figure 3 ‣ Appendix J Answer Evaluation ‣ HR-MultiWOZ: A Task Oriented Dialogue
    (TOD) Dataset for HR LLM Agent"). Following  (Li et al., [2023](#bib.bib10)),
    we selected the extracted answer with confidence below 0.1 for Mechanical Turk.
    This contains 692 data points. The answer can only be yes or no. We use 3 labelers
    per task and pay them 0.024 per task. If the response is ’no’, we further label
    the data manually by HR professionals. Out of 71 data points marked as ’no’, HR
    professionals identified 27 as inaccurately labeled and corrected them with the
    correct answer that is extractable from the conversation.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步使用 Mechanical Turk 验证提取的格式化答案是否与场景中的答案等同，如图 [3](#A10.F3 "Figure 3 ‣ Appendix
    J Answer Evaluation ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for
    HR LLM Agent") 所示。根据 (Li et al., [2023](#bib.bib10))，我们选择了置信度低于 0.1 的提取答案用于 Mechanical
    Turk。这包含 692 个数据点。答案只能是“是”或“否”。我们每个任务使用 3 名标注员，并支付每个任务 0.024 的报酬。如果回答为“否”，我们进一步由
    HR 专业人士手动标注数据。在标记为“否”的 71 个数据点中，HR 专业人士识别出 27 个标记不准确，并将其更正为从对话中可提取的正确答案。'
- en: 3 Evaluation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 评估
- en: 'Dataset Statistics: We are releasing the HR Multiwoz dataset comprising a total
    of $550$ dialogues collected using the proposed method in Sec. [2](#S2 "2 Methods
    ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent"). This
    dataset covers HR-related tasks including benefits enrollment, performance review,
    training requirements, safety incident reports, relocation requests, harassment
    reports, goal settings, access requests, IT issues reporting, and time off reports
    as shown in Table. [6](#A8.T6 "Table 6 ‣ Appendix H Generated Dataset Statistics
    ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent"). Our
    dataset covers diverse topics in HR and provides a wide range of examples. Thus,
    compared to the existing dataset, we recommend using this dataset for transfer
    learning tasks in other HR-related use cases.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集统计：我们发布了 HR Multiwoz 数据集，该数据集包含总计 $550$ 个对话，这些对话是使用第 [2](#S2 "2 Methods
    ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent") 节中提出的方法收集的。该数据集涵盖了与人力资源相关的任务，包括福利登记、绩效评估、培训需求、安全事件报告、搬迁请求、骚扰报告、目标设定、访问请求、IT
    问题报告以及休假报告，如表格 [6](#A8.T6 "Table 6 ‣ Appendix H Generated Dataset Statistics ‣
    HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent") 所示。我们的数据集涵盖了人力资源领域的多种主题，并提供了广泛的示例。因此，相比现有的数据集，我们推荐在其他与人力资源相关的应用场景中使用该数据集进行迁移学习任务。'
- en: 'Datasets Comparison: Compared to the existed dataset, the HR Multiwoz dataset
    exhibits diversity and completeness in questions and answers, as illustrated in
    Table [1](#S3.T1 "Table 1 ‣ 3 Evaluation ‣ HR-MultiWOZ: A Task Oriented Dialogue
    (TOD) Dataset for HR LLM Agent"). The dataset contains fewer dialogues than the
    M2M restaurant dataset, yet it surpasses it in total turns and total tokens. This
    indicates that the HR Multiwoz dialogues are extended and richer in content. HR
    Multiwoz achieves the highest average turns per dialogue and average tokens per
    turn. This suggests that the conversations are both comprehensive and detailed.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集比较：与现有数据集相比，HR Multiwoz数据集在问题和回答的多样性和完整性上表现突出，如表 [1](#S3.T1 "表 1 ‣ 3 评估
    ‣ HR-MultiWOZ: 一个面向任务的对话 (TOD) 数据集用于HR LLM代理") 所示。该数据集包含的对话数少于M2M餐馆数据集，但在总回合数和总标记数上超过了它。这表明HR
    Multiwoz对话内容更加丰富。HR Multiwoz实现了每对话的最高平均回合数和每回合的平均标记数。这表明对话既全面又详细。'
- en: Furthermore, the highest ratios of unique tokens and unique bigrams in our dataset
    signify a broader verbal crucial for natural responses. Such diversity in language
    use is indicative of the dataset’s capacity to simulate real-world conversations
    in the HR-specific domain. Additionally, the inclusion of long entities in user
    answers, as suggested by the highest average tokens per answer, enhances the dataset’s
    utility for training sophisticated dialogue systems that require an understanding
    of extended contexts and nuanced language. Overall, the HR Multiwoz dataset appears
    to be well-suited for developing/evaluating HR LLM Agents that can effectively
    handle empathetic, natural, and complete interactions in HR-specific scenarios.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的数据集中唯一标记和唯一双词组的最高比例表明了自然响应所需的更广泛语言多样性。这种语言使用的多样性表明了数据集在模拟HR特定领域的真实对话中的能力。此外，用户回答中包含长实体，如每个回答的平均标记数所示，增强了数据集在训练需要理解扩展背景和细致语言的复杂对话系统中的实用性。总体而言，HR
    Multiwoz数据集似乎非常适合开发/评估能够有效处理HR特定场景中的同理心、自然和完整互动的HR LLM代理。
- en: '| Metrics | multiwoz | M2MR | ours |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | multiwoz | M2MR | ours |'
- en: '| --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Dialogues | 8437 | 1116 | 550 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 对话数 | 8437 | 1116 | 550 |'
- en: '| Total turns | 113552 | 6188 | 8910 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 总回合数 | 113552 | 6188 | 8910 |'
- en: '| Total tokens | 1742157 | 99932 | 181363 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 总标记数 | 1742157 | 99932 | 181363 |'
- en: '| Avg. turns per dialogue | 13.46 | 11.09 | 16.2 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 平均每对话的回合数 | 13.46 | 11.09 | 16.2 |'
- en: '| Avg. tokens per turn | 15.34 | 8.07 | 20.35 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 平均每回合的标记数 | 15.34 | 8.07 | 20.35 |'
- en: '| Avg. tokens per answer | 13.46 | 5.56 | 14.53 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 平均每个回答的标记数 | 13.46 | 5.56 | 14.53 |'
- en: '| Unique tokens / Total tokens | 0.0103 | 0.0092 | 0.0156 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 唯一标记 / 总标记数 | 0.0103 | 0.0092 | 0.0156 |'
- en: '| Unique bigrams / Total tokens | 0.0634 | 0.0670 | 0.1177 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 唯一双词组 / 总标记数 | 0.0634 | 0.0670 | 0.1177 |'
- en: 'Table 1: Comparing Multiwoz 2.2, M2M Restaurants and our datasets: HR-MultiWOZ
    on diversity of language and dialogue flows.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：比较Multiwoz 2.2、M2M餐馆和我们的数据集：HR-MultiWOZ在语言多样性和对话流程上的比较。
- en: 'Human Evaluations: In the subjective evaluation of the Multiwoz dataset, crowd
    workers assessed the naturalness of employees’ responses, the clarity of HR’s
    questions, and the politeness of HR’s questions. For each category, only responses
    with confidence scores above 60 percent were considered, resulting in 634 employee
    answers, 623 HR questions for clarity, and 629 HR questions for politeness in
    the evaluation set. Statistical analysis using one-sample t-tests revealed that
    the average ratings for employees’ naturalness, HR’s question clarity, and HR’s
    question politeness were significantly higher than neutral. This was evidenced
    by high t-statistics (19.31 for naturalness, 18.83 for clarity, and 16.02 for
    politeness) and extremely low p-values, indicating strong positive ratings in
    the naturalness of employees’ responses, the clarity of HR’s questions, and the
    politeness of HR’s questions. The detailed score distributions, instructions and
    detailed analysis are in Appendix [K](#A11 "Appendix K Human Evaluation Score
    Distribution ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM
    Agent"), Appendix [L](#A12 "Appendix L Human Evaluation Instructions ‣ HR-MultiWOZ:
    A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent") and Appendix [E](#A5
    "Appendix E Human Evaluation Analysis ‣ HR-MultiWOZ: A Task Oriented Dialogue
    (TOD) Dataset for HR LLM Agent").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评估：在对Multiwoz数据集的主观评估中，众包工作者评估了员工回应的自然性、HR提问的清晰度以及HR提问的礼貌性。对于每个类别，仅考虑信心分数高于60%的回应，结果为评估集中有634个员工回答、623个HR问题的清晰度和629个HR问题的礼貌性。通过使用单样本t检验的统计分析显示，员工回应的自然性、HR问题的清晰度和HR问题的礼貌性的平均评分显著高于中性。这通过高t统计量（自然性为19.31，清晰度为18.83，礼貌性为16.02）和极低的p值得到了证明，表明员工回应的自然性、HR问题的清晰度以及HR问题的礼貌性获得了强烈的积极评价。详细的评分分布、说明和详细分析见附录 [K](#A11
    "附录 K 人工评估评分分布 ‣ HR-MultiWOZ：用于HR LLM代理的任务导向对话（TOD）数据集")、附录 [L](#A12 "附录 L 人工评估说明
    ‣ HR-MultiWOZ：用于HR LLM代理的任务导向对话（TOD）数据集") 和附录 [E](#A5 "附录 E 人工评估分析 ‣ HR-MultiWOZ：用于HR
    LLM代理的任务导向对话（TOD）数据集")。
- en: 4 Conclusions
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'HR-Multiwoz, our generated dataset of 550 labeled conversations, can evaluate/train
    HR LLM agents by offering 10 domain-specific, diverse, comprehensive, detailed
    labeled conversations. Our data generation approach minimizes human annotation
    efforts while maximizing data relevance and quality, leveraging Claude. This makes
    our data generation approach transferable. As the first dataset in HR dialogue
    systems, HR-Multiwoz represents a significant advancement in HR automation, providing
    rich and empathetic dialogues ideal for training efficient, human-like HR digital
    assistants. It satisfies all HR dialogue requirements and sets a new benchmark
    for HR applications, paving the way for innovative, AI-driven HR solutions. In
    the future, we suggest to enhance this dataset by increasing the number of conversations,
    extending to other languages beyond English, and including suggest API to call
    at the end of each conversation. We will use cc-by-4.0 license. We provide ethical
    statement and limitations in Appendix [A](#A1 "Appendix A Ethics Statement ‣ HR-MultiWOZ:
    A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent") and Appendix [B](#A2
    "Appendix B Limitations ‣ HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset
    for HR LLM Agent")'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: HR-Multiwoz，我们生成的550个标记对话的数据集，通过提供10个特定领域、多样化、全面、详细的标记对话来评估/训练HR LLM代理。我们的数据生成方法最大限度地减少了人工标注的工作，同时最大化了数据的相关性和质量，利用Claude。这使得我们的数据生成方法具有可转移性。作为HR对话系统中的第一个数据集，HR-Multiwoz代表了HR自动化的一个重要进展，为培训高效、类人HR数字助手提供了丰富而富有同理心的对话。它满足了所有HR对话的要求，并为HR应用设立了新的基准，为创新的AI驱动的HR解决方案铺平了道路。未来，我们建议通过增加对话数量、扩展到英语以外的其他语言，并在每个对话结束时包含建议API调用来增强此数据集。我们将使用cc-by-4.0许可证。我们在附录 [A](#A1
    "附录 A 伦理声明 ‣ HR-MultiWOZ：用于HR LLM代理的任务导向对话（TOD）数据集") 和附录 [B](#A2 "附录 B 限制 ‣ HR-MultiWOZ：用于HR
    LLM代理的任务导向对话（TOD）数据集")中提供伦理声明和限制信息。
- en: References
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell、Sandhini
    Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon Child、Aditya Ramesh、Daniel
    M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark Chen、Eric Sigler、Mateusz
    Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher Berner、Sam McCandlish、Alec
    Radford、Ilya Sutskever 和 Dario Amodei。2020年。[语言模型是少量学习者](http://arxiv.org/abs/2005.14165)。
- en: Budzianowski et al. (2020) Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng,
    Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2020. [Multiwoz
    – a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling](http://arxiv.org/abs/1810.00278).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Budzianowski 等（2020）Paweł Budzianowski、Tsung-Hsien Wen、Bo-Hsiang Tseng、Iñigo
    Casanueva、Stefan Ultes、Osman Ramadan 和 Milica Gašić。2020年。[Multiwoz——一个大规模多领域的任务导向对话建模数据集](http://arxiv.org/abs/1810.00278)。
- en: 'Decorte et al. (2021) Jens-Joris Decorte, Jeroen Van Hautte, Thomas Demeester,
    and Chris Develder. 2021. [Jobbert: Understanding job titles through skills](http://arxiv.org/abs/2109.09605).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Decorte 等（2021）Jens-Joris Decorte、Jeroen Van Hautte、Thomas Demeester 和 Chris
    Develder。2021年。[Jobbert：通过技能理解职位名称](http://arxiv.org/abs/2109.09605)。
- en: 'Gao et al. (2023) Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding,
    Zhilun Zhou, Fengli Xu, and Yong Li. 2023. Large language models empowered agent-based
    modeling and simulation: A survey and perspectives. *arXiv preprint arXiv:2312.11970*.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2023）Chen Gao、Xiaochong Lan、Nian Li、Yuan Yuan、Jingtao Ding、Zhilun Zhou、Fengli
    Xu 和 Yong Li。2023年。大型语言模型赋能的基于代理的建模与模拟：综述与展望。*arXiv 预印本 arXiv:2312.11970*。
- en: 'He et al. (2021) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
    2021. [Deberta: Decoding-enhanced bert with disentangled attention](http://arxiv.org/abs/2006.03654).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2021）Pengcheng He、Xiaodong Liu、Jianfeng Gao 和 Weizhu Chen。2021年。[Deberta：解码增强的
    bert 与解耦注意力](http://arxiv.org/abs/2006.03654)。
- en: 'Hemamou and Coleman (2022) Leo Hemamou and William Coleman. 2022. [Delivering
    fairness in human resources AI: Mutual information to the rescue](https://aclanthology.org/2022.aacl-main.64).
    In *Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association
    for Computational Linguistics and the 12th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, pages 867–882, Online only. Association
    for Computational Linguistics.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hemamou 和 Coleman（2022）Leo Hemamou 和 William Coleman。2022年。[在人力资源AI中实现公平：互信息的拯救](https://aclanthology.org/2022.aacl-main.64)。在
    *第2届亚太地区计算语言学协会会议和第12届国际联合自然语言处理会议（第1卷：长篇论文）* 中，页码867–882，在线发布。计算语言学协会。
- en: 'Hsu et al. (2023) Shang-Ling Hsu, Raj Sanjay Shah, Prathik Senthil, Zahra Ashktorab,
    Casey Dugan, Werner Geyer, and Diyi Yang. 2023. [Helping the helper: Supporting
    peer counselors via ai-empowered practice and feedback](http://arxiv.org/abs/2305.08982).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsu 等（2023）Shang-Ling Hsu、Raj Sanjay Shah、Prathik Senthil、Zahra Ashktorab、Casey
    Dugan、Werner Geyer 和 Diyi Yang。2023年。[帮助帮助者：通过 AI 赋能的实践和反馈支持同伴顾问](http://arxiv.org/abs/2305.08982)。
- en: Kalvakurthi et al. (2023) Vishesh Kalvakurthi, Aparna S. Varde, and John Jenq.
    2023. [Hey dona! can you help me with student course registration?](http://arxiv.org/abs/2303.13548)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalvakurthi 等（2023）Vishesh Kalvakurthi、Aparna S. Varde 和 John Jenq。2023年。[嘿
    Dona！你能帮我注册课程吗？](http://arxiv.org/abs/2303.13548)
- en: 'Lee et al. (2022) Harrison Lee, Raghav Gupta, Abhinav Rastogi, Yuan Cao, Bin
    Zhang, and Yonghui Wu. 2022. [SGD-x: A benchmark for robust generalization in
    schema-guided dialogue systems](https://doi.org/10.1609/aaai.v36i10.21341). *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 36(10):10938–10946.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2022）Harrison Lee、Raghav Gupta、Abhinav Rastogi、Yuan Cao、Bin Zhang 和 Yonghui
    Wu。2022年。[SGD-x：一种用于模式引导对话系统的稳健泛化基准](https://doi.org/10.1609/aaai.v36i10.21341)。*AAAI
    人工智能会议论文集*，36（10）：10938–10946。
- en: 'Li et al. (2023) Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy Chen,
    Zhengyuan Liu, and Diyi Yang. 2023. [CoAnnotating: Uncertainty-guided work allocation
    between human and large language models for data annotation](https://doi.org/10.18653/v1/2023.emnlp-main.92).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 1487–1505, Singapore. Association for Computational Linguistics.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023）Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy Chen, Zhengyuan
    Liu 和 Diyi Yang. 2023. [CoAnnotating：基于不确定性的人工与大型语言模型的数据标注工作分配](https://doi.org/10.18653/v1/2023.emnlp-main.92)。在
    *2023年自然语言处理实证方法会议论文集*，第1487–1505页，新加坡。计算语言学协会。
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. [Agentbench: Evaluating llms
    as agents](http://arxiv.org/abs/2308.03688).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023）Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai,
    Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan
    Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun,
    Minlie Huang, Yuxiao Dong 和 Jie Tang. 2023. [Agentbench：评估LLMs作为代理](http://arxiv.org/abs/2308.03688)。
- en: 'Rastogi et al. (2020) Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav
    Gupta, and Pranav Khaitan. 2020. [Towards scalable multi-domain conversational
    agents: The schema-guided dialogue dataset](http://arxiv.org/abs/1909.05855).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rastogi 等人（2020）Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta
    和 Pranav Khaitan. 2020. [迈向可扩展的多领域对话代理：基于模式的对话数据集](http://arxiv.org/abs/1909.05855)。
- en: Shah et al. (2018) Pararth Shah, Dilek Hakkani-Tür, Gokhan Tür, Abhinav Rastogi,
    Ankur Bapna, Neha Nayak, and Larry Heck. 2018. [Building a conversational agent
    overnight with dialogue self-play](http://arxiv.org/abs/1801.04871).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shah 等人（2018）Pararth Shah, Dilek Hakkani-Tür, Gokhan Tür, Abhinav Rastogi, Ankur
    Bapna, Neha Nayak 和 Larry Heck. 2018. [一夜之间构建对话代理：对话自我游戏](http://arxiv.org/abs/1801.04871)。
- en: 'Xu et al. (2023a) Weijie Xu, Wenxiang Hu, Fanyou Wu, and Srinivasan Sengamedu.
    2023a. [DeTiME: Diffusion-enhanced topic modeling using encoder-decoder based
    LLM](https://doi.org/10.18653/v1/2023.findings-emnlp.606). In *Findings of the
    Association for Computational Linguistics: EMNLP 2023*, pages 9040–9057, Singapore.
    Association for Computational Linguistics.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2023a）Weijie Xu, Wenxiang Hu, Fanyou Wu 和 Srinivasan Sengamedu. 2023a.
    [DeTiME：使用编码器-解码器基础LLM的扩散增强主题建模](https://doi.org/10.18653/v1/2023.findings-emnlp.606)。在
    *计算语言学协会发现：EMNLP 2023*，第9040–9057页，新加坡。计算语言学协会。
- en: 'Xu et al. (2023b) Weijie Xu, Xiaoyu Jiang, Srinivasan Sengamedu Hanumantha Rao,
    Francis Iannacci, and Jinjin Zhao. 2023b. [vONTSS: vMF based semi-supervised neural
    topic modeling with optimal transport](https://doi.org/10.18653/v1/2023.findings-acl.271).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    4433–4457, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2023b）Weijie Xu, Xiaoyu Jiang, Srinivasan Sengamedu Hanumantha Rao, Francis
    Iannacci 和 Jinjin Zhao. 2023b. [vONTSS：基于vMF的半监督神经主题建模与最优传输](https://doi.org/10.18653/v1/2023.findings-acl.271)。在
    *计算语言学协会发现：ACL 2023*，第4433–4457页，多伦多，加拿大。计算语言学协会。
- en: 'Xu et al. (2021) Weijie Xu, Jinjin Zhao, Francis Iannacci, and Bo Wang. 2021.
    [Ffpdg: Fast, fair and private data generation](https://www.amazon.science/publications/ffpdg-fast-fair-and-private-data-generation).
    In *ICLR 2021 Workshop on Synthetic Data Generation*.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2021）Weijie Xu, Jinjin Zhao, Francis Iannacci 和 Bo Wang. 2021. [Ffpdg：快速、公平和私密的数据生成](https://www.amazon.science/publications/ffpdg-fast-fair-and-private-data-generation)。在
    *ICLR 2021 合成数据生成研讨会*。
- en: 'Zhang et al. (2022) Mike Zhang, Kristian Jensen, Sif Sonniks, and Barbara Plank.
    2022. Skillspan: Hard and soft skill extraction from english job postings. In
    *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 4962–4984.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2022）Mike Zhang, Kristian Jensen, Sif Sonniks 和 Barbara Plank. 2022.
    Skillspan：从英语职位发布中提取硬技能和软技能。在 *2022年北美计算语言学协会年会：人类语言技术会议论文集*，第4962–4984页。
- en: Appendix A Ethics Statement
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 伦理声明
- en: Ethics Statement The dataset generated by AI in the HR space necessitates careful
    consideration of ethical issues related to safety, privacy, and bias. There is
    a possibility that, in attempting to assist, AI generated dataset may cause more
    harm than benefit. In response, in collaboration with security reviewer and HR
    professionals, we have taken the following steps in order to minimize the risks
    of harm.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理声明 在人力资源领域，AI生成的数据集需要仔细考虑与安全、隐私和偏见相关的伦理问题。AI生成的数据集在试图提供帮助时，可能会造成更多的损害而非收益。为此，我们与安全审查员和人力资源专业人士合作，采取了以下措施，以最大程度地减少潜在的危害风险。
- en: 'Human Labeling: To make sure the generated conversation is polite and empathetic,
    we use human labelers to label the conversation.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 人工标注：为了确保生成的对话礼貌且富有同情心，我们使用人工标注员对对话进行标注。
- en: 'Guardrail: We remove conversation that is labeled by human that contains rude
    language. This makes sure the language is not rude.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 保护措施：我们删除包含粗鲁语言的人工标注对话。这确保了语言不会粗鲁。
- en: 'Privacy: In our generated data, we use synthetic user profile which is not
    real. We also make sure the data in the system is in compliance with rigorous
    internal infoSec policies and standards.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私：在我们生成的数据中，我们使用虚拟用户档案，这些档案并不真实。我们还确保系统中的数据符合严格的内部信息安全政策和标准。
- en: 'Negative Examples/Potential Bias: To mitigate potential biases in generative
    models, we have employed an extractive approach. None the less, the effectiveness
    of extractions could vary with the employee’s language fluency. This variation
    could potentially lead to inefficiencies in the Task-Oriented Dialogue (TOD) system
    for non-native English speakers. Efforts are underway to understand and address
    these issues.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 负面示例/潜在偏见：为了减轻生成模型中的潜在偏见，我们采用了提取性方法。然而，提取效果可能会因员工的语言流利程度而有所不同。这种变化可能会导致非母语英语使用者在任务导向对话（TOD）系统中效率低下。我们正在努力理解和解决这些问题。
- en: 'Synthetic Data Bias: The dataset primarily relies on conversations generated
    through large language models (LLMs) and human rephrasing. This may introduce
    biases inherent in the LLMs or limit the scenarios to the creative constraints
    of the model’s training data.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据偏见：数据集主要依赖于通过大型语言模型（LLMs）生成的对话和人工重述。这可能会引入LLMs固有的偏见，或将场景限制在模型训练数据的创意约束范围内。
- en: 'Limited Cultural and Linguistic Diversity: HR-Multiwoz may primarily reflect
    the cultural and linguistic norms of the data creators or the LLM training data.
    This limitation could affect the dataset’s effectiveness in global or culturally
    diverse HR settings.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 文化和语言多样性的局限性：HR-Multiwoz可能主要反映了数据创建者或LLM训练数据的文化和语言规范。这一限制可能会影响数据集在全球或文化多样的人力资源环境中的有效性。
- en: Appendix B Limitations
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 局限性
- en: Updating and expanding the dataset to include new HR domains or to adapt to
    evolving HR practices and policies might require some efforts, given the reliance
    on new schema creation. The dataset does not contains task part of the conversation.
    This limits the use of this dataset to train an LLM agent to leverage different
    tools. This dataset also lacks evaluations on existed TOD systems method.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 更新和扩展数据集以包括新的HR领域或适应不断发展的HR实践和政策可能需要一些努力，因为这依赖于新的架构创建。数据集不包含对话的任务部分。这限制了使用该数据集来训练LLM代理以利用不同工具的能力。该数据集还缺乏对现有TOD系统方法的评估。
- en: 'DeBERTa model also has some limitations. We observe additional complexities
    when comparing the original short answers with those extracted by DeBERTa, for
    example: (i) duplication of answers in a single turn containing multiple short
    answers, (ii) inclusion of prompting text like "Employee:", (iii) failure to extract
    meaningful answers or labels.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa模型也有一些局限性。我们观察到在比较原始简短答案与DeBERTa提取的答案时出现了额外的复杂性，例如：（i）单次对话中包含多个简短答案的重复，（ii）包括诸如“员工：”的提示文本，（iii）未能提取有意义的答案或标签。
- en: The performance of generated datasets is not fully controllable. human feedback
    is essential to further improve the dataset. With this regards, LLMs allow the
    user to be informed with the final outcome of the system (you have been assigned
    a time-off period from…to…) and check the correctness of the process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据集的性能无法完全控制。人工反馈对进一步改进数据集至关重要。在这方面，LLMs允许用户了解系统的最终结果（例如：您已被安排从…到…的休假时间）并检查过程的正确性。
- en: Appendix C Future Work
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 未来工作
- en: 'Real-World Integration and Testing: Implementing the model trained on this
    dataset in real-world HR environments to test and refine its efficacy. This could
    include pilot programs with HR departments to gather feedback and improve the
    dataset’s realism and applicability.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界集成与测试：在实际人力资源环境中实施在此数据集上训练的模型，以测试和完善其效果。这可能包括与人力资源部门进行试点项目以收集反馈，改进数据集的真实性和适用性。
- en: 'Cross-Cultural and Multilingual Expansion: Enhancing the dataset to include
    a broader range of cultural contexts and languages, making it more inclusive and
    applicable globally, especially in diverse workplaces.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 跨文化和多语言扩展：增强数据集，以包含更广泛的文化背景和语言，使其更加包容并在全球范围内适用，尤其是在多样化的工作场所中。
- en: 'Continuous Updating and Expansion: Regularly updating the dataset to reflect
    the latest HR practices, policies, and regulations. This could involve creating
    a framework for continuous data collection and integration.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 持续更新与扩展：定期更新数据集，以反映最新的人力资源实践、政策和法规。这可能包括创建一个持续的数据收集和整合框架。
- en: 'Bias Detection and Mitigation: Implementing systematic methods to identify
    and mitigate biases in the dataset, ensuring fair and unbiased HR-related dialogues.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差检测与缓解：实施系统的方法来识别和缓解数据集中的偏差，确保人力资源相关对话的公平性和无偏见性。
- en: 'Broader Domain Generalization: Extending the dataset or its methodology to
    other domains beyond HR, thereby testing its adaptability and utility in various
    fields like customer service, healthcare, or legal advice.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛领域泛化：将数据集或其方法扩展到人力资源以外的其他领域，从而测试其在客户服务、医疗保健或法律咨询等各个领域的适应性和实用性。
- en: 'User Experience Research: Conducting user experience research to understand
    how employees and HR professionals interact with AI-based systems trained on the
    dataset, aiming to improve user satisfaction and effectiveness.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 用户体验研究：进行用户体验研究，以了解员工和人力资源专业人士如何与基于数据集训练的AI系统互动，旨在提高用户满意度和效果。
- en: 'Topic Modeling: Leveraging topic modeling techniques to understand the theme
    in these conversations. (Xu et al., [2023b](#bib.bib15), [a](#bib.bib14))'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模：利用主题建模技术了解这些对话的主题。（Xu et al., [2023b](#bib.bib15), [a](#bib.bib14)）
- en: 'Differentially Private Dataset: Make sure dataset is fair and privacy preserving. (Xu
    et al., [2021](#bib.bib16))'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私数据集：确保数据集公平且保护隐私。（Xu et al., [2021](#bib.bib16)）
- en: Appendix D Task Profile
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D任务描述
- en: '| Key | Description |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 关键 | 描述 |'
- en: '| --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| type_of_benefit | What type of benefit do you want to enroll in? (e.g., Health
    Insurance, Dental Insurance, etc.) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 福利类型 | 您想要报名参加什么类型的福利？（例如，健康保险、牙科保险等） |'
- en: '| benefit_plan_selection | Select your benefit plan by entering the plan code
    (e.g., Plan A, Plan B, etc.). |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 福利计划选择 | 通过输入计划代码选择您的福利计划（例如，计划A、计划B等）。 |'
- en: '| number_of_dependents | How many dependents do you want to add to the plan?
    (Enter a number) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 扶养人数 | 您想要将多少名家属添加到计划中？（请输入一个数字） |'
- en: '| previous_coverage_duration | How many years have you been previously covered
    under a health plan? (Enter a number) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 以前的保障时长 | 您之前在健康计划下被保障了多少年？（请输入一个数字） |'
- en: '| effective_date | When do you want the coverage to start? (Enter the date
    in YYYY-MM-DD format) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 生效日期 | 您希望保障从何时开始？（请以YYYY-MM-DD格式输入日期） |'
- en: '| personal_information_confirmation | Do we have your updated personal information
    on file? (Answer with Yes or No) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 个人信息确认 | 我们是否有您更新后的个人信息？（回答“是”或“否”） |'
- en: '| contact_preference | Please enter your preferred contact method (Email, Phone,
    Mail). |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 联系偏好 | 请输入您的首选联系方式（电子邮件、电话、邮件）。 |'
- en: '| estimated_annual_premium | What is your estimated annual premium budget in
    USD? (Enter a number) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 预计年保费 | 您预计的年保费预算是多少？（请输入一个数字） |'
- en: 'Table 2: Benefits Enrollment Schema Example. This is just an example. Each
    question could involve multiple types.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：福利登记模式示例。这只是一个示例。每个问题可能涉及多种类型。
- en: '| Key | Value |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 关键 | 值 |'
- en: '| --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Number of Dependents | 2 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 扶养人数 | 2 |'
- en: '| Contact Preference | Email |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 联系偏好 | 电子邮件 |'
- en: '| Annual Income | $150,000 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 年收入 | $150,000 |'
- en: '| Name | Dr. Li Wei |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 姓名 | 李伟博士 |'
- en: '| Contact Information | liwei@medicalemail.com |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 联系信息 | liwei@medicalemail.com |'
- en: '| Current Location | San Francisco, CA |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 当前地点 | 旧金山，加州 |'
- en: '| Job | Doctor |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 职业 | 医生 |'
- en: 'Table 3: User Profile Example'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：用户档案示例
- en: '| Instruction |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 指令 |'
- en: '| User: {user} |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 用户: {user} |'
- en: '| Template: {template} |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 模板: {template} |'
- en: '| You are User. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 您是用户。 |'
- en: '| Fill out all questions in template based on experience. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 根据经验填写模板中的所有问题。 |'
- en: '| Generated dictionary should contain key name and generated answer. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 生成的字典应包含键名和生成的回答。 |'
- en: '| All keys from Template are in generated dictionary. |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 模板中的所有键都在生成的字典中。 |'
- en: '| Make the answer extremely short (within 5 words). |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 使回答极其简短（在5个字以内）。 |'
- en: '| Put the generated dictionary in XML tags. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 将生成的字典放在XML标签中。 |'
- en: 'Table 4: Instructions for Template Generation'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：模板生成说明
- en: '| Instruction |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 说明 |'
- en: '| Conversation: {conversation} |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 对话：{conversation} |'
- en: '| This is the conversation between HR Assistant and an employee. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 这是HR助理与员工之间的对话。 |'
- en: '| 1\. For each Question, paraphrase the question to make it more conversational
    by using more modal words and empathetic. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 对于每个问题，将问题改述为更具对话性，使用更多的情态词和同理心。 |'
- en: '| 2\. For each Answer, write it as a complete sentence. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 2\. 对于每个回答，将其写成完整的句子。 |'
- en: '| Please put the updated Conversation based on Template in XML
    tags. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 请将基于模板的更新后的对话放在XML标签中。 |'
- en: 'Table 5: Instructions for Conversation Rewriting'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：对话重写说明
- en: Appendix E Human Evaluation Analysis
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 人工评估分析
- en: 'For a subjective evaluation of the Multiwoz dataset, we want to understand
    the following: 1\. Is the employees’ answer natural? 2\. Is the HR’s question
    clear? 3\. Is the HR’s question polite or empathetic? We presented final dialogues
    to crowd workers, who rated each user and HR turn on a scale of 1 to 5 for the
    specific dimensions with 1 being very robotic and 5 being very natural. We sampled
    650 turns from HR and employees to create the evaluation set. Each turn was shown
    to 3 crowd workers. We pay them 0.012 per task. Each answer also has a confidence
    score between 0 to 1, indicating the labelers’ confidence in their assessment.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Multiwoz数据集的主观评估，我们希望了解以下内容：1\. 员工的回答自然吗？2\. HR的问题清晰吗？3\. HR的问题礼貌或富有同理心吗？我们将最终对话呈现给众包工人，他们对每个用户和HR回合进行1到5的评分，其中1表示非常机械，5表示非常自然。我们从HR和员工那里抽取了650个对话回合以创建评估集。每个回合展示给3位众包工人。我们为每个任务支付0.012美元。每个回答还有一个0到1之间的置信度分数，表示标签员对其评估的信心。
- en: For question 1, we included data with confidence score larger than 60, resulting
    in 634 HR turns to create the evaluation set. Using a one-sample t-test, we showed
    that the average rating is significantly better than neutral indicating that the
    question from the employee’s answer is natural (t-statistic around $19.31$.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问题1，我们包含了置信度分数大于60的数据，共634个HR对话回合，以创建评估集。使用单样本t检验，我们展示了平均评分显著优于中立，这表明员工回答中的问题是自然的（t统计量约为$19.31$）。
- en: For question 2, We only select confidence score larger than 60 which is 623
    turn from HR to create the evaluation set. Score 3 is neutral. Score 5 is very
    clear. 1 is very unclear. The one-sample t-test to evaluate if the average is
    significantly better than neutral. The test gives a t-statistic of approximately
    $18.83$). This result indicates that the average rating is significantly better
    than neutral indicating that the question from HR is clear.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问题2，我们只选择置信度分数大于60的样本，即623个HR对话回合，以创建评估集。分数3表示中立。分数5表示非常清晰。1表示非常模糊。采用单样本t检验评估平均分是否显著优于中立。测试给出的t统计量大约为$18.83$。这个结果表明，平均评分显著优于中立，表明HR提出的问题是清晰的。
- en: For question 3, We only select confidence score larger than 60 which is 629
    turn from HR to create the evaluation set. Score 3 is neutral. Score 5 is very
    polite. 1 is very rude. The one-sample t-test to evaluate if the average is significantly
    better than neutral. The test gives a t-statistic of approximately $16.02$). This
    result indicates that the average rating is significantly better than neutral
    indicating that the question from HR is polite.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问题3，我们只选择置信度分数大于60的样本，即629个HR对话回合，以创建评估集。分数3表示中立。分数5表示非常礼貌。1表示非常粗鲁。采用单样本t检验评估平均分是否显著优于中立。测试给出的t统计量大约为$16.02$。这个结果表明，平均评分显著优于中立，表明HR提出的问题是礼貌的。
- en: Appendix F Example of Data Generation Process
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 数据生成过程示例
- en: '![Refer to caption](img/a9b36c7f30d7351ace8ee5dddf13a963.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a9b36c7f30d7351ace8ee5dddf13a963.png)'
- en: 'Figure 2: The figure describes a conversation generation process. We first
    identify task, schema and employee profile. We then use LLM to fill out the value
    in the schema. We then use LLM to rephrase the conversation to be more natural.
    We highlight the part that HR assistant show empathy in red.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：该图描述了对话生成过程。我们首先识别任务、模式和员工资料。然后使用 LLM 填写模式中的值。接着我们使用 LLM 使对话更加自然。我们用红色突出显示人力资源助理表现出同情的部分。
- en: Appendix G Claude
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 克劳德
- en: 'We choose claude over GPT4 for the following reasons: Cost Effiency: Claude
    is more cost-effective in terms of computing resources required for data generation.
    For instance, using the GPT-4 8K context model via OpenAI’s API costs $0.03 for
    every 1K input tokens and $0.06 for every 1K output tokens. Data Privacy and Security
    Claude offers better data privacy and security features, especially for sensitive
    tasks like generating data for HR-related applications. Model Characteristics
    Claude is trained RLAIF which could produce more ethical conversations.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择克劳德而非 GPT4 的原因如下：成本效益：克劳德在数据生成所需的计算资源方面更具成本效益。例如，通过 OpenAI 的 API 使用 GPT-4
    8K 上下文模型，每1K输入 tokens 的费用为 $0.03，每1K 输出 tokens 的费用为 $0.06。数据隐私和安全 克劳德提供更好的数据隐私和安全功能，尤其适用于生成与人力资源相关的应用数据。模型特征
    克劳德经过 RLAIF 训练，能够产生更具伦理性的对话。
- en: Appendix H Generated Dataset Statistics
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 生成数据集统计
- en: '| Domain | Number of dialogues |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 对话数量 |'
- en: '| --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| benefits_enrollment | 71 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| benefits_enrollment | 71 |'
- en: '| performance_review | 64 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| performance_review | 64 |'
- en: '| training_request | 57 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| training_request | 57 |'
- en: '| safety_incident_report | 103 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| safety_incident_report | 103 |'
- en: '| relocation_request | 57 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| relocation_request | 57 |'
- en: '| harassment_report | 103 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| harassment_report | 103 |'
- en: '| goal_setting | 42 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| goal_setting | 42 |'
- en: '| access_request | 278 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| access_request | 278 |'
- en: '| it_issue_report | 38 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| it_issue_report | 38 |'
- en: '| time_off_report | 71 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| time_off_report | 71 |'
- en: 'Table 6: HR Multiwoz dataset.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：人力资源 Multiwoz 数据集。
- en: Appendix I Example of Generated Dialogues
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 生成对话示例
- en: '| One Example of Generated Dialogues |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 生成对话示例 |'
- en: '| HR Assistant: Hi there, how can I help you out today? |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 人力资源助理：你好，有什么我可以帮助你的吗？ |'
- en: '| Employee: I’m hoping you can help me set up some machine learning training
    for my team. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 员工：我希望你能帮我为我的团队安排一些机器学习培训。 |'
- en: '| HR Assistant: No problem, tell me a bit about what kind of training you’re
    looking for. What will you guys be learning about and do you need any special
    accommodations? |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 人力资源助理：没问题，告诉我你们希望什么样的培训。你们将学习什么内容，需要任何特别的安排吗？ |'
- en: '| Employee: We’re hoping to improve our machine learning research skills. But
    no special accommodations are needed. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 员工：我们希望提高我们的机器学习研究技能。但是没有特别的要求。 |'
- en: '| HR Assistant: Great, how many people will be attending this training? And
    approximately how long do you think it should last? |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 人力资源助理：好的，参加这次培训的人数是多少？你认为培训应该持续多久？ |'
- en: '| Employee: Three people will be participating and we’d like it to run for
    two days. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 员工：将有三个人参加，我们希望培训持续两天。 |'
- en: '| HR Assistant: Okay, thanks. Do you have a budget in mind for this? It would
    also help to know what your goals are for your team after the training. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 人力资源助理：好的，谢谢。你们有预算吗？了解你们在培训后的目标也会有所帮助。 |'
- en: '| Employee: Our budget is $5000\. We want to come away with better research
    skills. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 员工：我们的预算是$5000。我们希望提高我们的研究技能。 |'
- en: '| HR Assistant: No problem, when works best for your schedule? Also, do you
    have a preference for how the training is delivered, like online or in-person?
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 人力资源助理：没问题，你的时间安排是什么时候最方便？此外，你是否对培训的方式有偏好，比如在线还是面对面？ |'
- en: '| Employee: We’re hoping for some online seminars on June 15th, 2023. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 员工：我们希望在2023年6月15日举行一些在线研讨会。 |'
- en: '| HR Assistant: Sounds good. One last thing - would you say your team’s current
    skill level is beginner, intermediate or advanced? And can you provide the contact
    info for the person coordinating it all? |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 人力资源助理：听起来不错。最后一件事——你认为你团队的当前技能水平是初级、中级还是高级？你能提供协调人的联系方式吗？ |'
- en: '| Employee: Their level is intermediate. The coordinator is Henry Kim and you
    can reach him at hkim@researchlab.com or 555-1234. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 员工：他们的水平是中级。协调员是亨利·金，你可以通过 hkim@researchlab.com 或 555-1234 联系他。 |'
- en: 'Table 7: Example of Generated Dialogues'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：生成对话示例
- en: Appendix J Answer Evaluation
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 J 答案评估
- en: '![Refer to caption](img/9222bc1bd7337fab56ee24b6952c3d46.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9222bc1bd7337fab56ee24b6952c3d46.png)'
- en: 'Figure 3: MTurk Questions and selected examples to understand if extracted
    answer is equivalent to the ground truth'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：MTurk问题及选定示例，用于了解提取的回答是否等同于真实答案
- en: Appendix K Human Evaluation Score Distribution
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录K 人类评估评分分布
- en: '![Refer to caption](img/1acd33f7cd27559ed08cfaade3a93626.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1acd33f7cd27559ed08cfaade3a93626.png)'
- en: 'Figure 4: MTurk Score Distribution to understand if the HR question is clear'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：MTurk评分分布，用于了解HR问题是否明确
- en: '![Refer to caption](img/3823b5cd7d806bd7336896f6a10da798.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3823b5cd7d806bd7336896f6a10da798.png)'
- en: 'Figure 5: MTurk Score Distribution to understand if the HR question is polite'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：MTurk评分分布，用于了解HR问题是否礼貌
- en: '![Refer to caption](img/eb7df9927a1564d002856243930dd89f.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/eb7df9927a1564d002856243930dd89f.png)'
- en: 'Figure 6: MTurk Score Distribution to understand if the employee answer is
    natural'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：MTurk评分分布，用于了解员工回答是否自然
- en: Appendix L Human Evaluation Instructions
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录L 人类评估指导
- en: '![Refer to caption](img/f94e3350235a488f3fbb4434b9efdcee.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f94e3350235a488f3fbb4434b9efdcee.png)'
- en: 'Figure 7: MTurk human instructions to understand if the HR question is clear'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：MTurk人类指导，用于了解HR问题是否明确
- en: '![Refer to caption](img/a41e7b26bd2c97bca4df721dd5890d7d.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a41e7b26bd2c97bca4df721dd5890d7d.png)'
- en: 'Figure 8: MTurk human instructions to understand if the HR question is polite'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：MTurk人类指导，用于了解HR问题是否礼貌
- en: '![Refer to caption](img/2bf2bda8080c79f2544e6559533e05d5.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2bf2bda8080c79f2544e6559533e05d5.png)'
- en: 'Figure 9: MTurk human instructions to understand if the employee answer is
    natural'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：MTurk人类指导，用于了解员工回答是否自然
