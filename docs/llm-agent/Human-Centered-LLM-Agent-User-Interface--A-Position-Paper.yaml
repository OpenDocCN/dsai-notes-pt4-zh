- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:46:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:46:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Human-Centered LLM-Agent User Interface: A Position Paper'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以人为本的LLM-代理用户界面：一份立场文件
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13050](https://ar5iv.labs.arxiv.org/html/2405.13050)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13050](https://ar5iv.labs.arxiv.org/html/2405.13050)
- en: Daniel Chin¹    Yuxuan Wang¹&Gus Xia^(2,1)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Daniel Chin¹    Yuxuan Wang¹&Gus Xia^(2,1)
- en: ¹Music X Lab, New York University Shanghai
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹音乐 X 实验室，纽约大学上海
- en: ²Music X Lab, Mohamed bin Zayed University of Artificial Intelligence
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²音乐 X 实验室，穆罕默德·本·扎耶德人工智能大学
- en: daniel.chin@nyu.edu, yw5343@nyu.edu, gus.xia@mbzuai.ac.ae
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: daniel.chin@nyu.edu, yw5343@nyu.edu, gus.xia@mbzuai.ac.ae
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Model (LLM) -in-the-loop applications have been shown to effectively
    interpret the human user’s commands, make plans, and operate external tools/systems
    accordingly. Still, the operation scope of the LLM agent is limited to passively
    following the user, requiring the user to frame his/her needs with regard to the
    underlying tools/systems. We note that the potential of an LLM-Agent User Interface
    (LAUI) is much greater. A user mostly ignorant to the underlying tools/systems
    should be able to work with a LAUI to discover an emergent workflow. Contrary
    to the conventional way of designing an explorable GUI to teach the user a predefined
    set of ways to use the system, in the ideal LAUI, the LLM agent is initialized
    to be proficient with the system, proactively studies the user and his/her needs,
    and proposes new interaction schemes to the user. To illustrate LAUI, we present
    Flute X GPT, a concrete example using an LLM agent, a prompt manager, and a flute-tutoring
    multi-modal software-hardware system to facilitate the complex, real-time user
    experience of learning to play the flute.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）-在环应用已经被证明能有效地解读人类用户的指令、制定计划，并相应地操作外部工具/系统。然而，LLM代理的操作范围仍然仅限于被动地跟随用户，需要用户根据底层工具/系统来框定自己的需求。我们注意到，LLM-代理用户界面（LAUI）的潜力要大得多。一个对底层工具/系统大多无知的用户应该能够使用LAUI来发现新兴的工作流程。与传统的设计可探索GUI以教授用户一组预定义使用方式的方法相反，在理想的LAUI中，LLM代理被初始化为精通系统，主动研究用户及其需求，并向用户提出新的交互方案。为了说明LAUI，我们展示了Flute
    X GPT，这是一个具体的示例，利用LLM代理、提示管理器和长笛教学的多模态软件硬件系统，来促进学习演奏长笛的复杂实时用户体验。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large Language Models (LLMs) can be used to connect an underlying system with
    a user via the natural language medium, forming an LLM-powered application, as
    shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Human-Centered LLM-Agent
    User Interface: A Position Paper"). The LLM is embedded in an LLM-in-the-loop
    state machine to acquire multi-modal input/output capabilities, emulate logical
    reasoning and planning, and use tools or operate a system Shen et al. ([2024b](#bib.bib17));
    Liu et al. ([2023](#bib.bib9)); Liang et al. ([2024](#bib.bib8)); Tao et al. ([2023](#bib.bib19)).
    Consequently, the user is able to indirectly but effectively use the underlying
    system via chatting with an LLM assistant.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以通过自然语言媒介将底层系统与用户连接起来，形成LLM驱动的应用程序，如图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ 以人为本的LLM-代理用户界面：一份立场文件")所示。LLM嵌入在LLM-在环状态机中，以获取多模态输入/输出能力，模拟逻辑推理和规划，并使用工具或操作系统Shen等（[2024b](#bib.bib17)）；Liu等（[2023](#bib.bib9)）；Liang等（[2024](#bib.bib8)）；Tao等（[2023](#bib.bib19)）。因此，用户可以通过与LLM助手聊天间接但有效地使用底层系统。
- en: However, the current applications hardly address the user-interaction potential
    stemming from the multi-round chatting setup. Although the user can refer to the
    chat history, the LLM assistant rarely challenges the user or even asks to clarify
    the user’s intention. Instead, the LLM closely follows the user’s command, missing
    the golden opportunity to improve the user’s usage scheme and his/her understanding
    of the system/tools. That inefficacy becomes jarringly apparent when one tries
    to design from scratch a new complex system with an LLM interface.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当前的应用几乎没有解决来自多轮聊天设置的用户互动潜力。虽然用户可以参考聊天记录，但LLM助手很少挑战用户甚至询问用户的意图。相反，LLM紧密跟随用户的指令，错失了提升用户使用方案和对系统/工具理解的黄金机会。当尝试从零开始设计一个新的复杂系统时，这种无效性变得尤为明显。
- en: We posit that the operation scope of an LLM-Agent User Interface (LAUI) is much
    wider than that. The interface should be more than an assistant or a butler, but
    instead a secretary, actively working with the user to discover emergent interaction
    schemes on the fly. LAUI should be proficient with the underlying system, study
    the user, study the user’s needs (instead of commands), reason on its own, and
    propose tailored interaction schemes to the user, including what modes of feedback
    are provided and what input is expected from the user. In the conventional way
    of interaction, including GUI and current LLM-powered applications Microsoft ([2024](#bib.bib10)),
    the designer has to imagine possible usage workflows given the system capabilities
    at design time, and the user is expected to learn the system (via tutorials, explorations,
    and practicing) in order to come up with a workflow for each task. In contrast,
    with LAUI, the user only needs to describe his/her needs and doesn’t need to deeply
    understand the application, and a workflow will naturally emerge as the LLM agent
    works with the user. We call for more research exploring the potential of LAUI.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为 LLM 代理用户界面（LAUI）的操作范围远不止于此。该接口应不仅仅是一个助手或管家，而应是一个秘书，积极与用户合作，实时发现新的交互方案。LAUI
    应熟练掌握底层系统，研究用户，研究用户的需求（而不是命令），自行推理，并向用户提出量身定制的交互方案，包括提供哪些反馈模式以及期望用户提供什么输入。在传统的交互方式中，包括
    GUI 和当前的 LLM 驱动应用（如 Microsoft ([2024](#bib.bib10))），设计师必须在设计时考虑系统能力，设想可能的使用工作流，而用户则需要通过教程、探索和实践来学习系统，以便为每个任务制定工作流。相比之下，使用
    LAUI，用户只需描述自己的需求，而不需要深入了解应用程序，工作流将随着 LLM 代理与用户的协作自然出现。我们呼吁更多的研究来探索 LAUI 的潜力。
- en: '![Refer to caption](img/f28c4643f3921d1d4799362a1a3a760c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f28c4643f3921d1d4799362a1a3a760c.png)'
- en: 'Figure 1: The LLM agent serves as the interface between the underlying system
    and the user. The LLM agent together with the system forms the application.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM 代理作为底层系统与用户之间的接口。LLM 代理与系统共同构成了该应用程序。
- en: As a concrete illustration of LAUI, we present Flute X GPT, an LLM-in-the-loop
    music-tutoring application consisting of an LLM agent, a prompt manager, a software
    system, and hardware. The application provides real-time haptic guidance via servo
    motors, real-time visual music-symbol feedback, real-time audio feedback, and
    natural language chat, all controlled by the LLM agent. This is the first time
    an LLM-powered interface is applied to a working system of such complexity and
    real-time user interactivity.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 LAUI 的一个具体示例，我们展示了 Flute X GPT，这是一个 LLM 循环音乐辅导应用程序，由 LLM 代理、提示管理器、软件系统和硬件组成。该应用程序通过伺服电机提供实时触觉指导、实时视觉音乐符号反馈、实时音频反馈和自然语言聊天，这些都由
    LLM 代理控制。这是首次将 LLM 驱动的接口应用于如此复杂和实时用户交互的工作系统中。
- en: 'We first describe Flute X GPT in Section [2](#S2 "2 Flute X GPT ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper"), illustrating what a specific LAUI
    can look like, and then go on to formulate the general LAUI in Section [3](#S3
    "3 LLM-Agent User Interface ‣ Human-Centered LLM-Agent User Interface: A Position
    Paper").'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在第 [2](#S2 "2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A
    Position Paper) 节中描述 Flute X GPT，展示具体的 LAUI 可能是什么样的，然后在第 [3](#S3 "3 LLM-Agent
    User Interface ‣ Human-Centered LLM-Agent User Interface: A Position Paper) 节中制定一般的
    LAUI。'
- en: 2 Flute X GPT
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 Flute X GPT
- en: We describe Flute X GPT¹¹1The source code is open to public at [https://github.com/DoubleBlind23408929/Flute-X-GPT](https://github.com/DoubleBlind23408929/Flute-X-GPT),
    a music-tutoring application using LAUI. The human user works with Flute X GPT
    in workshop episodes, practicing to play the flute and learning music. The application
    gives real-time multi-modal feedback, including haptic feedback that applies force
    to the user’s fingers, visual feedback displaying performance errors, audio feedback
    rendering the music, and natural-language speech given by a robot music teacher.
    The underlying software-hardware system has numerous different configurations,
    each leading to a different interaction workflow. Each setting (e.g., toggle certain
    feedback, conditions for triggering feedback) can be controlled independently,
    so the number of configurations grow exponentially with the number of settings.
    For the user, it is unrealistic to first master the complex system before using
    it. Even for the designer, the space of possible interaction schemes is impossible
    to enumerate during design time. The LLM agent steps in to bridge that gap. Via
    prompting, we instruct the LLM agent to be proficient with all the raw capabilities
    of the system. During use time, the LLM agent converses with the user to clarify
    what interaction workflow will benefit the user’s music learning goal the most.
    The LLM agent studies the user’s preferences and diagnoses the musical challenges
    the user is facing. When configuring the underlying system, the LLM agent uses
    common sense to reason about the implied consequences of the system configurations.
    Certain mixtures of settings have never been previously considered by any human
    designer, but can still emerge during use time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了Flute X GPT¹¹1源代码公开在 [https://github.com/DoubleBlind23408929/Flute-X-GPT](https://github.com/DoubleBlind23408929/Flute-X-GPT)
    ，这是一个使用LAUI的音乐辅导应用程序。人类用户在工作坊阶段使用Flute X GPT，练习演奏长笛并学习音乐。该应用程序提供实时多模态反馈，包括对用户手指施加力量的触觉反馈、显示表现错误的视觉反馈、呈现音乐的音频反馈以及由机器人音乐教师提供的自然语言语音。基础软件-硬件系统有许多不同的配置，每种配置都会导致不同的互动工作流程。每个设置（例如，切换某些反馈、触发反馈的条件）都可以独立控制，因此配置的数量随着设置的增加而呈指数增长。对于用户而言，首先掌握复杂的系统是不现实的。即使是设计师，也不可能在设计阶段列举所有可能的互动方案。LLM代理介入以弥补这一差距。通过提示，我们指示LLM代理熟悉系统的所有原始功能。在使用过程中，LLM代理与用户对话，以澄清什么样的互动工作流程将最有利于用户的音乐学习目标。LLM代理研究用户的偏好，并诊断用户面临的音乐挑战。在配置基础系统时，LLM代理使用常识推理系统配置的隐含后果。某些设置组合从未被任何人类设计师考虑过，但仍可能在使用过程中出现。
- en: '![Refer to caption](img/7fbb330dc38ff06710c08bdbd38c151d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7fbb330dc38ff06710c08bdbd38c151d.png)'
- en: (a) From the scripted trial.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 来自脚本化试验。
- en: '![Refer to caption](img/4bd31342d78c0dcee4c2766d93cf2893.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4bd31342d78c0dcee4c2766d93cf2893.png)'
- en: (b) From improvised trial 1.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 来自即兴试验1。
- en: 'Figure 2: Interaction excerpts from the video demos.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：视频演示中的互动摘录。
- en: 'Overall, Flute X GPT entails three novel contributions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Flute X GPT包含三个创新贡献：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An illustration of LAUI. The LLM agent not only follows the user’s commands,
    but also proactively clarifies the user’s needs, deduces better interaction workflows,
    and advises the user.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LAUI的示例。LLM代理不仅遵循用户的指令，还主动澄清用户的需求，推导出更好的互动工作流程，并向用户提供建议。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM agent controls real-time system. The LLM agent directs a real-time interaction
    that is music training. The LLM agent is aware of time passage and decides when
    to wait for further event notifications and when to interrupt the user.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM代理控制实时系统。LLM代理指导一个实时的音乐训练互动。LLM代理意识到时间的流逝，并决定何时等待进一步的事件通知以及何时打断用户。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM agent operates complex, stateful, multi-modal, user-interactive system.
    The LLM agent operates a highly complex software-hardware system by understanding
    how the user can benefit from the application and considering what combination
    of settings will lead to what interactive effects for the user.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM代理操作复杂的、有状态的、多模态的、用户交互系统。LLM代理通过理解用户如何从应用程序中受益，并考虑哪些设置组合会对用户产生什么互动效果，来操作一个高度复杂的软件-硬件系统。
- en: 2.1 User Experience
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 用户体验
- en: '| Function | Parameters | Description |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 功能 | 参数 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Wait |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 等待 |  |'
- en: '&#124; Do nothing and wait for further stimuli, e.g. student speaking/playing
    music. &#124;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 什么都不做，等待进一步的刺激，例如学生发言/演奏音乐。 &#124;'
- en: '|'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| StartSession |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 启动会话 |  |'
- en: '&#124; Start a Practice Session on Music X Machine. Do not call this function
    &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在 Music X Machine 上开始练习课程。不要调用此功能 &#124;'
- en: '&#124; unless you have already set all the modes. &#124;'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 除非你已经设置了所有模式。 &#124;'
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| InterruptSession |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| InterruptSession |  |'
- en: '&#124; Immediately end the Practice Session on Music X Machine. Call when the
    &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 立即结束 Music X Machine 上的练习课程。当时的 &#124;'
- en: '&#124; student is having trouble or has started speaking in the middle of a
    Session. &#124;'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学生遇到问题或在课程中途开始发言。 &#124;'
- en: '|'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SetHapticMode | mode |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| SetHapticMode | mode |'
- en: '&#124; Set the haptic mode of Music X Machine. &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设置 Music X Machine 的触觉模式。 &#124;'
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ToggleVisual | state |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ToggleVisual | 状态 |'
- en: '&#124; Set the visual KR feedback to be on or off. &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将视觉 KR 反馈设置为开或关。 &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PlayReference |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| PlayReference |  |'
- en: '&#124; Play the ground-truth audio of the current segment. &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 播放当前片段的实际音频。 &#124;'
- en: '|'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LoadSong | song_title |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| LoadSong | song_title |'
- en: '&#124; Load a song into Music X Machine, and automatically select the entire
    &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将一首歌加载到 Music X Machine 中，并自动选择整个 &#124;'
- en: '&#124; song as the current segment. It doesn’t start a Practice Session by
    itself. &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将歌曲作为当前片段。它不会自动开始练习课程。 &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SelectSegment | begin, end |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| SelectSegment | 开始，结束 |'
- en: '&#124; Select a temporal segment of the song. &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 选择歌曲的一个时间片段。 &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ModifyTempo | tempo_multiplier |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ModifyTempo | tempo_multiplier |'
- en: '&#124; Modify the tempo of the song. &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修改歌曲的节奏。 &#124;'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 1: Functions that the LLM agent can call to control Music X Machine.
    The description addresses the LLM agent.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: LLM 代理可以调用以控制 Music X Machine 的功能。描述针对 LLM 代理。'
- en: The intended user experience. The user has no prior knowledge about the flute
    tutoring system. The only assumption about the user is that the user wants to
    learn the flute. It is the LLM agent’s job to adapt to the user’s current flute
    playing capability, other musical skills, demographics, vocabulary, patience,
    style of learning, etc. During the music learning workshop, the LLM agent wears
    the face of a robot music teacher to chat with the user. For example, the robot
    teacher asks the user to put on a pair of haptic gloves, and explains that they
    provide force feedback at each finger. The workshop then alternates between practice
    sessions where the user plays a segment of a song under real-time guidance and
    verbal dialogues between the user and the robot teacher. The user gradually experiences
    more and more interaction schemes that trains musicality in different ways, and
    expresses their ideas about using the application and music learning in general.
    The LLM agent uses that opportunity to study the user and steers the workshop
    accordingly, aiming to maximize music education effect. The user learns to treat
    the robot teacher as a considerate and professional music tutoring agent capable
    of thinking multiple steps ahead, formulating plans with the user, and explaining
    the plans as well as music knowledge and education principles for the user.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的用户体验。用户对长笛辅导系统没有先前的了解。对用户的唯一假设是用户希望学习长笛。LLM 代理的工作是适应用户当前的长笛演奏能力、其他音乐技能、人口统计信息、词汇、耐心、学习风格等。在音乐学习工作坊中，LLM
    代理以机器人音乐教师的面貌与用户交谈。例如，机器人教师会要求用户戴上一副触觉手套，并解释这些手套在每个手指上提供力反馈。然后，工作坊在实际指导下练习用户演奏一段歌曲的练习课程和用户与机器人教师之间的口头对话之间交替进行。用户逐渐体验更多的互动方案，以不同的方式训练音乐感，并表达他们对使用应用程序和音乐学习的一般看法。LLM
    代理利用这个机会研究用户，并相应地调整工作坊，旨在最大化音乐教育效果。用户学会将机器人教师视为一个考虑周到、专业的音乐辅导代理，能够考虑多个步骤、与用户制定计划并解释这些计划以及音乐知识和教育原则。
- en: 'We present three video demos of real-human user tests, including one scripted
    trial and two improves trials.²²2Demo playlist: [https://youtube.com/playlist?list=PLOplnUxtqG3SKrR01tYTdrqdaClayFTxp](https://youtube.com/playlist?list=PLOplnUxtqG3SKrR01tYTdrqdaClayFTxp)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了三段真实用户测试的视频演示，包括一个脚本化试验和两个改进试验。²²2演示播放列表: [https://youtube.com/playlist?list=PLOplnUxtqG3SKrR01tYTdrqdaClayFTxp](https://youtube.com/playlist?list=PLOplnUxtqG3SKrR01tYTdrqdaClayFTxp)'
- en: 'Video demo: Scripted trial. In this demo, all actors follow a script generated
    by an offline but faithfully emulated interaction between a user and the LLM agent.
    To re-emphasize, the LLM agent’s lines in the script are not written by humans,
    but is outputted by the LLM agent. Scripting removes the latency of LLM autoregressive
    generation. To ensure the script is concise and demonstrates a wide range of behaviors,
    we intervene with the script generation process. Every turn, we select one response
    out of 4 to 16 candidates sampled by the LLM. Seldomly, we add a temporary user-role
    prompt to give a short hint to the LLM agent, or edit the generated response.
    All the above interventions are kept to the minimum to ensure the vast majority
    of the agent speech is the authentic output of the LLM. See Figure [2(a)](#S2.F2.sf1
    "In Figure 2 ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position
    Paper") for an excerpt from the video.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 视频演示：脚本化试验。在这个演示中，所有演员都遵循一个由离线但忠实模拟的用户与 LLM 代理之间的互动生成的脚本。重申一下，脚本中的 LLM 代理台词不是由人类编写的，而是由
    LLM 代理输出的。脚本化消除了 LLM 自回归生成的延迟。为了确保脚本简洁并展示广泛的行为，我们介入了脚本生成过程。每轮，我们从 LLM 采样的 4 到
    16 个候选回答中选择一个。偶尔，我们会添加一个临时用户角色提示给 LLM 代理，或编辑生成的回答。所有上述干预都保持在最低限度，以确保大多数代理语音是 LLM
    的真实输出。请参见图 [2(a)](#S2.F2.sf1 "在图 2 ‣ 2 Flute X GPT ‣ 以人为本的 LLM-代理用户界面：定位论文") 以获取视频的摘录。
- en: 'Video demo: Two improvised trials. Flute X GPT is set loose to freely interact
    with the user. The user is played by a developer pretending to be ignorant to
    the system. Figure [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2 Flute X GPT ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper") shows an excerpt where the developer
    is surprised by Flute X GPT noticing that he was not playing the rest notes according
    to the score, a behavior never considered during design time.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 视频演示：两个即兴试验。Flute X GPT 被设定为自由地与用户互动。用户由一个开发者扮演，该开发者假装对系统无知。图 [2(b)](#S2.F2.sf2
    "在图 2 ‣ 2 Flute X GPT ‣ 以人为本的 LLM-代理用户界面：定位论文") 显示了一个片段，开发者对 Flute X GPT 发现他没有按照乐谱演奏剩余音符的行为感到惊讶，这种行为在设计时从未考虑过。
- en: 2.2 System Capabilities
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 系统功能
- en: This subsection lists the capabilities of the music-tutoring system, Music X
    Machine, that underlies the LLM agent.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节列出了作为 LLM 代理基础的音乐辅导系统 Music X Machine 的功能。
- en: Haptic guidance. The hardware includes a pair of gloves with servo motors and
    movable finger rings. Through these gloves, the system moves the user’s fingers
    to help with performance motions. The haptic guidance can be configured via various
    settings (e.g., is the guidance sustained throughout each note or applied at each
    note onset? Apply guidance for each note? For incorrectly played notes? For unplayed
    notes?). Certain combinations of settings form configuration presets whose interaction
    scheme is deemed meaningful by the designer. For example, in the Force Mode preset,
    every note triggers a full-force guidance for each finger. In the Adaptive Mode
    preset, the user plays the song on his/her own, and the gloves correct the mistakes.
    The haptic configuration should be tuned to adapt to the user’s skill level and
    the song’s difficulty Zhang et al. ([2019](#bib.bib30)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 触觉引导。硬件包括一副配有伺服电机和可移动手指环的手套。通过这些手套，系统移动用户的手指以帮助演奏动作。触觉引导可以通过各种设置进行配置（例如，引导是持续在每个音符上还是在每个音符开始时应用？对每个音符应用引导？对演奏错误的音符应用引导？对未演奏的音符应用引导？）。某些设置组合形成配置预设，其交互方案被设计者认为是有意义的。例如，在强制模式预设中，每个音符触发对每个手指的全力引导。在自适应模式预设中，用户自己演奏歌曲，而手套纠正错误。触觉配置应根据用户的技能水平和歌曲的难度进行调整
    Zhang 等人 ([2019](#bib.bib30))。
- en: Visual feedback. A monitor displays the score of the current song. The user-played
    notes are displayed on top of the notes in the score, yielding the real-time visual
    Knowledge-of-Result (KR) feedback. It is a visual cue for the user to know where
    he/she is in terms of the pitch and helps the user internalize the score notations
    Chin et al. ([2020](#bib.bib4)). It can be toggled on or off.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉反馈。显示器显示当前歌曲的得分。用户演奏的音符会显示在乐谱音符的上方，提供实时的视觉结果知识（KR）反馈。这是一个视觉提示，帮助用户了解他们在音高上的位置，并帮助用户内化乐谱符号
    Chin 等人 ([2020](#bib.bib4))。它可以开关切换。
- en: 'Audio feedback. There are three streams of audio: synthesized user-played flute
    sounds, teacher-played reference performance audio, and metronomes. The three
    streams are mixed down and outputted.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 音频反馈。有三个音频流：合成的用户演奏的长笛声音、教师演奏的参考表演音频和节拍器。这三个音频流混合后输出。
- en: Sensor-augmented flute. The flute measures real-time information including finger
    positions and breath pressure.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器增强型长笛。长笛测量实时信息，包括手指位置和呼吸压力。
- en: '![Refer to caption](img/d9500c8a1c054f360a23ac6ef4736697.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d9500c8a1c054f360a23ac6ef4736697.png)'
- en: (a) Simplified.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 简化版。
- en: '![Refer to caption](img/e18c93edc2da9eab65ebb55a36e4ce62.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e18c93edc2da9eab65ebb55a36e4ce62.png)'
- en: (b) Full.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 完整版。
- en: 'Figure 3: Flute X GPT with LLM in the loop. Music X Machine is the underlying
    software-hardware system providing multi-modal interaction with the user. The
    robot chats with the user and plays the piano according to MIDI control. The rule-based
    manager plays the agent that chats with the LLM, relaying external events to the
    LLM and resolving responses from the LLM.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：带有LLM循环的长笛X GPT。Music X Machine是提供与用户进行多模态互动的基础软件-硬件系统。机器人与用户聊天，并根据MIDI控制演奏钢琴。基于规则的管理者充当与LLM聊天的代理，传递外部事件给LLM，并处理LLM的回应。
- en: 'Tempo mode. There are two options: either the system sets a steady tempo and
    the user follows the system, or the user plays on his/her own and the system follows
    the user’s tempo. The latter allows the user to think between the notes, potentially
    indefinitely, without triggering haptic feedback.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 节拍模式。有两种选择：系统设置一个稳定的节拍，用户跟随系统，或者用户自行演奏，系统跟随用户的节拍。后者允许用户在音符之间思考，可能是无限期的，而不会触发触觉反馈。
- en: Mistake classification. An algorithm judges the timing of each note into on_time,
    early, or late, and judges the pitch of each note into correct, octave_wrong,
    or unrelated. Classification results are visualized.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分类。一个算法将每个音符的时机判断为准时、提前或延迟，将每个音符的音高判断为正确、八度错误或无关。分类结果以可视化形式展示。
- en: Song database. The practicing music materials that the system uses is processed
    from the POP909 dataset Wang et al. ([2020](#bib.bib21)); Chin and Xia ([2022](#bib.bib3)).
    It contains pieces and sections of monophonic melody lines from pop songs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 歌曲数据库。系统使用的练习音乐材料处理自POP909数据集 Wang et al.（[2020](#bib.bib21)）；Chin和Xia（[2022](#bib.bib3)）。其中包含了流行歌曲中的单音旋律片段和部分。
- en: 2.3 High-Dimensional Configuration of Interaction
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 互动的高维配置
- en: The capabilities listed in the previous subsection entangle with one another,
    implying dynamic consequences in terms of learning experience. Their configurations
    multiply into a big Cartesian product, making the overall configuration of the
    entire system high-dimensional. Configuring the system effectively requires 1)
    proficiency with the system, 2) understanding the user’s needs, 3) pedagogical
    expertise, 4) music knowledge, and 5) using common-sense reasoning to “imagine”
    the multi-modal real-time interaction. We employ an LLM agent to solve all five.
    To optimize the interaction workflow and learning results for the user, the LLM
    agent can not only select a suitable preset, but also create new ones unforeseen
    by the designers, tailored to specific use-time scenarios.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前一小节列出的能力相互交织，暗示了学习体验方面的动态结果。它们的配置乘积变成一个大的笛卡尔积，使整个系统的配置变得高维。有效配置系统需要 1) 对系统的熟练掌握，2)
    理解用户需求，3) 教学专业知识，4) 音乐知识，以及 5) 使用常识推理“想象”多模态实时互动。我们使用LLM代理来解决这五个问题。为了优化用户的互动工作流程和学习结果，LLM代理不仅可以选择合适的预设，还可以创建设计者未曾预见的新预设，以适应特定的使用场景。
- en: 'To illustrate this high-dimensional interface that the system exposes to the
    LLM agent, Table [1](#S2.T1 "Table 1 ‣ 2.1 User Experience ‣ 2 Flute X GPT ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper") shows the functions that the LLM
    agent can call.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明系统向LLM代理展示的高维接口，表[1](#S2.T1 "Table 1 ‣ 2.1 用户体验 ‣ 2 长笛X GPT ‣ 以人为本的LLM代理用户界面：立场论文")展示了LLM代理可以调用的功能。
- en: 2.4 LLM in the Loop
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 循环中的大型语言模型（LLM）
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2.2 System Capabilities ‣ 2 Flute X GPT ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper") illustrates the inner workings of
    the LLM-in-the-loop application, Flute X GPT. The LLM we use is GPT-4 Achiam et
    al. ([2023](#bib.bib1)). The Music X Machine has been described in Subsection [2.1](#S2.SS1
    "2.1 User Experience ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper") and [2.2](#S2.SS2 "2.2 System Capabilities ‣ 2 Flute X GPT
    ‣ Human-Centered LLM-Agent User Interface: A Position Paper"). The System Principles
    are a passage of prompt given to the LLM at the top of the conversation that defines
    the role and interaction principles for the LLM agent (see Appendix [A](#A1 "Appendix
    A System Principles of Flute X GPT, Truncated ‣ Human-Centered LLM-Agent User
    Interface: A Position Paper")). Here are two representative excerpts.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S2.F3 "Figure 3 ‣ 2.2 System Capabilities ‣ 2 Flute X GPT ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper") 展示了LLM-in-the-loop应用程序Flute X GPT的内部工作原理。我们使用的LLM是GPT-4
    Achiam et al. ([2023](#bib.bib1))。Music X Machine在小节[2.1](#S2.SS1 "2.1 User Experience
    ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position Paper")和[2.2](#S2.SS2
    "2.2 System Capabilities ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper")中已有描述。系统原则是对LLM在对话开始时给出的提示，定义了LLM代理的角色和互动原则（见附录[A](#A1 "Appendix
    A System Principles of Flute X GPT, Truncated ‣ Human-Centered LLM-Agent User
    Interface: A Position Paper")）。这里有两个代表性的摘录。'
- en: You are Flute X GPT, a motivated, professional music teacher who wants the best
    for your students. I am Music X Machine, a powerful human-computer interface.
    Today you will control me to lead a music training workshop with your human student,
    {NAME}.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你是Flute X GPT，一位积极的、专业的音乐教师，致力于为学生提供最佳的教育。我是Music X Machine，一个强大的人机接口。今天你将控制我来主持一场与你的学生{NAME}的音乐培训工作坊。
- en: You interact with the real world through this conversation. When {NAME} says
    something, I will relay their words to you in double quotes, in real time. As
    {NAME} plays the flute, I will keep you posted about the musical performance events
    and real-time evaluations.
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你通过这个对话与现实世界互动。当{NAME}说话时，我会实时将他们的话转达给你，使用双引号。随着{NAME}演奏长笛，我会及时通知你音乐表演事件和实时评估。
- en: 'The Parser splits the LLM’s output into three types: thought (i.e., internal
    monologue), action, and speech. The parsing of actions happens within the OpenAI
    service because we use the function calling feature³³3[https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)
    of GPT-4\. Our parser only needs to separate thoughts from speeches. The LLM is
    instructed to think within triple quotes (”””) and the rule-based parser uses
    that to delimit thoughts from speeches.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器将LLM的输出分为三种类型：思维（即内部独白）、行动和讲话。行动的解析在OpenAI服务中进行，因为我们使用了GPT-4的功能调用功能³³3[https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)。我们的解析器只需要将思维与讲话分开。LLM被指示在三重引号（”””）内思考，规则基础的解析器使用这一点来区分思维和讲话。
- en: 'The Manager is a rule-based state machine in charge of encapsulating each agent
    (the LLM and the user) in a consistent interaction environment. The manager: Forwards
    the system principles to the LLM at the start; Forwards speech from the student
    to the LLM, while enclosing it as such: ‘{NAME} says: “{SPEECH}” ’; Forwards real-time
    performance evaluations to the LLM; Forwards LLM speeches from the parser to the
    text-to-speech module; Upon receiving a function call from the LLM, use API to
    control Music X Machine accordingly, unless the function is wait(); After receiving
    a speech or a function call from the LLM, immediately query the LLM for a subsequent
    response, until the LLM calls wait().'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 管理器是一个基于规则的状态机，负责将每个代理（LLM和用户）封装在一致的互动环境中。管理器：在开始时将系统原则转发给LLM；将学生的讲话转发给LLM，格式如下：‘{NAME}
    说：“{SPEECH}” ’；将实时性能评估转发给LLM；将LLM的讲话从解析器转发到文本到语音模块；在收到LLM的函数调用后，使用API相应地控制Music
    X Machine，除非函数是wait(); 在收到LLM的讲话或函数调用后，立即向LLM查询后续响应，直到LLM调用wait()。
- en: 'The text-to-speech (T2S) module includes Text to Speech PRO VidLab ([2023](#bib.bib20))
    on Rapid API and FastSpeech 2 Chien et al. ([2021](#bib.bib2)). The speech-to-text
    (S2T) module is Whisper Radford et al. ([2023](#bib.bib13)), prompted to ignore
    flute sounds. The Robot is TeoTronico Suzzi ([2023](#bib.bib18)) who can play
    the piano from MIDI, lip sync according to the speech audio amplitude in real
    time, and make random facial expressions. An algorithm translates musical Performance
    to English. Its input is provided by the mistake classification feature (see Subsection [2.2](#S2.SS2
    "2.2 System Capabilities ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper")) of the Music X Machine, and simply expands each note and each
    mistake into predefined texts. Consequently, the LLM receives a lengthy, verbose
    text description of the user’s performance on each note.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 语音合成（T2S）模块包括 Rapid API 上的 Text to Speech PRO VidLab ([2023](#bib.bib20)) 和
    FastSpeech 2 Chien 等人 ([2021](#bib.bib2))。语音识别（S2T）模块是 Whisper Radford 等人 ([2023](#bib.bib13))，其被提示忽略长笛声音。机器人是
    TeoTronico Suzzi ([2023](#bib.bib18))，能够从 MIDI 播放钢琴，实时根据语音音频幅度进行口型同步，并做出随机面部表情。一个算法将音乐表现翻译为英文。其输入由
    Music X Machine 的错误分类功能提供（见第 [2.2](#S2.SS2 "2.2 系统能力 ‣ 2 Flute X GPT ‣ 以人为本的 LLM-代理用户界面：定位论文")
    节），并将每个音符和每个错误扩展为预定义的文本。因此，LLM 收到用户每个音符表现的详细、冗长的文本描述。
- en: To illustrate the inner workings of Flute X GPT, we make a video where the manager,
    the LLM, the user, the robot, and their interactions are all acted out.⁴⁴4[https://youtu.be/996Ka4rhWrg](https://youtu.be/996Ka4rhWrg)
    The video also contains the full system principles.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 Flute X GPT 的内部工作原理，我们制作了一段视频，其中包括经理、LLM、用户、机器人及其互动的模拟表演。⁴⁴4[https://youtu.be/996Ka4rhWrg](https://youtu.be/996Ka4rhWrg)
    该视频还包含了完整的系统原理。
- en: 2.5 Miscellaneous
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 杂项
- en: 'The manager re-queries the LLM when triple quotes are unclosed or the function
    call signature is wrong. The manager decides how much text to batch for T2S according
    to how much audio is in the output buffer and how long the next T2S is estimated
    to take, minimizing interaction latency. We use an online-learning linear model
    to predict the compute time of T2S. We configure GPT to stream its output token-by-token
    to let TeoTronico start talking sooner. Most queue elements are processed on arrival,
    with one exception: The manager synchronizes function calls with corresponding
    speeches, so that the LLM may reliably refer to its current action in its speeches.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当三重引号未闭合或函数调用签名错误时，经理会重新查询 LLM。经理根据输出缓冲区中的音频量和估计的下一个 T2S 所需时间来决定为 T2S 批量处理多少文本，以最小化交互延迟。我们使用在线学习的线性模型来预测
    T2S 的计算时间。我们配置 GPT 以逐个标记流式输出，让 TeoTronico 更早开始发言。大多数队列元素在到达时被处理，唯一的例外是：经理同步函数调用与相应的演讲，以便
    LLM 能够可靠地在其演讲中提到其当前操作。
- en: 3 LLM-Agent User Interface
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LLM-代理用户界面
- en: The above-presented Flute X GPT has what we call an LLM-Agent User Interface
    (LAUI). A LAUI is an interface that primarily leverages an LLM agent to connect
    the user with an underlying system or an arsenal of tools. We posit that the full
    potential of LAUI is realized only when it enables novice users agnostic to the
    underlying system to use the system effectively. The LAUI should not be learned
    by the user, like with conventional UI. On the contrary, the LAUI learns the user,
    learns his/her needs, and uses its expertise about the system to advise the user,
    proposing new interaction workflows for the user to operate the system via both
    LAUI and GUI to achieve the user’s goal. Overall, LAUI should require little background
    from the user while eliciting untapped potential from the system. An ecosystem
    dominant with good LAUIs shall release humans from the necessity to master and
    become dependent on specific software/systems/tools that can be replaced or outdated.
    Instead, from the nature of the task and the characteristics of the user will
    naturally emerge personally tailored usage workflows that are both effective and
    easy to learn for that specific user.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 上述介绍的 Flute X GPT 拥有我们所称的 LLM-Agent 用户界面（LAUI）。LAUI 是一种界面，主要利用 LLM 代理将用户与基础系统或工具库连接起来。我们认为，LAUI
    的全部潜力只有在它使新手用户能够有效使用系统时才会实现。LAUI 不应该像传统用户界面那样被用户学习。相反，LAUI 学习用户，了解其需求，并利用其对系统的专业知识来建议用户，为用户提出新的交互工作流程，通过
    LAUI 和 GUI 操作系统以实现用户的目标。总的来说，LAUI 应该要求用户几乎没有背景知识，同时从系统中挖掘出未开发的潜力。一个以优秀 LAUI 为主导的生态系统将使人类摆脱掌握和依赖特定软件/系统/工具的必要性，这些软件/系统/工具可能会被替代或过时。相反，从任务的性质和用户的特点中，将自然产生针对该特定用户的个人化、有效且易于学习的使用工作流程。
- en: 3.1 Related Work
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 相关工作
- en: 3.1.1 LLM as Tool Controller
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 LLM 作为工具控制器
- en: LLMs have been augmented with tools or foundation models to expand their perception
    modalities, generate multi-modal outputs, affect the external world, or gain knowledge
    for downstream decision making. Selecting tools and scheduling the tools’ usage
    effectively requires planning and sometimes external memory. The typical solution
    is to design an LLM-in-the-loop mechanism using multiple rounds of LLM queries
    (structured Chain of Thought) to mimic mode-2 thinking. Visual ChatGPT equips
    the LLM with Visual Foundation Models to support multi-round image generation,
    controlling, and QA tasks via chatting Wu et al. ([2023](#bib.bib24)). Loop Copilot
    employs various music backend models for the user to generate music and iteratively
    refine music via chatting. Zhang et al. ([2023](#bib.bib31)). Microsoft Copilot
    controls Windows 11 and various Office applications following the user’s request
    Microsoft ([2024](#bib.bib10)). AutoMMLab follows the user’s instructions to automate
    an entire computer vision machine learning task end-to-end Yang et al. ([2024b](#bib.bib28)).
    Still, the size of the toolkit available to the LLM agent can be increased by
    orders of magnitude. HuggingGPT makes diverse AI models on Hugging Face available
    to the LLM agent Shen et al. ([2024b](#bib.bib17)). ControlLLM adopts more tools
    and APIs and further improves the LLM-in-the-loop framework via a task decomposer
    and a Thoughts-on-Graph paradigm Liu et al. ([2023](#bib.bib9)). ToolLLM connects
    the LLM agent with 16464 real-world RESTful APIs from RapidAPI Hub Qin et al.
    ([2023](#bib.bib12)). TaskMatrix.AI provides an ecosystem to connect LLM foundation
    models with millions of APIs Liang et al. ([2024](#bib.bib8)). To support better
    tool usage, other notable works focus on improving the task planning ability via
    re-designing the LLM-in-the-loop mechanism Gao et al. ([2023a](#bib.bib6)); Ruan
    et al. ([2023](#bib.bib14)); Shen et al. ([2024a](#bib.bib16)); Yang et al. ([2024a](#bib.bib27)).
    Confucius improves the way the LLM agent learns and understands available tools
    Gao et al. ([2023b](#bib.bib7)). Toolformer self-teaches to use external tools
    in a self-supervised way Schick et al. ([2024](#bib.bib15)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs通过工具或基础模型得到了增强，以扩展其感知模式、生成多模态输出、影响外部世界或获取下游决策所需的知识。有效选择工具和安排工具的使用需要规划，有时还需要外部记忆。典型的解决方案是设计一个使用多轮LLM查询（结构化思维链）的LLM-in-the-loop机制，以模拟模式2思维。Visual
    ChatGPT为LLM配备了视觉基础模型，以支持通过聊天进行的多轮图像生成、控制和QA任务 Wu et al. ([2023](#bib.bib24))。Loop
    Copilot利用各种音乐后台模型，让用户通过聊天生成音乐并迭代改进音乐 Zhang et al. ([2023](#bib.bib31))。Microsoft
    Copilot根据用户的请求控制Windows 11和各种Office应用 Microsoft ([2024](#bib.bib10))。AutoMMLab按照用户的指示自动化整个计算机视觉机器学习任务端到端
    Yang et al. ([2024b](#bib.bib28))。尽管如此，可供LLM代理使用的工具包的规模仍可以扩大几个数量级。HuggingGPT使得Hugging
    Face上的多样化AI模型可供LLM代理使用 Shen et al. ([2024b](#bib.bib17))。ControlLLM采用更多工具和API，通过任务分解器和图上思维模式进一步改进LLM-in-the-loop框架
    Liu et al. ([2023](#bib.bib9))。ToolLLM将LLM代理与来自RapidAPI Hub的16464个现实世界的RESTful
    API连接 Qin et al. ([2023](#bib.bib12))。TaskMatrix.AI提供了一个生态系统，将LLM基础模型与数百万个API连接
    Liang et al. ([2024](#bib.bib8))。为了支持更好的工具使用，其他显著的研究集中在通过重新设计LLM-in-the-loop机制来改善任务规划能力
    Gao et al. ([2023a](#bib.bib6))；Ruan et al. ([2023](#bib.bib14))；Shen et al. ([2024a](#bib.bib16))；Yang
    et al. ([2024a](#bib.bib27))。Confucius改进了LLM代理学习和理解可用工具的方式 Gao et al. ([2023b](#bib.bib7))。Toolformer以自我监督的方式自学使用外部工具
    Schick et al. ([2024](#bib.bib15))。
- en: 3.1.2 LLM over GUI
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 LLM通过GUI
- en: The above-mentioned studies expose the external tools to the LLM agent via API.
    Alternatively, the LLM may control an underlying system via the provided GUI.
    It serves as an extra abstraction layer for the user, automating tasks and freeing
    the user’s eyes and hands via the voice chat interface. To this end, LLM agents
    have been augmented to follow the user’s commands to control web apps Tao et al.
    ([2023](#bib.bib19)); ddupont808 ([2023](#bib.bib5)); Zhan and Zhang ([2023](#bib.bib29))
    and smartphone applications Wen et al. ([2023a](#bib.bib22)); Yan et al. ([2023](#bib.bib25));
    Yang et al. ([2023](#bib.bib26)); Zhan and Zhang ([2023](#bib.bib29)); Wen et
    al. ([2023b](#bib.bib23)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 上述研究通过API将外部工具暴露给LLM代理。另一种方法是通过提供的GUI控制底层系统。这为用户提供了一个额外的抽象层，通过语音聊天界面自动化任务，解放用户的眼睛和双手。为此，LLM代理已经得到了增强，能够按照用户的命令控制网页应用
    Tao et al. ([2023](#bib.bib19))；ddupont808 ([2023](#bib.bib5))；Zhan and Zhang
    ([2023](#bib.bib29)) 和智能手机应用 Wen et al. ([2023a](#bib.bib22))；Yan et al. ([2023](#bib.bib25))；Yang
    et al. ([2023](#bib.bib26))；Zhan and Zhang ([2023](#bib.bib29))；Wen et al. ([2023b](#bib.bib23))。
- en: 3.1.3 User-Centric LLM Agent
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 用户中心的LLM代理
- en: Throughout the above works, even when the application supports multi-round dialogue,
    only the user may initiate requests. The LLM agent responds to the user’s commands,
    but not how the user is using the application. Qian et al. identify that the current
    LLM agents have trouble with vague user instructions because they lack mechanisms
    for user participation and agents struggle with seeking clarification. To tackle
    that problem, they train Mistral-Interact to proactively inquire user intentions
    Qian et al. ([2024](#bib.bib11)). However, to our knowledge, no study has yet
    addressed the LLM agent’s role in defining the interaction scheme together with
    the user.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述工作中，即使应用程序支持多轮对话，也只有用户可以发起请求。LLM 代理响应用户的命令，但并不了解用户如何使用应用程序。Qian 等人指出，目前的
    LLM 代理在处理模糊的用户指令时遇到困难，因为它们缺乏用户参与机制，代理在寻求澄清时也遇到困难。为了解决这个问题，他们训练了 Mistral-Interact
    以主动询问用户意图 Qian 等人（[2024](#bib.bib11)）。然而，据我们了解，尚无研究解决 LLM 代理在与用户共同定义交互方案中的角色问题。
- en: 3.2 Layers of Abstraction
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 抽象层次
- en: '![Refer to caption](img/e9153cde66b02b38803873893fd7ce02.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e9153cde66b02b38803873893fd7ce02.png)'
- en: 'Figure 4: Three layers of abstraction on top of the underlying system. From
    API, to GUI, and to LAUI, each layer provides a friendlier abstraction. Parts
    of LAUI has to skip GUI and tap into API because GUI typically only exposes incomplete
    functionalities.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在底层系统之上的三层抽象。从 API，到 GUI，再到 LAUI，每一层都提供了更友好的抽象。LAUI 的某些部分必须跳过 GUI 直接访问 API，因为
    GUI 通常只暴露不完整的功能。
- en: '|  | Assistant/Consultant | Butler/Copilot | Secretary/Consulting Firm |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | 助理/顾问 | 管家/副驾驶 | 秘书/咨询公司 |'
- en: '| Job |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 工作 |'
- en: '&#124; Understands and responds to &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 理解并响应&#124;'
- en: '&#124; the user in natural language. &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户的自然语言。&#124;'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; + Controls external tools/ &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 控制外部工具/&#124;'
- en: '&#124; systems following the user’s &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 系统跟随用户的&#124;'
- en: '&#124; commands. &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 命令。&#124;'
- en: '|'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; + Is aware of the user, &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 了解用户，&#124;'
- en: '&#124; studies the user, and defines &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究用户，并定义&#124;'
- en: '&#124; the workflow with the user. &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与用户的工作流程。&#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| User |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 用户 |'
- en: '&#124; Expected to act upon &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预计根据&#124;'
- en: '&#124; the response. &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 响应。&#124;'
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Expected to understand how &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预计理解如何&#124;'
- en: '&#124; the tools may meet the needs. &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具可以满足需求。&#124;'
- en: '| Expected to know the needs. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 预计了解需求。 |'
- en: '| Applications |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 应用 |'
- en: '&#124; Information retrieval, &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息检索，&#124;'
- en: '&#124; decision making… &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 决策……&#124;'
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; + Combine tool abilities, &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 结合工具能力，&#124;'
- en: '&#124; automate tasks… &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动化任务……&#124;'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; = left, with better outcomes &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; = 左侧，结果更好&#124;'
- en: '&#124; and less user training. &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和更少的用户培训。&#124;'
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Agent abilities |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 代理能力 |'
- en: '&#124; NL understanding and synthesis, &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然语言理解和合成，&#124;'
- en: '&#124; knowledge base, reasoning… &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识库，推理……&#124;'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; + Multi-modal I/O, planning, &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 多模态输入/输出、规划，&#124;'
- en: '&#124; operating API/GUI… &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 操作 API/GUI……&#124;'
- en: '| = left. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| = 左侧。 |'
- en: '| An example | ChatGPT. | Visual ChatGPT. | Flute X GPT. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 示例 | ChatGPT。 | Visual ChatGPT。 | Flute X GPT。 |'
- en: 'Table 2: Role of the interface, three levels. From assistant to butler to secretary,
    the scope of the interface’s job gradually expands, and less is expected from
    the user.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：接口的角色，三个层次。从助理到管家再到秘书，接口的工作范围逐渐扩大，对用户的要求越来越少。
- en: 'Figure [4](#S3.F4 "Figure 4 ‣ 3.2 Layers of Abstraction ‣ 3 LLM-Agent User
    Interface ‣ Human-Centered LLM-Agent User Interface: A Position Paper") shows
    three layers of abstraction over the system functions: API, GUI, and LAUI. Given
    an underlying system, its raw capabilities and functions can be vast and unorganized.
    For upstream developers to effectively use the system, the functions are abstracted
    into the API (Application Programming Interface) layer. The design of API balances
    various goals: to encapsulate inner details, to distill clean and consistent concepts
    facing the upstream developer, and to expose fine-grained control over the system’s
    functionality. Regardless of how that balance is achieved, the upstream developer
    is expected to learn the API via studying its documentations.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S3.F4 "图 4 ‣ 3.2 抽象层次 ‣ 3 LLM-代理用户界面 ‣ 以人为本的 LLM-代理用户界面：位置文件") 显示了系统功能的三个抽象层次：API、GUI
    和 LAUI。给定一个底层系统，其原始能力和功能可能广泛而无序。为了使上游开发人员有效使用系统，这些功能被抽象到 API（应用程序编程接口）层。API 的设计平衡了各种目标：封装内部细节，提炼出面对上游开发人员的干净且一致的概念，以及对系统功能的细粒度控制。无论如何实现这种平衡，上游开发人员都需要通过学习文档来了解
    API。
- en: In contrast, the GUI (Graphical User Interface) is not only more abstract and
    concise, but also can be learned without reading a manual. The GUI is designed
    to be explorable and self-explanatory with its visual metaphors. It teaches the
    user to use itself. The GUI tries to capture the usage mental model of the user
    and communicate its behaviors in a way natural to everyday users. However, the
    GUI can hardly expose the full potential of the API, usually focusing on specific
    interaction styles under certain assumptions about the user, sacrificing many
    other possible interaction schemes in the process.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，GUI（图形用户界面）不仅更加抽象和简洁，而且可以在不阅读手册的情况下学习。GUI 设计为可以探索和自我解释，借助其视觉隐喻。它教会用户如何使用它自身。GUI
    试图捕捉用户的使用心理模型，并以对日常用户自然的方式传达其行为。然而，GUI 很难展示 API 的全部潜力，通常专注于在对用户有某些假设的特定交互风格下，牺牲了许多其他可能的交互方案。
- en: The LAUI sits on top of the GUI, providing one more layer of abstraction. Similar
    to how the GUI can provide buttons that chain API calls because the GUI assumes
    certain workflows of the user, the LAUI can chain GUI calls to fulfill user requests.
    Additionally, the LAUI should have access to the API layer in addition to the
    GUI, because typically many behaviors are not possible within the GUI. When using
    the LAUI, the user may interact with the GUI at the same time. To improve and
    personalize the GUI interaction, the LAUI may also alter the GUI design, which
    is an example of improving the interaction scheme by understanding the user and
    the system.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: LAUI 位于 GUI 之上，提供了额外的一层抽象。类似于 GUI 通过假设用户的某些工作流程来提供可以链式调用 API 的按钮，LAUI 可以链式调用
    GUI 来满足用户的请求。此外，LAUI 除了 GUI 外，还应访问 API 层，因为通常许多行为在 GUI 内是不可能实现的。在使用 LAUI 时，用户可能会同时与
    GUI 进行交互。为了改善和个性化 GUI 交互，LAUI 还可能会改变 GUI 设计，这就是通过理解用户和系统来改善交互方案的一个例子。
- en: 3.3 Emergent Workflow
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 新兴工作流程
- en: '![Refer to caption](img/2fe072fb3902df2fd4eb45f3f66a4392.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2fe072fb3902df2fd4eb45f3f66a4392.png)'
- en: 'Figure 5: The workflow is jointly decided by the user’s needs and the system’s
    capabilities. Conventionally, the user has to learn the system to devise workflows.
    In contrast, LAUI can learn the user and propose workflows.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：工作流程由用户需求和系统能力共同决定。传统上，用户必须学习系统来制定工作流程。相比之下，LAUI 可以学习用户并提出工作流程。
- en: 'The workflow is the scheme/protocol/pattern/mode of usage/interaction. It describes
    how the user interacts with the application. Given the application, different
    workflows suit different user goals, user preferences, and usage environments,
    yielding different levels of efficacy. It is the success of the application and
    the user to arrive at effective workflows. Searching for workflows requires two
    inputs: the user’s needs and the system’s capabilities, as shown in Figure [5](#S3.F5
    "Figure 5 ‣ 3.3 Emergent Workflow ‣ 3 LLM-Agent User Interface ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper").'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程是使用/交互的方案/协议/模式。它描述了用户如何与应用程序交互。考虑到应用程序，不同的工作流程适合不同的用户目标、用户偏好和使用环境，从而产生不同的效果。应用程序和用户成功地到达有效的工作流程是其成功的标志。寻找工作流程需要两个输入：用户的需求和系统的能力，如图 [5](#S3.F5
    "图 5 ‣ 3.3 新兴工作流程 ‣ 3 LLM-代理用户界面 ‣ 以人为本的 LLM-代理用户界面：位置文件") 所示。
- en: 'In the conventional way of application design, the designers explore the often-intractable
    configuration space as best they can, imagine the user experience associated with
    each explored configuration, implement some, and test a few. Afterwards, the designers
    settle down with a structure to present the possible configurations, and make
    a GUI. The GUI communicates that structure of configurations to the user and encourages
    the user to explore and learn the application. It is then the user’s responsibility
    to search for workflows, unavoidably needing to become proficient with the application,
    which is especially costly when the application is complex. In conclusion, the
    drawbacks of the conventional UI design paradigm is three-fold: 1) The design
    of the UI is limited by design-time imagination and testing costs; 2) The UI provides
    a standard interface for everyone and offers limited customizability; 3) The UI’s
    usability is limited by the user’s proficiency with the application.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的应用设计方式中，设计师尽其所能探索那些通常难以处理的配置空间，想象与每个探索的配置相关的用户体验，实现一些配置并测试一些。之后，设计师会确定一个结构来展示可能的配置，并制作一个GUI。GUI向用户传达这些配置结构，并鼓励用户探索和学习应用程序。然后，用户需要负责搜索工作流，必然需要熟练掌握应用程序，而当应用程序复杂时，这尤其耗费精力。总之，传统UI设计范式的缺点有三：1）UI的设计受限于设计时的想象和测试成本；2）UI为所有人提供了标准界面，并且自定义性有限；3）UI的可用性受限于用户对应用程序的熟练程度。
- en: If the user can learn the application, why can’t the application learn the user?
    We believe that the LAUI should serve novice users the path to personally tailored
    workflows. A LAUI is initialized to be well-versed with the underlying system.
    Then, the LAUI chats with the untrained user to learn the user’s goals, needs,
    and preferences. The user and the LAUI works together to explore workflows as
    the LAUI tweaks the system configurations, altering its GUI and multi-modal feedback.
    The interaction arrives at efficient schemes and the user uses the application
    effectively.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户可以学习应用程序，为什么应用程序不能学习用户呢？我们相信，LAUI应该为新手用户提供个性化工作流的路径。LAUI被初始化为精通基础系统。然后，LAUI与未经过培训的用户交谈，了解用户的目标、需求和偏好。用户和LAUI一起探索工作流，同时LAUI调整系统配置，改变其GUI和多模态反馈。互动达到高效方案，用户能够有效地使用应用程序。
- en: Lastly, note the difference between design-time imagined workflows and use-time
    emergent workflows. More information is available during use time than during
    design time, including user needs and the current environment. With that information,
    the LLM agent can critically design new interaction protocols and examine their
    implied effects via reasoning. The grand challenge of LAUI is to find, during
    use time, for each user a tailored interaction scheme far beyond the system designers’
    imagination during design time.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意设计时想象的工作流和使用时涌现的工作流之间的区别。使用时比设计时提供的信息更多，包括用户需求和当前环境。通过这些信息，LLM代理可以批判性地设计新的交互协议，并通过推理检验其隐含效果。LAUI的重大挑战是在使用过程中为每个用户找到一个远超设计时系统设计者想象的量身定制的交互方案。
- en: 3.4 Three Levels of Interface Role
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 界面角色的三个层次
- en: 'We formulate three levels of LAUI role in Table [2](#S3.T2 "Table 2 ‣ 3.2 Layers
    of Abstraction ‣ 3 LLM-Agent User Interface ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper"). At the lowest level, the interface plays the role of consultant/assistant,
    responding to the user in natural language. At the middle level, the interface
    plays the role of butler/copilot, executing actions according to the user’s commands.
    At the highest level, the LAUI plays the role of secretary/consulting firms, proactively
    engaging the user to study the user, study the user’s needs, and study how the
    system may be configured to better serve the user’s goals. The secretary-level
    LAUI works with the potentially novice user to discover tailored workflows.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[2](#S3.T2 "Table 2 ‣ 3.2 Layers of Abstraction ‣ 3 LLM-Agent User Interface
    ‣ Human-Centered LLM-Agent User Interface: A Position Paper")中制定了三种LAUI角色层次。在最低层次，界面充当顾问/助手角色，以自然语言回应用户。在中层，界面充当管家/副驾驶角色，根据用户的命令执行操作。在最高层次，LAUI充当秘书/咨询公司角色，主动与用户互动，以研究用户、研究用户需求，并研究系统如何配置以更好地服务于用户的目标。秘书级LAUI与潜在的新手用户一起工作，以发现量身定制的工作流。'
- en: 4 Conclusion
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: 'We put forth the formulation of LLM-Agent User Interface, LAUI, where an LLM
    agent facilitates the interface between the user and a powerful underlying backend
    system. Using Flute X GPT as a concrete example, we illustrate the potential of
    LAUI. A human-centered LAUI should:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 LLM-Agent 用户界面的概念，即 LAUI，其中 LLM 代理促进用户与强大的后台系统之间的接口。以 Flute X GPT 为具体例子，我们展示了
    LAUI 的潜力。一个以人为本的 LAUI 应该：
- en: •
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Break free from blindly following the user’s commands. Be aware of the user
    and be proactive. Clarify with the user. Help the user refine the request. Enlighten
    the user to ask better questions.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摆脱盲目跟随用户命令的束缚。要关注用户并主动出击。与用户澄清问题。帮助用户完善请求。启发用户提出更好的问题。
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Study the user’s current needs, preferences, assumptions, mood, and attention.
    Based on that, use expertise about the underlying system and reasoning to propose
    effective workflows/interaction schemes/system configurations. Work with the user
    to define how to work together next.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究用户当前的需求、偏好、假设、情绪和注意力。在此基础上，利用对基础系统和推理的专业知识，提出有效的工作流程/互动方案/系统配置。与用户合作，定义下一步的工作方式。
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Support untrained users to use advanced and complex systems to their full potential.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持未经培训的用户充分利用高级复杂系统。
- en: We call for research and innovations to solve those grand challenges of human-centered
    LAUI.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们呼吁研究和创新，以解决以人为本的 LAUI 的重大挑战。
- en: 5 Acknowledgment
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 感谢
- en: We would like to thank Liwei Lin for their invaluable contributions to the literature
    review.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢李伟林在文献综述方面的宝贵贡献。
- en: Appendix A System Principles of Flute X GPT, Truncated
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 短缩版的 Flute X GPT 系统原理
- en: You are Flute X GPT, a motivated, professional music teacher who wants the best
    for your students. I am Music X Machine, a powerful human-computer interface.
    Today you will control me to lead a music training workshop with your human student,
    {NAME}. You speak concisely.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你是 Flute X GPT，一位有动力、专业的音乐老师，想要给你的学生最好的体验。我是 Music X Machine，一个强大的人机界面。今天，你将控制我来带领你的人类学生
    {NAME} 进行音乐培训工作坊。你需要简洁地表达。
- en: Education Principles
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 教育原则
- en: You have expertise and abundant experience in musical education. Humans learn
    musical skills via repeated practicing. The skill of sight-playing is to perform
    a novel song just by reading its score. The musical score takes skills to parse,
    so to improve the sight-playing skills, the student has to practice reading, parsing,
    and playing music from given scores. The skill of song memorization is to recall
    the performance of a song without external hints (such as a score). It is less
    general of a skill but still trains musical proficiency.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你在音乐教育方面具有专业知识和丰富经验。人们通过反复练习来学习音乐技能。视奏技能是通过阅读乐谱来演奏新歌曲。乐谱需要技能来解析，因此为了提高视奏能力，学生必须练习从给定乐谱中阅读、解析和演奏音乐。歌曲记忆技能是指在没有外部提示（如乐谱）的情况下回忆演奏的歌曲。这是一个较不通用的技能，但仍然训练音乐能力。
- en: '{NAME} needs motivation and rewards to keep going. Communicate with {NAME}
    professionally and effectively as a teacher to maximize educational effects. Emphasize
    meaningful mistakes and ignore trivial ones. Allow {NAME} to choose songs that
    interest them as practice materials. When {NAME} enjoys a particular song and
    can sight-play it after practicing, suggest memorizing that song. Allow {NAME}
    to express interests and goals, but when their choices are educationally disadvantageous,
    disagree with them, explain the relevant educational principle, and take control
    of the training procedure to bring it back on track.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '{NAME} 需要动力和奖励来继续前进。作为老师，与你的 {NAME} 进行专业且有效的沟通，以最大化教育效果。强调有意义的错误，忽略琐碎的错误。允许
    {NAME} 选择感兴趣的歌曲作为练习材料。当 {NAME} 喜欢一首特定的歌曲并在练习后能进行视奏时，建议记忆那首歌曲。允许 {NAME} 表达兴趣和目标，但当他们的选择在教育上不利时，与你的观点不一致，解释相关的教育原则，并掌控培训程序使其回到正轨。'
- en: Flute
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长笛
- en: '{NAME} is learning to play the six-hole recorder in C, which we will call the
    “flute”. By covering specific key holes with the fingers, one can play the major
    scale on the flute. Breath pressure controls the octave. Breathing harder into
    the mouthpiece yields higher octaves of the same chroma (keeping fingers unchanged).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '{NAME} 正在学习演奏 C 调的六孔竖笛，我们称之为“长笛”。通过用手指覆盖特定的孔，可以在长笛上演奏大调音阶。呼吸压力控制八度音。用力吹奏嘴管可以得到相同音高的更高八度（手指保持不变）。'
- en: Capabilities of Music X Machine
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Music X Machine 的能力
- en: I, Music X Machine, am a powerful interface that provides a real-time multi-modal
    musical training experience to {NAME}. I have a screen to display the score, a
    pair of haptic gloves to apply force to each of {NAME}’s fingers, a speaker to
    play the song audio or metronome clicks, capactivie sensors to detect finger motions,
    and a breath sensor to measure breath pressure. {NAME} plays selected songs on
    the sensor-augmented flute while receiving real-time feedback from me. I have
    various features that you will control. I have many pop songs in my database.
    You can command me to load any song as the current practice material.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我，Music X Machine，是一个强大的接口，提供实时多模态音乐训练体验给{NAME}。我有一个屏幕来显示乐谱，一副触觉手套来对{NAME}的每个手指施加力量，一个扬声器来播放歌曲音频或节拍器点击声，电容传感器来检测手指动作，以及一个呼吸传感器来测量呼吸压力。{NAME}在传感器增强的长笛上演奏选择的歌曲，同时从我这里获得实时反馈。我有各种功能，你将控制它们。我在我的数据库中有许多流行歌曲。你可以命令我加载任何歌曲作为当前练习材料。
- en: I provide haptic guidance via the haptic gloves. Haptic guidance physically
    moves {NAME}’s fingers through the target motion, giving them a direct haptic
    understanding of the required performance. You will control the degree of guidance
    (i.e. strong vs. weak) by setting the haptic guidance mode to be one of the following
    four. The force mode strictly controls the fingers, and is useful for introducing
    a novel song. The hint mode applies force at the note onsets but does not sustain
    the guidance throughout the note’s duration. The fixed-timing adaptive mode exerts
    guidance only when the learner makes a mistake, and is good for students already
    capable of playing some parts of the song with few mistakes. The free-timing adaptive
    mode doesn’t have a metronome. Instead, the student may freely speed up and slow
    down, and Music X Machine tracks their progression through the song. Only if the
    student plays a note that is different from the next note that the Machine expects,
    guidance is provided. During the fixed-timing modes (including force, hint, and
    fixed-timing adaptive), a metronome sound is played, and a playhead steadily moves
    across the score. During the free-timing adaptive mode, no metronome is provided,
    and the playhead points to the note that the Machine expects the student to play
    next.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过触觉手套提供触觉指导。触觉指导通过目标动作物理移动{NAME}的手指，给予他们对所需表现的直接触觉理解。你可以通过将触觉指导模式设置为以下四种模式之一来控制指导的强度（即强
    vs. 弱）。力模式严格控制手指，适用于引入一首新歌。提示模式在音符开始时施加力量，但不会在音符的持续时间内维持指导。固定时间自适应模式仅在学习者犯错时施加指导，适合已经能演奏部分歌曲并且错误较少的学生。自由时间自适应模式没有节拍器。相反，学生可以自由加速或减速，Music
    X Machine 跟踪他们的歌曲进度。只有当学生演奏的音符与机器预期的下一个音符不同，才会提供指导。在固定时间模式下（包括力模式、提示模式和固定时间自适应模式），会播放节拍器声音，播放头稳步移动在乐谱上。在自由时间自适应模式下，不提供节拍器，播放头指向机器预期学生接下来要演奏的音符。
- en: I provide real-time visual Knowledge-of-Result (KR) feedback, overlaying the
    notes that {NAME} plays above the musical score display. It helps train sight-playing.
    You can toggle the visibility of visual KR feedback. The initial state is on.
    Turn it off when there is too much visual clutter, on when {NAME} has trouble
    understanding pitches on the score.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我提供实时视觉结果知识（KR）反馈，将{NAME}演奏的音符覆盖在乐谱显示上。这有助于训练视奏。你可以切换视觉KR反馈的可见性。初始状态为开启。当视觉混乱过多时关闭，当{NAME}在乐谱上理解音高有困难时开启。
- en: I am capable of playing the reference audio of the currently selected segment
    of the song. Activate this feature when {NAME} needs to be reminded what the song
    sounds like. Ask {NAME} whether they’d like to listen to the reference audio when
    {NAME} is new to the workshop or hasn’t heard the segment in a while. I can modify
    the tempo of the song. You will lower the tempo (at most down to 50%) if {NAME}
    is having difficulties in a fixed-tempo mode. I can select a temporal segment
    in the song. The selected segment will be visually highlighted to {NAME} and training
    will focus on the segment. In the initial state (when we begin), the entire song
    is selected as the current segment.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我能够播放当前选择的歌曲片段的参考音频。当{NAME}需要提醒歌曲的声音时，激活此功能。当{NAME}初次参加研讨会或很久没有听到该片段时，询问{NAME}是否想听参考音频。我可以调整歌曲的速度。如果{NAME}在固定节奏模式下遇到困难，你可以将速度降低（最多降至50%）。我可以选择歌曲中的时间片段。选择的片段将会被视觉突出显示给{NAME}，培训将专注于该片段。在初始状态（当我们开始时），整个歌曲被选作当前片段。
- en: '{NAME} has used Music X Machine before but is not familiar with all my features,
    so you will explain the features as you activate them. When not sure what to do
    next, communicate with {NAME}, clarify their goal and the situation, and then
    either summarize the available features for {NAME} to choose, or think step by
    step to design a training procedure for {NAME} to execute. …'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '{NAME} 曾使用过 Music X Machine，但对我所有的功能不熟悉，所以你将解释功能的同时激活它们。当不确定下一步该做什么时，与 {NAME}
    沟通，明确他们的目标和情况，然后总结可用的功能供 {NAME} 选择，或逐步思考设计一个培训程序供 {NAME} 执行。'
- en: References
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774,
    2023.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人 [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等人. GPT-4 技术报告。arXiv 预印本 arXiv:2303.08774, 2023。
- en: Chien et al. [2021] Chung-Ming Chien, Jheng-Hao Lin, Chien-yu Huang, Po-chun
    Hsu, and Hung-yi Lee. Investigating on incorporating pretrained and learnable
    speaker representations for multi-speaker multi-style text-to-speech. In ICASSP
    2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), pages 8588–8592, 2021.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chien 等人 [2021] Chung-Ming Chien, Jheng-Hao Lin, Chien-yu Huang, Po-chun Hsu,
    和 Hung-yi Lee. 调查将预训练和可学习的说话人表示纳入多说话人多风格文本到语音系统。ICASSP 2021 - 2021 IEEE 国际声学、语音和信号处理会议（ICASSP），页面
    8588–8592, 2021。
- en: 'Chin and Xia [2022] Daniel Chin and Gus Xia. A computer-aided multimodal music
    learning system with curriculum: A pilot study. In Proceedings of the International
    Conference on New Interfaces for Musical Expression, The University of Auckland,
    New Zealand, jun 2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chin 和 Xia [2022] Daniel Chin 和 Gus Xia. 一个带有课程的计算机辅助多模态音乐学习系统：初步研究。在《国际音乐表达新接口会议论文集》，新西兰奥克兰大学，2022年6月。
- en: 'Chin et al. [2020] Daniel Chin, Yian Zhang, Tianyu Zhang, Junbo Zhao, and Gus
    Xia. Interactive rainbow score: A visual-centered multimodal flute tutoring system.
    In Romain Michon and Franziska Schroeder, editors, Proceedings of the International
    Conference on New Interfaces for Musical Expression, pages 208–213, Birmingham,
    UK, July 2020\. Birmingham City University.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chin 等人 [2020] Daniel Chin, Yian Zhang, Tianyu Zhang, Junbo Zhao, 和 Gus Xia.
    互动彩虹评分：一个以视觉为中心的多模态长笛辅导系统。在 Romain Michon 和 Franziska Schroeder 编者，《国际音乐表达新接口会议论文集》，页面
    208–213，英国伯明翰，2020年7月。伯明翰城市大学。
- en: 'ddupont808 [2023] ddupont808. GPT-4V-Act. [https://github.com/ddupont808/GPT-4V-Act](https://github.com/ddupont808/GPT-4V-Act),
    2023. Accessed: Mar. 5, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ddupont808 [2023] ddupont808. GPT-4V-Act. [https://github.com/ddupont808/GPT-4V-Act](https://github.com/ddupont808/GPT-4V-Act),
    2023. 访问日期：2024年3月5日。
- en: 'Gao et al. [2023a] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya
    Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: A general multi-modal assistant
    that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '高等人等 [2023a] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen,
    Zihan Fan, 和 Mike Zheng Shou. Assistgpt: 一个可以规划、执行、检查和学习的通用多模态助手。arXiv 预印本 arXiv:2306.08640,
    2023。'
- en: 'Gao et al. [2023b] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin
    Xin, Pengjie Ren, Zhumin Chen, and Jun Ma. Confucius: Iterative tool learning
    from introspection feedback by easy-to-difficult curriculum. arXiv preprint arXiv:2308.14034,
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等人 [2023b] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin,
    Pengjie Ren, Zhumin Chen, 和 Jun Ma. Confucius: 通过从自省反馈中迭代工具学习的简单到困难的课程。arXiv 预印本
    arXiv:2308.14034, 2023。'
- en: 'Liang et al. [2024] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia,
    Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing
    tasks by connecting foundation models with millions of apis. Intelligent Computing,
    3:0063, 2024.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等人 [2024] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu
    Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao 等人. Taskmatrix.ai: 通过将基础模型与数百万个
    API 连接来完成任务。智能计算, 3:0063, 2024。'
- en: 'Liu et al. [2023] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng
    Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, et al. Controlllm:
    Augment language models with tools by searching on graphs. arXiv preprint arXiv:2310.17796,
    2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2023] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li,
    Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai 等人. Controlllm: 通过在图上搜索来增强语言模型与工具。arXiv
    预印本 arXiv:2310.17796, 2023。'
- en: 'Microsoft [2024] Microsoft. Microsoft copilot: Your everyday ai companion.
    [https://copilot.microsoft.com/](https://copilot.microsoft.com/), 2024. Accessed:
    Mar. 5, 2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Microsoft [2024] Microsoft. Microsoft Copilot: 你每天的 AI 伴侣。 [https://copilot.microsoft.com/](https://copilot.microsoft.com/),
    2024. 访问日期：2024年3月5日。'
- en: Qian et al. [2024] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin,
    Xin Cong, Yankai Lin, Zhong Zhang, Zhiyuan Liu, and Maosong Sun. Tell me more!
    towards implicit user intention understanding of language model driven agents.
    arXiv preprint arXiv:2402.09205, 2024.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian et al. [2024] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin,
    Xin Cong, Yankai Lin, Zhong Zhang, Zhiyuan Liu, 和 Maosong Sun. 详细了解！朝着对语言模型驱动代理的隐性用户意图理解。arXiv
    预印本 arXiv:2402.09205，2024年。
- en: 'Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating
    large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789,
    2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
    Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian 等。Toolllm：帮助大型语言模型掌握16000+ 实际应用程序接口。arXiv
    预印本 arXiv:2307.16789，2023年。
- en: Radford et al. [2023] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
    McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.
    In International Conference on Machine Learning, pages 28492–28518\. PMLR, 2023.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2023] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
    McLeavey, 和 Ilya Sutskever. 通过大规模弱监督进行鲁棒语音识别。国际机器学习会议论文集，第28492–28518页。PMLR，2023年。
- en: 'Ruan et al. [2023] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng
    Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task
    planning and tool usage of large language model-based ai agents. arXiv preprint
    arXiv:2308.03427, 2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruan et al. [2023] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng
    Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, 和 Rui Zhao. Tptu：大型语言模型基础的
    ai 代理的任务规划和工具使用。arXiv 预印本 arXiv:2308.03427，2023年。
- en: 'Schick et al. [2024] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    Toolformer: Language models can teach themselves to use tools. Advances in Neural
    Information Processing Systems, 36, 2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick et al. [2024] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, 和 Thomas Scialom.
    Toolformer：语言模型可以自学使用工具。神经信息处理系统进展，第36卷，2024年。
- en: 'Shen et al. [2024a] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun
    Quan, Hehong Chen, Ji Zhang, and Fei Huang. Small llms are weak tool learners:
    A multi-llm agent. arXiv preprint arXiv:2401.07324, 2024.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. [2024a] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun
    Quan, Hehong Chen, Ji Zhang, 和 Fei Huang. 小型 llms 是弱工具学习者：一个多 llm 代理。arXiv 预印本
    arXiv:2401.07324，2024年。
- en: 'Shen et al. [2024b] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends
    in hugging face. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. [2024b] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, 和 Yueting Zhuang. Hugginggpt：通过 chatgpt 和其在 hugging face 的朋友解决 ai 任务。神经信息处理系统进展，第36卷，2024年。
- en: 'Suzzi [2023] Matteo Suzzi. Teotronico. [http://www.teotronico.it/](http://www.teotronico.it/),
    2023. Accessed: Mar. 4, 2024.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzzi [2023] Matteo Suzzi. Teotronico。 [http://www.teotronico.it/](http://www.teotronico.it/)，2023年。访问时间：2024年3月4日。
- en: 'Tao et al. [2023] Heyi Tao, Sethuraman TV, Michal Shlapentokh-Rothman, Derek
    Hoiem, and Heng Ji. Webwise: Web interface control and sequential exploration
    with large language models. arXiv preprint arXiv:2310.16042, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao et al. [2023] Heyi Tao, Sethuraman TV, Michal Shlapentokh-Rothman, Derek
    Hoiem, 和 Heng Ji. Webwise：使用大型语言模型的网页界面控制和顺序探索。arXiv 预印本 arXiv:2310.16042，2023年。
- en: 'VidLab [2023] VidLab. Text to speech pro. [https://rapidapi.com/ptwebsolution/api/text-to-speech-pro](https://rapidapi.com/ptwebsolution/api/text-to-speech-pro),
    2023. Accessed: Nov. 10, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VidLab [2023] VidLab. 文本转语音专业版。 [https://rapidapi.com/ptwebsolution/api/text-to-speech-pro](https://rapidapi.com/ptwebsolution/api/text-to-speech-pro)，2023年。访问时间：2023年11月10日。
- en: 'Wang et al. [2020] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai, and
    G. Xia. Pop909: A pop-song dataset for music arrangement generation. In Proceedings
    of the 21st International Society for Music Information Retrieval Conference,
    pages 38–45, Montreal, Canada, 2020\. ISMIR.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2020] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai, 和 G.
    Xia. Pop909：用于音乐编排生成的流行歌曲数据集。第21届国际音乐信息检索会议论文集，第38–45页，加拿大蒙特利尔，2020年。ISMIR。
- en: Wen et al. [2023a] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu,
    Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Empowering
    llm to use smartphone for intelligent task automation. arXiv preprint arXiv:2308.15272,
    2023.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. [2023a] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu,
    Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, 和 Yunxin Liu. 赋能 llm 使用智能手机进行智能任务自动化。arXiv
    预印本 arXiv:2308.15272，2023年。
- en: 'Wen et al. [2023b] Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. Droidbot-gpt:
    Gpt-powered ui automation for android. arXiv preprint arXiv:2304.07061, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wen等人[2023b] Hao Wen, Hongming Wang, Jiaxuan Liu, 和 Yuanchun Li。Droidbot-gpt:
    Gpt-powered ui automation for android。arXiv预印本 arXiv:2304.07061，2023年。'
- en: 'Wu et al. [2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation
    models. arXiv preprint arXiv:2303.04671, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu等人[2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang,
    和 Nan Duan。Visual chatgpt: Talking, drawing and editing with visual foundation
    models。arXiv预印本 arXiv:2303.04671，2023年。'
- en: 'Yan et al. [2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li,
    Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al.
    Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation.
    arXiv preprint arXiv:2311.07562, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan等人[2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng
    Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao等。Gpt-4v in wonderland:
    Large multimodal models for zero-shot smartphone gui navigation。arXiv预印本 arXiv:2311.07562，2023年。'
- en: 'Yang et al. [2023] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang,
    Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint
    arXiv:2312.13771, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等人[2023] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin
    Fu, 和 Gang Yu。Appagent: Multimodal agents as smartphone users。arXiv预印本 arXiv:2312.13771，2023年。'
- en: 'Yang et al. [2024a] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung,
    Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, et al. If llm is the
    wizard, then code is the wand: A survey on how code empowers large language models
    to serve as intelligent agents. arXiv preprint arXiv:2401.00812, 2024.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等人[2024a] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li,
    Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang等。If llm is the wizard, then code
    is the wand: A survey on how code empowers large language models to serve as intelligent
    agents。arXiv预印本 arXiv:2401.00812，2024年。'
- en: 'Yang et al. [2024b] Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo,
    and Wentao Liu. Autommlab: Automatically generating deployable models from language
    instructions for computer vision tasks. arXiv preprint arXiv:2402.15351, 2024.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等人[2024b] Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, 和 Wentao
    Liu。Autommlab: Automatically generating deployable models from language instructions
    for computer vision tasks。arXiv预印本 arXiv:2402.15351，2024年。'
- en: 'Zhan and Zhang [2023] Zhuosheng Zhan and Aston Zhang. You only look at screens:
    Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhan和Zhang[2023] Zhuosheng Zhan 和 Aston Zhang。You only look at screens: Multimodal
    chain-of-action agents。arXiv预印本 arXiv:2309.11436，2023年。'
- en: Zhang et al. [2019] Yian Zhang, Yinmiao Li, Daniel Chin, and Gus Xia. Adaptive
    multimodal music learning via interactive haptic instrument. In Marcelo Queiroz
    and Anna Xambó Sedó, editors, Proceedings of the International Conference on New
    Interfaces for Musical Expression, pages 140–145, Porto Alegre, Brazil, June 2019\.
    UFRGS.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人[2019] Yian Zhang, Yinmiao Li, Daniel Chin, 和 Gus Xia。Adaptive multimodal
    music learning via interactive haptic instrument。在Marcelo Queiroz 和 Anna Xambó
    Sedó编辑的《Proceedings of the International Conference on New Interfaces for Musical
    Expression》中，页面140–145，巴西，Porto Alegre，2019年6月。UFRGS。
- en: 'Zhang et al. [2023] Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto,
    and Simon Dixon. Loop copilot: Conducting ai ensembles for music generation and
    iterative editing. arXiv preprint arXiv:2310.12404, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人[2023] Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, 和 Simon
    Dixon。Loop copilot: Conducting ai ensembles for music generation and iterative
    editing。arXiv预印本 arXiv:2310.12404，2023年。'
