- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:45:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于基于LLM的智能体的调查：常见工作流程和可重复使用的LLM-概况组件
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.05804](https://ar5iv.labs.arxiv.org/html/2406.05804)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.05804](https://ar5iv.labs.arxiv.org/html/2406.05804)
- en: Xinzhe Li
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xinzhe Li
- en: School of IT, Deakin University, Australia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚迪肯大学信息技术学院
- en: lixinzhe@deakin.edu.au
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: lixinzhe@deakin.edu.au
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advancements in Large Language Models (LLMs) have catalyzed the development
    of sophisticated frameworks for developing LLM-based agents. However, the complexity
    of these frameworks r poses a hurdle for nuanced differentiation at a granular
    level, a critical aspect for enabling efficient implementations across different
    frameworks and fostering future research. Hence, the primary purpose of this survey
    is to facilitate a cohesive understanding of diverse recently proposed frameworks
    by identifying common workflows and reusable LLM-Profiled Components (LMPCs).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在大型语言模型（LLMs）方面的进展促进了开发基于LLM的智能体的复杂框架的发展。然而，这些框架的复杂性对在细粒度层面上进行细致区分构成了障碍，这是实现不同框架之间高效实施和促进未来研究的关键方面。因此，本调查的主要目的是通过识别共同的工作流程和可重复使用的LLM-概况组件（LMPCs），促进对最近提出的各种框架的凝聚性理解。
- en: 'A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于LLM的智能体的调查：常见工作流程和可重复使用的LLM-概况组件
- en: Xinzhe Li School of IT, Deakin University, Australia lixinzhe@deakin.edu.au
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Xinzhe Li，澳大利亚迪肯大学信息技术学院，lixinzhe@deakin.edu.au
- en: '![Refer to caption](img/6bf7748a40c6d44bf2e54fbc8675ae89.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6bf7748a40c6d44bf2e54fbc8675ae89.png)'
- en: (a) Policy-Only Workflows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 政策仅工作流程。
- en: '![Refer to caption](img/6d50dbc6cf630a0fea8da76d86a89414.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6d50dbc6cf630a0fea8da76d86a89414.png)'
- en: (b) Search Workflows.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 搜索工作流程。
- en: '![Refer to caption](img/d5c3508514f409c449f1ca3ba1228666.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d5c3508514f409c449f1ca3ba1228666.png)'
- en: (c) Feedback-Learning Workflows.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 反馈学习工作流程。
- en: 'Figure 1: Eight Common Workflows based on Three LLM-profiled Components (Policy,
    Evaluator and Dynamic Model) under Task or/and Tool-Use Environments.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于三个LLM概况组件（政策、评估器和动态模型）在任务或/和工具使用环境下的八种常见工作流程。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Generative Large Language Models (GLMs or LLMs) have acquired extensive general
    knowledge and human-like reasoning capabilities (Santurkar et al., [2023](#bib.bib15);
    Wang et al., [2022](#bib.bib23); Zhong et al., [2022](#bib.bib33), [2023](#bib.bib32)),
    positioning them as pivotal in constructing AI agents known as LLM-based agents.
    In the context of this survey, LLM-based agents are defined by their ability to
    interact actively with external tools (such as Wikipedia) or environments (such
    as householding environments) and are designed to function as integral components
    of agency, including acting, planning, and evaluating.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 生成型大型语言模型（GLMs或LLMs）已获得广泛的常识和类人推理能力（Santurkar等，[2023](#bib.bib15)；Wang等，[2022](#bib.bib23)；Zhong等，[2022](#bib.bib33)，[2023](#bib.bib32)），使其在构建被称为LLM基础的智能体的AI代理中占据了核心地位。在本调查的背景下，LLM基础的智能体被定义为能够与外部工具（如维基百科）或环境（如家庭环境）进行积极互动，并被设计为代理的核心组成部分，包括行动、规划和评估。
- en: Purpose of the Survey
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调查目的
- en: 'The motivation behind this survey stems from the observation that many LLM-based
    agents incorporate similar workflows and components, despite the presence of a
    wide variety of technical and conceptual challenges, e.g., search algorithms (Yao
    et al., [2023a](#bib.bib29)), tree structures (Hao et al., [2023](#bib.bib7)),
    and Reinforcement Learning (RL) components (Shinn et al., [2023](#bib.bib16)).
    (Wu et al., [2023](#bib.bib27)) offer a modular approach but lack integration
    with prevalent agentic workflows. Wang et al. ([2024](#bib.bib21)) provide a comprehensive
    review of LLM agents, exploring their capabilities across profiling, memory, planning,
    and action. In contrast, our survey does not attempt to cover all components of
    LLM-based agents comprehensively. Instead, we concentrate on the involvement of
    LLMs within agentic workflows and aim to clarify the roles of LLMs in agent implementations.
    We create common workflows incorporating reusable LLM-Profiled Components (LMPCs),
    as depicted in Figure [1](#S0.F1 "Figure 1 ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查的动机源于观察到许多基于LLM的代理虽然面临各种技术和概念挑战（例如搜索算法（Yao et al., [2023a](#bib.bib29)）、树结构（Hao
    et al., [2023](#bib.bib7)）以及强化学习（RL）组件（Shinn et al., [2023](#bib.bib16)）），但它们仍然采用类似的工作流和组件。（Wu
    et al., [2023](#bib.bib27)）提供了一种模块化的方法，但与流行的代理工作流缺乏整合。Wang et al. ([2024](#bib.bib21))
    提供了对LLM代理的全面回顾，探索了其在配置、记忆、规划和行动方面的能力。与此相比，我们的调查并不试图全面覆盖基于LLM的代理的所有组件。相反，我们集中于LLM在代理工作流中的参与，并旨在澄清LLM在代理实现中的角色。我们创建了包含可重用LLM配置组件（LMPC）的通用工作流，如图
    [1](#S0.F1 "图 1 ‣ 基于LLM的代理：通用工作流和可重用LLM配置组件")所示。
- en: Contributions
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献
- en: 'This survey offers the following contributions. 1) Alleviating the understanding
    of complex frameworks: The complexity of existing frameworks can be simplified
    into implementable workflows, especially when they are extracted for specific
    tasks. This survey emphasizes reusable workflows and LMPCs across popular frameworks,
    such as ReAct (Yao et al., [2023b](#bib.bib30)), Reflexion (Shinn et al., [2023](#bib.bib16))
    and Tree-of-Thoughts (Yao et al., [2023a](#bib.bib29)). Specifically, based on
    the interaction environments (§[2](#S2 "2 Task Environments And Tool Environments
    ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components"))
    and the use of common LMPCs (§[3](#S3 "3 LLM-Profiled Components ‣ A Survey on
    LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components")), we
    categorize and detail various workflows, e.g., tool-use workflows, search workflows,
    and feedback-learning workflows. Many existing frameworks are composed of these
    workflows and LMPCs, along with some specific non-LLM components. 2) Helping researchers/practitioners
    assess current frameworks at a more granular and cohesive level: Section [4](#S4
    "4 Workflows of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") categorizes prominent frameworks and demonstrates
    how they are assembled by the common workflows and LMPCs, as summarized in Table [2](#S3.T2
    "Table 2 ‣ Task-Dependent LLM-Profiled Components ‣ 3 LLM-Profiled Components
    ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components")
    ¹¹1A more detailed summarization is demonstrated in Appendix [A](#A1 "Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components"). 3) Facilitating further extensions of
    existing frameworks: Existing frameworks could be modified by changing the implementations
    of LMPCs. To enable this, we not only summarize implementations of LMPCs but also
    their applicability across diverse workflows and tasks in Section [5](#S5 "5 Implementations
    of LMPCs ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查提供了以下贡献。 1) 缓解对复杂框架的理解：现有框架的复杂性可以简化为可实现的工作流，尤其是在针对特定任务提取时。本调查强调了在流行框架中可重复使用的工作流和LMPCs，例如
    ReAct (Yao et al., [2023b](#bib.bib30))、Reflexion (Shinn et al., [2023](#bib.bib16))
    和 Tree-of-Thoughts (Yao et al., [2023a](#bib.bib29))。具体来说，基于互动环境 (§[2](#S2 "2
    Task Environments And Tool Environments ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components")) 和常见 LMPCs 的使用 (§[3](#S3 "3 LLM-Profiled
    Components ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"))，我们对各种工作流进行了分类和详细说明，例如工具使用工作流、搜索工作流和反馈学习工作流。许多现有框架由这些工作流和 LMPCs 组成，并附有一些特定的非
    LLM 组件。 2) 帮助研究人员/实践者在更细化和一致的层面上评估当前框架：第 [4](#S4 "4 Workflows of LLM-Based Agents
    ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components")
    节对突出框架进行了分类，并展示了它们如何通过常见的工作流和 LMPCs 组装，如表 [2](#S3.T2 "Table 2 ‣ Task-Dependent
    LLM-Profiled Components ‣ 3 LLM-Profiled Components ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components") 中所总结的¹¹1更详细的总结见附录 [A](#A1
    "Appendix A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components")。 3) 促进现有框架的进一步扩展：通过更改 LMPCs 的实现可以修改现有框架。为了实现这一点，我们不仅总结了
    LMPCs 的实现，还总结了它们在各种工作流和任务中的适用性，第 [5](#S5 "5 Implementations of LMPCs ‣ A Survey
    on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components") 节中进行了详细说明。'
- en: '| Env Types | Entities Interacted With by Agent | Action Properties | Examples
    of Action Instances | Examples of Env Instances |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 环境类型 | 代理互动的实体 | 动作属性 | 动作实例示例 | 环境实例示例 |'
- en: '| Task Environments |  |  |  |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 任务环境 |  |  |  |  |'
- en: '| Gaming Environments | Virtual game elements (objects, avatars, other characters),
    and possibly other players or game narratives | Discrete, Executable,'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '| 游戏环境 | 虚拟游戏元素（对象、化身、其他角色），可能还有其他玩家或游戏叙事 | 离散的、可执行的，'
- en: Deterministic | Move(Right) | BlocksWorld, CrossWords |
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性的 | Move(Right) | BlocksWorld, CrossWords |
- en: '| Embodied Environments | Physical world (through sensors and actuators) |
    Discrete, Executable,'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '| 具象环境 | 物理世界（通过传感器和执行器） | 离散的、可执行的，'
- en: Deterministic | Pick_Up[Object] | AlfWorld (Shridhar et al., [2021](#bib.bib17)),
    VirtualHome,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性的 | Pick_Up[Object] | AlfWorld (Shridhar et al., [2021](#bib.bib17)), VirtualHome,
- en: Minecraft (Fan et al., [2022](#bib.bib3)) |
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Minecraft (Fan et al., [2022](#bib.bib3)) |
- en: '| NLIEs | Humans (through conversation or text) | Free-form, Discrete,'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '| NLIEs | 人类（通过对话或文本） | 自由形式、离散的，'
- en: Deterministic
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性的
- en: (Single-step QA)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （单步 QA）
- en: Stochastic
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随机的
- en: (Multi-step) | The answer is Answer Finish[Answer] | GSM8K, HotpotQA |
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: （多步） | 答案是 Answer Finish[Answer] | GSM8K, HotpotQA |
- en: '| Tool Environments ( Nested with Task Environments) |  |  |  |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 工具环境（嵌套于任务环境中） |  |  |  |  |'
- en: '| Retrieval | Retrieval | Discrete, Executable,'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '| 检索 | 检索 | 离散的、可执行的，'
- en: Deterministic,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性的，
- en: Non-State-Altering | Wiki_Search[Entity] | A Wikipedia API (Goldsmith, [2023](#bib.bib4))
    (used by ReAct (Yao et al., [2023b](#bib.bib30))) |
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不改变状态 | Wiki_Search[实体] | 一个维基百科API（Goldsmith，[2023](#bib.bib4)）（由ReAct（Yao等，[2023b](#bib.bib30)）使用）|
- en: '| Calculator | Calculator | Executable, Deterministic,'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '| 计算器 | 计算器 | 可执行的、确定性的，'
- en: Non-State-Altering | 2 x 62 = << Calculator >> | Python’s eval function (used
    by MultiTool-CoT (Inaba et al., [2023](#bib.bib9))) |
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不改变状态 | 2 x 62 = << 计算器 >> | Python的eval函数（由MultiTool-CoT（Inaba等，[2023](#bib.bib9)）使用）|
- en: 'Table 1: Common task environments and tool-use environments. We categorize
    all the benchmarks existing in the work of the 12 agentic workflows into four
    environment types. An action instance is commonly formalized by action predicates
    and action arguments. Tool use can be considered internal environments of an agent
    and commonly defined for QA tasks under NLIEs.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：常见的任务环境和工具使用环境。我们将12个智能体工作流中的所有基准分类为四种环境类型。一个行动实例通常由行动谓词和行动参数形式化。工具使用可以视为智能体的内部环境，并通常在NLIEs下定义用于QA任务。
- en: 2 Task Environments And Tool Environments
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 任务环境与工具环境
- en: 'This section explores task environments and tool environments, which present
    different settings compared to traditional AI and reinforcement learning (RL)
    agent frameworks (Russell and Norvig, [2010](#bib.bib14); Sutton and Barto, [2018](#bib.bib18))
    . After a brief overview of standard logic-based gaming and simulated embodied
    environments, we focus on two specific areas: Natural Language Interaction Environments
    (NLIEs) and Tool Environments.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了任务环境和工具环境，这些环境与传统的AI和强化学习（RL）智能体框架（Russell和Norvig，[2010](#bib.bib14)；Sutton和Barto，[2018](#bib.bib18)）相比呈现不同的设置。在简要概述标准的基于逻辑的游戏和模拟化体现环境后，我们重点关注两个具体领域：自然语言互动环境（NLIEs）和工具环境。
- en: 2.1 Typical Task Environments
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 典型任务环境
- en: 'Typically, there are two common types of task environments: 1) Rule-Based Gaming
    Environments: These environments, which are deterministic and fully observable,
    include a variety of abstract strategy games like Chess and Go, and logic puzzles
    such as the Game of 24 (Yao et al., [2023a](#bib.bib29)) and Blocksworld (Hao
    et al., [2023](#bib.bib7)). They demand deep logical reasoning and strategic planning
    to navigate and solve. 2) Simulated Embodied Environments: These settings simulate
    real-world physical interactions and spatial relationships. They require agents
    to engage in navigation, object manipulation, and other complex physical tasks,
    reflecting changes in the physical environment.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有两种常见的任务环境：1) 基于规则的游戏环境：这些环境是确定性的且完全可观测的，包括各种抽象策略游戏，如象棋和围棋，以及逻辑谜题，例如24点游戏（Yao等，
    [2023a](#bib.bib29)）和积木世界（Hao等， [2023](#bib.bib7)）。它们要求深入的逻辑推理和战略规划来导航和解决问题。2)
    模拟化体现环境：这些环境模拟现实世界的物理互动和空间关系。它们要求智能体参与导航、物体操作和其他复杂的物理任务，反映物理环境的变化。
- en: 2.2 Natural Language Interaction Environments
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 自然语言互动环境
- en: With the rise of LLM agents, there is a growing trend among NLP researchers
    to recontextualize typical NLP tasks as agentic environments (Yao et al., [2023b](#bib.bib30);
    Hao et al., [2023](#bib.bib7); Yao et al., [2023a](#bib.bib29)). These settings
    are referred to as Natural Language Interaction Environments in our survey.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM智能体的兴起，NLP研究人员越来越倾向于将典型的NLP任务重新定义为智能体环境（Yao等，[2023b](#bib.bib30)；Hao等，[2023](#bib.bib7)；Yao等，[2023a](#bib.bib29)）。在我们的调查中，这些环境被称为自然语言互动环境。
- en: In NLIEs, the environment remains static until the agent acts. Unlike typical
    task environments where natural language serves as an intermediary, in NLIEs,
    both the states and actions are defined linguistically, making the states conceptual
    and the actions often ambiguous and broadly defined.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLIEs中，环境在智能体采取行动之前保持静态。与典型的任务环境中自然语言作为中介的情况不同，在NLIEs中，状态和行动都是以语言定义的，使得状态具有概念性，而行动往往模糊且定义宽泛。
- en: Single-Step NLIEs for Question Answering
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单步NLIEs用于问题回答
- en: Many works (Yao et al., [2023b](#bib.bib30); Shinn et al., [2023](#bib.bib16))
    formulate the traditional QA setup as a single-step decision-making process, where
    the agent generates an answer in response to a question. The process starts with
    the question as the initial state and concludes when the answer is provided as
    an action.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究（Yao 等，[2023b](#bib.bib30)；Shinn 等，[2023](#bib.bib16)）将传统的 QA 设置表述为单步决策过程，其中代理根据问题生成答案。该过程以问题作为初始状态开始，以提供答案作为行动结束。
- en: Deliberate Multi-step NLIEs
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深思熟虑的多步骤 NLIE
- en: For tasks where "intermediate steps are not explicitly defined", several studies
    have transformed NLP tasks into a Markov Decision Process to facilitate agentic
    workflows. For example, Hao et al. ([2023](#bib.bib7)) reformulate subquestions
    in QA tasks as actions, enabling responses to user queries through a multi-step
    process. This approach allows the initial question to serve as the beginning of
    a series of state transitions. Actions may vary from providing direct, free-form
    answers in single-step QA to strategically formulating subquestions that navigate
    the agent through sequential updates toward a comprehensive solution. This method
    aligns more closely with a sequential decision-making process, making it apt for
    deployment in planning-based agent systems. Additionally, Wan et al. ([2024](#bib.bib20))
    suggest that "splitting an output sequence into tokens might be a good choice"
    for defining multi-step NLIEs methodically. Furthermore, Yao et al. ([2023a](#bib.bib29))
    formulate two-step NLIEs for creative writing by segmenting the problem-solving
    process into distinct planning and execution phases.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“中间步骤未明确定义”的任务，一些研究将 NLP 任务转化为马尔可夫决策过程，以促进代理的工作流程。例如，Hao 等（[2023](#bib.bib7)）将
    QA 任务中的子问题重新表述为行动，使得通过多步骤过程对用户查询作出响应。这种方法允许初始问题作为一系列状态转换的开始。行动可以从在单步 QA 中提供直接、自由格式的答案到策略性地形成子问题，这些子问题使代理通过顺序更新达到全面解决方案。这种方法更贴近于顺序决策过程，适合用于基于规划的代理系统。此外，Wan
    等（[2024](#bib.bib20)）建议“将输出序列拆分为标记可能是一个好的选择”，以系统地定义多步骤 NLIE。进一步地，Yao 等（[2023a](#bib.bib29)）通过将解决问题的过程分为明确的规划和执行阶段，提出了用于创意写作的两步
    NLIE。
- en: 2.3 Tool Environments
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 工具环境
- en: Modern LLM agents are often enhanced with external tools that improve their
    problem-solving capabilities (Inaba et al., [2023](#bib.bib9); Yao et al., [2023b](#bib.bib30)).
    The design and integration of these tools add complexity, requiring careful consideration
    of how LLMs interact not only with the task environments but also with these auxiliary
    tools. Typically, actions in tool environments involve interactions with resources
    that remain unaffected by these interactions. For instance, retrieving data from
    Wikipedia constitutes a "read-only" action, which does not modify the Wikipedia
    database. This feature distinguishes such tool-use actions from those in conventional
    task environments or typical reinforcement learning (RL) settings, where actions
    generally alter the environmental state. Nevertheless, it is important to recognize
    that tool environment can be dynamic that can undergo changes externally. This
    aspect reflects the nature that tools should be considered external environments
    rather than the agent’s internal processes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 LLM 代理通常通过外部工具来增强其问题解决能力（Inaba 等，[2023](#bib.bib9)；Yao 等，[2023b](#bib.bib30)）。这些工具的设计和整合增加了复杂性，需要仔细考虑
    LLM 如何与任务环境以及这些辅助工具进行互动。通常，工具环境中的操作涉及与资源的交互，而这些资源不会受到交互的影响。例如，从 Wikipedia 检索数据是一种“只读”操作，不会修改
    Wikipedia 数据库。这一特征将这种工具使用行为与传统任务环境或典型的强化学习（RL）设置中的行为区分开来，后者的行为通常会改变环境状态。然而，重要的是要认识到工具环境可以是动态的，可能会发生外部变化。这一点反映了工具应被视为外部环境而非代理的内部过程。
- en: Nested NLIE-QA + Tool Environments
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌套的 NLIE-QA + 工具环境
- en: Tool environments are frequently established along with NLIEs to aid in solving
    QA tasks. Shinn et al. ([2023](#bib.bib16)); Yao et al. ([2023b](#bib.bib30))
    incorporate tools to enhance the factuality of responses. They define command-like
    actions such as “Search” and “LookUp” to interact with Wikipedia, with “Search”
    suggesting the top-5 similar entities from the relevant wiki page, and “LookUp”
    simulating the Ctrl+F functionality in a browser. Beyond simple retrieval, Thoppilan
    et al. ([2022](#bib.bib19)) include a language translator and a calculator for
    dialog tasks. Similarly, Inaba et al. ([2023](#bib.bib9)) employ a calculator,
    implemented using the Python eval function, to resolve numerical queries within
    the NumGLUE benchmark.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 工具环境经常与 NLIEs 一起建立，以帮助解决 QA 任务。Shinn 等（[2023](#bib.bib16)）；Yao 等（[2023b](#bib.bib30)）纳入了工具以增强响应的真实性。他们定义了类似“搜索”和“查找”的命令式动作来与
    Wikipedia 互动，其中“搜索”从相关的 wiki 页面中建议前 5 个相似实体，而“查找”模拟浏览器中的 Ctrl+F 功能。除了简单检索，Thoppilan
    等（[2022](#bib.bib19)）还包括语言翻译器和计算器以应对对话任务。同样，Inaba 等（[2023](#bib.bib9)）使用 Python
    eval 函数实现的计算器来解决 NumGLUE 基准中的数值查询。
- en: 3 LLM-Profiled Components
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个 LLM-配置组件
- en: This section explores common agentic roles for which LLMs are typically profiled.
    The components leverage the internal commonsense knowledge and reasoning abilities
    of LLMs to generate actions, plans, estimate values ²²2Values refer to the estimated
    rewards (a quantitative measure of the success or desirability of the outcomes)
    associated with taking a certain action in a state, widely used in typical RL
    and MDP settings to learn policy models that perform desirable behaviors., and
    infer subsequent states.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了 LLM 通常进行角色配置的常见代理角色。这些组件利用 LLM 的内部常识知识和推理能力来生成动作、计划、估算值²²2值是指与在某状态下采取某种行动相关的估算奖励（对结果的成功或期望的定量衡量），广泛应用于典型的
    RL 和 MDP 设置中，以学习执行期望行为的策略模型。以及推断后续状态。
- en: Universal LLM-Profiled Components
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通用 LLM-配置组件
- en: Specifically, the following task-agnostic components are profiled and commonly
    used across various workflows. *1) LLM-Profiled Policy $\text{glm}_{\text{policy}}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，以下任务无关的组件被配置并在各种工作流程中广泛使用。*1）LLM-配置政策 $\text{glm}_{\text{policy}}$。
- en: Task-Dependent LLM-Profiled Components
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务依赖型 LLM-配置组件
- en: In addition to the universal components, certain LLM-profiled components are
    tailored to specific tasks. For instance, verbalizers are crucial in embodied
    environments but unnecessary in NLIEs. A verbalizer translates actions and observations
    into inputs for planners; for example, in the Planner-Actor-Reporter workflow
    (Wang et al., [2023a](#bib.bib22)), a fine-tuned Visual Language Model (VLM) along
    with $\text{glm}_{\text{planner}}$ (Shinn et al., [2023](#bib.bib16)), often guide
    descriptions according to specified criteria.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通用组件外，某些 LLM-配置组件还针对特定任务进行定制。例如，verbalizers 在具身环境中至关重要，但在 NLIEs 中不必要。一个 verbalizer
    将动作和观察转换为规划器的输入；例如，在规划-演员-报告者工作流程（Wang 等，[2023a](#bib.bib22)）中，经过微调的视觉语言模型（VLM）与
    $\text{glm}_{\text{planner}}$（Shinn 等，[2023](#bib.bib16)），通常根据指定的标准指导描述。
- en: '| Workflows | Related Frameworks | LMPCs | Applicable Environments |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 工作流程 | 相关框架 | LMPCs | 适用环境 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Policy-only | Base | LLM Planner (Huang et al., [2022](#bib.bib8)), DEPS
    (Wang et al., [2023b](#bib.bib24)), Planner-Actor-Reporter (Dasgupta et al., [2022](#bib.bib2)),
    Plan-and-solve (Wang et al., [2023a](#bib.bib22)), ReAct (Yao et al., [2023b](#bib.bib30))
    | $\text{glm}_{\text{planner}}$ (Optional for Embodied Env),'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '| 仅政策 | 基础 | LLM 规划器（Huang 等，[2022](#bib.bib8)），DEPS（Wang 等，[2023b](#bib.bib24)），规划-演员-报告者（Dasgupta
    等，[2022](#bib.bib2)），计划与解决（Wang 等，[2023a](#bib.bib22)），ReAct（Yao 等，[2023b](#bib.bib30)）
    | $\text{glm}_{\text{planner}}$（适用于具身环境）,'
- en: $\text{glm}_{\text{verbalizer}}$ (Optional for Embodied Env) | Embodied Env,
    NLIEs-Writing, NLIE-QA |
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{verbalizer}}$（适用于具身环境） | 具身环境，NLIEs-写作，NLIE-QA |
- en: '|  | Tool-Use | MultiTool-CoT (Inaba et al., [2023](#bib.bib9)), ReAct (Yao
    et al., [2023b](#bib.bib30)), Reflexion (Shinn et al., [2023](#bib.bib16)) | $\text{glm}_{\text{actor}}$
    | NLIEs |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | 工具使用 | MultiTool-CoT（Inaba 等，[2023](#bib.bib9)），ReAct（Yao 等，[2023b](#bib.bib30)），Reflexion（Shinn
    等，[2023](#bib.bib16)） | $\text{glm}_{\text{actor}}$ | NLIEs |'
- en: '| Search | Traversal & Heuristic | Tree-of-Thoughts (ToT) (Yao et al., [2023a](#bib.bib29)),
    Tree-BeamSearch (Xie et al., [2023](#bib.bib28)), Boost-of-Thoughts (Chen et al.,
    [2024](#bib.bib1)) | $\text{glm}_{\text{actor}}$ | Gaming, NLIEs-Writing, NLIE-QA
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 搜索 | 遍历与启发式 | Tree-of-Thoughts (ToT) (Yao et al., [2023a](#bib.bib29))，Tree-BeamSearch
    (Xie et al., [2023](#bib.bib28))，Boost-of-Thoughts (Chen et al., [2024](#bib.bib1))
    | $\text{glm}_{\text{actor}}$ | 游戏、NLIE写作、NLIE问答 |'
- en: '|  | MCTS | RAP (Hao et al., [2023](#bib.bib7)), Wan et al. ([2024](#bib.bib20))
    | $\text{glm}_{\text{actor}}$ | Gaming, NLIEs-QA |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | MCTS | RAP (Hao et al., [2023](#bib.bib7))，Wan et al. ([2024](#bib.bib20))
    | $\text{glm}_{\text{actor}}$ | 游戏，NLIE问答 |'
- en: '| Feedback Learning | from $\text{glm}_{\text{eval}}$ | NLIEs-QA |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 反馈学习 | 来自$\text{glm}_{\text{eval}}$ | NLIE问答 |'
- en: '|  | from $\text{glm}_{\text{eval}}$ | Embodied Env |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | 来自$\text{glm}_{\text{eval}}$ | 具身环境 |'
- en: '|  | from Tools & Humans | Guan et al. ([2023](#bib.bib6)) | $\text{glm}_{\text{planner}}$
    | Embodied Env |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 来自工具和人类 | Guan et al. ([2023](#bib.bib6)) | $\text{glm}_{\text{planner}}$
    | 具身环境 |'
- en: '|  | from Tools & $\text{glm}_{\text{eval}}$ | NLIEs |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 来自工具和$\text{glm}_{\text{eval}}$ | NLIE |'
- en: 'Table 2: Workflows of LLM-Based Agents and Related Frameworks. We summarize
    LLM-Profiled Components (LMPCs) and applicable environments based on the original
    papers of the listed frameworks, although other possibilities exist, e.g., using
    $\text{glm}_{\text{planner}}$ within the tool-use workflow and applying the base
    workflow to gaming environments.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基于LLM的代理和相关框架的工作流。我们根据所列框架的原始文献总结了LLM配置组件（LMPCs）和适用环境，尽管还存在其他可能性，例如在工具使用工作流中使用$\text{glm}_{\text{planner}}$，以及将基础工作流应用于游戏环境。
- en: 4 Workflows of LLM-Based Agents
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于LLM的代理的工作流
- en: 'This section explores different workflows and the utilization of various LLM-Profiled
    Components (LMPCs), as illustrated in Figure [1](#S0.F1 "Figure 1 ‣ A Survey on
    LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components"). Specifically,
    we will aggregate proposed frameworks based on workflow types, as demonstrated
    in Table [2](#S3.T2 "Table 2 ‣ Task-Dependent LLM-Profiled Components ‣ 3 LLM-Profiled
    Components ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"). This section delves into diverse workflows and the application of
    distinct LLM-Profiled Components (LMPCs), as visualized in Figure [1](#S0.F1 "Figure
    1 ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components").
    We categorize and consolidate the related frameworks according to the workflows,
    exemplified in Table [2](#S3.T2 "Table 2 ‣ Task-Dependent LLM-Profiled Components
    ‣ 3 LLM-Profiled Components ‣ A Survey on LLM-Based Agents: Common Workflows and
    Reusable LLM-Profiled Components"). It is noteworthy that one framework could
    employ distinct workflows according to tasks or environments. More details of
    the frameworks are summarized in Appendix [A](#A1 "Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了不同的工作流以及各种LLM配置组件（LMPCs）的使用，如图[1](#S0.F1 "图 1 ‣ 基于LLM的代理调查：常见工作流和可重用的LLM配置组件")所示。具体来说，我们将根据工作流类型汇总提议的框架，如表[2](#S3.T2
    "表 2 ‣ 任务依赖型LLM配置组件 ‣ 3 LLM配置组件 ‣ 基于LLM的代理调查：常见工作流和可重用的LLM配置组件")所示。本节深入探讨了多样的工作流和不同LLM配置组件（LMPCs）的应用，如图[1](#S0.F1
    "图 1 ‣ 基于LLM的代理调查：常见工作流和可重用的LLM配置组件")所示。我们根据工作流对相关框架进行分类和整合，如表[2](#S3.T2 "表 2
    ‣ 任务依赖型LLM配置组件 ‣ 3 LLM配置组件 ‣ 基于LLM的代理调查：常见工作流和可重用的LLM配置组件")所示。值得注意的是，一个框架可能根据任务或环境采用不同的工作流。框架的更多详细信息总结在附录[A](#A1
    "附录 A 基于LLM的代理框架 ‣ 基于LLM的代理调查：常见工作流和可重用的LLM配置组件")中。
- en: 4.1 Policy-Only Workflows
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 仅策略工作流
- en: Base and tool-use workflows only require LLMs to be profiled as policy models.
    In the realm of embodied tasks, many projects deploy base workflows with $\text{glm}_{\text{planner}}$
    is always applied to NLIEs like ReAct (Yao et al., [2023b](#bib.bib30)), Reflexion
    (Shinn et al., [2023](#bib.bib16)), and MultiTool-CoT (Inaba et al., [2023](#bib.bib9)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基础和工具使用工作流仅需将LLMs（大语言模型）配置为策略模型。在具身任务领域，许多项目采用基础工作流，其中$\text{glm}_{\text{planner}}$总是应用于NLIE（自然语言推理任务）如ReAct
    (Yao et al., [2023b](#bib.bib30))、Reflexion (Shinn et al., [2023](#bib.bib16))和MultiTool-CoT
    (Inaba et al., [2023](#bib.bib9))。
- en: 4.2 Search Workflows
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 搜索工作流
- en: Unlike base agents with $\text{glm}_{\text{planner}}$, which generates a sequence
    of actions for a plan at one generation, actions in search workflows are organized
    into tree (Yao et al., [2023a](#bib.bib29); Hao et al., [2023](#bib.bib7)) and
    graph (Liu et al., [2023](#bib.bib11)) for exploration. Planning or search algorithms
    can explore sequential decisions in a non-linear manner. During the process, the
    tree (or solution) is constructed by adding nodes, each representing a partial
    solution with the input and the sequence of thoughts/actions so far. Data structures
    such as trees enable strategic searches over actions derived from multiple reasoning
    paths. This is accomplished using algorithms like beam search (Xie et al., [2023](#bib.bib28)),
    depth-first and breadth-first search (DFS and BFS) (Yao et al., [2023a](#bib.bib29)),
    and Monte-Carlo Tree Search (MCTS) (Hao et al., [2023](#bib.bib7)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成一个计划的动作序列的基本代理 $\text{glm}_{\text{planner}}$ 不同，搜索工作流程中的动作被组织成树（Yao et al.,
    [2023a](#bib.bib29); Hao et al., [2023](#bib.bib7)）和图（Liu et al., [2023](#bib.bib11)）进行探索。规划或搜索算法可以以非线性方式探索顺序决策。在此过程中，通过添加节点来构建树（或解决方案），每个节点代表一个部分解决方案，包括输入和迄今为止的思考/行动序列。数据结构如树允许对源自多条推理路径的行动进行战略性搜索。这是通过像束搜索（Xie
    et al., [2023](#bib.bib28)）、深度优先搜索和广度优先搜索（DFS 和 BFS）（Yao et al., [2023a](#bib.bib29)）和蒙特卡罗树搜索（MCTS）（Hao
    et al., [2023](#bib.bib7)）等算法实现的。
- en: Generally, LMPCs are used to explore the path towards the goal. Instead of directly
    applying actions on external environments within policy-only workflows, $\text{glm}_{\text{policy}}$
    is used to calculate values for action/state evaluation during exploration (Yao
    et al., [2023a](#bib.bib29); Chen et al., [2024](#bib.bib1)) or as a reward model
    (Hao et al., [2023](#bib.bib7)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，LMPCs 用于探索实现目标的路径。与其在仅政策的工作流程中直接对外部环境采取行动，不如使用 $\text{glm}_{\text{policy}}$
    在探索期间计算行动/状态评估的值（Yao et al., [2023a](#bib.bib29); Chen et al., [2024](#bib.bib1)），或者作为奖励模型（Hao
    et al., [2023](#bib.bib7)）。
- en: Search via Traversal and Heuristic
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过遍历和启发式搜索
- en: The Tree-of-Thoughts (ToT) workflow (Yao et al., [2023a](#bib.bib29)) uses $\text{glm}_{\text{policy}}$
    most promising nodes. ⁴⁴4Typically, BFS does not use a utility model to decide
    which nodes to expand because it systematically explores all possible nodes at
    each depth.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Tree-of-Thoughts (ToT) 工作流程（Yao et al., [2023a](#bib.bib29)）使用 $\text{glm}_{\text{policy}}$
    中最有前景的节点。⁴⁴4 通常，广度优先搜索（BFS）不会使用效用模型来决定扩展哪些节点，因为它会系统地探索每一层的所有可能节点。
- en: Simulated-Based Search via MCTS
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于模拟的 MCTS 搜索
- en: The RAP workflow (Hao et al., [2023](#bib.bib7)) also builds a tree through
    searching and includes $\text{glm}_{\text{policy}}$, iteratively simulating the
    trajectory.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RAP 工作流程（Hao et al., [2023](#bib.bib7)）也通过搜索构建了一个树，并包含 $\text{glm}_{\text{policy}}$，迭代地模拟轨迹。
- en: 4.3 Feedback-Learning Workflows
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 反馈学习工作流程
- en: 'There are primarily four main sources of feedback: $\text{glm}_{\text{eval}}$
    (internal feedback), humans, task environments, and tools.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 主要有四种反馈来源：$\text{glm}_{\text{eval}}$（内部反馈）、人类、任务环境和工具。
- en: Reflexion (Shinn et al., [2023](#bib.bib16)) and Self-Refine (Madaan et al.,
    [2023](#bib.bib12)) utilize $\text{glm}_{\text{eval}}$, as noted by the workflow
    from Guan et al. ([2023](#bib.bib6)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Reflexion (Shinn et al., [2023](#bib.bib16)) 和 Self-Refine (Madaan et al., [2023](#bib.bib12))
    利用 $\text{glm}_{\text{eval}}$，正如 Guan et al. ([2023](#bib.bib6)) 的工作流程所指出的那样。
- en: '|  | Prompting | Example Works | Example Prompts (in Appendix) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 提示 | 示例工作 | 示例提示（见附录） |'
- en: '| $\text{glm}_{\text{actor}}$ | Few-shot | ReAct (Yao et al., [2023b](#bib.bib30)),
    Reflexion (Shinn et al., [2023](#bib.bib16)), RAP (Hao et al., [2023](#bib.bib7)),
    MultiTool-CoT (Inaba et al., [2023](#bib.bib9)) | Table [8](#A1.T8 "Table 8 ‣
    Appendix A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components"), [10](#A1.T10 "Table 10 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| $\text{glm}_{\text{actor}}$ | 少量示例 | ReAct (Yao et al., [2023b](#bib.bib30))、Reflexion
    (Shinn et al., [2023](#bib.bib16))、RAP (Hao et al., [2023](#bib.bib7))、MultiTool-CoT
    (Inaba et al., [2023](#bib.bib9)) | 表 [8](#A1.T8 "表 8 ‣ 附录 A 基于 LLM 的代理框架 ‣ LLM
    基于代理的调查：常见工作流程和可重用的 LLM-概要组件")、[10](#A1.T10 "表 10 ‣ 附录 A 基于 LLM 的代理框架 ‣ LLM 基于代理的调查：常见工作流程和可重用的
    LLM-概要组件") |'
- en: '| $\text{glm}_{\text{planner}}$ | Zero-shot | Plan-and-Solve (Wang et al.,
    [2023a](#bib.bib22)), LLM Planner (Huang et al., [2022](#bib.bib8)) | Table [6](#A1.T6
    "Table 6 ‣ Appendix A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components") |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| $\text{glm}_{\text{planner}}$ | 零-shot | Plan-and-Solve (Wang et al., [2023a](#bib.bib22)),
    LLM Planner (Huang et al., [2022](#bib.bib8)) | 表 [6](#A1.T6 "Table 6 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") |'
- en: '|  | Few-shot | DEPS (Wang et al., [2023b](#bib.bib24)), Planner-Actor-Reporter
    (Dasgupta et al., [2022](#bib.bib2)) |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 少量示例 | DEPS (Wang et al., [2023b](#bib.bib24)), Planner-Actor-Reporter
    (Dasgupta et al., [2022](#bib.bib2)) |  |'
- en: '| $\text{glm}_{\text{evaluator}}$ | Few-shot | RAP (Hao et al., [2023](#bib.bib7)),
    Tree-BeamSearch (Xie et al., [2023](#bib.bib28)), Reflexion (Shinn et al., [2023](#bib.bib16)),
    CRITIC (Gou et al., [2024](#bib.bib5)) | Table [12](#A2.T12 "Table 12 ‣ B.2 Prompts
    for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components"), [13](#A2.T13 "Table 13
    ‣ B.2 Prompts for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based
    Agents: Common Workflows and Reusable LLM-Profiled Components") |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $\text{glm}_{\text{evaluator}}$ | 少量示例 | RAP (Hao et al., [2023](#bib.bib7)),
    Tree-BeamSearch (Xie et al., [2023](#bib.bib28)), Reflexion (Shinn et al., [2023](#bib.bib16)),
    CRITIC (Gou et al., [2024](#bib.bib5)) | 表 [12](#A2.T12 "Table 12 ‣ B.2 Prompts
    for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components"), [13](#A2.T13 "Table 13
    ‣ B.2 Prompts for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based
    Agents: Common Workflows and Reusable LLM-Profiled Components") |'
- en: '| $\text{glm}_{\text{dynamic}}$ | Few-shot | RAP (Hao et al., [2023](#bib.bib7))
    | Table [16](#A2.T16 "Table 16 ‣ B.3 Prompts as Dynamic Models ‣ Appendix B Examples
    Prompts ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components") |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $\text{glm}_{\text{dynamic}}$ | 少量示例 | RAP (Hao et al., [2023](#bib.bib7))
    | 表 [16](#A2.T16 "Table 16 ‣ B.3 Prompts as Dynamic Models ‣ Appendix B Examples
    Prompts ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components") |'
- en: 'Table 3: Prompting Methods of LLM-Profiled Components'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: LLM-Profiled 组件的提示方法'
- en: '| Task Formulation | Feedback Types | Applicable Workflows | Example Works
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 任务制定 | 反馈类型 | 适用工作流 | 示例工作 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Text Generation | Free-form reflection | Feedback-learning workflows | Self-Refine
    (Madaan et al., [2023](#bib.bib12)), Reflexion (Shinn et al., [2023](#bib.bib16)),
    CRITIC (Gou et al., [2024](#bib.bib5)) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 文本生成 | 自由形式反思 | 反馈学习工作流 | Self-Refine (Madaan et al., [2023](#bib.bib12)),
    Reflexion (Shinn et al., [2023](#bib.bib16)), CRITIC (Gou et al., [2024](#bib.bib5))
    |'
- en: '| Binary/Multi-class Classification | Discrete values | Search workflows |
    RAP (Hao et al., [2023](#bib.bib7)), Tree-BeamSearch (Xie et al., [2023](#bib.bib28))
    ToT (Yao et al., [2023a](#bib.bib29)) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 二分类/多分类 | 离散值 | 搜索工作流 | RAP (Hao et al., [2023](#bib.bib7)), Tree-BeamSearch
    (Xie et al., [2023](#bib.bib28)) ToT (Yao et al., [2023a](#bib.bib29)) |'
- en: '| Binary Classification | Continuous values (logits) | Search workflow via
    MCTS | RAP (Hao et al., [2023](#bib.bib7)) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 二分类 | 连续值（logits） | 通过 MCTS 搜索工作流 | RAP (Hao et al., [2023](#bib.bib7)) |'
- en: '| Multi-choice QA | Choices of top-N actions | Search workflows via traversal
    and heuristic | ToT (Yao et al., [2023a](#bib.bib29)) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 多项选择问答 | 选择前N个动作 | 通过遍历和启发式搜索工作流 | ToT (Yao et al., [2023a](#bib.bib29))
    |'
- en: 'Table 4: Workflow-Specific LLM-Profiled Evaluators According to Task Formulation
    and Feedback Types'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 根据任务制定和反馈类型的工作流特定 LLM-Profiled 评估器'
- en: 5 Implementations of LMPCs
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 LMPC 的实现
- en: In this section, we explore different implementation approaches for LMPCs, covering
    strategies that are independent of specific workflows and tasks, implementations
    specifically designed for certain tasks, and those tailored to particular workflows.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了 LMPC 的不同实现方法，涵盖了与特定工作流和任务无关的策略、专门为某些任务设计的实现以及针对特定工作流量身定制的实现。
- en: 5.1 Universal Implementations
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 通用实现
- en: 'LLMs are normally profiled by leveraging Chain-of-Thought (CoT) prompting to
    transcend basic input-output inference, facilitating the creation of intermediate
    reasoning steps. This is achieved through two techniques: Zero-shot CoT integrates
    a CoT trigger, such as “Let’s think step-by-step” (Kojima et al., [2022](#bib.bib10)),
    within task instructions, while while few-shot CoT incorporates handcrafted reasoning
    steps within provided examples for in-context learning (Wei et al., [2022](#bib.bib25)).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 通常通过利用链式思维（CoT）提示来超越基本的输入输出推理，从而促进中间推理步骤的创建。这通过两种技术实现：零-shot CoT 在任务指令中集成
    CoT 触发器，例如“让我们一步步来想”（Kojima et al., [2022](#bib.bib10)），而少量示例 CoT 则在提供的示例中结合手工制作的推理步骤以进行上下文学习（Wei
    et al., [2022](#bib.bib25)）。
- en: 'As shown in Table [3](#S4.T3 "Table 3 ‣ 4.3 Feedback-Learning Workflows ‣ 4
    Workflows of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components"), some studies (Wang et al., [2023a](#bib.bib22))
    employ zero-shot CoT prompting, but most (Yao et al., [2023b](#bib.bib30); Shinn
    et al., [2023](#bib.bib16); Hao et al., [2023](#bib.bib7)) implement LLM policy
    models via few-shot CoT. Zero-shot CoT implementation of $\text{glm}_{\text{planner}}$
    often fails to produce long-horizon plans, unlike few-shot CoT prompting (Wang
    et al., [2023b](#bib.bib24)). While effective, few-shot prompting requires manual
    compilation of demonstrations with reasoning sequences, leading to increased manual
    work and computational resource use. Methods like Auto CoTs (Zhang et al., [2023](#bib.bib31))
    that automatically generate few-shot demonstrations could mitigate this challenge.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [3](#S4.T3 "Table 3 ‣ 4.3 Feedback-Learning Workflows ‣ 4 Workflows of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components") 所示，一些研究（Wang 等人，[2023a](#bib.bib22)）采用了零样本 CoT 提示，但大多数（Yao 等人，[2023b](#bib.bib30)；Shinn
    等人，[2023](#bib.bib16)；Hao 等人，[2023](#bib.bib7)）通过少样本 CoT 实现 LLM 策略模型。与少样本 CoT
    提示不同，零样本 CoT 实现的 $\text{glm}_{\text{planner}}$ 常常无法产生长时间跨度的计划。虽然有效，但少样本提示需要手动编译包含推理序列的示例，导致手工工作和计算资源的增加。像
    Auto CoTs（Zhang 等人，[2023](#bib.bib31)）这类自动生成少样本示例的方法可能有助于缓解这一挑战。'
- en: 5.2 Workflow-Specific Implementations
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 工作流特定实现
- en: $\text{glm}_{\text{policy}}$ Implementations
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{policy}}$ 实现
- en: Two distinct implementations can be employed to enable $\text{glm}_{\text{policy}}$
    to trigger tool usage within tool-use and feedback learning workflows (receiving
    feedback from tools).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 可以采用两种不同的实现来使得 $\text{glm}_{\text{policy}}$ 在工具使用和反馈学习工作流中触发工具使用（从工具处接收反馈）。
- en: '1.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Using In-Generation Triggers: Tools could be invoked during the reasoning generation
    process, e.g., MultiTool-CoT (Inaba et al., [2023](#bib.bib9)). The agent program
    monitors each token produced, pausing text generation when a tool trigger is detected.
    This pause allows for the invocation of tools, whose outputs are then inserted
    into the prompt to complete the reasoning. The triggers for these tools are defined
    either through tool descriptions, few-shot demonstrations ⁵⁵5In-generation triggers
    via few-shot demos: see an example prompt in Table [15](#A2.T15 "Table 15 ‣ B.2
    Prompts for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based
    Agents: Common Workflows and Reusable LLM-Profiled Components")), or a combination
    of both ⁶⁶6In-generation triggers via descriptions and few-shot demos: see an
    example prompt in Table [8](#A1.T8 "Table 8 ‣ Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components")).'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '使用生成过程中触发：工具可以在推理生成过程中被调用，例如 MultiTool-CoT（Inaba 等人，[2023](#bib.bib9)）。代理程序监控每个生成的标记，当检测到工具触发时，暂停文本生成。这种暂停允许调用工具，然后将工具输出插入到提示中以完成推理。这些工具的触发可以通过工具描述、少样本示例
    ⁵⁵5生成中的触发：参见表 [15](#A2.T15 "Table 15 ‣ B.2 Prompts for LLM Evaluators ‣ Appendix
    B Examples Prompts ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components") 中的示例提示，或两者结合 ⁶⁶6生成中的触发：参见表 [8](#A1.T8 "Table 8 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") 中的示例提示。'
- en: '2.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Reasoning-Acting (ReAct) Strategy for Tool Use: Introduced by Yao et al. ([2023b](#bib.bib30)),
    each reasoning or acting step is separately verbalized via a complete generation.
    Although the ReAct framework (Yao et al., [2023b](#bib.bib30)) unifies tool actions
    within the tool-use workflow and task-specific actions within the base workflow,
    we argue that the strategies for tool actions and task-specific actions should
    be distinguished. Because tool actions and task-specific actions are triggered
    differently, an aspect that will be emphasized further in the subsequent section
    about task-specific implementations.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具使用的推理-行动（ReAct）策略：由 Yao 等人提出 ([2023b](#bib.bib30))，每一步推理或行动都通过完整的生成进行表述。尽管
    ReAct 框架（Yao 等人， [2023b](#bib.bib30)）将工具行为统一在工具使用工作流中，将任务特定行为统一在基础工作流中，但我们认为工具行为和任务特定行为的策略应当区分开来。因为工具行为和任务特定行为的触发方式不同，这一点将在后续关于任务特定实现的部分中进一步强调。
- en: $\text{glm}_{\text{eval}}$ Implementations
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{eval}}$ 实现
- en: 'Commonly, different workflows require distinct feedback types and task formulations.
    There are four scenarios regarding the two perspectives, as summarized in Table
    [4](#S4.T4 "Table 4 ‣ 4.3 Feedback-Learning Workflows ‣ 4 Workflows of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"): 1) Generating free-form reflection: This reflective output is frequently
    integrated into the prompt of $\text{glm}_{\text{policy}}$ is designed to reflect
    on previous states and actions within feedback-learning workflows. Depending on
    specific feedback-learning workflows, it could incorporate external inputs from
    task or tool environments to enrich the reflection process. 2) Binary/Multiclass
    Classification: Feedback is obtained from discrete output tokens, commonly "no"
    or "yes." These can be converted into 0/1 values for search workflows. This scalar
    values can be employed as reward signals within Monte Carlo Tree Search (MCTS)
    simulations (Hao et al., [2023](#bib.bib7)), or they may be employed directly
    to guide decision-making at each step of tree traversal (Yao et al., [2023a](#bib.bib29)).
    3) Binary classification with scalar values: This approach differs from the previous
    one by employing the logit values of tokens to calculate scalar feedback values.
    For instance, the probability of a “yes” response is computed using the formula:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，不同的工作流程需要不同的反馈类型和任务制定。有四种场景涉及这两个视角，如表 [4](#S4.T4 "Table 4 ‣ 4.3 Feedback-Learning
    Workflows ‣ 4 Workflows of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components") 中总结：1) 生成自由形式的反思：这种反思输出通常被整合到
    $\text{glm}_{\text{policy}}$ 的提示中，用于反思反馈学习工作流程中的先前状态和行动。根据具体的反馈学习工作流程，它可能会融入来自任务或工具环境的外部输入，以丰富反思过程。2)
    二分类/多分类：反馈来自离散输出标记，通常是“否”或“是”。这些可以转换为 0/1 值用于搜索工作流程。这些标量值可以在蒙特卡洛树搜索 (MCTS) 模拟中作为奖励信号
    (Hao et al., [2023](#bib.bib7))，或者直接用于引导树遍历的每一步决策 (Yao et al., [2023a](#bib.bib29))。3)
    带标量值的二分类：这种方法与前一种方法不同，通过使用标记的 logit 值来计算标量反馈值。例如，“是”响应的概率使用以下公式计算：'
- en: '|  | $v=\text{probs}_{\text{&quot;yes&quot;}}=\frac{e^{l_{\text{&quot;yes&quot;}}}}{e^{l_{\text{&quot;yes&quot;}}}+e^{l_{\text{&quot;no&quot;}}}},$
    |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $v=\text{probs}_{\text{&quot;yes&quot;}}=\frac{e^{l_{\text{&quot;yes&quot;}}}}{e^{l_{\text{&quot;yes&quot;}}}+e^{l_{\text{&quot;no&quot;}}}},$
    |  |'
- en: 'where $l_{\text{&quot;yes&quot;}}$ are not accessible via black-box LLMs. These
    scalar values can then be utilized as rewards in search workflows for MCTS. 4)
    Multi-choice QA: Employed in settings where a selection from multiple choices
    is required, supporting tasks that involve choosing from top-N possible actions,
    as utilized in search workflows for action selection (Yao et al., [2023a](#bib.bib29)).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l_{\text{&quot;yes&quot;}}$ 通过黑箱 LLM 无法访问。这些标量值可以用作搜索工作流程中的 MCTS 奖励。4) 多选
    QA：在需要从多个选择中做出选择的设置中使用，支持选择从 top-N 可能行动中的任务，正如在行动选择的搜索工作流程中使用的 (Yao et al., [2023a](#bib.bib29))。
- en: 5.3 Task-Specific Implementations
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 特定任务的实现
- en: $\text{glm}_{\text{policy}}$ Implementations
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{policy}}$ 实现
- en: 'The noteworthy details include: 1) Multi-Step Generation: For tasks that inherently
    involve sequential decision-making (e.g., “put a cool tomato in the microwave”),
    post-processing steps are often required for $\text{glm}_{\text{policy}}$ are
    not explicitly implemented to generate plans, it may autonomously formulate plans
    during the reasoning phase before deciding on the current action (Shinn et al.,
    [2023](#bib.bib16); Yao et al., [2023b](#bib.bib30)) ⁹⁹9Implicit Planning: See
    an example prompt in Table [7](#A1.T7 "Table 7 ‣ Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"). These generated plans are maintained as internal states and do not
    serve as communication signals with other components.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '值得注意的细节包括：1) 多步生成：对于本质上涉及顺序决策的任务（例如，“把一个凉的番茄放进微波炉”），通常需要后处理步骤，对于 $\text{glm}_{\text{policy}}$，可能没有显式实现生成计划的功能，但它可能在推理阶段自主制定计划，然后再决定当前行动
    (Shinn et al., [2023](#bib.bib16); Yao et al., [2023b](#bib.bib30)) ⁹⁹9 隐性计划：见表
    [7](#A1.T7 "Table 7 ‣ Appendix A Frameworks of LLM-Based Agents ‣ A Survey on
    LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components") 中的示例提示。这些生成的计划被保持为内部状态，并不作为与其他组件的通信信号。'
- en: ReAct Implementation of $\text{glm}_{\text{actor}}$
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReAct 实现的 $\text{glm}_{\text{actor}}$
- en: 'The ReAct framework (Yao et al., [2023b](#bib.bib30)) unifies tool actions
    within the tool-use workflow and task-specific actions within the base workflow.
    However, the sequence in which reasoning and action outputs alternate is task-dependent.
    For question answering (QA), the generations of reasoning steps and tool actions
    are fixed, with alternating prompts for thinking and acting. ^(10)^(10)10Reasoning-Acting
    Strategy (QA tasks): See an example prompt in Table [10](#A1.T10 "Table 10 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components"). In contrast, for embodied tasks, the decision
    whether to proceed with thinking or acting in the next step is autonomously determined
    by $\text{glm}_{\text{policy}}$ ^(11)^(11)11Reasoning-Acting Strategy (embodied
    tasks): See an example prompt in Table [7](#A1.T7 "Table 7 ‣ Appendix A Frameworks
    of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components").'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct框架（Yao et al., [2023b](#bib.bib30)）在工具使用工作流程和任务特定工作流程中统一了工具操作。然而，推理和行动输出交替的顺序是依赖于任务的。对于问答（QA）任务，推理步骤和工具操作的生成是固定的，交替提示用于思考和行动。^(10)^(10)10推理-行动策略（QA任务）：见表[10](#A1.T10
    "表 10 ‣ 附录 A LLM基于的代理框架 ‣ 基于LLM的代理：常见工作流程和可重用的LLM配置组件")中的示例提示。相反，对于具身任务，是否在下一步中继续思考或行动由$\text{glm}_{\text{policy}}$自主决定。^(11)^(11)11推理-行动策略（具身任务）：见表[7](#A1.T7
    "表 7 ‣ 附录 A LLM基于的代理框架 ‣ 基于LLM的代理：常见工作流程和可重用的LLM配置组件")中的示例提示。
- en: $\text{glm}_{\text{eval}}$ Implementations
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{eval}}$ 实现
- en: '$\text{glm}_{\text{eval}}$ can be configured to assess different task-specific
    perspectives, with specific agentic prompts determining the evaluation criteria.
    Generally, usefulness could be used to evaluate any actions (Hao et al., [2023](#bib.bib7))
    ^(12)^(12)12Evaluation Criteria (Usefulness): See an example prompt in Table [12](#A2.T12
    "Table 12 ‣ B.2 Prompts for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey
    on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components").
    In NLIE-QA scenarios, a common metric is the factuality (truthfulness) of responses
    (Gou et al., [2024](#bib.bib5)) ^(13)^(13)13Evaluation Criteria (Factuality/Truthfulness):
    See example prompts in Table [14](#A2.T14 "Table 14 ‣ B.2 Prompts for LLM Evaluators
    ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") and [15](#A2.T15 "Table 15 ‣ B.2 Prompts
    for LLM Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components").'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{eval}}$ 可以配置以评估不同的任务特定视角，通过具体的代理提示确定评估标准。通常，可以使用实用性来评估任何动作（Hao
    et al., [2023](#bib.bib7)) ^(12)^(12)12评估标准（实用性）：见表[12](#A2.T12 "表 12 ‣ B.2 LLM评估者提示
    ‣ 附录 B 示例提示 ‣ 基于LLM的代理：常见工作流程和可重用的LLM配置组件")中的示例提示。在NLIE-QA场景中，常用的度量是回答的事实性（真实性）（Gou
    et al., [2024](#bib.bib5)) ^(13)^(13)13评估标准（事实性/真实性）：见表[14](#A2.T14 "表 14 ‣ B.2
    LLM评估者提示 ‣ 附录 B 示例提示 ‣ 基于LLM的代理：常见工作流程和可重用的LLM配置组件")和[15](#A2.T15 "表 15 ‣ B.2
    LLM评估者提示 ‣ 附录 B 示例提示 ‣ 基于LLM的代理：常见工作流程和可重用的LLM配置组件")中的示例提示。
- en: 6 Future Work
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来工作
- en: As we delve deeper into LMPCs and agentic workflows, several key directions
    for future research are identified to advance the development of fully autonomous
    agents across various tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们深入探讨LMPCs和代理工作流程，确定了若干未来研究的关键方向，以推动在各种任务中全面自主代理的发展。
- en: Universal Tool Use
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通用工具使用
- en: 'One direction is to move beyond predefined tool use for specific tasks and
    develop strategies that enable LLMs to autonomously determine tool usage based
    on the specific requirements of the task at hand. Another direction is to integrate
    tool use for both policy models and evaluators. In other words, LLMs could reason
    over the use of the tool across various tasks and flexibly jump between different
    roles. The insight of its potential is detailed in Appendix [C](#A3 "Appendix
    C Creation of a Task-Agnostic Tool Environment ‣ A Survey on LLM-Based Agents:
    Common Workflows and Reusable LLM-Profiled Components").'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一个方向是超越预定义的工具使用，为特定任务开发使LLMs能够根据任务的具体要求自主决定工具使用的策略。另一个方向是将工具使用整合到政策模型和评估者中。换句话说，LLMs可以在各种任务中推理工具的使用，并灵活地在不同角色之间跳跃。其潜力的洞见详见附录[C](#A3
    "附录 C 任务无关工具环境的创建 ‣ 基于LLM的代理：常见工作流程和可重用的LLM配置组件")。
- en: Unified Frameworks Across Tasks
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨任务统一框架
- en: 'As detailed in §[5](#S5 "5 Implementations of LMPCs ‣ A Survey on LLM-Based
    Agents: Common Workflows and Reusable LLM-Profiled Components"), despite many
    frameworks are presented with integrated conceptual workflows, many components
    are task-specific. For example, ReAct seeks to harmonize tool actions and task-specific
    actions. However, these workflows and specific implementations of LMPCs are different
    across tasks, such as hardcoded reasoning and acting steps in NLIE-QA versus autonomously
    determined reasoning and acting steps in embodied environments. Similarly, although
    the feedback-learning loop in Reflexion is theoretically unified, in practice,
    external feedback is generated only in embodied environments, not in NLIE-QA.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如§[5](#S5 "5 LMPCs的实现 ‣ 基于LLM的代理调查：常见工作流和可重用的LLM-特征组件")中详细说明，尽管许多框架提供了集成的概念工作流，但许多组件是任务特定的。例如，ReAct试图协调工具操作和任务特定操作。然而，这些工作流和LMPCs的具体实现因任务而异，例如NLIE-QA中的硬编码推理和行动步骤与在具身环境中自主决定的推理和行动步骤。类似地，尽管Reflexion中的反馈学习循环在理论上是统一的，但在实践中，外部反馈仅在具身环境中生成，而在NLIE-QA中则没有。
- en: Reducing Bandwidth
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 减少带宽
- en: 'There are several potential strategies for reducing the bandwidth required
    for LLM inference ^(14)^(14)14Here, bandwidth refers to the volume of information
    processed during a single LLM generation, including using Stochastic $\text{glm}_{\text{actor}}$
    (Details in Appendix [D](#A4 "Appendix D Stochastic "glm"_"actor" ‣ A Survey on
    LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components")).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种潜在策略可以减少LLM推理所需的带宽 ^(14)^(14)14此处，带宽指的是在单次LLM生成过程中处理的信息量，包括使用Stochastic $\text{glm}_{\text{actor}}$（详细信息见附录
    [D](#A4 "附录 D Stochastic "glm"_"actor" ‣ 基于LLM的代理调查：常见工作流和可重用的LLM-特征组件")）。
- en: 7 Conclusion
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This survey provides a summary of common workflows and LLM-Profiled Components
    to encourage the reuse of these components and the expansion of existing workflows
    through the integration of both task-specific LMPCs and non-LLM components. This
    approach aims to foster the development and reproducibility of agentic workflows.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查总结了常见的工作流和LLM-特征组件，以鼓励这些组件的重用，并通过集成任务特定的LMPCs和非LLM组件来扩展现有工作流。这种方法旨在促进代理工作流的发展和可重复性。
- en: Limitations
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'This survey omits discussions on memory design ^(15)^(15)15Appendix [E](#A5
    "Appendix E Memory ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components") provides a brief discussion on memory in LLM-based agents
    and the integration of peripheral components into agentic workflows ^(16)^(16)16These
    are concisely summarized in Appendix [A](#A1 "Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"), as our focus is on the details of common LLM-profiled components
    within agentic workflows to facilitate the implementation of reusable components
    and extensible workflows. This distinctly sets our work apart from other surveys.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查省略了对记忆设计的讨论 ^(15)^(15)15附录 [E](#A5 "附录 E 记忆 ‣ 基于LLM的代理调查：常见工作流和可重用的LLM-特征组件")提供了对LLM基于的代理中的记忆及其与外围组件集成的简要讨论
    ^(16)^(16)16这些在附录 [A](#A1 "附录 A 基于LLM的代理框架 ‣ 基于LLM的代理调查：常见工作流和可重用的LLM-特征组件")中简要总结，因为我们的重点是代理工作流中常见LLM-特征组件的细节，以促进可重用组件和可扩展工作流的实施。这显著将我们的工作与其他调查区分开来。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Chen et al. (2024) Sijia Chen, Baochun Li, and Di Niu. 2024. [Boosting of thoughts:
    Trial-and-error problem solving with large language models](https://openreview.net/forum?id=qBL04XXex6).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2024）Sijia Chen，Baochun Li 和 Di Niu。2024年。[思想的提升：使用大型语言模型进行试错问题解决](https://openreview.net/forum?id=qBL04XXex6)。在*第十二届学习表征国际会议*。
- en: Dasgupta et al. (2022) Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino,
    Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. 2022. [Collaborating with
    language models for embodied reasoning](https://openreview.net/forum?id=YoS-abmWjJc).
    In *Second Workshop on Language and Reinforcement Learning*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasgupta等（2022）Ishita Dasgupta，Christine Kaeser-Chen，Kenneth Marino，Arun Ahuja，Sheila
    Babayan，Felix Hill 和 Rob Fergus。2022年。[与语言模型进行具身推理的协作](https://openreview.net/forum?id=YoS-abmWjJc)。在*第二届语言与强化学习研讨会*。
- en: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.
    [Minedojo: Building open-ended embodied agents with internet-scale knowledge](https://openreview.net/forum?id=rc8o_j8I8PX).
    In *Thirty-sixth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan等（2022）Linxi Fan、Guanzhi Wang、Yunfan Jiang、Ajay Mandlekar、Yuncong Yang、Haoyi
    Zhu、Andrew Tang、De-An Huang、Yuke Zhu 和 Anima Anandkumar。2022年。[Minedojo: 构建具备互联网规模知识的开放式具身体智能体](https://openreview.net/forum?id=rc8o_j8I8PX)。发表于*第三十六届神经信息处理系统会议数据集与基准测试分会*。'
- en: 'Goldsmith (2023) Jonathan Goldsmith. 2023. [Wikipedia: A python library that
    makes it easy to access and parse data from wikipedia](https://pypi.org/project/wikipedia/).
    Python Package Index.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goldsmith（2023）Jonathan Goldsmith。2023年。[Wikipedia: 一个使访问和解析维基百科数据变得简单的Python库](https://pypi.org/project/wikipedia/)。Python包索引。'
- en: 'Gou et al. (2024) Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu
    Yang, Nan Duan, and Weizhu Chen. 2024. [CRITIC: Large language models can self-correct
    with tool-interactive critiquing](https://openreview.net/forum?id=Sx038qxjek).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gou等（2024）Zhibin Gou、Zhihong Shao、Yeyun Gong、Yelong Shen、Yujiu Yang、Nan Duan
    和 Weizhu Chen。2024年。[CRITIC: 大型语言模型通过工具交互批评进行自我修正](https://openreview.net/forum?id=Sx038qxjek)。发表于*第十二届国际学习表征会议*。'
- en: Guan et al. (2023) Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao
    Kambhampati. 2023. [Leveraging pre-trained large language models to construct
    and utilize world models for model-based task planning](https://openreview.net/forum?id=zDbsSscmuj).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan等（2023）Lin Guan、Karthik Valmeekam、Sarath Sreedharan 和 Subbarao Kambhampati。2023年。[利用预训练的大型语言模型构建和使用世界模型进行基于模型的任务规划](https://openreview.net/forum?id=zDbsSscmuj)。发表于*第三十七届神经信息处理系统会议*。
- en: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy
    Wang, and Zhiting Hu. 2023. [Reasoning with language model is planning with world
    model](https://aclanthology.org/2023.emnlp-main.507). In *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173,
    Singapore. Association for Computational Linguistics.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao等（2023）Shibo Hao、Yi Gu、Haodi Ma、Joshua Hong、Zhen Wang、Daisy Wang 和 Zhiting
    Hu。2023年。[用语言模型进行推理等同于用世界模型进行规划](https://aclanthology.org/2023.emnlp-main.507)。发表于*2023年自然语言处理实证方法会议论文集*，第8154–8173页，新加坡。计算语言学协会。
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language models as zero-shot planners: Extracting actionable knowledge for
    embodied agents. In *International Conference on Machine Learning*, pages 9118–9147\.
    PMLR.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2022）Wenlong Huang、Pieter Abbeel、Deepak Pathak 和 Igor Mordatch。2022年。语言模型作为零样本规划者：为具身体智能体提取可操作知识。发表于*国际机器学习会议*，第9118–9147页。PMLR。
- en: 'Inaba et al. (2023) Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao
    Kurohashi. 2023. [MultiTool-CoT: GPT-3 can use multiple external tools with chain
    of thought prompting](https://doi.org/10.18653/v1/2023.acl-short.130). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*, pages 1522–1532, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Inaba等（2023）Tatsuro Inaba、Hirokazu Kiyomaru、Fei Cheng 和 Sadao Kurohashi。2023年。[MultiTool-CoT:
    GPT-3 可以通过链式思维提示使用多个外部工具](https://doi.org/10.18653/v1/2023.acl-short.130)。发表于*第61届计算语言学协会年会（第2卷：短篇论文集）*，第1522–1532页，多伦多，加拿大。计算语言学协会。'
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. [Large language models are zero-shot reasoners](https://openreview.net/forum?id=e2TBb5y0yFf).
    In *Advances in Neural Information Processing Systems*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima等（2022）Takeshi Kojima、Shixiang Shane Gu、Machel Reid、Yutaka Matsuo 和 Yusuke
    Iwasawa。2022年。[大型语言模型是零样本推理者](https://openreview.net/forum?id=e2TBb5y0yFf)。发表于*神经信息处理系统进展*。
- en: 'Liu et al. (2023) Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji
    Zhou, and Yue Zhang. 2023. [Logicot: Logical chain-of-thought instruction-tuning](http://arxiv.org/abs/2305.12147).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等（2023）Hanmeng Liu、Zhiyang Teng、Leyang Cui、Chaoli Zhang、Qiji Zhou 和 Yue
    Zhang。2023年。[Logicot: 逻辑链式思维指令微调](http://arxiv.org/abs/2305.12147)。'
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2023. Self-refine: Iterative refinement with self-feedback. *arXiv preprint
    arXiv:2303.17651*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan等（2023）Aman Madaan、Niket Tandon、Prakhar Gupta、Skyler Hallinan、Luyu Gao、Sarah
    Wiegreffe、Uri Alon、Nouha Dziri、Shrimai Prabhumoye、Yiming Yang 等。2023年。Self-refine:
    自反馈的迭代优化。*arXiv预印本 arXiv:2303.17651*。'
- en: 'Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023.
    Selfcheckgpt: Zero-resource black-box hallucination detection for generative large
    language models. *arXiv preprint arXiv:2303.08896*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Manakul 等 (2023) Potsawee Manakul, Adian Liusie, 和 Mark JF Gales. 2023. Selfcheckgpt:
    零资源黑箱幻觉检测用于生成型大型语言模型。*arXiv 预印本 arXiv:2303.08896*。'
- en: Russell and Norvig (2010) Stuart J Russell and Peter Norvig. 2010. *Artificial
    intelligence a modern approach*. London.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell 和 Norvig (2010) Stuart J Russell 和 Peter Norvig. 2010. *人工智能：一种现代方法*.
    伦敦。
- en: Santurkar et al. (2023) Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo
    Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models
    reflect? *arXiv preprint arXiv:2303.17548*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santurkar 等 (2023) Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee,
    Percy Liang, 和 Tatsunori Hashimoto. 2023. 语言模型反映了谁的意见？ *arXiv 预印本 arXiv:2303.17548*。
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language agents with verbal
    reinforcement learning](http://arxiv.org/abs/2303.11366).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等 (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, 和 Shunyu Yao. 2023. [Reflexion: 语言代理与语言强化学习](http://arxiv.org/abs/2303.11366)。'
- en: 'Shridhar et al. (2021) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2021. [{ALFW}orld: Aligning text
    and embodied environments for interactive learning](https://openreview.net/forum?id=0IOX0YcCdTn).
    In *International Conference on Learning Representations*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar 等 (2021) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht. 2021. [{ALFW}orld: 对齐文本和具身环境以进行互动学习](https://openreview.net/forum?id=0IOX0YcCdTn)。在
    *国际学习表征会议*。'
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    learning: An introduction*. MIT press.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto (2018) Richard S Sutton 和 Andrew G Barto. 2018. *强化学习：导论*. MIT出版社。
- en: 'Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, et al. 2022. Lamda: Language models for dialog applications. *arXiv preprint
    arXiv:2201.08239*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thoppilan 等 (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer,
    Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu
    Du 等. 2022. Lamda: 对话应用的语言模型。*arXiv 预印本 arXiv:2201.08239*。'
- en: Wan et al. (2024) Ziyu Wan, Xidong Feng, Muning Wen, Ying Wen, Weinan Zhang,
    and Jun Wang. 2024. [Alphazero-like tree-search can guide large language model
    decoding and training](https://openreview.net/forum?id=fLO9VaAb3B).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等 (2024) Ziyu Wan, Xidong Feng, Muning Wen, Ying Wen, Weinan Zhang, 和 Jun
    Wang. 2024. [类似 AlphaZero 的树搜索可以指导大型语言模型的解码和训练](https://openreview.net/forum?id=fLO9VaAb3B)。
- en: Wang et al. (2024) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on
    large language model based autonomous agents. *Frontiers of Computer Science*,
    18(6):1–26.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2024) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin 等. 2024. 关于基于大型语言模型的自主代理的调查。*计算机科学前沿*,
    18(6):1–26。
- en: 'Wang et al. (2023a) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. [Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models](https://doi.org/10.18653/v1/2023.acl-long.147).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 2609–2634, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2023a) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy
    Ka-Wei Lee, 和 Ee-Peng Lim. 2023a. [计划与解决提示：通过大型语言模型改进零样本链式推理](https://doi.org/10.18653/v1/2023.acl-long.147)。在
    *第61届计算语言学协会年会（第1卷：长篇论文）会议记录*，页码 2609–2634，多伦多，加拿大。计算语言学协会。
- en: 'Wang et al. (2022) Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu
    Wei, Zhumin Chen, and Nan Duan. 2022. [From lsat: The progress and challenges
    of complex reasoning](https://doi.org/10.1109/TASLP.2022.3164218). *IEEE/ACM Trans.
    Audio, Speech and Lang. Proc.*, 30:2201–2216.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2022) Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei,
    Zhumin Chen, 和 Nan Duan. 2022. [从 LSAT: 复杂推理的进展与挑战](https://doi.org/10.1109/TASLP.2022.3164218)。*IEEE/ACM
    音频、语音与语言处理汇刊*, 30:2201–2216。'
- en: 'Wang et al. (2023b) Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. 2023b. [Describe, explain, plan and select: Interactive planning
    with LLMs enables open-world multi-task agents](https://openreview.net/forum?id=KtvPdGb31Z).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2023b) Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma,
    和 Yitao Liang. 2023b. [描述、解释、规划和选择：与 LLM 互动规划实现开放世界多任务代理](https://openreview.net/forum?id=KtvPdGb31Z)。在
    *第三十七届神经信息处理系统会议*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. [Chain of thought
    prompting elicits reasoning in large language models](https://openreview.net/forum?id=_VjQlMeSB_J).
    In *Advances in Neural Information Processing Systems*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter,
    Fei Xia, Ed H. Chi, Quoc V Le, 和 Denny Zhou. 2022. [链式思维提示激发大语言模型的推理](https://openreview.net/forum?id=_VjQlMeSB_J)。在
    *神经信息处理系统进展*。
- en: Willard and Louf (2023) Brandon T Willard and Rémi Louf. 2023. Efficient guided
    generation for llms. *arXiv preprint arXiv:2307.09702*.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Willard 和 Louf (2023) Brandon T Willard 和 Rémi Louf. 2023. 高效引导生成 LLM。*arXiv
    预印本 arXiv:2307.09702*。
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. *arXiv preprint
    arXiv:2308.08155*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. 2023. Autogen：通过多代理对话框架实现下一代
    LLM 应用。*arXiv 预印本 arXiv:2308.08155*。
- en: Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan,
    Junxian He, and Qizhe Xie. 2023. [Self-evaluation guided beam search for reasoning](https://openreview.net/forum?id=Bw82hwg5Q3).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等 (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian
    He, 和 Qizhe Xie. 2023. [自我评估引导的束搜索推理](https://openreview.net/forum?id=Bw82hwg5Q3)。在
    *第三十七届神经信息处理系统会议*。
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. [Tree of thoughts: Deliberate
    problem solving with large language models](http://arxiv.org/abs/2305.10601).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 2023a. [思想树：与大语言模型进行深思熟虑的问题解决](http://arxiv.org/abs/2305.10601)。
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. 2023b. [React: Synergizing reasoning and acting
    in language models](https://openreview.net/forum?id=WE_vluYUL-X). In *The Eleventh
    International Conference on Learning Representations*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    R Narasimhan, 和 Yuan Cao. 2023b. [React：在语言模型中协同推理与行动](https://openreview.net/forum?id=WE_vluYUL-X)。在
    *第十一届国际学习表征会议*。
- en: Zhang et al. (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023.
    Automatic chain of thought prompting in large language models. In *The Eleventh
    International Conference on Learning Representations (ICLR 2023)*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, 和 Alex Smola. 2023. 大语言模型中的自动链式思维提示。在
    *第十一届国际学习表征会议 (ICLR 2023)*。
- en: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric
    benchmark for evaluating foundation models. *arXiv preprint arXiv:2304.06364*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等 (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu,
    Yanlin Wang, Amin Saied, Weizhu Chen, 和 Nan Duan. 2023. Agieval: 一个以人为本的基准，用于评估基础模型。*arXiv
    预印本 arXiv:2304.06364*。'
- en: 'Zhong et al. (2022) Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo,
    Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2022. [Analytical
    reasoning of text](https://doi.org/10.18653/v1/2022.findings-naacl.177). In *Findings
    of the Association for Computational Linguistics: NAACL 2022*, pages 2306–2319,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等 (2022) Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining
    Chen, Jiahai Wang, Jian Yin, Ming Zhou, 和 Nan Duan. 2022. [文本的分析推理](https://doi.org/10.18653/v1/2022.findings-naacl.177)。在
    *计算语言学协会发现：NAACL 2022*，第 2306–2319 页，西雅图，美国。计算语言学协会。
- en: Appendix A Frameworks of LLM-Based Agents
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A LLM 基础代理框架
- en: 'Table [5](#A1.T5 "Table 5 ‣ Appendix A Frameworks of LLM-Based Agents ‣ A Survey
    on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components") demonstrates
    the workflow(s), LMPCs, and non-LMPC components of each framework.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表[5](#A1.T5 "表 5 ‣ 附录 A LLM 基于代理的框架 ‣ 关于 LLM 基于代理的调查：常见工作流和可重用的 LLM 配置组件")展示了每个框架的工作流、LMPCs
    和非 LMPC 组件。
- en: '|  | Involved Workflows | LMPCs | Non-LMPC Components | Applied Environments
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | 涉及的工作流 | LMPCs | 非LMPC组件 | 应用环境 |'
- en: '| Tree-of-Thoughts (ToT) (Yao et al., [2023a](#bib.bib29)) | Search via Traversal
    & Heuristic | $\text{glm}_{\text{actor}}$ (only for NLIEs-Writing) | Search Tree
    | Gaming; NLIEs-Writing |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 思维树（ToT）（姚等，[2023a](#bib.bib29)） | 通过遍历与启发式搜索 | $\text{glm}_{\text{actor}}$（仅限NLIEs-写作）
    | 搜索树 | 游戏；NLIEs-写作 |'
- en: '| Tree-BeamSearch (Xie et al., [2023](#bib.bib28)) | Search-based via Traversal
    & Heuristic | $\text{glm}_{\text{actor}}$ | Search Tree | NLIE-QA |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 树束搜索（谢等，[2023](#bib.bib28)） | 基于搜索的遍历与启发式 | $\text{glm}_{\text{actor}}$ |
    搜索树 | NLIE-QA |'
- en: '| RAP (Hao et al., [2023](#bib.bib7)) | Search via MCTS | $\text{glm}_{\text{actor}}$
    | Search Tree | Gaming; NLIEs-QA |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| RAP（郝等，[2023](#bib.bib7)） | 通过MCTS搜索 | $\text{glm}_{\text{actor}}$ | 搜索树
    | 游戏；NLIEs-QA |'
- en: '| LLM Planner (Huang et al., [2022](#bib.bib8)) | Base | $\text{glm}_{\text{planner}}$
    | MLM for action translation | Embodied Env |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| LLM 计划者（黄等，[2022](#bib.bib8)） | 基础 | $\text{glm}_{\text{planner}}$ | MLM用于动作翻译
    | 具身环境 |'
- en: '| DEPS (Wang et al., [2023b](#bib.bib24)) | Base | $\text{glm}_{\text{planner}}$
    | Immediate actor, VLM+GLM as verbalizer | Embodied Env |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| DEPS（王等，[2023b](#bib.bib24)） | 基础 | $\text{glm}_{\text{planner}}$ | 直接执行者，VLM+GLM作为表达器
    | 具身环境 |'
- en: '| Planner-Actor-Reporter (Dasgupta et al., [2022](#bib.bib2)) | Base | $\text{glm}_{\text{planner}}$
    | RL actor, Trained classifier+Hard code as verbalizer | Embodied Env |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 计划者-执行者-报告者（Dasgupta等，[2022](#bib.bib2)） | 基础 | $\text{glm}_{\text{planner}}$
    | RL 执行者，训练分类器+硬编码作为表达器 | 具身环境 |'
- en: '| Plan-and-solve (Wang et al., [2023a](#bib.bib22)) | Base | $\text{glm}_{\text{planner}}$
    | / | NLIEs-QA |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 计划并解决（王等，[2023a](#bib.bib22)） | 基础 | $\text{glm}_{\text{planner}}$ | / |
    NLIEs-QA |'
- en: '| MultiTool-CoT (Inaba et al., [2023](#bib.bib9)) | Tool-Use | $\text{glm}_{\text{actor}}$
    | / | NLIEs |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 多工具-CoT（稻场等，[2023](#bib.bib9)） | 工具使用 | $\text{glm}_{\text{actor}}$ | / |
    NLIEs |'
- en: '| ReAct (Yao et al., [2023b](#bib.bib30)) | Tool-Use | $\text{glm}_{\text{actor}}$
    | / | NLIEs |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ReAct（姚等，[2023b](#bib.bib30)） | 工具使用 | $\text{glm}_{\text{actor}}$ | / |
    NLIEs |'
- en: '|  | Base | $\text{glm}_{\text{actor}}$ | / | Embodied Env |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | 基础 | $\text{glm}_{\text{actor}}$ | / | 具身环境 |'
- en: '| Guan et al. ([2023](#bib.bib6)) | Feedback Learning (from Tools & Humans)
    | $\text{glm}_{\text{planner}}$ | Domain Experts, Domain-independent Planner |
    Embodied Env |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 关等（[2023](#bib.bib6)） | 反馈学习（来自工具和人类） | $\text{glm}_{\text{planner}}$ | 领域专家，领域无关计划者
    | 具身环境 |'
- en: '| CRITIC (Gou et al., [2024](#bib.bib5)) | Feedback Learning (from Tool & $\text{glm}_{\text{eval}}$
    | / | NLIEs |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| CRITIC（沟等，[2024](#bib.bib5)） | 反馈学习（来自工具和$\text{glm}_{\text{eval}}$ | / |
    NLIEs |'
- en: '| Self-refine (Madaan et al., [2023](#bib.bib12)) | Feedback Learning (from
    $\text{glm}_{\text{eval}}$, | / | NLIEs |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 自我精炼（马丹等，[2023](#bib.bib12)） | 反馈学习（来自$\text{glm}_{\text{eval}}$， | / | NLIEs
    |'
- en: '| Reflexion (Shinn et al., [2023](#bib.bib16)) | Feedback Learning (from $\text{glm}_{\text{eval}}$
    ),'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '| 反射（Shinn等，[2023](#bib.bib16)） | 反馈学习（来自$\text{glm}_{\text{eval}}$），'
- en: Tool-Use | $\text{glm}_{\text{actor}}$, | / | NLIEs-QA |
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 工具使用 | $\text{glm}_{\text{actor}}$ | / | NLIEs-QA |
- en: '|  | Feedback Learning (from Task Env & $\text{glm}_{\text{eval}}$ | Embodied
    Env |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 反馈学习（来自任务环境和$\text{glm}_{\text{eval}}$ | 具身环境 |'
- en: 'Table 5: Frameworks for LLM-based Agents. MLM: Masked Language Model.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：LLM 基于代理的框架。MLM：掩码语言模型。
- en: '| Q: In a dance class of 20 students, $20\%$ of the remaining enrolled in jazz
    dance, and the rest enrolled in hip-hop dance. What percentage of the entire students
    enrolled in hip-hop dance? A: Let’s first understand the problem and devise a
    plan to solve the problem.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '| 问：在一个20名学生的舞蹈班上，$20\%$的剩余学生报名了爵士舞，其余学生报名了嘻哈舞。整个班级中有多少百分比的学生报名了嘻哈舞？答：首先让我们理解问题并制定解决问题的计划。'
- en: Then, let’s carry out the plan and solve the problem step by step.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们实施计划，逐步解决问题。
- en: 'Plan: Step 1: Calculate the total number of students who enrolled in
    contemporary and jazz dance.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 计划：第一步：计算报名了当代舞和爵士舞的学生总数。
- en: 'Step 2: Calculate the total number of students who enrolled in hiphop dance.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步：计算报名了嘻哈舞的学生总数。
- en: 'Step 3: Calculate the percentage of students who enrolled in hiphop dance.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步：计算报名嘻哈舞的学生百分比。
- en: 'Solution:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：
- en: 'Step 1: $20\%$ of the remaining 16 students is 4 students. So, a total of 8
    students enrolled in contemporary and jazz dance.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步：剩下的16名学生中$20\%$是4名学生。因此，共有8名学生报名了当代舞和爵士舞。
- en: 'Step 2: The remaining 12 students enrolled in hip-hop dance.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步：剩下的12名学生报名了嘻哈舞。
- en: 'Step 3: The percentage of students who enrolled in hip-hop dance is $12/20=60\%$.
    Therefore, the answer (arabic numerals) is 60% |'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步：报名嘻哈舞的学生百分比是$12/20=60\%$。因此，答案（阿拉伯数字）是60% |
- en: 'Table 6: An LLM Planner under NLIE-QA, implemented by the zero-shot CoT method.
    The CoT trigger is highlighted in red font. The content in shadow box is generated
    by LLM. Source from Wang et al. ([2023a](#bib.bib22))'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：一个在NLIE-QA下的LLM计划，由零样本CoT方法实现。CoT触发器以红色字体突出显示。阴影框中的内容由LLM生成。来源于王等人 ([2023a](#bib.bib22))
- en: '| Interact with a household to solve a task. Here are two examples. You are
    in the middle of a room. Looking quickly around you, you see a cabinet 9, a cabinet
    8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet
    2, a cabinet 1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop
    1, a drawer 9, a drawer 8, a drawer 7, a drawer 6, a drawer 5, a drawer 4, a drawer
    3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf
    3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner
    2, a stoveburner 1, and a toaster 1\. Your task is to: put a cool mug in shelf.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '| 与家庭互动以完成任务。这里有两个例子。你站在房间的中间。快速环顾四周，你看到一个柜子9，一个柜子8，一个柜子7，一个柜子6，一个柜子5，一个柜子4，一个柜子3，一个柜子2，一个柜子1，一个咖啡机1，一个台面3，一个台面2，一个台面1，一个抽屉9，一个抽屉8，一个抽屉7，一个抽屉6，一个抽屉5，一个抽屉4，一个抽屉3，一个抽屉2，一个抽屉1，一个冰箱1，一个垃圾桶1，一个微波炉1，一个架子3，一个架子2，一个架子1，一个洗涤槽1，一个炉灶1，炉灶2，炉灶3，炉灶4，以及一个烤面包机1\。你的任务是：把一个冷却的杯子放到架子上。'
- en: 'think: To solve the task, I need to find and take a mug, then cool it with
    fridge, then put it in shelf.'
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 思考：要完成这个任务，我需要找一个杯子，然后用冰箱冷却它，然后把它放到架子上。
- en: OK.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
- en: 'think: First I need to find a mug. A mug is more likely to appear in countertop
    (1-3), coffeemachine (1), cabinet (1-9), shelf (1-3), drawer (1-9). I can check
    one by one, starting with countertop 1\.'
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 思考：首先我需要找一个杯子。一个杯子更有可能出现在台面（1-3），咖啡机（1），柜子（1-9），架子（1-3），抽屉（1-9）。我可以逐一检查，从台面1开始。
- en: OK.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。
- en: go to countertop 1
  id: totrans-210
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 前往台面1
- en: On the countertop 1, you see a knife 1, a pan 2, a pan 1, and a plate 1\.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在台面1上，你看到一个刀1，一个锅2，一个锅1，还有一个盘子1\。
- en: …
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'think: Now I cool the mug. Next, I need to put it in/on shelf 1. OK.'
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 思考：现在我把杯子冷却。接下来，我需要把它放到架子1上。好的。
- en: go to shelf 1
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 前往架子1
- en: On the shelf 1, you see a cup 2, a cup 1, a peppershaker 1, and a saltshaker
    1\.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在架子1上，你看到一个杯子2，一个杯子1，一个胡椒瓶1，以及一个盐瓶1\。
- en: put mug 3 in/on shelf 1
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 把杯子3放到架子1上
- en: 'You put the mug 3 in/on the shelf 1\. Here is the task: You are in the middle
    of a room. Looking quickly around you, you see a cabinet 6, a cabinet 5, a cabinet
    4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 3, a
    countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1,
    a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1,
    a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster
    1\.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你把杯子 3 放在/放到架子 1 上。任务是：你站在房间中间。快速环顾四周，你看到一个橱柜 6、一个橱柜 5、一个橱柜 4、一个橱柜 3、一个橱柜 2、一个橱柜
    1、一个咖啡机 1、一个台面 3、一个台面 2、一个台面 1、一个抽屉 3、一个抽屉 2、一个抽屉 1、一个冰箱 1、一个垃圾桶 1、一个微波炉 1、一个架子
    3、一个架子 2、一个架子 1、一个水槽盆 1、一个炉灶 4、一个炉灶 3、一个炉灶 2、一个炉灶 1 和一个烤面包机 1。
- en: 'Your task is to: put a cool tomato in microwave.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是：把一个冷却的西红柿放进微波炉里。
- en: 'think: To solve the task, I need to find a tomato, then cool it
    with the fridge, and finally put it in the microwave.
    OK.'
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 思考：为了解决这个任务，我需要找一个西红柿，然后用冰箱冷却它，最后把它放进微波炉里。
    好的。
- en: 'think: First, I need to find a tomato. A tomato is more likely
    to be found on the countertop (1-3), cabinet (1-6), drawer (1-3), shelf (1-3),
    or the sinkbasin. I can start by checking the countertop 1.
    OK. > go to countertop 1 On the countertop
    1, you see a dishsponge 1, and a mug 2. … |'
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 思考：首先，我需要找一个西红柿。西红柿更可能在台面（1-3）、橱柜（1-6）、抽屉（1-3）、架子（1-3）或水槽盆中找到。我可以从检查台面
    1 开始。 好的。 > 前往台面 1 在台面 1 上，你看到一个洗碗海绵 1 和一个杯子
    2。… |
- en: 'Table 7: An LLM Actor within Base Workflow under the Alfred World Benchmark,
    implemented by the few-shot CoT method. The content in the shadow box is generated
    by LLM. Source from (Yao et al., [2023b](#bib.bib30)).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在 Alfred World Benchmark 下的基础工作流中的 LLM Actor，由少量示例 CoT 方法实现。阴影框中的内容由 LLM
    生成。来源于 (Yao et al., [2023b](#bib.bib30))。
- en: '| Use the following tools at your disposal and answer some questions: |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 使用以下工具并回答一些问题： |'
- en: '| - <> |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| - <> |'
- en: '| - <> |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| - <> |'
- en: '| - <> |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| - <> |'
- en: '| Q: Find the amount of $\mathrm{H}2\mathrm{CO}3$. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Q: 找出 $\mathrm{H}2\mathrm{CO}3$ 的量。 |'
- en: '| A: First, use the chemical reaction predictor to determine the chemical equation
    for the reaction. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| A: 首先，使用化学反应预测器确定反应的化学方程式。 |'
- en: '| Reactants: $2\mathrm{CO}2,2\mathrm{H}2\mathrm{O}$ |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 反应物：$2\mathrm{CO}2,2\mathrm{H}2\mathrm{O}$ |'
- en: '| Products: ?$\mathrm{H}2\mathrm{CO}3$ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 产品：?$ \mathrm{H}2\mathrm{CO}3 $ |'
- en: '| <> |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| <> |'
- en: '| $2\mathrm{CO}2+2\mathrm{H}2\mathrm{O}\rightarrow 2\mathrm{H}2\mathrm{CO}3$
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| $2\mathrm{CO}2+2\mathrm{H}2\mathrm{O}\rightarrow 2\mathrm{H}2\mathrm{CO}3$
    |'
- en: '| Second, use the molar mass list to determine the molar mass of $\mathrm{H}2\mathrm{CO}3$
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 其次，使用摩尔质量列表确定$\mathrm{H}2\mathrm{CO}3$的摩尔质量 |'
- en: '| Third, use the calculator to calculate the amount of 2 moles of $\mathrm{H}2\mathrm{CO}3$
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 第三，使用计算器计算2摩尔$\mathrm{H}2\mathrm{CO}3$的量 |'
- en: '| Therefore, $124\mathrm{~{}g}$ is formed. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 因此，形成了$124\mathrm{~{}g}$。 |'
- en: '| Q: Find the amount of Calcium hydroxide that is required to react with 2
    moles of Carbon dioxide to form 2 moles of Calcium carbonate along with 2 moles
    of Water. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Q: 找出反应中需要多少氢氧化钙与2摩尔二氧化碳反应，以生成2摩尔碳酸钙和2摩尔水。 |'
- en: '| A: |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| A: |'
- en: 'Table 8: An LLM actor with tool use implemented by MultiTool-CoT (Inaba et al.,
    [2023](#bib.bib9)).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: 由MultiTool-CoT（Inaba等，[2023](#bib.bib9)）实现工具使用的LLM演员。'
- en: '| Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages
    and today, she read twice as many pages as yesterday. If she wants to read half
    of the remaining pages tomorrow, how many pages should she read? A: Julie read
    twice as many pages as yesterday, so she read 12 * 2 = 24 pages today. Since yesterday,
    Julie read 12 + 24 = 36 pages. So, there are 120 - 36 = 84 pages left to be read.
    Since she wants to read half of the remaining pages, she should read 84 / 2 =
    42 pages. The answer is 42.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '| Q: 珍妮正在读一本120页的书。昨天，她读了12页，今天，她读的页数是昨天的两倍。如果她想在明天读剩余页数的一半，她应该读多少页？A: 珍妮今天读的页数是昨天的两倍，所以她今天读了12
    * 2 = 24页。自昨天以来，珍妮读了12 + 24 = 36页。因此，还剩120 - 36 = 84页未读。由于她想读剩余页数的一半，她应该读84 /
    2 = 42页。答案是42。'
- en: 'Q: Ken created a care package to send to his brother, who was away at boarding
    school. Ken placed a box on a scale, and then he poured into the box enough jelly
    beans to bring the weight to 2 pounds. Then, he added enough brownies to cause
    the weight to triple. Next, he added another 2 pounds of jelly beans. And finally,
    he added enough gummy worms to double the weight once again. What was the final
    weight of the box of goodies, in pounds?'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 肯为他在寄宿学校的弟弟准备了一个保健包。肯把一个箱子放在秤上，然后往箱子里倒入足够的糖果，使重量达到2磅。接着，他添加了足够的布朗尼，使重量增加到三倍。然后，他又添加了2磅的糖果。最后，他添加了足够的橡皮糖，使重量再次翻倍。最终的礼品箱重量是多少磅？'
- en: 'A: Ken poured jelly beans into the box until the weight was 2 pounds, so the
    weight of the box was 2 pounds at first. Then Ken added enough brownies to cause
    the weight to triple, so the weight of the box was 2 * 3 = 6 pounds. After Ken
    added another 2 pounds of jelly beans, the weight of the box was 6 + 2 = 8 pounds.
    Finally, he added enough gummy worms to double the weight once again, so the weight
    of the box was 8 * 2 = 16 pounds. The answer is 16.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 肯把糖果倒进箱子里，直到重量达到2磅，因此箱子的初始重量是2磅。然后，肯添加了足够的布朗尼，使重量增加到三倍，因此箱子的重量是2 * 3 = 6磅。在此之后，肯又添加了2磅的糖果，箱子的重量变成了6
    + 2 = 8磅。最后，他添加了足够的橡皮糖，使重量再次翻倍，所以箱子的重量变成了8 * 2 = 16磅。答案是16。'
- en: 'Q: James writes a 3-page letter to 2 different friends twice a week. How many
    pages does he write a year?'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 詹姆斯每周给2个不同的朋友写3页信。他每年写多少页？'
- en: 'A: James writes a 3-page letter to 2 different friends twice a week, so he
    writes 3 * 2 * 2 = 12 pages every week. There are 52 weeks in a year, so he writes
    12 * 52 = 624 pages a year. The answer is 624.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 詹姆斯每周给2个不同的朋友写3页信，每周写的总页数是3 * 2 * 2 = 12页。每年有52周，所以他每年写12 * 52 = 624页。答案是624。'
- en: 'Q: Natalia sold clips to 48 of her friends in April, and then she sold half
    as many clips in May. How many clips did Natalia sell altogether in April and
    May?'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 娜塔莉亚在4月卖出了48个夹子，然后在5月卖出了一半的夹子。娜塔莉亚在4月和5月总共卖出了多少个夹子？'
- en: 'A: Natalia sold 48 clips in April and half as many clips in May, so she sold
    48 / 2 = 24 clips in May. Altogether, she sold 48 + 24 = 72 clips. The answer
    is 72.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 娜塔莉亚在4月卖出了48个夹子，而5月卖出了一半的夹子，因此她在5月卖出了48 / 2 = 24个夹子。总共，她卖出了48 + 24 = 72个夹子。答案是72。'
- en: 'Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning
    and bakes muffins for her friends every day with four. She sells the remainder
    at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does
    she make every day at the farmers’ market?'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 珍妮特的鸭子每天下16个蛋。她每天早上吃三个作为早餐，每天用四个做蛋糕给朋友。她将剩下的在农贸市场以每个新鲜鸭蛋2美元的价格出售。她每天在农贸市场赚了多少钱？'
- en: 'A:" Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four, so she uses
    3 + 4 = <>7 eggs daily. Thus, she sells 16 - 7 = <>9 eggs at the
    farmers’ market daily. Therefore, she makes 9 * $2 = $<>18 every day at
    the farmers’ market. Answer: 18. |'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: A:" 珍妮特的鸭子每天产 16 个鸡蛋。她每天早上吃 3 个鸡蛋，并且每天用 4 个鸡蛋为她的朋友们烤松饼，因此她每天使用 3 +
    4 = <>7 个鸡蛋。因此，她每天在农贸市场上卖 16 - 7 = <>9 个鸡蛋。因此，她每天在农贸市场上赚取 9 * $2
    = $<>18。答案：18。 |
- en: 'Table 9: An LLM Actor within Tool-Use Workflow under NLIE-QA, implemented by
    the few-Shot CoT method. The content in the shadow box is generated by LLM.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：在 NLIE-QA 下，通过少样本 CoT 方法实现的工具使用工作流中的 LLM 扮演者。阴影框中的内容由 LLM 生成。 |
- en: '| Solve a question answering task by having a Thought, then Finish with your
    answer. Thought can reason about the current situation. Finish[answer] returns
    the answer and finishes the task. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 通过进行思考来解决问题回答任务，然后用你的答案完成。思考可以推理当前情况。完成[答案] 返回答案并结束任务。 |'
- en: '| Here are some examples: |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 这里是一些示例： |'
- en: '| Question 1: What is the elevation range for the area that the eastern sector
    of the Colorado orogeny extends into? |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1：科罗拉多造山带东部地区的海拔范围是多少？ |'
- en: '| Thought: Let’s think step by step. The eastern sector of Colorado orogeny
    extends into the High Plains. High Plains rise in elevation from around 1,800
    to 7,000 ft, so the answer is 1,800 to 7,000 ft. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 思考：我们一步一步来思考。科罗拉多造山带东部地区延伸到高原。高原的海拔范围从大约 1,800 英尺升高到 7,000 英尺，所以答案是 1,800
    到 7,000 英尺。 |'
- en: '| Action: Finish[1,800 to 7,000 ft] |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 行动：完成[1,800 到 7,000 英尺] |'
- en: '| … |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question 6: Were Pavel Urysohn and Leonid Levin known for the same type of
    work? |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 问题 6：帕维尔·乌里索恩和列昂尼德·列文以相同类型的工作闻名吗？ |'
- en: '| Thought: Let’s think step by step. Pavel Urysohn is a mathematician. Leonid
    Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin
    have the same type of work. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 思考：我们一步一步来思考。帕维尔·乌里索恩是一名数学家。列昂尼德·列文是一名数学家和计算机科学家。所以帕维尔·乌里索恩和列昂尼德·列文有相同类型的工作。
    |'
- en: '| Action: Finish[Yes] |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 行动：完成[是] |'
- en: '| END OF EXAMPLES) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 示例结束) |'
- en: '| Question: VIVA Media AG changed itś name in 2004\. What does their new acronym
    stand for? |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 问题：VIVA Media AG 在 2004 年更改了其名称。它们的新缩写代表什么？ |'
- en: '| Thought: Let’s think step by step. VIVA Media AG changed its name in 2004\.
    The new acronym must stand for the new name they adopted. To find the answer,
    we need to research the new name of VIVA Media AG after 2004.Action: Finish[Further
    research is needed to determine the new acronym.]
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 思考：我们一步一步来思考。VIVA Media AG 在 2004 年更改了其名称。新的缩写必须代表他们所采用的新名称。要找到答案，我们需要研究
    VIVA Media AG 在 2004 年之后的新名称。行动：完成[进一步的研究是确定新缩写所需的。]
    |'
- en: '| Action: Finish[Further research is needed to determine the new acronym.]
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 行动: 完成[需要进一步研究以确定新的缩写。] |'
- en: 'Table 10: An LLM Actor within tool-use workflow for the HotPot-QA Benchmark,
    implemented by few-Shot CoT. The content in the shadow box is generated by LLM.
    Source from Yao et al. ([2023b](#bib.bib30)).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 在HotPot-QA基准下的工具使用工作流程中的LLM角色，由少样本CoT实现。阴影框中的内容由LLM生成。来源于Yao等人（[2023b](#bib.bib30)）。'
- en: '| Given a question, please decompose it into sub-questions. For each sub-question,
    please answer it in a complete sentence, ending with "The answer is". When the
    original question is answerable, please start the subquestion with "Now we can
    answer the question:" |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 给定一个问题，请将其分解为子问题。对每个子问题，请用完整的句子回答，并以“答案是”结束。当原问题可以回答时，请以“现在我们可以回答这个问题:”开始子问题。
    |'
- en: '| Question 1: James writes a 3-page letter to 2 different friends twice a week.
    How many pages does he write a year? |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1: 詹姆斯每周写2封3页的信给2个不同的朋友。他一年写多少页？ |'
- en: '| Question 1.1: How many pages does he write every week? |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.1: 他每周写多少页？ |'
- en: '| Answer 1.1: James writes a 3-page letter to 2 different friends twice a week,
    so he writes 3 * 2 * 2 = 12 pages every week. The answer is 12. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.1: 詹姆斯每周写2封3页的信给2个不同的朋友，因此他每周写3 * 2 * 2 = 12页。答案是12。 |'
- en: '| Question 1.2: How many weeks are there in a year? |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.2: 一年有多少周？ |'
- en: '| Answer 1.2: There are 52 weeks in a year. The answer is 52. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.2: 一年有52周。答案是52。 |'
- en: '| Question 1.3: Now we can answer the question: How many pages does he write
    a year? |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.3: 现在我们可以回答这个问题：他一年写多少页？ |'
- en: '| Answer 1.3: James writes 12 pages every week, so he writes 12 * 52 = 624
    pages a year. The answer is 624. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.3: 詹姆斯每周写12页，因此他一年写12 * 52 = 624页。答案是624。 |'
- en: '| … |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5: 简妮特的鸭子每天下16个鸡蛋。她每天早上吃三个鸡蛋，并用四个鸡蛋给朋友们做松饼。她每天在农贸市场上以每个新鲜鸭蛋2美元的价格出售剩余的鸡蛋。她每天在农贸市场上赚多少钱？
    |'
- en: '| Question 5.1: How many eggs does Janet have left after eating three for breakfast
    and baking muffins with four? |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5.1: 简妮特在吃了三只鸡蛋做早餐和用四只鸡蛋做松饼后，还剩下多少只鸡蛋？
    |'
- en: 'Table 11: An LLM Actor under the GSM8K Benchmark. The content in the shadow
    box is generated by LLM. Source from Hao et al. ([2023](#bib.bib7)).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 一个在GSM8K基准下的LLM角色。阴影框中的内容由LLM生成。来源于Hao等人（[2023](#bib.bib7)）。'
- en: Appendix B Examples Prompts
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 示例提示
- en: B.1 LLM-Profiled Policy Models
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 LLM-配置政策模型
- en: '1) Base Workflow: A planner for the NLIE-QA and an actor for the householding
    environment (ALFRED) are demonstrated in Table [6](#A1.T6 "Table 6 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") and [7](#A1.T7 "Table 7 ‣ Appendix A Frameworks
    of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components"), respectively. 2) Tool-Use Workflow: Actors under NLIE-QA
    is demonstrated in Table [8](#A1.T8 "Table 8 ‣ Appendix A Frameworks of LLM-Based
    Agents ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components"), [9](#A1.T9 "Table 9 ‣ Appendix A Frameworks of LLM-Based Agents
    ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components"),
    and [10](#A1.T10 "Table 10 ‣ Appendix A Frameworks of LLM-Based Agents ‣ A Survey
    on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components").
    The first two tables show the implementation of in-generation triggers, while
    the last one demonstrates the ReAct implementation. 3) Search Workflow: An actor
    is required during the expansion stage of MCTS in the RAP workflow. The prompts
    and expected generations are shown in Table [11](#A1.T11 "Table 11 ‣ Appendix
    A Frameworks of LLM-Based Agents ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components").'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基础工作流程：NLIE-QA的规划器和家庭环境（ALFRED）的演员分别展示在表[6](#A1.T6 "表 6 ‣ 附录 A LLM 基础代理的框架
    ‣ LLM 基础代理调研：常见工作流程和可重用的 LLM 组件")和[7](#A1.T7 "表 7 ‣ 附录 A LLM 基础代理的框架 ‣ LLM 基础代理调研：常见工作流程和可重用的
    LLM 组件")中。 2) 工具使用工作流程：NLIE-QA下的演员展示在表[8](#A1.T8 "表 8 ‣ 附录 A LLM 基础代理的框架 ‣ LLM
    基础代理调研：常见工作流程和可重用的 LLM 组件")、[9](#A1.T9 "表 9 ‣ 附录 A LLM 基础代理的框架 ‣ LLM 基础代理调研：常见工作流程和可重用的
    LLM 组件")和[10](#A1.T10 "表 10 ‣ 附录 A LLM 基础代理的框架 ‣ LLM 基础代理调研：常见工作流程和可重用的 LLM 组件")中。前两个表展示了生成触发器的实现，而最后一个表展示了ReAct的实现。
    3) 搜索工作流程：在RAP工作流程的扩展阶段，需要一个演员。提示和预期生成结果展示在表[11](#A1.T11 "表 11 ‣ 附录 A LLM 基础代理的框架
    ‣ LLM 基础代理调研：常见工作流程和可重用的 LLM 组件")中。 |
- en: B.2 Prompts for LLM Evaluators
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 LLM 评估者的提示 |
- en: 'Table [12](#A2.T12 "Table 12 ‣ B.2 Prompts for LLM Evaluators ‣ Appendix B
    Examples Prompts ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable
    LLM-Profiled Components") and [13](#A2.T13 "Table 13 ‣ B.2 Prompts for LLM Evaluators
    ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents: Common Workflows
    and Reusable LLM-Profiled Components") show LLM evaluators that are profiled as
    classification tasks, while Table [15](#A2.T15 "Table 15 ‣ B.2 Prompts for LLM
    Evaluators ‣ Appendix B Examples Prompts ‣ A Survey on LLM-Based Agents: Common
    Workflows and Reusable LLM-Profiled Components") demonstrates another to general
    free-form text.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 表[12](#A2.T12 "表 12 ‣ B.2 LLM 评估者的提示 ‣ 附录 B 示例提示 ‣ LLM 基础代理调研：常见工作流程和可重用的 LLM
    组件")和[13](#A2.T13 "表 13 ‣ B.2 LLM 评估者的提示 ‣ 附录 B 示例提示 ‣ LLM 基础代理调研：常见工作流程和可重用的
    LLM 组件")展示了以分类任务为特征的LLM评估者，而表[15](#A2.T15 "表 15 ‣ B.2 LLM 评估者的提示 ‣ 附录 B 示例提示 ‣
    LLM 基础代理调研：常见工作流程和可重用的 LLM 组件")展示了另一个以一般自由文本为特征的LLM评估者。 |
- en: '| Given a question and some sub-questions, determine whether the last sub-question
    is useful to answer the question. Output ’Yes’ or ’No’, and a reason. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 给定一个问题和一些子问题，确定最后一个子问题是否对回答问题有用。输出“是”或“否”，并给出理由。 |'
- en: '| Question 1: Four years ago, Kody was only half as old as Mohamed. If Mohamed
    is currently twice as 30 years old, how old is Kody? |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1：四年前，科迪的年龄只有穆罕默德的一半。如果穆罕默德目前的年龄是30岁，那么科迪现在多大了？ |'
- en: '| Question 1.1: How old is Mohamed? |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.1：穆罕默德今年多大了？ |'
- en: '| Question 1.2: How old was Mohamed four years ago? |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.2：四年前穆罕默德多大了？ |'
- en: '| New question 1.3: How old was Kody four years ago? |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 新问题 1.3：四年前科迪多大了？ |'
- en: '| Is the new question useful? Yes. We need the answer to calculate how old
    is Kody now. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 新的问题有用吗？是的。我们需要这个答案来计算科迪现在多大了。 |'
- en: '| … |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5：珍妮特的鸭子每天产16个蛋。她每天早上吃三个作为早餐，每天用四个蛋为朋友们烤松饼。她把剩下的蛋以每个新鲜鸭蛋2美元的价格在农贸市场上出售。她每天在农贸市场上赚多少美元？
    |'
- en: '| New question 5.1: Now we can answer the question: How much in dollars does
    she make every day at the farmers’ market? |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 新问题 5.1：现在我们可以回答这个问题：她每天在农贸市场赚多少钱？ |'
- en: '| Is the new question useful? |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 新问题是否有用？ |'
- en: 'Table 12: An LLM Evaluator within RAP Workflow under NLIE-QA, implemented by
    few-Shot CoT prompting. It assesses the usefulness of new sub-questions in solving
    the original question. Source from Hao et al. ([2023](#bib.bib7))'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：在 NLIE-QA 下的 RAP 工作流中的 LLM 评估器，通过少量样本 CoT 提示实现。它评估新子问题在解决原始问题中的有用性。来源：Hao
    等 ([2023](#bib.bib7))
- en: '| Evaluate if given numbers can reach 24 (sure/likely/impossible) 10 14'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '| 评估给定数字是否可以达到 24（确定/可能/不可能） 10 14'
- en: 10 + 14 = 24
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 10 + 14 = 24
- en: sure
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 当然
- en: …
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 1 3 3
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 1 3 3
- en: 1 * 3 * 3 = 9
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 1 * 3 * 3 = 9
- en: (1 + 3) * 3 = 12
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: (1 + 3) * 3 = 12
- en: 1 3 3 are all too small
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 1 3 3 都太小了
- en: impossible
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能
- en: 11, 12 |
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 11, 12 |
- en: 'Table 13: An LLM Evaluator within Tree-of-Thought Workflow under Game 24, implemented
    by few-Shot CoT prompting. The LLM is profiled for multi-class classification.
    Source from Yao et al. ([2023a](#bib.bib29)).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：在 Game 24 下的思维树工作流中的 LLM 评估器，通过少量样本 CoT 提示实现。该 LLM 适用于多类分类。来源：Yao 等 ([2023a](#bib.bib29))。
- en: '| Context: … Sentence: …'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '| 上下文：… 句子：…'
- en: Is the sentence supported by the context above?
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 上述句子是否得到上下文的支持？
- en: 'Answer Yes or No: |'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 回答是或否： |
- en: 'Table 14: An LLM Evaluator, implemented by zero-Shot CoT prompting. The LLM
    is profiled for multi-class classification. Source from Manakul et al. ([2023](#bib.bib13)).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：一个 LLM 评估器，通过零样本 CoT 提示实现。该 LLM 适用于多类分类。来源：Manakul 等 ([2023](#bib.bib13)).
- en: '| … |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question: Serianna is a band of what genre that combines elements of heavy
    metal and hardcore punk? |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 问题：Serianna 是一个结合了重金属和硬核朋克元素的乐队吗？ |'
- en: '| Proposed Answer: Let’s think step by step. Serianna is a band of metalcore
    genre. Metalcore is a subgenre of heavy metal and hardcore punk. So Serianna is
    a band of heavy metal and hardcore punk. So the answer is: heavy metal and hardcore
    punk. |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 提议的回答：我们一步步考虑。Serianna 是一个金属核风格的乐队。金属核是重金属和硬核朋克的一个子流派。所以 Serianna 是一个重金属和硬核朋克的乐队。因此答案是：重金属和硬核朋克。
    |'
- en: '| 1\. Plausibility: The question asks for the genre that combines elements of heavy
    metal and hardcore punk, and the answer is "heavy metal and hardcore punk", simply
    repeat the question. So it’s not plausible. 2\. Truthfulness: Let’s search the
    question in google: > Search Query: Serianna is a band of what genre that combines
    elements of heavy metal and hardcore punk? site: wikipedia.org > Evidence:
    [Metalcore - Wikipedia] Metalcore is a fusion music genre that combines elements
    of extreme metal and hardcore punk. |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 合理性：问题询问的是结合了重金属和硬核朋克元素的流派，答案是“重金属和硬核朋克”，只是重复了问题。因此不合理。2\. 真实性：让我们在谷歌上搜索一下：>
    搜索查询：Serianna 是一个结合了重金属和硬核朋克元素的乐队吗？ site: wikipedia.org > 证据：
    [金属核 - 维基百科] 金属核是一个融合了极端金属和硬核朋克元素的音乐流派。 |'
- en: '| The evidence suggests that metalcore is a genre that combines
    elements of extreme metal and hardcore punk, as stated in the proposed answer.
    Let’s search the proposed answer in google: > Search Query: Serianna is a band
    of metalcore genre. site: wikipedia.org > Evidence:
    [Serianna - Wikipedia] Serianna was a metalcore band from Madison, Wisconsin.
    The band formed in 2006… |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 证据表明，金属核是一种结合了极端金属和硬核朋克元素的流派，如提议的答案所述。让我们在谷歌上搜索提议的答案：> 搜索查询：Serianna
    是金属核流派的乐队。 site: wikipedia.org > 证据： [Serianna -
    Wikipedia] Serianna 是来自威斯康星州麦迪逊的金属核乐队。乐队成立于 2006 年… |'
- en: '| The evidence suggests Serianna is a metalcore band, the proposed
    answer is correct about this. Above all, the proposed answer correctly identifies
    that Serianna is a band of the metalcore genre, which combines elements of heavy
    metal and hardcore punk. However, the final answer is not plausible since it just
    lists the genres that metalcore combines. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 证据表明 Serianna 是一支金属核乐队，提议的答案对此是正确的。最重要的是，提议的答案正确地识别了 Serianna
    是一支结合了重金属和硬核朋克元素的金属核流派的乐队。然而，最终答案并不合理，因为它只是列出了金属核结合的流派。
    |'
- en: 'Table 15: An LLM Evaluator Within Feedback-Learning Workflow (Tools), implemented
    with few-shot demonstrations. The content in the shadow box is generated by LLM.
    We omit some demonstrations for brevity. Source from Gou et al. ([2024](#bib.bib5)).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：在反馈学习工作流程（工具）中的 LLM 评估器，使用了少量示例演示。阴影框中的内容由 LLM 生成。为了简洁，我们省略了一些演示。来源于 Gou
    等人 ([2024](#bib.bib5))。
- en: B.3 Prompts as Dynamic Models
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 动态模型的提示
- en: 'Table [16](#A2.T16 "Table 16 ‣ B.3 Prompts as Dynamic Models ‣ Appendix B Examples
    Prompts ‣ A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled
    Components") is profiled as a dynamic model.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [16](#A2.T16 "表 16 ‣ B.3 动态模型的提示 ‣ 附录 B 示例提示 ‣ 关于 LLM 基于代理的调查：常见工作流程和可重用的
    LLM 配置组件") 被描述为动态模型。
- en: '| Given a question, please decompose it into sub-questions. For each sub-question,
    please answer it in a complete sentence, ending with "The answer is". When the
    original question is answerable, please start the subquestion with "Now we can
    answer the question: ". |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 给定一个问题，请将其分解为子问题。对于每个子问题，请用完整的句子回答，句末以“答案是”结束。当原始问题可以回答时，请以“现在我们可以回答这个问题了：”开头。
    |'
- en: '| Question 1: Weng earns $12 an hour for babysitting. Yesterday, she just did
    50 minutes of babysitting. How much did she earn? |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1：Weng 的保姆时薪为 $12。昨天，她做了 50 分钟的保姆工作。她赚了多少？ |'
- en: '| Question 1.1: How much does Weng earn per minute? |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.1：Weng 每分钟赚多少钱？ |'
- en: '| Answer 1.1: Since Weng earns $12 an hour for babysitting, she earns $12 /
    60 = $0.2 per minute. The answer is 0.2. |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.1：由于 Weng 的保姆时薪为 $12，因此她每分钟赚 $12 / 60 = $0.2。答案是 0.2。 |'
- en: '| Question 1.2: Now we can answer the question: How much did she earn? |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.2：现在我们可以回答这个问题了：她赚了多少？ |'
- en: '| Answer 1.2: Working 50 minutes, she earned $0.2 x 50 = $10\. The answer is
    10. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.2：工作 50 分钟，她赚了 $0.2 x 50 = $10。答案是 10。 |'
- en: '| … |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5：珍妮特的鸭子每天下16个蛋。她每天早晨吃三个作为早餐，每天用四个蛋为朋友们烘焙松饼。她将剩余的蛋每天在农贸市场以每个2美元的价格出售。她每天在农贸市场赚多少钱？
    |'
- en: '| Question 5.1: How many eggs does Janet have left after eating three for breakfast
    and using four for muffins? |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5.1：珍妮特吃掉三个作为早餐，并用四个做松饼后剩下多少个蛋？ |'
- en: '| Answer 5.1: |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 答案 5.1： |'
- en: 'Table 16: An LLM-Profiled Dynamic Model.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16：一个LLM-配置的动态模型。
- en: Appendix C Creation of a Task-Agnostic Tool Environment
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 创建任务无关的工具环境
- en: Previous work always limits tools to specific applications like NLIE-QA, future
    work should aim to establish a comprehensive tool environment that encompasses
    a wide array of tools suitable for various tasks. A major challenge here is adapting
    a single actor to utilize such an environment effectively. While in-generation
    strategies are constrained as triggers are typically only straightforward for
    basic tools with simple arguments, a reasoning-acting strategy might offer more
    promise. Nonetheless, defining tools remains a challenge, especially in terms
    of efficient in-context learning or fine-tuning for tool utilization.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的工作总是将工具限制在特定的应用上，如NLIE-QA，未来的工作应旨在建立一个全面的工具环境，涵盖适用于各种任务的广泛工具。一个主要的挑战是将单一的行为者适应于有效利用这样的环境。虽然生成策略受到限制，因为触发器通常仅对具有简单参数的基本工具简单有效，但推理-行动策略可能更有前景。不过，定义工具仍然是一个挑战，特别是在高效的上下文学习或微调工具使用方面。
- en: Appendix D Stochastic $\text{glm}_{\text{actor}}$
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 随机 $\text{glm}_{\text{actor}}$
- en: Typically, a single action is sampled from the output of glmactor. Exploring
    a stochastic glmactor, which provides a distribution over possible actions, can
    enhance the stochastic nature of the glmpolicy and improve efficiency. This approach
    could include investigating constrained generation techniques (Willard and Louf,
    [2023](#bib.bib26)). Additionally, using such a distribution could efficiently
    serve as rewards for all possible actions, potentially eliminating the need for
    a separate glmeval to model rewards in certain workflows. This method allows for
    the simultaneous expansion of multiple potential nodes in one generation step,
    rather than expanding each node individually in search-based workflows (Hao et al.,
    [2023](#bib.bib7)).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从glmactor的输出中采样一个单一动作。探索提供可能动作分布的随机glmactor，可以增强glmpolicy的随机性并提高效率。这种方法可能包括研究约束生成技术（Willard和Louf，[2023](#bib.bib26)）。此外，使用这种分布可以高效地作为所有可能动作的奖励，可能无需在某些工作流程中单独使用glmeval来建模奖励。这种方法允许在一次生成步骤中同时扩展多个潜在节点，而不是在基于搜索的工作流程中逐个扩展每个节点（Hao等人，[2023](#bib.bib7)）。
- en: Appendix E Memory
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 记忆
- en: The implementations of memory in the reviewed works are typically straightforward
    and arbitrary. Commonly, static information (e.g., profiling messages) is manually
    constructed and stored, whereas dynamic information (e.g., feedback) is handled
    via runtime data structures during interactions within each workflow. While the
    management of hybrid memory systems—requiring the explicit processing and management
    of short-term and long-term memory—is extensively discussed in a previous survey
    by Wang et al. ([2024](#bib.bib21)), such memory management aspects are beyond
    the focus of this survey, which centers on LLM-based workflows.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在审阅的工作中，记忆的实现通常是直接而随意的。常见的静态信息（例如，配置消息）是手动构建和存储的，而动态信息（例如，反馈）则通过每个工作流程中的运行时数据结构处理。虽然混合记忆系统的管理—需要明确处理和管理短期和长期记忆—在Wang等人之前的调查中已广泛讨论（[2024](#bib.bib21)），但这些记忆管理方面超出了本调查的重点，本调查集中在基于LLM的工作流程上。
