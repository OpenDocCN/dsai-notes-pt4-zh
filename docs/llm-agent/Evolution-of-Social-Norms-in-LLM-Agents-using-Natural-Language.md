<!--yml

类别：未分类

日期：2025-01-11 12:16:59

-->

# 使用自然语言的LLM代理中的社会规范演化

> 来源：[https://arxiv.org/html/2409.00993/](https://arxiv.org/html/2409.00993/)

Ilya Horiguchi, Takahide Yoshida    Takashi Ikegami 东京大学研究生院综合系统学系，日本

电子邮件：horiguchi@sacral.c.u-tokyo.ac.jp

###### 摘要

最近，随着大型语言模型（LLMs）的最新进展，利用这些模型进行博弈论模拟的兴趣激增，其中LLM充当个体代理，参与社会互动。本研究探讨了LLM代理通过自然语言交流自发地生成并遵循规范策略的潜力，建立在Axelrod的元规范博弈基础之上。我们的实验表明，通过对话，LLM代理可以仅通过自然语言互动形成复杂的社会规范，例如元规范——一种强制惩罚那些不惩罚作弊者的规范。结果验证了使用LLM代理模拟社会互动和通过自然语言理解复杂策略与规范的生成与演化的有效性。未来的工作可以通过纳入更广泛的场景和代理特征，扩展这些发现，旨在揭示社会规范形成背后的更微妙机制。

## 引言

近年来，随着大型语言模型（LLMs）的出现，基于博弈论的模拟研究将LLM视为个体代理，已在全球范围内展开。例如，关于重复博弈的研究表明，虽然LLM在优先考虑自身利益的博弈中表现出色，但在需要协调的博弈中却表现出次优行为（Akata等，[2023](https://arxiv.org/html/2409.00993v1#bib.bib1)）。此外，谈判竞技场研究（Bianchi等，[2024](https://arxiv.org/html/2409.00993v1#bib.bib4)）展示了LLM代理如何通过灵活对话在谈判环境中进行复杂谈判，并通过采用特定的行为策略显著改善谈判结果。进一步地，Mao等（[2023](https://arxiv.org/html/2409.00993v1#bib.bib7)）的ALYMPICS研究提出了一种基于LLM代理的博弈论研究系统模拟框架，旨在模拟战略互动。然而，先前的研究仅限于让代理选择合作（C）或背叛（D）等简单选项，未能充分利用自然语言能力的优势。因此，本研究旨在分析LLM在使用自然语言进行丰富讨论时，多样化策略的产生过程。

我们从自发策略出现的角度，聚焦于 Axelrod 的元规范概念。元规范是一种规范，它惩罚那些不遵守某一特定规范的行为者（Axelrod，[1986](https://arxiv.org/html/2409.00993v1#bib.bib2)）。例如，如果存在惩罚作弊者的规范，那么相应的元规范就是惩罚那些选择不惩罚作弊者的人。Axelrod 进行了关于规范如何自发出现并稳定的模拟研究。这个概念促使了 Galán 等人（[2011](https://arxiv.org/html/2409.00993v1#bib.bib5)）的研究，探讨了不同网络结构如何影响元规范作为促进社会合作机制的有效性；以及 Axelrod 本人对元规范的思考，即元规范是保证规范稳定性所需的额外机制（Axelrod，[1998](https://arxiv.org/html/2409.00993v1#bib.bib3)）。Suzuki 和 Arita（[2024](https://arxiv.org/html/2409.00993v1#bib.bib9)）在博弈论关系中对人格特征的演化进行了建模，展示了通过使用 LLM 代理将与人格特征相关的语言描述作为基因，从而推动合作行为的演化。这表明，LLM 代理可以通过自然语言描述的演化，发展出策略。这些研究表明，LLM 代理可能通过演化协商和战略决策过程，自主获得多种策略，如合作和规范。

我们探索了在 LLM 代理之间通过自然语言讨论是否能产生元规范。本研究为理解多代理 LLM 的行为提供了新的见解。特别是在人工智能对齐领域，理解代理如何在其他价值观和社会规范的存在下，自发地创建自主规范，具有重要意义。

## 实验

Schick 等人（[2023](https://arxiv.org/html/2409.00993v1#bib.bib8)）展示了 LLM 自我教育使用外部工具的能力。这一能力使得 LLM 代理能够自主使用各种工具，如计算器、搜索引擎和日历。在我们的实验中，如 Bianchi 等人（[2024](https://arxiv.org/html/2409.00993v1#bib.bib4)）所示，我们采用了一种使用标签代替工具的方法，使 LLM 代理能够像在游戏中输入命令一样操作。这使我们能够使用 LLM 代理实现规范游戏的模拟。具体而言，我们开发了一个标签系统，用来指示 LLM 代理在规范游戏中的行为。该系统使得代理能够根据游戏情境选择合适的命令，并互动地整合信息。我们构建了一个环境，使得更加复杂的策略得以发展，元规范得以自发产生。

![参见说明](img/f6ab0ec4ca304a812aba103c96eb29a8.png)

图 1：规范游戏中的代理命令

### 规范游戏

在规范游戏实验中，我们修改了Axelrod的实验设置，以便代理人能够更灵活地发展策略。每个代理人可以用自然语言表达他们的“复仇心”和“大胆性”水平，例如“7个中的5个”，然后继续进行游戏，同时体现这一人格特征。

所有代理人都会经历测试阶段和讨论阶段。在测试阶段，玩家选择执行测试命令或欺骗命令（图[1](https://arxiv.org/html/2409.00993v1#Sx2.F1 "Figure 1 ‣ Experiments ‣ Evolution of Social Norms in LLM Agents using Natural Language")）。使用测试命令时，随机值的均值为50，方差为10，作为得分。如果选择欺骗，玩家可以比进行测试时获得平均多30分。然而，当分数向所有代理人宣布时，欺骗行为将被揭露。

在讨论阶段，代理人轮流发言，讨论彼此的分数。在这一阶段，他们可以选择“下一位”命令来指定谁下一个发言，或者选择“惩罚”命令来惩罚某个指定的代理人。使用该命令的代理人支付20分，惩罚目标代理人-90分。惩罚命令执行后，下一位发言者将被随机确定。这些数值和设置符合Axelrod原始论文中的规定。表[1](https://arxiv.org/html/2409.00993v1#Sx2.T1 "Table 1 ‣ Norms Game ‣ Experiments ‣ Evolution of Social Norms in LLM Agents using Natural Language")展示了每个事件发生后的命令选择和讨论后分数的示例。

表1：每个命令的收益示例

| 事件 | 每次事件的报酬 | 事件数量 | 总报酬 |
| --- | --- | --- | --- |
| 基线 | $50$ | 1 | $50$ |
| 欺骗 | $+30$ | 1 | $+30$ |
| 被惩罚 | $-90$ | 1 | $-90$ |
| 惩罚成本 | $-20$ | 2 | $-40$ |
| 得分 | $-50$ |

### 演化

在参考Axelrod原始论文中的实验设置时，我们在本研究中做了几处修改。在Axelrod的研究中，在计算所有报酬后，每个时期具有平均报酬的代理人留下一个后代，报酬高于均值一个标准差的基因数量翻倍，而报酬低于均值一个标准差的基因被淘汰。此外，个体的策略（大胆性或复仇心）会按概率重写。在原始论文中，人口固定为20，但在我们的研究中，由于讨论阶段每次只有一个个体发言，我们需要减少代理人的数量。因此，我们将代理人数量固定为7，并采用了一种方法，即当按照报酬排序时，3个中位数分数的代理人直接继承，而前2个代理人的基因数量翻倍。此外，随机选择一个个体，将其大胆性或复仇心改变为随机值。这使我们能够观察到更动态的基因变化和策略演化。

## 结果

本研究使用了OpenAI的GPT-4¹¹1gpt-4-0125-preview，温度参数为0.0。结果的原始数据和使用的代码可以在github²²2[https://github.com/NeoGendaijin/NormGame_LLM](https://github.com/NeoGendaijin/NormGame_LLM)上访问。

### 控制复仇心和大胆性的结果

在Axelrod的规范博弈中，给代理人提供了与复仇心和大胆性相关的以下提示。

<svg class="ltx_picture" height="58" id="Sx3.SSx1.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,58) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="30.44" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Character Traits: - Vengefulness: 5 out of 7 - Boldness: 6 out of 7 Trait Descriptions: - **Vengefulness** (omited) - **Boldness** (omited)</foreignobject></g></g></svg>

我们观察了在高/低复仇心和高/低大胆性的群体中，讨论和惩罚的进展。当复仇心低时，数字1-3被均匀随机分配；当复仇心高时，数字5-7被均匀随机分配。在每个小组中，讨论阶段设置为最多进行21轮发言，并且整个过程重复进行了10次。以下是结果。

![参见标题](img/f2e0182dbe127271faf5437212553713.png)

图2：每个小组内讨论中“惩罚”指令的数量

![参见标题](img/87d31821185b881dadc842dbc4701527.png)

图3：在引入基于收益的自然选择时，复仇心和大胆性的进展

![参见标题](img/0ad18f05e5fb7d0cbddc5de886ef4630.png)

图4：高复仇心、高大胆性的惩罚网络可视化

讨论中选择惩罚次数的分布如图[2](https://arxiv.org/html/2409.00993v1#Sx3.F2 "Figure 2 ‣ 复仇心和大胆行为控制的结果 ‣ 结果 ‣ 基于自然语言的LLM代理社会规范演变")所示。该图显示，在复仇心强和大胆行为强的群体中，"惩罚"指令的使用频率最高。垂直拉长的小提琴图表明，即使在同一群体内部，变化也非常大。相反，在复仇心或大胆行为较低的群体中，惩罚次数始终保持较低。为了更详细地分析复仇心强和大胆行为强的群体，图[4](https://arxiv.org/html/2409.00993v1#Sx3.F4 "Figure 4 ‣ 复仇心和大胆行为控制的结果 ‣ 结果 ‣ 基于自然语言的LLM代理社会规范演变")展示了惩罚网络的可视化，每个代理作为一个节点。作弊的代理用红色表示，未作弊的代理用浅蓝色表示。在像Dataset2这样的回合中，每个人互相惩罚，但也有像Dataset5和6那样的回合，其中代理彼此制约，根本没有使用惩罚指令。在Dataset4和9中，当一个作弊的代理被一个未作弊的代理攻击时，另一个作弊的代理会以惩罚进行报复。通过阅读代理的自然语言判断，可以观察到它们采取了保护使用相同作弊策略的代理，以提高自己分数的策略。类似地，在Dataset1和2中，几乎所有人都在作弊，惩罚作弊的代理反而遭到了报复。这是因为在每个人都作弊的情况下，惩罚被其他代理视为破坏秩序的行为。这个行动原则是一种由Axelrod描述的元规范，并且可以认为它是通过使用自然语言的群体讨论自然产生的。

### 基于报酬的演化结果

图[3](https://arxiv.org/html/2409.00993v1#Sx3.F3 "Figure 3 ‣ 复仇心和大胆行为控制的结果 ‣ 结果 ‣ 基于自然语言的LLM代理社会规范演变")显示了一个基于报酬的自然选择实验结果，该实验针对一组具有随机复仇心和大胆行为的个体。在每个时期，讨论阶段的最大回合数设置为7次（每人平均1次），并执行了40个周期。图中的圆圈大小对应每个周期的人口规模。从中可以看出，最初分散的分布逐渐趋于集中。同时，7个代理的平均值在复仇心和大胆行为的中间值附近循环。惩罚次数的演化呈现先增加再稳定的趋势。当发生作弊时，会被具有适度复仇心的代理惩罚并被淘汰。

### 自然语言

尝试将进化算法引入大语言模型（LLM）已经在许多研究中得到了探索（Guo et al.,, [2023](https://arxiv.org/html/2409.00993v1#bib.bib6)）。在先前的研究中（Suzuki and Arita,, [2024](https://arxiv.org/html/2409.00993v1#bib.bib9)），通过遗传算法进化了LLM的个性。该方法通过让LLM重新表述表达式来模拟自然语言的变化。在我们的实验设置中，我们偏离了Axelrod原始的参数，并引入了不受复仇心和大胆性限制的表达方式，通过重新表述进化个性和策略。每个个性用大约10个单词表达，重新表述也由LLM完成。为了启动每个周期，我们让LLM随机生成个性，并从这些个性池中选择。在每个周期内，按照之前的实验执行了21轮。基于最终得分，个性通过自然选择过程传承到下一代。我们考察了在40个周期中的进化轨迹，每个周期包含21轮。图[5](https://arxiv.org/html/2409.00993v1#Sx3.F5 "图5 ‣ 自然语言 ‣ 结果 ‣ 使用自然语言的LLM代理社会规范进化")展示了通过UMAP对五次实验的平均个性嵌入的投影。虽然实验1、2和3的结果似乎收敛于UMAP的相似点，但实验3和4则过渡到了不同的区域。图[6](https://arxiv.org/html/2409.00993v1#Sx3.F6 "图6 ‣ 自然语言 ‣ 结果 ‣ 使用自然语言的LLM代理社会规范进化")说明了嵌入的方差，确认了所有成员并未收敛于相同的个性，而是随着嵌入空间中的一定扩散而进化。

![请参见说明文字](img/a72fc0cacf4cc92392d5b82edf21db89.png)

图5：所有五次实验的嵌入图。箭头表示每个周期中均值的变化。

![请参见说明文字](img/08f90dd5bf33c9a619be839bb9306532.png)

图6：嵌入方差在40个周期中的变化。

![请参见说明文字](img/eb995e9f8a8ef124ead41071f4669049.png)

图7：成员作弊行为在40个周期中的发展。

聚焦于行为，图[7](https://arxiv.org/html/2409.00993v1#Sx3.F7 "图7 ‣ 自然语言 ‣ 结果 ‣ LLM代理中社会规范的演化")显示，实验3中有较高比例的成员参与作弊。相比之下，实验4和实验5显示，作弊者迅速被淘汰，形成了不作弊的社区。图[8](https://arxiv.org/html/2409.00993v1#Sx3.F8 "图8 ‣ 自然语言 ‣ 结果 ‣ LLM代理中社会规范的演化")表明，作弊率较高的实验和周期通常表现出较高的惩罚率。演化结果依赖于初始的个性池，即使从类似的个性池开始，我们也观察到不同策略的出现。这种通过演化自然语言表达方式的方式为复杂社会规范和策略的发展提供了丰富的见解，尽管在分析和解释上存在挑战，未来的研究需要解决这些问题。

![参见标题](img/bfac68d148fc362de35fa81c15c44d62.png)

图8：成员惩罚行为在40个周期中的发展。

## 结论

本研究通过在基于博弈论的模拟中使用大型语言模型（LLM）代理进行丰富的自然语言讨论，探讨了规范策略的出现。基于阿克塞尔罗德的元规范博弈，我们实现了一个规范博弈，在该博弈中，代理参与讨论，揭示了诸如惩罚作弊行为和对不惩罚他人的元规范等复杂社会规范的形成。我们的实验表明，代理的个性特征，特别是复仇心和大胆性，显著影响行为选择和惩罚模式。基于回报的进化实验表明，平衡这些特征对于代理的生存策略至关重要，且群体通常由既不过度作弊也不频繁惩罚他人的代理主导。使用自然语言来发展策略为我们的研究带来了丰富性和挑战性。虽然它使策略发展更加细致和复杂，但也带来了分析和解释上的困难。未来的研究应解决这些挑战，可能通过探讨语言歧义对策略出现的影响，进而带来有趣且出乎意料的结果。引入虚假信念任务的元素可能对研究策略如何在信息不完整的情况下进化，以及代理如何预测并响应他人的心理状态具有价值。研究相同的个性特征（例如复仇心和大胆性）在不同博弈情境中的表现，将有助于洞察进化策略的可转移性。分析群体规模如何影响策略进化，特别是考察小群体与较大群体之间的差异，后者可能会出现子群体形成，或许能够提供有价值的见解。此外，应用诸如五大人格模型等心理学框架来追踪代理在不同博弈情境中的个性进化，可能会为心理学家提供有趣的见解。

这些未来的研究方向可能为我们提供更加全面的理解，帮助我们理解复杂社会规范和策略如何通过自然语言互动出现和进化。通过解决语言基础的进化模型中的分析和解释难题，我们可以进一步完善研究AI系统中社会规范形成动态的方法。本研究证实，使用LLM代理进行社会互动模拟是理解复杂策略和规范通过自然语言出现和进化的有效方法。随着我们不断完善这些方法，我们预计能深入洞察多样化社会规范的内在机制，这可能对AI发展及我们对人类社会动态的理解产生重要影响。

## 参考文献

+   Akata et al., (2023) Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., 和 Schulz, E. (2023). 与大型语言模型进行重复博弈。arXiv 预印本 arXiv:2305.16867 [cs.CL]。

+   Axelrod, (1986) Axelrod, R. (1986). 《规范的进化方法》。美国政治科学评论, 80(4):1095–1111。

+   Axelrod, (1998) Axelrod, R. (1998). 《合作的复杂性：基于代理的竞争与合作模型》。普林斯顿大学出版社，普林斯顿。

+   Bianchi et al., (2024) Bianchi, F., Chia, P. J., Yuksekgonul, M., Tagliabue, J., Jurafsky, D., 和 Zou, J. (2024). 大型语言模型能否进行有效谈判？谈判竞技平台与分析。arXiv:2402.05863 [cs.AI]。

+   Galán et al., (2011) Galán, J. M., Łatek, M. M., 和 Rizi, S. M. M. (2011). Axelrod 的元规范博弈在网络中的应用。PLoS ONE, 6(5):e20474。

+   Guo et al., (2023) Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., 和 Yang, Y. (2023). 将大型语言模型与进化算法结合，产生强大的提示优化器。arXiv [cs.CL]。

+   Mao et al., (2023) Mao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F., Ge, T., 和 Wei, F. (2023). Alympics：大型语言模型代理与博弈论相遇——探索使用 AI 代理进行战略决策。arXiv 预印本 arXiv:2311.03220 [cs.CL]。

+   Schick et al., (2023) Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., 和 Scialom, T. (2023). Toolformer：语言模型可以自学使用工具。在 NeurIPS 2023 口头报告中展示。

+   Suzuki and Arita, (2024) Suzuki, R. 和 Arita, T. (2024). 使用大型语言模型的合作行为相关人格特征的进化模型。Sci. Rep., 14(1):5989。
