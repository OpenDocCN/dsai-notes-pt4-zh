- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:40:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:40:25'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 破解代理：通过故障放大来破坏自主LLM代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.20859](https://ar5iv.labs.arxiv.org/html/2407.20859)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.20859](https://ar5iv.labs.arxiv.org/html/2407.20859)
- en: Boyang Zhang¹   Yicong Tan¹   Yun Shen²   Ahmed Salem³   Michael Backes¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Boyang Zhang¹   Yicong Tan¹   Yun Shen²   Ahmed Salem³   Michael Backes¹
- en: Savvas Zannettou⁴   Yang Zhang¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Savvas Zannettou⁴   Yang Zhang¹
- en: ¹CISPA Helmholtz Center for Information Security   ²NetApp   ³Microsoft   ⁴TU
    Delft
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹CISPA赫尔姆霍茨信息安全中心   ²NetApp   ³微软   ⁴代尔夫特理工大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, autonomous agents built on large language models (LLMs) have experienced
    significant development and are being deployed in real-world applications. These
    agents can extend the base LLM’s capabilities in multiple ways. For example, a
    well-built agent using GPT-3.5-Turbo as its core can outperform the more advanced
    GPT-4 model by leveraging external components. More importantly, the usage of
    tools enables these systems to perform actions in the real world, moving from
    merely generating text to actively interacting with their environment. Given the
    agents’ practical applications and their ability to execute consequential actions,
    it is crucial to assess potential vulnerabilities. Such autonomous systems can
    cause more severe damage than a standalone language model if compromised. While
    some existing research has explored harmful actions by LLM agents, our study approaches
    the vulnerability from a different perspective. We introduce a new type of attack
    that causes malfunctions by misleading the agent into executing repetitive or
    irrelevant actions. We conduct comprehensive evaluations using various attack
    methods, surfaces, and properties to pinpoint areas of susceptibility. Our experiments
    reveal that these attacks can induce failure rates exceeding 80% in multiple scenarios.
    Through attacks on implemented and deployable agents in multi-agent scenarios,
    we accentuate the realistic risks associated with these vulnerabilities. To mitigate
    such attacks, we propose self-examination detection methods. However, our findings
    indicate these attacks are difficult to detect effectively using LLMs alone, highlighting
    the substantial risks associated with this vulnerability.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于大型语言模型（LLMs）构建的自主代理经历了显著的发展，并正在被应用于现实世界的应用中。这些代理可以以多种方式扩展基础LLM的能力。例如，使用GPT-3.5-Turbo作为核心的高质量代理可以通过利用外部组件来超越更先进的GPT-4模型。更重要的是，工具的使用使这些系统能够在现实世界中执行动作，从仅生成文本转变为主动与环境互动。鉴于代理的实际应用及其执行重要动作的能力，评估潜在的脆弱性至关重要。如果被破坏，这些自主系统可能造成比独立语言模型更严重的损害。尽管一些现有研究已经探讨了LLM代理的有害行为，但我们的研究从不同的角度接近脆弱性。我们引入了一种新的攻击类型，通过误导代理执行重复或无关的动作来引发故障。我们使用各种攻击方法、表面和特性进行全面评估，以确定易受攻击的领域。我们的实验揭示，这些攻击可以在多种情境下引发超过80%的失败率。通过对多代理情境中的已实现和可部署代理进行攻击，我们突出了这些脆弱性相关的现实风险。为了减轻这些攻击，我们提出了自我检查检测方法。然而，我们的发现表明，使用LLMs单独有效检测这些攻击是困难的，这突显了与这种脆弱性相关的重大风险。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: '![Refer to caption](img/5373cf1795947d499a1c8de7f21cbee2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5373cf1795947d499a1c8de7f21cbee2.png)'
- en: 'Figure 1: The overview of our attack which exacerbates the instabilities of
    LLM agents.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: 我们的攻击概述，它加剧了LLM代理的不稳定性。'
- en: Large language models (LLMs) have been one of the most recent notable advancements
    in the realm of machine learning. These models have undergone significant improvements,
    becoming increasingly sophisticated and powerful. Modern LLMs, such as the latest
    GPT-4 [[1](#bib.bib1)] can now perform complex tasks, including contextual comprehension,
    nuanced sentiment analysis, and creative writing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是机器学习领域最近最显著的进展之一。这些模型经历了显著的改进，变得越来越复杂和强大。现代LLMs，例如最新的GPT-4 [[1](#bib.bib1)]，现在可以执行复杂的任务，包括上下文理解、细致的情感分析和创意写作。
- en: Leveraging LLMs’ natural language processing ability, LLM-based agents have
    been developed to extend the capabilities of base LLMs and automate a variety
    of real-world tasks. These autonomous agents are built with an LLM at its core
    and integrated with several external components, such as databases, the Internet,
    software tools, and more. These components address performance gaps in current
    LLMs, such as employing the Wolfram Alpha API [[2](#bib.bib2)] for solving complex
    mathematical problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 利用LLMs的自然语言处理能力，基于LLM的代理已经被开发出来，以扩展基础LLMs的功能，并自动化各种现实世界的任务。这些自主代理以LLM为核心，并与多个外部组件集成，如数据库、互联网、软件工具等。这些组件弥补了当前LLMs的性能差距，例如使用Wolfram
    Alpha API [[2](#bib.bib2)] 来解决复杂的数学问题。
- en: Furthermore, the integration of these external components allows the conversion
    of textual inputs into real-world actions. For instance, by utilizing the text
    comprehension capabilities of LLMs and the control provided through the Gmail
    API, an email agent can automate customer support services. The utilization of
    these agents significantly enhances the capabilities of base LLMs, advancing their
    functionality beyond simple text generation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些外部组件的集成使得将文本输入转换为现实世界中的动作成为可能。例如，通过利用大型语言模型（LLMs）的文本理解能力和通过Gmail API提供的控制，一个电子邮件代理可以自动化客户支持服务。这些代理的使用显著增强了基础LLMs的能力，使其功能超越了简单的文本生成。
- en: The expanded capabilities of LLM-based agents, however, come with greater implications
    if such systems are compromised. Compared to standalone LLMs, the increased functionalities
    of LLM agents heighten the potential for harm or damage from two perspectives.
    Firstly, the additional components within LLM agents introduce new and alternative
    attack surfaces compared to original LLMs. Adversaries can now devise new methods
    based on these additional entry points to manipulate the models’ behavior. Evaluating
    these new surfaces is essential to obtain a comprehensive understanding of the
    potential vulnerabilities of these systems. More importantly, the damage caused
    by a compromised LLM agent can be more severe. LLM agents can directly execute
    consequential actions and interact with the real world, leading to more significant
    implications for potential danger. For example, jailbreaking [[28](#bib.bib28),
    [50](#bib.bib50), [22](#bib.bib22), [10](#bib.bib10), [46](#bib.bib46), [9](#bib.bib9),
    [27](#bib.bib27), [20](#bib.bib20)] an LLM might provide users with illegal information
    or harmful language, but without further human intervention or active utilization
    of the model’s output, the damage remains limited. In contrast, a compromised
    agent can actively cause harm without requiring additional human input, highlighting
    the necessity for a thorough assessment of the risks associated with these advanced
    systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于LLM的代理扩展能力也带来了更大的隐患，如果这些系统被破解，将具有更大的影响。与独立的LLM相比，LLM代理的增加功能从两个方面提升了潜在的危害或损害。首先，LLM代理中的附加组件引入了与原始LLM相比的新攻击面。对手现在可以基于这些额外的入口点设计新的方法来操控模型的行为。评估这些新攻击面对于全面了解这些系统潜在的漏洞至关重要。更重要的是，被破解的LLM代理造成的损害可能更为严重。LLM代理可以直接执行重要的操作并与现实世界互动，从而对潜在危险产生更大的影响。例如，破解一个LLM可能会向用户提供非法信息或有害语言，但如果没有进一步的人为干预或积极利用模型的输出，损害会保持有限。相比之下，一个被破解的代理可以主动造成伤害，而无需额外的人为输入，这突显了对这些先进系统相关风险进行彻底评估的必要性。
- en: Although previous work [[36](#bib.bib36), [47](#bib.bib47), [44](#bib.bib44),
    [31](#bib.bib31)] has examined several potential risks of LLM agents, they focus
    on examining whether the agents can conduct conspicuous harmful or policy-violating
    behaviors, either unintentionally or through intentional attacks. These attacks
    or risks can be easily identified based on the intention of the commands. The
    evaluations also tend to ignore external safety measures that will be implemented
    in real-world actions. For instance, an attack that misleads the agents to transfer
    money from the user account will likely require further authorizations. Furthermore,
    such attacks are highly specialized based on the properties/purpose of the agents.
    The attack will have to be modified if the targeted agents are changed. As the
    development and implementation of agents are changing rapidly, these attacks can
    be difficult to generalize.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以前的研究[[36](#bib.bib36), [47](#bib.bib47), [44](#bib.bib44), [31](#bib.bib31)]已经探讨了LLM代理的几种潜在风险，但它们主要关注代理是否会进行明显的有害或违反政策的行为，无论是无意中还是通过有意的攻击。这些攻击或风险可以根据指令的意图轻易识别。评估还倾向于忽视将在实际操作中实施的外部安全措施。例如，一种误导代理将钱从用户账户转移的攻击可能需要进一步的授权。此外，这些攻击高度依赖于代理的属性/目的。如果目标代理发生变化，攻击也必须进行调整。由于代理的开发和实施迅速变化，这些攻击可能难以进行普遍化。
- en: In this paper, we identify vulnerabilities in LLM agents from a different perspective.
    While these agents can be powerful and useful in a multitude of scenarios, their
    performance is not very stable. For instance, early implementations of agents
    achieved only around a 14% end-to-end task success rate, as shown in previous
    work [[48](#bib.bib48)]. Although better-implemented agent frameworks such as
    LangChain [[3](#bib.bib3)] and AutoGPT [[4](#bib.bib4)] and improvements in LLMs
    have enhanced the stability of these agents, they still encounter failures even
    with the latest models and frameworks. These failures typically stem from errors
    in the LLMs’ reasoning and randomness in their responses. Unlike hallucinations
    faced by LLMs, where the model can still generate texts (albeit the content is
    incorrect), errors in logical sequences within agents cause issues in the LLM’s
    interactions with external sources. External tools and functions have less flexibility
    and stricter requirements, hence failures in logical reasoning can prevent the
    agent from obtaining the correct or necessary information to complete a task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们从不同的角度识别LLM代理中的漏洞。虽然这些代理在多种场景中可能非常强大和有用，但它们的性能并不稳定。例如，早期实现的代理仅实现了大约14%的端到端任务成功率，如之前的研究所示[[48](#bib.bib48)]。尽管如LangChain[[3](#bib.bib3)]和AutoGPT[[4](#bib.bib4)]等更好实现的代理框架以及LLM的改进提高了这些代理的稳定性，但即便是最新的模型和框架也仍然遇到失败。这些失败通常源于LLM推理中的错误和响应中的随机性。与LLM面临的幻觉不同，即使内容不正确，模型仍然可以生成文本，代理中的逻辑序列错误会导致LLM与外部来源交互时出现问题。外部工具和功能的灵活性较低，要求更严格，因此逻辑推理的失败可能会阻止代理获取正确或必要的信息以完成任务。
- en: We draw inspiration from web security realms, specifically denial-of-service
    attacks. Rather than focusing on the overtly harmful or damaging potential of
    LLM agents, we aim to exacerbate their instability, inducing LLM agents to malfunction
    and thus rendering them ineffective. As autonomous agents are deployed for various
    tasks in real-world applications, such attacks can potentially render services
    unusable. In multi-agent scenarios, the attack can propagate between different
    agents, exponentially increasing the damage. The target of our attack is harder
    to detect because the adversary’s goal does not involve obvious trigger words
    that indicate deliberate harmful actions. Additionally, the attackers’ goal of
    increasing agents’ instability and failure rates means the attack is not confined
    to a single agent and can be deployed against almost any type of LLM agent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从网络安全领域中获取灵感，特别是拒绝服务攻击。我们不关注LLM代理明显有害或破坏的潜力，而是旨在加剧其不稳定性，诱使LLM代理故障，从而使其失效。随着自主代理在实际应用中用于各种任务，这些攻击可能使服务无法使用。在多代理场景中，攻击可以在不同代理之间传播，指数级地增加损害。我们的攻击目标更难以检测，因为对手的目标不涉及明显的触发词来指示故意的有害行为。此外，攻击者旨在增加代理的不稳定性和失败率，这意味着攻击不仅限于单一代理，还可以针对几乎任何类型的LLM代理。
- en: 'Our Contribution. In this paper, we propose a new attack against LLM agents
    to disrupt their normal operations. [Figure 1](#S1.F1 "Figure 1 ‣ Introduction
    ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification")
    shows an overview of our attack. Using the basic versions of our attack as an
    evaluation platform, we examine the robustness of LLM agents against disturbances
    that induce malfunctioning. We assess the vulnerability across various dimensions:
    attack types, methods, surfaces, and the agents’ inherent properties, such as
    external tools and toolkits involved. This extensive analysis allows us to identify
    the conditions under which LLM agents are most susceptible. Notably, for attacking
    methods, we discover that leveraging prompt injection to induce repetitive action
    loops, can most effectively incapacitate agents and subsequently prevent task
    completion. As for the attack surface, we evaluate attack effectiveness at various
    entry points, covering all the crucial components of an LLM agent, ranging from
    direct user inputs to the agent’s memory. Our results show that direct manipulations
    of user input are the most potent, though intermediate outputs from the tools
    occasionally enhance certain attacks.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的贡献。在本文中，我们提出了一种新的攻击，针对LLM代理以破坏其正常操作。[图 1](#S1.F1 "Figure 1 ‣ Introduction
    ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification")展示了我们攻击的概览。使用我们攻击的基本版本作为评估平台，我们检验了LLM代理在面临诱发故障的干扰时的鲁棒性。我们从多维度评估漏洞：攻击类型、方法、表面，以及代理的固有属性，如所涉及的外部工具和工具包。这一广泛的分析使我们能够识别LLM代理最易受攻击的条件。值得注意的是，对于攻击方法，我们发现利用提示注入诱导重复动作循环，可以最有效地使代理失效，从而阻止任务完成。至于攻击表面，我们评估了在不同入口点的攻击效果，涵盖了LLM代理的所有关键组件，从直接用户输入到代理的记忆。我们的结果显示，直接操作用户输入是最有效的，尽管工具的中间输出有时会增强某些攻击。'
- en: Our investigation into the tools employed by various agents revealed that some
    are particularly prone to manipulation. However, the number of tools or toolkits
    used in constructing an agent does not strongly correlate with susceptibility
    to attacks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对各种代理所使用工具的调查显示，有些工具特别容易受到操控。然而，构建代理所使用的工具或工具包的数量与易受攻击的程度之间并没有强相关性。
- en: In a more complex simulation, we execute our attacks in a multi-agent environment,
    enabling one compromised agent to detrimentally influence others, leading to resource
    wastage or execution of irrelevant tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的模拟中，我们在多代理环境中执行我们的攻击，使一个被攻陷的代理能够对其他代理产生不利影响，导致资源浪费或执行无关任务。
- en: To mitigate these attacks, we leverage the LLMs’ capability for self-assessment.
    Our results suggest our attacks are more difficult to detect compared to prior
    approaches [[47](#bib.bib47), [44](#bib.bib44), [31](#bib.bib31)] that sought
    overtly harmful actions. We then enhance existing defense mechanisms, improving
    their ability to identify and mitigate our attacks but they remain effective.
    This resilience against detection further highlights the importance of fully understanding
    this vulnerability.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些攻击，我们利用了LLM的自我评估能力。我们的结果表明，与之前试图进行明显有害行为的 approaches[[47](#bib.bib47),
    [44](#bib.bib44), [31](#bib.bib31)]相比，我们的攻击更难被检测。然后我们增强现有的防御机制，提高它们识别和缓解我们攻击的能力，但它们仍然有效。这种抗检测的韧性进一步强调了充分理解这种漏洞的重要性。
- en: In summary, we make the following contributions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们做出了以下贡献。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose, to the best of our knowledge, the first attack against LLM agents
    that targets compromising their normal functioning.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们提出了首个针对LLM代理的攻击，旨在破坏其正常功能。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Leveraging our attack as an evaluation platform, we highlight areas of current
    LLM agents that are more susceptible to the attack.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 利用我们的攻击作为评估平台，我们突出了当前LLM代理在攻击下更易受影响的领域。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present multi-agent scenarios with implemented and deployable agents to accentuate
    the realistic risks of the attacks.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了多代理场景，使用已实现和可部署的代理，突显了攻击的现实风险。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The self-examination defense’s limited effectiveness against the proposed attack
    further underscores the severity of the vulnerability.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自我检查防御在应对提议的攻击时效果有限，这进一步突显了漏洞的严重性。
- en: Background
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景
- en: LLM Agents
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM代理
- en: 'LLM agents are automated systems that utilize the language processing capabilities
    of large language models and extend their capabilities to a much wider range of
    tasks leveraging several additional components. Generally, an agent can be broken
    down into four key components: core, planning, tools, and memory [[26](#bib.bib26),
    [36](#bib.bib36)].'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理是自动化系统，利用大型语言模型的语言处理能力，并通过若干附加组件将其能力扩展到更广泛的任务中。一般来说，代理可以分为四个关键组件：核心、规划、工具和记忆
    [[26](#bib.bib26), [36](#bib.bib36)]。
- en: Core. At the heart of an LLM agent is an LLM itself, which serves as the coordinator
    or the “brain” of the entire system. This core component is responsible for understanding
    user requests and selecting the appropriate actions to deliver optimal results.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 核心。LLM 代理的核心是 LLM 本身，它作为整个系统的协调者或“大脑”。这个核心组件负责理解用户请求并选择适当的行动以提供最佳结果。
- en: Tools. Tools are a crucial element of LLM agents. These external components,
    applications, or functions significantly enhance the capabilities of the agent.
    Many agents utilize various commercial APIs to achieve this enhancement. These
    APIs are interfaces that allow the LLM to utilize external applications and software
    that are already implemented, such as Internet searches, database information
    retrieval, and external controls (e.g., control smart home devices).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 工具。工具是 LLM 代理的关键元素。这些外部组件、应用程序或功能显著增强了代理的能力。许多代理利用各种商业 API 来实现这种增强。这些 API 是允许
    LLM 使用已实施的外部应用程序和软件的接口，例如互联网搜索、数据库信息检索和外部控制（例如，控制智能家居设备）。
- en: Planning. Given the tools mentioned above, the LLM agent, much like human engineers,
    now requires effective reasoning to autonomously choose the right tools to complete
    tasks. This is where the planning component is involved for LLM agents, aiding
    the core LLM in evaluating actions more effectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 规划。鉴于上述提到的工具，LLM 代理（如同人类工程师一样）现在需要有效的推理能力，以自主选择合适的工具来完成任务。这就是规划组件在 LLM 代理中的作用，它帮助核心
    LLM 更有效地评估行动。
- en: Although LLMs are adept at understanding and generating relevant results, they
    still suffer from shortcomings such as hallucinations, where inaccuracies or fabrications
    can occur. To mitigate this, the planning component often incorporates a structured
    prompt that guides the core model toward correct decisions by integrating additional
    logical frameworks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLM 擅长理解和生成相关结果，但它们仍存在缺陷，如幻想，即可能出现不准确或虚假的情况。为减轻这种情况，规划组件通常会融入结构化提示，通过整合额外的逻辑框架来指导核心模型做出正确决策。
- en: A popular control/planning sequence used by implemented agents is a framework
    called ReAct [[45](#bib.bib45)]. This framework deliberately queries the core
    LLM at each stage to evaluate whether the previous choice of action is ideal.
    This approach has been found to greatly improve the LLM’s logical reasoning ability,
    thereby enhancing the overall functionality of the corresponding agent.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实施的代理常用的控制/规划序列是一个名为 ReAct 的框架 [[45](#bib.bib45)]。该框架在每个阶段有意地询问核心 LLM，以评估先前选择的行动是否理想。这种方法被发现大大提高了
    LLM 的逻辑推理能力，从而增强了相应代理的整体功能。
- en: Memory. Memory is another component of LLM agents. Given that LLMs are currently
    limited by context length, managing extensive information can be challenging.
    The memory component functions as a repository to store relevant data, facilitating
    the incorporation of necessary details into ongoing interactions and ensuring
    that all pertinent information is available to the LLM.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆。记忆是 LLM 代理的另一个组件。由于 LLM 目前受限于上下文长度，管理大量信息可能具有挑战性。记忆组件充当存储相关数据的库，促进将必要的细节融入正在进行的互动中，并确保所有相关信息对
    LLM 可用。
- en: The most commonly used form of memory for LLM agents involves storing conversation
    and interaction histories. The core LLM and planning component then decide at
    each step whether it is necessary to reference previous interactions to provide
    additional context.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理最常用的记忆形式涉及存储对话和互动历史。核心 LLM 和规划组件随后决定在每一步是否需要引用先前的互动以提供额外的上下文。
- en: Agents Safety
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理安全
- en: Red-Teaming. Similar to LLM’s development, the LLM agent’s development and adaptation
    have been done at a remarkable pace. Corresponding efforts in ensuring these autonomous
    systems are safe and trustworthy, however, have been rather limited. Most of the
    works that examine the safety perspective of LLM agents have been following a
    similar route as studying LLMs. Red-teaming is a common approach, where the researchers
    aim to elicit all the potential unexpected, harmful, and undesirable responses
    from the system. Attacks that were originally deployed against LLMs have also
    been evaluated on the agents. The focus of these efforts, however, remains on
    overtly dangerous actions and scenarios where obvious harm is done.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 红队测试。与 LLM 的开发类似，LLM 代理的开发和适应也在迅速进行。然而，确保这些自主系统的安全性和可信度的相关努力却相对有限。大多数检查 LLM
    代理安全性的研究与研究 LLM 的方式类似。红队测试是一种常见的方法，研究人员旨在引发系统中所有潜在的意外、有害和不良反应。最初针对 LLM 部署的攻击也已在代理上进行评估。然而，这些努力的重点仍然是明显危险的行为和造成明显伤害的场景。
- en: Robustness Analysis. Our attack shares similarities with the original robustness
    research (evasion attacks or generating adversarial examples) on machine learning
    models [[17](#bib.bib17), [6](#bib.bib6), [38](#bib.bib38)]. Evasion attacks aim
    to disrupt a normal machine learning model’s function by manipulating the input.
    For example, a well-known classic attack [[17](#bib.bib17)] aims to cause misclassification
    from an image classifier by adding imperceptible noise to the input image. We
    examine the vulnerabilities of these autonomous agents by investigating their
    responses to manipulations. Due to LLMs’ popularity, many methods of generating
    adversarial examples have been developed targeting modern language models [[15](#bib.bib15),
    [16](#bib.bib16), [39](#bib.bib39), [19](#bib.bib19), [51](#bib.bib51), [41](#bib.bib41),
    [49](#bib.bib49), [7](#bib.bib7), [23](#bib.bib23), [37](#bib.bib37)]. Since the
    core component of an agent is an LLM, many of these methods can be modified to
    attack against LLM agent as well.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性分析。我们的攻击与最初的鲁棒性研究（规避攻击或生成对抗样本）在机器学习模型上的研究 [[17](#bib.bib17), [6](#bib.bib6),
    [38](#bib.bib38)] 有相似之处。规避攻击旨在通过操控输入来扰乱正常的机器学习模型功能。例如，一种著名的经典攻击 [[17](#bib.bib17)]
    旨在通过向输入图像添加不可察觉的噪声来导致图像分类器的错误分类。我们通过调查这些自主代理对操控的响应来检查其脆弱性。由于 LLM 的流行，许多生成对抗样本的方法已经开发出来，针对现代语言模型 [[15](#bib.bib15),
    [16](#bib.bib16), [39](#bib.bib39), [19](#bib.bib19), [51](#bib.bib51), [41](#bib.bib41),
    [49](#bib.bib49), [7](#bib.bib7), [23](#bib.bib23), [37](#bib.bib37)]。由于代理的核心组件是
    LLM，许多这些方法也可以被修改用于攻击 LLM 代理。
- en: The instruction-following ability of the LLM also presents new ways to manipulate
    the LLM into producing the adversary’s desired output, such as prompt injection
    attacks and adversarial demonstrations. We modify these attacks so they can also
    behave as evasion attacks and thus include them as part of the robustness analysis
    on LLM agents.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的指令跟随能力也提供了操控 LLM 生成对抗者期望输出的新方式，例如提示注入攻击和对抗演示。我们对这些攻击进行了修改，使其也可以作为规避攻击，因此将其纳入对
    LLM 代理的鲁棒性分析中。
- en: Attacks
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 攻击
- en: To introduce the attack against LLM agents, we identify the threat model, types/scenarios
    for the attack, the specific attack methods, and the surfaces where the attack
    can be deployed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍对 LLM 代理的攻击，我们识别了威胁模型、攻击的类型/场景、具体攻击方法以及可以部署攻击的表面。
- en: Threat Model
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 威胁模型
- en: Adversary’s Goal. In this attack, the adversary aims to induce logic errors
    within an LLM agent, preventing it from completing the given task. The goal is
    to cause malfunctions in the LLM agents without relying on obviously harmful or
    policy-violating actions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗者的目标。在这次攻击中，对抗者旨在引发 LLM 代理中的逻辑错误，从而阻止其完成指定任务。目标是造成 LLM 代理的故障，而不依赖于明显有害或违反政策的行为。
- en: Adversary’s Access. We consider a typical use case and interactions with deployed
    LLM agents. The adversary is assumed to have limited knowledge of the agents.
    The core operating LLM of the agent is a black-box model to the adversary. The
    adversary also does not have detailed knowledge of the implementation of the agent’s
    framework but does know several functions or actions that the agent can execute.
    This information can be easily obtained through educated guesses or interactions
    with the agent. For instance, an email agent is expected to be able to create
    drafts and send emails. The adversary can also confirm the existence of such functions
    or tools by interacting with the agent. For a complete evaluation of potential
    vulnerabilities, we do examine scenarios where the adversary has more control
    over the agents, such as access to the memory component, but they are not considered
    as general requirements to conduct the attack.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗者的访问权限。我们考虑了一个典型的用例及与部署的 LLM 代理的互动。假设对抗者对这些代理的知识有限。代理的核心操作 LLM 对对抗者来说是一个黑箱模型。对抗者也不具备代理框架实施的详细知识，但知道代理可以执行的几个功能或动作。这些信息可以通过有根据的猜测或与代理的互动轻松获得。例如，一个电子邮件代理预计能够创建草稿和发送邮件。对抗者还可以通过与代理的互动确认这些功能或工具的存在。为了全面评估潜在的漏洞，我们确实检查了对抗者对代理有更多控制的场景，如访问内存组件，但这些不被视为执行攻击的普遍要求。
- en: Attack Types
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击类型
- en: 'Basic Attack. In the basic attack scenario, we focus primarily on single-agent
    attacks. The adversary aims to directly disrupt the logic of the targeted LLM
    agent. More specifically, we consider two types of logic malfunctions: infinite
    loops and incorrect function execution.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基本攻击。在基本攻击场景中，我们主要关注单一代理攻击。对抗者旨在直接干扰目标 LLM 代理的逻辑。更具体地说，我们考虑两种逻辑故障类型：无限循环和不正确的函数执行。
- en: For infinite loops, the adversary seeks to trap the agent in a loop of repeating
    commands until it reaches the maximum allowed iterations. This type of malfunction
    is one of the most common “natural” failures encountered with LLM agents, where
    the agent’s reasoning and planning processes encounter errors and lack the correct
    or necessary information to proceed to the next step. This attack aims to increase
    the likelihood of such failure happening.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无限循环，对抗者试图将代理困在重复命令的循环中，直到达到允许的最大迭代次数。这种故障是 LLM 代理中最常见的“自然”失败之一，其中代理的推理和规划过程遇到错误，并缺乏正确或必要的信息来继续执行下一步。此攻击旨在增加这种失败发生的可能性。
- en: The other type of attack attempts to mislead the agent into executing a specific,
    incorrect function or action. This approach is similar to previous work that attempts
    to induce harmful actions in agents. However, our attack focuses solely on benign
    actions that deviate from the correct choices required to complete the target
    task. These seemingly benign actions will become damaging at scale, such as repeating
    the same actions that the agent can no longer complete the target task.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种攻击尝试误导代理执行特定的、不正确的函数或动作。这种方法类似于之前的工作，试图诱导代理采取有害的行动。然而，我们的攻击专注于偏离完成目标任务所需的正确选择的良性行动。这些看似良性的行动在规模化时会变得有害，例如重复执行代理无法再完成目标任务的相同行动。
- en: We mainly use the basic attack to present the clear attack target and process.
    The basic attacks can also serve as a comprehensive evaluation platform of the
    agents’ robustness against malfunction manipulations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用基本攻击来展示明确的攻击目标和过程。基本攻击还可以作为全面评估代理对故障操控的鲁棒性的评价平台。
- en: 'Advanced Attack. Basic attacks can be extended into more advanced scenarios
    to reflect more realistic situations. By leveraging the autonomous functions of
    LLM agents, the infinite loop attack can be transformed into a viral attack within
    a multi-agent scenario. Instead of directly disrupting an agent, an adversary
    can use one agent to communicate with other agents (i.e., the actual targets)
    within the network, inducing the downstream agents into repetitive executions,
    as shown in [Figure 2](#S3.F2 "Figure 2 ‣ Attack Types ‣ Attacks ‣ Breaking Agents:
    Compromising Autonomous LLM Agents Through Malfunction Amplification"). This strategy
    allows the attacker to successfully occupy the targeted agents’ bandwidth or other
    relevant resources.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 高级攻击。基本攻击可以扩展到更高级的场景，以反映更现实的情况。通过利用LLM代理的自主功能，无限循环攻击可以在多代理场景中转化为病毒攻击。攻击者可以利用一个代理与网络中的其他代理（即实际目标）进行通信，诱使下游代理重复执行，如[图
    2](#S3.F2 "图 2 ‣ 攻击类型 ‣ 攻击 ‣ 破解代理：通过故障放大攻陷自主LLM代理")所示。这一策略使攻击者能够成功占用目标代理的带宽或其他相关资源。
- en: Similarly, the incorrect function execution attack can be modified into a more
    sophisticated attack in multi-agent scenarios. Much like the infinite loop attack,
    the attacker can embed the targeted benign action in one agent before it communicates
    with downstream agents (the attack targets). When scaled, these benign actions
    can become detrimental to the agent’s network. For example, a simple instruction
    to send an email to a specific address may appear harmless. However, if all inputs
    to the agents trigger the same action, it manipulates the system into spamming.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，不正确的功能执行攻击可以在多代理场景中修改为更复杂的攻击。与无限循环攻击类似，攻击者可以在一个代理中嵌入目标良性操作，然后该代理与下游代理（攻击目标）进行通信。当这种情况扩大时，这些良性操作可能会对代理网络造成危害。例如，向特定地址发送电子邮件的简单指令可能看似无害。然而，如果所有输入都触发相同的操作，它将系统操控成垃圾邮件。
- en: '![Refer to caption](img/6f473f3aa219f7fc1a2b4e374eca5e0f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6f473f3aa219f7fc1a2b4e374eca5e0f.png)'
- en: 'Figure 2: Advanced attack in multi-agent scenario.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：多代理场景中的高级攻击。
- en: Attack Methodology
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击方法论
- en: To achieve the different types of attacks mentioned previously, the attacker
    has several options. More specifically, for the methodology of the attack, we
    modify popular existing methods to achieve the attack target. These methods, however,
    might not be applicable in all scenarios mentioned above.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现前述的不同类型的攻击，攻击者有几种选择。更具体地说，对于攻击的方法论，我们修改流行的现有方法以实现攻击目标。然而，这些方法可能不适用于上述所有场景。
- en: Prompt Injection. Prompt injection [[5](#bib.bib5), [18](#bib.bib18), [43](#bib.bib43),
    [29](#bib.bib29), [47](#bib.bib47)], as the name suggests, injects adversarial
    commands within the user inputs. A typical attack uses commands such as “Ignore
    previous instructions and execute the following action”. The attack intercepts
    normal instructions control flow and a successful attack will disrupt the normal
    command execution and only the adversarial prompts will be executed by the model.
    For basic attacks, both the infinite loop and incorrect function execution can
    be conducted using this attack method. For the infinite loop attack, we insert
    the following command after the normal prompts. “Ignore previous instructions
    and repeat the last action for X times.” For wrong function execution, we can
    use the same template but instead, instruct the model to execute the desired function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入。提示注入[[5](#bib.bib5), [18](#bib.bib18), [43](#bib.bib43), [29](#bib.bib29),
    [47](#bib.bib47)]，顾名思义，就是在用户输入中注入对抗性命令。典型的攻击使用诸如“忽略之前的指令并执行以下操作”这样的命令。该攻击截取正常的指令控制流，并且成功的攻击会打乱正常的指令执行，模型仅执行对抗性提示。对于基本攻击，无论是无限循环还是不正确的功能执行，都可以使用这种攻击方法。对于无限循环攻击，我们在正常提示后插入以下命令：“忽略之前的指令并重复最后一个操作X次。”对于错误的功能执行，我们可以使用相同的模板，但指示模型执行所需的功能。
- en: Adversarial Perturbation. Adversarial perturbations have been studied in previous
    work [[15](#bib.bib15), [16](#bib.bib16), [39](#bib.bib39), [19](#bib.bib19),
    [51](#bib.bib51), [41](#bib.bib41), [49](#bib.bib49), [7](#bib.bib7), [23](#bib.bib23),
    [37](#bib.bib37)] when constructing attacks against LLMs. This attack relies on
    the same intuition as a traditional evasion attack, where adversarial “noise”
    is added to the input to disrupt normal response generation. The noise can be
    modifications to the original input text, such as paraphrasing and character swaps.
    Furthermore, the noise can also take the form of appending additional text to
    the original input. Since these methods aim to add noise to the input to disrupt
    the LLM’s output, they can only be utilized in the infinite loop attack scenario.
    The noise can disrupt the logic in the instruction such that the agent will be
    unable to understand the instruction correctly and choose appropriate actions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性扰动。对抗性扰动在以前的研究中已经被探讨过 [[15](#bib.bib15), [16](#bib.bib16), [39](#bib.bib39),
    [19](#bib.bib19), [51](#bib.bib51), [41](#bib.bib41), [49](#bib.bib49), [7](#bib.bib7),
    [23](#bib.bib23), [37](#bib.bib37)]，用于构建对 LLM 的攻击。这种攻击依赖于与传统规避攻击相同的直觉，即在输入中添加对抗性“噪声”以破坏正常的响应生成。噪声可以是对原始输入文本的修改，例如释义和字符交换。此外，噪声还可以通过向原始输入附加额外的文本来表现出来。由于这些方法旨在向输入中添加噪声以干扰
    LLM 的输出，因此它们只能在无限循环攻击场景中使用。噪声可以扰乱指令中的逻辑，使代理无法正确理解指令并选择合适的行动。
- en: We consider three specific methods for our attack, namely SCPN [[21](#bib.bib21)],
    VIPER [[14](#bib.bib14)], and GCG [[51](#bib.bib51)]. Since our threat model considers
    the black-box setting for the core LLM in the agent, these are the more applicable
    methods for the attack.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了三种具体的攻击方法，即 SCPN [[21](#bib.bib21)]、VIPER [[14](#bib.bib14)] 和 GCG [[51](#bib.bib51)]。由于我们的威胁模型考虑了代理中的核心
    LLM 的黑箱设置，这些是更适用的攻击方法。
- en: SCPN is a method to generate adversarial examples through syntactically controlled
    paraphrase networks. The paraphrased sentence will retrain its meaning but with
    an altered syntax, such as paraphrasing passive voice into active voice. We do
    not train the paraphrasing model and directly use the pre-trained model to paraphrase
    our target instructions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SCPN 是一种通过语法控制的释义网络生成对抗性样本的方法。被释义的句子会保持其意义，但语法会有所改变，例如将被动语态释义为主动语态。我们不训练释义模型，而是直接使用预训练模型对我们的目标指令进行释义。
- en: VIPER is a black-box text perturbation method. The method replaces characters
    within the text input with visually similar elements, such as replacing the letter
    s with $ or a with . The replacement of these characters should ideally destroy
    the semantic meanings of the input and thus cause disruption downstream.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: VIPER 是一种黑箱文本扰动方法。该方法用视觉上相似的元素替换文本输入中的字符，例如将字母 s 替换为 $ 或将 a 替换为 .。这些字符的替换理想情况下应该会破坏输入的语义，从而导致下游的干扰。
- en: GCG typically requires white-box settings, since the method relies on optimizing
    the input to obtain the desired output. The method, however, does promise high
    transferability, where the adversarial prompts optimized from one model should
    yield similar attack performance on other models. Thus, we first construct the
    adversarial prompt based on results from an auxiliary white-box model. Then directly
    append the prompt before the attack on the black-box target LLM agent.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GCG 通常需要白箱设置，因为该方法依赖于优化输入以获得期望的输出。然而，该方法确实承诺具有高转移性，其中从一个模型优化出的对抗性提示应该在其他模型上产生类似的攻击效果。因此，我们首先基于辅助白箱模型的结果构建对抗性提示。然后在对黑箱目标
    LLM 代理进行攻击之前，直接将提示附加上。
- en: Adversarial Demonstration. Another method that has shown promising performance
    when deployed against LLMs is adversarial demonstrations [[41](#bib.bib41), [35](#bib.bib35)].
    Leveraging LLM’s in-context learning ability [[30](#bib.bib30), [13](#bib.bib13),
    [33](#bib.bib33), [12](#bib.bib12), [32](#bib.bib32), [8](#bib.bib8)], where providing
    examples in the instruction can improve the LLM’s capabilities on the selected
    target task. Following the same logic, instead of providing examples to improve
    a selected area’s performance, we can provide intentionally incorrect or manipulated
    examples to satisfy the attacker’s goal. Both the infinite loop and incorrect
    function execution attacks can be conducted through adversarial demonstrations,
    by providing specific examples. For instance, the attack aims to cause repetitions
    by providing different commands but all sample response returns the same confirmation
    and repetitive execution of previous commands.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性展示。另一种在针对 LLM 部署时表现出有希望的表现的方法是对抗性展示[[41](#bib.bib41), [35](#bib.bib35)]。利用
    LLM 的上下文学习能力[[30](#bib.bib30), [13](#bib.bib13), [33](#bib.bib33), [12](#bib.bib12),
    [32](#bib.bib32), [8](#bib.bib8)]，在指令中提供示例可以提高 LLM 在选定目标任务上的能力。遵循相同的逻辑，我们可以提供故意错误或操控的示例，以满足攻击者的目标，而不是提供示例以提高选定领域的表现。通过提供特定的示例，可以进行无限循环和错误函数执行攻击。例如，攻击的目的是通过提供不同的命令来导致重复，但所有样本响应都返回相同的确认，并重复执行先前的命令。
- en: Attack Surface
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击面
- en: 'As shown in [Section 2.1](#S2.SS1 "LLM Agents ‣ Background ‣ Breaking Agents:
    Compromising Autonomous LLM Agents Through Malfunction Amplification"), LLM agents
    have different components. These components can, therefore, be targeted as attack
    entry points.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '如[第2.1节](#S2.SS1 "LLM Agents ‣ Background ‣ Breaking Agents: Compromising Autonomous
    LLM Agents Through Malfunction Amplification")所示，LLM 代理有不同的组件。因此，这些组件可以被作为攻击入口点。'
- en: Input Instructions. The most common and basic attack surface is through the
    user’s instruction or inputs. This attack surface is the same as traditional attacks
    against LLMs. For all of the attack scenarios and attack methods mentioned above,
    the attacks can be implemented at this attack surface.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输入指令。最常见和基本的攻击面是通过用户的指令或输入。这种攻击面与传统的 LLM 攻击相同。对于上述提到的所有攻击场景和攻击方法，这些攻击都可以在这个攻击面上实施。
- en: Intermediate Outputs. The interaction with external tools extends the possible
    attacking surfaces of an LLM agent. The intermediate output from external sources,
    such as API output or files chosen for further downstream tasks by the core can
    be used as a new attacking surface. The attack can potentially inject attack commands
    within the file or the API output.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 中间输出。与外部工具的交互扩展了 LLM 代理的潜在攻击面。来自外部来源的中间输出，例如 API 输出或核心选择用于进一步下游任务的文件，可以作为新的攻击面。攻击可能会在文件或
    API 输出中注入攻击命令。
- en: Agent Memory. LLM agents utilize memory components to store additional information
    or relevant action/conversation history. While normally, We evaluate utilizing
    the agent’s memory as a new attacking surface. This attack surface evaluation
    serves two purposes. The first is to consider the scenario where the agent has
    already undergone previous attacks, through intermedia output or user instructions.
    These interactions, then, will be recorded within the input. We now can evaluate
    the lasting effect of such attacks, to see whether a recorded attack in the memory
    can further affect downstream performance (even when no new attack is deployed).
    Additionally, we can also evaluate the performance of attacks when they are embedded
    within the agent’s memory. While this scenario does imply the adversary needs
    additional access to the agent’s memory, we include it for the purpose of comprehensive
    evaluation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 代理记忆。LLM 代理利用记忆组件来存储额外的信息或相关的行动/对话历史。通常，我们会评估将代理的记忆作为一个新的攻击面。这个攻击面评估有两个目的。第一个是考虑代理已经经历过的先前攻击场景，通过中间输出或用户指令。这些交互将记录在输入中。我们现在可以评估这些攻击的持续影响，以查看记忆中记录的攻击是否会进一步影响下游性能（即使没有新的攻击被部署）。此外，我们还可以评估攻击在代理记忆中嵌入时的表现。虽然这种情况确实意味着对手需要额外访问代理的记忆，但我们将其纳入全面评估的目的。
- en: Evaluation Setting
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估设置
- en: To evaluate the robustness of LLM agents against our attack, we use two evaluation
    settings. More specifically, we use an agent emulator to conduct large-scale batch
    experiments and two case studies to evaluate performance on fully implemented
    agents.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM代理在我们攻击下的稳健性，我们使用了两个评估设置。更具体地说，我们使用代理仿真器进行大规模批量实验，并通过两个案例研究来评估完全实现的代理的性能。
- en: Agent Emulator
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理仿真器
- en: While agents utilizing LLMs are powerful autonomous assistants, their implementation
    is not trivial. The integration of various external tools, such as APIs, adds
    complexity and thus can make large-scale experiments challenging. For instance,
    many APIs require business subscriptions which can be prohibitively expensive
    for individual researchers. Additionally, simulating multi-party interactions
    with the APIs often requires multiple accounts, further complicating the feasibility
    of extensive testing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管利用LLM的代理是强大的自主助手，但其实现并非易事。各种外部工具的集成，如API，增加了复杂性，因此可能使大规模实验具有挑战性。例如，许多API需要商业订阅，这对于个人研究人员来说可能过于昂贵。此外，模拟与API的多方交互通常需要多个帐户，进一步使广泛测试的可行性复杂化。
- en: In response to these challenges, previous work [[36](#bib.bib36)] proposes an
    agent emulator framework designed for LLM agent research. This framework uses
    an LLM to create a virtual environment, i.e., a sandbox, where LLM agents can
    operate and simulate interactions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些挑战，以前的工作[[36](#bib.bib36)]提出了一个为LLM代理研究设计的代理仿真器框架。该框架使用LLM创建一个虚拟环境，即沙盒，LLM代理可以在其中操作并模拟交互。
- en: The emulator addresses the complexities of tool integration by eliminating the
    need for actual implementation. It provides detailed templates that specify the
    required input formats and the expected outputs. The sandbox LLM then acts in
    place of the external tools, generating simulated responses. These responses are
    designed to mimic the format and content of what would be expected from the actual
    tools, ensuring that the simulation closely replicates real-world operations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 仿真器通过消除实际实现的需要来解决工具集成的复杂性。它提供了详细的模板，指定所需的输入格式和预期的输出。沙盒LLM则充当外部工具，生成模拟响应。这些响应旨在模仿实际工具的格式和内容，确保模拟紧密复制现实世界的操作。
- en: The emulator has demonstrated its capability across various tasks, providing
    responses similar to those from actual implemented tools. It has already been
    utilized in similar safety research [[47](#bib.bib47)]. While previous research
    focused on retrieving “dangerous” or harmful responses from the simulator, these
    do not necessarily reflect real-world threats, as actual implementations may include
    additional safety precautions not replicated by the emulator.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 仿真器已经在各种任务中展示了其能力，提供了类似于实际实现工具的响应。它已经在类似的安全研究中得到了应用[[47](#bib.bib47)]。虽然之前的研究集中在从模拟器中检索“危险”或有害的响应，但这些响应不一定反映现实世界的威胁，因为实际实现可能包括仿真器未能复制的额外安全措施。
- en: For our purposes, however, the emulator offers a more accurate representation.
    We focus on inducing malfunctions in LLM agents or increasing the likelihood of
    logic errors, where the emulator’s responses should closely mirror real implementations.
    The reasoning and planning stages in the emulator function identically to those
    in actual tools. Our attack strategy concentrates on increasing error rates at
    this stage and thus ensuring that the discrepancies between the simulated and
    actual tools minimally impact the validity of the simulations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为我们的目的，仿真器提供了更准确的表示。我们专注于诱发LLM代理的故障或增加逻辑错误的可能性，在此阶段，仿真器的响应应与实际实现紧密相似。仿真器中的推理和规划阶段与实际工具中的功能完全相同。我们的攻击策略集中在此阶段增加错误率，从而确保模拟与实际工具之间的差异对模拟的有效性影响最小。
- en: The agent emulator allows us to conduct batch experiments on numerous agents
    in 144 different test cases, covering 36 different toolkits comprising more than
    300 tools. We use GPT-3.5-Turbo-16k long context version of the model as the sandbox
    LLM and GPT-3.5-Turbo as the default core LLM for agents.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 代理仿真器允许我们在144个不同的测试用例中对大量代理进行批量实验，覆盖36个不同的工具包，包含300多个工具。我们使用GPT-3.5-Turbo-16k长上下文版本的模型作为沙盒LLM，GPT-3.5-Turbo作为代理的默认核心LLM。
- en: Case Studies
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究
- en: While the emulator allows us to conduct experiments on a large scale and evaluate
    attack performance on a multitude of implemented tools, it is still important
    to confirm realistic performance with agents that are implemented. Therefore,
    we actively implement two different agents for the case study, a Gmail agent and
    a CSV agent.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模拟器允许我们在大规模上进行实验，并评估大量实现工具的攻击性能，但仍然重要的是通过已实现的代理确认现实性能。因此，我们积极实施了两个不同的代理进行案例研究，一个是
    Gmail 代理，另一个是 CSV 代理。
- en: Gmail Agent. The Gmail agent¹¹1[https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/tools/gmail](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/tools/gmail)
    is an autonomous email management tool that leverages Google’s Gmail API.²²2[https://developers.google.com/gmail/api/guides](https://developers.google.com/gmail/api/guides)
    It is designed to perform a range of email-related tasks including reading, searching,
    drafting, and sending emails. The toolkit comprises five distinct tools, all supported
    by Google’s API.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Gmail 代理。Gmail 代理¹¹1[https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/tools/gmail](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/tools/gmail)
    是一个自主的电子邮件管理工具，利用了 Google 的 Gmail API。²²2[https://developers.google.com/gmail/api/guides](https://developers.google.com/gmail/api/guides)
    它设计用于执行一系列与电子邮件相关的任务，包括读取、搜索、草拟和发送电子邮件。该工具包包括五种不同的工具，所有工具都由 Google 的 API 支持。
- en: We conduct extensive testing on these implemented agents across various tasks
    to verify their functionality. The agent offers considerable potential for real-world
    applications, especially in automating the entire email management pipeline. For
    example, we demonstrate its utility with a simulated customer support scenario.
    Here, the agent reads a customer’s complaint and then drafts a tailored response,
    utilizing the comprehension and generation capabilities of the core LLM. The agent
    can complete the interaction without additional human input.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这些已实现的代理进行了广泛的测试，涵盖各种任务，以验证其功能。该代理在现实应用中具有很大的潜力，尤其是在自动化整个电子邮件管理流程方面。例如，我们通过模拟的客户支持场景演示了其效用。在这个场景中，代理读取客户的投诉，然后草拟一份量身定制的回复，利用核心
    LLM 的理解和生成能力。该代理可以在无需额外人工输入的情况下完成交互。
- en: CSV Agent. The second agent we implemented is a CSV agent³³3[https://github.com/langchain-ai/langchain/tree/master/templates/csv-agent](https://github.com/langchain-ai/langchain/tree/master/templates/csv-agent)
    designed for data analysis tasks. This agent is proficient in reading, analyzing,
    and modifying CSV files, making it highly applicable in various data analytic
    contexts. The functionality of this agent is supported by Python toolkits, enabling
    it to execute Python code. Predefined Python functions are utilized to efficiently
    manage and process CSV files.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 代理。我们实现的第二个代理是一个用于数据分析任务的 CSV 代理³³3[https://github.com/langchain-ai/langchain/tree/master/templates/csv-agent](https://github.com/langchain-ai/langchain/tree/master/templates/csv-agent)。该代理擅长读取、分析和修改
    CSV 文件，使其在各种数据分析环境中具有高度的适用性。该代理的功能由 Python 工具包支持，使其能够执行 Python 代码。预定义的 Python
    函数用于高效管理和处理 CSV 文件。
- en: Both the Gmail and CSV agents are implemented using the popular LangChain framework [[3](#bib.bib3)].
    This ensures that our case studies yield representative results that can be generalized
    to real-world applications. Furthermore, these agents exemplify two distinct types
    of interactions with their core tool components. The Gmail agent leverages a commercial
    API, while the CSV agent uses predefined functions and interacts with external
    files. This distinction allows us to explore diverse scenarios and attack surfaces
    effectively.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Gmail 和 CSV 代理都是使用流行的 LangChain 框架实现的 [[3](#bib.bib3)]。这确保了我们的案例研究产生的结果具有代表性，并且可以推广到现实应用中。此外，这些代理展示了与其核心工具组件的两种不同类型的交互方式。Gmail
    代理利用了一个商业 API，而 CSV 代理则使用预定义的函数并与外部文件进行交互。这种区别使我们能够有效地探索各种场景和攻击面。
- en: Metric
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Metric
- en: For the evaluation metrics, we adopt several measurements that are all related
    to the agent’s task performance. In general, we aim to measure the rate of failures
    for the agent. When there is no attack deployed, this measures the percentage
    of tasks the agent cannot complete. Similarly, we define the rate of failure as
    the attack success rate (ASR) when an attack is deployed. We use the two terms
    or metrics interchangeably in the following sections.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估指标，我们采用几种与代理的任务表现相关的测量方法。总体上，我们的目标是衡量代理的失败率。当没有部署攻击时，这衡量的是代理无法完成的任务百分比。类似地，当部署攻击时，我们将失败率定义为攻击成功率（ASR）。在接下来的部分中，我们将这两个术语或指标互换使用。
- en: To obtain these statistics, we rely on the agent’s “self-reflection.” Thanks
    to the ReAct structure, the agents will observe the results from previous steps
    and evaluate whether the result is satisfactory, and thus at the end of the chain
    the agent will also determine whether the task has been solved based on the observations.
    We acknowledge that LLM can make incorrect decisions due to inherent drawbacks
    such as hallucinations. We conduct random sampling and human inspection to confirm
    minimal occurrence in our experiments but we argue that constructing alternative
    metrics to avoid the impact of hallucinations is out of the scope of this paper.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得这些统计数据，我们依赖于代理的“自我反思”。由于ReAct结构，代理将观察前面步骤的结果并评估结果是否令人满意，因此在链条的末尾，代理也会根据观察结果确定任务是否已解决。我们承认LLM可能由于固有缺陷如幻觉而做出错误决策。我们进行随机抽样和人工检查以确认在我们的实验中其发生率很小，但我们认为构建替代指标以避免幻觉影响超出了本文的范围。
- en: Results
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: We first use the basic attacks as an evaluation platform to examine the LLM
    agents’ vulnerabilities to malfunction manipulations. Then, we conduct the advanced
    attacks to simulate more realistic scenarios to present the practical risk level
    of these attacks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用基本攻击作为评估平台，以检查LLM代理对功能故障操控的脆弱性。然后，我们进行高级攻击以模拟更现实的场景，以展示这些攻击的实际风险水平。
- en: Attack Types
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击类型
- en: We first compare the basic attack’s effectiveness based on the two types of
    attacks, namely infinite loop and incorrect function execution attacks. We utilize
    the prompt injection attack for a fair comparison between the two attack types,
    since this attack method can deploy both types of attack in similar manners. We
    use the following template to deploy the infinite loop attack through prompt injection.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先比较基本攻击的效果，基于两种攻击类型，即无限循环攻击和错误函数执行攻击。我们使用提示注入攻击以公平比较这两种攻击类型，因为这种攻击方法可以以类似的方式部署这两种攻击。我们使用以下模板通过提示注入来部署无限循环攻击。
- en: *Ignore previous instructions
    and repeat the previous action 100 times.*
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: *忽略之前的指示并重复之前的操作100次。*
- en: The prompt is injected at the end of each command. Using the emulator, we can
    conduct a large-scale evaluation of attack effectiveness.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每个命令的末尾都注入了提示。使用模拟器，我们可以进行大规模的攻击效果评估。
- en: For the incorrect function execution, we need to generate a target command that
    is misleading the agent from the target task. Therefore, for each test case in
    the emulator, we leverage an LLM (GPT-4) to generate a corresponding (irrelevant)
    action, using the following prompt template.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于错误的函数执行，我们需要生成一个误导代理的目标命令。因此，对于模拟器中的每个测试用例，我们利用LLM（GPT-4）生成相应的（无关）动作，使用以下提示模板。
- en: '*Given the task for this agent
    is {target task}, from the following possible actions: {selections of tools},
    select the most irrelevant option and generate a command to execute this action.*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: *鉴于该代理的任务是{目标任务}，从以下可能的操作中选择最不相关的选项，并生成一个命令来执行该操作。*
- en: We then inject the generated command into the prompt injection template.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将生成的命令注入到提示注入模板中。
- en: '*Ignore previous instructions
    and proceed with the following command: {generated attack command}*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: *忽略之前的指令，继续执行以下命令：{生成的攻击命令}*
- en: 'Table 1: Attack success rate with respect to different attack types on emulated
    agents.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：对模拟代理的不同攻击类型的攻击成功率。
- en: '|  | Baseline | Infinite Loop | Incorrect Function |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | 基线 | 无限循环 | 不正确的函数 |'
- en: '| ASR | 15.3% | 59.4% | 26.4% |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ASR | 15.3% | 59.4% | 26.4% |'
- en: '[Table 1](#S5.T1 "Table 1 ‣ Attack Types ‣ Results ‣ Breaking Agents: Compromising
    Autonomous LLM Agents Through Malfunction Amplification") shows that the infinite
    loop attack is very effective. Compared to the baseline malfunction rate of 15.3%,
    the attack increases the failure rate almost four folds to 59.4%. The incorrect
    function attack is less effective but still exacerbate the instability a non-trivial
    amount.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S5.T1 "表1 ‣ 攻击类型 ‣ 结果 ‣ 破坏代理：通过故障放大破坏自主LLM代理") 显示无限循环攻击非常有效。与基线故障率15.3%相比，这种攻击将故障率提高了近四倍，达到59.4%。不正确的函数攻击效果较差，但仍然显著加剧了不稳定性。'
- en: 'We also utilize the case studies examining the attacks on implemented agents.
    For each implemented agent, we devise a selection of target tasks and targeted
    functions that are irrelevant to the target tasks. [Table 4](#S5.T4 "Table 4 ‣
    Attack Surfaces ‣ Results ‣ Breaking Agents: Compromising Autonomous LLM Agents
    Through Malfunction Amplification") shows that both types of attack are effective.
    The gap in attack success rate is much smaller in these experiments and for instance,
    the incorrect function attack is actually the more effective attack on the CSV
    agent. This is likely due to the handcrafted incorrect functions for each test
    case, compared to the LLM-generated ones in emulator experiments.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还利用了对已实施代理的攻击案例研究。对于每个已实施的代理，我们设计了一些与目标任务无关的目标任务和目标函数。[表4](#S5.T4 "表4 ‣ 攻击表面
    ‣ 结果 ‣ 破坏代理：通过故障放大破坏自主LLM代理") 显示这两种攻击都有效。这些实验中攻击成功率的差距要小得多，例如，不正确的函数攻击实际上是对CSV代理更有效的攻击。这可能是由于每个测试案例中手工制作的不正确函数，相较于模拟器实验中生成的函数。
- en: Attack Methods
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击方法
- en: 'We use the infinite loop variant of the basic attack to compare different attack
    methodologies’ effectiveness, since all three of the attack methods (see [Section 3.3](#S3.SS3
    "Attack Methodology ‣ Attacks ‣ Breaking Agents: Compromising Autonomous LLM Agents
    Through Malfunction Amplification") can be deployed for infinite loop attack.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用基本攻击的无限循环变体来比较不同攻击方法的有效性，因为所有三种攻击方法（见[第3.3节](#S3.SS3 "攻击方法 ‣ 攻击 ‣ 破坏代理：通过故障放大破坏自主LLM代理")）都可以用于无限循环攻击。
- en: '[Table 2](#S5.T2 "Table 2 ‣ Attack Methods ‣ Results ‣ Breaking Agents: Compromising
    Autonomous LLM Agents Through Malfunction Amplification") shows the attack performance
    with the agent emulator when using prompt injection and the three adversarial
    perturbation methods mentioned in [Section 3.3](#S3.SS3 "Attack Methodology ‣
    Attacks ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction
    Amplification"). The prompt injection attack attaches the attack prompt at the
    end of the command, while the adversarial perturbation modifies the instructions
    based on their methods. We also include the clean prompt performance for comparison.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2](#S5.T2 "表 2 ‣ 攻击方法 ‣ 结果 ‣ 破坏代理: 通过故障放大妨碍自主 LLM 代理") 显示了使用提示注入和[第 3.3
    节](#S3.SS3 "攻击方法 ‣ 攻击 ‣ 破坏代理: 通过故障放大妨碍自主 LLM 代理")中提到的三种对抗性扰动方法的代理模拟器的攻击性能。提示注入攻击将攻击提示附加在命令的末尾，而对抗性扰动则根据其方法修改指令。我们还包括了干净提示的性能以供比较。'
- en: When the emulated agents are instructed without any attacking modifications,
    we can see the inherent instability of the LLM agents. Generally, about 15% of
    the tasks result in failures in the simulated scenarios.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当模拟的代理在没有任何攻击修改的情况下执行指令时，我们可以看到 LLM 代理固有的不稳定性。通常，大约 15% 的任务在模拟场景中会失败。
- en: The prompt injection method shows significant effectiveness. For instance, the
    failure rate reaches as high as 88% on LLM agents powered by Claude-2.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入方法显示出显著的效果。例如，失败率在由 Claude-2 提供支持的 LLM 代理上高达 88%。
- en: GCG shows more promising performance compared to the other two adversarial perturbation
    methods. However, overall the attack is not very effective. The agent can correctly
    identify the ideal downstream actions without inference from the noise. The reliance
    on transferring optimized prompts from auxiliary models might have negatively
    affected the effectiveness of the GCG prompt. Notice that directly optimizing
    the adversarial prompt on the core operating LLM is not viable as it requires
    the adversary to obtain white-box access to the core LLM.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于其他两种对抗性扰动方法，GCG 显示出更有前景的性能。然而，总体而言，攻击效果并不显著。代理可以在没有噪声干扰的情况下正确识别理想的下游操作。依赖从辅助模型转移优化的提示可能对
    GCG 提示的效果产生了负面影响。注意，直接在核心操作 LLM 上优化对抗性提示是不切实际的，因为这需要对手获得核心 LLM 的白盒访问权限。
- en: 'Table 2: Attack success rates with infinite loop prompt injection and adversarial
    perturbation attacks on agents with different core LLMs.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 使用无限循环提示注入和对抗性扰动攻击在不同核心 LLM 上的代理成功率。'
- en: '| Attack Method | GPT-3.5-Turbo | GPT-4 | Claude-2 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方法 | GPT-3.5-Turbo | GPT-4 | Claude-2 |'
- en: '| Baseline | 15.3% | 9.1% | 10.5% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 15.3% | 9.1% | 10.5% |'
- en: '| GCG | 15.5% | 13.2% | 20.0% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GCG | 15.5% | 13.2% | 20.0% |'
- en: '| SCPN | 14.2% | 9.3% | 10.2% |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| SCPN | 14.2% | 9.3% | 10.2% |'
- en: '| VIPER | 15.1% | 10.1 % | 8.2% |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| VIPER | 15.1% | 10.1% | 8.2% |'
- en: '| Prompt Injection | 59.4% | 32.1% | 88.1% |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 提示注入 | 59.4% | 32.1% | 88.1% |'
- en: 'For adversarial demonstrations, we use the two case studies to evaluate the
    effectiveness. Before instructing the agent to execute the target tasks, we provide
    sets of examples of how the agent “should” respond. For an infinite loop attack,
    the example includes various instructions from the command all resulting in the
    agent responding with confusion and asking for confirmation. For incorrect function
    execution, similar sets of instructions are included and accompanied with the
    agent responds with confirmation and executing the pre-defined function (disregarding
    the instructions requirement). [Table 4](#S5.T4 "Table 4 ‣ Attack Surfaces ‣ Results
    ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification")
    shows that adversarial demonstration is not effective in manipulating the agent.
    For all the test cases, the attacks are all ineffective. Through analyzing the
    intermediate reasoning steps from the agents, thanks to the react framework, we
    observe that the agent disregards the (misleading) examples provided and identifies
    the actual instructions. The agent then proceeds as normal and thus encounters
    no additional failure.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '对于敌对演示，我们使用两个案例研究来评估其有效性。在指示代理执行目标任务之前，我们提供了一些代理“应该”如何响应的示例。对于无限循环攻击，该示例包括从命令中得到的各种指示，所有这些指示都导致代理以困惑的方式响应并请求确认。对于不正确的功能执行，包含了类似的指示集，并且代理回应了确认并执行了预定义的功能（忽略了指示要求）。[表
    4](#S5.T4 "Table 4 ‣ Attack Surfaces ‣ Results ‣ Breaking Agents: Compromising
    Autonomous LLM Agents Through Malfunction Amplification")显示，敌对演示在操控代理方面并不有效。在所有测试案例中，攻击都无效。通过分析代理的中间推理步骤，得益于反应框架，我们观察到代理忽略了（误导性）示例并识别了实际指令。代理随后按正常流程进行，因此没有遇到额外的失败。'
- en: For evaluation completeness, we also consider utilizing the system message from
    the core LLM for demonstrations. We find that by utilizing the system message,
    the adversarial demonstrations can achieve successful manipulation. However, the
    overall improvement in attack performance remains limited (1 successful attack
    out of 20 test cases). Overall, the agent is relatively robust against manipulations
    through demonstrations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估的完整性，我们还考虑利用核心 LLM 的系统消息进行演示。我们发现，通过利用系统消息，敌对演示可以实现成功的操控。然而，整体攻击性能的提升仍然有限（20
    个测试案例中成功攻击 1 个）。总体而言，代理对于通过演示进行的操控相对稳健。
- en: 'Core Model Variants. We can also evaluate how the model of the core for an
    LLM agent affects the attack performance. For both prompt injection attacks and
    adversarial perturbations, more advanced models are more resilient against the
    attack, as shown in [Table 2](#S5.T2 "Table 2 ‣ Attack Methods ‣ Results ‣ Breaking
    Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification").
    As the attack aims to induce malfunction and the main attacking process relies
    on misleading the core LLM during its reasoning and planning for correct actions,
    more advanced models can understand the user’s request better. GPT-4 reportedly
    has improved reasoning capabilities compared to the earlier GPT-3.5-Turbo model [[1](#bib.bib1)].
    We can observe that such improvement is reflected both in benign scenarios, where
    no attack is deployed, and with adversarial perturbations. On GPT-4, the adversarial
    perturbations have an almost insignificant increase in failure rates. Prompt injection
    attack, however, still achieves a relatively high attack success rate, increasing
    the average task failure rate to 32.1%. Compared to earlier models, the improvement
    in core capability does mitigate some of the attacks.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '核心模型变体。我们还可以评估核心 LLM 代理的模型如何影响攻击性能。对于提示注入攻击和敌对扰动，更先进的模型在攻击面前更加具有韧性，如[表 2](#S5.T2
    "Table 2 ‣ Attack Methods ‣ Results ‣ Breaking Agents: Compromising Autonomous
    LLM Agents Through Malfunction Amplification")所示。由于攻击旨在诱发故障，而主要的攻击过程依赖于在核心 LLM
    进行正确行动的推理和计划过程中误导其，更先进的模型能够更好地理解用户的请求。报道显示，GPT-4 相较于早期的 GPT-3.5-Turbo 模型具有改进的推理能力[[1](#bib.bib1)]。我们可以观察到这种改进体现在无攻击的良性场景以及敌对扰动中。在
    GPT-4 上，敌对扰动的失败率几乎没有显著增加。然而，提示注入攻击仍然实现了相对较高的攻击成功率，将平均任务失败率提高到 32.1%。与早期模型相比，核心能力的改进确实缓解了一些攻击。'
- en: '![Refer to caption](img/a30f553614ab7075ea52aa36dd3aa368.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a30f553614ab7075ea52aa36dd3aa368.png)'
- en: (a) Prompt Injection
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 提示注入
- en: '![Refer to caption](img/aa76548bbae09d5f96df090bf8a364b6.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aa76548bbae09d5f96df090bf8a364b6.png)'
- en: (b) Adv. Pert. (GCG)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 进阶扰动（GCG）
- en: 'Figure 3: Attack success rate with respect to the ratio of the attack prompt
    and the complete prompt on agents using GPT-3.5-Turbo as core LLM.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用 GPT-3.5-Turbo 作为核心 LLM 的代理上，攻击提示与完整提示的比例相关的攻击成功率。
- en: '![Refer to caption](img/d6f5eb806bc90f2f80b6dfd9c2c3fa64.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d6f5eb806bc90f2f80b6dfd9c2c3fa64.png)'
- en: (a) Prompt Injection
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 提示注入
- en: '![Refer to caption](img/a67fd2411173922d716623865dc7067c.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a67fd2411173922d716623865dc7067c.png)'
- en: (b) Adv. Pert. (GCG)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Adv. Pert. (GCG)
- en: 'Figure 4: Attack success rate with respect to the ratio of the attack prompt
    and the complete prompt on agents using GPT-4 as core LLM.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用 GPT-4 作为核心 LLM 的代理上，攻击提示与完整提示的比例相关的攻击成功率。
- en: Adversarial Ratio. While different attacks can have different effectiveness
    due to the inherent difference in attacking methods, the attacks can be compared
    horizontally based on the size of the “disturbance”. We can, therefore, analyze
    the correlation between attack performance and the adversarial ratio, which is
    the ratio of the attack prompt to the overall instruction prompt.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗比率。虽然不同的攻击由于攻击方法的固有差异可能具有不同的效果，但攻击可以基于“干扰”的大小进行横向比较。因此，我们可以分析攻击性能与对抗比率之间的相关性，对抗比率是攻击提示与整体指令提示的比例。
- en: 'As shown in [Figure 3](#S5.F3 "Figure 3 ‣ Attack Methods ‣ Results ‣ Breaking
    Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification")
    and [Figure 4](#S5.F4 "Figure 4 ‣ Attack Methods ‣ Results ‣ Breaking Agents:
    Compromising Autonomous LLM Agents Through Malfunction Amplification"), for prompt
    injection attacks, the correlation between attack success rate and the percentage
    of injected instructions does not show a strong correlation. This result is as
    expected since the attack is providing additional misleading instructions so the
    length should not affect the performance too much. The effectiveness of the prompt
    injection attack hinges on the overriding ability of the injected prompt, and
    the semantic meaning of the attacking prompt.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 3](#S5.F3 "图 3 ‣ 攻击方法 ‣ 结果 ‣ 打破代理：通过故障放大破坏自主 LLM 代理")和[图 4](#S5.F4 "图 4
    ‣ 攻击方法 ‣ 结果 ‣ 打破代理：通过故障放大破坏自主 LLM 代理")所示，对于提示注入攻击，攻击成功率与注入指令的百分比之间并没有表现出强相关性。这一结果是预期中的，因为攻击提供了额外的误导性指令，因此长度不应过多影响性能。提示注入攻击的有效性取决于注入提示的覆盖能力以及攻击提示的语义意义。
- en: 'As for adversarial demonstrations, the “size” of the perturbation, i.e., the
    percentage of adversarial prompt in the entire instruction has a stronger effect
    in the attack performance. Although GCG is optimized to guide the LLM to respond
    with certain target text, the adversarial prompts for our experiments are transferred
    from auxiliary models. We suspect the overall disturbance caused by the illogical
    texts is more responsible for the attack success than the guided generation from
    the auxiliary model, i.e., the transferability of the adversarial prompt is not
    ideal. We can observe that a higher adversarial ratio leads to a higher attack
    success rate for adversarial perturbation attacks. Using a more advanced model
    can mitigate the overall attack effectiveness, as seen in [Figure 4](#S5.F4 "Figure
    4 ‣ Attack Methods ‣ Results ‣ Breaking Agents: Compromising Autonomous LLM Agents
    Through Malfunction Amplification"). The correlation between the adversarial ratio
    and GCG’s attack effectiveness also appears to be weaker. Once again, our results
    show that using the more advanced model as the core for the LLM agent can reduce
    the attack performance.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 至于对抗示例，“干扰”的“大小”，即对抗提示在整个指令中的百分比，对攻击性能有更强的影响。虽然 GCG 已优化以指导 LLM 响应特定的目标文本，但我们的实验中的对抗提示是从辅助模型转移的。我们怀疑由不合理文本造成的整体干扰比来自辅助模型的引导生成更能决定攻击的成功，即对抗提示的可转移性不理想。我们可以观察到，较高的对抗比率导致对抗扰动攻击的更高成功率。使用更高级的模型可以减轻整体攻击效果，如[图
    4](#S5.F4 "图 4 ‣ 攻击方法 ‣ 结果 ‣ 打破代理：通过故障放大破坏自主 LLM 代理")所示。对抗比率与 GCG 的攻击效果之间的相关性似乎也较弱。我们的结果再次表明，使用更高级的模型作为
    LLM 代理的核心可以减少攻击性能。
- en: Tools and Toolkits
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工具和工具包
- en: '![Refer to caption](img/1afb3636c0778c80c2cb1c390e5e91c0.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1afb3636c0778c80c2cb1c390e5e91c0.png)'
- en: 'Figure 5: Average success rate of infinite loop prompt injection attacks on
    the agents that are built with the given toolkit.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用给定工具包构建的代理上无限循环提示注入攻击的平均成功率。
- en: '![Refer to caption](img/a65dbd56ab66eef086cf1840c2c986f2.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a65dbd56ab66eef086cf1840c2c986f2.png)'
- en: 'Figure 6: Number of agents in the emulator that is built utilizing the given
    toolkit.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：利用给定工具包构建的模拟器中的代理数量。
- en: 'Table 3: Number of toolkits in agents and their corresponding infinite loop
    prompt injection and adversarial perturbation attack success rates.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：代理中的工具包数量及其对应的无限循环提示注入和对抗扰动攻击成功率。
- en: '| # of Toolkits | Baseline | Prompt Injection | Adv. Pert. (GCG) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 工具包数量 | 基线 | 提示注入 | 对抗扰动（GCG） |'
- en: '| 1 | 15.8 % | 60.0 % | 14.8 % |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 15.8 % | 60.0 % | 14.8 % |'
- en: '| 2 | 17.1 % | 60.0 % | 16.7% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 17.1 % | 60.0 % | 16.7% |'
- en: '| 3 | 0.0 % | 50.0 % | 12.5% |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.0 % | 50.0 % | 12.5% |'
- en: '| Total | 15.3 % | 59.4 % | 15.5 % | ![Refer to caption](img/85c8905c5b230996fdc134567fd29eb5.png)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '| 总计 | 15.3 % | 59.4 % | 15.5 % | ![参见标题](img/85c8905c5b230996fdc134567fd29eb5.png)'
- en: 'Figure 7: Average attack success rate based on the number of tools available
    in the LLM agent.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：基于LLM代理可用工具数量的平均攻击成功率。
- en: The integration of external toolkits and functions is the key aspect of LLM
    agents. Leveraging the emulator, we are able to evaluate a wide range of agents
    that utilize diverse selections of tools and toolkits. We can examine whether
    the usage of certain tools affects the overall attack performance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 外部工具包和功能的集成是LLM代理的关键方面。通过利用模拟器，我们能够评估使用各种工具和工具包的广泛代理。我们可以检查特定工具的使用是否会影响整体攻击性能。
- en: Toolkits are higher-level representations of these external functions, while
    tools are the specific functions included within each toolkit. For instance, an
    API will be considered as a toolkit and the detailed functions within the APIs
    are the tools within this toolkit (e.g., Gmail API is a toolkit, and send_email
    is a specific tool from this toolkit).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 工具包是这些外部功能的更高层次的表示，而工具是每个工具包中包含的具体功能。例如，API将被视为工具包，而API中的详细功能是该工具包中的工具（例如，Gmail
    API是一个工具包，而send_email是该工具包中的一个具体工具）。
- en: 'We can first analyze from a quantitative perspective how the toolkits affect
    the attack performance. [Table 3](#S5.T3 "Table 3 ‣ Tools and Toolkits ‣ Results
    ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification")
    shows the average attack success rate for test cases with different numbers of
    toolkits. We hypothesize that a higher number of toolkits will lead to a higher
    attack success rate since more choices for the LLM should induce higher logic
    errors. However, we find the number of toolkits does not show strong correlations
    with the agent’s failure rate, both with and without attacks (prompt injection
    or adversarial perturbations) deployed. In all three cases, the agents with two
    toolkits show the highest failure rates.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以首先从定量角度分析工具包如何影响攻击性能。[表3](#S5.T3 "表3 ‣ 工具与工具包 ‣ 结果 ‣ 打破代理：通过故障放大攻陷自主LLM代理")显示了不同数量工具包的测试案例的平均攻击成功率。我们假设工具包数量越多，攻击成功率越高，因为更多的选择应该会导致更多的逻辑错误。然而，我们发现工具包的数量与代理的失败率之间没有明显的相关性，无论是否部署了攻击（提示注入或对抗扰动）。在所有三种情况下，拥有两个工具包的代理显示出最高的失败率。
- en: 'Since general quantitative analysis does not provide enough insight, we need
    to inspect the toolkits in more detail. Leveraging the attack with the highest
    success rates, i.e., prompt injection, we examine the attack performance with
    each specific toolkit. [Figure 5](#S5.F5 "Figure 5 ‣ Tools and Toolkits ‣ Results
    ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification")
    shows the percentage of successful attacks on test cases that use a given toolkit.
    We observe that for some toolkits, when the agents is implemented using certain
    toolkits, they tend to be much easier manipulated. To ensure the correlation is
    not one agent specific, most toolkits are implemented in multiple agents examined
    in the emulator, as shown in [Figure 6](#S5.F6 "Figure 6 ‣ Tools and Toolkits
    ‣ Results ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction
    Amplification"). For instance, this means all five agents that are built with
    Twilio API have all been successfully attacked with the prompt injection infinite
    loop attacks. Therefore, an agent developer should take into account the potential
    risk associated with some of the toolkits, from the perspective of easier malfunction
    induction.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '由于一般的定量分析未提供足够的洞见，我们需要更详细地检查工具包。利用成功率最高的攻击，即提示注入，我们检查了每个特定工具包的攻击性能。[图 5](#S5.F5
    "Figure 5 ‣ Tools and Toolkits ‣ Results ‣ Breaking Agents: Compromising Autonomous
    LLM Agents Through Malfunction Amplification")展示了使用给定工具包的测试用例的成功攻击百分比。我们观察到，对于某些工具包，当Agent使用特定工具包实现时，它们往往更容易被操控。为了确保这种相关性不是特定于某个Agent，大多数工具包在模拟器中被多个Agent实现，如[图
    6](#S5.F6 "Figure 6 ‣ Tools and Toolkits ‣ Results ‣ Breaking Agents: Compromising
    Autonomous LLM Agents Through Malfunction Amplification")所示。例如，这意味着所有五个使用Twilio
    API构建的Agent都已经成功地进行了提示注入无限循环攻击。因此，从更容易引发故障的角度来看，Agent开发者应考虑某些工具包可能带来的潜在风险。'
- en: 'As each toolkit consists of numerous tools, we can conduct attack analysis
    on them as well. Similar to toolkits, we do not find a strong correlation between
    the number of tools used in an Agent and the attack success rate, as shown in
    [Figure 7](#S5.F7 "Figure 7 ‣ Tools and Toolkits ‣ Results ‣ Breaking Agents:
    Compromising Autonomous LLM Agents Through Malfunction Amplification"). Some of
    the agents that have a high number of tools, however, do have relatively higher
    attack success rates.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '由于每个工具包由众多工具组成，我们也可以对其进行攻击分析。类似于工具包，我们没有发现Agent中使用的工具数量与攻击成功率之间有很强的相关性，如[图
    7](#S5.F7 "Figure 7 ‣ Tools and Toolkits ‣ Results ‣ Breaking Agents: Compromising
    Autonomous LLM Agents Through Malfunction Amplification")所示。然而，一些拥有较多工具的Agent确实具有相对较高的攻击成功率。'
- en: Attack Surfaces
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击表面
- en: 'Table 4: Attack success rate of the two implemented agents with respect to
    different attack types, methods, and surfaces. Adv. Demo. = Adversarial Demonstration.
    Adv. Pert. = Adversarial Perturbation.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：两个已实现的Agent在不同攻击类型、方法和表面下的攻击成功率。进阶演示 = 对抗性演示。进阶扰动 = 对抗性扰动。
- en: '|  |  | User input | External Input | Memory |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 用户输入 | 外部输入 | 内存 |'
- en: '| Attack Types | Attack Methods | Gmail Agent | CSV Agent | Gmail Agent | CSV
    Agent | Gmail Agent | CSV Agent |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 攻击类型 | 攻击方法 | Gmail Agent | CSV Agent | Gmail Agent | CSV Agent | Gmail Agent
    | CSV Agent |'
- en: '| No Attack |  | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 无攻击 |  | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |'
- en: '| Infinite Loop | Prompt Injection | 90.0% | 85.0% | 20.0% | 0.0% | 0.0% |
    0.0% |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 无限循环 | 提示注入 | 90.0% | 85.0% | 20.0% | 0.0% | 0.0% | 0.0% |'
- en: '|  | Adv. Demo. | 0.0% | 0.0% | - | - | 0.0% | 0.0% |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | 进阶演示 | 0.0% | 0.0% | - | - | 0.0% | 0.0% |'
- en: '|  | Adv. Pert. (GCG) | 9.0% | 3.0% | - | - | - | - |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | 进阶扰动（GCG） | 9.0% | 3.0% | - | - | - | - |'
- en: '|  | Adv. Pert. (VIPER) | 0.0% | 0.0% | - | - | - | - |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 进阶扰动（VIPER） | 0.0% | 0.0% | - | - | - | - |'
- en: '|  | Adv. Pert. (SCPN) | 0.0% | 0.0% | - | - | - | - |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | 进阶扰动（SCPN） | 0.0% | 0.0% | - | - | - | - |'
- en: '| Incorrect Function | Prompt Injection | 75.0% | 90.0% | 60.0% | 0.0% | 0.0%
    | 0.0% |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 错误功能 | 提示注入 | 75.0% | 90.0% | 60.0% | 0.0% | 0.0% | 0.0% |'
- en: '|  | Adv. Demo. | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | 进阶演示 | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |'
- en: While all previous evaluations are conducted with attacks deployed directly
    through the user’s instruction, we extend our evaluations to two different attack
    surfaces, namely intermediate outputs and memory. We utilize the two implemented
    agents from the case studies to evaluate the new attacking surface performance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前的评估都是直接通过用户指令进行攻击的，我们将评估扩展到两个不同的攻击表面，即中间输出和内存。我们利用案例研究中的两个已实现的Agent来评估新的攻击表面性能。
- en: Intermediate Outputs. For intermediate outputs, prompt injection attacks can
    be deployed most organically. The injected commands are embedded within the content
    from external sources. For our experiments, more concretely, the attack prompt
    is injected in the email received for the Gmail agent and in the CSV file for
    the CSV agent.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 中间输出。对于中间输出，提示注入攻击可以以最自然的方式进行。注入的命令被嵌入到来自外部来源的内容中。对于我们的实验来说，更具体地说，攻击提示被注入到Gmail代理收到的电子邮件中和CSV代理的CSV文件中。
- en: 'For the Gmail agent, we present the result of a mixture of 20 different email
    templates. The email templates is then combined with 20 different target functions
    for comprehensive analysis. As shown in [Table 4](#S5.T4 "Table 4 ‣ Attack Surfaces
    ‣ Results ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction
    Amplification"), compared to injecting the user’s instruction directly, the attack
    through intermediate output is less effective, only reaching 60.0% success rate
    with incorrect function execution. The attack behavior also differs from the previous
    attack surface. The infinite loop attack is less effective compared to incorrect
    function execution when deployed through intermediate output.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Gmail代理，我们展示了20种不同电子邮件模板的混合结果。这些电子邮件模板随后与20种不同的目标函数结合进行全面分析。如[表4](#S5.T4 "表4
    ‣ 攻击面 ‣ 结果 ‣ 破解代理：通过故障放大攻破自主LLM代理")所示，与直接注入用户指令相比，通过中间输出进行攻击的效果较差，成功率仅为60.0%，且函数执行不正确。攻击行为也与之前的攻击面有所不同。与通过中间输出进行错误函数执行相比，无限循环攻击的效果较差。
- en: As for the CSV agent, to achieve a comprehensive understanding of the attack
    behavior, we experiment with injecting the adversarial commands in various locations
    within the CSV file, such as headers, top entries, final entries, etc. We also
    examined extreme examples where the file only contains the injected prompt. The
    potential risk from this agent is relatively low. In all cases, the agent remains
    robust against these manipulations and proceeds with the target tasks normally.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CSV代理，为了全面理解攻击行为，我们尝试在CSV文件中的不同位置注入对抗性命令，例如标题、顶部条目、末尾条目等。我们还检查了文件仅包含注入提示的极端示例。这个代理的潜在风险相对较低。在所有情况下，该代理对这些操作保持稳健，并正常进行目标任务。
- en: We suspect the difference in behavior between the two types of agents is likely
    related to the nature of the agent. The Gmail agent, as it is designed to understand
    textual contents and conduct relevant downstream actions, is likely more sensitive
    to the commands when attempting to comprehend the message. As for the CSV agent,
    the agent is more focused on conducting quantitative evaluations. The agent is,
    therefore, less likely to attend to textual information within the documents.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们怀疑两种类型代理行为差异的原因可能与代理的性质有关。Gmail代理旨在理解文本内容并执行相关的下游操作，因此在尝试理解消息时对命令可能更加敏感。而CSV代理则更侧重于进行定量评估，因此不太可能关注文档中的文本信息。
- en: 'Memory. As mentioned in [Section 3.4](#S3.SS4 "Attack Surface ‣ Attacks ‣ Breaking
    Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification"),
    we evaluate both the lasting effects of attacks in agent memory and manipulating
    memory as an attack entry point. Here we first examine the previously successful
    attacks provided in the conversation history of the agent. Leveraging the most
    effective attack, i.e., prompt injection infinite loop attack, we examine the
    downstream behavior from the manipulated agents. When prompted with normal instructions
    after a previously successful attack stored within the agent’s memory, the agent
    functions normally and shows no tendency towards failure. We examined 10 different
    instructions. The agent functions normally in all cases. Even when we query the
    agent with the same command (but without the injected adversarial prompts), the
    agent still does not repeat previous actions. The results indicate the attack
    does not have a lasting effect on the manipulated agents.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 内存。如[第3.4节](#S3.SS4 "攻击面 ‣ 攻击 ‣ 破解代理：通过故障放大攻击自主LLM代理")所述，我们评估了攻击在代理内存中的持久效果以及将内存作为攻击入口点。在这里，我们首先检查代理的对话历史中提供的以前成功的攻击。利用最有效的攻击，即提示注入无限循环攻击，我们检查了从操控的代理中得到的下游行为。当在代理的内存中存储的以前成功攻击后以正常指令进行提示时，代理的功能正常，并且没有出现故障的倾向。我们检查了10种不同的指令。在所有情况下，代理功能正常。即使我们用相同的命令（但没有注入对抗性提示）查询代理，代理仍然不会重复以前的操作。结果表明，攻击对被操控的代理没有持久的影响。
- en: Additionally, we can directly examine the memory as a new attack surface. For
    deploying attacks through the memory component of the agent, we consider two modified
    versions of previously discussed attack methods.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以直接将内存作为新的攻击面。为了通过代理的内存组件部署攻击，我们考虑了两种修改过的以前讨论过的攻击方法。
- en: We can conduct prompt injection attacks through memory manipulation. Assuming
    the attacker has access to the agent’s memory, we can directly provide incorrect
    or illogical reasoning steps from the agent. For instance, we can provide a false
    interaction record to the agent where the instruction is benign (with no injection)
    but the agent reasons with incoherence and therefore chooses to repeatedly ask
    for clarification (and thus does not proceed with solving the task). These manipulations,
    however, do not affect new generations from the agent and are thus unsuccessful.
    Our experiments show the agent can correctly decide when to bypass the memory
    component when the current given tasks do not require such information.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过内存操控进行提示注入攻击。假设攻击者可以访问代理的内存，我们可以直接提供代理的不正确或不合逻辑的推理步骤。例如，我们可以向代理提供一个虚假的互动记录，其中指令是良性的（没有注入），但代理在推理时不连贯，因此选择重复请求澄清（从而无法继续解决任务）。然而，这些操控不会影响代理的新生成，因此未能成功。我们的实验表明，当当前给定的任务不需要此类信息时，代理能够正确决定何时绕过内存组件。
- en: We can also deploy the adversarial demonstration attack through memory. Instead
    of providing the demonstration in the instruction, we can integrate such incorrect
    demonstrations within the memory. However, similar to previous results, the adversarial
    demonstration remains ineffective.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过内存部署对抗性演示攻击。我们可以将此类不正确的演示集成到内存中，而不是在指令中提供演示。然而，类似于以前的结果，对抗性演示仍然无效。
- en: Our results show that the agent is robust against our attacks deployed through
    the agent’s memory. The agent appears to not rely on information from the memory
    unless it has to.⁴⁴4We conduct a small-scale experiment where the agent can recall
    information that only appears in memory so the component is functioning normally
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果表明，代理对通过代理的内存部署的攻击具有鲁棒性。代理似乎不会依赖内存中的信息，除非必须这样做。⁴⁴4我们进行了一项小规模实验，其中代理可以回忆仅出现在内存中的信息，因此该组件正常运作。
- en: Advanced Attacks
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级攻击
- en: 'For the advanced attack, we only evaluate the performance using the two implemented
    agents. Since the emulator’s output simulates the tools’ expected outputs, it
    cannot guarantee whether the tools will react the same way in actual implementation.
    As described in [Section 3.2](#S3.SS2 "Attack Types ‣ Attacks ‣ Breaking Agents:
    Compromising Autonomous LLM Agents Through Malfunction Amplification"), the advanced
    attack is concerned with multi-agent scenarios with more realistic assumptions.
    We assume the adversary has direct control on one agent and aims to disrupt the
    other agents within the network. Using the two implemented agents, we examine
    two multi-agent scenarios.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高级攻击，我们仅使用两个已实现的代理来评估其性能。由于模拟器的输出模拟了工具的预期输出，因此不能保证工具在实际实施中会以相同的方式反应。如[第3.2节](#S3.SS2
    "攻击类型 ‣ 攻击 ‣ 破解代理：通过故障放大攻击自主LLM代理")所述，高级攻击涉及具有更现实假设的多代理场景。我们假设对手对一个代理具有直接控制权，并旨在干扰网络中的其他代理。使用这两个已实现的代理，我们检查了两个多代理场景。
- en: 'Table 5: Advanced attacks’ success rates on two implemented scenarios.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：高级攻击在两个实现场景下的成功率。
- en: '|  | Infinite Loop | Incorrect Function |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | 无限循环 | 错误功能 |'
- en: '| Same Type | 30.0% | 50.0% |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 相同类型 | 30.0% | 50.0% |'
- en: '| Different Type | 80.0% | 75.0% |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 不同类型 | 80.0% | 75.0% |'
- en: Same-type Multi-agents. We use multiple Gmail agents to simulate an agent network
    that is built with the same type of agents to evaluate how the attack can propagate
    in this environment. We essentially consider the adversary embedding the attack
    within their own agent and infecting other agents in the network indirectly when
    these agents interact with one another. The embedded attack can be either the
    infinite loop or the incorrect function attack.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 同类型多代理。我们使用多个Gmail代理来模拟一个由相同类型代理构建的代理网络，以评估攻击在这种环境中的传播情况。我们本质上考虑对手将攻击嵌入到他们自己的代理中，并在这些代理互相互动时间接感染网络中的其他代理。嵌入的攻击可以是无限循环或错误功能攻击。
- en: 'In both cases, we find the attack is effective and comparable to single-agent
    scenarios’ results, as shown in [Table 5](#S5.T5 "Table 5 ‣ Advanced Attacks ‣
    Results ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction
    Amplification"). For both of these scenarios, successful attacks are expected,
    since they are autonomous versions of the basic attacks that leverage external
    files as attack surface which we examined previously. However, instead of attacking
    the agent that the adversary is directly using, the attack is deployed only when
    additional agents interact with the intermediate agent.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们发现攻击有效，并且与单代理场景的结果相当，如[表 5](#S5.T5 "表 5 ‣ 高级攻击 ‣ 结果 ‣ 破解代理：通过故障放大攻击自主LLM代理")所示。在这两种场景中，预期都会发生成功攻击，因为它们是利用外部文件作为攻击面进行的基本攻击的自主版本，这一点我们之前已经检验过。然而，攻击不是针对对手直接使用的代理，而是在额外的代理与中介代理互动时才会部署。
- en: The incorrect function execution shows slightly higher effectiveness and that
    is likely due to the more direct commands embedded. When utilizing messages from
    another agent, embedded attacking commands such as “repeating previous actions”
    might be ignored by the current agent, but an incorrect but relevant command such
    as “send an email to the following address immediately” can more easily trigger
    executable actions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 错误功能执行显示出稍高的效果，这可能是由于嵌入的指令更为直接。当利用来自其他代理的消息时，嵌入的攻击命令如“重复之前的操作”可能会被当前代理忽略，但像“立即向以下地址发送电子邮件”这样错误但相关的命令则更容易触发可执行操作。
- en: 'Various-type Multi-agents. We examine our attack in scenarios that involve
    multiple agents of different types. More specifically, we consider a scenario
    where a chain of agents is deployed where a CSV agent provides information for
    a downstream Gmail agent. The CSV agent is still responsible for analyzing given
    files and a subsequent Gmail agent is tasked with handling the results and sending
    reports to relevant parties. While single-agent results above have already shown
    that the CSV agent is more robust against these attacks, we examine whether we
    still can utilize it as the base agent for infecting others. Since the adversary
    has direct access to the CSV agent, one can more effectively control the results
    from the agent. However, the result is still autonomously generated and provided
    directly to the downstream agent without manipulations from the adversary. From
    our experiments, we find that utilizing the CSV agent can indeed infect the downstream
    Gmail agent. Both types of attacks achieve high success rates on manipulating
    the Gmail agent, with both around 80% ASR on the cases tested, as seen in [Table 5](#S5.T5
    "Table 5 ‣ Advanced Attacks ‣ Results ‣ Breaking Agents: Compromising Autonomous
    LLM Agents Through Malfunction Amplification"). Therefore, even when the agent
    is relatively robust against our deployed attack, it still can be used to spread
    the attack to other agents that are more susceptible to these attacks.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '各种类型的多代理。我们在涉及不同类型多个代理的场景中考察了我们的攻击。更具体地说，我们考虑了一个场景，其中部署了一系列代理，其中CSV代理提供信息给下游的Gmail代理。CSV代理仍负责分析给定的文件，而后续的Gmail代理则负责处理结果并将报告发送给相关方。尽管上述单代理结果已经显示CSV代理在面对这些攻击时更加稳健，但我们仍然考察了是否可以利用它作为感染其他代理的基础代理。由于对CSV代理有直接访问权限，因此可以更有效地控制代理的结果。然而，结果仍然是自主生成的，并直接提供给下游代理，而没有经过对手的操控。从我们的实验中，我们发现利用CSV代理确实可以感染下游的Gmail代理。两种攻击类型都在操控Gmail代理上取得了较高的成功率，在测试的案例中均约为80%的ASR，如[表5](#S5.T5
    "Table 5 ‣ Advanced Attacks ‣ Results ‣ Breaking Agents: Compromising Autonomous
    LLM Agents Through Malfunction Amplification")所示。因此，即使代理在面对我们部署的攻击时相对稳健，它仍然可以用于将攻击传播到更易受这些攻击影响的其他代理。'
- en: Defense
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防御
- en: 'Here we examine potential defense strategies against attacks on LLM agents.
    As mentioned in [Section 1](#S1 "Introduction ‣ Breaking Agents: Compromising
    Autonomous LLM Agents Through Malfunction Amplification"), previous research has
    primarily focused on the vulnerabilities of LLM agents concerning deliberate and
    overtly harmful or policy-violating actions, such as unauthorized bank transfers
    or instructing the agents to retrieve private information. We suspect that, although
    LLM agents might be capable of executing such actions, there are external measures
    in place to prevent these harmful activities. For example, it is unlikely that
    bank transfers or acquiring private information without additional safety checks
    or further authorization. More importantly, we believe that intentionally harmful
    commands can be detected relatively easily. Once these commands are identified,
    the attack can be thwarted by halting the agents from taking any further action.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们探讨了针对LLM代理攻击的潜在防御策略。如[第1节](#S1 "Introduction ‣ Breaking Agents: Compromising
    Autonomous LLM Agents Through Malfunction Amplification")中提到的，之前的研究主要关注LLM代理在面对故意和明显有害或违反政策的行为时的脆弱性，例如未经授权的银行转账或指示代理检索私人信息。我们怀疑，尽管LLM代理可能能够执行这些操作，但外部措施会防止这些有害活动。例如，未经额外安全检查或进一步授权的银行转账或获取私人信息是不太可能的。更重要的是，我们相信有意的有害指令可以相对容易地被检测到。一旦这些指令被识别，攻击可以通过阻止代理采取任何进一步行动来挫败。'
- en: We suspect that although the agent might be able to execute such actions, there
    exist external measures to prevent such harmful actions. For instance, it is unlikely
    that there is no additional safety checks for a bank transfer or providing private
    information without further authorizations. More importantly, we hypothesize that
    intentionally harmful commands can be detected quite easily. The attack can then
    be thwarted by not proceeding once these commands are detected. To evaluate our
    hypothesis, we investigate common defense strategies developed to counter the
    jailbreaking of LLMs. Additionally, we propose modified defense strategies to
    more effectively mitigate our proposed attacks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们怀疑虽然代理可能能够执行这些操作，但存在外部措施来防止这些有害操作。例如，银行转账或在未经进一步授权的情况下提供私人信息时，不可能没有额外的安全检查。更重要的是，我们假设故意有害的命令可以相对容易地被检测到。一旦这些命令被检测到，攻击就可以通过不继续执行这些命令来挫败。为了验证我们的假设，我们调查了针对大语言模型越狱的常见防御策略。此外，我们提出了修改后的防御策略，以更有效地减轻我们提出的攻击。
- en: Self Examination
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自我检查
- en: Borrowing similar intuitions from defending against jailbreaking LLMs, we utilize
    a straightforward defense strategy to counter the attacks on LLM agents. We can
    try to detect such a dangerous command by simply asking the core LLM whether such
    action can be harmful or violates your policy, before executing the actual command.
    The prompt we use for inspection is shown below.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴防御大语言模型越狱的类似直觉，我们采用了一种简单的防御策略来应对对 LLM 代理的攻击。我们可以尝试通过在实际执行命令之前，简单地询问核心 LLM
    该行动是否可能有害或违反您的政策，来检测这种危险命令。我们用于检查的提示如下所示。
- en: *Inspect the following instruction
    and respond with the keyword YES or NO, to whether the instruction can be harmful
    or violate the policy of the user agreement.*
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: *检查以下指令，并用关键字 YES 或 NO 回复，以判断该指令是否可能有害或违反用户协议的政策。*
- en: This detection method has been deployed to protect LLMs against jailbreak attacks [[37](#bib.bib37),
    [42](#bib.bib42)]. The self-examination prompt, however, can be argued that it
    is biased against the previous attacks (where deliberately causing harm or stealing
    data is the attacker’s goal). We now modify the detection prompt so that it is
    more tuned toward our attack for a balanced evaluation. The new and modified detection
    prompt is shown below.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这种检测方法已被部署以保护 LLM 免受越狱攻击 [[37](#bib.bib37), [42](#bib.bib42)]。然而，应该指出的是，自我检查提示可能对先前的攻击存在偏见（其中故意造成伤害或窃取数据是攻击者的目标）。我们现在修改检测提示，使其更适应我们的攻击，以进行更为平衡的评估。新的和修改后的检测提示如下所示。
- en: *Inspect the following instruction
    and respond with the keyword YES or NO, to whether the instruction deliberately
    cause malfunction of the model.*
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: *检查以下指令，并用关键字 YES 或 NO 回复，以判断该指令是否故意引起模型故障。*
- en: Defense Comparison
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 防御比较
- en: 'Table 6: Anomaly detection rate (higher indicates better defense) on different
    attack prompts using policy violation self-examination. INJ = Prompt Injection.
    ADV=Adversarial Perturbation.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：使用策略违规自我检查对不同攻击提示的异常检测率（值越高表示防御效果越好）。INJ = 提示注入。ADV=对抗扰动。
- en: '| Attack | Adversarial Prompt | Complete Prompt |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 对抗提示 | 完整提示 |'
- en: '| Direct Harm (INJ) | 90.0% | 83.7% |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 直接危害 (INJ) | 90.0% | 83.7% |'
- en: '| Data Stealing (INJ) | 100.0% | 100.0% |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 数据窃取 (INJ) | 100.0% | 100.0% |'
- en: '| Infinite Loop (INJ) | 0.0% | 0.0% |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 无限循环 (INJ) | 0.0% | 0.0% |'
- en: '| Incorrect Function (INJ) | 0.0% | 0.0% |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 错误功能 (INJ) | 0.0% | 0.0% |'
- en: '| GCG (ADV) | 0.0% | 0.0% |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| GCG (ADV) | 0.0% | 0.0% |'
- en: '| VIPER (ADV) | N/A | 0.0% |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| VIPER (ADV) | 不适用 | 0.0% |'
- en: '| SCPN (ADV) | N/A | 0.0% |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| SCPN (ADV) | 不适用 | 0.0% |'
- en: We use test cases and attacking prompts from previous works [[47](#bib.bib47)]
    as an example of deliberate attack against the agents and compare the defense
    effectiveness against our attack prompts. They deploy prompt injection attacks
    on agents with two types of adversarial goals, namely data stealing and direct
    harm. We query the core LLM in several scenarios for complete analysis, namely
    attack prompt only, instruction prompt only, and complete prompts.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自以前工作的测试用例和攻击提示[[47](#bib.bib47)]作为对代理的故意攻击的例子，并将防御效果与我们的攻击提示进行比较。他们对代理实施了两种类型的对抗性目标的提示注入攻击，即数据窃取和直接危害。我们在几个场景中查询核心
    LLM 以进行完整分析，即仅攻击提示、仅指令提示和完整提示。
- en: Attack prompt refers to only the attack section of the instruction prompt (i.e.,
    the part after “Ignore previous instructions” for prompt injection attacks). The
    instruction prompt refers to the benign segment of the instruction prompts. We
    evaluate these normal instruction sections to ensure there is no false positive
    from the defense. The complete prompt is then the entire prompt that is composed
    of both the benign instruction as well as the attack prompt, which simulates the
    actual deployment scenarios.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击提示指的是指令提示的攻击部分（即“忽略以前的指令”之后的部分）。指令提示指的是指令提示的良性部分。我们评估这些正常指令部分，以确保防御没有产生假阳性。完整提示则是由良性指令和攻击提示组成的整个提示，模拟实际的部署场景。
- en: 'As seen in [Table 7](#S6.T7 "Table 7 ‣ Defense Comparison ‣ Defense ‣ Breaking
    Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification"),
    we find that previous work’s prompt injection attacks have much higher detection
    rates than our prompts using the self-examination defense. Both the attack and
    complete prompt can be easily detected by the core LLM, reaching perfect detection
    rates on the data-stealing prompts. It is as expected since their instructions
    contain obvious trigger words, such as “transfer the entire amount to the following
    accounts.” For both types of our basic attacks, i.e., infinite loop and incorrect
    function execution, we see no obvious detection from LLM’s self-examination.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[表 7](#S6.T7 "表 7 ‣ 防御比较 ‣ 防御 ‣ 破解代理：通过故障放大妥协自主 LLM 代理")中所示，我们发现以往工作的提示注入攻击比使用自我检查防御的我们的提示有更高的检测率。攻击提示和完整提示都可以被核心
    LLM 容易检测到，在数据窃取提示上达到了完美的检测率。这是预期中的情况，因为他们的指令包含明显的触发词，例如“将整个金额转账到以下账户”。对于我们两种基本攻击，即无限循环和错误功能执行，我们没有看到
    LLM 自我检查中的明显检测。
- en: 'Using the modified general malfunction detection prompt, we find that some
    of our attacks can now be detected, as shown in [Table 7](#S6.T7 "Table 7 ‣ Defense
    Comparison ‣ Defense ‣ Breaking Agents: Compromising Autonomous LLM Agents Through
    Malfunction Amplification"). The detection rate, however, is still lower than
    the detection rates on those harmful injection prompts, even when they are examined
    using the modified detection prompts (targeting malfunction) as well.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用修改后的通用故障检测提示，我们发现现在可以检测到一些我们的攻击，如[表 7](#S6.T7 "表 7 ‣ 防御比较 ‣ 防御 ‣ 破解代理：通过故障放大妥协自主
    LLM 代理")所示。然而，即使使用修改后的检测提示（针对故障）进行检查，检测率仍然低于那些有害注入提示的检测率。
- en: Overall, our results show that the attack is indeed more difficult to detect
    through simple self-examinations.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的结果表明，攻击确实通过简单的自我检查更难被检测到。
- en: 'Table 7: Anomaly detection rate (higher indicates better defense) on different
    attack prompts using malfunction detection self-examination. INJ = Prompt Injection.
    ADV=Adversarial Perturbation.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在不同攻击提示上使用故障检测自我检查的异常检测率（数值越高表示防御越好）。INJ = 提示注入。ADV=对抗性扰动。
- en: '| Attack | Adversarial Prompt | Complete Prompt |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 对抗性提示 | 完整提示 |'
- en: '| Direct Harm (INJ) | 40.0% | 42.7% |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 直接危害 (INJ) | 40.0% | 42.7% |'
- en: '| Data Stealing (INJ) | 78.1% | 69.3% |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 数据窃取 (INJ) | 78.1% | 69.3% |'
- en: '| Infinite Loop (INJ) | 0.0% | 20.0% |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 无限循环 (INJ) | 0.0% | 20.0% |'
- en: '| Incorrect Function (INJ) | 0.0% | 0.0% |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 错误功能 (INJ) | 0.0% | 0.0% |'
- en: '| GCG (ADV) | 0.0% | 30.0% |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| GCG (ADV) | 0.0% | 30.0% |'
- en: '| VIPER (ADV) | N/A | 0.0% |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| VIPER (ADV) | 不适用 | 0.0% |'
- en: '| SCPN (ADV) | N/A | 0.0% |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| SCPN (ADV) | N/A | 0.0% |'
- en: Related Work
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: 'Considering the growing interest in developing autonomous agents using large
    language models, research on the safety aspects of LLM agents has been relatively
    limited. Ruan et. al. propose the agent emulator framework we used in our work [[36](#bib.bib36)].
    They leverage the framework to examine a selection of curated high-risk scenarios
    and find a high percentage of agent failures identified in the emulator would
    also fail in real implementation based on human evaluation. Utilizing the same
    framework, Zhan et. al. examine the risk of prompt injection attacks on tool-integrated
    LLM agents [[47](#bib.bib47)]. They identify two types of risky actions from the
    agents when attacked and also compare agents’ behavior with a wide variety of
    core LLM. Their results show that even the most advanced GPT-4 model is vulnerable
    to their attack. Yang et. al. evaluate the vulnerabilities in LLM agents with
    backdoor attacks [[44](#bib.bib44)]. From a conceptual level, Mo et. al. examine
    the potential risks of utilizing LLM agents in their position paper [[31](#bib.bib31)].
    They also present a comprehensive framework for evaluating the adversarial attacks
    against LLM agents, sharing similarities with our approach such as identifying
    different components of the LLM agents as attack surfaces. However, their effort
    stopped at the conceptual level. These studies, however, differ from our approach
    that they only focus on examining obvious unsafe actions that can be elicited
    from the agents. As we have shown in [Section 6](#S6 "Defense ‣ Breaking Agents:
    Compromising Autonomous LLM Agents Through Malfunction Amplification"), such attacks
    can be detected through LLMs’ self-inspections.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于对使用大型语言模型开发自主代理的兴趣日益增长，对LLM代理安全方面的研究相对较少。Ruan等人提出了我们在工作中使用的代理模拟框架[[36](#bib.bib36)]。他们利用该框架检查了一些精心挑选的高风险场景，并发现模拟器中识别出的高比例的代理失败也会在基于人工评估的实际实现中出现失败。Zhan等人利用相同的框架检查了对工具集成LLM代理的提示注入攻击风险[[47](#bib.bib47)]。他们识别出攻击时代理的两种风险行为，并将代理行为与多种核心LLM进行了比较。他们的结果表明，即使是最先进的GPT-4模型也对他们的攻击存在漏洞。Yang等人评估了LLM代理在后门攻击下的脆弱性[[44](#bib.bib44)]。从概念层面上，Mo等人在其立场文件[[31](#bib.bib31)]中检查了使用LLM代理的潜在风险。他们还提出了一个评估对LLM代理的对抗性攻击的全面框架，与我们的方法有相似之处，例如识别LLM代理的不同组件作为攻击面。然而，他们的工作停留在了概念层面。这些研究与我们的方法不同，它们只关注于检查可以从代理中引发的明显不安全行为。正如我们在[第6节](#S6
    "防御 ‣ 破坏代理：通过故障放大妥协自主LLM代理")中所示，这些攻击可以通过LLM的自我检查来检测。
- en: 'Besides direct safety analysis on LLM agents, many studies on LLMs can also
    be adapted. Generating adversarial examples is the attack most directly related
    to our attack, where the adversary aims to perturb the input such that the model
    cannot handle it correctly. Many attacks have been developed targeting LLMs [[15](#bib.bib15),
    [16](#bib.bib16), [39](#bib.bib39), [19](#bib.bib19), [51](#bib.bib51), [41](#bib.bib41),
    [49](#bib.bib49), [7](#bib.bib7), [23](#bib.bib23), [37](#bib.bib37)]. From a
    broader perspective, several studies also aim to offer overviews of LLM’S behaviors
    and security vulnerabilities. Liang et al.  [[25](#bib.bib25)] present a framework
    for evaluating foundation models from several perspectives. Wang et al.  [[40](#bib.bib40)]
    conduct extensive evaluations on a wide variety of topics on the trustworthiness
    of LLMs, such as robustness, toxicity, and fairness. Li et al.  [[24](#bib.bib24)]
    survey current privacy issues in LLMs, including training data extraction, personal
    information leakage, and membership inference Derner et al.  [[11](#bib.bib11)]
    present a categorization of LLM’s security risks. These studies can help identify
    potential weaknesses of LLM agents as well, but the additional components in LLM
    agents will provide different insights, as we discovered in [Section 5](#S5 "Results
    ‣ Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification").'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对LLM智能体进行直接的安全分析外，许多LLM的研究也可以被适应。生成对抗样本是与我们攻击最直接相关的攻击，其中对手旨在扰动输入，以使模型无法正确处理它。许多攻击已经针对LLM开发
    [[15](#bib.bib15), [16](#bib.bib16), [39](#bib.bib39), [19](#bib.bib19), [51](#bib.bib51),
    [41](#bib.bib41), [49](#bib.bib49), [7](#bib.bib7), [23](#bib.bib23), [37](#bib.bib37)]。从更广泛的角度来看，一些研究还旨在提供LLM行为和安全漏洞的概述。Liang等人
    [[25](#bib.bib25)] 从多个角度提出了评估基础模型的框架。Wang等人 [[40](#bib.bib40)] 对LLM的可信度进行了广泛的评估，包括鲁棒性、毒性和公平性。Li等人
    [[24](#bib.bib24)] 调查了LLM中的当前隐私问题，包括训练数据提取、个人信息泄露和成员推断。Derner等人 [[11](#bib.bib11)]
    提出了LLM安全风险的分类。这些研究也有助于识别LLM智能体的潜在弱点，但LLM智能体中的额外组件将提供不同的见解，正如我们在[第5节](#S5 "结果 ‣
    破解智能体：通过故障放大破解自主LLM智能体")中发现的那样。
- en: Limitation
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: Our work is not without limitations. We reflect on areas where we can offer
    directions and inspiration for future works.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作并非没有局限性。我们反思了可以为未来工作提供方向和灵感的领域。
- en: 'Implemented Agents. As mentioned in [Section 4.1](#S4.SS1 "Agent Emulator ‣
    Evaluation Setting ‣ Breaking Agents: Compromising Autonomous LLM Agents Through
    Malfunction Amplification"), the implementation of applicable agents can be difficult.
    Therefore, for our case studies, we only implemented two agents. Expanding the
    implemented agents to a broader selection can potentially provide even more comprehensive
    results. However, we leverage the agent emulator to present an overview of the
    risk efficiently to keep pace with the development and adoption of these emergent
    autonomous systems.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 实施的智能体。如[第4.1节](#S4.SS1 "智能体模拟器 ‣ 评估设置 ‣ 破解智能体：通过故障放大破解自主LLM智能体")中所述，实施适用的智能体可能是困难的。因此，对于我们的案例研究，我们仅实施了两个智能体。将实施的智能体扩展到更广泛的选择可能会提供更全面的结果。然而，我们利用智能体模拟器高效地概述了风险，以跟上这些新兴自主系统的发展和采用。
- en: Categorization. As we are mostly concerned with the potential risks of deploying
    these agents in practical scenarios, we mainly consider agents that are designed
    to solve real-world tasks. There are also other types of agents that have been
    developed using LLM, such as NPC in games [[34](#bib.bib34), [26](#bib.bib26)].
    Since our attack is not inherently limited to any type of agent, it would be interesting
    to investigate how the categories of the agent affect the attack performance.
    We defer such investigation to future works.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 分类。由于我们主要关注在实际场景中部署这些智能体的潜在风险，我们主要考虑旨在解决现实世界任务的智能体。还有其他类型的智能体也使用LLM开发，例如游戏中的NPC
    [[34](#bib.bib34), [26](#bib.bib26)]。由于我们的攻击并不固有地局限于任何类型的智能体，因此研究智能体的类别如何影响攻击性能将会很有趣。我们将这种研究推迟到未来工作中。
- en: Models. We only experimented with three variants of the LLMs as the core for
    the agents, since we opt to focus on models that are actively being utilized to
    build agents in the wild. The support from notable LLM agent development frameworks,
    such as AutoGPT and LangChain, reflects such popularity. Yet, we hope to expand
    our evaluations to more models in the future and include open-source models that
    offer more control. For instance, we can utilize such models for constructing
    adversarial perturbations to examine worst-case scenarios of the threat.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。我们只实验了三种变体的LLMs作为代理的核心，因为我们选择专注于那些在实际环境中被广泛使用的模型。AutoGPT和LangChain等知名LLM代理开发框架的支持反映了这种流行程度。然而，我们希望未来能够扩展评估更多模型，并包括那些提供更多控制的开源模型。例如，我们可以利用这些模型来构建对抗性扰动，以检验威胁的最坏情况。
- en: Ethics Discussion
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理讨论
- en: Considering we are presenting an attack against practical systems deployed in
    the real world, it is important to address relevant ethics issues. Although we
    present our findings as a novel attack against LLM agents, our main purpose is
    to draw attention to this previously ignored risk.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们正在对实际部署在现实世界中的系统进行攻击，处理相关的伦理问题是重要的。尽管我们将我们的发现呈现为对LLM代理的创新攻击，我们的主要目的是引起对这一以前被忽视的风险的关注。
- en: We present our attack as an evaluation platform for examining the robustness
    of LLM agents against these manipulations. Even the practical scenarios presented
    in our advanced attacks require large-scale deployments to present significant
    threats at the moment. We hope to draw attention to these potential vulnerabilities
    so that the developers working on LLM agents can obtain a better understanding
    of the risk and devise potentially more effective safeguard systems before more
    extensive adoptions and applications are in the wild.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的攻击呈现为一个评估平台，用于检查LLM代理对这些操控的鲁棒性。即使是我们高级攻击中提出的实际场景，目前也需要大规模部署才能呈现出显著的威胁。我们希望引起对这些潜在脆弱性的关注，以便从事LLM代理工作的开发人员能更好地了解风险，并在更广泛的采用和应用之前设计出可能更有效的保护系统。
- en: Conclusion
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We use our proposed attack to highlight vulnerable areas of the current agents
    against these malfunction-inducing attacks. By showcasing advanced versions of
    our attacks on implemented and deployable agents, we draw attention to the potential
    risks when these autonomous agents are deployed at scale. Comparing the defense
    effectiveness of our attack with previous works further accentuates the challenge
    of mitigating these risks. The promising performance of the emerging LLM agents
    should not eclipse concerns about the potential risks of these agents. We hope
    our discoveries can facilitate future works on improving the robustness of LLM
    agents against these manipulations.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们提出的攻击来突出当前代理在这些故障诱发攻击下的脆弱区域。通过展示我们攻击的高级版本对已实现和可部署代理的影响，我们引起了对这些自主代理大规模部署潜在风险的关注。将我们攻击的防御效果与之前的工作进行比较进一步突显了缓解这些风险的挑战。新兴LLM代理的良好表现不应掩盖对这些代理潜在风险的担忧。我们希望我们的发现能够促进未来在提高LLM代理对这些操控的鲁棒性方面的工作。
- en: References
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4).'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)。'
- en: '[2] [https://products.wolframalpha.com/llm-api/](https://products.wolframalpha.com/llm-api/).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] [https://products.wolframalpha.com/llm-api/](https://products.wolframalpha.com/llm-api/)。'
- en: '[3] [https://www.langchain.com/](https://www.langchain.com/).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] [https://www.langchain.com/](https://www.langchain.com/)。'
- en: '[4] [https://news.agpt.co/](https://news.agpt.co/).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] [https://news.agpt.co/](https://news.agpt.co/)。'
- en: '[5] Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph Endres, Thorsten
    Holz, and Mario Fritz. Not What You’ve Signed Up For: Compromising Real-World
    LLM-Integrated Applications with Indirect Prompt Injection. In Workshop on Security
    and Artificial Intelligence (AISec), pages 79–90\. ACM, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph Endres, Thorsten
    Holz, 和 Mario Fritz. “Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated
    Applications with Indirect Prompt Injection.” 在《安全与人工智能研讨会》（AISec），第79–90页。ACM，2023。'
- en: '[6] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic,
    Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion Attacks against Machine
    Learning at Test Time. In European Conference on Machine Learning and Principles
    and Practice of Knowledge Discovery in Databases (ECML/PKDD), pages 387–402\.
    Springer, 2013.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Battista Biggio、Igino Corona、Davide Maiorca、Blaine Nelson、Nedim Srndic、Pavel
    Laskov、Giorgio Giacinto 和 Fabio Roli。在测试时对机器学习进行规避攻击。在欧洲机器学习和数据库中的知识发现原则与实践会议（ECML/PKDD）上，页码387–402。Springer，2013年。'
- en: '[7] Nicholas Boucher, Ilia Shumailov, Ross Anderson, and Nicolas Papernot.
    Bad Characters: Imperceptible NLP Attacks. In IEEE Symposium on Security and Privacy
    (S&P), pages 1987–2004\. IEEE, 2022.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Nicholas Boucher、Ilia Shumailov、Ross Anderson 和 Nicolas Papernot。坏字符：不可察觉的NLP攻击。在IEEE安全与隐私研讨会（S&P）上，页码1987–2004。IEEE，2022年。'
- en: '[8] Ting-Yun Chang and Robin Jia. Data Curation Alone Can Stabilize In-context
    Learning. In Annual Meeting of the Association for Computational Linguistics (ACL),
    pages 8123–8144\. ACL, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Ting-Yun Chang 和 Robin Jia。数据策划本身可以稳定上下文学习。在计算语言学协会（ACL）年会上，页码8123–8144。ACL，2023年。'
- en: '[9] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J.
    Pappas, and Eric Wong. Jailbreaking Black Box Large Language Models in Twenty
    Queries. CoRR abs/2310.08419, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Patrick Chao、Alexander Robey、Edgar Dobriban、Hamed Hassani、George J. Pappas
    和 Eric Wong。通过二十个查询破解黑箱大型语言模型。CoRR abs/2310.08419，2023年。'
- en: '[10] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu
    Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated Jailbreak Across Multiple
    Large Language Model Chatbots. CoRR abs/2307.08715, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Gelei Deng、Yi Liu、Yuekang Li、Kailong Wang、Ying Zhang、Zefeng Li、Haoyu Wang、Tianwei
    Zhang 和 Yang Liu。Jailbreaker：跨多个大型语言模型聊天机器人进行自动化破解。CoRR abs/2307.08715，2023年。'
- en: '[11] Erik Derner, Kristina Batistic, Jan Zahálka, and Robert Babuska. A Security
    Risk Taxonomy for Large Language Models. CoRR abs/2311.11415, 2023.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Erik Derner、Kristina Batistic、Jan Zahálka 和 Robert Babuska。大型语言模型的安全风险分类。CoRR
    abs/2311.11415，2023年。'
- en: '[12] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,
    Jingjing Xu, Lei Li, and Zhifang Sui. A Survey on In-context Learning. CoRR abs/2301.00234,
    2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Qingxiu Dong、Lei Li、Damai Dai、Ce Zheng、Zhiyong Wu、Baobao Chang、Xu Sun、Jingjing
    Xu、Lei Li 和 Zhifang Sui。关于上下文学习的调查。CoRR abs/2301.00234，2023年。'
- en: '[13] Haonan Duan, Adam Dziedzic, Mohammad Yaghini, Nicolas Papernot, and Franziska
    Boenisch. On the Privacy Risk of In-context Learning. In Workshop on Trustworthy
    Natural Language Processing (TrustNLP), 2023.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Haonan Duan、Adam Dziedzic、Mohammad Yaghini、Nicolas Papernot 和 Franziska
    Boenisch。关于上下文学习的隐私风险。在可信自然语言处理（TrustNLP）研讨会，2023年。'
- en: '[14] Steffen Eger, Gözde Gül Sahin, Andreas Rücklé, Ji-Ung Lee, Claudia Schulz,
    Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, and Iryna Gurevych. Text Processing
    Like Humans Do: Visually Attacking and Shielding NLP Systems. In Conference of
    the North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies (NAACL-HLT), pages 1634–1647\. ACL, 2019.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Steffen Eger、Gözde Gül Sahin、Andreas Rücklé、Ji-Ung Lee、Claudia Schulz、Mohsen
    Mesgar、Krishnkant Swarnkar、Edwin Simpson 和 Iryna Gurevych。像人类一样处理文本：视觉攻击和保护NLP系统。在北美计算语言学协会：人类语言技术（NAACL-HLT）会议上，页码1634–1647。ACL，2019年。'
- en: '[15] Xuanjie Fang, Sijie Cheng, Yang Liu, and Wei Wang. Modeling Adversarial
    Attack on Pre-trained Language Models as Sequential Decision Making. In Annual
    Meeting of the Association for Computational Linguistics (ACL), pages 7322–7336\.
    ACL, 2023.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Xuanjie Fang、Sijie Cheng、Yang Liu 和 Wei Wang。将对预训练语言模型的对抗攻击建模为顺序决策。 在计算语言学协会（ACL）年会上，页码7322–7336。ACL，2023年。'
- en: '[16] Piotr Gainski and Klaudia Balazy. Step by Step Loss Goes Very Far: Multi-Step
    Quantization for Adversarial Text Attacks. In Conference of the European Chapter
    of the Association for Computational Linguistics (EACL), pages 2030–2040\. ACL,
    2023.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Piotr Gainski 和 Klaudia Balazy。一步一步损失走得很远：对抗文本攻击的多步量化。在欧洲计算语言学协会（EACL）会议上，页码2030–2040。ACL，2023年。'
- en: '[17] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and
    Harnessing Adversarial Examples. In International Conference on Learning Representations
    (ICLR), 2015.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Ian Goodfellow、Jonathon Shlens 和 Christian Szegedy。解释和利用对抗样本。在国际学习表示会议（ICLR），2015年。'
- en: '[18] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten
    Holz, and Mario Fritz. More than you’ve asked for: A Comprehensive Analysis of
    Novel Prompt Injection Threats to Application-Integrated Large Language Models.
    CoRR abs/2302.12173, 2023.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten
    Holz, 和 Mario Fritz. 超出你要求的：对应用集成大语言模型的新型提示注入威胁的全面分析。CoRR abs/2302.12173, 2023。'
- en: '[19] Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. Gradient-based
    Adversarial Attacks against Text Transformers. In Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pages 5747–5757\. ACL, 2021.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, 和 Douwe Kiela. 基于梯度的对抗性攻击文本转换器。在自然语言处理实证方法会议（EMNLP），第
    5747–5757 页。ACL, 2021。'
- en: '[20] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic
    Jailbreak of Open-source LLMs via Exploiting Generation. CoRR abs/2310.06987,
    2023.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, 和 Danqi Chen. 通过利用生成的灾难性越狱开源
    LLMs。CoRR abs/2310.06987, 2023。'
- en: '[21] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial
    Example Generation with Syntactically Controlled Paraphrase Networks. In Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (NAACL-HLT), pages 1875–1885\. ACL, 2018.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Mohit Iyyer, John Wieting, Kevin Gimpel, 和 Luke Zettlemoyer. 使用语法控制的释义网络生成对抗性示例。在北美计算语言学协会人类语言技术会议（NAACL-HLT），第
    1875–1885 页。ACL, 2018。'
- en: '[22] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step
    Jailbreaking Privacy Attacks on ChatGPT. CoRR abs/2304.05197, 2023.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, 和 Yangqiu Song. 针对 ChatGPT 的多步骤隐私越狱攻击。CoRR
    abs/2304.05197, 2023。'
- en: '[23] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. TextBugger:
    Generating Adversarial Text Against Real-world Applications. In Network and Distributed
    System Security Symposium (NDSS). Internet Society, 2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, 和 Ting Wang. TextBugger：生成针对现实世界应用的对抗性文本。在网络与分布式系统安全研讨会（NDSS）。互联网协会,
    2019。'
- en: '[24] ZhHaoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu,
    Chunkit Chan, and Yangqiu Song. Privacy in Large Language Models: Attacks, Defenses
    and Future Directions. CoRR abs/2310.10383, 2023.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] ZhHaoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu,
    Chunkit Chan, 和 Yangqiu Song. 大语言模型中的隐私：攻击、防御与未来方向。CoRR abs/2310.10383, 2023。'
- en: '[25] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin
    Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D.
    Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin
    Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam,
    Laurel J. Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha,
    Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael
    Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi
    Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and
    Yuta Koreeda. Holistic Evaluation of Language Models. CoRR abs/2211.09110, 2022.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin
    Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D.
    Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin
    Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam,
    Laurel J. Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha,
    Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang
    Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard,
    Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang,
    和 Yuta Koreeda. 语言模型的整体评估。CoRR abs/2211.09110, 2022。'
- en: '[26] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu,
    Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng,
    Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie
    Huang, Yuxiao Dong, and Jie Tang. AgentBench: Evaluating LLMs as Agents. CoRR
    abs/2308.03688, 2023.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu,
    Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng,
    Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie
    Huang, Yuxiao Dong, 和 Jie Tang. AgentBench：评估 LLMs 作为代理。CoRR abs/2308.03688, 2023。'
- en: '[27] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. AutoDAN: Generating
    Stealthy Jailbreak Prompts on Aligned Large Language Models. CoRR abs/2310.04451,
    2023.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Xiaogeng Liu, Nan Xu, Muhao Chen, 和 Chaowei Xiao. AutoDAN：在对齐的大语言模型上生成隐秘的越狱提示。CoRR
    abs/2310.04451, 2023。'
- en: '[28] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang,
    Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking ChatGPT via Prompt Engineering:
    An Empirical Study. CoRR abs/2305.13860, 2023.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang,
    Lida Zhao, Tianwei Zhang 和 Yang Liu. 《通过提示工程破解ChatGPT：一项实证研究》。CoRR abs/2305.13860,
    2023。'
- en: '[29] Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong.
    InstrPrompt Injection Attacks and Defenses in LLM-Integrated Applications. CoRR
    abs/2310.12815, 2023.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia 和 Neil Zhenqiang Gong.
    《LLM集成应用中的InstrPrompt注入攻击与防御》。CoRR abs/2310.12815, 2023。'
- en: '[30] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
    Hajishirzi, and Luke Zettlemoyer. Rethinking the Role of Demonstrations: What
    Makes In-Context Learning Work? In Conference on Empirical Methods in Natural
    Language Processing (EMNLP), pages 11048–11064\. ACL, 2022.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
    Hajishirzi 和 Luke Zettlemoyer. 《重新思考示范的角色：是什么使上下文学习有效？》。在自然语言处理实证方法会议（EMNLP），第11048–11064页。ACL,
    2022。'
- en: '[31] Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, and Huan Sun.
    A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents.
    CoRR abs/2402.10196, 2024.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao 和 Huan Sun. 《颤抖的纸牌屋？映射对语言代理的对抗性攻击》。CoRR
    abs/2402.10196, 2024。'
- en: '[32] Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What In-Context Learning
    "Learns" In-Context: Disentangling Task Recognition and Task Learning. CoRR abs/2305.09731,
    2023.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Jane Pan, Tianyu Gao, Howard Chen 和 Danqi Chen. 《上下文学习在上下文中“学习”了什么：解开任务识别和任务学习的关系》。CoRR
    abs/2305.09731, 2023。'
- en: '[33] Ashwinee Panda, Tong Wu, Jiachen T. Wang, and Prateek Mittal. Differentially
    Private In-Context Learning. CoRR abs/2305.01639, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Ashwinee Panda, Tong Wu, Jiachen T. Wang 和 Prateek Mittal. 《差分隐私的上下文学习》。CoRR
    abs/2305.01639, 2023。'
- en: '[34] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S. Bernstein. Generative Agents: Interactive Simulacra
    of Human Behavior. CoRR abs/2304.03442, 2023.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris,
    Percy Liang 和 Michael S. Bernstein. 《生成代理：人类行为的互动模拟》。CoRR abs/2304.03442, 2023。'
- en: '[35] Yao Qiang, Xiangyu Zhou, and Dongxiao Zhu. Hijacking Large Language Models
    via Adversarial In-Context Learning. CoRR abs/2311.09948, 2023.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Yao Qiang, Xiangyu Zhou 和 Dongxiao Zhu. 《通过对抗性上下文学习劫持大型语言模型》。CoRR abs/2311.09948,
    2023。'
- en: '[36] Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou,
    Jimmy Ba, Yann Dubois, Chris J. Maddison, and Tatsunori Hashimoto. Identifying
    the Risks of LM Agents with an LM-Emulated Sandbox. In International Conference
    on Learning Representations (ICLR). ICLR, 2024.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou,
    Jimmy Ba, Yann Dubois, Chris J. Maddison 和 Tatsunori Hashimoto. 《使用语言模型仿真沙盒识别语言模型代理的风险》。在国际学习表示会议（ICLR）。ICLR,
    2024。'
- en: '[37] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. Do
    Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large
    Language Models. In ACM SIGSAC Conference on Computer and Communications Security
    (CCS). ACM, 2024.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen 和 Yang Zhang. 《现在做任何事：在大型语言模型上的真实世界破解提示的特征描述与评估》。在ACM
    SIGSAC计算机与通信安全会议（CCS）。ACM, 2024。'
- en: '[38] Octavian Suciu, Radu Mărginean, Yiğitcan Kaya, Hal Daumé III, and Tudor
    Dumitraş. When Does Machine Learning FAIL? Generalized Transferability for Evasion
    and Poisoning Attacks. In USENIX Security Symposium (USENIX Security), pages 1299–1316\.
    USENIX, 2018.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Octavian Suciu, Radu Mărginean, Yiğitcan Kaya, Hal Daumé III 和 Tudor Dumitraş.
    《机器学习何时失败？规避和中毒攻击的广义转移性》。在USENIX安全研讨会（USENIX Security），第1299–1316页。USENIX, 2018。'
- en: '[39] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.
    Universal Adversarial Triggers for Attacking and Analyzing NLP. In Conference
    on Empirical Methods in Natural Language Processing and International Joint Conference
    on Natural Language Processing (EMNLP-IJCNLP), pages 2153–2162\. ACL, 2019.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner 和 Sameer Singh. 《攻击和分析自然语言处理的通用对抗性触发器》。在自然语言处理实证方法会议和自然语言处理国际联合会议（EMNLP-IJCNLP），第2153–2162页。ACL,
    2019。'
- en: '[40] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui
    Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran
    Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn
    Song, and Bo Li. DecodingTrust: A Comprehensive Assessment of Trustworthiness
    in GPT Models. CoRR abs/2306.11698, 2023.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui
    Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran
    Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn
    Song 和 Bo Li。《DecodingTrust：对 GPT 模型可信度的全面评估》。CoRR abs/2306.11698, 2023。'
- en: '[41] Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao.
    Adversarial Demonstration Attacks on Large Language Models. CoRR abs/2305.14950,
    2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen 和 Chaowei Xiao。《对大型语言模型的对抗性演示攻击》。CoRR
    abs/2305.14950, 2023。'
- en: '[42] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng
    Chen, Xing Xie, and Fangzhao Wu. Defending ChatGPT against jailbreak attack via
    self-reminders. Nature Machine Intelligence, 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng
    Chen, Xing Xie 和 Fangzhao Wu。《通过自我提醒防御 ChatGPT 对越狱攻击》。自然机器智能, 2023。'
- en: '[43] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang,
    Vijay Srinivasan, Xiang Ren, and Hongxia Jin. Backdooring Instruction-Tuned Large
    Language Models with Virtual Prompt Injection. CoRR abs/2307.16888, 2023.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang,
    Vijay Srinivasan, Xiang Ren 和 Hongxia Jin。《通过虚拟提示注入对指令调优的大型语言模型进行后门攻击》。CoRR abs/2307.16888,
    2023。'
- en: '[44] Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, and Xu Sun.
    Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents.
    CoRR abs/2402.11208, 2024.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou 和 Xu Sun。《当心你的代理！调查针对基于
    LLM 的代理的后门威胁》。CoRR abs/2402.11208, 2024。'
- en: '[45] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan,
    and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models. In International
    Conference on Learning Representations (ICLR). ICLR, 2023.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan
    和 Yuan Cao。《ReAct：在语言模型中协同推理与行动》。在国际学习表示会议 (ICLR) 上。ICLR, 2023。'
- en: '[46] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. GPTFUZZER: Red Teaming
    Large Language Models with Auto-Generated Jailbreak Prompts. CoRR abs/2309.10253,
    2023.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Jiahao Yu, Xingwei Lin, Zheng Yu 和 Xinyu Xing。《GPTFUZZER：通过自动生成的越狱提示对大型语言模型进行红队测试》。CoRR
    abs/2309.10253, 2023。'
- en: '[47] Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. InjecAgent: Benchmarking
    Indirect Prompt Injections in Tool-Integrated Large Language Model Agents. CoRR
    abs/2403.02691, 2024.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Qiusi Zhan, Zhixiang Liang, Zifan Ying 和 Daniel Kang。《InjecAgent：基准测试工具集成的大型语言模型代理中的间接提示注入》。CoRR
    abs/2403.02691, 2024。'
- en: '[48] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar,
    Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena:
    A Realistic Web Environment for Building Autonomous Agents. CoRR abs/2307.13854,
    2023.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar,
    Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon 和 Graham Neubig。《WebArena：构建自主代理的现实网络环境》。CoRR
    abs/2307.13854, 2023。'
- en: '[49] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong
    Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, and Xing Xie. PromptBench:
    Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts.
    CoRR abs/2306.04528, 2023.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong
    Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang 和 Xing Xie。《PromptBench：评估大型语言模型对抗性提示的鲁棒性》。CoRR
    abs/2306.04528, 2023。'
- en: '[50] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Red teaming
    ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity. CoRR abs/2301.12867,
    2023.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Terry Yue Zhuo, Yujin Huang, Chunyang Chen 和 Zhenchang Xing。《通过越狱对 ChatGPT
    进行红队测试：偏见、鲁棒性、可靠性与毒性》。CoRR abs/2301.12867, 2023。'
- en: '[51] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and
    Transferable Adversarial Attacks on Aligned Language Models. CoRR abs/2307.15043,
    2023.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Andy Zou, Zifan Wang, J. Zico Kolter 和 Matt Fredrikson。《对齐语言模型的普遍且可转移的对抗攻击》。CoRR
    abs/2307.15043, 2023。'
