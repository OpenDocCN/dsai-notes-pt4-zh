- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:45:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and
    Committee Discussions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs的Auto Arena：通过代理对抗和委员会讨论自动化LLM评估
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20267](https://ar5iv.labs.arxiv.org/html/2405.20267)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20267](https://ar5iv.labs.arxiv.org/html/2405.20267)
- en: Ruochen Zhao^(1,2)  , Wenxuan Zhang², Yew Ken Chia^(2,3) , Deli Zhao², Lidong
    Bing²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ruochen Zhao^(1,2)  , Wenxuan Zhang², Yew Ken Chia^(2,3) , Deli Zhao², Lidong
    Bing²
- en: ¹ Nanyang Technological University, Singapore
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 南洋理工大学，新加坡
- en: ² DAMO Academy, Alibaba Group; ³ Singapore University of Technology and Design
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ² DAMO学院，阿里巴巴集团；³ 新加坡科技设计大学
- en: ruochen002@e.ntu.edu.sg;
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ruochen002@e.ntu.edu.sg;
- en: '{saike.zwx, yewken.chia, deli.zdl, l.bing}@alibaba-inc.com'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{saike.zwx, yewken.chia, deli.zdl, l.bing}@alibaba-inc.com'
- en: '[https://auto-arena.github.io/](https://auto-arena.github.io/)   Work done
    while the author was an intern at DAMO Academy, Alibaba. Yew Ken Chia is under
    the Joint Ph.D. Program between DAMO Academy and SUTD.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://auto-arena.github.io/](https://auto-arena.github.io/)   本工作在作者作为DAMO学院实习生期间完成。 Yew
    Ken Chia在DAMO学院与SUTD的联合博士项目下进行研究。'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As LLMs evolve on a daily basis, there is an urgent need for a trustworthy evaluation
    method that can provide robust evaluation results in a timely fashion. Currently,
    as static benchmarks are prone to contamination concerns, users tend to trust
    human voting platforms, such as Chatbot Arena. However, human annotations require
    extensive manual efforts. To provide an automatic, robust, and trustworthy evaluation
    framework, we innovatively propose the Auto-Arena of LLMs, which automates the
    entire evaluation process with LLM agents. Firstly, an examiner LLM devises queries.
    Then, a pair of candidate LLMs engage in a multi-round peer-battle around the
    query, during which the LLM’s true performance gaps become visible. Finally, a
    committee of LLM judges collectively discuss and determine the winner, which alleviates
    bias and promotes fairness. In our extensive experiment on the 17 newest LLMs,
    Auto-Arena shows the highest correlation with human preferences, providing a promising
    alternative to human evaluation platforms.¹¹1  We release the code at [https://github.com/Auto-Arena/Auto-Arena-LLMs.](https://github.com/Auto-Arena/Auto-Arena-LLMs.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）每天都在进化，迫切需要一种可靠的评估方法，能够及时提供强有力的评估结果。目前，由于静态基准测试容易受到污染，用户倾向于信任人工投票平台，如Chatbot
    Arena。然而，人工注释需要大量的手动工作。为了提供一种自动化、强大且可信的评估框架，我们创新性地提出了LLMs的Auto-Arena，它通过LLM代理自动化整个评估过程。首先，考官LLM设计查询。然后，一对候选LLM围绕查询进行多轮对抗，在此过程中LLM的真实表现差距显现出来。最后，一组LLM评审委员会集体讨论并决定获胜者，这减少了偏见，促进了公平。在我们对17个最新LLMs的广泛实验中，Auto-Arena显示出与人类偏好的最高相关性，为人类评估平台提供了一个有前景的替代方案。¹¹1 
    我们在[https://github.com/Auto-Arena/Auto-Arena-LLMs](https://github.com/Auto-Arena/Auto-Arena-LLMs)上发布了代码。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Since ChatGPT and GPT-4 [[23](#bib.bib23)] gained popularity, large language
    models (LLMs) have risen to the forefront of technological innovation, capturing
    broad industry and social interests. This enthusiasm has spurred numerous organizations
    to release their own LLMs [[27](#bib.bib27), [26](#bib.bib26)]. However, the rapid
    pace at which these models are released and updated poses a significant challenge
    for users attempting to understand their capabilities and monitor their evolution.
    Consequently, there has been a pressing demand for comprehensively evaluating
    LLMs recently.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自从ChatGPT和GPT-4 [[23](#bib.bib23)] 获得广泛关注以来，大型语言模型（LLMs）已成为技术创新的前沿，吸引了广泛的行业和社会兴趣。这种热情促使众多组织发布了自己的LLMs
    [[27](#bib.bib27), [26](#bib.bib26)]。然而，这些模型的快速发布和更新速度给用户理解其能力和监控其演变带来了重大挑战。因此，最近对全面评估LLMs的需求愈发迫切。
- en: One line of research conducts automatic evaluation with static datasets. Among
    these, static datasets with predefined metrics, such as GSM8k [[9](#bib.bib9)]
    and MMLU [[15](#bib.bib15)], are constructed with aspect-specific input-output
    pairs, such as questions and their corresponding answers. Given the questions,
    the LLM-produced answers are compared to ground-truth answers using metrics such
    as accuracy. However, these approaches suffer significantly from contamination
    concerns [[24](#bib.bib24)], where models may have been inadvertently exposed
    to elements of the test datasets during training, thereby skewing evaluation results.
    The rigid ground-truth answers also limit their utility in assessing models’ performance
    on general or open-ended questions. Model-based evaluation, such as MT-Bench [[32](#bib.bib32)]
    and AlpacaEval [[12](#bib.bib12)], provides an alternative for evaluating LLMs
    on open-ended questions. These methods typically ask two models to generate responses
    to the same open-ended question and then employ a strong judge model (e.g., GPT-4)
    to choose the better response. However, the static question sets still pose the
    issue of contamination. Additionally, the assumption of the existence of a strong
    judge model makes the evaluation framework less generalizable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一项研究通过静态数据集进行自动评估。其中，这些静态数据集带有预定义的指标，例如GSM8k [[9](#bib.bib9)]和MMLU [[15](#bib.bib15)]，它们由特定方面的输入-输出对（例如问题及其对应的答案）构成。给定问题后，LLM生成的答案将与基准答案进行比较，使用如准确度等指标。然而，这些方法在污染问题上受到显著影响 [[24](#bib.bib24)]，模型在训练过程中可能无意中接触到了测试数据集的元素，从而扭曲了评估结果。严格的基准答案也限制了它们在评估模型在一般或开放性问题上的表现的效用。基于模型的评估，例如MT-Bench [[32](#bib.bib32)]和AlpacaEval [[12](#bib.bib12)]，提供了一种替代方案来评估LLM在开放性问题上的表现。这些方法通常要求两个模型生成对同一开放性问题的回答，然后使用强大的评判模型（例如GPT-4）来选择更好的回答。然而，静态问题集仍然存在污染问题。此外，假设存在强大的评判模型使得评估框架的通用性较差。
- en: Aside from automated evaluations, human assessment, although requiring significant
    manual efforts, remains the gold standard for most users. A noticeable example
    is Chatbot Arena [[32](#bib.bib32)], which is a crowdsourced voting platform that
    gathers anonymous votes on LLM performances and calculates ELO scores to rank
    these models. The resulting leaderboard²²2  https://leaderboard.lmsys.org/ is
    widely considered a trustworthy indicator of an LLM’s general capabilities. However,
    a reliable model evaluation on this platform must be supported by more than 10k
    human votes, which requires considerable time and effort. Consequently, when newly
    developed models enter the scene, they often struggle to quickly amass a significant
    number of votes. Moreover, this strong reliance on human votes limits its application
    in various scenarios. For example, the performance of non-English languages is
    difficult to estimate, as most of the queries on the platform are in English.
    The completely open participation may also result in uneven evaluation quality.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动化评估，人工评估虽然需要大量的手动工作，但仍然是大多数用户的金标准。一个显著的例子是Chatbot Arena [[32](#bib.bib32)]，这是一个众包投票平台，用于收集对LLM表现的匿名投票并计算ELO分数来排名这些模型。生成的排行榜²²2 
    https://leaderboard.lmsys.org/ 被广泛认为是LLM一般能力的可信指标。然而，这个平台上的可靠模型评估必须得到超过10k个人投票的支持，这需要相当多的时间和努力。因此，当新开发的模型出现时，它们通常难以迅速积累大量投票。此外，对人工投票的高度依赖限制了其在各种场景中的应用。例如，非英语语言的表现难以估计，因为平台上的大多数查询都是英语。完全开放的参与也可能导致评估质量的不均衡。
- en: '![Refer to caption](img/37c12ca80da6f69fe065a2d43f4a3b37.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37c12ca80da6f69fe065a2d43f4a3b37.png)'
- en: 'Figure 1: An illustration of Auto Arena of LLMs.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLMs的Auto Arena示意图。
- en: 'To provide automatic, reliable, and human-like LLM evaluations, we propose
    Auto Arena of LLMs (Auto-Arena), a framework that automates the whole LLM evaluation
    process with LLM agents. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions"), the Auto-Arena framework consists of three stages. Firstly, an
    LLM examiner agent is tasked with generating questions, mimicking real-life users
    inputting queries. Secondly, two candidate LLMs interact with each other and engage
    in a multi-round peer battle by answering the seed question individually, criticizing
    the opponent’s weaknesses, and raising targeted follow-up queries to challenge
    the opponent further. During the multi-round battle process, the LLM’s true capabilities
    are drawn out and performance gaps become visible. Lastly, a committee of LLM
    judge agents collectively discusses and evaluates the ability of the two candidates,
    mimicking the human voting process. By automating the entire evaluation process
    with LLM agents, we alleviate data contamination concerns in static datasets,
    reduce single-model bias with collective decision-making, and avoid long wait
    times for new models entering the human voting platform.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提供自动化、可靠且类人的LLM评估，我们提出了LLM自动化竞技场（Auto-Arena），这是一个通过LLM代理自动化整个LLM评估过程的框架。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Auto Arena of LLMs: Automating LLM Evaluations with
    Agent Peer-battles and Committee Discussions")所示，Auto-Arena框架包含三个阶段。首先，一个LLM考官代理负责生成问题，模拟真实用户输入查询。其次，两个候选LLM相互互动，通过单独回答种子问题、批评对方的弱点，并提出有针对性的后续问题进一步挑战对方，进行多轮同伴对抗。在多轮对抗过程中，LLM的真实能力被显现，性能差距变得可见。最后，一个LLM评审委员会集体讨论和评估两个候选者的能力，模拟人类投票过程。通过用LLM代理自动化整个评估过程，我们缓解了静态数据集中数据污染的问题，通过集体决策减少了单模型偏差，避免了新模型进入人类投票平台时的长时间等待。'
- en: To verify the evaluation framework, we run an extensive experiment with 17 models.
    Compared to static and model-based benchmarks, Auto-Arena increases the Spearman
    correlation with human preferences by 4.5%, resulting in state-of-the-art alignment.
    Before and after peer battles, the Spearman correlation with human preferences
    increases by 46.4%, verifying our hypothesis the peer battles can better display
    performance gaps. Before and after committee discussions, committee agreement
    increases by 20%, showcasing the effectiveness of the committee discussion mechanism.
    By studying the peer battles, we discover intriguing LLM agent behaviors such
    as self-improvement and competitive actions. Using Chinese as an example, we also
    show that such an automated framework is easily extendable to non-mainstream languages
    and domains. Along with the project, we release a leaderboard, a website with
    demos, and the codebase. We are actively maintaining the leaderboard and altering
    the questions to provide reliable results in a timely manner.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证评估框架，我们进行了一个包含17个模型的大规模实验。与静态和基于模型的基准相比，Auto-Arena将与人类偏好的斯皮尔曼相关系数提高了4.5%，实现了最先进的对齐。在同伴对抗赛前后，与人类偏好的斯皮尔曼相关系数提高了46.4%，验证了我们的假设，即同伴对抗赛能更好地显示性能差距。在委员会讨论前后，委员会的一致性提高了20%，展示了委员会讨论机制的有效性。通过研究同伴对抗赛，我们发现了一些有趣的LLM代理行为，如自我改进和竞争性行为。以中文为例，我们还展示了这种自动化框架易于扩展到非主流语言和领域。随项目一起，我们发布了一个排行榜、一个带有演示的网站和代码库。我们正积极维护排行榜，并更改问题，以提供及时可靠的结果。
- en: 2 Related Work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Since LLMs demonstrated general and zero-shot capabilities, researchers have
    been attempting to use one single strong LLM to judge other LLMs’ outputs. LLM-as-a-judge [[32](#bib.bib32)]
    introduces Chatbot Arena and MT-Bench, a multi-turn question set to mimic real-life
    user queries. Using a strong LLM judge, such as GPT-4, to evaluate the MT-Bench
    results achieves over 80% agreement with human preferences, showcasing the potential
    of such methods. Besides such usefulness evaluations, some attempt to use LLM-as-a-judge
    to examine the knowledge boundaries of LLMs. Language-Model-as-an-Examiner [[2](#bib.bib2)]
    constructs the LMExamQA dataset, which comprises of LM-generated, knowledge-intensive
    questions covering three cognitive levels, including knowledge memorization, knowledge
    comprehension, and knowledge analysis. These questions are required to be answered
    by the LLM itself correctly, limiting their complexity. Then, the LM evaluator
    and the candidate interact in a series of follow-up queries and the evaluator
    rates the responses on dimensions including accuracy and factuality. Multiple
    evaluators examine the model independently and vote for a final result. KIEval [[29](#bib.bib29)]
    also incorporates an LLM-powered “interactor” role to examine deep comprehension
    of knowledge, which is shown to mitigate contamination issues. By utilizing the
    LLMs to generate questions, these methods can effectively mitigate the contamination
    concerns on static datasets. However, such evaluations only examine the LLMs’
    capabilities on answering knowledge-intensive, closed-domain, and simpler questions,
    as they rely on the examiner’s internal knowledge. They also require the examiner
    to interact with each candidate individually, creating computational overheads
    and limiting the scope of queries.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自从大语言模型（LLMs）展示了其通用性和零样本能力以来，研究人员一直尝试使用一个强大的LLM来评估其他LLM的输出。LLM-as-a-judge [[32](#bib.bib32)]
    引入了Chatbot Arena和MT-Bench，这是一套多轮问题集，用于模拟真实用户查询。使用强大的LLM评估者，如GPT-4，来评估MT-Bench的结果，与人类偏好的吻合度超过80%，展示了此类方法的潜力。除了这些有用性评估之外，还有一些尝试使用LLM-as-a-judge来检验LLM的知识边界。Language-Model-as-an-Examiner
    [[2](#bib.bib2)] 构建了LMExamQA数据集，该数据集包括由LLM生成的知识密集型问题，涵盖了三种认知层次，包括知识记忆、知识理解和知识分析。这些问题要求LLM自身正确回答，限制了其复杂性。然后，LM评估者与候选者进行一系列后续查询，评估者在准确性和真实性等维度上对回答进行评分。多个评估者独立检查模型并投票选出最终结果。KIEval
    [[29](#bib.bib29)] 还结合了一个LLM驱动的“互动者”角色来检查知识的深度理解，这被证明能缓解污染问题。通过利用LLMs生成问题，这些方法可以有效地减轻静态数据集上的污染问题。然而，这些评估仅仅检验了LLMs在回答知识密集型、封闭域以及较简单问题上的能力，因为它们依赖于评估者的内部知识。同时，它们还需要评估者与每个候选者进行单独互动，造成了计算开销并限制了查询的范围。
- en: Moreover, using a single LLM judge still suffer from a bias towards LLM-generated
    summaries [[20](#bib.bib20)], inflated scores in multilingual evaluation [[14](#bib.bib14)],
    verbosity bias [[12](#bib.bib12)], and difficulties when evaluating candidates
    with close performance [[25](#bib.bib25)]. To mitigate these problems, some recent
    works propose using multiple agents for collaborative evaluation, mimicking a
    peer review process. Some works simulate different personas with the same LLM.
    DRPE [[28](#bib.bib28)] uses multi-roleplayer prompting to mimic different roles
    with the same base model and integrate multiple outputs as votes for the final
    results. ChatEval [[6](#bib.bib6)] simulates different evaluator personas with
    the same base model to engage in debates, reaching a final evaluation result.
    While these methods come from the same motivation as our committee discussion
    component, they are still built on existing static datasets and do not mitigate
    the model-specific bias as they still use the same LLM to simulate different personas.
    To further incorporate different LLMs as evaluators, PRD [[17](#bib.bib17)] allows
    2 LLMs to discuss an evaluation and assigns higher voting weights to the LLM reviewers
    with stronger capabilities. Peer-review-in-LLMs [[22](#bib.bib22)] further optimizes
    the voting weights as a learnable parameter. WideDeep [[30](#bib.bib30)] sets
    up a multi-layer neural network structure where each LLM evaluator functions as
    neurons. PRE [[8](#bib.bib8)] selects a small group of reviewers to produce evaluations
    individually and then asks a “chair” to produce the final evaluation using the
    aggregated evaluations. They show that the multi-agent approach effectively mitigates
    single-model bias. This line of work is similar to our “LLM judge committee”.
    However, they are still limited on static datasets and specific domains. Moreover,
    they lack the collective discussion component, which is shown to improve agreement
    significantly.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用单一的 LLM 评审者仍然面临对 LLM 生成摘要的偏见[[20](#bib.bib20)]、多语言评估中的虚高评分[[14](#bib.bib14)]、冗长性偏见[[12](#bib.bib12)]，以及在评估表现接近的候选者时的困难[[25](#bib.bib25)]。为缓解这些问题，一些近期的研究提出了使用多个代理进行协作评估，模拟同行评审过程。一些研究使用相同的
    LLM 模拟不同的角色。DRPE [[28](#bib.bib28)] 使用多角色提示来模拟具有相同基础模型的不同角色，并将多个输出整合为最终结果的投票。ChatEval
    [[6](#bib.bib6)] 模拟具有相同基础模型的不同评审角色进行辩论，得出最终评估结果。虽然这些方法与我们的委员会讨论组件有相同的动机，但它们仍然建立在现有的静态数据集上，并且未能减轻模型特定的偏见，因为它们仍然使用相同的
    LLM 来模拟不同的角色。为了进一步纳入不同的 LLM 作为评审者，PRD [[17](#bib.bib17)] 允许两个 LLM 讨论评估，并为能力较强的
    LLM 评审者分配更高的投票权重。Peer-review-in-LLMs [[22](#bib.bib22)] 进一步优化了投票权重作为可学习的参数。WideDeep
    [[30](#bib.bib30)] 建立了一个多层神经网络结构，其中每个 LLM 评审者充当神经元。PRE [[8](#bib.bib8)] 选择一小组评审者分别进行评估，然后请一位“主席”使用汇总的评估结果进行最终评估。他们表明，多代理方法有效地缓解了单一模型的偏见。这一研究方向类似于我们的“LLM
    评审委员会”。然而，它们仍然局限于静态数据集和特定领域。此外，它们缺乏集体讨论组件，这已被证明可以显著提高一致性。
- en: Compared to using LLMs for evaluations, LLM-debate is a relatively new research
    area. Cohen et al. [[10](#bib.bib10)] shows that LLM interactions and cross-examination
    can effectively discover factual errors. Debate [[11](#bib.bib11)] shows that
    multi-agent debate can improve factuality and reasoning abilities. MAD [[19](#bib.bib19)]
    shows that LLM debate can encourage divergent thinking, which is helpful for tasks
    that require deep levels of contemplation. Khan et al. [[16](#bib.bib16)] shows
    that even non-expert weak LLMs can supervise expert LLMs if we allow the two LLM
    experts to engage in debates. Moreover, Zhao et al. [[31](#bib.bib31)] and Gu
    et al. [[13](#bib.bib13)] show interesting case studies where LLMs are engaged
    in simulated competitive environments, where they demonstrate human-like strategies.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用 LLM 进行评估相比，LLM 辩论是一个相对较新的研究领域。Cohen 等人 [[10](#bib.bib10)] 表明 LLM 互动和交叉审查可以有效发现事实错误。Debate
    [[11](#bib.bib11)] 表明多代理辩论可以提高事实准确性和推理能力。MAD [[19](#bib.bib19)] 表明 LLM 辩论可以鼓励发散性思维，这对于需要深度思考的任务非常有帮助。Khan
    等人 [[16](#bib.bib16)] 表明，即使是非专家的弱 LLM 如果允许两个 LLM 专家进行辩论，也可以监督专家 LLM。此外，赵等人 [[31](#bib.bib31)]
    和顾等人 [[13](#bib.bib13)] 展示了 LLM 参与模拟竞争环境的有趣案例，其中它们展示了类似人类的策略。
- en: 3 The Auto Arena Framework
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 自动竞技框架
- en: 'The Auto-Arena framework consists of three stages: question generation, peer
    battles, and committee discussions. These three stages are run sequentially and
    fully automated with LLM-based agents. The overall design of our Auto-Arena framework
    is presented in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    All prompts are included in Appendix [A.1](#A1.SS1 "A.1 Prompts Used ‣ Appendix
    A Appendix ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'Auto-Arena框架包含三个阶段：问题生成、同侪对战和委员会讨论。这三个阶段依次运行，并由基于LLM的代理完全自动化。我们的Auto-Arena框架的整体设计如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Auto Arena of LLMs: Automating LLM Evaluations
    with Agent Peer-battles and Committee Discussions")所示。所有提示包括在附录 [A.1](#A1.SS1
    "A.1 Prompts Used ‣ Appendix A Appendix ‣ Auto Arena of LLMs: Automating LLM Evaluations
    with Agent Peer-battles and Committee Discussions")。'
- en: 3.1 Question Generation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题生成
- en: 'For debate questions, as using a static dataset could incur data contamination
    concerns and result in unfair evaluations, we ask an LLM examiner agent to dynamically
    generate questions. The examiner agent could be any capable LLM. As the question
    simply serves as a starting point for the peer battle, which is the primary focus
    of evaluation, the question quality has relatively less impact on the overall
    evaluation framework. Similar to MT-Bench [[32](#bib.bib32)], we utilize 8 common
    categories in real-life conversations: writing, roleplay, extraction, reasoning,
    math, coding, STEM knowledge, and humanities/social science knowledge. In the
    prompt, the examiner is provided with a sample question and encouraged to generate
    diverse and difficult questions to ensure the depth and width of the debates examined.
    A subset of the generated questions is shown in Appendix [A.2](#A1.SS2 "A.2 Example
    Questions Generated ‣ Appendix A Appendix ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '对于辩论问题，由于使用静态数据集可能引发数据污染问题并导致评估不公，我们要求LLM考官代理动态生成问题。考官代理可以是任何具备能力的LLM。由于问题仅作为同侪对战的起点，而对战是评估的主要关注点，因此问题质量对整体评估框架的影响相对较小。类似于MT-Bench [[32](#bib.bib32)]，我们使用了8个真实对话中的常见类别：写作、角色扮演、提取、推理、数学、编码、STEM知识和人文/社会科学知识。在提示中，考官提供了一个样本问题，并鼓励生成多样化和困难的问题，以确保辩论的深度和广度。生成的问题子集展示在附录
    [A.2](#A1.SS2 "A.2 Example Questions Generated ‣ Appendix A Appendix ‣ Auto Arena
    of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions")。'
- en: 'Specifically, as the examiner agent will also participate in the following
    debates, previous methods [[2](#bib.bib2)] could incur self-enhancement bias as
    the examiner agents only devise questions that they are confident about. We try
    to avoid such bias with the following two designs: 1\. We do not disclose to the
    examiner that it will participate in this tournament. 2\. We do not ask the examiner
    to only generate questions that it can solve. Thus, the resulting questions should
    not cause much enhancement bias for the examiner agent itself.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，由于考官代理也将参与后续辩论，之前的方法 [[2](#bib.bib2)] 可能会引发自我增强偏见，因为考官代理只会设计他们自信的问题。我们通过以下两个设计尝试避免这种偏见：1.
    我们不向考官透露其将参与本次比赛。2. 我们不要求考官只生成他们能解决的问题。因此，生成的问题不应对考官代理自身造成过多的增强偏见。
- en: 3.2 Peer Debate
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 同侪辩论
- en: '![Refer to caption](img/5717dc86e1696f86303cba72779ad5db.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5717dc86e1696f86303cba72779ad5db.png)'
- en: 'Figure 2: The process of a Lincoln-Douglas-style peer-battle with the actions
    used.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：林肯-道格拉斯风格同侪对战的过程及使用的动作。
- en: After the questions are drafted, we conduct peer battles around these questions
    among the candidate LLMs. In one peer battle, two candidate LLMs debate around
    the given question, point out the opponent’s weaknesses, and raise follow-up questions
    that the opponent may fail to respond to.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 问题草稿完成后，我们在候选LLM之间进行围绕这些问题的同侪对战。在一次同侪对战中，两名候选LLM围绕给定问题辩论，指出对方的弱点，并提出对方可能无法回答的后续问题。
- en: 'In the peer battle, each candidate LLM can utilize 4 types of actions: 1\.
    Think, where the candidate thinks about the question and plans its strategy. This
    action is hidden to the opponent at all times. 2\. Respond, where the candidate
    responds to the aforementioned question. 3\. Criticize, where the candidate points
    out the loopholes and mistakes in the opponent’s previous responses. 4\. Raise,
    where the candidate raises follow-up questions that are specifically designed
    to make the opponent expose its weaknesses.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在对战中，每个候选 LLM 可以使用 4 种类型的操作：1\. 思考，其中候选者考虑问题并制定策略。此操作始终对对手隐藏。2\. 回应，其中候选者回答上述问题。3\.
    批评，其中候选者指出对手先前回应中的漏洞和错误。4\. 提问，其中候选者提出特别设计的问题以暴露对手的弱点。
- en: 'The peer battle is designed according to the Lincoln-Douglas debate format
    ³³3  [https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format](https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format),
    which is a competitive one-to-one debate used for popular competitions such as
    National Speech and Debate Association competitions. Overall, the peer battle
    consists of 3 rounds, where the candidates take turns to speak. The entire dialogue
    history is visible to both candidates. The process is illustrated in Figure [2](#S3.F2
    "Figure 2 ‣ 3.2 Peer Debate ‣ 3 The Auto Arena Framework ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    In the first round, A gives an initial response to the examiner’s question; B
    criticizes the weaknesses in A’s response and raises a targeted follow-up question;
    and A responds to B’s question. In the second round, A and B are reversed: B gives
    an initial response to the examiner’s question (without seeing A’s response);
    A criticizes and raises questions; and B responds to A’s question. In the third
    round, A and B cross-examine each other. A starts by criticizing B’s previous
    loopholes and raises follow-up questions. After responding, B also criticizes
    A’s loopholes and raises questions. A concludes the battle by responding again.
    In this process, both A and B get an equal number of each action to ensure fairness.
    To further reduce position bias, A and B’s order is randomly shuffled at the beginning
    of each debate.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '对战设计采用了 Lincoln-Douglas 辩论格式 ³³3  [https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format](https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format)，这是用于流行竞赛的竞争性一对一辩论，如全国演讲与辩论协会比赛。总体而言，对战包含
    3 轮，候选者轮流发言。整个对话历史对两位候选者都可见。过程如图 [2](#S3.F2 "Figure 2 ‣ 3.2 Peer Debate ‣ 3 The
    Auto Arena Framework ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent
    Peer-battles and Committee Discussions") 所示。在第一轮中，A 对考官的问题做出初步回应；B 批评 A 的回应中的弱点并提出有针对性的后续问题；A
    回应 B 的问题。在第二轮中，A 和 B 交换角色：B 对考官的问题做出初步回应（不看 A 的回应）；A 批评并提问；B 回应 A 的问题。在第三轮中，A
    和 B 互相交叉质询。A 先批评 B 先前的漏洞并提出后续问题。回应后，B 也批评 A 的漏洞并提出问题。A 通过再次回应来结束对战。在此过程中，A 和 B
    进行相同数量的每种操作以确保公平。为了进一步减少位置偏见，A 和 B 的顺序在每次辩论开始时会随机打乱。'
- en: Depending on which turn it is, we provide an action guide to the candidate,
    specifying the objectives and corresponding actions for this turn. Similar to
    human debate competitions, we time the candidates by providing a maximum length,
    which is also specified in the prompts. Any responses beyond the required length
    will be cut off. This design also mitigates verbosity bias in LLM-as-a-judge [[32](#bib.bib32)],
    where LLM judges prefer longer and more verbose responses.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据轮次，我们向候选者提供操作指南，指定该轮的目标和相应操作。类似于人工辩论比赛，我们通过提供最大长度来计时候选者，这也在提示中指定。超出要求长度的回应将被截断。这一设计也减少了
    LLM 作为评判者的冗长偏见 [[32](#bib.bib32)]，其中 LLM 评判者偏好更长且更详细的回应。
- en: 3.3 Committee Discussions
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 委员会讨论
- en: After the peer battle takes place, a committee of LLM judge agents will collectively
    determine the winner. The committee is always selected as the best five LLMs according
    to the current ranking. In the first round, the committee is initialized with
    MMLU [[15](#bib.bib15)] scores to approximate LLM performances. They will first
    be asked to read through the battle history, elaborate judgment reasons, and give
    a verdict on whether A is better, or B is better, or if there is a tie.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在对战结束后，一组 LLM 评判代理将共同决定胜者。委员会总是根据当前排名选出最好的五个 LLM。在第一轮中，委员会以 MMLU [[15](#bib.bib15)]
    分数初始化，以近似 LLM 表现。他们将首先阅读对战历史，详细阐述判断理由，并裁定 A 更好、B 更好，还是平局。
- en: After the initial judgments are formed, the committee engages in a discussion.
    Each judge will read the other judge’s verdicts and decide whether to adjust the
    ratings and elaborate on the reasons. The collective intelligence introduces diverse
    viewpoints, improves judgment quality, and mitigates single-model bias. Finally,
    the winning candidate is decided by the majority voting of the discussed judgments.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步判断形成后，委员会将进行讨论。每位评判者将阅读其他评判者的裁决，并决定是否调整评分并详细说明理由。集体智慧引入了多样化的观点，提升了判断质量，减少了单一模型的偏见。最终，获胜者由讨论中的多数票决定。
- en: For logical-reasoning questions that have ground-truth answers (reasoning, code,
    math), LLM-as-a-judge is known to show weak performances in judging the quality
    of responses. We adopt prior approaches to establish the reference-based judge [[32](#bib.bib32)].
    Specifically, we utilize the strongest model (according to the current ranking)
    to generate a reference answer and provide it to the judge when evaluating the
    peer battle.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些具有真实答案的逻辑推理问题（推理、代码、数学），LLM 作为评判者在判断回答质量时表现较弱。我们采用了先前的方法来建立基于参考的评判标准 [[32](#bib.bib32)]。具体来说，我们利用最强的模型（根据当前排名）生成参考答案，并在评估对战时提供给评判者。
- en: 4 Using Auto-Arena to Derive Trustworthy Rankings
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用 Auto-Arena 推导可靠的排名
- en: 4.1 Experimental Setup
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Model Selection:'
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型选择：
- en: For the main experiment, we first select 7 models from the top 30 list on Chatbot
    Arena with more than 10k votes each. The models are selected to be the best or
    the latest models from each popular model family at the time of experiments. To
    construct a comprehensive leaderboard⁴⁴4[https://huggingface.co/spaces/Auto-Arena/Leaderboard](https://huggingface.co/spaces/Auto-Arena/Leaderboard),
    we further consider 10 more models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主要实验，我们首先从 Chatbot Arena 上前 30 名的模型中选择 7 个，每个模型的投票超过 10k。选择的模型是在实验时来自每个流行模型系列中最好的或最新的模型。为了构建一个全面的排行榜⁴⁴4[https://huggingface.co/spaces/Auto-Arena/Leaderboard](https://huggingface.co/spaces/Auto-Arena/Leaderboard)，我们进一步考虑了
    10 个额外的模型。
- en: 'Baselines:'
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基线：
- en: For the baselines, we consider widely used evaluation benchmarks, including
    static datasets with fixed metrics and model-based metrics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基线，我们考虑了广泛使用的评估基准，包括静态数据集与固定指标和基于模型的指标。
- en: '1.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Static datasets with fixed metrics
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态数据集与固定指标
- en: (a)
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: MMLU (Massive Multitask Language Understanding) [[15](#bib.bib15)], an extensive
    benchmark dataset that covers 57 subjects and tests both world knowledge and problem-solving
    ability.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: MMLU（大规模多任务语言理解） [[15](#bib.bib15)]，这是一个涵盖 57 个主题的广泛基准数据集，测试世界知识和问题解决能力。
- en: (b)
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: OpenLLM Leaderboard [[3](#bib.bib3)], a leaderboard calculated by averaging
    performance metrics on 6 key benchmarks, including MMLU.
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: OpenLLM 排行榜 [[3](#bib.bib3)]，这是一个通过对 6 个关键基准（包括 MMLU）的性能指标进行平均计算得出的排行榜。
- en: '2.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Static datasets with model-based metrics
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态数据集与基于模型的指标
- en: (a)
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: MT-Bench [[32](#bib.bib32)], a multi-turn question set consisting of 80 questions.
    Model responses are graded by GPT-4.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: MT-Bench [[32](#bib.bib32)]，这是一个由 80 道问题组成的多轮问题集。模型的回答由 GPT-4 评分。
- en: (b)
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: MT-Bench Hard [[18](#bib.bib18)], a benchmark dataset with 1,000 challenging
    user queries collected on Chatbot Arena. Model responses are graded by GPT-4.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: MT-Bench Hard [[18](#bib.bib18)]，这是一个包含 1,000 个具有挑战性的用户查询的基准数据集，收集于 Chatbot
    Arena。模型的回答由 GPT-4 评分。
- en: (c)
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: Length-controlled AlpacaEval [[12](#bib.bib12)], an automatic benchmark based
    on the AlpacaFarm evaluation set, which tests the ability of models to follow
    general user instructions. Model responses are graded by GPT-4.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长度控制的 AlpacaEval [[12](#bib.bib12)]，这是一个基于 AlpacaFarm 评估集的自动化基准，测试模型遵循一般用户指令的能力。模型的回答由
    GPT-4 评分。
- en: 'Setup:'
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 设置：
- en: 'Among the 7 participants, we conduct a swiss-style tournament: For $n$.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 7 个参与者中，我们进行瑞士式比赛：对于 $n$。
- en: Each pair of candidates engage in 40 peer battles, with 5 questions from each
    of the 8 categories. The questions are generated by GPT-4\. As each battle consists
    of 3 rounds (each candidate speaks for 4 times), we expect the competition scale
    to be approximately the same as MT-Bench (80 questions, each candidate speaks
    twice).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每对候选者参与40场对战，每场对战包含8个类别中的5个问题。问题由GPT-4生成。由于每场对战包括3轮（每位候选者发言4次），我们预计比赛规模大致与MT-Bench相同（80个问题，每位候选者发言两次）。
- en: In the tournament, the rating scores are provided by calculating the ELO rating
    system [[1](#bib.bib1), [4](#bib.bib4)], which has become the standard practice
    in competitive games such as chess. Similar to the Chatbot Arena score calculation
    procedure [[7](#bib.bib7)], we compute the Bradley-Terry (BT) coefficients [[5](#bib.bib5)]
    for better statistical estimation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在比赛中，评分是通过计算ELO评级系统[[1](#bib.bib1), [4](#bib.bib4)]提供的，该系统已成为像国际象棋等竞争性游戏的标准实践。类似于Chatbot
    Arena评分计算程序[[7](#bib.bib7)]，我们计算Bradley-Terry (BT) 系数[[5](#bib.bib5)]以获得更好的统计估计。
- en: We initialize the Swiss tournament rankings according to MMLU scores, which
    is a static approximate of model performances. At the end of each pairing, we
    re-calculate ELO scores of current models. The committee is selected to be the
    remaining 5 models (besides the candidates). When the participant number increases,
    the committee is selected as the best 5 LLMs based on current ELO rankings. After
    the initial judgments, the committee members engage in one round of discussion.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据MMLU得分初始化瑞士锦标赛排名，这是模型性能的静态近似。在每轮配对结束时，我们重新计算当前模型的ELO得分。委员会由剩余的5个模型（除候选者外）组成。当参与者数量增加时，委员会根据当前ELO排名选择为最佳的5个LLMs。经过初步判断后，委员会成员进行一轮讨论。
- en: '4.2 Results: Alignment with Human Preferences'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果：与人类偏好的对齐
- en: 'Table 1: Correlation analysis on evaluation benchmarks on 7 LLMs.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：对7个LLM的评估基准的相关性分析。
- en: '|  | Spearman Correlation |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | Spearman相关性 |'
- en: '| --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| MMLU to Arena | 89.3% |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| MMLU到Arena | 89.3% |'
- en: '| OpenLLM to Arena | 89.3% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM到Arena | 89.3% |'
- en: '| MT-Bench to Arena | 82.9% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench到Arena | 82.9% |'
- en: '| MT-Bench-Hard to Arena | 89.3% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench-Hard到Arena | 89.3% |'
- en: '| LC-AlpacaEval to Arena | 90.0% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| LC-AlpacaEval到Arena | 90.0% |'
- en: '| *Auto-Arena to Arena* | 96.4% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| *Auto-Arena到Arena* | 96.4% |'
- en: 'We regard Chatbot Arena (referred to as “Arena” in the following tables) scores
    as a trustworthy indicator of human preferences and the general capabilities of
    LLMs. Table [1](#S4.T1 "Table 1 ‣ 4.2 Results: Alignment with Human Preferences
    ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating
    LLM Evaluations with Agent Peer-battles and Committee Discussions") shows the
    Spearman correlations with Chatbot Arena scores achieved by various benchmarks.
    As all benchmarks evaluate only in English, we compare with the English-only Chatbot
    Arena scores. As a result, we observe that both static and model-based baselines
    result in a similar level of Spearman correlation around 90%, with LC-AlpacaEval
    slightly surpassing other benchmarks. Then, Auto-Arena is able to improve the
    correlation to 96.4%, outperforming SOTA performance by 6.4%. Notably, among all
    benchmarks, Auto-Arena is the only one that requires no human efforts, eliminating
    the need for manual dataset collection. The high alignment with human preferences
    could originate from the human-like design, which effectively mimics the human
    users’ voting processes.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '我们认为Chatbot Arena（在下表中称为“Arena”）的得分是人类偏好和LLM整体能力的可靠指标。表[1](#S4.T1 "Table 1
    ‣ 4.2 Results: Alignment with Human Preferences ‣ 4 Using Auto-Arena to Derive
    Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent
    Peer-battles and Committee Discussions")展示了各个基准测试与Chatbot Arena得分的Spearman相关性。由于所有基准测试仅在英语中进行评估，我们与仅有英语的Chatbot
    Arena得分进行比较。结果显示，静态和基于模型的基准都产生了类似水平的Spearman相关性，约为90%，其中LC-AlpacaEval略微超过了其他基准。随后，Auto-Arena将相关性提高到96.4%，超越了SOTA表现6.4%。值得注意的是，在所有基准中，Auto-Arena是唯一一个不需要人工干预的，消除了手动数据集收集的需要。高一致性可能源于其类人设计，有效模拟了人类用户的投票过程。'
- en: 4.3 Analysis on Peer Battles
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 对战分析
- en: 'Table 2: Spearman Correlation between Auto-Arena and Chatbot Arena (“Arena”)
    With and Without Peer Battles.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：Auto-Arena与Chatbot Arena（“Arena”）之间在有无对战情况下的Spearman相关性。
- en: '|  | Without Peer Battles | With Peer Battles |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 无对战 | 有对战 |'
- en: '| --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Auto-Arena to Arena | 50.0% | 96.4% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Arena到Arena | 50.0% | 96.4% |'
- en: 'To investigate the effect of "peer battles" on the overall evaluation quality,
    we conduct an ablation study. As a baseline, we simply ask the committee to collectively
    evaluate the two candidates’ initial responses to the raised question. The procedure
    would be similar to model-based metrics such as MT-Bench and AlpacaEval. However,
    as we only included 40 questions, the evaluation is expected to be less reliable.
    The ablation results are shown in Table [2](#S4.T2 "Table 2 ‣ 4.3 Analysis on
    Peer Battles ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena
    of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions").
    Without the peer battle component, the Spearman correlation with human preferences
    drops from 96.4% to 50.0%. This result proves the importance of the peer battles,
    during which the performance gaps between candidates become more visible and robust
    to judges. Thus, peer battles can improve evaluation reliability and alignment
    with human preferences.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '为了调查“同伴对抗”对总体评价质量的影响，我们进行了一个消融研究。作为基准，我们简单地要求委员会集体评估两个候选人对提出问题的初始回应。该程序类似于基于模型的指标，例如
    MT-Bench 和 AlpacaEval。然而，由于我们只包含了 40 个问题，因此评价预计会较不可靠。消融结果见表 [2](#S4.T2 "Table
    2 ‣ 4.3 Analysis on Peer Battles ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions")。没有同伴对抗组件的情况下，与人类偏好的斯皮尔曼相关性从 96.4% 降至 50.0%。这一结果证明了同伴对抗的重要性，在这个过程中，候选人之间的表现差距变得更加明显并且对评审员更具稳健性。因此，同伴对抗可以提高评价的可靠性并与人类偏好对齐。'
- en: 4.4 Analysis on Committee Discussions
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 委员会讨论分析
- en: '![Refer to caption](img/c11c086b19932a74b8282497bf7615b4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c11c086b19932a74b8282497bf7615b4.png)'
- en: 'Figure 3: Cohen’s Kappa Agreement Among Judges Before Committee Discussions.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：委员会讨论前的 Cohen Kappa 一致性。
- en: '![Refer to caption](img/38f4cb7b54c0db43df0854b3bc5a5ad9.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/38f4cb7b54c0db43df0854b3bc5a5ad9.png)'
- en: 'Figure 4: Cohen’s Kappa Agreement Among Judges After One Round of Committee
    Discussion.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：委员会讨论后一轮的 Cohen Kappa 一致性。
- en: 'The committee discussion component is designed to introduce diverse viewpoints
    and produce more consistent verdicts. As shown in Figure [4](#S4.F4 "Figure 4
    ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy
    Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions"), the Cohen’s Kappa agreement [[21](#bib.bib21)] among
    judges before committee discussion is very low, averaging 0.10, which indicates
    slight agreement. We notice that weak model judges and strong model judges has
    an especially low agreement, such as GPT-4 and Llama -2\. This shows that general
    model capabilities could result in significant performance gaps when used as judges.
    After one round of Committee Discussion, Figure [4](#S4.F4 "Figure 4 ‣ 4.4 Analysis
    on Committee Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣
    Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions") shows an overall improvement in agreement across all judge combinations.
    In the discussion process, judges are exposed to more viewpoints, among which
    some may be convincing enough to result in a change in verdict. After discussions,
    the average Cohen’s Kappa increases to 0.38, indicating fair agreement.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '委员会讨论组件旨在引入多样的观点，并产生更一致的裁决。如图 [4](#S4.F4 "Figure 4 ‣ 4.4 Analysis on Committee
    Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of
    LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions")
    所示，委员会讨论前的 Cohen Kappa 一致性 [[21](#bib.bib21)] 在评审员之间非常低，平均为 0.10，表示一致性较差。我们注意到，弱模型评审员和强模型评审员之间的一致性特别低，例如
    GPT-4 和 Llama -2。这表明通用模型能力可能导致在用作评审员时存在显著的表现差距。在经过一轮委员会讨论后，图 [4](#S4.F4 "Figure
    4 ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using Auto-Arena to Derive Trustworthy
    Rankings ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions") 显示所有评审员组合之间的一致性整体提高。在讨论过程中，评审员接触到更多的观点，其中一些可能足够有说服力，以导致裁决的变化。讨论后，平均
    Cohen Kappa 增加到 0.38，表示一致性良好。'
- en: 'Table 3: Probability of Agreement among Judges. Agreement is defined as the
    mean probability of two random judges agreeing with each other.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：评审员之间一致性的概率。一致性定义为两个随机评审员互相一致的平均概率。
- en: '|  | Auto Arena | Auto Arena | MT-Bench |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | Auto Arena | Auto Arena | MT-Bench |'
- en: '|  | (Before discussion) | (After discussion) | Human evaluation |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | （讨论前） | （讨论后） | 人类评估 |'
- en: '| Agreement | 48% | 68% | 67% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 一致性 | 48% | 68% | 67% |'
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.4 Analysis on Committee Discussions ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the agreement
    probability among judges. Agreement probability is defined as the mean probability
    of two random judges agreeing with each other. After committee discussion, the
    agreement increases by 20%, matching the agreement level among human annotators
    on MT-Bench. This observation indicates that committee discussions can improve
    the quality of judgments significantly in terms of consistency and reliability,
    matching human-level performance.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S4.T3 "表 3 ‣ 4.4 委员会讨论分析 ‣ 4 使用 Auto-Arena 推导可信排名 ‣ LLM 的 Auto Arena：通过代理对战和委员会讨论自动化
    LLM 评估") 显示了评委之间的一致性概率。一致性概率定义为两位随机评委相互一致的平均概率。在委员会讨论后，一致性提高了 20%，与 MT-Bench 上的人类注释者的一致性水平相匹配。这一观察结果表明，委员会讨论可以显著提高判断的质量，提升一致性和可靠性，达到人类水平的表现。
- en: 4.5 Adding Models to the Leaderboard
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 向排行榜中添加模型
- en: '![Refer to caption](img/ca7a9ad542aeba5bd31ca016bd14b153.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca7a9ad542aeba5bd31ca016bd14b153.png)'
- en: 'Figure 5: Elo Score Changes of Adding Llama-3 to the Existing Ranking of 7
    Models.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：将 Llama-3 添加到现有 7 个模型排名中的 Elo 分数变化。
- en: '![Refer to caption](img/53fc1e7831eb0424a117a5198e51f05e.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/53fc1e7831eb0424a117a5198e51f05e.png)'
- en: 'Figure 6: Elo Scores of 11 Models by Auto-Arena on Chinese.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Auto-Arena 在中文中对 11 个模型的 Elo 分数。
- en: '![Refer to caption](img/95994034a6cc6cf9eeab1156a02a3169.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/95994034a6cc6cf9eeab1156a02a3169.png)'
- en: 'Figure 7: Elo Scores of 17 Models by Auto-Arena on English.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：Auto-Arena 在英文中对 17 个模型的 Elo 分数。
- en: To add a new candidate to the finished tournament, we ask it to debate with
    $log_{2}(n)$ is the number of total participants after adding the new candidate.
    We initialize the first pairing by asking the new candidate to debate with the
    opponent with the most similar MMLU score.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将一个新的候选者添加到已完成的比赛中，我们要求它与`$log_{2}(n)$`进行辩论，其中`n`是添加新候选者后的总参与者数量。我们通过要求新候选者与
    MMLU 得分最相似的对手进行第一次配对来初始化。
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the ELO
    score changes of adding a new participant (Llama-3, which was newly released)
    to the existing ranking. It has $log_{2}(8)=3$.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S4.F5 "图 5 ‣ 4.5 向排行榜中添加模型 ‣ 4 使用 Auto-Arena 推导可信排名 ‣ LLM 的 Auto Arena：通过代理对战和委员会讨论自动化
    LLM 评估") 显示了将新参与者（刚刚发布的 Llama-3）添加到现有排名中的 ELO 分数变化。它的`$log_{2}(8)=3$`。
- en: 'Therefore, we repeat the process and add a total of 10 models to the existing
    tournament, resulting in a comprehensive leaderboard. We choose the best-performing
    and newest models from each major model family in the top 20 list on Chatbot Arena.
    Moreover, we add 4 mainstream Chinese LLMs that are not on Chatbot Arena, which
    are GLM, SenseChat, Minimax, and Wenxin. Figure [7](#S4.F7 "Figure 7 ‣ 4.5 Adding
    Models to the Leaderboard ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee
    Discussions") shows the overall ELO scores produced by Auto-Arena on the 17 models.
    On models included in Chatbot Arena, we could recover rankings with 92.3% accuracy.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们重复这一过程，总共将 10 个模型添加到现有的比赛中，形成了一个全面的排行榜。我们从 Chatbot Arena 的前 20 名列表中选择了表现最佳且最新的模型。此外，我们还添加了
    4 个不在 Chatbot Arena 上的主流中文 LLM，包括 GLM、SenseChat、Minimax 和 Wenxin。图 [7](#S4.F7
    "图 7 ‣ 4.5 向排行榜中添加模型 ‣ 4 使用 Auto-Arena 推导可信排名 ‣ LLM 的 Auto Arena：通过代理对战和委员会讨论自动化
    LLM 评估") 显示了 Auto-Arena 对 17 个模型产生的整体 ELO 分数。在包括在 Chatbot Arena 中的模型上，我们可以以 92.3%
    的准确率恢复排名。
- en: 'Table 4: Correlation analysis on evaluation benchmarks on 17 LLMs after extension.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：扩展后对 17 个 LLM 的评估基准的相关性分析。
- en: '|  | Spearman Correlation |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | Spearman 相关性 |'
- en: '| --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| MMLU to Arena | 90.1% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MMLU 对 Arena | 90.1% |'
- en: '| OpenLLM to Arena | 85.7% |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM 对 Arena | 85.7% |'
- en: '| MT-Bench to Arena | 89.3% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench 对 Arena | 89.3% |'
- en: '| MT-Bench-Hard to Arena | 86.7% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench-Hard 对 Arena | 86.7% |'
- en: '| LC-AlpacaEval to Arena | 90.3% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| LC-AlpacaEval 对 Arena | 90.3% |'
- en: '| *Auto-Arena to Arena* | 94.5% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| *Auto-Arena 对 Arena* | 94.5% |'
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the Spearman
    correlations on the set of 17 models after expansion. Auto-Arena remains the model
    most aligned with human preferences by a margin of 4%. Therefore, Auto-Arena of
    LLMs is generalizable and robust for maintaining a leaderboard on a large number
    of LLMs.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[4](#S4.T4 "表格 4 ‣ 4.5 将模型添加到排行榜 ‣ 4 使用Auto-Arena推导可信排名 ‣ LLM的Auto Arena：通过代理同行对抗和委员会讨论自动化LLM评估")显示了在扩展后的17个模型集合上的Spearman相关性。Auto-Arena仍然是最符合人类偏好的模型，领先4%。因此，LLM的Auto-Arena具有广泛的适用性和稳定性，适用于维护大量LLM的排行榜。
- en: 4.6 Easy Extension to Other Domains and Languages
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 轻松扩展到其他领域和语言
- en: As Auto-Arena of LLMs is fully automatic, it can be easily adapted to evaluate
    LLMs in non-mainstream domains or languages. As a case study, we conduct a Chinese
    tournament on 11 out of the 17 models that are claimed to have multi-lingual proficiency.
    The only adaptation effort required is translating the prompts into Chinese. Then,
    the examiner automatically generates questions in Chinese and the candidates battle
    in Chinese. Similarly, for adaptation to another domain, the only effort is to
    change the “domain” specification in the examiner’s prompts.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM的Auto-Arena是全自动的，它可以轻松适应用于评估非主流领域或语言的LLM。作为案例研究，我们对17个模型中声称具备多语言能力的11个模型进行中文比赛。唯一需要的适配工作是将提示翻译成中文。然后，考官会自动生成中文问题，候选人在中文中进行对抗。类似地，对于适应到其他领域，唯一需要的工作是更改考官提示中的“领域”说明。
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ 4.5 Adding Models to the Leaderboard ‣ 4 Using
    Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating LLM
    Evaluations with Agent Peer-battles and Committee Discussions") shows the ELO
    scores derived by Auto-Arena for Chinese evaluation. We notice that the ranking
    differs significantly from English results. For example, SenseChat-5, an LLM specifically
    trained for Chinese usage, is able to improve from 10th place (in English) to
    1st place (in Chinese). On the contrary, Llama-3-70b, which only uses 5% multilingual
    data during pretraining, drops from 4th place (in English) to 10th place (in Chinese).
    As Chinese evaluation benchmarks are limited, we compare to the Chinese-only leaderboard
    on Chatbot Arena, which constitute for 10.36% of all collected dialogues. On the
    7 models that are also included in Chatbot Arena, Auto-Arena recovers their ELO
    scores with 92.86% correlation and restores rankings with 90.5% accuracy.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#S4.F7 "图7 ‣ 4.5 将模型添加到排行榜 ‣ 4 使用Auto-Arena推导可信排名 ‣ LLM的Auto Arena：通过代理同行对抗和委员会讨论自动化LLM评估")显示了Auto-Arena为中文评估推导的ELO分数。我们注意到排名与英文结果显著不同。例如，SenseChat-5，一个专门为中文使用训练的LLM，能够从第10名（英文）提升到第1名（中文）。相反，仅在预训练中使用5%多语言数据的Llama-3-70b，从第4名（英文）跌至第10名（中文）。由于中文评估基准有限，我们与Chatbot
    Arena上的中文排行榜进行比较，后者占所有收集对话的10.36%。在Chatbot Arena中也包括的7个模型中，Auto-Arena以92.86%的相关性恢复了它们的ELO分数，并以90.5%的准确性恢复了排名。
- en: 5 Investigation of LLM’s Behaviors in Competitive Peer Battles
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 研究LLM在竞争同行对抗中的行为
- en: Beyond quantitative analysis, we take a deeper look into the peer battles and
    find several interesting behaviors of LLM agents in competitive environments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定量分析外，我们还深入探讨了同行对抗中的几种有趣的LLM代理行为。
- en: '![Refer to caption](img/88ed4356e75ea12fa1485602e41a6ca4.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/88ed4356e75ea12fa1485602e41a6ca4.png)'
- en: 'Figure 8: LLM agents display competitive behaviors in peer battles.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：LLM代理在同行对抗中展示竞争行为。
- en: '![Refer to caption](img/9f6aa4c2e20b685fa1b4af83686e1815.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9f6aa4c2e20b685fa1b4af83686e1815.png)'
- en: 'Figure 9: LLM agents learn from each other in peer battles.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：LLM代理在同行对抗中相互学习。
- en: 5.1 LLM can display competitive behaviors
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 LLM可以表现出竞争行为
- en: 'The example in Figure [9](#S5.F9 "Figure 9 ‣ 5 Investigation of LLM’s Behaviors
    in Competitive Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with
    Agent Peer-battles and Committee Discussions") shows excerpts of a peer battle
    around question: “how many unique ways to arrange letters in ‘Letter’.” Candidate
    A (powered by Yi-34B-Chat) gives a wrong answer as it miscounts occurrences for
    repeated letters and miscalculates factorials. The opponent B (powered by Claude-3-Haiku)
    quickly and precisely points out these two issues and skillfully raised a follow-up
    that targets A’s weaknesses: “how about the word ‘BANANA’?” Then, A makes the
    same mistake as before. Seeing these results, the judge then determines that A
    is the better assistant. In peer battles, we see that LLM candidates efficiently
    understand the rules of the competitive environment and can design specific strategies
    to attack the opponent in order to win.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S5.F9 "图 9 ‣ 5 竞争性对战中的LLM行为调查 ‣ LLM自动竞技场：通过代理对战和委员会讨论自动化LLM评估")中的示例展示了围绕问题“‘Letter’中字母的独特排列方式有多少种”进行的对战片段。候选人A（由Yi-34B-Chat提供支持）给出了错误的答案，因为它错误计算了重复字母的出现次数和阶乘。对手B（由Claude-3-Haiku提供支持）迅速且准确地指出了这两个问题，并巧妙地提出了针对A弱点的后续问题：“那‘BANANA’这个词呢？”然后，A再次犯了之前的错误。看到这些结果，评审最终确定A是更好的助手。在对战中，我们看到LLM候选人能有效理解竞争环境的规则，并能够设计具体的策略来攻击对手以赢得比赛。
- en: 5.2 LLM candidates can improve by learning from its opponents
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 LLM候选人可以通过从对手那里学习来提高
- en: 'Figure [9](#S5.F9 "Figure 9 ‣ 5 Investigation of LLM’s Behaviors in Competitive
    Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions") shows a roleplay example between Claude-3-Haiku (Candidate
    A) and Command-r-Plus (Candidate B). In the first round, A answers the question
    plainly while B, in addition to answering the question, also employs the appropriate
    speech style, which better matches the “roleplay” instructions. Then, in the rounds
    after, without any explicit instructions, A learns from its opponent and also
    incorporates the speech style. This case shows an interesting observation that,
    even in competitive environments, LLM candidates can display learning behaviors
    and improve from the interactions. Expanding upon this observation, using the
    interplay between LLM agents to improve performances could be a promising future
    paradigm of learning.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S5.F9 "图 9 ‣ 5 竞争性对战中的LLM行为调查 ‣ LLM自动竞技场：通过代理对战和委员会讨论自动化LLM评估")展示了Claude-3-Haiku（候选人A）与Command-r-Plus（候选人B）之间的角色扮演示例。在第一轮中，A简单地回答了问题，而B除了回答问题外，还采用了合适的演讲风格，更好地符合了“角色扮演”的指示。然后，在随后的轮次中，A在没有明确指示的情况下从对手那里学习，并也采用了这种演讲风格。这一案例展示了一个有趣的观察，即使在竞争环境中，LLM候选人也能表现出学习行为并通过互动进行改进。在此观察的基础上，利用LLM代理之间的互动来提高性能可能成为未来学习的一个有前景的范式。
- en: 5.3 Peer-battles make the performance gaps become visible
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 对战使性能差距显现
- en: '![Refer to caption](img/cfe21eebd1d05c9ec23b8db5c2e058fc.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cfe21eebd1d05c9ec23b8db5c2e058fc.png)'
- en: 'Figure 10: Performance gaps between candidates become visible in peer battles.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：候选人之间的性能差距在对战中显现
- en: 'In the example shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.3 Peer-battles make
    the performance gaps become visible ‣ 5 Investigation of LLM’s Behaviors in Competitive
    Peer Battles ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles
    and Committee Discussions"), given a math question on infinite series, both candidate
    A (powered by Claude-3-Haiku) and candidate B (powered by GPT-4-Turbo) provide
    correct answers in the first round. However, as they raise follow-up questions
    to each other, the performance gap becomes more visible: Candidate B is able to
    provide a more elaborate and helpful response. Therefore, although the judges
    initially decided that it was a tie result, after seeing the subsequent debate
    rounds, they change to favoring assistant B.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](#S5.F10 "图 10 ‣ 5.3 对战使性能差距显现 ‣ 5 竞争性对战中的LLM行为调查 ‣ LLM自动竞技场：通过代理对战和委员会讨论自动化LLM评估")中的示例展示了关于无限级数的数学问题，候选人A（由Claude-3-Haiku提供支持）和候选人B（由GPT-4-Turbo提供支持）在第一轮中都给出了正确的答案。然而，当他们互相提出后续问题时，性能差距变得更加明显：候选人B能够提供更详细和有帮助的回答。因此，尽管评审最初决定结果为平局，但在看到后续的辩论轮次后，他们改变了对B助手的偏好。
- en: 'This example shows that the debate process indeed pushes the candidate LLM’s
    capabilities to the limit, testing deeper understandings and reasoning abilities.
    Also shown in the previous Table [2](#S4.T2 "Table 2 ‣ 4.3 Analysis on Peer Battles
    ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena of LLMs: Automating
    LLM Evaluations with Agent Peer-battles and Committee Discussions"), the peer-battle
    design is indispensable for a robust and comprehensive evaluation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '这个示例表明，辩论过程确实将候选LLM的能力推向了极限，测试了更深层次的理解和推理能力。前表[2](#S4.T2 "Table 2 ‣ 4.3 Analysis
    on Peer Battles ‣ 4 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto Arena
    of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions")中所示，同行对抗设计对于一个稳健而全面的评估是不可或缺的。'
- en: 6 Conclusions
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this paper, we innovatively design a completely automatic evaluation framework:
    Auto-Arena of LLMs. By using LLM agents to generate questions, employing LLM candidates
    in peer battles, and evaluating responses using LLM committee discussions, Auto-Arena
    produces less-contaminated, robust, and trustworthy evaluation results. In the
    extensive experiments, Auto-Arena achieves the highest correlation with human
    preferences, despite requiring zero human efforts. It is easily adaptable to other
    domains and resources, promoting fairness of AI system evaluations. The peer battles
    also demonstrate several interesting LLM behaviors in competitive environments,
    including attacking and learning from the opponents. Along with the paper, we
    release a website, demos, and an actively-maintained leaderboard. We hope this
    work could serve as a valuable tool for LLM evaluation.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本文创新性地设计了一个完全自动化的评估框架：LLMs的Auto-Arena。通过使用LLM代理生成问题，利用LLM候选者进行同行对抗，以及使用LLM委员会讨论来评估回答，Auto-Arena生成了污染较少、稳健且可信的评估结果。在广泛的实验中，Auto-Arena与人类偏好的相关性最高，尽管不需要任何人工努力。它易于适应其他领域和资源，促进了AI系统评估的公平性。同行对抗还展示了LLM在竞争环境中的一些有趣行为，包括攻击和从对手那里学习。我们还发布了一个网站、演示和一个积极维护的排行榜。我们希望这项工作能作为LLM评估的有价值工具。
- en: References
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful
    and harmless assistant with reinforcement learning from human feedback, 2022.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 使用来自人类反馈的强化学习训练有用且无害的助手，2022年。
- en: Bai et al. [2024] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi
    Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation
    models with language-model-as-an-examiner. *Advances in Neural Information Processing
    Systems*, 36, 2024.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2024] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi
    Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, 等。用语言模型作为评估者对基础模型进行基准测试。*神经信息处理系统进展*，第36卷，2024年。
- en: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, 和 Thomas
    Wolf. 开源LLM排行榜。[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023年。
- en: 'Boubdir et al. [2023] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker,
    and Marzieh Fadaee. Elo uncovered: Robustness and best practices in language model
    evaluation. In Sebastian Gehrmann, Alex Wang, João Sedoc, Elizabeth Clark, Kaustubh
    Dhole, Khyathi Raghavi Chandu, Enrico Santus, and Hooman Sedghamiz, editors, *Proceedings
    of the Third Workshop on Natural Language Generation, Evaluation, and Metrics
    (GEM)*, pages 339–352, Singapore, December 2023\. Association for Computational
    Linguistics. URL [https://aclanthology.org/2023.gem-1.28](https://aclanthology.org/2023.gem-1.28).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boubdir 等 [2023] Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker 和 Marzieh
    Fadaee. Elo 揭示：语言模型评估的鲁棒性和最佳实践。收录于 Sebastian Gehrmann, Alex Wang, João Sedoc,
    Elizabeth Clark, Kaustubh Dhole, Khyathi Raghavi Chandu, Enrico Santus 和 Hooman
    Sedghamiz 主编的 *第三届自然语言生成、评估与度量 (GEM) 研讨会论文集*，第 339–352 页，新加坡，2023 年 12 月。计算语言学协会。网址
    [https://aclanthology.org/2023.gem-1.28](https://aclanthology.org/2023.gem-1.28)。
- en: 'Bradley and Terry [1952] Ralph Allan Bradley and Milton E Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley 和 Terry [1952] Ralph Allan Bradley 和 Milton E Terry. 不完全区组设计的排名分析：I.
    成对比较方法。*Biometrika*，39(3/4)：324–345，1952。
- en: 'Chan et al. [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. *arXiv preprint arXiv:2308.07201*, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan 等 [2023] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang
    Zhang, Jie Fu 和 Zhiyuan Liu. Chateval：通过多代理辩论实现更好的 llm 基于评估器。*arXiv 预印本 arXiv:2308.07201*，2023。
- en: 'Chiang et al. [2024] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
    Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.
    Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms
    by human preference, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等 [2024] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
    Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph
    E. Gonzalez 和 Ion Stoica. 聊天机器人竞技场：一个通过人类偏好评估 llm 的开放平台，2024。
- en: 'Chu et al. [2024] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu.
    Pre: A peer review based large language model evaluator. *arXiv preprint arXiv:2401.15641*,
    2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu 等 [2024] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li 和 Yiqun Liu. Pre：基于同行评审的大型语言模型评估器。*arXiv
    预印本 arXiv:2401.15641*，2024。
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等 [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse 和 John Schulman. 训练验证器解决数学文字问题。*arXiv 预印本 arXiv:2110.14168*，2021。
- en: 'Cohen et al. [2023] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm
    vs lm: Detecting factual errors via cross examination. *arXiv preprint arXiv:2305.13281*,
    2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 等 [2023] Roi Cohen, May Hamri, Mor Geva 和 Amir Globerson. Lm 与 lm：通过交叉检查检测事实错误。*arXiv
    预印本 arXiv:2305.13281*，2023。
- en: Du et al. [2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate. *arXiv preprint arXiv:2305.14325*, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 [2023] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum 和 Igor
    Mordatch. 通过多代理辩论提高语言模型的事实性和推理能力。*arXiv 预印本 arXiv:2305.14325*，2023。
- en: 'Dubois et al. [2024] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B
    Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators.
    *arXiv preprint arXiv:2404.04475*, 2024.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubois 等 [2024] Yann Dubois, Balázs Galambosi, Percy Liang 和 Tatsunori B Hashimoto.
    长度控制的 alpacaeval：一种简单的去偏自动评估器方法。*arXiv 预印本 arXiv:2404.04475*，2024。
- en: 'Gu et al. [2024] Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai,
    Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, and
    Yanghua Xiao. Agentgroupchat: An interactive group chat simulacra for better eliciting
    emergent behavior, 2024.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 [2024] Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao Shen,
    Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng 和 Yanghua Xiao.
    Agentgroupchat：一种互动群聊模拟器以更好地引出突现行为，2024。
- en: Hada et al. [2023] Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee,
    Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. Are large
    language model-based evaluators the solution to scaling up multilingual evaluation?
    *arXiv preprint arXiv:2309.07462*, 2023.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hada 等 [2023] Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed
    Ahmed, Monojit Choudhury, Kalika Bali 和 Sunayana Sitaram. 基于大型语言模型的评估器是否是扩大多语言评估的解决方案？*arXiv
    预印本 arXiv:2309.07462*，2023。
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *Proceedings of the International Conference on Learning Representations
    (ICLR)*, 2021.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 [2021] Dan Hendrycks，Collin Burns，Steven Basart，Andy Zou，Mantas
    Mazeika，Dawn Song 和 Jacob Steinhardt。 测量大规模多任务语言理解。*国际学习表征会议 (ICLR) 会议论文集*，2021年。
- en: Khan et al. [2024] Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij
    Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel,
    and Ethan Perez. Debating with more persuasive llms leads to more truthful answers,
    2024.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等 [2024] Akbir Khan，John Hughes，Dan Valentine，Laura Ruis，Kshitij Sachan，Ansh
    Radhakrishnan，Edward Grefenstette，Samuel R. Bowman，Tim Rocktäschel 和 Ethan Perez。
    与更具说服力的 llms 辩论会导致更真实的回答，2024年。
- en: 'Li et al. [2023] Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and
    discussion improve large language model based evaluations. *arXiv preprint arXiv:2307.02762*,
    2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2023] Ruosen Li，Teerth Patel 和 Xinya Du。 Prd：同行排名和讨论改进大语言模型基础的评估。*arXiv
    预印本 arXiv:2307.02762*，2023年。
- en: 'Li* et al. [2024] Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua
    Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks:
    The arena-hard pipeline, April 2024. URL [https://lmsys.org/blog/2024-04-19-arena-hard/](https://lmsys.org/blog/2024-04-19-arena-hard/).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li* 等 [2024] Tianle Li*，Wei-Lin Chiang*，Evan Frick，Lisa Dunlap，Banghua Zhu，Joseph
    E. Gonzalez 和 Ion Stoica。 从实时数据到高质量基准：The arena-hard pipeline，2024年4月。URL [https://lmsys.org/blog/2024-04-19-arena-hard/](https://lmsys.org/blog/2024-04-19-arena-hard/)。
- en: Liang et al. [2023] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking
    in large language models through multi-agent debate, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等 [2023] Tian Liang，Zhiwei He，Wenxiang Jiao，Xing Wang，Yan Wang，Rui Wang，Yujiu
    Yang，Zhaopeng Tu 和 Shuming Shi。 通过多代理辩论鼓励大语言模型的发散思维，2023年。
- en: 'Liu et al. [2023] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment.
    *arXiv preprint arXiv:2303.16634*, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2023] Yang Liu，Dan Iter，Yichong Xu，Shuohang Wang，Ruochen Xu 和 Chenguang
    Zhu。 Gpteval：使用 gpt-4 进行更好的人类对齐的 nlg 评估。*arXiv 预印本 arXiv:2303.16634*，2023年。
- en: 'McHugh [2012] Mary L McHugh. Interrater reliability: the kappa statistic. *Biochemia
    medica*, 22(3):276–282, 2012.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McHugh [2012] Mary L McHugh. 评分者间可靠性：kappa 统计量。*生物化学医学*，22(3)：276–282，2012年。
- en: 'Ning et al. [2024] Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui
    Liu, Yu Wang, Ming Pang, and Li Yuan. Peer-review-in-llms: Automatic evaluation
    method for llms in open-environment. *arXiv preprint arXiv:2402.01830*, 2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ning 等 [2024] Kun-Peng Ning，Shuo Yang，Yu-Yang Liu，Jia-Yu Yao，Zhen-Hui Liu，Yu
    Wang，Ming Pang 和 Li Yuan。 Peer-review-in-llms：开放环境中 llms 的自动评估方法。*arXiv 预印本 arXiv:2402.01830*，2024年。
- en: OpenAI et al. [2024] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. Gpt-4 technical report, 2024.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人 [2024] OpenAI、Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge
    Akkaya、Florencia Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal
    Anadkat、Red Avila、Igor Babuschkin、Suchir Balaji、Valerie Balcom、Paul Baltescu、Haiming
    Bao、Mohammad Bavarian、Jeff Belgum、Irwan Bello、Jake Berdine、Gabriel Bernadett-Shapiro、Christopher
    Berner、Lenny Bogdonoff、Oleg Boiko、Madelaine Boyd、Anna-Luisa Brakman、Greg Brockman、Tim
    Brooks、Miles Brundage、Kevin Button、Trevor Cai、Rosie Campbell、Andrew Cann、Brittany
    Carey、Chelsea Carlson、Rory Carmichael、Brooke Chan、Che Chang、Fotis Chantzis、Derek
    Chen、Sully Chen、Ruby Chen、Jason Chen、Mark Chen、Ben Chess、Chester Cho、Casey Chu、Hyung
    Won Chung、Dave Cummings、Jeremiah Currier、Yunxing Dai、Cory Decareaux、Thomas Degry、Noah
    Deutsch、Damien Deville、Arka Dhar、David Dohan、Steve Dowling、Sheila Dunning、Adrien
    Ecoffet、Atty Eleti、Tyna Eloundou、David Farhi、Liam Fedus、Niko Felix、Simón Posada
    Fishman、Juston Forte、Isabella Fulford、Leo Gao、Elie Georges、Christian Gibson、Vik
    Goel、Tarun Gogineni、Gabriel Goh、Rapha Gontijo-Lopes、Jonathan Gordon、Morgan Grafstein、Scott
    Gray、Ryan Greene、Joshua Gross、Shixiang Shane Gu、Yufei Guo、Chris Hallacy、Jesse
    Han、Jeff Harris、Yuchen He、Mike Heaton、Johannes Heidecke、Chris Hesse、Alan Hickey、Wade
    Hickey、Peter Hoeschele、Brandon Houghton、Kenny Hsu、Shengli Hu、Xin Hu、Joost Huizinga、Shantanu
    Jain、Shawn Jain、Joanne Jang、Angela Jiang、Roger Jiang、Haozhun Jin、Denny Jin、Shino
    Jomoto、Billie Jonn、Heewoo Jun、Tomer Kaftan、Łukasz Kaiser、Ali Kamali、Ingmar Kanitscheider、Nitish
    Shirish Keskar、Tabarak Khan、Logan Kilpatrick、Jong Wook Kim、Christina Kim、Yongjik
    Kim、Jan Hendrik Kirchner、Jamie Kiros、Matt Knight、Daniel Kokotajlo、Łukasz Kondraciuk、Andrew
    Kondrich、Aris Konstantinidis、Kyle Kosic、Gretchen Krueger、Vishal Kuo、Michael Lampe、Ikai
    Lan、Teddy Lee、Jan Leike、Jade Leung、Daniel Levy、Chak Ming Li、Rachel Lim、Molly Lin、Stephanie
    Lin、Mateusz Litwin、Theresa Lopez、Ryan Lowe、Patricia Lue、Anna Makanju、Kim Malfacini、Sam
    Manning、Todor Markov、Yaniv Markovski、Bianca Martin、Katie Mayer、Andrew Mayne、Bob
    McGrew、Scott Mayer McKinney、Christine McLeavey、Paul McMillan、Jake McNeil、David
    Medina、Aalok Mehta、Jacob Menick、Luke Metz、Andrey Mishchenko、Pamela Mishkin、Vinnie
    Monaco、Evan Morikawa、Daniel Mossing、Tong Mu、Mira Murati、Oleg Murk、David Mély、Ashvin
    Nair、Reiichiro Nakano、Rajeev Nayak、Arvind Neelakantan、Richard Ngo、Hyeonwoo Noh、Long
    Ouyang、Cullen O’Keefe、Jakub Pachocki、Alex Paino、Joe Palermo、Ashley Pantuliano、Giambattista
    Parascandolo、Joel Parish、Emy Parparita、Alex Passos、Mikhail Pavlov、Andrew Peng、Adam
    Perelman、Filipe de Avila Belbute Peres、Michael Petrov、Henrique Ponde de Oliveira
    Pinto、Michael、Pokorny、Michelle Pokrass、Vitchyr H. Pong、Tolly Powell、Alethea Power、Boris
    Power、Elizabeth Proehl、Raul Puri、Alec Radford、Jack Rae、Aditya Ramesh、Cameron Raymond、Francis
    Real、Kendra Rimbach、Carl Ross、Bob Rotsted、Henri Roussez、Nick Ryder、Mario Saltarelli、Ted
    Sanders、Shibani Santurkar、Girish Sastry、Heather Schmidt、David Schnurr、John Schulman、Daniel
    Selsam、Kyla Sheppard、Toki Sherbakov、Jessica Shieh、Sarah Shoker、Pranav Shyam、Szymon
    Sidor、Eric Sigler、Maddie Simens、Jordan Sitkin、Katarina Slama、Ian Sohl、Benjamin
    Sokolowsky、Yang Song、Natalie Staudacher、Felipe Petroski Such、Natalie Summers、Ilya
    Sutskever、Jie Tang、Nikolas Tezak、Madeleine B. Thompson、Phil Tillet、Amin Tootoonchian、Elizabeth
    Tseng、Preston Tuggle、Nick Turley、Jerry Tworek、Juan Felipe Cerón Uribe、Andrea Vallone、Arun
    Vijayvergiya、Chelsea Voss、Carroll Wainwright、Justin Jay Wang、Alvin Wang、Ben Wang、Jonathan
    Ward、Jason Wei、CJ Weinmann、Akila Welihinda、Peter Welinder、Jiayi Weng、Lilian Weng、Matt
    Wiethoff、Dave Willner、Clemens Winter、Samuel Wolrich、Hannah Wong、Lauren Workman、Sherwin
    Wu、Jeff Wu、Michael Wu、Kai Xiao、Tao Xu、Sarah Yoo、Kevin Yu、Qiming Yuan、Wojciech
    Zaremba、Rowan Zellers、Chong Zhang、Marvin Zhang、Shengjia Zhao、Tianhao Zheng、Juntang
    Zhuang、William Zhuk 和 Barret Zoph。《GPT-4技术报告》，2024。
- en: Ravaut et al. [2024] Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen,
    Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, and Shafiq Joty. How much
    are llms contaminated? a comprehensive survey and the llmsanitize library, 2024.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉沃等人 [2024] 马修·拉沃、博升·丁、方凯·焦、海林·陈、邢轩·李、若晨·赵、成伟·秦、蔡明·熊和沙菲克·乔提。大型语言模型受污染的程度有多大？全面调查及llmsanitize库，2024年。
- en: 'Shen et al. [2023] Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and
    Lidong Bing. Large language models are not yet human-level evaluators for abstractive
    summarization. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, pages 4215–4233, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沈等人 [2023] 陈辉·沈、丽颖·程、轩非·阮、杨游和黎东·冯。大型语言模型尚未成为人类级别的抽象总结评估者。在*计算语言学协会：EMNLP 2023的发现*上，页码4215–4233，2023年。
- en: 'Team et al. [2024] Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson d’Autume,
    Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac
    Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant
    Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi
    Wang, Zhongkai Zhu, and Zhihui Xie. Reka core, flash, and edge: A series of powerful
    multimodal language models, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队等人 [2024] 雷卡团队、艾托尔·奥尔马萨巴尔、车正、赛普里安·德·马松·达图梅、丹尼·尤加塔马、德宇·傅、多诺万·翁、埃里克·陈、尤金妮·兰普雷赫特、海·范、艾萨克·翁、卡洛扬·阿列克谢夫、雷·李、马修·亨德森、马克斯·贝恩、米克尔·阿尔特克斯、尼尚特·雷兰、皮奥特·帕德莱夫斯基、齐·刘、任·陈、塞缪尔·普亚、雅正·杨、易·泰、余琦·王、钟凯·朱和志辉·谢。Reka
    core、flash和edge：一系列强大的多模态语言模型，2024年。
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图弗龙等人 [2023] 雨果·图弗龙、提博·拉夫里尔、高提耶·伊扎卡德、谢维尔·马尔蒂内、玛丽-安·拉肖、蒂莫西·拉克鲁瓦、巴蒂斯特·罗齐埃、纳曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔、奥雷利安·罗德里格斯、阿尔芒·朱利安、爱德华·格雷夫和吉约姆·兰普尔。Llama：开放且高效的基础语言模型，2023年。
- en: Wu et al. [2023] Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang.
    Large language models are diverse role-players for summarization evaluation. In
    *CCF International Conference on Natural Language Processing and Chinese Computing*,
    pages 695–707\. Springer, 2023.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023] 宁吴、明龚、林俊寿、闪亮梁和大新江。大型语言模型在总结评估中的多样角色扮演。在*CCF国际自然语言处理与中文计算会议*上，页码695–707。Springer，2023年。
- en: 'Yu et al. [2024] Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong
    Wang, Xing Xie, Yue Zhang, and Shikun Zhang. Kieval: A knowledge-grounded interactive
    evaluation framework for large language models. *arXiv preprint arXiv:2402.15043*,
    2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余等人 [2024] 卓浩·余、昌·高、文锦·姚、一东·王、伟业·叶、金东·王、邢·谢、岳·张和时坤·张。Kieval：一个基于知识的交互评估框架，用于大型语言模型。*arXiv预印本
    arXiv:2402.15043*，2024年。
- en: Zhang et al. [2023] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen
    Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer
    llm evaluators. *arXiv preprint arXiv:2308.01862*, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023] 星华·张、博文·余、海洋·余、杨宇·吕、廷文·刘、飞·黄、洪波·徐和永斌·李。更宽更深的llm网络是更公平的llm评估者。*arXiv预印本
    arXiv:2308.01862*，2023年。
- en: 'Zhao et al. [2023] Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie
    Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition behaviors
    in large language model-based agents, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2023] 秦林·赵、金东·王、益轩·张、义乔·金、凯杰·朱、浩·陈和邢·谢。Competeai：理解基于大型语言模型的代理中的竞争行为，2023年。
- en: Zheng et al. [2024] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information
    Processing Systems*, 36, 2024.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2024] 连敏·郑、魏林·蒋、英生·邢、思远·庄、张浩·吴、永浩·庄、子林·张、卓汉·李、大成·李、埃里克·邢等。使用mt-bench和聊天机器人竞技场评判llm作为评判者。*神经信息处理系统进展*，36，2024年。
- en: Appendix A Appendix
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: A.1 Prompts Used
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 使用的提示
- en: In this section, we list all prompts used, including prompts for question generation,
    peer battles, and examiners.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们列出了所有使用的提示，包括问题生成提示、同行对战提示和检查员提示。
- en: A.1.1 Prompts to Examiner agent
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 提示到检查员代理
- en: 'Table 5: Prompt components for the LLM Examiner agent.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：LLM检查员代理的提示组件。
- en: '| DOMAIN | DOMAIN_COMMAND | DOMAIN_EXAMPLE |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| DOMAIN | DOMAIN_COMMAND | DOMAIN_EXAMPLE |'
- en: '| --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| writing | It should be a user query that tasks the LLM to write something.
    | Compose an engaging travel blog post about a recent trip to Hawaii, highlighting
    cultural experiences and must-see attractions. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 写作 | 应该是一个用户查询，要求 LLM 撰写内容。 | 撰写一篇关于最近一次夏威夷旅行的引人入胜的旅行博客文章，重点介绍文化体验和必看的景点。
    |'
- en: '| roleplay | It should propose a scenario where the chatbot mimics a specific
    role/person. Give all necessary instructions and requests for its response. Then,
    send a beginning request to complete. | Pretend yourself to be Elon Musk in all
    the following conversations. Speak like Elon Musk as much as possible. Why do
    we need to go to Mars? |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演 | 应该提出一个场景，让聊天机器人模仿特定角色/人物。给出所有必要的指示和回应请求。然后，发送一个开始请求以完成。 | 假装你是埃隆·马斯克，在接下来的所有对话中都要像埃隆·马斯克一样说话。我们为什么需要去火星？
    |'
- en: '| extraction | It should consist of two parts: question and context. The question
    should test the chatbotś ability to correctly understand and extract information
    from the given context. Draft and provide a new context yourself. | Question:
    Evaluate the following movie reviews on a scale of 1 to 5, with 1 being very negative,
    3 being neutral, and 5 being very positive: Context: This movie released on Nov.
    18, 2019, was phenomenal. The cinematography, the acting, the plot - everything
    was top-notch. Never before have I been so disappointed with a movie. The plot
    was predictable and the characters were one-dimensional. In my opinion, this movie
    is the worst one to have been released in 2022\. The movie was okay. There were
    some parts I enjoyed, but there were also parts that felt lackluster. This is
    a movie that was released in Feb 2018 and seems to be quite ordinary. Return the
    answer as a JSON array of integers. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 提取 | 应包括两个部分：问题和背景。问题应该测试聊天机器人正确理解和提取给定背景信息的能力。自己拟定并提供一个新的背景。 | 问题：对以下电影评论进行
    1 到 5 的评分，其中 1 表示非常负面，3 表示中立，5 表示非常积极：背景：这部电影于 2019 年 11 月 18 日上映，表现非常出色。摄影、表演、情节——一切都很出色。我从未对一部电影如此失望。情节可预测，角色单一。依我看，这部电影是
    2022 年上映的最差电影。这部电影还不错。有些部分我很喜欢，但也有一些部分感觉平淡。这是一部于 2018 年 2 月上映的电影，似乎相当普通。返回一个整数的
    JSON 数组作为答案。 |'
- en: '| reasoning | It should be a specific question designed to test the LLMś reasoning
    skills. | Imagine you are participating in a race with a group of people. If you
    have just overtaken the second person, what’s your current position? Where is
    the person you just overtook? |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 应该是一个特定的问题，用于测试 LLM 的推理能力。 | 想象你正在与一群人比赛。如果你刚刚超过了第二个人，你现在的位置是什么？你刚刚超过的人在哪里？
    |'
- en: '| math | It should be a specific question designed to test the LLMś math skills.
    | The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3). What is
    the area of the triangle? |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 应该是一个特定的问题，用于测试 LLM 的数学技能。 | 一个三角形的顶点在 (0, 0)、(-1, 1) 和 (3, 3) 点上。这个三角形的面积是多少？
    |'
- en: '| coding | It should be a specific question designed to test the LLMś coding
    skills. | Develop a Python program that reads all the text files under a directory
    and returns top-5 words with the most number of occurrences. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 编码 | 应该是一个特定的问题，用于测试 LLM 的编码技能。 | 开发一个 Python 程序，读取目录下的所有文本文件，并返回出现次数最多的前
    5 个单词。 |'
- en: '| STEM knowledge | It should be a specific question designed to test the LLMś
    STEM knowledge. | In the field of quantum physics, what is superposition, and
    how does it relate to the phenomenon of quantum entanglement? |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| STEM 知识 | 应该是一个特定的问题，用于测试 LLM 的 STEM 知识。 | 在量子物理学中，什么是叠加态，它如何与量子纠缠现象相关联？
    |'
- en: '| humanities/social science knowledge | It should be a specific question designed
    to test the LLMś humanities/social science knowledge. | Provide insights into
    the correlation between economic indicators such as GDP, inflation, and unemployment
    rates. Explain how fiscal and monetary policies affect those indicators. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 人文学科/社会科学知识 | 应该是一个特定的问题，用于测试 LLM 的人文学科/社会科学知识。 | 提供对经济指标（如 GDP、通货膨胀和失业率）之间关联的见解。解释财政和货币政策如何影响这些指标。
    |'
- en: 'This is the prompt to the examiner agent for question generation. The domains
    and their respective commands are listed in [5](#A1.T5 "Table 5 ‣ A.1.1 Prompts
    to Examiner agent ‣ A.1 Prompts Used ‣ Appendix A Appendix ‣ Auto Arena of LLMs:
    Automating LLM Evaluations with Agent Peer-battles and Committee Discussions")'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给考官代理生成问题的提示。领域及其相应的命令列在[5](#A1.T5 "表 5 ‣ A.1.1 考官代理提示 ‣ A.1 使用的提示 ‣ 附录 A
    附录 ‣ LLM 自动竞技场：通过代理对战和委员会讨论自动化 LLM 评估")
- en: 'You have been assigned the task of drafting a set of [NUMBER] different user
    queries to a chat assistant on [DOMAIN]. Please strictly follow these 6 rules
    for the question: 1\. The question is likely for a user to ask in real life. Follow
    the format of the example query. [DOMAIN_COMMAND] 2\. It can be answered by the
    chatbot itself without additional inputs. 3\. You need to generate the queries
    as DIVERSIFED as possible. 4\. DO NOT add other words other than the query itself.
    5\. The question should be complicated and difficult, requiring in-depth understanding
    and analysis of the subject. Each question in one line, add the serial number
    in parenthesis (e.g., “(1).”, “(2).”) before each question. Example query: [DOMAIN_EXAMPLE]'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你被分配了一个任务，要求你为[DOMAIN]的聊天助手草拟一组[NUMBER]个不同的用户查询。请严格遵循以下6条规则来提问：1. 该问题可能是用户在现实生活中提出的。遵循示例查询的格式。[DOMAIN_COMMAND]
    2. 该问题可以由聊天机器人自己回答，无需额外输入。 3. 你需要生成尽可能多样化的查询。 4. 不要添加除了查询本身以外的其他词语。 5. 问题应当复杂且困难，需要对主题进行深入理解和分析。每个问题占一行，在每个问题前加上序号（例如，“（1）。”，“（2）。”）。示例查询：[DOMAIN_EXAMPLE]
- en: A.1.2 Prompts to Peer Battle Candidates
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 对战候选人提示
- en: 'Table 6: Action Guides for the Debater Agents.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：辩论代理的行动指南。
- en: '| actions | action guide |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| actions | 行动指南 |'
- en: '| --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | Action guide: only include . Use  if needed.
    Finish your whole response within 300 words, including . ENCLOSE EACH ACTION
    IN ITS RESPECTIVE TAGS! |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | 行动指南：仅包含。如有需要使用。将整个回答控制在300字以内，包括。用相应的标签包围每个行动！
    |'
- en: '| ,  | Action guide: include both  and .
    Use  if needed. Finish your whole response within 300 words, including
    . ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS! |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| ,  | 行动指南：包括和。如有需要使用。将整个回答控制在300字以内，包括。用相应的标签包围每个行动！
    |'
- en: '| , ,  | Action guide: include all of ,
    , and . Use  if needed. Finish your whole response within
    600 words, including . ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS! |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| , ,  | 行动指南：包括、和。如有需要使用。将整个回答控制在600字以内，包括。用相应的标签包围每个行动！
    |'
- en: 'This is the beginning prompt to the peer battle candidates. When possible,
    it is included as a system prompt. The action guide prompts are included in Table
    [6](#A1.T6 "Table 6 ‣ A.1.2 Prompts to Peer Battle Candidates ‣ A.1 Prompts Used
    ‣ Appendix A Appendix ‣ Auto Arena of LLMs: Automating LLM Evaluations with Agent
    Peer-battles and Committee Discussions"), where the actions are determined by
    the round and turn as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Peer Debate
    ‣ 3 The Auto Arena Framework ‣ Auto Arena of LLMs: Automating LLM Evaluations
    with Agent Peer-battles and Committee Discussions").'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给对战候选人的开始提示。如果可能的话，它会作为系统提示包含在内。行动指南提示包含在表[6](#A1.T6 "表 6 ‣ A.1.2 对战候选人提示
    ‣ A.1 使用的提示 ‣ 附录 A 附录 ‣ LLM 自动竞技场：通过代理对战和委员会讨论自动化 LLM 评估")中，其中的行动由回合和轮次决定，如图[2](#S3.F2
    "图 2 ‣ 3.2 对战辩论 ‣ 3 自动竞技场框架 ‣ LLM 自动竞技场：通过代理对战和委员会讨论自动化 LLM 评估")所示。
- en: You are a helpful assistant that provides accurate answers to user requests.
    As an experienced assistant, you follow the user’s requests and provide reliable
    responses as much as you can. You outline your reasons for the response to make
    it easy for the users to understand. While maintaining the important details in
    the responses, you aim to output concise and straight-to-the-point answers without
    being overly verbose.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个提供准确答案的有用助手。作为一个经验丰富的助手，你会遵循用户的请求，并尽可能提供可靠的回答。你会概述你的回答理由，使用户易于理解。在保持回答重要细节的同时，你力求简洁明了，避免过于冗长。
- en: 'This is a competitive chatbot arena. You are competing against another chatbot
    assistant in a debate and being judged by a committee on factors such as helpfulness,
    relevance, accuracy, depth, and creativity. After answering the initial user input,
    you will engage in a multi-round debate with your opponent. Below are your actions:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个竞争性的聊天机器人竞技场。你与另一位聊天助手进行辩论，并由委员会根据帮助性、相关性、准确性、深度和创造力等因素进行评判。在回答初始用户输入后，你将与对手进行多轮辩论。以下是你的行动：
- en: ': Think step-by-step to analyze the question or plan your strategy in
    the debate. This is hidden from the opponent. Only think when necessary and make
    it concise.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ': 逐步思考以分析问题或制定辩论策略。这对对手是隐藏的。仅在必要时思考，并保持简洁。'
- en: ': Answer to the user input as accurately as you can.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ': 尽可能准确地回答用户输入。'
- en: ': Criticize the weaknesses of your opponent’s response.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ': 批评对手回应的弱点。'
- en: ': Target your opponent’s weaknesses. Give a potential follow-up user
    input that the opponent could fail to respond. The input can be answered concisely
    and focus on variations or motivations of its previous response. Generate one
    input only. Be reasonable. Avoid becoming too specific or repetitive. DO NOT raise
    a follow-up if you DON’T SEE the opponent’s response!'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ': 针对对手的弱点。给出一个潜在的后续用户输入，对手可能无法回应。该输入可以简洁回答，并关注于对手之前回应的变化或动机。仅生成一个输入。要合理，避免过于具体或重复。如果没有看到对手的回应，请不要提出后续问题！'
- en: Follow the action guide strictly.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 严格按照行动指南执行。
- en: '[ACTION_GUIDE_PROMPT]'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[ACTION_GUIDE_PROMPT]'
- en: 'Initial user input: [QUESTION]'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '初始用户输入: [QUESTION]'
- en: 'After the agent responds, the opponent’s responses are fed in using this prompt:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理回应后，使用以下提示来输入对手的回应：
- en: '[ACTION_GUIDE_PROMPT] Opponent’s Response: [OPPONENT_RESPONSE]'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[ACTION_GUIDE_PROMPT] 对手的回应: [OPPONENT_RESPONSE]'
- en: For word limits, the  action is given 300 words. The  and
     actions are given 300 words in total. Including all 3 actions will have
    twice as many words. For writing-type questions that require a longer response
    (writing, roleplay, coding, humanities/social science knowledge), the 300 word
    limit is increased to 400\. Overall, both candidate A and B has the same amount
    of words for generation and the same amount of actions to ensure fairness. As
    LLMs have different tokenizers, we standardize all lengths by using the tiktoken
    package. Each word is approximated as $4/3$ tokens. The word limits are chosen
    after a carefully conducted length study.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于字数限制，行动有300个单词。和行动的总字数为300个单词。包括所有3个行动将有两倍的单词数。对于需要较长回应的写作类型问题（写作、角色扮演、编程、人文学科/社会科学知识），300字限制增加到400字。总体上，A和B候选人生成的字数和行动数相同，以确保公平。由于LLM使用不同的标记器，我们使用tiktoken包来标准化所有长度。每个单词大约等于$4/3$个标记。字数限制是在经过精心研究的长度研究后确定的。
- en: A.1.3 Prompts to Judges
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.3 对评委的提示
- en: 'This is the prompts to judge agents to derive the initial evaluations and verdicts:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于判断代理的初步评估和裁决的提示：
- en: 'This is a chatbot arena. Two AI assistants had a multi-round debate on who
    is more helpful. Please act as an impartial judge and evaluate the capability
    of two AI assistants. You should choose the assistant that follows instructions
    and answers questions better. Your evaluation should consider factors such as
    helpfulness, relevance, and accuracy. Begin your evaluation by comparing the responses
    of the two assistants and provide a short explanation. Avoid any position biases
    and ensure that the order in which the responses were presented does not influence
    your decision. DO NOT allow the LENGTH of the responses to influence your evaluation,
    choose the one that is straight-to-the-point instead of unnecessarily verbose.
    When the two candidates perform equally well, choose the SHORTER answer. Do not
    favor certain names of the assistants. Be as objective as possible. After providing
    your explanation concisely within 200 words, output your final verdict by strictly
    following this format: ‘‘[[A]]’’ if assistant A is better, ‘‘[[B]]’’ if assistant
    B is better, and ‘‘[[Tie]]’’ for a tie. Finish your judgement within 300 words.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个聊天机器人竞技场。两位 AI 助手就谁更有帮助进行了多轮辩论。请作为公正的评委，评估这两位 AI 助手的能力。你应该选择遵循指示和回答问题更好的助手。你的评估应考虑诸如帮助性、相关性和准确性等因素。通过比较两位助手的回答开始你的评估，并提供简短的解释。避免任何立场偏见，确保回答的顺序不会影响你的决定。不要让回答的长度影响你的评估，选择直接明了的回答而不是冗长的回答。当两个候选人表现相当时，选择较短的回答。不要偏袒某些助手的名字。尽量保持客观。在用200字以内简洁地提供解释后，按照以下格式严格给出最终裁决：“[[A]]”如果助手
    A 更好，“[[B]]”如果助手 B 更好，和“[[Tie]]”如果平局。完成你的判断在300字以内。
- en: 'This is the prompt to judges for discussion:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是供评委讨论的提示：
- en: 'Below are the responses from other judges in the committee. Please read them
    and decide whether you want to adjust your rating or maintain your original judgement.
    After providing your explanation, output your final verdict by strictly following
    this format: ‘‘[[A]]’’ if assistant A is better, ‘‘[[B]]’’ if assistant B is better,
    and ‘‘[[Tie]]’’ for a tie. Finish your judgement within 300 words.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是委员会其他评委的回应。请阅读这些回应，并决定是否调整你的评分或保持原判。提供解释后，按照以下格式严格给出最终裁决：“[[A]]”如果助手 A 更好，“[[B]]”如果助手
    B 更好，和“[[Tie]]”如果平局。完成你的判断在300字以内。
- en: A.2 Example Questions Generated
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 生成的示例问题
- en: 'We list some of the example questions generated here:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此列出了一些生成的示例问题：
- en: 'Writing:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 写作：
- en: 1\. Craft a detailed marketing strategy for a startup focusing on sustainable
    fashion, including social media campaigns and influencer partnerships.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 为一家专注于可持续时尚的初创公司制定详细的营销策略，包括社交媒体活动和网红合作。
- en: 2\. Write a comprehensive guide on the psychological effects of social media
    on teenagers, incorporating recent studies and expert opinions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 编写一份关于社交媒体对青少年心理影响的全面指南，结合最新研究和专家意见。
- en: 'Roleplay:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 角色扮演：
- en: 1\. Assume the role of a 19th-century British detective. How would you go about
    solving a mysterious disappearance in London using the technology and methods
    of your time?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 假设你是一名19世纪的英国侦探。你将如何利用当时的技术和方法来解决伦敦的神秘失踪案件？
- en: 2\. Pretend you are a Michelin-starred chef. Describe in detail how you would
    prepare a signature dish that embodies the essence of modern French cuisine.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 假装你是一位米其林星级厨师。详细描述你如何准备一份体现现代法国料理精髓的招牌菜。
- en: 'Extraction:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 提取：
- en: 1\. What are the three most significant historical events mentioned and their
    dates?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 提到的三大历史事件及其日期是什么？
- en: 'Context:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文：
- en: The article discusses several key moments in history, including the signing
    of the Magna Carta in 1215, which laid the groundwork for modern democracy. It
    also mentions the fall of the Berlin Wall in 1989 as a pivotal moment in the end
    of the Cold War. Another significant event highlighted is the moon landing on
    July 20, 1969, demonstrating major advancements in space exploration.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 文章讨论了几个历史关键时刻，包括1215年《大宪章》的签署，这为现代民主奠定了基础。它还提到1989年柏林墙的倒塌，作为冷战结束的一个关键时刻。另一个突出的事件是1969年7月20日的登月，展示了太空探索的重要进展。
- en: 2\. Identify the main therapeutic benefits and the active ingredient mentioned
    for each herbal remedy.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 确定每种草药治疗法的主要疗效和活性成分。
- en: 'Context:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文：
- en: The text provides an overview of various herbal remedies used for centuries.
    It mentions that Chamomile contains Bisabolol, which has anti-inflammatory and
    calming properties. Gingko Biloba, known for its flavonoids and terpenoids, enhances
    cognitive function and blood circulation. Lastly, Echinacea is recognized for
    its alkamides, which bolster the immune system.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文本概述了几种使用了数世纪的草药疗法。它提到**洋甘菊**含有双氢洋甘菊醇，具有抗炎和镇静作用。**银杏**以其黄酮和萜类化合物闻名，能够增强认知功能和血液循环。最后，**紫锥花**因其生物碱而被认可，这些成分有助于增强免疫系统。
- en: 'Reasoning:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 推理：
- en: 1\. If a cube’s volume is tripled, by what factor does the length of one of
    its sides increase?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如果一个立方体的体积被三倍增加，它的边长增加了多少倍？
- en: 2\. In a two-legged soccer match, Team A wins the first leg at home 3-0, but
    loses the second leg away 2-5\. Who advances to the next round, considering the
    away goals rule?
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在一场两回合的足球比赛中，A队在主场赢得第一回合3-0，但在客场输掉第二回合2-5。考虑到客场进球规则，哪支球队晋级到下一轮？
- en: 'math:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 数学：
- en: 1\. How do you solve the differential equation $dy/dx+2y=e^{(-2x)}$?
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何解方程 $dy/dx+2y=e^{(-2x)}$？
- en: 2\. What is the integral of ($x^{2}+2x+2)/(x^{3}+3x^{2}+3x+1)dx$?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 积分（$x^{2}+2x+2）/（x^{3}+3x^{2}+3x+1）dx$是什么？
- en: 'Coding:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 编程：
- en: 1\. How can I implement a function in C++ that dynamically allocates a 2D array
    based on user input sizes, initializes all elements to zero, and then deallocates
    the memory properly to avoid memory leaks?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何在C++中实现一个根据用户输入的大小动态分配2D数组的函数，初始化所有元素为零，并在最后正确释放内存以避免内存泄漏？
- en: 2\. Write a JavaScript function to fetch data from a given URL, parse the JSON
    response, and filter the results to return an array of items where a specific
    key’s value matches a condition.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 编写一个JavaScript函数，从给定的URL中获取数据，解析JSON响应，并筛选结果以返回一个数组，其中某个特定键的值满足条件。
- en: 'STEM knowledge:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: STEM知识：
- en: 1\. How do you calculate the Schwarzschild radius of a black hole, and what
    implications does this have for the concept of event horizons in general relativity?
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何计算黑洞的史瓦西半径，这对广义相对论中的事件视界概念有什么影响？
- en: 2\. Can you explain the process of splicing in eukaryotic gene expression and
    its significance in the diversity of the proteome?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 能否解释真核生物基因表达中的剪接过程及其在蛋白质组多样性中的重要性？
- en: 'Humanities/social science knowledge:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 人文学科/社会科学知识：
- en: 1\. Discuss the impact of colonial legacies on contemporary political structures
    in African countries, with examples.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 讨论殖民遗产对非洲国家当代政治结构的影响，并举例说明。
- en: 2\. Analyze the social and economic consequences of the one-child policy in
    China.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 分析中国独生子女政策的社会和经济后果。
