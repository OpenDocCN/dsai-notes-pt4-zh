- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 17:34:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:34:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释的交叉审查技术（ICE-T）：利用高度信息特征提升 LLM 性能
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06703](https://ar5iv.labs.arxiv.org/html/2405.06703)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06703](https://ar5iv.labs.arxiv.org/html/2405.06703)
- en: Goran Muric
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**戈兰·穆里奇**'
- en: InferLink Corporation
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: InferLink 公司
- en: Los Angeles, California
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 洛杉矶，加利福尼亚州
- en: gmuric@inferlink.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: gmuric@inferlink.com
- en: \AndBen Delay
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \AndBen Delay
- en: InferLink Corporation
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: InferLink 公司
- en: Los Angeles, California
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 洛杉矶，加利福尼亚州
- en: bdelay@inferlink.com
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: bdelay@inferlink.com
- en: \AndSteven Minton
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \AndSteven Minton
- en: InferLink Corporation
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: InferLink 公司
- en: Los Angeles, California
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 洛杉矶，加利福尼亚州
- en: sminton@inferlink.com
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: sminton@inferlink.com
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this paper, we introduce the Interpretable Cross-Examination Technique (ICE-T),
    a novel approach that leverages structured multi-prompt techniques with Large
    Language Models (LLMs) to improve classification performance over zero-shot and
    few-shot methods. In domains where interpretability is crucial, such as medicine
    and law, standard models often fall short due to their “black-box” nature. ICE-T
    addresses these limitations by using a series of generated prompts that allow
    an LLM to approach the problem from multiple directions. The responses from the
    LLM are then converted into numerical feature vectors and processed by a traditional
    classifier. This method not only maintains high interpretability but also allows
    for smaller, less capable models to achieve or exceed the performance of larger,
    more advanced models under zero-shot conditions. We demonstrate the effectiveness
    of ICE-T across a diverse set of data sources, including medical records and legal
    documents, consistently surpassing the zero-shot baseline in terms of classification
    metrics such as F1 scores. Our results indicate that ICE-T can be used for improving
    both the performance and transparency of AI applications in complex decision-making
    environments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了可解释的交叉审查技术（ICE-T），这是一种新颖的方法，它利用结构化的多提示技术与大语言模型（LLMs）结合，以提高分类性能，超越零样本和少样本方法。在解释性至关重要的领域，如医学和法律，标准模型常常由于其“黑箱”特性而表现不足。ICE-T
    通过使用一系列生成的提示，允许 LLM 从多个方向接近问题，从而解决这些限制。LLM 的响应随后被转换为数值特征向量，并由传统分类器处理。这种方法不仅保持了高可解释性，还允许较小、能力较弱的模型在零样本条件下达到或超越较大、更先进模型的性能。我们展示了
    ICE-T 在多种数据来源上的有效性，包括医疗记录和法律文档，在分类指标如 F1 分数上始终超越零样本基准。我们的结果表明，ICE-T 可以用于提高 AI
    应用在复杂决策环境中的性能和透明度。
- en: 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的交叉审查技术（ICE-T）：利用高度信息特征提升 LLM 性能
- en: Goran Muric InferLink Corporation Los Angeles, California gmuric@inferlink.com
                           Ben Delay InferLink Corporation Los Angeles, California
    bdelay@inferlink.com                        Steven Minton InferLink Corporation
    Los Angeles, California sminton@inferlink.com
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**戈兰·穆里奇** InferLink 公司 洛杉矶，加利福尼亚州 gmuric@inferlink.com                       
    **本·德雷** InferLink 公司 洛杉矶，加利福尼亚州 bdelay@inferlink.com                        **史蒂文·敏顿**
    InferLink 公司 洛杉矶，加利福尼亚州 sminton@inferlink.com'
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: There are numerous prompting strategies to achieve good performance using generative
    Large Language Models (LLMs). Take, for instance, a binary classification problem,
    where a system should classify the given text into one of two classes. A typical
    zero-shot approach is to prompt the model with a given text and carefully designed
    question, that will yield an appropriate answer. There are also multiple variations
    on that approach that include “chain-of-thought” prompting  Wei et al. ([2022c](#bib.bib35));
    Wang et al. ([2022a](#bib.bib30)); Kojima et al. ([2022](#bib.bib14)), “few-shot
    learning”  Schick and Schütze ([2022](#bib.bib25)); Gu et al. ([2021](#bib.bib12)),
    “self-instruct” Wang et al. ([2022b](#bib.bib31)); Yang et al. ([2024](#bib.bib39))
    prompting and “iterative refinement” Wu et al. ([2022a](#bib.bib36)); Trautmann
    ([2023](#bib.bib29)). These tactics are used to get a better sense of the model’s
    underlying reasoning or to surpass the performance achieved by the standard zero-shot
    method.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多提示策略可以利用生成式大型语言模型（LLMs）实现良好的性能。例如，在一个二分类问题中，系统应将给定的文本分类为两个类别之一。一个典型的零样本方法是用给定的文本和精心设计的问题来提示模型，从而得到合适的答案。还有多种变体，包括“思维链”提示 Wei等人（[2022c](#bib.bib35)）；Wang等人（[2022a](#bib.bib30)）；Kojima等人（[2022](#bib.bib14)）、“少样本学习” Schick和Schütze（[2022](#bib.bib25)）；Gu等人（[2021](#bib.bib12)）、“自我指导” Wang等人（[2022b](#bib.bib31)）；Yang等人（[2024](#bib.bib39)）提示和“迭代优化” Wu等人（[2022a](#bib.bib36)）；Trautmann（[2023](#bib.bib29)）。这些策略用于更好地理解模型的潜在推理，或超越标准零样本方法所取得的性能。
- en: These options are usually used in cases where using highly specialized fine-tuned
    LLMs is not a viable option because it is often of utmost importance to understand
    how decisions are made. This is especially true in fields like medicine, where
    decisions based on opaque, “black-box” models are usually not acceptable. Although
    zero-shot or few-shot prompting methods can potentially offer explanations for
    their reasoning, these explanations are often unstructured and lack quantifiability.
    On the other hand, while finely tuned models may achieve superior performance,
    they frequently struggle to articulate the rationale behind their outputs unless
    explicitly trained for this purpose, a process that is labor-intensive. Additionally,
    outputs from such models may also suffer from the lack of structured reasoning
    representation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项通常用于当使用高度专业化的微调LLMs不可行时，因为理解决策如何做出往往至关重要。这在医学等领域尤为真实，因为基于不透明的“黑箱”模型做出的决策通常是不可接受的。尽管零样本或少样本提示方法可能提供推理解释，这些解释通常是不结构化的且缺乏量化性。另一方面，虽然微调模型可能取得更好的性能，但它们通常难以阐明其输出背后的理由，除非专门针对此目的进行训练，这一过程是劳动密集型的。此外，这些模型的输出可能也会因缺乏结构化的推理表示而受到影响。
- en: 'In cases where using “black-box” models is not practical, and where interpretability
    is important, users have the option to develop a structured reasoning process
    by asking several questions to achieve a desired output. There are three main
    problems that arise with this approach: 1) Non-experts have little chance to develop
    a good set of questions and rules that ensure optimal model performance; 2) Designing
    an accurate rule set becomes challenging since individual instances may not perfectly
    align with all desired criteria, resulting in a mix of positive and negative responses
    to different rules; and 3) The potential combinations of these rules can become
    overwhelmingly numerous, making it impractical to hard-code every possible scenario.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用“黑箱”模型不切实际且可解释性重要的情况下，用户可以选择通过提出多个问题来开发结构化的推理过程，以获得所需的输出。这种方法存在三个主要问题：1)
    非专家很难开发出一套良好的问题和规则，以确保模型的最佳性能；2) 设计准确的规则集变得具有挑战性，因为个别实例可能与所有期望标准不完全一致，导致对不同规则的响应混合；3)
    这些规则的潜在组合可能会变得极其庞大，使得为每种可能的情况硬编码变得不切实际。
- en: In the paper, we propose a method that attempts to overcome the three issues
    outlined above. We refer to the method as the Interpretable Cross-Examination
    Technique, or ICE-T for brevity. Our approach exhibits strong performance, consistently
    surpasses the benchmark set by a zero-shot baseline, and also offers a high level
    of interpretability. The core concept here is that rather than using a single
    prompt to get a response from an LLM and making a decision based on that single
    output, we engage the LLM with multiple prompts, covering various questions. We
    then combine the responses from all these prompts and use the outputs to make
    a decision. Compared to other methods that are based on multi-prompting, our approach
    is fundamentally different in the way the decisions are made. Specifically, we
    take the responses from the LLM, convert them into numerical values to create
    a feature vector, and then input this vector into a traditional classifier to
    determine the final outcome. Since, in this process we create a low-dimensional
    feature vector with highly informative features, we can then use relatively small
    classifiers to make a decision.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种方法，旨在克服上述提到的三个问题。我们将这种方法称为可解释的交叉检查技术，简称ICE-T。我们的方法表现出强劲的性能，一致超越了零样本基准，并且提供了较高的可解释性。核心概念是，与其使用单一提示从LLM获得响应并基于单一输出做出决定，我们通过多个提示与LLM进行交互，涵盖各种问题。然后，我们将所有这些提示的响应结合起来，并使用这些输出做出决定。与基于多提示的方法相比，我们的方法在决策方式上有根本性的不同。具体而言，我们将LLM的响应转换为数值，创建一个特征向量，然后将该向量输入传统分类器以确定最终结果。由于在此过程中我们创建了一个具有高度信息性的低维特征向量，因此我们可以使用相对较小的分类器来做出决定。
- en: 'We established an experimental setup where we tested our Interpretable Cross-Examination
    Technique on a simple binary classification task. We tested our approach on a
    set of multiple datasets split on 17 different tasks and we show that:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立了一个实验设置，在一个简单的二元分类任务上测试了我们的可解释交叉检查技术。我们在17个不同任务拆分的多个数据集上测试了我们的方法，并且展示了：
- en: '1.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: ICE-T consistently outperforms the zero-shot baseline model in most classification
    metrics
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ICE-T在大多数分类指标中始终优于零样本基准模型。
- en: '2.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Using a smaller model with ICE-T we can achieve comparable or better results
    than using larger and essentially more capable model with zero-shot approach
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用较小的模型进行ICE-T，我们可以实现与使用更大且本质上更强大的零样本方法相当或更好的结果。
- en: Furthermore, this approach can be highly interpretable, allowing experts to
    clearly understand the rationale behind the decision-making process¹¹1Degree of
    interpretability may vary depending on the machine learning method selected for
    the final classification task. The decision on which method to employ should be
    guided by a consideration of the trade-offs between interpretability and performance
    tailored to the unique demands of each task. Additionally, tools commonly used
    for tabular machine learning can be employed to enhance the understanding of the
    data. While this technique is specifically evaluated for binary classification
    within this paper, its applicability potentially extends across a broad spectrum
    of scenarios.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种方法具有很高的可解释性，使专家能够清楚地理解决策过程背后的理由¹¹解释性程度可能因用于最终分类任务的机器学习方法而异。选择使用哪种方法应根据可解释性和性能之间的权衡，量体裁衣地考虑每个任务的独特需求。此外，常用于表格机器学习的工具可以增强对数据的理解。虽然本文中这种技术专门针对二元分类进行评估，但其适用性可能扩展到广泛的场景。
- en: 1.1 Motivation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 动机
- en: The ICE-T method was initially conceived at InferLink in a commercial consulting
    project, where we needed to address a complex challenge in biomedical text classification.
    The project’s goals were to develop a model that could perform at a level comparable
    to human experts, provide interpretable results, and allow for detection of potentially
    mislabelled data. Initially, conventional “black-box” models such like fine-tuned
    BERT-based ones underperformed, as well as zero-shot or few-shot learning methods
    using LLMs. This led to the creation of the ICE-T, which improved the performance
    of classification, while gaining interpretability and allowing for the correction
    of labeling errors. ICE-T was used initially for the purpose of classifying biomedical
    data for a specific commercial purpose. While the specifics of this initial task
    and data remain confidential, we have conducted further testing on additional
    publicly available datasets and decided to make the method publicly accessible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ICE-T 方法最初是在 InferLink 的一个商业咨询项目中构思的，当时我们需要解决生物医学文本分类中的复杂挑战。项目目标是开发一个能够与人类专家相媲美的模型，提供可解释的结果，并允许检测潜在的标注错误。最初，传统的“黑箱”模型，如微调的
    BERT 基础模型表现不佳，零样本或少样本学习方法也效果不佳。这导致了 ICE-T 的创建，它提高了分类性能，同时获得了可解释性并允许纠正标注错误。ICE-T
    最初用于特定商业目的的生物医学数据分类。虽然这个初步任务和数据的具体细节仍然保密，但我们已在额外的公开数据集上进行了进一步测试，并决定将该方法公开。
- en: 2 Related Work
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Our proposed solution addresses three core aspects of using large language
    models for inference: prompting, in-context learning, and interpretability. It
    is built on top of the ever-growing body of knowledge that comes from those domains.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的解决方案涵盖了使用大语言模型进行推理的三个核心方面：提示、上下文学习和可解释性。它建立在这些领域不断增长的知识基础之上。
- en: 2.1 Prompting techniques
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 提示技术
- en: Numerous techniques have been developed to improve the fundamental zero-shot
    approach. Among these, the “chain-of-thought” (CoT) prompting is particularly
    notable. This method is used to prompt the model to systematically articulate
    its reasoning process in a step-by-step manner before reaching a conclusion. Research
    has shown that chain-of-thought prompting improves performance on a range of arithmetic,
    commonsense, and symbolic reasoning tasks Wei et al. ([2022b](#bib.bib34), [c](#bib.bib35));
    Wang et al. ([2022a](#bib.bib30)). Even simple tweaks such as adding “Let’s think
    step by step” before each answer can significantly outperform zero-shot LLM performances
    on diverse benchmark reasoning tasks Kojima et al. ([2022](#bib.bib14)); Nye et al.
    ([2021](#bib.bib20)). Such generated chains that prompt language models to break
    down their reasoning into steps often cause errors in inference time. To reduce
    these errors, some researchers employ a method known as automatic Chain of Thought
    prompting. This technique, which generates demonstrable examples, has proven to
    be more effective than earlier, simpler CoT approaches Zhang et al. ([2022b](#bib.bib43)).
    Lastly, “iterative refinement” involves repeatedly prompting the model with slightly
    altered versions of the original text or question, honing in on a more accurate
    or nuanced answer through successive iterations. Each of these strategies can
    be tailored to the specific needs of a task, leveraging the model’s capabilities
    in different ways to achieve optimal performance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多技术已被开发以改进基本的零样本方法。在这些技术中，“思维链” (CoT) 提示特别值得注意。这种方法用于提示模型系统地逐步表达其推理过程，然后得出结论。研究表明，思维链提示在一系列算术、常识和符号推理任务中提高了性能
    Wei et al. ([2022b](#bib.bib34), [c](#bib.bib35)); Wang et al. ([2022a](#bib.bib30))。即使是简单的调整，如在每个答案之前添加“让我们一步步思考”，也能显著优于零样本
    LLM 在各种基准推理任务中的表现 Kojima et al. ([2022](#bib.bib14)); Nye et al. ([2021](#bib.bib20))。这种生成的链提示语言模型将其推理过程分解成步骤，通常会导致推理时出现错误。为了减少这些错误，一些研究人员采用了自动思维链提示的方法。这种生成示例的技术已被证明比早期的简单
    CoT 方法更有效 Zhang et al. ([2022b](#bib.bib43))。最后，“迭代精炼”涉及通过逐步提示模型稍微修改的原始文本或问题，通过连续迭代来找到更准确或更细致的答案。每种策略都可以根据任务的具体需求量身定制，利用模型的能力以不同方式实现最佳性能。
- en: Several approaches involve using multiple prompts in a chain, where the output
    of one step becomes the input for the next, thus aggregating the gains per step Wu
    et al. ([2022a](#bib.bib36)), or decomposing complex tasks into smaller, manageable
    components Trautmann ([2023](#bib.bib29)). Additionally, “self-instruct” Wang
    et al. ([2022b](#bib.bib31)); Yang et al. ([2024](#bib.bib39)) prompting can be
    used, where the model generates its own instructions or clarifications based on
    the initial prompt, attempting to refine or better understand the task before
    generating a response. Another set of approaches uses multiple models or multiple
    instances of the same model to improve the performance. The additionally trained
    models, called “verifiers” are used to judge the correctness of model completions.
    At the inference time, the verifiers would select the most likely answer Cobbe
    et al. ([2021](#bib.bib8)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法涉及使用多个提示链，其中一个步骤的输出成为下一个步骤的输入，从而累积每一步的收益 Wu 等 ([2022a](#bib.bib36))，或将复杂任务分解为更小、更易管理的组件
    Trautmann ([2023](#bib.bib29))。此外，可以使用“自我指令” Wang 等 ([2022b](#bib.bib31))；Yang
    等 ([2024](#bib.bib39)) 提示，在这种方法中，模型根据初始提示生成自己的指令或澄清，试图在生成响应之前细化或更好地理解任务。另一类方法使用多个模型或同一模型的多个实例以提高性能。附加训练的模型被称为“验证器”，用于判断模型输出的正确性。在推理时，验证器会选择最可能的答案
    Cobbe 等 ([2021](#bib.bib8))。
- en: 2.2 In-context learning
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 上下文学习
- en: Large Language Models possess the remarkable ability for in-context learning
    (ICL), in which they acquire knowledge from a few contextual examples either during
    inference or during training. Numerous studies have shown that through ICL, LLMs
    can effectively handle a diverse set of complex tasks Wei et al. ([2022a](#bib.bib33)).
    ICL offers several advantages, notably its ease in including human knowledge into
    LLMs by using various demonstrations and templates Liu et al. ([2021](#bib.bib16));
    Wu et al. ([2022b](#bib.bib37)). Furthermore, unlike traditional supervised training
    methods, ICL operates without the need for additional training, significantly
    lowering the computational costs when using models to solve new tasks Dong et al.
    ([2022](#bib.bib10)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型具备显著的上下文学习（ICL）能力，通过少量上下文示例在推理或训练过程中获取知识。大量研究表明，通过ICL，LLM能够有效处理各种复杂任务
    Wei 等 ([2022a](#bib.bib33))。ICL具有多个优点，尤其是通过使用各种演示和模板，将人类知识轻松地融入LLM中 Liu 等 ([2021](#bib.bib16))；Wu
    等 ([2022b](#bib.bib37))。此外，与传统的监督训练方法不同，ICL不需要额外的训练，从而显著降低了使用模型解决新任务时的计算成本 Dong
    等 ([2022](#bib.bib10))。
- en: One of the most recognizable techniques for in-context learning is “few-shot
    learning” Schick and Schütze ([2022](#bib.bib25), [2020](#bib.bib24)); Gu et al.
    ([2021](#bib.bib12)); Perez et al. ([2021](#bib.bib21)) during inference²²2A different
    approach to achieving few-shot learning can occur also during the training phase
    or during fine-tuning.. Using this approach, the model is provided with a few
    examples of text and their corresponding labels or desired outputs within the
    prompt itself. This method teaches the model the context of the decision-making
    process, improving its accuracy on similar tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文学习中，最具代表性的技术之一是“少量样本学习” Schick 和 Schütze ([2022](#bib.bib25), [2020](#bib.bib24))；Gu
    等 ([2021](#bib.bib12))；Perez 等 ([2021](#bib.bib21)) 在推理过程中²²2 在训练阶段或微调过程中也可以采用不同的方法来实现少量样本学习。采用这种方法时，模型会在提示中提供一些文本示例及其对应的标签或期望输出。这种方法教会模型决策过程的上下文，从而提高其在类似任务上的准确性。
- en: Multiple other studies contributed to refining the ICL methods, focusing on
    automation, ordering, and selection of prompts. Zhou et al. (2022) introduced
    the Automatic Prompt Engineer (APE), which automates the generation of instructional
    prompts, significantly reducing manual effort and improving scalability Zhou et al.
    ([2022](#bib.bib45)). Simultaneously, Lu et al. (2021) came up with the method
    to optimize the ordering of prompts. They employed entropy statistics to evaluate
    and identify the most effective prompt sequences Lu et al. ([2021](#bib.bib17)).
    Rubin et al. (2021) and Liu et al. (2021) both contribute to this area but from
    different perspectives. Rubin et al. (2021) developed a method for efficiently
    retrieving prompts using annotated data, streamlining the selection process Rubin
    et al. ([2021](#bib.bib23)). On the other hand, Liu et al. (2021) explored strategic
    selection methods that go beyond random sampling to leverage the few-shot capabilities
    of LLMs, aiming to enhance the model’s performance through example selection Liu
    et al. ([2021](#bib.bib16)). Adding to the discussion on selection strategies,
    Zhang et al. (2022) approached example selection as a sequential decision problem.
    They proposed using a reinforcement learning algorithm to discover policies that
    improve the generalizability of language models Zhang et al. ([2022a](#bib.bib42)).
    This perspective introduces a dynamic element to the selection process, aligning
    with the strategies discussed by Rubin and Liu but through an adaptive, policy-driven
    approach.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究致力于完善ICL方法，集中在自动化、排序和提示选择上。**Zhou et al.** (2022) 提出了自动提示工程师（APE），它自动生成指导提示，显著减少了人工工作量并提高了可扩展性**Zhou
    et al.** ([2022](#bib.bib45))。与此同时，**Lu et al.** (2021) 提出了优化提示排序的方法。他们使用熵统计来评估和识别最有效的提示序列**Lu
    et al.** ([2021](#bib.bib17))。**Rubin et al.** (2021) 和 **Liu et al.** (2021)
    从不同角度对此领域进行了贡献。**Rubin et al.** (2021) 开发了一种使用注释数据高效检索提示的方法，简化了选择过程**Rubin et
    al.** ([2021](#bib.bib23))。另一方面，**Liu et al.** (2021) 探索了超越随机采样的战略选择方法，利用LLMs的少量样本能力，通过示例选择提升模型性能**Liu
    et al.** ([2021](#bib.bib16))。在选择策略的讨论中，**Zhang et al.** (2022) 将示例选择视为一个序列决策问题。他们提出使用强化学习算法来发现改进语言模型泛化能力的策略**Zhang
    et al.** ([2022a](#bib.bib42))。这一观点为选择过程引入了动态元素，与**Rubin**和**Liu**讨论的策略相一致，但通过一种自适应的、政策驱动的方法。
- en: 2.3 Model interpretability
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 模型可解释性
- en: The challenge of interpreting complex decision processes made by LLMs has hindered
    their application in critical areas like medicine, where there are significant
    concerns about regulation Goodman and Flaxman ([2017](#bib.bib11)) and safety Amodei
    et al. ([2016](#bib.bib2)). Furthermore, this difficulty in understanding the
    workings of large language models (LLMs) and similar neural network models has
    restricted their use in domains like science and data analysis Kasneci et al.
    ([2023](#bib.bib13)). In such fields, the primary objective is often to derive
    a reliable interpretation rather than merely to implement an LLM Singh et al.
    ([2024](#bib.bib26)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 解释复杂决策过程的挑战使得大语言模型（LLMs）在医学等关键领域的应用受到限制，因为在这些领域对规章制度**Goodman 和 Flaxman** ([2017](#bib.bib11))和安全性**Amodei
    et al.** ([2016](#bib.bib2))存在显著关注。此外，理解大型语言模型（LLMs）及类似神经网络模型的运作的困难限制了它们在科学和数据分析等领域的使用**Kasneci
    et al.** ([2023](#bib.bib13))。在这些领域，主要目标通常是获得可靠的解释，而不仅仅是实现一个LLM**Singh et al.**
    ([2024](#bib.bib26))。
- en: The expression of uncertainty in language models is crucial for reliable LLM
    utilization, yet it remains a challenging area due to inherent overconfidence
    in model responses. Xiong et al. (2023) and Zhou et al. (2024) both highlight
    the overconfidence issue in LLMs. Xiong et al. question whether LLMs can express
    their uncertainty, observing a tendency in LLMs to mimic human patterns of expressing
    confidence Xiong et al. ([2023](#bib.bib38)). Simlarly, Zhou et al. note that
    while LLMs can be prompted to express confidence levels, they remain generally
    overconfident and unable to convey uncertainties effectively, also when providing
    incorrect responses Zhou et al. ([2024](#bib.bib44)). Ye et al. (2022) add that
    even when LLMs generate explanations, these may not accurately reflect the model’s
    predictions nor be factually grounded in the input, particularly in tasks requiring
    extractive explanations Ye and Durrett ([2022](#bib.bib40)). However, all the
    research mentioned above note that these flawed explanations can still serve a
    purpose, offering a means to verify LLM predictions post-hoc.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型中的不确定性表达对于可靠的 LLM 使用至关重要，但由于模型响应中固有的过度自信，这仍然是一个具有挑战性的领域。Xiong 等人（2023）和
    Zhou 等人（2024）都突出了 LLM 中的过度自信问题。Xiong 等人质疑 LLM 是否能表达其不确定性，观察到 LLM 倾向于模仿人类表达自信的模式
    Xiong 等人 ([2023](#bib.bib38))。同样，Zhou 等人指出，尽管可以促使 LLM 表达自信水平，但它们通常仍过度自信，无法有效地传达不确定性，尤其是在提供错误答案时
    Zhou 等人 ([2024](#bib.bib44))。Ye 等人（2022）补充说，即使 LLM 生成解释，这些解释也可能不准确地反映模型的预测，也可能没有在输入中有事实依据，特别是在需要抽取式解释的任务中
    Ye 和 Durrett ([2022](#bib.bib40))。然而，上述所有研究都指出，这些有缺陷的解释仍然可以发挥作用，为事后验证 LLM 预测提供了一种手段。
- en: It is worth mentioning feature attribution methods, used beyond the LLM realm
    in multiple deep-learning applications. Feature attributions in machine learning
    provide a relevance score to each input feature, reflecting its impact on the
    model’s output. This methodology helps in understanding how and why certain decisions
    or predictions are made by a model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是特征归因方法，这些方法在多个深度学习应用中超越了大型语言模型（LLM）领域。机器学习中的特征归因为每个输入特征提供了一个相关性分数，反映了其对模型输出的影响。这种方法有助于理解模型是如何以及为何做出某些决策或预测的。
- en: The approaches developed by Lundberg et al. (2017) and Sundararajan et al. (2017)
    both delve into this topic but offer distinct methodologies and theoretical foundations.
    Lundberg et al. Lundberg and Lee ([2017](#bib.bib18)) introduced SHAP (SHapley
    Additive exPlanations), which provides a unified framework for interpreting predictions.
    SHAP assigns an importance value to each feature for a specific prediction, leveraging
    the concept of Shapley values from cooperative game theory. In contrast, Sundararajan
    et al. Sundararajan et al. ([2017](#bib.bib28)) developed Integrated Gradients,
    another method focusing on the attribution of predictions to input features of
    deep networks. Unlike SHAP, which uses Shapley values, Integrated Gradients relies
    on the integration of gradients along the path from a chosen baseline to the actual
    input. Complementing these approaches, Ribeiro et al. (2016) proposed LIME (Local
    Interpretable Model-agnostic Explanations), which aims to make the predictions
    of any classifier understandable and reliable by learning an interpretable model
    localized around the prediction Ribeiro et al. ([2016](#bib.bib22)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Lundberg 等人（2017）和 Sundararajan 等人（2017）提出的方法都深入探讨了这一主题，但提供了不同的方法论和理论基础。Lundberg
    等人 Lundberg 和 Lee ([2017](#bib.bib18)) 介绍了 SHAP（SHapley Additive exPlanations），它提供了一个统一的预测解释框架。SHAP
    为特定预测的每个特征分配一个重要性值，利用了合作博弈论中的 Shapley 值概念。相比之下，Sundararajan 等人 Sundararajan 等人
    ([2017](#bib.bib28)) 开发了集成梯度，这是一种将预测归因于深度网络输入特征的另一种方法。与使用 Shapley 值的 SHAP 不同，集成梯度依赖于沿从选择的基线到实际输入的路径的梯度积分。作为对这些方法的补充，Ribeiro
    等人（2016）提出了 LIME（Local Interpretable Model-agnostic Explanations），旨在通过学习一个围绕预测的可解释模型来使任何分类器的预测变得可理解和可靠
    Ribeiro 等人 ([2016](#bib.bib22))。
- en: Another popular method for understanding neural-network representations is probing.
    Conneau et al. (2018) initially introduced multiple probing tasks designed to
    capture simple linguistic features of sentences, setting a foundation for understanding
    how neural networks encode linguistic properties Conneau et al. ([2018](#bib.bib9)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的理解神经网络表示的方法是探测。Conneau 等人（2018）最初提出了多个探测任务，旨在捕捉句子的简单语言特征，为理解神经网络如何编码语言特性奠定了基础
    Conneau 等人 ([2018](#bib.bib9))。
- en: Clark et al. (2019) focused primarily on the behavior of attention heads within
    transformers. They observed that these heads often broadly attend across entire
    sentences, and that attention patterns in the same layer tend to exhibit similar
    behaviors. Crucially, their research links specific attention heads to traditional
    linguistic concepts like syntax and coreference, suggesting a direct relationship
    between the model’s attention mechanisms and linguistic structures Clark et al.
    ([2019](#bib.bib7)), although there is an ongoing debate on the explanatory power
    of attention in neural network Bibal et al. ([2022](#bib.bib4)). Unlike Clark
    et al., who examine what the model attends to, Morris et al. Morris et al. ([2023](#bib.bib19))
    explore how information is preserved and can be retrieved from embeddings, offering
    insights into the reversibility and fidelity of the encoding process. Their method
    involves a multi-step process that iteratively corrects and re-embeds text, demonstrating
    the ability to recover most of the original text inputs exactly. Belrose et al.
    (2023) introduced a technique called causal basis extraction, which aims to identify
    influential features within neural networks Belrose et al. ([2023](#bib.bib3)).
    This method stands out by focusing on the causality within network decisions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Clark 等人（2019）主要关注变换器中的注意力头的行为。他们观察到这些注意力头通常广泛关注整个句子，并且同一层中的注意力模式往往表现出相似的行为。重要的是，他们的研究将特定的注意力头与传统语言学概念如句法和共指联系起来，暗示了模型的注意力机制与语言结构之间的直接关系 Clark
    等人（[2019](#bib.bib7)），尽管关于注意力在神经网络中的解释能力仍在争论中 Bibal 等人（[2022](#bib.bib4)）。与 Clark
    等人研究模型关注内容不同，Morris 等人（[2023](#bib.bib19)）探讨了信息如何在嵌入中被保留和检索，提供了对编码过程可逆性和保真度的见解。他们的方法包括一个多步骤过程，该过程迭代地修正和重新嵌入文本，展示了能够准确恢复大部分原始文本输入的能力。Belrose
    等人（2023）引入了一种称为因果基础提取的技术，旨在识别神经网络中的有影响力的特征 Belrose 等人（[2023](#bib.bib3)）。该方法通过关注网络决策中的因果关系而脱颖而出。
- en: In summary, while chain-of-thought prompting can generate errors during inference,
    requiring complex corrective approaches, in-context learning techniques also face
    challenges in prompt optimization and efficient retrieval. Furthermore, interpreting
    large language models remains problematic, exacerbated by models’ tendency to
    exhibit overconfidence and provide unreliable or unverifiable explanations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，尽管链式思维提示在推理过程中可能会产生错误，需要复杂的纠正方法，但上下文学习技术在提示优化和高效检索方面也面临挑战。此外，对大语言模型的解释仍然存在问题，加剧了模型倾向于表现过于自信并提供不可靠或不可验证解释的问题。
- en: 3 Method
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'Training the ICE-T system consists of the following steps:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ICE-T 系统的训练包括以下步骤：
- en: '1.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Generating questions: the process begins by generating a series of questions
    designed to prompt the Large Language Model (LLM);'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成问题：该过程开始于生成一系列旨在提示大型语言模型（LLM）的问题；
- en: '2.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Prompting the LLM: Previously generated questions are used to prompt the LLM
    and collect the yes/no answers;'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示 LLM：使用之前生成的问题来提示 LLM 并收集“是/否”答案；
- en: '3.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Verbalizing the answers: for each instance within the training dataset, responses
    to prompts are collected and converted into numerical form, thus creating a low-dimensional
    feature vector for each instance;'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表达答案：对于训练数据集中的每一个实例，收集对提示的响应并转换成数值形式，从而为每个实例创建一个低维特征向量；
- en: '4.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Training a classifier: Previously obtained vectors, together with their respective
    labels, are then used to train a classifier'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练分类器：利用之前获得的向量及其相应的标签来训练分类器
- en: 'The Inference stage mirrors the training process: the LLM is presented with
    the same collection of questions. The responses obtained are numerically encoded
    in the same manner before being processed by the classifier that was trained during
    the Training stage. Training and inference process is illustrated in Figure [1](#S3.F1
    "Figure 1 ‣ 3.1 Generating questions ‣ 3 Method ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").
    Each step is explained below.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 推理阶段与训练过程相似：LLM 被提供相同的问题集合。获得的响应以相同的方式进行数值编码，然后由在训练阶段训练的分类器进行处理。训练和推理过程如图 [1](#S3.F1
    "图 1 ‣ 3.1 生成问题 ‣ 3 方法 ‣ 可解释的交叉检查技术 (ICE-T)：利用高度信息化的特征提升 LLM 性能") 所示。每个步骤在下面解释。
- en: 3.1 Generating questions
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 生成问题
- en: To train and use the system, we need to create multiple questions that more
    closely reflect the core principles behind the initial yes/no question. Those
    questions should be crafted in a way to uncover some additional details about
    the problem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练和使用系统，我们需要创建多个问题，这些问题更贴近于最初的是/否问题背后的核心原则。这些问题应该被设计成揭示一些关于问题的额外细节。
- en: 'Consider a use case where an expert is building a classifier to determine eligibility
    for medical trials based on patient data. In such a scenario, the classifier needs
    to assess various clinical inclusion criteria, which are typically derived from
    patient medical records. One of these criteria could be the patient’s language
    proficiency, for instance, whether they speak English. A naive formulation of
    this question may be to present the question to the LLM in a prompt like the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个用例，其中专家正在构建一个分类器以根据患者数据确定是否符合医疗试验资格。在这种情况下，分类器需要评估各种临床纳入标准，这些标准通常来源于患者的医疗记录。其中一个标准可能是患者的语言能力，例如，他们是否说英语。对这个问题的简单表述可能是将问题呈现给LLM，像下面这样的提示：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where the `__RECORDS__` represents the appended textual medical records. Determining
    the answer to that question, which we call the “primary” question, may not be
    easy given the medical records under consideration, requiring an understanding
    of somewhat subtle indicators that show if a patient actually speaks English.
    It is highly unlikely that the medical records will directly state the answer
    to that question.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `__RECORDS__` 代表附加的文本医疗记录。确定这个问题的答案，我们称之为“主要”问题，可能并不容易，因为需要理解一些微妙的指标来显示患者是否实际上会说英语。医疗记录直接说明这个问题的答案的可能性很小。
- en: 'However, a series of “secondary” questions such as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一系列“次要”问题，如下：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: may allow the model to answer directly based on the information already contained
    in the documents presented to it, while also serving as strong indicators for
    the primary question. Secondary questions are also yes/no questions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能使模型能够直接基于已提供的文档中的信息回答问题，同时也作为主要问题的强指标。次要问题也是是/否问题。
- en: 'Creating the secondary questions can be done in multiple ways, such as writing
    the questions manually using the expert knowledge or using the LLM to automatically
    generate a fixed size set of questions that might be useful in answering the original
    question. Starting from the primary question $q_{0}$ we generate $n$ additional
    questions, creating a set of all questions $Q=\{q_{0},q_{1}\ldots q_{n}\}$, where
    $|Q|=n+1$. This process is shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Generating
    questions ‣ 3 Method ‣ Interpretable Cross-Examination Technique (ICE-T): Using
    highly informative features to boost LLM performance") with a red box, illustrating
    the creation of the questions and using them during the training and inference
    process. The same set $Q$ of questions is used for both training and inference.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '创建次要问题可以通过多种方式完成，例如使用专家知识手动编写问题或使用LLM自动生成一组固定大小的可能对回答原始问题有用的问题。从主要问题 $q_{0}$
    开始，我们生成 $n$ 个额外的问题，形成所有问题的集合 $Q=\{q_{0},q_{1}\ldots q_{n}\}$，其中 $|Q|=n+1$。这个过程在图 [1](#S3.F1
    "图 1 ‣ 3.1 生成问题 ‣ 3 方法 ‣ 可解释交叉检查技术 (ICE-T): 使用高度信息化特征提升LLM表现") 的红色框中展示，说明了问题的创建及其在训练和推理过程中的使用。相同的问题集合
    $Q$ 用于训练和推理。'
- en: 'The number $n$ of secondary questions is decided based on factors such as:
    number of training samples, availability of the expert knowledge and the level
    of interpretability needed for a specific task. Our prior small-scale experiments
    have shown that secondary questions crafted by experts generally lead to improved
    performance compared to those generated by LLMs. However, in the experiments reported
    here, we chose a straightforward and reproducible approach where we exclusively
    use secondary questions created by an LLM. This choice was made to minimize human
    bias and showcase the method’s effectiveness in scenarios where expert input is
    unavailable. The exact prompts used for creating secondary questions in our experiments
    are described in Section [5](#S5 "5 Experiments ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '次要问题的数量$n$基于以下因素决定：训练样本的数量、专家知识的可用性以及特定任务所需的可解释性水平。我们之前的小规模实验表明，由专家设计的次要问题通常比由LLM生成的问题表现更好。然而，在这里报告的实验中，我们选择了一种直接且可重复的方法，仅使用LLM生成的次要问题。这一选择旨在减少人为偏差，并展示在专家输入不可用的情况下方法的有效性。我们实验中创建次要问题的具体提示在第[5](#S5
    "5 Experiments ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly
    informative features to boost LLM performance")节中描述。'
- en: '![Refer to caption](img/c19e8f5e2b929b4c2c52b0add334fc1e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c19e8f5e2b929b4c2c52b0add334fc1e.png)'
- en: 'Figure 1: Illustration of training and inference process in ICE-T. In the training
    phase, the process begins by generating questions to prompt an LLM, which then
    provides yes/no answers. These answers are verbalized and converted into numerical
    feature vectors. A classifier is trained using these vectors along with their
    respective labels. During inference, the LLM is prompted with the same questions,
    and the answers are similarly processed to predict outcomes using the trained
    classifier.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：ICE-T中训练和推理过程的示意图。在训练阶段，过程开始于生成问题以提示LLM，然后LLM提供是/否答案。这些答案被口头表达并转换为数值特征向量。使用这些向量及其相应的标签训练分类器。在推理阶段，LLM使用相同的问题进行提示，答案同样被处理以利用训练好的分类器预测结果。
- en: 3.2 Prompting LLM
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 提示LLM
- en: 'The LLMs are prompted in two occasions. First, they are prompted to obtain
    the set of secondary questions $Q$, as described in Section [3.1](#S3.SS1 "3.1
    Generating questions ‣ 3 Method ‣ Interpretable Cross-Examination Technique (ICE-T):
    Using highly informative features to boost LLM performance"). Second, for each
    document, we prompt the LLM with the document and corresponding secondary questions.
    Then, for each question $q_{i}$ the output $a_{i}$ of the LLM is collected, creating
    a set of outputs for each document. The textual outputs are then assigned a numerical
    value and transformed into a feature vector $v_{i}$, through the verbalization
    process explained in Section [3.3](#S3.SS3 "3.3 Verbalizing the answers ‣ 3 Method
    ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM在两个场合下进行提示。首先，提示以获取次要问题集$Q$，如第[3.1](#S3.SS1 "3.1 Generating questions ‣
    3 Method ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance")节所述。其次，对于每个文档，我们用文档及相应的次要问题提示LLM。然后，收集LLM对每个问题$q_{i}$的输出$a_{i}$，为每个文档创建一组输出。文本输出随后被分配一个数值，并通过第[3.3](#S3.SS3
    "3.3 Verbalizing the answers ‣ 3 Method ‣ Interpretable Cross-Examination Technique
    (ICE-T): Using highly informative features to boost LLM performance")节中解释的口头化过程转换为特征向量$v_{i}$。'
- en: 3.3 Verbalizing the answers
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 口头化答案
- en: 'The output of the LLM in response to each prompt is limited to one of three
    possible values: Yes, No, or Unknown, depending on the answer to the question
    posed in the prompt. These responses are subsequently assigned numerical values
    for analysis, with “Yes” translating to 1, “No” to 0, and “Unknown” to 0.5.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: LLM对每个提示的响应限制为三种可能的值之一：Yes、No或Unknown，这取决于提示中提出的问题的答案。这些响应随后被分配数值进行分析，其中“Yes”转换为1，“No”转换为0，“Unknown”转换为0.5。
- en: 3.4 Training a classifier
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 训练分类器
- en: To train a classifier, we use a set $V$ of low-dimensional numerical vectors,
    where $|V|=n+1$ and corresponding labels $X$, where each vector $v_{i}$ has a
    corresponding binary label $x_{i}$. Vectors $V$ are obtained from the training
    textual data after prompting LLM to generate $n+1$ outputs that are then assigned
    a numerical value. A classifier is then trained using a 5-fold cross-validation
    process and grid search for the best parameters. A choice of a specific classification
    algorithm will depend on the size of training data, values distribution and desired
    performance on a specific classification metric.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练分类器，我们使用一组低维数值向量集 $V$，其中 $|V|=n+1$，以及相应的标签 $X$，每个向量 $v_{i}$ 都有一个对应的二进制标签
    $x_{i}$。向量 $V$ 是从训练文本数据中获得的，方法是让大型语言模型生成 $n+1$ 个输出，然后为其分配数值。接着，使用 5 折交叉验证过程和网格搜索来训练分类器，以找到最佳参数。具体的分类算法选择将取决于训练数据的大小、值的分布以及在特定分类指标上的期望表现。
- en: 4 Data
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据
- en: This work utilizes data compiled from a range of sources, attempting to include
    a variety of domains and document lengths. The data used in the experiments described
    here spans the fields of medicine, law, climate science, and politics. It also
    includes documents of varying sizes, from brief tweets to extensive legal documents
    and detailed medical records.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究利用了从多个来源编纂的数据，试图涵盖各种领域和文档长度。这里描述的实验所用的数据涵盖了医学、法律、气候科学和政治等领域。数据还包括各种大小的文档，从简短的推文到详细的法律文件和医疗记录。
- en: 4.1 Clinical trials
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 临床试验
- en: 'This dataset comes from Track 1 of the 2018 National NLP Clinical Challenges
    (n2c2) shared tasks³³3https://n2c2.dbmi.hms.harvard.edu/. It is designed to help
    in identifying patients within a corpus of longitudinal medical records who either
    meet or do not meet predefined selection criteria. These criteria are used for
    determining a patient’s eligibility for inclusion in clinical trials. Stubbs et al.
    ([2019](#bib.bib27)). The data consists of annotated American English clinical
    narratives for 288 patients according to whether they met a set of specific criteria.
    There are 13 criteria in total, and they include: DRUG-ABUSE: Drug abuse, current
    or past; ALCOHOL-ABUSE: Current alcohol use over weekly recommended limits; ENGLISH:
    Patient must speak English; MAKES-DECISIONS: Patient must make their own medical
    decisions; ABDOMINAL: History of intra-abdominal surgery, small or large intestine
    resection, or small bowel obstruction; MAJOR-DIABETES: Major diabetes-related
    complication; ADVANCED-CAD: Advanced cardiovascular disease (CAD); MI-6MOS: MI
    in the past 6 months; KETO-1YR: Diagnosis of ketoacidosis in the past year; DIETSUPP-2MOS:
    Taken a dietary supplement (excluding vitamin D) in the past 2 months; ASP-FOR-MI:
    Use of aspirin to prevent MI; HBA1C: Any hemoglobin A1c (HbA1c) value between
    6.5% and 9.5%; and CREATININE: Serum creatinine  upper
    limit of normal. For every medical record, each criterion can have one of two
    potential values: “met” or “not met.” The value based on whether an individual
    has fulfilled a particular criterion. Data is split 70/30 on training and test
    sets respectively. Training test contains 202 medical record while the test set
    contains 86 records. Note that for some criteria, the ratio between positive and
    negative class is highly imbalanced. In our analysis we excluded KETO-1YR criterion
    as it contains no positive samples in the test set and only one positive sample
    in the training set.⁴⁴4Our methodology employs classifiers that are trained based
    on data distributions. As a result, we consistently achieve peak classification
    metrics, which is not a realistic performance, as the minority class is absent
    from the test dataset.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自2018年国家NLP临床挑战赛（n2c2）的Track 1共享任务³³3https://n2c2.dbmi.hms.harvard.edu/。它旨在帮助识别纵向医学记录语料库中符合或不符合预定义选择标准的患者。这些标准用于确定患者是否符合纳入临床试验的资格（Stubbs
    et al. ([2019](#bib.bib27))）。数据包括288名患者的标注美式英语临床叙述，根据他们是否满足一组特定标准进行分类。总共有13个标准，包括：DRUG-ABUSE（药物滥用）：当前或过去的药物滥用；ALCOHOL-ABUSE（酒精滥用）：每周饮酒超过推荐限度；ENGLISH（英语）：患者必须讲英语；MAKES-DECISIONS（自主决策）：患者必须自行做医疗决策；ABDOMINAL（腹部）：有腹部手术、小肠或大肠切除术、或小肠梗阻史；MAJOR-DIABETES（主要糖尿病）：主要糖尿病相关并发症；ADVANCED-CAD（晚期冠心病）：晚期心血管疾病（CAD）；MI-6MOS（过去6个月心肌梗死）：过去6个月的心肌梗死；KETO-1YR（过去1年酮症酸中毒）：过去一年内诊断为酮症酸中毒；DIETSUPP-2MOS（过去2个月的膳食补充剂）：过去2个月内服用膳食补充剂（不包括维生素D）；ASP-FOR-MI（用于预防心肌梗死的阿司匹林）：使用阿司匹林预防心肌梗死；HBA1C（糖化血红蛋白）：糖化血红蛋白（HbA1c）值在6.5%到9.5%之间；CREATININE（肌酐）：血清肌酐低于正常上限。对于每份医学记录，每个标准可以有两个可能的值：“met（满足）”或“not
    met（未满足）”。该值基于个人是否满足特定标准。数据按70/30的比例分为训练集和测试集。训练集包含202份医学记录，而测试集包含86份记录。需要注意的是，对于某些标准，正负类别的比例高度不平衡。在我们的分析中，我们排除了KETO-1YR标准，因为测试集中没有正样本，而训练集中只有一个正样本⁴⁴4。我们的方法使用基于数据分布的分类器进行训练。因此，我们始终能够达到峰值分类指标，这并不反映实际性能，因为测试数据集中缺少少数类样本。
- en: 4.2 Catalonia Independence Corpus
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 加泰罗尼亚独立语料库
- en: 'This dataset contains a corpus in Spanish that consist of annotated Twitter
    messages for automatic stance detection Zotova et al. ([2020](#bib.bib46)). It
    encompasses data collected over a 12-day span in February and March 2019, from
    tweets originating in Barcelona. Originally, each tweet is categorized into one
    of three classes: AGAINST, FAVOR, and NEUTRAL. These classes represent the user’s
    stance towards the topic of Catalonia’s independence. For the purpose of binary
    classification and to facilitate more effective comparisons with other datasets,
    we have omitted the NEUTRAL class, focusing exclusively on the AGAINST and FAVOR
    categories.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含西班牙语语料库，包含用于自动立场检测的标注Twitter消息（Zotova et al. ([2020](#bib.bib46))）。它涵盖了2019年2月和3月在巴塞罗那发布的推文数据，收集时间跨度为12天。最初，每条推文被分类为三种类别之一：AGAINST（反对）、FAVOR（支持）和NEUTRAL（中立）。这些类别代表用户对加泰罗尼亚独立话题的立场。为了进行二分类并方便与其他数据集的比较，我们省略了NEUTRAL类别，专注于AGAINST和FAVOR类别。
- en: 4.3 Climate Detection Corpus
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 气候检测语料库
- en: This dataset contains climate-related paragraphs extracted from financial disclosures
    by companies. The text has been collected from corporate annual reports and sustainability
    reports. The paragraphs from those reports are hand-selected and then annotated
    as yes (climate-related) or no (not climate-related) Webersinke et al. ([2021](#bib.bib32)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含了从公司财务披露中提取的与气候相关的段落。文本来自企业年度报告和可持续发展报告。这些报告中的段落经过人工挑选，然后标注为“是”（与气候相关）或“否”（不与气候相关）
    Webersinke 等人 ([2021](#bib.bib32))。
- en: 4.4 Medical health advice data
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 医疗健康建议数据
- en: 'This dataset comprises a collection of sentences related to the medical domain,
    each accompanied by a label indicating whether the sentence offers medical advice.
    The labels can be one of three values: “strong advice”, “weak advice”, or “no
    advice”. Yu et al. ([2019](#bib.bib41)) For the purpose of binary classification
    task we combined “strong advice” and “weak advice” into a single class: “advice”.
    The dataset includes approximately 8,000 samples, which have been divided into
    training and test datasets following the 80/20 rule.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含了与医疗领域相关的句子，每个句子都有一个标签，指示该句子是否提供医疗建议。标签可以是以下三种值之一：“强建议”、“弱建议”或“没有建议”。Yu
    等人 ([2019](#bib.bib41)) 为了进行二分类任务，我们将“强建议”和“弱建议”合并为一个类别：“建议”。该数据集包括大约 8,000 个样本，这些样本已经按照
    80/20 规则划分为训练集和测试集。
- en: 4.5 The European Court of Human Rights (ECtHR) Data
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 欧洲人权法院（ECtHR）数据
- en: The European Court of Human Rights (ECtHR) hears allegations that a state has
    breached human rights provisions of the European Convention of Human Rights (ECHR) Chalkidis
    et al. ([2019](#bib.bib6)). The dataset for each case includes a series of facts
    in form of paragraphs extracted from the case description. Additionally, each
    case is associated with specific articles of the European Convention on Human
    Rights (ECHR) that may have been violated. In many cases, multiple articles are
    violated at the same time. To make this a binary categorization problem, we adopted
    a binary labeling system. Cases are marked with a “1” if any ECHR articles are
    violated, and a “0” if no violations are detected.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲人权法院（ECtHR）审理指控国家违反《欧洲人权公约》（ECHR）人权条款的案件。Chalkidis 等人 ([2019](#bib.bib6))
    每个案件的数据集包括一系列以段落形式提取的事实，来自案件描述。此外，每个案件都与可能被违反的《欧洲人权公约》（ECHR）的具体条款相关联。在许多案件中，多个条款可能同时被违反。为了将其转化为二分类问题，我们采用了二元标记系统。如果任何
    ECHR 条款被违反，案件标记为“1”；如果没有发现违反，案件标记为“0”。
- en: 4.6 UNFAIR-ToS Dataset
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 UNFAIR-ToS 数据集
- en: 'The UNFAIR-ToS dataset contains 50 relevant on-line consumer contracts, i.e.
    Terms of Service (ToS) from on-line platforms (e.g., YouTube, Ebay, Facebook,
    etc.). Each agreement has been annotated at the sentence level to identify various
    types of potentially unfair clauses, which could infringe upon user rights under
    European consumer law. This dataset categorizes unfair terms into eight distinct
    groups: Arbitration, Unilateral Change, Content Removal, Jurisdiction, Choice
    of Law, Limitation of Liability, Unilateral Termination, and Contract by Using Lippi
    et al. ([2019](#bib.bib15)). To transform the analysis into a binary classification
    problem, we re-labelled each sentence as either “unfair” if it contains any type
    of the identified unfair terms, or “not unfair’ if it does not fall into these
    categories.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: UNFAIR-ToS 数据集包含 50 个相关的在线消费者合同，即来自在线平台（如 YouTube、Ebay、Facebook 等）的服务条款（ToS）。每个协议在句子级别上进行了注释，以识别可能侵犯用户在欧洲消费者法下权利的各种不公平条款。该数据集将不公平条款分为八个不同的类别：仲裁、单方面变更、内容删除、管辖权、法律选择、责任限制、单方面终止和使用合同
    Lippi 等人 ([2019](#bib.bib15))。为了将分析转化为二分类问题，我们将每个句子重新标记为“unfair”如果它包含任何一种识别出的不公平条款，或“not
    unfair”如果它不属于这些类别。
- en: 5 Experiments
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: We performed the experiments on a set of binary classification tasks on datasets
    from various domains, as described in the previous section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在来自不同领域的数据集上进行了二分类任务的实验，如前一节所述。
- en: 'To generate the secondary questions, we employed a large language model. Prompting
    it only once, we obtained a set of $n$ secondary questions, which we accepted
    as provided, without any selection or modification. More specifically, we used
    the following prompt for creating all secondary questions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成二级问题，我们使用了一个大型语言模型。仅提示一次，我们获得了一组 $n$ 个二级问题，我们接受了这些问题，而没有进行任何选择或修改。更具体地说，我们使用了以下提示来创建所有二级问题：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'where $n$ is the number of additional questions we want to generate and `primary_question`
    is the primary question used to obtain the main information from the document.
    Note that in all our experiments $n=4$. That means that for each document we use
    one primary and four secondary questions that are treated equally when prompting
    the LLM. Thus, for each document we collect five answers from the LLM that are
    then verbalized (assigned a numerical value) in the next step. To generate the
    secondary questions for all our experiments we use OpenAI’s `gpt-4-0125-preview`
    model. To collect the answers in our experiments we use two generations of OpenAI’s
    models: `gpt-4-0125-preview` Achiam et al. ([2023](#bib.bib1)) and `gpt-3.5-turbo-0125` Brown
    et al. ([2020](#bib.bib5)).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n$ 是我们希望生成的额外问题的数量，而 `primary_question` 是用来从文档中获取主要信息的主要问题。请注意，在我们所有的实验中
    $n=4$。这意味着对于每个文档，我们使用一个主要问题和四个次要问题，这些问题在提示 LLM 时被视为同等重要。因此，对于每个文档，我们从 LLM 收集五个答案，然后在下一步骤中对这些答案进行语言化（分配数值）。为了生成所有实验的次要问题，我们使用了
    OpenAI 的 `gpt-4-0125-preview` 模型。为了收集答案，我们使用了 OpenAI 模型的两个版本：`gpt-4-0125-preview`
    Achiam et al. ([2023](#bib.bib1)) 和 `gpt-3.5-turbo-0125` Brown et al. ([2020](#bib.bib5))。
- en: To choose the best classifier, we train several different classification algorithms.
    These include K-Nearest Neighbors, Decision Trees, Random Forest, Gaussian Naive
    Bayes, Multinomial Naive Bayes, AdaBoost, and XGBoost. We use a 5-fold cross-validation
    on our training data and also perform a grid search to fine-tune the parameters
    for each classifier. After training, we test them on a hold-out test set and choose
    the classifier that gives us the highest Micro F1 score ($\mu F1$). Note that
    one can also adjust the training process to optimize for a specific performance
    metric if needed for a particular application. To perform these experiments, we
    used the `scikit-learn` library in Python.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择最佳分类器，我们训练了几种不同的分类算法。这些算法包括 K-Nearest Neighbors、决策树、随机森林、高斯朴素贝叶斯、多项式朴素贝叶斯、AdaBoost
    和 XGBoost。我们在训练数据上使用了 5 折交叉验证，并进行了网格搜索以微调每个分类器的参数。训练后，我们在保留的测试集上测试这些分类器，并选择得出最高
    Micro F1 分数（$\mu F1$）的分类器。请注意，如果需要针对特定应用优化特定性能指标，也可以调整训练过程。为了进行这些实验，我们使用了 Python
    中的 `scikit-learn` 库。
- en: 'Micro F1 score is particularly useful in datasets where some classes are significantly
    underrepresented, and where traditional metrics might give a misleading picture
    of model performance. It treats every instance as equally important, thereby giving
    a more accurate measure of the model’s performance across the board. To calculate
    the $\mu F1$, we use the following formula:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Micro F1 分数在某些类显著不足的数据集中特别有用，在这些数据集中，传统指标可能会对模型性能产生误导。它将每个实例视为同等重要，从而提供对模型整体性能的更准确衡量。计算
    $\mu F1$ 时，我们使用以下公式：
- en: '|  | $\mu F1=\frac{2\mu Precision\times\mu Recall}{\mu Precision+\mu Recall}$
    |  | (1) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu F1=\frac{2\mu Precision\times\mu Recall}{\mu Precision+\mu Recall}$
    |  | (1) |'
- en: where
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\mu Precision=\frac{\sum{TP_{i}}}{\sum{TP_{i}+\sum{FP_{i}}}}$ |  | (2)
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu Precision=\frac{\sum{TP_{i}}}{\sum{TP_{i}+\sum{FP_{i}}}}$ |  | (2)
    |'
- en: '|  | $\mu Recall=\frac{\sum{TP_{i}}}{\sum{TP_{i}+\sum{FN_{i}}}}$ |  | (3) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu Recall=\frac{\sum{TP_{i}}}{\sum{TP_{i}+\sum{FN_{i}}}}$ |  | (3) |'
- en: and $TP$, $FP$ and $FP$ represent number of true positives, false positives
    and false negatives respectively.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: $TP$、$FP$ 和 $FN$ 分别表示真正例、假正例和假负例的数量。
- en: 'Additionally, we conducted a sensitivity analysis to enhance our understanding
    of the relationship between the number of features and the improvement of the
    $\mu F1$. This analysis helps determine the requisite number of secondary questions
    to attain a desired $\mu F1$. For each dataset, we started by creating $n=9$ secondary
    questions and using the `gpt-3.5-turbo-0125` model to generate responses for each
    sample. The outputs from the large language model were then transformed into 10-dimensional
    feature vectors. Subsequently, we constructed a series of simple Random Forest
    classifiers, starting with a single feature and incrementally adding more features
    up to ten. Given the random selection of features for classification, we repeated
    the experiment 100 times. We computed the $\mu F1$ for each iteration and dataset.
    The findings are detailed in Section [6](#S6 "6 Results ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance")
    and illustrated in Figure [3](#S6.F3 "Figure 3 ‣ 6 Results ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们进行了灵敏度分析，以增强对特征数量与 $\mu F1$ 改进之间关系的理解。这项分析帮助确定了达到期望的 $\mu F1$ 所需的二级问题数量。对于每个数据集，我们从创建
    $n=9$ 个二级问题开始，并使用 `gpt-3.5-turbo-0125` 模型生成每个样本的响应。然后，将大型语言模型的输出转化为 10 维特征向量。随后，我们构建了一系列简单的随机森林分类器，从一个特征开始，逐步添加更多特征，直到十个特征。由于特征的随机选择用于分类，我们重复实验了
    100 次。我们计算了每次迭代和每个数据集的 $\mu F1$。这些发现详述于第[6](#S6 "6 结果 ‣ 可解释的交叉检查技术（ICE-T）：使用高信息量特征提升大语言模型的表现")节，并在图[3](#S6.F3
    "图 3 ‣ 6 结果 ‣ 可解释的交叉检查技术（ICE-T）：使用高信息量特征提升大语言模型的表现")中进行了说明。
- en: 6 Results
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结果
- en: 'The results of the classification experiments are summarized in Table [1](#S6.T1
    "Table 1 ‣ 6 Results ‣ Interpretable Cross-Examination Technique (ICE-T): Using
    highly informative features to boost LLM performance"). We can see that across
    all datasets, the ICE-T method consistently surpasses the zero-shot approach in
    performance for a given language models. Specifically, using the GPT-3.5 model,
    the average $\mu F1$ for the zero-shot approach is 0.683, but it increases to
    0.845 with the ICE-T method. A similar trend is observed with the larger GPT-4
    model, where the average F1 score improves from 0.7 using the zero-shot approach
    to 0.892 with the ICE-T technique. This improvement is not constant across the
    datasets as we can see a significant variations in performance and in improvements
    across different tasks.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 分类实验的结果总结在表[1](#S6.T1 "表 1 ‣ 6 结果 ‣ 可解释的交叉检查技术（ICE-T）：使用高信息量特征提升大语言模型的表现")中。我们可以看到，在所有数据集中，ICE-T
    方法在给定语言模型的性能上始终超过了零样本方法。具体而言，使用 GPT-3.5 模型时，零样本方法的平均 $\mu F1$ 为 0.683，但使用 ICE-T
    方法后提高到 0.845。类似的趋势也在更大的 GPT-4 模型中观察到，其中平均 F1 分数从使用零样本方法的 0.7 提升到使用 ICE-T 技术的 0.892。由于不同任务的性能和改进有显著变化，因此这种改进在各数据集中并不恒定。
- en: '|  | GPT-3.5 | GPT-4 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-3.5 | GPT-4 |'
- en: '|  | 0-shot | ICE-T | 0-shot | ICE-T |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | 0-shot | ICE-T | 0-shot | ICE-T |'
- en: '| ABDOMINAL | 0.791 | 0.814 | 0.802 | 0.884 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ABDOMINAL | 0.791 | 0.814 | 0.802 | 0.884 |'
- en: '| ADVANCED-CAD | 0.640 | 0.756 | 0.791 | 0.907 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ADVANCED-CAD | 0.640 | 0.756 | 0.791 | 0.907 |'
- en: '| ALCOHOL-ABUSE | 0.814 | 0.965 | 0.791 | 0.965 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| ALCOHOL-ABUSE | 0.814 | 0.965 | 0.791 | 0.965 |'
- en: '| ASP-FOR-MI | 0.849 | 0.86 | 0.860 | 0.895 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ASP-FOR-MI | 0.849 | 0.86 | 0.860 | 0.895 |'
- en: '| CREATININE | 0.349 | 0.721 | 0.488 | 0.872 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| CREATININE | 0.349 | 0.721 | 0.488 | 0.872 |'
- en: '| DIETSUPP-2MOS | 0.593 | 0.767 | 0.488 | 0.814 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| DIETSUPP-2MOS | 0.593 | 0.767 | 0.488 | 0.814 |'
- en: '| DRUG-ABUSE | 0.942 | 0.977 | 0.826 | 0.977 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DRUG-ABUSE | 0.942 | 0.977 | 0.826 | 0.977 |'
- en: '| ENGLISH | 0.919 | 0.988 | 0.233 | 0.965 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ENGLISH | 0.919 | 0.988 | 0.233 | 0.965 |'
- en: '| HBA1C | 0.477 | 0.837 | 0.523 | 0.942 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| HBA1C | 0.477 | 0.837 | 0.523 | 0.942 |'
- en: '| MAJOR-DIABETES | 0.733 | 0.814 | 0.570 | 0.884 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| MAJOR-DIABETES | 0.733 | 0.814 | 0.570 | 0.884 |'
- en: '| MAKES-DECISIONS | 0.605 | 0.965 | 0.663 | 0.965 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| MAKES-DECISIONS | 0.605 | 0.965 | 0.663 | 0.965 |'
- en: '| MI-6MOS | 0.651 | 0.919 | 0.849 | 0.953 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MI-6MOS | 0.651 | 0.919 | 0.849 | 0.953 |'
- en: '| Catalonia indep. | 0.528 | 0.579 | 0.562 | 0.604 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Catalonia indep. | 0.528 | 0.579 | 0.562 | 0.604 |'
- en: '| Climate detection | 0.702 | 0.8 | 0.912 | 0.925 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 气候检测 | 0.702 | 0.8 | 0.912 | 0.925 |'
- en: '| ECtHR | 0.853 | 0.873 | 0.861 | 0.873 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ECtHR | 0.853 | 0.873 | 0.861 | 0.873 |'
- en: '| Health advice | 0.836 | 0.841 | 0.846 | 0.846 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Health advice | 0.836 | 0.841 | 0.846 | 0.846 |'
- en: '| UNFAIR-ToS | 0.335 | 0.887 | 0.837 | 0.889 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| UNFAIR-ToS | 0.335 | 0.887 | 0.837 | 0.889 |'
- en: '| Average | 0.683 | 0.845 | 0.700 | 0.892 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 0.683 | 0.845 | 0.700 | 0.892 |'
- en: 'Table 1: Comparison of $\mu F1$ scores between zero-shot setting and ICE-T
    method. The values in bold represent the $\mu F1$ score of the winning approach
    for a specific task and a language model. Horizontal line in the middle splits
    the clinical trial datasets and other datasets. All tasks solve the binary classification
    problem.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：零样本设置与 ICE-T 方法之间的 $\mu F1$ 分数比较。粗体值表示特定任务和语言模型的获胜方法的 $\mu F1$ 分数。中间的水平线将临床试验数据集与其他数据集分开。所有任务解决的是二分类问题。
- en: 'The upper portion of Table [1](#S6.T1 "Table 1 ‣ 6 Results ‣ Interpretable
    Cross-Examination Technique (ICE-T): Using highly informative features to boost
    LLM performance") showcases the findings from the clinical trial dataset, as detailed
    in Section [4](#S4 "4 Data ‣ Interpretable Cross-Examination Technique (ICE-T):
    Using highly informative features to boost LLM performance"). The dataset’s contents
    remain consistent across all sub-tasks within this clinical trial dataset, though
    each sub-task involves a distinct classification criterion based on 12 different
    criteria. In some sub-tasks, substantial improvements were observed over the zero-shot
    method. For instance, in the task CREATININE (involving serum creatinine levels
    exceeding the upper normal limit), the zero-shot method achieved $\mu F1$ of 0.349\.
    In contrast, the ICE-T technique utilizing the same large language model significantly
    improved this score to 0.721\. Similarly, for the task ENGLISH (determining if
    a patient speaks English) using the larger GPT4 model, the greatest increase noted
    exceeded 0.733 points, with the zero-shot approach at a $\mu F1$ of 0.233 and
    the ICE-T technique improving it to 0.966\. Analysis of tasks outside the clinical
    trial dataset revealed varied results, dependent on the specific domain. The task
    assessing “Catalonia independence” presented a notable challenge in the zero-shot
    setup for both models, barely achieving a $\mu F1$ above 0.5, with no significant
    improvements noted with the ICE-T technique.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S6.T1 "表 1 ‣ 6 结果 ‣ 可解释交叉检查技术 (ICE-T)：利用高度信息特征提升大型语言模型性能") 的上半部分展示了来自临床试验数据集的结果，详见第
    [4](#S4 "4 数据 ‣ 可解释交叉检查技术 (ICE-T)：利用高度信息特征提升大型语言模型性能") 节。数据集的内容在所有子任务中保持一致，尽管每个子任务基于
    12 个不同标准涉及不同的分类标准。在一些子任务中，相对于零样本方法观察到了显著的改进。例如，在任务 CREATININE（涉及血清肌酐水平超过正常上限）中，零样本方法的
    $\mu F1$ 为 0.349。而应用 ICE-T 技术的同一大型语言模型将这一分数显著提高至 0.721。同样，对于任务 ENGLISH（判断患者是否讲英语）使用较大的
    GPT4 模型时，最大增幅超过了 0.733 分，零样本方法的 $\mu F1$ 为 0.233，而 ICE-T 技术将其提升至 0.966。对临床试验数据集之外任务的分析显示，结果因具体领域而异。评估“加泰罗尼亚独立”的任务在零样本设置下对两种模型都提出了显著挑战，仅勉强达到
    $\mu F1$ 0.5，ICE-T 技术没有显著改进。
- en: The task related to the European Court of Human Rights (ECtHR) already exhibited
    high baseline scores in the zero-shot setting, achieving 0.853 with GPT-3.5 and
    0.861 with GPT-4\. The application of the ICE-T technique yielded minimal improvement,
    with both models achieving a $\mu F1$ of 0.873\. A similar scenario was observed
    with the Health advice dataset, where enhancements were negligible.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与欧洲人权法院（ECtHR）相关的任务在零样本设置下已经展示了较高的基线分数，GPT-3.5 达到了 0.853，而 GPT-4 达到了 0.861。应用
    ICE-T 技术的改进非常有限，两个模型的 $\mu F1$ 都为 0.873。健康建议数据集也观察到了类似的情况，其中改进微乎其微。
- en: However, the UNFAIR-ToS task demonstrated significant improvement using the
    ICE-T approach, particularly with the GPT-3.5 model. Here, the $\mu F1$ score
    saw a dramatic increase from 0.335 to 0.887.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，UNFAIR-ToS 任务在使用 ICE-T 方法时表现出显著改进，特别是使用 GPT-3.5 模型时。这里，$\mu F1$ 分数从 0.335
    大幅提升至 0.887。
- en: 'Furthermore, our analysis reveals that the ICE-T technique, when applied to
    a smaller model, can surpass or match the performance of a larger model that uses
    the zero-shot approach. In our experiments, we assessed the $\mu F1$ of classification
    tasks executed by GPT-4 in a zero-shot setting against those performed by GPT-3.5
    using the ICE-T technique across various datasets. In nearly all cases, except
    for two, the ICE-T-enhanced GPT-3.5 either outperformed or equaled the larger
    GPT-4 model on identical tasks. These findings are depicted in Figure [2](#S6.F2
    "Figure 2 ‣ 6 Results ‣ Interpretable Cross-Examination Technique (ICE-T): Using
    highly informative features to boost LLM performance").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们的分析显示，当将ICE-T技术应用于较小的模型时，其表现可以超越或与使用零样本方法的大型模型相匹配。在我们的实验中，我们评估了在零样本设置下由GPT-4执行的分类任务的$\mu
    F1$，并将其与使用ICE-T技术的GPT-3.5在各种数据集上执行的任务进行了比较。除两个例外外，ICE-T增强版GPT-3.5在相同任务上通常超越或与较大的GPT-4模型表现相当。这些发现如图[2](#S6.F2
    "Figure 2 ‣ 6 Results ‣ Interpretable Cross-Examination Technique (ICE-T): Using
    highly informative features to boost LLM performance")所示。'
- en: '![Refer to caption](img/2663624b7277568cfb35f505d31d6e40.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2663624b7277568cfb35f505d31d6e40.png)'
- en: 'Figure 2: Comparative performance of ICE-T-enhanced GPT-3.5 versus zero-shot
    GPT-4. The figure illustrates the $\mu F1$ achieved by GPT-3.5 utilizing the ICE-T
    technique and GPT-4 in a zero-shot setting across multiple datasets.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：ICE-T增强版GPT-3.5与零样本GPT-4的比较表现。该图展示了GPT-3.5利用ICE-T技术和GPT-4在零样本设置下在多个数据集上取得的$\mu
    F1$。
- en: 'We observed a minor variation in performance across different task groups.
    By categorizing clinical trial tasks into one group and other tasks into another,
    we observed a comparable average performance improvement when comparing the zero-shot
    to the ICE-T approach, as detailed in Table [4](#A2.T4 "Table 4 ‣ Appendix B Additional
    results ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance") in Appendix [B](#A2 "Appendix B Additional
    results ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance"). This consistency underscores the versatility
    of the ICE-T method across various domains and tasks.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到不同任务组之间的性能有细微的变化。通过将临床试验任务归为一组，其他任务归为另一组，我们发现，在将零样本方法与ICE-T方法进行比较时，平均性能提升相当，如附录[B](#A2
    "Appendix B Additional results ‣ Interpretable Cross-Examination Technique (ICE-T):
    Using highly informative features to boost LLM performance")中的表[4](#A2.T4 "Table
    4 ‣ Appendix B Additional results ‣ Interpretable Cross-Examination Technique
    (ICE-T): Using highly informative features to boost LLM performance")所详述。这种一致性突显了ICE-T方法在各个领域和任务中的多样性。'
- en: 'To explore how the number of features impacts the micro F1 score ($\mu F1$),
    we conducted an additional sensitivity analysis. The outcomes of this analysis
    are depicted in Figure [3](#S6.F3 "Figure 3 ‣ 6 Results ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").
    This figure illustrates the change of the $\mu F1$ as we incrementally introduce
    more features (obtained by secondary questions). A solid orange line shows the
    average $\mu F1$ across all datasets, while the surrounding shaded area indicates
    one standard deviation from the mean, based on 100 iterations. As anticipated,
    there is a consistent increase in the micro F1 score with the addition of more
    secondary questions. On average, adding three secondary questions increases the
    $\mu F1$ score from 0.76 to 0.80, with further additions raising it to 0.82.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '为了探究特征数量如何影响微平均F1分数（$\mu F1$），我们进行了额外的敏感性分析。该分析的结果如图[3](#S6.F3 "Figure 3 ‣
    6 Results ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance")所示。该图展示了在逐步引入更多特征（通过次级问题获得）时$\mu F1$的变化。一个实心橙色线条表示所有数据集上的平均$\mu
    F1$，而周围阴影区域则表示基于100次迭代的均值的一次标准偏差。如预期的那样，随着次级问题的增加，微平均F1分数持续上升。平均而言，增加三个次级问题将$\mu
    F1$分数从0.76提高到0.80，而进一步增加则将其提升至0.82。'
- en: 'It is important to highlight that this figure averages results from 17 different
    datasets, using only the Random Forest classifier. Detailed results for each individual
    task are available in Figure [4](#A2.F4 "Figure 4 ‣ Appendix B Additional results
    ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance") in Appendix [B](#A2 "Appendix B Additional
    results ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance"). The use of a single classifier in this analysis
    was a deliberate choice to isolate the impact of increasing the number of features,
    thereby minimizing the influence of classifier selection on the results. However,
    this choice may also limit the generalizability of the findings, as it differs
    from previous analyses where the optimal classifier was selected for each task.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '需要强调的是，这个图表平均了来自17个不同数据集的结果，只使用了随机森林分类器。每个单独任务的详细结果可以在附录[B](#A2 "附录B 额外结果 ‣
    可解释的交叉检查技术 (ICE-T): 使用高信息量特征提升LLM性能")中的图[4](#A2.F4 "图4 ‣ 附录B 额外结果 ‣ 可解释的交叉检查技术
    (ICE-T): 使用高信息量特征提升LLM性能")找到。在此分析中使用单一分类器是为了隔离增加特征数量的影响，从而最小化分类器选择对结果的影响。然而，这种选择也可能限制了结果的普遍性，因为它不同于以往的分析，其中为每个任务选择了最佳分类器。'
- en: '![Refer to caption](img/955454c4a44217ae03be00a0f1f767e7.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/955454c4a44217ae03be00a0f1f767e7.png)'
- en: 'Figure 3: Sensitivity Analysis of Feature Count on $\mu F1$ Score. The figure
    illustrates the effect of increasing the number of features (secondary questions)
    on the $\mu F1$ score across 17 datasets. The solid orange line represents the
    average $\mu F1$ score, and the shaded area indicates the first standard deviation
    from the mean across 100 repetitions. The graph demonstrates a consistent improvement
    in $\mu F1$ as more features are added, with key points of increase highlighted
    at specific feature counts.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：特征数量对$\mu F1$分数的敏感性分析。图表展示了在17个数据集中增加特征（次要问题）数量对$\mu F1$分数的影响。橙色实线表示平均$\mu
    F1$分数，阴影区域表示在100次重复中的第一个标准偏差。图表展示了随着更多特征的增加，$\mu F1$的持续改善，关键的提升点在特定的特征数量下被突出显示。
- en: 7 Discussion
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: Our study introduces the Interpretable Cross-Examination Technique (ICE-T),
    a novel prompting method that integrates LLM responses with traditional classification
    algorithms to improve the performance on binary classification tasks. This technique
    addresses key limitations in zero-shot and few-shot learning by employing a structured,
    multi-prompt approach that transforms qualitative data into quantifiable metrics,
    thus allowing a small, traditional classifier to effectively make decisions. Our
    results confirm that ICE-T consistently surpasses zero-shot baselines across multiple
    datasets and metrics, particularly in scenarios where model interpretability is
    crucial. This prompting strategy also demonstrates the potential for fully automated,
    high-performing AI systems accessible even to non-experts.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究引入了可解释的交叉检查技术（ICE-T），这是一种新颖的提示方法，将LLM的响应与传统分类算法结合，以提高二分类任务的性能。这一技术通过采用结构化的多提示方法，将定性数据转化为可量化的指标，从而使得一个小型传统分类器能够有效地做出决策，从而解决了零-shot
    和少-shot 学习中的关键限制。我们的结果确认ICE-T在多个数据集和指标中始终超越零-shot 基线，特别是在模型可解释性至关重要的情况下。这种提示策略还展示了完全自动化的高性能AI系统的潜力，使即使是非专家也能使用。
- en: The ICE-T method has demonstrated its capability to not only enhance performance
    over the zero-shot approach but also to do so with smaller models that might not
    perform as well in a zero-shot configuration. For example, the improvement in
    the CREATININE and ENGLISH tasks within clinical trials data underscores the method’s
    ability to handle domain-specific challenges that require nuanced understanding,
    which zero-shot configurations typically struggle with.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ICE-T 方法已证明其不仅能提升零-shot 方法的性能，还能在使用可能在零-shot 配置中表现不佳的较小模型时做到这一点。例如，CREATININE
    和 ENGLISH 任务在临床试验数据中的改进凸显了该方法处理需要细致理解的领域特定挑战的能力，而这些挑战通常是零-shot 配置难以应对的。
- en: 7.1 Implications for Model Interpretability
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 模型可解释性的影响
- en: A major advantage of the ICE-T approach is its interpretability. By generating
    a feature vector based on direct responses to structured prompts, experts can
    trace back the decision-making process, understanding which factors contributed
    most significantly to the model’s classification. This is particularly valuable
    in fields like medicine and law, where decision rationale is as important as accuracy.
    The ability to dissect and validate each step of the model’s reasoning aligns
    with the growing demand for transparency in AI applications, ensuring that decisions
    made by AI systems can be audited and trusted.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ICE-T方法的一个主要优势是其可解释性。通过基于对结构化提示的直接回应生成特征向量，专家可以追溯决策过程，理解哪些因素对模型的分类贡献最大。这在医学和法律等领域尤其有价值，因为决策理由与准确性同样重要。能够剖析和验证模型推理的每一步符合对AI应用中透明度日益增长的需求，确保AI系统所做的决策可以被审计和信任。
- en: 'Moreover, ICE-T is particularly valuable in situations where fine-tuning models
    is not viable. Fine-tuned models often suffer from a significant drawback: they
    lack transparency and become “black boxes,” making their decision-making processes
    obscure. This lack of interpretability is particularly problematic in regulated
    sectors such as healthcare, law and finance, where it’s imperative to comprehend
    the basis of each decision. ICE-T overcomes these issues by employing a methodology
    that remains clear and interpretable, avoiding the opaqueness associated with
    fine-tuned systems.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ICE-T在无法进行模型微调的情况下尤其有价值。微调的模型通常存在一个显著的缺陷：它们缺乏透明性，变成了“黑箱”，使得决策过程变得模糊。这种缺乏可解释性在医疗、法律和金融等受监管的行业尤其成问题，因为这些领域中理解每个决策的依据至关重要。ICE-T通过采用保持清晰和可解释的方法，克服了这些问题，避免了与微调系统相关的晦涩。
- en: 7.2 Limitations and Future Work
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 局限性与未来工作
- en: Despite its strengths, the ICE-T method has some limitations. The quality of
    the output heavily relies on the initial set of questions generated for the model
    to answer. Poorly formulated questions or those that fail to capture the necessary
    subtleties of the task can limit the effectiveness of this technique. Moreover,
    the reliance on numerical scoring of textual answers might oversimplify complex
    answers.This can lead to a loss of nuance, especially when answers are confined
    to binary outputs.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ICE-T方法具有一定的优势，但也存在一些局限性。输出的质量很大程度上依赖于为模型生成的初始问题集。如果问题设计不佳或未能捕捉任务的必要细微之处，可能会限制这一技术的效果。此外，对文本答案的数值评分可能会过于简化复杂的回答，这可能会导致细微差别的丧失，尤其是当答案被限定为二元输出时。
- en: Future research could explore more sophisticated methods for question generation,
    perhaps incorporating active learning where the system identifies and prioritizes
    questions that would most improve its understanding and performance. Additionally,
    exploring different methods of encoding responses into feature vectors could further
    enhance the model’s accuracy and sensitivity to nuances in text.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究可以探索更复杂的问题生成方法，也许可以结合主动学习，让系统识别并优先考虑那些最能改善理解和性能的问题。此外，探索将响应编码为特征向量的不同方法，可能进一步提高模型的准确性和对文本细微差别的敏感性。
- en: Expanding the scope of ICE-T to tackle problems beyond binary classification
    could also prove beneficial. Applying this method to multi-class classification
    tasks or even regression problems could test the adaptability and scalability
    of the approach, potentially making it more applicable across a wider array of
    domains. This expansion could lead to significant advancements in the field of
    machine learning where interpretability and accuracy are crucial.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展ICE-T的范围以处理二元分类以外的问题也可能是有益的。将这种方法应用于多类分类任务或甚至回归问题，可以测试该方法的适应性和可扩展性，可能使其在更广泛的领域中更具应用性。这种扩展可能会在机器学习领域带来重要进展，在这里可解释性和准确性至关重要。
- en: In conclusion, the ICE-T method presents a promising avenue for enhancing the
    performance and interpretability of LLMs in binary classification tasks and beyond.
    By bridging the gap between traditional machine learning techniques and modern
    LLM capabilities, this approach offers a valuable tool for applications demanding
    high accuracy and clear reasoning in decision-making processes. Further refinements
    and adaptations of this technique could significantly impact the deployment of
    AI in critical sectors, enhancing both the reliability and accountability of automated
    systems.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，ICE-T方法为提升LLM在二分类任务及其他领域的表现和可解释性提供了一个有前景的途径。通过弥合传统机器学习技术和现代LLM能力之间的差距，这一方法为需要高准确性和明确推理的决策过程提供了宝贵的工具。进一步完善和调整这一技术可能对AI在关键领域的部署产生显著影响，提高自动化系统的可靠性和问责性。
- en: Reproducibility
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可重复性
- en: 'The experiment is composed of two primary phases: 1) collecting outputs from
    OpenAI’s ChatGPT models, specifically using either `gpt-4-0125-preview` or `gpt-3.5-turbo-0125`;
    and 2) verbalizing the answers (converting the responses into numerical form),
    training classifiers and evaluating their performance on a hold-out test set.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 实验分为两个主要阶段：1）从OpenAI的ChatGPT模型中收集输出，具体使用 `gpt-4-0125-preview` 或 `gpt-3.5-turbo-0125`；2）语言化答案（将响应转换为数值形式），训练分类器并评估其在持出测试集上的表现。
- en: 'The code to reproduce the verbalization, classfier training and testing is
    available on GitHub: https://github.com/gmuric/ICE-T'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 用于重现语言化、分类器训练和测试的代码可在GitHub上找到：https://github.com/gmuric/ICE-T
- en: 'Due to data usage and confidentiality constraints associated with the clinical
    trial dataset, we are unable to share the complete working code for the first
    phase. However, we provide the outputs of the LLMs we obtained. They are available
    in GitHub repository. We additionally provide pseudo-code that illustrates the
    extraction of outputs from the language models that can be used to reproduce the
    first part of the experiment. The complete references to the data used in the
    experiments are explained in Section [4](#S4 "4 Data ‣ Interpretable Cross-Examination
    Technique (ICE-T): Using highly informative features to boost LLM performance").
    Additionally, we include a comprehensive list of the questions used to prompt
    the language models in Appendix [A](#A1 "Appendix A Questions used in ICE-T method
    ‣ Interpretable Cross-Examination Technique (ICE-T): Using highly informative
    features to boost LLM performance"). The pseudo-code for obtaining the answers
    from the LLM is presented below:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与临床试验数据集相关的数据使用和保密限制，我们无法分享第一阶段的完整工作代码。不过，我们提供了我们获得的LLM输出。这些数据在GitHub仓库中可用。我们还提供了伪代码，展示了从语言模型中提取输出的过程，可用于重现实验的第一部分。实验中使用的数据的完整参考文献在第[4节](#S4
    "4 数据 ‣ 可解释的交叉检查技术 (ICE-T)：使用高度信息化特征提升LLM性能")中解释。此外，我们在附录[A](#A1 "附录 A ICE-T方法中使用的问题
    ‣ 可解释的交叉检查技术 (ICE-T)：使用高度信息化特征提升LLM性能")中包含了用于提示语言模型的详细问题列表。以下是从LLM获取答案的伪代码：
- en: Algorithm 1 Collecting outputs from LLM
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 收集LLM的输出
- en: 'for each textual document $t$ do     Prompt LLM with $t$ and corresponding
    questions {Refer to Section [3.2](#S3.SS2 "3.2 Prompting LLM ‣ 3 Method ‣ Interpretable
    Cross-Examination Technique (ICE-T): Using highly informative features to boost
    LLM performance")}     for each question $q_{i}$ related to $t$ do        $a_{i}\leftarrow$
    Output of LLM for $q_{i}$     end for     $A\leftarrow$ Set of all outputs $\{a_{1},a_{2},\ldots,a_{n}\}$     for each
    output $a_{i}$ in $A$ do        $v_{i}\leftarrow$ Numerical value of $a_{i}$ {Refer
    to Section [3.3](#S3.SS3 "3.3 Verbalizing the answers ‣ 3 Method ‣ Interpretable
    Cross-Examination Technique (ICE-T): Using highly informative features to boost
    LLM performance")}     end for     $V_{d}\leftarrow$ Feature vector of all $v_{i}$  end for'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个文本文件 $t$ 执行     使用 $t$ 和相关问题提示LLM {参见第[3.2节](#S3.SS2 "3.2 提示LLM ‣ 3 方法 ‣
    可解释的交叉检查技术 (ICE-T)：使用高度信息化特征提升LLM性能")}     对每个与 $t$ 相关的问题 $q_{i}$ 执行        $a_{i}\leftarrow$
    LLM对 $q_{i}$ 的输出     结束 对 $A$ 中的每个输出 $a_{i}$ 执行        $v_{i}\leftarrow$ $a_{i}$
    的数值 {参见第[3.3节](#S3.SS3 "3.3 语言化答案 ‣ 3 方法 ‣ 可解释的交叉检查技术 (ICE-T)：使用高度信息化特征提升LLM性能")}     结束 $V_{d}\leftarrow$
    所有 $v_{i}$ 的特征向量  结束
- en: Note that due to the stochastic nature of large language models, the outputs
    may vary with each experiment. While these variations are unlikely to significantly
    impact the results, minor discrepancies are possible.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于大型语言模型的随机性，输出可能因每次实验而异。虽然这些变化不太可能显著影响结果，但可能会有轻微的差异。
- en: Acknowledgment
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This material is based upon work supported by the Army ASA(ALT) SBIR CCOE under
    Contract No. W51701-22-C-0035 and the US Air Force under Contract No. FA8750-22-C-0511\.
    Any opinions, findings and conclusions or recommendations expressed in this material
    are those of the author(s) and do not necessarily reflect the views of the Army
    ASA(ALT) SBIR CCOE or the US Air Force.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本材料基于由陆军 ASA(ALT) SBIR CCOE 支持的工作，合同号为 W51701-22-C-0035，以及由美国空军支持的工作，合同号为 FA8750-22-C-0511。材料中表达的任何观点、发现和结论或建议均为作者的观点，不一定反映陆军
    ASA(ALT) SBIR CCOE 或美国空军的观点。
- en: References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等 (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等. 2023. GPT-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: Amodei et al. (2016) Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,
    John Schulman, and Dan Mané. 2016. Concrete problems in ai safety. *arXiv preprint
    arXiv:1606.06565*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amodei 等 (2016) Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,
    John Schulman, 和 Dan Mané. 2016. 人工智能安全中的具体问题。*arXiv 预印本 arXiv:1606.06565*。
- en: Belrose et al. (2023) Nora Belrose, Zach Furman, Logan Smith, Danny Halawi,
    Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. 2023. Eliciting
    latent predictions from transformers with the tuned lens. *arXiv preprint arXiv:2303.08112*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belrose 等 (2023) Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor
    Ostrovsky, Lev McKinney, Stella Biderman, 和 Jacob Steinhardt. 2023. 从调优透镜中引出变压器的潜在预测。*arXiv
    预印本 arXiv:2303.08112*。
- en: 'Bibal et al. (2022) Adrien Bibal, Rémi Cardon, David Alfter, Rodrigo Wilkens,
    Xiaoou Wang, Thomas François, and Patrick Watrin. 2022. Is attention explanation?
    an introduction to the debate. In *Proceedings of the 60th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers)*, pages 3889–3900,
    Dublin, Ireland. Association for Computational Linguistics.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bibal 等 (2022) Adrien Bibal, Rémi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou
    Wang, Thomas François, 和 Patrick Watrin. 2022. 注意力是解释吗？对辩论的介绍。载于 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*，页码
    3889–3900，爱尔兰都柏林。计算语言学协会。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. 语言模型是少样本学习者。*神经信息处理系统进展*, 33:1877–1901。
- en: Chalkidis et al. (2019) Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras.
    2019. [Neural legal judgment prediction in English](https://doi.org/10.18653/v1/P19-1424).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 4317–4323, Florence, Italy. Association for Computational
    Linguistics.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chalkidis 等 (2019) Ilias Chalkidis, Ion Androutsopoulos, 和 Nikolaos Aletras.
    2019. [英语中的神经法律判断预测](https://doi.org/10.18653/v1/P19-1424)。在 *第57届计算语言学协会年会论文集*，页码
    4317–4323，意大利佛罗伦萨。计算语言学协会。
- en: Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D
    Manning. 2019. What does bert look at? an analysis of bert’s attention. *arXiv
    preprint arXiv:1906.04341*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, 和 Christopher D Manning.
    2019. BERT 看到了什么？BERT 注意力的分析。*arXiv 预印本 arXiv:1906.04341*。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等 (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    等. 2021. 训练验证器以解决数学文字问题。*arXiv 预印本 arXiv:2110.14168*。
- en: 'Conneau et al. (2018) Alexis Conneau, German Kruszewski, Guillaume Lample,
    Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single vector:
    Probing sentence embeddings for linguistic properties. *arXiv preprint arXiv:1805.01070*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau 等（2018） Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault
    和 Marco Baroni。2018。你可以在单个向量中塞入什么：探测句子嵌入的语言属性。*arXiv 预印本 arXiv:1805.01070*。
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey on in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2022） Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang,
    Xu Sun, Jingjing Xu 和 Zhifang Sui。2022。关于上下文学习的调查。*arXiv 预印本 arXiv:2301.00234*。
- en: Goodman and Flaxman (2017) Bryce Goodman and Seth Flaxman. 2017. European union
    regulations on algorithmic decision-making and a “right to explanation”. *AI magazine*,
    38(3):50–57.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodman 和 Flaxman（2017） Bryce Goodman 和 Seth Flaxman。2017。欧盟关于算法决策和“解释权”的规定。*AI
    杂志*，38(3):50–57。
- en: 'Gu et al. (2021) Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. Ppt:
    Pre-trained prompt tuning for few-shot learning. *arXiv preprint arXiv:2109.04332*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu 等（2021） Yuxian Gu, Xu Han, Zhiyuan Liu 和 Minlie Huang。2021。Ppt: 预训练提示调优用于少样本学习。*arXiv
    预印本 arXiv:2109.04332*。'
- en: Kasneci et al. (2023) Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria
    Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann,
    Eyke Hüllermeier, et al. 2023. Chatgpt for good? on opportunities and challenges
    of large language models for education. *Learning and individual differences*,
    103:102274.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasneci 等（2023） Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert,
    Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke
    Hüllermeier 等。2023。Chatgpt 为善？关于大型语言模型在教育中的机遇和挑战。*学习与个体差异*，103:102274。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems*, 35:22199–22213.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等（2022） Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo
    和 Yusuke Iwasawa。2022。大型语言模型是零-shot 推理者。*神经信息处理系统进展*，35:22199–22213。
- en: 'Lippi et al. (2019) Marco Lippi, Przemysław Pałka, Giuseppe Contissa, Francesca
    Lagioia, Hans-Wolfgang Micklitz, Giovanni Sartor, and Paolo Torroni. 2019. Claudette:
    an automated detector of potentially unfair clauses in online terms of service.
    *Artificial Intelligence and Law*, 27:117–139.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lippi 等（2019） Marco Lippi, Przemysław Pałka, Giuseppe Contissa, Francesca Lagioia,
    Hans-Wolfgang Micklitz, Giovanni Sartor 和 Paolo Torroni。2019。Claudette：一个自动检测在线服务条款中潜在不公平条款的工具。*人工智能与法律*，27:117–139。
- en: Liu et al. (2021) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-$3$?
    *arXiv preprint arXiv:2101.06804*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021） Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin
    和 Weizhu Chen。2021。什么使得好的上下文示例适用于 gpt-$3$？ *arXiv 预印本 arXiv:2101.06804*。
- en: 'Lu et al. (2021) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them:
    Overcoming few-shot prompt order sensitivity. *arXiv preprint arXiv:2104.08786*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2021） Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel 和 Pontus Stenetorp。2021。神奇排序的提示及其发现：克服少样本提示排序敏感性。*arXiv
    预印本 arXiv:2104.08786*。
- en: Lundberg and Lee (2017) Scott M Lundberg and Su-In Lee. 2017. A unified approach
    to interpreting model predictions. *Advances in neural information processing
    systems*, 30.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg 和 Lee（2017） Scott M Lundberg 和 Su-In Lee。2017。解释模型预测的统一方法。*神经信息处理系统进展*，30。
- en: Morris et al. (2023) John X Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and
    Alexander M Rush. 2023. Text embeddings reveal (almost) as much as text. *arXiv
    preprint arXiv:2310.06816*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morris 等（2023） John X Morris, Volodymyr Kuleshov, Vitaly Shmatikov 和 Alexander
    M Rush。2023。文本嵌入揭示（几乎）与文本一样多的信息。*arXiv 预印本 arXiv:2310.06816*。
- en: 'Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation
    with language models. *arXiv preprint arXiv:2112.00114*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nye 等（2021） Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski,
    Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
    Luan 等。2021。展示你的工作：用于中间计算的语言模型草稿板。*arXiv 预印本 arXiv:2112.00114*。
- en: Perez et al. (2021) Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True
    few-shot learning with language models. *Advances in neural information processing
    systems*, 34:11054–11070.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 等（2021） Ethan Perez, Douwe Kiela 和 Kyunghyun Cho。2021。使用语言模型进行真正的少样本学习。*神经信息处理系统进展*，34:11054–11070。
- en: Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016. "why should i trust you?" explaining the predictions of any classifier.
    In *Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery
    and data mining*, pages 1135–1144.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, 和 Carlos Guestrin.
    2016. “我为什么应该相信你？”解释任何分类器的预测。见于 *第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，第1135–1144页。
- en: Rubin et al. (2021) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021.
    Learning to retrieve prompts for in-context learning. *arXiv preprint arXiv:2112.08633*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin et al. (2021) Ohad Rubin, Jonathan Herzig, 和 Jonathan Berant. 2021. 学习检索上下文学习的提示。*arXiv
    预印本 arXiv:2112.08633*。
- en: 'Schick and Schütze (2020) Timo Schick and Hinrich Schütze. 2020. It’s not just
    size that matters: Small language models are also few-shot learners. *arXiv preprint
    arXiv:2009.07118*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick and Schütze (2020) Timo Schick 和 Hinrich Schütze. 2020. 不是只有规模才重要：小型语言模型也是少样本学习者。*arXiv
    预印本 arXiv:2009.07118*。
- en: Schick and Schütze (2022) Timo Schick and Hinrich Schütze. 2022. True few-shot
    learning with prompts—a real-world perspective. *Transactions of the Association
    for Computational Linguistics*, 10:716–731.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick and Schütze (2022) Timo Schick 和 Hinrich Schütze. 2022. 真正的少样本学习与提示——一个现实世界的视角。*计算语言学协会会刊*，10:716–731。
- en: Singh et al. (2024) Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich
    Caruana, and Jianfeng Gao. 2024. Rethinking interpretability in the era of large
    language models. *arXiv preprint arXiv:2402.01761*.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. (2024) Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich
    Caruana, 和 Jianfeng Gao. 2024. 在大型语言模型时代重新思考可解释性。*arXiv 预印本 arXiv:2402.01761*。
- en: 'Stubbs et al. (2019) Amber Stubbs, Michele Filannino, Ergin Soysal, Samuel
    Henry, and Özlem Uzuner. 2019. [Cohort selection for clinical trials: n2c2 2018
    shared task track 1](https://doi.org/10.1093/JAMIA/OCZ163). *Journal of the American
    Medical Informatics Association : JAMIA*, 26:1163.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stubbs et al. (2019) Amber Stubbs, Michele Filannino, Ergin Soysal, Samuel Henry,
    和 Özlem Uzuner. 2019. [临床试验中的队列选择：n2c2 2018共享任务第1轨](https://doi.org/10.1093/JAMIA/OCZ163)。*美国医学信息学协会期刊：JAMIA*，26:1163。
- en: Sundararajan et al. (2017) Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.
    Axiomatic attribution for deep networks. In *Proceedings of the 34th International
    Conference on Machine Learning*, volume 70, pages 3319–3328\. PMLR.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sundararajan et al. (2017) Mukund Sundararajan, Ankur Taly, 和 Qiqi Yan. 2017.
    深度网络的公理归因。见于 *第34届国际机器学习大会论文集*，第70卷，第3319–3328页。PMLR。
- en: Trautmann (2023) Dietrich Trautmann. 2023. Large language model prompt chaining
    for long legal document classification. *arXiv preprint arXiv:2308.04138*.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trautmann (2023) Dietrich Trautmann. 2023. 用于长篇法律文档分类的大型语言模型提示链。*arXiv 预印本 arXiv:2308.04138*。
- en: Wang et al. (2022a) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2022a. 自我一致性提高了语言模型中的思维链推理。*arXiv
    预印本 arXiv:2203.11171*。
- en: 'Wang et al. (2022b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022b. Self-instruct:
    Aligning language models with self-generated instructions. *arXiv preprint arXiv:2212.10560*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, 和 Hannaneh Hajishirzi. 2022b. Self-instruct：使语言模型与自生成指令对齐。*arXiv
    预印本 arXiv:2212.10560*。
- en: 'Webersinke et al. (2021) Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler,
    and Markus Leippold. 2021. Climatebert: A pretrained language model for climate-related
    text. *arXiv preprint arXiv:2110.12010*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Webersinke et al. (2021) Nicolas Webersinke, Mathias Kraus, Julia Anna Bingler,
    和 Markus Leippold. 2021. Climatebert：一个用于气候相关文本的预训练语言模型。*arXiv 预印本 arXiv:2110.12010*。
- en: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. 2022a. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    等等。2022a. 大型语言模型的涌现能力。*arXiv 预印本 arXiv:2206.07682*。
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022b. Chain-of-thought prompting
    elicits reasoning in large language models. In *Advances in Neural Information
    Processing Systems*, volume 35, pages 24824–24837\. Curran Associates, Inc.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc V Le, 和 Denny Zhou. 2022b. Chain-of-thought prompting
    在大型语言模型中引发推理。见于 *Advances in Neural Information Processing Systems*，第35卷，第24824–24837页。Curran
    Associates, Inc.
- en: Wei et al. (2022c) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022c. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022c) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 2022c. 思维链提示引发大型语言模型中的推理。*神经信息处理系统进展*,
    35:24824–24837。
- en: 'Wu et al. (2022a) Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022a.
    Ai chains: Transparent and controllable human-ai interaction by chaining large
    language model prompts. In *Proceedings of the 2022 CHI conference on human factors
    in computing systems*, pages 1–22.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2022a) Tongshuang Wu, Michael Terry, 和 Carrie Jun Cai. 2022a. Ai
    链：通过链式大型语言模型提示实现透明和可控的人机交互。见于 *2022年计算机系统人因学会议论文集*，第 1–22 页。
- en: 'Wu et al. (2022b) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.
    2022b. Self-adaptive in-context learning: An information compression perspective
    for in-context example selection and ordering. *arXiv preprint arXiv:2212.10375*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2022b) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, 和 Lingpeng Kong. 2022b.
    自适应上下文学习：从信息压缩角度看上下文示例选择和排序。*arXiv 预印本 arXiv:2212.10375*。
- en: Xiong et al. (2023) Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian
    He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation
    of confidence elicitation in llms. *arXiv preprint arXiv:2306.13063*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong et al. (2023) Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian
    He, 和 Bryan Hooi. 2023. 大型语言模型能表达它们的不确定性吗？对大型语言模型中信心引发的实证评估。*arXiv 预印本 arXiv:2306.13063*。
- en: 'Yang et al. (2024) Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. 2024. Gpt4tools: Teaching large language model to use tools
    via self-instruction. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2024) Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, 和 Ying Shan. 2024. Gpt4tools：通过自我指导教大型语言模型使用工具。*神经信息处理系统进展*, 36。
- en: Ye and Durrett (2022) Xi Ye and Greg Durrett. 2022. The unreliability of explanations
    in few-shot prompting for textual reasoning. *Advances in neural information processing
    systems*, 35:30378–30392.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye and Durrett (2022) Xi Ye 和 Greg Durrett. 2022. 少样本提示中解释的不可靠性。*神经信息处理系统进展*,
    35:30378–30392。
- en: Yu et al. (2019) Bei Yu, Yingya Li, and Jun Wang. 2019. [Detecting causal language
    use in science findings](https://doi.org/10.18653/v1/D19-1473). In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 4664–4674, Hong Kong, China. Association for Computational Linguistics.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2019) Bei Yu, Yingya Li, 和 Jun Wang. 2019. [检测科学发现中的因果语言使用](https://doi.org/10.18653/v1/D19-1473)。见于
    *2019年自然语言处理经验方法大会暨第九届国际联合自然语言处理会议 (EMNLP-IJCNLP) 论文集*，第 4664–4674 页，中国香港。计算语言学协会。
- en: Zhang et al. (2022a) Yiming Zhang, Shi Feng, and Chenhao Tan. 2022a. Active
    example selection for in-context learning. *arXiv preprint arXiv:2211.04486*.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022a) Yiming Zhang, Shi Feng, 和 Chenhao Tan. 2022a. 上下文学习中的主动示例选择。*arXiv
    预印本 arXiv:2211.04486*。
- en: Zhang et al. (2022b) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022b.
    Automatic chain of thought prompting in large language models. *arXiv preprint
    arXiv:2210.03493*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022b) Zhuosheng Zhang, Aston Zhang, Mu Li, 和 Alex Smola. 2022b.
    大型语言模型中的自动思维链提示。*arXiv 预印本 arXiv:2210.03493*。
- en: 'Zhou et al. (2024) Kaitlyn Zhou, Jena D Hwang, Xiang Ren, and Maarten Sap.
    2024. Relying on the unreliable: The impact of language models’ reluctance to
    express uncertainty. *arXiv preprint arXiv:2401.06730*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2024) Kaitlyn Zhou, Jena D Hwang, Xiang Ren, 和 Maarten Sap. 2024.
    依赖于不可靠性：语言模型表达不确定性的影响。*arXiv 预印本 arXiv:2401.06730*。
- en: Zhou et al. (2022) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level
    prompt engineers. *arXiv preprint arXiv:2211.01910*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2022) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, 和 Jimmy Ba. 2022. 大型语言模型是人类级别的提示工程师。*arXiv 预印本 arXiv:2211.01910*。
- en: 'Zotova et al. (2020) Elena Zotova, Rodrigo Agerri, Manuel Nuñez, and German
    Rigau. 2020. Multilingual stance detection in tweets: The Catalonia independence
    corpus. In *Proceedings of the Twelfth Language Resources and Evaluation Conference*,
    pages 1368–1375, Marseille, France. European Language Resources Association.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zotova et al. (2020) Elena Zotova, Rodrigo Agerri, Manuel Nuñez, 和 German Rigau.
    2020. 推文中的多语言立场检测：加泰罗尼亚独立语料库。见于 *第十二届语言资源与评估会议论文集*，第 1368–1375 页，法国马赛。欧洲语言资源协会。
- en: Appendix A Questions used in ICE-T method
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A ICE-T 方法中使用的问题
- en: 'This is the list of questions used for each task in ICE-T technique. For each
    task, there are five questions in total, where the question with $id=0$ is used
    in a prompt in a zero-shot approach. Other questions are secondary questions obtained
    as explained in Section [3.1](#S3.SS1 "3.1 Generating questions ‣ 3 Method ‣ Interpretable
    Cross-Examination Technique (ICE-T): Using highly informative features to boost
    LLM performance"):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于ICE-T技术每个任务的问卷列表。每个任务共有五个问题，其中$ id=0 $的问题用于零样本方法的提示。其他问题为次要问题，具体说明见第[3.1节](#S3.SS1
    "3.1 生成问题 ‣ 3 方法 ‣ 可解释的交叉审查技术（ICE-T）：利用高信息量特征提升LLM表现")。
- en: DRUG-ABUSE
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DRUG-ABUSE
- en: '0'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Is there any indication in the patient’s records of current or past drug abuse?
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者的记录中是否有当前或过去药物滥用的迹象？
- en: '1'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Has the patient been prescribed medication with a high potential for abuse?
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否被开具了具有高滥用潜力的药物？
- en: '2'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Are there any notes in the patient’s records indicating substance abuse or dependency
    issues?
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者的记录中是否有表明药物滥用或依赖问题的备注？
- en: '3'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Has the patient previously sought treatment for substance abuse or addiction?
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否曾因药物滥用或成瘾问题寻求过治疗？
- en: '4'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Are there any irregularities in the patient’s prescription history that suggest
    misuse, such as early refill requests?
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者的处方记录中是否有任何异常迹象，显示可能存在误用，如提前请求补药？
- en: ALCOHOL-ABUSE
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ALCOHOL-ABUSE
- en: '0'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Has the patient consumed alcohol beyond the weekly recommended limits recently?
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者最近是否有过量饮酒，超过了每周推荐的限制？
- en: '1'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Did the patient consume more than 14 units of alcohol in the past week?
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者在过去一周内是否消费了超过14单位的酒精？
- en: '2'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Has the patient engaged in binge drinking sessions in the last month?
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否在过去一个月内有过暴饮暴食的情况？
- en: '3'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Does the patient frequently consume alcohol on more than 5 days in a week?
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否经常在一周中的5天以上饮酒？
- en: '4'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Has the patient expressed concerns about controlling their alcohol intake recently?
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者最近是否表达过控制饮酒的担忧？
- en: ENGLISH
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ENGLISH
- en: '0'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Does the patient speak English, according to their medical records?
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据医疗记录，患者是否会说英语？
- en: '1'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Is there a language preference indicated in the patient’s medical records?
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗记录中是否标明了语言偏好？
- en: '2'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Does the medical record include an interpreter request for non-English languages?
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗记录中是否包括了非英语语言的翻译需求？
- en: '3'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Is English listed as the patient’s primary language in the medical records?
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗记录中是否将英语列为患者的主要语言？
- en: '4'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Have previous medical consultations been conducted in English, as noted in the
    patient’s records?
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有记录显示以英语进行的之前的医疗咨询？
- en: MAKES-DECISIONS
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MAKES-DECISIONS
- en: '0'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Is there evidence that the patient makes their own medical decisions?
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有证据表明患者自己做出医疗决策？
- en: '1'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Does the patient have a documented history of discussing treatment options with
    their healthcare provider?
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否有记录显示曾与其医疗提供者讨论治疗选项？
- en: '2'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Has the patient previously filled out an advanced directive or a living will?
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否曾填写过预立医疗指示或生前遗嘱？
- en: '3'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Does the patient regularly attend medical appointments alone?
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否经常独自参加医疗预约？
- en: '4'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Have there been instances where the patient explicitly expressed their treatment
    preferences or declined certain medical interventions?
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否明确表达过他们的治疗偏好或拒绝过某些医疗干预？
- en: ABDOMINAL
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ABDOMINAL
- en: '0'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Is there a record of the patient undergoing intra-abdominal surgery, small or
    large intestine resection, or experiencing a small bowel obstruction?
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有记录表明患者接受过腹腔手术、小肠或大肠切除，或经历过小肠梗阻？
- en: '1'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Has the patient ever undergone any form of intra-abdominal surgery?
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否曾经接受过任何形式的腹腔内手术？
- en: '2'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Has the patient had a resection of either the small or large intestine?
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否接受过小肠或大肠的切除手术？
- en: '3'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Is there a record of the patient experiencing a small bowel obstruction?
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有记录表明患者曾经历过小肠梗阻？
- en: '4'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Has the patient had any surgical intervention related to the digestive system
    that is not explicitly mentioned above?
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否进行过未在上述明确提及的与消化系统相关的手术干预？
- en: MAJOR-DIABETES
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MAJOR-DIABETES
- en: '0'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Does the patient have any major diabetes-related complications such as amputation,
    kidney damage, skin conditions, retinopathy, nephropathy, or neuropathy?
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否有任何主要的糖尿病相关并发症，如截肢、肾损伤、皮肤病、视网膜病、肾病或神经病？
- en: '1'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Has the patient experienced any significant changes in their vision or been
    diagnosed with retinopathy?
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否经历过视力的重大变化或被诊断为视网膜病？
- en: '2'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Is there any history of skin conditions, wounds that heal poorly, or any amputations?
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有皮肤病、伤口愈合不良或任何截肢的历史？
- en: '3'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Does the patient have a history of kidney damage or been diagnosed with nephropathy?
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否有肾损伤的历史或被诊断为肾病？
- en: '4'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Has the patient reported any persistent numbness, pain, or tingling in their
    extremities indicating neuropathy?
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否报告过持续的麻木、疼痛或四肢刺痛，这可能表明神经病变？
- en: ADVANCED-CAD
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**ADVANCED-CAD**'
- en: '0'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Is the patient currently taking two or more medications to treat CAD?
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否当前正在服用两种或更多药物来治疗 CAD？
- en: '1'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Is the patient currently being treated for coronary artery disease (CAD)?
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否正在接受冠状动脉疾病（CAD）的治疗？
- en: '2'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Is the patient taking any medication to manage CAD symptoms?
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否在服用任何药物以管理 CAD 症状？
- en: '3'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Is the patient prescribed more than one medication specifically for CAD?
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否为冠心病（CAD）处方了多种药物？
- en: '4'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Are the medications the patient is taking to treat CAD being taken concurrently?
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者治疗 CAD 的药物是否同时服用？
- en: MI-6MOS
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**MI-6MOS**'
- en: '0'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Has the patient had a myocardial infarction within the past 6 months from the
    most recent record date?
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从最近的记录日期起，患者是否在过去 6 个月内有过心肌梗死？
- en: '1'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Has the patient reported any chest pain or symptoms consistent with a myocardial
    infarction in the past 6 months?
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否报告过过去 6 个月内的胸痛或与心肌梗死一致的症状？
- en: '2'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Has the patient undergone any cardiac diagnostic tests, such as an ECG or troponin
    levels, in the past 6 months?
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否在过去 6 个月内接受过任何心脏诊断测试，如 ECG 或肌钙蛋白水平？
- en: '3'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Did the patient receive any treatment specifically for myocardial infarction,
    such as medication, stenting, or bypass surgery, in the past 6 months?
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者在过去 6 个月内是否接受过针对心肌梗死的治疗，如药物、支架植入或旁路手术？
- en: '4'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Is there any notation in the patient’s medical records of a confirmed diagnosis
    of myocardial infarction within the past 6 months?
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者的病历中是否有确诊心肌梗死的记录，时间在过去 6 个月内？
- en: DIETSUPP-2MOS
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**饮食补充剂-2个月**'
- en: '0'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Has the patient taken any dietary supplements, excluding Vitamin D, in the past
    2 months?
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者在过去 2 个月内是否服用过任何饮食补充剂（不包括维生素 D）？
- en: '1'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Have you taken any vitamins or dietary supplements in the past 2 months, besides
    Vitamin D?
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了维生素 D 之外，您在过去 2 个月内是否服用过任何维生素或饮食补充剂？
- en: '2'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Are there any non-prescription supplements you have consumed regularly in the
    last 60 days?
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在过去 60 天内，您是否有定期消费任何非处方补充剂？
- en: '3'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Did you start or continue taking any herbal or nutritional supplements recently,
    except Vitamin D?
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近是否开始或继续服用任何草药或营养补充剂（除维生素 D 外）？
- en: '4'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Excluding Vitamin D, have you used any health or wellness supplements since
    two months ago?
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了维生素 D 之外，您自两个月前是否使用过任何健康或保健补充剂？
- en: ASP-FOR-MI
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**ASP-FOR-MI**'
- en: '0'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Is the patient using aspirin to prevent myocardial infarction based on their
    medical records?
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据病历，患者是否使用阿司匹林来预防心肌梗死？
- en: '1'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Is there a history of cardiovascular disease in the patient’s medical records?
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者的病历中是否有心血管疾病的历史？
- en: '2'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Has the patient been prescribed aspirin for long-term use?
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者是否被处方长期使用阿司匹林？
- en: '3'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Do the medical records indicate a doctor’s recommendation for aspirin to prevent
    myocardial infarction?
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗记录中是否提到医生推荐使用阿司匹林来预防心肌梗死？
- en: '4'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Is there any mention of aspirin under the patient’s current medications list?
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者当前的药物清单中是否提到阿司匹林？
- en: HBA1C
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**HbA1C**'
- en: '0'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Have any HbA1c test results been listed in the records with a value between
    6.5 and 9.5
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有任何 HbA1c 测试结果的值在 6.5 到 9.5 之间列入记录中？
- en: '1'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Are there any HbA1c test results listed in the records?
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记录中是否列出了任何 HbA1c 测试结果？
- en: '2'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Do any HbA1c test results fall within the 6.5 to 9.5
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有任何 HbA1c 测试结果落在 6.5 到 9.5 之间？
- en: '3'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Is the HbA1c value specifically mentioned for each test result?
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有针对每次测试结果具体提到 HbA1c 值？
- en: '4'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Have all the HbA1c test results been recorded and updated in the records?
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否所有的 HbA1c 测试结果都已记录并更新在记录中？
- en: CREATININE
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**肌酐**'
- en: '0'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Is there any indication in the patient’s records of serum creatinine levels
    above the upper limit of normal?
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者记录中的血清肌酐水平是否高于正常上限？
- en: '1'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Has the patient undergone any recent serum creatinine tests?
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者最近是否进行过血清肌酐测试？
- en: '2'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Are the results of the patient’s serum creatinine tests documented in their
    medical records?
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 患者的血清肌酐测试结果是否记录在病历中？
- en: '3'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Do the documented serum creatinine levels exceed the established normal range?
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记录的血清肌酐水平是否超出正常范围？
- en: '4'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Has there been any physician commentary or notes indicating concern over the
    patient’s serum creatinine levels?
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有医生评论或备注表明对患者血清肌酐水平的担忧？
- en: UNFAIR-ToS
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: UNFAIR-ToS
- en: '0'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Does this sentence from company’s Terms of Service violate the European Council
    Directive on unfair terms in consumer contracts?
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这句来自公司服务条款的句子是否违反了欧洲委员会关于消费者合同中不公平条款的指令？
- en: '1'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Does the sentence create a significant imbalance between the parties’ rights
    and obligations to the detriment of the consumer?
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否在权利和义务方面对消费者造成了显著的不平衡？
- en: '2'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Is the term clearly understandable and transparent to a typical consumer?
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 术语是否对典型消费者来说明确易懂？
- en: '3'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Does the term determine the main subject matter of the contract, or the appropriateness
    of the price or remuneration, in a way that is unfair to the consumer?
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 术语是否以对消费者不公平的方式确定了合同的主要内容，或价格或报酬的适当性？
- en: '4'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Has the consumer been given an opportunity to negotiate the terms of the contract?
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 消费者是否有机会协商合同条款？
- en: ECtHR
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ECtHR
- en: '0'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Considering all the facts, did a state breach human rights provisions of the
    European Convention of Human Rights?
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合所有事实，国家是否违反了《欧洲人权公约》的人权条款？
- en: '1'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Did the state fail to provide adequate protection to an individual or group,
    thereby violating Article 2 or 3 concerning the right to life and prohibition
    of torture?
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 国家是否未能为个人或群体提供足够的保护，从而违反了第2或第3条关于生命权和禁止酷刑的规定？
- en: '2'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Was there any discrimination in the state’s actions or laws that contravenes
    Article 14 or Protocol 12 regarding the prohibition of discrimination?
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 国家在行动或法律上是否存在违反第14条或第12号附加议定书关于禁止歧视的规定的歧视行为？
- en: '3'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Did the state unlawfully interfere with privacy, family life, freedom of expression,
    or freedom of assembly and association, as protected under Articles 8 to 11?
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 国家是否非法干涉了隐私、家庭生活、言论自由或集会和结社自由，这些权利受到第8至第11条的保护？
- en: '4'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Was there a failure by the state to ensure a fair trial or access to a court,
    in violation of Article 6 or 13, concerning the right to a fair trial and the
    right to an effective remedy?
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 国家是否未能确保公正审判或获得法院的机会，违反了第6条或第13条关于公正审判和有效救济权利的规定？
- en: Climate detection
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 气候检测
- en: '0'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Is a given paragraph climate-related?
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给定的段落是否与气候相关？
- en: '1'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Does the paragraph discuss weather, temperature trends, or climate patterns?
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落是否讨论天气、温度趋势或气候模式？
- en: '2'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Does the paragraph mention the impact of human activity on the environment?
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落是否提到人类活动对环境的影响？
- en: '3'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Does the paragraph reference scientific studies or data on climate change?
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落是否引用了关于气候变化的科学研究或数据？
- en: '4'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Does the paragraph address policy or actions taken to mitigate climate impacts?
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落是否涉及减缓气候影响的政策或行动？
- en: Health advice
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 健康建议
- en: '0'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Does the given sentence contain a medical advice?
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给定的句子是否包含医学建议？
- en: '1'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Does the sentence recommend or suggest a specific medication, treatment, or
    remedy?
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否推荐或建议特定的药物、治疗或疗法？
- en: '2'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Does the sentence advise on health behaviors, such as diet, exercise, or sleep
    patterns?
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否建议健康行为，如饮食、运动或睡眠模式？
- en: '3'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Does the sentence include any wording that implies or directly states a medical
    diagnosis or prognosis?
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否包含任何暗示或直接陈述医学诊断或预后的措辞？
- en: '4'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Does the sentence suggest consulting a healthcare professional or seeking medical
    attention?
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否建议咨询医疗专业人士或寻求医疗帮助？
- en: Catalonia indep.
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加泰罗尼亚独立
- en: '0'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '0'
- en: Does the given sentence speak in favor of Catalonia independence?
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给定的句子是否支持加泰罗尼亚独立？
- en: '1'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1'
- en: Does the sentence express a positive opinion about Catalonia’s political autonomy?
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否对加泰罗尼亚的政治自治表达了积极的看法？
- en: '2'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2'
- en: Does the sentence criticize the Spanish government’s policies towards Catalonia?
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否批评了西班牙政府对加泰罗尼亚的政策？
- en: '3'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3'
- en: Does the sentence highlight benefits or advantages of Catalonia being independent?
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否强调加泰罗尼亚独立的好处或优势？
- en: '4'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4'
- en: Does the sentence encourage actions or steps towards achieving independence
    for Catalonia?
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子是否鼓励采取行动或步骤以实现加泰罗尼亚的独立？
- en: Appendix B Additional results
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 额外结果
- en: This is the appendix section where additional results are provided.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这是提供额外结果的附录部分。
- en: '|  | $\mu F1$ | $MF1$ | $wF1$ | Sample size |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu F1$ | $MF1$ | $wF1$ | 样本大小 |'
- en: '|  | 0-shot | ICE-T | 0-shot | ICE-T | 0-shot | ICE-T | Train | Test |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|  | 0-shot | ICE-T | 0-shot | ICE-T | 0-shot | ICE-T | Train | Test |'
- en: '| ABDOMINAL | 0.791 | 0.814 | 0.740 | 0.774 | 0.775 | 0.803 | 202 | 86 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| ABDOMINAL | 0.791 | 0.814 | 0.740 | 0.774 | 0.775 | 0.803 | 202 | 86 |'
- en: '| ADVANCED-CAD | 0.640 | 0.756 | 0.593 | 0.743 | 0.600 | 0.746 | 202 | 86 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| ADVANCED-CAD | 0.640 | 0.756 | 0.593 | 0.743 | 0.600 | 0.746 | 202 | 86 |'
- en: '| ALCOHOL-ABUSE | 0.814 | 0.965 | 0.583 | 0.491 | 0.872 | 0.948 | 202 | 86
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| ALCOHOL-ABUSE | 0.814 | 0.965 | 0.583 | 0.491 | 0.872 | 0.948 | 202 | 86
    |'
- en: '| ASP-FOR-MI | 0.849 | 0.86 | 0.73 | 0.728 | 0.834 | 0.838 | 202 | 86 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| ASP-FOR-MI | 0.849 | 0.86 | 0.73 | 0.728 | 0.834 | 0.838 | 202 | 86 |'
- en: '| CREATININE | 0.349 | 0.721 | 0.319 | 0.419 | 0.256 | 0.604 | 202 | 86 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| CREATININE | 0.349 | 0.721 | 0.319 | 0.419 | 0.256 | 0.604 | 202 | 86 |'
- en: '| DIETSUPP-2MOS | 0.593 | 0.767 | 0.562 | 0.765 | 0.564 | 0.766 | 202 | 86
    |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| DIETSUPP-2MOS | 0.593 | 0.767 | 0.562 | 0.765 | 0.564 | 0.766 | 202 | 86
    |'
- en: '| DRUG-ABUSE | 0.942 | 0.977 | 0.757 | 0.827 | 0.954 | 0.977 | 202 | 86 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| DRUG-ABUSE | 0.942 | 0.977 | 0.757 | 0.827 | 0.954 | 0.977 | 202 | 86 |'
- en: '| ENGLISH | 0.919 | 0.988 | 0.810 | 0.978 | 0.910 | 0.989 | 202 | 86 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| ENGLISH | 0.919 | 0.988 | 0.810 | 0.978 | 0.910 | 0.989 | 202 | 86 |'
- en: '| HBA1C | 0.477 | 0.837 | 0.410 | 0.834 | 0.373 | 0.838 | 202 | 86 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| HBA1C | 0.477 | 0.837 | 0.410 | 0.834 | 0.373 | 0.838 | 202 | 86 |'
- en: '| MAJOR-DIABETES | 0.733 | 0.814 | 0.728 | 0.814 | 0.728 | 0.814 | 202 | 86
    |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| MAJOR-DIABETES | 0.733 | 0.814 | 0.728 | 0.814 | 0.728 | 0.814 | 202 | 86
    |'
- en: '| MAKES-DECISIONS | 0.605 | 0.965 | 0.426 | 0.491 | 0.724 | 0.948 | 202 | 86
    |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| MAKES-DECISIONS | 0.605 | 0.965 | 0.426 | 0.491 | 0.724 | 0.948 | 202 | 86
    |'
- en: '| MI-6MOS | 0.651 | 0.919 | 0.542 | 0.709 | 0.724 | 0.91 | 202 | 86 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| MI-6MOS | 0.651 | 0.919 | 0.542 | 0.709 | 0.724 | 0.91 | 202 | 86 |'
- en: '| Catalonia indep. | 0.528 | 0.579 | 0.414 | 0.536 | 0.413 | 0.536 | 2961 |
    1145 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| Catalonia indep. | 0.528 | 0.579 | 0.414 | 0.536 | 0.413 | 0.536 | 2961 |
    1145 |'
- en: '| Climate detection | 0.702 | 0.8 | 0.673 | 0.444 | 0.732 | 0.711 | 1300 |
    400 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| Climate detection | 0.702 | 0.8 | 0.673 | 0.444 | 0.732 | 0.711 | 1300 |
    400 |'
- en: '| ECtHR | 0.853 | 0.873 | 0.65 | 0.466 | 0.849 | 0.814 | 2384 | 591 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| ECtHR | 0.853 | 0.873 | 0.65 | 0.466 | 0.849 | 0.814 | 2384 | 591 |'
- en: '| Health advice | 0.836 | 0.841 | 0.778 | 0.787 | 0.830 | 0.835 | 3470 | 868
    |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| Health advice | 0.836 | 0.841 | 0.778 | 0.787 | 0.830 | 0.835 | 3470 | 868
    |'
- en: '| UNFAIR-ToS | 0.335 | 0.887 | 0.321 | 0.47 | 0.397 | 0.834 | 3319 | 964 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| UNFAIR-ToS | 0.335 | 0.887 | 0.321 | 0.47 | 0.397 | 0.834 | 3319 | 964 |'
- en: '| $\mu F1$ - Micro F1, $MF1$ - Macro F1, $wF1$ - Weighted F1 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| $\mu F1$ - 微观 F1, $MF1$ - 宏观 F1, $wF1$ - 加权 F1 |'
- en: 'Table 2: Evaluation of ICE-T and Zero-Shot Techniques on GPT-3.5 - This table
    presents a comparison of F1 micro, macro, and weighted scores for various classification
    tasks using GPT-3.5\. It compares the performance metrics between the zero-shot
    approach and the ICE-T technique. Additionally, the table includes sample sizes
    for both training and testing datasets across each task. Notable improvements
    can be seen in several tasks when utilizing the ICE-T technique over the zero-shot
    method.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：ICE-T 和零样本技术在 GPT-3.5 上的评估 - 本表展示了使用 GPT-3.5 进行的各种分类任务的 F1 微观、宏观和加权分数的比较。它比较了零样本方法和
    ICE-T 技术的性能指标。此外，表格还包括每个任务的训练和测试数据集的样本大小。当使用 ICE-T 技术时，多个任务的性能得到了显著改善。
- en: '|  | $\mu F1$ | $MF1$ | $wF1$ | Sample size |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu F1$ | $MF1$ | $wF1$ | 样本大小 |'
- en: '|  | 0-shot | ICE-T | 0-shot | ICE-T | 0-shot | ICE-T | Train | Test |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | 0-shot | ICE-T | 0-shot | ICE-T | 0-shot | ICE-T | 训练 | 测试 |'
- en: '| ABDOMINAL | 0.802 | 0.884 | 0.757 | 0.876 | 0.789 | 0.885 | 202 | 86 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| ABDOMINAL | 0.802 | 0.884 | 0.757 | 0.876 | 0.789 | 0.885 | 202 | 86 |'
- en: '| ADVANCED-CAD | 0.791 | 0.907 | 0.785 | 0.906 | 0.787 | 0.906 | 202 | 86 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| ADVANCED-CAD | 0.791 | 0.907 | 0.785 | 0.906 | 0.787 | 0.906 | 202 | 86 |'
- en: '| ALCOHOL-ABUSE | 0.791 | 0.965 | 0.564 | 0.691 | 0.856 | 0.962 | 202 | 86
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| ALCOHOL-ABUSE | 0.791 | 0.965 | 0.564 | 0.691 | 0.856 | 0.962 | 202 | 86
    |'
- en: '| ASP-FOR-MI | 0.860 | 0.895 | 0.770 | 0.802 | 0.854 | 0.881 | 202 | 86 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| ASP-FOR-MI | 0.860 | 0.895 | 0.770 | 0.802 | 0.854 | 0.881 | 202 | 86 |'
- en: '| CREATININE | 0.488 | 0.872 | 0.487 | 0.85 | 0.477 | 0.875 | 202 | 86 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| CREATININE | 0.488 | 0.872 | 0.487 | 0.85 | 0.477 | 0.875 | 202 | 86 |'
- en: '| DIETSUPP-2MOS | 0.488 | 0.814 | 0.328 | 0.814 | 0.336 | 0.814 | 202 | 86
    |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| DIETSUPP-2MOS | 0.488 | 0.814 | 0.328 | 0.814 | 0.336 | 0.814 | 202 | 86
    |'
- en: '| DRUG-ABUSE | 0.826 | 0.977 | 0.593 | 0.744 | 0.879 | 0.971 | 202 | 86 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| DRUG-ABUSE | 0.826 | 0.977 | 0.593 | 0.744 | 0.879 | 0.971 | 202 | 86 |'
- en: '| ENGLISH | 0.233 | 0.965 | 0.231 | 0.925 | 0.206 | 0.963 | 202 | 86 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| ENGLISH | 0.233 | 0.965 | 0.231 | 0.925 | 0.206 | 0.963 | 202 | 86 |'
- en: '| HBA1C | 0.523 | 0.942 | 0.479 | 0.941 | 0.451 | 0.942 | 202 | 86 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| HBA1C | 0.523 | 0.942 | 0.479 | 0.941 | 0.451 | 0.942 | 202 | 86 |'
- en: '| MAJOR-DIABETES | 0.570 | 0.884 | 0.523 | 0.884 | 0.523 | 0.884 | 202 | 86
    |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| MAJOR-DIABETES | 0.570 | 0.884 | 0.523 | 0.884 | 0.523 | 0.884 | 202 | 86
    |'
- en: '| MAKES-DECISIONS | 0.663 | 0.965 | 0.456 | 0.691 | 0.768 | 0.962 | 202 | 86
    |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| MAKES-DECISIONS | 0.663 | 0.965 | 0.456 | 0.691 | 0.768 | 0.962 | 202 | 86
    |'
- en: '| MI-6MOS | 0.849 | 0.953 | 0.674 | 0.844 | 0.868 | 0.95 | 202 | 86 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| MI-6MOS | 0.849 | 0.953 | 0.674 | 0.844 | 0.868 | 0.95 | 202 | 86 |'
- en: '| Catalonia indep. | 0.562 | 0.604 | 0.462 | 0.57 | 0.463 | 0.57 | 2961 | 1145
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| Catalonia indep. | 0.562 | 0.604 | 0.462 | 0.57 | 0.463 | 0.57 | 2961 | 1145
    |'
- en: '| Climate detection | 0.912 | 0.925 | 0.878 | 0.896 | 0.917 | 0.929 | 1300
    | 400 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 气候检测 | 0.912 | 0.925 | 0.878 | 0.896 | 0.917 | 0.929 | 1300 | 400 |'
- en: '| ECtHR | 0.861 | 0.873 | 0.631 | 0.466 | 0.848 | 0.814 | 2384 | 591 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 欧洲人权法院 | 0.861 | 0.873 | 0.631 | 0.466 | 0.848 | 0.814 | 2384 | 591 |'
- en: '| Health advice | 0.846 | 0.846 | 0.766 | 0.766 | 0.828 | 0.828 | 3470 | 868
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 健康建议 | 0.846 | 0.846 | 0.766 | 0.766 | 0.828 | 0.828 | 3470 | 868 |'
- en: '| Unfair TOS | 0.837 | 0.889 | 0.63 | 0.489 | 0.844 | 0.839 | 3319 | 964 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 不公平TOS | 0.837 | 0.889 | 0.63 | 0.489 | 0.844 | 0.839 | 3319 | 964 |'
- en: '| $\mu F1$ - Micro F1, $MF1$ - Macro F1, $wF1$ - Weighted F1 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| $\mu F1$ - 微F1, $MF1$ - 宏F1, $wF1$ - 加权F1 |'
- en: 'Table 3: Evaluation of ICE-T and Zero-Shot Techniques on GPT-4 - This table
    presents a comparison of F1 micro, macro, and weighted scores for various classification
    tasks using GPT-4\. It compares the performance metrics between the zero-shot
    approach and the ICE-T technique. Additionally, the table includes sample sizes
    for both training and testing datasets across each task. Notable improvements
    can be seen in several tasks when utilizing the ICE-T technique over the zero-shot
    method.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：GPT-4上ICE-T与零-shot技术的评估 - 此表展示了使用GPT-4进行各种分类任务的F1微观、宏观和加权得分的比较。它比较了零-shot方法和ICE-T技术之间的性能指标。此外，表中还包括了每个任务的训练和测试数据集的样本大小。使用ICE-T技术相较于零-shot方法在多个任务上可见显著的改善。
- en: '|  |  | $\mu F1$ | $MF1$ |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\mu F1$ | $MF1$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | 0-shot | ICE-T | 0-shot | ICE-T |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 0-shot | ICE-T | 0-shot | ICE-T |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Clinical Trial | GPT3.5 | 0.709 | 0.866 | 0.604 | 0.695 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 临床试验 | GPT3.5 | 0.709 | 0.866 | 0.604 | 0.695 |'
- en: '|  | GPT4 | 0.673 | 0.915 | 0.560 | 0.803 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT4 | 0.673 | 0.915 | 0.560 | 0.803 |'
- en: '| Other | GPT3.5 | 0.600 | 0.777 | 0.546 | 0.559 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | GPT3.5 | 0.600 | 0.777 | 0.546 | 0.559 |'
- en: '|  | GPT4 | 0.789 | 0.816 | 0.684 | 0.680 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT4 | 0.789 | 0.816 | 0.684 | 0.680 |'
- en: 'Table 4: Comparison of average performance between Zero-Shot and ICE-T methods
    across Clinical Trial and Other Task Group'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：临床试验与其他任务组中零-shot与ICE-T方法的平均性能比较
- en: '![Refer to caption](img/e12d0e193239644023a06b4d6a075c2f.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e12d0e193239644023a06b4d6a075c2f.png)'
- en: 'Figure 4: Task-Specific Sensitivity Analysis of Feature Count on $\mu F1$ Score.
    Detailed view of the changes in the $\mu F1$ score for individual tasks as the
    number of secondary questions increases. Each plot represents one of the 17 datasets
    analyzed, showing how the micro F1 score varies with the addition of features.
    The data underscores the variability in performance improvements across different
    tasks when using the Random Forest classifier.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：特征数量对$\mu F1$得分的任务特定敏感性分析。详细显示了在次级问题数量增加时，单个任务$\mu F1$得分的变化情况。每个图表表示分析的17个数据集中的一个，展示了在添加特征时微F1得分的变化。数据突显了使用随机森林分类器时，不同任务间性能提升的可变性。
