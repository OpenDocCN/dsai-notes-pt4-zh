- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 17:34:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:34:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于视觉的LLM预测的自动评估与GPT-4
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05680](https://ar5iv.labs.arxiv.org/html/2403.05680)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05680](https://ar5iv.labs.arxiv.org/html/2403.05680)
- en: '¹¹institutetext: National Center for Biotechnology Information, National Library
    of Medicine ²²institutetext: Imaging Biomarkers and Computer-Aided Diagnosis Laboratory,
    Clinical Center ³³institutetext: Center for Information Technology, National Institutes
    of Health ⁴⁴institutetext: King Abdullah University of Science & Technology ⁵⁵institutetext:
    Biomedical Image Analysis, Imperial College LondonQingqing Zhu¹ These authors
    contributed equally to this work    Benjamin Hou^(2,5)⁰    Tejas S. Mathai²   
    Pritam Mukherjee²    Qiao Jin¹    Xiuying Chen⁴    Zhizheng Wang¹    Ruida Cheng³
       Ronald M. Summers² Joint senior co-authors    Zhiyong Lu¹ ⁰'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构文本：国家生物技术信息中心，国家医学图书馆 ²²机构文本：影像生物标志物与计算机辅助诊断实验室，临床中心 ³³机构文本：信息技术中心，国家卫生研究院
    ⁴⁴机构文本：国王阿卜杜拉科技大学 ⁵⁵机构文本：生物医学图像分析，帝国理工学院Qingqing Zhu¹ 这些作者对这项工作贡献相同    Benjamin
    Hou^(2,5)⁰    Tejas S. Mathai²    Pritam Mukherjee²    Qiao Jin¹    Xiuying Chen⁴
       Zhizheng Wang¹    Ruida Cheng³    Ronald M. Summers² 共同高级合著者    Zhiyong Lu¹
    ⁰
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The volume of CT exams being done in the world has been rising every year, which
    has led to radiologist burn-out. Large Language Models (LLMs) have the potential
    to reduce their burden, but their adoption in the clinic depends on radiologist
    trust, and easy evaluation of generated content. Presently, many automated methods
    are available to evaluate the reports generated for chest radiographs, but such
    an approach is not available for CT presently. In this paper, we propose a novel
    evaluation framework to judge the capabilities of vision-language LLMs in generating
    accurate summaries of CT-based abnormalities. We input CT slices containing an
    abnormality (e.g., lesion) to a vision-based LLM (GPT-4V, LLaVA-Med, and RadFM),
    to generate a free-text summary of the predicted characteristics of the abnormality.
    Next, a GPT-4 model decomposed the summary into specific aspects (body part, location,
    type, and attributes), automatically evaluated the characteristics against the
    ground-truth, and generated a score for each aspect based on its clinical relevance
    and factual accuracy. These scores were then contrasted against those obtained
    from a clinician, and a high correlation ($\geq$ 85%, p $<$ .001) was observed.
    Although GPT-4V outperformed other models in our evaluation, it still requires
    overall improvement. Our evaluation method offers valuable insights into the specific
    areas that need the most enhancement, guiding future development in this field.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 全球的CT检查量每年都在上升，这导致了放射科医生的职业倦怠。大型语言模型（LLMs）有可能减轻他们的负担，但其在临床中的应用依赖于放射科医生的信任以及对生成内容的便捷评估。目前，许多自动化方法可以评估胸部X光片生成的报告，但目前尚无适用于CT的类似方法。在本文中，我们提出了一种新颖的评估框架，用于判断视觉-语言LLMs在生成准确的CT异常摘要方面的能力。我们将包含异常（例如病变）的CT切片输入视觉基础的LLM（GPT-4V、LLaVA-Med和RadFM），生成异常特征的自由文本摘要。接下来，GPT-4模型将摘要分解为具体方面（身体部位、位置、类型和属性），自动评估这些特征与实际情况的对比，并根据其临床相关性和事实准确性为每个方面生成一个评分。这些评分随后与临床医生获得的评分进行对比，观察到高相关性（$\geq$
    85%，p $<$ .001）。尽管GPT-4V在我们的评估中优于其他模型，但它仍然需要总体改进。我们的方法为未来在这一领域的开发提供了有价值的见解，指导了最需要改进的具体领域。
- en: 'Keywords:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Computer Tomography Deep Learning Automatic Evaluation Large Language Models
    GPT-4.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机断层扫描 深度学习 自动评估 大型语言模型 GPT-4。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In current clinical practice, a radiologist communicates the results of an imaging
    exam for a patient to their referring doctor through a signed report. While reading
    the patient exam, the radiologist uses Speech Recognition Software (SRS) that
    converts dictated speech into text. SRS is widely used and has significantly reduced
    the report turn-around time. However, any errors resulting from the dictation
    have to be corrected by the radiologists themselves, and persistent communication
    errors can negatively impact the interpretation of patient diagnoses and lead
    to medical malpractice suits [[1](#bib.bib1)]. These errors are most common for
    cross-sectional imaging [[2](#bib.bib2)], such as CT and MR, and the volume of
    these exams has steadily increased each year [[3](#bib.bib3)]. This has led to
    a 54-72% radiologist burn-out rate [[4](#bib.bib4)] where they are under increased
    pressure to deal with a substantially higher number of patients while maintaining
    a high level of accuracy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的临床实践中，放射科医生通过签署报告向患者的主治医生传达影像检查结果。在阅读患者检查时，放射科医生使用将口述语音转换为文本的语音识别软件（SRS）。SRS
    被广泛使用，并显著减少了报告的周转时间。然而，任何由口述引起的错误必须由放射科医生自行纠正，持续的沟通错误可能对患者诊断的解释产生负面影响，并导致医疗事故诉讼[[1](#bib.bib1)]。这些错误在横断面影像（如
    CT 和 MR）中最为常见，并且这些检查的数量每年稳步增加[[2](#bib.bib2)][[3](#bib.bib3)]。这导致了 54-72% 的放射科医生倦怠率[[4](#bib.bib4)]，他们在面对显著增加的患者数量时，承受着更大的压力，同时必须保持较高的准确性。
- en: To reduce their burden, a myriad number of transformer-based approaches have
    been proposed to generate radiology reports in one shot [[5](#bib.bib5), [6](#bib.bib6)].
    However, chest radiographs (CXR) have been the singular focus of these works with
    scant effort devoted to other modalities, such as CT [[7](#bib.bib7)]. The development
    of such a method for CT presents unique challenges, attributable to the intrinsic
    3D nature of CT data, computational complexity, and extensive reporting detail
    necessary for clinical applications. However, there have been recent advances
    with Large Language Models (LLMs), like GPT-4 [[8](#bib.bib8)], and vision-based
    LLMs, such as GPT-4 Vision (GPT-4V), LLaVA-Med [[9](#bib.bib9)], and Radiology
    Foundation Model (RadFM) [[10](#bib.bib10)]. These multimodal models have demonstrated
    their capabilities across several tasks, such as passing medical exams, medical
    note-taking and diagnosing diseases [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)].
    Therefore, they have immense potential to pre-fill the “findings” section of a
    radiology report with relevant information that a radiologist can quickly review
    [[14](#bib.bib14)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少他们的负担，已经提出了大量基于变换器的方法，以一键生成放射学报告[[5](#bib.bib5)][[6](#bib.bib6)]。然而，胸部 X
    光片（CXR）是这些研究的唯一重点，对于其他模态，如 CT [[7](#bib.bib7)]，则投入的精力较少。为 CT 开发这样的一个方法面临独特的挑战，这些挑战源于
    CT 数据的固有 3D 特性、计算复杂性以及临床应用所需的详细报告。然而，最近在大语言模型（LLMs）方面取得了进展，如 GPT-4 [[8](#bib.bib8)]，以及基于视觉的
    LLM，如 GPT-4 Vision（GPT-4V）、LLaVA-Med [[9](#bib.bib9)] 和放射学基础模型（RadFM）[[10](#bib.bib10)]。这些多模态模型在多个任务中展示了其能力，例如通过医学考试、医学笔记记录和疾病诊断[[11](#bib.bib11)][[12](#bib.bib12)][[13](#bib.bib13)]。因此，它们具有极大的潜力，可以用相关信息预填充放射学报告中的“发现”部分，供放射科医生快速审阅[[14](#bib.bib14)]。
- en: 'Despite these advances, crucial factors determining their clinical use involve:
    (1) radiologist trust, and (2) easy interpretation and evaluation of the generated
    content. Current evaluation metrics, including Natural Language Generation (NLG)
    and Clinical Efficacy (CE) metrics, are notoriously limited [[15](#bib.bib15),
    [14](#bib.bib14), [16](#bib.bib16), [17](#bib.bib17)] when it comes to capturing
    the semantic richness and clinical relevance necessary for radiology reports.
    Additionally, they lack the explanatory power that is required for clinical use.
    For CXR images, a variety of automated methods validating the clinical accuracy
    of reports have been established [[18](#bib.bib18), [19](#bib.bib19), [15](#bib.bib15)].
    But, an equivalent automated system for the validation of clinical accuracy in
    CT is notably absent. These limitations have prompted the exploration of LLM-based
    evaluation methods that promise a nuanced assessment aligned with human judgment
    [[20](#bib.bib20), [16](#bib.bib16)] and emphasize accuracy and relevance of generated
    content.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些进展，决定其临床使用的关键因素包括：(1) 放射科医生的信任，以及(2) 对生成内容的易解释和评估。目前的评估指标，包括自然语言生成（NLG）和临床效能（CE）指标，在捕捉放射学报告所需的语义丰富性和临床相关性方面存在著名的局限性[[15](#bib.bib15),
    [14](#bib.bib14), [16](#bib.bib16), [17](#bib.bib17)]。此外，它们缺乏临床使用所需的解释力。对于CXR图像，已经建立了多种验证报告临床准确性的自动化方法[[18](#bib.bib18),
    [19](#bib.bib19), [15](#bib.bib15)]。但，对于CT的临床准确性验证，显著缺乏等效的自动化系统。这些局限性促使了对LLM基于的方法的探索，这些方法承诺提供与人类判断一致的细致评估[[20](#bib.bib20),
    [16](#bib.bib16)]，并强调生成内容的准确性和相关性。
- en: In this paper, we present a novel evaluation framework that judges the ability
    of a vision-based LLM in generating diagnostically accurate findings, such that
    they can be pre-filled into the radiology report. CT slices containing an abnormality
    (e.g., lesion) were input to a vision-based LLM (e.g., GPT-4V), and it generated
    a free-text summary of the predicted characteristics of the abnormality. Next,
    a language-centric GPT-4 model decomposed the summary into specific aspects (body
    part, location, type, and attributes), automatically evaluated the characteristics
    against the ground-truth annotations, and generated a score for each aspect based
    on its clinical relevance and factual accuracy. These scores were then contrasted
    against those obtained from a clinician, and a high correlation ($\geq$ 85%, p
    $<$ .001) was observed. Our approach is unique in that it combines the expertise
    of radiologists with the Chain-of-Thought (COT) [[21](#bib.bib21)] reasoning from
    LLMs to evaluate the characteristics of abnormalities predicted by vision-based
    LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的评估框架，用于判断基于视觉的LLM生成的诊断准确性发现的能力，以便将其预填入放射学报告中。将包含异常（例如，病变）的CT切片输入到基于视觉的LLM（例如，GPT-4V），然后生成异常特征的自由文本摘要。接下来，基于语言的GPT-4模型将摘要拆解为具体方面（身体部位、位置、类型和属性），自动将这些特征与真实注释进行对比，并根据临床相关性和事实准确性为每个方面生成一个分数。然后将这些分数与临床医生获得的分数进行对比，观察到高度相关性（$\geq$
    85%，p $<$ .001）。我们的方法独特之处在于它结合了放射科医生的专业知识和LLM的链式思维（COT）[[21](#bib.bib21)]，来评估基于视觉的LLM预测的异常特征。
- en: 'Contributions: (1) The proposed auto-evaluation framework is a novel approach
    that decomposes the characteristics of a CT-based abnormal finding generated by
    a vision-based LLM into specific aspects, such that distinct dimensions of report
    quality can be effectively isolated and verified. (2) Three recent vision-based
    LLMs were evaluated for their capability to generate summarizations of CT-based
    findings. (3) Solidifies the limitations of traditional NLG metrics for capturing
    factual accuracy and reporting complexity.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献：(1) 所提出的自动评估框架是一种新颖的方法，它将基于视觉的LLM生成的CT异常发现的特征分解为具体方面，以便有效地隔离和验证报告质量的不同维度。(2)
    对三种最新的基于视觉的LLM在生成CT发现总结方面的能力进行了评估。(3) 确立了传统NLG指标在捕捉事实准确性和报告复杂性方面的局限性。
- en: '![Refer to caption](img/a9fd835c571f4a7a9a10c55dd57ff524.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a9fd835c571f4a7a9a10c55dd57ff524.png)'
- en: 'Figure 1: Pipeline for the auto-evaluation of CT-based findings characterization.
    First, CT slices with outlined lesions were analyzed by vision-based LLMs that
    generated a summary of their characteristics. The summaries were evaluated against
    the ground-truth annotations by a clinician, with NLG metrics, and auto-evaluation
    with GPT-4.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: CT发现特征自动评估的流程图。首先，具有标记病变的CT切片被基于视觉的LLM分析，这些LLM生成了其特征的总结。这些总结由临床医生通过NLG指标以及使用GPT-4进行自动评估。'
- en: 2 Methods
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 'Dataset: To the best our knowledge, there is no publicly available dataset
    with CT exams and paired radiology reports. Consequently, this study utilizes
    a subset of the publicly available DeepLesion dataset [[22](#bib.bib22), [23](#bib.bib23)],
    comprising 496 CT volumes (496 studies) from 486 patients. The subset contained
    500 lesions of various kinds (e.g., liver, kidney, bone, etc.) that were prospectively
    marked in 500 CT slices. The dataset also provided specific characteristics of
    lesions that were extracted from the sentences in the radiology reports using
    an automated method [[23](#bib.bib23)]. These included the body part location,
    type of lesion, and shape and appearance attributes. As certain lesion characteristics
    were missed by the automated tool, two board-certified radiologists, each with
    10+ years of experience, manually reviewed and comprehensively annotated any missing
    lesion characteristics. The dataset included both male and female participants
    (M: 294, F: 192), ranging in age from 2 to 87 years (mean: 52.2, s.d.: 17.7).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集: 据我们所知，目前没有公开可用的CT检查及配对放射学报告的数据集。因此，本研究利用了公开的DeepLesion数据集的一个子集 [[22](#bib.bib22),
    [23](#bib.bib23)]，包括来自486名患者的496个CT体积（496个研究）。该子集包含500个不同种类的病变（例如肝脏、肾脏、骨骼等），这些病变在500个CT切片中被前瞻性标记。数据集还提供了从放射学报告中的句子中提取的病变特定特征，使用自动化方法
    [[23](#bib.bib23)]。这些特征包括身体部位位置、病变类型以及形状和外观属性。由于某些病变特征被自动工具遗漏，两位拥有10年以上经验的认证放射科医生对遗漏的病变特征进行了手动审查和全面标注。数据集包括男性和女性参与者（M:
    294, F: 192），年龄范围从2岁到87岁（平均: 52.2岁，标准差: 17.7岁）。'
- en: 'Decomposing Vision-based LLM Predictions for Auto-Evaluation: The proposed
    framework decomposes the predicted descriptions of CT-based findings (e.g., lesions)
    by a vision-based LLM, such that a language-centric LLM can auto-evaluate the
    predictions by comparing them against the ground-truth annotations. First, several
    prominent vision-based LLMs from recent literature were tasked with analyzing
    a CT slice with a known abnormality (lesion) and generating a free-text description
    of its characteristics. Next, GPT-4 parsed this prediction, and provided a score
    for each aspect by comparing them against the ground truth annotations from DeepLesion.
    The objective was to move beyond conventional natural language generation (NLG)
    metrics, which despite their linguistic coherence, are insufficient to evaluate
    predictions for clinical accuracy. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4") illustrates
    the experimental design, consisting of three integral steps.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于视觉的LLM预测的自动评估**: 提出的框架通过分解由基于视觉的LLM生成的CT发现（例如病变）的预测描述，使得以语言为中心的LLM可以通过将这些预测与真实标注进行比较来自动评估预测结果。首先，近期文献中的几种突出的基于视觉的LLM被要求分析具有已知异常（病变）的CT切片，并生成其特征的自由文本描述。接下来，GPT-4解析了这些预测，并通过与DeepLesion的真实标注进行比较，为每个方面提供了评分。目标是超越传统的自然语言生成（NLG）指标，尽管这些指标在语言上连贯，但不足以评估临床准确性。图
    [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 基于GPT-4的视觉LLM预测自动评估") 说明了实验设计，包含三个关键步骤。'
- en: '(1) Visual Context Integration: The abnormal findings within the CT slices
    were delineated with a bounding box prior to being input into the vision-based
    LLM. The clear visual context provided to the vision-based LLMs was expected to
    enhance the accuracy of the generated summaries of the findings.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '(1) 视觉上下文整合: 在将CT切片输入到基于视觉的LLM之前，CT切片中的异常发现被用边框标出。提供清晰的视觉上下文给基于视觉的LLM预计会提高生成发现总结的准确性。'
- en: '(2) Text-Based Chain-of-Thought: In the Text-Based Chain of Thought (COT) approach,
    the vision-based LLM takes the input CT slice with an abnormality in it, and generates
    a free-text description of the abnormality. The output description should contain
    the following aspects: Body Part, Location (specific), Type, and Attributes. ‘Body
    Part’ is the larger anatomical region or organ of the body where the lesion or
    abnormality is situated. ‘Location’ refers to the precise area or specific site
    within a body part where a lesion or abnormality is located. ‘Type’ includes classifications,
    such as nodule, mass, or enlarged lymph node. ‘Attributes’ describe characteristics
    like size, shape, density (hypo or hyper), or calcification. The summary should
    be concise and clinically relevant, such that the characteristics of the findings
    can be pre-filled in the findings section of a radiology report. The summary should
    also lend itself to being auto-evaluated by a LLM (language model only). The prompt
    used for this task was designed to allow the model to concentrate on each aspect
    individually, thereby optimizing the use of its natural language generation capabilities
    to produce clinically relevant and informative descriptions of the findings. This
    approach contrasts with the one-shot methods [[21](#bib.bib21)] found in the literature,
    which attempt to generate entire radiology reports in a single step without explicit
    intermediate reasoning.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 基于文本的思维链：在基于文本的思维链（COT）方法中，基于视觉的 LLM 接收带有异常的 CT 切片输入，并生成异常的自由文本描述。输出描述应包含以下方面：身体部位、位置（具体）、类型和属性。“身体部位”是指病变或异常所在的较大解剖区域或器官。“位置”指的是在身体部位内病变或异常的精确区域或特定位置。“类型”包括分类，如结节、肿块或肿大的淋巴结。“属性”描述如大小、形状、密度（低或高）或钙化等特征。摘要应简洁且具有临床相关性，以便可以预填在放射科报告的发现部分。摘要还应便于由
    LLM（仅语言模型）进行自动评估。用于此任务的提示旨在使模型能够单独集中于每个方面，从而优化其自然语言生成能力，以生成临床相关和信息丰富的发现描述。这种方法与文献中的一次性方法[[21](#bib.bib21)]形成对比，后者试图在一步中生成整个放射科报告，而没有明确的中间推理。
- en: '(3) Auto-Evaluation using GPT-4: This is the core of our framework that involves
    employing GPT-4 to mimic the evaluation process of radiologists. Inspired by [[16](#bib.bib16)],
    GPT-4 compared the predicted summary from the vision-based LLM against ground-truth
    annotations from DeepLesion and assigned a score for each aspect of the abnormality.
    The scores provided were: {“Correct,” “Partially Correct,” “Incorrect,”, “Not
    Applicable”}. “Correct” indicated that the interpretation was entirely accurate.
    “Partially Correct” signified that the interpretation was somewhat accurate, but
    lacked full precision or completeness. “Incorrect” meant that the interpretation
    did not match the correct answer in any way. Lastly, “Not Applicable” denoted
    that the question was not relevant to the situation and thus could not be graded.
    These scores enabled the automated evaluation of the accuracy of the predicted
    descriptions. The prompt provided to GPT-4, detailed in supplementary material,
    instructed the model to assess the AI-generated predictions in the same way as
    a clinician.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 使用 GPT-4 进行自动评估：这是我们框架的核心部分，涉及使用 GPT-4 模拟放射科医生的评估过程。受到[[16](#bib.bib16)]的启发，GPT-4
    将基于视觉的 LLM 预测的摘要与 DeepLesion 的真实注释进行了比较，并为异常的每个方面分配了一个分数。提供的分数有：{“正确”、“部分正确”、“错误”、“不适用”}。“正确”表示解释完全准确。“部分正确”表示解释有些准确，但缺乏完整的精度或全面性。“错误”意味着解释与正确答案完全不符。最后，“不适用”表示问题与情况无关，因此无法评分。这些分数使得对预测描述的准确性进行自动评估成为可能。提供给
    GPT-4 的提示（详见补充材料）指示模型以与临床医生相同的方式评估 AI 生成的预测。
- en: 3 Experiments and Results
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验与结果
- en: 'Baseline Models: Three vision-based LLMs from recent literature were evaluated
    for their capability to generate a summary of the CT-based abnormality. These
    models included GPT-4 Vision (GPT-4V) [[24](#bib.bib24)], LLaVA-Med [[9](#bib.bib9)],
    and RadFM [[10](#bib.bib10)]. For both LLaVA-Med and RadFM, the default configurations
    set by the respective authors were used for inference. OpenAI API version “2023-03-15-preview”,
    utilizing the “gpt-4” engine, was employed for GPT-4V. The configuration was set
    as follows: {”temperature”: 0.7, ”top_p”: 0.95, ”max_tokens”: 4000, ”model_version”:
    ”2024-02-15-preview”}.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '基线模型：对最近文献中的三种基于视觉的LLM进行了评估，以测试其生成基于CT的异常摘要的能力。这些模型包括**GPT-4 Vision (GPT-4V)**
    [[24](#bib.bib24)]、**LLaVA-Med** [[9](#bib.bib9)]和**RadFM** [[10](#bib.bib10)]。对于LLaVA-Med和RadFM，使用了各自作者设定的默认配置进行推理。**OpenAI
    API版本“2023-03-15-preview”**，利用“gpt-4”引擎，应用于GPT-4V。配置如下设置：{”temperature”: 0.7,
    ”top_p”: 0.95, ”max_tokens”: 4000, ”model_version”: ”2024-02-15-preview”}。'
- en: 'Experiments: To determine the utility of GPT-4 for auto-evaluation of findings
    generated by vision-based LLMs, a baseline performance with respect to a clinician
    was required. In particular, 100 random lesions were chosen from the entire set
    of 500 lesions. As their characteristics were already verified previously by two
    board-certified radiologists, it was cumbersome to evaluate the free-text generations
    for all 500 lesions. Once their performance was quantified based on different
    metrics, the utility of using GPT-4 alone for auto-evaluation of the findings
    generated by vision-based LLMs compared against the ground-truth was determined.
    Evaluation of the generated findings was conducted based on four configurations:
    both with and without bounding boxes delineating the lesion in the CT slice, and
    combined with and without text-based COT.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实验：为了确定GPT-4在自动评估基于视觉的LLM生成的发现方面的实用性，需要与临床医生的基线表现进行比较。特别地，从500个病变中随机选择了100个病变。由于这些病变的特征已由两位注册放射科医师验证，因此评估所有500个病变的自由文本生成是繁琐的。一旦基于不同的指标量化了它们的性能，就确定了单独使用GPT-4进行自动评估基于视觉的LLM生成的发现的实用性。生成发现的评估基于四种配置进行：有和没有标记CT切片中病变的边界框，并且结合和不结合基于文本的COT。
- en: 'Metrics: The linguistic quality of the generated summaries were evaluated using
    traditional NLG metrics, such as BLEU, METEOR, and ROUGE. However, considering
    the variability in reporting across different institutions, these metrics have
    limitations in capturing the clinical relevance of the generated summary (see
    example in supplementary material). Inspired by [[16](#bib.bib16)], auto-evaluation
    by GPT-4 was done by comparing the predictions against the ground-truth annotations.
    This evaluation was also compared against the evaluation performed by a clinician.
    Correlations between the evaluations done by the clinician and GPT-4 was considered
    to be a measure of the reliability of GPT-4 for auto-evaluation.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 指标：生成的摘要的语言质量使用传统的自然语言生成（NLG）指标进行评估，如**BLEU**、**METEOR**和**ROUGE**。然而，考虑到不同机构报告的变异性，这些指标在捕捉生成摘要的临床相关性方面存在局限性（参见补充材料中的示例）。受[[16](#bib.bib16)]的启发，使用GPT-4进行的自动评估通过将预测与真实注释进行比较来完成。这项评估还与临床医生进行的评估进行了比较。临床医生与GPT-4进行的评估之间的相关性被认为是衡量GPT-4自动评估可靠性的一个标准。
- en: 'Results - Evaluation of Clinician vs. GPT-4: The clinician compared the ground
    truth against the free-text predictions generated by the vision-based LLMs for
    the 100 random lesions. The clinician provided a score for the prediction based
    on: body part, location, type, and attributes. The score fell into one of these
    categories: “Incorrect”: -1, “Partially Correct”: 0.5, “Correct”: 1\. The “Not
    Applicable” category was excluded from the analysis as these responses did not
    contribute meaningful information regarding the accuracy or quality of the generated
    findings. Next, GPT-4 compared the same predictions against the ground-truth and
    provided the corresponding scores. Finally, NLG metrics were computed using these
    scores. In Fig. [2](#S3.F2 "Figure 2 ‣ 3 Experiments and Results ‣ Decomposing
    Vision-based LLM Predictions for Auto-Evaluation with GPT-4"), heatmaps show the
    interaction between the various metrics computed based on the clinician and GPT-4
    evaluations.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 - 临床医生与 GPT-4 的评估：临床医生将实际情况与基于视觉的 LLM 对 100 个随机病变生成的自由文本预测进行比较。临床医生根据：身体部位、位置、类型和属性为预测提供了评分。评分分为以下几类：“不正确”：-1，“部分正确”：0.5，“正确”：1。由于“不可适用”类别的响应没有提供关于生成结果的准确性或质量的有意义信息，因此这些类别被排除在分析之外。接下来，GPT-4
    将相同的预测与实际情况进行比较，并提供相应的评分。最后，使用这些评分计算了 NLG 指标。在图 [2](#S3.F2 "图 2 ‣ 3 实验和结果 ‣ 使用
    GPT-4 对基于视觉的 LLM 预测进行自动评估") 中，热图展示了基于临床医生和 GPT-4 评价计算的各种指标之间的互动。
- en: From Fig. [2](#S3.F2 "Figure 2 ‣ 3 Experiments and Results ‣ Decomposing Vision-based
    LLM Predictions for Auto-Evaluation with GPT-4"), NLG metrics, such as BLEU and
    METEOR, revealed strong correlations amongst themselves (purple box), particularly
    at lower levels of precision like BLEU-1 and BLEU-2\. This indicated a consistency
    in evaluating the linguistic quality of generated texts at these levels. However,
    the correlation weakened considerably at increased precision levels of BLEU-3
    and BLEU-4\. Particularly for LLaVA-Med, the scores were predominantly at 0, and
    exhibited no correlation. This pattern aligned with expectations and highlighted
    the limitations of BLEU metrics in capturing the nuanced accuracy of more complex
    sentences within radiology reports.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 [2](#S3.F2 "图 2 ‣ 3 实验和结果 ‣ 使用 GPT-4 对基于视觉的 LLM 预测进行自动评估") 中可以看出，NLG 指标如
    BLEU 和 METEOR 之间存在强相关性（紫色框），尤其是在 BLEU-1 和 BLEU-2 这样的较低精度水平。这表明在这些水平上评估生成文本的语言质量具有一致性。然而，随着
    BLEU-3 和 BLEU-4 的精度水平提高，相关性显著减弱。特别是对于 LLaVA-Med，评分几乎全部为 0，且未显示出相关性。这种模式符合预期，突显了
    BLEU 指标在捕捉放射学报告中更复杂句子的细微准确性方面的局限性。
- en: '![Refer to caption](img/2906c1426d785ee3abc3012da7fc5926.png)![Refer to caption](img/8003765740843bad71aaca7b2085ad18.png)![Refer
    to caption](img/3a78c45b4856e96a873b548d239bde86.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/2906c1426d785ee3abc3012da7fc5926.png)![参见说明文字](img/8003765740843bad71aaca7b2085ad18.png)![参见说明文字](img/3a78c45b4856e96a873b548d239bde86.png)'
- en: 'Figure 2: Heatmap of pairwise Pearson’s Correlation Coefficient among various
    grading scores; NLG metrics, Radiologist evaluations and GPT-4 evaluations. L-to-R:
    vision-based LLM generated impression from GPT-4V, LLaVA-Med, RadFM. Color intensity
    indicates the strength of correlation, with darker shades representing higher
    correlation.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同评分间的配对 Pearson 相关系数热图；NLG 指标、放射科医生评价和 GPT-4 评价。自左至右：GPT-4V、LLaVA-Med、RadFM
    生成的基于视觉的 LLM 印象。颜色强度表示相关性的强度，较深的色调代表更高的相关性。
- en: Furthermore, the decomposition of the evaluation into specific aspects (location,
    body part, lesion type, and attributes) revealed insightful patterns (peach box).
    These aspects showed a lack of strong correlation with one another, and other
    pairings also displayed no significant correlation. These observations affirmed
    the efficacy of the approach in dissecting the findings into their granular elements
    (except location and body part), such that the distinct parts of report quality
    can be isolated and assessed independently. Comparing NLG metrics with the clinician
    evaluations showed a weak correlation, suggesting that the NLG metrics may not
    serve as reliable indicators of clinical accuracy within radiology reports (blue
    box). This highlighted a potential gap in utilizing NLG metrics for assessing
    the clinical relevance of generated reports, pointing to the necessity for domain-specific
    evaluation methods.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将评估分解为具体方面（位置、身体部位、病变类型和属性）揭示了有洞察力的模式（桃色框）。这些方面之间显示出较弱的相关性，其他配对也未显示显著相关性。这些观察结果确认了该方法在将发现分解为其细粒度元素中的有效性（位置和身体部位除外），使得报告质量的不同部分可以被隔离并独立评估。将NLG指标与临床医生评估进行比较显示出较弱的相关性，表明NLG指标可能不作为放射科报告临床准确性的可靠指标（蓝色框）。这突显了在利用NLG指标评估生成报告的临床相关性方面的潜在差距，指向了对特定领域评估方法的必要性。
- en: Lastly, the comparison between evaluations conducted by GPT-4 and the clinician
    showed the strength of the automated evaluation framework (pink box), and summarised
    in Table [2](#S3.T2 "Table 2 ‣ 3 Experiments and Results ‣ Decomposing Vision-based
    LLM Predictions for Auto-Evaluation with GPT-4"). The results from auto-evaluation
    showed a strong correlation with those from a clinician based on Pearson’s correlation
    coefficient of 0.87 $\pm$ 0.02 (p$<$0.001). This suggests that GPT-4’s evaluation
    method closely aligned with the clinical assessment paradigms utilized by radiologists.
    This observation underscored the potential of language models like GPT-4 in accurately
    mirroring radiologists’ evaluations, offering promise for automating report assessment
    with a high degree of fidelity to clinical standards.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将GPT-4与临床医生进行的评估进行比较显示了自动评估框架的强度（粉色框），并总结在表[2](#S3.T2 "Table 2 ‣ 3 Experiments
    and Results ‣ Decomposing Vision-based LLM Predictions for Auto-Evaluation with
    GPT-4")中。自动评估的结果与临床医生的结果显示出较强的相关性，基于Pearson相关系数为0.87 $\pm$ 0.02（p$<$0.001）。这表明GPT-4的评估方法与放射科医生所采用的临床评估范式紧密对齐。这一观察结果强调了像GPT-4这样的语言模型在准确反映放射科医生评估方面的潜力，提供了用高程度的忠实度于临床标准自动化报告评估的前景。
- en: 'Results - Auto Evaluation using GPT-4: As there was a strong correlation between
    the evaluations conducted by GPT-4 and the clinician, GPT-4 was utilized to compare
    the predicted findings against the ground-truth for all the 500 lesions. Figure
    [3](#S3.F3 "Figure 3 ‣ 3 Experiments and Results ‣ Decomposing Vision-based LLM
    Predictions for Auto-Evaluation with GPT-4") shows grading scores from GPT-4 across
    four configurations: both with and without lesion bounding boxes in the CT slice,
    and combined with and without text-based Chain-of-Thought (COT). Notably, LLaVA-Med
    and RadFM lack the functionality to utilize text-based COT processes, which aligns
    with findings in literature [[25](#bib.bib25)]. This sets them apart from GPT-4V,
    which demonstrated enhanced performance when employing COT reasoning. Furthermore,
    the inclusion of bounding boxes generally lead to improved model performance as
    it provided guidance for the models, with GPT-4V showing the most significant
    enhancement. Amongst the evaluated models, GPT-4V outperformed its counterparts.
    However, despite these advancements, the overall quality of pre-filled findings
    for CT reporting remained suboptimal. This indicated a pressing need for further
    development of vision-based LLMs to meet clinical standards.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 - 使用 GPT-4 的自动评估：由于 GPT-4 与临床医生的评估之间存在强相关性，因此使用 GPT-4 比较了所有 500 个病变的预测结果与真实情况。图
    [3](#S3.F3 "图 3 ‣ 3 实验与结果 ‣ 使用 GPT-4 进行自动评估的视觉基础 LLM 预测解构") 显示了 GPT-4 在四种配置下的评分：包括和不包括病变注释框的
    CT 切片，以及结合和不结合基于文本的思维链（COT）。值得注意的是，LLaVA-Med 和 RadFM 缺乏利用基于文本的 COT 过程的功能，这与文献中的发现一致
    [[25](#bib.bib25)]。这使它们与 GPT-4V 区别开来，后者在使用 COT 推理时表现出更好的性能。此外，注释框的加入通常会提高模型性能，因为它为模型提供了指导，GPT-4V
    显示出最显著的提升。在评估的模型中，GPT-4V 的表现优于其他模型。然而，尽管取得了这些进展，CT 报告中预填信息的总体质量仍不理想。这表明急需进一步发展基于视觉的
    LLM 以满足临床标准。
- en: Consider the ground truth text ‘left mediastinal pleural enhancing mass’ and
    a predicted result by RadFM ‘Axial contrast enhanced images through the thoracic
    inlet demonstrating a homogeneous mass posterior to the trachea’. It is evident
    that the NLG scores are low owing to the minimal textual overlap. Furthermore,
    this sentiment extends to individuals without medical training, as an understanding
    of anatomy is necessary to recognize that both texts pertain to the chest and
    lung regions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到实际情况文本‘左侧纵隔胸膜增强肿块’和 RadFM 预测结果‘经胸腔入口的轴位增强图像显示气管后方的均质肿块’，显然 NLG 分数较低，因为文本重叠较少。此外，这种观点也适用于没有医学培训的个体，因为理解解剖学知识是识别这两段文本都涉及胸部和肺部区域的必要条件。
- en: '![Refer to caption](img/200cac3d023407a10269ad7c6ff6faa0.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/200cac3d023407a10269ad7c6ff6faa0.png)'
- en: 'Figure 3: Comparative analysis of abnormality characterization by GPT-4V, LLaVA-Med,
    and RadFM with bounding boxes (Bx) vs. without bounding boxes (NoBx).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: GPT-4V、LLaVA-Med 和 RadFM 对异常特征的比较分析，使用带有注释框（Bx）与不带注释框（NoBx）的情况进行对比。'
- en: 'Color mapping = {orange: ‘Incorrect’, beige: ‘Partially Correct’, teal: ‘Correct’,
    white: ‘Not Applicable’}. Numeric labels = {0: ‘Location’, 1: ‘Body Part’, 2:
    ‘Type’, 3: ‘Attribute’}. X-axis denotes scores $\{\ x\in\mathbb{R}\ |\ 0<x<1\
    \}$, $N$ = 500 samples.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '颜色映射 = {橙色: ‘不正确’，米色: ‘部分正确’，青色: ‘正确’，白色: ‘不适用’}。数字标签 = {0: ‘位置’，1: ‘身体部位’，2:
    ‘类型’，3: ‘属性’}。X轴表示分数 $\{\ x\in\mathbb{R}\ |\ 0<x<1\ \}$，$N$ = 500 个样本。'
- en: '![[Uncaptioned image]](img/914287a77dd5a1a340385527f42306b5.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图片]](img/914287a77dd5a1a340385527f42306b5.png)'
- en: 'GT: ‘right renal parapelvic cyst’'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'GT: ‘右侧肾盂旁囊肿’'
- en: '| GPT-4V: ‘Location’: ‘Right abdomen, near the kidney’, ‘Body Part’: ‘Abdomen,
    kidney’, ‘Type’: ‘Mass’, ‘Attributes’: ‘Well-circumscribed, homogenous’, ‘Impression’:
    ‘Well-circumscribed homogenous mass in the right abdomen, adjacent to the right
    kidney’ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V: ‘位置’: ‘右侧腹部，靠近肾脏’，‘身体部位’: ‘腹部，肾脏’，‘类型’: ‘肿块’，‘属性’: ‘界限清晰，同质’，‘印象’:
    ‘右侧腹部的界限清晰的同质肿块，邻近右肾’ |'
- en: '| --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| LLaVA-Med: The image is an axial computed tomography (CT) scan that has been
    annotated with a bounding box by a radiologist. The bounding box is a rectangular
    box that encloses the lesion, which is indicated in green. This helps to highlight
    the area of interest and provides a clear view of the lesion’s location, size,
    and shape. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA-Med: 这张图片是经过放射科医生注释的轴位计算机断层扫描（CT）图像。注释框是一个矩形框，包围了病变区域，该区域用绿色标出。这有助于突出感兴趣的区域，并清楚地显示病变的位置、大小和形状。
    |'
- en: '| RadFM: { Lesion 1 } |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| RadFM: { 病变 1 } |'
- en: 'Table 1: Example Chain-of-Thought responses by vision-based LLMs for a renal
    cyst.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 基于视觉的 LLM 对肾囊肿的示例思维链响应。'
- en: '|  |   Location |   Body Part |   Type |   Attribute |   Avg. |   p-value |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  |   位置 |   身体部位 |   类型 |   属性 |   平均值 |   p 值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4V |   0.86 |   0.90 |   0.84 |   0.86 |   0.87 $\pm$ 0.02 |   $<$0.001
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V |   0.86 |   0.90 |   0.84 |   0.86 |   0.87 $\pm$ 0.02 |   $<$0.001
    |'
- en: '| LLaVA-Med |   0.59 |   0.83 |   0.76 |   0.80 |   0.75 $\pm$ 0.10 |   $<$0.001
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA-Med |   0.59 |   0.83 |   0.76 |   0.80 |   0.75 $\pm$ 0.10 |   $<$0.001
    |'
- en: '| RadFM |   0.99 |   0.92 |   0.82 |   0.89 |   0.90 $\pm$ 0.07 |   $<$0.001
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| RadFM |   0.99 |   0.92 |   0.82 |   0.89 |   0.90 $\pm$ 0.07 |   $<$0.001
    |'
- en: 'Table 2: Correlation scores between the clinician and GPT-4 grading of reports.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 临床医生与 GPT-4 对报告评分的相关性得分。'
- en: 4 Discussion and Conclusion
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论与结论
- en: Through experiments, it was observed that LLaVA-Med and RadFM were unable to
    leverage text-based COT as shown in Table [1](#S3.T1 "Table 1 ‣ 3 Experiments
    and Results ‣ Decomposing Vision-based LLM Predictions for Auto-Evaluation with
    GPT-4"). This reflects the architectural or design constraints that prevented
    these models from effectively breaking down and processing information in a stepwise
    manner. GPT-4V’s improved performance with COT suggested its architecture was
    better suited to sequential reasoning, mimicking a radiologist’s thought process,
    thereby leading to more accurate generation of the characteristics of CT findings.
    We also noticed that the bounding box delineating the lesion improved the performance
    of most models, especially GPT-4V. We attribute this to the added visual cues
    that bounding boxes provided. These cues helped focus the model’s attention on
    a specific area of interest, thereby improving the accuracy of the abnormality
    summary. GPT-4V outperforming other models can be linked to its advanced language
    processing capabilities, likely benefiting from a more sophisticated understanding
    of context, superior text generation algorithms, and perhaps a more extensive
    training dataset that includes a diverse range of medical imaging scenarios.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验观察发现，LLaVA-Med 和 RadFM 无法利用基于文本的 COT，如表[1](#S3.T1 "Table 1 ‣ 3 Experiments
    and Results ‣ Decomposing Vision-based LLM Predictions for Auto-Evaluation with
    GPT-4")所示。这反映了这些模型在架构或设计上的限制，阻碍了它们有效地逐步分解和处理信息。GPT-4V 在 COT 下的改进表现表明其架构更适合顺序推理，模拟放射科医师的思维过程，从而更准确地生成
    CT 发现的特征。我们还注意到，标出病灶的边界框提高了大多数模型的性能，尤其是 GPT-4V。我们将此归因于边界框提供的额外视觉线索。这些线索帮助将模型的注意力集中在特定的兴趣区域，从而提高了异常总结的准确性。GPT-4V
    超过其他模型可以与其先进的语言处理能力相关联，可能得益于对上下文的更复杂理解、更优越的文本生成算法，以及可能更广泛的训练数据集，包括多种医疗影像场景。
- en: With respect to the lesion characteristics (location, body part, type, attributes),
    a varied performance across the categories was seen. This reflected the differential
    impact of visual and textual guidance on model accuracy. For instance, the improvement
    in “Body Part” and “Type” with bounding boxes suggested these categories benefitted
    more from visual delineation. In contrast, the “Location” and “Attributes” categories,
    which may require more abstract reasoning or detailed textual information, showed
    mixed results. This indicated a complex interplay between model capabilities and
    the nature of the task.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关于病灶特征（位置、身体部位、类型、属性），在各类别之间表现出差异化的性能。这反映了视觉和文本指导对模型准确性的不同影响。例如，带有边界框的“身体部位”和“类型”的改进表明这些类别在视觉划分中受益更多。相比之下，“位置”和“属性”类别可能需要更抽象的推理或详细的文本信息，显示出混合结果。这表明模型能力与任务性质之间的复杂相互作用。
- en: Despite advancements, the overall performance of vision-based LLMs in describing
    the characteristics of findings in CT remains inadequate for clinical standards.
    This is largely due to the inherent complexity of medical images, the subtlety
    of pathologies, and the high standard of accuracy required for clinical diagnosis.
    Current vision-based LLMs are trained largely on natural images and lack sufficient
    training on diverse datasets. Thus, they still lack the ability to fully comprehend
    and articulate the complex interplay of visual features and clinical implications
    present in CT exams.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了进展，视觉基于LLMs在描述CT检查结果特征方面的整体表现仍未达到临床标准。这主要是由于医学图像的复杂性、病理的微妙性以及临床诊断所需的高精度标准。目前的视觉基于LLMs主要在自然图像上训练，缺乏在多样化数据集上的充分训练。因此，它们仍然无法完全理解和表述CT检查中存在的视觉特征与临床意义的复杂互动。
- en: In summary, an framework was proposed for the auto-evaluation of AI-generated
    characteristics for findings in CT exams, which would be pre-filled into the findings
    section of radiology reports. Results from auto-evaluation showed a strong correlation
    with those from a clinician based on Pearson’s correlation coefficient of 0.87
    $\pm$ 0.02 (p$<$0.001). GPT-4V outperformed other recent vision-language LLMs
    in predicting the characteristics of lesions in the dataset, and GPT-4 was sufficient
    for the auto-evaluation of the predictions from GPT-4V against ground-truth annotations.
    The evaluation approach pointed out particular weaknesses within various vision-based
    LLMs, and provided essential insights to enhance the precision and dependability
    of AI-generated interpretations from medical images. By addressing these identified
    gaps and focusing on comprehensive model development, there is potential to significantly
    improve the utility of vision- and language-based models in radiology.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，提出了一种框架用于自动评估AI生成的CT检查结果特征，这些特征将预填入放射学报告的发现部分。自动评估的结果与临床医生的结果显示出强烈的相关性，Pearson相关系数为0.87
    $\pm$ 0.02（p$<$0.001）。GPT-4V在预测数据集中病变特征方面优于其他近期的视觉-语言LLMs，而GPT-4对于对比GPT-4V的预测与真实标注的自动评估已足够。评估方法指出了各种基于视觉的LLMs的特定弱点，并提供了提升AI生成医学图像解释精确性和可靠性的关键见解。通过解决这些识别出的问题并专注于全面的模型开发，有可能显著提高视觉和语言模型在放射学中的实用性。
- en: 5 Acknowledgements
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research was supported by the Intramural Research Program of the National
    Library of Medicine and Clinical Center at the NIH.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了美国国家医学图书馆和临床中心内部研究计划的支持。
- en: References
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] John J. Smith and Leonard Berlin. Signing a colleague’s radiology report.
    American Journal of Roentgenology, 176(1):27–30, 2001. PMID: 11133532.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] John J. Smith 和 Leonard Berlin. 签署同事的放射学报告。《美国放射学杂志》，176(1):27–30，2001年。PMID:
    11133532。'
- en: '[2] Michael D Ringler, Brian C Goss, and Brian J Bartholmai. Syntactic and
    semantic errors in radiology reports associated with speech recognition software.
    Health Informatics Journal, 23(1):3–13, 2017. PMID: 26635322.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Michael D Ringler、Brian C Goss 和 Brian J Bartholmai. 语音识别软件在放射学报告中的句法和语义错误。《健康信息学期刊》，23(1):3–13，2017年。PMID:
    26635322。'
- en: '[3] M. Mahesh, A. J. Ansari, and Jr Mettler, F. A. Patient exposure from radiologic
    and nuclear medicine procedures in the united states and worldwide: 2009-2018.
    Radiology, 301, 2023.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Mahesh、A. J. Ansari 和 Jr Mettler, F. A. 美国及全球放射学和核医学程序的患者辐射暴露：2009-2018年。《放射学》，301，2023年。'
- en: '[4] N. A. Fawzy, M. J. Tahir, A. Saeed, M. J. Ghosheh, T. Alsheikh, A. Ahmed,
    and Z. Lee, K. Y. Yousaf. Incidence and factors associated with burnout in radiologists:
    A systematic review. European journal of radiology open, 11:100530, 2023.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] N. A. Fawzy、M. J. Tahir、A. Saeed、M. J. Ghosheh、T. Alsheikh、A. Ahmed 和 Z.
    Lee、K. Y. Yousaf. 放射科医生职业倦怠的发生率及相关因素：系统评价。《欧洲放射学开放期刊》，11:100530，2023年。'
- en: '[5] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating radiology
    reports via memory-driven transformer. In Bonnie Webber, Trevor Cohn, Yulan He,
    and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pages 1439–1449, Online, November 2020\.
    Association for Computational Linguistics.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Zhihong Chen、Yan Song、Tsung-Hui Chang 和 Xiang Wan. 通过记忆驱动的变换器生成放射学报告。在
    Bonnie Webber、Trevor Cohn、Yulan He 和 Yang Liu 编辑的《2020年自然语言处理实证方法会议（EMNLP）论文集》，第1439–1449页，在线，2020年11月。计算语言学协会。'
- en: '[6] Zhihong Chen, Yaling Shen, Yan Song, and Xiang Wan. Cross-modal memory
    networks for radiology report generation. In Chengqing Zong, Fei Xia, Wenjie Li,
    and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), pages 5904–5914, Online, August 2021\.
    Association for Computational Linguistics.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 陈志宏、沈雅玲、宋燕和万翔。用于放射科报告生成的跨模态记忆网络。见于钟成庆、夏飞、李文杰和罗伯托·纳维格利编辑的《第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）》，第5904–5914页，在线，2021年8月。计算语言学协会。'
- en: '[7] Akimichi Ichinose, Taro Hatsutani, Keigo Nakamura, Yoshiro Kitamura, Satoshi
    Iizuka, Edgar Simo-Serra, Shoji Kido, and Noriyuki Tomiyama. Visual grounding
    of whole radiology reports for 3d ct images. In Hayit Greenspan, Anant Madabhushi,
    Parvin Mousavi, Septimiu Salcudean, James Duncan, Tanveer Syeda-Mahmood, and Russell
    Taylor, editors, Medical Image Computing and Computer Assisted Intervention –
    MICCAI 2023, pages 611–621, Cham, 2023\. Springer Nature Switzerland.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 市之濑明、初谷太郎、中村圭吾、北村嘉郎、井冢聪、埃德加·西莫-塞拉、木户祥司和富山纪之。针对 3D CT 图像的整个放射科报告的视觉基础。见于海特·格林斯潘、阿南特·马达布希、帕尔文·穆萨维、塞普提米乌斯·萨尔库德安、詹姆斯·邓肯、坦维尔·赛义德-马哈茂德和拉塞尔·泰勒编辑的《医学图像计算与计算机辅助干预
    – MICCAI 2023》，第611–621页，施普林格·自然·瑞士，2023年。'
- en: '[8] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 乔希·阿赫亚姆、史蒂文·阿德勒、桑迪尼·阿加瓦尔、拉玛·艾哈迈德、伊尔格·阿卡亚、弗洛伦西亚·莱奥尼·阿莱曼、迪奥戈·阿尔梅达、扬科·阿尔滕施密特、萨姆·奥特曼、施亚玛尔·安纳德卡特、等。GPT-4
    技术报告。arXiv 预印本 arXiv:2303.08774，2023年。'
- en: '[9] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei
    Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. LLaVA-Med: Training a large
    language-and-vision assistant for biomedicine in one day. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 李春远、克里夫·黄、盛张、宇山尚人、刘浩天、杨剑伟、特里斯坦·诺伊曼、潘浩丰和高剑锋。LLaVA-Med：在一天内训练一个大型语言与视觉助手用于生物医学。神经信息处理系统进展，36，2024年。'
- en: '[10] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards
    generalist foundation model for radiology. arXiv preprint arXiv:2308.02463, 2023.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 乔伊·吴、肖曼·张、雅·张、彦锋·王和韦迪·谢。迈向放射学的通用基础模型。arXiv 预印本 arXiv:2308.02463，2023年。'
- en: '[11] Shubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai, Qingqing Zhu, Xiuying
    Chen, Yifan Yang, Qingyu Chen, Won Kim, Donald C Comeau, et al. Opportunities
    and challenges for chatgpt and large language models in biomedicine and health.
    Briefings in Bioinformatics, 25(1):bbad493, 2024.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 宋博·田、金巧、拉娜·叶甘诺娃、赖博廷、朱清清、陈秀英、杨一凡、陈清宇、金元、唐纳德·C·科莫、等。ChatGPT 和大型语言模型在生物医学和健康领域的机会与挑战。《生物信息学简报》，25(1)：bbad493，2024年。'
- en: '[12] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric
    Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375,
    2023.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 哈尔沙·诺里、尼古拉斯·金、斯科特·梅耶·麦金尼、迪恩·卡里甘和埃里克·霍维茨。GPT-4 在医疗挑战问题上的能力。arXiv 预印本 arXiv:2303.13375，2023年。'
- en: '[13] Qiao Jin, Robert Leaman, and Zhiyong Lu. Retrieve, summarize, and verify:
    How will chatgpt impact information seeking from the medical literature? Journal
    of the American Society of Nephrology, pages 10–1681, 2023.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 金巧、罗伯特·利曼和吕志勇。检索、总结和验证：ChatGPT 将如何影响医学文献的信息获取？《美国肾脏病学会杂志》，第10–1681页，2023年。'
- en: '[14] Qingqing Zhu, Tejas Sudharshan Mathai, Pritam Mukherjee, Yifan Peng, Ronald M.
    Summers, and Zhiyong Lu. Utilizing longitudinal chest x-rays and reports to pre-fill
    radiology reports. In MICCAI (5), volume 14224 of Lecture Notes in Computer Science,
    pages 189–198\. Springer, 2023.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 朱清清、特贾斯·苏德哈山·马泰、普里坦·穆赫吉、彭一凡、罗纳德·M·萨默斯和吕志勇。利用纵向胸部 X 射线和报告来预填放射科报告。见于 MICCAI（5），计算机科学讲义笔记第14224卷，第189–198页。施普林格，2023年。'
- en: '[15] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus,
    Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.
    Chexpert: A large chest radiograph dataset with uncertainty labels and expert
    comparison. In Thirty-Third AAAI Conference on Artificial Intelligence, 2019.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 杰里米·欧文、普拉纳夫·拉朱帕尔卡、迈克尔·科、余一凡、席尔维亚娜·丘雷亚-伊尔库斯、克里斯·楚特、亨里克·马克伦、贝赫扎德·哈格古、罗宾·鲍尔、凯蒂·什潘斯卡娅、等。Chexpert：一个带有不确定性标签和专家比较的大型胸部
    X 射线数据集。见于第三十三届 AAAI 人工智能会议，2019年。'
- en: '[16] Qingqing Zhu, Xiuying Chen, Qiao Jin, Benjamin Hou, Tejas Sudharshan Mathai,
    Pritam Mukherjee, Xin Gao, Ronald M Summers, and Zhiyong Lu. Leveraging professional
    radiologists’ expertise to enhance llms’ evaluation for radiology reports, 2024.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 朱青青，陈秀英，金乔，本杰明·侯，特贾斯·苏达尔尚·马泰，普里塔姆·穆克吉，郜鑫，罗纳德·M·萨默斯，和卢志勇。利用专业放射科医师的专业知识提升LLMs对放射学报告的评估，2024年。'
- en: '[17] Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu, Justin M Cheung, Robert
    Chen, Ronald M Summers, Justin F Rousseau, Peiyun Ni, Marc J Landsman, et al.
    Hidden flaws behind expert-level accuracy of gpt-4 vision in medicine. arXiv preprint
    arXiv:2401.08396, 2024.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 金乔，方圆·陈，周毅良，徐子扬，贾斯廷·M·张，罗伯特·陈，罗纳德·M·萨默斯，贾斯廷·F·鲁索，倪培云，马克·J·兰兹曼，等。GPT-4视图在医学领域的专家级准确性背后的隐性缺陷。arXiv预印本
    arXiv:2401.08396，2024年。'
- en: '[18] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and
    Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks
    on weakly-supervised classification and localization of common thorax diseases.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2097–2106, 2017.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 王晓松，一凡·彭，卢乐，卢志勇，穆罕默德哈迪·巴赫里，和罗纳德·M·萨默斯。Chestx-ray8：医院级胸部X光数据库及常见胸部疾病的弱监督分类和定位基准。见于IEEE计算机视觉与模式识别会议论文集，第2097–2106页，2017年。'
- en: '[19] Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri, Ronald Summers,
    and Zhiyong Lu. Negbio: a high-performance tool for negation and uncertainty detection
    in radiology reports. AMIA Summits on Translational Science Proceedings, 2018:188,
    2018.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 彭一凡，王晓松，卢乐，穆罕默德哈迪·巴赫里，罗纳德·萨默斯，和卢志勇。Negbio：一个高性能的工具，用于放射学报告中的否定和不确定性检测。AMIA
    翻译科学高峰会议论文集，2018:188，2018。'
- en: '[20] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang
    Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda
    Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference
    on Empirical Methods in Natural Language Processing, pages 2511–2522, Singapore,
    December 2023\. Association for Computational Linguistics.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 刘阳，丹·伊特，徐毅充，王硕航，徐若晨，和朱成光。G-eval：使用GPT-4进行NLG评估以获得更好的人工对齐。在Houda Bouamor、Juan
    Pino 和 Kalika Bali 编辑的2023年自然语言处理经验方法会议论文集，第2511–2522页，新加坡，2023年12月。计算语言学协会。'
- en: '[21] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in Neural Information Processing Systems, 35:24824–24837,
    2022.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 韦杰森，王雪芝，戴尔·舒尔曼斯，马尔滕·博斯马，费小，艾德·池，阮国维，丹尼·周，等。链式思维提示引发大型语言模型的推理。神经信息处理系统进展，35:24824–24837，2022年。'
- en: '[22] Ke Yan, Xiaosong Wang, Le Lu, and Ronald M. Summers. Deeplesion: Automated
    deep mining, categorization and detection of significant radiology image findings
    using large-scale clinical lesion annotations. CoRR, abs/1710.01766, 2017.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 闫科，王晓松，卢乐，和罗纳德·M·萨默斯。Deeplesion：使用大规模临床病灶注释的自动化深度挖掘、分类和检测显著放射学图像发现。CoRR，abs/1710.01766，2017年。'
- en: '[23] Ke Yan, Yifan Peng, Veit Sandfort, Mohammadhadi Bagheri, Zhiyong Lu, and
    Ronald M Summers. Holistic and comprehensive annotation of clinically significant
    findings on diverse ct images: learning from radiology reports and label ontology.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 8523–8532, 2019.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 闫科，一凡·彭，费特·桑德福特，穆罕默德哈迪·巴赫里，卢志勇，和罗纳德·M·萨默斯。对多样CT图像中临床显著发现的全面和综合注释：从放射学报告和标签本体中学习。见于IEEE/CVF计算机视觉与模式识别会议论文集，第8523–8532页，2019年。'
- en: '[24] OpenAI. GPT-4V(ision) System Card. https://cdn.openai.com/papers/GPTV\_System\_Card.pdf,
    2023.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] OpenAI。GPT-4V(ision)系统卡。https://cdn.openai.com/papers/GPTV\_System\_Card.pdf，2023年。'
- en: '[25] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin
    Shin, and Minjoon Seo. The cot collection: Improving zero-shot and few-shot learning
    of language models via chain-of-thought fine-tuning. In The 2023 Conference on
    Empirical Methods in Natural Language Processing, 2023.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 金承恩，朱世俊，金道勇，乔尔·张，成贤，詹敏俊，和徐敏俊。COT集合：通过链式思维微调提高语言模型的零样本和少样本学习。见于2023年自然语言处理经验方法会议，2023年。'
