- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 17:35:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:35:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Learning Interpretable Style Embeddings via Prompting LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过提示 LLMs 学习可解释的风格嵌入
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.12696](https://ar5iv.labs.arxiv.org/html/2305.12696)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.12696](https://ar5iv.labs.arxiv.org/html/2305.12696)
- en: Ajay Patel
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ajay Patel
- en: University of Pennsylvania
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚大学
- en: ajayp@upenn.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ajayp@upenn.edu
- en: '&Delip Rao'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Delip Rao'
- en: University of Pennsylvania
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚大学
- en: deliprao@gmail.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: deliprao@gmail.com
- en: '&Ansh Kothary'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Ansh Kothary'
- en: Columbia University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: ank2145@columbia.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ank2145@columbia.edu
- en: \ANDKathleen McKeown
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \ANDKathleen McKeown
- en: Columbia University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: kathy@cs.columbia.edu
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: kathy@cs.columbia.edu
- en: '&Chris Callison-Burch'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&Chris Callison-Burch'
- en: University of Pennsylvania
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚大学
- en: ccb@upenn.edu
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ccb@upenn.edu
- en: Abstract
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Style representation learning builds content-independent representations of
    author style in text. To date, no large dataset of texts with stylometric annotations
    on a wide range of style dimensions has been compiled, perhaps because the linguistic
    expertise to perform such annotation would be prohibitively expensive. Therefore,
    current style representation approaches make use of unsupervised neural methods
    to disentangle style from content to create style vectors. These approaches, however,
    result in uninterpretable representations, complicating their usage in downstream
    applications like authorship attribution where auditing and explainability is
    critical. In this work, we use prompting to perform stylometry on a large number
    of texts to generate a synthetic stylometry dataset. We use this synthetic data
    to then train human-interpretable style representations we call Lisa embeddings.
    We release our synthetic dataset (StyleGenome) and our interpretable style embedding
    model (Lisa) as resources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 风格表示学习构建了独立于内容的作者风格表示。迄今为止，尚未编制包含广泛风格维度的风格注释的大型文本数据集，可能是因为进行这种注释所需的语言学专业知识成本过高。因此，目前的风格表示方法利用无监督的神经网络方法来解开风格与内容的关系，从而创建风格向量。然而，这些方法会导致不可解释的表示，复杂化了它们在诸如作者归属这样的下游应用中的使用，因为审计和可解释性至关重要。在这项工作中，我们使用提示来对大量文本进行风格分析，以生成一个合成的风格分析数据集。我们使用这些合成数据来训练我们称之为
    Lisa 嵌入的人类可解释风格表示。我们发布了我们的合成数据集（StyleGenome）和我们的可解释风格嵌入模型（Lisa）作为资源。
- en: Learning Interpretable Style Embeddings via Prompting LLMs
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提示 LLMs 学习可解释的风格嵌入
- en: Ajay Patel University of Pennsylvania ajayp@upenn.edu                       
    Delip Rao University of Pennsylvania deliprao@gmail.com                       
    Ansh Kothary Columbia University ank2145@columbia.edu
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Ajay Patel 宾夕法尼亚大学 ajayp@upenn.edu                        Delip Rao 宾夕法尼亚大学
    deliprao@gmail.com                        Ansh Kothary 哥伦比亚大学 ank2145@columbia.edu
- en: Kathleen McKeown Columbia University kathy@cs.columbia.edu                       
    Chris Callison-Burch University of Pennsylvania ccb@upenn.edu
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kathleen McKeown 哥伦比亚大学 kathy@cs.columbia.edu                        Chris Callison-Burch
    宾夕法尼亚大学 ccb@upenn.edu
- en: 1 Introduction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Style representation learning aims to represent the stylistic attributes of
    an authored text. Prior work has treated the style of a text as separable from
    the content. Stylistic attributes have included, but are not limited to, linguistic
    choices in syntax, grammar, spelling, vocabulary, and punctuation (Jafaritazehjani
    et al., [2020](#bib.bib24)). Style representations should represent two texts
    with similar stylistic attributes more closely than texts with different attributes
    independent of what content is present in the texts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 风格表示学习旨在表示作者文本的风格属性。先前的工作将文本的风格视为与内容可分离的。风格属性包括但不限于语法、拼写、词汇和标点符号中的语言选择（Jafaritazehjani
    等，[2020](#bib.bib24)）。风格表示应该使两个具有相似风格属性的文本比具有不同属性的文本更紧密地匹配，而与文本中存在的内容无关。
- en: Stylometry, the analysis of style, applies forensic linguistics to tasks like
    authorship attribution. Stylometry often relies on semi-manual analysis by forensic
    linguistic experts (Mosteller and Wallace, [1963](#bib.bib34); Holmes, [1994](#bib.bib21);
    Rosso et al., [2016](#bib.bib49)). Computational stylometry often uses rule-based
    approaches utilizing count-based features like the frequencies of function words
    (Stamatatos, [2009](#bib.bib54); Koppel et al., [2009](#bib.bib29); Tausczik and
    Pennebaker, [2010](#bib.bib55)). More modern, neural approaches attempt to learn
    style representations in an unsupervised fashion through a proxy task like style
    transfer (Shen et al., [2017](#bib.bib53); Fu et al., [2018](#bib.bib15); John
    et al., [2019](#bib.bib25); Dai et al., [2019](#bib.bib11); Li et al., [2019](#bib.bib30);
    Yi et al., [2021](#bib.bib64); Zhu et al., [2022](#bib.bib67)) or authorship verification
    (Boenninghoff et al., [2019](#bib.bib7); Hay et al., [2020](#bib.bib19); Zhu and
    Jurgens, [2021](#bib.bib66); Wegmann et al., [2022](#bib.bib61)). These stronger
    neural approaches, unlike simpler frequency-based techniques, are uninterpretable.
    This makes it difficult to effectively analyze their representations and their
    failure modes, and precludes their usage in real-world authorship attribution
    scenarios because interpretability and verification is critical for legal admissibility
    (Tiersma and Solan, [2002](#bib.bib56)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 风格计量学，即风格分析，将法庭语言学应用于作者身份鉴定等任务。风格计量学通常依赖于法庭语言学专家的半手动分析（Mosteller 和 Wallace,
    [1963](#bib.bib34)；Holmes, [1994](#bib.bib21)；Rosso 等, [2016](#bib.bib49)）。计算风格计量学通常使用基于规则的方法，利用功能词的频率等基于计数的特征（Stamatatos,
    [2009](#bib.bib54)；Koppel 等, [2009](#bib.bib29)；Tausczik 和 Pennebaker, [2010](#bib.bib55)）。更现代的神经方法尝试通过风格迁移等代理任务以无监督的方式学习风格表示（Shen
    等, [2017](#bib.bib53)；Fu 等, [2018](#bib.bib15)；John 等, [2019](#bib.bib25)；Dai
    等, [2019](#bib.bib11)；Li 等, [2019](#bib.bib30)；Yi 等, [2021](#bib.bib64)；Zhu 等,
    [2022](#bib.bib67)）或作者身份验证（Boenninghoff 等, [2019](#bib.bib7)；Hay 等, [2020](#bib.bib19)；Zhu
    和 Jurgens, [2021](#bib.bib66)；Wegmann 等, [2022](#bib.bib61)）。这些更强大的神经方法与简单的基于频率的技术不同，难以解释。这使得有效分析它们的表示及其失败模式变得困难，也使得它们在现实世界的作者身份鉴定场景中难以使用，因为解释性和验证对于法律可接受性至关重要（Tiersma
    和 Solan, [2002](#bib.bib56)）。
- en: '![Refer to caption](img/fc92609b86b8f749977271a244e8a085.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/fc92609b86b8f749977271a244e8a085.png)'
- en: 'Figure 1: An example of a 768-dimensional interpretable style vector produced
    by Lisa, trained using a GPT-3 annotated synthetic stylometery dataset.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个由 Lisa 生成的 768 维可解释风格向量示例，该向量通过使用 GPT-3 注释的合成风格计量学数据集进行训练。
- en: 'With this motivation, we propose a human-interpretable style representation
    model $\mathcal{M}$ which, for a given text $t$, produces a $D$-dimensional vector
    $\mathcal{M}(t)\in[0.0,1.0]^{D}$. Each dimension corresponds to one of $D$ style
    attributes $\{a_{0},a_{1},\ldots,a_{D}\}$. Each element at dimension $d$ of this
    vector is constrained in the range $[0.0,1.0]$ to represent the probability of
    the corresponding style attribute $a_{d}$ being present in the text $t$. See Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs") for a visualization of a result from our final trained model
    with $D=768$ dimensions. An immediate obstacle to train such a model is that no
    large dataset of texts with stylometric annotations currently exists; annotating
    a large number of texts on a wide variety ($D=768$) of stylistic attributes would
    likely require annotators with linguistic expertise and be prohibitively expensive.
    Given this, we use GPT-3 (Brown et al., [2020](#bib.bib8)), a large language model
    (LLM), and zero-shot prompts to generate a synthetic dataset we call StyleGenome
    of human-interpretable stylometric annotations for various texts. Our approach
    is motivated by recent works showing models trained on synthetic datasets annotated
    by prompting LLMs can match and sometimes even outperform models trained on human-labeled
    datasets (Wang et al., [2022](#bib.bib57); Gilardi et al., [2023](#bib.bib16);
    Huang et al., [2022](#bib.bib23); Honovich et al., [2022](#bib.bib22)). Training
    on StyleGenome, we develop the Linguistically-Interpretable Style Attribute (Lisa)
    embedding model. We summarize our primary contributions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以此为动机，我们提出了一种人类可解释的风格表示模型 $\mathcal{M}$，该模型对于给定文本 $t$，生成一个 $D$ 维向量 $\mathcal{M}(t)\in[0.0,1.0]^{D}$。每一维对应
    $D$ 个风格属性中的一个 $\{a_{0},a_{1},\ldots,a_{D}\}$。该向量在维度 $d$ 的每个元素被限制在 $[0.0,1.0]$
    范围内，以表示对应风格属性 $a_{d}$ 在文本 $t$ 中出现的概率。有关我们最终训练模型结果的可视化，请参见图 [1](#S1.F1 "图 1 ‣ 1
    引言 ‣ 通过提示 LLM 学习可解释风格嵌入")，其中 $D=768$ 维。训练此类模型的一个直接障碍是目前没有大规模的风格计量注释文本数据集；对大量文本进行广泛
    ($D=768$) 风格属性的注释可能需要具有语言学专业知识的注释员，并且成本极高。鉴于此，我们使用 GPT-3 (Brown et al., [2020](#bib.bib8))，一种大型语言模型
    (LLM)，和零-shot 提示来生成一个我们称之为 StyleGenome 的合成数据集，为各种文本生成可解释的风格计量注释。我们的做法受到近期研究的启发，这些研究表明，训练于合成数据集的模型通过提示
    LLM 进行注释，可以匹配甚至有时超越训练于人类标注数据集的模型 (Wang et al., [2022](#bib.bib57); Gilardi et
    al., [2023](#bib.bib16); Huang et al., [2022](#bib.bib23); Honovich et al., [2022](#bib.bib22))。在
    StyleGenome 上进行训练，我们开发了语言学可解释的风格属性（Lisa）嵌入模型。我们总结了我们的主要贡献：
- en: '1.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We outline an unsupervised method for producing interpretable style embeddings
    using zero-shot prompting and distillation.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们概述了一种使用零-shot 提示和蒸馏生成可解释风格嵌入的无监督方法。
- en: '2.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We generate and release StyleGenome, a synthetic stylometry dataset with ~5.5M
    examples, the first large-scale dataset with texts paired with wide range of stylometric
    annotations.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们生成并发布了 StyleGenome，这是一个合成的风格计量数据集，包含约550万例，是第一个配有广泛风格计量注释的大规模数据集。
- en: '3.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We train, evaluate, and release Lisa, the first-ever interpretable style embedding
    model with a wide variety of linguistic dimensions ($D=768$). We find Lisa matches
    the performance of existing style representations, while allowing for explainability
    and transparency.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们训练、评估并发布了Lisa，这是第一个具有广泛语言维度（$D=768$）的可解释风格嵌入模型。我们发现Lisa的性能与现有风格表示相匹配，同时允许解释性和透明度。
- en: 2 Generating StyleGenome
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 生成 StyleGenome
- en: 'To create StyleGenome for training Lisa, we select Reddit data from the Million
    User Dataset (MUD) (Khan et al., [2021](#bib.bib27); Andrews and Bishop, [2019](#bib.bib3))
    to stylometrically annotate following prior work that uses Reddit to source a
    diversity of styles from different authors (Wegmann et al., [2022](#bib.bib61)).
    We sample 10 random posts per author for 1,000 random authors, resulting in 10,000
    total posts selected for annotation. We display some of the diversity of styles
    captured in the following examples from our Reddit authors. They vary in formality,
    punctuation, emoji usage, etc.:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建 StyleGenome 以训练 Lisa，我们从 Million User Dataset (MUD) (Khan et al., [2021](#bib.bib27);
    Andrews and Bishop, [2019](#bib.bib3)) 选择了 Reddit 数据，按照先前工作使用 Reddit 来获取不同作者的风格多样性
    (Wegmann et al., [2022](#bib.bib61)) 对其进行风格计量注释。我们从 1,000 个随机作者中每个作者随机抽取 10 条帖子，共选取
    10,000 条帖子进行注释。我们展示了以下 Reddit 作者中的一些风格多样性示例。这些示例在正式性、标点符号、表情符号使用等方面各不相同。
- en: 'Reddit User #47: forreal though sell that **** its worth like 650$ Reddit User
    #205: This was amazing :D Please, make more! Reddit User #216: I thought that
    was Israel…or Saudi Arabia…or Egypt…Venezuela? Reddit User #356: I wouldn’t watch
    a football game like that either. It’s novel, but I think its appeal is limited
    and again, not the same thing as actual VR where things feel tangible and to scale.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reddit 用户 #47: 说真的，卖掉那个****，它值大约 650 美元 Reddit 用户 #205: 这真棒 :D 请多做一些！ Reddit
    用户 #216: 我以为那是以色列……或者沙特阿拉伯……或者埃及……委内瑞拉？ Reddit 用户 #356: 我也不会看那样的足球比赛。这很新颖，但我认为它的吸引力有限，而且与实际的
    VR 不同，后者让人感觉到真实的规模和触感。'
- en: We use GPT-3¹¹1We use text-davinci-003 with a temperature setting of 0.0 and
    a top_p setting of 1.0. (Brown et al., [2020](#bib.bib8); Ouyang et al., [2022](#bib.bib37))
    to annotate each post in two stages.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 GPT-3¹¹1我们使用 text-davinci-003，温度设置为 0.0，top_p 设置为 1.0。 (Brown et al., [2020](#bib.bib8);
    Ouyang et al., [2022](#bib.bib37)) 对每个帖子进行两个阶段的标注。
- en: Stage 1
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一阶段
- en: 'We elicit the model with a zero-shot prompt to generate a description of the
    style of a given Reddit post:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用零样本提示生成给定 Reddit 帖子的风格描述：
- en: 'Prompt: Write a long paragraph describing the unique grammar style of the following
    passage without referring to specifics about the topic.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：写一个长段落，描述以下段落的独特语法风格，但不要涉及主题的具体内容。
- en: 'Passage: Wow that’s really cool. I’ll have to watch for that next time I watch
    it.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 段落：哇，那真的很酷。下次我看时得注意这个。
- en: 'Description:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 描述：
- en: $\downarrow$
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $\downarrow$
- en: 'Generation: The passage reflects a conversational style in its grammar, with
    short, simple sentences that are easy to understand. The language is informal
    and direct, with the speaker expressing their enthusiasm for the topic in a straightforward
    manner. The speaker also uses contractions, such as “I’ll," to emphasize their
    excitement. The speaker also uses a casual tone, with the phrase "really cool"
    to emphasize their interest in the topic. The speaker also uses the present tense
    to express their anticipation for the future, with the phrase “next time I watch
    it." This style of grammar is effective in conveying the speaker’s enthusiasm
    and interest in the topic.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 生成：该段落在语法上体现了对话风格，使用了简短、简单的句子，易于理解。语言非正式而直接，作者以直接的方式表达了对话题的热情。作者还使用了缩写词，如“我会”（I’ll），以强调他们的兴奋。作者还使用了随意的语气，用“真的很酷”（really
    cool）来强调他们对话题的兴趣。作者还使用了现在时态来表达对未来的期待，使用了“下次我看时”（next time I watch it）这一短语。这种语法风格在传达作者对话题的热情和兴趣方面非常有效。
- en: Stage 2
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一阶段
- en: 'We use another zero-shot prompt to standardize the generations from Stage 1
    into short, declarative sentences with a uniform structure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用另一个零样本提示，将第一阶段生成的内容标准化为简短的声明句，结构统一：
- en: 'Prompt: Here’s a description of an author’s writing style for a passage: The
    passage utilizes a conversational style of grammar, with short…'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：这是对某一段落作者写作风格的描述：该段落采用了对话式的语法风格，句子简短…
- en: Rewrite this description as a long list of short sentences describing the author’s
    writing style where each sentence is in the format of "The author is X." or "The
    author uses X.".
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将该描述改写为长列表，每个句子描述作者的写作风格，每个句子格式为“作者是 X。”或“作者使用 X。”。
- en: 'Output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: $\downarrow$
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: $\downarrow$
- en: 'Generation: The author is using a conversational style of grammar.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 生成：作者使用了对话式的语法风格。
- en: The author is using short, simple sentences.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了简短、简单的句子。
- en: The author is using language that is informal and direct.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了非正式而直接的语言。
- en: The author is expressing enthusiasm for the topic in a straightforward manner.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作者以直接的方式表达了对话题的热情。
- en: The author is using contractions, such as "I’ll".
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了缩写词，如“我会”（I’ll）。
- en: The author is using a casual tone.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了随意的语气。
- en: The author is emphasizing their interest in the topic with the phrase "really
    cool".
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用“真的很酷”（really cool）这一短语来强调他们对话题的兴趣。
- en: The author is using the present tense to express anticipation for the future.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了现在时态来表达对未来的期待。
- en: 'We run both stages with 93 different Stage 1 prompts for all 10,000 posts²²2We
    preprocess posts to the first 25 sentences and use the emoji Python package to
    convert emojis to textual representations for better tokenization. . 6 of the
    93 total prompts are open-ended prompts that elicit descriptions of a passage
    on a broad dimension of style:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 93 种不同的第一阶段提示运行两个阶段，涵盖所有 10,000 个帖子²²2我们将帖子预处理到前 25 个句子，并使用 emoji Python
    包将表情符号转换为文本表示，以便更好地进行标记化。总共 93 个提示中有 6 个是开放式提示，用于引出对段落在广泛风格维度上的描述：
- en: '1.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Write a long paragraph describing the unique
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 写一段长文描述以下段落的独特
- en: grammar style of the following passage…
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下段落的语法风格……
- en: '2.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Write a long paragraph describing the unique vocabulary style of the following
    passage…
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 写一段长文描述以下段落的独特词汇风格……
- en: '3.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Write a long paragraph describing the unique punctuation style of the following
    passage…
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 写一段长文描述以下段落的独特标点风格……
- en: '4.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: … see all 6 open-ended prompts in Appendix [A.1](#A1.SS1 "A.1 Open-ended Prompt
    Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs")
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ……请参见附录[A.1](#A1.SS1 "A.1 Open-ended Prompt Templates ‣ Appendix A Prompt Templates
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs")中的所有6个开放式提示
- en: 'The remaining 87 prompts target narrow and specific dimensions of style:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的87个提示集中于风格的狭窄和具体的维度：
- en: '1.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Write a description of whether the author of the following passage has any figurative
    language …
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 写一段描述以下段落的作者是否使用了任何隐喻语言……
- en: '2.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Write a description of whether the author of the following passage has any swear
    words …
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 写一段描述以下段落的作者是否使用了任何脏话……
- en: '3.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Write a description of whether the author of the following passage has any repeated
    words …
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 写一段描述以下段落的作者是否使用了任何重复词汇……
- en: '4.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: … see all 87 targeted prompts in Appendix [A.2](#A1.SS2 "A.2 Targeted Prompt
    Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs")
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ……请参见附录[A.2](#A1.SS2 "A.2 Targeted Prompt Templates ‣ Appendix A Prompt Templates
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs")中的所有87个目标提示
- en: The 87 targeted prompts are derived from surveys of stylometry literature, and
    they cover all of  (Tausczik and Pennebaker, [2010](#bib.bib55))’s linguistic
    and psychological categories. See Appendix [A.2](#A1.SS2 "A.2 Targeted Prompt
    Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs") for more details. We report the results of an ablation experiment
    between the two Stage 1 prompt categories in Appendix [C](#A3 "Appendix C Annotation
    Prompts Ablation ‣ Learning Interpretable Style Embeddings via Prompting LLMs").
    Appendix [D](#A4 "Appendix D StyleGenome Annotation Cost ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") details dataset annotation costs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 87个目标提示源自对风格计量学文献的调查，涵盖了(Tausczik和Pennebaker，[2010](#bib.bib55))的所有语言学和心理学类别。有关更多详细信息，请参见附录[A.2](#A1.SS2
    "A.2 Targeted Prompt Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs")。我们报告了附录[C](#A3 "Appendix C Annotation Prompts
    Ablation ‣ Learning Interpretable Style Embeddings via Prompting LLMs")中两种第一阶段提示类别的消融实验结果。附录[D](#A4
    "Appendix D StyleGenome Annotation Cost ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs")详细说明了数据集标注成本。
- en: StyleGenome
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: StyleGenome
- en: The output of Stage 2 is sentence tokenized³³3We use the sentence-splitter Python
    package. and filtered to keep only sentences beginning with “The author”. We refer
    to these sentences as human-interpretable style attributes. Our method annotates
    the texts with nearly 1.3M style attributes. These style attributes are represented
    in natural language so “The author creates a conversational tone” and “The author
    has a conversational tone” are counted separately in the raw dataset. Our training
    procedure in Section [3.1](#S3.SS1 "3.1 Sfam ‣ 3 Method ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") is able to train directly on these natural
    language style attributes, obviating a normalization step. Some annotations may
    be hallucinated resulting in a noisy dataset, but we choose to train on the full
    synthetic dataset, without manual intervention, to maintain an unsupervised procedure
    following prior work (Wang et al., [2022](#bib.bib57)). We hypothesize our model
    will find signal in the noise, which we evaluate in Section [4](#S4 "4 Results
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs"). The final dataset
    statistics can be found in Table [1](#S2.T1 "Table 1 ‣ StyleGenome ‣ 2 Generating
    StyleGenome ‣ Learning Interpretable Style Embeddings via Prompting LLMs").
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段的输出是句子分词³³3 我们使用 sentence-splitter Python 包。并过滤，只保留以 “作者” 开头的句子。我们将这些句子称为人类可解释的风格属性。我们的方法为文本标注了近
    1.3M 风格属性。这些风格属性以自然语言表示，因此“作者创造了对话语气”和“作者具有对话语气”在原始数据集中被分别计算。我们在第 [3.1](#S3.SS1
    "3.1 Sfam ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting LLMs")
    节中的训练过程能够直接在这些自然语言风格属性上训练，从而省略了标准化步骤。一些注释可能是虚构的，导致数据集嘈杂，但我们选择在完整的合成数据集上训练，无需人工干预，以保持无监督的程序，遵循先前的工作
    (Wang et al., [2022](#bib.bib57))。我们假设我们的模型会在噪声中找到信号，我们在第 [4](#S4 "4 Results ‣
    Learning Interpretable Style Embeddings via Prompting LLMs") 节中进行评估。最终数据集统计信息见表
    [1](#S2.T1 "Table 1 ‣ StyleGenome ‣ 2 Generating StyleGenome ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs")。
- en: '| # of Reddit Authors | 1,000 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Reddit 作者数量 | 1,000 |'
- en: '| # of Reddit Posts | 10,000 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Reddit 帖子数量 | 10,000 |'
- en: '| # of Interpretable Style Attributes | 1,255,874 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 可解释风格属性数量 | 1,255,874 |'
- en: '| # of (Text, Style Attribute) labeled pairs | 5,490,847 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| （文本，风格属性）标注对数量 | 5,490,847 |'
- en: 'Table 1: Statistics for the StyleGenome dataset.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：StyleGenome 数据集统计信息。
- en: 3 Method
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'We first distill stylometric annotation knowledge from GPT-3 into a Style Feature
    Agreement Model (Sfam). Given a text $t$ and a style attribute $a$ as input, $\textsc{Sfam}(t,a)$
    produces an agreement score between 0.0 and 1.0 representing the probability of
    the style attribute being present in the text. By selecting a set of $D$ style
    attributes $\{a_{0},a_{1},\ldots,a_{D}\}$, we can use Sfam to construct Lisa,
    our interpretable style representation model that produces $D$-dimensional vectors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将 GPT-3 中的风格计量注释知识提炼到风格特征一致性模型（Sfam）中。给定文本 $t$ 和风格属性 $a$ 作为输入，$\textsc{Sfam}(t,a)$
    生成一个介于 0.0 和 1.0 之间的一致性得分，表示文本中存在风格属性的概率。通过选择一组 $D$ 风格属性 $\{a_{0},a_{1},\ldots,a_{D}\}$，我们可以使用
    Sfam 构建 Lisa，我们的可解释风格表示模型，生成 $D$ 维向量：
- en: '|  | $\mathcal{M}_{\textsc{Lisa}}(t)=\bigl{(}{\textsc{Sfam}}(t,a_{0}),{\textsc{Sfam}}(t,a_{1}),\ldots,{\textsc{Sfam}}(t,a_{D})\bigr{)}$
    |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}_{\textsc{Lisa}}(t)=\bigl{(}{\textsc{Sfam}}(t,a_{0}),{\textsc{Sfam}}(t,a_{1}),\ldots,{\textsc{Sfam}}(t,a_{D})\bigr{)}$
    |  |'
- en: The Euclidean distance between style vectors for two texts ${\lVert\mathcal{M}_{\textsc{Lisa}}(t_{2})-\mathcal{M}_{\textsc{Lisa}}(t_{1})\rVert}_{2}$
    would not be particularly meaningful. We can multiply a trained weight vector
    $w$ or a weight matrix $W$ to the style vectors, that act as simple interpretable
    embedding layers. This operation would make the Euclidean distance more meaningful,
    for example ${\lVert\mathcal{M}_{\textsc{Lisa}}(t_{2})*w-\mathcal{M}_{\textsc{Lisa}}(t_{1})*w\rVert}_{2}$.
    We call the result of a Lisa style vector multiplied by $w$ or $W$ a Lisa style
    embedding. We discuss training in detail next, leaving hyperparameter and implementation
    specifics in Appendix [E](#A5 "Appendix E Training Details ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs").
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 两个文本的风格向量之间的欧几里得距离 ${\lVert\mathcal{M}_{\textsc{Lisa}}(t_{2})-\mathcal{M}_{\textsc{Lisa}}(t_{1})\rVert}_{2}$
    并不特别有意义。我们可以将训练的权重向量 $w$ 或权重矩阵 $W$ 乘以风格向量，这些权重作为简单的可解释嵌入层。这个操作会使欧几里得距离更有意义，例如
    ${\lVert\mathcal{M}_{\textsc{Lisa}}(t_{2})*w-\mathcal{M}_{\textsc{Lisa}}(t_{1})*w\rVert}_{2}$。我们将
    Lisa 风格向量乘以 $w$ 或 $W$ 的结果称为 Lisa 风格嵌入。我们接下来详细讨论训练，将超参数和实现细节留在附录 [E](#A5 "Appendix
    E Training Details ‣ Learning Interpretable Style Embeddings via Prompting LLMs")。
- en: '| Type | Dataset | Style Attribute | Spearman Correlation ($\rho$) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 数据集 | 风格属性 | 斯皮尔曼相关系数 ($\rho$) |'
- en: '| Formality | Formality in Online Communication | The author uses informal
    language. | 0.599 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 正式性 | 在线交流中的正式性 | 作者使用了非正式语言。 | 0.599 |'
- en: '| Grammarly’s Yahoo Answers Formality Corpus | 0.200 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Grammarly 的 Yahoo Answers 正式性语料库 | 0.200 |'
- en: '| Sentiment | Yelp Reviews | The author uses a negative tone. | 0.788 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 情感 | Yelp 评论 | 作者使用了消极的语气。 | 0.788 |'
- en: '| IMDB Large Movie Review Dataset | 0.665 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| IMDB 大型电影评论数据集 | 0.665 |'
- en: '| …abbreviated for space, see Appendix [F](#A6 "Appendix F Full Sfam Evaluation
    Results ‣ Learning Interpretable Style Embeddings via Prompting LLMs") for full
    results |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| …由于篇幅限制，详见附录[F](#A6 "附录 F 完整的 Sfam 评估结果 ‣ 通过提示 LLMs 学习可解释的风格嵌入") |  |'
- en: '| Emotion | DAIR.AI Emotion (Love vs. Anger) | The author is expressing {{emotion}}.
    | 0.542 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 情感 | DAIR.AI 情感（爱与愤怒） | 作者表达了{{emotion}}。 | 0.542 |'
- en: '| DAIR.AI Emotion (Joy vs. Sad) | 0.531 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| DAIR.AI 情感（快乐与悲伤） | 0.531 |'
- en: '| GoEmotions (Love vs. Anger) | 0.769 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions（爱与愤怒） | 0.769 |'
- en: '| GoEmotions (Joy vs. Sadness) | 0.639 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions（快乐与悲伤） | 0.639 |'
- en: '| GoEmotions (Disgust vs. Desire) | 0.630 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions（厌恶与欲望） | 0.630 |'
- en: '| …abbreviated for space, see Appendix [F](#A6 "Appendix F Full Sfam Evaluation
    Results ‣ Learning Interpretable Style Embeddings via Prompting LLMs") for full
    results |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| …由于篇幅限制，详见附录[F](#A6 "附录 F 完整的 Sfam 评估结果 ‣ 通过提示 LLMs 学习可解释的风格嵌入") |  |'
- en: '| Author Profiling | Political Slant | The author is a Democrat. | 0.005 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 作者画像 | 政治倾向 | 作者是民主党人。 | 0.005 |'
- en: '| Twitter User Gender Classification | The author is female. | 0.166 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Twitter 用户性别分类 | 作者是女性。 | 0.166 |'
- en: '| African-American Vernacular English | The author uses African-American Vernacular
    English. | 0.238 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 非裔美国口语英语 | 作者使用了非裔美国口语英语。 | 0.238 |'
- en: '| Shakespeare (Early Modern English) | The author uses Early Modern English.
    | 0.108 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 莎士比亚（早期现代英语） | 作者使用了早期现代英语。 | 0.108 |'
- en: '| Wikipedia Bias | The author has a biased point of view. | 0.014 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科偏见 | 作者有偏见的观点。 | 0.014 |'
- en: '| Harmful Speech | HateSpeech18 | The author’s writing contains hate speech.
    | 0.229 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 有害言论 | HateSpeech18 | 作者的写作包含仇恨言论。 | 0.229 |'
- en: '| Offensive Social Media | The author uses offensive language. | 0.401 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 冒犯性社交媒体 | 作者使用了冒犯性语言。 | 0.401 |'
- en: '| Text Simplification | Simple Wikipedia | The author uses simple language.
    | 0.043 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 文本简化 | 简单维基百科 | 作者使用了简单语言。 | 0.043 |'
- en: '| ASSET | 0.050 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| ASSET | 0.050 |'
- en: '| Linguistic Acceptability | CoLA | The author uses incorrect grammar. | 0.078
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 语言接受度 | CoLA | 作者使用了不正确的语法。 | 0.078 |'
- en: '| BLiMP | 0.020 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| BLiMP | 0.020 |'
- en: '| Average |  |  | 0.342 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 |  |  | 0.342 |'
- en: 'Table 2: Correlation of agreement scores produced by Sfam against human judgments
    on texts over a wide variety linguistic and authorship dimensions. The natural
    language style attributes used as input to Sfam when producing the agreement scores
    for each dataset are also provided.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：Sfam 生成的协议分数与人类对文本在各种语言和作者维度上的判断的相关性。用于生成每个数据集协议分数的自然语言风格属性也被提供。
- en: 3.1 Sfam
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 Sfam
- en: We use distillation (Ba and Caruana, [2014](#bib.bib5)) to teach the stylometric
    annotation capabilities of GPT-3 to EncT5⁴⁴4We use the t5-base model and truncate
    at 512 tokens. (Liu et al., [2021](#bib.bib31); Raffel et al., [2020](#bib.bib44)),
    a smaller, more efficient student model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用蒸馏（Ba 和 Caruana，[2014](#bib.bib5)）来教授 GPT-3 在 EncT5⁴⁴4 上的风格标注能力。我们使用 t5-base
    模型，并在 512 个标记处截断。（Liu 等，[2021](#bib.bib31)；Raffel 等，[2020](#bib.bib44)），一个更小、更高效的学生模型。
- en: Sampling Batches
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 采样批次
- en: We train EncT5 with a binary classifier head on randomly sampled batches of
    examples $(x_{i},y_{i})$ where each batch contains an equal number of positive
    ($y_{i}=1$) and negative ($y_{i}=0$) examples. The input $x_{i}$ consists of a
    style attribute $a$ and an author’s text $t$ concatenated in a string “{{a}}|||{{t}}”,
    for example $x_{i}=$ “The author is using a positive tone.|||You got this ;)”.
    Labeled pairs from StyleGenome are sampled as positive examples such that each
    style attribute is sampled with equal probability. For each positive example,
    we perform negative sampling and retrieve a negative example text where the positive
    example’s style attribute is likely not present. To do this, we find the 10,000
    most dissimilar style attributes to the positive example’s style attribute with
    SBERT⁵⁵5We use the nli-distilroberta-base-v2 model (Sanh et al., [2019](#bib.bib50);
    Liu et al., [2019](#bib.bib32); Devlin et al., [2019](#bib.bib14); Reimers and
    Gurevych, [2019](#bib.bib46)) for all SBERT usage in this paper. similarity. We
    select a text that is positively labeled with a randomly selected dissimilar style
    attribute as the negative example text.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用二分类器头对EncT5进行训练，样本批次$(x_{i},y_{i})$是随机抽样的，其中每个批次包含相同数量的正例（$y_{i}=1$）和负例（$y_{i}=0$）。输入$x_{i}$由样式属性$a$和作者的文本$t$串联成字符串“{{a}}|||{{t}}”组成，例如$x_{i}=$
    “作者使用了积极的语气。|||你能做到的 ;)”。从StyleGenome中抽取的标记对作为正例，以便每个样式属性都有相同的抽样概率。对于每个正例，我们进行负样本抽样，检索一个负样本文本，其中正例的样式属性可能不存在。为此，我们使用SBERT找出与正例样式属性最不相似的10,000个样式属性。我们选择一个用随机选择的不同样式属性正面标记的文本作为负样本文本。
- en: '| Text | Top 5 Lisa Vector Dimensions |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 前五名莉莎向量维度 |'
- en: '| --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Lol right on | 1\. 1.00 – The author is being polite. 2\. 1.00 – The author
    is writing in a cheerful manner.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '| 哈哈对了 | 1\. 1.00 – 作者在表现礼貌。 2\. 1.00 – 作者以愉快的方式写作。'
- en: 3\. 1.00 – The author is using a lighthearted tone.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 1.00 – 作者使用了轻松的语气。
- en: 4\. 1.00 – The author is laughing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 1.00 – 作者在笑。
- en: 5\. 1.00 – The author is complimentary. |
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 1.00 – 作者在赞美。
- en: '| There is the Toyota GT86 R3 ;) http://www.toyota-motorsport.com/motorsport/downloads/com_droppics/59/
    DSF4311.jpg | 1\. 0.99 – The author is simply describing a product. $\ast$ 2\.
    0.99 – The author is providing a visual cue to the reader.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '| 这里是丰田GT86 R3 ;) http://www.toyota-motorsport.com/motorsport/downloads/com_droppics/59/
    DSF4311.jpg | 1\. 0.99 – 作者只是描述了一个产品。 $\ast$ 2\. 0.99 – 作者向读者提供了视觉提示。'
- en: 3\. 0.98 – The author simply provides information.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 0.98 – 作者只是提供信息。
- en: 4\. 0.97 – The author is using an emoji.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 0.97 – 作者使用了表情符号。
- en: 5\. 0.97 – The author provides information. |
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 0.97 – 作者提供了信息。
- en: '| Every time i watched this episode as a kid i was always like "WTF, JAMES,
    A POKEBALL ISNT EVEN A POKEMON?! GET YOUR ACT TOGETHER, SON!!" | 1\. 1.00 – The
    author is discussing a television show. $\ast$ 2\. 0.99 – The author has a hint
    of nostalgia.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '| 每次我小时候看这个剧集时，总是会想：“卧槽，詹姆斯，一个精灵球甚至不是一个宝可梦？！赶紧整理好你的状态，儿子！！” | 1\. 1.00 – 作者在讨论一部电视节目。
    $\ast$ 2\. 0.99 – 作者带有一丝怀旧情感。'
- en: 3\. 0.99 – The author is making a humorous comment.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 0.99 – 作者在进行幽默评论。
- en: 4\. 0.99 – The author is animated in their writing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 0.99 – 作者在写作中充满生气。
- en: 5\. 0.99 – The author is laughing. |
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 0.99 – 作者在笑。
- en: '| 13 POT MORDE BABY WOOOOOOOOOOOOOOOO | 1\. 1.00 – The author is using an elongated
    word. 2\. 1.00 – The author is using only English words.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '| 13 POT MORDE BABY WOOOOOOOOOOOOOOOO | 1\. 1.00 – 作者使用了一个延长的词。 2\. 1.00 –
    作者只使用了英文单词。'
- en: 3\. 1.00 – The author is using a single word.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 1.00 – 作者使用了一个单词。
- en: 4\. 1.00 – The author is using swear words.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 1.00 – 作者使用了脏话。
- en: 5\. 1.00 – The author uses two exclamation marks. |
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 1.00 – 作者使用了两个感叹号。
- en: '| No wonder everyone resorts to performing a murder spree eventually. | 1\.
    1.00 – The author is scornful. 2\. 1.00 – The author is ungenerous.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '| 难怪每个人最终都会选择大规模杀戮。 | 1\. 1.00 – 作者在表现轻蔑。 2\. 1.00 – 作者表现得小气。'
- en: 3\. 0.99 – The author is expressing antisocial behaviors.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 0.99 – 作者表现出反社会行为。
- en: 4\. 0.99 – The author is uncaring.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 0.99 – 作者表现得漠不关心。
- en: 5\. 0.99 – The author is dramatic. |
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 0.99 – 作者表现得戏剧化。
- en: '| Podcast originally refers to an iPod, and before that there was definitely
    TWiT, which still calls itself a Netcast | 1\. 0.97 – The author uses a variety
    of words to describe              the same concept. 2\. 0.97 – The author is simply
    describing a product. $\ast$'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '| 播客最初指的是 iPod，在此之前肯定有 TWiT，它仍然称自己为 Netcast | 1\. 0.97 – 作者使用了多种词汇来描述同一概念。
    2\. 0.97 – 作者只是描述了一种产品。 $\ast$'
- en: 3. 0.95 – The author uses specific terms related to the              topic.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 0.95 – 作者使用了与主题相关的具体术语。
- en: 4\. 0.94 – The author has a deep understanding of the topic.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 0.94 – 作者对该主题有深入的理解。
- en: 5\. 0.94 – The author is using words focusing on the past. |
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 0.94 – 作者使用了关注过去的词汇。|
- en: 'Table 3: The five highest scoring dimensions from the 768-dimensional Lisa
    vector produced on various Reddit texts. The interpretable style attribute corresponding
    to each dimension is displayed along with the score. We manually inspect the top
    style attributes and annotate them as reasonable, plausible, or incorrect. Attributes
    annotated with $\ast$ blur the line between style and content. Error analysis
    can be found in Section [4.1](#S4.SS1 "4.1 Error Analysis ‣ 4 Results ‣ Learning
    Interpretable Style Embeddings via Prompting LLMs").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：从在各种 Reddit 文本上生成的 768 维 Lisa 向量中得出的五个最高评分维度。每个维度对应的可解释风格属性和评分一起显示。我们手动检查顶级风格属性，并将其标记为合理、可信或不正确。标记为
    $\ast$ 的属性模糊了风格和内容之间的界限。错误分析见第 [4.1](#S4.SS1 "4.1 错误分析 ‣ 4 结果 ‣ 通过提示 LLMs 学习可解释风格嵌入")
    节。
- en: Training and Inference
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练与推理
- en: Training over the ~1.3M unique style attributes in StyleGenome, our training
    dataset for Sfam is effectively a multitask mixture. Style attributes presented
    in natural language to the model allows the pre-trained T5 encoder to jointly
    learn between style attributes and generalize to unseen style attributes using
    the semantic information in those natural language descriptions. This is especially
    desirable since some style attributes only have a handful of text examples, while
    others may have thousands. This setup resembles the multitask mixture trained
    on in Raffel et al. ([2020](#bib.bib44)). To validate training, we hold-out 50
    random style attributes that have between 30-50 examples each as a validation
    set. We validate learning during training by measuring the ability of Sfam to
    generalize and produce accurate agreement scores for the unseen style attributes.
    At inference, we softmax the binary class logits to interpret them as probabilities
    and we take the probability of $y_{i}=1$ as the agreement score. We also study
    the effect of the size of StyleGenome on performance and find that as the synthetic
    dataset grows, validation performance improves and Sfam generalizes to better
    predict agreement scores for unseen style attribute and text pairs (see Appendix
    [B](#A2 "Appendix B Effect of StyleGenome Dataset Size ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs")).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在 StyleGenome 中训练大约 130 万个独特风格属性，我们的 Sfam 训练数据集实际上是一个多任务混合体。模型以自然语言呈现的风格属性使得预训练的
    T5 编码器能够共同学习风格属性，并利用这些自然语言描述中的语义信息对未见过的风格属性进行泛化。这尤其是理想的，因为某些风格属性只有少量的文本示例，而其他属性可能有成千上万的示例。这种设置类似于
    Raffel 等人（[2020](#bib.bib44)）训练的多任务混合体。为了验证训练效果，我们将 50 个随机风格属性（每个属性有 30-50 个示例）作为验证集。在训练过程中，我们通过测量
    Sfam 对未见风格属性进行泛化和产生准确一致性评分的能力来验证学习效果。在推理时，我们对二分类类 logits 进行 softmax 处理，将其解释为概率，并将
    $y_{i}=1$ 的概率作为一致性评分。我们还研究了 StyleGenome 的大小对性能的影响，发现随着合成数据集的增长，验证性能有所改善，Sfam 对未见风格属性和文本对的预测一致性评分也更为准确（参见附录
    [B](#A2 "附录 B StyleGenome 数据集大小的影响 ‣ 通过提示 LLMs 学习可解释风格嵌入")）。
- en: 3.2 Lisa Style Vectors
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Lisa 风格向量
- en: As discussed earlier, Sfam is directly used to produce the Lisa interpretable
    style vectors. We arbitrarily choose $D=768$ in this work, following the dimensionality
    of prior style vectors and BERT (Devlin et al., [2019](#bib.bib14)). We now detail
    how we select the style attributes associated with each dimension $\{a_{0},a_{1},\ldots,a_{768}\}$
    with little manual intervention. The first 87 attributes $\{a_{0},a_{1},\ldots,a_{86}\}$
    directly correspond to the features of our 87 targeted prompts in the form of
    “The author is using {{targeted_feature}}”. The remaining 681 are downselected
    from the ~1.3M style attributes with filtering heuristics, choosing those that
    appear for at least 10 authors, but no more than 600 authors (attempting to select
    for frequent, yet discriminative attributes). Once a style attribute is selected
    to be part of the 768, we do not select another style attribute with SBERT cosine
    similarity $> to largely avoid near-duplicates. We also reject style attributes
    for selection that are undesirable for interpretability.⁶⁶6We reject style attributes
    that are  的风格属性，以尽量避免近似重复。我们还排除了那些对可解释性不佳的风格属性。我们排除的风格属性包括包含  10,000
    posts), more diverse sources of texts, and larger and more performant LLMs may
    further broaden and improve learned stylometric annotation capabilities.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[2](#S3.T2 "表 2 ‣ 3 方法 ‣ 通过提示 LLMs 学习可解释风格嵌入")中对21个数据集进行了55项广泛的研究，涉及7个不同的语言风格和作者身份维度。我们测量了Sfam的一致性评分与人工判断的相关性。Sfam在形式性、情感和情绪等维度上的表现优于语言可接受性等维度。这可能是因为GPT-3在标注这些类别时的有效性，这一结果符合之前的研究，表明语言模型在识别这些特征时存在困难（Warstadt等，[2020](#bib.bib58)）。有趣的是，Sfam展示了一些有限的作者身份分析能力，这一任务与风格学相邻。以可解释的方式探究Sfam有助于识别其能够可靠表示的特征类别，而之前的方法则较为不透明。总体而言，表[2](#S3.T2
    "表 2 ‣ 3 方法 ‣ 通过提示 LLMs 学习可解释风格嵌入")的结果表明，Sfam的标注在一些重要的风格维度上确实与人工判断相关。我们假设，未来使用更大的数据集（>
    10,000条帖子）、更多样化的文本来源以及更大、更高效的LLMs的研究，可能会进一步拓宽和改善学习的风格学注释能力。 |
- en: '| Text | Style Attribute |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 风格属性 |'
- en: '| --- | --- |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Subscribed. Interesting idea. I would like to see some advanced stats on
    hitting percentages to different locations on the court. For example, having the
    court broken up into maybe 12 zones and then hitting percentages from each position
    to those zones. I remember seeing an article that did this years ago and I have
    never been able to find anything online. I said recently on this sub that the
    deep angle shot from the left side or right side was the highest percentage shot
    in volleyball, but I was not able to back up my claim with any sources or anything.
    Anyways, I am a VB nerd, no doubt. Interested to see what direction you take this.
    Cheers! | The author is being polite. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 订阅了。有趣的想法。我想看到一些关于击球百分比在场地不同位置的高级统计数据。例如，将场地划分为大约12个区域，然后从每个位置击球到这些区域的百分比。我记得多年前看到过一篇做这种分析的文章，但一直找不到相关的在线资料。我最近在这个子版块说过，从左侧或右侧发出的深角度射门是排球中百分比最高的射门，但我无法用任何来源来支持我的说法。不管怎样，我毫无疑问是个排球迷。很想看看你会如何处理这个问题。祝好！
    | 作者在礼貌地表达意见。 |'
- en: '| Yeah I also work in QA, and seeing this kind of stuff get released is maddening.
    About a year ago working on a new platform we were seeing bugs in the hundreds
    each week, we pushed back the release of the product 3 months because basically
    it didn’t work. If it was up to the devs, they’d have released it on time, because
    the stuff they’d written code for worked. Thorough doesn’t even cover the work
    we go through every 3 months, and Niantic’s approach seems completely amateur
    from this side. They’re putting bandaids on problems and hiding things like the
    3 step problem behind curtains without seemingly fixing anything, although I do
    have to say their balance tweaks to battling have been a big step in the right
    direction. | The author is using a personal anecdote to illustrate their point.
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 是的，我也在做QA，看到这种问题发布出来真的让人抓狂。大约一年前，我们在开发一个新平台时，每周看到数百个错误，我们推迟了产品的发布3个月，因为基本上它无法正常工作。如果由开发人员决定，他们会按时发布，因为他们编写的代码是有效的。我们每三个月的工作量都超过了彻底的程度，而Niantic的做法在我看来完全是业余的。他们只是对问题进行应急处理，把像三步问题这样的东西藏在幕布后面，似乎没有真正修复任何东西，尽管我必须说他们对战斗的平衡调整确实是朝着正确方向迈出的重要一步。
    | 作者通过个人轶事来说明他们的观点。 |'
- en: '| Thank you. I’d be interested in reading more about your experiences, in addition
    to the "American Wedding" story. Are you watching the stream? I wish there was
    a way to find out how many people in the world are watching it. The music is lovely,
    huh? God damn. He’s got his bunny Fair Isle sweater on, drinking Dunkin’ Donuts
    coffee. I would have thought him a Starbucks man. :-) | The author is using an
    emoji. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 谢谢。我很想阅读更多关于你的经历的内容，除了“美国婚礼”的故事之外。你在看直播吗？我希望能找到一种方法来了解全世界有多少人在观看。音乐很美妙，对吧？天啊。他穿着他的兔子
    Fair Isle 毛衣，喝着 Dunkin’ Donuts 咖啡。我本以为他是星巴克的常客。 :-) | 作者使用了一个表情符号。 |'
- en: 'Table 5: Sentence-level Lisa vectors over each sentence from a longer passage
    of text can help identify and quantify which sentences contribute to overall style
    attributes scored on the longer passage providing granular interpretability.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：从较长的文本段落中提取的句子级别 Lisa 向量可以帮助识别和量化哪些句子对总体风格属性的评分有贡献，从而提供详细的可解释性。
- en: STEL
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: STEL
- en: In Table [4](#S3.T4 "Table 4 ‣ 3.2 Lisa Style Vectors ‣ 3 Method ‣ Learning
    Interpretable Style Embeddings via Prompting LLMs"), we provide the results of
    evaluating Lisa using STEL. The STEL task evaluates whether two texts with similar
    styles can be matched using the distance/similarity metric defined by a style
    representation. We compare with other content-independent style representations,
    or methods that explicitly limit representation of content in favor of style.
    Lisa explicitly limits the representation of content through the 768 style-focused
    attributes that act as a bottleneck. Content-aware representations like SBERT,
    on the other hand, have direct access to the text and may be able to represent
    the content in the text to an extreme degree, representing the usage of a specific
    rare word or discussion of a specific concept. We provide the results of content-aware
    representations simply for reference. We find Lisa embeddings are able to closely
    match (and on average slightly outperform) prior style representations on STEL
    while providing interpretability.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [4](#S3.T4 "Table 4 ‣ 3.2 Lisa Style Vectors ‣ 3 Method ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") 中，我们提供了使用 STEL 评估 Lisa 的结果。STEL 任务评估了是否可以通过由风格表示定义的距离/相似性度量来匹配两个具有相似风格的文本。我们与其他内容无关的风格表示进行比较，或者与那些明确限制内容表示以优先考虑风格的方法进行比较。Lisa
    通过768个专注于风格的属性明确限制了内容表示，这些属性充当了瓶颈。而像 SBERT 这样的内容感知表示则直接访问文本，并可能能够极端地表示文本中的内容，例如具体稀有词汇的使用或特定概念的讨论。我们提供内容感知表示的结果仅供参考。我们发现
    Lisa 嵌入能够紧密匹配（并且平均略微超越）先前的风格表示，并且提供了可解释性。
- en: '| Texts | Top 3 Common/Distinct Style Attributes |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 前 3 名常见/独特的风格属性 |'
- en: '| --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Anchor: Devices that use two pronged instead of three pronged plugs are required
    to meet certain safe design requirements. Among other things, if a device has
    a switch, the switched line MUST BE hot, not neutral. The polarized plugs make
    sure that the right prong/wire is hot. This is why devices that have no switches
    (primarily wall warts) need not have polarized plugs. |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 锚点：使用两插头而不是三插头的设备必须符合某些安全设计要求。除此之外，如果设备有开关，开关线必须是热线，而不是中性线。极性插头确保正确的插头/线是热的。这就是为什么没有开关的设备（主要是墙插电源适配器）不需要极性插头。
    |  |'
- en: '| Same Author: Your diaphragm would be trying to contract against the air pressure
    in your lungs. That’s why deep sea diving requires regulators, to match the pressure
    of the air supply to the pressure surrounding your rib cage. You can breathe against
    a maximum of about 1/2 PSI, which is not enough pressure to adequately oxygenate
    your blood. | (0.89,  1.00)  –  The author is using a scientific approach. (0.96,
    0.98) – The author is using a combination of                     technical terms
    and everyday language.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '| 同一作者：你的隔膜会试图对抗你肺中的空气压力。这就是为什么深海潜水需要调节器，以使空气供应的压力与你胸腔周围的压力相匹配。你可以在最大约 1/2
    PSI 的压力下呼吸，这不足以有效地氧化你的血液。 | (0.89,  1.00)  –  作者使用了科学方法。 (0.96, 0.98) – 作者使用了技术术语和日常语言的结合。
    |'
- en: (0.91, 0.84)   –  The author is using formal and
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (0.91, 0.84)   –  作者使用了正式的
- en: professional language. |
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 专业语言。 |
- en: '| Different Author: That’s great! I’m glad it seems to be finding its’ niche.
    Now if they could just make a Star Wars version of this game, I’d happily swallow
    that fat learning curve and overcome my frustrations with the combat system. ;)
    | (0.06, 0.99)  – The author is using words related to the                     game
    they are discussing. $\ast$ (0.00, 0.88)   –  The author is using an emoji.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '| 不同作者：太棒了！我很高兴它似乎找到了自己的定位。如果他们能做一个《星球大战》的游戏版本，我愿意忍受那个陡峭的学习曲线，克服对战斗系统的挫折。;)
    | (0.06, 0.99) – 作者使用了与他们讨论的游戏相关的词汇。$\ast$ (0.00, 0.88) – 作者使用了一个表情符号。'
- en: (0.02, 0.87)   –  The author uses an emoticon at the end. |
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: (0.02, 0.87) – 作者在结尾使用了一个表情符号。
- en: 'Table 6: Example interpretable explanations on the Contrastive Authorship Verification
    task. The top style attributes in common between the Anchor text and a text by
    the Same Author are shown. The top distinct style attributes between the Anchor
    text and a text by a Different Author are also shown. The scores of each style
    attribute against the texts is shown in (•,  •/•). Attributes annotated with $\ast$
    blur the line between style and content. Error analysis can be found in Section
    [4.1](#S4.SS1 "4.1 Error Analysis ‣ 4 Results ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs"). Further examples and details on style attribute ranking
    can be found in Appendix [G](#A7 "Appendix G Interpretable Authorship Verification
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 6：对比作者验证任务中的示例可解释解释。显示了锚文本和同一作者文本之间的主要风格属性。还显示了锚文本和不同作者文本之间的主要独特风格属性。每个风格属性与文本的得分显示在（•,
    •/•）中。标注为$\ast$的属性模糊了风格和内容之间的界限。错误分析请参见第 [4.1](#S4.SS1 "4.1 错误分析 ‣ 4 结果 ‣ 通过提示
    LLM 学习可解释的风格嵌入") 节。有关风格属性排名的更多示例和详细信息，请参见附录 [G](#A7 "附录 G 可解释的作者身份验证 ‣ 通过提示 LLM
    学习可解释的风格嵌入")。
- en: Sentence-Level Interpretability
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 句子级可解释性
- en: In Table [5](#S4.T5 "Table 5 ‣ Correlation to Human Judgments ‣ 4 Results ‣
    Learning Interpretable Style Embeddings via Prompting LLMs"), we demonstrate how
    visualizing a dimension of sentence-level Lisa vectors can help explain which
    sentences contribute to a dimension activated on a passage-level Lisa vector.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [5](#S4.T5 "表格 5 ‣ 与人类判断的相关性 ‣ 4 结果 ‣ 通过提示 LLM 学习可解释的风格嵌入") 中，我们展示了如何可视化句子级别的
    Lisa 向量的一个维度，来解释哪些句子对激活段落级别的 Lisa 向量的维度有所贡献。
- en: Forensic Interpretability
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 法医可解释性
- en: Typically for authorship attribution tasks, content-aware representations that
    capture both content and style are used to make a determination. Author style,
    however, is still an important component in determining attribution (Rivera-Soto
    et al., [2021](#bib.bib48)). Offering a clear explanation and presenting supporting
    evidence is crucial, particularly in the context of forensic analysis, such as
    when presenting evidence in a court trial. Explainability has often been overlooked
    in neural approaches to authorship attribution tasks. To motivate this as a future
    research direction using our interpretable stylometric representations and our
    general approach, we provide an example of explanations on the Contrastive Authorship
    Verification task from Wegmann et al. ([2022](#bib.bib61)) in Table [6](#S4.T6
    "Table 6 ‣ STEL ‣ 4 Results ‣ Learning Interpretable Style Embeddings via Prompting
    LLMs") with Lisa (LUAR + $W$). Further examples and discussion on how the top
    common and distinct style attributes are ranked can be found in Appendix [G](#A7
    "Appendix G Interpretable Authorship Verification ‣ Learning Interpretable Style
    Embeddings via Prompting LLMs").
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通常对于作者身份归属任务，使用能够捕捉内容和风格的内容感知表示来进行判断。然而，作者风格在确定归属时仍然是一个重要的组成部分（Rivera-Soto et
    al., [2021](#bib.bib48)）。提供明确的解释和支持证据至关重要，特别是在法医分析的背景下，例如在法庭审判中呈现证据。解释性在神经网络作者身份归属任务中经常被忽视。为了将这一点作为未来的研究方向，利用我们可解释的风格度量表示和我们的一般方法，我们在表格
    [6](#S4.T6 "表格 6 ‣ STEL ‣ 4 结果 ‣ 通过提示 LLM 学习可解释的风格嵌入") 中提供了 Wegmann et al. ([2022](#bib.bib61))
    在对比作者验证任务中的解释示例，使用了 Lisa (LUAR + $W$)。有关如何排名前列的共同和独特风格属性的更多示例和讨论，请参见附录 [G](#A7
    "附录 G 可解释的作者身份验证 ‣ 通过提示 LLM 学习可解释的风格嵌入")。
- en: 4.1 Error Analysis
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 错误分析
- en: We highlight insights and observations around common failure modes of our technique
    in this section. We annotate the common failure modes with their percentage rate
    of occurrence⁷⁷7We manually inspect a small sample set of 2,000 style attribute
    annotations (the top 20 style attributes for 100 random texts) by Lisa..
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一部分突出了我们技术的常见失败模式的见解和观察。我们用发生率百分比标注了这些常见的失败模式⁷⁷7 我们手动检查了一小部分样本集，包含2000个风格属性标注（100个随机文本的前20个风格属性）。
- en: Content vs. Style Attributes (3%)
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内容与风格属性（3%）
- en: It is unclear whether style and content can truly be separated as some content
    features are important for style or profiling an author (Jafaritazehjani et al.,
    [2020](#bib.bib24); Bischoff et al., [2020](#bib.bib6); Patel et al., [2022](#bib.bib39)).
    Even after filtering, 3% of dimensions of Lisa still represent content. For example,
    “The author is using words related to the game they are discussing”. However,
    while Lisa may have the ability to represent that two texts are both discussing
    the topic of video games, it does not have the direct ability a content-aware
    approach would of representing which specific video game is being discussed, due
    to the limited set of 768 features that act as a bottleneck. Our approach also
    allows visibility into understanding how much of the representation derives from
    content-related features, while other neural representations are opaque and may
    use content-related features in a way that cannot be easily assessed.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 是否能够真正将风格和内容分开尚不清楚，因为某些内容特征对于风格或作者的个人资料是重要的（Jafaritazehjani et al., [2020](#bib.bib24);
    Bischoff et al., [2020](#bib.bib6); Patel et al., [2022](#bib.bib39)）。即使在过滤之后，Lisa
    的 3% 维度仍代表内容。例如，“作者使用了与他们讨论的游戏相关的词汇”。然而，尽管 Lisa 可能能够表示两个文本都在讨论视频游戏的主题，但由于 768
    个特征的限制，它并不能直接表示正在讨论的是哪个具体的视频游戏。我们的方法还允许了解表示中有多少来自内容相关特征，而其他神经表示则不透明，可能以无法轻易评估的方式使用内容相关特征。
- en: Conflating Style Attributes with Content (2%)
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将风格属性与内容混淆（2%）
- en: For some style attributes, Lisa conflates the content of text with the presence
    of the style attribute. For example, “The author is cautious”, may have a high
    agreement score on any text containing the word “caution” even if the author is
    not actually expressing caution in the text.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些风格属性，Lisa 将文本内容与风格属性的存在混为一谈。例如，“作者是谨慎的”可能会在包含“谨慎”一词的任何文本中得到较高的一致性得分，即使作者在文本中并未实际表达谨慎。
- en: Spurious Correlations (6%)
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 虚假相关性（6%）
- en: For other style attributes, Lisa has learned spurious correlations. For example,
    “The author uses two exclamation marks”, often has a high agreement score on any
    text that is exclamatory in nature, but does not actually use exclamation marks.
    An example can be found in Table [3](#S3.T3 "Table 3 ‣ Sampling Batches ‣ 3.1
    Sfam ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting LLMs").
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他风格属性，Lisa 已经学到了虚假的相关性。例如，“作者使用了两个感叹号”通常在任何具有感叹性质的文本中都会有较高的一致性得分，但实际上并未使用感叹号。一个例子可以在表格
    [3](#S3.T3 "Table 3 ‣ Sampling Batches ‣ 3.1 Sfam ‣ 3 Method ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") 中找到。
- en: Fundamental Errors (10%)
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本错误（10%）
- en: Lisa sometimes produces a high agreement score for text displaying the polar
    opposite of a style attribute or produces a high agreement score for an attribute
    that simply is not present in the text. Table [3](#S3.T3 "Table 3 ‣ Sampling Batches
    ‣ 3.1 Sfam ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting
    LLMs") demonstrates some of these incorrect examples. Inspecting our dataset,
    this error happens both due to EncT5’s internal representations likely aligning
    on relatedness instead of similarity (Hill et al., [2015](#bib.bib20)) and due
    to hallucination and annotation errors by GPT-3\. Hallucinated generations is
    a common issue with any LLM-guided approach and we discuss it further in [Limitations](#Sx1
    "Limitations and Broader Impacts ‣ Learning Interpretable Style Embeddings via
    Prompting LLMs") along with potential future mitigations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Lisa 有时会对展现与风格属性完全相反的文本产生较高的一致性得分，或者对文本中根本不存在的属性产生较高的一致性得分。表格 [3](#S3.T3 "Table
    3 ‣ Sampling Batches ‣ 3.1 Sfam ‣ 3 Method ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs") 展示了一些这些错误的例子。检查我们的数据集时，这种错误发生的原因既可能是由于 EncT5 的内部表示可能在相关性上对齐而非相似性（Hill
    et al., [2015](#bib.bib20)），也可能是 GPT-3 的幻觉和标注错误。幻觉生成是任何 LLM 指导方法的常见问题，我们在 [局限性](#Sx1
    "Limitations and Broader Impacts ‣ Learning Interpretable Style Embeddings via
    Prompting LLMs") 中进一步讨论了这个问题及潜在的未来缓解措施。
- en: 5 Conclusion
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we propose a promising novel approach to learning interpretable
    style representations. To overcome a lack of stylometrically annotated training
    data, we use a LLM to generate StyleGenome, a synthetic stylometry dataset. Our
    approach distills the stylometric knowledge from StyleGenome into two models,
    Sfam and Lisa. We find that these models learn style representations that match
    the performance of recent direct neural approaches and introduce interpretability
    grounded in explanations that correlate with human judgments. Our approach builds
    towards a research direction focused on making style representations more useful
    for downstream applications where such properties are desirable such as in a forensic
    analysis context. Future directions that introduce human-in-the-loop supervised
    annotations or newer, larger, and better aligned LLMs for annotation have the
    potential to yield further gains in both performance and interpretability.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种有前景的新方法来学习可解释的风格表示。为了克服缺乏风格学标注训练数据的问题，我们使用了一个 LLM 来生成 StyleGenome，这是一个合成的风格学数据集。我们的方法将
    StyleGenome 中的风格学知识提炼到两个模型 Sfam 和 Lisa 中。我们发现这些模型学习到的风格表示与近期直接神经方法的表现相匹配，并引入了与人类判断相关的解释性。我们的方法为一个研究方向奠定基础，该方向致力于使风格表示在下游应用中更有用，例如法医分析背景。未来的方向包括引入人工监督的标注或更新、更大、更好对齐的
    LLMs 进行标注，这有可能在性能和可解释性上取得进一步的提升。
- en: Model and Data Release
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型和数据发布
- en: We release our dataset (StyleGenome) and our two models (Sfam and Lisa) to further
    research in author style.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发布了我们的数据集（StyleGenome）以及两个模型（Sfam 和 Lisa），以便进一步研究作者风格。
- en: Limitations and Broader Impacts
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制和更广泛的影响
- en: Limitations
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制
- en: Handcrafted features by forensic linguists typically rely on frequency counts
    of word usage, usage of unique words or phrases, etc. (Mosteller and Wallace,
    [1963](#bib.bib34)). The space of these kinds of features is non-enumerable and
    would not be well-represented with our technique that scores a fixed set of 768
    interpretable features. Pure neural approaches may capture these kinds of features,
    but are non-interpretable and may capture undesirable content-related features.
    We explicitly trade-off the use of these kinds of features in this work to achieve
    interpretability. While we demonstrate our synthetic annotations are enough for
    a model to learn to identify stylistic properties in text in Table [2](#S3.T2
    "Table 2 ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting LLMs"),
    they cannot be fully relied on yet for the reasons we discuss in Section [4.1](#S4.SS1
    "4.1 Error Analysis ‣ 4 Results ‣ Learning Interpretable Style Embeddings via
    Prompting LLMs"). As large language models scale and improve, however, we believe
    this work could benefit from increasing coherency and decreasing hallucination
    in the annotations (Kaplan et al., [2020](#bib.bib26)). StyleGenome is collected
    only on 10,000 English Reddit posts, however, larger datasets may improve performance
    as we show in Figure [2](#A2.F2 "Figure 2 ‣ Appendix B Effect of StyleGenome Dataset
    Size ‣ Learning Interpretable Style Embeddings via Prompting LLMs") and future
    research in multilingual LLMs may make it feasible to replicate this procedure
    for other languages.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 法医语言学家手工制作的特征通常依赖于词汇使用的频率统计、独特词汇或短语的使用等（Mosteller 和 Wallace，[1963](#bib.bib34)）。这些特征的空间是不可枚举的，并且我们采用的技术对一组固定的
    768 个可解释特征进行评分，可能无法很好地表示这些特征。纯神经方法可能会捕捉到这些特征，但它们不可解释，并且可能捕捉到不希望出现的内容相关特征。我们在这项工作中明确地权衡了这些特征的使用，以实现可解释性。虽然我们在表
    [2](#S3.T2 "Table 2 ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting
    LLMs") 中展示了我们的合成注释足以让模型学习识别文本中的风格属性，但由于我们在第 [4.1](#S4.SS1 "4.1 Error Analysis
    ‣ 4 Results ‣ Learning Interpretable Style Embeddings via Prompting LLMs") 节中讨论的原因，它们还不能完全依赖。然而，随着大型语言模型的扩展和改进，我们相信这项工作可以通过提高注释的连贯性和减少幻觉来受益（Kaplan
    等，[2020](#bib.bib26)）。StyleGenome 仅基于 10,000 篇英文 Reddit 帖子收集，然而，较大的数据集可能会改善性能，如图
    [2](#A2.F2 "Figure 2 ‣ Appendix B Effect of StyleGenome Dataset Size ‣ Learning
    Interpretable Style Embeddings via Prompting LLMs") 所示，未来在多语言 LLMs 的研究可能使得为其他语言复制此过程成为可能。
- en: Ethical considerations
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 伦理考虑
- en: Style representations are useful for text style transfer (Riley et al., [2021](#bib.bib47))
    and in manipulating the output of machine generated text to match a user’s style,
    for example, in machine translation (Niu et al., [2017](#bib.bib36); Rabinovich
    et al., [2017](#bib.bib43)). While style transfer can be a useful benign commercial
    application of this work, superior style representations may aid the impersonation
    of authors. We demonstrate how style representations may aid legitimate cases
    of authorship attribution, a task that is typically done by forensic linguist
    experts. Our work introduces an interpretable approach, an important step in legitimizing
    the use of computational models for authorship attribution by providing explanations
    for predictions that can be audited and verified.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 风格表示对文本风格迁移（Riley 等，[2021](#bib.bib47)）以及在机器翻译（Niu 等，[2017](#bib.bib36); Rabinovich
    等，[2017](#bib.bib43)）中调整机器生成文本以匹配用户风格方面非常有用。虽然风格迁移可能是这项工作的有用且无害的商业应用，但优越的风格表示可能会助长作者冒充行为。我们展示了风格表示如何有助于作者身份认证的合法情况，这项任务通常由法医语言学专家完成。我们的工作引入了一种可解释的方法，这是将计算模型用于作者身份认证的一个重要步骤，通过提供可以审计和验证的预测解释来合法化这一使用。
- en: Diversity and inclusion
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多样性与包容性
- en: We believe style representations that capture wider dimensions of style can
    help aid in analyzing and representing minority writing styles in downstream applications
    like style transfer.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，捕捉更广泛风格维度的风格表示可以帮助分析和表示少数群体的写作风格，用于如风格迁移等下游应用。
- en: Acknowledgements
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research is based upon work supported in part by the DARPA KAIROS Program
    (contract FA8750-19-2-1004), the DARPA LwLL Program (contract FA8750-19-2-0201),
    the IARPA HIATUS Program (contract 2022-22072200005), and the NSF (Award 1928631).
    Approved for Public Release, Distribution Unlimited. The views and conclusions
    contained herein are those of the authors and should not be interpreted as necessarily
    representing the official policies, either expressed or implied, of DARPA, IARPA,
    NSF, or the U.S. Government.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究部分基于DARPA KAIROS项目（合同FA8750-19-2-1004）、DARPA LwLL项目（合同FA8750-19-2-0201）、IARPA
    HIATUS项目（合同2022-22072200005）和NSF（奖项1928631）的支持。已批准公开发布，无限制分发。文中观点和结论仅代表作者个人意见，不应被解读为DARPA、IARPA、NSF或美国政府的官方政策，无论是明示还是暗示。
- en: References
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Alva-Manchego et al. (2020) Fernando Alva-Manchego, Louis Martin, Antoine Bordes,
    Carolina Scarton, Benoît Sagot, and Lucia Specia. 2020. [ASSET: A dataset for
    tuning and evaluation of sentence simplification models with multiple rewriting
    transformations](https://doi.org/10.18653/v1/2020.acl-main.424). In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 4668–4679, Online. Association for Computational Linguistics.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alva-Manchego 等 (2020) Fernando Alva-Manchego, Louis Martin, Antoine Bordes,
    Carolina Scarton, Benoît Sagot 和 Lucia Specia. 2020. [ASSET：用于调整和评估带有多种重写变换的句子简化模型的数据集](https://doi.org/10.18653/v1/2020.acl-main.424)。在
    *第58届计算语言学协会年会论文集*，第4668–4679页，在线。计算语言学协会。
- en: Amazon.com (2018) Inc. Amazon.com. 2018. Amazon Customer Reviews Dataset — s3.amazonaws.com.
    [https://s3.amazonaws.com/amazon-reviews-pds/readme.html](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).
    [Accessed 17-May-2023].
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon.com (2018) Inc. Amazon.com. 2018. 亚马逊客户评论数据集 — s3.amazonaws.com. [https://s3.amazonaws.com/amazon-reviews-pds/readme.html](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).
    [访问时间：2023年5月17日]。
- en: Andrews and Bishop (2019) Nicholas Andrews and Marcus Bishop. 2019. Learning
    invariant representations of social media users. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 1684–1695.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrews 和 Bishop (2019) Nicholas Andrews 和 Marcus Bishop. 2019. 学习社交媒体用户的不变表示。在
    *2019年自然语言处理实证方法会议和第九届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*，第1684–1695页。
- en: 'Atwell et al. (2022) Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022.
    [APPDIA: A discourse-aware transformer-based style transfer model for offensive
    social media conversations](https://aclanthology.org/2022.coling-1.530). In *Proceedings
    of the 29th International Conference on Computational Linguistics*, pages 6063–6074,
    Gyeongju, Republic of Korea. International Committee on Computational Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atwell 等 (2022) Katherine Atwell, Sabit Hassan, 和 Malihe Alikhani. 2022. [APPDIA：一个关注话语的基于变换器的风格转移模型，用于攻击性社交媒体对话](https://aclanthology.org/2022.coling-1.530)。在
    *第29届国际计算语言学大会论文集*，第6063–6074页，韩国庆州。国际计算语言学委员会。
- en: Ba and Caruana (2014) Jimmy Ba and Rich Caruana. 2014. Do deep nets really need
    to be deep? *Advances in neural information processing systems*, 27.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba 和 Caruana (2014) Jimmy Ba 和 Rich Caruana. 2014. 深度网络真的需要那么深吗？*神经信息处理系统进展*,
    27。
- en: Bischoff et al. (2020) Sebastian Bischoff, Niklas Deckers, Marcel Schliebs,
    Ben Thies, Matthias Hagen, Efstathios Stamatatos, Benno Stein, and Martin Potthast.
    2020. The importance of suppressing domain style in authorship analysis. *arXiv
    preprint arXiv:2005.14714*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bischoff 等 (2020) Sebastian Bischoff, Niklas Deckers, Marcel Schliebs, Ben Thies,
    Matthias Hagen, Efstathios Stamatatos, Benno Stein, 和 Martin Potthast. 2020. 领域风格在作者身份分析中的抑制重要性。*arXiv
    预印本 arXiv:2005.14714*。
- en: Boenninghoff et al. (2019) Benedikt Boenninghoff, Robert M Nickel, Steffen Zeiler,
    and Dorothea Kolossa. 2019. Similarity learning for authorship verification in
    social media. In *ICASSP 2019-2019 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, pages 2457–2461\. IEEE.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boenninghoff 等 (2019) Benedikt Boenninghoff, Robert M Nickel, Steffen Zeiler,
    和 Dorothea Kolossa. 2019. 社交媒体中的作者身份验证相似性学习。 在 *ICASSP 2019-2019 IEEE 国际声学、语音与信号处理会议
    (ICASSP)*，第2457–2461页。IEEE。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 1877–1901\.
    Curran Associates, Inc.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020. [语言模型是少量样本学习者](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)。在
    *神经信息处理系统进展*，第33卷，第1877–1901页。Curran Associates, Inc.
- en: 'Coster and Kauchak (2011) William Coster and David Kauchak. 2011. [Simple English
    Wikipedia: A new text simplification task](https://aclanthology.org/P11-2117).
    In *Proceedings of the 49th Annual Meeting of the Association for Computational
    Linguistics: Human Language Technologies*, pages 665–669, Portland, Oregon, USA.
    Association for Computational Linguistics.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coster 和 Kauchak (2011) William Coster 和 David Kauchak. 2011. [简单英语维基百科：一个新的文本简化任务](https://aclanthology.org/P11-2117)。在
    *第49届计算语言学协会年会：人类语言技术会议论文集*，第665–669页，美国俄勒冈州波特兰。计算语言学协会。
- en: CrowdFlower (2017) CrowdFlower. 2017. Twitter User Gender Classification — kaggle.com.
    [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification).
    [Accessed 17-May-2023].
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CrowdFlower (2017) CrowdFlower. 2017. Twitter 用户性别分类 — kaggle.com. [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)。[访问日期
    2023年5月17日]。
- en: 'Dai et al. (2019) Ning Dai, Jianze Liang, Xipeng Qiu, and Xuan-Jing Huang.
    2019. Style transformer: Unpaired text style transfer without disentangled latent
    representation. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pages 5997–6007.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等 (2019) Ning Dai, Jianze Liang, Xipeng Qiu, 和 Xuan-Jing Huang. 2019. 风格变换器：无配对文本风格转移，无需解耦潜在表示。
    在 *第57届计算语言学协会年会论文集*，第5997–6007页。
- en: de Gibert et al. (2018) Ona de Gibert, Naiara Perez, Aitor García-Pablos, and
    Montse Cuadros. 2018. [Hate Speech Dataset from a White Supremacy Forum](https://doi.org/10.18653/v1/W18-5102).
    In *Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)*, pages
    11–20, Brussels, Belgium. Association for Computational Linguistics.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Gibert et al. (2018) Ona de Gibert、Naiara Perez、Aitor García-Pablos 和 Montse
    Cuadros. 2018. [来自白人至上主义论坛的仇恨言论数据集](https://doi.org/10.18653/v1/W18-5102)。在*第二届在线滥用语言研讨会
    (ALW2) 论文集*，第11–20页，比利时布鲁塞尔。计算语言学协会。
- en: 'Demszky et al. (2020) Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko,
    Alan Cowen, Gaurav Nemade, and Sujith Ravi. 2020. [GoEmotions: A dataset of fine-grained
    emotions](https://doi.org/10.18653/v1/2020.acl-main.372). In *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics*, pages 4040–4054,
    Online. Association for Computational Linguistics.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demszky et al. (2020) Dorottya Demszky、Dana Movshovitz-Attias、Jeongwoo Ko、Alan
    Cowen、Gaurav Nemade 和 Sujith Ravi. 2020. [GoEmotions：一个细粒度情感数据集](https://doi.org/10.18653/v1/2020.acl-main.372)。在*第58届计算语言学协会年会论文集*，第4040–4054页，在线。计算语言学协会。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*, pages 4171–4186.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova.
    2019. Bert：用于语言理解的深度双向变换器的预训练。在*2019年北美计算语言学协会会议：人类语言技术，卷1（长文与短文）*，第4171–4186页。
- en: 'Fu et al. (2018) Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui
    Yan. 2018. Style transfer in text: Exploration and evaluation. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 32.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2018) Zhenxin Fu、Xiaoye Tan、Nanyun Peng、Dongyan Zhao 和 Rui Yan. 2018.
    文本中的风格迁移：探索与评估。在*AAAI 人工智能会议论文集*，第32卷。
- en: Gilardi et al. (2023) Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023.
    Chatgpt outperforms crowd-workers for text-annotation tasks. *arXiv preprint arXiv:2303.15056*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilardi et al. (2023) Fabrizio Gilardi、Meysam Alizadeh 和 Maël Kubli. 2023. ChatGPT
    在文本注释任务中优于众包工作者。*arXiv 预印本 arXiv:2303.15056*。
- en: 'Grano et al. (2017) Giovanni Grano, Andrea Di Sorbo, Francesco Mercaldo, Corrado A.
    Visaggio, Gerardo Canfora, and Sebastiano Panichella. 2017. [Android apps and
    user feedback: A dataset for software evolution and quality improvement](https://doi.org/10.1145/3121264.3121266).
    In *Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics*,
    WAMA 2017, page 8–11, New York, NY, USA. Association for Computing Machinery.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grano et al. (2017) Giovanni Grano、Andrea Di Sorbo、Francesco Mercaldo、Corrado
    A. Visaggio、Gerardo Canfora 和 Sebastiano Panichella. 2017. [Android 应用程序和用户反馈：用于软件演化和质量改进的数据集](https://doi.org/10.1145/3121264.3121266)。在*第二届
    ACM SIGSOFT 国际应用市场分析研讨会*，WAMA 2017，第8–11页，美国纽约。计算机协会。
- en: Groenwold et al. (2020) Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli,
    Sharon Levy, Diba Mirza, and William Yang Wang. 2020. [Investigating African-American
    Vernacular English in transformer-based text generation](https://doi.org/10.18653/v1/2020.emnlp-main.473).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 5877–5883, Online. Association for Computational Linguistics.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Groenwold et al. (2020) Sophie Groenwold、Lily Ou、Aesha Parekh、Samhita Honnavalli、Sharon
    Levy、Diba Mirza 和 William Yang Wang. 2020. [研究变换器模型中的非洲裔美国人方言英语](https://doi.org/10.18653/v1/2020.emnlp-main.473)。在*2020年自然语言处理经验方法会议
    (EMNLP) 论文集*，第5877–5883页，在线。计算语言学协会。
- en: Hay et al. (2020) Julien Hay, Bich-Lien Doan, Fabrice Popineau, and Ouassim Ait
    Elhara. 2020. Representation learning of writing style. In *Proceedings of the
    Sixth Workshop on Noisy User-generated Text (W-NUT 2020)*, pages 232–243.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hay et al. (2020) Julien Hay、Bich-Lien Doan、Fabrice Popineau 和 Ouassim Ait Elhara.
    2020. 写作风格的表征学习。在*第六届嘈杂用户生成文本研讨会 (W-NUT 2020) 论文集*，第232–243页。
- en: 'Hill et al. (2015) Felix Hill, Roi Reichart, and Anna Korhonen. 2015. [SimLex-999:
    Evaluating semantic models with (genuine) similarity estimation](https://doi.org/10.1162/COLI_a_00237).
    *Computational Linguistics*, 41(4):665–695.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill et al. (2015) Felix Hill、Roi Reichart 和 Anna Korhonen. 2015. [SimLex-999：通过（真实）相似性估计评估语义模型](https://doi.org/10.1162/COLI_a_00237)。*计算语言学*，41(4):665–695。
- en: Holmes (1994) David I. Holmes. 1994. [Authorship attribution](http://www.jstor.org/stable/30200315).
    *Computers and the Humanities*, 28(2):87–106.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holmes (1994) David I. Holmes. 1994. [作者归属](http://www.jstor.org/stable/30200315)。*计算机与人文*，28(2):87–106。
- en: 'Honovich et al. (2022) Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
    2022. Unnatural instructions: Tuning language models with (almost) no human labor.
    *arXiv preprint arXiv:2212.09689*.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Honovich 等（2022）Or Honovich, Thomas Scialom, Omer Levy 和 Timo Schick。2022。不自然的指令：用（几乎）无需人工的方式调整语言模型。*arXiv
    预印本 arXiv:2212.09689*。
- en: Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve.
    *arXiv preprint arXiv:2210.11610*.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2022）Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang,
    Hongkun Yu 和 Jiawei Han。2022。大型语言模型可以自我改进。*arXiv 预印本 arXiv:2210.11610*。
- en: 'Jafaritazehjani et al. (2020) Somayeh Jafaritazehjani, Gwénolé Lecorvé, Damien
    Lolive, and John Kelleher. 2020. Style versus content: A distinction without a
    (learnable) difference? In *International Conference on Computational Linguistics*.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jafaritazehjani 等（2020）Somayeh Jafaritazehjani, Gwénolé Lecorvé, Damien Lolive
    和 John Kelleher。2020。风格与内容：一个没有（可学习）差异的区分？在 *国际计算语言学会议* 上发表。
- en: John et al. (2019) Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova.
    2019. Disentangled representation learning for non-parallel text style transfer.
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 424–434.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: John 等（2019）Vineet John, Lili Mou, Hareesh Bahuleyan 和 Olga Vechtomova。2019。用于非平行文本风格迁移的解耦表示学习。在
    *第57届计算语言学协会年会* 论文集中，第424–434页。
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei.
    2020. Scaling laws for neural language models. *ArXiv*, abs/2001.08361.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan 等（2020）Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu 和 Dario Amodei。2020。神经语言模型的缩放规律。*ArXiv*，abs/2001.08361。
- en: 'Khan et al. (2021) Aleem Khan, Elizabeth Fleming, Noah Schofield, Marcus Bishop,
    and Nicholas Andrews. 2021. A deep metric learning approach to account linking.
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 5275–5287.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等（2021）Aleem Khan, Elizabeth Fleming, Noah Schofield, Marcus Bishop 和 Nicholas
    Andrews。2021。一种深度度量学习方法用于帐户关联。在 *2021年北美计算语言学协会：人类语言技术会议* 论文集中，第5275–5287页。
- en: Khosla et al. (2020) Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
    Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.
    Supervised contrastive learning. *Advances in neural information processing systems*,
    33:18661–18673.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosla 等（2020）Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong
    Tian, Phillip Isola, Aaron Maschinot, Ce Liu 和 Dilip Krishnan。2020。监督对比学习。*神经信息处理系统进展*，33:18661–18673。
- en: Koppel et al. (2009) Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2009.
    [Computational methods in authorship attribution](https://doi.org/https://doi.org/10.1002/asi.20961).
    *Journal of the American Society for Information Science and Technology*, 60(1):9–26.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koppel 等（2009）Moshe Koppel, Jonathan Schler 和 Shlomo Argamon。2009。[作者身份鉴定中的计算方法](https://doi.org/https://doi.org/10.1002/asi.20961)。*美国信息科学与技术学会期刊*，60(1)：9–26。
- en: Li et al. (2019) Dianqi Li, Yizhe Zhang, Zhe Gan, Yu Cheng, Chris Brockett,
    William B Dolan, and Ming-Ting Sun. 2019. Domain adaptive text style transfer.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3304–3313.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019）Dianqi Li, Yizhe Zhang, Zhe Gan, Yu Cheng, Chris Brockett, William
    B Dolan 和 Ming-Ting Sun。2019。领域自适应文本风格迁移。在 *2019年自然语言处理经验方法会议暨第九届国际自然语言处理联合会议（EMNLP-IJCNLP）*
    论文集中，第3304–3313页。
- en: 'Liu et al. (2021) Frederick Liu, Siamak Shakeri, Hongkun Yu, and Jing Li. 2021.
    Enct5: Fine-tuning t5 encoder for non-autoregressive tasks. *arXiv preprint arXiv:2110.08426*.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021）Frederick Liu, Siamak Shakeri, Hongkun Yu 和 Jing Li。2021。Enct5：为非自回归任务微调
    T5 编码器。*arXiv 预印本 arXiv:2110.08426*。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019）Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
    Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 和 Veselin Stoyanov。2019。Roberta：一种鲁棒优化的
    BERT 预训练方法。*arXiv 预印本 arXiv:1907.11692*。
- en: 'Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. 2011. [Learning word vectors for sentiment
    analysis](http://www.aclweb.org/anthology/P11-1015). In *Proceedings of the 49th
    Annual Meeting of the Association for Computational Linguistics: Human Language
    Technologies*, pages 142–150, Portland, Oregon, USA. Association for Computational
    Linguistics.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maas 等 (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew
    Y. Ng, 和 Christopher Potts. 2011. [用于情感分析的词向量学习](http://www.aclweb.org/anthology/P11-1015)。在
    *第49届计算语言学协会年会：人类语言技术会议论文集*，第 142–150 页，俄勒冈州波特兰，美国。计算语言学协会。
- en: Mosteller and Wallace (1963) Frederick Mosteller and David L. Wallace. 1963.
    [Inference in an authorship problem](http://www.jstor.org/stable/2283270). *Journal
    of the American Statistical Association*, 58(302):275–309.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mosteller 和 Wallace (1963) Frederick Mosteller 和 David L. Wallace. 1963. [作者身份问题的推断](http://www.jstor.org/stable/2283270)。*美国统计协会期刊*，58(302)：275–309。
- en: 'Naji (2012) Ibrahim Naji. 2012. TSATC: Twitter Sentiment Analysis Training
    Corpus. In *thinknook*.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naji (2012) Ibrahim Naji. 2012. TSATC：Twitter 情感分析训练语料库。在 *thinknook*。
- en: 'Niu et al. (2017) Xing Niu, Marianna Martindale, and Marine Carpuat. 2017.
    A study of style in machine translation: Controlling the formality of machine
    translation output. In *Proceedings of the 2017 Conference on Empirical Methods
    in Natural Language Processing*, pages 2814–2819.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 等 (2017) Xing Niu, Marianna Martindale, 和 Marine Carpuat. 2017. 机器翻译中的风格研究：控制机器翻译输出的正式性。在
    *2017年自然语言处理实证方法会议论文集*，第 2814–2819 页。
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *arXiv preprint arXiv:2203.02155*.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等. 2022.
    通过人类反馈训练语言模型以遵循指令。*arXiv 预印本 arXiv:2203.02155*。
- en: 'Pang and Lee (2005) Bo Pang and Lillian Lee. 2005. [Seeing stars: Exploiting
    class relationships for sentiment categorization with respect to rating scales](https://doi.org/10.3115/1219840.1219855).
    In *Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics*,
    ACL ’05, page 115–124, USA. Association for Computational Linguistics.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 和 Lee (2005) Bo Pang 和 Lillian Lee. 2005. [星级评价：利用类别关系进行情感分类](https://doi.org/10.3115/1219840.1219855)。在
    *第43届计算语言学协会年会论文集*，ACL ’05，第 115–124 页，美国。计算语言学协会。
- en: Patel et al. (2022) Ajay Patel, Nicholas Andrews, and Chris Callison-Burch.
    2022. Low-resource authorship style transfer with in-context learning. *arXiv
    preprint arXiv:2212.08986*.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel 等 (2022) Ajay Patel, Nicholas Andrews, 和 Chris Callison-Burch. 2022. 低资源作者风格迁移与上下文学习。*arXiv
    预印本 arXiv:2212.08986*。
- en: Pavlick and Tetreault (2016) Ellie Pavlick and Joel Tetreault. 2016. [An empirical
    analysis of formality in online communication](https://doi.org/10.1162/tacl_a_00083).
    *Transactions of the Association for Computational Linguistics*, 4:61–74.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlick 和 Tetreault (2016) Ellie Pavlick 和 Joel Tetreault. 2016. [在线通信正式性的实证分析](https://doi.org/10.1162/tacl_a_00083)。*计算语言学协会会刊*，4：61–74。
- en: 'Prabhumoye et al. (2018) Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov,
    and Alan W Black. 2018. [Style transfer through back-translation](https://doi.org/10.18653/v1/P18-1080).
    In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 866–876, Melbourne, Australia. Association
    for Computational Linguistics.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prabhumoye 等 (2018) Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov,
    和 Alan W Black. 2018. [通过反向翻译进行风格迁移](https://doi.org/10.18653/v1/P18-1080)。在 *第56届计算语言学协会年会论文集（第一卷：长篇论文）*，第
    866–876 页，澳大利亚墨尔本。计算语言学协会。
- en: Pryzant et al. (2020) Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao
    Kurohashi, Dan Jurafsky, and Diyi Yang. 2020. [Automatically neutralizing subjective
    bias in text](https://doi.org/10.1609/aaai.v34i01.5385). *Proceedings of the AAAI
    Conference on Artificial Intelligence*, 34(01):480–489.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pryzant 等 (2020) Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi,
    Dan Jurafsky, 和 Diyi Yang. 2020. [自动中和文本中的主观偏见](https://doi.org/10.1609/aaai.v34i01.5385)。*AAAI
    人工智能会议论文集*，34(01)：480–489。
- en: 'Rabinovich et al. (2017) Ella Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia
    Specia, and Shuly Wintner. 2017. Personalized machine translation: Preserving
    original author traits. In *Proceedings of the 15th Conference of the European
    Chapter of the Association for Computational Linguistics: Volume 1, Long Papers*,
    pages 1074–1084.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabinovich 等（2017）Ella Rabinovich、Raj Nath Patel、Shachar Mirkin、Lucia Specia
    和 Shuly Wintner。2017年。个性化机器翻译：保留原作者特征。见于*第15届欧洲计算语言学协会会议论文集：第1卷，长论文*，第1074–1084页。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21:1–6.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。2020年。利用统一的文本到文本转换器探索迁移学习的极限。*机器学习研究杂志*，21:1–6。
- en: 'Rao and Tetreault (2018) Sudha Rao and Joel Tetreault. 2018. [Dear sir or madam,
    may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality
    style transfer](https://doi.org/10.18653/v1/N18-1012). In *Proceedings of the
    2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 129–140,
    New Orleans, Louisiana. Association for Computational Linguistics.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rao 和 Tetreault（2018）Sudha Rao 和 Joel Tetreault。2018年。[亲爱的先生或女士，我可以介绍一下 GYAFC
    数据集：正式风格转移的语料库、基准和指标](https://doi.org/10.18653/v1/N18-1012)。见于*2018年北美计算语言学协会会议论文集：人类语言技术，第1卷（长论文）*，第129–140页，新奥尔良，路易斯安那州。计算语言学协会。
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
    Sentence embeddings using siamese bert-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 3982–3992.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych（2019）Nils Reimers 和 Iryna Gurevych。2019年。Sentence-BERT：使用
    Siamese BERT 网络的句子嵌入。见于*2019年自然语言处理实证方法会议和第9届国际联合自然语言处理会议（EMNLP-IJCNLP）*，第3982–3992页。
- en: 'Riley et al. (2021) Parker Riley, Noah Constant, Mandy Guo, Girish Kumar, David C
    Uthus, and Zarana Parekh. 2021. Textsettr: Few-shot text style extraction and
    tunable targeted restyling. In *Proceedings of the 59th Annual Meeting of the
    Association for Computational Linguistics and the 11th International Joint Conference
    on Natural Language Processing (Volume 1: Long Papers)*, pages 3786–3800.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riley 等（2021）Parker Riley、Noah Constant、Mandy Guo、Girish Kumar、David C Uthus
    和 Zarana Parekh。2021年。TextSettr：少量样本文本风格提取和可调目标重塑。见于*第59届计算语言学协会年会暨第11届国际联合自然语言处理会议（第1卷：长论文）*，第3786–3800页。
- en: Rivera-Soto et al. (2021) Rafael A Rivera-Soto, Olivia Elizabeth Miano, Juanita
    Ordonez, Barry Y Chen, Aleem Khan, Marcus Bishop, and Nicholas Andrews. 2021.
    Learning universal authorship representations. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pages 913–919.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rivera-Soto 等（2021）Rafael A Rivera-Soto、Olivia Elizabeth Miano、Juanita Ordonez、Barry
    Y Chen、Aleem Khan、Marcus Bishop 和 Nicholas Andrews。2021年。学习通用的作者表示。见于*2021年自然语言处理实证方法会议论文集*，第913–919页。
- en: 'Rosso et al. (2016) Paolo Rosso, Francisco Rangel, Martin Potthast, Efstathios
    Stamatatos, Michael Tschuggnall, and Benno Stein. 2016. Overview of pan’16: new
    challenges for authorship analysis: cross-genre profiling, clustering, diarization,
    and obfuscation. In *Experimental IR Meets Multilinguality, Multimodality, and
    Interaction: 7th International Conference of the CLEF Association, CLEF 2016,
    Évora, Portugal, September 5-8, 2016, Proceedings 7*, pages 332–350\. Springer.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosso 等（2016）Paolo Rosso、Francisco Rangel、Martin Potthast、Efstathios Stamatatos、Michael
    Tschuggnall 和 Benno Stein。2016年。PAN’16 概述：作者分析的新挑战：跨类型概况、聚类、分段和掩蔽。见于*实验信息检索与多语言性、多模态性和互动：第7届
    CLEF 协会国际会议，CLEF 2016，葡萄牙埃武拉，2016年9月5-8日，论文集第7卷*，第332–350页。Springer。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper
    and lighter. *ArXiv*, abs/1910.01108.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等（2019）Victor Sanh、Lysandre Debut、Julien Chaumond 和 Thomas Wolf。2019年。DistilBERT，BERT
    的蒸馏版本：更小、更快、更便宜、更轻量。*ArXiv*，abs/1910.01108。
- en: 'Saravia et al. (2018) Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin
    Wu, and Yi-Shin Chen. 2018. [CARER: Contextualized affect representations for
    emotion recognition](https://doi.org/10.18653/v1/D18-1404). In *Proceedings of
    the 2018 Conference on Empirical Methods in Natural Language Processing*, pages
    3687–3697, Brussels, Belgium. Association for Computational Linguistics.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saravia等（2018）Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu,
    和 Yi-Shin Chen. 2018. [CARER: 语境化情感表示用于情感识别](https://doi.org/10.18653/v1/D18-1404).
    载于 *2018年自然语言处理经验方法会议论文集*，第3687–3697页，比利时布鲁塞尔。计算语言学协会。'
- en: 'Schroff et al. (2015) Florian Schroff, Dmitry Kalenichenko, and James Philbin.
    2015. Facenet: A unified embedding for face recognition and clustering. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pages 815–823.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schroff等（2015）Florian Schroff, Dmitry Kalenichenko, 和 James Philbin. 2015.
    Facenet: 面部识别与聚类的统一嵌入。 载于 *IEEE计算机视觉与模式识别会议论文集*，第815–823页。'
- en: Shen et al. (2017) Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola.
    2017. Style transfer from non-parallel text by cross-alignment. *Advances in neural
    information processing systems*, 30.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等（2017）Tianxiao Shen, Tao Lei, Regina Barzilay, 和 Tommi Jaakkola. 2017.
    通过跨对齐进行风格迁移。 *神经信息处理系统进展*，30。
- en: Stamatatos (2009) Efstathios Stamatatos. 2009. [A survey of modern authorship
    attribution methods](https://doi.org/https://doi.org/10.1002/asi.21001). *Journal
    of the American Society for Information Science and Technology*, 60(3):538–556.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stamatatos（2009）Efstathios Stamatatos. 2009. [现代作者归属方法的综述](https://doi.org/https://doi.org/10.1002/asi.21001).
    *美国信息科学与技术学会期刊*，60(3):538–556。
- en: 'Tausczik and Pennebaker (2010) Yla R Tausczik and James W Pennebaker. 2010.
    The psychological meaning of words: Liwc and computerized text analysis methods.
    *Journal of language and social psychology*, 29(1):24–54.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tausczik和Pennebaker（2010）Yla R Tausczik 和 James W Pennebaker. 2010. 词汇的心理学意义：Liwc和计算文本分析方法。
    *语言与社会心理学杂志*，29(1):24–54。
- en: 'Tiersma and Solan (2002) Peter Tiersma and Lawrence M. Solan. 2002. [The linguist
    on the witness stand: Forensic linguistics in american courts](http://www.jstor.org/stable/3086556).
    *Language*, 78(2):221–239.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tiersma和Solan（2002）Peter Tiersma 和 Lawrence M. Solan. 2002. [证人席上的语言学家：美国法庭中的法医语言学](http://www.jstor.org/stable/3086556).
    *语言*，78(2):221–239。
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning
    language model with self generated instructions. *arXiv preprint arXiv:2212.10560*.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等（2022）Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith,
    Daniel Khashabi, 和 Hannaneh Hajishirzi. 2022. Self-instruct: 将语言模型与自生成指令对齐。 *arXiv预印本
    arXiv:2212.10560*。'
- en: 'Warstadt et al. (2020) Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey,
    Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. [BLiMP: The benchmark of
    linguistic minimal pairs for English](https://doi.org/10.1162/tacl_a_00321). *Transactions
    of the Association for Computational Linguistics*, 8:377–392.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Warstadt等（2020）Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey,
    Wei Peng, Sheng-Fu Wang, 和 Samuel R. Bowman. 2020. [BLiMP: 英语语言最小对的基准](https://doi.org/10.1162/tacl_a_00321).
    *计算语言学协会会刊*，8:377–392。'
- en: Warstadt et al. (2019) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman.
    2019. [Neural network acceptability judgments](https://doi.org/10.1162/tacl_a_00290).
    *Transactions of the Association for Computational Linguistics*, 7:625–641.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Warstadt等（2019）Alex Warstadt, Amanpreet Singh, 和 Samuel R. Bowman. 2019. [神经网络可接受性判断](https://doi.org/10.1162/tacl_a_00290).
    *计算语言学协会会刊*，7:625–641。
- en: Wegmann and Nguyen (2021) Anna Wegmann and Dong Nguyen. 2021. Does it capture
    stel? a modular, similarity-based linguistic style evaluation framework. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    7109–7130.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wegmann和Nguyen（2021）Anna Wegmann 和 Dong Nguyen. 2021. 它能捕捉风格吗？一个模块化、基于相似性的语言风格评估框架。
    载于 *2021年自然语言处理经验方法会议论文集*，第7109–7130页。
- en: Wegmann et al. (2022) Anna Wegmann, Marijn Schraagen, Dong Nguyen, et al. 2022.
    Same author or just same topic? towards content-independent style representations.
    In *Proceedings of the 7th Workshop on Representation Learning for NLP*, page
    249\. Association for Computational Linguistics.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wegmann等（2022）Anna Wegmann, Marijn Schraagen, Dong Nguyen, 等. 2022. 同一作者还是仅同一话题？迈向内容独立的风格表示。
    载于 *第7届自然语言处理表示学习研讨会论文集*，第249页。计算语言学协会。
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    and Jamie Brew. 2019. [Huggingface’s transformers: State-of-the-art natural language
    processing](http://arxiv.org/abs/1910.03771). *CoRR*, abs/1910.03771.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    和 Jamie Brew. 2019. [Huggingface的变压器：前沿的自然语言处理](http://arxiv.org/abs/1910.03771)。*CoRR*，abs/1910.03771。
- en: 'Xu (2017) Wei Xu. 2017. [From shakespeare to Twitter: What are language styles
    all about?](https://doi.org/10.18653/v1/W17-4901) In *Proceedings of the Workshop
    on Stylistic Variation*, pages 1–9, Copenhagen, Denmark. Association for Computational
    Linguistics.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu (2017) Wei Xu. 2017. [从莎士比亚到推特：语言风格是什么？](https://doi.org/10.18653/v1/W17-4901)
    收录于*风格变异研讨会会议录*，第1–9页，丹麦哥本哈根。计算语言学协会。
- en: Yi et al. (2021) Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, and Maosong Sun. 2021.
    Text style transfer via learning style instance supported latent space. In *Proceedings
    of the Twenty-Ninth International Conference on International Joint Conferences
    on Artificial Intelligence*, pages 3801–3807.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi et al. (2021) Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, 和 Maosong Sun. 2021.
    通过学习风格实例支持的潜在空间进行文本风格迁移。收录于*第二十九届国际人工智能联合会议论文集*，第3801–3807页。
- en: Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. [Character-level
    convolutional networks for text classification](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 28\. Curran Associates,
    Inc.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2015) Xiang Zhang, Junbo Zhao, 和 Yann LeCun. 2015. [字符级卷积网络用于文本分类](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf)。收录于*神经信息处理系统进展*，第28卷。Curran
    Associates, Inc.
- en: 'Zhu and Jurgens (2021) Jian Zhu and David Jurgens. 2021. Idiosyncratic but
    not arbitrary: Learning idiolects in online registers reveals distinctive yet
    consistent individual styles. In *Proceedings of the 2021 Conference on Empirical
    Methods in Natural Language Processing*, pages 279–297.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 和 Jurgens (2021) Jian Zhu 和 David Jurgens. 2021. 特异但非任意：在线注册中学习方言揭示了独特而一致的个人风格。收录于*2021年自然语言处理经验方法会议论文集*，第279–297页。
- en: 'Zhu et al. (2022) Kangchen Zhu, Zhiliang Tian, Ruifeng Luo, and Xiaoguang Mao.
    2022. Styleflow: Disentangle latent representations via normalizing flow for unsupervised
    text style transfer. *arXiv preprint arXiv:2212.09670*.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2022) Kangchen Zhu, Zhiliang Tian, Ruifeng Luo, 和 Xiaoguang Mao.
    2022. Styleflow：通过归一化流解开潜在表示以进行无监督文本风格迁移。*arXiv预印本 arXiv:2212.09670*。
- en: Appendix A Prompt Templates
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 提示模板
- en: A.1 Open-ended Prompt Templates
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 开放式提示模板
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Grammar Style: Write a long paragraph describing the unique grammar style of
    the following passage without referring to specifics about the topic.'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语法风格：写一段长文描述以下段落的独特语法风格，而不涉及主题的具体内容。
- en: 'Passage: {{passage}}'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落：{{passage}}
- en: 'Description:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：
- en: •
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Vocabulary Style: Write a long paragraph describing the unique vocabulary style
    of the following passage without referring to specifics about the topic.'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 词汇风格：写一段长文描述以下段落的独特词汇风格，而不涉及主题的具体内容。
- en: 'Passage: {{passage}}'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落：{{passage}}
- en: 'Description:'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：
- en: •
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Punctuation Style: Write a long paragraph describing the unique punctuation
    style of the following passage without referring to specifics about the topic.'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标点风格：写一段长文描述以下段落的独特标点风格，而不涉及主题的具体内容。
- en: 'Passage: {{passage}}'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落：{{passage}}
- en: 'Description:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Grammar Errors: Write a long paragraph describing the grammar errors (if any)
    of the following passage without referring to specifics about the topic.'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语法错误：写一段长文描述以下段落的语法错误（如果有的话），而不涉及主题的具体内容。
- en: 'Passage: {{passage}}'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落：{{passage}}
- en: 'Description:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：
- en: •
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Spelling Errors: Write a long paragraph describing the spelling errors (if
    any) of the following passage without referring to specifics about the topic.'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拼写错误：写一段长文描述以下段落的拼写错误（如果有的话），而不涉及主题的具体内容。
- en: 'Passage: {{passage}}'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落：{{passage}}
- en: 'Description:'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Forensic Linguist: Write a long paragraph describing the unique stylometric
    features of the following passage without referring to specifics about the topic
    from the perspective of a forensic linguist psychoanalyzing the writer.'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 法医语言学家：从法医语言学家的角度，写一段长文描述以下段落的独特风格特征，而不涉及主题的具体内容。
- en: 'Passage: {{passage}}'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 段落：{{passage}}
- en: 'Description:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述：
- en: A.2 Targeted Prompt Templates
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 有针对性的提示模板
- en: 'We use the following template:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下模板：
- en: '{{target_feature_definition}}'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '{{target_feature_definition}}'
- en: Write a description of whether the author of the following passage has any {{target_feature}}?
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 写下对以下段落的描述，作者是否具有任何{{target_feature}}？
- en: 'Passage: {{passage}}'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 段落：{{passage}}
- en: 'Description:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 描述：
- en: 'for targeted prompts, substituting {{target_feature}} with each of the following
    targeted features:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有针对性的提示，将{{target_feature}}替换为以下每个有针对性的特征：
- en: '| • figurative language • sarcasm • sentence fragment • run on sentences •
    an active voice • a passive voice • agreement errors • male pronouns • female
    pronouns • prosocial behaviors • antisocial behaviors • being polite • showing
    interpersonal conflict • moralizing • communication words • indicators of power
    • talk of achievement • indication of certitude • being tentative • insight •
    all or none thinking • words related to memory • positive emotion • negative emotion
    • anxiety • anger • sadness • swear words • positive tone | • negative tone •
    neutral tone • words related to auditory perception • words related to visual
    perception • words related to space perception • words related to motion perception
    • words related to attention • words related to allure • words related to curiosity
    • words related to risk • words related to reward • words expressing needs • words
    expressing wants • words expressing acquisition • words expressing lack • words
    expressing fulfillment • words expressing fatigue • words expressing illness •
    words expressing wellness • words related to mental health • words related to
    food or eating • words related to death • words related to self-harm • sexual
    content • words related to leisure • words related to home • words related to
    work • words related to money • words related to religion | • words related to
    politics • words related to culture • swear words • foreign words • scholarly
    words • slang words • social media slang words • filler words • words focusing
    on the past • words focusing on the present • words focusing on the future • words
    related to time • misspelled words • repeated words • words expressing quantity
    • words indicating family • words indicating friends • words indicating men •
    words indicating women • words indicating pets • words indicating social status
    • words indicating poverty • words indicating wealth • punctuation symbols • hyphenated
    words • oxford comma • parentheticals • numbers • elongated words |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| • 比喻语言 • 讽刺 • 句子片段 • 连续句 • 主动语态 • 被动语态 • 一致性错误 • 男性代词 • 女性代词 • 亲社会行为 • 反社会行为
    • 礼貌 • 显示人际冲突 • 道德化 • 交流词 • 权力指标 • 成就谈话 • 确定性指标 • 犹豫 • 洞察力 • 非黑即白思维 • 记忆相关词 •
    积极情绪 • 消极情绪 • 焦虑 • 愤怒 • 伤心 • 亵渎词 • 积极语气 | • 消极语气 • 中性语气 • 听觉感知相关词 • 视觉感知相关词 •
    空间感知相关词 • 动作感知相关词 • 注意力相关词 • 吸引力相关词 • 好奇心相关词 • 风险相关词 • 奖励相关词 • 表达需求的词 • 表达想要的词
    • 表达获得的词 • 表达缺乏的词 • 表达满足的词 • 表达疲劳的词 • 表达疾病的词 • 表达健康的词 • 心理健康相关词 • 食物或饮食相关词 • 死亡相关词
    • 自残相关词 • 性内容 • 休闲相关词 • 家庭相关词 • 工作相关词 • 钱相关词 • 宗教相关词 | • 政治相关词 • 文化相关词 • 亵渎词 •
    外语词 • 学术词汇 • 俚语 • 社交媒体俚语 • 填充词 • 关注过去的词 • 关注现在的词 • 关注未来的词 • 时间相关词 • 拼写错误的词 • 重复词
    • 表达数量的词 • 指示家庭的词 • 指示朋友的词 • 指示男性的词 • 指示女性的词 • 指示宠物的词 • 指示社会地位的词 • 指示贫困的词 • 指示财富的词
    • 标点符号 • 连字符词 • 牛津逗号 • 括号 • 数字 • 延长词 |'
- en: To give GPT-3 more context, we also substitute {{target_feature_definition}}
    with a definition of the target feature, also generated by GPT-3\. The full set
    of targeted prompts can be found in the released source package for this paper.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给GPT-3更多的上下文，我们还将{{target_feature_definition}}替换为目标特征的定义，该定义也是由GPT-3生成的。完整的有针对性提示集可以在本文发布的源包中找到。
- en: A.3 Standardization Prompt Templates
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 标准化提示模板
- en: 'The descriptions of style generated from the prompts in Appendix [A.1](#A1.SS1
    "A.1 Open-ended Prompt Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") and Appendix [A.2](#A1.SS2 "A.2 Targeted
    Prompt Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable Style
    Embeddings via Prompting LLMs") are substituted into the following standardization
    prompt:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 从附录[A.1](#A1.SS1 "A.1 开放式提示模板 ‣ 附录 A 提示模板 ‣ 通过提示LLMs学习可解释的风格嵌入")和附录[A.2](#A1.SS2
    "A.2 有针对性的提示模板 ‣ 附录 A 提示模板 ‣ 通过提示LLMs学习可解释的风格嵌入")生成的风格描述被替换到以下标准化提示中：
- en: 'Here’s a description of an author’s writing style for a passage: {{description}}'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '这是对一段文字的作者写作风格的描述: {{description}}'
- en: Rewrite this description as a long list of short sentences describing the author’s
    writing style where each sentence is in the format of "The author is X." or "The
    author uses X.".
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 将此描述重写为长列表，每个句子简短地描述作者的写作风格，每个句子格式为“作者是 X。”或“作者使用 X。”。
- en: 'Output:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '输出:'
- en: which transforms the verbose descriptions into short, declarative, uniform sentences
    beginning with “The author…,” which are the final style attributes used in building
    the StyleGenome dataset that Sfam is trained on.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这将冗长的描述转换为简短、声明式、统一的句子，以“作者……”开头，这些句子是用于构建 Sfam 训练的 StyleGenome 数据集的最终风格属性。
- en: Appendix B Effect of StyleGenome Dataset Size
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B StyleGenome 数据集规模的影响
- en: When training Sfam, we experiment with artificially limiting the size of the
    synthetic dataset, by limiting the number of authors in the dataset, to determine
    the effect of dataset size on the validation performance. In Figure [2](#A2.F2
    "Figure 2 ‣ Appendix B Effect of StyleGenome Dataset Size ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs"), we find that as the synthetic dataset grows,
    validation performance improves and Sfam generalizes to better predict agreement
    scores for unseen style attribute and text pairs.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 Sfam 时，我们通过限制数据集中的作者数量来人为限制合成数据集的规模，以确定数据集规模对验证性能的影响。在图 [2](#A2.F2 "图 2
    ‣ 附录 B StyleGenome 数据集规模的影响 ‣ 通过提示 LLM 学习可解释的风格嵌入") 中，我们发现随着合成数据集的增长，验证性能得到了改善，Sfam
    更好地预测了未见风格属性和文本对的协议分数。
- en: $10^{1}$$10^{2}$$10^{3}$$0.6$$0.7$$0.8$$0.9$Number
    of AuthorsValidation F1
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: $10^{1}$$10^{2}$$10^{3}$$0.6$$0.7$$0.8$$0.9$Number
    of AuthorsValidation F1
- en: 'Figure 2: Best F1 achieved by Sfam on a held-out validation set of examples
    at various dataset sizes.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: Sfam 在不同数据集规模的保留验证集示例上获得的最佳 F1 分数。'
- en: Appendix C Annotation Prompts Ablation
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 注释提示消融
- en: '| Prompts Used to Generate StyleGenome | Validation F1 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 生成 StyleGenome 所用的提示 | 验证 F1 |'
- en: '| --- | --- |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Open-ended Prompts | 0.865 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 开放式提示 | 0.865 |'
- en: '| Targeted Prompts | 0.898 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 针对性提示 | 0.898 |'
- en: '| Open-ended Prompts & Targeted Prompts | 0.920 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 开放式提示 & 针对性提示 | 0.920 |'
- en: 'Table 7: Best F1 achieved by Sfam on a held-out validation set of examples
    with different sets of Stage 1 prompts used to annotate Reddit posts and generate
    the synthetic training data used during distillation.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: Sfam 在使用不同的 Stage 1 提示集注释 Reddit 帖子并生成用于蒸馏的合成训练数据的保留验证集示例上的最佳 F1 分数。'
- en: Appendix D StyleGenome Annotation Cost
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D StyleGenome 注释成本
- en: Our inference cost with the OpenAI API was priced at $0.02 / 1K tokens, with
    a cost of ~$8 to annotate 10 Reddit posts by a single author with all of our prompts.
    Our full dataset of 1,000 authors cost ~$8,000 to annotate.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 OpenAI API 的推断成本为每 1K 个令牌 $0.02，使用所有提示对 10 篇由单一作者撰写的 Reddit 帖子进行注释的成本约为
    $8。我们 1,000 名作者的完整数据集注释成本约为 $8,000。
- en: Appendix E Training Details
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 训练细节
- en: E.1 Sfam
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 Sfam
- en: We use the EncT5 architecture (Liu et al., [2021](#bib.bib31)) with a binary
    classifier in Hugging Face (Wolf et al., [2019](#bib.bib62)). We randomly sample
    training batches of size 1,440 and use the AdamW optimizer with a learning rate
    of 0.001\. We employ early stopping with a threshold of 0.01 on the validation
    set F1 metric and a patience of 50 batches.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 EncT5 架构（Liu 等，[2021](#bib.bib31)），在 Hugging Face（Wolf 等，[2019](#bib.bib62)）中使用二分类器。我们随机抽取大小为
    1,440 的训练批次，并使用学习率为 0.001 的 AdamW 优化器。我们对验证集 F1 指标采用 0.01 的早停阈值，耐心等待 50 个批次。
- en: E.2 Lisa
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 Lisa
- en: We use the EncT5 architecture (Liu et al., [2021](#bib.bib31)) with 768 regression
    labels in Hugging Face (Wolf et al., [2019](#bib.bib62)) and use MSELoss. We randomly
    sample training batches of size 1,440 and use the AdamW optimizer with a learning
    rate of 0.001\. We employ early stopping with a threshold of 1e-6 on the validation
    set MSE metric and a patience of 20 epochs.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 EncT5 架构（Liu 等，[2021](#bib.bib31)），在 Hugging Face（Wolf 等，[2019](#bib.bib62)）中具有
    768 个回归标签，并使用 MSELoss。我们随机抽取大小为 1,440 的训练批次，并使用学习率为 0.001 的 AdamW 优化器。我们对验证集 MSE
    指标采用 1e-6 的早停阈值，耐心等待 20 个 epoch。
- en: E.3 Lisa Embedding Layers
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 Lisa 嵌入层
- en: We experiment with two types of embedding layers $w$ and $W$. We also experiment
    with two training datasets, the Wegmann dataset (Wegmann et al., [2022](#bib.bib61))
    and the LUAR dataset (Rivera-Soto et al., [2021](#bib.bib48)). For LUAR, we use
    the train split with 5% held out as validation and we sample random authors as
    negative examples. For Wegmann, we use the Conversation dataset train split and
    the dev split as validation. We use a margin of 1.0, a batch size of 32, an AdamW
    optimizer with a learning rate of 0.001, and employ early stopping with a threshold
    of 0.001 on the validation set loss.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验了两种类型的嵌入层 $w$ 和 $W$。我们还实验了两个训练数据集，Wegmann 数据集（Wegmann et al., [2022](#bib.bib61)）和
    LUAR 数据集（Rivera-Soto et al., [2021](#bib.bib48)）。对于 LUAR，我们使用训练集的 5% 作为验证集，并将随机作者作为负例。对于
    Wegmann，我们使用对话数据集的训练集分割和开发集作为验证集。我们使用 1.0 的边际，32 的批量大小，学习率为 0.001 的 AdamW 优化器，并在验证集损失上使用
    0.001 的早停阈值。
- en: Appendix F Full Sfam Evaluation Results
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 完整的 Sfam 评估结果
- en: '| Type | Dataset | Style Attribute | Spearman Correlation ($\rho$) |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 数据集 | 风格属性 | 斯皮尔曼相关系数 ($\rho$) |'
- en: '| Formality | Formality in Online Communication | The author uses informal
    language. | 0.599 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 正式性 | 在线沟通中的正式性 | 作者使用非正式语言。 | 0.599 |'
- en: '| Grammarly’s Yahoo Answers Formality Corpus | 0.200 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Grammarly 的 Yahoo Answers 正式性语料库 | 0.200 |'
- en: '| Sentiment | Yelp Reviews | The author uses a negative tone. | 0.788 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 情绪 | Yelp 评论 | 作者使用负面语气。 | 0.788 |'
- en: '| IMDB Large Movie Review Dataset | 0.665 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| IMDB 大型电影评论数据集 | 0.665 |'
- en: '| Amazon Customer Reviews Dataset | 0.432 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊客户评论数据集 | 0.432 |'
- en: '| Rotten Tomatoes Movie Review Data | 0.463 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 烂番茄电影评论数据 | 0.463 |'
- en: '| App Reviews | 0.350 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 应用评论 | 0.350 |'
- en: '| Twitter Sentiment Analysis Training Corpus | 0.299 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Twitter 情感分析训练语料库 | 0.299 |'
- en: '| Emotion | DAIR.AI Emotion (Love vs. Anger) | The author is expressing {{emotion}}.
    | 0.542 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 情感 | DAIR.AI 情感 (爱 vs. 愤怒) | 作者在表达{{emotion}}。 | 0.542 |'
- en: '| DAIR.AI Emotion (Joy vs. Sad) | 0.531 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| DAIR.AI 情感 (喜悦 vs. 悲伤) | 0.531 |'
- en: '| GoEmotions (Love vs. Anger) | 0.769 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions (爱 vs. 愤怒) | 0.769 |'
- en: '| GoEmotions (Joy vs. Sadness) | 0.639 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions (喜悦 vs. 悲伤) | 0.639 |'
- en: '| GoEmotions (Disgust vs. Desire) | 0.630 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions (厌恶 vs. 欲望) | 0.630 |'
- en: '| GoEmotions (Disappointment vs. Admiration) | 0.571 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions (失望 vs. 钦佩) | 0.571 |'
- en: '| GoEmotions (Pride vs. Embarassment) | 0.419 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions (骄傲 vs. 尴尬) | 0.419 |'
- en: '| GoEmotions (Nervousness vs. Optimism) | 0.447 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions (紧张 vs. 乐观) | 0.447 |'
- en: '| GoEmotions (Disapproval vs. Approval) | 0.432 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotions (反感 vs. 赞同) | 0.432 |'
- en: '|  | GoEmotions (Admiration vs. Neutral) | 0.578 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (钦佩 vs. 中立) | 0.578 |'
- en: '|  | GoEmotions (Amusement vs. Neutral) | 0.508 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (娱乐 vs. 中立) | 0.508 |'
- en: '|  | GoEmotions (Anger vs. Neutral) | 0.372 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (愤怒 vs. 中立) | 0.372 |'
- en: '|  | GoEmotions (Annoyance vs. Neutral) | 0.355 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (烦恼 vs. 中立) | 0.355 |'
- en: '|  | GoEmotions (Approval vs. Neutral) | 0.257 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (赞同 vs. 中立) | 0.257 |'
- en: '|  | GoEmotions (Caring vs. Neutral) | 0.303 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (关怀 vs. 中立) | 0.303 |'
- en: '|  | GoEmotions (Confusion vs. Neutral) | 0.343 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (困惑 vs. 中立) | 0.343 |'
- en: '|  | GoEmotions (Curiosity vs. Neutral) | 0.464 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (好奇 vs. 中立) | 0.464 |'
- en: '|  | GoEmotions (Desire vs. Neutral) | 0.315 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (欲望 vs. 中立) | 0.315 |'
- en: '|  | GoEmotions (Disappointment vs. Neutral) | 0.317 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (失望 vs. 中立) | 0.317 |'
- en: '|  | GoEmotions (Disapproval vs. Neutral) | 0.284 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (反感 vs. 中立) | 0.284 |'
- en: '|  | GoEmotions (Disgust vs. Neutral) | 0.307 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (厌恶 vs. 中立) | 0.307 |'
- en: '|  | GoEmotions (Embarrassment vs. Neutral) | 0.173 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (尴尬 vs. 中立) | 0.173 |'
- en: '|  | GoEmotions (Excitement vs. Neutral) | 0.295 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (兴奋 vs. 中立) | 0.295 |'
- en: '|  | GoEmotions (Fear vs. Neutral) | 0.309 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (恐惧 vs. 中立) | 0.309 |'
- en: '|  | GoEmotions (Gratitude vs. Neutral) | 0.626 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (感激 vs. 中立) | 0.626 |'
- en: '|  | GoEmotions (Grief vs. Neutral) | 0.093 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (悲痛 vs. 中立) | 0.093 |'
- en: '|  | GoEmotions (Joy vs. Neutral) | 0.390 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (喜悦 vs. 中立) | 0.390 |'
- en: '|  | GoEmotions (Love vs. Neutral) | 0.497 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (爱 vs. 中立) | 0.497 |'
- en: '|  | GoEmotions (Nervousness vs. Neutral) | 0.171 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (紧张 vs. 中立) | 0.171 |'
- en: '|  | GoEmotions (Optimism vs. Neutral) | 0.407 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (乐观 vs. 中立) | 0.407 |'
- en: '|  | GoEmotions (Pride vs. Neutral) | 0.097 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (骄傲 vs. 中立) | 0.097 |'
- en: '|  | GoEmotions (Realization vs. Neutral) | 0.121 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (领悟 vs. 中立) | 0.121 |'
- en: '|  | GoEmotions (Relief vs. Neutral) | 0.123 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (宽慰 vs. 中立) | 0.123 |'
- en: '|  | GoEmotions (Remorse vs. Neutral) | 0.276 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (懊悔 vs. 中立) | 0.276 |'
- en: '|  | GoEmotions (Sadness vs. Neutral) | 0.413 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (悲伤 vs. 中立) | 0.413 |'
- en: '|  | GoEmotions (Surprise vs. Neutral) | 0.317 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | GoEmotions (惊讶 vs. 中立) | 0.317 |'
- en: '| Author Profiling | Political Slant | The author is a Democrat. | 0.005 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 作者画像 | 政治倾向 | 作者是民主党人。 | 0.005 |'
- en: '| Twitter User Gender Classification | The author is female. | 0.166 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 推特用户性别分类 | 作者是女性。 | 0.166 |'
- en: '| African-American Vernacular English | The author uses African-American Vernacular
    English. | 0.238 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 非裔美国人方言英语 | 作者使用了非裔美国人方言英语。 | 0.238 |'
- en: '| Shakespeare (Early Modern English) | The author uses Early Modern English.
    | 0.108 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 莎士比亚（早期现代英语） | 作者使用了早期现代英语。 | 0.108 |'
- en: '| Wikipedia Bias | The author has a biased point of view. | 0.014 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科偏见 | 作者有偏见的观点。 | 0.014 |'
- en: '| Harmful Speech | HateSpeech18 | The author’s writing contains hate speech.
    | 0.229 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 有害言论 | HateSpeech18 | 作者的写作包含仇恨言论。 | 0.229 |'
- en: '| Offensive Social Media | The author uses offensive language. | 0.401 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 冒犯性社交媒体 | 作者使用了冒犯性的语言。 | 0.401 |'
- en: '| Text Simplification | Simple Wikipedia | The author uses simple language.
    | 0.043 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 文本简化 | 简单维基百科 | 作者使用简单的语言。 | 0.043 |'
- en: '| ASSET | 0.050 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| ASSET | 0.050 |'
- en: '| Linguistic Acceptability | CoLA | The author uses incorrect grammar. | 0.078
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 语言可接受性 | CoLA | 作者使用了不正确的语法。 | 0.078 |'
- en: '| BLiMP | 0.020 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| BLiMP | 0.020 |'
- en: '| Average |  |  | 0.342 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 |  |  | 0.342 |'
- en: 'Table 8: Correlation of agreement scores produced by Sfam against human judgments
    on texts over a wide variety linguistic and authorship dimensions. The natural
    language style attributes used as input to Sfam when producing the agreement scores
    for each dataset are also provided.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：Sfam与人类对文本的判断在各种语言学和作者维度上的一致性得分的相关性。生成每个数据集的一致性得分时，作为输入的自然语言风格属性也一并提供。
- en: Appendix G Interpretable Authorship Verification
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 可解释的作者验证
- en: '| Texts | Top 3 Common/Distinct Style Attributes |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 最常见/不同的风格属性前三 |'
- en: '| --- | --- |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Anchor: Devices that use two pronged instead of three pronged plugs are required
    to meet certain safe design requirements. Among other things, if a device has
    a switch, the switched line MUST BE hot, not neutral. The polarized plugs make
    sure that the right prong/wire is hot. This is why devices that have no switches
    (primarily wall warts) need not have polarized plugs. |  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 锚点：使用两脚插头而不是三脚插头的设备必须符合某些安全设计要求。其他要求中，如果设备有开关，则开关线路必须是带电的，而不是中性线。极性插头确保正确的插脚/电线是带电的。这就是为什么没有开关的设备（主要是墙壁插座）不需要极性插头的原因。
    |  |'
- en: '| Same Author: Your diaphragm would be trying to contract against the air pressure
    in your lungs. That’s why deep sea diving requires regulators, to match the pressure
    of the air supply to the pressure surrounding your rib cage. You can breathe against
    a maximum of about 1/2 PSI, which is not enough pressure to adequately oxygenate
    your blood. | (0.89,  1.00)  –  The author is using a scientific approach. (0.96,
    0.98) – The author is using a combination of                     technical terms
    and everyday language.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '| 同一作者：你的膈肌会试图对抗你肺部的气压。这就是为什么深海潜水需要调节器，以使空气供应的压力与你胸腔周围的压力相匹配。你只能在最大约1/2 PSI的压力下呼吸，这不足以充分氧化你的血液。
    | (0.89, 1.00) – 作者使用了科学方法。 (0.96, 0.98) – 作者使用了技术术语和日常语言的结合。'
- en: (0.91, 0.84)   –  The author is using formal and
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: (0.91, 0.84) – 作者使用了正式且
- en: professional language. |
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 专业的语言。 |
- en: '| Different Author: That’s great! I’m glad it seems to be finding its’ niche.
    Now if they could just make a Star Wars version of this game, I’d happily swallow
    that fat learning curve and overcome my frustrations with the combat system. ;)
    | (0.06, 0.99)  – The author is using words related to the                     game
    they are discussing. $\ast$ (0.00, 0.88)   –  The author is using an emoji.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '| 不同的作者：太棒了！我很高兴它似乎找到了自己的定位。现在如果他们能做一个《星球大战》版本的游戏，我会愉快地接受那段艰难的学习曲线，并克服我对战斗系统的挫折。;)
    | (0.06, 0.99) – 作者使用了与他们讨论的游戏相关的词汇。 $\ast$ (0.00, 0.88) – 作者使用了一个表情符号。'
- en: (0.02, 0.87)   –  The author uses an emoticon at the end. |
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: (0.02, 0.87) – 作者在最后使用了一个表情符号。 |
- en: '| Anchor: Not sure what the income tax is in Germany, but in the Netherlands
    the income can be up 50% for the higher income classes. |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 锚点：不确定德国的所得税是多少，但在荷兰，高收入阶层的所得税可以高达50%。 |  |'
- en: '| Same Author: The salaries in the US alway blow my mind. A software developer
    in Amsterdam gets like €40.000/year, maybe €50.000/year if your good, and maybe
    €60.000/year if you’re some kind of manager. Anything position over €100.000/year
    is basically running the entire company. | (0.84, 0.90)  –  The author is using
    words indicating                    poverty.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '| 相同作者：美国的薪水总是让我惊讶。阿姆斯特丹的软件开发者年薪大约€40,000，如果表现好可能€50,000，如果是某种管理者可能€60,000。任何超过€100,000/年的职位基本上都是在管理整个公司。
    | (0.84, 0.90) – 作者使用了表示                   贫困的词汇。'
- en: (0.87, 1.00)  –  The author is using words indicating
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: (0.87, 1.00) – 作者使用了表示
- en: wealth.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 财富。
- en: (0.80, 0.93)  –  The author is using words related to money. |
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: (0.80, 0.93) – 作者使用了与金钱相关的词汇。 |
- en: '| Different Author: How would you even test this software? The setup would
    be just insane. | (0.00, 1.00) – The author is comfortable with technology. (0.00,
    0.85) – The author is discussing a product. $\ast$'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '| 不同作者：你怎么测试这个软件？设置简直疯狂。 | (0.00, 1.00) – 作者对技术感到舒适。 (0.00, 0.85) – 作者讨论了一个产品。
    $\ast$'
- en: (0.13, 0.92) – The author is using formal and
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: (0.13, 0.92) – 作者使用了正式且
- en: professional language. |
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 专业语言。 |
- en: '| Anchor: If only there was something he could have done to avoid this backlash.
    Like maybe not acting like a complete d**khead. |  |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 锚点：如果他能做些什么来避免这种反弹就好了。比如不要表现得像个完全的**傻瓜**。 |  |'
- en: '| Same Author: I take issue with a faster landing being marked as less skilled.
    By that logic the slowest, smoothest possible landing would be the most skilled
    and that is plain wrong. Maybe war machine intentionally does faster and harder
    landings. | (1.00, 0.96) – The author is emphasizing the contrast                   between
    the two ideas.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '| 相同作者：我对更快的着陆被标记为技能较差感到不满。按照这个逻辑，最慢、最平滑的着陆会是最有技巧的，这显然是错误的。也许战争机器故意做得更快更猛。
    | (1.00, 0.96) – 作者强调了两个想法之间的对比。'
- en: (0.78, 0.89) – The author is able to draw conclusions.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: (0.78, 0.89) – 作者能够得出结论。
- en: (0.97, 0.86) – The author is using an all-or-none thinking
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: (0.97, 0.86) – 作者使用了全有或全无的思维方式。
- en: style. |
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 风格的词汇。 |
- en: '| Different Author: She was the Ronald Reagan of the UK in the same time period.
    | (0.83, 0.05) – The author is describing sexual content. $\ast$ (0.38, 0.94)
     – The author is using words related to'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '| 不同作者：她是在同一时间段的英国版罗纳德·里根。 | (0.83, 0.05) – 作者描述了性内容。 $\ast$ (0.38, 0.94) –
    作者使用了与'
- en: politics. $\ast$
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 政治。 $\ast$
- en: (0.74, 0.38) –  The author is using parentheticals. |
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: (0.74, 0.38) – 作者使用了括号注释。 |
- en: 'Table 9: Example interpretable explanations on the Contrastive Authorship Verification
    task. The top style attributes in common between the Anchor text and a text by
    the Same Author are shown. The top distinct style attributes between the Anchor
    text and a text by a Different Author are also shown. The scores of each style
    attribute against the texts is shown in (•,  •/•). We manually inspect the style
    attributes and annotate them as reasonable, plausible, or incorrect explanations.
    Attributes annotated with $\ast$ blur the line between style and content. Error
    analysis can be found in Section [4.1](#S4.SS1 "4.1 Error Analysis ‣ 4 Results
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs").'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：对比作者验证任务中的示例可解释性解释。展示了锚点文本和相同作者文本之间的顶级共同风格属性。也展示了锚点文本和不同作者文本之间的顶级独特风格属性。每个风格属性与文本的得分显示在（•，•/•）中。我们手动检查风格属性并将其标注为合理、可信或不正确的解释。标注为$\ast$的属性模糊了风格和内容之间的界限。错误分析见第[4.1](#S4.SS1
    "4.1 错误分析 ‣ 4 结果 ‣ 通过提示LLMs学习可解释风格嵌入")节。
- en: 'We perform this task with Lisa (LUAR + $W$) and demonstrate interpretability
    on a few task instances. To rank the top common or distinct style attributes between
    two style vectors $\vec{v_{1}}$ and $\vec{v_{2}}$, we perform a simple calculation.
    We first calculate the contribution of each dimension $d$ to the Euclidean distance
    as a measure of the general importance of each dimension. The importance score
    is defined as:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Lisa（LUAR + $W$）执行此任务，并在一些任务实例上展示可解释性。为了排名两个风格向量$\vec{v_{1}}$和$\vec{v_{2}}$之间的顶级共同或独特风格属性，我们进行简单的计算。我们首先计算每个维度$d$对欧几里得距离的贡献，以衡量每个维度的一般重要性。重要性评分定义为：
- en: '|  | $\mathcal{I}(d)={\lVert\vec{v_{2}}-\vec{v_{1}}\rVert}_{2}-\sqrt{{{\sum\limits_{k=0}^{D}}}{\begin{cases}(\vec{v_{2}}_{k}-\vec{v_{1}}_{k})^{2}&amp;k\neq
    d\\ 0&amp;k=d\end{cases}}}$ |  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{I}(d)={\lVert\vec{v_{2}}-\vec{v_{1}}\rVert}_{2}-\sqrt{{{\sum\limits_{k=0}^{D}}}{\begin{cases}(\vec{v_{2}}_{k}-\vec{v_{1}}_{k})^{2}&amp;k\neq
    d\\ 0&amp;k=d\end{cases}}}$ |  |'
- en: 'To retrieve the top common style attributes, we rank the dimensions, and the
    corresponding style attributes, in descending order by the following score function:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取最常见的风格属性，我们根据以下评分函数对维度及其对应的风格属性进行降序排名：
- en: '|  | $\textsc{Score}_{\text{common}}(d)=\frac{\mathcal{I}(d)}{\sum\limits_{k=0}^{D}{\mathcal{I}(k)}}*\vec{v_{1}}_{d}*\vec{v_{2}}_{d}$
    |  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{Score}_{\text{common}}(d)=\frac{\mathcal{I}(d)}{\sum\limits_{k=0}^{D}{\mathcal{I}(k)}}*\vec{v_{1}}_{d}*\vec{v_{2}}_{d}$
    |  |'
- en: 'To retrieve the top distinct style attributes, we rank the dimensions, and
    the corresponding style attributes, in descending order by the following score
    function:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取最独特的风格属性，我们根据以下评分函数对维度及其对应的风格属性进行降序排名：
- en: '|  | $\textsc{Score}_{\text{distinct}}(d)=\frac{\mathcal{I}(d)}{\sum\limits_{k=0}^{D}{\mathcal{I}(k)}}*\text{max}(\vec{v_{1}}_{d},\vec{v_{2}}_{d})*\bigl{(}1.0-\text{min}(\vec{v_{1}}_{d},\vec{v_{2}}_{d})\bigr{)}$
    |  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{Score}_{\text{distinct}}(d)=\frac{\mathcal{I}(d)}{\sum\limits_{k=0}^{D}{\mathcal{I}(k)}}*\text{max}(\vec{v_{1}}_{d},\vec{v_{2}}_{d})*\bigl{(}1.0-\text{min}(\vec{v_{1}}_{d},\vec{v_{2}}_{d})\bigr{)}$
    |  |'
- en: Appendix H Resources
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 资源
- en: We provide links and citations to resources used in this paper which provide
    license information, documentation, and their intended use. Our usage follows
    the intended usage of all resources.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了本文中使用的资源的链接和引用，这些资源提供了许可信息、文档和预期用途。我们的使用遵循所有资源的预期用途。
- en: 'We utilize the following models:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下模型：
- en: •
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\text{GPT-3}_{\text{175B}}$ (text-davinci-003) (Brown et al., [2020](#bib.bib8);
    Ouyang et al., [2022](#bib.bib37))
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\text{GPT-3}_{\text{175B}}$ (text-davinci-003) (Brown et al., [2020](#bib.bib8);
    Ouyang et al., [2022](#bib.bib37))
- en: •
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: EncT5 (t5-base) (Devlin et al., [2019](#bib.bib14); Liu et al., [2019](#bib.bib32))
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: EncT5 (t5-base) (Devlin et al., [2019](#bib.bib14); Liu et al., [2019](#bib.bib32))
- en: •
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DistilRoBERTa (nli-distilroberta-base-v2) (Sanh et al., [2019](#bib.bib50);
    Liu et al., [2019](#bib.bib32); Devlin et al., [2019](#bib.bib14); Reimers and
    Gurevych, [2019](#bib.bib46))
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DistilRoBERTa (nli-distilroberta-base-v2) (Sanh et al., [2019](#bib.bib50);
    Liu et al., [2019](#bib.bib32); Devlin et al., [2019](#bib.bib14); Reimers and
    Gurevych, [2019](#bib.bib46))
- en: •
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Learning Universal Authorship Representations (LUAR) Embedding model (Rivera-Soto
    et al., [2021](#bib.bib48))
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习通用作者表示 (LUAR) 嵌入模型 (Rivera-Soto et al., [2021](#bib.bib48))
- en: •
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Style embedding model from Wegmann et al. ([2022](#bib.bib61))
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Wegmann 等人的风格嵌入模型 ([2022](#bib.bib61))
- en: 'We utilize the following datasets:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下数据集：
- en: •
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reddit Million User Dataset (Khan et al., [2021](#bib.bib27); Andrews and Bishop,
    [2019](#bib.bib3))
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Reddit 百万用户数据集 (Khan et al., [2021](#bib.bib27); Andrews and Bishop, [2019](#bib.bib3))
- en: •
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: STEL dataset (Wegmann and Nguyen, [2021](#bib.bib60))
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: STEL 数据集 (Wegmann and Nguyen, [2021](#bib.bib60))
- en: •
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Contrastive Authorship Verification dataset (Wegmann et al., [2022](#bib.bib61))
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对比作者身份验证数据集 (Wegmann et al., [2022](#bib.bib61))
- en: •
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Formality in Online Communication (Pavlick and Tetreault, [2016](#bib.bib40))
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在线沟通中的正式性 (Pavlick and Tetreault, [2016](#bib.bib40))
- en: •
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Grammarly’s Yahoo Answers Formality Corpus (Rao and Tetreault, [2018](#bib.bib45))
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Grammarly 的 Yahoo Answers 正式性语料库 (Rao and Tetreault, [2018](#bib.bib45))
- en: •
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Yelp Reviews Dataset (Zhang et al., [2015](#bib.bib65))
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Yelp 评论数据集 (Zhang et al., [2015](#bib.bib65))
- en: •
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: IMDB Large Movie Review Dataset (Maas et al., [2011](#bib.bib33))
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IMDB 大型电影评论数据集 (Maas et al., [2011](#bib.bib33))
- en: •
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Amazon Customer Reviews Dataset (Amazon.com, [2018](#bib.bib2)) –  [https://s3.amazonaws.com/amazon-reviews-pds/readme.html](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 亚马逊客户评论数据集 (Amazon.com, [2018](#bib.bib2)) – [https://s3.amazonaws.com/amazon-reviews-pds/readme.html](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)
- en: •
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rotten Tomatoes Movie Review Data (Pang and Lee, [2005](#bib.bib38))
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Rotten Tomatoes 电影评论数据 (Pang and Lee, [2005](#bib.bib38))
- en: •
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: App Reviews Dataset (Grano et al., [2017](#bib.bib17))
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用评论数据集 (Grano et al., [2017](#bib.bib17))
- en: •
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Twitter Sentiment Analysis Training Corpus (Naji, [2012](#bib.bib35))
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Twitter 情感分析训练语料库 (Naji, [2012](#bib.bib35))
- en: •
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DAIR.AI Emotion Dataset (Saravia et al., [2018](#bib.bib51))
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DAIR.AI 情感数据集 (Saravia et al., [2018](#bib.bib51))
- en: •
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GoEmotions Dataset (Demszky et al., [2020](#bib.bib13))
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GoEmotions 数据集 (Demszky et al., [2020](#bib.bib13))
- en: •
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Political Slant Dataset (Prabhumoye et al., [2018](#bib.bib41))
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 政治倾向数据集 (Prabhumoye et al., [2018](#bib.bib41))
- en: •
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Twitter User Gender Classification Dataset (CrowdFlower, [2017](#bib.bib10))
    –  [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Twitter 用户性别分类数据集 (CrowdFlower, [2017](#bib.bib10)) – [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)
- en: •
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: African-American Vernacular English Dataset (Groenwold et al., [2020](#bib.bib18))
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非裔美国人口音英语数据集 (Groenwold et al., [2020](#bib.bib18))
- en: •
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Shakespeare Dataset (Xu, [2017](#bib.bib63))
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Shakespeare Dataset（Xu， [2017](#bib.bib63)）
- en: •
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Wikipedia Bias Dataset (Pryzant et al., [2020](#bib.bib42))
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Wikipedia Bias Dataset（Pryzant 等， [2020](#bib.bib42)）
- en: •
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: HateSpeech18 (de Gibert et al., [2018](#bib.bib12))
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HateSpeech18（de Gibert 等， [2018](#bib.bib12)）
- en: •
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Offensive Social Media Dataset (Atwell et al., [2022](#bib.bib4))
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Offensive Social Media Dataset（Atwell 等， [2022](#bib.bib4)）
- en: •
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Simple Wikipedia Dataset (Coster and Kauchak, [2011](#bib.bib9))
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Simple Wikipedia Dataset（Coster 和 Kauchak， [2011](#bib.bib9)）
- en: •
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ASSET (Alva-Manchego et al., [2020](#bib.bib1))
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ASSET（Alva-Manchego 等， [2020](#bib.bib1)）
- en: •
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CoLA (Warstadt et al., [2019](#bib.bib59))
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoLA（Warstadt 等， [2019](#bib.bib59)）
- en: •
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BLiMP (Warstadt et al., [2019](#bib.bib59))
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BLiMP（Warstadt 等， [2019](#bib.bib59)）
- en: 'We utilize the following software:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了以下软件：
- en: •
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transformers (Wolf et al., [2019](#bib.bib62))
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformers（Wolf 等， [2019](#bib.bib62)）
- en: •
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sentence-Transformers (Reimers and Gurevych, [2019](#bib.bib46))
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Sentence-Transformers（Reimers 和 Gurevych， [2019](#bib.bib46)）
- en: •
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: emoji –  [https://pypi.org/project/sentence-splitter/](https://pypi.org/project/sentence-splitter/)
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: emoji –  [https://pypi.org/project/sentence-splitter/](https://pypi.org/project/sentence-splitter/)
- en: •
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: sentence-splitter –  [https://pypi.org/project/sentence-splitter/](https://pypi.org/project/sentence-splitter/)
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: sentence-splitter –  [https://pypi.org/project/sentence-splitter/](https://pypi.org/project/sentence-splitter/)
- en: 'We estimate the total compute budget and detail computing infrastructure used
    to run the computational experiments found in this paper below:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 我们估算了总计算预算和详细的计算基础设施，用于运行本文中的计算实验：
- en: •
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 1x NVIDIA RTX A6000 / 30GB RAM / 4x CPU – 230 hours
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1x NVIDIA RTX A6000 / 30GB RAM / 4x CPU – 230 hours
