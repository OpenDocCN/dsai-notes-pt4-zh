- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 17:35:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:35:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Augmenting Interpretable Models with LLMs during Training
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLMs增强可解释模型的训练
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2209.11799](https://ar5iv.labs.arxiv.org/html/2209.11799)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2209.11799](https://ar5iv.labs.arxiv.org/html/2209.11799)
- en: 1]\orgdivMicrosoft Research
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 1]\orgdiv微软研究院
- en: 2]\orgdivUniversity of California, Berkeley
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 2]\orgdiv加州大学伯克利分校
- en: \fnmChandan \surSingh [chansingh@microsoft.com](mailto:chansingh@microsoft.com)
       \fnmArmin \surAskari [aaskari@berkeley.edu](mailto:aaskari@berkeley.edu)   
    \fnmRich \surCaruana [rcaruana@microsoft.com](mailto:rcaruana@microsoft.com)   
    \fnmJianfeng \surGao [jfgao@microsoft.com](mailto:jfgao@microsoft.com) [ [
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \fnmChandan \surSingh [chansingh@microsoft.com](mailto:chansingh@microsoft.com)
       \fnmArmin \surAskari [aaskari@berkeley.edu](mailto:aaskari@berkeley.edu)   
    \fnmRich \surCaruana [rcaruana@microsoft.com](mailto:rcaruana@microsoft.com)   
    \fnmJianfeng \surGao [jfgao@microsoft.com](mailto:jfgao@microsoft.com) [ [
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Recent large language models (LLMs) have demonstrated remarkable prediction
    performance for a growing array of tasks. However, their proliferation into high-stakes
    domains (e.g. medicine) and compute-limited settings has created a burgeoning
    need for interpretability and efficiency. We address this need by proposing Augmented
    Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned
    by LLMs to build extremely efficient and interpretable models. Aug-imodels use
    LLMs during fitting but not during inference, allowing complete transparency and
    often a speed/memory improvement of greater than 1,000x for inference compared
    to LLMs. We explore two instantiations of Aug-imodels in natural-language processing:
    (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings
    from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature
    expansions. Across a variety of text-classification datasets, both outperform
    their non-augmented counterparts. Aug-GAM can even outperform much larger models
    (e.g. a 6-billion parameter GPT-J model), despite having 10,000x fewer parameters
    and being fully transparent. We further explore Aug-imodels in a natural-language
    fMRI study, where they generate interesting interpretations from scientific data.
    All code for using Aug-imodels and reproducing results is made available on Github.¹¹1Scikit-learn-compatible
    API available at [\faGithub github.com/csinva/imodelsX](https://github.com/csinva/imodelsX)
    and experiments code available at [\faGithub github.com/microsoft/augmented-interpretable-models](https://github.com/microsoft/augmented-interpretable-models).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的大型语言模型（LLMs）在越来越多的任务中表现出显著的预测性能。然而，它们在高风险领域（例如医学）和计算资源有限的环境中的广泛应用，促使对解释性和效率的需求不断增长。我们通过提出增强型可解释模型（Aug-imodels）来满足这种需求，这是一个利用LLMs学习的知识来构建极其高效和可解释模型的框架。Aug-imodels
    在拟合过程中使用LLMs，但在推断过程中不使用LLMs，从而允许完全的透明度，并且在推断时通常比LLMs快/节省内存超过1,000倍。我们探讨了Aug-imodels在自然语言处理中的两个实例：（i）Aug-GAM，它通过来自LLM的解耦嵌入来增强广义加法模型，以及（ii）Aug-Tree，它通过LLM特征扩展来增强决策树。在各种文本分类数据集上，这两者都优于其未增强的对手。尽管Aug-GAM的参数比GPT-J模型少10,000倍，并且完全透明，但它甚至可以超越更大的模型（例如6亿参数的GPT-J模型）。我们还在自然语言fMRI研究中进一步探讨了Aug-imodels，其中它们从科学数据中生成有趣的解释。所有使用Aug-imodels和重现结果的代码都可以在Github上获得。¹¹1Scikit-learn兼容API可以在
    [\faGithub github.com/csinva/imodelsX](https://github.com/csinva/imodelsX) 上找到，实验代码可以在
    [\faGithub github.com/microsoft/augmented-interpretable-models](https://github.com/microsoft/augmented-interpretable-models)
    上找到。
- en: 'keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Explainability, Interpretability, Transparent models, XAI, Large language models
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性、透明模型、XAI、大型语言模型
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have demonstrated remarkable predictive performance
    across a growing range of diverse tasks [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)].
    However, their proliferation has led to two burgeoning problems. First, like most
    deep neural nets, LLMs have become increasingly difficult to interpret, often
    leading to them being characterized as black boxes and debilitating their use
    in high-stakes applications such as science [[4](#bib.bib4)], medicine [[5](#bib.bib5)],
    and policy-making [[6](#bib.bib6)]. Moreover, the use of black-box models such
    as LLMs has come under increasing scrutiny in settings where users require explanations
    or where models struggle with issues such as fairness [[7](#bib.bib7)] and regulatory
    pressure [[8](#bib.bib8)]. Second, black-box LLMs have grown to massive sizes,
    incurring enormous energy costs [[9](#bib.bib9)] and making them costly and difficult
    to deploy, particularly in low-compute settings (e.g. edge devices).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在越来越多的多样化任务中展示了显著的预测性能[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]。然而，它们的普及导致了两个新兴的问题。首先，像大多数深度神经网络一样，LLMs变得越来越难以解释，常常被称为黑箱，这削弱了它们在科学[[4](#bib.bib4)]、医学[[5](#bib.bib5)]和政策制定[[6](#bib.bib6)]等高风险应用中的使用。此外，黑箱模型如LLMs的使用在用户需要解释或模型在公平性[[7](#bib.bib7)]和监管压力[[8](#bib.bib8)]等问题上面临越来越多的审查。其次，黑箱LLMs的规模不断扩大，产生了巨大的能源成本[[9](#bib.bib9)]，使得它们在低计算环境（例如边缘设备）中部署成本高昂且困难。
- en: As an alternative to large black-box models, transparent models, such as generalized
    additive models [[10](#bib.bib10)] and decision trees [[11](#bib.bib11)] can maintain
    complete interpretability. Additionally, transparent models tend to be dramatically
    more computationally efficient than LLMs. While transparent models can sometimes
    perform as well as black-box LLMs [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15)], in many settings (such as natural-language processing (NLP)),
    there is often a considerable gap between the performance of transparent models
    and black-box LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为大型黑箱模型的替代品，透明模型，如广义加性模型[[10](#bib.bib10)]和决策树[[11](#bib.bib11)]，可以保持完全的可解释性。此外，透明模型通常比LLMs在计算上更为高效。虽然透明模型有时能达到与黑箱LLMs相当的性能[[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)]，但在许多场景（如自然语言处理（NLP））中，透明模型和黑箱LLMs之间的性能差距往往相当大。
- en: '![Refer to caption](img/0126952b5b3fba9ff046a08c91838067.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/0126952b5b3fba9ff046a08c91838067.png)'
- en: 'Figure 1: Aug-imodels use an LLM to augment an interpretable model during fitting
    but not inference (toy example for movie-review classification). (A) Aug-GAM fits
    an augmented additive model by extracting fixed-size embeddings for decoupled
    ngrams in a given sequence, summing them, and using them to train a supervised
    linear model. (B) At test time, Aug-GAM can be interpreted exactly as a generalized
    additive model. A linear coefficient for each ngram in the input is obtained by
    taking the dot product between the ngram’s embedding and the shared vector $w$.
    (C) Aug-Tree improves each split of a decision tree during fitting by (D) augmenting
    each keyphrase found by CART with similar keyphrases generated by an LLM.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Aug-imodels在拟合时使用LLM来增强一个可解释模型，但在推断时不使用（电影评论分类的玩具示例）。(A) Aug-GAM通过提取给定序列中解耦的n-gram的固定大小嵌入，求和，并使用它们来训练一个监督线性模型，从而拟合一个增强的加性模型。(B)
    在测试时，Aug-GAM可以被完全解释为一个广义加性模型。通过计算n-gram嵌入与共享向量$w$的点积，获得输入中每个n-gram的线性系数。(C) Aug-Tree在拟合过程中通过(D)用LLM生成的类似关键短语增强CART找到的每个关键短语，从而改进决策树的每个划分。
- en: We address this gap by proposing Augmented Interpretable Models (Aug-imodels),
    a framework to leverage the knowledge learned by LLMs to build extremely efficient
    and interpretable models. Specifically, we define an Aug-imodel as a method that
    leverages an LLM to fit an interpretable model, but *does not use the LLM during
    inference*. This allows complete transparency and often a substantial efficiency
    improvement (both in terms of speed and memory). Aug-imodels can address shortcomings
    in existing transparent models by using the world knowledge present in modern
    LLMs, such as information about feature correlations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过提出增强可解释模型（Aug-imodels）来解决这一差距，这是一种利用LLMs学习到的知识来构建极其高效且可解释模型的框架。具体而言，我们定义Aug-imodel为一种利用LLM来拟合可解释模型的方法，但*在推断时不使用LLM*。这允许完全透明，且通常带来显著的效率提升（无论是速度还是内存）。Aug-imodels可以通过利用现代LLMs中存在的世界知识（如特征关联信息）来弥补现有透明模型的不足。
- en: 'We explore two instantiations of Aug-imodels: (i) Aug-GAM, which augments a
    generalized additive model via decoupled embeddings from an LLM and (ii) Aug-Tree,
    which augments a decision tree with improved features generated by calling an
    LLM (see [Fig 1](#S1.F1 "In 1 Introduction ‣ Augmenting Interpretable Models with
    LLMs during Training")). At inference time, both are completely transparent and
    efficient: Aug-GAM requires only summing coefficients from a fixed dictionary
    while Aug-Tree requires checking for the presence of keyphrases in an input.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了两种 Aug-imodels 的实例：（i）Aug-GAM，它通过来自 LLM 的解耦嵌入增强广义加法模型，以及（ii）Aug-Tree，它通过调用
    LLM 生成的改进特征来增强决策树（见 [图 1](#S1.F1 "在引言中 ‣ 使用 LLM 增强可解释模型的训练")）。在推理阶段，两者都完全透明且高效：Aug-GAM
    只需对来自固定词典的系数进行求和，而 Aug-Tree 只需检查输入中是否存在关键短语。
- en: 'Across a variety of natural-language-processing datasets, both proposed Aug-imodels
    outperform their non-augmented counterparts. Aug-GAM can even outperform much
    larger models (e.g. a 6-billion parameter GPT-J model [[16](#bib.bib16)]), despite
    having 10,000x fewer parameters and no nonlinearities. We further explore Aug-imodels
    in a natural-language fMRI context, where we find that they can predict well and
    generate interesting interpretations. In what follows, [Sec 2](#S2 "2 Aug-imodels
    methodology: Aug-GAM and Aug-Tree ‣ Augmenting Interpretable Models with LLMs
    during Training") formally introduces Aug-imodels, [Sec 3](#S3 "3 Results: Prediction
    performance ‣ Augmenting Interpretable Models with LLMs during Training") and
    [Sec 4](#S4 "4 Interpreting fitted models ‣ Augmenting Interpretable Models with
    LLMs during Training") shows results for predictive performance and interpretation,
    [Sec 5](#S5 "5 Analyzing fMRI data with Aug-imodels ‣ Augmenting Interpretable
    Models with LLMs during Training") explores Aug-imodels in an fMRI prediction
    setting, [Sec 6](#S6 "6 Background and related work ‣ Augmenting Interpretable
    Models with LLMs during Training") reviews related work, and [Sec 7](#S7 "7 Discussion
    ‣ Augmenting Interpretable Models with LLMs during Training") concludes with a
    discussion.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种自然语言处理数据集上，提出的 Aug-imodels 都优于其未增强的对应模型。尽管参数数量比大型模型（例如 60 亿参数的 GPT-J 模型 [[16](#bib.bib16)]）少
    10,000 倍且没有非线性，Aug-GAM 甚至能超越这些更大的模型。我们进一步在自然语言 fMRI 背景下探讨 Aug-imodels，发现它们可以进行良好的预测并生成有趣的解释。接下来，[Sec
    2](#S2 "2 Aug-imodels 方法论：Aug-GAM 和 Aug-Tree ‣ 使用 LLM 增强可解释模型的训练") 正式介绍 Aug-imodels，[Sec
    3](#S3 "3 结果：预测性能 ‣ 使用 LLM 增强可解释模型的训练") 和 [Sec 4](#S4 "4 解释拟合模型 ‣ 使用 LLM 增强可解释模型的训练")
    显示预测性能和解释的结果，[Sec 5](#S5 "5 使用 Aug-imodels 分析 fMRI 数据 ‣ 使用 LLM 增强可解释模型的训练") 探索了在
    fMRI 预测设置中使用 Aug-imodels，[Sec 6](#S6 "6 背景和相关工作 ‣ 使用 LLM 增强可解释模型的训练") 评审了相关工作，[Sec
    7](#S7 "7 讨论 ‣ 使用 LLM 增强可解释模型的训练") 以讨论作为结尾。
- en: '2 Aug-imodels methodology: Aug-GAM and Aug-Tree'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 Aug-imodels 方法论：Aug-GAM 和 Aug-Tree
- en: 'In this section, [Sec 2.1](#S2.SS1 "2.1 Limitations of existing transparent
    methods ‣ 2 Aug-imodels methodology: Aug-GAM and Aug-Tree ‣ Augmenting Interpretable
    Models with LLMs during Training") overviews limitations of existing transparent
    methods, [Sec 2.2](#S2.SS2 "2.2 Aug-GAM method description ‣ 2 Aug-imodels methodology:
    Aug-GAM and Aug-Tree ‣ Augmenting Interpretable Models with LLMs during Training")
    introduces Aug-GAM, and [Sec 2.3](#S2.SS3 "2.3 Aug-Tree method description ‣ 2
    Aug-imodels methodology: Aug-GAM and Aug-Tree ‣ Augmenting Interpretable Models
    with LLMs during Training") introduces Aug-Tree.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，[Sec 2.1](#S2.SS1 "2.1 现有透明方法的局限性 ‣ 2 Aug-imodels 方法论：Aug-GAM 和 Aug-Tree
    ‣ 使用 LLM 增强可解释模型的训练") 概述了现有透明方法的局限性，[Sec 2.2](#S2.SS2 "2.2 Aug-GAM 方法描述 ‣ 2 Aug-imodels
    方法论：Aug-GAM 和 Aug-Tree ‣ 使用 LLM 增强可解释模型的训练") 介绍了 Aug-GAM，[Sec 2.3](#S2.SS3 "2.3
    Aug-Tree 方法描述 ‣ 2 Aug-imodels 方法论：Aug-GAM 和 Aug-Tree ‣ 使用 LLM 增强可解释模型的训练") 介绍了
    Aug-Tree。
- en: 2.1 Limitations of existing transparent methods
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 现有透明方法的局限性
- en: We are given a dataset of $n$ natural language strings $X_{\text{text}}$ and
    corresponding labels $\mathbf{y}\in\mathbb{R}^{n}$. In transparent modeling, often
    each string $x$ is represented by a bag-of-words, in which each feature $x_{i}$
    is a binary indicator (or count) of the presence of a single token (e.g. the word
    good). To model interactions between tokens, one can instead use a bag-of-ngrams
    representation, whereby each feature is formed by concatenating multiple tokens
    (e.g. the phrase not good). Using a bag-of-ngrams representation maps $X_{\text{text}}$
    to a feature matrix $X\in\mathbb{R}^{n\times p}$, where $p$ is the number of unique
    ngrams in $X_{\text{text}}$. While this representation enables interpretability,
    the number of ngrams in a dataset grows exponentially with the size of the ngram
    (how many tokens it contains) and the vocab-size; even for a modest vocab-size
    of 10,000 tokens, the number of possible trigrams is already $10^{12}$. This makes
    it difficult for existing transparent methods to model all trigrams without overfitting.
    Moreover, existing transparent methods completely fail to learn about ngrams not
    seen in the training set.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含$n$个自然语言字符串$X_{\text{text}}$及其对应标签$\mathbf{y}\in\mathbb{R}^{n}$的数据集。在透明建模中，每个字符串$x$通常由词袋模型表示，其中每个特征$x_{i}$是单一标记（例如，单词good）的存在的二进制指示符（或计数）。为了建模标记之间的交互，可以改用n-gram词袋表示法，其中每个特征由多个标记（例如，短语not
    good）连接而成。使用n-gram词袋表示法将$X_{\text{text}}$映射到特征矩阵$X\in\mathbb{R}^{n\times p}$，其中$p$是$X_{\text{text}}$中唯一n-gram的数量。尽管这种表示方法使模型具有可解释性，但数据集中n-gram的数量随着n-gram的大小（包含多少个标记）和词汇大小的增长呈指数级增长；即使对于10,000个标记的适中词汇大小，可能的三元组数量已经是$10^{12}$。这使得现有的透明方法难以在不发生过拟合的情况下建模所有三元组。此外，现有的透明方法完全无法学习训练集中未见过的n-gram。
- en: 'Preliminaries: GAMs'
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础知识：GAMs
- en: 'Generalized additive models, or GAMs [[10](#bib.bib10)] take the form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 广义加性模型，或GAMs [[10](#bib.bib10)] 的形式为：
- en: '|  | $\displaystyle g(\mathbb{E}[y])=\beta+f_{1}\left(x_{1}\right)+f_{2}\left(x_{2}\right)+\cdots+f_{K}\left(x_{p}\right),$
    |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g(\mathbb{E}[y])=\beta+f_{1}\left(x_{1}\right)+f_{2}\left(x_{2}\right)+\cdots+f_{K}\left(x_{p}\right),$
    |  | (1) |'
- en: where $\left(x_{1},x_{2},\ldots,x_{p}\right)$ are the input features (i.e. ngrams),
    $y$ is the target variable, $g(\cdot)$ is the link function (e.g., logistic function)
    and each $f_{i}$ is a univariate shape function with $\mathbb{E}\left[f_{i}\right]=0$.
    Due to the function’s additivity, each component function $f_{i}$ can be interpreted
    independently. Generalized linear models, such as logistic regression, are a special
    form of GAMs where each $f_{i}$ is restricted to be linear.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\left(x_{1},x_{2},\ldots,x_{p}\right)$是输入特征（即n-gram），$y$是目标变量，$g(\cdot)$是链接函数（例如，逻辑函数），每个$f_{i}$是一个一元形状函数，且$\mathbb{E}\left[f_{i}\right]=0$。由于函数的可加性，每个组件函数$f_{i}$可以独立解释。广义线性模型，如逻辑回归，是GAMs的一种特殊形式，其中每个$f_{i}$被限制为线性。
- en: 'Preliminaries: decision trees'
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础知识：决策树
- en: CART [[11](#bib.bib11)] fits a binary decision tree via recursive partitioning.
    When growing a tree, CART chooses for each node $\mathfrak{t}$ the split $s$ that
    maximizes the impurity decrease in the responses $\mathbf{y}$. For a given node
    $t$, the impurity decrease has the expression
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CART [[11](#bib.bib11)] 通过递归分区拟合一个二叉决策树。在生长树的过程中，CART为每个节点$\mathfrak{t}$选择使响应$\mathbf{y}$的不纯度减少最大化的分裂$s$。对于给定的节点$t$，不纯度减少的表达式是
- en: '|  | $\hat{\Delta}(s,\mathfrak{t},\mathbf{y}):=\sum_{\mathbf{x}_{i}\in\mathfrak{t}}h\left(y_{i},\bar{y}_{\mathfrak{t}}\right)-\sum_{\mathbf{x}_{i}\in\mathfrak{t}_{L}}h\left(y_{i},\bar{y}_{\mathfrak{t}_{L}}\right)-\sum_{\mathbf{x}_{i}\in\mathfrak{t}_{R}}h\left(y_{i},\bar{y}_{\mathfrak{t}_{R}}\right),$
    |  | (2) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\Delta}(s,\mathfrak{t},\mathbf{y}):=\sum_{\mathbf{x}_{i}\in\mathfrak{t}}h\left(y_{i},\bar{y}_{\mathfrak{t}}\right)-\sum_{\mathbf{x}_{i}\in\mathfrak{t}_{L}}h\left(y_{i},\bar{y}_{\mathfrak{t}_{L}}\right)-\sum_{\mathbf{x}_{i}\in\mathfrak{t}_{R}}h\left(y_{i},\bar{y}_{\mathfrak{t}_{R}}\right),$
    |  | (2) |'
- en: where $\mathfrak{t}_{L}$ and $\mathfrak{t}_{R}$ denote the left and right child
    nodes of $\mathfrak{t}$ respectively, and $\bar{y}_{\mathfrak{t}},\bar{y}_{\mathfrak{t}_{L}},\bar{y}_{\mathfrak{t}_{R}}$
    denote the mean responses in each of the nodes. For classification, $h(\cdot,\cdot)$
    corresponds to the Gini impurity, and for regression, $h(\cdot,\cdot)$ is the
    mean-squared error. Each split $s$ is a partition of the data based on a feature
    in $X$. To grow the tree, the splitting process is repeated recursively for each
    child node until a stopping criteria (e.g. a max depth) is satisfied. At inference
    time, we predict the response of an example by following its path from the root
    to a leaf and then predicting with the mean value found in that leaf.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathfrak{t}_{L}$ 和 $\mathfrak{t}_{R}$ 分别表示 $\mathfrak{t}$ 的左子节点和右子节点，$\bar{y}_{\mathfrak{t}},\bar{y}_{\mathfrak{t}_{L}},\bar{y}_{\mathfrak{t}_{R}}$
    表示各节点中的均值响应。对于分类，$h(\cdot,\cdot)$ 对应于基尼不纯度，对于回归，$h(\cdot,\cdot)$ 是均方误差。每个分裂 $s$
    是基于 $X$ 中的一个特征的数据划分。为了生长树，分裂过程对每个子节点递归重复，直到满足停止标准（例如最大深度）。在推断时，我们通过从根到叶子的路径来预测一个示例的响应，然后使用该叶子中找到的均值进行预测。
- en: 2.2 Aug-GAM method description
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 Aug-GAM 方法描述
- en: 'To remedy the issues with the GAM model in [Eq. 1](#S2.E1 "In Preliminaries:
    GAMs ‣ 2.1 Limitations of existing transparent methods ‣ 2 Aug-imodels methodology:
    Aug-GAM and Aug-Tree ‣ Augmenting Interpretable Models with LLMs during Training"),
    we propose Aug-GAM, an intuitive model which leverages a pre-trained LLM to extract
    a feature representation $\phi(x_{i})$ for each input ngram $x_{i}$. This allows
    learning only a single linear weight vector $w$ with a fixed dimension (which
    depends on the embedding dimension produced by the LLM), regardless of the number
    of ngrams. As a result, Aug-GAM can learn efficiently as the number of input features
    grows, and can also infer coefficients for unseen features. The fitted model is
    still a GAM, ensuring that the model can be precisely interpreted as a linear
    function of its inputs:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决[GAM模型中的问题](#S2.E1 "在初步介绍：GAMs ‣ 2.1 现有透明方法的局限性 ‣ 2 Aug-imodels 方法论：Aug-GAM
    和 Aug-Tree ‣ 在训练过程中通过LLMs增强可解释模型")，我们提出了Aug-GAM，这是一种直观的模型，利用预训练的LLM为每个输入ngram
    $x_{i}$ 提取特征表示 $\phi(x_{i})$。这使得无论ngram的数量如何，只需学习一个固定维度（取决于LLM生成的嵌入维度）的线性权重向量
    $w$。因此，Aug-GAM 可以随着输入特征数量的增加而高效学习，并且还能推断未见特征的系数。拟合的模型仍然是GAM，确保模型可以被精确地解释为其输入的线性函数：
- en: '|  | $g(\mathbb{E}[y])=\beta+w^{T}\sum_{i}\phi(x_{i})$ |  | (3) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $g(\mathbb{E}[y])=\beta+w^{T}\sum_{i}\phi(x_{i})$ |  | (3) |'
- en: 'Fitting Aug-GAM is similar to the popular approach of finetuning a single linear
    layer on top of LLM embeddings. However, it requires extra steps that separately
    extract/embed each ngram to keep the contributions to the prediction strictly
    additive across ngrams (see [Fig 1](#S1.F1 "In 1 Introduction ‣ Augmenting Interpretable
    Models with LLMs during Training")A): (i) Extracting ngrams: To ensure input ngrams
    are interpretable, ngrams are constructed using a word-level tokenizer (here,
    spaCy [[17](#bib.bib17)]). We select the size of ngrams to be used via cross-validation.
    (ii) Extracting embeddings: Each ngram is fed through the LLM to retrieve a fixed-size
    embedding.²²2If a transformer returns a variable-length embedding (e.g. the embedding
    is the size of the sequence length), we average over its variable-length dimension.
    A common alternative for bi-directional (masked) language models is to use the
    embedding for a special token (i.e. [CLS]), but we aim to keep the approach here
    more general. (iii) Summing embeddings: The embeddings of each ngram in the input
    are summed to yield a single fixed-size vector, ensuring additivity of the final
    model. (iv) Fitting the final linear model to make predictions: A linear model
    is fit on the summed embedding vector. We choose the link function $g$ to be the
    logit function (or the softmax for multi-class) for classification and the identity
    function for regression. In both cases, we add $\ell_{2}$ regularization over
    the parameters $w$ in [Eq. 3](#S2.E3 "In 2.2 Aug-GAM method description ‣ 2 Aug-imodels
    methodology: Aug-GAM and Aug-Tree ‣ Augmenting Interpretable Models with LLMs
    during Training").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合 Aug-GAM 类似于在 LLM 嵌入上微调单个线性层的流行方法。然而，它需要额外的步骤来分别提取/嵌入每个 ngram，以保持对预测的贡献在 ngram
    之间严格加性（见 [图 1](#S1.F1 "在 1 引言 ‣ 在训练期间通过 LLM 增强可解释模型")A）：（i）提取 ngram：为了确保输入的 ngram
    是可解释的，使用词级标记器（这里是 spaCy [[17](#bib.bib17)]）来构造 ngram。我们通过交叉验证选择要使用的 ngram 的大小。（ii）提取嵌入：每个
    ngram 通过 LLM 获取固定大小的嵌入。²²如果 transformer 返回可变长度的嵌入（例如，嵌入的大小为序列长度），我们在其可变长度维度上取平均。对于双向（掩蔽）语言模型，一个常见的替代方法是使用特殊标记的嵌入（即
    [CLS]），但我们这里的目标是保持方法更通用。（iii）求和嵌入：对输入中的每个 ngram 的嵌入求和，得到一个固定大小的向量，确保最终模型的加性。（iv）拟合最终线性模型以进行预测：在求和嵌入向量上拟合线性模型。我们选择链接函数
    $g$ 为逻辑函数（分类用）或 softmax（多类）以及回归的恒等函数。在这两种情况下，我们在参数 $w$ 上添加 $\ell_{2}$ 正则化，详见 [公式
    3](#S2.E3 "在 2.2 Aug-GAM 方法描述 ‣ 2 Aug-imodels 方法论：Aug-GAM 和 Aug-Tree ‣ 在训练期间通过
    LLM 增强可解释模型")。
- en: Computational considerations
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算考虑因素
- en: During fitting, Aug-GAM is inexpensive to fit as (1) the pre-trained LLM is
    not modified in any way, and can be any existing off-the-shelf model and (2) Aug-GAM
    only requires fitting a fixed-size linear model. After training, the model can
    be converted to a dictionary of scalar coefficients for each ngram, where the
    coefficient is the dot product between the ngram’s embedding and the fitted weight
    vector $w$ ([Fig 1](#S1.F1 "In 1 Introduction ‣ Augmenting Interpretable Models
    with LLMs during Training")B). This makes inference extremely fast and converts
    the model to have size equal to the number of fitted ngrams. When new ngrams are
    encountered at test-time, the coefficients for these ngrams can optionally be
    inferred by again taking the dot product between the ngram’s embedding and the
    fitted weight vector $w$;
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合过程中，Aug-GAM 拟合成本低廉，因为（1）预训练的 LLM 不会以任何方式修改，并且可以是任何现有的现成模型，（2）Aug-GAM 仅需拟合一个固定大小的线性模型。训练完成后，模型可以转换为每个
    ngram 的标量系数字典，其中系数是 ngram 嵌入与拟合权重向量 $w$ 的点积 ([图 1](#S1.F1 "在 1 引言 ‣ 在训练期间通过 LLM
    增强可解释模型")B)。这使得推理速度极快，并将模型转换为等于拟合的 ngram 数量的大小。当在测试时遇到新的 ngram 时，这些 ngram 的系数可以通过再次计算
    ngram 嵌入与拟合权重向量 $w$ 的点积来选择性地推断；
- en: 2.3 Aug-Tree method description
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 Aug-Tree 方法描述
- en: 'Aug-Tree improves upon a CART decision tree by augmenting features with generations
    from an LLM. This helps capture correlations between ngrams, including correlations
    with ngrams that are not present in the training data. Aug-Tree does not modify
    the objective in [Eq. 2](#S2.E2 "In Preliminaries: decision trees ‣ 2.1 Limitations
    of existing transparent methods ‣ 2 Aug-imodels methodology: Aug-GAM and Aug-Tree
    ‣ Augmenting Interpretable Models with LLMs during Training") but rather modifies
    the procedure for fitting each individual split $s$ ([Fig 1](#S1.F1 "In 1 Introduction
    ‣ Augmenting Interpretable Models with LLMs during Training")D). While CART restricts
    each split to a single ngram, Aug-Tree lets each split fit a disjunction of ngrams
    (e.g. ngram1 $\lor$ ngram2 $\lor$ ngram3). The disjunction allows a split to capture
    sparse interactions, such as synonyms in natural language. This can help mitigate
    overfitting by allowing individual splits to capture concrete concepts, rather
    than requiring many interacting splits.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'Aug-Tree 通过使用来自 LLM 的生成内容来改进 CART 决策树。这有助于捕捉 ngram 之间的相关性，包括训练数据中不存在的 ngram
    的相关性。Aug-Tree 不会修改 [Eq. 2](#S2.E2 "In Preliminaries: decision trees ‣ 2.1 Limitations
    of existing transparent methods ‣ 2 Aug-imodels methodology: Aug-GAM and Aug-Tree
    ‣ Augmenting Interpretable Models with LLMs during Training") 中的目标，而是修改拟合每个单独分裂
    $s$ 的过程（[Fig 1](#S1.F1 "In 1 Introduction ‣ Augmenting Interpretable Models with
    LLMs during Training")D）。虽然 CART 将每个分裂限制为单个 ngram，Aug-Tree 允许每个分裂适合 ngram 的析取（例如，ngram1
    $\lor$ ngram2 $\lor$ ngram3）。析取允许分裂捕捉稀疏的交互作用，例如自然语言中的同义词。这可以通过允许单独的分裂捕捉具体的概念，而不是要求多个交互分裂，从而帮助缓解过拟合。'
- en: 'When fitting each split, Aug-Tree starts with the ngram which maximizes the
    objective in [Eq. 2](#S2.E2 "In Preliminaries: decision trees ‣ 2.1 Limitations
    of existing transparent methods ‣ 2 Aug-imodels methodology: Aug-GAM and Aug-Tree
    ‣ Augmenting Interpretable Models with LLMs during Training"), just as CART would
    do, e.g. not good. Then, we query an LLM to generate similar ngrams to include
    in the split, e.g. bad, poor, awful, …, horrendous. Specifically, we query GPT-3
    (text-davinci-003) [[1](#bib.bib1)] with the prompt Generate 100 concise phrases
    that are very similar to the keyphrase:\nKeyphrase: “{keyphrase}”\n1. and parse
    the outputs into a list of ngrams. We greedily screen each ngram by evaluating
    the impurity of the split when including the ngram in the disjunction; we then
    exclude any ngram which increases the split’s impurity, resulting in a shortened
    list of ngrams, e.g. bad, poor, dull. See extended algorithm details in [Algorithm B2](#alg1
    "In Appendix B Aug-Tree ‣ Augmenting Interpretable Models with LLMs during Training").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '在拟合每个分裂时，Aug-Tree 从最大化 [Eq. 2](#S2.E2 "In Preliminaries: decision trees ‣ 2.1
    Limitations of existing transparent methods ‣ 2 Aug-imodels methodology: Aug-GAM
    and Aug-Tree ‣ Augmenting Interpretable Models with LLMs during Training") 中的目标的
    ngram 开始，就像 CART 会做的那样，例如，不好。然后，我们查询 LLM 生成类似的 ngram 以包含在分裂中，例如，bad，poor，awful，……，horrendous。具体来说，我们用提示
    Generate 100 concise phrases that are very similar to the keyphrase:\nKeyphrase:
    “{keyphrase}”\n1. 查询 GPT-3 (text-davinci-003) [[1](#bib.bib1)] 并将输出解析为 ngram 列表。我们贪婪地筛选每个
    ngram，通过评估在包含 ngram 的情况下分裂的不纯度；然后排除任何增加分裂不纯度的 ngram，从而得到缩短的 ngram 列表，例如，bad，poor，dull。有关扩展算法的详细信息，请参见
    [Algorithm B2](#alg1 "In Appendix B Aug-Tree ‣ Augmenting Interpretable Models
    with LLMs during Training")。'
- en: Computational considerations
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算考虑
- en: As opposed to Aug-GAM, Aug-Tree uses an LLM API rather than LLM embeddings,
    which may be more desirable depending on access to compute. The number of LLM
    calls required is proportional to the number of nodes in the tree. During inference,
    the LLM is no longer needed and making a prediction simply requires checking an
    input for the presence of specific ngrams along one path in the tree. Storing
    an Aug-GAM model requires memory proportional to the number of raw strings associated
    with tree splits, usually substantially reducing memory over the already small
    Aug-GAM model. We explore variations of Aug-Tree (such as using LLM embeddings
    rather than an API) in [Appendix B](#A2 "Appendix B Aug-Tree ‣ Augmenting Interpretable
    Models with LLMs during Training").
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Aug-GAM 相对，Aug-Tree 使用 LLM API 而不是 LLM 嵌入，这可能会根据计算资源的获取情况更为理想。所需的 LLM 调用次数与树中的节点数量成正比。在推断过程中，不再需要
    LLM，只需检查输入是否在树中的一条路径上存在特定的 ngram 即可进行预测。存储 Aug-GAM 模型需要的内存与与树分裂相关的原始字符串的数量成正比，通常显著减少了已经较小的
    Aug-GAM 模型的内存。我们在[附录 B](#A2 "Appendix B Aug-Tree ‣ Augmenting Interpretable Models
    with LLMs during Training")中探索了 Aug-Tree 的变体（例如，使用 LLM 嵌入而非 API）。
- en: '3 Results: Prediction performance'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 结果：预测性能
- en: 3.1 Experimental setup
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: Datasets
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: '[Table 1](#S3.T1 "In Datasets ‣ 3.1 Experimental setup ‣ 3 Results: Prediction
    performance ‣ Augmenting Interpretable Models with LLMs during Training") shows
    the datasets we study: 4 widely used text classification datasets spanning different
    domains (e.g. classifying the emotion of tweets [[18](#bib.bib18)], the sentiment
    of financial news sentences [[19](#bib.bib19)], or the sentiment of movie reviews [[20](#bib.bib20),
    [21](#bib.bib21)]), and 1 scientific text regression dataset (described in [Sec 5](#S5
    "5 Analyzing fMRI data with Aug-imodels ‣ Augmenting Interpretable Models with
    LLMs during Training")) [[22](#bib.bib22)]. Across datasets, the number of unique
    ngrams grows quickly from unigrams to bigrams to trigrams. Moreover, many ngrams
    appear very rarely, e.g., in the Financial Phrasebank (FPB) dataset, 91% of trigrams
    appear only once in the training dataset.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1](#S3.T1 "在数据集 ‣ 3.1 实验设置 ‣ 3 结果：预测性能 ‣ 在训练期间通过LLMs增强可解释模型") 显示了我们研究的数据集：4个广泛使用的文本分类数据集，涵盖不同领域（例如：对推文的情感进行分类[[18](#bib.bib18)]，对金融新闻句子的情感进行分类[[19](#bib.bib19)]，或对电影评论的情感进行分类[[20](#bib.bib20),
    [21](#bib.bib21)]），以及1个科学文本回归数据集（在[第 5 节](#S5 "5 使用Aug-imodels分析fMRI数据 ‣ 在训练期间通过LLMs增强可解释模型")中描述）[[22](#bib.bib22)]。在这些数据集中，从单词组到二元组再到三元组，唯一的n-gram数量迅速增长。此外，许多n-gram的出现频率非常低，例如，在金融短语库（FPB）数据集中，91%的三元组仅在训练数据集中出现一次。'
- en: 'Table 1: Overview of datasets studied here. The number of ngrams grows quickly
    with the size of the ngram.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：本研究中数据集的概述。n-gram的数量随着n-gram大小的增加而迅速增长。
- en: '|  | FPB | Rotten tomatoes | SST2 | Emotion | fMRI |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | FPB | 腐烂的番茄 | SST2 | 情感 | fMRI |'
- en: '| Samples (train) | 2,313 | 8,530 | 67,349 | 16,000 | 9,461 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 样本（训练） | 2,313 | 8,530 | 67,349 | 16,000 | 9,461 |'
- en: '| Samples (val) | 1,140 | 1,066 | 872 | 2,000 | 291 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 样本（验证） | 1,140 | 1,066 | 872 | 2,000 | 291 |'
- en: '| Classes | 3 | 2 | 2 | 6 | Regression |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 3 | 2 | 2 | 6 | 回归 |'
- en: '| Unigrams | 7,169 | 16,631 | 13,887 | 15,165 | 4,980 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 单词组 | 7,169 | 16,631 | 13,887 | 15,165 | 4,980 |'
- en: '| Bigrams | 28,481 | 93,921 | 72,501 | 106,201 | 27,247 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 二元组 | 28,481 | 93,921 | 72,501 | 106,201 | 27,247 |'
- en: '| Trigrams | 39,597 | 147,426 | 108,800 | 201,404 | 46,834 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 三元组 | 39,597 | 147,426 | 108,800 | 201,404 | 46,834 |'
- en: '| Trigrams that appear only once | 91% | 93% | 13% | 89% | 71% |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 仅出现一次的三元组 | 91% | 93% | 13% | 89% | 71% |'
- en: Aug-GAM settings
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Aug-GAM 设置
- en: 'We compare Aug-GAM to four interpretable baseline models: Bag of ngrams, TF-IDF
    (Term frequency–inverse document frequency) [[23](#bib.bib23)], GloVE [[24](#bib.bib24)]³³3We
    use the pre-trained Glove embeddings trained on Common Crawl (840 billion tokens,
    2.2 million vocab-size, cased, 300-dimensional vectors)., and a model trained
    on BERT embeddings for each unigram in the input (which can be viewed as running
    Aug-GAM with only unigrams). We use BERT (bert-base-uncased) [[3](#bib.bib3)]
    as the LLM for extracting embeddings, after finetuning on each dataset.⁴⁴4Pre-trained
    language models are retrieved from HuggingFace [[25](#bib.bib25)]. See [Table 4](#A1.T4
    "In Appendix A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training")
    for details on all models and downloadable checkpoints. In each case, a model
    is fit via cross-validation on the training set (to tune the amount of $\ell_{2}$
    regularization added) and its accuracy is evaluated on the test set.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Aug-GAM与四种可解释基准模型进行比较：n-gram包，TF-IDF（词频-逆文档频率）[[23](#bib.bib23)]，GloVE[[24](#bib.bib24)]³³3我们使用预训练的GloVe嵌入，训练于Common
    Crawl（8400亿标记，220万词汇量，大小写区分，300维向量）。，以及一个针对每个单词组在输入中的BERT嵌入训练的模型（这可以视为仅使用单词组运行Aug-GAM）。我们使用BERT（bert-base-uncased）[[3](#bib.bib3)]作为提取嵌入的LLM，在每个数据集上进行微调。⁴⁴4预训练语言模型从HuggingFace
    [[25](#bib.bib25)] 获取。有关所有模型和可下载检查点的详细信息，请参见[表 4](#A1.T4 "在附录A Aug-GAM ‣ 在训练期间通过LLMs增强可解释模型")。在每种情况下，通过交叉验证拟合模型（以调整添加的$\ell_{2}$正则化量），并在测试集上评估其准确性。
- en: Aug-Tree settings
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Aug-Tree 设置
- en: 'We compare Aug-Tree to two decision tree baselines: CART [[11](#bib.bib11)]
    and ID3 [[26](#bib.bib26)], and we use bigram features. In addition to individual
    trees, we fit bagging ensembles, where each tree is created using a bootstrap
    sample the same size as the original dataset (as done in Random Forest [[27](#bib.bib27)])
    and has depth 8. This hurts interpretability, but can improve predictive performance
    and calibration. For simplicity, we run Aug-GAM only in a binary classification
    setting; to do so, we take two opposite classes from each multiclass dataset (Negative/Positive
    for FPB and Sadness/Joy for Emotion).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Aug-Tree与两个决策树基线进行比较：CART [[11](#bib.bib11)] 和 ID3 [[26](#bib.bib26)]，并使用二元特征。除了单个树，我们还拟合了bagging集成，其中每棵树是使用与原始数据集大小相同的自举样本创建的（如随机森林[[27](#bib.bib27)]所做），且深度为8。这会影响可解释性，但可以改善预测性能和校准。为了简单起见，我们仅在二分类设置中运行Aug-GAM；为此，我们从每个多分类数据集中取两个相反的类别（FPB的负面/正面和情感的悲伤/快乐）。
- en: 3.2 Aug-GAM text-classification performance
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Aug-GAM文本分类性能
- en: Generalization as a function of ngram size
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一般化作为ngram大小的函数
- en: '[Fig 2](#S3.F2 "In Generalization as a function of ngram size ‣ 3.2 Aug-GAM
    text-classification performance ‣ 3 Results: Prediction performance ‣ Augmenting
    Interpretable Models with LLMs during Training")A shows the test accuracy of Aug-GAM
    as a function of the ngram size used for computing features. Aug-GAM outperforms
    the interpretable baselines, achieving a considerable increase in accuracy across
    three of the four datasets. Notably, Aug-GAM accuracy increases with ngram size,
    whereas the accuracy of baseline methods decreases or plateaus. This is likely
    due to Aug-GAM fitting only a fixed-size parameter vector, helping to prevent
    overfitting.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2](#S3.F2 "在ngram大小作为函数的一般化 ‣ 3.2 Aug-GAM文本分类性能 ‣ 3 结果：预测性能 ‣ 在训练期间用LLM增强可解释模型")A
    显示了Aug-GAM在用于计算特征的ngram大小的测试准确率。Aug-GAM在四个数据集中的三个数据集上都优于可解释基线，准确率有显著提高。值得注意的是，Aug-GAM的准确率随着ngram大小的增加而增加，而基线方法的准确率则减少或趋于平稳。这可能是因为Aug-GAM仅拟合固定大小的参数向量，有助于防止过拟合。'
- en: '| \begin{overpic}[width=173.44534pt,tics=10]{figs/acc_by_ngrams_full.pdf} \put(2.0,95.0){\large{A}}
    \end{overpic} | \begin{overpic}[width=195.12767pt,tics=10]{figs/acc_interpolate_gam.pdf}
    \put(0.0,83.0){\large{B}} \end{overpic} |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| \begin{overpic}[width=173.44534pt,tics=10]{figs/acc_by_ngrams_full.pdf} \put(2.0,95.0){\large{A}}
    \end{overpic} | \begin{overpic}[width=195.12767pt,tics=10]{figs/acc_interpolate_gam.pdf}
    \put(0.0,83.0){\large{B}} \end{overpic} |'
- en: 'Figure 2: (A) Test accuracy as a function of ngram size. As the ngram size
    (i.e. the number of tokens in the ngram) increases, the gap between Aug-GAM and
    the baselines grows. Averaged over three random cross-validation splits; error
    bars are standard errors of the mean (many are within the points). (B) Accuracy
    when using Aug-GAM in combination with BERT. A large percentage of samples can
    be accurately predicted with Aug-GAM.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图2: (A) 准确率作为ngram大小的函数。随着ngram大小（即ngram中的标记数量）的增加，Aug-GAM与基线之间的差距增大。对三个随机交叉验证拆分取平均；误差条为均值的标准误差（许多在点内）。(B)
    使用Aug-GAM与BERT结合时的准确率。可以用Aug-GAM准确预测大量样本。'
- en: Comparing Aug-GAM performance with black-box baselines
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较Aug-GAM与黑箱基线的性能
- en: '[Table 2](#S3.T2 "In Comparing Aug-GAM performance with black-box baselines
    ‣ 3.2 Aug-GAM text-classification performance ‣ 3 Results: Prediction performance
    ‣ Augmenting Interpretable Models with LLMs during Training") shows the test accuracy
    results for various models when choosing the size of ngrams via cross-validation.
    Compared with interpretable baselines, Aug-GAM shows considerable gains on three
    of the datasets and only a minor gain on the tweet dataset (Emotion), likely because
    this dataset requires fitting less high-order interactions.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2](#S3.T2 "在比较Aug-GAM与黑箱基线的性能 ‣ 3.2 Aug-GAM文本分类性能 ‣ 3 结果：预测性能 ‣ 在训练期间用LLM增强可解释模型")
    显示了在通过交叉验证选择ngram大小时各种模型的测试准确率结果。与可解释基线相比，Aug-GAM在三个数据集上显示了显著的提升，而在推文数据集（情感）上的提升较小，这可能是因为该数据集需要拟合的高阶交互较少。'
- en: Compared with the zero-shot performance of the much larger GPT models (6-billion
    parameter GPT-J [[16](#bib.bib16)] and 175-billion parameter GPT-3, text-davinci-002 [[1](#bib.bib1)])⁵⁵5Accuracy
    for GPT models is computed by averaging over human-written prompts take from PromptSource
    ([[28](#bib.bib28)]); see details in [Appendix A](#A1 "Appendix A Aug-GAM ‣ Augmenting
    Interpretable Models with LLMs during Training"))., Aug-GAM outperforms GPT-J.
    Aug-GAM lags slightly behind GPT-3 for binary classification problems (Rotten
    Tomatoes and SST2) but outperforms GPT-3 for multi-class classification problems
    (FPB and Emotion). The best black-box baseline (a BERT finetuned model) outperforms
    Aug-GAM by 4%-6% accuracy. This is potentially a reasonable tradeoff in settings
    where interpretability, speed, or memory bottlenecks are critical.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与更大的 GPT 模型（60 亿参数的 GPT-J [[16](#bib.bib16)] 和 1750 亿参数的 GPT-3, text-davinci-002
    [[1](#bib.bib1)])⁵⁵5 的零样本表现相比，Aug-GAM 超过了 GPT-J。Aug-GAM 在二分类问题（Rotten Tomatoes
    和 SST2）上略逊于 GPT-3，但在多分类问题（FPB 和 Emotion）上表现优于 GPT-3。最佳的黑箱基线（BERT 微调模型）在准确率上超越
    Aug-GAM 4%-6%。在可解释性、速度或内存瓶颈至关重要的环境中，这可能是一个合理的权衡。
- en: 'Table 2: Test accuracy for different models. Aug-GAM yields improvements over
    interpretable baselines and is competitive with some black-box baselines. Errors
    show standard error of the mean over 3 random data splits (or 3 different prompts
    for GPT models).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同模型的测试准确率。Aug-GAM 在可解释基线之上有提升，并且与一些黑箱基线具有竞争力。错误显示的是对 3 个随机数据拆分（或 GPT 模型的
    3 个不同提示）的均值的标准误差。
- en: '|  |  | FPB | Rotten tomatoes | SST2 | Emotion |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  |  | FPB | Rotten tomatoes | SST2 | Emotion |'
- en: '| Ours | Aug-GAM | 92.8  $\pm$ 0.37 | 81.6  $\pm$0.05 | 86.9  $\pm$ 0.10 |
    89.5  $\pm$ 0.03 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | Aug-GAM | 92.8 $\pm$ 0.37 | 81.6 $\pm$0.05 | 86.9 $\pm$ 0.10 | 89.5
    $\pm$ 0.03 |'
- en: '| Interpretable baselines | Bag of ngrams | 85.0 $\pm$ 0.11 | 75.0 $\pm$ 0.09
    | 82.8 $\pm$ 0.00 | 89.0 $\pm$ 0.09 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 可解释基线 | n-gram 包 | 85.0 $\pm$ 0.11 | 75.0 $\pm$ 0.09 | 82.8 $\pm$ 0.00 |
    89.0 $\pm$ 0.09 |'
- en: '| TF-IDF | 84.9 $\pm$ 0.16 | 75.9 $\pm$ 0.06 | 83.4 $\pm$ 0.11 | 89.2 $\pm$
    0.04 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| TF-IDF | 84.9 $\pm$ 0.16 | 75.9 $\pm$ 0.06 | 83.4 $\pm$ 0.11 | 89.2 $\pm$
    0.04 |'
- en: '| GloVe | 80.5 $\pm$ 0.06 | 78.7 $\pm$ 0.03 | 80.1 $\pm$ 0.10 | 73.1 $\pm$
    0.09 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| GloVe | 80.5 $\pm$ 0.06 | 78.7 $\pm$ 0.03 | 80.1 $\pm$ 0.10 | 73.1 $\pm$
    0.09 |'
- en: '| BERT unigram embeddings | 86.4 $\pm$ 0.13 | 76.8 $\pm$ 0.19 | 81.7 $\pm$
    0.07 | 87.2 $\pm$ 0.06 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| BERT unigram 嵌入 | 86.4 $\pm$ 0.13 | 76.8 $\pm$ 0.19 | 81.7 $\pm$ 0.07 | 87.2
    $\pm$ 0.06 |'
- en: '| Black-box baselines | BERT finetuned | 98.0 | 87.5 | 92.4 | 93.6 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 黑箱基线 | BERT 微调 | 98.0 | 87.5 | 92.4 | 93.6 |'
- en: '| GPT-3 | 39.6 $\pm$1.6 | 82.7 $\pm$3.3 | 90.5 $\pm$3.9 | 45.1 $\pm$4.1 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 | 39.6 $\pm$1.6 | 82.7 $\pm$3.3 | 90.5 $\pm$3.9 | 45.1 $\pm$4.1 |'
- en: '| GPT-J | 27.0 $\pm$1.9 | 58.9 $\pm$3.1 | 58.4 $\pm$2.8 | 19.3 $\pm$1.9 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| GPT-J | 27.0 $\pm$1.9 | 58.9 $\pm$3.1 | 58.4 $\pm$2.8 | 19.3 $\pm$1.9 |'
- en: Complementing Aug-GAM with a black-box model
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 补充 Aug-GAM 以黑箱模型
- en: 'In some settings, it may be useful to use Aug-GAM on relatively simple samples
    (for interpretability/memory/speed) but relegate relatively difficult samples
    to a black-box model. To study this setting, we first predict each sample with
    Aug-GAM, then assess its confidence (how close its predicted probability for the
    top class is to 1). If the confidence is above a pre-specified threshold, we use
    the Aug-GAM prediction. Otherwise, we compute the sample’s prediction using a
    finetuned BERT model. [Fig 2](#S3.F2 "In Generalization as a function of ngram
    size ‣ 3.2 Aug-GAM text-classification performance ‣ 3 Results: Prediction performance
    ‣ Augmenting Interpretable Models with LLMs during Training")B shows the accuracy
    for the entire test set as we vary the percentage of samples predicted with Aug-GAM.
    Since Aug-GAM yields probabilities that are reasonably calibrated (see [Fig 6](#A1.F6
    "In Appendix A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training")),
    rather than the accuracy linearly interpolating between Aug-GAM and BERT, a large
    percentage of samples can be predicted with Aug-GAM while incurring little to
    no drop in accuracy. For example, when using Aug-GAM on 50% of samples, the average
    drop in test accuracy is only 0.0053.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，使用 Aug-GAM 对相对简单的样本进行预测可能会更有用（考虑到可解释性/内存/速度），而将相对困难的样本交给黑箱模型处理。为了研究这种情况，我们首先使用
    Aug-GAM 对每个样本进行预测，然后评估其置信度（即其对顶级类别的预测概率距离 1 的程度）。如果置信度超过预先设定的阈值，我们使用 Aug-GAM 的预测。否则，我们使用微调过的
    BERT 模型计算样本的预测结果。[图 2](#S3.F2 "在 ngram 大小时的泛化 ‣ 3.2 Aug-GAM 文本分类性能 ‣ 3 结果：预测性能
    ‣ 在训练期间使用 LLM 扩展可解释模型")B 显示了我们在改变使用 Aug-GAM 进行预测的样本百分比时，对整个测试集的准确率。由于 Aug-GAM
    产生的概率经过合理的校准（见 [图 6](#A1.F6 "在附录 A Aug-GAM ‣ 在训练期间使用 LLM 扩展可解释模型")），而不是准确率在 Aug-GAM
    和 BERT 之间线性插值，大比例的样本可以使用 Aug-GAM 进行预测，同时几乎不会降低准确率。例如，当使用 Aug-GAM 预测 50% 的样本时，测试准确率的平均下降仅为
    0.0053。
- en: Tradeoffs between accuracy and efficiency
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准确率和效率之间的权衡
- en: 'In cases involving inference memory/speed, Aug-GAM can be converted to a dictionary
    of coefficients, whose size is the number of ngrams that appeared in training
    (see [Table 1](#S3.T1 "In Datasets ‣ 3.1 Experimental setup ‣ 3 Results: Prediction
    performance ‣ Augmenting Interpretable Models with LLMs during Training")). For
    a trigram model, this yields roughly a 1,000x reduction in model size compared
    to the $\sim$110 million trainable parameters in BERT, with much room for further
    size reduction (e.g. simply removing coefficients for trigrams that appear only
    once yields another 10-fold size reduction). Inference is nearly instantaneous,
    as it requires looking up coefficients in a dictionary and then a single sum (and
    does not require a GPU).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及推断内存/速度的情况下，Aug-GAM 可以被转换为一个系数字典，其大小为训练中出现的 ngram 数量（见 [表 1](#S3.T1 "在数据集
    ‣ 3.1 实验设置 ‣ 3 结果：预测性能 ‣ 在训练期间使用 LLM 扩展可解释模型")）。对于三元组模型，这使得模型大小相较于 BERT 中约 1.1
    亿可训练参数减少了大约 1,000 倍，并且还有进一步减小大小的空间（例如，仅移除出现次数为 1 的三元组的系数就能实现另外 10 倍的大小减少）。推断几乎是瞬时完成的，因为它只需在字典中查找系数，然后进行一次求和（且不需要
    GPU）。
- en: '[Sec A.1](#A1.SS1 "A.1 Test-time tradeoffs between accuracy and interpretability/speed
    ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training")
    explores accuracy/efficiency tradeoffs. For example, Aug-GAM performance degrades
    gracefully when the model is compressed by removing its smallest coefficients.
    In fact, the test accuracy of Aug-GAM models trained with 4-grams on the Emotion
    and Financial phrasebank datasets actually improves after removing 50% of the
    original coefficients ([Fig 7](#A1.F7 "In A.1 Test-time tradeoffs between accuracy
    and interpretability/speed ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable Models
    with LLMs during Training")A). Additionally, one can vary the size of ngrams used
    at test-time without a severe performance drop, potentially enabling compressing
    the model by orders of magnitude (see [Fig 7](#A1.F7 "In A.1 Test-time tradeoffs
    between accuracy and interpretability/speed ‣ Appendix A Aug-GAM ‣ Augmenting
    Interpretable Models with LLMs during Training")B, [Fig 8](#A1.F8 "In A.1 Test-time
    tradeoffs between accuracy and interpretability/speed ‣ Appendix A Aug-GAM ‣ Augmenting
    Interpretable Models with LLMs during Training")). For example, when fitting a
    model with 4-grams and testing with 3-grams, the average performance drop is $\sim$2%.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sec A.1](#A1.SS1 "A.1 测试时的准确性与可解释性/速度的权衡 ‣ 附录 A Aug-GAM ‣ 在训练过程中通过 LLMs 增强可解释模型")
    探讨了准确性/效率的权衡。例如，当通过移除最小系数来压缩模型时，Aug-GAM 的性能优雅地下降。事实上，在 Emotion 和 Financial phrasebank
    数据集上用 4-grams 训练的 Aug-GAM 模型在移除 50% 原始系数后，其测试准确性实际上有所提高（[Fig 7](#A1.F7 "在 A.1
    测试时的准确性与可解释性/速度的权衡 ‣ 附录 A Aug-GAM ‣ 在训练过程中通过 LLMs 增强可解释模型")A）。此外，可以在测试时调整使用的 ngrams
    大小而不会严重下降性能，从而可能使模型的压缩达到数量级（参见 [Fig 7](#A1.F7 "在 A.1 测试时的准确性与可解释性/速度的权衡 ‣ 附录 A
    Aug-GAM ‣ 在训练过程中通过 LLMs 增强可解释模型")B, [Fig 8](#A1.F8 "在 A.1 测试时的准确性与可解释性/速度的权衡 ‣
    附录 A Aug-GAM ‣ 在训练过程中通过 LLMs 增强可解释模型")）。例如，当用 4-grams 训练模型并用 3-grams 测试时，平均性能下降约为
    2%。'
- en: 3.3 Aug-Tree generalization performance
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 Aug-Tree 泛化性能
- en: 'We now investigate the predictive performance of Aug-Tree, measured by the
    test ROC AUC on the previous text classification datasets altered for binary classification.
    Note that the performance of all tree-based methods on the studied datasets is
    below the performance of the GAM methods in [Sec 3.2](#S3.SS2 "3.2 Aug-GAM text-classification
    performance ‣ 3 Results: Prediction performance ‣ Augmenting Interpretable Models
    with LLMs during Training") (see [Table 8](#A2.T8 "In B.1 Aug-Tree variations
    ‣ Appendix B Aug-Tree ‣ Augmenting Interpretable Models with LLMs during Training")
    for a direct comparison). Nevertheless, Aug-Tree models maintain potential advantages,
    such as storing far fewer parameters, clustering important features together,
    and better modeling long-range interactions.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在调查 Aug-Tree 的预测性能，通过在之前的文本分类数据集上进行二分类的测试 ROC AUC 来衡量。请注意，所有树基方法在研究的数据集上的性能均低于
    [Sec 3.2](#S3.SS2 "3.2 Aug-GAM 文本分类性能 ‣ 3 结果：预测性能 ‣ 在训练过程中通过 LLMs 增强可解释模型") 中
    GAM 方法的性能（参见 [Table 8](#A2.T8 "在 B.1 Aug-Tree 变体 ‣ 附录 B Aug-Tree ‣ 在训练过程中通过 LLMs
    增强可解释模型") 进行直接比较）。尽管如此，Aug-Tree 模型仍保持潜在优势，如存储远少于的参数、将重要特征聚集在一起，并更好地建模长期交互。
- en: '[Fig 3](#S3.F3 "In 3.3 Aug-Tree generalization performance ‣ 3 Results: Prediction
    performance ‣ Augmenting Interpretable Models with LLMs during Training")A shows
    the performance for Aug-Tree as a function of tree depth compared to decision
    tree baselines. Aug-Tree shows improvements that are sometimes small (e.g. for
    Financial phrasebank) and sometimes relatively large (e.g. for Emotion). [Fig 3](#S3.F3
    "In 3.3 Aug-Tree generalization performance ‣ 3 Results: Prediction performance
    ‣ Augmenting Interpretable Models with LLMs during Training")B shows the performance
    of a bagging ensemble of trees with different tree methods used as the base estimator.
    Here, using Aug-Tree shows a reliable and significant gain across all datasets
    compared to ensembles of baseline decision-tree methods. This suggests that LLM
    augmentation may help to diversify or decorrelate individual trees in the ensemble.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3](#S3.F3 "在 3.3 Aug-Tree 泛化性能 ‣ 3 结果：预测性能 ‣ 在训练过程中使用 LLM 扩展可解释模型")A 显示了
    Aug-Tree 的性能，作为树深度的函数，与决策树基线进行比较。Aug-Tree 显示出改进，有时是微小的（例如，对 Financial phrasebank），有时是相对较大的（例如，对
    Emotion）。[图 3](#S3.F3 "在 3.3 Aug-Tree 泛化性能 ‣ 3 结果：预测性能 ‣ 在训练过程中使用 LLM 扩展可解释模型")B
    显示了使用不同树方法作为基估计器的树的 bagging 集合的性能。在这里，与基线决策树方法的集合相比，使用 Aug-Tree 显示出在所有数据集上的可靠和显著的提升。这表明
    LLM 扩展可能有助于多样化或去相关集成中的单个树。'
- en: '| ![Refer to caption](img/23a60582d00815b6ef82164249a4914e.png) | ![Refer to
    caption](img/ae99957eed49541642a2f1e7e7b2614e.png) | ![Refer to caption](img/e210eaf6215919a1afbc14caed183bb2.png)
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/23a60582d00815b6ef82164249a4914e.png) | ![参见说明](img/ae99957eed49541642a2f1e7e7b2614e.png)
    | ![参见说明](img/e210eaf6215919a1afbc14caed183bb2.png) |'
- en: '| (A) Individual tree | (B) Bagging ensemble |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| (A) 单个树 | (B) Bagging 集合 |  |'
- en: 'Figure 3: Test performance as a function of (A) tree depth and (B) number of
    estimators. Values are averaged over 3 random dataset splits; error bars show
    the standard error of the mean (many are within the points).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：作为（A）树深度和（B）估计器数量的函数的测试性能。值是对 3 次随机数据集拆分的平均值；误差条显示了均值的标准误差（许多都在点内）。
- en: Varying Aug-imodels settings
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 变化的 Aug-imodels 设置
- en: We investigate many variations in the settings used for Aug-imodels. [Table 7](#A2.T7
    "In B.1 Aug-Tree variations ‣ Appendix B Aug-Tree ‣ Augmenting Interpretable Models
    with LLMs during Training") shows variations of different hyperparameters for
    Aug-Tree, such as using embeddings or dataset-specific prompts to expand keyphrases.
    [Table 5](#A1.T5 "In Evaluating zero-shot accuracy with language models ‣ Appendix
    A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training") shows
    how generalization accuracy changes when the LLM used to extract embeddings for
    Aug-GAM is varied, or different layers / ngram selection techniques are used.
    Across the variations, embeddings from finetuned models yield considerably better
    results than embeddings from non-finetuned models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了 Aug-imodels 使用的设置中的许多变化。[表 7](#A2.T7 "在 B.1 Aug-Tree 变体 ‣ 附录 B Aug-Tree
    ‣ 在训练过程中使用 LLM 扩展可解释模型") 显示了 Aug-Tree 的不同超参数变体，例如使用嵌入或数据集特定的提示来扩展关键词短语。[表 5](#A1.T5
    "在使用语言模型评估零样本准确性 ‣ 附录 A Aug-GAM ‣ 在训练过程中使用 LLM 扩展可解释模型") 显示了当用于提取 Aug-GAM 嵌入的
    LLM 变化，或使用不同的层 / ngram 选择技术时，泛化准确性如何变化。在这些变化中，来自微调模型的嵌入比来自未微调模型的嵌入产生了显著更好的结果。
- en: 4 Interpreting fitted models
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 解释拟合模型
- en: In this section, we interpret fitted Aug-imodels. We first inspect an Aug-GAM
    model fitted using unigram and bigram features on the SST2 dataset which achieves
    84% test accuracy. Next, we analyze the keyphrase expansions made in fitted Aug-Tree
    bagging ensembles.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们解释拟合的 Aug-imodels。我们首先检查一个使用 unigram 和 bigram 特征在 SST2 数据集上拟合的 Aug-GAM
    模型，该模型实现了 84% 的测试准确率。接下来，我们分析了拟合的 Aug-Tree bagging 集合中所做的关键词短语扩展。
- en: Fitted Aug-GAM coefficients match human scores
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 拟合的 Aug-GAM 系数与人工评分匹配
- en: A fitted Aug-GAM model can be interpreted for a single prediction (i.e. getting
    a score for each ngram in a single input, as in [Fig 1](#S1.F1 "In 1 Introduction
    ‣ Augmenting Interpretable Models with LLMs during Training")) or for an entire
    dataset (i.e. by inspecting its fitted coefficients). [Fig 4](#S4.F4 "In Fitted
    Aug-GAM coefficients match human scores ‣ 4 Interpreting fitted models ‣ Augmenting
    Interpretable Models with LLMs during Training")A visualizes the fitted Aug-GAM
    coefficients (i.e. the contribution to the prediction $w^{T}\phi(x_{i})$) with
    the largest absolute values across the SST2 dataset. To show a diversity of ngrams,
    we show every fifth ngram. The fitted coefficients are semantically reasonable
    and many contain strong interactions (e.g. not very is assigned to be negative
    whereas without resorting is assigned to be positive). This form of model visualization
    allows a user to audit the model with prior knowledge. Moreover, these coefficients
    are exact and therefore avoid summarizing interactions, making them considerably
    more faithful than post-hoc methods, such as LIME [[29](#bib.bib29)] and SHAP [[30](#bib.bib30)]
    (see [Sec A.2](#A1.SS2 "A.2 Comparison with post-hoc feature importance ‣ Appendix
    A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training") for a
    comparison).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个拟合的 Aug-GAM 模型可以用于单个预测（即为单个输入中的每个 n-gram 评分，如 [图 1](#S1.F1 "在 1 引言 ‣ 在训练中利用
    LLM 扩展可解释模型")）或整个数据集（即通过检查其拟合系数）。[图 4](#S4.F4 "在拟合的 Aug-GAM 系数与人工分数匹配 ‣ 4 解释拟合模型
    ‣ 在训练中利用 LLM 扩展可解释模型") A 可视化了 SST2 数据集中具有最大绝对值的拟合 Aug-GAM 系数（即对预测 $w^{T}\phi(x_{i})$
    的贡献）。为了展示 n-gram 的多样性，我们显示每第五个 n-gram。拟合的系数在语义上是合理的，并且许多系数包含强交互作用（例如，"not very"
    被指定为负值，而 "without resorting" 被指定为正值）。这种模型可视化方式允许用户使用先验知识审核模型。此外，这些系数是精确的，因此避免了总结交互作用，使其比事后方法，如
    LIME [[29](#bib.bib29)] 和 SHAP [[30](#bib.bib30)]，更为忠实（见 [附录 A.2](#A1.SS2 "A.2
    与事后特征重要性比较 ‣ 附录 A Aug-GAM ‣ 在训练中利用 LLM 扩展可解释模型") 进行比较）。
- en: '[Fig 4](#S4.F4 "In Fitted Aug-GAM coefficients match human scores ‣ 4 Interpreting
    fitted models ‣ Augmenting Interpretable Models with LLMs during Training")B compares
    the fitted Aug-GAM coefficients to human-labeled sentiment phrase scores for unigrams/bigrams
    in SST (note: these continuous scores are separate from the binary sentence labels
    used for training in the SST2 dataset). Both are centered, so that 0 is neutral
    sentiment and positive/negative values correspond to positive/negative sentiment,
    respectively. There is a strong positive correlation between the coefficients
    and the human-labeled scores (Spearman rank correlation $\rho=0.63$), which considerably
    improves over coefficients from a bag-of-bigrams model trained on SST2 ($\rho=0.39$).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4](#S4.F4 "在拟合的 Aug-GAM 系数与人工分数匹配 ‣ 4 解释拟合模型 ‣ 在训练中利用 LLM 扩展可解释模型") B 比较了拟合的
    Aug-GAM 系数与 SST 中单字/双字短语的人工标注情感分数（注意：这些连续分数与 SST2 数据集中用于训练的二元句子标签分开）。两者都已中心化，使得
    0 为中性情感，正值/负值分别对应于正面/负面情感。系数与人工标注分数之间存在强正相关（Spearman 排名相关 $\rho=0.63$），显著优于在 SST2
    上训练的二元组模型的系数（$\rho=0.39$）。'
- en: '| \begin{overpic}[width=286.19078pt,tics=10]{figs/ngram-contribution.pdf} \put(1.0,30.0){\large{A}}
    \end{overpic} | \begin{overpic}[width=143.09538pt,tics=10]{figs/sst_embgam.png}
    \put(1.0,66.0){\large{B}} \end{overpic} |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| \begin{overpic}[width=286.19078pt,tics=10]{figs/ngram-contribution.pdf} \put(1.0,30.0){\large{A}}
    \end{overpic} | \begin{overpic}[width=143.09538pt,tics=10]{figs/sst_embgam.png}
    \put(1.0,66.0){\large{B}} \end{overpic} |'
- en: '| \begin{overpic}[width=286.19078pt,tics=10]{figs/trigram-unseen-contributions.pdf}
    \put(1.0,35.0){\large{C}} \end{overpic} | \begin{overpic}[width=143.09538pt,tics=10]{figs/sst_auggam_tri.png}
    \put(1.0,66.0){\large{D}} \end{overpic} |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| \begin{overpic}[width=286.19078pt,tics=10]{figs/trigram-unseen-contributions.pdf}
    \put(1.0,35.0){\large{C}} \end{overpic} | \begin{overpic}[width=143.09538pt,tics=10]{figs/sst_auggam_tri.png}
    \put(1.0,66.0){\large{D}} \end{overpic} |'
- en: 'Figure 4: Top and bottom contributing ngrams to an Aug-GAM model trained on
    SST2 bigrams are (A) qualitatively semantically accurate and (B) match human-labeled
    phrase sentiment scores. For the same Aug-GAM model, which is trained only on
    bigrams, inferred trigrams coefficients are (C) qualitatively semantically accurate
    and (D) match human-labeled phrase sentiment scores.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：对 SST2 二元组训练的 Aug-GAM 模型中，顶部和底部贡献的 n-gram 是 (A) 质性上语义准确的，并且 (B) 与人工标注的短语情感分数匹配。对于相同的
    Aug-GAM 模型，该模型仅训练于二元组，推断出的三元组系数是 (C) 质性上语义准确的，并且 (D) 与人工标注的短语情感分数匹配。
- en: Inferred Aug-GAM coefficients for unseen ngrams match human scores
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推断出的 Aug-GAM 系数与未见过的 n-gram 的人工分数匹配
- en: One strength of Aug-GAM is its ability to infer linear coefficients for ngrams
    that were not seen during training. Whereas baseline models generally assign each
    unknown ngram the same coefficient (e.g. 0), Aug-GAM can effectively assign these
    new ngrams accurate coefficients. As one example, [Fig 4](#S4.F4 "In Fitted Aug-GAM
    coefficients match human scores ‣ 4 Interpreting fitted models ‣ Augmenting Interpretable
    Models with LLMs during Training")C shows that the Aug-GAM model trained only
    on bigrams in [Fig 4](#S4.F4 "In Fitted Aug-GAM coefficients match human scores
    ‣ 4 Interpreting fitted models ‣ Augmenting Interpretable Models with LLMs during
    Training")A/B can automatically infer coefficients for trigrams (which were not
    fit during training). The inferred coefficients are semantically meaningful, even
    capturing three-way interactions, such as not very amusing. To show a diversity
    of ngrams, we show every 20th ngram. [Fig 4](#S4.F4 "In Fitted Aug-GAM coefficients
    match human scores ‣ 4 Interpreting fitted models ‣ Augmenting Interpretable Models
    with LLMs during Training")D shows the coefficients compared to the human-labeled
    SST phrase sentiment for all trigrams in SST. Again, there is a strong correlation,
    where the Aug-GAM coefficients achieves a rank correlation $\rho=0.71$, which
    even outperforms the bag-of-words model directly trained on trigrams ($\rho=0.49$).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Aug-GAM 的一个优势是它能够推断在训练期间未见的 n-gram 的线性系数。而基线模型通常为每个未知的 n-gram 分配相同的系数（例如 0），Aug-GAM
    可以有效地为这些新的 n-gram 分配准确的系数。例如，[图 4](#S4.F4 "In Fitted Aug-GAM coefficients match
    human scores ‣ 4 Interpreting fitted models ‣ Augmenting Interpretable Models
    with LLMs during Training")C 显示了仅在 [图 4](#S4.F4 "In Fitted Aug-GAM coefficients
    match human scores ‣ 4 Interpreting fitted models ‣ Augmenting Interpretable Models
    with LLMs during Training")A/B 上训练的 Aug-GAM 模型能够自动推断三元组的系数（这些三元组在训练期间未进行拟合）。推断的系数具有语义意义，甚至捕捉了三元互动，例如
    not very amusing。为了展示 n-gram 的多样性，我们展示了每第 20 个 n-gram。[图 4](#S4.F4 "In Fitted
    Aug-GAM coefficients match human scores ‣ 4 Interpreting fitted models ‣ Augmenting
    Interpretable Models with LLMs during Training")D 显示了与 SST 中所有三元组的人工标注情感相比的系数。再次，存在强相关性，Aug-GAM
    系数实现了排名相关性 $\rho=0.71$，甚至优于直接在三元组上训练的 bag-of-words 模型（$\rho=0.49$）。
- en: Aug-Tree augmented splits contain relevant phrases
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Aug-Tree 增强的拆分包含相关短语。
- en: A fitted Aug-Tree model can be easily interpreted for a single prediction (i.e.
    by inspecting the ngrams that triggered relevant splits) or by visualizing the
    entire tree (e.g. [Fig 1](#S1.F1 "In 1 Introduction ‣ Augmenting Interpretable
    Models with LLMs during Training")C). Here, we additionally analyze how well each
    ngram found by CART matches the augmented ngrams found by the LLM; the better
    this match is, the easier it is to interpret a split.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合的 Aug-Tree 模型可以通过检查触发相关拆分的 n-gram 或通过可视化整个树（例如 [图 1](#S1.F1 "In 1 Introduction
    ‣ Augmenting Interpretable Models with LLMs during Training")C）轻松解释单个预测。在这里，我们还分析了
    CART 找到的每个 n-gram 与 LLM 发现的增强 n-gram 的匹配程度；这种匹配程度越好，解释拆分就越容易。
- en: '[Table 3](#S4.T3 "In Aug-Tree augmented splits contain relevant phrases ‣ 4
    Interpreting fitted models ‣ Augmenting Interpretable Models with LLMs during
    Training") shows examples of the ngrams which were most frequently augmented when
    fitting a bagging ensemble of 40 Aug-Trees to the four text-classification datasets
    in [Table 1](#S3.T1 "In Datasets ‣ 3.1 Experimental setup ‣ 3 Results: Prediction
    performance ‣ Augmenting Interpretable Models with LLMs during Training"). Added
    ngrams seem qualitatively reasonable, e.g. the keyphrase good expands to fine,
    highly, solid, …, valuable. We evaluate how well the expansions match the original
    CART ngram via human evaluation scores. Human evaluators are given the original
    ngram and the added ngrams, then instructed “You are given a keyphrase along with
    related keyphrases. On a scale of 1 (worst) to 5 (best), how well do the related
    keyphrases match the example keyphrase?”⁶⁶6Human evaluation scores are averaged
    over 3 PhD students in machine learning not affiliated with the study and 15 random
    ngrams from each dataset.. [Table 3](#S4.T3 "In Aug-Tree augmented splits contain
    relevant phrases ‣ 4 Interpreting fitted models ‣ Augmenting Interpretable Models
    with LLMs during Training") shows that the average human score for splits in each
    dataset is consistently greater than 4. This is substantially higher than the
    baseline score of 1.3 assigned by human evaluators when 15 ngrams and expansions
    are randomly paired and evaluated. [Table 6](#A2.T6 "In Appendix B Aug-Tree ‣
    Augmenting Interpretable Models with LLMs during Training") gives more details
    on ngram expansions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3](#S4.T3 "在 Aug-Tree 增强拆分包含相关短语 ‣ 4 解释拟合模型 ‣ 使用 LLM 训练期间增强可解释模型") 显示了在拟合
    40 个 Aug-Trees 的 bagging 集成模型时，对 [表 1](#S3.T1 "在数据集 ‣ 3.1 实验设置 ‣ 3 结果：预测性能 ‣ 使用
    LLM 训练期间增强可解释模型") 中四个文本分类数据集最频繁增强的 ngrams 的示例。添加的 ngrams 似乎在定性上是合理的，例如，关键短语 good
    扩展为 fine, highly, solid, …, valuable。我们通过人类评价评分评估扩展与原始 CART ngram 的匹配程度。人类评估者获得原始
    ngram 和添加的 ngrams，然后指示“你得到了一个关键短语和相关的关键短语。请在 1（最差）到 5（最佳）的评分范围内，评估相关关键短语与示例关键短语的匹配程度？”⁶⁶6人类评价评分是对
    3 名未参与该研究的机器学习博士生和每个数据集中的 15 个随机 ngrams 进行平均得出的。[表 3](#S4.T3 "在 Aug-Tree 增强拆分包含相关短语
    ‣ 4 解释拟合模型 ‣ 使用 LLM 训练期间增强可解释模型") 显示每个数据集中拆分的平均人类评分始终大于 4。这明显高于人类评估者在 15 个 ngrams
    和扩展随机配对并评估时给出的 1.3 的基线评分。[表 6](#A2.T6 "在附录 B Aug-Tree ‣ 使用 LLM 训练期间增强可解释模型") 提供了有关
    ngram 扩展的更多详细信息。'
- en: 'Table 3: Examples of most frequently augmented ngrams for each dataset when
    fitting an ensemble of 40 Aug-Trees. Human scores measure the similarity between
    an ngram and its expansion. They range from 1 (worst match) to 5 (best match),
    and the baseline score when ngrams and expansions are randomly paired and evaluated
    is 1.3$\pm$0.1. Error bars show standard error of the mean. Abbreviations: FPB
    = Financial Phrasebank, RT = Rotten tomatoes.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 显示了在拟合 40 个 Aug-Trees 的集成模型时，每个数据集中最频繁增强的 ngrams 的示例。人类评分衡量了 ngram 与其扩展之间的相似度。评分范围从
    1（最差匹配）到 5（最佳匹配），而 ngrams 和扩展随机配对并评估时的基线评分为 1.3$\pm$0.1。误差条表示均值的标准误差。缩写：FPB =
    Financial Phrasebank, RT = Rotten tomatoes。'
- en: '| Dataset | Human score | Example CART ngram | Added ngrams |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 人类评分 | 示例 CART ngram | 添加的 ngrams |'
- en: '| SST2 | 4.6$\pm$0.1 | good | fine, highly, solid, worthy, pleasing, satisfactory,
    outstanding, honorable, unwavering, valuable, … |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SST2 | 4.6$\pm$0.1 | good | fine, highly, solid, worthy, pleasing, satisfactory,
    outstanding, honorable, unwavering, valuable, … |'
- en: '| best | most remarkable, outstanding, superb, flawless, splendid, superlative,
    exceptional, impeccable, … |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| best | most remarkable, outstanding, superb, flawless, splendid, superlative,
    exceptional, impeccable, … |'
- en: '| RT | 4.4$\pm$0.1 | dull | dreary, uninteresting, lackluster, listless, lifeless,
    uninspired, wearisome, drab, joylessly, … |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| RT | 4.4$\pm$0.1 | dull | dreary, uninteresting, lackluster, listless, lifeless,
    uninspired, wearisome, drab, joylessly, … |'
- en: '| bad | unpleasant, dire, despicable, terrible, heinous, disgusting, vile,
    putrid, atrocious, nasty, poor, … |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| bad | unpleasant, dire, despicable, terrible, heinous, disgusting, vile,
    putrid, atrocious, nasty, poor, … |'
- en: '| Emotion | 4.4$\pm$0.2 | miserable | gloomy, disillusioned, pathetic, doomed,
    agonized, despairing, pointless, despondent, … |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Emotion | 4.4$\pm$0.2 | miserable | gloomy, disillusioned, pathetic, doomed,
    agonized, despairing, pointless, despondent, … |'
- en: '| sorry | embarrassed, sorrowful, remorseful, excuse, unsatisfied, guilt, regretful,
    forgive, apologies, … |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| sorry | embarrassed, sorrowful, remorseful, excuse, unsatisfied, guilt, regretful,
    forgive, apologies, … |'
- en: '| FPB | 4.2$\pm$0.2 | increased | widened, consolidated |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| FPB | 4.2$\pm$0.2 | increased | widened, consolidated |'
- en: '| fell | slipped, slumped, diminished, plunged, dropped, weakened, lost ground
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| fell | slipped, slumped, diminished, plunged, dropped, weakened, lost ground
    |'
- en: 5 Analyzing fMRI data with Aug-imodels
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 章 分析 fMRI 数据与 Aug-imodels
- en: We now explore Aug-imodels in a real-world neuroscience context. A central challenge
    in neuroscience is understanding how and where semantic concepts are represented
    in the brain. To meet this challenge, one line of study predicts the response
    of different brain voxels (i.e. small regions in space) to natural-language stimuli.
    We analyze data from a recent study in which the authors collect functional MRI
    (fMRI) responses as human subjects listen to hours of narrative stories [[22](#bib.bib22)].
    The fMRI responses studied here contain 95,556 voxels from a single subject, with
    9,461 time points used for training/cross-validation and 291 time points used
    for testing. We predict the continuous response for each voxel at each time point
    using the 20 words that precede the time point.⁷⁷7The most recent 4 words are
    skipped due to a time delay in the fMRI BOLD response. Seminal work on this task
    found that linear models of word vectors could effectively predict voxel responses [[31](#bib.bib31)],
    and more recent work shows that LLMs can further improve predictive performance [[32](#bib.bib32),
    [33](#bib.bib33)]. Aug-GAM is well-suited to this task, as it combines low-level
    word information with the contextualized information present in higher-order ngrams,
    both of which have been found to contribute to fMRI representations of text [[34](#bib.bib34)].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在现实世界的神经科学背景中探讨 Aug-imodels。神经科学中的一个核心挑战是理解语义概念如何以及在大脑中的何处被表示。为应对这一挑战，一种研究方向预测不同大脑体素（即空间中的小区域）对自然语言刺激的响应。我们分析了一项近期研究中的数据，其中作者收集了功能性
    MRI (fMRI) 响应，作为受试者听取数小时叙述故事时的反应[[22](#bib.bib22)]。这里研究的 fMRI 响应包含来自单个受试者的 95,556
    个体素，9,461 个时间点用于训练/交叉验证，291 个时间点用于测试。我们使用在时间点之前的 20 个词预测每个体素在每个时间点的连续响应。⁷⁷7由于
    fMRI BOLD 响应的时间延迟，最新的 4 个词被跳过。对这一任务的开创性工作发现，词向量的线性模型能够有效预测体素响应[[31](#bib.bib31)]，而近期的工作显示
    LLMs 可以进一步提高预测性能[[32](#bib.bib32), [33](#bib.bib33)]。Aug-GAM 非常适合这一任务，因为它结合了低级词信息和存在于高阶
    n-grams 中的上下文信息，这两者都被发现对 fMRI 文本表示有贡献[[34](#bib.bib34)]。
- en: '[Fig 5](#S5.F5 "In 5 Analyzing fMRI data with Aug-imodels ‣ Augmenting Interpretable
    Models with LLMs during Training")A visualizes the voxels in the cortex which
    are better predicted by Aug-GAM than BERT. The improvements are often spatially
    localized within well-studied brain regions such as auditory cortex (AC). [Fig 5](#S5.F5
    "In 5 Analyzing fMRI data with Aug-imodels ‣ Augmenting Interpretable Models with
    LLMs during Training")B shows that the test performance for Aug-GAM (measured
    by the Pearson correlation coefficient $\rho$) outperforms the black-box BERT
    baseline. [Appendix C](#A3 "Appendix C fMRI experiment details ‣ Augmenting Interpretable
    Models with LLMs during Training") gives further data details and comparisons,
    e.g. Aug-GAM also outperforms other linear baselines.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5](#S5.F5 "在第5章 分析 fMRI 数据与 Aug-imodels ‣ 通过训练中的 LLMs 增强可解释模型")A 直观展示了在皮层中，Aug-GAM
    比 BERT 更能有效预测的体素。这些改进通常局限于经过充分研究的大脑区域，如听觉皮层（AC）。[图 5](#S5.F5 "在第5章 分析 fMRI 数据与
    Aug-imodels ‣ 通过训练中的 LLMs 增强可解释模型")B 显示了 Aug-GAM 的测试性能（通过 Pearson 相关系数 $\rho$
    测量）优于黑箱 BERT 基线。[附录 C](#A3 "附录 C fMRI 实验细节 ‣ 通过训练中的 LLMs 增强可解释模型") 提供了更多数据细节和比较，例如
    Aug-GAM 也优于其他线性基线。'
- en: 'Going beyond prediction performance, [Fig 5](#S5.F5 "In 5 Analyzing fMRI data
    with Aug-imodels ‣ Augmenting Interpretable Models with LLMs during Training")C
    investigates a simple example of how Aug-GAM could help interpret an underlying
    brain region. We first select the voxel which is best-predicted by Aug-GAM (achieving
    a test correlation of 0.76) and then visualize the largest fitted Aug-GAM coefficients
    for that voxel. These correspond to which ngrams increase the activity of the
    fMRI voxel the most. Interestingly, these ngrams qualitatively correspond to understandable
    concepts: questioning, e.g. “are you sure”, often combined with disbelief/incredulity,
    e.g. “wow I never”. [Fig 5](#S5.F5 "In 5 Analyzing fMRI data with Aug-imodels
    ‣ Augmenting Interpretable Models with LLMs during Training")D shows two examples
    of voxels that are better predicted by Aug-Tree than Aug-GAM (Aug-Tree yields
    test correlations of 0.35 and 0.36). These two voxels are both related to someone
    speaking, but they seem to depend on interactions between the noun (me or you)
    and the verb (says). To elicit a large response both must be present, something
    which is difficult to capture in additive models, even with ngrams, since these
    words may not be close together in a sentence.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 超越预测性能，[图5](#S5.F5 "在5 分析fMRI数据与Aug-imodels ‣ 在训练期间用LLMs增强可解释模型")C 研究了Aug-GAM如何帮助解释一个潜在的大脑区域的简单示例。我们首先选择由Aug-GAM最佳预测的体素（测试相关性为0.76），然后可视化该体素的最大拟合Aug-GAM系数。这些系数对应于哪些n-grams最能提高fMRI体素的活动。有趣的是，这些n-grams在质上与可理解的概念相对应：例如，“你确定吗”常与不相信/难以置信的表述如“哇，我从未”结合。[图5](#S5.F5
    "在5 分析fMRI数据与Aug-imodels ‣ 在训练期间用LLMs增强可解释模型")D展示了两个由Aug-Tree比Aug-GAM更好预测的体素示例（Aug-Tree的测试相关性为0.35和0.36）。这两个体素都与某人说话有关，但似乎依赖于名词（我或你）和动词（说）的相互作用。要引发强烈反应，这两者都必须存在，这在加法模型中难以捕捉，即使使用n-grams，因为这些词在句子中可能并不相邻。
- en: This interpretation approach could be applied more rigorously to generate hypotheses
    for text inputs that activate brain regions, and then testing them with followup
    fMRI experiments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解释方法可以更严格地应用于生成假设，以激活大脑区域的文本输入，然后通过后续的fMRI实验来测试这些假设。
- en: '![Refer to caption](img/ddab9c84bdd7ba9cdb1e86b436ea3b27.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ddab9c84bdd7ba9cdb1e86b436ea3b27.png)'
- en: 'Figure 5: Aug-imodels prediction performance and interpretation for fMRI voxels.
    (A) Map of the difference between the performance of Aug-GAM and BERT for fMRI
    voxel prediction across the cortex. Positive values (red) show where Aug-GAM outperforms
    BERT (measured by correlation on the test set). (B) Aug-GAM outperforms BERT when
    averaging across all voxels (or just over the 1%/5% with the highest test correlations).
    Standard errors of the mean are all less than 0.0015. (C) Example Aug-GAM model
    for a single voxel (visualized with the top Aug-GAM coefficients). (D) Example
    Aug-Tree model for two voxels.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Aug-imodels对fMRI体素的预测性能和解释。 (A) 脑皮层中Aug-GAM和BERT在fMRI体素预测中的性能差异图。正值（红色）表示Aug-GAM在测试集上的相关性超过了BERT。
    (B) 在对所有体素（或仅对1%/5%具有最高测试相关性的体素）进行平均时，Aug-GAM优于BERT。均值的标准误差均小于0.0015。 (C) 单个体素的Aug-GAM模型示例（用顶部的Aug-GAM系数可视化）。
    (D) 两个体素的Aug-Tree模型示例。
- en: 6 Background and related work
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 背景与相关工作
- en: GAMs
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GAMs
- en: There is a large literature on additive models being used for interpretable
    modeling. This includes generalized additive models (GAMs) [[10](#bib.bib10)],
    which have achieved strong performance in various domains by modeling individual
    component functions/interactions using regularized boosted decision trees [[35](#bib.bib35)]
    and more recently using neural networks [[36](#bib.bib36)]. However, existing
    GAM methods are limited in their ability to model the high-order feature interactions
    that arise in NLP. Meanwhile, NLP has seen great success in models which build
    strong word-level representations, e.g. word2vec [[37](#bib.bib37), [38](#bib.bib38)],
    GloVe [[24](#bib.bib24)], FastText [[39](#bib.bib39)] and ELMo [[40](#bib.bib40)].
    By replacing such models with LLM embeddings, Aug-GAM enables easily modeling
    ngrams of different lengths without training a new model. Moreover, unlike earlier
    methods, LLMs can incorporate information about labels into the embeddings (e.g.
    by first finetuning an LLM on a downstream prediction task).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可解释建模中使用加性模型的文献非常丰富。这包括广义加性模型 (GAMs) [[10](#bib.bib10)]，这些模型通过使用正则化的提升决策树 [[35](#bib.bib35)]
    和最近使用神经网络 [[36](#bib.bib36)] 来建模单个组件函数/交互，已经在各种领域取得了优异的性能。然而，现有的 GAM 方法在建模 NLP
    中出现的高阶特征交互方面存在局限。同时，NLP 模型在构建强大的词级表示方面取得了巨大的成功，例如 word2vec [[37](#bib.bib37)、[38](#bib.bib38)]、GloVe [[24](#bib.bib24)]、FastText [[39](#bib.bib39)]
    和 ELMo [[40](#bib.bib40)]。通过用 LLM 嵌入替代这些模型，Aug-GAM 可以轻松地建模不同长度的 n-gram 而无需训练新模型。此外，与早期方法不同，LLM
    可以将标签信息纳入嵌入中（例如，通过先在下游预测任务上微调 LLM）。
- en: Decision trees
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 决策树
- en: 'There is a long history of greedy methods for fitting decision trees, e.g.,
    CART [[11](#bib.bib11)], ID3 [[26](#bib.bib26)], and C4.5 [[41](#bib.bib41)].
    More recent work has explored fitting trees via global optimization rather than
    greedy algorithms [[42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]; this
    can improve performance given a fixed tree size but incurs a high computational
    cost. Other recent studies have improved trees after fitting through regularization [[45](#bib.bib45)]
    or iterative updates [[46](#bib.bib46)]. Beyond trees, there are many popular
    classes of rule-based models, such as rule sets [[47](#bib.bib47)], rule lists [[48](#bib.bib48),
    [49](#bib.bib49)], and tree sums [[15](#bib.bib15)]. Aug-Tree addresses a common
    problem shared by rule-based approaches: modeling the sparse, correlated features
    that are common in tasks such as text classification.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合决策树的贪婪方法有着悠久的历史，例如 CART [[11](#bib.bib11)]、ID3 [[26](#bib.bib26)] 和 C4.5 [[41](#bib.bib41)]。最近的研究探索了通过全局优化而非贪婪算法来拟合树模型 [[42](#bib.bib42)、[43](#bib.bib43)、[44](#bib.bib44)]；这在固定树大小的情况下可以提高性能，但会带来较高的计算成本。其他近期研究通过正则化 [[45](#bib.bib45)]
    或迭代更新 [[46](#bib.bib46)] 来改进拟合后的树模型。除了树模型，还有许多流行的基于规则的模型，例如规则集 [[47](#bib.bib47)]、规则列表 [[48](#bib.bib48)、[49](#bib.bib49)]
    和树和 [[15](#bib.bib15)]。Aug-Tree 解决了基于规则的方法常见的一个问题：建模文本分类等任务中常见的稀疏且相关的特征。
- en: Beyond fitting a single tree, tree ensembles such as Random Forest [[27](#bib.bib27)],
    gradient-boosted trees [[50](#bib.bib50)], XGBoost [[51](#bib.bib51)], and BART [[52](#bib.bib52)],
    have all shown strong predictive performance in diverse settings. These ensembling
    approaches are compatible with Aug-Tree, as it can be used as the base estimator
    in any of these approaches.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拟合单棵树，树集成方法如随机森林 [[27](#bib.bib27)]、梯度提升树 [[50](#bib.bib50)]、XGBoost [[51](#bib.bib51)]
    和 BART [[52](#bib.bib52)] 在各种环境中都展示了强大的预测性能。这些集成方法与 Aug-Tree 兼容，因为 Aug-Tree 可以作为这些方法中的基础估计器使用。
- en: Interpreting/distilling neural networks
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解读/提炼神经网络
- en: The work here is related to studies that aim to make neural networks more interpretable.
    For example, models can make predictions by comparing inputs to prototypes [[53](#bib.bib53),
    [54](#bib.bib54)], by predicting intermediate interpretable concepts [[55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57)], using LLMs to extract prompt-based features [[58](#bib.bib58),
    [59](#bib.bib59)], distilling a neural network into a mostly transparent model [[60](#bib.bib60)]
    or distilling into a fully transparent model (e.g. adaptive wavelets [[13](#bib.bib13)]
    or an additive model [[61](#bib.bib61)]). Separately, many works use neural network
    distillation to build more efficient (but still black-box) neural network models,
    e.g. [[62](#bib.bib62), [63](#bib.bib63)].
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的工作与旨在使神经网络更具可解释性的研究相关。例如，模型可以通过将输入与原型进行比较来进行预测 [[53](#bib.bib53)]、[[54](#bib.bib54)]，通过预测中间可解释的概念 [[55](#bib.bib55)]、[[56](#bib.bib56)]、[[57](#bib.bib57)]，使用
    LLMs 提取基于提示的特征 [[58](#bib.bib58)]、[[59](#bib.bib59)]，将神经网络蒸馏为主要透明模型 [[60](#bib.bib60)]
    或完全透明模型（例如，自适应小波 [[13](#bib.bib13)] 或加性模型 [[61](#bib.bib61)]）。此外，许多工作使用神经网络蒸馏来构建更高效（但仍为黑箱）的神经网络模型，例如 [[62](#bib.bib62)]、[[63](#bib.bib63)]。
- en: Feature and feature-interaction importances
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征和特征交互的重要性
- en: Loosely related to this work are post-hoc methods that aim to help understand
    a black-box model, i.e. by providing feature importances using methods such as
    LIME [[29](#bib.bib29)], SHAP [[64](#bib.bib64)], and others [[65](#bib.bib65),
    [66](#bib.bib66)]. However, these methods lose some information by summarizing
    the model and suffer from issues with summarizing interactions [[67](#bib.bib67),
    [68](#bib.bib68)]. Slightly more related are works which aim to explain feature
    interactions or transformations in neural networks [[69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71)], but these works fail to explain the model as a whole and are
    again less reliable than having a fully transparent model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相关的是后处理方法，旨在帮助理解黑箱模型，即通过使用如 LIME [[29](#bib.bib29)]、SHAP [[64](#bib.bib64)]
    等方法提供特征重要性。然而，这些方法通过总结模型丢失了一些信息，并且在总结交互时存在问题 [[67](#bib.bib67)]、[[68](#bib.bib68)]。稍微相关的是旨在解释神经网络中的特征交互或转换的工作 [[69](#bib.bib69)]、[[70](#bib.bib70)]、[[71](#bib.bib71)]，但这些工作无法解释整个模型，且比起完全透明的模型来说仍然不够可靠。
- en: 7 Discussion
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: 'Aug-imodels provide a promising direction towards future methods that reap
    the benefits of both LLMs and transparent models in NLP: high accuracy along with
    interpretability/efficiency. This potentially opens the door for introducing LLM-augmented
    models in high-stakes domains, such as medical decision-making and in new applications
    on compute-limited hardware. Aug-imodels is currently limited to applications
    for which an effective LLM is available, and thus may not work well for very esoteric
    NLP tasks. However, as LLMs improve, the predictive performance of Aug-imodels
    should continue to improve and expand to more diverse NLP tasks. More generally,
    Aug-imodels can be applied to domains outside of NLP where effective foundation
    models are available (e.g. computer vision or protein engineering).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Aug-imodels 提供了一种有前景的方向，朝着结合 LLMs 和透明模型在 NLP 中的优势发展：高准确率和可解释性/效率。这有可能为引入 LLM
    增强模型于高风险领域（如医疗决策）以及在计算能力受限的硬件上开展新应用打开大门。Aug-imodels 目前仅限于有效的 LLM 可用的应用，因此可能不适用于非常冷门的
    NLP 任务。然而，随着 LLM 的改进，Aug-imodels 的预测性能应会继续提升，并扩展到更多多样化的 NLP 任务。更广泛地说，Aug-imodels
    还可以应用于 NLP 以外的领域，只要有效的基础模型可用（例如计算机视觉或蛋白质工程）。
- en: Aug-imodels can be readily extended to new model forms beyond additive models
    and trees. Other transparent models, such as rule lists, rule sets, and prototype-based
    models could all potentially benefit from LLM augmentation during training time.
    In all these cases, LLM augmentation could use LLM embeddings (as is done in Aug-GAM),
    use LLM generations (as is done in Aug-Tree), or use LLMs in new ways. Aug-GAM
    could be augmented by building on the nonlinearity present in GAMs such as the
    explainable boosting machine [[35](#bib.bib35)], to nonlinearly transform the
    embedding for each ngram with a model before summing to obtain the final prediction.
    Additionally, Aug-GAM could fit long-range interaction terms as opposed to only
    ngrams. Aug-Tree could leverage domain knowledge to engineer more meaningful prompts
    for expanding ngrams or for extracting relevant ngrams. Both models can be further
    studied to improve their compression (potentially with LLM-guided compression
    techniques) or to extend their capabilities to tasks beyond classification/regression,
    such as sequence prediction or outlier detection. We hope that the introduction
    of Aug-imodels can help push improved performance prediction into high-stakes
    applications, improve interpretability for scientific data, and reduce unnecessary
    energy/compute usage.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Aug-imodels 可以轻松扩展到除加法模型和树模型之外的新模型形式。其他透明模型，例如规则列表、规则集和基于原型的模型，在训练过程中都可能从 LLM
    增强中受益。在所有这些情况下，LLM 增强可以使用 LLM 嵌入（如在 Aug-GAM 中所做的），使用 LLM 生成（如在 Aug-Tree 中所做的），或以新的方式使用
    LLM。Aug-GAM 可以通过建立在 GAMs 中存在的非线性上，例如可解释增强机器 [[35](#bib.bib35)]，来对每个 ngram 的嵌入进行非线性变换，然后汇总以获得最终预测。此外，Aug-GAM
    可以拟合长程交互项，而不仅仅是 ngrams。Aug-Tree 可以利用领域知识来设计更有意义的提示，以扩展 ngrams 或提取相关的 ngrams。两个模型都可以进一步研究，以提高它们的压缩（可能通过
    LLM 指导的压缩技术）或将其能力扩展到分类/回归之外的任务，例如序列预测或异常检测。我们希望 Aug-imodels 的引入可以推动高风险应用中的性能预测改进，提高科学数据的可解释性，并减少不必要的能源/计算使用。
- en: References
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: \bibcommenthead
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \bibcommenthead
- en: 'Brown et al. [2020] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language
    models are few-shot learners. Advances in neural information processing systems
    33, 1877–1901 (2020)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brown 等人 [2020] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., 等: 语言模型是少量样本学习者。神经信息处理系统进展
    33, 1877–1901 (2020)'
- en: 'Bubeck et al. [2023] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
    Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., et al.: Sparks
    of artificial general intelligence: Early experiments with gpt-4. arXiv preprint
    arXiv:2303.12712 (2023)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bubeck 等人 [2023] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz,
    E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., 等: 人工通用智能的火花：与 gpt-4
    的早期实验。arXiv 预印本 arXiv:2303.12712 (2023)'
- en: 'Devlin et al. [2018] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805 (2018)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等人 [2018] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: Bert: 用于语言理解的深度双向变换器的预训练。arXiv
    预印本 arXiv:1810.04805 (2018)'
- en: 'Angermueller et al. [2016] Angermueller, C., Pärnamaa, T., Parts, L., Stegle,
    O.: Deep learning for computational biology. Molecular systems biology 12(7),
    878 (2016)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Angermueller 等人 [2016] Angermueller, C., Pärnamaa, T., Parts, L., Stegle, O.:
    计算生物学的深度学习。分子系统生物学 12(7), 878 (2016)'
- en: 'Kornblith et al. [2022] Kornblith, A.E., Singh, C., Devlin, G., Addo, N., Streck,
    C.J., Holmes, J.F., Kuppermann, N., Grupp-Phelan, J., Fineman, J., Butte, A.J.,
    Yu, B.: Predictability and stability testing to assess clinical decision instrument
    performance for children after blunt torso trauma. PLOS Digital Health (2022)
    [https://doi.org/10.1371/journal.pdig.0000076](https://doi.org/10.1371/journal.pdig.0000076)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kornblith 等人 [2022] Kornblith, A.E., Singh, C., Devlin, G., Addo, N., Streck,
    C.J., Holmes, J.F., Kuppermann, N., Grupp-Phelan, J., Fineman, J., Butte, A.J.,
    Yu, B.: 预测性和稳定性测试，以评估儿童在钝性躯干创伤后的临床决策工具性能。PLOS Digital Health (2022) [https://doi.org/10.1371/journal.pdig.0000076](https://doi.org/10.1371/journal.pdig.0000076)'
- en: 'Brennan and Oliver [2013] Brennan, T., Oliver, W.L.: The emergence of machine
    learning techniques in criminology. Criminology & Public Policy 12(3), 551–562
    (2013)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brennan 和 Oliver [2013] Brennan, T., Oliver, W.L.: 机器学习技术在犯罪学中的兴起。犯罪学与公共政策
    12(3), 551–562 (2013)'
- en: 'Dwork et al. [2012] Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel,
    R.: Fairness through awareness. In: Proceedings of the 3rd Innovations in Theoretical
    Computer Science Conference, pp. 214–226 (2012). ACM'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dwork et al. [2012] Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel,
    R.: 通过意识实现公平。在：第三届理论计算机科学创新会议论文集，pp. 214–226 (2012)。ACM'
- en: 'Goodman and Flaxman [2016] Goodman, B., Flaxman, S.: European union regulations
    on algorithmic decision-making and a” right to explanation”. arXiv preprint arXiv:1606.08813
    (2016)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goodman and Flaxman [2016] Goodman, B., Flaxman, S.: 欧盟关于算法决策和“解释权”的法规。arXiv
    预印本 arXiv:1606.08813 (2016)'
- en: 'Bommasani et al. [2023] Bommasani, R., Soylu, D., Liao, T.I., Creel, K.A.,
    Liang, P.: Ecosystem graphs: The social footprint of foundation models. arXiv
    preprint arXiv:2303.15772 (2023)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bommasani et al. [2023] Bommasani, R., Soylu, D., Liao, T.I., Creel, K.A.,
    Liang, P.: 生态系统图：基础模型的社会足迹。arXiv 预印本 arXiv:2303.15772 (2023)'
- en: 'Hastie and Tibshirani [1986] Hastie, T., Tibshirani, R.: Generalized additive
    models. Statistical Science 1(3), 297–318 (1986)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hastie and Tibshirani [1986] Hastie, T., Tibshirani, R.: 广义加性模型。统计科学 1(3),
    297–318 (1986)'
- en: 'Breiman et al. [1984] Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J.:
    Classification and Regression Trees. Wadsworth and Brooks, Monterey, CA (1984).
    [https://www.routledge.com/Classification-and-Regression-Trees/Breiman-Friedman-Stone-Olshen/p/book/9780412048418](https://www.routledge.com/Classification-and-Regression-Trees/Breiman-Friedman-Stone-Olshen/p/book/9780412048418)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breiman et al. [1984] Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J.:
    分类与回归树。Wadsworth 和 Brooks, Monterey, CA (1984)。 [https://www.routledge.com/Classification-and-Regression-Trees/Breiman-Friedman-Stone-Olshen/p/book/9780412048418](https://www.routledge.com/Classification-and-Regression-Trees/Breiman-Friedman-Stone-Olshen/p/book/9780412048418)'
- en: 'Rudin et al. [2021] Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L.,
    Zhong, C.: Interpretable machine learning: Fundamental principles and 10 grand
    challenges. arXiv preprint arXiv:2103.11251 (2021)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rudin et al. [2021] Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L.,
    Zhong, C.: 可解释的机器学习：基本原则与10大挑战。arXiv 预印本 arXiv:2103.11251 (2021)'
- en: 'Ha et al. [2021] Ha, W., Singh, C., Lanusse, F., Upadhyayula, S., Yu, B.: Adaptive
    wavelet distillation from neural networks through interpretations. Advances in
    Neural Information Processing Systems 34 (2021)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ha et al. [2021] Ha, W., Singh, C., Lanusse, F., Upadhyayula, S., Yu, B.: 通过解释从神经网络中自适应小波蒸馏。神经信息处理系统进展
    34 (2021)'
- en: 'Mignan and Broccardo [2019] Mignan, A., Broccardo, M.: One neuron versus deep
    learning in aftershock prediction. Nature 574(7776), 1–3 (2019)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mignan and Broccardo [2019] Mignan, A., Broccardo, M.: 单神经元与深度学习在余震预测中的对比。Nature
    574(7776), 1–3 (2019)'
- en: 'Tan et al. [2022] Tan, Y.S., Singh, C., Nasseri, K., Agarwal, A., Yu, B.: Fast
    interpretable greedy-tree sums (figs). arXiv:2201.11931 [cs, stat] (2022). arXiv:
    2201.11931'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan et al. [2022] Tan, Y.S., Singh, C., Nasseri, K., Agarwal, A., Yu, B.: 快速可解释的贪婪树总和（figs）。arXiv:2201.11931
    [cs, stat] (2022). arXiv: 2201.11931'
- en: 'Wang and Komatsuzaki [2021] Wang, B., Komatsuzaki, A.: GPT-J-6B: A 6 Billion
    Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
    (2021)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang and Komatsuzaki [2021] Wang, B., Komatsuzaki, A.: GPT-J-6B：一个60亿参数的自回归语言模型。
    [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
    (2021)'
- en: 'Honnibal and Montani [2017] Honnibal, M., Montani, I.: spaCy 2: Natural language
    understanding with Bloom embeddings, convolutional neural networks and incremental
    parsing. To appear (2017)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Honnibal and Montani [2017] Honnibal, M., Montani, I.: spaCy 2：使用 Bloom 嵌入、卷积神经网络和增量解析的自然语言理解。即将发表
    (2017)'
- en: 'Saravia et al. [2018] Saravia, E., Liu, H.-C.T., Huang, Y.-H., Wu, J., Chen,
    Y.-S.: Carer: Contextualized affect representations for emotion recognition. In:
    Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
    pp. 3687–3697 (2018)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saravia et al. [2018] Saravia, E., Liu, H.-C.T., Huang, Y.-H., Wu, J., Chen,
    Y.-S.: Carer：用于情感识别的上下文感知情感表示。在：2018年自然语言处理实证方法会议论文集，pp. 3687–3697 (2018)'
- en: 'Malo et al. [2014] Malo, P., Sinha, A., Korhonen, P., Wallenius, J., Takala,
    P.: Good debt or bad debt: Detecting semantic orientations in economic texts.
    Journal of the Association for Information Science and Technology 65 (2014)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Malo et al. [2014] Malo, P., Sinha, A., Korhonen, P., Wallenius, J., Takala,
    P.: 好债务还是坏债务：检测经济文本中的语义取向。信息科学与技术协会杂志 65 (2014)'
- en: 'Pang and Lee [2005] Pang, B., Lee, L.: Seeing stars: Exploiting class relationships
    for sentiment categorization with respect to rating scales. In: Proceedings of
    the ACL (2005)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pang and Lee [2005] Pang, B., Lee, L.: 观察明星：利用类别关系进行情感分类相对于评分尺度。在：ACL 会议论文集
    (2005)'
- en: 'Socher et al. [2013] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
    C.D., Ng, A., Potts, C.: Recursive deep models for semantic compositionality over
    a sentiment treebank. In: Proceedings of the 2013 Conference on Empirical Methods
    in Natural Language Processing, pp. 1631–1642 (2013)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Socher 等 [2013] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D.,
    Ng, A., Potts, C.: 用于情感树库的递归深度模型。载: 2013年自然语言处理实证方法会议论文集，第1631–1642页 (2013)'
- en: 'LeBel et al. [2022] LeBel, A., Wagner, L., Jain, S., Adhikari-Desai, A., Gupta,
    B., Morgenthal, A., Tang, J., Xu, L., Huth, A.G.: A natural language fmri dataset
    for voxelwise encoding models. bioRxiv (2022)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeBel 等 [2022] LeBel, A., Wagner, L., Jain, S., Adhikari-Desai, A., Gupta,
    B., Morgenthal, A., Tang, J., Xu, L., Huth, A.G.: 用于体素级编码模型的自然语言 fMRI 数据集。bioRxiv
    (2022)'
- en: 'Jones [1972] Jones, K.S.: A statistical interpretation of term specificity
    and its application in retrieval. Journal of documentation (1972)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jones [1972] Jones, K.S.: 术语特异性的统计解释及其在检索中的应用。文献学期刊 (1972)'
- en: 'Pennington et al. [2014] Pennington, J., Socher, R., Manning, C.D.: Glove:
    Global vectors for word representation. In: Proceedings of the 2014 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543 (2014)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pennington 等 [2014] Pennington, J., Socher, R., Manning, C.D.: Glove: 用于词汇表示的全局向量。载:
    2014年自然语言处理实证方法会议论文集 (EMNLP)，第1532–1543页 (2014)'
- en: 'Wolf et al. [2019] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al.: Huggingface’s
    transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771
    (2019)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等 [2019] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi,
    A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., 等: Huggingface 的 Transformers：最先进的自然语言处理。arXiv
    预印本 arXiv:1910.03771 (2019)'
- en: 'Quinlan [1986] Quinlan, J.R.: Induction of decision trees. Machine learning
    1(1), 81–106 (1986)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Quinlan [1986] Quinlan, J.R.: 决策树的归纳。机器学习 1(1), 81–106 (1986)'
- en: 'Breiman [2001] Breiman, L.: Random forests. Machine Learning 45(1), 5–32 (2001)
    [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Breiman [2001] Breiman, L.: 随机森林。机器学习 45(1), 5–32 (2001) [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)'
- en: 'Bach et al. [2022] Bach, S.H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C.,
    Nayak, N.V., Sharma, A., Kim, T., Bari, M.S., Fevry, T., et al.: Promptsource:
    An integrated development environment and repository for natural language prompts.
    arXiv preprint arXiv:2202.01279 (2022)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bach 等 [2022] Bach, S.H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C., Nayak,
    N.V., Sharma, A., Kim, T., Bari, M.S., Fevry, T., 等: Promptsource：一个集成开发环境和自然语言提示库。arXiv
    预印本 arXiv:2202.01279 (2022)'
- en: 'Ribeiro et al. [2016] Ribeiro, M.T., Singh, S., Guestrin, C.: Why should i
    trust you?: Explaining the predictions of any classifier. In: Proceedings of the
    22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
    pp. 1135–1144 (2016). ACM'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ribeiro 等 [2016] Ribeiro, M.T., Singh, S., Guestrin, C.: 我为什么应该信任你？：解释任何分类器的预测。载:
    第22届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集，第1135–1144页 (2016). ACM'
- en: 'Lundberg and Lee [2016] Lundberg, S., Lee, S.-I.: An unexpected unity among
    methods for interpreting model predictions. arXiv preprint arXiv:1611.07478 (2016)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lundberg 和 Lee [2016] Lundberg, S., Lee, S.-I.: 对模型预测解释方法的意外统一性。arXiv 预印本 arXiv:1611.07478
    (2016)'
- en: 'Huth et al. [2016] Huth, A.G., De Heer, W.A., Griffiths, T.L., Theunissen,
    F.E., Gallant, J.L.: Natural speech reveals the semantic maps that tile human
    cerebral cortex. Nature 532(7600), 453–458 (2016)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huth 等 [2016] Huth, A.G., De Heer, W.A., Griffiths, T.L., Theunissen, F.E.,
    Gallant, J.L.: 自然语言揭示了覆盖人类大脑皮层的语义地图。自然 532(7600), 453–458 (2016)'
- en: 'Schrimpf et al. [2021] Schrimpf, M., Blank, I.A., Tuckute, G., Kauf, C., Hosseini,
    E.A., Kanwisher, N., Tenenbaum, J.B., Fedorenko, E.: The neural architecture of
    language: Integrative modeling converges on predictive processing. Proceedings
    of the National Academy of Sciences 118(45), 2105646118 (2021)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schrimpf 等 [2021] Schrimpf, M., Blank, I.A., Tuckute, G., Kauf, C., Hosseini,
    E.A., Kanwisher, N., Tenenbaum, J.B., Fedorenko, E.: 语言的神经结构：整合建模收敛于预测处理。美国国家科学院院刊
    118(45), 2105646118 (2021)'
- en: 'Antonello and Huth [2022] Antonello, R.J., Huth, A.: Predictive coding or just
    feature discovery? an alternative account of why language models fit brain data.
    Neurobiology of Language, 1–39 (2022)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Antonello 和 Huth [2022] Antonello, R.J., Huth, A.: 预测编码还是只是特征发现？对语言模型适配大脑数据的另一种解释。神经语言学
    1–39 (2022)'
- en: 'Caucheteux and King [2022] Caucheteux, C., King, J.-R.: Brains and algorithms
    partially converge in natural language processing. Communications biology 5(1),
    1–10 (2022)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caucheteux 和 King [2022] Caucheteux, C., King, J.-R.: 大脑和算法在自然语言处理中的部分趋同。生物通讯
    5(1), 1–10 (2022)'
- en: 'Caruana et al. [2015] Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M.,
    Elhadad, N.: Intelligible models for healthcare: Predicting pneumonia risk and
    hospital 30-day readmission. In: Proceedings of the 21th ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining, pp. 1721–1730 (2015)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caruana 等 [2015] Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., Elhadad,
    N.: 面向医疗的可解释模型：预测肺炎风险和医院 30 天再入院率。发表于：第 21 届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集，第 1721–1730
    页 (2015)'
- en: 'Agarwal et al. [2021] Agarwal, R., Melnick, L., Frosst, N., Zhang, X., Lengerich,
    B., Caruana, R., Hinton, G.E.: Neural additive models: Interpretable machine learning
    with neural nets. Advances in Neural Information Processing Systems 34, 4699–4711
    (2021)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agarwal 等 [2021] Agarwal, R., Melnick, L., Frosst, N., Zhang, X., Lengerich,
    B., Caruana, R., Hinton, G.E.: 神经加性模型：具有可解释性的神经网络机器学习。神经信息处理系统进展 34, 4699–4711
    (2021)'
- en: 'Mikolov et al. [2013a] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient
    estimation of word representations in vector space. arXiv preprint arXiv:1301.3781
    (2013)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mikolov 等 [2013a] Mikolov, T., Chen, K., Corrado, G., Dean, J.: 在向量空间中高效估计词表示。arXiv
    预印本 arXiv:1301.3781 (2013)'
- en: 'Mikolov et al. [2013b] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S.,
    Dean, J.: Distributed representations of words and phrases and their compositionality.
    Advances in neural information processing systems 26 (2013)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mikolov 等 [2013b] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean,
    J.: 词语和短语的分布式表示及其组合性。神经信息处理系统进展 26 (2013)'
- en: 'Joulin et al. [2016] Joulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag
    of tricks for efficient text classification. arXiv preprint arXiv:1607.01759 (2016)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joulin 等 [2016] Joulin, A., Grave, E., Bojanowski, P., Mikolov, T.: 高效文本分类的技巧包。arXiv
    预印本 arXiv:1607.01759 (2016)'
- en: 'Peters et al. [2018] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark,
    C., Lee, K., Zettlemoyer, L.: Deep contextualized word representations. In: Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 2227–2237.
    Association for Computational Linguistics, New Orleans, Louisiana (2018). [https://doi.org/10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202)
    . [https://aclanthology.org/N18-1202](https://aclanthology.org/N18-1202)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peters 等 [2018] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,
    Lee, K., Zettlemoyer, L.: 深度上下文化的词表示。发表于：2018 年北美计算语言学协会：人类语言技术会议论文集，第 1 卷（长篇论文），第
    2227–2237 页。计算语言学协会，新奥尔良，路易斯安那州 (2018). [https://doi.org/10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202)
    . [https://aclanthology.org/N18-1202](https://aclanthology.org/N18-1202)'
- en: 'Quinlan [2014] Quinlan, J.R.: C4\. 5: Programs for Machine Learning. Elsevier,
    ??? (2014)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Quinlan [2014] Quinlan, J.R.: C4\. 5: 机器学习程序。Elsevier, ??? (2014)'
- en: 'Lin et al. [2020] Lin, J., Zhong, C., Hu, D., Rudin, C., Seltzer, M.: Generalized
    and scalable optimal sparse decision trees. In: International Conference on Machine
    Learning, pp. 6150–6160 (2020). PMLR'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 [2020] Lin, J., Zhong, C., Hu, D., Rudin, C., Seltzer, M.: 广义和可扩展的最优稀疏决策树。发表于：国际机器学习会议，第
    6150–6160 页 (2020). PMLR'
- en: 'Hu et al. [2019] Hu, X., Rudin, C., Seltzer, M.: Optimal sparse decision trees.
    Advances in Neural Information Processing Systems (NeurIPS) (2019)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 [2019] Hu, X., Rudin, C., Seltzer, M.: 最优稀疏决策树。神经信息处理系统进展 (NeurIPS) (2019)'
- en: 'Bertsimas and Dunn [2017] Bertsimas, D., Dunn, J.: Optimal classification trees.
    Machine Learning 106(7), 1039–1082 (2017)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bertsimas 和 Dunn [2017] Bertsimas, D., Dunn, J.: 最优分类树。机器学习 106(7), 1039–1082
    (2017)'
- en: 'Agarwal et al. [2022] Agarwal, A., Tan, Y.S., Ronen, O., Singh, C., Yu, B.:
    Hierarchical shrinkage: improving the accuracy and interpretability of tree-based
    methods. arXiv:2202.00858 [cs, stat] (2022). arXiv: 2202.00858'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agarwal 等 [2022] Agarwal, A., Tan, Y.S., Ronen, O., Singh, C., Yu, B.: 分层收缩：提高树基方法的准确性和可解释性。arXiv:2202.00858
    [cs, stat] (2022). arXiv: 2202.00858'
- en: 'Carreira-Perpinán and Tavallali [2018] Carreira-Perpinán, M.A., Tavallali,
    P.: Alternating optimization of decision trees, with application to learning sparse
    oblique trees. Advances in neural information processing systems 31 (2018)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carreira-Perpinán 和 Tavallali [2018] Carreira-Perpinán, M.A., Tavallali, P.:
    决策树的交替优化及其在学习稀疏倾斜树中的应用。神经信息处理系统进展 31 (2018)'
- en: 'Friedman and Popescu [2008] Friedman, J.H., Popescu, B.E.: Predictive learning
    via rule ensembles. The Annals of Applied Statistics 2(3), 916–954 (2008) [https://doi.org/10.1214/07-aoas148](https://doi.org/10.1214/07-aoas148)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Friedman 和 Popescu [2008] Friedman, J.H., Popescu, B.E.: 通过规则集进行预测学习。应用统计学年鉴
    2(3), 916–954 (2008) [https://doi.org/10.1214/07-aoas148](https://doi.org/10.1214/07-aoas148)'
- en: 'Angelino et al. [2017] Angelino, E., Larus-Stone, N., Alabi, D., Seltzer, M.,
    Rudin, C.: Learning certifiably optimal rule lists for categorical data. arXiv
    preprint arXiv:1704.01701 (2017)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Angelino 等人 [2017] Angelino, E., Larus-Stone, N., Alabi, D., Seltzer, M., Rudin,
    C.: 为分类数据学习可证明最优的规则列表。arXiv 预印本 arXiv:1704.01701 (2017)'
- en: 'Singh et al. [2021] Singh, C., Nasseri, K., Tan, Y.S., Tang, T., Yu, B.: imodels:
    a python package for fitting interpretable models. Journal of Open Source Software
    6(61), 3192 (2021) [https://doi.org/10.21105/joss.03192](https://doi.org/10.21105/joss.03192)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 等人 [2021] Singh, C., Nasseri, K., Tan, Y.S., Tang, T., Yu, B.: imodels:
    一个用于拟合可解释模型的 Python 包。开放源代码软件期刊 6(61), 3192 (2021) [https://doi.org/10.21105/joss.03192](https://doi.org/10.21105/joss.03192)'
- en: 'Freund et al. [1996] Freund, Y., Schapire, R.E., et al.: Experiments with a
    new boosting algorithm. In: Icml, vol. 96, pp. 148–156 (1996). Citeseer'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Freund 等人 [1996] Freund, Y., Schapire, R.E., 等.: 新的提升算法实验。在：ICML, vol. 96,
    pp. 148–156 (1996). Citeseer'
- en: 'Chen and Guestrin [2016] Chen, T., Guestrin, C.: Xgboost: A scalable tree boosting
    system. In: Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge
    Discovery and Data Mining, pp. 785–794 (2016)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 和 Guestrin [2016] Chen, T., Guestrin, C.: Xgboost: 一个可扩展的树提升系统。在：第22届ACM
    SIGKDD国际知识发现与数据挖掘大会论文集，pp. 785–794 (2016)'
- en: 'Chipman et al. [2010] Chipman, H.A., George, E.I., McCulloch, R.E.: Bart: Bayesian
    additive regression trees. The Annals of Applied Statistics 4(1), 266–298 (2010)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chipman 等人 [2010] Chipman, H.A., George, E.I., McCulloch, R.E.: Bart: 贝叶斯加法回归树。《应用统计年鉴》
    4(1), 266–298 (2010)'
- en: 'Li et al. [2018] Li, O., Liu, H., Chen, C., Rudin, C.: Deep learning for case-based
    reasoning through prototypes: A neural network that explains its predictions.
    In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32 (2018)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2018] Li, O., Liu, H., Chen, C., Rudin, C.: 通过原型的案例推理深度学习：一个解释其预测的神经网络。发表于《AAAI人工智能会议论文集》，第
    32 卷 (2018)'
- en: 'Chen et al. [2019] Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.:
    This looks like that: deep learning for interpretable image recognition. Advances
    in neural information processing systems 32 (2019)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2019] Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.:
    这看起来像那样：深度学习用于可解释的图像识别。《神经信息处理系统进展》 32 (2019)'
- en: 'Koh et al. [2020] Koh, P.W., Nguyen, T., Tang, Y.S., Mussmann, S., Pierson,
    E., Kim, B., Liang, P.: Concept bottleneck models. In: International Conference
    on Machine Learning, pp. 5338–5348 (2020). PMLR'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Koh 等人 [2020] Koh, P.W., Nguyen, T., Tang, Y.S., Mussmann, S., Pierson, E.,
    Kim, B., Liang, P.: 概念瓶颈模型。在：国际机器学习会议，pp. 5338–5348 (2020). PMLR'
- en: 'Yang et al. [2022] Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-Burch,
    C., Yatskar, M.: Language in a bottle: Language model guided concept bottlenecks
    for interpretable image classification. arXiv preprint arXiv:2211.11158 (2022)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 [2022] Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-Burch,
    C., Yatskar, M.: 语言瓶子：语言模型引导的概念瓶颈用于可解释的图像分类。arXiv 预印本 arXiv:2211.11158 (2022)'
- en: 'Ghosh et al. [2023] Ghosh, S., Yu, K., Arabshahi, F., Batmanghelich, K.: Route,
    interpret, repeat: Blurring the line between post hoc explainability and interpretable
    models. arXiv preprint arXiv:2302.10289 (2023)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ghosh 等人 [2023] Ghosh, S., Yu, K., Arabshahi, F., Batmanghelich, K.: 路径、解释、重复：模糊后验解释性与可解释模型之间的界限。arXiv
    预印本 arXiv:2302.10289 (2023)'
- en: 'Yuksekgonul et al. [2022] Yuksekgonul, M., Wang, M., Zou, J.: Post-hoc concept
    bottleneck models. arXiv preprint arXiv:2205.15480 (2022)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuksekgonul 等人 [2022] Yuksekgonul, M., Wang, M., Zou, J.: 后验概念瓶颈模型。arXiv 预印本
    arXiv:2205.15480 (2022)'
- en: 'McInerney et al. [2023] McInerney, D.J., Young, G., Meent, J.-W., Wallace,
    B.C.: Chill: Zero-shot custom interpretable feature extraction from clinical notes
    with large language models. arXiv preprint arXiv:2302.12343 (2023)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'McInerney 等人 [2023] McInerney, D.J., Young, G., Meent, J.-W., Wallace, B.C.:
    Chill: 零样本自定义可解释特征提取与大型语言模型。arXiv 预印本 arXiv:2302.12343 (2023)'
- en: 'Frosst and Hinton [2017] Frosst, N., Hinton, G.: Distilling a neural network
    into a soft decision tree. arXiv preprint arXiv:1711.09784 (2017)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frosst 和 Hinton [2017] Frosst, N., Hinton, G.: 将神经网络提炼成软决策树。arXiv 预印本 arXiv:1711.09784
    (2017)'
- en: 'Tan et al. [2018] Tan, S., Caruana, R., Hooker, G., Koch, P., Gordo, A.: Learning
    global additive explanations for neural nets using model distillation. arXiv preprint
    arXiv:1801.08640 (2018)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan 等人 [2018] Tan, S., Caruana, R., Hooker, G., Koch, P., Gordo, A.: 使用模型提炼学习神经网络的全局加法解释。arXiv
    预印本 arXiv:1801.08640 (2018)'
- en: 'Hinton et al. [2015] Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge
    in a neural network. arXiv preprint arXiv:1503.02531 (2015)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hinton 等人 [2015] Hinton, G., Vinyals, O., Dean, J.: 提取神经网络中的知识。arXiv 预印本 arXiv:1503.02531
    (2015)'
- en: 'Sanh et al. [2019] Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint
    arXiv:1910.01108 (2019)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sanh et al. [2019] Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert，BERT
    的精简版本：更小、更快、更便宜、更轻。arXiv 预印本 arXiv:1910.01108 (2019)'
- en: 'Lundberg et al. [2019] Lundberg, S.M., Erion, G., Chen, H., DeGrave, A., Prutkin,
    J.M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N., Lee, S.-I.: Explainable
    ai for trees: From local explanations to global understanding. arXiv preprint
    arXiv:1905.04610 (2019)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lundberg et al. [2019] Lundberg, S.M., Erion, G., Chen, H., DeGrave, A., Prutkin,
    J.M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N., Lee, S.-I.: 解释树的可解释 AI：从局部解释到全局理解。arXiv
    预印本 arXiv:1905.04610 (2019)'
- en: 'Friedman [2001] Friedman, J.H.: Greedy function approximation: a gradient boosting
    machine. Annals of statistics, 1189–1232 (2001)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Friedman [2001] Friedman, J.H.: 贪婪函数近似：一种梯度提升机器。《统计年鉴》，1189–1232 (2001)'
- en: 'Devlin et al. [2019] Devlin, S., Singh, C., Murdoch, W.J., Yu, B.: Disentangled
    attribution curves for interpreting random forests and boosted trees. arXiv preprint
    arXiv:1905.07631 (2019)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. [2019] Devlin, S., Singh, C., Murdoch, W.J., Yu, B.: 解耦归因曲线用于解释随机森林和提升树。arXiv
    预印本 arXiv:1905.07631 (2019)'
- en: 'Rudin [2018] Rudin, C.: Please stop explaining black box models for high stakes
    decisions. arXiv preprint arXiv:1811.10154 (2018)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rudin [2018] Rudin, C.: 请停止解释用于高风险决策的黑箱模型。arXiv 预印本 arXiv:1811.10154 (2018)'
- en: 'Murdoch et al. [2019] Murdoch, W.J., Singh, C., Kumbier, K., Abbasi-Asl, R.,
    Yu, B.: Definitions, methods, and applications in interpretable machine learning.
    Proceedings of the National Academy of Sciences of the United States of America
    116(44), 22071–22080 (2019) [https://doi.org/10.1073/pnas.1900654116](https://doi.org/10.1073/pnas.1900654116)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Murdoch et al. [2019] Murdoch, W.J., Singh, C., Kumbier, K., Abbasi-Asl, R.,
    Yu, B.: 可解释机器学习中的定义、方法和应用。《美国国家科学院院刊》 116(44), 22071–22080 (2019) [https://doi.org/10.1073/pnas.1900654116](https://doi.org/10.1073/pnas.1900654116)'
- en: 'Janizek et al. [2021] Janizek, J.D., Sturmfels, P., Lee, S.-I.: Explaining
    explanations: Axiomatic feature interactions for deep networks. J. Mach. Learn.
    Res. 22, 104–1 (2021)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Janizek et al. [2021] Janizek, J.D., Sturmfels, P., Lee, S.-I.: 解释解释：深度网络的公理特征交互。J.
    Mach. Learn. Res. 22, 104–1 (2021)'
- en: 'Singh et al. [2019] Singh, C., Murdoch, W.J., Yu, B.: Hierarchical interpretations
    for neural network predictions. International Conference on Learning Representations,
    26 (2019)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh et al. [2019] Singh, C., Murdoch, W.J., Yu, B.: 神经网络预测的层次解释。国际表示学习会议，26
    (2019)'
- en: 'Singh et al. [2020] Singh, C., Ha, W., Lanusse, F., Boehm, V., Liu, J., Yu,
    B.: Transformation Importance with Applications to Cosmology (2020)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh et al. [2020] Singh, C., Ha, W., Lanusse, F., Boehm, V., Liu, J., Yu,
    B.: 转换重要性及其在宇宙学中的应用 (2020)'
- en: 'Hazourli [2022] Hazourli, A.: Financialbert - a pretrained language model for
    financial text mining (2022) [https://doi.org/10.13140/RG.2.2.34032.12803](https://doi.org/10.13140/RG.2.2.34032.12803)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hazourli [2022] Hazourli, A.: Financialbert - 用于金融文本挖掘的预训练语言模型 (2022) [https://doi.org/10.13140/RG.2.2.34032.12803](https://doi.org/10.13140/RG.2.2.34032.12803)'
- en: 'Morris et al. [2020] Morris, J.X., Lifland, E., Yoo, J.Y., Grigsby, J., Jin,
    D., Qi, Y.: Textattack: A framework for adversarial attacks, data augmentation,
    and adversarial training in nlp. arXiv preprint arXiv:2005.05909 (2020)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Morris et al. [2020] Morris, J.X., Lifland, E., Yoo, J.Y., Grigsby, J., Jin,
    D., Qi, Y.: Textattack：一个用于对抗攻击、数据增强和对抗训练的框架。arXiv 预印本 arXiv:2005.05909 (2020)'
- en: 'Akl et al. [2021] Akl, H.A., Mariko, D., De Mazancourt, H.: Yseop at finsim-3
    shared task 2021: Specializing financial domain learning with phrase representations.
    arXiv preprint arXiv:2108.09485 (2021)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Akl et al. [2021] Akl, H.A., Mariko, D., De Mazancourt, H.: Yseop 在 finsim-3
    共享任务 2021：通过短语表示专注于金融领域学习。arXiv 预印本 arXiv:2108.09485 (2021)'
- en: 'Liu et al. [2019] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized
    bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2019] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta：一种稳健优化的 BERT 预训练方法。arXiv
    预印本 arXiv:1907.11692 (2019)'
- en: 'Pedregosa et al. [2011] Pedregosa, F., Varoquaux, G.ë.l., Gramfort, A., Michel,
    V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg,
    V., et al.: Scikit-learn: Machine learning in python. the Journal of machine Learning
    research 12(Oct), 2825–2830 (2011)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pedregosa et al. [2011] Pedregosa, F., Varoquaux, G.ë.l., Gramfort, A., Michel,
    V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg,
    V., 等：Scikit-learn：Python 中的机器学习。《机器学习研究杂志》 12(十月), 2825–2830 (2011)
- en: 'Su et al. [2022] Su, H., Kasai, J., Wang, Y., Hu, Y., Ostendorf, M., Yih, W.-t.,
    Smith, N.A., Zettlemoyer, L., Yu, T., et al.: One embedder, any task: Instruction-finetuned
    text embeddings. arXiv preprint arXiv:2212.09741 (2022)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等 [2022] Su, H., Kasai, J., Wang, Y., Hu, Y., Ostendorf, M., Yih, W.-t.,
    Smith, N.A., Zettlemoyer, L., Yu, T., 等：一个嵌入器，任何任务：指令微调文本嵌入。arXiv 预印本 arXiv:2212.09741
    (2022)
- en: 'Raffel et al. [2020] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., Liu, P.J., et al.: Exploring the limits of transfer
    learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21(140),
    1–67 (2020)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 [2020] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., Liu, P.J., 等：通过统一的文本到文本变换器探索迁移学习的极限。J. Mach. Learn. Res.
    21(140), 1–67 (2020)
- en: Appendix A Aug-GAM
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A Aug-GAM
- en: 'Table 4: Table of pre-trained models with unique huggingface identifiers. All
    models are used through huggingface [[25](#bib.bib25)], and linear/tree baselines
    are fit with scikit-learn [[76](#bib.bib76)] and imodels [[49](#bib.bib49)].'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：具有独特 huggingface 标识符的预训练模型表。所有模型都通过 huggingface [[25](#bib.bib25)] 使用，线性/树基线模型通过
    scikit-learn [[76](#bib.bib76)] 和 imodels [[49](#bib.bib49)] 进行拟合。
- en: '|  | BERT |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | BERT |'
- en: '| Base (no finetuning) | bert-base-uncased [[3](#bib.bib3)] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 基础（无微调） | bert-base-uncased [[3](#bib.bib3)] |'
- en: '| Emotion | nateraw/bert-base-uncased-emotion |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 情感 | nateraw/bert-base-uncased-emotion |'
- en: '| Financial phrasebank | ahmedrachid/FinancialBERT-Sentiment-Analysis [[72](#bib.bib72)]
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 财务短语库 | ahmedrachid/FinancialBERT-Sentiment-Analysis [[72](#bib.bib72)] |'
- en: '| Rotten tomatoes | textattack/bert-base-uncased-rotten_tomatoes [[73](#bib.bib73)]
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 烂番茄 | textattack/bert-base-uncased-rotten_tomatoes [[73](#bib.bib73)] |'
- en: '| SST2 | textattack/bert-base-uncased-SST-2 [[73](#bib.bib73)] |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SST2 | textattack/bert-base-uncased-SST-2 [[73](#bib.bib73)] |'
- en: '|  | DistilBERT |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | DistilBERT |'
- en: '| Base (no finetuning) | distilbert-base-uncased [[63](#bib.bib63)] |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 基础（无微调） | distilbert-base-uncased [[63](#bib.bib63)] |'
- en: '| Emotion | aatmasidha/distilbert-base-uncased-finetuned-emotion |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 情感 | aatmasidha/distilbert-base-uncased-finetuned-emotion |'
- en: '| Financial phrasebank | yseop/distilbert-base-financial-relation-extraction [[74](#bib.bib74)]
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 财务短语库 | yseop/distilbert-base-financial-relation-extraction [[74](#bib.bib74)]
    |'
- en: '| Rotten tomatoes | textattack/distilbert-base-uncased-rotten-tomatoes [[73](#bib.bib73)]
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 烂番茄 | textattack/distilbert-base-uncased-rotten-tomatoes [[73](#bib.bib73)]
    |'
- en: '| SST2 | distilbert-base-uncased-finetuned-sst-2-english |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| SST2 | distilbert-base-uncased-finetuned-sst-2-english |'
- en: '|  | RoBERTa [[75](#bib.bib75)] |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | RoBERTa [[75](#bib.bib75)] |'
- en: '| Emotion | bhadresh-savani/roberta-base-emotion |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 情感 | bhadresh-savani/roberta-base-emotion |'
- en: '| Financial phrasebank | abhilash1910/financial_roberta |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 财务短语库 | abhilash1910/financial_roberta |'
- en: '| Rotten tomatoes | textattack/roberta-base-rotten-tomatoes [[73](#bib.bib73)]
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 烂番茄 | textattack/roberta-base-rotten-tomatoes [[73](#bib.bib73)] |'
- en: '| SST2 | textattack/roberta-base-SST-2 [[73](#bib.bib73)] |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| SST2 | textattack/roberta-base-SST-2 [[73](#bib.bib73)] |'
- en: '![Refer to caption](img/f11f2d513466eab2168bd10ba9b7ddde.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f11f2d513466eab2168bd10ba9b7ddde.png)'
- en: 'Figure 6: Model performance decreases with increasing model uncertainty. Cumulative
    validation accuracy decreases as more uncertain samples (based on the model’s
    predicted probability) are added.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：模型性能随着模型不确定性的增加而下降。随着更多不确定样本（基于模型的预测概率）的加入，累积验证准确性下降。
- en: Varying Aug-GAM settings
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 变化的 Aug-GAM 设置
- en: By default ([Table 5](#A1.T5 "In Evaluating zero-shot accuracy with language
    models ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during
    Training")), we use the final embedding layer of the model (and average it over
    the sequence length to get a fixed size vector), but [Table 5](#A1.T5 "In Evaluating
    zero-shot accuracy with language models ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable
    Models with LLMs during Training") also shows results using the pooler output
    layer of the BERT model. The choice of layer (i.e. final embedding layer versus
    pooler output) does not seem to make a large difference in the final performance
    results. [Table 5](#A1.T5 "In Evaluating zero-shot accuracy with language models
    ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training")
    also shows one variation of the model (BERT finetuned (noun chunks)) where rather
    than training on all ngrams, the model is fit to only noun-phrases extracted by
    spaCy’s dependency parser [[17](#bib.bib17)]. This results in a performance drop
    across the datasets, suggesting that these noun-phrases alone are insufficient
    to perform the classification task. We also run an experiment where we extract
    embeddings using Instructor ([[77](#bib.bib77)], hkunlp/instructor-xl), which
    allows giving a contextual prompt for each dataset.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下（[表5](#A1.T5 "在评估语言模型的零样本准确性 ‣ 附录A Aug-GAM ‣ 在训练过程中增强可解释模型")），我们使用模型的最终嵌入层（并对序列长度进行平均以获得固定大小的向量），但[表5](#A1.T5
    "在评估语言模型的零样本准确性 ‣ 附录A Aug-GAM ‣ 在训练过程中增强可解释模型")也展示了使用BERT模型的池化输出层的结果。层的选择（即最终嵌入层与池化输出）似乎对最终性能结果影响不大。[表5](#A1.T5
    "在评估语言模型的零样本准确性 ‣ 附录A Aug-GAM ‣ 在训练过程中增强可解释模型")还展示了模型的一种变体（BERT 微调（名词短语）），该变体不是在所有n-gram上进行训练，而是拟合仅由spaCy的依赖解析器提取的名词短语[[17](#bib.bib17)]。这导致在数据集上的性能下降，表明这些名词短语本身不足以完成分类任务。我们还运行了一个实验，通过Instructor
    ([[77](#bib.bib77)], hkunlp/instructor-xl)提取嵌入，这允许为每个数据集提供上下文提示。
- en: Evaluating zero-shot accuracy with language models
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估语言模型的零样本准确性
- en: 'To measure generalization ability, we evaluate explanations based on accuracy
    as a prompt for other models. Accuracy is computed following [[1](#bib.bib1),
    [78](#bib.bib78)]: using exact matching with beam search, a beam width of 4, and
    a length penalty of $\alpha=0.6$. For sentiment evaluation, we use each prompt
    with the template Input: “${input}”{prompt}.⁸⁸8In initial experiments, we find
    that performance drops significantly when learning a prompt that comes before
    the input. We use positive and negative as positive and negative labels and require
    the LLM to rank the two options. Human-written prompts are adapted to this template
    from open-source prompts available through PromptSource [[28](#bib.bib28)].'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '为了测量泛化能力，我们根据准确性评估解释，作为其他模型的提示。准确性是按照[[1](#bib.bib1)、[78](#bib.bib78)]计算的：使用束搜索、束宽为4，以及长度惩罚$\alpha=0.6$。对于情感评估，我们使用每个提示与模板Input:
    “${input}”{prompt}。⁸⁸8在初步实验中，我们发现学习一个在输入之前的提示时，性能显著下降。我们将正面和负面标签分别作为正面和负面，并要求LLM对这两个选项进行排序。人工编写的提示从通过PromptSource
    [[28](#bib.bib28)] 提供的开源提示中适应到这个模板。'
- en: 'Table 5: Generalization accuracy varies depending on the model used to extract
    embeddings. Finetuning the embedding model improves Aug-GAM performance, using
    a BERT model seems to outperform a DistilBERT model, and the layer used to extract
    embeddings does not have too large an effect. Top two methods are bolded in each
    column.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：泛化准确性因提取嵌入所用的模型而异。微调嵌入模型提高了Aug-GAM性能，使用BERT模型似乎优于DistilBERT模型，并且提取嵌入所用的层没有太大影响。每列中前两种方法以粗体显示。
- en: '|  | Financial phrasebank | Rotten tomatoes | SST2 | Emotion |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | 财务短语库 | 腐烂番茄 | SST2 | 情感 |'
- en: '| BERT finetuned | 92.8  $\pm$0.37 | 81.6  $\pm$0.05 | 86.9 $\pm$0.1 | 89.5  $\pm$0.03
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| BERT 微调 | 92.8 $\pm$0.37 | 81.6 $\pm$0.05 | 86.9 $\pm$0.1 | 89.5 $\pm$0.03
    |'
- en: '| BERT finetuned (pooler output) | 93.5  $\pm$0.05 | 81.3 $\pm$0.13 | 87.8  $\pm$0.21
    | 89.8  $\pm$0.07 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| BERT 微调（池化输出） | 93.5 $\pm$0.05 | 81.3 $\pm$0.13 | 87.8 $\pm$0.21 | 89.8 $\pm$0.07
    |'
- en: '| BERT finetuned (noun chunks) | 87.9 $\pm$0.08 | 79.7 $\pm$0.45 | 84.1 $\pm$0.14
    | 87.1 $\pm$0.2 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| BERT 微调（名词短语） | 87.9 $\pm$0.08 | 79.7 $\pm$0.45 | 84.1 $\pm$0.14 | 87.1 $\pm$0.2
    |'
- en: '| BERT | 84.1 $\pm$0.08 | 78.1 $\pm$0.16 | 82.8 $\pm$0.27 | 67.1 $\pm$0.06
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 84.1 $\pm$0.08 | 78.1 $\pm$0.16 | 82.8 $\pm$0.27 | 67.1 $\pm$0.06
    |'
- en: '| BERT (pooler output) | 82.7 $\pm$0.28 | 78.5 $\pm$0.03 | 80.7 $\pm$0.11 |
    58.0 $\pm$0.29 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| BERT（池化输出） | 82.7 $\pm$0.28 | 78.5 $\pm$0.03 | 80.7 $\pm$0.11 | 58.0 $\pm$0.29
    |'
- en: '| DistilBERT finetuned | 85.8 $\pm$0.34 | 78.5 $\pm$0.34 | 81.7 $\pm$0.07 |
    68.8 $\pm$0.11 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| DistilBERT 微调 | 85.8 $\pm$0.34 | 78.5 $\pm$0.34 | 81.7 $\pm$0.07 | 68.8 $\pm$0.11
    |'
- en: '| DistilBERT | 81.7 $\pm$0.34 | 79.8 $\pm$0.08 | 86.8 $\pm$0.1 | 87.5 $\pm$0.11
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| DistilBERT | 81.7 $\pm$0.34 | 79.8 $\pm$0.08 | 86.8 $\pm$0.1 | 87.5 $\pm$0.11
    |'
- en: '| RoBERTa finetuned | 77.8 $\pm$0.31 | 83.6 $\pm$0.03 | 89.1  $\pm$0.24 | 88.5
    $\pm$0.19 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa 微调 | 77.8 $\pm$0.31 | 83.6 $\pm$0.03 | 89.1 $\pm$0.24 | 88.5 $\pm$0.19
    |'
- en: '| Instructor prompted | 76.1 | 80.0 | 84.7 | 71.0 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 指导员提示 | 76.1 | 80.0 | 84.7 | 71.0 |'
- en: A.1 Test-time tradeoffs between accuracy and interpretability/speed
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 测试时在准确性与可解释性/速度之间的权衡
- en: The ability to effectively generalize to unseen tokens in [Fig 4](#S4.F4 "In
    Fitted Aug-GAM coefficients match human scores ‣ 4 Interpreting fitted models
    ‣ Augmenting Interpretable Models with LLMs during Training")C/D raises the question
    of whether one can vary the order of ngrams used at test-time, to get a tradeoff
    between accuracy and interpretability (i.e. how many features are used to make
    a prediction). Depending on the relative importance of accuracy and interpretability
    for a given problem, one may select to use a different number of features for
    testing. [Fig 7](#A1.F7 "In A.1 Test-time tradeoffs between accuracy and interpretability/speed
    ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training")
    suggests that this is feasible.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4](#S4.F4 "在拟合的 Aug-GAM 系数与人类评分匹配 ‣ 4 解释拟合模型 ‣ 在训练过程中用 LLM 增强可解释模型")C/D中，能够有效地对未见过的词汇进行泛化，这引出了一个问题，即是否可以在测试时改变使用的n-grams的顺序，以在准确性和可解释性（即用于预测的特征数量）之间取得权衡。根据给定问题中准确性和可解释性的相对重要性，可能会选择在测试时使用不同数量的特征。[图
    7](#A1.F7 "在 A.1 测试时在准确性与可解释性/速度之间的权衡 ‣ 附录 A Aug-GAM ‣ 在训练过程中用 LLM 增强可解释模型")表明这是可行的。
- en: '[Fig 7](#A1.F7 "In A.1 Test-time tradeoffs between accuracy and interpretability/speed
    ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training")A
    shows the prediction performance when compressing the Aug-GAM model (fit using
    4-grams and finetuned BERT) by setting the coefficients with the smallest magnitudee
    to zero. Some models require only a few coefficients to perform well and some
    models (e.g. the Emotion and Financial phrasebank models) predict more accurately
    when using less than 50% of the original coefficients. [Fig 7](#A1.F7 "In A.1
    Test-time tradeoffs between accuracy and interpretability/speed ‣ Appendix A Aug-GAM
    ‣ Augmenting Interpretable Models with LLMs during Training")B it shows the accuracy
    of the same models in [Fig 7](#A1.F7 "In A.1 Test-time tradeoffs between accuracy
    and interpretability/speed ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable Models
    with LLMs during Training")A, as the order of ngrams used only for testing is
    varied. As the number of features used for testing increases, the performance
    tends to increase but interpretations become more difficult.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7](#A1.F7 "在 A.1 测试时在准确性与可解释性/速度之间的权衡 ‣ 附录 A Aug-GAM ‣ 在训练过程中用 LLM 增强可解释模型")A
    显示了当通过将系数的最小幅度设为零来压缩 Aug-GAM 模型（使用 4-grams 进行拟合和微调 BERT）时的预测性能。一些模型只需要少量系数就能表现良好，而一些模型（例如情感和金融短语库模型）在使用少于原始系数的
    50% 时预测更准确。[图 7](#A1.F7 "在 A.1 测试时在准确性与可解释性/速度之间的权衡 ‣ 附录 A Aug-GAM ‣ 在训练过程中用 LLM
    增强可解释模型")B 显示了[图 7](#A1.F7 "在 A.1 测试时在准确性与可解释性/速度之间的权衡 ‣ 附录 A Aug-GAM ‣ 在训练过程中用
    LLM 增强可解释模型")A 中相同模型的准确性，随着仅用于测试的 n-grams 顺序的变化。当用于测试的特征数量增加时，性能趋于提高，但解释性变得更加困难。'
- en: '[Fig 8](#A1.F8 "In A.1 Test-time tradeoffs between accuracy and interpretability/speed
    ‣ Appendix A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training")
    characterizes the full tradeoff between the number of ngrams used for fitting
    versus testing for all datasets. Generally, the best performance is achieved when
    the same number of ngrams is used for training and testing (the diagonal). Performance
    tends to degrade significantly when fewer ngrams are used for testing than training
    (lower-left).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8](#A1.F8 "在 A.1 测试时在准确性与可解释性/速度之间的权衡 ‣ 附录 A Aug-GAM ‣ 在训练过程中用 LLM 增强可解释模型")
    描述了所有数据集中用于拟合与测试的 n-grams 数量之间的全面权衡。通常，当训练和测试使用相同数量的 n-grams 时，能够达到最佳性能（对角线）。当测试时使用的
    n-grams 少于训练时（左下角），性能往往显著下降。'
- en: \begin{overpic}[width=195.12767pt,tics=10]{figs/vary_sparsity.pdf} \put(1.0,55.0){\large{A}}
    \end{overpic}\begin{overpic}[width=195.12767pt,tics=10]{figs/vary_ngrams_test/vary_ngrams_single.pdf}
    \put(1.0,55.0){\large{B}} \end{overpic}
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{overpic}[width=195.12767pt,tics=10]{figs/vary_sparsity.pdf} \put(1.0,55.0){\large{A}}
    \end{overpic}\begin{overpic}[width=195.12767pt,tics=10]{figs/vary_ngrams_test/vary_ngrams_single.pdf}
    \put(1.0,55.0){\large{B}} \end{overpic}
- en: 'Figure 7: Aug-GAM performance when varying the ngrams used for testing. (A)
    Performance when removing the smallest coefficients from an Aug-GAM model. (B)
    Performance when varying the order of ngrams used for testing.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：当测试时使用的 n-gram 发生变化时，Aug-GAM 的性能。 (A) 从 Aug-GAM 模型中移除最小系数后的性能。 (B) 当测试时
    n-gram 的顺序发生变化时的性能。
- en: '![Refer to caption](img/34b4d477084ef52e2e70cbd50a723a42.png)![Refer to caption](img/7aa9f752cdf38c01470e07e65116db19.png)![Refer
    to caption](img/9136685c396a70ac317f656ef2df94f8.png)![Refer to caption](img/7b69eb60aa8ff66623ba8c8c19567303.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/34b4d477084ef52e2e70cbd50a723a42.png)![参考说明](img/7aa9f752cdf38c01470e07e65116db19.png)![参考说明](img/9136685c396a70ac317f656ef2df94f8.png)![参考说明](img/7b69eb60aa8ff66623ba8c8c19567303.png)'
- en: 'Figure 8: Varying the order of ngrams used for training and testing across
    each of the five datasets in [Table 1](#S3.T1 "In Datasets ‣ 3.1 Experimental
    setup ‣ 3 Results: Prediction performance ‣ Augmenting Interpretable Models with
    LLMs during Training"). Some models (i.e. rows) perform reasonably well as the
    order of ngrams used for testing is varied, potentially enabling a test-time tradeoff
    between accuracy and interpretability. Generally, using higher-order ngrams during
    testing improves performance and testing with less ngrams than used for training
    hurts performance considerably.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8：在 [表 1](#S3.T1 "In Datasets ‣ 3.1 Experimental setup ‣ 3 Results: Prediction
    performance ‣ Augmenting Interpretable Models with LLMs during Training") 中的五个数据集上变化训练和测试时使用的
    n-gram 顺序。一些模型（即行）在测试时 n-gram 的顺序发生变化时表现相当不错，可能在准确性和可解释性之间实现测试时的权衡。一般而言，在测试时使用高阶
    n-gram 可以提高性能，而测试时使用少于训练时的 n-gram 则会显著降低性能。'
- en: A.2 Comparison with post-hoc feature importance
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 与事后特征重要性的比较
- en: The coefficients learned by Aug-GAM often differ from importances assigned by
    post-hoc feature-importance methods. Aug-GAM learns a single coefficient for each
    ngram across the dataset, allowing for auditing/editing the model with visualizations
    such as [Fig 4](#S4.F4 "In Fitted Aug-GAM coefficients match human scores ‣ 4
    Interpreting fitted models ‣ Augmenting Interpretable Models with LLMs during
    Training"). In contrast, popular methods for post-hoc feature importance, such
    as LIME [[29](#bib.bib29)] and SHAP [[30](#bib.bib30)] yield importance scores
    that vary based on the context in each input. This can be useful for debugging
    complex nonlinear models, but these scores (i) are approximations, (ii) must summarize
    nonlinear feature interactions, and (iii) vary across predictions, making transparent
    models preferable whenever possible.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Aug-GAM 学到的系数通常与事后特征重要性方法分配的权重不同。Aug-GAM 为数据集中每个 n-gram 学习一个系数，允许通过 [图 4](#S4.F4
    "In Fitted Aug-GAM coefficients match human scores ‣ 4 Interpreting fitted models
    ‣ Augmenting Interpretable Models with LLMs during Training") 这样的可视化对模型进行审计/编辑。相比之下，流行的事后特征重要性方法，如
    LIME [[29](#bib.bib29)] 和 SHAP [[30](#bib.bib30)]，产生的权重基于每个输入的上下文变化。这对于调试复杂的非线性模型是有用的，但这些权重
    (i) 是近似值，(ii) 必须总结非线性特征交互，且 (iii) 在预测中变化，因此尽可能使用透明模型更为优选。
- en: '[Fig 9](#A1.F9 "In A.2 Comparison with post-hoc feature importance ‣ Appendix
    A Aug-GAM ‣ Augmenting Interpretable Models with LLMs during Training") shows
    an example of the Aug-GAM coefficients for the SST2 model from [Fig 4](#S4.F4
    "In Fitted Aug-GAM coefficients match human scores ‣ 4 Interpreting fitted models
    ‣ Augmenting Interpretable Models with LLMs during Training") for different ngrams
    when making a prediction for the phrase not very good. While Aug-GAM yields scores
    for each subphrase that match human judgement (as seen in [Fig 4](#S4.F4 "In Fitted
    Aug-GAM coefficients match human scores ‣ 4 Interpreting fitted models ‣ Augmenting
    Interpretable Models with LLMs during Training")B/D), posthoc feature importance
    methods summarize the interactions between different ngrams into individual words,
    potentially making interpretation difficult. Scores are rescaled to be between
    -1 and 1 to make them comparable. See Aug-GAM scores for many top-interacting
    phrases in [Fig 10](#A1.F10 "In Summing embeddings meaningfully captures interactions
    ‣ A.2 Comparison with post-hoc feature importance ‣ Appendix A Aug-GAM ‣ Augmenting
    Interpretable Models with LLMs during Training").'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9](#A1.F9 "在 A.2 与后处理特征重要性比较 ‣ 附录 A Aug-GAM ‣ 在训练过程中使用 LLMs 扩展可解释模型") 显示了不同
    n-gram 的 Aug-GAM 系数示例，来自 [图 4](#S4.F4 "在拟合的 Aug-GAM 系数匹配人类评分 ‣ 4 解读拟合模型 ‣ 在训练过程中使用
    LLMs 扩展可解释模型")，用于预测短语 not very good。虽然 Aug-GAM 为每个子短语提供的评分与人类判断一致（如 [图 4](#S4.F4
    "在拟合的 Aug-GAM 系数匹配人类评分 ‣ 4 解读拟合模型 ‣ 在训练过程中使用 LLMs 扩展可解释模型")B/D 所见），但后处理特征重要性方法将不同
    n-gram 之间的交互作用总结为单个词，可能使解释变得困难。为了便于比较，评分被重新缩放到 -1 和 1 之间。请参见 [图 10](#A1.F10 "在汇总嵌入有意义地捕捉交互作用
    ‣ A.2 与后处理特征重要性比较 ‣ 附录 A Aug-GAM ‣ 在训练过程中使用 LLMs 扩展可解释模型") 中 Aug-GAM 对许多高交互短语的评分。'
- en: '![Refer to caption](img/b3ab472b0f67a30728857702adbbf4ce.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b3ab472b0f67a30728857702adbbf4ce.png)'
- en: 'Figure 9: Comparing Aug-GAM ngram coefficients (left) to word-level feature
    importances from posthoc methods (right): LIME and SHAP.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：比较 Aug-GAM ngram 系数（左）与后处理方法的单词级特征重要性（右）：LIME 和 SHAP。
- en: Summing embeddings meaningfully captures interactions
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 汇总嵌入有意义地捕捉交互作用
- en: One potential concern with the Aug-GAM model is that it may fail to learn interactions
    since it simply sums the embeddings of individual ngrams, and the language model
    extractor may not sufficiently capture interactions in its embedding space. To
    investigate this concern, we first identify bigrams that involve interaction by
    fitting a unigram bag-of-words model and a bigram bag-of-ngrams model to SST2.
    We then use these two models to select the 10 bigrams for which the bigram coefficient
    is farthest from the sum of the coefficients for each unigram.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Aug-GAM 模型的一个潜在问题是它可能无法学习交互作用，因为它仅仅汇总了各个 n-gram 的嵌入，而语言模型提取器可能未能充分捕捉其嵌入空间中的交互作用。为了解决这个问题，我们首先通过拟合一个
    unigram 词袋模型和一个 bigram 词袋模型来识别涉及交互的 bigram。然后，我们使用这两个模型选择那些 bigram 系数最远离每个 unigram
    系数之和的 10 个 bigram。
- en: '[Fig 10](#A1.F10 "In Summing embeddings meaningfully captures interactions
    ‣ A.2 Comparison with post-hoc feature importance ‣ Appendix A Aug-GAM ‣ Augmenting
    Interpretable Models with LLMs during Training") shows the resulting bigrams containing
    interactions. For each bigram, it shows the Aug-GAM learned coefficient (i.e.
    the contribution to the prediction $w^{T}\phi(x_{i})$) for the bigram (gray bar)
    along with each of its constituent unigrams (blue and orange bars). It is clear
    that the bigram coefficient is not the simple naive sum of the unigram coefficients
    (dashed black bar), and the learned coefficients make intuitive sense, suggesting
    that this Aug-GAM model has successfully learned interactions.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10](#A1.F10 "在汇总嵌入有意义地捕捉交互作用 ‣ A.2 与后处理特征重要性比较 ‣ 附录 A Aug-GAM ‣ 在训练过程中使用
    LLMs 扩展可解释模型") 显示了包含交互作用的结果 bigram。对于每个 bigram，图中展示了 Aug-GAM 学到的系数（即对预测 $w^{T}\phi(x_{i})$
    的贡献）（灰色条）以及其各个组成 unigram（蓝色和橙色条）。很明显，bigram 系数不是 unigram 系数的简单天真和（虚线黑条），而且学到的系数具有直观意义，这表明这个
    Aug-GAM 模型成功地学习了交互作用。'
- en: '![Refer to caption](img/644213485347772ae98d580429a7918e.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/644213485347772ae98d580429a7918e.png)'
- en: 'Figure 10: Aug-GAM accurately learns interactions rather than simply summing
    the contributions of individual unigrams.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：Aug-GAM 准确地学习了交互作用，而不仅仅是简单地汇总各个 unigram 的贡献。
- en: Appendix B Aug-Tree
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B Aug-Tree
- en: Algorithm B2 Aug-Tree algorithm for fitting a single split.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 B2 Aug-Tree 算法用于拟合单一分裂。
- en: '1:Split-Aug-Tree(X, y, LLM):2:  # Add original CART keyphrase3:keyphrase =
    split_CART(X, y)4:keyphrases_expanded = LLM(“Generate similar keyphrases to ”
    + keyphrase)5:keyphrases_running = [keyphrase]6:impurity_decrease_best = calc_impurity_decrease(X,
    y, keyphrases_running)7:  # Try adding new keyphrases8:for  k in keyphrases_expanded:9:    keyphrases_running.push(k)10:    impurity_decrease_new
    = calc_impurity_decrease(X, y, keyphrases_running)11:    if impurity_decrease_new  $<$  impurity_decrease_best then12:       keyphrases_running.pop()
        13:  return  keyphrases_running'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '1:Split-Aug-Tree(X, y, LLM):2:  # 添加原始CART关键词3:关键词 = split_CART(X, y)4:关键词扩展
    = LLM("生成类似的关键词" + 关键词)5:关键词进行中 = [关键词]6:纯度减少最佳 = calc_impurity_decrease(X, y,
    关键词进行中)7:  # 尝试添加新关键词8:对于  k 在 关键词扩展中:9:    关键词进行中.push(k)10:    纯度减少新 = calc_impurity_decrease(X,
    y, 关键词进行中)11:    如果 纯度减少新  $<$  纯度减少最佳 则12:       关键词进行中.pop()     13:  返回  关键词进行中'
- en: 'Table 6: Metadata on keyphrase expansions. Results are averaged over keyphrases
    found in the 4 text-classification datasets in [Table 1](#S3.T1 "In Datasets ‣
    3.1 Experimental setup ‣ 3 Results: Prediction performance ‣ Augmenting Interpretable
    Models with LLMs during Training") when fitting a 40-tree bagging ensemble. The
    LLM is queried for 100 expansion candidates, but due to imperfect LLM generations,
    only 91.6 candidates are generated on average. After deduplication (converting
    to lowercase, removing whitespaces, etc.), only 83.3 candidates remain. Screening
    removes almost all candidates, leaving only 0.8 candidates on average.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：关键词扩展的元数据。结果是对[表1](#S3.T1 "在数据集 ‣ 3.1 实验设置 ‣ 3 结果：预测性能 ‣ 在训练过程中使用LLMs增强可解释模型")中4个文本分类数据集找到的关键词的平均值。当拟合40棵树的包外集时，LLM被查询100个扩展候选，但由于LLM生成不完美，平均仅生成91.6个候选。去重后（转换为小写，去除空格等），只剩下83.3个候选。筛选几乎去除了所有候选，平均只剩下0.8个候选。
- en: '| # Expansion candidates (Before deduplication) | # Expansion candidates |
    # Expansions (After screening) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| # 扩展候选（去重前） | # 扩展候选 | # 扩展（筛选后） |'
- en: '| 91.6$\pm$0.7 | 83.3$\pm$0.8 | 0.8$\pm$0.1 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 91.6$\pm$0.7 | 83.3$\pm$0.8 | 0.8$\pm$0.1 |'
- en: B.1 Aug-Tree variations
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 Aug-Tree变体
- en: '[Table 7](#A2.T7 "In B.1 Aug-Tree variations ‣ Appendix B Aug-Tree ‣ Augmenting
    Interpretable Models with LLMs during Training") explores different variations
    of Aug-Tree. The top row shows learning a single tree with Aug-Tree using its
    default parameters, achieving the best performance across the datasets. [Table 7](#A2.T7
    "In B.1 Aug-Tree variations ‣ Appendix B Aug-Tree ‣ Augmenting Interpretable Models
    with LLMs during Training") shows results for different algorithmic choices, such
    as replacing the generic prompt with a dataset-specific one (Aug-Tree (Contextual
    prompt)), and searching for new keyphrases using 5 CART features instead of one
    (Aug-Tree (5 CART features)). We also consider preprocessing the data differently,
    using Stemming (with the Porter Stemmer) or using Trigrams, rather than bigrams.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7](#A2.T7 "在B.1 Aug-Tree变体 ‣ 附录B Aug-Tree ‣ 在训练过程中使用LLMs增强可解释模型") 探讨了Aug-Tree的不同变体。顶部行显示了使用默认参数学习单棵树的Aug-Tree，在数据集上取得了最佳表现。[表7](#A2.T7
    "在B.1 Aug-Tree变体 ‣ 附录B Aug-Tree ‣ 在训练过程中使用LLMs增强可解释模型") 显示了不同算法选择的结果，例如将通用提示替换为数据集特定的提示（Aug-Tree（上下文提示）），以及使用5个CART特征而不是一个（Aug-Tree（5
    CART特征））来寻找新关键词。我们还考虑了不同的数据预处理方法，使用词干提取（使用Porter词干提取器）或使用三元组，而不是二元组。'
- en: One major variation we study is using LLM embeddings to find keyphrases, rather
    than querying via a prompt (Aug-Tree (Embeddings). Specifically, we consider expanding
    keywords by finding the keyphrases that are closest in embedding space (measured
    by euclidean distance) to the original keyphrase. This option may be desirable
    computationally, as it may require a smaller LLM to compute effective embeddings
    (e.g. BERT [[3](#bib.bib3)]) compared to a larger LLM required to directly generate
    relevant keyphrases (e.g. GPT3 [[1](#bib.bib1)]). However, finding closest embeddings
    requires making more calls to the LLM, as embeddings must be calculated and compared
    across all ngrams in $X_{\text{text}}$.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的一个主要变体是使用LLM嵌入来寻找关键词，而不是通过提示查询（Aug-Tree（嵌入））。具体来说，我们考虑通过查找在嵌入空间中最接近原始关键词的关键词来扩展关键词（通过欧氏距离度量）。这一选项在计算上可能更可取，因为它可能只需要较小的LLM来计算有效的嵌入（例如BERT
    [[3](#bib.bib3)]），而不需要直接生成相关关键词的较大LLM（例如GPT3 [[1](#bib.bib1)]）。然而，查找最接近的嵌入需要多次调用LLM，因为必须计算并比较$X_{\text{text}}$中所有的ngram。
- en: 'Table 7: Performance (ROC AUC) for variations of Aug-Tree. Values are averaged
    over 3 random dataset splits; error bars are standard error of the mean (many
    are within the points).'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：Aug-Tree 的不同变体的性能（ROC AUC）。数值取自 3 个随机数据集划分的平均值；误差条为均值的标准误差（许多误差条位于点内）。
- en: '|  | Emotion | Financial phrasebank | Rotten tomatoes | SST2 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | 情感 | 财务短语库 | 腐烂番茄 | SST2 |'
- en: '| Aug-Tree | 0.680 $\pm$0.029 | 0.825 $\pm$0.006 | 0.622 $\pm$0.007 | 0.673
    $\pm$0.008 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Aug-Tree | 0.680 $\pm$0.029 | 0.825 $\pm$0.006 | 0.622 $\pm$0.007 | 0.673
    $\pm$0.008 |'
- en: '| Aug-Tree (Embeddings) | 0.599 $\pm$0.008 | 0.776 $\pm$0.018 | 0.600 $\pm$0.011
    | 0.663 $\pm$0.002 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Aug-Tree (嵌入) | 0.599 $\pm$0.008 | 0.776 $\pm$0.018 | 0.600 $\pm$0.011 |
    0.663 $\pm$0.002 |'
- en: '| Aug-Tree (Contextual prompt) | 0.667 $\pm$0.011 | 0.820 $\pm$0.004 | 0.627
    $\pm$0.008 | 0.669 $\pm$0.005 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Aug-Tree (上下文提示) | 0.667 $\pm$0.011 | 0.820 $\pm$0.004 | 0.627 $\pm$0.008
    | 0.669 $\pm$0.005 |'
- en: '| Aug-Tree (5 CART features) | 0.711 $\pm$0.039 | 0.730 $\pm$0.026 | 0.608
    $\pm$0.009 | 0.674 $\pm$0.003 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Aug-Tree (5 CART 特征) | 0.711 $\pm$0.039 | 0.730 $\pm$0.026 | 0.608 $\pm$0.009
    | 0.674 $\pm$0.003 |'
- en: '| Aug-Tree (Stemming) | 0.640 $\pm$0.019 | 0.520 $\pm$0.016 | 0.625 $\pm$0.004
    | 0.679 $\pm$0.005 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Aug-Tree (词干提取) | 0.640 $\pm$0.019 | 0.520 $\pm$0.016 | 0.625 $\pm$0.004
    | 0.679 $\pm$0.005 |'
- en: '| Aug-Tree (Trigrams) | 0.676 $\pm$0.030 | 0.826 $\pm$0.006 | 0.619 $\pm$0.010
    | 0.669 $\pm$0.006 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Aug-Tree (三元组) | 0.676 $\pm$0.030 | 0.826 $\pm$0.006 | 0.619 $\pm$0.010 |
    0.669 $\pm$0.006 |'
- en: '| CART | 0.574 $\pm$0.002 | 0.775 $\pm$0.005 | 0.599 $\pm$0.005 | 0.636 $\pm$0.002
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| CART | 0.574 $\pm$0.002 | 0.775 $\pm$0.005 | 0.599 $\pm$0.005 | 0.636 $\pm$0.002
    |'
- en: '| ID3 | 0.573 $\pm$0.004 | 0.795 $\pm$0.010 | 0.589 $\pm$0.002 | 0.638 $\pm$0.009
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| ID3 | 0.573 $\pm$0.004 | 0.795 $\pm$0.010 | 0.589 $\pm$0.002 | 0.638 $\pm$0.009
    |'
- en: 'Table 8: Performance (Accuracy) for Aug-Treeand Aug-Tree Ensemble. Values are
    averaged over 3 random dataset splits; error bars are standard error of the mean
    (many are within the points). *Emotion and Financial phrasebank results are not
    directly comparable to [Table 2](#S3.T2 "In Comparing Aug-GAM performance with
    black-box baselines ‣ 3.2 Aug-GAM text-classification performance ‣ 3 Results:
    Prediction performance ‣ Augmenting Interpretable Models with LLMs during Training"),
    as they have been modified for binary classification.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：Aug-Tree 和 Aug-Tree Ensemble 的性能（准确率）。数值取自 3 个随机数据集划分的平均值；误差条为均值的标准误差（许多误差条位于点内）。*情感和财务短语库的结果与
    [表 2](#S3.T2 "在将 Aug-GAM 性能与黑箱基准进行比较 ‣ 3.2 Aug-GAM 文本分类性能 ‣ 3 结果：预测性能 ‣ 在训练期间使用
    LLM 增强可解释模型") 不直接可比，因为它们已经被修改为二分类。
- en: '|  | Emotion* | Financial phrasebank* | Rotten tomatoes | SST2 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | 情感* | 财务短语库* | 腐烂番茄 | SST2 |'
- en: '| Aug-Tree | 0.637 $\pm$0.045 | 0.818 $\pm$0.014 | 0.613 $\pm$0.009 | 0.571
    $\pm$0.018 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Aug-Tree | 0.637 $\pm$0.045 | 0.818 $\pm$0.014 | 0.613 $\pm$0.009 | 0.571
    $\pm$0.018 |'
- en: '| Aug-Tree Ensemble | 0.800 $\pm$0.008 | 0.848 $\pm$0.006 | 0.619 $\pm$0.004
    | 0.614 $\pm$0.016 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Aug-Tree Ensemble | 0.800 $\pm$0.008 | 0.848 $\pm$0.006 | 0.619 $\pm$0.004
    | 0.614 $\pm$0.016 |'
- en: Appendix C fMRI experiment details
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C fMRI 实验细节
- en: This section gives more details on the fMRI experiment analyzed in [Sec 5](#S5
    "5 Analyzing fMRI data with Aug-imodels ‣ Augmenting Interpretable Models with
    LLMs during Training"); for more scientific details see the original study [[22](#bib.bib22)].
    [Sec 5](#S5 "5 Analyzing fMRI data with Aug-imodels ‣ Augmenting Interpretable
    Models with LLMs during Training") analyzes data from one human subject (UTS03)
    in the original study, as the subject listened to approximately hours of narrative
    speech from the Moth Radio Hour, which consists of short autobiographical stories.
    The subject underwent fMRI scanning as they listened, yielding an fMRI volume
    brain scan consisting of 95,556 voxels roughly every two seconds.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了 [第 5 节](#S5 "5 使用 Aug-imodels 分析 fMRI 数据 ‣ 在训练期间使用 LLM 增强可解释模型") 中分析的
    fMRI 实验的更多细节；有关更多科学细节，请参见原始研究 [[22](#bib.bib22)]。 [第 5 节](#S5 "5 使用 Aug-imodels
    分析 fMRI 数据 ‣ 在训练期间使用 LLM 增强可解释模型") 分析了原始研究中一个人类受试者（UTS03）的数据，该受试者听了大约几个小时的 Moth
    Radio Hour 叙述性演讲，这些演讲由简短的自传故事组成。受试者在听的过程中接受了 fMRI 扫描，得到一个包含 95,556 个体素的大脑 fMRI
    体积扫描，每隔大约两秒钟获取一次。
- en: The individual voxel models described in [Sec 5](#S5 "5 Analyzing fMRI data
    with Aug-imodels ‣ Augmenting Interpretable Models with LLMs during Training")
    are each fit to 9,461 training points, each corresponding to a different time
    point (after accounting for various preprocessing steps, such as trimming the
    beginning and end of the sequence). They are evaluated on 291 volumes which come
    from a narrative story that was not seen during training.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5节](#S5 "5 分析fMRI数据与Aug-imodels ‣ 在训练过程中通过LLMs增强可解释模型")中描述的各个体素模型分别拟合了9,461个训练点，每个点对应一个不同的时间点（考虑了各种预处理步骤，例如修剪序列的开始和结束部分）。它们在291个来自未见过的叙事故事的体积上进行了评估。
- en: '[Fig 11](#A3.F11 "In Appendix C fMRI experiment details ‣ Augmenting Interpretable
    Models with LLMs during Training") shows the generalization performance of the
    model for each voxel, measured by the correlation between the predicted response
    and the measured response. [Fig 5](#S5.F5 "In 5 Analyzing fMRI data with Aug-imodels
    ‣ Augmenting Interpretable Models with LLMs during Training") shows the performance
    difference between the Aug-GAM model and the BERT baseline.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11](#A3.F11 "在附录C fMRI实验细节 ‣ 在训练过程中通过LLMs增强可解释模型") 显示了模型在每个体素上的泛化性能，测量方法是预测响应与实际响应之间的相关性。
    [图5](#S5.F5 "在5中分析fMRI数据与Aug-imodels ‣ 在训练过程中通过LLMs增强可解释模型") 显示了Aug-GAM模型与BERT基线之间的性能差异。'
- en: '![Refer to caption](img/f51d76826edbf19603b011dceab7031e.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f51d76826edbf19603b011dceab7031e.png)'
- en: 'Figure 11: Generalization performance for individual-voxel models, measured
    by correlation between the predicted response and the measured response on the
    held-out test set. Some regions are very poorly predicted (blue), but many voxels
    can be predicted quite well (red).'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：个体体素模型的泛化性能，通过预测响应与在保留的测试集上的实际响应之间的相关性来衡量。一些区域的预测效果非常差（蓝色），但许多体素的预测效果相当好（红色）。
- en: 'Table 9: fMRI prediction performance using different methods. Eng1000 is a
    linear word-embedding baseline similar to word2vec which has been used in the
    neuroscience literature [[31](#bib.bib31)].'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：使用不同方法的fMRI预测性能。Eng1000是一个类似于word2vec的线性词嵌入基线，已在神经科学文献中使用[[31](#bib.bib31)]。
- en: '| Model | Order of ngram | $\rho$ | $\rho$ (top 1%) | $\rho$ (top 5%) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ngram顺序 | $\rho$ | $\rho$ (前1%) | $\rho$ (前5%) |'
- en: '| Eng1000 | 1 | 0.041 | 0.529 | 0.439 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Eng1000 | 1 | 0.041 | 0.529 | 0.439 |'
- en: '| GloVe | 1 | 0.044 | 0.521 | 0.426 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| GloVe | 1 | 0.044 | 0.521 | 0.426 |'
- en: '| BERT | 5 | 0.022 | 0.386 | 0.302 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 5 | 0.022 | 0.386 | 0.302 |'
- en: '| BERT | 10 | 0.035 | 0.457 | 0.365 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 10 | 0.035 | 0.457 | 0.365 |'
- en: '| BERT | 20 | 0.053 | 0.524 | 0.429 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 20 | 0.053 | 0.524 | 0.429 |'
- en: '| Aug-GAM (BERT) | 5 | 0.061 | 0.583 | 0.489 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Aug-GAM (BERT) | 5 | 0.061 | 0.583 | 0.489 |'
- en: '| Aug-GAM (BERT) | 10 | 0.062 | 0.586 | 0.489 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Aug-GAM (BERT) | 10 | 0.062 | 0.586 | 0.489 |'
- en: '| Aug-GAM (BERT) | 20 | 0.056 | 0.566 | 0.470 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Aug-GAM (BERT) | 20 | 0.056 | 0.566 | 0.470 |'
