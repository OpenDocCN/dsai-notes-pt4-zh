- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 17:33:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:33:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.07238](https://ar5iv.labs.arxiv.org/html/2408.07238)
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.07238](https://ar5iv.labs.arxiv.org/html/2408.07238)
- en: \DoubleSpacedXII\TheoremsNumberedThrough\ECRepeatTheorems\EquationsNumberedThrough\RRHFirstLine\LRHFirstLine\ECRRHFirstLine\ECLRHFirstLine\RRHSecondLine
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \DoubleSpacedXII\TheoremsNumberedThrough\ECRepeatTheorems\EquationsNumberedThrough\RRHFirstLine\LRHFirstLine\ECRRHFirstLine\ECLRHFirstLine\RRHSecondLine
- en: Using Advanced LLMs to Enhance Smaller LLMs\LRHSecondLineUsing Advanced LLMs
    to Enhance Smaller LLMs\ECRRHSecondLineUsing Advanced LLMs to Enhance Smaller
    LLMs\ECLRHSecondLineUsing Advanced LLMs to Enhance Smaller LLMs
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用先进的大型语言模型提升较小的大型语言模型\LRHSecondLine使用先进的大型语言模型提升较小的大型语言模型\ECRRHSecondLine使用先进的大型语言模型提升较小的大型语言模型\ECLRHSecondLine使用先进的大型语言模型提升较小的大型语言模型
- en: \RUNAUTHOR
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \RUNAUTHOR
- en: Wang et al.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人
- en: \RUNTITLE
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \RUNTITLE
- en: Library-based Interpretable Knowledge Distillation
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图书馆的可解释知识蒸馏
- en: \TITLE
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Using Better LLMs to Teach Lesser LLMs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更好的大型语言模型来教导较小的大型语言模型：
- en: '“Strategy” Knowledge Distillation for Dynamic Prompting \TITLEUsing Better
    LLMs to Teach Lesser LLMs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 动态提示的“策略”知识蒸馏 \TITLE使用更好的大型语言模型来教导较小的大型语言模型：
- en: A “Strategy” Knowledge Distillation Approach \TITLEInterpretable “Strategy”
    Knowledge Distillation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一种“策略”知识蒸馏方法 \TITLE可解释的“策略”知识蒸馏
- en: to Teach Lesser LLMs Using Better LLMs
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更好的大型语言模型来教导较小的大型语言模型
- en: \TITLE
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Using Better LLMs to Teach Lesser LLMs:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更好的大型语言模型来教导较小的大型语言模型：
- en: Interpretable Knowledge Distillation
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的知识蒸馏
- en: \TITLE
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Using Better LLMs to Teach Lesser LLMs:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更好的大型语言模型来教导较小的大型语言模型：
- en: Interpretable Knowledge Distillation via Strategy Teaching
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过策略教学的可解释知识蒸馏
- en: \TITLE
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Distilling Knowledge from Advanced LLMs:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从先进的大型语言模型中提取知识：
- en: An Interpretable Approach to Enhance Smaller LLMs
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可解释的方法来提升较小的大型语言模型
- en: \TITLE
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \TITLE
- en: 'Using Advanced LLMs to Enhance Smaller LLMs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用先进的大型语言模型提升较小的大型语言模型：
- en: An Interpretable Knowledge Distillation Approach
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可解释的知识蒸馏方法
- en: \ARTICLEAUTHORS\AUTHOR
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \ARTICLEAUTHORS\AUTHOR
- en: Tong Wang, K. Sudhir and Dat Hong¹¹1The authors are grateful for insightful
    comments from participants at the 2024 Biz AI conference, 2024 Four Schools (Columbia,
    NYU, Wharton, Yale) Conference, 2024 HongKong Quant Marketing Mini-Conference,
    Summer Workshop on AI for Business 2024, 2024 ISMS Doctoral Symposium in Sydney,
    2024 Monash Marketing Mini-Symposium, Yale Quantitative Marketing Brown Bag Seminar,
    marketing seminars at HKU and CUHK. \AFFYale School of Management
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 汤王、K. 苏迪尔和Dat Hong¹¹1 作者感谢2024年Biz AI会议、2024年四校（哥伦比亚大学、纽约大学、沃顿商学院、耶鲁大学）会议、2024年香港量化营销迷你会议、2024年夏季AI商业研讨会、2024年悉尼ISMS博士生研讨会、2024年蒙纳士大学营销迷你研讨会、耶鲁大学定量营销午餐研讨会、香港大学和中文大学的营销研讨会的参与者提供的有益意见。
    \AFFYale School of Management
- en: \EMAILtong.wang.tw687@yale.edu, k.sudhir@yale.edu, dat.hong@yale.edu
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \EMAILtong.wang.tw687@yale.edu, k.sudhir@yale.edu, dat.hong@yale.edu
- en: \ABSTRACT
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \ABSTRACT
- en: Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior
    performance in complex human-like interactions. But they are costly, or too large
    for edge devices such as smartphones and harder to self-host, leading to security
    and privacy concerns. This paper introduces a novel interpretable knowledge distillation
    approach to enhance the performance of smaller, more economical LLMs that firms
    can self-host. We study this problem in the context of building a customer service
    agent aimed at achieving high customer satisfaction through goal-oriented dialogues.
    Unlike traditional knowledge distillation, where the “student” model learns directly
    from the “teacher” model’s responses via fine-tuning, our interpretable “strategy”
    teaching approach involves the teacher providing strategies to improve the student’s
    performance in various scenarios. This method alternates between a “scenario generation”
    step and a “strategies for improvement” step, creating a customized library of
    scenarios and optimized strategies for automated prompting. The method requires
    only black-box access to both student and teacher models; hence it can be used
    without manipulating model parameters. In our customer service application, the
    method improves performance, and the learned strategies are transferable to other
    LLMs and scenarios beyond the training set. The method’s interpretabilty helps
    safeguard against potential harms through human audit.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 高级大型语言模型（LLMs），如GPT-4或LlaMa 3，在复杂的人类互动中表现出色。但它们价格昂贵，或过于庞大而不适合智能手机等边缘设备，并且难以自托管，导致安全和隐私问题。本文介绍了一种新颖的可解释知识蒸馏方法，以增强较小、更经济的LLMs的性能，这些模型可以由公司自托管。我们在构建一个旨在通过目标导向对话实现高客户满意度的客户服务代理的背景下研究这个问题。与传统知识蒸馏不同，传统知识蒸馏中“学生”模型通过微调直接从“教师”模型的响应中学习，我们的可解释“策略”教学方法涉及教师提供改进学生在各种场景下表现的策略。这种方法在“场景生成”步骤和“改进策略”步骤之间交替进行，创建了一个定制的场景库和优化的策略库，用于自动提示。该方法仅需要对学生和教师模型的黑箱访问；因此，可以在不操作模型参数的情况下使用。在我们的客户服务应用中，该方法提高了性能，并且学习到的策略可转移到其他LLMs和超出训练集的场景。该方法的可解释性有助于通过人工审核防范潜在的风险。
- en: \KEYWORDS
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \KEYWORDS
- en: Large Language Models, Knowledge Distillation, Interpretability, Customer Satisfaction
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型、知识蒸馏、可解释性、客户满意度
- en: 1 Introduction
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Advancements in large language models (LLMs) have enabled low-cost automation
    of many marketing tasks. Choosing the right LLM for a task involves balancing
    cost, feasibility, and performance. While high-performance models like GPT-4 are
    appealing and may seem like the obvious choice, many businesses prefer smaller,
    more affordable, or open-source LLMs. In high volume applications such as customer
    service, even small cost differences per query can lead to large differences in
    total cost; as such cheaper or free models like LlaMa may offer a better balance
    in tradeoffs, despite lower performance. Additionally, data privacy concerns drive
    firms to self-host LLMs instead of using third-party APIs, which could compromise
    data confidentiality. Consequently, there is growing interest in enhancing the
    performance of smaller, cost-effective LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的进步使得许多营销任务的低成本自动化成为可能。选择适合任务的LLM需要权衡成本、可行性和性能。虽然像GPT-4这样的高性能模型很有吸引力，并且可能看起来是显而易见的选择，但许多企业更喜欢较小、更实惠或开源的LLMs。在客户服务等高频应用中，即使每个查询的小成本差异也可能导致总成本的巨大差异；因此，像LlaMa这样的便宜或免费的模型可能在权衡中提供更好的平衡，尽管其性能较低。此外，数据隐私问题促使公司自托管LLMs，而不是使用可能危及数据机密性的第三方API。因此，越来越多的关注集中在提高较小、经济高效的LLMs的性能上。
- en: This paper examines whether we can augment the effectiveness of smaller, but
    more economical LLMs using the knowledge embedded in larger and superior LLMs.
    This concept is called knowledge distillation, where a more advanced LLM, referred
    to as the “teacher,” imparts its knowledge to a less sophisticated LLM, termed
    the “student,” to enhance its performance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了我们是否可以利用嵌入在更大、更高级的语言模型（LLMs）中的知识来增强较小但更经济的LLMs的效果。这个概念被称为知识蒸馏，其中更先进的LLM，称为“教师”，将其知识传授给较不复杂的LLM，称为“学生”，以提高其性能。
- en: While enhancing a weaker LLM through a more advanced LLM is a general problem
    applicable in many contexts, in this paper, we illustrate it for the problem of
    goal-oriented dialogue tasks (e.g. Wei et al. [2018](#bib.bib31), Bordes et al.
    [2016](#bib.bib3)). Goal-oriented dialogues aim to steer conversations toward
    specific desired outcomes (Ham et al. [2020a](#bib.bib8)); as such it is relevant
    for conversations involving negotiations (Samad et al. [2022](#bib.bib25)) and
    persuasion (Wang et al. [2019](#bib.bib30)). Specifically, we consider a customer
    service application, where an LLM acts as a customer service agent that interacts
    with the customer with the goal of achieving high customer satisfaction.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过更高级的 LLM 来提升较弱的 LLM 是一个适用于许多背景的普遍问题，但在本文中，我们将其应用于目标导向对话任务（例如 Wei et al.
    [2018](#bib.bib31)，Bordes et al. [2016](#bib.bib3)）。目标导向对话旨在将对话引导到特定的期望结果（Ham
    et al. [2020a](#bib.bib8)）；因此，它对涉及谈判（Samad et al. [2022](#bib.bib25)）和劝说（Wang
    et al. [2019](#bib.bib30)）的对话具有相关性。具体来说，我们考虑一个客户服务应用，其中一个 LLM 作为客户服务代表与客户互动，目标是实现高客户满意度。
- en: A common strategy for improving student performance through knowledge transfer
    from a teacher is fine-tuning, where the teacher generates training data to train
    the student (e.g., Tang et al. [2019](#bib.bib29), Agarwal et al. [2023](#bib.bib2),
    [2024](#bib.bib1)). However, this method has significant limitations. Firstly,
    fine-tuning requires access to and updates of model parameters, which is not always
    feasible, especially for LLMs that only allow API access; and also costly since
    it involves updating billions of parameters. Secondly, the distilled “knowledge”
    is encapsulated in model parameters, making it unintelligible to humans. This
    opacity hinders debugging and raises safety concerns, particularly when the teacher
    is an external, unverifiable model. The inaccessibility of distilled knowledge
    complicates maintenance, as tracking changes and ensuring consistency with previous
    iterations can be problematic. Finally, fine-tuning focuses only on lexical similarities
    in the outputs, ignoring the underlying strategies that lead a model to produce
    specific responses and the fact that different lexical choices can express identical
    meanings or follow the same response strategy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过知识传递来提高学生表现的一种常见策略是微调，其中教师生成训练数据来训练学生（例如，Tang et al. [2019](#bib.bib29)，Agarwal
    et al. [2023](#bib.bib2)，[2024](#bib.bib1)）。然而，这种方法有显著的局限性。首先，微调需要访问和更新模型参数，这并非总是可行，尤其是对于仅允许
    API 访问的 LLM（大型语言模型）；而且成本高，因为涉及到更新数十亿个参数。其次，提炼出的“知识”被封装在模型参数中，使其对人类不可理解。这种不透明性妨碍了调试并引发了安全问题，特别是当教师是一个外部的、不可验证的模型时。提炼知识的不可访问性使维护变得复杂，因为跟踪变化并确保与之前的迭代保持一致可能会遇到问题。最后，微调仅关注输出中的词汇相似性，忽略了导致模型产生特定回应的基本策略，以及不同的词汇选择可以表达相同的含义或遵循相同的回应策略这一事实。
- en: A student model can also be improved via prompt tuning (Lester et al. [2021](#bib.bib16),
    Hu et al. [2021](#bib.bib13)), also known as soft prompt tuning), where specific
    prompt tokens are fine-tuned to elicit better performance. This involves adjusting
    the prompt to align with desired outputs, enhancing the model’s ability to generate
    accurate and contextually appropriate responses. However, prompt tuning lacks
    interpretability, as the prompts are represented as embeddings that are hard to
    understand. Additionally, a single, fixed prompt tuned for an entire task cannot
    capture the richness and diversity of strategies needed for different scenarios.
    Furthermore, prompt tuning requires access to the LLM’s internals, making it inapplicable
    to LLMs that are only accessible via APIs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 学生模型还可以通过提示调优（Lester et al. [2021](#bib.bib16)，Hu et al. [2021](#bib.bib13)，也称为软提示调优）来改进，其中特定的提示令牌被微调以引发更好的表现。这涉及调整提示以与期望的输出对齐，从而增强模型生成准确且上下文适当的回应的能力。然而，提示调优缺乏可解释性，因为提示被表示为难以理解的嵌入。此外，为整个任务调优的单一固定提示无法捕捉到不同场景所需的丰富性和多样性。此外，提示调优需要访问
    LLM 的内部，这使得其不适用于仅通过 API 访问的 LLM。
- en: Therefore, we propose an interpretable knowledge distillation method. Instead
    of directly distilling teacher’s “knowledge” into student’s model parameters,
    which would require accessing and updating the student model, our method constructs
    a knowledge base that the student can query externally without altering the student
    model itself. This knowledge base, which we refer to as a library, consists of
    representative scenarios that the student could encounter during deployment, along
    with corresponding strategies. Each scenario is represented by an on-going dialogue
    between an agent and a customer, and the corresponding strategies guide the student
    on *how* to respond when continuing the dialogue. Then, during deployment, the
    library functions similarly to retrieval-augmented generation (RAG) (Lewis et al.
    [2020](#bib.bib17)); the student LLM identifies the most relevant scenarios through
    embeddings and applies the corresponding strategies to stimulate an appropriate
    response.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们提出了一种可解释的知识蒸馏方法。我们的方法不直接将教师的“知识”蒸馏到学生的模型参数中，这样做需要访问和更新学生模型。相反，我们的方法构建了一个知识库，学生可以在不更改学生模型本身的情况下，外部查询这个知识库。我们称这个知识库为库，它包含了学生在部署过程中可能遇到的代表性场景及其相应的策略。每个场景由代理和客户之间的持续对话表示，相应的策略指导学生在继续对话时*如何*回应。然后，在部署过程中，库的功能类似于检索增强生成（RAG）（Lewis等人
    [2020](#bib.bib17)）；学生LLM通过嵌入识别最相关的场景，并应用相应的策略来刺激适当的回应。
- en: Building a library for a multi-step, goal-oriented dialog problem like customer
    service presents additional challenges. First, generating scenarios involves interactive
    dialogues between the teacher and a customer to capture diverse conversational
    paths. However, during deployment, the student cannot perfectly replicate the
    teacher’s quality of interactions, leading to deviations. In multi-step settings
    such as conversations, even minor quality differences from the teacher’s responses
    at a given stage can lead to more negative customer response, and this negativity
    can cumulatively amplify over the entire conversation. As such, a student-customer
    conversation can veer into unencountered scenarios if the library is constructed
    based on only teacher-customer conversation scenarios. This phenomenon, known
    as distribution shift, poses a significant challenge when the student’s training
    data is generated exclusively from the teacher. Second, the strategies in the
    library must be adaptable to each student and specific scenario. These strategies
    should guide the student’s behavior by considering the student’s current capabilities
    and providing targeted suggestions. Finally, a third important issue is that agents
    must adhere to firm policies, business requirements, and cost constraints when
    addressing customer requests. Thus, these policies and constraints must be integrated
    into the strategy development process to ensure the agent’s solutions are practical
    and compliant with firm policies.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个针对多步骤、目标导向的对话问题（如客户服务）的库会带来额外的挑战。首先，生成场景涉及教师与客户之间的互动对话，以捕捉多样的对话路径。然而，在部署过程中，学生无法完美地复制教师的互动质量，从而导致偏差。在多步骤的设置中，如对话阶段，即使是教师在某一阶段的回应中的小质量差异也会导致客户反应更为负面，这种负面情绪可能会在整个对话中累积放大。因此，如果库仅基于教师-客户对话场景构建，那么学生-客户对话可能会偏离未遇到的场景。这种现象被称为分布转移，当学生的训练数据完全来源于教师时，这会带来显著的挑战。其次，库中的策略必须能够适应每个学生和具体场景。这些策略应通过考虑学生的当前能力并提供针对性的建议来指导学生的行为。最后，第三个重要问题是，代理在处理客户请求时必须遵守严格的政策、业务要求和成本限制。因此，这些政策和限制必须融入到策略开发过程中，以确保代理的解决方案既实用又符合公司政策。
- en: We design a novel method to address the aforementioned challenges. Our method
    is an iterative process where, in each iteration, a new batch of scenarios and
    their corresponding strategies are added to the library. To generate scenarios,
    both the student and teacher interact with the environment (e.g., a donor in a
    persuasion task or a customer in customer service) to produce conversations and
    sample scenarios. To address the issue of distribution shift, we ensure that enough
    interactions involve the student by gradually increasing the probability of selecting
    the student for scenario generation, so that the student eventually dominates
    this process. In the strategy learning phase, strategies are generated and refined
    iteratively. The teacher LLM evaluates both its own and the student’s responses,
    providing targeted strategies for the student to follow. These strategies are
    incorporated into prompts for subsequent refinement rounds, progressively honing
    the student’s ability to mimic the teacher. The library grows iteratively, with
    the teacher monitoring the student’s progress and deciding when to terminate the
    process based on the need for further feedback or significant improvement. The
    output is a customized library of scenarios and strategies, containing context-specific
    knowledge from the teacher that is most relevant to the student and the task.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了一种新颖的方法来解决上述挑战。我们的方法是一个迭代过程，在每次迭代中，新的场景及其对应的策略会被添加到库中。为了生成场景，学生和教师与环境进行互动（例如，在说服任务中的捐赠者或客户服务中的顾客），以生成对话和样本场景。为了解决分布转移的问题，我们通过逐步增加选择学生生成场景的概率来确保足够的互动，使学生最终主导这一过程。在策略学习阶段，策略被迭代生成和完善。教师LLM评估自己和学生的回答，提供针对性的策略供学生遵循。这些策略被纳入提示中用于后续的完善轮次，逐步提高学生模仿教师的能力。库会迭代增长，教师监控学生的进展，并根据是否需要进一步反馈或显著改善来决定何时终止过程。输出是一个包含教师针对学生和任务最相关的上下文特定知识的定制化场景和策略库。
- en: Our interpretable knowledge distillation approach offers several advantages
    over fine-tuning. First, by teaching strategies rather than responses, our method
    enables LLMs to understand how to handle different scenarios at a strategic level,
    providing a global view of the problem rather than simply mimicking lexical choices.
    Second, our method ensures easy transferability across LLMs and contexts. The
    strategy library allows LLMs to adapt to new, unseen situations, broadening their
    problem-solving capabilities. This library can be used by other student LLMs or
    for related customer service problems, potentially enhancing performance without
    direct training the student LLMs or on specific tasks. Third, the interpretability
    of our approach significantly enhances AI safety. By extracting explicit strategies,
    domain experts can review and understand the LLMs’ decision-making processes.
    This transparency facilitates trust and provides safeguards against misuse, errors,
    or adversarial influences. Finally, as our method does not require access to or
    modification of the student or teacher LLMs’ parameters (both can be queried solely
    via black-box access, such as APIs), it distinguishes itself from traditional
    knowledge distillation methods such as prompt tuning and fine-tuning, which require
    internal model access or parameter updates. This makes it particularly suitable
    for environments where direct model manipulation is impractical or restricted.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的可解释知识蒸馏方法相较于微调具有若干优势。首先，通过教授策略而非回答，我们的方法使LLMs能够在战略层面上理解如何处理不同场景，提供了问题的全局视角，而不仅仅是模仿词汇选择。其次，我们的方法确保了在LLMs和环境中的易迁移性。策略库使LLMs能够适应新的、未见过的情况，拓宽其问题解决能力。这个库可以被其他学生LLMs使用或用于相关的客户服务问题，可能提升性能而无需直接训练学生LLMs或针对特定任务。第三，我们方法的可解释性显著增强了AI安全性。通过提取明确的策略，领域专家可以审查和理解LLMs的决策过程。这种透明性促进了信任，并提供了防止误用、错误或对抗性影响的保障。最后，由于我们的方法不需要访问或修改学生或教师LLMs的参数（两者仅通过黑箱访问，如API进行查询），它区别于传统的知识蒸馏方法，如提示调优和微调，这些方法需要内部模型访问或参数更新。这使得它特别适用于直接模型操控不切实际或受限的环境。
- en: 'Based on our empirical application, i.e., multi-turn conversations in customer
    service, our key findings are as follows: First, teaching strategy is more effective
    than teaching responses for multi-turn generation. Second, context-specific strategies
    are more effective than global strategies, since the former can provide more targeted
    strategies for different scenarios. Third, even though the library is learned
    for a particular student LLM and specific contexts, it contains common knowledge
    that is transferrable across models and across contexts.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们的实证应用，即客户服务中的多轮对话，我们的主要发现如下：首先，教学策略比教学响应对于多轮生成更为有效。其次，特定情境的策略比全球策略更为有效，因为前者可以为不同场景提供更有针对性的策略。第三，即使库是为特定的学生LLM和特定情境而学习的，它仍然包含可在模型和情境之间转移的常识。
- en: 'The rest of this paper is organized as follows: §[2](#S2 "2 Related Work")
    discusses the related literature on LLM and knowledge distillation. §[3](#S3 "3
    Library-based Interpretable Knowledge Distillation") presents the proposed method,
    while §[4](#S4 "4 Experiments") evaluates it with an extensive set of experiments.
    §[5](#S5 "5 Conclusion") concludes the paper. More analyses are included in the
    online appendix.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：§[2](#S2 "2 相关工作") 讨论了关于LLM和知识蒸馏的相关文献。§[3](#S3 "3 基于库的可解释知识蒸馏")
    介绍了所提出的方法，而§[4](#S4 "4 实验") 则通过一系列广泛的实验对其进行了评估。§[5](#S5 "5 结论") 总结了全文。更多分析内容请参见在线附录。
- en: 2 Related Work
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Our work contributes to the recent but growing literature on marketing applications
    using LLMs. This body of research has typically focused on how LLMs can be used
    for market research and the study of human behavior, while also highlighting attendant
    challenges (Gui and Toubia [2023](#bib.bib7), Qiu et al. [2023](#bib.bib23), Horton
    [2023](#bib.bib11))). Specific marketing research applications include perceptual
    maps (e.g., Li et al. [2024](#bib.bib18)) and conjoint analysis (e.g., Brand et al.
    [2023](#bib.bib4), Gui and Toubia [2023](#bib.bib7)). In contrast, this paper
    is focused on how to effectively adapt and engineer an LLM for marketing tasks
    such as customer service. We will next discuss how our method relates to existing
    literature on knowledge distillation in LLMs and advances the literature on goal-oriented
    dialogs, particularly in multi-turn interactions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作对使用LLM的市场营销应用的最新而不断增长的文献做出了贡献。这些研究通常关注LLM如何用于市场调研和人类行为研究，同时也突出了相关的挑战（Gui和Toubia
    [2023](#bib.bib7)，Qiu等人 [2023](#bib.bib23)，Horton [2023](#bib.bib11)）。具体的市场研究应用包括感知图（例如，Li等人
    [2024](#bib.bib18)）和联合分析（例如，Brand等人 [2023](#bib.bib4)，Gui和Toubia [2023](#bib.bib7)）。相比之下，本文重点讨论如何有效地调整和工程化LLM以用于市场营销任务，例如客户服务。接下来，我们将讨论我们的方法与现有LLM知识蒸馏文献的关系，并推动了目标导向对话的文献，特别是在多轮互动中的进展。
- en: 2.1 Knowledge Distillation for LLM
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM的知识蒸馏
- en: The concept of utilizing a superior model to enhance a less powerful one is
    known as knowledge distillation. This technique was first introduced by Hinton
    et al. ([2015](#bib.bib10)) in the context of supervised learning and has been
    adapted for use with language models in recent years (Sanh et al. [2019](#bib.bib26),
    Sun et al. [2019](#bib.bib28)). In the realm of language models, knowledge distillation
    involves using a larger and more capable LLM to generate data that trains specialized,
    smaller models. Existing research on knowledge distillation for language models
    typically employs objective functions that either maximize the likelihood of high-probability
    sequences generated by the teacher model (Kim and Rush [2016](#bib.bib15)) or
    guide the student model to mimic the token-level probability distributions provided
    by the teacher (Sanh et al. [2019](#bib.bib26)). Some recent work also proposes
    to teach the student the rationale for solving a task (Hsieh et al. [2023](#bib.bib12),
    Magister et al. [2022](#bib.bib20)). However, all these methods require training
    the student model and updating its parameters. In our approach, however, we distill
    the knowledge into an external library that the student can query during inference,
    without the need for training the student model. The teacher’s knowledge is utilized
    by the student through retrieval-augmented generation (RAG), another popular technique
    in LLM. In doing so, our method requires only black-box access to the student
    model, such as through an API, which is not feasible with existing knowledge distillation
    techniques.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 利用更强大的模型来增强较弱模型的概念被称为知识蒸馏。这一技术最早由Hinton et al. ([2015](#bib.bib10)) 在监督学习的背景下提出，并在近年来被调整用于语言模型（Sanh
    et al. [2019](#bib.bib26)、Sun et al. [2019](#bib.bib28)）。在语言模型领域，知识蒸馏涉及使用更大、更强的LLM生成数据，以训练专门的、更小的模型。现有的语言模型知识蒸馏研究通常采用的目标函数是最大化教师模型生成的高概率序列的似然（Kim
    and Rush [2016](#bib.bib15)），或指导学生模型模仿教师提供的令牌级概率分布（Sanh et al. [2019](#bib.bib26)）。一些近期的研究还提出了教学生模型解决任务的理由（Hsieh
    et al. [2023](#bib.bib12)、Magister et al. [2022](#bib.bib20)）。然而，所有这些方法都需要训练学生模型并更新其参数。我们的方式则不同，我们将知识蒸馏到一个外部库中，学生模型可以在推理期间进行查询，无需训练学生模型。教师的知识通过检索增强生成（RAG），另一种在LLM中流行的技术，被学生模型利用。在这样做的过程中，我们的方法仅需对学生模型进行黑箱访问，例如通过API，这在现有的知识蒸馏技术中并不可行。
- en: 2.2 Goal-Oriented Dialogues
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 以目标为导向的对话
- en: Recent advancements in LLMs have significantly improved their application in
    complex goal-oriented dialogues (e.g., Ham et al. [2020b](#bib.bib9), Li et al.
    [2023](#bib.bib19), Snell et al. [2022](#bib.bib27)), but challenges and limitations
    remain. First, smaller LLMs often lack a strategic understanding of overall dialogue
    progression and fail to achieve dialogue objectives through multi-turn interactions
    (Cheng et al. [2024](#bib.bib5), Deng et al. [2023](#bib.bib6)). Second, the multi-step
    nature of goal-oriented dialogues fundamentally differs from one-step tasks like
    text classification and summarization as fine-tuning at each utterance level overlooks
    the interdependence of multi-turn utterances and the high-level strategy. Zhang
    et al. ([2023](#bib.bib33)) proposes an “Ask an Expert” solution, where a lesser
    model seeks advice from a better LLM for generating utterances, but this increases
    inference (i.e., response) time. Finally, some approaches rely on dialogues with
    specifically annotated strategies for training (e.g., Zhang et al. [2022](#bib.bib32),
    [2023](#bib.bib33), Joshi et al. [2021](#bib.bib14)), but the dependence on labeled
    datasets creates a significant barrier for practical application.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在大语言模型（LLMs）方面的进展显著提升了它们在复杂的目标导向对话中的应用（例如，Ham et al. [2020b](#bib.bib9)、Li
    et al. [2023](#bib.bib19)、Snell et al. [2022](#bib.bib27)），但仍然存在挑战和局限性。首先，小型的LLMs通常缺乏对整体对话进展的战略理解，并且无法通过多轮交互实现对话目标（Cheng
    et al. [2024](#bib.bib5)、Deng et al. [2023](#bib.bib6)）。其次，目标导向对话的多步骤性质与文本分类和摘要等单步骤任务根本不同，因为在每次发言级别进行微调忽略了多轮发言之间的相互依赖和高级策略。Zhang
    et al. ([2023](#bib.bib33)) 提出了一个“请教专家”的解决方案，其中较小的模型寻求更优秀的LLM的建议来生成发言，但这增加了推理（即响应）时间。最后，一些方法依赖于具有特定标注策略的对话进行训练（例如，Zhang
    et al. [2022](#bib.bib32)、[2023](#bib.bib33)、Joshi et al. [2021](#bib.bib14)），但对标注数据集的依赖为实际应用创造了重大障碍。
- en: 3 Library-based Interpretable Knowledge Distillation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于库的可解释知识蒸馏
- en: Given a student LLM, denoted as $\mathcal{S}(\cdot)$ (e.g., LlaMa 2 or GPT-3.5),
    our method involves creating a library consisting of a set of representative scenarios,
    paired with corresponding strategies constructed by a teacher LLM $\mathcal{T}(\cdot)$
    optimized for instructing the students on *how* to respond in those scenarios.
    We first set up the learning environment and then describe the algorithm. Then,
    we will show how the library is used during deployment and explain the benefits
    of our method.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个学生语言模型，记作$\mathcal{S}(\cdot)$（例如LlaMa 2或GPT-3.5），我们的方法涉及创建一个库，包含一组代表性场景，以及由教师语言模型$\mathcal{T}(\cdot)$优化构建的相应策略，用于指导学生在这些场景中*如何*回应。我们首先建立学习环境，然后描述算法。接着，我们将展示库在部署过程中的使用方式，并解释我们方法的好处。
- en: 3.1 Learning Environment
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 学习环境
- en: We set up a learning environment where the student attempts to improve its performance
    by mimicking the teacher. For each input, the teacher compares the student’s output
    with its own output to help the student respond like itself. To facilitate effective
    learning, we simulate a customer using GPT-4, which we denote as $\mathcal{C}(\cdot)$.
    The learning of the student relies on the interaction with the customer LLM. This
    design is motivated by recent research that advocates using LLMs to simulate human
    responses to reduce costs and improve efficiency (Li et al. [2024](#bib.bib18)),
    compared to conducting real human studies.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立了一个学习环境，让学生通过模仿教师来提高其表现。对于每个输入，教师将学生的输出与自己的输出进行比较，以帮助学生像自己一样回应。为了促进有效学习，我们使用GPT-4模拟客户，称之为$\mathcal{C}(\cdot)$。学生的学习依赖于与客户语言模型的互动。这一设计受到近期研究的启发，研究建议使用语言模型来模拟人类响应，以降低成本并提高效率（Li
    et al. [2024](#bib.bib18)），与进行真实的人类研究相比。
- en: '3.1.1 Simulating Customers:'
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 模拟客户：
- en: To simulate the customer, we describe the task in the prompt where we request
    the LLM to role-play as a customer calling an airline company to request customer
    service. Here, we focus on a specific context where the customer bought a restricted
    ticket (non-changeable and non-refundable) and requests to cancel it without penalty.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟客户，我们在提示中描述任务，请求语言模型扮演一个打电话给航空公司请求客户服务的客户角色。在这里，我们专注于一个具体的情境，即客户购买了一张受限票（不可更改且不可退款），并请求在没有罚款的情况下取消它。
- en: 'To increase the heterogeneity of the customers and the richness of the conversations,
    we vary the customer’s social styles, initial emotions, and difficulty. For social
    styles, we use four types based on the classification from Merrill and Reid ([1981](#bib.bib21)).
    (i) Driver: results-driven, confident, and assertive; (ii) Analytical: detail-oriented,
    systematic, and logical; (iii) Amiable: cooperative, empathetic, and relationship-focused;
    and (iv) Expressive: enthusiastic, creative, and spontaneous²²2We provide more
    detailed description of each type in the prompt for simulating customers with
    GPT-4.. For initial emotions, we set four different customer emotions when initiating
    the call: calm, confused, concerned, and frustrated, leading to varied dialogue
    developments. Additionally, we vary the difficulty level of the customer by including/not
    including the keyword “demanding” in the customer role description to the LLM.
    We observe that including “demanding” changes customer behavior, making customers
    more persistent with their requests. We use $q(s,e,d)$ to represent the prompt
    for the customer LLM, parameterized by the social style $s$, initial emotion $e$,
    and difficulty level $d$.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加客户的异质性和对话的丰富性，我们变化客户的社交风格、初始情感和难度。社交风格方面，我们使用四种类型，这些类型基于Merrill和Reid（[1981](#bib.bib21)）的分类。（i）驱动型：结果导向、自信且果断；（ii）分析型：注重细节、系统且逻辑；（iii）友善型：合作、富有同情心且以关系为重点；（iv）表达型：热情、富有创意且即兴²²2我们在模拟客户的提示中提供了每种类型的更详细描述。对于初始情感，我们设定了四种不同的客户情感：冷静、困惑、关切和沮丧，导致对话发展的变化。此外，我们通过在客户角色描述中包含/不包含关键词“demanding”来变化客户的难度水平。我们观察到包含“demanding”会改变客户的行为，使客户在请求时更加坚持。我们使用$q(s,e,d)$来表示客户语言模型的提示，参数化为社交风格$s$、初始情感$e$和难度水平$d$。
- en: Given the four social styles, four initial emotions, and two difficulty levels,
    we create a total of 32 types of customers. When simulating customers, each of
    the 32 types generates many different conversations because we set the temperature
    of the LLM to non-zero.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于四种社交风格、四种初始情感和两个难度等级，我们创建了总共32种类型的客户。在模拟客户时，由于我们将语言模型的温度设置为非零，每种类型的客户会生成许多不同的对话。
- en: '3.1.2 Simulating Teacher:'
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 模拟教师：
- en: 'We then describe how to prompt a teacher LLM to act as an agent. Here we choose
    the state-of-the-art LLM, GPT-4, as the teacher. We define a base prompt instructing
    GPT-4 to role-play as a customer service agent. We denote the prompt as $p_{\text{base}}$,
    which contains three key components: role, goal, and constraints.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们描述了如何提示教师 LLM 扮演代理角色。在这里，我们选择最先进的 LLM GPT-4 作为教师。我们定义了一个基础提示，指示 GPT-4 扮演客服代理的角色。我们将这个提示表示为
    $p_{\text{base}}$，它包含三个关键组件：角色、目标和约束。
- en: The role specifies the position or function that GPT-4 needs to assume. For
    example, prompting GPT-4 to “role-play as a customer service agent” sets the context
    for the interaction, guiding the model to generate responses suitable for customer
    service scenarios. The goal establishes the desired outcome and ultimate objective
    of the interaction, guiding GPT-4 to adjust its strategies accordingly. For instance,
    if the goal is to achieve high customer satisfaction, GPT-4 will tailor its responses
    to be more empathetic, helpful, and solution-focused. Including the goal in the
    prompt is essential because it provides clear direction for the LLM, ensuring
    that its responses align with the intended results. The constraint establishes
    the limitations and boundaries that the LLM needs to follow, and as explained
    earlier, it helps to ensure that the LLM accounts for business constraints and
    firm policy. As such, the agent is instructed to adhere to these constraints while
    aiming to achieve high customer satisfaction. This ensures that all agents follow
    the same company policy, differing only in their communication strategies, which
    the teacher aims to teach the student.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 角色指定了 GPT-4 需要担任的位置或职能。例如，提示 GPT-4 “扮演客服代理” 设置了互动的背景，指导模型生成适合客服场景的回应。目标确立了互动的期望结果和最终目标，指导
    GPT-4 根据需要调整其策略。例如，如果目标是实现高客户满意度，GPT-4 将调整其回应，使其更加富有同情心、有帮助，并以解决问题为导向。在提示中包含目标是至关重要的，因为它为
    LLM 提供了明确的方向，确保其回应与预期结果一致。约束确定了 LLM 需要遵循的限制和边界，如前所述，它有助于确保 LLM 考虑到业务限制和公司政策。因此，代理被指示遵守这些约束，同时致力于实现高客户满意度。这确保了所有代理遵循相同的公司政策，仅在其沟通策略上有所不同，这正是教师旨在教授学生的内容。
- en: The base prompt is fixed for the entire task and included in the prompt for
    both the teacher and student.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基础提示在整个任务中是固定的，并且包含在教师和学生的提示中。
- en: 3.2 Knowledge Distillation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 知识蒸馏
- en: 'Our approach iterates over three steps: scenario generation, strategy teaching,
    and goal evaluation, progressively building up a customized library consisting
    of scenarios and strategies for handling them. We call each execution of the three
    steps as one iteration of the algorithm. We use $\mathcal{L}^{(t)}=\{(\mathbf{s}^{(t)}_{i},p_{\text{strategy}}(\mathbf{s}^{(t)}_{i}))_{i=1}^{n_{t}}\}$
    to represent the library at the end of iteration $t$, where $\mathbf{s}^{(t)}_{i}$
    represents a scenario indexed by $i$, $p_{\text{strategy}}(\mathbf{s}^{(t)}_{i})$
    is the corresponding prompt, and $n_{t}$ is the total number of scenarios in the
    library at iteration $t$. As $t$ increases, the library grows larger. Below, we
    detail each step.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法包括三个步骤：场景生成、策略教学和目标评估，逐步构建一个包含处理场景和策略的定制化库。我们将三步骤的每次执行称为算法的一次迭代。我们使用 $\mathcal{L}^{(t)}=\{(\mathbf{s}^{(t)}_{i},p_{\text{strategy}}(\mathbf{s}^{(t)}_{i}))_{i=1}^{n_{t}}\}$
    来表示第 $t$ 次迭代结束时的库，其中 $\mathbf{s}^{(t)}_{i}$ 代表按 $i$ 索引的场景，$p_{\text{strategy}}(\mathbf{s}^{(t)}_{i})$
    是相应的提示，$n_{t}$ 是第 $t$ 次迭代时库中的场景总数。随着 $t$ 的增加，库会不断扩大。下面，我们详细介绍每一步。
- en: '3.2.1 Scenario Generation:'
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 场景生成：
- en: 'In this step, the LLM agent interacts with the customer $\mathcal{C}(\cdot)$
    to generate conversations, denoted as $\mathbf{x}^{(t,l)}$, where $t$ is the iteration
    index and $l$ is the conversation index. $\mathbf{x}^{(t,l)}$ consists of a sequence
    of utterances from the customer $\mathcal{C}(\cdot)$ and the customer service
    agent LLM (e.g., $\mathcal{T}$ or $\mathcal{S}$):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，LLM 代理与客户 $\mathcal{C}(\cdot)$ 进行互动，生成对话，表示为 $\mathbf{x}^{(t,l)}$，其中
    $t$ 是迭代索引，$l$ 是对话索引。$\mathbf{x}^{(t,l)}$ 包含来自客户 $\mathcal{C}(\cdot)$ 和客服代理 LLM（例如，$\mathcal{T}$
    或 $\mathcal{S}$）的一系列话语：
- en: '|  | $\mathbf{x}^{(t,l)}=(\mathbf{a}^{(t,l)}_{1},\mathbf{c}^{(t,l)}_{1},\mathbf{a}^{(t,l)}_{2}\cdots),$
    |  | (1) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}^{(t,l)}=(\mathbf{a}^{(t,l)}_{1},\mathbf{c}^{(t,l)}_{1},\mathbf{a}^{(t,l)}_{2}\cdots),$
    |  | (1) |'
- en: where $\mathbf{a}^{(t,l)}_{k}$ represents the agent’s utterance at the $k$-th
    turn and $\mathbf{c}^{(t,l)}_{k}$ represents the customer’s utterance at the $k$-the
    turn. Without loss of generality, we assume a conversation always starts with
    the agent’s utterance, for example, “Hello, how may I help you?”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{a}^{(t,l)}_{k}$表示第$k$轮的代理发言，$\mathbf{c}^{(t,l)}_{k}$表示第$k$轮的客户发言。为简单起见，我们假设对话总是以代理的发言开始，例如，“您好，我能帮您什么？”
- en: Since the generation of an utterance depends on all previous exchanges, we define
    a subconversation with the first $k$ turns, ending with the customer’s utterance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于发言的生成依赖于所有之前的交流，我们定义了一个以第$k$轮结束的客户发言的子对话。
- en: '|  | $\mathbf{x}^{(t,l)}_{[:k]}=(\mathbf{a}^{(t,l)}_{1},\mathbf{c}^{(t,l)}_{1},\cdots,\mathbf{a}^{(t,l)}_{k},\mathbf{c}^{(t,l)}_{k}).$
    |  | (2) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}^{(t,l)}_{[:k]}=(\mathbf{a}^{(t,l)}_{1},\mathbf{c}^{(t,l)}_{1},\cdots,\mathbf{a}^{(t,l)}_{k},\mathbf{c}^{(t,l)}_{k}).$
    |  | (2) |'
- en: 'The generation of the teacher’s utterance at the $k$-th turn is based on the
    prior conversation $x^{(t,l)}_{[:k]}$ as well as the base prompt $p_{\text{base}}$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在第$k$轮生成教师发言是基于之前的对话$x^{(t,l)}_{[:k]}$以及基础提示$p_{\text{base}}$：
- en: '|  | $\mathbf{a}^{(t,l)}_{k}=\mathcal{T}(\mathbf{x}^{(t,l)}_{[:k-1]}&#124;p_{\text{base}}).$
    |  | (3) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{a}^{(t,l)}_{k}=\mathcal{T}(\mathbf{x}^{(t,l)}_{[:k-1]}&#124;p_{\text{base}}).$
    |  | (3) |'
- en: 'The customer’s utterance is determined by the prior conversation and the parameterized
    prompt $q(s,e,d)$:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 客户的发言由之前的对话和参数化提示$q(s,e,d)$决定：
- en: '|  | $\mathbf{c}^{(t,l)}_{k}=\mathcal{C}(\mathbf{x}^{(t,l)}_{[:k-1]},\mathbf{a}^{(t,l)}_{k}&#124;q(s,e,d)).$
    |  | (4) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{c}^{(t,l)}_{k}=\mathcal{C}(\mathbf{x}^{(t,l)}_{[:k-1]},\mathbf{a}^{(t,l)}_{k}&#124;q(s,e,d)).$
    |  | (4) |'
- en: For each iteration $t$, we generate a set of conversations, denoted as $\mathbf{X}^{(t)}$,
    by varying the customer prompt parameters $s$(social styles), $e$(initial emotions)
    and $d$ (difficulty) and running multiple times for each set of parameters with
    a non-zero temperature.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代$t$，我们通过改变客户提示参数$s$(社交风格)、$e$(初始情绪)和$d$ (难度)，并对每组参数运行多次生成一组对话，记作$\mathbf{X}^{(t)}$。
- en: Subconversations, defined in Equation (2), are randomly sampled from each conversation
    in $\mathbf{X}^{(t)}$. We call these subconversations scenarios. Each scenario
    includes the entire prior conversation ending with the customer’s utterance. We
    use $\mathbf{S}^{(t)}$ to represent the set of scenarios sampled from $\mathbf{X}^{(t)}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 子对话，如方程（2）所定义，从$\mathbf{X}^{(t)}$中的每个对话中随机抽取。我们称这些子对话为场景。每个场景包括以客户发言结束的整个之前对话。我们用$\mathbf{S}^{(t)}$表示从$\mathbf{X}^{(t)}$中抽取的场景集合。
- en: In a naive solution, the conversations in $\mathbf{X}^{(t)}$ are generated by
    $\mathcal{T}$ interacting with $\mathcal{C}$. However, each scenario extracted
    from $\mathbf{X}^{(t)}$ reflects the specific dynamics and decisions made by the
    teacher $\mathcal{T}$, not necessarily those a student would encounter or make
    when interacting independently in similar contexts. This leads to compounding
    errors because the scenarios the student encounters during deployment will differ
    from those it observed during training. In a multi-step decision-making process,
    even a small deviation in one step can compound over subsequent steps, leading
    to significant differences at the conversation level. This mismatch between training
    and deployment scenarios is a significant for challenge multi-step imitations,
    often referred to as distribution shift (Pomerleau [1991](#bib.bib22), Ross and
    Bagnell [2010](#bib.bib24)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个简单的解决方案中，$\mathbf{X}^{(t)}$中的对话是通过$\mathcal{T}$与$\mathcal{C}$的交互生成的。然而，从$\mathbf{X}^{(t)}$中提取的每个场景反映了教师$\mathcal{T}$所做的具体动态和决策，而不一定是学生在类似环境下独立互动时会遇到或做出的决策。这导致了累积错误，因为学生在部署过程中遇到的场景将不同于训练过程中观察到的场景。在多步骤决策过程中，即使是一步的小偏差也会在后续步骤中累积，从而导致对话级别上的显著差异。这种训练与部署场景之间的差异是多步骤模仿中的一个重要挑战，通常被称为分布偏移（Pomerleau
    [1991](#bib.bib22)，Ross和Bagnell [2010](#bib.bib24)）。
- en: To overcome this challenge, we let the student participate in the scenario generation
    step. We assign a probability $p$ for the teacher to be selected to interact with
    the customer for conversation generation and $1-p$ for the student to be selected.
    Initially, $p$ is set to 1 in iteration 1 and gradually decreases in subsequent
    iterations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一挑战，我们让学生参与场景生成步骤。我们为教师与客户互动生成对话分配概率$p$，为学生选择分配$1-p$。最初，$p$在第1次迭代中设置为1，并在后续迭代中逐渐减少。
- en: 'In each iteration, when the student is selected to interact with the customer
    LLM, it will produce an output based on the scenarios and strategies retrieved
    from the current library:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，当学生被选择与客户LLM互动时，它将根据从当前库中检索到的场景和策略生成输出：
- en: '|  | $\mathbf{a}^{(t,l)}_{k}=\mathcal{S}(\mathbf{x}^{(t,l)}_{[:k-1]}&#124;p_{\text{base}},\mathcal{L}^{(t)}).$
    |  | (5) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{a}^{(t,l)}_{k}=\mathcal{S}(\mathbf{x}^{(t,l)}_{[:k-1]}&#124;p_{\text{base}},\mathcal{L}^{(t)}).$
    |  | (5) |'
- en: We will explain how to retrieve the relevant strategy from the library in Section
    [3.3](#S3.SS3 "3.3 Deployment ‣ 3 Library-based Interpretable Knowledge Distillation").
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第[3.3](#S3.SS3 "3.3 Deployment ‣ 3 Library-based Interpretable Knowledge
    Distillation")节中解释如何从库中检索相关策略。
- en: As the iterations progress, more scenarios will be accumulated into $\mathcal{L}^{(t)}$,
    covering a more diverse set of situations the student could encounter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 随着迭代的进行，更多的场景将被累积到$\mathcal{L}^{(t)}$中，覆盖学生可能遇到的更多样化的情况。
- en: '3.2.2 Strategy Teaching:'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 策略教学：
- en: 'In this step, the teacher iteratively generates and refines strategies for
    each scenario until the student accurately mimics the teacher’s output for that
    scenario. See Figure [1](#S3.F1 "Figure 1 ‣ 3.2.2 Strategy Teaching: ‣ 3.2 Knowledge
    Distillation ‣ 3 Library-based Interpretable Knowledge Distillation") for an illustration
    of the process.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '在此步骤中，教师迭代生成和细化每个场景的策略，直到学生准确模仿教师在该场景中的输出。有关该过程的说明，请参见图[1](#S3.F1 "Figure 1
    ‣ 3.2.2 Strategy Teaching: ‣ 3.2 Knowledge Distillation ‣ 3 Library-based Interpretable
    Knowledge Distillation")。'
- en: '![Refer to caption](img/03c2080736cf4fa24fa106e83dd76814.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/03c2080736cf4fa24fa106e83dd76814.png)'
- en: 'Figure 1: An iterative process of strategy teaching'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：策略教学的迭代过程
- en: We call the prompt generated at this step the *strategy prompt*. The strategy
    prompt will be combined with the base prompt to instruct the student *how* to
    behave like the teacher.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称此步骤生成的提示为*策略提示*。策略提示将与基础提示结合，以指导学生*如何*像教师一样行为。
- en: Specifically, for each scenario, denoted as $\mathbf{s}$, both $\mathcal{T}$
    and $\mathcal{S}$ generate a response. These responses are then evaluated by $\mathcal{T}$,
    which identifies discrepancies and proposes updates to the strategy. The updates
    are suggestions made to the student instructing what they should or should not
    do, based on their current output. They can vary from general, such as “use a
    more empathetic tone” to highly nuanced, such as “address the customer by their
    first name”, tailored to the specific demands of each scenario and the difference
    between the student and the teacher.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，对于每个场景，记作$\mathbf{s}$，$\mathcal{T}$和$\mathcal{S}$都生成一个响应。然后，这些响应由$\mathcal{T}$进行评估，识别差异并提出对策略的更新。这些更新是对学生的建议，指示他们应该或不应该做什么，基于他们当前的输出。更新可以是一般性的，例如“使用更具同理心的语气”，也可以是高度细致的，例如“用名字称呼客户”，根据每个场景的具体要求和学生与教师之间的差异进行调整。
- en: These updates are incorporated into the strategy prompt, denoted as $p_{\text{strategy}}(\mathbf{s})$.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更新被纳入到策略提示中，记作$p_{\text{strategy}}(\mathbf{s})$。
- en: '|  | teacher’s output: | $\displaystyle\mathbf{a}=\mathcal{T}(\mathbf{s}&#124;p_{\text{base}}).$
    |  | (6) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 教师的输出： | $\displaystyle\mathbf{a}=\mathcal{T}(\mathbf{s}&#124;p_{\text{base}}).$
    |  | (6) |'
- en: '|  | student’s output: | $\displaystyle\mathbf{a}=\mathcal{S}(\mathbf{s}&#124;p_{\text{base}},p_{\text{strategy}}(\mathbf{s})).$
    |  | (7) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 学生的输出： | $\displaystyle\mathbf{a}=\mathcal{S}(\mathbf{s}&#124;p_{\text{base}},p_{\text{strategy}}(\mathbf{s})).$
    |  | (7) |'
- en: 'This process of iterative refinement of $p_{\text{strategy}}(\mathbf{s})$ continues
    until there are no further updates from the teacher or a pre-defined maximum number
    of refinements is reached. By progressively refining its responses based on learned
    strategies and feedback, the student model becomes more adept at handling the
    scenarios, aligning its behavior with the teacher’s expertise and improving overall
    performance³³3A detailed example of updating the strategies and improving the
    response is shown in Appendix [8](#S8 "8 How Teacher LLM Iteratively Updates Strategies
    for Student LLM: Example")..'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '$p_{\text{strategy}}(\mathbf{s})$的迭代细化过程将持续进行，直到没有进一步的教师更新或达到预定义的最大细化次数。通过根据学习到的策略和反馈逐步改进其响应，学生模型在处理场景时变得更加熟练，使其行为与教师的专业知识一致，并提高整体表现³³一个详细的策略更新和响应改进的示例见附录[8](#S8
    "8 How Teacher LLM Iteratively Updates Strategies for Student LLM: Example")。'
- en: '3.2.3 Termination Mechanism:'
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 终止机制：
- en: Our main algorithm is an iterative process that gradually grows the library
    by adding a new batch of scenarios and their corresponding strategy prompts at
    each iteration. The thorough coverage of the data space by the scenarios is crucial
    for maintaining the robustness and reliability of the strategy prompts during
    deployment. To determine when the sufficiency is reached, we incorporate a goal
    evaluation step in the iterative process, where the student LLM is evaluated,
    based on the interactions with the customer LLM, after each new batch is added
    to the library, to determine whether the student LLM has achieved satisfactory
    performance using the current library.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要算法是一个迭代过程，通过在每次迭代中添加一批新的场景及其对应的策略提示来逐步扩展库。场景对数据空间的彻底覆盖对于保持策略提示在部署期间的鲁棒性和可靠性至关重要。为了确定何时达到充分性，我们在迭代过程中引入了一个目标评估步骤，其中学生LLM在每次新的批次被添加到库中后，会根据与客户LLM的互动进行评估，以确定学生LLM是否在使用当前库的情况下达到了令人满意的性能。
- en: We choose to use LLMs for the evaluation, motivated by recent research showing
    the potential of using LLMs to substitute human evaluations in various tasks,
    particularly in assessing other LLMs, with findings of up to 80% agreement with
    human judgment. Leveraging this advancement, our approach employs the teacher,
    a state-of-the-art LLM, to directly evaluate the outputs of the student LLM utilizing
    the current library. This direct evaluation is especially coherent since the student
    has been trained on the teacher’s strategies, making the teacher an ideal evaluator
    due to its understanding of the intricacies and subtleties required in the responses.
    We let the student interact with the environment to generate a validation set
    and let the teacher score it. This score is recorded and compared with the score
    from the previous iteration. The algorithm stops if there is no (significant)
    improvement after a pre-defined number of iterations (we set it to 2).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用LLMs进行评估，受到近期研究的启发，这些研究显示LLMs在各种任务中代替人工评估的潜力，特别是在评估其他LLMs方面，发现与人工判断的符合度高达80%。利用这一进展，我们的方法使用教师，一个最先进的LLM，直接评估学生LLM利用当前库生成的输出。这种直接评估尤其一致，因为学生已经在教师的策略下进行过训练，使得教师成为理想的评估者，因为它了解回答中所需的复杂性和细微之处。我们让学生与环境互动生成验证集，并让教师对其进行评分。记录下这个评分，并与之前迭代中的评分进行比较。如果在预定义的迭代次数（我们设置为2）之后没有（显著的）改进，算法将停止。
- en: 3.3 Deployment
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 部署
- en: The training algorithm generates a library comprising a set of scenarios and
    their associated strategy prompts. We will now discuss how this library is utilized
    during deployment.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法生成一个包含一组场景及其相关策略提示的库。我们现在将讨论这个库在部署期间如何使用。
- en: 'As shown in Equation ([7](#S3.E7 "Equation 7 ‣ 3.2.2 Strategy Teaching: ‣ 3.2
    Knowledge Distillation ‣ 3 Library-based Interpretable Knowledge Distillation")),
    the student’s output is determined by the scenario, the base prompt, and the strategy
    prompt optimized for that specific scenario during training. However, during deployment,
    the prior conversations differ from the training scenarios, and there is no predefined
    $p_{\text{strategy}}$ for the test input. To address this, we employ a method
    akin to retrieval-augmented generation (Lewis et al. [2020](#bib.bib17)), where
    we identify the most similar scenario(s) from a library based on their embeddings.
    The corresponding strategy from these similar scenarios is then applied to the
    new, unseen input, i.e.,'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '如公式 ([7](#S3.E7 "公式 7 ‣ 3.2.2 策略教学: ‣ 3.2 知识蒸馏 ‣ 3 基于库的可解释知识蒸馏")) 所示，学生的输出由场景、基础提示和在训练过程中为该特定场景优化的策略提示决定。然而，在部署过程中，之前的对话与训练场景不同，且测试输入没有预定义的
    $p_{\text{strategy}}$。为了解决这个问题，我们采用类似于检索增强生成的方法（Lewis et al. [2020](#bib.bib17)），通过其嵌入从库中识别出最相似的场景。然后将这些相似场景中的策略应用于新的、未见过的输入，即，'
- en: '|  | student’s output: | $\displaystyle\quad\mathbf{a}=\mathcal{S}(\mathbf{s}&#124;p_{\text{base}},p_{\text{strategy}}(\tilde{\mathbf{s}})),\
    \text{where}$ | $\displaystyle\tilde{\mathbf{s}}=\argmin_{\mathbf{s}^{\prime}\in\mathcal{L}}d(\mathbf{s},\mathbf{s}^{\prime}).$
    |  | (8) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 学生的输出： | $\displaystyle\quad\mathbf{a}=\mathcal{S}(\mathbf{s}&#124;p_{\text{base}},p_{\text{strategy}}(\tilde{\mathbf{s}})),\
    \text{其中}$ | $\displaystyle\tilde{\mathbf{s}}=\argmin_{\mathbf{s}^{\prime}\in\mathcal{L}}d(\mathbf{s},\mathbf{s}^{\prime}).$
    |  | (8) |'
- en: By leveraging scenario embeddings, the library can efficiently match new, unseen
    inputs to the most similar scenarios, thereby providing a tailored strategy prompt.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用场景嵌入，库可以高效地将新的、未见过的输入匹配到最相似的场景，从而提供量身定制的策略提示。
- en: Notably, the use of the library is flexible. When retrieving strategies, it
    is not necessary to limit retrieval to a single closest scenario; instead, one
    can retrieve $k> scenarios from the library, following the Scenario Agent: Hello, how can I help
    you? Customer: Hello, I recently booked a flight with your airline, and I need
    to cancel it. I tried to do it online, but I couldn’t find an option to cancel.
    Can you assist me with canceling my flight? Agent: Of course, I’d be happy to
    assist you with that. Could you please provide me with your booking reference
    number so I can pull up your reservation details? Customer: Sure, my booking reference
    number is ABC123.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 'Scenario Agent: Hello, how can I help
    you? Customer: Hello, I recently booked a flight with your airline, and I need
    to cancel it. I tried to do it online, but I couldn’t find an option to cancel.
    Can you assist me with canceling my flight? Agent: Of course, I’d be happy to
    assist you with that. Could you please provide me with your booking reference
    number so I can pull up your reservation details? Customer: Sure, my booking reference
    number is ABC123.'
- en: The student’s initial response (from the base LLM, without following any strategies
    from the teacher) is very different from the teacher’s response. The student does
    not provide any explanation of the policy or offer alternative solutions but directly
    expresses rejection. It overall sounds very cold and blunt.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 学生的初始回应（来自基础 LLM，没有遵循教师的任何策略）与教师的回应非常不同。学生没有提供任何政策解释或提供替代方案，而是直接表示拒绝。整体上听起来非常冷漠和直接。
- en: 'Teacher Response Teacher: Thank you for providing
    the booking reference number. Please allow me a moment to retrieve your booking
    details. [After a moment] I see that you have booked a restricted ticket. As per
    our policy, restricted tickets are non-refundable and non-changeable. This is
    because these tickets are offered at a lower cost compared to our flexible tickets.
    I understand this might not be the news you were hoping for, and I apologize for
    any inconvenience this may cause. However, we do have some alternative solutions
    that might be helpful. We can offer you travel credits for the value of your ticket,
    or the possibility of a one-time rescheduling, subject to applicable fees. Would
    you like to hear more about these options?Student Response Student (base LLM): Thank you
    for providing that information. I have pulled up your reservation, and I see that
    you have a restricted ticket, which is non-refundable and non-changeable. I understand
    that you wish to cancel your flight, but I must inform you that as per the purchase
    agreement, restricted tickets are not eligible for free cancellations or refunds.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 教师回复 教师：感谢您提供了预订参考号码。请允许我稍等片刻以检索您的预订详情。[稍等片刻]
    我看到您预订的是一张限制票。根据我们的政策，限制票是不可以退还和更改的。因为这些票的价格低于我们的灵活票。我理解这可能不是您所希望得到的消息，对由此造成的不便我深表歉意。不过，我们有一些替代方案可能对您有帮助。我们可以为您提供相当于票面价值的旅行积分，或者提供一次性重新安排的可能性，但需支付相关费用。您是否想了解更多关于这些选项的信息？学生回复 学生（基础 LLM）：感谢您提供的信息。我已经调出了您的预订记录，并看到您持有的是一张限制票，这张票不可退还也不可更改。我理解您希望取消航班，但我必须通知您，根据购买协议，限制票不符合免费取消或退款的条件。
- en: Based on these two responses, the teacher compares and then proposes changes
    to the student to improve, using the teacher’s response as the desired output.
    The teacher’s strategies at the first iteration point out the issues and offer
    suggestions for improvement. Incorporating the suggestions, the student’s response
    after iteration 1 improved significantly. The student offered an explanation of
    the policy and offered alternative solutions to the customer.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这两次回应，教师对比并提出改进建议，以教师的回应作为期望输出。教师在第一次迭代时指出问题并提供改进建议。结合这些建议，学生在迭代1后的回应显著改善。学生提供了政策解释并提出了替代解决方案。
- en: 'Strategies and Updated Student Responses At Iteration 1
    Strategies: 1\. Acknowledge Customer’s Request: Acknowledge the
    customer’s request promptly and express willingness to assist. 2\. Provide Explanation:
    Offer clear and empathetic explanations of the situation, such as ticket restrictions.
    3\. Offer Alternatives: Demonstrate a proactive approach by providing alternative
    solutions or options to assist the customer. 4\. Avoid Blunt Statements: Avoid
    using blunt statements without context or empathy, as it can be dismissive to
    the customer. 5\. Avoid Lack of Explanation: Provide clear explanations to avoid
    leaving the customer confused or frustrated. 6\. Avoid Failure to Offer Solutions:
    Offer alternative solutions or options to prevent a negative customer experience.
    Student: Thank you for providing your booking reference number, ABC123\. After
    reviewing your reservation, I see that you have purchased a restricted ticket.
    I understand that you wish to cancel your flight; however, I must inform you that
    restricted tickets are non-refundable and non-changeable as per the purchase agreement.
    This policy allows us to offer these tickets at a lower cost.While I cannot provide
    a full refund for your ticket, I can offer you some alternatives. You may be eligible
    for travel credits or a one-time rescheduling of your flight, subject to applicable
    fees. Would you like to explore these options further?'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 'Strategies and Updated Student Responses At Iteration 1
    Strategies: 1\. Acknowledge Customer’s Request: Acknowledge the
    customer’s request promptly and express willingness to assist. 2\. Provide Explanation:
    Offer clear and empathetic explanations of the situation, such as ticket restrictions.
    3\. Offer Alternatives: Demonstrate a proactive approach by providing alternative
    solutions or options to assist the customer. 4\. Avoid Blunt Statements: Avoid
    using blunt statements without context or empathy, as it can be dismissive to
    the customer. 5\. Avoid Lack of Explanation: Provide clear explanations to avoid
    leaving the customer confused or frustrated. 6\. Avoid Failure to Offer Solutions:
    Offer alternative solutions or options to prevent a negative customer experience.
    Student: Thank you for providing your booking reference number, ABC123\. After
    reviewing your reservation, I see that you have purchased a restricted ticket.
    I understand that you wish to cancel your flight; however, I must inform you that
    restricted tickets are non-refundable and non-changeable as per the purchase agreement.
    This policy allows us to offer these tickets at a lower cost.While I cannot provide
    a full refund for your ticket, I can offer you some alternatives. You may be eligible
    for travel credits or a one-time rescheduling of your flight, subject to applicable
    fees. Would you like to explore these options further?'
- en: This process continues as the strategies are updated over iterations and the
    student further improves its response, until it sounds “close enough” to the teacher’s
    response. The final strategies and the student response are shown below. These
    strategies are then paired with the scenario and stored in the library, and included
    in the prompt during deployment.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程持续进行，策略在迭代中不断更新，学生的回应进一步改善，直到它听起来“足够接近”教师的回应。最终的策略和学生回应如下所示。这些策略随后与场景配对并存储在库中，在部署时包含在提示中。
- en: 'Strategies and Student Responses at the Last Iteration
    Strategies:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一次迭代中的策略和学生回应 策略：
- en: '1\. Acknowledge Customer’s Request: Promptly acknowledge the customer’s request
    and express willingness to assist.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 确认客户请求：迅速确认客户的请求并表达愿意提供帮助。
- en: '2\. Provide Explanation: Offer clear and empathetic explanations of policies
    or restrictions that may affect the customer’s request.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 提供解释：清晰而富有同理心地解释可能影响客户请求的政策或限制。
- en: '3\. Offer Alternatives: Demonstrate a proactive approach by providing alternative
    solutions or options to assist the customer.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 提供替代方案：通过提供替代解决方案或选项，展示主动的处理方式，以帮助客户。
- en: '4\. Express Empathy and Understanding: Show empathy and understanding towards
    the customer’s situation.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 表达同理心和理解：对客户的情况表现出同理心和理解。
- en: '5\. Maintain Professional and Courteous Tone: Use polite language and maintain
    a professional tone throughout the conversation.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 保持专业和礼貌的语气：使用礼貌的语言，保持专业的语气贯穿整个对话。
- en: '6\. Avoid Blunt or Overly Formal Language: Refrain from using blunt or overly
    formal language that may sound robotic or impersonal.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 避免生硬或过于正式的语言：避免使用可能显得机械或不近人情的生硬或过于正式的语言。
- en: '7\. Avoid Lack of Empathy: Ensure empathy and acknowledgment of customer disappointment
    in responses.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 避免缺乏同理心：在回应中确保表现出同理心并承认客户的失望。
- en: '8\. Avoid Transactional Communication: Be proactive in offering options and
    solutions rather than being too transactional in responses. Student: AGENT: Thank
    you for providing your booking reference number, ABC123\. After reviewing your
    reservation, I see that you have purchased a restricted ticket. I regret to inform
    you that restricted tickets are non-refundable and non-changeable as per the purchase
    agreement. This policy allows us to offer these tickets at a lower cost. I understand
    that you need to cancel your flight, and I empathize with your situation. While
    I cannot provide a full refund for the restricted ticket, I can offer you some
    alternative solutions. One option is to consider converting the value of your
    ticket into travel credits that you can use for future bookings with our airline.
    Additionally, if you need to reschedule your flight, we can explore the possibility
    of a one-time rescheduling, subject to applicable fees. I hope these alternatives
    can be helpful to you. Please let me know how you would like to proceed, and I
    will assist you accordingly.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 避免交易性沟通：在回应中要主动提供选项和解决方案，而不是过于交易化。学生：代理：感谢您提供您的预订参考号，ABC123。经过审查您的预订，我发现您购买的是受限票。很遗憾地通知您，根据购买协议，受限票是不可退款和不可更改的。这一政策使我们能够以更低的价格提供这些票。我理解您需要取消航班，并且对您的情况表示同情。虽然我无法为受限票提供全额退款，但我可以为您提供一些替代方案。一个选项是考虑将您的票面值转换为旅行积分，以便您在未来的预订中使用。此外，如果您需要重新安排航班，我们可以探索一次性重新安排的可能性，但需支付相关费用。希望这些替代方案对您有所帮助。请告知您希望如何进行，我会相应地协助您。
