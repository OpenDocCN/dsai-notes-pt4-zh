- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 17:34:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:34:10
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更加可信赖和可解释的代码LLM的语法基础解释
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08983](https://ar5iv.labs.arxiv.org/html/2407.08983)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08983](https://ar5iv.labs.arxiv.org/html/2407.08983)
- en: \useunder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: \ul
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \ul
- en: David N. Palacio [danaderpalacio@wm.edu](mailto:danaderpalacio@wm.edu) [0001-6166-7595](https://orcid.org/0001-6166-7595
    "ORCID identifier") William & MaryWilliamsburgVirginiaUSA23185 ,  Daniel Rodriguez-Cardenas
    [0002-3238-1229](https://orcid.org/0002-3238-1229 "ORCID identifier") William
    & MaryWilliamsburgVirginiaUSA23185 [dhrodriguezcar@wm.edu](mailto:dhrodriguezcar@wm.edu)
    ,  Alejandro Velasco William & MaryWilliamsburgVirginiaUSA23185 [svelascodimate@wm.edu](mailto:svelascodimate@wm.edu)
    [0002-4829-1017](https://orcid.org/0002-4829-1017 "ORCID identifier") ,  Dipin
    Khati William & MaryWilliamsburgVirginiaUSA23188 [dkhati@wm.edu](mailto:dkhati@wm.edu)
    ,  Kevin Moran University of Central FloridaOrlandoFloridaUSA [kpmoran@ucf.edu](mailto:kpmoran@ucf.edu)
     and  Denys Poshyvanyk William & MaryWilliamsburgVirginiaUSA23185 [dposhyvanyk@wm.edu](mailto:dposhyvanyk@wm.edu)(2024)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: David N. Palacio [danaderpalacio@wm.edu](mailto:danaderpalacio@wm.edu) [0001-6166-7595](https://orcid.org/0001-6166-7595
    "ORCID identifier") 威廉与玛丽学院 威廉斯堡 弗吉尼亚州 美国 23185，Daniel Rodriguez-Cardenas [0002-3238-1229](https://orcid.org/0002-3238-1229
    "ORCID identifier") 威廉与玛丽学院 威廉斯堡 弗吉尼亚州 美国 23185 [dhrodriguezcar@wm.edu](mailto:dhrodriguezcar@wm.edu)，Alejandro
    Velasco 威廉与玛丽学院 威廉斯堡 弗吉尼亚州 美国 23185 [svelascodimate@wm.edu](mailto:svelascodimate@wm.edu)
    [0002-4829-1017](https://orcid.org/0002-4829-1017 "ORCID identifier")，Dipin Khati
    威廉与玛丽学院 威廉斯堡 弗吉尼亚州 美国 23188 [dkhati@wm.edu](mailto:dkhati@wm.edu)，Kevin Moran
    中佛罗里达大学 奥兰多 佛罗里达州 美国 [kpmoran@ucf.edu](mailto:kpmoran@ucf.edu) 和 Denys Poshyvanyk
    威廉与玛丽学院 威廉斯堡 弗吉尼亚州 美国 23185 [dposhyvanyk@wm.edu](mailto:dposhyvanyk@wm.edu)（2024）
- en: Abstract.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Trustworthiness and interpretability are inextricably linked concepts for LLMs.
    The more interpretable an LLM is, the more trustworthy it becomes. However, current
    techniques for interpreting LLMs when applied to code-related tasks largely focus
    on accuracy measurements, measures of how models react to change, or individual
    task performance instead of the fine-grained explanations needed at prediction
    time for greater interpretability, and hence trust. To improve upon this status
    quo, this paper introduces ASTrust, an interpretability method for LLMs of code
    that generates explanations grounded in the relationship between model confidence
    and syntactic structures of programming languages. ASTrust explains generated
    code in the context of syntax categories based on Abstract Syntax Trees and aids
    practitioners in understanding model predictions at both local (individual code
    snippets) and global (larger datasets of code) levels. By distributing and assigning
    model confidence scores to well-known syntactic structures that exist within ASTs,
    our approach moves beyond prior techniques that perform token-level confidence
    mapping by offering a view of model confidence that directly aligns with programming
    language concepts with which developers are familiar.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可信赖性和可解释性是LLM不可分割的概念。LLM越可解释，它的可信赖性也就越高。然而，目前解释LLM的技术在代码相关任务中主要关注于准确性测量、模型对变化的反应或个别任务的表现，而不是在预测时所需的细致解释，从而提高可解释性和可信赖性。为改善这种现状，本文介绍了ASTrust，一种针对代码LLM的可解释性方法，它生成基于模型信心与编程语言的语法结构之间关系的解释。ASTrust在抽象语法树的语法类别上下文中解释生成的代码，并帮助从业者理解模型预测在局部（个别代码片段）和全球（较大代码数据集）层面的表现。通过分配和赋予存在于AST中的知名语法结构模型信心分数，我们的方法超越了以前在token级别进行信心映射的技术，提供了与开发者熟悉的编程语言概念直接对齐的模型信心视图。
- en: To put ASTrust into practice, we developed an automated visualization that illustrates
    the aggregated model confidence scores superimposed on sequence, heat-map, and
    graph-based visuals of syntactic structures from ASTs. We examine both the practical
    benefit that ASTrust can provide through a data science study on 12 popular LLMs
    on a curated set of GitHub repos and the usefulness of ASTrust through a human
    study. Our findings illustrate that there is a causal connection between learning
    error and an LLM’s ability to predict different syntax categories according to
    ASTrust – illustrating that our approach can be used to interpret model effectiveness
    in the context of its syntactic categories. Finally, users generally found ASTrust’s
    visualizations useful in understanding the trustworthiness of model predictions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 ASTrust 付诸实践，我们开发了一种自动化可视化工具，展示了叠加在 AST 语法结构的序列、热图和图形视觉上的聚合模型置信度分数。我们通过对
    12 个流行 LLM 在精心挑选的 GitHub 仓库上的数据科学研究，和通过人类研究检验了 ASTrust 的实际效用。我们的发现表明，学习错误与 LLM
    根据 ASTrust 预测不同语法类别的能力之间存在因果关系——这表明我们的方法可以用于在其语法类别的背景下解释模型的有效性。最后，用户普遍认为 ASTrust
    的可视化工具对于理解模型预测的可信度非常有用。
- en: 'Causality, Interpretability, LLMs for Code, Trustworthiness^†^†copyright: acmlicensed^†^†journalyear:
    2024^†^†doi: XXXXXXX.XXXXXXX^†^†journal: TOSEM^†^†ccs: Software and its engineering Software
    development techniques'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '因果关系、可解释性、LLMs 用于代码、可信度^†^†版权：acmlicensed^†^†期刊年份：2024^†^†doi: XXXXXXX.XXXXXXX^†^†期刊：TOSEM^†^†ccs：软件及其工程
    软件开发技术'
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: The proliferation of open-source software projects and rapid scaling of transformer-based
    Large Language Models (LLMs) has catalyzed research leading to the increased effectiveness
    of automated Software Engineering (SE) tools. LLMs have demonstrated considerable
    proficiency across a diverse array of generative SE tasks (Chen et al., [2021](#bib.bib8);
    Watson et al., [2020a](#bib.bib63)), including, but not limited to, code completion
     (Raychev et al., [2014](#bib.bib51); Ciniselli et al., [2021](#bib.bib11)), program
    repair  (Chen et al., [2019](#bib.bib10); Ahmad et al., [[n. d.]](#bib.bib4)),
    and test case generation  (Watson et al., [2020b](#bib.bib65)). Current research
    in both designing LLMs for code and applying them to programming tasks typically
    makes use of existing benchmarks (e.g., CodeSearchNet (Husain et al., [2019](#bib.bib23)),
    or HumanEval (Chen et al., [[n. d.]](#bib.bib9))) and canonical metrics (by canonical,
    we refer to metrics that reflect an aggregate performance across many model predictions,
    for example, percentage accuracy). These canonical metrics have been adapted from
    the field of Natural Language Processing (NLP) to evaluate the performance of
    deep code generation models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 开源软件项目的普及和基于变换器的大型语言模型（LLMs）的快速扩展促进了研究，使自动化软件工程（SE）工具的效果得到了提升。LLMs 在各种生成性 SE
    任务中表现出相当的熟练度（Chen et al., [2021](#bib.bib8); Watson et al., [2020a](#bib.bib63)），包括但不限于代码完成（Raychev
    et al., [2014](#bib.bib51); Ciniselli et al., [2021](#bib.bib11)）、程序修复（Chen et
    al., [2019](#bib.bib10); Ahmad et al., [[n. d.]](#bib.bib4)）和测试用例生成（Watson et
    al., [2020b](#bib.bib65)）。目前，无论是在设计 LLMs 以进行代码编写，还是将其应用于编程任务，通常都会使用现有的基准（例如，CodeSearchNet（Husain
    et al., [2019](#bib.bib23)），或 HumanEval（Chen et al., [[n. d.]](#bib.bib9)））和规范的度量标准（所谓规范的度量标准是指反映多个模型预测的汇总性能的度量，例如，百分比准确度）。这些规范的度量标准已从自然语言处理（NLP）领域调整过来，用于评估深度代码生成模型的性能。
- en: Recent work has illustrated the limitations of benchmarks such as HumanEval (Liu
    et al., [2023b](#bib.bib31)) and there has been growing criticism of canonical
    metrics within the NLP community due to the lack of an interpretable context that
    allows for a deeper understanding of LLMs’ predictions or outputs (Molnar, [2019](#bib.bib41);
    Kim et al., [2018](#bib.bib28); Wan et al., [2022](#bib.bib62); Liu et al., [2023a](#bib.bib33);
    Doshi-Velez and Kim, [[n. d.]](#bib.bib14)). While code-specific metrics such
    as CodeBLEU (Ren et al., [2020](#bib.bib52)) may provide more robust aggregate
    pictures of model accuracy, they cannot provide the fine-grained context required
    to truly explain model predictions. The general lack of widely adopted interpretability
    or explainability tools is a barrier to the adoption of any deep learning model,
    and in particular LLMs of code, as practitioners are skeptical of models’ trustworthiness (Lo,
    [[n. d.]](#bib.bib35)). This deficiency largely stems from the fact that such
    benchmarks and canonical metrics are often aimed at evaluating functional correctness
    or standard performance of generated code at a glance. That is, the evaluation
    is reduced to a single aggregate metric in which relevant information related
    to individual predictions is obfuscated (Burnell et al., [[n. d.]](#bib.bib7)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究揭示了如HumanEval（Liu等，[2023b](#bib.bib31)）等基准的局限性，并且由于缺乏可解释的背景以便更深入地理解LLMs的预测或输出，NLP社区对经典度量标准的批评日益增加（Molnar，[2019](#bib.bib41)；Kim等，[2018](#bib.bib28)；Wan等，[2022](#bib.bib62)；Liu等，[2023a](#bib.bib33)；Doshi-Velez和Kim，[n.
    d.](#bib.bib14)）。虽然像CodeBLEU（Ren等，[2020](#bib.bib52)）这样的代码特定度量可能提供模型准确性的更全面的汇总图像，但它们无法提供真正解释模型预测所需的细致背景。广泛采用的可解释性或解释工具的普遍缺乏是深度学习模型，尤其是代码LLMs，采用的障碍，因为从业者对模型的可信度持怀疑态度（Lo，[n.
    d.](#bib.bib35)）。这种缺陷主要源于这些基准和经典度量标准通常旨在一目了然地评估生成代码的功能正确性或标准性能。也就是说，评估被简化为一个单一的汇总度量，其中与单个预测相关的相关信息被模糊化（Burnell等，[n.
    d.](#bib.bib7)）。
- en: '![Refer to caption](img/e9b62401dd2e07268cbd76bb4785212f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9b62401dd2e07268cbd76bb4785212f.png)'
- en: Figure 1. The Conceptual Framework of Syntax-Grounded Interpretability
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 语法基础可解释性的概念框架
- en: \Description
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Sytanx-Grounded Interpretability Overview
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 语法基础可解释性概述
- en: Methods for interpreting and trusting LLMs for code are inextricably linked.
    A trustworthy LLM for code requires some degree of interpretability of its predictions,
    such that model behavior can be understood at a fine-grained enough level to judge
    which parts of the output are correct or not, and why. The more interpretable
    an LLM for code is, the higher the confidence and trust in the deployment and
    use of the model (Doshi-Velez and Kim, [2018](#bib.bib16); Ji et al., [2024](#bib.bib24)).
    Notably, interpretability has been identified as an important component for enhancing
    trustworthiness in various studies  (Lundberg and Lee, [2017](#bib.bib36); Weller,
    [2019](#bib.bib66); Liao et al., [2020](#bib.bib30)). When evaluating trustworthiness,
    a clear understanding of how and why a model reaches specific predictions is critical.
    This transparency not only addresses challenges related to uncertainty and the
    potential for bugs or vulnerabilities but also plays a pivotal role in transforming
    a model perceived as untrustworthy into one deemed as reliable  (Ribeiro et al.,
    [2016b](#bib.bib54)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解释和信任代码的LLMs（大规模语言模型）是密不可分的。一个值得信赖的代码LLM需要一定程度的预测可解释性，使得模型行为可以被理解到足够细致的层面，以判断输出的哪些部分是正确的，哪些部分是错误的，以及原因。一个LLM对代码的可解释性越高，对模型部署和使用的信心和信任也越高（Doshi-Velez和Kim，[2018](#bib.bib16)；Ji等，[2024](#bib.bib24)）。值得注意的是，可解释性已被确定为提升可信度的重要组成部分（Lundberg和Lee，[2017](#bib.bib36)；Weller，[2019](#bib.bib66)；Liao等，[2020](#bib.bib30)）。在评估可信度时，清楚理解模型如何以及为何达到特定预测是至关重要的。这种透明度不仅解决了与不确定性以及潜在错误或漏洞相关的挑战，还在将一个被认为不可信的模型转变为一个被认为可靠的模型中发挥了关键作用（Ribeiro等，[2016b](#bib.bib54)）。
- en: 'We assert that a LLM for code is interpretable, and hence more trustworthy,
    if the reasoning behind its predictions is easy for a practitioner to comprehend.
    In other words, a useful interpretability technique must provide a conceptual
    mapping between descriptions of a model’s reasoning process and concepts inherently
    understood by programmers. In this paper, we explore the possibility of using
    a model’s confidence in its predictions as a proxy for describing its reasoning
    process and develop a technique, which we call ASTrust that automatically aligns
    and clusters model confidence measures with groups of tokens based on syntactic
    categories derived from Abstract Syntax Trees (ASTs) that we call Syntax Categories
    (SCs). This method enables a fine-grained understanding of the correctness of
    model predictions rooted in syntax-grounded explanations. As illustrated by the
    overview of our approach in Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations"),
    ASTrust enables two different granularities of interpretability, local explanations
    at the code snippet level, and global explanations for large collections of code.
    ASTrust also makes two main contributions: (i) a statistical technique for aligning
    and aggregating confidence scores to syntactic code structures of different granularities,
    and (ii) an automated technique for generating visualizations of these aligned
    confidence scores. At the local level these visualizations take the form of model
    confidence scores overlaid on both sequence and graph-based illustrations of ASTs
    and different syntactic structures. At the global level, these take the form of
    a heat map with confidence values clustered around higher-level syntactic categories.
    An example of the type of explanation that a developer may derive from ASTrust’s
    visualizations is as follows, “The model’s prediction of the type of the character
    parameter may be incorrect due to low confidence.”'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们断言，如果一个用于代码的LLM（大型语言模型）能够让从业者轻松理解其预测背后的推理过程，那么它就是可解释的，因此更值得信赖。换句话说，一个有用的可解释性技术必须在模型推理过程的描述和程序员固有理解的概念之间提供概念映射。在本文中，我们探索了将模型对其预测的信心作为描述其推理过程的代理的可能性，并开发了一种技术，我们称之为ASTrust，它自动将模型信心度量与基于从抽象语法树（ASTs）中派生的句法类别（我们称之为句法类别（SCs））的令牌组进行对齐和聚类。这种方法使我们能够基于语法根植的解释，细致地理解模型预测的正确性。如图[1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Towards More Trustworthy and Interpretable LLMs
    for Code through Syntax-Grounded Explanations")所示，ASTrust实现了两种不同粒度的可解释性：代码片段级别的局部解释和大规模代码集合的全局解释。ASTrust还做出了两个主要贡献：（i）一种将信心分数对齐并聚合到不同粒度的句法代码结构的统计技术，以及（ii）一种自动生成这些对齐信心分数可视化的技术。在局部层面，这些可视化形式为覆盖在序列和基于图的ASTs和不同句法结构上的模型信心分数。在全局层面，这些可视化形式为围绕更高级句法类别聚类的信心值热图。开发者可能从ASTrust的可视化中得到的解释示例如下：“模型对字符参数类型的预测可能由于低信心而不正确。”
- en: Grounding explanations of model confidence in code syntax provides an informative
    context to practitioners allowing for interpretability. This is due to the fact
    that code semantics and syntax are tightly coupled. That is, descriptions of code
    meaning, or semantics, are often grounded in syntax. For instance, consider the
    following example of a developer describing program behavior in numpy in which
    the description of functionality is grounded in terms of data structures, “Convert
    an array representing the coefficients of a Legendre series,”¹¹1[https://github.com/numpy/numpy/blob/main/numpy/polynomial/legendre.py#L152C5-L152C72](https://github.com/numpy/numpy/blob/main/numpy/polynomial/legendre.py#L152C5-L152C72)
    where the underlined word refers explicitly to the syntactic category of a data
    structure. One may ask “why not ground explanations in code semantics directly?”
    However, such semantic-based grounding is difficult to achieve, as it requires
    reasoning among model confidence, input code, predicted code, and widely variable
    interpretations of code meaning – leading to the potential for incorrect explanations
    that would undermine a technique ultimately meant to build trust. However, as
    we illustrate in this paper, it is possible to directly map measures of model
    confidence to different syntactic categories of code, providing a statistically
    sound method of understanding the potential correctness of model predictions rooted
    in concepts that developers can easily understand.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型信心的解释与代码语法关联起来，为从业者提供了一个信息丰富的背景，从而提高了解释性。这是因为代码语义和语法紧密相关。也就是说，代码含义的描述，或语义，通常是基于语法的。例如，考虑下面的一个开发者描述numpy程序行为的例子，其中功能的描述是以数据结构为基础的，“转换一个表示Legendre级数系数的数组，”¹¹1[https://github.com/numpy/numpy/blob/main/numpy/polynomial/legendre.py#L152C5-L152C72](https://github.com/numpy/numpy/blob/main/numpy/polynomial/legendre.py#L152C5-L152C72)
    其中下划线的词明确指代数据结构的语法类别。有人可能会问，“为什么不直接将解释与代码语义相关联？”然而，这种基于语义的基础很难实现，因为它需要在模型信心、输入代码、预测代码以及广泛变化的代码含义解释之间进行推理——这可能导致不正确的解释，从而削弱了最终旨在建立信任的技术。然而，正如我们在本文中所示，可以将模型信心的度量直接映射到代码的不同语法类别，从而提供一种统计上可靠的方法，以理解基于开发人员容易理解的概念的模型预测潜在正确性。
- en: We explore the practical benefit of ASTrust through a large-scale data science
    study examining the relationship between model effectiveness and global explanations
    and evaluate the usefulness of our method through a human study targeted at local
    explanations of code snippets using ASTrust’s different visualizations. The context
    of our empirical evaluation includes 12 popular LLMs for code and a curated set
    of code taken from recent commits of the 200 most popular Python projects on GitHub.
    Using a carefully crafted causal inference study, our analysis illustrates causal
    connections between learning error and a model’s ability to predict different
    syntax categories according to ASTrust – showing that our approach can be used
    to interpret model effectiveness in the context of its syntactic categories. Our
    human study included 27 participants who examined code snippets completed by GPT
    3 and one of four of ASTrust’s visualization techniques for local explanations.
    Our results illustrate that developers generally found ASTrust and its visualizations
    useful in understanding model predictions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一项大规模的数据科学研究探讨了ASTrust的实际好处，该研究考察了模型有效性与全局解释之间的关系，并通过针对ASTrust不同可视化技术的代码片段的局部解释的人类研究评估了我们方法的有效性。我们的实证评估背景包括12种流行的代码LLM和从GitHub上200个最受欢迎的Python项目的最近提交中挑选的一组代码。通过精心设计的因果推断研究，我们的分析展示了学习错误与模型根据ASTrust预测不同语法类别的能力之间的因果关系——这表明我们的方法可以用于解释模型在其语法类别背景下的有效性。我们的人类研究包括27名参与者，他们检查了由GPT
    3生成的代码片段以及ASTrust四种可视化技术中的一种进行局部解释。我们的结果表明，开发人员普遍认为ASTrust及其可视化工具在理解模型预测方面非常有用。
- en: The results of our studies illustrate that mapping token-level predictions of
    LLMs to segregated Syntax Categories are of considerable practical benefit to
    SE researchers and practitioners because it allows them to interpret and trust
    parts of generated code based on the structural functionality, which contextualizes
    model predictions beyond the canonical evaluation (i.e., measuring intrinsic and
    extrinsic metrics). We hope other researchers build upon our method to create
    new types of interpretability techniques in the future, and we provide an online
    appendix with the code for ASTrust, and our data and experimental infrastructure
    to facilitate replication (Palacio et al., [2024](#bib.bib45)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的结果表明，将LLMs的令牌级预测映射到分隔的语法类别对SE研究人员和从业者具有相当大的实际好处，因为这使他们能够基于结构功能解释和信任生成的代码的部分，从而将模型预测的上下文扩展到超越经典评估（即，衡量内在和外在指标）。我们希望其他研究人员在此基础上开发新的解释性技术，并提供了ASTrust的代码、数据和实验基础设施的在线附录，以便于复制（Palacio等，[2024](#bib.bib45)）。
- en: 2\. Background & Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景与相关工作
- en: In this section, we present background on interpretability and trustworthiness
    as complementary terms for generating syntax-grounded post hoc (e.g., generated
    after training) explanations for LLMs of code.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了解释性和可信度的背景，作为生成针对代码的基于语法的事后（例如，在训练后生成）解释的互补术语。
- en: Interpretability. The brittleness of LLMs can be formulated as an incompleteness
    in problem formalization  (Doshi-Velez and Kim, [2017](#bib.bib15)), which means
    that it is insufficient that models only infer predictions for certain tasks (the
    what?). The models must also explain how they arrive at such predictions (the
    why?). To mitigate such incompleteness in problem formalization, the field of
    interpretability has risen to encompass techniques and methods that aim to solve
    the why question. Although authors in this field generally use the terms explainability
    and interpretability interchangeably, these definitions are inconsistent throughout
    the literature (Flora et al., [[n. d.]](#bib.bib17)). We distinguish between the
    terms to avoid confusion with the purposes of our approach. We will use explainability
    for methods whose goal is to understand how a LLM operates and comes to a decision
    by exploring inner mechanisms or layers. Conversely, we will use interpretability
    for methods that define conceptual mapping mechanisms whose goal is to contextualize
    models’ predictions by associating them with an understandable concept, which
    in this paper is the syntax of programming languages.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性。LLMs的脆弱性可以被表述为问题形式化中的不完整性（Doshi-Velez和Kim，[2017](#bib.bib15)），这意味着模型仅仅推断某些任务的预测（即“什么？”）是不够的。模型还必须解释它们如何得出这些预测（即“为什么？”）。为了缓解问题形式化中的这种不完整性，解释性领域已兴起，涵盖了旨在解决“为什么”问题的技术和方法。尽管这一领域的作者通常将解释性和可解释性这两个术语互换使用，但这些定义在文献中是不一致的（Flora等，[[n.
    d.]](#bib.bib17)）。为了避免与我们方法目的的混淆，我们区分这两个术语。我们将使用“解释性”来指代旨在通过探索内部机制或层来理解LLM如何运作并做出决策的方法。相反，我们将使用“可解释性”来指代定义概念映射机制的方法，其目标是通过将模型的预测与易于理解的概念关联来将其上下文化，在本文中即编程语言的语法。
- en: Related Work on Interpretability in NLP. There are existing techniques in both
    natural language processing (NLP) and SE literature focused on interpretability,
    including LIME (Ribeiro et al., [2016a](#bib.bib53)), Kernel SHAP (Lundberg and
    Lee, [2017](#bib.bib36)), Integrated Gradient (Sundararajan et al., [2017](#bib.bib59))
    and Contextual Decomposition (Murdoch et al., [2018](#bib.bib43)). These techniques
    generally try to approximate an interpretable model that either attempts to attribute
    meaning to hidden representations of neural networks, or illustrate the relationship
    between input features and model performance. However, we argue that such techniques
    are difficult to make practical in the context of LLMs for code, given the lack
    of conceptual mappings explained earlier. However, the most closely related interpretability
    technique to ASTrust, and one of the only to have adapted to LLMs of code is that
    of probing which is a supervised analysis to determine which type of parameters
    (e.g., input code snippets, tokenization process, number of hidden layers, and
    model size) influence the learning process in ML models (Troshin and Chirkova,
    [2022](#bib.bib61)). Probing aims to assess whether hidden representations of
    LLMs encode specific linguistic properties such as syntactic structures of programming
    languages. Given our generated visualizations, there may be an inclination to
    characterize ASTrust as a probing technique. However, it is important to note
    that ASTrust is focused on estimating the correctness of predicted syntactic code
    elements rather than mapping meaning to internal model representations of data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）领域中的可解释性相关工作。现有技术在自然语言处理（NLP）和软件工程（SE）文献中都集中于可解释性，包括LIME（Ribeiro等，[2016a](#bib.bib53)），Kernel
    SHAP（Lundberg和Lee，[2017](#bib.bib36)），Integrated Gradient（Sundararajan等，[2017](#bib.bib59)）和Contextual
    Decomposition（Murdoch等，[2018](#bib.bib43)）。这些技术通常试图近似一个可解释的模型，该模型要么尝试为神经网络的隐藏表示赋予意义，要么阐明输入特征与模型性能之间的关系。然而，我们认为这些技术在处理代码的LLMs（大型语言模型）时难以实际应用，鉴于之前提到的缺乏概念映射。然而，与ASTrust最相关的可解释性技术之一，并且是为代码的LLMs适应的唯一技术之一的是探测（probing），它是一种监督分析方法，用于确定哪些类型的参数（例如，输入代码片段、分词过程、隐藏层数量和模型大小）影响机器学习模型的学习过程（Troshin和Chirkova，[2022](#bib.bib61)）。探测旨在评估LLMs的隐藏表示是否编码了编程语言的特定语言学特性，如句法结构。鉴于我们生成的可视化，可能会倾向于将ASTrust归类为一种探测技术。然而，需要注意的是，ASTrust的重点是估计预测的句法代码元素的正确性，而不是将意义映射到数据的内部模型表示。
- en: 'Related Work on Interpretability in SE. In the realm of SE research, prior
    work has taken two major directions: (i) techniques for task-specific explanations (Fu
    et al., [2023](#bib.bib18); Liu et al., [2022](#bib.bib32); Pornprasit et al.,
    [2021](#bib.bib50)), and (ii) empirical interpretability studies using existing
    NLP techniques (Liu et al., [2024](#bib.bib34); Tantithamthavorn et al., [2023](#bib.bib60);
    Mohammadkhani et al., [2023a](#bib.bib39)). Previous authors have proposed techniques
    for explaining specific tasks including vulnerability explanation (Fu et al.,
    [2023](#bib.bib18)), vulnerability prediction for Android (Liu et al., [2022](#bib.bib32)),
    and defect prediction models (Pornprasit et al., [2021](#bib.bib50)). More recently
    Liu et al. conducted large empirical study using existing explainability techniques
    for global explanations of code to better understand generative language models
    of code (Liu et al., [2024](#bib.bib34)). Mohammadkhani et al. conducted a study
    using LLM’s attention mechanism to interpret their performance on generating code.
    Finally, one paper that proposed a code-specific interpretability technique is
    that of Cito et al. (Cito et al., [2022](#bib.bib12)) who formulated a method
    to generate explanations using counterfactual reasoning of models. Our work on
    ASTrust complements this body of past work by developing a new, generally applicable
    interpretability method that can be applied to both local and global explanations
    of code, which no prior study or technique has done.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SE领域的可解释性相关工作。在SE研究领域，之前的工作主要有两个方向：（i）针对特定任务的解释技术（Fu et al., [2023](#bib.bib18);
    Liu et al., [2022](#bib.bib32); Pornprasit et al., [2021](#bib.bib50)），以及（ii）使用现有NLP技术的实证可解释性研究（Liu
    et al., [2024](#bib.bib34); Tantithamthavorn et al., [2023](#bib.bib60); Mohammadkhani
    et al., [2023a](#bib.bib39)）。之前的作者提出了包括漏洞解释（Fu et al., [2023](#bib.bib18)）、Android漏洞预测（Liu
    et al., [2022](#bib.bib32)）和缺陷预测模型（Pornprasit et al., [2021](#bib.bib50)）在内的特定任务解释技术。最近，Liu
    et al.使用现有的解释技术进行了大规模实证研究，以获取代码的全局解释，从而更好地理解生成语言模型（Liu et al., [2024](#bib.bib34)）。Mohammadkhani
    et al.进行了使用LLM注意机制解释其生成代码性能的研究。最后，提出代码特定可解释性技术的论文是Cito et al.（Cito et al., [2022](#bib.bib12)），他们制定了一种使用模型的反事实推理生成解释的方法。我们的ASTrust工作通过开发一种新的、通用的可解释性方法，补充了这些过去的工作，该方法可用于代码的局部和全局解释，而这一点在以前的研究或技术中未曾实现。
- en: Trustworthiness. This research is inspired by definitions of trust from automated
    systems, SE, and NLP. In automated systems, trust is defined as “the attitude
    that an agent will help achieve an individual’s goal in a situation characterized
    by uncertainty and vulnerability” (Lee and See, [2004](#bib.bib29)). Bianco et
    al. define software trust as the degree of confidence when the software meets
    certain requirements (del Bianco et al., [2011](#bib.bib13)). In NLP, Sun et al.
    argue that LLMs must appropriately reflect truthfulness, safety, fairness, robustness,
    privacy, machine ethics, transparency, and accountability for them to be trustworthy (Sun
    et al., [2024](#bib.bib58)). We define trust as the confidence that practitioners
    and researchers have in LLMs’ code prediction, anticipating that these predictions
    will effectively align with their intended goals. Trustworthiness in LLMs implies
    a sense of interpretability in a given LLM’s performance, instilling confidence
    among practitioners in their abilities to perform code-related tasks. To the best
    of our knowledge, no paper proposes a concrete definition of trust based on interpretability
    within the SE research community. Yet, several researchers have called for the
    importance of trustworthiness in LLMs for code (Lo, [[n. d.]](#bib.bib35); Spiess
    et al., [2024](#bib.bib57)). In our work we present a concrete definition of trustworthiness,
    highlight its importance, and show how syntax-grounded explanations such as ASTrust
    contribute to more trustworthy LLMs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可信度。本研究的灵感来源于自动化系统、SE（软件工程）和NLP（自然语言处理）中的信任定义。在自动化系统中，信任被定义为“在不确定和脆弱的情况下，代理将帮助实现个人目标的态度”（Lee
    和 See，[2004](#bib.bib29)）。Bianco 等人将软件信任定义为在软件满足某些要求时的信心程度（del Bianco 等人，[2011](#bib.bib13)）。在NLP中，Sun
    等人认为 LLM 必须适当反映真实性、安全性、公平性、鲁棒性、隐私、机器伦理、透明性和问责制，才能被认为是值得信赖的（Sun 等人，[2024](#bib.bib58)）。我们将信任定义为从业者和研究人员对
    LLM 代码预测的信心，预计这些预测将有效地与他们的目标对齐。LLM 的可信度意味着对特定 LLM 性能的可解释性，使从业者对其执行代码相关任务的能力充满信心。据我们了解，目前没有文献提出基于可解释性的信任的具体定义。然而，几位研究人员呼吁关注
    LLM 代码中的可信度（Lo，[[n. d.]](#bib.bib35)；Spiess 等人，[2024](#bib.bib57)）。在我们的工作中，我们提出了一个具体的可信度定义，强调其重要性，并展示了像
    ASTrust 这样的语法基础解释如何有助于提高 LLM 的可信度。
- en: 3\. Syntax-Grounded Explanations
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 语法基础解释
- en: At a high level, ASTrust queries a LLM for probabilities per token, estimates
    the median across tokens that are part of one AST node, and presents those averages
    as confidence performance values segregated by hand-assigned syntax categories.
    We also refer to this confidence performance as ASTrust Interpretability Performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，ASTrust 查询 LLM 的每个标记的概率，估算一个 AST 节点中所有标记的中位数，并将这些平均值作为按手动分配的语法类别划分的置信度性能值。我们还将这种置信度性能称为
    ASTrust 可解释性性能。
- en: 'ASTrust consists of four steps depicted in Fig. [1](#S1.F1 "Figure 1 ‣ 1\.
    Introduction ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations"). In step 1,
    a code snippet for local or a testbed for global explanations is the starting
    point of the interpretability process. Each sequence within the snippet or the
    testbed is processed by a tokenizer (e.g., Byte-Pair Encoding (BPE)). In step
    2, the tokenizer
    sets a vocabulary we named token set. Once code sequences are preprocessed, an
    LLM under analysis generates token-level predictions (TLP) for each position in
    a sequence. Next, in step 3,
    the generated token-level predictions are aligned with the associated Abstract
    Syntax Tree (AST) terminal nodes. Terminal nodes only store TLP, while non-terminal
    nodes hierarchically store clustered and aggregated TLP. Terminal and non-terminal
    nodes comprise the subcategory set. For example, consider if_ BPE token from the
    token set. This token is aligned with the ‘if’ terminal AST node while clustered
    in the ‘if_statement’ non-terminal node. Finally, in step 4, ten syntax
    categories are proposed to summarize a model’s predictions. Syntax Categories
    aim to group the sub-categories into higher-level, more human-understandable categories.
    These syntax categories are a fixed category set that comprises more interpretable
    elements and include:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ASTrust 包含四个步骤，如图 [1](#S1.F1 "图 1 ‣ 1. 引言 ‣ 通过语法基础解释实现更值得信赖和可解释的 LLM") 所示。在第
    1 步骤中，本地代码片段或全球解释的测试平台是可解释性过程的起点。代码片段或测试平台中的每个序列都由分词器（例如，字节对编码（BPE））处理。在第
    2 步骤中，分词器设置了我们称之为令牌集的词汇表。一旦代码序列被预处理，待分析的
    LLM 会为序列中的每个位置生成令牌级预测（TLP）。接下来，在第 3
    步骤中，生成的令牌级预测与关联的抽象语法树（AST）终端节点对齐。终端节点仅存储 TLP，而非终端节点分层存储聚类和汇总的 TLP。终端节点和非终端节点组成子类别集。例如，考虑令牌集中的
    if_ BPE 令牌。该令牌与 ‘if’ 终端 AST 节点对齐，同时聚类于 ‘if_statement’ 非终端节点。最后，在第 4 步骤中，提出了十种语法类别以总结模型的预测。语法类别旨在将子类别归为更高级、更加易于理解的类别。这些语法类别是一个固定的类别集，包含更具可解释性的元素，包括：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Decisions
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 决策
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data Structures
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据结构
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Exceptions
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 异常
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Iterations
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Functional Programming
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 函数式编程
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Operators
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运算符
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Testing
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scope
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 范围
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data Types
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据类型
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Natural
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自然
- en: Language
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言
- en: For instance, the sub-categories ‘if_statement’ and ‘if’ are both clustered
    into one syntax category Decisions. In the end, ASTrust generates an averaged
    score per category for global explanations and an AST tree visualization with
    stored scores at each node for local explanations. In essence, we propose that
    syntax elements contain semantic information that contextualizes predicted probabilities.
    However, this semantic information varies across the granularity of these elements.
    We can claim, for example, that token-level elements carry less interpretable
    information than category-level elements.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，子类别‘if_statement’ 和 ‘if’ 都被聚集到一个语法类别决策中。最终，ASTrust 为全局解释生成每个类别的平均分数，并为局部解释生成具有每个节点存储分数的
    AST 树可视化。实质上，我们提出语法元素包含语义信息，这些信息对预测概率进行上下文化。然而，这些语义信息在这些元素的粒度上有所不同。例如，我们可以声称，标记级别的元素所携带的信息不如类别级别的元素那样可解释。
- en: ASTrust produces post-hoc local and global explanations of generated code snippets.
    A local explanation intends to interpret the generation of a code snippet by decomposing
    it into AST elements. Conversely, a global explanation uses a set of generated
    snippets (or existing benchmark dataset) to interpret a given model holistically
    into Syntax Categories (SCs). The following sub-sections introduce the building
    blocks of syntax-grounded explanations. Sec. [3.1](#S3.SS1 "3.1\. Interpretable
    Syntax Sets ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy and
    Interpretable LLMs for Code through Syntax-Grounded Explanations") defines the
    interpretable sets (e.g., Token, Subcategory, and Category) that contain the syntax
    elements employed for the interpretability process. Sec. [3.2](#S3.SS2 "3.2\.
    Alignment and Clustering Formalism ‣ 3\. Syntax-Grounded Explanations ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    formalizes two function interactions that communicate previously interpretable
    sets. Such communication consists of aligning and clustering elements from code
    tokens to syntax categories. Finally, Sec. [3.3](#S3.SS3 "3.3\. Post Hoc Local
    and Global Explanations ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations") shows the
    process of generating local and global explanations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ASTrust 提供生成代码片段的事后局部和全局解释。局部解释旨在通过将代码片段分解为 AST 元素来解释代码片段的生成。相反，全局解释使用一组生成的片段（或现有的基准数据集）将给定模型从整体上解释为语法类别（SCs）。以下子部分介绍了语法基础解释的构建模块。第
    [3.1](#S3.SS1 "3.1\. Interpretable Syntax Sets ‣ 3\. Syntax-Grounded Explanations
    ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations") 节定义了用于解释过程的可解释集合（例如，Token、子类别和类别）。第 [3.2](#S3.SS2 "3.2\. Alignment
    and Clustering Formalism ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations") 节形式化了两个函数交互，这些交互传递先前可解释的集合。这种通信包括对代码标记到语法类别的元素进行对齐和聚类。最后，第
    [3.3](#S3.SS3 "3.3\. Post Hoc Local and Global Explanations ‣ 3\. Syntax-Grounded
    Explanations ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations") 节展示了生成局部和全局解释的过程。
- en: 3.1\. Interpretable Syntax Sets
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 可解释的语法集合
- en: Token Set $\mathcal{V}$. Although ASTrust was designed to be compatible with
    different types of LLMs, this paper concentrated on Decoder-Only models due to
    their auto-regressive capacity to generate code (Xu et al., [2022](#bib.bib68))
    by preserving long-range dependencies (Karpathy et al., [2015](#bib.bib27)). A
    Decoder-only model can be employed as a generative process such as any token $w_{i}$
    is being predicted by $\hat{w_{i}}\backsim P(w_{i}|w_{2 depicts the
    alignment of try_ token to the terminal $\lambda$ ‘try’ node. Note that the alignment
    ignores the character ”_” from try_. A tokenizer may produce a sequence in which
    each token does not necessarily match one-to-one with a terminal $\lambda$ node,
    e.g., Fig. [2](#S3.F2 "Figure 2 ‣ 3.2\. Alignment and Clustering Formalism ‣ 3\.
    Syntax-Grounded Explanations ‣ Towards More Trustworthy and Interpretable LLMs
    for Code through Syntax-Grounded Explanations")- 3
    illustrates the tokens flo_ and at are aligned with the $\lambda$ node ‘float’.
    Formally, $\delta(flo\_,at)\to[float]$ in a many-to-one interaction. Consequently,
    the alignment between code tokens and terminal nodes is certainly many-to-one,
    including one-to-one, but never one-to-many or many-to-many.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐交互。图 [2](#S3.F2 "图 2 ‣ 3.2\. 对齐与聚类形式主义 ‣ 3\. 语法基础解释 ‣ 通过语法基础解释实现更值得信赖和可解释的代码
    LLM") 说明了如何将 AST 中的终端节点 $\delta$ 对齐到其相应的代码标记 $w_{i}$。这个对齐过程从将输入片段 $s$ 分解为标记 $w_{<=i}\in\mathcal{V}$
    开始。例如，图 [2](#S3.F2 "图 2 ‣ 3.2\. 对齐与聚类形式主义 ‣ 3\. 语法基础解释 ‣ 通过语法基础解释实现更值得信赖和可解释的代码
    LLM") - 2
    展示了 try_ 标记与终端 $\lambda$ ‘try’ 节点的对齐。注意，对齐过程忽略了 try_ 中的字符 ”_”。一个分词器可能生成一个序列，其中每个标记不一定与终端
    $\lambda$ 节点一一对应，例如，图 [2](#S3.F2 "图 2 ‣ 3.2\. 对齐与聚类形式主义 ‣ 3\. 语法基础解释 ‣ 通过语法基础解释实现更值得信赖和可解释的代码
    LLM") - 3
    说明标记 flo_ 和 at 对齐到 $\lambda$ 节点 ‘float’。形式上，$\delta(flo\_,at)\to[float]$ 是多对一的交互。因此，代码标记与终端节点之间的对齐肯定是多对一的，包括一对一的情况，但绝不是一对多或多对多。
- en: Definition 0.
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 0。
- en: Alignment ($\delta$). The function $\delta:w_{<=i}\to\vec{\lambda}$ where $w_{<=i}$
    corresponds to a code sub-sequence whose tokens are many-to-one associated to
    the corresponding terminal node vector $\vec{\lambda}$ of syntax subcategories.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐 ($\delta$)。函数 $\delta:w_{<=i}\to\vec{\lambda}$ 其中 $w_{<=i}$ 对应于代码子序列，其标记与语法子类别的对应终端节点向量
    $\vec{\lambda}$ 具有多对一的关联。
- en: Clustering Interaction. A clustering function $\theta$ estimates the confidence
    performance of $\lambda$ and $\alpha$ nodes (subcategories) from an AST by hierarchically
    aggregating the Token-Level Predictions (TLP) to a Category $c\in\mathcal{C}$.
    Once the tokens are aligned with their corresponding nodes using $\delta$ from
    Def.[3.1](#S3.Thmtheorem1 "Definition 0\. ‣ 3.2\. Alignment and Clustering Formalism
    ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations"), ASTrust clusters them into
    their respective category or non-terminal $\alpha$ node according to the AST representation.
    Some terminal $\lambda$ nodes can directly be aggregated into a category without
    considering intermediate non-terminal $\alpha$ nodes. A terminal $\lambda$ node
    can initiate a block sentence (i.e., a category) and a block sequence parameters
    (i.e., non-terminal if_statement node). For instance, Fig. [2](#S3.F2 "Figure
    2 ‣ 3.2\. Alignment and Clustering Formalism ‣ 3\. Syntax-Grounded Explanations
    ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations")- 1
    depicts the terminal $\lambda$ ‘if’ node aggregated into the Decisions category
    and also starts the non-terminal $\alpha$ ‘if_statement’ node. To estimate the
    confidence performance, we traverse the entire AST and aggregate the TLP probabilities
    of respective tokens.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类交互。聚类函数 $\theta$ 通过逐级聚合 Token-Level Predictions (TLP) 到一个类别 $c\in\mathcal{C}$，估计
    $\lambda$ 和 $\alpha$ 节点（子类别）的置信度表现。一旦 token 使用 Def.[3.1](#S3.Thmtheorem1 "定义 0\.
    ‣ 3.2\. 对齐与聚类形式主义 ‣ 3\. 语法基础解释 ‣ 通过语法基础解释走向更可信和可解释的代码 LLM") 中的 $\delta$ 与其对应的节点对齐，ASTrust
    会根据 AST 表示将它们聚类到相应的类别或非终端 $\alpha$ 节点中。一些终端 $\lambda$ 节点可以直接聚合到一个类别中，而无需考虑中间非终端
    $\alpha$ 节点。一个终端 $\lambda$ 节点可以启动一个块句子（即类别）和一个块序列参数（即非终端 if_statement 节点）。例如，图
    [2](#S3.F2 "图 2 ‣ 3.2\. 对齐与聚类形式主义 ‣ 3\. 语法基础解释 ‣ 通过语法基础解释走向更可信和可解释的代码 LLM")- 1 描述了终端 $\lambda$
    ‘if’ 节点被聚合到 Decisions 类别中，并且还启动了非终端 $\alpha$ ‘if_statement’ 节点。为了估计置信度表现，我们遍历整个
    AST 并聚合相应 token 的 TLP 概率。
- en: 'The $\theta$ function can adopt average, median, or max aggregations depending
    on the user configuration. Fig. [3](#S3.F3 "Figure 3 ‣ 3.3\. Post Hoc Local and
    Global Explanations ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations") shows the
    clustering function applied to a concrete code generation sample. This application
    constitutes a local post hoc explanation: the parent node ‘parameters’ has a $0.23$
    associated confidence performance. This parent node average was aggregated with
    its terminal values: ‘(’ with $0.07$, ‘identifier’ with $0.4$ and $0.1$, ‘,’ with
    $0.5$, and ‘)’ with $0.1$. Formally, $\theta(\vec{\lambda}=[0.07,0.4,0.1,0.5,0.1])\to[(parameters,0.23)]$.
    If a sample snippet does not contain any particular syntax element (i.e., token,
    subcategory, or category), such an element is therefore never considered for clustering.
    An absent syntax element is reported as a null value to avoid biased syntax-grounded
    explanations.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: $\theta$ 函数可以根据用户配置采用平均值、中位数或最大值聚合。图 [3](#S3.F3 "图 3 ‣ 3.3\. 事后局部和全局解释 ‣ 3\.
    语法基础解释 ‣ 通过语法基础解释走向更可信和可解释的代码 LLM") 显示了应用于具体代码生成样本的聚类函数。这个应用构成了局部事后解释：父节点‘parameters’的关联置信度表现为
    $0.23$。该父节点平均值与其终端值进行了聚合：‘(’ 的值为 $0.07$，‘identifier’ 的值为 $0.4$ 和 $0.1$，‘,’ 的值为
    $0.5$，以及‘)’ 的值为 $0.1$。形式上，$\theta(\vec{\lambda}=[0.07,0.4,0.1,0.5,0.1])\to[(parameters,0.23)]$。如果样本片段不包含任何特定的语法元素（即
    token、子类别或类别），则该元素将不会被考虑进行聚类。缺失的语法元素会被报告为 null 值，以避免偏向的语法基础解释。
- en: Definition 0.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 0。
- en: Clustering ($\theta$). The function $\theta:\vec{\lambda}\to median(\vec{n})$
    where $\vec{\lambda}$ is the resulting vector of a sub-sequence and $\vec{n}$
    is the vector of hierarchical associated non-terminal nodes for each terminal
    $\lambda$. The vector $\vec{n}$, therefore, contains the TLP of non-terminal and
    the corresponding terminal nodes²²2In our study, we set the aggregation $\theta:N\to
    median(\hat{w}_{<=i})$ for a subset of tokens $w_{<=i}$..
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类（$\theta$）。函数 $\theta:\vec{\lambda}\to median(\vec{n})$ 其中 $\vec{\lambda}$
    是子序列的结果向量，$\vec{n}$ 是每个终端 $\lambda$ 的层次关联非终端节点的向量。因此，向量 $\vec{n}$ 包含非终端的 TLP 以及对应的终端节点²²在我们的研究中，我们为一组标记
    $w_{<=i}$ 设置了聚合 $\theta:N\to median(\hat{w}_{<=i})$。.
- en: 3.3\. Post Hoc Local and Global Explanations
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 事后局部和全局解释
- en: LLMs are more understandable when they reflect human knowledge (Kim et al.,
    [2018](#bib.bib28)). One way of determining whether an LLM trained on code reflects
    human knowledge is testing it to see whether or not it operates similar to how
    a developer would estimate the prediction of a sequence (Palacio et al., [2023](#bib.bib46)).
    ASTrust can adopt the form of a post-hoc local or a global explanation to make
    code predictions humanly understandable.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当LLMs反映人类知识时，它们更容易理解（Kim 等， [2018](#bib.bib28)）。确定LLM是否反映了人类知识的一种方法是测试它是否以类似于开发者预测序列的方式进行操作（Palacio
    等， [2023](#bib.bib46)）。ASTrust 可以采用事后局部或全局解释的形式，使代码预测变得更加易于理解。
- en: '![Refer to caption](img/9cde97c356ae587de4f9ab32bf927fdc.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9cde97c356ae587de4f9ab32bf927fdc.png)'
- en: Figure 3. Post Hoc Local Explanation. A snippet is decomposed into code tokens.
    The highest annotated probabilities (i.e., best predictions) are in blue.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 事后局部解释。代码片段被分解为代码标记。最高标注的概率（即最佳预测）以蓝色显示。
- en: \Description
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: Post hoc local explanation
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 事后局部解释
- en: ASTrust for local interpretability allows us to interpret a single snippet $s$
    by generating a visual explanation based on an Abstract Syntax Tree (AST) as illustrated
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3.3\. Post Hoc Local and Global Explanations ‣
    3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations"). A practitioner can explain
    the code predictions observing the probabilities associated with each element
    on the AST. In other words, we use $\theta$ from Def. [3.2](#S3.Thmtheorem2 "Definition
    0\. ‣ 3.2\. Alignment and Clustering Formalism ‣ 3\. Syntax-Grounded Explanations
    ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations") to cluster around AST nodes across all levels (i.e., AST probability
    annotations). Therefore, the syntax-grounded local explanation comprises a conceptual
    mapping from the code prediction to a terminal and non-terminal node (or sub-categories).
    Fig. [3](#S3.F3 "Figure 3 ‣ 3.3\. Post Hoc Local and Global Explanations ‣ 3\.
    Syntax-Grounded Explanations ‣ Towards More Trustworthy and Interpretable LLMs
    for Code through Syntax-Grounded Explanations") is a visual representation of
    the conceptual mapping using code predictions by gpt-3 [1.3B] model. The visualization
    displays a confidence value for each $\lambda$ and $\delta$ sub-categories after
    parsing the AST. The auto-completed snippet is processed with the clustering $\theta$
    function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ASTrust 的局部可解释性允许我们通过生成基于抽象语法树（AST）的可视化解释来解释单个代码片段 $s$，如图 [3](#S3.F3 "Figure
    3 ‣ 3.3\. Post Hoc Local and Global Explanations ‣ 3\. Syntax-Grounded Explanations
    ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations") 所示。实践者可以通过观察与 AST 上每个元素相关的概率来解释代码预测。换句话说，我们使用来自 Def. [3.2](#S3.Thmtheorem2
    "Definition 0\. ‣ 3.2\. Alignment and Clustering Formalism ‣ 3\. Syntax-Grounded
    Explanations ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations") 的 $\theta$ 来围绕 AST 节点进行聚类（即 AST 概率注释）。因此，基于语法的局部解释包含从代码预测到终端和非终端节点（或子类别）的概念映射。图
    [3](#S3.F3 "Figure 3 ‣ 3.3\. Post Hoc Local and Global Explanations ‣ 3\. Syntax-Grounded
    Explanations ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations") 是使用 gpt-3 [1.3B] 模型进行代码预测的概念映射的可视化表示。该可视化显示了在解析
    AST 后每个 $\lambda$ 和 $\delta$ 子类别的置信度值。自动完成的代码片段经过聚类 $\theta$ 函数处理。
- en: '![Refer to caption](img/5c338652aff769ab8aa2f6c958d373ec.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5c338652aff769ab8aa2f6c958d373ec.png)'
- en: Figure 4. Post Hoc Global Explanations Segregated by Categories and Sub-Categories
    for gpt-3 [125M] and mono-lang [2B]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 按类别和子类别分隔的 gpt-3 [125M] 和 mono-lang [2B] 的事后全局解释
- en: \Description
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: Global explanation post hoc explanation
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 全局解释事后解释
- en: ASTrust for global interpretability allows us to interpret a LLM by decomposing
    the canonical performance into segregated confidence performance. This segregated
    confidence is attached to Syntax Categories (SCs). SCs are tied to AST tree-sitter
    nodes (Brunsfeld et al., [2023](#bib.bib6)) and inspired by common object-oriented
    programming definitions. Although our approach is focused on the Python syntax,
    these categories may apply to multiple Programming Languages as they are being
    processed for similar AST elements.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 全球可解释性的ASTrust使我们能够通过将标准化性能分解为分隔的信心表现来解释LLM。这种分隔的信心附加在**语法类别**（SCs）上。SCs与AST树-坐标节点（Brunsfeld
    et al., [2023](#bib.bib6)）相关，并受到常见面向对象编程定义的启发。虽然我们的方法集中于Python语法，但这些类别可能适用于多种编程语言，因为它们正在处理类似的AST元素。
- en: Syntax-grounded global explanations comprise a conceptual mapping from code
    predictions to ten syntax categories. These predictions are calculated for the
    entire testbed rather than a particular snippet following Def. [3.2](#S3.Thmtheorem2
    "Definition 0\. ‣ 3.2\. Alignment and Clustering Formalism ‣ 3\. Syntax-Grounded
    Explanations ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations") about clustering function $\theta$. This clustering
    has an extra step in which a bootstrapping mechanism is used to estimate the confidence
    performance mean of the category set.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 语法基础的全球解释包括从代码预测到十个**语法类别**的概念映射。这些预测是针对整个测试环境计算的，而不是根据定义[3.2](#S3.Thmtheorem2
    "Definition 0\. ‣ 3.2\. Alignment and Clustering Formalism ‣ 3\. Syntax-Grounded
    Explanations ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations")中的聚类函数$\theta$计算的特定片段。该聚类有一个额外的步骤，即使用自助机制来估计类别集合的信心表现均值。
- en: Consequently, practitioners can compare Syntax Categories among models and explain
    the overall behavior of a given LLM by observing the segregated confidence performance.
    Note that previous analysis can be enriched with the information provided by canonical
    metrics. For instance, Fig. [4](#S3.F4 "Figure 4 ‣ 3.3\. Post Hoc Local and Global
    Explanations ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy and
    Interpretable LLMs for Code through Syntax-Grounded Explanations") depicts $M1$
    with an intrinsic performance of $0.48$ while showing a ASTrust interpretability
    performance of $0.74$ for the iteration category. Details of this global categorization
    can be further explored and visualized in our online appendix (Palacio et al.,
    [2024](#bib.bib45)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实践者可以比较模型之间的**语法类别**，并通过观察分隔的信心表现来解释给定LLM的整体行为。请注意，之前的分析可以通过标准化指标提供的信息得到丰富。例如，图[4](#S3.F4
    "Figure 4 ‣ 3.3\. Post Hoc Local and Global Explanations ‣ 3\. Syntax-Grounded
    Explanations ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations")展示了$M1$的固有表现为$0.48$，同时在迭代类别上显示了$0.74$的ASTrust可解释性表现。可以进一步在我们的在线附录中探讨和可视化这一全球分类的详细信息(Palacio
    et al., [2024](#bib.bib45))。
- en: It should be noted that the validity of both global and local syntax-grounded
    explanations is dependent upon the calibration of LLMs for code in terms of token
    probabilities and prediction correctness. That is, we assume that token probabilities
    are a reasonable proxy for the likelihood that a model is correct about a given
    token prediction. Recent work on calibration for LLMs of code has illustrated
    that, for code completion (which subsumes the experimental settings in this paper),
    LLMs tend to be well calibrated to token probabilities (Spiess et al., [2024](#bib.bib57)).
    We also further confirm this finding using causal inference in Section [5.3](#S5.SS3
    "5.3\. RQ3 ASTrust Validity ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations").
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应当注意，全球和局部语法基础解释的有效性取决于LLM在代码方面的**标定**，即令牌概率和预测正确性。也就是说，我们假设令牌概率是模型对给定令牌预测正确性的合理代理。近期对LLM代码标定的研究表明，对于代码完成（涵盖了本文的实验设置），LLM往往在令牌概率上标定良好(Spiess
    et al., [2024](#bib.bib57))。我们还在第[5.3](#S5.SS3 "5.3\. RQ3 ASTrust Validity ‣ 5\.
    Results ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations")节通过因果推断进一步确认了这一发现。
- en: 4\. Empirical Study Design
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实证研究设计
- en: 'We study the applicability of ASTrust in interpreting code completion tasks.
    We conducted a human study to investigate the usefulness of local explanations
    in real-world settings. In contrast, we conducted a data science study to showcase
    the effectiveness of global explanations on a diverse set of LLMs. Finally, we
    carried out a causal inference study to assess the validity of the syntax-grounded
    explanations as they relate to the statistical learning error of the studied models.
    The following research questions were formulated:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了ASTrust在解释代码补全任务中的适用性。我们进行了一个人类研究，探讨本地解释在现实世界设置中的有用性。相比之下，我们进行了一个数据科学研究，展示了全局解释在不同LLMs上的有效性。最后，我们进行了因果推断研究，评估语法基础解释与所研究模型统计学习误差之间的有效性。以下是制定的研究问题：
- en: RQ[1]
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ[1]
- en: '[Usefulness] How useful are local explanations in real-world settings? We validate
    the extent to which AST probability annotations are useful in locally explaining
    code predictions. We measure usefulness in three key factors: complexity, readability,
    and LLMs’ reliability.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[有用性] 本地解释在现实世界中的实际效用如何？我们验证AST概率注释在本地解释代码预测中的有效性。我们从复杂性、可读性和LLMs的可靠性三个关键因素来衡量其有用性。'
- en: RQ[2]
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ[2]
- en: '[Effectiveness] To what extent do LLMs for code correctly predict different
    syntactic structures? We interpret the performance of 12 LLMs on each Syntax Category
    (SC). The conceptual mapping allows us to obtain an interpretable and segregated
    confidence value per category, so we can detect categories that are easier or
    harder to predict – moving beyond canonical aggregate metrics.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[有效性] LLMs在代码中的不同语法结构预测准确程度如何？我们对12个LLMs在每个语法类别（SC）上的表现进行了解释。概念映射使我们能够获得每个类别的可解释和分隔的置信度值，从而可以检测更容易或更难预测的类别——超越传统的汇总指标。'
- en: RQ[3]
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ[3]
- en: '[Validity] How do Syntax Concepts impact LLMs’ statistical learning error?
    We validate the causal connection between learning error and LLMs’ ability to
    predict different syntax categories using ASTrust.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[有效性] 语法概念如何影响大型语言模型（LLMs）的统计学习误差？我们通过ASTrust验证学习误差与LLMs预测不同语法类别能力之间的因果关系。'
- en: 4.1\. Experimental Context
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 实验背景
- en: 4.1.1\. Model Collection.
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 模型收集
- en: To perform our global analysis, we conducted an interpretability analysis of
    12 open Decoder-only LLMs, selected based on their popularity. The largest among
    these models boasts 2.7 billion parameters. Tab. [1](#S4.T1 "Table 1 ‣ 4.1.3\.
    Machine Configuration. ‣ 4.1\. Experimental Context ‣ 4\. Empirical Study Design
    ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations") categorizes these LLMs into four distinct groups, each aligned
    with a specific fine-tuning strategy. The initial category comprises GPT-3-based
    models primarily trained on natural language, exemplified by Pile (Gao et al.,
    [2020](#bib.bib20)). The second category encompasses models trained on natural
    language but constructed upon the codegen architecture (Nijkamp et al., [2023](#bib.bib44)).
    Moving to the third category, we find models trained on multiple programming languages
    (PLs) using BigQuery (big, [2024](#bib.bib2)), implemented on both the gpt-2 and
    codegen architectures. The final category consists of both Multi-Language-Type
    models fine-tuned on BigPython (Nijkamp et al., [2023](#bib.bib44)), denoted as
    Mono-Language-Type, and gpt-2 models such as codeparrot (Gao et al., [2021](#bib.bib19)).
    All the datasets for training the LLMs encompass repositories/files sourced from
    GitHub up to 2021.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行全局分析，我们对12个开源Decoder-only LLMs进行了可解释性分析，这些模型是基于它们的受欢迎程度进行选择的。其中最大的模型拥有27亿个参数。表[1](#S4.T1
    "表1 ‣ 4.1.3\. 机器配置 ‣ 4.1\. 实验背景 ‣ 4\. 实证研究设计 ‣ 通过语法基础解释实现更可信和可解释的LLMs")将这些LLMs分为四个不同的组，每个组对应一个特定的微调策略。第一类包括主要在自然语言上训练的基于GPT-3的模型，例如Pile（Gao
    et al., [2020](#bib.bib20)）。第二类包括在自然语言上训练但基于codegen架构（Nijkamp et al., [2023](#bib.bib44)）构建的模型。第三类是使用BigQuery（big,
    [2024](#bib.bib2)）训练的多编程语言（PLs）模型，这些模型在gpt-2和codegen架构上实现。最后一类包括在BigPython（Nijkamp
    et al., [2023](#bib.bib44)）上微调的多语言类型模型（称为单语言类型）和gpt-2模型，例如codeparrot（Gao et al.,
    [2021](#bib.bib19)）。所有训练LLMs的数据集都包含来自GitHub的仓库/文件，截止到2021年。
- en: 4.1.2\. Evaluation Dataset
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 评估数据集
- en: To ensure the integrity of our ASTrust evaluation, it is imperative to avoid
    data contamination by excluding samples used in the training process of the LLMs.
    We extended and used SyxTestbed (Rodriguez-Cardenas et al., [2023](#bib.bib55))
    to overcome this challenge. SyxTestbed exclusively comprises code commits from
    the top 200 most popular Python GitHub repositories between 01/01/22 and 01/01/23\.
    Notably, SyxTestbed incorporates comprehensive data, including commit messages,
    method comments, the entire AST structure, node count, AST levels, AST errors,
    whitespace details, lines of code, cyclomatic complexity, and token counts.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保我们ASTrust评估的完整性，必须避免数据污染，通过排除用于LLMs训练过程中的样本来实现这一点。我们扩展并使用了SyxTestbed (Rodriguez-Cardenas等，
    [2023](#bib.bib55)) 来克服这个挑战。SyxTestbed专门包含了从2022年01月01日到2023年01月01日之间前200个最受欢迎的Python
    GitHub仓库中的代码提交。值得注意的是，SyxTestbed包含了全面的数据，包括提交消息、方法注释、整个AST结构、节点计数、AST级别、AST错误、空白细节、代码行数、圈复杂度和令牌计数。
- en: 4.1.3\. Machine Configuration.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 机器配置
- en: We performed the experiments using 20.04 Ubuntu with an AMD EPYC 7532 32-Core
    CPU, A100 NVIDIA GPU with 40GB VRAM, and 1TB RAM. For the model inference process,
    we used HugginFace and Pytorch (Wolf et al., [2020](#bib.bib67); Paszke et al.,
    [2019](#bib.bib48)). All models were loaded into the GPU to boost the inference
    time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了20.04 Ubuntu系统，配备AMD EPYC 7532 32核CPU、40GB VRAM的A100 NVIDIA GPU和1TB RAM进行实验。对于模型推理过程，我们使用了HugginFace和Pytorch
    (Wolf等， [2020](#bib.bib67); Paszke等，[2019](#bib.bib48))。所有模型都加载到GPU中，以加快推理时间。
- en: Table 1. Large Language Models Descriptions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 大型语言模型描述。
- en: '| Large Language Models (LLMs) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 大型语言模型 (LLMs) |'
- en: '| --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Type | ID | Name | Architecture | Size |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | ID | 名称 | 架构 | 大小 |'
- en: '| Natural L. gpt-3 | $M_{1}$ | gpt-neo-125m | gpt-3 | 125M |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Natural L. gpt-3 | $M_{1}$ | gpt-neo-125m | gpt-3 | 125M |'
- en: '| $M_{2}$* | gpt-neo-1.3B | gpt-3 | 1.3B |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| $M_{2}$* | gpt-neo-1.3B | gpt-3 | 1.3B |'
- en: '| $M_{3}$ | gpt-neo-2.7B | gpt-3 | 2.7B |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| $M_{3}$ | gpt-neo-2.7B | gpt-3 | 2.7B |'
- en: '| Natural L. codegen | $M_{4}$ | codegen-350M-nl | codegen | 350M |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Natural L. codegen | $M_{4}$ | codegen-350M-nl | codegen | 350M |'
- en: '| $M_{5}$ | codegen-2B-nl | codegen | 2B |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| $M_{5}$ | codegen-2B-nl | codegen | 2B |'
- en: '| Multi- Language | $M_{6}$ | codeparrot-small-multi | gpt-2 | 110M |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Multi- Language | $M_{6}$ | codeparrot-small-multi | gpt-2 | 110M |'
- en: '| $M_{7}$ | codegen-350M-multi | codegen-350M-nl | 350M |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| $M_{7}$ | codegen-350M-multi | codegen-350M-nl | 350M |'
- en: '| $M_{8}$ | codegen-2B-multi | codegen-2B-nl | 2B |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| $M_{8}$ | codegen-2B-multi | codegen-2B-nl | 2B |'
- en: '| Mono- Language | $M_{9}$ | codeparrot-small | gpt-2 | 110M |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Mono- Language | $M_{9}$ | codeparrot-small | gpt-2 | 110M |'
- en: '| $M_{10}$ | codeparrot | gpt-2 | 1.5B |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| $M_{10}$ | codeparrot | gpt-2 | 1.5B |'
- en: '| $M_{11}$ | codegen-350M-mono | codegen-350M-multi | 350M |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| $M_{11}$ | codegen-350M-mono | codegen-350M-multi | 350M |'
- en: '| $M_{12}$ | codegen-2B-mono | codegen-2B-multi | 2B |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| $M_{12}$ | codegen-2B-mono | codegen-2B-multi | 2B |'
- en: '*The human study was conducted using $M_{2}$ - gpt-3 [1.3B]'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*人类研究使用了$M_{2}$ - gpt-3 [1.3B]*'
- en: 4.2\. Human Study for ASTrust Usefulness
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 人类研究以评估ASTrust的有效性
- en: This section presents a preliminary human study comprising a control/treatment
    experimental design to assess ASTrust usefulness in practical settings. We followed
    a purposive sampling approach (Baltes and Ralph, [2021](#bib.bib5)) since our
    primary goal was to gather preliminary data and insights from practitioners with
    expertise in ML and SE combined. We selected our subjects carefully rather than
    randomly to study the usefulness of ASTrust (at local explanations). ASTrust is
    designed to enhance the interpretation of model decisions in code completion tasks
    for practitioners with diverse backgrounds, including researchers, students, and
    data scientists. By targeting individuals with specific expertise, we ensured
    that the feedback received was relevant and informed, thereby enhancing the quality
    of our preliminary findings.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一项初步的人类研究，包括一个对照/处理实验设计，以评估ASTrust在实际设置中的有效性。由于我们的主要目标是从具备ML和SE结合的专业知识的从业者那里收集初步数据和见解，我们采用了有目的抽样方法
    (Baltes和Ralph，[2021](#bib.bib5))。我们精心选择了研究对象，而不是随机选择，以研究ASTrust（在局部解释中的有效性）。ASTrust旨在提高模型决策在代码完成任务中的解释性，适用于包括研究人员、学生和数据科学家在内的具有不同背景的从业者。通过针对具有特定专业知识的个体，我们确保了所获得的反馈相关且经过深思熟虑，从而提升了我们初步发现的质量。
- en: 4.2.1\. Survey Structure
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 调查结构
- en: Each survey consists of three sections. The first section is aimed at gathering
    participant profiling information. The profiling section aims to collect information
    related to how proficient the participants are when using Python and AI-assisted
    tools in code generation tasks. In particular, we asked about their level of expertise
    in Programming Languages (PL) and how familiar they are with AST structure. Furthermore,
    we asked about any challenges they encountered when using AI-assisted tools. This
    information is relevant because we want to control external factors that may influence
    their perception in validating ASTrust. The second section aimed to present four
    code completion scenarios and ask participants to rate their quality. For each
    scenario, we presented an incomplete Python method as a prompt, the generated
    code completing the previous method (using gpt-3 [1.3B] in Tab. [1](#S4.T1 "Table
    1 ‣ 4.1.3\. Machine Configuration. ‣ 4.1\. Experimental Context ‣ 4\. Empirical
    Study Design ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations")), and a specific type of local explanation (e.g.,
    $U_{SEQ}$, $U_{AST[p]}$, and $U_{AST[c]}$ in Fig. [5](#S4.F5 "Figure 5 ‣ 4.2.7\.
    Statistical Analysis ‣ 4.2\. Human Study for ASTrust Usefulness ‣ 4\. Empirical
    Study Design ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations")). In all four scenarios, the prompt contained either
    a semantic error (e.g., using an undefined variable, incorrect condition statement(s),
    calling an undefined function) or a syntax error (e.g., missing colon, missing
    parenthesis, incorrect indentation) that participants needed to reason about after
    considering a given local explanation type. We aimed to capture the participant’s
    perspective regarding the explanation by asking them to describe the cause of
    a syntax or semantic error from the generated code. To facilitate the analysis,
    we highlighted the portion of the code generated by the model (see green highlight
    Fig. [3](#S3.F3 "Figure 3 ‣ 3.3\. Post Hoc Local and Global Explanations ‣ 3\.
    Syntax-Grounded Explanations ‣ Towards More Trustworthy and Interpretable LLMs
    for Code through Syntax-Grounded Explanations")). We did not provide any detail
    about the LLM used to generate the predictions in order prevent introducing bias
    related to preconceived notions about particular LLMs in the responses. Lastly,
    the third section aim to collect feedback about the effectiveness of the different
    types of local explanation. Specifically, we asked participants about the complexity
    of the visualizations and potential opportunities for enhancement.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 每个调查问卷由三个部分组成。第一部分旨在收集参与者的个人信息。个人信息部分的目标是收集与参与者在使用 Python 和 AI 辅助工具进行代码生成任务时的熟练程度相关的信息。特别地，我们询问了他们在编程语言（PL）方面的专业水平以及他们对
    AST 结构的熟悉程度。此外，我们还询问了他们在使用 AI 辅助工具时遇到的任何挑战。这些信息是相关的，因为我们希望控制可能影响他们在验证 ASTrust
    时感知的外部因素。第二部分旨在呈现四种代码补全场景，并要求参与者对这些场景的质量进行评分。在每个场景中，我们展示了一个不完整的 Python 方法作为提示，生成的代码完成了之前的方法（使用
    gpt-3 [1.3B] 在表 [1](#S4.T1 "Table 1 ‣ 4.1.3\. Machine Configuration. ‣ 4.1\. Experimental
    Context ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations")），以及一种特定类型的局部解释（例如，$U_{SEQ}$、$U_{AST[p]}$
    和 $U_{AST[c]}$ 在图 [5](#S4.F5 "Figure 5 ‣ 4.2.7\. Statistical Analysis ‣ 4.2\.
    Human Study for ASTrust Usefulness ‣ 4\. Empirical Study Design ‣ Towards More
    Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")）。在所有四种场景中，提示都包含一个语义错误（例如，使用未定义的变量、不正确的条件语句、调用未定义的函数）或语法错误（例如，缺少冒号、缺少括号、不正确的缩进），参与者需要在考虑给定的局部解释类型后进行推理。我们旨在通过要求他们描述生成代码中的语法或语义错误的原因来捕捉参与者对解释的看法。为了方便分析，我们突出显示了模型生成的代码部分（见绿色高亮图
    [3](#S3.F3 "Figure 3 ‣ 3.3\. Post Hoc Local and Global Explanations ‣ 3\. Syntax-Grounded
    Explanations ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations")）。我们没有提供关于生成预测的 LLM 的任何细节，以防引入与对特定 LLM 的先入之见相关的偏见。最后，第三部分旨在收集有关不同类型局部解释的有效性的反馈。具体而言，我们询问了参与者对可视化的复杂性和潜在改进机会的看法。
- en: 4.2.2\. Survey Treatments
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 调查处理
- en: 'To collect the perception of practitioners regarding the usability of ASTrust,
    we devised a control survey ($U_{CTR}$) and three treatments with two types of
    local explanations: sequential ($U_{SEQ}$) and AST-based ($U_{AST}$) explanations.
    $U_{CTR}$ represents the absence of a local explanation and only collects the
    participants’ perceptions regarding the correctness of LLMs’ output. By contrast,
    treatment surveys $U_{SEQ}$ and $U_{AST}$ yield syntax-grounded explanations.
    It is worth noting that we define correctness as the degree to which the generated
    code reflects the purpose of the algorithm contained in the prompt. In other words,
    we ask participants to judge whether the model predicted a valid or closely accurate
    set of tokens given the information context within the prompt.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了收集从业者对 ASTrust 可用性的感知，我们设计了一个对照调查 ($U_{CTR}$) 和三个处理组，包含两种类型的局部解释：顺序 ($U_{SEQ}$)
    和基于 AST ($U_{AST}$) 的解释。$U_{CTR}$ 表示没有局部解释，仅收集参与者对 LLM 输出正确性的感知。相比之下，处理组调查 $U_{SEQ}$
    和 $U_{AST}$ 产生基于语法的解释。值得注意的是，我们将正确性定义为生成的代码反映提示中算法目的的程度。换句话说，我们要求参与者判断模型是否根据提示中的信息上下文预测了一组有效或接近准确的标记。
- en: 'Fig. [5](#S4.F5 "Figure 5 ‣ 4.2.7\. Statistical Analysis ‣ 4.2\. Human Study
    for ASTrust Usefulness ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations") depicts
    all the explanation types considered in the survey. $U_{SEQ}$ displays the tokens
    and their corresponding probabilities in a sequential (i.e., linear layout). Linear
    representations are commonly used by feature-importance explainability techniques
    such as attention-based (Mohammadkhani et al., [2023b](#bib.bib40)) and Shapley
    (Kalai and Samet, [1983](#bib.bib25)) values. Therefore, $U_{SEQ}$ serves as a
    baseline to determine how our local explanations $U_{AST}$ provide insightful
    information beyond the linear layout. Conversely, $U_{AST}$ uses an AST visualization,
    comprising two types of local explanations: AST-Complete (i.e., $U_{AST[c]}$)
    and AST-partial (i.e., $U_{AST[p]}$). $U_{AST[c]}$ represents the entire sample’s
    AST (i.e., prompt and generated code) including ASTrust confidence performance
    for all nodes. Conversely, $U_{AST[p]}$ is a filtered $U_{AST[c]}$ representation
    that only exposes the confidence performance of the generated code and omits the
    nodes from the prompt.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S4.F5 "图 5 ‣ 4.2.7\. 统计分析 ‣ 4.2\. ASTrust 有效性的人工研究 ‣ 4\. 实证研究设计 ‣ 朝着更值得信赖和可解释的代码
    LLMs 迈进，通过基于语法的解释") 展示了调查中考虑的所有解释类型。 $U_{SEQ}$ 显示了以顺序（即线性布局）展示的标记及其相应的概率。线性表示通常被特征重要性解释技术使用，如基于注意力的
    (Mohammadkhani 等，[2023b](#bib.bib40)) 和 Shapley (Kalai 和 Samet，[1983](#bib.bib25))
    值。因此，$U_{SEQ}$ 作为基线来确定我们的局部解释 $U_{AST}$ 如何提供超越线性布局的有见地信息。相反，$U_{AST}$ 使用 AST 可视化，包括两种类型的局部解释：AST-Complete（即
    $U_{AST[c]}$）和 AST-partial（即 $U_{AST[p]}$）。$U_{AST[c]}$ 表示整个样本的 AST（即提示和生成的代码），包括所有节点的
    ASTrust 置信度性能。相反，$U_{AST[p]}$ 是过滤后的 $U_{AST[c]}$ 表示，只展示生成代码的置信度性能，并省略了来自提示的节点。
- en: 4.2.3\. Survey Metrics
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 调查指标
- en: 'When evaluating the usefulness of our approach to answer [RQ[1]](#S4.I1.i1
    "item RQ1 ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations"), we measure the qualitative
    features of local explanations depicted in Fig. [5](#S4.F5 "Figure 5 ‣ 4.2.7\.
    Statistical Analysis ‣ 4.2\. Human Study for ASTrust Usefulness ‣ 4\. Empirical
    Study Design ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations"). More precisely, we proposed five qualitative metrics
    to evaluate the usefulness of our approach: Information Usefulness, Local Explanation
    Complexity, Local Explanation Readability, Visualization Usefulness, and LLM’s
    reliability. We used a Likert scale with three options for quantitatively measuring
    the responses. Specifically for Information Usefulness: Agree, Neutral and Disagree.
    For Local Explanation Complexity, Local Explanation Readability and Visualization
    Usefulness: Useful, Slightly useful and Not useful. Finally, for LLM’s Reliability:
    Not reliable, Highly reliable and Impossible to tell. Each of the survey metrics
    corresponds to one of the following survey questions.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估我们的方法是否能有效回答[RQ[1]](#S4.I1.i1 "item RQ1 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")时，我们衡量了图[5](#S4.F5
    "Figure 5 ‣ 4.2.7\. Statistical Analysis ‣ 4.2\. Human Study for ASTrust Usefulness
    ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy and Interpretable LLMs
    for Code through Syntax-Grounded Explanations")中所示的局部解释的定性特征。更具体地，我们提出了五个定性指标来评估我们方法的有效性：信息有用性、局部解释复杂性、局部解释可读性、可视化有用性和LLM的可靠性。我们使用了一个三项选择的李克特量表来定量测量反馈。具体而言，对于信息有用性：同意、中立和不同意。对于局部解释复杂性、局部解释可读性和可视化有用性：有用、稍微有用和无用。最后，对于LLM的可靠性：不可靠、高度可靠和无法判断。每个调查指标对应以下调查问题之一。
- en: 'Metric[1]: Information Usefulness - ‘Q: How useful was the information for
    interpreting the model’s decisions?’ In the treatment surveys, we ask the participants
    to explain the LLM’s behavior when completing the code of individual samples,
    and we gauge their perception regarding the usefulness of the provided information
    to accomplish this task. We anticipate correlations between the explanation types
    and perceived usefulness.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '指标[1]：信息有用性 - ‘Q: 信息对于解释模型决策有多大帮助？’在处理调查中，我们要求参与者解释LLM在完成个别样本的代码时的行为，并评估他们对提供的信息在完成任务中的有用性的看法。我们预期解释类型与感知有用性之间存在相关性。'
- en: 'Metric[2]: Local Explanation Complexity - ‘Q: I found the visualization unnecessarily
    complex’. The local explanation complexity refers to the degree of intricacy of
    its types. The degree of complexity may affect perceptions of usefulness.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '指标[2]：局部解释复杂性 - ‘Q: 我认为可视化过于复杂’。局部解释复杂性指的是其类型的复杂程度。复杂程度可能会影响对有用性的感知。'
- en: 'Metric[3]: Local Explanation Readability - ‘Q: I thought the visualization
    was easy to read and use’. We define readability as the degree to which our local
    explanations are intuitive and easy to understand. We hypothesize that if the
    explanation fits this criterion, we can consider it useful. Readability accounts
    for factors such as the amount of consigned information, the arrangement of tokens
    and categories, and the color scheme.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '指标[3]：局部解释可读性 - ‘Q: 我认为可视化易于阅读和使用’。我们将可读性定义为我们的局部解释是否直观且易于理解。我们假设，如果解释符合这一标准，我们可以认为它是有用的。可读性考虑了如信息量、标记和类别的排列以及配色方案等因素。'
- en: 'Metric[4]: Visualization Usefulness - ‘Q: I thought the visualization was useful
    for explaining the model’s behavior’. The visualization is the graphical representation
    of the local explanation (refer to Fig. [3](#S3.F3 "Figure 3 ‣ 3.3\. Post Hoc
    Local and Global Explanations ‣ 3\. Syntax-Grounded Explanations ‣ Towards More
    Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")).
    Each treatment survey uses a type of visualization (i.e., $U_{SEQ}$ or $U_{AST}$).
    We formulate this question to determine which visualization is considered more
    useful.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '指标[4]：可视化有用性 - ‘Q: 我认为可视化对解释模型行为很有用’。可视化是局部解释的图形表示（参见图[3](#S3.F3 "Figure 3
    ‣ 3.3\. Post Hoc Local and Global Explanations ‣ 3\. Syntax-Grounded Explanations
    ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations")）。每种处理调查使用一种类型的可视化（即 $U_{SEQ}$ 或 $U_{AST}$）。我们制定这个问题以确定哪种可视化被认为更有用。'
- en: 'Metric[5]: LLM’s Reliability - ‘Q: What is your perception of the model’s reliability
    in generating code?’. We define reliability as the degree to which a user trusts
    the LLM’s output based on the outcomes from local explanations. We ask the participants
    to reflect on the LLM’s reliability across our surveys using local explanations.
    Considering all four code completion scenarios in the surveys include errors,
    the greater the number of participants in each survey who would not rely on the
    model, the more valuable the syntax-grounded local explanation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 'Metric[5]: LLM的可靠性 - ‘Q: 您对模型生成代码的可靠性有何看法？’。我们将可靠性定义为用户基于局部解释对LLM输出的信任程度。我们要求参与者反思在使用局部解释的调查中LLM的可靠性。考虑到调查中的所有四种代码补全场景均包含错误，参与者中越多的人不依赖于模型，则基于语法的局部解释越有价值。'
- en: 4.2.4\. Open Questions
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 开放问题
- en: In addition to survey metrics, we formulated several open-ended questions for
    collecting the participants’ perception about the correctness of the predictions
    (Open[1]) and the most helpful parts of the visual explanations including potential
    improvement aspects (Open[2]). Each of these open metrics corresponds to one or
    more survey questions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调查指标外，我们制定了几个开放式问题，以收集参与者对预测正确性的看法（Open[1]）和可视化解释中最有帮助部分的意见，包括潜在的改进方面（Open[2]）。这些开放式指标分别对应一个或多个调查问题。
- en: 'Open[1]: LLM’s Prediction Correctness - ‘Q: If the generated code is incorrect,
    can you explain why the model might have made the mistake? Otherwise, If the generated
    code is correct, can you speculate on why the model may have been able to correctly
    predict the above snippet?’. We asked the participants to use the provided information
    per sample to analyze whether the prompt or the generated code contained any syntax
    or semantic error. In $U_{CTR}$, we aimed to assess the extent to which participants
    could reason about the source code correctness without any type of explanation
    provided. Conversely, in $U_{SEQ}$ and $U_{AST}$, we inspected if the layout information
    somehow contributed to detecting and reasoning about the cause of the error.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 'Open[1]: LLM的预测正确性 - ‘Q: 如果生成的代码不正确，您能解释模型可能犯错的原因吗？否则，如果生成的代码正确，您能推测模型可能为何能正确预测上述片段吗？’。我们要求参与者利用每个样本中提供的信息分析提示或生成的代码是否包含任何语法或语义错误。在$U_{CTR}$中，我们旨在评估参与者在没有任何类型解释的情况下，对源代码正确性的推理能力。相反，在$U_{SEQ}$和$U_{AST}$中，我们检查了布局信息是否在某种程度上有助于检测和推理错误原因。'
- en: 'Open[2]: Importance of visual explanations - ‘$Q_{1}:$ What information from
    the visualization did you find useful in explaining the model’s predictions?’,
    ‘$Q_{2}:$ What information from the visualization did you find useful in explaining
    the model’s predictions?’, ‘$Q_{3}:$ What other information (if any) would you
    like to see in the visualization?’, ‘$Q_{4}:$ What elements of the visualization
    did you like most?’, ‘$Q_{5}:$ What elements of the visualization did you like
    least?’. We asked the participants to provide overall feedback about the type
    of representation used in the treatment surveys ($U_{SEQ}$ and $U_{AST}$). We
    aimed to identify the most and least useful elements, as well as gather potential
    ideas for improvement.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'Open[2]: 可视化解释的重要性 - ‘$Q_{1}:$ 从可视化中获取了哪些信息对解释模型的预测有帮助？’，‘$Q_{2}:$ 从可视化中获取了哪些信息对解释模型的预测有帮助？’，‘$Q_{3}:$
    您还希望在可视化中看到哪些其他信息（如果有的话）？’，‘$Q_{4}:$ 您最喜欢可视化中的哪些元素？’，‘$Q_{5}:$ 您最不喜欢可视化中的哪些元素？’。我们要求参与者对治疗调查中使用的表示类型（$U_{SEQ}$
    和 $U_{AST}$）提供总体反馈。我们的目标是识别最有用和最无用的元素，并收集潜在的改进建议。'
- en: To collect, standardize, and analyze the previous group of open-ended questions,
    two authors independently gathered and reviewed each survey’s responses. Any differences
    were resolved through discussion to reach a consensus.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了收集、标准化和分析之前的一组开放式问题，两位作者独立收集并审查了每份调查的反馈。任何差异通过讨论解决以达成共识。
- en: 4.2.5\. Population Profiling
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. 人口统计分析
- en: The target population consists of software engineering practitioners experienced
    in using AI tools for code generation (e.g., ChatGPT, Copilot). Participants were
    meant to be knowledgeable in Python and understand how algorithms are structured
    in programming languages and represented in Abstract Syntax Trees (ASTs). While
    certain knowledge in Deep Learning architectures used for Text Generation (e.g.,
    GPT, BERT, T5) is preferred, it was not required. Individuals of any gender were
    welcome to participate, with a minimum age requirement of 21 years. Participation
    was entirely voluntary, and no incentives were offered beyond contributing to
    our efforts to enhance deep learning interpretability for code generation. Furthermore,
    participants were informed of the voluntary nature of the study during solicitation
    ³³3This study was reviewed by the protection of human subjects committee at the
    College of William & Mary under protocol number PHSC-2023-03-03-16218-dposhyvanyk
    titled A Survey Research on Code Concepts for Interpreting Neural Language Models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 目标人群由有经验使用AI工具进行代码生成（例如ChatGPT，Copilot）的软件工程实践者组成。参与者应熟悉Python，并了解算法在编程语言中的结构以及在抽象语法树（ASTs）中的表示。虽然对深度学习架构（例如GPT，BERT，T5）在文本生成中的应用有一定了解是优选的，但不是必需的。欢迎任何性别的个人参与，年龄要求至少21岁。参与完全自愿，除了为我们提升深度学习解释性做出贡献外，没有其他激励。此外，参与者在邀请时被告知研究的自愿性质³³3本研究已由威廉与玛丽学院人类研究保护委员会审核，协议编号为PHSC-2023-03-03-16218-dposhyvanyk，标题为“对解释神经语言模型的代码概念的调查研究”。
- en: 4.2.6\. Data Collection
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.6\. 数据收集
- en: We reached out to $50$ potential participants who were unaware of the purpose
    of this work, from industrial and academic backgrounds with varying levels of
    expertise in machine learning and Python. Participants were contacted via email
    invitations. Out of this group, $27$ completed one of the surveys, with the assignment
    uniformly distributed among the surveys. but we excluded three for low-quality
    responses, leaving $24$ valid submissions. The study was performed on Qualtrics
    (noa, [2024](#bib.bib3)) and the anonymized survey data can be found in our appendix (Palacio
    et al., [2024](#bib.bib45)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们联系了$50$名潜在参与者，他们对本研究的目的并不知情，背景涵盖了工业界和学术界，具有不同层次的机器学习和 Python 专业知识。参与者通过电子邮件邀请进行联系。在这组人中，有$27$人完成了其中一项调查，任务在调查中均匀分配，但我们排除了三份低质量的回答，剩下$24$份有效提交。研究在Qualtrics上进行（noa,
    [2024](#bib.bib3)），匿名调查数据可以在我们的附录中找到（Palacio et al., [2024](#bib.bib45)）。
- en: 4.2.7\. Statistical Analysis
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.7\. 统计分析
- en: 'We use $U_{SEQ}$ as a baseline for our study. We expose the participants to
    ASTrust with two treatments: $U_{AST[p]}$ and $U_{AST[c]}$ (refer to Fig. [5](#S4.F5
    "Figure 5 ‣ 4.2.7\. Statistical Analysis ‣ 4.2\. Human Study for ASTrust Usefulness
    ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy and Interpretable LLMs
    for Code through Syntax-Grounded Explanations")). The result of each question
    is influenced by these two treatments. To compare the influence of $U_{AST[p]}$
    and $U_{AST[c]}$ against $U_{SEQ}$, we compute the weighted average of the responses
    from surveys $U_{AST[p]}$ and $U_{AST[c]}$. We refer to the weighted average as
    $U_{AST}$. First, we calculate the results of each treatment individually for
    all the answers. Then, the weight of each answer is estimated by averaging the
    number of responses per answer across all samples. We then normalize this weight
    to get the final weighted average for $U_{AST}$. We use this weighted average
    for all our statistical analyses in the paper.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$U_{SEQ}$作为我们的基线。我们向参与者展示了ASTrust的两种处理方式：$U_{AST[p]}$和$U_{AST[c]}$（参见图[5](#S4.F5
    "图 5 ‣ 4.2.7\. 统计分析 ‣ 4.2\. 人体研究的ASTrust有用性 ‣ 4\. 实证研究设计 ‣ 朝着更可信和可解释的LLMs为代码通过语法基础解释")）。每个问题的结果受这两种处理的影响。为了比较$U_{AST[p]}$和$U_{AST[c]}$对$U_{SEQ}$的影响，我们计算了来自调查$U_{AST[p]}$和$U_{AST[c]}$的加权平均值。我们将加权平均值称为$U_{AST}$。首先，我们分别计算每种处理的结果。然后，通过对所有样本中每个回答的回应次数进行平均来估算每个回答的权重。接着，我们对这个权重进行归一化，以获得$U_{AST}$的最终加权平均值。我们在论文中的所有统计分析中使用这一加权平均值。
- en: '![Refer to caption](img/165330e69e9885c2994bf8244d8f3cc6.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/165330e69e9885c2994bf8244d8f3cc6.png)'
- en: Figure 5. ASTrust Local Explanation Treatments.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. ASTrust 本地解释处理。
- en: 4.2.8\. Survey Validity
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.8\. 调查有效性
- en: To validate the design of the human study, we conducted a pilot experiment with
    10 individuals excluded from the pool of participants. Based on this pilot, the
    quality and appropriateness of the control and treatment surveys were solidified.
    Initially, the pilot survey included only the $U_{CTR}$ control and the $U_{AST[c]}$
    treatment. However, the pilot revealed the need for an intermediate representation
    serving as a baseline explanation, which is less complex than an AST visualization,
    to ensure a fair comparison. Consequently, we introduced $U_{SEQ}$, inspired by
    techniques such as SHAP (Lundberg and Lee, [2017](#bib.bib36)), as a baseline
    treatment with a less complex representation. Additionally, we introduced $U_{AST[p]}$,
    a partial representation of $U_{AST[c]}$, as a less complex treatment highlighting
    only the hierarchical structure of the generated code.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证人类研究的设计，我们对10名不在参与者池中的个体进行了试点实验。根据这次试点，控制和处理问卷的质量和适当性得到了巩固。最初，试点调查仅包括$U_{CTR}$控制组和$U_{AST[c]}$处理组。然而，试点揭示了需要一个作为基线解释的中间表示，该表示比AST可视化更简单，以确保公平比较。因此，我们引入了$U_{SEQ}$，其灵感来自于SHAP等技术（Lundberg和Lee，[2017](#bib.bib36)），作为基线处理，具有较少复杂的表示。此外，我们引入了$U_{AST[p]}$，这是$U_{AST[c]}$的部分表示，作为一种较简单的处理，仅突出了生成代码的层次结构。
- en: 4.3\. Data Science Study for ASTrust Effectiveness
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 数据科学研究与ASTrust有效性
- en: To answer [RQ[2]](#S4.I1.i2 "item RQ2 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    we implemented a data science study to globally interpret 12 LLMs’ performance
    described in Tab. [1](#S4.T1 "Table 1 ‣ 4.1.3\. Machine Configuration. ‣ 4.1\.
    Experimental Context ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy and
    Interpretable LLMs for Code through Syntax-Grounded Explanations") on the SyxTestbed
    dataset. We performed code completion with different input prompts. The input
    prompt combines the code completion task, a description, and a partial code snippet.
    Each prompt has a standard maximum size of 1024 tokens for all considered LLMs.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答[RQ[2]](#S4.I1.i2 "项目 RQ2 ‣ 4\. 实证研究设计 ‣ 通过语法基础解释提升代码LLM的可信度和可解释性")，我们实施了一项数据科学研究，以全球性地解读表[1](#S4.T1
    "表 1 ‣ 4.1.3\. 机器配置 ‣ 4.1\. 实验背景 ‣ 4\. 实证研究设计 ‣ 通过语法基础解释提升代码LLM的可信度和可解释性")中描述的12种LLM在SyxTestbed数据集上的表现。我们进行了代码补全实验，使用了不同的输入提示。输入提示结合了代码补全任务、描述和部分代码片段。每个提示对所有考虑的LLM都有一个标准的最大尺寸为1024个标记。
- en: We first compute the normalized log-probabilities (Sec.[3.1](#S3.SS1 "3.1\.
    Interpretable Syntax Sets ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")) or TLP
    $\hat{w_{i}}$ for each SyxTestbed snippet $s\in\mathcal{S}$. These log-probabilities
    were obtained across the 12 LLMs for every token position. The log-probability
    distributions maintain a consistent vector size $|\mathcal{V}|$ for each token
    position. Subsequently, these distributions underwent processing to extract the
    log-probability aligned with the expected token at position $i$. As a result,
    each token position corresponds to a stored prediction value $\hat{w_{i}}$ for
    constructing the TLP sequence $w_{<=i}$. As discussed earlier, this experimental
    setting is based on the premise that token probabilities are well-calibrated to
    model correctness, which has been confirmed in code completion settings by prior
    work (Spiess et al., [2024](#bib.bib57)). Additionally, we confirm this finding
    in answering RQ[3] by observing a causal link between learning error and the probabilities
    used within ASTrust.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算每个SyxTestbed片段$s\in\mathcal{S}$的归一化对数概率（参见[3.1](#S3.SS1 "3.1\. 可解释的语法集
    ‣ 3\. 语法基础解释 ‣ 通过语法基础解释提升代码LLM的可信度和可解释性")）或TLP $\hat{w_{i}}$。这些对数概率是针对12种LLM在每个标记位置上获得的。对数概率分布在每个标记位置上保持一致的向量大小$|\mathcal{V}|$。随后，这些分布经过处理以提取与位置$i$的期望标记对齐的对数概率。因此，每个标记位置对应于一个存储的预测值$\hat{w_{i}}$，用于构建TLP序列$w_{<=i}$。如前所述，这种实验设置基于标记概率与模型正确性的良好校准的前提，这在代码补全设置中已由先前的工作确认（Spiess等，[2024](#bib.bib57)）。此外，我们通过观察学习误差与ASTrust中使用的概率之间的因果关系来确认这一发现，以回答RQ[3]。
- en: We used the alignment function $\delta$ to obtain the terminal node $\lambda$
    vector (see Def.[3.1](#S3.Thmtheorem1 "Definition 0\. ‣ 3.2\. Alignment and Clustering
    Formalism ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations")). Next, we traversed the
    AST for each terminal node $\lambda$ and clustered them into the corresponding
    final $\lambda,\alpha$ node and their correspondent TLP by applying the $\theta$
    function (see Sec.[3](#S3 "3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")). The clustering
    was fixed to generate 32 subcategories and their probability values. We estimated
    a single confidence performance metric (a.k.a. ASTrust Interpretability Performance)
    per model by averaging the subcategories probabilities. The confidence performance
    per model was bootstrapped with the median (size of 500 samplings) to ensure a
    fair comparison. Lastly, we mapped the subcategories to the SCs obtaining a value
    per Category $\mathcal{C}$ (e.g., Data Structures, Decision, or Scope).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用对齐函数 $\delta$ 来获得终端节点 $\lambda$ 向量（参见定义 [3.1](#S3.Thmtheorem1 "定义 0\. ‣
    3.2\. 对齐与聚类形式主义 ‣ 3\. 语法基础解释 ‣ 朝着更可信和可解释的 LLM 迈进，通过语法基础解释")）。接下来，我们遍历了每个终端节点 $\lambda$
    的 AST，并通过应用 $\theta$ 函数将它们聚类到相应的最终 $\lambda,\alpha$ 节点及其对应的 TLP（参见第 [3](#S3 "3\.
    语法基础解释 ‣ 朝着更可信和可解释的 LLM 迈进，通过语法基础解释") 节）。聚类被固定生成 32 个子类别及其概率值。我们通过平均子类别概率来估计每个模型的单一置信度性能指标（即
    ASTrust 可解释性性能）。每个模型的置信度性能通过中位数（500 次采样）进行了自助法处理，以确保公平比较。最后，我们将子类别映射到 SCs，为每个类别
    $\mathcal{C}$ （例如，数据结构、决策或范围）获取一个值。
- en: To provide a baseline comparison, we calculated canonical extrinsic metrics
    BLUE-4 (Papineni et al., [2002](#bib.bib47)) and CodeBLEU (Ren et al., [2020](#bib.bib52)),
    and intrinsic performance. Extrinsic metrics evaluate downstream tasks directly
    (i.e., code completion), while intrinsic metrics assess how well a language model
    can accurately predict the next word given an incomplete sequence or prompt (Karampatsis
    et al., [2020](#bib.bib26)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供基准比较，我们计算了经典的外部指标 BLUE-4（Papineni et al.，[2002](#bib.bib47)）和 CodeBLEU（Ren
    et al.，[2020](#bib.bib52)），以及内部性能。外部指标直接评估下游任务（即代码补全），而内部指标评估语言模型在给定不完整序列或提示时能多准确地预测下一个单词（Karampatsis
    et al.，[2020](#bib.bib26)）。
- en: '![Refer to caption](img/e089124c8af026509f06416d93cbc204.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e089124c8af026509f06416d93cbc204.png)'
- en: Figure 6. SCM to estimate Syntax Effect on Learning Error.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. SCM 估计语法对学习误差的影响。
- en: Our analysis also includes a corner case experiment that compares the smaller
    gpt-3 [125M] to the largest mono-lang [2B]. We contrasted the subcategories for
    each LLM to obtain a segregated global explanation Fig. [9](#S5.F9 "Figure 9 ‣
    5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations"). Since we mapped the subcategories
    to categories, we can observe the ASTrust probability gaps between LLMs at more
    interpretable levels (see. Fig. [4](#S3.F4 "Figure 4 ‣ 3.3\. Post Hoc Local and
    Global Explanations ‣ 3\. Syntax-Grounded Explanations ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")). The probability
    values for subcategories and categories are the bootstrapped median.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析还包括一个角落案例实验，比较了较小的 gpt-3 [125M] 和最大的单语言模型 [2B]。我们对每个 LLM 的子类别进行了对比，以获得一个分隔的全球解释图
    [9](#S5.F9 "图 9 ‣ 5.2\. RQ2 ASTrust 有效性 ‣ 5\. 结果 ‣ 朝着更可信和可解释的 LLM 迈进，通过语法基础解释")。由于我们将子类别映射到类别，我们可以观察到
    LLM 之间在更可解释层次上的 ASTrust 概率差距（参见图 [4](#S3.F4 "图 4 ‣ 3.3\. 事后局部和全局解释 ‣ 3\. 语法基础解释
    ‣ 朝着更可信和可解释的 LLM 迈进，通过语法基础解释")）。子类别和类别的概率值为自助法中位数。
- en: 4.4\. Causal Inference Study for ASTrust Validity
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. ASTrust 有效性的因果推断研究
- en: We validate our ASTrust approach using causal inference to answer [RQ[3]](#S4.I1.i3
    "item RQ3 ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations"). To accomplish this, we formulated
    a Structural Causal Model (SCM) designed to estimate the impact of SC predictions
    on the overall learning error of LLMs (Pearl et al., [2016](#bib.bib49)). We consider
    that the learning error (i.e., cross-entropy loss) of an LLM is causally impacted
    by the predicted probabilities of syntax elements. This impact indicates that
    SCs influence the quality of an LLM. We conducted a causal inference analysis
    using the $do_{code}$ technique (Palacio et al., [2023](#bib.bib46)) to estimate
    SCs influence. Inherently, a developer mentally rationalizes several things such
    as the concept of the Iteration category (see Fig. [9](#S5.F9 "Figure 9 ‣ 5.2\.
    RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations")). If an LLM can make a similar
    mapping, it suggests that it has statistically learned some understanding of the
    syntax cycle structure.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用因果推断来验证我们的ASTrust方法，以回答[RQ[3]](#S4.I1.i3 "item RQ3 ‣ 4\. Empirical Study
    Design ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations")。为此，我们制定了一个结构性因果模型（SCM），旨在估计SC预测对LLMs整体学习误差的影响（Pearl et al., [2016](#bib.bib49)）。我们认为LLM的学习误差（即交叉熵损失）受到语法元素预测概率的因果影响。这种影响表明SCs会影响LLM的质量。我们使用$do_{code}$技术（Palacio
    et al., [2023](#bib.bib46)）进行因果推断分析，以估计SCs的影响。从本质上讲，开发者在心理上会理顺多个概念，如迭代类别的概念（见图[9](#S5.F9
    "Figure 9 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")）。如果LLM能够进行类似的映射，这表明它在统计上学到了对语法循环结构的某种理解。
- en: We calculate the causal influence using the SCM as the Average Treatment Effect
    (ATE) with the probability $p(Y|do(T))$ for both gpt-3 [125M] and mono-lang [2B]
    models. That is, we estimate the causal effect of the variable $T$ on $Y$ after
    controlling for confounders $Z$ (see Fig. [6](#S4.F6 "Figure 6 ‣ 4.3\. Data Science
    Study for ASTrust Effectiveness ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")). This
    probability function is estimated using the doWhy tool  (Sharma et al., [2021](#bib.bib56);
    Palacio et al., [2023](#bib.bib46)). The proposed treatments ($T$) embodies Syntax
    Categories $\mathcal{C}$ such as Decision, Natural Language, or Iterative.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SCM计算因果影响，作为平均处理效果（ATE），并使用概率$p(Y|do(T))$对gpt-3 [125M]和mono-lang [2B]模型进行计算。即，我们在控制混杂因素$Z$后，估计变量$T$对$Y$的因果效应（见图[6](#S4.F6
    "Figure 6 ‣ 4.3\. Data Science Study for ASTrust Effectiveness ‣ 4\. Empirical
    Study Design ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations")）。这个概率函数是使用doWhy工具（Sharma et al., [2021](#bib.bib56);
    Palacio et al., [2023](#bib.bib46)）估计的。所提出的处理（$T$）体现了如决策、自然语言或迭代等语法类别$\mathcal{C}$。
- en: 'The first step of this validity evaluation is to obtain the global intrinsic
    accuracy. We computed the cross-entropy loss for each snippet $s$. After obtaining
    the cross-entropy loss, we estimate Pearson correlation $\rho$ and ATE for 14
    sub-categories ($\lambda$ and $\alpha$ nodes) (Tab. [4](#S5.T4 "Table 4 ‣ 5.3\.
    RQ3 ASTrust Validity ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations")). Each sub-category and its
    cross-entropy loss is correlated with four confounding variables (i.e., Cyclomatic
    Complexity, AST Levels, #AST Nodes, and Sequence Size) calculating the average
    value from the set of snippets $S$ from SyxTestbed dataset (Rodriguez-Cardenas
    et al., [2023](#bib.bib55)). The second step is to validate the obtained ATE by
    testing the SCM robustness (i.e., refutation methods  (Palacio et al., [2023](#bib.bib46))).
    We limited our exploration to the best and worst models by intrinsic accuracy
    as we observed similar correlation values across LLMs.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这一有效性评估的第一步是获得全球内在准确性。我们计算了每个片段$s$的交叉熵损失。在获得交叉熵损失后，我们估计了14个子类别（$\lambda$和$\alpha$节点）的Pearson相关系数$\rho$和ATE（表[4](#S5.T4
    "Table 4 ‣ 5.3\. RQ3 ASTrust Validity ‣ 5\. Results ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")）。每个子类别及其交叉熵损失与四个混杂变量（即圈复杂度、AST级别、#AST节点和序列大小）相关，计算从SyxTestbed数据集$S$的片段集合中的平均值（Rodriguez-Cardenas
    et al., [2023](#bib.bib55)）。第二步是通过测试SCM的稳健性（即反驳方法（Palacio et al., [2023](#bib.bib46)））来验证获得的ATE。由于我们观察到不同LLMs间相似的相关性值，我们将探索限制在内在准确性最好的和最差的模型上。
- en: 5\. Results
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结果
- en: In this section, we present our findings for human, data science, and causal
    studies. The local analysis is focused on answering [RQ[1]](#S4.I1.i1 "item RQ1
    ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy and Interpretable LLMs
    for Code through Syntax-Grounded Explanations") by using ASTrust to interpret
    concrete snippets. Similarly, we provide insights into our global analysis to
    answer [RQ[2]](#S4.I1.i2 "item RQ2 ‣ 4\. Empirical Study Design ‣ Towards More
    Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    and [RQ[3]](#S4.I1.i3 "item RQ3 ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations"), which
    incorporates the interpretation of LLMs’ performance segregated by Syntax Categories,
    a comparison of edge cases, and a causal assessment of ASTrust validity.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了关于人工、数据科学和因果研究的发现。本地分析集中于通过使用 ASTrust 解释具体代码片段来回答 [RQ[1]](#S4.I1.i1
    "项目 RQ1 ‣ 4\. 实证研究设计 ‣ 朝着更可信和可解释的 LLMs 的代码通过基于语法的解释")。类似地，我们提供了全球分析的见解，以回答 [RQ[2]](#S4.I1.i2
    "项目 RQ2 ‣ 4\. 实证研究设计 ‣ 朝着更可信和可解释的 LLMs 的代码通过基于语法的解释") 和 [RQ[3]](#S4.I1.i3 "项目
    RQ3 ‣ 4\. 实证研究设计 ‣ 朝着更可信和可解释的 LLMs 的代码通过基于语法的解释")，这些分析包括按语法类别分隔的 LLM 性能的解释、边缘案例的比较，以及
    ASTrust 有效性的因果评估。
- en: 'Before presenting the results, we point out basic stats about AST data processing:
    The average tree height of the samples in the empirical study was 30, with an
    average of 104 tokens and 166 nodes. In the human study, the four samples have
    distinct complexity levels. The smallest sample has 80 tokens, with 47 AST nodes
    and a tree of height eight. The biggest sample has a token length of 139, with
    117 AST nodes and a tree height of 14.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示结果之前，我们指出关于 AST 数据处理的基本统计信息：实证研究中样本的平均树高为 30，平均包含 104 个标记和 166 个节点。在人工研究中，四个样本具有不同的复杂性水平。最小的样本有
    80 个标记，47 个 AST 节点，树高为 8。最大的样本有 139 个标记，117 个 AST 节点，树高为 14。
- en: 5.1\. [RQ[1]](#S4.I1.i1 "item RQ1 ‣ 4\. Empirical Study Design ‣ Towards More
    Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    ASTrust Usefulness
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. [RQ[1]](#S4.I1.i1 "项目 RQ1 ‣ 4\. 实证研究设计 ‣ 朝着更可信和可解释的 LLMs 的代码通过基于语法的解释")
    ASTrust 的有用性
- en: Below, we present the results for each survey question as introduced in Sec. [4.2](#S4.SS2
    "4.2\. Human Study for ASTrust Usefulness ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations").
    Quantified responses are detailed in Tab. [2](#S5.T2 "Table 2 ‣ 5.1\. RQ1 ASTrust
    Usefulness ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable LLMs for
    Code through Syntax-Grounded Explanations"). In addition, we summarize the most
    relevant feedback received in the open-ended questions. The full human study’s
    results can be accessed in the appendix (Palacio et al., [2024](#bib.bib45)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们展示了每个调查问题的结果，如第 [4.2](#S4.SS2 "4.2\. 人工研究 ASTrust 有用性 ‣ 4\. 实证研究设计 ‣ 朝着更可信和可解释的
    LLMs 的代码通过基于语法的解释") 节介绍的。量化的回应详细列在表 [2](#S5.T2 "表 2 ‣ 5.1\. RQ1 ASTrust 有用性 ‣
    5\. 结果 ‣ 朝着更可信和可解释的 LLMs 的代码通过基于语法的解释") 中。此外，我们总结了开放性问题中收到的最相关反馈。完整的人工研究结果可以在附录中查阅（Palacio
    等，[2024](#bib.bib45)）。
- en: Table 2. Survey results for the ASTrust Local Study.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. ASTrust 本地研究的调查结果。
- en: '| Survey Question |  | Results (% answers) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 调查问题 |  | 结果（%回答） |'
- en: '| --- | --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  |  | Useful | Slightly Useful | Not useful |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 有用 | 略有用 | 不有用 |'
- en: '|  |  | $U_{SEQ}$ | 50.00 | 25.00 | 25.00 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{SEQ}$ | 50.00 | 25.00 | 25.00 |'
- en: '|  |  | $U_{AST}$ | 43.57 | 23.91 | 32.52 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST}$ | 43.57 | 23.91 | 32.52 |'
- en: '|  |  | $U_{AST[p]}$ | 39.29 | 28.57 | 32.14 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST[p]}$ | 39.29 | 28.57 | 32.14 |'
- en: '| Information Usefulness |  | $U_{AST[c]}$ | 41.66 | 20.84 | 37.50 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 信息有用性 |  | $U_{AST[c]}$ | 41.66 | 20.84 | 37.50 |'
- en: '|  |  |  | Agree | Neutral | Disagree |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 同意 | 中立 | 不同意 |'
- en: '|  |  | $U_{SEQ}$ | 42.00 | 29.00 | 29.00 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{SEQ}$ | 42.00 | 29.00 | 29.00 |'
- en: '|  |  | $U_{AST}$ | 44.00 | 28.00 | 28.00 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST}$ | 44.00 | 28.00 | 28.00 |'
- en: '|  |  | $U_{AST[p]}$ | 14.00 | 43.00 | 43.00 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST[p]}$ | 14.00 | 43.00 | 43.00 |'
- en: '| Local Explanation Complexity |  | $U_{AST[c]}$ | 67.00 | 17.00 | 16.00 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 本地解释复杂性 |  | $U_{AST[c]}$ | 67.00 | 17.00 | 16.00 |'
- en: '|  |  | $U_{SEQ}$ | 29.00 | 29.00 | 42.00 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{SEQ}$ | 29.00 | 29.00 | 42.00 |'
- en: '|  |  | $U_{AST}$ | 35.00 | 21.00 | 44.00 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST}$ | 35.00 | 21.00 | 44.00 |'
- en: '|  |  | $U_{AST[p]}$ | 57.00 | 29.00 | 14.00 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST[p]}$ | 57.00 | 29.00 | 14.00 |'
- en: '| Local Explanation Readability |  | $U_{AST[c]}$ | 17.00 | 33.00 | 50.00 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 本地解释可读性 |  | $U_{AST[c]}$ | 17.00 | 33.00 | 50.00 |'
- en: '|  |  | $U_{SEQ}$ | 57.00 | 29.00 | 14.00 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{SEQ}$ | 57.00 | 29.00 | 14.00 |'
- en: '|  |  | $U_{AST}$ | 49.80 | 27.78 | 22.42 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST}$ | 49.80 | 27.78 | 22.42 |'
- en: '|  |  | $U_{AST[p]}$ | 42.00 | 29.00 | 29.00 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST[p]}$ | 42.00 | 29.00 | 29.00 |'
- en: '| Visualization Usefulness |  | $U_{AST[c]}$ | 50.00 | 33.00 | 17.00 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 可视化的有用性 |  | $U_{AST[c]}$ | 50.00 | 33.00 | 17.00 |'
- en: '|  |  |  | Highly Reliable | Not Reliable | Impossible to Tell |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 高度可靠 | 不可靠 | 无法判断 |'
- en: '|  |  | $U_{SEQ}$ | 29.00 | 42.00 | 29.00 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{SEQ}$ | 29.00 | 42.00 | 29.00 |'
- en: '|  |  | $U_{AST}$ | 0.00 | 62.00 | 38.00 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST}$ | 0.00 | 62.00 | 38.00 |'
- en: '|  |  | $U_{AST[p]}$ | 0.00 | 57.00 | 43.00 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $U_{AST[p]}$ | 0.00 | 57.00 | 43.00 |'
- en: '| LLM’s Reliability |  | $U_{AST[c]}$ | 0.00 | 67.00 | 33.00 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| LLM 的可靠性 |  | $U_{AST[c]}$ | 0.00 | 67.00 | 33.00 |'
- en: '* bold:Highest %, background:Highest % $U_{SEQ}$ or $U_{AST}$'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '* 粗体：最高百分比，背景：最高百分比 $U_{SEQ}$ 或 $U_{AST}$'
- en: 'Metric[1]: Information Usefulness. The data reveals that $67.48\%$ of participants
    who evaluated $U_{AST}$ explanations, found the presented information useful or
    slightly useful, with a slight preference for $U_{AST[p]}$ ($67.86\%$) over $U_{AST[c]}$
    ($62.5\%$). However, $75\%$ of participants who evaluated $U_{SEQ}$ felt that
    it was useful, indicating a stronger preference towards it.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 指标[1]：信息的有用性。数据显示，$67.48\%$ 的参与者在评估 $U_{AST}$ 解释时，认为所提供的信息有用或稍微有用， $U_{AST[p]}$
    ($67.86\%$) 相比于 $U_{AST[c]}$ ($62.5\%$) 有轻微的偏好。然而，$75\%$ 的参与者在评估 $U_{SEQ}$ 时感到它有用，表明对其的偏好更强。
- en: 'Metric[2]: Local Explanation Complexity. Participants found $U_{AST}$ explanations
    slightly more complex ($44\%$) than $U_{SEQ}$ ($42\%$). In particular, $U_{AST[c]}$
    was found substantially more complex ($67\%$) than $U_{AST[p]}$. This is not surprising,
    given that complete ASTs, even for small code snippets can appear complex.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 指标[2]：局部解释的复杂性。参与者发现 $U_{AST}$ 的解释比 $U_{SEQ}$ 稍微复杂 ($44\%$ 对比 $42\%$)。特别是，$U_{AST[c]}$
    被发现比 $U_{AST[p]}$ 复杂得多 ($67\%$ 对比 $43\%$)。考虑到完整的 AST，即使是对于小代码片段也可能显得复杂，这并不令人惊讶。
- en: 'Metric[3]: Local Explanation Readability. Both $U_{AST}$ and $U_{SEQ}$ were
    found to be similarly readable: $35\%$ participants found $U_{AST}$ easy to read
    and use, compared to $29\%$ for $U_{SEQ}$. However, between the two AST types
    $U_{AST[p]}$ ($57\%$) was far preferred in contrast to $U_{AST[c]}$ ($17\%$),
    again likely due to the complexity of $U_{AST[c]}$.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 指标[3]：局部解释的可读性。 $U_{AST}$ 和 $U_{SEQ}$ 被发现具有相似的可读性：$35\%$ 的参与者认为 $U_{AST}$ 易于阅读和使用，而
    $U_{SEQ}$ 的比例为 $29\%$。然而，在两个 AST 类型之间，$U_{AST[p]}$ ($57\%$) 比 $U_{AST[c]}$ ($17\%$)
    受欢迎得多，这可能是由于 $U_{AST[c]}$ 的复杂性。
- en: 'Metric[4]: Visualization Usefulness. $U_{SEQ}$ visualization was found useful
    by more than half of the participants who evaluated it ($57\%$). Similarly, $49.8\%$
    considered the $U_{AST}$ visualizations useful, with an appreciable preference
    for $U_{AST[c]}$ ($50\%$) over $U_{AST[p]}$ ($42\%$).'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 指标[4]：可视化的有用性。超过一半的评估 $U_{SEQ}$ 的参与者 ($57\%$) 认为其可视化有用。类似地，$49.8\%$ 的人认为 $U_{AST}$
    的可视化有用，$U_{AST[c]}$ ($50\%$) 相比于 $U_{AST[p]}$ ($42\%$) 有显著的偏好。
- en: 'Metric[5]: LLM’s Reliability. The high number of participants who judged the
    LLM as unreliable suggests that all types of explanations helped them assess the
    quality of the predicted code. However, $U_{AST[c]}$ stood out, with $67\%$ participants
    favoring it. Meanwhile, $29\%$ participants felt confident about the model based
    on the information presented by $U_{SEQ}$, indicating the potential of formulating
    incorrect interpretations when using this type of explanation. Interestingly,
    a high percentage of participants ($43\%$) found that $U_{AST[p]}$ cannot help
    to conclude whether LLM is reliable.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 指标[5]：LLM 的可靠性。大量参与者将 LLM 评判为不可靠，表明所有类型的解释都有助于他们评估预测代码的质量。然而，$U_{AST[c]}$ 脱颖而出，$67\%$
    的参与者偏爱它。同时，$29\%$ 的参与者对基于 $U_{SEQ}$ 提供的信息对模型感到有信心，这表明使用这种类型的解释可能会导致错误的解释。有趣的是，高比例的参与者
    ($43\%$) 认为 $U_{AST[p]}$ 无法帮助判断 LLM 是否可靠。
- en: 'Open[1]: LLM’s Prediction Correctness. Participants attributed the cause of
    an incorrect prediction in the model’s output to syntax and semantic errors in
    both treatment and control surveys. The attribution to training bias was prevalent
    in $U_{CTR}$ as evidenced in answers such as “Model has not seen enough samples
    to differentiate between [the characters] = and ==” or “Maybe the model is trained
    in problems with similar error”. However, in $U_{SEQ}$ and $U_{AST}$ those responses
    included attribution to the low ASTrust confidence performance, such as “The probabilities
    are very low so the predictions are not correct” and “[the character] = has low
    probability score of 0.0096”. These results reveal that ASTrust explanations provided
    insightful information for the participants to judge the model’s decisions.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 'Open[1]: LLM的预测正确性。参与者将模型输出中预测错误的原因归因于处理和控制调查中的语法和语义错误。在 $U_{CTR}$ 中，归因于训练偏差的情况较为普遍，答案如“模型未见过足够的样本来区分
    [字符] = 和 ==” 或 “也许模型在处理类似错误的问题时受过训练”。然而，在 $U_{SEQ}$ 和 $U_{AST}$ 中，这些回答包括了对低ASTrust置信度表现的归因，例如“概率非常低，所以预测不正确”和“[字符]
    = 的概率分数为0.0096”。这些结果表明，ASTrust 解释为参与者判断模型的决策提供了有价值的信息。'
- en: 'Open[2]: Importance of visual explanations. Participants favored the color
    scheme and the ASTrust confidence performance associated with each token as the
    most liked elements in the visual explanations. Conversely, they disfavored the
    inclusion of certain syntax-related tokens, such as white spaces and punctuation
    marks, in the interpretability analysis. We also encountered contradictory premises:
    $U_{AST[p]}$ participants believe the explanation missed important details, while
    in $U_{AST[c]}$ participants criticized the information overload. Participants
    also suggested improving the navigation in $U_{AST}$ representations by incorporating
    a mechanism to interactively collapse AST nodes.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'Open[2]: 视觉解释的重要性。参与者认为配色方案和与每个标记相关的ASTrust置信度表现是视觉解释中最受喜爱的元素。相反，他们不喜欢在可解释性分析中包含某些语法相关的标记，例如空格和标点符号。我们还遇到了矛盾的前提：$U_{AST[p]}$
    参与者认为解释遗漏了重要细节，而 $U_{AST[c]}$ 参与者批评了信息过载。参与者还建议通过引入一个交互式折叠AST节点的机制来改善 $U_{AST}$
    表示中的导航。'
- en: Profiling. We found that the participants were well-qualified to take our survey.
    They all had some background in Machine Learning (Formal or Informal). Similarly,
    81.25% of participants were also familiar with the concept of AST.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 个人档案。我们发现参与者非常适合参与我们的调查。他们都有一定的机器学习背景（正式或非正式）。同样，81.25% 的参与者也熟悉AST的概念。
- en: '[RQ[1]](#S4.I1.i1 "item RQ1 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations"):
    Although Sequence Explanations contain useful information, AST visualizations
    were viewed most favorably among explanation types. In fact, AST-based Explanations
    were found most effective to judge LLM reliability.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ[1]](#S4.I1.i1 "item RQ1 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations"):
    Although Sequence Explanations contain useful information, AST visualizations
    were viewed most favorably among explanation types. In fact, AST-based Explanations
    were found most effective to judge LLM reliability.'
- en: 5.2\. [RQ[2]](#S4.I1.i2 "item RQ2 ‣ 4\. Empirical Study Design ‣ Towards More
    Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    ASTrust Effectiveness
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. [RQ[2]](#S4.I1.i2 "item RQ2 ‣ 4\. Empirical Study Design ‣ Towards More
    Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    ASTrust 效果
- en: To answer [RQ[2]](#S4.I1.i2 "item RQ2 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    we computed both the canonical intrinsic performance and the ASTrust interpretability
    performance for 12 LLMs (Tab. [1](#S4.T1 "Table 1 ‣ 4.1.3\. Machine Configuration.
    ‣ 4.1\. Experimental Context ‣ 4\. Empirical Study Design ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")). Fig. [7](#S5.F7
    "Figure 7 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations") depicts
    the canonical intrinsic performance for each LLM (i.e., box-plot) and the density
    canonical intrinsic performance (i.e., density plot) by model type (e.g., NL GPT-3,
    NL Codegen, Mono-Language-Type, and Multi-Language-Type). The intrinsic performance
    comprises an aggregated metric that allows us to compare models at a glance. For
    instance, on average the smallest mono-lang [110M] ($M_{9}$) has a similar intrinsic
    performance as the largest GPT-based gpt-3 [2.7B] ($M_{3}$) model with intrinsic
    performance of $0.61$ and $0.62$ respectively. After grouping the models by types,
    we observe that Mono-Language-Type models excel in the intrinsic performance with
    the highest density of $0.9$ for performance values between $(0.6-0.8)$ and an
    average intrinsic performance of $\approx 0.7$. Despite the fact canonical intrinsic
    performance can statistically describe, on average, how the model performs at
    generating code, these metrics are limited to explaining which categories are
    being predicted more confidently than others.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答[RQ[2]](#S4.I1.i2 "项 RQ2 ‣ 4\. 实证研究设计 ‣ 向更可信和可解释的代码 LLM 迈进，通过语法基础解释")，我们计算了12个LLM的典型内在性能和ASTrust可解释性性能（表[1](#S4.T1
    "表 1 ‣ 4.1.3\. 机器配置。 ‣ 4.1\. 实验背景 ‣ 4\. 实证研究设计 ‣ 向更可信和可解释的代码 LLM 迈进，通过语法基础解释")）。图[7](#S5.F7
    "图 7 ‣ 5.2\. RQ2 ASTrust 效能 ‣ 5\. 结果 ‣ 向更可信和可解释的代码 LLM 迈进，通过语法基础解释") 展示了每个LLM的典型内在性能（即，箱线图）以及按模型类型的典型内在性能密度（即，密度图）。内在性能包括一个聚合指标，使我们能够一目了然地比较模型。例如，平均而言，最小的单语言模型[110M]（$M_{9}$）与最大的基于GPT的gpt-3
    [2.7B]（$M_{3}$）模型的内在性能相似，分别为$0.61$和$0.62$。在按类型分组模型后，我们观察到单语言类型模型在内在性能上表现突出，其性能值在$(0.6-0.8)$区间的最高密度为$0.9$，平均内在性能约为$0.7$。尽管典型内在性能可以统计性地描述模型生成代码的表现，这些指标仍然有限于解释哪些类别的预测更为自信。
- en: '![Refer to caption](img/76d2ad4a910fa5609f34655bfc8b7a93.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76d2ad4a910fa5609f34655bfc8b7a93.png)'
- en: 'Figure 7. Canonical intrinsic performance for the models $M1$ to $M12$. Left:
    box-plots of performance distribution for each model. Right: density plot of performance
    by model type.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 模型$M1$到$M12$的典型内在性能。左侧：每个模型的性能分布箱线图。右侧：按模型类型的性能密度图。
- en: \Description
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: Canonical performance for each model under evaluation
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 每个评估模型的典型性能
- en: '![Refer to caption](img/ce6b3e3778d31d5d1f7fbbdad2bd0d22.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce6b3e3778d31d5d1f7fbbdad2bd0d22.png)'
- en: Figure 8. Segregated ASTrust confidence by Syntax Categories (dotted line is
    the performance threshold).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. 按语法类别划分的ASTrust置信度（虚线为性能阈值）。
- en: To assess the prediction confidence of each Syntax Category (SC) for the 12
    LLMs we present an empirical ASTrust interpretability performance value (bootstrapped
    median columns in Tab. [3](#S5.T3 "Table 3 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣
    5\. Results ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations")). Fig. [8](#S5.F8 "Figure 8 ‣ 5.2\. RQ2 ASTrust
    Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable LLMs
    for Code through Syntax-Grounded Explanations") illustrates the ASTrust interpretability
    performance segregated by Syntax Categories (SCs) for each model type. Similarly,
    Tab. [3](#S5.T3 "Table 3 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    shows bootstrapped median for each model. We set a confidence prediction threshold
    of $> across all analyses. It is worth noting that this threshold is a
    tunable parameter that can be modified to obtain tailored interpretations of model
    performance. We easily identify that Mono-Language-Type and Multi-Language-Type
    surpass our confidence threshold of 在所有分析中。值得注意的是，这个阈值是一个可调参数，可以修改以获得模型性能的定制解释。我们很容易发现Mono-Language-Type和Multi-Language-Type在所有SC中都超越了我们的置信度阈值，但数据类型除外。相反，我们观察到GPT-3类型模型在数据类型类别上面临挑战，而在迭代类别中表现优异。
- en: 'Table 3. Syntax Concept Empirical Evaluation Results (bold: best, underlined:
    worst).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '表3. 语法概念经验评估结果（**粗体**: 最佳，\ul下划线: 最差）。'
- en: '|  |  | ASTrust Interpretable Performance (bootstrapped median) |  | Extrinsic
    | Intrinsic |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ASTrust 可解释性性能（自助法中位数） |  | 外在 | 内在 |'
- en: '| LLMs |  | Data Str. | Decision | Except. | F. Prog. | Iter. | NL | Oper.
    | Scope | Testing | Data Ts. |  | BLEU-4 | CodeBLEU | Perf. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| LLMs |  | 数据结构 | 决策 | 例外 | F. Prog. | 迭代 | NL | 操作 | 范围 | 测试 | 数据传输 |  |
    BLEU-4 | CodeBLEU | 性能 |'
- en: '| $M_{1}$ |  | 0.50 | 0.52 | \ul0.43 | \ul0.49 | 0.74 | \ul0.32 | \ul0.48 |
    0.51 | 0.59 | \ul0.33 |  | 0.013 | 0.139 | 0.48 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| $M_{1}$ |  | 0.50 | 0.52 | \ul0.43 | \ul0.49 | 0.74 | \ul0.32 | \ul0.48 |
    0.51 | 0.59 | \ul0.33 |  | 0.013 | 0.139 | 0.48 |'
- en: '| $M_{2}$ |  | 0.60 | 0.61 | 0.53 | 0.62 | 0.79 | \ul0.43 | 0.57 | 0.68 | 0.68
    | \ul0.44 |  | 0.016 | 0.151 | 0.59 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| $M_{2}$ |  | 0.60 | 0.61 | 0.53 | 0.62 | 0.79 | \ul0.43 | 0.57 | 0.68 | 0.68
    | \ul0.44 |  | 0.016 | 0.151 | 0.59 |'
- en: '| $M_{3}$ |  | 0.62 | 0.63 | 0.56 | 0.66 | 0.81 | \ul0.46 | 0.60 | 0.74 | 0.70
    | \ul0.47 |  | 0.015 | 0.163 | 0.62 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| $M_{3}$ |  | 0.62 | 0.63 | 0.56 | 0.66 | 0.81 | \ul0.46 | 0.60 | 0.74 | 0.70
    | \ul0.47 |  | 0.015 | 0.163 | 0.62 |'
- en: '| $M_{4}$ |  | 0.56 | 0.57 | \ul0.45 | 0.57 | 0.77 | \ul0.39 | 0.54 | 0.64
    | 0.64 | \ul0.40 |  | 0.015 | 0.151 | 0.55 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| $M_{4}$ |  | 0.56 | 0.57 | \ul0.45 | 0.57 | 0.77 | \ul0.39 | 0.54 | 0.64
    | 0.64 | \ul0.40 |  | 0.015 | 0.151 | 0.55 |'
- en: '| $M_{5}$ |  | 0.65 | 0.65 | 0.58 | 0.68 | 0.82 | \ul0.48 | 0.61 | 0.78 | 0.72
    | 0.50 |  | 0.016 | 0.155 | 0.65 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| $M_{5}$ |  | 0.65 | 0.65 | 0.58 | 0.68 | 0.82 | \ul0.48 | 0.61 | 0.78 | 0.72
    | 0.50 |  | 0.016 | 0.155 | 0.65 |'
- en: '| $M_{6}$ |  | 0.54 | 0.55 | 0.64 | 0.60 | \ul0.60 | \ul0.40 | 0.54 | 0.71
    | 0.67 | \ul0.42 |  | 0.010 | 0.189 | 0.57 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| $M_{6}$ |  | 0.54 | 0.55 | 0.64 | 0.60 | \ul0.60 | \ul0.40 | 0.54 | 0.71
    | 0.67 | \ul0.42 |  | 0.010 | 0.189 | 0.57 |'
- en: '| $M_{7}$ |  | 0.63 | 0.72 | 0.75 | 0.70 | 0.69 | 0.51 | 0.62 | 0.83 | 0.73
    | 0.51 |  | 0.015 | 0.171 | 0.68 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| $M_{7}$ |  | 0.63 | 0.72 | 0.75 | 0.70 | 0.69 | 0.51 | 0.62 | 0.83 | 0.73
    | 0.51 |  | 0.015 | 0.171 | 0.68 |'
- en: '| $M_{8}$ |  | 0.74 | 0.79 | 0.83 | 0.81 | 0.77 | 0.65 | 0.74 | 0.91 | 0.80
    | 0.71 |  | 0.016 | 0.177 | 0.79 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| $M_{8}$ |  | 0.74 | 0.79 | 0.83 | 0.81 | 0.77 | 0.65 | 0.74 | 0.91 | 0.80
    | 0.71 |  | 0.016 | 0.177 | 0.79 |'
- en: '| $M_{9}$ |  | 0.58 | 0.58 | 0.68 | 0.66 | 0.63 | \ul0.46 | 0.57 | 0.73 | 0.69
    | \ul0.47 |  | 0.011 | 0.194 | 0.61 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| $M_{9}$ |  | 0.58 | 0.58 | 0.68 | 0.66 | 0.63 | \ul0.46 | 0.57 | 0.73 | 0.69
    | \ul0.47 |  | 0.011 | 0.194 | 0.61 |'
- en: '| $M_{10}$ |  | 0.67 | 0.67 | 0.80 | 0.76 | 0.70 | 0.59 | 0.66 | 0.82 | 0.74
    | 0.64 |  | 0.011 | 0.196 | 0.71 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| $M_{10}$ |  | 0.67 | 0.67 | 0.80 | 0.76 | 0.70 | 0.59 | 0.66 | 0.82 | 0.74
    | 0.64 |  | 0.011 | 0.196 | 0.71 |'
- en: '| $M_{11}$ |  | 0.68 | 0.76 | 0.78 | 0.76 | 0.73 | 0.57 | 0.68 | 0.86 | 0.77
    | 0.58 |  | 0.014 | 0.179 | 0.73 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| $M_{11}$ |  | 0.68 | 0.76 | 0.78 | 0.76 | 0.73 | 0.57 | 0.68 | 0.86 | 0.77
    | 0.58 |  | 0.014 | 0.179 | 0.73 |'
- en: '| $M_{12}$ |  | 0.79 | 0.84 | 0.90 | 0.85 | 0.81 | 0.73 | 0.82 | 0.94 | 0.85
    | 0.83 |  | 0.016 | 0.181 | 0.84 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| $M_{12}$ |  | 0.79 | 0.84 | 0.90 | 0.85 | 0.81 | 0.73 | 0.82 | 0.94 | 0.85
    | 0.83 |  | 0.016 | 0.181 | 0.84 |'
- en: '*Erroneous ASTrust values are in red. Confident ASTrust scores are in blue.
    Canonical values serve as a baseline.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*错误的ASTrust值用红色标出。置信的ASTrust分数用蓝色标出。规范值作为基准。*'
- en: We found that categories such as Iteration, Except, and Scope surpass our threshold
    for the majority of our LLMs under analysis. For instance, Tab. [3](#S5.T3 "Table
    3 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy and
    Interpretable LLMs for Code through Syntax-Grounded Explanations") shows the Iteration
    category consistently surpasses our threshold for all LLMs, except for multi-lang
    [110M] ($M_{6}$), which records an average median ASTrust of $0.6$ and a highest
    value of $0.82$ for codegen-nl [2B] ($M_{5}$). Notably, our smaller model gpt-3
    [125M] ($M_{1}$) still outperforms the Iteration category prediction with an average
    median of $0.74$. Finally, we note that models trained largely on code, i.e. codegen-nl
    [2B] ($M_{5}$), mono-lang [1.5B] ($M_{10}$), mono-lang [2B] ($M_{12}$), could
    predict the Data Types category with and ASTrust performance of $0.71$, $0.64$
    and $0.83$ respectively.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，像迭代、异常和范围这样的类别在我们分析的大多数LLMs中都超过了我们的阈值。例如，Tab. [3](#S5.T3 "表 3 ‣ 5.2\. RQ2
    ASTrust 有效性 ‣ 5\. 结果 ‣ 通过基于语法的解释实现更值得信赖和可解释的LLMs") 显示，迭代类别在所有LLMs中都持续超过了我们的阈值，除了
    multi-lang [110M] ($M_{6}$)，它记录的平均中位数ASTrust为 $0.6$ 和 codegen-nl [2B] ($M_{5}$)
    的最高值 $0.82$。值得注意的是，我们较小的模型 gpt-3 [125M] ($M_{1}$) 在迭代类别预测上仍然表现优于平均中位数为 $0.74$。最后，我们注意到，主要以代码训练的模型，即
    codegen-nl [2B] ($M_{5}$)、mono-lang [1.5B] ($M_{10}$)、mono-lang [2B] ($M_{12}$)，可以对数据类型类别进行预测，ASTrust表现分别为
    $0.71$、$0.64$ 和 $0.83$。
- en: By contrast, our LLMs struggle to generate good predictions for Natural language
    and Data Types categories. We can observe that only mono-lang [2B] ($M_{12}$)
    surpasses our threshold for Natural language with confidence of $0.73$, which
    is still not an outstanding probability. We attribute poor ASTrust performance
    in certain models to the nature of syntax categories like Natural Language and
    Data Types, which demand a larger input context for accurate prediction
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们的LLMs在自然语言和数据类型类别的预测上表现不佳。我们可以观察到，只有单语 [2B] ($M_{12}$) 在自然语言类别上超过了我们的阈值，置信度为
    $0.73$，这仍然不是一个突出的概率。我们将某些模型的ASTrust表现不佳归因于像自然语言和数据类型这样的语法类别，这些类别需要更大的输入上下文才能进行准确预测。
- en: '![Refer to caption](img/cb20db4dd3d8dbd2469d9765ea0be7ec.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/cb20db4dd3d8dbd2469d9765ea0be7ec.png)'
- en: Figure 9. ASTrust of Syntax Categories and Subcategories (dotted boxes) for
    corner cases ($M_{1}$ and $M_{12}$)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9. 角落案例（$M_{1}$ 和 $M_{12}$）的语法类别和子类别的ASTrust（虚线框）
- en: \Description
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Syntax Categories heatmap comparing smaller and largest models
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 语法类别热图比较了较小和较大的模型
- en: Our observations indicate that scaling LLMs’ parameters positively influences
    the prediction of SCs. This scaling observation is consistent with canonical scores
    since our largest models gpt-3 [2.7B] ($M_{3}$), codegen-nl [2B] ($M_{5}$), multi-lang
    [2B] ($M_{8}$), and mono-lang [2B] ($M_{12}$) report not only intrinsic accuracy
    that surpasses our threshold but also ASTrust confidence over $0.8$ for categories
    such as Exception, Iteration, and Scope (see Tab. [3](#S5.T3 "Table 3 ‣ 5.2\.
    RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations") in blue). For instance, the
    largest model, $M_{12}$ exhibits the highest intrinsic accuracy with an avg. median
    of $0.84$ and exceeds our threshold for each category.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的观察表明，扩大LLMs的参数对SCs的预测有积极影响。这一扩展观察与规范分数一致，因为我们的最大模型 gpt-3 [2.7B] ($M_{3}$)、codegen-nl
    [2B] ($M_{5}$)、multi-lang [2B] ($M_{8}$) 和 mono-lang [2B] ($M_{12}$) 不仅在固有准确性上超过了我们的阈值，而且在类别如异常、迭代和范围上报告了超过
    $0.8$ 的ASTrust置信度（见蓝色的Tab. [3](#S5.T3 "表 3 ‣ 5.2\. RQ2 ASTrust 有效性 ‣ 5\. 结果 ‣
    通过基于语法的解释实现更值得信赖和可解释的LLMs")）。例如，最大的模型 $M_{12}$ 展示了最高的固有准确性，平均中位数为 $0.84$，并且在每个类别上都超过了我们的阈值。
- en: By comparing our ASTrust against extrinsic metrics, we observe that $M_{12}$
    does not achieve the highest CodeBLUE score, recording a $0.181$. Thus, ASTrust
    offers additional insights into the performance of syntax categories. For example,
    while mono-lang [110M] ($M_{1}$) outperforms mono-lang [2B] ($M_{12}$) with a
    CodeBLUE of $0.194$, it struggles with inferring Natural Language and Data Types
    categories with $0.46$ and $0.47$ respectively (see Tab. [3](#S5.T3 "Table 3 ‣
    5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations")).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将我们的ASTrust与外部指标进行比较，我们观察到$M_{12}$未能获得最高的CodeBLUE分数，记录为$0.181$。因此，ASTrust提供了有关语法类别性能的额外见解。例如，虽然单语言[110M]($M_{1}$)以$0.194$的CodeBLUE表现优于单语言[2B]($M_{12}$)，但它在推断自然语言和数据类型类别时表现不佳，分别为$0.46$和$0.47$（参见Tab. [3](#S5.T3
    "Table 3 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")）。
- en: Corner Case Experiment. Fig. [9](#S5.F9 "Figure 9 ‣ 5.2\. RQ2 ASTrust Effectiveness
    ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations"), shows a heatmap with the smallest and largest
    LLMs under analysis, gpt-3 [125M] ($M_{1}$) and mono-lang [2B] ($M_{12}$) respectively.
    We selected subcategories and categories with the greatest score jumps for going
    further into the SC analysis. For instance, Data Types reported the biggest difference
    with $0.51$ meanwhile the difference between subcategories such as ‘string’ and
    ‘finally’ are $0.38$ and $0.9$, respectively. This difference suggests that model
    scaling positively impacts the ‘finally’ subcategory, while ‘string’ or ‘if_statement’
    subcategories are slightly affected by the model size. We hypothesize that the
    poor performance of the Natural Language is due to limited context windows in
    the prompt to predict this category. However, a complementary large-scale exploratory
    analysis of the proportionality types in the training and testing data is required
    beforehand to determine other causes of poor performance. We also observe that
    the Data Types is prone to errors, especially as these types may frequently appear
    at the beginning of code snippets, particularly in Python, where dynamic typing
    is prevalent. This inter-comparison (a.k.a. across models) would have been infeasible
    by just using canonical accuracy metrics.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 边角案例实验。图 [9](#S5.F9 "Figure 9 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results
    ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations") 显示了分析中最小和最大的LLM，即gpt-3 [125M]($M_{1}$)和单语言[2B]($M_{12}$)。我们选择了得分跳跃最大的子类别和类别，以进一步进行SC分析。例如，数据类型报告了最大的差异为$0.51$，同时子类别如‘string’和‘finally’的差异分别为$0.38$和$0.9$。这一差异表明模型扩展对‘finally’子类别有积极影响，而‘string’或‘if_statement’子类别则受到模型大小的轻微影响。我们假设自然语言的差性能较差是由于提示中的上下文窗口有限，难以预测该类别。然而，在确定其他性能差的原因之前，还需要对训练和测试数据中的比例类型进行补充的大规模探索性分析。我们还观察到数据类型易于出错，特别是这些类型可能频繁出现在代码片段的开头，尤其是在动态类型普遍存在的Python中。这种模型间比较（即跨模型比较）仅通过使用规范准确性指标是不可行的。
- en: '[RQ[2]](#S4.I1.i2 "item RQ2 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations"):
    ASTrust allows us to segregate the prediction performance of LLMs according to
    Syntax Categories, showing a more interpretable way of comparing models. Syntax-grounded
    explanations demonstrate, for instance, the struggle of LLMs to statistically
    learn Natural Language nested within code structures.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ[2]](#S4.I1.i2 "item RQ2 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations"):
    ASTrust allows us to segregate the prediction performance of LLMs according to
    Syntax Categories, showing a more interpretable way of comparing models. Syntax-grounded
    explanations demonstrate, for instance, the struggle of LLMs to statistically
    learn Natural Language nested within code structures.'
- en: 5.3\. [RQ[3]](#S4.I1.i3 "item RQ3 ‣ 4\. Empirical Study Design ‣ Towards More
    Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    ASTrust Validity
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. [RQ[3]](#S4.I1.i3 "item RQ3 ‣ 4\. Empirical Study Design ‣ Towards More
    Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations")
    ASTrust有效性
- en: We quantitatively demonstrate that cross-entropy loss of LLMs tends to be negatively
    impacted by ASTrust probabilities. Therefore, we can explain at syntax category
    granularity which parts of the code LLMs perform poorly (see red boxes in Tab. [3](#S5.T3
    "Table 3 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")). We showcase
    empirical evidence that the previous statement holds for correlations $\rho$ and
    causal effects $p(y|do(t))$. Tab. [4](#S5.T4 "Table 4 ‣ 5.3\. RQ3 ASTrust Validity
    ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable LLMs for Code through
    Syntax-Grounded Explanations") shows, in general, SCs (e.g., Iterative, Scope,
    or Operator) negatively influence the cross-entropy loss for our best (i.e., $M_{12}$)
    and worst (i.e., $M_{1}$) models. Negative effects indicate that the better a
    syntax category is predicted, the lower the learning error associated.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定量展示了 LLMs 的交叉熵损失倾向于受到 ASTrust 概率的负面影响。因此，我们可以在语法类别粒度上解释 LLMs 在代码中表现较差的部分（见表格
    [3](#S5.T3 "Table 3 ‣ 5.2\. RQ2 ASTrust Effectiveness ‣ 5\. Results ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations)
    中的红色框）。我们展示了先前陈述在相关性 $\rho$ 和因果效应 $p(y|do(t))$ 上成立的实证证据。表格 [4](#S5.T4 "Table 4
    ‣ 5.3\. RQ3 ASTrust Validity ‣ 5\. Results ‣ Towards More Trustworthy and Interpretable
    LLMs for Code through Syntax-Grounded Explanations") 显示，通常，SC（例如，迭代、范围或操作符）对我们最佳（即
    $M_{12}$）和最差（即 $M_{1}$）模型的交叉熵损失产生负面影响。负面效应表明，语法类别的预测越准确，学习误差越低。
- en: Table 4. ASTrust influence on Learning Error.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4. ASTrust 对学习误差的影响。
- en: '| [T] Syntax Categories |  | [Y] Learning Error |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| [T] 语法类别 |  | [Y] 学习误差 |'
- en: '| Categories | Sub-Categories |  | gpt-125 | mono-2B |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 子类别 |  | gpt-125 | mono-2B |'
- en: '| $\mathcal{C}$ | $\alpha,\lambda$ |  | $\rho$ | ATE | $\rho$ | ATE |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{C}$ | $\alpha,\lambda$ |  | $\rho$ | ATE | $\rho$ | ATE |'
- en: '|  | for_statement |  | -0.16 | -0.10 | -0.07 | -0.01 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | for_statement |  | -0.16 | -0.10 | -0.07 | -0.01 |'
- en: '| Iterative | while_statement |  | -0.05 | -0.11 | -0.03 | -0.08 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 迭代 | while_statement |  | -0.05 | -0.11 | -0.03 | -0.08 |'
- en: '|  | identifier |  | -0.56 | -1.78 | -0.80 | -2.89 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | identifier |  | -0.56 | -1.78 | -0.80 | -2.89 |'
- en: '| Natural Language | string |  | -0.31 | -0.36 | -0.43 | -0.55 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言 | string |  | -0.31 | -0.36 | -0.43 | -0.55 |'
- en: '|  | return_statement |  | -0.04 | -0.09 | -0.22 | -0.09 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | return_statement |  | -0.04 | -0.09 | -0.22 | -0.09 |'
- en: '|  | ] |  | -0.16 | -0.04 | -0.22 | -0.10 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | ] |  | -0.16 | -0.04 | -0.22 | -0.10 |'
- en: '| Scope | ) |  | -0.37 | -0.85 | -0.54 | -1.49 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 范围 | ) |  | -0.37 | -0.85 | -0.54 | -1.49 |'
- en: '| Decision | if_statement |  | -0.22 | -0.21 | -0.11 | -0.11 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 决策 | if_statement |  | -0.22 | -0.21 | -0.11 | -0.11 |'
- en: '|  | comparison_operator |  | -0.13 | 0.02 | -0.11 | 0.00 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | comparison_operator |  | -0.13 | 0.02 | -0.11 | 0.00 |'
- en: '| Operator | boolean_operator |  | -0.10 | 0.01 | -0.08 | -0.09 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 操作符 | boolean_operator |  | -0.10 | 0.01 | -0.08 | -0.09 |'
- en: '|  | for_in_clause |  | -0.03 | 0.09 | -0.03 | 0.04 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | for_in_clause |  | -0.03 | 0.09 | -0.03 | 0.04 |'
- en: '|  | if_clause |  | -0.01 | 0.19 | 0.01 | 0.13 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | if_clause |  | -0.01 | 0.19 | 0.01 | 0.13 |'
- en: '|  | lambda |  | -0.04 | 0.20 | -0.05 | 0.06 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | lambda |  | -0.04 | 0.20 | -0.05 | 0.06 |'
- en: '| Functional Programming | list_comprehension |  | -0.04 | 0.06 | -0.03 | 0.04
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 函数式编程 | 列表推导 |  | -0.04 | 0.06 | -0.03 | 0.04 |'
- en: '| Baseline |  |  |  |  |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 基准 |  |  |  |  |  |'
- en: '| Intrinsic | Avg. Accuracy |  | -0.38 | -1.60 | -0.38 | -1.78 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 内在 | 平均准确率 |  | -0.38 | -1.60 | -0.38 | -1.78 |'
- en: '* bold:Highest impact, shadowed: causal effect'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '* bold: 最大影响, shadowed: 因果效应'
- en: The most outstanding finding is that the Natural Language category has the largest
    impact on the cross-entropy loss. For example, the SC ‘identifier’ has a causal
    effect of $-1.78$ for $M_{1}$ and $-2.89$ for $M_{12}$. In contrast, Functional
    Programming categories present the lowest impact on cross-entropy loss with a
    subtle ‘lambda’ positive causal effect of $0.2$ for $M_{1}$. This subtle positive
    effect was expected as NL-based LLMs have not been fine-tuned on code corpora
    with ‘lambda’ expressions.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最突出的发现是，自然语言类别对交叉熵损失的影响最大。例如，SC ‘identifier’ 对 $M_{1}$ 的因果效应为 $-1.78$，对 $M_{12}$
    的因果效应为 $-2.89$。相比之下，函数式编程类别对交叉熵损失的影响最低，‘lambda’ 在 $M_{1}$ 的微小正向因果效应为 $0.2$。这种微小的正向效应是可以预期的，因为基于自然语言的
    LLM 尚未在包含 ‘lambda’ 表达式的代码语料库上进行微调。
- en: '[RQ[3]](#S4.I1.i3 "item RQ3 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations"):
    The cross-entropy loss of LLMs tends to be negatively impacted by ASTrust probabilities.
    This demonstrates that syntax-grounded explanations are indeed representing the
    syntax learning mechanisms of LLMs at segregated granularity.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[RQ[3]](#S4.I1.i3 "item RQ3 ‣ 4\. Empirical Study Design ‣ Towards
    More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations"):
    The cross-entropy loss of LLMs tends to be negatively impacted by ASTrust probabilities.
    This demonstrates that syntax-grounded explanations are indeed representing the
    syntax learning mechanisms of LLMs at segregated granularity.'
- en: 6\. Discussion
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 讨论
- en: 'Below, we pose three aspects for discussion: 1) some general insights ($GIs$)
    from the empirical study, 2) a logical analysis of the connection between trustworthiness
    and interpretability, and 3) the threats to validity of our approach.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下，我们提出三个讨论方面：1）来自实证研究的一些一般性洞察（$GIs$），2）信任度与可解释性之间关系的逻辑分析，以及 3）我们方法的有效性威胁。
- en: 6.1\. Empirical Study Insights
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 实证研究洞察
- en: '$GI_{1}$: Token Predictions Reliability. ASTrust relies on logit extraction
    to generate post-hoc explanations as syntax categories. If logits are wrongly
    predicted (by over/underfitting), our causal validation process detects such inconsistency
    by reducing the Average Treatment Effect (ATE) of syntax categories on the statistical
    learning error. Our Structural Causal Model (SCM) was designed to test the robustness
    and fidelity of our approach under models’ misconfigurations or unreliable performance.
    Also, as stated earlier in the paper, recent work on calibration for LLMs of code
    has illustrated that, for code completion (which subsumes the experimental settings
    in this paper), LLMs tend to be well calibrated to token probabilities/logits (Spiess
    et al., [2024](#bib.bib57)). This helps to mitigate issues that may arise due
    to model confidence and correctness being misaligned.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: $GI_{1}$：token 预测的可靠性。ASTrust 依赖于 logit 提取来生成作为语法类别的事后解释。如果 logit 被错误预测（由于过拟合/欠拟合），我们的因果验证过程通过降低语法类别对统计学习误差的平均处理效果（ATE）来检测这种不一致性。我们的结构因果模型（SCM）旨在测试在模型配置错误或性能不可靠的情况下，我们方法的鲁棒性和可靠性。此外，正如论文早前所述，最近关于代码
    LLM 校准的研究表明，对于代码补全（涵盖了本文中的实验设置），LLM 通常对 token 概率/logit 校准良好（Spiess et al., [2024](#bib.bib57)）。这有助于缓解模型信心和正确性不一致可能引发的问题。
- en: '$GI_{2}$: Syntax Aggregations Improves Explanations. Due to its granular nature,
    token-level predictions are less informative than a hierarchical aggregated level.
    BPE can make the interpretation of individual tokens much more difficult when
    code-based sequences are split into tokens that may be meaningless. We posit that
    practitioners can more easily understand syntax categories rather than individual
    tokens because these categories are already defined by context-free grammars,
    which are semantically rich. Moreover, our human study provides evidence of this
    claim since AST-based explanations were found to be easy to read and use by participants.
    AST-based explanations also capture semantics by allowing visualization of the
    full AST structure. This approach helps practitioners evaluate the model’s implementation
    more effectively by providing a clearer, structured view of the code’s semantics.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: $GI_{2}$：语法聚合改善解释。由于其粒度特性，基于 token 的预测信息不如层次聚合级别那样有用。当基于代码的序列被拆分为可能毫无意义的 token
    时，BPE 使得单个 token 的解释变得更加困难。我们认为，实践者能够更容易理解语法类别而不是单个 token，因为这些类别已由上下文无关文法定义，这些文法在语义上是丰富的。此外，我们的人工研究提供了这一主张的证据，因为参与者发现基于
    AST 的解释易于阅读和使用。基于 AST 的解释还通过允许可视化完整的 AST 结构来捕获语义。这种方法通过提供更清晰、结构化的代码语义视图，帮助实践者更有效地评估模型的实现。
- en: '$GI_{3}$: Natural Language Imbalance. Our approach indicates a poor performance
    on NL sub-categories. We hypothesize this low performance is due to an unbalanced
    distribution of NL training samples compared to other categories. Before increasing
    the context window, we believe that a better analysis would be measuring the proportionality
    of NL sub-categories on the training set and, then, fine-tuning current LLMs to
    fix possible data bias. Unfortunately, this analysis is currently out of scope
    since it demands a complementary Exploratory Data Analysis that we envision for
    future research stages.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: $GI_{3}$：自然语言不平衡。我们的方法在自然语言子类别上表现不佳。我们假设这种低表现是由于与其他类别相比，自然语言训练样本的分布不均衡。在扩大上下文窗口之前，我们认为更好的分析是测量训练集上自然语言子类别的比例，然后对当前
    LLM 进行微调，以修复可能的数据偏差。不幸的是，这一分析目前超出了我们的范围，因为它需要补充的探索性数据分析，我们预计将在未来的研究阶段进行。
- en: '$GI_{4}$: Foundational Interpretability Research. ASTrust is meant to serve
    as a more foundational approach required to guide the future development of interpretability
    tools for users of different backgrounds (e.g., researchers, students, and data
    scientists). We aimed to not only propose a formal methodology to conduct interpretability
    in our field but also perform a preliminary assessment of ASTrust ’s usefulness
    by conducting a control/treatment experiment (i.e., with and without the approach)
    on a visualization technique based on ASTrust under clearly defined qualitative
    metrics.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: $GI_{4}$：基础解释性研究。ASTrust 旨在作为一种更基础的方法，指导未来针对不同背景用户（例如研究人员、学生和数据科学家）的可解释性工具的发展。我们旨在不仅提出一个正式的方法论来进行我们领域的解释性研究，还通过在基于
    ASTrust 的可视化技术上进行控制/处理实验（即，有和没有该方法）并使用明确的定性指标，初步评估 ASTrust 的有用性。
- en: '$GI_{5}$: Contradictions about the Usefulness of Explanations. In our human
    study, we found that AST-based explanations were preferred over sequential-based
    ones. Results revealed that the AST-partial representation was considered more
    useful than AST-Complete, as it presents the AST representation and ASTrust confidence
    performance only for the generated portion of the code. However, the feedback
    received in the open-ended questions revealed contradictory opinions. Some participants
    indicated that the AST-partial representation missed important details, while
    others felt that the AST-Complete representation was excessively detailed. These
    findings suggest the need for more tailored representations for explanations,
    aiming to present useful information while maintaining readability. We envision
    incorporating ASTrust into a tool that adds interactivity to navigate the explanations.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: $GI_{5}$：关于解释有用性的矛盾。在我们的用户研究中，我们发现基于AST的解释比基于序列的解释更受欢迎。结果表明，AST-部分表示被认为比AST-完整表示更有用，因为它仅针对生成的代码部分展示AST表示和ASTrust置信度表现。然而，在开放式问题中收到的反馈显示出矛盾的意见。一些参与者指出AST-部分表示遗漏了重要细节，而另一些则觉得AST-完整表示过于详细。这些发现表明需要更具针对性的解释表示，旨在呈现有用的信息，同时保持可读性。我们设想将ASTrust纳入一个工具中，为解释添加互动性。
- en: 6.2\. Trustworthiness & Interpretability Connection
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. **可信度**与**可解释性**的联系
- en: We outline two premises based on state-of-the-art definitions of trustworthiness
    and direct observations from our quantitative and qualitative analyses. Then,
    we use logical deduction supported by documented and empirical evidence to link
    the concept of trustworthiness with ASTrust highlighting the significance of syntax-grounded
    explanations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据最新的**可信度**定义和我们定量与定性分析的直接观察，概述了两个前提。然后，我们利用有文献记录和实证证据支持的逻辑推理，将**可信度**的概念与ASTrust联系起来，强调基于语法的解释的重要性。
- en: 'Premise[1]: Interpretability is a cornerstone for trustworthiness in Language
    Models for Code (LLMs). The interpretability field enhances transparency and provides
    insights into the decision-making process serving as a key factor in fostering
    practitioner trust and adoption. In the realm of Deep Learning for Software Engineering
    (DL4SE)  (Watson et al., [2021](#bib.bib64)), the significance of interpretability
    in models to engender trust cannot be overstated. Jiaming et al. (Ji et al., [2024](#bib.bib24))
    underscore interpretability as a pivotal element in aligned models, integral to
    the RICE principle alongside Robustness, Controllability, and Ethicality. By enhancing
    transparency in the decision-making process, interpretability plays a crucial
    role in building trust, a sentiment echoed by Weller et al., who stress the need
    to extend transparency beyond algorithms to foster trust  (Weller, [2019](#bib.bib66)).
    The dilemma between accuracy and interpretability, claimed by Lundberg et al. (Lundberg
    and Lee, [2017](#bib.bib36)), is magnified by the challenge posed by large, complex
    models that even experts find difficult to interpret. A user study with UX and
    design practitioners supports this notion, revealing that explanations are sought
    to gain deeper insights into AI tool decision-making, providing a remedy for the
    “black box” perception and contributing to user trust and adoption  (Liao et al.,
    [2020](#bib.bib30)). Therefore, the indisputable importance of interpretability
    in LLM decision-making lies in its pivotal role in establishing trustworthiness.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 前提[1]：**可解释性**是代码语言模型（LLMs）中**可信度**的基石。**可解释性**领域提高了透明度，并提供了对决策过程的洞察，作为促进从业者信任和采用的关键因素。在深度学习软件工程（DL4SE）（Watson等，[2021](#bib.bib64)）领域，模型中的**可解释性**对建立信任的重要性不容忽视。Jiaming等（Ji等，[2024](#bib.bib24)）强调**可解释性**作为对齐模型中的关键元素，与稳健性、可控性和伦理性一起构成RICE原则。通过提高决策过程的透明度，**可解释性**在建立信任方面发挥着关键作用，这一点得到了Weller等的认可，他们强调需要将透明度扩展到算法之外以促进信任（Weller，[2019](#bib.bib66)）。Lundberg等（Lundberg和Lee，[2017](#bib.bib36)）所声称的准确性与**可解释性**之间的困境，被大型复杂模型的挑战所放大，甚至专家也觉得难以解释。一项与UX和设计从业者的用户研究支持这一观点，显示出解释被用来深入了解AI工具的决策过程，为“黑箱”认知提供了补救措施，并有助于用户信任和采纳（Liao等，[2020](#bib.bib30)）。因此，**可解释性**在LLM决策中的无可争议的重要性在于它在建立**可信度**中的核心作用。
- en: 'Premise[2]: ASTrust improves interpretability. It is feasible to segregate
    intrinsic metrics (i.e., standard accuracy) into interpretable Syntax Categories
    revealing the LLMs’ inner workings concerning code structure and contributing
    towards interpretability. By conducting extensive qualitative and quantitative
    studies involving 12 prominent LLMs, we have demonstrated the effectiveness of
    ASTrust in enhancing interpretability. We do not claim that our set of categories
    is complete; however, we consider that a good alignment of the generated categories
    by the LLM with the ones expected by humans configures a good explanation (Ghorbani
    et al., [2019](#bib.bib21)). Our ASTrust clusters tokens to meaningful categories
    that are easier for human concept association. Furthermore, we uncovered valuable
    insights, such as the causal influence of AST categories on the cross-entropy
    loss of LLMs after accounting for confounding factors. Our human study participants
    attested to the usefulness of our ASTrust in explaining the predictions of Python
    code snippets by a LLM [5.1](#S5.SS1 "5.1\. RQ1 ASTrust Usefulness ‣ 5\. Results
    ‣ Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded
    Explanations"). By breaking down intrinsic metrics into segregated and interpretable
    terminal and non-terminal nodes, our approach not only enhances the understandability
    of LLMs but also unveils crucial insights into the inner workings of syntax elements.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 前提[2]：ASTrust 提高了可解释性。将内在指标（即标准准确性）划分为可解释的语法类别，揭示 LLM 关于代码结构的内部工作原理，并有助于提高可解释性是可行的。通过对
    12 个显著 LLM 进行广泛的定性和定量研究，我们展示了 ASTrust 在增强可解释性方面的有效性。我们不声称我们的类别集合是完整的；然而，我们认为 LLM
    生成的类别与人类预期的类别良好对齐，构成了一个良好的解释（Ghorbani et al., [2019](#bib.bib21)）。我们的 ASTrust
    将标记聚类到对人类概念关联更为友好的有意义类别中。此外，我们发现了有价值的见解，例如在考虑混杂因素后，AST 类别对 LLM 的交叉熵损失的因果影响。我们的人工研究参与者证明了
    ASTrust 在解释 LLM 对 Python 代码片段的预测方面的有用性 [5.1](#S5.SS1 "5.1. RQ1 ASTrust 有用性 ‣ 5.
    结果 ‣ 通过语法基础解释，走向更可信赖和可解释的 LLMs")。通过将内在指标分解为分离的和可解释的终结节点和非终结节点，我们的方法不仅增强了 LLM 的可理解性，还揭示了语法元素内部工作的重要见解。
- en: Conclusion. Given the first premise that interpretability is fundamental for
    trustworthiness in LLMs, supported by several shreds of evidence, and from the
    second premise asserting that ASTrust enhances interpretability by segregating
    intrinsic metrics into interpretable syntax categories and subcategories, collectively
    supports the fact that ASTrust contributes to the improvement of trustworthiness
    in LLMs for Code using syntax-grounded explanations.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 结论。鉴于第一个前提是可解释性对 LLM 的可信赖性至关重要，并有若干证据支持，以及第二个前提主张 ASTrust 通过将内在指标分解为可解释的语法类别和子类别来增强可解释性，两者共同支持
    ASTrust 有助于通过语法基础解释提高 LLM 在代码中的可信赖性。
- en: 6.3\. Threats to Validity
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. 有效性威胁
- en: Threats to construct validity concern the intentionality of ASTrust in providing
    useful explanations. Instead of attempting to disentangle information represented
    between the layers learned by LLMs (i.e., probing  (López et al., [2022](#bib.bib37))),
    ASTrust focuses on conceptually mapping LLMs’ code predictions to present the
    accuracy in a segregated way. We quantitatively and qualitatively validated the
    extent to which ASTrust is interpretable through causal analyses and a human study.
    While we cannot claim that the results from our study generalize beyond the population
    of users that participated in our study, our participants represent a diverse
    range of backgrounds mitigating this threat. Nonetheless, the purpose of our study
    was to conduct a preliminary assessment of ASTrust representations. As such, all
    code completion scenarios were designed to include a syntax or semantic error
    since we assessed how useful our approach is in assisting users in understanding
    models’ incorrect behavior, increasing the reliability of our findings.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对构建有效性的威胁涉及到ASTrust在提供有用解释方面的意图性。ASTrust不尝试解开LLMs学习层之间所表示的信息（即探测（López等，[2022](#bib.bib37)）），而是专注于概念性地将LLMs的代码预测映射，以分隔的方式呈现准确性。我们通过因果分析和人类研究定量和定性地验证了ASTrust的可解释程度。虽然我们不能声称我们的研究结果能推广到参与我们研究的用户群体之外，但我们的参与者代表了背景多样化的群体，从而减轻了这一威胁。尽管如此，我们研究的目的是对ASTrust表示进行初步评估。因此，所有代码补全场景都设计了语法或语义错误，因为我们评估了我们的方法在帮助用户理解模型不正确行为方面的有效性，从而提高了我们发现的可靠性。
- en: Threats to internal validity refer to the degree of confidence in which the
    ASTrust study results are reliable. Firstly, in our causal study, the potential
    for unidentified confounders in the code may bias the causal relationship between
    cross-entropy loss and the Syntax Categories. That is why we ensured the robustness
    of the Structural Causal Model by performing placebo refutations, which involves
    simulating unrelated treatments and then re-estimating the causal effects. Secondly,
    we used rigorous statistical techniques such as bootstrapping to guarantee a consistent
    comparison between aggregated Token-Level Predictions by syntax elements.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对内部有效性的威胁涉及对ASTrust研究结果可靠性的信心程度。首先，在我们的因果研究中，代码中未识别的混杂因素可能会影响交叉熵损失与语法类别之间的因果关系。这就是为什么我们通过进行安慰剂反驳来确保结构因果模型的稳健性，该方法包括模拟无关的处理，然后重新估计因果效应。其次，我们使用了如自助法等严格的统计技术，以确保通过语法元素对汇总的Token-Level
    Predictions进行一致的比较。
- en: Threats to external validity represent the extent to which ASTrust can be used
    to contextualize the performance of other LLMs or datasets. We excluded GPT-4
    based models from our empirical experiments due to the constraints of the current
    OpenAI API, which restricts access to softmax layers values (a key factor in the
    intrinsic evaluations). While our evaluation relied on decoder-only based models,
    ASTrust can also be used to interpret encoders and other types of auto-regressive
    architectures. Finally, our SyxTestbed dataset may not contain enough samples
    to represent all the syntax categories or Python project attributes fairly. Nonetheless,
    we designed a data mining pipeline to guarantee the diversity of collected samples.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 对外部有效性的威胁代表了ASTrust在将其他LLMs或数据集的性能背景化时的适用程度。由于当前OpenAI API的限制（限制了对softmax层值的访问，这是内在评估的关键因素），我们在实证实验中排除了基于GPT-4的模型。虽然我们的评估依赖于仅解码器模型，但ASTrust也可以用于解释编码器和其他类型的自回归架构。最后，我们的SyxTestbed数据集可能不包含足够的样本来公平地代表所有语法类别或Python项目属性。尽管如此，我们设计了一个数据挖掘管道，以保证收集样本的多样性。
- en: 7\. Lessons Learned & Conclusions
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 经验教训与结论
- en: 'Lesson[1]: Aggregated metrics may give false impressions about LLMs’ capabilities.
    The research community should incentivize researchers to report AI4SE results
    in a granular way, as opposed to more traditional aggregated accuracy metrics.
    After controlling for code confounders, we demonstrated that segregated syntax
    elements influence the cross-entropy loss of LLMs. This influence persists across
    models at different parameter sizes and fine-tuning strategies. Syntax information
    is also relevant for any posterior static analysis of code enabling further evaluations
    of LLMs in downstream tasks that entail elements of software design (e.g., refactoring).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 课程[1]：聚合指标可能会给LLMs的能力留下错误印象。研究界应激励研究人员以更细致的方式报告AI4SE结果，而不是传统的聚合准确率指标。在控制代码混杂因素后，我们展示了分隔的语法元素对LLMs交叉熵损失的影响。这种影响在不同参数规模和微调策略的模型中都存在。语法信息对于任何后续的代码静态分析也很重要，从而使得对LLMs在涉及软件设计元素（例如重构）的下游任务中的进一步评估成为可能。
- en: 'Lesson[2]: New interpretability methods are required to enable trustworthiness.
    In our studies, we have noted an absence of a concrete definition for the term
    trust in the Software Engineering research. However, several researchers have
    highlighted the importance of establishing trust in work on AI4SE. Research has
    also shown that interpretability is one of the keys to improving trustworthiness,
    but at the same time, there is a scarcity of interpretable methods linked to trustworthiness.
    Despite this limitation, surveyed participants agreed that ASTrust was useful
    to understand why and how a LLM produced certain errors in code-completion tasks.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 课程[2]：需要新的解释方法以实现可信度。在我们的研究中，我们注意到软件工程研究中缺乏对“信任”一词的具体定义。然而，几位研究人员强调了在AI4SE研究中建立信任的重要性。研究还表明，解释性是提高可信度的关键之一，但同时与可信度相关的可解释方法稀缺。尽管存在这种限制，受访者一致认为ASTrust在理解LLM在代码补全任务中产生某些错误的原因和方式方面是有用的。
- en: 'Lesson[3]: Grounding model explanations in the relationship between syntactic
    structures and prediction confidence is useful. It is feasible to segregate intrinsic
    metrics (i.e., standard accuracy) into interpretable Syntax Categories revealing
    the LLMs’ inner workings concerning code structure and contributing towards interpretability.
    By conducting extensive qualitative and quantitative studies involving 12 prominent
    LLMs, we have demonstrated the effectiveness of ASTrust in enhancing interpretability.
    We do not claim that our set of categories is complete; however, we consider that
    a good alignment of the generated categories by the LLM with the ones expected
    by humans configures a good explanation (Ghorbani et al., [2019](#bib.bib21)).
    Our ASTrust clusters tokens to meaningful categories that are easier for human
    concept association. Furthermore, we uncovered valuable insights, such as the
    causal influence of AST categories on the cross-entropy loss of LLMs after accounting
    for confounding factors. Our human study participants attested to the usefulness
    of our ASTrust in explaining the predictions of Python code snippets by a LLM
    [5.1](#S5.SS1 "5.1\. RQ1 ASTrust Usefulness ‣ 5\. Results ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations"). By breaking
    down intrinsic metrics into segregated and interpretable terminal and non-terminal
    nodes, our approach not only enhances the understandability of LLMs but also unveils
    crucial insights into the inner workings of syntax elements.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 课程[3]：将模型解释建立在句法结构与预测置信度之间的关系上是有用的。将内在指标（即标准准确率）分解为可解释的句法类别，从而揭示LLMs在代码结构方面的内部工作，并有助于提高解释性，这是可行的。通过对12个主要LLMs进行广泛的定性和定量研究，我们展示了ASTrust在增强解释性方面的有效性。我们并不声称我们的类别集合是完整的；然而，我们认为LLM生成的类别与人类预期类别的良好对齐构成了一个好的解释 (Ghorbani
    et al., [2019](#bib.bib21))。我们的ASTrust将令牌聚类到更易于人类概念关联的有意义类别中。此外，我们发现了有价值的见解，例如在考虑混杂因素后，AST类别对LLMs交叉熵损失的因果影响。我们的人工研究参与者证实了ASTrust在解释LLM预测Python代码片段中的有用性
    [5.1](#S5.SS1 "5.1\. RQ1 ASTrust Usefulness ‣ 5\. Results ‣ Towards More Trustworthy
    and Interpretable LLMs for Code through Syntax-Grounded Explanations")。通过将内在指标分解为隔离和可解释的终结和非终结节点，我们的方法不仅增强了LLMs的可理解性，还揭示了语法元素内部工作的重要见解。
- en: 'Lesson[4]: The usability of proposed techniques must be further evaluated for
    industry adoption. We adapted the non-mathematical definition of interpretability
    by Doshi-Velez & Kim  (Doshi-Velez and Kim, [[n. d.]](#bib.bib14)), Molnar  (Molnar
    et al., [2020](#bib.bib42)) and Miller  (Miller, [2018](#bib.bib38)) to the field
    of AI4SE (Watson et al., [2021](#bib.bib64)). However, as our preliminary human
    study suggests, ASTrust solution is incomplete until being extensively evaluated
    for industry settings.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 课程[4]：必须进一步评估所提技术的可用性以便于工业应用。我们将Doshi-Velez & Kim  (Doshi-Velez and Kim, [[n.
    d.]](#bib.bib14))、Molnar  (Molnar et al., [2020](#bib.bib42)) 和Miller  (Miller,
    [2018](#bib.bib38))提出的非数学化解释性定义调整到AI4SE领域 (Watson et al., [2021](#bib.bib64))。然而，正如我们初步的人类研究所示，ASTrust
    解决方案在经过广泛评估工业设置之前仍不完整。
- en: 'Artifact Availability: Experimental data, curated datasets, source code, and
    complementary statistical analysis used in this research are published in an open-source
    repository (Palacio et al., [2024](#bib.bib45)).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 文献可用性：本研究中使用的实验数据、精心策划的数据集、源代码和补充统计分析已发布在一个开源仓库中 (Palacio et al., [2024](#bib.bib45))。
- en: References
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: big (2024) 2024. Bigquery Dataset. [https://cloud.google.com/bigquery](https://cloud.google.com/bigquery)
    [Accessed 23-03-2024].
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: big (2024) 2024. Bigquery 数据集。[https://cloud.google.com/bigquery](https://cloud.google.com/bigquery)
    [访问日期 2024年3月23日]。
- en: 'noa (2024) 2024. Qualtrics XM: The Leading Experience Management Software.
    [https://www.qualtrics.com/](https://www.qualtrics.com/) [Accessed 23-03-2024].'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: noa (2024) 2024. Qualtrics XM：领先的体验管理软件。[https://www.qualtrics.com/](https://www.qualtrics.com/)
    [访问日期 2024年3月23日]。
- en: Ahmad et al. ([n. d.]) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray,
    and Kai-Wei Chang. [n. d.]. Unified Pre-training for Program Understanding and
    Generation. arXiv:2103.06333 [cs] [http://arxiv.org/abs/2103.06333](http://arxiv.org/abs/2103.06333)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿赫迈德等人 ([n. d.]) 瓦西·乌丁·阿赫迈德，赛卡特·查克拉博提，拜沙基·雷和凯-维·张。[n. d.]. 统一预训练用于程序理解和生成。arXiv:2103.06333
    [cs] [http://arxiv.org/abs/2103.06333](http://arxiv.org/abs/2103.06333)
- en: 'Baltes and Ralph (2021) Sebastian Baltes and Paul Ralph. 2021. Sampling in
    Software Engineering Research: A Critical Review and Guidelines. [https://doi.org/10.48550/arXiv.2002.07764](https://doi.org/10.48550/arXiv.2002.07764)
    arXiv:2002.07764 [cs].'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴尔特斯和拉尔夫 (2021) 塞巴斯蒂安·巴尔特斯和保罗·拉尔夫。2021。软件工程研究中的抽样：批判性审查和指南。[https://doi.org/10.48550/arXiv.2002.07764](https://doi.org/10.48550/arXiv.2002.07764)
    arXiv:2002.07764 [cs]。
- en: 'Brunsfeld et al. (2023) Max Brunsfeld, Andrew Hlynskyi, Patrick Thomson, Josh
    Vera, Phil Turnbull, et al. 2023. tree-sitter/tree-sitter: v0.20.8. [https://doi.org/10.5281/zenodo.7798573](https://doi.org/10.5281/zenodo.7798573)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '布伦斯费尔德等人 (2023) 马克斯·布伦斯费尔德，安德鲁·赫林斯基，帕特里克·汤姆森，乔什·维拉，菲尔·特恩布尔等人。2023。tree-sitter/tree-sitter:
    v0.20.8。[https://doi.org/10.5281/zenodo.7798573](https://doi.org/10.5281/zenodo.7798573)'
- en: Burnell et al. ([n. d.]) Ryan Burnell, Wout Schellaert, John Burden, Tomer D.
    Ullman, Fernando Martinez-Plumed, et al. [n. d.]. Rethink reporting of evaluation
    results in AI. 380, 6641 ([n. d.]), 136–138. [https://doi.org/10.1126/science.adf6369](https://doi.org/10.1126/science.adf6369)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伯内尔等人 ([n. d.]) 瑞安·伯内尔，沃特·谢拉特，约翰·伯登，托梅尔·D·乌尔曼，费尔南多·马丁内斯-普卢梅德等人。[n. d.]. 重新思考AI评估结果的报告。380,
    6641 ([n. d.]), 136–138。[https://doi.org/10.1126/science.adf6369](https://doi.org/10.1126/science.adf6369)
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, et al. 2021. Evaluating Large Language Models Trained
    on Code. (2021). [http://arxiv.org/abs/2107.03374](http://arxiv.org/abs/2107.03374)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 (2021) 马克·陈，杰瑞·特沃雷克，赫吴·俊，齐明·袁，亨里克·庞德·德·奥利维拉·平托等人。2021。评估训练于代码的大型语言模型。(2021)。[http://arxiv.org/abs/2107.03374](http://arxiv.org/abs/2107.03374)
- en: Chen et al. ([n. d.]) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, et al. [n. d.]. Evaluating Large Language Models Trained
    on Code. ([n. d.]). arXiv:2107.03374 [http://arxiv.org/abs/2107.03374](http://arxiv.org/abs/2107.03374)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 ([n. d.]) 马克·陈，杰瑞·特沃雷克，赫吴·俊，齐明·袁，亨里克·庞德·德·奥利维拉·平托等人。[n. d.]. 评估训练于代码的大型语言模型。([n.
    d.]). arXiv:2107.03374 [http://arxiv.org/abs/2107.03374](http://arxiv.org/abs/2107.03374)
- en: 'Chen et al. (2019) Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël
    Pouchet, Denys Poshyvanyk, et al. 2019. SEQUENCER: Sequence-to-Sequence Learning
    for End-to-End Program Repair. *IEEE Transactions on Software Engineering* (2019),
    1–1. [https://doi.org/10.1109/TSE.2019.2940179](https://doi.org/10.1109/TSE.2019.2940179)'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 (2019) 子敏·陈，史蒂夫·詹姆斯·科姆鲁施，米歇尔·图法诺，路易斯-诺埃尔·普歇，丹尼斯·波希瓦尼克等人。2019。SEQUENCER：端到端程序修复的序列到序列学习。*IEEE
    软件工程学报* (2019)，1–1。[https://doi.org/10.1109/TSE.2019.2940179](https://doi.org/10.1109/TSE.2019.2940179)
- en: Ciniselli et al. (2021) Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys
    Poshyvanyk, Massimiliano Di Penta, et al. 2021. An Empirical Study on the Usage
    of BERT Models for Code Completion. *CoRR* abs/2103.07115 (2021). arXiv:2103.07115
    [https://arxiv.org/abs/2103.07115](https://arxiv.org/abs/2103.07115)
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciniselli 等人 (2021) Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys
    Poshyvanyk, Massimiliano Di Penta 等人. 2021. 关于BERT模型用于代码补全的实证研究. *CoRR* abs/2103.07115
    (2021). arXiv:2103.07115 [https://arxiv.org/abs/2103.07115](https://arxiv.org/abs/2103.07115)
- en: 'Cito et al. (2022) Jürgen Cito, Isil Dillig, Vijayaraghavan Murali, and Satish
    Chandra. 2022. Counterfactual explanations for models of code. In *Proceedings
    of the 44th international conference on software engineering: software engineering
    in practice*. 125–134.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cito 等人 (2022) 朱根·Cito, Isil Dillig, Vijayaraghavan Murali, 和 Satish Chandra.
    2022. 代码模型的反事实解释. 收录于*第44届国际软件工程会议：实践中的软件工程*，125–134.
- en: del Bianco et al. (2011) Vieri del Bianco, Luigi Lavazza, Sandro Morasca, and
    Davide Taibi. 2011. A Survey on Open Source Software Trustworthiness. *IEEE Software*
    28, 5 (2011), 67–75. [https://doi.org/10.1109/MS.2011.93](https://doi.org/10.1109/MS.2011.93)
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: del Bianco 等人 (2011) Vieri del Bianco, Luigi Lavazza, Sandro Morasca 和 Davide
    Taibi. 2011. 关于开源软件可信度的调查. *IEEE Software* 28, 5 (2011), 67–75. [https://doi.org/10.1109/MS.2011.93](https://doi.org/10.1109/MS.2011.93)
- en: Doshi-Velez and Kim ([n. d.]) Finale Doshi-Velez and Been Kim. [n. d.]. Towards
    A Rigorous Science of Interpretable Machine Learning. ([n. d.]), 1–13. Issue Ml.
    arXiv:1702.08608 [http://arxiv.org/abs/1702.08608](http://arxiv.org/abs/1702.08608)
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doshi-Velez 和 Kim ([n. d.]) Finale Doshi-Velez 和 Been Kim. [n. d.]. 朝着一种严格的可解释机器学习科学.
    ([n. d.]), 1–13. 问题 Ml. arXiv:1702.08608 [http://arxiv.org/abs/1702.08608](http://arxiv.org/abs/1702.08608)
- en: Doshi-Velez and Kim (2017) Finale Doshi-Velez and Been Kim. 2017. Towards A
    Rigorous Science of Interpretable Machine Learning. Ml (2017), 1–13. [http://arxiv.org/abs/1702.08608](http://arxiv.org/abs/1702.08608)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doshi-Velez 和 Kim (2017) Finale Doshi-Velez 和 Been Kim. 2017. 朝着一种严格的可解释机器学习科学.
    Ml (2017), 1–13. [http://arxiv.org/abs/1702.08608](http://arxiv.org/abs/1702.08608)
- en: Doshi-Velez and Kim (2018) Finale Doshi-Velez and Been Kim. 2018. Considerations
    for Evaluation and Generalization in Interpretable Machine Learning. (2018), 3–17.
    [https://doi.org/10.1007/978-3-319-98131-4{_}1](https://doi.org/10.1007/978-3-319-98131-4%7B_%7D1)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doshi-Velez 和 Kim (2018) Finale Doshi-Velez 和 Been Kim. 2018. 可解释机器学习中的评估与泛化的考虑.
    (2018)，3–17. [https://doi.org/10.1007/978-3-319-98131-4{_}1](https://doi.org/10.1007/978-3-319-98131-4%7B_%7D1)
- en: 'Flora et al. ([n. d.]) Montgomery Flora, Corey Potvin, Amy McGovern, and Shawn
    Handler. [n. d.]. Comparing Explanation Methods for Traditional Machine Learning
    Models Part 1: An Overview of Current Methods and Quantifying Their Disagreement.
    arXiv:2211.08943 [physics, stat] [http://arxiv.org/abs/2211.08943](http://arxiv.org/abs/2211.08943)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flora 等人 ([n. d.]) Montgomery Flora, Corey Potvin, Amy McGovern 和 Shawn Handler.
    [n. d.]. 比较传统机器学习模型的解释方法 第1部分：当前方法概述及其分歧的量化. arXiv:2211.08943 [physics, stat]
    [http://arxiv.org/abs/2211.08943](http://arxiv.org/abs/2211.08943)
- en: 'Fu et al. (2023) Michael Fu, Van Nguyen, Chakkrit Kla Tantithamthavorn, Trung
    Le, and Dinh Phung. 2023. VulExplainer: A Transformer-Based Hierarchical Distillation
    for Explaining Vulnerability Types. *IEEE Transactions on Software Engineering*
    49, 10 (2023), 4550–4565. [https://doi.org/10.1109/TSE.2023.3305244](https://doi.org/10.1109/TSE.2023.3305244)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人 (2023) Michael Fu, Van Nguyen, Chakkrit Kla Tantithamthavorn, Trung Le
    和 Dinh Phung. 2023. VulExplainer：一种基于变换器的层次化蒸馏方法，用于解释漏洞类型. *IEEE Transactions
    on Software Engineering* 49, 10 (2023), 4550–4565. [https://doi.org/10.1109/TSE.2023.3305244](https://doi.org/10.1109/TSE.2023.3305244)
- en: Gao et al. (2021) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, et al. 2021. Codeparrot. (2021). [https://github.com/huggingface/blog/blob/main/codeparrot.md](https://github.com/huggingface/blog/blob/main/codeparrot.md)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2021) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe 等人. 2021. Codeparrot. (2021). [https://github.com/huggingface/blog/blob/main/codeparrot.md](https://github.com/huggingface/blog/blob/main/codeparrot.md)
- en: 'Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, et al. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling.
    arXiv:cs.CL/2101.00027'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe 等人. 2020. The Pile：一个800GB的多样化文本数据集，用于语言建模. arXiv:cs.CL/2101.00027
- en: Ghorbani et al. (2019) Amirata Ghorbani, James Wexler, James Zou, and Been Kim.
    2019. Towards Automatic Concept-based Explanations. [http://arxiv.org/abs/1902.03129](http://arxiv.org/abs/1902.03129)
    arXiv:1902.03129 [cs, stat].
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghorbani 等人 (2019) Amirata Ghorbani, James Wexler, James Zou 和 Been Kim. 2019.
    朝着自动化的概念性解释. [http://arxiv.org/abs/1902.03129](http://arxiv.org/abs/1902.03129)
    arXiv:1902.03129 [cs, stat].
- en: Hopcroft et al. (2006) John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman.
    2006. *Introduction to Automata Theory, Languages, and Computation (3rd Edition)*.
    Addison-Wesley Longman Publishing Co., Inc., USA.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopcroft 等（2006）John E. Hopcroft、Rajeev Motwani 和 Jeffrey D. Ullman。2006。*自动机理论、语言与计算导论（第3版）*。Addison-Wesley
    Longman Publishing Co., Inc., USA.
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. 2019. CodeSearchNet challenge: Evaluating the state of
    semantic code search. *arXiv preprint arXiv:1909.09436* (2019).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Husain 等（2019）Hamel Husain、Ho-Hsiang Wu、Tiferet Gazit、Miltiadis Allamanis 和
    Marc Brockschmidt。2019。CodeSearchNet 挑战：评估语义代码搜索的现状。*arXiv 预印本 arXiv:1909.09436*（2019）。
- en: 'Ji et al. (2024) Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao
    Lou, et al. 2024. AI Alignment: A Comprehensive Survey. arXiv:cs.AI/2310.19852'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等（2024）Jiaming Ji、Tianyi Qiu、Boyuan Chen、Borong Zhang、Hantao Lou 等。2024。AI
    对齐：综合调查。arXiv:cs.AI/2310.19852
- en: Kalai and Samet (1983) Ehud Kalai and Dov Samet. 1983. On weighted Shapley values.
    *International Journal of Game Theory* 16 (1983), 205–222. [https://api.semanticscholar.org/CorpusID:9418424](https://api.semanticscholar.org/CorpusID:9418424)
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalai 和 Samet（1983）Ehud Kalai 和 Dov Samet。1983。关于加权 Shapley 值。*国际博弈论期刊* 16（1983），205–222。
    [https://api.semanticscholar.org/CorpusID:9418424](https://api.semanticscholar.org/CorpusID:9418424)
- en: 'Karampatsis et al. (2020) Rafael Michael Karampatsis, Hlib Babii, Romain Robbes,
    Charles Sutton, and Andrea Janes. 2020. Big code != big vocabulary: Open-vocabulary
    models for source code. *Proceedings - International Conference on Software Engineering*
    (2020), 1073–1085. [https://doi.org/10.1145/3377811.3380342](https://doi.org/10.1145/3377811.3380342)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karampatsis 等（2020）Rafael Michael Karampatsis、Hlib Babii、Romain Robbes、Charles
    Sutton 和 Andrea Janes。2020。大代码 != 大词汇量：开源词汇模型。*国际软件工程会议论文集*（2020），1073–1085。 [https://doi.org/10.1145/3377811.3380342](https://doi.org/10.1145/3377811.3380342)
- en: Karpathy et al. (2015) Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.
    Visualizing and Understanding Recurrent Networks. *CoRR* abs/1506.02078 (2015).
    arXiv:1506.02078 [http://arxiv.org/abs/1506.02078](http://arxiv.org/abs/1506.02078)
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy 等（2015）Andrej Karpathy、Justin Johnson 和 Fei-Fei Li。2015。可视化和理解递归网络。*CoRR*
    abs/1506.02078（2015）。arXiv:1506.02078 [http://arxiv.org/abs/1506.02078](http://arxiv.org/abs/1506.02078)
- en: 'Kim et al. (2018) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James
    Wexler, et al. 2018. Interpretability beyond feature attribution: Quantitative
    Testing with Concept Activation Vectors (TCAV). *35th International Conference
    on Machine Learning, ICML 2018* 6 (2018), 4186–4195.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2018）Been Kim、Martin Wattenberg、Justin Gilmer、Carrie Cai、James Wexler
    等。2018。超越特征归因的可解释性：用概念激活向量（TCAV）进行定量测试。*第35届国际机器学习会议（ICML 2018）* 6（2018），4186–4195。
- en: 'Lee and See (2004) J. D. Lee and K. A. See. 2004. Trust in automation: Designing
    for appropriate Reliance. *Human Factors: The Journal of the Human Factors and
    Ergonomics Society* 46, 1 (Jan 2004), 50–80. [https://doi.org/10.1518/hfes.46.1.50_30392](https://doi.org/10.1518/hfes.46.1.50_30392)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 和 See（2004）J. D. Lee 和 K. A. See。2004。自动化中的信任：设计合适的依赖。*人因学杂志：人因与工程学会期刊*
    46，1（2004年1月），50–80。 [https://doi.org/10.1518/hfes.46.1.50_30392](https://doi.org/10.1518/hfes.46.1.50_30392)
- en: 'Liao et al. (2020) Q. Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning
    the AI: Informing Design Practices for Explainable AI User Experiences. In *Proceedings
    of the 2020 CHI Conference on Human Factors in Computing Systems* *(CHI ’20)*.
    ACM. [https://doi.org/10.1145/3313831.3376590](https://doi.org/10.1145/3313831.3376590)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao 等（2020）Q. Vera Liao、Daniel Gruen 和 Sarah Miller。2020。质疑 AI：为可解释 AI 用户体验提供设计实践的建议。在
    *2020 年 CHI 人机交互会议论文集* *(CHI ’20)*。ACM。 [https://doi.org/10.1145/3313831.3376590](https://doi.org/10.1145/3313831.3376590)
- en: Liu et al. (2023b) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. 2023b. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation
    of Large Language Models for Code Generation. arXiv:cs.SE/2305.01210
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023b）Jiawei Liu、Chunqiu Steven Xia、Yuyao Wang 和 Lingming Zhang。2023b。你的代码是由
    ChatGPT 生成的，真的正确吗？对大型语言模型进行严格的代码生成评估。arXiv:cs.SE/2305.01210
- en: 'Liu et al. (2022) Yue Liu, Chakkrit Tantithamthavorn, Li Li, and Yepang Liu.
    2022. Explainable ai for android malware detection: Towards understanding why
    the models perform so well?. In *2022 IEEE 33rd International Symposium on Software
    Reliability Engineering (ISSRE)*. IEEE, 169–180.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2022）Yue Liu、Chakkrit Tantithamthavorn、Li Li 和 Yepang Liu。2022。面向安卓恶意软件检测的可解释
    AI：理解模型表现如此优秀的原因？在 *2022 IEEE 第33届国际软件可靠性工程研讨会（ISSRE）*。IEEE，169–180。
- en: Liu et al. (2023a) Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, and Li Li.
    2023a. On the Reliability and Explainability of Automated Code Generation Approaches.
    [http://arxiv.org/abs/2302.09587](http://arxiv.org/abs/2302.09587) arXiv:2302.09587
    [cs].
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023a）Yue Liu、Chakkrit Tantithamthavorn、Yonghui Liu 和 Li Li. 2023a. 关于自动化代码生成方法的可靠性和可解释性。
    [http://arxiv.org/abs/2302.09587](http://arxiv.org/abs/2302.09587) arXiv:2302.09587
    [cs].
- en: Liu et al. (2024) Yue Liu, Chakkrit Tantithamthavorn, Yonghui Liu, and Li Li.
    2024. On the reliability and explainability of language models for program generation.
    *ACM Transactions on Software Engineering and Methodology* (2024).
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024）Yue Liu、Chakkrit Tantithamthavorn、Yonghui Liu 和 Li Li. 2024. 关于语言模型在程序生成中的可靠性和可解释性。*ACM
    软件工程与方法学交易*（2024）。
- en: 'Lo ([n. d.]) David Lo. [n. d.]. Trustworthy and Synergistic Artificial Intelligence
    for Software Engineering: Vision and Roadmaps. arXiv:2309.04142 [cs] [http://arxiv.org/abs/2309.04142](http://arxiv.org/abs/2309.04142)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lo（[n. d.]）David Lo. [n. d.]。值得信赖且协同的人工智能用于软件工程：愿景与路线图。arXiv:2309.04142 [cs]
    [http://arxiv.org/abs/2309.04142](http://arxiv.org/abs/2309.04142)
- en: Lundberg and Lee (2017) Scott Lundberg and Su-In Lee. 2017. A Unified Approach
    to Interpreting Model Predictions. arXiv:cs.AI/1705.07874
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg 和 Lee（2017）Scott Lundberg 和 Su-In Lee. 2017. 统一模型预测解释方法。arXiv:cs.AI/1705.07874
- en: 'López et al. (2022) José Antonio Hernández López, Martin Weyssow, Jesús Sánchez
    Cuadrado, and Houari Sahraoui. 2022. AST-Probe: Recovering abstract syntax trees
    from hidden representations of pre-trained language models. [http://arxiv.org/abs/2206.11719](http://arxiv.org/abs/2206.11719)
    arXiv:2206.11719 [cs].'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: López 等（2022）José Antonio Hernández López、Martin Weyssow、Jesús Sánchez Cuadrado
    和 Houari Sahraoui. 2022. AST-Probe：从预训练语言模型的隐藏表示中恢复抽象语法树。 [http://arxiv.org/abs/2206.11719](http://arxiv.org/abs/2206.11719)
    arXiv:2206.11719 [cs].
- en: 'Miller (2018) Tim Miller. 2018. Explanation in Artificial Intelligence: Insights
    from the Social Sciences. arXiv:cs.AI/1706.07269'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller（2018）Tim Miller. 2018. 人工智能中的解释：来自社会科学的见解。arXiv:cs.AI/1706.07269
- en: 'Mohammadkhani et al. (2023a) A. Mohammadkhani, C. Tantithamthavorn, and H.
    Hemmatif. 2023a. Explaining Transformer-based Code Models: What Do They Learn?
    When They Do Not Work?. In *2023 IEEE 23rd International Working Conference on
    Source Code Analysis and Manipulation (SCAM)*. IEEE Computer Society, Los Alamitos,
    CA, USA, 96–106. [https://doi.org/10.1109/SCAM59687.2023.00020](https://doi.org/10.1109/SCAM59687.2023.00020)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohammadkhani 等（2023a）A. Mohammadkhani、C. Tantithamthavorn 和 H. Hemmatif. 2023a.
    解释基于 Transformer 的代码模型：它们学到了什么？当它们不工作时？在 *2023 IEEE 第23届国际源代码分析与操作会议（SCAM）* 中。IEEE
    计算机学会，洛杉矶，CA，美国，96–106。 [https://doi.org/10.1109/SCAM59687.2023.00020](https://doi.org/10.1109/SCAM59687.2023.00020)
- en: 'Mohammadkhani et al. (2023b) A. Mohammadkhani, C. Tantithamthavorn, and H.
    Hemmatif. 2023b. Explaining Transformer-based Code Models: What Do They Learn?
    When They Do Not Work?. In *2023 IEEE 23rd International Working Conference on
    Source Code Analysis and Manipulation (SCAM)*. IEEE Computer Society, Los Alamitos,
    CA, USA, 96–106. [https://doi.org/10.1109/SCAM59687.2023.00020](https://doi.org/10.1109/SCAM59687.2023.00020)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohammadkhani 等（2023b）A. Mohammadkhani、C. Tantithamthavorn 和 H. Hemmatif. 2023b.
    解释基于 Transformer 的代码模型：它们学到了什么？当它们不工作时？在 *2023 IEEE 第23届国际源代码分析与操作会议（SCAM）* 中。IEEE
    计算机学会，洛杉矶，CA，美国，96–106。 [https://doi.org/10.1109/SCAM59687.2023.00020](https://doi.org/10.1109/SCAM59687.2023.00020)
- en: Molnar (2019) Christoph Molnar. 2019. *Interpretable Machine Learning*. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molnar（2019）Christoph Molnar. 2019. *可解释的机器学习*。 [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/).
- en: 'Molnar et al. (2020) Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl.
    2020. Interpretable Machine Learning – A Brief History, State-of-the-Art and Challenges.
    01 (2020), 1–15. [http://arxiv.org/abs/2010.09337](http://arxiv.org/abs/2010.09337)
    arXiv: 2010.09337.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Molnar 等（2020）Christoph Molnar、Giuseppe Casalicchio 和 Bernd Bischl. 2020. 可解释的机器学习——简要历史、现状及挑战。01（2020），1–15。
    [http://arxiv.org/abs/2010.09337](http://arxiv.org/abs/2010.09337) arXiv: 2010.09337.'
- en: 'Murdoch et al. (2018) W James Murdoch, Peter J Liu, and Bin Yu. 2018. Beyond
    word importance: Contextual decomposition to extract interactions from lstms.
    *arXiv preprint arXiv:1801.05453* (2018).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murdoch 等（2018）W James Murdoch、Peter J Liu 和 Bin Yu. 2018. 超越词语重要性：上下文分解以从 LSTMs
    中提取交互。*arXiv 预印本 arXiv:1801.05453*（2018）。
- en: 'Nijkamp et al. (2023) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, et al. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn
    Program Synthesis. arXiv:cs.LG/2203.13474'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等（2023）Erik Nijkamp、Bo Pang、Hiroaki Hayashi、Lifu Tu、Huan Wang 等. 2023.
    CodeGen：一个用于代码的开放大型语言模型，支持多轮程序合成。arXiv:cs.LG/2203.13474
- en: 'Palacio et al. (2024) David N. Palacio, Daniel Rodriguez-Cardenas, and Alejandro
    Velasco. 2024. ASTrust: Github Repository. [https://github.com/WM-SEMERU/CodeSyntaxConcept](https://github.com/WM-SEMERU/CodeSyntaxConcept)
    [Accessed 23-03-2024].'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Palacio et al. (2024) David N. Palacio, Daniel Rodriguez-Cardenas, 和 Alejandro
    Velasco. 2024. ASTrust: Github 仓库。 [https://github.com/WM-SEMERU/CodeSyntaxConcept](https://github.com/WM-SEMERU/CodeSyntaxConcept)
    [访问日期 23-03-2024]。'
- en: Palacio et al. (2023) David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro
    Rodriguez, Kevin Moran, et al. 2023. Toward a Theory of Causation for Interpreting
    Neural Code Models. arXiv:cs.SE/2302.03788
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palacio et al. (2023) David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro
    Rodriguez, Kevin Moran 等. 2023. 关于神经代码模型解释的因果理论。arXiv:cs.SE/2302.03788
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th Annual Meeting on Association for Computational Linguistics*
    *(ACL ’02)*. Association for Computational Linguistics, USA, 311–318. [https://doi.org/10.3115/1073083.1073135](https://doi.org/10.3115/1073083.1073135)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing
    Zhu. 2002. BLEU: 一种自动评估机器翻译的方法。见于 *第40届计算语言学协会年会论文集* *(ACL ’02)*. 计算语言学协会，美国，311–318.
    [https://doi.org/10.3115/1073083.1073135](https://doi.org/10.3115/1073083.1073135)'
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, et al. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
    Library. In *Advances in Neural Information Processing Systems 32*, H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox, et al. (Eds.). Curran
    Associates, Inc., 8024–8035. [http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury 等. 2019. PyTorch: 一种命令式风格的高性能深度学习库。见于 *神经信息处理系统进展 32*，H. Wallach, H.
    Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox 等（编）。Curran Associates, Inc.,
    8024–8035. [http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)'
- en: Pearl et al. (2016) Judea Pearl, Madelyn Glymour, and Nicholas P.Jewell. 2016.
    *Causal Inference in Statistics, A Primer*.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearl et al. (2016) Judea Pearl, Madelyn Glymour, 和 Nicholas P.Jewell. 2016.
    *统计学中的因果推断：入门*。
- en: 'Pornprasit et al. (2021) Chanathip Pornprasit, Chakkrit Tantithamthavorn, Jirayus
    Jiarpakdee, Michael Fu, and Patanamon Thongtanunam. 2021. PyExplainer: Explaining
    the Predictions of Just-In-Time Defect Models. In *2021 36th IEEE/ACM International
    Conference on Automated Software Engineering (ASE)*. 407–418. [https://doi.org/10.1109/ASE51524.2021.9678763](https://doi.org/10.1109/ASE51524.2021.9678763)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pornprasit et al. (2021) Chanathip Pornprasit, Chakkrit Tantithamthavorn, Jirayus
    Jiarpakdee, Michael Fu, 和 Patanamon Thongtanunam. 2021. PyExplainer: 解释即时缺陷模型的预测。见于
    *2021年第36届IEEE/ACM国际自动化软件工程会议（ASE）*。407–418. [https://doi.org/10.1109/ASE51524.2021.9678763](https://doi.org/10.1109/ASE51524.2021.9678763)'
- en: Raychev et al. (2014) Veselin Raychev, Martin T. Vechev, and Eran Yahav. 2014.
    Code completion with statistical language models. *Proceedings of the 35th ACM
    SIGPLAN Conference on Programming Language Design and Implementation* (2014).
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raychev et al. (2014) Veselin Raychev, Martin T. Vechev, 和 Eran Yahav. 2014.
    使用统计语言模型进行代码补全。*第35届ACM SIGPLAN编程语言设计与实现会议论文集*（2014）。
- en: 'Ren et al. (2020) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, et al.
    2020. CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. *CoRR* abs/2009.10297
    (2020). arXiv:2009.10297 [https://arxiv.org/abs/2009.10297](https://arxiv.org/abs/2009.10297)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren et al. (2020) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu 等. 2020.
    CodeBLEU: 一种自动评估代码生成的方法。*CoRR* abs/2009.10297（2020）。arXiv:2009.10297 [https://arxiv.org/abs/2009.10297](https://arxiv.org/abs/2009.10297)'
- en: Ribeiro et al. (2016a) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016a. ” Why should i trust you?” Explaining the predictions of any classifier.
    In *Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery
    and data mining*. 1135–1144.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro et al. (2016a) Marco Tulio Ribeiro, Sameer Singh, 和 Carlos Guestrin.
    2016a. “为什么我应该相信你？” 解释任何分类器的预测。见于 *第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。1135–1144.
- en: 'Ribeiro et al. (2016b) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016b. ”Why Should I Trust You?”: Explaining the Predictions of Any Classifier.
    arXiv:cs.LG/1602.04938'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro et al. (2016b) Marco Tulio Ribeiro, Sameer Singh, 和 Carlos Guestrin.
    2016b. “为什么我应该相信你？”：解释任何分类器的预测。arXiv:cs.LG/1602.04938
- en: Rodriguez-Cardenas et al. (2023) Daniel Rodriguez-Cardenas, David N. Palacio,
    Dipin Khati, Henry Burke, and Denys Poshyvanyk. 2023. Benchmarking Causal Study
    to Interpret Large Language Models for Source Code. In *2023 IEEE International
    Conference on Software Maintenance and Evolution (ICSME)*. 329–334. [https://doi.org/10.1109/ICSME58846.2023.00040](https://doi.org/10.1109/ICSME58846.2023.00040)
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rodriguez-Cardenas et al. (2023) Daniel Rodriguez-Cardenas, David N. Palacio,
    Dipin Khati, Henry Burke, 和 Denys Poshyvanyk. 2023. 基准因果研究以解读大型语言模型的源代码。发表于*2023
    IEEE 国际软件维护与演化会议 (ICSME)*。329–334. [https://doi.org/10.1109/ICSME58846.2023.00040](https://doi.org/10.1109/ICSME58846.2023.00040)
- en: 'Sharma et al. (2021) Amit Sharma, Vasilis Syrgkanis, Cheng Zhang, and Emre
    Kıcıman. 2021. DoWhy : Addressing Challenges in Expressing and Validating Causal
    Assumptions. (2021).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sharma et al. (2021) Amit Sharma, Vasilis Syrgkanis, Cheng Zhang, 和 Emre Kıcıman.
    2021. DoWhy: 解决表达和验证因果假设中的挑战。（2021年）。'
- en: Spiess et al. (2024) Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel,
    Md Rafiqul Islam Rabin, et al. 2024. Quality and Trust in LLM-generated Code.
    arXiv:cs.SE/2402.02047
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spiess et al. (2024) Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel,
    Md Rafiqul Islam Rabin, 等. 2024. LLM生成代码的质量与可信度。arXiv:cs.SE/2402.02047
- en: 'Sun et al. (2024) Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang,
    et al. 2024. TrustLLM: Trustworthiness in Large Language Models. arXiv:cs.CL/2401.05561'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2024) Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang,
    等. 2024. TrustLLM: 大型语言模型的可信度。arXiv:cs.CL/2401.05561'
- en: Sundararajan et al. (2017) Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.
    Axiomatic attribution for deep networks. In *International conference on machine
    learning*. PMLR, 3319–3328.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sundararajan et al. (2017) Mukund Sundararajan, Ankur Taly, 和 Qiqi Yan. 2017.
    深度网络的公理化归因。发表于*国际机器学习会议*。PMLR, 3319–3328.
- en: 'Tantithamthavorn et al. (2023) Chakkrit Tantithamthavorn, Jürgen Cito, Hadi
    Hemmati, and Satish Chandra. 2023. Explainable AI for SE: Challenges and Future
    Directions. *IEEE Software* 40, 3 (2023), 29–33. [https://doi.org/10.1109/MS.2023.3246686](https://doi.org/10.1109/MS.2023.3246686)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tantithamthavorn et al. (2023) Chakkrit Tantithamthavorn, Jürgen Cito, Hadi
    Hemmati, 和 Satish Chandra. 2023. 面向软件工程的可解释 AI：挑战与未来方向。*IEEE 软件* 40, 3 (2023),
    29–33. [https://doi.org/10.1109/MS.2023.3246686](https://doi.org/10.1109/MS.2023.3246686)
- en: Troshin and Chirkova (2022) Sergey Troshin and Nadezhda Chirkova. 2022. Probing
    Pretrained Models of Source Code. [http://arxiv.org/abs/2202.08975](http://arxiv.org/abs/2202.08975)
    arXiv:2202.08975 [cs].
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Troshin and Chirkova (2022) Sergey Troshin 和 Nadezhda Chirkova. 2022. 探索预训练源代码模型。
    [http://arxiv.org/abs/2202.08975](http://arxiv.org/abs/2202.08975) arXiv:2202.08975
    [cs].
- en: Wan et al. (2022) Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, et al.
    2022. What Do They Capture? – A Structural Analysis of Pre-Trained Language Models
    for Source Code. [http://arxiv.org/abs/2202.06840](http://arxiv.org/abs/2202.06840)
    arXiv:2202.06840 [cs].
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2022) Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, 等.
    2022. 他们捕捉了什么？– 预训练语言模型对源代码的结构分析。 [http://arxiv.org/abs/2202.06840](http://arxiv.org/abs/2202.06840)
    arXiv:2202.06840 [cs].
- en: Watson et al. (2020a) Cody Watson, Nathan Cooper, David Nader-Palacio, Kevin
    Moran, and Denys Poshyvanyk. 2020a. A Systematic Literature Review on the Use
    of Deep Learning in Software Engineering Research. *CoRR* abs/2009.06520 (2020).
    arXiv:2009.06520 [https://arxiv.org/abs/2009.06520](https://arxiv.org/abs/2009.06520)
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watson et al. (2020a) Cody Watson, Nathan Cooper, David Nader-Palacio, Kevin
    Moran, 和 Denys Poshyvanyk. 2020a. 深度学习在软件工程研究中的系统文献综述。*CoRR* abs/2009.06520 (2020).
    arXiv:2009.06520 [https://arxiv.org/abs/2009.06520](https://arxiv.org/abs/2009.06520)
- en: Watson et al. (2021) Cody Watson, Nathan Cooper, David Nader Palacio, Kevin
    Moran, and Denys Poshyvanyk. 2021. A Systematic Literature Review on the Use of
    Deep Learning in Software Engineering Research. arXiv:cs.SE/2009.06520
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watson et al. (2021) Cody Watson, Nathan Cooper, David Nader Palacio, Kevin
    Moran, 和 Denys Poshyvanyk. 2021. 深度学习在软件工程研究中的系统文献综述。arXiv:cs.SE/2009.06520
- en: Watson et al. (2020b) Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota,
    and Denys Poshyvanyk. 2020b. On Learning Meaningful Assert Statements for Unit
    Test Cases. In *Proceedings of the ACM/IEEE 42nd International Conference on Software
    Engineering* *(ICSE ’20)*. Association for Computing Machinery, New York, NY,
    USA, 1398–1409. [https://doi.org/10.1145/3377811.3380429](https://doi.org/10.1145/3377811.3380429)
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watson et al. (2020b) Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota,
    和 Denys Poshyvanyk. 2020b. 学习有意义的断言语句用于单元测试案例。发表于*ACM/IEEE 第42届国际软件工程会议* *(ICSE
    ’20)*。计算机协会, 纽约, NY, USA, 1398–1409. [https://doi.org/10.1145/3377811.3380429](https://doi.org/10.1145/3377811.3380429)
- en: 'Weller (2019) Adrian Weller. 2019. Transparency: Motivations and Challenges.
    arXiv:cs.CY/1708.01870'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weller (2019) Adrian Weller. 2019. 透明性：动机与挑战。arXiv:cs.CY/1708.01870
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, et al. 2020. Transformers: State-of-the-Art Natural Language
    Processing. In *Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing: System Demonstrations*. Association for Computational Linguistics,
    Online, 38–45. [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6)'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf等（2020）Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue 等. 2020. Transformers: 最新自然语言处理技术。在*2020年自然语言处理经验方法会议：系统演示*中。计算语言学协会，在线，38–45。
    [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6)'
- en: Xu et al. (2022) Frank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn.
    2022. A Systematic Evaluation of Large Language Models of Code. [http://arxiv.org/abs/2202.13169](http://arxiv.org/abs/2202.13169)
    arXiv:2202.13169 [cs].
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等（2022）Frank F. Xu, Uri Alon, Graham Neubig, 和 Vincent J. Hellendoorn. 2022.
    对大型语言模型的系统评估。 [http://arxiv.org/abs/2202.13169](http://arxiv.org/abs/2202.13169)
    arXiv:2202.13169 [cs]。
