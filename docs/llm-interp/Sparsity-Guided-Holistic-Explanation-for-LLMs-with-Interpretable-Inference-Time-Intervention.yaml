- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 17:34:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:34:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time
    Intervention
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏引导的LLMs整体解释与可解释推理时间干预
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.15033](https://ar5iv.labs.arxiv.org/html/2312.15033)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.15033](https://ar5iv.labs.arxiv.org/html/2312.15033)
- en: Zhen Tan¹, Tianlong Chen², Zhenyu Zhang³, Huan Liu¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhen Tan¹, Tianlong Chen², Zhenyu Zhang³, Huan Liu¹
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) have achieved unprecedented breakthroughs in various
    natural language processing domains. However, the enigmatic “black-box” nature
    of LLMs remains a significant challenge for interpretability, hampering transparent
    and accountable applications. While past approaches, such as attention visualization,
    pivotal subnetwork extraction, and concept-based analyses, offer some insight,
    they often focus on either local or global explanations within a single dimension,
    occasionally falling short in providing comprehensive clarity. In response, we
    propose a novel methodology anchored in sparsity-guided techniques, aiming to
    provide a holistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively
    integrates sparsity to elucidate three intertwined layers of interpretation: input,
    subnetwork, and concept levels. In addition, the newly introduced dimension of
    interpretable inference-time intervention facilitates dynamic adjustments to the
    model during deployment. Through rigorous empirical evaluations on real-world
    datasets, we demonstrate that SparseCBM delivers a profound understanding of LLM
    behaviors, setting it apart in both interpreting and ameliorating model inaccuracies.
    Codes are provided in supplements.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理领域取得了前所未有的突破。然而，LLMs神秘的“黑箱”特性仍然是解释性方面的重大挑战，阻碍了透明和可追溯的应用。虽然过去的一些方法，如注意力可视化、关键子网络提取和基于概念的分析，提供了一些洞察，但它们通常关注单一维度内的局部或全局解释，偶尔未能提供全面的清晰度。对此，我们提出了一种基于稀疏引导技术的新方法，旨在提供对LLMs的整体解释。我们的框架称为SparseCBM，创新性地将稀疏性融入解释的三个交织层次：输入层、子网络层和概念层。此外，新引入的可解释推理时间干预维度在部署过程中便于对模型进行动态调整。通过对真实数据集的严格实证评估，我们证明了SparseCBM提供了对LLM行为的深刻理解，使其在解释和改善模型不准确性方面具有独特优势。代码已附在补充材料中。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'The advent of Large Language Models (LLMs) has captured the intricacies of
    language patterns with striking finesse, rivaling, and at times, surpassing human
    performance (Zhou et al. [2022](#bib.bib52); OpenAI [2023](#bib.bib32)). However,
    their laudable success story is shadowed by a pressing concern: a distinct lack
    of transparency and interpretability. As LLMs burgeon in complexity and scale,
    the elucidation of their internal mechanisms and decision-making processes has
    become a daunting challenge. The opaque “black-box” characteristics of these models
    obfuscate the transformation process from input data to generated output, presenting
    a formidable barrier to trust, debugging, and optimal utilization of these potent
    computational tools. Consequently, advancing the interpretability of LLMs has
    emerged as a crucial frontier in machine learning and natural language processing
    research, aiming to reconcile the dichotomy between superior model performance
    and comprehensive usability.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现以惊人的精细度捕捉了语言模式的复杂性，有时甚至超越了人类的表现（Zhou et al. [2022](#bib.bib52);
    OpenAI [2023](#bib.bib32)）。然而，它们令人称赞的成功故事却被一个迫切的担忧所掩盖：缺乏透明性和可解释性。随着LLMs在复杂性和规模上的不断扩展，阐明其内部机制和决策过程已成为一项艰巨的挑战。这些模型的不透明“黑箱”特性使从输入数据到生成输出的转换过程变得晦涩难懂，成为信任、调试和最佳利用这些强大计算工具的巨大障碍。因此，提高LLMs的可解释性已成为机器学习和自然语言处理研究中的关键前沿，旨在调和模型性能与全面可用性之间的矛盾。
- en: '![Refer to caption](img/d4c5edcb83969aaf87c1e4eb1290d681.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4c5edcb83969aaf87c1e4eb1290d681.png)'
- en: 'Figure 1: The illustration includes: (a) Attention visualization provides a
    localized, attention-driven explanation. While insightful, this might be less
    decipherable or intuitive for users outside the realm of computer science. (b)
    CBMs deliver a broader, concept-level understanding, resonating naturally with
    human cognition. However, they sometimes miss out on the nuanced, granular insights
    of the LLM’s workings. (c) SparseCBMs outline a holistic decision pathway for
    each input, seamlessly progressing from tokens, via pertinent subnetworks and
    concepts, to the final task label. This approach marries the strengths of both
    local and global explanations, addressing their respective shortcomings.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：该插图包括：(a) 注意力可视化提供了一个局部的、基于注意力的解释。虽然有洞察力，但这对于计算机科学领域以外的用户可能不太容易理解或直观。(b)
    CBMs 提供了更广泛的概念级理解，自然与人类认知相契合。然而，它们有时忽略了LLM工作中的细微和具体的见解。(c) SparseCBMs 为每个输入概述了一个全面的决策路径，从标记，通过相关子网络和概念，直至最终任务标签。这种方法将局部和全球解释的优点结合起来，解决了它们各自的不足之处。
- en: The spectrum of interpretability solutions for language models can be broadly
    bifurcated into two categories. ❶ Initial approaches predominantly leverage local
    explanations, employing techniques such as visualization of attention weights (Galassi,
    Lippi, and Torroni [2020](#bib.bib6)), probing of feature representations (Mishra,
    Sturm, and Dixon [2017](#bib.bib30); Lundberg and Lee [2017](#bib.bib27)), and
    utilization of counterfactuals (Wu et al. [2021](#bib.bib49); Ross, Marasović,
    and Peters [2021](#bib.bib35)), among others. These methods focus on providing
    explanations at granular levels, such as individual tokens, instances, neurons,
    or subnetworks, as exemplified in Figure [1](#Sx1.F1 "Figure 1 ‣ Introduction
    ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time
    Intervention") (a). While these low-level explanations offer a degree of reliability,
    they often sacrifice readability and intuitiveness (Losch, Fritz, and Schiele
    [2019](#bib.bib26)), thereby constraining their practical applicability. ❷ More
    recently, researchers have tended to global explanations, such as concept-based
    analyses that inherently resonate with human cognition (Wang et al. [2023a](#bib.bib42);
    Abraham et al. [2022](#bib.bib1)). For instance, one recent work (Tan et al. [2023](#bib.bib39))
    incorporates Concept Bottleneck Models (CBMs) (Koh et al. [2020](#bib.bib13))
    into pretrained language models, leading to an impressive “interpretability-utility”
    Pareto front. Figure [1](#Sx1.F1 "Figure 1 ‣ Introduction ‣ Sparsity-Guided Holistic
    Explanation for LLMs with Interpretable Inference-Time Intervention") (b) exemplifies
    this for sentiment analysis tasks, where human-intelligible concepts like “Food”,
    “Ambiance”, and “Service” correspond to neurons in the concept bottleneck layer.
    The final decision layer is designed as a linear function of these concepts, rendering
    the decision rules easily understandable. However, these methods excessively focus
    on global explanations. The underlying reasoning between raw input and concepts
    remains unclear.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的可解释性解决方案的范围可以大致分为两类。❶ 初始方法主要依赖于局部解释，采用技术如注意力权重的可视化（Galassi, Lippi, and
    Torroni [2020](#bib.bib6)），特征表示的探测（Mishra, Sturm, and Dixon [2017](#bib.bib30);
    Lundberg and Lee [2017](#bib.bib27)），以及反事实的利用（Wu et al. [2021](#bib.bib49); Ross,
    Marasović, and Peters [2021](#bib.bib35)）等。这些方法专注于在细粒度水平提供解释，例如个别标记、实例、神经元或子网络，如图[1](#Sx1.F1
    "Figure 1 ‣ Introduction ‣ Sparsity-Guided Holistic Explanation for LLMs with
    Interpretable Inference-Time Intervention") (a) 所示。虽然这些低级解释提供了一定程度的可靠性，但它们通常牺牲了可读性和直观性（Losch,
    Fritz, and Schiele [2019](#bib.bib26)），从而限制了其实际应用性。❷ 最近，研究人员倾向于全球解释，例如固有地与人类认知相契合的基于概念的分析（Wang
    et al. [2023a](#bib.bib42); Abraham et al. [2022](#bib.bib1)）。例如，一项近期工作（Tan et
    al. [2023](#bib.bib39)）将概念瓶颈模型（CBMs）（Koh et al. [2020](#bib.bib13)）融入预训练语言模型中，形成了一个令人印象深刻的“可解释性-实用性”帕累托前沿。图[1](#Sx1.F1
    "Figure 1 ‣ Introduction ‣ Sparsity-Guided Holistic Explanation for LLMs with
    Interpretable Inference-Time Intervention") (b) 举例说明了这一点，在情感分析任务中，“Food”、“Ambiance”和“Service”等对人类可理解的概念对应于概念瓶颈层中的神经元。最终决策层设计为这些概念的线性函数，使决策规则易于理解。然而，这些方法过度关注全球解释。原始输入与概念之间的基本推理仍然不清楚。
- en: To address these limitations, our work champions a holistic interpretation of
    LLM predictions. We unveil SparseCBM, an evolved CBM variant that melds the complementary
    “strengths” of local and global explanations, thereby addressing the individual
    “weaknesses” of each. This confluence is born from rigorous sparsity-guided refinement
    designed specifically for LLMs, as depicted in Figure [1](#Sx1.F1 "Figure 1 ‣
    Introduction ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable
    Inference-Time Intervention") (c). Concretely, SparseCBM iteratively prunes the
    LLM backbone guided by a joint objective of optimizing for both concept and task
    labels until the desired sparsity level is accomplished. This exercise distills
    the LLM into distinct yet interconnected subnetworks, each corresponding to a
    predefined concept. As such, SparseCBM provides a comprehensive and intelligible
    decision-making pathway for each input text, tracing from tokens through subnetworks
    and concepts, ultimately leading to the final task label.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些局限性，我们的工作提倡对LLM预测的整体解释。我们揭示了SparseCBM，这是一种演变的CBM变体，它融合了局部和全局解释的互补**“优势”**，从而解决了每种方法的个别**“弱点”**。这一融合源于专为LLM设计的严格稀疏引导优化，如图 [1](#Sx1.F1
    "Figure 1 ‣ Introduction ‣ Sparsity-Guided Holistic Explanation for LLMs with
    Interpretable Inference-Time Intervention") (c)所示。具体而言，SparseCBM通过一个联合目标——同时优化概念和任务标签——迭代修剪LLM骨干，直到达到所需的稀疏水平。这一过程将LLM提炼成相互独立但又相互关联的子网络，每个子网络对应一个预定义的概念。因此，SparseCBM为每个输入文本提供了一个全面且易于理解的决策路径，从tokens经过子网络和概念，*最终*到达最终任务标签。
- en: 'Another unique feature is that, SparseCBMs allow interpretable inference-time
    intervention (Koh et al. [2020](#bib.bib13); Li et al. [2023a](#bib.bib18)). The
    inherent sparsity-driven structure of SparseCBM allows it to adjust its internal
    parameters dynamically, based on the context of the input. In practical terms,
    this means that, during inference, SparseCBM can identify potential areas of ambiguity
    or misconception, and proactively modify its internal decision-making routes without
    a full-scale retraining. This “on-the-fly” adaptability not only enhances prediction
    accuracy but also offers users a window into how the model adjusts its reasoning
    in real time. By making these modifications both accessible and understandable,
    SparseCBM bridges the common chasm between interpretability and agility for LLMs.
    This real-time decision pathway modification, stands as a beacon for fostering
    trust and facilitating more nuanced human-model interactions. In summary, SparseCBM
    carries the following advantages:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个独特的特点是，SparseCBMs允许**可解释的推理时间干预** (Koh et al. [2020](#bib.bib13); Li et al.
    [2023a](#bib.bib18))。SparseCBM固有的稀疏驱动结构使其能够基于输入的上下文动态调整其内部参数。实际而言，这意味着在推理过程中，SparseCBM能够识别潜在的模糊或误解区域，并在不进行全面重新训练的情况下，主动修改其内部决策路径。这种**“即时”**适应性不仅提高了预测准确性，还为用户提供了一个窗口，了解模型如何实时调整其推理。通过使这些修改既可访问又易于理解，SparseCBM弥合了LLM的**可解释性**与**灵活性**之间的常见鸿沟。这种实时决策路径修改，成为促进信任和促进更细致的人机交互的灯塔。总而言之，SparseCBM具有以下优势：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Empirical Validation: Our experiments reveal that SparseCBM enables interpretability
    at the token, subnetwork, and concept levels, creating a synergy that surpasses
    the mere aggregation of these elements.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实证验证：我们的实验表明，SparseCBM实现了对token、子网络和概念层次的可解释性，创造了一种超越这些元素单纯聚合的协同效应。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Superior Performance: SparseCBM demonstrates state-of-the-art performance on
    conventional benchmarks, both in terms of concept and task label predictions.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卓越的性能：SparseCBM在传统基准测试中展示了先进的性能，无论是在概念还是任务标签预测方面。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Metacognitive Inference-Time Intervention: Compared to vanilla CBMs, SparseCBM
    exhibits a unique capability for efficient and interpretable inference-time intervention.
    By subtly modulating internal sparsity, SparseCBM learns to sidestep known pitfalls.
    This property bolsters user trust in SparseCBMs and, by extension, LLMs.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 元认知推理时间干预：与普通CBM相比，SparseCBM展现了高效且可解释的推理时间干预能力。通过微妙地调节内部稀疏，SparseCBM学会了规避已知的陷阱。这一特性增强了用户对SparseCBM以及LLM的信任。
- en: Related Work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Interpreting Language Models
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解读语言模型
- en: Research on the interpretability of language models has been robust, with previous
    work focusing on visualization of hidden states and attention weights in transformer-based
    models (Vig [2019](#bib.bib41); Galassi, Lippi, and Torroni [2020](#bib.bib6)).
    These techniques, while valuable, often provided granular insights that were not
    easily interpretable at a high level. Feature importance methods like LIME (Ribeiro,
    Singh, and Guestrin [2016](#bib.bib34)) and SHAP (Lundberg and Lee [2017](#bib.bib27))
    provided valuable insights into how each input feature contributes to the prediction,
    but still fail to offer a global understanding of the model behavior, and often
    lack intuitiveness and readability.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对语言模型可解释性的研究已经相当深入，早期工作主要集中在变换器模型中对隐藏状态和注意力权重的可视化上 (Vig [2019](#bib.bib41);
    Galassi, Lippi, and Torroni [2020](#bib.bib6))。这些技术虽然有价值，但通常提供的细粒度见解在高层次上不易解释。特征重要性方法如LIME (Ribeiro,
    Singh, and Guestrin [2016](#bib.bib34))和SHAP (Lundberg and Lee [2017](#bib.bib27))
    提供了关于每个输入特征如何影响预测的有价值见解，但仍然无法提供对模型行为的全球理解，且常常缺乏直观性和可读性。
- en: The advent of concept-based interpretability has marked a significant development,
    offering more global, high-level explanations (Koh et al. [2020](#bib.bib13);
    Abraham et al. [2022](#bib.bib1); Wang et al. [2023a](#bib.bib42)). Concept Bottleneck
    Models (CBMs) (Koh et al. [2020](#bib.bib13); Oikarinen et al. [2023](#bib.bib31))
    which incorporate a concept layer into the model, have gained traction recently (Tan
    et al. [2023](#bib.bib39)). CBMs are trained with task labels and concept labels
    either independently, sequentially, or jointly. This design enables inference-time
    debugging by calibrating the activations of concepts. Yet, current CBMs are deficient
    in their ability to offer granular interpretations, and inference-time interventions
    remain incapable of altering the language model backbone, leading to recurrent
    errors. On the other hand, the interpretability of LLMs remains a less explored
    area. Although some progress has been made, such as guiding LLMs to generate explanations
    for their predictions using finely tuned prompts (Li et al. [2022](#bib.bib19)),
    the reliability of these explanations remains questionable. In summary, a reliable
    method facilitating holistic insights into model behavior is still wanting. In
    response, our work advances this field by introducing SparseCBM, a holistic interpretation
    framework for LLMs that tackles both local and global interpretations, thus enhancing
    the usability and trustworthiness of LLMs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 概念基础的可解释性的出现标志着一个重要的发展，提供了更全球化的高层次解释 (Koh et al. [2020](#bib.bib13); Abraham
    et al. [2022](#bib.bib1); Wang et al. [2023a](#bib.bib42))。概念瓶颈模型（CBMs） (Koh et al.
    [2020](#bib.bib13); Oikarinen et al. [2023](#bib.bib31)) 在模型中加入了概念层，最近获得了关注 (Tan
    et al. [2023](#bib.bib39))。CBMs可以通过任务标签和概念标签独立、顺序或联合进行训练。这种设计通过校准概念的激活来实现推理时的调试。然而，当前的CBMs在提供细粒度解释的能力上存在不足，并且推理时的干预仍然无法改变语言模型的骨干，导致错误频发。另一方面，LLMs的可解释性仍然是一个较少探索的领域。尽管已经取得了一些进展，例如通过精细调整的提示引导LLMs生成对其预测的解释 (Li
    et al. [2022](#bib.bib19))，但这些解释的可靠性仍然令人质疑。总之，仍然需要一种可靠的方法来促进对模型行为的全面洞察。对此，我们的工作通过引入SparseCBM，一个针对LLMs的整体解释框架，来推动这一领域的发展，它解决了局部和全局解释的问题，从而提高了LLMs的可用性和可信度。
- en: Sparsity Mining for Language Models
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型的稀疏性挖掘
- en: Sparsity-driven techniques, often associated with model pruning, form an energetic
    subset of research primarily in the pursuit of model compression. At their core,
    these methods focus on the elimination of less influential neurons while retaining
    the more critical ones, thereby sustaining optimal model performance (LeCun, Denker,
    and Solla [1990a](#bib.bib16); Han, Mao, and Dally [2016](#bib.bib8); Han et al.
    [2015](#bib.bib9); LeCun, Denker, and Solla [1990b](#bib.bib17); Liu et al. [2017](#bib.bib25);
    He, Zhang, and Sun [2017](#bib.bib11); Zhou, Alvarez, and Porikli [2016](#bib.bib51)).
    Contemporary research has shed light on the heightened robustness of pruned models
    against adversarial conditions, such as overfitting and distribution shifts. Typical
    pruning methods for language models encompass structured pruning (Michel, Levy,
    and Neubig [2019](#bib.bib29)), fine-grained structured pruning (Lagunas et al.
    [2021](#bib.bib15)), and unstructured pruning (Gale, Elsen, and Hooker [2019](#bib.bib7)).
    In brief, unstructured pruning removes individual weights in a network, leading
    to a sparse matrix, structured pruning eliminates entire structures like neurons
    or layers for a dense model, while fine-grained structured pruning prunes smaller
    structures like channels or weight vectors, offering a balance between the previous
    two. We direct the readers to the benchmark (Liu et al. [2023](#bib.bib23)) for
    a comprehensive overview. In our case, we focus on unstructured pruning for its
    effectiveness and better interpretability.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动稀疏性的技术，通常与模型剪枝相关，是研究中一个充满活力的子集，主要致力于模型压缩。这些方法的核心在于消除影响较小的神经元，同时保留更关键的神经元，从而维持模型的最佳性能（LeCun,
    Denker, and Solla [1990a](#bib.bib16); Han, Mao, and Dally [2016](#bib.bib8);
    Han et al. [2015](#bib.bib9); LeCun, Denker, and Solla [1990b](#bib.bib17); Liu
    et al. [2017](#bib.bib25); He, Zhang, and Sun [2017](#bib.bib11); Zhou, Alvarez,
    and Porikli [2016](#bib.bib51)）。当代研究揭示了剪枝模型在对抗条件下的增强鲁棒性，如过拟合和分布偏移。语言模型的典型剪枝方法包括结构化剪枝（Michel,
    Levy, and Neubig [2019](#bib.bib29)）、细粒度结构化剪枝（Lagunas et al. [2021](#bib.bib15)）和非结构化剪枝（Gale,
    Elsen, and Hooker [2019](#bib.bib7)）。简而言之，非结构化剪枝删除网络中的个别权重，导致稀疏矩阵；结构化剪枝则消除整个结构，如神经元或层，适用于密集模型，而细粒度结构化剪枝则修剪较小的结构，如通道或权重向量，提供了前两者之间的平衡。我们建议读者参考基准（Liu
    et al. [2023](#bib.bib23)）以获得全面概述。在我们的研究中，我们专注于非结构化剪枝，因为其效果显著且可解释性更好。
- en: Recently, studies have underscored the interpretability afforded by sparse networks (Subramanian
    et al. [2018](#bib.bib37)). For instance, Meister et al. ([2021](#bib.bib28))
    delve into the interpretability of sparse attention mechanisms in language models,
    Liu et al. ([2022](#bib.bib22)) incorporate sparse contrastive learning in an
    ancillary sparse coding layer to facilitate word-level interpretability, and Oikarinen
    et al. ([2023](#bib.bib31)) demonstrate that a sparsity constraint on the final
    linear predictor enhances concept-level interpretation of CBMs. Despite their
    effectiveness, these frameworks restrict sparsity to a handful of layers, leading
    to unidimensional interpretability that falls short of the desired comprehensiveness.
    In contrast, our proposed framework, SparseCBM, imposes sparsity across the entire
    LLM backbone, enabling holistic interpretation at the token, subnetwork, and concept
    levels.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究强调了稀疏网络所提供的可解释性（Subramanian et al. [2018](#bib.bib37)）。例如，Meister et al.
    ([2021](#bib.bib28)) 探讨了语言模型中稀疏注意力机制的可解释性，Liu et al. ([2022](#bib.bib22)) 在附加的稀疏编码层中融入了稀疏对比学习，以促进词级可解释性，而
    Oikarinen et al. ([2023](#bib.bib31)) 展示了对最终线性预测器施加稀疏性约束能够增强 CBMs 的概念级解释能力。尽管这些框架有效，但它们将稀疏性限制在少数几个层级，导致了单维度的可解释性，无法达到所期望的全面性。相比之下，我们提出的框架
    SparseCBM 在整个 LLM 骨干网络中施加稀疏性，从而实现了在令牌、子网络和概念级别的整体解释。
- en: Methodology
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法论
- en: 'Preliminary: Concept Bottleneck Models for Language Models'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初步：用于语言模型的概念瓶颈模型
- en: Problem Setup.
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题设置
- en: In this study, we aim to interpret the predictions of fine-tuned Large Language
    Models (LLMs) in text classification tasks. Given a dataset $\mathcal{D}=\{(\bm{x}^{(i)},y^{(i)},\bm{c}^{(i)})_{i=1}^{N}\}$,
    we consider an LLM $f_{\bm{\theta}}$ that encodes an input text $\bm{x}\in\mathbb{R}^{D}$
    into a latent representation $\bm{z}\in\mathbb{R}^{E}$, and a linear classifier
    $g_{\bm{\phi}}$ that maps $\bm{z}$ into the task label $y$.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们旨在解释微调后的大型语言模型（LLMs）在文本分类任务中的预测。给定一个数据集 $\mathcal{D}=\{(\bm{x}^{(i)},y^{(i)},\bm{c}^{(i)})_{i=1}^{N}\}$，我们考虑一个
    LLM $f_{\bm{\theta}}$，该模型将输入文本 $\bm{x}\in\mathbb{R}^{D}$ 编码为潜在表示 $\bm{z}\in\mathbb{R}^{E}$，以及一个线性分类器
    $g_{\bm{\phi}}$，将 $\bm{z}$ 映射到任务标签 $y$。
- en: Incorporate Concept Bottlenecks for Large Language Models.
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为大型语言模型引入概念瓶颈。
- en: 'Our architecture mainly follows Tan et al. ([2023](#bib.bib39)). Instead of
    modifying LLM encoders, which could significantly affect the quality of the learned
    text representation, we introduce a linear layer with sigmoid activation $p_{\bm{\psi}}$.
    This layer projects the learned latent representation $\bm{z}\in\mathbb{R}^{E}$
    into the concept space $\bm{c}\in\mathbb{R}^{K}$, resulting in a pathway represented
    as $\bm{x}\rightarrow\bm{z}\rightarrow\bm{c}\rightarrow y$. Here, we allow multi-class
    concepts for more flexible interpretation. For convenience, we represent CBM-incorporated
    LLMs as LLM-CBMs (e.g., BERT-CBM). LLM-CBMs are trained with two objectives: (1)
    align concept prediction $\hat{\bm{c}}=p_{\bm{\psi}}(f_{\bm{\theta}}(\bm{x}))$
    to $\bm{x}$’s ground-truth concept labels $\bm{c}$, and (2) align label prediction
    $\hat{y}=g_{\bm{\phi}}(p_{\bm{\psi}}(f_{\bm{\theta}}(\bm{x})))$ to ground-truth
    task labels $y$. We mainly experiment with our framework optimized through the
    joint training strategy for its significantly better performance, as also demonstrated
    in Tan et al. ([2023](#bib.bib39)). Jointly training LLM with the concept and
    task labels entails learning the concept encoder and label predictor via a weighted
    sum, $\mathcal{L}_{joint}$, of the two objectives:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的架构主要遵循Tan et al. ([2023](#bib.bib39))。我们没有修改LLM编码器，因为这可能会显著影响学习到的文本表示质量，而是引入了一个带有sigmoid激活的线性层$p_{\bm{\psi}}$。该层将学习到的潜在表示$\bm{z}\in\mathbb{R}^{E}$映射到概念空间$\bm{c}\in\mathbb{R}^{K}$，形成一个表示为$\bm{x}\rightarrow\bm{z}\rightarrow\bm{c}\rightarrow
    y$的路径。在这里，我们允许多类概念以便于更灵活的解释。为方便起见，我们将CBM结合的LLM表示为LLM-CBM（例如，BERT-CBM）。LLM-CBM的训练有两个目标：（1）将概念预测$\hat{\bm{c}}=p_{\bm{\psi}}(f_{\bm{\theta}}(\bm{x}))$与$\bm{x}$的真实概念标签$\bm{c}$对齐，（2）将标签预测$\hat{y}=g_{\bm{\phi}}(p_{\bm{\psi}}(f_{\bm{\theta}}(\bm{x})))$与真实任务标签$y$对齐。我们主要实验了通过联合训练策略优化的框架，因为它的性能显著优于其他方法，这在Tan
    et al. ([2023](#bib.bib39))中也有所展示。联合训练LLM与概念和任务标签需要通过两者目标的加权和$\mathcal{L}_{joint}$来学习概念编码器和标签预测器：
- en: '|  | $\displaystyle\bm{\theta}^{\ast},\bm{\psi}^{\ast},\bm{\phi}^{\ast}$ |
    $\displaystyle=\operatorname*{arg\,min}_{\bm{\theta},\bm{\psi},\bm{\phi}}\mathcal{L}_{joint}(\bm{x},\bm{c},y)$
    |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{\theta}^{\ast},\bm{\psi}^{\ast},\bm{\phi}^{\ast}$ |
    $\displaystyle=\operatorname*{arg\,min}_{\bm{\theta},\bm{\psi},\bm{\phi}}\mathcal{L}_{joint}(\bm{x},\bm{c},y)$
    |  | (1) |'
- en: '|  |  | $\displaystyle=\operatorname*{arg\,min}_{\bm{\theta},\bm{\psi},\bm{\phi}}[\mathcal{L}_{CE}(g_{\bm{\phi}}(p_{\bm{\psi}}(f_{\bm{\theta}}(\bm{x}),y)$
    |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\operatorname*{arg\,min}_{\bm{\theta},\bm{\psi},\bm{\phi}}[\mathcal{L}_{CE}(g_{\bm{\phi}}(p_{\bm{\psi}}(f_{\bm{\theta}}(\bm{x}),y)$
    |  |'
- en: '|  |  | $\displaystyle\quad+\gamma\mathcal{L}_{CE}(p_{\bm{\psi}}(f_{\bm{\theta}}(\bm{x})),\bm{c})].$
    |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad+\gamma\mathcal{L}_{CE}(p_{\bm{\psi}}(f_{\bm{\theta}}(\bm{x})),\bm{c})].$
    |  |'
- en: It’s worth noting that the LLM-CBMs trained jointly are sensitive to the loss
    weight $\gamma$. We set the default value for $\gamma$ as $5.0$ for its better
    performance (Tan et al. [2023](#bib.bib39)). Despite the promising progress having
    been made, present LLM-CBMs typically train all concepts concurrently, leading
    to intertwined parameters for concept prediction, making the process less transparent
    and hampering targeted intervention.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，联合训练的LLM-CBM对损失权重$\gamma$很敏感。我们将$\gamma$的默认值设置为$5.0$以获得更好的性能 (Tan et
    al. [2023](#bib.bib39))。尽管取得了有希望的进展，但目前的LLM-CBM通常同时训练所有概念，导致概念预测的参数相互交织，使得过程不够透明，并妨碍了针对性的干预。
- en: SparseCBMs
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SparseCBM
- en: To address the aforementioned issue, the goal of this paper is to provide a
    holistic and intelligible decision-making pathway for each input text, tracing
    from tokens through subnetworks and concepts, ultimately leading to the final
    task label. To this end, we introduce SparseCBM, a pioneering framework capable
    of unraveling the intricate LLM architectures into a number of concept-specific
    subnetworks. Our approach not only outperforms conventional CBMs in concept and
    task label prediction performance but also proffers enhanced interpretation concerning
    neuron activations, for instance, illuminating which weights inside the LLM backbone
    play pivotal roles in learning specific concepts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，本文的目标是为每个输入文本提供一个全面且易于理解的决策路径，从标记到子网络和概念，最终到达最终任务标签。为此，我们引入了SparseCBM，一个开创性的框架，能够将复杂的LLM架构解开成若干个特定概念的子网络。我们的方法不仅在概念和任务标签预测性能上优于传统的CBM，而且提供了关于神经元激活的更好解释，例如，揭示了LLM主干中哪些权重在学习特定概念中发挥了关键作用。
- en: 'Our framework starts with decomposing the joint optimization defined in Eq. ([1](#Sx3.E1
    "In Incorporate Concept Bottlenecks for Large Language Models. ‣ Preliminary:
    Concept Bottleneck Models for Language Models ‣ Methodology ‣ Sparsity-Guided
    Holistic Explanation for LLMs with Interpretable Inference-Time Intervention"))
    according to each concept $c_{k}$, which is formulated as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架首先对 Eq. ([1](#Sx3.E1 "在大规模语言模型中引入概念瓶颈。 ‣ 初步：语言模型的概念瓶颈模型 ‣ 方法 ‣ 面向可解释推理的稀疏性引导整体解释"))
    中定义的联合优化进行分解，按每个概念 $c_{k}$ 进行，公式如下：
- en: '|  | $\displaystyle\bm{\theta}^{\ast},\bm{\psi}^{\ast},\bm{\phi}^{\ast}$ |
    $\displaystyle=\{(\bm{\theta}^{\ast}_{k})_{k=1}^{K}\},\{(\bm{\psi}^{\ast}_{k})_{k=1}^{K}\},\{(\bm{\phi}^{\ast}_{k})_{k=1}^{K}\}$
    |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{\theta}^{\ast},\bm{\psi}^{\ast},\bm{\phi}^{\ast}$ |
    $\displaystyle=\{(\bm{\theta}^{\ast}_{k})_{k=1}^{K}\},\{(\bm{\psi}^{\ast}_{k})_{k=1}^{K}\},\{(\bm{\phi}^{\ast}_{k})_{k=1}^{K}\}$
    |  | (2) |'
- en: '|  |  | $\displaystyle=\operatorname*{arg\,min}_{\bm{\theta},\bm{\psi},\bm{\phi}}\sum_{k=1}^{K}\mathcal{L}_{joint}(\bm{x},c_{k},y).$
    |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\operatorname*{arg\,min}_{\bm{\theta},\bm{\psi},\bm{\phi}}\sum_{k=1}^{K}\mathcal{L}_{joint}(\bm{x},c_{k},y).$
    |  |'
- en: '|  |  | $\displaystyle=\operatorname*{arg\,min}_{\bm{\theta},\bm{\psi},\bm{\phi}}\sum_{k=1}^{K}[\mathcal{L}_{CE}(g_{\bm{\phi}_{k}}(p_{\bm{\psi}_{k}}(f_{\bm{\theta}}(x),y)$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\operatorname*{arg\,min}_{\bm{\theta},\bm{\psi},\bm{\phi}}\sum_{k=1}^{K}[\mathcal{L}_{CE}(g_{\bm{\phi}_{k}}(p_{\bm{\psi}_{k}}(f_{\bm{\theta}}(x),y)$
    |  |'
- en: '|  |  | $\displaystyle\quad+\gamma\mathcal{L}_{CE}(p_{\bm{\psi}_{k}}(f_{\bm{\theta}}(\bm{x})),c_{k})],$
    |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad+\gamma\mathcal{L}_{CE}(p_{\bm{\psi}_{k}}(f_{\bm{\theta}}(\bm{x})),c_{k})],$
    |  |'
- en: where $\bm{\phi}_{k},\bm{\psi}_{k}$ are the weights of the $k$th parameter of
    the projector and classifier, and $\bm{\theta}_{k}$ is the subnetwork specific
    for the concept $c_{k}$, which is explained later. Since both of them are comprised
    of a single linear layer (with or without the activation function), the involved
    parameters for $c_{k}$ can be directly indexed from these models and are self-interpretable (Koh
    et al. [2020](#bib.bib13); Tan et al. [2023](#bib.bib39)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{\phi}_{k},\bm{\psi}_{k}$ 是第 $k$ 个投影器和分类器的权重，而 $\bm{\theta}_{k}$ 是针对概念
    $c_{k}$ 的特定子网络，稍后将解释。由于它们都由单个线性层（有或没有激活函数）组成，$c_{k}$ 的相关参数可以直接从这些模型中索引，并且是自解释的（Koh
    et al. [2020](#bib.bib13); Tan et al. [2023](#bib.bib39)）。
- en: The remaining task is to excavate concept-specific subnetworks for each concept
    from the vast architecture of Large Language Models (LLMs). The guiding intuition
    behind this strategy is to perceive the prediction of concept labels as individual
    classification tasks, ones that should not strain the entirety of pretrained LLMs
    given their colossal reserves of knowledge encapsulated in multi-million to multi-billion
    parameters. We propose an unstructured pruning of the LLM backbone for each concept
    classification task, such that distinct pruned subnetworks are accountable for
    different concepts while preserving prediction performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的任务是从大规模语言模型（LLMs）的庞大架构中挖掘每个概念特定的子网络。这一策略的指导直觉是将概念标签的预测视为单独的分类任务，这些任务不应使预训练LLMs的整个体系承受压力，因为它们在多百万到多十亿的参数中封装了巨大的知识储备。我们建议对每个概念分类任务进行无结构的LLM骨干修剪，使得不同的修剪子网络负责不同的概念，同时保持预测性能。
- en: Holistic and Intelligible Decision-making Pathways.
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 整体和易懂的决策路径。
- en: We leverage unstructured pruning strategies to carve out concept-specific subnetworks
    within the LLM backbones. The noteworthy edge of such unstructured pruning strategies
    lies in their ability to engender weight masks in accordance with the weight importance.
    Such masks naturally can offer an immediate and clear interpretation. Concretely,
    we introduce a 0/1 weight mask $M_{k}$ for each corresponding subnetwork. Consequently,
    the weights of each subnetwork can be represented as $\bm{\theta}_{\bm{M}_{k}}=\bm{M}_{k}\odot\bm{\theta}^{\ast}$,
    representing the Hadamard (element-wise) product between the LLM weights $\bm{\theta}^{\ast}\in\mathbb{R}^{L}$
    and the weight mask $\bm{M}_{k}\in\{0,1\}^{L}$ for the concept $c_{k}$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用无结构修剪策略在LLM骨干中雕刻出概念特定的子网络。这种无结构修剪策略的显著优势在于其能够根据权重的重要性生成权重掩码。这些掩码自然可以提供即时和清晰的解释。具体来说，我们为每个相应的子网络引入一个
    0/1 权重掩码 $M_{k}$。因此，每个子网络的权重可以表示为 $\bm{\theta}_{\bm{M}_{k}}=\bm{M}_{k}\odot\bm{\theta}^{\ast}$，表示
    LLM 权重 $\bm{\theta}^{\ast}\in\mathbb{R}^{L}$ 和概念 $c_{k}$ 的权重掩码 $\bm{M}_{k}\in\{0,1\}^{L}$
    之间的 Hadamard（逐元素）乘积。
- en: 'With well-optimized $\{(\bm{M})_{k=1}^{K}\}$, during inference, the decision-making
    pathway can be represented as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在良好优化的 $\{(\bm{M})_{k=1}^{K}\}$ 下，在推理过程中，决策路径可以表示为：
- en: '|  | $\hat{y}=\sum_{k=1}^{K}\bm{\phi}_{k}^{\ast}\cdot\sigma(\bm{\psi}_{k}^{\ast}\cdot
    f_{\bm{\theta}_{\bm{M}_{k}}}(\bm{x}))=\sum_{k=1}^{K}\bm{\phi}_{k}^{\ast}\cdot\sigma(\bm{\psi}_{k}^{\ast}\cdot
    f_{\bm{M}_{k}^{\odot}\bm{\theta}^{\ast}}(\bm{x})),$ |  | (3) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}=\sum_{k=1}^{K}\bm{\phi}_{k}^{\ast}\cdot\sigma(\bm{\psi}_{k}^{\ast}\cdot
    f_{\bm{\theta}_{\bm{M}_{k}}}(\bm{x}))=\sum_{k=1}^{K}\bm{\phi}_{k}^{\ast}\cdot\sigma(\bm{\psi}_{k}^{\ast}\cdot
    f_{\bm{M}_{k}^{\odot}\bm{\theta}^{\ast}}(\bm{x})),$ |  | (3) |'
- en: where $\sigma(\cdot)$ is the sigmoid activation function of the projector. This
    decision-making pathway defined in Eq. ([3](#Sx3.E3 "In Holistic and Intelligible
    Decision-making Pathways. ‣ SparseCBMs ‣ Methodology ‣ Sparsity-Guided Holistic
    Explanation for LLMs with Interpretable Inference-Time Intervention")) factorizes
    the parameters of the SparseCBM, and can be optimized through one backward pass
    of the discomposed joint loss defined in Eq. ([2](#Sx3.E2 "In SparseCBMs ‣ Methodology
    ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time
    Intervention")) with $\bm{\theta}_{k}^{\ast}=\bm{\theta}_{\bm{M}_{k}}$. Importantly,
    we posit that such decision-making pathways can deliver holistic explanations
    for the model’s predictions. For instance, by scrutinizing the weights in the
    classifier $g_{\bm{\phi}}$ and the concept activation post the $\sigma$ function,
    we can get a concept-level explanation regarding the importance of different concepts.
    Also, visualizing each subnetwork mask $\bm{M}_{k}$ will furnish a subnetwork-level
    comprehension of neuron behavior and its importance in acquiring a specific concept
    and forming predictions. Additionally, the study of the gradient of input tokens
    in masked concept-specific subnetworks can provide more accurate token-concept
    mapping. Notably, our experiments demonstrate that SparseCBMs, in addition to
    providing multi-dimensional interpretations, can match or even surpass their dense
    counterparts in performance on both concept and task label prediction. Another
    unique feature of SparseCBMs lies in that, the weight masks $\{(\bm{M}_{k})_{k=1}^{K}\}$
    engendered by unstructured pruning facilitates the process of efficient and interpretable
    Sparsity-based Inference-time Intervention, which is expounded later.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma(\cdot)$ 是投影器的 sigmoid 激活函数。此决策路径在方程 ([3](#Sx3.E3 "在整体性和可理解的决策路径中。
    ‣ 稀疏CBMs ‣ 方法论 ‣ 稀疏性引导的整体解释用于具有可解释推理时干预的LLMs")) 中定义，分解了 SparseCBM 的参数，并且可以通过方程 ([2](#Sx3.E2
    "在稀疏CBMs ‣ 方法论 ‣ 稀疏性引导的整体解释用于具有可解释推理时干预的LLMs")) 中定义的分解联合损失的一个反向传递来优化，且 $\bm{\theta}_{k}^{\ast}=\bm{\theta}_{\bm{M}_{k}}$。重要的是，我们假设这种决策路径能够为模型的预测提供整体解释。例如，通过审查分类器
    $g_{\bm{\phi}}$ 中的权重以及 $\sigma$ 函数后的概念激活，我们可以获得有关不同概念重要性的概念级解释。此外，可视化每个子网络掩码 $\bm{M}_{k}$
    将提供对神经元行为的子网络级理解及其在获取特定概念和形成预测中的重要性。此外，研究被掩码的概念特定子网络中的输入令牌的梯度可以提供更准确的令牌-概念映射。值得注意的是，我们的实验表明，除了提供多维解释外，SparseCBMs
    在概念和任务标签预测的性能上能够与其稠密对应物相匹配甚至超越。SparseCBMs 的另一个独特特性在于，由非结构化剪枝生成的权重掩码 $\{(\bm{M}_{k})_{k=1}^{K}\}$
    促进了高效且可解释的基于稀疏性的推理时干预过程，这将在后面详细阐述。
- en: Concept-Induced Sparsity Mining.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概念诱导的稀疏性挖掘。
- en: 'Next, we elaborate on how to compute those sparsity masks, given an optimized
    LLM backbone. A second-order unstructured pruning (Hassibi and Stork [1992](#bib.bib10);
    Kurtic et al. [2022](#bib.bib14)) for LLMs has been incorporated. Initially, the
    joint loss $\mathcal{L}$ (we omit the subscript $joint$ for brevity in subsequent
    equations) can be expanded at the weights of subnetwork $\bm{\theta}_{\bm{M}_{k}}$
    via Taylor expansion:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们详细说明如何计算这些稀疏性掩码，给定一个优化的 LLM 主干。已经纳入了 LLM 的二阶非结构化剪枝 (Hassibi 和 Stork [1992](#bib.bib10);
    Kurtic 等 [2022](#bib.bib14))。最初，联合损失 $\mathcal{L}$（为简洁起见，我们在后续方程中省略了下标 $joint$）可以通过泰勒展开在子网络权重
    $\bm{\theta}_{\bm{M}_{k}}$ 上展开：
- en: '|  | $\displaystyle\mathcal{L}(\bm{\theta}_{\bm{M}_{k}})$ | $\displaystyle\simeq\mathcal{L}(\bm{\theta}^{\ast})+(\bm{\theta}_{\bm{M}_{k}}-\bm{\theta}^{\ast})^{\top}\nabla\mathcal{L}(\bm{\theta}^{\ast})$
    |  | (4) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}(\bm{\theta}_{\bm{M}_{k}})$ | $\displaystyle\simeq\mathcal{L}(\bm{\theta}^{\ast})+(\bm{\theta}_{\bm{M}_{k}}-\bm{\theta}^{\ast})^{\top}\nabla\mathcal{L}(\bm{\theta}^{\ast})$
    |  | (4) |'
- en: '|  |  | $\displaystyle\quad+\frac{1}{2}(\bm{\theta}_{\bm{M}_{k}}-\bm{\theta}^{\ast})^{\top}\bm{H}_{\mathcal{L}}(\bm{\theta}^{\ast})(\bm{\theta_{\bm{M}_{k}}}-\bm{\theta}^{\ast}),$
    |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad+\frac{1}{2}(\bm{\theta}_{\bm{M}_{k}}-\bm{\theta}^{\ast})^{\top}\bm{H}_{\mathcal{L}}(\bm{\theta}^{\ast})(\bm{\theta_{\bm{M}_{k}}}-\bm{\theta}^{\ast}),$
    |  |'
- en: 'where $\bm{H}_{\mathcal{L}}(\bm{\theta}^{\ast})$ stands for the Hessian matrix
    of the decomposed joint loss at $\bm{\theta}^{\ast}$. Since $\bm{\theta}^{\ast}$
    is well-optimized, we assume $\nabla\mathcal{L}(\bm{\theta}^{\ast})\approx 0$
    as the common practice (Hassibi and Stork [1992](#bib.bib10); Kurtic et al. [2022](#bib.bib14)).
    Then, the change in loss after pruning can be represented as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\bm{H}_{\mathcal{L}}(\bm{\theta}^{\ast})$ 代表在 $\bm{\theta}^{\ast}$ 处分解的联合损失的
    Hessian 矩阵。由于 $\bm{\theta}^{\ast}$ 已经经过良好的优化，我们假设 $\nabla\mathcal{L}(\bm{\theta}^{\ast})\approx
    0$，这是常见的做法（Hassibi 和 Stork [1992](#bib.bib10); Kurtic 等 [2022](#bib.bib14)）。然后，剪枝后的损失变化可以表示为：
- en: '|  | $\Delta\mathcal{L}(\Delta\bm{\theta})=\mathcal{L}(\bm{\theta}_{\bm{M}_{k}})-\mathcal{L}(\bm{\theta}^{\ast})\simeq\frac{1}{2}\Delta\bm{\theta}^{\top}\bm{H}_{\mathcal{L}}\Delta\bm{\theta},$
    |  | (5) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\mathcal{L}(\Delta\bm{\theta})=\mathcal{L}(\bm{\theta}_{\bm{M}_{k}})-\mathcal{L}(\bm{\theta}^{\ast})\simeq\frac{1}{2}\Delta\bm{\theta}^{\top}\bm{H}_{\mathcal{L}}\Delta\bm{\theta},$
    |  | (5) |'
- en: 'where, $\Delta\bm{\theta}=\bm{\theta}_{\bm{M}_{k}}-\bm{\theta}^{\ast}$ signifies
    the change in LLM weights, that is, pruned parameters. Given a target sparsity
    $s\in[0,1)$, we seek the minimum loss change incurred by pruning. In our case,
    the default sparsity is designed as: $s\geq 1-\frac{1}{K}$, implying each subnetwork
    contains a maximum of $\frac{1}{K}$ parameters in the dense counterpart. Ideally,
    we desire separate parameters in the LLM backbone to ensure optimal interpretability.
    Then, the problem of computing the sparsity masks can be formulated as a constrained
    optimization task:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\Delta\bm{\theta}=\bm{\theta}_{\bm{M}_{k}}-\bm{\theta}^{\ast}$ 表示 LLM 权重的变化，即被剪枝的参数。给定一个目标稀疏度
    $s\in[0,1)$，我们寻求通过剪枝所带来的最小损失变化。在我们的情况下，默认的稀疏度设计为：$s\geq 1-\frac{1}{K}$，这意味着每个子网络在密集对等体中包含最多
    $\frac{1}{K}$ 个参数。理想情况下，我们希望在 LLM 主干中具有独立的参数，以确保最佳的可解释性。然后，计算稀疏掩码的问题可以被表述为一个有约束的优化任务：
- en: '|  |  | $\displaystyle\min_{\Delta\bm{\theta}}\quad\frac{1}{2}\Delta\bm{\theta}^{\top}\bm{H}_{\mathcal{L}}(\bm{\theta^{\ast}})\Delta\bm{\theta},$
    |  | (6) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\min_{\Delta\bm{\theta}}\quad\frac{1}{2}\Delta\bm{\theta}^{\top}\bm{H}_{\mathcal{L}}(\bm{\theta^{\ast}})\Delta\bm{\theta},$
    |  | (6) |'
- en: '|  |  | $\displaystyle s.t.\quad\bm{e}_{b}^{\top}\Delta\bm{\theta}+\theta_{b}=0,\quad\forall
    b\in\bm{Q},$ |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle s.t.\quad\bm{e}_{b}^{\top}\Delta\bm{\theta}+\theta_{b}=0,\quad\forall
    b\in\bm{Q},$ |  |'
- en: where $\bm{e}_{b}$ denotes the $b$th canonical basis vector of the block of
    weights $\bm{Q}$ to be pruned. This optimization can be solved by approximating
    the Hessian at $\bm{\theta}^{\ast}$ via the dampened empirical Fisher information
    matrix (Hassibi and Stork [1992](#bib.bib10); Kurtic et al. [2022](#bib.bib14)).
    Hence, we can derive the optimized concept-specific masks $\{(\bm{M}_{k})_{k=1}^{K}\}$.
    More details are in Appendix C.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\bm{e}_{b}$ 表示待剪枝的权重块 $\bm{Q}$ 的第 $b$ 个典型基向量。该优化问题可以通过在 $\bm{\theta}^{\ast}$
    处近似计算 Hessian 矩阵来解决，使用的是阻尼的经验 Fisher 信息矩阵（Hassibi 和 Stork [1992](#bib.bib10);
    Kurtic 等 [2022](#bib.bib14)）。因此，我们可以推导出优化后的概念特定掩码 $\{(\bm{M}_{k})_{k=1}^{K}\}$。更多细节见附录
    C。
- en: Sparsity-based Inference-time intervention.
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于稀疏性的推断时干预。
- en: 'SparseCBMs also exhibit the capability to allow inference-time concept intervention
    (a trait inherited from CBMs), thus enabling more comprehensive and user-friendly
    interactions. SparseCBMs allow modulation of the inferred concept activations:
    $\hat{\bm{a}}=\sigma(p_{\bm{\phi}}(f_{\bm{\theta}}(\bm{x})))$. There are two straightforward
    strategies for undertaking such intervention. The first option is the oracle intervention (Koh
    et al. [2020](#bib.bib13)), where human experts manually calibrate the concept
    activations $\hat{\bm{a}}$ and feed them into the classifier. Despite its apparent
    simplicity, oracle intervention directly operates on concept activations and,
    therefore, cannot fix the flawed mapping learned by the LLM backbone. As a consequence,
    the model will replicate the same error when presented with the same input. Meanwhile,
    another strategy involves further fine-tuning the LLM backbone on the test data.
    However, this approach is not only inefficient but also has a high risk of leading
    to significant overfitting on the test data. Those limitations present a barrier
    to the practical implementation of CBMs in high-stakes or time-sensitive applications.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**SparseCBMs** 还展现了允许推理时间概念干预的能力（这一特性继承自CBMs），从而实现更全面和用户友好的互动。**SparseCBMs**
    允许调节推断出的概念激活：$\hat{\bm{a}}=\sigma(p_{\bm{\phi}}(f_{\bm{\theta}}(\bm{x})))$。有两种直接的策略来进行这种干预。第一种选择是oracle干预（Koh等人
    [2020](#bib.bib13)），其中人类专家手动校准概念激活 $\hat{\bm{a}}$ 并将其输入分类器。尽管其表面上看起来很简单，oracle干预直接操作于概念激活，因此不能修复LLM骨干网学到的有缺陷的映射。因此，当模型面对相同的输入时，将重复相同的错误。同时，另一种策略涉及在测试数据上进一步微调LLM骨干网。然而，这种方法不仅效率低下，而且有很高的风险导致对测试数据的过度拟合。这些限制对在高风险或时间敏感的应用中实际实施CBMs构成了障碍。'
- en: As a remedy, we further propose a sparsity-based intervention that is self-interpretable
    and congruent with SparseCBMs. It helps LLMs to learn from each erroneously predicted
    concept during inference time, while preserving overall performance. The core
    idea is to subtly modify the concept-specific masks for the LLM backbone when
    a mispredicted concept is detected. Specifically, parameters of the LLM backbone
    $f_{\theta}$, projector $p_{\psi}$, and the classifier $g_{\phi}$ are frozen,
    while the concept-specific masks $\{(\bm{M}_{k})_{k=1}^{K}\}$ is kept trainable.
    During the test phase, if a concept prediction $\hat{c}_{k}$ for an input text
    $\bm{x}$ is incorrect, we acquire the gradient ${\mathcal{G}_{k}}(\bm{x})$ for
    the corresponding subnetwork $f_{\bm{\theta}_{\bm{M}_{k}}}$, and modulate the
    learned mask $\bm{M}_{k}$ accordingly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种补救措施，我们进一步提出了一种基于稀疏性的干预方法，该方法是自我解释的，并且与**SparseCBMs**一致。它帮助LLMs在推理过程中从每一个错误预测的概念中学习，同时保持整体性能。核心思想是在检测到预测错误的概念时，微妙地修改LLM骨干网的概念特定掩码。具体来说，LLM骨干网
    $f_{\theta}$、投影器 $p_{\psi}$ 和分类器 $g_{\phi}$ 的参数保持冻结，而概念特定掩码 $\{(\bm{M}_{k})_{k=1}^{K}\}$
    保持可训练。在测试阶段，如果对输入文本 $\bm{x}$ 的概念预测 $\hat{c}_{k}$ 不正确，我们将获取相应子网络 $f_{\bm{\theta}_{\bm{M}_{k}}}$
    的梯度 ${\mathcal{G}_{k}}(\bm{x})$，并相应地调节学习到的掩码 $\bm{M}_{k}$。
- en: 'Inspired by Evci et al. ([2020](#bib.bib5)); Sun et al. ([2023](#bib.bib38)),
    we define the saliency scores for LLM parameters by the $l_{2}$-norm of the product
    of the gradient of the mask and the parameter weights: $\mathcal{S}=\|\mathcal{G}_{k}(\bm{x})\cdot\bm{\theta}^{\ast}\|$.
    Subsequently, we perform the following two operations based on the saliency scores:
    (1) Drop a proportion of $r$ unpruned weights with the lowest saliency scores:
    $\operatorname*{arg\,min}_{m}^{r\cdot|\bm{\theta}|}\mathcal{S}_{m}$, $\forall
    m\in|\bm{\theta}_{\bm{M}_{k}}|$. (2) Grow a proportion of $r$ pruned weights with
    the highest saliency scores: $\operatorname*{arg\,max}_{m}^{r\cdot|\bm{\theta}|}\mathcal{S}_{m}$,
    $\forall m\in|\bm{\theta}\setminus\bm{\theta}_{\bm{M}_{k}}|$. Here $m$ refers
    to the parameter index of the LLM backbone. By dropping and growing an equal number
    of parameters, the overall sparsity $s$ of the LLM backbone remains unchanged.
    This mask-level intervention is further optimized through the decomposed joint
    loss $\mathcal{L}_{joint}$ defined in Eq. ([2](#Sx3.E2 "In SparseCBMs ‣ Methodology
    ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time
    Intervention")). Note that $r$ is set as a relatively small value (e.g., 0.01)
    to compel the model to retain the overall performance while learning from the
    mistake. Our experiments validate that the proposed sparsity-based intervention
    can effectively enhance inference-time accuracy without necessitating training
    of the entire LLM backbone. Also, the intervened parameters provide insight into
    the parameters that contributed to each misprediction.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 受 Evci 等人（[2020](#bib.bib5)）和 Sun 等人（[2023](#bib.bib38)）的启发，我们通过掩码梯度和参数权重乘积的
    $l_{2}$-范数定义 LLM 参数的显著性分数：$\mathcal{S}=\|\mathcal{G}_{k}(\bm{x})\cdot\bm{\theta}^{\ast}\|$。随后，我们根据显著性分数执行以下两个操作：（1）丢弃比例为
    $r$ 的最低显著性分数的未修剪权重：$\operatorname*{arg\,min}_{m}^{r\cdot|\bm{\theta}|}\mathcal{S}_{m}$，$\forall
    m\in|\bm{\theta}_{\bm{M}_{k}}|$。（2）增长比例为 $r$ 的最高显著性分数的修剪权重：$\operatorname*{arg\,max}_{m}^{r\cdot|\bm{\theta}|}\mathcal{S}_{m}$，$\forall
    m\in|\bm{\theta}\setminus\bm{\theta}_{\bm{M}_{k}}|$。这里的 $m$ 指的是 LLM 主干网络的参数索引。通过丢弃和增长相等数量的参数，LLM
    主干网络的整体稀疏度 $s$ 保持不变。该掩码级干预通过在公式 ([2](#Sx3.E2 "In SparseCBMs ‣ Methodology ‣ Sparsity-Guided
    Holistic Explanation for LLMs with Interpretable Inference-Time Intervention"))
    中定义的分解联合损失 $\mathcal{L}_{joint}$ 进一步优化。注意，$r$ 被设置为相对较小的值（例如 0.01），以促使模型在从错误中学习的同时保持整体性能。我们的实验验证了所提出的基于稀疏度的干预可以有效提高推理时的准确性，而无需训练整个
    LLM 主干网络。此外，干预后的参数提供了关于每个预测错误贡献的参数的见解。
- en: Experiments
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: Experimental Setup
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验设置
- en: Datasets.
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集。
- en: 'Our experiments are conducted on two widely-used real-world datasets: CEBaB (Abraham
    et al. [2022](#bib.bib1)) and IMDB-C (Tan et al. [2023](#bib.bib39)). Each of
    them is a text-classification dataset comprised of human-annotated concept and
    task labels. Their statistics are presented in Table [1](#Sx4.T1 "Table 1 ‣ LLM
    backbones. ‣ Experimental Setup ‣ Experiments ‣ Sparsity-Guided Holistic Explanation
    for LLMs with Interpretable Inference-Time Intervention").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验在两个广泛使用的真实世界数据集上进行：CEBaB（Abraham 等人 [2022](#bib.bib1)）和 IMDB-C（Tan 等人 [2023](#bib.bib39)）。每个数据集都是一个由人工标注的概念和任务标签组成的文本分类数据集。它们的统计数据呈现在表
    [1](#Sx4.T1 "Table 1 ‣ LLM backbones. ‣ Experimental Setup ‣ Experiments ‣ Sparsity-Guided
    Holistic Explanation for LLMs with Interpretable Inference-Time Intervention")
    中。
- en: LLM backbones.
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 主干网络。
- en: 'In this research, we primarily consider two widely-recognized, open-source
    lineages of pretrained LLMs: the BERT-family models (Devlin et al. [2018](#bib.bib4);
    Liu et al. [2019](#bib.bib24); Sanh et al. [2019](#bib.bib36)) and OPT-family
    models (Zhang et al. [2022](#bib.bib50)). Specially, we also include directly
    prompting GPT4 (OpenAI [2023](#bib.bib32)) as a baseline to let it generate concept
    and task labels for given texts. Even though being proprietary, GPT4 is widely
    regarded as the most capable LLM currently, so we choose it as the baseline backbone.
    For better performance, we obtain the representations of the input texts by pooling
    the embedding of all tokens. Reported scores are the averages of three independent
    runs. Our work is based on general text classification implementations. The PyTorch
    Implementation is available at https://github.com/Zhen-Tan-dmml/SparseCBM.git.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们主要考虑了两种广泛认可的开源预训练LLM系列：BERT家族模型（Devlin等人 [2018](#bib.bib4); Liu等人 [2019](#bib.bib24);
    Sanh等人 [2019](#bib.bib36)）和OPT家族模型（Zhang等人 [2022](#bib.bib50)）。特别地，我们还将直接提示GPT4（OpenAI
    [2023](#bib.bib32)）作为基准，让其为给定文本生成概念和任务标签。尽管GPT4是专有的，但它被广泛认为是目前最强大的LLM，因此我们选择它作为基准骨干。为了获得更好的性能，我们通过汇总所有标记的嵌入来获取输入文本的表示。报告的分数是三次独立运行的平均值。我们的工作基于通用文本分类实现。PyTorch实现可在
    https://github.com/Zhen-Tan-dmml/SparseCBM.git 获得。
- en: '| Dataset | CEBaB (5-way classification) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | CEBaB (5类分类) |'
- en: '| Train/Dev/Test | 1755 / 1673 / 1685 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 训练/开发/测试 | 1755 / 1673 / 1685 |'
- en: '| Concept | Concept | Negative | Positive | Unknown |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 概念 | 概念 | 负面 | 正面 | 未知 |'
- en: '| Food | 1693 (33%) | 2087 (41%) | 1333 (26%) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 食品 | 1693 (33%) | 2087 (41%) | 1333 (26%) |'
- en: '| Ambiance | 787 (15%) | 994 (20%) | 3332 (65%) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 787 (15%) | 994 (20%) | 3332 (65%) |'
- en: '| Service | 1249 (24%) | 1397 (27%) | 2467 (49%) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | 1249 (24%) | 1397 (27%) | 2467 (49%) |'
- en: '| Noise | 645 (13%) | 442 (9%) | 4026 (78%) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 噪音 | 645 (13%) | 442 (9%) | 4026 (78%) |'
- en: '| Dataset | IMDB-C (2-way classification) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | IMDB-C (2类分类) |'
- en: '| Train/Dev/Test | 100 / 50 / 50 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 训练/开发/测试 | 100 / 50 / 50 |'
- en: '| Concept | Concept | Negative | Positive | Unknown |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 概念 | 概念 | 负面 | 正面 | 未知 |'
- en: '| Acting | 76 (38%) | 66 (33%) | 58 (29%) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 表演 | 76 (38%) | 66 (33%) | 58 (29%) |'
- en: '| Storyline | 80 (40%) | 77 (38%) | 43 (22%) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 故事情节 | 80 (40%) | 77 (38%) | 43 (22%) |'
- en: '| Emotion | 74 (37%) | 73 (36%) | 53 (27%) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 情感 | 74 (37%) | 73 (36%) | 53 (27%) |'
- en: '| Cinematography | 118 (59%) | 43 (22%) | 39 (19%) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 电影摄影 | 118 (59%) | 43 (22%) | 39 (19%) |'
- en: 'Table 1: Statistics of experimented datasets and concepts.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 实验数据集和概念的统计信息。'
- en: '| Backbone | Acc. / F1 | CEBaB | IMDB-C |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 主干 | 准确率 / F1 | CEBaB | IMDB-C |'
- en: '| --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Concept | Task | Concept | Task |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 概念 | 任务 | 概念 | 任务 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT4 | Prompt | 75.9/71.5 | 51.3/45.9 | 64.5/61.5 | 71.4/68.7 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 提示 | 75.9/71.5 | 51.3/45.9 | 64.5/61.5 | 71.4/68.7 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DistilBERT | Standard | - | 70.3/80.4 | - | 77.1/73.8 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| DistilBERT | 标准 | - | 70.3/80.4 | - | 77.1/73.8 |'
- en: '| CBM | 81.1/83.5 | 63.9/76.5 | 67.5/63.8 | 76.5/69.8 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| CBM | 81.1/83.5 | 63.9/76.5 | 67.5/63.8 | 76.5/69.8 |'
- en: '| SparseCBM | 82.0/84.0 | 64.7/77.1 | 68.4/64.3 | 76.9/71.4 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SparseCBM | 82.0/84.0 | 64.7/77.1 | 68.4/64.3 | 76.9/71.4 |'
- en: '| BERT | Standard | - | 67.9/79.8 | - | 78.3/72.1 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 标准 | - | 67.9/79.8 | - | 78.3/72.1 |'
- en: '| CBM | 83.2/85.3 | 66.9/78.1 | 68.2/62.8 | 77.3/70.4 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| CBM | 83.2/85.3 | 66.9/78.1 | 68.2/62.8 | 77.3/70.4 |'
- en: '| SparseCBM | 83.5/85.6 | 66.9/79.1 | 69.8/65.2 | 76.5/71.6 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SparseCBM | 83.5/85.6 | 66.9/79.1 | 69.8/65.2 | 76.5/71.6 |'
- en: '| RoBERTa | Standard | - | 71.8/81.3 | - | 82.2/77.3 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | 标准 | - | 71.8/81.3 | - | 82.2/77.3 |'
- en: '| CBM | 82.6/84.9 | 70.1/81.3 | 69.9/68.9 | 81.4/79.3 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| CBM | 82.6/84.9 | 70.1/81.3 | 69.9/68.9 | 81.4/79.3 |'
- en: '| SparseCBM | 82.8/85.5 | 70.3/81.4 | 70.2/69.7 | 81.5/79.9 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SparseCBM | 82.8/85.5 | 70.3/81.4 | 70.2/69.7 | 81.5/79.9 |'
- en: '| OPT-125M | Standard | - | 70.8/81.4 | - | 84.3/80.0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125M | 标准 | - | 70.8/81.4 | - | 84.3/80.0 |'
- en: '| CBM | 85.4/87.3 | 68.9/79.7 | 68.7/66.5 | 81.8/78.2 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| CBM | 85.4/87.3 | 68.9/79.7 | 68.7/66.5 | 81.8/78.2 |'
- en: '| SparseCBM | 86.2/88.0 | 68.9/79.8 | 70.0/67.4 | 82.6/79.9 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SparseCBM | 86.2/88.0 | 68.9/79.8 | 70.0/67.4 | 82.6/79.9 |'
- en: '| OPT-350M | Standard | - | 71.6/82.6 | - | 86.4/83.5 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350M | 标准 | - | 71.6/82.6 | - | 86.4/83.5 |'
- en: '| CBM | 87.8/89.4 | 69.9/80.7 | 72.6/70.5 | 84.5/82.4 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CBM | 87.8/89.4 | 69.9/80.7 | 72.6/70.5 | 84.5/82.4 |'
- en: '| SparseCBM | 87.3/88.7 | 68.2/79.8 | 73.3/71.1 | 85.0/82.5 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SparseCBM | 87.3/88.7 | 68.2/79.8 | 73.3/71.1 | 85.0/82.5 |'
- en: '| OPT-1.3B | Standard | - | 74.7/83.9 | - | 88.4/83.7 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3B | 标准 | - | 74.7/83.9 | - | 88.4/83.7 |'
- en: '| CBM | 90.0/91.5 | 73.6/82.1 | 76.8/74.6 | 85.7/83.3 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| CBM | 90.0/91.5 | 73.6/82.1 | 76.8/74.6 | 85.7/83.3 |'
- en: '| SparseCBM | 89.9/91.6 | 73.8/82.6 | 76.4/74.7 | 86.6/83.9 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SparseCBM | 89.9/91.6 | 73.8/82.6 | 76.4/74.7 | 86.6/83.9 |'
- en: 'Table 2: Comparisons of task accuracy and interpretability using CEBaB and
    IMDB-C datasets with BERT-family and OPT-family models as the backbones. Metrics
    for both task and concept labels are Accuracy/Macro F1 in $\%$. A score in bold
    indicate that the SparseCBM under the current setting outperforms its dense CBM
    counterpart.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：使用 CEBaB 和 IMDB-C 数据集，对 BERT 系列和 OPT 系列模型作为骨干的任务准确性和可解释性的比较。任务和概念标签的指标为
    $\%$ 中的准确率/宏 F1。粗体分数表示在当前设置下，SparseCBM 的表现优于其稠密 CBM 对应物。
- en: Interpretability
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释性
- en: Utility v.s. Interpretability.
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实用性与可解释性。
- en: 'Table [2](#Sx4.T2 "Table 2 ‣ LLM backbones. ‣ Experimental Setup ‣ Experiments
    ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time
    Intervention") presents the performance of the concept and task label prediction:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#Sx4.T2 "表 2 ‣ LLM 骨干。 ‣ 实验设置 ‣ 实验 ‣ 稀疏引导的 LLM 全面解释与可解释的推理干预") 展示了概念和任务标签预测的性能：
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multidimensional Interpretability: SparseCBMs stand out by offering multidimensional
    interpretability without compromising task prediction performance. In comparison
    with standard LLMs (which are fine-tuned exclusively with task labels), SparseCBMs
    grant concept-level interpretability with only a slight dip in task prediction
    accuracy. Impressively, SparseCBMs can match or even outperform their dense CBM
    counterparts while providing multifaceted explanations that extend beyond mere
    concepts. This underlines the potency of SparseCBMs in striking a balance between
    interpretability and utility.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多维可解释性：SparseCBM 的特点是提供多维可解释性，同时不影响任务预测性能。与标准 LLM（仅通过任务标签进行微调）相比，SparseCBM 仅在任务预测准确率略有下降的情况下，提供概念级别的可解释性。令人印象深刻的是，SparseCBM
    可以匹配甚至超越其稠密 CBM 对应物，同时提供超越单一概念的多方面解释。这突显了 SparseCBM 在实现可解释性和实用性之间平衡的强大能力。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scalability with Larger LLM Backbones: The utilization of larger LLMs within
    SparseCBMs leads to superior interpretability-utility Pareto fronts. This observation
    validates our guiding hypothesis that predicting concept labels should not strain
    the entirety of pretrained LLMs as they are individual classification tasks. Larger
    LLMs, being repositories of more knowledge through increased parameters, facilitate
    easier pruning.'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着更大 LLM 骨干的可扩展性：在 SparseCBM 中使用更大的 LLMs 使得可解释性-实用性 Pareto 前沿得到提升。这一观察验证了我们的指导假设，即预测概念标签不应对预训练
    LLM 的整体产生压力，因为它们是单独的分类任务。较大的 LLMs 通过增加参数存储更多知识，从而使得修剪变得更容易。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Limitations of Direct Prompting: When directly prompting LLMs, such as GPT4
    (without fine-tuning on the target datasets), to predict concept and task labels,
    the resulting performance is noticeably inferior. This highlights the necessity
    of learning concepts and task labels in target domains. Additionally, since LLMs’
    task predictions are autoregressively generated and do not rely entirely on the
    generated concepts, doubts arise regarding the reliability of concept-level explanations.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接提示的局限性：当直接提示 LLM，如 GPT4（没有在目标数据集上微调）来预测概念和任务标签时，结果性能明显较差。这突显了在目标领域学习概念和任务标签的必要性。此外，由于
    LLM 的任务预测是自回归生成的，并不完全依赖生成的概念，因此对概念级别解释的可靠性产生了疑问。
- en: '![Refer to caption](img/e2f825975d6a60fe7e2a2bdec49d8ebe.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/e2f825975d6a60fe7e2a2bdec49d8ebe.png)'
- en: 'Figure 2: The illustration of a decision pathway of a toy example from the
    SparseCBM framework with BERT as the backbone. The binary weight masks for each
    concept is represented as a heatmap.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：来自 SparseCBM 框架的玩具示例决策路径的示意图，BERT 作为骨干。每个概念的二进制权重掩码以热图形式表示。
- en: Explainable Prediction Pathways.
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可解释的预测路径。
- en: 'The centerpiece of this paper revolves around providing a transparent and interpretive
    decision-making pathway for each input text vector $\bm{x}=[t_{1},t_{2},\cdots,t_{d},\cdots,t_{D}]$,
    where $t_{d},\forall d\in D$ denotes the tokens in the input text. SparseCBMs,
    at inference time, unravel the following layers of understanding across the decision-making
    trajectory:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的核心在于为每个输入文本向量 $\bm{x}=[t_{1},t_{2},\cdots,t_{d},\cdots,t_{D}]$ 提供透明和可解释的决策路径，其中
    $t_{d},\forall d\in D$ 表示输入文本中的令牌。在推理时，SparseCBM 揭示了决策路径中以下理解层次：
- en: '1.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Subnetwork-Level Explanation: Identification of specific neurons within the
    LLM backbones responsible for corresponding concepts. This insight is achieved
    by visualizing individual binary subnetwork masks $\bm{M}_{k}$.'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 子网络级解释：识别LLM主干中负责对应概念的特定神经元。这一洞察通过可视化单个二进制子网络掩码$\bm{M}_{k}$来实现。
- en: '2.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Token-Level Explanation: Detection of the tokens instrumental in shaping a
    particular concept. This analysis is carried out by evaluating the gradient of
    each subnetwork mask with respect to individual tokens $\|\mathcal{G}_{k}(t_{d})\|$.'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令牌级解释：检测对形成特定概念至关重要的令牌。这项分析通过评估每个子网络掩码相对于单个令牌的梯度$\|\mathcal{G}_{k}(t_{d})\|$来进行。
- en: '3.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Concept-Level Explanation: Understanding the predicted concept labels $\hat{c}_{k}$
    and their contribution to the final prediction. This is captured by computing
    the dot product between each predicted concept activation and the corresponding
    weight of the linear predictor: $\bm{\phi}_{k}\cdot\hat{\bm{a}}_{k}$.'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 概念级解释：理解预测的概念标签$\hat{c}_{k}$及其对最终预测的贡献。这通过计算每个预测概念激活与线性预测器对应权重之间的点积来捕捉：$\bm{\phi}_{k}\cdot\hat{\bm{a}}_{k}$。
- en: 'A schematic representation of the decision-making process for a representative
    example is provided in Figure [2](#Sx4.F2 "Figure 2 ‣ Utility v.s. Interpretability.
    ‣ Interpretability ‣ Experiments ‣ Sparsity-Guided Holistic Explanation for LLMs
    with Interpretable Inference-Time Intervention"), with “Neg Concept” denoting
    negative concept values. Additional real-world examples are delineated in Appendix D.
    Several interesting findings can be drawn from those results:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#Sx4.F2 "Figure 2 ‣ Utility v.s. Interpretability. ‣ Interpretability ‣
    Experiments ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable
    Inference-Time Intervention")提供了一个代表性示例的决策过程示意图，其中“Neg Concept”表示负概念值。附录D中详细描述了更多的现实世界示例。从这些结果中可以得出几个有趣的发现：
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Neural Responsibility Across Concepts: Various concepts necessitate differing
    proportions of neurons in the LLM backbone for learning. This resonates with our
    ambition to demystify the “black-box” LLM backbones by partitioning them into
    distinct subnetworks, each accountable for an individual concept. Intriguingly,
    overlaps exist among subnetworks, reflecting that strict disentanglement constraints
    were not imposed on the backbone parameters. This opens avenues for future research
    into entirely concept-wise disentangled LLM backbones.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经责任与概念：不同的概念需要LLM主干中的不同神经元比例进行学习。这与我们通过将LLM主干划分为不同子网络，每个子网络负责一个单独概念的愿景相吻合。令人感兴趣的是，子网络之间存在重叠，这反映出对主干参数没有施加严格的解耦约束。这为未来研究完全按概念解耦的LLM主干开辟了新的方向。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Holistic Decision Pathway: The SparseCBM framework successfully crafts a comprehensive
    decision-making path that navigates from tokens, through subnetworks and concepts,
    culminating in the final task label. This rich interpretability paves the way
    for unique insights into practical applications. For instance, although concepts
    like “Food” and “Ambiance” may carry identical positive values, the “Food” concept
    may wield greater influence on the final task label. Additionally, careful examination
    of parameter masks can shed light on the root causes behind mispredicted concepts,
    enabling effective and interpretable interventions. We explore this topic further
    in the subsequent section.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 整体决策路径：SparseCBM框架成功地构建了一个全面的决策路径，从令牌，通过子网络和概念，最终到达任务标签。这种丰富的可解释性为实际应用提供了独特的见解。例如，虽然“食品”和“氛围”这样的概念可能具有相同的正值，但“食品”概念可能对最终任务标签有更大的影响。此外，对参数掩码的仔细检查可以揭示预测错误概念的根本原因，从而实现有效且可解释的干预。我们将在接下来的部分进一步探讨这一主题。
- en: Inference-time Intervention
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理时间干预
- en: '![Refer to caption](img/ea049a10dc8e89478cad637b6f78ff71.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ea049a10dc8e89478cad637b6f78ff71.png)'
- en: (a) Concept Prediction
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: （a）概念预测
- en: '![Refer to caption](img/f0f5d9514901becaef4bc9b7c00ef7cd.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f0f5d9514901becaef4bc9b7c00ef7cd.png)'
- en: (b) Task Prediction
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: （b）任务预测
- en: 'Figure 3: The results of Test-time Intervention. “NI” denotes “no intervention”,
    “SI” denotes “Sparsity-based Intervention”. (a) and (b) represent the results
    for concept and task label prediction respectively. The x-axis indicates the proportion
    ($r$) of the weights to perform the intervention.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：测试时间干预的结果。“NI”表示“无干预”，“SI”表示“基于稀疏性的干预”。（a）和（b）分别表示概念和任务标签预测的结果。x轴表示进行干预的权重比例($r$)。
- en: SparseCBMs distinguish themselves by enabling sparsity-based inference-time
    intervention, compared to vannila CBMs. This innovative feature creates a pathway
    for more refined, user-centric interactions by subtly adjusting the masks without
    the need for direct retraining of the LLM backbone. The significance of this intervention
    approach lies in its application to real-world scenarios where users often find
    it easier to articulate broad concepts (e.g., food quality) rather than precise
    sentiment scores or categorical labels.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SparseCBMs通过实现基于稀疏性的推理时干预，与原始CBMs区分开来。这一创新特性通过微妙调整掩码而无需直接重新训练LLM骨干网络，为更精细、以用户为中心的交互提供了一条途径。这种干预方法的重要性在于其应用于实际场景中，在这些场景中，用户通常更容易表述广泛的概念（例如，食品质量），而不是具体的情感分数或类别标签。
- en: Experimental Evaluation.
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验评估。
- en: To methodically evaluate this intervention strategy, extensive experiments were
    conducted on the CEBaB dataset, employing DistilBERT as the representative LLM
    backbone. The insights gleaned from these experiments apply consistently to other
    LLMs as well. Figure [3](#Sx4.F3 "Figure 3 ‣ Inference-time Intervention ‣ Experiments
    ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time
    Intervention") provides a detailed comparison between concept and task label predictions
    using SparseCBMs against a baseline, where a vanilla DistilBERT is independently
    trained to classify concept or task labels. These baseline scores serve as a theoretical
    upper bound for prediction accuracy, providing a reliable and illustrative benchmark.
    This analytical exploration not only validates the proposed sparsity-based intervention’s
    efficacy in enhancing inference-time accuracy for both concept and task predictions
    but also reveals its elegance in execution. With minimal alterations to the underlying
    model structure, remarkable improvements are achieved. Even for a relatively small
    model, DistilBERT, the optimal adjustment proportion is found to be a mere $1\%$,
    translating to modifications in only $2\%$ of the backbone parameters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了系统评估这一干预策略，我们在CEBaB数据集上进行了广泛的实验，使用DistilBERT作为代表性LLM骨干网络。这些实验所得的见解也适用于其他LLM。图 [3](#Sx4.F3
    "Figure 3 ‣ Inference-time Intervention ‣ Experiments ‣ Sparsity-Guided Holistic
    Explanation for LLMs with Interpretable Inference-Time Intervention")提供了使用SparseCBMs与基准模型对概念和任务标签预测的详细比较，其中一个原始DistilBERT被独立训练用于分类概念或任务标签。这些基准分数作为预测准确性的理论上限，提供了一个可靠且具有说明性的基准。这一分析探讨不仅验证了所提出的基于稀疏性的干预在提高概念和任务预测的推理准确性方面的有效性，还揭示了其在执行中的优雅性。即使对相对较小的模型DistilBERT，最优调整比例也仅为$1\%$，这意味着只修改了$2\%$的骨干参数。
- en: '![Refer to caption](img/3fe55be858452d5b96e30dbf22ae8922.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3fe55be858452d5b96e30dbf22ae8922.png)'
- en: 'Figure 4: Illustration of the explainable prediction for a real-world example
    from the IMDB-C dataset using OPT-350m as the backbone. The brown boxes with dash
    lines indicate the test-time intervention on corresponding concepts by modulating
    the corresponding mask. $\bm{M}_{2}$ and $\bm{M}_{2}^{\prime}$ denote the parameter
    masks for the second concept, “Acting”, before and after the intervention, respectively.
    We visualize $\bm{M}_{2}^{\prime}$ after seeing all test samples.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用OPT-350m作为骨干的IMDB-C数据集真实世界示例的可解释预测说明。棕色框和虚线表示通过调整相应的掩码对相应概念进行测试时干预。$\bm{M}_{2}$和$\bm{M}_{2}^{\prime}$分别表示第二个概念“表演”的干预前后参数掩码。我们在查看所有测试样本后可视化$\bm{M}_{2}^{\prime}$。
- en: Robustness and Adaptability.
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 鲁棒性与适应性。
- en: These results shed light on the broader applicability and resilience of sparsity-based
    intervention across various contexts and domains. The capacity to implement such
    nuanced adjustments without the resource-intensive process of retraining the entire
    model marks a substantial advancement toward more agile, responsive machine learning
    systems. This adaptability resonates with the growing demand for models that can
    quickly adapt to ever-changing requirements without compromising on performance
    or interpretability.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果揭示了基于稀疏性的干预在各种背景和领域中的广泛适用性和弹性。能够在不需要资源密集型的重新训练整个模型的过程中实现这种细微调整，标志着朝着更灵活、响应迅速的机器学习系统迈出了重要一步。这种适应性与对能够快速适应不断变化需求而不妥协于性能或可解释性的模型的日益增长的需求相呼应。
- en: Case Study and Insights.
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例研究与见解。
- en: 'To provide an in-depth illustration, a case study depicting the sparsity-based
    intervention process is presented in Figure [4](#Sx4.F4 "Figure 4 ‣ Experimental
    Evaluation. ‣ Inference-time Intervention ‣ Experiments ‣ Sparsity-Guided Holistic
    Explanation for LLMs with Interpretable Inference-Time Intervention"). This visualization
    elucidates how the predicted label for the concept “Acting” can be transformed
    from incorrect “ -” to correct “+”, subsequently refining the final task label.
    But the insights run deeper: by visualizing the parameter masks before ($\bm{M}_{2}$)
    and after ($\bm{M}_{2}^{\prime}$) the intervention, we expose the neural mechanics
    behind the misprediction and the corrective strategy at the neuron level. This
    ability to not only correct but also interpret the underlying reasons for prediction
    errors enhances the overall trustworthiness and usability of the model. In conjunction
    with the experimental findings, this case study amplifies our understanding of
    the potential for sparsity-based interventions, not merely as a method for model
    fine-tuning, but as a principled approach towards more transparent and adaptable
    AI systems.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供深入的说明，图[4](#Sx4.F4 "Figure 4 ‣ Experimental Evaluation. ‣ Inference-time
    Intervention ‣ Experiments ‣ Sparsity-Guided Holistic Explanation for LLMs with
    Interpretable Inference-Time Intervention")中展示了一个描绘稀疏性干预过程的案例研究。该可视化展示了如何将概念“Acting”的预测标签从错误的“
    -”转变为正确的“+”，从而细化最终任务标签。但这些见解更深层次：通过在干预前（$\bm{M}_{2}$）和干预后（$\bm{M}_{2}^{\prime}$）可视化参数掩码，我们揭示了误预测的神经机制以及在神经元层面上的修正策略。这种不仅能纠正而且能解释预测错误根本原因的能力增强了模型的整体可信度和可用性。与实验结果结合，这个案例研究增强了我们对基于稀疏性的干预潜力的理解，不仅仅是作为模型微调的方法，更是朝向更透明和适应性更强的AI系统的原则性方法。
- en: Implication.
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 含义。
- en: The integration of sparsity-based inference-time intervention within SparseCBMs
    represents a confluence of accuracy, flexibility, and interpretability. Through
    careful experimentation and insightful case studies, this work lays the groundwork
    for models that respond dynamically to the needs of users, augmenting human-machine
    collaboration in complex decision-making processes. It is a promising step towards
    building AI models that are not only more effective but also more aligned with
    the human-centered objectives and ethical considerations of modern machine learning
    applications.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在SparseCBMs中，基于稀疏性的推理干预的整合代表了精确性、灵活性和可解释性的汇聚。通过精心实验和深刻的案例研究，这项工作为能够动态响应用户需求的模型奠定了基础，增强了人机协作在复杂决策过程中的作用。这是朝着建立更有效且更符合以人为本目标和现代机器学习应用伦理考量的AI模型迈出的有前景的一步。
- en: Sensitivity Analysis on the Sparsity $s$
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏性$s$的敏感性分析
- en: 'In Figure [5](#Sx4.F5 "Figure 5 ‣ Sensitivity Analysis on the Sparsity 𝑠 ‣
    Experiments ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable
    Inference-Time Intervention"), we study the effect of target Large Language Model
    (LLM) sparsity on concept and task prediction performance across various LLM sizes.
    The results reveal an interesting trend: larger LLMs tend to have a higher optimal
    sparsity level compared to smaller ones. This is attributed to the greater knowledge
    repository and higher redundancy present in larger LLMs, allowing for more extensive
    pruning without significant performance loss. However, a delicate balance must
    be struck. While larger LLMs can accommodate more pruning, overdoing it may harm
    performance. Identifying this balance remains an intriguing avenue for future
    research, as well as investigating how different pruning strategies interact with
    various tasks and data distributions.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](#Sx4.F5 "Figure 5 ‣ Sensitivity Analysis on the Sparsity 𝑠 ‣ Experiments
    ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time
    Intervention")中，我们研究了目标大语言模型（LLM）稀疏性对概念和任务预测性能的影响，涵盖了不同LLM尺寸的情况。结果揭示了一个有趣的趋势：较大的LLM倾向于具有比小型LLM更高的最佳稀疏性水平。这归因于较大的LLM拥有更丰富的知识库和更高的冗余性，允许进行更广泛的修剪而不会显著损失性能。然而，必须找到一个微妙的平衡点。虽然较大的LLM可以容纳更多的修剪，但过度修剪可能会损害性能。确定这一平衡仍然是未来研究的一个引人入胜的方向，同时研究不同修剪策略如何与各种任务和数据分布相互作用。
- en: '![Refer to caption](img/053c99e0191af1b2a0e0414a9ebba68a.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/053c99e0191af1b2a0e0414a9ebba68a.png)'
- en: 'Figure 5: The performance of SparseCBMs across varying LLM backbones in relation
    to the target sparsity $s$ on the CEBaB dataset. Solid lines delineate scores
    for concept label predictions. Dashed lines capture those for task label predictions.
    Notably, larger LLM backbones are adept at handling increased sparsity without
    compromising on prediction efficacy. Nonetheless, excessive pruning invariably
    impinges on the performance across all LLM backbones.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：SparseCBMs 在 CEBaB 数据集上针对目标稀疏度 $s$ 不同 LLM 骨干网络的性能。实线表示概念标签预测的得分，虚线表示任务标签预测的得分。值得注意的是，较大的
    LLM 骨干网络能够处理增加的稀疏性而不影响预测效果。然而，过度剪枝必然会影响所有 LLM 骨干网络的性能。
- en: Conclusion
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this study, we introduced Sparse Concept Bottleneck Models (SparseCBMs),
    a novel method integrating the interpretability of Concept Bottleneck Models (CBMs)
    with the efficiency of unstructured pruning. By exploiting the properties of second-order
    pruning, we constructed concept-specific sparse subnetworks in a Large Language
    Model (LLM) backbone, thereby providing multidimensional interpretability while
    retaining model performance. Additionally, we proposed a sparsity-driven inference-time
    intervention mechanism that improves accuracy at inference time, without the need
    for expensive fine-tuning LLMs. This intervention mechanism effectively identifies
    the parameters that contribute to each misprediction, enhancing interpretability
    further. Through rigorous experiments, we demonstrated that SparseCBMs match the
    performance of full LLMs while offering the added benefits of increased interpretability.
    Our work underscores the potential of sparsity in LLMs, paving the way for further
    exploration of this intersection. We envisage future investigations to refine
    the use of structured sparsity, such as group or block sparsity, to further enhance
    model transparency and efficiency.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们引入了稀疏概念瓶颈模型（SparseCBMs），这是一种将概念瓶颈模型（CBMs）的可解释性与非结构化剪枝的效率相结合的创新方法。通过利用二阶剪枝的特性，我们在大型语言模型（LLM）骨干网络中构建了概念特定的稀疏子网络，从而在保留模型性能的同时提供多维度的可解释性。此外，我们提出了一种基于稀疏性的推理时干预机制，该机制在推理时提高了准确性，无需昂贵的
    LLM 微调。该干预机制有效识别每次预测错误的参数，进一步增强了可解释性。通过严格的实验，我们证明了 SparseCBMs 的性能与完整的 LLM 相匹配，同时提供了增强的可解释性。我们的工作强调了稀疏性在
    LLM 中的潜力，为进一步探索这一交集铺平了道路。我们展望未来的研究以完善结构化稀疏性（如组稀疏性或块稀疏性）的使用，以进一步提高模型的透明性和效率。
- en: Ethical Statement
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This research explores methods to enhance the interpretability and reliability
    of large language models (LLMs) through the proposed Sparse Concept Bottleneck
    Models (SparseCBMs). While the development and application of such technology
    have benefits, including improved model understanding, and more efficient use
    of computational resources, several considerations arise that warrant discussion.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究探索了通过提出的稀疏概念瓶颈模型（SparseCBMs）来增强大型语言模型（LLMs）的可解释性和可靠性的方法。虽然这种技术的发展和应用带来了诸多好处，包括改善模型理解和更高效地使用计算资源，但也出现了几个值得讨论的考虑因素。
- en: 'Transparency and Explainability: Though our work aims to make models more interpretable,
    the actual understanding of these models can still be quite complex and may be
    beyond the reach of the general public. Furthermore, the opacity of these models
    can potentially be exploited, reinforcing the need for ongoing work in model transparency.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 透明性与可解释性：尽管我们的工作旨在使模型更具可解释性，但对这些模型的实际理解仍然可能非常复杂，超出了一般公众的理解范围。此外，这些模型的不透明性可能被利用，从而强化了在模型透明性方面持续工作的必要性。
- en: 'Robustness: As indicated in (Tan et al. [2023](#bib.bib39); Wang et al. [2023b](#bib.bib44)),
    the proposed framework is sensitive to the noisy concept and target task labels,
    requesting future work in model robustness. Potential direction include selective
    learning (Li et al. [2023b](#bib.bib20), [c](#bib.bib21)), knowledge editting (Wang
    et al. [2023d](#bib.bib46)), to name a few.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性：正如 (Tan et al. [2023](#bib.bib39); Wang et al. [2023b](#bib.bib44)) 中所指出的，所提出的框架对噪声概念和目标任务标签敏感，要求未来的工作集中在模型鲁棒性上。潜在的方向包括选择性学习
    (Li et al. [2023b](#bib.bib20), [c](#bib.bib21))，知识编辑 (Wang et al. [2023d](#bib.bib46))
    等。
- en: 'Efficiency: It is worthnoteing that, even though the inference-time intervention
    is highly efficient, SparseCBM require more training time due to the cocnept-specific
    pruning. Potential way to enhance the training efficiency is to share part of
    the sparsity among concepts, as studied in (Wang et al. [2020](#bib.bib47); Chen
    et al. [2021](#bib.bib3)).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 效率：值得注意的是，即使推理时间干预效率很高，SparseCBM由于概念特定的剪枝而需要更多的训练时间。提高训练效率的潜在方法是共享部分稀疏性，如(Wang等
    [2020](#bib.bib47); Chen等 [2021](#bib.bib3))所研究的。
- en: 'Label Reliance: SparseCBMs, along with other CBM variants, necessitate the
    annotation of concepts. To reduce this burden, several approaches are promising.
    These include leveraging other LLMs for automated annotation, as discussed in (Tan
    et al. [2023](#bib.bib39); Wang et al. [2023c](#bib.bib45)), employing data-efficient
    learning techniques (Tan et al. [2022](#bib.bib40)), and exploring the acquisition
    of implicit concepts through dictionary learning methods (Wang et al. [2022](#bib.bib43)).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 标签依赖：SparseCBMs及其他CBM变体需要对概念进行注释。为了减少这一负担，有几种方法是有前景的。这些方法包括利用其他LLMs进行自动注释，如(Tan等
    [2023](#bib.bib39); Wang等 [2023c](#bib.bib45))所讨论的，采用数据高效学习技术（Tan等 [2022](#bib.bib40)），以及通过字典学习方法探索隐性概念的获取（Wang等
    [2022](#bib.bib43)）。
- en: 'Misuse: Advanced AI models like LLMs can be repurposed for harmful uses, including
    disinformation campaigns or privacy infringement (Jiang et al. [2023](#bib.bib12);
    Chen and Shu [2023](#bib.bib2)). It’s crucial to implement strong ethical guidelines
    and security measures to prevent misuse.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 滥用：像LLMs这样的高级AI模型可能被重新用于有害的用途，包括虚假信息传播或隐私侵犯（Jiang等 [2023](#bib.bib12); Chen和Shu
    [2023](#bib.bib2)）。至关重要的是实施强有力的伦理指南和安全措施，以防止滥用。
- en: 'Automation and Employment: The advancements in AI and machine learning could
    lead to increased automation and potential job displacement. We must consider
    the societal implications of this technology and work towards strategies to manage
    potential employment shifts.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化与就业：AI和机器学习的进步可能导致自动化增加和潜在的工作岗位流失。我们必须考虑这项技术的社会影响，并制定管理潜在就业变动的策略。
- en: 'Data Bias: If the training data contains biases, LLMs may amplify these biases
    and result in unfair outcomes. We need to continue to develop methods to mitigate
    these biases in AI systems and promote fair and equitable AI use.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 数据偏见：如果训练数据中包含偏见，LLMs可能会放大这些偏见，导致不公平的结果。我们需要继续开发方法来减轻AI系统中的这些偏见，并促进公平和公正的AI使用。
- en: In conducting this research, we adhered to OpenAI’s use case policy and are
    committed to furthering responsible and ethical AI development. As AI technology
    advances, continuous dialogue on these topics will be needed to manage the potential
    impacts and ensure the technology is used for the betterment of all.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行这项研究时，我们遵循了OpenAI的使用案例政策，并致力于推进负责任和伦理的AI发展。随着AI技术的发展，我们需要持续讨论这些主题，以管理潜在影响，并确保技术用于改善全人类的福祉。
- en: Acknowledgments
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by the National Science Foundation (NSF) under grants
    IIS-2229461.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了国家科学基金会（NSF）在拨款IIS-2229461下的支持。
- en: References
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abraham et al. (2022) Abraham, E. D.; D’Oosterlinck, K.; Feder, A.; Gat, Y.;
    Geiger, A.; Potts, C.; Reichart, R.; and Wu, Z. 2022. CEBaB: Estimating the causal
    effects of real-world concepts on NLP model behavior. *NeurIPS*, 35: 17582–17596.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abraham等（2022）Abraham, E. D.; D’Oosterlinck, K.; Feder, A.; Gat, Y.; Geiger,
    A.; Potts, C.; Reichart, R.; 和 Wu, Z. 2022. CEBaB: 估计真实世界概念对NLP模型行为的因果影响。*NeurIPS*，35:
    17582–17596。'
- en: 'Chen and Shu (2023) Chen, C.; and Shu, K. 2023. Combating misinformation in
    the age of llms: Opportunities and challenges. *arXiv:2311.05656*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen和Shu（2023）Chen, C.; 和 Shu, K. 2023. 在大型语言模型时代对抗虚假信息：机遇与挑战。*arXiv:2311.05656*。
- en: 'Chen et al. (2021) Chen, T.; Zhang, Z.; Liu, S.; Chang, S.; and Wang, Z. 2021.
    Long live the lottery: The existence of winning tickets in lifelong learning.
    In *ICLR*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2021）Chen, T.; Zhang, Z.; Liu, S.; Chang, S.; 和 Wang, Z. 2021. 彩票长存：终身学习中的中奖票据存在。发表于*ICLR*。
- en: 'Devlin et al. (2018) Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
    Bert: Pre-training of deep bidirectional transformers for language understanding.
    *arXiv:1810.04805*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin等（2018）Devlin, J.; Chang, M.-W.; Lee, K.; 和 Toutanova, K. 2018. Bert:
    深度双向变换器的预训练用于语言理解。*arXiv:1810.04805*。'
- en: 'Evci et al. (2020) Evci, U.; Gale, T.; Menick, J.; Castro, P. S.; and Elsen,
    E. 2020. Rigging the lottery: Making all tickets winners. In *ICML*, 2943–2952.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evci等（2020）Evci, U.; Gale, T.; Menick, J.; Castro, P. S.; 和 Elsen, E. 2020.
    操控彩票：让所有票据成为赢家。发表于*ICML*，2943–2952。
- en: 'Galassi, Lippi, and Torroni (2020) Galassi, A.; Lippi, M.; and Torroni, P.
    2020. Attention in natural language processing. *IEEE transactions on neural networks
    and learning systems*, 32(10): 4291–4308.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Galassi, Lippi, 和 Torroni (2020) Galassi, A.; Lippi, M.; 和 Torroni, P. 2020.
    自然语言处理中的注意力。*IEEE 神经网络与学习系统交易*, 32(10): 4291–4308。'
- en: Gale, Elsen, and Hooker (2019) Gale, T.; Elsen, E.; and Hooker, S. 2019. The
    state of sparsity in deep neural networks. *arXiv:1902.09574*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale, Elsen, 和 Hooker (2019) Gale, T.; Elsen, E.; 和 Hooker, S. 2019. 深度神经网络中的稀疏性现状。*arXiv:1902.09574*。
- en: 'Han, Mao, and Dally (2016) Han, S.; Mao, H.; and Dally, W. J. 2016. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. In *ICLR*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han, Mao, 和 Dally (2016) Han, S.; Mao, H.; 和 Dally, W. J. 2016. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。见
    *ICLR*。
- en: Han et al. (2015) Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning
    both Weights and Connections for Efficient Neural Network. In *NeurIPS*, 1135–1143.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2015) Han, S.; Pool, J.; Tran, J.; 和 Dally, W. 2015. 学习权重和连接以实现高效的神经网络。见
    *NeurIPS*, 1135–1143。
- en: 'Hassibi and Stork (1992) Hassibi, B.; and Stork, D. 1992. Second order derivatives
    for network pruning: Optimal brain surgeon. *NeurIPS*, 5.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi 和 Stork (1992) Hassibi, B.; 和 Stork, D. 1992. 网络剪枝的二阶导数：最佳脑外科医生。*NeurIPS*,
    5。
- en: He, Zhang, and Sun (2017) He, Y.; Zhang, X.; and Sun, J. 2017. Channel pruning
    for accelerating very deep neural networks. In *Proceedings of ICCV*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, Zhang, 和 Sun (2017) He, Y.; Zhang, X.; 和 Sun, J. 2017. 为加速非常深的神经网络进行通道剪枝。见
    *ICCV 会议论文集*。
- en: 'Jiang et al. (2023) Jiang, B.; Tan, Z.; Nirmal, A.; and Liu, H. 2023. Disinformation
    Detection: An Evolving Challenge in the Age of LLMs. *arXiv:2309.15847*.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023) Jiang, B.; Tan, Z.; Nirmal, A.; 和 Liu, H. 2023. 错误信息检测：在 LLM
    时代不断演变的挑战。*arXiv:2309.15847*。
- en: Koh et al. (2020) Koh, P. W.; Nguyen, T.; Tang, Y. S.; Mussmann, S.; Pierson,
    E.; Kim, B.; and Liang, P. 2020. Concept bottleneck models. In *ICML*, 5338–5348.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh 等 (2020) Koh, P. W.; Nguyen, T.; Tang, Y. S.; Mussmann, S.; Pierson, E.;
    Kim, B.; 和 Liang, P. 2020. 概念瓶颈模型。见 *ICML*, 5338–5348。
- en: 'Kurtic et al. (2022) Kurtic, E.; Campos, D.; Nguyen, T.; Frantar, E.; Kurtz,
    M.; Fineran, B.; Goin, M.; and Alistarh, D. 2022. The Optimal BERT Surgeon: Scalable
    and Accurate Second-Order Pruning for Large Language Models. In *Proceedings of
    the 2022 Conference on Empirical Methods in Natural Language Processing*, 4163–4181.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic 等 (2022) Kurtic, E.; Campos, D.; Nguyen, T.; Frantar, E.; Kurtz, M.;
    Fineran, B.; Goin, M.; 和 Alistarh, D. 2022. 最佳 BERT 外科医生：大规模语言模型的可扩展和准确的二阶剪枝。见
    *2022 年自然语言处理方法实证会议论文集*, 4163–4181。
- en: Lagunas et al. (2021) Lagunas, F.; Charlaix, E.; Sanh, V.; and Rush, A. M. 2021.
    Block pruning for faster transformers. *arXiv:2109.04838*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagunas 等 (2021) Lagunas, F.; Charlaix, E.; Sanh, V.; 和 Rush, A. M. 2021. 更快的变换器的块剪枝。*arXiv:2109.04838*。
- en: LeCun, Denker, and Solla (1990a) LeCun, Y.; Denker, J. S.; and Solla, S. A.
    1990a. Optimal brain damage. In *NeurIPS*, 598–605.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun, Denker, 和 Solla (1990a) LeCun, Y.; Denker, J. S.; 和 Solla, S. A. 1990a.
    最佳脑损伤。见 *NeurIPS*, 598–605。
- en: LeCun, Denker, and Solla (1990b) LeCun, Y.; Denker, J. S.; and Solla, S. A.
    1990b. Optimal Brain Damage. In Touretzky, D. S., ed., *NeurIPS*, 598–605\. Morgan-Kaufmann.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun, Denker, 和 Solla (1990b) LeCun, Y.; Denker, J. S.; 和 Solla, S. A. 1990b.
    最佳脑损伤。见 Touretzky, D. S., ed., *NeurIPS*, 598–605。Morgan-Kaufmann。
- en: 'Li et al. (2023a) Li, K.; Patel, O.; Viégas, F.; Pfister, H.; and Wattenberg,
    M. 2023a. Inference-Time Intervention: Eliciting Truthful Answers from a Language
    Model. *arXiv:2306.03341*.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023a) Li, K.; Patel, O.; Viégas, F.; Pfister, H.; 和 Wattenberg, M. 2023a.
    推理时干预：从语言模型中获取真实答案。*arXiv:2306.03341*。
- en: Li et al. (2022) Li, S.; Chen, J.; Shen, Y.; Chen, Z.; Zhang, X.; Li, Z.; Wang,
    H.; Qian, J.; Peng, B.; Mao, Y.; et al. 2022. Explanations from large language
    models make small reasoners better. *arXiv:2210.06726*.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2022) Li, S.; Chen, J.; Shen, Y.; Chen, Z.; Zhang, X.; Li, Z.; Wang, H.;
    Qian, J.; Peng, B.; Mao, Y.; 等 2022. 从大语言模型获得的解释使小型推理者更优秀。*arXiv:2210.06726*。
- en: 'Li et al. (2023b) Li, Y.; Han, H.; Shan, S.; and Chen, X. 2023b. DISC: Learning
    from Noisy Labels via Dynamic Instance-Specific Selection and Correction. In *Proceedings
    of the IEEE/CVF Conference on CVPR*, 24070–24079.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023b) Li, Y.; Han, H.; Shan, S.; 和 Chen, X. 2023b. DISC：通过动态实例特定选择和修正从嘈杂标签中学习。见
    *IEEE/CVF CVPR 会议论文集*, 24070–24079。
- en: 'Li et al. (2023c) Li, Y.; Tan, Z.; Shu, K.; Cao, Z.; Kong, Y.; and Liu, H.
    2023c. CSGNN: Conquering Noisy Node labels via Dynamic Class-wise Selection. *arXiv:2311.11473*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023c) Li, Y.; Tan, Z.; Shu, K.; Cao, Z.; Kong, Y.; 和 Liu, H. 2023c. CSGNN：通过动态类别选择征服嘈杂节点标签。*arXiv:2311.11473*。
- en: 'Liu et al. (2022) Liu, J.; Lin, Y.; Jiang, L.; Liu, J.; Wen, Z.; and Peng,
    X. 2022. Improve Interpretability of Neural Networks via Sparse Contrastive Coding.
    In *Findings of the Association for Computational Linguistics: EMNLP 2022*, 460–470.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2022) Liu, J.; Lin, Y.; Jiang, L.; Liu, J.; Wen, Z.; 和 Peng, X. 2022.
    通过稀疏对比编码提高神经网络的可解释性。在 *Findings of the Association for Computational Linguistics:
    EMNLP 2022*, 460–470。'
- en: 'Liu et al. (2023) Liu, S.; Chen, T.; Zhang, Z.; Chen, X.; Huang, T.; Jaiswal,
    A.; and Wang, Z. 2023. Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks
    Together! *arXiv:2303.02141*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023) Liu, S.; Chen, T.; Zhang, Z.; Chen, X.; Huang, T.; Jaiswal, A.;
    和 Wang, Z. 2023. 稀疏性可能会哭泣：让我们一起失败（当前）稀疏神经网络！*arXiv:2303.02141*。
- en: 'Liu et al. (2019) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
    Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly
    optimized bert pretraining approach. *arXiv:1907.11692*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,
    O.; Lewis, M.; Zettlemoyer, L.; 和 Stoyanov, V. 2019. Roberta：一种强健优化的 BERT 预训练方法。*arXiv:1907.11692*。
- en: Liu et al. (2017) Liu, Z.; Li, J.; Shen, Z.; Huang, G.; Yan, S.; and Zhang,
    C. 2017. Learning efficient convolutional networks through network slimming. In
    *Proceedings of ICCV*, 2736–2744.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2017) Liu, Z.; Li, J.; Shen, Z.; Huang, G.; Yan, S.; 和 Zhang, C. 2017.
    通过网络缩减学习高效的卷积网络。在 *Proceedings of ICCV*, 2736–2744。
- en: 'Losch, Fritz, and Schiele (2019) Losch, M.; Fritz, M.; and Schiele, B. 2019.
    Interpretability beyond classification output: Semantic bottleneck networks. *arXiv:1907.10882*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Losch, Fritz 和 Schiele (2019) Losch, M.; Fritz, M.; 和 Schiele, B. 2019. 超越分类输出的可解释性：语义瓶颈网络。*arXiv:1907.10882*。
- en: Lundberg and Lee (2017) Lundberg, S. M.; and Lee, S.-I. 2017. A unified approach
    to interpreting model predictions. *NeurIPS*, 30.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg 和 Lee (2017) Lundberg, S. M.; 和 Lee, S.-I. 2017. 一种统一的模型预测解释方法。*NeurIPS*,
    30。
- en: Meister et al. (2021) Meister, C.; Lazov, S.; Augenstein, I.; and Cotterell,
    R. 2021. Is Sparse Attention more Interpretable? In *ACL-IJCNLP*, 122–129.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meister 等 (2021) Meister, C.; Lazov, S.; Augenstein, I.; 和 Cotterell, R. 2021.
    稀疏注意力是否更具可解释性？在 *ACL-IJCNLP*, 122–129。
- en: Michel, Levy, and Neubig (2019) Michel, P.; Levy, O.; and Neubig, G. 2019. Are
    sixteen heads really better than one? *NeurIPS*, 32.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michel, Levy 和 Neubig (2019) Michel, P.; Levy, O.; 和 Neubig, G. 2019. 十六个头真的比一个头更好吗？*NeurIPS*,
    32。
- en: Mishra, Sturm, and Dixon (2017) Mishra, S.; Sturm, B. L.; and Dixon, S. 2017.
    Local interpretable model-agnostic explanations for music content analysis. In
    *ISMIR*, volume 53, 537–543.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra, Sturm 和 Dixon (2017) Mishra, S.; Sturm, B. L.; 和 Dixon, S. 2017. 音乐内容分析的局部可解释模型无关解释。在
    *ISMIR*, volume 53, 537–543。
- en: Oikarinen et al. (2023) Oikarinen, T.; Das, S.; Nguyen, L. M.; and Weng, T.-W.
    2023. Label-free Concept Bottleneck Models. In *ICLR*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oikarinen 等 (2023) Oikarinen, T.; Das, S.; Nguyen, L. M.; 和 Weng, T.-W. 2023.
    无标签概念瓶颈模型。在 *ICLR*。
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。*arXiv:2303.08774*。
- en: Paszke et al. (2017) Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang,
    E.; DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Automatic
    differentiation in pytorch. In *NeurIPS*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等 (2017) Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito,
    Z.; Lin, Z.; Desmaison, A.; Antiga, L.; 和 Lerer, A. 2017. PyTorch 中的自动微分。在 *NeurIPS*。
- en: Ribeiro, Singh, and Guestrin (2016) Ribeiro, M. T.; Singh, S.; and Guestrin,
    C. 2016. ” Why should i trust you?” Explaining the predictions of any classifier.
    In *Proceedings of the 22nd ACM SIGKDD Conference*, 1135–1144.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro, Singh 和 Guestrin (2016) Ribeiro, M. T.; Singh, S.; 和 Guestrin, C. 2016.
    “我为什么应该相信你？” 解释任何分类器的预测。在 *Proceedings of the 22nd ACM SIGKDD Conference*, 1135–1144。
- en: 'Ross, Marasović, and Peters (2021) Ross, A.; Marasović, A.; and Peters, M. E.
    2021. Explaining NLP Models via Minimal Contrastive Editing (MiCE). In *Findings
    of the Association for Computational Linguistics: ACL-IJCNLP 2021*, 3840–3852.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ross, Marasović 和 Peters (2021) Ross, A.; Marasović, A.; 和 Peters, M. E. 2021.
    通过最小对比编辑 (MiCE) 解释 NLP 模型。在 *Findings of the Association for Computational Linguistics:
    ACL-IJCNLP 2021*, 3840–3852。'
- en: 'Sanh et al. (2019) Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019. DistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter. *arXiv:1910.01108*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等 (2019) Sanh, V.; Debut, L.; Chaumond, J.; 和 Wolf, T. 2019. DistilBERT,
    BERT 的精简版本：更小、更快、更便宜、更轻量。*arXiv:1910.01108*。
- en: 'Subramanian et al. (2018) Subramanian, A.; Pruthi, D.; Jhamtani, H.; Berg-Kirkpatrick,
    T.; and Hovy, E. 2018. Spine: Sparse interpretable neural embeddings. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 32.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Subramanian 等 (2018) Subramanian, A.; Pruthi, D.; Jhamtani, H.; Berg-Kirkpatrick,
    T.; 和 Hovy, E. 2018. Spine: 稀疏可解释的神经嵌入。在 *Proceedings of the AAAI conference on
    artificial intelligence*, volume 32。'
- en: Sun et al. (2023) Sun, M.; Liu, Z.; Bair, A.; and Kolter, Z. 2023. A Simple
    and Effective Pruning Approach for Large Language Models. *arXiv:2306.11695*.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) Sun, M.; Liu, Z.; Bair, A.; 和 Kolter, Z. 2023. 一种简单有效的大型语言模型剪枝方法。*`arXiv:2306.11695`*。
- en: Tan et al. (2023) Tan, Z.; Cheng, L.; Wang, S.; Bo, Y.; Li, J.; and Liu, H.
    2023. Interpreting Pretrained Language Models via Concept Bottlenecks. *arXiv:2311.05014*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. (2023) Tan, Z.; Cheng, L.; Wang, S.; Bo, Y.; Li, J.; 和 Liu, H. 2023.
    通过概念瓶颈解释预训练语言模型。*`arXiv:2311.05014`*。
- en: Tan et al. (2022) Tan, Z.; Ding, K.; Guo, R.; and Liu, H. 2022. Graph few-shot
    class-incremental learning. In *Proceedings of the Fifteenth ACM International
    Conference on WSDM*, 987–996.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. (2022) Tan, Z.; Ding, K.; Guo, R.; 和 Liu, H. 2022. 图少样本类别增量学习。在 *Proceedings
    of the Fifteenth ACM International Conference on WSDM*，987–996。
- en: 'Vig (2019) Vig, J. 2019. A Multiscale Visualization of Attention in the Transformer
    Model. In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*, 37–42.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vig (2019) Vig, J. 2019. 变换器模型的多尺度注意力可视化。在 *Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics: System Demonstrations*，37–42。'
- en: 'Wang et al. (2023a) Wang, H.; Hong, Z.; Zhang, D.; and Wang, H. 2023a. Intepreting
    & Improving Pretrained Language Models: A Probabilistic Conceptual Approach. Openreview:id=kwF1ZfHf0W.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Wang, H.; Hong, Z.; Zhang, D.; 和 Wang, H. 2023a. 解释与改进预训练语言模型：一种概率概念方法。`Openreview:id=kwF1ZfHf0W`。
- en: Wang et al. (2022) Wang, P.; Fan, Z.; Chen, T.; and Wang, Z. 2022. Neural implicit
    dictionary learning via mixture-of-expert training. In *ICML*, 22613–22624.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) Wang, P.; Fan, Z.; Chen, T.; 和 Wang, Z. 2022. 通过专家混合训练的神经隐式字典学习。在
    *ICML*，22613–22624。
- en: Wang et al. (2023b) Wang, S.; Tan, Z.; Guo, R.; and Li, J. 2023b. Noise-Robust
    Fine-Tuning of Pretrained Language Models via External Guidance. *arXiv:2311.01108*.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Wang, S.; Tan, Z.; Guo, R.; 和 Li, J. 2023b. 噪声鲁棒的预训练语言模型微调通过外部指导。*`arXiv:2311.01108`*。
- en: Wang et al. (2023c) Wang, S.; Tan, Z.; Liu, H.; and Li, J. 2023c. Contrastive
    Meta-Learning for Few-shot Node Classification. In *Proceedings of the 29th ACM
    SIGKDD Conference*, 2386–2397.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023c) Wang, S.; Tan, Z.; Liu, H.; 和 Li, J. 2023c. 对比元学习用于少样本节点分类。在
    *Proceedings of the 29th ACM SIGKDD Conference*，2386–2397。
- en: 'Wang et al. (2023d) Wang, S.; Zhu, Y.; Liu, H.; Zheng, Z.; Chen, C.; et al.
    2023d. Knowledge Editing for Large Language Models: A Survey. *arXiv:2310.16218*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023d) Wang, S.; Zhu, Y.; Liu, H.; Zheng, Z.; Chen, C.; 等. 2023d.
    大型语言模型的知识编辑：综述。*`arXiv:2310.16218`*。
- en: Wang et al. (2020) Wang, Z.; Jian, T.; Chowdhury, K.; Wang, Y.; Dy, J.; and
    Ioannidis, S. 2020. Learn-prune-share for lifelong learning. In *2020 ICDM*, 641–650\.
    IEEE.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Wang, Z.; Jian, T.; Chowdhury, K.; Wang, Y.; Dy, J.; 和 Ioannidis,
    S. 2020. 终身学习的学会-剪枝-分享。在 *2020 ICDM*，641–650。IEEE。
- en: 'Wolf et al. (2020) Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;
    Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer,
    S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Scao, T. L.; Gugger,
    S.; Drame, M.; Lhoest, Q.; and Rush, A. M. 2020. HuggingFace’s Transformers: State-of-the-art
    Natural Language Processing. arXiv:1910.03771.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf et al. (2020) Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;
    Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer,
    S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Scao, T. L.; Gugger,
    S.; Drame, M.; Lhoest, Q.; 和 Rush, A. M. 2020. HuggingFace的Transformers: 先进的自然语言处理。`arXiv:1910.03771`。'
- en: 'Wu et al. (2021) Wu, T.; Ribeiro, M. T.; Heer, J.; and Weld, D. 2021. Polyjuice:
    Generating Counterfactuals for Explaining, Evaluating, and Improving Models. In
    *ACL-IJCNLP*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2021) Wu, T.; Ribeiro, M. T.; Heer, J.; 和 Weld, D. 2021. Polyjuice:
    生成反事实以解释、评估和改进模型。在 *ACL-IJCNLP*。'
- en: 'Zhang et al. (2022) Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
    Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; et al. 2022. Opt: Open pre-trained
    transformer language models. *arXiv:2205.01068*.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
    Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; 等. 2022. Opt: 开放预训练变换器语言模型。*`arXiv:2205.01068`*。'
- en: 'Zhou, Alvarez, and Porikli (2016) Zhou, H.; Alvarez, J. M.; and Porikli, F.
    2016. Less is more: Towards compact cnns. In *ECCV*, 662–677\. Springer.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou, Alvarez, and Porikli (2016) Zhou, H.; Alvarez, J. M.; 和 Porikli, F. 2016.
    少即是多：走向紧凑的卷积神经网络。在 *ECCV*，662–677。Springer。
- en: Zhou et al. (2022) Zhou, Y.; Muresanu, A. I.; Han, Z.; Paster, K.; Pitis, S.;
    Chan, H.; and Ba, J. 2022. Large Language Models are Human-Level Prompt Engineers.
    In *ICLR*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2022) Zhou, Y.; Muresanu, A. I.; Han, Z.; Paster, K.; Pitis, S.;
    Chan, H.; 和 Ba, J. 2022. 大型语言模型是人类级的提示工程师。在 *ICLR*。
- en: A. Definitions of Training Strategies
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A. Training Strategies的定义
- en: 'Given a text input $x\in\mathbb{R}^{D}$, concepts $c\in\mathbb{R}^{K}$ and
    its label $y$, the strategies for fine-tuning the text encoder $f_{\theta}$, the
    projector $p_{\psi}$ and the label predictor $g_{\phi}$ are defined as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 给定文本输入 $x\in\mathbb{R}^{D}$、概念 $c\in\mathbb{R}^{K}$ 和其标签 $y$，对文本编码器 $f_{\theta}$、投影器
    $p_{\psi}$ 和标签预测器 $g_{\phi}$ 进行微调的策略定义如下：
- en: 'i) Vanilla fine-tuning an LLM: The concept labels are ignored, and then the
    text encoder $f_{\theta}$ and the label predictor $g_{\phi}$ are fine-tuned either
    as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: i) 经典微调 LLM：忽略概念标签，然后对文本编码器 $f_{\theta}$ 和标签预测器 $g_{\phi}$ 进行微调，方法如下：
- en: '|  | $\theta,\phi=\operatorname*{arg\,min}_{\theta,\phi}\mathcal{L}_{CE}(g_{\phi}(f_{\theta}(x),y),$
    |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta,\phi=\operatorname*{arg\,min}_{\theta,\phi}\mathcal{L}_{CE}(g_{\phi}(f_{\theta}(x),y),$
    |  |'
- en: 'or as follows (frozen text encoder $f_{\theta}$):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 或如下（冻结文本编码器 $f_{\theta}$）：
- en: '|  | $\phi=\operatorname*{arg\,min}_{\phi}\mathcal{L}_{CE}(g_{\phi}(f_{\theta}(x),y),$
    |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi=\operatorname*{arg\,min}_{\phi}\mathcal{L}_{CE}(g_{\phi}(f_{\theta}(x),y),$
    |  |'
- en: where $\mathcal{L}_{CE}$ indicates the cross-entropy loss. In this work we only
    consider the former option for its significant better performance.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{CE}$ 表示交叉熵损失。在这项工作中，我们仅考虑前者选项，因为其性能显著更好。
- en: 'ii) Independently training LLM with the concept and task labels: The text encoder
    $f_{\theta}$, the projector $p_{\psi}$ and the label predictor $g_{\phi}$ are
    trained seperately with ground truth concepts labels and task labels as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ii) 独立训练 LLM 以包含概念和任务标签：文本编码器 $f_{\theta}$、投影器 $p_{\psi}$ 和标签预测器 $g_{\phi}$
    以如下方式分别用真实概念标签和任务标签进行训练：
- en: '|  | $\displaystyle\theta,\psi$ | $\displaystyle=\operatorname*{arg\,min}_{\theta,\psi}\mathcal{L}_{CE}(p_{\psi}(f_{\theta}(x)),c),$
    |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta,\psi$ | $\displaystyle=\operatorname*{arg\,min}_{\theta,\psi}\mathcal{L}_{CE}(p_{\psi}(f_{\theta}(x)),c),$
    |  |'
- en: '|  | $\displaystyle\phi$ | $\displaystyle=\operatorname*{arg\,min}_{\phi}\mathcal{L}_{CE}(g_{\phi}(c),y).$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi$ | $\displaystyle=\operatorname*{arg\,min}_{\phi}\mathcal{L}_{CE}(g_{\phi}(c),y).$
    |  |'
- en: During inference, the label predictor will use the output from the projector
    rather than the ground-truth concepts.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，标签预测器将使用来自投影器的输出，而不是真实概念。
- en: 'iii) Sequentilally training LLM with the concept and task labels: We first
    learn the concept encoder as the independent training strategy above, and then
    use its output to train the label predictor:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: iii) 按顺序训练 LLM 以包含概念和任务标签：我们首先按照上述独立训练策略学习概念编码器，然后利用其输出训练标签预测器：
- en: '|  | $\displaystyle\phi=\operatorname*{arg\,min}_{\phi}\mathcal{L}_{CE}(g_{\phi}(p_{\psi}(f_{\theta}(x),y).$
    |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi=\operatorname*{arg\,min}_{\phi}\mathcal{L}_{CE}(g_{\phi}(p_{\psi}(f_{\theta}(x),y).$
    |  |'
- en: 'iv) Jointly training LLM with the concept and task labels: Learn the concept
    encoder and label predictor via a weighted sum $\mathcal{L}_{joint}$ of the two
    objectives described above:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: iv) 共同训练 LLM 以包含概念和任务标签：通过上述两个目标的加权和 $\mathcal{L}_{joint}$ 来学习概念编码器和标签预测器：
- en: '|  | $\displaystyle\theta,\psi,\phi$ | $\displaystyle=\operatorname*{arg\,min}_{\theta,\psi,\phi}\mathcal{L}_{joint}(x,c,y)$
    |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta,\psi,\phi$ | $\displaystyle=\operatorname*{arg\,min}_{\theta,\psi,\phi}\mathcal{L}_{joint}(x,c,y)$
    |  |'
- en: '|  |  | $\displaystyle=\operatorname*{arg\,min}_{\theta,\psi,\phi}[\mathcal{L}_{CE}(g_{\phi}(p_{\psi}(f_{\theta}(x),y)$
    |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\operatorname*{arg\,min}_{\theta,\psi,\phi}[\mathcal{L}_{CE}(g_{\phi}(p_{\psi}(f_{\theta}(x),y)$
    |  |'
- en: '|  |  | $\displaystyle+\gamma\mathcal{L}_{CE}(p_{\psi}(f_{\theta}(x)),c)].$
    |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\gamma\mathcal{L}_{CE}(p_{\psi}(f_{\theta}(x)),c)].$
    |  |'
- en: It’s worth noting that the LLM-CBMs trained jointly are sensitive to the loss
    weight $\gamma$. We tune the value for $\gamma$ for better performance (Tan et al.
    [2023](#bib.bib39)).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，联合训练的 LLM-CBM 对损失权重 $\gamma$ 非常敏感。我们调整 $\gamma$ 的值以获得更好的性能（Tan 等 [2023](#bib.bib39)）。
- en: B. Implementation Detail
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B. 实施细节
- en: 'In this section, we provide more details on the implementation settings of
    our experiments. Specifically, we implement our framework with PyTorch (Paszke
    et al. [2017](#bib.bib33)) and HuggingFace (Wolf et al. [2020](#bib.bib48)) and
    train our framework on a single 80GB Nvidia A100 GPU. We follow a prior work (Abraham
    et al. [2022](#bib.bib1)) for backbone implementation. All backbone models have
    a maximum token number of 512 and a batch size of 8 (for larger LLMs such as OPT-1.3B,
    we reduce the batch size to 1). We use the Adam optimizer to update the backbone,
    projector, and label predictor according to Section [Problem Setup.](#Sx3.SSx1.SSSx1
    "Problem Setup. ‣ Preliminary: Concept Bottleneck Models for Language Models ‣
    Methodology ‣ Sparsity-Guided Holistic Explanation for LLMs with Interpretable
    Inference-Time Intervention"). The values of other hyperparameters (Table [3](#Sx9.T3
    "Table 3 ‣ B. Implementation Detail ‣ Sparsity-Guided Holistic Explanation for
    LLMs with Interpretable Inference-Time Intervention") in the next page) for each
    specific PLM type are determined through grid search. We run all the experiments
    on an Nvidia A100 GPU with 80GB RAM.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了实验实施设置的更多细节。具体来说，我们使用 PyTorch（Paszke 等 [2017](#bib.bib33)）和 HuggingFace（Wolf
    等 [2020](#bib.bib48)）实现了我们的框架，并在一台 80GB 的 Nvidia A100 GPU 上训练了我们的框架。我们遵循先前的工作（Abraham
    等 [2022](#bib.bib1)）进行骨干模型的实现。所有骨干模型的最大 token 数为 512，批量大小为 8（对于像 OPT-1.3B 这样较大的
    LLM，我们将批量大小减少到 1）。我们使用 Adam 优化器根据第 [问题设置](#Sx3.SSx1.SSSx1 "问题设置 ‣ 初步：语言模型的概念瓶颈模型
    ‣ 方法论 ‣ 针对 LLM 的稀疏性引导的整体解释") 更新骨干模型、投影器和标签预测器。每种特定 PLM 类型的其他超参数值（见第 [3](#Sx9.T3
    "表 3 ‣ B. 实施细节 ‣ 针对 LLM 的稀疏性引导的整体解释") 页）通过网格搜索确定。我们在一台配备 80GB RAM 的 Nvidia A100
    GPU 上运行所有实验。
- en: 'Table 3: Key parameters in this paper with their annotations and evaluated
    values. Note that bold values indicate the optimal ones.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：本文中的关键参数及其注释和评估值。注意粗体值表示最优值。
- en: '| Notations | Specification | Definitions or Descriptions | Values |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 规格 | 定义或描述 | 值 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| max_len | - | maximum token number of input | 128 / 256 / 512 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| max_len | - | 输入的最大 token 数 | 128 / 256 / 512 |'
- en: '| batch_size | - | batch size | 8 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| batch_size | - | 批量大小 | 8 |'
- en: '| plm_epoch | - | maximum training epochs for LLMs and the Projector | 20 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| plm_epoch | - | LLM 和投影器的最大训练周期 | 20 |'
- en: '| clf_epoch | - | maximum training epochs for the linear classifier | 20 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| clf_epoch | - | 线性分类器的最大训练周期 | 20 |'
- en: '| lr | DistilBERT | learning rate when the backbone is DistilBERT | 1e-3 /
    1e-4 / 1e-5 / 1e-6 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| lr | DistilBERT | 当骨干模型为 DistilBERT 时的学习率 | 1e-3 / 1e-4 / 1e-5 / 1e-6 |'
- en: '| BERT | learning rate when the backbone is BERT | 1e-3 / 1e-4 / 1e-5 / 1e-6
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 当骨干模型为 BERT 时的学习率 | 1e-3 / 1e-4 / 1e-5 / 1e-6 |'
- en: '| RoBERT | learning rate when the backbone is RoBERT | 1e-3 / 1e-4 / 1e-5 /
    1e-6 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| RoBERT | 当骨干模型为 RoBERT 时的学习率 | 1e-3 / 1e-4 / 1e-5 / 1e-6 |'
- en: '| OPT-125M | learning rate when the backbone is OPT-125M | 1e-3 / 1e-4 / 1e-5
    / 1e-6 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| OPT-125M | 当骨干模型为 OPT-125M 时的学习率 | 1e-3 / 1e-4 / 1e-5 / 1e-6 |'
- en: '| OPT-350M | learning rate when the backbone is OPT-350 | 1e-4 / 1e-5 / 1e-6
    / 1e-7 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| OPT-350M | 当骨干模型为 OPT-350 时的学习率 | 1e-4 / 1e-5 / 1e-6 / 1e-7 |'
- en: '| OPT-1.3B | learning rate when the backbone is OPT-1.3B | 1e-4 / 1e-5 / 1e-6
    / 1e-7 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3B | 当骨干模型为 OPT-1.3B 时的学习率 | 1e-4 / 1e-5 / 1e-6 / 1e-7 |'
- en: '| $\gamma$ | - | loss weight in the joint loss $L_{joint}$ | 1 / 3 / 5 / 7
    / 10 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma$ | - | 联合损失 $L_{joint}$ 中的损失权重 | 1 / 3 / 5 / 7 / 10 |'
- en: C. Details to Solve the Optimization Task
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C. 解决优化任务的细节
- en: 'As a common practice, we approximate the Hessian at $\bm{w}$ via a dampened
    empirical Fisher information matrix (Hassibi and Stork [1992](#bib.bib10)):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们通过加权经验 Fisher 信息矩阵在 $\bm{w}$ 处近似 Hessian（Hassibi 和 Stork [1992](#bib.bib10)）：
- en: '|  | $\bm{H}_{\mathcal{L}}(\bm{\theta})\simeq\hat{\bm{F}}(\bm{\theta})=\zeta\bm{I}+\frac{1}{m}\sum_{i=1}^{m}\nabla\mathcal{L}_{i}(\bm{\theta})\nabla\mathcal{L}_{i}^{\top}(\bm{\theta}),$
    |  | (7) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{H}_{\mathcal{L}}(\bm{\theta})\simeq\hat{\bm{F}}(\bm{\theta})=\zeta\bm{I}+\frac{1}{m}\sum_{i=1}^{m}\nabla\mathcal{L}_{i}(\bm{\theta})\nabla\mathcal{L}_{i}^{\top}(\bm{\theta}),$
    |  | (7) |'
- en: where $\zeta> is a small damplening constant, <math   alttext=$ is the
    indentity matrix. $m$ is the number of gradient outer products used to approximate
    the Hessian.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\zeta$ 是一个小的阻尼常数，$ \bm{I} $ 是单位矩阵。$m$ 是用于近似 Hessian 的梯度外积数量。
- en: 'Following Kurtic et al. ([2022](#bib.bib14)), based on Eq. ([6](#Sx3.E6 "In
    Concept-Induced Sparsity Mining. ‣ SparseCBMs ‣ Methodology ‣ Sparsity-Guided
    Holistic Explanation for LLMs with Interpretable Inference-Time Intervention")),
    we express the system of $|\bm{Q}|$ equality constrains in matrix as:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Kurtic等人 ([2022](#bib.bib14))，基于方程 ([6](#Sx3.E6 "在概念诱导稀疏性挖掘中 ‣ SparseCBMs
    ‣ 方法论 ‣ 面向LLM的可解释推理时间干预的稀疏性引导整体解释"))，我们将$|\bm{Q}|$个平等约束系统表示为矩阵：
- en: '|  | $\bm{E}_{\bm{Q}}\Delta\bm{\theta}+\bm{E}_{\bm{Q}}\bm{\theta}^{\ast}=0,$
    |  | (8) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{E}_{\bm{Q}}\Delta\bm{\theta}+\bm{E}_{\bm{Q}}\bm{\theta}^{\ast}=0,$
    |  | (8) |'
- en: 'where $\bm{E}_{\bm{Q}}\in\mathbb{R}^{\bm{Q}\times b}$ is a matrix composed
    of the corresponding canonical basis vectors $\bm{e}_{b}(\forall b\in|\bm{Q}|)$.
    Using Lagrange multipliers, we hope to find stationary points of the Lagrangian
    $L(\Delta\bm{\theta},\bm{\lambda})$, where $\bm{\lambda}\in\mathbb{R}^{|\bm{Q}|}$
    denotes a vector of Lagrange multipliers. Then, we need to solve the following
    system of equations:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{E}_{\bm{Q}}\in\mathbb{R}^{\bm{Q}\times b}$是由相应的标准基向量$\bm{e}_{b}(\forall
    b\in|\bm{Q}|)$组成的矩阵。使用拉格朗日乘子，我们希望找到拉格朗日函数$L(\Delta\bm{\theta},\bm{\lambda})$的驻点，其中$\bm{\lambda}\in\mathbb{R}^{|\bm{Q}|}$表示拉格朗日乘子的向量。然后，我们需要解决以下方程组：
- en: '|  | $\displaystyle\frac{\partial L(\Delta\bm{\theta},\bm{\lambda})}{\partial\Delta\bm{\theta}}=0,\quad\quad\frac{\partial
    L(\Delta\bm{\theta},\bm{\lambda})}{\partial\bm{\lambda}}=0,$ |  | (9) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial L(\Delta\bm{\theta},\bm{\lambda})}{\partial\Delta\bm{\theta}}=0,\quad\quad\frac{\partial
    L(\Delta\bm{\theta},\bm{\lambda})}{\partial\bm{\lambda}}=0,$ |  | (9) |'
- en: 'which gives the following optimal weight update:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下*最优权重更新*：
- en: '|  | $\Delta\bm{\theta}^{\ast}=-\hat{\bm{F}}^{-1}(\bm{\theta}^{\ast})\bm{E}_{\bm{Q}}^{\top}(\bm{E}_{\bm{Q}}\hat{\bm{F}}^{-1}(\bm{\theta}^{\ast})\bm{E}_{\bm{Q}}^{\top})^{-1}\bm{E}_{\bm{Q}}\bm{\theta}^{\ast}.$
    |  | (10) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\bm{\theta}^{\ast}=-\hat{\bm{F}}^{-1}(\bm{\theta}^{\ast})\bm{E}_{\bm{Q}}^{\top}(\bm{E}_{\bm{Q}}\hat{\bm{F}}^{-1}(\bm{\theta}^{\ast})\bm{E}_{\bm{Q}}^{\top})^{-1}\bm{E}_{\bm{Q}}\bm{\theta}^{\ast}.$
    |  | (10) |'
- en: 'It prunes a set of weights $\bm{Q}$ and updates the remaining weights to preserve
    the loss. The corresponding loss increase incurred by the optimal weight update
    $\Delta\bm{\theta}^{\ast}$ can be represented as:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 它修剪一组权重$\bm{Q}$并更新剩余权重以保持损失。由最优权重更新$\Delta\bm{\theta}^{\ast}$引起的对应损失增加可以表示为：
- en: '|  | $\rho_{\bm{Q}}=\frac{1}{2}(\bm{E}_{\bm{Q}}\bm{\theta}^{\ast})^{\top}(\bm{E}_{\bm{Q}}\hat{\bm{F}}^{-1}(\bm{\theta}^{\ast})\bm{E}_{\bm{Q}}^{\top})^{-1}\bm{E}_{\bm{Q}}\bm{\theta}^{\ast}.$
    |  | (11) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | $\rho_{\bm{Q}}=\frac{1}{2}(\bm{E}_{\bm{Q}}\bm{\theta}^{\ast})^{\top}(\bm{E}_{\bm{Q}}\hat{\bm{F}}^{-1}(\bm{\theta}^{\ast})\bm{E}_{\bm{Q}}^{\top})^{-1}\bm{E}_{\bm{Q}}\bm{\theta}^{\ast}.$
    |  | (11) |'
- en: We use this as the importance score to rank groups of weights for pruning.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其用作重要性评分，以对权重组进行修剪排名。
- en: D. Decision Pathways for Real-world Examples
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D. 真实世界示例的决策路径
- en: An example from the CEBaB dataset is given in Figure [6](#Sx11.F6 "Figure 6
    ‣ D. Decision Pathways for Real-world Examples ‣ Sparsity-Guided Holistic Explanation
    for LLMs with Interpretable Inference-Time Intervention") in the next page. An
    example from the IMDB-C dataset is given in Figure [7](#Sx11.F7 "Figure 7 ‣ D.
    Decision Pathways for Real-world Examples ‣ Sparsity-Guided Holistic Explanation
    for LLMs with Interpretable Inference-Time Intervention") in the next page.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: CEBaB数据集的一个示例见下一页的图[6](#Sx11.F6 "图6 ‣ D. 真实世界示例的决策路径 ‣ 面向LLM的可解释推理时间干预的稀疏性引导整体解释")。IMDB-C数据集的一个示例见下一页的图[7](#Sx11.F7
    "图7 ‣ D. 真实世界示例的决策路径 ‣ 面向LLM的可解释推理时间干预的稀疏性引导整体解释")。
- en: '![Refer to caption](img/bfd839be75dd03ea4368f692b8f9ab6e.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bfd839be75dd03ea4368f692b8f9ab6e.png)'
- en: 'Figure 6: The illustration of a decision pathway of an real-world example (CEBaB
    dataset) from the SparseCBM framework with BERT as the backbone. The binary weight
    masks for each concept is represented as a heatmap.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：来自SparseCBM框架的真实世界示例（CEBaB数据集）的决策路径示意图，其中BERT作为骨干网络。每个概念的二进制权重掩码以热图形式表示。
- en: '![Refer to caption](img/8d7630c90378f1e0573c6e7d043ff612.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d7630c90378f1e0573c6e7d043ff612.png)'
- en: 'Figure 7: The illustration of a decision pathway of an real-world example (IMDB-C
    dataset) from the SparseCBM framework with BERT as the backbone. The binary weight
    masks for each concept is represented as a heatmap.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：来自SparseCBM框架的真实世界示例（IMDB-C数据集）的决策路径示意图，其中BERT作为骨干网络。每个概念的二进制权重掩码以热图形式表示。
