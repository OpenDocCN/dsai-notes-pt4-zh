- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 17:34:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 17:34:19'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Transcoders Find Interpretable LLM Feature Circuits
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换器发现可解释的LLM特征电路
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11944](https://ar5iv.labs.arxiv.org/html/2406.11944)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11944](https://ar5iv.labs.arxiv.org/html/2406.11944)
- en: Jacob Dunefsky
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jacob Dunefsky
- en: Yale University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 耶鲁大学
- en: New Haven, CT 06511
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 纽黑文，CT 06511
- en: jacob.dunefsky@yale.edu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: jacob.dunefsky@yale.edu
- en: '&Philippe Chlenski ¹¹footnotemark: 1'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Philippe Chlenski ¹¹脚注标记：1'
- en: Columbia University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: New York, NY 11227
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约，NY 11227
- en: pac@cs.columbia.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: pac@cs.columbia.edu
- en: Neel Nanda Equal contribution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Neel Nanda 平等贡献。
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'A key goal in mechanistic interpretability is circuit analysis: finding sparse
    subgraphs of models corresponding to specific behaviors or capabilities. However,
    MLP sublayers make fine-grained circuit analysis on transformer-based language
    models difficult. In particular, interpretable features—such as those found by
    sparse autoencoders (SAEs)—are typically linear combinations of extremely many
    neurons, each with its own nonlinearity to account for. Circuit analysis in this
    setting thus either yields intractably large circuits or fails to disentangle
    local and global behavior. To address this we explore transcoders, which seek
    to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating
    MLP layer. We successfully train transcoders on language models with 120M, 410M,
    and 1.4B parameters, and find them to perform at least on par with SAEs in terms
    of sparsity, faithfulness, and human-interpretability. We then introduce a novel
    method for using transcoders to perform weights-based circuit analysis through
    MLP sublayers. The resulting circuits neatly factorize into input-dependent and
    input-invariant terms. Finally, we apply transcoders to reverse-engineer unknown
    circuits in the model, and we obtain novel insights regarding the “greater-than
    circuit” in GPT2-small. Our results suggest that transcoders can prove effective
    in decomposing model computations involving MLPs into interpretable circuits.
    Code is available at [https://github.com/jacobdunefsky/transcoder_circuits](https://github.com/jacobdunefsky/transcoder_circuits).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机制解释的一个关键目标是电路分析：寻找与特定行为或能力对应的模型稀疏子图。然而，MLP子层使得对基于变压器的语言模型进行细粒度电路分析变得困难。特别是，可解释特征—例如稀疏自编码器（SAEs）发现的特征—通常是极其多的神经元的线性组合，每个神经元都有其自身的非线性需要考虑。因此，这种设置下的电路分析要么产生难以处理的大电路，要么无法解开局部和全局行为。为了解决这个问题，我们探索了转换器，这些转换器试图用一个更宽的、稀疏激活的MLP层来忠实地近似一个密集激活的MLP层。我们成功地在具有120M、410M和1.4B参数的语言模型上训练了转换器，发现它们在稀疏性、忠实性和人类可解释性方面至少与SAEs表现相当。然后，我们介绍了一种新方法，利用转换器通过MLP子层进行基于权重的电路分析。结果电路巧妙地分解为输入相关和输入不变的部分。最后，我们应用转换器来反向工程模型中的未知电路，并获得了有关GPT2-small中“greater-than电路”的新见解。我们的结果表明，转换器在将涉及MLPs的模型计算分解为可解释电路方面可能非常有效。代码可在[https://github.com/jacobdunefsky/transcoder_circuits](https://github.com/jacobdunefsky/transcoder_circuits)找到。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'In recent years, transformer-based large language models (LLMs) have displayed
    outstanding performance on a wide variety of tasks [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)]. However, the mechanisms by which LLMs perform these tasks are
    opaque by default [[4](#bib.bib4), [5](#bib.bib5)]. The field of mechanistic interpretablity [[6](#bib.bib6)]
    seeks to understand these mechanisms, and doing so relies on decomposing a model
    into circuits [[7](#bib.bib7)]: interpretable subcomputations responsible for
    specific model behaviors [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)].'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于变压器的大型语言模型（LLMs）在各种任务上表现出色 [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]。然而，LLMs执行这些任务的机制默认是模糊不清的 [[4](#bib.bib4),
    [5](#bib.bib5)]。机制解释领域 [[6](#bib.bib6)]旨在理解这些机制，这依赖于将模型分解为电路 [[7](#bib.bib7)]：负责特定模型行为的可解释子计算 [[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]。
- en: 'A core problem in fine-grained circuit analysis is incorporating MLP sublayers [[12](#bib.bib12),
    [10](#bib.bib10)]. Attempting to analyze MLP neurons directly suffers from “polysemanticity” [[13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]: the tendency of neurons
    to activate on many unrelated concepts. To address this, sparse autoencoders (SAEs) [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)] have been used to perform fine-grained circuit
    analysis by instead looking at features—vectors in the model’s representation
    space—instead of individual neurons [[20](#bib.bib20), [21](#bib.bib21)]. However,
    while SAE features are often interpretable, these vectors tend to be dense linear
    combinations of many neurons [[22](#bib.bib22)]. Thus, mechanistically understanding
    how an SAE feature before one or more MLP layers affects a later SAE feature may
    require considering an infeasible number of neurons and their nonlinearities.
    Prior attempts to circumvent this [[20](#bib.bib20), [21](#bib.bib21)] use a mix
    of causal interventions and gradient-based approximations to MLP layers. But these
    approaches fail to exhibit input-invariance: the connections between features
    can only ever be described for a given input, and not for the model as a whole.
    Attempts to address this, e.g. by averaging results over many inputs, conversely
    lose their ability to yield input-dependent information that describes a connection’s
    importance on a single input.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 精细电路分析中的一个核心问题是如何融入 MLP 子层 [[12](#bib.bib12), [10](#bib.bib10)]。直接分析 MLP 神经元会遭遇“多义性”
    [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]：神经元激活许多不相关的概念。为了解决这个问题，稀疏自编码器（SAEs）[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)] 已被用于通过查看特征——即模型表示空间中的向量——而不是单独的神经元 [[20](#bib.bib20),
    [21](#bib.bib21)] 来执行精细电路分析。然而，尽管 SAE 特征通常是可解释的，这些向量往往是许多神经元的稠密线性组合 [[22](#bib.bib22)]。因此，机制上理解一个
    SAE 特征在一个或多个 MLP 层之前如何影响一个后续 SAE 特征可能需要考虑大量神经元及其非线性。以前的尝试 [[20](#bib.bib20), [21](#bib.bib21)]
    采用了因果干预和基于梯度的 MLP 层近似的混合方法。但这些方法未能展示输入不变性：特征之间的连接只能为给定输入描述，而不能为整个模型描述。尝试解决这一问题的方法，例如通过对许多输入结果取平均，反而失去了产生描述单一输入连接重要性的输入依赖信息的能力。
- en: 'Motivated by this, in this work, we explore transcoders (an idea proposed,
    but not explored, in Templeton et al. [[23](#bib.bib23)] and Li et al. [[24](#bib.bib24)]):
    wide, sparsely-activating approximations of a model’s original MLP sublayer. Specifically,
    an MLP transcoder is a wide ReLU MLP sublayer (with one hidden layer) that is
    trained to faithfully approximate the original narrower MLP sublayer’s output,
    with an L1 regularization penalty on neuron activations to encourage sparse activations.
    The main advantage of transcoders is they replace a difficult-to-interpret model
    component—the MLP sublayer—with an interpretable approximation that is otherwise
    faithful to the original computations. This allows us to interpret transcoder
    neurons rather than dense linear combinations of original MLP neurons.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 受到此启发，在这项工作中，我们探索了转码器（这是在 Templeton 等人 [[23](#bib.bib23)] 和 Li 等人 [[24](#bib.bib24)]
    中提出但未深入探讨的一个想法）：对模型原始 MLP 子层的宽度稀疏激活近似。具体来说，MLP 转码器是一个宽的 ReLU MLP 子层（具有一个隐藏层），它被训练以忠实地近似原始较窄的
    MLP 子层的输出，并对神经元激活施加 L1 正则化惩罚，以鼓励稀疏激活。转码器的主要优点在于它们用一个易于解释的近似模型替代了难以解释的模型组件——MLP
    子层，同时又忠实于原始计算。这使我们能够解释转码器神经元，而不是原始 MLP 神经元的稠密线性组合。
- en: Our contributions.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我们的贡献。
- en: Our main contributions are (1) to confirm that transcoders are a faithful and
    interpretable approximation to MLP sublayers, and (2) to demonstrate a novel method
    for circuit analysis using transcoders.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献是（1）确认转码器是对 MLP 子层的忠实和可解释的近似，并（2）展示了一种使用转码器进行电路分析的新方法。
- en: In §[3.2](#S3.SS2 "3.2 Relationship to SAEs ‣ 3 Transcoders ‣ Transcoders Find
    Interpretable LLM Feature Circuits"), we evaluate transcoders’ interpretability,
    sparsity, and faithfulness to the original model. Because SAEs are the standard
    method for finding sparse decompositions of model activations, we compare transcoders
    to SAEs on models up to 1.4 billion parameters and verify that transcoders are
    on par with SAEs or better with respect to these properties.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 §[3.2](#S3.SS2 "3.2 Relationship to SAEs ‣ 3 Transcoders ‣ Transcoders Find
    Interpretable LLM Feature Circuits") 中，我们评估了转码器的可解释性、稀疏性以及对原始模型的忠实性。由于 SAEs 是找到模型激活的稀疏分解的标准方法，我们将转码器与
    SAEs 进行比较，涉及到最多 14 亿参数的模型，并验证了转码器在这些特性方面与 SAEs 相当或更优。
- en: 'Beyond this, however, transcoders additionally enable circuit-finding techniques
    that are not possible using SAEs: in §[4.1](#S4.SS1 "4.1 Circuit analysis method
    ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable LLM Feature
    Circuits") we introduce a novel method for performing circuit analysis with transcoders
    and demonstrate that transcoders cleanly factorize circuits into input-invariant
    and input-dependent components. We apply transcoder circuit analysis to a variety
    of tasks in §[4.2](#S4.SS2 "4.2 Blind case study: reverse-engineering a feature
    ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable LLM Feature
    Circuits") and §[4.3](#S4.SS3 "4.3 Analyzing the GPT2-small “greater-than” circuit
    ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable LLM Feature
    Circuits"), including “blind case studies,” which demonstrate how this approach
    allows us to understand features without looking at specific examples, and an
    in-depth analysis of the GPT2-small “greater-than circuit” previously studied
    by Hanna et al. [[25](#bib.bib25)].'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，超越这些，转码器还额外启用了使用 SAE 无法实现的电路查找技术：在 §[4.1](#S4.SS1 "4.1 Circuit analysis
    method ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable
    LLM Feature Circuits") 中，我们介绍了一种使用转码器进行电路分析的新方法，并展示了转码器如何将电路干净地分解为输入不变和输入依赖的组件。我们在
    §[4.2](#S4.SS2 "4.2 Blind case study: reverse-engineering a feature ‣ 4 Circuit
    analysis with transcoders ‣ Transcoders Find Interpretable LLM Feature Circuits")
    和 §[4.3](#S4.SS3 "4.3 Analyzing the GPT2-small “greater-than” circuit ‣ 4 Circuit
    analysis with transcoders ‣ Transcoders Find Interpretable LLM Feature Circuits")
    中将转码器电路分析应用于各种任务，包括“盲目案例研究”，这些研究展示了这种方法如何让我们在不查看特定示例的情况下理解特征，以及对由 Hanna 等人 [[25](#bib.bib25)]
    之前研究的 GPT2-small “greater-than 电路”的深入分析。'
- en: Code for training transcoders and carrying out our experiments is available
    at [https://github.com/jacobdunefsky/transcoder_circuits](https://github.com/jacobdunefsky/transcoder_circuits).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练转码器和进行实验的代码可以在 [https://github.com/jacobdunefsky/transcoder_circuits](https://github.com/jacobdunefsky/transcoder_circuits)
    获取。
- en: EmbeddingAttention+MLP
    input$\mathbf{W_{in}}$Activation$\mathbf{W_{out}}$MLP output+UnembedTranscoderSAETransformer
    layer ($\times n_{L}$)MLP
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: EmbeddingAttention+MLP
    input$\mathbf{W_{in}}$Activation$\mathbf{W_{out}}$MLP output+UnembedTranscoderSAETransformer
    layer ($\times n_{L}$)MLP
- en: 'Figure 1: A comparison between SAEs, MLP transcoders, and MLP sublayers for
    a transformer-based language model. SAEs learn to reconstruct model activations,
    whereas transcoders imitate sublayers’ input-output behavior.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：基于变换器的语言模型中 SAEs、MLP 转码器和 MLP 子层的比较。SAEs 学习重建模型激活，而转码器模仿子层的输入输出行为。
- en: 2 Transformers preliminaries
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 Transformers preliminaries
- en: 'Following the approach of Elhage et al. [[9](#bib.bib9)], the computation of
    a transformer model can be represented as follows. First, the model maps input
    tokens (and their positions) to embeddings $\mathbf{x_{pre}^{(0,t)}}\in\mathbb{R}^{d_{\text{model}}}$,
    where $t$ is the token index and $d_{\text{model}}$ is the model dimensionality.
    Then, the model applies a series of “layers,” which map the hidden state at the
    end of the previous block to the new hidden state. This can be expressed as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 Elhage 等人 [[9](#bib.bib9)] 的方法，变换器模型的计算可以表示如下。首先，模型将输入标记（及其位置）映射到嵌入 $\mathbf{x_{pre}^{(0,t)}}\in\mathbb{R}^{d_{\text{model}}}$，其中
    $t$ 是标记索引，$d_{\text{model}}$ 是模型维度。然后，模型应用一系列“层”，将前一个块结束时的隐藏状态映射到新的隐藏状态。这可以表示为：
- en: '|  | $\displaystyle\mathbf{x_{mid}^{(l,t)}}$ | $\displaystyle=\mathbf{x_{pre}^{(l,t)}}+\sum_{\text{head
    $h$}}\operatorname{attn}^{(l,h)}\left(\mathbf{x^{(l,t)}_{pre}};\mathbf{x^{(l,1:t)}_{pre}}\right)$
    |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{x_{mid}^{(l,t)}}$ | $\displaystyle=\mathbf{x_{pre}^{(l,t)}}+\sum_{\text{head
    $h$}}\operatorname{attn}^{(l,h)}\left(\mathbf{x^{(l,t)}_{pre}};\mathbf{x^{(l,1:t)}_{pre}}\right)$
    |  | (1) |'
- en: '|  | $\displaystyle\mathbf{x_{pre}^{(l+1,t)}}$ | $\displaystyle=\mathbf{x_{mid}^{(l,t)}}+\operatorname{MLP}^{(l)}\left(\mathbf{x^{(l,t)}_{mid}}\right)$
    |  | (2) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{x_{pre}^{(l+1,t)}}$ | $\displaystyle=\mathbf{x_{mid}^{(l,t)}}+\operatorname{MLP}^{(l)}\left(\mathbf{x^{(l,t)}_{mid}}\right)$
    |  | (2) |'
- en: where $l$ is the layer index, $t$ is the token index, $\operatorname{attn}^{(l,h)}(\mathbf{x^{(l,t)}_{pre}};\mathbf{x^{(l,1:t)}_{pre}})$
    denotes the output of attention head $h$ at layer $l$ given all preceding source
    tokens $\mathbf{x^{(l,1:t)}_{pre}}$ and destination token $\mathbf{x^{(l,t)}_{pre}}$,
    and $\operatorname{MLP}^{(l)}(\mathbf{x^{(l,t)}_{mid}})$ denotes the output of
    the layer $l$ MLP.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l$ 是层索引，$t$ 是标记索引，$\operatorname{attn}^{(l,h)}(\mathbf{x^{(l,t)}_{pre}};\mathbf{x^{(l,1:t)}_{pre}})$
    表示在层 $l$ 中，给定所有前面的源标记 $\mathbf{x^{(l,1:t)}_{pre}}$ 和目标标记 $\mathbf{x^{(l,t)}_{pre}}$
    时，注意力头 $h$ 的输出，而 $\operatorname{MLP}^{(l)}(\mathbf{x^{(l,t)}_{mid}})$ 表示层 $l$
    MLP 的输出。
- en: Equation [1](#S2.E1 "In 2 Transformers preliminaries ‣ Transcoders Find Interpretable
    LLM Feature Circuits") shows how the attention sublayer updates the hidden state
    at token $t$, and Equation [2](#S2.E2 "In 2 Transformers preliminaries ‣ Transcoders
    Find Interpretable LLM Feature Circuits") shows how the MLP sublayer updates the
    hidden state. Importantly, each sublayer always adds its output to the current
    hidden state. As such, the hidden state always can be additively decomposed into
    the outputs of all previous sublayers. This motivates Elhage et al. [[9](#bib.bib9)]
    to refer to each token’s hidden state as its residual stream, which is “read from”
    and “written to” by each sublayer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 方程[1](#S2.E1 "在2 Transformers初步 ‣ 转码器发现可解释的LLM特征电路")展示了注意力子层如何在令牌$t$处更新隐藏状态，方程[2](#S2.E2
    "在2 Transformers初步 ‣ 转码器发现可解释的LLM特征电路")展示了MLP子层如何更新隐藏状态。重要的是，每个子层总是将其输出添加到当前隐藏状态中。因此，隐藏状态总是可以被加性地分解为所有先前子层的输出。这激发了Elhage等人[[9](#bib.bib9)]将每个令牌的隐藏状态称为其残差流，每个子层“读取”和“写入”这个残差流。
- en: 3 Transcoders
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 转码器
- en: 3.1 Architecture and training
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 架构和训练
- en: 'Transcoders aim to learn a “sparsified” approximation of an MLP sublayer: a
    transcoder approximates the output of an MLP sublayer as a sparse linear combination
    of feature vectors. Formally, the transcoder architecture can be expressed as'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 转码器旨在学习MLP子层的“稀疏”近似：转码器将MLP子层的输出近似为特征向量的稀疏线性组合。形式上，转码器的架构可以表示为
- en: '|  | $\displaystyle\mathbf{z_{TC}(\mathbf{x})}$ | $\displaystyle=\operatorname{ReLU}\left(\mathbf{W_{enc}}\mathbf{x}+\mathbf{b_{enc}}\right)$
    |  | (3) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{z_{TC}(\mathbf{x})}$ | $\displaystyle=\operatorname{ReLU}\left(\mathbf{W_{enc}}\mathbf{x}+\mathbf{b_{enc}}\right)$
    |  | (3) |'
- en: '|  | $\displaystyle\operatorname{TC}(\mathbf{x})$ | $\displaystyle=\mathbf{W_{dec}}\mathbf{z_{TC}(\mathbf{x})}+\mathbf{b_{dec}},$
    |  | (4) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{TC}(\mathbf{x})$ | $\displaystyle=\mathbf{W_{dec}}\mathbf{z_{TC}(\mathbf{x})}+\mathbf{b_{dec}},$
    |  | (4) |'
- en: where $\mathbf{x}$ is the input to the MLP sublayer, $\mathbf{W_{enc}}\in\mathbb{R}^{d_{\text{features}}\times
    d_{\text{model}}}$, $\mathbf{W_{dec}}\in\mathbb{R}^{d_{\text{model}}\times d_{\text{features}}}$,
    $\mathbf{b_{enc}}\in\mathbb{R}^{d_{\text{features}}}$, $\mathbf{b_{dec}}\in\mathbb{R}^{d_{\text{model}}}$,
    $d_{\text{features}}$ is the number of feature vectors in the transcoder, and
    $d_{\text{model}}$ is the dimensionality of the MLP input activations. Usually,
    $d_{\text{features}}$ is far greater than $d_{\text{model}}$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x}$ 是MLP子层的输入，$\mathbf{W_{enc}}\in\mathbb{R}^{d_{\text{features}}\times
    d_{\text{model}}}$，$\mathbf{W_{dec}}\in\mathbb{R}^{d_{\text{model}}\times d_{\text{features}}}$，$\mathbf{b_{enc}}\in\mathbb{R}^{d_{\text{features}}}$，$\mathbf{b_{dec}}\in\mathbb{R}^{d_{\text{model}}}$，$d_{\text{features}}$
    是转码器中特征向量的数量，$d_{\text{model}}$ 是MLP输入激活的维度。通常，$d_{\text{features}}$ 远大于 $d_{\text{model}}$。
- en: 'Each feature in a transcoder is associated with two vectors: the $i$-th row
    of $\mathbf{W_{enc}}$ is the encoder feature vector of feature $i$, and the $i$-th
    column of $\mathbf{W_{dec}}$ is the decoder feature vector of feature $i$. The
    $i$-th component of $\mathbf{z_{TC}(\mathbf{x})}$ is called the activation of
    feature $i$. Intuitively, for each feature, the encoder vector is used to determine
    how much the feature should activate; the decoder vector is then scaled by this
    amount, and the resulting weighted sum of decoder vectors is the output of the
    transcoder. In this paper, the notation $\mathbf{f^{(l,i)}_{enc}}$ and $\mathbf{f^{(l,i)}_{dec}}$
    are used to denote the $i$-th encoder feature vector and decoder feature vector,
    respectively, in the layer $l$ transcoder.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 转码器中的每个特征都与两个向量相关联：$\mathbf{W_{enc}}$ 的第 $i$ 行是特征 $i$ 的编码器特征向量，而 $\mathbf{W_{dec}}$
    的第 $i$ 列是特征 $i$ 的解码器特征向量。$\mathbf{z_{TC}(\mathbf{x})}$ 的第 $i$ 个分量称为特征 $i$ 的激活。直观地，对于每个特征，编码器向量用于确定特征应激活的程度；然后，解码器向量按这个程度缩放，最终得到的解码器向量的加权和就是转码器的输出。在本文中，记号
    $\mathbf{f^{(l,i)}_{enc}}$ 和 $\mathbf{f^{(l,i)}_{dec}}$ 用于表示第 $l$ 层转码器中的第 $i$
    个编码器特征向量和解码器特征向量。
- en: 'Because we want transcoders to learn to approximate an MLP sublayer’s computation
    with a sparse linear combination of feature vectors, transcoders are trained with
    the following loss, where $\lambda_{1}$ is a hyperparmeter mediating the tradeoff
    between sparsity and faithfulness:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望转码器能够学习通过特征向量的稀疏线性组合来近似MLP子层的计算，所以转码器是通过以下损失函数进行训练的，其中 $\lambda_{1}$ 是调节稀疏性与真实性之间权衡的超参数：
- en: '|  | $\mathcal{L}_{TC}(\mathbf{x})=\underbrace{\left\&#124;\operatorname{MLP}(\mathbf{x})-\operatorname{TC}(\mathbf{x})\right\&#124;^{2}_{2}}_{\text{faithfulness
    loss}}+\underbrace{\lambda_{1}\left\&#124;\mathbf{z_{TC}(\mathbf{x})}\right\&#124;_{1}}_{\text{sparsity
    penalty}}.$ |  | (5) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{TC}(\mathbf{x})=\underbrace{\left\&#124;\operatorname{MLP}(\mathbf{x})-\operatorname{TC}(\mathbf{x})\right\&#124;^{2}_{2}}_{\text{忠实度损失}}+\underbrace{\lambda_{1}\left\&#124;\mathbf{z_{TC}(\mathbf{x})}\right\&#124;_{1}}_{\text{稀疏性惩罚}}.$
    |  | (5) |'
- en: 3.1.1 Evaluation metrics
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 评估指标
- en: We evaulate transcoders qualitatively on their features’ interpretability as
    judged by a human rater, and quantitatively according to the sparsity of their
    activations and their fidelity to the original MLP’s computation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从定性上评估转码器的特性，评判标准由人工评估者决定，从定量上则依据其激活的稀疏性和对原始MLP计算的忠实度来评估。
- en: As a qualitative proxy measure for the interpretability of a feature, we follow Bricken
    et al. [[17](#bib.bib17)] in assuming that interpretable features should demonstrate
    interpretable patterns in the examples that cause them to activate. To this end,
    one can run the transcoder on a large dataset of text, see which dataset examples
    cause the feature to activate, and see if there is an interpretable pattern among
    these tokens. While this is an imperfect metric [[26](#bib.bib26)], it is still
    a reasonable proxy for an inherently qualitative concept.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作为特征解释性的定性代理测量，我们遵循Bricken等人[[17](#bib.bib17)]的假设，即可解释的特征应在导致其激活的示例中显示出可解释的模式。为此，可以在一个大文本数据集上运行转码器，查看哪些数据集示例导致特征激活，并查看这些标记中是否存在可解释的模式。虽然这是一个不完美的指标[[26](#bib.bib26)]，但它仍然是对固有定性概念的合理代理。
- en: To measure the sparsity of a transcoder, one can run the transcoder on a dataset
    of inputs, and calculate the mean number of features active on each token (the
    mean $L_{0}$ norm of the activations). To measure the fidelity of the transcoder,
    one can perform the following procedure. First, run the original model on a large
    dataset of inputs, and measure the next-token-prediction cross entropy loss on
    the dataset. Then, replace the model’s MLP sublayer corresponding to the transcoder
    with the transcoder, and measure the modified model’s mean loss on the dataset.
    Now, the faithfulness of the transcoder can be quantified as the difference between
    the modified model’s loss and the original model’s loss.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量转码器的稀疏性，可以在一个输入数据集上运行转码器，并计算每个标记上激活的特征的平均数量（激活的平均 $L_{0}$ 范数）。为了测量转码器的忠实度，可以执行以下步骤。首先，在一个大数据集上运行原始模型，并测量数据集上的下一个标记预测交叉熵损失。然后，将模型的MLP子层替换为转码器，并测量修改后的模型在数据集上的平均损失。现在，转码器的忠实度可以量化为修改后模型的损失与原始模型的损失之间的差异。
- en: 3.2 Relationship to SAEs
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 与SAEs的关系
- en: 'Transcoders were originally conceived as a variant of SAEs, and as such, there
    are many similarities between them. SAEs have the exact same architecture as transcoders
    in the sense that they also have encoder and decoder feature vectors, but differ
    in how they are trained: because SAEs are autoencoders, the faithfulness term
    in the SAE loss measures the reconstruction error between the SAE’s output and
    its original input. In contrast, the faithfulness term of the transcoder loss
    measures the error between the transcoder’s output and the original MLP sublayer’s
    output.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 转码器最初被构思为SAEs的一种变体，因此它们之间有许多相似之处。SAEs在架构上与转码器完全相同，因为它们也具有编码器和解码器特征向量，但训练方式不同：由于SAEs是自编码器，SAE损失中的忠实度项测量SAE输出与其原始输入之间的重建误差。相比之下，转码器损失中的忠实度项测量转码器输出与原始MLP子层输出之间的误差。
- en: Because of the extensive similarities between SAEs and transcoders, SAEs can
    be quantitatively evaluated (for sparsity and fidelity) and qualitatively evaluated
    (for feature interpretability) in precisely the same way as transcoders. In fact,
    the aforementioned transcoder evaluation methods are also standard for evaluating
    SAEs [[27](#bib.bib27), [28](#bib.bib28)]. We now report the results of evaluations
    comparing SAEs to transcoders on these metrics, and find that transcoders are
    comparable to or better than SAEs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SAEs和转码器之间的广泛相似性，SAEs可以以与转码器完全相同的方式进行定量评估（针对稀疏性和忠实度）和定性评估（针对特征解释性）。事实上，上述的转码器评估方法也标准化用于评估SAEs
    [[27](#bib.bib27), [28](#bib.bib28)]。我们现在报告了比较SAEs与转码器在这些指标上的评估结果，并发现转码器与SAEs相比具有可比性或更优。
- en: 3.2.1 Blind interpretability comparison of transcoders to SAEs
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 转码器与SAEs的盲解释性比较
- en: In order to evaluate the interpretability of transcoders, we manually attempted
    to interpret 50 random features from a Pythia-410M [[29](#bib.bib29)] layer 15
    transcoder and 50 random features from a Pythia-410M layer 15 SAE trained on MLP
    inputs. For each feature, the examples in a subset of the OpenWebText corpus [[30](#bib.bib30)]
    that caused the feature to activate the most were computed ahead of time. Then,
    the features from both the SAE and the transcoder were randomly shuffled. For
    each feature, the maximum-activating examples were displayed, but not whether
    the feature came from an SAE or transcoder. We recorded for each feature whether
    or not there seemed to be an interpretable pattern, and only after examining every
    feature did we look at which features came from where. The results, shown in Table
    [1](#S3.T1 "Table 1 ‣ 3.2.1 Blind interpretability comparison of transcoders to
    SAEs ‣ 3.2 Relationship to SAEs ‣ 3 Transcoders ‣ Transcoders Find Interpretable
    LLM Feature Circuits"), suggest transcoder features are approximately as interpretable
    as SAE features. This further suggests that transcoders incur no penalties compared
    to SAEs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估转码器的可解释性，我们手动尝试解释了50个来自Pythia-410M [[29](#bib.bib29)] 层15转码器的随机特征和50个来自Pythia-410M层15
    SAE在MLP输入上训练的随机特征。对于每个特征，计算了导致特征最激活的OpenWebText语料库[[30](#bib.bib30)]中的一个子集中的示例。然后，将SAE和转码器的特征随机打乱。对于每个特征，展示了最大激活的示例，但未显示特征来源于SAE还是转码器。我们记录了每个特征是否存在可解释的模式，只有在检查完每个特征后才查看特征的来源。结果如表[1](#S3.T1
    "Table 1 ‣ 3.2.1 Blind interpretability comparison of transcoders to SAEs ‣ 3.2
    Relationship to SAEs ‣ 3 Transcoders ‣ Transcoders Find Interpretable LLM Feature
    Circuits")所示，表明转码器特征的可解释性与SAE特征大致相当。这进一步表明，与SAE相比，转码器没有额外的惩罚。
- en: 'Table 1: The number of interpretable features, possibly-interpretable features,
    and uninterpretable features for the transcoder and SAE. Of the interepretable
    features, we additionally deemed 6 transcoder features and 16 SAE features to
    be “context-free”, meaning they appeared to fire on a single token without any
    evident context-depdendent patterns.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：转码器和SAE的可解释特征、可能可解释特征和不可解释特征的数量。在可解释特征中，我们额外认为6个转码器特征和16个SAE特征为“无上下文”，即它们似乎在没有明显上下文相关模式的情况下对单个标记进行激活。
- en: '|  | # interpretable | # maybe | # uninterpretable |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | # interpretable | # maybe | # uninterpretable |'
- en: '| --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Transcoder | 41 | 8 | 1 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 转码器 | 41 | 8 | 1 |'
- en: '| SAE | 38 | 8 | 4 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| SAE | 38 | 8 | 4 |'
- en: 3.2.2 Quantitative comparison of transcoders to sparse autoencoders
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 转码器与稀疏自编码器的定量比较
- en: We now compare transcoders to SAEs according to the sparsity and fidelity metrics
    discussed in §[3.1.1](#S3.SS1.SSS1 "3.1.1 Evaluation metrics ‣ 3.1 Architecture
    and training ‣ 3 Transcoders ‣ Transcoders Find Interpretable LLM Feature Circuits").
    We trained transcoders on MLP activations and SAEs on MLP output activations from
    GPT2-small [[31](#bib.bib31)], Pythia-410M, and Pythia-1.4B. For each model, we
    trained multiple SAEs and transcoders on the same activations, but varying the
    $\lambda_{1}$ hyperparameter controlling the fidelity-sparsity tradeoff. We evaluated
    each SAE and transcoder on the same 3.2 million tokens of OpenWebText data. We
    also recorded the unmodified model’s loss and the loss after mean-ablating the
    entire MLP sublayer (always replacing its output with its mean output over the
    dataset) as best- and worst-case bounds, respectively.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在根据第§[3.1.1](#S3.SS1.SSS1 "3.1.1 Evaluation metrics ‣ 3.1 Architecture and
    training ‣ 3 Transcoders ‣ Transcoders Find Interpretable LLM Feature Circuits")节讨论的稀疏性和保真度指标比较转码器和SAE。我们在GPT2-small
    [[31](#bib.bib31)]、Pythia-410M 和 Pythia-1.4B的MLP激活上训练了转码器，并在MLP输出激活上训练了SAE。对于每个模型，我们在相同的激活上训练了多个SAE和转码器，但改变了控制保真度-稀疏性权衡的$\lambda_{1}$超参数。我们在相同的320万OpenWebText数据上评估了每个SAE和转码器。我们还记录了未修改模型的损失和在均值剥离整个MLP子层（始终用数据集的均值输出替换其输出）后的损失，作为最佳和最差情况的界限。
- en: '![Refer to caption](img/cca042781ce59ea4425c0b90f5062a63.png)![Refer to caption](img/b5fc33f7404a187b183d4d26d02cad33.png)![Refer
    to caption](img/d3b4f9d25cd617b9887db49c37bf68a3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cca042781ce59ea4425c0b90f5062a63.png)![参见说明](img/b5fc33f7404a187b183d4d26d02cad33.png)![参见说明](img/d3b4f9d25cd617b9887db49c37bf68a3.png)'
- en: 'Figure 2: The sparsity-accuracy tradeoff of transcoders versus SAEs on GPT2-small,
    Pythia-410M, and Pythia-1.4B. Each point corresponds to a trained SAE or transcoder,
    and is labeled with the L1 regularization penalty $\lambda_{1}$ used during training.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：GPT2-small、Pythia-410M和Pythia-1.4B上转码器与SAEs的稀疏性-准确性权衡。每个点对应一个训练过的SAE或转码器，并标记了训练过程中使用的L1正则化惩罚$\lambda_{1}$。
- en: We summarize the Pareto frontiers of the sparsity-accuracy tradeoff for all
    models in Figure [2](#S3.F2 "Figure 2 ‣ 3.2.2 Quantitative comparison of transcoders
    to sparse autoencoders ‣ 3.2 Relationship to SAEs ‣ 3 Transcoders ‣ Transcoders
    Find Interpretable LLM Feature Circuits"). In all cases, transcoders are equal
    to or better than SAEs. In fact, the gap between transcoders and SAEs seems to
    widen on larger models. Note, however, that compute limitations prevented us from
    performing more exhaustive hyperparameter sweeps; as such, it might be possible
    that a different set of hyperparameters could have allowed SAEs to surpass transcoders.
    Nonetheless, these results make us optimistic that using transcoders incurs no
    penalties versus SAEs trained on MLP activations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[2](#S3.F2 "图2 ‣ 3.2.2 转码器与稀疏自编码器的定量比较 ‣ 3.2 与SAEs的关系 ‣ 3 转码器 ‣ 转码器找到可解释的LLM特征电路")中总结了所有模型的稀疏性-准确性权衡的Pareto前沿。在所有情况下，转码器的表现等同于或优于SAEs。实际上，转码器与SAEs之间的差距似乎在更大的模型上有所扩大。然而，需要注意的是，计算限制阻止了我们进行更为全面的超参数搜索；因此，可能存在另一组超参数可以使SAEs超过转码器。尽管如此，这些结果使我们对使用转码器没有比在MLP激活上训练的SAEs更大的惩罚持乐观态度。
- en: 4 Circuit analysis with transcoders
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用转码器的电路分析
- en: 4.1 Circuit analysis method
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 电路分析方法
- en: We now introduce a novel method for performing feature-level circuit analysis
    with transcoders, which provides a scalable and interpretable way to identify
    which transcoder features in different layers connect to compute a given task.
    Moreover, this method neatly factorizes the importance of computational subgraphs
    into input-invariant terms, which can be computed just from model and transcoder
    weights, and input-dependent terms, which depend on the specific model input.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在介绍了一种使用转码器进行特征级电路分析的新方法，它提供了一种可扩展且可解释的方式来识别不同层中哪些转码器特征与计算给定任务相关。此外，这种方法巧妙地将计算子图的重要性分解为输入不变的项，这些项可以仅通过模型和转码器权重计算得出，以及依赖于输入的项，这些项依赖于特定的模型输入。
- en: 4.1.1 Attribution between feature pairs
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 特征对之间的归因
- en: The primary goal of circuit analysis is to identify a subgraph of the model’s
    computational graph that is responsible for (most of) the model’s behavior on
    a given task [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]; this requires
    a means of evaluating a computational subgraph’s importance to the task in question.
    Unfortunately, as discussed in §[1](#S1 "1 Introduction ‣ Transcoders Find Interpretable
    LLM Feature Circuits"), MLP sublayers make this difficult. But with more interpretable
    and sufficiently faithful transcoders, we can replace the MLP sublayers to obtain
    a more interpretable computational graph more amenable to circuit analysis.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 电路分析的主要目标是识别模型计算图中的一个子图，该子图负责模型在给定任务上的（大部分）行为[[32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34)]；这需要一种评估计算子图对任务重要性的手段。不幸的是，正如在§[1](#S1 "1 介绍 ‣ 转码器找到可解释的LLM特征电路")中讨论的那样，MLP子层使得这一点变得困难。但通过更具可解释性且足够忠实的转码器，我们可以替换掉MLP子层，从而获得一个更具可解释性的计算图，便于电路分析。
- en: 'In order to identify the relevant subgraph in this transcoder computational
    graph, we begin with the following insight: every transcoder feature contributes
    some (possibly zero) amount to the residual stream, which results in some contribution
    to all subsequent transcoder features. This means that we can quantify the attribution
    of an earlier-layer feature to a later-layer feature’s activation—which allows
    us to identify important edges in the computational graph. This attribution is
    given by the product of two terms: the earlier feature’s activation (which depends
    on the input to the model), and the dot product of the earlier feature’s decoder
    vector with the later feature’s encoder vector (which is independent of the model
    input).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在这个转码器计算图中识别相关的子图，我们从以下见解开始：每个转码器特征对剩余流的贡献（可能为零），这导致对所有后续转码器特征的贡献。这意味着我们可以量化早期层特征对后期层特征激活的归因——这使我们能够识别计算图中的重要边。这个归因由两个术语的乘积给出：早期特征的激活（依赖于模型的输入），以及早期特征解码器向量与后期特征编码器向量的点积（独立于模型输入）。
- en: The following is a more formal restatement. Let $z_{TC}^{(l,i)}\left(\mathbf{x^{(l,t)}_{mid}}\right)$
    denote the scalar activation of the $i$-th feature in the layer $l$ transcoder
    on token $t$, as a function of the MLP input $\mathbf{x^{(l,t)}_{mid}}$ at token
    $t$ in layer $l$. Then for layer $lattn0[5]@37embed0@37attn0[1]@37attn0[3]@37tc0[9188]@37tc0[16632]@371.1tc2[3900]@37tc1[22184]@371.1tc3[6238]@37tc8[355]@372.4
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: attn0[5]@37embed0@37attn0[1]@37attn0[3]@37tc0[9188]@37tc0[16632]@371.1tc2[3900]@37tc1[22184]@371.1tc3[6238]@37tc8[355]@372.4
- en: 'Figure 3: An example of a computational graph produced using the method in
    §[4.1.2](#S4.SS1.SSS2 "4.1.2 Finding computational subgraphs ‣ 4.1 Circuit analysis
    method ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable
    LLM Feature Circuits") characterizing how our unknown feature is computed on an
    unseen input. A single path is highlighted in red and annotated with component-by-component
    attributions.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用§[4.1.2](#S4.SS1.SSS2 "4.1.2 查找计算子图 ‣ 4.1 电路分析方法 ‣ 4 电路分析与转码器 ‣ 转码器寻找可解释的LLM特征电路")中描述的方法生成的计算图示例，展示了我们未知特征在未见输入上的计算方式。一个路径用红色高亮显示，并逐个组件进行注释。
- en: '4.1.3 De-embeddings: a special case of input-invariant information'
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 去嵌入：输入不变信息的特殊情况
- en: Earlier, we discussed how to compute the input-invariant connection between
    a pair of transcoder features, providing insights on general behavior of the model.
    A related technique is something that we call de-embeddings. A de-embedding vector
    for a transcoder feature is a vector that contains the direct effect of the embedding
    of each token in the model’s vocabulary on the transcoder feature. The de-embedding
    vector for feature $i$ in the layer $l$ transcoder is given by $\mathbf{W_{E}}^{T}\mathbf{f^{(l,i)}_{enc}}$,
    where $\mathbf{W_{E}}$ is the model’s token embedding matrix. Importantly, this
    vector gives us input-invariant information about how much each possible input
    token would directly contribute to the feature’s activation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了如何计算一对转码器特征之间的输入不变连接，这为模型的一般行为提供了见解。相关的技术是我们称之为去嵌入的技术。一个转码器特征的去嵌入向量是一个向量，包含模型词汇表中每个标记的嵌入对该转码器特征的直接影响。层$l$转码器中特征$i$的去嵌入向量由$\mathbf{W_{E}}^{T}\mathbf{f^{(l,i)}_{enc}}$给出，其中$\mathbf{W_{E}}$是模型的标记嵌入矩阵。重要的是，这个向量提供了关于每个可能的输入标记如何直接贡献于特征激活的输入不变信息。
- en: 'Given a de-embedding vector, looking at which tokens in the model’s vocabulary
    have the highest de-embedding scores tells us about the feature’s general behavior.
    For example, for a certain GPT2-small MLP0 transcoder feature that we investigated,
    the tokens with the highest scores were oglu, owsky, zyk, chenko, and kowski.
    Notice the interpretable pattern: all of these tokens come from European surnames,
    primarily Polish ones. This suggests that the general behavior of the feature
    is to fire on Polish surnames.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个去嵌入向量，查看模型词汇表中哪些标记具有最高的去嵌入分数，可以了解该特征的总体行为。例如，对于我们调查的某个GPT2-small MLP0转码器特征，具有最高分数的标记是oglu、owsky、zyk、chenko和kowski。注意到可解释的模式：所有这些标记来自欧洲姓氏，主要是波兰姓氏。这表明该特征的一般行为是对波兰姓氏有响应。
- en: '4.2 Blind case study: reverse-engineering a feature'
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 盲例研究：特征的逆向工程
- en: 'To understand the utility of transcoders for circuit analysis, we carried out
    nine blind case studies, where we randomly selected individual transcoder features
    in a ninth-layer (of 12) GPT2-small transcoder and used circuit analysis to form
    a hypothesis about the semantics of the feature—without looking at the text of
    examples that cause the feature to activate. In blind case studies, we use a combination
    of input-invariant and input-dependent information to allow us to evaluate transcoders
    as a tool to infer model behavior with minimal prompt information. We believe
    that if our circuit analysis tools can cleanly disentangle input-dependent and
    input-invariant information, then we can better understand the extent to which
    their insights are likely to generalise and allow us to predict out-of-distribution
    behavior, a key goal for mechanistic interpretability. The “rules of the game”
    for blind case studies are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解转码器在电路分析中的实用性，我们进行了九项盲案例研究，其中我们随机选择了第9层（共12层）GPT2-small转码器中的单个转码器特征，并使用电路分析来形成对特征语义的假设——在不查看导致特征激活的示例文本的情况下。在盲案例研究中，我们使用输入不变和输入相关的信息的组合，允许我们将转码器作为工具来推断模型行为，同时提供最少的提示信息。我们相信，如果我们的电路分析工具能够清晰地解开输入相关和输入不变的信息，那么我们就可以更好地理解这些洞察的推广程度，并使我们能够预测分布外行为，这是机械解释性的一个关键目标。盲案例研究的“游戏规则”如下：
- en: '1.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The specific tokens contained in any prompt are not allowed to be directly seen.
    As such, prompts and tokens can only be referenced by their index in the dataset.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任何提示中包含的具体标记不得直接看到。因此，提示和标记只能通过它们在数据集中的索引进行引用。
- en: '2.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: These prompts may be used to compute input-dependent information (activations
    and circuits), as long as the tokens themselves remain hidden.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些提示可以用于计算输入相关的信息（激活和电路），只要标记本身保持隐藏。
- en: '3.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Any input-invariant information, including feature de-embeddings, is allowed.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任何输入不变的信息，包括特征去嵌入，是允许的。
- en: In this section, we summarise a specific blind case study, how we used our circuits
    to reverse-engineer feature 355 in our layer 8 transcoder. Other studies, as well
    as a longer description of the study summarized here, can be found in App. [H](#A8
    "Appendix H Full case studies ‣ Transcoders Find Interpretable LLM Feature Circuits").
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了一个具体的盲案例研究，即我们如何使用电路来逆向工程第8层转码器中的特征355。其他研究以及对这里总结的研究的更长描述可以在附录 [H](#A8
    "Appendix H Full case studies ‣ Transcoders Find Interpretable LLM Feature Circuits")
    中找到。
- en: 'Note that we use the following compact notation for transcoder features: tcA[B]@C
    refers to feature B in the layer A transcoder at token C.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们对转码器特征使用以下紧凑符号表示法：tcA[B]@C表示第A层转码器中标记C的特征B。
- en: Building the first circuit. We started by getting a list of indices of the top-activating
    prompts in the dataset for tc8[355]. Importantly, we did not look at the actual
    tokens in these prompts, as doing so would violate Rule 1. For our first input,
    we chose example 5701, token 37; tc8[355] fires at strength 11.91 on this token
    in this input. Our greedy algorithm for finding the most important computational
    paths for causing tc8[355]@37 to fire revealed contributions from the current
    token (37) and earlier tokens (like 35, 36, and 31).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 构建第一个电路。我们首先获取了数据集中tc8[355]的顶级激活提示的索引列表。重要的是，我们没有查看这些提示中的实际标记，因为这样做会违反规则1。对于我们的第一个输入，我们选择了示例5701，标记37；在此输入中的该标记上，tc8[355]的激活强度为11.91。我们寻找导致tc8[355]@37激活的最重要计算路径的贪婪算法揭示了当前标记（37）和早期标记（如35、36和31）的贡献。
- en: Current-token features. From token 37, we found strong contributions from tc0[16632]@37
    and tc0[9188]@37. Input-invariant de-embeddings of these layer 0 features revealed
    that they primarily activate on variants of ;, causing us to hypothesize that
    token 37 contributed to the feature by virtue of being a semicolon. Another feature
    which contributed strongly through the current token, tc6[11831], showed a similar
    pattern. Among the top input-invariant connections from layer 0 transcoder features
    to tc6[11831], we once again found the same semicolon features tc0[16632] and
    tc0[9188].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当前标记特征。从标记37开始，我们发现tc0[16632]@37和tc0[9188]@37有强烈的贡献。对这些第0层特征进行输入不变的去嵌入分析显示，它们主要在`;`的变体上激活，因此我们推测标记37通过作为分号来贡献了这一特征。另一个通过当前标记强烈贡献的特征tc6[11831]表现出类似的模式。在第0层转码器特征到tc6[11831]的输入不变连接中，我们再次发现了相同的分号特征tc0[16632]和tc0[9188]。
- en: Previous-token features. Next we checked computational paths from previous tokens
    through attention heads. Looking at these contextual computational paths revealed
    a contribution from tc0[13196]@36; the top de-embeddings for this feature were
    years like 1973, 1971, 1967, and 1966. Additionally, there was a contribution
    from tc0[10109]@31, for which the top de-embedding was (.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 之前标记的特征。接下来，我们检查了从之前标记通过注意力头的计算路径。查看这些上下文计算路径揭示了tc0[13196]@36的贡献；该特征的主要去嵌入标记为1973、1971、1967和1966等年份。此外，还有tc0[10109]@31的贡献，其主要去嵌入标记为(。
- en: Furthermore, there was a contribution from tc6[21046]@35. The top input-invariant
    connections to this feature from layer 0 were tc0[16382] and tc0[5468]. The top
    de-embeddings for the former were tokens associated with Eastern European last
    names (e.g. kowski, chenko, owicz) and the top de-embeddings for the latter feature
    were English surnames (e.g. Burnett, Hawkins, Johnston). This heavily suggested
    that tc6[21046] was a surname feature.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，tc6[21046]@35也有贡献。层0中与此特征的主要输入不变连接为tc0[16382]和tc0[5468]。前者的主要去嵌入表示为与东欧姓氏相关的词汇（例如kowski、chenko、owicz），而后者特征的主要去嵌入表示为英文姓氏（例如Burnett、Hawkins、Johnston）。这强烈表明tc6[21046]是一个姓氏特征。
- en: 'Thus, the circuit revealed this pattern was important to our feature: “( -
    [?] -[?] - [?] - [surname] - [year] - ;”.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，电路揭示了此模式对我们的特征非常重要：“( - [?] -[?] - [?] - [surname] - [year] - ;”。
- en: Analysis.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分析。
- en: We hypothesized that tc8[355] fires on semicolons in parenthetical citations
    like “(Vaswani et al. 2017; Elhage et al. 2021)”. Further investigation on another
    input yielded a similar pattern—along with a feature whose top de-embedding tokens
    included Accessed, Retrieved, Neuroscience, and Springer. This bolstered our hypothesis
    even more.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设tc8[355]会在括号引用中的分号上激活，例如“(Vaswani et al. 2017; Elhage et al. 2021)”。对另一输入的进一步调查显示了类似的模式——还有一个特征，其主要去嵌入标记包括Accessed、Retrieved、Neuroscience和Springer。这进一步支持了我们的假设。
- en: Here, we decided to end the blind case study and check if our hypothesis was
    correct. Sure enough, the top activating examples included semicolons in citations
    such as “(Poeck, 1969; Rinn, 1984)” and “(Robinson et al., 1984; Starkstein et
    al., 1988)”. We note that the first of these is the example at index $(5701,37)$
    we analyzed above.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们决定结束盲例研究，并检查我们的假设是否正确。果然，主要激活的示例包括引文中的分号，例如“(Poeck, 1969; Rinn, 1984)”和“(Robinson
    et al., 1984; Starkstein et al., 1988)”。我们注意到，第一个示例是我们之前分析的索引为$(5701,37)$的例子。
- en: “Restricted” blind case studies.
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: “受限”盲例研究。
- en: Because MLP0 features tend to be single-token, significant information about
    the original prompt can be obtained by looking at which MLP0 transcoder features
    are active and then taking their de-embeddings. In order to address this and more
    fully investigate the power of input-invariant circuit analysis, six of the eight
    case studies that we carried out were restricted blind case studies, in which
    all input-dependent MLP0 feature information is forbidden to use. For more details
    on these case studies, see Appendix [H.2](#A8.SS2 "H.2 Restricted blind case studies
    ‣ Appendix H Full case studies ‣ Transcoders Find Interpretable LLM Feature Circuits").
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MLP0特征往往是单标记的，通过查看哪些MLP0转码器特征处于活动状态，然后获取它们的去嵌入表示，可以获得关于原始提示的重要信息。为了解决这个问题，并更全面地调查输入不变电路分析的能力，我们进行的八个案例研究中的六个是受限盲例研究，在这些研究中禁止使用所有依赖输入的MLP0特征信息。有关这些案例研究的更多详细信息，请参见附录[H.2](#A8.SS2
    "H.2 受限盲例研究 ‣ 附录H 完整案例研究 ‣ 转码器发现可解释的LLM特征电路")。
- en: 4.3 Analyzing the GPT2-small “greater-than” circuit
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 分析GPT2-small的“greater-than”电路
- en: 'We now turn to address the “greater-than” circuit in GPT2-small previously
    considered by Hanna et al. [[35](#bib.bib35)]. They considered the following question:
    given a prompt such as “The war lasted from 1737 to 17”, how does the model know
    that the predicted next year token has to be greater than 1737? In their original
    work, they analyzed the circuit responsible for this behavior and demonstrated
    that MLP10 plays an important role, looking into the operation of MLP10 at a neuronal
    level. We now apply transcoders and the circuit analysis tools accompanying them
    to this same problem.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在来讨论Hanna等人之前考虑过的GPT2-small中的“greater-than”电路[[35](#bib.bib35)]。他们考虑了以下问题：给定一个提示，比如“The
    war lasted from 1737 to 17”，模型如何知道预测的下一年标记必须大于1737？在他们的原始工作中，他们分析了负责这一行为的电路，并展示了MLP10在神经层面上的重要作用。我们现在将应用转码器及其附带的电路分析工具来解决这个问题。
- en: 4.3.1 Initial investigation
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 初步调查
- en: First, we used the methods from Sec. [4.1.2](#S4.SS1.SSS2 "4.1.2 Finding computational
    subgraphs ‣ 4.1 Circuit analysis method ‣ 4 Circuit analysis with transcoders
    ‣ Transcoders Find Interpretable LLM Feature Circuits") to investigate a single
    prompt and obtain the computational paths most relevant to the task. This placed
    a high attribution on MLP10 features, which were in turn activated by earlier-layer
    features mediated by attention head 1 in layer 9. This corroborates the analysis
    in the original work.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用了第 [4.1.2节](#S4.SS1.SSS2 "4.1.2 查找计算子图 ‣ 4.1 电路分析方法 ‣ 4 电路分析与转码器 ‣ 转码器发现可解释的LLM特征电路")
    中的方法来调查单个提示，并获得与任务最相关的计算路径。这对MLP10特征赋予了高归因，这些特征又被早期层特征通过第9层的注意力头1激活。这证实了原始工作的分析。
- en: Next, we investigated which MLP10 transcoder features were most important on
    a variety of prompts, and how their activations are mediated by attention head
    1 in layer 9. Following the original work, we generated all 100 prompts of the
    form “The war lasted from 17YY to 17”, where YY denotes a two-digit number. We
    found that the MLP10 features with the highest variance in activations over this
    set of prompts also had top input-dependent connections from MLP0 features through
    attention head 1 in layer 9 whose top de-embeddings were two-digit numbers. We
    then turned to look at the MLP0 features with the top input-invariant connections
    to these MLP10 features, as mediated by attention head 1 in layer 9. Looking at
    the top de-embedding tokens in turn for these MLP0 features gives us input-invariant
    information about which tokens in the input are most important for causing the
    MLP10 features to activate.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调查了在各种提示下，MLP10 转码器特征中哪些是最重要的，以及它们的激活如何通过第9层的注意力头1进行调节。根据原始工作，我们生成了所有100个形式为“The
    war lasted from 17YY to 17”的提示，其中YY表示两位数。我们发现，在这一组提示中，激活方差最高的MLP10特征也具有来自MLP0特征通过第9层的注意力头1的顶级输入依赖连接，这些顶级解嵌入是两位数。然后，我们转向查看具有顶级输入不变连接的MLP0特征，这些连接通过第9层的注意力头1与这些MLP10特征相互作用。依次查看这些MLP0特征的顶级解嵌入令牌给出了关于输入中哪些令牌对激活MLP10特征最重要的输入不变信息。
- en: As it turned out, almost always among the top five de-embedding tokens for these
    MLP10 features were two-digit numbers—indicating that two-digit number tokens
    were among the most important for causing these MLP10 features to fire out of
    all tokens in the model’s vocabulary, without reference to any specific input.
    This positive result was somewhat unexpected, given that there are only 100 two-digit
    number tokens in the model’s vocabulary of over 50k tokens.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，在这些MLP10特征的前五名解嵌入令牌中几乎总是出现两位数——这表明两位数令牌是导致这些MLP10特征激活的最重要的令牌之一，而不参考任何特定输入。这个积极的结果有些出乎意料，因为在模型的5万多个令牌词汇中，只有100个两位数令牌。
- en: We then used direct logit attribution (DLA) [[9](#bib.bib9)] to look at the
    effect of each transcoder feature on the predicted logits of each YY token in
    the model’s vocabulary. These results, along with normalized de-embedding scores
    (details in App. [G](#A7 "Appendix G Details on Section 4.3 ‣ Transcoders Find
    Interpretable LLM Feature Circuits")) for each YY token in the model’s vocabulary,
    can be seen in Figure [4](#S4.F4 "Figure 4 ‣ 4.3.1 Initial investigation ‣ 4.3
    Analyzing the GPT2-small “greater-than” circuit ‣ 4 Circuit analysis with transcoders
    ‣ Transcoders Find Interpretable LLM Feature Circuits"). We see that the de-embedding
    scores are highest for YY tokens where years following them are boosted and years
    preceding them are inhibited.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用直接逻辑归因（DLA）[[9](#bib.bib9)]来查看每个转码器特征对模型词汇中每个YY令牌预测对数的影响。这些结果，以及模型词汇中每个YY令牌的标准化解嵌入分数（详细信息见附录
    [G](#A7 "附录G 第4.3节 ‣ 转码器发现可解释的LLM特征电路")），可以在图 [4](#S4.F4 "图4 ‣ 4.3.1 初步调查 ‣ 4.3
    分析GPT2-small的“greater-than”电路 ‣ 转码器电路分析 ‣ 转码器发现可解释的LLM特征电路") 中看到。我们看到，解嵌入分数在YY令牌之后的年份被增强，而之前的年份被抑制时最高。
- en: '![Refer to caption](img/bd3f52536d7bb37d0afbd142db6b6248.png)![Refer to caption](img/bd0837b106edf9df64fc82a83a460286.png)![Refer
    to caption](img/7c2126aa7ac66d7eab1b3fa227198b20.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bd3f52536d7bb37d0afbd142db6b6248.png)![参考说明](img/bd0837b106edf9df64fc82a83a460286.png)![参考说明](img/7c2126aa7ac66d7eab1b3fa227198b20.png)'
- en: 'Figure 4: For the three MLP10 transcoder features with the highest activation
    variance over the “greater-than” dataset, and for every possible YY token, we
    plot the DLA (the extent to which the feature boosts the output probability of
    YY) and the de-embedding score (an input-invariant measurement of how much YY
    causes the feature to fire).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：对于“greater-than”数据集中激活方差最高的三个MLP10转码器特征，以及每个可能的YY标记，我们绘制了DLA（特征提升YY输出概率的程度）和去嵌入分数（特征触发YY的输入不变测量）。
- en: 4.3.2 Comparison with neuronal approach
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 与神经元方法的比较
- en: Next, we compared the transcoder approach to the neuronal approach to see whether
    transcoders give a sparser description of the circuit than MLP neurons do. To
    do this, we computed the highest-variance layer 10 transcoder features and MLP10
    neurons. Then, for $1\leq k\leq 65$, we zero-ablated all but the top $k$ features
    in the transcoder/neurons in MLP10 and measured how this affected the model’s
    performance according to the mean probability difference metric presented in the
    original paper. We also evaluated the original model with respect to this metric,
    along with the model when MLP10 is replaced with the full transcoder.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转码器方法与神经元方法进行比较，以查看转码器是否提供了比MLP神经元更稀疏的电路描述。为此，我们计算了最高方差的第10层转码器特征和MLP10神经元。然后，对于$1\leq
    k\leq 65$，我们零化了转码器/MLP10中所有除前$k$个特征外的特征，并根据原论文中呈现的平均概率差异指标测量了这对模型性能的影响。我们还评估了原始模型与此指标的相关性，以及将MLP10替换为完整转码器后的模型。
- en: '![Refer to caption](img/6849b7e2260405f9c2450f3eaed2b6fb.png)![Refer to caption](img/caca7b32918f8a36cd44fa87804c05a9.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6849b7e2260405f9c2450f3eaed2b6fb.png)![参考说明](img/caca7b32918f8a36cd44fa87804c05a9.png)'
- en: 'Figure 5: Left: Performance according to the probability difference metric
    when all but the top $k$ features or neurons in MLP10 are zero-ablated. Right:
    The DLA and de-embedding score for tc10[5315], which contributed negatively to
    the transcoder’s performance.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：左侧：当MLP10中所有特征或神经元除前$k$个特征外都被零化时，根据概率差异指标的性能。右侧：tc10[5315]的DLA和去嵌入分数，它对转码器性能产生了负面影响。
- en: The results are shown in the left half of Figure [5](#S4.F5 "Figure 5 ‣ 4.3.2
    Comparison with neuronal approach ‣ 4.3 Analyzing the GPT2-small “greater-than”
    circuit ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable
    LLM Feature Circuits"). For fewer than 24 features, the transcoder approach outperforms
    the neuronal approach; its performance drops sharply, however, around this point.
    Further investigation revealed that tc10[5315], the 24th-highest-variance transcoder
    feature, was responsible for this drop in performance. The DLA for this feature
    is plotted in the right half of Figure [5](#S4.F5 "Figure 5 ‣ 4.3.2 Comparison
    with neuronal approach ‣ 4.3 Analyzing the GPT2-small “greater-than” circuit ‣
    4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable LLM Feature
    Circuits"). Notice how, in contrast with the three highest-variance transcoder
    features, tc10[5315] displays a flatter DLA, boosting all tokens equally. This
    might explain why it contributes to poor performance. To account for this, note
    that Figure [5](#S4.F5 "Figure 5 ‣ 4.3.2 Comparison with neuronal approach ‣ 4.3
    Analyzing the GPT2-small “greater-than” circuit ‣ 4 Circuit analysis with transcoders
    ‣ Transcoders Find Interpretable LLM Feature Circuits") also demonstrates the
    performance of the transcoder when this “bad feature” is removed.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在图[5](#S4.F5 "Figure 5 ‣ 4.3.2 Comparison with neuronal approach ‣ 4.3 Analyzing
    the GPT2-small “greater-than” circuit ‣ 4 Circuit analysis with transcoders ‣
    Transcoders Find Interpretable LLM Feature Circuits")的左半部分。对于少于24个特征的情况，转码器方法优于神经元方法；然而，性能在此点附近急剧下降。进一步调查发现，tc10[5315]，即第24个方差最高的转码器特征，导致了这种性能下降。该特征的DLA绘制在图[5](#S4.F5
    "Figure 5 ‣ 4.3.2 Comparison with neuronal approach ‣ 4.3 Analyzing the GPT2-small
    “greater-than” circuit ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find
    Interpretable LLM Feature Circuits")的右半部分。请注意，与方差最高的三个转码器特征相比，tc10[5315]显示出较平坦的DLA，均匀提升所有标记。这可能解释了为何它对性能造成了负面影响。为了解释这一点，请注意图[5](#S4.F5
    "Figure 5 ‣ 4.3.2 Comparison with neuronal approach ‣ 4.3 Analyzing the GPT2-small
    “greater-than” circuit ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find
    Interpretable LLM Feature Circuits")还展示了在去除这个“糟糕特征”后的转码器性能。
- en: 'While the transcoder does not recover the full performance of the original
    model, it needs only a handful of features to recover most of the original model’s
    performance; many more MLP neurons are needed to achieve the same level of performance.
    This suggests that the transcoder is particularly useful for obtaining a sparse,
    understandable approximation of MLP10. Furthermore, the transcoder features suggest
    a simple way that the MLP10 computation may (approximately) happen: by a small
    set of features that fire on years in certain ranges and boost the logits for
    the following years.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然转换器不能恢复原始模型的全部性能，但它只需要少量特征就能恢复大部分原始模型的性能；而要达到相同的性能水平，MLP神经元需要更多。这表明转换器特别适用于获取MLP10的稀疏、可理解的近似。此外，转换器特征暗示了MLP10计算可能（大致上）发生的简单方式：通过一小组在特定年份范围内激活的特征，并提升后续年份的logits。
- en: 5 Related work
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Circuit analysis is a common framework for exploring model internals [[7](#bib.bib7),
    [9](#bib.bib9), [10](#bib.bib10)]. A number of approaches exist to find circuits
    and meaningful components in models, including causal approaches [[32](#bib.bib32)],
    automated circuit discovery [[33](#bib.bib33)], and sparse probing [[16](#bib.bib16)].
    Causal methods include activation patching [[36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38)], attribution patching [[39](#bib.bib39), [40](#bib.bib40)],
    and path patching [[41](#bib.bib41), [8](#bib.bib8)]. Much circuit analysis work
    has focused on attention head circuits [[42](#bib.bib42)], including copying heads [[9](#bib.bib9)],
    induction heads [[11](#bib.bib11)], copy suppression [[43](#bib.bib43)], and successor
    heads [[44](#bib.bib44)]. Methods connecting circuit analysis to SAEs include He
    et al. [[45](#bib.bib45)], Batson et al. [[46](#bib.bib46)] and Marks et al. [[20](#bib.bib20)].
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 电路分析是探索模型内部的常用框架 [[7](#bib.bib7), [9](#bib.bib9), [10](#bib.bib10)]。存在多种方法用于发现模型中的电路和有意义的组件，包括因果方法 [[32](#bib.bib32)]，自动电路发现 [[33](#bib.bib33)]，以及稀疏探测 [[16](#bib.bib16)]。因果方法包括激活补丁 [[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38)]，归因补丁 [[39](#bib.bib39), [40](#bib.bib40)]，以及路径补丁 [[41](#bib.bib41),
    [8](#bib.bib8)]。许多电路分析工作集中于注意力头电路 [[42](#bib.bib42)]，包括复制头 [[9](#bib.bib9)]，诱导头 [[11](#bib.bib11)]，复制抑制 [[43](#bib.bib43)]，以及后继头 [[44](#bib.bib44)]。将电路分析与SAE连接的方法包括 He等人
    [[45](#bib.bib45)]，Batson等人 [[46](#bib.bib46)] 和 Marks等人 [[20](#bib.bib20)]。
- en: Sparse autoencoders have been used to disentangle model activations into interpretable
    features [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]. The development
    of SAEs was motivated by the theory of superposition in neural representations [[47](#bib.bib47)].
    Since then, much recent work has focused on exploring and interpreting SAEs, and
    connecting them to preexisting mechanistic interpretability techniques. Notable
    contributions include tools for exploring SAE features, such as SAE lens [[48](#bib.bib48)];
    applications of SAEs to attention sublayers [[27](#bib.bib27)]; scaling up SAEs
    to Claude 3 Sonnet [[49](#bib.bib49)], and improved SAE architectures [[50](#bib.bib50)].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏自编码器已被用于将模型激活分解为可解释的特征 [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]。SAE的发展受到了神经表征叠加理论的启发 [[47](#bib.bib47)]。此后，许多近期工作集中于探索和解释SAE，并将其与现有的机制可解释性技术联系起来。值得注意的贡献包括探索SAE特征的工具，如SAE镜头 [[48](#bib.bib48)]；SAE在注意力子层中的应用 [[27](#bib.bib27)]；将SAE扩展到Claude
    3 Sonnet [[49](#bib.bib49)]，以及改进的SAE架构 [[50](#bib.bib50)]。
- en: Transcoders have been originally proposed as a variant of SAEs under the names
    “predicting future activations”  [[23](#bib.bib23)] and “MLP stretchers” [[24](#bib.bib24)].
    More recently, concurrent work by Ge et al. [[51](#bib.bib51)] also investigated
    the use of transcoders (presented in their paper under the name of “skip-SAEs”)
    for performing circuit analysis, and applied them to understanding “bracketed
    text” features and the GPT2-small “indirect object identification” circuit first
    studied by Wang et al. [[8](#bib.bib8)]. Our contributions differ from those of
     Ge et al. [[51](#bib.bib51)] in a number of ways, including our factorization
    of attributions into input-dependent and input-invariant terms, our transcoder
    interpretability analysis, our investigation of the greater-than circuit, and
    our blind case studies that attempt to reverse-engineer unseen features. Furthermore,
    we evaluate the sparsity and accuracy of transcoders across multiple hyperparameters
    and larger models, whereas Ge et al. [[51](#bib.bib51)] evaluate multiple layers
    in GPT2-small at a single hyperparameter. Despite these differences, we find it
    exciting to see that the general utility of transcoders for mechanistic interpretability
    is being more broadly recognized.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 转码器最初被提出作为自编码器（SAEs）的变体，称为“预测未来激活”[[23](#bib.bib23)]和“MLP扩展器”[[24](#bib.bib24)]。最近，Ge等人[[51](#bib.bib51)]的并行工作也研究了转码器（在他们的论文中称为“跳跃-SAEs”）在电路分析中的应用，并将其应用于理解“括号文本”特征以及Wang等人[[8](#bib.bib8)]首次研究的GPT2-small“间接对象识别”电路。我们的贡献与Ge等人[[51](#bib.bib51)]的工作在多个方面有所不同，包括我们将归因分解为依赖输入和不依赖输入的项，我们的转码器可解释性分析，我们对“大于”电路的研究，以及我们试图逆向工程未知特征的盲目案例研究。此外，我们评估了转码器在多个超参数和更大模型上的稀疏性和准确性，而Ge等人[[51](#bib.bib51)]则在单一超参数下评估了GPT2-small的多个层。尽管存在这些差异，我们仍然很高兴看到转码器在机械可解释性方面的普遍实用性被更广泛地认可。
- en: 6 Conclusion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'Fine-grained circuit analysis requires an approach to handling MLP sublayers.
    To our knowledge, the transcoder-based circuit analysis method presented here
    is the only such approach that cleanly disentangles input-invariant information
    from input-dependent information. Importantly, transcoders bring these benefits
    without sacrificing fidelity and interpretability: when compared to state-of-the-art
    feature-level interpretability tools (SAEs), we find that transcoders achieve
    equal or better performance. We thus believe that transcoders are an improvement
    over other forms of feature-level interpretability tools for MLPs, such as SAEs
    on MLP outputs.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度电路分析需要一种处理MLP子层的方法。根据我们的知识，这里提出的基于转码器的电路分析方法是唯一能够将不依赖输入的信息与依赖输入的信息清晰分离的方法。重要的是，转码器在不牺牲保真性和可解释性的情况下带来了这些好处：与最先进的特征级可解释性工具（SAEs）相比，我们发现转码器的性能相当或更好。因此，我们认为转码器相比于其他形式的MLP特征级可解释性工具（如SAEs）是一种改进。
- en: Future work on transcoders includes directions such as comparing the features
    learned by transcoders to those learned by SAEs, seeing if there are classes of
    features that transcoders struggle to learn, finding interesting examples of novel
    circuits, and scaling circuit analysis to larger models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对转码器的未来研究方向包括比较转码器学到的特征与自编码器（SAEs）学到的特征，查看转码器是否在学习某些特征类别时遇到困难，寻找新颖电路的有趣例子，以及将电路分析扩展到更大的模型。
- en: Overall, we believe that transcoders are an exciting new development for circuit
    analysis and hope that they can continue to yield deeper insights into model behaviors.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们认为转码器是电路分析领域的一项令人兴奋的新发展，并希望它们能够继续深入揭示模型行为。
- en: Limitations.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 限制。
- en: Transcoders, like SAEs, are approximations to the underlying model, and the
    resulting error may lose key information. We find transcoders to be approximately
    as unfaithful to the model’s computations as SAEs are (as measured by the cross-entropy
    loss), although we leave comparing the errors to future work. Our circuit analysis
    method (App. [D.5](#A4.SS5 "D.5 Full circuit-finding algorithm ‣ Appendix D Detailed
    description of circuit analysis ‣ Transcoders Find Interpretable LLM Feature Circuits"))
    does not engage with how attention patterns are computed, and treats them as fixed.
    A promising direction of future work would be trying to extend transcoders to
    understand the computation of attention patterns, approximating the attention
    softmax. We only present circuit analysis results for a few qualitative case studies,
    and our results would be stronger with more systematic analysis.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 转码器，像 SAE 一样，是对基础模型的近似，结果误差可能丢失关键的信息。我们发现转码器对模型计算的忠实度与 SAE 相当（按交叉熵损失度量），虽然我们将比较误差的工作留给未来。我们的电路分析方法（附录
    [D.5](#A4.SS5 "D.5 完整电路发现算法 ‣ 附录 D 详细描述电路分析 ‣ 转码器发现可解释的 LLM 特征电路")）不涉及注意力模式是如何计算的，而是将其视为固定的。未来的一个有前景的方向是尝试扩展转码器以理解注意力模式的计算，近似注意力
    softmax。我们只展示了几个定性案例研究的电路分析结果，如果能有更系统的分析，结果将更有力。
- en: Broader impacts.
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更广泛的影响。
- en: This paper seeks to advance the field of mechanistic interpretability by contributing
    a new tool for circuit analysis. We see this as foundational research, and expect
    the impact to come indirectly from future applications of circuit analysis such
    as understanding and debugging unexpected model behavior and controlling and steering
    models to be more useful to users.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在通过提供一种新的电路分析工具来推动机制解释领域的发展。我们将这视为基础研究，期望其影响将间接来源于电路分析的未来应用，如理解和调试意外的模型行为以及控制和引导模型更好地服务用户。
- en: Acknowledgments and Disclosure of Funding
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢和资金披露
- en: Jacob and Philippe were funded by a grant from AI Safety Support Ltd. Jacob
    was additionally funded by a grant from the Long-Term Future Fund. Philippe was
    additionally funded by NSF GRFP grant DGE-2036197\. Compute was generously provided
    by Yale University.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Jacob 和 Philippe 的研究资金由 AI Safety Support Ltd 提供。Jacob 的研究还得到了 Long-Term Future
    Fund 的资助。Philippe 还获得了 NSF GRFP 资助，编号 DGE-2036197。计算资源由耶鲁大学慷慨提供。
- en: We would like to thank Andy Arditi, Lawrence Chan, and Matt Wearden for providing
    detailed feedback on our manuscript. We would also like to thank Senthooran Rajamanoharan
    and Juan David Gil for discussions during the research process, and Joseph Bloom
    for advice on how to use (and extend) the SAELens library. Finally, we would like
    to thank Joshua Batson for a discussion that inspired us to investigate transcoders
    in the first place.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Andy Arditi、Lawrence Chan 和 Matt Wearden 对我们手稿提供的详细反馈。我们还感谢 Senthooran
    Rajamanoharan 和 Juan David Gil 在研究过程中进行的讨论，以及 Joseph Bloom 对 SAELens 库的使用（和扩展）提供的建议。最后，我们感谢
    Joshua Batson 的讨论，他的讨论激发了我们最初研究转码器的兴趣。
- en: References
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language Models are Few-Shot Learners, July 2020. URL [http://arxiv.org/abs/2005.14165](http://arxiv.org/abs/2005.14165).
    arXiv:2005.14165 [cs].
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。语言模型是少量学习者，2020年7月。网址 [http://arxiv.org/abs/2005.14165](http://arxiv.org/abs/2005.14165)。arXiv:2005.14165
    [cs]。
- en: OpenAI et al. [2024] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute
    Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle
    Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth
    Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis
    Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario
    Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David
    Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica
    Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan
    Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, C. J. Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. GPT-4 Technical Report, March 2024. URL [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774).
    arXiv:2303.08774 [cs].
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人 [2024] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul
    Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine,
    Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine
    Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button,
    Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory
    Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby
    Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung,
    Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah
    Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,
    Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
    Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian
    Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon,
    Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei
    Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke,
    Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny
    Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang,
    Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn,
    Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish
    Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim,
    Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute
    Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael Pokorny, Michelle
    Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth
    Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis
    Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario
    Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David
    Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica
    Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan
    Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, C. J. Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk 和 Barret
    Zoph。GPT-4 技术报告，2024年3月。网址 [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774)。arXiv:2303.08774
    [cs]。
- en: 'Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team等 [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth等。Gemini：一系列高能力的多模态模型。*arXiv预印本
    arXiv:2312.11805*，2023年。
- en: 'Chrupała and Alishahi [2019] Grzegorz Chrupała and Afra Alishahi. Correlating
    neural and symbolic representations of language. In *Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics*, pages 2952–2962, 2019.
    doi: 10.18653/v1/P19-1283. URL [http://arxiv.org/abs/1905.06401](http://arxiv.org/abs/1905.06401).
    arXiv:1905.06401 [cs].'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chrupała和Alishahi [2019] Grzegorz Chrupała和Afra Alishahi。语言的神经和符号表示的相关性。见于*第57届计算语言学协会年会论文集*，第2952–2962页，2019年。doi:
    10.18653/v1/P19-1283。网址 [http://arxiv.org/abs/1905.06401](http://arxiv.org/abs/1905.06401)。arXiv:1905.06401
    [cs]。'
- en: Lipton [2017] Zachary C. Lipton. The mythos of model interpretability, 2017.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lipton [2017] Zachary C. Lipton。模型可解释性的神话，2017年。
- en: Chris Olah [2022] Chris Olah. Mechanistic Interpretability, Variables, and the
    Importance of Interpretable Bases, 2022. URL [https://transformer-circuits.pub/2022/mech-interp-essay/index.html](https://transformer-circuits.pub/2022/mech-interp-essay/index.html).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chris Olah [2022] Chris Olah。机制性可解释性、变量以及可解释基础的重要性，2022年。网址 [https://transformer-circuits.pub/2022/mech-interp-essay/index.html](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)。
- en: 'Olah et al. [2020] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh,
    Michael Petrov, and Shan Carter. Zoom In: An Introduction to Circuits. *Distill*,
    5(3):e00024.001, March 2020. ISSN 2476-0757. doi: 10.23915/distill.00024.001.
    URL [https://distill.pub/2020/circuits/zoom-in](https://distill.pub/2020/circuits/zoom-in).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Olah等 [2020] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael
    Petrov和Shan Carter。放大：电路入门。*Distill*，5(3):e00024.001，2020年3月。ISSN 2476-0757。doi:
    10.23915/distill.00024.001。网址 [https://distill.pub/2020/circuits/zoom-in](https://distill.pub/2020/circuits/zoom-in)。'
- en: 'Wang et al. [2022] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris,
    and Jacob Steinhardt. Interpretability in the Wild: a Circuit for Indirect Object
    Identification in GPT-2 small, November 2022. URL [http://arxiv.org/abs/2211.00593](http://arxiv.org/abs/2211.00593).
    arXiv:2211.00593 [cs].'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等 [2022] Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris和Jacob
    Steinhardt。野外的可解释性：GPT-2小型模型中的间接对象识别电路，2022年11月。网址 [http://arxiv.org/abs/2211.00593](http://arxiv.org/abs/2211.00593)。arXiv:2211.00593
    [cs]。
- en: Elhage et al. [2021] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan,
    Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
    Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,
    Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown,
    Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A Mathematical Framework
    for Transformer Circuits. *Transformer Circuits Thread*, 2021.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhage等 [2021] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas
    Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma,
    Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
    Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared
    Kaplan, Sam McCandlish和Chris Olah。变换器电路的数学框架。*变换器电路专帖*，2021年。
- en: Lieberum et al. [2023] Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda,
    Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does Circuit Analysis Interpretability
    Scale? Evidence from Multiple Choice Capabilities in Chinchilla, July 2023. URL
    [http://arxiv.org/abs/2307.09458](http://arxiv.org/abs/2307.09458). arXiv:2307.09458
    [cs].
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lieberum等 [2023] Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey
    Irving, Rohin Shah和Vladimir Mikulik。电路分析的可解释性是否可以扩展？来自Chinchilla的多项选择能力的证据，2023年7月。网址
    [http://arxiv.org/abs/2307.09458](http://arxiv.org/abs/2307.09458)。arXiv:2307.09458
    [cs]。
- en: Olsson et al. [2022] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph,
    Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom
    Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
    Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei,
    Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context
    Learning and Induction Heads, September 2022. URL [http://arxiv.org/abs/2209.11895](http://arxiv.org/abs/2209.11895).
    arXiv:2209.11895 [cs].
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olsson等 [2022] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph,
    Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom
    Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
    Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei,
    Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish和Chris Olah。上下文学习和归纳头，2022年9月。网址
    [http://arxiv.org/abs/2209.11895](http://arxiv.org/abs/2209.11895)。arXiv:2209.11895
    [cs]。
- en: 'Nanda et al. [2023] Neel Nanda, Senthooran Rajamanoharan, J\’anos Kram\’ar,
    and Rohin Shah. Fact Finding: Attempting to Reverse-Engineer Factual Recall on
    the Neuron Level, December 2023. URL [https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB](https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB).
    Publication Title: Alignment Forum.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nanda et al. [2023] Neel Nanda, Senthooran Rajamanoharan, János Kramár, 和 Rohin
    Shah. 事实发现：尝试在神经元层面逆向工程事实回忆，2023年12月。URL [https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB](https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB)。出版标题：Alignment
    Forum。
- en: 'Olah et al. [2017] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.
    Feature visualization. *Distill*, 2017. doi: 10.23915/distill.00007. https://distill.pub/2017/feature-visualization.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Olah et al. [2017] Chris Olah, Alexander Mordvintsev, 和 Ludwig Schubert. 特征可视化。*Distill*，2017年。doi:
    10.23915/distill.00007。 https://distill.pub/2017/feature-visualization。'
- en: Elhage et al. [2022a] Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda,
    Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben
    Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna
    Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion,
    Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli
    Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei,
    and Christopher Olah. Softmax linear units. *Transformer Circuits Thread*, 2022a.
    https://transformer-circuits.pub/2022/solu/index.html.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhage et al. [2022a] Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda,
    Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben
    Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna
    Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion,
    Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli
    Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei,
    和 Christopher Olah. Softmax线性单元。*Transformer Circuits Thread*，2022年。 https://transformer-circuits.pub/2022/solu/index.html。
- en: Bills et al. [2023] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman,
    Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders.
    Language models can explain neurons in language models, 2023. URL [https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bills et al. [2023] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman,
    Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, 和 William Saunders.
    语言模型可以解释语言模型中的神经元，2023年。URL [https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)。
- en: 'Gurnee et al. [2023] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey,
    Dmitrii Troitskii, and Dimitris Bertsimas. Finding Neurons in a Haystack: Case
    Studies with Sparse Probing, June 2023. URL [http://arxiv.org/abs/2305.01610](http://arxiv.org/abs/2305.01610).
    arXiv:2305.01610 [cs].'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gurnee et al. [2023] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey,
    Dmitrii Troitskii, 和 Dimitris Bertsimas. 在大海捞针中寻找神经元：稀疏探测的案例研究，2023年6月。URL [http://arxiv.org/abs/2305.01610](http://arxiv.org/abs/2305.01610)。arXiv:2305.01610
    [cs]。
- en: 'Bricken et al. [2023] Trenton Bricken, Adly Templeton, Joshua Batson, Brian
    Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda
    Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell,
    Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,
    Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah.
    Towards Monosemanticity: Decomposing Language Models With Dictionary Learning.
    *Transformer Circuits Thread*, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bricken et al. [2023] Trenton Bricken, Adly Templeton, Joshua Batson, Brian
    Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda
    Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell,
    Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,
    Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, 和 Christopher Olah. 朝向单一语义性：使用字典学习分解语言模型。*Transformer
    Circuits Thread*，2023年。
- en: Cunningham et al. [2023] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert
    Huben, and Lee Sharkey. Sparse Autoencoders Find Highly Interpretable Features
    in Language Models, October 2023. URL [http://arxiv.org/abs/2309.08600](http://arxiv.org/abs/2309.08600).
    arXiv:2309.08600 [cs].
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cunningham et al. [2023] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert
    Huben, 和 Lee Sharkey. 稀疏自编码器在语言模型中找到高度可解释的特征，2023年10月。URL [http://arxiv.org/abs/2309.08600](http://arxiv.org/abs/2309.08600)。arXiv:2309.08600
    [cs]。
- en: 'Yun et al. [2023] Zeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann LeCun.
    Transformer visualization via dictionary learning: contextualized embedding as
    a linear superposition of transformer factors, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yun et al. [2023] Zeyu Yun, Yubei Chen, Bruno A Olshausen, 和 Yann LeCun. 通过字典学习进行变换器可视化：上下文化嵌入作为变换器因子的线性叠加，2023年。
- en: 'Marks et al. [2024] Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov,
    David Bau, and Aaron Mueller. Sparse Feature Circuits: Discovering and Editing
    Interpretable Causal Graphs in Language Models, March 2024. URL [http://arxiv.org/abs/2403.19647](http://arxiv.org/abs/2403.19647).
    arXiv:2403.19647 [cs].'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marks et al. [2024] Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov,
    David Bau 和 Aaron Mueller. 《稀疏特征电路：发现和编辑语言模型中的可解释因果图》，2024年3月。网址 [http://arxiv.org/abs/2403.19647](http://arxiv.org/abs/2403.19647)。arXiv:2403.19647
    [cs]。
- en: 'Dunefsky et al. [2024] Jacob Dunefsky, Philippe Chlenski, Senthooran Rajamanoharan,
    and Neel Nanda. Case Studies in Reverse-Engineering Sparse Autoencoder Features
    by Using MLP Linearization, 2024. URL [https://www.alignmentforum.org/posts/93nKtsDL6YY5fRbQv](https://www.alignmentforum.org/posts/93nKtsDL6YY5fRbQv).
    Published: Alignment Forum.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dunefsky et al. [2024] Jacob Dunefsky, Philippe Chlenski, Senthooran Rajamanoharan
    和 Neel Nanda. 《通过使用 MLP 线性化逆向工程稀疏自编码器特征的案例研究》，2024。网址 [https://www.alignmentforum.org/posts/93nKtsDL6YY5fRbQv](https://www.alignmentforum.org/posts/93nKtsDL6YY5fRbQv)。发布于：Alignment
    Forum。
- en: Nanda [2023] Neel Nanda. Open source replication & commentary on anthropic’s
    dictionary learning paper. *Alignment Forum*, 2023. https://www.alignmentforum.org/posts/fKuugaxt2XLTkASkk.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nanda [2023] Neel Nanda. 对 Anthropic 的词典学习论文的开源复制与评论。*Alignment Forum*，2023。网址
    https://www.alignmentforum.org/posts/fKuugaxt2XLTkASkk。
- en: Templeton et al. [2024a] Adly Templeton, Joshua Batson, Adam Jermyn, and Chris
    Olah. Predicting Future Activations, January 2024a. URL [https://transformer-circuits.pub/2024/jan-update/index.html#predict-future](https://transformer-circuits.pub/2024/jan-update/index.html#predict-future).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Templeton et al. [2024a] Adly Templeton, Joshua Batson, Adam Jermyn 和 Chris
    Olah. 《预测未来激活》，2024年1月。网址 [https://transformer-circuits.pub/2024/jan-update/index.html#predict-future](https://transformer-circuits.pub/2024/jan-update/index.html#predict-future)。
- en: Li et al. [2023] Max Li, Sam Marks, and Aaron Mueller. dictionary_learning repository,
    2023. https://github.com/saprmarks/dictionary_learning?tab=readme-ov-file#extra-functionality-supported-by-this-repo.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023] Max Li, Sam Marks 和 Aaron Mueller. 《dictionary_learning 仓库》，2023。网址
    https://github.com/saprmarks/dictionary_learning?tab=readme-ov-file#extra-functionality-supported-by-this-repo。
- en: 'Hanna et al. [2024] Michael Hanna, Sandro Pezzelle, and Yonatan Belinkov. Have
    Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms,
    2024. _eprint: 2403.17806.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hanna et al. [2024] Michael Hanna, Sandro Pezzelle 和 Yonatan Belinkov. 《信任信度：超越电路重叠寻找模型机制》，2024。_eprint:
    2403.17806。'
- en: Bolukbasi et al. [2021] Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen,
    Emily Reif, Fernanda Viégas, and Martin Wattenberg. An interpretability illusion
    for bert. *arXiv preprint arXiv:2104.07143*, 2021.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolukbasi et al. [2021] Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen,
    Emily Reif, Fernanda Viégas 和 Martin Wattenberg. 《BERT 的可解释性幻觉》。*arXiv 预印本 arXiv:2104.07143*，2021。
- en: 'Kissane et al. [2024] Connor Kissane, Robert Krzyzanowski, Arthur Conmy, and
    Neel Nanda. Attention SAEs Scale to GPT-2 Small, 2024. URL [https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr](https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr).
    Published: Alignment Forum.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kissane et al. [2024] Connor Kissane, Robert Krzyzanowski, Arthur Conmy 和 Neel
    Nanda. 《注意力 SAEs 扩展到 GPT-2 Small》，2024。网址 [https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr](https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr)。发布于：Alignment
    Forum。
- en: Bloom [2024a] Joseph Bloom. Open Source Sparse Autoencoders for all Residual
    Stream Layers of GPT2-Small, 2024a. URL [https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD](https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bloom [2024a] Joseph Bloom. 《GPT2-Small 所有残差流层的开源稀疏自编码器》，2024a。网址 [https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD](https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD)。
- en: 'Biderman et al. [2023] Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der
    Wal. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,
    May 2023. URL [http://arxiv.org/abs/2304.01373](http://arxiv.org/abs/2304.01373).
    arXiv:2304.01373 [cs].'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biderman et al. [2023] Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika 和 Oskar van
    der Wal. 《Pythia：一个分析大型语言模型在训练和扩展中的工具套件》，2023年5月。网址 [http://arxiv.org/abs/2304.01373](http://arxiv.org/abs/2304.01373)。arXiv:2304.01373
    [cs]。
- en: Gokaslan and Cohen [2019] Aaron Gokaslan and Vanya Cohen. OpenWebText Corpus,
    2019.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gokaslan and Cohen [2019] Aaron Gokaslan 和 Vanya Cohen. 《OpenWebText 语料库》，2019。
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners.
    2019.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei 和 Ilya Sutskever. 《语言模型是无监督的多任务学习者》，2019。
- en: Geiger et al. [2021] Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher
    Potts. Causal Abstractions of Neural Networks, October 2021. URL [http://arxiv.org/abs/2106.02997](http://arxiv.org/abs/2106.02997).
    arXiv:2106.02997 [cs].
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geiger et al. [2021] Atticus Geiger, Hanson Lu, Thomas Icard, 和 Christopher
    Potts. 神经网络的因果抽象，2021 年 10 月。URL [http://arxiv.org/abs/2106.02997](http://arxiv.org/abs/2106.02997).
    arXiv:2106.02997 [cs]。
- en: Conmy et al. [2023] Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan
    Heimersheim, and Adrià Garriga-Alonso. Towards Automated Circuit Discovery for
    Mechanistic Interpretability, October 2023. URL [http://arxiv.org/abs/2304.14997](http://arxiv.org/abs/2304.14997).
    arXiv:2304.14997 [cs].
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conmy et al. [2023] Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan
    Heimersheim, 和 Adrià Garriga-Alonso. 面向机制解释的自动电路发现，2023 年 10 月。URL [http://arxiv.org/abs/2304.14997](http://arxiv.org/abs/2304.14997).
    arXiv:2304.14997 [cs]。
- en: Gandelsman et al. [2024] Yossi Gandelsman, Alexei A. Efros, and Jacob Steinhardt.
    Interpreting CLIP’s Image Representation via Text-Based Decomposition, March 2024.
    URL [http://arxiv.org/abs/2310.05916](http://arxiv.org/abs/2310.05916). arXiv:2310.05916
    [cs].
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gandelsman et al. [2024] Yossi Gandelsman, Alexei A. Efros, 和 Jacob Steinhardt.
    通过基于文本的分解解释 CLIP 的图像表示，2024 年 3 月。URL [http://arxiv.org/abs/2310.05916](http://arxiv.org/abs/2310.05916).
    arXiv:2310.05916 [cs]。
- en: 'Hanna et al. [2023] Michael Hanna, Ollie Liu, and Alexandre Variengien. How
    does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained
    language model, November 2023. URL [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586).
    arXiv:2305.00586 [cs].'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanna et al. [2023] Michael Hanna, Ollie Liu, 和 Alexandre Variengien. GPT-2
    如何计算大于？：解释预训练语言模型中的数学能力，2023 年 11 月。URL [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586).
    arXiv:2305.00586 [cs]。
- en: 'Zhang and Nanda [2024] Fred Zhang and Neel Nanda. Towards Best Practices of
    Activation Patching in Language Models: Metrics and Methods, January 2024. URL
    [http://arxiv.org/abs/2309.16042](http://arxiv.org/abs/2309.16042). arXiv:2309.16042
    [cs].'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang and Nanda [2024] Fred Zhang 和 Neel Nanda. 面向语言模型的最佳激活补丁实践：指标和方法，2024 年
    1 月。URL [http://arxiv.org/abs/2309.16042](http://arxiv.org/abs/2309.16042). arXiv:2309.16042
    [cs]。
- en: Vig et al. [2020] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian,
    Daniel Nevo, Yaron Singer, and Stuart Shieber. Investigating Gender Bias in Language
    Models Using Causal Mediation Analysis. In *Advances in Neural Information Processing
    Systems*, volume 33, pages 12388–12401\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vig et al. [2020] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian,
    Daniel Nevo, Yaron Singer, 和 Stuart Shieber. 使用因果中介分析研究语言模型中的性别偏差。发表于 *Neural
    Information Processing Systems 的进展*，第 33 卷，第 12388–12401 页。Curran Associates,
    Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html)。
- en: Heimersheim and Nanda [2024] Stefan Heimersheim and Neel Nanda. How to use and
    interpret activation patching. *arXiv preprint arXiv:2404.15255*, 2024.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heimersheim and Nanda [2024] Stefan Heimersheim 和 Neel Nanda. 如何使用和解释激活补丁。*arXiv
    预印本 arXiv:2404.15255*，2024 年。
- en: 'Neel Nanda [2024] Neel Nanda. Attribution Patching: Activation Patching At
    Industrial Scale, 2024. URL [https://www.neelnanda.io/mechanistic-interpretability/attribution-patching](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neel Nanda [2024] Neel Nanda. 归因补丁：工业规模下的激活补丁，2024 年。URL [https://www.neelnanda.io/mechanistic-interpretability/attribution-patching](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)。
- en: 'Kramár et al. [2024] János Kramár, Tom Lieberum, Rohin Shah, and Neel Nanda.
    AtP*: An efficient and scalable method for localizing LLM behaviour to components,
    2024. _eprint: 2403.00745.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kramár et al. [2024] János Kramár, Tom Lieberum, Rohin Shah, 和 Neel Nanda.
    AtP*: 一种高效且可扩展的本地化 LLM 行为到组件的方法，2024 年。_eprint: 2403.00745_。'
- en: Goldowsky-Dill et al. [2023] Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato,
    and Aryaman Arora. Localizing Model Behavior with Path Patching, May 2023. URL
    [http://arxiv.org/abs/2304.05969](http://arxiv.org/abs/2304.05969). arXiv:2304.05969
    [cs].
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldowsky-Dill et al. [2023] Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato,
    和 Aryaman Arora. 使用路径补丁本地化模型行为，2023 年 5 月。URL [http://arxiv.org/abs/2304.05969](http://arxiv.org/abs/2304.05969).
    arXiv:2304.05969 [cs]。
- en: 'Ferrando et al. [2024] Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and
    Marta R. Costa-jussà. A Primer on the Inner Workings of Transformer-based Language
    Models, May 2024. URL [http://arxiv.org/abs/2405.00208](http://arxiv.org/abs/2405.00208).
    arXiv:2405.00208 [cs] version: 2.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrando et al. [2024] Javier Ferrando, Gabriele Sarti, Arianna Bisazza, 和 Marta
    R. Costa-jussà. 关于基于 Transformer 的语言模型内部工作原理的入门指南，2024 年 5 月。URL [http://arxiv.org/abs/2405.00208](http://arxiv.org/abs/2405.00208).
    arXiv:2405.00208 [cs] 版本：2。
- en: 'McDougall et al. [2023] Callum McDougall, Arthur Conmy, Cody Rushing, Thomas
    McGrath, and Neel Nanda. Copy Suppression: Comprehensively Understanding an Attention
    Head. *ArXiv*, abs/2310.04625, 2023. URL [https://api.semanticscholar.org/CorpusID:263831290](https://api.semanticscholar.org/CorpusID:263831290).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDougall et al. [2023] 卡勒姆·麦克道格、亚瑟·康米、科迪·拉申、托马斯·麦格拉斯和尼尔·南达。《复制抑制：全面理解一个注意力头》。*ArXiv*，abs/2310.04625，2023年。网址
    [https://api.semanticscholar.org/CorpusID:263831290](https://api.semanticscholar.org/CorpusID:263831290)。
- en: 'Gould et al. [2023] Rhys Gould, Euan Ong, George Ogden, and Arthur Conmy. Successor
    Heads: Recurring, Interpretable Attention Heads In The Wild, December 2023. URL
    [http://arxiv.org/abs/2312.09230](http://arxiv.org/abs/2312.09230). arXiv:2312.09230
    [cs].'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gould et al. [2023] 瑞斯·古尔德、尤安·翁、乔治·奥格登和亚瑟·康米。《继任头：野外重复出现的可解释注意力头》，2023年12月。网址
    [http://arxiv.org/abs/2312.09230](http://arxiv.org/abs/2312.09230)。arXiv:2312.09230
    [cs]。
- en: 'He et al. [2024] Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun, Qinyuan
    Cheng, and Xipeng Qiu. Dictionary Learning Improves Patch-Free Circuit Discovery
    in Mechanistic Interpretability: A Case Study on Othello-GPT, February 2024. URL
    [http://arxiv.org/abs/2402.12201](http://arxiv.org/abs/2402.12201). arXiv:2402.12201
    [cs].'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2024] 郑福·何、徐阳·葛、琼·唐、天祥·孙、钦源·程和西鹏·邱。《字典学习在机械解释中的无补丁电路发现改进：以 Othello-GPT
    为例》，2024年2月。网址 [http://arxiv.org/abs/2402.12201](http://arxiv.org/abs/2402.12201)。arXiv:2402.12201
    [cs]。
- en: Batson et al. [2024] Joshua Batson, Brian Chen, and Andy Jones. Using features
    for easy circuit identification, 2024. URL [https://transformer-circuits.pub/2024/march-update/index.html#feature-heads](https://transformer-circuits.pub/2024/march-update/index.html#feature-heads).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Batson et al. [2024] 乔舒亚·巴特森、布赖恩·陈和安迪·琼斯。《利用特征进行简便电路识别》，2024年。网址 [https://transformer-circuits.pub/2024/march-update/index.html#feature-heads](https://transformer-circuits.pub/2024/march-update/index.html#feature-heads)。
- en: Elhage et al. [2022b] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas
    Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn
    Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin
    Wattenberg, and Christopher Olah. Toy Models of Superposition, September 2022b.
    URL [http://arxiv.org/abs/2209.10652](http://arxiv.org/abs/2209.10652). arXiv:2209.10652
    [cs].
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhage et al. [2022b] 纳尔逊·埃尔哈格、特里斯坦·休姆、凯瑟琳·奥尔森、尼古拉斯·谢弗、汤姆·亨尼根、肖娜·克拉维克、扎克·哈特菲尔德-多兹、罗伯特·拉森比、道恩·德雷恩、卡罗尔·陈、罗杰·格罗斯、萨姆·麦坎德利什、贾里德·卡普兰、大里奥·阿莫代伊、马丁·瓦滕伯格和克里斯托弗·奥拉赫。《叠加的玩具模型》，2022年9月。网址
    [http://arxiv.org/abs/2209.10652](http://arxiv.org/abs/2209.10652)。arXiv:2209.10652
    [cs]。
- en: Bloom [2024b] Joseph Bloom. SAELens Training, 2024b. URL [https://jbloomaus.github.io/SAELens/](https://jbloomaus.github.io/SAELens/).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bloom [2024b] 约瑟夫·布卢姆。《SAELens 训练》，2024b年。网址 [https://jbloomaus.github.io/SAELens/](https://jbloomaus.github.io/SAELens/)。
- en: 'Templeton et al. [2024b] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack
    Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen,
    Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid,
    C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn,
    Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting
    interpretable features from claude 3 sonnet. *Transformer Circuits Thread*, 2024b.
    URL [https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Templeton et al. [2024b] 阿德利·坦普尔顿、汤姆·科纳利、乔纳森·马库斯、杰克·林赛、特伦顿·布里肯、布赖恩·陈、亚当·皮尔斯、克雷格·西特罗、埃曼纽尔·阿梅森、安迪·琼斯、霍吉·坎宁安、尼古拉斯·L·特纳、卡勒姆·麦克道格、蒙特·麦克迪亚米德、C·丹尼尔·弗里曼、西奥多·R·萨默斯、爱德华·里斯、乔舒亚·巴特森、亚当·杰敏、尚·卡特、克里斯·奥拉赫和汤姆·亨尼根。《规模化单义性：从
    Claude 3 诗集提取可解释特征》。*Transformer Circuits Thread*，2024b年。网址 [https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)。
- en: Rajamanoharan et al. [2024] Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith,
    Tom Lieberum, Vikrant Varma, János Kramár, Rohin Shah, and Neel Nanda. Improving
    Dictionary Learning with Gated Sparse Autoencoders, April 2024. URL [http://arxiv.org/abs/2404.16014](http://arxiv.org/abs/2404.16014).
    arXiv:2404.16014 [cs].
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajamanoharan et al. [2024] 森图兰·拉贾马诺哈兰、亚瑟·康米、刘易斯·史密斯、汤姆·利伯姆、维克兰特·瓦尔玛、雅诺什·克拉马尔、罗欣·沙阿和尼尔·南达。《通过门控稀疏自编码器改进字典学习》，2024年4月。网址
    [http://arxiv.org/abs/2404.16014](http://arxiv.org/abs/2404.16014)。arXiv:2404.16014
    [cs]。
- en: Ge et al. [2024] Xuyang Ge, Fukang Zhu, Wentao Shu, Junxuan Wang, Zhengfu He,
    and Xipeng Qiu. Automatically identifying local and global circuits with linear
    computation graphs. *arXiv preprint arXiv:2405.13868*, 2024.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2024] 徐阳·葛、傅康·朱、温涛·舒、俊轩·王、郑福·何和西鹏·邱。《使用线性计算图自动识别局部和全局电路》。*arXiv 预印本
    arXiv:2405.13868*，2024年。
- en: Nanda and Bloom [2022] Neel Nanda and Joseph Bloom. TransformerLens, 2022. URL
    [https://github.com/TransformerLensOrg/TransformerLens](https://github.com/TransformerLensOrg/TransformerLens).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nanda 和 Bloom [2022] Neel Nanda 和 Joseph Bloom. TransformerLens, 2022. 网址 [https://github.com/TransformerLensOrg/TransformerLens](https://github.com/TransformerLensOrg/TransformerLens)。
- en: 'Dunefsky and Cohan [2024] Jacob Dunefsky and Arman Cohan. Observable propagation:
    Uncovering feature vectors in transformers. *arXiv preprint arXiv:2312.16291*,
    2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dunefsky 和 Cohan [2024] Jacob Dunefsky 和 Arman Cohan. Observable propagation:
    Uncovering feature vectors in transformers. *arXiv 预印本 arXiv:2312.16291*, 2024.'
- en: Appendix A Assets used
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 使用的资产
- en: 'Table 2: Assets used in preparing this paper, along with licenses and links'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：准备本文所使用的资产，包括许可和链接
- en: '| Asset type | Asset name | Link | License | Citation |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 资产类型 | 资产名称 | 链接 | 许可 | 引用 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Code | TransformerLens | [GitHub: TransformerLens](https://github.com/TransformerLensOrg/TransformerLens)
    | MIT | [[52](#bib.bib52)] |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 代码 | TransformerLens | [GitHub: TransformerLens](https://github.com/TransformerLensOrg/TransformerLens)
    | MIT | [[52](#bib.bib52)] |'
- en: '| Code | SAELens | [Github: SAELens](https://github.com/jbloomAus/SAELens)
    | MIT | [[48](#bib.bib48)] |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 代码 | SAELens | [Github: SAELens](https://github.com/jbloomAus/SAELens) |
    MIT | [[48](#bib.bib48)] |'
- en: '| Data | OpenWebText | [HuggingFace: OpenWebText](https://huggingface.co/datasets/Skylion007/openwebtext)
    | CC0-1.0 | [[30](#bib.bib30)] |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | OpenWebText | [HuggingFace: OpenWebText](https://huggingface.co/datasets/Skylion007/openwebtext)
    | CC0-1.0 | [[30](#bib.bib30)] |'
- en: '| Model | GPT2-small | [HuggingFace: GPT2](https://huggingface.co/openai-community/gpt2)
    | MIT | [[31](#bib.bib31)] |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GPT2-small | [HuggingFace: GPT2](https://huggingface.co/openai-community/gpt2)
    | MIT | [[31](#bib.bib31)] |'
- en: '| Model | Pythia-410M | [HuggingFace: Pythia-410M](https://huggingface.co/EleutherAI/pythia-410m)
    | Apache-2.0 | [[29](#bib.bib29)] |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pythia-410M | [HuggingFace: Pythia-410M](https://huggingface.co/EleutherAI/pythia-410m)
    | Apache-2.0 | [[29](#bib.bib29)] |'
- en: '| Model | Pythia-1.4B | [HuggingFace: Pythia-1.4B](https://huggingface.co/EleutherAI/pythia-1.4b)
    | Apache-2.0 | [[29](#bib.bib29)] |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pythia-1.4B | [HuggingFace: Pythia-1.4B](https://huggingface.co/EleutherAI/pythia-1.4b)
    | Apache-2.0 | [[29](#bib.bib29)] |'
- en: Appendix B Compute details
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 计算细节
- en: 'The most compute-intensive parts of the research presented in this work were
    training the SAEs and transcoders used in Section [3.2.2](#S3.SS2.SSS2 "3.2.2
    Quantitative comparison of transcoders to sparse autoencoders ‣ 3.2 Relationship
    to SAEs ‣ 3 Transcoders ‣ Transcoders Find Interpretable LLM Feature Circuits"),
    along with the set of GPT2-small transcoders used in Sections [4.2](#S4.SS2 "4.2
    Blind case study: reverse-engineering a feature ‣ 4 Circuit analysis with transcoders
    ‣ Transcoders Find Interpretable LLM Feature Circuits") and [4.3](#S4.SS3 "4.3
    Analyzing the GPT2-small “greater-than” circuit ‣ 4 Circuit analysis with transcoders
    ‣ Transcoders Find Interpretable LLM Feature Circuits"). Training all of these
    SAEs and transcoders involved GPUs. The SAEs and transcoders from Section [3.2.2](#S3.SS2.SSS2
    "3.2.2 Quantitative comparison of transcoders to sparse autoencoders ‣ 3.2 Relationship
    to SAEs ‣ 3 Transcoders ‣ Transcoders Find Interpretable LLM Feature Circuits")
    were trained on an internal cluster using an A100 GPU with 80 GB of VRAM. The
    VRAM used by each training run ranged from approximately 16 GB for the GPT2-small
    runs to approximately 60 GB for the Pythia-1.4B runs. The time taken by each training
    run ranged from approximately 30 minutes for the GPT2-small transcoders/SAEs to
    approximately 3.5 hours for the Pythia-1.4B runs.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究中计算最密集的部分是训练第 [3.2.2](#S3.SS2.SSS2 "3.2.2 Quantitative comparison of transcoders
    to sparse autoencoders ‣ 3.2 Relationship to SAEs ‣ 3 Transcoders ‣ Transcoders
    Find Interpretable LLM Feature Circuits") 节中使用的 SAEs 和转码器，以及第 [4.2](#S4.SS2 "4.2
    Blind case study: reverse-engineering a feature ‣ 4 Circuit analysis with transcoders
    ‣ Transcoders Find Interpretable LLM Feature Circuits") 和 [4.3](#S4.SS3 "4.3 Analyzing
    the GPT2-small “greater-than” circuit ‣ 4 Circuit analysis with transcoders ‣
    Transcoders Find Interpretable LLM Feature Circuits") 节中使用的 GPT2-small 转码器。训练这些
    SAEs 和转码器涉及使用 GPU。第 [3.2.2](#S3.SS2.SSS2 "3.2.2 Quantitative comparison of transcoders
    to sparse autoencoders ‣ 3.2 Relationship to SAEs ‣ 3 Transcoders ‣ Transcoders
    Find Interpretable LLM Feature Circuits") 节中的 SAEs 和转码器在一个内部集群上使用配备 80 GB VRAM
    的 A100 GPU 进行训练。每次训练运行使用的 VRAM 从 GPT2-small 运行的约 16 GB 到 Pythia-1.4B 运行的约 60 GB
    不等。每次训练运行所需的时间从 GPT2-small 转码器/SAEs 的大约 30 分钟到 Pythia-1.4B 运行的约 3.5 小时不等。'
- en: The transcoders that were trained on each layer of GPT2-small were trained using
    a cloud provider, with a similar amount of time and VRAM used per training run.
    For these transcoders, a hyperparameter sweep was performed that involved approximately
    200 training runs, which did not produce results used in the final paper.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层 GPT2-small 上训练的转码器是使用云服务提供商进行的，每次训练运行所用的时间和 VRAM 相似。对于这些转码器，进行了大约 200 次训练运行的超参数扫描，但未产生用于最终论文的结果。
- en: No significant amount of storage was used, as datasets were streamed during
    training.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 没有使用大量的存储，因为数据集在训练过程中是流式传输的。
- en: In addition to these training runs, our case studies were carried out on internal
    cluster nodes with GPUs. These case studies used no more than 6 GB of VRAM. The
    total amount of compute used during each case study is variable (depending on
    how in-depth one wants to investigate a case study), but is de minimis in comparison
    to the training runs. The same goes for the computation of top activating examples
    used in Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Blind interpretability comparison
    of transcoders to SAEs ‣ 3.2 Relationship to SAEs ‣ 3 Transcoders ‣ Transcoders
    Find Interpretable LLM Feature Circuits").
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些训练运行，我们的案例研究是在内部集群节点上的 GPU 上进行的。这些案例研究使用的 VRAM 不超过 6 GB。每个案例研究所用的计算总量是可变的（取决于对案例研究的深入调查程度），但与训练运行相比是微不足道的。对
    Section [3.2.1](#S3.SS2.SSS1 "3.2.1 转码器与 SAE 的盲解释性比较 ‣ 3.2 与 SAE 的关系 ‣ 3 转码器 ‣
    转码器寻找可解释的 LLM 特征电路") 中所用的顶级激活示例的计算也是如此。
- en: Appendix C SAE details
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C SAE 详细信息
- en: Sparse autoencoders (SAEs) are autoencoders trained to decompose a model’s activations
    at a given point into a sparse linear combination of feature vectors. As a hypothetical
    example, given the input “Sally threw the ball to me”, an SAE might decompose
    the model’s activations on the token me into a linear combination of a “personal
    pronoun” feature vector, an “indirect object” feature, and a “playing sports”
    feature—where all of these feature vectors are automatically learned by the SAE.
    An SAE’s architecture can be expressed as
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏自编码器（SAEs）是一种自编码器，旨在将模型在某个点的激活值分解为特征向量的稀疏线性组合。作为一个假设性的例子，给定输入“萨莉把球扔给我”，一个
    SAE 可能会将模型在 token “我” 上的激活值分解为一个“人称代词”特征向量，一个“间接宾语”特征，以及一个“运动”特征——所有这些特征向量都是由
    SAE 自动学习的。SAE 的结构可以表示为
- en: '|  | $\displaystyle\mathbf{z_{SAE}(\mathbf{x})}$ | $\displaystyle=\operatorname{ReLU}\left(\mathbf{W_{enc}}\mathbf{x}+\mathbf{b_{enc}}\right)$
    |  | (7) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{z_{SAE}(\mathbf{x})}$ | $\displaystyle=\operatorname{ReLU}\left(\mathbf{W_{enc}}\mathbf{x}+\mathbf{b_{enc}}\right)$
    |  | (7) |'
- en: '|  | $\displaystyle\operatorname{SAE}(\mathbf{x})$ | $\displaystyle=\mathbf{W_{dec}}\mathbf{z_{SAE}(\mathbf{x})}+\mathbf{b_{dec}},$
    |  | (8) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{SAE}(\mathbf{x})$ | $\displaystyle=\mathbf{W_{dec}}\mathbf{z_{SAE}(\mathbf{x})}+\mathbf{b_{dec}},$
    |  | (8) |'
- en: where $\mathbf{W_{enc}}\in\mathbb{R}^{d_{\text{features}}\times d_{\text{model}}}$,
    $\mathbf{W_{dec}}\in\mathbb{R}^{d_{\text{model}}\times d_{\text{features}}}$,
    $\mathbf{b_{enc}}\in\mathbb{R}^{d_{\text{features}}}$, $\mathbf{b_{dec}}\in\mathbb{R}^{d_{\text{model}}}$,
    $d_{\text{features}}$ is the number of feature vectors in the SAE, and $d_{\text{model}}$
    is the dimensionality of the model activations. Usually, $d_{\text{features}}$
    is far greater than $d_{\text{model}}$.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W_{enc}}\in\mathbb{R}^{d_{\text{features}}\times d_{\text{model}}}$，$\mathbf{W_{dec}}\in\mathbb{R}^{d_{\text{model}}\times
    d_{\text{features}}}$，$\mathbf{b_{enc}}\in\mathbb{R}^{d_{\text{features}}}$，$\mathbf{b_{dec}}\in\mathbb{R}^{d_{\text{model}}}$，$d_{\text{features}}$
    是 SAE 中特征向量的数量，$d_{\text{model}}$ 是模型激活值的维度。通常，$d_{\text{features}}$ 要远大于 $d_{\text{model}}$。
- en: Intuitively, Equation [7](#A3.E7 "In Appendix C SAE details ‣ Transcoders Find
    Interpretable LLM Feature Circuits") transforms the neuron activations $\mathbf{x}$
    into a sparse vector of SAE feature activations $\mathbf{z_{SAE}(\mathbf{x})}$.
    Each feature in an SAE is associated with an “encoder” vector (the $i$-th row
    of $\mathbf{W_{enc}}$) and a “decoder” vector (the $i$-th column of $\mathbf{W_{dec}}$).
    Equation [8](#A3.E8 "In Appendix C SAE details ‣ Transcoders Find Interpretable
    LLM Feature Circuits") then reconstructs the original activations as a linear
    combination of decoder vectors, weighted by the feature activations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，方程 [7](#A3.E7 "在附录 C SAE 详细信息 ‣ 转码器寻找可解释的 LLM 特征电路") 将神经元激活值 $\mathbf{x}$
    转换为 SAE 特征激活的稀疏向量 $\mathbf{z_{SAE}(\mathbf{x})}$。SAE 中的每个特征都与一个“编码器”向量（$\mathbf{W_{enc}}$
    的第 $i$ 行）和一个“解码器”向量（$\mathbf{W_{dec}}$ 的第 $i$ 列）相关联。方程 [8](#A3.E8 "在附录 C SAE 详细信息
    ‣ 转码器寻找可解释的 LLM 特征电路") 然后将原始激活值重构为解码器向量的线性组合，按特征激活值加权。
- en: The basic loss function on which SAEs are trained is
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: SAE 训练的基本损失函数是
- en: '|  | $\mathcal{L}_{SAE}(\mathbf{x})=\underbrace{\left\&#124;\mathbf{x}-\operatorname{SAE}(\mathbf{x})\right\&#124;^{2}_{2}}_{\text{reconstruction
    loss}}+\underbrace{\lambda_{1}\left\&#124;\mathbf{z_{SAE}(\mathbf{x})}\right\&#124;_{1}}_{\text{sparsity
    penalty}},$ |  | (9) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{SAE}(\mathbf{x})=\underbrace{\left\|\mathbf{x}-\operatorname{SAE}(\mathbf{x})\right\|^{2}_{2}}_{\text{重建损失}}+\underbrace{\lambda_{1}\left\|\mathbf{z_{SAE}(\mathbf{x})}\right\|_{1}}_{\text{稀疏性惩罚}},$
    |  | (9) |'
- en: where $\lambda_{1}$ is a hyperparameter and $\|\cdot\|_{1}$ denotes the $L_{1}$
    norm. The first term in the loss is the reconstruction loss associated with the
    SAE. The second term in the loss is a sparsity penalty, which approximately measures
    the number of features active on each input (the $L_{1}$ norm is used as a differentiable
    approximation of the $L_{0}$ “norm”). SAEs are thus pushed to reconstruct inputs
    accurately with a sparse number of features, with $\lambda_{1}$ controlling the
    accuracy-sparsity tradeoff. Empirically, the result of this is that SAEs learn
    to decompose model activations into highly interpretable features [[17](#bib.bib17)].
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1}$ 是一个超参数，$\|\cdot\|_{1}$ 表示 $L_{1}$ 范数。损失中的第一个项是与 SAE 相关的重建损失。损失中的第二个项是稀疏性惩罚，它大致衡量每个输入上激活的特征数量（$L_{1}$
    范数被用作 $L_{0}$ “范数”的可微分近似）。因此，SAE 被推动以稀疏的特征准确重建输入，$\lambda_{1}$ 控制准确性与稀疏性之间的权衡。经验上，这使得
    SAE 学会将模型激活分解为高度可解释的特征[[17](#bib.bib17)]。
- en: A standard method for quantitatively evaluating an SAE’s performance is as follows.
    To measure its sparsity, evaluate the mean number of features active on any given
    input (the mean $L_{0}$). To measure its accuracy, replace the original language
    model’s activations with the SAE’s reconstructed activations and measure the change
    in the language model’s loss (in this paper, this is the cross entropy loss for
    next token prediction).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 定量评估 SAE 性能的标准方法如下。为了测量其稀疏性，评估任何给定输入上激活的特征的平均数量（平均 $L_{0}$）。为了测量其准确性，将原语言模型的激活替换为
    SAE 重建的激活，并测量语言模型损失的变化（在本文中，这就是下一个 token 预测的交叉熵损失）。
- en: Appendix D Detailed description of circuit analysis
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 电路分析的详细描述
- en: D.1 Notation
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 符号说明
- en: $\mathbf{x^{(l,t)}_{pre}}$ denotes the hidden state for token $t$ at layer $l$
    before the attention sublayer.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{x^{(l,t)}_{pre}}$ 表示在注意力子层之前，第 $l$ 层中 token $t$ 的隐藏状态。
- en: $\mathbf{x^{(l,t)}_{mid}}$ denotes the hidden state for token $t$ at layer $l$
    before the MLP sublayer.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{x^{(l,t)}_{mid}}$ 表示在 MLP 子层之前，第 $l$ 层中 token $t$ 的隐藏状态。
- en: When we want to refer to the hidden state of the model for all tokens, we will
    do by omitting the token index, writing $\mathbf{x^{(l,1:t)}_{pre}}$ and $\mathbf{x^{(l,1:t)}_{mid}}$.
    These are matrices of size $\mathbb{R}^{d_{\text{model}}\times n_{\text{tokens}}}$,
    where $d_{\text{model}}$ is the dimensionality of model activation vectors and
    $n_{\text{tokens}}$ is the number of input tokens.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要引用模型中所有 token 的隐藏状态时，我们将通过省略 token 索引来表示，写作 $\mathbf{x^{(l,1:t)}_{pre}}$
    和 $\mathbf{x^{(l,1:t)}_{mid}}$。这些是大小为 $\mathbb{R}^{d_{\text{model}}\times n_{\text{tokens}}}$
    的矩阵，其中 $d_{\text{model}}$ 是模型激活向量的维度，$n_{\text{tokens}}$ 是输入 token 的数量。
- en: The MLP sublayer at layer $l$ is denoted by $\operatorname{MLP}^{(l)}(\cdot)$.
    Similarly, the transcoder for the layer $l$ MLP is denoted by $\operatorname{TC}^{(l)}(\cdot)$.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 第 $l$ 层的 MLP 子层表示为 $\operatorname{MLP}^{(l)}(\cdot)$。类似地，第 $l$ 层 MLP 的转码器表示为
    $\operatorname{TC}^{(l)}(\cdot)$。
- en: 'As for attention sublayers: following Elhage et al. [[9](#bib.bib9)], each
    attention sublayer can be decomposed into the sum of $n_{\text{heads}}$ independently-acting
    attention heads. Each attention head depends on the hidden states of all tokens
    in the input, but also distinguishes the token whose hidden state is to be modified
    by the attention head. Thus, the output of the layer $l$ attention sublayer for
    token $t$ is denoted $\sum_{\text{head $h$}}\operatorname{attn}^{(l,h)}\left(\mathbf{x^{(l,t)}_{pre}};\mathbf{x^{(l,1:t)}_{pre}}\right)$.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 至于注意力子层：参见 Elhage 等人 [[9](#bib.bib9)]，每个注意力子层可以分解为 $n_{\text{heads}}$ 个独立作用的注意力头的总和。每个注意力头依赖于输入中所有
    token 的隐藏状态，但也区分了那个隐藏状态将被注意力头修改的 token。因此，第 $l$ 层注意力子层对 token $t$ 的输出表示为 $\sum_{\text{head
    $h$}}\operatorname{attn}^{(l,h)}\left(\mathbf{x^{(l,t)}_{pre}};\mathbf{x^{(l,1:t)}_{pre}}\right)$。
- en: Each attention head can further be decomposed as a sum over “source” tokens.
    In particular, the output of layer $l$ attention head $h$ for token $t$ can be
    written as
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每个注意力头可以进一步分解为“源” tokens 的总和。特别地，第 $l$ 层注意力头 $h$ 对 token $t$ 的输出可以写作
- en: '|  | $\operatorname{attn}^{(l,h)}\left(\mathbf{x^{(l,t)}_{pre}};\mathbf{x^{(l,1:t)}_{pre}}\right)=\sum_{\text{source
    token $s$}}\operatorname{score}^{(l,h)}\left(\mathbf{x^{(l,t)}_{pre}},\mathbf{x^{(l,s)}_{pre}}\right)\mathbf{W_{OV}^{(l,h)}}\mathbf{x^{(l,s)}_{pre}}$
    |  | (10) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{attn}^{(l,h)}\left(\mathbf{x^{(l,t)}_{pre}};\mathbf{x^{(l,1:t)}_{pre}}\right)=\sum_{\text{源令牌
    $s$}}\operatorname{score}^{(l,h)}\left(\mathbf{x^{(l,t)}_{pre}},\mathbf{x^{(l,s)}_{pre}}\right)\mathbf{W_{OV}^{(l,h)}}\mathbf{x^{(l,s)}_{pre}}$
    |  | (10) |'
- en: Here, $\operatorname{score}^{(l,h)}:\mathbb{R}^{d_{\text{model}}\times d_{\text{model}}}\to\mathbb{R}$
    is a scalar “scoring” function that weights the importance of each source token
    to the destination token. Additionally, $\mathbf{W_{OV}^{(l,h)}}$ is a low-rank
    $\mathbb{R}^{d_{\text{model}}\times d_{\text{model}}}$ matrix that transforms
    the hidden state of each source token. $\operatorname{score}^{(l,h)}$ is often
    referred to as the “QK circuit” of attention and $\mathbf{W_{OV}^{(l,h)}}$ is
    often referred to as the “OV circuit” of attention.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\operatorname{score}^{(l,h)}:\mathbb{R}^{d_{\text{model}}\times d_{\text{model}}}\to\mathbb{R}$是一个标量“评分”函数，用于加权每个源令牌对目标令牌的重要性。此外，$\mathbf{W_{OV}^{(l,h)}}$是一个低秩$\mathbb{R}^{d_{\text{model}}\times
    d_{\text{model}}}$矩阵，用于转换每个源令牌的隐藏状态。$\operatorname{score}^{(l,h)}$通常被称为注意力的“QK电路”，而$\mathbf{W_{OV}^{(l,h)}}$则常被称为注意力的“OV电路”。
- en: D.2 Derivation of Equation [6](#S4.E6 "In 4.1.1 Attribution between feature
    pairs ‣ 4.1 Circuit analysis method ‣ 4 Circuit analysis with transcoders ‣ Transcoders
    Find Interpretable LLM Feature Circuits")
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 方程[6](#S4.E6 "在4.1.1 特征对之间的归因 ‣ 4.1 电路分析方法 ‣ 4 使用转码器的电路分析 ‣ 转码器找到可解释的LLM特征电路")的推导
- en: We want to understand what causes feature $i^{\prime}$ in the transcoder at
    layer $l^{\prime}$ to activate on token $t$. The activation of this feature is
    given by
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望了解在层$l^{\prime}$的转码器中，特征$i^{\prime}$在令牌$t$上激活的原因。该特征的激活由以下公式给出
- en: '|  | $\operatorname{ReLU}\left(\mathbf{f^{(l^{\prime},i^{\prime})}_{enc}}\cdot\mathbf{x^{(l^{\prime},t)}_{mid}}+b^{(l^{\prime},i^{\prime})}_{enc}\right),$
    |  | (11) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{ReLU}\left(\mathbf{f^{(l^{\prime},i^{\prime})}_{enc}}\cdot\mathbf{x^{(l^{\prime},t)}_{mid}}+b^{(l^{\prime},i^{\prime})}_{enc}\right),$
    |  | (11) |'
- en: where $\mathbf{f^{(l^{\prime},i^{\prime})}_{enc}}$ is the $i^{\prime}$-th row
    of $\mathbf{W_{enc}}$ for the layer $l^{\prime}$ transcoder and $b^{(l^{\prime},i^{\prime})}_{enc}$
    is the learned encoder bias for feature $i^{\prime}$ in the layer $l^{\prime}$
    transcoder. Therefore, if we ignore the constant bias term $b^{(l^{\prime},i^{\prime})}_{enc}$,
    then, assuming that this feature is active (which allows us to ignore the ReLU),
    the activation of feature $i^{\prime}$ depends solely on $\mathbf{f^{(l^{\prime},i^{\prime})}_{enc}}\cdot\mathbf{x^{(l^{\prime},t)}_{mid}}$.
    Because of residual connections in the transformer, $\mathbf{x^{(l^{\prime},t)}_{mid}}$
    can be decomposed as the sum of the outputs of all previous components in the
    model. For instance, in a two-layer model, if $\mathbf{x_{mid}^{(2,t)}}$ is the
    hidden state of the model right before the second MLP sublayer, then
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{f^{(l^{\prime},i^{\prime})}_{enc}}$是层$l^{\prime}$转码器中$\mathbf{W_{enc}}$的第$i^{\prime}$行，而$b^{(l^{\prime},i^{\prime})}_{enc}$是层$l^{\prime}$转码器中特征$i^{\prime}$的学习编码器偏置。因此，如果忽略常数偏置项$b^{(l^{\prime},i^{\prime})}_{enc}$，则假设该特征是激活的（这使我们可以忽略ReLU），特征$i^{\prime}$的激活仅依赖于$\mathbf{f^{(l^{\prime},i^{\prime})}_{enc}}\cdot\mathbf{x^{(l^{\prime},t)}_{mid}}$。由于变换器中的残差连接，$\mathbf{x^{(l^{\prime},t)}_{mid}}$可以分解为模型中所有先前组件输出的总和。例如，在一个两层模型中，如果$\mathbf{x_{mid}^{(2,t)}}$是模型在第二个MLP子层之前的隐藏状态，则
- en: '|  | $\mathbf{x_{mid}^{(2,t)}}=\sum_{h}\operatorname{attn}^{(2,h)}\left(\mathbf{x^{(2,t)}_{pre}};\mathbf{x^{(2,1:t)}_{pre}}\right)+\operatorname{MLP^{(1)}}\left(\mathbf{x_{mid}^{(1,t)}}\right)+\sum_{h}\operatorname{attn}^{(1,h)}\left(\mathbf{x^{(1,t)}_{pre}};\mathbf{x^{(1,1:t)}_{pre}}\right).$
    |  | (12) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x_{mid}^{(2,t)}}=\sum_{h}\operatorname{attn}^{(2,h)}\left(\mathbf{x^{(2,t)}_{pre}};\mathbf{x^{(2,1:t)}_{pre}}\right)+\operatorname{MLP^{(1)}}\left(\mathbf{x_{mid}^{(1,t)}}\right)+\sum_{h}\operatorname{attn}^{(1,h)}\left(\mathbf{x^{(1,t)}_{pre}};\mathbf{x^{(1,1:t)}_{pre}}\right).$
    |  | (12) |'
- en: Because of linearity, this means that the amount that $\operatorname{MLP^{(1)}}\left(\mathbf{x_{mid}^{(1,t)}}\right)$
    contributes to $\mathbf{f^{(2,i^{\prime})}_{enc}}\cdot\mathbf{x_{mid}^{(2,t)}}$
    is given by
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线性性，这意味着$\operatorname{MLP^{(1)}}\left(\mathbf{x_{mid}^{(1,t)}}\right)$对$\mathbf{f^{(2,i^{\prime})}_{enc}}\cdot\mathbf{x_{mid}^{(2,t)}}$的贡献量由下式给出
- en: '|  | $\mathbf{f^{(2,i^{\prime})}_{enc}}\cdot\operatorname{MLP^{(1)}}\left(\mathbf{x_{mid}^{(1,t)}}\right).$
    |  | (13) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{f^{(2,i^{\prime})}_{enc}}\cdot\operatorname{MLP^{(1)}}\left(\mathbf{x_{mid}^{(1,t)}}\right).$
    |  | (13) |'
- en: This is generally true for understanding the contribution of MLP $l$ to the
    activation of feature $i^{\prime}$ in transcoder $l^{\prime}$, whenever $l do     Initialize
    0$ 时     初始化 $\mathcal{P}_{next}\leftarrow\{\}$
    $\triangleright$ 这将包含下一次迭代的计算路径     对每个 $P\in\mathcal{P}$ 执行         将 $\mathbf{f_{cur}},l_{cur},t_{cur},a_{cur}$
    设置为 $P$ 的最后一个元素中的值         初始化 $\mathcal{A}\leftarrow\{\}$ $\triangleright$ 所有下层特征的归属集合         对每个在层
    $l$ 中的转码特征 $i$，其中 $l "ADVERTISEMENT Thanks" > "olog" NOT INTERPRETABLE.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 对于标记为“不可解释”的特征，提供了顶级激活示例。具体标注为“ Whats” > “ADVERTISEMENT Thanks” > “olog”
    不可解释。
- en: '![Refer to caption](img/dcbc1bef58a343d57e626aaca7ef1abb.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dcbc1bef58a343d57e626aaca7ef1abb.png)'
- en: (d) Top-activating examples for a feature annotated as “context-free”. The specific
    annotation was "oc" in middle of words single-token feature.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 对于标记为“无上下文”的特征，提供了顶级激活示例。具体标注为单词中间的“oc”单词特征。
- en: 'Figure 6: Examples of “feature-dashboards” used in the feature interpretation
    experiments.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在特征解释实验中使用的“特征仪表板”的示例。
- en: Appendix G Details on Section [4.3](#S4.SS3 "4.3 Analyzing the GPT2-small “greater-than”
    circuit ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable
    LLM Feature Circuits")
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 第[4.3节](#S4.SS3 "4.3 分析GPT2-small“greater-than”电路 ‣ 4 转码器电路分析 ‣ 转码器发现可解释LLM特征电路")的详细信息
- en: To obtain the de-embedding scores shown in Figures [4](#S4.F4 "Figure 4 ‣ 4.3.1
    Initial investigation ‣ 4.3 Analyzing the GPT2-small “greater-than” circuit ‣
    4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable LLM Feature
    Circuits") and [5](#S4.F5 "Figure 5 ‣ 4.3.2 Comparison with neuronal approach
    ‣ 4.3 Analyzing the GPT2-small “greater-than” circuit ‣ 4 Circuit analysis with
    transcoders ‣ Transcoders Find Interpretable LLM Feature Circuits"), the following
    method was used. First, we used the method presented in Appendix [D.3](#A4.SS3
    "D.3 Attribution through attention heads ‣ Appendix D Detailed description of
    circuit analysis ‣ Transcoders Find Interpretable LLM Feature Circuits") to determine
    which MLP0 transcoder features had the highest input-invariant connections to
    the given MLP10 transcoder feature through attention head 1 in layer 9\. Specifically,
    for MLP0 transcoder feature $i$ and MLP10 transcoder feature $j$, this attribution
    is given by $\left(\mathbf{f_{dec}^{(0,i)}}\right)^{T}\left(\mathbf{W_{OV}^{(9,1)}}\right)^{T}\mathbf{f_{enc}^{(10,j)}}$.
    For each MLP10 transcoder feature, the top ten MLP0 transcoder features were considered.
    Then, for each MLP0 transcoder feature, the de-embedding score of each YY token
    for that MLP0 feature was computed. The total de-embedding score of each YY token
    for an MLP10 feature was computed as the sum of the de-embedding scores of that
    token over the top ten MLP0 features, with each de-embedding score weighted by
    the input-invariant attribution of the MLP0 feature. In Figures [4](#S4.F4 "Figure
    4 ‣ 4.3.1 Initial investigation ‣ 4.3 Analyzing the GPT2-small “greater-than”
    circuit ‣ 4 Circuit analysis with transcoders ‣ Transcoders Find Interpretable
    LLM Feature Circuits") and [5](#S4.F5 "Figure 5 ‣ 4.3.2 Comparison with neuronal
    approach ‣ 4.3 Analyzing the GPT2-small “greater-than” circuit ‣ 4 Circuit analysis
    with transcoders ‣ Transcoders Find Interpretable LLM Feature Circuits"), the
    de-embedding scores were scaled and recentered in order to fit on the graph.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得图 [4](#S4.F4 "图 4 ‣ 4.3.1 初步调查 ‣ 4.3 分析 GPT2-small “大于”电路 ‣ 4 使用转码器的电路分析
    ‣ 转码器发现可解释的 LLM 特征电路") 和 [5](#S4.F5 "图 5 ‣ 4.3.2 与神经元方法比较 ‣ 4.3 分析 GPT2-small
    “大于”电路 ‣ 4 使用转码器的电路分析 ‣ 转码器发现可解释的 LLM 特征电路") 中所示的去嵌入分数，采用了以下方法。首先，我们使用了附录 [D.3](#A4.SS3
    "D.3 通过注意力头的归因 ‣ 附录 D 电路分析的详细描述 ‣ 转码器发现可解释的 LLM 特征电路") 中介绍的方法，确定哪些 MLP0 转码器特征通过第
    9 层的注意力头 1 与给定的 MLP10 转码器特征具有最高的输入不变连接。具体来说，对于 MLP0 转码器特征 $i$ 和 MLP10 转码器特征 $j$，该归因由
    $\left(\mathbf{f_{dec}^{(0,i)}}\right)^{T}\left(\mathbf{W_{OV}^{(9,1)}}\right)^{T}\mathbf{f_{enc}^{(10,j)}}$
    给出。对于每个 MLP10 转码器特征，考虑了前十个 MLP0 转码器特征。然后，对于每个 MLP0 转码器特征，计算了该 MLP0 特征的每个 YY 标记的去嵌入分数。计算每个
    YY 标记的总去嵌入分数作为该标记在前十个 MLP0 特征上的去嵌入分数之和，每个去嵌入分数按 MLP0 特征的输入不变归因加权。在图 [4](#S4.F4
    "图 4 ‣ 4.3.1 初步调查 ‣ 4.3 分析 GPT2-small “大于”电路 ‣ 4 使用转码器的电路分析 ‣ 转码器发现可解释的 LLM 特征电路")
    和 [5](#S4.F5 "图 5 ‣ 4.3.2 与神经元方法比较 ‣ 4.3 分析 GPT2-small “大于”电路 ‣ 4 使用转码器的电路分析 ‣
    转码器发现可解释的 LLM 特征电路") 中，去嵌入分数被缩放并重新中心化以适应图形。
- en: The mean probability difference metric discussed in the original greater-than
    work is as follows. Given the logits for each YY token, compute the softmax over
    these logits in order to obtain a probability distribution over the YY tokens;
    let $p_{y}$ denote the probability of the token corresponding to year $y$. Then,
    the probability difference for a given prompt containing a certain input year
    $y$ is given by . The mean probability
    difference is the mean of the probability differences over all 100 prompts.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 原始“大于”工作的均值概率差度量如下。给定每个 YY 标记的 logits，计算这些 logits 的 softmax，以获得 YY 标记的概率分布；令
    $p_{y}$ 表示与年份 $y$ 对应的标记的概率。然后，包含特定输入年份 $y$ 的给定提示的概率差由 
    给出。均值概率差是所有 100 个提示上的概率差的均值。
- en: Appendix H Full case studies
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 完整案例研究
- en: H.1 Classic blind case studies
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.1 经典盲目案例研究
- en: 'H.1.1 Citation feature: tc8[355]'
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.1.1 引用特征：tc8[355]
- en: First, we checked activations for the first 12,800 prompts in the training data.
    Using this, we identified the prompt indexed at $(5701,37)$ as one of 11 prompts
    for which tc8[355] activated above a score of 11.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查了训练数据中前 12,800 个提示的激活情况。通过这些，我们识别出索引为 $(5701,37)$ 的提示是 11 个提示中的一个，其中
    tc8[355] 的激活分数超过了 11。
- en: Path-based analysis on input index $(5701,37)$ revealed contributions from various
    tokens, notably attn7[7]@35 and attn5[6]@36. However, we first decided to focus
    on the current token.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对输入索引 $(5701,37)$ 的路径分析揭示了各种令牌的贡献，特别是 attn7[7]@35 和 attn5[6]@36。然而，我们首先决定专注于当前令牌。
- en: Current-token features.
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 当前令牌特性。
- en: 'Top de-embeddings for both tc0[9188] and tc0[16632] were all variants of a
    semicolon: ;, ’;, %;, and .;. We also checked tc6[11831]@-1 and found that its
    top contributing features from layer 0 were tc0[16632] and tc0[9188]: the same
    two semicolon features. On the basis of this, we concluded that the final token
    is a semicolon.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: tc0[9188] 和 tc0[16632] 的顶级去嵌入都是分号的变体：;、’;、%; 和 .;。我们还检查了 tc6[11831]@-1，发现其来自第
    0 层的主要贡献特性是 tc0[16632] 和 tc0[9188]：这两个分号特性。因此，我们得出结论，最终令牌是分号。
- en: Surname features.
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 姓氏特性。
- en: 'Next we focused on attn7[7]@35. Some interpretable features with high attributions
    through this component included tc0[13196]@36 (years), tc0[10109]@31 (open parentheses),
    mlp8tc[355]attn7[7]attn0[1]@35 (components of last names), tc0[12584]@32: P, and
    tc0[7659]@34: ck.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们专注于 attn7[7]@35。通过该组件具有高贡献的可解释特性包括 tc0[13196]@36（年份）、tc0[10109]@31（开放括号）、mlp8tc[355]attn7[7]attn0[1]@35（姓氏组成部分）、tc0[12584]@32:
    P 和 tc0[7659]@34: ck。'
- en: Input-independent investigation of tc6[21046]@35 revealed high contributions
    from tc0[16382] and tc0[5468]. tc0[16382] corresponded to tokens such as oglu,
    owski, and zyk; tc0[5468] corresponded to tokens such as Burnett, Hawkins, and
    MacDonald. Observing that all of these are (components of) surnames, we decided
    that token 35 was likely (part of) a surname.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对 tc6[21046]@35 的输入独立调查显示 tc0[16382] 和 tc0[5468] 的高贡献。tc0[16382] 对应的令牌如 oglu、owski
    和 zyk；tc0[5468] 对应的令牌如 Burnett、Hawkins 和 MacDonald。观察到这些都是（部分）姓氏，我们决定令牌 35 可能是（部分）姓氏。
- en: Repeating analysis with prompt $(6063,47)$.
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用提示 $(6063,47)$ 进行重复分析。
- en: Top attributions for this prompt once again identified tc0[9188], the semicolon
    feature from earlier. We filtered our computational paths to exclude this transcoder
    feature, since we already had a hypothesis about what it was doing. This identified
    tc0[10109]@39 and tc0[21019]@46 as top-contributing features.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 对此提示的主要归因再次识别了 tc0[9188]，即早期的分号特性。我们过滤了计算路径以排除此转码器特性，因为我们已经对其功能有了假设。这识别出了 tc0[10109]@39
    和 tc0[21019]@46 作为主要贡献特性。
- en: The top de-embedding tokens for tc0[10109]@39 were (, (=, and (~. On the basis
    of this, we determined that token 39 was likely an open parenthesis. Meanwhile,
    the top de-embedding tokens for tc0[21019]@46 were 1983, 1982, and 1981. This
    caused us to conclude that token 46 was likely a year.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: tc0[10109]@39 的顶级去嵌入令牌是（、(= 和 (~。基于此，我们确定令牌 39 可能是开放括号。同时，tc0[21019]@46 的顶级去嵌入令牌是
    1983、1982 和 1981。这使我们得出结论，令牌 46 可能是年份。
- en: 'We noted that, in the previous prompt, the attribution for the year features
    went through attn5[6], whereas on this prompt it went through attn2[9]. We decided
    to investigate the behavior of attn5[6] on this prompt, and found that it was
    attributing to features tc0[16542]@11, tc0[4205]@11, and tc0[19728]@11. The de-embedding
    results for these were mixed: tc0[16542] were both close-parenthesis features,
    whereas tc0[4205] included citation-related tokens like Accessed, Neuroscience,
    and Springer.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，在之前的提示中，年份特性的归因通过了 attn5[6]，而在此提示中则通过了 attn2[9]。我们决定调查 attn5[6] 在此提示中的行为，并发现它将归因于特性
    tc0[16542]@11、tc0[4205]@11 和 tc0[19728]@11。这些的去嵌入结果混合：tc0[16542] 都是右括号特性，而 tc0[4205]
    包括了像 Accessed、Neuroscience 和 Springer 这样的引用相关令牌。
- en: Final result.
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最终结果。
- en: We decided that tc8[355] was likely a semicolon-in-citations feature and looked
    at activating prompts. Top-activating prompts included “Res. 15, 241–247; 1978).
    In their paper, ”, “aythamah , 2382; Tahdhīb al-”, and “lesions (Poeck, 1969;
    Rinn, 1984). It”. Note that the last of these was prompt $(5701,37)$, i.e. the
    first case study we considered.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定 tc8[355] 可能是分号引用特性，并查看了激活提示。主要激活提示包括“Res. 15, 241–247; 1978). 在他们的论文中，”、“aythamah
    , 2382; Tahdhīb al-”以及“lesions (Poeck, 1969; Rinn, 1984). It”。请注意，最后一个提示是 $(5701,37)$，即我们考虑的第一个案例研究。
- en: In general, the top-activating features corroborated our hypothesis, and we
    did not find any unrelated prompts. We noticed that many of the top activating
    prompts had a comma before the year in citations, but our circuit analysis never
    identified a comma feature.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，主要激活特性证实了我们的假设，我们没有发现任何无关的提示。我们注意到许多主要激活提示在引用中年份前都有一个逗号，但我们的电路分析从未识别到逗号特性。
- en: We compared transcoder activations on the prompts “(Leisman, 1976;” and “(Leisman
    1976;” and found tc8[355] to activate almost identically for both when all preceding
    MLPs were replaced by transcoders (4.855 and 4.906, respectively) and on the original
    model (12.484 and 12.13, respectively).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了“(Leisman, 1976;”和“(Leisman 1976;”的转码器激活情况，发现 tc8[355] 对两者的激活几乎完全相同，当所有前面的
    MLP 被转码器替换后，分别为 4.855 和 4.906，而在原始模型中，分别为 12.484 和 12.13。
- en: 'H.1.2 “Caught” feature: tc8[235].'
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.1.2 “Caught” 特征：tc8[235]。
- en: First, we checked activations for the first 12,800 prompts in the training data.
    Using this, we identified prompt (8531, 111) as one of 13 prompts for which tc8[235]
    activated above a score of 11.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查了训练数据中前 12,800 个提示的激活情况。通过这些数据，我们识别出提示 (8531, 111) 是 13 个 tc8[235] 激活分数超过
    11 的提示之一。
- en: Input $(8531,111)$.
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入 $(8531,111)$。
- en: 'Path analysis revealed that this feature almost exclusively depends on the
    final token in the input. Input-independent connections to the top-contributing
    transcoder feature, tc7[14382], revealed the layer-0 transcoder features tc0[1636]
    (de-embeddings: caught, aught) tc0[5637] (de-embeddings: captured, caught), tc0[3981]
    (catch, catch) as top contributors.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 路径分析揭示了这一特征几乎完全依赖于输入中的最终标记。对顶级贡献转码器特征 tc7[14382] 的输入无关连接显示，层-0 转码器特征 tc0[1636]（去嵌入：caught,
    aught）、tc0[5637]（去嵌入：captured, caught）、tc0[3981]（catch, catch）是主要贡献者。
- en: Inputs $(6299,39)$ and $(817,63)$.
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入 $(6299,39)$ 和 $(817,63)$。
- en: For input $(6299,39)$, we again saw top computational paths depended mostly
    on the final token. This time, we identified tc7[14382] and tc0[1636]—both of
    which were already identified for the previous prompt—as top contributors.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入 $(6299,39)$，我们再次观察到顶级计算路径主要依赖于最终标记。这次，我们识别了 tc7[14382] 和 tc0[1636]——这两个特征已经在前一个提示中被识别为顶级贡献者。
- en: For input $(6299,39)$ we also observed the same pattern. This caused us to hypothesize
    that this feature fires on past-tense synonyms of “to catch.”
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入 $(6299,39)$，我们也观察到了相同的模式。这使我们推测该特征在“to catch”的过去时同义词上激活。
- en: Final result.
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最终结果。
- en: Top activating prompts for this feature were all forms of “caught,” but the
    various synonyms, such as “uncovered,” were nowhere to be found.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一特征的顶级激活提示均为“caught”的各种形式，但诸如“uncovered”等不同同义词却没有出现。
- en: “Caught” as participle.
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: “Caught” 作为分词。
- en: Additionally, we noticed that “caught” was used as a participle rather than
    a finite verb in all top-activating examples. To explore this, we investigated
    the difference in activations between the prompts “He was caught” and “He caught
    the ball”, and found that the former caused tc8[235] to activate strongly (19.97)
    whereas the latter activated very weakly (0.8145).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们注意到在所有顶级激活示例中，“caught”被用作分词而非有限动词。为探讨这一点，我们调查了提示“他被抓住了”和“他抓住了球”之间的激活差异，发现前者使
    tc8[235] 强烈激活（19.97），而后者激活非常弱（0.8145）。
- en: 'When we tested the same prompts while replacing all preceding MLPs with transcoders,
    we found the difference much less stark: 16.45 for “He was caught” and 9.00 for
    “He caught the ball”. This suggests that transcoders were not accurately modeling
    this particular nuance of the feature behavior.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们测试相同的提示并用转码器替换所有前面的 MLP 时，发现差异明显减少：“他被抓住了”为 16.45，“他抓住了球”为 9.00。这表明转码器未能准确建模这一特征行为的特定细微差别。
- en: Finally, we checked top paths for contributions through the was token on the
    prompt “He was caught” to see whether we could find anything related to this nuance
    in our circuits. This analysis revealed attn1[0]@2 as important, and were able
    to discover mild attributions to transcoder features whose top de-embeddings were
    was and related tokens.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们检查了通过提示“他被抓住了”中的 was 标记的顶级路径，看看是否能在电路中找到与这一细微差别相关的内容。这项分析揭示了 attn1[0]@2
    是重要的，并发现了对顶级去嵌入为 was 和相关标记的转码器特征的轻微归因。
- en: H.2 Restricted blind case studies
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.2 限制盲目案例研究
- en: Beyond a simple blind case study, we carried out a number of “restricted blind
    case studies.” In these, all of the rules of a regular blind case study apply,
    and additionally it is prohibited to look at input-dependent information about
    layer-0 transcoder features.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简单的盲目案例研究外，我们还进行了若干“限制盲目案例研究”。在这些研究中，所有常规盲目案例研究的规则适用，此外，还禁止查看关于层-0 转码器特征的输入依赖信息。
- en: Since layer 0 features are more commonly single-token features, and in general
    there is almost no contextual information available for the MLP yet, layer 0 features
    tend to be substantially more informative about the tokens in the prompt than
    features in other layers are. Thus, it is often possible to reconstruct large
    portions of the prompt just from the de-embeddings of which layer 0 transcoder
    features are active—and, although we never look at these activations directly,
    they are frequently revealed and analyzed as part of active computational graphs
    leading to some downstream feature.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第0层特征更常见为单标记特征，并且一般来说，目前MLP几乎没有上下文信息，第0层特征通常比其他层的特征更能提供关于提示的有用信息。因此，通常可以仅通过解嵌重建提示的大部分内容——虽然我们从未直接查看这些激活，但它们常常作为主动计算图的一部分被揭示和分析，导致一些下游特征。
- en: 'By omitting input-dependent information about layer 0 features from our analysis,
    we must rely more on circuit-level information, and remain substantially more
    ignorant of the prompts for activating examples. Note that input-independent information
    about layer 0 features can still be used: for instance, we can look at top input-independent
    connections to layer 0 features, and the de-embeddings for those as well—at the
    expense of not knowing whether those features are active or not.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从我们的分析中省略有关第0层特征的输入依赖信息，我们必须更多地依赖电路级信息，并且对激活示例的提示保持相对较大的无知。注意，第0层特征的输入独立信息仍然可以使用：例如，我们可以查看到第0层特征的顶级输入独立连接，以及它们的解嵌——尽管这样会失去是否这些特征处于激活状态的信息。
- en: 'H.2.1 Local context feature: tc8[479].'
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.2.1 本地上下文特征：tc8[479]。
- en: Our first example of a blind case study follows tc8[479], which we fail to correctly
    annotate through circuit analysis. We include this case study for transparency,
    and as an instructive example of how things can go awry during blind case studies.
    First, we measured feature activations over 12,800 prompts and identified 6 prompts
    that activated above a threshold of 10.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个盲例研究的例子参考了tc8[479]，在电路分析中未能正确注释。我们包括这个案例研究是为了透明度，并作为一个示例，说明在盲例研究中可能出现的错误。首先，我们测量了12,800个提示的特征激活，并识别出6个提示的激活值高于10的阈值。
- en: Input $(3511,64)$.
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入 $(3511,64)$。
- en: 'For this prompt, path analysis revealed a lot of attention head involvement
    from many previous tokens. For our first analysis, we chose the path mlp8tc[479]@-1
    <- attn8[5]@62: 8.1 <- mlp7tc[10719]@62, since we could look at de-embeddings
    for tc7[10719]@62. Top input-independent connections from tc7[10719]@62 to layer
    0 were tc0[22324] and tc0[2523], which had estimated and estimate as their top
    de-embeddings, respectively. Thus, we hypothesized that token 62 is “estimate(d)”.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '对于这个提示，路径分析揭示了许多来自先前标记的注意力头的参与。在我们的第一次分析中，我们选择了路径mlp8tc[479]@-1 <- attn8[5]@62:
    8.1 <- mlp7tc[10719]@62，因为我们可以查看tc7[10719]@62的解嵌。从tc7[10719]@62到第0层的顶级输入独立连接是tc0[22324]和tc0[2523]，它们的顶级解嵌分别是estimated和estimate。因此，我们假设标记62是“estimate(d)”。'
- en: 'Next, we looked at the pullback of tc8[479] through attn8[5] through attn7[5]@57.
    This revealed top input-independnet connections to tc0[23855] (top de-embedding
    tokens: spree, havoc, frenzy), tc0[8917] (took de-embedding tokens: amounts, quantities,
    amount), and tc0[327] (massive, massive, huge). We found this aspect of the analysis
    to be inconclusive.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们查看了通过attn8[5]到attn7[5]@57的tc8[479]的回溯。这揭示了与tc0[23855]（顶级解嵌标记：spree, havoc,
    frenzy）、tc0[8917]（解嵌标记：amounts, quantities, amount）和tc0[327]（massive, massive,
    huge）之间的顶级输入独立连接。我们发现这个分析方面没有结论。
- en: The pullback of tc8[479] through attn8[5] through attn6[11]@57 revealed connections
    to tc0[13184] (total), tc0[12266] ( comparable), and tc0[12610] ( averaging).
    This led us to believe that token 57 relates to quantities.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 通过attn8[5]到attn6[11]@57的tc8[479]的回溯揭示了与tc0[13184]（total）、tc0[12266]（comparable）和tc0[12610]（averaging）之间的连接。这使我们相信标记57与数量有关。
- en: We found that tc3[18655] was a top transcoder feature active on the current
    token. This showed top input-independent connections to tc0[11334] and tc0[5270],
    both of which de-embedded as be. This led us to hypothesize that tc8[479] features
    on phrases like “the amount/total/average is estimated to be…”.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现tc3[18655]是当前标记上最活跃的转码器特征。这显示出与tc0[11334]和tc0[5270]之间的顶级输入独立连接，这两个特征都解嵌为be。这使我们假设tc8[479]特征出现在像“amount/total/average
    is estimated to be…”这样的短语中。
- en: Input $(668,122)$.
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入 $(668,122)$。
- en: For this prompt, most contributions once again came from previous tokens. The
    top contributor was attn8[5]@121, which had input-independet connections to tc0[12151]
    ( airport), tc0[8192] (pired), tc0[13184] (total), and tc0[1300] ( resulted).
    This was inconclusive, but this is the second time that tc0[13184] has appeared
    in de-embeddings.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个提示，大部分贡献再次来自之前的标记。主要贡献者是 attn8[5]@121，它与 tc0[12151]（机场）、tc0[8192]（pired）、tc0[13184]（总计）和
    tc0[1300]（结果）有输入无关的连接。这没有得出明确结论，但这是 tc0[13184] 第二次出现在去嵌入中。
- en: 'Next, we investigated attn8[7]@121: it connected to tc0[16933] ( population),
    tc0[14006] (kinson, rahim, LU, …), tc0[19887] ( blacks), and tc0[6821] ( crowds).
    These seemed related to groups of people, but this analysis was also inconclusive.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调查了 attn8[7]@121：它连接到 tc0[16933]（人口）、tc0[14006]（kinson, rahim, LU, ...）、tc0[19887]（黑人）和
    tc0[6821]（人群）。这些似乎与人群相关，但这一分析也没有得出结论。
- en: When we investigated tc4[18899]@121, top input-idependent connections to layer-0
    features included tc0[22324], which de-embedded to estimated again. This was more
    consistent with the behavior on the previous prompt.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调查 tc4[18899]@121 时，层-0 特征的主要输入无关连接包括 tc0[22324]，这个特征再次被去嵌入为估计的。这与之前提示的行为更一致。
- en: To understand the current-token behavior, we looked at tc7[13166]@-1. Top input-indendent
    connections were tc0[18204] ( discrepancy) and tc0[14717] ( velocity). tc1[19616]@-1
    and tc3[22544]-1, both of which also contributed, each had top connections to
    tc0[19815] ( length). This led us to guess that this prompt relates to estimated
    length.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解当前标记的行为，我们查看了 tc7[13166]@-1。主要的输入无关连接是 tc0[18204]（差异）和 tc0[14717]（速度）。tc1[19616]@-1
    和 tc3[22544]-1，也都贡献了，每个都有与 tc0[19815]（长度）的主要连接。这使我们猜测这个提示与估计长度有关。
- en: Next, we looked at previous tokens. One feature, tc5[10350]@119, was connected
    to tc0[23607] and tc0[4252], both of which de-embedded to variants of With. For
    the next token, tc6[15690]@120 was connected to tc0[22463] and tc0[18052] (both
    a). This updated our hypothesis to something like “with an estimated length.”
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们查看了之前的标记。一个特征，tc5[10350]@119，与 tc0[23607] 和 tc0[4252] 连接，这些都被去嵌入为 With
    的变体。对于下一个标记，tc6[15690]@120 连接到 tc0[22463] 和 tc0[18052]（均为 a）。这更新了我们的假设为“具有估计长度”。
- en: 'Further back in the prompt, we saw tc4[23257]@29 (connected to tc0[12475]:
    remaining, tc0[16996]: entirety).'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示的更早部分，我们看到 tc4[23257]@29（连接到 tc0[12475]：剩余的，tc0[16996]：全部）。
- en: Input $(7589,89)$.
  id: totrans-337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入 $(7589,89)$。
- en: One feature, tc7[6]@87, pulled back to tc0[22324], which de-embedded to estimated.
    A following-token feature, tc1[14473], pulled back to tc0[4746] ( annual, yearly),
    and the next-token feature tc1[12852]@89, pulled back to tc0[923] ( revenue).
    Thus, this prompt seemed to end in “estimated yearly revenue.”
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特征，tc7[6]@87，回溯到 tc0[22324]，这个特征被去嵌入为估计的。一个接下来的标记特征，tc1[14473]，回溯到 tc0[4746]（年度，按年），以及下一个标记特征
    tc1[12852]@89，回溯到 tc0[923]（收入）。因此，这个提示似乎以“估计的年度收入”结束。
- en: 'Estimates for earlier tokens included tc4[23699]@85 (tc0[10924]: with), tc5[6568]@86
    (tc0[1595]: a). This matched the pattern from earlier, where we expected a prompt
    like “with an estimated length”—but now we expect “with an estimated annual revenue.”'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 早期标记的估计包括 tc4[23699]@85（tc0[10924]：与），tc5[6568]@86（tc0[1595]：a）。这与之前的模式一致，我们期望提示类似“具有估计长度”—但现在我们期望“具有估计的年度收入。”
- en: Looking at the pulled-back feature mlp8tc[479]attn3[2]@86, none of the connections
    we found to be very informative. This is consistent with patterns observed in
    other case studies, where pullbacks through attention tended to be harder to interpret.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 查看被回溯的特征 mlp8tc[479]attn3[2]@86，我们发现的连接没有很有用的信息。这与其他案例研究中观察到的模式一致，其中通过注意力的回溯往往更难解释。
- en: Final guess.
  id: totrans-341
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最终猜测。
- en: On the basis of the above examples, we guessed that this feature fires on prompts
    like “with a total estimated…”. When we viewed top activating examples, we found
    a number of examples that matched this pattern, especially among the highest total
    activations. However, for many of the lowest-activation prompts we saw quite different
    behaviors. Activating prompts revealed that this is a local context feature, which
    in retrospect may have been apparent through the very high levels of attention
    head involvement in all circuits we analyzed.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述示例，我们猜测这个特征在类似“总估计……”的提示上激活。当我们查看主要激活示例时，我们发现了一些匹配这一模式的示例，尤其是在总激活最高的情况。然而，对于许多最低激活提示，我们看到的行为却截然不同。激活的提示表明这是一个局部上下文特征，回顾起来可能通过我们分析的所有电路中非常高的注意力头参与度得以显现。
- en: 'H.2.2 Single-token All feature: tc8[1447]'
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.2.2 单标记所有特征：tc8[1447]
- en: An analysis of the first 12,800 prompts revealed 21 features activating above
    a threshold of 11. One of these was input $(3067,79)$. The computational paths
    for this prompt revealed all contributions came from the final token.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 对前12,800个提示的分析揭示了21个特性在11的阈值以上激活。其中之一是输入 $(3067,79)$。该提示的计算路径揭示了所有贡献都来自最终标记。
- en: The top attribution was due to tc7[10932], with a top input-independent connection
    to tc0[4012], which de-embedded to All. The next-highest was tc6[8713], which
    connected to tc0[6533], which de-embedded to All (note the leading space). These
    observations led us to hypothesize this is probably a simple, single-token feature
    for “All.”
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的归因是 tc7[10932]，与 tc0[4012] 具有顶级输入独立连接，后者解嵌到 All。次高的是 tc6[8713]，它与 tc0[6533]
    连接，后者解嵌到 All（注意前导空格）。这些观察使我们假设这可能是一个简单的单标记特性，用于“所有”。
- en: We also looked at context-based contributions by filtering out current-token
    features, and found the top attributions to max out at 0.23 (compared to 3.5 from
    tc7[10932]@79). This was quite low, indicating context was probably not very important.
    Nevertheless, we explored the pullback of tc8[1447] through the OV circuit of
    attn4[11]@78 and discovered several seemingly-unrelated connections with low attributions.
    When we pulled back through the OV circuit of attn1[1]@78 and attn2[0]@78, both
    showed input-independnt connections to features that de-embedded as punctuation
    tokens. Overall, the context seemed to contribute little, except to suggest that
    there may be punctuation preceding this instance of All.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过过滤掉当前标记特性来查看基于上下文的贡献，发现顶级归因的最大值为0.23（与 tc7[10932]@79 的 3.5 相比）。这相当低，表明上下文可能不是很重要。然而，我们通过
    attn4[11]@78 的 OV 电路回溯了 tc8[1447]，发现几个似乎无关的连接具有较低的归因。当我们通过 attn1[1]@78 和 attn2[0]@78
    的 OV 电路回溯时，都显示出与解嵌为标点符号的特性的输入独立连接。总体来看，上下文似乎贡献不大，只是暗示可能在这个 All 实例之前有标点符号。
- en: 'We repeated this analysis with another input, $(8053,72)$, and found the same
    features contributing: tc7[10932], followed by tc6[8713]. This led us to conclude
    this is a single-token “All” feature. Top activating examples confirmed this:
    the feature activated most highly for All, then All, and finally all. Overall,
    this feature turned out to be quite straightforward, and it was easy to understand
    its function purely from transcoder circuits.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用另一个输入 $(8053,72)$ 重复了这个分析，发现相同的特性有贡献：tc7[10932]，然后是 tc6[8713]。这使我们得出结论，这是一个单标记的“所有”特性。顶级激活示例证实了这一点：该特性对
    All 的激活最高，然后是 All，最后是 all。总体来看，这个特性非常直接，通过转码电路很容易理解其功能。
- en: 'H.2.3 Interview feature: tc8[6569]'
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.2.3 面试特性：tc8[6569]
- en: For this feature, we found 15 out of 12,800 prompts to activate above a threshold
    of 16.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特性，我们发现从12,800个提示中，有15个提示在16的阈值以上被激活。
- en: Input $(755,122)$.
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入 $(755,122)$。
- en: We started by exploring input $(755,122)$, which revealed several contributions
    from other tokens.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始探索输入 $(755,122)$，这揭示了来自其他标记的几个贡献。
- en: We began by looking at components that contributed to the final token. The top
    feature was tc7[17738], which connected to tc0[15432] (variants of interview),
    tc0[12425] (variants of interviewed), and tc0[12209] (tokens like Transcript,
    Interview, and rawdownloadcloneembedreportprint). The next feature, tc3[11401],
    was connected to tc0[15432] and tc0[12425] (same as the previous), as well as
    tc0[21414], which de-embedded to variants of spoke. This raised the possibility
    that “interview” is being used as a verb in this part of the prompt.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先查看了对最终标记有贡献的组件。最重要的特性是 tc7[17738]，它与 tc0[15432]（面试的变体）、tc0[12425]（被采访的变体）和
    tc0[12209]（如 Transcript、Interview 和 rawdownloadcloneembedreportprint 的标记）连接。下一个特性
    tc3[11401] 连接到 tc0[15432] 和 tc0[12425]（与之前相同），以及 tc0[21414]，它解嵌到说话的变体。这提高了“interview”
    在这个提示部分作为动词使用的可能性。
- en: 'Next, we turned our attention to previous tokens in the context, in hopes that
    this would clarify the sense in which “interview” was being used. The top attribution
    for the previous token (121) was through attn4[11]. The de-embeddings for top
    input-independent features were uninformative: tc0[22216] seemed to cover variants
    of gest), while tc0[7791] covered variants of sector. For token 120, pullbacks
    through attn2[2] showed connections to tc0[10564] and tc0[9519], both of which
    de-embedded to variants of In. This led us to believe “interview” was in fact
    being used as a noun, e.g. “in an interview…”'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将注意力转向上下文中的早期标记，希望这能澄清“interview”使用的意义。前一个标记（121）的最高归因通过attn4[11]。顶级输入无关特征的解嵌结果无信息：tc0[22216]似乎涵盖了gest的变体，而tc0[7791]涵盖了sector的变体。对于标记120，通过attn2[2]的回退显示了与tc0[10564]和tc0[9519]的连接，它们都解嵌到In的变体。这使我们相信“interview”实际上是作为名词使用的，例如“在采访中…”
- en: 'The top attribution for token 119 came through attn4[9], and showed connections
    to:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 标记119的最高归因通过attn4[9]，显示了以下连接：
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tc0[625]: allegations, accusations, allegation, …,'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc0[625]：指控、指责、指控，…，
- en: •
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tc0[10661]: allegedly, purportedly, supposedly, …, and'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc0[10661]：allegedly, purportedly, supposedly, …，和
- en: •
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tc0[22588]: reportedly, rumored, stockp, ….'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc0[22588]：reportedly, rumored, stockp, ….
- en: 'The next-highest attribution came through attn8[5], and showed connections
    to:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个最高的归因通过attn8[5]，显示了以下连接：
- en: •
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tc0[4771]: Casey, Chase, depot, …, and'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc0[4771]：Casey, Chase, depot, …，和
- en: •
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tc0[5436]: didn, didn, wasn …'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc0[5436]：didn, didn, wasn …
- en: 'The next-highest was tc2[5264]@119, which showed connections to:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个最高的特征是tc2[5264]@119，它显示了以下连接：
- en: •
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tc0[5870]: unlocks, upstairs, downstairs, …,'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc0[5870]：unlock, upstairs, downstairs, …,
- en: •
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tc0[14674]: said and variants, and'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc0[14674]：said及其变体，和
- en: •
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'tc0[12915]: said and variants'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc0[12915]：said及其变体
- en: This led us to believe that this feature fires on “said in an interview”-type
    prompts.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们相信，这个特征在“在采访中说”-类型的提示符上会触发。
- en: Input $(1777,53)$.
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入 $(1777,53)$。
- en: 'Next we tried another prompt, $(1777,53)$. The top features for the current
    token were identical to the previous example: tc7[17738], tc3[11401], tc6[24442],
    and so on.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们尝试了另一个提示符，$(1777,53)$。当前标记的主要特征与之前的示例相同：tc7[17738]、tc3[11401]、tc6[24442]，等等。
- en: For the context, we first looked at the pullback of our feature through the
    OV circuit of attn2[2]@51. This showed input-independent connections to tc0[10564],
    which once again de-embedded to In. Next up, attn4[9]@50. This feature connected
    to tc0[625], tc0[10661], and tc0[22588], exactly like before. Recall that these
    features de-embed to “said” and “allegedly”-type tokens.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上下文，我们首先查看了通过attn2[2]@51的OV电路对我们特征的回退。这显示了与tc0[10564]的输入无关的连接，该连接再次解嵌到In。接下来是attn4[9]@50。这个特征连接到tc0[625]、tc0[10661]和tc0[22588]，与之前完全相同。请记住，这些特征解嵌到“said”和“allegedly”-类型的标记。
- en: Finally, we saw a high attribution from a much earlier token via attn8[9]@16.
    The pullback of our feature through this head showed high input-independent connections
    to tc0[14048], whose de-embeddings were all variants of election.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过attn8[9]@16看到了一来自更早标记的高归因。我们特征的回退通过这个头显示了与tc0[14048]的高输入无关连接，其解嵌结果都是election的变体。
- en: Input $(10179,90)$.
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入 $(10179,90)$。
- en: 'For our last input, we once again found the same transcoder features contributing
    through the current token. For earlier tokens, we tried:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的最后输入，我们再次发现相同的转码器特征通过当前标记作出贡献。对于早期标记，我们尝试了：
- en: •
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: attn2[2]@88, finding tc0[10564] (In) again;
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: attn2[2]@88，再次找到tc0[10564]（In）；
- en: •
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: attn8[9]@86, finding tc0[16885], which also de-embedded to elections despite
    being a new feature;
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: attn8[9]@86，找到tc0[16885]，尽管这是一个新特征，但它也解嵌到elections；
- en: •
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: attn6[20291]@86, finding tc0[372] ( told); and
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: attn6[20291]@86，找到tc0[372]（told）；和
- en: •
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: tc6[20291]@86, finding tc0[372] again.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: tc6[20291]@86，再次找到tc0[372]。
- en: Final guess.
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最终猜测。
- en: In sum, we decided this feature fires for prompts conveying “told/said in an
    interview.” Top activating examples corroborated this, without any notable deviations
    from this pattern.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们决定这个特征在传达“在采访中告诉/说”的提示符中会触发。顶级激活示例证实了这一点，没有任何显著的偏离这一模式。
- en: H.2.4 Four more restricted blind case studies
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: H.2.4 四个更为受限的盲目案例研究
- en: We present the results of four more restricted blind case studies in Table [3](#A8.T3
    "Table 3 ‣ H.2.4 Four more restricted blind case studies ‣ H.2 Restricted blind
    case studies ‣ Appendix H Full case studies ‣ Transcoders Find Interpretable LLM
    Feature Circuits"). In the interest of conserving space, only the results of these
    case studies are presented; the original Jupyter Notebooks in which the studies
    were carried out are available in our code, which can be found at [https://github.com/jacobdunefsky/transcoder_circuits](https://github.com/jacobdunefsky/transcoder_circuits).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格 [3](#A8.T3 "表 3 ‣ H.2.4 四个更为有限的盲目案例研究 ‣ H.2 限制盲目案例研究 ‣ 附录 H 完整案例研究 ‣ 转码器发现可解释的
    LLM 特征电路") 中展示了四个更为有限的盲目案例研究结果。为了节省空间，仅展示了这些案例研究的结果；进行这些研究的原始 Jupyter Notebooks
    可以在我们的代码中找到，代码可在 [https://github.com/jacobdunefsky/transcoder_circuits](https://github.com/jacobdunefsky/transcoder_circuits)
    中获取。
- en: 'Table 3: The results of four more restricted blind case studies.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：四个更为有限的盲目案例研究结果。
- en: '| Feature | Final hypothesis | Actual interpretation | Outcome |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 最终假设 | 实际解释 | 结果 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| tc8[9030] | Fires on biology when in the context of being a subject of study
    | Fires on scientific subjects of study like chemistry, psychology, biology, economics
    | Failure |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| tc8[9030] | 在作为研究主题的生物学上下文中触发 | 在科学研究主题如化学、心理学、生物学、经济学中触发 | 失败 |'
- en: '| tc8[4911] | Fires on though or although in the beginning of a clause | Fires
    on though or although in the beginning of a clause | Success |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| tc8[4911] | 在从句开头的 though 或 although 上触发 | 在从句开头的 though 或 although 上触发 |
    成功 |'
- en: '| tc8[6414] | Largely uninterpretable feature that sometimes fires on Cyrillic
    text | Largely uninterpretable feature that sometimes fires on Cyrillic text |
    Success |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| tc8[6414] | 主要不可解释的特征，有时在西里尔字母文本上触发 | 主要不可解释的特征，有时在西里尔字母文本上触发 | 成功 |'
- en: '| tc8[2725] | Fires on phrases about not offering things or not providing things.
    (As a stretch: particularly in legalese context?) | Fires on phrases about not
    offering things or not providing things, in general | Mostly a success |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| tc8[2725] | 在关于不提供东西或不提供服务的短语上触发。（作为扩展：特别是在法律术语上下文中？） | 在关于不提供东西或不提供服务的短语上触发，一般情况下
    | 大多数成功 |'
