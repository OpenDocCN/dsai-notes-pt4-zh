- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 17:34:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 17:34:06'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CIBench: Evaluating Your LLMs with a Code Interpreter Plugin'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'CIBench: 使用代码解释器插件评估您的LLM'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.10499](https://ar5iv.labs.arxiv.org/html/2407.10499)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.10499](https://ar5iv.labs.arxiv.org/html/2407.10499)
- en: Songyang Zhang^(1,∗), Chuyu Zhang^(1,2,), Yingfan Hu^∗, Haowen Shen, Kuikun
    Liu¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Songyang Zhang^(1,∗), Chuyu Zhang^(1,2,), Yingfan Hu^∗, Haowen Shen, Kuikun
    Liu¹,
- en: Zerun Ma¹, Fengzhe Zhou¹, Wenwei Zhang¹, Xuming He², Dahua Lin¹, Kai Chen^(1,†)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zerun Ma¹, Fengzhe Zhou¹, Wenwei Zhang¹, Xuming He², Dahua Lin¹, Kai Chen^(1,†)
- en: ¹Shanghai Artificial Intelligence Laboratory  ²ShanghaiTech University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹上海人工智能实验室  ²上海科技大学
- en: '{zhangsongyang}@pjlab.org.cn Equal contribution. Code is available at [https://github.com/open-compass/CIBench](https://github.com/open-compass/CIBench)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{zhangsongyang}@pjlab.org.cn 平等贡献。代码可在 [https://github.com/open-compass/CIBench](https://github.com/open-compass/CIBench)
    获得'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While LLM-Based agents, which use external tools to solve complex problems,
    have made significant progress, benchmarking their ability is challenging, thereby
    hindering a clear understanding of their limitations. In this paper, we propose
    an interactive evaluation framework, named CIBench, to comprehensively assess
    LLMs’ ability to utilize code interpreters for data science tasks. Our evaluation
    framework includes an evaluation dataset and two evaluation modes. The evaluation
    dataset is constructed using an LLM-human cooperative approach and simulates an
    authentic workflow by leveraging consecutive and interactive IPython sessions.
    The two evaluation modes assess LLMs’ ability with and without human assistance.
    We conduct extensive experiments to analyze the ability of 24 LLMs on CIBench and
    provide valuable insights for future LLMs in code interpreter utilization.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于LLM的代理使用外部工具来解决复杂问题已经取得了显著进展，但对其能力进行基准测试仍然具有挑战性，这阻碍了对其局限性的清晰理解。在本文中，我们提出了一个互动评估框架，名为CIBench，以全面评估LLM在数据科学任务中利用代码解释器的能力。我们的评估框架包括一个评估数据集和两种评估模式。评估数据集通过LLM与人类的合作方法构建，并通过利用连续的互动IPython会话来模拟真实的工作流程。这两种评估模式分别评估LLM在有无人工协助的情况下的能力。我们进行了大量实验，以分析24个LLM在CIBench上的表现，并为未来LLM在代码解释器利用方面提供了宝贵的见解。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Empowered by the emerging abilities of Large Language Models (LLMs), numerous
    agents have surfaced to harness these generalist models for utilizing external
    tools, such as web browsing, document retrieval, and code interpretation, in tackling
    complex real-world problems. Notable examples include ToolLLM [[23](#bib.bib23)],
    LangChain [[5](#bib.bib5)], AutoGPT [[25](#bib.bib25)], and QwenAgent [[2](#bib.bib2)].
    Specifically, agents equipped with a code interpreter leverage the advanced programming
    skills of LLMs through a natural language interface, thereby facilitating the
    creation of workflows that are both effortless and efficient. However, assessing
    the agents’ capability to generate executable code and effectively use code interpreters
    remains a challenge, hindering a clear understanding of their limitations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）新兴能力的推动下，许多代理出现了，旨在利用这些通用模型来使用外部工具，如网页浏览、文档检索和代码解释，以应对复杂的现实世界问题。值得注意的例子包括ToolLLM
    [[23](#bib.bib23)]、LangChain [[5](#bib.bib5)]、AutoGPT [[25](#bib.bib25)] 和 QwenAgent
    [[2](#bib.bib2)]。具体而言，配备代码解释器的代理通过自然语言接口利用LLM的高级编程技能，从而促进了既轻松又高效的工作流程的创建。然而，评估这些代理生成可执行代码和有效使用代码解释器的能力仍然是一个挑战，这阻碍了对其局限性的清晰理解。
- en: In this study, we focus on assessing the proficiency of LLMs in leveraging code
    interpreters to address data science problems across several distinct domains,
    like data analysis, visualization, and machine learning. These tasks necessitate
    that LLMs exhibit advanced capabilities in instruction following, reasoning, and
    programming. Existing benchmarks, including GSM8K [[8](#bib.bib8)], MathBench [[20](#bib.bib20)]
    and HumanEval [[6](#bib.bib6)], primarily assess the models’ abilities to solve
    mathematical or coding problems solely. These benchmarks, while valuable for measuring
    specific competencies, fail to fully represent the LLMs’ aptitude for building
    complex workflows involving code interpreters in practical applications. More
    recent efforts, such as MINT [[27](#bib.bib27)], QwenAgent [[2](#bib.bib2)], CodeGen [[21](#bib.bib21)],
    and DS-1000 [[16](#bib.bib16)], aim to bridge the existing gap by constructing
    novel benchmarks, particularly tailored to mathematical problems and data analysis
    tasks. Despite considerable advancements, they either focus on single-turn question
    assessments or have a limited scope in data science. The substantial insights
    they provided inadequately reflect the LLM’s ability to invoke a code interpreter
    to solve real-world data science problems, which typically require consecutive
    sessions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们专注于评估 LLMs 在利用代码解释器解决多个不同领域的数据科学问题方面的能力，例如数据分析、可视化和机器学习。这些任务要求 LLMs
    展现出在指令跟随、推理和编程方面的高级能力。现有的基准测试，如 GSM8K [[8](#bib.bib8)]、MathBench [[20](#bib.bib20)]
    和 HumanEval [[6](#bib.bib6)]，主要评估模型解决数学或编码问题的能力。这些基准测试虽然在衡量特定技能方面有价值，但无法完全代表 LLMs
    在实际应用中利用代码解释器构建复杂工作流的能力。最近的努力，如 MINT [[27](#bib.bib27)]、QwenAgent [[2](#bib.bib2)]、CodeGen [[21](#bib.bib21)]
    和 DS-1000 [[16](#bib.bib16)]，旨在通过构建新的基准测试，特别是针对数学问题和数据分析任务来填补这一现有的空白。尽管取得了相当大的进展，但它们要么专注于单轮问题评估，要么在数据科学领域范围有限。它们所提供的显著见解并不足以全面反映
    LLM 通过调用代码解释器解决实际数据科学问题的能力，这些问题通常需要连续的会话。
- en: 'To address these shortcomings, as depicted in Fig.[1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin"),
    we introduce a novel evaluation framework that encompasses a benchmark with consecutive
    and diverse tasks, along with comprehensive assessment protocols. This framework
    aims to provide a thorough evaluation of LLMs’ ability to use code interpreters.
    Notably, the benchmark employs a distinctive LLM-human cooperative approach, as
    depicted in Fig. [2](#S3.F2 "Figure 2 ‣ 3 CIBench ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin"). It simulates authentic workflow scenarios using
    interactive IPython sessions with sequential, interconnected questions focused
    on popular Python modules such as Matplotlib, Pandas, and PyTorch. Additionally,
    we utilize two evaluation modes and a series of fine-grained metrics for various
    outputs. The statistics of our benchmark are shown in Tab.[1](#S1.T1 "Table 1
    ‣ 1 Introduction ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin").'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这些不足，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ CIBench: Evaluating Your
    LLMs with a Code Interpreter Plugin")所示，我们引入了一个新颖的评估框架，该框架包括一个具有连续和多样化任务的基准测试，以及全面的评估协议。该框架旨在对
    LLMs 使用代码解释器的能力进行全面评估。值得注意的是，该基准测试采用了一种独特的 LLM-人类合作方法，如图[2](#S3.F2 "Figure 2 ‣
    3 CIBench ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin")所示。它通过使用
    IPython 会话模拟真实工作流场景，设置了集中在流行 Python 模块（如 Matplotlib、Pandas 和 PyTorch）上的连续、相互关联的问题。此外，我们采用了两种评估模式和一系列细化的输出指标。我们的基准测试统计数据显示在表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ CIBench: Evaluating Your LLMs with a Code Interpreter
    Plugin")中。'
- en: '![Refer to caption](img/450ab0950929a352e5d8cf25181be850.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/450ab0950929a352e5d8cf25181be850.png)'
- en: 'Figure 1: Features of our benchmark. Our benchmark consists of interactive
    sessions, diverse tasks covering various Python modules, and comprehensive evaluations
    (The tool-call rate is not displayed).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们的基准测试特点。我们的基准测试包括交互式会话、多样化的任务覆盖各种 Python 模块，以及全面的评估（工具调用频率未显示）。
- en: 'Table 1: Comparison of different datasets. "Multi-turn" denotes that LLMs can
    attempt to solve tasks through multiple trials. "Code interpreter" means whether
    LLM needs to call the code interpreter to solve the problem. "Consecutive" indicates
    that each task consists of a series of sequential questions rather than a single
    question.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同数据集的比较。"多轮"表示 LLMs 可以通过多次尝试来解决任务。"代码解释器"指 LLM 是否需要调用代码解释器来解决问题。"连续"表示每个任务由一系列连续的问题组成，而不是单个问题。
- en: '| Dataset | Topic | Size | Multi-turn | Code Interpreter | Consecutive |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 主题 | 大小 | 多轮 | 代码解释器 | 连续 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DS-1000 | Data Science | 1000 questions | No | No | No |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| DS-1000 | 数据科学 | 1000个问题 | 否 | 否 | 否 |'
- en: '| MINT | Math, Reasoning | 586 questions | Yes | Yes | No |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| MINT | 数学，推理 | 586个问题 | 是 | 是 | 否 |'
- en: '| CodeGen | Math, Data Science, etc | 115 tasks | No | No | Yes |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| CodeGen | 数学，数据科学等 | 115个任务 | 否 | 否 | 是 |'
- en: '| QwenAgent | Math, Data visualization | 295 questions | No | Yes | No |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| QwenAgent | 数学，数据可视化 | 295个问题 | 否 | 是 | 否 |'
- en: '| CIBench | Data Science | 234 tasks, 1900+ questions | Yes | Yes | Yes |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| CIBench | 数据科学 | 234个任务，1900+个问题 | 是 | 是 | 是 |'
- en: Specifically, we build the evaluation dataset by initially identifying ten highly
    prevalent Python libraries within the domain of data science. Then, we prompt
    advanced LLM, such as GPT-4, to generate instructions and code snippets within
    Jupyter Notebook format. Each notebook is structured to contain a sequence of
    10 to 15 progressive steps, with increasing levels of complexity. To mitigate
    any inherent biases or limitations in the LLM-generated content, we employ human
    experts to generate template tasks based on the common patterns observed in the
    LLM-generated tasks and online resources. Those tasks can incorporate multiple
    interchangeable datasets for evaluation. Those designs ensure that the benchmark
    encapsulates both diversity and quality, thereby offering a comprehensive and
    balanced assessment of code interpreter capabilities.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们通过首先识别数据科学领域内十个广泛使用的Python库来构建评估数据集。然后，我们提示先进的LLM，如GPT-4，生成Jupyter Notebook格式的指令和代码片段。每个Notebook的结构包含10到15个逐步递进的步骤，并逐渐增加复杂性。为了减轻LLM生成内容中固有的偏见或局限性，我们聘请人类专家基于LLM生成任务和在线资源中观察到的常见模式生成模板任务。这些任务可以包含多个可互换的数据集进行评估。这些设计确保基准测试既包含多样性又具备质量，从而提供对代码解释器能力的全面且平衡的评估。
- en: 'To thoroughly assess the LLMs’ performance on our benchmark, we have instituted
    two distinct evaluation modes: the end-to-end mode and the oracle mode. In the
    end-to-end mode, LLMs are tasked with a holistic problem-solving process where
    they must reason through given instructions and generate corresponding code. This
    requires them to iteratively refine their output based on feedback from the code
    interpreter, as they attempt to solve multiple consecutive questions that build
    upon one another. Additionally, the oracle mode simulates guided learning by providing
    the LLM with the correct code snippet when it fails. This immediate feedback mimics
    human guidance and equips the model to use this accurate example for tackling
    subsequent tasks in the same context. Furthermore, we introduce two types of metrics:
    the process-oriented (i.e., tool-call rate, executable rate) and output-oriented
    (i.e., numeric accuracy, text score, visualization score), to provide a comprehensive
    analysis of the model’s performance.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面评估LLM在我们的基准测试中的表现，我们制定了两种不同的评估模式：端到端模式和oracle模式。在端到端模式中，LLM需要完成一个整体问题解决过程，必须根据给定的指令进行推理并生成相应的代码。这要求它们根据代码解释器的反馈迭代完善其输出，因为它们需要尝试解决多个互相衔接的问题。此外，oracle模式通过在LLM失败时提供正确的代码片段来模拟指导学习。这种即时反馈模拟了人类指导，并使模型能够利用该准确示例来处理同一上下文中的后续任务。此外，我们引入了两种类型的指标：过程导向（即工具调用率、可执行率）和输出导向（即数值准确性、文本评分、可视化评分），以提供对模型性能的全面分析。
- en: 'Based on our evaluation framework, we conduct extensive experiments and analysis
    using 19 LLMs. The results indicate that open-sourced LLMs struggle to utilize
    PyTorch- and TensorFlow-like modules (Fig.[10](#S4.F10 "Figure 10 ‣ Limitation
    ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin")), and the best-open-sourced LLMs lag behind GPT-4 by 10.0%
    (Tab.[3](#S4.T3 "Table 3 ‣ 4.1 Experiments Setup ‣ 4 Experiments ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin")). In summary, our contributions are
    three-fold:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '根据我们的评估框架，我们使用19个LLM进行了广泛的实验和分析。结果表明，开源LLM在利用类似PyTorch和TensorFlow的模块时表现不佳（图。[10](#S4.F10
    "Figure 10 ‣ Limitation ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin")），而最佳的开源LLM落后于GPT-4 10.0%（表。[3](#S4.T3
    "Table 3 ‣ 4.1 Experiments Setup ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin")）。总的来说，我们的贡献有三方面：'
- en: $\bullet$ We build a new benchmark for agents with code interpreters using an
    LLM-human cooperative method. It consists of interactive IPython sessions with
    interconnected questions on key data science libraries, simulating interactive
    problem-solving scenarios in practical workflows.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们使用 LLM-人类合作方法建立了一个新的基准测试，针对具有代码解释器的代理。它由交互式 IPython 会话组成，包含关于关键数据科学库的相互关联的问题，模拟实际工作流程中的交互式问题解决场景。
- en: $\bullet$ We devise unique assessment strategies involving both end-to-end and
    oracle modes. We also introduce several evaluation metrics to assess various outputs,
    offering a comprehensive gauge of LLMs’ coding prowess within the benchmark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们设计了独特的评估策略，涉及端到端模式和 oracle 模式。我们还引入了若干评估指标来评估各种输出，提供了对 LLM 编码能力的全面衡量。
- en: $\bullet$ We conduct thorough experiments with 24 LLMs to analyze their performance
    on our benchmark. The results indicate that open-sourced LLMs are inferior to
    GPT-4 by a large margin and perform poorly in the modeling category modules.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们对 24 个 LLM 进行了详细实验，以分析它们在我们基准测试中的表现。结果表明，开源 LLM 相比 GPT-4 逊色较多，并且在建模类别模块中的表现较差。
- en: 2 Related Works
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: CIBench is an evaluation framework that assesses LLMs’ [[26](#bib.bib26), [2](#bib.bib2),
    [10](#bib.bib10), [7](#bib.bib7), [4](#bib.bib4)] ability to utilize external
    code interpreters for solving data science tasks. Therefore, we focus on presenting
    work related to invoking code interpreters and benchmarks related to data science.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CIBench 是一个评估框架，用于评估 LLM 利用外部代码解释器解决数据科学任务的能力。因此，我们重点介绍与调用代码解释器和数据科学相关的基准测试相关的工作。
- en: 2.1 Model with Plugins
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 插件模型
- en: LLM-based agents use external tools via APIs to solve complex tasks and have
    been regarded as a promising direction [[5](#bib.bib5), [23](#bib.bib23), [25](#bib.bib25),
    [24](#bib.bib24), [13](#bib.bib13), [30](#bib.bib30)]. Specifically, [[18](#bib.bib18),
    [13](#bib.bib13), [22](#bib.bib22)] develop efficient workflows to coordinate
    multi-agent systems for automatic programming. [[24](#bib.bib24), [5](#bib.bib5),
    [23](#bib.bib23), [11](#bib.bib11)] equip LLMs with external tools, such as search
    engines, calculators, and code interpreters, to augment LLMs’ problem-solving
    ability. Among these tools, the code interpreter can promote LLMs’ reasoning and
    coding ability and has gradually gained attention in works like [[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LLM 的代理通过 API 使用外部工具来解决复杂任务，并被视为一种有前途的方向[[5](#bib.bib5), [23](#bib.bib23),
    [25](#bib.bib25), [24](#bib.bib24), [13](#bib.bib13), [30](#bib.bib30)]。具体来说，[[18](#bib.bib18),
    [13](#bib.bib13), [22](#bib.bib22)] 开发了高效的工作流程来协调多代理系统进行自动编程。[[24](#bib.bib24),
    [5](#bib.bib5), [23](#bib.bib23), [11](#bib.bib11)] 为 LLM 配备了外部工具，如搜索引擎、计算器和代码解释器，以增强
    LLM 的问题解决能力。在这些工具中，代码解释器可以提升 LLM 的推理和编码能力，并在像 [[32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34)] 的工作中逐渐受到关注。
- en: In contrast, we aim to develop an evaluation framework to benchmark LLMs’ ability
    with code interpreters for solving data science tasks, providing insights for
    future work to promote LLMs for better realistic utilization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们的目标是开发一个评估框架，以基准测试 LLM 在解决数据科学任务时的代码解释器能力，为未来的工作提供洞察，以推动 LLM 更好地实际应用。
- en: 2.2 Related Benchmarks
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 相关基准
- en: Various benchmarks [[8](#bib.bib8), [6](#bib.bib6), [23](#bib.bib23), [2](#bib.bib2)]
    have been proposed to measure LLMs’ reasoning, coding, and tool utilization ability.
    Classic benchmarks, such as GSM8K [[8](#bib.bib8)], MATH [[12](#bib.bib12)], HumanEval [[6](#bib.bib6)],
    and MBPP [[1](#bib.bib1)], focus on evaluating the mathematical reasoning or code
    capability of LLMs. ToolBench [[23](#bib.bib23)] and MS-Agent Bench [[17](#bib.bib17)]
    aim to evaluate LLMs’ capability in effectively utilizing various tools and generating
    accurate and contextually appropriate responses.  [[27](#bib.bib27)] introduce
    MINT, a benchmark that evaluates LLMs’ ability to solve challenging tasks with
    multi-turn interactions by using tools and leveraging natural language feedback.
    However, the above benchmarks cannot measure LLMs’ ability in data science tasks,
    which require instruction following, coding, and tool utilization abilities. To
    address this gap,  [[21](#bib.bib21)] introduces a multi-turn programming benchmark
    consisting of 115 diverse problems, covering topics such as mathematics, algorithms,
    and data science. QwenAgent [[2](#bib.bib2)] introduces a benchmark for data science,
    focusing mainly on mathematical problems and data visualization.  [[16](#bib.bib16)]
    introduce DS-1000, a code generation benchmark with a thousand data science problems.
    Meanwhile, [[14](#bib.bib14)] introduces DABench, which evaluates various concepts
    with individual questions assigned to each dataset. However, this approach does
    not effectively mimic practical scenarios where interconnected questions are raised.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 各种基准测试 [[8](#bib.bib8), [6](#bib.bib6), [23](#bib.bib23), [2](#bib.bib2)]已被提出以测量LLM的推理、编码和工具使用能力。经典基准测试，如GSM8K [[8](#bib.bib8)]、MATH [[12](#bib.bib12)]、HumanEval [[6](#bib.bib6)]和MBPP [[1](#bib.bib1)]，专注于评估LLM的数学推理或编码能力。ToolBench [[23](#bib.bib23)]和MS-Agent
    Bench [[17](#bib.bib17)]旨在评估LLM在有效使用各种工具和生成准确且上下文适当的响应方面的能力。[[27](#bib.bib27)]引入了MINT，这是一种通过使用工具和利用自然语言反馈解决具有挑战性任务的基准测试。然而，上述基准测试不能衡量LLM在数据科学任务中的能力，而这些任务需要指令跟随、编码和工具使用能力。为了解决这一缺口，[[21](#bib.bib21)]引入了一个多回合编程基准测试，包含115个多样化的问题，涵盖数学、算法和数据科学等主题。QwenAgent [[2](#bib.bib2)]引入了一个数据科学基准测试，主要集中在数学问题和数据可视化方面。[[16](#bib.bib16)]引入了DS-1000，这是一个包含一千个数据科学问题的代码生成基准测试。同时，[[14](#bib.bib14)]引入了DABench，通过为每个数据集分配单独的问题来评估各种概念。然而，这种方法未能有效模拟实际场景中的互联问题。
- en: In contrast to the above works, CIBench  simulates a real-world data science
    workflow by leveraging sequential interactive IPython sessions and covers most
    concepts in data science by including commonly used Python modules. Furthermore,
    we devise two evaluation modes and five metrics to holistically evaluate LLMs’
    abilities.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述工作相比，CIBench通过利用顺序交互的IPython会话来模拟真实世界的数据科学工作流程，并通过包含常用的Python模块覆盖大多数数据科学概念。此外，我们设计了两种评估模式和五个指标，以全面评估LLM的能力。
- en: 3 CIBench
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CIBench
- en: 'To benchmark LLM’s ability to leverage code interpreters for addressing data
    science problems, we propose a novel evaluation framework (Fig.[2](#S3.F2 "Figure
    2 ‣ 3 CIBench ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin")),
    which comprises a diverse evaluation dataset and two newly devised evaluation
    modes. The evaluation dataset is generated through an LLM-human cooperative approach
    and simulates authentic workflow scenarios for solving sequential and interconnected
    tasks. Given the evaluation dataset, we adhere to the ReAct protocol [[31](#bib.bib31)]
    to generate reasoning traces and invoke code interpreters alternately. And, we
    allow LLMs to attempt to solve tasks multiple times, enabling exploration of their
    self-debugging capabilities based on feedback from the code interpreter. Finally,
    we propose two evaluation modes: the end-to-end mode and the oracle mode, to comprehensively
    measure LLM’s ability with and without human interaction.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估LLM在利用代码解释器解决数据科学问题的能力，我们提出了一个新颖的评估框架（图[2](#S3.F2 "图 2 ‣ 3 CIBench ‣ CIBench:
    使用代码解释器插件评估您的LLM")），该框架包含一个多样化的评估数据集和两种新制定的评估模式。评估数据集通过LLM与人类合作生成，模拟了解决顺序性和相互关联任务的真实工作流程场景。基于评估数据集，我们遵循ReAct协议 [[31](#bib.bib31)]生成推理痕迹，并交替调用代码解释器。同时，我们允许LLM多次尝试解决任务，以便根据代码解释器的反馈探索其自我调试能力。最后，我们提出了两种评估模式：端到端模式和oracle模式，以全面测量LLM在有无人工交互下的能力。'
- en: 'In the following sections, we will detail the construction of the dataset in
    Sec.[3.1](#S3.SS1 "3.1 Evaluation Dataset ‣ 3 CIBench ‣ CIBench: Evaluating Your
    LLMs with a Code Interpreter Plugin") and the evaluation modes and metrics in
    Sec.[3.2](#S3.SS2 "3.2 Evaluation Modes and Metrics ‣ 3 CIBench ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细介绍第 [3.1](#S3.SS1 "3.1 评估数据集 ‣ 3 CIBench ‣ CIBench：使用代码解释器插件评估您的
    LLM") 节中的数据集构建和第 [3.2](#S3.SS2 "3.2 评估模式和指标 ‣ 3 CIBench ‣ CIBench：使用代码解释器插件评估您的
    LLM") 节中的评估模式和指标。
- en: '![Refer to caption](img/9c9c2a9750e0b9d053aa9dd5ed2ae2c3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9c9c2a9750e0b9d053aa9dd5ed2ae2c3.png)'
- en: 'Figure 2: Overview of CIBench. CIBench first selects Python modules to generate
    candidate topics and then generates tasks based on these modules and the selected
    topic. Additionally, humans are engaged to generate new tasks to ensure diversity
    and filter out incorrect questions to enhance quality.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：CIBench 概述。CIBench 首先选择 Python 模块生成候选主题，然后基于这些模块和选定的主题生成任务。此外，人工参与生成新任务，以确保多样性并筛选出不正确的问题，从而提高质量。
- en: 'Table 2: Selected Python modules and their categories.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：选定的 Python 模块及其类别。
- en: '| Category | Python Modules |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | Python 模块 |'
- en: '| --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Data Cleaning and Manipulation | Pandas |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 数据清洗与处理 | Pandas |'
- en: '| Data Visualization | Matplotlib, Seaborn |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 数据可视化 | Matplotlib, Seaborn |'
- en: '| Modeling | Scikit-learn, PyTorch, TensorFlow, LightGBM |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 建模 | Scikit-learn, PyTorch, TensorFlow, LightGBM |'
- en: '| Natural Language Processing | NLTK |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言处理 | NLTK |'
- en: '| Image Processing | OpenCV-Python |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 图像处理 | OpenCV-Python |'
- en: '| Mathematics and Statistics | SciPy |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 数学与统计 | SciPy |'
- en: 'Task
    Generation Prompt: Please create jupyter
    notebook experiment based on Python module {}. Please follow these rules: 1\.
    The experiment should be conducted in a jupyter notebook manner, but use the markdown
    format. 2\. The experiment should only use Python code. 3\. The experiment has
    around 10-15 continuous steps, from the easiest to the hardest. 4\. The step description
    should be concise. 5\. The step description should be precise and contain exact
    parameter names and values to instruct. 6\. Each step requires Python code to
    solve and the executed result should be the numeric answer, structured output,
    or visualized result. 7\. Please use ‘matplotlib’ to visualize if necessary. 8\.
    DO NOT have any steps to save or write any output files. 9\. Please provide an
    input data file with an external link. The experiment topic is {}. You should
    generate the experiment file without any other statements.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'Task
    Generation Prompt: Please create jupyter
    notebook experiment based on Python module {}. Please follow these rules: 1\.
    The experiment should be conducted in a jupyter notebook manner, but use the markdown
    format. 2\. The experiment should only use Python code. 3\. The experiment has
    around 10-15 continuous steps, from the easiest to the hardest. 4\. The step description
    should be concise. 5\. The step description should be precise and contain exact
    parameter names and values to instruct. 6\. Each step requires Python code to
    solve and the executed result should be the numeric answer, structured output,
    or visualized result. 7\. Please use ‘matplotlib’ to visualize if necessary. 8\.
    DO NOT have any steps to save or write any output files. 9\. Please provide an
    input data file with an external link. The experiment topic is {}. You should
    generate the experiment file without any other statements.'
- en: 'Figure 3: An example prompt of task generation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：任务生成的示例提示。
- en: 3.1 Evaluation Dataset
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 评估数据集
- en: Python Modules Selection
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Python 模块选择
- en: 'We carefully choose modules that pertain to the fields of data science. It
    encompasses a wide array of topics such as data cleansing, visualization, image
    analysis, mathematical computations, and statistical methods. The modules we have
    selected are detailed in Tab.[2](#S3.T2 "Table 2 ‣ 3 CIBench ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们精心选择与数据科学领域相关的模块。它涵盖了广泛的主题，如数据清洗、可视化、图像分析、数学计算和统计方法。我们选择的模块在表 [2](#S3.T2 "表
    2 ‣ 3 CIBench ‣ CIBench：使用代码解释器插件评估您的 LLM") 中有详细描述。
- en: Topic Candidates Generation
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主题候选生成
- en: 'After selecting modules, we prompt (Appendix [B](#A2 "Appendix B Construction
    Prompts and Rules ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin"))
    GPT-4 to summarize 50 topics for each module, to encapsulate the vast majority
    of the module’s functionalities, thereby offering precise guidance for the subsequent
    generation of more targeted questions.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模块后，我们提示（附录 [B](#A2 "附录 B 构造提示和规则 ‣ CIBench：使用代码解释器插件评估您的 LLM")） GPT-4 为每个模块总结
    50 个主题，以涵盖模块的大部分功能，从而为后续生成更具针对性的问题提供精确指导。
- en: Tasks Generation and Refinement
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务生成与优化
- en: 'We sample a module and topic, then prompt GPT-4 to generate questions and code
    based on the prompt in Fig.[3](#S3.F3 "Figure 3 ‣ 3 CIBench ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin"). The prompt is designed to enable GPT-4
    to generate a Jupyter notebook with sequential steps and various outputs, including
    numeric answers, structured output, and visualizations, mirroring real-world scenarios.
    Despite our request for concise descriptions, the generated content may lack conciseness
    and continuity. To address this, we undertake iterative refinement of these tasks.
    This process involves presenting both good and bad cases, along with additional
    modifications, to enhance the quality of questions and reduce ambiguity. Details
    of prompts used for refinement are in Appendix [B](#A2 "Appendix B Construction
    Prompts and Rules ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采样一个模块和主题，然后提示 GPT-4 基于图 [3](#S3.F3 "图 3 ‣ 3 CIBench ‣ CIBench：使用代码解释器插件评估您的
    LLM") 中的提示生成问题和代码。提示设计使 GPT-4 能生成一个包含顺序步骤和各种输出的 Jupyter 笔记本，包括数值答案、结构化输出和可视化，模拟真实世界场景。尽管我们要求简洁描述，但生成的内容可能缺乏简洁性和连贯性。为此，我们进行任务的迭代优化。这个过程包括展示好的和差的案例，以及额外的修改，以提高问题的质量并减少模糊性。用于优化的提示详细信息见附录
    [B](#A2 "附录 B 构造提示和规则 ‣ CIBench：使用代码解释器插件评估您的 LLM")。
- en: Human Refinement
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工优化
- en: Despite the excellent capabilities of LLMs, we found that the generated tasks
    often focus on typical datasets, such as Titanic and Iris, which limits task diversity.
    Additionally, ensuring perfect accuracy in question descriptions and result correctness
    is challenging.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM具有出色的能力，我们发现生成的任务通常集中在典型的数据集上，如Titanic和Iris，这限制了任务的多样性。此外，确保问题描述和结果准确性是具有挑战性的。
- en: 'To enhance diversity, we summarize template tasks based on the generated tasks
    and existing high-quality tutorials available in various Python libraries (Fig.[11](#A7.F11
    "Figure 11 ‣ Appendix G Ethical Consideration ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin")). With minor modifications, these template tasks
    can be adapted to a wide range of datasets. To further diversify the benchmark,
    we collect new datasets through two approaches: 1) We prompt GPT-4 to generate
    datasets tailored to different templates, leveraging GPT-4’s ability to flexibly
    specify characteristics and data attributes. 2) We incorporate the latest datasets
    from the past year, ensuring authenticity and diversity, and significantly reducing
    the likelihood that the model has previously encountered this data.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '为了增强多样性，我们总结了基于生成任务和各种Python库中现有高质量教程的模板任务（图 [11](#A7.F11 "图 11 ‣ 附录 G 道德考量
    ‣ CIBench: 使用代码解释器插件评估你的 LLM")）。通过一些小的修改，这些模板任务可以适应各种数据集。为了进一步多样化基准，我们通过两种方法收集新的数据集：1）我们提示
    GPT-4 生成适应不同模板的数据集，利用 GPT-4 灵活指定特征和数据属性的能力。2）我们结合过去一年最新的数据集，确保数据的真实性和多样性，并显著降低模型之前遇到这些数据的可能性。'
- en: 'To improve quality, we employ several experts who use a code interpreter for
    manual double-checking. This approach ensures that questions are written by real
    users and that various factors, such as runtime, are carefully controlled. For
    a comprehensive overview of the rules governing quality control, please refer
    to the Appendix [B](#A2.SS0.SSS0.Px3 "Quality Control Rules ‣ Appendix B Construction
    Prompts and Rules ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin").
    The statistics of the dataset are also provided in the Appendix [A.2](#A1.SS2
    "A.2 Dataset Statistics ‣ Appendix A Dataset Details ‣ CIBench: Evaluating Your
    LLMs with a Code Interpreter Plugin").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提高质量，我们聘请了几位专家，他们使用代码解释器进行人工双重检查。这种方法确保了问题是由真实用户编写的，并且控制了各种因素，如运行时间。有关质量控制规则的详细信息，请参阅附录
    [B](#A2.SS0.SSS0.Px3 "质量控制规则 ‣ 附录 B 构建提示和规则 ‣ CIBench: 使用代码解释器插件评估你的 LLM")。数据集的统计信息也提供在附录
    [A.2](#A1.SS2 "A.2 数据集统计 ‣ 附录 A 数据集详细信息 ‣ CIBench: 使用代码解释器插件评估你的 LLM")。'
- en: '![Refer to caption](img/fcfedc0d9a277b28260609b80ce6ab40.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fcfedc0d9a277b28260609b80ce6ab40.png)'
- en: 'Figure 4: Evaluation modes: In end-to-end mode, the LLM addresses the user’s
    question (bottom) within the context of its response, while in oracle mode, it
    answers the user’s question (bottom) within the context of ground truth.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：评估模式：在端到端模式下，LLM 在其响应的上下文中处理用户的问题（底部），而在oracle模式下，它在真实情况的上下文中回答用户的问题（底部）。
- en: 3.2 Evaluation Modes and Metrics
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估模式与指标
- en: Evaluation Modes
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估模式
- en: 'As shown in Fig.[4](#S3.F4 "Figure 4 ‣ Human Refinement ‣ 3.1 Evaluation Dataset
    ‣ 3 CIBench ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin"),
    CIBench  includes the end-to-end and oracle mode. It not only assesses the model’s
    proficiency in autonomously resolving continuous problems but also assesses its
    capacity in conjunction with human interaction. In end-to-end mode, the model
    must solve the problem by itself. Each subsequent response is contingent upon
    the model’s previous outcomes, necessitating self-correction based on code interpreter
    feedback. What’s more, to reflect the real-world scenarios where human assistance
    is available, we introduce oracle mode to incorporate ground truth code and thought
    as context, helping the model to address subsequent tasks. Conceptually, oracle
    mode emulates a few-shot testing or in-context learning scenario, providing the
    model with comprehensive context to facilitate a more precise solution.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[4](#S3.F4 "Figure 4 ‣ Human Refinement ‣ 3.1 Evaluation Dataset ‣ 3 CIBench
    ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin")所示，CIBench包括端到端模式和oracle模式。它不仅评估模型在自主解决连续问题方面的熟练程度，还评估其与人类互动结合的能力。在端到端模式下，模型必须独立解决问题。每个后续的回答都依赖于模型先前的结果，需要根据代码解释器的反馈进行自我纠正。此外，为了反映现实世界中有人工协助的场景，我们引入了oracle模式，以将真实代码和思路作为背景，帮助模型处理后续任务。从概念上讲，oracle模式模拟了少量测试或上下文学习场景，为模型提供了全面的上下文，以促进更精确的解决方案。'
- en: Evaluation Metrics
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标
- en: 'In two evaluation modes, we introduce two types of evaluation metrics: process-oriented
    and output-oriented, to provide a comprehensive analysis of the model’s performance.
    Process-oriented metrics focus on the correct invocation of tools and the successful
    compilation and execution of code. These metrics include the Tool Call Rate, which
    measures the proportion of instances where the model correctly follows the instructions
    to invoke a code interpreter, and the Executable Rate, which indicates the percentage
    of code that is executed without any errors.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种评估模式下，我们引入了两种类型的评估指标：过程导向型和输出导向型，以全面分析模型的表现。过程导向型指标关注工具的正确调用以及代码的成功编译和执行。这些指标包括工具调用率，该指标测量模型正确遵循指令调用代码解释器的比例，以及可执行率，该指标表示没有错误的代码执行的百分比。
- en: On the other hand, output-oriented metrics focus on the outcomes of the model.
    These metrics include Numeric Accuracy, which assesses the accuracy of the numerical
    results; Text Score, which measures the quality of the structural text output
    using the Rouge metric [[19](#bib.bib19)]; and Visualization Score, which evaluates
    the quality of visual output. Instead of using GPT-4V like Qwen-Agent, which is
    expensive and ineffective, we propose using structural similarities [[28](#bib.bib28)]
    between predictions and ground truth images as the visualization score. These
    metrics provide a holistic evaluation of the LLM’s capabilities.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，输出导向型指标关注模型的输出结果。这些指标包括数值准确性，该指标评估数值结果的准确性；文本评分，该指标使用Rouge度量来衡量结构化文本输出的质量[[19](#bib.bib19)]；以及可视化评分，该指标评估视觉输出的质量。我们建议使用预测与真实图像之间的结构相似性[[28](#bib.bib28)]作为可视化评分，而不是像Qwen-Agent那样昂贵且效果不佳的GPT-4V。这些指标提供了对LLM能力的全面评估。
- en: 4 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experiments Setup
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'To provide a thorough analysis, we evaluate 19 chat models, including popular
    open-sourced LLMs and the private GPT-4, using the CIBench benchmark. During inference,
    we allow LLMs to attempt up to 3 times. The specific versions of Python modules
    utilized in the code interpreter are provided in Appendix [A.1](#A1.SS1 "A.1 Module
    Version Settings ‣ Appendix A Dataset Details ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin"). All experiments are conducted within the OpenCompass [[9](#bib.bib9)]
    evaluation platform.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进行全面分析，我们使用CIBench基准评估了19个聊天模型，包括流行的开源LLMs和私人GPT-4。在推理过程中，我们允许LLMs尝试最多3次。代码解释器中使用的Python模块的具体版本在附录[A.1](#A1.SS1
    "A.1 Module Version Settings ‣ Appendix A Dataset Details ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin")中提供。所有实验均在OpenCompass [[9](#bib.bib9)]评估平台上进行。'
- en: 'Table 3: Main results of CIBench. Tool, Exe, Num, Text, and Vis denote the
    tool call rate, executable rate, numeric accuracy, text score, and visualization
    score respectively. bold denotes the best score among the same model scale. Average
    is the mean of Num, Text, and Vis in two modes..'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：CIBench 的主要结果。Tool、Exe、Num、Text 和 Vis 分别表示工具调用率、可执行率、数值准确度、文本评分和可视化评分。**粗体**表示同一模型规模中的最佳得分。Average
    是两种模式下 Num、Text 和 Vis 的平均值。
- en: '| Model | End-to-end Mode | Oracle Mode | Average |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 端到端模式 | 神谕模式 | 平均值 |'
- en: '| Tool | Exe | Num | Text | Vis | Tool | Exe | Num | Text | Vis |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Tool | Exe | Num | Text | Vis | Tool | Exe | Num | Text | Vis |'
- en: '| Llama-2-7B-Chat | 80.6 | 28.8 | 10.1 | 10.4 | 17.4 | 92.7 | 41.5 | 22.4 |
    31.2 | 24.8 | 19.4 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-7B-Chat | 80.6 | 28.8 | 10.1 | 10.4 | 17.4 | 92.7 | 41.5 | 22.4 |
    31.2 | 24.8 | 19.4 |'
- en: '| Yi-6B-Chat | 90.6 | 50.2 | 22.5 | 21.7 | 31.1 | 99.7 | 61.5 | 29.5 | 40.8
    | 40.2 | 31.0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-Chat | 90.6 | 50.2 | 22.5 | 21.7 | 31.1 | 99.7 | 61.5 | 29.5 | 40.8
    | 40.2 | 31.0 |'
- en: '| ChatGLM3-6B-32K | 77.9 | 48.8 | 18.9 | 33.0 | 29.3 | 98.6 | 55.7 | 28.2 |
    52.0 | 34.2 | 32.6 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32K | 77.9 | 48.8 | 18.9 | 33.0 | 29.3 | 98.6 | 55.7 | 28.2 |
    52.0 | 34.2 | 32.6 |'
- en: '| Vicuna-7B-v1.5-16k | 89.4 | 54.5 | 19.9 | 27.1 | 32.7 | 99.9 | 62.9 | 29.3
    | 43.8 | 43.1 | 32.7 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B-v1.5-16k | 89.4 | 54.5 | 19.9 | 27.1 | 32.7 | 99.9 | 62.9 | 29.3
    | 43.8 | 43.1 | 32.7 |'
- en: '| DeepSeek-7B-Chat | 62.9 | 45.3 | 16.0 | 17.4 | 30.8 | 93.8 | 67.1 | 33.0
    | 56.7 | 48.5 | 33.7 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-7B-Chat | 62.9 | 45.3 | 16.0 | 17.4 | 30.8 | 93.8 | 67.1 | 33.0
    | 56.7 | 48.5 | 33.7 |'
- en: '| Qwen-7B-Chat | 99.8 | 72.2 | 42.3 | 55.7 | 43.9 | 97.9 | 71.9 | 36.6 | 58.1
    | 52.1 | 48.1 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-7B-Chat | 99.8 | 72.2 | 42.3 | 55.7 | 43.9 | 97.9 | 71.9 | 36.6 | 58.1
    | 52.1 | 48.1 |'
- en: '| Mistral-7b-Instruct | 97.5 | 67.2 | 41.9 | 53.6 | 41.7 | 99.2 | 75.7 | 48.0
    | 55.4 | 49.1 | 48.3 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7b-Instruct | 97.5 | 67.2 | 41.9 | 53.6 | 41.7 | 99.2 | 75.7 | 48.0
    | 55.4 | 49.1 | 48.3 |'
- en: '| Qwen1.5-7B-Chat | 99.1 | 69.9 | 35.4 | 59.1 | 39.1 | 99.8 | 75.9 | 48.6 |
    61.7 | 52.1 | 49.3 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-7B-Chat | 99.1 | 69.9 | 35.4 | 59.1 | 39.1 | 99.8 | 75.9 | 48.6 |
    61.7 | 52.1 | 49.3 |'
- en: '| InternLM2-7B-Chat | 99.7 | 77.7 | 52.6 | 64.6 | 51.8 | 99.3 | 70.4 | 48.1
    | 44.3 | 42.9 | 50.7 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7B-Chat | 99.7 | 77.7 | 52.6 | 64.6 | 51.8 | 99.3 | 70.4 | 48.1
    | 44.3 | 42.9 | 50.7 |'
- en: '| DeepSeek-Coder-6.7B | 75.9 | 66.0 | 40.7 | 48.5 | 47.7 | 96.8 | 83.0 | 59.2
    | 77.2 | 62.5 | 56.0 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-Coder-6.7B | 75.9 | 66.0 | 40.7 | 48.5 | 47.7 | 96.8 | 83.0 | 59.2
    | 77.2 | 62.5 | 56.0 |'
- en: '| Llama-3-8B-Instruct | 98.5 | 78.6 | 55.6 | 70.8 | 54.6 | 95.5 | 84.5 | 60.4
    | 83.7 | 61.4 | 64.4 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B-Instruct | 98.5 | 78.6 | 55.6 | 70.8 | 54.6 | 95.5 | 84.5 | 60.4
    | 83.7 | 61.4 | 64.4 |'
- en: '| Llama-2-13B-Chat | 90.7 | 42.7 | 12.1 | 14.8 | 24.0 | 99.0 | 52.4 | 28.9
    | 31.0 | 29.3 | 23.4 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-13B-Chat | 90.7 | 42.7 | 12.1 | 14.8 | 24.0 | 99.0 | 52.4 | 28.9
    | 31.0 | 29.3 | 23.4 |'
- en: '| Vicuna-13B-v1.5-16k | 91.0 | 62.2 | 25.9 | 45.4 | 35.4 | 99.9 | 72.1 | 39.7
    | 45.2 | 45.9 | 39.6 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B-v1.5-16k | 91.0 | 62.2 | 25.9 | 45.4 | 35.4 | 99.9 | 72.1 | 39.7
    | 45.2 | 45.9 | 39.6 |'
- en: '| Qwen-14B-Chat | 93.1 | 70.9 | 48.2 | 60.7 | 48.7 | 99.2 | 82.4 | 55.5 | 74.6
    | 57.3 | 57.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14B-Chat | 93.1 | 70.9 | 48.2 | 60.7 | 48.7 | 99.2 | 82.4 | 55.5 | 74.6
    | 57.3 | 57.5 |'
- en: '| Qwen1.5-14B-Chat | 99.7 | 84.8 | 56.9 | 63.3 | 51.4 | 99.9 | 92.7 | 61.7
    | 62.6 | 63.1 | 59.8 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-14B-Chat | 99.7 | 84.8 | 56.9 | 63.3 | 51.4 | 99.9 | 92.7 | 61.7
    | 62.6 | 63.1 | 59.8 |'
- en: '| Mixtral-8x7B-Instruct | 99.1 | 83.0 | 45.4 | 55.9 | 53.0 | 99.9 | 88.6 |
    67.5 | 73.5 | 67.0 | 60.4 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B-Instruct | 99.1 | 83.0 | 45.4 | 55.9 | 53.0 | 99.9 | 88.6 |
    67.5 | 73.5 | 67.0 | 60.4 |'
- en: '| InternLM2-20B-Chat | 96.6 | 82.2 | 47.5 | 67.2 | 47.1 | 96.7 | 81.0 | 60.4
    | 85.0 | 56.7 | 60.6 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-20B-Chat | 96.6 | 82.2 | 47.5 | 67.2 | 47.1 | 96.7 | 81.0 | 60.4
    | 85.0 | 56.7 | 60.6 |'
- en: '| Yi-34B-Chat | 92.9 | 66.2 | 42.2 | 40.3 | 42.2 | 99.8 | 77.2 | 68.2 | 72.2
    | 52.7 | 53.0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Yi-34B-Chat | 92.9 | 66.2 | 42.2 | 40.3 | 42.2 | 99.8 | 77.2 | 68.2 | 72.2
    | 52.7 | 53.0 |'
- en: '| Llama-2-70B-Chat | 92.4 | 53.8 | 23.7 | 22.7 | 24.9 | 94.2 | 61.6 | 36.7
    | 36.0 | 35.6 | 29.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-70B-Chat | 92.4 | 53.8 | 23.7 | 22.7 | 24.9 | 94.2 | 61.6 | 36.7
    | 36.0 | 35.6 | 29.9 |'
- en: '| DeepSeek-67B-Chat | 76.9 | 70.7 | 47.5 | 63.3 | 54.8 | 91.1 | 82.8 | 60.6
    | 74.7 | 63.8 | 60.8 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-67B-Chat | 76.9 | 70.7 | 47.5 | 63.3 | 54.8 | 91.1 | 82.8 | 60.6
    | 74.7 | 63.8 | 60.8 |'
- en: '| Qwen-72B-Chat | 98.3 | 87.2 | 60.5 | 73.2 | 55.9 | 99.7 | 90.1 | 62.8 | 73.6
    | 64.9 | 65.1 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72B-Chat | 98.3 | 87.2 | 60.5 | 73.2 | 55.9 | 99.7 | 90.1 | 62.8 | 73.6
    | 64.9 | 65.1 |'
- en: '| Llama-3-70B-Instruct | 98.0 | 91.1 | 64.9 | 55.7 | 63.6 | 96.6 | 82.8 | 72.9
    | 74.2 | 62.0 | 65.5 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B-Instruct | 98.0 | 91.1 | 64.9 | 55.7 | 63.6 | 96.6 | 82.8 | 72.9
    | 74.2 | 62.0 | 65.5 |'
- en: '| gpt-4o | 100.0 | 97.2 | 74.1 | 79.0 | 61.1 | 99.2 | 94.8 | 81.7 | 79.7 |
    71.6 | 74.5 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4o | 100.0 | 97.2 | 74.1 | 79.0 | 61.1 | 99.2 | 94.8 | 81.7 | 79.7 |
    71.6 | 74.5 |'
- en: '| GPT-4-1106-preview | 99.8 | 98.6 | 77.8 | 78.9 | 64.0 | 99.6 | 97.7 | 77.5
    | 81.6 | 73.1 | 75.5 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-1106-preview | 99.8 | 98.6 | 77.8 | 78.9 | 64.0 | 99.6 | 97.7 | 77.5
    | 81.6 | 73.1 | 75.5 |'
- en: 4.2 Main Results
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: 'We categorize the models into distinct groups based on their scales to enable
    more effective comparisons. As illustrated in Table [3](#S4.T3 "Table 3 ‣ 4.1
    Experiments Setup ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin"), LLama-3-8B-Instruct achieves the highest performance in
    the  7B group, while InternLM2-20B-Chat demonstrates the top performance in the
    13B-20B group. Within the 70B group, LLama-3-70B-Instruct secures the leading
    position with an overall score that is 10.0% lower than GPT-4-1106-preview. The
    API model GPT-4-1106-preview and gpt-4o outperform all other models, especially
    in end-to-end mode, highlighting the significant potential for improvement in
    current open-source models. What’s more, larger models tend to exhibit superior
    performance across various metrics, in line with established trends [[3](#bib.bib3),
    [15](#bib.bib15), [29](#bib.bib29)]. Moreover, models within the same series (such
    as Mistral, InternLM, Qwen, Llama3, etc.) consistently maintain relatively stable
    rankings within their respective parameter groups, underscoring the stability
    and efficacy of our approach.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据模型的规模将其分为不同的组，以便进行更有效的比较。如表 [3](#S4.T3 "表 3 ‣ 4.1 实验设置 ‣ 4 实验 ‣ CIBench：用代码解释器插件评估您的
    LLMs") 所示，LLama-3-8B-Instruct 在 7B 组中表现最佳，而 InternLM2-20B-Chat 在 13B-20B 组中表现最佳。在
    70B 组中，LLama-3-70B-Instruct 以其总体得分领先，尽管其得分比 GPT-4-1106-preview 低 10.0%。API 模型
    GPT-4-1106-preview 和 gpt-4o 超越了所有其他模型，特别是在端到端模式下，突显了当前开源模型的显著改进潜力。此外，大型模型往往在各种指标上表现更优，符合已建立的趋势
    [[3](#bib.bib3), [15](#bib.bib15), [29](#bib.bib29)]。此外，同一系列的模型（如 Mistral、InternLM、Qwen、Llama3
    等）在其各自的参数组内始终保持相对稳定的排名，突显了我们方法的稳定性和有效性。
- en: 'When comparing the end-to-end mode and oracle mode, it becomes evident that
    the oracle mode surpasses the end-to-end mode across all metrics for most models.
    This observation suggests that LLMs can achieve better results with human interaction,
    hinting at a promising avenue for integrating LLMs to assist humans in data science.
    The experiment demos are shown in Appendix [C](#A3 "Appendix C Experiment Example
    Demo ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin").'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较端到端模式和 oracle 模式时，可以明显看出，大多数模型在所有指标上，oracle 模式都超过了端到端模式。这一观察结果表明，LLMs 可以通过与人类互动取得更好的效果，暗示了将
    LLMs 融入数据科学以协助人类的一个有前景的途径。实验演示见附录 [C](#A3 "附录 C 实验示例演示 ‣ CIBench：用代码解释器插件评估您的
    LLMs")。
- en: 4.3 Error Mode Analysis
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 错误模式分析
- en: '![Refer to caption](img/fa406ae93588f9ed61860063efc29c24.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa406ae93588f9ed61860063efc29c24.png)'
- en: 'Figure 5: Correlation of CIBench with other benchmarks. The small p-value (top-left)
    and high Pearson correlation coefficients (title) indicate a strong correlation
    between CIBench and IFEval, BBH, GSM8K, MATH, HumanEval, and MBPP. These benchmarks
    evaluate the instruction-following, reasoning, and coding abilities of LLMs, respectively.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：CIBench 与其他基准的相关性。较小的 p 值（左上角）和较高的 Pearson 相关系数（标题）表明，CIBench 与 IFEval、BBH、GSM8K、MATH、HumanEval
    和 MBPP 之间存在强相关性。这些基准分别评估 LLMs 的指令跟随、推理和编码能力。
- en: 'In the evaluation of CIBench, we identify four prevalent types of errors in
    the code generated by the model. These errors are categorized as follows: 1) Instruction
    Following Errors: These encompass instances where the model deviates from or disregards
    provided instructions, reflecting a lack of adherence to specified guidelines;
    2) Hallucination Errors: This category pertains to the phenomenon of the model
    generating code that contains hallucinated elements, such as utilizing undefined
    parameters or referencing irrelevant variables; 3) Reasoning Errors: These errors
    occur when the model encounters complex problems, often resulting in logical errors
    in the generated code. Such errors offer valuable insights into the model’s ability
    to handle intricate tasks in code generation; 4) Code Errors: Basic errors in
    code generation fall under this category. While these errors may sometimes appear
    trivial, they signify potential deficiencies in the model’s code-generation process.
    These identified errors effectively underscore the current limitations of LLMs
    in terms of their coding capabilities, providing valuable insights for the ongoing
    development of CIBench. Since automatically classifying errors is challenging,
    we manually categorized the four types of errors in GPT-4-1106-preview. The proportions
    of these error types are 31.9%, 4.3%, 40.4%, and 23.4%, respectively. Detailed
    examples of these errors are presented in Appendix [E](#A5 "Appendix E Dataset
    Error Analysis ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin").'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在CIBench的评估中，我们识别出了模型生成的代码中四种常见的错误类型。这些错误的分类如下：1）指令遵循错误：这类错误包含模型偏离或忽略给定指令的情况，反映出对指定指导方针的遵循不足；2）幻觉错误：这一类别涉及模型生成的代码中包含虚构元素的现象，如使用未定义的参数或引用不相关的变量；3）推理错误：这些错误发生在模型遇到复杂问题时，通常导致生成代码中的逻辑错误。这些错误提供了关于模型处理复杂任务能力的宝贵见解；4）代码错误：这类错误涉及代码生成中的基本错误。虽然这些错误有时看起来微不足道，但它们意味着模型代码生成过程中的潜在缺陷。这些识别出的错误有效地突出了LLM在编码能力方面的当前局限性，为CIBench的持续开发提供了宝贵的见解。由于自动分类错误具有挑战性，我们在GPT-4-1106-preview中手动对四种错误类型进行了分类。这些错误类型的比例分别为31.9%、4.3%、40.4%和23.4%。这些错误的详细示例在附录[E](#A5
    "Appendix E Dataset Error Analysis ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin")中呈现。'
- en: 'Furthermore, to provide insight into enhancing performance on CIBench, we analyze
    its correlation with existing benchmarks such as IFEval, BBH, GSM8K, MATH, HumanEval,
    and MBPP. These benchmarks evaluate the instruction-following, reasoning, and
    coding abilities of LLMs. As shown in Fig.[5](#S4.F5 "Figure 5 ‣ 4.3 Error Mode
    Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a Code Interpreter
    Plugin"), CIBench performance exhibits a strong correlation with these benchmarks,
    with Pearson correlation coefficients exceeding 0.7, particularly for HumanEval
    and GSM8K. This result indicates that improvements in reasoning and coding can
    boost LLM performance on CIBench.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，为了提供关于提高CIBench性能的见解，我们分析了其与现有基准测试（如IFEval、BBH、GSM8K、MATH、HumanEval和MBPP）的相关性。这些基准测试评估了LLM的指令遵循、推理和编码能力。如图[5](#S4.F5
    "Figure 5 ‣ 4.3 Error Mode Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your
    LLMs with a Code Interpreter Plugin")所示，CIBench的表现与这些基准测试表现出强烈的相关性，皮尔逊相关系数超过0.7，特别是在HumanEval和GSM8K上。这一结果表明，推理和编码能力的提高可以提升LLM在CIBench上的表现。'
- en: 4.4 More Analysis
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 更多分析
- en: Debug Ability Analysis
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调试能力分析
- en: 'In the ReAct protocol, we allow LLMs to try to solve tasks multiple times.
    During each trial, the model can use feedback from the code interpreter to rectify
    any bugs in the generated code. To assess the LLMs’ ability to autonomously correct
    bugs, we vary the number of trials. As shown in Fig.[6](#S4.F6 "Figure 6 ‣ Cross
    Language Analysis ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your
    LLMs with a Code Interpreter Plugin"), increasing the number of trials correlates
    with improvements across all metrics. Significantly, for most LLMs, there is a
    notable enhancement when the number of trials reaches two, particularly evident
    in metrics such as executable rate, numeric accuracy, and visualization score.
    This suggests that the LLM can autonomously rectify bugs to a certain extent.
    In our experiments, to balance evaluation time and performance, we set the number
    of trials to three.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '在 ReAct 协议中，我们允许大型语言模型（LLMs）多次尝试解决任务。在每次尝试过程中，模型可以利用代码解释器的反馈来纠正生成代码中的任何错误。为了评估
    LLMs 自主修复错误的能力，我们改变了尝试的次数。如图[6](#S4.F6 "Figure 6 ‣ Cross Language Analysis ‣ 4.4
    More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a Code Interpreter
    Plugin")所示，增加尝试次数与所有指标的改善相关。显著地，对于大多数 LLMs，当尝试次数达到两次时，尤其在可执行率、数值准确性和可视化得分等指标中，效果显著提升。这表明
    LLM 可以在一定程度上自主修复错误。在我们的实验中，为了平衡评估时间和性能，我们将尝试次数设置为三次。'
- en: Visualization Metric Analysis
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可视化指标分析
- en: 'To validate the effectiveness of our proposed visualization metric, we follow
    QwenAgent [[2](#bib.bib2)] and use GPT-4V to assess visualization scores on a
    subset of CIBench tasks. The prompt is provided in Appendix [D](#A4 "Appendix
    D Subjective Visualization Evaluation ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin"). As shown in Fig.[8](#S4.F8 "Figure 8 ‣ Cross Language Analysis
    ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin"), despite structural similarities being derived from low-level
    features, there is a strong correlation between them and GPT-4V scores, demonstrating
    remarkable consistency between the two metrics. Therefore, we can utilize structural
    similarities as a simplified visualization metric to subject GPT-4V for effective
    analysis.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证我们提出的可视化指标的有效性，我们参考了 QwenAgent [[2](#bib.bib2)] 并使用 GPT-4V 对 CIBench 任务的子集进行可视化得分评估。提示内容见附录
    [D](#A4 "Appendix D Subjective Visualization Evaluation ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin")。如图[8](#S4.F8 "Figure 8 ‣ Cross Language
    Analysis ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with
    a Code Interpreter Plugin")所示，尽管结构相似性来源于低级特征，但它们与 GPT-4V 得分之间存在强相关性，表明这两个指标之间具有显著一致性。因此，我们可以利用结构相似性作为简化的可视化指标，对
    GPT-4V 进行有效分析。'
- en: Cross Language Analysis
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨语言分析
- en: 'To benchmark the LLMs’ ability in Chinese, we created a Chinese version of
    CIBench by translating the human-created tasks into Chinese. This allows us to
    evaluate the Code Interpreter performance in Chinese. As shown in Fig.[8](#S4.F8
    "Figure 8 ‣ Cross Language Analysis ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench:
    Evaluating Your LLMs with a Code Interpreter Plugin"), we observe that: 1) most
    models exhibit a slight decrease in Chinese CIBench  compared to their English
    counterparts.; 2) the strong DeepSeek-67B-Chat and Qwen-72B-Chat drop a lot on
    Chinese  CIBench, compared to the English version. Further research and development
    efforts are necessary to address these discrepancies and improve the performance
    of LLMs in multilingual scenarios.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '为了基准测试 LLMs 在中文中的能力，我们通过将人工创建的任务翻译成中文，创建了中文版本的 CIBench。这使我们能够评估代码解释器在中文中的表现。如图[8](#S4.F8
    "Figure 8 ‣ Cross Language Analysis ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench:
    Evaluating Your LLMs with a Code Interpreter Plugin")所示，我们观察到：1) 与英文版相比，大多数模型在中文
    CIBench 中表现略有下降；2) 强大的 DeepSeek-67B-Chat 和 Qwen-72B-Chat 在中文 CIBench 上下降幅度较大，相比于英文版本。需要进一步的研究和开发工作来解决这些差异，并提高
    LLMs 在多语言场景中的表现。'
- en: '![Refer to caption](img/de5cab8d599d25a182b4e3041e3eec14.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/de5cab8d599d25a182b4e3041e3eec14.png)'
- en: 'Figure 6: Debug ability analysis. Max trial denotes the trial times in ReAct
    protocol.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 调试能力分析。最大尝试次数表示 ReAct 协议中的尝试次数。'
- en: '![Refer to caption](img/09567defa989d2901fd1b293d8221591.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/09567defa989d2901fd1b293d8221591.png)'
- en: 'Figure 7: Structural Similarities v.s. GPT-4V.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 结构相似性对比 GPT-4V。'
- en: '![Refer to caption](img/89ab5c803cf9357fa15c6d23b23d09fe.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/89ab5c803cf9357fa15c6d23b23d09fe.png)'
- en: 'Figure 8: Chinese CIBench  v.s. English CIBench .'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 中文 CIBench 对比 英文 CIBench。'
- en: Difficulty Analysis
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 难度分析
- en: 'CIbench comprises interactive tasks, and we assume that varying interactive
    steps denote distinct levels of difficulty. To demonstrate model performance across
    these difficulty levels, we decompose each task into three categories: easy (up
    to 2 steps), medium (2-4 steps), and difficult (more than 4 steps), and then evaluate
    performance accordingly. As Tab.[10](#S4.F10 "Figure 10 ‣ Limitation ‣ 4.4 More
    Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a Code Interpreter
    Plugin") shows, for most models, the performance of most models decreases as the
    number of steps increases.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'CIbench 包含互动任务，我们假设不同的互动步骤表示不同的难度级别。为了展示模型在这些难度级别上的表现，我们将每个任务分解为三个类别：简单（最多
    2 步）、中等（2-4 步）和困难（超过 4 步），然后进行相应的性能评估。如表 [10](#S4.F10 "Figure 10 ‣ Limitation
    ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin") 所示，对于大多数模型，随着步骤数量的增加，大多数模型的性能会下降。'
- en: Different Category Modules Analysis
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同类别模块分析
- en: 'We assess the capabilities of different LLMs with various category modules
    (refer to Tab. [2](#S3.T2 "Table 2 ‣ 3 CIBench ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin")). As shown in Fig. [10](#S4.F10 "Figure 10 ‣
    Limitation ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin"), LLMs demonstrate proficiency in tackling mathematical
    and statistical tasks utilizing SciPy modules. However, open-sourced LLMs encounter
    challenges when handling modeling tasks that demand advanced coding and reasoning
    skills. We hope that future open-sourced LLMs will excel in modeling tasks.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过不同类别的模块评估了不同 LLM 的能力（参见表 [2](#S3.T2 "Table 2 ‣ 3 CIBench ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin")）。如图 [10](#S4.F10 "Figure 10 ‣ Limitation
    ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin") 所示，LLM 在处理数学和统计任务时，能够熟练地利用 SciPy 模块。然而，开源 LLM 在处理需要高级编码和推理技能的建模任务时遇到挑战。我们希望未来的开源
    LLM 能在建模任务中表现出色。'
- en: Limitation
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制
- en: 'Our work has two main limitations: 1) CIBench is currently limited to Python,
    despite it could be extended to include other programming languages using a similar
    methodology; 2) the evaluation metric of CIBench has limitations in measuring
    certain data science tasks, such as "training a model with PyTorch" and tasks
    involving randomness.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作有两个主要限制：1）CIBench 目前仅限于 Python，尽管它可以通过类似的方法扩展到其他编程语言；2）CIBench 的评估指标在测量某些数据科学任务时存在限制，例如“用
    PyTorch 训练模型”和涉及随机性的任务。
- en: '| Model | Easy | Medium | Hard |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 简单 | 中等 | 困难 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Yi-6B-chat | 37.5 | 33.1 | 23.1 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-chat | 37.5 | 33.1 | 23.1 |'
- en: '| Llama-2-7B-chat | 25.7 | 19.2 | 15.7 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-7B-chat | 25.7 | 19.2 | 15.7 |'
- en: '| Qwen-7B-chat | 51.0 | 45.5 | 36.5 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-7B-chat | 51.0 | 45.5 | 36.5 |'
- en: '| Vicuna-7B-v1.5-16k | 39.0 | 35.9 | 23.8 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B-v1.5-16k | 39.0 | 35.9 | 23.8 |'
- en: '| DeepSeek-7B-chat | 38.8 | 41.8 | 22.8 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-7B-chat | 38.8 | 41.8 | 22.8 |'
- en: '| Mistral-7B-instruct-v0.2 | 52.3 | 52.4 | 33.0 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-instruct-v0.2 | 52.3 | 52.4 | 33.0 |'
- en: '| Chatglm3-6B-32k | 43.2 | 33.3 | 19.0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Chatglm3-6B-32k | 43.2 | 33.3 | 19.0 |'
- en: '| Mistral-8x7B-instruct-v0.1 | 62.1 | 61.2 | 47.6 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-8x7B-instruct-v0.1 | 62.1 | 61.2 | 47.6 |'
- en: '| Internlm2-chat-7B | 55.5 | 52.0 | 33.9 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Internlm2-chat-7B | 55.5 | 52.0 | 33.9 |'
- en: '| Qwen-14B-chat | 59.6 | 58.8 | 42.0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14B-chat | 59.6 | 58.8 | 42.0 |'
- en: '| Vicuna-13B-v1.5-16k | 46.1 | 41.6 | 27.7 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B-v1.5-16k | 46.1 | 41.6 | 27.7 |'
- en: '| Llama-2-13B-chat | 34.8 | 23.5 | 18.7 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-13B-chat | 34.8 | 23.5 | 18.7 |'
- en: '| Internlm2-chat-20B | 51.5 | 59.2 | 45.1 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Internlm2-chat-20B | 51.5 | 59.2 | 45.1 |'
- en: '| Yi-34b-chat | 55.2 | 58.1 | 38.0 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Yi-34b-chat | 55.2 | 58.1 | 38.0 |'
- en: '| Llama-2-70B-chat | 38.1 | 33.9 | 18.6 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-70B-chat | 38.1 | 33.9 | 18.6 |'
- en: '| DeepSeek-67B-chat | 60.4 | 63.9 | 43.7 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-67B-chat | 60.4 | 63.9 | 43.7 |'
- en: '| Qwen-72B-chat | 60.7 | 66.2 | 50.3 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72B-chat | 60.7 | 66.2 | 50.3 |'
- en: '| Llama-3-70B-chat | 62.3 | 67.1 | 42.0 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B-chat | 62.3 | 67.1 | 42.0 |'
- en: 'Figure 9: Performance on different difficulty levels. Easy means 4 steps.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：不同难度水平的表现。Easy 表示 4 步。
- en: '![Refer to caption](img/27a8a0717dea5b9392bc7e55b8310b15.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/27a8a0717dea5b9392bc7e55b8310b15.png)'
- en: 'Figure 10: Performance on different categories.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：不同类别的表现。
- en: 5 Conclusion
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'We propose a novel benchmark, named CIBench , to comprehensively assess LLMs’
    ability to leverage code interpreters for complex data science tasks. It includes
    an evaluation dataset covering widely used Python modules in data science and
    two evaluation modes measuring LLMs’ ability with and without human assistance.
    The evaluation dataset is constructed using an LLM-human cooperative approach,
    leveraging interactive IPython sessions to simulate realistic scenarios in data
    science. Thorough experimental analysis with 24 LLMs on CIBench indicates that
    LLMs perform poorly in modeling category modules. Based on our experimental analysis,
    we offer several insights for the future development of LLMs: enhancing the LLM’s
    ability to correct errors based on feedback, improving its understanding of user
    intentions across multiple rounds of interactions, and, most importantly, strengthening
    its reasoning capabilities.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一个新颖的基准测试，名为 CIBench ，旨在全面评估大语言模型（LLMs）利用代码解释器进行复杂数据科学任务的能力。该基准测试包括一个评估数据集，覆盖了数据科学中广泛使用的
    Python 模块，并提供了两种评估模式，分别测量 LLMs 在有无人工辅助情况下的能力。评估数据集采用 LLM-人类合作的方法构建，利用交互式 IPython
    会话模拟数据科学中的实际场景。对 24 个 LLM 在 CIBench 上进行的全面实验分析表明，LLMs 在建模分类模块方面表现不佳。基于我们的实验分析，我们对
    LLMs 的未来发展提出了几点见解：增强 LLMs 根据反馈纠错的能力，提升其在多轮交互中的理解用户意图的能力，以及最重要的，强化其推理能力。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
    David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.
    Program synthesis with large language models, 2021.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
    David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, 和 Charles Sutton.
    使用大型语言模型的程序合成, 2021年。'
- en: '[2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang
    Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
    Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang
    Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang,
    Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng
    Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,
    Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
    Zhu. Qwen technical report, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang
    Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
    Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang
    Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang,
    Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng
    Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,
    Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, 和 Tianhang
    Zhu. Qwen 技术报告, 2023年。'
- en: '[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
    Language models are few-shot learners, 2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario Amodei.
    语言模型是少量样本学习者, 2020年。'
- en: '[4] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun
    Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv
    preprint arXiv:2403.17297, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun
    Chen, Zehui Chen, Zhi Chen, Pei Chu 等. Internlm2 技术报告. arXiv 预印本 arXiv:2403.17297,
    2024年。'
- en: '[5] Harrison Chase. LangChain, October 2022.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Harrison Chase. LangChain, 2022年10月。'
- en: '[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
    Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex
    Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry,
    Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
    Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski
    Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
    Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
    Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
    Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec
    Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,
    Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
    Evaluating large language models trained on code, 2021.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
    Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
    Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry,
    Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
    Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski
    Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
    Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
    Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
    Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec
    Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,
    Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, 和 Wojciech Zaremba.
    评估在代码上训练的大型语言模型，2021年。'
- en: '[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna:
    An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https://vicuna.lmsys.org,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez 等人. Vicuna: 一个开源聊天机器人以90%*
    chatgpt质量打动gpt-4。 https://vicuna.lmsys.org，2023年。'
- en: '[8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, and John Schulman. Training verifiers to solve math word problems,
    2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman. 训练验证者解决数学文字问题，2021年。'
- en: '[9] OpenCompass Contributors. Opencompass: A universal evaluation platform
    for foundation models. [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass),
    2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] OpenCompass Contributors. Opencompass: 一个通用的基础模型评估平台。 [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass)，2023年。'
- en: '[10] DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism,
    2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] DeepSeek-AI. Deepseek llm: 通过长期主义扩展开源语言模型，2024年。'
- en: '[11] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang,
    Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International
    Conference on Machine Learning, pages 10764–10799\. PMLR, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang,
    Jamie Callan, 和 Graham Neubig. Pal: 程序辅助语言模型。发表于国际机器学习会议，页码10764–10799。PMLR，2023年。'
- en: '[12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart,
    Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving
    with the math dataset, 2021.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart,
    Eric Tang, Dawn Song, 和 Jacob Steinhardt. 用数学数据集测量数学问题解决能力，2021年。'
- en: '[13] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao
    Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt:
    Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352,
    2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao
    Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou 等人. Metagpt: 面向多智能体协作框架的元编程。arXiv
    预印本 arXiv:2308.00352，2023年。'
- en: '[14] Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Guoyin Wang, Xuwu Wang, Jing
    Su, Jingjing Xu, Ming Zhu, Yao Cheng, et al. Infiagent-dabench: Evaluating agents
    on data analysis tasks. arXiv preprint arXiv:2401.05507, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Guoyin Wang, Xuwu Wang, Jing
    Su, Jingjing Xu, Ming Zhu, Yao Cheng 等人. Infiagent-dabench: 在数据分析任务上评估代理。arXiv
    预印本 arXiv:2401.05507，2024年。'
- en: '[15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,
    Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws
    for neural language models, 2020.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,
    Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和 Dario Amodei. 神经语言模型的扩展法则，2020年。'
- en: '[16] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer,
    Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable
    benchmark for data science code generation. In International Conference on Machine
    Learning, pages 18319–18345\. PMLR, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer,
    Wen-tau Yih, Daniel Fried, Sida Wang 和 Tao Yu。Ds-1000: 一个自然且可靠的数据科学代码生成基准。在国际机器学习大会上，第18319–18345页，PMLR，2023年。'
- en: '[17] Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai
    Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang,
    Fei Huang, and Jingren Zhou. Modelscope-agent: Building your customizable agent
    system with open-source large language models, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai
    Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang,
    Fei Huang 和 Jingren Zhou。Modelscope-agent: 使用开源大型语言模型构建可自定义的代理系统，2023年。'
- en: '[18] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large
    scale language model society. arXiv preprint arXiv:2303.17760, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin
    和 Bernard Ghanem。Camel: 用于大型语言模型社会的“心智”探索的交流代理。arXiv 预印本 arXiv:2303.17760，2023年。'
- en: '[19] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.
    In Text summarization branches out, pages 74–81, 2004.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Chin-Yew Lin。Rouge: 一个用于自动评估摘要的工具包。在《文本摘要的分支》，第74–81页，2004年。'
- en: '[20] Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe
    Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating
    the theory and application proficiency of llms with a hierarchical mathematics
    benchmark. arXiv preprint arXiv:2405.12209, 2024.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe
    Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin 和 Kai Chen。Mathbench: 用于评估大型语言模型理论和应用能力的分层数学基准。arXiv
    预印本 arXiv:2405.12209，2024年。'
- en: '[21] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
    Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for
    code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
    Silvio Savarese 和 Caiming Xiong。Codegen: 一个用于代码的开放大型语言模型，支持多轮程序合成。arXiv 预印本 arXiv:2203.13474，2022年。'
- en: '[22] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu 和 Maosong Sun。用于软件开发的交流代理。arXiv 预印本 arXiv:2307.07924，2023年。'
- en: '[23] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai
    Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,
    Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    Toolllm: Facilitating large language models to master 16000+ real-world apis,
    2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai
    Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,
    Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu 和 Maosong Sun。Toolllm:
    帮助大型语言模型掌握16000+个真实世界的API，2023年。'
- en: '[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models
    can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Luke Zettlemoyer, Nicola Cancedda 和 Thomas Scialom。Toolformer: 语言模型可以自我学习使用工具。arXiv
    预印本 arXiv:2302.04761，2023年。'
- en: '[25] Significant Gravitas. AutoGPT.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Significant Gravitas。AutoGPT。'
- en: '[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等人。Llama 2: 开放的基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288，2023年。'
- en: '[27] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng,
    and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language
    feedback, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng
    和 Heng Ji。Mint: 在与工具和语言反馈的多轮互动中评估大型语言模型，2023年。'
- en: '[28] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality
    assessment: from error visibility to structural similarity. IEEE Transactions
    on Image Processing, 13(4):600–612, 2004.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Zhou Wang, A.C. Bovik, H.R. Sheikh 和 E.P. Simoncelli。图像质量评估: 从误差可见性到结构相似性。IEEE
    图像处理汇刊，13(4):600–612，2004年。'
- en: '[29] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
    Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi,
    Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
    Emergent abilities of large language models, 2022.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 韦杰森, 袁怡, 博马萨尼·瑞希, 拉菲尔·科林, 佐夫·巴雷特, 博尔戈德·塞巴斯蒂安, 约加塔玛·达尼, 博斯马·马尔滕, 周登尼, 梅茨勒·唐纳德,
    Chi Ed H., 哈希莫托·辰纪, 维尼尔斯·奥里奥尔, 梁佩西, 迪恩·杰夫, 和费杜斯·威廉。大型语言模型的突现能力, 2022。'
- en: '[30] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen
    llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155,
    2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 吴青云, 甘甘·班萨尔, 张洁瑜, 吴怡然, 张绍坤, 朱尔康, 李贝宾, 姜丽, 张晓云, 和王迟。Autogen: 通过多智能体对话框架实现下一代llm应用。arXiv预印本arXiv:2308.08155,
    2023。'
- en: '[31] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 姚顺宇, 赵杰弗里, 于甸, 杜楠, 沙弗兰·伊扎克, 纳拉希斯南·卡尔蒂克, 和曹源。React: 在语言模型中协同推理和行动, 2023。'
- en: '[32] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye
    Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open
    math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332,
    2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 应怀远, 张硕, 李林扬, 周哲坚, 邵云帆, 费朝晔, 马一川, 洪佳伟, 刘奎昆, 王子怡, 等。Internlm-math: 面向可验证推理的开放数学大型语言模型。arXiv预印本arXiv:2402.06332,
    2024。'
- en: '[33] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing
    Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems
    using gpt-4 code interpreter with code-based self-verification. arXiv preprint
    arXiv:2308.07921, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 周奥君, 王可, 卢子木, 施伟康, 罗思春, 秦子鹏, 陆绍清, 贾安雅, 宋林奇, 詹名杰, 等。使用gpt-4代码解释器和基于代码的自我验证解决具有挑战性的数学词汇问题。arXiv预印本arXiv:2308.07921,
    2023。'
- en: '[34] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa:
    A dataset for llm question answering with external tools. arXiv preprint arXiv:2306.13304,
    2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 庄宇晨, 于越, 王宽, 孙昊天, 和张超。Toolqa: 一个用于llm问答与外部工具的数据集。arXiv预印本arXiv:2306.13304,
    2023。'
- en: Checklist
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查清单
- en: 'The checklist follows the references. Please read the checklist guidelines
    carefully for information on how to answer these questions. For each question,
    change the default [TODO] to [Yes] , [No] , or [N/A] . You are strongly encouraged
    to include a justification to your answer, either by referencing the appropriate
    section of your paper or providing a brief inline description. For example:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 检查清单在参考文献之后。请仔细阅读检查清单指南以获取有关如何回答这些问题的信息。对于每个问题，将默认的[TODO]更改为[是]、[否]或[N/A]。强烈建议您为您的回答提供理由，既可以引用论文的相关部分，也可以提供简要的内联描述。例如：
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Did you include the license to the code and datasets? [Yes] See
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您是否包含了代码和数据集的许可证？[是] 参见
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Did you include the license to the code and datasets? [No] The code and the
    data are proprietary.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您是否包含了代码和数据集的许可证？[否] 代码和数据是专有的。
- en: •
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Did you include the license to the code and datasets? [N/A]
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您是否包含了代码和数据集的许可证？[N/A]
- en: Please do not modify the questions and only use the provided macros for your
    answers. Note that the Checklist section does not count towards the page limit.
    In your paper, please delete this instructions block and only keep the Checklist
    section heading above along with the questions/answers below.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要修改问题，只使用提供的宏来回答。请注意，检查清单部分不计入页面限制。在您的论文中，请删除此指令块，仅保留上方的检查清单部分标题以及下面的问题/答案。
- en: '1.'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: For all authors…
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于所有作者……
- en: (a)
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （a）
- en: Do the main claims made in the abstract and introduction accurately reflect
    the paper’s contributions and scope? [Yes] See Introduction
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要和引言中提出的主要声明是否准确反映了论文的贡献和范围？[是] 参见引言
- en: (b)
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （b）
- en: 'Did you describe the limitations of your work? [Yes] See Sec.[4.4](#S4.SS4.SSS0.Px6
    "Limitation ‣ 4.4 More Analysis ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin")'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '您是否描述了您工作的局限性？[是] 参见第[4.4节](#S4.SS4.SSS0.Px6 "限制 ‣ 4.4 更多分析 ‣ 4 实验 ‣ CIBench:
    使用代码解释器插件评估您的LLMs")'
- en: (c)
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （c）
- en: Did you discuss any potential negative societal impacts of your work? [N/A]
    Our paper proposes a dataset to measure the programming ability of large language
    models. There is currently no negative social impact.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您是否讨论了您工作可能带来的负面社会影响？[N/A] 我们的论文提出了一个用于测量大型语言模型编程能力的数据集。目前没有负面社会影响。
- en: (d)
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （d）
- en: Have you read the ethics review guidelines and ensured that your paper conforms
    to them? [Yes] See Appendix
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否阅读了伦理审查指南并确保你的论文符合这些指南？ [Yes] 见附录
- en: '2.'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: If you are including theoretical results…
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你包括了理论结果……
- en: (a)
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: Did you state the full set of assumptions of all theoretical results? [N/A]
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否陈述了所有理论结果的完整假设集合？ [N/A]
- en: (b)
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Did you include complete proofs of all theoretical results? [N/A]
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否包括了所有理论结果的完整证明？ [N/A]
- en: '3.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: If you ran experiments (e.g. for benchmarks)…
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你进行了实验（例如，为了基准测试）……
- en: (a)
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: Did you include the code, data, and instructions needed to reproduce the main
    experimental results (either in the supplemental material or as a URL)? [Yes]
    [https://github.com/open-compass/CIBench](https://github.com/open-compass/CIBench)
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否包括了重现主要实验结果所需的代码、数据和说明（在补充材料中或作为URL）？ [Yes] [https://github.com/open-compass/CIBench](https://github.com/open-compass/CIBench)
- en: (b)
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Did you specify all the training details (e.g., data splits, hyperparameters,
    how they were chosen)? [Yes] [https://github.com/open-compass/CIBench](https://github.com/open-compass/CIBench)
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否指定了所有的训练细节（例如，数据拆分、超参数、如何选择它们）？ [Yes] [https://github.com/open-compass/CIBench](https://github.com/open-compass/CIBench)
- en: (c)
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: Did you report error bars (e.g., with respect to the random seed after running
    experiments multiple times)? [Yes] The experiments are conducted at a temperature
    of 0.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否报告了误差条（例如，关于运行实验多次后的随机种子）？ [Yes] 实验在温度为0的条件下进行。
- en: (d)
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: 'Did you include the total amount of compute and the type of resources used
    (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See [4.1](#S4.SS1
    "4.1 Experiments Setup ‣ 4 Experiments ‣ CIBench: Evaluating Your LLMs with a
    Code Interpreter Plugin")'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否包括了计算总量和使用的资源类型（例如，GPU类型、内部集群或云提供商）？ [Yes] 见 [4.1](#S4.SS1 "4.1 实验设置 ‣ 4
    实验 ‣ CIBench：使用代码解释器插件评估你的LLMs")
- en: '4.'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: If you are using existing assets (e.g., code, data, models) or curating/releasing
    new assets…
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你使用现有资源（例如，代码、数据、模型）或策划/发布新资源……
- en: (a)
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: If your work uses existing assets, did you cite the creators? [N/A]
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你的工作使用了现有资源，你是否引用了创作者？ [N/A]
- en: (b)
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Did you mention the license of the assets? [N/A]
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否提到资源的许可情况？ [N/A]
- en: (c)
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: Did you include any new assets either in the supplemental material or as a URL?
    [N/A]
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否在补充材料或作为URL中包含了任何新资源？ [N/A]
- en: (d)
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: Did you discuss whether and how consent was obtained from people whose data
    you’re using/curating? [N/A]
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否讨论了如何从数据提供者那里获得同意？ [N/A]
- en: (e)
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (e)
- en: Did you discuss whether the data you are using/curating contains personally
    identifiable information or offensive content? [N/A]
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否讨论了你使用/策划的数据是否包含个人身份信息或冒犯性内容？ [N/A]
- en: '5.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: If you used crowdsourcing or conducted research with human subjects…
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你使用了众包或进行人类受试者研究……
- en: (a)
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: Did you include the full text of instructions given to participants and screenshots,
    if applicable? [Yes] See Appendix
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否包括了给参与者的指令全文和截图（如适用）？ [Yes] 见附录
- en: (b)
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Did you describe any potential participant risks, with links to Institutional
    Review Board (IRB) approvals, if applicable? [N/A]
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否描述了任何潜在的参与者风险，并提供了机构审查委员会（IRB）批准的链接（如适用）？ [N/A]
- en: (c)
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: Did you include the estimated hourly wage paid to participants and the total
    amount spent on participant compensation? [N/A]
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否包括了支付给参与者的估计时薪和总补偿金额？ [N/A]
- en: Appendix A Dataset Details
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 数据集详情
- en: A.1 Module Version Settings
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 模块版本设置
- en: 'The version of Python modules used in code interpreters is listed in Tab.[4](#A1.T4
    "Table 4 ‣ A.1 Module Version Settings ‣ Appendix A Dataset Details ‣ CIBench:
    Evaluating Your LLMs with a Code Interpreter Plugin").'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 代码解释器中使用的Python模块版本列在表[4](#A1.T4 "表4 ‣ A.1 模块版本设置 ‣ 附录A 数据集详情 ‣ CIBench：使用代码解释器插件评估你的LLMs")中。
- en: 'Table 4: The module version settings in CIBench.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：CIBench中的模块版本设置。
- en: '| Module | Version |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 模块 | 版本 |'
- en: '| --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Pandas | 1.5.3 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Pandas | 1.5.3 |'
- en: '| Matplotlib | 3.7.2 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Matplotlib | 3.7.2 |'
- en: '| Seaborn | 0.13.0 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Seaborn | 0.13.0 |'
- en: '| Scikit-learn | 1.2.1 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Scikit-learn | 1.2.1 |'
- en: '| PyTorch | 1.13.1 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | 1.13.1 |'
- en: '| TensorFlow | 2.14.0 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow | 2.14.0 |'
- en: '| LightGBM | 4.1.0 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| LightGBM | 4.1.0 |'
- en: '| NLTK | 3.8 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| NLTK | 3.8 |'
- en: '| PyTorch | 1.131 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | 1.131 |'
- en: '| OpenCV-Python | 4.8.1.78 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| OpenCV-Python | 4.8.1.78 |'
- en: '| SciPy | 1.11.2 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| SciPy | 1.11.2 |'
- en: A.2 Dataset Statistics
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 数据集统计
- en: 'The CIBench comprises generation tasks, template tasks, and Chinese template
    tasks, which produce three types of output: numerical, text, and visualization.
    The statistics of CIBench are shown in Tab.[5](#A1.T5 "Table 5 ‣ A.2 Dataset Statistics
    ‣ Appendix A Dataset Details ‣ CIBench: Evaluating Your LLMs with a Code Interpreter
    Plugin").'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 'CIBench 包含生成任务、模板任务和中文模板任务，这些任务产生三种类型的输出：数值、文本和可视化。CIBench 的统计数据见表 [5](#A1.T5
    "表 5 ‣ A.2 数据集统计 ‣ 附录 A 数据集详情 ‣ CIBench: 使用代码解释器插件评估你的 LLM")。'
- en: 'Table 5: Dataset statistics of CIBench. "generation", "template", and "template_cn"
    represent generation tasks, template tasks, and Chinese template tasks, respectively.
    Other refers to the questions that only require successful execution without any
    output comparison.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: CIBench 数据集统计。 "generation"、"template" 和 "template_cn" 分别代表生成任务、模板任务和中文模板任务。其他指的是仅需要成功执行而不需要任何输出比较的问题。'
- en: '| Subset | Num | Text | Vis | Other | Total |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 子集 | 数量 | 文本 | 视觉 | 其他 | 总计 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| generation | 210 | 76 | 466 | 208 | 960 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| generation | 210 | 76 | 466 | 208 | 960 |'
- en: '| template | 147 | 20 | 161 | 142 | 470 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| template | 147 | 20 | 161 | 142 | 470 |'
- en: '| template_cn | 147 | 20 | 161 | 142 | 470 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| template_cn | 147 | 20 | 161 | 142 | 470 |'
- en: '| total | 504 | 116 | 788 | 492 | 1900 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 504 | 116 | 788 | 492 | 1900 |'
- en: Appendix B Construction Prompts and Rules
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 构建提示和规则
- en: Topic Generation
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主题生成
- en: 'The prompt used for topic generation is shown in Fig. [13](#A7.F13 "Figure
    13 ‣ Appendix G Ethical Consideration ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin").'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '用于主题生成的提示见图 [13](#A7.F13 "图 13 ‣ 附录 G 伦理考虑 ‣ CIBench: 使用代码解释器插件评估你的 LLM")。'
- en: Question Refinement
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题优化
- en: 'The prompts used for question refinement are shown in Fig. [14](#A7.F14 "Figure
    14 ‣ Appendix G Ethical Consideration ‣ CIBench: Evaluating Your LLMs with a Code
    Interpreter Plugin"), [15](#A7.F15 "Figure 15 ‣ Appendix G Ethical Consideration
    ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin").'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '用于问题优化的提示见图 [14](#A7.F14 "图 14 ‣ 附录 G 伦理考虑 ‣ CIBench: 使用代码解释器插件评估你的 LLM")，
    [15](#A7.F15 "图 15 ‣ 附录 G 伦理考虑 ‣ CIBench: 使用代码解释器插件评估你的 LLM")。'
- en: Quality Control Rules
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 质量控制规则
- en: 'We include manual quality checking in the final steps due to limitations in
    the LLMs’ ability to control runtime or file size. The rules used for quality
    control are shown in Fig.[16](#A7.F16 "Figure 16 ‣ Appendix G Ethical Consideration
    ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin").'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 LLM 在控制运行时间或文件大小方面的能力有限，我们在最后步骤中包括了手动质量检查。用于质量控制的规则见图 [16](#A7.F16 "图 16
    ‣ 附录 G 伦理考虑 ‣ CIBench: 使用代码解释器插件评估你的 LLM")。'
- en: Appendix C Experiment Example Demo
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 实验示例演示
- en: 'An example of the experiment is shown in Fig. [11](#A7.F11 "Figure 11 ‣ Appendix
    G Ethical Consideration ‣ CIBench: Evaluating Your LLMs with a Code Interpreter
    Plugin") and Fig. [12](#A7.F12 "Figure 12 ‣ Appendix G Ethical Consideration ‣
    CIBench: Evaluating Your LLMs with a Code Interpreter Plugin").'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '实验示例见图 [11](#A7.F11 "图 11 ‣ 附录 G 伦理考虑 ‣ CIBench: 使用代码解释器插件评估你的 LLM") 和图 [12](#A7.F12
    "图 12 ‣ 附录 G 伦理考虑 ‣ CIBench: 使用代码解释器插件评估你的 LLM")。'
- en: Appendix D Subjective Visualization Evaluation
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 主观可视化评估
- en: 'The prompt used for subjective visualization scoring is shown in Fig.[17](#A7.F17
    "Figure 17 ‣ Appendix G Ethical Consideration ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin").'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '用于主观可视化评分的提示见图 [17](#A7.F17 "图 17 ‣ 附录 G 伦理考虑 ‣ CIBench: 使用代码解释器插件评估你的 LLM")。'
- en: Appendix E Dataset Error Analysis
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 数据集错误分析
- en: 'In the evaluation of CIBench, we identify four prevalent types of errors in
    the code generated by the model. These errors are categorized as follows: 1) Instruction
    Following Errors (Fig.[18](#A7.F18 "Figure 18 ‣ Appendix G Ethical Consideration
    ‣ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin")): These encompass
    instances where the model deviates from or disregards provided instructions, reflecting
    a lack of adherence to specified guidelines; 2) Hallucination Errors (Fig.[19](#A7.F19
    "Figure 19 ‣ Appendix G Ethical Consideration ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin")): This category pertains to the phenomenon of
    the model generating code that contains hallucinated elements, such as utilizing
    undefined parameters or referencing irrelevant variables; 3) Reasoning Errors
    (Fig.[20](#A7.F20 "Figure 20 ‣ Appendix G Ethical Consideration ‣ CIBench: Evaluating
    Your LLMs with a Code Interpreter Plugin")): These errors occur when the model
    encounters complex problems, often resulting in logical errors in the generated
    code. Such errors offer valuable insights into the model’s ability to handle intricate
    tasks in code generation; 4) Code Errors (Fig.[21](#A7.F21 "Figure 21 ‣ Appendix
    G Ethical Consideration ‣ CIBench: Evaluating Your LLMs with a Code Interpreter
    Plugin")): Basic errors in code generation fall under this category. While these
    errors may sometimes appear trivial, they signify potential deficiencies in the
    model’s code-generation process. These identified errors effectively underscore
    the current limitations of LLMs in terms of their Code Interpreter capabilities,
    providing valuable insights for the ongoing development of CIBench.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在CIBench的评估中，我们识别出模型生成的代码中存在的四种常见错误。这些错误的分类如下：1) 指令跟随错误（图[18](#A7.F18 "图18 ‣
    附录G 伦理考虑 ‣ CIBench：使用代码解释器插件评估您的LLMs")）：这些错误包括模型偏离或忽视提供的指令，反映出对指定指南的遵循不足；2) 幻觉错误（图[19](#A7.F19
    "图19 ‣ 附录G 伦理考虑 ‣ CIBench：使用代码解释器插件评估您的LLMs")）：这一类别涉及模型生成包含幻觉元素的代码，例如使用未定义的参数或引用无关的变量；3)
    推理错误（图[20](#A7.F20 "图20 ‣ 附录G 伦理考虑 ‣ CIBench：使用代码解释器插件评估您的LLMs")）：这些错误发生在模型遇到复杂问题时，通常导致生成的代码中出现逻辑错误。这些错误提供了关于模型处理复杂任务能力的有价值见解；4)
    代码错误（图[21](#A7.F21 "图21 ‣ 附录G 伦理考虑 ‣ CIBench：使用代码解释器插件评估您的LLMs")）：这一类别包括代码生成中的基本错误。尽管这些错误有时看似微不足道，但它们标志着模型代码生成过程中的潜在缺陷。这些识别出的错误有效地突显了LLMs在代码解释器能力方面的当前局限，为CIBench的持续发展提供了有价值的见解。
- en: Appendix F Human Annotator
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 人工注释员
- en: In our paper, the authors also serve as annotators for CIBench. These annotators
    are tasked with creating template tasks for various data science challenges, ensuring
    coverage of a wide range of concepts in the field. Importantly, our data collection
    process for these tasks is unbiased and does not involve any private information.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的论文中，作者们也担任了CIBench的注释员。这些注释员负责为各种数据科学挑战创建模板任务，确保涵盖该领域的广泛概念。重要的是，我们对这些任务的数据收集过程是公正的，并且不涉及任何私人信息。
- en: Appendix G Ethical Consideration
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 伦理考虑
- en: We use GPT-4 and online resources to construct our benchmark. The benchmarks
    are carefully processed by experts to exclude any private information. Additionally,
    we utilize ChatGPT for text refinement and to correct any typographical errors
    during the writing process.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用GPT-4和在线资源来构建我们的基准。基准由专家仔细处理，以排除任何私人信息。此外，我们利用ChatGPT进行文本优化，并在写作过程中纠正任何印刷错误。
- en: '![Refer to caption](img/d1f389de90f84cd336a7e095d97ecc74.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d1f389de90f84cd336a7e095d97ecc74.png)'
- en: 'Figure 11: Example of successful template task. The penguins dataset and the
    specific attribute "flipper_length_mm" can be substituted with alternative datasets
    and columns. Such a template task enriches the diversity of CIBench.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：成功模板任务的示例。企鹅数据集和特定属性“flipper_length_mm”可以用其他数据集和列进行替代。这种模板任务丰富了CIBench的多样性。
- en: '![Refer to caption](img/d0e5bfbd2e711210b2ef5d07b31b7422.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d0e5bfbd2e711210b2ef5d07b31b7422.png)'
- en: 'Figure 12: Example of successful template task. Continue of Fig. [11](#A7.F11
    "Figure 11 ‣ Appendix G Ethical Consideration ‣ CIBench: Evaluating Your LLMs
    with a Code Interpreter Plugin").'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：成功模板任务的示例。继续查看图[11](#A7.F11 "图11 ‣ 附录G 伦理考虑 ‣ CIBench：使用代码解释器插件评估您的LLMs")。
- en: 'Topic
    Generation Prompt: Please create 50 different
    topics base on Python module {}. These topics can cover the majority of functionality
    of this module.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 'Topic
    Generation Prompt: Please create 50 different
    topics base on Python module {}. These topics can cover the majority of functionality
    of this module.'
- en: 'Figure 13: An example prompt of topic generation.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：主题生成的示例提示。
- en: 'Question Refinement Prompt:
    Based on the experiment document and each step, please refine these steps into
    specific instructions for code writing to solve a real world question. Provide
    parameter settings if needed, ensuring that the execution results are identical
    for any reasonable person attempting to write the code. Please refrain from directly
    mentioning the name of the API, just target on purpose and give parameter settings.
    Please also summarize the required external dataset if applicable. Bad case step:
    1\. "Remove the ’Cabin’ column from the dataframe due to its numerous missing
    values. Make sure to specify that you are removing a column and that the change
    should be permanent." Too wordy, "Remove the ’Cabin’ column and make the change
    permanent." should be fine. 2\. "Identify and count how many values are missing
    in each column of the dataframe. Use a chain of two functions: the first checks
    if each value is null, and the second adds up these null instances for each column."
    Too specific, "Count how many values are missing in each column of the dataframe.
    Print the count." should be fine. Good case step: 1\. "Convert the ’Sex’ column
    to numerical values. Map ’male’ to 0 and ’female’ to 1." Concise, clear instruction.
    Your output format should be as follows starting with import necessary libraries:
    $\left[Optional\right]$ Dataset Link: $\left[Optional\right]$ Dataset Description:
    Step 1\. xx # code blcok Step 2\. xx # code blcok …'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 'Question Refinement Prompt:
    Based on the experiment document and each step, please refine these steps into
    specific instructions for code writing to solve a real world question. Provide
    parameter settings if needed, ensuring that the execution results are identical
    for any reasonable person attempting to write the code. Please refrain from directly
    mentioning the name of the API, just target on purpose and give parameter settings.
    Please also summarize the required external dataset if applicable. Bad case step:
    1\. "Remove the ’Cabin’ column from the dataframe due to its numerous missing
    values. Make sure to specify that you are removing a column and that the change
    should be permanent." Too wordy, "Remove the ’Cabin’ column and make the change
    permanent." should be fine. 2\. "Identify and count how many values are missing
    in each column of the dataframe. Use a chain of two functions: the first checks
    if each value is null, and the second adds up these null instances for each column."
    Too specific, "Count how many values are missing in each column of the dataframe.
    Print the count." should be fine. Good case step: 1\. "Convert the ’Sex’ column
    to numerical values. Map ’male’ to 0 and ’female’ to 1." Concise, clear instruction.
    Your output format should be as follows starting with import necessary libraries:
    $\left[Optional\right]$ Dataset Link: $\left[Optional\right]$ Dataset Description:
    Step 1\. xx # code blcok Step 2\. xx # code blcok …'
- en: 'Figure 14: An example prompt of question refinement.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 问题精炼的示例提示。'
- en: 'Another Question Refinement Prompt:
    Given the above experiment document. Do the following modification: 1\. Remove
    all the installation steps. 2\. Remove all the file saving steps. 3\. Elaborate
    the steps to be more specific with number and inputs that the execution results
    are identical for any reasonable person attempting to solve this step. 4\. Reorder
    the steps. Response with the new experiment document.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 'Another Question Refinement Prompt:
    Given the above experiment document. Do the following modification: 1\. Remove
    all the installation steps. 2\. Remove all the file saving steps. 3\. Elaborate
    the steps to be more specific with number and inputs that the execution results
    are identical for any reasonable person attempting to solve this step. 4\. Reorder
    the steps. Response with the new experiment document.'
- en: 'Figure 15: Another example prompt of question refinement.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: 问题精炼的另一个示例提示。'
- en: 'Quality
    Control Rules Check Rules: • Assure the questions
    is written from the perspective of a real user. • Assure file path informed in
    the head for all the experiment required external files. • Control the runtime,
    and each step should ideally produce results within 1 minute. • Control the file
    size, the file used for single experiment should ideally not exceed 50M. • Assure
    the output is valid and unambiguous as ground truth.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 'Quality
    Control Rules Check Rules: • Assure the questions
    is written from the perspective of a real user. • Assure file path informed in
    the head for all the experiment required external files. • Control the runtime,
    and each step should ideally produce results within 1 minute. • Control the file
    size, the file used for single experiment should ideally not exceed 50M. • Assure
    the output is valid and unambiguous as ground truth.'
- en: 'Figure 16: Rules used for manual dataset quality control.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: 用于手动数据集质量控制的规则。'
- en: 'Subjective Visualization Scoring Prompt
    Prompt: You are an assistant skilled in assessing visualization
    capabilities. In the capacity of a fair judge, you will evaluate the quality of
    images drawn by an AI model generating code for visualization-related problems.
    We will provide you with a code visualization problem and an image drawn by the
    code created by the AI model you need to assess. When you start your assessment,
    you must adhere to the following process: 1\. Rate the visualization capability
    with a score between 1 and 10 for the image, returning only the number without
    any additional descriptions. 2\. Your scoring needs to be as rigorous as possible,
    and it should follow the scoring rules below: Overall, the higher the quality
    of the model’s response, the higher the score. A score of 1 to 2 is given when
    the image cannot reflect the given instruction content at all. A score of 3 to
    4 is given when the image can partly reflect the given instruction content, but
    there is a significant lack of specific detail expression. If the image basically
    meets the given instructions, but the aesthetic quality of the image is average
    without any outstanding features, this kind of rating can get a score of 5 to
    6. When the image matches the given instructions well, and shows some aesthetic
    appeal, such as some originality in color matching and shape design, this kind
    of rating can get a score of 7 to 8. Only when the image completely matches the
    given instructions, covers all the details in the instructions, and performs excellently
    in terms of aesthetics, can this kind of rating get a score of 9 to 10. [Visualization
    Problem]:{question}'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 'Subjective Visualization Scoring Prompt
    Prompt: You are an assistant skilled in assessing visualization
    capabilities. In the capacity of a fair judge, you will evaluate the quality of
    images drawn by an AI model generating code for visualization-related problems.
    We will provide you with a code visualization problem and an image drawn by the
    code created by the AI model you need to assess. When you start your assessment,
    you must adhere to the following process: 1\. Rate the visualization capability
    with a score between 1 and 10 for the image, returning only the number without
    any additional descriptions. 2\. Your scoring needs to be as rigorous as possible,
    and it should follow the scoring rules below: Overall, the higher the quality
    of the model’s response, the higher the score. A score of 1 to 2 is given when
    the image cannot reflect the given instruction content at all. A score of 3 to
    4 is given when the image can partly reflect the given instruction content, but
    there is a significant lack of specific detail expression. If the image basically
    meets the given instructions, but the aesthetic quality of the image is average
    without any outstanding features, this kind of rating can get a score of 5 to
    6. When the image matches the given instructions well, and shows some aesthetic
    appeal, such as some originality in color matching and shape design, this kind
    of rating can get a score of 7 to 8. Only when the image completely matches the
    given instructions, covers all the details in the instructions, and performs excellently
    in terms of aesthetics, can this kind of rating get a score of 9 to 10. [Visualization
    Problem]:{question}'
- en: 'Figure 17: An example prompt of subjective visualization scoring prompt.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '图 17: 主观可视化评分提示的示例。'
- en: 'Following error User:
    Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5
    and blocksize=10, mark the corners with circles on the image. The radius and thickness
    of the circle are 5 and 1\. Show the marked image.Please use opencv and numpy
    modules. Assistant: #  Detect  corners  using  Shi-Tomasi  corner  detector corners  =  cv2.goodFeaturesToTrack(equalized_image,
    maxCorners=max_corners, qualityLevel=quality_level, minDistance=min_distance,
    blockSize=block_size) #  Mark  the  corners  with  circles  on  the  image marked_image  =  equalized_image.copy()
    for  corner  in  corners: x,  y  =  corner.ravel() cv2.circle(marked_image,  (int(x),  int(y)),  5,  (255,  0,  0),  -1)  #  Draw  a  blue  filled  circle  at  each  corner
    #  Show  the  marked  image plt.imshow(marked_image,  cmap=’gray’) plt.axis(’off’)  #  Turn  off  axis  numbers  and  ticks
    plt.show() System: NameError  Traceback  (most  recent  call  last) Cell  In[6],  line  2
    1  #  Detect  corners  using  Shi-Tomasi  corner  detector ---->  2  corners  =  cv2.goodFeaturesToTrack(equalized_image,  maxCorners=max_corners,  qualityLevel=quality_level,  minDistance=min_distance,  blockSize=block_size)
    4  #  Mark  the  corners  with  circles  on  the  image 5  marked_image  =  equalized_image.copy()
    NameError:  name  ’max_corners’  is  not  defined User: Define the CrossEntropyLoss
    function and the Adam optimizer. Train the model for 3 epochs. Display the loss
    for the last epoch(Keep to two decimal places). Assistant: import  torch.optim  as  optim
    criterion  =  nn.CrossEntropyLoss() optimizer  =  optim.Adam(model.parameters(),  lr=0.001)
    epochs  =  3 for  epoch  in  range(epochs): model.train() running_loss  =  0.0
    for  inputs,  labels  in  train_loader: inputs,  labels  =  inputs.to(device),  labels.to(device)
    optimizer.zero_grad() outputs  =  model(inputs) loss  =  criterion(outputs.squeeze(),  labels.float())
    loss.backward() optimizer.step() running_loss  +=  loss.item() final_loss  =  round(running_loss/len(train_loader),  2)
    print(final_loss) System: 933.55 933.45 933.09'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 'Following error User:
    Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5
    and blocksize=10, mark the corners with circles on the image. The radius and thickness
    of the circle are 5 and 1\. Show the marked image.Please use opencv and numpy
    modules. Assistant: #  Detect  corners  using  Shi-Tomasi  corner  detector corners  =  cv2.goodFeaturesToTrack(equalized_image,
    maxCorners=max_corners, qualityLevel=quality_level, minDistance=min_distance,
    blockSize=block_size) #  Mark  the  corners  with  circles  on  the  image marked_image  =  equalized_image.copy()
    for  corner  in  corners: x,  y  =  corner.ravel() cv2.circle(marked_image,  (int(x),  int(y)),  5,  (255,  0,  0),  -1)  #  Draw  a  blue  filled  circle  at  each  corner
    #  Show  the  marked  image plt.imshow(marked_image,  cmap=’gray’) plt.axis(’off’)  #  Turn  off  axis  numbers  and  ticks
    plt.show() System: NameError  Traceback  (most  recent  call  last) Cell  In[6],  line  2
    1  #  Detect  corners  using  Shi-Tomasi  corner  detector ---->  2  corners  =  cv2.goodFeaturesToTrack(equalized_image,  maxCorners=max_corners,  qualityLevel=quality_level,  minDistance=min_distance,  blockSize=block_size)
    4  #  Mark  the  corners  with  circles  on  the  image 5  marked_image  =  equalized_image.copy()
    NameError:  name  ’max_corners’  is  not  defined User: Define the CrossEntropyLoss
    function and the Adam optimizer. Train the model for 3 epochs. Display the loss
    for the last epoch(Keep to two decimal places). Assistant: import  torch.optim  as  optim
    criterion  =  nn.CrossEntropyLoss() optimizer  =  optim.Adam(model.parameters(),  lr=0.001)
    epochs  =  3 for  epoch  in  range(epochs): model.train() running_loss  =  0.0
    for  inputs,  labels  in  train_loader: inputs,  labels  =  inputs.to(device),  labels.to(device)
    optimizer.zero_grad() outputs  =  model(inputs) loss  =  criterion(outputs.squeeze(),  labels.float())
    loss.backward() optimizer.step() running_loss  +=  loss.item() final_loss  =  round(running_loss/len(train_loader),  2)
    print(final_loss) System: 933.55 933.45 933.09'
- en: 'Figure 18: Examples of Following error'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18: 跟随错误的示例。'
- en: 'Hallucination error User:
    Build a vocabulary of all the tokens and assign an index to the vocabulary. Display
    the index of the word "happy". Please use nltk modules. Assistant: #  Check  the  vocabulary
    vocab #  Display  the  index  of  the  word  "happy" word_to_index.get(’happy’,  ’Word  not  found  in  vocabulary’)
    System: NameError  Traceback  (most  recent  call  last) Cell  In[2],  line  2
    1  #  Check  the  vocabulary ---->  2  vocab 3  #  Display  the  index  of  the  word  "happy"
    4  word_to_index.get(’happy’,  ’Word  not  found  in  vocabulary’) NameError:  name  ’vocab’  is  not  defined
    User: Define a LightGBM model with max_depth=4, n_estimators=120, learning_rate=0.01
    and num_leaves=31\. Train the model with Evaluation Metric=’logloss’. Please use
    LightGBM modules. Assistant:import  lightgbm  as  lgb #  Define  the  LightGBM  model
    model  =  lgb.LGBMClassifier(max_depth=4,  n_estimators=120,  learning_rate=0.01,  num_leaves=31)
    #  Train  the  model  with  evaluation  metric  as  ’logloss’ model.fit(X_train,  y_train,  eval_set=(X_test,  y_test),  eval_metric=’logloss’,  early_stopping_rounds=20)
    System: TypeError  Traceback  (most  recent  call  last) Cell  In[3],  line  7
    4  model  =  lgb.LGBMClassifier(max_depth=4,  n_estimators=120,  learning_rate=0.01,  num_leaves=31)
    6  #  Train  the  model  with  evaluation  metric  as  ’logloss’ ---->  7  model.fit(X_train,  y_train,  eval_set=(X_test,  y_test),  eval_metric=’logloss’,  early_stopping_rounds=20)
    TypeError:  LGBMClassifier.fit()  got  an  unexpected  keyword  argument  ’early_stopping_rounds’'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hallucination error User:
    Build a vocabulary of all the tokens and assign an index to the vocabulary. Display
    the index of the word "happy". Please use nltk modules. Assistant: #  Check  the  vocabulary
    vocab #  Display  the  index  of  the  word  "happy" word_to_index.get(’happy’,  ’Word  not  found  in  vocabulary’)
    System: NameError  Traceback  (most  recent  call  last) Cell  In[2],  line  2
    1  #  Check  the  vocabulary ---->  2  vocab 3  #  Display  the  index  of  the  word  "happy"
    4  word_to_index.get(’happy’,  ’Word  not  found  in  vocabulary’) NameError:  name  ’vocab’  is  not  defined
    User: Define a LightGBM model with max_depth=4, n_estimators=120, learning_rate=0.01
    and num_leaves=31\. Train the model with Evaluation Metric=’logloss’. Please use
    LightGBM modules. Assistant:import  lightgbm  as  lgb #  Define  the  LightGBM  model
    model  =  lgb.LGBMClassifier(max_depth=4,  n_estimators=120,  learning_rate=0.01,  num_leaves=31)
    #  Train  the  model  with  evaluation  metric  as  ’logloss’ model.fit(X_train,  y_train,  eval_set=(X_test,  y_test),  eval_metric=’logloss’,  early_stopping_rounds=20)
    System: TypeError  Traceback  (most  recent  call  last) Cell  In[3],  line  7
    4  model  =  lgb.LGBMClassifier(max_depth=4,  n_estimators=120,  learning_rate=0.01,  num_leaves=31)
    6  #  Train  the  model  with  evaluation  metric  as  ’logloss’ ---->  7  model.fit(X_train,  y_train,  eval_set=(X_test,  y_test),  eval_metric=’logloss’,  early_stopping_rounds=20)
    TypeError:  LGBMClassifier.fit()  got  an  unexpected  keyword  argument  ’early_stopping_rounds’'
- en: 'Figure 19: Examples of Hallucination error'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '图 19: 幻觉错误的示例。'
- en: 'Reasoning error User:
    Split the dataset into training and testing sets using 0.2 as the test size, then
    define the train_loader and test_loader. Set batch size as 64\. Please use pytorch
    and sklearn modules. Assistant: from  sklearn.model_selection  import  train_test_split
    import  torch from  torch.utils.data  import  DataLoader,  TensorDataset #  Split  the  dataset  into  training  and  testing  sets
    X  =  df[’text’].values y  =  df[’sentiment’].values X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  test_size=0.2,  random_state=42)
    #  Define  the  train_loader  and  test_loader def  create_data_loader(X,  y,  batch_size):
    X_tensor  =  torch.tensor(X) y_tensor  =  torch.tensor(y,  dtype=torch.long) dataset  =  TensorDataset(X_tensor,  y_tensor)
    data_loader  =  DataLoader(dataset,  batch_size=batch_size) return  data_loader
    batch_size  =  64 train_loader  =  create_data_loader(X_train,  y_train,  batch_size)
    test_loader  =  create_data_loader(X_test,  y_test,  batch_size) System: TypeError  Traceback  (most  recent  call  last)
    Cell  In[4],  line  19 16  return  data_loader 18  batch_size  =  64 --->  19  train_loader  =  create_data_loader(X_train,  y_train,  batch_size)
    20  test_loader  =  create_data_loader(X_test,  y_test,  batch_size) Cell  In[4],  line  12
    11  def  create_data_loader(X,  y,  batch_size): --->  12  X_tensor  =  torch.tensor(X)
    13  y_tensor  =  torch.tensor(y,  dtype=torch.long) 14  dataset  =  TensorDataset(X_tensor,  y_tensor)
    TypeError:  can’t  convert  np.ndarray  of  type  numpy.object_.  The  only  supported  types  are:  float64,  float32,  float16,  complex64,  complex128,  int64,  int32,  int16,  int8,  uint8,  and  bool.
    User: Group and aggregate data by weight and calculate the average of each numerical
    group. Find out how many attributes have a negative correlation with weight.Please
    use pandas modules. Assistant: #  Group  and  aggregate  data  by  weight  and  calculate  the  average  of  each  numerical  group
    grouped_data  =  df.groupby(’Weight’).mean() #  Find  attributes  with  negative  correlation  with  weight
    negative_correlation_count  =  (grouped_data.corr()[’Weight’]  <  0).sum() negative_correlation_count
    System: KeyError  Traceback  (most  recent  call  last) File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802,  in  Index.get_loc(self,  key,  method,  tolerance)
    3801  try: ->  3802  return  self._engine.get_loc(casted_key) 3803  except  KeyError  as  err:
    File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/pandas/_libs/index.pyx:138,  in  pandas._libs.index.IndexEngine.get_loc()
    File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/pandas/_libs/index.pyx:165,  in  pandas._libs.index.IndexEngine.get_loc()
    File  pandas/_libs/hashtable_class_helper.pxi:5745,  in  pandas._libs.hashtable.PyObjectHashTable.get_item()
    File  pandas/_libs/hashtable_class_helper.pxi:5753,  in  pandas._libs.hashtable.PyObjectHashTable.get_item()
    KeyError:  ’Weight’'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 'Reasoning error User:
    Split the dataset into training and testing sets using 0.2 as the test size, then
    define the train_loader and test_loader. Set batch size as 64\. Please use pytorch
    and sklearn modules. Assistant: from  sklearn.model_selection  import  train_test_split
    import  torch from  torch.utils.data  import  DataLoader,  TensorDataset #  Split  the  dataset  into  training  and  testing  sets
    X  =  df[’text’].values y  =  df[’sentiment’].values X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  test_size=0.2,  random_state=42)
    #  Define  the  train_loader  and  test_loader def  create_data_loader(X,  y,  batch_size):
    X_tensor  =  torch.tensor(X) y_tensor  =  torch.tensor(y,  dtype=torch.long) dataset  =  TensorDataset(X_tensor,  y_tensor)
    data_loader  =  DataLoader(dataset,  batch_size=batch_size) return  data_loader
    batch_size  =  64 train_loader  =  create_data_loader(X_train,  y_train,  batch_size)
    test_loader  =  create_data_loader(X_test,  y_test,  batch_size) System: TypeError  Traceback  (most  recent  call  last)
    Cell  In[4],  line  19 16  return  data_loader 18  batch_size  =  64 --->  19  train_loader  =  create_data_loader(X_train,  y_train,  batch_size)
    20  test_loader  =  create_data_loader(X_test,  y_test,  batch_size) Cell  In[4],  line  12
    11  def  create_data_loader(X,  y,  batch_size): --->  12  X_tensor  =  torch.tensor(X)
    13  y_tensor  =  torch.tensor(y,  dtype=torch.long) 14  dataset  =  TensorDataset(X_tensor,  y_tensor)
    TypeError:  can’t  convert  np.ndarray  of  type  numpy.object_.  The  only  supported  types  are:  float64,  float32,  float16,  complex64,  complex128,  int64,  int32,  int16,  int8,  uint8,  and  bool.
    User: Group and aggregate data by weight and calculate the average of each numerical
    group. Find out how many attributes have a negative correlation with weight.Please
    use pandas modules. Assistant: #  Group  and  aggregate  data  by  weight  and  calculate  the  average  of  each  numerical  group
    grouped_data  =  df.groupby(’Weight’).mean() #  Find  attributes  with  negative  correlation  with  weight
    negative_correlation_count  =  (grouped_data.corr()[’Weight’]  <  0).sum() negative_correlation_count
    System: KeyError  Traceback  (most  recent  call  last) File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802,  in  Index.get_loc(self,  key,  method,  tolerance)
    3801  try: ->  3802  return  self._engine.get_loc(casted_key) 3803  except  KeyError  as  err:
    File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/pandas/_libs/index.pyx:138,  in  pandas._libs.index.IndexEngine.get_loc()
    File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/pandas/_libs/index.pyx:165,  in  pandas._libs.index.IndexEngine.get_loc()
    File  pandas/_libs/hashtable_class_helper.pxi:5745,  in  pandas._libs.hashtable.PyObjectHashTable.get_item()
    File  pandas/_libs/hashtable_class_helper.pxi:5753,  in  pandas._libs.hashtable.PyObjectHashTable.get_item()
    KeyError:  ’Weight’'
- en: 'Figure 20: Examples of Reasoning error'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '图 20: 推理错误的示例。'
- en: 'Code error User: Generate a pair plot
    for all the numerical columns, using color = "pastel".Please use seaborn modules.
    Assistant: sns.pairplot(df,  kind=’scatter’,  plot_kws={’color’:’pastel’}) plt.show()
    System:ValueError  Traceback  (most  recent  call  last) File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4372,  in  Axes._parse_scatter_color_args(c,  edgecolors,  kwargs,  xsize,  get_next_color_func)
    4371  try: ->  4372  mcolors.to_rgba_array(kwcolor) 4373  except  ValueError  as  err:
    File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/matplotlib/colors.py:471,  in  to_rgba_array(c,  alpha)
    470  if  isinstance(c,  str): -->  471  raise  ValueError(f"{c!r}  is  not  a  valid  color  value.")
    473  if  len(c)  ==  0: ValueError:  ’pastel’  is  not  a  valid  color  value.
    User: Blur the initial image by applying a Gaussian filter with a sigma value
    of 3\. Sigma is the standard deviation for the Gaussian kernel, which controls
    the amount of blur. Then display the smoothed image. Assistant: smooth  =  ndimage.gaussian_filter(image,  sigma=3)
    plt.imshow(smooth) plt.show() System: NameError  Traceback  (most  recent  call  last)
    Cell  In[48],  line  2 1  smooth  =  ndimage.gaussian_filter(image,  sigma=3)
    ---->  2  plt.imshow(smooth) 3  plt.show() NameError:  name  ’plt’  is  not  defined'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 'Code error User: Generate a pair plot
    for all the numerical columns, using color = "pastel".Please use seaborn modules.
    Assistant: sns.pairplot(df,  kind=’scatter’,  plot_kws={’color’:’pastel’}) plt.show()
    System:ValueError  Traceback  (most  recent  call  last) File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4372,  in  Axes._parse_scatter_color_args(c,  edgecolors,  kwargs,  xsize,  get_next_color_func)
    4371  try: ->  4372  mcolors.to_rgba_array(kwcolor) 4373  except  ValueError  as  err:
    File  anaconda3/envs/prod-torch1.13/lib/python3.10/site-packages/matplotlib/colors.py:471,  in  to_rgba_array(c,  alpha)
    470  if  isinstance(c,  str): -->  471  raise  ValueError(f"{c!r}  is  not  a  valid  color  value.")
    473  if  len(c)  ==  0: ValueError:  ’pastel’  is  not  a  valid  color  value.
    User: Blur the initial image by applying a Gaussian filter with a sigma value
    of 3\. Sigma is the standard deviation for the Gaussian kernel, which controls
    the amount of blur. Then display the smoothed image. Assistant: smooth  =  ndimage.gaussian_filter(image,  sigma=3)
    plt.imshow(smooth) plt.show() System: NameError  Traceback  (most  recent  call  last)
    Cell  In[48],  line  2 1  smooth  =  ndimage.gaussian_filter(image,  sigma=3)
    ---->  2  plt.imshow(smooth) 3  plt.show() NameError:  name  ’plt’  is  not  defined'
- en: 'Figure 21: Examples of Code error'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '图 21: 代码错误的示例。'
