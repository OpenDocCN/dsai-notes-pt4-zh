- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 17:34:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 17:34:50'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Data Science with LLMs and Interpretable Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用大型语言模型和可解释模型的数据科学
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.14474](https://ar5iv.labs.arxiv.org/html/2402.14474)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.14474](https://ar5iv.labs.arxiv.org/html/2402.14474)
- en: Sebastian Bordt¹, Ben Lengerich², Harsha Nori³, Rich Caruana³ Work done at Microsoft
    Research.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Sebastian Bordt¹、Ben Lengerich²、Harsha Nori³、Rich Caruana³的工作完成于微软研究院。
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent years have seen important advances in the building of interpretable models,
    machine learning models that are designed to be easily understood by humans. In
    this work, we show that large language models (LLMs) are remarkably good at working
    with interpretable models, too. In particular, we show that LLMs can describe,
    interpret, and debug Generalized Additive Models (GAMs). Combining the flexibility
    of LLMs with the breadth of statistical patterns accurately described by GAMs
    enables dataset summarization, question answering, and model critique. LLMs can
    also improve the interaction between domain experts and interpretable models,
    and generate hypotheses about the underlying phenomenon. We release https://github.com/interpretml/TalkToEBM
    as an open-source LLM-GAM interface.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在构建可解释模型方面取得了重要进展，这些模型旨在让人类易于理解。在这项工作中，我们展示了大型语言模型（LLMs）在处理可解释模型方面也表现出色。特别是，我们展示了LLMs可以描述、解释和调试广义加性模型（GAMs）。将LLMs的灵活性与GAMs准确描述的统计模式的广度结合，可以实现数据集总结、问题回答和模型批评。LLMs还可以改善领域专家与可解释模型之间的互动，并生成有关基础现象的假设。我们发布了[https://github.com/interpretml/TalkToEBM](https://github.com/interpretml/TalkToEBM)作为一个开源LLM-GAM接口。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Recent years have seen important advances in the building of interpretable models
    (Caruana et al. [2015](#bib.bib5); Lin et al. [2020](#bib.bib13)). On tabular
    machine learning problems, interpretable glass-box models are now frequently on
    par with black box methods (Rudin [2019](#bib.bib20)). In other words, today’s
    interpretable models act as powerful pattern detectors.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，在构建可解释模型方面取得了重要进展（Caruana 等 [2015](#bib.bib5); Lin 等 [2020](#bib.bib13)）。在表格机器学习问题上，可解释的玻璃盒模型现在通常与黑箱方法相当（Rudin
    [2019](#bib.bib20)）。换句话说，今天的可解释模型充当了强大的模式探测器。
- en: In this work, we show that combining interpretable models with large language
    models (LLMs) holds broad potential to improve many different tasks in data science.
    Using an interpretable model allows LLMs to handle complex statistical patterns,
    enabling them to perform some data science tasks semi-autonomously (Hollmann,
    Müller, and Hutter [2023](#bib.bib9)). For example, we can ask the LLM whether
    the statistical relationships learned by an interpretable model are surprising
    given what would be expected from prior knowledge in the underlying real-world
    problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们展示了将可解释模型与大型语言模型（LLMs）结合，具有广泛的潜力来改进数据科学中的许多不同任务。使用可解释模型使得LLMs能够处理复杂的统计模式，从而使它们能够半自动地执行一些数据科学任务（Hollmann,
    Müller, 和 Hutter [2023](#bib.bib9)）。例如，我们可以询问LLM，考虑到基础实际问题的先验知识，解释性模型所学习的统计关系是否令人惊讶。
- en: 'We explore this approach with the interpretable model class of Generalized
    Additive Models (GAMs) (Hastie and Tibshirani [1990](#bib.bib7); Caruana et al.
    [2015](#bib.bib5)). GAMs represent complex outcomes as a sum of univariate component
    functions (graphs). This means that the model is additive, and we can make use
    of this additivity to scale our approach to large datasets with many features.
    By analyzing the graphs of a GAM one at a time, the LLM can analyze large models
    with many different features, without ever having the entire model fit into the
    context window (Liu et al. [2023](#bib.bib14)). This is the advantage of using
    interpretable models: they have short textual description lengths. In contrast,
    black-box models are monolithic and have long textual descriptions, prohibiting
    their use as contexts for LLMs.¹¹1The textual description length of the GAM on
    the pneumonia dataset is 43,592 GPT-4 tokens, too many to be directly encoded
    in GPT-4’s 32k context window. The individual graphs are much more compact: the
    maximum description length of a single graph in the pneumonia GAM is 2,345 GPT-4
    tokens. This easily fits within GPT-4’s context window.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用通用加性模型（GAMs）类的可解释模型来探索这种方法（Hastie 和 Tibshirani [1990](#bib.bib7); Caruana
    等 [2015](#bib.bib5)）。GAMs 将复杂结果表示为单变量组件函数（图）的总和。这意味着模型是加性的，我们可以利用这种加性将我们的方法扩展到具有许多特征的大型数据集。通过逐一分析
    GAM 的图形，LLM 可以分析具有许多不同特征的大型模型，而无需将整个模型适配到上下文窗口中（Liu 等 [2023](#bib.bib14)）。这就是使用可解释模型的优势：它们具有较短的文本描述长度。相比之下，黑箱模型是整体的，并且有较长的文本描述，这使得它们不能作为LLMs的上下文使用。¹¹1
    在肺炎数据集上，GAM 的文本描述长度为 43,592 GPT-4 令牌，过多，无法直接编码到 GPT-4 的 32k 上下文窗口中。单个图形则紧凑得多：肺炎
    GAM 中单个图形的最大描述长度为 2,345 GPT-4 令牌。这完全可以容纳在 GPT-4 的上下文窗口中。
- en: In addition to serving as a useful tool for data science, the combination of
    LLMs with interpretable models also holds the potential to improve domain expert’s
    interaction with machine learning. This is because it provides domain experts
    with textual access to the models, and allows them to formulate questions to the
    model in natural language (Slack et al. [2022](#bib.bib21)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作为数据科学的有用工具外，将大型语言模型（LLMs）与可解释模型结合起来，还具有改善领域专家与机器学习互动的潜力。这是因为它为领域专家提供了对模型的文本访问，并允许他们用自然语言向模型提出问题（Slack
    等 [2022](#bib.bib21)）。
- en: This paper is organized as follows. We first provide a brief overview of our
    approach. We then provide simple baseline evaluations (is the LLM able to read
    a value from a graph?), and continue with examples of more complex qualitative
    tasks (summarization, anomaly detection). We then discusses the problem of hallucination,
    and presents some simple experimental evidence on the grounding of model responses
    in the interpretable model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文组织如下。我们首先提供我们方法的简要概述。然后，我们提供简单的基线评估（LLM 是否能够从图中读取值？），接着是更复杂的定性任务的示例（总结、异常检测）。接下来，我们讨论了幻觉问题，并提供了一些关于可解释模型中模型响应基础的简单实验证据。
- en: Our Approach
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的方法
- en: '![Refer to caption](img/01db5a579f8564c427a13f0e868a57d6.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/01db5a579f8564c427a13f0e868a57d6.png)'
- en: 'Figure 1: Conceptual overview of our approach. (1) We first train an interpretable
    model. (2) Next, we convert the different components of the interpretable model
    to text. (3) Then we provide these components as inputs to an LLM. (4) The user
    asks the LLM questions about the model or the dataset. The LLM can provide model-level
    summaries, or answer specific questions about particular aspects of the model.
    The LLM can also be used to automatically generate hypotheses about the real-world
    phenomenon that underlies the model and data.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们方法的概念性概述。（1）我们首先训练一个可解释模型。（2）接下来，我们将可解释模型的不同组件转换为文本。（3）然后我们将这些组件作为输入提供给LLM。（4）用户向LLM询问关于模型或数据集的问题。LLM
    可以提供模型级别的总结，或回答关于模型特定方面的具体问题。LLM 还可以用于自动生成关于模型和数据所涉及的现实世界现象的假设。
- en: The short technical description of our approach is as follows. We first train
    a GAM. Then we convert the individual graphs of the GAM to text, representing
    them as key-value pairs in JSON. The graphs are then provided as context to an
    LLM, together with a general introductory prompt about the task, and a description
    of the dataset. We engage in chain-of-thought (CoT) prompting (Wei et al. [2022](#bib.bib26)),
    first asking the LLM to describe the general pattern of the graph, then asking
    more specific questions (these questions depend on the task that we want the LLM
    to perform). We also request succinct summarizations of entire conversations about
    graphs. To elevate the discussion to the level of the entire model and dataset,
    the summaries of the conversations about individual graphs are aggregated and
    provided as context in a new query to the LLM. A graphical overview of this approach
    is given in Figure [1](#Sx2.F1 "Figure 1 ‣ Our Approach ‣ Data Science with LLMs
    and Interpretable Models"). The task of describing an individual graph is depicted
    in Figure [2](#Sx3.F2 "Figure 2 ‣ Baseline Tasks ‣ Experimental Results ‣ Data
    Science with LLMs and Interpretable Models"). The prompt structure is given in
    the Supplement.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的简短技术描述如下。我们首先训练一个 GAM。然后，我们将 GAM 的单个图形转换为文本，将其表示为 JSON 中的键值对。然后，将图形作为上下文提供给
    LLM，并附上一段关于任务的一般介绍提示以及数据集描述。我们进行思路链（CoT）提示（Wei 等人 [2022](#bib.bib26)），首先要求 LLM
    描述图形的一般模式，然后提出更具体的问题（这些问题取决于我们希望 LLM 执行的任务）。我们还请求对有关图形的整个对话进行简洁总结。为了将讨论提升到整个模型和数据集的层面，关于单个图形的对话总结被汇总并作为新的查询上下文提供给
    LLM。该方法的图形概述见图 [1](#Sx2.F1 "图 1 ‣ 我们的方法 ‣ 数据科学与 LLMs 和可解释模型")。描述单个图形的任务见图 [2](#Sx3.F2
    "图 2 ‣ 基线任务 ‣ 实验结果 ‣ 数据科学与 LLMs 和可解释模型")。提示结构见补充材料。
- en: Experimental Results
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验结果
- en: This sections provides the results of different experiments. We begin by investigating
    the ability of GPT-3.5 and GPT-4 to perform baseline tasks with graphs. Performance
    on these tasks can be quantified. This leads us to the conclusion that our approach
    requires relatively powerful LLMs. We then demonstrate the impressive abilities
    of GPT-4 to perform complex qualitative tasks with GAMs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了不同实验的结果。我们首先调查 GPT-3.5 和 GPT-4 在图形基线任务中的能力。对这些任务的表现可以量化。这使我们得出结论，我们的方法需要相对强大的
    LLMs。然后，我们展示了 GPT-4 在执行复杂定性任务时与 GAMs 结合的令人印象深刻的能力。
- en: Datasets. We use the California Housing (Kelley Pace and Barry [1997](#bib.bib10)),
    OpenML Diabetes (Smith et al. [1988](#bib.bib22); Vanschoren et al. [2014](#bib.bib23)),
    Iris, Kaggle Titanic²²2https://www.kaggle.com/competitions/titanic, Kaggle Spaceship
    Titanic³³3https://www.kaggle.com/c/spaceship-titanic, and Pneumonia (Caruana et al.
    [2015](#bib.bib5)) dataset. The Spaceship Titanic dataset is from 2022, and the
    Pneumonia dataset has never been released on the Internet, meaning that GPT-3.5
    and GPT-4 have not seen these datasets during training (the pneumonia dataset
    has been described in papers).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们使用加州住房（Kelley Pace 和 Barry [1997](#bib.bib10)）、OpenML 糖尿病（Smith 等人 [1988](#bib.bib22)；Vanschoren
    等人 [2014](#bib.bib23)）、鸢尾花、Kaggle 泰坦尼克号²²2https://www.kaggle.com/competitions/titanic、Kaggle
    太空船泰坦尼克号³³3https://www.kaggle.com/c/spaceship-titanic 和肺炎（Caruana 等人 [2015](#bib.bib5)）数据集。太空船泰坦尼克号数据集来自
    2022 年，肺炎数据集从未在互联网上发布，这意味着 GPT-3.5 和 GPT-4 在训练期间没有见过这些数据集（肺炎数据集已在论文中描述）。
- en: Interpretable Models. We train Generalized Additive Models (GAMs) using the
    interpretml software package (Nori et al. [2019](#bib.bib17)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释模型。我们使用 interpretml 软件包（Nori 等人 [2019](#bib.bib17)）训练广义加性模型（GAMs）。
- en: LLMs. We perform our experiments with gpt-3.5-turbo-16k-0613 and gpt-4-0613,
    using the OpenAI API (OpenAI [2023](#bib.bib19)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs。我们使用 gpt-3.5-turbo-16k-0613 和 gpt-4-0613 进行实验，使用 OpenAI API（OpenAI [2023](#bib.bib19)）。
- en: Baseline Tasks
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基线任务
- en: We conduct the following experiments using 31 different graphs, obtained from
    training GAMs on the different datasets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用从不同数据集上训练 GAMs 获得的 31 个不同图形进行以下实验。
- en: '![Refer to caption](img/08e3bcfee13268b6fd7c8203928eab54.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/08e3bcfee13268b6fd7c8203928eab54.png)'
- en: 'Figure 2: The basic building block of our framework is the ability of LLMs
    to describe and summarize the individual graphs of Generalized Additive Models
    (GAMs). Considering a GAM one graph at a time allows the LLM to work with interpretable
    models even on large-scale datasets with hundreds of features while staying within
    confined context windows.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们框架的基本构建块是 LLM 描述和总结广义加性模型（GAM）中各个图表的能力。逐个考虑 GAM 图表使得 LLM 能够在具有数百个特征的大规模数据集上处理可解释模型，同时保持在有限的上下文窗口内。
- en: Reading a Value from a Graph. As a test of whether the LLM is able to perform
    basic tasks with the JSON format in which we provide the graph, we ask the LLM
    to read a value from the graph (”What is the mean value of the graph at 3.528?”).
    This means that we sample a random point on the x-axis and ask the LLM about the
    value of the graph at that point. The results of this experiment are depicted
    in Table [1](#Sx3.T1 "Table 1 ‣ Qualitative Tasks ‣ Experimental Results ‣ Data
    Science with LLMs and Interpretable Models"). GPT-4 correctly reads 64 out of
    75 values from the different graphs. In fact, the success rate of GPT-4 on some
    graphs is much higher, and the mistakes frequently occur on graphs that require
    many decimal places to encode. GPT-3.5, in contrast, is not able to perform this
    simple task (of course, this can also depend on prompting).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中读取值。作为测试 LLM 是否能够处理我们提供的 JSON 格式图表的基本任务，我们要求 LLM 从图表中读取一个值（“图表在 3.528 处的均值是多少？”）。这意味着我们在
    x 轴上随机抽取一个点，并询问 LLM 该点处图表的值。此实验的结果在表格 [1](#Sx3.T1 "Table 1 ‣ Qualitative Tasks
    ‣ Experimental Results ‣ Data Science with LLMs and Interpretable Models") 中展示。GPT-4
    从不同的图表中正确读取了 75 个值中的 64 个。实际上，GPT-4 在某些图表上的成功率要高得多，错误通常发生在需要多位小数的图表上。相比之下，GPT-3.5
    无法执行这一简单任务（当然，这也可能取决于提示）。
- en: Deciding Monotonicity. One of the most important properties that a graph in
    an interpretable model can have is monotonicity. If a graph is monotone, it would
    be important to include this in a summarization of the graph, and later the summary
    of the full model. We test the ability of GPT-3.5 and GPT-4 to decide whether
    a graph is monotone (”Please tell me whether the graph is monotone increasing,
    monotone decreasing, or not monotone.”). By using both the original and y-axis
    inverted graphs, we conduct this experiment with a total of 62 different graphs
    (if the original graph is monotone increasing, then the y-axis inverted graph
    is monotone decreasing). The result of this experiment is depicted in the second
    row of Table [1](#Sx3.T1 "Table 1 ‣ Qualitative Tasks ‣ Experimental Results ‣
    Data Science with LLMs and Interpretable Models"). GPT-4 correctly identifies
    11 of 12 monotone graphs. It also claims that 9 out of 50 non-monotone graphs
    are monotone. However, most of these graphs 9 are “honest mistakes”, meaning that
    they are almost, but not perfectly, monotone. We thus find that GPT-4 has a surprisingly
    strong ability to decide the monotonicity of a graph purely from its textual representation.
    GPT-3.5, in contrast, is hardly able to identify the monotone graphs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 判定单调性。图表在可解释模型中可以具有的最重要的属性之一是单调性。如果图表是单调的，则在图表的总结中以及后来的完整模型总结中包含这一点将很重要。我们测试了
    GPT-3.5 和 GPT-4 判定图表是否单调的能力（“请告诉我图表是单调递增、单调递减还是非单调。”）。通过使用原始图表和 y 轴翻转图表，我们总共对
    62 个不同的图表进行了此实验（如果原始图表是单调递增，则 y 轴翻转图表是单调递减）。此实验的结果在表格 [1](#Sx3.T1 "Table 1 ‣ Qualitative
    Tasks ‣ Experimental Results ‣ Data Science with LLMs and Interpretable Models")
    的第二行展示。GPT-4 正确识别了 12 个单调图表中的 11 个。它还声称 50 个非单调图表中有 9 个是单调的。然而，这些图表中的大多数 9 个是“诚实的错误”，意味着它们几乎，但不完全是单调的。因此，我们发现
    GPT-4 具有令人惊讶的能力，仅通过文本表示来判定图表的单调性。相比之下，GPT-3.5 几乎无法识别单调图表。
- en: Finding the Largest Jump. Inspired by the strong performance of GPT-4 on the
    previous two tasks, we decided to ask the model to identify the largest jump in
    a graph. The graphs of EBMs are piecewise constant, which means that they have
    jumps in between the different intervals (Nori et al. [2019](#bib.bib17)). The
    learned jumps often have real-world interpretations (Lengerich et al. [2022](#bib.bib12)).
    However, this task is challenging for the LLMs since a perfect solution requires
    taking the max absolute value of the subtraction of adjacent values (Dziri et al.
    [2023](#bib.bib6)). The result of the experiment is depicted in the third column
    of Table [1](#Sx3.T1 "Table 1 ‣ Qualitative Tasks ‣ Experimental Results ‣ Data
    Science with LLMs and Interpretable Models"). We ask the LLMs to identify the
    biggest jumps in our 31 different graphs. Using chain-of-thought prompting (first
    listing a number of important jumps in the graph, then listing the largest one)
    GPT-4 is able to achieve an impressive success rate of 27 correctly identified
    jumps out of a total of 31 graphs. Again, the number of jumps identified by GPT-3.5
    is much smaller.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 查找最大跳跃。受到 GPT-4 在前两个任务中出色表现的启发，我们决定让模型识别图中的最大跳跃。EBM 的图是分段常数的，这意味着它们在不同区间之间存在跳跃（Nori
    等人 [2019](#bib.bib17)）。学习到的跳跃通常具有实际意义（Lengerich 等人 [2022](#bib.bib12)）。然而，对于 LLM
    来说，这个任务是具有挑战性的，因为完美的解决方案需要取相邻值之差的最大绝对值（Dziri 等人 [2023](#bib.bib6)）。实验结果在表格 [1](#Sx3.T1
    "Table 1 ‣ Qualitative Tasks ‣ Experimental Results ‣ Data Science with LLMs and
    Interpretable Models") 的第三列中展示。我们要求 LLM 识别我们 31 个不同图中的最大跳跃。通过链式思维提示（首先列出图中若干重要跳跃，然后列出最大跳跃），GPT-4
    能够在 31 个图中正确识别 27 个跳跃，取得了令人印象深刻的成功率。而 GPT-3.5 识别的跳跃数量要小得多。
- en: Qualitative Tasks
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定性任务
- en: In the previous Section, we saw that GPT-4 shows good performance on simple
    tasks with graphs. In this Section, we provide different examples that demonstrate
    the impressive capabilities of GPT-4 on more complex qualitative tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到 GPT-4 在简单图表任务上表现良好。在本节中，我们提供了不同的示例，展示了 GPT-4 在更复杂的定性任务中的出色能力。
- en: Summarizing Graphs. A basic building block of our framework is the ability of
    GPT-4 to describe and summarize individual graphs. Here we ask the model to describe
    the influence of the categorical feature “Home Planet” in the fictional Spaceship
    Titanic task from Kaggle. The model predicts the probability of a passenger “being
    transported to an alternative dimension”.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 总结图表。我们框架的一个基本构建块是 GPT-4 描述和总结单个图表的能力。在这里，我们要求模型描述在 Kaggle 的虚构 Spaceship Titanic
    任务中“Home Planet”这一类别特征的影响。模型预测乘客“被传送到另一个维度”的概率。
- en: GPT-4:”The HomePlanet feature has a significant influence on the probability
    of a passenger being transported to an alternate dimension during the collision
    with the spacetime anomaly. Passengers from Europa have the highest positive effect,
    indicating a higher likelihood of being transported. On the other hand, passengers
    from Earth have a negative effect, suggesting a lower likelihood of being transported.
    […]”
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT-4：“HomePlanet 特征对乘客在与时空异常碰撞期间被传送到另一个维度的概率有显著影响。来自 Europa 的乘客具有最高的正面效应，表明被传送的可能性较高。另一方面，来自地球的乘客具有负面效应，表明被传送的可能性较低。
    […]”
- en: Summarizing Models. Based on the ability of LLMs to describe individual graphs,
    we can use them to describe entire models. For this task, we additionally provide
    the LLM with the global feature importances. Consider the following description
    of a GAM trained to predict the likelihood of death from pneumonia (Caruana et al.
    [2015](#bib.bib5)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 总结模型。基于 LLM 描述单个图表的能力，我们可以利用它们来描述整个模型。对于这个任务，我们还向 LLM 提供了全局特征的重要性。考虑以下描述：一个
    GAM 被训练来预测死亡的可能性（Caruana 等人 [2015](#bib.bib5)）。
- en: GPT-4:”The model predicts the likelihood of death from pneumonia based on several
    patient features. The most influential global feature is age, with increasing
    age associated with a higher likelihood of death, particularly after the age of
    73\. Respiration rate and BUN level also have substantial influences on the model’s
    predictions, with higher levels of both associated with an increased risk of death.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT-4：“该模型根据几个患者特征预测死亡的可能性。最具全球影响力的特征是年龄，年龄越大，死亡的可能性越高，特别是在 73 岁之后。呼吸频率和 BUN
    水平对模型预测也有显著影响，两者的水平越高，死亡风险越大。”
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Other significant features include the number of diseases a patient has, their
    systolic blood pressure, and their pO2 levels. More diseases, lower systolic blood
    pressure, and very low or very high pO2 levels are all associated with a higher
    likelihood of death.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其他重要特征包括患者的疾病数量、收缩压和pO2水平。更多的疾病、更低的收缩压以及非常低或非常高的pO2水平都与较高的死亡可能性相关。
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The model also shows that certain medical readings can increase or decrease
    the likelihood of death, depending on their levels. For example, both very low
    and very high glucose levels, pH levels, and potassium levels can increase the
    risk of death, while moderate levels are associated with a lower risk.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型还显示，某些医疗读数可以增加或减少死亡的可能性，具体取决于它们的水平。例如，极低和极高的葡萄糖水平、pH水平和钾水平都可能增加死亡风险，而中等水平则与较低的风险相关。
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Finally, the model shows a considerable amount of uncertainty in its predictions
    for certain features, such as age, heart rate, and albumin levels. This is likely
    due to a lack of data points in certain ranges or the complex, non-linear relationships
    these features may have with the outcome. […]”
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最终，模型在某些特征（如年龄、心率和白蛋白水平）的预测中表现出相当大的不确定性。这可能是由于某些范围内缺乏数据点或这些特征与结果之间可能存在复杂的非线性关系所致。
    […]”
- en: This description was generated using the approach outlined above. Any aspects
    mentioned in the description are entirely correct and grounded in the actual model.
    (We discuss the problem of hallucination in the next Section).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 该描述是使用上述方法生成的。描述中提到的任何方面完全正确，并基于实际模型。（我们将在下一节讨论幻觉问题）。
- en: 'Table 1: Performance of GPT-3.5 and GPT-4 on baseline tasks with graphs. The
    table depicts the number of cases where the model succeeded versus the total number
    of cases in the task.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：GPT-3.5和GPT-4在基准任务中的表现，图表显示了模型成功的案例数与任务中的总案例数。
- en: '|  | GPT-3.5 | GPT-4 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-3.5 | GPT-4 |'
- en: '| Reading a Value from a Graph | 3/75 | 64/75 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 从图表中读取数值 | 3/75 | 64/75 |'
- en: '| Deciding Monotonicity | 2/12 | 11/12 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 决定单调性 | 2/12 | 11/12 |'
- en: '| Finding the Largest Jump | 9/31 | 27/31 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 寻找最大的跃升 | 9/31 | 27/31 |'
- en: Anomaly Detection. In order to show how the combination of LLMs and interpretable
    models can be useful for practitioners, we demonstrate the ability of GPT-4 at
    anomaly detection. In this task, we ask the LLM to critique the model and find
    any aspects that might be surprising or anti-causal (Lengerich et al. [2022](#bib.bib12)).
    We again use the pneumonia model and ask the LLM to find surprising patterns in
    the graph for the feature “creatinine level”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测。为了展示大语言模型（LLM）和可解释模型的结合对实践者的有用性，我们展示了GPT-4在异常检测方面的能力。在此任务中，我们要求LLM批评模型并找出可能令人惊讶或反直觉的方面（Lengerich
    et al. [2022](#bib.bib12)）。我们再次使用肺炎模型，并要求LLM在“肌酐水平”特征的图表中找出令人惊讶的模式。
- en: 'GPT-4: The graph for creatinine level shows a surprising decrease in the predicted
    log-odds of death from pneumonia as creatinine levels increase beyond approximately
    5.0 and then again after about 10.4, which is counterintuitive given that higher
    creatinine levels usually indicate worse health outcomes. This could potentially
    be due to a skewed distribution of data with fewer patients having very high creatinine
    levels, causing the model to learn inaccurate patterns. Alternatively, it might
    reflect that patients with extremely high creatinine levels receive more aggressive
    or specialized care, improving their outcomes. The wide confidence intervals in
    these ranges also suggest a high level of uncertainty in the model’s predictions.
    […]”'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GPT-4：肌酐水平的图表显示，随着肌酐水平超过约5.0，再次超过10.4，肺炎死亡的预测对数几率意外下降，这与肌酐水平通常表示健康状况较差的预期相反。这可能是由于数据分布偏斜，极高肌酐水平的患者较少，导致模型学习到不准确的模式。或者，这可能反映了肌酐水平极高的患者接受了更积极或专业的护理，改善了他们的结果。这些范围内的广泛置信区间也表明模型预测的不确定性很高。
    […]”
- en: A clinician rated this response as accurate and potentially useful. For a more
    detailed analysis of how our approach can be useful for anomaly and surprise detection
    in healthcare, see (Lengerich et al. [2023](#bib.bib11)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一位临床医生将此响应评为准确且潜在有用。有关我们的方法如何在医疗保健中有助于异常和惊讶检测的更详细分析，请参见（Lengerich et al. [2023](#bib.bib11)）。
- en: Grounding and Hallucination
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础与幻觉
- en: In this Section, we conduct an empirical investigation of the problem of hallucination.
    Our approach is to modify the learned graphs in a way that is counterfactual to
    that found in the original data. We then ask the LLM to describe both the original
    and counterfactual graph. In the absence of hallucination, the description of
    the counterfactual graph should reflect the modifications. We first discuss results
    with individual graphs. We then discuss the task of describing a counterfactual
    model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对幻觉问题进行实证研究。我们的方法是以一种与原始数据中的发现相反的方式修改学习到的图表。然后我们要求 LLM 描述原始图表和反事实图表。在没有幻觉的情况下，反事实图表的描述应反映修改的内容。我们首先讨论单独图表的结果。然后讨论描述反事实模型的任务。
- en: Describing a Counterfactual Graph
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述反事实图
- en: We first discuss the example of the “Sex” feature on the Kaggle Titanic dataset.
    The reason why we chose this example because the LLM has likely learned about
    the role of sex in emergency evacuations during pre-training, which might lead
    it to ignore the graph and provide a generic response. Here is GPT-4 describing
    the graph of the “Sex” feature in a GAM that models the survival probability on
    the Titanic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论“性别”特征在 Kaggle 泰坦尼克号数据集上的例子。选择这个例子的原因是因为 LLM 可能在预训练期间学习了性别在紧急疏散中的作用，这可能导致它忽视图表并提供一个通用的响应。以下是
    GPT-4 描述在 GAM 中建模泰坦尼克号生存概率的“性别”特征的图表。
- en: 'GPT-4: ”The ’Sex’ feature significantly influenced the survival outcome in
    the Titanic disaster. Being female greatly increased the chances of survival,
    as indicated by a positive mean value of 1.397\. [..] the overall trend suggests
    that females had a higher survival rate compared to males, aligning with the historical
    context of the ”women and children first” maritime evacuation policy.”'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'GPT-4: “‘性别’特征显著影响了泰坦尼克号灾难中的生存结果。女性的生存几率大大增加，正如平均值1.397所示。[……] 总体趋势表明，女性的生存率高于男性，这与历史背景中的‘女士和儿童优先’的海事疏散政策相符。”'
- en: We can see how the LLM provides an interpretation within the historical context.
    Now, we flip the values of ’male’ and ’female’ in the model, and see how the description
    of the LLM changes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 LLM 如何在历史背景中提供解释。现在，我们将模型中的‘男性’和‘女性’的值翻转，看看 LLM 的描述如何变化。
- en: 'GPT-4: ”Being male contributes positively to the log-odds of survival, with
    a mean contribution of 1.397, while being female contributes negatively […] it
    should be noted that these do not represent raw survival probabilities. This counterintuitive
    finding may be influenced by other confounding variables in the dataset. Further
    investigation is needed to fully understand this output.”'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'GPT-4: “男性对生存的对数几率有积极的贡献，平均贡献为1.397，而女性则有负面影响[……]。需要注意的是，这些数据并不代表原始的生存概率。这个反直觉的发现可能受到数据集中其他混杂变量的影响。需要进一步研究以全面理解这一结果。”'
- en: We observe that the model response changes in accordance with the graph. Interestingly,
    the LLM also notes that the result is counterintuitive, even if it does not directly
    explain why this is the case (which would be even better and might be achieved
    with better prompting).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到模型的响应与图表一致。有趣的是，LLM 还指出结果是反直觉的，即使它没有直接解释原因（如果能做到这一点会更好，并且可能通过更好的提示实现）。
- en: Describing a Counterfactual Model
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述反事实模型
- en: We now discuss an experiment where GPT-4 describes a counterfactual model. We
    use the popular Iris dataset. The reason why we chose this dataset is because
    GPT-4 has seen it during pre-training (Bordt, Nori, and Caruana [2023](#bib.bib2)),
    meaning that GPT-4’s responses could potentially be based on the original data
    and not on our interpretable model. We first ask GPT-4 to describe a GAM that
    models the probability of a flower being Iris-setosa.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们讨论一个实验，其中 GPT-4 描述了一个反事实模型。我们使用了流行的 Iris 数据集。选择这个数据集的原因是因为 GPT-4 在预训练期间已经见过它（Bordt,
    Nori, and Caruana [2023](#bib.bib2)），这意味着 GPT-4 的响应可能基于原始数据，而不是我们的可解释模型。我们首先要求
    GPT-4 描述一个建模花朵是 Iris-setosa 概率的 GAM。
- en: 'GPT-4: ”[…] Iris-setosa flowers typically have shorter and narrower petals
    compared to other species. […] As sepal length increases beyond 5.55 units, the
    likelihood of a flower being Iris-setosa decreases. However, Iris-setosa flowers
    are more likely to have wider sepals, with the likelihood increasing drastically
    for widths greater than 2.85 units. Each feature exhibits an abrupt change in
    contribution at specific thresholds, suggesting potential distinct cutoffs that
    differentiate Iris-setosa from other species. […]'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'GPT-4: ”[…] Iris-setosa花朵通常具有比其他物种更短、更窄的花瓣。[…] 当花萼长度超过5.55单位时，花朵属于Iris-setosa的可能性减少。然而，Iris-setosa花朵更可能具有更宽的花萼，宽度超过2.85单位时可能性急剧增加。每个特征在特定阈值处表现出贡献的急剧变化，暗示了区分Iris-setosa与其他物种的潜在明显截止点。
    […]'
- en: We then invert the graphs of the GAM on the y-axis and again pretend to the
    LLM that this GAM predicts the probability of a flower being Iris-setosa. Here
    is the description of the modified model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将GAM图的y轴反转，再次假装LLM这个GAM预测花朵属于Iris-setosa的概率。以下是修改后模型的描述。
- en: 'GPT-4: ”[…] The model suggests that Iris Setosa is more likely to have longer
    sepals, smaller sepal  widths, longer petal lengths, and wider petals. Notably,
    there are threshold effects at certain intervals for all the features. […] Further
    validation with expert biological knowledge or additional data would be beneficial.”'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'GPT-4: ”[…] 模型建议Iris Setosa更可能具有较长的花萼、较小的花萼宽度、较长的花瓣长度以及较宽的花瓣。值得注意的是，所有特征在某些区间存在阈值效应。[…]
    进一步的验证，结合专家生物学知识或额外数据，将会有所帮助。”'
- en: We observe that the model summarization by the LLM changes in accordance with
    the model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到LLM对模型的总结会随着模型的变化而变化。
- en: In summary, while the evidence from our simple qualitative experiments is far
    from conclusive, they do demonstrate that the responses of GPT-4 are frequently
    grounded in the provided interpretable model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，尽管我们简单的定性实验的证据还远未结论，但它们确实展示了GPT-4的响应通常是基于提供的可解释模型。
- en: Related Work
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Slack et al. (Slack et al. [2022](#bib.bib21)) develop a natural language interface
    to give practitioners conversational access to model explanations. Their work
    is similar in spirit to ours but intends to provide access to generic (potentially
    black-box) models, precluding the LLM from direct access to model internals. Bisercic
    et al. (Bisercic et al. [2023](#bib.bib1)) showed that LLMs can extract tabular
    datasets from unstructured text and then train interpretable models (linear regression
    and small decision trees) on top of this data. Various recent works have explored
    the potential of LLMs for data science with tabular data by data wrangling and
    cleaning (Narayan et al. [2022](#bib.bib16); Vos, Döhmen, and Schelter [2022](#bib.bib24)),
    or traditional supervised and unsupervised learning tasks like classification
    or density estimation (Borisov et al. [2022](#bib.bib4); Hegselmann et al. [2023](#bib.bib8);
    Wang et al. [2023](#bib.bib25); Onishi, Oono, and Hayashi [2023](#bib.bib18)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Slack et al. (Slack et al. [2022](#bib.bib21)) 开发了一个自然语言接口，为从业者提供对模型解释的对话式访问。他们的工作在精神上与我们的相似，但旨在提供对通用（可能是黑箱）模型的访问，排除了LLM对模型内部的直接访问。Bisercic
    et al. (Bisercic et al. [2023](#bib.bib1)) 展示了LLM可以从非结构化文本中提取表格数据，并在这些数据上训练可解释模型（线性回归和小型决策树）。各种近期的研究探讨了LLM在数据科学中处理表格数据的潜力，包括数据整理和清洗（Narayan
    et al. [2022](#bib.bib16); Vos, Döhmen, 和 Schelter [2022](#bib.bib24)），或传统的监督和非监督学习任务如分类或密度估计（Borisov
    et al. [2022](#bib.bib4); Hegselmann et al. [2023](#bib.bib8); Wang et al. [2023](#bib.bib25);
    Onishi, Oono, 和 Hayashi [2023](#bib.bib18)）。
- en: Previous work with GAMs has shown the ubiquity of hidden confounding in real-world
    datasets and the utility of GAMs to reveal surprising effects in datasets (Caruana
    et al. [2015](#bib.bib5); Lengerich et al. [2022](#bib.bib12)). This prior work
    relied on human experts to design explicit statistical tests for surprising patterns
    in GAM feature graphs, limiting the potential applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 之前对广义加性模型（GAM）的研究显示了实际数据集中隐藏混杂因素的普遍存在，以及GAM在揭示数据集中意外效应方面的实用性（Caruana et al.
    [2015](#bib.bib5); Lengerich et al. [2022](#bib.bib12)）。这些先前的工作依赖于人工专家设计明确的统计测试来发现GAM特征图中的意外模式，从而限制了其潜在应用。
- en: Discussion
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In this work we have explored synergies between LLMs and interpretable models
    and found this to be a powerful combination with broad potential to assist data
    scientists and practitioners.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们探索了LLM与可解释模型之间的协同作用，并发现这是一种强大的组合，具有广泛的潜力来帮助数据科学家和从业者。
- en: As a preliminary investigation, we explored the capabilities of GPT-3.5 and
    GPT-4 to perform a number of different tasks with graphs generated by glass-box
    models. Although we gave only limited effort to prompt tuning, the results already
    are reasonably strong and probably could be further improved with more advanced
    prompting strategies. We observe that it would be unclear how to give an LLM the
    same degree of access to a black-box model. In principle, however, a related approach
    might be possible with post-hoc methods, for example, Shapley Values, which are
    known to be closely related to GAMs (Lundberg and Lee [2017](#bib.bib15); Bordt
    and von Luxburg [2023](#bib.bib3)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为初步调查，我们探索了GPT-3.5和GPT-4在执行由玻璃盒模型生成的图任务中的能力。虽然我们在提示调整上只投入了有限的努力，但结果已经相当强劲，可能通过更先进的提示策略进一步改进。我们观察到，如何让LLM获得相同程度的黑盒模型访问权是不明确的。然而，原则上，可能通过事后方法实现相关方法，例如Shapley值，这与GAMs（Lundberg和Lee
    [2017](#bib.bib15); Bordt和von Luxburg [2023](#bib.bib3)）密切相关。
- en: Our preliminary experiments raise a variety of avenues of future study. For
    example, how does LLM performance change with graph complexity? Under what conditions
    is the LLM response grounded in the graph, and when does hallucination occur?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初步实验开辟了多种未来研究的方向。例如，LLM的性能如何随图的复杂性而变化？在什么条件下LLM的响应基于图，而何时会出现幻觉？
- en: It is important to note that, given the current state of the technology, there
    are no guarantees that an answer from an LLM is a truthful response to the prompt.
    For this reason, all the answers and descriptions provided by the LLMs should
    be seen as helpful suggestions rather than final answers. Similarly, the answers
    provided by GPT-4 in the anomaly detection tasks should be seen as useful hypothesis
    that have to be verified by human experts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，鉴于当前技术状态，没有保证LLM的回答是对提示的真实回应。因此，LLM提供的所有答案和描述应被视为有用的建议，而非最终答案。类似地，GPT-4在异常检测任务中提供的答案应视为有用的假设，需要由人工专家验证。
- en: All in all, we believe that the approach of combining LLMs with interpretable
    models holds broad potential to assist practitioners and domain experts across
    a broad range of tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们相信将LLMs与可解释模型结合的方法具有广泛的潜力，可以帮助实践者和领域专家处理广泛的任务。
- en: References
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bisercic et al. (2023) Bisercic, A.; Nikolic, M.; van der Schaar, M.; Delibasic,
    B.; Lio, P.; and Petrovic, A. 2023. Interpretable Medical Diagnostics with Structured
    Data Extraction by Large Language Models. *arXiv preprint arXiv:2306.05052*.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisercic等（2023）Bisercic, A.; Nikolic, M.; van der Schaar, M.; Delibasic, B.;
    Lio, P.; 和 Petrovic, A. 2023. 使用大型语言模型进行结构化数据提取的可解释医疗诊断。*arXiv预印本 arXiv:2306.05052*。
- en: 'Bordt, Nori, and Caruana (2023) Bordt, S.; Nori, H.; and Caruana, R. 2023.
    Elephants Never Forget: Testing Language Models for Memorization of Tabular Data.
    In *NeurIPS 2023 Second Table Representation Learning Workshop*.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bordt, Nori, 和 Caruana（2023）Bordt, S.; Nori, H.; 和 Caruana, R. 2023. 大象从不忘记：测试语言模型对表格数据的记忆能力。在*NeurIPS
    2023 第二届表格表示学习研讨会*。
- en: Bordt and von Luxburg (2023) Bordt, S.; and von Luxburg, U. 2023. From Shapley
    values to generalized additive models and back. In *International Conference on
    Artificial Intelligence and Statistics*, 709–745\. PMLR.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bordt 和 von Luxburg（2023）Bordt, S.; 和 von Luxburg, U. 2023. 从Shapley值到广义加法模型及其回归。在*国际人工智能与统计会议*，709–745。PMLR。
- en: Borisov et al. (2022) Borisov, V.; Seßler, K.; Leemann, T.; Pawelczyk, M.; and
    Kasneci, G. 2022. Language models are realistic tabular data generators. *arXiv
    preprint arXiv:2210.06280*.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borisov等（2022）Borisov, V.; Seßler, K.; Leemann, T.; Pawelczyk, M.; 和 Kasneci,
    G. 2022. 语言模型是现实的表格数据生成器。*arXiv预印本 arXiv:2210.06280*。
- en: 'Caruana et al. (2015) Caruana, R.; Lou, Y.; Gehrke, J.; Koch, P.; Sturm, M.;
    and Elhadad, N. 2015. Intelligible models for healthcare: Predicting pneumonia
    risk and hospital 30-day readmission. In *Proceedings of the 21th ACM SIGKDD international
    conference on knowledge discovery and data mining*, 1721–1730.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caruana等（2015）Caruana, R.; Lou, Y.; Gehrke, J.; Koch, P.; Sturm, M.; 和 Elhadad,
    N. 2015. 医疗领域的可解释模型：预测肺炎风险和医院30天再入院。发表于*第21届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，1721–1730。
- en: 'Dziri et al. (2023) Dziri, N.; Lu, X.; Sclar, M.; Li, X. L.; Jian, L.; Lin,
    B. Y.; West, P.; Bhagavatula, C.; Bras, R. L.; Hwang, J. D.; et al. 2023. Faith
    and Fate: Limits of Transformers on Compositionality. *arXiv preprint arXiv:2305.18654*.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dziri等（2023）Dziri, N.; Lu, X.; Sclar, M.; Li, X. L.; Jian, L.; Lin, B. Y.; West,
    P.; Bhagavatula, C.; Bras, R. L.; Hwang, J. D.; 等。2023. 信仰与命运：变压器在组合性上的限制。*arXiv预印本
    arXiv:2305.18654*。
- en: Hastie and Tibshirani (1990) Hastie, T. J.; and Tibshirani, R. J. 1990. *Generalized
    additive models*, volume 43. CRC press.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hastie 和 Tibshirani (1990) Hastie, T. J.; 和 Tibshirani, R. J. 1990. *广义加性模型*，第
    43 卷。CRC press。
- en: 'Hegselmann et al. (2023) Hegselmann, S.; Buendia, A.; Lang, H.; Agrawal, M.;
    Jiang, X.; and Sontag, D. 2023. Tabllm: Few-shot classification of tabular data
    with large language models. In *International Conference on Artificial Intelligence
    and Statistics*, 5549–5581\. PMLR.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hegselmann 等 (2023) Hegselmann, S.; Buendia, A.; Lang, H.; Agrawal, M.; Jiang,
    X.; 和 Sontag, D. 2023. Tabllm：使用大型语言模型对表格数据进行少量示例分类。在 *国际人工智能与统计学会议*，5549–5581。PMLR。
- en: 'Hollmann, Müller, and Hutter (2023) Hollmann, N.; Müller, S.; and Hutter, F.
    2023. GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware
    Automated Feature Engineering. *arXiv preprint arXiv:2305.03403*.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hollmann, Müller 和 Hutter (2023) Hollmann, N.; Müller, S.; 和 Hutter, F. 2023.
    GPT 用于半自动化数据科学：引入 CAAFE 进行上下文感知自动特征工程。*arXiv 预印本 arXiv:2305.03403*。
- en: 'Kelley Pace and Barry (1997) Kelley Pace, R.; and Barry, R. 1997. Sparse spatial
    autoregressions. *Statistics & Probability Letters*, 33(3): 291–297.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kelley Pace 和 Barry (1997) Kelley Pace, R.; 和 Barry, R. 1997. 稀疏空间自回归。*统计与概率通讯*，33(3):
    291–297。'
- en: Lengerich et al. (2023) Lengerich, B. J.; Bordt, S.; Nori, H.; Nunnally, M. E.;
    Aphinyanaphongs, Y.; Kellis, M.; and Caruana, R. 2023. LLMs Understand Glass-Box
    Models, Discover Surprises, and Suggest Repairs. *arXiv preprint arXiv:2308.01157*.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lengerich 等 (2023) Lengerich, B. J.; Bordt, S.; Nori, H.; Nunnally, M. E.; Aphinyanaphongs,
    Y.; Kellis, M.; 和 Caruana, R. 2023. 大型语言模型理解玻璃盒模型，发现惊喜并建议修复。*arXiv 预印本 arXiv:2308.01157*。
- en: 'Lengerich et al. (2022) Lengerich, B. J.; Caruana, R.; Nunnally, M. E.; and
    Kellis, M. 2022. Death by Round Numbers: Glass-Box Machine Learning Uncovers Biases
    in Medical Practice. *medRxiv*, 2022–04.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lengerich 等 (2022) Lengerich, B. J.; Caruana, R.; Nunnally, M. E.; 和 Kellis,
    M. 2022. 被圆整数击败：玻璃盒机器学习揭示了医疗实践中的偏见。*medRxiv*，2022–04。
- en: Lin et al. (2020) Lin, J.; Zhong, C.; Hu, D.; Rudin, C.; and Seltzer, M. 2020.
    Generalized and scalable optimal sparse decision trees. In *International Conference
    on Machine Learning*, 6150–6160\. PMLR.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2020) Lin, J.; Zhong, C.; Hu, D.; Rudin, C.; 和 Seltzer, M. 2020. 广义和可扩展的最优稀疏决策树。在
    *国际机器学习大会*，6150–6160。PMLR。
- en: 'Liu et al. (2023) Liu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua,
    M.; Petroni, F.; and Liang, P. 2023. Lost in the Middle: How Language Models Use
    Long Contexts. *arXiv preprint arXiv:2307.03172*.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023) Liu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua, M.;
    Petroni, F.; 和 Liang, P. 2023. 中间的迷失：语言模型如何使用长上下文。*arXiv 预印本 arXiv:2307.03172*。
- en: Lundberg and Lee (2017) Lundberg, S. M.; and Lee, S.-I. 2017. A unified approach
    to interpreting model predictions. *Advances in neural information processing
    systems*, 30.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg 和 Lee (2017) Lundberg, S. M.; 和 Lee, S.-I. 2017. 解释模型预测的统一方法。*神经信息处理系统进展*，30。
- en: Narayan et al. (2022) Narayan, A.; Chami, I.; Orr, L.; Arora, S.; and Ré, C.
    2022. Can Foundation Models Wrangle Your Data? *arXiv preprint arXiv:2205.09911*.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan 等 (2022) Narayan, A.; Chami, I.; Orr, L.; Arora, S.; 和 Ré, C. 2022.
    基础模型能处理你的数据吗？*arXiv 预印本 arXiv:2205.09911*。
- en: 'Nori et al. (2019) Nori, H.; Jenkins, S.; Koch, P.; and Caruana, R. 2019. Interpretml:
    A unified framework for machine learning interpretability. *arXiv preprint arXiv:1909.09223*.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nori 等 (2019) Nori, H.; Jenkins, S.; Koch, P.; 和 Caruana, R. 2019. Interpretml：一个统一的机器学习可解释性框架。*arXiv
    预印本 arXiv:1909.09223*。
- en: 'Onishi, Oono, and Hayashi (2023) Onishi, S.; Oono, K.; and Hayashi, K. 2023.
    TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. *arXiv
    preprint arXiv:2303.15747*.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Onishi, Oono 和 Hayashi (2023) Onishi, S.; Oono, K.; 和 Hayashi, K. 2023. TabRet：为未见列预训练基于
    Transformer 的表格模型。*arXiv 预印本 arXiv:2303.15747*。
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: 'Rudin (2019) Rudin, C. 2019. Stop explaining black box machine learning models
    for high stakes decisions and use interpretable models instead. *Nature machine
    intelligence*, 1(5): 206–215.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rudin (2019) Rudin, C. 2019. 停止为高风险决策解释黑箱机器学习模型，而改用可解释模型。*自然机器智能*，1(5): 206–215。'
- en: 'Slack et al. (2022) Slack, D.; Krishna, S.; Lakkaraju, H.; and Singh, S. 2022.
    Talktomodel: Understanding machine learning models with open ended dialogues.
    *arXiv preprint arXiv:2207.04154*.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slack 等 (2022) Slack, D.; Krishna, S.; Lakkaraju, H.; 和 Singh, S. 2022. Talktomodel：通过开放式对话理解机器学习模型。*arXiv
    预印本 arXiv:2207.04154*。
- en: Smith et al. (1988) Smith, J. W.; Everhart, J. E.; Dickson, W.; Knowler, W. C.;
    and Johannes, R. S. 1988. Using the ADAP learning algorithm to forecast the onset
    of diabetes mellitus. In *Proceedings of the annual symposium on computer application
    in medical care*, 261\. American Medical Informatics Association.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 等人（1988）Smith, J. W.; Everhart, J. E.; Dickson, W.; Knowler, W. C.; 和
    Johannes, R. S. 1988. 使用 ADAP 学习算法预测糖尿病的发生。在 *计算机医疗应用年会论文集*，261页。美国医学信息学协会。
- en: 'Vanschoren et al. (2014) Vanschoren, J.; Van Rijn, J. N.; Bischl, B.; and Torgo,
    L. 2014. OpenML: networked science in machine learning. *ACM SIGKDD Explorations
    Newsletter*, 15(2): 49–60.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vanschoren 等人（2014）Vanschoren, J.; Van Rijn, J. N.; Bischl, B.; 和 Torgo, L.
    2014. OpenML: 机器学习中的网络科学。*ACM SIGKDD 探索新闻*，15(2): 49–60。'
- en: Vos, Döhmen, and Schelter (2022) Vos, D.; Döhmen, T.; and Schelter, S. 2022.
    Towards Parameter-Efficient Automation of Data Wrangling Tasks with Prefix-Tuning.
    In *NeurIPS 2022 First Table Representation Workshop*.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vos, Döhmen 和 Schelter（2022）Vos, D.; Döhmen, T.; 和 Schelter, S. 2022. 通过前缀调优实现数据清理任务的参数高效自动化。在
    *NeurIPS 2022 首届表格表示工作坊*。
- en: 'Wang et al. (2023) Wang, Z.; Gao, C.; Xiao, C.; and Sun, J. 2023. AnyPredict:
    Foundation Model for Tabular Prediction. *arXiv preprint arXiv:2305.12081*.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023）Wang, Z.; Gao, C.; Xiao, C.; 和 Sun, J. 2023. AnyPredict: 表格预测的基础模型。*arXiv
    预印本 arXiv:2305.12081*。'
- en: 'Wei et al. (2022) Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi,
    E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-of-thought prompting elicits reasoning
    in large language models. *Advances in Neural Information Processing Systems*,
    35: 24824–24837.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等人（2022）Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.;
    Le, Q. V.; Zhou, D.; 等. 2022. 思维链提示激发大型语言模型的推理能力。*神经信息处理系统进展*，35: 24824–24837。'
- en: Appendix A Appendix
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: 'System: You are an expert statistician and data scientist. You interpret global
    explanations produced by a generalized additive model (GAM). GAMs produce explanations
    in the form of graphs that contain the effect of a specific input feature. The
    user will first provide a general description of the dataset. Then you will be
    given graphs from the model, and the user will ask you questions about the graphs.
    Answer all questions to the best of your ability, combining both the data contained
    in the graph, the data set description you were given, and your knowledge about
    the real world. Graphs will be presented as a JSON object with keys representing
    the x-axis and values representing the y-axis. For continuous features, the keys
    are intervals that represent ranges where the function predicts the same value.
    For categorical features, each key represents a possible value that the feature
    can take. The y-axis depicts the contribution of the graph to the logprobs to
    the probability that the passenger survived. The user will provide graphs in the
    following format: - The name of the feature depicted in the graph - The type of
    the feature (continuous, categorical, or boolean) - Mean values - Lower bounds
    of confidence interval - Upper bounds of confidence interval User: This is the
    titanic dataset from kaggle. The sinking of the Titanic is one of the most infamous
    shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely
    considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately,
    there weren’t enough lifeboats for everyone onboard, resulting in the death of
    1502 out of 2224 passengers and crew. While there was some element of luck involved
    in surviving, it seems some groups of people were more likely to survive than
    others. This dataset is used to answers the question: “what sorts of people were
    more likely to survive?” using passenger data (ie name, age, gender, socio-economic
    class, etc). Assistant: Thanks for this general description of the data set. Please
    continue and provide more information, for example about the graphs from the model.
    User: Consider the following graph from the model. This graph represents a continuous-valued
    feature. The keys are intervals that represent ranges where the function predicts
    the same value. Feature Name: Age Feature Type: continuous Means: ”(2.0, 2.5)”:
    -0.308, ”(2.5, 3.5)”: 0.839, ”(3.5, 12.5)”: 0.91, ”(12.5, 17.5)”: 0.904, ”(17.5,
    20.0)”: 0.035, ”(20.0, 21.5)”: 0.144, ”(21.5, 25.5)”: 0.304, ”(25.5, 28.5)”: 0.375,
    ”(28.5, 31.5)”: 0.254, ”(31.5, 33.5)”: 0.349, ”(33.5, 36.25)”: 0.399, ”(36.25,
    36.75)”: 0.047, ”(36.75, 37.5)”: 0.038, ”(37.5, 38.5)”: 0.246, ”(38.5, 39.5)”:
    0.2, ”(39.5, 41.0)”: 0.103, ”(41.0, 43.0)”: 0.086, ”(43.0, 44.5)”: -0.93, ”(44.5,
    46.5)”: -1.153, ”(46.5, 47.5)”: -1.132, ”(47.5, 50.5)”: -0.301, ”(50.5, 51.5)”:
    0.104, ”(51.5, 52.5)”: 0.121, ”(52.5, 53.5)”: 0.065, ”(53.5, 55.0)”: -0.637, ”(55.0,
    56.5)”: -0.627, ”(56.5, 57.5)”: -0.648, ”(57.5, 60.5)”: -0.628, ”(60.5, 62.5)”:
    -0.977, ”(62.5, 67.5)”: -0.945, ”(67.5, 80.0)”: -0.887'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '系统：你是一位专家级统计学家和数据科学家。你将解释由广义加性模型（GAM）产生的全球解释。GAMs 通过图形形式提供解释，这些图形包含特定输入特征的影响。用户将首先提供数据集的一般描述。然后你将获得来自模型的图形，用户会询问你有关这些图形的问题。尽你所能回答所有问题，结合图形中包含的数据、你获得的数据集描述以及你对现实世界的知识。图形将以
    JSON 对象的形式呈现，键表示 x 轴，值表示 y 轴。对于连续特征，键是表示函数预测相同值的区间。对于分类特征，每个键表示特征可能取的一个值。y 轴描绘了图形对乘客生存概率的
    logprobs 贡献。用户将以以下格式提供图形： - 图形中描绘的特征名称 - 特征类型（连续的、分类的或布尔的） - 平均值 - 置信区间下限 - 置信区间上限
    用户：这是来自 Kaggle 的 Titanic 数据集。泰坦尼克号的沉没是历史上最臭名昭著的船难之一。1912 年 4 月 15 日，在她的处女航期间，被广泛认为“永不沉没”的
    RMS Titanic 在撞上冰山后沉没。不幸的是，船上没有足够的救生艇，导致 2224 名乘客和船员中有 1502 人遇难。虽然生存中确实有一些运气因素，但似乎有些人群比其他人更可能幸存。这个数据集用于回答问题：“哪些人更有可能幸存？”使用乘客数据（即姓名、年龄、性别、社会经济阶层等）。
    助手：感谢你提供的数据集的一般描述。请继续提供更多信息，例如关于模型中的图形。 用户：考虑以下模型中的图形。这个图形代表一个连续值特征。键是表示函数预测相同值的区间。特征名称：年龄
    特征类型：连续的 平均值：“(2.0, 2.5)”: -0.308, “(2.5, 3.5)”: 0.839, “(3.5, 12.5)”: 0.91, “(12.5,
    17.5)”: 0.904, “(17.5, 20.0)”: 0.035, “(20.0, 21.5)”: 0.144, “(21.5, 25.5)”: 0.304,
    “(25.5, 28.5)”: 0.375, “(28.5, 31.5)”: 0.254, “(31.5, 33.5)”: 0.349, “(33.5, 36.25)”:
    0.399, “(36.25, 36.75)”: 0.047, “(36.75, 37.5)”: 0.038, “(37.5, 38.5)”: 0.246,
    “(38.5, 39.5)”: 0.2, “(39.5, 41.0)”: 0.103, “(41.0, 43.0)”: 0.086, “(43.0, 44.5)”:
    -0.93, “(44.5, 46.5)”: -1.153, “(46.5, 47.5)”: -1.132, “(47.5, 50.5)”: -0.301,
    “(50.5, 51.5)”: 0.104, “(51.5, 52.5)”: 0.121, “(52.5, 53.5)”: 0.065, “(53.5, 55.0)”:
    -0.637, “(55.0, 56.5)”: -0.627, “(56.5, 57.5)”: -0.648, “(57.5, 60.5)”: -0.628,
    “(60.5, 62.5)”: -0.977, “(62.5, 67.5)”: -0.945, “(67.5, 80.0)”: -0.887'
- en: 'Figure 3: (continues on the next page)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：（续下页）
- en: 'Lower Bounds (95%-Confidence Interval): ”(2.0, 2.5)”: -2.464, ”(2.5, 3.5)”:
    -0.518, ”(3.5, 12.5)”: -0.303, ”(12.5, 17.5)”: -0.314, ”(17.5, 20.0)”: -0.405,
    ”(20.0, 21.5)”: -0.299, ”(21.5, 25.5)”: 0.097, ”(25.5, 28.5)”: -0.12, ”(28.5,
    31.5)”: 0.11, ”(31.5, 33.5)”: 0.153, ”(33.5, 36.25)”: 0.132, ”(36.25, 36.75)”:
    -0.329, ”(36.75, 37.5)”: -0.321, ”(37.5, 38.5)”: -0.285, ”(38.5, 39.5)”: -0.36,
    ”(39.5, 41.0)”: -0.514, ”(41.0, 43.0)”: -0.467, ”(43.0, 44.5)”: -2.148, ”(44.5,
    46.5)”: -2.244, ”(46.5, 47.5)”: -2.242, ”(47.5, 50.5)”: -0.49, ”(50.5, 51.5)”:
    -0.522, ”(51.5, 52.5)”: -0.483, ”(52.5, 53.5)”: -0.59, ”(53.5, 55.0)”: -1.135,
    ”(55.0, 56.5)”: -1.135, ”(56.5, 57.5)”: -1.186, ”(57.5, 60.5)”: -1.165, ”(60.5,
    62.5)”: -1.889, ”(62.5, 67.5)”: -1.804, ”(67.5, 80.0)”: -1.604 Upper Bounds (95%-Confidence
    Interval): ”(2.0, 2.5)”: 1.848, ”(2.5, 3.5)”: 2.196, ”(3.5, 12.5)”: 2.123, ”(12.5,
    17.5)”: 2.122, ”(17.5, 20.0)”: 0.475, ”(20.0, 21.5)”: 0.588, ”(21.5, 25.5)”: 0.511,
    ”(25.5, 28.5)”: 0.869, ”(28.5, 31.5)”: 0.399, ”(31.5, 33.5)”: 0.544, ”(33.5, 36.25)”:
    0.665, ”(36.25, 36.75)”: 0.423, ”(36.75, 37.5)”: 0.396, ”(37.5, 38.5)”: 0.778,
    ”(38.5, 39.5)”: 0.759, ”(39.5, 41.0)”: 0.72, ”(41.0, 43.0)”: 0.639, ”(43.0, 44.5)”:
    0.288, ”(44.5, 46.5)”: -0.061, ”(46.5, 47.5)”: -0.023, ”(47.5, 50.5)”: -0.112,
    ”(50.5, 51.5)”: 0.73, ”(51.5, 52.5)”: 0.725, ”(52.5, 53.5)”: 0.719, ”(53.5, 55.0)”:
    -0.14, ”(55.0, 56.5)”: -0.118, ”(56.5, 57.5)”: -0.111, ”(57.5, 60.5)”: -0.09,
    ”(60.5, 62.5)”: -0.065, ”(62.5, 67.5)”: -0.087, ”(67.5, 80.0)”: -0.169 Please
    describe the general pattern of the graph. Response: [First generated response]
    User: Great, now please study the graph carefully and highlight any regions you
    may find surprising or counterintuitive. You may also suggest an explanation for
    why this behavior is surprising, and what may have caused it. Response: [Second
    generated response] User: Thanks. Now please provide a brief, at most 7 sentence
    summary of the influence of the feature on the outcome. Response: [Third generated
    response]'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下限（95%置信区间）：“（2.0，2.5）”：-2.464，“（2.5，3.5）”：-0.518，“（3.5，12.5）”：-0.303，“（12.5，17.5）”：-0.314，“（17.5，20.0）”：-0.405，“（20.0，21.5）”：-0.299，“（21.5，25.5）”：0.097，“（25.5，28.5）”：-0.12，“（28.5，31.5）”：0.11，“（31.5，33.5）”：0.153，“（33.5，36.25）”：0.132，“（36.25，36.75）”：-0.329，“（36.75，37.5）”：-0.321，“（37.5，38.5）”：-0.285，“（38.5，39.5）”：-0.36，“（39.5，41.0）”：-0.514，“（41.0，43.0）”：-0.467，“（43.0，44.5）”：-2.148，“（44.5，46.5）”：-2.244，“（46.5，47.5）”：-2.242，“（47.5，50.5）”：-0.49，“（50.5，51.5）”：-0.522，“（51.5，52.5）”：-0.483，“（52.5，53.5）”：-0.59，“（53.5，55.0）”：-1.135，“（55.0，56.5）”：-1.135，“（56.5，57.5）”：-1.186，“（57.5，60.5）”：-1.165，“（60.5，62.5）”：-1.889，“（62.5，67.5）”：-1.804，“（67.5，80.0）”：-1.604
    上限（95%置信区间）：“（2.0，2.5）”：1.848，“（2.5，3.5）”：2.196，“（3.5，12.5）”：2.123，“（12.5，17.5）”：2.122，“（17.5，20.0）”：0.475，“（20.0，21.5）”：0.588，“（21.5，25.5）”：0.511，“（25.5，28.5）”：0.869，“（28.5，31.5）”：0.399，“（31.5，33.5）”：0.544，“（33.5，36.25）”：0.665，“（36.25，36.75）”：0.423，“（36.75，37.5）”：0.396，“（37.5，38.5）”：0.778，“（38.5，39.5）”：0.759，“（39.5，41.0）”：0.72，“（41.0，43.0）”：0.639，“（43.0，44.5）”：0.288，“（44.5，46.5）”：-0.061，“（46.5，47.5）”：-0.023，“（47.5，50.5）”：-0.112，“（50.5，51.5）”：0.73，“（51.5，52.5）”：0.725，“（52.5，53.5）”：0.719，“（53.5，55.0）”：-0.14，“（55.0，56.5）”：-0.118，“（56.5，57.5）”：-0.111，“（57.5，60.5）”：-0.09，“（60.5，62.5）”：-0.065，“（62.5，67.5）”：-0.087，“（67.5，80.0）”：-0.169
    请描述图表的一般模式。回应：[第一生成的回应] 用户：很好，现在请仔细研究图表，突出任何你发现的令人惊讶或违反直觉的区域。你也可以提供对这些行为为何令人惊讶的解释，以及可能的原因。回应：[第二生成的回应]
    用户：谢谢。现在请提供一个简短的、最多7句话的总结，说明特征对结果的影响。回应：[第三生成的回应]
- en: 'Figure 4: The prompt structure to describe graphs that is the basis for the
    results in this paper. In the depicted example, we ask the model to describe the
    Age graph of a GAM trained on the Titanic dataset from Kaggle. Note that we provide
    the LLM with a description of the dataset and the meaning of the values on the
    y-axis in the graphs. The prompt structure is fairly general and can be easily
    amended to better fit various specific use cases.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：描述图表的提示结构，这是本文结果的基础。在所示的示例中，我们要求模型描述在Kaggle的泰坦尼克数据集上训练的GAM的年龄图。请注意，我们向LLM提供了数据集的描述以及图表中y轴值的含义。提示结构相当通用，可以很容易地调整以更好地适应各种特定用例。
