- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 17:35:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:35:15
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can LLMs Facilitate Interpretation of Pre-trained Language Models?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型能否促进对预训练语言模型的解释？
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.13386](https://ar5iv.labs.arxiv.org/html/2305.13386)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.13386](https://ar5iv.labs.arxiv.org/html/2305.13386)
- en: Basel Mousi            Nadir Durrani            Fahim Dalvi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Basel Mousi            Nadir Durrani            Fahim Dalvi
- en: Qatar Computing Research Institute, HBKU, Doha, Qatar
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 卡塔尔计算研究院，HBKU，多哈，卡塔尔
- en: '{bmousi,ndurrani,faimaduddin}@hbku.edu.qa'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{bmousi,ndurrani,faimaduddin}@hbku.edu.qa'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Work done to uncover the knowledge encoded within pre-trained language models
    rely on annotated corpora or human-in-the-loop methods. However, these approaches
    are limited in terms of scalability and the scope of interpretation. We propose
    using a large language model, ChatGPT, as an annotator to enable fine-grained
    interpretation analysis of pre-trained language models. We discover latent concepts
    within pre-trained language models by applying agglomerative hierarchical clustering
    over contextualized representations and then annotate these concepts using ChatGPT.
    Our findings demonstrate that ChatGPT produces accurate and semantically richer
    annotations compared to human-annotated concepts. Additionally, we showcase how
    GPT-based annotations empower interpretation analysis methodologies of which we
    demonstrate two: probing frameworks and neuron interpretation. To facilitate further
    exploration and experimentation in the field, we make available a substantial
    ConceptNet dataset (TCN) comprising 39,000 annotated concepts.¹¹1[https://neurox.qcri.org/projects/transformers-concept-net/](https://neurox.qcri.org/projects/transformers-concept-net/)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示预训练语言模型中编码知识的工作依赖于注释语料库或人工参与方法。然而，这些方法在扩展性和解释范围方面存在局限。我们提出使用大型语言模型 ChatGPT
    作为注释工具，以实现对预训练语言模型的细粒度解释分析。我们通过对上下文化表示进行聚合层次聚类来发现预训练语言模型中的潜在概念，然后使用 ChatGPT 对这些概念进行注释。我们的发现表明，ChatGPT
    生成的注释相比人工注释概念更准确且语义更丰富。此外，我们展示了基于 GPT 的注释如何增强解释分析方法论，我们展示了其中的两种：探测框架和神经元解释。为了促进该领域的进一步探索和实验，我们提供了一个包含
    39,000 个注释概念的 ConceptNet 数据集（TCN）。¹¹1[https://neurox.qcri.org/projects/transformers-concept-net/](https://neurox.qcri.org/projects/transformers-concept-net/)
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'A large body of work done on interpreting pre-trained language models answers
    the question: *What knowledge is learned within these models?* Researchers have
    investigated the concepts encoded in pre-trained language models by probing them
    against various linguistic properties, such as morphological (Vylomova et al.,
    [2017](#bib.bib46); Belinkov et al., [2017a](#bib.bib5)), syntactic (Linzen et al.,
    [2016](#bib.bib35); Conneau et al., [2018](#bib.bib10); Durrani et al., [2021](#bib.bib18)),
    and semantic (Qian et al., [2016](#bib.bib42); Belinkov et al., [2017b](#bib.bib7))
    tasks, among others. Much of the methodology used in these analyses heavily rely
    on either having access to an annotated corpus that pertains to the linguistic
    concept of interest Tenney et al. ([2019](#bib.bib45)); Liu et al. ([2019a](#bib.bib36));
    Belinkov et al. ([2020](#bib.bib6)), or involve human-in-the-loop Karpathy et al.
    ([2015](#bib.bib29)); Kádár et al. ([2017](#bib.bib28)); Geva et al. ([2021](#bib.bib21));
    Dalvi et al. ([2022](#bib.bib14)) to facilitate such an analysis. The use of pre-defined
    linguistic concepts restricts the scope of interpretation to only very general
    linguistic concepts, while human-in-the-loop methods are not scalable. *We circumvent
    this bottleneck by using a large language model, ChatGPT, as an annotator to enable
    fine-grained interpretation analysis.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大量关于解释预训练语言模型的研究回答了这个问题：*这些模型中学习了什么知识？* 研究人员通过将预训练语言模型与各种语言属性进行对比，探讨了其中编码的概念，如形态学（Vylomova
    等，[2017](#bib.bib46); Belinkov 等，[2017a](#bib.bib5)），句法学（Linzen 等，[2016](#bib.bib35);
    Conneau 等，[2018](#bib.bib10); Durrani 等，[2021](#bib.bib18)），以及语义学（Qian 等，[2016](#bib.bib42);
    Belinkov 等，[2017b](#bib.bib7)）任务等。许多分析中使用的方法主要依赖于获得与感兴趣的语言概念相关的注释语料库（Tenney 等，[2019](#bib.bib45);
    Liu 等，[2019a](#bib.bib36); Belinkov 等，[2020](#bib.bib6)），或涉及人工参与（Karpathy 等，[2015](#bib.bib29);
    Kádár 等，[2017](#bib.bib28); Geva 等，[2021](#bib.bib21); Dalvi 等，[2022](#bib.bib14)）以促进这种分析。使用预定义语言概念限制了解释的范围，仅限于非常一般的语言概念，而人工参与的方法则无法扩展。*我们通过使用大型语言模型
    ChatGPT 作为注释工具来绕过这一瓶颈，从而实现细粒度的解释分析。*
- en: '![Refer to caption](img/fcb4c4d6751f069fb12292f161f4242d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fcb4c4d6751f069fb12292f161f4242d.png)'
- en: 'Figure 1: ChatGPT as an annotator: Human annotation or taggers trained on pre-defined
    concepts, cover only a fraction of a model’s concept space. ChatGPT enables scaling
    up annotation to include nearly all concepts, including the concepts that may
    not have been manually annotated before.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：ChatGPT 作为标注工具：人工标注或基于预定义概念训练的标注员仅覆盖了模型概念空间的一部分。ChatGPT 使标注工作得以扩展，涵盖几乎所有的概念，包括那些之前可能没有手动标注过的概念。
- en: Generative Pre-trained Transformers (GPT) have been trained on an unprecedented
    amount of textual data, enabling them to develop a substantial understanding of
    natural language. As their capabilities continue to improve, researchers are finding
    creative ways to leverage their assistance for various applications, such as question-answering
    in financial and medical domains Guo et al. ([2023](#bib.bib24)), simplifying
    medical reports Jeblick et al. ([2022](#bib.bib27)), and detecting stance Zhang
    et al. ([2023](#bib.bib49)). We carry out an investigation of whether GPT models,
    specifically ChatGPT, can aid in the interpretation of pre-trained language models
    (pLMs).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式预训练变换器（GPT）已在前所未有的大量文本数据上进行训练，使其能够对自然语言有深入的理解。随着其能力的不断提升，研究人员正在寻找创新的方法来利用其协助进行各种应用，如金融和医疗领域的问答
    Guo et al. ([2023](#bib.bib24))，简化医疗报告 Jeblick et al. ([2022](#bib.bib27))，以及检测立场
    Zhang et al. ([2023](#bib.bib49))。我们进行了一项调查，以确定 GPT 模型，特别是 ChatGPT，是否能够帮助解释预训练语言模型（pLMs）。
- en: A fascinating characteristic of neural language models is that words sharing
    any linguistic relationship cluster together in high-dimensional spaces Mikolov
    et al. ([2013](#bib.bib41)). Recent research Michael et al. ([2020](#bib.bib40));
    Fu and Lapata ([2022](#bib.bib20)); Dalvi et al. ([2022](#bib.bib14)) has built
    upon this idea by exploring representation analysis through latent spaces in pre-trained
    models. Building on the work of Dalvi et al. ([2022](#bib.bib14)) we aim to identify
    encoded concepts within pre-trained models using agglomerative hierarchical clustering
    Gowda and Krishna ([1978](#bib.bib23)) on contextualized representations. The
    underlying hypothesis is that these clusters represent latent concepts, capturing
    the language knowledge acquired by the model. Unlike previous approaches that
    rely on predefined concepts Michael et al. ([2020](#bib.bib40)); Sajjad et al.
    ([2022b](#bib.bib44)) or human annotation Alam et al. ([2023](#bib.bib2)) to label
    these concepts, we leverage the ChatGPT model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言模型的一个迷人特性是，具有任何语言学关系的词汇在高维空间中会聚集在一起 Mikolov et al. ([2013](#bib.bib41))。最近的研究
    Michael et al. ([2020](#bib.bib40)); Fu and Lapata ([2022](#bib.bib20)); Dalvi
    et al. ([2022](#bib.bib14)) 在此基础上通过对预训练模型中的潜在空间进行表示分析。基于 Dalvi et al. ([2022](#bib.bib14))
    的工作，我们旨在通过对上下文化表示的聚合层次聚类 Gowda and Krishna ([1978](#bib.bib23)) 来识别预训练模型中的编码概念。基本假设是这些簇代表潜在概念，捕捉了模型所获得的语言知识。与依赖于预定义概念
    Michael et al. ([2020](#bib.bib40)); Sajjad et al. ([2022b](#bib.bib44)) 或人工标注
    Alam et al. ([2023](#bib.bib2)) 来标注这些概念的先前方法不同，我们利用了 ChatGPT 模型。
- en: 'Our findings indicate that the annotations produced by ChatGPT are semantically
    richer and accurate compared to the human-annotated concepts (for instance BERT
    Concept NET). Notably, ChatGPT correctly labeled the majority of concepts deemed
    uninterpretable by human annotators. Using an LLM like ChatGPT improves scalability
    and accuracy. For instance, the work in Dalvi et al. ([2022](#bib.bib14)) was
    limited to 269 concepts in the final layer of the BERT-base-cased Devlin et al.
    ([2019](#bib.bib15)) model, while human annotations in Geva et al. ([2021](#bib.bib21))
    were confined to 100 keys per layer. Using ChatGPT, the exploration can be scaled
    to the entire latent space of the models and many more architectures. We used
    GPT to annotate 39K concepts across 5 pre-trained language models. Building upon
    this finding, we further demonstrate that GPT-based annotations empowers methodologies
    in interpretation analysis of which we show two: i) probing framework Belinkov
    et al. ([2017a](#bib.bib5)), ii) neuron analysis Antverg and Belinkov ([2022](#bib.bib3)).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究结果表明，与人工标注的概念（例如 BERT Concept NET）相比，ChatGPT生成的注释在语义上更丰富、更准确。值得注意的是，ChatGPT正确标记了大多数被人工注释者认为不可解释的概念。使用像
    ChatGPT 这样的 LLM 可以提高可扩展性和准确性。例如，Dalvi 等人的工作 ([2022](#bib.bib14)) 限于 BERT-base-cased
    Devlin 等人的最终层模型 ([2019](#bib.bib15)) 的 269 个概念，而 Geva 等人的人工注释 ([2021](#bib.bib21))
    限于每层 100 个键。使用 ChatGPT，可以将探索扩展到模型的整个潜在空间以及更多架构。我们使用 GPT 注释了 39K 个概念，涉及 5 个预训练语言模型。在此基础上，我们进一步证明了基于
    GPT 的注释如何增强解释分析的方法，我们展示了两种：i) 探测框架 Belinkov 等人 ([2017a](#bib.bib5))，ii) 神经元分析
    Antverg 和 Belinkov ([2022](#bib.bib3))。
- en: '![Refer to caption](img/068a4679820f32018d75d52eac4502f3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/068a4679820f32018d75d52eac4502f3.png)'
- en: (a) Hyphenated Superlatives
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 连字符超级词
- en: '![Refer to caption](img/0767f76a6494c889bfb352a25154e0e8.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0767f76a6494c889bfb352a25154e0e8.png)'
- en: (b) Verbs
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 动词
- en: '![Refer to caption](img/59bfd14190292ac90b825dc81a8822a2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/59bfd14190292ac90b825dc81a8822a2.png)'
- en: (c) Arab Names
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 阿拉伯名字
- en: 'Figure 2: Illustrative Examples of Concept Learned in BERT: word groups organized
    based on (a) Lexical, (b) Parts of Speech, and (c) Semantic property'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：BERT中学到的概念的说明性示例：基于(a) 词汇、(b) 词性和(c) 语义属性组织的词组
- en: Probing Framework
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探测框架
- en: We train probes from GPT-annotated concept representations to explore concepts
    that go beyond conventional linguistic categories. For instance, instead of probing
    for named entities (e.g. NE:PER), we can investigate whether a model distinguishes
    between male and female names or probing for “Cities in the southeastern United
    States” instead of NE:LOC.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练从 GPT 注释的概念表示中获得探针，以探索超越传统语言类别的概念。例如，我们可以调查模型是否区分男性和女性名字，或者探测“美国东南部的城市”而不是
    NE:LOC。
- en: Neuron Analysis
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经元分析
- en: Another line of work that we illustrate to benefit from GPT-annotated latent
    concepts is the neuron analysis i.e. discovering neurons that capture a linguistic
    phenomenon. In contrast to the holistic view offered by representation analysis,
    neuron analysis highlights the *role of individual neurons* (or groups of them)
    within a neural network (Sajjad et al. ([2022a](#bib.bib43)). We obtain neuron
    rankings for GPT-annotated latent concepts using a neuron ranking method called
    Probeless Antverg and Belinkov ([2022](#bib.bib3)). Such fine-grained interpertation
    analyses of latent spaces enable us to see *how neurons distribute in hierarchical
    ontologies.* For instance, instead of simply identifying neurons associated with
    the POS:Adverbs, we can now uncover how neurons are distributed across sub-concepts
    such as adverbs of time (e.g., “tomorrow”) and adverbs of frequency (e.g., “daily”).
    Or instead of discovering neurons for named entities (e.g. NE:PER), we can discover
    neurons that capture “Muslim Names” versus “Hindu Names”.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示的另一个从 GPT 注释的潜在概念中受益的工作是神经元分析，即发现捕捉语言现象的神经元。与表示分析提供的整体视图相比，神经元分析突出了*单个神经元的作用*（或其组）在神经网络中的作用（Sajjad
    等人 ([2022a](#bib.bib43))）。我们使用一种称为 Probeless 的神经元排名方法来获得 GPT 注释的潜在概念的神经元排名（Antverg
    和 Belinkov ([2022](#bib.bib3))）。这种对潜在空间的细致解释分析使我们能够看到*神经元在层次本体中的分布*。例如，与简单地识别与
    POS:Adverbs 相关的神经元不同，我们现在可以发现神经元如何在时间副词（例如，“明天”）和频率副词（例如，“每日”）等子概念之间分布。或者与发现命名实体的神经元（例如
    NE:PER）相比，我们可以发现捕捉“穆斯林名字”与“印度教名字”的神经元。
- en: 'To summarize, we make the following contributions in this work:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们在这项工作中做出了以下贡献：
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our demonstration reveals that ChatGPT offers comprehensive and precise labels
    for latent concepts acquired within pLMs.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的演示揭示了 ChatGPT 为在 pLMs 中获得的潜在概念提供了全面而准确的标签。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We showcased the GPT-based annotations of latent concepts empower methods in
    interpretation analysis by showing two applications: Probing Classifiers and Neuron
    Analysis.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了基于 GPT 的潜在概念注释如何通过两个应用来增强解释分析方法：探测分类器和神经元分析。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We release *Transformers Concept-Net*, an extensive dataset containing 39K annotated
    concepts to facilitate the interpretation of pLMs.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发布了 *Transformers Concept-Net*，这是一个包含 39K 注释概念的广泛数据集，以便于 pLMs 的解释。
- en: 2 Methodology
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: We discover latent concepts by applying clustering on feature vectors (§[2.1](#S2.SS1
    "2.1 Concept Discovery ‣ 2 Methodology ‣ Can LLMs Facilitate Interpretation of
    Pre-trained Language Models?")). They are then labeled using ChatGPT (§[2.2](#S2.SS2
    "2.2 Concept Annotation ‣ 2 Methodology ‣ Can LLMs Facilitate Interpretation of
    Pre-trained Language Models?")) and used for fine-grained interpretation analysis
    (§[2.3](#S2.SS3 "2.3 Concept Probing ‣ 2 Methodology ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?") and [2.4](#S2.SS4 "2.4 Concept Neurons ‣ 2 Methodology
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")). A visual
    representation of this process is shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?").
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对特征向量应用聚类来发现潜在概念 (§[2.1](#S2.SS1 "2.1 Concept Discovery ‣ 2 Methodology
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")）。然后，使用
    ChatGPT (§[2.2](#S2.SS2 "2.2 Concept Annotation ‣ 2 Methodology ‣ Can LLMs Facilitate
    Interpretation of Pre-trained Language Models?")) 对其进行标注，并用于细粒度解释分析 (§[2.3](#S2.SS3
    "2.3 Concept Probing ‣ 2 Methodology ‣ Can LLMs Facilitate Interpretation of Pre-trained
    Language Models?") 和 [2.4](#S2.SS4 "2.4 Concept Neurons ‣ 2 Methodology ‣ Can
    LLMs Facilitate Interpretation of Pre-trained Language Models?"))。该过程的视觉表示见图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can LLMs Facilitate Interpretation of Pre-trained
    Language Models?")。
- en: 2.1 Concept Discovery
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 概念发现
- en: 'Contextualized word representations learned in pre-trained language models,
    can identify meaningful groupings based on various linguistic phenomenon. These
    groups represent concepts encoded within pLMs. Our investigation expands upon
    the work done in discovering latent ontologies in contextualized representations
    Michael et al. ([2020](#bib.bib40)); Dalvi et al. ([2022](#bib.bib14)). At a high
    level, feature vectors (contextualized representations) are first generated by
    performing a forward pass on the model. These representations are then clustered
    to discover the encoded concepts. Consider a pre-trained model $\mathbf{M}$ with
    $L$ layers: ${l_{1},l_{2},\ldots,l_{L}}$. Using dataset ${\mathbb{D}}={w_{1},w_{2},...,w_{N}}$,
    we generate feature vectors ${\mathbb{D}}\xrightarrow{\mathbb{M}}\mathbf{z}^{l}={\mathbf{z}^{l}_{1},\dots,\mathbf{z}^{l}_{n}}$.²²2$z_{i}$
    denotes the contextualized representation for word $w_{i}$ Agglomerative hierarchical
    clustering is employed to cluster the words. Initially, each word forms its own
    cluster. Clusters are then merged iteratively based on Ward’s minimum variance
    criterion, using intra-cluster variance as dissimilarity measure. The squared
    Euclidean distance evaluates the similarity between vector representations. The
    algorithm stops when $K$ clusters (encoded concepts) are formed, with $K$ being
    a hyper-parameter.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语言模型中学习的上下文词表示可以基于各种语言现象识别有意义的分组。这些分组表示了在 pLMs 中编码的概念。我们的研究扩展了在上下文表示中发现潜在本体论的工作，参考了
    Michael 等人（[2020](#bib.bib40)）和 Dalvi 等人（[2022](#bib.bib14)）。从高层次来看，特征向量（上下文表示）首先通过对模型进行前向传播生成。然后对这些表示进行聚类以发现编码的概念。考虑一个有
    $L$ 层的预训练模型 $\mathbf{M}$：${l_{1},l_{2},\ldots,l_{L}}$。使用数据集 ${\mathbb{D}}={w_{1},w_{2},...,w_{N}}$，我们生成特征向量
    ${\mathbb{D}}\xrightarrow{\mathbb{M}}\mathbf{z}^{l}={\mathbf{z}^{l}_{1},\dots,\mathbf{z}^{l}_{n}}$。$z_{i}$
    表示词 $w_{i}$ 的上下文表示。使用聚合层次聚类方法对词进行聚类。最初，每个词形成自己的簇。然后，根据 Ward 的最小方差准则，使用簇内方差作为不相似性度量，迭代地合并簇。平方欧几里得距离评估向量表示之间的相似性。当形成
    $K$ 个簇（编码的概念）时，算法停止，其中 $K$ 是一个超参数。
- en: 2.2 Concept Annotation
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 概念注释
- en: Encoded concepts capture latent relationships among words within a cluster,
    encompassing various forms of similarity such as lexical, syntactic, semantic,
    or specific patterns relevant to the task or data. Figure [2](#S1.F2 "Figure 2
    ‣ 1 Introduction ‣ Can LLMs Facilitate Interpretation of Pre-trained Language
    Models?") provides illustrative examples of concepts encoded in the BERT-base-cased
    model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 编码的概念捕捉了集群中单词之间的潜在关系，包括词汇、句法、语义等各种形式的相似性，或与任务或数据相关的特定模式。图[2](#S1.F2 "图 2 ‣ 1
    介绍 ‣ LLMs 能否促进对预训练语言模型的解释？")展示了在 BERT-base-cased 模型中编码的概念的示例。
- en: This work leverages the recent advancements in prompt-based approaches, which
    are enabled by large language models such as GPT-3 Brown et al. ([2020](#bib.bib8)).
    Specifically, we utilize a zero-shot learning strategy, where the model is solely
    provided with a natural language instruction that describes the task of labeling
    the concept. We used ChatGPT with zero-shot prompt to annotate the latent concepts
    with the following settings:³³3We experimented with several prompts, see Appendix
    [A.1](#A1.SS1 "A.1 Optimal Prompt ‣ Appendix A Prompts ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?") for details.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作利用了基于提示的方法的最新进展，这些方法由像 GPT-3 Brown et al. ([2020](#bib.bib8)) 这样的语言模型提供支持。具体来说，我们使用了零样本学习策略，即模型仅通过描述标注概念任务的自然语言指令进行训练。我们使用了带有零样本提示的
    ChatGPT 来标注潜在概念，设置见附录 [A.1](#A1.SS1 "A.1 最佳提示 ‣ 附录 A 提示 ‣ LLMs 能否促进对预训练语言模型的解释？")。
- en: Assistant is a large language model trained by OpenAI
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Assistant 是由 OpenAI 训练的大型语言模型
- en: 'Instructions:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 指令：
- en: 'Give a short and concise label that best describes the following list of words:
    [‘‘word 1’’, ‘‘word 2’’, ..., ‘‘word N’’]'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一个简短且精确的标签，最佳描述以下单词列表：[‘‘word 1’’, ‘‘word 2’’, ..., ‘‘word N’’]
- en: 2.3 Concept Probing
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 概念探测
- en: 'Our large scale annotations of the concepts in pLMs enable training probes
    towards fine-grained concepts that lack pre-defined annotations. For example we
    can use probing to assess whether a model has learned concepts that involve biases
    related to gender, race, or religion. By tracing the input sentences that correspond
    to an encoded concept $C$ in a pre-trained model, we create annotations for a
    particular concept. We perform fine-grained concept probing by extracting feature
    vectors from annotated data through a forward pass on the model of interest. Then,
    we train a binary classifier to predict the concept and use the probe accuracy
    as a qualitative measure of how well the model represents the concept. Formally,
    given a set of tokens ${\mathbb{W}}=\{w_{1},w_{2},...,w_{N}\}\in C$, we generate
    feature vectors, a sequence of latent representations: ${\mathbb{W}}\xrightarrow{\mathbb{M}}\mathbf{z}^{l}=\{\mathbf{z}^{l}_{1},\dots,\mathbf{z}^{l}_{n}\}$
    for each word $w_{i}$ by doing a forward pass over $s_{i}$. We then train a binary
    classifier over the representations to predict the concept $C$ minimizing the
    cross-entropy loss:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 pLMs 中概念的大规模注释使得针对缺乏预定义注释的细粒度概念的探测成为可能。例如，我们可以使用探测来评估模型是否学会了涉及性别、种族或宗教偏见的概念。通过追踪与预训练模型中编码的概念
    $C$ 相对应的输入句子，我们为特定概念创建注释。我们通过在感兴趣的模型上进行前向传递，从注释数据中提取特征向量，从而进行细粒度概念探测。然后，我们训练一个二分类器来预测概念
    $C$，并使用探测准确率作为模型表示该概念的定性指标。形式上，给定一组标记 ${\mathbb{W}}=\{w_{1},w_{2},...,w_{N}\}\in
    C$，我们生成特征向量，即潜在表示的序列：${\mathbb{W}}\xrightarrow{\mathbb{M}}\mathbf{z}^{l}=\{\mathbf{z}^{l}_{1},\dots,\mathbf{z}^{l}_{n}\}$，通过对
    $s_{i}$ 进行前向传递来处理每个单词 $w_{i}$。然后，我们在表示上训练一个二分类器来预测概念 $C$，最小化交叉熵损失：
- en: '|  | $\mathcal{L}(\theta)=-\sum_{i}\log P_{\theta}(\mathbf{c}_{i}&#124;\mathbf{w}_{i})$
    |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\theta)=-\sum_{i}\log P_{\theta}(\mathbf{c}_{i}&#124;\mathbf{w}_{i})$
    |  |'
- en: where $P_{\theta}(\mathbf{c}_{i}|\mathbf{z}_{i})=\frac{\exp(\theta_{l}\cdot\mathbf{z}_{i})}{\sum_{c^{\prime}}\exp(\theta_{l^{\prime}}\cdot\mathbf{z}_{i})}$
    is the probability that word $\mathbf{x}_{i}$ is assigned concept $\mathbf{c}$.
    We learn the weights $\theta\in\mathbb{R}^{D\times L}$ using gradient descent.
    Here $D$ is the dimensionality of the latent representations $\mathbf{z}_{i}$
    and $L$ is the size of the concept set which is 2 for a binary classifier.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{\theta}(\mathbf{c}_{i}|\mathbf{z}_{i})=\frac{\exp(\theta_{l}\cdot\mathbf{z}_{i})}{\sum_{c^{\prime}}\exp(\theta_{l^{\prime}}\cdot\mathbf{z}_{i})}$
    是单词 $\mathbf{x}_{i}$ 被分配到概念 $\mathbf{c}$ 的概率。我们使用梯度下降法学习权重 $\theta\in\mathbb{R}^{D\times
    L}$。这里 $D$ 是潜在表示 $\mathbf{z}_{i}$ 的维度，而 $L$ 是概念集的大小，对于二分类器来说为 2。
- en: 2.4 Concept Neurons
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 概念神经元
- en: 'An alternative area of research in interpreting NLP models involves conducting
    representation analysis at a more fine-grained level, specifically focusing on
    individual neurons. Our demonstration showcases how the extensive annotations
    of latent concepts enhance the analysis of neurons towards more intricate concepts.
    We show this by using a neuron ranking method called Probeless Antverg and Belinkov
    ([2022](#bib.bib3)) over our concept representations. The method obtains neuron
    rankings using an accumulative strategy, where the score of a given neuron $n$
    towards a concept $C$ is defined as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解释NLP模型的研究领域涉及在更精细的层面上进行表示分析，特别是关注单个神经元。我们的演示展示了潜在概念的广泛注释如何增强对神经元的分析，进而理解更复杂的概念。我们通过使用一种叫做Probeless
    Antverg的神经元排名方法以及Belinkov ([2022](#bib.bib3))对我们的概念表示进行展示。该方法使用累积策略获得神经元排名，其中给定神经元$n$对概念$C$的得分定义如下：
- en: '|  | $R(n,\mathcal{C})=\mu(\mathcal{C})-\mu(\hat{\mathcal{C}})$ |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(n,\mathcal{C})=\mu(\mathcal{C})-\mu(\hat{\mathcal{C}})$ |  |'
- en: where $\mu(\mathcal{C})$ is the average of all activations $z(n,w)$, $w\in\mathcal{C}$,
    and $\mu(\hat{\mathcal{C}})$ is the average of activations over the random concept
    set. Note that the ranking for each neuron $n$ is computed independently.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mu(\mathcal{C})$是所有激活值$z(n,w)$的平均值，$w\in\mathcal{C}$，$\mu(\hat{\mathcal{C}})$是随机概念集上激活值的平均值。请注意，每个神经元$n$的排名是独立计算的。
- en: 3 Experimental Setup
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: Latent Concept Data
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 潜在概念数据
- en: We used a subset of the WMT News 2018 dataset, containing 250K randomly chosen
    sentences ($\approx$5M tokens). We set a word occurrence threshold of 10 and restricted
    each word type to a maximum of 10 occurrences. This selection was made to reduce
    computational and memory requirements when clustering high-dimensional vectors.
    We preserved the original embedding space to avoid information loss through dimensionality
    reduction techniques like PCA. Consequently, our final dataset consisted of 25,000
    word types, each represented by 10 contexts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了WMT News 2018数据集的一个子集，其中包含250K随机选择的句子（$\approx$5M tokens）。我们设置了10的词频阈值，并将每个词类型限制为最多10次出现。这一选择是为了在聚类高维向量时减少计算和内存需求。我们保留了原始嵌入空间，以避免通过降维技术（如PCA）造成的信息丢失。因此，我们的最终数据集包含25,000个词类型，每个词类型由10个上下文表示。
- en: Concept Discovery
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概念发现
- en: We apply agglomerative hierarchical clustering on contextualized feature vectors
    acquired through a forward pass on a pLM for the given data. The resulting representations
    in each layer are then clustered into 600 groups.⁴⁴4Dalvi et al. ([2022](#bib.bib14))
    discovered that selecting $K$ within the range of $600-1000$ struck a satisfactory
    balance between the pitfalls of excessive clustering and insufficient clustering.
    Their exploration of other methods ELbow and Silhouette did not yield reliable
    results.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对通过前向传递在pLM上获得的上下文特征向量应用了聚合层次聚类。然后，将每一层中得到的表示聚类成600个组。⁴⁴4Dalvi等人 ([2022](#bib.bib14))
    发现，在$600-1000$的范围内选择$K$，能够在过度聚类和不足聚类的陷阱之间找到令人满意的平衡。他们对其他方法ELbow和Silhouette的探索未能产生可靠的结果。
- en: Concept Annotation
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概念注释
- en: We used ChatGPT available through Azure OpenAI service⁵⁵5[https://azure.microsoft.com/en-us/products/cognitive-services/openai-service](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service)
    to carryout the annotations. We used a *temperature* of 0 and a *top p* value
    of 0.95\. Setting the temperature to 0 controls the randomness in the output and
    produces deterministic responses.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了通过Azure OpenAI服务提供的ChatGPT⁵⁵5[https://azure.microsoft.com/en-us/products/cognitive-services/openai-service](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service)来进行注释。我们使用了*temperature*为0和*top
    p*值为0.95。将温度设置为0可以控制输出的随机性，产生确定性的响应。
- en: Pre-trained Models
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Our study involved several 12-layered transformer models, including BERT-cased
    (Devlin et al., [2019](#bib.bib15)), RoBERTa Liu et al. ([2019b](#bib.bib38)),
    XLNet Yang et al. ([2019](#bib.bib48)), and ALBERT Lan et al. ([2019](#bib.bib32))
    and XLM-RoBERTa (XLM-R) (Conneau et al., [2020](#bib.bib9)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究涉及几个12层的变换器模型，包括BERT-cased (Devlin等人，[2019](#bib.bib15))，RoBERTa Liu等人
    ([2019b](#bib.bib38))，XLNet Yang等人 ([2019](#bib.bib48))，以及ALBERT Lan等人 ([2019](#bib.bib32))和XLM-RoBERTa
    (XLM-R) (Conneau等人，[2020](#bib.bib9))。
- en: Probing and Neuron Analysis
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探测和神经元分析
- en: For each annotated concept, we extract feature vectors using the relevant data.
    We then train linear classifiers with a categorical cross-entropy loss function,
    optimized using Adam Kingma and Ba ([2014](#bib.bib30)). The training process
    involved shuffled mini-batches of size 512 and was concluded after 10 epochs.
    We used a data split of 60-20-20 for train, dev, test when training classifiers.
    We use the same representations to obtain neuron rankings. We use NeuroX toolkit
    Dalvi et al. ([2023a](#bib.bib11)) to train our probes and run neuron analysis.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个标注的概念，我们使用相关数据提取特征向量。然后，我们使用分类交叉熵损失函数训练线性分类器，并采用 Adam Kingma 和 Ba ([2014](#bib.bib30))
    进行优化。训练过程涉及大小为 512 的随机小批量，并在 10 个周期后结束。我们在训练分类器时使用了 60-20-20 的数据划分用于训练、开发和测试。我们使用相同的表示方法来获得神经元排名。我们使用
    NeuroX 工具包 Dalvi 等人 ([2023a](#bib.bib11)) 来训练我们的探测器并进行神经元分析。
- en: '| Q1 | Acceptable | Unacceptable |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Q1 | 可接受 | 不可接受 |'
- en: '| --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Majority | 244 | 25 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 大多数 | 244 | 25 |'
- en: '| Fliess Kappa | 0.71 ("Substantial agreement") |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Fliess Kappa | 0.71（“显著一致”） |'
- en: '| Q2 | Precise | Imprecise |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Q2 | 精准 | 不精准 |'
- en: '| Majority | 181 | 60 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 大多数 | 181 | 60 |'
- en: '| Fliess Kappa | 0.34 ("Fair agreement") |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Fliess Kappa | 0.34（“公平一致”） |'
- en: 'Table 1: Inter-annotator agreement with 3 annotators. Q1: Whether the label
    is acceptable or unacceptable? Q2: Of the acceptable annotations how many are
    precise versus imprecise?'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：与 3 名标注者的标注一致性。Q1：标签是可接受的还是不可接受的？Q2：在可接受的标注中，有多少是精准的与不精准的？
- en: '| Q3 | GPT $\uparrow$ | Equal | BCN $\uparrow$ | No Majority |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Q3 | GPT $\uparrow$ | 相等 | BCN $\uparrow$ | 无多数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Majority | 82 | 121 | 58 | 8 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 大多数 | 82 | 121 | 58 | 8 |'
- en: '| Fliess Kappa | 0.56 ("Moderate agreement") |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Fliess Kappa | 0.56（“中等一致”） |'
- en: 'Table 2: Annotation for Q3 with 3 choices: GPT is better, labels are equivalent,
    human annotation is better.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：Q3 的标注，包含 3 个选择：GPT 更好、标签相等、人工标注更好。
- en: 4 Evaluation and Analysis
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估与分析
- en: 4.1 Results
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 结果
- en: 'To validate ChatGPT’s effectiveness as an annotator, we conducted a human evaluation.
    Evaluators were shown a concept through a word cloud, along with sample sentences
    representing the concept and the corresponding GPT annotation. They were then
    asked the following questions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证 ChatGPT 作为标注者的有效性，我们进行了人工评估。评估者通过词云查看一个概念，以及表示该概念的示例句子和相应的 GPT 标注。然后，他们被问以下问题：
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Q1: Is the label produced by ChatGPT Acceptable or Unacceptable? Unacceptable
    annotations include incorrect labels or those that ChatGPT was unable to annotate.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Q1：ChatGPT 产生的标签是可接受的还是不可接受的？不可接受的标注包括错误标签或 ChatGPT 无法标注的标签。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Q2: If a label is Acceptable, is it Precise or Imprecise? While a label may
    be deemed acceptable, it may not convey the relationship between the underlying
    words in the concept accurately. This question aims to measure the precision of
    the label itself.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Q2：如果标签是可接受的，它是精准的还是不精准的？虽然一个标签可能被认为是可接受的，但它可能无法准确传达概念中潜在词语之间的关系。这个问题旨在衡量标签本身的精准度。
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Q3: Is the ChatGPT label Superior or Inferior to human annotation? BCN labels
    provided by Dalvi et al. ([2022](#bib.bib14)) are used as human annotations for
    this question.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Q3：ChatGPT 的标签是否优于或劣于人工标注？由 Dalvi 等人提供的 BCN 标签 ([2022](#bib.bib14)) 用作该问题的人工标注。
- en: In the first half of Table [1](#S3.T1 "Table 1 ‣ Probing and Neuron Analysis
    ‣ 3 Experimental Setup ‣ Can LLMs Facilitate Interpretation of Pre-trained Language
    Models?"), the results indicate that $90.7\%$ of the ChatGPT labels were considered
    Acceptable. Within the acceptable labels, $75.1\%$ were deemed Precise, while
    $24.9\%$ were found to be Imprecise (indicated by Q2 in Table [1](#S3.T1 "Table
    1 ‣ Probing and Neuron Analysis ‣ 3 Experimental Setup ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?")). We also computed Fleiss’ Kappa (Fleiss et al.,
    [2013](#bib.bib19)) to measure agreement among the 3 annotators. For Q1, the inter-annotator
    agreement was found to be $0.71$ which is considered *substantial* according to
     Landis and Koch ([1977](#bib.bib33)). However, for Q2, the agreement was $0.34$
    (indicating a fair level of agreement among annotators). This was expected due
    to the complexity and subjectivity of the task in Q2 for example annotators’ knowledge
    and perspective on precise and imprecise labels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[1](#S3.T1 "表1 ‣ 探测与神经元分析 ‣ 3 实验设置 ‣ LLM能否促进预训练语言模型的解释？")的前半部分，结果显示$90.7\%$的ChatGPT标签被认为是可接受的。在这些可接受的标签中，$75.1\%$被认为是精确的，而$24.9\%$被发现是不精确的（在表[1](#S3.T1
    "表1 ‣ 探测与神经元分析 ‣ 3 实验设置 ‣ LLM能否促进预训练语言模型的解释？")中由Q2表示）。我们还计算了Fleiss’ Kappa (Fleiss等，[2013](#bib.bib19))以衡量3名注释者之间的一致性。对于Q1，注释者之间的一致性为$0.71$，根据Landis和Koch
    ([1977](#bib.bib33))，这一水平被认为是*显著的*。然而，对于Q2，一致性为$0.34$（表示注释者之间的一致性水平一般）。这是由于Q2任务的复杂性和主观性，例如注释者对精确和不精确标签的知识和观点。
- en: ChatGPT Labels versus Human Annotations
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ChatGPT标签与人工注释
- en: 'Next we compare the quality of ChatGPT labels to the human annotations using
    BERT Concept Net, a human annotated collection of latent concepts learned within
    the representations of BERT. BCN, however, was annotated in the form of Concept
    Type:Concept Sub Type (e.g., SEM:entertainment:sport:ice_hockey) unlike GPT-based
    annotations that are natural language descriptions (e.g. Terms related to ice
    hockey). Despite their lack of natural language, these reference annotations prove
    valuable for drawing comparative analysis between humans and ChatGPT. For Q3,
    we presented humans with a word cloud and three options to choose from: whether
    the LLM annotations are better, equalivalent, or worse than the BCN annotations.
    We found that ChatGPT outperformed or achieved equal performance to BCN annotations
    in $75.5\%$ of cases, as shown in Table [2](#S3.T2 "Table 2 ‣ Probing and Neuron
    Analysis ‣ 3 Experimental Setup ‣ Can LLMs Facilitate Interpretation of Pre-trained
    Language Models?"). The inter-annotator agreement for Q3 was found to be $0.56$
    which is considered *moderate*.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用BERT概念网将ChatGPT标签的质量与人工注释进行比较。BCN以概念类型：概念子类型（例如，SEM:entertainment:sport:ice_hockey）的形式进行注释，而GPT-based注释则是自然语言描述（例如，与冰球相关的术语）。尽管缺乏自然语言，这些参考注释对于在人工和ChatGPT之间进行比较分析仍然非常有价值。对于Q3，我们向人类展示了一个词云和三个选项：LLM注释是否比BCN注释更好、相等或更差。我们发现，在$75.5\%$的情况下，ChatGPT表现优于或与BCN注释相当，如表[2](#S3.T2
    "表2 ‣ 探测与神经元分析 ‣ 3 实验设置 ‣ LLM能否促进预训练语言模型的解释？")所示。Q3的注释者之间的一致性为$0.56$，被认为是*中等的*。
- en: '| Annotation | SEM | LEX | Morph | SYN | Unint. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 注释 | SEM | LEX | Morph | SYN | Unint. |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| ChatGPT | 85.5 | 1.1 | 3.0 | X | 3.3 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 85.5 | 1.1 | 3.0 | X | 3.3 |'
- en: '| BCN | 68.4 | 16.7 | 3.0 | 2.2 | 9.7 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| BCN | 68.4 | 16.7 | 3.0 | 2.2 | 9.7 |'
- en: 'Table 3: Distribution (percentages) of concept types in ChatGPT Labels vs.
    Human Annotations: Semantic, Lexical, Morphological, Syntactic, Uninterpretable'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '表3: ChatGPT标签与人工注释中的概念类型分布（百分比）：语义、词汇、形态学、句法、不可解释'
- en: 4.2 Error Analysis
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 错误分析
- en: '![Refer to caption](img/324bd6088f6c06e6594ed4da47c49c74.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/324bd6088f6c06e6594ed4da47c49c74.png)'
- en: (a) Crime and Assault
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 犯罪与攻击
- en: '![Refer to caption](img/fa647bb21e902f879d94c03338818916.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa647bb21e902f879d94c03338818916.png)'
- en: (b) 3rd Person Singular Present-tense
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第三人称单数现在时
- en: '![Refer to caption](img/b1151e1ece906b97d33103424d136ee1.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b1151e1ece906b97d33103424d136ee1.png)'
- en: (c) Rock Bands and Artists in the US
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 美国的摇滚乐队和艺术家
- en: '![Refer to caption](img/eeea7c592072fdec5ce4246a0be9dad3.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eeea7c592072fdec5ce4246a0be9dad3.png)'
- en: A total of 285 could prove to be rather impressive on a pitch that was turning
    almost from the first ball .
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 总计285分在几乎从第一球开始就显得相当令人印象深刻的投球中显得尤为突出。
- en: But there were runs at Cheltenham , where Gloucestershire finished on 315 for
    seven , with the only century of the entire Championship day going to Ryan Higgins
    , out for a 161-ball 105 .
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 但在切尔滕纳姆，格洛斯特郡以315分七人出局结束比赛，整个锦标赛日唯一的世纪分数归属于Ryan Higgins，他打了161球得105分。
- en: Back in Nottingham , it was the turn of Australia ’s bowlers to be eviscerated
    this time as Eoin Morgan ’s team rewrote the record books once again by making
    a phenomenal 481 for six in this third one-day international .
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回到诺丁汉，这一次轮到澳大利亚的投球手被彻底击溃，Eoin Morgan 的球队再次刷新了纪录，在这场第三场一日国际赛中打出了惊人的481分六人出局。
- en: (d) Cricket Scores
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 板球得分
- en: 'Figure 3: Failed cases for ChatGPT labeling: a) Non-labeled concepts due to
    LLM content policy, b) Failing to identify correct linguistic relation, c) Imprecise
    labeling d) Imprecise labels despite providing context'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：ChatGPT标记失败的案例：a) 由于LLM内容政策未标记的概念，b) 未能识别正确的语言关系，c) 标记不准确，d) 尽管提供了上下文，但标签不准确
- en: The annotators identified 58 concepts where human annotated BCN labels were
    deemed superior. We have conducted an error analysis of these instances and will
    now delve into the cases where GPT did not perform well.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注释员识别出58个概念，其中人类注释的BCN标签被认为更优。我们已经对这些实例进行了错误分析，现在将深入探讨GPT表现不佳的案例。
- en: Sensitive Content Models
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 敏感内容模型
- en: In 10 cases, the API calls triggered one of the content policy models and failed
    to provide a label. The content policy models aim to prevent the dissemination
    of harmful, abusive, or offensive content, including hate speech, misinformation,
    and illegal activities. Figure [3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4.2 Error Analysis
    ‣ 4 Evaluation and Analysis ‣ Can LLMs Facilitate Interpretation of Pre-trained
    Language Models?") shows an example of a sensitive concept that includes words
    related to crime and assault. This problem can be mitigated by using a version
    of LLM where content policy models are not enabled.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在10个案例中，API调用触发了内容政策模型，并未提供标签。内容政策模型旨在防止传播有害、虐待性或冒犯性内容，包括仇恨言论、虚假信息和非法活动。图[3(a)](#S4.F3.sf1
    "在图3 ‣ 4.2 错误分析 ‣ 4 评估与分析 ‣ LLMs能否促进对预训练语言模型的解释？")展示了一个包含与犯罪和攻击相关的词汇的敏感概念。这个问题可以通过使用没有启用内容政策模型的LLM版本来缓解。
- en: Linguistic Ontologies
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言本体
- en: In 8 of the concepts, human annotations (BCN) were better because the concepts
    were composed of words that were related through a lexical, morphological, or
    syntactic relationship. The default prompt we used to label the concept tends
    to find semantic similarity between the words, which did not exist in these concepts.
    For example, Figure [3(b)](#S4.F3.sf2 "In Figure 3 ‣ 4.2 Error Analysis ‣ 4 Evaluation
    and Analysis ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")
    shows a concept composed of 3rd person singular present-tense verbs, but ChatGPT
    incorrectly labels it as Actions/Events in News Articles. However, humans are
    robust and can fall back to consider various linguistic ontologies.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在8个概念中，人类注释（BCN）更好，因为这些概念由通过词汇、形态或句法关系相关的词汇组成。我们用于标记概念的默认提示倾向于找到词汇之间的语义相似性，而这些概念中并不存在这种相似性。例如，图[3(b)](#S4.F3.sf2
    "在图3 ‣ 4.2 错误分析 ‣ 4 评估与分析 ‣ LLMs能否促进对预训练语言模型的解释？")展示了一个由第三人称单数现在时动词组成的概念，但ChatGPT错误地将其标记为新闻文章中的动作/事件。然而，人类能够灵活考虑各种语言本体。
- en: The BCN concepts are categorized into semantic, syntactic, morphological, and
    lexical groups (See Table [3](#S4.T3 "Table 3 ‣ ChatGPT Labels versus Human Annotations
    ‣ 4.1 Results ‣ 4 Evaluation and Analysis ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?")). As observed, both humans and ChatGPT found
    semantic meaning to the concept in majority of the cases. However, humans were
    also able to identify other linguistic relations such as lexical (e.g. grouped
    by a lexical property like abbreviations), morphological (e.g. grouped by the
    same parts-of-speech), or syntactic (e.g. grouped by position in the sentence).
    Note however, that prompts can be modified to capture specific linguistic property.
    We encourage interested readers to see our experiments on this in Appendix [A.2](#A1.SS2
    "A.2 Prompts For Lexical Concepts ‣ Appendix A Prompts ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?")-[A.3](#A1.SS3 "A.3 Prompts for POS Concepts
    ‣ Appendix A Prompts ‣ Can LLMs Facilitate Interpretation of Pre-trained Language
    Models?").
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: BCN概念被分类为语义、句法、形态学和词汇组（见表[3](#S4.T3 "表 3 ‣ ChatGPT 标签与人工注释 ‣ 4.1 结果 ‣ 4 评估与分析
    ‣ 大型语言模型能否促进预训练语言模型的解释？")）。如观察到的，人工和ChatGPT在大多数情况下都找到了概念的语义意义。然而，人类还能够识别其他语言关系，如词汇（例如，按词汇属性如缩写分组）、形态学（例如，按相同词性分组）或句法（例如，按句子中的位置分组）。但请注意，提示可以被修改以捕捉特定的语言属性。我们鼓励有兴趣的读者查看我们在附录[A.2](#A1.SS2
    "A.2 词汇概念提示 ‣ 附录 A 提示 ‣ 大型语言模型能否促进预训练语言模型的解释？")-[A.3](#A1.SS3 "A.3 词性概念提示 ‣ 附录
    A 提示 ‣ 大型语言模型能否促进预训练语言模型的解释？")中的实验。
- en: Insufficient Context
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文不足
- en: 'Sometimes context contextual information is important to correctly label a
    concept. While human annotators (of the BCN corpus) were provided with the sentences
    in which the underlying words appeared, we did not provide the same to ChatGPT
    to keep the prompt cost-effective. However, providing context sentences in the
    prompt⁶⁶6We gave 10 context sentences to ChatGPT. along with the concept to label
    resulted in improved labels for 11 of the remaining 40 error cases. Figure [3(d)](#S4.F3.sf4
    "In Figure 3 ‣ 4.2 Error Analysis ‣ 4 Evaluation and Analysis ‣ Can LLMs Facilitate
    Interpretation of Pre-trained Language Models?") shows one such example where
    providing contextual information made ChatGPT to correctly label the concept as
    Cricket Scores as opposed to Numerical Data the label that it gives without seeing
    contextual information. However, providing context information didn’t consistently
    prove helpful. Figure [3(c)](#S4.F3.sf3 "In Figure 3 ‣ 4.2 Error Analysis ‣ 4
    Evaluation and Analysis ‣ Can LLMs Facilitate Interpretation of Pre-trained Language
    Models?") shows a concept, where providing contextual information did not result
    in the accurate label: Rock Bands and Artists in the US, as identified by the
    humans.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，上下文信息对于正确标记概念很重要。虽然BCN语料库的人工注释者获得了出现基础词的句子，但我们没有提供相同的内容给ChatGPT以保持提示的成本效益。然而，将上下文句子与要标记的概念一起提供，导致剩余40个错误案例中的11个改进标签。图[3(d)](#S4.F3.sf4
    "图 3 ‣ 4.2 错误分析 ‣ 4 评估与分析 ‣ 大型语言模型能否促进预训练语言模型的解释？")展示了一个这样的例子，其中提供上下文信息使ChatGPT正确地将概念标记为板球比分，而不是在没有看到上下文信息的情况下给出的数值数据标签。然而，提供上下文信息并没有始终证明有帮助。图[3(c)](#S4.F3.sf3
    "图 3 ‣ 4.2 错误分析 ‣ 4 评估与分析 ‣ 大型语言模型能否促进预训练语言模型的解释？")展示了一个概念，其中提供上下文信息并没有产生准确的标签：美国的摇滚乐队和艺术家，正如人工注释者所识别的。
- en: Uninterpretable Concepts
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无法解释的概念
- en: Conversely, we also annotated concepts that were considered uninterpretable
    or non-meaningful by the human annotators in the BCN corpus and in 21 out 26 cases,
    ChatGPT accurately assigned labels to these concepts. The proficiency of ChatGPT
    in processing extensive textual data enables it to provide accurate labels for
    these concepts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们也对BCN语料库中被认为不可解释或无意义的概念进行了注释，在26个案例中的21个，ChatGPT准确地为这些概念分配了标签。ChatGPT处理大量文本数据的能力使其能够为这些概念提供准确的标签。
- en: 5 Concept-based Interpretation Analysis
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基于概念的解释分析
- en: Now that we have established the capability of large language models like ChatGPT
    in providing rich semantic annotations, we will showcase how these annotations
    can facilitate extensive fine-grained analysis on a large scale.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定了像ChatGPT这样的语言模型在提供丰富语义注释方面的能力，我们将展示这些注释如何促进大规模的细粒度分析。
- en: 5.1 Probing Classifiers
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 探测分类器
- en: '| tag | Label | ALBERT | XLNet |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| tag | 标签 | ALBERT | XLNet |'
- en: '| --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| c301 | Gender-related Nouns and pronouns | 0.95 | 0.86 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| c301 | 性别相关名词和代词 | 0.95 | 0.86 |'
- en: '| c533 | LGBTQ+ | 0.97 | 0.97 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| c533 | LGBTQ+ | 0.97 | 0.97 |'
- en: '| c439 | Sports commentary terms | 0.91 | 0.81 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| c439 | 体育评论术语 | 0.91 | 0.81 |'
- en: '| c173 | Football team names and stadiums | 0.96 | 0.94 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| c173 | 足球队名称和体育场馆 | 0.96 | 0.94 |'
- en: '| c348 | Female names and titles | 0.98 | 0.94 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| c348 | 女性姓名和头衔 | 0.98 | 0.94 |'
- en: '| c149 | Tennis players’ names | 0.95 | 0.92 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| c149 | 网球运动员姓名 | 0.95 | 0.92 |'
- en: '| c487 | Spanish Male Names | 0.96 | 0.94 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| c487 | 西班牙男性姓名 | 0.96 | 0.94 |'
- en: '| c564 | Cities and Universities in southeastern US | 0.97 | 0.90 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| c564 | 东南部城市和大学 | 0.97 | 0.90 |'
- en: '| c263 | Locations in New York City | 0.95 | 0.92 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| c263 | 纽约市的地点 | 0.95 | 0.92 |'
- en: '| c247 | Scandinavian/Nordic names and places | 0.98 | 0.95 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| c247 | 斯堪的纳维亚/北欧姓名和地点 | 0.98 | 0.95 |'
- en: '| c438 | Verbs for various actions and outcomes | 0.94 | 0.87 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| c438 | 各种动作和结果的动词 | 0.94 | 0.87 |'
- en: '| c44 | Southeast Asian Politics and Ethnic Conflict | 0.97 | 0.94 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| c44 | 东南亚政治和民族冲突 | 0.97 | 0.94 |'
- en: '| c421 | Names of people and places in the middle east | 0.94 | 0.95 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| c421 | 中东地区的人员和地点名称 | 0.94 | 0.95 |'
- en: '| c245 | Middle East conflict | 1.00 | 0.93 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| c245 | 中东冲突 | 1.00 | 0.93 |'
- en: '| c553 | Islamic terminology | 0.96 | 0.89 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| c553 | 伊斯兰术语 | 0.96 | 0.89 |'
- en: '| c365 | Criminal activities | 0.93 | 0.89 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| c365 | 犯罪活动 | 0.93 | 0.89 |'
- en: '| c128 | Medical and Healthcare terminology | 0.98 | 0.95 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| c128 | 医疗和保健术语 | 0.98 | 0.95 |'
- en: 'Table 4: Using latent concepts to make cross-model comparison using Probing
    Classifiers'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 使用潜在概念进行跨模型比较，使用探测分类器'
- en: 'Probing classifiers is among the earlier techniques used for interpretability,
    aimed at examining the knowledge encapsulated in learned representations. However,
    their application is constrained by the availability of supervised annotations,
    which often focus on conventional linguistic knowledge and are subject to inherent
    limitations Hewitt and Liang ([2019](#bib.bib26)). We demonstrate that using GPT-based
    annotation of latent concepts learned within these models enables a direct application
    towards fine-grained probing analysis. By annotating the latent space of five
    renowned pre-trained language models (pLMs): BERT, ALBERT, XLM-R, XLNet, and RoBERTa
    – we developed a comprehensive Transformers Concept Net. This net encompasses
    39,000 labeled concepts, facilitating cross-architectural comparisons among the
    models. Table [4](#S5.T4 "Table 4 ‣ 5.1 Probing Classifiers ‣ 5 Concept-based
    Interpretation Analysis ‣ Can LLMs Facilitate Interpretation of Pre-trained Language
    Models?") showcases a subset⁷⁷7For a larger sample of concepts and additional
    models, please refer to Appendix [B](#A2 "Appendix B Probing Classifiers ‣ Can
    LLMs Facilitate Interpretation of Pre-trained Language Models?"). of results comparing
    ALBERT and XLNet through probing classifiers.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 探索分类器是早期用于解释性分析的技术之一，旨在检查学习到的表示中的知识。然而，它们的应用受到监督注释的限制，这些注释通常集中于传统语言知识，并且存在固有的局限性
    Hewitt 和 Liang ([2019](#bib.bib26))。我们展示了使用基于GPT的潜在概念注释，这些模型中学习到的概念能够直接用于细粒度的探测分析。通过对五个著名的预训练语言模型（pLMs）：BERT、ALBERT、XLM-R、XLNet
    和 RoBERTa 的潜在空间进行注释，我们开发了一个全面的 Transformers Concept Net。该网络涵盖了39,000个标记概念，便于在模型之间进行跨架构比较。表
    [4](#S5.T4 "Table 4 ‣ 5.1 Probing Classifiers ‣ 5 Concept-based Interpretation
    Analysis ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")
    展示了通过探测分类器比较 ALBERT 和 XLNet 的一个子集⁷⁷7有关更多概念样本和附加模型，请参见附录 [B](#A2 "Appendix B Probing
    Classifiers ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")。
- en: We can see that the model learns concepts that may not directly align with the
    pre-defined human onotology. For example, it learns a concept based on Spanish
    Male Names or Football team names and stadiums. Identifying how fine-grained concepts
    are encoded within the latent space of a model enable applications beyond interpretation
    analysis. For example it has direct application in model editing Meng et al. ([2023](#bib.bib39))
    which first trace where the model store any concept and then change the relevant
    parameters to modify its behavior. Moreover, identifying concepts that are associated
    with gender (e.g., Female names and titles), religion (e.g. Islamic Terminology),
    and ethnicity (e.g., Nordic names) can aid in elucidating the biases present in
    these models.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模型学习的概念可能不会直接与预定义的人类本体对齐。例如，它学习了基于西班牙男性名字或足球队名称和体育场的概念。识别模型潜在空间中如何编码细粒度概念，使得应用超越了解释分析。例如，这在模型编辑
    Meng 等人 ([2023](#bib.bib39)) 中有直接应用，首先追踪模型存储任何概念的位置，然后更改相关参数以修改其行为。此外，识别与性别（例如，女性名字和头衔）、宗教（例如，伊斯兰术语）和民族（例如，北欧名字）相关的概念，有助于阐明这些模型中存在的偏见。
- en: 5.2 Neuron Analysis
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 神经元分析
- en: '| Super Concept | # Sub Concepts | Alignment |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 超级概念 | # 子概念 | 对齐 |'
- en: '| Adverbs | 17 | 0.36 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 副词 | 17 | 0.36 |'
- en: '| $\hookrightarrow$ c155: Frequency and manner | 0.30 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c155: 频率和方式 | 0.30 |'
- en: '| $\hookrightarrow$ c136: Degree/Intensity | 0.30 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c136: 程度/强度 | 0.30 |'
- en: '| $\hookrightarrow$ c057: Frequency | 0.40 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c057: 频率 | 0.40 |'
- en: '| Nouns | 13 | 0.28 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 名词 | 13 | 0.28 |'
- en: '| $\hookrightarrow$ c231: Activities and Objects | 0.60 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c231: 活动和物体 | 0.60 |'
- en: '| $\hookrightarrow$ c279: Industries/Sectors | 0.60 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c279: 行业/部门 | 0.60 |'
- en: '| $\hookrightarrow$ c440: Professions | 0.10 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c440: 职业 | 0.10 |'
- en: '| Adjectives | 17 | 0.21 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 形容词 | 17 | 0.21 |'
- en: '| $\hookrightarrow$ c299: Product Attributes | 0.30 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c299: 产品属性 | 0.30 |'
- en: '| $\hookrightarrow$ c053: Comparative Adjectives | 0.30 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c053: 比较级形容词 | 0.30 |'
- en: '| $\hookrightarrow$ c128: Quality/Appropriateness | 0.40 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c128: 质量/适当性 | 0.40 |'
- en: '| Numbers | 17 | 0.23 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 数字 | 17 | 0.23 |'
- en: '| $\hookrightarrow$ c549: Prices | 0.50 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c549: 价格 | 0.50 |'
- en: '| $\hookrightarrow$ c080: Quantities | 0.10 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c080: 数量 | 0.10 |'
- en: '| $\hookrightarrow$ c593: Monetary Values | 0.10 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| $\hookrightarrow$ c593: 货币价值 | 0.10 |'
- en: 'Figure 4: Neuron Analysis on *Super Concepts* extracted from BERT-base-cased-POS
    model. The alignment column shows the intersection between the top 10 neurons
    in the Super concept and the Sub concepts. For detailed results please check Appendix
    [C](#A3 "Appendix C Neuron Analysis Results ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?") (See Table [10](#A3.T10 "Table 10 ‣ Neurons
    Associated with the Names concepts ‣ Appendix C Neuron Analysis Results ‣ Can
    LLMs Facilitate Interpretation of Pre-trained Language Models?"))'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：从BERT-base-cased-POS模型中提取的*超级概念*的神经元分析。对齐列显示超级概念中的前10个神经元与子概念之间的交集。有关详细结果，请参阅附录[C](#A3
    "附录 C 神经元分析结果 ‣ LLM是否有助于对预训练语言模型的解释？")（见表[10](#A3.T10 "表 10 ‣ 与名称概念相关的神经元 ‣ 附录
    C 神经元分析结果 ‣ LLM是否有助于对预训练语言模型的解释？")）
- en: '![Refer to caption](img/bbd8a7d26e0ecda89a3d572f662979ac.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bbd8a7d26e0ecda89a3d572f662979ac.png)'
- en: 'Figure 5: Neuron overlap between an Adverb Super Concept and sub concepts.
    Sub concepts shown are Adverbs of frequency and manner (c155), Adverbs of degree/intensity
    (c136), Adverbs of Probability and Certainty (c265), Adverbs of Frequency (c57),
    Adverbs of manner and opinion (c332), Adverbs of preference/choice (c570), Adverbs
    indicating degree or extent (c244), Adverbs of Time (c222).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：副词超级概念与子概念之间的神经元重叠。显示的子概念包括频率和方式的副词（c155）、程度/强度的副词（c136）、概率和确定性的副词（c265）、频率副词（c57）、方式和观点副词（c332）、偏好/选择副词（c570）、表示程度或范围的副词（c244）、时间副词（c222）。
- en: Neuron analysis examines the individual neurons or groups of neurons within
    neural NLP models to gain insights into how the model represents linguistic knowledge.
    However, similar to general interpretability, previous studies in neuron analysis
    are also constrained by human-in-the-loop Karpathy et al. ([2015](#bib.bib29));
    Kádár et al. ([2017](#bib.bib28)) or pre-defined linguistic knowledge Hennigen
    et al. ([2020](#bib.bib25)); Durrani et al. ([2022](#bib.bib17)). Consequently,
    the resulting neuron explanations are subject to the same limitations we address
    in this study.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元分析研究神经自然语言处理模型中的单个神经元或神经元组，以深入了解模型如何表示语言知识。然而，与一般可解释性相似，之前的神经元分析研究也受到人为干预的限制（Karpathy等，[2015](#bib.bib29)）；Kádár等，[2017](#bib.bib28)）或预定义语言知识（Hennigen等，[2020](#bib.bib25)）；Durrani等，[2022](#bib.bib17)）。因此，所得的神经元解释也受到我们在本研究中讨论的相同限制。
- en: 'Our work demonstrates that annotating the latent space enables neuron analysis
    of intricate linguistic hierarchies learned within these models. For example,
    Dalvi et al. ([2019](#bib.bib12)) and Hennigen et al. ([2020](#bib.bib25)) only
    carried out analysis using very coarse morphological categories (e.g. adverbs,
    nouns etc.) in parts-of-speech tags. We now showcase how our discovery and annotations
    of fine-grained latent concepts leads to a deeper neuron analysis of these models.
    In our analysis of BERT-based part-of-speech tagging model, we discovered 17 fine-grained
    concepts of adverb (in the final layer). It is evident that BERT learns a highly
    detailed semantic hierarchy, as maintains separate concepts for the adverbs of
    frequency (e.g., “rarely, sometimes”) versus adverbs of manner (e.g., “quickly,
    softly”). We employed the *Probeless* method Antverg and Belinkov ([2022](#bib.bib3))
    to search for neurons associated with specific kinds of adverbs. We also create
    a super adverb concept encompassing all types of adverbs, serving as the overarching
    and generic representation for this linguistic category and obtain neurons associated
    with the concept. We then compare the neuron ranking obtained from the super concept
    to the individual rankings from sub concepts. Interestingly, our findings revealed
    that the top-ranking neurons responsible for learning the super concept are often
    distributed among the top neurons associated with specialized concepts, as shown
    in Figure [5](#S5.F5 "Figure 5 ‣ 5.2 Neuron Analysis ‣ 5 Concept-based Interpretation
    Analysis ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")
    for adverbial concepts. The results, presented in Table [5](#S5.F5 "Figure 5 ‣
    5.2 Neuron Analysis ‣ 5 Concept-based Interpretation Analysis ‣ Can LLMs Facilitate
    Interpretation of Pre-trained Language Models?"), include the number of discovered
    sub concepts in the column labeled # Sub Concepts and the Alignment column indicates
    the percentage of overlap in the top 10 neurons between the super and sub concepts
    for each specific adverb concept. The average alignment across all sub concepts
    is indicated next to the super concept. This observation held consistently across
    various properties (e.g. Nouns, Adjectives and Numbers) as shown in Table [5](#S5.F5
    "Figure 5 ‣ 5.2 Neuron Analysis ‣ 5 Concept-based Interpretation Analysis ‣ Can
    LLMs Facilitate Interpretation of Pre-trained Language Models?"). For further
    details please refer to Appendix [C](#A3 "Appendix C Neuron Analysis Results ‣
    Can LLMs Facilitate Interpretation of Pre-trained Language Models?")).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的研究表明，标注潜在空间可以实现对这些模型中学习到的复杂语言层级的神经元分析。例如，Dalvi 等人（[2019](#bib.bib12)）和 Hennigen
    等人（[2020](#bib.bib25)）仅使用了非常粗略的形态学类别（例如副词、名词等）进行分析。我们现在展示了我们对细粒度潜在概念的发现和标注如何导致对这些模型进行更深入的神经元分析。在我们对基于
    BERT 的词性标注模型的分析中，我们发现了 17 个细粒度的副词概念（在最终层）。显而易见，BERT 学习了一个高度详细的语义层级，因为它为频率副词（例如，“很少，有时”）和方式副词（例如，“迅速，轻柔”）维持了不同的概念。我们采用了*Probeless*
    方法（Antverg 和 Belinkov，[2022](#bib.bib3)）来搜索与特定类型副词相关的神经元。我们还创建了一个超级副词概念，涵盖所有类型的副词，作为这一语言类别的总体和通用表示，并获取与该概念相关的神经元。然后，我们将从超级概念中获得的神经元排名与来自子概念的个别排名进行比较。有趣的是，我们的发现揭示了，负责学习超级概念的高排名神经元往往分布在与专门概念相关的高排名神经元之间，如图
    [5](#S5.F5 "Figure 5 ‣ 5.2 Neuron Analysis ‣ 5 Concept-based Interpretation Analysis
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?") 中的副词概念所示。结果见表
    [5](#S5.F5 "Figure 5 ‣ 5.2 Neuron Analysis ‣ 5 Concept-based Interpretation Analysis
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")，包括在标记为
    # Sub Concepts 的列中发现的子概念数量，Alignment 列指示超级概念和每个特定副词概念的子概念之间前 10 个神经元的重叠百分比。所有子概念的平均对齐度在超级概念旁边标示。这个观察在各种属性（例如名词、形容词和数字）中一致存在，如表
    [5](#S5.F5 "Figure 5 ‣ 5.2 Neuron Analysis ‣ 5 Concept-based Interpretation Analysis
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?") 所示。有关更多详细信息，请参见附录
    [C](#A3 "Appendix C Neuron Analysis Results ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?")。'
- en: Note that previously, we couldn’t identify neurons with such specific explanations,
    like distinguishing neurons for numbers related to currency values from those
    for years of birth or neurons differentiating between cricket and hockey-related
    terms. Our large scale concept annotation enables locating neurons that capture
    the fine-grained aspects of a concept. This enables applications such as manipulating
    network’s behavior in relation to that concept. For instance, Bau et al. ([2019](#bib.bib4))
    identified “tense” neurons within Neural Machine Translation (NMT) models and
    successfully changed the output from past to present tense by modifying the activation
    of these specific neurons. However, their study was restricted to very few coarse
    concepts for which annotations were available.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以前我们无法用如此具体的解释来识别神经元，例如区分与货币值相关的数字神经元与与出生年份相关的神经元，或区分与板球和曲棍球相关术语的神经元。我们的大规模概念标注使得定位捕捉概念细微差别的神经元成为可能。这使得可以在网络的行为中与该概念相关的应用成为可能。例如，Bau
    等人（[2019](#bib.bib4)）在神经机器翻译（NMT）模型中识别了“时态”神经元，并通过修改这些特定神经元的激活成功地将输出从过去时改为现在时。然而，他们的研究仅限于少数几种可用标注的粗略概念。
- en: 6 Related Work
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: With the ever-evolving capabilities of the LLMs, researchers are actively exploring
    innovative ways to harness their assistance. Prompt engineering, the process of
    crafting instructions to guide the behavior and extract relevant knowledge from
    these oracles, has emerged as a new area of research Lester et al. ([2021](#bib.bib34));
    Liu et al. ([2021](#bib.bib37)); Kojima et al. ([2023](#bib.bib31)); Abdelali
    et al. ([2023](#bib.bib1)); Dalvi et al. ([2023b](#bib.bib13)). Recent work has
    established LLMs as highly proficient annotators. Ding et al. ([2022](#bib.bib16))
    carried out evaluation of GPT-3’s performance as a data annotator for text classification
    and named entity recognition tasks, employing three primary methodologies to assess
    its effectiveness. Wang et al. ([2021](#bib.bib47)) showed that GPT-3 as an annotator
    can reduce cost from 50-96% compared to human annotations on 9 NLP tasks. They
    also showed that models trained using GPT-3 labeled data outperformed the GPT-3
    few-shot learner. Similarly, Gilardi et al. ([2023](#bib.bib22)) showed that ChatGPT
    achieves higher zero-shot accuracy compared to crowd-source workers in various
    annotation tasks, encompassing relevance, stance, topics, and frames detection.
    Our work is different from previous work done using GPT as annotator. We annotate
    the latent concepts encoded within the embedding space of pre-trained language
    models. We demonstrate how such a large scale annotation enriches representation
    analysis via application in probing classifiers and neuron analysis.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）能力的不断发展，研究人员正在积极探索利用其帮助的创新方式。提示工程，即制定指导行为并从这些神谕中提取相关知识的指令的过程，已成为一个新的研究领域
    Lester 等人（[2021](#bib.bib34)）；Liu 等人（[2021](#bib.bib37)）；Kojima 等人（[2023](#bib.bib31)）；Abdelali
    等人（[2023](#bib.bib1)）；Dalvi 等人（[2023b](#bib.bib13)）。最近的研究已经确立了LLMs作为高效标注者的地位。Ding
    等人（[2022](#bib.bib16)）对GPT-3作为文本分类和命名实体识别任务的数据标注者的性能进行了评估，采用了三种主要方法来评估其有效性。Wang
    等人（[2021](#bib.bib47)）显示，相比于人工标注，GPT-3作为标注者在9个NLP任务中能够将成本降低50-96%。他们还显示，使用GPT-3标注数据训练的模型优于GPT-3少样本学习者。同样，Gilardi
    等人（[2023](#bib.bib22)）显示，ChatGPT在各种标注任务中，如相关性、立场、主题和框架检测，较众包工人具有更高的零样本准确率。我们的工作不同于以前使用GPT作为标注者的研究。我们标注了预训练语言模型的嵌入空间中编码的潜在概念。我们展示了大规模标注如何通过应用于探测分类器和神经元分析来丰富表示分析。
- en: 7 Conclusion
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'The scope of previous studies in interpreting neural language models is limited
    to general ontologies or small-scale manually labeled concepts. In our research,
    we showcase the effectiveness of Large Language Models, specifically ChatGPT,
    as a valuable tool for annotating latent spaces in pre-trained language models.
    This large-scale annotation of latent concepts broadens the scope of interpretation
    from human-defined ontologies to encompass all concepts learned within the model,
    and eliminates the human-in-the-loop effort for annotating these concepts. We
    release a comprehensive GPT-annotated Transformers Concept Net (TCN) consisting
    of 39,000 concepts, extracted from a wide range of transformer language models.
    TCN empowers the researchers to carry out large-scale interpretation studies of
    these models. To demonstrate this, we employ two widely used techniques in the
    field of interpretability: probing classifiers and neuron analysis. This novel
    dimension of analysis, previously absent in earlier studies, sheds light on intricate
    aspects of these models. By showcasing the superiority, adaptability, and diverse
    applications of ChatGPT annotations, we lay the groundwork for a more comprehensive
    understanding of NLP models.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以往研究中对神经语言模型的解释范围局限于一般本体论或小规模手工标记的概念。在我们的研究中，我们展示了大型语言模型，特别是 ChatGPT，作为对预训练语言模型中的潜在空间进行标注的有价值工具的有效性。这种大规模潜在概念的标注将解释范围从人类定义的本体论拓展到涵盖模型中学习的所有概念，并消除了人工标注这些概念的努力。我们发布了一个全面的
    GPT 注释变压器概念网（TCN），包含 39,000 个概念，这些概念从广泛的变压器语言模型中提取。TCN 使研究人员能够进行大规模的模型解释研究。为了展示这一点，我们采用了两个广泛使用的可解释性领域技术：探测分类器和神经元分析。这种以前研究中未曾出现的新颖分析维度揭示了这些模型的复杂方面。通过展示
    ChatGPT 注释的优越性、适应性和多样化应用，我们为更全面理解 NLP 模型奠定了基础。
- en: Limitations
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'We list below limitations of our work:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了以下工作的局限性：
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While it has been demonstrated that LLMs significantly reduce the cost of annotations,
    the computational requirements and response latency can still become a significant
    challenge when dealing with extensive or high-throughput annotation pipeline like
    ours. In some cases it is important to provide contextual information along with
    the concept to obtain an accurate annotation, causing the cost go up. Nevertheless,
    this is a one time cost for any specific model, and there is optimism that future
    LLMs will become more cost-effective to run.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管已经证明 LLMs 显著降低了标注成本，但在处理大量或高吞吐量标注管道（如我们的管道）时，计算需求和响应延迟仍可能成为重大挑战。在某些情况下，提供上下文信息以获得准确的标注是很重要的，这会增加成本。然而，这对于任何特定模型来说都是一次性成本，并且对未来的
    LLMs 更具成本效益持乐观态度。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Existing LLMs are deployed with content policy filters aimed at preventing the
    dissemination of harmful, abusive, or offensive content. However, this limitation
    prevents the models from effectively labeling concepts that reveal sensitive information,
    such as cultural and racial biases learned within the model to be interpreted.
    For example, we were unable to extract a label for racial slurs in the hate speech
    detection task. This restricts our concept annotation approach to only tasks that
    are not sensitive to the content policy.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有的 LLMs 部署了内容政策过滤器，旨在防止传播有害、滥用或冒犯性的内容。然而，这一限制阻止了模型有效标注揭示敏感信息的概念，如模型中学到的文化和种族偏见。例如，我们无法在仇恨言论检测任务中提取种族侮辱的标签。这限制了我们的概念标注方法仅适用于对内容政策不敏感的任务。
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The information in the world is evolving, and LLMs will require continuous updates
    to reflect the accurate state of the world. This may pose a challenge for some
    problems (e.g. news summarization task) where the model needs to reflect an updated
    state of the world.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 世界上的信息在不断演变，LLMs 需要持续更新以反映世界的准确状态。这可能对某些问题（例如新闻摘要任务）构成挑战，因为模型需要反映更新后的世界状态。
- en: References
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdelali et al. (2023) Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury,
    Maram Hasanain, Basel Mousi, Sabri Boughorbel, Yassine El Kheir, Daniel Izham,
    Fahim Dalvi, Majd Hawasly, Nizi Nazar, Yousseif Elshahawy, Ahmed Ali, Nadir Durrani,
    Natasa Milic-Frayling, and Firoj Alam. 2023. [Benchmarking arabic ai with large
    language models](http://arxiv.org/abs/2305.14982).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelali 等人 (2023) Ahmed Abdelali、Hamdy Mubarak、Shammur Absar Chowdhury、Maram
    Hasanain、Basel Mousi、Sabri Boughorbel、Yassine El Kheir、Daniel Izham、Fahim Dalvi、Majd
    Hawasly、Nizi Nazar、Yousseif Elshahawy、Ahmed Ali、Nadir Durrani、Natasa Milic-Frayling
    和 Firoj Alam。2023年。[使用大型语言模型基准化阿拉伯语 AI](http://arxiv.org/abs/2305.14982)。
- en: 'Alam et al. (2023) Firoj Alam, Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Khan,
    Abdul Rafae, and Jia Xu. 2023. Conceptx: A framework for latent concept analysis.
    In *Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence
    (AAAI, Poster presentation)*, pages 16395–16397.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alam 等人 (2023) Firoj Alam、Fahim Dalvi、Nadir Durrani、Hassan Sajjad、Khan、Abdul
    Rafae 和 Jia Xu。2023年。Conceptx: 潜在概念分析框架。在 *第37届AAAI人工智能会议（AAAI，海报展示）* 中，页码 16395–16397。'
- en: Antverg and Belinkov (2022) Omer Antverg and Yonatan Belinkov. 2022. [On the
    pitfalls of analyzing individual neurons in language models](https://openreview.net/forum?id=8uz0EWPQIMu).
    In *International Conference on Learning Representations*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antverg 和 Belinkov (2022) Omer Antverg 和 Yonatan Belinkov。2022年。[关于分析语言模型中个别神经元的陷阱](https://openreview.net/forum?id=8uz0EWPQIMu)。在
    *国际学习表示会议* 中。
- en: Bau et al. (2019) Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani,
    Fahim Dalvi, and James Glass. 2019. [Identifying and controlling important neurons
    in neural machine translation](https://openreview.net/forum?id=H1z-PsR5KX). In
    *Proceedings of the Seventh International Conference on Learning Representations*,
    ICLR ’19, New Orleans, USA.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bau 等人 (2019) Anthony Bau、Yonatan Belinkov、Hassan Sajjad、Nadir Durrani、Fahim
    Dalvi 和 James Glass。2019年。[识别和控制神经机器翻译中的重要神经元](https://openreview.net/forum?id=H1z-PsR5KX)。在
    *第七届学习表示国际会议* 中，ICLR ’19，新奥尔良，美国。
- en: Belinkov et al. (2017a) Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan
    Sajjad, and James Glass. 2017a. [What do neural machine translation models learn
    about morphology?](https://doi.org/10.18653/v1/P17-1080) In *Proceedings of the
    55th Annual Meeting of the Association for Computational Linguistics*, ACL ’17,
    pages 861–872, Vancouver, Canada. Association for Computational Linguistics.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belinkov 等人 (2017a) Yonatan Belinkov、Nadir Durrani、Fahim Dalvi、Hassan Sajjad
    和 James Glass。2017a。[神经机器翻译模型学习到形态学的内容是什么？](https://doi.org/10.18653/v1/P17-1080)
    在 *第55届计算语言学协会年会* 中，ACL ’17，页码 861–872，温哥华，加拿大。计算语言学协会。
- en: Belinkov et al. (2020) Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan
    Sajjad, and James Glass. 2020. On the linguistic representational power of neural
    machine translation models. *Computational Linguistics*, 45(1):1–57.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belinkov 等人 (2020) Yonatan Belinkov、Nadir Durrani、Fahim Dalvi、Hassan Sajjad
    和 James Glass。2020年。神经机器翻译模型的语言表示能力。*计算语言学*，45(1):1–57。
- en: 'Belinkov et al. (2017b) Yonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir
    Durrani, Fahim Dalvi, and James Glass. 2017b. [Evaluating layers of representation
    in neural machine translation on part-of-speech and semantic tagging tasks](https://www.aclweb.org/anthology/I17-1001).
    In *Proceedings of the Eighth International Joint Conference on Natural Language
    Processing (Volume 1: Long Papers)*, pages 1–10, Taipei, Taiwan. Asian Federation
    of Natural Language Processing.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belinkov 等人 (2017b) Yonatan Belinkov、Lluís Màrquez、Hassan Sajjad、Nadir Durrani、Fahim
    Dalvi 和 James Glass。2017b。[在神经机器翻译中评估表示层的部分语法和语义标记任务](https://www.aclweb.org/anthology/I17-1001)。在
    *第八届国际自然语言处理联合会议（第1卷：长篇论文）* 中，页码 1–10，台北，台湾。亚洲自然语言处理联合会。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
    *CoRR*, abs/2005.14165.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared
    Kaplan、Prafulla Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda
    Askell、Sandhini Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon
    Child、Aditya Ramesh、Daniel M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark
    Chen、Eric Sigler、Mateusz Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher
    Berner、Sam McCandlish、Alec Radford、Ilya Sutskever 和 Dario Amodei。2020年。[语言模型是少样本学习者](http://arxiv.org/abs/2005.14165)。*CoRR*，abs/2005.14165。
- en: Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
    and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning
    at scale. In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8440–8451\. Association for Computational Linguistics.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau 等人 (2020) Alexis Conneau、Kartikay Khandelwal、Naman Goyal、Vishrav Chaudhary、Guillaume
    Wenzek、Francisco Guzmán、Edouard Grave、Myle Ott、Luke Zettlemoyer 和 Veselin Stoyanov。2020年。大规模无监督跨语言表示学习。发表于
    *第58届计算语言学协会年会论文集*，页面 8440–8451。计算语言学协会。
- en: 'Conneau et al. (2018) Alexis Conneau, German Kruszewski, Guillaume Lample,
    Loïc Barrault, and Marco Baroni. 2018. [What you can cram into a single $&!#*
    vector: Probing sentence embeddings for linguistic properties](https://doi.org/10.18653/v1/P18-1198).
    In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics*, ACL ’18, pages 2126–2136, Melbourne, Australia. Association for
    Computational Linguistics.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau 等人 (2018) Alexis Conneau、German Kruszewski、Guillaume Lample、Loïc Barrault
    和 Marco Baroni。2018年。 [你可以塞入单一 $&!#* 向量中的内容：探测句子嵌入的语言属性](https://doi.org/10.18653/v1/P18-1198)。发表于
    *第56届计算语言学协会年会论文集*，ACL ’18，页面 2126–2136，澳大利亚墨尔本。计算语言学协会。
- en: 'Dalvi et al. (2023a) Fahim Dalvi, Nadir Durrani, and Hassan Sajjad. 2023a.
    Neurox library for neuron analysis of deep nlp models. In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics: System Demonstrations*,
    pages 75–83, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalvi 等人 (2023a) Fahim Dalvi、Nadir Durrani 和 Hassan Sajjad。2023a年。用于深度 NLP 模型神经元分析的
    Neurox 库。发表于 *第61届计算语言学协会年会：系统演示*，页面 75–83，加拿大多伦多。计算语言学协会。
- en: Dalvi et al. (2019) Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov,
    D. Anthony Bau, and James Glass. 2019. What is one grain of sand in the desert?
    analyzing individual neurons in deep nlp models. In *Proceedings of the Thirty-Third
    AAAI Conference on Artificial Intelligence (AAAI, Oral presentation)*.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalvi 等人 (2019) Fahim Dalvi、Nadir Durrani、Hassan Sajjad、Yonatan Belinkov、D.
    Anthony Bau 和 James Glass。2019年。沙漠中的一粒沙子是什么？分析深度 NLP 模型中的单个神经元。发表于 *第三十三届 AAAI
    人工智能会议论文集 (AAAI，口头报告)*。
- en: 'Dalvi et al. (2023b) Fahim Dalvi, Maram Hasanain, Sabri Boughorbel, Basel Mousi,
    Samir Abdaljalil, Nizi Nazar, Ahmed Abdelali, Shammur Absar Chowdhury, Hamdy Mubarak,
    Ahmed Ali, Majd Hawasly, Nadir Durrani, and Firoj Alam. 2023b. [Llmebench: A flexible
    framework for accelerating llms benchmarking](http://arxiv.org/abs/2308.04945).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalvi 等人 (2023b) Fahim Dalvi、Maram Hasanain、Sabri Boughorbel、Basel Mousi、Samir
    Abdaljalil、Nizi Nazar、Ahmed Abdelali、Shammur Absar Chowdhury、Hamdy Mubarak、Ahmed
    Ali、Majd Hawasly、Nadir Durrani 和 Firoj Alam。2023b年。 [Llmebench：加速 LLMs 基准测试的灵活框架](http://arxiv.org/abs/2308.04945)。
- en: Dalvi et al. (2022) Fahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir Durrani,
    Jia Xu, and Hassan Sajjad. 2022. [Discovering latent concepts learned in BERT](https://openreview.net/forum?id=POTMtpYI1xH).
    In *International Conference on Learning Representations*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalvi 等人 (2022) Fahim Dalvi、Abdul Rafae Khan、Firoj Alam、Nadir Durrani、Jia Xu
    和 Hassan Sajjad。2022年。 [发现 BERT 中学习的潜在概念](https://openreview.net/forum?id=POTMtpYI1xH)。发表于
    *国际学习表征会议*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*,
    NAACL-HLT ’19, pages 4171–4186, Minneapolis, Minnesota, USA. Association for Computational
    Linguistics.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人 (2019) Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019年。BERT：用于语言理解的深度双向变换器的预训练。发表于
    *2019年北美计算语言学协会：人类语言技术会议论文集*，NAACL-HLT ’19，页面 4171–4186，美国明尼苏达州明尼阿波利斯。计算语言学协会。
- en: Ding et al. (2022) Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq
    Joty, and Boyang Li. 2022. [Is gpt-3 a good data annotator?](http://arxiv.org/abs/2212.10450)
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人 (2022) Bosheng Ding、Chengwei Qin、Linlin Liu、Lidong Bing、Shafiq Joty
    和 Boyang Li。2022年。 [GPT-3 是一个好的数据标注员吗？](http://arxiv.org/abs/2212.10450)
- en: 'Durrani et al. (2022) Nadir Durrani, Fahim Dalvi, and Hassan Sajjad. 2022.
    [Linguistic correlation analysis: Discovering salient neurons in deepnlp models](http://arxiv.org/abs/2206.13288).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durrani 等人 (2022) Nadir Durrani、Fahim Dalvi 和 Hassan Sajjad。2022年。 [语言相关性分析：发现深度
    NLP 模型中的显著神经元](http://arxiv.org/abs/2206.13288)。
- en: 'Durrani et al. (2021) Nadir Durrani, Hassan Sajjad, and Fahim Dalvi. 2021.
    How transfer learning impacts linguistic knowledge in deep nlp models? In *Findings
    of the Association for Computational Linguistics: ACL 2021*, Online. Association
    for Computational Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durrani 等 (2021) Nadir Durrani, Hassan Sajjad 和 Fahim Dalvi. 2021. 转移学习对深度 NLP
    模型中的语言知识的影响？发表于 *计算语言学协会：ACL 2021 发现*，在线。计算语言学协会。
- en: Fleiss et al. (2013) Joseph L Fleiss, Bruce Levin, and Myunghee Cho Paik. 2013.
    *Statistical methods for rates and proportions*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleiss 等 (2013) Joseph L Fleiss, Bruce Levin 和 Myunghee Cho Paik. 2013. *比率和比例的统计方法*。
- en: Fu and Lapata (2022) Yao Fu and Mirella Lapata. 2022. [Latent topology induction
    for understanding contextualized representations](https://doi.org/10.48550/ARXIV.2206.01512).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 和 Lapata (2022) Yao Fu 和 Mirella Lapata. 2022. [潜在拓扑推导以理解上下文化表示](https://doi.org/10.48550/ARXIV.2206.01512)。
- en: Geva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    2021. [Transformer feed-forward layers are key-value memories](https://doi.org/10.18653/v1/2021.emnlp-main.446).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 5484–5495, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva 等 (2021) Mor Geva, Roei Schuster, Jonathan Berant 和 Omer Levy. 2021. [Transformer
    前馈层是关键-值记忆](https://doi.org/10.18653/v1/2021.emnlp-main.446)。发表于 *2021年自然语言处理实证方法会议论文集*，第5484–5495页，在线及多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Gilardi et al. (2023) Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023.
    [Chatgpt outperforms crowd-workers for text-annotation tasks](http://arxiv.org/abs/2303.15056).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilardi 等 (2023) Fabrizio Gilardi, Meysam Alizadeh 和 Maël Kubli. 2023. [ChatGPT
    在文本注释任务中优于众包工人](http://arxiv.org/abs/2303.15056)。
- en: Gowda and Krishna (1978) K Chidananda Gowda and G Krishna. 1978. Agglomerative
    clustering using the concept of mutual nearest neighbourhood. *Pattern recognition*,
    10(2):105–112.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gowda 和 Krishna (1978) K Chidananda Gowda 和 G Krishna. 1978. 使用互近邻概念的聚合聚类。*模式识别*，10(2):105–112。
- en: Guo et al. (2023) Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie,
    Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. [How close is chatgpt to human
    experts? comparison corpus, evaluation, and detection](http://arxiv.org/abs/2301.07597).
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2023) Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan
    Ding, Jianwei Yue 和 Yupeng Wu. 2023. [ChatGPT 与人类专家的接近程度如何？比较语料库、评估和检测](http://arxiv.org/abs/2301.07597)。
- en: Hennigen et al. (2020) Lucas Torroba Hennigen, Adina Williams, and Ryan Cotterell.
    2020. [Intrinsic probing through dimension selection](https://doi.org/10.18653/v1/2020.emnlp-main.15).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 197–216, Online. Association for Computational Linguistics.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hennigen 等 (2020) Lucas Torroba Hennigen, Adina Williams 和 Ryan Cotterell. 2020.
    [通过维度选择的内在探测](https://doi.org/10.18653/v1/2020.emnlp-main.15)。发表于 *2020年自然语言处理实证方法会议论文集
    (EMNLP)*，第197–216页，在线。计算语言学协会。
- en: Hewitt and Liang (2019) John Hewitt and Percy Liang. 2019. [Designing and interpreting
    probes with control tasks](https://doi.org/10.18653/v1/D19-1275). In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 2733–2743, Hong Kong, China.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hewitt 和 Liang (2019) John Hewitt 和 Percy Liang. 2019. [设计和解释带控制任务的探测器](https://doi.org/10.18653/v1/D19-1275)。发表于
    *2019年自然语言处理实证方法会议与第九届国际联合自然语言处理会议 (EMNLP-IJCNLP) 论文集*，第2733–2743页，中国香港。
- en: 'Jeblick et al. (2022) Katharina Jeblick, Balthasar Schachtner, Jakob Dexl,
    Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp
    Wesp, Bastian Sabel, Jens Ricke, and Michael Ingrisch. 2022. [Chatgpt makes medicine
    easy to swallow: An exploratory case study on simplified radiology reports](http://arxiv.org/abs/2212.14882).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeblick 等 (2022) Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas
    Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp Wesp,
    Bastian Sabel, Jens Ricke 和 Michael Ingrisch. 2022. [ChatGPT 使医学更易于理解：关于简化放射学报告的探索性案例研究](http://arxiv.org/abs/2212.14882)。
- en: Kádár et al. (2017) Akos Kádár, Grzegorz Chrupała, and Afra Alishahi. 2017.
    Representation of linguistic form and function in recurrent neural networks. *Computational
    Linguistics*, 43(4):761–780.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kádár 等 (2017) Akos Kádár, Grzegorz Chrupała 和 Afra Alishahi. 2017. 递归神经网络中语言形式和功能的表征。*计算语言学*，43(4):761–780。
- en: Karpathy et al. (2015) Andrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.
    Visualizing and understanding recurrent networks. *arXiv preprint arXiv:1506.02078*.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy 等 (2015) Andrej Karpathy, Justin Johnson 和 Li Fei-Fei. 2015. 视觉化和理解递归网络。*arXiv
    预印本 arXiv:1506.02078*。
- en: 'Kingma and Ba (2014) Diederik Kingma and Jimmy Ba. 2014. Adam: A Method for
    Stochastic Optimization. *arXiv preprint arXiv:1412.6980*.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma and Ba (2014) Diederik Kingma and Jimmy Ba. 2014. Adam: A Method for
    Stochastic Optimization. *arXiv preprint arXiv:1412.6980*.'
- en: Kojima et al. (2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2023. [Large language models are zero-shot reasoners](http://arxiv.org/abs/2205.11916).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2023. [Large language models are zero-shot reasoners](http://arxiv.org/abs/2205.11916).
- en: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2019. [ALBERT: a lite BERT for self-supervised
    learning of language representations](http://arxiv.org/abs/1909.11942). *ArXiv:1909.11942*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2019. [ALBERT: a lite BERT for self-supervised
    learning of language representations](http://arxiv.org/abs/1909.11942). *ArXiv:1909.11942*.'
- en: Landis and Koch (1977) J Richard Landis and Gary G Koch. 1977. The measurement
    of observer agreement for categorical data. *biometrics*, pages 159–174.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landis and Koch (1977) J Richard Landis and Gary G Koch. 1977. The measurement
    of observer agreement for categorical data. *biometrics*, pages 159–174.
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. [The
    power of scale for parameter-efficient prompt tuning](https://doi.org/10.18653/v1/2021.emnlp-main.243).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. [The
    power of scale for parameter-efficient prompt tuning](https://doi.org/10.18653/v1/2021.emnlp-main.243).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
- en: Linzen et al. (2016) Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing
    the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. *Transactions of
    the Association for Computational Linguistics*, 4:521–535.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linzen et al. (2016) Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing
    the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. *Transactions of
    the Association for Computational Linguistics*, 4:521–535.
- en: 'Liu et al. (2019a) Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E.
    Peters, and Noah A. Smith. 2019a. [Linguistic knowledge and transferability of
    contextual representations](https://www.aclweb.org/anthology/N19-1112). In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, NAACL ’19, pages 1073–1094, Minneapolis,
    Minnesota, USA. Association for Computational Linguistics.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2019a) Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E.
    Peters, and Noah A. Smith. 2019a. [Linguistic knowledge and transferability of
    contextual representations](https://www.aclweb.org/anthology/N19-1112). In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, NAACL ’19, pages 1073–1094, Minneapolis,
    Minnesota, USA. Association for Computational Linguistics.'
- en: 'Liu et al. (2021) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2021. [Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing](http://arxiv.org/abs/2107.13586).
    *CoRR*, abs/2107.13586.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2021) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2021. [Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing](http://arxiv.org/abs/2107.13586).
    *CoRR*, abs/2107.13586.'
- en: 'Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
    [RoBERTa: A robustly optimized BERT pretraining approach](https://arxiv.org/abs/1907.11692).
    *ArXiv:1907.11692*.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
    [RoBERTa: A robustly optimized BERT pretraining approach](https://arxiv.org/abs/1907.11692).
    *ArXiv:1907.11692*.'
- en: Meng et al. (2023) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
    2023. [Locating and editing factual associations in gpt](http://arxiv.org/abs/2202.05262).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng et al. (2023) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
    2023. [Locating and editing factual associations in gpt](http://arxiv.org/abs/2202.05262).
- en: 'Michael et al. (2020) Julian Michael, Jan A. Botha, and Ian Tenney. 2020. [Asking
    without telling: Exploring latent ontologies in contextual representations](https://doi.org/10.18653/v1/2020.emnlp-main.552).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing*, EMNLP ’20, pages 6792–6812, Online. Association for Computational
    Linguistics.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Michael et al. (2020) Julian Michael, Jan A. Botha, and Ian Tenney. 2020. [Asking
    without telling: Exploring latent ontologies in contextual representations](https://doi.org/10.18653/v1/2020.emnlp-main.552).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing*, EMNLP ’20, pages 6792–6812, Online. Association for Computational
    Linguistics.'
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. Efficient estimation of word representations in vector space. In *Proceedings
    of the ICLR Workshop*, Scottsdale, AZ, USA.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. 在向量空间中高效估计词表示。在*ICLR研讨会论文集*中，亚利桑那州斯科茨代尔，美国。
- en: Qian et al. (2016) Peng Qian, Xipeng Qiu, and Xuanjing Huang. 2016. [Investigating
    Language Universal and Specific Properties in Word Embeddings](http://www.aclweb.org/anthology/P16-1140).
    In *Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics*, ACL ’16, pages 1478–1488, Berlin, Germany. Association for Computational
    Linguistics.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian et al. (2016) Peng Qian, Xipeng Qiu, and Xuanjing Huang. 2016. [研究词嵌入中的语言普遍性和特异性属性](http://www.aclweb.org/anthology/P16-1140)。在*第54届计算语言学协会年会论文集*中，ACL
    ’16，页1478–1488，德国柏林。计算语言学协会。
- en: 'Sajjad et al. (2022a) Hassan Sajjad, Nadir Durrani, and Fahim Dalvi. 2022a.
    [Neuron-level interpretation of deep NLP models: A survey](https://doi.org/10.1162/tacl_a_00519).
    *Transactions of the Association for Computational Linguistics*, 10:1285–1303.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sajjad et al. (2022a) Hassan Sajjad, Nadir Durrani, and Fahim Dalvi. 2022a.
    [深度NLP模型的神经元级解释：综述](https://doi.org/10.1162/tacl_a_00519)。*计算语言学协会学报*，10:1285–1303。
- en: Sajjad et al. (2022b) Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam,
    Abdul Rafae Khan, and Jia Xu. 2022b. Analyzing encoded concepts in transformer
    language models. In *Proceedings of the 2022 Conference of the North American
    Chapter of the Association for Computational Linguistics*, NAACL ’22, Seattle,
    Washington, USA. Association for Computational Linguistics.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sajjad et al. (2022b) Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam,
    Abdul Rafae Khan, and Jia Xu. 2022b. 分析变换器语言模型中的编码概念。在*2022年北美计算语言学协会年会论文集*中，NAACL
    ’22，华盛顿州西雅图，美国。计算语言学协会。
- en: Tenney et al. (2019) Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. [BERT
    rediscovers the classical NLP pipeline](https://doi.org/10.18653/v1/P19-1452).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 4593–4601, Florence, Italy. Association for Computational
    Linguistics.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tenney et al. (2019) Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. [BERT
    重新发现经典NLP流程](https://doi.org/10.18653/v1/P19-1452)。在*第57届计算语言学协会年会论文集*中，页4593–4601，意大利佛罗伦萨。计算语言学协会。
- en: Vylomova et al. (2017) Ekaterina Vylomova, Trevor Cohn, Xuanli He, and Gholamreza
    Haffari. 2017. [Word representation models for morphologically rich languages
    in neural machine translation](https://doi.org/10.18653/v1/W17-4115). pages 103–108.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vylomova et al. (2017) Ekaterina Vylomova, Trevor Cohn, Xuanli He, and Gholamreza
    Haffari. 2017. [用于形态丰富语言的词表示模型在神经机器翻译中的应用](https://doi.org/10.18653/v1/W17-4115)。页103–108。
- en: Wang et al. (2021) Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael
    Zeng. 2021. [Want to reduce labeling cost? gpt-3 can help](http://arxiv.org/abs/2108.13487).
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021) Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael
    Zeng. 2021. [想要降低标注成本？gpt-3 可以帮助](http://arxiv.org/abs/2108.13487)。
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining
    for language understanding. *Advances in neural information processing systems*,
    32.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ
    R Salakhutdinov, and Quoc V Le. 2019. Xlnet：用于语言理解的广义自回归预训练。*神经信息处理系统进展*，32。
- en: Zhang et al. (2023) Bowen Zhang, Daijun Ding, and Liwen Jing. 2023. [How would
    stance detection techniques evolve after the launch of chatgpt?](http://arxiv.org/abs/2212.14548)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Bowen Zhang, Daijun Ding, and Liwen Jing. 2023. [在chatgpt发布后，立场检测技术将如何演变？](http://arxiv.org/abs/2212.14548)
- en: Appendix
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Prompts
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 提示
- en: A.1 Optimal Prompt
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 最优提示
- en: 'Initially, we used a simple prompt to ask the model to provide labels for a
    list of words keeping the system description unchanged:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们使用了一个简单的提示，要求模型为单词列表提供标签，同时保持系统描述不变：
- en: Assistant is a large language model trained by OpenAI
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Assistant 是一个由 OpenAI 训练的大型语言模型
- en: 'Prompt Body: Give the following list of words a short label: [‘‘word 1’’, ‘‘word
    2’’, ..., ‘‘word N’’]'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 提示主体：给以下单词列表一个简短标签：[‘‘word 1’’, ‘‘word 2’’, ..., ‘‘word N’’]
- en: 'The output format from the first prompt was unclear as it included illustrations,
    which was not our intention. After multiple design iterations, we developed a
    prompt that returned the labels in the desired format. In this revised prompt,
    we modified the system description as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个提示的输出格式不清晰，因为它包含了插图，这不是我们的意图。经过多次设计迭代后，我们开发了一个可以以所需格式返回标签的提示。在这个修订后的提示中，我们将系统描述修改如下：
- en: Assistant is a large language model trained by OpenAI.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Assistant 是由 OpenAI 训练的大型语言模型。
- en: 'Instructions: When asked for labels, only the labels and nothing else should
    be returned.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 指示：当被要求提供标签时，仅返回标签而不添加其他内容。
- en: 'We also modified the prompt body to:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还修改了提示内容为：
- en: 'Give a short and concise label that best describes the following list of words:
    [‘‘word 1’’, ‘‘word 2’’, ..., ‘‘word N’’]'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一个简短而精确的标签来描述以下单词列表：[‘word 1’, ‘word 2’, ..., ‘word N’]
- en: Figure [6](#A1.F6 "Figure 6 ‣ A.1 Optimal Prompt ‣ Appendix A Prompts ‣ Can
    LLMs Facilitate Interpretation of Pre-trained Language Models?") shows some sample
    concepts learned in the last layer of BERT-base-cased along with their labels.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#A1.F6 "Figure 6 ‣ A.1 Optimal Prompt ‣ Appendix A Prompts ‣ Can LLMs
    Facilitate Interpretation of Pre-trained Language Models?") 显示了一些在 BERT-base-cased
    最后一层学习到的样本概念及其标签。
- en: '![Refer to caption](img/de81423447673da21d0076a0f16a09d9.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/de81423447673da21d0076a0f16a09d9.png)'
- en: (a) Geographic Locations in the US
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 美国地理位置
- en: '![Refer to caption](img/bcf293a76e02e1ebd1e9ec9d9e0a5bc8.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bcf293a76e02e1ebd1e9ec9d9e0a5bc8.png)'
- en: (b) Middle East Conflict
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 中东冲突
- en: '![Refer to caption](img/76e298077d4b7620ea47e1aa6ad77814.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76e298077d4b7620ea47e1aa6ad77814.png)'
- en: (c) LGBTQ+
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: (c) LGBTQ+
- en: '![Refer to caption](img/9c7864361c26febfa0c261e7542f0c38.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9c7864361c26febfa0c261e7542f0c38.png)'
- en: (d) Female Names and titles
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 女性名字和头衔
- en: '![Refer to caption](img/d3d714a3b83ca93a49dbdd2440f8227e.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d3d714a3b83ca93a49dbdd2440f8227e.png)'
- en: (e) Spanish Male Names
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 西班牙男性名字
- en: '![Refer to caption](img/bdf16269bd981e057e590ae0579d0868.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bdf16269bd981e057e590ae0579d0868.png)'
- en: (f) Gender related nouns and pronouns
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 性别相关名词和代词
- en: '![Refer to caption](img/68b836b98e5e4416ba0f1e8313ebae2c.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/68b836b98e5e4416ba0f1e8313ebae2c.png)'
- en: (g) Geographic Locations in California
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 加州地理位置
- en: '![Refer to caption](img/d6257e3ebb75b8baef2044150360c075.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d6257e3ebb75b8baef2044150360c075.png)'
- en: (h) SE Asian Politics and Ethnic Conflict
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 东南亚政治与民族冲突
- en: '![Refer to caption](img/00f1cf55d4b56bded1718ca9bad778f7.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/00f1cf55d4b56bded1718ca9bad778f7.png)'
- en: (i) List of cities and universities in southeastern US
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 美国东南部城市和大学列表
- en: 'Figure 6: Sample Concepts Learned in the last layer of BERT'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: BERT 最后一层学习到的样本概念'
- en: '![Refer to caption](img/05d90564a47d55da295557e69cc12895.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/05d90564a47d55da295557e69cc12895.png)'
- en: (a)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/5930f5ab222ee0ac77e3794d73d50e1a.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5930f5ab222ee0ac77e3794d73d50e1a.png)'
- en: (b)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/4c987d6d1908b5bf7d85d150e39c1230.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4c987d6d1908b5bf7d85d150e39c1230.png)'
- en: (c)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: '![Refer to caption](img/05d90564a47d55da295557e69cc12895.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/05d90564a47d55da295557e69cc12895.png)'
- en: (d)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (d)
- en: '![Refer to caption](img/4cb4af64fb109c53d75d93883f83585a.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4cb4af64fb109c53d75d93883f83585a.png)'
- en: (e)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: (e)
- en: '![Refer to caption](img/dae2ce1bc7bee24e902341f88d4eb5f5.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dae2ce1bc7bee24e902341f88d4eb5f5.png)'
- en: (f)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: (f)
- en: 'Figure 7: Failed Requests in Albert'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: Albert 中的失败请求'
- en: '| Tag | Human Label | 1 Label Response | 3-Keyword Response |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 人工标签 | 1 标签响应 | 3-关键词响应 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| c533 | LEX:hyphenated | Superlative and ordinal adjectives. | [’second’,
    ’highest’, ’biggest’] |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| c533 | LEX:连字符 | 最高级和序数形容词。 | [‘第二’, ‘最高’, ‘最大’] |'
- en: '| c84 | LEX:hyphenated | Accomplishments and Awards | [’Award-winning’, ’Nominated’,
    ’Multi-time’] |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| c84 | LEX:连字符 | 成就和奖项 | [‘获奖’, ‘提名’, ‘多次’] |'
- en: '| c783 | LEX:hyphenated | Sports scores and point differentials. | [’points’,
    ’wins’, ’scores’] |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| c783 | LEX:连字符 | 体育得分和积分差异。 | [‘积分’, ‘胜利’, ‘得分’] |'
- en: '| c621 | LEX:hyphenated | Describing people’s relationships and family status.
    | [family, relationships, parenthood] |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| c621 | LEX:连字符 | 描述人际关系和家庭状况。 | [家庭, 关系, 父母身份] |'
- en: '| c869 | LEX:hyphenated | Tennis Scores. | [’Tennis’, ’Scores’, ’Games’] |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| c869 | LEX:连字符 | 网球得分。 | [‘网球’, ‘得分’, ‘比赛’] |'
- en: '| c833 | LEX:hyphenated | Location-based adjectives | [based, area, listed]
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| c833 | LEX:连字符 | 基于地点的形容词 | [基于, 区域, 列出] |'
- en: '| c588 | LEX:hyphenated | US political party affiliations by state and district.
    | [Republican, Democrat, State Abbreviations] |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| c588 | LEX:连字符 | 按州和地区的美国政治党派隶属关系。 | [共和党, 民主党, 州缩写] |'
- en: '| c639 | LEX:hyphenated | Football scores. | [’Scores’, ’Football’, ’Winning’]
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| c639 | LEX:连字符 | 足球得分。 | [‘得分’, ‘足球’, ‘获胜’] |'
- en: '| c934 | LEX:hyphenated | Sports scores. | [’scores’, ’victories’, ’defeats’]
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| c934 | LEX:连字符 | 体育得分。 | [‘得分’, ‘胜利’, ‘失败’] |'
- en: '| c850 | LEX:case:title_case | Philippine Places and Names | [’Philippines’,
    ’Tourism’, ’Volcano’] |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| c850 | LEX:标题格式 | 菲律宾地方和名字 | [‘菲律宾’, ‘旅游’, ‘火山’] |'
- en: '| c286 | LEX:case:title_case | List of surnames. | [Last names, English, List]
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| c286 | LEX:case:title_case | 姓氏列表。 | [姓氏, 英文, 列表] |'
- en: '| c982 | LEX:case:title_case | Sports-related terms. | [’Football’, ’Sports’,
    ’Legends’] |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| c982 | LEX:case:title_case | 体育相关术语。 | [‘足球’, ‘体育’, ‘传奇’] |'
- en: '| c231 | SYN:position:first_word | Sports Terminology | [’Footballers’, ’Tries’,
    ’Substitutes’] |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| c231 | SYN:position:first_word | 体育术语 | [‘足球运动员’, ‘尝试’, ‘替补’] |'
- en: '| c784 | SYN:position:first_word | Numerical data. | [Numbers, Decimals, List]
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| c784 | SYN:position:first_word | 数值数据。 | [数字, 小数, 列表] |'
- en: '| c728 | SYN:position:first_word | Action-oriented verbs and adjectives. |
    [Improving, Ensuring, Capturing] |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| c728 | SYN:position:first_word | 面向动作的动词和形容词。 | [提高, 确保, 捕捉] |'
- en: '| c672 | SYN:position:first_word | Verbs describing actions and states. | [Fluent,
    Struggling, Showcasing] |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| c672 | SYN:position:first_word | 描述动作和状态的动词。 | [流畅,  struggling, 展示] |'
- en: '| c886 | LEX:case:title_case | Describing communication actions. | [Referring,
    Recalling, Revealing] |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| c886 | LEX:case:title_case | 描述沟通行动。 | [提及, 回忆, 揭示] |'
- en: '| c865 | LEX:case:title_case | Baseball player names. | [Bregman, Scherzer,
    Puig] |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| c865 | LEX:case:title_case | 棒球运动员名字。 | [布雷格曼, 谢尔泽, 普伊格] |'
- en: '| c734 | LEX:case:title_case | Island names. | [’Islands’, ’Caribbean’, ’Indian
    Ocean’] |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| c734 | LEX:case:title_case | 岛屿名称。 | [‘岛屿’, ‘加勒比海’, ‘印度洋’] |'
- en: '| c818 | LEX:case:title_case | Ethnicities and Cities in the Balkans | [’Bosnian’,
    ’Albanian’, ’Yugoslavia’] |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| c818 | LEX:case:title_case | 巴尔干地区的民族和城市 | [‘波斯尼亚’, ‘阿尔巴尼亚’, ‘南斯拉夫’] |'
- en: 'Table 5: Prompting ChatGPT to label a concept with keywords instead of one
    label'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：提示 ChatGPT 使用关键词标记概念而不是单一标签
- en: A.2 Prompts For Lexical Concepts
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 词汇概念提示
- en: During the error analysis (Section [4.2](#S4.SS2 "4.2 Error Analysis ‣ 4 Evaluation
    and Analysis ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")),
    we discovered that GPT struggled to accurately label concepts composed of words
    sharing a lexical property, such as a common suffix. However, we were able to
    devise a solution to address this issue by curating the prompt to effectively
    label such concepts. We modified the prompt to identify concepts that contain
    common n-grams.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在错误分析（第 [4.2](#S4.SS2 "4.2 错误分析 ‣ 4 评估与分析 ‣ LLM 是否能促进预训练语言模型的解释？") 节）中，我们发现
    GPT 难以准确标记由共享词汇属性（如共同后缀）的单词组成的概念。然而，我们能够通过调整提示来解决此问题，从而有效标记这些概念。我们修改了提示以识别包含共同
    n-grams 的概念。
- en: Give a short and concise label describing the common ngrams between the words
    of the given list
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一个简短而清晰的标签，描述给定列表中的单词之间的常见 n-gram
- en: 'Note: Only one common ngram should be returned. If there is no common ngram
    reply with ‘NA’'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注：应仅返回一个共同的 n-gram。如果没有共同的 n-gram，请回复‘NA’
- en: Using this improved we were able to correct 100% of the labeling errors in the
    concepts having lexical coherence. See Figure [8](#A1.F8 "Figure 8 ‣ A.3 Prompts
    for POS Concepts ‣ Appendix A Prompts ‣ Can LLMs Facilitate Interpretation of
    Pre-trained Language Models?")a for example. With the default prompt it was labelled
    as Superlative and ordinal adjectives and with the modified prompt, it was labeled
    as Hyphenated, cased & -based suffix.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使用改进后的提示，我们能够纠正 100% 词汇一致性的概念标记错误。请参见图 [8](#A1.F8 "图 8 ‣ A.3 词性概念提示 ‣ 附录 A 提示
    ‣ LLM 是否能促进预训练语言模型的解释？")a 作为示例。使用默认提示，它被标记为最高级和序数形容词，而使用修改后的提示，它被标记为连字符、大小写 &
    基于后缀。
- en: A.3 Prompts for POS Concepts
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 词性概念提示
- en: 'Similarly we were able to modify the prompt to correctly label concepts that
    were made from words having common parts-of-speech. From the prompts we tested,
    the best performing one is below:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们能够修改提示以正确标记由具有共同词性部分的单词组成的概念。从我们测试的提示中，表现最佳的是以下提示：
- en: Give a short and concise label describing the common part of speech tag between
    the words of the given list
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一个简短而清晰的标签，描述给定列表中单词之间的共同词性标签
- en: 'Note: The part of speech tag should be chosen from the Penn Treebank. If there’s
    no common part of speech tag reply with ‘NA’'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 注：词性标签应从 Penn Treebank 中选择。如果没有常见的词性标签，请回复‘NA’
- en: 'In Figure [8](#A1.F8 "Figure 8 ‣ A.3 Prompts for POS Concepts ‣ Appendix A
    Prompts ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")b,
    we present an example of a concept labeled as Surnames with ‘Mc’ prefix. However,
    it is important to note that not all the names in this concept actually begin
    with the “Mc” prefix. The appropriate label for this concept would be NNP: Proper
    Nouns or SEM: Irish Names. With the POS-based prompt, we are able to achieve the
    former.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [8](#A1.F8 "图 8 ‣ A.3 词性概念提示 ‣ 附录 A 提示 ‣ LLMs 能否促进预训练语言模型的解释？")b 中，我们展示了一个标记为姓氏且以“Mc”前缀开头的概念的示例。然而，需要注意的是，该概念中的所有名字实际上并不是以“Mc”前缀开头。此概念的适当标签应为
    NNP: 专有名词或 SEM: 爱尔兰名字。通过基于词性的提示，我们能够实现前者。'
- en: '![Refer to caption](img/a864683d5c02f87042dc868ca20b70d4.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a864683d5c02f87042dc868ca20b70d4.png)'
- en: (a) Hyphenated,cased & -based suffix
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 带连字符、大小写和 -based 后缀
- en: '![Refer to caption](img/72c33cd6eb157b27b1bd594aca632e83.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/72c33cd6eb157b27b1bd594aca632e83.png)'
- en: '(b) NNP: Proper Nouns'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '(b) NNP: 专有名词'
- en: 'Figure 8: Illustrating lexical and POS concepts: (a) A concept that exhibits
    multiple lexical properties, such as being hyphenated and cased. ChatGPT assigns
    a label based on the shared "-based" ngram found among most words in the cluster.
    (b) ChatGPT labeled this concept as NNP (proper noun)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 说明词汇和词性概念: (a) 一个展示多重词汇属性的概念，如带有连字符和大小写。ChatGPT 基于大多数单词在集群中共享的“-based”
    ngram 分配标签。 (b) ChatGPT 将此概念标记为 NNP（专有名词）'
- en: A.4 Providing Context
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 提供上下文
- en: Our analysis revealed that including contextual information is crucial for accurately
    labeling concepts in certain cases. As shown in Figure [9](#A1.F9 "Figure 9 ‣
    A.4 Providing Context ‣ Appendix A Prompts ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?"), concepts were incorrectly labeled as Numerical
    Data despite representing different entities. Incorporating context enables us
    to obtain more specific labels. However, we face limitations in the number of
    input tokens we can provide to the model, which impacts the quality of the labels.
    Using context of 10 sentences we were able to correct 9 of the 38 erroneous labels.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析表明，包含上下文信息对准确标记某些概念至关重要。如图 [9](#A1.F9 "图 9 ‣ A.4 提供上下文 ‣ 附录 A 提示 ‣ LLMs
    能否促进预训练语言模型的解释？") 所示，尽管这些概念代表不同的实体，但却被错误地标记为数值数据。融入上下文使我们能够获得更具体的标签。然而，我们面临着可以提供给模型的输入标记数量的限制，这影响了标签的质量。使用
    10 个句子的上下文，我们能够纠正 38 个错误标签中的 9 个。
- en: '![Refer to caption](img/e66e1e00b1c4eb80626e33ea6e9575e6.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e66e1e00b1c4eb80626e33ea6e9575e6.png)'
- en: (a)
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/a4e0d19a27a8d73740a1b2c5409f1aa6.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a4e0d19a27a8d73740a1b2c5409f1aa6.png)'
- en: (b)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/63b75ff82b27819a26839f0d0d202c9d.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/63b75ff82b27819a26839f0d0d202c9d.png)'
- en: (c)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 9: Highlighting the Significance of Context: (a) Money Figures (b) Percentages
    (c) Baseball Scores. All of these concepts were mislabeled as Numerical values
    by ChatGPT. Providing the context sentences we are able to obtain the correct
    label'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 强调上下文的重要性: (a) 金额 (b) 百分比 (c) 棒球比分。这些概念都被 ChatGPT 错误地标记为数值。提供上下文句子后，我们能够获得正确的标签。'
- en: A.5 Other Details
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 其他细节
- en: Tokens Versus Types
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词汇与类型
- en: We observed that the quality of labels is influenced by the word frequency in
    the given list. Using tokens instead of types leads to more meaningful labels.
    However, when the latent concept includes hate speech words, passing a token list
    results in failed requests due to content policy violations. In such cases, we
    opted to pass the list of types instead. Although this mitigates the issue to
    a certain extent, it does not completely resolve it. Refer to Figure [7](#A1.F7
    "Figure 7 ‣ A.1 Optimal Prompt ‣ Appendix A Prompts ‣ Can LLMs Facilitate Interpretation
    of Pre-trained Language Models?") for examples of failed requests with Albert.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，标签的质量受所给列表中单词频率的影响。使用词标而非类型会导致更有意义的标签。然而，当潜在概念包含仇恨言论词汇时，传递标记列表会因内容政策违反而导致请求失败。在这种情况下，我们选择传递类型列表。尽管这在一定程度上缓解了问题，但并未完全解决。有关与
    Albert 的请求失败的示例，请参见图 [7](#A1.F7 "图 7 ‣ A.1 最佳提示 ‣ 附录 A 提示 ‣ LLMs 能否促进预训练语言模型的解释？")。
- en: Keyword prompts
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键词提示
- en: We also explored prompts to return 3 keywords that describe the concept instead
    of returning a concise label in an effort to produce multiple labels like BCN.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探索了返回描述概念的 3 个关键词的提示，而不是返回简洁标签，以努力生成类似 BCN 的多个标签。
- en: 'Instructions:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '指示:'
- en: When asked for keywords, only the keywords and nothing else should be returned.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在要求关键词时，仅应返回关键词，不要其他内容。
- en: If asked for 3 keywords, the keywords should be returned in the form of [keyword_1,
    keyword_2, keyword_3]
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要求提供3个关键词，应以[keyword_1, keyword_2, keyword_3]的形式返回关键词
- en: 'To ensure compliance with our desired output format, we introduced a second
    instruction since the model was not following the first instruction as intended.
    We also modified the prompt body to:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保符合我们期望的输出格式，我们引入了第二个指令，因为模型没有按照第一个指令进行操作。我们还修改了提示内容为：
- en: Give 3 keywords that best describe the following list of words
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 给出3个最佳描述以下词汇列表的关键词
- en: Unfortunately, this prompt did not provide accurate labels, as illustrated in
    Table [5](#A1.T5 "Table 5 ‣ A.1 Optimal Prompt ‣ Appendix A Prompts ‣ Can LLMs
    Facilitate Interpretation of Pre-trained Language Models?").
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个提示没有提供准确的标签，如表[5](#A1.T5 "Table 5 ‣ A.1 Optimal Prompt ‣ Appendix A Prompts
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?")所示。
- en: '| tag | Label | BERT | Sel | ALBERT | Sel | XLNet | Sel | XLM-R | Sel | RoBERTa
    | Sel |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 标签 | BERT | 选择性 | ALBERT | 选择性 | XLNet | 选择性 | XLM-R | 选择性 | RoBERTa
    | 选择性 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| c301 | Gender-related Nouns and pronouns | 0.98 | 0.16 | 0.95 | 0.14 | 0.86
    | 0.24 | 0.94 | 0.23 | 0.95 | 0.26 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| c301 | 与性别相关的名词和代词 | 0.98 | 0.16 | 0.95 | 0.14 | 0.86 | 0.24 | 0.94 | 0.23
    | 0.95 | 0.26 |'
- en: '| c533 | LGBTQ+ | 1 | 0.18 | 0.97 | 0.33 | 0.97 | 0.43 | 1 | 0.25 | 1 | 0.14
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| c533 | LGBTQ+ | 1 | 0.18 | 0.97 | 0.33 | 0.97 | 0.43 | 1 | 0.25 | 1 | 0.14
    |'
- en: '| c439 | Sports commentary terms | 0.94 | 0.2 | 0.91 | 0.18 | 0.81 | 0.05 |
    0.87 | 0.11 | 0.86 | 0.09 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| c439 | 体育评论术语 | 0.94 | 0.2 | 0.91 | 0.18 | 0.81 | 0.05 | 0.87 | 0.11 | 0.86
    | 0.09 |'
- en: '| c173 | Football team names and stadiums | 0.94 | 0.2 | 0.96 | 0.27 | 0.94
    | 0.24 | 0.95 | 0.2 | 0.97 | 0.34 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| c173 | 足球队名字和体育场 | 0.94 | 0.2 | 0.96 | 0.27 | 0.94 | 0.24 | 0.95 | 0.2 |
    0.97 | 0.34 |'
- en: '| c348 | Female names and titles | 0.98 | 0.29 | 0.98 | 0.29 | 0.94 | 0.21
    | 0.96 | 0.16 | 0.97 | 0.24 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| c348 | 女性名字和称谓 | 0.98 | 0.29 | 0.98 | 0.29 | 0.94 | 0.21 | 0.96 | 0.16 |
    0.97 | 0.24 |'
- en: '| c149 | Tennis players’ names | 0.98 | 0.27 | 0.95 | 0.25 | 0.92 | 0.19 |
    0.92 | 0.17 | 0.92 | 0.19 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| c149 | 网球运动员的名字 | 0.98 | 0.27 | 0.95 | 0.25 | 0.92 | 0.19 | 0.92 | 0.17 |
    0.92 | 0.19 |'
- en: '| c487 | Spanish Male Names | 0.95 | 0.26 | 0.96 | 0.07 | 0.94 | 0.37 | 0.91
    | 0.25 | 0.98 | 0.28 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| c487 | 西班牙男性名字 | 0.95 | 0.26 | 0.96 | 0.07 | 0.94 | 0.37 | 0.91 | 0.25 |
    0.98 | 0.28 |'
- en: '| c564 | Cities and Universities in southeastern US | 0.97 | 0.12 | 0.97 |
    0.11 | 0.9 | 0.18 | 0.97 | 0.29 | 0.96 | 0.22 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| c564 | 美国东南部的城市和大学 | 0.97 | 0.12 | 0.97 | 0.11 | 0.9 | 0.18 | 0.97 | 0.29
    | 0.96 | 0.22 |'
- en: '| c263 | Locations in New York City | 0.95 | 0.25 | 0.95 | 0.22 | 0.92 | 0.26
    | 0.95 | 0.26 | 0.95 | 0.17 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| c263 | 纽约市的地点 | 0.95 | 0.25 | 0.95 | 0.22 | 0.92 | 0.26 | 0.95 | 0.26 | 0.95
    | 0.17 |'
- en: '| c247 | Scandinavian/Nordic names and places | 0.97 | 0.22 | 0.98 | 0.27 |
    0.95 | 0.29 | 0.96 | 0.21 | 0.98 | 0.29 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| c247 | 斯堪的纳维亚/北欧名字和地方 | 0.97 | 0.22 | 0.98 | 0.27 | 0.95 | 0.29 | 0.96 |
    0.21 | 0.98 | 0.29 |'
- en: '| c438 | Verbs for various actions and outcomes | 0.97 | 0.12 | 0.94 | 0.09
    | 0.87 | 0.23 | 0.92 | 0.11 | 0.92 | 0.14 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| c438 | 各种动作和结果的动词 | 0.97 | 0.12 | 0.94 | 0.09 | 0.87 | 0.23 | 0.92 | 0.11
    | 0.92 | 0.14 |'
- en: '| c44 | Southeast Asian Politics and Ethnic Conflict | 0.97 | 0.17 | 0.97 |
    0.19 | 0.94 | 0.25 | 0.93 | 0.09 | 0.95 | 0.16 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| c44 | 东南亚政治和民族冲突 | 0.97 | 0.17 | 0.97 | 0.19 | 0.94 | 0.25 | 0.93 | 0.09
    | 0.95 | 0.16 |'
- en: '| c421 | Names of people and places in the middle east | 0.97 | 0.06 | 0.94
    | 0.28 | 0.95 | 0.22 | 0.93 | 0.31 | 0.92 | 0.12 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| c421 | 中东地区的人名和地名 | 0.97 | 0.06 | 0.94 | 0.28 | 0.95 | 0.22 | 0.93 | 0.31
    | 0.92 | 0.12 |'
- en: '| c245 | Middle East conflict | 0.98 | 0.26 | 1 | 0.25 | 0.93 | 0.29 | 0.93
    | 0.25 | 0.95 | 0.22 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| c245 | 中东冲突 | 0.98 | 0.26 | 1 | 0.25 | 0.93 | 0.29 | 0.93 | 0.25 | 0.95 |
    0.22 |'
- en: '| c553 | Islamic terminology | 1 | 0.15 | 0.96 | 0.4 | 0.89 | 0.29 | 0.89 |
    0.16 | 0.95 | 0.26 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| c553 | 伊斯兰术语 | 1 | 0.15 | 0.96 | 0.4 | 0.89 | 0.29 | 0.89 | 0.16 | 0.95 |
    0.26 |'
- en: '| c365 | Criminal activities | 0.97 | 0.15 | 0.93 | 0.17 | 0.89 | 0.35 | 0.9
    | 0.15 | 0.93 | 0.21 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| c365 | 犯罪活动 | 0.97 | 0.15 | 0.93 | 0.17 | 0.89 | 0.35 | 0.9 | 0.15 | 0.93
    | 0.21 |'
- en: '| c128 | Medical and Healthcare terminology | 0.98 | 0.17 | 0.98 | 0.21 | 0.95
    | 0.15 | 0.94 | 0.24 | 0.95 | 0.27 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| c128 | 医疗和健康术语 | 0.98 | 0.17 | 0.98 | 0.21 | 0.95 | 0.15 | 0.94 | 0.24 |
    0.95 | 0.27 |'
- en: 'Table 6: Training Probes towards latent concepts discovered in various Models.
    Reporting classifier accuracy on test-set along with respective selectivity numbers'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：对各种模型中发现的潜在概念进行训练探测。报告分类器在测试集上的准确率及相应的选择性数字
- en: Appendix B Probing Classifiers
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 探测分类器
- en: B.1 Running Probes At Scale
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 大规模运行探测
- en: Probing For Fine-grained Semantic Concepts
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 细粒度语义概念探测
- en: We used the NeuroX toolkit to train a linear probe for several concepts chosen
    from layers 3, 9 and 12 of BERT-base-cased. We used a train/val/test splits of
    0.6, 0.2, 0.2 respectively. Tables [7](#A2.T7 "Table 7 ‣ Probing For Fine-grained
    Semantic Concepts ‣ B.1 Running Probes At Scale ‣ Appendix B Probing Classifiers
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?") and [8](#A2.T8
    "Table 8 ‣ Probing For Fine-grained Semantic Concepts ‣ B.1 Running Probes At
    Scale ‣ Appendix B Probing Classifiers ‣ Can LLMs Facilitate Interpretation of
    Pre-trained Language Models?") show the data statistics and the probe results
    respectively. Table [9](#A2.T9 "Table 9 ‣ Probing For Fine-grained Semantic Concepts
    ‣ B.1 Running Probes At Scale ‣ Appendix B Probing Classifiers ‣ Can LLMs Facilitate
    Interpretation of Pre-trained Language Models?") shows results of probes trained
    on concepts chosen from multiple layers of ALBERT. In Table [6](#A1.T6 "Table
    6 ‣ Keyword prompts ‣ A.5 Other Details ‣ Appendix A Prompts ‣ Can LLMs Facilitate
    Interpretation of Pre-trained Language Models?") we carried out a cross architectural
    comparison across the models by training probes towards the same set of concepts.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 NeuroX 工具包来训练线性探针，针对从 BERT-base-cased 的第 3、9 和 12 层选择的几个概念。我们使用了 0.6、0.2、0.2
    的训练/验证/测试分割。表格 [7](#A2.T7 "Table 7 ‣ Probing For Fine-grained Semantic Concepts
    ‣ B.1 Running Probes At Scale ‣ Appendix B Probing Classifiers ‣ Can LLMs Facilitate
    Interpretation of Pre-trained Language Models?") 和 [8](#A2.T8 "Table 8 ‣ Probing
    For Fine-grained Semantic Concepts ‣ B.1 Running Probes At Scale ‣ Appendix B
    Probing Classifiers ‣ Can LLMs Facilitate Interpretation of Pre-trained Language
    Models?") 分别展示了数据统计和探针结果。表格 [9](#A2.T9 "Table 9 ‣ Probing For Fine-grained Semantic
    Concepts ‣ B.1 Running Probes At Scale ‣ Appendix B Probing Classifiers ‣ Can
    LLMs Facilitate Interpretation of Pre-trained Language Models?") 显示了在从多个 ALBERT
    层选择的概念上训练的探针结果。在表格 [6](#A1.T6 "Table 6 ‣ Keyword prompts ‣ A.5 Other Details ‣
    Appendix A Prompts ‣ Can LLMs Facilitate Interpretation of Pre-trained Language
    Models?") 中，我们通过训练探针对相同的概念集进行了跨架构比较。
- en: '| Layer | Tag | Label | Tokens | Types | Sents | Train | Val | Test |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| Layer | Tag | Label | Tokens | Types | Sents | Train | Val | Test |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 3 | c90 | Financial terms. | 220 | 22 | 214 | 285 | 95 | 96 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c90 | 财务术语。 | 220 | 22 | 214 | 285 | 95 | 96 |'
- en: '| 3 | c336 | Photography-related terms. | 290 | 29 | 273 | 388 | 130 | 130
    |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c336 | 摄影相关术语。 | 290 | 29 | 273 | 388 | 130 | 130 |'
- en: '| 3 | c112 | Middle Eastern Conflict | 620 | 62 | 523 | 992 | 331 | 331 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c112 | 中东冲突 | 620 | 62 | 523 | 992 | 331 | 331 |'
- en: '| 3 | c506 | Diversity and Ethnicity. | 240 | 24 | 225 | 331 | 111 | 112 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c506 | 多样性与种族。 | 240 | 24 | 225 | 331 | 111 | 112 |'
- en: '| 3 | c390 | List of surnames. | 4298 | 430 | 4049 | 5530 | 1844 | 1844 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c390 | 姓氏列表。 | 4298 | 430 | 4049 | 5530 | 1844 | 1844 |'
- en: '| 3 | c331 | Emotions/Feelings. | 400 | 40 | 396 | 484 | 162 | 162 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c331 | 情感/感觉。 | 400 | 40 | 396 | 484 | 162 | 162 |'
- en: '| 3 | c592 | Animal names. | 220 | 22 | 208 | 268 | 90 | 90 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c592 | 动物名称。 | 220 | 22 | 208 | 268 | 90 | 90 |'
- en: '| 3 | c25 | Keywords related to discrimination and inequality. | 340 | 34 |
    325 | 440 | 147 | 147 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c25 | 与歧视和不平等相关的关键词。 | 340 | 34 | 325 | 440 | 147 | 147 |'
- en: '| 3 | c500 | List of female names. | 2913 | 292 | 2735 | 3867 | 1289 | 1290
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c500 | 女性名字列表。 | 2913 | 292 | 2735 | 3867 | 1289 | 1290 |'
- en: '| 3 | c414 | Healthcare | 510 | 51 | 475 | 752 | 251 | 251 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c414 | 医疗保健 | 510 | 51 | 475 | 752 | 251 | 251 |'
- en: '| 3 | c31 | List of male first names. | 1130 | 113 | 1078 | 1422 | 474 | 474
    |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c31 | 男性名字列表。 | 1130 | 113 | 1078 | 1422 | 474 | 474 |'
- en: '| 3 | c173 | Animals | 760 | 76 | 704 | 994 | 332 | 332 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c173 | 动物 | 760 | 76 | 704 | 994 | 332 | 332 |'
- en: '| 3 | c72 | Natural Disasters and Weather Events | 701 | 71 | 635 | 1022 |
    341 | 341 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c72 | 自然灾害和天气事件 | 701 | 71 | 635 | 1022 | 341 | 341 |'
- en: '| 3 | c514 | English counties | 297 | 30 | 286 | 373 | 124 | 125 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c514 | 英国郡 | 297 | 30 | 286 | 373 | 124 | 125 |'
- en: '| 3 | c178 | Body Parts | 430 | 43 | 405 | 588 | 196 | 196 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c178 | 身体部位 | 430 | 43 | 405 | 588 | 196 | 196 |'
- en: '| 3 | c340 | Media and Journalism. | 379 | 38 | 365 | 518 | 173 | 173 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c340 | 媒体与新闻。 | 379 | 38 | 365 | 518 | 173 | 173 |'
- en: '| 3 | c432 | Power and Status. | 310 | 31 | 306 | 385 | 128 | 129 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c432 | 权力与地位。 | 310 | 31 | 306 | 385 | 128 | 129 |'
- en: '| 3 | c8 | Verbs | 1028 | 103 | 1018 | 1243 | 414 | 415 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c8 | 动词 | 1028 | 103 | 1018 | 1243 | 414 | 415 |'
- en: '| 3 | c408 | -Verbs ending in "-ing" | 510 | 51 | 504 | 615 | 205 | 206 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c408 | 以“-ing”结尾的动词 | 510 | 51 | 504 | 615 | 205 | 206 |'
- en: '| 3 | c479 | City names | 130 | 13 | 127 | 159 | 53 | 54 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c479 | 城市名称 | 130 | 13 | 127 | 159 | 53 | 54 |'
- en: '| 3 | c343 | Surnames | 490 | 49 | 464 | 613 | 204 | 205 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c343 | 姓氏 | 490 | 49 | 464 | 613 | 204 | 205 |'
- en: '| 3 | c577 | Disability-related terms. | 140 | 14 | 133 | 172 | 58 | 58 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c577 | 残疾相关术语。 | 140 | 14 | 133 | 172 | 58 | 58 |'
- en: '| 9 | c26 | Negative sentiment. | 798 | 118 | 782 | 1036 | 346 | 346 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c26 | 消极情绪。 | 798 | 118 | 782 | 1036 | 346 | 346 |'
- en: '| 9 | c122 | Security Measures | 457 | 70 | 446 | 584 | 195 | 195 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c122 | 安全措施 | 457 | 70 | 446 | 584 | 195 | 195 |'
- en: '| 9 | c423 | Label: Islamic Extremism/Terrorism | 248 | 30 | 222 | 357 | 119
    | 120 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c423 | 标签：伊斯兰极端主义/恐怖主义 | 248 | 30 | 222 | 357 | 119 | 120 |'
- en: '| 9 | c299 | Middle Eastern and North African countries and cities | 531 |
    57 | 460 | 844 | 282 | 282 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c299 | 中东和北非国家及城市 | 531 | 57 | 460 | 844 | 282 | 282 |'
- en: '| 9 | c192 | Diversity and Identity | 314 | 50 | 279 | 506 | 169 | 169 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c192 | 多样性与身份 | 314 | 50 | 279 | 506 | 169 | 169 |'
- en: '| 9 | c468 | Russian male names. | 125 | 18 | 123 | 153 | 51 | 52 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c468 | 俄罗斯男性姓名。 | 125 | 18 | 123 | 153 | 51 | 52 |'
- en: '| 9 | c588 | Gender-related terms. | 161 | 19 | 146 | 236 | 79 | 79 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c588 | 性别相关术语。 | 161 | 19 | 146 | 236 | 79 | 79 |'
- en: '| 9 | c74 | Financial terms | 672 | 96 | 607 | 1118 | 373 | 373 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c74 | 财务术语 | 672 | 96 | 607 | 1118 | 373 | 373 |'
- en: '| 9 | c503 | Middle East Conflict. | 230 | 27 | 185 | 404 | 135 | 135 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c503 | 中东冲突。 | 230 | 27 | 185 | 404 | 135 | 135 |'
- en: '| 9 | c325 | Violent Crimes | 292 | 60 | 287 | 386 | 129 | 129 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c325 | 暴力犯罪 | 292 | 60 | 287 | 386 | 129 | 129 |'
- en: '| 9 | c535 | Academic Research. | 233 | 26 | 227 | 332 | 111 | 111 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c535 | 学术研究。 | 233 | 26 | 227 | 332 | 111 | 111 |'
- en: '| 9 | c256 | List of names | 1069 | 149 | 1026 | 1375 | 458 | 459 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c256 | 姓名列表 | 1069 | 149 | 1026 | 1375 | 458 | 459 |'
- en: '| 9 | c507 | Positive Adjectives | 389 | 69 | 380 | 505 | 168 | 169 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c507 | 积极形容词 | 389 | 69 | 380 | 505 | 168 | 169 |'
- en: '| 9 | c345 | List of Chinese surnames. | 407 | 65 | 378 | 567 | 189 | 190 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c345 | 中文姓氏列表。 | 407 | 65 | 378 | 567 | 189 | 190 |'
- en: '| 12 | c259 | List of names | 223 | 174 | 221 | 273 | 91 | 92 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c259 | 姓名列表 | 223 | 174 | 221 | 273 | 91 | 92 |'
- en: '| 12 | c62 | Adverbs | 1221 | 351 | 1133 | 3769 | 1256 | 1257 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c62 | 副词 | 1221 | 351 | 1133 | 3769 | 1256 | 1257 |'
- en: '| 12 | c128 | Medical and Healthcare Terminology. | 395 | 70 | 369 | 662 |
    221 | 221 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c128 | 医疗和健康术语。 | 395 | 70 | 369 | 662 | 221 | 221 |'
- en: '| 12 | c301 | Gender-related nouns and pronouns. | 418 | 74 | 377 | 883 | 294
    | 295 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c301 | 性别相关名词和代词。 | 418 | 74 | 377 | 883 | 294 | 295 |'
- en: '| 12 | c37 | List of male names. | 872 | 372 | 807 | 1460 | 487 | 487 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c37 | 男性姓名列表。 | 872 | 372 | 807 | 1460 | 487 | 487 |'
- en: '| 12 | c281 | Adverbs | 928 | 264 | 927 | 1178 | 393 | 393 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c281 | 副词 | 928 | 264 | 927 | 1178 | 393 | 393 |'
- en: '| 12 | c220 | List of surnames. | 3886 | 832 | 3652 | 6378 | 2126 | 2126 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c220 | 姓氏列表。 | 3886 | 832 | 3652 | 6378 | 2126 | 2126 |'
- en: '| 12 | c432 | List of Male Names | 279 | 159 | 227 | 474 | 158 | 158 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c432 | 男性姓名列表 | 279 | 159 | 227 | 474 | 158 | 158 |'
- en: '| 12 | c439 | Sports commentary terms. | 250 | 181 | 189 | 687 | 229 | 230
    |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c439 | 体育评论术语。 | 250 | 181 | 189 | 687 | 229 | 230 |'
- en: '| 12 | c173 | List of football team names and stadiums. | 373 | 81 | 287 |
    849 | 283 | 284 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c173 | 足球队名称和体育场列表。 | 373 | 81 | 287 | 849 | 283 | 284 |'
- en: '| 12 | c348 | List of female names and titles. | 575 | 301 | 571 | 774 | 258
    | 258 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c348 | 女性姓名和头衔列表。 | 575 | 301 | 571 | 774 | 258 | 258 |'
- en: '| 12 | c142 | Conflict and War | 407 | 106 | 385 | 582 | 194 | 194 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c142 | 冲突与战争 | 407 | 106 | 385 | 582 | 194 | 194 |'
- en: '| 12 | c245 | Middle East Conflict | 249 | 42 | 196 | 453 | 151 | 152 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c245 | 中东冲突 | 249 | 42 | 196 | 453 | 151 | 152 |'
- en: '| 12 | c210 | List of male first names. | 317 | 205 | 268 | 470 | 157 | 157
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c210 | 男性名字列表。 | 317 | 205 | 268 | 470 | 157 | 157 |'
- en: '| 12 | c564 | List of cities and universities in the southeastern United States.
    | 175 | 21 | 162 | 229 | 76 | 77 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c564 | 美国东南部城市和大学列表。 | 175 | 21 | 162 | 229 | 76 | 77 |'
- en: '| 12 | c533 | LGBTQ+ | 131 | 15 | 118 | 188 | 63 | 63 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c533 | LGBTQ+ | 131 | 15 | 118 | 188 | 63 | 63 |'
- en: '| 12 | c19 | Complex relationships and interactions between family members
    and partners. | 346 | 56 | 333 | 546 | 182 | 182 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c19 | 家庭成员和伴侣之间复杂的关系与互动。 | 346 | 56 | 333 | 546 | 182 | 182 |'
- en: '| 12 | c263 | Locations in New York City | 205 | 48 | 186 | 386 | 129 | 129
    |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c263 | 纽约市的地点 | 205 | 48 | 186 | 386 | 129 | 129 |'
- en: '| 12 | c487 | List of Spanish male names. | 184 | 63 | 174 | 242 | 81 | 81
    |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c487 | 西班牙男性姓名列表。 | 184 | 63 | 174 | 242 | 81 | 81 |'
- en: '| 12 | c247 | Scandinavian/Nordic names and places. | 334 | 64 | 305 | 502
    | 168 | 168 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c247 | 斯堪的纳维亚/北欧姓名和地点。 | 334 | 64 | 305 | 502 | 168 | 168 |'
- en: '| 12 | c44 | Southeast Asian Politics and Ethnic Conflict | 210 | 33 | 149
    | 332 | 111 | 111 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c44 | 东南亚政治与民族冲突 | 210 | 33 | 149 | 332 | 111 | 111 |'
- en: '| 12 | c438 | Verbs for various actions and outcomes. | 896 | 377 | 847 | 1600
    | 534 | 534 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c438 | 各种动作和结果的动词。 | 896 | 377 | 847 | 1600 | 534 | 534 |'
- en: '| 12 | c421 | Names of people and places in the Middle East | 270 | 48 | 230
    | 361 | 120 | 121 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c421 | 中东地区的人名和地名 | 270 | 48 | 230 | 361 | 120 | 121 |'
- en: '| 12 | c553 | Islamic Terminology. | 164 | 26 | 146 | 253 | 84 | 85 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c553 | 伊斯兰术语。 | 164 | 26 | 146 | 253 | 84 | 85 |'
- en: '| 12 | c149 | List of tennis players’ names. | 238 | 82 | 183 | 394 | 132 |
    132 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c149 | 网球运动员姓名列表。 | 238 | 82 | 183 | 394 | 132 | 132 |'
- en: '| 12 | c365 | Criminal activities | 365 | 88 | 337 | 496 | 166 | 166 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c365 | 犯罪活动 | 365 | 88 | 337 | 496 | 166 | 166 |'
- en: 'Table 7: Statistics for concepts extracted from Bert-base-cased and the training,
    dev, test splits used to train the classifier'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 7: 从 Bert-base-cased 提取的概念的统计数据及用于训练分类器的训练、开发、测试数据集'
- en: '| Layer | Tag | Label | val acc | val C acc | sel val | test acc | test c acc
    | sel test |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| Layer | Tag | Label | val acc | val C acc | sel val | test acc | test c acc
    | sel test |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 3 | c90 | Financial terms. | 0.98 | 0.65 | 0.33 | 0.98 | 0.78 | 0.20 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c90 | 财务术语 | 0.98 | 0.65 | 0.33 | 0.98 | 0.78 | 0.20 |'
- en: '| 3 | c336 | Photography-related terms. | 0.99 | 0.74 | 0.25 | 1 | 0.76 | 0.24
    |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c336 | 摄影相关术语 | 0.99 | 0.74 | 0.25 | 1 | 0.76 | 0.24 |'
- en: '| 3 | c112 | Middle Eastern Conflict | 0.99 | 0.89 | 0.10 | 1 | 0.86 | 0.14
    |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c112 | 中东冲突 | 0.99 | 0.89 | 0.10 | 1 | 0.86 | 0.14 |'
- en: '| 3 | c506 | Diversity and Ethnicity. | 1 | 0.78 | 0.22 | 0.98 | 0.75 | 0.23
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c506 | 多样性和种族 | 1 | 0.78 | 0.22 | 0.98 | 0.75 | 0.23 |'
- en: '| 3 | c390 | List of surnames. | 0.97 | 0.82 | 0.15 | 0.97 | 0.82 | 0.15 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c390 | 姓氏列表 | 0.97 | 0.82 | 0.15 | 0.97 | 0.82 | 0.15 |'
- en: '| 3 | c331 | Emotions/Feelings. | 0.98 | 0.82 | 0.16 | 0.99 | 0.78 | 0.21 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c331 | 情感/感受 | 0.98 | 0.82 | 0.16 | 0.99 | 0.78 | 0.21 |'
- en: '| 3 | c592 | Animal names. | 1 | 0.68 | 0.32 | 1 | 0.73 | 0.27 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c592 | 动物名称 | 1 | 0.68 | 0.32 | 1 | 0.73 | 0.27 |'
- en: '| 3 | c25 | Keywords related to discrimination and inequality. | 0.99 | 0.81
    | 0.18 | 0.98 | 0.77 | 0.21 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c25 | 与歧视和不平等相关的关键词 | 0.99 | 0.81 | 0.18 | 0.98 | 0.77 | 0.21 |'
- en: '| 3 | c500 | List of female names. | 0.98 | 0.82 | 0.16 | 0.99 | 0.83 | 0.16
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c500 | 女性名字列表 | 0.98 | 0.82 | 0.16 | 0.99 | 0.83 | 0.16 |'
- en: '| 3 | c414 | Healthcare | 1 | 0.77 | 0.23 | 1 | 0.79 | 0.21 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c414 | 医疗保健 | 1 | 0.77 | 0.23 | 1 | 0.79 | 0.21 |'
- en: '| 3 | c31 | List of male first names. | 0.99 | 0.85 | 0.14 | 1 | 0.83 | 0.17
    |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c31 | 男性名字列表 | 0.99 | 0.85 | 0.14 | 1 | 0.83 | 0.17 |'
- en: '| 3 | c173 | Animals | 0.99 | 0.78 | 0.21 | 0.99 | 0.75 | 0.24 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c173 | 动物 | 0.99 | 0.78 | 0.21 | 0.99 | 0.75 | 0.24 |'
- en: '| 3 | c72 | Natural Disasters and Weather Events | 0.99 | 0.80 | 0.19 | 0.99
    | 0.78 | 0.21 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c72 | 自然灾害和天气事件 | 0.99 | 0.80 | 0.19 | 0.99 | 0.78 | 0.21 |'
- en: '| 3 | c514 | English counties | 1 | 0.74 | 0.26 | 1 | 0.76 | 0.24 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c514 | 英国郡 | 1 | 0.74 | 0.26 | 1 | 0.76 | 0.24 |'
- en: '| 3 | c178 | Body Parts | 0.99 | 0.84 | 0.15 | 0.98 | 0.89 | 0.9 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c178 | 身体部位 | 0.99 | 0.84 | 0.15 | 0.98 | 0.89 | 0.9 |'
- en: '| 3 | c340 | Media and Journalism. | 0.98 | 0.76 | 0.22 | 1 | 0.78 | 0.22 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c340 | 媒体和新闻 | 0.98 | 0.76 | 0.22 | 1 | 0.78 | 0.22 |'
- en: '| 3 | c432 | Power and Status. | 0.99 | 0.79 | 0.20 | 1 | 0.78 | 0.22 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c432 | 权力和地位 | 0.99 | 0.79 | 0.20 | 1 | 0.78 | 0.22 |'
- en: '| 3 | c8 | Verbs | 0.99 | 0.88 | 0.11 | 0.99 | 0.89 | 0.10 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c8 | 动词 | 0.99 | 0.88 | 0.11 | 0.99 | 0.89 | 0.10 |'
- en: '| 3 | c408 | -Verbs ending in "-ing" | 1 | 0.68 | 0.32 | 1 | 0.73 | 0.27 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c408 | 以“-ing”结尾的动词 | 1 | 0.68 | 0.32 | 1 | 0.73 | 0.27 |'
- en: '| 3 | c479 | City names | 0.98 | 0.68 | 0.30 | 1 | 0.83 | 0.17 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c479 | 城市名称 | 0.98 | 0.68 | 0.30 | 1 | 0.83 | 0.17 |'
- en: '| 3 | c343 | Surnames | 1 | 0.74 | 0.26 | 0.98 | 0.74 | 0.24 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c343 | 姓氏 | 1 | 0.74 | 0.26 | 0.98 | 0.74 | 0.24 |'
- en: '| 3 | c577 | Disability-related terms. | 1 | 0.82 | 0.18 | 1 | 0.78 | 0.22
    |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c577 | 与残疾相关的术语 | 1 | 0.82 | 0.18 | 1 | 0.78 | 0.22 |'
- en: '| 9 | c26 | Negative sentiment. | 0.98 | 0.79 | 0.19 | 0.99 | 0.8 | 0.19 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c26 | 负面情绪 | 0.98 | 0.79 | 0.19 | 0.99 | 0.8 | 0.19 |'
- en: '| 9 | c122 | Security Measures | 0.98 | 0.81 | 0.17 | 0.99 | 0.82 | 0.17 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c122 | 安全措施 | 0.98 | 0.81 | 0.17 | 0.99 | 0.82 | 0.17 |'
- en: '| 9 | c423 | Label: Islamic Extremism/Terrorism | 1 | 0.77 | 0.23 | 1 | 0.85
    | 0.15 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c423 | 标签：伊斯兰极端主义/恐怖主义 | 1 | 0.77 | 0.23 | 1 | 0.85 | 0.15 |'
- en: '| 9 | c299 | Middle Eastern and North African countries and cities | 0.99 |
    0.79 | 0.2 | 0.99 | 0.78 | 0.21 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c299 | 中东和北非国家及城市 | 0.99 | 0.79 | 0.2 | 0.99 | 0.78 | 0.21 |'
- en: '| 9 | c192 | Diversity and Identity | 0.99 | 0.88 | 0.11 | 0.98 | 0.88 | 0.1
    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c192 | 多样性和身份 | 0.99 | 0.88 | 0.11 | 0.98 | 0.88 | 0.1 |'
- en: '| 9 | c468 | Russian male names. | 1 | 0.63 | 0.37 | 1 | 0.61 | 0.39 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c468 | 俄罗斯男性名字 | 1 | 0.63 | 0.37 | 1 | 0.61 | 0.39 |'
- en: '| 9 | c588 | Gender-related terms. | 1 | 0.69 | 0.31 | 0.99 | 0.76 | 0.23 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c588 | 性别相关术语 | 1 | 0.69 | 0.31 | 0.99 | 0.76 | 0.23 |'
- en: '| 9 | c74 | Financial terms | 0.99 | 0.86 | 0.13 | 0.97 | 0.83 | 0.14 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c74 | 财务术语 | 0.99 | 0.86 | 0.13 | 0.97 | 0.83 | 0.14 |'
- en: '| 9 | c503 | Middle East Conflict. | 0.99 | 0.75 | 0.24 | 0.99 | 0.71 | 0.28
    |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c503 | 中东冲突 | 0.99 | 0.75 | 0.24 | 0.99 | 0.71 | 0.28 |'
- en: '| 9 | c325 | Violent Crimes | 0.99 | 0.78 | 0.21 | 0.98 | 0.82 | 0.16 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c325 | 暴力犯罪 | 0.99 | 0.78 | 0.21 | 0.98 | 0.82 | 0.16 |'
- en: '| 9 | c535 | Academic Research. | 1 | 0.88 | 0.12 | 0.99 | 0.84 | 0.15 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c535 | 学术研究 | 1 | 0.88 | 0.12 | 0.99 | 0.84 | 0.15 |'
- en: '| 9 | c256 | List of names | 0.98 | 0.76 | 0.22 | 0.98 | 0.74 | 0.24 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c256 | 名字列表 | 0.98 | 0.76 | 0.22 | 0.98 | 0.74 | 0.24 |'
- en: '| 9 | c507 | Positive Adjectives | 0.98 | 0.78 | 0.2 | 0.98 | 0.79 | 0.19 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c507 | 积极形容词 | 0.98 | 0.78 | 0.2 | 0.98 | 0.79 | 0.19 |'
- en: '| 9 | c345 | List of Chinese surnames. | 0.99 | 0.86 | 0.13 | 1 | 0.87 | 0.13
    |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 9 | c345 | 中文姓氏列表。 | 0.99 | 0.86 | 0.13 | 1 | 0.87 | 0.13 |'
- en: '| 12 | c259 | List of names | 0.98 | 0.89 | 0.09 | 0.99 | 0.89 | 0.1 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c259 | 名字列表 | 0.98 | 0.89 | 0.09 | 0.99 | 0.89 | 0.1 |'
- en: '| 12 | c62 | Adverbs | 0.97 | 0.82 | 0.15 | 0.96 | 0.81 | 0.15 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c62 | 副词 | 0.97 | 0.82 | 0.15 | 0.96 | 0.81 | 0.15 |'
- en: '| 12 | c128 | Medical and Healthcare Terminology. | 0.99 | 0.8 | 0.19 | 0.98
    | 0.82 | 0.18 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c128 | 医疗和健康术语。 | 0.99 | 0.8 | 0.19 | 0.98 | 0.82 | 0.18 |'
- en: '| 12 | c301 | Gender-related nouns and pronouns. | 0.98 | 0.8 | 0.18 | 0.98
    | 0.82 | 0.16 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c301 | 与性别相关的名词和代词。 | 0.98 | 0.8 | 0.18 | 0.98 | 0.82 | 0.16 |'
- en: '| 12 | c37 | List of male names. | 0.98 | 0.8 | 0.18 | 0.99 | 0.8 | 0.19 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c37 | 男性名字列表。 | 0.98 | 0.8 | 0.18 | 0.99 | 0.8 | 0.19 |'
- en: '| 12 | c281 | Adverbs | 0.99 | 0.8 | 0.19 | 0.99 | 0.78 | 0.21 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c281 | 副词 | 0.99 | 0.8 | 0.19 | 0.99 | 0.78 | 0.21 |'
- en: '| 12 | c220 | List of surnames. | 0.97 | 0.86 | 0.11 | 0.96 | 0.85 | 0.11 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c220 | 姓氏列表。 | 0.97 | 0.86 | 0.11 | 0.96 | 0.85 | 0.11 |'
- en: '| 12 | c432 | List of Male Names | 1 | 0.71 | 0.29 | 0.97 | 0.73 | 0.24 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c432 | 男性名字列表 | 1 | 0.71 | 0.29 | 0.97 | 0.73 | 0.24 |'
- en: '| 12 | c439 | Sports commentary terms. | 0.9 | 0.82 | 0.08 | 0.94 | 0.74 |
    0.20 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c439 | 体育评论术语。 | 0.9 | 0.82 | 0.08 | 0.94 | 0.74 | 0.20 |'
- en: '| 12 | c173 | List of football team names and stadiums. | 0.99 | 0.82 | 0.17
    | 0.99 | 0.87 | 0.12 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c173 | 足球队名称和体育场列表。 | 0.99 | 0.82 | 0.17 | 0.99 | 0.87 | 0.12 |'
- en: '| 12 | c348 | List of female names and titles. | 0.99 | 0.75 | 0.24 | 0.98
    | 0.7 | 0.28 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c348 | 女性名字和称谓列表。 | 0.99 | 0.75 | 0.24 | 0.98 | 0.7 | 0.28 |'
- en: '| 12 | c142 | Conflict and War | 0.97 | 0.86 | 0.11 | 0.96 | 0.86 | 0.1 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c142 | 冲突与战争 | 0.97 | 0.86 | 0.11 | 0.96 | 0.86 | 0.1 |'
- en: '| 12 | c245 | Middle East Conflict | 0.99 | 0.76 | 0.23 | 0.98 | 0.72 | 0.26
    |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c245 | 中东冲突 | 0.99 | 0.76 | 0.23 | 0.98 | 0.72 | 0.26 |'
- en: '| 12 | c210 | List of male first names | 0.97 | 0.71 | 0.26 | 0.97 | 0.74 |
    0.23 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c210 | 男性名字列表 | 0.97 | 0.71 | 0.26 | 0.97 | 0.74 | 0.23 |'
- en: '| 12 | c564 | List of cities and universities in the southeastern United States.
    | 0.99 | 0.76 | 0.23 | 0.97 | 0.85 | 0.12 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c564 | 美国东南部城市和大学列表。 | 0.99 | 0.76 | 0.23 | 0.97 | 0.85 | 0.12 |'
- en: '| 12 | c533 | LGBTQ+ | 1 | 0.71 | 0.29 | 1 | 0.82 | 0.18 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c533 | LGBTQ+ | 1 | 0.71 | 0.29 | 1 | 0.82 | 0.18 |'
- en: '| 12 | c19 | Complex relationships and interactions between family members
    and partners. | 0.98 | 0.79 | 0.19 | 0.98 | 0.81 | 0.17 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c19 | 家庭成员和伴侣之间复杂的关系和互动。 | 0.98 | 0.79 | 0.19 | 0.98 | 0.81 | 0.17 |'
- en: '| 12 | c263 | Locations in New York City | 0.95 | 0.67 | 0.28 | 0.95 | 0.7
    | 0.25 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c263 | 纽约市的地点 | 0.95 | 0.67 | 0.28 | 0.95 | 0.7 | 0.25 |'
- en: '| 12 | c487 | List of Spanish male names. | 0.98 | 0.84 | 0.14 | 0.95 | 0.69
    | 0.26 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c487 | 西班牙男性名字列表。 | 0.98 | 0.84 | 0.14 | 0.95 | 0.69 | 0.26 |'
- en: '| 12 | c247 | Scandinavian/Nordic names and places. | 0.98 | 0.77 | 0.21 |
    0.97 | 0.75 | 0.22 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c247 | 斯堪的纳维亚/北欧名字和地点。 | 0.98 | 0.77 | 0.21 | 0.97 | 0.75 | 0.22 |'
- en: '| 12 | c44 | Southeast Asian Politics and Ethnic Conflict | 0.96 | 0.85 | 0.11
    | 0.97 | 0.8 | 0.17 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c44 | 东南亚政治与民族冲突 | 0.96 | 0.85 | 0.11 | 0.97 | 0.8 | 0.17 |'
- en: '| 12 | c438 | Verbs for various actions and outcomes. | 0.97 | 0.83 | 0.14
    | 0.97 | 0.85 | 0.12 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c438 | 各种动作和结果的动词。 | 0.97 | 0.83 | 0.14 | 0.97 | 0.85 | 0.12 |'
- en: '| 12 | c421 | Names of people and places in the Middle East | 0.98 | 0.91 |
    0.07 | 0.97 | 0.9 | 0.07 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c421 | 中东地区的地名和人名 | 0.98 | 0.91 | 0.07 | 0.97 | 0.9 | 0.07 |'
- en: '| 12 | c553 | Islamic Terminology. | 1 | 0.7 | 0.3 | 1 | 0.85 | 0.15 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c553 | 伊斯兰术语。 | 1 | 0.7 | 0.3 | 1 | 0.85 | 0.15 |'
- en: '| 12 | c149 | List of tennis players’ names. | 0.95 | 0.73 | 0.22 | 0.98 |
    0.72 | 0.26 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c149 | 网球运动员名字列表。 | 0.95 | 0.73 | 0.22 | 0.98 | 0.72 | 0.26 |'
- en: '| 12 | c365 | Criminal activities | 0.95 | 0.77 | 0.18 | 0.97 | 0.82 | 0.15
    |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c365 | 犯罪活动 | 0.95 | 0.77 | 0.18 | 0.97 | 0.82 | 0.15 |'
- en: 'Table 8: Training Probing Classifiers for the concepts shown in Table [7](#A2.T7
    "Table 7 ‣ Probing For Fine-grained Semantic Concepts ‣ B.1 Running Probes At
    Scale ‣ Appendix B Probing Classifiers ‣ Can LLMs Facilitate Interpretation of
    Pre-trained Language Models?")'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: 训练探测分类器用于表[7](#A2.T7 "表 7 ‣ 细粒度语义概念的探测 ‣ B.1 大规模运行探测 ‣ 附录 B 探测分类器 ‣ LLM
    能否促进对预训练语言模型的解释？")中展示的概念'
- en: '| Layer | Cluster Tag | Label | val acc | val C acc | sel val | test acc |
    test c acc | sel test |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| Layer | Cluster Tag | Label | val acc | val C acc | sel val | test acc |
    test c acc | sel test |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 12 | c189 | Superlatives | 0.98 | 0.61 | 0.37 | 0.96 | 0.79 | 0.17 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c189 | 最高级 | 0.98 | 0.61 | 0.37 | 0.96 | 0.79 | 0.17 |'
- en: '| 12 | c248 | Substance abuse. | 0.97 | 0.6 | 0.37 | 1 | 0.81 | 0.19 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c248 | 物质滥用。 | 0.97 | 0.6 | 0.37 | 1 | 0.81 | 0.19 |'
- en: '| 12 | c361 | LGBTQ+ and Gender-related Terms | 1 | 0.88 | 0.12 | 1 | 0.9 |
    0.1 |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c361 | LGBTQ+ 和性别相关术语 | 1 | 0.88 | 0.12 | 1 | 0.9 | 0.1 |'
- en: '| 3 | c756 | Gender and Sex Labels | 0.87 | 0.72 | 0.15 | 1 | 0.8 | 0.2 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c756 | 性别和性标签 | 0.87 | 0.72 | 0.15 | 1 | 0.8 | 0.2 |'
- en: '| 0 | c720 | Gender and Sex Labels | 1 | 0.74 | 0.26 | 1 | 0.55 | 0.45 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c720 | 性别和性标签 | 1 | 0.74 | 0.26 | 1 | 0.55 | 0.45 |'
- en: '| 0 | c402 | List of female names. | 0.97 | 0.72 | 0.25 | 0.98 | 0.82 | 0.16
    |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c402 | 女性名字列表 | 0.97 | 0.72 | 0.25 | 0.98 | 0.82 | 0.16 |'
- en: '| 12 | c127 | Geopolitical entities and affiliations. | 0.97 | 0.68 | 0.29
    | 0.98 | 0.57 | 0.41 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c127 | 地缘政治实体和隶属关系 | 0.97 | 0.68 | 0.29 | 0.98 | 0.57 | 0.41 |'
- en: '| 0 | c707 | Names of US Presidents and Politicians | 1 | 0.55 | 0.45 | 1 |
    0.55 | 0.45 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c707 | 美国总统和政治人物的名字 | 1 | 0.55 | 0.45 | 1 | 0.55 | 0.45 |'
- en: '| 6 | c101 | Speech verbs. | 0.98 | 0.84 | 0.14 | 1 | 0.7 | 0.3 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 6 | c101 | 言语动词 | 0.98 | 0.84 | 0.14 | 1 | 0.7 | 0.3 |'
- en: '| 0 | c820 | Negative adjectives. | 1 | 0.81 | 0.19 | 0.97 | 0.86 | 0.11 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c820 | 负面形容词 | 1 | 0.81 | 0.19 | 0.97 | 0.86 | 0.11 |'
- en: '| 12 | c769 | Food items. | 0.92 | 0.67 | 0.25 | 0.96 | 0.8 | 0.16 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c769 | 食品项目 | 0.92 | 0.67 | 0.25 | 0.96 | 0.8 | 0.16 |'
- en: '| 0 | c149 | Fruit and plant-related words. | 1 | 0.7 | 0.3 | 0.95 | 0.81 |
    0.14 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c149 | 水果和植物相关的词汇 | 1 | 0.7 | 0.3 | 0.95 | 0.81 | 0.14 |'
- en: '| 3 | c705 | Tourism-related terms | 0.95 | 0.67 | 0.28 | 0.91 | 0.83 | 0.08
    |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c705 | 旅游相关术语 | 0.95 | 0.67 | 0.28 | 0.91 | 0.83 | 0.08 |'
- en: '| 12 | c196 | Verbs of Authority and Request | 0.95 | 0.68 | 0.27 | 0.98 |
    0.89 | 0.09 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c196 | 权威和请求动词 | 0.95 | 0.68 | 0.27 | 0.98 | 0.89 | 0.09 |'
- en: '| 12 | c398 | Energy sources. | 1 | 0.67 | 0.33 | 1 | 0.69 | 0.31 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c398 | 能源来源 | 1 | 0.67 | 0.33 | 1 | 0.69 | 0.31 |'
- en: '| 6 | c185 | Gender-related terms | 0.98 | 0.64 | 0.34 | 0.96 | 0.68 | 0.28
    |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| 6 | c185 | 性别相关术语 | 0.98 | 0.64 | 0.34 | 0.96 | 0.68 | 0.28 |'
- en: '| 3 | c213 | Finance and Taxation. | 0.97 | 0.81 | 0.16 | 0.98 | 0.65 | 0.33
    |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c213 | 财务和税收 | 0.97 | 0.81 | 0.16 | 0.98 | 0.65 | 0.33 |'
- en: '| 0 | c92 | Descriptors of geographic regions and types of organizations. |
    1 | 0.73 | 0.27 | 0.98 | 0.84 | 0.14 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c92 | 地理区域和组织类型的描述词汇 | 1 | 0.73 | 0.27 | 0.98 | 0.84 | 0.14 |'
- en: '| 3 | c659 | Locations in the United States | 1 | 0.88 | 0.12 | 1 | 0.61 |
    0.39 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c659 | 美国的地点 | 1 | 0.88 | 0.12 | 1 | 0.61 | 0.39 |'
- en: '| 0 | c673 | List of Italian first names. | 1 | 0.93 | 0.07 | 0.89 | 0.8 |
    0.09 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c673 | 意大利名字列表 | 1 | 0.93 | 0.07 | 0.89 | 0.8 | 0.09 |'
- en: '| 3 | c67 | List of male names. | 0.99 | 0.81 | 0.18 | 0.99 | 0.81 | 0.18 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c67 | 男性名字列表 | 0.99 | 0.81 | 0.18 | 0.99 | 0.81 | 0.18 |'
- en: '| 0 | c883 | Nouns | 0.97 | 0.83 | 0.14 | 0.99 | 0.81 | 0.18 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c883 | 名词 | 0.97 | 0.83 | 0.14 | 0.99 | 0.81 | 0.18 |'
- en: '| 6 | c898 | TV Networks | 1 | 0.68 | 0.32 | 1 | 0.55 | 0.45 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 6 | c898 | 电视网络 | 1 | 0.68 | 0.32 | 1 | 0.55 | 0.45 |'
- en: '| 12 | c653 | List of years. | 1 | 0.9 | 0.1 | 1 | 0.91 | 0.09 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 12 | c653 | 年份列表 | 1 | 0.9 | 0.1 | 1 | 0.91 | 0.09 |'
- en: '| 0 | c697 | Military Terminology | 1 | 0.62 | 0.38 | 1 | 0.62 | 0.38 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 0 | c697 | 军事术语 | 1 | 0.62 | 0.38 | 1 | 0.62 | 0.38 |'
- en: '| 3 | c560 | Political ideologies and systems. | 1 | 0.58 | 0.42 | 0.94 | 0.75
    | 0.19 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 3 | c560 | 政治意识形态和制度 | 1 | 0.58 | 0.42 | 0.94 | 0.75 | 0.19 |'
- en: 'Table 9: Probe Results for some concepts chosen from several layers in ALBERT'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 从ALBERT的多个层次中选择的一些概念的探测结果'
- en: Appendix C Neuron Analysis Results
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 神经元分析结果
- en: Neurons Associated with POS concepts
  id: totrans-515
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与POS概念相关的神经元
- en: We performed an annotation process on the final layer of a fine-tuned version
    of BERT-base-cased, specifically focusing on the task of parts-of-speech tagging.
    Once we obtained the labels, we organized them into super concepts based on a
    shared characteristic among smaller concepts. For instance, we grouped together
    various concepts labeled as nouns, as well as concepts representing adjectives,
    adverbs, and numerical data. To assess the alignment between the sub concepts
    and the super concept, we calculated the occurrence percentage of the top 10 neurons
    from the sub concept within the top 10 neurons of the super concept. The outcomes
    of this analysis can be found in table [10](#A3.T10 "Table 10 ‣ Neurons Associated
    with the Names concepts ‣ Appendix C Neuron Analysis Results ‣ Can LLMs Facilitate
    Interpretation of Pre-trained Language Models?"), illustrating the average alignment
    between the sub concepts and the super concepts.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对BERT-base-cased的最终层进行了标注处理，专注于词性标注任务。获得标签后，我们根据较小概念之间的共享特征将其组织成超级概念。例如，我们将标记为名词的各种概念以及代表形容词、副词和数值数据的概念归为一组。为了评估子概念与超级概念之间的对齐程度，我们计算了子概念的前10个神经元在超级概念前10个神经元中的出现百分比。该分析的结果可以在表[10](#A3.T10
    "表 10 ‣ 与名称概念相关的神经元 ‣ 附录C 神经元分析结果 ‣ LLM是否有助于解释预训练语言模型？")中找到，展示了子概念与超级概念之间的平均对齐情况。
- en: Neurons Associated with the Names concepts
  id: totrans-517
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与名称概念相关的神经元
- en: We replicated the experiment using named entity concepts derived from the final
    layer of bert-base-cased. The findings are presented in table [11](#A3.T11 "Table
    11 ‣ Neurons Associated with the Names concepts ‣ Appendix C Neuron Analysis Results
    ‣ Can LLMs Facilitate Interpretation of Pre-trained Language Models?").
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用从 bert-base-cased 的最终层中提取的命名实体概念重复了实验。结果呈现在表格[11](#A3.T11 "表格 11 ‣ 与名称概念相关的神经元
    ‣ 附录 C 神经元分析结果 ‣ LLMs 能否促进对预训练语言模型的解释？")中。
- en: '| cluster | label | score |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| cluster | 标签 | 分数 |'
- en: '| c55 | Nouns | 0.2 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| c55 | 名词 | 0.2 |'
- en: '| c13 | Nouns | 0.3 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| c13 | 名词 | 0.3 |'
- en: '| c273 | Nouns | 0.1 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| c273 | 名词 | 0.1 |'
- en: '| c268 | Nouns | 0.4 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| c268 | 名词 | 0.4 |'
- en: '| c405 | Nouns | 0.0 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| c405 | 名词 | 0.0 |'
- en: '| c315 | Nouns | 0.3 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| c315 | 名词 | 0.3 |'
- en: '| c231 | Nouns related to various activities and objects | 0.6 |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| c231 | 与各种活动和物体相关的名词 | 0.6 |'
- en: '| c468 | Nouns | 0.2 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| c468 | 名词 | 0.2 |'
- en: '| c524 | Nouns | 0.2 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| c524 | 名词 | 0.2 |'
- en: '| c387 | Nouns | 0.3 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| c387 | 名词 | 0.3 |'
- en: '| c279 | Nouns related to various industries and sectors | 0.6 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| c279 | 与各种行业和领域相关的名词 | 0.6 |'
- en: '| c440 | Nouns related to various professions and groups | 0.1 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| c440 | 与各种职业和群体相关的名词 | 0.1 |'
- en: '| c202 | Nouns | 0.3 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| c202 | 名词 | 0.3 |'
- en: '| c237 | Adjectives with no clear category or theme | 0.2 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| c237 | 没有明确类别或主题的形容词 | 0.2 |'
- en: '| c299 | Adjectives describing attributes of products or services | 0.3 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| c299 | 描述产品或服务属性的形容词 | 0.3 |'
- en: '| c96 | Adjectives describing ownership, operation or support of various entities
    and technologies | 0.3 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| c96 | 描述各种实体和技术的拥有、操作或支持的形容词 | 0.3 |'
- en: '| c95 | Adjectives describing various types of related events or phenomena
    | 0.1 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| c95 | 描述各种相关事件或现象的形容词 | 0.1 |'
- en: '| c198 | Adjectives with no clear label | 0.2 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| c198 | 没有明确标签的形容词 | 0.2 |'
- en: '| c53 | Comparative Adjectives | 0.3 |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| c53 | 比较级形容词 | 0.3 |'
- en: '| c335 | Comparative Adjectives | 0.2 |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| c335 | 比较级形容词 | 0.2 |'
- en: '| c531 | Comparative Adjectives | 0.1 |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| c531 | 比较级形容词 | 0.1 |'
- en: '| c131 | Descriptive/Adjective Labels | 0.4 |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| c131 | 描述性/形容词标签 | 0.4 |'
- en: '| c505 | Location-based Adjectives | 0.2 |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| c505 | 基于位置的形容词 | 0.2 |'
- en: '| c11 | Adjectives describing various types of entities | 0.0 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| c11 | 描述各种类型实体的形容词 | 0.0 |'
- en: '| c466 | Adjectives describing ownership, operation, or support of various
    entities and technologies. | 0.1 |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| c466 | 描述各种实体和技术的拥有、操作或支持的形容词 | 0.1 |'
- en: '| c419 | Adjectives describing negative or challenging situations. | 0.6 |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| c419 | 描述负面或具有挑战性情况的形容词 | 0.6 |'
- en: '| c128 | Adjectives describing the quality or appropriateness of something.
    | 0.4 |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| c128 | 描述某物质量或适当性的形容词 | 0.4 |'
- en: '| c458 | Adjectives | 0.0 |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| c458 | 形容词 | 0.0 |'
- en: '| c401 | Comparative Adjectives | 0.1 |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| c401 | 比较级形容词 | 0.1 |'
- en: '| c444 | Time-related frequency adjectives | 0.0 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| c444 | 与时间相关的频率形容词 | 0.0 |'
- en: '| c52 | Adverbs. | 0.6 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| c52 | 副词 | 0.6 |'
- en: '| c155 | Adverbs of frequency and manner. | 0.3 |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| c155 | 频率和方式的副词 | 0.3 |'
- en: '| c136 | Adverbs of degree/intensity. | 0.3 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| c136 | 程度/强度的副词 | 0.3 |'
- en: '| c58 | Adverbs of time and transition. | 0.5 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| c58 | 时间和过渡的副词 | 0.5 |'
- en: '| c41 | Adverbs of degree and frequency. | 0.5 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| c41 | 程度和频率的副词 | 0.5 |'
- en: '| c589 | Adverb intensity/degree | 0.2 |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| c589 | 副词强度/程度 | 0.2 |'
- en: '| c265 | Adverbs of Probability and Certainty | 0.3 |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| c265 | 概率和确定性的副词 | 0.3 |'
- en: '| c251 | Adverbs of frequency and manner. | 0.2 |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| c251 | 频率和方式的副词 | 0.2 |'
- en: '| c57 | Adverbs of Frequency | 0.4 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| c57 | 频率副词 | 0.4 |'
- en: '| c555 | Temporal Adverbs. | 0.4 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| c555 | 时间副词 | 0.4 |'
- en: '| c302 | Frequency Adverbs | 0.2 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| c302 | 频率副词 | 0.2 |'
- en: '| c332 | Adverbs of manner and opinion. | 0.4 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| c332 | 描述方式和意见的副词 | 0.4 |'
- en: '| c546 | Adverbs of degree/intensity. | 0.3 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| c546 | 程度/强度的副词 | 0.3 |'
- en: '| c570 | Adverbs of preference/choice. | 0.5 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| c570 | 偏好/选择的副词 | 0.5 |'
- en: '| c244 | Adverbs indicating degree or extent. | 0.3 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| c244 | 表示程度或范围的副词 | 0.3 |'
- en: '| c222 | Adverbs of Time | 0.5 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| c222 | 时间副词 | 0.5 |'
- en: '| c309 | Adverbs describing degree or intensity. | 0.2 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| c309 | 描述程度或强度的副词 | 0.2 |'
- en: '| c487 | List of numerical values. | 0.2 |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| c487 | 数值列表 | 0.2 |'
- en: '| c179 | Numerical Data. | 0.3 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| c179 | 数字数据 | 0.3 |'
- en: '| c420 | Numerical data. | 0.2 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| c420 | 数值数据 | 0.2 |'
- en: '| c390 | List of numbers | 0.3 |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| c390 | 数字列表 | 0.3 |'
- en: '| c287 | Numeric Data. | 0.0 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| c287 | 数字数据 | 0.0 |'
- en: '| c101 | List of numerical values. | 0.5 |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| c101 | 数值列表 | 0.5 |'
- en: '| c494 | List of numerical values. | 0.5 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| c494 | 数值列表 | 0.5 |'
- en: '| c579 | Numerical data. | 0.2 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| c579 | 数值数据 | 0.2 |'
- en: '| c537 | List of numerical values. | 0.2 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| c537 | 数值列表 | 0.2 |'
- en: '| c435 | Numerical data. | 0.3 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| c435 | 数值数据 | 0.3 |'
- en: '| c528 | List of numerical values. | 0.3 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| c528 | 数值列表 | 0.3 |'
- en: '| c549 | List of prices. | 0.5 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| c549 | 价格列表 | 0.5 |'
- en: '| c398 | Numerical Data. | 0.0 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| c398 | 数值数据。 | 0.0 |'
- en: '| c359 | List of numerical values. | 0.1 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| c359 | 数值列表。 | 0.1 |'
- en: '| c477 | List of monetary values. | 0.1 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| c477 | 货币值列表。 | 0.1 |'
- en: '| c593 | List of monetary values. | 0.1 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| c593 | 货币值列表。 | 0.1 |'
- en: '| c80 | Numeric quantities. | 0.1 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| c80 | 数量。 | 0.1 |'
- en: 'Table 10: Neuron Analysis Results on *Super Concepts* extracted from BERT-base-cased
    model. The alignment column shows the intersection between the top 10 neurons
    in the Super concept and the Sub concepts.'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 从 BERT-base-cased 模型中提取的*超级概念*的神经元分析结果。对齐列显示超级概念与子概念中前 10 个神经元的交集。'
- en: '![Refer to caption](img/f16b7a56f2876cfb7364423e417d95ef.png)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f16b7a56f2876cfb7364423e417d95ef.png)'
- en: (a) Political Figures
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 政治人物
- en: '![Refer to caption](img/b146a2ad326c2bec8b5e85d9dd5e204f.png)'
  id: totrans-587
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b146a2ad326c2bec8b5e85d9dd5e204f.png)'
- en: (b) List of Months
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 月份列表
- en: '![Refer to caption](img/66c925e76a001e76d87525f0e516d1af.png)'
  id: totrans-589
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66c925e76a001e76d87525f0e516d1af.png)'
- en: (c) Energy Related terms
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 能源相关术语
- en: '![Refer to caption](img/a189b0c59f60f524961b26b16a44d5d5.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a189b0c59f60f524961b26b16a44d5d5.png)'
- en: (d) Verbs Ending in -ing
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 以 -ing 结尾的动词
- en: '![Refer to caption](img/7f6b962e1fb555a2906b3a19adcacc16.png)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7f6b962e1fb555a2906b3a19adcacc16.png)'
- en: (e) Measurement Units
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 测量单位
- en: '![Refer to caption](img/31803c4444af20287cdf2dea51e6a5db.png)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/31803c4444af20287cdf2dea51e6a5db.png)'
- en: (f) Verbs in Various Tense Forms
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 各种时态的动词
- en: '![Refer to caption](img/dbde329106c8c364bed2d66548b663dc.png)'
  id: totrans-597
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dbde329106c8c364bed2d66548b663dc.png)'
- en: (g) Royalty and Monarchy
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 皇室与君主制
- en: '![Refer to caption](img/10d2436a8450d5e5a2b720b21690c464.png)'
  id: totrans-599
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10d2436a8450d5e5a2b720b21690c464.png)'
- en: (h) Adjectives with less suffix
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 较少后缀的形容词
- en: '![Refer to caption](img/6e5b0362f674211e71e44a2d610f41f5.png)'
  id: totrans-601
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e5b0362f674211e71e44a2d610f41f5.png)'
- en: (i) Monetary Values
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 货币值
- en: 'Figure 10: Sample Concepts learned in the ALBERT Model'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: ALBERT 模型中学习的样本概念'
- en: '| cluster | label | score |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| cluster | 标签 | 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| c259 | List of names | 0.4 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| c259 | 名字列表 | 0.4 |'
- en: '| c37 | List of male names. | 0.3 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| c37 | 男性名字列表。 | 0.3 |'
- en: '| c328 | List of names of politicians, public figures, and athletes. | 0.2
    |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| c328 | 政治家、公众人物和运动员的名字列表。 | 0.2 |'
- en: '| c220 | List of surnames. | 0.4 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| c220 | 姓氏列表。 | 0.4 |'
- en: '| c433 | List of names. | 0.5 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| c433 | 名字列表。 | 0.5 |'
- en: '| c262 | List of surnames | 0.4 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| c262 | 姓氏列表 | 0.4 |'
- en: '| c210 | List of male first names. | 0.1 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| c210 | 男性名字列表。 | 0.1 |'
- en: '| c231 | List of female names. | 0.4 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| c231 | 女性名字列表。 | 0.4 |'
- en: '| c383 | List of names | 0.2 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| c383 | 名字列表 | 0.2 |'
- en: '| c280 | List of names. | 0.2 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| c280 | 名字列表。 | 0.2 |'
- en: '| c202 | List of surnames. | 0.2 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| c202 | 姓氏列表。 | 0.2 |'
- en: '| c344 | Irish surnames | 0.3 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| c344 | 爱尔兰姓氏 | 0.3 |'
- en: '| c6 | Surnames | 0.4 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| c6 | 姓氏 | 0.4 |'
- en: '| c75 | List of female names. | 0.7 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| c75 | 女性名字列表。 | 0.7 |'
- en: '| c269 | List of celebrity names | 0.4 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| c269 | 名人名字列表 | 0.4 |'
- en: '| c578 | List of surnames. | 0.2 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| c578 | 姓氏列表。 | 0.2 |'
- en: '| c535 | List of names | 0.2 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| c535 | 名字列表 | 0.2 |'
- en: '| c487 | List of Spanish male names. | 0.2 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| c487 | 西班牙男性名字列表。 | 0.2 |'
- en: '| c340 | Last names. | 0.4 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| c340 | 姓氏。 | 0.4 |'
- en: '| c48 | List of surnames. | 0.0 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| c48 | 姓氏列表。 | 0.0 |'
- en: '| c70 | List of names. | 0.1 |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| c70 | 名字列表。 | 0.1 |'
- en: '| c353 | List of names in the entertainment industry. | 0.2 |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| c353 | 娱乐行业名字列表。 | 0.2 |'
- en: '| c568 | List of names. | 0.4 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| c568 | 名字列表。 | 0.4 |'
- en: '| c378 | List of surnames. | 0.1 |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| c378 | 姓氏列表。 | 0.1 |'
- en: '| c575 | Surnames | 0.4 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| c575 | 姓氏 | 0.4 |'
- en: '| c149 | List of tennis players’ names. | 0.4 |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| c149 | 网球运动员名字列表。 | 0.4 |'
- en: '| c325 | List of names. | 0.2 |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| c325 | 名字列表。 | 0.2 |'
- en: '| c436 | List of sports players’ names | 0.2 |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| c436 | 体育运动员名字列表 | 0.2 |'
- en: '| c594 | List of surnames. | 0.6 |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| c594 | 姓氏列表。 | 0.6 |'
- en: 'Table 11: Name clusters extracted from the last layer of BERT-base-cased'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 从 BERT-base-cased 的最后一层提取的名字簇'
- en: '![Refer to caption](img/8f0df01bfe02674af85f8706ee6bc8f5.png)'
  id: totrans-636
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f0df01bfe02674af85f8706ee6bc8f5.png)'
- en: (a) Motorsport Terminology
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 赛车术语
- en: '![Refer to caption](img/dcea4b194094a2fce198162295dcb21e.png)'
  id: totrans-638
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dcea4b194094a2fce198162295dcb21e.png)'
- en: (b) US State Abbreviations
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 美国州缩写
- en: '![Refer to caption](img/e92b879eeac482b47d1c7393b7a7cd4e.png)'
  id: totrans-640
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e92b879eeac482b47d1c7393b7a7cd4e.png)'
- en: (c) Education-related terms
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 教育相关术语
- en: 'Figure 11: Example of concepts that were deemed uninterpretable in the BCN
    but were correctly labeled by ChatGPT'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: BCN 中被认为无法解释的概念示例，但 ChatGPT 正确标注了这些概念'
