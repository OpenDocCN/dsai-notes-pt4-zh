- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 17:34:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:34:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Crafting Interpretable Embeddings by Asking LLMs Questions
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过向LLMs提问来制作可解释的嵌入
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16714](https://ar5iv.labs.arxiv.org/html/2405.16714)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16714](https://ar5iv.labs.arxiv.org/html/2405.16714)
- en: Vinamra Benara*
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**维纳姆拉·贝纳拉**'
- en: UC Berkeley &Chandan Singh*
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: UC Berkeley &**钱丹·辛格**
- en: Microsoft Research &John X. Morris
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Research &**约翰·X·莫里斯**
- en: Cornell University &Richard Antonello
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Cornell University &**理查德·安东内洛**
- en: UT Austin Ion Stoica
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: UT Austin **伊昂·斯托伊卡**
- en: UC Berkeley &Alexander G. Huth
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: UC Berkeley &**亚历山大·G·赫斯**
- en: UT Austin &Jianfeng Gao
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: UT Austin &**建锋·高**
- en: Microsoft Research
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Research
- en: '*Equal contribution'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*同等贡献'
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have rapidly improved text embeddings for a growing
    array of natural-language processing tasks. However, their opaqueness and proliferation
    into scientific domains such as neuroscience have created a growing need for interpretability.
    Here, we ask whether we can obtain interpretable embeddings through LLM prompting.
    We introduce question-answering embeddings (QA-Emb), embeddings where each feature
    represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces
    to selecting a set of underlying questions rather than learning model weights.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理任务中迅速改进了文本嵌入。然而，它们的不可解释性以及在神经科学等科学领域的广泛应用，造成了对可解释性的日益需求。在这里，我们探讨了是否可以通过LLM提示获得可解释的嵌入。我们介绍了问答嵌入（QA-Emb），每个特征代表对LLM提出的“是/否”问题的答案。训练QA-Emb转化为选择一组基础问题，而不是学习模型权重。
- en: We use QA-Emb to flexibly generate interpretable models for predicting fMRI
    voxel responses to language stimuli. QA-Emb significantly outperforms an established
    interpretable baseline, and does so while requiring very few questions. This paves
    the way towards building flexible feature spaces that can concretize and evaluate
    our understanding of semantic brain representations. We additionally find that
    QA-Emb can be effectively approximated with an efficient model, and we explore
    broader applications in simple NLP tasks.¹¹1All code for QA-Emb is made available
    on Github at [\faGithub github.com/csinva/interpetable-embeddings](https://github.com/csinva/interpretable-embeddings).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用QA-Emb灵活地生成可解释的模型，以预测fMRI体素对语言刺激的反应。QA-Emb显著优于已有的可解释基线，而且只需要非常少的问题。这为构建能够具体化和评估我们对语义脑表征理解的灵活特征空间铺平了道路。我们还发现QA-Emb可以通过高效模型有效地进行近似，并探讨了在简单NLP任务中的更广泛应用。¹¹1所有QA-Emb的代码都在Github上公开，网址为
    [\faGithub github.com/csinva/interpretable-embeddings](https://github.com/csinva/interpretable-embeddings)。
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Text embeddings are critical to many applications, including information retrieval,
    semantic clustering, retrieval-augmented generation, and language neuroscience.
    Traditionally, text embeddings leveraged interpretable representations such as
    bag-of-words or BM-25 [[1](#bib.bib1)]. Modern methods often replace these embeddings
    with representations from large language models (LLMs), which may better capture
    nuanced contexts and interactions [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]. However, these embeddings are
    essentially black-box representations, making it difficult to understand the predictive
    models built on top of them (as well as why they judge different texts to be similar
    in a retrieval context). This opaqueness is detrimental in scientific fields,
    such as neuroscience [[8](#bib.bib8)] or social science [[9](#bib.bib9)], where
    trustworthy interpretation itself is the end goal. Moreover, this opaqueness has
    debilitated the use of LLM embeddings (for prediction or retrieval) in high-stakes
    applications such as medicine [[10](#bib.bib10)], and raised issues related to
    regulatory pressure, safety, and alignment [[11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入对许多应用至关重要，包括信息检索、语义聚类、检索增强生成和语言神经科学。传统上，文本嵌入利用可解释的表示方法，如词袋模型或BM-25 [[1](#bib.bib1)]。现代方法通常用大型语言模型（LLMs）中的表示替代这些嵌入，这些表示可能更好地捕捉细微的上下文和交互[[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]。然而，这些嵌入本质上是黑箱表示，使得很难理解建立在其上的预测模型（以及为什么它们在检索上下文中判断不同的文本相似）。这种不透明性在科学领域，如神经科学[[8](#bib.bib8)]或社会科学[[9](#bib.bib9)]，对可信的解释本身就是最终目标。这种不透明性还削弱了LLM嵌入在高风险应用如医学[[10](#bib.bib10)]中的使用，并引发了与监管压力、安全性和一致性相关的问题[[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]。
- en: To ameliorate these issues, we introduce question-answering embeddings (QA-Emb),
    a method that builds an interpretable embedding by repeatedly querying a pre-trained
    autoregressive LLM with a set of questions that are selected for a problem ([Fig. 1](#S1.F1
    "In 1 Introduction ‣ Crafting Interpretable Embeddings by Asking LLMs Questions")).
    Each element of the embedding represents the answer to a different question asked
    to an LLM, making the embedding human-inspectable. For example, the first element
    may be the answer to the question Does the input mention time? and the output
    would map yes/no to 1/0. Training QA-Emb requires only black-box access to the
    LLM (it does not require access to the LLM internals) and modifies only natural-language
    prompts, rather than LLM parameters. The learning problem is similar to the optimization
    faced in natural-language autoprompting [[15](#bib.bib15), [16](#bib.bib16)] or
    single-neuron explanation [[17](#bib.bib17), [18](#bib.bib18)], but seeks a set
    of questions rather than an individual prompt.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善这些问题，我们引入了问答嵌入（QA-Emb），这是一种通过反复向预训练的自回归LLM询问一组为某个问题选择的问题来构建可解释嵌入的方法（[Fig. 1](#S1.F1
    "在1介绍 ‣ 通过向LLMs提出问题来构建可解释的嵌入")）。嵌入的每个元素代表对LLM提出的不同问题的回答，使得该嵌入对人类可检查。例如，第一个元素可能是对问题“输入是否提及时间？”的回答，输出将会映射“是/否”到“1/0”。训练QA-Emb只需黑箱访问LLM（不需要访问LLM内部）并仅修改自然语言提示，而不是LLM参数。学习问题类似于自然语言自动提示[[15](#bib.bib15),
    [16](#bib.bib16)]或单神经元解释[[17](#bib.bib17), [18](#bib.bib18)]中面临的优化问题，但寻求的是一组问题而不是单个提示。
- en: We focus on a single neuroscience problem in close collaboration with neuroscientists.
    Grounding in a neuroscience context allows us to avoid common pitfalls in evaluating
    interpretation methods [[19](#bib.bib19), [20](#bib.bib20)] that seek to test
    “interpretability” generally. Additionally, this focus allows to more realistically
    integrate domain knowledge to select and evaluate the questions needed for QA-Emb,
    one of its core strengths. Nevertheless, QA-Emb may be generally applicable in
    other domains where it is important to meaningfully interpret text embeddings.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于与神经科学家密切合作的单一神经科学问题。扎根于神经科学背景使我们能够避免在评估解释方法时常见的陷阱[[19](#bib.bib19), [20](#bib.bib20)]，这些方法旨在一般性地测试“可解释性”。此外，这种专注使我们能够更现实地整合领域知识，选择和评估QA-Emb所需的问题，这是其核心优势之一。尽管如此，QA-Emb可能在其他领域也具有普遍适用性，在这些领域，重要的是要有意义地解释文本嵌入。
- en: In our neuroscience setting, we build QA-Emb representations from natural-language
    questions that can predict human brain responses measured by fMRI to natural-language
    stimuli. This allows for converting informal verbal hypotheses about the semantic
    selectivity of the brain into quantitative models, a pressing challenge in fields
    such as psychology [[21](#bib.bib21)]. We find that predictive models built on
    top of QA-Embs are quite accurate, providing a 26% improvement over an established
    interpretable baseline [[22](#bib.bib22)] and even slightly outperforming a black-box
    BERT baseline [[23](#bib.bib23)]. Additionally, QA-Emb yields concise embeddings,
    outperforming the interpretable baseline (that consists of 985 features) with
    only 29 questions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的神经科学设置中，我们从自然语言问题中构建 QA-Emb 表示，这些问题可以预测人脑对自然语言刺激的 fMRI 响应。这使得将关于大脑语义选择性的非正式口头假设转换为定量模型成为可能，这在心理学等领域是一个迫切挑战
    [[21](#bib.bib21)]。我们发现，基于 QA-Embs 构建的预测模型非常准确，相较于现有的可解释基准提高了 26%，甚至略微超越了黑箱 BERT
    基准 [[22](#bib.bib22)] [[23](#bib.bib23)]。此外，QA-Emb 生成的简洁嵌入在仅用 29 个问题的情况下就超过了包含
    985 个特征的可解释基准。
- en: 'We investigate two major limitations of QA-Emb in [Sec. 5](#S5 "5 Evaluating
    the limitations of QA-Emb ‣ Crafting Interpretable Embeddings by Asking LLMs Questions").
    First, with regards to computational efficiency, we find that we can drastically
    reduce the computational cost of QA-Emb by distilling it into a model that computes
    the answers to all selected questions in a single feedforward pass by using many
    classification heads. Second, we evaluate the accuracy of modern LLMs at reliably
    answering diverse yes/no questions. Finally, [Sec. 6](#S6 "6 Secondary results:
    evaluating QA-Emb in simple NLP tasks ‣ Crafting Interpretable Embeddings by Asking
    LLMs Questions") explores broader applications for QA-Emb in a simple information
    retrieval setting and text-clustering setting.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [第 5 节](#S5 "5 评估 QA-Emb 的局限性 ‣ 通过询问 LLM 问题来打造可解释的嵌入") 中调查了 QA-Emb 的两个主要局限性。首先，就计算效率而言，我们发现可以通过将
    QA-Emb 蒸馏成一个使用多个分类头在单次前向传递中计算所有选择问题的答案的模型，从而大幅降低计算成本。其次，我们评估了现代 LLM 在可靠回答各种是/否问题方面的准确性。最后，[第
    6 节](#S6 "6 次要结果：在简单 NLP 任务中评估 QA-Emb ‣ 通过询问 LLM 问题来打造可解释的嵌入") 探索了 QA-Emb 在简单信息检索和文本聚类设置中的更广泛应用。
- en: '![Refer to caption](img/0c409600e08dfcba9f1c37e4545a7895.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0c409600e08dfcba9f1c37e4545a7895.png)'
- en: 'Figure 1: QA-Emb produces an embedding for an input text by prompting an LLM
    with a series of yes/no questions. This embedding can then be used in downstream
    tasks such as fMRI response prediction or information retrieval.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：QA-Emb 通过用一系列是/否问题来提示 LLM，为输入文本生成一个嵌入。这个嵌入随后可以用于下游任务，例如 fMRI 反应预测或信息检索。
- en: 2 Methods
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: QA-Emb is an intuitive method to generate text embeddings from a pre-trained
    autoregressive LLM ([Fig. 1](#S1.F1 "In 1 Introduction ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions")). Given a text input, QA-Emb builds an interpretable
    embedding by querying the LLM with a set of questions about the input. Each element
    of the embedding represents the answer to a different question asked to an LLM.
    This procedure allows QA-Emb to capture nuanced and relevant details in the input
    while staying interpretable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: QA-Emb 是一种直观的方法，通过预训练的自回归 LLM 生成文本嵌入（[图 1](#S1.F1 "在 1 引言 ‣ 通过询问 LLM 问题来打造可解释的嵌入")）。给定文本输入，QA-Emb
    通过查询 LLM 一组关于输入的问题来构建可解释的嵌入。嵌入的每个元素代表对 LLM 提问的不同问题的答案。这一过程使得 QA-Emb 能够捕捉输入中的细微和相关的细节，同时保持可解释性。
- en: Learning a set of yes/no questions
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习一组是/否问题
- en: 'QA-Emb requires specifying a set of yes/no questions $Q\in\mathcal{Q}_{\text{yes/no}}$
    that yield a binary embedding $v_{Q}(x)\in\{0,1\}^{d}$ for an input string $x$.
    The questions are chosen to yield embeddings that are suitable for a downstream
    task. In our fMRI prediction task, we optimize for supervised linear regression:
    given a list of $n$ input strings $X$ and a multi-dimensional continuous output
    $Y\in\mathbb{R}^{nxd}$, we seek embeddings that allow for learning effective ridge
    regression models:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: QA-Emb 需要指定一组是/否问题 $Q\in\mathcal{Q}_{\text{yes/no}}$，这些问题会生成一个二进制嵌入 $v_{Q}(x)\in\{0,1\}^{d}$
    作为输入字符串 $x$ 的表示。这些问题的选择旨在生成适用于下游任务的嵌入。在我们的 fMRI 预测任务中，我们优化监督线性回归：给定 $n$ 个输入字符串
    $X$ 和一个多维连续输出 $Y\in\mathbb{R}^{nxd}$，我们寻找可以学习有效岭回归模型的嵌入。
- en: '|  | $Q=\underset{Q\in\mathcal{Q}_{\text{yes/no}}}{\text{argmin}}\left[\underset{\theta\in\mathbb{R}^{d}}{\min}\sum_{i}^{n}&#124;&#124;Y^{(i)}-\theta^{T}v_{Q}(X^{(i)})&#124;&#124;+\lambda&#124;&#124;\theta&#124;&#124;_{2}\right],$
    |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q=\underset{Q\in\mathcal{Q}_{\text{yes/no}}}{\text{argmin}}\left[\underset{\theta\in\mathbb{R}^{d}}{\min}\sum_{i}^{n}&#124;&#124;Y^{(i)}-\theta^{T}v_{Q}(X^{(i)})&#124;&#124;+\lambda&#124;&#124;\theta&#124;&#124;_{2}\right],$
    |  | (1) |'
- en: where $\theta$ is a learned coefficient vector for predicting the fMRI responses
    and $\lambda$ is the ridge regularization parameter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 是用于预测 fMRI 响应的学习系数向量，$\lambda$ 是岭回归的正则化参数。
- en: Directly optimizing over the space of yes/no questions is difficult, as it requires
    searching over a discrete space with a constraint set $\mathcal{Q}_{\text{yes/no}}$
    that is hard to specify. Instead, we heuristically optimize the set of questions
    $Q$, by prompting a highly capable LLM (e.g. GPT-4 [[24](#bib.bib24)]) to generate
    questions relevant to our task, e.g. Generate a bulleted list of questions with
    yes/no answers that is relevant for {{task description}}. Customizing the task
    description helps yield relevant questions. The prompt can flexibly specify more
    prior information when available. For example, it can include examples from the
    input dataset to help the LLM identify data-relevant questions. Taking this a
    step further, questions can be generated sequentially (similar to gradient boosting)
    by having the LLM summarize input examples that incur high prediction error to
    generate new questions focused on those examples. While we focus on optimizing
    embeddings for fMRI ridge regression in [Eq. 1](#S2.E1 "In Learning a set of yes/no
    questions ‣ 2 Methods ‣ Crafting Interpretable Embeddings by Asking LLMs Questions"),
    different downstream tasks may require different inner optimization procedures,
    e.g. maximizing the similarity of relevant documents for retrieval.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 直接在是/否问题的空间上进行优化是困难的，因为这需要在一个难以指定的离散空间 $\mathcal{Q}_{\text{yes/no}}$ 上进行搜索。相反，我们通过提示一个高能力的
    LLM（例如 GPT-4 [[24](#bib.bib24)]）生成与我们的任务相关的问题集 $Q$ 来进行启发式优化，例如：生成一个包含是/否回答的项目符号问题列表，这些问题与{{task
    description}}相关。自定义任务描述有助于产生相关的问题。提示可以灵活地在可用时指定更多的先验信息。例如，它可以包含来自输入数据集的示例，以帮助
    LLM 识别数据相关的问题。更进一步地，可以通过让 LLM 总结那些预测误差较高的输入示例来生成问题，类似于梯度提升，从而生成专注于这些示例的新问题。虽然我们专注于[Eq. 1](#S2.E1
    "In Learning a set of yes/no questions ‣ 2 Methods ‣ Crafting Interpretable Embeddings
    by Asking LLMs Questions")中的 fMRI 岭回归的嵌入优化，不同的下游任务可能需要不同的内部优化程序，例如，最大化检索相关文档的相似性。
- en: Post-hoc pruning of $Q$.
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 后处理修剪 $Q$。
- en: The set of learned questions $Q$ can be easily pruned to be made compact and
    useful in different settings. For example, in our fMRI regression setting, a feature-selection
    procedure such as Elastic net [[25](#bib.bib25)] can be used to remove redundant/uninformative
    questions from the specified set of questions $Q$. Alternatively, an LLM can be
    used to directly adapt $Q$ to yield task-specific embeddings. Since the questions
    are all in natural language, they can be listed in a prompt, and an LLM can be
    asked to filter the task-relevant ones, e.g. Here is a list of questions:{{question
    list}} List the subset of these questions that are relevant for {{task description}}.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的问题集 $Q$ 可以很容易地进行修剪，以在不同的设置中使其紧凑且有用。例如，在我们的 fMRI 回归设置中，可以使用弹性网 [[25](#bib.bib25)]
    等特征选择程序来从指定的问题集 $Q$ 中移除冗余/无信息的问题。或者，可以使用 LLM 直接调整 $Q$ 以产生特定任务的嵌入。由于问题都是自然语言的，它们可以列在提示中，然后可以要求
    LLM 过滤出与任务相关的问题，例如：这里是一个问题列表：{{question list}} 列出这些问题中与{{task description}}相关的子集。
- en: 'Limitations: computational cost and LLM inaccuracies.'
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 局限性：计算成本和 LLM 不准确。
- en: While effective, the QA-Emb pipeline described here has two major limitations.
    First, QA-Emb is computationally intensive, requiring $d$ LLM calls to compute
    an embedding. This is often prohibitively expensive, but may be worthwhile in
    high-value applications (such as our fMRI setting) and will likely become more
    tenable as LLM inference costs continue to rapidly decrease. We find that we can
    dramatically reduce this cost by distilling the QA-Emb model into a single LLM
    model with many classification heads in [Sec. 5.1](#S5.SS1 "5.1 Improving computational
    efficiency via model distillation ‣ 5 Evaluating the limitations of QA-Emb ‣ Crafting
    Interpretable Embeddings by Asking LLMs Questions"). Otherwise, LLM inference
    costs are partially mitigated by the ability to reuse the KV-cache for each question
    and the need to only generate a single token for each question. While computing
    embeddings with QA-Emb is expensive, searching embeddings is made faster by the
    fact that the resulting embeddings are binary and often relatively compact.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有效，但此处描述的 QA-Emb 流水线有两个主要限制。首先，QA-Emb 在计算上非常密集，需要 $d$ 次 LLM 调用来计算嵌入。这通常是过于昂贵的，但在高价值应用（例如我们的
    fMRI 设置）中可能是值得的，并且随着 LLM 推理成本的迅速下降，这种方法可能变得更加可行。我们发现，通过将 QA-Emb 模型蒸馏成一个具有多个分类头的单一
    LLM 模型，我们可以显著降低这一成本，详见[第 5.1 节](#S5.SS1 "5.1 Improving computational efficiency
    via model distillation ‣ 5 Evaluating the limitations of QA-Emb ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions")。否则，LLM 推理成本部分得到缓解，因为可以重用每个问题的 KV-cache，并且每个问题只需生成一个
    token。虽然使用 QA-Emb 计算嵌入很昂贵，但搜索嵌入速度更快，因为生成的嵌入是二进制的，并且通常相对紧凑。
- en: Second, QA-Emb requires that the pre-trained LLM can faithfully answer the given
    yes-no questions. If an LLM is unable to accurately answer the questions, it hurts
    explanation’s faithfulness. Thus, QA-Emb requires the use of fairly strong LLMs
    and the set of chosen questions should be accurately answered by these LLMs ([Sec. 5.2](#S5.SS2
    "5.2 Evaluating question-answering faithfulness ‣ 5 Evaluating the limitations
    of QA-Emb ‣ Crafting Interpretable Embeddings by Asking LLMs Questions") provides
    analysis on the question-answering accuracy of different LLMs).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，QA-Emb 需要预训练的 LLM 能够忠实地回答给定的是非问题。如果 LLM 无法准确回答这些问题，就会影响解释的可信度。因此，QA-Emb 需要使用相当强大的
    LLM，并且所选问题集应由这些 LLM 准确回答，[第 5.2 节](#S5.SS2 "5.2 Evaluating question-answering
    faithfulness ‣ 5 Evaluating the limitations of QA-Emb ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions") 提供了不同 LLM 问题回答准确性的分析。
- en: Hyperparameter settings
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超参数设置
- en: For answering questions, we average the answers from Mistral-7B [[26](#bib.bib26)]
    (mistralai/Mistral-7B-Instruct-v0.2) and LLaMA-3 8B [[27](#bib.bib27)] (meta-llama/Meta-Llama-3-8B-Instruct)
    with two prompts. All perform similarly and averaging their answers yields a small
    performance improvement ([Table A2](#A1.T2 "In A.2 fMRI prediction results extended
    ‣ Appendix A Appendix ‣ Crafting Interpretable Embeddings by Asking LLMs Questions")).
    For generating questions, we prompt GPT-4 [[24](#bib.bib24)] (gpt-4-0125-preview).
    Experiments were run using 64 AMD MI210 GPUs, each with 64 gigabytes of memory,
    and reproducing all experiments in the paper requires approximately 4 days (initial
    explorations required roughly 5 times this amount of compute). All prompts used
    and generated questions are given in the appendix or on Github.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回答问题，我们对 Mistral-7B [[26](#bib.bib26)]（mistralai/Mistral-7B-Instruct-v0.2）和
    LLaMA-3 8B [[27](#bib.bib27)]（meta-llama/Meta-Llama-3-8B-Instruct） 的回答进行平均，这两者在使用两个提示时表现相似，平均它们的回答可以带来小的性能提升（见[表
    A2](#A1.T2 "In A.2 fMRI prediction results extended ‣ Appendix A Appendix ‣ Crafting
    Interpretable Embeddings by Asking LLMs Questions")）。生成问题时，我们提示 GPT-4 [[24](#bib.bib24)]（gpt-4-0125-preview）。实验使用了
    64 台 AMD MI210 GPU，每台具有 64 GB 的内存，重现实验约需 4 天（初步探索大约需要 5 倍的计算量）。所有使用的提示和生成的问题都在附录或
    Github 上给出。
- en: 3 Related work
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关工作
- en: Text embeddings
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文本嵌入
- en: Text embeddings models, which produce vector representations of document inputs,
    have been foundational to NLP. Recently, transformer-based models have been trained
    to yield embeddings in a variety of ways [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)], including producing embeddings
    that are sparse [[28](#bib.bib28)] or have variable lengths [[29](#bib.bib29)].
    Recent works have also leveraged autoregressive LLMs to build embeddings, e.g.
    by repeating embeddings [[30](#bib.bib30)], generating synthetic data [[6](#bib.bib6),
    [31](#bib.bib31)], or using the last-token distribution of an autoregressive LLM
    as an embedding [[32](#bib.bib32)]. Similar to QA-Emb, various works have used
    LLM answers to multiple prompts for different purposes, e.g. text classification [[33](#bib.bib33),
    [34](#bib.bib34)] or data exploration [[35](#bib.bib35)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 文本嵌入模型生成文档输入的向量表示，这些模型是NLP的基础。最近，基于变换器的模型已被训练以多种方式生成嵌入 [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]，包括生成稀疏的嵌入 [[28](#bib.bib28)]
    或具有可变长度的嵌入 [[29](#bib.bib29)]。最近的研究还利用自回归LLMs来构建嵌入，例如通过重复嵌入 [[30](#bib.bib30)]、生成合成数据 [[6](#bib.bib6),
    [31](#bib.bib31)]，或使用自回归LLM的最后一个令牌分布作为嵌入 [[32](#bib.bib32)]。类似于QA-Emb，各种研究已经利用LLM回答多个提示来实现不同的目的，例如文本分类 [[33](#bib.bib33),
    [34](#bib.bib34)] 或数据探索 [[35](#bib.bib35)]。
- en: Interpreting representations
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解释表示
- en: A few works have focused on building intrinsically interpretable text representations,
    e.g. word or ngram-based embeddings such as word2vec [[36](#bib.bib36)], Glove [[37](#bib.bib37)],
    and LLM word embeddings. Although their dimensions are not natively interpretable,
    for some tasks, such as classification, they can be projected into a space that
    is interpretable [[38](#bib.bib38)], i.e. a word-level representation. Note that
    it is difficult to learn a sparse interpretable model from these dense embeddings,
    as standard techniques (e.g. Elastic net) cannot be directly applied.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究集中于构建内在可解释的文本表示，例如基于词或n-gram的嵌入，如word2vec [[36](#bib.bib36)]、Glove [[37](#bib.bib37)]
    和LLM词嵌入。尽管它们的维度本质上不可解释，但对于某些任务（如分类），它们可以被投影到一个可解释的空间中 [[38](#bib.bib38)]，即词级别的表示。注意，从这些稠密的嵌入中学习稀疏的可解释模型是困难的，因为标准技术（例如Elastic
    net）不能直接应用。
- en: When instead using black-box representations, there are many post-hoc methods
    to interpret embeddings, e.g. probing [[39](#bib.bib39), [40](#bib.bib40)], categorizing
    elements into categories [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)], categorizing directions in representation space [[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)], or connecting multimodal embeddings with
    text embeddings/text concepts [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)]. For a single pair of text embeddings, prediction-level
    methods can be applied to approximately explain why the two embeddings are similar [[53](#bib.bib53),
    [54](#bib.bib54)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用黑箱表示时，有许多事后方法来解释嵌入，例如探测 [[39](#bib.bib39), [40](#bib.bib40)]、将元素分类到不同类别 [[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]、在表示空间中对方向进行分类 [[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)]，或将多模态嵌入与文本嵌入/文本概念连接 [[48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)]。对于一对文本嵌入，可以应用预测级别的方法来大致解释这两个嵌入为何相似 [[53](#bib.bib53),
    [54](#bib.bib54)]。
- en: Natural language representations in fMRI
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自然语言表示在fMRI中的应用
- en: Using LLM representations to help predict brain responses to natural language
    has recently become popular among neuroscientists studying language processing [[55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60)]
    (see [[61](#bib.bib61), [62](#bib.bib62)] for reviews). This paradigm of using
    “encoding models” [[63](#bib.bib63)] to better understand how the brain processes
    language has been applied to help understand the cortical organization of language
    timescales [[64](#bib.bib64), [65](#bib.bib65)], examine the relationship between
    visual and semantic information in the brain [[66](#bib.bib66)], and explore to
    what extent syntax, semantics, or discourse drives brain activity [[22](#bib.bib22),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71),
    [72](#bib.bib72), [73](#bib.bib73), [18](#bib.bib18)]. The approach here extends
    these works to build an increasingly flexible, interpretable feature space for
    modeling fMRI responses to text data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，使用 LLM 表示来帮助预测大脑对自然语言的反应在研究语言处理的神经科学家中变得越来越流行[[55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60)]（参见[[61](#bib.bib61),
    [62](#bib.bib62)]的综述）。这种使用“编码模型”[[63](#bib.bib63)]来更好地理解大脑如何处理语言的范式已被应用于帮助理解语言时间尺度的皮层组织[[64](#bib.bib64),
    [65](#bib.bib65)]，检验大脑中视觉信息和语义信息之间的关系[[66](#bib.bib66)]，以及探索句法、语义或话语在多大程度上驱动大脑活动[[22](#bib.bib22),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71),
    [72](#bib.bib72), [73](#bib.bib73), [18](#bib.bib18)]。这里的方法将这些工作扩展到构建一个越来越灵活、可解释的特征空间，以便对文本数据的
    fMRI 反应进行建模。
- en: '4 Main results: fMRI interpretation'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 主要结果：fMRI 解释
- en: A central challenge in neuroscience is understanding how and where semantic
    concepts are represented in the brain. To meet this challenge, we extend the line
    of study that fits models to predict the response of different brain voxels (i.e.
    small regions in the brain) to natural language stimuli. Using QA-Emb, we seek
    to bridge models that are interpretable [[1](#bib.bib1), [22](#bib.bib22)] with
    more recent LLM models that are accurate but opaque [[55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 神经科学中的一个核心挑战是理解语义概念在大脑中的表示方式和位置。为了解决这一挑战，我们扩展了将模型拟合到预测不同大脑体素（即大脑中的小区域）对自然语言刺激反应的研究方向。使用
    QA-Emb，我们寻求将可解释的模型[[1](#bib.bib1), [22](#bib.bib22)]与更近期的 LLM 模型（虽然准确但不透明）[[55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57)]结合起来。
- en: 4.1 fMRI experimental setup
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 fMRI 实验设置
- en: Dataset
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集
- en: We analyze data from two recent studies [[74](#bib.bib74), [75](#bib.bib75)]
    (released under the MIT license), which contain fMRI responses for 3 human subjects
    listening to 20+ hours of narrative stories from podcasts. We extract text embeddings
    from the story that each subject hears and fit a ridge regression to predict the
    fMRI responses ([Eq. 1](#S2.E1 "In Learning a set of yes/no questions ‣ 2 Methods
    ‣ Crafting Interpretable Embeddings by Asking LLMs Questions")). Each subject
    listens to either 79 or 82 stories (consisting of 27,449 time points) and 2 test
    stories (639 time points); Each subject’s fMRI data consists of approximately
    100,000 voxels; we preprocess it by running principal component analysis (PCA)
    and extracting the coefficients of the top 100 components.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了两项最近的研究数据[[74](#bib.bib74), [75](#bib.bib75)]（在 MIT 许可证下发布），这些数据包含了 3 位人类受试者在听取
    20 多小时播客叙述故事时的 fMRI 反应。我们从每位受试者听到的故事中提取文本嵌入，并拟合岭回归以预测 fMRI 反应（[Eq. 1](#S2.E1 "在学习一组是/否问题
    ‣ 2 方法 ‣ 通过询问 LLM 问题来制作可解释的嵌入")）。每位受试者听了 79 或 82 个故事（包含 27,449 个时间点）和 2 个测试故事（639
    个时间点）；每位受试者的 fMRI 数据包含约 100,000 个体素；我们通过运行主成分分析（PCA）并提取前 100 个成分的系数来对数据进行预处理。
- en: Regression modeling
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回归建模
- en: We fit ridge regression models to predict these 100 coefficients and evaluate
    the models in the original voxel space (by applying the inverse PCA mapping and
    measuring the correlation between the response and prediction for each voxel).
    We deal with temporal sampling following [[22](#bib.bib22), [57](#bib.bib57)];
    an embedding is produced at the timepoint for each word in the input story and
    these embeddings are interpolated using Lanczos resampling. Embeddings at each
    timepoint are produced from the ngram consisting of the 10 words preceding the
    current timepoint. We select the best-performing hyperparameters via cross-validation
    on 5 time-stratified bootstrap samples of the training set. We select the best
    ridge parameters from 12 logarithmically spaced values between 10 and 10,000.
    To model temporal delays in the fMRI signal, we also select between adding 4,
    8, or 12 time-lagged duplicates of the stimulus features.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拟合 Ridge 回归模型以预测这 100 个系数，并在原始体素空间中评估模型（通过应用逆 PCA 映射并测量每个体素的响应与预测之间的相关性）。我们按照
    [[22](#bib.bib22), [57](#bib.bib57)] 处理时间采样；为输入故事中的每个单词生成一个嵌入，并使用 Lanczos 重采样来插值这些嵌入。在每个时间点生成的嵌入来自于当前时间点前
    10 个单词组成的 ngram。我们通过在训练集的 5 个时间分层自助样本上进行交叉验证来选择最佳性能的超参数。从 12 个对数间隔的值中选择最佳 Ridge
    参数，范围从 10 到 10,000。为了对 fMRI 信号中的时间延迟建模，我们还选择添加 4、8 或 12 个时间滞后的刺激特征重复项。
- en: Generating QA-Emb questions
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成 QA-Emb 问题
- en: To generate the questions underlying QA-Emb, we prompt GPT-4 with 6 prompts
    that aim to elicit knowledge useful for predicting fMRI responses (precise prompts
    in [Sec. A.3](#A1.SS3 "A.3 Prompts ‣ Appendix A Appendix ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions")). This includes directly asking the LLM
    to use its knowledge of neuroscience, to brainstorm semantic properties of narrative
    sentences, to summarize examples from the input data, and to generate questions
    similar to single-voxel explanations found in a prior work [[18](#bib.bib18)].
    This process yields 674 questions ([Fig. 1](#S1.F1 "In 1 Introduction ‣ Crafting
    Interpretable Embeddings by Asking LLMs Questions") and [Table A1](#A1.T1 "In
    A.1 fMRI question details ‣ Appendix A Appendix ‣ Crafting Interpretable Embeddings
    by Asking LLMs Questions") show examples, see all questions on Github). We perform
    feature selection by running multi-task Elastic net with 20 logarithmically spaced
    regularization parameters ranging from $10^{-3}$ to 1 and then fit a Ridge regression
    to the selected features.²²2We run Elastic net using the MultiTaskElasticNet class
    from scikit-learn [[76](#bib.bib76)]. See extended details on the fMRI experimental
    setup in [Sec. A.1](#A1.SS1 "A.1 fMRI question details ‣ Appendix A Appendix ‣
    Crafting Interpretable Embeddings by Asking LLMs Questions") and all prompts in
    [Sec. A.3](#A1.SS3 "A.3 Prompts ‣ Appendix A Appendix ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions").
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成 QA-Emb 的问题，我们使用 6 个提示来激发 GPT-4，旨在引出对预测 fMRI 响应有用的知识（精确提示见 [Sec. A.3](#A1.SS3
    "A.3 Prompts ‣ Appendix A Appendix ‣ Crafting Interpretable Embeddings by Asking
    LLMs Questions")）。这包括直接要求 LLM 使用其神经科学知识，头脑风暴叙述句子的语义属性，总结输入数据中的示例，以及生成类似于先前研究中单体素解释的问题
    [[18](#bib.bib18)]。这一过程产生了 674 个问题（[Fig. 1](#S1.F1 "In 1 Introduction ‣ Crafting
    Interpretable Embeddings by Asking LLMs Questions") 和 [Table A1](#A1.T1 "In A.1
    fMRI question details ‣ Appendix A Appendix ‣ Crafting Interpretable Embeddings
    by Asking LLMs Questions") 显示了示例，所有问题见 Github）。我们通过运行多任务 Elastic net 来执行特征选择，使用
    20 个对数间隔的正则化参数，范围从 $10^{-3}$ 到 1，然后将 Ridge 回归拟合到所选特征上。我们使用 scikit-learn 的 MultiTaskElasticNet
    类运行 Elastic net [[76](#bib.bib76)]。有关 fMRI 实验设置的详细信息见 [Sec. A.1](#A1.SS1 "A.1
    fMRI question details ‣ Appendix A Appendix ‣ Crafting Interpretable Embeddings
    by Asking LLMs Questions")，所有提示见 [Sec. A.3](#A1.SS3 "A.3 Prompts ‣ Appendix A
    Appendix ‣ Crafting Interpretable Embeddings by Asking LLMs Questions")。
- en: Baselines
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基线
- en: We compare QA-Emb to Eng1000, an interpretable baseline developed in the neuroscience
    literature specifically for the task of predicting fMRI responses from narrative
    stories [[22](#bib.bib22)]. Each element in an Eng1000 embedding corresponds to
    a cooccurence statistic with a different word, allowing full interpretation of
    the underlying representation in terms of related words. We additionally compare
    to embeddings from BERT [[23](#bib.bib23)] (bert-base-uncased) and LLaMA models [[77](#bib.bib77),
    [27](#bib.bib27)]. For each subject, we sweep over 5 layers from LLaMA-2 7B (meta-llama/Llama-2-7b-hf,
    layers 6, 12, 18, 24, 30), LLaMA-2 70B (meta-llama/Llama-2-70b-hf, layers 12,
    24, 36, 48, 60), and LLaMA-3 8B (meta-llama/Meta-Llama-3-8B, layers 6, 12, 18,
    24, 30), then report the test performance for the model that yields the best cross-validated
    accuracy (see breakdown in [Table A3](#A1.T3 "In A.2 fMRI prediction results extended
    ‣ Appendix A Appendix ‣ Crafting Interpretable Embeddings by Asking LLMs Questions")).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 QA-Emb 与 Eng1000 进行比较，后者是神经科学文献中专门为预测 fMRI 响应从叙述故事中开发的可解释基准[[22](#bib.bib22)]。Eng1000
    嵌入中的每个元素对应于与不同单词的共现统计，从而完全解释了相关单词的底层表示。我们还与 BERT [[23](#bib.bib23)]（bert-base-uncased）和
    LLaMA 模型[[77](#bib.bib77), [27](#bib.bib27)] 的嵌入进行比较。对于每个受试者，我们遍历了 LLaMA-2 7B（meta-llama/Llama-2-7b-hf，第
    6、12、18、24、30 层）、LLaMA-2 70B（meta-llama/Llama-2-70b-hf，第 12、24、36、48、60 层）和 LLaMA-3
    8B（meta-llama/Meta-Llama-3-8B，第 6、12、18、24、30 层）中的 5 层，然后报告了模型的测试性能，以获得最佳交叉验证准确性的模型（请参见[表
    A3](#A1.T3 "在 A.2 fMRI 预测结果扩展 ‣ 附录 A 附录 ‣ 通过向 LLM 提问来制作可解释的嵌入")）。
- en: 4.2 fMRI predictive performance
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 fMRI 预测性能
- en: \begin{overpic}[width=212.47617pt]{figs/corr_best.pdf} \put(3.0,70.0){{A}} \end{overpic}\begin{overpic}[width=212.47617pt]{figs/sparsity.pdf}
    \put(3.0,70.0){{B}} \end{overpic}\begin{overpic}[width=212.47617pt]{figs/flatmaps/S03_qa_flatmap.pdf}
    \put(3.0,50.0){{C}} \end{overpic}\begin{overpic}[width=212.47617pt]{figs/flatmaps/S03_qa-bert_flatmap.pdf}
    \put(3.0,50.0){{D}} \end{overpic}![Refer to caption](img/28afb7cdb9627bd6e6ea3f526a197d19.png)![Refer
    to caption](img/897be02afc2ba08b905ac88825f20c83.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{overpic}[width=212.47617pt]{figs/corr_best.pdf} \put(3.0,70.0){{A}} \end{overpic}\begin{overpic}[width=212.47617pt]{figs/sparsity.pdf}
    \put(3.0,70.0){{B}} \end{overpic}\begin{overpic}[width=212.47617pt]{figs/flatmaps/S03_qa_flatmap.pdf}
    \put(3.0,50.0){{C}} \end{overpic}\begin{overpic}[width=212.47617pt]{figs/flatmaps/S03_qa-bert_flatmap.pdf}
    \put(3.0,50.0){{D}} \end{overpic}![参见说明](img/28afb7cdb9627bd6e6ea3f526a197d19.png)![参见说明](img/897be02afc2ba08b905ac88825f20c83.png)
- en: 'Figure 2: Predictive performance for QA-Emb compared to baselines. (A) Test
    correlation for QA-Emb outperforms the interpretable Eng1000 baseline, is on par
    with the black-box BERT baseline, and is worse than the best-performing LLaMA
    model. (B) Test correlation for method quickly grows as a function of the number
    of included questions. (C) Test correlation per voxel for QA-Emb. (D) Difference
    in the test correlation per voxel for subject between QA-Emb and BERT. Error bars
    for (A) and (B) (standard error of the mean) are within the points (all are below
    0.001). (B), (C), and (D) show results for subject S03.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：QA-Emb 与基准模型的预测性能对比。(A) QA-Emb 的测试相关性优于可解释的 Eng1000 基准，与黑箱 BERT 基准持平，但逊色于表现最佳的
    LLaMA 模型。(B) 随着包含问题数量的增加，方法的测试相关性迅速增长。(C) QA-Emb 的每个体素的测试相关性。(D) QA-Emb 与 BERT
    在每个体素的测试相关性的差异。(A) 和 (B) 的误差条（均值标准误）位于数据点内（全部低于 0.001）。(B)、(C) 和 (D) 显示了受试者 S03
    的结果。
- en: 'We find that QA-Emb predicts fMRI responses fairly well across subjects ([Fig. 2](#S4.F2
    "In 4.2 fMRI predictive performance ‣ 4 Main results: fMRI interpretation ‣ Crafting
    Interpretable Embeddings by Asking LLMs Questions")A), achieving an average test
    correlation of 0.116. QA-Emb significantly outperforms the interpretable baseline
    Eng1000 (26% average improvement). Comparing to the two transformer-based baselines
    (which do not yield straightforward interpretations), we find that QA-Emb slightly
    outperforms BERT (5% improvement) and worse than the best cross-validated LLaMA-based
    model (7% decrease). Trends are consistent across all 3 subjects.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 QA-Emb 在各受试者中对 fMRI 响应的预测相当准确（[图 2](#S4.F2 "在 4.2 fMRI 预测性能 ‣ 4 主要结果：fMRI
    解释 ‣ 通过向 LLM 提问来制作可解释的嵌入")A），实现了平均测试相关性 0.116。QA-Emb 显著优于可解释的基准 Eng1000（平均提高 26%）。与两个基于变换器的基准模型（这些模型无法提供直接解释）相比，我们发现
    QA-Emb 略微优于 BERT（提高 5%），但逊色于最佳交叉验证 LLaMA 基于的模型（减少 7%）。这些趋势在所有 3 个受试者中都一致。
- en: 'To yield a compact and interpretable model, [Fig. 2](#S4.F2 "In 4.2 fMRI predictive
    performance ‣ 4 Main results: fMRI interpretation ‣ Crafting Interpretable Embeddings
    by Asking LLMs Questions")B further investigates the compressibility of the two
    interpretable methods (through Elastic net regularization). Compared to Eng1000,
    QA-Emb improves performance very quickly as a function of the number of features
    included, even outperforming the final Eng1000 performance with only 29 questions
    (mean test correlation 0.122 versus 0.118). [Table A1](#A1.T1 "In A.1 fMRI question
    details ‣ Appendix A Appendix ‣ Crafting Interpretable Embeddings by Asking LLMs
    Questions") shows the 29 selected questions, which constitute a human-readable
    description of the entire model.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '为了生成一个紧凑且易于解释的模型，[图 2](#S4.F2 "在 4.2 fMRI 预测性能 ‣ 4 主要结果: fMRI 解释 ‣ 通过提问 LLMs
    来创建可解释的嵌入")B 进一步研究了两种可解释方法的压缩性（通过弹性网正则化）。与 Eng1000 相比，QA-Emb 随着包含特征数量的增加，性能提升非常迅速，甚至在仅用
    29 个问题时就超越了最终的 Eng1000 性能（平均测试相关性 0.122 对比 0.118）。[表 A1](#A1.T1 "在 A.1 fMRI 问题细节
    ‣ 附录 A 附录 ‣ 通过提问 LLMs 来创建可解释的嵌入") 显示了这 29 个选定的问题，这些问题构成了对整个模型的可读描述。'
- en: '[Fig. 2](#S4.F2 "In 4.2 fMRI predictive performance ‣ 4 Main results: fMRI
    interpretation ‣ Crafting Interpretable Embeddings by Asking LLMs Questions")C-D
    further break down the predictive performance across different brain regions for
    a particular subject (S03). The regions that are well-predicted by QA-Emb ([Fig. 2](#S4.F2
    "In 4.2 fMRI predictive performance ‣ 4 Main results: fMRI interpretation ‣ Crafting
    Interpretable Embeddings by Asking LLMs Questions")C) align with language-specific
    areas that are seen in the literature [[56](#bib.bib56), [78](#bib.bib78)]. They
    do not show any major diversions from transformer-based encoding models ([Fig. 2](#S4.F2
    "In 4.2 fMRI predictive performance ‣ 4 Main results: fMRI interpretation ‣ Crafting
    Interpretable Embeddings by Asking LLMs Questions")D), with the distribution of
    differences being inconsistent across subjects (see  [Fig. A1](#A1.F1 "In A.2
    fMRI prediction results extended ‣ Appendix A Appendix ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions")).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2](#S4.F2 "在 4.2 fMRI 预测性能 ‣ 4 主要结果: fMRI 解释 ‣ 通过提问 LLMs 来创建可解释的嵌入")C-D
    进一步分解了特定被试 (S03) 在不同脑区的预测性能。QA-Emb 预测准确的区域 ([图 2](#S4.F2 "在 4.2 fMRI 预测性能 ‣ 4
    主要结果: fMRI 解释 ‣ 通过提问 LLMs 来创建可解释的嵌入")C) 与文献中看到的语言特异性区域一致 [[56](#bib.bib56), [78](#bib.bib78)]。它们与基于变压器的编码模型
    ([图 2](#S4.F2 "在 4.2 fMRI 预测性能 ‣ 4 主要结果: fMRI 解释 ‣ 通过提问 LLMs 来创建可解释的嵌入")D) 没有显示出任何重大偏差，差异的分布在被试间不一致（见
    [图 A1](#A1.F1 "在 A.2 fMRI 预测结果扩展 ‣ 附录 A 附录 ‣ 通过提问 LLMs 来创建可解释的嵌入")）。'
- en: '![Refer to caption](img/19f7d60d7ba61044eda6ab7fd680170d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/19f7d60d7ba61044eda6ab7fd680170d.png)'
- en: 'Figure 3: Learned feature weights for 3 example questions capture known selectivity
    and are consistent across subjects. All feature weights are jointly rescaled to
    the range (-1, 1) for visualization. Abbreviations: Pr = precuneus, pTemp = posterior
    temporal cortex, PFC = prefrontal cortex, IPS = intraparietal sulcus, RSC = retrosplenial
    complex, OPA = occipital place area, PPA = parahippocampal place area, Broca =
    Broca’s area, sPMv = superior premotor ventral speech area, AC = auditory cortex.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：3 个示例问题的学习特征权重捕捉了已知的选择性，并且在被试间保持一致。所有特征权重被联合重新缩放到范围 (-1, 1) 以便于可视化。缩写：Pr
    = 后扣带皮层，pTemp =  posterior temporal cortex，PFC = 前额皮层，IPS = 内侧顶叶沟，RSC = 回顾性皮层复合体，OPA
    = 枕叶位置区，PPA = 海马旁位置区，Broca = 布洛卡区，sPMv = 上前运动腹侧语言区，AC = 听觉皮层。
- en: 4.3 Interpreting the fitted representation from QA-Emb
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 从 QA-Emb 解释拟合的表示
- en: 'The QA-Emb representation enables not only identifying which questions are
    important for fMRI prediction, but also mapping their selectivity across the cortex.
    We analyze the QA-Emb model which uses 29 questions and visualize the learned
    regression weights for different questions. [Fig. 3](#S4.F3 "In 4.2 fMRI predictive
    performance ‣ 4 Main results: fMRI interpretation ‣ Crafting Interpretable Embeddings
    by Asking LLMs Questions") shows example flatmaps of the regression coefficients
    for 3 of the questions across the 2 best-predicted subjects (S02 and S03). Learned
    feature weights for the example questions capture known selectivity and are highly
    consistent across subjects. In particular, the weights for the question "Does
    the sentence involve a description of a physical environment or setting?" captures
    classical place areas including occipital place area [[79](#bib.bib79)] and retrosplenial
    complex [[80](#bib.bib80)], as well as intraparietal sulcus [[81](#bib.bib81)].
    The weights for the question "Is the sentence grammatically complex?" bear striking
    similarity to the language network [[78](#bib.bib78), [82](#bib.bib82)], which
    is itself localized from a contrast between sentences and nonwords. Other questions,
    such as "Does the sentence describe a physical action?", which has strong right
    laterality, do not have a strong basis in prior literature. These questions point
    to potentially new insights into poorly understood cortical regions.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: QA-Emb表示不仅能够识别哪些问题对fMRI预测重要，还能绘制它们在皮层中的选择性分布。我们分析了使用29个问题的QA-Emb模型，并可视化了不同问题的回归权重。
    [图3](#S4.F3 "在4.2 fMRI预测性能 ‣ 4 主要结果：fMRI解释 ‣ 通过询问LLMs问题来制作可解释的嵌入") 显示了3个问题在2个最佳预测受试者（S02和S03）中的回归系数的示例平面图。对于示例问题的学习特征权重捕捉了已知的选择性，并且在受试者之间高度一致。特别是，问题“句子是否涉及对物理环境或设置的描述？”的权重捕捉了经典的地点区域，包括枕叶地点区
    [[79](#bib.bib79)] 和后扣带复合体 [[80](#bib.bib80)]，以及顶叶沟 [[81](#bib.bib81)]。问题“句子是否语法复杂？”的权重与语言网络
    [[78](#bib.bib78), [82](#bib.bib82)] 的相似性显著，而语言网络本身是通过句子与无意义词的对比来定位的。其他问题，例如“句子是否描述了一个物理动作？”，具有较强的右侧偏好，但在现有文献中没有强有力的依据。这些问题指向可能对了解皮层区域有新见解的方向。
- en: 5 Evaluating the limitations of QA-Emb
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估QA-Emb的局限性
- en: 5.1 Improving computational efficiency via model distillation
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 通过模型蒸馏提高计算效率
- en: 'Table 1: Mean test correlation when comparing QA-Emb computed via many LLM
    calls to QA-Emb computed via a single distilled model. Distillation does not significantly
    degrade performance. All standard errors of the mean are below $10^{-3}$.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：将通过多个LLM调用计算的QA-Emb与通过单一蒸馏模型计算的QA-Emb进行比较时的平均测试相关性。蒸馏不会显著降低性能。所有均值的标准误差均低于$10^{-3}$。
- en: '|  | QA-Emb | QA-Emb (distill, binary) | QA-Emb (distill, probabilistic) |
    Eng1000 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | QA-Emb | QA-Emb（蒸馏，二进制） | QA-Emb（蒸馏，概率） | Eng1000 |'
- en: '| UTS01 | 0.081 | 0.083 | 0.080 | 0.077 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| UTS01 | 0.081 | 0.083 | 0.080 | 0.077 |'
- en: '| UTS02 | 0.124 | 0.118 | 0.118 | 0.096 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| UTS02 | 0.124 | 0.118 | 0.118 | 0.096 |'
- en: '| UTS03 | 0.136 | 0.132 | 0.142 | 0.117 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| UTS03 | 0.136 | 0.132 | 0.142 | 0.117 |'
- en: '| AVG | 0.114 | 0.111 | 0.113 | 0.097 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| AVG | 0.114 | 0.111 | 0.113 | 0.097 |'
- en: To reduce the computational cost of running inference with QA-Emb, we explore
    distilling the many LLM calls needed to compute QA-Emb into a single model with
    many classification heads. Specifically, we finetune a RoBERTa model [[83](#bib.bib83)]
    (roberta-base) with 674 classification heads to predict all answers required for
    QA-Emb in a single feedforward pass. We finetune the model on answers from LLaMA-3
    8B with a few-shot prompt for 80% of the 10-grams in the 82 fMRI training stories
    (123,203 examples), use the remaining 20% as a validation set for early stopping
    (30,801 examples), and evaluate on all 10-grams in the 2 testing stories (4,594
    examples). We finetune using AdamW [[84](#bib.bib84)] with a learning rate of
    $5\cdot 10^{-5}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低运行QA-Emb推理的计算成本，我们探索将计算QA-Emb所需的多个LLM调用蒸馏成一个具有多个分类头的单一模型。具体而言，我们对RoBERTa模型
    [[83](#bib.bib83)]（roberta-base）进行微调，使用674个分类头来预测QA-Emb所需的所有答案，在一次前向传播中完成。我们在LLaMA-3
    8B的答案上进行微调，使用少量提示处理82个fMRI训练故事中的80% 10-grams（123,203个示例），将剩余20%用作早期停止的验证集（30,801个示例），并在2个测试故事中的所有10-grams上进行评估（4,594个示例）。我们使用AdamW
    [[84](#bib.bib84)] 进行微调，学习率为$5\cdot 10^{-5}$。
- en: When evaluated on the fMRI prediction task, the distilled model (QA-Emb (distill,
    binary) in [Table 1](#S5.T1 "In 5.1 Improving computational efficiency via model
    distillation ‣ 5 Evaluating the limitations of QA-Emb ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions")) yields a performance only slightly below
    the original model. If we relax the restriction that the finetuned model yields
    binary embeddings and instead use the predicted probability for yes, the performance
    rises slightly to nearly match the original model (0.113 instead of 0.114 average
    test correlation) and maintains a significant improvement over the Eng1000 baseline.
    Note that the distilled model achieves an 88.5% match for yes/no answers on 10-grams
    for the test set. Nevertheless, the fMRI prediction for any given timepoint is
    computed from many questions and ngrams, mitigating the effect of individual errors
    in answering a question.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在fMRI预测任务中进行评估时，经过蒸馏的模型（[表1](#S5.T1 "在5.1通过模型蒸馏提高计算效率 ‣ 5 评估QA-Emb的局限性 ‣ 通过向LLMs提问来制作可解释的嵌入")中的QA-Emb（蒸馏，二分类））的表现仅稍低于原始模型。如果我们放宽限制，允许微调模型产生概率嵌入而不是二分类嵌入，而是使用预测的“是”的概率，性能会略微上升，接近原始模型（0.113而不是0.114的平均测试相关性），并保持对Eng1000基线的显著改进。请注意，蒸馏模型在测试集的10-grams中实现了88.5%的“是/否”答案匹配。然而，fMRI预测是基于许多问题和n-grams计算的，从而减轻了回答单个问题时的个体错误影响。
- en: 5.2 Evaluating question-answering faithfulness
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评估问答的忠实度
- en: '![Refer to caption](img/a11f4237d0772061cb3a3ca5828f29c7.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a11f4237d0772061cb3a3ca5828f29c7.png)'
- en: 'Figure 4: Performance of question-answering for underlying LLMs on the D3 collection
    of binary classification datasets. Each point shows an individual dataset and
    error bars show the 95% confidence interval.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：对D3集合中的二分类数据集进行的问答性能评估。每个点表示一个单独的数据集，误差条显示95%的置信区间。
- en: We evaluate the faithfulness of our question-answering models on a recent diverse
    collection of 54 binary classification datasets [[85](#bib.bib85), [86](#bib.bib86)]
    (see data details in [Table A4](#A1.T4 "In A.5 Details on question-answering evaluation
    datasets ‣ Appendix A Appendix ‣ Crafting Interpretable Embeddings by Asking LLMs
    Questions")). These datasets are difficult, as they are intended to encompass
    a wider-ranging and more realistic list of questions than traditional NLP datasets.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在最近收集的54个二分类数据集[[85](#bib.bib85), [86](#bib.bib86)]上评估了我们的问答模型的忠实度（见[表A4](#A1.T4
    "在A.5问答评估数据集的详细信息 ‣ 附录A 附录 ‣ 通过向LLMs提问来制作可解释的嵌入")）。这些数据集非常困难，因为它们旨在涵盖比传统NLP数据集更广泛、更现实的问题列表。
- en: '[Fig. 4](#S5.F4 "In 5.2 Evaluating question-answering faithfulness ‣ 5 Evaluating
    the limitations of QA-Emb ‣ Crafting Interpretable Embeddings by Asking LLMs Questions")
    shows the classification accuracy for the 3 LLMs used previously along with GPT-3.5
    (gpt-3.5-turbo-0125). On average, each of the LLMs answers these questions with
    fairly high accuracy, with GPT-4 slightly outperforming the other models. However,
    we observe poor performance on some tasks, which we attribute to the task difficulty
    and the lack of task-specific prompt engineering. For example, the dataset yielding
    the lowest accuracy asks the question Is the input about math research?. While
    this may seem like a fairly simple question for an LLM to answer, the examples
    in the negative class consist of texts from other quantitative fields (e.g. chemistry)
    that usually contain numbers, math notation, and statistical analysis. Thus the
    LLMs answer yes to most examples and achieve accuracy near chance (50%). Note
    that these tasks are more difficult than the relatively simple questions we answer
    in the fMRI experiments, especially since the fMRI input lengths are each 10 words,
    whereas the input lengths for these datasets are over 50 words on average (with
    some inputs spanning over 1,000 words).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4](#S5.F4 "在 5.2 评估问答的真实性 ‣ 5 评估 QA-Emb 的局限性 ‣ 通过提问 LLM 制作可解释的嵌入") 显示了之前使用的
    3 个 LLM 与 GPT-3.5（gpt-3.5-turbo-0125）的分类准确率。平均而言，这些 LLM 对这些问题的回答准确性都相当高，其中 GPT-4
    略微优于其他模型。然而，我们观察到在某些任务上的表现较差，这归因于任务的难度和缺乏任务特定的提示工程。例如，导致最低准确率的数据集询问的问题是“输入内容是否与数学研究有关？”。尽管这对于
    LLM 来说似乎是一个相当简单的问题，但负类示例的文本来自其他定量领域（如化学），通常包含数字、数学符号和统计分析。因此，LLM 对大多数示例的回答是“是”，并且准确率接近于偶然的（50%）。请注意，这些任务比我们在
    fMRI 实验中回答的相对简单的问题更具挑战性，特别是因为 fMRI 输入长度每个为 10 个单词，而这些数据集的输入长度平均超过 50 个单词（有些输入跨度超过
    1,000 个单词）。'
- en: '6 Secondary results: evaluating QA-Emb in simple NLP tasks'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 次要结果：在简单 NLP 任务中评估 QA-Emb
- en: 6.1 Benchmarking QA-Emb for information retrieval
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 对信息检索中 QA-Emb 的基准测试
- en: In this section, we investigate applying QA-Emb to a simplified information
    retrieval task. We take a random subset of 4,000 queries from the MSMarco dataset
    ([[87](#bib.bib87)], Creative Commons License) and their corresponding groundtruth
    documents, resulting in 5,210 documents. We use 25% of the queries to build a
    training set and keep the remaining 75% for testing. For evaluation, we calculate
    the cosine similarity match between the embeddings for each query and its groundtruth
    documents using mean reciprocal rank and recall.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了将 QA-Emb 应用于简化的信息检索任务。我们从 MSMarco 数据集中随机抽取了 4,000 个查询（[[87](#bib.bib87)]，创作共享许可证）及其对应的真实文档，结果得到
    5,210 个文档。我们使用 25% 的查询构建训练集，并将剩余的 75% 用于测试。为了评估，我们计算每个查询及其真实文档之间的嵌入的余弦相似度匹配，使用平均倒数排名和召回率。
- en: To compute QA-Emb, we first generate 2,000 questions through prompting GPT-4
    based on its knowledge of queries in information retrieval (see prompts in the
    Github). We use a regex to slightly rewrite the resulting questions for queries
    to apply to documents (e.g. Is this query related to a specific timeframe? $\to$
    Is this text related to a specific timeframe?). We then answer the questions both
    for each query and for each corpus document, again using LLaMA-3 8B. Rather than
    fitting a ridge regression as in [Eq. 1](#S2.E1 "In Learning a set of yes/no questions
    ‣ 2 Methods ‣ Crafting Interpretable Embeddings by Asking LLMs Questions"), we
    use the training set to learn a scalar for each question that multiplies its binary
    output to change both its sign and magnitude in the embedding (optimization details
    in [Sec. A.4](#A1.SS4 "A.4 Information retrieval details ‣ Appendix A Appendix
    ‣ Crafting Interpretable Embeddings by Asking LLMs Questions")).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 QA-Emb，我们首先通过提示 GPT-4 生成 2,000 个问题，基于其对信息检索查询的知识（请参阅 Github 上的提示）。我们使用正则表达式稍微改写生成的问题，以便将查询应用于文档（例如，“这个查询是否与特定时间段相关？”
    $\to$ “这段文本是否与特定时间段相关？”）。然后，我们对每个查询和每个语料库文档回答这些问题，再次使用 LLaMA-3 8B。与 [方程 1](#S2.E1
    "在学习一组是/否问题 ‣ 2 方法 ‣ 通过提问 LLM 制作可解释的嵌入") 中拟合岭回归不同，我们使用训练集来学习每个问题的标量，以乘以其二进制输出，从而改变其在嵌入中的符号和幅度（优化细节见
    [第 A.4 节](#A1.SS4 "A.4 信息检索细节 ‣ 附录 A 附录 ‣ 通过提问 LLM 制作可解释的嵌入")）。
- en: '[Table 2](#S6.T2 "In 6.1 Benchmarking QA-Emb for information retrieval ‣ 6
    Secondary results: evaluating QA-Emb in simple NLP tasks ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions") shows the information retrieval results.
    Combining BM-25 with QA-Emb achieves a small but significant improvement over
    the interpretable baselines. QA-Emb on its own achieves modest performance, slightly
    improving slightly over a bag-of-words representation, but significantly underperforming
    BM-25. Nevertheless, its size is considerably smaller than the other interpretable
    baselines making it quicker to interpret and to use for retrieval.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2](#S6.T2 "在 6.1 基准测试 QA-Emb 用于信息检索 ‣ 6 次要结果：评估 QA-Emb 在简单 NLP 任务中的表现 ‣
    通过向 LLM 提问来打造可解释的嵌入") 显示了信息检索结果。将 BM-25 与 QA-Emb 结合使用，相比可解释基准实现了小幅但显著的提升。单独使用
    QA-Emb 的表现适中，相较于词袋模型略有提高，但远不及 BM-25。尽管如此，其大小远小于其他可解释基准，使得解释和使用更为迅速。'
- en: 'Table 2: Information retrieval results for different interpretable embedding
    models. QA-Emb in combination with BM-25 achieves a slight improvement over the
    interpretable baselines. QA-Emb additionally yields reasonably strong performance
    compared to its embedding size. ^†Note that QA-Emb embeddings are binary, so the
    raw number of dimensions overrepresents the embedding’s size relative to other
    methods. Error bars show standard error of the mean.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同可解释嵌入模型的信息检索结果。QA-Emb 与 BM-25 结合使用，相比可解释基准实现了轻微的提升。与其嵌入大小相比，QA-Emb 的表现也相当强劲。^†请注意，QA-Emb
    嵌入是二进制的，因此原始维度数量相对于其他方法过度代表了嵌入的大小。误差条显示均值的标准误差。
- en: '|  | Mean reciprocal rank | Recall@1 | Recall@5 | Size |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均倒排排名 | 召回率@1 | 召回率@5 | 大小 |'
- en: '| Bag of words | 0.37$\pm$.01 | 0.28$\pm$.02 | 0.42$\pm$.02 | 27,677 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 词袋模型 | 0.37$\pm$.01 | 0.28$\pm$.02 | 0.42$\pm$.02 | 27,677 |'
- en: '| Bag of bigrams | 0.39$\pm$.01 | 0.30$\pm$.02 | 0.44$\pm$.02 | 197,924 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 二元词袋 | 0.39$\pm$.01 | 0.30$\pm$.02 | 0.44$\pm$.02 | 197,924 |'
- en: '| Bag of trigrams | 0.39$\pm$.02 | 0.30$\pm$.02 | 0.44$\pm$.02 | 444,403 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 三元组词袋 | 0.39$\pm$.02 | 0.30$\pm$.02 | 0.44$\pm$.02 | 444,403 |'
- en: '| QA-Emb | 0.45$\pm$.01 | 0.34$\pm$.01 | 0.50$\pm$.01 | ^†2,000 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| QA-Emb | 0.45$\pm$.01 | 0.34$\pm$.01 | 0.50$\pm$.01 | ^†2,000 |'
- en: '| BM-25 | 0.77$\pm$.01 | 0.69$\pm$.01 | 0.82$\pm$.01 | 27,677 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| BM-25 | 0.77$\pm$.01 | 0.69$\pm$.01 | 0.82$\pm$.01 | 27,677 |'
- en: '| BM-25 + QA-Emb | 0.80$\pm$.01 | 0.71$\pm$.01 | 0.84$\pm$.01 | 29,677 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| BM-25 + QA-Emb | 0.80$\pm$.01 | 0.71$\pm$.01 | 0.84$\pm$.01 | 29,677 |'
- en: 6.2 Zero-shot adaptation in text clustering
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 零样本适应在文本聚类中的应用
- en: 'We now investigate QA-Emb in a simplified text clustering setting. To do so,
    we study 4 text-classification datasets: Financial phrasebank ([[88](#bib.bib88)],
    creative commons license), Emotion [[89](#bib.bib89)] (CC BY-SA 4.0 license),
    AGNews [[90](#bib.bib90)], and Rotten tomatoes [[91](#bib.bib91)]. For each dataset,
    we treat each class as a cluster and evaluate the clustering score, defined as
    the difference between the average inter-class embedding distance and the average
    intra-class embedding distance (embedding distance is measured via Euclidean distance).
    A larger clustering score suggests that embeddings are well-clustered within each
    class.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在简化的文本聚类设置中研究 QA-Emb。为此，我们研究了 4 个文本分类数据集：Financial phrasebank ([[88](#bib.bib88)]，创意共享许可)，Emotion
    [[89](#bib.bib89)]（CC BY-SA 4.0 许可），AGNews [[90](#bib.bib90)]，以及 Rotten tomatoes
    [[91](#bib.bib91)]。对于每个数据集，我们将每个类别视为一个簇，并评估聚类得分，定义为类别间嵌入距离的平均值与类别内嵌入距离的平均值之间的差异（嵌入距离通过欧氏距离测量）。较大的聚类得分表明嵌入在每个类别内聚类良好。
- en: 'In our experiment, we build a 100-dimensional embedding by prompting GPT-4
    to generate 25 yes/no questions related to the semantic content of each dataset
    (e.g. for Rotten tomatoes, Generate 25 yes/no questions related to movie reviews).
    We then concatenate the answers for all 100 questions to form our embedding. These
    general embeddings do not yield particularly strong clustering scores ([Table 3](#S6.T3
    "In 6.2 Zero-shot adaptation in text clustering ‣ 6 Secondary results: evaluating
    QA-Emb in simple NLP tasks ‣ Crafting Interpretable Embeddings by Asking LLMs
    Questions") top), as the questions are diverse and not particularly selective
    for each dataset.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们通过提示 GPT-4 生成 25 个与每个数据集语义内容相关的是/否问题（例如，对于 Rotten tomatoes，生成 25 个与电影评论相关的是/否问题）来构建一个
    100 维的嵌入。然后，我们将所有 100 个问题的答案连接起来形成我们的嵌入。这些通用嵌入未能产生特别强的聚类得分（[表 3](#S6.T3 "在 6.2
    零样本适应在文本聚类中的应用 ‣ 6 次要结果：评估 QA-Emb 在简单 NLP 任务中的表现 ‣ 通过向 LLM 提问来打造可解释的嵌入") 顶部），因为问题种类繁多，对于每个数据集并不特别具有选择性。
- en: 'However, simply through prompting, we can adapt these general embeddings to
    each individual dataset. We call GPT-4 with a prompt that includes the full list
    of questions and ask it to select a subset of questions that are relevant to each
    task. The result embeddings ([Table 3](#S6.T3 "In 6.2 Zero-shot adaptation in
    text clustering ‣ 6 Secondary results: evaluating QA-Emb in simple NLP tasks ‣
    Crafting Interpretable Embeddings by Asking LLMs Questions") bottom) yield higher
    clustering scores, suggesting that QA-Emb can be adapted to each task in a zero-shot
    manner (in this simplified setting). Moreover, the resulting task-specific embeddings
    are now considerably smaller.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅通过提示，我们可以将这些通用嵌入适应到每个单独的数据集。我们调用 GPT-4 并提供包含完整问题列表的提示，要求它选择与每个任务相关的问题子集。结果嵌入（[表
    3](#S6.T3 "在 6.2 零-shot 适应于文本聚类 ‣ 6 次要结果：在简单 NLP 任务中评估 QA-Emb ‣ 通过询问 LLM 问题来制作可解释的嵌入")
    底部）产生了更高的聚类评分，表明 QA-Emb 可以以零-shot 方式适应每个任务（在这种简化设置中）。此外，结果特定任务的嵌入现在明显更小。
- en: 'Table 3: Clustering scores before and after zero-shot adaptation (higher is
    better). Errors give standard error of the mean.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：零-shot 适应前后的聚类评分（分数越高越好）。误差表示均值的标准误差。
- en: '|  | Rotten tomatoes | AG News | Emotion | Financial phrasebank | AVG | Embedding
    size (AVG) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 烂番茄 | AG 新闻 | 情感 | 财务短语库 | 平均 | 嵌入大小（平均） |'
- en: '| Original | 0.126$\pm$0.011 | 0.124$\pm$0.007 | 0.046$\pm$0.007 | 0.084$\pm$0.008
    | 0.095 | 100 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 0.126$\pm$0.011 | 0.124$\pm$0.007 | 0.046$\pm$0.007 | 0.084$\pm$0.008
    | 0.095 | 100 |'
- en: '| Adapted | 0.248$\pm$0.016 | 0.166$\pm$0.012 | 0.057$\pm$0.010 | 0.292$\pm$0.017
    | 0.191 | 25.75$\pm$0.95 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 调整后 | 0.248$\pm$0.016 | 0.166$\pm$0.012 | 0.057$\pm$0.010 | 0.292$\pm$0.017
    | 0.191 | 25.75$\pm$0.95 |'
- en: 7 Discussion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: We find that QA-Emb can effectively produce interpretable and high-performing
    text embeddings. While we focus on a language fMRI setting, QA-Emb may be able
    to help flexibly build an interpretable text feature space in a variety of domains,
    such as social science [[9](#bib.bib9)], medicine [[10](#bib.bib10)], or economics [[92](#bib.bib92)],
    where meaningful properties of text can help discover something about an underlying
    phenomenon or build trust in high-stakes settings. Alternatively, it could be
    used in mechanistic interpretability, to help improve post-hoc explanations of
    learned LLM representations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 QA-Emb 能有效生成可解释且高性能的文本嵌入。虽然我们主要关注语言 fMRI 设置，QA-Emb 也许能够灵活地在各种领域中构建可解释的文本特征空间，如社会科学
    [[9](#bib.bib9)]、医学 [[10](#bib.bib10)] 或经济学 [[92](#bib.bib92)]，这些领域中的文本有意义的属性可以帮助发现潜在现象或在高风险环境中建立信任。另一方面，它也可以用于机制解释，以帮助改进对学习到的
    LLM 表示的事后解释。
- en: As LLMs improve in both efficiency and capability, QA-Emb can be incorporated
    into a variety of common NLP applications as well, such as RAG or information
    retrieval. For example, in RAG systems such as RAPTOR [[93](#bib.bib93)] or Graph-RAG [[94](#bib.bib94)],
    explanations may help an LLM not only retrieve relevant texts, but also specify
    why they are relevant and how they may be helpful.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 在效率和能力上的提升，QA-Emb 也可以被纳入各种常见的 NLP 应用中，如 RAG 或信息检索。例如，在 RAPTOR [[93](#bib.bib93)]
    或 Graph-RAG [[94](#bib.bib94)] 等 RAG 系统中，解释可能帮助 LLM 不仅检索相关文本，还能够说明它们为何相关以及它们如何有用。
- en: Learning text questions rather than model weights is a challenging research
    area, furthering work in automatic prompt engineering [[15](#bib.bib15), [16](#bib.bib16)].
    Our approach takes a heuristic first step at solving this problem, but future
    work could explore more directly optimizing the set of learned questions $Q$ in
    [Eq. 1](#S2.E1 "In Learning a set of yes/no questions ‣ 2 Methods ‣ Crafting Interpretable
    Embeddings by Asking LLMs Questions") via improved discrete optimization approaches
    and constraints. One possible approach may involve having LLMs themselves identify
    the errors the current model is making and improving based on these errors, similar
    to general trends in LLM self-improvement and autoprompting [[95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)]. Another approach may involve
    improving the explanation capabilities of LLMs to help extract more questions
    more faithfully from data [[99](#bib.bib99), [100](#bib.bib100)].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 学习文本问题而非模型权重是一个具有挑战性的研究领域，进一步推进了自动提示工程的工作[[15](#bib.bib15)、[16](#bib.bib16)]。我们的方法在解决这个问题上迈出了启发性的一步，但未来的工作可以通过改进的离散优化方法和约束，更直接地优化学习问题集
    $Q$，如[Eq. 1](#S2.E1 "在学习一组是/否问题 ‣ 2 方法 ‣ 通过向 LLM 提问来打造可解释的嵌入")。一种可能的方法是让 LLM 自身识别当前模型的错误并根据这些错误进行改进，类似于
    LLM 自我改进和自动提示的普遍趋势[[95](#bib.bib95)、[96](#bib.bib96)、[97](#bib.bib97)、[98](#bib.bib98)]。另一种方法可能涉及提高
    LLM 的解释能力，以更忠实地从数据中提取更多问题[[99](#bib.bib99)、[100](#bib.bib100)]。
- en: Broader Impacts
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: QA-Emb seeks to advance the field of LLM interpretation, a crucial step toward
    addressing the challenges posed by these often opaque models. Although LLMs have
    gained widespread use, their lack of transparency can lead to significant harm,
    underscoring the importance of interpretable AI. There are many potential positive
    societal consequences of this form of interpretability, e.g., facilitating a better
    understanding of scientific data and models, along with a better understanding
    of LLMs and how to use them safely. Nevertheless, as is the case with most ML
    research, the interpretations could be used to interpret and potentially improve
    an LLM or dataset that is being used for nefarious purposes. Moreover, QA-Emb
    requires substantial computational resources, contributing to increased concerns
    over sustainability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: QA-Emb 旨在推动 LLM 解释领域的发展，这是解决这些往往不透明的模型所带来的挑战的重要一步。尽管 LLM 已被广泛使用，但其缺乏透明度可能会造成重大危害，凸显了可解释
    AI 的重要性。这种形式的可解释性可能带来许多积极的社会影响，例如促进对科学数据和模型的更好理解，以及对 LLM 及其安全使用的更好理解。然而，与大多数 ML
    研究一样，这些解释也可能被用来解读并潜在地改进被用于恶意目的的 LLM 或数据集。此外，QA-Emb 需要大量计算资源，增加了对可持续性的担忧。
- en: References
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework:
    Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333–389,
    2009.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Stephen Robertson、Hugo Zaragoza 等人. 概率相关性框架：BM25 及其他。信息检索基础与趋势®，3(4):333–389,
    2009。'
- en: '[2] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using
    siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Nils Reimers 和 Iryna Gurevych. Sentence-BERT：使用 Siamese BERT 网络的句子嵌入。arXiv
    预印本 arXiv:1908.10084, 2019。'
- en: '[3] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage
    search via contextualized late interaction over bert. In Proceedings of the 43rd
    International ACM SIGIR Conference on Research and Development in Information
    Retrieval, SIGIR ’20, New York, NY, USA, 2020\. Association for Computing Machinery.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Omar Khattab 和 Matei Zaharia. COLBERT：通过 BERT 上下文化晚期交互进行高效且有效的段落搜索。发表于第
    43 届国际 ACM SIGIR 信息检索研究与开发会议，SIGIR ’20，纽约，NY，美国，2020。计算机协会。'
- en: '[4] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning
    of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tianyu Gao、Xingcheng Yao 和 Danqi Chen. Simcse：简单的对比学习句子嵌入。arXiv 预印本 arXiv:2104.08821,
    2021。'
- en: '[5] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search.
    arXiv preprint arXiv:2202.08904, 2022.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Niklas Muennighoff. SGPT：用于语义搜索的 GPT 句子嵌入。arXiv 预印本 arXiv:2202.08904, 2022。'
- en: '[6] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and
    Furu Wei. Improving text embeddings with large language models. arXiv preprint
    arXiv:2401.00368, 2023.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Liang Wang、Nan Yang、Xiaolong Huang、Linjun Yang、Rangan Majumder 和 Furu Wei.
    使用大型语言模型改进文本嵌入。arXiv 预印本 arXiv:2401.00368, 2023。'
- en: '[7] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan
    Zhang. Towards general text embeddings with multi-stage contrastive learning.
    arXiv preprint arXiv:2308.03281, 2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, 和 Meishan
    Zhang. 通过多阶段对比学习实现通用文本嵌入。arXiv 预印本 arXiv:2308.03281，2023年。'
- en: '[8] Shailee Jain, Vy A Vo, Leila Wehbe, and Alexander G Huth. Computational
    language modeling and the promise of in silico experimentation. Neurobiology of
    Language, 5(1):80–106, 2024.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Shailee Jain, Vy A Vo, Leila Wehbe, 和 Alexander G Huth. 计算语言建模与计算实验的前景。语言神经生物学，5(1)：80–106，2024年。'
- en: '[9] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi
    Yang. Can large language models transform computational social science? arXiv
    preprint arXiv:2305.03514, 2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, 和 Diyi
    Yang. 大语言模型能否转变计算社会科学？arXiv 预印本 arXiv:2305.03514，2023年。'
- en: '[10] Xiao Zhang, Dejing Dou, and Ji Wu. Learning conceptual-contextual embeddings
    for medical text. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 34, pages 9579–9586, 2020.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Xiao Zhang, Dejing Dou, 和 Ji Wu. 学习医疗文本的概念-上下文嵌入。发表于《AAAI 人工智能会议论文集》，卷34，页码
    9579–9586，2020年。'
- en: '[11] Bryce Goodman and Seth Flaxman. European union regulations on algorithmic
    decision-making and a" right to explanation". arXiv preprint arXiv:1606.08813,
    2016.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Bryce Goodman 和 Seth Flaxman. 欧盟关于算法决策和“解释权”的法规。arXiv 预印本 arXiv:1606.08813，2016年。'
- en: '[12] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman,
    and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565,
    2016.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman,
    和 Dan Mané. 人工智能安全中的具体问题。arXiv 预印本 arXiv:1606.06565，2016年。'
- en: '[13] Iason Gabriel. Artificial intelligence, values, and alignment. Minds and
    machines, 30(3):411–437, 2020.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Iason Gabriel. 人工智能、价值观与对齐。思维与机器，30(3)：411–437，2020年。'
- en: '[14] Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng
    Gao. Rethinking interpretability in the era of large language models. arXiv preprint
    arXiv:2402.01761, 2024.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, 和 Jianfeng
    Gao. 在大语言模型时代重新思考可解释性。arXiv 预印本 arXiv:2402.01761，2024年。'
- en: '[15] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu
    Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt
    engineers. arXiv preprint arXiv:2211.01910, 2022.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu
    Pitis, Harris Chan, 和 Jimmy Ba. 大语言模型是人类级别的提示工程师。arXiv 预印本 arXiv:2211.01910，2022年。'
- en: '[16] Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, and Jianfeng
    Gao. Explaining patterns in data with language models via interpretable autoprompting.
    arXiv preprint arXiv:2210.01848, 2022.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, 和 Jianfeng
    Gao. 通过可解释的自动提示解释数据中的模式。arXiv 预印本 arXiv:2210.01848，2022年。'
- en: '[17] Steven Bills, Nick Cammarata, Dan Mossing, William Saunders, Jeff Wu,
    Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, and Jan Leike. Language models
    can explain neurons in language models. [https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html),
    2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Steven Bills, Nick Cammarata, Dan Mossing, William Saunders, Jeff Wu,
    Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, 和 Jan Leike. 语言模型可以解释语言模型中的神经元。
    [https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)，2023年。'
- en: '[18] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G
    Huth, Bin Yu, and Jianfeng Gao. Explaining black box text modules in natural language
    with language models. arXiv preprint arXiv:2305.09863, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander
    G Huth, Bin Yu, 和 Jianfeng Gao. 使用语言模型用自然语言解释黑箱文本模块。arXiv 预印本 arXiv:2305.09863，2023年。'
- en: '[19] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz
    Hardt, and Been Kim. Sanity checks for saliency maps. In Advances in Neural Information
    Processing Systems, pages 9505–9515, 2018.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz
    Hardt, 和 Been Kim. 针对显著性图的合理性检查。发表于《神经信息处理系统进展》，页码 9505–9515，2018年。'
- en: '[20] Finale Doshi-Velez and Been Kim. A roadmap for a rigorous science of interpretability.
    arXiv preprint arXiv:1702.08608, 2017.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Finale Doshi-Velez 和 Been Kim. 解释性科学的严格路线图。arXiv 预印本 arXiv:1702.08608，2017年。'
- en: '[21] Tal Yarkoni. The generalizability crisis. Behavioral and Brain Sciences,
    45:e1, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Tal Yarkoni. 泛化性危机。行为与脑科学，45:e1，2022年。'
- en: '[22] Alexander G Huth, Wendy A De Heer, Thomas L Griffiths, Frédéric E Theunissen,
    and Jack L Gallant. Natural speech reveals the semantic maps that tile human cerebral
    cortex. Nature, 532(7600):453–458, 2016.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Alexander G Huth, Wendy A De Heer, Thomas L Griffiths, Frédéric E Theunissen,
    和 Jack L Gallant。自然语音揭示了铺展人类大脑皮层的语义图。Nature，532(7600):453–458，2016年。'
- en: '[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。Bert：用于语言理解的深度双向转换器的预训练。arXiv预印本
    arXiv:1810.04805，2018年。'
- en: '[24] OpenAI. GPT-4 technical report. arXiv:2303.08774, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] OpenAI。GPT-4技术报告。arXiv:2303.08774，2023年。'
- en: '[25] Hui Zou and Trevor Hastie. Regularization and variable selection via the
    elastic net. Journal of the Royal Statistical Society Series B: Statistical Methodology,
    67(2):301–320, 2005.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Hui Zou 和 Trevor Hastie。通过弹性网进行正则化和变量选择。《皇家统计学会B系列期刊：统计方法》，67(2):301–320，2005年。'
- en: '[26] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre
    Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and William El
    Sayed. Mistral 7b, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre
    Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, 和 William
    El Sayed。Mistral 7b，2023年。'
- en: '[27] AI@Meta. Llama 3 model card. 2024.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] AI@Meta。Llama 3 模型卡。2024年。'
- en: '[28] Kyoung-Rok Jang, Junmo Kang, Giwon Hong, Sung-Hyon Myaeng, Joohee Park,
    Taewon Yoon, and Heecheol Seo. Ultra-high dimensional sparse representations with
    binarization for efficient text retrieval. arXiv preprint arXiv:2104.07198, 2021.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Kyoung-Rok Jang, Junmo Kang, Giwon Hong, Sung-Hyon Myaeng, Joohee Park,
    Taewon Yoon, 和 Heecheol Seo。超高维稀疏表示与二值化用于高效文本检索。arXiv预印本 arXiv:2104.07198，2021年。'
- en: '[29] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya
    Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek
    Jain, et al. Matryoshka representation learning. Advances in Neural Information
    Processing Systems, 35:30233–30249, 2022.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya
    Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek
    Jain 等。Matryoshka 表示学习。《神经信息处理系统进展》，35:30233–30249，2022年。'
- en: '[30] Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and
    Aditi Raghunathan. Repetition improves language model embeddings. arXiv preprint
    arXiv:2402.15449, 2024.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, 和 Aditi
    Raghunathan。重复改善语言模型嵌入。arXiv预印本 arXiv:2402.15449，2024年。'
- en: '[31] Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R.
    Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik
    Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek
    Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar Naim. Gecko:
    Versatile text embeddings distilled from large language models, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R.
    Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik
    Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek
    Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, 和 Iftekhar Naim。Gecko：从大型语言模型中提炼的多功能文本嵌入，2024年。'
- en: '[32] Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon.
    Promptreps: Prompting large language models to generate dense and sparse representations
    for zero-shot document retrieval, 2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, 和 Guido Zuccon。Promptreps：提示大型语言模型生成密集和稀疏表示以实现零样本文档检索，2024年。'
- en: '[33] Chandan Singh, John Morris, Alexander M Rush, Jianfeng Gao, and Yuntian
    Deng. Tree prompting: Efficient task adaptation without fine-tuning. In Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing, pages
    6253–6267, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Chandan Singh, John Morris, Alexander M Rush, Jianfeng Gao, 和 Yuntian
    Deng。树状提示：无需微调的高效任务适配。在2023年自然语言处理实证方法会议论文集，第6253–6267页，2023年。'
- en: '[34] Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, and Chris
    Callison-Burch. Interpretable-by-design text classification with iteratively generated
    concept bottleneck. arXiv preprint arXiv:2310.19660, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, 和 Chris
    Callison-Burch。设计即可解释的文本分类与迭代生成的概念瓶颈。arXiv预印本 arXiv:2310.19660，2023年。'
- en: '[35] Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu,
    Zihan Wang, and Jingbo Shang. Answer is all you need: Instruction-following text
    embedding via answering the question. arXiv preprint arXiv:2402.09642, 2024.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Letian Peng、Yuwei Zhang、Zilong Wang、Jayanth Srinivasa、Gaowen Liu、Zihan
    Wang 和 Jingbo Shang。答案就是你需要的一切：通过回答问题进行的指令跟随文本嵌入。arXiv 预印本 arXiv:2402.09642，2024。'
- en: '[36] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
    of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Tomas Mikolov、Kai Chen、Greg Corrado 和 Jeffrey Dean。在向量空间中高效估计词表示。arXiv
    预印本 arXiv:1301.3781，2013。'
- en: '[37] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove:
    Global vectors for word representation. In Proceedings of the 2014 conference
    on empirical methods in natural language processing (EMNLP), pages 1532–1543,
    2014.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Jeffrey Pennington、Richard Socher 和 Christopher D Manning。Glove：词表示的全局向量。2014年自然语言处理实证方法会议论文集（EMNLP），页1532–1543，2014。'
- en: '[38] Chandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting
    interpretable models with large language models during training. Nature Communications,
    14(1):7913, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Chandan Singh、Armin Askari、Rich Caruana 和 Jianfeng Gao。在训练过程中用大语言模型增强可解释模型。自然通讯，14(1):7913，2023。'
- en: '[39] Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, and
    Marco Baroni. What you can cram into a single vector: Probing sentence embeddings
    for linguistic properties. arXiv preprint arXiv:1805.01070, 2018.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Alexis Conneau、German Kruszewski、Guillaume Lample、Loïc Barrault 和 Marco
    Baroni。你可以将什么塞入单个向量：探测句子嵌入的语言属性。arXiv 预印本 arXiv:1805.01070，2018。'
- en: '[40] Frederick Liu and Besim Avci. Incorporating priors with feature attribution
    on text classification. arXiv preprint arXiv:1906.08286, 2019.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Frederick Liu 和 Besim Avci。在文本分类中结合先验与特征归因。arXiv 预印本 arXiv:1906.08286，2019。'
- en: '[41] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.
    Network dissection: Quantifying interpretability of deep visual representations.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 6541–6549, 2017.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] David Bau、Bolei Zhou、Aditya Khosla、Aude Oliva 和 Antonio Torralba。网络剖析：量化深度视觉表示的可解释性。IEEE
    计算机视觉与模式识别会议论文集，页6541–6549，2017。'
- en: '[42] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou,
    and Antonio Torralba. Understanding the role of individual units in a deep neural
    network. Proceedings of the National Academy of Sciences, 117(48):30071–30078,
    2020.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] David Bau、Jun-Yan Zhu、Hendrik Strobelt、Agata Lapedriza、Bolei Zhou 和 Antonio
    Torralba。理解深度神经网络中个别单元的作用。美国国家科学院院刊，117(48):30071–30078，2020。'
- en: '[43] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii,
    and Dimitris Bertsimas. Finding neurons in a haystack: Case studies with sparse
    probing, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Wes Gurnee、Neel Nanda、Matthew Pauly、Katherine Harvey、Dmitrii Troitskii
    和 Dimitris Bertsimas。在干草堆中寻找神经元：稀疏探测的案例研究，2023。'
- en: '[44] Nicholas Bai, Rahul A Iyer, Tuomas Oikarinen, and Tsui-Wei Weng. Describe-and-dissect:
    Interpreting neurons in vision networks with language models. arXiv preprint arXiv:2403.13771,
    2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Nicholas Bai、Rahul A Iyer、Tuomas Oikarinen 和 Tsui-Wei Weng。描述与剖析：用语言模型解释视觉网络中的神经元。arXiv
    预印本 arXiv:2403.13771，2024。'
- en: '[45] Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas,
    and Antonio Torralba. Toward a visual concept vocabulary for gan latent space.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    6804–6812, 2021.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Sarah Schwettmann、Evan Hernandez、David Bau、Samuel Klein、Jacob Andreas
    和 Antonio Torralba。迈向 GAN 潜在空间的视觉概念词汇。IEEE/CVF 国际计算机视觉会议论文集，页6804–6812，2021。'
- en: '[46] Ruochen Zhao, Shafiq Joty, Yongjie Wang, and Tan Wang. Explaining language
    models’ predictions with high-impact concepts. arXiv preprint arXiv:2305.02160,
    2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Ruochen Zhao、Shafiq Joty、Yongjie Wang 和 Tan Wang。用高影响概念解释语言模型的预测。arXiv
    预印本 arXiv:2305.02160，2023。'
- en: '[47] Yibo Jiang, Bryon Aragam, and Victor Veitch. Uncovering meanings of embeddings
    via partial orthogonality. Advances in Neural Information Processing Systems,
    36, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Yibo Jiang、Bryon Aragam 和 Victor Veitch。通过部分正交性揭示嵌入的意义。神经信息处理系统进展，36，2024。'
- en: '[48] Tuomas Oikarinen and Tsui-Wei Weng. Clip-dissect: Automatic description
    of neuron representations in deep vision networks. arXiv preprint arXiv:2204.10965,
    2022.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Tuomas Oikarinen 和 Tsui-Wei Weng。Clip-dissect：深度视觉网络中神经元表示的自动描述。arXiv
    预印本 arXiv:2204.10965，2022。'
- en: '[49] Tuomas Oikarinen, Subhro Das, Lam M Nguyen, and Tsui-Wei Weng. Label-free
    concept bottleneck models. arXiv preprint arXiv:2304.06129, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Tuomas Oikarinen、Subhro Das、Lam M Nguyen 和 Tsui-Wei Weng。无标签概念瓶颈模型。arXiv
    预印本 arXiv:2304.06129，2023。'
- en: '[50] Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P Calmon, and Himabindu
    Lakkaraju. Interpreting clip with sparse linear concept embeddings (splice). arXiv
    preprint arXiv:2402.10376, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 乌莎·巴拉, 亚历克斯·奥斯特林, 苏拉杰·斯里尼瓦斯, 弗拉维奥·P·卡尔蒙, 和希玛宾杜·拉卡拉朱。用稀疏线性概念嵌入（splice）解释clip。arXiv预印本arXiv:2402.10376，2024年。'
- en: '[51] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch,
    and Mark Yatskar. Language in a bottle: Language model guided concept bottlenecks
    for interpretable image classification. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 19187–19197, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 岳阳, 阿耳忒弥斯·帕纳戈普卢, 朱盛豪, 丹尼尔·金, 克里斯·卡利森-伯奇, 和马克·雅茨卡尔。瓶中语言：语言模型引导的概念瓶颈用于可解释的图像分类。发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，第19187–19197页，2023年。'
- en: '[52] Aya Abdelsalam Ismail, Julius Adebayo, Hector Corrada Bravo, Stephen Ra,
    and Kyunghyun Cho. Concept bottleneck generative models. In The Twelfth International
    Conference on Learning Representations, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 阿雅·阿卜杜勒萨拉姆·伊斯梅尔, 朱利叶斯·阿德巴约, 赫克托·科拉达·布拉沃, 斯蒂芬·拉, 和崔京贤。概念瓶颈生成模型。发表于第十二届国际学习表示会议，2023年。'
- en: '[53] Ruoyu Chen, Jingzhi Li, Hua Zhang, Changchong Sheng, Li Liu, and Xiaochun
    Cao. Sim2word: Explaining similarity with representative attribute words via counterfactual
    explanations. ACM Trans. Multimedia Comput. Commun. Appl., 19(6), jul 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 陈若宇, 李晶芝, 张华, 盛长冲, 刘丽, 和曹晓春。Sim2word：通过反事实解释解释相似性及代表性属性词。ACM《多媒体计算、通信与应用》杂志，19(6)，2023年7月。'
- en: '[54] Karthikeyan Natesan Ramamurthy, Amit Dhurandhar, Dennis Wei, and Zaid Bin
    Tariq. Analogies and feature attributions for model agnostic explanation of similarity
    learners. arXiv preprint arXiv:2202.01153, 2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 卡尔蒂凯扬·奈特桑·拉马穆尔西, 阿米特·杜兰达, 丹尼斯·魏, 和扎伊德·宾·塔里克。类比和特征归因用于模型无关的相似性学习者解释。arXiv预印本arXiv:2202.01153，2022年。'
- en: '[55] Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A
    Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. The neural
    architecture of language: Integrative modeling converges on predictive processing.
    Proceedings of the National Academy of Sciences, 118(45):e2105646118, 2021.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 马丁·施林普夫, 伊丹·阿舍·布兰克, 格雷塔·塔克特, 卡里娜·考夫, 埃赫巴尔·A·霍塞因, 南希·坎威舍, 约书亚·B·特嫩鲍姆, 和埃维琳娜·费多伦科。语言的神经结构：综合建模收敛于预测处理。《国家科学院学报》，118(45):e2105646118，2021年。'
- en: '[56] Richard Antonello, Aditya Vaidya, and Alexander Huth. Scaling laws for
    language encoding models in fmri. Advances in Neural Information Processing Systems,
    36, 2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 理查德·安东内洛, 阿迪提亚·韦迪亚, 和亚历山大·赫斯。fMRI中语言编码模型的缩放定律。《神经信息处理系统进展》，36卷，2024年。'
- en: '[57] Shailee Jain and Alexander Huth. Incorporating context into language encoding
    models for fmri. Advances in neural information processing systems, 31, 2018.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 沙伊利·贾因 和 亚历山大·赫斯。在fMRI中将上下文纳入语言编码模型。《神经信息处理系统进展》，31卷，2018年。'
- en: '[58] Leila Wehbe, Ashish Vaswani, Kevin Knight, and Tom Mitchell. Aligning
    context-based statistical models of language with brain activity during reading.
    In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pages 233–243, Doha, Qatar, October 2014\. Association for
    Computational Linguistics.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 莱拉·韦赫比, 阿希什·瓦斯瓦尼, 凯文·奈特, 和汤姆·米切尔。对齐语言的基于上下文的统计模型与阅读期间的大脑活动。发表于2014年《自然语言处理经验方法会议（EMNLP）》论文集，第233–243页，卡塔尔多哈，2014年10月。计算语言学协会。'
- en: '[59] Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language
    processing (in machines) with natural language-processing (in the brain). In H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox, and R. Garnett, editors,
    Advances in Neural Information Processing Systems, volume 32\. Curran Associates,
    Inc., 2019.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 玛丽亚·托涅瓦 和 莱拉·韦赫比。用自然语言处理（机器中）解释和改进自然语言处理（大脑中的）。在H.·沃拉赫, H.·拉罗谢尔, A.·贝戈尔齐默,
    F.·达尔切-布克, E.·福克斯, 和R.·加内特主编的《神经信息处理系统进展》，第32卷。柯伦协会，2019年。'
- en: '[60] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price,
    Bobbi Aubrey, Samuel A. Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, Aren Jansen,
    Harshvardhan Gazula, Gina Choe, Aditi Rao, Catherine Kim, Colton Casto, Lora Fanda,
    Werner Doyle, Daniel Friedman, Patricia Dugan, Lucia Melloni, Roi Reichart, Sasha
    Devore, Adeen Flinker, Liat Hasenfratz, Omer Levy, Avinatan Hassidim, Michael
    Brenner, Yossi Matias, Kenneth A. Norman, Orrin Devinsky, and Uri Hasson. Shared
    computational principles for language processing in humans and deep language models.
    Nature Neuroscience, 25(3):369–380, March 2022. Number: 3 Publisher: Nature Publishing
    Group.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price,
    Bobbi Aubrey, Samuel A. Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, Aren Jansen,
    Harshvardhan Gazula, Gina Choe, Aditi Rao, Catherine Kim, Colton Casto, Lora Fanda,
    Werner Doyle, Daniel Friedman, Patricia Dugan, Lucia Melloni, Roi Reichart, Sasha
    Devore, Adeen Flinker, Liat Hasenfratz, Omer Levy, Avinatan Hassidim, Michael
    Brenner, Yossi Matias, Kenneth A. Norman, Orrin Devinsky 和 Uri Hasson. 人类和深度语言模型中语言处理的共享计算原理。《自然神经科学》，25(3):369–380,
    2022年3月。出版商: 自然出版集团。'
- en: '[61] John T. Hale, Luca Campanelli, Jixing Li, Shohini Bhattasali, Christophe
    Pallier, and Jonathan R. Brennan. Neurocomputational models of language processing.
    Annual Review of Linguistics, 8(1):427–446, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] John T. Hale, Luca Campanelli, Jixing Li, Shohini Bhattasali, Christophe
    Pallier 和 Jonathan R. Brennan. 语言处理的神经计算模型。《语言学年鉴》，8(1):427–446, 2022年。'
- en: '[62] Shailee Jain, Vy A. Vo, Leila Wehbe, and Alexander G. Huth. Computational
    Language Modeling and the Promise of in Silico Experimentation. Neurobiology of
    Language, pages 1–27, March 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Shailee Jain, Vy A. Vo, Leila Wehbe 和 Alexander G. Huth. 计算语言建模与硅基实验的前景。《语言神经生物学》，第1–27页，2023年3月。'
- en: '[63] Michael C.-K. Wu, Stephen V. David, and Jack L. Gallant. Complete functional
    characterization of sensory neurons by system identification. Annual Review of
    Neuroscience, 29:477–505, 2006.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Michael C.-K. Wu, Stephen V. David 和 Jack L. Gallant. 通过系统辨识对感觉神经元进行完全功能特征化。《神经科学年鉴》，29:477–505,
    2006年。'
- en: '[64] Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel, Javier S Turek, and
    Alexander Huth. Interpretable multi-timescale models for predicting fmri responses
    to continuous natural speech. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
    Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
    volume 33, pages 13738–13749\. Curran Associates, Inc., 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Shailee Jain, Vy Vo, Shivangi Mahto, Amanda LeBel, Javier S Turek 和 Alexander
    Huth. 可解释的多时间尺度模型用于预测对连续自然语言的 fMRI 反应。编者 H. Larochelle, M. Ranzato, R. Hadsell,
    M. F. Balcan 和 H. Lin，《神经信息处理系统进展》，第33卷，第13738–13749页。Curran Associates, Inc.,
    2020年。'
- en: '[65] Catherine Chen, Tom Dupré la Tour, Jack Gallant, Daniel Klein, and Fatma
    Deniz. The cortical representation of language timescales is shared between reading
    and listening. bioRxiv, pages 2023–01, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Catherine Chen, Tom Dupré la Tour, Jack Gallant, Daniel Klein 和 Fatma
    Deniz. 语言时间尺度的皮层表征在阅读和听力之间共享。bioRxiv，第2023–01页，2023年。'
- en: '[66] Sara F Popham, Alexander G Huth, Natalia Y Bilenko, Fatma Deniz, James S
    Gao, Anwar O Nunez-Elizalde, and Jack L Gallant. Visual and linguistic semantic
    representations are aligned at the border of human visual cortex. Nature neuroscience,
    24(11):1628–1636, 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Sara F Popham, Alexander G Huth, Natalia Y Bilenko, Fatma Deniz, James
    S Gao, Anwar O Nunez-Elizalde 和 Jack L Gallant. 视觉和语言语义表征在人体视觉皮层边界对齐。《自然神经科学》，24(11):1628–1636,
    2021年。'
- en: '[67] Charlotte Caucheteux, Alexandre Gramfort, and Jean-Remi King. Disentangling
    syntax and semantics in the brain with deep networks. In Proceedings of the 38th
    International Conference on Machine Learning, pages 1336–1348\. PMLR, July 2021.
    ISSN: 2640-3498.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Charlotte Caucheteux, Alexandre Gramfort 和 Jean-Remi King. 使用深度网络解开大脑中的句法和语义。第38届国际机器学习会议论文集，第1336–1348页。PMLR,
    2021年7月。ISSN: 2640-3498。'
- en: '[68] Carina Kauf, Greta Tuckute, Roger Levy, Jacob Andreas, and Evelina Fedorenko.
    Lexical semantic content, not syntactic structure, is the main contributor to
    ann-brain similarity of fmri responses in the language network. bioRxiv, pages
    2023–05, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Carina Kauf, Greta Tuckute, Roger Levy, Jacob Andreas 和 Evelina Fedorenko.
    词汇语义内容，而非句法结构，是语言网络中 fMRI 反应的主要贡献者。bioRxiv，第2023–05页，2023年。'
- en: '[69] Aniketh Janardhan Reddy and Leila Wehbe. Can fMRI reveal the representation
    of syntactic structure in the brain? preprint, Neuroscience, June 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Aniketh Janardhan Reddy 和 Leila Wehbe. fMRI 能否揭示大脑中句法结构的表征？预印本，《神经科学》，2020年6月。'
- en: '[70] Alexandre Pasquiou, Yair Lakretz, Bertrand Thirion, and Christophe Pallier.
    Information-Restricted Neural Language Models Reveal Different Brain Regions’
    Sensitivity to Semantics, Syntax and Context, February 2023. arXiv:2302.14389
    [cs].'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Alexandre Pasquiou、Yair Lakretz、Bertrand Thirion 和 Christophe Pallier.
    信息限制的神经语言模型揭示了不同脑区对语义、句法和上下文的敏感性，2023年2月。arXiv:2302.14389 [cs]。'
- en: '[71] Khai Loong Aw and Mariya Toneva. Training language models for deeper understanding
    improves brain alignment, December 2022. arXiv:2212.10898 [cs, q-bio].'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Khai Loong Aw 和 Mariya Toneva. 训练语言模型以获得更深刻的理解提高了大脑对齐，2022年12月。arXiv:2212.10898
    [cs, q-bio]。'
- en: '[72] Sreejan Kumar, Theodore R. Sumers, Takateru Yamakoshi, Ariel Goldstein,
    Uri Hasson, Kenneth A. Norman, Thomas L. Griffiths, Robert D. Hawkins, and Samuel A.
    Nastase. Reconstructing the cascade of language processing in the brain using
    the internal computations of a transformer-based language model. Technical report,
    bioRxiv, June 2022. Section: New Results Type: article.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Sreejan Kumar、Theodore R. Sumers、Takateru Yamakoshi、Ariel Goldstein、Uri
    Hasson、Kenneth A. Norman、Thomas L. Griffiths、Robert D. Hawkins 和 Samuel A. Nastase.
    利用基于变换器的语言模型的内部计算重建大脑中的语言处理级联。技术报告，bioRxiv，2022年6月。部分：新结果 类型：文章。'
- en: '[73] Subba Reddy Oota, Manish Gupta, and Mariya Toneva. Joint processing of
    linguistic properties in brains and language models, December 2022. arXiv:2212.08094
    [cs, q-bio].'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Subba Reddy Oota、Manish Gupta 和 Mariya Toneva. 大脑和语言模型中的语言属性联合处理，2022年12月。arXiv:2212.08094
    [cs, q-bio]。'
- en: '[74] Amanda LeBel, Lauren Wagner, Shailee Jain, Aneesh Adhikari-Desai, Bhavin
    Gupta, Allyson Morgenthal, Jerry Tang, Lixiang Xu, and Alexander G Huth. A natural
    language fmri dataset for voxelwise encoding models. bioRxiv, pages 2022–09, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Amanda LeBel、Lauren Wagner、Shailee Jain、Aneesh Adhikari-Desai、Bhavin Gupta、Allyson
    Morgenthal、Jerry Tang、Lixiang Xu 和 Alexander G Huth. 用于体素编码模型的自然语言fmri数据集。bioRxiv，第2022-09页，2022年。'
- en: '[75] Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth. Semantic
    reconstruction of continuous language from non-invasive brain recordings. Nature
    Neuroscience, pages 1–9, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Jerry Tang、Amanda LeBel、Shailee Jain 和 Alexander G Huth. 从非侵入性脑记录中对连续语言进行语义重构。*自然神经科学*，第1-9页，2023年。'
- en: '[76] Fabian Pedregosa, Ga ë l Varoquaux, Alexandre Gramfort, Vincent Michel,
    Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,
    Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal
    of machine Learning research, 12(Oct):2825–2830, 2011.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Fabian Pedregosa、Gaël Varoquaux、Alexandre Gramfort、Vincent Michel、Bertrand
    Thirion、Olivier Grisel、Mathieu Blondel、Peter Prettenhofer、Ron Weiss、Vincent Dubourg
    等人. Scikit-learn：Python中的机器学习。*机器学习研究杂志*，12(10)：2825–2830，2011年。'
- en: '[77] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等人. Llama
    2：开放基础和微调聊天模型。arXiv预印本 arXiv:2307.09288，2023年。'
- en: '[78] Evelina Fedorenko, Anna A Ivanova, and Tamar I Regev. The language network
    as a natural kind within the broader landscape of the human brain. Nature Reviews
    Neuroscience, pages 1–24, 2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Evelina Fedorenko、Anna A Ivanova 和 Tamar I Regev. 语言网络作为人脑更广泛景观中的自然类别。*自然评论神经科学*，第1-24页，2024年。'
- en: '[79] Joshua B Julian, Jack Ryan, Roy H Hamilton, and Russell A Epstein. The
    occipital place area is causally involved in representing environmental boundaries
    during navigation. Current Biology, 26(8):1104–1109, 2016.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Joshua B Julian、Jack Ryan、Roy H Hamilton 和 Russell A Epstein. 枕叶位置区在导航过程中因果性地涉及到表示环境边界。*当前生物学*，26(8)：1104–1109，2016年。'
- en: '[80] Anna S Mitchell, Rafal Czajkowski, Ningyu Zhang, Kate Jeffery, and Andrew JD
    Nelson. Retrosplenial cortex and its role in spatial cognition. Brain and neuroscience
    advances, 2:2398212818757098, 2018.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Anna S Mitchell、Rafal Czajkowski、Ningyu Zhang、Kate Jeffery 和 Andrew JD
    Nelson. 回顾皮层及其在空间认知中的作用。*脑与神经科学进展*，2:2398212818757098，2018年。'
- en: '[81] Ilenia Salsano, Valerio Santangelo, and Emiliano Macaluso. The lateral
    intraparietal sulcus takes viewpoint changes into account during memory-guided
    attention in natural scenes. Brain Structure and Function, 226(4):989–1006, 2021.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Ilenia Salsano、Valerio Santangelo 和 Emiliano Macaluso. 侧坐标脑沟在自然场景中的记忆指导注意时考虑了视点变化。*脑结构与功能*，226(4)：989–1006，2021年。'
- en: '[82] Saima Malik-Moraleda, Dima Ayyash, Jeanne Gallée, Josef Affourtit, Malte
    Hoffmann, Zachary Mineroff, Olessia Jouravlev, and Evelina Fedorenko. An investigation
    across 45 languages and 12 language families reveals a universal language network.
    Nature Neuroscience, 25(8):1014–1019, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Saima Malik-Moraleda、Dima Ayyash、Jeanne Gallée、Josef Affourtit、Malte Hoffmann、Zachary
    Mineroff、Olessia Jouravlev 和 Evelina Fedorenko. 跨越45种语言和12种语言族的调查揭示了一个普遍的语言网络。《自然神经科学》，25(8):1014–1019，2022年。'
- en: '[83] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
    optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Yinhan Liu、Myle Ott、Naman Goyal、Jingfei Du、Mandar Joshi、Danqi Chen、Omer
    Levy、Mike Lewis、Luke Zettlemoyer 和 Veselin Stoyanov. Roberta: 一种稳健优化的 BERT 预训练方法。arXiv
    预印本 arXiv:1907.11692，2019年。'
- en: '[84] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.
    arXiv preprint arXiv:1711.05101, 2017.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Ilya Loshchilov 和 Frank Hutter. 解耦权重衰减正则化。arXiv 预印本 arXiv:1711.05101，2017年。'
- en: '[85] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language
    models for zero-shot learning by meta-tuning on dataset and prompt collections.
    arXiv preprint arXiv:2104.04670, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Ruiqi Zhong、Kristy Lee、Zheng Zhang 和 Dan Klein. 通过在数据集和提示集合上进行元调优来适应语言模型进行零样本学习。arXiv
    预印本 arXiv:2104.04670，2021年。'
- en: '[86] Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing
    differences between text distributions with natural language. In International
    Conference on Machine Learning, pages 27099–27116\. PMLR, 2022.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Ruiqi Zhong、Charlie Snell、Dan Klein 和 Jacob Steinhardt. 使用自然语言描述文本分布之间的差异。在国际机器学习大会上，页码
    27099–27116。PMLR，2022年。'
- en: '[87] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
    Majumder, and Li Deng. Ms marco: A human-generated machine reading comprehension
    dataset. 2016.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Tri Nguyen、Mir Rosenberg、Xia Song、Jianfeng Gao、Saurabh Tiwary、Rangan Majumder
    和 Li Deng. Ms marco: 一个人工生成的机器阅读理解数据集。2016年。'
- en: '[88] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt
    or bad debt: Detecting semantic orientations in economic texts. Journal of the
    Association for Information Science and Technology, 65, 2014.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] P. Malo、A. Sinha、P. Korhonen、J. Wallenius 和 P. Takala. 好债务还是坏债务：检测经济文本中的语义取向。信息科学与技术协会杂志，65，2014年。'
- en: '[89] Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin
    Chen. CARER: Contextualized affect representations for emotion recognition. In
    Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
    pages 3687–3697, Brussels, Belgium, October-November 2018\. Association for Computational
    Linguistics.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Elvis Saravia、Hsien-Chi Toby Liu、Yen-Hao Huang、Junlin Wu 和 Yi-Shin Chen.
    CARER: 用于情感识别的上下文化情感表示。在2018年自然语言处理实证方法会议上，页码 3687–3697，比利时布鲁塞尔，2018年10月-11月。计算语言学协会。'
- en: '[90] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional
    networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama,
    and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28\.
    Curran Associates, Inc., 2015.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Xiang Zhang、Junbo Zhao 和 Yann LeCun. 用于文本分类的字符级卷积网络。在 C. Cortes、N. Lawrence、D.
    Lee、M. Sugiyama 和 R. Garnett 主编的《神经信息处理系统进展》中，第 28 卷。Curran Associates, Inc.，2015年。'
- en: '[91] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships
    for sentiment categorization with respect to rating scales. In Proceedings of
    the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),
    pages 115–124, Ann Arbor, Michigan, June 2005\. Association for Computational
    Linguistics.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Bo Pang 和 Lillian Lee. 看到星星：利用类别关系进行情感分类的评级尺度。于 2005 年 43 届计算语言学协会年会（ACL’05）上，页码
    115–124，美国密歇根州安娜堡。计算语言学协会。'
- en: '[92] Anton Korinek. Language models and cognitive automation for economic research.
    Technical report, National Bureau of Economic Research, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Anton Korinek. 语言模型和认知自动化用于经济研究。技术报告，国家经济研究局，2023年。'
- en: '[93] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie,
    and Christopher D Manning. Raptor: Recursive abstractive processing for tree-organized
    retrieval. arXiv preprint arXiv:2401.18059, 2024.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Parth Sarthi、Salman Abdullah、Aditi Tuli、Shubh Khanna、Anna Goldie 和 Christopher
    D Manning. Raptor: 用于树状检索的递归抽象处理。arXiv 预印本 arXiv:2401.18059，2024年。'
- en: '[94] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva
    Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach
    to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Darren Edge、Ha Trinh、Newman Cheng、Joshua Bradley、Alex Chao、Apurva Mody、Steven
    Truitt 和 Jonathan Larson. 从局部到全球：一种图形 RAG 方法进行查询聚焦的摘要。arXiv 预印本 arXiv:2404.16130，2024年。'
- en: '[95] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt.
    Goal driven discovery of distributional differences via language descriptions.
    arXiv preprint arXiv:2302.14233, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, 和 Jacob Steinhardt.
    通过语言描述驱动的目标发现分布差异。arXiv 预印本 arXiv:2302.14233, 2023.'
- en: '[96] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou,
    and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409,
    2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou,
    和 Xinyun Chen. 大型语言模型作为优化器。arXiv 预印本 arXiv:2309.03409, 2023.'
- en: '[97] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang,
    Nebojsa Jojic, Eric P Xing, and Zhiting Hu. Promptagent: Strategic planning with
    language models enables expert-level prompt optimization. arXiv preprint arXiv:2310.16427,
    2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang,
    Nebojsa Jojic, Eric P Xing, 和 Zhiting Hu. Promptagent：与语言模型的战略规划使专家级提示优化成为可能。arXiv
    预印本 arXiv:2310.16427, 2023.'
- en: '[98] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar,
    Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020,
    2024.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar,
    Jing Xu, 和 Jason Weston. 自奖励语言模型。arXiv 预印本 arXiv:2401.10020, 2024.'
- en: '[99] Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, and Jacob
    Andreas. Deductive closure training of language models for coherence, accuracy,
    and updatability. arXiv preprint arXiv:2401.08574, 2024.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, 和 Jacob
    Andreas. 为语言模型的连贯性、准确性和可更新性进行演绎闭包训练。arXiv 预印本 arXiv:2401.08574, 2024.'
- en: '[100] Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He, and
    Jianfeng Gao. Towards consistent natural-language explanations via explanation-consistency
    finetuning. arXiv preprint arXiv:2401.13986, 2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He, 和
    Jianfeng Gao. 通过解释一致性微调实现一致的自然语言解释。arXiv 预印本 arXiv:2401.13986, 2024.'
- en: '[101] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980, 2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Diederik P Kingma 和 Jimmy Ba. Adam：一种随机优化方法。arXiv 预印本 arXiv:1412.6980,
    2014.'
- en: Appendix A Appendix
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 fMRI question details
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 fMRI 问题细节
- en: 'Table A1: Questions list for model with 29 questions. Importance denotes the
    average absolute coefficient for each question (normalized by the importance of
    the top question).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A1：模型问题列表，共29个问题。重要性表示每个问题的平均绝对系数（按顶级问题的重要性归一化）。
- en: '| Question | Importance |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 重要性 |'
- en: '| Is the sentence expressing skepticism or disbelief towards something or someone?
    | 1.000 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否对某事或某人表达了怀疑或不信任？ | 1.000 |'
- en: '| Does the sentence include dialogue? | 0.983 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否包含对话？ | 0.983 |'
- en: '| Does the sentence describe a relationship between people? | 0.924 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否描述了人与人之间的关系？ | 0.924 |'
- en: '| Does the sentence involve the mention of a specific object or item? | 0.900
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否涉及提到特定的物体或项目？ | 0.900 |'
- en: '| Does the sentence include technical or specialized terminology? | 0.882 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否包含技术或专业术语？ | 0.882 |'
- en: '| Does the sentence contain a proper noun? | 0.861 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否包含专有名词？ | 0.861 |'
- en: '| Does the input involve planning or organizing? | 0.861 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 输入是否涉及计划或组织？ | 0.861 |'
- en: '| Does the sentence include numerical information? | 0.850 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否包含数字信息？ | 0.850 |'
- en: '| Is time mentioned in the input? | 0.844 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 输入中是否提到了时间？ | 0.844 |'
- en: '| Is the sentence grammatically complex? | 0.815 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否在语法上复杂？ | 0.815 |'
- en: '| Does the sentence include dialogue or thoughts directed towards another character?
    | 0.811 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否包含针对另一个角色的对话或思想？ | 0.811 |'
- en: '| Does the sentence describe a physical action? | 0.809 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否描述了身体动作？ | 0.809 |'
- en: '| Does the sentence include a conditional clause? | 0.782 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否包含条件从句？ | 0.782 |'
- en: '| Does the sentence describe a visual experience or scene? | 0.771 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否描述了视觉体验或场景？ | 0.771 |'
- en: '| Does the input include a philosophical or reflective thought? | 0.759 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 输入是否包含哲学或反思性思考？ | 0.759 |'
- en: '| Is the sentence conveying the narrator’s physical movement or action in detail?
    | 0.749 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否详细描述了叙述者的身体移动或动作？ | 0.749 |'
- en: '| Does the sentence describe a physical sensation? | 0.744 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否描述了身体感受？ | 0.744 |'
- en: '| Does the sentence involve a discussion about personal or social values? |
    0.739 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否涉及讨论个人或社会价值观？ | 0.739 |'
- en: '| Does the sentence reference a specific time or date? | 0.719 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否提到具体的时间或日期？ | 0.719 |'
- en: '| Does the sentence express a philosophical or existential query or observation?
    | 0.705 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 句子是否表达了哲学或存在性的问题或观察？ | 0.705 |'
- en: '| Does the sentence involve a description of physical environment or setting?
    | 0.693 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 该句子是否涉及对物理环境或设置的描述？ | 0.693 |'
- en: '| Does the input describe a sensory experience? | 0.688 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 该输入是否描述了感官体验？ | 0.688 |'
- en: '| Does the sentence involve planning or decision-making? | 0.684 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 该句子是否涉及计划或决策？ | 0.684 |'
- en: '| Is the sentence a command? | 0.682 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 该句子是否是命令？ | 0.682 |'
- en: '| Does the sentence describe a specific sensation or feeling? | 0.672 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 该句子是否描述了特定的感觉或情感？ | 0.672 |'
- en: '| Does the sentence contain a cultural reference? | 0.667 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 该句子是否包含文化参考？ | 0.667 |'
- en: '| Does the input include dialogue between characters? | 0.594 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 该输入是否包含角色之间的对话？ | 0.594 |'
- en: '| Does the sentence mention a specific location or place? | 0.547 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 该句子是否提及特定地点或位置？ | 0.547 |'
- en: '| Does the sentence reference a specific location or place? | 0.545 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 该句子是否提及特定地点或位置？ | 0.545 |'
- en: A.2 fMRI prediction results extended
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 fMRI 预测结果扩展
- en: \begin{overpic}[width=433.62pt,trim=0.0pt 0.0pt 284.52756pt 0.0pt,clip]{figs/flatmaps/S01_qa_flatmap.pdf}
    \put(45.0,50.0){{S01}} \end{overpic}![Refer to caption](img/27ff14a25a717cf56107c25bc2c2788b.png)  \begin{overpic}[width=433.62pt]{figs/flatmaps/S02_qa_flatmap.pdf}
    \put(45.0,50.0){{S02}} \end{overpic}![Refer to caption](img/c018af15732ad01ddeafc3391b8aa7b1.png)  \begin{overpic}[width=433.62pt]{figs/flatmaps/S03_qa_flatmap.pdf}
    \put(45.0,50.0){{S03}} \end{overpic}![Refer to caption](img/ac788dd9972f6930fac08c158fee5dba.png)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{overpic}[width=433.62pt,trim=0.0pt 0.0pt 284.52756pt 0.0pt,clip]{figs/flatmaps/S01_qa_flatmap.pdf}
    \put(45.0,50.0){{S01}} \end{overpic}![参见图注](img/27ff14a25a717cf56107c25bc2c2788b.png)  \begin{overpic}[width=433.62pt]{figs/flatmaps/S02_qa_flatmap.pdf}
    \put(45.0,50.0){{S02}} \end{overpic}![参见图注](img/c018af15732ad01ddeafc3391b8aa7b1.png)  \begin{overpic}[width=433.62pt]{figs/flatmaps/S03_qa_flatmap.pdf}
    \put(45.0,50.0){{S03}} \end{overpic}![参见图注](img/ac788dd9972f6930fac08c158fee5dba.png)
- en: \begin{overpic}[width=433.62pt,trim=0.0pt 0.0pt 284.52756pt 0.0pt,clip]{figs/flatmaps/S01_qa-bert_flatmap.pdf}
    \put(45.0,50.0){{S01}} \end{overpic}![Refer to caption](img/df248e161c85abe7ae8d6d4c7902b4e4.png)  \begin{overpic}[width=433.62pt]{figs/flatmaps/S02_qa-bert_flatmap.pdf}
    \put(45.0,50.0){{S02}} \end{overpic}![Refer to caption](img/38b141140e5f7177180f04648ab6f891.png)  \begin{overpic}[width=433.62pt]{figs/flatmaps/S03_qa-bert_flatmap.pdf}
    \put(45.0,50.0){{S03}} \end{overpic}![Refer to caption](img/691a0137d967d2d2b798813bd862f771.png)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \begin{overpic}[width=433.62pt,trim=0.0pt 0.0pt 284.52756pt 0.0pt,clip]{figs/flatmaps/S01_qa-bert_flatmap.pdf}
    \put(45.0,50.0){{S01}} \end{overpic}![参见图注](img/df248e161c85abe7ae8d6d4c7902b4e4.png)  \begin{overpic}[width=433.62pt]{figs/flatmaps/S02_qa-bert_flatmap.pdf}
    \put(45.0,50.0){{S02}} \end{overpic}![参见图注](img/38b141140e5f7177180f04648ab6f891.png)  \begin{overpic}[width=433.62pt]{figs/flatmaps/S03_qa-bert_flatmap.pdf}
    \put(45.0,50.0){{S03}} \end{overpic}![参见图注](img/691a0137d967d2d2b798813bd862f771.png)
- en: 'Figure A1: Predictive performance for QA-Emb (top row) and the difference between
    QA-Emb and BERT (bottom row).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A1：QA-Emb（顶部行）的预测性能，以及 QA-Emb 和 BERT 之间的差异（底部行）。
- en: 'Table A2: Mean test correlation for QA-Emb with different settings: varying
    the underlying prompts to source questions and the LLM used to answer the questions
    (fixing the number of time-lagged delays to 8). Ensemble generally provides a
    small boost over other models and Mistral slightly underperforms LLaMA-3 (8B).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A2：不同设置下 QA-Emb 的均值测试相关性：变更基础提示以获取问题来源和用于回答问题的 LLM（将时间滞后的数量固定为 8）。集成通常比其他模型提供了小幅提升，Mistral
    的表现略低于 LLaMA-3（8B）。
- en: Ensemble LLaMA-3 (8B) LLaMA-3 (8B)-fewshot Mistral (7B) Subject Questions S01
    Prompts 1-3 (376 questions) 0.081 0.078 0.078 0.076 Prompts 1-5 (518 questions)
    0.089 0.085 0.085 0.082 Prompts 1-6 (674 questions) 0.084 0.081 0.085 0.076 S02
    Prompts 1-3 (376 questions) 0.120 0.112 0.119 0.112 Prompts 1-5 (518 questions)
    0.118 0.120 0.121 0.114 Prompts 1-6 (674 questions) 0.124 0.119 0.121 0.108 S03
    Prompts 1-3 (376 questions) 0.132 0.131 0.127 0.126 Prompts 1-5 (518 questions)
    0.137 0.136 0.135 0.129 Prompts 1-6 (674 questions) 0.141 0.136 0.136 0.132 AVG
    Prompts 1-3 (376 questions) 0.111 0.107 0.108 0.104 Prompts 1-5 (518 questions)
    0.115 0.114 0.114 0.108 Prompts 1-6 (674 questions) 0.116 0.112 0.114 0.105
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 集成 LLaMA-3（8B） LLaMA-3（8B）-fewshot Mistral（7B） 主题 问题 S01 提示 1-3（376 问题） 0.081
    0.078 0.078 0.076 提示 1-5（518 问题） 0.089 0.085 0.085 0.082 提示 1-6（674 问题） 0.084
    0.081 0.085 0.076 S02 提示 1-3（376 问题） 0.120 0.112 0.119 0.112 提示 1-5（518 问题） 0.118
    0.120 0.121 0.114 提示 1-6（674 问题） 0.124 0.119 0.121 0.108 S03 提示 1-3（376 问题） 0.132
    0.131 0.127 0.126 提示 1-5（518 问题） 0.137 0.136 0.135 0.129 提示 1-6（674 问题） 0.141
    0.136 0.136 0.132 平均值 提示 1-3（376 问题） 0.111 0.107 0.108 0.104 提示 1-5（518 问题） 0.115
    0.114 0.114 0.108 提示 1-6（674 问题） 0.116 0.112 0.114 0.105
- en: 'Table A3: Mean test correlation for different baseline models as a function
    of hyperparameters (number of time-lagged delays and layer for extracting embeddings)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A3：不同基线模型的平均测试相关性，作为超参数（时间延迟数和提取嵌入的层数）的函数
- en: Subject S01 S02 S03 AVG Delays 4 8 12 4 8 12 4 8 12 4 8 12 BERT 0.084 0.080
    0.075 0.114 0.108 0.107 0.136 0.139 0.136 0.111 0.109 0.106 Eng1000 0.079 0.067
    0.077 0.096 0.092 0.082 0.110 0.117 0.116 0.095 0.092 0.092 LLaMA-2 (70B) (lay
    12) 0.055 0.055 0.054 0.101 0.095 0.085 0.143 0.144 0.130 0.100 0.098 0.089 LLaMA-2
    (70B) (lay 24) 0.075 0.059 0.049 0.097 0.104 0.092 0.149 0.153 0.152 0.107 0.105
    0.098 LLaMA-2 (70B) (lay 36) 0.058 0.068 0.057 0.131 0.101 0.084 0.153 0.156 0.152
    0.114 0.108 0.098 LLaMA-2 (70B) (lay 48) 0.093 0.060 0.052 0.114 0.094 0.091 0.148
    0.151 0.149 0.118 0.102 0.098 LLaMA-2 (70B) (lay 60) 0.095 0.048 0.050 0.119 0.089
    0.088 0.148 0.152 0.150 0.121 0.097 0.096 LLaMA-2 (7B) (lay 06) 0.074 0.067 0.039
    0.120 0.088 0.084 0.138 0.144 0.133 0.111 0.100 0.085 LLaMA-2 (7B) (lay 12) 0.097
    0.058 0.053 0.116 0.111 0.087 0.150 0.155 0.152 0.121 0.108 0.097 LLaMA-2 (7B)
    (lay 18) 0.079 0.076 0.042 0.123 0.103 0.090 0.143 0.153 0.150 0.115 0.111 0.094
    LLaMA-2 (7B) (lay 24) 0.088 0.057 0.068 0.129 0.100 0.106 0.144 0.148 0.149 0.120
    0.102 0.108 LLaMA-2 (7B) (lay 30) 0.057 0.045 0.045 0.130 0.098 0.099 0.139 0.149
    0.148 0.109 0.097 0.097 LLaMA-3 (8B) (lay 06) 0.071 0.066 0.054 0.122 0.119 0.095
    0.144 0.147 0.148 0.112 0.111 0.099 LLaMA-3 (8B) (lay 12) 0.089 0.073 0.050 0.110
    0.099 0.095 0.146 0.151 0.153 0.115 0.108 0.099 LLaMA-3 (8B) (lay 18) 0.073 0.052
    0.052 0.125 0.102 0.096 0.153 0.154 0.155 0.117 0.103 0.101 LLaMA-3 (8B) (lay
    24) 0.090 0.053 0.047 0.106 0.113 0.095 0.146 0.149 0.148 0.114 0.105 0.097 LLaMA-3
    (8B) (lay 30) 0.082 0.066 0.060 0.120 0.117 0.101 0.147 0.151 0.148 0.117 0.111
    0.103
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 主题 S01 S02 S03 AVG 延迟 4 8 12 4 8 12 4 8 12 4 8 12 BERT 0.084 0.080 0.075 0.114
    0.108 0.107 0.136 0.139 0.136 0.111 0.109 0.106 Eng1000 0.079 0.067 0.077 0.096
    0.092 0.082 0.110 0.117 0.116 0.095 0.092 0.092 LLaMA-2 (70B) (lay 12) 0.055 0.055
    0.054 0.101 0.095 0.085 0.143 0.144 0.130 0.100 0.098 0.089 LLaMA-2 (70B) (lay
    24) 0.075 0.059 0.049 0.097 0.104 0.092 0.149 0.153 0.152 0.107 0.105 0.098 LLaMA-2
    (70B) (lay 36) 0.058 0.068 0.057 0.131 0.101 0.084 0.153 0.156 0.152 0.114 0.108
    0.098 LLaMA-2 (70B) (lay 48) 0.093 0.060 0.052 0.114 0.094 0.091 0.148 0.151 0.149
    0.118 0.102 0.098 LLaMA-2 (70B) (lay 60) 0.095 0.048 0.050 0.119 0.089 0.088 0.148
    0.152 0.150 0.121 0.097 0.096 LLaMA-2 (7B) (lay 06) 0.074 0.067 0.039 0.120 0.088
    0.084 0.138 0.144 0.133 0.111 0.100 0.085 LLaMA-2 (7B) (lay 12) 0.097 0.058 0.053
    0.116 0.111 0.087 0.150 0.155 0.152 0.121 0.108 0.097 LLaMA-2 (7B) (lay 18) 0.079
    0.076 0.042 0.123 0.103 0.090 0.143 0.153 0.150 0.115 0.111 0.094 LLaMA-2 (7B)
    (lay 24) 0.088 0.057 0.068 0.129 0.100 0.106 0.144 0.148 0.149 0.120 0.102 0.108
    LLaMA-2 (7B) (lay 30) 0.057 0.045 0.045 0.130 0.098 0.099 0.139 0.149 0.148 0.109
    0.097 0.097 LLaMA-3 (8B) (lay 06) 0.071 0.066 0.054 0.122 0.119 0.095 0.144 0.147
    0.148 0.112 0.111 0.099 LLaMA-3 (8B) (lay 12) 0.089 0.073 0.050 0.110 0.099 0.095
    0.146 0.151 0.153 0.115 0.108 0.099 LLaMA-3 (8B) (lay 18) 0.073 0.052 0.052 0.125
    0.102 0.096 0.153 0.154 0.155 0.117 0.103 0.101 LLaMA-3 (8B) (lay 24) 0.090 0.053
    0.047 0.106 0.113 0.095 0.146 0.149 0.148 0.114 0.105 0.097 LLaMA-3 (8B) (lay
    30) 0.082 0.066 0.060 0.120 0.117 0.101 0.147 0.151 0.148 0.117 0.111 0.103
- en: A.3 Prompts
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 提示
- en: A.3.1 Prompts for question generation
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.1 问题生成提示
- en: Prompt 1
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示 1
- en: Generate a bulleted list of 500 diverse, non-overlapping questions that can
    be used to classify an input based on its semantic properties. Phrase the questions
    in diverse ways.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个包含 500 个多样化、不重叠的问题的项目列表，这些问题可以用于根据其语义属性对输入进行分类。以多种方式表述这些问题。
- en: 'Here are some example questions: {{examples}}'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些示例问题：{{examples}}
- en: Return only a bulleted list of questions and nothing else
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 仅返回一个问题的项目列表，不要其他内容
- en: Prompt 2
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示 2
- en: Generate a bulleted list of 100 diverse, non-overlapping questions that can
    be used to classify sentences from a first-person story. Phrase the questions
    in diverse ways.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个包含 100 个多样化、不重叠的问题的项目列表，这些问题可以用于根据第一人称故事中的句子进行分类。以多种方式表述这些问题。
- en: 'Here are some example questions: {{examples}}'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些示例问题：{{examples}}
- en: Return only a bulleted list of questions and nothing else
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 仅返回一个问题的项目列表，不要其他内容
- en: Prompt 3
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示 3
- en: Generate a bulleted list of 200 diverse, non-overlapping questions that can
    be used to classify sentences from a first-person story. Phrase the questions
    in diverse ways.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个包含 200 个多样化、不重叠的问题的项目列表，这些问题可以用于根据第一人称故事中的句子进行分类。以多种方式表述这些问题。
- en: 'Here are some example questions: {{examples}}'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些示例问题：{{examples}}
- en: Return only a bulleted list of questions and nothing else
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 仅返回一个问题的项目列表，不要其他内容
- en: Prompt 4
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示 4
- en: Based on what you know from the neuroscience and psychology literature, generate
    a bulleted list of 100 diverse, non-overlapping yes/no questions that ask about
    properties of a sentence that might be important for predicting brain activity.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你对神经科学和心理学文献的了解，生成一个包含 100 个多样化、不重叠的“是/否”问题的项目列表，这些问题询问句子的属性，这些属性可能对预测大脑活动很重要。
- en: Return only a bulleted list of questions and nothing else
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 仅返回一个项目符号列表，其他内容不要返回
- en: Prompt 5
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示 5
- en: Example narrative sentences {{example sentences from dataset}}
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例叙述句 {{数据集中的示例句子}}
- en: Example yes/no questions {{example questions already asked}}
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例“是/否”问题 {{已经问过的示例问题}}
- en: Generate a bulleted list of 100 specific, non-overlapping yes/no questions that
    ask about aspects of the example narrative sentences that are important for classifying
    them. Focus on the given narrative sentences and form questions that combine shared
    properties from multiple sentences above. Do not repeat information in the example
    questions that are already given above. Instead, generate complementary questions
    that are not covered by the example questions. Return only a bulleted list of
    questions and nothing else.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一个包含 100 个具体且不重叠的“是/否”问题的项目符号列表，这些问题询问有关示例叙述句的方面，这些方面对对其进行分类很重要。关注给定的叙述句，并形成结合多个句子共享属性的问题。不要重复示例问题中已经给出的信息。相反，生成未在示例问题中涵盖的补充问题。仅返回一个项目符号列表，其他内容不要返回。
- en: Prompt 6
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示 6
- en: Generate more diverse questions that may occur for a single sentence in a first-person
    narrative story
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 生成更多可能出现在第一人称叙事故事中的多样化问题
- en: See exact prompts with examples in the Github repo.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 GitHub 仓库中的具体提示和示例。
- en: A.3.2 Prompts for question answering
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.2 问题回答提示
- en: 'Standard prompt : Input text: {example}'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '标准提示 : 输入文本：{example}'
- en: 'Question: {question}'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：{问题}
- en: Answer with yes or no, then give an explanation.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 答案请以“是”或“否”作答，然后给出解释。
- en: 'Few-shot prompt : You are a concise, helpful assistant.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '少量提示 : 你是一个简洁、实用的助手。'
- en: ': Input text: and i just kept on laughing because it was so'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ': 输入文本：而且我只是一直在笑，因为太'
- en: 'Question: Does the input mention laughter?'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：输入文本中提到了笑声吗？
- en: Answer with Yes or No.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 答案请以“是”或“否”作答。
- en: ': Yes'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ': 是'
- en: ' Input text: what a crazy day things just kept on happening'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh:  输入文本：真是疯狂的一天，事情一直在发生
- en: 'Question: Is the sentence related to food preparation?'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：句子是否与食物准备有关？
- en: Answer with Yes or No.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 答案请以“是”或“否”作答。
- en: ': No'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ': 不'
- en: ' Input text: i felt like a fly on the wall just waiting for'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh:  输入文本：我感觉自己像墙上的一只苍蝇，只是在等待
- en: 'Question: Does the text use a metaphor or figurative language?'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：文本中是否使用了隐喻或比喻语言？
- en: Answer with Yes or No.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 答案请以“是”或“否”作答。
- en: ': Yes'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ': 是'
- en: ' Input text: he takes too long in there getting the pans from'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh:  输入文本：他在那儿拿锅花了太长时间
- en: 'Question: Is there a reference to sports?'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：是否提到了体育？
- en: Answer with Yes or No.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 答案请以“是”或“否”作答。
- en: Answer with Yes or No.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 答案请以“是”或“否”作答。
- en: ': No'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ': 不'
- en: ' Input text: was silent and lovely and there was no sound except'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh:  输入文本：一片寂静而美丽，除了
- en: 'Question: Is the sentence expressing confusion or uncertainty?'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：句子是否表达了困惑或不确定性？
- en: Answer with Yes or No.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 答案请以“是”或“否”作答。
- en: ': No'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ': 不'
- en: ' Input text: {example}'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh:  输入文本：{example}
- en: 'Question: {question}'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：{问题}
- en: Answer with Yes or No.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 答案请以“是”或“否”作答。
- en: ':'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ：
- en: See exact prompts with examples in the Github repo.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 GitHub 仓库中的具体提示和示例。
- en: A.4 Information retrieval details
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 信息检索详细信息
- en: Optimization details
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优化细节
- en: When fitting our QA-Emb model for information retrieval, we learn a single scalar
    per-question that is multiplied by each embedding before computing a similarity.
    To learn these scalars, we minimize a two-part loss. The first loss is the negative
    cosine similarity between each query and its similar documents. The second loss
    is the cosine similarity between each query and the remaining documents. We weight
    the first loss as 10 times higher than the second loss and optimize using Adam [[101](#bib.bib101)]
    with a learning rate of $10^{-4}$. We run for 8 epochs, when the training loss
    seems to plateau.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在为信息检索调整我们的 QA-Emb 模型时，我们为每个问题学习一个单一的标量，该标量在计算相似度之前乘以每个嵌入。为了学习这些标量，我们最小化一个双重损失。第一个损失是每个查询与其相似文档之间的负余弦相似度。第二个损失是每个查询与其余文档之间的余弦相似度。我们将第一个损失的权重设置为第二个损失的
    10 倍，并使用 Adam [[101](#bib.bib101)] 以 $10^{-4}$ 的学习率进行优化。我们运行 8 个周期，当训练损失似乎稳定时停止。
- en: A.5 Details on question-answering evaluation datasets
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 问题回答评估数据集详细信息
- en: 'Table A4: 54 binary classification datasets along with their underlying yes/no
    question and corpus statistics from a recent collection [[85](#bib.bib85), [86](#bib.bib86)].'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A4：54 个二元分类数据集及其底层的“是/否”问题和最新集合中的语料统计 [[85](#bib.bib85), [86](#bib.bib86)]。
- en: '| Dataset name | Dataset topic | Underlying yes/no question | Examples | Unique
    unigrams |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | 数据集主题 | 基础是是/否问题 | 示例 | 唯一单词 |'
- en: '| 0-irony | sarcasm | contains irony | 590 | 3897 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 0-讽刺 | 讽刺 | 包含讽刺 | 590 | 3897 |'
- en: '| 1-objective | unbiased | is a more objective description of what happened
    | 739 | 5628 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 1-客观 | 客观的 | 更客观地描述发生了什么 | 739 | 5628 |'
- en: '| 2-subjective | subjective | contains subjective opinion | 757 | 5769 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 2-主观 | 主观的 | 包含主观意见 | 757 | 5769 |'
- en: '| 3-god | religious | believes in god | 164 | 1455 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 3-上帝 | 宗教的 | 相信上帝 | 164 | 1455 |'
- en: '| 4-atheism | atheistic | is against religion | 172 | 1472 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 4-无神论 | 无神论的 | 反对宗教 | 172 | 1472 |'
- en: '| 5-evacuate | evacuation | involves a need for people to evacuate | 2670 |
    16505 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 5-疏散 | 疏散 | 涉及人员疏散的需要 | 2670 | 16505 |'
- en: '| 6-terorrism | terrorism | describes a situation that involves terrorism |
    2640 | 16608 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 6-恐怖主义 | 恐怖主义 | 描述涉及恐怖主义的情况 | 2640 | 16608 |'
- en: '| 7-crime | crime | involves crime | 2621 | 16333 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 7-犯罪 | 犯罪 | 涉及犯罪 | 2621 | 16333 |'
- en: '| 8-shelter | shelter | describes a situation where people need shelter | 2620
    | 16347 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 8-避难所 | 避难所 | 描述人们需要避难所的情况 | 2620 | 16347 |'
- en: '| 9-food | hunger | is related to food security | 2642 | 16276 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 9-食物 | 饥饿 | 与食品安全相关 | 2642 | 16276 |'
- en: '| 10-infrastructure | infrastructure | is related to infrastructure | 2664
    | 16548 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 10-基础设施 | 基础设施 | 与基础设施相关 | 2664 | 16548 |'
- en: '| 11-regime change | regime change | describes a regime change | 2670 | 16382
    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 11-政权更迭 | 政权更迭 | 描述政权更迭 | 2670 | 16382 |'
- en: '| 12-medical | health | is related to a medical situation | 2675 | 16223 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 12-医疗 | 健康 | 与医疗情况相关 | 2675 | 16223 |'
- en: '| 13-water | water | involves a situation where people need clean water | 2619
    | 16135 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 13-水 | 水 | 涉及人们需要清洁水的情况 | 2619 | 16135 |'
- en: '| 14-search | rescue | involves a search/rescue situation | 2628 | 16131 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 14-搜索 | 救援 | 涉及搜索/救援情况 | 2628 | 16131 |'
- en: '| 15-utility | utility | expresses need for utility, energy or sanitation |
    2640 | 16249 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 15-实用 | 实用 | 表达对实用、能源或卫生的需求 | 2640 | 16249 |'
- en: '| 16-hillary | Hillary | is against Hillary | 224 | 1693 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 16-希拉里 | 希拉里 | 反对希拉里 | 224 | 1693 |'
- en: '| 17-hillary | Hillary | supports hillary | 218 | 1675 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 17-希拉里 | 希拉里 | 支持希拉里 | 218 | 1675 |'
- en: '| 18-offensive | derogatory | contains offensive content | 652 | 6109 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 18-冒犯 | 贬低的 | 包含冒犯内容 | 652 | 6109 |'
- en: '| 19-offensive | toxic | insult women or immigrants | 2188 | 11839 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 19-冒犯 | 有毒 | 侮辱女性或移民 | 2188 | 11839 |'
- en: '| 20-pro-life | pro-life | is pro-life | 213 | 1633 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 20-反堕胎 | 反堕胎 | 反对堕胎 | 213 | 1633 |'
- en: '| 21-pro-choice | abortion | supports abortion | 209 | 1593 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 21-支持选择权 | 堕胎 | 支持堕胎 | 209 | 1593 |'
- en: '| 22-physics | physics | is about physics | 10360 | 93810 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 22-物理学 | 物理学 | 关乎物理学 | 10360 | 93810 |'
- en: '| 23-computer science | computers | is related to computer science | 10441
    | 93947 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 23-计算机科学 | 计算机 | 与计算机科学相关 | 10441 | 93947 |'
- en: '| 24-statistics | statistics | is about statistics | 9286 | 86874 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 24-统计学 | 统计学 | 关乎统计学 | 9286 | 86874 |'
- en: '| 25-math | math | is about math research | 8898 | 85118 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 25-数学 | 数学 | 关乎数学研究 | 8898 | 85118 |'
- en: '| 26-grammar | ungrammatical | is ungrammatical | 834 | 2217 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 26-语法 | 语法错误的 | 语法错误 | 834 | 2217 |'
- en: '| 27-grammar | grammatical | is grammatical | 826 | 2236 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 27-语法 | 语法的 | 符合语法 | 826 | 2236 |'
- en: '| 28-sexis | sexist | is offensive to women | 209 | 1641 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 28-性别歧视 | 性别歧视的 | 冒犯女性 | 209 | 1641 |'
- en: '| 29-sexis | feminism | supports feminism | 215 | 1710 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 29-性别平等 | 女性主义 | 支持女性主义 | 215 | 1710 |'
- en: '| 30-news | world | is about world news | 5778 | 13023 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 30-新闻 | 世界 | 关乎世界新闻 | 5778 | 13023 |'
- en: '| 31-sports | sports news | is about sports news | 5674 | 12849 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 31-体育 | 体育新闻 | 关乎体育新闻 | 5674 | 12849 |'
- en: '| 32-business | business | is related to business | 5699 | 12913 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 32-商业 | 商业 | 与商业相关 | 5699 | 12913 |'
- en: '| 33-tech | technology | is related to technology | 5727 | 12927 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 33-科技 | 科技 | 与科技相关 | 5727 | 12927 |'
- en: '| 34-bad | negative | contains a bad movie review | 357 | 16889 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 34-差 | 消极的 | 包含差评的电影评论 | 357 | 16889 |'
- en: '| 35-good | good | thinks the movie is good | 380 | 17497 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 35-好 | 好 | 认为电影好 | 380 | 17497 |'
- en: '| 36-quantity | quantity | asks for a quantity | 1901 | 5144 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 36-数量 | 数量 | 询问数量 | 1901 | 5144 |'
- en: '| 37-location | location | asks about a location | 1925 | 5236 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 37-位置 | 位置 | 询问有关位置 | 1925 | 5236 |'
- en: '| 38-person | person | asks about a person | 1848 | 5014 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 38-人物 | 人物 | 询问有关某个人 | 1848 | 5014 |'
- en: '| 39-entity | entity | asks about an entity | 1896 | 5180 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 39-实体 | 实体 | 询问有关某个实体 | 1896 | 5180 |'
- en: '| 40-abbrevation | abbreviation | asks about an abbreviation | 1839 | 5045
    |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 40-缩写 | 缩写 | 询问有关缩写 | 1839 | 5045 |'
- en: '| 41-defin | definition | contains a definition | 651 | 4508 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 41-定义 | 定义 | 包含定义 | 651 | 4508 |'
- en: '| 42-environment | environmentalism | is against environmentalist | 124 | 1117
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 42-environment | 环保主义 | 反对环保主义者 | 124 | 1117 |'
- en: '| 43-environment | environmentalism | is environmentalist | 119 | 1072 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 43-environment | 环保主义 | 是环保主义者 | 119 | 1072 |'
- en: '| 44-spam | spam | is a spam | 360 | 2470 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 44-spam | 垃圾邮件 | 是垃圾邮件 | 360 | 2470 |'
- en: '| 45-fact | facts | asks for factual information | 704 | 11449 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 45-fact | 事实 | 询问事实信息 | 704 | 11449 |'
- en: '| 46-opinion | opinion | asks for an opinion | 719 | 11709 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 46-opinion | 意见 | 询问意见 | 719 | 11709 |'
- en: '| 47-math | science | is related to math and science | 7514 | 53973 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 47-math | 数学 | 与数学和科学相关 | 7514 | 53973 |'
- en: '| 48-health | health | is related to health | 7485 | 53986 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 48-health | 健康 | 与健康相关 | 7485 | 53986 |'
- en: '| 49-computer | computers | related to computer or internet | 7486 | 54256
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 49-computer | 计算机 | 与计算机或互联网相关 | 7486 | 54256 |'
- en: '| 50-sport | sports | is related to sports | 7505 | 54718 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 50-sport | 体育 | 与体育相关 | 7505 | 54718 |'
- en: '| 51-entertainment | entertainment | is about entertainment | 7461 | 53573
    |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 51-entertainment | 娱乐 | 讲述娱乐内容 | 7461 | 53573 |'
- en: '| 52-family | relationships | is about family and relationships | 7438 | 54680
    |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 52-family | 关系 | 讲述家庭和关系 | 7438 | 54680 |'
- en: '| 53-politic | politics | is related to politics or government | 7410 | 53393
    |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 53-politic | 政治 | 与政治或政府相关 | 7410 | 53393 |'
