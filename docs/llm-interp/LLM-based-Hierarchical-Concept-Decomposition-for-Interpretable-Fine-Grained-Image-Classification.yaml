- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 17:34:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:34:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained
    Image Classification
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的层次化概念分解用于可解释的细粒度图像分类
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18672](https://ar5iv.labs.arxiv.org/html/2405.18672)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18672](https://ar5iv.labs.arxiv.org/html/2405.18672)
- en: Renyi Qu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Renyi Qu
- en: University of Pennsylvania
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 佩恩大学
- en: requ@seas.upenn.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: requ@seas.upenn.edu
- en: '&Mark Yatskar'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Mark Yatskar'
- en: University of Pennsylvania
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 佩恩大学
- en: myatskar@seas.upenn.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: myatskar@seas.upenn.edu
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: '(Renyi Qu’s Master’s Thesis) Recent advancements in interpretable models for
    vision-language tasks have achieved competitive performance; however, their interpretability
    often suffers due to the reliance on unstructured text outputs from large language
    models (LLMs). This introduces randomness and compromises both transparency and
    reliability, which are essential for addressing safety issues in AI systems. We
    introduce a novel framework designed to enhance model interpretability through
    structured concept analysis. Our approach consists of two main components: (1)
    We use GPT-4 to decompose an input image into a structured hierarchy of visual
    concepts, thereby forming a visual concept tree. (2) We then employ an ensemble
    of simple linear classifiers that operate on concept-specific features derived
    from CLIP to perform classification. Our approach not only aligns with the performance
    of state-of-the-art models but also advances transparency by providing clear insights
    into the decision-making process and highlighting the importance of various concepts.
    This allows for a detailed analysis of potential failure modes and improves model
    compactness, therefore setting a new benchmark in interpretability without compromising
    the accuracy.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: （Renyi Qu的硕士论文）近年来在视觉-语言任务中，可解释模型取得了竞争力的性能；然而，由于依赖于大型语言模型（LLMs）生成的非结构化文本输出，其可解释性往往受到影响。这引入了随机性，损害了透明度和可靠性，而这些对解决AI系统中的安全问题至关重要。我们引入了一个新颖的框架，旨在通过结构化概念分析来增强模型的可解释性。我们的方法包括两个主要组件：（1）我们使用GPT-4将输入图像分解为一个结构化的视觉概念层次，从而形成一个视觉概念树。（2）然后，我们使用一组简单的线性分类器，这些分类器基于从CLIP中获得的概念特征进行分类。我们的方法不仅与最先进模型的性能相匹配，而且通过提供对决策过程的清晰见解和突显各种概念的重要性来提升透明度。这允许对潜在故障模式进行详细分析，并提高模型的紧凑性，因此在不妨碍准确性的情况下设立了可解释性的一个新基准。
- en: LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained
    Image Classification
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的层次化概念分解用于可解释的细粒度图像分类
- en: Renyi Qu University of Pennsylvania requ@seas.upenn.edu                       
    Mark Yatskar University of Pennsylvania myatskar@seas.upenn.edu
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Renyi Qu 佩恩大学 requ@seas.upenn.edu                        Mark Yatskar 佩恩大学 myatskar@seas.upenn.edu
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Consider a scenario in wildlife conservation where researchers need a model
    to classify species from camera trap images. A biologist might need to understand
    what specific patterns the model uses to identify certain images as a threatened
    species - Is the inference based on the differences in fur color, body shape,
    or perhaps confusing background elements with the animal? Similarly, adjusting
    the model’s input by clarifying that a visual feature is a plant rather than an
    animal part could potentially alter its classification. Furthermore, if new information
    becomes available, such as unexpected rainfall in which certain animals change
    their fur color, how might this change the model’s predictions? The lack of clear,
    structured interpretability not only hinders collaboration between human experts
    and AI systems but also limits the ability to trust and effectively manage these
    tools in dynamic, real-world environments. This is especially critical in high-stakes
    scenarios with fine-grained inference tasks, which involve identifying the subtle
    differences between similar subcategories within a category (Zhang et al., [2022](#bib.bib34)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 设想一个野生动物保护的场景，研究人员需要一个模型来从相机陷阱图像中分类物种。一位生物学家可能需要了解模型用来识别某些图像为受威胁物种的具体模式——推断是基于毛色差异、身体形状，还是可能混淆背景元素与动物？类似地，通过明确某个视觉特征是植物而不是动物部位，调整模型的输入可能会改变其分类。此外，如果有新信息出现，比如某些动物因意外降雨而改变毛色，这可能会如何改变模型的预测？缺乏清晰、结构化的可解释性不仅阻碍了人类专家与AI系统之间的合作，还限制了在动态现实环境中信任和有效管理这些工具的能力。这在涉及细微推断任务的高风险场景中尤为关键，这些任务涉及识别类别内类似子类别之间的细微差异（Zhang
    et al., [2022](#bib.bib34)）。
- en: 'There are two main approaches to enhancing model interpretability: post-hoc
    explanations, which often fail to consistently reflect the model’s actual decision-making
    processes (Rudin, [2019](#bib.bib25)), and interpretable-by-design models, which
    are constrained by the inherent trade-off between interpretability and performance
    (Gunning and Aha, [2019](#bib.bib10); Gosiewska et al., [2021](#bib.bib9)). Current
    interpretable vision-language models leverage the unstructured and stochastic
    text outputs from LLMs (Yang et al., [2023](#bib.bib33); Pratt et al., [2023](#bib.bib22);
    Liu et al., [2024](#bib.bib18); Singh et al., [2024](#bib.bib27); Stan et al.,
    [2024](#bib.bib29)). While effective, the inherent randomness and lack of structure
    can obscure the underlying reasons for their decisions, reduce the readability,
    and complicate the debugging process and error analysis (Zhang et al., [2023](#bib.bib35)).
    Moreover, they often lack the flexibility needed to adapt to unseen scenarios.
    For instance, consider an image of an airplane taken from the rear. Features like
    the windshield are not visible, rendering typical visual cues for subclass identification
    irrelevant; if the airplane is damaged, standard visual indicators might also
    fail, which highlights the inflexibility of current interpretable-by-design approaches
    in accommodating such variations.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 提升模型可解释性主要有两种方法：事后解释，这种方法通常无法一致地反映模型的实际决策过程（Rudin, [2019](#bib.bib25)），以及可解释设计模型，这些模型受到解释性与性能之间固有权衡的限制（Gunning
    and Aha, [2019](#bib.bib10); Gosiewska et al., [2021](#bib.bib9)）。目前的可解释视觉语言模型利用了LLMs的非结构化和随机文本输出（Yang
    et al., [2023](#bib.bib33); Pratt et al., [2023](#bib.bib22); Liu et al., [2024](#bib.bib18);
    Singh et al., [2024](#bib.bib27); Stan et al., [2024](#bib.bib29)）。虽然有效，但固有的随机性和缺乏结构可能掩盖决策的根本原因，降低可读性，并使调试过程和错误分析变得复杂（Zhang
    et al., [2023](#bib.bib35)）。此外，它们通常缺乏适应未见场景所需的灵活性。例如，考虑从后方拍摄的飞机图像。诸如挡风玻璃等特征不可见，使得子类识别的典型视觉线索变得无关；如果飞机受损，标准的视觉指示也可能失效，这突显了当前可解释设计方法在适应这种变化时的局限性。
- en: '![Refer to caption](img/d0538df327744ecc7b750608e05a8d11.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d0538df327744ecc7b750608e05a8d11.png)'
- en: 'Figure 1: Our system.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们的系统。
- en: Our approach significantly advances the framework established by LaBo (Yang
    et al., [2023](#bib.bib33)) and CBM (Koh et al., [2020](#bib.bib15)) by refining
    the interpretability through hierarchical concept decomposition, coupled with
    an ensemble of simple, linear classifiers that are straightforward to interpret
    and debug. Unlike traditional methods that generate random visual clues and select
    the optimal ones based on complex, opaque metrics, our concept generation module
    systematically decomposes the input object category into a clear hierarchical
    tree of visual parts and their associated attributes. This decomposition is consistent
    across different inputs of the same category, and all root-to-leaf paths in the
    tree are considered jointly when computing features for image classification.
    This comprehensive approach enables high coverage of non-standard scenarios and
    complex cases. Following this, each classifier within our ensemble makes decisions
    based on the features of a specific visual part. The final prediction is derived
    from a collective vote among these classifiers. This structured method not only
    clarifies which parts are pivotal in the classification but also elucidates why
    certain features are crucial in distinguishing between positive and negative examples,
    enhancing both transparency and reliability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法通过层次概念分解显著提升了由LaBo（Yang等，[2023](#bib.bib33)）和CBM（Koh等，[2020](#bib.bib15)）建立的框架，通过简单的线性分类器集合进一步完善了解释能力，这些分类器易于解释和调试。与生成随机视觉线索并基于复杂、不透明指标选择最佳线索的传统方法不同，我们的概念生成模块系统地将输入对象类别分解为清晰的视觉部分及其关联属性的层次树。这种分解在同一类别的不同输入之间保持一致，在计算图像分类特征时，所有从根到叶的路径都会共同考虑。这种全面的方法能够高覆盖非标准场景和复杂案例。随后，我们的每个分类器根据特定视觉部分的特征做出决策。最终预测通过这些分类器的集体投票得出。这种结构化方法不仅阐明了分类中哪些部分是关键，还阐释了为何某些特征在区分正负样本时至关重要，提高了透明度和可靠性。
- en: We experimented on prominent fine-grained image datasets (Maji et al., [2013](#bib.bib19);
    Wah et al., [2011](#bib.bib32); Krause et al., [2013](#bib.bib16); Khosla et al.,
    [2011](#bib.bib13); Nilsback and Zisserman, [2008](#bib.bib20); Bossard et al.,
    [2014](#bib.bib2)) and demonstrated that our model achieves performance comparable
    to both interpretable vision-language models and traditional SOTA image classification
    models. Through careful ablation studies, we determined the optimal configuration
    for maximizing inference accuracy involves decomposing to the level of the most
    detailed visual parts. Deviating from this level, either by decomposing less or
    more, detrimentally affects performance. In the results section, we elaborate
    on how our hierarchical structure enhances the model by improving readability,
    facilitating debugging and error analysis, and adding flexibility in feature updates.
    Additionally, we discuss how our model maintains compactness effectively through
    a pruning process that eliminates redundant or non-informative parts, further
    enhancing model efficiency and interpretability.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在著名的细粒度图像数据集（Maji等，[2013](#bib.bib19)；Wah等，[2011](#bib.bib32)；Krause等，[2013](#bib.bib16)；Khosla等，[2011](#bib.bib13)；Nilsback和Zisserman，[2008](#bib.bib20)；Bossard等，[2014](#bib.bib2)）上进行了实验，并证明我们的模型在性能上与可解释的视觉语言模型和传统的SOTA图像分类模型相当。通过细致的消融研究，我们确定了最大化推理准确性的最佳配置是分解到最详细的视觉部分级别。偏离这个级别，无论是分解得更少还是更多，都会对性能产生不利影响。在结果部分，我们详细阐述了我们的层次结构如何通过提高可读性、促进调试和错误分析以及在特征更新中增加灵活性来增强模型。此外，我们还讨论了我们的模型如何通过修剪过程有效地保持紧凑性，消除冗余或无信息的部分，进一步提高模型的效率和可解释性。
- en: 'In summary, our contributions are:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献是：
- en: '1.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We introduce fully-automated hierarchical concept decomposition using GPT-4,
    which generates a structured tree of visual parts and attributes, offering a level
    of interpretability that surpasses that of existing interpretable-by-design models.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了使用GPT-4进行的全自动层次概念分解，这种方法生成了一个结构化的视觉部分和属性树，提供了超越现有设计可解释模型的解释能力。
- en: '2.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We employ an ensemble of simple linear concept classifiers that enable direct
    inspection of each visual part and attribute, which facilitates more effective
    debugging and visualization while maintaining a competitive standard of classification
    performance.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们采用了简单线性概念分类器的集合，这使得直接检查每个视觉部分和属性成为可能，从而在保持分类性能竞争标准的同时，促进了更有效的调试和可视化。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM-assisted vision-language systems   Recently, the integration of Large Language
    Models (LLMs) into vision-language tasks has been explored to leverage their success
    in language processing for improved performance and interpretability in multimodal
    applications. For instance, researchers have utilized LLMs as expansive knowledge
    bases to augment Visual Question Answering (VQA) systems by providing relevant
    concepts (Fu et al., [2023](#bib.bib8)). Others have implemented LLM-assisted
    approaches to enhance image captioning within knowledge-guided VQA models (Du
    et al., [2023](#bib.bib6)). Additionally, some studies have shifted from traditional
    knowledge extraction to generating executable programming codes using LLMs to
    directly address VQA tasks (Surís et al., [2023](#bib.bib31); Subramanian et al.,
    [2023](#bib.bib30)). Further advancements include the incorporation of LLMs into
    the training procedures of models like BLIP-2 and BLIVA, aiming to create multimodal
    LLMs specifically tailored for VQA challenges (Li et al., [2023](#bib.bib17);
    Hu et al., [2023](#bib.bib12)). Despite these models showing promise, they often
    fall short of providing clear interpretability, particularly in elucidating the
    reasoning processes behind their decisions. Moreover, the inherent randomness
    and the unstructured nature of LLMs tend to obscure the rationale for their decisions,
    complicating the readability, debugging, and error analysis of these systems (Zhang
    et al., [2023](#bib.bib35)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM辅助的视觉语言系统** 最近，大型语言模型（LLMs）在视觉语言任务中的整合已被探索，以利用其在语言处理中的成功，提升多模态应用中的性能和可解释性。例如，研究人员利用LLMs作为广泛的知识库，通过提供相关概念来增强视觉问答（VQA）系统（Fu
    et al., [2023](#bib.bib8)）。其他人实施了LLM辅助的方法，以在知识引导的VQA模型中提升图像描述（Du et al., [2023](#bib.bib6)）。此外，一些研究已从传统的知识提取转向使用LLMs生成可执行的编程代码，直接解决VQA任务（Surís
    et al., [2023](#bib.bib31); Subramanian et al., [2023](#bib.bib30)）。进一步的进展包括将LLMs纳入像BLIP-2和BLIVA这样的模型的训练过程中，旨在创建专门针对VQA挑战的多模态LLMs（Li
    et al., [2023](#bib.bib17); Hu et al., [2023](#bib.bib12)）。尽管这些模型展现出一定的前景，但它们常常无法提供明确的可解释性，特别是在阐明其决策过程方面。此外，LLMs固有的随机性和非结构化特性往往掩盖其决策的依据，使这些系统的可读性、调试和错误分析变得复杂（Zhang
    et al., [2023](#bib.bib35)）。'
- en: 'Interpretable vision-language systems   Two primary strategies are employed
    to enhance model interpretability: post-hoc explanations and interpretable-by-design
    models. Post-hoc explanations are applied after model development, typically through
    various methods of explanation generation (Hendricks et al., [2016](#bib.bib11);
    Kim et al., [2018](#bib.bib14); Nishida et al., [2022](#bib.bib21); Singh et al.,
    [2022](#bib.bib28)). However, these methods do not inherently increase the interpretability
    of the model itself; the core model remains a black box and these explanations
    often fail to accurately represent the model’s decision-making processes (Rudin,
    [2019](#bib.bib25)).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**可解释的视觉语言系统** 主要有两种策略来增强模型的可解释性：事后解释和可解释设计模型。事后解释在模型开发后应用，通常通过各种解释生成方法（Hendricks
    et al., [2016](#bib.bib11); Kim et al., [2018](#bib.bib14); Nishida et al., [2022](#bib.bib21);
    Singh et al., [2022](#bib.bib28)）。然而，这些方法本质上并不增加模型自身的可解释性；核心模型仍然是黑箱，这些解释通常无法准确反映模型的决策过程（Rudin,
    [2019](#bib.bib25)）。'
- en: In contrast, interpretable-by-design models are explicitly constructed to be
    understandable. A significant approach within this category is Concept Bottleneck
    Models (CBMs), which use high-level, human-understandable concepts as an intermediate
    layer. These concepts are then linearly combined to predict outcomes. For instance,
    one application of CBMs utilized a linear layer to integrate CLIP scores with
    expert-designed concepts, assessing CLIP’s efficacy in concept grounding (Bhalla,
    [2022](#bib.bib1)). Efforts have been made to improve the human readability of
    CBMs by incorporating comprehensible textual guidance (Bujwid and Sullivan, [2021](#bib.bib3);
    Roth et al., [2022](#bib.bib24); Shen et al., [2022](#bib.bib26)). As large language
    models (LLMs) became more prevalent, current interpretable vision-language models
    often rely on the stochastic and unstructured text outputs from LLMs, which introduces
    new challenges (Yang et al., [2023](#bib.bib33); Pratt et al., [2023](#bib.bib22);
    Liu et al., [2024](#bib.bib18); Singh et al., [2024](#bib.bib27); Stan et al.,
    [2024](#bib.bib29)). A notable drawback of CBMs is their high reliance on costly
    and unreliable manual annotations, which typically results in poorer performance
    compared to more opaque models. Attempts to mitigate these issues have included
    substituting the traditional knowledge base with concepts generated by LLMs, which
    has led to notable gains in both interpretability and performance (Yang et al.,
    [2023](#bib.bib33)). Nevertheless, the resulting visual concepts often remain
    unstructured and ambiguous, limiting their effectiveness in clearly elucidating
    the attributes relevant to each classified image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，可解释设计模型被明确构建为易于理解。该类别中的一个重要方法是概念瓶颈模型（CBMs），它们使用高层次的、易于理解的概念作为中间层。这些概念随后被线性组合以预测结果。例如，CBMs的一个应用使用线性层将CLIP分数与专家设计的概念进行整合，评估了CLIP在概念基础上的有效性（Bhalla，[2022](#bib.bib1)）。为了提高CBMs的人类可读性，已经通过整合易于理解的文本指导进行了改进（Bujwid和Sullivan，[2021](#bib.bib3)；Roth等，[2022](#bib.bib24)；Shen等，[2022](#bib.bib26)）。随着大型语言模型（LLMs）的普及，目前的可解释视觉-语言模型通常依赖于LLMs生成的随机且非结构化的文本输出，这带来了新的挑战（Yang等，[2023](#bib.bib33)；Pratt等，[2023](#bib.bib22)；Liu等，[2024](#bib.bib18)；Singh等，[2024](#bib.bib27)；Stan等，[2024](#bib.bib29)）。CBMs的一个显著缺点是其对昂贵且不可靠的人工注释的高度依赖，这通常导致比更不透明的模型表现更差。为解决这些问题，已经尝试用LLMs生成的概念替代传统知识库，这在可解释性和性能上都取得了显著的提升（Yang等，[2023](#bib.bib33)）。然而，生成的视觉概念往往仍然是非结构化和模糊的，这限制了它们在清晰阐明每个分类图像相关属性方面的有效性。
- en: 3 Method
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'The problem setting for fine-grained image classification is defined as follows:
    given an image-label pair $(i,y)$, where $i$ is a raw image and $y\in\mathcal{Y}$
    is a subclass from a set of similar subclasses within the same domain $K$, we
    first convert the raw image into a feature representation $x=g(i)\in\mathcal{X}$.
    The classification model then predicts a label $\hat{y}=f(x)$. During training,
    the model’s objective is to minimize the discrepancy between the predicted output
    and the actual label $\mathcal{L}(\hat{y},y)$. During inference, the prediction
    is used directly for evaluation or deployment purposes.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化图像分类的问题设置如下：给定一个图像-标签对 $(i,y)$，其中 $i$ 是原始图像，$y\in\mathcal{Y}$ 是来自同一领域 $K$
    中一组相似子类的子类，我们首先将原始图像转换为特征表示 $x=g(i)\in\mathcal{X}$。分类模型随后预测标签 $\hat{y}=f(x)$。在训练过程中，模型的目标是最小化预测输出与实际标签之间的差异
    $\mathcal{L}(\hat{y},y)$。在推断过程中，预测结果直接用于评估或部署目的。
- en: In the context of language-assisted fine-grained image classification, where
    an alignment model like CLIP is utilized, each sample is augmented to include
    a text description, forming a triplet $(i,t,y)$. This introduces an additional
    component to feature engineering involving text data. Image features $x_{i}$ are
    derived using an image encoder $x_{i}=E_{I}(i)\in\mathbb{R}^{d}$, where $d$ is
    the dimension of the image embeddings. Similarly, text features $x_{t}$ are derived
    using a text encoder $x_{t}=E_{t}(t)\in\mathbb{R}^{d}$, with the assumption that
    text embeddings share the same dimensional space as image embeddings. The final
    features $x$ are formed by integrating both image and text embeddings, $x=g(x_{i},x_{t})$.
    This enriched feature set is used to predict the subclass $\hat{y}=f(x)$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言辅助的细粒度图像分类背景下，使用如 CLIP 的对齐模型，每个样本都会扩展为包含文本描述，形成三元组 $(i,t,y)$。这引入了一个额外的特征工程组件，涉及文本数据。图像特征
    $x_{i}$ 通过图像编码器得到 $x_{i}=E_{I}(i)\in\mathbb{R}^{d}$，其中 $d$ 是图像嵌入的维度。同样，文本特征 $x_{t}$
    通过文本编码器得到 $x_{t}=E_{t}(t)\in\mathbb{R}^{d}$，假设文本嵌入与图像嵌入共享相同的维度空间。最终特征 $x$ 通过整合图像和文本嵌入形成，即
    $x=g(x_{i},x_{t})$。这个丰富的特征集用于预测子类 $\hat{y}=f(x)$。
- en: 'Our method aims to enhance both interpretability and performance by improving
    the quality of text features $x_{t}$ and refining the modeling component $f(x)$.
    This approach encompasses two main elements: concept tree decomposition and concept
    classification. To clarify the discussion, we define three key terms that are
    used extensively throughout this section.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法旨在通过提高文本特征 $x_{t}$ 的质量和优化建模组件 $f(x)$ 来增强可解释性和性能。这种方法包括两个主要元素：概念树分解和概念分类。为澄清讨论，我们定义了在本节中广泛使用的三个关键术语。
- en: 'Visual part: This refers to a discernible physical segment of an object visible
    to the human eye. For example, within the category of dogs, ’head’ is a visual
    part, ’mouth’ is a part of the head, and ’tongue’ is a part of the mouth. This
    is denoted as $p\in\mathcal{P}$, where $\mathcal{P}$ denotes the set of all possible
    visual parts for the specific class domain.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉部分：这是指人眼可见的物体的明显物理部分。例如，在狗的类别中，“头部”是一个视觉部分，“嘴部”是头部的一部分，而“舌头”是嘴部的一部分。这表示为 $p\in\mathcal{P}$，其中
    $\mathcal{P}$ 表示特定类别域的所有可能视觉部分的集合。
- en: 'Visual attribute: These are observable characteristics of a visual part, such
    as size, shape, color, material, finish, and design complexity. For instance,
    visual attributes of a car’s front bumper might include its color and material.
    This is denoted as $a\in\mathcal{A}_{p}$, where $\mathcal{A}_{p}$ denotes the
    set of all possible visual attributes for a specific visual part $p$.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉属性：这些是视觉部分的可观察特征，如大小、形状、颜色、材料、表面处理和设计复杂性。例如，汽车前保险杠的视觉属性可能包括其颜色和材料。这表示为 $a\in\mathcal{A}_{p}$，其中
    $\mathcal{A}_{p}$ 表示特定视觉部分 $p$ 的所有可能视觉属性的集合。
- en: 'Attribute value: This is the specific manifestation of a visual attribute.
    For example, the color attribute of an American crow’s primary features would
    be black, and the shape attribute of its eyes would be rounded. This is denoted
    as $v\in\mathcal{V}_{a}$, where $\mathcal{V}$ denotes the set of all possible
    visual attributes for a specific visual attribute $a$.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 属性值：这是视觉属性的具体表现。例如，美国渡鸦的主要特征的颜色属性为黑色，其眼睛的形状属性为圆形。这表示为 $v\in\mathcal{V}_{a}$，其中
    $\mathcal{V}$ 表示特定视觉属性 $a$ 的所有可能视觉属性的集合。
- en: We assume all subclasses share the same visual parts and visual attributes and
    differ in attribute values. Therefore, all subclasses share the same $\mathcal{P}$
    and the same set $\mathcal{A}_{p}$ for each $p\in\mathcal{P}$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设所有子类共享相同的视觉部分和视觉属性，但在属性值上有所不同。因此，所有子类共享相同的 $\mathcal{P}$ 和每个 $p\in\mathcal{P}$
    的相同集合 $\mathcal{A}_{p}$。
- en: 3.1 Concept Tree Decomposition
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概念树分解
- en: 'Our approach decomposes an arbitrary object category into a concept tree where
    each intermediate node corresponds to a visual part of the category in question,
    each leaf node represents a visual attribute value pertinent to its parent visual
    attribute, and each visual attribute node bridges its parent visual part and its
    corresponding child value. The decomposition process unfolds in three sequential
    steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法将任意对象类别分解为一个概念树，其中每个中间节点对应于该类别的视觉部分，每个叶节点表示与其父视觉属性相关的视觉属性值，每个视觉属性节点连接其父视觉部分及其对应的子值。分解过程分为三个连续步骤：
- en: Visual Part Decomposition   Starting with a given object domain $K$, we use
    GPT-4 to generate a hierarchical arrangement of all possible visual parts of the
    object via zero-shot prompting, denoted as $\mathcal{P}=\text{LLM}_{\text{zero}}(K)$.
    This hierarchy is then formatted and preserved in a JSON structure, where each
    node denotes a visual part identified by GPT-4.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉部分分解：从给定的对象域 $K$ 开始，我们使用 GPT-4 通过零样本提示生成对象所有可能的视觉部分的层次结构，记作 $\mathcal{P}=\text{LLM}_{\text{zero}}(K)$。然后，将该层次结构格式化并保存在
    JSON 结构中，每个节点表示 GPT-4 识别出的视觉部分。
- en: Visual Attribute Generation   For every visual part identified $p\in\mathcal{P}$,
    we use GPT-4 to enumerate 3-7 pertinent visual attributes via few-shot prompting,
    denoted as $\mathcal{A}_{p}=\text{LLM}_{\text{few}}(p,\mathcal{M}_{p\rightarrow
    a})$, where $\mathcal{M}_{p\rightarrow a}$ consists of three fixed examples of
    part-to-attribute mappings for consistency and diversity. This stage not only
    identifies common attributes such as size, shape, and color but also specific
    ones like luminosity, opacity, and thickness. The aim is to capture a diverse
    array of attributes, enhancing the richness of the concept tree.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉属性生成：对于每个识别出的视觉部分 $p\in\mathcal{P}$，我们使用 GPT-4 通过少样本提示枚举 3-7 个相关的视觉属性，记作 $\mathcal{A}_{p}=\text{LLM}_{\text{few}}(p,\mathcal{M}_{p\rightarrow
    a})$，其中 $\mathcal{M}_{p\rightarrow a}$ 包含三个固定的部分到属性映射的示例，以确保一致性和多样性。这一阶段不仅识别常见的属性如大小、形状和颜色，还包括特定的属性如亮度、透明度和厚度。目标是捕捉多样的属性，增强概念树的丰富性。
- en: 'Concept Tree Generation   In this final step, we assign attribute values to
    each visual attribute $a\in\mathcal{A}_{p}$ of each visual part $p\in\mathcal{P}$
    of each specific subclass $y\in\mathcal{Y}$, denoted as $\mathcal{V}_{a}^{y}=\text{LLM}_{\text{crit}}(a,p,y)$.
    Using self-critique prompting that involves a sequence of three critical queries,
    we refine these assignments:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 概念树生成：在这最后一步中，我们将属性值分配给每个特定子类 $y\in\mathcal{Y}$ 的每个视觉部分 $p\in\mathcal{P}$ 的每个视觉属性
    $a\in\mathcal{A}_{p}$，记作 $\mathcal{V}_{a}^{y}=\text{LLM}_{\text{crit}}(a,p,y)$。通过包含一系列三条关键查询的自我批评提示，我们对这些分配进行完善：
- en: '1.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Logical Relationships: We determine whether multiple attribute values should
    be combined using logical operators (AND/OR). For instance, if a cat’s fur is
    described as "black AND white," the attribute value implies the presence of both
    colors simultaneously. However, if it is described as "black OR white", then either
    color satisfies this condition, so this phrase should be separated into two leaf
    nodes.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逻辑关系：我们通过逻辑运算符（AND/OR）来确定多个属性值是否应该结合在一起。例如，如果一只猫的毛发被描述为“黑色 AND 白色”，则该属性值表示同时存在这两种颜色。然而，如果描述为“黑色
    OR 白色”，那么任意一种颜色都符合这一条件，因此该短语应该分解为两个叶节点。
- en: '2.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Attribute Value Consistency: To ensure compatibility with the CLIP text encoder,
    which differs in handling various Part-Of-Speech (POS) tags, we standardize multi-word
    and noun attribute values into an "of" attributive format, while single-word adjectives
    remain unchanged.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 属性值一致性：为了确保与 CLIP 文本编码器的兼容性（其处理不同的词性标签有所不同），我们将多词和名词属性值标准化为“of”属性格式，而单词形容词保持不变。
- en: '3.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Redundancy Reduction: We scrutinize the attribute list to eliminate any repetitions
    that might bias the model, ensuring that each visual attribute and its values
    are uniquely represented.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 冗余减少：我们审查属性列表，消除可能导致模型偏差的重复项，确保每个视觉属性及其值唯一表示。
- en: Upon completing these steps, we convert each root-to-leaf path into a coherent
    natural language description denoted as $h(y,p,a,v)\in\mathcal{C}_{K}$, where
    $\mathcal{C}_{K}\subset\mathbb{C}$ is the subset of all visual clues for the problem
    domain generated by GPT-4\. Note that it is infeasible to obtain the full visual
    clue set $\mathbb{C}$. We then encode each visual clue via the CLIP text encoder
    $E_{T}$, and store the embeddings $x_{t}=E_{T}(h(y,p,a,v))$ for subsequent classification
    tasks. Although trees within the same object category share structural similarities,
    they are distinguished by their unique attribute values. Figures [2](#S3.F2 "Figure
    2 ‣ 3.1 Concept Tree Decomposition ‣ 3 Method ‣ LLM-based Hierarchical Concept
    Decomposition for Interpretable Fine-Grained Image Classification") and [3](#S3.F3
    "Figure 3 ‣ 3.1 Concept Tree Decomposition ‣ 3 Method ‣ LLM-based Hierarchical
    Concept Decomposition for Interpretable Fine-Grained Image Classification") present
    snippets from the visual representation of the generated concept trees using Graphviz
    (Ellson et al., [2002](#bib.bib7)). Due to the extensive breadth of the entire
    tree, it is impractical to display it in its entirety within this thesis.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，我们将每条从根到叶的路径转换为连贯的自然语言描述，记作 $h(y,p,a,v)\in\mathcal{C}_{K}$，其中 $\mathcal{C}_{K}\subset\mathbb{C}$
    是由 GPT-4 生成的针对问题领域的所有视觉线索的子集。请注意，获得完整的视觉线索集 $\mathbb{C}$ 是不可行的。然后，我们通过 CLIP 文本编码器
    $E_{T}$ 对每个视觉线索进行编码，并存储嵌入 $x_{t}=E_{T}(h(y,p,a,v))$ 以供后续分类任务使用。尽管同一对象类别中的树共享结构相似性，但它们通过其独特的属性值加以区分。图
    [2](#S3.F2 "图 2 ‣ 3.1 概念树分解 ‣ 3 方法 ‣ 基于 LLM 的层次概念分解用于可解释的细粒度图像分类") 和 [3](#S3.F3
    "图 3 ‣ 3.1 概念树分解 ‣ 3 方法 ‣ 基于 LLM 的层次概念分解用于可解释的细粒度图像分类") 展示了使用 Graphviz (Ellson
    et al., [2002](#bib.bib7)) 生成的概念树的视觉表示片段。由于整个树的广泛性，在本论文中无法展示其全部内容。
- en: '![Refer to caption](img/6931c3da0724db3b64f3bdaf7c56341d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6931c3da0724db3b64f3bdaf7c56341d.png)'
- en: 'Figure 2: A snippet of a Part-Attribute subtree for the "Skull" part. The "Head"
    node serves as the parent visual part of "Skull". The "Attrs" node displays a
    comprehensive subtree of Name-Value attribute pairs associated with the "skull"
    part. Each score represents the contribution of its corresponding attribute value
    to the prediction of the "Head" classifier.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: "Skull" 部件的一个 Part-Attribute 子树片段。"Head" 节点作为 "Skull" 的父视觉部件。"Attrs" 节点显示与
    "skull" 部件相关的 Name-Value 属性对的全面子树。每个分数表示其对应属性值对 "Head" 分类器预测的贡献。'
- en: '![Refer to caption](img/1fa7a7563822f7a07f68c6bed9de1fc5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1fa7a7563822f7a07f68c6bed9de1fc5.png)'
- en: 'Figure 3: A snippet of the hierarchy of visual parts. The "Mouth" node originates
    from the "Head" parent node. Subparts of "Mouth," including "Lips," "Teeth," and
    "Tongue," are represented as child nodes. Each part features its own attribute
    subtree, independent of its level of decomposition.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 视觉部件层次的一个片段。"Mouth" 节点源自 "Head" 父节点。"Mouth" 的子部分，包括 "Lips"、"Teeth" 和 "Tongue"，作为子节点表示。每个部件都有自己独立的属性子树，与其分解级别无关。'
- en: 4 Concept Classification
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 概念分类
- en: Given an input image $i$, we first encode it using the CLIP image encoder $E_{I}$.
    We then calculate the similarity scores between this encoded image and each pre-computed
    visual clue embedding. The final features for classification $x$ are derived from
    the similarity function $g(x_{i},x_{t})=\text{sim}\left(E_{I}(i),E_{T}\left(h(y,p,a,v)\right)\right)$
    across all visual clues in the set $\mathcal{C}_{K}$ for each image sample $i$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入图像 $i$，我们首先使用 CLIP 图像编码器 $E_{I}$ 对其进行编码。然后，我们计算该编码图像与每个预计算的视觉线索嵌入之间的相似度分数。分类的最终特征
    $x$ 来源于相似度函数 $g(x_{i},x_{t})=\text{sim}\left(E_{I}(i),E_{T}\left(h(y,p,a,v)\right)\right)$，适用于每个图像样本
    $i$ 中的所有视觉线索集合 $\mathcal{C}_{K}$。
- en: Although this process could be performed by any classifier, a linear probe LP
    is commonly utilized due to its simplicity and effectiveness. However, to enhance
    interpretability, we employ an ensemble of linear probes where each $\text{LP}_{p}$
    is tasked with predicting the actual subclass $y$ based on part-specific features
    $x_{p}$. For instance, in the "dog" domain, an "eye classifier" predicts the dog
    breed using only the eye features, while an "ear" classifier does so using only
    the ear features. The part-specific features $x_{p}$ are extracted from the flattened
    feature vector $x$ by segmenting on visual parts $p$ instead of subclasses $y$,
    as illustrated in Figure LABEL:fig:model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个过程可以由任何分类器执行，但由于其简单性和有效性，通常使用线性探测器 LP。然而，为了提高可解释性，我们使用一个线性探测器的集成，每个 $\text{LP}_{p}$
    负责根据特定部分的特征 $x_{p}$ 预测实际子类 $y$。例如，在“狗”领域中，“眼睛分类器”仅使用眼睛特征来预测狗的品种，而“耳朵分类器”则使用耳朵特征。特定部分的特征
    $x_{p}$ 是从展平的特征向量 $x$ 中提取的，通过对视觉部分 $p$ 进行分段，而不是子类 $y$，如图 Figure LABEL:fig:model
    所示。
- en: During training, each $\text{LP}_{p}$ is independently trained to minimize the
    cross-entropy loss $\mathcal{L}(\hat{y}_{p},y)$. In the inference phase, each
    $\text{LP}_{p}$ generates a subclass prediction $\hat{y}_{p}$ and a corresponding
    probability vector $\textbf{P}_{p}=[P_{p}(y=1),\cdots,P_{p}(y=N)]$, where $P$
    denotes probability and $N$ is the total number of subclasses. The final subclass
    prediction $\hat{y}$ is then determined either by a majority vote $\hat{y}=\text{mode}(\hat{\textbf{y}})$
    or by a top-probability vote $\hat{y}=\operatorname*{arg\,max}_{j}\sum_{p\in\mathcal{P}}P_{p}(y=j)$.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，每个 $\text{LP}_{p}$ 被独立训练，以最小化交叉熵损失 $\mathcal{L}(\hat{y}_{p},y)$。在推理阶段，每个
    $\text{LP}_{p}$ 生成一个子类预测 $\hat{y}_{p}$ 和一个相应的概率向量 $\textbf{P}_{p}=[P_{p}(y=1),\cdots,P_{p}(y=N)]$，其中
    $P$ 表示概率，$N$ 是子类的总数。最终的子类预测 $\hat{y}$ 由多数投票 $\hat{y}=\text{mode}(\hat{\textbf{y}})$
    或最高概率投票 $\hat{y}=\operatorname*{arg\,max}_{j}\sum_{p\in\mathcal{P}}P_{p}(y=j)$
    决定。
- en: 'In summary, our method follows the following pipeline:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的方法遵循以下流程：
- en: Algorithm 1 Hierarchical Concept Decomposition
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 层次概念分解
- en: $K,\mathcal{Y},\mathcal{M}_{p\rightarrow a}$$\mathcal{C}_{K}=\{\}$$\mathcal{P}\leftarrow\text{LLM}_{\text{zero}}(K)$
    $\triangleright$ Part Decompositionfor $p\in\mathcal{P}$ do $\triangleright$ Attribute
    Generation     $\mathcal{A}_{p}\leftarrow\text{LLM}_{\text{few}}(p,\mathcal{M}_{p\rightarrow
    a})$     for $y\in\mathcal{Y}$ do         for $a\in\mathcal{A}_{p}$ do $\triangleright$
    Tree Generation              $\mathcal{V}_{a}^{y}\leftarrow\text{LLM}_{\text{crit}}(a,p,y)$              for $v\in\mathcal{V}_{a}^{y}$ do                  $\mathcal{C}_{K}$.insert($h(y,p,a,v)$)              end for         end for     end forend for
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: $K,\mathcal{Y},\mathcal{M}_{p\rightarrow a}$$\mathcal{C}_{K}=\{\}$$\mathcal{P}\leftarrow\text{LLM}_{\text{zero}}(K)$
    $\triangleright$ 部分分解对于 $p\in\mathcal{P}$ 执行 $\triangleright$ 属性生成     $\mathcal{A}_{p}\leftarrow\text{LLM}_{\text{few}}(p,\mathcal{M}_{p\rightarrow
    a})$     对于 $y\in\mathcal{Y}$ 执行         对于 $a\in\mathcal{A}_{p}$ 执行 $\triangleright$
    树生成              $\mathcal{V}_{a}^{y}\leftarrow\text{LLM}_{\text{crit}}(a,p,y)$              对于 $v\in\mathcal{V}_{a}^{y}$ 执行                  $\mathcal{C}_{K}$.insert($h(y,p,a,v)$)              结束循环         结束循环     结束循环结束循环
- en: Algorithm 2 Concept Classification
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 概念分类
- en: $(i,y),\mathcal{C}_{K}$$\text{LPs}=\text{map}(p\rightarrow\text{LP})$$X_{t}\leftarrow
    E_{T}(\mathcal{C}_{K})$$x_{i}\leftarrow E_{I}(i)$$X\leftarrow\text{sim}(x_{i},X_{t})$$\hat{y}\leftarrow\text{LP}(X)$if training then     for $p\in\mathcal{P}$ do         $\text{LPs}[p]\leftarrow\operatorname*{arg\,min}_{\text{LP}}\mathcal{L}(\text{LP}(X_{p}),y)$     end forelse     Return
    $\text{vote}([\text{LPs}[p](X)\text{ for }p\in\mathcal{P}])$end if
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: $(i,y),\mathcal{C}_{K}$$\text{LPs}=\text{map}(p\rightarrow\text{LP})$$X_{t}\leftarrow
    E_{T}(\mathcal{C}_{K})$$x_{i}\leftarrow E_{I}(i)$$X\leftarrow\text{sim}(x_{i},X_{t})$$\hat{y}\leftarrow\text{LP}(X)$如果 训练 则     对于 $p\in\mathcal{P}$ 执行         $\text{LPs}[p]\leftarrow\operatorname*{arg\,min}_{\text{LP}}\mathcal{L}(\text{LP}(X_{p}),y)$     结束循环否则     返回
    $\text{vote}([\text{LPs}[p](X)\text{ for }p\in\mathcal{P}])$结束如果
- en: 5 Experiment
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Datasets
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集
- en: 'Our research focuses on fine-grained image classification. To this end, we
    selected six open-source, certified fine-grained datasets from diverse domains.
    Due to budget constraints associated with generating concept trees via GPT-4,
    we limited our study to 20 subclasses from each dataset, utilizing all available
    training and test images while adhering to the original train-test split ratios.
    The datasets we used include: FGVC-Aircraft (Maji et al., [2013](#bib.bib19)),
    CUB-200-2011 (Wah et al., [2011](#bib.bib32)), Stanford Cars (Krause et al., [2013](#bib.bib16)),
    Stanford Dogs (Khosla et al., [2011](#bib.bib13)), Flower-102 (Nilsback and Zisserman,
    [2008](#bib.bib20)), and Food-101 (Bossard et al., [2014](#bib.bib2)).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究专注于细粒度图像分类。为此，我们从不同领域选择了六个开源、认证的细粒度数据集。由于通过GPT-4生成概念树的预算限制，我们将研究限制在每个数据集的20个子类，利用所有可用的训练和测试图像，同时遵循原始的训练-测试拆分比例。我们使用的数据集包括：FGVC-Aircraft（Maji
    et al., [2013](#bib.bib19)）、CUB-200-2011（Wah et al., [2011](#bib.bib32)）、Stanford
    Cars（Krause et al., [2013](#bib.bib16)）、Stanford Dogs（Khosla et al., [2011](#bib.bib13)）、Flower-102（Nilsback
    and Zisserman, [2008](#bib.bib20)）和Food-101（Bossard et al., [2014](#bib.bib2)）。
- en: 5.2 Baselines
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基准
- en: 'Our system extends the LaBo framework (Yang et al., [2023](#bib.bib33)), which
    serves as our primary baseline together with its baseline model, CLIP + LP. We
    employ several deployment strategies for LP, described below using our notations:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统扩展了LaBo框架（Yang et al., [2023](#bib.bib33)），作为我们的主要基准及其基准模型CLIP + LP。我们采用了几种LP的部署策略，以下使用我们的符号进行描述：
- en: 'CLIP+LP (image only): The most common approach applies LP directly to the image
    embeddings generated by the CLIP image encoder, particularly the Vision Transformer
    (Dosovitskiy et al., [2020](#bib.bib5)). In our notation, the final prediction
    for an image is given by $\hat{y}=\text{LP}(E_{I}(i))$. This method remains the
    benchmark baseline, as no interpretable model has surpassed it due to the intrinsic
    trade-off between interpretability and performance (Gunning and Aha, [2019](#bib.bib10);
    Gosiewska et al., [2021](#bib.bib9)).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP+LP（仅图像）：最常见的方法是将LP直接应用于CLIP图像编码器生成的图像嵌入，特别是Vision Transformer（Dosovitskiy
    et al., [2020](#bib.bib5)）。在我们的符号中，图像的最终预测为$\hat{y}=\text{LP}(E_{I}(i))$。由于解释性与性能之间的固有权衡（Gunning
    and Aha, [2019](#bib.bib10); Gosiewska et al., [2021](#bib.bib9)），此方法仍然是基准基线，因为没有解释性模型超越它。
- en: 'CLIP+LP (image+label): In our work, which incorporates text clues, LP is applied
    to the similarity scores between image embeddings and label embeddings. These
    label embeddings are derived by encoding the class names directly with the CLIP
    text encoder. The final prediction for an image in our notation is $\hat{y}=\text{LP}(\text{sim}(E_{I}(i),E_{T}(h(y))))$,
    where $h(y)$ translates the label $y$ into a natural language format, following
    the prompting style used in CLIP’s pretraining: $h(y)=$"A photo of $y$".'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP+LP（图像+标签）：在我们的工作中，结合了文本线索，LP被应用于图像嵌入和标签嵌入之间的相似性分数。这些标签嵌入通过直接用CLIP文本编码器对类别名称进行编码来获得。在我们的符号中，图像的最终预测为$\hat{y}=\text{LP}(\text{sim}(E_{I}(i),E_{T}(h(y))))$，其中$h(y)$将标签$y$转换为自然语言格式，遵循CLIP预训练中使用的提示样式：$h(y)=$"A
    photo of $y$"。
- en: 'LaBo: Our approach differentiates from LaBo in the selection of the concept
    space $\mathcal{C}_{K}$ and the predictive model $f(\cdot)$. LaBo uses randomly
    generated visual clues from GPT-3 and a submodular optimization module to select
    them. It employs a learnable class-concept matrix to assign weights to each visual
    clue and then applies a single linear probe to the weighted combination of image-clue
    similarity scores. See Yang et al. ([2023](#bib.bib33)) for detailed mathematical
    descriptions. Due to a lack of visual clues for the Stanford Car and Stanford
    Dog datasets from the original authors, LaBo will be excluded from these two datasets.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LaBo：我们的方法与LaBo的不同之处在于概念空间$\mathcal{C}_{K}$和预测模型$f(\cdot)$的选择。LaBo使用来自GPT-3的随机生成视觉线索和一个子模优化模块来选择它们。它采用可学习的类别-概念矩阵为每个视觉线索分配权重，然后将单个线性探测器应用于图像-线索相似性分数的加权组合。有关详细的数学描述，请参见Yang
    et al. ([2023](#bib.bib33))。由于原作者未提供斯坦福汽车和斯坦福狗数据集的视觉线索，LaBo将被排除在这两个数据集之外。
- en: '|  | Aircraft | CUB | Car | Dog | Flower | Food |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | Aircraft | CUB | Car | Dog | Flower | Food |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| SOTA (for reference) | 0.954 | 0.931 | 0.971 | 0.973 | 0.998 | 0.986 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| SOTA（供参考） | 0.954 | 0.931 | 0.971 | 0.973 | 0.998 | 0.986 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| CLIP+LP (image only) | 0.798 | 0.882 | 0.973 | 0.906 | 0.985 | 0.953 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| CLIP+LP（仅图像） | 0.798 | 0.882 | 0.973 | 0.906 | 0.985 | 0.953 |'
- en: '| CLIP+LP (image+label) | 0.708 | 0.856 | 0.952 | 0.907 | 0.952 | 0.961 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| CLIP+LP（图像+标签） | 0.708 | 0.856 | 0.952 | 0.907 | 0.952 | 0.961 |'
- en: '| LaBo | 0.736 | 0.819 | - | - | 0.955 | 0.931 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LaBo | 0.736 | 0.819 | - | - | 0.955 | 0.931 |'
- en: '| Concept Classifier (ours) | 0.783 | 0.871 | 0.974 | 0.927 | 0.976 | 0.966
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 概念分类器（我们的） | 0.783 | 0.871 | 0.974 | 0.927 | 0.976 | 0.966 |'
- en: 'Table 1: Test Accuracy'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：测试准确率
- en: 'SOTA: Given the early stage of research in interpretable vision-language models,
    we include state-of-the-art (SOTA) fine-grained image classification scores as
    benchmarks for each dataset. These are for reference only and are not intended
    for direct comparison, as these scores are derived from experiments encompassing
    all classes, not just our subsets.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: SOTA：鉴于解释性视觉语言模型的研究处于早期阶段，我们将最先进（SOTA）细粒度图像分类分数作为每个数据集的基准。这些仅供参考，并不用于直接比较，因为这些分数来源于涵盖所有类别的实验，而不仅仅是我们的子集。
- en: 6 Evaluation
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 评估
- en: As indicated in Table [1](#S5.T1 "Table 1 ‣ 5.2 Baselines ‣ 5 Experiment ‣ LLM-based
    Hierarchical Concept Decomposition for Interpretable Fine-Grained Image Classification"),
    CLIP+LP (image only) performs well across most datasets. However, among the interpretable
    models, our model stands out, achieving significantly better performance than
    both CLIP+LP (image+label) and LaBo. Notably, our model even surpasses the CLIP+LP
    (image only) on the Stanford Dogs and Food-101 datasets. This clearly demonstrates
    that our model not only leads in performance among interpretable vision-language
    models in fine-grained image classification but also competes effectively with
    traditional, non-interpretable image classification baseline.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[1](#S5.T1 "Table 1 ‣ 5.2 Baselines ‣ 5 Experiment ‣ LLM-based Hierarchical
    Concept Decomposition for Interpretable Fine-Grained Image Classification")所示，CLIP+LP（仅图像）在大多数数据集上表现良好。然而，在解释性模型中，我们的模型脱颖而出，表现明显优于CLIP+LP（图像+标签）和LaBo。值得注意的是，我们的模型甚至超越了CLIP+LP（仅图像）在斯坦福狗和Food-101数据集上的表现。这清楚地表明，我们的模型不仅在细粒度图像分类中在解释性视觉语言模型中领先，而且在传统的非解释性图像分类基准中也具有有效竞争力。
- en: Although our results lag behind the SOTA benchmarks achieved using specially
    trained, complex neural networks with large architectures, our approach utilizes
    only an ensemble of linear probes and a single, pretrained CLIP model. This highlights
    the efficiency and potential of our simpler, more interpretable model in achieving
    competitive performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的结果落后于使用专门训练的大型复杂神经网络所获得的SOTA基准，但我们的方法仅利用了线性探针的集成和一个预训练的CLIP模型。这突显了我们更简单、更具解释性的模型在实现竞争性能方面的效率和潜力。
- en: 7 Ablation Studies
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 消融研究
- en: '7.1 Comparison 1: Decomposition Depth'
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 比较1：分解深度
- en: '|  | Aircraft | CUB | Car | Dog | Flower | Food |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 飞机 | CUB | 汽车 | 狗 | 花卉 | 食物 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LP on top-parts | 0.722 | 0.785 | 0.823 | 0.857 | 0.944 | 0.959 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| LP on top-parts | 0.722 | 0.785 | 0.823 | 0.857 | 0.944 | 0.959 |'
- en: '| LP on all-parts | 0.756 | 0.836 | 0.873 | 0.898 | 0.948 | 0.959 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| LP on all-parts | 0.756 | 0.836 | 0.873 | 0.898 | 0.948 | 0.959 |'
- en: '| LP on Attrs | 0.759 | 0.841 | 0.913 | 0.912 | 0.956 | 0.955 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| LP on Attrs | 0.759 | 0.841 | 0.913 | 0.912 | 0.956 | 0.955 |'
- en: '| LP on Attr-vals | 0.754 | 0.831 | 0.912 | 0.902 | 0.950 | 0.952 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LP on Attr-vals | 0.754 | 0.831 | 0.912 | 0.902 | 0.950 | 0.952 |'
- en: 'Table 2: Comparison 1: Decomposition Depth'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：比较1：分解深度
- en: As outlined in Section 3, the two lowest levels of our hierarchy are dedicated
    to visual parts, visual attributes, and attribute values, respectively. When calculating
    similarity for a given image, each leaf node within this hierarchy is assigned
    a similarity score. Each visual attribute node receives a score that reflects
    the maximum score among all its associated leaf nodes, embodying an "OR" logic.
    For instance, if the "Color" attribute includes the leaf nodes "Blue" and "Red",
    and "Blue" scores 0.99 while "Red" scores only 0.01, this suggests that the visual
    part in question is predominantly blue, thus the leaf node for "Blue" is activated.
    Averaging the scores in this context would not be appropriate; instead, taking
    the maximum score is more logical.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如第3节所述，我们层级结构的两个最低层分别用于视觉部分、视觉属性和属性值。在计算给定图像的相似度时，该层级结构中的每个叶节点都被分配一个相似度分数。每个视觉属性节点接收一个分数，反映了其所有相关叶节点中的最高分，体现了“OR”逻辑。例如，如果“颜色”属性包括叶节点“蓝色”和“红色”，其中“蓝色”得分为0.99，而“红色”仅得0.01，这表明该视觉部分主要是蓝色，因此“蓝色”叶节点被激活。在这种情况下平均分是不合适的；而取最大分数则更为合理。
- en: Conversely, each visual part node is assigned an average score calculated across
    all its child nodes, as these nodes collectively represent the visual part. For
    example, the visual part representing a dog’s eye might include attributes like
    size, shape, color, brightness, and pupil size, all considered simultaneously.
    Similarly, a dog’s head encompasses the parts of eyes, ears, nose, and mouth,
    making an average score a rational choice here.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，每个视觉部分节点分配一个平均分数，该分数是计算所有子节点的平均值，因为这些节点共同表示视觉部分。例如，代表狗眼睛的视觉部分可能包括大小、形状、颜色、亮度和瞳孔大小等属性，所有这些属性会同时被考虑。同样，狗的头部包含眼睛、耳朵、鼻子和嘴巴等部分，这使得平均分数在这里是一个合理的选择。
- en: To summarize, LP on attribute values utilizes raw similarity scores for each
    value. LP on attributes employs the maximum score from the leaf nodes under each
    attribute. LP on all parts calculates an average score across all attribute nodes
    within each visual part node. LP on top parts averages the scores across all subpart
    nodes of each major visual part node located just below the root node in the hierarchy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，LP 对属性值使用每个值的原始相似度分数。LP 对属性使用每个属性下叶节点的最大分数。LP 对所有部分计算每个视觉部分节点内所有属性节点的平均分数。LP
    对顶层部分则平均计算每个主要视觉部分节点下所有子部分节点的分数，这些节点位于层次结构中的根节点下方。
- en: 'For this ablation study, a single linear probe was used instead of an ensemble.
    As demonstrated in Table [2](#S7.T2 "Table 2 ‣ 7.1 Comparison 1: Decomposition
    Depth ‣ 7 Ablation Studies ‣ LLM-based Hierarchical Concept Decomposition for
    Interpretable Fine-Grained Image Classification"), attribute values are crucial
    for performance across most datasets, with performance increasing as the decomposition
    depth increases. However, delving deeper into individual attribute values can
    reduce performance, as some values may not be activated and thus contribute little
    to the final prediction, as illustrated in the "Blue" or "Red" example.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '对于本次消融研究，使用了单个线性探测器，而不是一个集合。正如表 [2](#S7.T2 "表 2 ‣ 7.1 比较 1: 分解深度 ‣ 7 消融研究 ‣
    基于LLM的层次概念分解用于可解释的细粒度图像分类") 中所示，属性值对大多数数据集的性能至关重要，随着分解深度的增加，性能也会提升。然而，深入研究单个属性值可能会降低性能，因为某些值可能没有被激活，因此对最终预测贡献很小，如“蓝色”或“红色”示例所示。'
- en: '7.2 Comparison 2: Label Inclusion'
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '7.2 比较 2: 标签包含'
- en: '|  | Aircraft | CUB | Car | Dog | Flower | Food |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 飞机 | CUB | 汽车 | 狗 | 花 | 食物 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LP (w/o) | 0.720 | 0.815 | 0.893 | 0.829 | 0.903 | 0.938 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| LP（无） | 0.720 | 0.815 | 0.893 | 0.829 | 0.903 | 0.938 |'
- en: '| LP (common) | 0.734 | 0.833 | 0.901 | 0.888 | 0.939 | 0.947 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| LP（普通） | 0.734 | 0.833 | 0.901 | 0.888 | 0.939 | 0.947 |'
- en: '| LP (with) | 0.759 | 0.841 | 0.913 | 0.912 | 0.956 | 0.955 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| LP（有） | 0.759 | 0.841 | 0.913 | 0.912 | 0.956 | 0.955 |'
- en: '| Concept Classifier (w/o) | 0.783 | 0.871 | 0.974 | 0.927 | 0.976 | 0.966
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 概念分类器（无） | 0.783 | 0.871 | 0.974 | 0.927 | 0.976 | 0.966 |'
- en: '| Concept Classifier (common) | 0.782 | 0.870 | 0.972 | 0.926 | 0.974 | 0.968
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 概念分类器（普通） | 0.782 | 0.870 | 0.972 | 0.926 | 0.974 | 0.968 |'
- en: '| Concept Classifier (with) | 0.783 | 0.871 | 0.973 | 0.927 | 0.975 | 0.966
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 概念分类器（有） | 0.783 | 0.871 | 0.973 | 0.927 | 0.975 | 0.966 |'
- en: 'Table 3: Comparison 2: Label Inclusion'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 比较 2: 标签包含'
- en: '|  | Aircraft | CUB | Car | Dog | Flower | Food |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | 飞机 | CUB | 汽车 | 狗 | 花 | 食物 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Concept Classifier (weighted) | 0.518 | 0.529 | 0.699 | 0.638 | 0.787 | 0.754
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 概念分类器（加权） | 0.518 | 0.529 | 0.699 | 0.638 | 0.787 | 0.754 |'
- en: '| Concept Classifier (maj) | 0.765 | 0.841 | 0.938 | 0.913 | 0.959 | 0.962
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 概念分类器（主要） | 0.765 | 0.841 | 0.938 | 0.913 | 0.959 | 0.962 |'
- en: '| Concept Classifier (prob) | 0.783 | 0.871 | 0.974 | 0.927 | 0.976 | 0.966
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 概念分类器（概率） | 0.783 | 0.871 | 0.974 | 0.927 | 0.976 | 0.966 |'
- en: 'Table 4: Comparison 3: Voting Strategy'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 比较 3: 投票策略'
- en: It is well-documented that including the label in the text prompt for the CLIP
    text encoder can significantly enhance its performance, as it is pretrained on
    texts that typically include the label (Radford et al., [2021](#bib.bib23); Chen
    et al., [2023](#bib.bib4); Yang et al., [2023](#bib.bib33)). In light of this,
    we evaluate whether our concept classifier can effectively counter this issue.
    The term "w/o" denotes scenarios where the label is not included, using the format
    $h=$"A photo of $h(p,a,v)$". The term "with" indicates scenarios where the label
    is included, formatted as $h=$"A photo of $y$ with $h(p,a,v)$". "Common" refers
    to the inclusion of the domain name for all labels, where $h=$"A photo of $K$
    with $h(p,a,v)$".
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，将标签包含在 CLIP 文本编码器的文本提示中可以显著提升其性能，因为它是在通常包含标签的文本上预训练的（Radford 等， [2021](#bib.bib23);
    Chen 等， [2023](#bib.bib4); Yang 等， [2023](#bib.bib33)）。考虑到这一点，我们评估了我们的概念分类器是否能有效解决这一问题。术语“w/o”表示未包含标签的场景，使用格式
    $h=$"A photo of $h(p,a,v)$"。术语“with”表示包含标签的场景，格式为 $h=$"A photo of $y$ with $h(p,a,v)$"。“Common”指所有标签包含领域名称的情况，其中
    $h=$"A photo of $K$ with $h(p,a,v)$"。
- en: 'As demonstrated in Table [3](#S7.T3 "Table 3 ‣ 7.2 Comparison 2: Label Inclusion
    ‣ 7 Ablation Studies ‣ LLM-based Hierarchical Concept Decomposition for Interpretable
    Fine-Grained Image Classification"), LPs are affected by this issue, showing that
    the inclusion of the label name significantly outperforms the other two approaches.
    However, this discrepancy is mitigated when we deploy our ensemble of LPs. The
    variations in the prompt do not significantly impact our performance, indicating
    that our system maintains stability regardless of how the prompt is structured.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [3](#S7.T3 "表 3 ‣ 7.2 比较 2：标签包含 ‣ 7 消融研究 ‣ 基于 LLM 的层次概念分解用于可解释的细粒度图像分类")
    所示，LPs 受到该问题的影响，显示出包含标签名称的效果显著优于其他两种方法。然而，当我们部署 LPs 集合时，这种差异得到缓解。提示中的变化对我们的性能影响不大，这表明无论提示如何构造，我们的系统都保持稳定。
- en: '7.3 Comparison 3: Voting Strategy'
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 比较 3：投票策略
- en: In addition to the two voting strategies outlined in Section 3.2, we explored
    the integration of a learnable part-attribute weight matrix, analogous to the
    class-concept weight matrix used in the LaBo framework. Contrary to expectations,
    incorporating learnable weights significantly impaired our performance. Upon a
    detailed examination of the feature importance assigned to each attribute within
    each visual part classifier, we found that the feature importance was nearly uniform
    across all attributes. This uniformity rendered the additional training of a weight
    matrix unnecessary.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 除了第 3.2 节中概述的两种投票策略外，我们还探讨了整合一个可学习的部分属性权重矩阵，这类似于 LaBo 框架中使用的类别概念权重矩阵。与预期相反，纳入可学习的权重显著降低了我们的性能。经过详细检查每个视觉部分分类器中分配给每个属性的特征重要性后，我们发现特征重要性在所有属性中几乎是均匀的。这种均匀性使得额外训练权重矩阵变得不必要。
- en: Furthermore, our analysis showed that the top-probability voting strategy outperforms
    the majority voting strategy. This suggests that while some visual part classifiers
    may incorrectly predict the label, they often still assign a high probability
    to the correct label, albeit not the highest compared to the incorrect one. This
    finding highlights the effectiveness of using probability-based decision metrics
    over simple majority rule in enhancing classification accuracy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的分析显示，最高概率投票策略优于多数投票策略。这表明，虽然一些视觉部分分类器可能错误地预测标签，但它们通常仍然为正确标签分配较高的概率，尽管相比于错误标签不是最高的。这一发现突显了使用基于概率的决策度量比简单的多数规则在提高分类准确性方面的有效性。
- en: 8 Conclusion
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this paper, we have presented a novel approach to fine-grained image classification
    that significantly enhances interpretability without sacrificing performance.
    Our method builds upon existing frameworks by introducing a structured hierarchical
    concept decomposition, coupled with an ensemble of linear classifiers, to clarify
    decision-making processes in vision-language models. This approach not only advances
    the state of interpretable-by-design models but also addresses the critical need
    for reliability and transparency in applications such as wildlife conservation,
    where understanding the nuances of model decisions is paramount. Our experiments
    across various fine-grained image datasets demonstrate that our model competes
    effectively with both current interpretable models and traditional state-of-the-art
    image classification systems. The integration of a hierarchical concept tree with
    an ensemble of classifiers ensures detailed and comprehensible insights into the
    classification process, thereby allowing users to pinpoint the specific visual
    parts and attributes influencing the model’s predictions. This granularity facilitates
    robust debugging, precise error analysis, and the ability to adapt to new or changing
    scenarios, which are often challenging for more rigid models. Moreover, the simplicity
    of the linear classifiers within our ensemble underscores our commitment to maintaining
    an interpretable system that can be easily understood and manipulated by practitioners,
    thereby bridging the gap between complex machine learning models and practical
    applications that require user trust and understanding. Further research in developing
    a comprehensive interpretability evaluation framework is necessary to justify
    our claim and further facilitate the advancement of interpretable vision-language
    models.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的细粒度图像分类方法，显著提高了可解释性而不牺牲性能。我们的方法在现有框架基础上，引入了结构化的分层概念分解，并结合了线性分类器的集成，以澄清视觉-语言模型中的决策过程。这种方法不仅推动了可解释设计模型的发展，还解决了在如野生动物保护等应用中，对模型决策细微差别的理解这一关键需求。我们在各种细粒度图像数据集上的实验表明，我们的模型在与当前可解释模型和传统最先进的图像分类系统的竞争中表现出色。通过将分层概念树与分类器集成，确保了对分类过程的详细和易懂的见解，从而允许用户准确找到影响模型预测的具体视觉部分和属性。这种细致性有助于强健的调试、精确的错误分析，并适应新或变化的场景，这对于更为僵化的模型通常是具有挑战性的。此外，我们集成中的线性分类器的简单性突显了我们致力于保持一个易于理解和操作的可解释系统，从而弥合复杂机器学习模型与需要用户信任和理解的实际应用之间的差距。进一步研究以开发全面的可解释性评估框架是必要的，以证实我们的声明并进一步推动可解释视觉-语言模型的发展。
- en: Limitation
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Our approach utilizes the pretrained CLIP model without fine-tuning, which may
    result in suboptimal performance and not fully leverage the potential of the alignment
    model structure. Additionally, due to limited computational resources, we were
    unable to explore more advanced alignment models like BLIP. Moreover, our experiments
    were conducted on only six domain-specific datasets, which might not provide sufficient
    evidence to generalize our findings widely. We believe that these modifications
    could significantly enhance performance, and further experimentation is needed
    to substantiate this assumption.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法利用了预训练的 CLIP 模型而不进行微调，这可能导致性能次优，无法充分发挥对齐模型结构的潜力。此外，由于计算资源有限，我们无法探索更先进的对齐模型，如
    BLIP。此外，我们的实验仅在六个领域特定的数据集上进行，这可能不足以广泛推广我们的发现。我们相信这些修改可能显著提升性能，进一步实验有助于证实这一假设。
- en: Interpretability in vision-language modeling remains a relatively uncharted
    field, lacking well-established benchmarks or comprehensive evaluation frameworks
    to validate our claims of interpretability. Although we have developed a demonstration
    tool, showcased in Appendix A and soon to be made public on Github, a formally
    recognized and rigorously tested evaluation framework is still needed to support
    our claims definitively.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉-语言建模中的可解释性仍然是一个相对未开发的领域，缺乏成熟的基准或全面的评估框架来验证我们关于可解释性的声明。尽管我们开发了一个演示工具，在附录
    A 中展示，并将在 Github 上公开，但仍需要一个正式认可且经过严格测试的评估框架来确切支持我们的声明。
- en: Throughout the project, we made several structural changes to improve the efficiency
    of storing and utilizing the generated concept trees, transitioning from Python
    dictionaries to JSON trees and finally to Pandas dataframes. The current system,
    based on Pandas dataframes, offers the fastest performance and greatest interpretability
    among the structures we tested. However, we believe that there are additional
    opportunities to enhance efficiency and performance further.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个项目过程中，我们进行了几次结构性变化，以提高存储和利用生成的概念树的效率，从 Python 字典过渡到 JSON 树，最终到 Pandas 数据框。目前，基于
    Pandas 数据框的系统在我们测试的结构中提供了最快的性能和最大的可解释性。然而，我们相信还有进一步提高效率和性能的机会。
- en: 'Due to the significant amount of time used on exploration and narrowing down
    the research topics, we could not run human evaluation studies, which are critical
    for validating the interpretability of our model. Future work will prioritize
    the establishment of human evaluation protocols, dividing them into two categories:
    concept generation and concept classification.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在探索和缩小研究主题上花费了大量时间，我们未能进行关键的人工评估研究，以验证我们模型的可解释性。未来的工作将优先建立人工评估协议，将其分为两个类别：概念生成和概念分类。
- en: In concept generation, we plan to assess the generated concepts on a Likert
    scale for factuality, sufficiency, and compactness. Factuality will measure the
    accuracy of the concept hierarchy relative to the specific subclass. Sufficiency
    will evaluate whether the hierarchy comprehensively represents all visual aspects
    of the subclass. Compactness will scrutinize the hierarchy for any redundancies,
    suggesting improvements where necessary.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念生成方面，我们计划通过利克特量表评估生成的概念的真实性、充分性和简洁性。真实性将衡量概念层次结构相对于特定子类的准确性。充分性将评估该层次结构是否全面代表了子类的所有视觉方面。简洁性将审查层次结构中的任何冗余，并在必要时提出改进建议。
- en: In concept classification, our focus will shift to groundability and consistency
    through pairwise comparisons. Groundability will examine whether the top-$k$ clues
    provided by our classifiers accurately and representatively describe the image
    compared to a baseline model (LaBo). Consistency will assess whether these clues
    maintain their reliability across visually similar images of the same subclass,
    taken from nearly identical angles.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念分类方面，我们将重点关注可基于性和一致性，通过成对比较进行评估。可基于性将检验我们分类器提供的前-$k$ 个线索是否准确且具有代表性地描述了图像，相比于基线模型（LaBo）。一致性将评估这些线索在从几乎相同角度拍摄的同一子类的视觉相似图像中是否保持其可靠性。
- en: However, the challenge of defining and measuring interpretability remains. The
    subjective nature of interpretability necessitates a robust, theoretically sound,
    and empirically backed framework. This ambitious task, suitable for an extended
    research endeavor such as a PhD project, involves developing a comprehensive interpretability
    evaluation framework. As such, while this initial study lays the groundwork, future
    research will be required to advance these efforts and further refine our system.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，定义和衡量可解释性仍然是一个挑战。可解释性的主观性需要一个稳健、理论上合理且有实证支持的框架。这个雄心勃勃的任务，适合于像博士项目这样的长期研究工作，涉及到开发一个全面的可解释性评估框架。因此，虽然这项初步研究奠定了基础，但未来的研究将需要进一步推进这些努力，并进一步完善我们的系统。
- en: Acknowledgments
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This project is the Master’s Thesis of Renyi Qu under the supervision of Prof.
    Mark Yatskar at the University of Pennsylvania.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目是阮易的硕士论文，由宾夕法尼亚大学的马克·亚茨卡教授监督完成。
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bhalla (2022) Usha Bhalla. 2022. Do vision-language pretrained models learn
    primitive concepts?
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhalla（2022）Usha Bhalla. 2022. 视觉-语言预训练模型是否学习了原始概念？
- en: Bossard et al. (2014) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
    2014. Food-101 – mining discriminative components with random forests. In *European
    Conference on Computer Vision*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bossard 等（2014）Lukas Bossard, Matthieu Guillaumin, 和 Luc Van Gool. 2014. Food-101
    – 使用随机森林挖掘区分组件。发表于*欧洲计算机视觉会议*。
- en: Bujwid and Sullivan (2021) Sebastian Bujwid and Josephine Sullivan. 2021. Large-scale
    zero-shot image classification from rich and diverse textual descriptions. *arXiv
    preprint arXiv:2103.09669*.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bujwid 和 Sullivan（2021）Sebastian Bujwid 和 Josephine Sullivan. 2021. 从丰富多样的文本描述中进行大规模零样本图像分类。*arXiv
    预印本 arXiv:2103.09669*。
- en: 'Chen et al. (2023) Zhihong Chen, Guiming Chen, Shizhe Diao, Xiang Wan, and
    Benyou Wang. 2023. On the difference of bert-style and clip-style text encoders.
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    13710–13721.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023）Zhihong Chen、Guiming Chen、Shizhe Diao、Xiang Wan 和 Benyou Wang。2023。BERT
    风格和 CLIP 风格文本编码器的差异。 在 *计算语言学协会年会：ACL 2023*，第 13710–13721 页。
- en: 'Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words:
    Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等人（2020）Alexey Dosovitskiy、Lucas Beyer、Alexander Kolesnikov、Dirk
    Weissenborn、Xiaohua Zhai、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg
    Heigold、Sylvain Gelly 等人。2020。图像值 16x16 个词：大规模图像识别的 Transformers。*arXiv 预印本 arXiv:2010.11929*。
- en: Du et al. (2023) Yifan Du, Junyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong
    Wen. 2023. Zero-shot visual question answering with language model feedback. *arXiv
    preprint arXiv:2305.17006*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人（2023）Yifan Du、Junyi Li、Tianyi Tang、Wayne Xin Zhao 和 Ji-Rong Wen。2023。带有语言模型反馈的零样本视觉问答。*arXiv
    预印本 arXiv:2305.17006*。
- en: 'Ellson et al. (2002) John Ellson, Emden Gansner, Lefteris Koutsofios, Stephen C
    North, and Gordon Woodhull. 2002. Graphviz—open source graph drawing tools. In
    *Graph Drawing: 9th International Symposium, GD 2001 Vienna, Austria, September
    23–26, 2001 Revised Papers 9*, pages 483–484\. Springer.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ellson 等人（2002）John Ellson、Emden Gansner、Lefteris Koutsofios、Stephen C North
    和 Gordon Woodhull。2002。Graphviz—开源图形绘制工具。在 *图形绘制：第九届国际研讨会，GD 2001 维也纳，奥地利，2001年9月23日至26日
    修订论文集 9*，第 483–484 页。Springer。
- en: 'Fu et al. (2023) Xingyu Fu, Sheng Zhang, Gukyeong Kwon, Pramuditha Perera,
    Henghui Zhu, Yuhao Zhang, Alexander Hanbo Li, William Yang Wang, Zhiguo Wang,
    Vittorio Castelli, et al. 2023. Generate then select: Open-ended visual question
    answering guided by world knowledge. *arXiv preprint arXiv:2305.18842*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人（2023）Xingyu Fu、Sheng Zhang、Gukyeong Kwon、Pramuditha Perera、Henghui Zhu、Yuhao
    Zhang、Alexander Hanbo Li、William Yang Wang、Zhiguo Wang、Vittorio Castelli 等人。2023。生成再选择：由世界知识引导的开放式视觉问答。*arXiv
    预印本 arXiv:2305.18842*。
- en: 'Gosiewska et al. (2021) Alicja Gosiewska, Anna Kozak, and Przemysław Biecek.
    2021. Simpler is better: Lifting interpretability-performance trade-off via automated
    feature engineering. *Decision Support Systems*, 150:113556.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gosiewska 等人（2021）Alicja Gosiewska、Anna Kozak 和 Przemysław Biecek。2021。更简单更好：通过自动特征工程提升可解释性与性能的权衡。*决策支持系统*，150:113556。
- en: Gunning and Aha (2019) David Gunning and David Aha. 2019. Darpa’s explainable
    artificial intelligence (xai) program. *AI magazine*, 40(2):44–58.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunning 和 Aha（2019）David Gunning 和 David Aha。2019。DARPA 的可解释人工智能（XAI）计划。*AI
    杂志*，40(2):44–58。
- en: 'Hendricks et al. (2016) Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach,
    Jeff Donahue, Bernt Schiele, and Trevor Darrell. 2016. Generating visual explanations.
    In *Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
    October 11–14, 2016, Proceedings, Part IV 14*, pages 3–19\. Springer.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendricks 等人（2016）Lisa Anne Hendricks、Zeynep Akata、Marcus Rohrbach、Jeff Donahue、Bernt
    Schiele 和 Trevor Darrell。2016。生成视觉解释。在 *计算机视觉–ECCV 2016：第十四届欧洲会议，荷兰阿姆斯特丹，2016年10月11日至14日，会议论文集，第四部分
    14*，第 3–19 页。Springer。
- en: 'Hu et al. (2023) Wenbo Hu, Yifan Xu, Y Li, W Li, Z Chen, and Z Tu. 2023. Bliva:
    A simple multimodal llm for better handling of text-rich visual questions. *arXiv
    preprint arXiv:2308.09936*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2023）Wenbo Hu、Yifan Xu、Y Li、W Li、Z Chen 和 Z Tu。2023。Bliva：一种简单的多模态 LLM
    更好地处理文本丰富的视觉问题。*arXiv 预印本 arXiv:2308.09936*。
- en: Khosla et al. (2011) Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao,
    and Li Fei-Fei. 2011. Novel dataset for fine-grained image categorization. In
    *First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer
    Vision and Pattern Recognition*, Colorado Springs, CO.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosla 等人（2011）Aditya Khosla、Nityananda Jayadevaprakash、Bangpeng Yao 和 Li Fei-Fei。2011。用于细粒度图像分类的新数据集。在
    *第一届细粒度视觉分类研讨会，IEEE 计算机视觉与模式识别会议*，科罗拉多斯普林斯，CO。
- en: Kim et al. (2018) Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and
    Zeynep Akata. 2018. Textual explanations for self-driving vehicles. In *Proceedings
    of the European conference on computer vision (ECCV)*, pages 563–578.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2018）Jinkyu Kim、Anna Rohrbach、Trevor Darrell、John Canny 和 Zeynep Akata。2018。自驾车的文本解释。在
    *欧洲计算机视觉会议（ECCV）论文集*，第 563–578 页。
- en: Koh et al. (2020) Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann,
    Emma Pierson, Been Kim, and Percy Liang. 2020. Concept bottleneck models. In *International
    conference on machine learning*, pages 5338–5348\. PMLR.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh 等人（2020）Pang Wei Koh、Thao Nguyen、Yew Siang Tang、Stephen Mussmann、Emma Pierson、Been
    Kim 和 Percy Liang。2020。概念瓶颈模型。在 *国际机器学习会议*，第 5338–5348 页。PMLR。
- en: Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
    2013. 3d object representations for fine-grained categorization. In *Proceedings
    of the IEEE international conference on computer vision workshops*, pages 554–561.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, 和 Li Fei-Fei.
    2013. 用于细粒度分类的3d对象表示。见 *IEEE国际计算机视觉会议工作坊论文集*，页554–561。
- en: 'Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.
    Blip-2: Bootstrapping language-image pre-training with frozen image encoders and
    large language models. *arXiv preprint arXiv:2301.12597*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, 和 Steven Hoi. 2023.
    Blip-2: 使用冻结图像编码器和大型语言模型引导语言-图像预训练。*arXiv预印本 arXiv:2301.12597*。'
- en: Liu et al. (2024) Mingxuan Liu, Subhankar Roy, Wenjing Li, Zhun Zhong, Nicu
    Sebe, and Elisa Ricci. 2024. Democratizing fine-grained visual recognition with
    large language models. *arXiv preprint arXiv:2401.13837*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024) Mingxuan Liu, Subhankar Roy, Wenjing Li, Zhun Zhong, Nicu
    Sebe, 和 Elisa Ricci. 2024. 利用大型语言模型实现细粒度视觉识别的民主化。*arXiv预印本 arXiv:2401.13837*。
- en: Maji et al. (2013) S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.
    2013. [Fine-grained visual classification of aircraft](https://arxiv.org/abs/1306.5151).
    Technical report.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maji et al. (2013) S. Maji, J. Kannala, E. Rahtu, M. Blaschko, 和 A. Vedaldi.
    2013. [细粒度航空器视觉分类](https://arxiv.org/abs/1306.5151). 技术报告。
- en: Nilsback and Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. 2008.
    Automated flower classification over a large number of classes. In *2008 Sixth
    Indian conference on computer vision, graphics & image processing*, pages 722–729\.
    IEEE.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nilsback 和 Zisserman (2008) Maria-Elena Nilsback 和 Andrew Zisserman. 2008. 大量类别的自动花卉分类。见
    *2008年第六届印度计算机视觉、图形与图像处理会议*，页722–729\. IEEE。
- en: Nishida et al. (2022) Kosuke Nishida, Kyosuke Nishida, and Shuichi Nishioka.
    2022. Improving few-shot image classification using machine-and user-generated
    natural language descriptions. *arXiv preprint arXiv:2207.03133*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nishida et al. (2022) Kosuke Nishida, Kyosuke Nishida, 和 Shuichi Nishioka. 2022.
    使用机器和用户生成的自然语言描述改进少样本图像分类。*arXiv预印本 arXiv:2207.03133*。
- en: Pratt et al. (2023) Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. 2023.
    What does a platypus look like? generating customized prompts for zero-shot image
    classification. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, pages 15691–15701.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pratt et al. (2023) Sarah Pratt, Ian Covert, Rosanne Liu, 和 Ali Farhadi. 2023.
    鸭嘴兽长什么样？生成自定义提示以进行零样本图像分类。见 *IEEE/CVF国际计算机视觉会议论文集*，页15691–15701。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. Learning transferable visual models from natural language
    supervision. In *International conference on machine learning*, pages 8748–8763\.
    PMLR.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, 等. 2021. 从自然语言监督中学习可转移的视觉模型。见 *国际机器学习会议*，页8748–8763\. PMLR。
- en: Roth et al. (2022) Karsten Roth, Oriol Vinyals, and Zeynep Akata. 2022. Integrating
    language guidance into vision-based deep metric learning. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 16177–16189.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roth et al. (2022) Karsten Roth, Oriol Vinyals, 和 Zeynep Akata. 2022. 将语言引导融入基于视觉的深度度量学习。见
    *IEEE/CVF计算机视觉与模式识别会议论文集*，页16177–16189。
- en: Rudin (2019) Cynthia Rudin. 2019. Stop explaining black box machine learning
    models for high stakes decisions and use interpretable models instead. *Nature
    machine intelligence*, 1(5):206–215.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rudin (2019) Cynthia Rudin. 2019. 停止为高风险决策解释黑箱机器学习模型，转而使用可解释的模型。*自然机器智能*, 1(5):206–215。
- en: 'Shen et al. (2022) Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei
    Yang, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, et al. 2022. K-lite:
    Learning transferable visual models with external knowledge. *Advances in Neural
    Information Processing Systems*, 35:15558–15573.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2022) Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei
    Yang, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, 等. 2022. K-lite:
    利用外部知识学习可转移的视觉模型。*神经信息处理系统进展*, 35:15558–15573。'
- en: Singh et al. (2024) Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich
    Caruana, and Jianfeng Gao. 2024. Rethinking interpretability in the era of large
    language models. *arXiv preprint arXiv:2402.01761*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. (2024) Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich
    Caruana, 和 Jianfeng Gao. 2024. 在大型语言模型时代重新思考可解释性。*arXiv预印本 arXiv:2402.01761*。
- en: Singh et al. (2022) Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush,
    and Jianfeng Gao. 2022. Explaining patterns in data with language models via interpretable
    autoprompting. *arXiv preprint arXiv:2210.01848*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等（2022）Chandan Singh、John X Morris、Jyoti Aneja、Alexander M Rush 和 Jianfeng
    Gao。2022年。《通过可解释的自动提示解释数据中的模式》。*arXiv 预印本 arXiv:2210.01848*。
- en: 'Stan et al. (2024) Gabriela Ben Melech Stan, Raanan Yehezkel Rohekar, Yaniv
    Gurwicz, Matthew Lyle Olson, Anahita Bhiwandiwalla, Estelle Aflalo, Chenfei Wu,
    Nan Duan, Shao-Yen Tseng, and Vasudev Lal. 2024. Lvlm-intrepret: An interpretability
    tool for large vision-language models. *arXiv preprint arXiv:2404.03118*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stan 等（2024）Gabriela Ben Melech Stan、Raanan Yehezkel Rohekar、Yaniv Gurwicz、Matthew
    Lyle Olson、Anahita Bhiwandiwalla、Estelle Aflalo、Chenfei Wu、Nan Duan、Shao-Yen Tseng
    和 Vasudev Lal。2024年。《Lvlm-intrepret: 大型视觉语言模型的可解释性工具》。*arXiv 预印本 arXiv:2404.03118*。'
- en: Subramanian et al. (2023) Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar,
    Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan
    Klein. 2023. Modular visual question answering via code generation. *arXiv preprint
    arXiv:2306.05392*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramanian 等（2023）Sanjay Subramanian、Medhini Narasimhan、Kushal Khangaonkar、Kevin
    Yang、Arsha Nagrani、Cordelia Schmid、Andy Zeng、Trevor Darrell 和 Dan Klein。2023年。《通过代码生成的模块化视觉问答》。*arXiv
    预印本 arXiv:2306.05392*。
- en: 'Surís et al. (2023) Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt:
    Visual inference via python execution for reasoning. *arXiv preprint arXiv:2303.08128*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Surís 等（2023）Dídac Surís、Sachit Menon 和 Carl Vondrick。2023年。《Vipergpt: 通过 Python
    执行进行视觉推理》。*arXiv 预印本 arXiv:2303.08128*。'
- en: Wah et al. (2011) C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
    2011. Caltech-ucsd birds-200-2011 (cub-200-2011). Technical Report CNS-TR-2011-001,
    California Institute of Technology.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wah 等（2011）C. Wah、S. Branson、P. Welinder、P. Perona 和 S. Belongie。2011年。《Caltech-ucsd
    birds-200-2011 (cub-200-2011)》。技术报告 CNS-TR-2011-001，加州理工学院。
- en: 'Yang et al. (2023) Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin,
    Chris Callison-Burch, and Mark Yatskar. 2023. Language in a bottle: Language model
    guided concept bottlenecks for interpretable image classification. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    19187–19197.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2023）Yue Yang、Artemis Panagopoulou、Shenghao Zhou、Daniel Jin、Chris Callison-Burch
    和 Mark Yatskar。2023年。《瓶中的语言：语言模型指导的概念瓶颈用于可解释的图像分类》。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，页码
    19187–19197。
- en: 'Zhang et al. (2022) Xiaoge Zhang, Felix TS Chan, and Sankaran Mahadevan. 2022.
    Explainable machine learning in image classification models: An uncertainty quantification
    perspective. *Knowledge-Based Systems*, 243:108418.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）Xiaoge Zhang、Felix TS Chan 和 Sankaran Mahadevan。2022年。《图像分类模型中的可解释机器学习：一种不确定性量化视角》。*Knowledge-Based
    Systems*，243:108418。
- en: 'Zhang et al. (2023) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen
    Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren’s song
    in the ai ocean: a survey on hallucination in large language models. *arXiv preprint
    arXiv:2309.01219*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Yue Zhang、Yafu Li、Leyang Cui、Deng Cai、Lemao Liu、Tingchen Fu、Xinting
    Huang、Enbo Zhao、Yu Zhang、Yulong Chen 等。2023年。《人工智能领域的塞壬之歌：大型语言模型中的幻觉调查》。*arXiv
    预印本 arXiv:2309.01219*。
- en: Appendix A Inference Demo
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 推理演示
- en: The subsequent three pages feature a demonstration of an inference tool developed
    using Streamlit, showcasing its application on a randomly selected Dandelion image.
    The probability assigned by each concept classifier to its prediction is clearly
    displayed. Additionally, the contribution of each attribute value towards the
    prediction made by each concept classifier is transparently presented. While it
    is feasible to render a hierarchical tree in real-time, similar to the format
    depicted in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Concept Tree Decomposition ‣ 3 Method
    ‣ LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained
    Image Classification") and Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Concept Tree Decomposition
    ‣ 3 Method ‣ LLM-based Hierarchical Concept Decomposition for Interpretable Fine-Grained
    Image Classification"), these visualizations are too extensive to be included
    in print within this thesis. The tool’s straightforward visualization and enhanced
    interpretability facilitate easy debugging and inspection. For example, it is
    evident from the visualization that the Anther and Petal classifiers, which predicted
    incorrect labels at a relatively lower probability, highlight potential failure
    modes and areas for improvement. See pages - of [inference_demo.pdf](inference_demo.pdf)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的三页展示了一个使用 Streamlit 开发的推理工具，展示了它在随机选择的蒲公英图像上的应用。每个概念分类器分配给其预测的概率被清晰地显示出来。此外，每个属性值对每个概念分类器做出的预测的贡献也得到了透明的展示。虽然实时呈现分层树结构是可行的，类似于图
    [2](#S3.F2 "Figure 2 ‣ 3.1 Concept Tree Decomposition ‣ 3 Method ‣ LLM-based Hierarchical
    Concept Decomposition for Interpretable Fine-Grained Image Classification") 和图
    [3](#S3.F3 "Figure 3 ‣ 3.1 Concept Tree Decomposition ‣ 3 Method ‣ LLM-based Hierarchical
    Concept Decomposition for Interpretable Fine-Grained Image Classification") 中所示的格式，但这些可视化效果过于庞大，不适合在本论文中打印。该工具的直观可视化和增强的可解释性有助于轻松调试和检查。例如，从可视化中可以明显看出，预测错误标签的花药和花瓣分类器的概率较低，这突出了潜在的失败模式和改进空间。见
    [inference_demo.pdf](inference_demo.pdf) 页。
