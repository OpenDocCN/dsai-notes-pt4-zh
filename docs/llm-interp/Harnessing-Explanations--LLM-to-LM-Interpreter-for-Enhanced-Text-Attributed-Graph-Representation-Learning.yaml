- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 17:35:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 17:35:06'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用解释：用于增强文本属性图表示学习的LLM-to-LM解释器
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19523](https://ar5iv.labs.arxiv.org/html/2305.19523)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19523](https://ar5iv.labs.arxiv.org/html/2305.19523)
- en: Xiaoxin He¹   Xavier Bresson¹   Thomas Laurent²   Adam Perold³   Yann LeCun^(4,5)
      Bryan Hooi¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xiaoxin He¹   Xavier Bresson¹   Thomas Laurent²   Adam Perold³   Yann LeCun^(4,5)
      Bryan Hooi¹
- en: '{xiaoxin, xaviercs, bhooi}@comp.nus.edu.sg, tlaurent@lmu.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '{xiaoxin, xaviercs, bhooi}@comp.nus.edu.sg, tlaurent@lmu.edu'
- en: research@provenance.ai, yann@cs.nyu.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: research@provenance.ai, yann@cs.nyu.edu
- en: ¹National University of Singapore    ²Loyola Marymount University    ³Provenance
    AI
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹新加坡国立大学    ²洛约拉玛丽蒙大学    ³Provenance AI
- en: ⁴New York University    ⁵Meta AI
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴纽约大学    ⁵Meta AI
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Representation learning on text-attributed graphs (TAGs) has become a critical
    research problem in recent years. A typical example of a TAG is a paper citation
    graph, where the text of each paper serves as node attributes. Initial graph neural
    network (GNN) pipelines handled these text attributes by transforming them into
    shallow or hand-crafted features, such as skip-gram or bag-of-words features.
    Recent efforts have focused on enhancing these pipelines with language models
    (LMs), which typically demand intricate designs and substantial computational
    resources. With the advent of powerful large language models (LLMs) such as GPT
    or Llama2, which demonstrate an ability to reason and to utilize general knowledge,
    there is a growing need for techniques which combine the textual modelling abilities
    of LLMs with the structural learning capabilities of GNNs. Hence, in this work,
    we focus on leveraging LLMs to capture textual information as features, which
    can be used to boost GNN performance on downstream tasks. A key innovation is
    our use of *explanations as features*: we prompt an LLM to perform zero-shot classification,
    request textual explanations for its decision-making process, and design an *LLM-to-LM
    interpreter* to translate these explanations into informative features that enhance
    downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art
    results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv,
    as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method
    significantly speeds up training, achieving a 2.88 times improvement over the
    closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed
    method extends beyond TAGs and holds the potential to enhance other tasks involving
    graph-text data ¹¹1Our codes and datasets are available at: [https://github.com/XiaoxinHe/TAPE](https://github.com/XiaoxinHe/TAPE).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在近年来，文本属性图（TAGs）上的表示学习已成为一个关键的研究问题。TAG的一个典型例子是论文引用图，其中每篇论文的文本作为节点属性。最初的图神经网络（GNN）管道通过将这些文本属性转化为浅层或手工制作的特征（如跳字模型或词袋模型特征）来处理这些文本属性。最近的努力集中在通过语言模型（LMs）来增强这些管道，这通常需要复杂的设计和大量的计算资源。随着强大的大型语言模型（LLMs）如GPT或Llama2的出现，这些模型展示了推理和利用通用知识的能力，因此越来越需要将LLMs的文本建模能力与GNNs的结构学习能力相结合的技术。因此，在这项工作中，我们专注于利用LLMs来捕捉文本信息作为特征，这些特征可以用来提升GNN在下游任务中的表现。一个关键的创新是我们使用*解释作为特征*：我们提示LLM执行零样本分类，要求其提供决策过程的文本解释，并设计一个*LLM-to-LM解释器*来将这些解释转化为增强下游GNN的有用特征。我们的实验表明，我们的方法在公认的TAG数据集上取得了最先进的结果，包括Cora、PubMed、ogbn-arxiv以及我们新引入的数据集tape-arxiv23。此外，我们的方法显著加快了训练速度，在ogbn-arxiv上比最接近的基线提高了2.88倍。最后，我们认为该方法的通用性超越了TAGs，并有潜力提升其他涉及图-文本数据的任务。¹¹1我们的代码和数据集可在：[https://github.com/XiaoxinHe/TAPE](https://github.com/XiaoxinHe/TAPE)。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Many real-world graphs possess textual information, and are often referred to
    text-attributed graphs [[37](#bib.bibx37)]. In TAGs, nodes typically represent
    text entities, such as documents or sentences, while edges signify relationships
    between these entities. For example, the ogbn-arxiv dataset [[13](#bib.bibx13)]
    represents a citation network in TAG form, where each node corresponds to a paper,
    with its title and abstract serving as node attributes. More generally, the combination
    of textual attributes with graph topology provides a rich source of information,
    significantly enhancing representation learning for important applications, such
    as text classification [[36](#bib.bibx36), [34](#bib.bibx34), [40](#bib.bibx40),
    [3](#bib.bibx3), [45](#bib.bibx45)], recommendation systems [[48](#bib.bibx48)],
    social networks, and fake news detection [[20](#bib.bibx20)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的图形包含文本信息，通常被称为文本属性图（TAGs）[[37](#bib.bibx37)]。在TAGs中，节点通常表示文本实体，例如文档或句子，而边则表示这些实体之间的关系。例如，ogbn-arxiv数据集
    [[13](#bib.bibx13)] 表示一个以TAG形式存在的引用网络，其中每个节点对应一篇论文，其标题和摘要作为节点属性。更一般来说，文本属性与图形拓扑的结合提供了丰富的信息来源，显著提升了对重要应用的表示学习，如文本分类
    [[36](#bib.bibx36)、[34](#bib.bibx34)、[40](#bib.bibx40)、[3](#bib.bibx3)、[45](#bib.bibx45)]、推荐系统
    [[48](#bib.bibx48)]、社交网络，以及虚假新闻检测 [[20](#bib.bibx20)]。
- en: '![Refer to caption](img/6db2689fbd5fc8086c3a2bf7da432030.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6db2689fbd5fc8086c3a2bf7da432030.png)'
- en: 'Figure 1: Overview of our framework leveraging large language models (LLMs)
    to enhance representation learning on TAGs. First, the text attributes associated
    with each node, *i.e.,* title and abstract, are wrapped in a custom prompt (green
    box) and used to query the LLM, here GPT-3.5 [[1](#bib.bibx1)], which generates
    a ranked prediction list and explanation (yellow box). Next, the original text,
    predictions, and explanation are used to fine-tune an language model (LM), here
    DeBERTa [[12](#bib.bibx12)], then transformed into vectorial node features. Finally,
    these enriched node features, *i.e.,* $h_{\textrm{orig}}$, $h_{\textrm{expl}}$
    and $h_{\textrm{pred}}$, are used in any downstream GNN, *e.g.,* RevGAT [[17](#bib.bibx17)]
    to predict unknown node classes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们的框架概述利用大语言模型（LLMs）来增强对TAGs的表示学习。首先，与每个节点相关的文本属性，*即*标题和摘要，被封装在自定义提示中（绿色框），并用来查询LLM，这里是GPT-3.5
    [[1](#bib.bibx1)]，它生成一个排名预测列表和解释（黄色框）。接着，原始文本、预测和解释被用来微调一个语言模型（LM），这里是DeBERTa
    [[12](#bib.bibx12)]，然后转化为向量化的节点特征。最后，这些丰富的节点特征，*即* $h_{\textrm{orig}}$、$h_{\textrm{expl}}$
    和 $h_{\textrm{pred}}$，被用于任何下游GNN，*例如* RevGAT [[17](#bib.bibx17)] 来预测未知节点类别。
- en: 'Representation learning on TAGs. Prior research has explored various approaches
    for representation learning on TAGs. The standard GNN pipeline (illustrated in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning") in light
    yellow), first encodes the textual attributes of each node using shallow or hand-crafted
    features such as skip-gram [[22](#bib.bibx22)] or bag-of-words (BoW) [[11](#bib.bibx11)]
    (refer to Table [5](#A3.T5 "Table 5 ‣ C.3 Shallow Embedding Methods for Node Feature
    Extraction ‣ Appendix C Dataset ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning")). The resulting node
    features are then used as input for a GNN. For instance, the Open Graph Benchmark
    (OGB) [[13](#bib.bibx13)] generated BoW and skip-gram [[22](#bib.bibx22)] features
    for the ogbn-products and ogbn-arxiv datasets respectively. These processed features
    are readily available within popular graph libraries, such as PyTorch Geometric
    (PyG) [[8](#bib.bibx8)] and Deep Graph Library (DGL) [[33](#bib.bibx33)], and
    have been widely used by the graph community. However, these shallow text embeddings
    are limited in the complexity of the semantic features they can capture, especially
    when compared to approaches based on multi-layer LMs.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '对 TAG 的表征学习。之前的研究探讨了多种对 TAG 进行表征学习的方法。标准的 GNN 流程（如图 [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") 中的浅黄色所示），首先使用浅层或手工制作的特征，例如 skip-gram [[22](#bib.bibx22)]
    或 bag-of-words (BoW) [[11](#bib.bibx11)]，对每个节点的文本属性进行编码（参见表 [5](#A3.T5 "Table
    5 ‣ C.3 Shallow Embedding Methods for Node Feature Extraction ‣ Appendix C Dataset
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")）。得到的节点特征随后作为 GNN 的输入。例如，Open Graph Benchmark (OGB)
    [[13](#bib.bibx13)] 为 ogbn-products 和 ogbn-arxiv 数据集分别生成了 BoW 和 skip-gram [[22](#bib.bibx22)]
    特征。这些处理后的特征在流行的图形库中可以方便地获取，例如 PyTorch Geometric (PyG) [[8](#bib.bibx8)] 和 Deep
    Graph Library (DGL) [[33](#bib.bibx33)]，并被图形社区广泛使用。然而，这些浅层文本嵌入在捕捉语义特征的复杂性方面有限，特别是与基于多层
    LMs 的方法相比。'
- en: 'LM-based pipeline for TAGs. Recent works have therefore focused on designing
    LM-based techniques to better capture the context and nuances of text within TAGs [[3](#bib.bibx3),
    [45](#bib.bibx45), [6](#bib.bibx6)]. In this approach, pre-trained LMs are fine-tuned
    and used to generate node embeddings that are tailored to the specific TAG tasks
    (depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning")
    in light gray). For example, [[3](#bib.bibx3)] fine-tuned an LM using a neighborhood
    prediction task, while [[45](#bib.bibx45)] fine-tuned an LM to predict the label
    distribution from a GNN’s outputs. LM-based models have achieved state-of-the-art
    (SOTA) results in node classification on ogbn-arxiv and ogbn-products [[45](#bib.bibx45)].
    However, these works typically entail intricate designs and demand substantial
    computational resources. Furthermore, for scalability reasons, existing works
    mostly rely on relatively small LMs, such as BERT [[5](#bib.bibx5)] and DeBERTa [[12](#bib.bibx12)],
    and thus lack the complex reasoning abilities associated with larger language
    models.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '基于 LM 的 TAG 流程。最近的研究因此集中于设计基于 LM 的技术，以更好地捕捉 TAG 中文本的上下文和细微差别 [[3](#bib.bibx3),
    [45](#bib.bibx45), [6](#bib.bibx6)]。在这种方法中，预训练的 LMs 会经过微调并用于生成针对特定 TAG 任务的节点嵌入（如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning") 中的浅灰色所示）。例如，[[3](#bib.bibx3)]
    使用邻域预测任务对 LM 进行微调，而 [[45](#bib.bibx45)] 对 LM 进行微调以预测 GNN 输出的标签分布。基于 LM 的模型在 ogbn-arxiv
    和 ogbn-products 的节点分类任务中取得了最先进的 (SOTA) 结果 [[45](#bib.bibx45)]。然而，这些工作通常涉及复杂的设计，并且需要大量的计算资源。此外，由于可扩展性的原因，现有工作大多依赖于相对较小的
    LMs，如 BERT [[5](#bib.bibx5)] 和 DeBERTa [[12](#bib.bibx12)]，因此缺乏与更大语言模型相关的复杂推理能力。'
- en: Large Language Models. The advent of large pre-trained models, exemplified by
    GPT [[1](#bib.bibx1)], has revolutionized the field of language modeling. LLMs
    have notably enhanced performance across various natural language processing (NLP)
    tasks, and enabled sophisticated language processing capabilities such as complex
    and zero-shot reasoning. Furthermore, scaling laws [[15](#bib.bibx15)] have revealed
    predictable rules for performance improvements with model and training data size.
    Additionally, LLMs have exhibited “emergent abilities” that were not explicitly
    trained for, such as arithmetic, multi-step reasoning and instruction following [[35](#bib.bibx35)].
    While LLMs have found new success in domains like computer vision [[28](#bib.bibx28)],
    their potential benefits when applied to TAG tasks remain largely uncharted. This
    presents an exciting and promising avenue for future research, and it is precisely
    this untapped potential that we aim to explore in this work.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型。大型预训练模型的出现，以GPT[[1](#bib.bibx1)]为例，彻底改变了语言建模领域。LLMs显著提高了各种自然语言处理（NLP）任务的性能，并使得复杂和零样本推理等高级语言处理能力成为可能。此外，规模法则[[15](#bib.bibx15)]揭示了模型和训练数据规模对性能提升的可预测规则。此外，LLMs还展现了“涌现能力”，这些能力并未被明确训练，例如算术、多步骤推理和指令跟随[[35](#bib.bibx35)]。虽然LLMs在计算机视觉等领域取得了新的成功[[28](#bib.bibx28)]，但它们在TAG任务中的潜在益处仍然基本未被探索。这为未来的研究提供了一个令人兴奋且充满前景的方向，正是这种未被开发的潜力是我们在这项工作中旨在探索的。
- en: LMs vs. LLMs. In this paper, we make a clear distinction between “LMs” and “LLMs”.
    We use LMs to refer to relatively small language models that can be trained and
    fine-tuned within the constraints of an academic lab budget. We refer to LLMs
    as very large language models that are capable of learning significantly more
    complex linguistic patterns than LMs, such as GPT-3/4\. These models typically
    have tens or hundreds of billions of parameters and require substantial computational
    resources to train and use, *e.g.,* GPT-3 was trained on a supercomputer with
    10,000 GPUs. The size and complexity of recent LLMs have raised concerns about
    their scalability, as they can be too large even to run inference on the machines
    typically available within academic research labs. To address this issue, LLMs
    are often made accessible through language modeling as a service (LMaaS) [[26](#bib.bibx26)].
    This approach enables developers to harness the power of LLMs without necessitating
    extensive computational resources or specialized expertise. In the context of
    this paper, one of our primary objectives is to extract information from an LLM
    in a LMaaS-compatible manner. As a result, we do not require fine-tuning the LLM
    or extracting its logits; rather, we focus solely on obtaining its output in textual
    form. In contrast, existing LM-based techniques [[3](#bib.bibx3), [45](#bib.bibx45),
    [6](#bib.bibx6)] are not directly compatible with LLMs, as they require fine-tuning
    of LMs, as well as accessing their latent embeddings or logits, which GPT-3/4
    do not provide. Consequently, to the best of our knowledge, the use of LLMs in
    TAG tasks remains an unexplored area.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（LMs）与大型语言模型（LLMs）。在本文中，我们明确区分了“LMs”和“LLMs”。我们使用LMs来指代那些可以在学术实验室预算范围内进行训练和微调的相对较小的语言模型。我们将LLMs指为那些能够学习比LMs更复杂的语言模式的非常大的语言模型，例如GPT-3/4。这些模型通常拥有数十亿或数百亿个参数，并且需要大量计算资源来训练和使用，*例如*，GPT-3是在配备10,000个GPU的超级计算机上进行训练的。最近LLMs的规模和复杂性引发了对其可扩展性的担忧，因为它们甚至可能过大，以至于无法在通常在学术研究实验室中可用的机器上运行推理。为了解决这个问题，LLMs通常通过语言建模即服务（LMaaS）[[26](#bib.bibx26)]提供。这种方法使开发者能够利用LLMs的强大功能，而无需大量计算资源或专业知识。在本文的背景下，我们的主要目标之一是以LMaaS兼容的方式从LLM中提取信息。因此，我们不需要对LLM进行微调或提取其logits；而是专注于以文本形式获取其输出。相比之下，现有的基于LM的方法[[3](#bib.bibx3),
    [45](#bib.bibx45), [6](#bib.bibx6)]与LLMs不直接兼容，因为它们需要对LMs进行微调，并且访问其潜在的嵌入或logits，而GPT-3/4并不提供这些。因此，据我们所知，在TAG任务中使用LLMs仍然是一个未被探索的领域。
- en: Preliminary study. To assess the potential of LLMs in enhancing representation
    learning for TAGs, we conducted an initial investigation into leveraging GPT-3.5
    for zero-shot classification on the ogbn-arxiv dataset. Using task-specific prompts
    consisting of paper titles, abstracts, and questions, GPT-3.5 achieved a promising
    accuracy of 73.5%, along with high-quality text explanations, surpassing several
    fully trained GNN baselines like RevGAT [[17](#bib.bibx17)] with OGB features
    (70.8% accuracy), but falling short of the SOTA accuracy of 76.6% [[45](#bib.bibx45)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 初步研究。为了评估LLMs在增强TAGs表示学习中的潜力，我们进行了初步调查，利用GPT-3.5在ogbn-arxiv数据集上进行零样本分类。使用包含论文标题、摘要和问题的任务特定提示，GPT-3.5实现了73.5%的令人鼓舞的准确率，并提供了高质量的文本解释，超越了几种完全训练的GNN基线模型，如RevGAT
    [[17](#bib.bibx17)]（OGB特征的准确率为70.8%），但未能达到SOTA的76.6%准确率 [[45](#bib.bibx45)]。
- en: 'The present work: LLM augmentation using explanations. We introduce a novel
    framework that leverages LLMs to improve representation learning on TAGs. A key
    innovation is the concept of *explanations as features*. By prompting a powerful
    LLM to explain its predictions, we extract its relevant prior knowledge and reasoning
    steps, making this information digestible for smaller models, akin to how human
    experts use explanations to convey insights. To illustrate this concept further,
    observe in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning")
    that the explanations (in the yellow box) highlight and expand upon key crucial
    information from the text, such as “deep learning techniques such as DeconvNet,”
    and the relationship between text recognition and information retrieval. These
    explanations draw from the LLM’s general knowledge and serve as valuable features
    for enhancing subsequent TAG pipeline phases. In practice, we design a tailored
    prompt to query an LLM such as GPT or Llama2 to generate both a *ranked prediction
    list* and a *textual explanation* for its predictions. These predictions and explanations
    are then transformed into informative node features through fine-tuning a smaller
    LM such as DeBERTa [[12](#bib.bibx12)] for the target task, providing tailored
    features for any downstream GNNs. This smaller model acts as an interpreter, facilitating
    seamless communication between the LLM (handling text) and the GNN (managing vectorial
    representation).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '当前工作：使用解释增强LLM。我们引入了一种新颖的框架，利用LLMs来改善TAGs上的表示学习。一个关键的创新是*解释作为特征*的概念。通过提示强大的LLM解释其预测，我们提取了其相关的先前知识和推理步骤，使这些信息对较小的模型易于消化，类似于人类专家使用解释传达见解。进一步说明这一概念，参见图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Harnessing Explanations: LLM-to-LM Interpreter for
    Enhanced Text-Attributed Graph Representation Learning")，解释（在黄色框中）突出并扩展了文本中的关键重要信息，例如“深度学习技术如DeconvNet”，以及文本识别和信息检索之间的关系。这些解释来自LLM的一般知识，并作为增强后续TAG管道阶段的有价值特征。在实践中，我们设计了一个量身定制的提示来查询LLM，如GPT或Llama2，以生成*排序预测列表*和*文本解释*。这些预测和解释随后通过微调较小的LM（如DeBERTa
    [[12](#bib.bibx12)]）为目标任务，转化为信息丰富的节点特征，为任何下游GNN提供量身定制的特征。这个较小的模型充当解释器，促进LLM（处理文本）与GNN（管理向量表示）之间的无缝沟通。'
- en: '![Refer to caption](img/54a92bac8996e0607d154875cb41ab34.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54a92bac8996e0607d154875cb41ab34.png)'
- en: 'Figure 2: The performance trade-off between node classification accuracy and
    total training time on ogbn-arxiv [[13](#bib.bibx13)] for various training approaches
    that combine language models (LMs) and graph neural networks (GNNs). The experiment
    employs DeBERTa-base [[12](#bib.bibx12)] as the LM backbone and RevGAT [[17](#bib.bibx17)]
    as the GNN backbone, with the size of the marker indicating the number of parameters.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同训练方法在ogbn-arxiv [[13](#bib.bibx13)]上节点分类准确率与总训练时间之间的性能权衡，这些方法结合了语言模型（LMs）和图神经网络（GNNs）。实验使用DeBERTa-base
    [[12](#bib.bibx12)]作为LM骨干网，RevGAT [[17](#bib.bibx17)]作为GNN骨干网，标记的大小表示参数数量。
- en: 'Our contributions are summarized as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Novel LMaaS-compatible approach. We propose the first LMaaS-compatible approach,
    to the best of our knowledge, for leveraging LLMs to enhance representation learning
    on TAGs. Our innovations involve extracting explanations from an LLM, here GPT-3.5
    and Llama2, and subsequently employing an LLM-to-LM interpreter to translate textual
    explanations into enriched node vector representations for downstream GNNs. Our
    approach improves modularity and efficiency compared to prior LM+GNN models.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新颖的 LMaaS 兼容方法。我们提出了首个 LMaaS 兼容的方法，据我们所知，旨在利用 LLMs 增强 TAGs 上的表示学习。我们的创新涉及从 LLM（这里是
    GPT-3.5 和 Llama2）中提取解释，然后使用 LLM-to-LM 解释器将文本解释转换为丰富的节点向量表示，以供下游 GNN 使用。与之前的 LM+GNN
    模型相比，我们的方法在模块化和效率方面有所改进。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SOTA performance. Extensive experiments demonstrate that our method significantly
    boost the performance of various GNN models across diverse datasets. Notably,
    we achieve top-1 performance on ogbn-arxiv with significantly lower computation
    time, *i.e.,* $2.88\times$ faster than GLEM, and also excel in the TAG versions
    of PubMed and Cora datasets.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**SOTA** 性能。大量实验表明，我们的方法显著提升了各种 GNN 模型在不同数据集上的表现。特别是，我们在 ogbn-arxiv 数据集上实现了顶级性能，计算时间显著更短，*即*
    比 GLEM 快了 $2.88\times$，同时在 PubMed 和 Cora 数据集的 TAG 版本中也表现出色。'
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data contribution. We provide open-source access to our codes, pre-trained networks
    and enriched features. Additionally, recognizing the absence of raw text data
    for Cora and PubMed in common repositories (*e.g.,* PyG, DGL), we have collected
    and released these datasets in TAG format. Furthermore, we introduce the new tape-arxiv23
    citation graph dataset, extending beyond GPT-3’s knowledge cutoff, *i.e.,* Sept.
    2021\. These datasets can serve as valuable resources for the NLP and GNN research
    community.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据贡献。我们提供了代码、预训练网络和丰富特征的开源访问权限。此外，鉴于常见资源库中缺少 Cora 和 PubMed 的原始文本数据（*例如*，PyG,
    DGL），我们已经收集并发布了这些数据集的 TAG 格式。进一步地，我们引入了新的 tape-arxiv23 引文图数据集，扩展至 GPT-3 知识截止日期之后的
    *即* 2021 年 9 月。这些数据集可以作为 NLP 和 GNN 研究社区的宝贵资源。
- en: 2 Related Work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Shallow embedding pipeline for TAGs. In the context of learning representations
    on TAGs, a common approach involves combining graph-based learning with language
    modeling techniques. One prevalent strategy is to transform text attributes into
    shallow or hand-crafted features, such as skip-gram [[22](#bib.bibx22)] or BoW [[11](#bib.bibx11)]
    features. Detailed information is available in Table [5](#A3.T5 "Table 5 ‣ C.3
    Shallow Embedding Methods for Node Feature Extraction ‣ Appendix C Dataset ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"). These engineered features can then be fed as inputs to a graph-based
    learning algorithm, such as a graph convolutional network (GCN) [[16](#bib.bibx16)],
    which learns embeddings capturing the graph structure while incorporating the
    extracted text features. Shallow embedding methods are widely used in the graph
    community due to their simplicity and computational efficiency, such as for designing
    GNN architectures [[30](#bib.bibx30), [2](#bib.bibx2), [29](#bib.bibx29), [43](#bib.bibx43)]
    or benchmarking graph learning [[38](#bib.bibx38), [13](#bib.bibx13)]. However,
    they may have limitations in capturing complex semantic relationships and fully
    leveraging the richness of text attributes, particularly in scenarios involving
    intricate semantic relationships and contextual information.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'TAGs 的浅层嵌入管道。在 TAGs 上学习表示的背景下，一种常见的方法是将图基学习与语言建模技术结合。一个流行的策略是将文本属性转换为浅层或手工制作的特征，例如
    skip-gram [[22](#bib.bibx22)] 或 BoW [[11](#bib.bibx11)] 特征。详细信息见表 [5](#A3.T5 "Table
    5 ‣ C.3 Shallow Embedding Methods for Node Feature Extraction ‣ Appendix C Dataset
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")。这些工程特征可以作为输入提供给基于图的学习算法，例如图卷积网络 (GCN) [[16](#bib.bibx16)]，GCN
    学习捕捉图结构的嵌入，同时结合提取的文本特征。浅层嵌入方法因其简单性和计算效率在图社区中得到广泛应用，例如用于设计 GNN 架构 [[30](#bib.bibx30),
    [2](#bib.bibx2), [29](#bib.bibx29), [43](#bib.bibx43)] 或基准测试图学习 [[38](#bib.bibx38),
    [13](#bib.bibx13)]。然而，它们可能在捕捉复杂语义关系和充分利用文本属性的丰富性方面存在局限性，特别是在涉及复杂语义关系和上下文信息的场景中。'
- en: LM-based pipeline for TAGs. To overcome the limitations of shallow embedding
    approaches, researchers have explored deep embedding techniques by fine-tuning
    pre-trained LMs, such as BERT [[5](#bib.bibx5)], to generate node embeddings that
    are specifically adapted to the domain and context of the TAGs. These deep embeddings
    effectively capture the semantic richness of text attributes, leading to improved
    performance on various TAG-related tasks. Integrating LM-based embeddings and
    graph-based learning can be done through different approaches. One approach is
    to use a cascaded architecture, where the node features are first encoded independently
    by the LMs, and then fed into GNN models. This representation paradigm has been
    widely adopted in subsequent works, such as TextGNN [[48](#bib.bibx48)], GIANT [[3](#bib.bibx3)],
    GPT-GNN [[14](#bib.bibx14)], SimTeg [[7](#bib.bibx7)], as well as in studies related
    to knowledge graphs [[39](#bib.bibx39), [44](#bib.bibx44)] and fact verification [[20](#bib.bibx20),
    [47](#bib.bibx47)] that are beyond the scope of this work. An alternative approach
    involves fusing text encoding and graph aggregation into an iterative workflow,
    enabling the model to refine both the text representations and the node embeddings
    simultaneously, such as Graphormer [[37](#bib.bibx37)], DRAGON [[41](#bib.bibx41)],
    and GLEM [[45](#bib.bibx45)], to name a few.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LM的TAG管道。为了克服浅层嵌入方法的局限性，研究人员通过微调预训练的LMs（如BERT [[5](#bib.bibx5)]）来生成特别适应TAG领域和上下文的节点嵌入。这些深度嵌入有效地捕捉了文本属性的语义丰富性，从而在各种TAG相关任务中表现出色。将基于LM的嵌入与基于图的学习集成可以通过不同的方法实现。一种方法是使用级联架构，其中节点特征首先由LMs独立编码，然后输入GNN模型。这种表示范式已在随后的工作中得到广泛采用，如TextGNN [[48](#bib.bibx48)]、GIANT [[3](#bib.bibx3)]、GPT-GNN [[14](#bib.bibx14)]、SimTeg [[7](#bib.bibx7)]，以及与知识图谱相关的研究 [[39](#bib.bibx39),
    [44](#bib.bibx44)]和事实验证 [[20](#bib.bibx20), [47](#bib.bibx47)]，这些都超出了本工作的范围。另一种方法涉及将文本编码和图聚合融合到一个迭代工作流程中，使模型能够同时优化文本表示和节点嵌入，例如Graphormer [[37](#bib.bibx37)]、DRAGON [[41](#bib.bibx41)]和GLEM [[45](#bib.bibx45)]等。
- en: LLM-based pipeline for TAGs. Incorporating LLMs into TAG tasks presents a promising
    frontier. LLMs such as ChatGPT [[1](#bib.bibx1)] by OpenAI, PaLM [[4](#bib.bibx4)]
    by Google, and LLaMA [[27](#bib.bibx27)] by Meta, have demonstrated their effectiveness
    across a spectrum of NLP tasks. However, their potential benefits for TAG tasks
    have yet to be fully explored. While some recent research efforts have sought
    to evaluate the capacity of LLMs in understanding graph-structured data and enhance
    their graph processing capabilities [[31](#bib.bibx31), [42](#bib.bibx42), [9](#bib.bibx9)],
    these endeavors, while valuable, may not be directly aligned with our specific
    focus on TAGs. By exploring LLM-based methods designed specifically for TAGs,
    we can unlock new possibilities for improving TAG prediction performance and advancing
    our understanding of text attributes within graph-based data. Notably, our initial
    attempt has already inspired further research endeavors in this direction.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的TAG管道。将LLM应用于TAG任务是一个有前景的领域。诸如OpenAI的ChatGPT [[1](#bib.bibx1)]、Google的PaLM [[4](#bib.bibx4)]和Meta的LLaMA [[27](#bib.bibx27)]等LLM已经在各种NLP任务中展示了其有效性。然而，它们在TAG任务中的潜在好处尚未得到充分探索。尽管一些最近的研究努力试图评估LLM在理解图结构数据方面的能力，并增强其图处理能力 [[31](#bib.bibx31),
    [42](#bib.bibx42), [9](#bib.bibx9)]，这些努力虽然有价值，但可能并不直接与我们对TAG的具体关注对接。通过探索专门针对TAG设计的LLM方法，我们可以开辟新的可能性，以提高TAG预测性能，并推动我们对图数据中文本属性的理解。值得注意的是，我们的初步尝试已经激发了该方向上的进一步研究工作。
- en: 3 Formalization
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 正式化
- en: In this section, we introduce notation and formalize some concepts related to
    language models, large language models, and graph neural networks for node classification
    on TAGs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了与语言模型、大型语言模型以及用于TAG节点分类的图神经网络相关的符号和一些概念的正式化。
- en: Text-attributed graphs. Formally, a TAG can be represented as $\mathcal{G}=(\mathcal{V},A,\{s_{n}\}_{n\in\mathcal{V}})$,
    where $\mathcal{V}$ is a set of $N$ nodes, $A\in\mathbb{R}^{N\times N}$ is the
    adjacency matrix, and $s_{n}\in\mathcal{D}^{L_{n}}$ is a sequential text associated
    with node $n\in\mathcal{V}$, with $\mathcal{D}$ as the words or tokens dictionary,
    and $L_{n}$ as the sequence length. In this paper, we investigate node classification
    on TAGs. Specifically, given some labeled nodes $\mathcal{L}\subset\mathcal{V}$
    , the goal is to predict the labels of the remaining unlabeled nodes $\mathcal{U}=\mathcal{V}\setminus\mathcal{L}$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 带文本属性的图。形式上，TAG 可以表示为 $\mathcal{G}=(\mathcal{V},A,\{s_{n}\}_{n\in\mathcal{V}})$，其中
    $\mathcal{V}$ 是 $N$ 个节点的集合，$A\in\mathbb{R}^{N\times N}$ 是邻接矩阵，而 $s_{n}\in\mathcal{D}^{L_{n}}$
    是与节点 $n\in\mathcal{V}$ 关联的序列文本，$\mathcal{D}$ 为词汇或标记词典，$L_{n}$ 为序列长度。在本文中，我们研究了在
    TAG 上的节点分类。具体来说，给定一些标记节点 $\mathcal{L}\subset\mathcal{V}$，目标是预测其余未标记节点 $\mathcal{U}=\mathcal{V}\setminus\mathcal{L}$
    的标签。
- en: 'Language models for text classification. In the context of TAGs, LMs can be
    employed to encode the text attributes associated with each node and learn a representation
    that captures the semantic meaning of the text. Let $s_{n}\in\mathcal{D}^{L_{n}}$
    denote the text attributes of node $n$, and LM be a pre-trained network, such
    as BERT [[5](#bib.bibx5)] or DeBERTa [[12](#bib.bibx12)]. Then, the text attributes
    of node $n$ can be encoded by applying the LM to $s_{n}$ as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本分类的语言模型。在 TAG 的背景下，可以使用 LMs 来编码与每个节点关联的文本属性，并学习一个捕捉文本语义的表示。设 $s_{n}\in\mathcal{D}^{L_{n}}$
    表示节点 $n$ 的文本属性，而 LM 是一个预训练的网络，例如 BERT [[5](#bib.bibx5)] 或 DeBERTa [[12](#bib.bibx12)]。然后，通过将
    LM 应用到 $s_{n}$ 上，可以对节点 $n$ 的文本属性进行编码，如下所示：
- en: '|  | $h_{n}=\textrm{LM}(s_{n})\in\mathbb{R}^{d},$ |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{n}=\textrm{LM}(s_{n})\in\mathbb{R}^{d},$ |  | (1) |'
- en: where $h_{n}$ is the output of the LM, and $d$ is the dimension of the output
    vector.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{n}$ 是 LM 的输出，$d$ 是输出向量的维度。
- en: To perform node classification, the output is employed as input to a classifier,
    such as a logistic regression or a neural network. The goal is to learn a function
    that maps the encoded text attributes to the corresponding node labels.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行节点分类，输出作为分类器的输入使用，例如逻辑回归或神经网络。目标是学习一个函数，将编码的文本属性映射到相应的节点标签。
- en: Large language models and prompting. LLMs have introduced a new paradigm for
    task-adaptation known as “pre-train, prompt, and predict”, replacing the traditional
    “pre-train, fine-tune” procedure. In this paradigm, the LLM is first pre-trained
    on a large corpus of text data to learn general language representations. Then,
    rather than fine-tuning the model on task-specific labeled data, the model is
    prompted with a natural language prompt that specifies the task and context, and
    the model generates the output directly based on the prompt and the input [[19](#bib.bibx19)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型和提示。LLMs 引入了一种新的任务适应范式，称为“预训练、提示和预测”，取代了传统的“预训练、微调”过程。在这一范式中，LLM 首先在大量文本数据上进行预训练，以学习一般语言表示。然后，与其在任务特定的标注数据上进行微调不同，该模型通过自然语言提示来指定任务和上下文，并根据提示和输入直接生成输出 [[19](#bib.bibx19)]。
- en: 'The prompt can take various forms, such as a single sentence or a longer passage,
    and can include additional information or constraints to guide the model’s behavior.
    Let $\mathcal{M}$ be an LLM that takes as input a sequence of tokens $x=(x_{1},x_{2},\ldots,x_{q})$
    and produces as output a sequence of tokens $y=(y_{1},y_{2},\ldots,y_{m})$. The
    model $\mathcal{M}$ is typically trained to optimize a conditional probability
    distribution $p(y|x)$, which assigns a probability to each possible output sequence
    $y$ given $x$. To include a prompt $p$ with the input sequence $x$, we can concatenate
    them into a new sequence $\hat{x}=(p,x_{1},x_{2},\ldots,x_{q})$. We then use $\hat{x}$
    to compute the conditional probability distribution $p(y|\hat{x})$. Formally,
    the probability of the output sequence $y$ given $\hat{x}$ is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提示可以采取各种形式，如单句或较长的段落，并可以包含额外的信息或约束来引导模型的行为。设 $\mathcal{M}$ 为一个 LLM，它接收一个标记序列
    $x=(x_{1},x_{2},\ldots,x_{q})$ 作为输入，并生成一个标记序列 $y=(y_{1},y_{2},\ldots,y_{m})$ 作为输出。模型
    $\mathcal{M}$ 通常被训练以优化条件概率分布 $p(y|x)$，该分布为给定 $x$ 的每个可能输出序列 $y$ 分配概率。为了将提示 $p$
    包含在输入序列 $x$ 中，我们可以将它们连接成一个新序列 $\hat{x}=(p,x_{1},x_{2},\ldots,x_{q})$。然后，我们使用 $\hat{x}$
    计算条件概率分布 $p(y|\hat{x})$。形式上，给定 $\hat{x}$ 的输出序列 $y$ 的概率为：
- en: '|  | $p(y&#124;\hat{x})=\prod_{i=1}^{m}p(y_{i}&#124;y_{Abstract: [paper abstract] Title: [paper title] Question: [ask
    the model to predict one or more class labels of the paper, ordered from most
    to least likely, and provide explanations for its predictions] Answer:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'Abstract: [paper abstract] Title: [paper title] Question: [ask
    the model to predict one or more class labels of the paper, ordered from most
    to least likely, and provide explanations for its predictions] Answer:'
- en: 'Querying the LLM results in a ranked prediction list and a textual explanation
    for each paper:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 查询 LLM 会得到一个排序的预测列表以及每篇论文的文本解释：
- en: (Ranked Predictions) [a ranked prediction list] (Explanations)
    [model-generated explanation for the predictions]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (Ranked Predictions) [a ranked prediction list] (Explanations)
    [model-generated explanation for the predictions]
- en: These predictions and explanations serve as supplementary text attributes for
    the downstream LMs and GNN models, as detailed in the subsequent section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测和解释作为下游 LMs 和 GNN 模型的补充文本属性，如后续部分所述。
- en: 4.2 Fine-Tuning LM Interpreter and Node Feature Extraction
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 微调 LM 解释器和节点特征提取
- en: 'Original text and explanation features. Our initial step involves converting
    both the original text, *i.e.,* title and abstract, and the LLM’s explanations
    into fixed-length node features suitable for downstream GNN applications. Our
    approach is to fine-tune a smaller LM, which acts as an “interpreter” for the
    LLM’s text explanations. The rationale behind this step is that both the LLM and
    LM possess distinct advantages: the LLM has greater power and more knowledge but
    is less flexible, while the LM has less skills but is compact enough to be fine-tuned
    to a specific task. Thus, the LM serves to interpret the LLM’s output for the
    GNN, with the text explanation acting as an effective intermediate medium for
    communication. Then, fine-tuning the LM enables it to extract the most valuable
    and task-relevant features from the explanations.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文本和解释特征。我们的初步步骤涉及将原始文本，即标题和摘要，以及 LLM 的解释转换为适用于下游 GNN 应用的固定长度节点特征。我们的方法是微调一个较小的
    LM，它作为 LLM 文本解释的“解释器”。这样做的理由是 LLM 和 LM 各有其优势：LLM 功能强大且知识丰富，但灵活性较差；而 LM 技能较少，但足够紧凑，可以针对特定任务进行微调。因此，LM
    作为 GNN 的 LLM 输出的解释器，而文本解释则作为有效的中介沟通工具。随后，微调 LM 使其能够从解释中提取最有价值和任务相关的特征。
- en: 'Concretely, we first fine-tune pre-trained LMs as follows: let $\textrm{LM}_{\textrm{orig}}$
    and $\textrm{LM}_{\textrm{expl}}$ be pre-trained LMs that take as input the original
    $s^{\textrm{orig}}$ and the explanation $s^{\textrm{expl}}$ text sequences, respectively.
    We obtain text embeddings for each source as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们首先按照以下步骤微调预训练的 LMs：设 $\textrm{LM}_{\textrm{orig}}$ 和 $\textrm{LM}_{\textrm{expl}}$
    为预训练的 LMs，分别输入原始 $s^{\textrm{orig}}$ 和解释 $s^{\textrm{expl}}$ 文本序列。我们为每个源获得文本嵌入：
- en: '|  | $\begin{split}h_{\textrm{orig}}=\textrm{LM}_{\textrm{orig}}(s^{\textrm{orig}})\in\mathbb{R}^{N\times
    d},\quad h_{\textrm{expl}}=\textrm{LM}_{\textrm{expl}}(s^{\textrm{expl}})\in\mathbb{R}^{N\times
    d}.\end{split}$ |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}h_{\textrm{orig}}=\textrm{LM}_{\textrm{orig}}(s^{\textrm{orig}})\in\mathbb{R}^{N\times
    d},\quad h_{\textrm{expl}}=\textrm{LM}_{\textrm{expl}}(s^{\textrm{expl}})\in\mathbb{R}^{N\times
    d}.\end{split}$ |  | (4) |'
- en: 'We further apply a Multi-Layer Perceptron (MLP) to the output of the LMs to
    obtain a $N\times C$-dimensional prediction matrix representing the LM’s predictions
    for each node (in logits):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步将多层感知器（MLP）应用于 LMs 的输出，以获得一个 $N\times C$ 维的预测矩阵，表示 LM 对每个节点的预测（以 logits
    形式）：
- en: '|  | $\begin{split}y_{\textrm{orig}}=\textrm{MLP}_{\textrm{orig}}(h_{\textrm{orig}})\in\mathbb{R}^{N\times
    C},\quad y_{\textrm{expl}}=\textrm{MLP}_{\textrm{expl}}(h_{\textrm{expl}})\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (5) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}y_{\textrm{orig}}=\textrm{MLP}_{\textrm{orig}}(h_{\textrm{orig}})\in\mathbb{R}^{N\times
    C},\quad y_{\textrm{expl}}=\textrm{MLP}_{\textrm{expl}}(h_{\textrm{expl}})\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (5) |'
- en: We fine-tune these LMs and MLPs using cross-entropy loss. Finally, the text
    embeddings from both sources, $h_{\textrm{orig}}$ and $h_{\textrm{expl}}$, are
    used as enriched features for training downstream GNNs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用交叉熵损失对这些LM和MLP进行微调。最后，来自两个来源的文本嵌入，$h_{\textrm{orig}}$和$h_{\textrm{expl}}$，被用作训练下游GNN的丰富特征。
- en: Ranked prediction features. In addition to the explanations, the LLM also provides
    a top-$k$ ranked prediction list for each node, which adds valuable information.
    To incorporate this knowledge, the top-$k$ predictions for node $i$ are first
    one-hot encoded as vectors $p_{i,1},\dots,p_{i,k}\in\mathbb{R}^{C}$. These vectors
    are subsequently concatenated into a $kC$-dimensional vector, followed by a linear
    transformation to produce a fixed-sized vector of length $d_{P}$. This process
    produces a prediction feature matrix as $h_{\textrm{pred}}\in\mathbb{R}^{N\times
    d_{P}}$ across all nodes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 排名预测特征。除了解释外，LLM还为每个节点提供了一个前$k$的排名预测列表，这增加了有价值的信息。为了融入这些知识，对节点$i$的前$k$预测首先被独热编码为向量$p_{i,1},\dots,p_{i,k}\in\mathbb{R}^{C}$。这些向量随后被连接成一个$kC$维的向量，然后通过线性变换生成一个长度为$d_{P}$的固定大小的向量。这个过程生成一个预测特征矩阵$h_{\textrm{pred}}\in\mathbb{R}^{N\times
    d_{P}}$，覆盖所有节点。
- en: In summary, we denote our features as $h_{\textrm{TAPE}}=\{h_{\textrm{orig}},h_{\textrm{expl}},h_{\textrm{pred}}\}$,
    where “TAPE” stands for Title, Abstract, Prediction and Explanation for each node.
    Importantly, our framework requires these features to remain frozen during downstream
    GNN training, ensuring that the LM and LLM do not participate in the GNN training
    process. This characteristic significantly enhances ease-of-use, modularity, and
    efficiency compared to approaches like GLEM, which involve an expensive iterative
    LM-GNN training process. As a result, we achieve a substantial speedup over GLEM,
    *e.g.,* a $2.88\times$ speedup on ogbn-arxiv even when utilizing the same backbone
    LM and GNN.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们将特征表示为$h_{\textrm{TAPE}}=\{h_{\textrm{orig}},h_{\textrm{expl}},h_{\textrm{pred}}\}$，其中“TAPE”代表每个节点的标题、摘要、预测和解释。重要的是，我们的框架要求这些特征在下游GNN训练期间保持冻结，确保LM和LLM不参与GNN训练过程。与像GLEM这样的需要昂贵的迭代LM-GNN训练过程的方法相比，这一特性显著提高了易用性、模块化和效率。因此，即使使用相同的基础LM和GNN，我们也能在ogbn-arxiv上实现$2.88\times$的显著加速，*例如*。
- en: 4.3 GNN Training on Enriched Features
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 在丰富特征上的GNN训练
- en: 'Our final step is to train a GNN using the $h_{\textrm{TAPE}}$ features. We
    aim to achieve this without increasing the memory requirements of the GNN or making
    any changes to its architecture. To accomplish this, we use an ensemble approach,
    as a simple and effective way of combining the features. Specifically, we independently
    train GNN models $f_{\textrm{orig}}$, $f_{\textrm{expl}}$, and $f_{\textrm{pred}}$
    on the features $h_{\textrm{orig}}$, $h_{\textrm{expl}}$, and $h_{\textrm{pred}}$,
    respectively, to predict the ground truth node labels:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是使用$h_{\textrm{TAPE}}$特征来训练GNN。我们的目标是在不增加GNN内存需求或对其架构进行任何更改的情况下实现这一点。为此，我们使用了集成方法，这是结合特征的一种简单有效的方法。具体而言，我们分别在特征$h_{\textrm{orig}}$、$h_{\textrm{expl}}$和$h_{\textrm{pred}}$上独立训练GNN模型$f_{\textrm{orig}}$、$f_{\textrm{expl}}$和$f_{\textrm{pred}}$，以预测真实的节点标签：
- en: '|  | $\begin{split}\hat{y}_{\textrm{orig}/\textrm{expl}/\textrm{pred}}=f_{\textrm{orig}/\textrm{expl}/\textrm{pred}}(h_{\textrm{orig}/\textrm{expl}/\textrm{pred}},A)\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (6) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\hat{y}_{\textrm{orig}/\textrm{expl}/\textrm{pred}}=f_{\textrm{orig}/\textrm{expl}/\textrm{pred}}(h_{\textrm{orig}/\textrm{expl}/\textrm{pred}},A)\in\mathbb{R}^{N\times
    C}.\end{split}$ |  | (6) |'
- en: 'We then fuse these predictions by taking their average:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过取这些预测的平均值来融合它们：
- en: '|  | $\hat{y}=\textrm{mean}(\hat{y}_{\textrm{orig}},\hat{y}_{\textrm{expl}},\hat{y}_{\textrm{pred}})\in\mathbb{R}^{N\times
    C}.$ |  | (7) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}=\textrm{mean}(\hat{y}_{\textrm{orig}},\hat{y}_{\textrm{expl}},\hat{y}_{\textrm{pred}})\in\mathbb{R}^{N\times
    C}.$ |  | (7) |'
- en: 'Each of the three models performs well individually as shown in Table [3](#S5.T3
    "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments ‣ Harnessing Explanations: LLM-to-LM
    Interpreter for Enhanced Text-Attributed Graph Representation Learning"), which
    validates the effectiveness of simple averaging. This strategy enables us to capture
    complementary information from diverse input sources, ultimately enhancing the
    overall model’s performance.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [3](#S5.T3 "表3 ‣ 5.2 扩展性 ‣ 5 实验 ‣ 利用解释：LLM到LM的解释器提升文本属性图表示学习")所示，每个模型单独表现良好，这验证了简单平均的有效性。该策略使我们能够从不同的输入源捕捉互补信息，最终提升整体模型的性能。
- en: 4.4 Theoretical Analysis
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 理论分析
- en: 'In this section, we aim to demonstrate that explanations generated by an LLM
    can be valuable features for a smaller LM. Specifically, the explanations $E$
    are helpful if they possess *fidelity* in describing the LLM’s reasoning; and
    the LLM is *non-redundant*, utilizing information not used by the smaller LM.
    Let $E$ be the textual explanations generated by an LLM; $Z_{L}$ and $Z$ are embeddings
    from the LLM and smaller LM respectively, $y$ is the target and $H(\cdot|\cdot)$
    is the conditional entropy. The detailed proof is in Appendix [A](#A1 "Appendix
    A Theoretical Analysis ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced
    Text-Attributed Graph Representation Learning").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们旨在展示由LLM生成的解释对于较小的LM可以是有价值的特征。具体而言，若解释$E$在描述LLM的推理时具有*保真度*，且LLM是*非冗余*的，利用了较小LM未使用的信息，那么这些解释$E$就是有用的。设$E$为LLM生成的文本解释；$Z_{L}$和$Z$分别为LLM和较小LM的嵌入，$y$为目标，$H(\cdot|\cdot)$为条件熵。详细证明见附录 [A](#A1
    "附录 A 理论分析 ‣ 利用解释：LLM到LM的解释器提升文本属性图表示学习")。
- en: 'Theorem. Given the following conditions 1) *Fidelity*: $E$ is a good proxy
    for $Z_{L}$ such that $H(Z_{l}|E)=\epsilon$, with $\epsilon>, 2) *Non-redundancy*:
    . Then it follows that ，2)
    *非冗余*：。那么可以推导出 |  | (8) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $$H(Z_{l}|E)=\epsilon,{\quad\epsilon<$$ |  | (8) |'
- en: '2) Non-redundancy: $Z_{L}$ contains information not present in $Z$, expressed
    as'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 非冗余性：$Z_{L}$包含$Z$中不存在的信息，如下所示
- en: '|  | , we have:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当  时，我们有：
- en: '|  | $H(y&#124;Z)-\epsilon^{\prime}+\epsilon  \n Title:   \n Question: Which
    of the following sub-categories of AI does this paper belong to: Case Based, Genetic
    Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule
    Learning, Theory? If multiple options apply, provide a comma-separated list ordered
    from most to least related, then for each choice you gave, explain how it is present
    in the text. \n \n Answer: |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Cora | 摘要:   \n 标题:   \n 问题: 这篇论文属于以下哪个 AI 子类别：基于案例、遗传算法、神经网络、概率方法、强化学习、规则学习、理论？如果适用多个选项，请按最相关到最不相关的顺序提供以逗号分隔的列表，然后对于你给出的每个选项，解释它在文本中如何呈现。
    \n \n 答案: |'
- en: '| Pubmed | Abstract:   \n Title:   \n Question:
    Does the paper involve any cases of Type 1 diabetes, Type 2 diabetes, or Experimentally
    induced diabetes? Please give one or more answers of either Type 1 diabetes, Type
    2 diabetes, or Experimentally induced diabetes; if multiple options apply, provide
    a comma-separated list ordered from most to least related, then for each choice
    you gave, give a detailed explanation with quotes from the text explaining why
    it is related to the chosen option. \n \n Answer: |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Pubmed | 摘要:   \n 标题:   \n 问题: 论文是否涉及任何 1 型糖尿病、2
    型糖尿病或实验性诱导糖尿病的案例？请给出一个或多个 1 型糖尿病、2 型糖尿病或实验性诱导糖尿病的答案；如果适用多个选项，请按最相关到最不相关的顺序提供以逗号分隔的列表，然后对于你给出的每个选项，提供详细的解释，并引用文本说明为什么它与所选选项相关。
    \n \n 答案: |'
- en: '| ogbn-arxiv | Abstract:   \n Title:   \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | 摘要:   \n 标题:   \n 问题: 这篇论文属于哪个 arXiv
    CS 子类别？请给出 5 个可能的 arXiv CS 子类别，按最可能到最不可能的顺序以逗号分隔的列表形式表示，格式为“cs.XX”，并提供你的推理。 \n
    \n 答案: |'
- en: '| ogbn-products | Product description:   \n Question:
    Which of the following category does this product belong to: 1) Home & Kitchen,
    2) Health & Personal Care, 3) Beauty, 4) Sports & Outdoors, 5) Books, 6) Patio,
    Lawn & Garden, 7) Toys & Games, 8) CDs & Vinyl, 9) Cell Phones & Accessories,
    10) Grocery & Gourmet Food, 11) Arts, Crafts & Sewing, 12) Clothing, Shoes & Jewelry,
    13) Electronics, 14) Movies & TV, 15) Software, 16) Video Games, 17) Automotive,
    18) Pet Supplies, 19) Office Products, 20) Industrial & Scientific, 21) Musical
    Instruments, 22) Tools & Home Improvement, 23) Magazine Subscriptions, 24) Baby
    Products, 25) NAN, 26) Appliances, 27) Kitchen & Dining, 28) Collectibles & Fine
    Art, 29) All Beauty, 30) Luxury Beauty, 31) Amazon Fashion, 32) Computers, 33)
    All Electronics, 34) Purchase Circles, 35) MP3 Players & Accessories, 36) Gift
    Cards, 37) Office & School Supplies, 38) Home Improvement, 39) Camera & Photo,
    40) GPS & Navigation, 41) Digital Music, 42) Car Electronics, 43) Baby, 44) Kindle
    Store, 45) Kindle Apps, 46) Furniture & Decor? Give 5 likely categories as a comma-separated
    list ordered from most to least likely, and provide your reasoning. \n \n Answer:
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-products | 产品描述：  \n 问题：以下哪个类别适合该产品：1) 家居与厨房，2)
    健康与个人护理，3) 美容，4) 体育与户外，5) 图书，6) 露台、草坪与花园，7) 玩具与游戏，8) CD与黑胶唱片，9) 手机及配件，10) 杂货与美食，11)
    艺术、工艺与缝纫，12) 服装、鞋类与珠宝，13) 电子产品，14) 电影与电视，15) 软件，16) 视频游戏，17) 汽车，18) 宠物用品，19) 办公产品，20)
    工业与科学，21) 乐器，22) 工具与家居改善，23) 杂志订阅，24) 婴儿产品，25) NAN，26) 家用电器，27) 厨房与餐厅，28) 收藏品与美术，29)
    全部美容，30) 奢华美容，31) 亚马逊时尚，32) 电脑，33) 全部电子产品，34) 购买圈，35) MP3播放器及配件，36) 礼品卡，37) 办公与学校用品，38)
    家居改善，39) 照相机与照片，40) GPS与导航，41) 数字音乐，42) 汽车电子产品，43) 婴儿，44) Kindle商店，45) Kindle应用，46)
    家具与装饰？请给出5个可能的类别，按从最可能到最不可能的顺序排列，并提供你的理由。 \n \n 答案： |'
- en: '| tape-arxiv23 | Abstract:   \n Title:   \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| tape-arxiv23 | 摘要：  \n 标题：  \n 问题：这篇论文属于哪个arXiv
    CS子类别？请给出5个可能的arXiv CS子类别，按从最可能到最不可能的顺序排列，形式为“cs.XX”，并提供你的理由。 \n \n 答案： |'
- en: Additional Prompt Experiments.
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 附加提示实验。
- en: 'Table 8: Prompts used for our experiments studying the effect of different
    prompts. Most prompts have similar performance.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：用于研究不同提示对实验效果影响的提示。大多数提示具有类似的表现。
- en: '| Description | Prompt | Accuracy |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 提示 | 准确性 |'
- en: '| Default prompt | Abstract:   \n Title:   \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: | 0.720 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 默认提示 | 摘要：  \n 标题：  \n 问题：这篇论文属于哪个arXiv CS子类别？请给出5个可能的arXiv
    CS子类别，按从最可能到最不可能的顺序排列，形式为“cs.XX”，并提供你的理由。 \n \n 答案： | 0.720 |'
- en: '| Title first | Title:   \n Abstract:   \n Question:
    Which arXiv CS sub-category does this paper belong to? Give 5 likely arXiv CS
    sub-categories as a comma-separated list ordered from most to least likely, in
    the form “cs.XX”, and provide your reasoning. \n \n Answer: | 0.695 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 标题优先 | 标题：  \n 摘要：  \n 问题：这篇论文属于哪个arXiv CS子类别？请给出5个可能的arXiv
    CS子类别，按从最可能到最不可能的顺序排列，形式为“cs.XX”，并提供你的理由。 \n \n 答案： | 0.695 |'
- en: '| Focus on text content | Title:   \n Abstract:   \n
    Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely
    arXiv CS sub-categories as a comma-separated list ordered from most to least likely,
    in the form “cs.XX”. Focus only on content in the actual text and avoid making
    false associations. Then provide your reasoning. | 0.695 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 专注于文本内容 | 标题：  \n 摘要：  \n 问题：这篇论文属于哪个arXiv CS子类别？请给出5个可能的arXiv
    CS子类别，按从最可能到最不可能的顺序排列，形式为“cs.XX”。仅关注实际文本中的内容，避免错误关联。然后提供你的理由。 | 0.695 |'
- en: '| Chain of thought prompt | Title:   \n Abstract:   \n
    Question: Which arXiv CS sub-category does this paper belong to? Give 5 likely
    arXiv CS sub-categories as a comma-separated list ordered from most to least likely,
    in the form “cs.XX”. Please think about the categorization in a step by step manner
    and avoid making false associations. Then provide your reasoning. | 0.705 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 思考链提示 | 标题：  \n 摘要：  \n 问题：这篇论文属于哪个arXiv计算机科学子类别？请提供5个可能的arXiv计算机科学子类别，以逗号分隔的形式从最可能到最不可能的顺序排列，格式为“cs.XX”。请逐步考虑分类，避免错误关联。然后提供你的推理。
    | 0.705 |'
- en: 'To study the effect of different prompts, we consider a variety of prompts
    and evaluate the zero-shot accuracy of the LLM (ChatGPT) on each prompt. We evaluate
    all prompts on 200 sample papers from the ogbn-arxiv dataset, see Table [8](#A4.T8
    "Table 8 ‣ Additional Prompt Experiments. ‣ D.3 Prompt Design ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning"). We accompany each prompt by a brief summary of
    the change being made. In summary, most prompts have similar performance, with
    a slight performance gain when placing the title after the abstract, which seems
    to agree with the notion in [[46](#bib.bibx46)] that more important information
    (like the title) should be placed later in the prompt.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究不同提示的效果，我们考虑了多种提示，并评估了LLM（ChatGPT）在每个提示上的零样本准确性。我们在ogbn-arxiv数据集中的200篇样本论文上评估所有提示，见表格[8](#A4.T8
    "Table 8 ‣ Additional Prompt Experiments. ‣ D.3 Prompt Design ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")。我们为每个提示附上了所做更改的简要总结。总结来说，大多数提示的表现相似，将标题放在摘要之后稍微提高了性能，这似乎与[[46](#bib.bibx46)]的观点一致，即更重要的信息（如标题）应放在提示的后面。'
- en: D.4 Detailed Ablation Study
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4 详细消融研究
- en: 'We conducted a detailed ablation study on the ogbn-arxiv dataset to assess
    the impact of different sources of node features. The study focused on three types
    of node features: original text features ($h_{\textrm{orig}}$), explanation as
    features ($h_{\textrm{expl}}$), and predictions as features ($h_{\textrm{pred}}$).
    We systematically removed one of these features at a time while keeping the other
    components unchanged in our model.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对ogbn-arxiv数据集进行了详细的消融研究，以评估不同来源的节点特征的影响。研究集中在三种类型的节点特征上：原始文本特征（$h_{\textrm{orig}}$）、解释作为特征（$h_{\textrm{expl}}$）和预测作为特征（$h_{\textrm{pred}}$）。我们系统地逐一移除这些特征中的一个，同时保持模型中的其他组件不变。
- en: 'The results of the ablation study are illustrated in Figure [3](#A4.F3 "Figure
    3 ‣ D.4 Detailed Ablation Study ‣ Appendix D Experiments ‣ Harnessing Explanations:
    LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning").
    The figure presents the performance of the model when each type of node feature
    is removed. It is observed that using the full set of features yields the best
    performance, while leaving out any of the features leads to a drop in performance.
    However, the extent of the performance drop may vary depending on the specific
    GNN model being used.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '消融研究的结果如图[3](#A4.F3 "Figure 3 ‣ D.4 Detailed Ablation Study ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")所示。该图展示了在移除每种类型的节点特征时模型的表现。观察到，使用完整的特征集可以获得最佳性能，而移除任何特征都会导致性能下降。然而，性能下降的程度可能会根据所使用的具体GNN模型而有所不同。'
- en: '![Refer to caption](img/0d49dff0fd6769254e47715956b346e8.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0d49dff0fd6769254e47715956b346e8.png)'
- en: 'Figure 3: Effect of node features. We study the effects of different sources
    of node features on the ogbn-arxiv dataset, *i.e.,* original text features ($h_{\textrm{orig}}$),
    explanation as features ($h_{\textrm{expl}}$) and predictions as features ($h_{\textrm{pred}}$),
    by removing one of them in turn from our model while keeping the other components
    unchanged.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：节点特征的影响。我们研究了不同来源的节点特征对ogbn-arxiv数据集的影响，*即，* 原始文本特征（$h_{\textrm{orig}}$）、解释作为特征（$h_{\textrm{expl}}$）和预测作为特征（$h_{\textrm{pred}}$），通过在保持其他组件不变的情况下逐一从模型中移除它们。
- en: 'This ablation study provides additional insights to complement the findings
    presented in section [5.3](#S5.SS3 "5.3 Ablation Study ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning"). While Table [3](#S5.T3 "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") compared the performance of using the full set
    of features versus using just one of them, this ablation study specifically focuses
    on comparing the performance of using the full set of features versus leaving
    one of them out. Although the experimental design differs, the overall message
    conveyed remains consistent, emphasizing the significance of considering all the
    various sources of node features for achieving optimal performance in node classification
    tasks.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '本次消融研究提供了额外的见解，以补充第[5.3](#S5.SS3 "5.3 Ablation Study ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning")节中提出的发现。虽然表[3](#S5.T3 "Table 3 ‣ 5.2 Scalability ‣ 5 Experiments ‣ Harnessing
    Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation
    Learning")比较了使用全套特征与仅使用其中一个特征的性能，本次消融研究特别关注于比较使用全套特征与省略其中一个特征的性能。尽管实验设计有所不同，但传达的整体信息保持一致，强调了考虑所有不同节点特征来源对于实现节点分类任务的最佳性能的重要性。'
- en: D.5 Llama as a cost-efficient alternative
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5 Llama作为一种成本效益高的替代方案
- en: 'We extend out experiment to the open-source LLM "llama-2-13b-chat" (llama for
    short), which demonstrates the feasibility of a cost-effective (free) alternative,
    see Table [9](#A4.T9 "Table 9 ‣ D.5 Llama as a cost-efficient alternative ‣ Appendix
    D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning").'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将实验扩展到开源LLM“llama-2-13b-chat”（简称llama），这展示了一个经济实惠（免费）替代方案的可行性，见表 [9](#A4.T9
    "Table 9 ‣ D.5 Llama as a cost-efficient alternative ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")。'
- en: It is worth noting that although llama exhibits a lower performance compared
    to GPT-3.5 in terms of both zero-shot accuracy and explanation quality, our pipeline
    still maintains its robust performance. As an illustration, we achieved an accuracy
    of 76.19% on the ogbn-arxiv dataset using llama, slightly below the 77.50% achieved
    with GPT-3.5\. We attribute this impressive level of generalization to the complementary
    nature of the explanations themselves, which serve as a rich source of semantic
    information supplementing the original text such as title and abstract.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管llama在零样本准确性和解释质量方面的表现低于GPT-3.5，但我们的流程仍然保持了其强大的性能。作为说明，我们在ogbn-arxiv数据集上使用llama达到了76.19%的准确率，略低于GPT-3.5取得的77.50%。我们将这一令人印象深刻的泛化水平归因于解释本身的互补性质，它们作为丰富的语义信息来源，补充了原始文本如标题和摘要。
- en: 'Table 9: Node classification accuracy for the Cora, PubMed, ogbn-arxiv and
    tape-arxiv23 datasets.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：Cora、PubMed、ogbn-arxiv 和 tape-arxiv23 数据集的节点分类准确率。
- en: '| Dataset | Method | llama2-13b-chat | GPT3.5 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | llama2-13b-chat | GPT3.5 |'
- en: '| LLM | LM${}_{\textrm{finetune}}$ | $h_{\textrm{TAPE}}$ | LLM | LM${}_{\textrm{finetune}}$
    | $h_{\textrm{TAPE}}$ |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| LLM | LM${}_{\textrm{finetune}}$ | $h_{\textrm{TAPE}}$ | LLM | LM${}_{\textrm{finetune}}$
    | $h_{\textrm{TAPE}}$ |'
- en: '| Cora | MLP | 0.5746 | 0.6845 ± 0.0194 | 0.7675 ± 0.0187 | 0.6769 | 0.7606
    ± 0.0378 | 0.8086 ± 0.0190 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Cora | MLP | 0.5746 | 0.6845 ± 0.0194 | 0.7675 ± 0.0187 | 0.6769 | 0.7606
    ± 0.0378 | 0.8086 ± 0.0190 |'
- en: '| GCN | 0.5746 | 0.6845 ± 0.0194 | 0.8630 ± 0.0101 | 0.6769 | 0.7606 ± 0.0378
    | 0.8773 ± 0.0063 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.5746 | 0.6845 ± 0.0194 | 0.8630 ± 0.0101 | 0.6769 | 0.7606 ± 0.0378
    | 0.8773 ± 0.0063 |'
- en: '| SAGE | 0.5746 | 0.6845 ± 0.0194 | 0.8625 ± 0.0093 | 0.6769 | 0.7606 ± 0.0378
    | 0.8792 ± 0.0107 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.5746 | 0.6845 ± 0.0194 | 0.8625 ± 0.0093 | 0.6769 | 0.7606 ± 0.0378
    | 0.8792 ± 0.0107 |'
- en: '|  | RevGAT | 0.5746 | 0.6845 ± 0.0194 | 0.8884 ± 0.0100 | 0.6769 | 0.7606
    ± 0.0378 | 0.8930 ± 0.0072 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.5746 | 0.6845 ± 0.0194 | 0.8884 ± 0.0100 | 0.6769 | 0.7606
    ± 0.0378 | 0.8930 ± 0.0072 |'
- en: '| PubMed | MLP | 0.3958 | 0.9121 ± 0.0026 | 0.9475 ± 0.0046 | 0.9342 | 0.9494
    ± 0.0046 | 0.9473 ± 0.0040 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| PubMed | MLP | 0.3958 | 0.9121 ± 0.0026 | 0.9475 ± 0.0046 | 0.9342 | 0.9494
    ± 0.0046 | 0.9473 ± 0.0040 |'
- en: '| GCN | 0.3958 | 0.9121 ± 0.0026 | 0.9257 ± 0.0063 | 0.9342 | 0.9494 ± 0.0046
    | 0.9392 ± 0.0023 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.3958 | 0.9121 ± 0.0026 | 0.9257 ± 0.0063 | 0.9342 | 0.9494 ± 0.0046
    | 0.9392 ± 0.0023 |'
- en: '| SAGE | 0.3958 | 0.9121 ± 0.0026 | 0.9464 ± 0.0033 | 0.9342 | 0.9494 ± 0.0046
    | 0.9530 ± 0.0035 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.3958 | 0.9121 ± 0.0026 | 0.9464 ± 0.0033 | 0.9342 | 0.9494 ± 0.0046
    | 0.9530 ± 0.0035 |'
- en: '|  | RevGAT | 0.3958 | 0.9121 ± 0.0026 | 0.9465 ± 0.0042 | 0.9342 | 0.9494
    ± 0.0046 | 0.9526 ± 0.0032 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.3958 | 0.9121 ± 0.0026 | 0.9465 ± 0.0042 | 0.9342 | 0.9494
    ± 0.0046 | 0.9526 ± 0.0032 |'
- en: '| ogbn-arxiv | MLP | 0.4423 | 0.6941 ± 0.0020 | 0.7361 ± 0.0009 | 0.7350 |
    0.7361 ± 0.0004 | 0.7587 ± 0.0015 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | MLP | 0.4423 | 0.6941 ± 0.0020 | 0.7361 ± 0.0009 | 0.7350 |
    0.7361 ± 0.0004 | 0.7587 ± 0.0015 |'
- en: '| GCN | 0.4423 | 0.6941 ± 0.0020 | 0.7418 ± 0.0031 | 0.7350 | 0.7361 ± 0.0004
    | 0.7520 ± 0.0003 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.4423 | 0.6941 ± 0.0020 | 0.7418 ± 0.0031 | 0.7350 | 0.7361 ± 0.0004
    | 0.7520 ± 0.0003 |'
- en: '| SAGE | 0.4423 | 0.6941 ± 0.0020 | 0.7536 ± 0.0028 | 0.7350 | 0.7361 ± 0.0004
    | 0.7672 ± 0.0007 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.4423 | 0.6941 ± 0.0020 | 0.7536 ± 0.0028 | 0.7350 | 0.7361 ± 0.0004
    | 0.7672 ± 0.0007 |'
- en: '|  | RevGAT | 0.4423 | 0.6941 ± 0.0020 | 0.7619 ± 0.0027 | 0.7350 | 0.7361
    ± 0.0004 | 0.7750 ± 0.0012 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.4423 | 0.6941 ± 0.0020 | 0.7619 ± 0.0027 | 0.7350 | 0.7361
    ± 0.0004 | 0.7750 ± 0.0012 |'
- en: '| tape-arxiv23 | MLP | 0.4452 | 0.7677 ± 0.0042 | 0.7905 ± 0.0041 | 0.7356
    | 0.7832 ± 0.0052 | 0.7999 ± 0.0037 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| tape-arxiv23 | MLP | 0.4452 | 0.7677 ± 0.0042 | 0.7905 ± 0.0041 | 0.7356
    | 0.7832 ± 0.0052 | 0.7999 ± 0.0037 |'
- en: '| GCN | 0.4452 | 0.7677 ± 0.0042 | 0.7751 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.7827 ± 0.0037 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 0.4452 | 0.7677 ± 0.0042 | 0.7751 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.7827 ± 0.0037 |'
- en: '| SAGE | 0.4452 | 0.7677 ± 0.0042 | 0.7935 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.8020 ± 0.0024 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 0.4452 | 0.7677 ± 0.0042 | 0.7935 ± 0.0029 | 0.7356 | 0.7832 ± 0.0052
    | 0.8020 ± 0.0024 |'
- en: '|  | RevGAT | 0.4452 | 0.7677 ± 0.0042 | 0.7993 ± 0.0043 | 0.7356 | 0.7832
    ± 0.0052 | 0.8074 ± 0.0021 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | RevGAT | 0.4452 | 0.7677 ± 0.0042 | 0.7993 ± 0.0043 | 0.7356 | 0.7832
    ± 0.0052 | 0.8074 ± 0.0021 |'
- en: D.6 Case Study
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6 案例研究
- en: '![Refer to caption](img/4da50160638a3fd9299866f50c433471.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4da50160638a3fd9299866f50c433471.png)'
- en: 'Figure 4: Case study comparing features for node classification on the PubMed
    dataset: (a) Original text attributes and (b) Explanations generated by LLMs.
    The GNN model trained with (b) accurately predicts the label for node 12390 (type
    2 diabetes), while the model trained with (a) predicts the incorrect label (experimentally
    induced diabetes). This improvement can be attributed to the concise and focused
    nature of LLM-generated explanations, as well as their reasoning ability and utilization
    of external knowledge.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：比较PubMed数据集上节点分类特征的案例研究：(a) 原始文本属性和(b) LLM生成的解释。使用(b)训练的GNN模型准确预测了节点12390的标签（2型糖尿病），而使用(a)训练的模型预测了错误的标签（实验性诱导糖尿病）。这种改进可以归因于LLM生成解释的简洁和集中性质，以及它们的推理能力和外部知识的利用。
- en: 'To investigate the impact of using explanations as features in improving node
    classification on TAGs, we conduct an analysis on predicted samples from the PubMed
    dataset. Figure [4](#A4.F4 "Figure 4 ‣ D.6 Case Study ‣ Appendix D Experiments
    ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning") presents a case where the GNN model trained with
    original text attributes as features incorrectly predicts the label for node 12390
    (as experimentally induced diabetes), while the model trained with explanations
    generated by LLMs as features correctly predicts the label (as type 2 diabetes).'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究使用解释作为特征对提高节点分类在TAGs上的影响，我们对PubMed数据集中的预测样本进行了分析。图[4](#A4.F4 "图 4 ‣ D.6
    案例研究 ‣ 附录 D 实验 ‣ 利用解释：LLM到LM解释器以增强文本属性图表示学习")展示了一个案例，其中使用原始文本属性作为特征训练的GNN模型错误地预测了节点12390的标签（为实验性诱导糖尿病），而使用LLM生成的解释作为特征训练的模型正确预测了标签（为2型糖尿病）。
- en: This improvement can be attributed to two main factors. Firstly, compared to
    the original text attributes, which consist of the title and abstract text, the
    explanations generated by the LLM are more concise and focused. This aids the
    subsequent LM in generating node embeddings that capture the essential semantics
    without the need to compress an excessive amount of information into a fixed-length
    representation. Secondly, LLMs possess reasoning capabilities and the ability
    to leverage general knowledge, which prove crucial in achieving accurate predictions.
    For instance, the explanations generated by LLMs explicitly link type 2 diabetes
    to MKR mice and db/db mice (which are common animal models of type 2 diabetes),
    as well as the insulinopenic mice / streptozotocin to experimentally induced diabetes.
    This knowledge is either absent or only implicitly specified in the original text
    attributes.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这种改进可以归因于两个主要因素。首先，与包含标题和摘要文本的原始文本属性相比，LLM生成的解释更为简洁和集中。这有助于随后的LM生成捕捉基本语义的节点嵌入，而不需要将过多的信息压缩成固定长度的表示。其次，LLM具有推理能力和利用通用知识的能力，这在实现准确预测方面至关重要。例如，LLM生成的解释明确将2型糖尿病与MKR小鼠和db/db小鼠（这两者是2型糖尿病的常见动物模型）以及胰岛素缺乏小鼠/链脲佐菌素与实验性诱导糖尿病联系起来。这些知识在原始文本属性中要么缺失，要么仅以隐含的方式指定。
- en: D.7 GLEM
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7 GLEM
- en: '[[45](#bib.bibx45)] evaluated GLEM on the ogbn-arxiv dataset. We extended our
    evaluation of GLEM with the Cora and PubMed datasets for a more comprehensive
    comparison with our method. Results are reported in Table [10](#A4.T10 "Table
    10 ‣ D.7 GLEM ‣ Appendix D Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter
    for Enhanced Text-Attributed Graph Representation Learning")'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[[45](#bib.bibx45)] 在 ogbn-arxiv 数据集上评估了 GLEM。我们通过使用 Cora 和 PubMed 数据集扩展了对
    GLEM 的评估，以便与我们的方法进行更全面的比较。结果见表 [10](#A4.T10 "Table 10 ‣ D.7 GLEM ‣ Appendix D
    Experiments ‣ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed
    Graph Representation Learning")'
- en: 'Table 10: GLEM [[45](#bib.bibx45)]'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：GLEM [[45](#bib.bibx45)]
- en: '| Dataset | GCN | SAGE | RevGAT |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | GCN | SAGE | RevGAT |'
- en: '| Cora | 0.8732 ± 0.0066 | 0.8801 ± 0.0054 | 0.8856 ± 0.0060 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Cora | 0.8732 ± 0.0066 | 0.8801 ± 0.0054 | 0.8856 ± 0.0060 |'
- en: '| PubMed | 0.9469 ± 0.0010 | 0.9459 ± 0.0018 | 0.9471 ± 0.0020 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| PubMed | 0.9469 ± 0.0010 | 0.9459 ± 0.0018 | 0.9471 ± 0.0020 |'
- en: '| ogbn-arxiv | 0.7593 ± 0.0019 | 0.7550 ± 0.0024 | 0.7697 ± 0.0019 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | 0.7593 ± 0.0019 | 0.7550 ± 0.0024 | 0.7697 ± 0.0019 |'
