- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 17:34:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 17:34:38'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'AIOS 编译器: LLM 作为自然语言编程和AI代理流程编程的解释器'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06907](https://ar5iv.labs.arxiv.org/html/2405.06907)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06907](https://ar5iv.labs.arxiv.org/html/2405.06907)
- en: Shuyuan Xu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 徐书源
- en: Rutgers University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: shuyuan.xu@rutgers.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: shuyuan.xu@rutgers.edu
- en: 'Zelong Li ¹¹footnotemark: 1'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '李泽龙 ¹¹脚注标记: 1'
- en: Rutgers University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: zelong.li@rutgers.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: zelong.li@rutgers.edu
- en: Kai Mei
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 梅凯
- en: Rutgers University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: kai.mei@rutgers.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: kai.mei@rutgers.edu
- en: Yongfeng Zhang
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 张永锋
- en: Rutgers University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 罗格斯大学
- en: yongfeng.zhang@rutgers.edu Both authors contributed equally to this work.Corresponding
    author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: yongfeng.zhang@rutgers.edu 两位作者对本工作均有贡献。通讯作者
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Since their inception, programming languages have trended towards greater readability
    and lower barriers for programmers. Following this trend, natural language can
    be a promising type of programming language that provides great flexibility and
    usability and helps towards the democracy of programming. However, the inherent
    vagueness, ambiguity, and verbosity of natural language pose significant challenges
    in developing an interpreter that can accurately understand the programming logic
    and execute instructions written in natural language. Fortunately, recent advancements
    in Large Language Models (LLMs) have demonstrated remarkable proficiency in interpreting
    complex natural language. Inspired by this, we develop a novel system for Code
    Representation and Execution (CoRE), which employs LLM as interpreter to interpret
    and execute natural language programs (NLPg). The proposed system unifies natural
    language programming, pseudo-code programming, and flow programming under the
    same representation for constructing language agents, while LLM serves as the
    interpreter to interpret and execute the agent programs. In this paper, we begin
    with defining the programming syntax that structures natural language instructions
    logically. During the execution, we incorporate external memory to minimize redundancy.
    Furthermore, we equip the designed interpreter with the capability to invoke external
    tools, compensating for the limitations of LLM in specialized domains or when
    accessing real-time information. This work is open-source at [https://github.com/agiresearch/CoRE](https://github.com/agiresearch/CoRE),
    [https://github.com/agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI),
    and [https://github.com/agiresearch/AIOS](https://github.com/agiresearch/AIOS).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自编程语言诞生以来，它们的趋势一直是提高可读性和降低程序员的门槛。遵循这一趋势，自然语言可能成为一种有前景的编程语言，它提供了极大的灵活性和可用性，有助于编程的普及。然而，自然语言固有的模糊性、歧义性和冗长性在开发一个能够准确理解编程逻辑并执行用自然语言编写的指令的解释器时，带来了显著的挑战。幸运的是，最近在大语言模型（LLMs）方面的进展显示了它们在解释复杂自然语言方面的卓越能力。受到这一点的启发，我们开发了一个新颖的代码表示和执行系统（CoRE），它使用LLM作为解释器来解释和执行自然语言程序（NLPg）。该系统将自然语言编程、伪代码编程和流程编程统一在同一表示下，用于构建语言代理，同时LLM作为解释器解释和执行代理程序。在本文中，我们首先定义了编程语法，以逻辑地结构化自然语言指令。在执行过程中，我们结合了外部记忆以减少冗余。此外，我们为设计的解释器配备了调用外部工具的能力，以弥补LLM在专业领域或访问实时信息时的局限性。这项工作是开源的，见[https://github.com/agiresearch/CoRE](https://github.com/agiresearch/CoRE)、[https://github.com/agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI)和[https://github.com/agiresearch/AIOS](https://github.com/agiresearch/AIOS)。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/2c5a6aaf59f8898c31659aa93b9514be.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/2c5a6aaf59f8898c31659aa93b9514be.png)'
- en: 'Figure 1: In our CoRE system, we design the CoRE language to unify natural
    language programming, pseudo-code programming, and flow programming in the same
    syntax representative. We use the program for OpenAGI [[14](#bib.bib14)] platform
    as an example.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 在我们的 CoRE 系统中，我们设计了 CoRE 语言，以统一自然语言编程、伪代码编程和流程编程的语法表示。我们以 OpenAGI [[14](#bib.bib14)]
    平台的程序作为示例。'
- en: Programming is crucial for computers as it enables them to execute specific
    tasks based on a predefined set of instructions. It allows us to utilize logical
    algorithms to enable computers to solve problems. Programming has evolved significantly
    since its inception, with new technologies and innovations driving its growth.
    Initially, programming languages were based on binary machine language, such as
    punched cards, which can be directly executed by the machine. However, machine
    language was hardly readable to humans. Subsequently, low-level programming languages,
    such as assembly language, use mnemonic instructions and operands to represent
    machine code, which enhances the readability [[18](#bib.bib18)]. However, due
    to the requirement of controlling memory locations and registers, assembly language
    still has a high entry barrier for programmers. With the design of high-level
    programming languages like C/C++, Java and Python, coding has become more user-friendly
    and efficient. They offer programmers a more productive and accessible approach,
    leading to increased participation in programming and software development. Consequently,
    programming languages are becoming more integrated into everyday life.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 编程对计算机至关重要，因为它使计算机能够根据预定义的一系列指令执行特定任务。它使我们能够利用逻辑算法，使计算机解决问题。编程自诞生以来发生了显著变化，新技术和创新推动了其发展。最初，编程语言基于二进制机器语言，例如穿孔卡，这些语言可以被机器直接执行。然而，机器语言对人类来说几乎不可读。随后，低级编程语言如汇编语言，使用助记符指令和操作数来表示机器代码，这提高了可读性[[18](#bib.bib18)]。然而，由于需要控制内存位置和寄存器，汇编语言仍然对程序员有较高的门槛。随着C/C++、Java和Python等高级编程语言的出现，编码变得更加用户友好和高效。这些语言为程序员提供了更具生产力和可及性的方法，从而增加了对编程和软件开发的参与。因此，编程语言正日益融入我们的日常生活。
- en: From the history of programming languages, we can observe a clear trend toward
    increased usability, readability, and democracy of programming. Following this
    trend, natural language can be a desirable choice for coding due to its accessibility,
    readability, and minimal training requirements for programmers. However, the application
    of natural language programming presents challenges due to the inherent vagueness,
    ambiguity, and verbosity of natural language. The recently emerged Large Language
    Models (LLMs) serve as a solution to this challenge due to their extraordinary
    capability in language understanding [[38](#bib.bib38), [8](#bib.bib8)], tool
    use and function calling [[14](#bib.bib14), [42](#bib.bib42)], as well as interacting
    with human or environments [[43](#bib.bib43), [12](#bib.bib12)]. In this work,
    we propose a novel system for Code Representation and Execution (CoRE), which
    takes LLM as the interpreter to interpret and execute the instructions in natural
    language, enabling agent programming in natural language.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程语言的历史中，我们可以观察到一种明显的趋势：编程的可用性、可读性和民主化程度不断提高。沿着这一趋势，自然语言可能成为编程的一个理想选择，因为它的可及性、可读性以及对程序员培训要求较少。然而，自然语言编程的应用面临挑战，因为自然语言固有的模糊性、歧义性和冗长性。最近出现的大型语言模型（LLMs）由于其在语言理解[[38](#bib.bib38),
    [8](#bib.bib8)]、工具使用和函数调用[[14](#bib.bib14), [42](#bib.bib42)]，以及与人类或环境交互[[43](#bib.bib43),
    [12](#bib.bib12)]方面的卓越能力，为解决这一挑战提供了一个方案。在这项工作中，我们提出了一种新颖的代码表示和执行系统（CoRE），该系统以LLM作为解释器来解释和执行自然语言中的指令，从而实现自然语言的代理编程。
- en: 'CoRE can be used for natural language programming, pseudo-code programming,
    and flow programming, as the three forms of agent programs unify into our CoRE
    language, as shown by the example in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents"). In the realm of programming, the fundamental task
    involves designing and developing logically structured instructions to address
    specific problems. Natural language programming offers a method where instructions
    are formulated in everyday language, making the code intuitive and accessible.
    When we structure all natural language instructions in a logical way, it inherently
    mirrors the essence of pseudo-code programming. Pseudo-code, by design, simplifies
    the coding process by stripping down syntax complexities and focusing on the algorithmic
    logic for easy understanding. Therefore, when the instructions are expressed in
    natural language, the structured instructions can be identified as pseudo-code.
    Moreover, pseudo-code shares a direct relationship with flow programming, as it
    essentially represents the algorithm’s logic that can seamlessly be visualized
    as a workflow. Workflow, in turn, provides a graphical representation of the step-by-step
    execution of programs, emphasizing the decision-making visualization process and
    the flow of control across the program.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'CoRE 可用于自然语言编程、伪代码编程和流程编程，因为这三种形式的代理程序统一到我们的 CoRE 语言中，如图 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents") 所示。在编程领域，基本任务包括设计和开发逻辑结构化的指令以解决特定问题。自然语言编程提供了一种方法，将指令用日常语言表达，使代码直观易懂。当我们以逻辑方式构造所有自然语言指令时，它本质上反映了伪代码编程的精髓。伪代码通过设计简化了编码过程，去除了语法复杂性，专注于算法逻辑以便于理解。因此，当指令以自然语言表达时，结构化的指令可以被识别为伪代码。此外，伪代码与流程编程有直接关系，因为它本质上代表了可以无缝地以工作流形式可视化的算法逻辑。工作流反过来提供了程序逐步执行的图形表示，强调了决策可视化过程和程序中的控制流。'
- en: 'We face several significant challenges when designing the novel system for
    natural language programming with LLM as an interpreter. First, how to represent
    the logic of the program using natural language instructions. To tackle this issue,
    we design a set of programming syntax to logically structure natural language
    instructions, and unify the natural language programming, pseudo-code programming,
    and flow programming in the same representation. Second, given that the programs
    consist of step-by-step instructions, it is crucial to make sure that each step
    is executed according to its corresponding instruction. To ensure precise execution
    of the instructions in natural language for each step, we design two additional
    components: one for retrieving information from memory, and the other for invoking
    external tools. Considering the LLM’s limitation on the number of input tokens
    (context window size), including all runtime information in the input prompt is
    impractical. To address this problem, we store a large volume of intermediate
    results in temporary memory, retrieving relevant information as needed in subsequent
    steps [[48](#bib.bib48), [34](#bib.bib34), [27](#bib.bib27), [4](#bib.bib4)].
    Besides, while LLMs excel at processing textual information, they often fall short
    in tasks that require domain-specific knowledge or up-to-date information [[15](#bib.bib15)].
    To mitigate these limitations, we enable the LLM to utilize external tools to
    solve the problems [[14](#bib.bib14), [42](#bib.bib42), [30](#bib.bib30), [2](#bib.bib2)].
    Finally, when executing the natural language program, incorrectly determining
    the next step can lead to different final results. We solve this problem by demanding
    the LLM interpreter to evaluate the current results so as to identify the most
    suitable subsequent step. The overall execution pipeline of CoRE is depicted in
    Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents").'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '在为自然语言编程设计新系统时，我们面临几个重大挑战，其中包括使用LLM作为解释器。首先，如何用自然语言指令表示程序逻辑。为了解决这个问题，我们设计了一套编程语法，以逻辑结构化自然语言指令，并将自然语言编程、伪代码编程和流程编程统一在同一表示中。其次，由于程序由逐步指令组成，确保每一步按照其对应指令执行至关重要。为确保每一步中的自然语言指令精确执行，我们设计了两个额外的组件：一个用于从内存中检索信息，另一个用于调用外部工具。考虑到LLM在输入标记数（上下文窗口大小）上的限制，将所有运行时信息包括在输入提示中是不切实际的。为了解决这个问题，我们将大量的中间结果存储在临时内存中，在后续步骤中根据需要检索相关信息[[48](#bib.bib48),
    [34](#bib.bib34), [27](#bib.bib27), [4](#bib.bib4)]。此外，尽管LLM在处理文本信息方面表现出色，但在需要领域特定知识或最新信息的任务中往往有所欠缺[[15](#bib.bib15)]。为了缓解这些限制，我们使LLM能够利用外部工具来解决问题[[14](#bib.bib14),
    [42](#bib.bib42), [30](#bib.bib30), [2](#bib.bib2)]。最后，在执行自然语言程序时，不正确地确定下一步可能导致不同的最终结果。我们通过要求LLM解释器评估当前结果来解决这个问题，从而识别最合适的后续步骤。CoRE的整体执行流程如图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents")所示。'
- en: '![Refer to caption](img/4f521cb6d72ec1df2e6f205c7cb0016b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4f521cb6d72ec1df2e6f205c7cb0016b.png)'
- en: 'Figure 2: An example showing how the CoRE system executes one step.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个示例展示了CoRE系统如何执行一步操作。
- en: 'In summary, the key contributions of the work are listed as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，工作的主要贡献如下：
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design a CoRE language that unifies natural language programming, pseudo-code
    programming and flow programming. The CoRE language logically structures natural
    language instructions.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种CoRE语言，统一了自然语言编程、伪代码编程和流程编程。CoRE语言逻辑地结构化自然语言指令。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose the CoRE system, which utilizes Large Language Model (LLM) as an
    interpreter to interpret and execute instructions step-by-step. During execution,
    the LLM follows the instructions and leverages both information retrieval and
    external tools to enhance its effectiveness.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了CoRE系统，该系统利用大型语言模型（LLM）作为解释器来逐步解释和执行指令。在执行过程中，LLM按照指令操作，并利用信息检索和外部工具来提升其有效性。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We verify the effectiveness and efficiency of our system based on public benchmark
    datasets. Specifically, we employ our proposed system for agent task solving based
    on natural language programs, showcasing its practical capabilities.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们基于公开的基准数据集验证了系统的有效性和效率。具体来说，我们使用我们提出的系统进行基于自然语言程序的代理任务解决，展示了其实际能力。
- en: 'In the following part of this paper, we first provide the related work in Section
    [2](#S2 "2 Related Work ‣ AIOS Compiler: LLM as Interpreter for Natural Language
    Programming and Flow Programming of AI Agents"). In Section [3](#S3 "3 The CoRE
    System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and
    Flow Programming of AI Agents") we present the CoRE framework and how the framework
    can be applied to LLM agents. We provide the experimental results in Section [4](#S4
    "4 Experiments ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents"), and conclude the work together with future
    directions in Section [5](#S5 "5 Conclusions and Future Work ‣ AIOS Compiler:
    LLM as Interpreter for Natural Language Programming and Flow Programming of AI
    Agents").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文的以下部分，我们首先在第[2](#S2 "2 Related Work ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents")节中提供相关工作。在第[3](#S3
    "3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents")节中，我们介绍了CoRE框架及其如何应用于LLM代理。在第[4](#S4 "4 Experiments
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents")节中，我们提供了实验结果，并在第[5](#S5 "5 Conclusions and Future Work
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents")节中总结了工作并展望未来方向。'
- en: 2 Related Work
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Natural Language Programming
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 自然语言编程
- en: Research in natural language programming [[17](#bib.bib17), [5](#bib.bib5),
    [46](#bib.bib46), [28](#bib.bib28), [10](#bib.bib10), [13](#bib.bib13)] primarily
    focus on addressing the ambiguity in translating natural language into programming
    language statements. [Heidorn](#bib.bib17) [[17](#bib.bib17)] proposes to adopt
    heuristic NLP encoding and decoding rules to develop an automatic programming
    system that can accept natural language dialogues. [Vadas and Curran](#bib.bib46)
    [[46](#bib.bib46)] introduce a prototype system that can translate certain English
    instructions into executable Python code using Combinatory Categorial Grammar
    (CCG) parser, which uses unrestricted syntax to cover a wide range of user instruction
    semantics. [Mihalcea et al.](#bib.bib35) [[35](#bib.bib35)] implement a procedural
    natural language programming system to convert natural language to programming
    language. Early natural language programming techniques are restricted in extensibility
    by the need to create domain-specific languages (DSLs). To avoid the problems
    of repeatedly designing new DSLs, [Desai et al.](#bib.bib10) [[10](#bib.bib10)]
    propose a general generative framework for constructing a program that takes natural
    language input and produces the expressions in the target DSL. Further, [Ernst](#bib.bib13)
    [[13](#bib.bib13)] leverages neural networks, i.e., the recurrent neural networks
    (RNN), to convert English specifications of file system operations into corresponding
    bash commands.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言编程的研究[[17](#bib.bib17), [5](#bib.bib5), [46](#bib.bib46), [28](#bib.bib28),
    [10](#bib.bib10), [13](#bib.bib13)]主要集中在解决将自然语言翻译成编程语言语句中的歧义问题。[Heidorn](#bib.bib17)
    [[17](#bib.bib17)]建议采用启发式NLP编码和解码规则，开发一个可以接受自然语言对话的自动编程系统。[Vadas和Curran](#bib.bib46)
    [[46](#bib.bib46)]介绍了一个原型系统，该系统可以使用组合类别语法（CCG）解析器将某些英语指令翻译成可执行的Python代码，该解析器使用无限制的语法来涵盖广泛的用户指令语义。[Mihalcea等](#bib.bib35)
    [[35](#bib.bib35)]实现了一个过程性自然语言编程系统，将自然语言转换为编程语言。早期的自然语言编程技术由于需要创建特定领域语言（DSLs）而限制了可扩展性。为了避免反复设计新DSLs的问题，[Desai等](#bib.bib10)
    [[10](#bib.bib10)]提出了一个通用生成框架，用于构建一个接受自然语言输入并生成目标DSL表达式的程序。此外，[Ernst](#bib.bib13)
    [[13](#bib.bib13)]利用神经网络，即递归神经网络（RNN），将文件系统操作的英语规范转换为相应的bash命令。
- en: 2.2 Large Language Models and AI Agents for Problem Solving
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大语言模型与AI代理在问题解决中的应用
- en: Large Language Models (LLMs) have emerged as powerful tools for problem solving,
    encompassing tasks in reasoning, planning, and code generation. LLM reasoning
    typically involves decomposing a complex task into a sequence of steps, also known
    as a reasoning chain [[49](#bib.bib49)]. Prominent approaches in LLM reasoning
    include Chain-of-Thought (CoT) and its derivatives [[49](#bib.bib49), [26](#bib.bib26)].
    To further improve the reasoning ability of LLM, several work has been proposed.
    The Self-consistency method [[47](#bib.bib47)] samples multiple reasoning paths
    and selects the most consistent outcome by voting. Additionally, classical data
    structures like trees and graphs are utilized to enhance reasoning efficiency
    and accuracy in fewer steps [[52](#bib.bib52), [3](#bib.bib3)]. Apart from reasoning,
    planning is also an important task that can be used to solve problems. LLM Planning
    involves generating a series of actions to achieve the predefined goals [[16](#bib.bib16)].
    Recent advancements include direct prompting of LLMs for planning tasks, showing
    promising results [[20](#bib.bib20), [45](#bib.bib45), [11](#bib.bib11)]. Finite
    state machines have been integrated into LLM to enhance the planning ability [[29](#bib.bib29),
    [51](#bib.bib51)]. ReAct [[53](#bib.bib53)] proposes to leverage external tools
    like search engine to enhance the LLM planning. Besides, considering the powerful
    ability of LLM in programming, recent work propose to generate programming code
    to solve problems [[32](#bib.bib32), [22](#bib.bib22), [31](#bib.bib31), [6](#bib.bib6),
    [23](#bib.bib23), [40](#bib.bib40), [36](#bib.bib36)]. Furthermore, the “self-reflection”
    mechanism [[33](#bib.bib33), [39](#bib.bib39), [44](#bib.bib44)] enables LLMs
    to critique their own outputs, significantly enhancing performance in tasks such
    as reasoning [[3](#bib.bib3)] and code generation [[7](#bib.bib7)]. In contrast
    to existing methods that directly use LLMs for generating solutions, the proposed
    CoRE system utilizes LLMs as interpreters, executing solutions designed by humans
    to address complex questions. This approach leverages human creativity in solution
    design, coupled with LLM’s ability, to enhance problem-solving capabilities in
    natural language programming contexts.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已成为解决问题的强大工具，涵盖推理、规划和代码生成等任务。LLM 推理通常涉及将复杂任务分解为一系列步骤，也称为推理链 [[49](#bib.bib49)]。LLM
    推理中的突出方法包括思维链（CoT）及其衍生方法 [[49](#bib.bib49), [26](#bib.bib26)]。为了进一步提高 LLM 的推理能力，已提出几种方法。自我一致性方法
    [[47](#bib.bib47)] 通过投票从多个推理路径中选择最一致的结果。此外，经典数据结构如树和图被利用于在更少的步骤中提高推理效率和准确性 [[52](#bib.bib52),
    [3](#bib.bib3)]。除了推理，规划也是解决问题的重要任务。LLM 规划涉及生成一系列行动以实现预定目标 [[16](#bib.bib16)]。近期进展包括直接提示
    LLM 进行规划任务，显示出有前景的结果 [[20](#bib.bib20), [45](#bib.bib45), [11](#bib.bib11)]。有限状态机已被集成到
    LLM 中以增强规划能力 [[29](#bib.bib29), [51](#bib.bib51)]。ReAct [[53](#bib.bib53)] 提出了利用搜索引擎等外部工具来增强
    LLM 规划能力。此外，考虑到 LLM 在编程方面的强大能力，近期工作提出生成编程代码来解决问题 [[32](#bib.bib32), [22](#bib.bib22),
    [31](#bib.bib31), [6](#bib.bib6), [23](#bib.bib23), [40](#bib.bib40), [36](#bib.bib36)]。此外，“自我反思”机制
    [[33](#bib.bib33), [39](#bib.bib39), [44](#bib.bib44)] 使 LLM 能够批判自己的输出，大大提升了在推理
    [[3](#bib.bib3)] 和代码生成 [[7](#bib.bib7)] 等任务中的表现。与现有直接使用 LLM 生成解决方案的方法不同，提出的 CoRE
    系统利用 LLM 作为解释器，执行由人类设计的解决方案以解决复杂问题。这种方法结合了人类在解决方案设计中的创造力与 LLM 的能力，以增强在自然语言编程环境中的问题解决能力。
- en: '![Refer to caption](img/ef8bf251abb6c21bf49116f3966e4d4a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ef8bf251abb6c21bf49116f3966e4d4a.png)'
- en: 'Figure 3: An overview of the CoRE LLM interpreter system.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: CoRE LLM 解释器系统概述。'
- en: 3 The CoRE System
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CoRE 系统
- en: In this section, we will introduce how we define the natural language programming
    syntax and how to use LLM as an interpreter to interpret and execute natural language
    programs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何定义自然语言编程语法以及如何使用 LLM 作为解释器来解释和执行自然语言程序。
- en: 3.1 CoRE Language Syntax
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 CoRE 语言语法
- en: 'To organize natural language instructions, we define the basic structural representation
    for each step, which consists of four components. An example can be found in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AIOS Compiler: LLM as Interpreter for
    Natural Language Programming and Flow Programming of AI Agents").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了组织自然语言指令，我们定义了每个步骤的基本结构表示，它由四个组件组成。一个示例见图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ AIOS 编译器：LLM
    作为自然语言编程和 AI 代理流编程的解释器")。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Name: Each step in the program is uniquely identified by a step name.
    This identifier is analogous to function identifiers in traditional programming
    languages, which facilitates navigation and reference within the program structure,
    ensuring that each operation within the program can be distinctly addressed and
    accessed.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤名称：程序中的每个步骤都通过步骤名称唯一标识。这个标识符类似于传统编程语言中的函数标识符，有助于在程序结构中进行导航和引用，确保程序中的每个操作可以明确地进行定位和访问。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Type: The step type categorizes the nature of the operation being performed
    in each step, analogous to control structures in conventional programming. We
    define three primary step types:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤类型：步骤类型对每个步骤中执行的操作性质进行分类，类似于传统编程中的控制结构。我们定义了三种主要的步骤类型：
- en: –
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Process: Akin to a procedural statement in traditional programming, this step
    type executes a specific operation and transitions to the next specified step.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过程：类似于传统编程中的过程语句，这种步骤类型执行特定操作并过渡到下一个指定步骤。
- en: –
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Decision: Corresponding to conditional statements (e.g., “if-else”), this step
    involves branching the program flow based on evaluated conditions, leading to
    multiple potential paths.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 决策：对应于条件语句（例如，“if-else”），此步骤涉及根据评估条件对程序流进行分支，导致多个潜在路径。
- en: –
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Terminal: Similar to the “end” or “return” statement, this step marks the conclusion
    of the program, indicating that no further steps are to be executed.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 终端：类似于“end”或“return”语句，这个步骤标志着程序的结束，表示不再执行任何进一步的步骤。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Instruction: The step instruction explicates the task to be conducted
    at a step. This component is integral as it provides the instruction and content
    for execution, paralleling the statement block in traditional programming languages.
    By demonstrating operations in natural language, NLPg lowers the barrier to programming,
    making it more readable for non-expert programmers.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤指令：步骤指令阐明了在步骤中要执行的任务。这个组件是至关重要的，因为它提供了执行的指令和内容，与传统编程语言中的语句块类似。通过用自然语言展示操作，NLPg
    降低了编程的门槛，使其对非专业程序员更具可读性。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step Connection: Step connections define the progression from one step to another,
    establishing the flow of the program execution. In process steps, a single subsequent
    step is specified. In decision steps, multiple pathways are delineated based on
    conditions. Terminal steps, by definition, do not lead to any future steps, indicating
    the end of program execution.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤连接：步骤连接定义了从一个步骤到另一个步骤的进展，建立了程序执行的流。在过程步骤中，指定一个单独的后续步骤。在决策步骤中，根据条件描述多个路径。终端步骤，按定义，不会导致任何未来的步骤，表示程序执行的结束。
- en: 'For each step in the program, the above four components are separated by “:::”
    (as illustrated in the CoRE language in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow
    Programming of AI Agents")). Other special tokens can also be used to separate
    different components.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于程序中的每个步骤，上述四个组件由“:::”分隔（如图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ AIOS 编译器：LLM 作为自然语言编程和
    AI 代理流编程的解释器") 中 CoRE 语言所示）。也可以使用其他特殊标记来分隔不同组件。
- en: '![Refer to caption](img/83df2ac177fb5aba7e0603621082d0dc.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83df2ac177fb5aba7e0603621082d0dc.png)'
- en: 'Figure 4: An example showing how the CoRE system retrieves relevant information.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：示例展示了 CoRE 系统如何检索相关信息。
- en: 'In programming languages, there are three basic control constructs in programming
    [[9](#bib.bib9), [41](#bib.bib41)]: sequence, selection and iteration. These three
    basic constructs can be easily designed within the CoRE language.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程语言中，编程有三种基本控制结构[[9](#bib.bib9), [41](#bib.bib41)]：序列、选择和迭代。这三种基本结构可以轻松地在
    CoRE 语言中设计。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sequence: Sequence in programming is the execution of statements in a linear
    order, with each statement leading to the next. In the CoRE framework, this construct
    is designed by setting the “Step Connection” to point to the subsequent step.
    Each step operates under the Process type until the sequence concludes.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 序列：编程中的序列是按线性顺序执行语句，每条语句引导到下一条。在 CoRE 框架中，这种结构通过将“步骤连接”设置为指向后续步骤来设计。每个步骤在过程类型下操作，直到序列结束。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Selection: Selection in programming languages facilitates conditional branching,
    allowing the program to execute different sequences of steps based on specific
    conditions. This is implemented using the Decision step type where the “Step Connection”
    part explicitly outlines multiple potential paths. Each branch is defined by a
    condition stated within the “Step Connection” part, guiding the program flow to
    various steps depending on the conditions.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择：编程语言中的选择功能有助于条件分支，使程序能够根据特定条件执行不同的步骤序列。这是通过决策步骤类型来实现的，其中“步骤连接”部分明确列出了多个潜在路径。每个分支由“步骤连接”部分中列出的条件定义，指导程序流程根据条件前往不同的步骤。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Iteration: Iteration involves repeating a set of operations until a certain
    condition is met, akin to loops in conventional programming. In the CoRE framework,
    we utilize a step with the Decision type to assess whether the loop condition
    has been fulfilled. At the end of one loop cycle, the “Step Connection” is configured
    to point back to the previous Decision step, thereby enabling the continuation
    of the loop.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代：迭代涉及重复一组操作，直到满足某个条件，类似于传统编程中的循环。在 CoRE 框架中，我们利用决策类型的步骤来评估循环条件是否已满足。在一个循环周期结束时，“步骤连接”配置为指向之前的决策步骤，从而使循环得以继续。
- en: 3.2 LLM as Interpreter
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLM 作为解释器
- en: 'In this section, we will discuss how the CoRE system utilizes a Large Language
    Model (LLM) as an interpreter to execute programs written in the CoRE language.
    We will demonstrate the execution of a single step within the CoRE system, which
    is illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Large Language Models and
    AI Agents for Problem Solving ‣ 2 Related Work ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents"). More specifically,
    the system executes a single step in four procedures. First of all, the interpreter
    determines the useful information to execute the current step. Then the interpreter
    will integrate all relevant information to construct the prompt. Based on the
    generated prompt, the interpreter will generate response and may utilize tools
    to execute the current step. Finally, after executing the current step, the interpreter
    will determine the next step based on step type and execution results. We will
    introduce the four parts in details.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 CoRE 系统如何利用大型语言模型 (LLM) 作为解释器来执行用 CoRE 语言编写的程序。我们将演示 CoRE 系统中单个步骤的执行，如图
    [3](#S2.F3 "图 3 ‣ 2.2 大型语言模型和 AI 代理用于问题解决 ‣ 2 相关工作 ‣ AIOS 编译器：LLM 作为自然语言编程和 AI
    代理流编程的解释器") 所示。更具体地说，该系统在四个程序中执行一个单一步骤。首先，解释器确定执行当前步骤所需的有用信息。然后解释器将整合所有相关信息以构建提示。根据生成的提示，解释器将生成响应，并可能利用工具来执行当前步骤。最后，在执行当前步骤后，解释器将根据步骤类型和执行结果确定下一个步骤。我们将详细介绍这四个部分。
- en: 3.2.1 Observation Retrieval from Memory
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 从内存中检索观察
- en: 'This initial procedure is critical since it sets the stage for the entire execution
    process of the current step. Figure [4](#S3.F4 "Figure 4 ‣ 3.1 CoRE Language Syntax
    ‣ 3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents") shows an example. The system’s memory serves
    as a repository of all prior observations related to the program, where the observation
    represents the results of tool execution, such as search results. During this
    phase, the interpreter scans the memory to identify records that are relevant
    to the current instruction. This selective retrieval ensures that the interpreter’s
    decisions are informed by accurate and contextually relevant data, which is crucial
    for the successful execution of the program.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个初始程序是关键，因为它为当前步骤的整个执行过程奠定了基础。图 [4](#S3.F4 "图 4 ‣ 3.1 CoRE 语言语法 ‣ 3 CoRE 系统
    ‣ AIOS 编译器：LLM 作为自然语言编程和 AI 代理流编程的解释器") 展示了一个示例。系统的内存作为所有与程序相关的先前观察的存储库，其中观察代表工具执行的结果，例如搜索结果。在此阶段，解释器扫描内存以识别与当前指令相关的记录。这种选择性检索确保了解释器的决策是基于准确和上下文相关的数据，这对于程序的成功执行至关重要。
- en: '![Refer to caption](img/11b695f56fb432a75407286203b85b0b.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/11b695f56fb432a75407286203b85b0b.png)'
- en: 'Figure 5: An example showing how the CoRE system analyze the output from the
    LLM interpreter.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：一个示例展示了 CoRE 系统如何分析 LLM 解释器的输出。
- en: 3.2.2 Input Prompt Construction
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 输入提示构建
- en: 'Constructing the prompt is essentially about synthesizing the information into
    a comprehensive and coherent query that the LLM can understand and respond to
    effectively. This involves combining multiple information into a single, structured
    prompt that guides the LLM towards generating the most appropriate and contextually
    relevant response. In the CoRE system, the interpreter constructs a detailed prompt
    with four elements:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 构建提示本质上是将信息综合成一个全面且连贯的查询，使 LLM 能够理解并有效回应。这涉及将多条信息合并成一个结构化的提示，引导 LLM 生成最合适且具有上下文相关性的回应。在
    CoRE 系统中，解释器构建一个包含四个元素的详细提示：
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task Description: The query that defines the entire program, acting as the
    primary input to guide the system’s operations.'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务描述：定义整个程序的查询，作为指导系统操作的主要输入。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Current Progress: Summarizes the previous steps including what has been done
    or decided, helping maintain a narrative flow.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前进展：总结了之前的步骤，包括已经完成或决定的内容，帮助维持叙事流。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Observation: This part may not be included in every step. When relevant information
    is retrieved from the memory by the interpreter, it is incorporated here.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察：这一部分可能不会出现在每一步中。当解释器从记忆中提取相关信息时，这里将包含这些信息。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Current Instruction: Specifies the action to be taken in natural language,
    directing the interpreter on how to proceed in the current step.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前指令：以自然语言指定要采取的行动，指导解释器在当前步骤中如何进行。
- en: 3.2.3 Output Analysis
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 输出分析
- en: 'While the LLM can generate direct responses, complex tasks may require capabilities
    beyond its immediate scope. Incorporating the use of specialized tools when necessary
    extends the LLM’s capabilities, allowing the system to handle a broader range
    of tasks effectively. A demonstrative example of the execution process is shown
    in Figure [5](#S3.F5 "Figure 5 ‣ 3.2.1 Observation Retrieval from Memory ‣ 3.2
    LLM as Interpreter ‣ 3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for
    Natural Language Programming and Flow Programming of AI Agents"). In the CoRE
    system, the interpreter will make a decision about if or not to employ specialized
    tools based on the LLM’s initial response and the demands of the task at hand,
    which ensures that the system remains highly functional and versatile, actively
    solving problems rather than merely processing the language prompt for the current
    step. Specifically, if tool usage is warranted, the system will select the suitable
    tool, configure it with the necessary parameters, execute it, and integrate the
    output into the ongoing process.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管 LLM 可以生成直接回应，但复杂任务可能需要超出其即时范围的能力。在必要时引入专门工具的使用，扩展了 LLM 的能力，使系统能够有效处理更广泛的任务。图
    [5](#S3.F5 "Figure 5 ‣ 3.2.1 Observation Retrieval from Memory ‣ 3.2 LLM as Interpreter
    ‣ 3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents") 展示了执行过程的示例。在 CoRE 系统中，解释器将根据 LLM 的初步回应和当前任务的需求决定是否使用专门工具，这确保系统保持高度功能性和多样性，主动解决问题而不仅仅是处理当前步骤的语言提示。具体而言，如果需要使用工具，系统将选择合适的工具，配置必要的参数，执行它，并将输出集成到正在进行的过程中。'
- en: 3.2.4 Branching Analysis
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 分支分析
- en: '![Refer to caption](img/eb570220bb754c763f4e54d30a5d1817.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/eb570220bb754c763f4e54d30a5d1817.png)'
- en: 'Figure 6: An example showing how the CoRE system determines the next step in
    the flow.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：一个示例，展示了 CoRE 系统如何确定流程中的下一步。
- en: 'Determining the appropriate next step in the program is critical, especially
    in multi-branch scenarios where different outcomes can lead to different subsequent
    actions. Figure [6](#S3.F6 "Figure 6 ‣ 3.2.4 Branching Analysis ‣ 3.2 LLM as Interpreter
    ‣ 3 The CoRE System ‣ AIOS Compiler: LLM as Interpreter for Natural Language Programming
    and Flow Programming of AI Agents") shows an example. In the CoRE language interpreter,
    the Decision steps indicate multiple branches with the corresponding conditions.
    The interpreter uses LLM to decide if the prompt satisfies the natural language
    described branching condition or not and which next step to take. This adaptive
    approach allows the system to navigate through decision points effectively, ensuring
    logical progression toward the program’s goals.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 确定程序中的适当下一步至关重要，尤其是在多分支场景中，不同的结果可能导致不同的后续操作。图 [6](#S3.F6 "图 6 ‣ 3.2.4 分支分析 ‣
    3.2 LLM作为解释器 ‣ 3 CoRE系统 ‣ AIOS编译器：LLM作为自然语言编程和AI代理流编程的解释器") 显示了一个示例。在CoRE语言解释器中，决策步骤指示了具有相应条件的多个分支。解释器使用LLM来决定提示是否满足自然语言描述的分支条件，以及采取哪个下一步骤。这种自适应方法使系统能够有效地通过决策点，确保逻辑上的程序目标推进。
- en: 4 Experiments
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Backbone Large Language Model (LLM)
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主干大型语言模型 (LLM)
- en: 'We conduct experiments on both closed-source and open-source LLMs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在闭源和开源LLM上进行实验：
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-4 [[37](#bib.bib37)] (Closed-source) is a generative pre-trained transformer
    of OpenAI. In this work, we use the GPT-4-1106-preview version.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4 [[37](#bib.bib37)]（闭源）是OpenAI的一个生成式预训练变换器。在这项工作中，我们使用的是GPT-4-1106-preview版本。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixtral-8x7B [[21](#bib.bib21)] (Open-source) is a pre-trained generative Sparse
    Mixture of Experts with 46.7 billion parameters.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mixtral-8x7B [[21](#bib.bib21)]（开源）是一个预训练的生成式稀疏专家混合模型，具有46.7亿个参数。
- en: 4.2 Planning Schema of LLMs
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LLM的规划方案
- en: 'We adopt the following LLM-based agent planning schema:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用以下基于LLM的代理规划方案：
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Zero-shot Learning (Zero) directly inputs the query to the LLM.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零-shot学习（Zero）直接将查询输入LLM。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Chain-of-Thought (CoT) [[49](#bib.bib49)] induces the LLM to generate a coherent
    language sequence that serves as a meaningful intermediate step bridging the input
    query and the output answer.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 思维链（CoT） [[49](#bib.bib49)] 促使LLM生成一个连贯的语言序列，作为输入查询和输出答案之间有意义的中间步骤。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Few-shot Learning (Few) presents a set of high-quality demonstrations in the
    prompt, each consisting of both input and desired output on the target task.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 少量示例学习（Few）在提示中展示了一组高质量的示例，每个示例包括目标任务的输入和期望输出。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CoRE is our natural language programming method with LLM as an interpreter.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoRE是我们以LLM作为解释器的自然语言编程方法。
- en: 4.3 Benchmark Datasets
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基准数据集
- en: 'We conduct experiments on a benchmark dataset, OpenAGI [[14](#bib.bib14)].
    The OpenAGI benchmark tasks are categorized based on their output type and ground-truth
    label type (Task 1, 2, and 3). Then, based on different task types, different
    metrics are employed to gauge the performance: CLIP Score [[19](#bib.bib19)],
    assessing the similarity between text and image, is utilized for Text-to-Image
    tasks; BERT Score [[55](#bib.bib55)], evaluating text generation with BERT, is
    applied when both data labels and the expected outputs are texts; and ViT Score
    [[50](#bib.bib50)] gauges the similarity between the image label and image output.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在基准数据集OpenAGI [[14](#bib.bib14)]上进行实验。OpenAGI基准任务根据输出类型和真实标签类型（任务1、2和3）进行分类。然后，根据不同的任务类型，采用不同的指标来衡量性能：CLIP
    Score [[19](#bib.bib19)]用于评估文本与图像之间的相似性，适用于文本到图像任务；BERT Score [[55](#bib.bib55)]用于评估文本生成，应用于数据标签和期望输出均为文本的情况；ViT
    Score [[50](#bib.bib50)]用于衡量图像标签与图像输出之间的相似性。
- en: 4.4 Implementation Details
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实施细节
- en: Our framework and all baselines are implemented by PyTorch, an open-source library.
    We follow the implementation setting of the OpenAGI platform [[14](#bib.bib14)]
    for Zero-shot and few-shot learnings. We leverage the DSPy framework [[24](#bib.bib24),
    [25](#bib.bib25)] to apply the CoT strategy to the OpenAGI platform. We also tried
    Program-of-Thought [[6](#bib.bib6)] and ReAct [[54](#bib.bib54)] strategies on
    the OpenAGI platform. However, the ReAct strategy requires text observation, which
    is unsuitable for our OpenAGI task since some observations are in image format,
    and Program-of-Thought cannot generate executable codes. Thus, we did not include
    them as the baselines.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架和所有基线模型均由 PyTorch 实现，这是一种开源库。我们遵循 OpenAGI 平台 [[14](#bib.bib14)] 的实现设置用于
    Zero-shot 和 Few-shot 学习。我们利用 DSPy 框架 [[24](#bib.bib24), [25](#bib.bib25)] 将 CoT
    策略应用于 OpenAGI 平台。我们还尝试了 Program-of-Thought [[6](#bib.bib6)] 和 ReAct [[54](#bib.bib54)]
    策略在 OpenAGI 平台上。然而，ReAct 策略需要文本观察，这不适合我们的 OpenAGI 任务，因为某些观察是图像格式，而 Program-of-Thought
    无法生成可执行代码。因此，我们没有将它们作为基线模型。
- en: 4.5 Experimental Analysis
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 实验分析
- en: 'The experiment results on the OpenAGI benchmark are shown in Table [1](#S4.T1
    "Table 1 ‣ 4.5 Experimental Analysis ‣ 4 Experiments ‣ AIOS Compiler: LLM as Interpreter
    for Natural Language Programming and Flow Programming of AI Agents"). Each row
    stands for a type of task, each column represents the planning schema of an LLM
    interpreter, and every four columns are the results of the same LLM interpreter.
    From the results, we can see that our CoRE planning schema is better on average
    performance than any baseline under both Mixtral and GPT-4 as the interpreters.
    When using Mixtral as the interpreter, CoRE outperforms Zero-shot and CoT under
    each type of task, and is better than Few-shot learning on Task 2 and average
    score, though worse on Task 3 and slightly worse on Task 1\. When using GPT-4
    as the interpreter, CoT, Few-shot has similar performance on Task 1 and Task 3,
    while on Task 2 and average score, CoRE is still the best. It may be worth noting
    that it is unfair to compare CoRE with Few-shot learning since we do not directly
    provide the output format and output example in the prompt. However, even without
    using such examples, the CoRE planning strategy is still better than the Few-shot
    strategy on average. We also find that even for the same CoRE program, the system
    may perform differently when using different LLM as interpreters, which means
    that the performance of natural language programming depends on the natural language
    understanding ability of the LLM interpreter.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果在 OpenAGI 基准测试中的表现见表 [1](#S4.T1 "表 1 ‣ 4.5 实验分析 ‣ 4 实验 ‣ AIOS 编译器：LLM 作为自然语言编程和
    AI 代理流编程的解释器")。每一行代表一种任务类型，每一列表示 LLM 解释器的规划模式，每四列为同一 LLM 解释器的结果。从结果可以看出，我们的 CoRE
    规划模式在 Mixtral 和 GPT-4 作为解释器的情况下，在平均性能上优于任何基线。在使用 Mixtral 作为解释器时，CoRE 在每种任务类型下都优于
    Zero-shot 和 CoT，并且在任务 2 和平均得分上优于 Few-shot learning，尽管在任务 3 和任务 1 上稍逊一筹。在使用 GPT-4
    作为解释器时，CoT 和 Few-shot 在任务 1 和任务 3 上表现相似，而在任务 2 和平均得分上，CoRE 仍然是最佳的。值得注意的是，由于我们没有在提示中直接提供输出格式和输出示例，因此将
    CoRE 与 Few-shot learning 进行比较是不公平的。然而，即使没有使用这些示例，CoRE 规划策略在平均性能上仍然优于 Few-shot
    策略。我们还发现，即使对于相同的 CoRE 程序，当使用不同的 LLM 作为解释器时，系统的表现可能会有所不同，这意味着自然语言编程的性能依赖于 LLM 解释器的自然语言理解能力。
- en: '| Metrics / Task | Mixtral (open source) as LLM interpreter | GPT-4 (closed-source)
    as LLM interpreter |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 指标 / 任务 | Mixtral（开源）作为 LLM 解释器 | GPT-4（闭源）作为 LLM 解释器 |'
- en: '| Zero | CoT | Few | CoRE (Ours) | Zero | CoT | Few | CoRE (Ours) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Zero | CoT | Few | CoRE（我们的） | Zero | CoT | Few | CoRE（我们的） |'
- en: '| Task 1 (CLIP Score) | 0.0 | 0.0 | $0.1839$ | 0.1825 | 0.0 | 0.2732 | 0.1837
    | $0.3030$ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 任务 1（CLIP 得分） | 0.0 | 0.0 | $0.1839$ | 0.1825 | 0.0 | 0.2732 | 0.1837 | $0.3030$
    |'
- en: '| Task 2 (BERT Score) | 0.1092 | 0.1987 | 0.0687 | $0.2593$ | 0.2076 | 0.2266
    | 0.5277 | $0.5756$ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 任务 2（BERT 得分） | 0.1092 | 0.1987 | 0.0687 | $0.2593$ | 0.2076 | 0.2266 | 0.5277
    | $0.5756$ |'
- en: '| Task 3 (ViT Score) | 0.1949 | 0.1562 | $0.5501$ | 0.2437 | 0.5058 | 0.6736
    | $0.6916$ | 0.6611 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 任务 3（ViT 得分） | 0.1949 | 0.1562 | $0.5501$ | 0.2437 | 0.5058 | 0.6736 | $0.6916$
    | 0.6611 |'
- en: '| Average over tasks | 0.1206 | 0.1736 | 0.1887 | $0.2483$ | 0.2378 | 0.3359
    | 0.5391 | $0.5744$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 任务平均得分 | 0.1206 | 0.1736 | 0.1887 | $0.2483$ | 0.2378 | 0.3359 | 0.5391 |
    $0.5744$ |'
- en: '| % of Valid Plans | 23.08 | 38.46 | 46.15 | $56.92$ | 53.85 | 60.00 | 83.08
    | $92.31$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 有效计划的百分比 | 23.08 | 38.46 | 46.15 | $56.92$ | 53.85 | 60.00 | 83.08 | $92.31$
    |'
- en: 'Table 1: OpenAGI [[14](#bib.bib14)] benchmark task performances under different
    settings. Zero is for Zero-shot Learning, Few is for Few-shot Learning. The boldface
    numbers denote the highest score under each task type using the same LLM.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在不同设置下 OpenAGI [[14](#bib.bib14)] 基准任务性能。Zero 代表零样本学习，Few 代表少样本学习。粗体数字表示在每种任务类型下使用相同
    LLM 时的最高得分。
- en: 5 Conclusions and Future Work
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: In this study, we introduce a novel system, CoRE, for Code Representation and
    Execution. CoRE is designed to bridge natural language programming, pseudo-code,
    and flow programming through the development of a unified CoRE language for the
    construction of AI Agents. CoRE leverages natural language as the programming
    interface, which lowers the programming barrier and advocates the democracy of
    programming, so that even ordinary users can create their AI Agents. Our system
    leverages Large Language Models (LLMs) as interpreters to process and execute
    natural language instructions. Throughout execution, the interpreter dynamically
    retrieves necessary information, utilizes appropriate external tools, and navigates
    through instructions based on previous outputs. The experimental outcomes validate
    the efficacy of the CoRE system in natural language programming.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们引入了一种新颖的系统，CoRE，用于代码表示和执行。CoRE 旨在通过开发一个统一的 CoRE 语言来架起自然语言编程、伪代码和流程编程之间的桥梁，从而构建
    AI 代理。CoRE 利用自然语言作为编程接口，降低了编程门槛，倡导编程的民主化，使得即使是普通用户也能创建他们的 AI 代理。我们的系统利用大型语言模型（LLMs）作为解释器来处理和执行自然语言指令。在执行过程中，解释器动态地检索必要的信息，利用适当的外部工具，并根据先前的输出导航指令。实验结果验证了
    CoRE 系统在自然语言编程中的有效性。
- en: While CoRE demonstrates promising results, it currently relies on manually crafted
    programs, which may introduce inefficiencies due to the inherent ambiguities of
    natural language. To address this, future research could explore the development
    of automated systems for generating natural language programming instructions.
    This automation would help standardize instruction clarity and precision, potentially
    improving system performance. Additionally, a future direction is to expand CoRE’s
    language support to facilitate international use and implement real-time debugging
    features to aid in education and assist novice programmers, further broadening
    the system’s utility and accessibility.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CoRE 展示了令人期待的结果，但它目前依赖于手工编写的程序，这可能由于自然语言固有的模糊性而引入低效。为了解决这个问题，未来的研究可以探索开发自动生成自然语言编程指令的系统。这种自动化将有助于标准化指令的清晰性和准确性，可能改善系统性能。此外，一个未来的方向是扩展
    CoRE 的语言支持，以促进国际使用，并实现实时调试功能，以帮助教育和辅助新手程序员，进一步扩大系统的实用性和可及性。
- en: References
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1]'
- en: 'Ahn et al. [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
    Karol Hausman, et al. 2022. Do as i can, not as i say: Grounding language in robotic
    affordances. *arXiv preprint arXiv:2204.01691* (2022).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等 [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar
    Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
    Hausman 等 2022。按我能做的，不是按我说的做：将语言与机器人能力对接。*arXiv 预印本 arXiv:2204.01691*（2022）。
- en: 'Besta et al. [2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski,
    Piotr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with
    large language models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 38\. 17682–17690.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 等 [2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski,
    Piotr Nyczyk 等 2024。思想图谱：用大型语言模型解决复杂问题。在 *AAAI 人工智能会议论文集*，第 38 卷。17682–17690。
- en: Borgeaud et al. [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by
    retrieving from trillions of tokens. In *International conference on machine learning*.
    PMLR, 2206–2240.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud 等 [2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark 等 2022。通过从万亿个标记中检索来改进语言模型。在 *国际机器学习会议* 上。PMLR，2206–2240。
- en: Bruckman and Edwards [1999] Amy Bruckman and Elizabeth Edwards. 1999. Should
    we leverage natural-language knowledge? An analysis of user errors in a natural-language-style
    programming language. In *Proceedings of the SIGCHI conference on Human Factors
    in Computing Systems*. 207–214.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruckman and Edwards [1999] Amy Bruckman 和 Elizabeth Edwards. 1999. 我们是否应该利用自然语言知识？对自然语言风格编程语言中用户错误的分析。见于*SIGCHI
    计算机系统人因会议论文集*。207–214。
- en: 'Chen et al. [2023b] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    2023b. Program of Thoughts Prompting: Disentangling Computation from Reasoning
    for Numerical Reasoning Tasks. *Transactions on Machine Learning Research* (2023).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023b] Wenhu Chen, Xueguang Ma, Xinyi Wang, 和 William W. Cohen.
    2023b. 思维提示程序：将计算与推理解耦以处理数值推理任务。*机器学习研究交易* (2023)。
- en: Chen et al. [2023a] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023a. Teaching large language models to self-debug. *arXiv preprint arXiv:2304.05128*
    (2023).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023a] Xinyun Chen, Maxwell Lin, Nathanael Schärli, 和 Denny Zhou.
    2023a. 教授大型语言模型自我调试。*arXiv 预印本 arXiv:2304.05128* (2023)。
- en: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,
    et al. 2024. Scaling instruction-finetuned language models. *Journal of Machine
    Learning Research* 25, 70 (2024), 1–53.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi
    Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma
    等。2024. 扩展指令微调的语言模型。*机器学习研究杂志* 25, 70 (2024)，1–53。
- en: Dahl et al. [1972] Ole-Johan Dahl, Edsger Wybe Dijkstra, and Charles Antony Richard
    Hoare. 1972. *Structured programming*. Academic Press Ltd.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dahl et al. [1972] Ole-Johan Dahl, Edsger Wybe Dijkstra, 和 Charles Antony Richard
    Hoare. 1972. *结构化编程*。学术出版社有限公司。
- en: Desai et al. [2016] Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain,
    Amey Karkare, Mark Marron, and Subhajit Roy. 2016. Program synthesis using natural
    language. In *Proceedings of the 38th International Conference on Software Engineering*.
    345–356.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Desai et al. [2016] Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain,
    Amey Karkare, Mark Marron, 和 Subhajit Roy. 2016. 使用自然语言进行程序合成。见于*第38届国际软件工程大会论文集*。345–356。
- en: Ding et al. [2023] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. 2023.
    Task and motion planning with large language models for object rearrangement.
    In *2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.
    IEEE, 2086–2092.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. [2023] Yan Ding, Xiaohan Zhang, Chris Paxton, 和 Shiqi Zhang. 2023.
    使用大型语言模型进行任务和动作规划以实现物体重排。见于*2023 IEEE/RSJ 国际智能机器人与系统会议 (IROS)*。IEEE，2086–2092。
- en: 'Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. *arXiv
    preprint arXiv:2303.03378* (2023).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu
    等。2023. Palm-e：一种具身的多模态语言模型。*arXiv 预印本 arXiv:2303.03378* (2023)。
- en: 'Ernst [2017] Michael D Ernst. 2017. Natural language is a programming language:
    Applying natural language processing to software development. In *2nd Summit on
    Advances in Programming Languages (SNAPL 2017)*. Schloss-Dagstuhl-Leibniz Zentrum
    für Informatik.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ernst [2017] Michael D Ernst. 2017. 自然语言是一种编程语言：将自然语言处理应用于软件开发。见于*第二届编程语言进展峰会
    (SNAPL 2017)*。Schloss-Dagstuhl-Leibniz 信息学中心。
- en: 'Ge et al. [2023a] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, and Yongfeng Zhang. 2023a. OpenAGI: When LLM Meets Domain
    Experts. *In Advances in Neural Information Processing Systems (NeurIPS)* (2023).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2023a] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan,
    Shuyuan Xu, Zelong Li, 和 Yongfeng Zhang. 2023a. OpenAGI：当大型语言模型遇上领域专家。*在神经信息处理系统进展
    (NeurIPS)* (2023)。
- en: 'Ge et al. [2023b] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan,
    and Yongfeng Zhang. 2023b. LLM as OS, Agents as Apps: Envisioning AIOS, Agents
    and the AIOS-Agent Ecosystem. *arXiv e-prints* (2023), arXiv–2312.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2023b] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan,
    和 Yongfeng Zhang. 2023b. 大型语言模型作为操作系统，代理作为应用：展望 AIOS、代理和 AIOS-代理生态系统。*arXiv 电子印刷本*
    (2023)，arXiv–2312。
- en: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning
    with world model. *arXiv preprint arXiv:2305.14992* (2023).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, 和 Zhiting Hu. 2023. 用语言模型进行推理即用世界模型进行规划。*arXiv 预印本 arXiv:2305.14992*
    (2023)。
- en: 'Heidorn [1976] George E Heidorn. 1976. Automatic programming through natural
    language dialogue: A survey. *IBM Journal of research and development* 20, 4 (1976),
    302–313.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidorn [1976] George E Heidorn. 1976. 通过自然语言对话进行自动编程：综述。*IBM 研究与开发期刊* 20, 4
    (1976), 302–313。
- en: 'Hennessy and Patterson [2011] John L Hennessy and David A Patterson. 2011.
    *Computer architecture: a quantitative approach*. Elsevier.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hennessy and Patterson [2011] John L Hennessy 和 David A Patterson. 2011. *计算机架构：一种定量方法*。Elsevier。
- en: 'Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hessel et al. [2021] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    和 Yejin Choi. 2021. CLIPScore：一种无参考图像描述评估指标。
- en: 'Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    2022. Inner monologue: Embodied reasoning through planning with language models.
    *arXiv preprint arXiv:2207.05608* (2022).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 等.
    2022. 内部独白：通过规划与语言模型进行体现性推理。*arXiv 预印本 arXiv:2207.05608* (2022)。
- en: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. *arXiv preprint
    arXiv:2401.04088* (2024).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, 等. 2024. 专家混合模型。*arXiv 预印本 arXiv:2401.04088*
    (2024)。
- en: 'Jojic et al. [2023] Ana Jojic, Zhen Wang, and Nebojsa Jojic. 2023. Gpt is becoming
    a turing machine: Here are some ways to program it. *arXiv preprint arXiv:2303.14310*
    (2023).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jojic et al. [2023] Ana Jojic, Zhen Wang, 和 Nebojsa Jojic. 2023. GPT 正在变成图灵机：以下是一些编程方法。*arXiv
    预印本 arXiv:2303.14310* (2023)。
- en: 'Josifoski et al. [2023] Martin Josifoski, Lars Klein, Maxime Peyrard, Yifei
    Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, and
    Robert West. 2023. Flows: Building blocks of reasoning and collaborating ai. *arXiv
    preprint arXiv:2308.01285* (2023).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Josifoski et al. [2023] Martin Josifoski, Lars Klein, Maxime Peyrard, Yifei
    Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, 和
    Robert West. 2023. Flows：推理和协作 AI 的构建块。*arXiv 预印本 arXiv:2308.01285* (2023)。
- en: 'Khattab et al. [2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-Search-Predict:
    Composing Retrieval and Language Models for Knowledge-Intensive NLP. *arXiv preprint
    arXiv:2212.14024* (2022).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khattab et al. [2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, 和 Matei Zaharia. 2022. Demonstrate-Search-Predict:
    组合检索和语言模型以应对知识密集型自然语言处理任务。*arXiv 预印本 arXiv:2212.14024* (2022)。'
- en: 'Khattab et al. [2023] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan
    Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T.
    Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023.
    DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.
    *arXiv preprint arXiv:2310.03714* (2023).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khattab et al. [2023] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan
    Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas
    T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, 和 Christopher Potts. 2023.
    DSPy：将声明性语言模型调用编译为自我改进的管道。*arXiv 预印本 arXiv:2310.03714* (2023)。
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems* 35 (2022), 22199–22213.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, 和 Yusuke Iwasawa. 2022. 大型语言模型是零-shot 推理器。*神经信息处理系统进展* 35 (2022), 22199–22213。
- en: Lewis et al. [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems* 33 (2020), 9459–9474.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. [2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, 等. 2020. 检索增强生成用于知识密集型自然语言处理任务。*神经信息处理系统进展* 33 (2020), 9459–9474。
- en: 'Li and Hovy [2015] Jiwei Li and Eduard Hovy. 2015. The NLP engine: A universal
    turing machine for nlp. *arXiv preprint arXiv:1503.00168* (2015).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Hovy [2015] Jiwei Li 和 Eduard Hovy. 2015. NLP 引擎：自然语言处理的通用图灵机。*arXiv
    预印本 arXiv:1503.00168* (2015)。
- en: 'Li et al. [2024] Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and Yongfeng Zhang.
    2024. Formal-LLM: Integrating Formal Language and Natural Language for Controllable
    LLM-based Agents. *arXiv:2402.00798* (2024).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2024] Zelong Li, Wenyue Hua, Hao Wang, He Zhu 和 Yongfeng Zhang。2024年。Formal-LLM:
    将形式语言与自然语言集成以实现可控的 LLM 基于代理。*arXiv:2402.00798* (2024)。'
- en: 'Liang et al. [2023] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia,
    Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. 2023. Taskmatrix. ai:
    Completing tasks by connecting foundation models with millions of apis. *arXiv
    preprint arXiv:2303.16434* (2023).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等人 [2023] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu
    Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao 等。2023年。Taskmatrix.ai: 通过连接基础模型和数百万个
    API 完成任务。*arXiv 预印本 arXiv:2303.16434* (2023)。'
- en: 'Liu et al. [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. 2023. Llm+ p: Empowering large language models
    with optimal planning proficiency. *arXiv preprint arXiv:2304.11477* (2023).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas 和 Peter Stone。2023年。Llm+ p: 赋能大型语言模型以实现最佳规划能力。*arXiv 预印本 arXiv:2304.11477*
    (2023)。'
- en: Lyu et al. [2023] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-of-thought
    reasoning. *arXiv preprint arXiv:2301.13379* (2023).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu 等人 [2023] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric
    Wong, Marianna Apidianaki 和 Chris Callison-Burch。2023年。忠实的思维链推理。*arXiv 预印本 arXiv:2301.13379*
    (2023)。
- en: 'Madaan et al. [2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in
    Neural Information Processing Systems* 36 (2024).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan 等人 [2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang
    等。2024年。Self-refine: 通过自我反馈进行迭代精炼。*神经信息处理系统进展* 36 (2024)。'
- en: 'Mei et al. [2024] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge,
    and Yongfeng Zhang. 2024. AIOS: LLM Agent Operating System. *arXiv* (2024).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mei 等人 [2024] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge 和 Yongfeng
    Zhang。2024年。AIOS: LLM 代理操作系统。*arXiv* (2024)。'
- en: 'Mihalcea et al. [2006] Rada Mihalcea, Hugo Liu, and Henry Lieberman. 2006.
    NLP (natural language processing) for NLP (natural language programming). In *Computational
    Linguistics and Intelligent Text Processing: 7th International Conference, CICLing
    2006, Mexico City, Mexico, February 19-25, 2006\. Proceedings 7*. Springer, 319–330.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mihalcea 等人 [2006] Rada Mihalcea, Hugo Liu 和 Henry Lieberman。2006年。NLP (自然语言处理)
    用于 NLP (自然语言编程)。收录于*计算语言学与智能文本处理: 第七届国际会议，CICLing 2006，墨西哥城，墨西哥，2006年2月19-25日。论文集
    7*。Springer，319–330。'
- en: 'Nijkamp et al. [2022] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp 等人 [2022] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang,
    Yingbo Zhou, Silvio Savarese 和 Caiming Xiong。2022年。Codegen: 一个用于多轮程序合成的开源大型语言模型。*arXiv
    预印本 arXiv:2203.13474* (2022)。'
- en: OpenAI [2023] Josh et al OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] Josh 等人 OpenAI。2023年。GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems* 35 (2022), 27730–27744.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等。2022年。用人类反馈训练语言模型以遵循指令。*神经信息处理系统进展*
    35 (2022), 27730–27744。
- en: 'Paul et al. [2023] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback
    on intermediate representations. *arXiv preprint arXiv:2304.01904* (2023).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paul 等人 [2023] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West 和 Boi Faltings。2023年。Refiner: 对中间表示进行推理反馈。*arXiv
    预印本 arXiv:2304.01904* (2023)。'
- en: 'Poesia et al. [2022] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari,
    Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable
    code generation from pre-trained language models. *arXiv preprint arXiv:2201.11227*
    (2022).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Poesia 等人 [2022] Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo
    Soares, Christopher Meek 和 Sumit Gulwani。2022年。Synchromesh: 从预训练语言模型生成可靠代码。*arXiv
    预印本 arXiv:2201.11227* (2022)。'
- en: Prather [1997] Ronald E Prather. 1997. Regular expressions for program computations.
    *The American mathematical monthly* 104, 2 (1997), 120–130.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prather [1997] Ronald E Prather. 1997. 程序计算的正则表达式. *美国数学月刊* 104, 2 (1997), 120–130.
- en: 'Qin et al. [2023] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789* (2023).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等人 [2023] Yujia Qin、Shihao Liang、Yining Ye、Kunlun Zhu、Lan Yan、Yaxi Lu、Yankai
    Lin、Xin Cong、Xiangru Tang、Bill Qian 等人. 2023. Toolllm: 帮助大型语言模型掌握 16000+ 个现实世界的
    API. *arXiv 预印本 arXiv:2307.16789* (2023).'
- en: 'Ross et al. [2023] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael
    Muller, and Justin D Weisz. 2023. The programmer’s assistant: Conversational interaction
    with a large language model for software development. In *Proceedings of the 28th
    International Conference on Intelligent User Interfaces*. 491–514.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ross 等人 [2023] Steven I Ross、Fernando Martinez、Stephanie Houde、Michael Muller
    和 Justin D Weisz. 2023. 程序员助手: 与大型语言模型进行对话交互以进行软件开发. 在 *第28届国际智能用户界面会议论文集*. 491–514.'
- en: 'Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*
    (2023).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人 [2023] Noah Shinn、Beck Labash 和 Ashwin Gopinath. 2023. Reflexion:
    具有动态记忆和自我反思的自主代理. *arXiv 预印本 arXiv:2303.11366* (2023).'
- en: 'Singh et al. [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.
    Progprompt: Generating situated robot task plans using large language models.
    In *2023 IEEE International Conference on Robotics and Automation (ICRA)*. IEEE,
    11523–11530.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 等人 [2023] Ishika Singh、Valts Blukis、Arsalan Mousavian、Ankit Goyal、Danfei
    Xu、Jonathan Tremblay、Dieter Fox、Jesse Thomason 和 Animesh Garg. 2023. Progprompt:
    利用大型语言模型生成定位机器人任务计划. 在 *2023 IEEE 国际机器人与自动化大会 (ICRA)*. IEEE, 11523–11530.'
- en: Vadas and Curran [2005] David Vadas and James R Curran. 2005. Programming with
    unrestricted natural language. In *Proceedings of the Australasian Language Technology
    Workshop 2005*. 191–199.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vadas 和 Curran [2005] David Vadas 和 James R Curran. 2005. 使用无限制自然语言编程. 在 *2005年澳大利亚语言技术研讨会论文集*.
    191–199.
- en: Wang et al. [2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*
    (2022).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022] Xuezhi Wang、Jason Wei、Dale Schuurmans、Quoc Le、Ed Chi、Sharan Narang、Aakanksha
    Chowdhery 和 Denny Zhou. 2022. 自一致性提高语言模型中的思维链推理. *arXiv 预印本 arXiv:2203.11171*
    (2022).
- en: Wang et al. [2023] Yubo Wang, Xueguang Ma, and Wenhu Chen. 2023. Augmenting
    black-box llms with medical textbooks for clinical question answering. *arXiv
    preprint arXiv:2309.02233* (2023).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023] Yubo Wang、Xueguang Ma 和 Wenhu Chen. 2023. 通过医学教科书增强黑箱 LLM 用于临床问题回答.
    *arXiv 预印本 arXiv:2309.02233* (2023).
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems* 35 (2022), 24824–24837.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2022] Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed
    Chi、Quoc V Le、Denny Zhou 等人. 2022. 思维链提示引发大型语言模型中的推理. *神经信息处理系统进展* 35 (2022),
    24824–24837.
- en: 'Wu et al. [2020] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao
    Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter
    Vajda. 2020. Visual Transformers: Token-based Image Representation and Processing
    for Computer Vision. arXiv:2006.03677 [cs.CV]'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 [2020] Bichen Wu、Chenfeng Xu、Xiaoliang Dai、Alvin Wan、Peizhao Zhang、Zhicheng
    Yan、Masayoshi Tomizuka、Joseph Gonzalez、Kurt Keutzer 和 Peter Vajda. 2020. 视觉变换器:
    基于标记的图像表示和计算机视觉处理. arXiv:2006.03677 [cs.CV]'
- en: 'Wu et al. [2024] Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, and Qingyun
    Wu. 2024. StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows.
    *arXiv preprint arXiv:2403.11322* (2024).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 [2024] Yiran Wu、Tianwei Yue、Shaokun Zhang、Chi Wang 和 Qingyun Wu. 2024.
    StateFlow: 通过状态驱动的工作流增强 LLM 任务解决. *arXiv 预印本 arXiv:2403.11322* (2024).'
- en: 'Yao et al. [2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*
    36 (2024).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2024] Shunyu Yao、Dian Yu、Jeffrey Zhao、Izhak Shafran、Tom Griffiths、Yuan
    Cao 和 Karthik Narasimhan. 2024. 思维树: 使用大型语言模型进行深思熟虑的问题解决. *神经信息处理系统进展* 36 (2024).'
- en: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629* (2022).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2022. React: 在语言模型中协同推理与行动。*arXiv 预印本 arXiv:2210.03629*
    (2022)。'
- en: 'Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2023. ReAct: 在语言模型中协同推理与行动。发表于*国际学习表征会议（ICLR）*。'
- en: 'Zhang et al. [2020] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2020] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    和 Yoav Artzi. 2020. BERTScore: 使用BERT评估文本生成。'
