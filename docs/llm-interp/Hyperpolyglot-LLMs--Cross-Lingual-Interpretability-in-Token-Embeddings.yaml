- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 17:35:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 17:35:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超多语言模型：跨语言嵌入的可解释性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.18034](https://ar5iv.labs.arxiv.org/html/2311.18034)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.18034](https://ar5iv.labs.arxiv.org/html/2311.18034)
- en: Andrea W Wen-Yi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Andrea W Wen-Yi
- en: Cornell University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学
- en: andreawwenyi@infosci.cornell.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: andreawwenyi@infosci.cornell.edu
- en: \AndDavid Mimno
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \AndDavid Mimno
- en: Cornell University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学
- en: mimno@cornell.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: mimno@cornell.edu
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Cross-lingual transfer learning is an important property of multilingual large
    language models (LLMs). But how do LLMs represent relationships between languages?
    Every language model has an input layer that maps tokens to vectors. This ubiquitous
    layer of language models is often overlooked. We find that similarities between
    these input embeddings are highly interpretable and that the geometry of these
    embeddings differs between model families. In one case (XLM-RoBERTa), embeddings
    encode language: tokens in different writing systems can be linearly separated
    with an average of 99.2% accuracy. Another family (mT5) represents cross-lingual
    semantic similarity: the 50 nearest neighbors for any token represent an average
    of 7.61 writing systems, and are frequently translations. This result is surprising
    given that there is no explicit parallel cross-lingual training corpora and no
    explicit incentive for translations in pre-training objectives. Our research opens
    the door for investigations in 1) The effect of pre-training and model architectures
    on representations of languages and 2) The applications of cross-lingual representations
    embedded in language models.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 跨语言迁移学习是多语言大型语言模型（LLMs）的一个重要特性。但LLMs是如何表示语言之间的关系的呢？每个语言模型都有一个输入层，将令牌映射到向量。这一层在语言模型中非常普遍，但常被忽视。我们发现这些输入嵌入之间的相似性具有高度的可解释性，而且这些嵌入的几何形状在模型家族之间有所不同。在一种情况（XLM-RoBERTa）中，嵌入编码了语言：不同书写系统中的令牌可以以99.2%的平均准确率线性分离。另一家族（mT5）表示跨语言语义相似性：任何令牌的50个最近邻代表了平均7.61种书写系统，并且经常是翻译。这一结果令人惊讶，因为没有显式的平行跨语言训练语料库，也没有在预训练目标中对翻译的显式激励。我们的研究为1）预训练和模型架构对语言表示的影响以及2）嵌入在语言模型中的跨语言表示的应用开辟了研究的新领域。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Multilingual large language models (LLMs) have the potential to support transfer
    learning between languages with little to no additional training data Lauscher
    et al. ([2020](#bib.bib14)); Wu and Dredze ([2019](#bib.bib24)); Conneau et al.
    ([2020](#bib.bib7)); Winata et al. ([2021](#bib.bib23)). But we have limited theory
    for how LLMs represent meanings across languages. This work describes a mechanism
    for cross-lingual transfer learning by measuring the properties of the input embedding
    vectors. While most of the interest in LLMs rightly focuses on their ability to
    produce contextualized output, in this study we focus specifically on the lowest
    network layer: the initial token embedding layer. This layer often comprises a
    large percentage of the total parameters in a model. It also serves as the connection
    between human-readable strings and the latent vector representations that initialize
    the remaining layers. As such, the initial token embedding layer is both geometrically
    expressive and readily interpretable. We explore the initial token embeddings
    of two highly multilingual model families, XLM-RoBERTa (XLM-R) Conneau et al.
    ([2020](#bib.bib7)) and mT5 Xue et al. ([2021](#bib.bib26)). We find that mT5
    discovers a universal, cross-lingual semantic space that assigns words (or word
    fragments) with similar meanings to nearby vector positions.¹¹1Code is available
    at: [https://github.com/andreawwenyi/hyperpolyglot](https://github.com/andreawwenyi/hyperpolyglot)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言大型语言模型（LLMs）有潜力在几乎不需要额外训练数据的情况下支持语言间的迁移学习 Lauscher 等人 ([2020](#bib.bib14))；Wu
    和 Dredze ([2019](#bib.bib24))；Conneau 等人 ([2020](#bib.bib7))；Winata 等人 ([2021](#bib.bib23))。但我们对
    LLMs 如何跨语言表示意义的理论仍然有限。本研究描述了一种通过测量输入嵌入向量的属性来实现跨语言迁移学习的机制。虽然大多数对 LLMs 的关注正当集中在其生成上下文化输出的能力上，但在本研究中，我们专注于最低的网络层：初始令牌嵌入层。这个层通常占模型总参数的很大一部分。它还充当了人类可读字符串与初始化其余层的潜在向量表示之间的连接。因此，初始令牌嵌入层既具有几何表现力，又容易解释。我们探讨了两个高度多语言模型家族的初始令牌嵌入，XLM-RoBERTa（XLM-R）Conneau
    等人 ([2020](#bib.bib7)) 和 mT5 Xue 等人 ([2021](#bib.bib26))。我们发现 mT5 发现了一个普遍的跨语言语义空间，将具有相似意义的词（或词片段）分配到相邻的向量位置。¹¹1
    代码可在：[https://github.com/andreawwenyi/hyperpolyglot](https://github.com/andreawwenyi/hyperpolyglot)
- en: '![Refer to caption](img/73a1c670dd21c83d1e4bb4b5ffefe4fb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/73a1c670dd21c83d1e4bb4b5ffefe4fb.png)'
- en: 'Figure 1: 2D Projections of input token embeddings. In XLM-RoBERTa-XL (left),
    writing systems are widely spaced; while for mT5-XL (right), vectors are more
    mixed.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：输入令牌嵌入的 2D 投影。在 XLM-RoBERTa-XL（左）中，书写系统的间距较大；而在 mT5-XL（右）中，向量更加混合。
- en: Previous work has shown that algorithms exist to train multilingual word embeddings
    without explicit parallel text or glossaries Ammar et al. ([2016](#bib.bib2));
    Chen and Cardie ([2018](#bib.bib5)). What is novel about the current work is the
    discovery that certain highly multilingual LLMs contain such an embedding as an
    emergent property without any explicit instruction to do so.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，存在可以在没有明确平行文本或词汇表的情况下训练多语言词嵌入的算法 Ammar 等人 ([2016](#bib.bib2))；Chen 和
    Cardie ([2018](#bib.bib5))。当前工作的创新之处在于发现某些高度多语言的 LLMs 具有这种嵌入，作为一种突现属性，而无需任何明确的指令。
- en: Explaining the factors that cause LLMs to find cross-lingual semantic embeddings
    would require pre-training experiments that are beyond the scope of this short
    paper. Describing these behaviors is, however, a necessary first step. At the
    same time, there is increasing attention on producing language technology for
    lower-resource languages Kumar and Albuquerque ([2021](#bib.bib12)), where the
    data-hungry methods that have been successful for high-resource languages may
    not be applicable. Creating predictive theories that explain the relationship
    between properties of pre-training data and representations learned by LLMs could
    lead us to build collections, architectures, and algorithms that most efficiently
    improve performance. This will be an extremely valuable step to enhance language
    technology for low-resource languages.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 解释导致大型语言模型（LLMs）发现跨语言语义嵌入的因素需要进行超出本短文范围的预训练实验。然而，描述这些行为是必要的第一步。同时，越来越多的关注放在为低资源语言生产语言技术上
    Kumar 和 Albuquerque ([2021](#bib.bib12))，在这些语言中，成功应用于高资源语言的数据饥饿方法可能不适用。创建预测理论以解释预训练数据的属性与
    LLMs 学到的表示之间的关系，可能会帮助我们构建最有效提升性能的集合、架构和算法。这将是提升低资源语言语言技术的极其宝贵的一步。
- en: 2 Related work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Previous work has found interpretable patterns in feed-forward layers Geva et al.
    ([2021](#bib.bib10), [2022](#bib.bib9)), self-attention Mrini et al. ([2020](#bib.bib19));
    Clark et al. ([2019](#bib.bib6)); Serrano and Smith ([2019](#bib.bib22)), and
    input embeddings from the perspective of adversarial perturbation Sato et al.
    ([2018](#bib.bib21)). In this work, we directly interpret the relative positions
    of the token embeddings through the lens of the vocabularies.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究已经在前馈层 Geva 等人 ([2021](#bib.bib10), [2022](#bib.bib9))、自注意力 Mrini 等人 ([2020](#bib.bib19));
    Clark 等人 ([2019](#bib.bib6)); Serrano 和 Smith ([2019](#bib.bib22)) 以及从对抗扰动的角度来解释输入嵌入
    Sato 等人 ([2018](#bib.bib21)) 中发现了可解释的模式。在这项工作中，我们通过词汇的视角直接解释了令牌嵌入的相对位置。
- en: Prior to the explosion of contextualized language models, there was substantial
    interest in cross-lingual word embeddings (CLWE) Ruder et al. ([2019](#bib.bib20));
    Mikolov et al. ([2013](#bib.bib18)); Lazaridou et al. ([2015](#bib.bib15)). The
    goal is often to map monolingual word embeddings from different languages to the
    same shared space, so that words of the same meaning are close to each other.
    CLWE approaches involve some levels of supervised alignment Faruqui and Dyer ([2014](#bib.bib8));
    Zou et al. ([2013](#bib.bib29)), seed dictionaries Artetxe et al. ([2017](#bib.bib3));
    Gouws and Søgaard ([2015](#bib.bib11)) or adversarial training Lample et al. ([2018](#bib.bib13));
    Artetxe et al. ([2018](#bib.bib4)); Zhang et al. ([2017](#bib.bib28)); Miceli Barone
    ([2016](#bib.bib17)). Contexualized embeddings from language models have also
    been used in combination with static word embeddings to improve alignments of
    cross-lingual word vectors Aldarmaki and Diab ([2019](#bib.bib1)); Zhang et al.
    ([2021](#bib.bib27)). Contrary to our findings for the token embeddings of LLMs,
    it was not clear that aligning word vectors is possible without some level of
    supervision, or to more than two languages at a time. Previous results also showed
    that CLWE approaches are sensitive to language pairs, where languages with large
    semantic or structural differences usually failed, such as English-Japanese and
    Spanish-Chinese Xu et al. ([2022](#bib.bib25)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文化语言模型爆发之前，跨语言词嵌入（CLWE） Ruder 等人 ([2019](#bib.bib20)); Mikolov 等人 ([2013](#bib.bib18));
    Lazaridou 等人 ([2015](#bib.bib15)) 引起了广泛关注。目标通常是将不同语言的单语词嵌入映射到相同的共享空间，使得具有相同意义的词彼此接近。CLWE
    方法涉及一些监督对齐的级别 Faruqui 和 Dyer ([2014](#bib.bib8)); Zou 等人 ([2013](#bib.bib29))，种子词典
    Artetxe 等人 ([2017](#bib.bib3)); Gouws 和 Søgaard ([2015](#bib.bib11)) 或对抗训练 Lample
    等人 ([2018](#bib.bib13)); Artetxe 等人 ([2018](#bib.bib4)); Zhang 等人 ([2017](#bib.bib28));
    Miceli Barone ([2016](#bib.bib17))。上下文化的词嵌入也与静态词嵌入结合使用，以改善跨语言词向量的对齐 Aldarmaki
    和 Diab ([2019](#bib.bib1)); Zhang 等人 ([2021](#bib.bib27))。与我们对 LLMs 令牌嵌入的发现相反，尚不清楚在没有某种程度的监督的情况下对齐词向量是否可行，或者是否可以对齐多于两种语言。之前的结果还表明，CLWE
    方法对语言对敏感，其中具有较大语义或结构差异的语言通常会失败，例如英语-日语和西班牙语-中文 Xu 等人 ([2022](#bib.bib25))。
- en: 3 Multilingual vocabularies
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 多语言词汇
- en: Sub-word tokenization and writing systems
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子词分词和书写系统
- en: Initial embedding layer maps tokens to vectors. Most contemporary language models
    use sub-word tokenization schemes such as byte-pair encoding. These methods balance
    representational richness (providing distinct vectors for as many inputs as possible)
    with efficiency in limiting overall vocabulary size. While some methods produce
    tokens consisting of byte sequences that are not valid UTF-8 characters, in practice
    almost all tokens are valid, displayable sequences of Unicode characters, and
    a large number are recognizable words ²²2We find 39.93% of mT5 and 47.78% of XLM-R
    vocabularies in multilingual dictionaries provided by Lample et al. ([2018](#bib.bib13))..
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 初始嵌入层将令牌映射到向量。大多数现代语言模型使用诸如字节对编码的子词分词方案。这些方法在提供尽可能多的输入的独特向量的表示丰富性与限制总体词汇大小的效率之间取得平衡。虽然某些方法产生的令牌由不是有效
    UTF-8 字符的字节序列组成，但在实践中，几乎所有的令牌都是有效的、可显示的 Unicode 字符序列，而且大量是可识别的单词²²2我们发现 mT5 的
    39.93% 和 XLM-R 的 47.78% 词汇在 Lample 等人 ([2018](#bib.bib13)) 提供的多语言词典中。
- en: 'As it is difficult to assign tokens to specific languages (e.g. war can be
    an English noun or a German verb), we use Unicode metadata to define categories
    of tokens. For example, a is LATIN SMALL LETTER A. Most characters are letters
    (L), but vocabularies also include punctuation (P), numbers (N), and symbols (S,
    which include emoji). Letters are also marked by writing system, such as LATIN,
    CYRILLIC, or BENGALI. We define the category of a character as either its writing
    system (for letters) or its Unicode class for non-letters. A token can mix characters
    of different categories, but they typically have a most frequent category: the
    string doesn’t contains letters and punctuation, but is primarily LATIN letters.
    We define the category of a token based on its majority character category. As
    a result, doesn’t is classified as LATIN.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于很难将标记分配给特定语言（例如，war可以是英语名词或德语动词），我们使用Unicode元数据来定义标记的类别。例如，a 是 LATIN SMALL
    LETTER A。大多数字符是字母（L），但词汇表还包括标点符号（P）、数字（N）和符号（S，包括表情符号）。字母还按书写系统标记，如 LATIN、CYRILLIC
    或 BENGALI。我们根据书写系统（对于字母）或Unicode类别（对于非字母）来定义字符的类别。一个标记可以混合不同类别的字符，但它们通常有一个最常见的类别：该字符串不包含字母和标点符号，但主要是
    LATIN 字母。我们根据标记的主要字符类别来定义标记的类别。因此，doesn’t 被归类为 LATIN。
- en: Overlap between model vocabularies
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型词汇的重叠
- en: While model families have distinct sets of vocabularies, they are often comparable.
    Current vocabulary sizes of monolingual models are often $\approx$ 32,000 (BERT,
    T5, LLaMa), 50,000 (GPT2, GPT-J, Pythia), or 65,000 (falcon). Highly multilingual
    models have larger sizes around 250,000 (mT5, umT5, XLM-R, BLOOM).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型家族有不同的词汇集，但它们通常是可以比较的。目前单语言模型的词汇量通常约为 $\approx$ 32,000（BERT，T5，LLaMa），50,000（GPT2，GPT-J，Pythia）或65,000（falcon）。高度多语言模型的词汇量较大，约为250,000（mT5，umT5，XLM-R，BLOOM）。
- en: To make strings comparable across tokenzation methods, we replace the space
    character in BPE tokenizations with Unicode LOWER ONE EIGHTH BLOCK, which looks
    like a long underscore, as it is more readable and compatible with T5-style SentencePiece
    vocabularies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使不同分词方法下的字符串可比，我们将BPE分词中的空格字符替换为Unicode LOWER ONE EIGHTH BLOCK，这看起来像一个长下划线，因为它更易读且与T5风格的SentencePiece词汇表兼容。
- en: 'Figure [2](#S3.F2 "Figure 2 ‣ Overlap between model vocabularies ‣ 3 Multilingual
    vocabularies ‣ Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings")
    shows the total overlap in vocabularies between six LLMs. We select mT5 and XLM-R
    as the most multilingual comparison due to their large vocabulary size, significant
    overlap, and large overlap in non-LATIN tokens. Of the models with a large vocabulary,
    BLOOM focuses more on African and South Asian languages and thus has a much smaller
    token overlap in CYRILLIC, HANGUL, HIRAGANA, and KATAKANA.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S3.F2 "图2 ‣ 模型词汇的重叠 ‣ 3 多语言词汇 ‣ 超多语言LLM：标记嵌入中的跨语言可解释性") 显示了六个LLM之间词汇的总重叠。由于它们的大词汇量、显著的重叠以及大量的非LATIN标记重叠，我们选择了mT5和XLM-R作为最具多语言性的比较。对于拥有大量词汇的模型，BLOOM更注重非洲和南亚语言，因此在CYRILLIC、HANGUL、HIRAGANA和KATAKANA中的标记重叠较小。
- en: '![Refer to caption](img/581551f7ddd92d78b36f52b41f7a297e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/581551f7ddd92d78b36f52b41f7a297e.png)'
- en: 'Figure 2: There is substantial overlap in vocabulary between models (mean 60.4%),
    but much of the overlap is Unicode LATIN. mT5 and XLM-R have the largest non-Latin
    intersection (59.4%). While large, BLOOM vocabularies do not include Slavic languages,
    Japanese, or Korean.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：模型之间的词汇有显著重叠（平均60.4%），但大部分重叠是Unicode LATIN。mT5和XLM-R有最大的非拉丁交集（59.4%）。虽然很大，BLOOM的词汇表不包括斯拉夫语言、日语或韩语。
- en: '![Refer to caption](img/5f79e23c3b3ebb11ae403d0a515b49da.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5f79e23c3b3ebb11ae403d0a515b49da.png)'
- en: 'Figure 3: Token embeddings can predict Unicode categories. Both models have
    high accuracy, but XLM-R-XL is significantly higher than mT5-XL across the majority
    of categories.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：标记嵌入可以预测Unicode类别。两个模型的准确度都很高，但XLM-R-XL在大多数类别中的准确度显著高于mT5-XL。
- en: 4 Results
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: Models encode languages differently.
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型对语言的编码方式不同。
- en: 'We make comparisons between the embeddings of XL-scale models from mT5 and
    XLM-R families. In addition to -XL models being potentially more expressive, XLM-R-XL
    and mT5-XL are also more comparable in parameter size (3.5B and 3.7B, respectively).
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Hyperpolyglot LLMs: Cross-Lingual
    Interpretability in Token Embeddings") shows UMAP McInnes et al. ([2018](#bib.bib16))
    projections of the embeddings from XLM-R-XL and mT5-XL for each token in the shared
    vocabulary, colored by Unicode category. We find that XLM-R’s representations
    encode languages — tokens of different categories form isolated clusters. Clusters
    of Unicode categories are also noticeable in mT5 but they are more overlapping.
    The exception is Symbols (S) and Numbers (N): they are scattered in XLM-R-XL,
    but clustered in mT5-XL.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了来自mT5和XLM-R家族的XL规模模型的嵌入。除了-XL模型可能更具表达力之外，XLM-R-XL和mT5-XL的参数规模也更为接近（分别为3.5B和3.7B）。图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 超多语言LLM：跨语言在标记嵌入中的可解释性")展示了XLM-R-XL和mT5-XL在共享词汇中的每个标记的UMAP McInnes等人（[2018](#bib.bib16)）投影，按Unicode类别着色。我们发现，XLM-R的表示能够编码语言——不同类别的标记形成了孤立的簇。在mT5中也能看到Unicode类别的簇，但它们更为重叠。例外的是符号（S）和数字（N）：它们在XLM-R-XL中分散，但在mT5-XL中则聚集在一起。
- en: 'To further show how embeddings encode languages, we use logistic regression
    to predict Unicode category from embeddings. For each category, we construct a
    balanced dataset and run 10-fold cross-validation. Embeddings from XLM-R-XL and
    mT5-XL both encode a high level of language information. Tokens with different
    Unicode category could be linearly separated with an average of 99.24% and 93.32%
    accuracy, respectively. Figure [3](#S3.F3 "Figure 3 ‣ Overlap between model vocabularies
    ‣ 3 Multilingual vocabularies ‣ Hyperpolyglot LLMs: Cross-Lingual Interpretability
    in Token Embeddings") shows accuracy for selected categories. XLM-R-XL has significantly
    higher (near perfect) accuracy across the majority of categories than mT5-XL.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步展示嵌入如何编码语言，我们使用逻辑回归从嵌入中预测Unicode类别。对于每个类别，我们构建了一个平衡的数据集并进行10折交叉验证。XLM-R-XL和mT5-XL的嵌入都编码了较高水平的语言信息。具有不同Unicode类别的标记可以线性分离，准确率分别为99.24%和93.32%。图[3](#S3.F3
    "图 3 ‣ 模型词汇之间的重叠 ‣ 3 多语言词汇 ‣ 超多语言LLM：跨语言在标记嵌入中的可解释性")展示了选定类别的准确率。XLM-R-XL在大多数类别中的准确率显著高于mT5-XL（接近完美）。
- en: '![Refer to caption](img/e49738d3786483d4fb0bc35023996238.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e49738d3786483d4fb0bc35023996238.png)'
- en: 'Figure 4: The top 20 neighbors from mT5-XL are often direct translations; whereas
    those from XLM-R-XL are often semantically or linguistically similar words in
    the same language. The last row is an example of KATAKANA, the Japanese writing
    system used for words of foreign origin. ISO 639-1 language abbreviations are
    included in the Appendix. Some words exist in several languages, we record only
    one. Closely related languages are often neighbors, e.g. Swedish (sv) and Danish
    (da). Malay (ms) words are translated from root terms.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：mT5-XL的前20个邻居通常是直接翻译的词，而XLM-R-XL的邻居则通常是相同语言中的语义或语言学上相似的词。最后一行是KATAKANA的一个例子，这是用于外来词的日语书写系统。ISO
    639-1语言缩写包含在附录中。一些词存在于几种语言中，我们仅记录一种。密切相关的语言通常是邻居，例如瑞典语（sv）和丹麦语（da）。马来语（ms）词汇是从根词翻译而来的。
- en: '![Refer to caption](img/22783a5f2d60f8419b2e7dd940668b48.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22783a5f2d60f8419b2e7dd940668b48.png)'
- en: 'Figure 5: The 50 nearest neighbors for mT5-XL tokens have an average of 7.61
    Unicode categories, and XLM-R-XL has 1.64\. The figure is made by randomly selected
    1% of tokens.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：mT5-XL标记的50个最近邻的Unicode类别平均为7.61，而XLM-R-XL为1.64。该图基于随机选择的1%的标记制作。
- en: Embedding neighborhoods encode semantics across languages.
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入邻域编码了跨语言的语义。
- en: 'We next zoom in to study the neighbors of individual tokens. Neighbors from
    mT5-XL are often direct translations, whereas XLM-R-XL finds tokens in the same
    language that are semantically or linguistically similar. Figure [5](#S4.F5 "Figure
    5 ‣ Models encode languages differently. ‣ 4 Results ‣ Hyperpolyglot LLMs: Cross-Lingual
    Interpretability in Token Embeddings") shows that the 50 nearest neighbors for
    mT5-XL tokens have an average of 7.61 different Unicode categories, whereas XLM-R-XL
    has 1.64. Figure [4](#S4.F4 "Figure 4 ‣ Models encode languages differently. ‣
    4 Results ‣ Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings")
    shows examples of 20 nearest neighbors of selected tokens.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们深入研究单个令牌的邻居。mT5-XL的邻居通常是直接翻译的，而XLM-R-XL则发现了在语义或语言上相似的同语言令牌。图[5](#S4.F5
    "图5 ‣ 模型以不同方式编码语言。 ‣ 4 结果 ‣ 超多语言LLM：令牌嵌入中的跨语言可解释性")显示，mT5-XL令牌的50个最近邻具有平均7.61个不同的Unicode类别，而XLM-R-XL则为1.64。图[4](#S4.F4
    "图4 ‣ 模型以不同方式编码语言。 ‣ 4 结果 ‣ 超多语言LLM：令牌嵌入中的跨语言可解释性")显示了所选令牌的20个最近邻的示例。
- en: 'The neighboring categories of tokens in mT5 vary by the category of a token,
    as shown in Figure [6](#S4.F6 "Figure 6 ‣ Embedding neighborhoods encode semantics
    across languages. ‣ 4 Results ‣ Hyperpolyglot LLMs: Cross-Lingual Interpretability
    in Token Embeddings"). We find interesting comparisons between two Japanese writing
    systems, KATAKANA and HIRAGANA. KATAKANA, a writing system often used for words
    of foreign origin, has more diverse neighbors (most often LATIN). HIRAGANA, used
    more for native Japanese words, has more neighbors in HIRAGANA and CJK (Kanji).
    We do not find evidence to suggest that tokens appearing more often in pre-training
    corpora have more diverse neighbors (Figure [9](#A2.F9 "Figure 9 ‣ Appendix B
    Frequency of tokens does not correlate with diversity of neighbors. ‣ Hyperpolyglot
    LLMs: Cross-Lingual Interpretability in Token Embeddings") in the Appendix).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: mT5中令牌的邻接类别因令牌类别而异，如图[6](#S4.F6 "图6 ‣ 嵌入邻域跨语言编码语义。 ‣ 4 结果 ‣ 超多语言LLM：令牌嵌入中的跨语言可解释性")所示。我们发现了两个日语书写系统之间有趣的比较，即KATAKANA和HIRAGANA。KATAKANA是一个常用于外来词的书写系统，其邻居更为多样（通常是LATIN）。HIRAGANA更多用于本土日语单词，其邻居更多是HIRAGANA和CJK（汉字）。我们没有发现证据表明，在预训练语料库中出现频率更高的令牌拥有更多样的邻居（见附录中的图[9](#A2.F9
    "图9 ‣ 附录B 令牌频率与邻居多样性不相关。 ‣ 超多语言LLM：令牌嵌入中的跨语言可解释性")）。
- en: '![Refer to caption](img/fd9d3135e603b13ad2f377bd89c9eb40.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fd9d3135e603b13ad2f377bd89c9eb40.png)'
- en: 'Figure 6: In the 50 nearest neighbors of mT5-XL tokens, HIRAGANA and LATIN
    find the majority of neighbors in their respective writing systems; whereas KATAKANA
    and CYRILLIC tokens have more diverse neighbors.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在mT5-XL令牌的50个最近邻中，HIRAGANA和LATIN在各自的书写系统中找到了大多数邻居；而KATAKANA和CYRILLIC令牌的邻居则更加多样。
- en: '![Refer to caption](img/045edefd43b009a6c2b11f1d13a777bd.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/045edefd43b009a6c2b11f1d13a777bd.png)'
- en: 'Figure 7: There is a high level of overlap between the nearest neighbors of
    tokens, derived from embeddings. XLM-R models are the most similar, with up to
    60 out of 100 shared neighbors. mT5 is less consistent, and BLOOM is significantly
    different.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：由嵌入生成的令牌的最近邻之间存在高度重叠。XLM-R模型最为相似，最多有100个邻居中有60个是共享的。mT5的一致性较差，而BLOOM则显著不同。
- en: Embedding geometries are similar across parameter scales.
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入几何在参数尺度上是相似的。
- en: Finally, we consider whether initial token embeddings as a whole are similar
    across model families and parameter scales. We use two metrics to quantify similarity
    in geometric structures of embedding space.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们考虑初始令牌嵌入在模型家族和参数尺度上是否相似。我们使用两个指标来量化嵌入空间几何结构的相似性。
- en: 'The first metric measures the local similarity between vectors in two matrices.
    We first find the set of tokens shared between model families and create subset
    embedding matrices that only include those tokens. For each token in the shared
    vocabulary, we then find the 100 nearest neighbors within each matrix using cosine
    distance. Finally, we calculate the overlap between these neighbor sets from each
    pair of models. For example, on average, 60 of the 100 nearest neighbors of a
    token in XLM-R-XL will also be nearest neighbors of the same token in XLM-R-XXL.
    Figure [7](#S4.F7 "Figure 7 ‣ Embedding neighborhoods encode semantics across
    languages. ‣ 4 Results ‣ Hyperpolyglot LLMs: Cross-Lingual Interpretability in
    Token Embeddings") compares the average number of overlapping terms out of 100
    nearest neighbors for pairs of models, including BLOOM for comparison. Results
    are averaged over 45,000 shared tokens between mT5, XLM-R, and BLOOM. We find
    that XLM-R models have the largest similarity. mT5 are more varied, but still
    find 20–30 shared tokens on average. BLOOM is the most different: the 1.1B model
    shares only 5–10 tokens with other models, the 3B 15–20.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个度量测量两个矩阵中向量的局部相似度。我们首先找到模型系列之间共享的标记集，并创建仅包含这些标记的子集嵌入矩阵。对于共享词汇中的每个标记，我们然后使用余弦距离在每个矩阵中找到100个最近邻。最后，我们计算每对模型之间这些邻居集的重叠情况。例如，平均而言，XLM-R-XL中的标记的100个最近邻中有60个也将是XLM-R-XXL中相同标记的最近邻。图[7](#S4.F7
    "Figure 7 ‣ Embedding neighborhoods encode semantics across languages. ‣ 4 Results
    ‣ Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings")比较了模型对的100个最近邻中的重叠术语的平均数量，包括用于比较的BLOOM。结果是基于mT5、XLM-R和BLOOM之间45,000个共享标记的平均值。我们发现XLM-R模型具有最大的相似性。mT5模型则更具变化，但仍然平均找到20–30个共享标记。BLOOM最为不同：1.1B模型与其他模型仅共享5–10个标记，而3B模型共享15–20个。'
- en: 'The second metric uses canonical angles, a linear algebraic measure of rotational
    alignment. While it is extremely unlikely that input embedding matrices will be
    comparable at the level of individual columns or entries, the matrices of two
    models may be identical up to rotation or permutation. We can measure the degree
    to which two matrices are rotations through the method of canonical angles. Given
    matrices $A$ and $B$ both with $n$ rows, we calculate $Q_{A}R_{A}=A$ and $Q_{B}R_{B}=B$.
    Then we form the SVD $U\Sigma V^{T}=Q_{A}^{T}Q_{B}$. The singular values $\Sigma$
    are the cosines of the angles of rotation. The single largest value is the cosine
    of the smallest angle, so it forms an upper bound on our ability to rotate one
    matrix to match the other. Figure [8](#S4.F8 "Figure 8 ‣ Embedding geometries
    are similar across parameter scales. ‣ 4 Results ‣ Hyperpolyglot LLMs: Cross-Lingual
    Interpretability in Token Embeddings") shows the first singular values for pairs
    of models restricted to rows for their shared vocabularies, including a random
    “embedding” matrix with size 512. We find that XLM-R models have a high rotational
    similarity to each other (0.99 out of 1.0), while mT5 models are more differentiated
    (0.95–0.97) but still highly similar. All models are significantly more similar
    to each other than to a random matrix (0.15–0.27).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '第二个度量使用标准角度，这是一个线性代数的旋转对齐度量。尽管输入嵌入矩阵在单个列或条目的级别上不太可能是可比的，但两个模型的矩阵可能在旋转或排列下是相同的。我们可以通过标准角度的方法来测量两个矩阵的旋转程度。给定矩阵$A$和$B$都有$n$行，我们计算$Q_{A}R_{A}=A$和$Q_{B}R_{B}=B$。然后我们形成SVD
    $U\Sigma V^{T}=Q_{A}^{T}Q_{B}$。奇异值$\Sigma$是旋转角度的余弦值。单一的最大值是最小角度的余弦值，因此它为我们将一个矩阵旋转以匹配另一个矩阵的能力形成一个上界。图[8](#S4.F8
    "Figure 8 ‣ Embedding geometries are similar across parameter scales. ‣ 4 Results
    ‣ Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings")显示了对其共享词汇的模型对的第一个奇异值，包括一个大小为512的随机“嵌入”矩阵。我们发现XLM-R模型彼此之间具有很高的旋转相似度（0.99/1.0），而mT5模型则更加分化（0.95–0.97），但仍然高度相似。所有模型之间的相似度明显高于与随机矩阵的相似度（0.15–0.27）。'
- en: '![Refer to caption](img/44aed24d3ff725dd70daf6c1c15f9056.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/44aed24d3ff725dd70daf6c1c15f9056.png)'
- en: 'Figure 8: Embeddings for overlapping tokens have similar geometries, as measured
    by canonical angles. XLM-R models are extremely similar to each other and mT5
    small and base. All models are far from random (0.14–0.27).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：重叠标记的嵌入具有相似的几何形状，这些几何形状通过标准角度进行测量。XLM-R模型之间非常相似，而mT5小型和基础模型也相似。所有模型的相似度都远离随机值（0.14–0.27）。
- en: 5 Conclusion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: While input embeddings and sub-word tokenized vocabularies of LLMs may appear
    inscrutable, we find that they are in fact interpretable and meaningful. We observe
    significant patterns that differ by model families, including an emergent ability
    of mT5 to discover a shared semantic space that spans languages — accidentally
    achieving a goal that defied a decade of research on cross-lingual word embeddings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLM 的输入嵌入和子词标记化词汇可能显得难以理解，但我们发现它们实际上是可解释和有意义的。我们观察到不同模型家族之间存在显著的模式，包括 mT5
    发现跨语言共享语义空间的能力——意外地实现了一个在跨语言词嵌入研究中存在十年的目标。
- en: Future directions include explaining factors that cause the different token
    embedding patterns we observe in XLM-R and mT5\. This could include investigations
    in model architectures, pre-training procedures, and data-centric strategies such
    as the curation of pre-training corpora. Finding an efficient path to a more equitable
    language model performance will be valuable for enhancing language technology
    for low-resource languages. An interesting next study could explore the utility
    of combining low-resource languages with closely related higher-resource languages
    in pre-training corpora. Another future direction is to explore the potential
    applications of cross-lingual representations embedded in LLMs — What does it
    say about downstream applications? Could it be used to guide practitioners to
    select models that are more appropriate for their intended uses?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的方向包括解释导致我们在 XLM-R 和 mT5 中观察到的不同词嵌入模式的因素。这可能包括对模型架构、预训练程序和数据中心策略（如预训练语料库的策划）的调查。找到一种有效的路径以实现更公平的语言模型性能将对提高低资源语言的语言技术具有重要价值。一个有趣的下一步研究可以探索将低资源语言与密切相关的高资源语言结合在预训练语料库中的效用。另一个未来的方向是探索
    LLM 中嵌入的跨语言表示的潜在应用——这对下游应用有什么意义？它是否可以用来指导从业者选择更适合其预期用途的模型？
- en: 6 Limitations
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: This work is descriptive rather than explanatory. We observe that there are
    patterns in the geometric structure of input embedding matrices in families of
    LLMs, but we are unable to identify why these patterns emerge and differ. There
    are many differences in model architectures, training methods, and pre-training
    corpora between LLM families. It is out of the scope of this work to determine
    what factors are causal. As we have limited ability to carry out pre-training
    experiments, we chose to focus on descriptive observations of existing LLMs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作是描述性的而非解释性的。我们观察到在 LLM 家族中，输入嵌入矩阵的几何结构存在模式，但我们无法确定这些模式为何出现和不同。LLM 家族之间的模型架构、训练方法和预训练语料库存在许多差异。确定哪些因素是因果关系超出了这项工作的范围。由于我们进行预训练实验的能力有限，我们选择专注于对现有
    LLM 的描述性观察。
- en: Acknowledgements
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank the anonymous reviewers for their valuable comments.
    We’d also like to thank Inle Bush, Hyunju Kim, Omary Mzava, Daniel Mwesigwa, Cheng
    Perng Phoo, Top Piriyakulkij, and Ahkln Wong for their assistance in vocabulary
    translation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢匿名评审员的宝贵意见。我们还要感谢 Inle Bush、Hyunju Kim、Omary Mzava、Daniel Mwesigwa、Cheng
    Perng Phoo、Top Piriyakulkij 和 Ahkln Wong 在词汇翻译方面的帮助。
- en: References
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aldarmaki and Diab (2019) Hanan Aldarmaki and Mona Diab. 2019. [Context-aware
    cross-lingual mapping](https://doi.org/10.18653/v1/N19-1391). In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    3906–3911, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aldarmaki 和 Diab（2019）Hanan Aldarmaki 和 Mona Diab. 2019. [上下文感知的跨语言映射](https://doi.org/10.18653/v1/N19-1391).
    见 *2019年北美计算语言学协会会议论文集：人类语言技术，第1卷（长文和短文）*，第3906–3911页，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: Ammar et al. (2016) Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume
    Lample, Chris Dyer, and Noah A Smith. 2016. [Massively multilingual word embeddings](https://arxiv.org/abs/1602.01925).
    *arXiv preprint arXiv:1602.01925*.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ammar 等（2016）Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample,
    Chris Dyer 和 Noah A Smith. 2016. [大规模多语言词嵌入](https://arxiv.org/abs/1602.01925).
    *arXiv 预印本 arXiv:1602.01925*。
- en: 'Artetxe et al. (2017) Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
    [Learning bilingual word embeddings with (almost) no bilingual data](https://doi.org/10.18653/v1/P17-1042).
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 451–462, Vancouver, Canada. Association
    for Computational Linguistics.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Artetxe et al. (2017) Mikel Artetxe, Gorka Labaka 和 Eneko Agirre。2017年。[利用（几乎）无双语数据学习双语词嵌入](https://doi.org/10.18653/v1/P17-1042)。在
    *第55届计算语言学协会年会论文集（第1卷：长论文）*，页码451–462，加拿大温哥华。计算语言学协会。
- en: 'Artetxe et al. (2018) Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.
    [A robust self-learning method for fully unsupervised cross-lingual mappings of
    word embeddings](https://doi.org/10.18653/v1/P18-1073). In *Proceedings of the
    56th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pages 789–798, Melbourne, Australia. Association for Computational
    Linguistics.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Artetxe et al. (2018) Mikel Artetxe, Gorka Labaka 和 Eneko Agirre。2018年。[一种稳健的自学习方法用于完全无监督的跨语言词嵌入映射](https://doi.org/10.18653/v1/P18-1073)。在
    *第56届计算语言学协会年会论文集（第1卷：长论文）*，页码789–798，澳大利亚墨尔本。计算语言学协会。
- en: Chen and Cardie (2018) Xilun Chen and Claire Cardie. 2018. [Unsupervised multilingual
    word embeddings](https://doi.org/10.18653/v1/D18-1024). In *Proceedings of the
    2018 Conference on Empirical Methods in Natural Language Processing*, pages 261–270,
    Brussels, Belgium. Association for Computational Linguistics.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen and Cardie (2018) Xilun Chen 和 Claire Cardie。2018年。[无监督多语言词嵌入](https://doi.org/10.18653/v1/D18-1024)。在
    *2018年自然语言处理实证方法会议论文集*，页码261–270，比利时布鲁塞尔。计算语言学协会。
- en: 'Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. 2019. [What does BERT look at? an analysis of BERT’s attention](https://doi.org/10.18653/v1/W19-4828).
    In *Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting
    Neural Networks for NLP*, pages 276–286, Florence, Italy. Association for Computational
    Linguistics.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy 和 Christopher
    D. Manning。2019年。[BERT关注什么？BERT注意力的分析](https://doi.org/10.18653/v1/W19-4828)。在
    *2019年ACL黑箱NLP研讨会：分析和解释NLP中的神经网络*，页码276–286，意大利佛罗伦萨。计算语言学协会。
- en: Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
    and Veselin Stoyanov. 2020. [Unsupervised cross-lingual representation learning
    at scale](https://doi.org/10.18653/v1/2020.acl-main.747). In *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics*, pages 8440–8451,
    Online. Association for Computational Linguistics.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer
    和 Veselin Stoyanov。2020年。[大规模无监督跨语言表示学习](https://doi.org/10.18653/v1/2020.acl-main.747)。在
    *第58届计算语言学协会年会论文集*，页码8440–8451，在线。计算语言学协会。
- en: Faruqui and Dyer (2014) Manaal Faruqui and Chris Dyer. 2014. [Improving vector
    space word representations using multilingual correlation](https://doi.org/10.3115/v1/E14-1049).
    In *Proceedings of the 14th Conference of the European Chapter of the Association
    for Computational Linguistics*, pages 462–471, Gothenburg, Sweden. Association
    for Computational Linguistics.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faruqui and Dyer (2014) Manaal Faruqui 和 Chris Dyer。2014年。[利用多语言相关性改进向量空间词表示](https://doi.org/10.3115/v1/E14-1049)。在
    *第14届计算语言学协会欧洲分会会议论文集*，页码462–471，瑞典哥德堡。计算语言学协会。
- en: Geva et al. (2022) Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022.
    [Transformer feed-forward layers build predictions by promoting concepts in the
    vocabulary space](https://aclanthology.org/2022.emnlp-main.3). In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages
    30–45, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva et al. (2022) Mor Geva, Avi Caciularu, Kevin Wang 和 Yoav Goldberg。2022年。[Transformer前馈层通过促进词汇空间中的概念来建立预测](https://aclanthology.org/2022.emnlp-main.3)。在
    *2022年自然语言处理实证方法会议论文集*，页码30–45，阿联酋阿布扎比。计算语言学协会。
- en: Geva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    2021. [Transformer feed-forward layers are key-value memories](https://doi.org/10.18653/v1/2021.emnlp-main.446).
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 5484–5495, Online and Punta Cana, Dominican Republic. Association
    for Computational Linguistics.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva 等 (2021) Mor Geva、Roei Schuster、Jonathan Berant 和 Omer Levy. 2021. [变换器前馈层是键值记忆](https://doi.org/10.18653/v1/2021.emnlp-main.446)。在
    *2021年自然语言处理实证方法会议论文集*，第5484–5495页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Gouws and Søgaard (2015) Stephan Gouws and Anders Søgaard. 2015. [Simple task-specific
    bilingual word embeddings](https://doi.org/10.3115/v1/N15-1157). In *Proceedings
    of the 2015 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 1386–1390, Denver, Colorado.
    Association for Computational Linguistics.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gouws 和 Søgaard (2015) Stephan Gouws 和 Anders Søgaard. 2015. [简单的任务特定双语词嵌入](https://doi.org/10.3115/v1/N15-1157)。在
    *2015年北美计算语言学协会会议：人类语言技术会议论文集*，第1386–1390页，丹佛，科罗拉多。计算语言学协会。
- en: Kumar and Albuquerque (2021) Akshi Kumar and Victor Hugo C. Albuquerque. 2021.
    [Sentiment analysis using xlm-r transformer and zero-shot transfer learning on
    resource-poor indian language](https://doi.org/10.1145/3461764). *ACM Trans. Asian
    Low-Resour. Lang. Inf. Process.*, 20(5).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 和 Albuquerque (2021) Akshi Kumar 和 Victor Hugo C. Albuquerque. 2021. [利用
    xlm-r 变换器和零样本迁移学习进行情感分析](https://doi.org/10.1145/3461764)。*ACM Trans. Asian Low-Resour.
    Lang. Inf. Process.*, 20(5)。
- en: Lample et al. (2018) Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato,
    Ludovic Denoyer, and Hervé Jégou. 2018. [Word translation without parallel data](https://openreview.net/forum?id=H196sainb).
    In *International Conference on Learning Representations*.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample 等 (2018) Guillaume Lample、Alexis Conneau、Marc’Aurelio Ranzato、Ludovic
    Denoyer 和 Hervé Jégou. 2018. [无需平行数据的词翻译](https://openreview.net/forum?id=H196sainb)。在
    *国际学习表征会议*。
- en: 'Lauscher et al. (2020) Anne Lauscher, Vinit Ravishankar, Ivan Vulić, and Goran
    Glavaš. 2020. [From zero to hero: On the limitations of zero-shot language transfer
    with multilingual Transformers](https://doi.org/10.18653/v1/2020.emnlp-main.363).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 4483–4499, Online. Association for Computational Linguistics.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lauscher 等 (2020) Anne Lauscher、Vinit Ravishankar、Ivan Vulić 和 Goran Glavaš.
    2020. [从零到英雄：多语言变换器的零样本语言迁移的局限性](https://doi.org/10.18653/v1/2020.emnlp-main.363)。在
    *2020年自然语言处理实证方法会议（EMNLP）*，第4483–4499页，在线。计算语言学协会。
- en: 'Lazaridou et al. (2015) Angeliki Lazaridou, Georgiana Dinu, and Marco Baroni.
    2015. [Hubness and pollution: Delving into cross-space mapping for zero-shot learning](https://doi.org/10.3115/v1/P15-1027).
    In *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 270–280, Beijing, China. Association for Computational
    Linguistics.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lazaridou 等 (2015) Angeliki Lazaridou、Georgiana Dinu 和 Marco Baroni. 2015. [Hubness
    和污染：深入探讨零样本学习的跨空间映射](https://doi.org/10.3115/v1/P15-1027)。在 *第53届计算语言学协会年会和第7届国际联合自然语言处理会议（第一卷：长篇论文）*，第270–280页，北京，中国。计算语言学协会。
- en: 'McInnes et al. (2018) Leland McInnes, John Healy, and James Melville. 2018.
    [Umap: Uniform manifold approximation and projection for dimension reduction](https://arxiv.org/abs/1802.03426).
    *arXiv preprint arXiv:1802.03426*.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'McInnes 等 (2018) Leland McInnes、John Healy 和 James Melville. 2018. [Umap: 用于维度降低的统一流形近似与投影](https://arxiv.org/abs/1802.03426)。*arXiv
    预印本 arXiv:1802.03426*。'
- en: Miceli Barone (2016) Antonio Valerio Miceli Barone. 2016. [Towards cross-lingual
    distributed representations without parallel text trained with adversarial autoencoders](https://doi.org/10.18653/v1/W16-1614).
    In *Proceedings of the 1st Workshop on Representation Learning for NLP*, pages
    121–126, Berlin, Germany. Association for Computational Linguistics.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miceli Barone (2016) Antonio Valerio Miceli Barone. 2016. [朝向无平行文本的跨语言分布式表示，利用对抗自编码器进行训练](https://doi.org/10.18653/v1/W16-1614)。在
    *第1届自然语言处理表示学习研讨会*，第121–126页，柏林，德国。计算语言学协会。
- en: Mikolov et al. (2013) Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013. [Exploiting
    similarities among languages for machine translation](https://arxiv.org/abs/1309.4168).
    *arXiv preprint arXiv:1309.4168*.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等 (2013) Tomas Mikolov、Quoc V Le 和 Ilya Sutskever. 2013. [利用语言间的相似性进行机器翻译](https://arxiv.org/abs/1309.4168)。*arXiv
    预印本 arXiv:1309.4168*。
- en: 'Mrini et al. (2020) Khalil Mrini, Franck Dernoncourt, Quan Hung Tran, Trung
    Bui, Walter Chang, and Ndapa Nakashole. 2020. [Rethinking self-attention: Towards
    interpretability in neural parsing](https://doi.org/10.18653/v1/2020.findings-emnlp.65).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    731–742, Online. Association for Computational Linguistics.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mrini等（2020）。卡利尔·姆里尼、弗朗克·德农科、权洪、陈中和、沃尔特·张及恩达帕·纳卡肖尔。2020年。[重新思考自注意力：朝向神经解析的可解释性](https://doi.org/10.18653/v1/2020.findings-emnlp.65)。在*计算语言学协会发现：EMNLP
    2020*中，页码731–742，在线。计算语言学协会。
- en: Ruder et al. (2019) Sebastian Ruder, Ivan Vulić, and Anders Søgaard. 2019. A
    survey of cross-lingual word embedding models. *Journal of Artificial Intelligence
    Research*, 65:569–631.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鲁德尔等（2019）。塞巴斯蒂安·鲁德尔、伊万·武利奇和安德斯·索加德。2019年。跨语言词嵌入模型综述。*人工智能研究期刊*，65:569–631。
- en: Sato et al. (2018) Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Matsumoto.
    2018. [Interpretable adversarial perturbation in input embedding space for text](https://doi.org/10.24963/ijcai.2018/601).
    In *Proceedings of the Twenty-Seventh International Joint Conference on Artificial
    Intelligence, IJCAI-18*, pages 4323–4330. International Joint Conferences on Artificial
    Intelligence Organization.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 佐藤等（2018）。佐藤基树、铃木纯、进藤博之和松本悠司。2018年。[输入嵌入空间中可解释的对抗扰动](https://doi.org/10.24963/ijcai.2018/601)。在*第二十七届国际人工智能联合会议，IJCAI-18*中，页码4323–4330。国际人工智能联合会议组织。
- en: Serrano and Smith (2019) Sofia Serrano and Noah A. Smith. 2019. [Is attention
    interpretable?](https://doi.org/10.18653/v1/P19-1282) In *Proceedings of the 57th
    Annual Meeting of the Association for Computational Linguistics*, pages 2931–2951,
    Florence, Italy. Association for Computational Linguistics.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 塞拉诺和史密斯（2019）。索非亚·塞拉诺和诺亚·A·史密斯。2019年。[注意力机制是否可解释？](https://doi.org/10.18653/v1/P19-1282)。在*第57届计算语言学协会年会上*，页码2931–2951，意大利佛罗伦萨。计算语言学协会。
- en: Winata et al. (2021) Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne
    Liu, Jason Yosinski, and Pascale Fung. 2021. [Language models are few-shot multilingual
    learners](https://doi.org/10.18653/v1/2021.mrl-1.1). In *Proceedings of the 1st
    Workshop on Multilingual Representation Learning*, pages 1–15, Punta Cana, Dominican
    Republic. Association for Computational Linguistics.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温纳塔等（2021）。温达·印德拉、安德烈亚·马多托、林兆江、刘罗珊、杰森·约辛斯基和帕斯卡尔·冯。2021年。[语言模型是少量样本的多语言学习者](https://doi.org/10.18653/v1/2021.mrl-1.1)。在*第1届多语言表示学习研讨会*中，页码1–15，多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: 'Wu and Dredze (2019) Shijie Wu and Mark Dredze. 2019. [Beto, bentz, becas:
    The surprising cross-lingual effectiveness of BERT](https://doi.org/10.18653/v1/D19-1077).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 833–844, Hong Kong, China. Association for Computational
    Linguistics.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '吴世杰和马克·德雷泽（2019）。2019年。[Beto, bentz, becas: BERT的惊人跨语言效果](https://doi.org/10.18653/v1/D19-1077)。在*2019年自然语言处理经验方法会议和第九届国际联合自然语言处理会议（EMNLP-IJCNLP）*中，页码833–844，香港，中国。计算语言学协会。'
- en: 'Xu et al. (2022) Yuemei Xu, Han Cao, Wanze Du, and Wenqing Wang. 2022. A survey
    of cross-lingual sentiment analysis: Methodologies, models and evaluations. *Data
    Science and Engineering*, 7(3):279–299.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许月梅等（2022）。许月梅、曹涵、杜万泽和王文清。2022年。跨语言情感分析综述：方法、模型与评估。*数据科学与工程*，7(3):279–299。
- en: 'Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami
    Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. [mT5: A massively
    multilingual pre-trained text-to-text transformer](https://doi.org/10.18653/v1/2021.naacl-main.41).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 483–498, Online.
    Association for Computational Linguistics.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 薛琳婷等（2021）。薛琳婷、诺亚·康斯坦特、亚当·罗伯茨、米赫尔·凯尔、拉米·阿尔-弗法、阿迪提亚·西丹特、阿迪提亚·巴鲁阿和科林·拉费尔。2021年。[mT5：一个大规模多语言预训练的文本到文本转换器](https://doi.org/10.18653/v1/2021.naacl-main.41)。在*2021年北美计算语言学协会会议：人类语言技术*中，页码483–498，在线。计算语言学协会。
- en: 'Zhang et al. (2021) Jinpeng Zhang, Baijun Ji, Nini Xiao, Xiangyu Duan, Min
    Zhang, Yangbin Shi, and Weihua Luo. 2021. [Combining static word embeddings and
    contextual representations for bilingual lexicon induction](https://doi.org/10.18653/v1/2021.findings-acl.260).
    In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 2943–2955, Online. Association for Computational Linguistics.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2021）金鹏张、白军季、宁妮·肖、向宇段、敏张、杨斌·施和伟华罗。2021。[结合静态词嵌入和上下文表示进行双语词典诱导](https://doi.org/10.18653/v1/2021.findings-acl.260)。发表于*计算语言学协会发现：ACL-IJCNLP
    2021*，页码2943–2955，在线。计算语言学协会。
- en: 'Zhang et al. (2017) Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017.
    [Adversarial training for unsupervised bilingual lexicon induction](https://doi.org/10.18653/v1/P17-1179).
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 1959–1970, Vancouver, Canada. Association
    for Computational Linguistics.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2017）孟张、杨刘、欢博栾和毛松孙。2017。[用于无监督双语词典诱导的对抗训练](https://doi.org/10.18653/v1/P17-1179)。发表于*第55届计算语言学协会年会（第1卷：长篇论文）*，页码1959–1970，加拿大温哥华。计算语言学协会。
- en: Zou et al. (2013) Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D.
    Manning. 2013. [Bilingual word embeddings for phrase-based machine translation](https://aclanthology.org/D13-1141).
    In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language
    Processing*, pages 1393–1398, Seattle, Washington, USA. Association for Computational
    Linguistics.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邹等人（2013）威尔·Y·邹、理查德·索彻、丹尼尔·塞尔和克里斯托弗·D·曼宁。2013。[用于基于短语的机器翻译的双语词嵌入](https://aclanthology.org/D13-1141)。发表于*2013年自然语言处理实证方法会议论文集*，页码1393–1398，美国华盛顿州西雅图。计算语言学协会。
- en: Appendix A ISO 639-1 language codes
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A ISO 639-1语言代码
- en: 'The language codes for Figure [4](#S4.F4 "Figure 4 ‣ Models encode languages
    differently. ‣ 4 Results ‣ Hyperpolyglot LLMs: Cross-Lingual Interpretability
    in Token Embeddings"): ar: Arabic, bg: Bulgarian, da: Danish, de: German, es:
    Spanish, et: Estonian, fa: Persian, fr: French, he: Hebrew, hu: Hungarian, is:
    Icelandic, ja: Japanese, km: Khmer, ko: Korean, mk: Macedonian, ml: Malayalam,
    ms: Indonesian/Malay, my: Burmese, pt: Portuguese, ru: Russian, sw: Swahili, sv:
    Swedish, sq: Albanian, tr: Turkish, th: Thai, zh: Chinese'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S4.F4 "图 4 ‣ 模型以不同方式编码语言。 ‣ 4 结果 ‣ 超多语言 LLMs：标记嵌入中的跨语言可解释性")的语言代码：ar：阿拉伯语，bg：保加利亚语，da：丹麦语，de：德语，es：西班牙语，et：爱沙尼亚语，fa：波斯语，fr：法语，he：希伯来语，hu：匈牙利语，is：冰岛语，ja：日语，km：高棉语，ko：韩语，mk：马其顿语，ml：马拉雅拉姆语，ms：印尼语/马来语，my：缅甸语，pt：葡萄牙语，ru：俄语，sw：斯瓦希里语，sv：瑞典语，sq：阿尔巴尼亚语，tr：土耳其语，th：泰语，zh：中文
- en: Appendix B Frequency of tokens does not correlate with diversity of neighbors.
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 标记的频率与邻居的多样性无关。
- en: 'It could be that words with fewer occurrences in the training corpus would
    have more or less diversity in Unicode categories in their neighbor sets. We calculated
    an estimate of the frequency of mT5 SP tokens based on a sample from the mC4 dataset.
    We then took a stratified sample of tokens from the mT5 vocabulary from 10 frequency
    bands and calculated the mean number of distinct Unicode categories for their
    neighbors, see Figure [9](#A2.F9 "Figure 9 ‣ Appendix B Frequency of tokens does
    not correlate with diversity of neighbors. ‣ Hyperpolyglot LLMs: Cross-Lingual
    Interpretability in Token Embeddings"). We find no correlation between the frequency
    of terms and the diversity of their nearest neighbor sets.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是训练语料库中出现频率较少的词在其邻居集中在Unicode类别上的多样性有所不同。我们基于mC4数据集的样本计算了mT5 SP标记的频率估计值。然后，我们从mT5词汇表中按10个频率区间进行分层抽样，并计算了其邻居的不同Unicode类别的平均数量，见图[9](#A2.F9
    "图 9 ‣ 附录B 标记的频率与邻居的多样性无关。 ‣ 超多语言 LLMs：标记嵌入中的跨语言可解释性")。我们发现术语的频率与其最近邻集的多样性之间没有相关性。
- en: '![Refer to caption](img/e4b146c59434bccf2262f17cf7580060.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4b146c59434bccf2262f17cf7580060.png)'
- en: 'Figure 9: There is no correlation between how often a token is found in pre-training
    corpora and how diverse a token’s 50 nearest neighbor set is. The neighbor set
    is calculated with mT5-XL embeddings.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：标记在预训练语料库中的出现频率与标记的50个最近邻集的多样性之间没有相关性。邻居集是使用mT5-XL嵌入计算的。
