- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 17:34:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 17:34:02'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted
    Prompt Interpretation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实与幻想的构造：使用LLM辅助的提示解释进行场景生成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12579](https://ar5iv.labs.arxiv.org/html/2407.12579)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12579](https://ar5iv.labs.arxiv.org/html/2407.12579)
- en: '¹¹institutetext: National Yang Ming Chiao Tung University, Taiwan,'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹机构: 国立阳明交通大学，台湾，'
- en: '¹¹email: {leo81005.ee10, cfhsu311510211.ee11, hhshuai}@nycu.edu.tw ²²institutetext:
    Jilin University, China, ³³institutetext: National Taiwan University, Taiwan'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹电子邮件: {leo81005.ee10, cfhsu311510211.ee11, hhshuai}@nycu.edu.tw ²²机构: 吉林大学，中国，³³机构:
    国立台湾大学，台湾'
- en: '³³email: {wenhuang@ntu.edu.tw} ^*^*footnotetext: These authors contributed
    equally to this workYi Yao 1*1* [0000-0001-8227-5662](https://orcid.org/0000-0001-8227-5662
    "ORCID identifier")    Chan-Feng Hsu 1*1*    Jhe-Hao Lin 11    Hongxia Xie 22
    [0000-0002-5652-4327](https://orcid.org/0000-0002-5652-4327 "ORCID identifier")
       Terence Lin 11    Yi-Ning Huang 11    Hong-Han Shuai 11 [0000-0003-2216-077X](https://orcid.org/0000-0003-2216-077X
    "ORCID identifier")    Wen-Huang Cheng 33 [0000-0002-4662-7875](https://orcid.org/0000-0002-4662-7875
    "ORCID identifier")'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '³³电子邮件: {wenhuang@ntu.edu.tw} ^*^*脚注: 这些作者对本工作贡献相等Yi Yao 1*1* [0000-0001-8227-5662](https://orcid.org/0000-0001-8227-5662
    "ORCID identifier")    Chan-Feng Hsu 1*1*    Jhe-Hao Lin 11    Hongxia Xie 22
    [0000-0002-5652-4327](https://orcid.org/0000-0002-5652-4327 "ORCID identifier")
       Terence Lin 11    Yi-Ning Huang 11    Hong-Han Shuai 11 [0000-0003-2216-077X](https://orcid.org/0000-0003-2216-077X
    "ORCID identifier")    Wen-Huang Cheng 33 [0000-0002-4662-7875](https://orcid.org/0000-0002-4662-7875
    "ORCID identifier")'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In spite of recent advancements in text-to-image generation, limitations persist
    in handling complex and imaginative prompts due to the restricted diversity and
    complexity of training data. This work explores how diffusion models can generate
    images from prompts requiring artistic creativity or specialized knowledge. We
    introduce the Realistic-Fantasy Benchmark (RFBench), a novel evaluation framework
    blending realistic and fantastical scenarios. To address these challenges, we
    propose the Realistic-Fantasy Network (RFNet), a training-free approach integrating
    diffusion models with LLMs. Extensive human evaluations and GPT-based compositional
    assessments demonstrate our approach’s superiority over state-of-the-art methods.
    Our code and dataset is available at [https://leo81005.github.io/Reality-and-Fantasy/](https://leo81005.github.io/Reality-and-Fantasy/).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文本到图像生成领域最近取得了进展，但由于训练数据的多样性和复杂性受限，处理复杂和富有想象力的提示仍存在局限。本工作探讨了扩散模型如何从需要艺术创意或专业知识的提示中生成图像。我们引入了现实-幻想基准（RFBench），这是一个将现实和幻想情境相结合的新型评估框架。为了解决这些挑战，我们提出了现实-幻想网络（RFNet），这是一种无训练的方式，将扩散模型与LLM集成。大量的人类评估和基于GPT的组合评估展示了我们方法相对于最先进方法的优势。我们的代码和数据集可以在
    [https://leo81005.github.io/Reality-and-Fantasy/](https://leo81005.github.io/Reality-and-Fantasy/)
    获得。
- en: 'Keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '关键词:'
- en: Text-to-image Generation Realistic-Fantasy Benchmark Diffusion Model Large Language
    Models (LLMs)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像生成 现实-幻想基准 扩散模型 大型语言模型（LLMs）
- en: '![Refer to caption](img/00740728149d9e7554a397734c1c227a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/00740728149d9e7554a397734c1c227a.png)'
- en: 'Figure 1: Text-to-image diffusion models such as Stable Diffusion [[31](#bib.bib31)]
    often struggle to accurately follow prompts that involve scientific and empirical
    reasoning, metaphorical thinking, role conflicting, or imaginative scenarios.
    Our method achieves enhanced prompt understanding capabilities and accurately
    follows these types of prompts.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: 文本到图像扩散模型如Stable Diffusion [[31](#bib.bib31)] 经常难以准确遵循涉及科学和实证推理、隐喻思维、角色冲突或想象情境的提示。我们的方法实现了增强的提示理解能力，并能准确地遵循这些类型的提示。'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Considerable advancements have been made in the field of text-to-image generation,
    especially with the introduction of diffusion models, e.g., Stable Diffusion [[31](#bib.bib31)],
    GLIDE [[22](#bib.bib22)], DALLE2 [[30](#bib.bib30)] and Imagen [[32](#bib.bib32)].
    These models exhibit remarkable proficiency in generating diverse and high-fidelity
    images based on natural language prompts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本到图像生成领域取得了显著进展，特别是引入了扩散模型，例如，Stable Diffusion [[31](#bib.bib31)]，GLIDE [[22](#bib.bib22)]，DALLE2 [[30](#bib.bib30)]
    和 Imagen [[32](#bib.bib32)]。这些模型在根据自然语言提示生成多样化且高保真度的图像方面展现出了卓越的能力。
- en: 'However, despite their impressive capabilities, diffusion models occasionally
    face challenges in accurately interpreting complex prompts that demand a deep
    understanding or specialized knowledge [[43](#bib.bib43), [39](#bib.bib39), [41](#bib.bib41)].
    This limitation becomes particularly apparent with creative and abstract prompts,
    which require a nuanced grasp of context and subtleties. For example, in scenarios
    where the prompt involves unconventional scenarios like “a rat is hunting a lion”,
    traditional diffusion models might not accurately represent the intended dynamics
    or relationships between entities (as shown in Fig.[1](#S0.F1 "Figure 1 ‣ The
    Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation")).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管扩散模型具有令人印象深刻的能力，但它们有时在准确解读需要深刻理解或专业知识的复杂提示时会面临挑战[[43](#bib.bib43), [39](#bib.bib39),
    [41](#bib.bib41)]。这种限制在处理创意和抽象提示时尤为明显，这类提示需要对上下文和细微差别有深刻的把握。例如，在提示涉及非常规场景如“老鼠在猎捕狮子”时，传统的扩散模型可能无法准确表现实体之间的预期动态或关系（如图[1](#S0.F1
    "Figure 1 ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted
    Prompt Interpretation")所示）。'
- en: A significant obstacle for traditional diffusion models in creating abstract
    images is the bias present within their training datasets [[24](#bib.bib24), [6](#bib.bib6)].
    These datasets often do not include images of scenarios that defy conventional
    reality, such as a mouse hunting a lion. Traditionally, mitigating these challenges
    has required costly data collection and complex filtering, as well as model retraining
    or fine-tuning [[8](#bib.bib8), [12](#bib.bib12), [42](#bib.bib42), [40](#bib.bib40)].
    Research costs significantly increase due to these long and labor-intensive processes.
    Moreover, fine-tuning neural networks and model editing can lead to catastrophic
    forgetting and overall performance degradation [[14](#bib.bib14), [33](#bib.bib33)].
    Recently, it has been demonstrated that utilizing Large Language Models (LLMs)
    to aid in the generation process ensures the production of accurate details [[36](#bib.bib36),
    [16](#bib.bib16), [15](#bib.bib15), [40](#bib.bib40), [5](#bib.bib5), [38](#bib.bib38),
    [27](#bib.bib27)]. Directly integrating these models during the generation phase
    signifies a more efficient strategy.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 传统扩散模型在生成抽象图像时面临的一个重大障碍是训练数据集中存在的偏差[[24](#bib.bib24), [6](#bib.bib6)]。这些数据集通常不包括违背常规现实的场景图像，例如老鼠猎捕狮子。传统上，缓解这些挑战需要昂贵的数据收集和复杂的筛选，以及模型的重新训练或微调[[8](#bib.bib8),
    [12](#bib.bib12), [42](#bib.bib42), [40](#bib.bib40)]。由于这些漫长且劳动密集的过程，研究成本显著增加。此外，微调神经网络和模型编辑可能导致灾难性遗忘和整体性能下降[[14](#bib.bib14),
    [33](#bib.bib33)]。最近的研究表明，利用大型语言模型（LLMs）来辅助生成过程可以确保细节的准确性[[36](#bib.bib36), [16](#bib.bib16),
    [15](#bib.bib15), [40](#bib.bib40), [5](#bib.bib5), [38](#bib.bib38), [27](#bib.bib27)]。在生成阶段直接整合这些模型标志着一种更高效的策略。
- en: 'In this paper, we want to address the question: how can generative models be
    improved to better capture imaginative and abstract concepts in images? In response
    to the existing gap in benchmarks for abstract and creative text-to-image synthesis,
    our work introduces a novel benchmark, Realistic-Fantasy Benchmark (RFBench).
    This benchmark is designed to evaluate both Realistic & Analytical and Creativity
    & Imagination interpretations in generated images. The Realistic & Analytical
    category includes four sub-categories, focusing on the models’ ability to adhere
    to realism and analytical depth. Images are generated in response to prompts that
    require not only precision in science but also cultural sensitivity and nuanced
    expression of symbolic meaning. On the other hand, Creativity & Imagination, is
    segmented into five specific sub-categories based on attribute distinctions, challenges
    models to navigate the complexities of generating images from prompts that necessitate
    a high degree of creativity and abstract reasoning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们希望解决一个问题：如何改进生成模型以更好地捕捉图像中的想象性和抽象概念？为了填补抽象和创意文本到图像合成的基准缺口，我们的工作引入了一个新颖的基准——现实幻想基准（RFBench）。该基准旨在评估生成图像中的现实与分析性以及创意与想象力的解读。现实与分析性类别包括四个子类别，关注模型对现实主义和分析深度的遵循。图像是针对需要科学精确度、文化敏感性和象征意义细腻表达的提示生成的。另一方面，创意与想象力被细分为五个特定子类别，根据属性区分，挑战模型在生成需要高度创造力和抽象推理的图像时应对复杂性。
- en: To empower diffusion models with the capability to generate imaginative and
    abstract images, we introduce an innovative training-free approach Realistic-Fantasy
    Network (RFNet) that integrates diffusion models with LLMs. Given a prompt describing
    the desired image, the LLM generates an image layout, which includes bounding
    boxes for main subjects and background elements, along with textual details to
    support logic or interpret scientific data. To refine image generation, we further
    propose the Semantic Alignment Assessment (SAA), ensuring consistency with the
    scene’s objects. This crucial step improves the final image quality. The enhanced
    details direct the diffusion model, enabling precise object placement through
    guidance constraints. Our method, leveraging pre-trained models, is compatible
    with independently trained LLMs and diffusion models, eliminating the need for
    parameter adjustments.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了赋予扩散模型生成富有想象力和抽象图像的能力，我们引入了一种创新的无训练方法Realistic-Fantasy Network (RFNet)，它将扩散模型与LLM结合起来。在给定描述所需图像的提示后，LLM生成一个图像布局，其中包括主要对象和背景元素的边界框，以及支持逻辑或解释科学数据的文本细节。为了完善图像生成，我们进一步提出了语义对齐评估（SAA），确保与场景中的对象一致。这一关键步骤提高了最终图像的质量。增强的细节引导扩散模型，通过指导约束实现精确的对象放置。我们的方法利用预训练模型，兼容独立训练的LLM和扩散模型，无需调整参数。
- en: 'In summary, our key contributions are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的关键贡献包括：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We have collected a novel Realistic-Fantasy Benchmark (RFBench), which is a
    meticulously curated benchmark that stands out for its rich diversity of scenarios.
    It challenges and extends the boundaries of generative model creativity and inference
    capabilities, establishing a new standard for assessing imaginative data processing.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们收集了一个新颖的Realistic-Fantasy Benchmark (RFBench)，这是一个经过精心策划的基准测试，以其丰富多样的场景而独树一帜。它挑战并扩展了生成模型的创造力和推理能力的边界，为评估想象力数据处理建立了新的标准。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: To empower diffusion models with the capability to generate imaginative and
    abstract images, we introduce an innovative training-free approach, Realistic-Fantasy
    Network (RFNet), that integrates diffusion models with LLMs.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了赋予扩散模型生成富有想象力和抽象图像的能力，我们引入了一种创新的无训练方法，Realistic-Fantasy Network (RFNet)，它将扩散模型与LLM结合起来。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Through our proposed RFBench, extensive human evaluations coupled with GPT-based
    compositional assessments have demonstrated our approach’s superiority over other
    state-of-the-art methods.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过我们提出的RFBench，广泛的人类评估结合GPT基础的组成评估证明了我们方法相较于其他最先进方法的优越性。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Text-guided diffusion models
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 文本引导的扩散模型
- en: Diffusion models, utilizing stochastic differential equations, have emerged
    as effective tools for generating realistic images [[31](#bib.bib31), [1](#bib.bib1),
    [9](#bib.bib9), [4](#bib.bib4), [21](#bib.bib21), [30](#bib.bib30), [22](#bib.bib22),
    [32](#bib.bib32)]. DALL-E 2 [[30](#bib.bib30)] pioneered the approach of converting
    textual descriptions into joint image-text embeddings with the aid of CLIP [[29](#bib.bib29)].
    GLIDE [[22](#bib.bib22)] demonstrated that classifier-free guidance [[11](#bib.bib11)]
    is favored by human evaluators over CLIP guidance for generating images based
    on text descriptions. Imagen [[32](#bib.bib32)] follows GLIDE but uses pretrained
    text encoder instead, further reducing negligible computation burden to the online
    training of the text-to-image diffusion prior, and can improve sample quality
    significantly by simply scaling the text encoder.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型利用随机微分方程，已经成为生成现实图像的有效工具[[31](#bib.bib31), [1](#bib.bib1), [9](#bib.bib9),
    [4](#bib.bib4), [21](#bib.bib21), [30](#bib.bib30), [22](#bib.bib22), [32](#bib.bib32)]。DALL-E
    2 [[30](#bib.bib30)] 开创了将文本描述转化为联合图像-文本嵌入的方法，借助CLIP [[29](#bib.bib29)]。GLIDE [[22](#bib.bib22)]
    证明了无分类器指导 [[11](#bib.bib11)] 相比CLIP指导更受人类评估者的青睐，用于基于文本描述生成图像。Imagen [[32](#bib.bib32)]
    继承了GLIDE，但使用预训练的文本编码器，进一步减少了对文本到图像扩散先验在线训练的计算负担，通过简单地扩展文本编码器显著提高了样本质量。
- en: Although text-to-image capabilities have seen significant development, there
    has been limited focus on generating images involving high levels of creativity,
    scientific principles, cultural references, and symbolic meanings. The primary
    reason is the data bias in the training dataset [[20](#bib.bib20), [34](#bib.bib34),
    [23](#bib.bib23)]. Several studies [[24](#bib.bib24), [18](#bib.bib18), [20](#bib.bib20)]
    have investigated the impact of data bias on diffusion models, particularly in
    the context of Text-to-Image generation. Perera et al.[[24](#bib.bib24)] investigates
    the bias exhibited by diffusion models across various attributes in face generation.
    Luccioni et al.[[18](#bib.bib18)] evaluates bias levels in text-to-image systems
    regarding gender and ethnicity.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文本到图像的能力已经取得了显著进展，但对于涉及高水平创造力、科学原理、文化参考和象征意义的图像生成关注较少。主要原因是训练数据集中的数据偏差[[20](#bib.bib20),
    [34](#bib.bib34), [23](#bib.bib23)]。一些研究[[24](#bib.bib24), [18](#bib.bib18), [20](#bib.bib20)]调查了数据偏差对扩散模型的影响，特别是在文本到图像生成的背景下。Perera等人[[24](#bib.bib24)]研究了扩散模型在面部生成中的各种属性的偏差。Luccioni等人[[18](#bib.bib18)]评估了文本到图像系统中关于性别和种族的偏差水平。
- en: 'In this work, we introduce a new task: reality and fantasy scene generation.
    Recognizing the absence of a dedicated evaluation framework for such tasks, we
    introduce a new benchmark, the Realistic-Fantasy Benchmark (RFBench), which blends
    scenarios from both realistic and fantastical realms.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们引入了一项新任务：现实与幻想场景生成。鉴于缺乏专门的评估框架，我们引入了一个新的基准——现实与幻想基准（RFBench），它融合了现实和幻想领域的场景。
- en: 2.2 LLMs for image generation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM在图像生成中的应用
- en: Recently, researchers have explored using LLMs to provide guidance or auxiliary
    information for text-to-image generation systems [[28](#bib.bib28), [19](#bib.bib19),
    [15](#bib.bib15), [7](#bib.bib7), [38](#bib.bib38), [36](#bib.bib36), [27](#bib.bib27)].
    In LMD [[15](#bib.bib15)], foreground objects are identified using LLMs, and then
    images are generated based on the layout determined by the diffusion model. Phung
    et al. [[25](#bib.bib25)] proposes attention-refocusing losses to constrain the
    generated objects on their assigned boxes generated by LLMs. LVD [[16](#bib.bib16)]
    requires LLMs to generate continuous spatial constraints to accomplish video generation.
    Besides using LLMs to generate spatial layout from user prompts, some studies
    [[36](#bib.bib36), [38](#bib.bib38), [27](#bib.bib27)] investigate integrating
    LLMs directly into the image generation pipeline. SLD [[36](#bib.bib36)] integrates
    open-vocabulary object detection with LLMs to enhance image editing. RPG [[38](#bib.bib38)]
    integrates LLMs in a closed-loop manner, allowing generated images to continuously
    improve through LLMs feedback, and uses Chain-of-Thought [[35](#bib.bib35)] to
    further improve generation quality.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员探索了利用LLM为文本到图像生成系统提供指导或辅助信息[[28](#bib.bib28), [19](#bib.bib19), [15](#bib.bib15),
    [7](#bib.bib7), [38](#bib.bib38), [36](#bib.bib36), [27](#bib.bib27)]。在LMD[[15](#bib.bib15)]中，使用LLM识别前景对象，然后基于扩散模型确定的布局生成图像。Phung等人[[25](#bib.bib25)]提出了注意力重定向损失，以约束LLM生成的对象在其分配的框中的位置。LVD[[16](#bib.bib16)]要求LLM生成连续的空间约束以完成视频生成。除了使用LLM从用户提示生成空间布局外，一些研究[[36](#bib.bib36),
    [38](#bib.bib38), [27](#bib.bib27)]还探讨了将LLM直接集成到图像生成流程中的方法。SLD[[36](#bib.bib36)]将开放词汇对象检测与LLM结合，以增强图像编辑。RPG[[38](#bib.bib38)]以闭环方式集成LLM，使生成的图像能够通过LLM反馈持续改进，并使用Chain-of-Thought[[35](#bib.bib35)]进一步提高生成质量。
- en: As a result of these developments, LLMs can be incorporated into pipelines for
    the generation of images. In this work, we use LLMs to uncover and elaborate upon
    the complexities embedded within complex and abstract prompts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些发展，LLM（大语言模型）可以被纳入生成图像的流程中。在这项工作中，我们利用LLM揭示并阐述嵌入在复杂和抽象提示中的复杂性。
- en: 3 Our Proposed Realistic-Fantasy Benchmark
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 我们提出的现实与幻想基准
- en: In this study, we explore how diffusion models can effectively process and generate
    imagery from prompts that pose significant challenges due to their reliance on
    creative thinking or specialized knowledge. Recognizing the absence of a dedicated
    evaluation framework for such tasks, we introduce a new benchmark, the Realistic-Fantasy
    Benchmark (RFBench), which blends scenarios from both realistic and fantastical
    realms.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们探索了扩散模型如何有效地处理和生成那些由于依赖于创造性思维或专业知识而面临重大挑战的提示所产生的图像。鉴于缺乏专门的评估框架，我们引入了一个新的基准——现实与幻想基准（RFBench），它融合了现实和幻想领域的场景。
- en: 'Benchmark Collection. We focus on two main categories, each with distinct subcategories,
    Realistic & Analytical and Creativity & Imagination, totaling nine subcategories.
    Each sub-category is meticulously crafted with around 25 text prompts, leading
    to an aggregate of 229 unique compositional text prompts designed to test the
    models against both conventional and unprecedented creative challenges. The collection
    process, outlined in [Fig. 2](#S3.F2 "In 3 Our Proposed Realistic-Fantasy Benchmark
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), employs a hybrid method combining in-context learning and predefined
    rules, leveraging powerful language models such as ChatGPT and Bard for diverse
    text prompts creation. By alternating between these models, we achieve a diverse
    set of responses, capitalizing on the distinct advantages of each LLM. It boosts
    the variety and complexity of prompts while reducing the reliance on manual labeling.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基准收集。我们专注于两个主要类别，每个类别具有不同的子类别，即现实与分析和创造力与想象，总共有九个子类别。每个子类别都经过精心设计，包含约25个文本提示，总共229个独特的组合文本提示，旨在测试模型在传统和前所未有的创造性挑战下的表现。收集过程如[图2](#S3.F2
    "在3 我们提出的现实-幻想基准 ‣ 现实与幻想的构造：LLM辅助提示解释的场景生成")所述，采用了结合上下文学习和预定义规则的混合方法，利用强大的语言模型如ChatGPT和Bard创建多样化的文本提示。通过交替使用这些模型，我们实现了响应的多样性，并充分利用了每个LLM的独特优势。这增强了提示的多样性和复杂性，同时减少了对人工标注的依赖。
- en: '![Refer to caption](img/a560b5aaf755b857f869790347e0f29c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a560b5aaf755b857f869790347e0f29c.png)'
- en: 'Figure 2: The collection pipeline of our proposed RFBench.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们提出的RFBench的收集流程。
- en: 'Table 1: Categories of our proposed Realistic-Fantasy Benchmark (RFBench).
    The full dataset are show in our supplementary material.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们提出的现实-幻想基准（RFBench）的类别。完整数据集显示在我们的补充材料中。
- en: '| Categories | Definition | Example |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 定义 | 示例 |'
- en: '| Realistic & Analytical |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 现实与分析 |'
- en: '| To evaluate models’ comprehensive understanding of complex ideas, factual
    accuracy, and cultural insights |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 评估模型对复杂思想的全面理解、事实准确性和文化洞察 |'
- en: '| Scientific and Empirical Reasoning | Relates to hypothesis testing, deduction,
    and scientific methodology | “A drop of water on the International Space Station.”
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 科学与实证推理 | 关系到假设检验、推理和科学方法 | “国际空间站上的一滴水。” |'
- en: '| Cultural and Temporal Awareness | Necessitates understanding and knowledge
    of particular cultural or historical events | “Children in costumes going door-to-door
    on October 31st.” |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 文化和时间意识 | 需要了解特定的文化或历史事件 | “10月31日，穿着服装的孩子们挨家挨户敲门。”'
- en: '| Factual or Literal Descriptions | Evaluates the models’ capacity to generate
    images that adhere closely to factual accuracy and realistic depiction | “A tank
    that’s been sitting on the beach for 50 years.” |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 事实或字面描述 | 评估模型生成准确且逼真的图像的能力 | “一个在海滩上放了50年的坦克。” |'
- en: '| Conceptual and Metaphorical Thinking | Focuses on the models’ ability to
    comprehend and depict the underlying symbolic messages within the prompts | “A
    man is as brave as a lion.” |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 概念和隐喻思维 | 关注模型理解和描绘提示中潜在符号信息的能力 | “一个人像狮子一样勇敢。” |'
- en: '| Creativity & Imagination |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 创造力与想象 |'
- en: '| To evaluate model’s capability in employing creativity and abstract thinking
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 评估模型在运用创造力和抽象思维方面的能力 |'
- en: '| Common Objects in Unusual Contexts | Challenges models’ capacity to maintain
    object integrity while adapting to surreal environments | “A rubber duck sailing
    across a field of hot lava.” |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 不寻常情境中的常见物体 | 挑战模型在适应超现实环境时保持物体完整性的能力 | “一只橡皮鸭在火热的熔岩场上航行。” |'
- en: '| Imaginative Scenarios | Evaluate the models’ abilities to craft scenes involving
    animals or humans in fantastical or unlikely scenarios | “An octopus playing chess
    with a seahorse.” |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 富有想象力的情境 | 评估模型在处理涉及动物或人类的奇幻或不太可能的情境中的能力 | “一只章鱼在和一只海马下棋。” |'
- en: '| Counterfactual Scenarios | Focuses on scenarios that defy conventional expectations
    of reality | “Fish swimming in the clouds.” |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 反事实情境 | 关注违背现实常规期望的情境 | “鱼在云中游泳。” |'
- en: '| Role Reversal or Conflicting | Stimulates consideration of perspectives and
    scenarios where typical roles or expectations are inverted | “A cat is chased
    by a mouse.” |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 角色倒置或冲突 | 激发考虑那些典型角色或期望被颠倒的视角和情境 | “一只猫被一只老鼠追赶。” |'
- en: '| Anthropomorphic Scenarios | Examines the model’s ability to imbue inanimate
    objects or phenomena with human-like characteristics | “A snowman building a friend
    in the blizzard.” |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 拟人化场景 | 评估模型赋予无生命物体或现象人类特征的能力 | “一个雪人在暴风雪中制造朋友。” |'
- en: 'Realistic & Analytical Category. There are four sub-categories: Scientific
    and Empirical Reasoning, Cultural and Temporal Awareness, Factual or Literal Descriptions,
    and Conceptual and Metaphorical Thinking (details are shown in the upper part
    of [Tab. 1](#S3.T1 "In 3 Our Proposed Realistic-Fantasy Benchmark ‣ The Fabrication
    of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation")).
    These sub-categories are anchored in real-world contexts, emphasizing logical
    reasoning, accurate data, and an understanding of cultural or historical contexts.
    They contain scientific exploration, realistic descriptions, and culturally symbolic
    narratives. This demands that the models not only draw from an extensive knowledge
    pool but also demonstrate an ability to grasp and articulate underlying concepts.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现实与分析类别。该类别包括四个子类别：科学和实证推理、文化和时间意识、事实或字面描述，以及概念和隐喻思维（详细信息见[表 1](#S3.T1 "在3 我们提出的现实幻想基准
    ‣ 现实与幻想的制造：基于LLM辅助提示解释的场景生成")的上半部分）。这些子类别以现实世界为基础，强调逻辑推理、准确数据以及对文化或历史背景的理解。它们包含科学探索、现实描述和文化象征叙事。这要求模型不仅要从广泛的知识库中汲取，还要展示理解和阐述潜在概念的能力。
- en: 'Creativity & Imagination Category. It consists of five sub-categories: Common
    Objects in Unusual Contexts, Imaginative Scenarios, Counterfactual Scenarios,
    Role Reversal or Conflicting, and Anthropomorphic Scenarios (details are shown
    in the lower part of [Tab. 1](#S3.T1 "In 3 Our Proposed Realistic-Fantasy Benchmark
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation")). This evaluation focuses on the model’s capacity to innovatively
    repurpose familiar objects, attribute human-like characteristics to inanimate
    objects, and generate novel environments for everyday items. This category tests
    the model’s out-of-the-box thinking and imaginative capabilities.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 创造力与想象力类别。它包含五个子类别：不寻常背景中的常见物体、富有想象力的场景、反事实场景、角色互换或冲突，以及拟人化场景（详细信息见[表 1](#S3.T1
    "在3 我们提出的现实幻想基准 ‣ 现实与幻想的制造：基于LLM辅助提示解释的场景生成")的下半部分）。该评估关注模型创新性地重新利用熟悉物体的能力，赋予无生命物体人类特征，并为日常物品生成新颖环境。此类别测试模型的创新思维和想象能力。
- en: 4 Our Proposed Realistic-Fantasy Network
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 我们提出的现实幻想网络
- en: '![Refer to caption](img/ad2ad9fb288469cca2706c443510a1bc.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ad2ad9fb288469cca2706c443510a1bc.png)'
- en: 'Figure 3: Overview of our proposed Realistic-Fantasy Network (RFNet). In stage
    1, the user’s input prompt is first processed by a LLM to extract the layout and
    descriptions. The descriptions then go through a text encoder, which is the text-processing
    component of the CLIP model, and are refined by the SAA to form a better prompt.
    In stage 2, the refined prompts are fed into the diffusion model for in-depth
    object generation, which creates each target object with precision. The resulting
    cross-attention map and mask latent are then utilized for seamless background
    integration, merging objects into one single image.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们提出的现实幻想网络（RFNet）概述。在第1阶段，用户的输入提示首先由大型语言模型（LLM）处理，以提取布局和描述。描述随后通过文本编码器，即CLIP模型的文本处理组件，并由SAA进行优化，以形成更好的提示。在第2阶段，优化后的提示被输入到扩散模型中进行深入对象生成，从而精确创建每个目标对象。生成的交叉注意力图和掩码潜变量然后用于无缝的背景融合，将对象合并成一个单一的图像。
- en: 'In this section, we propose a Realistic-Fantasy Network (RFNet) for the benchmark
    scenario we proposed in the previous section. To thoroughly interpret the details
    from the input prompt, we divide our approach into two stages, as shown in [Fig. 3](#S4.F3
    "In 4 Our Proposed Realistic-Fantasy Network ‣ The Fabrication of Reality and
    Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation"). In the first
    stage, we transform the initial input prompt into a refined version specifically
    tailored for image generation by LLMs. In the second stage, we utilize a diffusion
    model through a two-step process to generate outputs with extraordinary details.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了一个用于我们在前一节中提出的基准场景的现实幻想网络（RFNet）。为了彻底解释输入提示中的细节，我们将我们的方法分为两个阶段，如[图
    3](#S4.F3 "在 4 我们提出的现实幻想网络 ‣ 现实与幻想的编织：通过 LLM 辅助的提示解释生成场景")所示。在第一阶段，我们将初始输入提示转化为专门针对图像生成的精细版本。在第二阶段，我们通过两步过程利用扩散模型生成具有非凡细节的输出。
- en: 4.1 LLM-Driven Detail Synthesis
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于 LLM 的细节合成
- en: 'In the first stage of our methodology, we concentrate on utilizing LLMs to
    uncover and elaborate on the intricacies embedded within the user’s input prompt.
    This process involves specifying task requirements to more accurately define the
    task and incorporating in-context learning to enhance understanding for LLMs.
    The enriched response from the LLM encompasses additional information, such as
    layout, detailed descriptions, background scenes, and negative prompts ¹¹1One
    detailed sample can be found in our supplementary material.. This step is crucial
    as it aims to mitigate the primary challenge we seek to overcome: the training
    data bias inherent in current diffusion models. By leveraging the pre-trained
    LLM for logical reasoning and conjecture, we aim to compensate for the gaps left
    by these biases, ensuring a more accurate and coherent image generation process.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们方法的第一阶段，我们专注于利用 LLM 来揭示和阐述用户输入提示中嵌入的复杂性。该过程包括指定任务要求，以更准确地定义任务，并结合上下文学习以增强对
    LLM 的理解。LLM 的丰富响应包括附加信息，如布局、详细描述、背景场景和负面提示¹¹1一个详细的样本可以在我们的补充材料中找到。这一步骤至关重要，因为它旨在减轻我们希望克服的主要挑战：当前扩散模型中的训练数据偏差。通过利用预训练的
    LLM 进行逻辑推理和推测，我们旨在弥补这些偏差留下的空白，确保更准确和连贯的图像生成过程。
- en: 4.2 Semantic Alignment Assessment
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 语义对齐评估
- en: 'As we proceed with generating images using the diffusion model using the details
    generated by the previous step, there is a critical challenge: the description
    lists generated by LLMs for one object usually overlook the relationships among
    them. For example, interpretations of “a lion” could range from being “unaware
    and asleep” to “frightened and trying to escape.” Although both depictions are
    valid, descriptions such as “unaware” and “trying to escape” can lead to conflicting
    interpretations, thus complicating the image generation process.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续使用扩散模型生成图像时，使用前一步生成的细节，会遇到一个关键挑战：LLMs 生成的关于某一对象的描述列表通常忽视了它们之间的关系。例如，对“狮子”的解释可以从“无意识地睡着”到“惊恐地试图逃跑”不等。尽管这两种描述都是有效的，但“无意识”和“试图逃跑”这样的描述可能导致冲突的解释，从而使图像生成过程复杂化。
- en: To overcome this challenge, we introduce the Semantic Alignment Assessment (SAA)
    module. This module calculates the relevance between different object vectors,
    thereby selecting the candidate description that best fits the current scenario.
    By conducting the cosine similarity among different descriptions, we can navigate
    the complexities introduced by the LLM’s output, selecting the most compatible
    details for the diffusion model. This step is crucial for maintaining the coherence
    and accuracy of the generated images, highlighting our novel approach to mitigating
    the risk of conflicting descriptions. Through this module, we ensure textual precision
    and compatibility, and provide clear, consistent instructions for the subsequent
    diffusion model to generate visually coherent representations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一挑战，我们引入了语义对齐评估（SAA）模块。该模块计算不同对象向量之间的相关性，从而选择最适合当前场景的候选描述。通过对不同描述进行余弦相似度计算，我们可以导航
    LLM 输出引入的复杂性，为扩散模型选择最兼容的细节。这一步对于保持生成图像的一致性和准确性至关重要，突显了我们在减轻冲突描述风险方面的新方法。通过该模块，我们确保文本的准确性和兼容性，并为后续的扩散模型提供明确、一致的指令，以生成视觉上连贯的表现。
- en: '![Refer to caption](img/d457cfeedc263c00d942c960235c0c85.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d457cfeedc263c00d942c960235c0c85.png)'
- en: 'Figure 4: Comprehensive Image Synthesis. In step 1, utilizing the prompt refined
    by the SAA module, the frozen stable diffusion model generates each foreground
    object independently. During the denoising phase, the cross-attention map is extracted
    and saved for Guidance Constraint in the next step. In addition, a Suppression
    Constraint is also added in step 2 to minimize influence between different objects.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：综合图像合成。在步骤1中，利用由SAA模块精炼的提示，冻结的稳定扩散模型独立生成每个前景物体。在去噪阶段，提取并保存交叉注意力图以供下一步的引导约束。此外，步骤2中还添加了抑制约束，以最小化不同物体之间的影响。
- en: 4.3 Comprehensive Image Synthesis
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 综合图像合成
- en: 'In the second stage of our proposed RFNet, following LMD [[15](#bib.bib15)],
    we propose a two-step generation process for imaginative and abstract concepts.
    As shown in [Fig. 4](#S4.F4 "In 4.2 Semantic Alignment Assessment ‣ 4 Our Proposed
    Realistic-Fantasy Network ‣ The Fabrication of Reality and Fantasy: Scene Generation
    with LLM-Assisted Prompt Interpretation"), in the first step, we focus on generating
    each foreground object with comprehensive details. In the second step, we integrate
    the objects generated in the first step into corresponding background derived
    from the initial prompt. This structured approach ensures a cohesive integration
    of detailed foreground objects into a contextually relevant background, enhancing
    the overall effectiveness of our framework.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们提出的RFNet的第二阶段，参照LMD [[15](#bib.bib15)]，我们提出了一个针对富有想象力和抽象概念的两步生成过程。如[图4](#S4.F4
    "In 4.2 Semantic Alignment Assessment ‣ 4 Our Proposed Realistic-Fantasy Network
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation")所示，在第一步中，我们专注于生成每个前景物体的详细信息。在第二步中，我们将第一步生成的物体整合到从初始提示中得出的相应背景中。这种结构化的方法确保了详细的前景物体与上下文相关的背景的有机整合，从而提升了我们框架的整体效果。'
- en: 'Step 1: In-Depth Object Generation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1：深入物体生成。
- en: We re-organize the SAA description lists to consider both the layout of specific
    objects and their descriptions. By concatenating the background prompt with the
    target object and its relevant descriptions, we set up the input prompt as “[background
    prompt] with [target object], [descriptions]” (e.g., “A grassland scene with a
    rat, roaring, with big mouth and sharp teeth, leap out at…”). Following LMD [[15](#bib.bib15)],
    the initial latent representation for each target object is fixed to facilitate
    the fusion of various objects into a cohesive background scene.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重新组织了SAA描述列表，以考虑特定物体的布局及其描述。通过将背景提示与目标物体及其相关描述连接起来，我们将输入提示设置为“[背景提示] 与 [目标物体]，[描述]”（例如，“一片草地场景中出现一只老鼠，咆哮，张着大嘴和尖锐的牙齿，跳出来……”）。参照LMD
    [[15](#bib.bib15)]，每个目标物体的初始潜在表示被固定，以便将各种物体融合成一个连贯的背景场景。
- en: 'During generation, the diffusion model uses cross-attention layers to manage
    the influence of textual information on the visual output, allowing precise control
    over image details. The cross-attention map’s constraint function integrates objects
    within the bounding box by enhancing cross-attention inside the box for accurate
    object representation while minimizing it outside the box. This function guides
    the update of the noised latent vector during denoising to ensure spatial conditions
    match predefined specifications. The constraint function is defined as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成过程中，扩散模型使用交叉注意力层来管理文本信息对视觉输出的影响，从而精确控制图像细节。交叉注意力图的约束函数通过增强盒内的交叉注意力来整合边界框内的物体，以确保准确的物体表示，同时最小化盒外的影响。此函数引导去噪过程中的噪声潜在向量更新，以确保空间条件符合预定义规范。约束函数定义为：
- en: '|  | $\mathcal{L}_{obj}(\textbf{A},i,v)=[1-\rm{Topk}_{u}(\textbf{A}_{uv}\cdot\textbf{m}_{i})]+[\rm{Topk}_{u}(\textbf{A}_{uv}\cdot(1-\textbf{m}_{i}))],$
    |  | (1) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{obj}(\textbf{A},i,v)=[1-\rm{Topk}_{u}(\textbf{A}_{uv}\cdot\textbf{m}_{i})]+[\rm{Topk}_{u}(\textbf{A}_{uv}\cdot(1-\textbf{m}_{i}))],$
    |  | (1) |'
- en: 'where $\textbf{m}_{i}$ denotes the binary mask of the bounding box associated
    with object $i$, performing element-wise multiplication over the cross-attention
    map A. The cross-attention map A is aggregated by summing the contributions across
    all layers. For object $i$, the operation $\rm{Topk}_{u}$ computes the mean of
    the top-k values within the spatial dimension $u$. Prior to each denoising step,
    the latent is refined by minimizing the constraint function:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{m}_{i}$ 表示与对象 $i$ 相关的边界框的二进制掩码，对交叉注意力图 A 进行逐元素乘法操作。交叉注意力图 A 通过在所有层之间求和来聚合。对于对象
    $i$，操作 $\rm{Topk}_{u}$ 计算空间维度 $u$ 中前 k 个值的平均值。在每个去噪步骤之前，潜在变量通过最小化约束函数来优化：
- en: '|  | $z_{t}^{{}^{\prime}}\leftarrow z_{t}-\alpha\cdot\nabla_{z_{t}}\sum_{v\in
    V}\mathcal{L}(\textbf{A},i,v),$ |  | (2) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{t}^{{}^{\prime}}\leftarrow z_{t}-\alpha\cdot\nabla_{z_{t}}\sum_{v\in
    V}\mathcal{L}(\textbf{A},i,v),$ |  | (2) |'
- en: '|  | $z_{t-1}\leftarrow DiffusionStep(z_{t}^{{}^{\prime}},\mathcal{P}^{(i)}),$
    |  | (3) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{t-1}\leftarrow DiffusionStep(z_{t}^{{}^{\prime}},\mathcal{P}^{(i)}),$
    |  | (3) |'
- en: where $\alpha$ denotes the hyperparameter that controls the magnitude of the
    gradient update, and $V$ contains the set of token indices for the target object
    in the prompt. As in the diffusion step, the updated $z_{t}^{{}^{\prime}}$ along
    with the modified prompt $\mathcal{P}^{(i)}$ of object $i$, are served as the
    inputs to the diffusion model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 表示控制梯度更新幅度的超参数，$V$ 包含提示中目标对象的标记索引集合。如同扩散步骤中，更新后的 $z_{t}^{{}^{\prime}}$
    以及对象 $i$ 的修改提示 $\mathcal{P}^{(i)}$ 作为扩散模型的输入。
- en: After the generation, the cross-attention map derived from each target object
    is then converted into a saliency mask. This mask is applied to the latent representation
    of the target object through element-wise multiplication at each step of the denoising
    process. Both the cross-attention map and the masked latent representation of
    the target object between each denoising step are transmitted to the next step
    for background integration.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 生成后，从每个目标对象派生的交叉注意力图会转换为显著性掩码。这个掩码通过逐元素乘法应用到目标对象的潜在表示上，并在每个去噪过程中使用。每个去噪步骤中的交叉注意力图和目标对象的掩蔽潜在表示都会传递到下一个步骤以进行背景融合。
- en: 'Step 2: Seamless Background Integration. This step involves fusing the generated
    results with the background while preserving the high-quality generation achieved
    in the first step. To accomplish this, we first replace the generated latent $z_{t}^{{}^{\prime}}$
    with the masking latent $z_{t}^{(masked,i)}$ for each object $i$:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2：无缝背景融合。此步骤涉及将生成的结果与背景融合，同时保持第一步中实现的高质量生成。为此，我们首先将生成的潜在变量 $z_{t}^{{}^{\prime}}$
    替换为每个对象 $i$ 的掩蔽潜在变量 $z_{t}^{(masked,i)}$：
- en: '|  | $z_{t}^{{}^{\prime}}\leftarrow Replacement(z_{t}^{{}^{\prime}},z_{t}^{(masked,i)}),\
    \forall i.$ |  | (4) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{t}^{{}^{\prime}}\leftarrow Replacement(z_{t}^{{}^{\prime}},z_{t}^{(masked,i)}),\
    \forall i.$ |  | (4) |'
- en: Following the approach established in Step 1, the initial latent representation
    $z_{T}^{{}^{\prime}}$ is also fixed with the initial latent of each target object
    in the first step. This alignment ensures a seamless integration of the object
    into the background. The replacement approach is performed within timestep $rT$,
    where $r\in[0,1]$, reserving principles from LMD [[15](#bib.bib15)]. According
    to LMD, the diffusion model determines the position of the objects in the early
    denoising steps, while adjusting details in the later steps. This helps us to
    preserve exceptional control over the layout.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据步骤 1 中建立的方法，初始潜在表示 $z_{T}^{{}^{\prime}}$ 也与第一步中每个目标对象的初始潜在变量对齐。这种对齐确保了对象与背景的无缝融合。替换方法在时间步
    $rT$ 内进行，其中 $r\in[0,1]$，遵循 LMD 的原则 [[15](#bib.bib15)]。根据 LMD，扩散模型在早期去噪步骤中确定对象的位置，同时在后期步骤中调整细节。这有助于我们保持对布局的卓越控制。
- en: 'Furthermore, we incorporate a specialized constraint function designed to enhance
    the integration of generated objects with their background, distinguished by two
    key components: guidance constraint and suppression constraint. As shown in [Fig. 4](#S4.F4
    "In 4.2 Semantic Alignment Assessment ‣ 4 Our Proposed Realistic-Fantasy Network
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), the guidance constraint is engineered to reduce the cross-attention
    within each bounding box relative to the original object’s attention. With the
    purpose of seamlessly integrating with the detailed object generated in the first
    step. Conversely, the suppression constraint works to minimize cross-attention
    outside the bounding box, thereby mitigating interference among multiple objects
    when processed together, as illustrated in [Eq. 5](#S4.E5 "In 4.3 Comprehensive
    Image Synthesis ‣ 4 Our Proposed Realistic-Fantasy Network ‣ The Fabrication of
    Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation").
    These constraint functions mark a departure from conventional methods that predominantly
    use loss to fix the layout.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还引入了一个专门的约束函数，旨在增强生成对象与背景的融合，该函数由两个关键组件组成：指导约束和抑制约束。如[图4](#S4.F4 "在4.2语义对齐评估
    ‣ 4 我们提出的现实-幻想网络 ‣ 现实与幻想的构建：基于LLM辅助提示解析的场景生成")所示，指导约束的设计目的是减少每个边界框内相对于原始对象注意力的交叉注意力，以便与第一步生成的详细对象无缝融合。相反，抑制约束则致力于最小化边界框外的交叉注意力，从而在多个对象一起处理时减轻干扰，如[公式5](#S4.E5
    "在4.3综合图像合成 ‣ 4 我们提出的现实-幻想网络 ‣ 现实与幻想的构建：基于LLM辅助提示解析的场景生成")所示。这些约束函数标志着传统方法的离经叛道，传统方法主要使用损失来修正布局。
- en: '|  | $\mathcal{L}_{bg}(\textbf{A}^{{}^{\prime}},\textbf{A}^{(i)},i,v)=\underbrace{\beta\cdot\sum_{u}\left&#124;(\textbf{A}_{uv}^{{}^{\prime}}-\textbf{A}_{uv}^{(i)})\cdot\textbf{m}_{i}\right&#124;}_{\text{guidance
    constraint}}+\underbrace{\gamma\cdot\mathrm{Topk}_{u}(\textbf{A}_{uv}^{{}^{\prime}}\cdot(1-\textbf{m}_{i}))}_{\text{suppression
    constraint}},\ \forall i,$ |  | (5) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{bg}(\textbf{A}^{{}^{\prime}},\textbf{A}^{(i)},i,v)=\underbrace{\beta\cdot\sum_{u}\left&#124;(\textbf{A}_{uv}^{{}^{\prime}}-\textbf{A}_{uv}^{(i)})\cdot\textbf{m}_{i}\right&#124;}_{\text{指导约束}}+\underbrace{\gamma\cdot\mathrm{Topk}_{u}(\textbf{A}_{uv}^{{}^{\prime}}\cdot(1-\textbf{m}_{i}))}_{\text{抑制约束}},\
    \forall i,$ |  | (5) |'
- en: 'where $\textbf{A}^{{}^{\prime}}$ represents the cross-attention map post the
    substitution procedure and $\textbf{A}^{(i)}$ is the cross-attention map of object
    $i$ extracted from the diffusion step in [Eq. 3](#S4.E3 "In 4.3 Comprehensive
    Image Synthesis ‣ 4 Our Proposed Realistic-Fantasy Network ‣ The Fabrication of
    Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation").
    The hyperparameters $\beta$ and $\gamma$ indicate the intensity of guidance constraint
    and suppression constraint, respectively. It is noteworthy that adjusting $\beta$
    amplifies the importance of the guidance loss within the overall loss function.
    This adjustment ensures the latent representation is precisely aligned with the
    object generated in the initial phase, thereby guaranteeing a high level of accuracy
    and consistency in the integration process. Upon completion of all denoising steps,
    the latent $z_{0}$ is fed into the decoder to produce the final image.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\textbf{A}^{{}^{\prime}}$ 表示替换程序后的交叉注意力图，$\textbf{A}^{(i)}$ 是[公式3](#S4.E3
    "在4.3综合图像合成 ‣ 4 我们提出的现实-幻想网络 ‣ 现实与幻想的构建：基于LLM辅助提示解析的场景生成")中从扩散步骤中提取的对象$i$的交叉注意力图。超参数$\beta$和$\gamma$分别表示指导约束和抑制约束的强度。值得注意的是，调整$\beta$会放大指导损失在整体损失函数中的重要性。这个调整确保潜在表示与初始阶段生成的对象精确对齐，从而保证高水平的准确性和一致性。在所有去噪步骤完成后，潜在的$z_{0}$被输入到解码器中以生成最终图像。
- en: Our strategy focuses on maintaining the integrity and coherence of the foreground
    objects generated, emphasizing the preservation of their quality and interaction
    with the background. By doing so, the generated visual elements are fidelity-aware
    and contextually appropriate.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略专注于保持生成前景对象的完整性和一致性，强调其质量和与背景的互动的保存。通过这样做，生成的视觉元素既保真又符合上下文。
- en: 5 Experiments
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Implementation Details
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实施细节
- en: Experimental Setup. In this work, we choose versions 1.4 and 2.1 of Stable Diffusion [[31](#bib.bib31)]
    as the text-to-image baseline model. The number of denoising steps is set as 50
    with a fixed guidance scale of 7.5, and the synthetic images are in a resolution
    of 512 × 512\. All experiments are conducted on the NVIDIA RTX 3090 GPU with 24
    GB memory.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。在这项工作中，我们选择了 Stable Diffusion 的 1.4 和 2.1 版本 [[31](#bib.bib31)] 作为文本到图像的基线模型。去噪步骤的数量设置为
    50，固定的指导尺度为 7.5，合成图像的分辨率为 512 × 512。所有实验均在 NVIDIA RTX 3090 GPU 上进行，具有 24 GB 的内存。
- en: 'Evaluation Metrics. We generate 32 images for each text prompt in RFBench for
    automatic evaluation. We selected the following two metrics: (1) GPT4-CLIP ²²2We
    adopt GPT4-CLIP due to BLIP-CLIP’s [[3](#bib.bib3)] limitations in accurately
    capturing image meanings through generated captions.. By utilizing GPT4 for captioning
    and calculating CLIP text-text cosine similarity, GPT4-CLIP ensures a more precise
    reflection of the intended meanings between images and prompts. (2) GPT4Score.
    Inspired by  [[13](#bib.bib13), [17](#bib.bib17)], we adopt GPT4Score to evaluate
    image alignment with text prompts, where GPT4 rates images on a 0-100 scale based
    on their fidelity to the prompts, enabling precise assessment of model-generated
    visuals against specified criteria³³3The widely recognized metric, CLIPScore [[29](#bib.bib29),
    [10](#bib.bib10)] exhibits limitations in evaluating our task. For detailed examples,
    please see the supplementary materials..'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '评估指标。我们为 RFBench 中的每个文本提示生成 32 张图像以进行自动评估。我们选择了以下两个指标: (1) GPT4-CLIP²²2 我们采用
    GPT4-CLIP 是因为 BLIP-CLIP [[3](#bib.bib3)] 在通过生成的标题准确捕捉图像含义方面的局限性。通过利用 GPT4 进行标题生成并计算
    CLIP 文本-文本余弦相似度，GPT4-CLIP 确保了图像与提示之间意图的更准确反映。 (2) GPT4Score。受 [[13](#bib.bib13)、[17](#bib.bib17)]
    的启发，我们采用 GPT4Score 来评估图像与文本提示的对齐情况，其中 GPT4 根据图像对提示的忠实度在 0-100 的范围内进行评分，从而精确评估模型生成的视觉效果与指定标准的符合程度³³3
    广泛认可的指标 CLIPScore [[29](#bib.bib29)、[10](#bib.bib10)] 在评估我们的任务时存在局限性。详细示例请参见补充材料。.'
- en: 'Table 2: Benchmarking on GPT4-CLIP and GPT4Score. R & A, C & I, Avg represent
    Realistic & Analytical category, Creativity & Imagination category, and average
    of both categories, respectively. The red text indicates the improvement ratio
    of our method compared to Stable Diffusion.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 在 GPT4-CLIP 和 GPT4Score 上的基准测试。R & A、C & I、平均分别代表现实与分析类别、创造力与想象力类别以及两个类别的平均值。红色文本表示我们的方法相较于
    Stable Diffusion 的改进比例。'
- en: '| Model | GPT4-CLIP | GPT4Score |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GPT4-CLIP | GPT4Score |'
- en: '| R & A | C & I | Avg | R & A | C & I | Avg |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| R & A | C & I | 平均 | R & A | C & I | 平均 |'
- en: '| Stable Diffusion [[31](#bib.bib31)] | 0.573 | 0.552 | 0.561 | 0.667 | 0.440
    | 0.541 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Stable Diffusion [[31](#bib.bib31)] | 0.573 | 0.552 | 0.561 | 0.667 | 0.440
    | 0.541 |'
- en: '| MultiDiffusion [[2](#bib.bib2)] | 0.510 | 0.510 | 0.510 | 0.517 | 0.493 |
    0.504 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MultiDiffusion [[2](#bib.bib2)] | 0.510 | 0.510 | 0.510 | 0.517 | 0.493 |
    0.504 |'
- en: '| Attend and Excite [[3](#bib.bib3)] | 0.523 | 0.560 | 0.546 | 0.633 | 0.520
    | 0.570 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Attend and Excite [[3](#bib.bib3)] | 0.523 | 0.560 | 0.546 | 0.633 | 0.520
    | 0.570 |'
- en: '| LLM-groundedDiffusion [[15](#bib.bib15)] | 0.457 | 0.536 | 0.501 | 0.550
    | 0.600 | 0.578 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| LLM-groundedDiffusion [[15](#bib.bib15)] | 0.457 | 0.536 | 0.501 | 0.550
    | 0.600 | 0.578 |'
- en: '| BoxDiff [[37](#bib.bib37)] | 0.532 | 0.553 | 0.543 | 0.583 | 0.520 | 0.548
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| BoxDiff [[37](#bib.bib37)] | 0.532 | 0.553 | 0.543 | 0.583 | 0.520 | 0.548
    |'
- en: '| SDXL [[26](#bib.bib26)] | 0.536 | 0.619 | 0.582 | 0.567 | 0.587 | 0.578 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SDXL [[26](#bib.bib26)] | 0.536 | 0.619 | 0.582 | 0.567 | 0.587 | 0.578 |'
- en: '| RFNet(ours) | 0.587 (2%$\uparrow$) | 0.623 (13%$\uparrow$) | 0.607 (8%$\uparrow$)
    | 0.833 (25%$\uparrow$) | 0.627(43%$\uparrow$) | 0.719 (33%$\uparrow$) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| RFNet（我们的） | 0.587 (2%$\uparrow$) | 0.623 (13%$\uparrow$) | 0.607 (8%$\uparrow$)
    | 0.833 (25%$\uparrow$) | 0.627 (43%$\uparrow$) | 0.719 (33%$\uparrow$) |'
- en: Comparison with Existing Methods. We benchmark our proposed RFNet against various
    open-source scene generation methods, including Stable Diffusion [[31](#bib.bib31)],
    Attend and Excite [[3](#bib.bib3)], LMD [[15](#bib.bib15)], BoxDiff [[37](#bib.bib37)],
    MultiDiffusion [[2](#bib.bib2)], and SDXL [[26](#bib.bib26)]. Notably, all methods,
    including ours, utilize Stable Diffusion 2.1 as the foundational model, ensuring
    a fair comparison.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有方法的比较。我们将我们提出的 RFNet 与各种开源场景生成方法进行了基准测试，包括 Stable Diffusion [[31](#bib.bib31)]、Attend
    and Excite [[3](#bib.bib3)]、LMD [[15](#bib.bib15)]、BoxDiff [[37](#bib.bib37)]、MultiDiffusion
    [[2](#bib.bib2)] 和 SDXL [[26](#bib.bib26)]。值得注意的是，包括我们的方法在内的所有方法都使用了 Stable Diffusion
    2.1 作为基础模型，确保了公平比较。
- en: 5.2 Quantitative Evaluation
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 定量评估
- en: 'Evaluation on RFBench. As evidenced in [Tab. 5](#S4.T5 "In D Human Evaluation
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), our approach significantly outperforms other methods for both
    Realistic & Analytical and Creativity & Imagination tasks. For Realistic & Analytical
    task, our method seamlessly integrates LLM-based insights, achieving a remarkable
    accuracy improvement. Unlike Attend-and-excite, which focuses on semantic guidance,
    our method ensures precise adherence to detailed and complex prompt requirements.
    For the Creativity & Imagination, which demands high degrees of creativity and
    abstract conceptualization, our method outperforms others by not only adhering
    to the imaginative aspects of prompts but also maintaining coherent structure
    and contexts. For instance, SDXL, while adept at high-resolution image synthesis,
    occasionally lacks in capturing the nuanced creativity intended in prompts; our
    method fills this gap effectively. Similarly, LMD, though enhancing prompt understanding
    through LLMs, sometimes struggles with the scientific reasoning required for Realistic&
    Analytical tasks.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在RFBench上的评估。如[表5](#S4.T5 "在D人的评估 ‣ 现实与幻想的编造：通过LLM辅助提示解释生成场景")所示，我们的方法在现实主义与分析类和创造力与想象力任务上明显优于其他方法。对于现实主义与分析类任务，我们的方法无缝集成了基于LLM的洞察力，实现了显著的准确性提升。与专注于语义指导的Attend-and-excite不同，我们的方法确保了对详细且复杂提示要求的精确遵守。在创造力与想象力任务中，我们的方法不仅遵守了提示的想象性方面，还保持了连贯的结构和上下文。例如，虽然SDXL在高分辨率图像合成方面表现出色，但有时难以捕捉提示中的细腻创造力；我们的方法有效填补了这一空白。同样，尽管LMD通过LLM提升了提示理解，但有时在现实主义与分析类任务中难以应对所需的科学推理。
- en: Notably, for Realistic & Analytical category, our approach shows a 61% performance
    increase over MultiDiffusion on GPT4Score. Meanwhile, in Creativity & Imagination
    task, we observe a substantial enhancement, outperforming Stable Diffusion by
    over 43%. In light of the above, our method is unique in its ability to bridge
    the gap between realistic reasoning and imagination, creating a new benchmark
    for text-to-image generation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，对于现实主义与分析类任务，我们的方法在GPT4Score上显示出比MultiDiffusion高61%的性能提升。与此同时，在创造力与想象力任务中，我们观察到显著的提升，表现超过Stable
    Diffusion超过43%。鉴于上述情况，我们的方法在弥合现实推理与想象力之间的差距方面具有独特性，为文本到图像生成设立了新的基准。
- en: 'Table 3: GPT4Score comparison with Imagen on DrawBench subset.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：GPT4Score与Imagen在DrawBench子集上的比较。
- en: '| Prompt | Imagen | Ours |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | Imagen | 我们的方法 |'
- en: '| A bird scaring a scarecrow | 0.069 | 0.275 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 吓唬稻草人的鸟 | 0.069 | 0.275 |'
- en: '| A blue coloured pizza | 0.425 | 0.125 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 蓝色的披萨 | 0.425 | 0.125 |'
- en: '| A fish eating a pelican | 0.000 | 0.000 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 一条鱼吃鹈鹕 | 0.000 | 0.000 |'
- en: '| A horse riding an astronaut | 0.000 | 0.000 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 一匹马骑在宇航员身上 | 0.000 | 0.000 |'
- en: '| A panda making latte art | 0.050 | 0.250 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 一只熊猫制作拿铁艺术 | 0.050 | 0.250 |'
- en: '| A pizza cooking an oven | 0.700 | 0.831 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 一块披萨在烤箱里烹饪 | 0.700 | 0.831 |'
- en: '| A shark in the desert | 0.194 | 0.713 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 沙漠中的鲨鱼 | 0.194 | 0.713 |'
- en: '| An elephant under the sea | 0.300 | 0.900 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 海底的大象 | 0.300 | 0.900 |'
- en: '| Hovering cow abducting aliens | 0.025 | 0.144 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 悬浮的牛绑架外星人 | 0.025 | 0.144 |'
- en: '| Rainbow coloured penguin | 0.394 | 0.519 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 彩虹色的企鹅 | 0.394 | 0.519 |'
- en: 'Evaluation on DrawBench. We also evaluate our method on DrawBench [[32](#bib.bib32)],
    a comprehensive and challenging benchmark for text-to-image models. Similar to
    us, DrawBench also includes some Creativity & Imagination prompts, and we evaluate
    our method with Imagen [[32](#bib.bib32)] on these prompts. As shown in [Tab. 3](#S5.T3
    "In 5.2 Quantitative Evaluation ‣ 5 Experiments ‣ The Fabrication of Reality and
    Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation"), our approach
    significantly outperforms Imagen on most prompt settings, demonstrating the generalization
    ability of our model.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在DrawBench上的评估。我们还在DrawBench[[32](#bib.bib32)]上评估了我们的方法，这是一个综合且具有挑战性的文本到图像模型基准。与我们类似，DrawBench也包括一些创造力与想象力提示，我们将我们的方法与Imagen[[32](#bib.bib32)]在这些提示上进行比较。如[表3](#S5.T3
    "在5.2定量评估 ‣ 5次实验 ‣ 现实与幻想的编造：通过LLM辅助提示解释生成场景")所示，我们的方法在大多数提示设置中明显优于Imagen，展示了我们模型的泛化能力。
- en: 5.3 Qualitative Evaluation
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 质量评估
- en: '![Refer to caption](img/99241f26add22f86eece1409d8d0d51d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/99241f26add22f86eece1409d8d0d51d.png)'
- en: 'Figure 5: Qualitative comparison on RFBench. The compared models include (a)
    Stable Diffusion, (b) MultiDiffusion, (c) Attend and Excite, (d) LMD, (e) BoxDiff,
    (f) SDXL, (g) Ours (Best viewed in color and zoom in. More samples can be found
    in our supplementary material.)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：RFBench 上的定性比较。比较的模型包括 (a) 稳定扩散, (b) 多重扩散, (c) 注意和激发, (d) LMD, (e) BoxDiff,
    (f) SDXL, (g) 我们的方法（最佳效果请在彩色和放大查看。更多样本可以在我们的补充材料中找到。）
- en: 'In the qualitative comparison of text-guided image generation, we select some
    advanced baseline methods, including Attend-and-excite [[3](#bib.bib3)], BoxDiff [[37](#bib.bib37)],
    LMD [[15](#bib.bib15)], MultiDiffusion [[2](#bib.bib2)], and SDXL [[26](#bib.bib26)].
    Attend-and-excite focuses on enhancing the semantic understanding of prompts through
    attention mechanisms, while BoxDiff introduces a novel approach to text-to-image
    synthesis with box-constrained diffusion without the need for explicit training.
    MultiDiffusion proposes a method for fusing multiple diffusion paths to achieve
    greater control over the image generation process, and SDXL aims at improving
    the capabilities of latent diffusion models for synthesizing high-resolution images.
    As shown in Fig. [5](#S5.F5 "Figure 5 ‣ 5.3 Qualitative Evaluation ‣ 5 Experiments
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), our method, produces more precise editing results than the aforementioned
    methods. This is attributed to our In-Depth Object Generation and Seamless Background
    Integration strategy. It ensures outstanding fidelity in outcomes and flawlessly
    retains the semantic structure of the source image, highlighting our approach’s
    superior capability in complex editing tasks.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在文本引导的图像生成的定性比较中，我们选择了一些先进的基线方法，包括 Attend-and-excite [[3](#bib.bib3)], BoxDiff [[37](#bib.bib37)],
    LMD [[15](#bib.bib15)], MultiDiffusion [[2](#bib.bib2)], 和 SDXL [[26](#bib.bib26)]。Attend-and-excite
    通过注意机制增强了对提示的语义理解，而 BoxDiff 引入了一种新的文本到图像合成方法，通过箱约束扩散实现，而无需显式训练。MultiDiffusion
    提出了一种融合多个扩散路径以实现对图像生成过程的更大控制的方法，SDXL 旨在提高潜在扩散模型在合成高分辨率图像方面的能力。如图 [5](#S5.F5 "Figure
    5 ‣ 5.3 Qualitative Evaluation ‣ 5 Experiments ‣ The Fabrication of Reality and
    Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation") 所示，我们的方法在编辑结果的精确性方面优于上述方法。这归功于我们的深入对象生成和无缝背景整合策略。它确保了结果的卓越忠实度，并完美地保留了源图像的语义结构，突显了我们方法在复杂编辑任务中的卓越能力。'
- en: 5.4 User study
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 用户研究
- en: Through an extensive user study, we benchmarked our model against other methods
    to assess real human preferences for the generated images. Utilizing our newly
    proposed benchmark, the RFBench, we selected a diverse set of 27 prompts and generated
    six images per prompt to ensure a broad representation of the model’s capabilities.
    Detailed feedbacks were collected from 120 participants, evaluating each image
    for visual quality and text prompt fidelity ⁴⁴4Details of survey samples can be
    found in our supplementary material.. These criteria are critical, which measure
    the image’s quality and correctness of semantics in the synthesized image. Participants
    rated images on a scale from {1, 2, 3, 4, 5}, with scores normalized by dividing
    by 5\. We calculated the average score across all images and participants.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过广泛的用户研究，我们将我们的模型与其他方法进行了基准测试，以评估生成图像的真实人类偏好。利用我们新提出的基准测试 RFBench，我们选择了一组多样化的
    27 个提示，并为每个提示生成了六张图像，以确保模型能力的广泛代表性。从 120 位参与者处收集了详细的反馈，对每张图像进行视觉质量和文本提示忠实度的评价 ⁴⁴4详细的调查样本可以在我们的补充材料中找到。这些标准至关重要，衡量图像的质量和合成图像中语义的正确性。参与者按
    {1, 2, 3, 4, 5} 的等级对图像进行评分，分数通过除以 5 进行标准化。我们计算了所有图像和参与者的平均分数。
- en: 'As illustrated in Fig. [6](#S5.F6 "Figure 6 ‣ 5.4 User study ‣ 5 Experiments
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), participants uniformly favored our model’s output, recognizing
    it as superior in both quality and alignment with the textual descriptions.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [6](#S5.F6 "Figure 6 ‣ 5.4 User study ‣ 5 Experiments ‣ The Fabrication
    of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation")
    所示，参与者一致偏爱我们模型的输出，认为其在质量和与文本描述的一致性方面都更优。'
- en: '![Refer to caption](img/de8597446ecfa7e34dd10faf2c12003c.png)![Refer to caption](img/3cd4f402d2c27847e82b211cd7e424b0.png)![Refer
    to caption](img/ab51159eb21167a64daf738074a4d1c7.png)![Refer to caption](img/09010d7494e269e019cac3c1794bcb93.png)![Refer
    to caption](img/ffee3ec22d2c1a0e560f418eee2d8414.png)![Refer to caption](img/6f38e5244f00cd4f0d9aa98aee00418d.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/de8597446ecfa7e34dd10faf2c12003c.png)![参见标题](img/3cd4f402d2c27847e82b211cd7e424b0.png)![参见标题](img/ab51159eb21167a64daf738074a4d1c7.png)![参见标题](img/09010d7494e269e019cac3c1794bcb93.png)![参见标题](img/ffee3ec22d2c1a0e560f418eee2d8414.png)![参见标题](img/6f38e5244f00cd4f0d9aa98aee00418d.png)'
- en: 'Figure 6: Comparison between our method and other advanced methods on RFBench.
    The image-text Alignment and Fidelity of our method are highly preferred by users.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：我们的方法与其他先进方法在RFBench上的比较。我们方法的图像-文本对齐和保真度受到用户的高度偏好。
- en: 5.5 Ablation study
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 消融研究
- en: 'Table 4: Ablation studies on various components on RFBench.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：RFBench上各组件的消融研究。
- en: '| SAA | guidance | suppression | GPT4Score |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| SAA | 指导 | 抑制 | GPT4Score |'
- en: '|  |  |  | 0.295 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 0.295 |'
- en: '|  |  | $\checkmark$ | 0.407 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\checkmark$ | 0.407 |'
- en: '|  | $\checkmark$ |  | 0.554 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\checkmark$ |  | 0.554 |'
- en: '|  | $\checkmark$ | $\checkmark$ | 0.572 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\checkmark$ | $\checkmark$ | 0.572 |'
- en: '| $\checkmark$ | $\checkmark$ | $\checkmark$ | 0.719 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| $\checkmark$ | $\checkmark$ | $\checkmark$ | 0.719 |'
- en: 'Impact of Various Constraints. To validate the impact of guidance constraint
    and suppression constraint, we perform ablation studies on different combinations
    of constraints, and the results are listed in [Tab. 4](#S5.T4 "In 5.5 Ablation
    study ‣ 5 Experiments ‣ The Fabrication of Reality and Fantasy: Scene Generation
    with LLM-Assisted Prompt Interpretation"). As shown, the baseline model (Stable
    Diffusion) achieves a 0.295 in terms of GPT4Score without any constraints. As
    guidance constraint and suppression constraint work complementary to restrict
    the cross-attention of objects inside the conditional boxes, a higher GPT4Score
    of 0.572 is achieved on the generated images. Both proposed constraints are effective
    in controlling image quality and layout of synthesized foreground objects.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 各种约束的影响。为了验证指导约束和抑制约束的影响，我们对不同约束组合进行了消融研究，结果列在[表4](#S5.T4 "在5.5消融研究 ‣ 5 实验 ‣
    现实与幻想的制造：基于LLM的提示解释的场景生成")中。如所示，基线模型（Stable Diffusion）在没有任何约束的情况下，GPT4Score为0.295。由于指导约束和抑制约束在限制条件框内部物体的交叉注意力方面相互补充，因此生成图像的GPT4Score提高到0.572。提出的两个约束在控制图像质量和合成前景物体布局方面均有效。
- en: 'Impact of Semantic Alignment Assessment (SAA) Module. As aforementioned, using
    conflict descriptions in the denoise step may potentially affect image synthesis.
    The quantitative evaluation is presented in [Tab. 4](#S5.T4 "In 5.5 Ablation study
    ‣ 5 Experiments ‣ The Fabrication of Reality and Fantasy: Scene Generation with
    LLM-Assisted Prompt Interpretation"). In the absence of SAA, the model attains
    a GPT4Score of 0.572\. Similarly, with SAA, the model reaches a GPT4Score of 0.719\.
    This indicates a lack of consistency between the semantics generated in the images
    and the provided text prompts, leading to a reduction in image quality. It is
    important to note that the inclusion of SAA significantly enhances the clarity
    of the images obtained. One visual illustration can be found in  [Fig. 7](#S5.F7
    "In 5.5 Ablation study ‣ 5 Experiments ‣ The Fabrication of Reality and Fantasy:
    Scene Generation with LLM-Assisted Prompt Interpretation").'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 语义对齐评估（SAA）模块的影响。如前所述，在去噪步骤中使用冲突描述可能会对图像合成产生潜在影响。定量评估见[表4](#S5.T4 "在5.5消融研究
    ‣ 5 实验 ‣ 现实与幻想的制造：基于LLM的提示解释的场景生成")。在没有SAA的情况下，模型的GPT4Score为0.572。相似地，使用SAA时，模型的GPT4Score达到了0.719。这表明图像中生成的语义与提供的文本提示之间缺乏一致性，从而导致图像质量下降。需要注意的是，SAA的加入显著提高了图像的清晰度。一个视觉插图可以在[图7](#S5.F7
    "在5.5消融研究 ‣ 5 实验 ‣ 现实与幻想的制造：基于LLM的提示解释的场景生成")中找到。
- en: '![Refer to caption](img/ff1b7552f48a4f17512a2a3cfa699183.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ff1b7552f48a4f17512a2a3cfa699183.png)'
- en: 'Figure 7: The illustration of final generated results of minimum (min) and
    maximum (max) similarity descriptions during the SAA. It can be observed that
    prompts with higher similarity yield images of higher quality, which is significantly
    better than the one with the lowest similarity.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：SAA过程中最小（min）和最大（max）相似度描述的最终生成结果的插图。可以观察到，相似度更高的提示生成了更高质量的图像，这比相似度最低的图像显著更好。
- en: 6 Conclusion and Future Work
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: 'In this research, we present a novel challenge: generating scenes that blend
    reality and fantasy. We investigate the capacity of diffusion models to create
    visuals from prompts that demand high levels of creativity or specific knowledge.
    Noting the lack of a specific evaluation mechanism for such tasks, we establish
    the Realistic-Fantasy Benchmark (RFBench), combining elements of both realistic
    and imaginary scenarios. To address the task of generating realistic and fantastical
    scenes, we introduce a unique, training-free, two-tiered method, Realistic-Fantasy
    Network (RFNet), that combines diffusion models with large language models (LLMs).
    Our approach, evaluated through the RFBench using thorough human assessments and
    GPT-based compositional evaluations, has proven to be superior to existing cutting-edge
    techniques. Given the novelty of our task, future research could develop additional
    evaluation metrics beyond those used in this study, enhancing the assessment of
    generated scenes.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们提出了一个新的挑战：生成现实与幻想相结合的场景。我们探讨了扩散模型在从需要高创意或特定知识的提示中创建视觉效果的能力。鉴于缺乏针对此类任务的特定评估机制，我们建立了现实-幻想基准（RFBench），结合了现实和虚构场景的元素。为了解决生成现实和幻想场景的任务，我们引入了一种独特的、无训练的两层方法，现实-幻想网络（RFNet），它将扩散模型与大型语言模型（LLMs）结合起来。我们的方式通过RFBench进行评估，采用了详尽的人类评估和基于GPT的组合评估，已证明优于现有的前沿技术。鉴于我们任务的创新性，未来的研究可以开发额外的评估指标，超越本研究中使用的指标，进一步提升生成场景的评估。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Anciukevičius, T., Xu, Z., Fisher, M., Henderson, P., Bilen, H., Mitra,
    N.J., Guerrero, P.: Renderdiffusion: Image diffusion for 3d reconstruction, inpainting
    and generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition. pp. 12608–12618 (2023)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Anciukevičius, T., Xu, Z., Fisher, M., Henderson, P., Bilen, H., Mitra,
    N.J., Guerrero, P.: Renderdiffusion：用于3D重建、修复和生成的图像扩散。在：IEEE/CVF计算机视觉与模式识别会议论文集。第12608–12618页（2023）'
- en: '[2] Bar-Tal, O., Yariv, L., Lipman, Y., Dekel, T.: Multidiffusion: Fusing diffusion
    paths for controlled image generation. In: International Conference on Machine
    Learning (2023)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Bar-Tal, O., Yariv, L., Lipman, Y., Dekel, T.: Multidiffusion：融合扩散路径以实现受控图像生成。在：国际机器学习大会（2023）'
- en: '[3] Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., Cohen-Or, D.: Attend-and-excite:
    Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions
    on Graphics (TOG) 42(4), 1–10 (2023)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., Cohen-Or, D.: 注意力激励：基于注意力的文本到图像扩散模型的语义指导。ACM图形学交易（TOG）
    42(4), 1–10 (2023)'
- en: '[4] Feng, W., He, X., Fu, T.J., Jampani, V., Akula, A.R., Narayana, P., Basu,
    S., Wang, X.E., Wang, W.Y.: Training-free structured diffusion guidance for compositional
    text-to-image synthesis. In: International Conference on Learning Representations
    (2023), [https://openreview.net/forum?id=PUIqjT4rzq7](https://openreview.net/forum?id=PUIqjT4rzq7)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Feng, W., He, X., Fu, T.J., Jampani, V., Akula, A.R., Narayana, P., Basu,
    S., Wang, X.E., Wang, W.Y.: 无需训练的结构化扩散指导用于组合文本到图像合成。在：国际学习表示大会（2023），[https://openreview.net/forum?id=PUIqjT4rzq7](https://openreview.net/forum?id=PUIqjT4rzq7)'
- en: '[5] Feng, W., Zhu, W., Fu, T.j., Jampani, V., Akula, A., He, X., Basu, S.,
    Wang, X.E., Wang, W.Y.: Layoutgpt: Compositional visual planning and generation
    with large language models. In: Advances in Neural Information Processing Systems.
    vol. 36 (2024)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Feng, W., Zhu, W., Fu, T.j., Jampani, V., Akula, A., He, X., Basu, S.,
    Wang, X.E., Wang, W.Y.: Layoutgpt：使用大型语言模型进行组合视觉规划和生成。在：神经信息处理系统进展。第36卷（2024）'
- en: '[6] Friedrich, F., Brack, M., Struppek, L., Hintersdorf, D., Schramowski, P.,
    Luccioni, S., Kersting, K.: Fair diffusion: Instructing text-to-image generation
    models on fairness. arXiv preprint arXiv:2302.10893 (2023)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Friedrich, F., Brack, M., Struppek, L., Hintersdorf, D., Schramowski, P.,
    Luccioni, S., Kersting, K.: 公平扩散：对文本到图像生成模型的公平性指导。arXiv预印本 arXiv:2302.10893 (2023)'
- en: '[7] Gani, H., Bhat, S.F., Naseer, M., Khan, S., Wonka, P.: Llm blueprint: Enabling
    text-to-image generation with complex and detailed prompts. In: International
    Conference on Learning Representations (2024)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Gani, H., Bhat, S.F., Naseer, M., Khan, S., Wonka, P.: Llm蓝图：通过复杂和详细的提示实现文本到图像生成。在：国际学习表示大会（2024）'
- en: '[8] Golnari, P.A.: Lora-enhanced distillation on guided diffusion models. arXiv
    preprint arXiv:2312.06899 (2023)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Golnari, P.A.: 基于引导扩散模型的Lora增强蒸馏。arXiv预印本 arXiv:2312.06899 (2023)'
- en: '[9] Gong, J., Foo, L.G., Fan, Z., Ke, Q., Rahmani, H., Liu, J.: Diffpose: Toward
    more reliable 3d pose estimation. In: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition. pp. 13041–13051 (2023)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Gong, J., Foo, L.G., Fan, Z., Ke, Q., Rahmani, H., Liu, J.：**Diffpose**：迈向更可靠的3D姿态估计。载于：IEEE/CVF计算机视觉与模式识别会议论文集。第13041–13051页（2023年）'
- en: '[10] Hessel, J., Holtzman, A., Forbes, M., Bras, R.L., Choi, Y.: Clipscore:
    A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718
    (2021)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Hessel, J., Holtzman, A., Forbes, M., Bras, R.L., Choi, Y.：**Clipscore**：一种无参考的图像字幕评价指标。arXiv预印本
    arXiv:2104.08718（2021年）'
- en: '[11] Ho, J., Salimans, T.: Classifier-free diffusion guidance. In: Advances
    in Neural Information Processing Systems Workshop (2021), [https://openreview.net/forum?id=qw8AKxfYbI](https://openreview.net/forum?id=qw8AKxfYbI)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Ho, J., Salimans, T.：**无分类器扩散指导**。载于：神经信息处理系统进展研讨会（2021年），[https://openreview.net/forum?id=qw8AKxfYbI](https://openreview.net/forum?id=qw8AKxfYbI)'
- en: '[12] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W.: Lora: Low-rank adaptation of large language models. In: International
    Conference on Learning Representations (2022)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W.：**Lora**：大型语言模型的低秩适应。载于：国际学习表征会议（2022年）'
- en: '[13] Huang, K., Sun, K., Xie, E., Li, Z., Liu, X.: T2i-compbench: A comprehensive
    benchmark for open-world compositional text-to-image generation. In: Advances
    in Neural Information Processing Systems. vol. 36 (2024)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Huang, K., Sun, K., Xie, E., Li, Z., Liu, X.：**T2i-compbench**：一个用于开放世界组合文本到图像生成的综合基准。载于：神经信息处理系统进展。第36卷（2024年）'
- en: '[14] Kemker, R., McClure, M., Abitino, A., Hayes, T., Kanan, C.: Measuring
    catastrophic forgetting in neural networks. In: Proceedings of the AAAI Conference
    on Artificial Intelligence. vol. 32 (2018)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Kemker, R., McClure, M., Abitino, A., Hayes, T., Kanan, C.：**测量神经网络中的灾难性遗忘**。载于：AAAI人工智能会议论文集。第32卷（2018年）'
- en: '[15] Lian, L., Li, B., Yala, A., Darrell, T.: Llm-grounded diffusion: Enhancing
    prompt understanding of text-to-image diffusion models with large language models.
    arXiv preprint arXiv:2305.13655 (2023)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Lian, L., Li, B., Yala, A., Darrell, T.：**LLM基础的扩散**：利用大型语言模型增强文本到图像扩散模型的提示理解。arXiv预印本
    arXiv:2305.13655（2023年）'
- en: '[16] Lian, L., Shi, B., Yala, A., Darrell, T., Li, B.: Llm-grounded video diffusion
    models. arXiv preprint arXiv:2309.17444 (2023)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Lian, L., Shi, B., Yala, A., Darrell, T., Li, B.：**LLM基础的视频扩散模型**。arXiv预印本
    arXiv:2309.17444（2023年）'
- en: '[17] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Advances
    in Neural Information Processing Systems (2023)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Liu, H., Li, C., Wu, Q., Lee, Y.J.：**视觉指令调整**。载于：神经信息处理系统进展（2023年）'
- en: '[18] Luccioni, S., Akiki, C., Mitchell, M., Jernite, Y.: Stable bias: Evaluating
    societal representations in diffusion models. In: Advances in Neural Information
    Processing Systems. vol. 36 (2024)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Luccioni, S., Akiki, C., Mitchell, M., Jernite, Y.：**稳定偏见**：评估扩散模型中的社会表现。载于：神经信息处理系统进展。第36卷（2024年）'
- en: '[19] Mantri, K.S.I., Sasikumar, N.: Interactive fashion content generation
    using llms and latent diffusion models. In: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshop (2023)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Mantri, K.S.I., Sasikumar, N.：**使用LLMs和潜在扩散模型的互动时尚内容生成**。载于：IEEE/CVF计算机视觉与模式识别会议研讨会论文集（2023年）'
- en: '[20] Naik, R., Nushi, B.: Social biases through the text-to-image generation
    lens. In: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. p.
    786–808\. AIES ’23, Association for Computing Machinery, New York, NY, USA (2023).
    https://doi.org/10.1145/3600211.3604711'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Naik, R., Nushi, B.：**通过文本到图像生成镜头中的社会偏见**。载于：AAAI/ACM人工智能、伦理与社会会议论文集。第786–808页。AIES
    ’23，美国纽约计算机协会（2023年）。https://doi.org/10.1145/3600211.3604711'
- en: '[21] Nair, N.G., Cherian, A., Lohit, S., Wang, Y., Koike-Akino, T., Patel,
    V.M., Marks, T.K.: Steered diffusion: A generalized framework for plug-and-play
    conditional image synthesis. In: Proceedings of the IEEE/CVF International Conference
    on Computer Vision. pp. 20850–20860 (2023)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Nair, N.G., Cherian, A., Lohit, S., Wang, Y., Koike-Akino, T., Patel,
    V.M., Marks, T.K.：**引导扩散**：一种用于即插即用条件图像合成的通用框架。载于：IEEE/CVF国际计算机视觉会议论文集。第20850–20860页（2023年）'
- en: '[22] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew,
    B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and
    editing with text-guided diffusion models. In: Proceedings of Machine Learning
    Research. pp. 16784–16804 (2022)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew,
    B., Sutskever, I., Chen, M.：**Glide**：迈向具有文本指导的照片级图像生成和编辑。载于：机器学习研究论文集。第16784–16804页（2022年）'
- en: '[23] Orgad, H., Kawar, B., Belinkov, Y.: Editing implicit assumptions in text-to-image
    diffusion models. In: 2023 IEEE/CVF International Conference on Computer Vision.
    pp. 7030–7038\. IEEE Computer Society, Los Alamitos, CA, USA (oct 2023). https://doi.org/10.1109/ICCV51070.2023.00649'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Orgad, H., Kawar, B., Belinkov, Y.: 编辑文本到图像扩散模型中的隐性假设。载于: 2023 IEEE/CVF
    International Conference on Computer Vision，第7030–7038页。IEEE Computer Society,
    Los Alamitos, CA, USA (2023年10月)。 https://doi.org/10.1109/ICCV51070.2023.00649'
- en: '[24] Perera, M.V., Patel, V.M.: Analyzing bias in diffusion-based face generation
    models. arXiv preprint arXiv:2305.06402 (2023)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Perera, M.V., Patel, V.M.: 分析基于扩散的面部生成模型中的偏见。arXiv 预印本 arXiv:2305.06402
    (2023)'
- en: '[25] Phung, Q., Ge, S., Huang, J.B.: Grounded text-to-image synthesis with
    attention refocusing. arXiv preprint arXiv:2306.05427 (2023)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Phung, Q., Ge, S., Huang, J.B.: 基于注意力重新聚焦的文本到图像合成。arXiv 预印本 arXiv:2306.05427
    (2023)'
- en: '[26] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller,
    J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution
    image synthesis. In: International Conference on Learning Representations (2024),
    [https://openreview.net/forum?id=di52zR8xgf](https://openreview.net/forum?id=di52zR8xgf)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller,
    J., Penna, J., Rombach, R.: SDXL: 提高潜在扩散模型用于高分辨率图像合成。载于: International Conference
    on Learning Representations (2024), [https://openreview.net/forum?id=di52zR8xgf](https://openreview.net/forum?id=di52zR8xgf)'
- en: '[27] Qin, J., Wu, J., Chen, W., Ren, Y., Li, H., Wu, H., Xiao, X., Wang, R.,
    Wen, S.: Diffusiongpt: Llm-driven text-to-image generation system. arXiv preprint
    arXiv:2401.10061 (2024)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Qin, J., Wu, J., Chen, W., Ren, Y., Li, H., Wu, H., Xiao, X., Wang, R.,
    Wen, S.: Diffusiongpt: LLM 驱动的文本到图像生成系统。arXiv 预印本 arXiv:2401.10061 (2024)'
- en: '[28] Qu, L., Wu, S., Fei, H., Nie, L., Chua, T.S.: Layoutllm-t2i: Eliciting
    layout guidance from llm for text-to-image generation. In: Proceedings of the
    ACM International Conference on Multimedia (2023)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Qu, L., Wu, S., Fei, H., Nie, L., Chua, T.S.: Layoutllm-t2i: 从 LLM 生成布局指导以进行文本到图像生成。载于:
    Proceedings of the ACM International Conference on Multimedia (2023)'
- en: '[29] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable
    visual models from natural language supervision. In: International Conference
    on Machine Learning. pp. 8748–8763\. PMLR (2021)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., 等: 从自然语言监督中学习可迁移的视觉模型。载于: International
    Conference on Machine Learning，第8748–8763页。PMLR (2021)'
- en: '[30] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical
    text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
    1(2),  3 (2022)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: 基于 CLIP 潜变量的层次化文本条件图像生成。arXiv
    预印本 arXiv:2204.06125 1(2), 3 (2022)'
- en: '[31] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
    image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition. pp. 10684–10695 (2022)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: 使用潜在扩散模型进行高分辨率图像合成。载于:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition，第10684–10695页
    (2022)'
- en: '[32] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,
    K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic
    text-to-image diffusion models with deep language understanding. In: Advances
    in Neural Information Processing Systems. vol. 35, pp. 36479–36494 (2022)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,
    K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., 等: 具有深度语言理解的 photorealistic
    文本到图像扩散模型。载于: Advances in Neural Information Processing Systems，第35卷，第36479–36494页
    (2022)'
- en: '[33] Smith, J.S., Hsu, Y.C., Zhang, L., Hua, T., Kira, Z., Shen, Y., Jin, H.:
    Continual diffusion: Continual customization of text-to-image diffusion with c-lora.
    arXiv preprint arXiv:2304.06027 (2023)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Smith, J.S., Hsu, Y.C., Zhang, L., Hua, T., Kira, Z., Shen, Y., Jin, H.:
    持续扩散: 使用 C-LoRA 持续定制文本到图像扩散。arXiv 预印本 arXiv:2304.06027 (2023)'
- en: '[34] Su, X., Ren, Y., Qiang, W., Song, Z., Gao, H., Wu, F., Zheng, C.: Unbiased
    image synthesis via manifold-driven sampling in diffusion models. arXiv preprint
    arXiv:2307.08199 (2023)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Su, X., Ren, Y., Qiang, W., Song, Z., Gao, H., Wu, F., Zheng, C.: 通过流形驱动的采样在扩散模型中进行无偏图像合成。arXiv
    预印本 arXiv:2307.08199 (2023)'
- en: '[35] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language
    models. In: Advances in Neural Information Processing Systems. vol. 35, pp. 24824–24837
    (2022)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., 等: 通过思考链提示在大型语言模型中引发推理。载于: Advances in Neural Information Processing
    Systems，第35卷，第24824–24837页 (2022)'
- en: '[36] Wu, T.H., Lian, L., Gonzalez, J.E., Li, B., Darrell, T.: Self-correcting
    llm-controlled diffusion models. arXiv preprint arXiv:2311.16090 (2023)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Wu, T.H., Lian, L., Gonzalez, J.E., Li, B., Darrell, T.: 自我修正的 LLM 控制扩散模型。arXiv
    预印本 arXiv:2311.16090 (2023)'
- en: '[37] Xie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y., Shou, M.Z.:
    Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion.
    In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.
    7452–7461 (2023)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Xie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y., Shou, M.Z.:
    Boxdiff：无训练的盒子约束扩散的文本到图像合成。载于：IEEE/CVF 国际计算机视觉大会论文集。第 7452–7461 页 (2023)'
- en: '[38] Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., Cui, B.: Mastering text-to-image
    diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv
    preprint arXiv:2401.11708 (2024)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., Cui, B.: 精通文本到图像扩散：重标注、规划和与多模态
    LLM 生成。arXiv 预印本 arXiv:2401.11708 (2024)'
- en: '[39] Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W.,
    Cui, B., Yang, M.H.: Diffusion models: A comprehensive survey of methods and applications.
    ACM Comput. Surv. 56(4) (2023). https://doi.org/10.1145/3626235'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W.,
    Cui, B., Yang, M.H.: 扩散模型：方法和应用的全面调查。ACM 计算机调查。56(4) (2023)。https://doi.org/10.1145/3626235'
- en: '[40] Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z.,
    Liu, C., Zeng, M., Wang, L.: Reco: Region-controlled text-to-image generation.
    In: Proceedings of the IEEE/CVF International Conference on Computer Vision and
    Pattern Recognition. pp. 14246–14255 (2023)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z.,
    Liu, C., Zeng, M., Wang, L.: Reco：区域控制的文本到图像生成。载于：IEEE/CVF 国际计算机视觉与模式识别大会论文集。第
    14246–14255 页 (2023)'
- en: '[41] Zhang, C., Zhang, C., Zhang, M., Kweon, I.S.: Text-to-image diffusion
    model in generative ai: A survey. arXiv preprint arXiv:2303.07909 (2023)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Zhang, C., Zhang, C., Zhang, M., Kweon, I.S.: 生成 AI 中的文本到图像扩散模型：一项调查。arXiv
    预印本 arXiv:2303.07909 (2023)'
- en: '[42] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
    diffusion models. In: Proceedings of the IEEE/CVF International Conference on
    Computer Vision. pp. 3836–3847 (2023)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Zhang, L., Rao, A., Agrawala, M.: 向文本到图像扩散模型添加条件控制。载于：IEEE/CVF 国际计算机视觉大会论文集。第
    3836–3847 页 (2023)'
- en: '[43] Zhang, T., Wang, Z., Huang, J., Tasnim, M.M., Shi, W.: A survey of diffusion
    based image generation models: Issues and their solutions. arXiv preprint arXiv:2308.13142
    (2023)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Zhang, T., Wang, Z., Huang, J., Tasnim, M.M., Shi, W.: 扩散基础图像生成模型的调查：问题及其解决方案。arXiv
    预印本 arXiv:2308.13142 (2023)'
- en: A LLM-Driven Detail Synthesis
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 驱动的详细合成
- en: 'In this work, as described in the Sec. 4.1 of the main paper, we emphasized
    that by leveraging LLMs, we have significantly enriched responses to encompass
    additional information, such as layout, detailed descriptions, background scenes,
    and negative prompts. To achieve this, we facilitated an interaction with a LLM
    as shown in [Fig. 8](#S1.F8 "In A LLM-Driven Detail Synthesis ‣ The Fabrication
    of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation").
    The input given to the LLM, depicted on the left side of the figure, includes
    detailed task specifications and in-context learning examples to enhance the LLM’s
    comprehension. The response from the LLM, shown on the right, is rich with details
    extracted from the prompt. Notably, the descriptions are particularly crucial
    for our work, serving as indispensable information for the later image generation
    stage.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，如主文献第 4.1 节所述，我们强调了通过利用 LLM，我们显著丰富了响应，以涵盖额外的信息，如布局、详细描述、背景场景和负面提示。为实现这一目标，我们与
    LLM 进行了交互，如[图 8](#S1.F8 "在 LLM 驱动的详细合成 ‣ 现实与幻想的制造：基于 LLM 辅助提示解释的场景生成")所示。输入给 LLM
    的内容，如图左侧所示，包括详细的任务规范和上下文学习示例，以增强 LLM 的理解。LLM 的响应，如图右侧所示，详细丰富，从提示中提取了丰富的细节。值得注意的是，这些描述对于我们的工作尤为重要，作为后续图像生成阶段不可或缺的信息。
- en: '![Refer to caption](img/3fa4b7bd8c21299ce5ab8c6e012e48eb.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3fa4b7bd8c21299ce5ab8c6e012e48eb.png)'
- en: 'Figure 8: Detail Synthesis. The illustration of the interaction with a LLM
    in our work.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：详细合成。展示了我们工作中与 LLM 的交互。
- en: '![Refer to caption](img/d22883f60838316c001b1a45648043a7.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d22883f60838316c001b1a45648043a7.png)'
- en: 'Figure 9: More results on Realistic and Analytical. The compared models include
    (a) Stable Diffusion, (b) MultiDiffusion, (c) AttendandExcite, (d) LMD, (e) BoxDiff,
    (f) SDXL, (g) Ours'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：关于现实和分析的更多结果。对比的模型包括 (a) Stable Diffusion, (b) MultiDiffusion, (c) AttendandExcite,
    (d) LMD, (e) BoxDiff, (f) SDXL, (g) 我们的模型
- en: '![Refer to caption](img/2be88a6f76470e3cd89a744ddc24c3ea.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2be88a6f76470e3cd89a744ddc24c3ea.png)'
- en: 'Figure 10: More results on Creativity and Imagination. The compared models
    include (a) Stable Diffusion, (b) MultiDiffusion, (c) AttendandExcite, (d) LMD,
    (e) BoxDiff, (f) SDXL, (g) Ours'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：创意和想象力的更多结果。比较的模型包括（a）Stable Diffusion，（b）MultiDiffusion，（c）AttendandExcite，（d）LMD，（e）BoxDiff，（f）SDXL，（g）我们的模型
- en: B Qualitative Comparison on RFBench
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B RFBench上的定性比较
- en: 'In [Fig. 9](#S1.F9 "In A LLM-Driven Detail Synthesis ‣ The Fabrication of Reality
    and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation") and [Fig. 10](#S1.F10
    "In A LLM-Driven Detail Synthesis ‣ The Fabrication of Reality and Fantasy: Scene
    Generation with LLM-Assisted Prompt Interpretation"), we present additional qualitative
    examples to showcase the exceptional outcomes of our work. [Fig. 9](#S1.F9 "In
    A LLM-Driven Detail Synthesis ‣ The Fabrication of Reality and Fantasy: Scene
    Generation with LLM-Assisted Prompt Interpretation") shows the results under the
    category Realistic and Analytical, while [Fig. 10](#S1.F10 "In A LLM-Driven Detail
    Synthesis ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted
    Prompt Interpretation") shows the category Creativity and Imagination. Both figures
    demonstrate that our method achieves more accurate editing results compared to
    other approaches.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图9](#S1.F9 "在LLM驱动的细节合成中 ‣ 现实与幻想的制造：利用LLM辅助提示解释生成场景")和[图10](#S1.F10 "在LLM驱动的细节合成中
    ‣ 现实与幻想的制造：利用LLM辅助提示解释生成场景")中，我们展示了额外的定性示例，以展示我们工作的卓越成果。[图9](#S1.F9 "在LLM驱动的细节合成中
    ‣ 现实与幻想的制造：利用LLM辅助提示解释生成场景")展示了现实和分析类别下的结果，而[图10](#S1.F10 "在LLM驱动的细节合成中 ‣ 现实与幻想的制造：利用LLM辅助提示解释生成场景")展示了创意和想象力类别。两张图都表明，我们的方法在编辑结果的准确性上优于其他方法。
- en: '![Refer to caption](img/fca95df7d04f51287981f62a5996b4d4.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fca95df7d04f51287981f62a5996b4d4.png)'
- en: 'Figure 11: Survey on Image-Text Alignment and Image Fidelity'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：图像-文本对齐和图像保真度调查
- en: C GPT4Score
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C GPT4Score
- en: We follow the approach of T2I-Compbench, using Multimodal LLM (MLLM) to measure
    the similarity between generated images and input prompts. The key deviation lies
    in our observation that MiniGPT4, employed in T2I-Compbench, struggles to comprehend
    the surreal aspects of the images effectively. Therefore, we employ GPT4, a more
    powerful MLLM, as our new benchmarking model for evaluation, as mentioned in the
    Sec. 5.1 of the main paper.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循T2I-Compbench的方法，使用多模态LLM（MLLM）来测量生成图像与输入提示之间的相似性。关键的偏差在于我们观察到，在T2I-Compbench中使用的MiniGPT4，难以有效理解图像的超现实方面。因此，我们采用了更强大的MLLM
    GPT4作为新的基准模型进行评估，如主论文第5.1节所述。
- en: 'Specifically, given a generated image and its prompt, we input both the image
    and prompt into GPT4\. Subsequently, we pose two questions to the model: “*Describe
    the image*” and “*Predict the image-text alignment score*”, the generated image
    is then assigned the final output score predicted by GPT4\. For detailed prompts,
    please refer to the appendix of T2I-Compbench.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，给定生成的图像及其提示，我们将图像和提示输入到GPT4中。随后，我们向模型提出两个问题：“*描述图像*”和“*预测图像-文本对齐分数*”，然后将生成的图像分配GPT4预测的最终输出分数。有关详细提示，请参见T2I-Compbench的附录。
- en: D Human Evaluation
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D 人工评估
- en: 'In the human evaluation process, as introduced in the Sec. 5.4 of the main
    paper, we request annotators to assess the correspondence between a produced image
    and the textual prompt employed to create the image.  [Fig. 11](#S2.F11 "In B
    Qualitative Comparison on RFBench ‣ The Fabrication of Reality and Fantasy: Scene
    Generation with LLM-Assisted Prompt Interpretation") show the interfaces for human
    evaluation. The participants can choose a score from {1, 2, 3, 4, 5} and we normalize
    the scores by dividing them by 5\. We then compute the average score across all
    images and all participants.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工评估过程中，如主论文第5.4节所介绍，我们要求标注者评估生成图像与用于创建图像的文本提示之间的对应关系。[图11](#S2.F11 "在RFBench上的定性比较
    ‣ 现实与幻想的制造：利用LLM辅助提示解释生成场景")展示了人工评估的界面。参与者可以从{1, 2, 3, 4, 5}中选择一个分数，我们通过将分数除以5来规范化这些分数。然后，我们计算所有图像和所有参与者的平均分数。
- en: 'Table 5: The correlation between automatic evaluation metrics and human evaluation'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：自动评估指标与人工评估的相关性
- en: '|   Metrics | CLIPScore | GPT4Score |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|   指标 | CLIPScore | GPT4Score |'
- en: '| $\tau$ ($\uparrow$) | $\rho$ ($\uparrow$) | $\tau$ ($\uparrow$) | $\rho$
    ($\uparrow$) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| $\tau$ ($\uparrow$) | $\rho$ ($\uparrow$) | $\tau$ ($\uparrow$) | $\rho$
    ($\uparrow$) |'
- en: '| Realistic & Analytical |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 现实与分析 |'
- en: '|   Scientific and Empirical Reasoning | -0.4880 | -0.5946 | 0.6351 | 0.7157
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|   科学和实证推理 | -0.4880 | -0.5946 | 0.6351 | 0.7157 |'
- en: '|   Cultural and Temporal Awareness | -0.0476 | -0.1429 | 0.3273 | 0.3780 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|   文化和时间意识 | -0.0476 | -0.1429 | 0.3273 | 0.3780 |'
- en: '|   Factual or Literal Descriptions | 0.2333 | 0.3656 | 0.7620 | 0.8909 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|   事实或字面描述 | 0.2333 | 0.3656 | 0.7620 | 0.8909 |'
- en: '|   Conceptual and Metaphorical Thinking | -0.1952 | -0.1982 | 0.9234 | 0.9633
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|   概念和隐喻思维 | -0.1952 | -0.1982 | 0.9234 | 0.9633 |'
- en: '| Creativity & Imagination |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 创造力与想象力 |'
- en: '|   Common Objects in Unusual Contexts | -0.2381 | -0.2857 | -0.5345 | -0.6124
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|   不寻常背景中的常见物体 | -0.2381 | -0.2857 | -0.5345 | -0.6124 |'
- en: '|   Imaginative Scenarios | 0.3752 | 0.6335 | 0.7265 | 0.8432 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|   想象性场景 | 0.3752 | 0.6335 | 0.7265 | 0.8432 |'
- en: '|   Role Reversal or Conflicting | 0.0476 | 0.1429 | 0.5040 | 0.5774 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|   角色反转或冲突 | 0.0476 | 0.1429 | 0.5040 | 0.5774 |'
- en: '|   Anthropomorphic Scenarios | -0.1429 | -0.1429 | -0.5345 | -0.6124 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|   拟人化场景 | -0.1429 | -0.1429 | -0.5345 | -0.6124 |'
- en: E Human Correlation of the Evaluation Metrics
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E 评估指标的人工相关性
- en: 'We adopt the methodology from T2I-Compbench, calculating Kendall’s tau ($\tau$)
    and Spearman’s rho ($\rho$) to evaluate the ranking correlation between CLIPScore,
    GPT4Score, and human evaluation. For better comparison, the scores predicted by
    each evaluation metric are normalized to a 0-1 scale. The human correlation results
    are presented in  [Tab. 5](#S4.T5 "In D Human Evaluation ‣ The Fabrication of
    Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation").
    These results indicate that CLIP underperforms in both categories, as discussed
    in Section 5.1 of the main paper. This underperformance may be due to CLIP’s approach
    to image understanding, which is often too simplistic. Nevertheless, both metrics
    encounter challenges with Creativity and Imagination, highlighting that although
    GPT4Score offers a broader understanding of images, accurately assessing creativity
    remains a difficult task for both.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了T2I-Compbench的方法论，通过计算Kendall的tau（$\tau$）和Spearman的rho（$\rho$）来评估CLIPScore、GPT4Score和人工评估之间的排名相关性。为了更好地进行比较，使用每个评估指标预测的分数都被归一化到0-1的范围内。人工相关性结果展示在[表5](#S4.T5
    "在D部分 人工评估 ‣ 现实与幻想的编织：通过LLM辅助提示解释生成场景")中。这些结果表明，CLIP在两个类别中的表现都不佳，如主文第5.1节所讨论。这种表现不佳可能是由于CLIP对图像理解的方式通常过于简单。然而，这两个指标在创造力和想象力方面都面临挑战，突出表明尽管GPT4Score提供了对图像的更广泛理解，但准确评估创造力仍然是两个指标都面临的困难任务。
- en: '![Refer to caption](img/dc065e19094f950681e6d4d558cef4ae.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dc065e19094f950681e6d4d558cef4ae.png)'
- en: 'Figure 12: Ablation study on various components in our work.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：我们工作中各种组件的消融研究。
- en: F Visualization of Ablation Study
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F 消融研究的可视化
- en: 'In addition to the quantitative results presented in our ablation study, we
    have also included visual examples to showcase the impact of different components
    in our work. As shown in [Fig. 12](#S5.F12 "In E Human Correlation of the Evaluation
    Metrics ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted
    Prompt Interpretation"), the removal of guidance constraint and suppression constraint
    both causes the diffusion model to become muddled when dealing with multiple objects.
    Besides, eliminating the SAA module leads to unclear outcomes with the generated
    objects.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在我们的消融研究中展示的定量结果外，我们还包括了视觉示例，以展示我们工作中不同组件的影响。如[图12](#S5.F12 "在E部分 评估指标的人工相关性
    ‣ 现实与幻想的编织：通过LLM辅助提示解释生成场景")所示，去除指导约束和抑制约束都会导致扩散模型在处理多个物体时变得模糊。此外，消除SAA模块会导致生成物体的结果不清晰。
- en: F.1 Effect of the hyperparameter $\beta$ of guidance constraint
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 指导约束超参数$\beta$的效果
- en: 'In our paper, we emphasize the critical role of the guidance constraint in
    integrating multiple objects into the background. To underscore its significance,
    we performed an additional ablation study focusing on the hyperparameter $\beta$,
    which influences the strength of guidance constraint. As shown in [Fig. 13](#S6.F13
    "In F.1 Effect of the hyperparameter 𝛽 of guidance constraint ‣ F Visualization
    of Ablation Study ‣ The Fabrication of Reality and Fantasy: Scene Generation with
    LLM-Assisted Prompt Interpretation"), we varied $\beta$ from 0.1 to 30 to observe
    the effects on the generated results. The findings reveal that an optimal $\beta$
    value (e.g., setting it to 15) ensures objects are accurately aligned with the
    layout and are of high quality. However, extreme $\beta$ values, such as 0.1 or
    30, disrupt the layout and diminish the overall quality of the generated images.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的论文中，我们强调了引导约束在将多个对象整合到背景中的关键作用。为了突出其重要性，我们进行了额外的消融研究，重点关注超参数 $\beta$，该参数影响引导约束的强度。如[图
    13](#S6.F13 "在 F.1 引导约束的超参数 𝛽 的影响 ‣ F 消融研究的可视化 ‣ 现实与幻想的编织：使用 LLM 辅助提示解释的场景生成")所示，我们将
    $\beta$ 从 0.1 调整到 30，以观察对生成结果的影响。研究结果显示，最佳的 $\beta$ 值（例如，将其设置为 15）能确保对象与布局准确对齐且质量较高。然而，极端的
    $\beta$ 值，如 0.1 或 30，会破坏布局并降低生成图像的整体质量。
- en: '![Refer to caption](img/30c422688cded664c2fcd3d6108330b3.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/30c422688cded664c2fcd3d6108330b3.png)'
- en: 'Figure 13: Effect of the hyperparameter $\beta$ of guidance constraint.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：引导约束的超参数 $\beta$ 的影响。
