- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:53:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:53:10'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN
    Manipulation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'UniBias: 通过内部注意力和前馈神经网络（FFN）操控揭示和减轻LLM偏见'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20612](https://ar5iv.labs.arxiv.org/html/2405.20612)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20612](https://ar5iv.labs.arxiv.org/html/2405.20612)
- en: Hanzhang Zhou^(1,2), Zijian Feng^(1,2), Zixiao Zhu^(1,2), Junlang Qian¹, Kezhi
    Mao^(1,2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 侯章周^(1,2)、冯子健^(1,2)、朱子晓^(1,2)、钱军浪¹、毛克志^(1,2)
- en: ¹Nanyang Technological University         ²Singapore-ETH Centre
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹南洋理工大学         ²新加坡-ETH中心
- en: '{hanzhang001, feng0119, zixiao001, junlang001}@e.ntu.edu.sg'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{hanzhang001, feng0119, zixiao001, junlang001}@e.ntu.edu.sg'
- en: ekzmao@ntu.edu.sg
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ekzmao@ntu.edu.sg
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have demonstrated impressive capabilities in various
    tasks using the in-context learning (ICL) paradigm. However, their effectiveness
    is often compromised by inherent bias, leading to prompt brittleness—sensitivity
    to design settings such as example selection, order, and prompt formatting. Previous
    studies have addressed LLM bias through external adjustment of model outputs,
    but the internal mechanisms that lead to such bias remain unexplored. Our work
    delves into these mechanisms, particularly investigating how feedforward neural
    networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting
    the contribution of individual FFN vectors and attention heads, we identify the
    biased LLM components that skew LLMs’ prediction toward specific labels. To mitigate
    these biases, we introduce UniBias, an inference-only method that effectively
    identifies and eliminates biased FFN vectors and attention heads. Extensive experiments
    across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance
    and alleviates prompt brittleness of LLMs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在使用上下文学习（ICL）范式的各种任务中表现出了令人印象深刻的能力。然而，它们的效果常常因固有的偏见而受到影响，导致提示脆弱性——对设计设置如示例选择、顺序和提示格式的敏感性。之前的研究通过对模型输出的外部调整来解决LLM偏见，但导致这些偏见的内部机制仍未被探索。我们的工作深入探讨了这些机制，特别是调查了前馈神经网络（FFNs）和注意力头如何导致LLM的偏见。通过解释个别FFN向量和注意力头的贡献，我们识别出那些使LLM的预测偏向特定标签的偏见组件。为减轻这些偏见，我们引入了UniBias，这是一种仅在推理阶段有效识别和消除偏见FFN向量和注意力头的方法。对12个NLP数据集的广泛实验表明，UniBias显著提升了ICL性能，并缓解了LLMs的提示脆弱性。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have shown exceptional capabilities in various
    natural language processing (NLP) tasks, employing the in-context learning (ICL)
    paradigm. This paradigm conditions LLMs on a context prompt comprising of a few
    example-label pairs [[1](#bib.bib1), [5](#bib.bib5)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展示了卓越的能力，采用了上下文学习（ICL）范式。该范式将LLMs的条件设定在由几个示例-标签对组成的上下文提示上[[1](#bib.bib1),
    [5](#bib.bib5)]。
- en: 'Despite their impressive performance, LLMs are prone to prompt brittleness,
    characterized by high sensitivity to the choice [[29](#bib.bib29)] and order [[14](#bib.bib14)]
    of examples, and prompt formatting [[15](#bib.bib15)], as demonstrated in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UniBias: Unveiling and Mitigating LLM
    Bias through Internal Attention and FFN Manipulation"). Such prompt brittleness
    is found to be arise from the bias in LLMs towards predicting certain answers
    [[29](#bib.bib29)]. The presence of the bias undermines the robustness and adaptability
    of LLMs in diverse applications.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管大型语言模型（LLMs）表现出色，但它们容易受到提示脆弱性的影响，这种现象表现为对示例的选择[[29](#bib.bib29)]和顺序[[14](#bib.bib14)]、提示格式[[15](#bib.bib15)]的高度敏感，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ UniBias: Unveiling and Mitigating LLM Bias through
    Internal Attention and FFN Manipulation")所示。这种提示脆弱性被发现源于LLMs在预测特定答案时的偏见[[29](#bib.bib29)]。这种偏见削弱了LLMs在多样化应用中的鲁棒性和适应性。'
- en: Extensive research has focused on identifying factors that lead to LLM bias
    and strategies for mitigation. For instance, vanilla label bias [[7](#bib.bib7)]
    and recency bias [[29](#bib.bib29)] demonstrate the LLM’s inherent non-contextual
    preference for certain labels and contextual preference for specific positions,
    respectively. Additionally, several calibration methods [[7](#bib.bib7), [10](#bib.bib10),
    [29](#bib.bib29)] are proposed to counteract the bias by adjusting decision boundaries
    of model output probabilities. However, these approaches are derived from *external*
    observations or adjustments of LLM outputs, leaving the *internal* mechanisms
    within LLMs that cause such bias poorly understood.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大量研究集中在识别导致LLM偏见的因素和减轻策略上。例如，普通标签偏见[[7](#bib.bib7)]和近期偏见[[29](#bib.bib29)]分别展示了LLM对某些标签的固有非上下文偏好和对特定位置的上下文偏好。此外，提出了几种校准方法[[7](#bib.bib7),
    [10](#bib.bib10), [29](#bib.bib29)]，通过调整模型输出概率的决策边界来对抗偏见。然而，这些方法源于*外部*观察或调整LLM输出，导致LLM内部产生此类偏见的*内部*机制仍然了解不足。
- en: 'In this work, we investigate the internal mechanism of LLM bias, specifically
    how feedforward neural networks (FFNs) and attention heads contribute to such
    bias. Building on findings in mechanistic interpretability [[6](#bib.bib6), [4](#bib.bib4)],
    we assess the contribution of individual attention heads and FFN vectors¹¹1FFN
    vector refers to the value vector in the second weight matrix of the FFN layer.
    We elaborate on this in Section [2.1](#S2.SS1 "2.1 Background ‣ 2 Internal Mechanisms
    Causing the Bias of LLMs ‣ UniBias: Unveiling and Mitigating LLM Bias through
    Internal Attention and FFN Manipulation") to label predictions in LLMs. By identifying
    FFN vectors and attention heads that convey biased influences towards label prediction,
    we reveal the internal mechanisms behind several key bias factors, including vanilla
    label bias [[7](#bib.bib7)], recency bias [[29](#bib.bib29)], and selection bias
    [[30](#bib.bib30)]. For instance, our analysis of FFN vectors without input context
    demonstrates that their cumulative impact biases the LLM towards specific labels,
    indicating a non-contextual preference for certain labels, i.e., vanilla label
    bias. We elaborate on the background of mechanistic interpretability in Section
    [2.1](#S2.SS1 "2.1 Background ‣ 2 Internal Mechanisms Causing the Bias of LLMs
    ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN
    Manipulation") and present our findings on the internal mechanisms of LLM biases
    in next section.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们调查了LLM偏见的内部机制，特别是前馈神经网络（FFNs）和注意力头如何对这种偏见产生贡献。基于机制解释的研究[[6](#bib.bib6),
    [4](#bib.bib4)]，我们评估了各个注意力头和FFN向量¹¹FFN向量指的是FFN层第二个权重矩阵中的值向量。我们在第[2.1](#S2.SS1
    "2.1 Background ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation")节中详细说明了这一点，以标记LLMs中的预测。通过识别传达偏见影响的FFN向量和注意力头，我们揭示了几个关键偏见因素的内部机制，包括普通标签偏见[[7](#bib.bib7)]、近期偏见[[29](#bib.bib29)]和选择偏见[[30](#bib.bib30)]。例如，我们对没有输入上下文的FFN向量的分析表明，它们的累积影响使LLM偏向特定标签，表明对某些标签的非上下文偏好，即普通标签偏见。我们在第[2.1](#S2.SS1
    "2.1 Background ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation")节中详细阐述了机制解释的背景，并在下一节中介绍了LLM偏见的内部机制研究结果。'
- en: 'we pose the question: Can we identify the biased components of LLMs and mitigate
    their detrimental impact on label prediction? Motivated by this intuition, we
    propose UniBias, an inference-only method designed to identify and eliminate biased
    FFN vectors and attention heads in LLMs. Specifically, we begin by projecting
    each FFN vector and attention head into the vocabulary space to interpret the
    information conveyed by their outputs. We then detect biased components based
    on three criteria we defined: the relatedness criterion, the bias criterion, and
    the low variance criterion. After identification, we mitigate their impact by
    masking these biased components. Extensive experimental results demonstrate that
    LLMs, from which biased components have been removed, consistently outperform
    their original counterparts by a significant margin. Further, as illustrated in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UniBias: Unveiling and Mitigating
    LLM Bias through Internal Attention and FFN Manipulation"), our method significantly
    improves both the performance and robustness of ICL with perturbations of various
    design settings.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一个问题：我们能否识别 LLM 中的偏见组件并减轻其对标签预测的负面影响？受这一直觉的启发，我们提出了 UniBias，一种仅用于推理的方法，旨在识别和消除
    LLM 中的偏见 FFN 向量和注意力头。具体而言，我们首先将每个 FFN 向量和注意力头投影到词汇空间，以解释其输出传达的信息。然后，我们根据我们定义的三个标准来检测偏见组件：相关性标准、偏见标准和低方差标准。识别后，我们通过遮蔽这些偏见组件来减轻其影响。大量实验结果表明，从中移除偏见组件的
    LLM 一致地比其原始版本表现优异。此外，如图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ UniBias：通过内部注意力和 FFN 操作揭示和缓解
    LLM 偏见") 所示，我们的方法显著提高了 ICL 的性能和鲁棒性，适用于各种设计设置的扰动。
- en: '![Refer to caption](img/963cfe0f92c661b3f42adb0b2d966af0.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/963cfe0f92c661b3f42adb0b2d966af0.png)'
- en: 'Figure 1: illustrates the prompt brittleness of ICL and the effectiveness of
    our method in mitigating this issue. Experiments are conducted in one-shot setting,
    using SST2 [[20](#bib.bib20)] dataset for experiments on example selection and
    prompt formatting and AGnews [[28](#bib.bib28)] dataset for example order experiment
    due to more diverse combination of orders.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示了 ICL 的提示脆弱性以及我们方法在缓解这一问题方面的有效性。实验在单次设置下进行，使用 SST2 [[20](#bib.bib20)]
    数据集进行示例选择和提示格式化实验，使用 AGnews [[28](#bib.bib28)] 数据集进行示例顺序实验，因为后者的顺序组合更为多样。
- en: 'The contributions of our work are summarized as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的贡献总结如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In contrast to existing works based on external observation of LLM outputs,
    we unveil the internal mechanisms within LLMs that lead to their bias towards
    predicting certain answers.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与现有基于外部观察 LLM 输出的研究不同，我们揭示了 LLM 内部机制中导致其对某些答案有偏见的原因。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose the UniBias method to mitigate LLM bias by identifying and eliminating
    biased FFN vectors and attention heads within LLMs. Moreover, our method demonstrate
    an effective way to manipulate internal structures of LLMs.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 UniBias 方法，通过识别和消除 LLM 内部的偏见 FFN 向量和注意力头来缓解 LLM 的偏见。此外，我们的方法展示了操作 LLM
    内部结构的有效方式。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extensive experiments across 12 NLP datasets demonstrate that, by removing the
    biased components, our UniBias method significantly enhances ICL performance compared
    to the original LLM, achieving state-of-the-art results.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 12 个 NLP 数据集上的广泛实验表明，通过移除偏见组件，我们的 UniBias 方法显著提升了 ICL 性能，相比于原始 LLM 实现了最先进的结果。
- en: 2 Internal Mechanisms Causing the Bias of LLMs
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 LLM 偏见的内部机制
- en: This section reveals the internal mechanisms within LLMs that lead to various
    bias factors.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节揭示了 LLM 内部机制中导致各种偏见因素的原因。
- en: 2.1 Background
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 背景
- en: The theoretical background of this work is based on research on mechanistic
    interpretability [[6](#bib.bib6), [23](#bib.bib23), [8](#bib.bib8)], which aims
    to explain the internal processes in language models (LMs), facilitating the interpretation
    of the contributions of individual model components to the final prediction.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的理论背景基于对机制解释性的研究 [[6](#bib.bib6), [23](#bib.bib23), [8](#bib.bib8)]，旨在解释语言模型
    (LMs) 中的内部过程，促进对单个模型组件对最终预测贡献的解释。
- en: We are focusing on decoder-only LMs in this paper. They are composed by a sequence
    of transformer layers, each composed of a multi-head self-attention layer and
    an feedforward neural network layer. The background knowledge for interpreting
    the contribution of each FFN vector and attention head to the models’ prediction
    are demonstrated as follows.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文专注于仅解码器LM。这些模型由一系列变换器层组成，每层包括一个多头自注意力层和一个前馈神经网络层。解释每个FFN向量和注意力头对模型预测贡献的背景知识如下所示。
- en: The Residual Stream We interpret Transformers following the view of residual
    stream [[6](#bib.bib6), [4](#bib.bib4)]. Due to the residdual connection of Transformers,
    each layer takes a hidden state as input, and adds information obtained by its
    attention layer and FFN layer to the hidden state through residual connection.
    In this sence, the hidden state is a residual stream passed along layers, and
    each attention layer and FFN layer contribute to the final prediction by adding
    information to the residual stream.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 残差流 我们根据残差流的观点解释变换器 [[6](#bib.bib6), [4](#bib.bib4)]。由于变换器的残差连接，每层将隐藏状态作为输入，并通过残差连接将注意力层和FFN层获得的信息添加到隐藏状态中。从这个意义上说，隐藏状态是沿层传递的残差流，每个注意力层和FFN层通过将信息添加到残差流中来贡献最终预测。
- en: 'Attention Heads   Following Elhage et al. [[6](#bib.bib6)], Dar et al. [[4](#bib.bib4)],
    the output of each attention layer of LM can be computed as the sum of all its
    attention heads. Specifically, for $l$, $W_{V}^{l}$ and $W_{O}^{\ell,j}\in\mathbb{R}^{\frac{d}{H}\times
    d}$ is the number of attention heads. We then find that:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力头   根据Elhage等人 [[6](#bib.bib6)] 和Dar等人 [[4](#bib.bib4)] 的研究，LM的每个注意力层的输出可以计算为其所有注意力头的总和。具体来说，对于
    $l$，$W_{V}^{l}$ 和 $W_{O}^{\ell,j}\in\mathbb{R}^{\frac{d}{H}\times d}$ 是注意力头的数量。我们发现：
- en: '|  | $1$2 |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $A^{\ell,j}=\text{softmax}\left(\frac{({X}^{\ell}W_{Q}^{\ell,j})({X}^{\ell}W_{K}^{\ell,j})^{T}}{\sqrt{d/H}}+M^{\ell,j}\right)$
    is the attention mask. Therefore, the output of an attention layer is equivalent
    to computing attention heads independently, multiplying each by its own output
    matrix, and adding them into the residual stream of the LM.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A^{\ell,j}=\text{softmax}\left(\frac{({X}^{\ell}W_{Q}^{\ell,j})({X}^{\ell}W_{K}^{\ell,j})^{T}}{\sqrt{d/H}}+M^{\ell,j}\right)$
    是注意力掩码。因此，注意力层的输出等于独立计算注意力头，将每个头乘以其自身的输出矩阵，并将它们添加到LM的残差流中。
- en: 'FFN   In line with Geva et al. [[8](#bib.bib8), [9](#bib.bib9)], transformer
    FFN layers can be cast as linear combination of vectors. Specifically, for an
    input vector $\mathbf{x}^{\ell}\in\mathbb{R}^{d}$, the FFN output can be derived
    as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: FFN   根据Geva等人 [[8](#bib.bib8), [9](#bib.bib9)] 的研究，变换器FFN层可以视为向量的线性组合。具体来说，对于输入向量
    $\mathbf{x}^{\ell}\in\mathbb{R}^{d}$，FFN输出可以推导为：
- en: '|  | $1$2 |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $f$ produces the coefficient $m_{i}^{\ell}$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 产生系数 $m_{i}^{\ell}$。
- en: '![Refer to caption](img/2e09c367b8ba2a30fbbeb1af8e7a1867.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2e09c367b8ba2a30fbbeb1af8e7a1867.png)'
- en: 'Figure 2: Unveiling vanilla label bias by uncontextual accumulated FFN logits.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：通过非上下文累积FFN logits揭示香草标签偏差。
- en: Logit Lens   The logit lens [[16](#bib.bib16)] is a technique that directly
    decode hidden states into the vocabulary space using the unembedding matrix of
    the LLM for interpretation. This approach has been validated in various studies
    as an efficient method for interpreting the weight matrix or hidden states of
    LLMs [[4](#bib.bib4), [11](#bib.bib11), [27](#bib.bib27), [8](#bib.bib8)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Logit Lens   Logit lens [[16](#bib.bib16)] 是一种直接使用LLM的去嵌入矩阵将隐藏状态解码到词汇空间中的技术。这种方法在各种研究中被验证为一种高效的解释LLM权重矩阵或隐藏状态的方法
    [[4](#bib.bib4), [11](#bib.bib11), [27](#bib.bib27), [8](#bib.bib8)]。
- en: In summary, each attention layer and FFN layer contribute to the final prediction
    by adding their output hidden states to the residual stream. These outputs can
    be viewed as the sum of their respective attention heads and FFN vectors. Each
    attention head or FFN vector’s output can be interpreted through the logit lens.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，每个注意力层和FFN层通过将它们的输出隐藏状态添加到残差流中来贡献于最终预测。这些输出可以视为各自注意力头和FFN向量的总和。每个注意力头或FFN向量的输出可以通过logit
    lens进行解释。
- en: 2.2 Internal Mechanisms of Bias Factors
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 偏差因素的内部机制
- en: We delve into the mechanisms behind several bias factors, analyzing the contributions
    of attention heads and FFN vectors to the biased predictions in LLMs. We explore
    vanilla label bias, position bias, and selection bias using the Llama-2 7B model
    [[21](#bib.bib21)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入探讨了多个偏差因素背后的机制，分析了注意力头和FFN向量对LLM偏差预测的贡献。我们使用Llama-2 7B模型[[21](#bib.bib21)]探讨了原始标签偏差、位置偏差和选择偏差。
- en: Vanilla Label Bias   The vanilla label bias [[7](#bib.bib7)], also known as
    common token bias [[29](#bib.bib29)], is the inherent, uncontextual preference
    of the model towards predicting certain label names. Given the contextual nature
    of attention layers, our investigation focuses on the FFN layers, where we identified
    a corresponding uncontextual preference. Specifically, by projecting the FFN value
    vectors into the vocabulary space, we compute the logits for various label names
    for each FFN vector. Utilizing the residual stream insight, we then aggregate
    these logits for all FFN vectors whose label logits rank within the top $10$ over
    the vocabulary, reflecting uncontextual influences of FFN vectors that are effective
    in label prediction. This process yields what we term *uncontextual accumulated
    FFN logits*, revealing the intrinsic bias of the LLM towards predicting label
    names without the influence of input.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Vanilla Label Bias   “原始标签偏差”[[7](#bib.bib7)]，也称为“常见标记偏差”[[29](#bib.bib29)]，是模型对预测某些标签名称的固有、非上下文相关的偏好。鉴于注意力层的上下文特性，我们的研究重点放在了FFN层，在这里我们识别出相应的非上下文相关的偏好。具体来说，通过将FFN值向量投影到词汇空间，我们为每个FFN向量计算不同标签名称的logits。利用残差流的洞察，我们然后汇总所有FFN向量中标签logits在词汇表中排名前$10$的logits，反映了FFN向量在标签预测中有效的非上下文相关的影响。这个过程产生了我们所称的*非上下文累计FFN
    logits*，揭示了LLM在预测标签名称时对输入的固有偏见。
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Background ‣ 2 Internal Mechanisms Causing
    the Bias of LLMs ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal
    Attention and FFN Manipulation") illustrates the accumulated uncontextual FFN
    logits across different label names in the sentiment analysis task, alongside
    their corresponding zero-shot prediction frequencies on the SST-2 dataset. For
    example, the label name ’positive’ exhibits higher uncontextual accumulated FFN
    logits compared to ’negative,’ leading to a higher frequency of ’positive’ predictions.
    Additionally, when comparing the labels ’good’ and ’bad’, the difference in their
    uncontextual accumulated FFN logits is more pronounced than that between ’positive’
    and ’negative,’ resulting in a larger discrepancy in prediction frequency. Conversely,
    the accumulated logits for the labels ’satisfied’ and ’disappointed’ show a reverse
    trend relative to ’positive’ and ’negative’, which results in a corresponding
    reverse trend in their prediction frequency ratios.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](#S2.F2 "Figure 2 ‣ 2.1 Background ‣ 2 Internal Mechanisms Causing the
    Bias of LLMs ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention
    and FFN Manipulation")展示了在情感分析任务中不同标签名称的累计非上下文FFN logits，以及它们在SST-2数据集上的相应零-shot预测频率。例如，标签名称“positive”相比于“negative”展示了更高的非上下文累计FFN
    logits，导致“positive”预测的频率更高。此外，在比较标签“good”和“bad”时，它们的非上下文累计FFN logits差异比“positive”和“negative”之间的差异更为明显，从而导致预测频率的差异更大。相反，标签“satisfied”和“disappointed”的累计logits相对于“positive”和“negative”展示了相反的趋势，从而在它们的预测频率比中产生了相应的反向趋势。'
- en: '![Refer to caption](img/0b5e080aac8764db09f56e0ed6759321.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0b5e080aac8764db09f56e0ed6759321.png)'
- en: 'Figure 3: The internal mechanism of the recency bias.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：最近性偏差的内部机制。
- en: Recency Bias   Recency bias refers to the tendency of LLMs to favor the label
    of the example at the end of the prompt [[29](#bib.bib29)]. By examining the behavior
    of attention heads within LLMs, we observe that specific heads consistently prioritize
    the example at the end of the prompt, providing an internal perspective on the
    origin of recency bias.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Recency Bias   “最近性偏差”指的是LLM倾向于偏好提示文本结尾处的示例标签[[29](#bib.bib29)]。通过检查LLM内部注意力头的行为，我们观察到特定的头部始终优先考虑提示文本结尾处的示例，从而提供了关于最近性偏差来源的内部视角。
- en: 'We identify the biased attention head using the method introduced in Section
    [3](#S3 "3 Methodology ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal
    Attention and FFN Manipulation"). We compare the behaviors of a biased attention
    head (layer 16, head 29) and an unbiased attention head (layer 16, head 19) in
    terms of the attention weight assigned to examples at different positions and
    the label logits of the corresponding attention head’s output. Specifically, we
    use the SST-2 dataset, including one positive and one negative example in the
    prompt, and test with 40 samples, evenly split between positive and negative examples.
    More experimental details are provided in Appendix [A](#A1 "Appendix A Experimental
    Details ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention
    and FFN Manipulation").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用第[3](#S3 "3 方法 ‣ UniBias：通过内部注意力和FFN操作揭示和缓解LLM偏置")节中介绍的方法来识别偏置的注意力头。我们比较了偏置的注意力头（第16层，第29头）和未偏置的注意力头（第16层，第19头）在不同位置示例上的注意力权重分配及对应注意力头输出的标签logits。具体来说，我们使用SST-2数据集，其中提示中包含一个正面示例和一个负面示例，并使用40个样本进行测试，这些样本在正面和负面示例之间均匀分配。更多实验细节见附录[A](#A1
    "附录 A 实验细节 ‣ UniBias：通过内部注意力和FFN操作揭示和缓解LLM偏置")。
- en: 'Experimental results in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Internal Mechanisms
    of Bias Factors ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation") reveal
    that the biased attention head (layer 16, head 29) consistently assigns significantly
    larger attention weights to the final example, irrespective of the ground truth
    labels of the test samples. This bias persists even when the sequence of examples
    is reversed, as shown in the second subfigure, indicating a biased preference
    of this attention head for the last example in the prompt. Furthermore, the biased
    attention weight assignment leads to biased logits, as shown in the third subfigure.
    In contrast, the unbiased attention head (layer 16, head 19) assigns very close
    averaged attention weights to both examples in the prompt. Interestingly, we observe
    that this unbiased head generally assigns larger weights to the example whose
    label matches the ground truth label of the test sample, resulting in 35 out of
    40 samples being correctly classified based on this pattern by this single attention
    head. The preference shown by specific attention heads for the example at the
    end of the prompt reveals the internal mechanism of recency bias.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S2.F3 "图 3 ‣ 2.2 偏置因素的内部机制 ‣ 2 造成LLM偏置的内部机制 ‣ UniBias：通过内部注意力和FFN操作揭示和缓解LLM偏置")中的实验结果显示，偏置的注意力头（第16层，第29头）始终将显著更大的注意力权重分配给最后一个示例，无论测试样本的真实标签是什么。即使当示例的顺序被反转时，这种偏置仍然存在，如第二个子图所示，表明这个注意力头对提示中的最后一个示例有偏置的偏好。此外，偏置的注意力权重分配导致了偏置的logits，如第三个子图所示。相比之下，未偏置的注意力头（第16层，第19头）对提示中的两个示例分配了非常接近的平均注意力权重。有趣的是，我们观察到这个未偏置的头通常对标签与测试样本的真实标签匹配的示例分配更大的权重，这使得基于这种模式的单个注意力头能够正确分类40个样本中的35个。特定注意力头对提示中最后一个示例的偏好揭示了近期偏置的内部机制。
- en: '![Refer to caption](img/55bf77e539792b0755039d756a8b7e26.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/55bf77e539792b0755039d756a8b7e26.png)'
- en: 'Figure 4: The internal mechanism of the selection bias.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：选择偏置的内部机制。
- en: Selection Bias   The selection bias refers that LLMs prefer to select specific
    option ID (like "Option A") as answers for multiple choice questions [[30](#bib.bib30)].
    We have identified both FFN vectors and attention heads that consistently favor
    a specific option regardless of the ground truth label of the test sample, revealing
    the internal mechanism of selection bias.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 选择偏置 选择偏置指的是LLM在多项选择题中偏好选择特定选项ID（如“选项A”）作为答案[[30](#bib.bib30)]。我们已经识别出FFN向量和注意力头，它们始终偏爱特定选项，而不管测试样本的真实标签是什么，这揭示了选择偏置的内部机制。
- en: 'We evaluate the Llama-2 7B model on the ARC dataset, which contains four options
    (A, B, C, D). We use a zero-shot setting to avoid the influence of position bias
    from multiple examples. More details are provided in Appendix [A](#A1 "Appendix
    A Experimental Details ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal
    Attention and FFN Manipulation").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在包含四个选项（A、B、C、D）的ARC数据集上评估Llama-2 7B模型。我们使用零-shot设置以避免多个示例位置偏置的影响。更多细节见附录[A](#A1
    "附录 A 实验细节 ‣ UniBias：通过内部注意力和FFN操作揭示和缓解LLM偏置")。
- en: 'Experimental results are illustrated in Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Internal
    Mechanisms of Bias Factors ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣
    UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN
    Manipulation"). Firstly, we observe that the LLM exhibits a vanilla label bias
    favoring option "A", as shown in the first subfigure. Additionally, we identify
    a biased attention head that demonstrates a position bias consistently favoring
    the first option regardless of the ground truth labels of the test samples (second
    subfigure) or changes in the sequence of options (third subfigure). Since option
    A is usually the first option, these two biases both lead to the LLM’s preference
    for option A.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果如图 [4](#S2.F4 "Figure 4 ‣ 2.2 Internal Mechanisms of Bias Factors ‣ 2 Internal
    Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling and Mitigating LLM Bias
    through Internal Attention and FFN Manipulation") 所示。首先，我们观察到 LLM 表现出一个原始的标签偏差，偏向选项
    "A"，如第一幅子图所示。此外，我们识别出了一个有偏见的注意力头，它表现出一个位置偏差，持续偏向第一个选项，无论测试样本的真实标签如何（第二幅子图）或选项的顺序如何变化（第三幅子图）。由于选项
    A 通常是第一个选项，这两个偏差都导致 LLM 更倾向于选项 A。'
- en: 3 Methodology
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'In the previous section, we unveil that various bias factors are stem from
    the biased behaviors of attention heads and FFN vectors. Naturally, we pose the
    question: Can we identify the biased components of LLMs and mitigate their impact
    on label prediction? Therefore, we propose our UniBias method to Unveil and mitigate
    LLMs’ label Bias through internal attention and FFN manipulation. Notably, our
    method is proposed for decoder-only LLMs.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们揭示了各种偏差因素源于注意力头和 FFN 向量的偏差行为。自然地，我们提出了这样一个问题：我们是否可以识别 LLM 的偏见成分并减轻其对标签预测的影响？因此，我们提出了
    UniBias 方法，通过内部注意力和 FFN 操作来揭示和减轻 LLM 的标签偏差。值得注意的是，我们的方法是针对仅解码器的 LLM 提出的。
- en: 3.1 Biased FFN Vectors Identification
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 有偏见的 FFN 向量识别
- en: 'Identifying biased FFN vectors in LLMs hinges on whether the contribution of
    each FFN vector is independent and interpretable. As discussed in Section [2.1](#S2.SS1
    "2.1 Background ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation"), the
    output of an FFN layer can be cast as a linear combination of FFN vectors. Each
    FFN vector contributes to the final prediction by adding information encoded in
    its value vector, $\mathbf{v}_{i}^{\ell}$ can be interpreted through the logit
    lens, enabling us to interpret it as a distribution of logits across the vocabulary
    space.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '识别 LLM 中有偏见的 FFN 向量取决于每个 FFN 向量的贡献是否独立且可解释。如第 [2.1](#S2.SS1 "2.1 Background
    ‣ 2 Internal Mechanisms Causing the Bias of LLMs ‣ UniBias: Unveiling and Mitigating
    LLM Bias through Internal Attention and FFN Manipulation") 节所讨论，FFN 层的输出可以表示为
    FFN 向量的线性组合。每个 FFN 向量通过添加其值向量中编码的信息对最终预测做出贡献，$\mathbf{v}_{i}^{\ell}$ 可以通过 logit
    视角进行解释，使我们能够将其解释为词汇空间中的 logits 分布。'
- en: 'How to identify an FFN vector as biased? we assess whether it consistently
    introduces a biased preference towards specific labels into the residual stream,
    regardless of variations in the test samples. Such consistent biases can skew
    the LLM’s predictions. We introduce the following criteria to detect biased components
    in LLMs, which are also applicable for identifying biased attention heads:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如何识别一个 FFN 向量是否存在偏差？我们评估其是否在残差流中持续引入对特定标签的偏见，不受测试样本变化的影响。这种一致的偏见可能会扭曲 LLM 的预测。我们引入了以下标准来检测
    LLM 中的偏差成分，这些标准也适用于识别有偏见的注意力头：
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Relatedness Criterion: The information introduced by the FFN vector (or attention
    head) should closely relate to label prediction.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相关性标准：FFN 向量（或注意力头）引入的信息应与标签预测紧密相关。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Biased Criterion: The information contributed to the residual stream by the
    FFN vector (or attention head) exhibits a biased distribution, favoring certain
    labels over others.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏差标准：FFN 向量（或注意力头）对残差流的贡献表现出偏向某些标签的分布。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Low Variance Criterion: The label prediction information added by the FFN vector
    (or attention head) to the residual stream is almost identical across a set of
    test samples with different labels, i.e., exhibits very small variance.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 低方差标准：FFN 向量（或注意力头）添加到残差流中的标签预测信息在不同标签的一组测试样本中几乎相同，即表现出非常小的方差。
- en: The third criterion is key to identifying biased FFN vectors (or attention heads),
    as consistently low variance indicates that the FFN vector is not adequately responsive
    to varying inputs. Combined with the second criterion, this suggests a bias towards
    certain predictions regardless of the input’s contextual differences.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个标准对于识别有偏的FFN向量（或注意力头）至关重要，因为一致的低方差表明FFN向量对变化的输入响应不足。结合第二个标准，这表明对某些预测有偏，无论输入的上下文差异如何。
- en: To examine these criteria, we interpret the information contributed by each
    FFN vector, i.e., $m\mathbf{v}$ is fixed, changes in the FFN coefficient $m$ across
    different samples reflect the change in information brought by the FFN vector.
    We interpret this information by projecting each FFN value vector into the vocabulary
    space and analyzing the logit distribution over label tokens, termed *label logits*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查这些标准，我们解释每个FFN向量贡献的信息，即，$m\mathbf{v}$固定，FFN系数$m$在不同样本中的变化反映了FFN向量带来的信息变化。我们通过将每个FFN值向量投射到词汇空间中并分析标签令牌上的logit分布来解释这些信息，这称为*标签logits*。
- en: 'Specifically, given an FFN value vector $\mathbf{v}\in{\mathbb{R}^{d}}$ (where
    $c$-th sample can be obtained by:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，给定一个FFN值向量$\mathbf{v}\in{\mathbb{R}^{d}}$（其中第$c$-样本可以通过以下方式获得：
- en: '|  | $\mathbf{g}=\mathbf{v}\cdot E\cdot L^{\top}$ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{g}=\mathbf{v}\cdot E\cdot L^{\top}$ |  |'
- en: 'We use $p$, respectively. An FFN vector is considered biased if it meets the
    following conditions:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$p$，分别。如果FFN向量满足以下条件，则认为其有偏：
- en: '|  | $$\left\{\begin{aligned} &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(\mathbf{G}_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(\mathbf{g^{(k)}}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}g_{j}^{(k)}>th_{FFN}^{1}\\
    &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\mathbf{G}_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\mathbf{g^{(k)}}\right)=\frac{1}{p}\frac{1}{c}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}\left(g_{j}^{(k)}-\mu(\mathbf{g^{(k)}})\right)>th_{FFN}^{2}\\'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\left\{\begin{aligned} &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(\mathbf{G}_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(\mathbf{g^{(k)}}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}g_{j}^{(k)}>th_{FFN}^{1}\\
    &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\mathbf{G}_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\mathbf{g^{(k)}}\right)=\frac{1}{p}\frac{1}{c}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}\left(g_{j}^{(k)}-\mu(\mathbf{g^{(k)}})\right)>th_{FFN}^{2}\\'
- en: '&amp;CV\left(\mathbf{m}\right)=\frac{\sigma(\mathbf{m})}{\mu(\mathbf{m})}=\frac{\sqrt{\frac{1}{p}\sum_{j=0}^{p-1}\left(m_{k}-\mu(\mathbf{m})\right)^{2}}}{\frac{1}{p}\sum_{k=0}^{p-1}m_{k}}th_{Att}^{1}\\
    &amp;\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(A_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\textbf{a}^{(k)}\right)=\frac{1}{p}\frac{1}{c}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}\left(a_{j}^{(k)}-\mu(\mathbf{a}^{(k)})\right)>th_{Att}^{2}\\'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\left\{\begin{aligned} &\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(A_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Sum}\left(\mathbf{a}^{(k)}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\sum_{j=1}^{c}\mathbf{a}_{j}^{(k)}>th_{Att}^{1}\\
    &\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(A_{k,:}\right)=\frac{1}{p}\sum_{k=0}^{p-1}\text{Bias}\left(\textbf{a}^{(k)}\right)=\frac{1}{p}\frac{1}{c}\sum_{k=0}^{p-1}\sum_{j=0}^{c-1}\left(a_{j}^{(k)}-\mu(\mathbf{a}^{(k)})\right)>th_{Att}^{2}\\'
- en: '&amp;\sum_{j=0}^{c-1}w_{j}\cdot CV\left(A_{:,j}\right)=w_{j}\cdot\frac{\sigma(A_{:,j})}{\mu(A_{:,j})}<th_{Att}^{3}\end{aligned}\right.$$
    |  |'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&$\sum_{j=0}^{c-1}w_{j}\cdot CV\left(A_{:,j}\right)=w_{j}\cdot\frac{\sigma(A_{:,j})}{\mu(A_{:,j})}<th_{Att}^{3}\end{aligned}\right.$$
    |  |'
- en: where $w_{j}=\sum_{j=0}^{c-1}\frac{\mu(A_{:,j})}{\sum\mu(A_{:,j})}$.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w_{j}=\sum_{j=0}^{c-1}\frac{\mu(A_{:,j})}{\sum\mu(A_{:,j})}$。
- en: The functions of the first two criteria are identical to those for biased FFN
    vector identification. The third function is the weighted sum of the coefficient
    variance of each label across test samples. The thresholds for biased attention
    head identification are also derived by grid search.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个标准的函数与有偏FFN向量识别的函数相同。第三个函数是每个标签在测试样本中的系数方差的加权和。有偏注意力头识别的阈值也是通过网格搜索得出的。
- en: 3.3 Biased FFN Vectors and Attention Heads Manipulation
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 有偏FFN向量和注意力头操作
- en: After identifying the biased components of the LLM, we eliminate their influence
    by masking these biased FFN vectors and attention heads. Specifically, we create
    masks for the attention heads in each attention layer and reset the coefficient
    of the biased FFN vector and biased attention head mask.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别出LLM的有偏成分后，我们通过屏蔽这些有偏的FFN向量和注意力头来消除它们的影响。具体而言，我们为每个注意力层中的注意力头创建掩码，并重置有偏FFN向量和有偏注意力头掩码的系数。
- en: 3.4 Grid Searching
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 网格搜索
- en: 'Specifically, we utilize a small subset of training data as a support set,
    with 20 samples for each class. We then grid search all combinations of threshold
    values and select the combination that results in the most balanced distribution
    of average label logits. Specifically, let $\mathbf{T}$ that minimizes the bias
    of label logits: $t^{*}=\arg\min_{t\in\mathbf{T}}\text{Bias}(\mathbf{P}(t))$.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们利用一个小的训练数据子集作为支持集，每个类别20个样本。然后，我们对所有阈值组合进行网格搜索，并选择使平均标签对数分布最平衡的组合。具体而言，设
    $\mathbf{T}$ 使标签对数的偏差最小化：$t^{*}=\arg\min_{t\in\mathbf{T}}\text{Bias}(\mathbf{P}(t))$。
- en: It is noteworthy that although there are multiple combinations of thresholds,
    they usually result in a few set of different biased components. For example,
    for a grid search of thresholds of FFN vectors with 80 combinations, it only result
    in 4 different sets of biased FFN vectors that need to be examined with the support
    set on the SST-2 dataset. Additionally, during the inference stage of evaluating
    test samples, the computation time of the UniBias method is completely identical
    to that of the original LLMs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管阈值有多种组合，但通常会导致少量不同的偏差成分。例如，对于 FFN 向量的 80 种组合的网格搜索，它只会导致 4 种不同的偏差 FFN
    向量集，这些集需要在 SST-2 数据集上使用支持集进行检查。此外，在评估测试样本的推理阶段，UniBias 方法的计算时间与原始 LLM 完全相同。
- en: 4 Experiments
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we aims to investigate a few research questions (RQ). RQ 1:
    After eliminating biased components from LLMs, does the ICL performance improve
    compared to the original LLM? Additionally, how does our UniBias method compare
    to existing calibration methods? RQ 2: Given that ICL suffers from prompt brittleness,
    can our UniBias method contribute to more robust ICL performance? RQ 3: Are there
    any observable patterns of biased FFN vectors and attention heads within and across
    tasks? RQ 4: What is the performance of LLMs after eliminating only the biased
    FFN vectors and only the biased attention heads, respectively? RQ 5: What is the
    impact of support set size on the performance of the UniBias method?'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在探讨几个研究问题（RQ）。RQ 1：在消除 LLM 中的偏差成分后，ICL 的性能是否比原始 LLM 更好？此外，我们的 UniBias
    方法与现有的校准方法相比如何？RQ 2：鉴于 ICL 存在提示脆弱性，我们的 UniBias 方法能否有助于提高 ICL 性能的鲁棒性？RQ 3：在任务之间以及任务内部是否有可观察到的偏差
    FFN 向量和注意力头的模式？RQ 4：在仅消除偏差 FFN 向量和仅消除偏差注意力头的情况下，LLM 的性能如何？RQ 5：支持集的大小对 UniBias
    方法的性能有何影响？
- en: 4.1 Experimental Setup
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Datasets   We evaluate our UniBias method on 12 diverse natural language processing
    datasets across various tasks, including sentiment analysis, topic classification,
    natural language inference, reasoning, and word disambiguation. Statistics and
    details about the datasets can be found in Table [3](#A1.T3 "Table 3 ‣ A.1 Datasets
    ‣ Appendix A Experimental Details ‣ UniBias: Unveiling and Mitigating LLM Bias
    through Internal Attention and FFN Manipulation") in Appendix.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集   我们在 12 个不同的自然语言处理数据集上评估我们的 UniBias 方法，这些数据集涵盖情感分析、主题分类、自然语言推理、推理和词义消歧等任务。有关数据集的统计信息和详细信息请参见附录中的表格
    [3](#A1.T3 "Table 3 ‣ A.1 Datasets ‣ Appendix A Experimental Details ‣ UniBias:
    Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation")。'
- en: Baselines   In addition to the standard ICL, we compare our proposed UniBias
    with state-of-the-art LLM debiasing and calibration baselines, including Contextual
    Calibration (CC) [[29](#bib.bib29)], Domain-Context Calibration (DC) [[7](#bib.bib7)],
    and Prototypical Calibration (PC) [[10](#bib.bib10)]. We reproduce all baselines
    strictly follows the authors’ instructions and recommendations to ensure a fair
    comparison.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 基线   除了标准的 ICL，我们还将我们提出的 UniBias 与最先进的 LLM 去偏差和校准基线进行比较，包括上下文校准（CC） [[29](#bib.bib29)]、领域上下文校准（DC）
    [[7](#bib.bib7)] 和原型校准（PC） [[10](#bib.bib10)]。我们严格按照作者的说明和建议重新生成所有基线，以确保公平比较。
- en: 'Models and implementation details   We evaluate our method on Llama-2 7b and
    Llama-2 13b models [[21](#bib.bib21)]. For all experiments, unless stated otherwise,
    we use 1-shot ICL setting, i.e. one example per class, and repeat five times under
    different random seeds. We use $k=20$ sampes per class as the support set to obtain
    all threshold values by grid searching, as mentioned in the method section. The
    prompt template and more implementation details are specified in Appendix [A](#A1
    "Appendix A Experimental Details ‣ UniBias: Unveiling and Mitigating LLM Bias
    through Internal Attention and FFN Manipulation").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '模型和实施细节   我们在 Llama-2 7b 和 Llama-2 13b 模型上评估我们的方法 [[21](#bib.bib21)]。除非另有说明，否则所有实验使用
    1-shot ICL 设置，即每个类别一个示例，并在不同随机种子下重复五次。我们使用每个类别 $k=20$ 个样本作为支持集，通过网格搜索获得所有阈值，如方法部分所述。提示模板和更多实施细节在附录
    [A](#A1 "Appendix A Experimental Details ‣ UniBias: Unveiling and Mitigating LLM
    Bias through Internal Attention and FFN Manipulation") 中详细说明。'
- en: 'Table 1: Comparison of one-shot ICL performance for different methods across
    datasets using Llama-2 7b and Llama-2 13b models. The mean and standard deviation
    are reported for five repetitions with different ICL examples.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 使用 Llama-2 7b 和 Llama-2 13b 模型在不同数据集上进行的单次 ICL 性能比较。报告了五次重复的均值和标准差，使用了不同的
    ICL 示例。'
- en: '| Dataset | Llama-2 7b | Llama-2 13b |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | Llama-2 7b | Llama-2 13b |'
- en: '| --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method | ICL | CC | DC | PC | UniBias | ICL | CC | DC | PC | UniBias |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ICL | CC | DC | PC | UniBias | ICL | CC | DC | PC | UniBias |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SST-2 | 87.22[6.03] | 92.24[3.39] | 94.15[1.22] | 93.90[1.54] | 94.54[0.62]
    | 93.90[1.79] | 95.25[0.93] | 95.37[0.70] | 94.56[1.71] | 95.46[0.52] |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 87.22[6.03] | 92.24[3.39] | 94.15[1.22] | 93.90[1.54] | 94.54[0.62]
    | 93.90[1.79] | 95.25[0.93] | 95.37[0.70] | 94.56[1.71] | 95.46[0.52] |'
- en: '| MNLI | 53.83[2.22] | 53.36[3.16] | 52.19[2.55] | 45.38[5.01] | 54.97[0.88]
    | 62.43[1.49] | 63.89[0.81] | 61.86[1.23] | 57.47[3.53] | 64.65[2.73] |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| MNLI | 53.83[2.22] | 53.36[3.16] | 52.19[2.55] | 45.38[5.01] | 54.97[0.88]
    | 62.43[1.49] | 63.89[0.81] | 61.86[1.23] | 57.47[3.53] | 64.65[2.73] |'
- en: '| WiC | 50.00[0.16] | 52.19[2.00] | 52.40[1.69] | 57.11[2.49] | 53.71[1.16]
    | 54.48[3.19] | 50.63[1.73] | 49.72[0.30] | 55.67[1.67] | 57.93[1.70] |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| WiC | 50.00[0.16] | 52.19[2.00] | 52.40[1.69] | 57.11[2.49] | 53.71[1.16]
    | 54.48[3.19] | 50.63[1.73] | 49.72[0.30] | 55.67[1.67] | 57.93[1.70] |'
- en: '| COPA | 67.60[2.30] | 67.80[2.17] | 60.40[2.79] | 67.80[3.70] | 69.00[2.74]
    | 67.50[10.40] | 75.20[7.80] | 71.00[8.80] | 76.80[6.30] | 83.20[2.70] |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| COPA | 67.60[2.30] | 67.80[2.17] | 60.40[2.79] | 67.80[3.70] | 69.00[2.74]
    | 67.50[10.40] | 75.20[7.80] | 71.00[8.80] | 76.80[6.30] | 83.20[2.70] |'
- en: '| CR | 91.54[0.39] | 92.13[0.40] | 92.61[0.44] | 91.97[0.35] | 92.61[0.11]
    | 91.01[1.30] | 92.13[0.88] | 92.23[0.76] | 91.65[0.64] | 92.34[0.74] |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CR | 91.54[0.39] | 92.13[0.40] | 92.61[0.44] | 91.97[0.35] | 92.61[0.11]
    | 91.01[1.30] | 92.13[0.88] | 92.23[0.76] | 91.65[0.64] | 92.34[0.74] |'
- en: '| AGNews | 85.59[1.87] | 83.54[1.96] | 89.08[0.86] | 86.81[2.92] | 88.29[1.24]
    | 89.14[0.44] | 88.23[1.14] | 89.34[0.61] | 86.03[0.65] | 88.68[0.43] |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AGNews | 85.59[1.87] | 83.54[1.96] | 89.08[0.86] | 86.81[2.92] | 88.29[1.24]
    | 89.14[0.44] | 88.23[1.14] | 89.34[0.61] | 86.03[0.65] | 88.68[0.43] |'
- en: '| MR | 89.37[1.83] | 91.77[1.42] | 92.35[0.23] | 91.39[1.65] | 92.19[0.37]
    | 90.10[2.10] | 93.20[0.57] | 93.00[0.52] | 92.80[0.86] | 92.23[1.12] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| MR | 89.37[1.83] | 91.77[1.42] | 92.35[0.23] | 91.39[1.65] | 92.19[0.37]
    | 90.10[2.10] | 93.20[0.57] | 93.00[0.52] | 92.80[0.86] | 92.23[1.12] |'
- en: '| RTE | 66.21[7.30] | 64.33[3.68] | 65.49[2.09] | 62.59[4.71] | 67.65[6.44]
    | 76.10[4.73] | 71.99[5.02] | 66.21[1.09] | 75.31[2.90] | 78.23[2.13] |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RTE | 66.21[7.30] | 64.33[3.68] | 65.49[2.09] | 62.59[4.71] | 67.65[6.44]
    | 76.10[4.73] | 71.99[5.02] | 66.21[1.09] | 75.31[2.90] | 78.23[2.13] |'
- en: '| SST-5 | 46.97[0.87] | 51.36[1.69] | 51.92[1.77] | 55.41[1.51] | 53.79[1.46]
    | 51.03[1.25] | 47.20[1.69] | 48.98[2.11] | 53.63[0.95] | 51.80[1.00] |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SST-5 | 46.97[0.87] | 51.36[1.69] | 51.92[1.77] | 55.41[1.51] | 53.79[1.46]
    | 51.03[1.25] | 47.20[1.69] | 48.98[2.11] | 53.63[0.95] | 51.80[1.00] |'
- en: '| TREC | 72.92[12.42] | 76.44[3.21] | 77.16[3.94] | 74.92[5.78] | 80.80[3.17]
    | 74.70[12.10] | 83.80[3.86] | 80.50[9.07] | 81.85[9.53] | 81.25[6.86] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 72.92[12.42] | 76.44[3.21] | 77.16[3.94] | 74.92[5.78] | 80.80[3.17]
    | 74.70[12.10] | 83.80[3.86] | 80.50[9.07] | 81.85[9.53] | 81.25[6.86] |'
- en: '| ARC | 51.90[0.60] | 53.10[0.40] | 53.00[0.60] | 40.40[0.50] | 53.10[0.60]
    | 66.54[0.33] | 64.33[0.99] | 64.88[0.59] | 59.47[1.07] | 66.81[0.37] |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ARC | 51.90[0.60] | 53.10[0.40] | 53.00[0.60] | 40.40[0.50] | 53.10[0.60]
    | 66.54[0.33] | 64.33[0.99] | 64.88[0.59] | 59.47[1.07] | 66.81[0.37] |'
- en: '| MMLU | 41.73[2.25] | 43.72[0.97] | 43.57[1.38] | 34.12[3.41] | 44.83[0.24]
    | 53.53[1.55] | 50.84[1.57] | 51.81[1.24] | 45.50[1.65] | 53.55[1.05] |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 41.73[2.25] | 43.72[0.97] | 43.57[1.38] | 34.12[3.41] | 44.83[0.24]
    | 53.53[1.55] | 50.84[1.57] | 51.81[1.24] | 45.50[1.65] | 53.55[1.05] |'
- en: '| Avg. | 67.07 | 68.49 | 68.70 | 66.81 | 70.46 | 72.54 | 73.06 | 72.08 | 72.56
    | 75.51 | ![Refer to caption](img/f29f4bc90f8ef4459ddd889a7bfe1e0f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '| 平均 | 67.07 | 68.49 | 68.70 | 66.81 | 70.46 | 72.54 | 73.06 | 72.08 | 72.56
    | 75.51 | ![参见说明](img/f29f4bc90f8ef4459ddd889a7bfe1e0f.png)'
- en: 'Figure 5: The performance comparison under different numbers of ICL shots.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 不同数量 ICL 训练样本下的性能比较。'
- en: 4.2 Main Experiments
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要实验
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ UniBias:
    Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation")
    presents the performance of various datasets and model sizes under the 1-shot
    setting. Our proposed UniBias method consistently achieves the highest accuracies
    in most cases. In terms of overall average accuracy, UniBias improves upon the
    standard ICL by a substantial margin of 3.39% and exceeds the state-of-the-art
    (SOTA) DC by 1.76% using Llama-2 7b. With Llama-2 13b, UniBias surpasses the standard
    ICL and the SOTA CC by 2.97% and 2.45%, respectively. Figure [5](#S4.F5 "Figure
    5 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ UniBias: Unveiling and Mitigating
    LLM Bias through Internal Attention and FFN Manipulation") further illustrates
    the results under zero-shot and various few-shot settings for COPA, SST2, and
    MMLU. Our proposed UniBias consistently surpasses other baselines in all scenarios,
    underscoring its effectiveness.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S4.T1 "表 1 ‣ 4.1 实验设置 ‣ 4 实验 ‣ UniBias：通过内部注意力和FFN操作揭示和缓解LLM偏差") 展示了在1-shot设置下各种数据集和模型尺寸的性能。我们提出的UniBias方法在大多数情况下持续实现了最高的准确率。在整体平均准确率方面，UniBias在Llama-2
    7b上比标准ICL提高了3.39%，并超越了当前最先进的（SOTA）DC 1.76%。在Llama-2 13b上，UniBias分别超越了标准ICL和SOTA
    CC 2.97%和2.45%。图 [5](#S4.F5 "图 5 ‣ 4.1 实验设置 ‣ 4 实验 ‣ UniBias：通过内部注意力和FFN操作揭示和缓解LLM偏差")
    进一步展示了COPA、SST2和MMLU在零-shot和各种少量-shot设置下的结果。我们提出的UniBias在所有场景中始终超越其他基线，突显了其有效性。
- en: In response to RQ 1, UniBias not only enhances the performance of original LLMs
    but also outperforms existing methods. We attribute this success to its internal
    analysis and bias mitigation techniques, which leverage FFNs and attentions, unlike
    other methods that rely solely on external observations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 针对RQ 1，UniBias不仅提升了原始LLM的性能，还超越了现有的方法。我们将这种成功归因于其内部分析和偏差缓解技术，这些技术利用了FFNs和注意力机制，而不是仅仅依赖外部观察的方法。
- en: 4.3 Alleviating Prompt Brittleness
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 缓解提示脆弱性
- en: Existing studies have found that LLMs are prone to prompt brittleness, with
    various factors such as the selection and order of examples, as well as the prompt
    formatting. To address RQ 2, we simulate these brittle scenarios by choosing different
    demonstration samples, using different prompt formats, and changing the example
    order to observe variations in LLM performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现有研究发现，LLMs容易出现提示脆弱性，这与示例的选择和顺序以及提示格式有关。为了回应RQ 2，我们通过选择不同的示例样本、使用不同的提示格式和改变示例顺序来模拟这些脆弱情景，从而观察LLM性能的变化。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ UniBias: Unveiling and Mitigating
    LLM Bias through Internal Attention and FFN Manipulation") presents Llama-2 7b’s
    performance both with and without UniBias. Without UniBias, the standard ICL’s
    performance varies significantly, ranging from 8% to 26%, demonstrating its instability.
    After applying UniBias, the accuracy stabilizes, with variations consistently
    less than 4% under perturbations of various design settings. This evidence verifies
    that UniBias effectively reduces prompt brittleness and enhances robustness.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ UniBias：通过内部注意力和FFN操作揭示和缓解LLM偏差") 展示了Llama-2 7b在使用和不使用UniBias时的性能。没有UniBias时，标准ICL的性能变化显著，范围从8%到26%，表现出其不稳定性。在应用UniBias后，准确率稳定，变化始终小于4%，在不同设计设置的扰动下也保持一致。这一证据验证了UniBias有效减少了提示的脆弱性并增强了鲁棒性。
- en: 'Table 2: Performance comparison of only removing biased FFN vectors (FFN-only),
    only removing biased attention heads (attention-only), our Unibias method, and
    the ICL of original LLM.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：仅去除偏见FFN向量（仅FFN）、仅去除偏见注意力头（仅注意力）、我们的方法UniBias和原始LLM的ICL的性能比较
- en: '| Method | SST-2 | MNLI | WiC | COPA | CR | AGNews | MR | RTE | SST-5 | TREC
    | ARC | MMLU |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SST-2 | MNLI | WiC | COPA | CR | AGNews | MR | RTE | SST-5 | TREC |
    ARC | MMLU |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| ICL | 87.22 | 53.83 | 50.00 | 67.60 | 91.54 | 85.59 | 89.37 | 66.21 | 46.97
    | 72.92 | 51.90 | 41.73 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ICL | 87.22 | 53.83 | 50.00 | 67.60 | 91.54 | 85.59 | 89.37 | 66.21 | 46.97
    | 72.92 | 51.90 | 41.73 |'
- en: '| FFN-only | 94.17 | 54.59 | 50.88 | 69.20 | 92.57 | 85.52 | 91.78 | 67.33
    | 47.09 | 73.04 | 51.92 | 42.62 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 仅FFN | 94.17 | 54.59 | 50.88 | 69.20 | 92.57 | 85.52 | 91.78 | 67.33 | 47.09
    | 73.04 | 51.92 | 42.62 |'
- en: '| Attention-only | 94.22 | 52.83 | 52.76 | 68.50 | 91.49 | 86.25 | 92.61 |
    66.55 | 52.68 | 80.68 | 53.00 | 44.67 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 仅注意力 | 94.22 | 52.83 | 52.76 | 68.50 | 91.49 | 86.25 | 92.61 | 66.55 | 52.68
    | 80.68 | 53.00 | 44.67 |'
- en: '| UniBias | 94.54 | 54.97 | 53.71 | 69.00 | 92.61 | 88.29 | 92.19 | 67.65 |
    53.79 | 80.80 | 53.10 | 44.83 | ![Refer to caption](img/5d3b58c348b5a8bbc294a6ed892251e8.png)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '| UniBias | 94.54 | 54.97 | 53.71 | 69.00 | 92.61 | 88.29 | 92.19 | 67.65 |
    53.79 | 80.80 | 53.10 | 44.83 | ![参见说明](img/5d3b58c348b5a8bbc294a6ed892251e8.png)'
- en: 'Figure 6: Analysis of biased attention heads (AHs) and FFN vectors (FFNs).
    The frequency count of biased LLM components across five repeat experiments with
    different example selections is reported.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：有偏见注意力头（AHs）和FFN向量（FFNs）的分析。报告了在五次重复实验中不同示例选择的有偏见LLM组件的频率计数。
- en: 4.4 Biased LLM Components Analysis
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 有偏见LLM组件分析
- en: 'In response to RQ3, we present the frequency counts of identified biased attention
    heads (AHs) and FFNs under repeated experiments in Figure [6](#S4.F6 "Figure 6
    ‣ 4.3 Alleviating Prompt Brittleness ‣ 4 Experiments ‣ UniBias: Unveiling and
    Mitigating LLM Bias through Internal Attention and FFN Manipulation"). A large
    frequency count for an LLM component indicates a higher repeat of being identified
    as biased in the corresponding dataset. The first subfigure displays the biased
    components for various example selections, revealing several commonly biased LLM
    components across different prompts within a single dataset. The second subfigure
    highlights the common biased components across different datasets (ARC and MMLU)
    for the reasoning task, indicating that different datasets with similar tasks
    could share common biased LLM components. The third subfigure demonstrates the
    presence of common biased LLM components across different tasks. Experimental
    results suggest an interesting future direction: we may identify global biased
    components that would mitigate bias across multiple tasks and diverse prompt design
    settings.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '针对RQ3，我们在图[6](#S4.F6 "图6 ‣ 4.3 缓解提示脆弱性 ‣ 4 实验 ‣ UniBias: 通过内部注意力和FFN操作揭示与减轻LLM偏见")中展示了在重复实验下识别的有偏见注意力头（AHs）和FFN的频率统计。LLM组件的大频率计数表示在相应数据集中被识别为有偏见的重复次数较高。第一个子图展示了不同示例选择的有偏见组件，揭示了在单一数据集内不同提示中的一些常见有偏见LLM组件。第二个子图突出了在不同数据集（ARC和MMLU）中推理任务的常见有偏见组件，表明具有相似任务的不同数据集可能共享相似的有偏见LLM组件。第三个子图展示了在不同任务中的常见有偏见LLM组件。实验结果表明了一个有趣的未来方向：我们可能会识别出全球性的有偏见组件，以减轻多任务和多样化提示设计设置中的偏见。'
- en: 4.5 Ablations
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 消融实验
- en: 'We conduct ablation studies to analyze the impact of exclusively eliminating
    biased AHs or FFNs to address RQ 4. Table [2](#S4.T2 "Table 2 ‣ 4.3 Alleviating
    Prompt Brittleness ‣ 4 Experiments ‣ UniBias: Unveiling and Mitigating LLM Bias
    through Internal Attention and FFN Manipulation") presents the results of removing
    only biased FFN vectors (FFN-only) and only biased attention heads (attention-only).
    Both FFN-only and attention-only methods outperform the standard ICL, demonstrating
    their effectiveness. When combined as UniBias, the method achieves the best results
    across most datasets, indicating that the two approaches are complementary.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行消融研究，以分析专门消除有偏见的注意力头（AHs）或前馈网络（FFNs）的影响，以解决RQ 4。表[2](#S4.T2 "表2 ‣ 4.3 缓解提示脆弱性
    ‣ 4 实验 ‣ UniBias: 通过内部注意力和FFN操作揭示与减轻LLM偏见")展示了仅移除有偏见的FFN向量（仅FFN）和仅移除有偏见的注意力头（仅注意力）的结果。无论是仅FFN还是仅注意力方法，都优于标准ICL，显示了它们的有效性。结合为UniBias时，该方法在大多数数据集上取得了最佳结果，表明这两种方法是互补的。'
- en: 'Additionally, we further conduct experiments to investigate the impact of support
    set size (RQ 5), which is detailed in Appendix [C](#A3 "Appendix C Impact of support
    set size ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention
    and FFN Manipulation").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们进一步进行实验以调查支持集大小的影响（RQ 5），详细信息见附录[C](#A3 "附录C 支持集大小的影响 ‣ UniBias: 通过内部注意力和FFN操作揭示与减轻LLM偏见")。'
- en: 5 Related Work
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 'Bias in LLMs:   It is well recognized that LLMs are unstable under various
    ICL design settings, and this instability arises from biases in LLMs toward predicting
    certain answers [[29](#bib.bib29), [14](#bib.bib14)]. To understand these biases,
    existing studies have identified various bias factors, including recency bias,
    majority label bias, common token bias [[29](#bib.bib29)], and domain label bias
    [[7](#bib.bib7)] in classification tasks. More recently, selection bias, which
    consistently favors specific options in multiple-choice questions, has also been
    identified [[30](#bib.bib30), [25](#bib.bib25)]. To address these biases, several
    calibration methods have been proposed, including contextual calibration [[29](#bib.bib29)],
    domain-context calibration [[7](#bib.bib7)], and prototypical calibration [[10](#bib.bib10)].
    However, these identified bias factors and calibration methods are derived from
    external observations or adjustments of LLM outputs, leaving the underlying mechanisms
    within LLMs that cause such biases poorly understood.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: LLM中的偏见：   众所周知，LLM在各种ICL设计设置下是不稳定的，这种不稳定性源于LLM对预测特定答案的偏见[[29](#bib.bib29),
    [14](#bib.bib14)]。为了理解这些偏见，现有研究已识别出各种偏见因素，包括近期偏见、多数标签偏见、常见标记偏见[[29](#bib.bib29)]和领域标签偏见[[7](#bib.bib7)]。最近，还识别出了选择偏见，这种偏见在多项选择题中始终倾向于特定选项[[30](#bib.bib30),
    [25](#bib.bib25)]。为了解决这些偏见，提出了几种校准方法，包括上下文校准[[29](#bib.bib29)]、领域上下文校准[[7](#bib.bib7)]和原型校准[[10](#bib.bib10)]。然而，这些识别出的偏见因素和校准方法源于对LLM输出的外部观察或调整，对导致这些偏见的LLM内部机制了解仍不够深入。
- en: 'Mechanistic Interpretability:   Mechanistic interpretability [[6](#bib.bib6),
    [23](#bib.bib23)] aims to explain the internal processes in language models, facilitating
    the interpretation of the contributions of individual model components to the
    final prediction. Our work builds on the understanding of the residual stream
    [[6](#bib.bib6)], the logit lens [[16](#bib.bib16)], and the interpretation of
    LLM components in the vocabulary space [[4](#bib.bib4), [8](#bib.bib8)].'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 机制可解释性：   机制可解释性[[6](#bib.bib6), [23](#bib.bib23)]旨在解释语言模型中的内部过程，促进对个体模型组件对最终预测贡献的解释。我们的工作建立在对残差流[[6](#bib.bib6)]、logit镜头[[16](#bib.bib16)]以及词汇空间中LLM组件解释[[4](#bib.bib4),
    [8](#bib.bib8)]的理解基础上。
- en: 6 Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we have deepened the understanding of biases in LLMs by unveiling
    the internal mechanisms that contribute to various bias factors. Building on this
    understanding, we proposed our UniBias method to mitigate these biases by identifying
    and eliminating biased FFN vectors and attention heads, demonstrating an effective
    way to manipulate the internal structures of LLMs. Extensive experiments show
    that our UniBias method achieves state-of-the-art performance across 12 NLP datasets
    and different ICL settings. Additionally, our method successfully alleviates prompt
    brittleness and enhances the robustness of ICL.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们通过揭示导致各种偏见因素的内部机制，深化了对LLM中偏见的理解。基于这一理解，我们提出了UniBias方法，通过识别和消除偏见的FFN向量和注意力头来缓解这些偏见，展示了一种有效操控LLM内部结构的方法。大量实验表明，我们的UniBias方法在12个NLP数据集和不同ICL设置中实现了最先进的性能。此外，我们的方法成功减轻了提示脆弱性，并增强了ICL的鲁棒性。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等人。语言模型是少样本学习者。*神经信息处理系统的进展*，33:1877–1901，2020年。
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick 和 Oyvind Tafjord。认为你已经解决了问答问题？试试arc，AI2推理挑战。*arXiv预印本
    arXiv:1803.05457*，2018年。
- en: Dagan et al. [2005] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal
    recognising textual entailment challenge. In *Machine learning challenges workshop*,
    pages 177–190\. Springer, 2005.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dagan et al. [2005] Ido Dagan, Oren Glickman 和 Bernardo Magnini。Pascal 识别文本蕴涵挑战。在
    *机器学习挑战研讨会*，第177–190页。Springer，2005年。
- en: 'Dar et al. [2023] Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing
    transformers in embedding space. In Anna Rogers, Jordan Boyd-Graber, and Naoaki
    Okazaki, editors, *Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, pages 16124–16170, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.893.
    URL [https://aclanthology.org/2023.acl-long.893](https://aclanthology.org/2023.acl-long.893).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dar 等 [2023] Guy Dar, Mor Geva, Ankit Gupta 和 Jonathan Berant. 在嵌入空间中分析变换器。在
    Anna Rogers, Jordan Boyd-Graber 和 Naoaki Okazaki 编辑的 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*
    中，页码 16124–16170，加拿大多伦多，2023年7月。计算语言学协会。doi: 10.18653/v1/2023.acl-long.893。网址
    [https://aclanthology.org/2023.acl-long.893](https://aclanthology.org/2023.acl-long.893)。'
- en: Dong et al. [2023] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning,
    2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 [2023] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li 和 Zhifang Sui. 关于上下文学习的综述，2023年。
- en: Elhage et al. [2021] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan,
    Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
    Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,
    Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown,
    Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework
    for transformer circuits. *Transformer Circuits Thread*, 2021. https://transformer-circuits.pub/2021/framework/index.html.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhage 等 [2021] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas
    Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma,
    Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
    Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared
    Kaplan, Sam McCandlish 和 Chris Olah. 变换器电路的数学框架。*变换器电路主题*，2021年。https://transformer-circuits.pub/2021/framework/index.html。
- en: 'Fei et al. [2023] Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. Mitigating
    label biases for in-context learning. In Anna Rogers, Jordan Boyd-Graber, and
    Naoaki Okazaki, editors, *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 14014–14031, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.783.
    URL [https://aclanthology.org/2023.acl-long.783](https://aclanthology.org/2023.acl-long.783).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fei 等 [2023] Yu Fei, Yifan Hou, Zeming Chen 和 Antoine Bosselut. 缓解上下文学习中的标签偏差。在
    Anna Rogers, Jordan Boyd-Graber 和 Naoaki Okazaki 编辑的 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*
    中，页码 14014–14031，加拿大多伦多，2023年7月。计算语言学协会。doi: 10.18653/v1/2023.acl-long.783。网址
    [https://aclanthology.org/2023.acl-long.783](https://aclanthology.org/2023.acl-long.783)。'
- en: 'Geva et al. [2021] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    Transformer feed-forward layers are key-value memories. In Marie-Francine Moens,
    Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, *Proceedings of
    the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    5484–5495, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL [https://aclanthology.org/2021.emnlp-main.446](https://aclanthology.org/2021.emnlp-main.446).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Geva 等 [2021] Mor Geva, Roei Schuster, Jonathan Berant 和 Omer Levy. 变换器前馈层是键值记忆。在
    Marie-Francine Moens, Xuanjing Huang, Lucia Specia 和 Scott Wen-tau Yih 编辑的 *2021年自然语言处理实证方法会议论文集*
    中，页码 5484–5495，在线及多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。doi: 10.18653/v1/2021.emnlp-main.446。网址
    [https://aclanthology.org/2021.emnlp-main.446](https://aclanthology.org/2021.emnlp-main.446)。'
- en: 'Geva et al. [2022] Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg.
    Transformer feed-forward layers build predictions by promoting concepts in the
    vocabulary space. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
    *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*,
    pages 30–45, Abu Dhabi, United Arab Emirates, December 2022\. Association for
    Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.3. URL [https://aclanthology.org/2022.emnlp-main.3](https://aclanthology.org/2022.emnlp-main.3).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Geva 等 [2022] Mor Geva, Avi Caciularu, Kevin Wang 和 Yoav Goldberg. 变换器前馈层通过在词汇空间中提升概念来构建预测。在
    Yoav Goldberg, Zornitsa Kozareva 和 Yue Zhang 编辑的 *2022年自然语言处理实证方法会议论文集* 中，页码 30–45，阿布扎比，阿联酋，2022年12月。计算语言学协会。doi:
    10.18653/v1/2022.emnlp-main.3。网址 [https://aclanthology.org/2022.emnlp-main.3](https://aclanthology.org/2022.emnlp-main.3)。'
- en: Han et al. [2023] Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei.
    Prototypical calibration for few-shot learning of language models. In *The Eleventh
    International Conference on Learning Representations*, 2023.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 [2023] Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun 和 Furu Wei. 语言模型的少样本学习的原型校准。发表于
    *第十一届国际学习表示会议*，2023年。
- en: 'Hanna et al. [2023] Michael Hanna, Ollie Liu, and Alexandre Variengien. How
    does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained
    language model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,
    editors, *Advances in Neural Information Processing Systems*, volume 36, pages
    76033–76060\. Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/efbba7719cc5172d175240f24be11280-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/efbba7719cc5172d175240f24be11280-Paper-Conference.pdf).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanna 等 [2023] Michael Hanna, Ollie Liu 和 Alexandre Variengien. GPT-2 如何计算大于？：解释预训练语言模型的数学能力。发表于
    A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt 和 S. Levine 主编的 *神经信息处理系统进展*，第36卷，第76033–76060页，Curran
    Associates, Inc.，2023年。网址 [https://proceedings.neurips.cc/paper_files/paper/2023/file/efbba7719cc5172d175240f24be11280-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/efbba7719cc5172d175240f24be11280-Paper-Conference.pdf)。
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *International Conference on Learning Representations*, 2020.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song 和 Jacob Steinhardt. 测量大规模多任务语言理解。发表于 *国际学习表示会议*，2020年。
- en: Hu and Liu [2004] Minqing Hu and Bing Liu. Mining and summarizing customer reviews.
    In *Proceedings of the tenth ACM SIGKDD international conference on Knowledge
    discovery and data mining*, pages 168–177, 2004.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 和 Liu [2004] Minqing Hu 和 Bing Liu. 挖掘和总结客户评论。发表于 *第十届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，第168–177页，2004年。
- en: 'Lu et al. [2022] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming
    few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline
    Villavicencio, editors, *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 8086–8098, Dublin,
    Ireland, May 2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556.
    URL [https://aclanthology.org/2022.acl-long.556](https://aclanthology.org/2022.acl-long.556).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等 [2022] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel 和 Pontus
    Stenetorp. 奇妙排序的提示及其发现：克服少样本提示顺序敏感性。发表于 Smaranda Muresan, Preslav Nakov 和 Aline
    Villavicencio 主编的 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*，第8086–8098页，爱尔兰都柏林，2022年5月。计算语言学协会。doi:
    10.18653/v1/2022.acl-long.556。网址 [https://aclanthology.org/2022.acl-long.556](https://aclanthology.org/2022.acl-long.556)。'
- en: 'Min et al. [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and
    Yue Zhang, editors, *Proceedings of the 2022 Conference on Empirical Methods in
    Natural Language Processing*, pages 11048–11064, Abu Dhabi, United Arab Emirates,
    December 2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.759.
    URL [https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Min 等 [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi 和 Luke Zettlemoyer. 重新思考示范的角色：是什么使上下文学习有效？发表于 Yoav Goldberg,
    Zornitsa Kozareva 和 Yue Zhang 主编的 *2022年自然语言处理实证方法会议论文集*，第11048–11064页，阿布扎比，阿拉伯联合酋长国，2022年12月。计算语言学协会。doi:
    10.18653/v1/2022.emnlp-main.759。网址 [https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759)。'
- en: 'Nostalgebraist [2020] Nostalgebraist. Interpreting gpt: the logit lens, 2020.
    URL [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nostalgebraist [2020] Nostalgebraist. 解释GPT：logit镜头，2020年。网址 [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)。
- en: 'Pang and Lee [2005] Bo Pang and Lillian Lee. Seeing stars: Exploiting class
    relationships for sentiment categorization with respect to rating scales. In Kevin
    Knight, Hwee Tou Ng, and Kemal Oflazer, editors, *Proceedings of the 43rd Annual
    Meeting of the Association for Computational Linguistics (ACL’05)*, pages 115–124,
    Ann Arbor, Michigan, June 2005\. Association for Computational Linguistics. doi:
    10.3115/1219840.1219855. URL [https://aclanthology.org/P05-1015](https://aclanthology.org/P05-1015).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pang and Lee [2005] Bo Pang 和 Lillian Lee。星级评分：利用类别关系进行基于评分量表的情感分类。在 Kevin
    Knight、Hwee Tou Ng 和 Kemal Oflazer 编辑的 *第43届计算语言学协会年会会议论文集（ACL’05）* 中，第115–124页，美国密歇根州安娜堡，2005年6月。计算语言学协会。doi:
    10.3115/1219840.1219855。网址 [https://aclanthology.org/P05-1015](https://aclanthology.org/P05-1015)。'
- en: 'Pilehvar and Camacho-Collados [2019] Mohammad Taher Pilehvar and Jose Camacho-Collados.
    WiC: the word-in-context dataset for evaluating context-sensitive meaning representations.
    In Jill Burstein, Christy Doran, and Thamar Solorio, editors, *Proceedings of
    the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    1267–1273, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.
    doi: 10.18653/v1/N19-1128. URL [https://aclanthology.org/N19-1128](https://aclanthology.org/N19-1128).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pilehvar and Camacho-Collados [2019] Mohammad Taher Pilehvar 和 Jose Camacho-Collados。WiC：用于评估上下文敏感意义表示的词汇-上下文数据集。在
    Jill Burstein、Christy Doran 和 Thamar Solorio 编辑的 *2019年北美计算语言学协会年会会议论文集：人类语言技术，第1卷（长文和短文）*
    中，第1267–1273页，明尼苏达州明尼阿波利斯，2019年6月。计算语言学协会。doi: 10.18653/v1/N19-1128。网址 [https://aclanthology.org/N19-1128](https://aclanthology.org/N19-1128)。'
- en: 'Roemmele et al. [2011] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S
    Gordon. Choice of plausible alternatives: An evaluation of commonsense causal
    reasoning. In *2011 AAAI Spring Symposium Series*, 2011.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roemmele et al. [2011] Melissa Roemmele、Cosmin Adrian Bejan 和 Andrew S Gordon。合理选择的备选项：对常识因果推理的评估。发表于
    *2011 AAAI 春季研讨会系列*，2011年。
- en: Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy
    Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, *Proceedings
    of the 2013 Conference on Empirical Methods in Natural Language Processing*, pages
    1631–1642, Seattle, Washington, USA, October 2013\. Association for Computational
    Linguistics. URL [https://aclanthology.org/D13-1170](https://aclanthology.org/D13-1170).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. [2013] Richard Socher、Alex Perelygin、Jean Wu、Jason Chuang、Christopher
    D. Manning、Andrew Ng 和 Christopher Potts。用于情感树库的递归深度模型进行语义组合。由 David Yarowsky、Timothy
    Baldwin、Anna Korhonen、Karen Livescu 和 Steven Bethard 编辑，发表于 *2013年自然语言处理实证方法会议论文集*，第1631–1642页，美国华盛顿州西雅图，2013年10月。计算语言学协会。网址
    [https://aclanthology.org/D13-1170](https://aclanthology.org/D13-1170)。
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. [2023] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad
    Almahairi、Yasmine Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti
    Bhosale 等。Llama 2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023年。
- en: Voorhees and Tice [2000] Ellen M Voorhees and Dawn M Tice. Building a question
    answering test collection. In *Proceedings of the 23rd annual international ACM
    SIGIR conference on Research and development in information retrieval*, pages
    200–207, 2000.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voorhees and Tice [2000] Ellen M Voorhees 和 Dawn M Tice。构建问答测试集合。发表于 *第23届国际ACM
    SIGIR信息检索研究与发展大会*，第200–207页，2000年。
- en: 'Wang et al. [2022] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck
    Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect
    object identification in gpt-2 small. In *The Eleventh International Conference
    on Learning Representations*, 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022] Kevin Ro Wang、Alexandre Variengien、Arthur Conmy、Buck Shlegeris
    和 Jacob Steinhardt。野外的可解释性：GPT-2 Small中的间接对象识别电路。发表于 *第十一届国际学习表征会议*，2022年。
- en: 'Wang et al. [2023a] Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong
    Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective
    for understanding in-context learning. In Houda Bouamor, Juan Pino, and Kalika
    Bali, editors, *Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing*, pages 9840–9855, Singapore, December 2023a. Association
    for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.609. URL [https://aclanthology.org/2023.emnlp-main.609](https://aclanthology.org/2023.emnlp-main.609).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2023a] 李安、李磊、戴大麦、陈德力、周浩、孟凡东、周杰和孙旭。标签词是锚点：一种从信息流的角度理解上下文学习的方法。见霍达·布阿莫尔、胡安·皮诺和卡利卡·巴利编，《2023年自然语言处理领域实证方法会议论文集》，页9840–9855，新加坡，2023年12月。计算语言学协会。doi:
    10.18653/v1/2023.emnlp-main.609。网址 [https://aclanthology.org/2023.emnlp-main.609](https://aclanthology.org/2023.emnlp-main.609)。'
- en: Wang et al. [2023b] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not
    fair evaluators. *arXiv preprint arXiv:2305.17926*, 2023b.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023b] 王佩怡、李磊、陈梁、朱大伟、林冰怀、曹云博、刘琪、刘天宇和隋志芳。大型语言模型不是公平的评估者。*arXiv 预印本 arXiv:2305.17926*，2023b。
- en: 'Williams et al. [2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A
    broad-coverage challenge corpus for sentence understanding through inference.
    In Marilyn Walker, Heng Ji, and Amanda Stent, editors, *Proceedings of the 2018
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1112–1122,
    New Orleans, Louisiana, June 2018\. Association for Computational Linguistics.
    doi: 10.18653/v1/N18-1101. URL [https://aclanthology.org/N18-1101](https://aclanthology.org/N18-1101).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Williams 等人 [2018] 阿迪娜·威廉姆斯、尼基塔·南贾和塞缪尔·博曼。用于通过推理理解句子的广覆盖挑战语料库。见玛丽莲·沃克、亨格·吉和阿曼达·斯坦特编，《2018年北美计算语言学协会：人类语言技术会议论文集，第1卷（长篇论文）》，页1112–1122，美国路易斯安那州新奥尔良，2018年6月。计算语言学协会。doi:
    10.18653/v1/N18-1101。网址 [https://aclanthology.org/N18-1101](https://aclanthology.org/N18-1101)。'
- en: 'Yu et al. [2023] Qinan Yu, Jack Merullo, and Ellie Pavlick. Characterizing
    mechanisms for factual recall in language models. In Houda Bouamor, Juan Pino,
    and Kalika Bali, editors, *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*, pages 9924–9959, Singapore, December 2023\. Association
    for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.615. URL [https://aclanthology.org/2023.emnlp-main.615](https://aclanthology.org/2023.emnlp-main.615).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 [2023] 于其南、杰克·梅鲁洛和艾莉·帕夫利克。表征语言模型中事实回忆的机制。见霍达·布阿莫尔、胡安·皮诺和卡利卡·巴利编，《2023年自然语言处理领域实证方法会议论文集》，页9924–9959，新加坡，2023年12月。计算语言学协会。doi:
    10.18653/v1/2023.emnlp-main.615。网址 [https://aclanthology.org/2023.emnlp-main.615](https://aclanthology.org/2023.emnlp-main.615)。'
- en: Zhang et al. [2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level
    convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee,
    M. Sugiyama, and R. Garnett, editors, *Advances in Neural Information Processing
    Systems*, volume 28\. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2015] 张翔、赵俊博和扬·勒昆。用于文本分类的字符级卷积网络。见C. 科尔特斯、N. 劳伦斯、D. 李、M. 杉山和R. 加内特编，《神经信息处理系统进展》，第28卷。Curran
    Associates, Inc.，2015年。网址 [https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf)。
- en: 'Zhao et al. [2021] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. Calibrate before use: Improving few-shot performance of language models.
    In Marina Meila and Tong Zhang, editors, *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning
    Research*, pages 12697–12706\. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/zhao21c.html](https://proceedings.mlr.press/v139/zhao21c.html).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2021] 赵子豪、埃里克·沃勒斯、石峰、丹·克莱因和萨米尔·辛格。使用前校准：提高语言模型的少量样本性能。见玛丽娜·梅拉和汤姆·张编，《第38届国际机器学习大会论文集》，《机器学习研究论文集》第139卷，页12697–12706。PMLR，2021年7月18–24日。网址
    [https://proceedings.mlr.press/v139/zhao21c.html](https://proceedings.mlr.press/v139/zhao21c.html)。
- en: Zheng et al. [2023] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie
    Huang. Large language models are not robust multiple choice selectors. In *The
    Twelfth International Conference on Learning Representations*, 2023.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 [2023] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, 和 Minlie Huang。大语言模型不是鲁棒的多项选择选择器。发表于
    *第十二届国际学习表征会议*，2023。
- en: Appendix A Experimental Details
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实验细节
- en: A.1 Datasets
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数据集
- en: 'We evaluate our Unibias method using 12 diverse natural language processing
    datasets across tasks such as sentiment analysis, topic classification, reasoning,
    natural language inference, and word disambiguation, as presented in Table [3](#A1.T3
    "Table 3 ‣ A.1 Datasets ‣ Appendix A Experimental Details ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation"). In
    our experiments, we utilize $k$-shot ICL. For testing, we randomly select 2000
    samples for MMLU and 3000 samples for MNLI and MR, while employing the original
    testing sets for other datasets. Detailed dataset statistics are available in
    Table [3](#A1.T3 "Table 3 ‣ A.1 Datasets ‣ Appendix A Experimental Details ‣ UniBias:
    Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation").'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 12 个不同的自然语言处理数据集来评估我们的 Unibias 方法，涉及任务包括情感分析、主题分类、推理、自然语言推断和词义消歧，如表 [3](#A1.T3
    "Table 3 ‣ A.1 Datasets ‣ Appendix A Experimental Details ‣ UniBias: Unveiling
    and Mitigating LLM Bias through Internal Attention and FFN Manipulation") 所示。在我们的实验中，我们使用
    $k$-shot ICL。测试时，我们随机选择 2000 个 MMLU 样本和 3000 个 MNLI 和 MR 样本，同时对其他数据集使用原始测试集。详细的数据集统计信息请参见表
    [3](#A1.T3 "Table 3 ‣ A.1 Datasets ‣ Appendix A Experimental Details ‣ UniBias:
    Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation")。'
- en: 'Table 3: Detailed Dataset information'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 详细的数据集信息'
- en: '| Dataset | # Classes | # Testing Size |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类别数量 | 测试样本数量 |'
- en: '| Sentiment classification |  |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 情感分类 |  |  |'
- en: '| SST2 [[20](#bib.bib20)] | 2 | 872 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| SST2 [[20](#bib.bib20)] | 2 | 872 |'
- en: '| SST-5 [[20](#bib.bib20)] | 5 | 2210 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| SST-5 [[20](#bib.bib20)] | 5 | 2210 |'
- en: '| MR [[17](#bib.bib17)] | 2 | 3000 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| MR [[17](#bib.bib17)] | 2 | 3000 |'
- en: '| CR [[13](#bib.bib13)] | 2 | 376 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| CR [[13](#bib.bib13)] | 2 | 376 |'
- en: '| Topic classification |  |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 主题分类 |  |  |'
- en: '| AGNews [[28](#bib.bib28)] | 4 | 7600 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| AGNews [[28](#bib.bib28)] | 4 | 7600 |'
- en: '| TREC [[22](#bib.bib22)] | 6 | 500 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| TREC [[22](#bib.bib22)] | 6 | 500 |'
- en: '| Natural language inference |  |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 |  |  |'
- en: '| MNLI [[26](#bib.bib26)] | 3 | 3000 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| MNLI [[26](#bib.bib26)] | 3 | 3000 |'
- en: '| RTE [[3](#bib.bib3)] | 2 | 277 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| RTE [[3](#bib.bib3)] | 2 | 277 |'
- en: '| Reasoning |  |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 推理 |  |  |'
- en: '| ARC-Challenge [[2](#bib.bib2)] | 4 | 1170 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ARC-Challenge [[2](#bib.bib2)] | 4 | 1170 |'
- en: '| MMLU [[12](#bib.bib12)] | 4 | 2000 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| MMLU [[12](#bib.bib12)] | 4 | 2000 |'
- en: '| COPA [[19](#bib.bib19)] | 2 | 100 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| COPA [[19](#bib.bib19)] | 2 | 100 |'
- en: '| Word disambiguation |  |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 词义消歧 |  |  |'
- en: '| WiC [[18](#bib.bib18)] | 2 | 638 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| WiC [[18](#bib.bib18)] | 2 | 638 |'
- en: A.2 Implementation Details
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 实施细节
- en: 'Experiments on internal mechanisms of biased factors: All experiments are conducted
    on Llama-2 7b model. For the vanilla label bias experiment, we projecting all
    FFN value vectors into the vocabulary space and sum the label logits for all FFN
    vectors whose label logits rank within the top $10$ over the vocabulary to calculate
    uncontextual accumulated FFN logits. We change different set of label words in
    prompt to derive the label prediction frequency of different label pairs. For
    the recency bias experiment, based on findings in [[24](#bib.bib24)], instead
    of the summed attention weights over the whole example, we adopt the sum of attention
    weights on label words of the example, e.g. "Answer: positive" as the effective
    attention weight on each example. For the selection bias experiment, we use zeroshot
    ARC dataset prompts in Table [6](#A4.T6 "Table 6 ‣ Appendix D Prompt Templates
    ‣ UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN
    Manipulation"), and we use 12 samples for each class. The attention weight is
    also summed on label words instead of the whole option.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '偏差因素的内部机制实验：所有实验都在 Llama-2 7b 模型上进行。对于原始标签偏差实验，我们将所有 FFN 值向量投射到词汇空间，并对所有 FFN
    向量的标签 logits 进行求和，这些 FFN 向量的标签 logits 在词汇表中排名前 $10$ 以内，以计算未上下文累积的 FFN logits。我们更改提示中的不同标签词集，以推导不同标签对的标签预测频率。对于近期偏差实验，根据
    [[24](#bib.bib24)] 的发现，我们不使用整个示例的注意力权重总和，而是采用示例标签词的注意力权重总和，例如“Answer: positive”，作为每个示例的有效注意力权重。对于选择偏差实验，我们使用表
    [6](#A4.T6 "Table 6 ‣ Appendix D Prompt Templates ‣ UniBias: Unveiling and Mitigating
    LLM Bias through Internal Attention and FFN Manipulation") 中的零样本 ARC 数据集提示，每类使用
    12 个样本。注意力权重也在标签词上求和，而不是整个选项。'
- en: 'Baselines:   We reproduce all baselines using the publicly available code released
    by the authors to ensure a fair comparison. For the PC method, instead of using
    test samples as in the original work, we employ 200 training samples per class
    as the estimate set for parameter estimation using the EM algorithm. This adjustment
    is made to reflect real-world scenarios where test samples are not readily available.
    Additionally, the number of samples used by the PC method is significantly larger
    than that used by our UniBias method.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 基准：   我们使用作者发布的公开代码重现所有基准，以确保公平比较。对于PC方法，我们使用每类200个训练样本作为估计集，以使用EM算法进行参数估计，而不是像原作中那样使用测试样本。这一调整是为了反映现实世界中测试样本不易获得的情况。此外，PC方法使用的样本数量远大于我们UniBias方法使用的样本数量。
- en: 'Unibias:   In our method, all threshold values are determined through grid
    searching as described in the methodology section. Specifically, we use 20 samples
    per class as the support set for grid searching in all experiments. For each repetition
    of the experiment, the support set is randomly selected based on different random
    seeds. Additionally, to manipulate biased FFN vectors and attention heads, we
    create masks for the attention heads of all attention layers and adjust the FFN
    coefficient values and attention head masks using the hook operation. Additionally,
    we conduct the experiment on four A5000 GPUs. We will release our code upon acceptance
    to facilitate easy reproduction.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 'Unibias:   在我们的方法中，所有阈值是通过方法学部分描述的网格搜索确定的。具体而言，我们在所有实验中使用每类20个样本作为网格搜索的支持集。对于每次实验的重复，支持集是基于不同的随机种子随机选择的。此外，为了操作有偏的FFN向量和注意力头，我们为所有注意力层的注意力头创建掩码，并使用钩子操作调整FFN系数值和注意力头掩码。此外，我们在四个A5000
    GPU上进行实验。我们将在接受后发布代码以便于轻松复现。'
- en: Appendix B Limitation and Future Work
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 限制与未来工作
- en: In this work, we provide a novel insight into the internal mechanisms behind
    the bias of LLMs. As a pioneering effort in mitigating LLM bias through manipulation
    of the model’s internal structures, our approach relies on grid searching with
    a small set of labeled training samples. Future research could focus on reducing
    this reliance, potentially improving the efficiency and applicability of our method.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提供了对LLM偏见背后内部机制的新见解。作为通过操控模型内部结构来缓解LLM偏见的开创性工作，我们的方法依赖于使用小规模标记训练样本的网格搜索。未来的研究可以集中在减少这种依赖上，可能会提高我们方法的效率和适用性。
- en: '![Refer to caption](img/dd7000e21b475bf213744ce1df6f65c6.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd7000e21b475bf213744ce1df6f65c6.png)'
- en: 'Figure 7: Performance of Unibias under different support set.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：不同支持集下Unibias的性能。
- en: There are many interesting avenues for future research. For instance, instead
    of identifying biased components for each ICL prompt, future work could explore
    the identification of global biased components that mitigate bias across multiple
    tasks and diverse prompt design settings. Additionally, the biased FFN vectors
    and attention heads we identify could potentially serve as sensors for guiding
    effective prompt generation. We expect that this internal perspective on LLM bias
    will inspire more innovative applications in both bias mitigation methods and
    prompt engineering.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 未来有许多有趣的研究方向。例如，未来的工作可以探索识别全球有偏组件，以缓解多个任务和多样化提示设计设置中的偏见，而不是为每个ICL提示识别有偏组件。此外，我们识别出的有偏FFN向量和注意力头可能作为指导有效提示生成的传感器。我们期望这种对LLM偏见的内部视角将激发更多在偏见缓解方法和提示工程方面的创新应用。
- en: Appendix C Impact of support set size
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 支持集大小的影响
- en: Our proposed UniBias method employs a small support set for grid searching.
    To analyze its effect, we vary the size of the support set. Figure 7 illustrates
    Unibias’s performance with support set sizes ranging from 5 to 50 samples. The
    results indicate that the performance stabilizes when the support set contains
    20 or more samples per class. Notably, for the SST2 dataset, even with much fewer
    support samples, Unibias significantly outperforms the standard ICL.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的UniBias方法使用小规模支持集进行网格搜索。为了分析其效果，我们变化了支持集的大小。图7展示了支持集大小从5到50个样本时Unibias的性能。结果表明，当支持集每类包含20个或更多样本时，性能稳定下来。值得注意的是，对于SST2数据集，即使支持样本远少于此，Unibias仍显著优于标准ICL。
- en: Appendix D Prompt Templates
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 提示模板
- en: 'The prompt templates used in this work are provided below. We generate few-shot
    ICL templates follow the template styles in [[10](#bib.bib10), [7](#bib.bib7)],
    as illustrated in Table [4](#A4.T4 "Table 4 ‣ Appendix D Prompt Templates ‣ UniBias:
    Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation").'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究中使用的提示模板如下。我们生成了几个示例的 ICL 模板，遵循 [[10](#bib.bib10), [7](#bib.bib7)] 中的模板样式，如表
    [4](#A4.T4 "表4 ‣ 附录D 提示模板 ‣ UniBias: 揭示和缓解 LLM 偏差通过内部注意力和 FFN 操作") 所示。'
- en: 'Table 4: Prompt templates for all $k$-shot ICL experiments.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '表4: 所有 $k$-shot ICL 实验的提示模板。'
- en: '| Dataset | Template | Label Space |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 模板 | 标签空间 |'
- en: '| SST-2 | Review: {sentence} | negative / positive |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 评论: {sentence} | 消极 / 积极 |'
- en: '| CR | Sentiment: {label} |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| CR | 情感: {label} |  |'
- en: '| MR |  |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| MR |  |  |'
- en: '| MNLI | Premise: {premise} | yes / maybe / no |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| MNLI | 前提: {premise} | 是 / 可能 / 否 |'
- en: '|  | Hypothesis: {hypothesis} |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | 假设: {hypothesis} |  |'
- en: '|  | Answer: {label} |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {label} |  |'
- en: '| ARC | Question: {question} | A / B / C / D |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| ARC | 问题: {question} | A / B / C / D |'
- en: '| MMLU | {options} |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | {options} |  |'
- en: '|  | Answer: {label} |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {label} |  |'
- en: '| SST-5 | Review: {sentence} | terrible / bad / okay / good / great |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| SST-5 | 评论: {sentence} | 糟糕 / 差 / 一般 / 好 / 极好 |'
- en: '|  | Sentiment: {label} |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | 情感: {label} |  |'
- en: '| AGNews | Article: {passage} | world / sports / business / technology & science
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| AGNews | 文章: {passage} | 世界 / 体育 / 商业 / 科技与科学 |'
- en: '|  | Answer: {label} |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {label} |  |'
- en: '| TREC | Question: {sentence} | abbreviation / entity / description / person
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 问题: {sentence} | 缩写 / 实体 / 描述 / 人物 |'
- en: '|  | Answer Type: {label} | / location / number |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案类型: {label} | / 位置 / 数量 |'
- en: '| COPA | Premise: {premise} | 1 / 2 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| COPA | 前提: {premise} | 1 / 2 |'
- en: '|  | Choice1: {choice1} |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | 选项1: {choice1} |  |'
- en: '|  | Choice2: {choice2} |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | 选项2: {choice2} |  |'
- en: '|  | Answer: {label} |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {label} |  |'
- en: '| RTE | Premise: {sentence1} | yes / no |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| RTE | 前提: {sentence1} | 是 / 否 |'
- en: '|  | Hypothesis: {sentence2} |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | 假设: {sentence2} |  |'
- en: '|  | Answer: {label} |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {label} |  |'
- en: '| WiC | Sentence1: {sentence1} | false / true |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| WiC | 句子1: {sentence1} | 错误 / 正确 |'
- en: '|  | Sentence2: {sentence2} |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | 句子2: {sentence2} |  |'
- en: '|  | Word: {word} |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | 单词: {word} |  |'
- en: '|  | Answer: {label} |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {label} |  |'
- en: 'Table 5: Templates of different prompt formatting used in the prompt brittleness
    experiment for SST-2.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '表5: 用于 SST-2 提示脆弱性实验的不同提示格式化模板。'
- en: '| ID | Template | Label Space |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ID | 模板 | 标签空间 |'
- en: '| --- | --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Review: {Sentence} | Positive / Negative |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 评论: {Sentence} | 积极 / 消极 |'
- en: '|  | Sentiment: {Label} |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | 情感: {Label} |  |'
- en: '| 2 | Input: {Sentence} | Positive / Negative |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 输入: {Sentence} | 积极 / 消极 |'
- en: '|  | Prediction: {Label} |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | 预测: {Label} |  |'
- en: '| 3 | Review: {Sentence} | good / bad |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 评论: {Sentence} | 好 / 坏 |'
- en: '|  | Sentiment: {Label} |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | 情感: {Label} |  |'
- en: '| 4 | {Sentence} It was {Label} | good / bad |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 4 | {Sentence} 这是 {Label} | 好 / 坏 |'
- en: '| 5 | Review: {Sentence} | Yes / No |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 评论: {Sentence} | 是 / 否 |'
- en: '|  | Positive Review: {Label} |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | 积极评论: {Label} |  |'
- en: '| 6 | {Sentence} My overall feeling was that the movie was {Label} | good /
    bad |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 6 | {Sentence} 我对电影的总体感觉是这部电影 {Label} | 好 / 坏 |'
- en: '| 7 | Review: {Sentence} | Positive / Negative |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 评论: {Sentence} | 积极 / 消极 |'
- en: '|  | Question: Is the sentiment of the above review Positive or Negative? |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 问题: 上述评论的情感是积极还是消极？ |  |'
- en: '|  | Answer: {Label} |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {Label} |  |'
- en: '| 8 | My review for last night’s film: {Sentence}The critics agreed that this
    | good / bad |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 我对昨晚电影的评论: {Sentence}评论家一致认为这部 | 好 / 坏 |'
- en: '|  | movie was {Label} |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | 电影是 {Label} |  |'
- en: 'Table 6: Prompt templates for the 0-shot experiments.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '表6: 0-shot 实验的提示模板。'
- en: '| Dataset | Template | Label Set |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 模板 | 标签集 |'
- en: '| SST-2 | Review: {sentence} | negative / positive |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 评论: {sentence} | 消极 / 积极 |'
- en: '|  | Sentiment: {label} |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | 情感: {label} |  |'
- en: '| COPA | Premise: {premise} | 1 / 2 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| COPA | 前提: {premise} | 1 / 2 |'
- en: '|  | Choice1: {choice1} |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | 选项1: {choice1} |  |'
- en: '|  | Choice2: {choice2} |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | 选项2: {choice2} |  |'
- en: '|  | Answer: {label} |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {label} |  |'
- en: '| MMLU | Question: {question} | A / B / C / D |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 问题: {question} | A / B / C / D |'
- en: '|  | {options} |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | {options} |  |'
- en: '|  | Answer: {label} |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | 答案: {label} |  |'
