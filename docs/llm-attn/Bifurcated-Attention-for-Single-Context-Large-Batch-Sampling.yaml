- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:53:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Bifurcated Attention for Single-Context Large-Batch Sampling
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单上下文大批量采样的*Bifurcated Attention*
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.08845](https://ar5iv.labs.arxiv.org/html/2403.08845)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.08845](https://ar5iv.labs.arxiv.org/html/2403.08845)
- en: Ben Athiwaratkun    Sujan Kumar Gonugondla    Sanjay Krishna Gouda    Haifeng
    Qian    Hantian Ding    Qing Sun    Jun Wang    Jiacheng Guo    Liangfu Chen   
    Parminder Bhatia    Ramesh Nallapati    Sudipta Sengupta    Bing Xiang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ben Athiwaratkun    Sujan Kumar Gonugondla    Sanjay Krishna Gouda    Haifeng
    Qian    Hantian Ding    Qing Sun    Jun Wang    Jiacheng Guo    Liangfu Chen   
    Parminder Bhatia    Ramesh Nallapati    Sudipta Sengupta    Bing Xiang
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In our study, we present *bifurcated attention*, a method developed for language
    model inference in single-context batch sampling contexts. This approach aims
    to reduce redundant memory IO costs, a significant factor in latency for high
    batch sizes and long context lengths. Bifurcated attention achieves this by dividing
    the attention mechanism during incremental decoding into two distinct GEMM operations,
    focusing on the KV cache from prefill and the decoding process. This method ensures
    precise computation and maintains the usual computational load (FLOPs) of standard
    attention mechanisms, but with reduced memory IO. Bifurcated attention is also
    compatible with multi-query attention mechanism known for reduced memory IO for
    KV cache, further enabling higher batch size and context length. The resulting
    efficiency leads to lower latency, improving suitability for real-time applications,
    e.g., enabling massively-parallel answer generation without substantially increasing
    latency, enhancing performance when integrated with post-processing techniques
    such as reranking.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们提出了*bifurcated attention*，这是一种为单一上下文批量采样场景开发的语言模型推断方法。此方法旨在减少冗余的内存IO开销，这在高批量大小和长上下文长度的延迟中是一个重要因素。*Bifurcated
    attention*通过在递增解码期间将注意力机制划分为两个不同的GEMM操作，专注于来自预填充的KV缓存和解码过程，从而实现这一目标。这种方法确保了精确的计算，并保持了标准注意力机制的通常计算负载（FLOPs），但减少了内存IO。*Bifurcated
    attention*还兼容于多查询注意力机制，这种机制以减少KV缓存的内存IO而闻名，进一步支持了更高的批量大小和上下文长度。由此产生的效率降低了延迟，提高了实时应用的适用性，例如，在不显著增加延迟的情况下支持大规模并行回答生成，并在与后处理技术（如重排名）集成时提升性能。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The advent of large language models (LLMs) has ushered in a new era of machine
    learning, exhibiting remarkable performance on a wide array of tasks (Brown et al.,
    [2020](#bib.bib8); OpenAI, [2023](#bib.bib53); Chowdhery et al., [2022](#bib.bib15);
    Touvron et al., [2023](#bib.bib63); Chen et al., [2021](#bib.bib12); Hoffmann
    et al., [2022](#bib.bib27); Li et al., [2022](#bib.bib39); [Microsoft,](#bib.bib46)
    ; Amazon, [2022](#bib.bib4); Nijkamp et al., [2023](#bib.bib51)). Despite their
    impressive capabilities, the deployment of these large-scale models in practical
    applications poses significant challenges, particularly in terms of inference
    latency and efficiency. Enhancing these aspects is critical, as they directly
    influence the computational resources required to generate predictions and enable
    the practical implementation of these advanced models across various industries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现标志着机器学习的新纪元，在各种任务上表现出卓越的性能（Brown et al., [2020](#bib.bib8); OpenAI,
    [2023](#bib.bib53); Chowdhery et al., [2022](#bib.bib15); Touvron et al., [2023](#bib.bib63);
    Chen et al., [2021](#bib.bib12); Hoffmann et al., [2022](#bib.bib27); Li et al.,
    [2022](#bib.bib39); [Microsoft,](#bib.bib46) ; Amazon, [2022](#bib.bib4); Nijkamp
    et al., [2023](#bib.bib51))。尽管它们的能力令人印象深刻，但这些大规模模型在实际应用中的部署仍面临重大挑战，特别是在推断延迟和效率方面。提高这些方面的表现至关重要，因为它们直接影响生成预测所需的计算资源，并使这些先进模型在各行各业的实际应用成为可能。
- en: A particularly demanding inference scenario is single-context batch sampling,
    where the goal is to generate multiple completions from a single context. This
    task is commonly encountered in numerous applications such as code-editing IDE
    tools that provide multiple recommendations, or in cases where ranking among many
    generations is needed for optimal performance (via ranking metrics like mean log
    probability, majority voting, etc). The incremental decoding of such sampling
    scenario is memory IO intensive, which becomes a latency bottleneck for high batches
    and context lengths.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特别具有挑战性的推理场景是单上下文批量采样，其目标是从单一上下文生成多个完成。这种任务在许多应用中常见，例如提供多个推荐的代码编辑 IDE 工具，或者在需要通过排序指标（如平均对数概率、主要投票等）对众多生成结果进行排名以获得最佳性能的情况下。这种采样场景的增量解码内存
    IO 需求高，成为高批量和上下文长度的延迟瓶颈。
- en: 'In this study, we investigate two compatible strategies to address the memory
    IO challenges in tranformers inference: (1) an investigation of multi-query and
    its trade-offs, and (2) a novel technique called context-aware bifurcated attention.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们探讨了两种兼容的策略来应对变换器推理中的内存 IO 挑战：(1) 对多查询及其权衡的研究，以及 (2) 一种称为上下文感知分叉注意力的新技术。
- en: Our investigation begins with an analysis of the generalized multi-query attention
    (Ainslie et al., [2023](#bib.bib2)), which includes multi-query (Shazeer, [2019](#bib.bib60)),
    as well as the established multi-head attention mechanism (Vaswani et al., [2017](#bib.bib65))
    for performance and latency trade-off. Our findings show smooth performance scaling
    with increasing model size for a fixed value of the number of groups $g$ for generalized
    multi-query¹¹1Lower values of attention groups $g$ lead to higher compression
    of the key-value tensors, as in the multi-query case where $g=1$, hence improving
    inference efficiency and latency due to reduced KV cache compared to the multi-head
    case where $g=h$, the number of query attention heads.. Lowering $g$ results in
    an upward shift of the validation loss vs model size scaling curves. The consistent
    relationship between the cache compression, model size and validation loss allows
    us to trade-off inference efficiency with model size, i.e., enables us to select
    higher compression for use cases requiring high efficiency, while still matching
    the performance of multi-head attention by compensating with a larger model size.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究首先分析了通用多查询注意力机制（Ainslie et al., [2023](#bib.bib2)），其中包括多查询（Shazeer, [2019](#bib.bib60)），以及为性能和延迟权衡而建立的多头注意力机制（Vaswani
    et al., [2017](#bib.bib65)）。我们的发现显示，通用多查询的模型规模增加时性能平滑扩展，对于固定的查询组数 $g$，较低的注意力组数
    $g$ 导致键值张量的更高压缩，就如多查询情况中 $g=1$ 一样，因此由于 KV 缓存减少，相比于多头注意力的 $g=h$，推理效率和延迟有所提高。降低
    $g$ 导致验证损失与模型规模扩展曲线的上升。缓存压缩、模型规模和验证损失之间的一致关系使我们能够在推理效率和模型规模之间进行权衡，即允许我们为需要高效率的用例选择更高的压缩，同时通过使用更大的模型规模来补偿，从而匹配多头注意力的性能。
- en: Secondly, we introduce context-aware bifurcated attention, a technique that
    bifurcates any attention in the generalized multi-query family into context and
    decoding components during incremental decoding. Such bifurcation involves the
    same number of FLOPs and yields identical results compared to the original attention,
    but can significantly reduces memory IO cost and thus latency in high batch and
    context length scenarios. This approach allows the generation of multiple real-time
    completions without incurring much additional latency costs, or enables much higher
    batch sizes leading to improved ranking performance. For instance, for CodeGen
    16B multi-head model (Nijkamp et al., [2022](#bib.bib50)) with 2k context length,
    we are able to increase the batch size to $128$ with bifurcated attention, compared
    to batch size of only $5$ without, resulting in the pass@k (Chen et al., [2021](#bib.bib12))
    increasing from $59.0\%$ to $84.6\%$, or pass@top3 via mean log-p increasing from
    $55.2\%$ to $58.1\%$.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们引入了上下文感知的分叉注意力技术，这种技术在增量解码过程中将任何广义多查询家族中的注意力分为上下文和解码组件。这种分叉涉及相同数量的FLOPs，并且与原始注意力相比，产生相同的结果，但在高批量和上下文长度场景中可以显著减少内存IO成本，从而降低延迟。这种方法允许生成多个实时补全而不会产生额外的延迟成本，或者支持更大的批量大小，从而提高排名性能。例如，对于具有2k上下文长度的CodeGen
    16B多头模型（Nijkamp et al.，[2022](#bib.bib50)），我们能够将批量大小增加到$128$，而不使用时的批量大小仅为$5$，使得pass@k（Chen
    et al.，[2021](#bib.bib12)）从$59.0\%$提升到$84.6\%$，或pass@top3通过均值对数p增加从$55.2\%$到$58.1\%$。
- en: 2 Related Work
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: In the literature, there are multiple avenues to improve inference latency and/or
    latency. Quantization reduces memory usage by using low-bitwidth representations
    such as int8, int4, and fp8 (Wei et al., [2023](#bib.bib67); Yao et al., [2022](#bib.bib70);
    Dettmers et al., [2022](#bib.bib19); Frantar et al., [2022](#bib.bib21); Kuzmin
    et al., [2022](#bib.bib34); Xiao et al., [2022](#bib.bib69)). Quantization when
    applied only to model parameters offer diminishing results as with longer sequence
    lengths and large batch sizes where memory access and compute associated with
    dot-product attention dominates the overall inference latency.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，有多种途径可以提高推理延迟和/或延迟。量化通过使用低位宽表示（如int8、int4和fp8）来减少内存使用（Wei et al.，[2023](#bib.bib67)；Yao
    et al.，[2022](#bib.bib70)；Dettmers et al.，[2022](#bib.bib19)；Frantar et al.，[2022](#bib.bib21)；Kuzmin
    et al.，[2022](#bib.bib34)；Xiao et al.，[2022](#bib.bib69)）。当量化仅应用于模型参数时，在序列长度较长和批量大小较大的情况下，由于内存访问和与点积注意力相关的计算主导了总体推理延迟，量化的效果逐渐减弱。
- en: Sparse attention (Beltagy et al., [2020](#bib.bib7); Child et al., [2019](#bib.bib13);
    Zaheer et al., [2020](#bib.bib73)) has been extensively studied as a way to reduce
    the complexity of attention for longer contexts and faster inference. Pope et al.
    ([2022](#bib.bib56)) investigates generative inference efficiency of large language
    models by using multi-dimensional partitioning techniques optimized for TPUs (collective
    einsum) to achieve a Pareto frontier on latency and model FLOPs utilization. The
    paper also shows that multi-query attention allows scaling up to 32x larger context
    length with an emphasis on the efficiency under high batch size. Paged attention
    (Kwon et al., [2023](#bib.bib35)) enhances memory management of the KV cache by
    dividing it into blocks and employing a block table for mapping purposes. This
    approach effectively accommodates dynamic workload shifts and reduces memory storage
    requirements through the sharing of the prompt’s KV cache across multiple output
    sequences. However, this does not reduce the memory reads of KV cache.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏注意力（Beltagy et al.，[2020](#bib.bib7)；Child et al.，[2019](#bib.bib13)；Zaheer
    et al.，[2020](#bib.bib73)）已被广泛研究作为减少长上下文的注意力复杂性和加快推理速度的一种方法。Pope et al.（[2022](#bib.bib56)）通过使用针对TPU优化的多维分区技术（集体einsum）来研究大型语言模型的生成推理效率，以实现延迟和模型FLOPs利用率的Pareto前沿。该论文还显示，多个查询注意力允许扩展到32倍更大的上下文长度，重点关注在高批量大小下的效率。分页注意力（Kwon
    et al.，[2023](#bib.bib35)）通过将KV缓存划分为块并使用块表进行映射，增强了KV缓存的内存管理。这种方法有效地适应了动态工作负载的变化，并通过在多个输出序列之间共享提示的KV缓存来减少内存存储需求。然而，这并没有减少KV缓存的内存读取。
- en: Speculative decoding, and its variants uses a smaller draft model to propose
    multiple sequential tokens, which are processed in parallel by the main model
    to accept or reject such tokens (Chen et al., [2023](#bib.bib11); Leviathan et al.,
    [2022](#bib.bib37); Li et al., [2024](#bib.bib40); Cai et al., [2024](#bib.bib10);
    Fu et al., [2023](#bib.bib23)). The key idea is to enable decoding of multiple
    tokens at every step, thereby amortizing the memory IO usages of the main model.
    However, the latency of decoding will be still dominated by KV cache I/O bandwidth
    at large context sizes, where bifurcated attention can enhance the decoding speed
    further. In short, incremental decoding focuses on lowering the amortized memory
    IO of model loading while multi-query and bifurcated attention lowers the memory
    IO of KV cache.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 推测解码及其变体使用较小的草稿模型来提出多个连续的标记，这些标记由主模型并行处理以接受或拒绝（Chen 等，[2023](#bib.bib11)；Leviathan
    等，[2022](#bib.bib37)；Li 等，[2024](#bib.bib40)；Cai 等，[2024](#bib.bib10)；Fu 等，[2023](#bib.bib23)）。关键思想是使每一步都能解码多个标记，从而摊销主模型的内存
    IO 使用。然而，在大上下文尺寸下，解码的延迟仍将由KV缓存的I/O带宽主导，而分叉注意力可以进一步提升解码速度。简而言之，增量解码侧重于降低模型加载的摊销内存
    IO，而多查询和分叉注意力则降低KV缓存的内存 IO。
- en: 3 Background
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 背景
- en: 3.1 Notation
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 符号
- en: We use the following notation throughout the paper.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整篇论文中使用以下符号。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$K$: key tensor, $V$: value tensor, $q$: query tensor, $P_{x}$: projection
    tensor associated with key, value or query tensor.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $K$：键张量，$V$：值张量，$q$：查询张量，$P_{x}$：与键、值或查询张量相关的投影张量。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We denote $\langle A,B\rangle$ as a tensor operation between $A$ and $B$. The
    actual operation can be specified in Einstein sum notation. We use $\oplus$ to
    denote concatenation.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们用$\langle A,B\rangle$表示$A$和$B$之间的张量操作。实际操作可以用爱因斯坦求和记号指定。我们用$\oplus$表示连接。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$N$ the number of model parameters, $d$: hidden dimension, $h$: number of attention
    heads, $k$: $\frac{d}{h}$, or head dimension, $\ell$: number of layers, $m$: context
    length (or key/value tensor length), $n$: query tensor length where $n=m$ during
    context encoding and $n=1$ for incremental decoding, $g$: number of attention
    groups (to be explained). We also use $v$ to represent the head dimension for
    the value tensor where practically $k=v$.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $N$：模型参数的数量，$d$：隐藏维度，$h$：注意力头的数量，$k$：$\frac{d}{h}$，即头维度，$\ell$：层的数量，$m$：上下文长度（或键/值张量长度），$n$：查询张量长度，其中上下文编码时$n=m$，增量解码时$n=1$，$g$：注意力组的数量（待解释）。我们还用$v$来表示值张量的头维度，其中实际上$k=v$。
- en: 3.2 Language Model Inference
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 语言模型推理
- en: There are many inference scenarios for language model, including batch inference
    and single-context batch sampling (Figure [1](#S3.F1 "Figure 1 ‣ 3.2 Language
    Model Inference ‣ 3 Background ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling")). Batch inference refers to the case where we process multiple inputs
    together in a batch, and generate subsequent tokens for each batch index independently.
    In the case where the batch size is 1, this reduces to the single-context inference.
    Another scenario is the single-context batch sampling where we generates multiple
    sequences based on a single context, where difference between the batch inference
    case is that the prefill only needs to be done for a single context to obtain
    the KV cache, then broadcasted to other batch indices.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型有许多推理场景，包括批量推理和单上下文批量采样（图 [1](#S3.F1 "图 1 ‣ 3.2 语言模型推理 ‣ 3 背景 ‣ 单上下文大批量采样的分叉注意力")）。批量推理指的是将多个输入一起处理，并为每个批量索引独立生成后续标记。在批量大小为1的情况下，这将减少为单上下文推理。另一种场景是单上下文批量采样，其中我们基于单个上下文生成多个序列，与批量推理的区别在于预填充仅需针对单个上下文进行，以获得KV缓存，然后广播到其他批量索引。
- en: 'Figure [1](#S3.F1 "Figure 1 ‣ 3.2 Language Model Inference ‣ 3 Background ‣
    Bifurcated Attention for Single-Context Large-Batch Sampling") also illustrates
    the two phases of language model inference: (a) the context encoding or prefilling
    and (b) the incremental decoding. The context encoding refers to a single forward
    pass that computes the key and value tensors for all token positions in the context.
    Once the key and value tensors are computed, we cache these key and value tensors
    to be used for the attention mechanism during the incremental decoding phase,
    which sequentially generates one token at a time²²2Or $k$ tokens at a time, in
    case of speculative decoding (Chen et al., [2023](#bib.bib11); Leviathan et al.,
    [2022](#bib.bib37)).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S3.F1 "Figure 1 ‣ 3.2 Language Model Inference ‣ 3 Background ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling") 还展示了语言模型推断的两个阶段：（a）上下文编码或预填充和（b）增量解码。上下文编码指的是一次前向传递，该过程计算上下文中所有令牌位置的键和值张量。一旦计算出键和值张量，我们将这些张量缓存起来，用于增量解码阶段的注意力机制，该阶段逐次生成一个令牌（在投机解码的情况下，$k$
    个令牌同时生成）²²2参见 Chen et al., [2023](#bib.bib11); Leviathan et al., [2022](#bib.bib37)。
- en: '![Refer to caption](img/641ab7972f677bcf63587f89d5b920db.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/641ab7972f677bcf63587f89d5b920db.png)'
- en: 'Figure 1: Illustration of the two phases of language model inference: context
    encoding and incremental decoding, as well as different inference scenarios. In
    batch inference scenario, we process multiple inputs at once and perform incremental
    decoding steps. In batch inference, we group multiple inputs in batch to perform
    both context encoding and the subsequent incremental decoding. In the single-context
    batch sampling scenario, we perform context encoding on a single input to obtain
    the context KV cache, then perform incremental decoding (with temperature sampling)
    to obtain potentially different generations.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示了语言模型推断的两个阶段：上下文编码和增量解码，以及不同的推断场景。在批量推断场景中，我们一次处理多个输入，并执行增量解码步骤。在批量推断中，我们将多个输入分组到批量中，以执行上下文编码和随后的增量解码。在单上下文批量采样场景中，我们对单个输入执行上下文编码以获得上下文
    KV 缓存，然后进行增量解码（使用温度采样）以获得可能不同的生成结果。
- en: 'During the context encoding phase, the number of floating point operations
    relative to the memory input/output (IO) operations is high, corresponding to
    the compute-bound regime where the latency is influenced by the FLOPs. However,
    during incremental decoding where we perform attention on a single query token,
    this falls into a memory-bound regime where the number of computation per memory
    access is roughly 1-to-1 (see Appendix [D.1](#A4.SS1 "D.1 Detailed Analysis on
    Memory Access ‣ Appendix D Multi-Group Attention Family ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling") for details). The memory IO refers to
    the read and write operations from the high bandwidth memory (HBM) (Jia et al.,
    [2018](#bib.bib30)) to the fast on-chip SRAM where the actual computation happens.
    The memory IO of the incremental decoding itself consists of two components: (1)
    the model parameter loading and (2) KV cache loading. Component (1) is constant
    regardless of the context length $m$ or batch size $b$ where component (2) depends
    on both $m$ and $b$ and dominate the overall memory IO if $m$ or $b$ are high,
    which can become a significant bottleneck for inference. Our work primarily focuses
    on reducing component (2).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文编码阶段，相对于内存输入/输出（IO）操作，浮点运算的数量较高，这对应于计算受限的情况，其中延迟受 FLOPs 影响。然而，在增量解码阶段，当我们对单个查询令牌执行注意力操作时，这属于内存受限的情况，其中每次内存访问的计算量大约为
    1 对 1（详情见附录 [D.1](#A4.SS1 "D.1 Detailed Analysis on Memory Access ‣ Appendix D
    Multi-Group Attention Family ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling")）。内存 IO 指的是从高带宽内存（HBM）(Jia et al., [2018](#bib.bib30)) 到实际计算发生的快速片上
    SRAM 的读写操作。增量解码的内存 IO 本身由两个部分组成：（1）模型参数加载和（2）KV 缓存加载。部分（1）是恒定的，与上下文长度 $m$ 或批量大小
    $b$ 无关，而部分（2）依赖于 $m$ 和 $b$，当 $m$ 或 $b$ 较高时，部分（2）主导整体内存 IO，这可能成为推断的重大瓶颈。我们的工作主要集中在减少部分（2）。
- en: 3.3 Multi-Query, Multi-Head and the Generalized Multi-Query Attention
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 多查询、多头和广义多查询注意力
- en: Multi-query attention, proposed by Shazeer ([2019](#bib.bib60)), is an attention
    mechanism for transformers models that uses a single head for the key and value
    tensors, compared to $h$ heads in the traditional multi-head attention (Vaswani
    et al., [2017](#bib.bib65)). This technique effectively reduces the KV memory
    IO by $h$ times, which leads to higher inference efficiency during incremental
    decoding. In effect, the single-head key or value tensor is shared and used to
    attend to all the multi-head query, hence the name multi-query. This corresponds
    to a compression in representation power of the key and value tensor, which we
    will see in the scaling laws study (Section [5.1](#S5.SS1 "5.1 Comparing Capabilities
    of Multi-Head, Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")) that it results in a reduced
    expressiveness in terms of model parameter efficiency. Such reduced expressiveness
    can be compensated by scaling the model bigger than the multi-head counterpart
    to match the representation power.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Shazeer（[2019](#bib.bib60)）提出的多查询注意力是一种用于变压器模型的注意力机制，与传统的多头注意力（Vaswani et al.,
    [2017](#bib.bib65)）相比，它只使用一个头用于键和值张量，而不是$h$个头。这个技术有效减少了$h$倍的KV内存IO，从而在增量解码过程中提高了推理效率。实际上，单头的键或值张量被共享并用于关注所有的多头查询，因此得名多查询。
    这对应于键和值张量在表示能力上的压缩，我们将在缩放定律研究（第[5.1节](#S5.SS1 "5.1 Comparing Capabilities of Multi-Head,
    Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling")）中看到，这会导致模型参数效率方面的表现力减少。这种减少的表现力可以通过将模型规模扩大到比多头对等模型更大的规模来补偿，以匹配表示能力。
- en: We can also extrapolate these insights to a generalized multi-query attention
    mechanism (Ainslie et al., [2023](#bib.bib2)), which provides a framework to understand
    both multi-query and multi-head attention, and everything in between. Here, the
    degree of KV compression is dictated by the number of attention groups $g$, where
    we alternatively refer to the generalized multi-query as multi-group. Each attention
    group can be interpreted as the broadcasted attention between a single head of
    key or value tensor, and multiple heads of query.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将这些见解推广到一个广义的多查询注意力机制（Ainslie et al., [2023](#bib.bib2)），它提供了一个框架来理解多查询和多头注意力以及其中的所有情况。在这里，KV压缩的程度由注意力组的数量$g$决定，其中我们交替地将广义多查询称为多组。每个注意力组可以被解释为单个键或值张量头与多个查询头之间的广播注意力。
- en: 'In this paradigm, multi-query attention is a special case where the number
    of groups $g=1$; that is, there is exactly one such group. Conversely, multi-head
    attention is another special case where the number of attention groups matches
    the number of heads ($g=h$), in which case each head in the key or value tensor
    attends to one head in the query. More generally, the number of groups $g$ can
    lie anywhere between $1$ and $h$, indicating various degrees of compression. For
    practical purposes, it is most convenient when $g$ divides $h$. The attention
    mechanism in this setting can be expressed in terms of Einstein summation as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种范式中，多查询注意力是一个特殊的情况，其中组的数量$g=1$；也就是说，只有一个这样的组。相反，多头注意力是另一个特殊情况，其中注意力组的数量与头的数量匹配（$g=h$），在这种情况下，键或值张量中的每个头都关注查询中的一个头。更一般地说，组的数量$g$可以介于$1$和$h$之间，表示各种压缩程度。对于实际目的，当$g$能整除$h$时最为方便。在这种设置下，注意力机制可以用爱因斯坦求和表示为：
- en: '|  | $\displaystyle\mathrm{logits}=\langle q,K\rangle$ | $\displaystyle:\text{einsum}(bgpnk,bgmk)\to
    bgpnm$ |  | (1) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{logits}=\langle q,K\rangle$ | $\displaystyle:\text{einsum}(bgpnk,bgmk)\to
    bgpnm$ |  | (1) |'
- en: '|  | $\displaystyle o=\langle w,V\rangle$ | $\displaystyle:\text{einsum}(bgpmn,bgmv)\to
    bgpnv$ |  | (2) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o=\langle w,V\rangle$ | $\displaystyle:\text{einsum}(bgpmn,bgmv)\to
    bgpnv$ |  | (2) |'
- en: where $p=\frac{h}{g}$ represents the attention group size. Other operations
    in the attention mechanism are analogous, as detailed in Appendix [D.1](#A4.SS1
    "D.1 Detailed Analysis on Memory Access ‣ Appendix D Multi-Group Attention Family
    ‣ Bifurcated Attention for Single-Context Large-Batch Sampling"). The memory IO
    complexity for the multi-query attention becomes $bgmk$ compared to $bhmk$ in
    the multi-head setting, a reduction by a factor of $\frac{h}{g}$ times. The FLOPs,
    however, are $bgpnmk=bdnm$, independent of the compression $g$, implying that
    in the compute-bound scenario of context encoding, the latency would be quite
    similar among multi-group models of different $g$’s, including between $g=1$ and
    $g=h$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p=\frac{h}{g}$ 表示注意力组的大小。注意力机制中的其他操作也是类似的，具体细节见附录 [D.1](#A4.SS1 "D.1 Detailed
    Analysis on Memory Access ‣ Appendix D Multi-Group Attention Family ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")。多查询注意力的内存IO复杂度为 $bgmk$，相比于多头设置中的
    $bhmk$，减少了 $\frac{h}{g}$ 倍。然而，FLOPs 为 $bgpnmk=bdnm$，与压缩 $g$ 无关，这意味着在上下文编码的计算受限场景下，不同
    $g$ 值的多组模型之间的延迟会非常相似，包括 $g=1$ 和 $g=h$ 之间。
- en: This generalized multi-group attention mechanism thus provides a unified perspective
    on the design space of attention architectures. By adjusting the number of attention
    groups $g$, one can flexibly tune these trade-offs, potentially yielding new regimes
    of performance for transformer models. In Section [5.1](#S5.SS1 "5.1 Comparing
    Capabilities of Multi-Head, Multi-Query, and Multi-Group Attention ‣ 5 Experiments
    ‣ Bifurcated Attention for Single-Context Large-Batch Sampling"), we will look
    into such capability vs latency trade-off.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通用的多组注意力机制因此提供了注意力架构设计空间的统一视角。通过调整注意力组的数量 $g$，可以灵活地调节这些权衡，可能为变换器模型带来新的性能范围。在第
    [5.1](#S5.SS1 "5.1 Comparing Capabilities of Multi-Head, Multi-Query, and Multi-Group
    Attention ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling") 节中，我们将探讨这种能力与延迟的权衡。
- en: 4 Context-Aware Bifurcated Attention
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 上下文感知分叉注意力
- en: In this section, we present a novel context-aware bifurcated attention method
    that aims to reduce the memory IO cost during incremental decoding by efficiently
    handling the computation of attention for shared context across samples, as shown
    in Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Motivation ‣ 4 Context-Aware Bifurcated Attention
    ‣ Bifurcated Attention for Single-Context Large-Batch Sampling").
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一种新颖的上下文感知分叉注意力方法，旨在通过高效处理跨样本共享上下文的注意力计算来减少增量解码期间的内存IO成本，如图 [2](#S4.F2
    "Figure 2 ‣ 4.1 Motivation ‣ 4 Context-Aware Bifurcated Attention ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling") 所示。
- en: 4.1 Motivation
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 动机
- en: We observe that the memory IO during the incremental decoding phase can be significantly
    improved due to the fact that the KV corresponding to the context are shared and
    can be loaded only once. During incremental decoding, the accumulated key tensor
    ($K$) for a multi-head model is of size $bhmk=bh(m_{c}+m_{d})k$. The two parts
    of $K$ correspond to $K_{c}$ of size $bhm_{c}k$ and $K_{d}$ of size $bhm_{d}k$
    where $m_{c}$ is length of the original input and $m_{d}$ is the length due to
    previous incremental decoding steps. Since tensor $K_{c}$ is the same across all
    indices in the $b$ axis, we can also represent $K_{c}$ with a more compact shape
    $1hm_{c}k$ or simply $hm_{c}k$. The query-key attention (Equation [1](#S3.E1 "Equation
    1 ‣ 3.3 Multi-Query, Multi-Head and the Generalized Multi-Query Attention ‣ 3
    Background ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")) is
    typically performed by accessing different batch indices of $K=K_{c}\oplus K_{d}$
    separately, even though all batch indices in $K_{c}$ correspond to the same attention
    values. That is, if we “naively” pass the entire tensor to the GEMM/BLAS operators,
    the incurred memory I/O cost = $bhmk$, meaning that $K_{c}$ tensor is loaded $b$
    times (Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Motivation ‣ 4 Context-Aware Bifurcated
    Attention ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")). Since
    memory loading of $KV$ is the bottleneck for incremental decoding, reducing such
    IO can bring significant reductions in latency saving.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，在增量解码阶段，由于上下文对应的 KV 是共享的且只需加载一次，因此内存 IO 可以显著改善。在增量解码过程中，多头模型的累积键张量（$K$）的大小为
    $bhmk=bh(m_{c}+m_{d})k$。$K$ 的两部分对应于 $K_{c}$（大小为 $bhm_{c}k$）和 $K_{d}$（大小为 $bhm_{d}k$），其中
    $m_{c}$ 是原始输入的长度，$m_{d}$ 是由于之前增量解码步骤导致的长度。由于 $K_{c}$ 张量在 $b$ 轴上的所有索引处都是相同的，我们还可以用更紧凑的形状
    $1hm_{c}k$ 或简单的 $hm_{c}k$ 来表示 $K_{c}$。查询-键注意力（方程 [1](#S3.E1 "方程 1 ‣ 3.3 多查询、多头和广义多查询注意力
    ‣ 3 背景 ‣ 单一上下文大批次采样的双分支注意力")）通常通过分别访问 $K=K_{c}\oplus K_{d}$ 的不同批次索引来执行，即使 $K_{c}$
    中的所有批次索引对应于相同的注意力值。也就是说，如果我们“简单地”将整个张量传递给 GEMM/BLAS 操作符，则产生的内存 I/O 成本 = $bhmk$，意味着
    $K_{c}$ 张量被加载 $b$ 次（图 [2](#S4.F2 "图 2 ‣ 4.1 动机 ‣ 4 上下文感知双分支注意力 ‣ 单一上下文大批次采样的双分支注意力")）。由于
    $KV$ 的内存加载是增量解码的瓶颈，减少这样的 IO 可以显著降低延迟。
- en: '![Refer to caption](img/fcc8cf62a09a4081747d9ccfec7bb688.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fcc8cf62a09a4081747d9ccfec7bb688.png)'
- en: 'Figure 2: Context-aware bifurcated attention for single-context batch sampling.
    The figure depicts the incremental decoding step where the batched query $q$ attends
    with the cached key tensor $K$ where different colors in the $q$ tensor correspond
    to different batch indices. The key tensor consists of two parts: key cache corresponding
    to the single context $K_{c}$ (which was computed during context encoding, as
    in Figure [1](#S3.F1 "Figure 1 ‣ 3.2 Language Model Inference ‣ 3 Background ‣
    Bifurcated Attention for Single-Context Large-Batch Sampling")), and the key cache
    corresponding to previous incremental decoding steps $K_{d}$. The query-key attention
    is bifurcated into two parts, $\langle q,K_{c}\rangle$ and $\langle q,K_{d}\rangle$,
    and joined back via concatenation, resulting in an identical results using the
    same FLOPs but with lower memory IO (Eq. [3](#S4.E3 "Equation 3 ‣ 4.2 Formulation
    ‣ 4 Context-Aware Bifurcated Attention ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling")). The weight-value attention is bifurcated similarly, as
    outlined in Eq. [4](#S4.E4 "Equation 4 ‣ 4.2 Formulation ‣ 4 Context-Aware Bifurcated
    Attention ‣ Bifurcated Attention for Single-Context Large-Batch Sampling").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：针对单一上下文批次采样的上下文感知双分支注意力。图中展示了增量解码步骤，其中批次查询 $q$ 与缓存的键张量 $K$ 进行对齐，其中 $q$ 张量中的不同颜色对应于不同的批次索引。键张量由两部分组成：与单一上下文
    $K_{c}$ 对应的键缓存（如图 [1](#S3.F1 "图 1 ‣ 3.2 语言模型推理 ‣ 3 背景 ‣ 单一上下文大批次采样的双分支注意力") 所示，在上下文编码期间计算），以及与之前增量解码步骤
    $K_{d}$ 对应的键缓存。查询-键注意力被分为两部分，即 $\langle q,K_{c}\rangle$ 和 $\langle q,K_{d}\rangle$，然后通过拼接合并，结果在使用相同的
    FLOPs 的情况下，内存 IO 更低（方程 [3](#S4.E3 "方程 3 ‣ 4.2 公式 ‣ 4 上下文感知双分支注意力 ‣ 单一上下文大批次采样的双分支注意力")）。权重-值注意力也以类似方式进行双分支处理，如方程
    [4](#S4.E4 "方程 4 ‣ 4.2 公式 ‣ 4 上下文感知双分支注意力 ‣ 单一上下文大批次采样的双分支注意力") 所述。
- en: 4.2 Formulation
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 公式
- en: 'Below outlines the proposed context-aware bifurcated attention for single-context
    batch sampling. This operation splits any attention in the multi-group family
    during incremental decoding into two parts: (1) attention associated with KV cache
    from the single context $\langle q,K_{c}\rangle$ and (2) attention associated
    with KV cache from prior incremental decoding steps $\langle q,K_{d}\rangle$.
    That is,'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下述概述了针对单上下文批量采样的上下文感知分叉注意力。该操作在增量解码过程中将多组家族中的任何注意力分为两部分：(1) 与单个上下文的KV缓存相关的注意力$\langle
    q,K_{c}\rangle$，和(2) 与之前增量解码步骤的KV缓存相关的注意力$\langle q,K_{d}\rangle$。也就是说，
- en: '|  | $\displaystyle\langle q,K\rangle$ | $\displaystyle=\langle q,K_{c}\rangle\oplus\langle
    q,K_{d}\rangle$ |  | (3) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle q,K\rangle$ | $\displaystyle=\langle q,K_{c}\rangle\oplus\langle
    q,K_{d}\rangle$ |  | (3) |'
- en: '|  | $\displaystyle\langle q,K_{c}\rangle$ | $\displaystyle:\text{einsum}(bgpnk,gm_{c}k)\to
    bgpnm_{c}$ |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle q,K_{c}\rangle$ | $\displaystyle:\text{einsum}(bgpnk,gm_{c}k)\to
    bgpnm_{c}$ |  |'
- en: '|  | $\displaystyle\langle q,K_{d}\rangle$ | $\displaystyle:\text{einsum}(bgpnk,bgm_{d}k)\to
    bgpnm_{d}$ |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle q,K_{d}\rangle$ | $\displaystyle:\text{einsum}(bgpnk,bgm_{d}k)\to
    bgpnm_{d}$ |  |'
- en: The context part computes attention with $K_{c}$ that corresponds to any batch
    index, since they are all identical. Hence, the axis $b$ does not appears in the
    einsum for $\langle q,K_{c}\rangle$. The result $\langle q,K_{c}\rangle$ and $\langle
    q,K_{d}\rangle$ are then joined together via concatenation. The weight-value attention
    $\langle w,V\rangle$ is bifurcated similarly, where the weight and value tensors
    are split along length $m$, and the results are joined back via summation (Eq.
    [4](#S4.E4 "Equation 4 ‣ 4.2 Formulation ‣ 4 Context-Aware Bifurcated Attention
    ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")). We also demonstrate
    the code for bifurcated attention in Appendix [E.3](#A5.SS3 "E.3 Implementation
    of Bifurcated Attention ‣ Appendix E Context-Aware Bifurcated Attention ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling").
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文部分使用与任何批次索引对应的$K_{c}$计算注意力，因为它们都是相同的。因此，轴$b$在$\langle q,K_{c}\rangle$的einsum中没有出现。结果$\langle
    q,K_{c}\rangle$和$\langle q,K_{d}\rangle$通过连接结合在一起。权重-值注意力$\langle w,V\rangle$类似地被分叉，其中权重和值张量沿长度$m$分割，结果通过求和重新结合（参见公式
    [4](#S4.E4 "公式 4 ‣ 4.2 公式 ‣ 4 上下文感知分叉注意力 ‣ 单上下文大批量采样的分叉注意力")）。我们还在附录 [E.3](#A5.SS3
    "E.3 分叉注意力的实现 ‣ 附录 E 上下文感知分叉注意力 ‣ 单上下文大批量采样的分叉注意力") 中展示了分叉注意力的代码。
- en: '|  | $\displaystyle\langle w,V\rangle$ | $\displaystyle=\langle w_{c},V_{c}\rangle+\langle
    w_{d},V_{d}\rangle$ |  | (4) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle w,V\rangle$ | $\displaystyle=\langle w_{c},V_{c}\rangle+\langle
    w_{d},V_{d}\rangle$ |  | (4) |'
- en: '|  | $\displaystyle\langle w_{c},V_{c}\rangle$ | $\displaystyle:\text{einsum}(bgpnm_{c},gm_{c}k)\to
    bgpnk=bnd$ |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle w_{c},V_{c}\rangle$ | $\displaystyle:\text{einsum}(bgpnm_{c},gm_{c}k)\to
    bgpnk=bnd$ |  |'
- en: '|  | $\displaystyle\langle w_{d},V_{d}\rangle$ | $\displaystyle:\text{einsum}(bgpnm_{d},bgm_{d}k)\to
    bgpnk=bnd$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle w_{d},V_{d}\rangle$ | $\displaystyle:\text{einsum}(bgpnm_{d},bgm_{d}k)\to
    bgpnk=bnd$ |  |'
- en: The proposed operations yield the exact same results $\langle w,V\rangle$ as
    the original attention in Equation [1](#S3.E1 "Equation 1 ‣ 3.3 Multi-Query, Multi-Head
    and the Generalized Multi-Query Attention ‣ 3 Background ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling") and [2](#S3.E2 "Equation 2 ‣ 3.3 Multi-Query,
    Multi-Head and the Generalized Multi-Query Attention ‣ 3 Background ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling"), but can significantly reduce
    memory I/O during incremental decoding (proof in Appendix [E.1](#A5.SS1 "E.1 Proof
    ‣ Appendix E Context-Aware Bifurcated Attention ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling")).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的操作生成与公式 [1](#S3.E1 "公式 1 ‣ 3.3 多查询、多头和广义多查询注意力 ‣ 3 背景 ‣ 单上下文大批量采样的分叉注意力")
    和 [2](#S3.E2 "公式 2 ‣ 3.3 多查询、多头和广义多查询注意力 ‣ 3 背景 ‣ 单上下文大批量采样的分叉注意力") 中的原始注意力完全相同的结果$\langle
    w,V\rangle$，但可以显著减少增量解码过程中的内存I/O（证明见附录 [E.1](#A5.SS1 "E.1 证明 ‣ 附录 E 上下文感知分叉注意力
    ‣ 单上下文大批量采样的分叉注意力")）。
- en: 4.3 Memory IO Complexity
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 内存I/O复杂度
- en: The memory IO complexity corresponding to loading KV changes from
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于从
- en: '|  | memory IO w/o bifurcated attention | $\displaystyle=gk\cdot bm$ |  | (5)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | memory IO w/o bifurcated attention | $\displaystyle=gk\cdot bm$ |  | (5)
    |'
- en: '|  |  | $\displaystyle=gk\cdot b(m_{c}+m_{d})$ |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=gk\cdot b(m_{c}+m_{d})$ |  |'
- en: '|  | memory IO w. bifurcated attention | $\displaystyle=gk\cdot(m_{c}+bm_{d})$
    |  | (6) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | memory IO w. bifurcated attention | $\displaystyle=gk\cdot(m_{c}+bm_{d})$
    |  | (6) |'
- en: The new memory IO is more efficient since $m_{c}+bm_{d} (high context length compared
    to the number of generated tokens). The absolute efficiency gain, however, is
    more substantially for high  b m_{d}$ 的情况下，效率提升可以高达 $b$ 倍。然而，对于高 $m_{c}$ 的情况，如多头注意力中的 $g=h$，绝对效率提升更加显著。对于多查询（$g=1$），在高
    $m_{c}$ 或 $b$ 的情况下，效率提升也可能很大。
- en: 5 Experiments
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个实验
- en: We first conduct experiments to see how capabilities scale with respect to model
    size for each attention type in Section [5.1](#S5.SS1 "5.1 Comparing Capabilities
    of Multi-Head, Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling"). We find that attention types
    with higher compression (lower number of attention groups $g$) require model size
    compensation, $\approx 10\%$ for multi-query ($g=1)$. We use such findings to
    compare the latency between the multi-head and the larger multi-query models of
    equal capabilities in Section [5.2](#S5.SS2 "5.2 Latencies of Capabilities-Equivalent
    Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling").
    In Section [5.2.2](#S5.SS2.SSS2 "5.2.2 Single-Context Batch Sampling ‣ 5.2 Latencies
    of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling"), we focus on the single-context batch sampling scenario
    where we demonstrate the significant latency reduction of bifurcated attention
    and revisit the comparison between multi-head and multi-query in light of bifurcated
    attention. We outline inference details in Appendix [C.5](#A3.SS5 "C.5 Inference
    Setup ‣ Appendix C Setup ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling").
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先进行实验，以查看各注意力类型在模型规模方面的能力扩展情况，详见第 [5.1](#S5.SS1 "5.1 Comparing Capabilities
    of Multi-Head, Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling") 节。我们发现，具有较高压缩比（注意力组数量较少 $g$）的注意力类型需要模型规模的补偿，多查询（$g=1$）大约需补偿
    $\approx 10\%$。我们利用这些发现来比较第 [5.2](#S5.SS2 "5.2 Latencies of Capabilities-Equivalent
    Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")
    节中具有相同能力的多头和更大多查询模型之间的延迟。在第 [5.2.2](#S5.SS2.SSS2 "5.2.2 Single-Context Batch Sampling
    ‣ 5.2 Latencies of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling") 节中，我们专注于单一上下文批量采样场景，展示了分叉注意力的显著延迟减少，并重新审视了在分叉注意力下多头与多查询的比较。我们在附录
    [C.5](#A3.SS5 "C.5 Inference Setup ‣ Appendix C Setup ‣ Bifurcated Attention for
    Single-Context Large-Batch Sampling") 中概述了推理细节。
- en: 5.1 Comparing Capabilities of Multi-Head, Multi-Query, and Multi-Group Attention
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 比较多头、多查询和多组注意力的能力
- en: For a given model configuration, a multi-group model with $g<h$ has fewer parameters
    in comparison to its multi-head counterpart. This reduction is a result of the
    decreased size of the key and value projection matrices $P_{K}$ and $P_{V}$. Specifically,
    each tensor in this case has a size of $P_{K}:d\times gk$, where $k$ is the head
    dimension. For instance, a 13B multi-head model will correspond to a 11B multi-query
    model, with all other model configurations fixed (see Appendix [D.1](#A4.SS1 "D.1
    Detailed Analysis on Memory Access ‣ Appendix D Multi-Group Attention Family ‣
    Bifurcated Attention for Single-Context Large-Batch Sampling") for more details).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的模型配置，具有 $g<h$ 的多组模型相比于其多头对应模型具有更少的参数。这种减少是由于键和值投影矩阵 $P_{K}$ 和 $P_{V}$ 的大小减少。具体来说，每个张量的大小为
    $P_{K}:d\times gk$，其中 $k$ 是头维度。例如，一个 13B 的多头模型对应于一个 11B 的多查询模型，其他模型配置固定（更多细节见附录
    [D.1](#A4.SS1 "D.1 Detailed Analysis on Memory Access ‣ Appendix D Multi-Group
    Attention Family ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")）。
- en: To compare the capabilities of different attention mechanisms, one can either
    scale other model configurations such as the number of layers $\ell$, the number
    of heads $h$ in order to make match the total model sizes between different attentions.
    However, it is often difficult to match the number of parameters exactly. In this
    work, we compare different attention mechanisms via the loss-vs-size scaling laws.
    For the setup, we use the model hyperparameters similar to that of GPT-3, where
    the size ranges from $125$M to $13$B, with hyperparameters such as $\ell,h,k$
    increasing in tandem. Then, we consider three cases where $g=1$ (multi-query),
    $g=h$ (multi-head) and $1<g<h$ (multi-group) where Appendix [C.1](#A3.SS1 "C.1
    Model Training Details ‣ Appendix C Setup ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling") and [C.2](#A3.SS2 "C.2 Model Configurations ‣ Appendix
    C Setup ‣ Bifurcated Attention for Single-Context Large-Batch Sampling") shows
    the training and model configuration details. We train all three attention models
    of each size and plot the validation loss versus model size, shown in Figure [3](#S5.F3
    "Figure 3 ‣ Matching capabilities by model size compensation ‣ 5.1 Comparing Capabilities
    of Multi-Head, Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling"). Our findings are summarized
    below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较不同注意力机制的能力，可以通过调整其他模型配置，如层数$\ell$、头数$h$，以使不同注意力机制之间的总模型大小匹配。然而，通常很难精确匹配参数数量。在这项工作中，我们通过损失与大小的缩放规律来比较不同的注意力机制。我们使用类似于GPT-3的模型超参数，模型大小范围从$125$M到$13$B，其中超参数如$\ell,h,k$同步增加。然后，我们考虑三种情况：$g=1$（多查询）、$g=h$（多头）和$1<g<h$（多组），其中附录[C.1](#A3.SS1
    "C.1 Model Training Details ‣ Appendix C Setup ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling")和[C.2](#A3.SS2 "C.2 Model Configurations ‣ Appendix C Setup
    ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")展示了训练和模型配置细节。我们训练了每种大小的所有三种注意力模型，并绘制了验证损失与模型大小的关系，如图[3](#S5.F3
    "Figure 3 ‣ Matching capabilities by model size compensation ‣ 5.1 Comparing Capabilities
    of Multi-Head, Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")所示。我们的发现总结如下。
- en: Higher number of attention groups $g$ leads to higher expressiveness
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更多的注意力组数$g$导致更高的表达能力
- en: The results in Figure [3](#S5.F3 "Figure 3 ‣ Matching capabilities by model
    size compensation ‣ 5.1 Comparing Capabilities of Multi-Head, Multi-Query, and
    Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling") shows the validation loss versus model size (log scale).
    The results indicate that, for the same model size (vertical slice across the
    plot), multi-head attention $g=h$ achieves the lowest validation loss compared
    to $1<g<h$ (multi-group) and $g=1$ (multi-query). This trend holds consistently
    over three orders of magnitude of model sizes, where the curves corresponding
    to multi-head, multi-group and multi-query do not cross, implying that the rank
    of model expressiveness, or relative capabilities per number of parameters, is
    quite stable. An intuitive explanation is that the lower $g$ corresponds to a
    lower rank representation of the key and value tensors, which encodes lower representation
    power of the past context and therefore yields lower capabilities than higher
    $g$, given the same model size.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S5.F3 "Figure 3 ‣ Matching capabilities by model size compensation ‣ 5.1
    Comparing Capabilities of Multi-Head, Multi-Query, and Multi-Group Attention ‣
    5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")中的结果显示了验证损失与模型大小的关系（对数刻度）。结果表明，对于相同的模型大小（图中的垂直切片），多头注意力$g=h$相比于$1<g<h$（多组）和$g=1$（多查询）具有最低的验证损失。这一趋势在三个数量级的模型大小中保持一致，其中多头、多组和多查询的曲线没有交叉，意味着模型表达能力的排名或每参数的相对能力是相当稳定的。一个直观的解释是，较低的$g$对应于较低等级的键和值张量表示，这编码了对过去上下文的较低表示能力，因此在相同模型大小下，其能力低于较高的$g$。
- en: Scaling laws via downstream performance
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过下游性能的缩放规律
- en: We use the average scores from two code generation benchmarks, multi-lingual
    HumanEval and MBXP (Athiwaratkun et al., [2022](#bib.bib5)), as a proxy for model
    capabilities in addition to the validation loss. This approach is similar to that
    of the GPT-4 technical report (OpenAI, [2023](#bib.bib53)) where HumanEval (Python)
    (Chen et al., [2021](#bib.bib12)) is used to track the performance across multiple
    magnitudes of compute. In our case, we average across all 13 evaluation languages
    and two benchmarks to obtain a more stable proxy for capabilities. The result
    in Figure [3](#S5.F3 "Figure 3 ‣ Matching capabilities by model size compensation
    ‣ 5.1 Comparing Capabilities of Multi-Head, Multi-Query, and Multi-Group Attention
    ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")
    demonstrates similar trend compared to the validation loss where the pass rate
    curves indicate the same relative expressiveness for multi-head, multi-group and
    multi-query attention.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个代码生成基准测试——多语言 HumanEval 和 MBXP（Athiwaratkun 等人，[2022](#bib.bib5)）的平均分数作为模型能力的代理，此外还包括验证损失。这种方法类似于
    GPT-4 技术报告（OpenAI，[2023](#bib.bib53)），其中 HumanEval（Python）（Chen 等人，[2021](#bib.bib12)）用于跟踪多个计算量级的性能。在我们的案例中，我们在所有
    13 种评估语言和两个基准测试中进行平均，以获得更稳定的能力代理。图 [3](#S5.F3 "Figure 3 ‣ Matching capabilities
    by model size compensation ‣ 5.1 Comparing Capabilities of Multi-Head, Multi-Query,
    and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling") 中的结果显示，与验证损失相比，传递率曲线指示了多头、多组和多查询注意力的相同相对表达能力。
- en: Matching capabilities by model size compensation
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过模型规模补偿匹配能力
- en: Given the same capabilities (horizontal slice of the plot in Figure [3](#S5.F3
    "Figure 3 ‣ Matching capabilities by model size compensation ‣ 5.1 Comparing Capabilities
    of Multi-Head, Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")), the distance between two
    curves indicates the model size difference that the lower-rank attention needs
    to compensate in order to match the multi-head model performance. Empirically,
    we average the distance along the interpolated lines (log scale) and find this
    to correspond to $1.104$ times; that is, a multi-query model can have the same
    capabilities as the multi-head model if the size is increased by $\approx 10\%$
    of the multi-head model size. Similarly, the gap is $<10\%$ for multi-group attention.
    Alternatively, one can argue that a multi-query model of the same size could match
    a multi-head if the multi-query model is given more compute. However, in the regime
    where we train language models until or close to convergence and the performance
    saturates with respect to compute, the difference in capabilities will likely
    remain. Therefore, the size compensation is likely the most fair approach for
    comparison.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同能力（图 [3](#S5.F3 "Figure 3 ‣ Matching capabilities by model size compensation
    ‣ 5.1 Comparing Capabilities of Multi-Head, Multi-Query, and Multi-Group Attention
    ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")
    中的水平切片）的情况下，两条曲线之间的距离表示低阶注意力需要弥补的模型规模差异，以匹配多头模型的性能。经验上，我们沿插值线（对数尺度）平均距离，并发现这对应于
    $1.104$ 倍；也就是说，如果将多查询模型的规模增加 $\approx 10\%$ 的多头模型规模，则其能力可以与多头模型相匹配。类似地，多组注意力的差距为
    $<10\%$。或者，可以认为，如果多查询模型获得更多计算资源，则相同规模的多查询模型可以与多头模型匹配。然而，在我们训练语言模型直到或接近收敛并且性能在计算量上趋于饱和的情况下，能力差异可能会保持不变。因此，规模补偿可能是最公平的比较方法。
- en: '![Refer to caption](img/3f08d0f9c38705df6859b4bb8ae485bc.png)![Refer to caption](img/9b58c117b927fc7bacf4f3100001475c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3f08d0f9c38705df6859b4bb8ae485bc.png)![参见说明](img/9b58c117b927fc7bacf4f3100001475c.png)'
- en: 'Figure 3: (Left) The plots of validation loss versus model size demonstrate
    that the scaling laws curves of different attention mechanisms have different
    expressiveness or performance efficiency. That is, the capabilities given the
    same model size depends on $g$ where higher $g$ yields the best capabilities.
    (Right) We demonstrate a similar trend where we use code generation abilities
    as a proxy for general capabilities. Here, we average the execution pass rates
    evaluated on Multi-lingual HumanEval and MBXP benchmarks under 13 programming
    languages.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: （左）验证损失与模型规模的图示显示，不同注意力机制的缩放法则曲线具有不同的表达能力或性能效率。也就是说，在相同模型规模下，能力取决于 $g$，其中更高的
    $g$ 产生最佳能力。（右）我们展示了类似的趋势，其中我们使用代码生成能力作为一般能力的代理。这里，我们对 13 种编程语言下在 Multi-lingual
    HumanEval 和 MBXP 基准测试中评估的执行通过率进行平均。'
- en: 5.2 Latencies of Capabilities-Equivalent Models
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 可比能力模型的延迟
- en: 'As detailed in Section [5.1](#S5.SS1 "5.1 Comparing Capabilities of Multi-Head,
    Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling"), we’ve observed that an increase in
    the multi-query model’s size is required for it to match the performance of a
    multi-head model. In this section, we focus on examining the latency trade-offs
    across diverse scenarios with both multi-query and multi-head models of similar
    performance capabilities. For these latency experiments, we utilize two models,
    each with an approximate size of 1 billion: a multi-head model and a multi-query
    model (detailed information can be found in [C.3](#A3.SS3 "C.3 Model Details of
    1B Latency Experiment ‣ Appendix C Setup ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling")). The multi-query model chosen for these studies is larger
    by a multiplicative factor $F$, where $F=1.1$.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如[5.1节](#S5.SS1 "5.1 Comparing Capabilities of Multi-Head, Multi-Query, and
    Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling")中详细说明，我们观察到，为了使多查询模型的性能匹配多头模型，其模型大小需要增加。在本节中，我们重点考察了在具有类似性能能力的多查询和多头模型之间的延迟权衡。在这些延迟实验中，我们使用了两个模型，每个模型的大小约为10亿：一个多头模型和一个多查询模型（详细信息可以在[C.3节](#A3.SS3
    "C.3 Model Details of 1B Latency Experiment ‣ Appendix C Setup ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling")中找到）。用于这些研究的多查询模型比选定的多头模型大一个倍增因子$F$，其中$F=1.1$。
- en: Overall, there is some overhead cost of using multi-query attention due to the
    larger size (see Figure [4](#S5.F4 "Figure 4 ‣ 5.2 Latencies of Capabilities-Equivalent
    Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")
    and Appendix [D.3.1](#A4.SS3.SSS1 "D.3.1 Context encoding ‣ D.3 Comparing Capabilities-Equivalent
    Models ‣ Appendix D Multi-Group Attention Family ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling") and [D.3.2](#A4.SS3.SSS2 "D.3.2 Incremental Decoding ‣
    D.3 Comparing Capabilities-Equivalent Models ‣ Appendix D Multi-Group Attention
    Family ‣ Bifurcated Attention for Single-Context Large-Batch Sampling") for analysis).
    That is, context encoding latency of the multi-query model will be slightly larger,
    as well as the low-context and low-batch incremental decoding scenario. However,
    multi-query can have significantly lower latency compared to multi-head in the
    scenario with high number of decoding steps which makes the incremental decoding
    phase being latency-dominating, and high context or batch size which heavily impacts
    the memory IO of incremental decoding. We outline three different inference scenarios
    below.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，由于较大的尺寸，使用多查询注意力存在一定的开销成本（参见图[4](#S5.F4 "Figure 4 ‣ 5.2 Latencies of Capabilities-Equivalent
    Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")和附录[D.3.1](#A4.SS3.SSS1
    "D.3.1 Context encoding ‣ D.3 Comparing Capabilities-Equivalent Models ‣ Appendix
    D Multi-Group Attention Family ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling")以及[D.3.2](#A4.SS3.SSS2 "D.3.2 Incremental Decoding ‣ D.3 Comparing Capabilities-Equivalent
    Models ‣ Appendix D Multi-Group Attention Family ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling")的分析）。也就是说，多查询模型的上下文编码延迟会略大，以及低上下文和低批量的增量解码场景。然而，在解码步骤数量较多的场景下，多查询模型的延迟可能显著低于多头模型，这使得增量解码阶段成为主导延迟因素，并且在高上下文或批量大小下，这对增量解码的内存IO影响很大。我们在下面概述了三种不同的推理场景。
- en: '![Refer to caption](img/829e95d53cecd83bc8a636dd60e31d4c.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/829e95d53cecd83bc8a636dd60e31d4c.png)'
- en: 'Figure 4: High-level latency comparison between an MH model and a larger MQ
    model with comparable capabilities. Overall, there’s an overhead cost for the
    initial context encoding latency due the additional compute with the larger MQ
    model size. For low context and batch size, the per step latency of MQ is also
    slightly higher to start due to the memory IO required for larger model size,
    but does not change much as context length $m$ or batch size $b$ grow, as supposed
    to the multi-head case where the per step latency can grow more rapidly with respect
    to $m$ and $b$.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：MH模型与具有可比能力的大型MQ模型之间的高层延迟比较。总体而言，由于大型MQ模型大小带来的额外计算，初始上下文编码延迟存在开销成本。对于低上下文和批量大小，MQ模型的每步延迟最初也略高，因为需要处理更大的模型大小所需的内存IO，但随着上下文长度$m$或批量大小$b$的增长，延迟变化不大，这与多头模型情况相反，在多头模型中，每步延迟可能随着$m$和$b$的增长而更快增加。
- en: 5.2.1 Single Context Scenario
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 单上下文场景
- en: In the single batch inference scenario, the multi-query/-group attention can
    achieve lower latency when the context length and the number of generated tokens
    are high, as demonstrated in Figure [5](#S5.F5 "Figure 5 ‣ 5.2.1 Single Context
    Scenario ‣ 5.2 Latencies of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling"). Different implementations
    that are more efficient in loading KV cache (such as lower-level kernel that can
    avoid duplicated IO) can cause the overall curves of MH to be flatter. However,
    the overall trend still remains where given sufficiently high context $m$, MQ
    will begin to be faster than MH.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在单批量推理场景中，当上下文长度和生成的令牌数量较高时，多查询/多组注意力可以实现较低的延迟，如图 [5](#S5.F5 "图 5 ‣ 5.2.1 单上下文场景
    ‣ 5.2 同等能力模型的延迟 ‣ 5 实验 ‣ 单上下文大批量采样的分叉注意力")所示。更高效加载KV缓存的不同实现（例如可以避免重复IO的低级内核）可以使MH的整体曲线更平坦。然而，总体趋势仍然存在：给定足够高的上下文
    $m$，MQ将开始比MH更快。
- en: '![Refer to caption](img/fa11368fa9e2e21f94a2ac61a2f4fcb2.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/fa11368fa9e2e21f94a2ac61a2f4fcb2.png)'
- en: 'Figure 5: Incremental decoding (per step) latency and the context encoding
    latency, as a function of input context length. In this plot, we compare an multi-head
    model and an multi-query model of comparable capabilities, whose size is slightly
    larger. (Leftmost: Per-step incremental decoding latency) For low context length
    such as $m<2500$, due to the larger size of the MQ model, the inference latency
    is higher. However, the growth with respect to context length of the MQ model
    is much lower (almost flat), resulting in lower per step latency when the context
    length is high. (Second: Context encoding latency) The context encoding latency
    depends on the FLOPs where the MH and MQ are quite similar. Note that the MQ model
    is slightly larger, and therefore corresponds to a steeper curve. (Third, Fourth):
    Total latency for 15 or 256 generated steps The two plots illustrates the *total*
    latency, which is the sum of context encoding and the the number of steps times
    incremental decoding latency. The benefits of MQ model becomes clear in the case
    of high decoding steps $(256)$ whereas in the case of $15$ generated tokens, the
    total latency of MQ can still be slightly higher than MH.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：增量解码（每步）延迟和上下文编码延迟，作为输入上下文长度的函数。在此图中，我们比较了一个多头模型和一个多查询模型，其能力相当，但大小略大。（最左侧：每步增量解码延迟）对于上下文长度较小的情况，如$m<2500$，由于MQ模型较大，推理延迟较高。然而，MQ模型的上下文长度增长几乎是平坦的，导致当上下文长度较高时每步延迟较低。（第二：上下文编码延迟）上下文编码延迟取决于FLOPs，MH和MQ相似。注意MQ模型略大，因此对应于更陡的曲线。（第三、第四）：15步或256步生成的总延迟。这两个图表说明了*总*延迟，即上下文编码和步数乘以增量解码延迟的总和。在高解码步数$(256)$的情况下，MQ模型的优势变得明显，而在生成$15$个令牌的情况下，MQ的总延迟仍可能略高于MH。
- en: '![Refer to caption](img/88443af12943f686d267f370da2eedb6.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/88443af12943f686d267f370da2eedb6.png)'
- en: (a) Multi-Head
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 多头
- en: '![Refer to caption](img/325d6505ea08d8ccb51f714b7c545551.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/325d6505ea08d8ccb51f714b7c545551.png)'
- en: (b) Multi-Query
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 多查询
- en: 'Figure 6: Context-aware bifurcated attention with multi-head attention (a)
    and multi-query attention (b). The bifurcated attention loads the KV cache in
    a context-aware manner, resulting in significantly lower latency for sampling
    under high batch sizes. For instance, in the case of multi-head attention with
    batch size $128$ and context length $10,000$, bifurcated attention results in
    $\approx 4\times$ lower the incremental decoding latency. Additionally, growth
    with respect to context length is relatively flat with bifurcated attention. With
    multi-query attention, bifurcated attention permits us to use batch sizes as high
    as $256$ or $512$ with lower latency than in the multi-head scenario.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：上下文感知的分叉注意力与多头注意力 (a) 和多查询注意力 (b)。分叉注意力以上下文感知的方式加载KV缓存，从而显著降低高批量大小下的采样延迟。例如，在批量大小为$128$和上下文长度为$10,000$的多头注意力情况下，分叉注意力使增量解码延迟降低了$\approx
    4\times$。此外，分叉注意力在上下文长度方面的增长相对平坦。使用多查询注意力时，分叉注意力使我们能够使用高达$256$或$512$的批量大小，同时延迟低于多头注意力场景。
- en: 5.2.2 Single-Context Batch Sampling
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 单上下文批量采样
- en: In this scenario, we are given a single context and generates multiple completions
    based on temperature sampling. In this case, the context encoding is independent
    of the batch size $b$ since it is performed on the single context and broadcasted
    for other batch indices (Figure [1](#S3.F1 "Figure 1 ‣ 3.2 Language Model Inference
    ‣ 3 Background ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")).
    In contrast to the batch inference scenario, this is a more practical online inference
    scenario since we are not bottlenecked by the context encoding step. Our proposed
    context-aware bifurcated attention is exactly applicable for such scenario where
    in this section we demonstrate the results in conjunction with both multi-head
    and multi-query.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们给定一个单一的上下文，并基于温度采样生成多个完成。在这种情况下，上下文编码与批量大小$b$无关，因为它是在单一上下文上执行的，并广播到其他批量索引（图
    [1](#S3.F1 "图 1 ‣ 3.2 语言模型推理 ‣ 3 背景 ‣ 单一上下文大批量采样的分叉注意力")）。与批量推理场景相比，这是一个更实际的在线推理场景，因为我们不受限于上下文编码步骤。我们提出的上下文感知分叉注意力正好适用于这种情况，在本节中我们展示了与多头和多查询的结果。
- en: Multi-head benefits significantly from bifurcated attention
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多头注意力显著受益于分叉注意力
- en: Figure [6(a)](#S5.F6.sf1 "Figure 6(a) ‣ Figure 6 ‣ 5.2.1 Single Context Scenario
    ‣ 5.2 Latencies of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling") demonstrates the per step
    latency results for a multi-head model. For instance, with batch size $8$, the
    per step latency without bifurcated attention grows rapidly with context length,
    from $\approx 10$ ms to $\approx 100$ ms at context length $10000$. However, with
    bifurcated attention, the latency remains relatively flat with respect to context
    length. In practice, bifurcated attention also reduces memory consumption at high
    batch size and context lengths without encountering out-of-memory error as early
    as without bifurcated attention.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6(a)](#S5.F6.sf1 "图 6(a) ‣ 图 6 ‣ 5.2.1 单一上下文场景 ‣ 5.2 能力等效模型的延迟 ‣ 5 实验 ‣ 单一上下文大批量采样的分叉注意力")
    展示了多头模型的每步延迟结果。例如，在批量大小为$8$的情况下，没有分叉注意力时每步延迟随上下文长度迅速增长，从$\approx 10$ ms增长到上下文长度为$10000$时的$\approx
    100$ ms。然而，使用分叉注意力时，延迟与上下文长度相对保持平稳。实际上，分叉注意力还减少了在高批量大小和上下文长度下的内存消耗，避免了比没有分叉注意力更早遇到的内存不足错误。
- en: '![Refer to caption](img/213a8fe023d82386ecf65b5d813a52c2.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/213a8fe023d82386ecf65b5d813a52c2.png)'
- en: (a) Without bifurcated attention
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在没有分叉注意力的情况下
- en: '![Refer to caption](img/cf50eec323aa84ce1040d844e5c7a85b.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cf50eec323aa84ce1040d844e5c7a85b.png)'
- en: (b) With bifurcated attention
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用分叉注意力
- en: 'Figure 7: Latency comparison between multi-head and a larger multi-query model
    of equal capabilities. Without bifurcated attention, MQ is clearly much more inference
    efficient. However, with bifurcated attention, MH can have better latency than
    MQ in moderate scenario (up to batch size 64 in this case) where MQ can handle
    more extreme scenarios better than MH.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：多头注意力与具有相同能力的较大多查询模型之间的延迟比较。在没有分叉注意力的情况下，MQ明显更具推理效率。然而，在分叉注意力的情况下，MH在中等场景（在这种情况下批量大小最多为64）中的延迟可能比MQ更好，而MQ在处理更极端的场景时比MH表现更佳。
- en: '![Refer to caption](img/e9e454e930b5829118fe83591e7dfd6a.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e9e454e930b5829118fe83591e7dfd6a.png)'
- en: 'Figure 8: Bifurcated attention improves accuracy by enabling more generated
    samples over a fixed latency budget, applicable for both multi-head attention
    (CodeGen) and multi-query attention (StarCoder). Given the $n$ samples, pass@n
    reflects the execution pass rate of the best sample among $n$, shown in (a) and
    (c). Filtering $n$ samples with mean log probability ranking yields a subset of
    best three samples, reflected by pass@top3 in (b) and (d). The increased number
    of samples within the same latency budget results in increased performance via
    either pass@n or pass@top-k.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：分叉注意力通过在固定延迟预算内生成更多样本来提高准确性，这适用于多头注意力（CodeGen）和多查询注意力（StarCoder）。给定$n$个样本，pass@n
    反映了$n$个样本中最佳样本的执行通过率，如（a）和（c）所示。使用均值对数概率排名筛选$n$个样本可以获得最佳三个样本的子集，由pass@top3在（b）和（d）中反映。在相同的延迟预算内增加样本数量会通过pass@n或pass@top-k提高性能。
- en: Bifurcated attention + multi-head rivals multi-query
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分叉注意力 + 多头注意力对抗多查询
- en: Figure [7](#S5.F7 "Figure 7 ‣ Multi-head benefits significantly from bifurcated
    attention ‣ 5.2.2 Single-Context Batch Sampling ‣ 5.2 Latencies of Capabilities-Equivalent
    Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")
    shows the comparison between MH and MQ with and without bifurcated attention.
    Without bifurcated attention, MQ is clearly much more inference efficient. However,
    with bifurcated attention, MQ and MH under moderate batch size scenarios (up to
    64) seems comparable, where multi-head is even has lower latency. The results
    indicate that, given an existing MH model, we can support batch sampling scenarios
    using bifurcated attention without the need of a multi-query model (which requires
    training a new model, or at least continuous training) (Ainslie et al., [2023](#bib.bib2)).
    With a more inference-intensive scenarios, including batch inference scenario
    where the bifurcated attention is not applicable, switching to multi-query can
    be worth the effort.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#S5.F7 "图 7 ‣ 多头在分叉注意力下的显著收益 ‣ 5.2.2 单上下文批处理采样 ‣ 5.2 能力等效模型的延迟 ‣ 5 实验 ‣
    单上下文大批量采样的分叉注意力")展示了MH和MQ在有无分叉注意力下的比较。在没有分叉注意力的情况下，MQ显然更具推理效率。然而，采用分叉注意力后，MQ和MH在中等批量大小场景（最多64）下似乎是可比的，其中多头甚至具有更低的延迟。结果表明，给定一个现有的MH模型，我们可以使用分叉注意力支持批量采样场景，而无需多查询模型（这需要训练一个新模型，或至少是持续训练）（Ainslie等，[2023](#bib.bib2)）。在更具推理密集型的场景中，包括分叉注意力不适用的批量推理场景，切换到多查询可能是值得的努力。
- en: Bifurcated attention with multi-query enables more extreme batch size and context
    lengths
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 采用多查询的分叉注意力支持更极端的批量大小和上下文长度
- en: Multi-query has overall $h$ times lower memory IO and can already reduce latency
    for some inference scenarios. With bifurcated attention, the supported context
    lengths and batch sizes can become much more extreme, as demonstrated in Figure
    [6(b)](#S5.F6.sf2 "Figure 6(b) ‣ Figure 6 ‣ 5.2.1 Single Context Scenario ‣ 5.2
    Latencies of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling").
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 多查询整体上具有 $h$ 倍较低的内存IO，并且已经可以减少一些推理场景的延迟。通过分叉注意力，支持的上下文长度和批量大小可以变得更加极端，如图[6(b)](#S5.F6.sf2
    "图 6(b) ‣ 图 6 ‣ 5.2.1 单上下文场景 ‣ 5.2 能力等效模型的延迟 ‣ 5 实验 ‣ 单上下文大批量采样的分叉注意力")所示。
- en: 5.3 Applications
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 应用
- en: 'Efficient large-scale sampling is particularly useful for downstream applications
    that require multiple generations but has latency constraints, e.g., AI code assistants.
    In this case, bifurcated attention enables generating more candidates by using
    larger batch size without incurring much additional latency. To verify our point,
    we empirically evaluate CodeGen-16B-mono (Nijkamp et al., [2022](#bib.bib50))
    and StarCoder (15.5B) (Li et al., [2023](#bib.bib38)) on MBPP dataset (Austin
    et al., [2021](#bib.bib6)), and plot pass rates with respect to latency in Figure
    [8](#S5.F8 "Figure 8 ‣ Multi-head benefits significantly from bifurcated attention
    ‣ 5.2.2 Single-Context Batch Sampling ‣ 5.2 Latencies of Capabilities-Equivalent
    Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling"),
    where we also indicate the batch size $n$. We consider two accuracy measurements:
    (1) pass@$n$ corresponds to the oracle scenario, where we evaluate all the generated
    samples and check if any of them is correct; (2) pass@top3, where we are only
    allowed to evaluate three examples no matter how many we generate. In the top-3
    case, we deduplicate the $n$ samples, and rank by their mean log probability scores
    (Chen et al., [2021](#bib.bib12)) to determine three candidates. All experiments
    use nucleus sampling with $p=0.95$ (Holtzman et al., [2020](#bib.bib28)) and temperature
    $0.8$. The results show much sharper improvement in either metrics relative to
    additional latency. This approach opens up avenues for performance improvement
    given a fixed budget of latency.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的大规模采样对需要多次生成但有延迟约束的下游应用特别有用，例如AI代码助手。在这种情况下，分叉注意力通过使用更大的批量大小生成更多候选项，而不会带来过多的额外延迟。为了验证我们的观点，我们在MBPP数据集（Austin
    et al., [2021](#bib.bib6)）上对CodeGen-16B-mono（Nijkamp et al., [2022](#bib.bib50)）和StarCoder（15.5B）（Li
    et al., [2023](#bib.bib38)）进行了实证评估，并在图[8](#S5.F8 "Figure 8 ‣ Multi-head benefits
    significantly from bifurcated attention ‣ 5.2.2 Single-Context Batch Sampling
    ‣ 5.2 Latencies of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")中绘制了通过率与延迟的关系图，其中我们还标出了批量大小$n$。我们考虑了两种准确性测量：（1）pass@$n$对应于oracle场景，我们评估所有生成的样本，并检查是否有正确的样本；（2）pass@top3，在这种情况下，无论生成多少样本，我们仅允许评估三个示例。在top-3情况下，我们对$n$个样本进行去重，并根据它们的平均对数概率分数（Chen
    et al., [2021](#bib.bib12)）进行排序以确定三个候选项。所有实验使用的是具有$p=0.95$（Holtzman et al., [2020](#bib.bib28)）的nucleus采样和温度$0.8$。结果显示，任何一个指标相对于额外的延迟都有更明显的改进。这种方法为在固定的延迟预算下提高性能开辟了新的途径。
- en: 6 Conclusion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Bifurcated attention provides a complementary approach to the existing inference
    acceleration methods, with a particular focus on minimizing the memory IO of the
    incremental decoding, thereby enhancing inference efficiency. Our work helps support
    demanding inference scenarios due to larger context during incremental decoding,
    which are emerging from, e.g., more complex applications that requires long context
    such as complex reasoning, planning, or retrieval augmented generations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分叉注意力为现有的推理加速方法提供了一种补充方法，特别关注于最小化增量解码的内存IO，从而提高推理效率。我们的工作有助于支持由于增量解码过程中更大上下文而对推理要求高的场景，这些场景来自于例如需要长上下文的复杂应用，如复杂推理、规划或增强检索生成。
- en: References
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ahmad et al. [2021] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang. Unified
    pre-training for program understanding and generation. *arXiv preprint arXiv:2103.06333*,
    2021.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad et al. [2021] W. U. Ahmad, S. Chakraborty, B. Ray, 和 K.-W. Chang. 统一预训练用于程序理解和生成。*arXiv
    preprint arXiv:2103.06333*，2021年。
- en: 'Ainslie et al. [2023] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy,
    F. Lebrón, and S. Sanghai. GQA: training generalized multi-query transformer models
    from multi-head checkpoints. *CoRR*, abs/2305.13245, 2023. doi: 10.48550/arXiv.2305.13245.
    URL [https://doi.org/10.48550/arXiv.2305.13245](https://doi.org/10.48550/arXiv.2305.13245).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ainslie et al. [2023] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy,
    F. Lebrón, 和 S. Sanghai. GQA：从多头检查点训练通用的多查询变换器模型。*CoRR*，abs/2305.13245，2023年。doi:
    10.48550/arXiv.2305.13245。网址 [https://doi.org/10.48550/arXiv.2305.13245](https://doi.org/10.48550/arXiv.2305.13245)。'
- en: 'Allal et al. [2023] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M.
    Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, et al. Santacoder: don’t
    reach for the stars! *arXiv preprint arXiv:2301.03988*, 2023.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Allal et al. [2023] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M.
    Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey 等. Santacoder: 不要伸手去摘星星！ *arXiv
    preprint arXiv:2301.03988*，2023年。'
- en: Amazon [2022] Amazon. Amazon code whisperer. [https://aws.amazon.com/codewhisperer/](https://aws.amazon.com/codewhisperer/),
    2022.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon [2022] Amazon. Amazon代码助手。 [https://aws.amazon.com/codewhisperer/](https://aws.amazon.com/codewhisperer/)，2022年。
- en: 'Athiwaratkun et al. [2022] B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian,
    M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, S. K. Gonugondla, H. Ding, V. Kumar,
    N. Fulton, A. Farahani, S. Jain, R. Giaquinto, H. Qian, M. K. Ramanathan, R. Nallapati,
    B. Ray, P. Bhatia, S. Sengupta, D. Roth, and B. Xiang. Multi-lingual evaluation
    of code generation models. *CoRR*, abs/2210.14868, 2022. doi: 10.48550/arXiv.2210.14868.
    URL [https://doi.org/10.48550/arXiv.2210.14868](https://doi.org/10.48550/arXiv.2210.14868).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Athiwaratkun 等人 [2022] B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y. Tian,
    M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, S. K. Gonugondla, H. Ding, V.
    Kumar, N. Fulton, A. Farahani, S. Jain, R. Giaquinto, H. Qian, M. K. Ramanathan,
    R. Nallapati, B. Ray, P. Bhatia, S. Sengupta, D. Roth, 和 B. Xiang. 代码生成模型的多语言评估。*CoRR*,
    abs/2210.14868, 2022。doi: 10.48550/arXiv.2210.14868。URL [https://doi.org/10.48550/arXiv.2210.14868](https://doi.org/10.48550/arXiv.2210.14868)。'
- en: Austin et al. [2021] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski,
    D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton. Program synthesis
    with large language models. *CoRR*, abs/2108.07732, 2021. URL [https://arxiv.org/abs/2108.07732](https://arxiv.org/abs/2108.07732).
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin 等人 [2021] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D.
    Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, 和 C. Sutton. 使用大型语言模型进行程序合成。*CoRR*,
    abs/2108.07732, 2021。URL [https://arxiv.org/abs/2108.07732](https://arxiv.org/abs/2108.07732)。
- en: 'Beltagy et al. [2020] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The
    long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等人 [2020] I. Beltagy, M. E. Peters, 和 A. Cohan. Longformer: 长文档 Transformer。*arXiv
    预印本 arXiv:2004.05150*, 2020。'
- en: Brown et al. [2020] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
    T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,
    E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei. Language models are few-shot learners. *CoRR*, abs/2005.14165,
    2020. URL [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
    A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G.
    Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C.
    Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner,
    S. McCandlish, A. Radford, I. Sutskever, 和 D. Amodei. 语言模型是少样本学习者。*CoRR*, abs/2005.14165,
    2020。URL [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)。
- en: Bulatov et al. [2023] A. Bulatov, Y. Kuratov, and M. S. Burtsev. Scaling transformer
    to 1m tokens and beyond with rmt. *arXiv preprint arXiv:2304.11062*, 2023.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulatov 等人 [2023] A. Bulatov, Y. Kuratov, 和 M. S. Burtsev. 将 Transformer 扩展到
    1M 令牌及更远，使用 RMT。*arXiv 预印本 arXiv:2304.11062*, 2023。
- en: 'Cai et al. [2024] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and
    T. Dao. Medusa: Simple llm inference acceleration framework with multiple decoding
    heads, 2024.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai 等人 [2024] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, 和 T. Dao.
    Medusa: 具有多个解码头的简单 LLM 推理加速框架, 2024。'
- en: 'Chen et al. [2023] C. Chen, S. Borgeaud, G. Irving, J. Lespiau, L. Sifre, and
    J. Jumper. Accelerating large language model decoding with speculative sampling.
    *CoRR*, abs/2302.01318, 2023. doi: 10.48550/arXiv.2302.01318. URL [https://doi.org/10.48550/arXiv.2302.01318](https://doi.org/10.48550/arXiv.2302.01318).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 [2023] C. Chen, S. Borgeaud, G. Irving, J. Lespiau, L. Sifre, 和 J.
    Jumper. 使用猜测采样加速大型语言模型解码。*CoRR*, abs/2302.01318, 2023。doi: 10.48550/arXiv.2302.01318。URL
    [https://doi.org/10.48550/arXiv.2302.01318](https://doi.org/10.48550/arXiv.2302.01318)。'
- en: Chen et al. [2021] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto,
    J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger,
    M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino,
    N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N.
    Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba. Evaluating large language models trained on code. *CoRR*, abs/2107.03374,
    2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2021] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto,
    J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger,
    M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A.
    Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
    A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
    M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
    I. Sutskever, 和 W. Zaremba. 评估基于代码训练的大型语言模型。*CoRR*, abs/2107.03374, 2021。URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374)。
- en: Child et al. [2019] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating
    long sequences with sparse transformers. *URL https://openai.com/blog/sparse-transformers*,
    2019.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等人 [2019] R. Child, S. Gray, A. Radford, 和 I. Sutskever. 使用稀疏转换器生成长序列。*网址
    https://openai.com/blog/sparse-transformers*，2019。
- en: 'Choquette et al. [2021] J. Choquette, W. Gandhi, O. Giroux, N. Stam, and R. Krashinsky.
    Nvidia a100 tensor core gpu: Performance and innovation. *IEEE Micro*, 41(2):29–35,
    2021.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choquette 等人 [2021] J. Choquette, W. Gandhi, O. Giroux, N. Stam, 和 R. Krashinsky.
    Nvidia A100 张量核心 GPU：性能与创新。*IEEE Micro*，41(2):29–35，2021。
- en: 'Chowdhery et al. [2022] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
    A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi,
    S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran,
    E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari,
    P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia,
    V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph,
    A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S.
    Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou,
    X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern,
    D. Eck, J. Dean, S. Petrov, and N. Fiedel. Palm: Scaling language modeling with
    pathways. *CoRR*, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL [https://doi.org/10.48550/arXiv.2204.02311](https://doi.org/10.48550/arXiv.2204.02311).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等人 [2022] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
    A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi,
    S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran,
    E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari,
    P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia,
    V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph,
    A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S.
    Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z.
    Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern,
    D. Eck, J. Dean, S. Petrov, 和 N. Fiedel. Palm: 通过路径扩展语言建模。*CoRR*，abs/2204.02311，2022。doi:
    10.48550/arXiv.2204.02311。网址 [https://doi.org/10.48550/arXiv.2204.02311](https://doi.org/10.48550/arXiv.2204.02311)。'
- en: 'Costa-jussà et al. [2022] M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad,
    K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, et al.
    No language left behind: Scaling human-centered machine translation. *arXiv preprint
    arXiv:2207.04672*, 2022.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa-jussà 等人 [2022] M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K.
    Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard 等人. 无语言被遗忘：以人为本的机器翻译扩展。*arXiv
    预印本 arXiv:2207.04672*，2022。
- en: 'Dao et al. [2022] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. In *NeurIPS*, 2022.
    URL [http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao 等人 [2022] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, 和 C. Ré. Flashattention:
    快速且内存高效的精确注意力与 io 感知。在 *NeurIPS*，2022。网址 [http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html)。'
- en: 'Dathathri et al. [2019] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank,
    P. Molino, J. Yosinski, and R. Liu. Plug and play language models: A simple approach
    to controlled text generation. *arXiv preprint arXiv:1912.02164*, 2019.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dathathri 等人 [2019] S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P.
    Molino, J. Yosinski, 和 R. Liu. 插件和玩语言模型：一种简单的受控文本生成方法。*arXiv 预印本 arXiv:1912.02164*，2019。
- en: 'Dettmers et al. [2022] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer.
    Llm.int8(): 8-bit matrix multiplication for transformers at scale. *CoRR*, abs/2208.07339,
    2022. doi: 10.48550/arXiv.2208.07339. URL [https://doi.org/10.48550/arXiv.2208.07339](https://doi.org/10.48550/arXiv.2208.07339).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2022] T. Dettmers, M. Lewis, Y. Belkada, 和 L. Zettlemoyer. Llm.int8():
    适用于大规模转换器的 8 位矩阵乘法。*CoRR*，abs/2208.07339，2022。doi: 10.48550/arXiv.2208.07339。网址
    [https://doi.org/10.48550/arXiv.2208.07339](https://doi.org/10.48550/arXiv.2208.07339)。'
- en: Farhad et al. [2021] A. Farhad, A. Arkady, B. Magdalena, B. Ondřej, C. Rajen,
    C. Vishrav, M. R. Costa-jussa, E.-B. Cristina, F. Angela, F. Christian, et al.
    Findings of the 2021 conference on machine translation (wmt21). In *Proceedings
    of the Sixth Conference on Machine Translation*, pages 1–88\. Association for
    Computational Linguistics, 2021.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Farhad 等人 [2021] A. Farhad, A. Arkady, B. Magdalena, B. Ondřej, C. Rajen, C.
    Vishrav, M. R. Costa-jussa, E.-B. Cristina, F. Angela, F. Christian 等人. 2021 年机器翻译会议（wmt21）发现。在
    *第六届机器翻译会议论文集* 中，第 1–88 页。计算语言学协会，2021。
- en: 'Frantar et al. [2022] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh.
    Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等 [2022] E. Frantar, S. Ashkboos, T. Hoefler 和 D. Alistarh。Gptq：生成预训练变换器的准确后训练量化。*arXiv
    预印本 arXiv:2210.17323*，2022。
- en: 'Fried et al. [2022] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi,
    R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis. Incoder: A generative model
    for code infilling and synthesis. *arXiv preprint arXiv:2204.05999*, 2022.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fried 等 [2022] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi,
    R. Zhong, W.-t. Yih, L. Zettlemoyer 和 M. Lewis。Incoder：一种用于代码填充和合成的生成模型。*arXiv
    预印本 arXiv:2204.05999*，2022。
- en: Fu et al. [2023] Y. Fu, P. Bailis, I. Stoica, and H. Zhang. Breaking the sequential
    dependency of llm inference using lookahead decoding, November 2023. URL [https://lmsys.org/blog/2023-11-21-lookahead-decoding/](https://lmsys.org/blog/2023-11-21-lookahead-decoding/).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 [2023] Y. Fu, P. Bailis, I. Stoica 和 H. Zhang。通过前瞻解码打破 LLM 推理的顺序依赖，2023年11月。网址
    [https://lmsys.org/blog/2023-11-21-lookahead-decoding/](https://lmsys.org/blog/2023-11-21-lookahead-decoding/)。
- en: 'Gehman et al. [2020] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith.
    Realtoxicityprompts: Evaluating neural toxic degeneration in language models.
    *arXiv preprint arXiv:2009.11462*, 2020.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehman 等 [2020] S. Gehman, S. Gururangan, M. Sap, Y. Choi 和 N. A. Smith。Realtoxicityprompts：评估语言模型中的神经毒性退化。*arXiv
    预印本 arXiv:2009.11462*，2020。
- en: Google [2023] Google. Bard. [https://blog.google/technology/ai/try-bard/](https://blog.google/technology/ai/try-bard/),
    2023.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google [2023] Google。Bard。 [https://blog.google/technology/ai/try-bard/](https://blog.google/technology/ai/try-bard/)，2023。
- en: Guu et al. [2020] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval
    augmented language model pre-training. In *International conference on machine
    learning*, pages 3929–3938\. PMLR, 2020.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu 等 [2020] K. Guu, K. Lee, Z. Tung, P. Pasupat 和 M. Chang。检索增强语言模型的预训练。在 *国际机器学习会议*，第
    3929–3938 页。PMLR，2020。
- en: Hoffmann et al. [2022] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya,
    T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al.
    Training compute-optimal large language models. *arXiv preprint arXiv:2203.15556*,
    2022.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann 等 [2022] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,
    E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark 等。计算优化的大型语言模型训练。*arXiv
    预印本 arXiv:2203.15556*，2022。
- en: Holtzman et al. [2020] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi.
    The curious case of neural text degeneration. In *8th International Conference
    on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*.
    OpenReview.net, 2020. URL [https://openreview.net/forum?id=rygGQyrFvH](https://openreview.net/forum?id=rygGQyrFvH).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman 等 [2020] A. Holtzman, J. Buys, L. Du, M. Forbes 和 Y. Choi。神经文本退化的奇怪案例。在
    *第八届国际学习表示会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26日至30日*。OpenReview.net，2020。网址 [https://openreview.net/forum?id=rygGQyrFvH](https://openreview.net/forum?id=rygGQyrFvH)。
- en: 'Izacard et al. [2022] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni,
    T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave. Atlas: Few-shot
    learning with retrieval augmented language models. *arXiv preprint arXiv*, 2208,
    2022.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 等 [2022] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T.
    Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel 和 E. Grave。Atlas：具有检索增强语言模型的少量学习。*arXiv
    预印本 arXiv*，2208，2022。
- en: Jia et al. [2018] Z. Jia, M. Maggioni, B. Staiger, and D. P. Scarpazza. Dissecting
    the NVIDIA volta GPU architecture via microbenchmarking. *CoRR*, abs/1804.06826,
    2018. URL [http://arxiv.org/abs/1804.06826](http://arxiv.org/abs/1804.06826).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等 [2018] Z. Jia, M. Maggioni, B. Staiger 和 D. P. Scarpazza。通过微基准测试剖析 NVIDIA
    volta GPU 架构。*CoRR*，abs/1804.06826，2018。网址 [http://arxiv.org/abs/1804.06826](http://arxiv.org/abs/1804.06826)。
- en: Kalamkar et al. [2019] D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee,
    S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, et al. A study
    of bfloat16 for deep learning training. *arXiv preprint arXiv:1905.12322*, 2019.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalamkar 等 [2019] D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee,
    S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen 等。关于深度学习训练的 bfloat16
    研究。*arXiv 预印本 arXiv:1905.12322*，2019。
- en: Kaplan et al. [2020] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,
    R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language
    models. *CoRR*, abs/2001.08361, 2020. URL [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan 等 [2020] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,
    R. Child, S. Gray, A. Radford, J. Wu 和 D. Amodei。神经语言模型的规模法则。*CoRR*，abs/2001.08361，2020。网址
    [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)。
- en: 'Kingma and Ba [2014] D. P. Kingma and J. Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba [2014] D. P. Kingma 和 J. Ba。Adam：一种用于随机优化的方法。*arXiv 预印本 arXiv:1412.6980*，2014。
- en: 'Kuzmin et al. [2022] A. Kuzmin, M. Van Baalen, Y. Ren, M. Nagel, J. Peters,
    and T. Blankevoort. Fp8 quantization: The power of the exponent. *arXiv preprint
    arXiv:2208.09225*, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuzmin 等人 [2022] A. Kuzmin, M. Van Baalen, Y. Ren, M. Nagel, J. Peters, 和 T.
    Blankevoort. Fp8 量化: 指数的力量。*arXiv 预印本 arXiv:2208.09225*, 2022。'
- en: Kwon et al. [2023] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu,
    J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large
    language model serving with pagedattention, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人 [2023] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E.
    Gonzalez, H. Zhang, 和 I. Stoica. 针对大型语言模型服务的高效内存管理与分页注意力, 2023。
- en: 'Le et al. [2022] H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi.
    Coderl: Mastering code generation through pretrained models and deep reinforcement
    learning. *Advances in Neural Information Processing Systems*, 35:21314–21328,
    2022.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Le 等人 [2022] H. Le, Y. Wang, A. D. Gotmare, S. Savarese, 和 S. C. H. Hoi. Coderl:
    通过预训练模型和深度强化学习掌握代码生成。*神经信息处理系统进展*, 35:21314–21328, 2022。'
- en: 'Leviathan et al. [2022] Y. Leviathan, M. Kalman, and Y. Matias. Fast inference
    from transformers via speculative decoding. *CoRR*, abs/2211.17192, 2022. doi:
    10.48550/arXiv.2211.17192. URL [https://doi.org/10.48550/arXiv.2211.17192](https://doi.org/10.48550/arXiv.2211.17192).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Leviathan 等人 [2022] Y. Leviathan, M. Kalman, 和 Y. Matias. 通过推测解码实现快速推理。*CoRR*,
    abs/2211.17192, 2022. doi: 10.48550/arXiv.2211.17192. URL [https://doi.org/10.48550/arXiv.2211.17192](https://doi.org/10.48550/arXiv.2211.17192)。'
- en: 'Li et al. [2023] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,
    M. Marone, C. Akiki, J. Li, J. Chim, et al. Starcoder: may the source be with
    you! *arXiv preprint arXiv:2305.06161*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2023] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,
    M. Marone, C. Akiki, J. Li, J. Chim, 等人. Starcoder: 愿源代码与你同在！*arXiv 预印本 arXiv:2305.06161*,
    2023。'
- en: Li et al. [2022] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
    T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation
    with alphacode. *Science*, 378(6624):1092–1097, 2022.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2022] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
    T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, 等人. 竞争级代码生成与 alphacode。*科学*, 378(6624):1092–1097,
    2022。
- en: 'Li et al. [2024] Y. Li, F. Wei, C. Zhang, and H. Zhang. Eagle: Speculative
    sampling requires rethinking feature uncertainty, 2024.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2024] Y. Li, F. Wei, C. Zhang, 和 H. Zhang. Eagle: 推测采样需要重新思考特征不确定性,
    2024。'
- en: 'Lin and Riedl [2021] Z. Lin and M. Riedl. Plug-and-blend: A framework for controllable
    story generation with blended control codes. *arXiv preprint arXiv:2104.04039*,
    2021.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 和 Riedl [2021] Z. Lin 和 M. Riedl. Plug-and-blend: 一种用于可控故事生成的混合控制代码框架。*arXiv
    预印本 arXiv:2104.04039*, 2021。'
- en: Loshchilov and Hutter [2017] I. Loshchilov and F. Hutter. Decoupled weight decay
    regularization. *arXiv preprint arXiv:1711.05101*, 2017.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter [2017] I. Loshchilov 和 F. Hutter. 解耦权重衰减正则化。*arXiv 预印本 arXiv:1711.05101*,
    2017。
- en: Madaan et al. [2023] A. Madaan, A. Shypula, U. Alon, M. Hashemi, P. Ranganathan,
    Y. Yang, G. Neubig, and A. Yazdanbakhsh. Learning performance-improving code edits.
    *arXiv preprint arXiv:2302.07867*, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等人 [2023] A. Madaan, A. Shypula, U. Alon, M. Hashemi, P. Ranganathan,
    Y. Yang, G. Neubig, 和 A. Yazdanbakhsh. 学习提升性能的代码编辑。*arXiv 预印本 arXiv:2302.07867*,
    2023。
- en: Menick et al. [2022] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song,
    M. Chadwick, M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al. Teaching
    language models to support answers with verified quotes. *arXiv preprint arXiv:2203.11147*,
    2022.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menick 等人 [2022] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M.
    Chadwick, M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, 等人. 教授语言模型以支持带有验证引用的回答。*arXiv
    预印本 arXiv:2203.11147*, 2022。
- en: 'Miao et al. [2023] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y.
    Wong, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia. Specinfer: Accelerating generative
    LLM serving with speculative inference and token tree verification. *CoRR*, abs/2305.09781,
    2023. doi: 10.48550/ARXIV.2305.09781. URL [https://doi.org/10.48550/arXiv.2305.09781](https://doi.org/10.48550/arXiv.2305.09781).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miao 等人 [2023] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong,
    Z. Chen, D. Arfeen, R. Abhyankar, 和 Z. Jia. Specinfer: 通过推测推理和令牌树验证加速生成 LLM 服务。*CoRR*,
    abs/2305.09781, 2023. doi: 10.48550/ARXIV.2305.09781. URL [https://doi.org/10.48550/arXiv.2305.09781](https://doi.org/10.48550/arXiv.2305.09781)。'
- en: '[46] Microsoft. Github copilot. [https://github.com/features/copilot](https://github.com/features/copilot).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] 微软. Github copilot. [https://github.com/features/copilot](https://github.com/features/copilot)。'
- en: 'Mirowski et al. [2023] P. Mirowski, K. W. Mathewson, J. Pittman, and R. Evans.
    Co-writing screenplays and theatre scripts with language models: Evaluation by
    industry professionals. In *Proceedings of the 2023 CHI Conference on Human Factors
    in Computing Systems*, pages 1–34, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirowski 等人 [2023] P. Mirowski, K. W. Mathewson, J. Pittman 和 R. Evans. 与语言模型共同编写剧本和戏剧剧本：由行业专业人士评估。发表于
    *2023年CHI人机计算系统会议论文集*，第1–34页，2023年。
- en: 'Nadeem et al. [2020] M. Nadeem, A. Bethke, and S. Reddy. Stereoset: Measuring
    stereotypical bias in pretrained language models. *arXiv preprint arXiv:2004.09456*,
    2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nadeem 等人 [2020] M. Nadeem, A. Bethke 和 S. Reddy. Stereoset：衡量预训练语言模型中的刻板偏见。*arXiv预印本
    arXiv:2004.09456*，2020年。
- en: 'Nakano et al. [2021] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,
    C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering
    with human feedback. *arXiv preprint arXiv:2112.09332*, 2021.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano 等人 [2021] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,
    C. Hesse, S. Jain, V. Kosaraju, W. Saunders 等人。WebGPT：通过人工反馈进行浏览器辅助问答。*arXiv预印本
    arXiv:2112.09332*，2021年。
- en: 'Nijkamp et al. [2022] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,
    S. Savarese, and C. Xiong. Codegen: An open large language model for code with
    multi-turn program synthesis. *arXiv preprint arXiv:2203.13474*, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等人 [2022] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,
    S. Savarese 和 C. Xiong. Codegen：一种用于代码的开放大型语言模型，具有多轮程序合成能力。*arXiv预印本 arXiv:2203.13474*，2022年。
- en: 'Nijkamp et al. [2023] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou.
    Codegen2: Lessons for training llms on programming and natural languages. *arXiv
    preprint arXiv:2305.02309*, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等人 [2023] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese 和 Y. Zhou. Codegen2：关于在编程和自然语言上训练LLMs的经验教训。*arXiv预印本
    arXiv:2305.02309*，2023年。
- en: '[52] NVIDIA. Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] NVIDIA. Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)。'
- en: 'OpenAI [2023] OpenAI. GPT-4 technical report. *CoRR*, abs/2303.08774, 2023.
    doi: 10.48550/arXiv.2303.08774. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI [2023] OpenAI. GPT-4技术报告。*CoRR*，abs/2303.08774，2023年。doi: 10.48550/arXiv.2303.08774。网址
    [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)。'
- en: 'Paszke et al. [2019] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
    G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf,
    E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
    J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning
    library. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B.
    Fox, and R. Garnett, editors, *Advances in Neural Information Processing Systems
    32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, 2019, Vancouver, BC, Canada*, pages 8024–8035, 2019. URL [https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paszke 等人 [2019] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
    T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpff, E. Z. Yang,
    Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai
    和 S. Chintala. Pytorch：一种命令式风格的高性能深度学习库。发表于 H. M. Wallach, H. Larochelle, A. Beygelzimer,
    F. d’Alché-Buc, E. B. Fox 和 R. Garnett 编辑的 *2019年神经信息处理系统年会：NeurIPS 2019，2019年12月8-14日，加拿大温哥华*，第8024–8035页，2019年。网址
    [https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html)。
- en: Pearce et al. [2022] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri.
    Asleep at the keyboard? assessing the security of github copilot’s code contributions.
    In *2022 IEEE Symposium on Security and Privacy (SP)*, pages 754–768\. IEEE, 2022.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce 等人 [2022] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt 和 R. Karri. 在键盘上打盹？评估
    GitHub Copilot 代码贡献的安全性。发表于 *2022年IEEE安全与隐私研讨会（SP）*，第754–768页。IEEE，2022年。
- en: 'Pope et al. [2022] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury,
    A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer
    inference. *CoRR*, abs/2211.05102, 2022. doi: 10.48550/arXiv.2211.05102. URL [https://doi.org/10.48550/arXiv.2211.05102](https://doi.org/10.48550/arXiv.2211.05102).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pope 等人 [2022] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A.
    Levskaya, J. Heek, K. Xiao, S. Agrawal 和 J. Dean. 高效扩展变换器推理。*CoRR*，abs/2211.05102，2022年。doi:
    10.48550/arXiv.2211.05102。网址 [https://doi.org/10.48550/arXiv.2211.05102](https://doi.org/10.48550/arXiv.2211.05102)。'
- en: 'Rasley et al. [2020] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed:
    System optimizations enable training deep learning models with over 100 billion
    parameters. In *Proceedings of the 26th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining*, pages 3505–3506, 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasley 等 [2020] J. Rasley, S. Rajbhandari, O. Ruwase, 和 Y. He. Deepspeed：系统优化使得训练超过
    1000 亿参数的深度学习模型成为可能。在 *第26届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*，页 3505–3506，2020年。
- en: Roziere et al. [2020] B. Roziere, M.-A. Lachaux, L. Chanussot, and G. Lample.
    Unsupervised translation of programming languages. *Advances in Neural Information
    Processing Systems*, 33:20601–20611, 2020.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere 等 [2020] B. Roziere, M.-A. Lachaux, L. Chanussot, 和 G. Lample. 无监督编程语言翻译。*神经信息处理系统进展*，33:20601–20611，2020年。
- en: 'Schick et al. [2023] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli,
    L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach
    themselves to use tools. *arXiv preprint arXiv:2302.04761*, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 等 [2023] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli,
    L. Zettlemoyer, N. Cancedda, 和 T. Scialom. Toolformer：语言模型可以自学使用工具。*arXiv 预印本
    arXiv:2302.04761*，2023年。
- en: 'Shazeer [2019] N. Shazeer. Fast transformer decoding: One write-head is all
    you need. *CoRR*, abs/1911.02150, 2019. URL [http://arxiv.org/abs/1911.02150](http://arxiv.org/abs/1911.02150).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer [2019] N. Shazeer. 快速变换器解码：一个写头就足够了。*CoRR*，abs/1911.02150，2019年。网址 [http://arxiv.org/abs/1911.02150](http://arxiv.org/abs/1911.02150)。
- en: 'Shoeybi et al. [2019] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper,
    and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models
    using model parallelism. *arXiv preprint arXiv:1909.08053*, 2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shoeybi 等 [2019] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, 和
    B. Catanzaro. Megatron-lm：使用模型并行训练数十亿参数的语言模型。*arXiv 预印本 arXiv:1909.08053*，2019年。
- en: 'Team [2023] M. N. Team. Introducing mpt-7b: A new standard for open-source,
    commercially usable llms. 2023. URL [www.mosaicml.com/blog/mpt-7b](www.mosaicml.com/blog/mpt-7b).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队 [2023] M. N. Team. 介绍 mpt-7b：开源、商业化使用的 llms 的新标准。2023年。网址 [www.mosaicml.com/blog/mpt-7b](www.mosaicml.com/blog/mpt-7b)。
- en: 'Touvron et al. [2023] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux,
    T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin,
    E. Grave, and G. Lample. Llama: Open and efficient foundation language models.
    *CoRR*, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL [https://doi.org/10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 [2023] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux,
    T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin,
    E. Grave, 和 G. Lample. Llama：开源且高效的基础语言模型。*CoRR*，abs/2302.13971，2023年。doi: 10.48550/arXiv.2302.13971。网址
    [https://doi.org/10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971)。'
- en: Tran et al. [2021] C. Tran, S. Bhosale, J. Cross, P. Koehn, S. Edunov, and A. Fan.
    Facebook ai wmt21 news translation task submission. *arXiv preprint arXiv:2108.03265*,
    2021.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等 [2021] C. Tran, S. Bhosale, J. Cross, P. Koehn, S. Edunov, 和 A. Fan.
    Facebook ai wmt21 新闻翻译任务提交。*arXiv 预印本 arXiv:2108.03265*，2021年。
- en: 'Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon,
    U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and
    R. Garnett, editors, *Advances in Neural Information Processing Systems 30: Annual
    Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
    Long Beach, CA, USA*, pages 5998–6008, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vaswani 等 [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, 和 I. Polosukhin. 注意力机制是你所需要的一切。在 I. Guyon, U. von Luxburg,
    S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, 和 R. Garnett 编者，《*神经信息处理系统进展
    30: 2017 年神经信息处理系统年会，2017年12月4-9日，加州长滩*》，页 5998–6008，2017年。网址 [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)。'
- en: 'Wang et al. [2020] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer:
    Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*, 2020.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2020] S. Wang, B. Z. Li, M. Khabsa, H. Fang, 和 H. Ma. Linformer：具有线性复杂度的自注意力机制。*arXiv
    预印本 arXiv:2006.04768*，2020年。
- en: 'Wei et al. [2023] X. Wei, S. K. Gonugondla, W. U. Ahmad, S. Wang, B. Ray, H. Qian,
    X. Li, V. Kumar, Z. Wang, Y. Tian, Q. Sun, B. Athiwaratkun, M. Shang, M. K. Ramanathan,
    P. Bhatia, and B. Xiang. Greener yet powerful: Taming large code generation models
    with quantization. *CoRR*, abs/2303.05378, 2023. doi: 10.48550/arXiv.2303.05378.
    URL [https://doi.org/10.48550/arXiv.2303.05378](https://doi.org/10.48550/arXiv.2303.05378).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等人 [2023] X. Wei, S. K. Gonugondla, W. U. Ahmad, S. Wang, B. Ray, H. Qian,
    X. Li, V. Kumar, Z. Wang, Y. Tian, Q. Sun, B. Athiwaratkun, M. Shang, M. K. Ramanathan,
    P. Bhatia, 和 B. Xiang. 更环保而强大：用量化驯服大型代码生成模型。*CoRR*，abs/2303.05378，2023。doi: 10.48550/arXiv.2303.05378。网址
    [https://doi.org/10.48550/arXiv.2303.05378](https://doi.org/10.48550/arXiv.2303.05378)。'
- en: 'Wolf et al. [2019] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,
    P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface’s transformers:
    State-of-the-art natural language processing. *arXiv preprint arXiv:1910.03771*,
    2019.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等人 [2019] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,
    P. Cistac, T. Rault, R. Louf, M. Funtowicz, 等人. Huggingface 的变换器：最先进的自然语言处理。*arXiv
    预印本 arXiv:1910.03771*，2019。
- en: 'Xiao et al. [2022] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant:
    Accurate and efficient post-training quantization for large language models. *arXiv
    preprint arXiv:2211.10438*, 2022.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 [2022] G. Xiao, J. Lin, M. Seznec, J. Demouth, 和 S. Han. Smoothquant：大型语言模型的准确高效后训练量化。*arXiv
    预印本 arXiv:2211.10438*，2022。
- en: 'Yao et al. [2022] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He.
    Zeroquant: Efficient and affordable post-training quantization for large-scale
    transformers. In *NeurIPS*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2022] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, 和 Y. He. Zeroquant：用于大规模变换器的高效且经济的后训练量化。收录于*NeurIPS*，2022。网址
    [http://papers.nips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html)。
- en: Yee et al. [2019] K. Yee, N. Ng, Y. N. Dauphin, and M. Auli. Simple and effective
    noisy channel modeling for neural machine translation. *arXiv preprint arXiv:1908.05731*,
    2019.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yee 等人 [2019] K. Yee, N. Ng, Y. N. Dauphin, 和 M. Auli. 简单有效的神经机器翻译噪声通道建模。*arXiv
    预印本 arXiv:1908.05731*，2019。
- en: 'Yuan et al. [2022] A. Yuan, A. Coenen, E. Reif, and D. Ippolito. Wordcraft:
    story writing with large language models. In *27th International Conference on
    Intelligent User Interfaces*, pages 841–852, 2022.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 [2022] A. Yuan, A. Coenen, E. Reif, 和 D. Ippolito. Wordcraft：使用大型语言模型进行故事写作。收录于*第
    27 届智能用户界面国际会议*，页码 841–852，2022。
- en: 'Zaheer et al. [2020] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,
    S. Ontañón, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed. Big bird: Transformers
    for longer sequences. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
    H. Lin, editors, *Advances in Neural Information Processing Systems 33: Annual
    Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
    6-12, 2020, virtual*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等人 [2020] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,
    S. Ontañón, P. Pham, A. Ravula, Q. Wang, L. Yang, 和 A. Ahmed. 大鸟：用于更长序列的变换器。收录于
    H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, 和 H. Lin 主编的*《神经信息处理系统年会论文集
    33: 2020 年神经信息处理系统大会，NeurIPS 2020，2020 年 12 月 6-12 日，虚拟会议》*，2020。网址 [https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html)。'
- en: Zhen et al. [2022] C. Zhen, Y. Shang, X. Liu, Y. Li, Y. Chen, and D. Zhang.
    A survey on knowledge-enhanced pre-trained language models. *arXiv preprint arXiv:2212.13428*,
    2022.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhen 等人 [2022] C. Zhen, Y. Shang, X. Liu, Y. Li, Y. Chen, 和 D. Zhang. 关于知识增强预训练语言模型的调查。*arXiv
    预印本 arXiv:2212.13428*，2022。
- en: Appendix A FAQs
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 常见问题
- en: '1.'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Q: If we already have an MQ model that seems to be quite efficient at large
    batch sampling, is bifurcated attention necessary?'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Q: 如果我们已经有一个在大批次采样方面相当高效的 MQ 模型，那么分叉注意力是否有必要？'
- en: 'A: The proposed context-aware bifurcated attention is an exact computation
    that provides a different way to perform attention, so one can use it "for free"
    without a performance tradeoff. Due to the reduced memory I/O, it enables more
    extreme cases of batch sampling, such as a larger batch, even for long contexts.'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'A: 提出的上下文感知分叉注意力是一种精确计算的方法，提供了一种不同的注意力执行方式，因此可以“免费”使用，而不会影响性能。由于内存I/O的减少，它支持更极端的批次采样情况，如更大的批次，即使对于长上下文也是如此。'
- en: '2.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Q: How applicable is multi-query for single-batch inference without high batch
    sampling?'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问：多查询在单批次推理中没有高批次采样的适用性如何？
- en: 'A: If the context is long and the number of generated tokens is high, then
    the benefits of multi-query are clear. Please see Section [5.2.1](#S5.SS2.SSS1
    "5.2.1 Single Context Scenario ‣ 5.2 Latencies of Capabilities-Equivalent Models
    ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling").'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：如果上下文很长且生成的令牌数量很高，那么多查询的好处是明显的。请参见[5.2.1节](#S5.SS2.SSS1 "5.2.1 单上下文场景 ‣ 5.2
    功能等效模型的延迟 ‣ 5 实验 ‣ 单上下文大批次采样的分叉注意力")。
- en: '3.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Q: Is bifurcated attention applicable for the case where we process different
    inputs in a batch?'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问：分叉注意力是否适用于我们在一个批次中处理不同输入的情况？
- en: 'A: No. In that case, if we need a solution to reduce memory I/O during incremental
    decoding, then multi-query attention can be appealing, especially in scenarios
    with a high number of generated tokens where the incremental decoding phase dominates
    the overall latency. This is because there is an overhead to multi-query due to
    the context encoding phase, as outlined in the main paper.'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：不。在这种情况下，如果我们需要一种减少增量解码期间内存I/O的解决方案，那么多查询注意力可能会很有吸引力，尤其是在生成令牌数量很高的情况下，其中增量解码阶段主导了整体延迟。这是因为多查询在上下文编码阶段会有开销，如主论文中所述。
- en: '4.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Q: Any caveats to using bifurcated attention?'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问：使用分叉注意力是否有任何注意事项？
- en: 'A: For small workloads (low context length and batch size), due to the fact
    that we split the attention into two parts, there can be less parallelization
    of the GEMM kernels, which could lead to higher latency, especially for MQ models.
    However, one can get the best of both worlds given any model by triggering bifurcated
    attention under high workload scenarios and using normal attention otherwise.
    With such a workload-based switch, bifurcated attention is guaranteed to provide
    better latency and efficiency.'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：对于小型工作负载（低上下文长度和批次大小），由于我们将注意力分成两部分，这可能导致GEMM内核的并行化减少，从而可能导致较高的延迟，尤其是对于MQ模型。然而，针对任何模型，通过在高工作负载场景下触发分叉注意力并在其他情况下使用普通注意力，可以兼顾两者的优势。通过这种基于工作负载的切换，分叉注意力能够提供更好的延迟和效率。
- en: '5.'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Q: How does model quantization (or lower precision arithmetic) affect the findings?'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问：模型量化（或较低精度运算）如何影响这些发现？
- en: 'A: There are two regimes for quantization: model weight quantization and attention
    quantization. To date, most quantization only focuses on the weight since the
    attention computation is precision-sensitive and quantization has not proved to
    be viable.'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：量化有两种模式：模型权重量化和注意力量化。迄今为止，大多数量化仅关注权重，因为注意力计算对精度敏感，量化尚未被证明可行。
- en: Model quantization can make incremental decoding faster due to lower memory
    I/O of the model itself, since the effective model size in memory is smaller.
    This shifts the latency curve downward for all context lengths or batch sizes.
    The overall conclusion for the bifurcated and multi-query attention remains the
    same, however, since the improvement proposed in the paper is on the attention
    component, which is orthogonal to the model weight.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型量化可以使增量解码更快，因为模型本身的内存I/O更低，模型在内存中的有效大小更小。这会使所有上下文长度或批次大小的延迟曲线向下移动。然而，分叉和多查询注意力的整体结论保持不变，因为论文中提出的改进是针对注意力组件的，这与模型权重是正交的。
- en: If attention quantization is viable in the future, the lower memory on the attention
    tensor will effectively reduce the memory I/O for KV cache by a factor of 2 in
    the case of `int8` quantization (compared to `fp16` or `bf16`) or a factor of
    4 in the case of `int4`. Overall, this will flatten the latency growth with respect
    to batch size or context length. The overall comparative complexity (a) with or
    without bifurcated attention or (b) multi-head vs. multi-query remains the same.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未来注意力量化可行，注意力张量的较低内存将有效减少KV缓存的内存I/O，在`int8`量化的情况下减少2倍，在`int4`的情况下减少4倍。总体来说，这将使得延迟增长在批次大小或上下文长度方面趋于平稳。分叉注意力的整体比较复杂度（a）有无分叉注意力或（b）多头与多查询之间保持不变。
- en: '6.'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Q: Does the conclusion depend on the inference implementation or different
    hardware?'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问：结论是否依赖于推理实现或不同硬件？
- en: 'A: Different inference platforms, such as FasterTransformers (GPUs) or PaLM
    inference (TPUs), can yield different latency numbers. However, the relative I/O
    complexity among different attention mechanisms does not change, resulting in
    similar relative trends among different attention mechanisms. That being said,
    it is possible that more efficient implementations or more performant chip/system
    configurations, including different tensor parallelism degrees, can result in
    different slopes for the latency growth with respect to context length and batch
    size. In that case, the trade-off points in terms of context length or batch size
    can be different. The comparative complexity remains the same based on the analysis.'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：不同的推理平台，如 FasterTransformers（GPUs）或 PaLM 推理（TPUs），可能会产生不同的延迟数字。然而，不同注意力机制之间的相对
    I/O 复杂性不会改变，因此不同注意力机制之间的相对趋势相似。也就是说，更高效的实现或更高性能的芯片/系统配置，包括不同的张量并行度，可能会导致在上下文长度和批量大小方面延迟增长的斜率不同。在这种情况下，基于上下文长度或批量大小的权衡点可能会有所不同。基于分析的比较复杂性保持不变。
- en: '7.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Q: How does bifurcated attention differ from using attention mask for sampling
    as in done in SpecInfer (Miao et al., [2023](#bib.bib45)) ?'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问：双分支注意力与 SpecInfer（Miao et al., [2023](#bib.bib45)）中用于采样的注意力掩码有何不同？
- en: 'A: The attention mask approach can have a different FLOP usage compared to
    the original attention. We can consider a scenario where the attention mask corresponds
    to sampling with batch $b$ and incremental decoding length $\ell$, with the original
    context of length $m$. The attention FLOPs are $O(mb\ell+b^{2}\ell^{2})$. In contrast,
    the original FLOPs is $O(mb\ell)$. If $b\ell$ is sufficiently large, then the
    FLOPs via attention mask can be much higher. However, for the purpose of speculative
    decoding where the number of draft tokens is small, this additional FLOPs can
    be negligible.'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答：注意力掩码方法的 FLOP 使用情况可能与原始注意力不同。我们可以考虑一个场景，其中注意力掩码对应于批量 $b$ 和增量解码长度 $\ell$，原始上下文长度为
    $m$。注意力 FLOPs 为 $O(mb\ell+b^{2}\ell^{2})$。相比之下，原始 FLOPs 为 $O(mb\ell)$。如果 $b\ell$
    足够大，则通过注意力掩码的 FLOPs 可能会高得多。然而，对于草拟解码数量较少的推测解码目的，这些额外的 FLOPs 可以忽略不计。
- en: Appendix B Related Work
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 相关工作
- en: B.1 Applications of Single-Context Batch Sampling
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 单上下文批量采样的应用
- en: 'The observed latency reduction we achieve can have a profound impact on many
    applications. Some of these applications include:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到的延迟减少可以对许多应用产生深远的影响。其中一些应用包括：
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Code Generation: In software development, AI-assisted code generation can benefit
    greatly from reduced latency, especially when generating multiple code snippets
    or suggestions for a given context. This can lead to a more responsive and efficient
    user experience for developers using AI-powered Integrated Development Environments
    (IDEs) or code completion tools (Nijkamp et al., [2023](#bib.bib51), [2022](#bib.bib50);
    Chen et al., [2021](#bib.bib12); Le et al., [2022](#bib.bib36); Fried et al.,
    [2022](#bib.bib22); Li et al., [2022](#bib.bib39); Allal et al., [2023](#bib.bib3);
    Li et al., [2023](#bib.bib38); Ahmad et al., [2021](#bib.bib1)).'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码生成：在软件开发中，AI 辅助代码生成可以从减少延迟中受益，特别是在为给定上下文生成多个代码片段或建议时。这可以为使用 AI 驱动的集成开发环境（IDEs）或代码补全工具的开发人员提供更具响应性和效率的用户体验（Nijkamp
    et al., [2023](#bib.bib51), [2022](#bib.bib50); Chen et al., [2021](#bib.bib12);
    Le et al., [2022](#bib.bib36); Fried et al., [2022](#bib.bib22); Li et al., [2022](#bib.bib39);
    Allal et al., [2023](#bib.bib3); Li et al., [2023](#bib.bib38); Ahmad et al.,
    [2021](#bib.bib1)）。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Machine Translation: In situations where multiple translations are needed for
    a single input, such as generating translations with varying degrees of formality
    or generating translations for different dialects, the context-aware bifurcated
    attention can provide more efficient computation, resulting in faster and more
    scalable machine translation services (Costa-jussà et al., [2022](#bib.bib16);
    Farhad et al., [2021](#bib.bib20); Tran et al., [2021](#bib.bib64); Yee et al.,
    [2019](#bib.bib71)).'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器翻译：在需要对单一输入进行多次翻译的情况下，如生成不同程度正式或生成不同方言的翻译时，基于上下文的双分支注意力可以提供更高效的计算，从而实现更快速和更可扩展的机器翻译服务（Costa-jussà
    et al., [2022](#bib.bib16); Farhad et al., [2021](#bib.bib20); Tran et al., [2021](#bib.bib64);
    Yee et al., [2019](#bib.bib71)）。
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Chatbots and Conversational AI: Conversational agents often need to generate
    multiple responses to handle different interpretations of a user’s input or to
    provide multiple suggestions. The reduced latency offered by the proposed method
    can significantly improve the responsiveness of chatbots, leading to a more natural
    and fluid conversation with users (Google, [2023](#bib.bib25)).'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聊天机器人和对话 AI：对话代理通常需要生成多个响应，以处理用户输入的不同解释或提供多个建议。所提方法提供的延迟减少可以显著改善聊天机器人的响应速度，从而与用户进行更自然流畅的对话（Google,
    [2023](#bib.bib25)）。
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Creative Content Generation: In applications like poetry, story, or advertisement
    generation, the ability to generate multiple variations for a given prompt is
    crucial. The proposed method enables more efficient generation of diverse content,
    making it more feasible for real-time or large-scale applications (Lin and Riedl,
    [2021](#bib.bib41); Mirowski et al., [2023](#bib.bib47); Team, [2023](#bib.bib62);
    Yuan et al., [2022](#bib.bib72)).'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创意内容生成：在诗歌、故事或广告生成等应用中，为给定的提示生成多个变体的能力至关重要。所提方法能够更高效地生成多样化的内容，使其在实时或大规模应用中更具可行性（Lin
    和 Riedl, [2021](#bib.bib41)；Mirowski 等, [2023](#bib.bib47)；Team, [2023](#bib.bib62)；Yuan
    等, [2022](#bib.bib72)）。
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Augmentation: In the context of data augmentation for machine learning,
    generating multiple alternative examples for a given input can help improve model
    robustness and generalization. With the reduced latency provided by context-aware
    bifurcated attention, the process of generating augmented data can be made faster,
    enabling more efficient use of computational resources during training.'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据增强：在机器学习的数据增强背景下，为给定输入生成多个替代示例可以帮助提高模型的鲁棒性和泛化能力。通过上下文感知分叉注意力提供的延迟减少，生成增强数据的过程可以加快，从而在训练过程中更高效地使用计算资源。
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'General Large Scale Evaluation: In addition to the aforementioned use-cases
    there are many niche use-cases where LLM and other open-ended generation models
    are explored for toxicity (Dathathri et al., [2019](#bib.bib18); Gehman et al.,
    [2020](#bib.bib24); Nadeem et al., [2020](#bib.bib48)), detection of vulnerable
    code in generations (Pearce et al., [2022](#bib.bib55)), performance improving
    code edit generation (Madaan et al., [2023](#bib.bib43)), programming language
    translations (Roziere et al., [2020](#bib.bib58)) and many others. In all of these
    scenarios many generations per each prompt are gathered for a deeper understanding
    of the models, bifurcated attention can drastically speed up the generation process
    in such cases.'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一般大规模评估：除了前述的用例外，还有许多特定领域的用例，其中 LLM 和其他开放式生成模型被探索用于毒性检测（Dathathri 等, [2019](#bib.bib18)；Gehman
    等, [2020](#bib.bib24)；Nadeem 等, [2020](#bib.bib48)）、生成中的易受攻击代码检测（Pearce 等, [2022](#bib.bib55)）、性能提升的代码编辑生成（Madaan
    等, [2023](#bib.bib43)）、编程语言翻译（Roziere 等, [2020](#bib.bib58)）等。在所有这些场景中，为了更深入地理解模型，每个提示生成了许多实例，分叉注意力可以在这些情况下大幅加快生成过程。
- en: In conclusion, the proposed context-aware bifurcated attention method can significantly
    reduce memory I/O cost and improve latency in various applications, leading to
    increased efficiency and scalability. This method has the potential to enable
    new use cases and enhance the user experience in numerous AI-powered systems,
    making them more practical for real-world deployment.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，所提的上下文感知分叉注意力方法可以显著降低内存 I/O 成本并提高各种应用的延迟，进而提升效率和可扩展性。这种方法有潜力开启新的用例，并增强众多
    AI 驱动系统的用户体验，使其在实际部署中更具实用性。
- en: B.2 Supporting Long Context Requires IO-Efficient Attention
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 支持长上下文需要高效的 I/O 注意力
- en: As language models are becoming general purpose and highly capable, the demand
    for language models to handle longer context sequences has grown significantly.
    Recently, there is an ongoing focus on models that can handle even longer context
    sequences (Bulatov et al., [2023](#bib.bib9); OpenAI, [2023](#bib.bib53); Team,
    [2023](#bib.bib62)). As of today, GPT-4 (OpenAI, [2023](#bib.bib53)) supports
    context length of 32k tokens, and MPT-7B (Team, [2023](#bib.bib62)) extends it
    to 64k while Anthropic’s Claude ³³3https://www.anthropic.com/index/100k-context-windows
    supports as long as 100k input length. Most recently, Bulatov et al proposed 1M
    token input context length for transformers. These models push the boundaries
    of context understanding and generation capabilities, enabling more comprehensive
    discourse understanding and contextually informed responses.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语言模型变得越来越通用和强大，对处理更长上下文序列的需求显著增加。最近，越来越关注可以处理更长上下文序列的模型 (Bulatov 等人, [2023](#bib.bib9);
    OpenAI, [2023](#bib.bib53); Team, [2023](#bib.bib62))。截至目前，GPT-4 (OpenAI, [2023](#bib.bib53))
    支持 32k 代币的上下文长度，而 MPT-7B (Team, [2023](#bib.bib62)) 扩展到 64k，而 Anthropic 的 Claude
    ³³3https://www.anthropic.com/index/100k-context-windows 支持最长 100k 的输入长度。最近，Bulatov
    等人提出了针对变压器的 1M 代币输入上下文长度。这些模型推动了上下文理解和生成能力的边界，实现了更全面的话语理解和具有上下文信息的响应。
- en: This trend is driven by the need for comprehensive discourse understanding in
    applications like Retrieval-Augmented Generation (RAG), as well as many complex
    prompting methods. Applications such as RAG (Guu et al., [2020](#bib.bib26); Izacard
    et al., [2022](#bib.bib29); Menick et al., [2022](#bib.bib44); Zhen et al., [2022](#bib.bib74))
    retrieve extensive passages or documents from external corpora, providing rich
    and grounded context for generating responses. Additionally, models like Toolformer
    (Schick et al., [2023](#bib.bib59)) and WebGPT (Nakano et al., [2021](#bib.bib49))
    leverage external tools, such as APIs and search engines, to expand the context
    and enhance generation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这一趋势是由于在检索增强生成 (RAG) 等应用中对全面话语理解的需求，以及许多复杂的提示方法。应用程序如 RAG (Guu 等人, [2020](#bib.bib26);
    Izacard 等人, [2022](#bib.bib29); Menick 等人, [2022](#bib.bib44); Zhen 等人, [2022](#bib.bib74))
    从外部语料库中检索大量段落或文档，为生成响应提供丰富且有据可依的上下文。此外，像 Toolformer (Schick 等人, [2023](#bib.bib59))
    和 WebGPT (Nakano 等人, [2021](#bib.bib49)) 这样的模型利用外部工具，如 APIs 和搜索引擎，扩展上下文并增强生成能力。
- en: Long context is disproportionately expensive for transformer family models because
    for vanilla self-attention both memory and time complexity are quadratic to the
    sequence length. To effectively handle longer context sequences, optimizing memory
    I/O and reducing computational overhead are critical. Currently, the dominant
    approaches to addressing this challenge have been to make the attention computation
    less expensive. Beltagy et al. ([2020](#bib.bib7)) proposed to sparsify self-attention
    using various attention patterns. Wang et al. ([2020](#bib.bib66)) explores low-rank
    approximation of self-attention. In addition to the compute bound improvements,
    advancements in memory-efficient attention mechanisms and techniques for reducing
    memory I/O will continue to propel the field forward, facilitating the handling
    of longer context sequences in language models. FlashAttention (Dao et al., [2022](#bib.bib17))
    is proposed to speed up self-attention and reduce the memory footprint without
    any approximation. It leverages fused kernel for matrix multiplication and softmax
    operation which greatly reduces memory IO during training.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于变压器家族模型来说，长上下文的计算代价极高，因为对于原始自注意力机制，内存和时间复杂度与序列长度的平方成正比。为了有效处理更长的上下文序列，优化内存
    I/O 和减少计算开销至关重要。目前，解决这一挑战的主流方法是使注意力计算变得更为高效。Beltagy 等人 ([2020](#bib.bib7)) 提出了使用各种注意力模式来稀疏化自注意力。Wang
    等人 ([2020](#bib.bib66)) 探索了自注意力的低秩近似。除了计算限制的改进，内存高效的注意力机制以及减少内存 I/O 的技术也将持续推动该领域的发展，促进语言模型处理更长上下文序列的能力。FlashAttention
    (Dao 等人, [2022](#bib.bib17)) 提出了加速自注意力并减少内存占用的方案，无需任何近似。它利用了融合的矩阵乘法和 softmax 操作，大大减少了训练期间的内存
    I/O。
- en: Appendix C Setup
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 设置
- en: C.1 Model Training Details
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 模型训练细节
- en: We trained multiple models with varying sizes, ranging from 125 million parameters
    to 13 billion parameters, using code data with a context size of 2048 and adjusting
    the per-GPU batch size and total number of steps according to the model size.
    For model training we used multiple p4 instances each equipped with 8 40GB Nvidia
    A100 GPUs per instance.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了多个不同规模的模型，参数从1.25亿到130亿不等，使用上下文大小为2048的代码数据，并根据模型规模调整每GPU的批次大小和总步数。模型训练时，我们使用了多个
    p4 实例，每个实例配备了8块40GB的Nvidia A100 GPU。
- en: For our largest model family, the 13 billion parameter model, we used a global
    batch size of 1024, which approximately translates to 2 million tokens per batch.
    The settings for each model within each model-size family were kept consistent.
    The remaining training hyperparameters are summarized in the following table [1](#A3.T1
    "Table 1 ‣ C.1 Model Training Details ‣ Appendix C Setup ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling").
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们最大的模型系列——130亿参数模型，我们使用了1024的全局批次大小，这大约相当于每批次200万标记。每个模型规模系列内的每个模型设置保持一致。其余的训练超参数总结见下表[1](#A3.T1
    "Table 1 ‣ C.1 Model Training Details ‣ Appendix C Setup ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling")。
- en: We use AdamW optimizer (Kingma and Ba, [2014](#bib.bib33)) with $\beta_{1}=0.9$,
    $\beta_{2}=0.95$, and $\epsilon=10^{-8}$. The warm-up steps were set to 2000,
    and a cosine annealing learning rate schedule was employed after reaching the
    peak learning rate. The minimum learning rate was set to 10% of the corresponding
    peak learning rate. A weight decay (Loshchilov and Hutter, [2017](#bib.bib42))
    of 0.01 and gradient clipping of 1.0 were applied to enhance training stability.
    Following the approach in (Shoeybi et al., [2019](#bib.bib61)), the standard deviation
    for random weight initialization was rescaled for larger models. Our training
    pipeline is based on PyTorch Lightning and we use bfloat16 (Kalamkar et al., [2019](#bib.bib31))
    and DeepSpeed (Rasley et al., [2020](#bib.bib57)) for training optimization. Finally,
    a random split of 0.1% of the data was reserved as a validation set.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用AdamW优化器（Kingma和Ba，[2014](#bib.bib33)），设置 $\beta_{1}=0.9$，$\beta_{2}=0.95$，和
    $\epsilon=10^{-8}$。预热步骤设置为2000步，达到峰值学习率后采用了余弦退火学习率调度。最小学习率设置为相应峰值学习率的10%。应用了0.01的权重衰减（Loshchilov和Hutter，[2017](#bib.bib42)）和1.0的梯度裁剪以增强训练稳定性。按照(Shoeybi等，[2019](#bib.bib61))的方法，随机权重初始化的标准差针对较大的模型进行了重新缩放。我们的训练管道基于PyTorch
    Lightning，并使用bfloat16（Kalamkar等，[2019](#bib.bib31)）和DeepSpeed（Rasley等，[2020](#bib.bib57)）进行训练优化。最后，随机保留了0.1%的数据作为验证集。
- en: 'Table 1: Training Hyperparameters'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：训练超参数
- en: '| Model Size | Total Training Steps | Batch Size | Compute Nodes | Max Learning
    Rate |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 模型规模 | 总训练步骤 | 批次大小 | 计算节点 | 最大学习率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 125M | 400k | 256 | 8 | $2.5\times 10^{-4}$ |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 125M | 400k | 256 | 8 | $2.5\times 10^{-4}$ |'
- en: '| 672M | 200k | 256 | 8 | $2.5\times 10^{-4}$ |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 672M | 200k | 256 | 8 | $2.5\times 10^{-4}$ |'
- en: '| 2.8B | 200k | 512 | 16 | $1.6\times 10^{-4}$ |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 2.8B | 200k | 512 | 16 | $1.6\times 10^{-4}$ |'
- en: '| 13B | 100k | 1024 | 32 | $1.0\times 10^{-4}$ |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 13B | 100k | 1024 | 32 | $1.0\times 10^{-4}$ |'
- en: C.2 Model Configurations
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 模型配置
- en: For each model size we train three models with attention variations; multi head
    where $g=h$, multi group where $1<g<h$ and multi query where $g=1$. Additionally,
    for 672m and 2.8b models we train a multi group model variant where the fanout
    in feed forward layer is decreased from $4\times d$ to $2\times d$. Each model
    variant yields different number of total parameters therefore we group these models
    into family of model sizes. The detailed architectural choices for each of the
    model family is found in the table [2](#A3.T2 "Table 2 ‣ C.2 Model Configurations
    ‣ Appendix C Setup ‣ Bifurcated Attention for Single-Context Large-Batch Sampling").
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型规模，我们训练了三种不同的注意力变体的模型：多头注意力，其中 $g=h$，多组注意力，其中 $1<g<h$，以及多查询注意力，其中 $g=1$。此外，对于672M和2.8B模型，我们还训练了一个多组模型变体，其前馈层的fanout从
    $4\times d$ 减少到 $2\times d$。每个模型变体的参数总数不同，因此我们将这些模型分为不同的模型规模系列。每个模型系列的详细架构选择见表[2](#A3.T2
    "Table 2 ‣ C.2 Model Configurations ‣ Appendix C Setup ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling")。
- en: 'Table 2: Model Specifications table presenting architecture details for the
    three variants: multi head (MH), multi query (MQ), and multi group (MG) including
    parameter count, number of attention groups, head dimensions, and number of layers.
    The additional fanout-based MG variant is described here as MG + $2\times d$'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：模型规格表展示了三种变体的架构细节：多头（MH）、多查询（MQ）和多组（MG），包括参数数量、注意力组数量、头部维度和层数。附加的基于扇出（fanout）的
    MG 变体在这里描述为 MG + $2\times d$。
- en: '| Model Family | Attention Type | $groups$ | $d_{head}$ | $n_{layer}$ | $N_{params}$
    (billions) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 模型家族 | 注意力类型 | $groups$ | $d_{head}$ | $n_{layer}$ | $N_{params}$ (十亿) |'
- en: '| 125M | MH | 12 | 64 | 12 | 0.125 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 125M | MH | 12 | 64 | 12 | 0.125 |'
- en: '| MG | 4 | 64 | 12 | 0.115 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| MG | 4 | 64 | 12 | 0.115 |'
- en: '|  | MQ | 1 | 64 | 12 | 0.112 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | MQ | 1 | 64 | 12 | 0.112 |'
- en: '| 672M | MH | 20 | 72 | 24 | 0.672 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 672M | MH | 20 | 72 | 24 | 0.672 |'
- en: '| MG | 4 | 72 | 24 | 0.592 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| MG | 4 | 72 | 24 | 0.592 |'
- en: '|  | MG + $2\times d$ | 4 | 72 | 24 | 0.393 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | MG + $2\times d$ | 4 | 72 | 24 | 0.393 |'
- en: '|  | MQ | 1 | 72 | 24 | 0.578 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | MQ | 1 | 72 | 24 | 0.578 |'
- en: '| 2.8B | MH | 24 | 128 | 24 | 2.878 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 2.8B | MH | 24 | 128 | 24 | 2.878 |'
- en: '| MG | 4 | 128 | 24 | 2.501 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| MG | 4 | 128 | 24 | 2.501 |'
- en: '|  | MG + $2\times d$ | 4 | 128 | 24 | 1.595 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | MG + $2\times d$ | 4 | 128 | 24 | 1.595 |'
- en: '|  | MQ | 1 | 128 | 24 | 2.444 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | MQ | 1 | 128 | 24 | 2.444 |'
- en: '| 13B | MH | 40 | 128 | 40 | 12.852 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 13B | MH | 40 | 128 | 40 | 12.852 |'
- en: '| MG | 8 | 128 | 40 | 11.174 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| MG | 8 | 128 | 40 | 11.174 |'
- en: '|  | MQ | 1 | 128 | 40 | 10.807 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | MQ | 1 | 128 | 40 | 10.807 |'
- en: C.3 Model Details of 1B Latency Experiment
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 1B 延迟实验模型细节
- en: In Section [5.2.2](#S5.SS2.SSS2 "5.2.2 Single-Context Batch Sampling ‣ 5.2 Latencies
    of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling"), we use candidate models of sizes roughly 1B to study the
    effect of bifurcated attention. We outline the hyperparameters of such models
    below.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [5.2.2](#S5.SS2.SSS2 "5.2.2 Single-Context Batch Sampling ‣ 5.2 Latencies
    of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling") 节中，我们使用了大约 1B 尺寸的候选模型来研究分叉注意力的效果。我们在下文中概述了这些模型的超参数。
- en: 'Table 3: Model Specifications for Latency Experiment in Section [5.2.2](#S5.SS2.SSS2
    "5.2.2 Single-Context Batch Sampling ‣ 5.2 Latencies of Capabilities-Equivalent
    Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling").'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：延迟实验的模型规格，见第 [5.2.2](#S5.SS2.SSS2 "5.2.2 Single-Context Batch Sampling ‣
    5.2 Latencies of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated Attention
    for Single-Context Large-Batch Sampling") 节。
- en: '| Model Family | Attention Type | $groups$ | $d_{head}$ | $n_{layer}$ | $N_{params}$
    (billions) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 模型家族 | 注意力类型 | $groups$ | $d_{head}$ | $n_{layer}$ | $N_{params}$ (十亿) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1B | MH | 20 | 128 | 12 | 1.077 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 1B | MH | 20 | 128 | 12 | 1.077 |'
- en: '| MG | 4 | 128 | 15 | 1.156 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| MG | 4 | 128 | 15 | 1.156 |'
- en: '|  | MQ | 1 | 128 | 16 | 1.193 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | MQ | 1 | 128 | 16 | 1.193 |'
- en: 'C.4 Ablation Studies: $2d$ Intermediate Feature Dimension'
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 消融研究：$2d$ 中间特征维度
- en: 'One can also argue that different $g$ results in different balance of the number
    of parameters in the feedforward versus the attention components. We performed
    an ablation study where we reduce the typical intermediate feature size of $4d$
    to $2d$ and train models for three model sizes (which we will refer to as the
    $2d$ experiment). The ablation study reveals that the scaling laws curves for
    the $2d$ experiment crosses the usual $4d$ curves, which implies that the reduced
    size of the attention component alone compared to feedforward does not provide
    a consistent explanation of model capabilities. This can be seen from Figure [9](#A3.F9
    "Figure 9 ‣ C.4 Ablation Studies: 2⁢𝑑 Intermediate Feature Dimension ‣ Appendix
    C Setup ‣ Bifurcated Attention for Single-Context Large-Batch Sampling").'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '也可以争辩说，不同的 $g$ 导致前馈与注意力组件参数数量的不同平衡。我们进行了一个消融研究，将典型的中间特征大小从 $4d$ 减少到 $2d$，并训练了三种模型尺寸（我们称之为
    $2d$ 实验）。消融研究揭示，$2d$ 实验的缩放规律曲线穿过了通常的 $4d$ 曲线，这意味着仅与前馈相比，注意力组件的尺寸减少并未提供对模型能力的一致解释。这可以从图
    [9](#A3.F9 "Figure 9 ‣ C.4 Ablation Studies: 2⁢𝑑 Intermediate Feature Dimension
    ‣ Appendix C Setup ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")
    中看到。'
- en: '![Refer to caption](img/29f59b9bc673c9d698d120b05bd1aa5b.png)![Refer to caption](img/65e12f5ad7b4456c29292faa4b244512.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/29f59b9bc673c9d698d120b05bd1aa5b.png)![参见说明](img/65e12f5ad7b4456c29292faa4b244512.png)'
- en: 'Figure 9: Capabilities versus size plots including the $2d$-intermediate-size
    feedforward model. The plot shows that the balance between the number of feedforward
    parameters and the attention parameters alone does not explain the relative expressiveness
    of multi-head, multi-group, and multi-query attentions. Rather, we argue that
    what explains relative expressiveness is the representation power associated with
    the key and value tensors (Section [5.1](#S5.SS1 "5.1 Comparing Capabilities of
    Multi-Head, Multi-Query, and Multi-Group Attention ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：能力与大小的关系图，包括 $2d$-中间大小的前馈模型。图中显示，前馈参数数量和注意力参数之间的平衡并不能解释多头、多组和多查询注意力的相对表达能力。相反，我们认为解释相对表达能力的是与键和值张量相关的表示能力（见第
    [5.1](#S5.SS1 "5.1 Comparing Capabilities of Multi-Head, Multi-Query, and Multi-Group
    Attention ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling") 节）。
- en: C.5 Inference Setup
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5 推理设置
- en: We use Nvidia A100 GPUs for inference hardware (Choquette et al., [2021](#bib.bib14)).
    We perform latency studies using Deepspeed inference (Rasley et al., [2020](#bib.bib57))
    on top of Huggingface transformers (Wolf et al., [2019](#bib.bib68)), where we
    wrote custom code to handle the generalize multi-group attention as well as bifurcated
    attention. Future work includes extending the implementation to FasterTransformer
    ([NVIDIA,](#bib.bib52) ).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Nvidia A100 GPU 作为推理硬件 (Choquette 等人，[2021](#bib.bib14))。我们使用 Deepspeed
    推理 (Rasley 等人，[2020](#bib.bib57)) 进行延迟研究，基于 Huggingface transformers (Wolf 等人，[2019](#bib.bib68))，在此基础上我们编写了自定义代码来处理通用的多组注意力以及分叉注意力。未来的工作包括将实现扩展到
    FasterTransformer ([NVIDIA,](#bib.bib52))。
- en: Appendix D Multi-Group Attention Family
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 多组注意力家族
- en: D.1 Detailed Analysis on Memory Access
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 内存访问的详细分析
- en: We show in Table [4](#A4.T4 "Table 4 ‣ D.1 Detailed Analysis on Memory Access
    ‣ Appendix D Multi-Group Attention Family ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling") that the memory IO cost for $\langle q,K\rangle$ is dominated
    by the loading of $K$ which costs $bmhk$ in the case of multi-head where $g=h$.
    This cost is particularly high due to the coupling of batch size $b$, context
    length $m$, and the entire hidden dimension $d$. Compared to the number of computations,
    which has complexity $bmd$, this attention module requires one memory IO per one
    tensor operation (memory-io bound). In contrast, other operations such as feedforward
    has much lower ratio of memory IO per compute (compute bound). These attention
    computation can be the main bottleneck for incremental decoding and our paper
    aims to tackle such problems.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 [4](#A4.T4 "Table 4 ‣ D.1 Detailed Analysis on Memory Access ‣ Appendix
    D Multi-Group Attention Family ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling") 中显示，$\langle q,K\rangle$ 的内存 IO 成本由 $K$ 的加载主导，在多头情况下 $g=h$ 时成本为 $bmhk$。由于批量大小
    $b$、上下文长度 $m$ 和整个隐藏维度 $d$ 的耦合，这个成本特别高。相比计算数量，其复杂度为 $bmd$，此注意力模块每进行一次张量操作需要一次内存
    IO（内存-IO 绑定）。相比之下，其他操作如前馈操作每计算的内存 IO 比率要低得多（计算绑定）。这些注意力计算可能是增量解码的主要瓶颈，我们的论文旨在解决这些问题。
- en: Concretely, we can see that the context encoding in single-batch scenario in
    Appendix [5.2.1](#S5.SS2.SSS1 "5.2.1 Single Context Scenario ‣ 5.2 Latencies of
    Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling") is $400$ ms for context length $10000$, implying that the
    amortized latency per token during this phase is $0.04$ ms per token. However,
    the per token latency during incremental decoding is in the order of $\approx
    10$ ms per token, which is $\frac{10}{0.04}=250$ times slower. This number clearly
    demonstrates that compute is not a dominating factor, but the memory IO required
    to load both model and KV cache.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们可以看到附录 [5.2.1](#S5.SS2.SSS1 "5.2.1 Single Context Scenario ‣ 5.2 Latencies
    of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling") 中单批次场景的上下文编码时间为 $400$ 毫秒，针对上下文长度为 $10000$，这意味着在这一阶段每个 token
    的平均延迟为 $0.04$ 毫秒。然而，增量解码过程中每个 token 的延迟大约为 $\approx 10$ 毫秒，即 $\frac{10}{0.04}=250$
    倍更慢。这个数字清楚地表明计算不是主要因素，而是加载模型和 KV 缓存所需的内存 IO。
- en: 'Table 4: Comparison of memory access and computation between Multi Head, Multi
    Query, and Multi Group attention mechanisms. The memory access is for incremental
    decoding with the query length $n=1$.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：多头、多查询和多组注意力机制之间的内存访问和计算比较。内存访问用于查询长度 $n=1$ 的增量解码。
- en: '| Operation | Einsum | Memory Access | Computation |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | Einsum | 内存访问 | 计算 |'
- en: '| Input ($x$): $bd$ |  |  |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 输入 ($x$): $bd$ |  |  |  |'
- en: '| $q=\langle x,P_{q}\rangle$ | $bd,hdk\rightarrow bhk$ | $bd+hdk=bd+d^{2}$
    | $bdhk=bd^{2}$ |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| $q=\langle x,P_{q}\rangle$ | $bd,hdk\rightarrow bhk$ | $bd+hdk=bd+d^{2}$
    | $bdhk=bd^{2}$ |'
- en: '| $K=\langle x,P_{k}\rangle\ (+K_{prev})$ | [MH] $bd,hdk\rightarrow bhk\ (+bmhk)$
    | $bd+d^{2}$ | $bdhk=bd^{2}$ |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| $K=\langle x,P_{k}\rangle\ (+K_{prev})$ | [MH] $bd,hdk\rightarrow bhk\ (+bmhk)$
    | $bd+d^{2}$ | $bdhk=bd^{2}$ |'
- en: '|  | [MQ] $bd,dk\rightarrow bk\ (+bmk)$ | $bd+dk$ |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | [MQ] $bd,dk\rightarrow bk\ (+bmk)$ | $bd+dk$ |  |'
- en: '|  | [MG] $bd,gdk\rightarrow bgk\ (+bgmk)$ | $bd+gdk$ |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | [MG] $bd,gdk\rightarrow bgk\ (+bgmk)$ | $bd+gdk$ |  |'
- en: '| $V=\langle x,P_{v}\rangle\ (+V_{prev})$ | [MH] $bd,hdv\rightarrow bhv\ (+bmhv)$
    | $bd+d^{2}$ | $bdhv=bd^{2}$ |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| $V=\langle x,P_{v}\rangle\ (+V_{prev})$ | [MH] $bd,hdv\rightarrow bhv\ (+bmhv)$
    | $bd+d^{2}$ | $bdhv=bd^{2}$ |'
- en: '|  | [MQ] $bd,dv\rightarrow bv\ (+bmv)$ | $bd+dv$ |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | [MQ] $bd,dv\rightarrow bv\ (+bmv)$ | $bd+dv$ |  |'
- en: '|  | [MG] $bd,gdv\rightarrow bgv\ (+bgmv)$ | $bd+gdv$ |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | [MG] $bd,gdv\rightarrow bgv\ (+bgmv)$ | $bd+gdv$ |  |'
- en: '| logits $=\langle q,K\rangle$ | [MH] $bhk,bhmk\rightarrow bhm$ | $bhk+bhmk=bd+bmd$
    | $bhmk=bmd$ |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| logits $=\langle q,K\rangle$ | [MH] $bhk,bhmk\rightarrow bhm$ | $bhk+bhmk=bd+bmd$
    | $bhmk=bmd$ |'
- en: '|  | [MQ] $bhk,bmk\rightarrow bhm$ | $bd+bmk+bhm$ |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | [MQ] $bhk,bmk\rightarrow bhm$ | $bd+bmk+bhm$ |  |'
- en: '|  | [MG] $bhk,bgmk\rightarrow bhm$ | $bhk+bgmk+bhm$ |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | [MG] $bhk,bgmk\rightarrow bhm$ | $bhk+bgmk+bhm$ |  |'
- en: '| weights: softmax |  | $bhm$ | $bhm$ |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 权重: softmax |  | $bhm$ | $bhm$ |'
- en: '| out($O$) $=\langle$ weights, $V\rangle$ | [MH] $bhm,bhmv\rightarrow bhv$
    | $bhm+bhmv=bhm+bmd$ | $bhmv=d$ |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| out($O$) $=\langle$ 权重, $V\rangle$ | [MH] $bhm,bhmv\rightarrow bhv$ | $bhm+bhmv=bhm+bmd$
    | $bhmv=d$ |'
- en: '|  | [MQ] $bhm,bmv\rightarrow bhv$ | $bhm+bmv+bhv$ |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | [MQ] $bhm,bmv\rightarrow bhv$ | $bhm+bmv+bhv$ |  |'
- en: '|  | [MG] $bhm,bgmv\rightarrow bhv$ | $bhm+bgmv+bhv$ |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | [MG] $bhm,bgmv\rightarrow bhv$ | $bhm+bgmv+bhv$ |  |'
- en: '| $y=\langle O,P_{O}\rangle$ | $bhv,hdv\rightarrow bd$ | $bd+d^{2}$ | $bdhv=bd^{2}$
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| $y=\langle O,P_{O}\rangle$ | $bhv,hdv\rightarrow bd$ | $bd+d^{2}$ | $bdhv=bd^{2}$
    |'
- en: '| Total: Multi Head |  | $bd+bmd+d^{2}$ | $bhm+bmd+bd^{2}\approx bd^{2}$ |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 总计: 多头 |  | $bd+bmd+d^{2}$ | $bhm+bmd+bd^{2}\approx bd^{2}$ |'
- en: '| Total: Multi Query |  | $bd+bmk+d^{2}$ |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 总计: 多查询 |  | $bd+bmk+d^{2}$ |  |'
- en: '| Total: Multi Group |  | $bd+bgmk+d^{2}$ |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 总计: 多组 |  | $bd+bgmk+d^{2}$ |  |'
- en: '| $r$: Multi Head |  | $1/d+m/d+1/b$ |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| $r$: 多头 |  | $1/d+m/d+1/b$ |  |'
- en: '| $r$: Multi Query |  | $1/d+m/(dh)+1/b$ |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| $r$: 多查询 |  | $1/d+m/(dh)+1/b$ |  |'
- en: '| $r$: Multi Group |  | $1/d+g/(dh)+1/b$ |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| $r$: 多组 |  | $1/d+g/(dh)+1/b$ |  |'
- en: D.2 Model FLOPs
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 模型 FLOPs
- en: The scaling laws by Kaplan et al. ([2020](#bib.bib32)) shows that the model-related
    FLOPs during the forward pass is $2N$ where $N$ is the number of parameters (without
    the embeddings). We show that it holds for a general multi-group model as well.
    The only difference between the multi-group and the multi-head case is the projection
    $P_{K}$ and $P_{V}$ where they are of size $dgk$ instead of $dhk$. Since this
    is a linear layer, the forward pass FLOPs for any input is still proportional
    such projection size. Therefore, it follows that for any multi-group attention,
    including multi-head, the forward FLOPs is $2N$ where $N$ is the respective number
    of parameters.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Kaplan 等人（[2020](#bib.bib32)）的扩展定律表明，前向传播过程中的模型相关 FLOPs 是 $2N$，其中 $N$ 是参数数量（不包括嵌入）。我们证明这对于一般的多组模型也是成立的。多组模型和多头模型的唯一区别在于投影
    $P_{K}$ 和 $P_{V}$，它们的大小是 $dgk$ 而不是 $dhk$。由于这是一个线性层，任何输入的前向传播 FLOPs 仍然与这种投影大小成正比。因此，对于任何多组注意力，包括多头，前向
    FLOPs 是 $2N$，其中 $N$ 是相应的参数数量。
- en: D.3 Comparing Capabilities-Equivalent Models
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 比较能力-等效模型
- en: This section outlines the analysis of latency change when we switch from an
    MH model to an MG model with $F$ times the size.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了当我们从 MH 模型切换到 $F$ 倍大小的 MG 模型时延迟变化的分析。
- en: D.3.1 Context encoding
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.3.1 上下文编码
- en: The dominating factor for latency in context encoding is the compute rather
    than the memory IO. The compute can be broken down into two parts (a) tensor projections
    related to model parameters and (b) KV attention involving no model parameters.
    For both parts, the large multi-group model will involve higher latency proportional
    to the size factor $F$. The context encoding time is $\propto N\times bm$ where
    $N$ is the model size except embeddings for (a) since the FLOPs per token is $2N$  (Kaplan
    et al., [2020](#bib.bib32)), which holds for all multi-group attention (Appendix
    [D.2](#A4.SS2 "D.2 Model FLOPs ‣ Appendix D Multi-Group Attention Family ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")). For (b), the encoding time
    is $\propto\ell\cdot bhm^{2}\propto Nbm^{2}$ for (b). Overall, the multi-group
    model with similar capabilities as the multi-head model will incur slightly higher
    context encoding time due to the larger size since $N$ to increase to $FN$.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文编码中，延迟的主要因素是计算而非内存 I/O。计算可以分解为两个部分：（a）与模型参数相关的张量投影和（b）不涉及模型参数的 KV 注意力。对于这两个部分，大型多组模型将涉及与大小因子
    $F$ 成比例的更高延迟。上下文编码时间是 $\propto N\times bm$，其中 $N$ 是模型大小（不包括嵌入），对于（a）而言，因为每个令牌的
    FLOP 为 $2N$  (Kaplan 等人，[2020](#bib.bib32))，这适用于所有多组注意力（附录 [D.2](#A4.SS2 "D.2
    Model FLOPs ‣ Appendix D Multi-Group Attention Family ‣ Bifurcated Attention for
    Single-Context Large-Batch Sampling")）。对于（b），编码时间是 $\propto\ell\cdot bhm^{2}\propto
    Nbm^{2}$。总体而言，具有类似功能的多组模型相比于多头模型，由于尺寸较大，可能会导致稍微更高的上下文编码时间，因为 $N$ 会增加到 $FN$。
- en: D.3.2 Incremental Decoding
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.3.2 增量解码
- en: 'The incremental decoding component can dominate the overall inference latency
    compared to the context encoding, especially in the scenario where we decode in
    many steps. Incremental decoding is memory bound, meaning that the latency of
    this step is limited by memory I/O throughput. We can divide the memory I/O usage
    into two parts: reading (a) model parameters and (b) cached key-value pairs. With
    multi-group, we expect the model parameters to increase by a factor of $F(g)$,
    leading to an increase in I/O usage in (a) by the same factor. The memory IO in
    (b) changes by a factor of $\frac{g}{h}$ when moving from multi-head with KV cache
    size $2bhmk$ to multi-group with cache size $2bgmk$ (more precisely $\frac{g}{h}\cdot
    F(g)$ but $\frac{g}{h}$ is a dominating term since $F(g)$ is close to $1$).'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于上下文编码，增量解码组件在整体推理延迟中可能占据主导地位，特别是在我们分多步进行解码的情况下。增量解码是内存绑定的，这意味着此步骤的延迟受限于内存
    I/O 吞吐量。我们可以将内存 I/O 使用分为两个部分：（a）模型参数和（b）缓存的键值对。对于多组情况，我们预计模型参数将增加 $F(g)$ 倍，从而使（a）的
    I/O 使用也增加相同的倍数。在（b）中，内存 I/O 变化的倍数为 $\frac{g}{h}$，当从具有 KV 缓存大小 $2bhmk$ 的多头转移到具有缓存大小
    $2bgmk$ 的多组时（更准确地说是 $\frac{g}{h}\cdot F(g)$，但 $\frac{g}{h}$ 是主要项，因为 $F(g)$ 接近
    $1$）。
- en: Appendix E Context-Aware Bifurcated Attention
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 上下文感知分叉注意力
- en: E.1 Proof
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 证明
- en: Here, we outline the proof that the proposed bifurcated attention in Equation
    [3](#S4.E3 "Equation 3 ‣ 4.2 Formulation ‣ 4 Context-Aware Bifurcated Attention
    ‣ Bifurcated Attention for Single-Context Large-Batch Sampling") and [4](#S4.E4
    "Equation 4 ‣ 4.2 Formulation ‣ 4 Context-Aware Bifurcated Attention ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling") recovers the same attention
    as the operations in [1](#S3.E1 "Equation 1 ‣ 3.3 Multi-Query, Multi-Head and
    the Generalized Multi-Query Attention ‣ 3 Background ‣ Bifurcated Attention for
    Single-Context Large-Batch Sampling") and [2](#S3.E2 "Equation 2 ‣ 3.3 Multi-Query,
    Multi-Head and the Generalized Multi-Query Attention ‣ 3 Background ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling") for the case of single-context
    batch sampling. We use the fact that the KV part corresponding to context length,
    all the batch indices correspond to the tensors.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们概述了证明所提出的分叉注意力在方程 [3](#S4.E3 "Equation 3 ‣ 4.2 Formulation ‣ 4 Context-Aware
    Bifurcated Attention ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")
    和 [4](#S4.E4 "Equation 4 ‣ 4.2 Formulation ‣ 4 Context-Aware Bifurcated Attention
    ‣ Bifurcated Attention for Single-Context Large-Batch Sampling") 中恢复了与 [1](#S3.E1
    "Equation 1 ‣ 3.3 Multi-Query, Multi-Head and the Generalized Multi-Query Attention
    ‣ 3 Background ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")
    和 [2](#S3.E2 "Equation 2 ‣ 3.3 Multi-Query, Multi-Head and the Generalized Multi-Query
    Attention ‣ 3 Background ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling") 中的操作相同的注意力，适用于单上下文批量采样的情况。我们利用了与上下文长度对应的 KV 部分，所有批次索引对应于张量的事实。
- en: '|  | $\displaystyle\langle q,K\rangle$ | $\displaystyle:\text{einsum}(bgpnk,bgmk)\to
    bgpnm$ |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle q,K\rangle$ | $\displaystyle:\text{einsum}(bgpnk,bgmk)\to
    bgpnm$ |  |'
- en: '|  |  | $\displaystyle=\text{einsum}(bgpnk,bg(m_{c}+m_{d})k)\to bgpnm$ |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\text{einsum}(bgpnk,bg(m_{c}+m_{d})k)\to bgpnm$ |  |'
- en: '|  |  | $\displaystyle=\text{einsum}(bgpnk,bgm_{c}k)\to bgpnm\oplus$ |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\text{einsum}(bgpnk,bgm_{c}k)\to bgpnm\oplus$ |  |'
- en: '|  |  | $\displaystyle\ \ \ \ \ \text{einsum}(bgpnk,bgm_{d}k)\to bgpnm$ |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\ \ \ \ \ \text{einsum}(bgpnk,bgm_{d}k)\to bgpnm$ |  |'
- en: '|  |  | $\displaystyle=\text{einsum}(bgpnk,bgm_{c}k)\to bgpnm\oplus$ |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\text{einsum}(bgpnk,bgm_{c}k)\to bgpnm\oplus$ |  |'
- en: '|  |  | $\displaystyle\ \ \ \ \ \text{einsum}(bgpnk,gm_{d}k)\to bgpnm$ |  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\ \ \ \ \ \text{einsum}(bgpnk,gm_{d}k)\to bgpnm$ |  |'
- en: '|  |  | $\displaystyle=\langle q,K_{c}\rangle\oplus\langle q,K_{d}\rangle$
    |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\langle q,K_{c}\rangle\oplus\langle q,K_{d}\rangle$
    |  |'
- en: '|  | $\displaystyle\langle w,V\rangle$ | $\displaystyle:\text{einsum}(bgpnm,bgmk)\to
    bgpnk=bnd$ |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle w,V\rangle$ | $\displaystyle:\text{einsum}(bgpnm,bgmk)\to
    bgpnk=bnd$ |  |'
- en: '|  |  | $\displaystyle=\text{einsum}(bgpnm_{c},bgm_{c}k)\to bgpnk+$ |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\text{einsum}(bgpnm_{c},bgm_{c}k)\to bgpnk+$ |  |'
- en: '|  |  | $\displaystyle\ \ \ \ \ \text{einsum}(bgpnm_{d},bgm_{d}k)\to bgpnk$
    |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\ \ \ \ \ \text{einsum}(bgpnm_{d},bgm_{d}k)\to bgpnk$
    |  |'
- en: '|  |  | $\displaystyle=\text{einsum}(bgpnm_{c},gm_{c}k)\to bgpnk+$ |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\text{einsum}(bgpnm_{c},gm_{c}k)\to bgpnk+$ |  |'
- en: '|  |  | $\displaystyle\ \ \ \ \ \text{einsum}(bgpnm_{d},bgm_{d}k)\to bgpnk$
    |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\ \ \ \ \ \text{einsum}(bgpnm_{d},bgm_{d}k)\to bgpnk$
    |  |'
- en: '|  |  | $\displaystyle=\langle w_{c},V_{c}\rangle+\langle w_{d},V_{d}\rangle$
    |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\langle w_{c},V_{c}\rangle+\langle w_{d},V_{d}\rangle$
    |  |'
- en: E.2 Detailed Memory I/O Analysis
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 详细内存 I/O 分析
- en: Overall, the memory I/O complexity changes from
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，内存 I/O 复杂度变化从
- en: •
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Original memory I/O cost: $bhnk+bgmk+bhnm$ (for $\langle q,K\rangle$) + $bhnm+bgmk+bnd$
    (for $\langle w,V\rangle$)'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始内存 I/O 成本：$bhnk+bgmk+bhnm$（用于 $\langle q,K\rangle$）+ $bhnm+bgmk+bnd$（用于 $\langle
    w,V\rangle$）
- en: •
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bifurcated attention memory I/O cost: $bhnk+gm_{c}k+bgm_{d}k+bhnm$ (for $\langle
    q,K\rangle$) + $bhnm+gm_{c}k+bgm_{d}k+bnd$ (for $\langle w,V\rangle$)'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双分支注意力内存 I/O 成本：$bhnk+gm_{c}k+bgm_{d}k+bhnm$（用于 $\langle q,K\rangle$）+ $bhnm+gm_{c}k+bgm_{d}k+bnd$（用于
    $\langle w,V\rangle$）
- en: There is an associated memory IO to write the $\langle w,V_{c}\rangle$ and $\langle
    w,V_{d}\rangle$ output twice. However, it is typically very small ($bnd$) compared
    to the IO of KV cache component $bgmk$ since $$m>.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个相关的内存 IO 用于将 $\langle w,V_{c}\rangle$ 和 $\langle w,V_{d}\rangle$ 输出两次。然而，相比于
    KV 缓存组件 $bgmk$ 的 IO，通常这个内存 IO 非常小（$bnd$），因为 $$m>。
- en: E.3 Implementation of Bifurcated Attention
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 双分支注意力的实现
- en: Despite the dramatic gain in inference efficiency of the bifurcated attention,
    we demonstrate the simplicity of our implementation involving  20 lines of code
    using Pytorch (Paszke et al., [2019](#bib.bib54)).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管双分支注意力在推理效率上取得了显著提升，我们展示了我们的实现的简单性，仅使用了 20 行代码，采用了 Pytorch（Paszke et al.,
    [2019](#bib.bib54)）。
- en: '1def  attn(query,  key,  value,  bifurcated_attention):2  #  3  if  bifurcated_attention  and  type(key)  ==  dict:4  #  g  =  number  of  groups5  #  h  =  gp  where  p  =  num  heads  per  group6  #  n  =  1  for  incremental  decoding7  attn_weights_context  =  torch.einsum(8  "bgpnk,gmk->bgpnm",  query,  key["context_past_key"][0]9  )10  attn_weights_incremental  =  torch.einsum(11  "bgpnk,bgmk->bgpnm",  query,  key["incremental_past_key"]12  )13  attn_weights  =  torch.cat(14  [attn_weights_context,  attn_weights_incremental],  dim=-115  )16  else:17  attn_weights  =  torch.einsum(18  "bgpnk,bgmk->bgpnm",  query,  key19  )20  #  softmax  and  causal  mask  (omitted)21  #  22  if  bifurcated_attention  and  type(value)  ==  dict:23  #  n  =  1  for  incremental  decoding24  context_past_value_length  =  value["context_past_value"].size(-2)25  attn_output_context  =  torch.einsum(26  "bgpnm,gmv->bgpnv",27  attn_weights[:,  :,  :,  :,  :context_past_value_length],28  value["context_past_value"][0],29  )30  attn_output_incremental  =  torch.einsum(31  "bgpnm,bgmv->bgpnv",32  attn_weights[:,  :,  :,  :,  context_past_value_length:],33  value["incremental_past_value"],34  )35  attn_output  =  attn_output_context  +  attn_output_incremental36  else:37  attn_output  =  torch.einsum(38  "bgpnm,bgmv->bgpnv",  attn_weights,  value39  )40  return  attn_output![Refer
    to caption](img/c45cbb47b2d5dee157e337b5988f2ff0.png)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '1def  attn(query,  key,  value,  bifurcated_attention):2  #  3  如果  bifurcated_attention  并且  type(key)  ==  dict:4  #  g  =  组数5  #  h  =  组  其中  p  =  每组
    头数6  #  n  =  1  用于  增量解码7  attn_weights_context  =  torch.einsum(8  "bgpnk,gmk->bgpnm",  query,  key["context_past_key"][0]9  )10  attn_weights_incremental  =  torch.einsum(11  "bgpnk,bgmk->bgpnm",  query,  key["incremental_past_key"]12  )13  attn_weights  =  torch.cat(14  [attn_weights_context,  attn_weights_incremental],  dim=-115  )16  否则:17  attn_weights  =  torch.einsum(18  "bgpnk,bgmk->bgpnm",  query,  key19  )20  #  softmax  和  其他掩码  (省略)21  #  22  如果  bifurcated_attention  并且  type(value)  ==  dict:23  #  n  =  1  用于  增量解码24  context_past_value_length  =  value["context_past_value"].size(-2)25  attn_output_context  =  torch.einsum(26  "bgpnm,gmv->bgpnv",27  attn_weights[:,  :,  :,  :,  :context_past_value_length],28  value["context_past_value"][0],29  )30  attn_output_incremental  =  torch.einsum(31  "bgpnm,bgmv->bgpnv",32  attn_weights[:,  :,  :,  :,  context_past_value_length:],33  value["incremental_past_value"],34  )35  attn_output  =  attn_output_context  +  attn_output_incremental36  否则:37  attn_output  =  torch.einsum(38  "bgpnm,bgmv->bgpnv",  attn_weights,  value39  )40  返回  attn_output![参见标题](img/c45cbb47b2d5dee157e337b5988f2ff0.png)'
- en: (a) MBXP Java
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MBXP Java
- en: '![Refer to caption](img/e5523c19a8b7421e1f72048e33f07cc7.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e5523c19a8b7421e1f72048e33f07cc7.png)'
- en: (b) MBXP JavaScript
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MBXP JavaScript
- en: 'Figure 10: Bifurcated attention enables high batch sampling with minimal increase
    in latency with respect to batch size, resulting in more diverse and improved
    quality samples as indicated by pass@n and pass@top3 on MBXP-Java (Figure [10(a)](#A5.F10.sf1
    "Figure 10(a) ‣ Figure 10 ‣ E.3 Implementation of Bifurcated Attention ‣ Appendix
    E Context-Aware Bifurcated Attention ‣ Bifurcated Attention for Single-Context
    Large-Batch Sampling")). This trend extends to other evaluation such as JavaScript
    (Figure [10(b)](#A5.F10.sf2 "Figure 10(b) ‣ Figure 10 ‣ E.3 Implementation of
    Bifurcated Attention ‣ Appendix E Context-Aware Bifurcated Attention ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")) and Python (Figure [8](#S5.F8
    "Figure 8 ‣ Multi-head benefits significantly from bifurcated attention ‣ 5.2.2
    Single-Context Batch Sampling ‣ 5.2 Latencies of Capabilities-Equivalent Models
    ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")).'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：分叉注意力使得批量采样在批量大小方面的延迟增加最小，从而产生更为多样化和改进的质量样本，这一点通过 MBXP-Java 上的 pass@n 和
    pass@top3 来指示（图 [10(a)](#A5.F10.sf1 "图 10(a) ‣ 图 10 ‣ E.3 分叉注意力的实现 ‣ 附录 E 上下文感知分叉注意力
    ‣ 单上下文大批量采样的分叉注意力")）。这种趋势也延续到其他评估中，如 JavaScript（图 [10(b)](#A5.F10.sf2 "图 10(b)
    ‣ 图 10 ‣ E.3 分叉注意力的实现 ‣ 附录 E 上下文感知分叉注意力 ‣ 单上下文大批量采样的分叉注意力")）和 Python（图 [8](#S5.F8
    "图 8 ‣ 多头显著受益于分叉注意力 ‣ 5.2.2 单上下文批量采样 ‣ 5.2 相当能力模型的延迟 ‣ 5 实验 ‣ 单上下文大批量采样的分叉注意力")）。
- en: 'Appendix F Applications: Additional Results'
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 应用：额外结果
- en: We demonstrate additional results to the evaluation in Section [5.3](#S5.SS3
    "5.3 Applications ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch
    Sampling") on MBXP-Java and MBXP-Javascript, in addition to the Python results.
    We replace CodeGen-16B-mono with CodeGen-16B-multi for the evaluation on Java
    and JavaScript and use the same StarCoder model. From Figure [10](#A5.F10 "Figure
    10 ‣ E.3 Implementation of Bifurcated Attention ‣ Appendix E Context-Aware Bifurcated
    Attention ‣ Bifurcated Attention for Single-Context Large-Batch Sampling"), we
    observe similar trends as in Python (Figure [8](#S5.F8 "Figure 8 ‣ Multi-head
    benefits significantly from bifurcated attention ‣ 5.2.2 Single-Context Batch
    Sampling ‣ 5.2 Latencies of Capabilities-Equivalent Models ‣ 5 Experiments ‣ Bifurcated
    Attention for Single-Context Large-Batch Sampling")), which furthers demonstrates
    the wide applicability of of bifurcated attention in improving accuracy under
    latency-constrained scenarios.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了除了Python结果外，还包括MBXP-Java和MBXP-Javascript的评估附加结果，参考[5.3](#S5.SS3 "5.3 Applications
    ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")节。我们将CodeGen-16B-mono替换为CodeGen-16B-multi，以评估Java和JavaScript，并使用相同的StarCoder模型。从图[10](#A5.F10
    "Figure 10 ‣ E.3 Implementation of Bifurcated Attention ‣ Appendix E Context-Aware
    Bifurcated Attention ‣ Bifurcated Attention for Single-Context Large-Batch Sampling")中，我们观察到与Python中的趋势类似(图[8](#S5.F8
    "Figure 8 ‣ Multi-head benefits significantly from bifurcated attention ‣ 5.2.2
    Single-Context Batch Sampling ‣ 5.2 Latencies of Capabilities-Equivalent Models
    ‣ 5 Experiments ‣ Bifurcated Attention for Single-Context Large-Batch Sampling"))，进一步证明了双重注意力在提高延迟受限场景下的准确性方面的广泛适用性。
- en: Appendix G Compatibility with Speculative Decoding and Fast Decoding techniques
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 与投机解码和快速解码技术的兼容性
- en: Unlike standard auto-regressive decoding, fast decoding techniques such as Speculative
    decoding(Chen et al., [2023](#bib.bib11); Leviathan et al., [2022](#bib.bib37)),
    Medusa (Cai et al., [2024](#bib.bib10)), Lookahead (Fu et al., [2023](#bib.bib23)),
    and Eagle (Li et al., [2024](#bib.bib40)) attempt to decode multiple tokens at
    each step. This reduces I/O bandwidth requirements because model parameters and
    KV cache are fetched only once per step and can be amortized across all generated
    tokens.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准的自回归解码不同，快速解码技术如**投机解码**(Chen et al., [2023](#bib.bib11); Leviathan et al.,
    [2022](#bib.bib37))、**美杜莎**(Cai et al., [2024](#bib.bib10))、**前瞻解码**(Fu et al.,
    [2023](#bib.bib23))和**鹰眼解码**(Li et al., [2024](#bib.bib40))尝试在每一步解码多个标记。这降低了I/O带宽需求，因为模型参数和KV缓存每步仅需提取一次，并且可以在所有生成的标记上分摊。
- en: The fundamental principle behind these techniques is to first draft (or guess)
    a set of tokens (denoted as $n_{g}$) and then validate their accuracy by parallelly
    decoding with the model. After each step, up to a tokens (where $a\leq n_{g}$)
    may be accepted as valid, allowing for memory usage amortization across these
    accepted tokens. This approach is successful because decoding is primarily constrained
    by memory I/O.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术的基本原理是首先草拟（或猜测）一组标记（记作$n_{g}$），然后通过并行解码模型来验证它们的准确性。在每一步之后，最多有a个标记（其中$a\leq
    n_{g}$）可能被接受为有效，从而允许在这些接受的标记上分摊内存使用。这种方法成功的原因在于解码主要受内存I/O的限制。
- en: The benefits of bifurcated attention are orthogonal to those of speculative
    sampling, leading to further memory I/O improvements. This can be observed by
    extrapolating per-step memory I/O costs from Section [E.2](#A5.SS2 $$ continues
    to hold, the advantages of bifurcated attention persist even when combined with
    speculative decoding.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 双重注意力的好处与投机采样的好处是正交的，从而带来了进一步的内存I/O改进。这可以通过从[ E.2](#A5.SS2 $$继续保持的每步内存I/O成本来观察，即使与投机解码结合使用，双重注意力的优势仍然存在。
