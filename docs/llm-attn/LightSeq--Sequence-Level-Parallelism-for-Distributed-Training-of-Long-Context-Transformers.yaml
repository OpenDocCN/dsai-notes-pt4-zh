- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:03:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:03:12'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LightSeq: 用于长上下文变换器的分布式训练的序列级并行'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.03294](https://ar5iv.labs.arxiv.org/html/2310.03294)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.03294](https://ar5iv.labs.arxiv.org/html/2310.03294)
- en: 'Dacheng Li   ²²footnotemark: 2 &Rulin Shao ¹¹footnotemark: 1  ³³footnotemark:
    3 &Anze Xie ⁴⁴footnotemark: 4 &Eric P. Xing ⁸⁸footnotemark: 8 \ANDJoseph E. Gonzalez ²²footnotemark:
    2 &Ion Stoica ²²footnotemark: 2 &Xuezhe Ma ⁷⁷footnotemark: 7 &Hao Zhang ⁴⁴footnotemark:
    4 &'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dacheng Li   ²²脚注标记: 2 &Rulin Shao ¹¹脚注标记: 1  ³³脚注标记: 3 &Anze Xie ⁴⁴脚注标记: 4
    &Eric P. Xing ⁸⁸脚注标记: 8 \ANDJoseph E. Gonzalez ²²脚注标记: 2 &Ion Stoica ²²脚注标记: 2
    &Xuezhe Ma ⁷⁷脚注标记: 7 &Hao Zhang ⁴⁴脚注标记: 4 &'
- en: ^b UC Berkeley     ^w University of Washington     ^s UCSD     ^c CMU     ^m
    MBZUAI     ^u USC Authors contributed equally.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^b UC Berkeley     ^w 华盛顿大学     ^s UCSD     ^c 卡内基梅隆大学     ^m MBZUAI     ^u
    南加州大学 作者贡献相等。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Increasing the context length of large language models (LLMs) unlocks fundamentally
    new capabilities, but also significantly increases the memory footprints of training.
    Previous model-parallel systems such as Megatron-LM partition and compute different
    attention heads in parallel, resulting in large communication volumes, so they
    cannot scale beyond the number of attention heads, thereby hindering its adoption.
    In this paper, we introduce a new approach, LightSeq, for long-context LLMs training.
    LightSeq has many notable advantages. First, LightSeq partitions over the sequence
    dimension, hence is agnostic to model architectures and readily applicable for
    models with varying numbers of attention heads, such as Multi-Head, Multi-Query
    and Grouped-Query attention. Second, LightSeq not only requires up to 4.7$\times$
    end-to-end speedup, and a 2-8$\times$ longer sequence length on models with fewer
    heads, compared to Megatron-LM. Codes will be available at [https://github.com/RulinShao/LightSeq](https://github.com/RulinShao/LightSeq).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 增加大语言模型（LLMs）的上下文长度可以解锁根本的新能力，但也显著增加了训练的内存占用。以往的模型并行系统，如 Megatron-LM，通过并行处理不同的注意力头进行分区计算，导致了大量的通信开销，因此无法超越注意力头的数量，从而限制了其应用。在本文中，我们介绍了一种新的方法——LightSeq，用于长上下文
    LLMs 的训练。LightSeq 具有许多显著优势。首先，LightSeq 在序列维度上进行分区，因此对模型架构不敏感，能够适用于具有不同数量注意力头的模型，如多头、多查询和分组查询注意力。其次，LightSeq
    不仅需要比 Megatron-LM 高达 4.7$\times$ 的端到端加速，而且在注意力头较少的模型上，序列长度增加了 2-8$\times$。代码将会在
    [https://github.com/RulinShao/LightSeq](https://github.com/RulinShao/LightSeq)
    上发布。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transformers with long-context capabilities have enabled fundamentally new applications,
    such as comprehensive document understanding, generating a complete codebase,
    and extended interactive chatting (Osika, [2023](#bib.bib18); Liu et al., [2023](#bib.bib15);
    Li et al., [2023](#bib.bib11)). However, training LLMs with long sequences induces
    large activation memory footprints, posing new challenges to existing distributed
    systems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 具有长上下文能力的变换器已启用根本性的新应用，如全面的文档理解、生成完整的代码库和扩展的互动聊天 (Osika, [2023](#bib.bib18);
    Liu et al., [2023](#bib.bib15); Li et al., [2023](#bib.bib11))。然而，用长序列训练 LLMs
    会引发较大的激活内存占用，给现有的分布式系统带来了新的挑战。
- en: One effective method for reducing these large activation memory footprints is
    to partition the activation across devices. To achieve this, existing systems
    like Megatron-LM (Korthikanti et al., [2023](#bib.bib9); Shoeybi et al., [2019](#bib.bib21))
    usually partition the attention heads. However, this design poses a strong assumption
    that the number of attention heads must be divisible by the parallelism degree,
    which does not hold for many model architectures. For example, Llama-33B has 52
    attention heads, which is not divisible by commonly chosen parallelism degrees
    such as 8, 16, and 32, according to the topology of NVIDIA clusters. In addition,
    partitioning attention heads restricts the maximum parallelism degree to be no
    greater than the number of attention heads. However, many popular LLMs do not
    have enough attention heads for it to scale up, e.g., CodeGen (Nijkamp et al.,
    [2022](#bib.bib17)) only has 16 attention heads. Moreover, many works have shown
    that the future Transformer architecture design may have even fewer attention
    heads. For example, Bian et al. ([2021](#bib.bib2)) demonstrates that Transformers
    with a single head outperforms its multi-head counterparts, representing a challenging
    scenario for solutions like Megatron-LM.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 减少这些大规模激活内存占用的一个有效方法是将激活分配到不同的设备上。为实现这一点，现有系统如 Megatron-LM (Korthikanti 等，[2023](#bib.bib9);
    Shoeybi 等，[2019](#bib.bib21)) 通常会对注意力头进行分区。然而，这种设计提出了一个强假设，即注意力头的数量必须能够被并行度整除，这对许多模型架构来说并不成立。例如，Llama-33B
    具有 52 个注意力头，根据 NVIDIA 集群的拓扑结构，这个数量不能被常见的并行度（如 8、16 和 32）整除。此外，分区注意力头将最大并行度限制为不超过注意力头的数量。然而，许多流行的
    LLM 并没有足够的注意力头来进行扩展，例如，CodeGen (Nijkamp 等，[2022](#bib.bib17)) 仅有 16 个注意力头。此外，许多研究表明，未来的
    Transformer 架构设计可能会有更少的注意力头。例如，Bian 等 ([2021](#bib.bib2)) 证明了具有单个头的 Transformer
    优于其多头对应物，这对像 Megatron-LM 这样的解决方案构成了挑战。
- en: 'To scale beyond the number of heads, we propose partitioning solely the input
    tokens (i.e., sequence parallelism) rather than the attention heads. We present
    a solution that is agnostic to the model architecture and exhibits a maximal parallelism
    degree that scales with the sequence length. Specifically, we introduce a parallelizable
    and memory-efficient exact attention mechanism, DistAttn, in (§[3.1](#S3.SS1 "3.1
    DistAttn: distributed memory-efficient attention ‣ 3 Method ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")). Our
    design enables opportunities for overlapping, where we can hide communication
    into attention computation(§ [3.2](#S3.SS2 "3.2 Load balanced scheduling with
    communication and computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")). We also propose a load-balancing
    technique to avoid the computation bubble caused by the unbalanced workload in
    causal language modeling (§[3.2](#S3.SS2 "3.2 Load balanced scheduling with communication
    and computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")). While extending the FlashAttention (Dao,
    [2023](#bib.bib5)) algorithm to DistAttn, we found a way to leverage the underlying
    rematerialization logic to si gnificantly improve the speed of gradient checkpointing
    training (§ [3.3](#S3.SS3 "3.3 Rematerialization-aware checkpointing strategy
    ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed Training of
    Long Context Transformers")). This technique also applies to non-distributed usage
    of memory-efficient attention, and in our experiments translates to an additional
    1.31$\times$ speedup (§ [4.3](#S4.SS3 "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq:
    Sequence Level Parallelism for Distributed Training of Long Context Transformers")).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '为了超越头数的限制，我们建议仅对输入标记（即序列并行性）进行分区，而不是对注意力头进行分区。我们提出了一种与模型架构无关的解决方案，并展示了一个最大并行度随着序列长度扩展的机制。具体来说，我们在（§[3.1](#S3.SS1
    "3.1 DistAttn: distributed memory-efficient attention ‣ 3 Method ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")）中介绍了一种可并行化且内存高效的精确注意力机制，DistAttn。我们的设计提供了重叠的机会，我们可以将通信隐藏在注意力计算中（§
    [3.2](#S3.SS2 "3.2 Load balanced scheduling with communication and computation
    overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers")）。我们还提出了一种负载平衡技术，以避免因因果语言建模中的工作负载不均而导致的计算泡沫（§ [3.2](#S3.SS2
    "3.2 Load balanced scheduling with communication and computation overlap ‣ 3 Method
    ‣ LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers")）。在将 FlashAttention（Dao, [2023](#bib.bib5)）算法扩展到 DistAttn 时，我们找到了一种利用底层再物化逻辑的方法，以显著提高梯度检查点训练的速度（§
    [3.3](#S3.SS3 "3.3 Rematerialization-aware checkpointing strategy ‣ 3 Method ‣
    LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers")）。该技术也适用于非分布式使用内存高效的注意力，并且在我们的实验中，带来了额外的 1.31$\times$ 的加速（§ [4.3](#S4.SS3
    "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")）。'
- en: 'Our main contributions are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献是：
- en: '1.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We design LightSeq, a long-context LLM training prototype based on sequence-level
    parallelism. We develop a distributed memory-efficient exact attention DistAttn,
    with novel load balancing and communication overlapping scheduling for causal
    language modeling.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了 LightSeq，一个基于序列级并行性的长上下文 LLM 训练原型。我们开发了一种分布式内存高效的精确注意力 DistAttn，并针对因果语言建模设计了新颖的负载平衡和通信重叠调度。
- en: '2.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We propose a novel checkpointing strategy that bypasses one attention forward
    pass when using memory-efficient attention with gradient checkpointing training.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的检查点策略，该策略在使用内存高效的注意力和梯度检查点训练时跳过一个注意力前向传递。
- en: '3.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We evaluate LightSeq on Llama-7B and its variants with different attention heads
    patterns, and demonstrate up to 2.01$\times$ longer sequences training.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在 Llama-7B 及其变体上评估了 LightSeq，并展示了最多 2.01$\times$ 的长序列训练能力。
- en: 2 Related work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Memory-efficient attention.
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存高效的注意力。
- en: Dao et al. ([2022](#bib.bib6)) and Lefaudeux et al. ([2022](#bib.bib10)) propose
    to use an online normalizer (Milakov & Gimelshein, [2018](#bib.bib16)) to compute
    the attention in a blockwise and memory-efficient way. It reduces peak memory
    usage by not materializing large intermediate states, e.g. the attention matrix
    or the up projection matrix output of the MLP layers (Liu & Abbeel, [2023](#bib.bib13)).
    Instead, the attentions are computed in smaller blocks and only the final activation
    are stored. In the backward pass, the intermediate states need to be recomputed.
    Research on sparse attention computes only a sparse subset of the attention score,
    which also reduces the memory footprints yet may lead to inferior performance (Beltagy
    et al., [2020](#bib.bib1); Sun et al., [2022](#bib.bib22); Zaheer et al., [2020](#bib.bib26)).
    In this work, we limit our scope to exact attention.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Dao 等人 ([2022](#bib.bib6)) 和 Lefaudeux 等人 ([2022](#bib.bib10)) 提出了使用在线归一化器 (Milakov
    & Gimelshein，[2018](#bib.bib16)) 以块状和内存高效的方式计算注意力。通过不物化大规模中间状态，例如注意力矩阵或 MLP 层的上投影矩阵输出，减少了峰值内存使用。相反，注意力在较小的块中计算，只有最终的激活被存储。在反向传播过程中，需要重新计算中间状态。对稀疏注意力的研究计算了注意力得分的稀疏子集，这也减少了内存占用，但可能导致性能下降
    (Beltagy 等人，[2020](#bib.bib1); Sun 等人，[2022](#bib.bib22); Zaheer 等人，[2020](#bib.bib26))。在这项工作中，我们将范围限制在精确注意力上。
- en: Sequence parallelism, model parallelism, and FSDP.
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 序列并行性、模型并行性和 FSDP。
- en: Li et al. ([2021](#bib.bib12)) is among the first to parallelize along the sequence
    dimension. However, it is not optimized for the computational pattern of causal
    language modeling and is incompatible with memory-efficient attention, which are
    crucial to long-context LLM training. Model parallelism partitions model parameters
    and also distributes the activation in parallel LLM training. Megatron-LM (Korthikanti
    et al., [2023](#bib.bib9)) proposes a hybrid usage of tensor parallelism and sequence
    parallelism to better reduce the activation on a single device and is the main
    baseline of the paper. Fully sharded data-parallelism (FSDP) (Zhao et al., [2023](#bib.bib27);
    Rajbhandari et al., [2020](#bib.bib20)) distributes optimizer states, gradients,
    and model parameters onto different devices and gathers them on-the-fly. It is
    orthogonal to our work, and we use LightSeq in tandem with FSDP to further reduce
    memory acquired by models in experiments.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 ([2021](#bib.bib12)) 是首批沿序列维度进行并行化的研究之一。然而，该方法未针对因果语言建模的计算模式进行优化，并且与内存高效的注意力机制不兼容，而这些在长上下文的
    LLM 训练中至关重要。模型并行性将模型参数进行分割，并在并行 LLM 训练中分配激活。Megatron-LM (Korthikanti 等人，[2023](#bib.bib9))
    提出了混合使用张量并行性和序列并行性的方法，以更好地减少单一设备上的激活，是本文的主要基准。完全分片数据并行性 (FSDP) (Zhao 等人，[2023](#bib.bib27);
    Rajbhandari 等人，[2020](#bib.bib20)) 将优化器状态、梯度和模型参数分布到不同设备上，并动态汇总。它与我们的工作是正交的，我们在实验中将
    LightSeq 与 FSDP 结合使用，以进一步减少模型占用的内存。
- en: Gradient checkpointing.
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度检查点。
- en: Gradient checkpointing (Chen et al., [2016](#bib.bib4)) trades computation for
    memory by not storing the activation for certain layers and recomputing their
    activations during forward. Selective checkpointing (Korthikanti et al., [2023](#bib.bib9))
    proposes to only recompute the attention module as it requires large memory but
    with small FLOPs (in smaller context length). Checkmate (Jain et al., [2020](#bib.bib7))
    searches optimal checkpointing using integer linear programming. However, none
    of these designs have considered memory-efficient attention kernels which perform
    recomputation inside the computational kernel to avoid materializing large tensors.
    As a result, many previous recomputation policies become less effective. In this
    work, we focus on checkpointing at the boundary of every transformer layer, which
    is a popular strategy adopted by many current open-sourced projects such as FastChat (Zheng
    et al., [2023](#bib.bib28)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查点 (Chen 等人，[2016](#bib.bib4)) 通过不存储某些层的激活，而在前向传播过程中重新计算其激活，来权衡计算和内存。选择性检查点
    (Korthikanti 等人，[2023](#bib.bib9)) 提出了仅重新计算注意力模块的方法，因为它需要大量内存但 FLOPs 较小 (在较小的上下文长度中)。Checkmate
    (Jain 等人，[2020](#bib.bib7)) 使用整数线性规划来搜索最佳检查点。然而，这些设计都没有考虑内存高效的注意力内核，它在计算内核内部执行重新计算，以避免物化大规模张量。因此，许多之前的重新计算策略变得不那么有效。在这项工作中，我们专注于在每个变换器层的边界进行检查点，这是许多当前开源项目如
    FastChat (Zheng 等人，[2023](#bib.bib28)) 采用的流行策略。
- en: '![Refer to caption](img/c69a50b39096e966b8dafb393b184ee0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c69a50b39096e966b8dafb393b184ee0.png)'
- en: 'Figure 1: Left: Sequence parallelism in LightSeq. The input sequence is split
    into chunks along the sequence dimension and distributed to different workers
    (8 workers in the illustration). During forward and backward, only the attention
    module, DistAttn, requires communication of intermediate tensors like $k$. Some
    modules like LayerNorm are ignored for simplicity. Right: Illustration of the
    load-balanced scheduling. “Bubble size” represents the times that a worker is
    idle. Causal language modeling naturally introduces imbalanced workloads, e.g.,
    worker 1 is idle from time step 2 to time step 8 before balancing. We reduce the
    bubble fraction by allocating computation from the busy worker (e.g., worker 8)
    to the idle worker (e.g., worker 1), so worker 1 is only idle at time step 5 after
    balancing.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：左侧：LightSeq 中的序列并行性。输入序列沿序列维度被分割成多个块，并分发到不同的工作节点（图中为 8 个工作节点）。在前向和反向过程中，只有注意力模块
    DistAttn 需要通信中间张量，如 $k$。为了简化起见，像 LayerNorm 这样的模块被忽略。右侧：负载均衡调度的示意图。“气泡大小”表示工作节点空闲的时间。例如，因果语言建模自然引入了不平衡的工作负载，例如工作节点
    1 在平衡之前从时间步 2 到时间步 8 空闲。通过将繁忙工作节点（例如，工作节点 8）的计算分配给空闲工作节点（例如，工作节点 1），我们减少了气泡比例，从而使工作节点
    1 在平衡后仅在时间步 5 空闲。
- en: '![Refer to caption](img/bcbdd1ebd17667b0680f45cb12c1900d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bcbdd1ebd17667b0680f45cb12c1900d.png)'
- en: 'Figure 2: Forward pass example of overlapping communication using worker 7
    out of 8 workers. $o$ for worker 7\. In the communication stream, “S” stands for
    sending, and “R” stands for receiving. For instance, $S:kv_{7}\rightarrow p_{8}$
    to the remote worker $p_{8}$.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 8 个工作节点中的工作节点 7 进行重叠通信的前向传递示例。$o$ 为工作节点 7。在通信流中，“S”表示发送，“R”表示接收。例如，$S:kv_{7}\rightarrow
    p_{8}$ 发送给远程工作节点 $p_{8}$。
- en: 3 Method
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'In this section, we describe the design of the key components in LightSeq.
    We first introduce a distributed memory-efficient attention, DistAttn (§[3.1](#S3.SS1
    "3.1 DistAttn: distributed memory-efficient attention ‣ 3 Method ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")) which
    parallelizes the computation along the sequence dimension. We then introduce a
    load-balanced scheduling for causal language modeling to reduce the computation
    bubble as well as an asynchronous communication design that overlaps the communication
    into computation (§[3.2](#S3.SS2 "3.2 Load balanced scheduling with communication
    and computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")). Finally, we propose a rematerialization-aware
    checkpointing strategy (§[3.3](#S3.SS3 "3.3 Rematerialization-aware checkpointing
    strategy ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers")) which effectively cuts off the recomputation time
    in gradient checkpointing.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们描述了 LightSeq 中关键组件的设计。我们首先介绍了一种分布式内存高效的注意力机制 DistAttn (§[3.1](#S3.SS1
    "3.1 DistAttn: distributed memory-efficient attention ‣ 3 方法 ‣ LightSeq: 用于长上下文变换器的分布式训练的序列级并行"))，该机制在序列维度上并行计算。接着，我们介绍了一种负载均衡的调度策略，用于因果语言建模，以减少计算气泡，以及一种异步通信设计，将通信与计算重叠
    (§[3.2](#S3.SS2 "3.2 负载均衡调度与通信计算重叠 ‣ 3 方法 ‣ LightSeq: 用于长上下文变换器的分布式训练的序列级并行"))。最后，我们提出了一种考虑再物化的检查点策略
    (§[3.3](#S3.SS3 "3.3 考虑再物化的检查点策略 ‣ 3 方法 ‣ LightSeq: 用于长上下文变换器的分布式训练的序列级并行"))，它有效地缩短了梯度检查点中的重新计算时间。'
- en: '3.1 DistAttn: distributed memory-efficient attention'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 DistAttn：分布式内存高效的注意力机制
- en: The core idea in DistAttn is to split the input sequence consisting of $N$ workers
    (e.g. GPUs) along the sequence dimension. Each worker is therefore responsible
    for computing the forward and backward pass for only $N/P$ tokens. For modules
    like the Feed Forward Layer (FFN), Layer Norm (LN), and the embedding layer the
    tokens can be computed independently without coordination (embarrasingly parallel)
    and the work is balanced across workers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DistAttn 的核心思想是将由 $N$ 个工作节点（例如 GPU）组成的输入序列沿序列维度进行拆分。因此，每个工作节点只负责计算 $N/P$ 个标记的前向和反向传递。对于像前馈层（FFN）、层归一化（LN）和嵌入层这样的模块，标记可以独立计算而无需协调（尴尬并行），工作负载在工作节点之间平衡。
- en: 'Unfortunately, for the attention modules where local tokens may need to attend
    to remote tokens, coordination is required. To address this, each worker collects
    all the keys and values associated with other tokens and then locally computes
    the attention following Dao ([2023](#bib.bib5)). To address the memory pressure
    introduced by collecting all other keys and values, this process is done online
    by streaming the key and values from workers with earlier tokens to workers with
    later tokens. More formally, denote $\mathbf{q}_{p}$, $\mathbf{v}_{p}$-th worker
    ($p=\{1,\cdots,P\}$ as the attention computation w.r.t. $p$-th chunk of the key
    and value, denote $p_{\text{local}}\in\{1,\cdots,P\}$ as one of the remote ranks.
    Figure. [1](#S2.F1 "Figure 1 ‣ Gradient checkpointing. ‣ 2 Related work ‣ LightSeq:
    Sequence Level Parallelism for Distributed Training of Long Context Transformers")
    (“Before Balancing”) shows the vanilla version of DistAttn, where each worker
    computes the attention for $\mathbf{q}_{p_{\text{local}}}$ and $\mathbf{v}_{p_{\text{remote}}}$
    before the computation of ${attn}(\mathbf{q}_{p_{\text{local}}},\mathbf{k}_{p_{\text{remote}}},\mathbf{v}_{p_{\text{remote}}})$-th
    worker where there are $P$ total workers.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '不幸的是，对于那些本地 token 可能需要关注远程 token 的注意力模块，需要进行协调。为了解决这个问题，每个工作节点收集与其他 token 相关的所有键和值，然后按照
    Dao ([2023](#bib.bib5)) 的方法在本地计算注意力。为了应对收集所有其他键和值所引入的内存压力，这个过程通过将键和值从具有早期 token
    的工作节点流式传输到具有后期 token 的工作节点来在线完成。更正式地表示，设 $\mathbf{q}_{p}$，$\mathbf{v}_{p}$-th
    工作节点（$p=\{1,\cdots,P\}$ 作为相对于第 $p$ 个键值块的注意力计算），设 $p_{\text{local}}\in\{1,\cdots,P\}$
    作为远程排名之一。图 [1](#S2.F1 "Figure 1 ‣ Gradient checkpointing. ‣ 2 Related work ‣ LightSeq:
    Sequence Level Parallelism for Distributed Training of Long Context Transformers")（“平衡之前”）显示了
    DistAttn 的原始版本，其中每个工作节点在计算 ${attn}(\mathbf{q}_{p_{\text{local}}},\mathbf{k}_{p_{\text{remote}}},\mathbf{v}_{p_{\text{remote}}})$-th
    工作节点之前计算 $\mathbf{q}_{p_{\text{local}}}$ 和 $\mathbf{v}_{p_{\text{remote}}}$ 的注意力，总共有
    $P$ 个工作节点。'
- en: 3.2 Load balanced scheduling with communication and computation overlap
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 具有通信和计算重叠的负载均衡调度
- en: Load balanced scheduling.
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负载均衡调度。
- en: 'Causal language modeling objective (Brown et al., [2020](#bib.bib3); Touvron
    et al., [2023](#bib.bib24)) is one of the most prevalent objectives for LLMs,
    where each token only attends to its previous tokens. This naturally introduces
    a work imbalance between workers in our block-wise attention: as shown in Figure [1](#S2.F1
    "Figure 1 ‣ Gradient checkpointing. ‣ 2 Related work ‣ LightSeq: Sequence Level
    Parallelism for Distributed Training of Long Context Transformers") (“Before Balancing”),
    in an 8-worker ($P=8$. In a general form, the idle fraction is $\frac{P^{2}-P}{2P^{2}}$
    when $P\rightarrow\infty$ to help compute for $\mathbf{q}_{p_{\text{remote}}}$
    compute ${attn}(\mathbf{q}_{8},\mathbf{k}_{1},\mathbf{v}_{1})$. When the number
    of workers is odd, the idle fraction is 0\. When the number of workers is even,
    the idle fraction is $\frac{1}{2P}$ when scaling to more number of workers.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '因果语言建模目标（Brown et al., [2020](#bib.bib3); Touvron et al., [2023](#bib.bib24)）是
    LLMs 最常见的目标之一，其中每个 token 只关注其之前的 tokens。这自然会在我们的块级注意力中引入工作不平衡：如图 [1](#S2.F1 "Figure
    1 ‣ Gradient checkpointing. ‣ 2 Related work ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")（“平衡之前”）所示，在 8 个工作节点的情况下（$P=8$）。一般来说，当
    $P\rightarrow\infty$ 时，空闲比例为 $\frac{P^{2}-P}{2P^{2}}$，以帮助计算 $\mathbf{q}_{p_{\text{remote}}}$
    的 ${attn}(\mathbf{q}_{8},\mathbf{k}_{1},\mathbf{v}_{1})$。当工作节点数量为奇数时，空闲比例为 0。当工作节点数量为偶数时，空闲比例在扩展到更多工作节点时为
    $\frac{1}{2P}$。'
- en: Communication and computation overlap.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通信与计算重叠。
- en: 'DistAttn relies on peer-to-peer (P2P) communication to fetch the $\mathbf{k},\mathbf{v}$
    chunks in the load balanced scheduling) from remote devices before computing the
    corresponding attention block. However, these communications can be easily overlapped
    with the computation of the former blocks. For instance, When the first worker
    is computing attention for its local token, it can pre-fetch the next chunk of
    tokens it needs for the next time step. In modern accelerators, this can be done
    by placing the attention computation kernel in the main GPU stream, and the P2P
    communication kernel in another stream, where they can run in parallel (Zhao et al.,
    [2023](#bib.bib27)). We demonstrate the overlapped scheduling for worker 7 on
    the 8 workers example in Figure. [2](#S2.F2 "Figure 2 ‣ Gradient checkpointing.
    ‣ 2 Related work ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers"). Empirically, we find this optimization greatly
    reduces the communication overhead (§[4.3](#S4.SS3 "4.3 Ablation Study ‣ 4 Experiments
    ‣ LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers")).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'DistAttn 依赖于点对点（P2P）通信从远程设备中获取 $\mathbf{k},\mathbf{v}$ 块（在负载均衡调度中），然后再计算相应的注意力块。然而，这些通信可以很容易地与前面块的计算重叠。例如，当第一个工作节点正在计算其本地令牌的注意力时，它可以预取下一个时间步所需的令牌块。在现代加速器中，这可以通过将注意力计算内核放置在主
    GPU 流中，并将 P2P 通信内核放置在另一个流中来实现，它们可以并行运行（Zhao et al., [2023](#bib.bib27)）。我们在图 [2](#S2.F2
    "图 2 ‣ 梯度检查点 ‣ 2 相关工作 ‣ LightSeq: 长上下文 Transformer 的分布式训练的序列级并行") 中展示了工作节点 7 的重叠调度示例。根据经验，我们发现这种优化显著减少了通信开销（§[4.3](#S4.SS3
    "4.3 消融研究 ‣ 4 实验 ‣ LightSeq: 长上下文 Transformer 的分布式训练的序列级并行")）。'
- en: '![Refer to caption](img/ef3017aa3585ae28e7072805e4013ca7.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ef3017aa3585ae28e7072805e4013ca7.png)'
- en: 'Figure 3: Time breakdown of attention versus other modules in a forward pass.
    Time measured with Flash-Attention (Dao, [2023](#bib.bib5)) (Unit ms).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：前向传递中注意力与其他模块的时间划分。时间使用 Flash-Attention 测量（Dao, [2023](#bib.bib5)）（单位：毫秒）。
- en: 3.3 Rematerialization-aware checkpointing strategy
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 关注重材料化的检查点策略
- en: 'The de-facto way of training transformers requires gradient checkpointing.
    Often, the system uses heuristics to insert gradient checkpoints at each Transformer
    layer (Wolf et al., [2019](#bib.bib25)). However, with the presence of Dao et al.
    ([2022](#bib.bib6)), we found the previous gradient checkpointing strategy will
    cause an extra recomputation of the flash attention forward kernel. Concretely,
    when computing the gradient of the MLP layer, Wolf et al. ([2019](#bib.bib25))
    will re-compute the forward of the entire Transformer layer, including the one
    in flash attention. However, when computing the gradient of the flash attention
    kernel, it needs to re-compute the forward of the flash attention again. Essentially,
    this is because flash attention will not materialize the intermediate values during
    the forward, and will recompute it during the backward, regardless of the re-computation
    strategy in the outer system level. To tackle this, we propose to insert checkpoints
    at the output of the flash attention kernel, instead of at the Transformer layer
    boundary. In this case, we only need to recompute the forward of flash attention
    once, effectively saving a forward of attention for each Transformer layer as
    shown in Figure. [4](#S3.F4 "Figure 4 ‣ 3.3 Rematerialization-aware checkpointing
    strategy ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers"). In Figure. [3](#S3.F3 "Figure 3 ‣ Communication
    and computation overlap. ‣ 3.2 Load balanced scheduling with communication and
    computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers"), we show the attention time dominates
    in the forward pass when scaling up the sequence length, which indicates our method
    can save $\sim 0.23\times 32$) seconds when training a 64K sequence example on
    Llama-7b using the local version of flash attention. In addition, this saves a
    communication brought by our DistAttn forward in the distributed training scenario.
    We benchmark the end-to-end speedup brought by this materialization-aware checkpointing
    strategy in §[4.3](#S4.SS3 "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 训练变换器的事实标准方法需要梯度检查点。通常，系统使用启发式方法在每个变换器层插入梯度检查点（Wolf 等，[2019](#bib.bib25)）。然而，随着
    Dao 等人（[2022](#bib.bib6)）的出现，我们发现之前的梯度检查点策略会导致额外的闪存注意力前向内核的重新计算。具体而言，当计算 MLP 层的梯度时，Wolf
    等人（[2019](#bib.bib25)）将重新计算整个变换器层的前向计算，包括闪存注意力中的部分。然而，当计算闪存注意力内核的梯度时，它需要再次重新计算闪存注意力的前向计算。本质上，这是因为闪存注意力在前向计算过程中不会生成中间值，而是在反向计算过程中重新计算，无论外部系统级的重新计算策略如何。为了解决这个问题，我们建议在闪存注意力内核的输出处插入检查点，而不是在变换器层边界处。在这种情况下，我们只需要重新计算一次闪存注意力的前向计算，从而有效地节省了每个变换器层的前向计算，如图[4](#S3.F4
    "图4 ‣ 3.3 材料化感知检查点策略 ‣ 3 方法 ‣ LightSeq：用于分布式训练长上下文变换器的序列级并行")所示。在图[3](#S3.F3 "图3
    ‣ 通信与计算重叠。 ‣ 3.2 具有通信和计算重叠的负载均衡调度 ‣ 3 方法 ‣ LightSeq：用于分布式训练长上下文变换器的序列级并行")中，我们展示了在扩展序列长度时前向传递中的注意力时间占主导地位，这表明我们的方法在使用闪存注意力的本地版本训练
    64K 序列示例时可以节省约 $0.23\times 32$) 秒。此外，这也节省了在分布式训练场景中由我们的 DistAttn 前向计算带来的通信开销。我们在
    §[4.3](#S4.SS3 "4.3 消融研究 ‣ 4 实验 ‣ LightSeq：用于分布式训练长上下文变换器的序列级并行") 中对这种材料化感知检查点策略带来的端到端加速进行了基准测试。
- en: '![Refer to caption](img/14b14ee7899d18f6b2fe1cf5fc1d8db2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/14b14ee7899d18f6b2fe1cf5fc1d8db2.png)'
- en: 'Figure 4: Comparison of HuggingFace gradient checkpointing strategy and our
    materialization-aware gradient checkpointing strategy. Note that our checkpointing
    strategy saves an entire flash attention forward per layer in recomputation.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：HuggingFace 梯度检查点策略与我们材料化感知梯度检查点策略的比较。请注意，我们的检查点策略在重新计算时会为每层保存整个闪存注意力的前向计算。
- en: Communication and memory analysis Denote the hidden dimension as $d$ before
    performing the corresponding chunk-wise computation. Thus, the total communication
    volume in the $P$. With the causal language objective, half of the keys and values
    do not need to be attended, halving the forward communication volume to $Nd$ volume.
    It adds up to $3Nd$ size tensor, thus giving a total communication volume of $10Nd$.
    On the other hand, our communication volume remains $3Nd$ because of the rematerialization-aware
    strategy. In conclusion, LightSeq achieves 4.7x communication volume reduction
    compared with Megatron-LM.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通信和内存分析在执行相应的块计算之前，隐藏维度记为$d$。因此，总通信量为$P$。对于因果语言目标，一半的键和值不需要被关注，从而将前向通信量减少为$Nd$。它加起来形成$3Nd$大小的张量，因此总通信量为$10Nd$。另一方面，由于重计算感知策略，我们的通信量保持为$3Nd$。总之，与Megatron-LM相比，LightSeq实现了4.7倍的通信量减少。
- en: 'In practice, we combine LightSeq with FSDP to also distribute the model weights
    for large models. We note that the communication introduced by FSDP is only proportional
    to the size of model weights, which does not scale up with long sequence length.
    We show the end-to-end speedup with FSDP in Table [1](#S4.T1 "Table 1 ‣ Implementation.
    ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers"). In the situations where the model uses MQA or
    GQA, LightSeq further saves the communication volumes by the shared key and values,
    which we discuss in detail in § [4.1](#S4.SS1 "4.1 faster training speed and better
    support for different model architectures ‣ 4 Experiments ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers"). However,
    we also note that this is a theoretical analysis, where the wall-clock time may
    differ because of factors such as implementations. In the experiment section,
    we provide wall-clock end-to-end results for comparison.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '实际上，我们将LightSeq与FSDP结合使用，也为大模型分配模型权重。我们注意到，FSDP引入的通信仅与模型权重的大小成正比，这不会随着长序列长度的增加而增加。我们在表[1](#S4.T1
    "Table 1 ‣ Implementation. ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")中展示了FSDP的端到端加速情况。在使用MQA或GQA的模型情况下，LightSeq通过共享键和值进一步节省了通信量，我们在§
    [4.1](#S4.SS1 "4.1 faster training speed and better support for different model
    architectures ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers")中详细讨论。然而，我们也注意到这是一个理论分析，实际的墙钟时间可能会因为实现等因素有所不同。在实验部分，我们提供了墙钟端到端结果以供比较。'
- en: 4 Experiments
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we evaluate LightSeq against Megatron-LM (Korthikanti et al.,
    [2023](#bib.bib9)) and show:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将LightSeq与Megatron-LM（Korthikanti et [2023](#bib.bib9)）进行比较，并展示：
- en: '1.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: LightSeq has faster training speed on a wide range of models. It achieves up
    to 2.01$\times$ speedup over Megatron-LM on various MHA and GQA models.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LightSeq在各种模型上的训练速度更快。它在不同的MHA和GQA模型上相比Megatron-LM实现了高达2.01$\times$的加速。
- en: '2.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: LightSeq supports longer sequence length by scaling beyond the number of attention
    heads. We show our method can support 2x-8x longer sequences than Megatron-LM.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LightSeq通过超越注意力头的数量来支持更长的序列长度。我们展示了我们的方法可以支持比Megatron-LM长2x-8x的序列。
- en: 'In the ablation study, we provide the gain from each component of LightSeq:
    Load balancing, computation-communication overlapping, and rematerialization-aware
    checkpointing.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在消融研究中，我们提供了LightSeq各组件的收益：负载均衡、计算-通信重叠和重计算感知的检查点。
- en: Cluster setup.
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集群设置。
- en: We evaluate our method and the baseline in (1) A single A100 DGX box with 8x80
    GB GPUs. These GPUs are connected with NVLink; (2) 2 DGX boxes with the same setting.
    These two boxes are interconnected by 100 Gbps Infiniband. This is representative
    of cross-node training, where the communication overhead has a larger effect.
    (3) Our in-house cluster with 2x8 A100 40GB GPUs without Inifiniband. We report
    some results on this cluster where conclusions can be drawn from a single-node
    setup or without involving cross-node training time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下环境中评估了我们的方法和基线：（1）一台配有8x80 GB GPU的单A100 DGX箱。这些GPU通过NVLink连接；（2）两个DGX箱，配置相同，这两个箱通过100
    Gbps Infiniband互联。这代表了跨节点训练，其中通信开销的影响更大。（3）我们内部集群配备2x8 A100 40GB GPU且没有Inifiniband。我们报告了在该集群上的一些结果，这些结果可以从单节点设置中得出结论或不涉及跨节点训练时间。
- en: Model setup.
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型设置。
- en: 'We evaluate our system on Llama-7B and its variants of different representative
    families: (1) Multi-head attention(MHA) models: LLama-7B with 4096 hidden size
    and 32 query(key and value) heads (Touvron et al., [2023](#bib.bib24)); (2) Grouped-Query
    attention (GQA) models: Llama-GQA, same as Llama-7B but with 8 key and value heads;
    (3) models with more general number of attention heads: Llama-33H same as Llama-7B
    but with 33 query (key and value) attention heads. (4) models with fewer attention
    heads: we design Llama-16H, Llama-8H, Llama-4H, Llama-2H with 16, 8, 4, and 2
    heads. According to Liu et al. ([2021](#bib.bib14)), we keep the number of attention
    heads by scaling the number of layers properly and keep the intermediate FFN layer
    size the same to make the model sizes still comparable. For example, Llama-16H
    has 16 attention heads per layer, a hidden size of 2048, an FFN layer of size
    11008, and 64 layers.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Llama-7B及其不同代表性家族的变体上评估我们的系统：（1）多头注意力（MHA）模型：LLama-7B，具有4096隐藏层大小和32个查询（键和值）头（Touvron
    et al., [2023](#bib.bib24)）；（2）分组查询注意力（GQA）模型：Llama-GQA，与Llama-7B相同，但具有8个键和值头；（3）具有更多通用数量注意力头的模型：Llama-33H，与Llama-7B相同，但具有33个查询（键和值）注意力头；（4）具有较少注意力头的模型：我们设计了Llama-16H、Llama-8H、Llama-4H、Llama-2H，分别具有16、8、4和2个头。根据Liu
    et al.（[2021](#bib.bib14)），我们通过适当地调整层数来保持注意力头的数量，并保持中间FFN层大小不变，以使模型大小仍然可比。例如，Llama-16H每层具有16个注意力头，隐藏层大小为2048，FFN层大小为11008，并且有64层。
- en: Implementation.
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施。
- en: 'LightSeq is a lightweight scheduling level prototype. In particular, we implement
    the load balancing and overlapping in Python and NCCL Pytorch bindings in 1000
    lines of codes (Paszke et al., [2019](#bib.bib19); Jeaugey, [2017](#bib.bib8)),
    and the checkpointing strategy in 600 lines of Pytorch. It is attention backend
    agnostic. To reduce the memory consumption and reach faster speed in the attention
    module, we use the FlashAttention2 algorithm (Dao, [2023](#bib.bib5)). We use
    the triton (Tillet et al., [2019](#bib.bib23)) implementation and minimally modify
    it to keep around statistics in the flash attention algorithm. We tweak all block
    sizes to 128 and the number of stages to 1 for the best performance in our cluster.
    We reuse the C++ backward kernels of FlashAttention2 because we do not need to
    modify the backward logic. We run LightSeq using FSDP to reduce the memory footprint
    of data parallelism (Zhao et al., [2023](#bib.bib27)). For fair comparisons, we
    run all comparisons using the same attention backend. We also add support for
    Megatron-LM so that comparing with them can produce a more insightful analysis:
    (1) not materializing the causal attention mask, greatly reducing the memory footprint.
    For instance, without this support, Megatron-LM will run out of memory with Llama-7B
    at a sequence length of 16K per GPU. (2) head padding where the attention heads
    cannot be divided by device number. All results are gathered with Adam optimizer,
    10 iterations of warm-up, and averaged over the additional 10 iterations.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: LightSeq是一个轻量级调度级原型。特别地，我们在Python和NCCL Pytorch绑定中实现了负载平衡和重叠，总共1000行代码（Paszke
    et al., [2019](#bib.bib19); Jeaugey, [2017](#bib.bib8)），并在600行Pytorch中实现了检查点策略。它对注意力后端无依赖。为了减少内存消耗并在注意力模块中实现更快的速度，我们使用了FlashAttention2算法（Dao,
    [2023](#bib.bib5)）。我们使用triton（Tillet et al., [2019](#bib.bib23)）实现，并对其进行最小修改以保持闪存注意力算法中的统计数据。我们将所有块大小调整为128，将阶段数调整为1，以在我们的集群中获得最佳性能。我们重用了FlashAttention2的C++反向内核，因为我们不需要修改反向逻辑。我们使用FSDP运行LightSeq以减少数据并行的内存占用（Zhao
    et al., [2023](#bib.bib27)）。为了公平比较，我们使用相同的注意力后端进行所有比较。我们还添加了对Megatron-LM的支持，以便与它们进行比较可以产生更有洞察力的分析：（1）不实现因果注意力掩码，极大地减少了内存占用。例如，未经此支持，Megatron-LM在每个GPU上序列长度为16K时将耗尽内存。（2）头部填充，其中注意力头不能被设备数量整除。所有结果均使用Adam优化器、10次热身迭代，并在额外的10次迭代中取平均。
- en: 'Table 1: Per iteration wall-clock time of LightSeq and Megatron-LM (Korthikanti
    et al., [2023](#bib.bib9)) (Unit: seconds). Speedup in bold denotes the better
    of the two systems in the same configuration.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LightSeq和Megatron-LM的每次迭代墙钟时间（Korthikanti et al., [2023](#bib.bib9)）（单位：秒）。加速比用粗体表示，表示相同配置下两个系统中的较好者。
- en: '| Method | # GPUs | Sequence Length | Llama-7B | Llama-GQA | Llama-33H |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GPU数量 | 序列长度 | Llama-7B | Llama-GQA | Llama-33H |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  |  | Per GPU | Total | Time | speedup | Time | speedup | Time | speedup
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 每个GPU | 总计 | 时间 | 加速比 | 时间 | 加速比 | 时间 | 加速比 |'
- en: '| Megatron-LM | 1x8 | 4K | 32K | 2.54 | 1.0x | 2.43 | 1.0x | 3.15 | 1.0x |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM | 1x8 | 4K | 32K | 2.54 | 1.0x | 2.43 | 1.0x | 3.15 | 1.0x |'
- en: '| 1x8 | 8K | 64K | 6.81 | 1.0x | 6.60 | 1.0x | 8.37 | 1.0x |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 8K | 64K | 6.81 | 1.0x | 6.60 | 1.0x | 8.37 | 1.0x |'
- en: '| 1x8 | 16K | 128K | 20.93 | 1.0x | 20.53 | 1.0x | 25.75 | 1.0x |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 16K | 128K | 20.93 | 1.0x | 20.53 | 1.0x | 25.75 | 1.0x |'
- en: '| 1x8 | 32K | 256K | 72.75 | 1.0x | 71.93 | 1.0x | 90.21 | 1.0x |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 32K | 256K | 72.75 | 1.0x | 71.93 | 1.0x | 90.21 | 1.0x |'
- en: '| LightSeq | 1x8 | 4K | 32K | 2.50 | 1.02x | 2.30 | 1.06x | 2.58 | 1.22x |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LightSeq | 1x8 | 4K | 32K | 2.50 | 1.02x | 2.30 | 1.06x | 2.58 | 1.22x |'
- en: '| 1x8 | 8K | 64K | 5.98 | 1.14x | 5.61 | 1.18x | 6.08 | 1.38x |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 8K | 64K | 5.98 | 1.14x | 5.61 | 1.18x | 6.08 | 1.38x |'
- en: '| 1x8 | 16K | 128K | 17.26 | 1.21x | 16.86 | 1.22x | 17.77 | 1.45x |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 16K | 128K | 17.26 | 1.21x | 16.86 | 1.22x | 17.77 | 1.45x |'
- en: '| 1x8 | 32K | 256K | 58.46 | 1.24x | 57.01 | 1.26x | 59.96 | 1.50x |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 32K | 256K | 58.46 | 1.24x | 57.01 | 1.26x | 59.96 | 1.50x |'
- en: '| Megatron-LM | 2x8 | 4K | 64K | 5.29 | 1.0x | 5.26 | 1.0x | 7.52 | 1.0x |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM | 2x8 | 4K | 64K | 5.29 | 1.0x | 5.26 | 1.0x | 7.52 | 1.0x |'
- en: '| 2x8 | 8K | 128K | 14.26 | 1.0x | 14.21 | 1.0x | 20.63 | 1.0x |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 14.26 | 1.0x | 14.21 | 1.0x | 20.63 | 1.0x |'
- en: '| 2x8 | 16K | 256K | 43.44 | 1.0x | 43.20 | 1.0x | 62.78 | 1.0x |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 43.44 | 1.0x | 43.20 | 1.0x | 62.78 | 1.0x |'
- en: '| 2x8 | 32K | 512K | 147.06 | 1.0x | 146.38 | 1.0x | 216.70 | 1.0x |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 147.06 | 1.0x | 146.38 | 1.0x | 216.70 | 1.0x |'
- en: '| LightSeq | 2x8 | 4K | 64K | 6.85 | 0.77x | 4.92 | 1.07x | 7.03 | 1.07x |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LightSeq | 2x8 | 4K | 64K | 6.85 | 0.77x | 4.92 | 1.07x | 7.03 | 1.07x |'
- en: '| 2x8 | 8K | 128K | 12.75 | 1.12x | 9.74 | 1.46x | 13.12 | 1.57x |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 12.75 | 1.12x | 9.74 | 1.46x | 13.12 | 1.57x |'
- en: '| 2x8 | 16K | 256K | 30.21 | 1.44x | 28.49 | 1.52x | 31.33 | 2.00x |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 30.21 | 1.44x | 28.49 | 1.52x | 31.33 | 2.00x |'
- en: '| 2x8 | 32K | 512K | 106.37 | 1.38x | 102.34 | 1.43x | 107.76 | 2.01x |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 106.37 | 1.38x | 102.34 | 1.43x | 107.76 | 2.01x |'
- en: 'Table 2: The maximal sequence length Per GPU supported by LightSeq and Megatron-LM
    with tensor parallelism and pipeline parallelism on 16xA100 40GB GPUs.  LightSeq
    supports 512K sequence length in all models, while Megatron-LM strategy maximal
    sequence length decreases with fewer heads, with either data parallelism or pipeline
    parallelism.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：LightSeq 和 Megatron-LM 在 16xA100 40GB GPU 上支持的每个 GPU 的最大序列长度。LightSeq 在所有模型中支持
    512K 的序列长度，而 Megatron-LM 的策略最大序列长度随着头的减少而下降，无论是数据并行还是流水线并行。
- en: '|  | Llama-16H | Llama-8H | Llama-4H | Llama-2H |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama-16H | Llama-8H | Llama-4H | Llama-2H |  |'
- en: '| Megatron TP+DP | 512K | 256K | 128K | 64K |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Megatron TP+DP | 512K | 256K | 128K | 64K |  |'
- en: '| Megatron-LM TP+PP | 512K | 256K | 256K | 128K |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM TP+PP | 512K | 256K | 256K | 128K |  |'
- en: '|  LightSeq | 512K | 512K | 512K | 512K |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  LightSeq | 512K | 512K | 512K | 512K |  |'
- en: 4.1 faster training speed and better support for different model architectures
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 更快的训练速度和对不同模型架构的更好支持
- en: 'In this section, we compare our method with Megatron-LM on three settings:
    (1) the multi-head attention (MHA) models where the number of key and value heads
    equals the number of query heads; (2) the grouped-query attention (GQA) models
    where the number of key and value heads is less than the number of query heads;
    (3) the models with arbitrary numbers of heads, i.e. the number heads is unnecessarily
    a multiple of the parallelism degree.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将我们的方法与 Megatron-LM 在三种设置下进行比较：（1）多头注意力（MHA）模型，其中键值头的数量等于查询头的数量；（2）分组查询注意力（GQA）模型，其中键值头的数量少于查询头的数量；（3）具有任意数量头的模型，即头的数量不必是并行度的倍数。
- en: Multi-head attention (MHA).
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多头注意力（MHA）。
- en: 'On the Llama-7B model, our method achieves 1.24$\times$ speedup compared to
    Megatron-LM in single node and cross node setting, up to the longest sequence
    length we experiment. This is a joint result of our overlapping communication
    technique and our rematerialization-aware checkpointing strategy. We analyze how
    much each factor contributes to this result in the ablation study ( § [4.3](#S4.SS3
    "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")). We do note that our method
    does not achieve better performance in shorter sequences, such as per GPU 4K setting
    for cross node. This is because the communication dominates the training run-time,
    where our overlapping technique has not been able to reduce much. We leave the
    optimization of P2P communication on MHA models and shorter sequence length as
    an exciting future work.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Llama-7B 模型上，我们的方法在单节点和跨节点设置中相对于 Megatron-LM 实现了 1.24$\times$ 的加速，这是基于我们实验的最长序列长度。这是我们重叠通信技术和重新材料感知检查点策略共同作用的结果。我们在消融研究中分析了每个因素对这一结果的贡献（ § [4.3](#S4.SS3
    "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")）。我们确实注意到，在较短的序列中，例如跨节点的每 GPU
    4K 设置，我们的方法未能实现更好的性能。这是因为通信主导了训练运行时间，而我们的重叠技术未能显著减少这一时间。我们将 P2P 通信在 MHA 模型和较短序列长度上的优化作为未来的一个令人兴奋的研究方向。'
- en: Grouped-query attention (GQA).
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分组查询注意力（GQA）。
- en: On LLama-GQA model, our method achieves better speedup because our communication
    of key and value vectors significantly reduces. Note that our communication time
    is proportional to the sum of query, key, value, and output (for load balancing)
    vectors, where reducing key and value sizes to 8 almost half-en our communication
    time. On the contrary, the communication time in Megatron-LM does not decrease
    because its communication happens outside of the attention module, i.e. not influenced
    by optimization inside the attention module. Thus, its overall training run-time
    does not decrease as much as  LightSeq.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLama-GQA 模型上，我们的方法实现了更好的加速，因为我们的关键和价值向量的通信显著减少。请注意，我们的通信时间与查询、关键、价值和输出（用于负载均衡）向量的总和成正比，其中将关键和价值的大小减少到
    8 使我们的通信时间几乎减少了一半。相比之下，Megatron-LM 的通信时间并未减少，因为其通信发生在注意力模块之外，即不受注意力模块内部优化的影响。因此，其整体训练运行时间并没有像
    LightSeq 那样显著减少。
- en: We take the 4K per-GPU sequence length and 2x8 GPUs as an example for analysis.
    In the MHA experiment, the communication in a forward and a backward pass of a
    single attention module is roughly 143ms and the computation time is roughly 53ms.
    In addition, our overlapping technique is able to hide 45ms into the computation,
    resulting in a total run-time of 151ms and a net communication overhead of 98
    ms. As a reference, the communication in Megatron-LM takes 33ms, which is why
    Megatron-LM is faster than LightSeq under this particular setting in the MHA experiment.
    When considering the GQA case, the communication in LightSeq roughly reduces to
    71 ms. Overlapping with the computation, the communication overhead is now less
    than that of Megatron-LM. Combined with the checkpointing technique, we are seeing
    a positive speedup gain at 4K per-GPU sequence length. As the sequence length
    increases, our overlapping technique, driven by the fact that computation time
    surpasses communication time, and our checkpointing method, due to the rising
    ratio of a single attention forward, both contribute to greater speedup. Overall,
    we can observe speedups up to 1.52$\times$ on the cross-node setting, making an
    additional eight percent enhancement compared to the results in the MHA experiment
    of the same setting.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以每 GPU 4K 序列长度和 2x8 GPU 为例进行分析。在 MHA 实验中，单个注意力模块的前向和反向传播中的通信时间大约为 143ms，计算时间大约为
    53ms。此外，我们的重叠技术能够将 45ms 隐藏到计算中，导致总运行时间为 151ms，净通信开销为 98 ms。作为参考，Megatron-LM 的通信时间为
    33ms，这就是为什么在这种特定设置下，Megatron-LM 在 MHA 实验中比 LightSeq 更快。当考虑 GQA 情况时，LightSeq 的通信时间大约减少到
    71 ms。与计算重叠后，通信开销现在低于 Megatron-LM。结合检查点技术，我们在每 GPU 4K 序列长度时看到了正向的加速增益。随着序列长度的增加，由于计算时间超过通信时间，我们的重叠技术以及由于单个注意力前向传播比率上升而导致的检查点方法，都贡献了更大的加速。总体而言，我们在跨节点设置中观察到了高达
    1.52$\times$ 的加速，与相同设置下的 MHA 实验结果相比，额外提升了八个百分点。
- en: In support of arbitrary numbers of heads.
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 支持任意数量的头。
- en: With Llama-33H models, Megatron-LM exhibits an additional performance decline
    compared to LightSeq. This is due to its requirement to pad the number of attention
    heads so that the number of attention heads is divisible by the number of devices.
    On the other hand,  LightSeq does not need to partition attention heads and can
    support an arbitrary number of heads efficiently. For instance, when using 8 GPUs,
    Megatron-LM must pad the attention heads to 40, resulting in 21.2% of the computation
    being wasted. In the case of 16 GPUs, Megatron-LM is compelled to pad the attention
    heads to 48, leading to a more substantial computation wastage of 45.5%. This
    roughly corresponds to a 1.21$\times$ increase in run-time compared to  LightSeq
    when training a Llama-7B model. This performance degradation of Megatron-LM is
    primarily because the training time is dominated by the attention module’s computation
    time when scaling to longer sequence lengths. Empirically, we observe a 1.50$\times$
    speedup (an additional 20% and 45% speedup compared to Llama-7B cases, aligned
    with the theoretical analysis).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Llama-33H模型，Megatron-LM表现出相对于LightSeq的额外性能下降。这是由于其需要填充注意力头的数量，以便注意力头的数量能被设备数量整除。另一方面，LightSeq不需要分区注意力头，并且能够高效地支持任意数量的头。例如，当使用8个GPU时，Megatron-LM必须将注意力头填充到40，导致21.2%的计算被浪费。在16个GPU的情况下，Megatron-LM被迫将注意力头填充到48，导致计算浪费更大，为45.5%。这大致对应于与LightSeq相比，训练Llama-7B模型时的运行时间增加了1.21$\times$。Megatron-LM的性能下降主要是因为在扩展到更长的序列长度时，训练时间被注意力模块的计算时间主导。通过经验观察，我们发现相较于Llama-7B的情况，速度提高了1.50$\times$（额外提高了20%和45%的速度，符合理论分析）。
- en: 4.2 Scaling beyond the number of heads.
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 超越头的数量的扩展
- en: 'Assuming the number of heads being a multiple of the tensor parallelism degree
    constraints Megatron-LM to scale its tensor parallelism degree beyond the number
    of heads, thus limiting its scaling ability to longer sequence lengths. When the
    number of GPUs exceeds the number of attention heads, there will be three possible
    solutions to use Megatron-LM. First, the user can pad dummy heads as in the Llama-33H
    scenario. However, when scaling to longer sequences, the percentage of dummy heads
    padded almost directly translates to the percentage of slowdown. For instance,
    for Llama-8H, this solution pads 2$\times$ slowdown, which is very inefficient.
    Second, the user can use data parallelism for excess GPUs. For instance, a user
    with 16 GPUs can choose to use 4-way data parallelism and 4-way tensor parallelism
    on the Llama-4H model. Since data parallelism does not partition the activation,
    the system can only support sequences as if the user only has 4 GPUs. Lastly,
    the user may choose to use pipeline parallelism to partition activation. However,
    the memory usage at each stage of the pipeline is not evenly distributed, still
    limiting the maximal sequence length supported. In particular, the first pipeline
    stage usually stores more activations because it will hold the most active micro-batches.
    For instance, in the Llama-2H experiment, we find that different stages consume
    from 18GB to 32GB in a 64K sequence length. In addition, using pipeline parallelism
    introduces an extra fraction of GPU idle time. We demonstrate the effect of using
    the latter two solutions in Table [2](#S4.T2 "Table 2 ‣ Implementation. ‣ 4 Experiments
    ‣ LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers"). In 16 A100 40GB GPUs, LightSeq supports the training of 2$\times$
    longer sequences.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 假设头的数量是张量并行度的倍数，这会限制Megatron-LM将张量并行度扩展到头的数量以上，从而限制了其对更长序列长度的扩展能力。当GPU的数量超过注意力头的数量时，使用Megatron-LM会有三种可能的解决方案。首先，用户可以像在Llama-33H场景中那样填充虚拟头。然而，当扩展到更长的序列时，填充的虚拟头的百分比几乎直接转化为减慢的百分比。例如，对于Llama-8H，这种解决方案会导致2$\times$的减慢，这非常低效。其次，用户可以对多余的GPU使用数据并行。例如，一个拥有16个GPU的用户可以选择在Llama-4H模型上使用4路数据并行和4路张量并行。由于数据并行不对激活进行分区，因此系统只能支持用户只有4个GPU的序列。最后，用户可以选择使用流水线并行来分区激活。然而，流水线每个阶段的内存使用并不均匀，仍然限制了最大支持的序列长度。特别是，第一个流水线阶段通常存储更多的激活，因为它会持有最多的活跃微批次。例如，在Llama-2H实验中，我们发现不同阶段的消耗从18GB到32GB不等，序列长度为64K。此外，使用流水线并行会引入额外的GPU闲置时间。我们在表[2](#S4.T2
    "表 2 ‣ 实现 ‣ 4 实验 ‣ LightSeq：用于长上下文变换器的分布式训练的序列级并行")中展示了使用后两种解决方案的效果。在16个A100 40GB
    GPU中，LightSeq支持训练2$\times$更长的序列。
- en: 4.3 Ablation Study
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: Effect of load balancing.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负载均衡的效果。
- en: 'We study the effect of load balancing using the forward pass of an attention
    operation in Llama-7B model, on 8 A100 40GB GPUs. The backward pass follows a
    similar analysis. With an unbalanced schedule (Figure  [1](#S2.F1 "Figure 1 ‣
    Gradient checkpointing. ‣ 2 Related work ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")), the total work done
    is 36, where the total work could be done in 8 units of time is 64\. Thus, the
    expected maximal speedup is 4.5x. In the balanced schedule, the expected maximal
    speedup is 7.2x. We scale the total sequence length from 4K to 256K. The unbalanced
    version saturates in  4.5x speedup compared to a single GPU implementation, while
    the balanced version saturates  7.5x ¹¹1We find the single machine attention flops
    drop with very long sequence length, resulting in a slightly higher speedup than
    assuming its perfect scalability. speedup. Both of them align with our earlier
    theoretical analysis and show the importance of our balanced scheduling.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究了在 8 个 A100 40GB GPU 上使用 Llama-7B 模型中，前向传递的注意力操作对负载均衡的影响。反向传递遵循类似的分析。在不平衡调度下（图
    [1](#S2.F1 "Figure 1 ‣ Gradient checkpointing. ‣ 2 Related work ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")），总工作量为
    36，而总工作量可以在 8 个时间单位内完成的是 64。因此，预期的最大加速比为 4.5x。在平衡调度下，预期的最大加速比为 7.2x。我们将总序列长度从
    4K 扩展到 256K。与单 GPU 实现相比，不平衡版本的加速比饱和在 4.5x，而平衡版本饱和在 7.5x¹¹1我们发现单机注意力的浮点运算在非常长的序列长度下下降，导致加速比略高于假设其完美可扩展性的情况。加速比。这两者都与我们早期的理论分析一致，并显示了平衡调度的重要性。'
- en: '![Refer to caption](img/2664a01862a3803521ec7b6084c13446.png)![Refer to caption](img/1fec7276ed901b66c49fe3d84ad164c1.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2664a01862a3803521ec7b6084c13446.png)![参考说明](img/1fec7276ed901b66c49fe3d84ad164c1.png)'
- en: 'Figure 5: Ablation on the effect of balanced schedule (left) and the effect
    of overlapping (right).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：平衡调度（左侧）和重叠（右侧）效果的消融研究。
- en: Effect of overlapping communication and computation.
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通信与计算重叠的效果。
- en: We study the benefits of overlapping communication on Llama-7B and 2 DGX boxes.
    We find that overlapping greatly reduce the communication overhead. For instance,
    on a global sequence length of 128K, the communication overhead is reduced from
    105% to 44%. This overlapping scheme maximizes its functionality when the communication
    overhead is less than 100%, where all communication can be potentially overlapped.
    Empirically, we find the system only exhibits 8% and 1% overhead in these cases,
    showing a close performance to an ideal system without communication.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了在 2 个 DGX 盒子上 Llama-7B 的通信重叠的好处。我们发现重叠显著减少了通信开销。例如，在全球序列长度为 128K 的情况下，通信开销从
    105% 降低到 44%。当通信开销小于 100% 时，这种重叠方案最大化其功能，因为所有通信可能都可以重叠。经验上，我们发现系统在这些情况下仅显示 8%
    和 1% 的开销，表明性能接近于没有通信的理想系统。
- en: Effect of materialization-aware checkpointing.
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重材料化感知检查点的效果。
- en: 'We show in Table. [3](#S4.T3 "Table 3 ‣ Effect of materialization-aware checkpointing.
    ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers") the ablation results of our
    rematerialization-aware gradient checkpointing. Our method achieves 1.16x, 1.24x,
    and 1.31x speedup at the sequence length of 8K, 16K, and 32K per GPU respectively.
    The materialization-aware checkpointing strategy speeds up more at longer sequence
    lengths because it saves an entire attention forward which dominates the computation
    at longer sequence lengths.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表 [3](#S4.T3 "Table 3 ‣ Effect of materialization-aware checkpointing. ‣
    4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers") 中展示了我们的重材料化感知梯度检查点的消融结果。我们的方法在每
    GPU 序列长度为 8K、16K 和 32K 时分别实现了 1.16x、1.24x 和 1.31x 的加速。由于节省了在较长序列长度下主导计算的整个注意力前向过程，材料化感知检查点策略在较长序列长度下加速效果更明显。'
- en: 'Table 3: Ablation study on the effect of the rematerialization-aware gradient
    checkpointing on 8 A100s in a single node with a batch size of 1\. We report the
    end-to-end run time in seconds and show the speedup of our gradient checkpointing
    strategy (“Our ckpt”) over the HuggingFace gradient checkpointing strategy (“HF
    ckpt”).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在单节点的 8 个 A100 上，使用批大小为 1 的情况下，对重材料化感知梯度检查点的效果进行的消融研究。我们报告了端到端的运行时间（以秒为单位），并展示了我们梯度检查点策略（“我们的
    ckpt”）相对于 HuggingFace 梯度检查点策略（“HF ckpt”）的加速效果。
- en: '| Ckpt Method | Sequence Length Per GPU |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Ckpt 方法 | 每 GPU 序列长度 |'
- en: '|  | 1K | 2K | 4K | 8K | 16K | 32K |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 1K | 2K | 4K | 8K | 16K | 32K |'
- en: '| HF ckpt | 0.84 | 1.29 | 2.64 | 6.93 | 21.44 | 76.38 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| HF ckpt | 0.84 | 1.29 | 2.64 | 6.93 | 21.44 | 76.38 |'
- en: '| Our ckpt | 0.84 | 1.36 | 2.50 | 5.98 | 17.26 | 58.46 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 ckpt | 0.84 | 1.36 | 2.50 | 5.98 | 17.26 | 58.46 |'
- en: '| Speedup | 1.0x | 0.94x | 1.06x | 1.16x | 1.24x | 1.31x |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 加速比 | 1.0x | 0.94x | 1.06x | 1.16x | 1.24x | 1.31x |'
- en: 4.4 Discussion
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 讨论
- en: In this section, we first discuss the future directions that can further improve
    LightSeq. We then compare our method with one concurrent open-sourced project
    which also splits the attention heads. Finally, we discuss the role of pipeline
    parallelism in supporting long sequence training and shows it is less effective
    than tensor parallelism, which is the reason we do not consider it as a major
    baseline.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先讨论可以进一步改进 LightSeq 的未来方向。然后，我们将我们的方法与一个并发的开源项目进行比较，该项目也对注意力头进行分割。最后，我们讨论流水线并行在支持长序列训练中的作用，并表明它不如张量并行有效，这也是我们未将其视为主要基准的原因。
- en: Optimizing P2P communication and better support for shorter context length.
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化 P2P 通信和更好地支持较短上下文长度。
- en: 'As shown in §[4.1](#S4.SS1 "4.1 faster training speed and better support for
    different model architectures ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers"),  LightSeq may be slower
    in shorter context length and MHA models (Llama-7B on per GPU sequence length
    4K). Based on our preliminary investigation, this is because our usage of P2P
    is not as optimized as primitives used in tensor model parallelism, such as all-gather
    kernels. For instance, they are not aware of the underlying cluster topology.
    In the future, we plan to implement the P2P scheduling in a topology-aware way
    to further improve the communication time.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 §[4.1](#S4.SS1 "4.1 更快的训练速度和对不同模型架构的更好支持 ‣ 4 实验 ‣ LightSeq：用于长上下文变换器的序列级并行")中所示，LightSeq
    在较短上下文长度和 MHA 模型（Llama-7B 在每 GPU 序列长度 4K）上可能较慢。根据我们的初步调查，这是因为我们使用的 P2P 不如在张量模型并行中使用的原语（例如
    all-gather 内核）优化得那么好。例如，它们不了解底层集群拓扑结构。未来，我们计划以拓扑感知的方式实现 P2P 调度，以进一步提高通信时间。
- en: Comparison to DeepSpeed Ulysses.
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 DeepSpeed Ulysses 的比较。
- en: 'DeepSpeed-Ulysses ²²2[https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses)
    is a concurrent open-sourced implementation, which uses all-to-all communication
    primitive to reduce the communication volume. In our testing, we verified that
    their communication is lower than Megatron-LM. Yet, as it is also partitioning
    the attention head dimension, it suffers from similar problems as analyzed above.
    We provide some end-to-end comparisons in Appendix [B](#A2 "Appendix B Comparison
    with DeepSpeed Ulysses ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers"). We note that the communication in DeepSpeed
    Ulysses can be faster than LightSeq, especially with shorter context length and
    slower network, where the overlapping technique in LightSeq cannot perfectly hide
    all the communication. This can be potentially addressed by optimizing the P2P
    communication as discussed above.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed-Ulysses ²²2[https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses)
    是一个并发的开源实现，它使用全到全通信原语来减少通信量。在我们的测试中，我们验证了它们的通信量低于 Megatron-LM。然而，由于它也对注意力头维度进行分割，因此遭遇了类似上述分析的问题。我们在附录 [B](#A2
    "附录 B 与 DeepSpeed Ulysses 的比较 ‣ LightSeq：用于长上下文变换器的序列级并行")中提供了一些端到端的比较。我们注意到，DeepSpeed
    Ulysses 的通信速度可能比 LightSeq 更快，特别是在较短上下文长度和较慢网络的情况下，此时 LightSeq 中的重叠技术无法完美隐藏所有通信。这可以通过优化上述讨论的
    P2P 通信来潜在解决。
- en: Pipeline parallelism.
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 流水线并行。
- en: 'Pipeline parallelism also partitions the activation. However, as mentioned
    in § [4.2](#S4.SS2 "4.2 Scaling beyond the number of heads. ‣ 4 Experiments ‣
    LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers"), it does not partition the activations evenly across stage, leaving
    high memory pressure to the first stage. Thus, we mainly focus on comparing with
    tensor model parallelism (combined with sequence parallelism) in this work and
    only consider including pipeline parallelism for comparison when the tensor parallelism
    is limited by the number of heads.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '流水线并行性也会对激活进行分区。然而，如§ [4.2](#S4.SS2 "4.2 Scaling beyond the number of heads.
    ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers")中提到的，它不会在各阶段之间均匀分配激活，从而使第一阶段承受高内存压力。因此，我们在这项工作中主要关注与张量模型并行（结合序列并行）的比较，仅在张量并行受到头数限制时考虑将流水线并行性纳入比较。'
- en: 5 Conclusion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we introduce LightSeq, a sequence parallel prototype for long-context
    transformer training. LightSeq presents novel system optimizations including load
    balancing for causal language modelings, overlapped communication with computation
    in the distributed attention computation, and a re-materialization-aware checkpointing
    strategy. Our experiments evaluate multiple families of transformer models and
    on different cluster types, showing that it achieves up to 2.01$\times$ speedup
    and scales up to 8x longer sequences, compared to another popular system, Megatron-LM,.
    Future directions include implementing topology-aware P2P operations to further
    reduce training time in lower sequence lengths.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了LightSeq，一个用于长上下文变换器训练的序列并行原型。LightSeq提出了包括因果语言建模的负载平衡、分布式注意力计算中的通信与计算重叠以及一个感知重材料化的检查点策略在内的新型系统优化。我们的实验评估了多个变换器模型系列和不同的集群类型，显示出它相比另一热门系统Megatron-LM，最高实现了2.01$\times$的加速，并且可以扩展到8倍更长的序列。未来的方向包括实现拓扑感知的P2P操作，以进一步减少较短序列的训练时间。
- en: References
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy et al. (2020) Iz Beltagy, Matthew E Peters 和 Arman Cohan。Longformer：长文档变换器。*arXiv
    预印本 arXiv:2004.05150*，2020年。
- en: 'Bian et al. (2021) Yuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan, and
    Kenneth Church. On attention redundancy: A comprehensive study. In *Proceedings
    of the 2021 conference of the north american chapter of the association for computational
    linguistics: human language technologies*, pp.  930–945, 2021.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bian et al. (2021) Yuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan 和 Kenneth
    Church。关于注意力冗余：一项综合研究。发表于*2021年北美计算语言学协会会议：人类语言技术*，第930–945页，2021年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020年。
- en: Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*,
    2016.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang 和 Carlos Guestrin。用亚线性内存成本训练深度网络。*arXiv
    预印本 arXiv:1604.06174*，2016年。
- en: 'Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *arXiv preprint arXiv:2307.08691*, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao (2023) Tri Dao. Flashattention-2: 更快的注意力机制，具有更好的并行性和工作分区。*arXiv 预印本 arXiv:2307.08691*，2023年。'
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra 和 Christopher Ré。Flashattention：快速且内存高效的精确注意力机制，具备I/O感知。*神经信息处理系统进展*，35:16344–16359，2022年。
- en: 'Jain et al. (2020) Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami,
    Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. Checkmate: Breaking
    the memory wall with optimal tensor rematerialization. *Proceedings of Machine
    Learning and Systems*, 2:497–511, 2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain et al. (2020) Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami,
    Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer 和 Ion Stoica。Checkmate：通过最优的张量重材料化突破内存墙。*机器学习与系统会议论文集*，2:497–511，2020年。
- en: Jeaugey (2017) Sylvain Jeaugey. Nccl 2.0. In *GPU Technology Conference (GTC)*,
    volume 2, 2017.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeaugey (2017) Sylvain Jeaugey。Nccl 2.0。发表于*GPU技术大会 (GTC)*，第2卷，2017年。
- en: Korthikanti et al. (2023) Vijay Anand Korthikanti, Jared Casper, Sangkug Lym,
    Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing
    activation recomputation in large transformer models. *Proceedings of Machine
    Learning and Systems*, 5, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korthikanti 等 (2023) Vijay Anand Korthikanti、Jared Casper、Sangkug Lym、Lawrence
    McAfee、Michael Andersch、Mohammad Shoeybi 和 Bryan Catanzaro. 减少大型变换器模型中的激活重新计算。
    *机器学习与系统会议论文集*, 5, 2023。
- en: 'Lefaudeux et al. (2022) Benjamin Lefaudeux, Francisco Massa, Diana Liskovich,
    Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore,
    Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable
    transformer modelling library. [https://github.com/facebookresearch/xformers](https://github.com/facebookresearch/xformers),
    2022.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lefaudeux 等 (2022) Benjamin Lefaudeux、Francisco Massa、Diana Liskovich、Wenhan
    Xiong、Vittorio Caggiano、Sean Naren、Min Xu、Jieru Hu、Marta Tintore、Susan Zhang、Patrick
    Labatut 和 Daniel Haziza. xformers: 一个模块化和可破解的变换器建模库。 [https://github.com/facebookresearch/xformers](https://github.com/facebookresearch/xformers),
    2022。'
- en: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source
    llms truly promise on context length, 2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023) Dacheng Li、Rulin Shao、Anze Xie、Ying Sheng、Lianmin Zheng、Joseph E
    Gonzalez、Ion Stoica、Xuezhe Ma 和 Hao Zhang. 开源 llms 对上下文长度的真正承诺有多长, 2023。
- en: 'Li et al. (2021) Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence
    parallelism: Making 4d parallelism possible. *arXiv preprint arXiv:2105.13120*,
    2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2021) Shenggui Li、Fuzhao Xue、Yongbin Li 和 Yang You. 序列并行: 实现 4d 并行的可能性。
    *arXiv 预印本 arXiv:2105.13120*, 2021。'
- en: Liu & Abbeel (2023) Hao Liu and Pieter Abbeel. Blockwise parallel transformer
    for long context large models. *arXiv preprint arXiv:2305.19370*, 2023.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu & Abbeel (2023) Hao Liu 和 Pieter Abbeel. 针对长上下文大模型的块级并行变换器。 *arXiv 预印本 arXiv:2305.19370*,
    2023。
- en: Liu et al. (2021) Liyuan Liu, Jialu Liu, and Jiawei Han. Multi-head or single-head?
    an empirical comparison for transformer training. *arXiv preprint arXiv:2106.09650*,
    2021.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2021) Liyuan Liu、Jialu Liu 和 Jiawei Han. 多头还是单头？针对变换器训练的经验比较。 *arXiv
    预印本 arXiv:2106.09650*, 2021。
- en: 'Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models
    use long contexts. *arXiv preprint arXiv:2307.03172*, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2023) Nelson F Liu、Kevin Lin、John Hewitt、Ashwin Paranjape、Michele Bevilacqua、Fabio
    Petroni 和 Percy Liang. 迷失在中间: 语言模型如何使用长上下文。 *arXiv 预印本 arXiv:2307.03172*, 2023。'
- en: Milakov & Gimelshein (2018) Maxim Milakov and Natalia Gimelshein. Online normalizer
    calculation for softmax. *arXiv preprint arXiv:1805.02867*, 2018.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Milakov & Gimelshein (2018) Maxim Milakov 和 Natalia Gimelshein. 用于 softmax 的在线归一化计算。
    *arXiv 预印本 arXiv:1805.02867*, 2018。
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large
    language model for code with multi-turn program synthesis. *arXiv preprint arXiv:2203.13474*,
    2022.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp 等 (2022) Erik Nijkamp、Bo Pang、Hiroaki Hayashi、Lifu Tu、Huan Wang、Yingbo
    Zhou、Silvio Savarese 和 Caiming Xiong. Codegen: 一个用于代码的开放大语言模型，支持多轮程序合成。 *arXiv
    预印本 arXiv:2203.13474*, 2022。'
- en: Osika (2023) Anton Osika. gpt-engineer, 2023. URL [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osika (2023) Anton Osika. gpt-engineer, 2023. URL [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer)。
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
    *Advances in neural information processing systems*, 32, 2019.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke 等 (2019) Adam Paszke、Sam Gross、Francisco Massa、Adam Lerer、James Bradbury、Gregory
    Chanan、Trevor Killeen、Zeming Lin、Natalia Gimelshein、Luca Antiga 等. Pytorch: 一种命令式风格的高性能深度学习库。
    *神经信息处理系统进展*, 32, 2019。'
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*, pp.  1–16\. IEEE, 2020.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari 等 (2020) Samyam Rajbhandari、Jeff Rasley、Olatunji Ruwase 和 Yuxiong
    He. Zero: 训练万亿参数模型的内存优化。 在 *SC20: 高性能计算、网络、存储与分析国际会议* 上, 第 1–16 页。IEEE, 2020。'
- en: 'Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion
    parameter language models using model parallelism. *arXiv preprint arXiv:1909.08053*,
    2019.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shoeybi 等 (2019) Mohammad Shoeybi、Mostofa Patwary、Raul Puri、Patrick LeGresley、Jared
    Casper 和 Bryan Catanzaro. Megatron-lm: 使用模型并行训练数十亿参数的语言模型。 *arXiv 预印本 arXiv:1909.08053*,
    2019。'
- en: Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang,
    Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable
    transformer. *arXiv preprint arXiv:2212.10554*, 2022.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2022） Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon
    Benhaim, Vishrav Chaudhary, Xia Song, 和 Furu Wei。一个长度可外推的 Transformer。*arXiv 预印本
    arXiv:2212.10554*，2022。
- en: 'Tillet et al. (2019) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton:
    an intermediate language and compiler for tiled neural network computations. In
    *Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning
    and Programming Languages*, pp.  10–19, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tillet 等（2019） Philippe Tillet, Hsiang-Tsung Kung, 和 David Cox。Triton: 用于分块神经网络计算的中间语言和编译器。在
    *第 3 届 ACM SIGPLAN 国际机器学习与编程语言研讨会论文集*，第 10–19 页，2019。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023） Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等。Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023。'
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*, 2019.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等（2019） Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz
    等。Huggingface 的 transformers: 最先进的自然语言处理。*arXiv 预印本 arXiv:1910.03771*，2019。'
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, et al. Big bird: Transformers for longer sequences. *Advances in neural
    information processing systems*, 33:17283–17297, 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等（2020） Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang 等。Big bird: 长序列的 Transformer。*神经信息处理系统进展*，33:17283–17297，2020。'
- en: 'Zhao et al. (2023) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin
    Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al.
    Pytorch fsdp: experiences on scaling fully sharded data parallel. *arXiv preprint
    arXiv:2304.11277*, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等（2023） Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang,
    Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer 等。Pytorch fsdp:
    扩展完全分片数据并行的经验。*arXiv 预印本 arXiv:2304.11277*，2023。'
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2023） Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph
    E. Gonzalez, 和 Ion Stoica。使用 mt-bench 和 chatbot arena 评估 llm-as-a-judge，2023。
- en: Appendix
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Using DistAttn in LightSeq
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 在 LightSeq 中使用 DistAttn
- en: Algorithm 1 DistAttn in LightSeq (forward pass)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 LightSeq 中的 DistAttn（前向传递）
- en: 1:Matrices $\mathbf{Q}^{p},\mathbf{K}^{p},\mathbf{V}^{p}\in\mathbb{R}^{\frac{N}{\mathbb{P}}\times
    d}$, $B_{r}$, m, causal, last)3:     Divide $q$ blocks $q_{1},\dots,q_{T_{r}}$
    each,4:     and divide $k,v$ blocks $k_{1},\dots,k_{T_{c}}$, of size $B_{c}\times
    d$ into $T_{r}$ of size $B_{r}\times d$ into $T_{r}$ of size $B_{r}$ do7:         
    Load $q_{i}$, $\ell_{i}\in\mathbb{R}^{B_{r}}$ from HBM to on-chip SRAM as $o_{i}^{(0)}$,
    $m_{i}^{(0)}$ do10:              if causal and $i\leq j$ from HBM to on-chip SRAM.14:              
    On chip, compute $s_{i}^{(j)}=q_{i}k^{T}_{j}\in\mathbb{R}^{B_{r}\times B_{c}}$,
    $\tilde{p}_{i}^{(j)}=\exp(S_{i}^{(j)}-m_{i}^{(j)})\in\mathbb{R}^{B_{r}\times B_{c}}$.16:              
    On chip, compute $o_{i}^{(j)}=\mathrm{diag}(e^{m_{i}^{(j-1)}-m_{i}^{(j)}})^{-1}o_{i}^{(j-1)}+\tilde{p}_{i}^{(j)}v^{p}_{j}$.19:         Write
    $o_{i}$-th block of $o$.22:              Write $L_{i}$-th block of $L$.23:         end if24:     end for25:     Return
    $o,\ell,m$ and the logsumexp $L$.26:end function27:Initialize $\mathbf{O}^{p}=(0)_{\frac{N}{\mathbb{P}}\times
    d}\in\mathbb{R}^{\frac{N}{\mathbb{P}}\times d},\ell^{(p)}=(0)_{\frac{N}{\mathbb{P}}}\in\mathbb{R}^{\frac{N}{\mathbb{P}}},m^{p}=(-\infty)_{\frac{N}{\mathbb{P}}}\in\mathbb{R}^{\frac{N}{\mathbb{P}}}$.28:$\mathbf{O}^{p}$,
    $\ell^{p}$, $m^{p}$, $L^{p}$ = standalone_fwd($\mathbf{Q}^{p},\mathbf{K}^{p},\mathbf{V}^{p}$,
    $\mathbf{O}^{p}$, $\ell^{p}$, $m^{p}$, True, p=1)29:for $1\leq r<p$ do30:     
    Receive $\mathbf{K}^{r}$ and $\mathbf{V}^{r}$ from Remote worker $r$ into HBM.31:     $\mathbf{O}^{p}$,
    $\ell^{p}$, $m^{p}$, $L^{p}$ = standalone_fwd($\mathbf{Q}^{p},\mathbf{K}^{y},\mathbf{V}^{y}$,
    $\mathbf{O}^{p}$, $\ell^{p}$, $m^{p}$, False, r=(p-1)32:     Delete $\mathbf{K}^{r}$
    and $\mathbf{V}^{r}$ from HBM.33:end for34:Return the output $\mathbf{O}^{p}$
    and the logsumexp $L$.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 矩阵 $\mathbf{Q}^{p},\mathbf{K}^{p},\mathbf{V}^{p}\in\mathbb{R}^{\frac{N}{\mathbb{P}}\times
    d}$, $B_{r}$, m, causal, last)3:      将 $q$ 块 $q_{1},\dots,q_{T_{r}}$ 每个，4:     
    将 $k,v$ 块 $k_{1},\dots,k_{T_{c}}$，大小为 $B_{c}\times d$ 划分为 $T_{r}$ 个大小为 $B_{r}\times
    d$ 的块，7:          从 HBM 中加载 $q_{i}$, $\ell_{i}\in\mathbb{R}^{B_{r}}$ 到片上 SRAM
    作为 $o_{i}^{(0)}$, $m_{i}^{(0)}$ 进行10:              如果是因果的且 $i\leq j$ 从 HBM 中加载到片上
    SRAM。14:               在芯片上计算 $s_{i}^{(j)}=q_{i}k^{T}_{j}\in\mathbb{R}^{B_{r}\times
    B_{c}}$, $\tilde{p}_{i}^{(j)}=\exp(S_{i}^{(j)}-m_{i}^{(j)})\in\mathbb{R}^{B_{r}\times
    B_{c}}$。16:               在芯片上计算 $o_{i}^{(j)}=\mathrm{diag}(e^{m_{i}^{(j-1)}-m_{i}^{(j)}})^{-1}o_{i}^{(j-1)}+\tilde{p}_{i}^{(j)}v^{p}_{j}$。19:         
    写入 $o_{i}$-th 块的 $o$。22:              写入 $L_{i}$-th 块的 $L$。23:          结束 if24:     
    结束 for25:      返回 $o,\ell,m$ 和 logsumexp $L$。26: 结束函数27: 初始化 $\mathbf{O}^{p}=(0)_{\frac{N}{\mathbb{P}}\times
    d}\in\mathbb{R}^{\frac{N}{\mathbb{P}}\times d},\ell^{(p)}=(0)_{\frac{N}{\mathbb{P}}}\in\mathbb{R}^{\frac{N}{\mathbb{P}}},m^{p}=(-\infty)_{\frac{N}{\mathbb{P}}}\in\mathbb{R}^{\frac{N}{\mathbb{P}}}$。28:
    $\mathbf{O}^{p}$, $\ell^{p}$, $m^{p}$, $L^{p}$ = standalone_fwd($\mathbf{Q}^{p},\mathbf{K}^{p},\mathbf{V}^{p}$,
    $\mathbf{O}^{p}$, $\ell^{p}$, $m^{p}$, True, p=1)29: 对于 $1\leq r<p$ 做30:     
    从远程工作者 $r$ 接收 $\mathbf{K}^{r}$ 和 $\mathbf{V}^{r}$ 到 HBM。31:      $\mathbf{O}^{p}$,
    $\ell^{p}$, $m^{p}$, $L^{p}$ = standalone_fwd($\mathbf{Q}^{p},\mathbf{K}^{y},\mathbf{V}^{y}$,
    $\mathbf{O}^{p}$, $\ell^{p}$, $m^{p}$, False, r=(p-1)32:      从 HBM 中删除 $\mathbf{K}^{r}$
    和 $\mathbf{V}^{r}$。33: 结束 for34: 返回输出 $\mathbf{O}^{p}$ 和 logsumexp $L$。'
- en: 'In this section, we provide more details of DistAttn, and how it can be used
    with the outer LightSeq logic of the forward pass (Alg [1](#alg1 "Algorithm 1
    ‣ Appendix A Using DistAttn in LightSeq ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")). For conceptual simplicity,
    we demonstrate it in the most vanilla version, without the actual scheduling (e.g.
    load balancing and overlapping). We also demonstrate it with the causal language
    modeling objective. The standalone attention is mainly borrowed from the FlashAttention2
    paper (Dao, [2023](#bib.bib5)). To make it compatible with DistAttn, we mainly
    revised the several points:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了更多关于 DistAttn 的细节，以及如何将其与前向传递的外部 LightSeq 逻辑结合使用（算法 [1](#alg1 "Algorithm
    1 ‣ Appendix A Using DistAttn in LightSeq ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")）。为了概念上的简洁性，我们以最基础的版本演示，没有实际调度（例如负载均衡和重叠）。我们还以因果语言建模目标进行了演示。独立的注意力主要借鉴了
    FlashAttention2 论文（Dao，[2023](#bib.bib5)）。为了使其与 DistAttn 兼容，我们主要修订了以下几点：'
- en: '1.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Accumulate results statistics $o$, $m$ and $l$ from previous computation, instead
    of initializing them inside the function.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从先前的计算中累积结果统计 $o$, $m$ 和 $l$，而不是在函数内部初始化它们。
- en: '2.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Pass an extra argument ”last”, which means whether this is the last chunk of
    attention computation. Only when it is true, we compute the logsumexp $L$.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传递额外的参数 "last"，这意味着是否为最后一个注意力计算块。只有当为 true 时，我们才计算 logsumexp $L$。
- en: At a high level, on a worker $p$, LightSeq first initializes local statistics
    $m,l,L$. Then LightSeq loops over all its previous workers. In each iteration,
    it fetches the key and the value from a worker and invokes the revised standalone
    attention to update local statistics. At the end of the iteration, it needs to
    delete the remote key and value from HBM so that the memory does not accumulate.
    At the last iteration of the loop, it additionally calculates the logsumexp according
    to the final $m$ and $l$ (the ”last” variable in the algorithm). At the end of
    the forward pass, worker $p$ has the correct $m,l,L$. The backward pass is similar
    and conceptually simpler because we do not need to keep track of statistics such
    as $m$ and $l$. Instead, we only need to use the logsumexp stored in the forward
    pass.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，对于一个工作节点 $p$，LightSeq 首先初始化本地统计 $m,l,L$。然后 LightSeq 遍历所有之前的工作节点。在每次迭代中，它从一个工作节点获取键和值，并调用修订后的独立注意力来更新本地统计。迭代结束时，需要从
    HBM 中删除远程键和值，以防止内存累积。在循环的最后一次迭代中，它还根据最终的 $m$ 和 $l$（算法中的“最后”变量）计算 logsumexp。在前向传播结束时，工作节点
    $p$ 拥有正确的 $m,l,L$。反向传播类似且概念上更简单，因为我们不需要跟踪 $m$ 和 $l$ 等统计数据。相反，我们只需要使用在前向传播中存储的
    logsumexp。
- en: Appendix B Comparison with DeepSpeed Ulysses
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 与 DeepSpeed Ulysses 的比较
- en: '| Method | # GPUs | Sequence Length | Time | Speedup |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GPU 数量 | 序列长度 | 时间 | 加速比 |'
- en: '|  |  | Per GPU | Total |  |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 每 GPU | 总计 |  |  |'
- en: '| Llama-7B |  |  |  |  |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Llama-7B |  |  |  |  |  |'
- en: '| Megatron-LM | 2x8 | 4K | 64K | 5.29 | 1.0x |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM | 2x8 | 4K | 64K | 5.29 | 1.0x |'
- en: '| 2x8 | 8K | 128K | 14.26 | 1.0x |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 14.26 | 1.0x |'
- en: '| 2x8 | 16K | 256K | 43.44 | 1.0x |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 43.44 | 1.0x |'
- en: '| 2x8 | 32K | 512K | 147.06 | 1.0x |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 147.06 | 1.0x |'
- en: '| DeepSpeed-Ulysses | 2x8 | 4K | 64K | 4.29 | 1.23x |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| DeepSpeed-Ulysses | 2x8 | 4K | 64K | 4.29 | 1.23x |'
- en: '| 2x8 | 8K | 128K | 11.61 | 1.23x |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 11.61 | 1.23x |'
- en: '| 2x8 | 16K | 256K | 37.53 | 1.16x |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 37.53 | 1.16x |'
- en: '| 2x8 | 32K | 512K | 134.09 | 1.10x |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 134.09 | 1.10x |'
- en: '| LightSeq | 2x8 | 4K | 64K | 6.85 | 0.77x |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| LightSeq | 2x8 | 4K | 64K | 6.85 | 0.77x |'
- en: '| 2x8 | 8K | 128K | 12.75 | 1.12x |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 12.75 | 1.12x |'
- en: '| 2x8 | 16K | 256K | 30.21 | 1.44x |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 30.21 | 1.44x |'
- en: '| 2x8 | 32K | 512K | 106.37 | 1.38x |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 106.37 | 1.38x |'
- en: '| Llama-33H |  |  |  |  |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Llama-33H |  |  |  |  |  |'
- en: '| Megatron-LM | 2x8 | 4K | 64K | 7.52 | 1.0x |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM | 2x8 | 4K | 64K | 7.52 | 1.0x |'
- en: '| 2x8 | 8K | 128K | 20.63 | 1.0x |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 20.63 | 1.0x |'
- en: '| 2x8 | 16K | 256K | 62.78 | 1.0x |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 62.78 | 1.0x |'
- en: '| 2x8 | 32K | 512K | 216.70 | 1.0x |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 216.70 | 1.0x |'
- en: '| DeepSpeed-Ulysses | 2x8 | 4K | 64K | 6.42 | 1.17x |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| DeepSpeed-Ulysses | 2x8 | 4K | 64K | 6.42 | 1.17x |'
- en: '| 2x8 | 8K | 128K | 17.47 | 1.18x |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 17.47 | 1.18x |'
- en: '| 2x8 | 16K | 256K | 56.63 | 1.11x |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 56.63 | 1.11x |'
- en: '| 2x8 | 32K | 512K | 202.89 | 1.07x |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 202.89 | 1.07x |'
- en: '| LightSeq | 2x8 | 4K | 64K | 7.03 | 1.07x |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| LightSeq | 2x8 | 4K | 64K | 7.03 | 1.07x |'
- en: '| 2x8 | 8K | 128K | 13.12 | 1.57x |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 13.12 | 1.57x |'
- en: '| 2x8 | 16K | 256K | 31.33 | 2.00x |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 31.33 | 2.00x |'
- en: '| 2x8 | 32K | 512K | 107.76 | 2.01x |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 107.76 | 2.01x |'
- en: 'Table 4: Per iteration wall-clock time of LightSeq, Megatron-LM (Korthikanti
    et al., [2023](#bib.bib9)) and DeepSpeed Ulysses (Unit: seconds). Speedup in bold
    denotes the better of the three systems. We calculate the speedup based on Megatron-LM
    iteration time.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: LightSeq、Megatron-LM (Korthikanti 等，[2023](#bib.bib9)) 和 DeepSpeed Ulysses
    的每次迭代墙钟时间（单位：秒）。加速比用粗体表示，表示三种系统中的最佳。我们根据 Megatron-LM 的迭代时间计算加速比。'
- en: 'We run a subset of the experiments compared with DeepSpeed-Ulysses. Firstly,
    DeepSpeed-Ulysses does reduce the communication overhead, and thus better than
    Megatron-LM on scenarios listed in Table [4](#A2.T4 "Table 4 ‣ Appendix B Comparison
    with DeepSpeed Ulysses ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers").  LightSeq achieves better performance
    than DeepSpeed-Ulysses on longer sequences or models with a more general number
    of heads (e.g. Llama-33H). We also note that DeepSpeed-Ulysses can not scale beyond
    the number of attention heads because it also relies on sharding the attention
    heads. However, we need to point out that in shorter sequences and MHA models
    (where  LightSeq does not have a communication advantage, compared to GQA/MQA
    models), the communication primitives used in DeepSpeed-Ulysses are more advantageous.
    We leave our further optimization in P2P in shorter sequences and MHA models as
    an exciting future work.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对比了部分实验与 DeepSpeed-Ulysses。首先，DeepSpeed-Ulysses 确实减少了通信开销，因此在表格 [4](#A2.T4
    "Table 4 ‣ Appendix B Comparison with DeepSpeed Ulysses ‣ LightSeq: Sequence Level
    Parallelism for Distributed Training of Long Context Transformers") 中列出的场景下比 Megatron-LM
    更好。LightSeq 在较长的序列或具有更多头数（例如 Llama-33H）的模型上表现优于 DeepSpeed-Ulysses。我们还注意到，DeepSpeed-Ulysses
    无法超出注意力头的数量进行扩展，因为它也依赖于对注意力头的分片。然而，我们需要指出，在较短序列和 MHA 模型中（与 GQA/MQA 模型相比，LightSeq
    在这方面没有通信优势），DeepSpeed-Ulysses 使用的通信原语更具优势。我们将进一步优化 P2P 在较短序列和 MHA 模型中的应用作为一个令人兴奋的未来工作。'
