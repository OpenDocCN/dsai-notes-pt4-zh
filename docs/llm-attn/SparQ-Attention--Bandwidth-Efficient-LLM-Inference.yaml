- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:03:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:03:04'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SparQ Attention: Bandwidth-Efficient LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'SparQ Attention: Bandwidth-Efficient LLM Inference'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.04985](https://ar5iv.labs.arxiv.org/html/2312.04985)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.04985](https://ar5iv.labs.arxiv.org/html/2312.04985)
- en: Luka Ribar       Ivan Chelombiev       Luke Hudlass-Galley       Charlie Blake
          Carlo Luschi       Douglas Orr
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Luka Ribar       Ivan Chelombiev       Luke Hudlass-Galley       Charlie Blake
          Carlo Luschi       Douglas Orr
- en: Graphcore Research
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Graphcore 研究
- en: '{lukar, ivanc, lukehg, charlieb, carlo, douglaso}@graphcore.ai'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{lukar, ivanc, lukehg, charlieb, carlo, douglaso}@graphcore.ai'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Generative large language models (LLMs) have opened up numerous novel possibilities,
    but due to their significant computational requirements their ubiquitous use remains
    challenging. Some of the most useful applications require processing large numbers
    of samples at a time and using long contexts, both significantly increasing the
    memory communication load of the models. We introduce SparQ Attention, a technique
    for increasing the inference throughput of LLMs by reducing the memory bandwidth
    requirements within the attention blocks through selective fetching of the cached
    history. Our proposed technique can be applied directly to off-the-shelf LLMs
    during inference, without requiring any modification to the pre-training setup
    or additional fine-tuning. We show how SparQ Attention can decrease the attention
    memory bandwidth requirements *up to eight times* without any loss in accuracy
    by evaluating Llama $2$ and Pythia models on a wide range of downstream tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生成大型语言模型（LLMs）开辟了许多新颖的可能性，但由于其显著的计算需求，其普遍使用仍然具有挑战性。一些最有用的应用需要同时处理大量样本和使用长上下文，这两者都会显著增加模型的内存通信负载。我们介绍了
    SparQ Attention，这是一种通过选择性地提取缓存历史来减少注意力块内存带宽需求的技术，从而提高 LLM 的推理吞吐量。我们提出的技术可以直接应用于现成的
    LLM 推理中，而无需对预训练设置进行任何修改或额外的微调。我们通过在各种下游任务上评估 Llama $2$ 和 Pythia 模型，展示了 SparQ Attention
    如何在不损失准确性的情况下将注意力内存带宽需求 *降低至八倍*。
- en: $\bm{q}_{[{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$$-0.2$$0.4$$\bm{K}_{[:,{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$$\bm{q}$$-0.2$$0.4$$\bm{K}_{[{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}},:]}$$\bm{\otimes}$$\alpha=\displaystyle\sum_{\mathclap{i\in{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}}}}\hat{s}_{i}$$\bm{\oplus}$$\times\,(1-\alpha)$
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: $\bm{q}_{[{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$$-0.2$$0.4$$\bm{K}_{[:,{\color[rgb]{0.6015625,0.1484375,0.1796875}\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.1484375,0.1796875}\bm{i}_{1}}]}$$\bm{q}$$-0.2$$0.4$$\bm{K}_{[{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}},:]}$$\bm{\otimes}$$\alpha=\displaystyle\sum_{\mathclap{i\in{\color[rgb]{0.1484375,0.3515625,0.6015625}\definecolor[named]{pgfstrokecolor}{rgb}{0.1484375,0.3515625,0.6015625}\bm{i}_{2}}}}\hat{s}_{i}$$\bm{\oplus}$$\times\,(1-\alpha)$
- en: 'Figure 1: SparQ Attention for a single attention head. The algorithm consists
    of three steps. First, we find the $r$. This allows us to approximate the full
    attention scores ($\bm{\hat{s}}$ largest scores in the approximation and proceed
    to gather the corresponding full key and value vectors from the cache. As a final
    step, to compensate for the missing value vectors, we additionally maintain and
    fetch the running mean value vector and reassign it the leftover mass based on
    approximate score weightings. The attention output is then calculated as usual
    using the top-$k$ fetched key and value pairs, together with the averaged vector.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：单个注意力头的 SparQ Attention。该算法包括三个步骤。首先，我们找到 $r$。这使我们能够近似全注意力分数（在近似中最大的 $\bm{\hat{s}}$
    分数），并从缓存中获取相应的全键和值向量。最后一步，为了补偿缺失的值向量，我们还维护并提取运行均值向量，并根据近似分数权重重新分配剩余的质量。然后，使用获取的
    top-$k$ 键值对以及平均向量，按常规计算注意力输出。
- en: '![Refer to caption](img/67e1294eb2b06085ae10cbd633606dab.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/67e1294eb2b06085ae10cbd633606dab.png)'
- en: 'Figure 2: Llama $2$-shot performance versus memory transfers over a range of
    compression ratios. SparQ Attention achieves matching performance, while transferring
    between $1/8$ as much data as the original dense model. The curves show the mean
    $\pm$ examples. This pattern is representative of the performance across multiple
    models and tasks, shown in [Figure A1](#A0.F1 "In SparQ Attention: Bandwidth-Efficient
    LLM Inference").'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：Llama $2$-shot 性能与不同压缩比下的内存传输。SparQ Attention 实现了匹配的性能，同时传输的数据量仅为原始稠密模型的
    $1/8$。曲线显示了均值 $\pm$ 示例。这种模式代表了多个模型和任务的性能，见 [图 A1](#A0.F1 "In SparQ Attention:
    Bandwidth-Efficient LLM Inference")。'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transformer models trained on large corpora of text have recently shown remarkable
    capabilities in solving complex natural language processing tasks [Kaplan et al.,
    [2020](#bib.bib10), Brown et al., [2020](#bib.bib5)]. With the dramatic increase
    in model scale, LLMs became useful generalist tools that can be used for a multitude
    of text-based tasks due to in-context learning capabilities that emerge in LLMs
    at scale. These capabilities are unlocked by incorporating information and instructions
    through textual prompts [Wei et al., [2022](#bib.bib29)], which are absorbed during
    generation as part of the attention cache without modifying the model weights.
    Therefore, one of the key obstacles to efficient inference deployment of LLMs
    remains their high memory bandwidth requirement when processing long sequences
    [Pope et al., [2023](#bib.bib17)], i.e. long instruction prompts, lengthy chat
    histories or information retrieval from documents.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量文本语料上训练的变换器模型最近在解决复杂的自然语言处理任务方面展示了显著的能力 [Kaplan et al., [2020](#bib.bib10),
    Brown et al., [2020](#bib.bib5)]。随着模型规模的戏剧性增长，LLMs 变成了有用的通用工具，可以用于多种基于文本的任务，这得益于在大规模
    LLM 中出现的上下文学习能力。这些能力通过通过文本提示 [Wei et al., [2022](#bib.bib29)] 融入信息和指令来解锁，这些信息在生成过程中作为注意力缓存的一部分被吸收，而不修改模型权重。因此，实现
    LLM 高效推理部署的关键障碍之一仍然是处理长序列时其高内存带宽需求 [Pope et al., [2023](#bib.bib17)]，即长指令提示、冗长的聊天记录或从文档中检索信息。
- en: 'The main bottleneck appears due to the auto-regressive nature of transformer
    inference: the output is generated token by token, and for each model call, the
    full previous state (i.e. the *key-value cache*, or KV cache) needs to be fetched
    from memory. The size of the KV cache scales linearly with the sequence length
    as well as the batch size, thus rendering inference using long sequence lengths
    increasingly memory-bandwidth limited.¹¹1Note that this problem is less severe
    during training, since teacher forcing allows whole sequences to be processed
    in parallel.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 主要瓶颈出现在变换器推理的自回归特性上：输出是逐个生成的，对于每次模型调用，需要从内存中获取完整的先前状态（即*键值缓存*或KV缓存）。KV缓存的大小随着序列长度和批量大小线性增长，因此使用长序列长度的推理越来越受限于内存带宽¹¹1请注意，这个问题在训练期间不那么严重，因为教师强制可以并行处理整个序列。
- en: However, tokens generally only attend to a small part of the sequence at a time
    [Vig, [2019](#bib.bib28), Yun et al., [2020](#bib.bib30)]; thus, if it were possible
    to efficiently predict the tokens that will have high attention scores, memory
    traffic could be significantly reduced by only transferring the key and value
    pairs of high-scoring tokens. Building upon this idea, we present SparQ Attention,
    a technique for significantly reducing the memory bandwidth requirements of transformer
    inference. By approximating attention scores using a subset of query/key vector
    components, we fetch only the most relevant tokens for each inference step, greatly
    decreasing memory traffic without affecting task performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，tokens 通常一次只关注序列的一小部分 [Vig, [2019](#bib.bib28), Yun et al., [2020](#bib.bib30)]；因此，如果能够高效预测将具有高注意力分数的tokens，通过仅传输高分数tokens的键值对，可以显著减少内存流量。基于这个想法，我们提出了
    SparQ Attention 一种显著减少变换器推理内存带宽需求的技术。通过使用查询/键向量组件的子集来近似注意力分数，我们仅获取每次推理步骤中最相关的
    tokens，从而大大减少内存流量，而不影响任务性能。
- en: In order to evaluate the effectiveness of our technique, we curate a selection
    of downstream tasks that aim to test the model’s abilities to effectively utilise
    the information within the provided textual context. This setup allows us to evaluate
    the trade-off between task performance and data transfer reduction, as well as
    compare different sparse attention techniques with respect to memory transfer
    efficiency. Thus, we clearly show how SparQ Attention performs favourably compared
    to state of the art and can lead up to $8\times$ compression with no loss in accuracy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们技术的有效性，我们策划了一系列下游任务，旨在测试模型有效利用所提供文本上下文中的信息的能力。这种设置使我们能够评估任务性能与数据传输减少之间的权衡，以及比较不同稀疏注意力技术在内存传输效率方面的表现。因此，我们清晰地展示了
    SparQ Attention 相比于最先进技术的优越表现，并且在不损失准确性的情况下能够实现高达 $8\times$ 的压缩。
- en: 2 Attention Memory Transfer
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 注意力内存转移
- en: 'Consider a standard attention block and its associated memory requirements.
    A single head within an attention layer has a head dimension $d_{h}$. During token-by-token
    inference the output of an attention head is calculated as:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个标准的注意力块及其相关的内存需求。一个注意力层中的单个头有一个头维度 $d_{h}$。在逐个令牌推理期间，注意力头的输出计算如下：
- en: '|  | $\bm{y}=\mathrm{softmax}\Bigl{(}\dfrac{\bm{q}\cdot\bm{K}^{\top}}{\sqrt{d_{h}}}\Bigr{)}\cdot\bm{V}$
    |  | (1) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{y}=\mathrm{softmax}\Bigl{(}\dfrac{\bm{q}\cdot\bm{K}^{\top}}{\sqrt{d_{h}}}\Bigr{)}\cdot\bm{V}$
    |  | (1) |'
- en: where $\bm{q}$ and $\bm{V}\in\mathbb{R}^{S\times d_{h}}$ are the key and value
    caches respectively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{q}$ 和 $\bm{V}\in\mathbb{R}^{S\times d_{h}}$ 分别是键和值缓存。
- en: 'For each forward pass, we need to fetch the key and value matrices from memory,
    as well as write (append) $\bm{k}$ vectors for the current token, giving a total
    number of elements transferred per attention head:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次前向传递，我们需要从内存中提取键和值矩阵，并为当前令牌写入（追加）$\bm{k}$ 向量，计算每个注意力头传输的总元素数量：
- en: '|  | $\mathcal{M}_{\mathrm{base}}=\underbrace{2\,S\,d_{h}}_{\text{Read $\bm{K}$
    and $\bm{V}$}}+\underbrace{2\,d_{h}}_{\text{Write current $\bm{k}$ and $\bm{v}$}}$
    |  | (2) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}_{\mathrm{base}}=\underbrace{2\,S\,d_{h}}_{\text{读取 $\bm{K}$
    和 $\bm{V}$}}+\underbrace{2\,d_{h}}_{\text{写入当前 $\bm{k}$ 和 $\bm{v}$}}$ |  | (2)
    |'
- en: Total memory transfers for inference also include model parameters, however
    these can be amortised over a large batch of sequences, unlike the $\bm{K}$ caches.
    Fetching these from memory dominates processing time for large $S$, thus creating
    a bottleneck to efficient transformer inference.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 推理的总内存传输还包括模型参数，但这些参数可以在大批量序列中分摊，这与 $\bm{K}$ 缓存不同。从内存中提取这些数据在较大的 $S$ 下主导了处理时间，从而成为高效变换器推理的瓶颈。
- en: 3 SparQ Attention
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 SparQ 注意力
- en: Algorithm 1 SparQ Attention Algorithm
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 SparQ 注意力算法
- en: procedure SparseQAttention($\bm{q}\in\mathbb{R}^{d_{h}},\bm{K}\in\mathbb{R}^{S\times
    d_{h}},\bm{V}\in\mathbb{R}^{S\times d_{h}},\bm{\overline{v}}\in\mathbb{R}^{d_{h}},r\in\mathbb{N},k\in\mathbb{N},l\in\mathbb{N}$
    $\triangleright$ elements of $\bm{|q|}$ $\triangleright$ if $i> $ .'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '为了测试得分是否足够稀疏，以便在$\bm{V}$得分中丢弃位置，这称为覆盖率，在[图 A2](#A4.F2 "在附录 D 注意力稀疏性分析 ‣ SparQ
    Attention: Bandwidth-Efficient LLM Inference")中进行。如果注意力得分是平坦的，我们会期望覆盖率 $\approx
    k/S\approx 0.03$，这意味着非常稀疏的分布。我们从[图 A2b](#A4.F2.sf2 "在图 A2 ‣ 附录 D 注意力稀疏性分析 ‣ SparQ
    Attention: Bandwidth-Efficient LLM Inference")中注意到，覆盖率在每个头和每层之间确实有所不同，例如，较早的层具有“更平坦”的头，但即便这些头也显示出覆盖率
    。'
- en: '![Refer to caption](img/d8aa94c7736ec94b3035ddd125506c96.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d8aa94c7736ec94b3035ddd125506c96.png)'
- en: (a) Coverage over (example, layer, head)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 按（示例，层，头）的覆盖率
- en: '![Refer to caption](img/54f4c3b9b0c2d70d20fc8e2d6a86cfe1.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54f4c3b9b0c2d70d20fc8e2d6a86cfe1.png)'
- en: (b) Coverage by (layer, head)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 按（层，头）的覆盖率
- en: 'Figure A2: Coverage of the softmax output for the top $32$ (varying across
    examples). Calculated for the first token generated in answer to a SQuAD prompt,
    using Pythia-$1.4$ examples.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A2：top $32$ 的 softmax 输出覆盖率（在示例之间变化）。计算为回答 SQuAD 提示生成的第一个 token，使用 Pythia-$1.4$
    示例。
- en: '![Refer to caption](img/c4be75b79bc93a1585d3a33f1f01fed2.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c4be75b79bc93a1585d3a33f1f01fed2.png)'
- en: (a) Attention approximation top-$k$ overlap, over (batch, layer, head)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力近似 top-$k$ 重叠，按（批次，层，头）
- en: '![Refer to caption](img/ca229b1727d8e8f7ec2d5666feb1aa40.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca229b1727d8e8f7ec2d5666feb1aa40.png)'
- en: (b) Attention approximation top-$k$ overlap, by (layer, head)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力近似 top-$k$ 重叠，按（层，头）
- en: 'Figure A3: Proportion of the top-$k$ absolute components of $\bm{q}$ components.
    Note $d_{h}=128$. Calculated for the first token generated to answer a SQuAD prompt,
    using Pythia-$1.4$ examples.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A3：$\bm{q}$ 组件的 top-$k$ 绝对分量的比例。注意 $d_{h}=128$。计算为回答 SQuAD 提示生成的第一个 token，使用
    Pythia-$1.4$ 示例。
- en: It is useful to drop positions in $\bm{V}$ is needed to calculate these scores.
    We propose approximating these scores using a subset of the components of $\bm{K}$
    positions in the approximated and true scores. If overlap is high, we can use
    the approximation to avoid downloading the whole $\bm{K}$ for all positions, then
    all components of $\bm{K}$ for some positions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在$\bm{V}$中丢弃位置对计算这些得分是有用的。我们建议使用$\bm{K}$中位置的组件子集来近似这些得分。如果重叠度较高，我们可以使用近似值来避免下载所有位置的整个$\bm{K}$，然后仅对某些位置下载$\bm{K}$的所有组件。
- en: Our hypothesis is that the $r$ are most useful to predicting the score, $\bm{q}\bm{K}^{\top}$,
    but that some later layers are harder to predict. Using the top-$r$ baseline considerably.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的假设是$r$对于预测得分$\bm{q}\bm{K}^{\top}$最有用，但一些较晚的层更难预测。使用 top-$r$ 基线显著。
- en: Appendix E Practical Implementation Considerations
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 实际实现注意事项
- en: One limitation of our theoretical model of memory transfers is that it does
    not account for the granularity of memory access. Since the $\bm{K}$ twice, once
    in $S$-major format. This increases KV cache memory usage by $50\%$ twice. This
    extra write is non-contiguous, but small, so should not form a bottleneck.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理论模型的一个限制是没有考虑内存访问的粒度。由于 $\bm{K}$ 两次，第一次是 $S$-major 格式。这会使 KV 缓存内存使用量增加 $50\%$
    两次。这个额外的写入是非连续的，但很小，因此不应形成瓶颈。
- en: Appendix F Methodology
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 方法论
- en: 'We provide a comprehensive set of hyperparameters for reference in [Table A1](#A6.T1
    "In Appendix F Methodology ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").
    Typical compression ratios for settings of $(r,k)$ are given in [Table A2](#A6.T2
    "In Appendix F Methodology ‣ SparQ Attention: Bandwidth-Efficient LLM Inference").'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[表 A1](#A6.T1 "在附录 F 方法论 ‣ SparQ Attention: 带宽高效的 LLM 推理")中提供了一组全面的超参数供参考。典型的压缩比对于$(r,k)$的设置见[表
    A2](#A6.T2 "在附录 F 方法论 ‣ SparQ Attention: 带宽高效的 LLM 推理")。'
- en: We use our own implementation of H[2]O [Zhang et al., [2023](#bib.bib32)], which
    differs from the author’s implementation in that it uses a fixed cache size $k$-shot,
    with Pythia-$1.4$, $l=64$ of $200$ (the dense baseline achieved $74$ times that
    either output differed from dense, $41$-character prefix match between our implementation
    and theirs. The fact that the two implementations often generate the same errors
    (despite minor implementation differences) reassures us that our results should
    be a fair representation of $\mathrm{H_{2}O}$.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了我们自己实现的 H[2]O [Zhang et al., [2023](#bib.bib32)]，与作者的实现不同的是，它使用了固定的缓存大小
    $k$-shot，与 Pythia-$1.4$, $l=64$ 的 $200$（密集基线达到了 $74$ 倍，其输出与密集型有所不同，$41$ 字符前缀匹配在我们的实现和他们之间。两个实现经常产生相同的错误（尽管有些微的实现差异），这让我们确信我们的结果应该公平地代表
    $\mathrm{H_{2}O}$。
- en: '| Group | Hyperparameter | Value or range |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 组 | 超参数 | 值或范围 |'
- en: '| Dense model | Family | Llama $2$B), Pythia ($6.9$B, $1.4$B) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 密集模型 | 家族 | Llama $2$B), Pythia ($6.9$B, $1.4$B) |'
- en: '| $d_{h}$ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| $d_{h}$ |'
- en: '| Max $S$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 最大 $S$ |'
- en: '| Tasks | QA (#samples) | SQuAD $1$) TriviaQA $0$) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | QA (#样本) | SQuAD $1$) TriviaQA $0$) |'
- en: '| Summarisation (#samples) | CNN/DailyMail $0$) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 摘要（#样本） | CNN/DailyMail $0$) |'
- en: '| Language Modelling (#samples) | WikiText-$103$) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 语言建模（#样本） | WikiText-$103$) |'
- en: '| Artificial (#samples) | Repetition ($1000$) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 人工（#样本） | 重复 ($1000$) |'
- en: '| Baselines | Eviction | keep $(k-l)$ and the most recent $l=k/4$'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '| 基线 | 驱逐 | 保留 $(k-l)$ 和最新的 $l=k/4$'
- en: $k\in\{128,192,256,384,512,768\}$ |
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: $k\in\{128,192,256,384,512,768\}$ |
- en: '| Local | take the first $16$ $k\in\{128,192,256,384,512,768\}$ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 局部 | 取前 $16$ $k\in\{128,192,256,384,512,768\}$ |'
- en: '| SparQ | Rank $r$ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| SparQ | 排名 $r$ |'
- en: '| Number of values $k$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 值的数量 $k$ |'
- en: '| Local window $l$ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 局部窗口 $l$ |'
- en: 'Table A1: Experiment hyperparameters'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A1: 实验超参数'
- en: '| Method | $r$ | Compression |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $r$ | 压缩 |'
- en: '| SparQ Attention | 16 | 64 | 0.10-0.12 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| SparQ Attention | 16 | 64 | 0.10-0.12 |'
- en: '| 128 | 0.13-0.17 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0.13-0.17 |'
- en: '| 32 | 64 | 0.16-0.18 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64 | 0.16-0.18 |'
- en: '| 128 | 0.19-0.23 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0.19-0.23 |'
- en: '| 64 | 64 | 0.28-0.30 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 64 | 0.28-0.30 |'
- en: '| 128 | 0.31-0.36 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0.31-0.36 |'
- en: '| H[2]O | – | 128 | 0.07-0.11 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| H[2]O | – | 128 | 0.07-0.11 |'
- en: '| 192 | 0.10-0.17 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 192 | 0.10-0.17 |'
- en: '| 256 | 0.13-0.22 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 0.13-0.22 |'
- en: '| 384 | 0.20-0.33 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 384 | 0.20-0.33 |'
- en: '| 512 | 0.26-0.43 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 0.26-0.43 |'
- en: '| 768 | 0.39-0.65 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 768 | 0.39-0.65 |'
- en: 'Table A2: Range of compression ratios for different settings of $(r,k)$, for
    Llama 2 7B and Pythia 6.9B. The compression ratio achieved varies across models
    and tasks, based on the sequence length and head size.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A2: 不同$(r,k)$设置的压缩比范围，适用于 Llama 2 7B 和 Pythia 6.9B。实现的压缩比因模型和任务而异，取决于序列长度和头大小。'
- en: F.1 Examples
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 示例
- en: We illustrate the task setup with a single example per task, showing the prompt
    formatting and a cherry-picked example. In each case, we show outputs from a dense
    Llama 2 model, SparQ ($r=16,k=64$) Attention. Where “…” appears, we have truncated
    the line of text for brevity.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过每个任务的单个示例来说明任务设置，展示了提示格式和精心挑选的示例。在每种情况下，我们展示了来自密集型 Llama 2 模型的输出，SparQ ($r=16,k=64$)
    Attention。出现“...”的地方，我们为了简洁起见已截断文本行。
- en: F.1.1 Question Answering (SQuAD 1-shot)
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.1 问答（SQuAD 1-shot）
- en: '[PRE0]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: F.1.2 Question Answering (TriviaQA 0-shot)
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.2 问答（TriviaQA 0-shot）
- en: '[PRE1]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that for Pythia, the prompt “Single-word answer:” was used in place of
    “Answer:”, as this helped prevent the model from restating the question in the
    answer (often qualitatively correct, but not a regex match).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于 Pythia，提示“单词回答:”被用来代替“回答:”，因为这有助于防止模型在答案中重复问题（尽管通常是定性的正确，但不符合正则表达式匹配）。
- en: F.1.3 Summarisation (CNN/DailyMail)
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.3 摘要（CNN/每日邮报）
- en: '[PRE2]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: F.1.4 Repetition (Shakespeare)
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.4 重复（莎士比亚）
- en: '[PRE3]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: F.1.5 Language Modelling (WikiText-103)
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.1.5 语言建模（WikiText-103）
- en: '[PRE4]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since the flood a new , roughly 100 @-@ foot @-@ tall ( 30 m ) bridge has been
    constructed on CR 510 , ensuring that any future flooding on the Dead River would
    not necessitate closure of the modern bridge . This structure was built at that
    height in order to benefit commercial interests in the area , as well as to provide
    the area with reliable public and emergency access . Rio Tinto Group , the British
    @-@ based parent company of Kennecott Minerals received permission
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 自从洪水之后，在CR 510上建造了一座新的、约100英尺（30米）高的桥梁，确保未来任何在死亡河上的洪水都不会导致现代桥梁关闭。这座结构的建设高度考虑到了该地区的商业利益，并为该地区提供了可靠的公共和紧急通道。英国总部的Rio
    Tinto Group，作为Kennecott Minerals的母公司，获得了许可。
- en: '[PRE5]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
