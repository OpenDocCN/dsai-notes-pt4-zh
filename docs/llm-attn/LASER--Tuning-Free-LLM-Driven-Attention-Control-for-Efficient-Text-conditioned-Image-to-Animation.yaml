- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LASER: 无需调优的 LLM 驱动注意力控制以实现高效的文本条件图像到动画'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13558](https://ar5iv.labs.arxiv.org/html/2404.13558)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13558](https://ar5iv.labs.arxiv.org/html/2404.13558)
- en: \useunder
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: \ul \useunder\ul
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \ul \useunder\ul
- en: Haoyu Zheng   Wenqiao Zhang   Yaoke Wang   Hao Zhou   Jiang Liu  and  Juncheng
    Li   Zheqi Lv   Siliang Tang   Yueting Zhuang [zhenghaoyu404@gmail.com, wenqiaozhang@zju.edu.cn,
    wangyaoke@zju.edu.cn, 2021210665@stu.hit.edu.cn](mailto:zhenghaoyu404@gmail.com,%20wenqiaozhang@zju.edu.cn,%20wangyaoke@zju.edu.cn,%202021210665@stu.hit.edu.cn)
    [20201785@stu.cqu.edu.cn, junchengli@zju.edu.cn, zheqilv@zju.edu.cn, siliang@zju.edu.cn,
    yzhuang@zju.edu.cn](mailto:20201785@stu.cqu.edu.cn,%20junchengli@zju.edu.cn,%20zheqilv@zju.edu.cn,%20siliang@zju.edu.cn,%20yzhuang@zju.edu.cn)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Haoyu Zheng   Wenqiao Zhang   Yaoke Wang   Hao Zhou   Jiang Liu  和  Juncheng
    Li   Zheqi Lv   Siliang Tang   Yueting Zhuang [zhenghaoyu404@gmail.com, wenqiaozhang@zju.edu.cn,
    wangyaoke@zju.edu.cn, 2021210665@stu.hit.edu.cn](mailto:zhenghaoyu404@gmail.com,%20wenqiaozhang@zju.edu.cn,%20wangyaoke@zju.edu.cn,%202021210665@stu.hit.edu.cn)
    [20201785@stu.cqu.edu.cn, junchengli@zju.edu.cn, zheqilv@zju.edu.cn, siliang@zju.edu.cn,
    yzhuang@zju.edu.cn](mailto:20201785@stu.cqu.edu.cn,%20junchengli@zju.edu.cn,%20zheqilv@zju.edu.cn,%20siliang@zju.edu.cn,%20yzhuang@zju.edu.cn)
- en: Abstract.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Revolutionary advancements in text-to-image models have unlocked new dimensions
    for sophisticated content creation, *e.g.,*, text-conditioned image editing, allowing
    us to edit the diverse images that convey highly complex visual concepts according
    to the textual guidance. Despite promising, existing methods focus on texture-
    or non-rigid-based visual manipulation, which struggles to produce the fine-grained
    animation of the smooth text-conditioned image morphing without fine-tuning, *i.e.*,
    due to their highly unstructured latent space. In this paper, we introduce a tuning-free
    LLM-driven attention control framework, encapsulated by the progressive process
    of LLM planning $\rightarrow$ prompt-Aware editing $\rightarrow$ StablE animation
    geneRation, abbreviated as LASER: i) Given a general and coarse-grained description
    for image editing, LASER initially employs the large language model (LLM) that
    refines it into consistent and fine-grained prompts, which serve as linguistic
    guidance for the pre-trained text-to-image models in subsequent image generation;
    ii) We manipulate the model’s spatial features and self-attention mechanisms to
    maintain animation integrity and enable seamless image morphing directly from
    text prompts, obviating the need for annotations for additional fine-tuning; iii)
    Our meticulous control of spatial features and self-attention within the model
    ensures the preservation of structural consistency in the images. This paper introduces
    a novel framework that integrates large language models (LLM) with pre-trained
    text-to-image models to create high-quality image translation animations from
    just one input text. The proposed LASER introduces a novel tuning-free framework
    that integrates LLM with pre-trained text-to-image models to facilitate high-quality
    text-conditioned image-to-animation translation. In addition, we propose a Text-conditioned
    Image-to-Animation Benchmark to validate the effectiveness and efficacy of the
    proposed LASER. Extensive experiments show that the proposed LASER produces impressive
    results in both consistent and efficient animation generation, positioning it
    as a powerful tool for producing detailed animations and opening new avenues in
    digital content creation.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像模型的革命性进展为复杂内容创作解锁了新维度，*例如*，文本条件的图像编辑，使我们能够根据文本指导编辑传达高度复杂视觉概念的多样化图像。尽管前景光明，现有方法专注于纹理或非刚性视觉操作，难以在不进行微调的情况下生成细致的平滑文本条件图像变形动画，*即*，由于其高度非结构化的潜在空间。本文介绍了一种无调优的LLM驱动的注意力控制框架，由LLM规划
    $\rightarrow$ prompt-Aware 编辑 $\rightarrow$ StablE 动画生成的渐进过程 encapsulated，简称LASER：i)
    给定图像编辑的一般且粗略的描述，LASER最初采用大型语言模型（LLM），将其细化为一致且细致的提示，这些提示作为后续图像生成中的预训练文本到图像模型的语言指导；ii)
    我们操控模型的空间特征和自注意力机制，以保持动画完整性，并直接从文本提示中实现无缝图像变形，无需额外的微调注释；iii) 我们对模型中空间特征和自注意力的细致控制确保了图像结构的一致性。本文介绍了一种新颖的框架，将大型语言模型（LLM）与预训练的文本到图像模型结合，从单一输入文本中创建高质量的图像翻译动画。提出的LASER引入了一种新颖的无调优框架，将LLM与预训练文本到图像模型结合，以促进高质量的文本条件图像到动画翻译。此外，我们提出了一个文本条件图像到动画基准，以验证提出的LASER的有效性和效率。大量实验表明，提出的LASER在一致和高效的动画生成方面产生了令人印象深刻的结果，将其定位为生成详细动画的强大工具，并为数字内容创作开辟了新途径。
- en: 'Multimodal, Diffusion Model, Large Language Model^†^†ccs: Computing methodologies Computer
    vision![Refer to caption](img/bf73944f6263e9cd350d27daeefe747b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '多模态，扩散模型，大型语言模型^†^†ccs: 计算方法 计算机视觉![参考说明](img/bf73944f6263e9cd350d27daeefe747b.png)'
- en: Figure 1\. Given multimodal inputs (image and textual guidance), our method
    is capable of guiding the generation of smooth animations based on textual content.
    The first row shows the combined texture and non-rigid changes; the second row,
    only the texture changes; the third row, purely non-rigid transformations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 给定多模态输入（图像和文本指导），我们的方法能够基于文本内容引导生成平滑的动画。第一行显示了纹理和非刚性变化的组合；第二行，仅纹理变化；第三行，纯非刚性变换。
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 'Diffusion models (Dhariwal and Nichol, [2021](#bib.bib9); Ho et al., [2020](#bib.bib13);
    Nichol and Dhariwal, [2021](#bib.bib25)) form a category of deep generative models
    that has recently become one of the hottest topics in multimodal intelligence,
    showcasing impressive capabilities of text-to-image (T2I) generation, ranging
    from the high level of details to the diversity of the generated examples. Such
    diffusion models also unlock a new world of creative processes in content creation,
    *e.g.*, text-guided image editing (Brooks et al., [2023](#bib.bib6); Cao et al.,
    [2023](#bib.bib7); Hertz et al., [2022](#bib.bib11)), involves editing the diverse
    images that convey highly complex visual concepts with text-to-image models solely
    through the textual guidance. Broadly, the contemporary image editing paradigm
    can be summarized in two aspects: i) *Texture editing* (Brooks et al., [2023](#bib.bib6);
    Cao et al., [2023](#bib.bib7); Hertz et al., [2022](#bib.bib11)), manipulating
    a given image’s stylization and appearance while maintaining the input structure
    and scene layout; ii) *Non-rigid Editing* (Cao et al., [2023](#bib.bib7); Kawar
    et al., [2023](#bib.bib19)), enabling non-rigid image editing (*e.g.,* posture
    changes) while preserving its original characteristics.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型（Dhariwal 和 Nichol，[2021](#bib.bib9)；Ho 等，[2020](#bib.bib13)；Nichol 和 Dhariwal，[2021](#bib.bib25)）形成了一类深度生成模型，近年来已成为多模态智能中最热门的话题之一，展示了从高度细节到生成示例多样性的文本到图像（T2I）生成的令人印象深刻的能力。这些扩散模型还开启了内容创作的新创意过程，例如，文本引导的图像编辑（Brooks
    等，[2023](#bib.bib6)；Cao 等，[2023](#bib.bib7)；Hertz 等，[2022](#bib.bib11)），涉及仅通过文本指导对传达高度复杂视觉概念的多样化图像进行编辑。广义上，当前的图像编辑范式可以总结为两个方面：i)
    *纹理编辑*（Brooks 等，[2023](#bib.bib6)；Cao 等，[2023](#bib.bib7)；Hertz 等，[2022](#bib.bib11)），在保持输入结构和场景布局的同时，操控给定图像的风格化和外观；ii)
    *非刚性编辑*（Cao 等，[2023](#bib.bib7)；Kawar 等，[2023](#bib.bib19)），在保留图像原始特征的同时，实现非刚性图像编辑（例如，姿势变化）。
- en: 'Despite achieving impressive image-level editing effects, the aforementioned
    methods fail to harness the editing animation, *i.e.*, the smooth transition of
    the sequence of intermediary images according to the user’s textual requirement,
    including the fine-grained texture and non-rigid transformation. Such text-conditioned
    image-to-animation serves as an imperative component in various real-world content
    creation tasks, ranging from cinematic effects to computer games, as well as photo-editing
    tools for artistic and entertainment purposes to enrich people’s imagination.
    Nevertheless, realizing animation-level editing is highly challenging, primarily
    due to the highly unstructured latent space of the intermediary images. Of course,
    we can introduce more animation data to fine-tune the entire T2I diffusion models,
    thereby capturing the smooth animation edit. However, it comes at a tremendous
    cost and deteriorates the flexibility of the pre-trained diffusion models under
    the animation-level editing setting. Based on the above insights, one question
    is thrown: Given the input image and textual description, could we achieve the
    high-quality animation editing effect with the pre-trained text-to-image models
    without fine-tuning?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管实现了令人印象深刻的图像级编辑效果，但上述方法未能利用编辑动画，即根据用户的文本要求平滑过渡的中间图像序列，包括细粒度纹理和非刚性变换。这种文本条件的图像到动画转化是各种现实世界内容创作任务中一个重要组成部分，从电影效果到计算机游戏，再到艺术和娱乐目的的照片编辑工具，以丰富人们的想象力。然而，实现动画级别的编辑非常具有挑战性，主要是因为中间图像的潜在空间高度无结构。当然，我们可以引入更多的动画数据来微调整个T2I扩散模型，从而捕捉平滑的动画编辑。然而，这会付出巨大的成本，并降低预训练扩散模型在动画级编辑设置下的灵活性。基于以上见解，提出了一个问题：在给定输入图像和文本描述的情况下，我们能否在不进行微调的情况下，通过预训练的文本到图像模型实现高质量的动画编辑效果？
- en: 'In this paper, we introduce a novel tuning-free LLM-driven attention control
    framework framework for text-conditioned image-to-animation, through LLM planing
    $\rightarrow$ prompt-Aware Editing $\rightarrow$ StablE moRphing, named as LASER.
    The core of our framework is that by leveraging the large language models (LLMs)(Touvron
    et al., [2023](#bib.bib38); Achiam et al., [2023](#bib.bib2); Li et al., [2023](#bib.bib22);
    Zhang et al., [2024](#bib.bib49)) with significant potential in natural language
    processing, to effectively parse the textual description into relevant and continuous
    control statements for pre-trained T2I diffusion models, thereby transforming
    the given image to animation. Specifically, LASER comprises the following progressive
    steps: Step 1, given a multimodal input, *i.e.*, a description of the animation
    $P_{0}$ and an initial image $I_{0}$ (which can be optional, allowing the T2I
    model generation), LLM decomposes the general and coarse-grained description $P_{0}$
    into multiple fine-grained and consistent prompts. These prompts are closely aligned
    and exhibit subtle variations, aiding in the guided editing of subsequently corresponding
    keyframes; Step 2, the LLM analyzes these prompts to the feature and attention
    injection control signals, adapting to the nuanced differences between adjacent
    prompts. This enables tailored injection strategies for editing different keyframe
    types. Notably, the injection strategy delineates into two base categories: Feature
    and Association Injection (FAI) for texture-based editing and Key-Value Attention
    Injection (KVAI) for non-rigid editing. Notably, to facilitate the simultaneous
    portrayal of both texture and non-rigid editing within a singular animation phase,
    we propose the forward hybrid Attention Injection (HAI) for the image editing;
    Step 3, effectively synthesizing intermediate frames between keyframes, ensuring
    animations are coherent and fluid. This generator utilizes advanced interpolation
    methods, such as spherical linear interpolation, to ensure smooth transitions
    and reduce artifacts. Additionally, Adaptive Instance Normalization (AdaIN) is
    applied to enhance color and brightness consistency. The Hybrid Attention Injection
    (HAI) strategy is also employed to integrate texture and structural transformations
    within a single animation phase, further enhancing the animation’s overall quality
    and coherence. Additionally, we inaugurate a Text-conditioned Image-to-Animation
    Benchmark, a comprehensive collection designed to challenge and quantify the adaptability
    and precision of the proposed LASER.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一种新颖的无调节 LLM 驱动的注意力控制框架，应用于文本条件的图像到动画转换，称为 LASER。我们框架的核心在于，通过利用在自然语言处理方面具有显著潜力的大型语言模型（LLMs）（Touvron
    et al., [2023](#bib.bib38); Achiam et al., [2023](#bib.bib2); Li et al., [2023](#bib.bib22);
    Zhang et al., [2024](#bib.bib49)），有效地将文本描述解析为相关且连续的控制语句，以供预训练的 T2I 扩散模型使用，从而将给定的图像转换为动画。具体而言，LASER
    包括以下逐步步骤：步骤 1，给定多模态输入，即动画描述 $P_{0}$ 和初始图像 $I_{0}$（可以是可选的，允许 T2I 模型生成），LLM 将一般且粗略的描述
    $P_{0}$ 细分为多个细化且一致的提示。这些提示紧密对齐，并表现出微妙的变化，有助于对随后对应的关键帧进行引导编辑；步骤 2，LLM 将这些提示分析为特征和注意力注入控制信号，适应相邻提示之间的细微差异。这使得针对不同关键帧类型的定制注入策略成为可能。值得注意的是，注入策略分为两大类：用于基于纹理的编辑的特征和关联注入（FAI）以及用于非刚性编辑的键值注意力注入（KVAI）。特别地，为了在单一动画阶段同时呈现纹理和非刚性编辑，我们提出了前向混合注意力注入（HAI）用于图像编辑；步骤
    3，有效地在关键帧之间合成中间帧，确保动画连贯流畅。该生成器利用先进的插值方法，如球面线性插值，以确保平滑过渡并减少伪影。此外，应用自适应实例归一化（AdaIN）以增强颜色和亮度的一致性。混合注意力注入（HAI）策略也用于在单一动画阶段整合纹理和结构转换，进一步提升动画的整体质量和连贯性。此外，我们推出了一个文本条件图像到动画的基准测试，这是一个全面的集合，旨在挑战和量化所提出的
    LASER 的适应性和精确性。
- en: 'Summing up, our contributions can be concluded as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的贡献可以归纳为：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce the tuning-free text-conditioned image-to-animation task, designed
    to craft high-quality animations based on the multimodal input using the pre-trained
    text-to-image models, without additional fine-tuning or annotations. To evaluate
    the efficacy of our approach, we introduce the Text-conditioned Image-to-Animation
    Benchmark, hoping that it may support future studies within this domain.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一种无需调优的文本条件图像到动画任务，旨在利用预训练的文本到图像模型，根据多模态输入制作高质量动画，无需额外的调优或注释。为了评估我们方法的有效性，我们推出了文本条件图像到动画基准测试，希望它能支持未来在这一领域的研究。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The proposed the LASER encapsulated by the progressive process of LLM planing
    $\rightarrow$ Prompt-aware editing $\rightarrow$ Stable morphing, enabling the
    smooth texture- and non-rigid animation generation.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所提的LASER通过LLM规划的渐进过程 $\rightarrow$ 提示感知编辑 $\rightarrow$ 稳定变形，封装了平滑纹理和非刚性动画生成的能力。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Both qualitative and quantitative assessments underscore the superior efficacy
    of the proposed framework, showcasing its proficiency in generating animations
    that are not only smooth and of high quality but also diverse.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定性和定量评估都强调了所提框架的优越效果，展示了其生成动画的能力，这些动画不仅流畅且高质量，还具有多样性。
- en: 2\. Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: Text-to-Image Generation. In artificial intelligence(Zhang et al., [2022](#bib.bib52),
    [2019](#bib.bib51), [2023a](#bib.bib50)), text-to-image (T2I) Generation aims
    to generate high-quality images based on text descriptions. Previous text-conditioned
    image generation approaches were primarily based on Generative Adversarial Networks
    (GANs) (Brock et al., [2018](#bib.bib5); Zhang et al., [2017](#bib.bib44), [2018b](#bib.bib45);
    Xu et al., [2018](#bib.bib42); Zhu et al., [2019](#bib.bib53)), leveraging their
    robust capabilities for high-fidelity image synthesis. These models, through multimodal
    vision-language learning, have endeavored to align text descriptions with synthesized
    image contents, yielding gratifying synthesis results on specific domain datasets.
    Recently, diffusion models (Ho et al., [2020](#bib.bib13); Nichol and Dhariwal,
    [2021](#bib.bib25); Dhariwal and Nichol, [2021](#bib.bib9)) have demonstrated
    exceptional generative capabilities, achieving state-of-the-art results in terms
    of generation quality and diversity. By incorporating text prompts into diffusion
    models, various text-to-image diffusion models (Rombach et al., [2022](#bib.bib31);
    Saharia et al., [2022](#bib.bib32); Podell et al., [2023](#bib.bib29)) have been
    developed. They are intricately conditioned on the provided text via cross-attention
    layers, ensuring that the generated images are not only visually coherent but
    also semantically consistent with the input descriptions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像生成。在人工智能领域（Zhang et al., [2022](#bib.bib52), [2019](#bib.bib51), [2023a](#bib.bib50)），文本到图像（T2I）生成旨在基于文本描述生成高质量图像。以前的文本条件图像生成方法主要基于生成对抗网络（GANs）（Brock
    et al., [2018](#bib.bib5); Zhang et al., [2017](#bib.bib44), [2018b](#bib.bib45);
    Xu et al., [2018](#bib.bib42); Zhu et al., [2019](#bib.bib53)），利用其强大的高保真图像合成功能。这些模型通过多模态视觉语言学习，努力将文本描述与合成图像内容对齐，在特定领域数据集上产生了令人满意的合成结果。最近，扩散模型（Ho
    et al., [2020](#bib.bib13); Nichol and Dhariwal, [2021](#bib.bib25); Dhariwal
    and Nichol, [2021](#bib.bib9)）展现了卓越的生成能力，在生成质量和多样性方面取得了最先进的成果。通过将文本提示融入扩散模型，开发了各种文本到图像扩散模型（Rombach
    et al., [2022](#bib.bib31); Saharia et al., [2022](#bib.bib32); Podell et al.,
    [2023](#bib.bib29)）。这些模型通过交叉注意力层精确地根据提供的文本进行条件处理，确保生成的图像不仅在视觉上连贯，而且在语义上与输入描述一致。
- en: Text-guided Image Editing. Text-guided image editing is a challenging task that
    aims to edit images based on textual descriptions, enabling users to achieve desired
    changes in natural language. Previous deep-learning-based approaches based on
    GANs (Li et al., [2020](#bib.bib21); Nam et al., [2018](#bib.bib24); Patashnik
    et al., [2021](#bib.bib28); Xia et al., [2021](#bib.bib41)) have achieved certain
    success, but they are limited to specific domain datasets and exhibit limited
    applicability and generalization. VQGANCLIP (Crowson et al., [2022](#bib.bib8))
    is an autoregressive model that combines VQGAN (Esser et al., [2021](#bib.bib10))
    and CLIP (Radford et al., [2021](#bib.bib30)) to produce high-quality images and
    enable precise editing, yielding diverse and controllable results. However, this
    method suffers from slow generation speed and high computational cost. Recently,
    diffusion models trained on large-scale text-image pairs such as Imagen (Saharia
    et al., [2022](#bib.bib32)) and Stable Diffusion (Rombach et al., [2022](#bib.bib31))
    have achieved unprecedented success in text-to-image generation. Therefore, they
    serve as a robust prior for various editing tasks, including text-guided image
    manipulation (Brooks et al., [2023](#bib.bib6); Cao et al., [2023](#bib.bib7);
    Hertz et al., [2022](#bib.bib11); Kawar et al., [2023](#bib.bib19); Parmar et al.,
    [2023](#bib.bib27); Tumanyan et al., [2023](#bib.bib39)). Prompt-to-Prompt (Hertz
    et al., [2022](#bib.bib11)) and Plug-and-Play (Tumanyan et al., [2023](#bib.bib39))
    utilize cross-attention or spatial features to edit both global and local aspects
    of the image by directly modifying the text prompt. MasaCtrl (Cao et al., [2023](#bib.bib7))
    and Imagic (Kawar et al., [2023](#bib.bib19)) can handle non-rigid transformations
    such as changing object poses. Particularly, Plug-and-Play (Tumanyan et al., [2023](#bib.bib39))
    consider the task of text-guided image-to-image translation that aims to estimate
    a mapping of an image from a source domain to a target domain, where the target
    domain is not specified through a dataset of images but rather via a target text
    prompt. However, most of these approaches directly generate the final edited image,
    with limited exploration concerning continuous animations such as image morphing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本引导的图像编辑**。文本引导的图像编辑是一项具有挑战性的任务，旨在根据文本描述编辑图像，使用户能够用自然语言实现期望的变化。之前基于深度学习的生成对抗网络（GANs）方法（Li
    et al., [2020](#bib.bib21); Nam et al., [2018](#bib.bib24); Patashnik et al.,
    [2021](#bib.bib28); Xia et al., [2021](#bib.bib41)）取得了一定的成功，但它们仅限于特定领域的数据集，表现出有限的适用性和泛化能力。**VQGANCLIP**（Crowson
    et al., [2022](#bib.bib8)）是一个自回归模型，结合了**VQGAN**（Esser et al., [2021](#bib.bib10)）和**CLIP**（Radford
    et al., [2021](#bib.bib30)）以生成高质量图像并实现精确编辑，产生多样且可控的结果。然而，这种方法的生成速度较慢，计算成本较高。最近，基于大规模文本-图像对训练的扩散模型，如**Imagen**（Saharia
    et al., [2022](#bib.bib32)）和**Stable Diffusion**（Rombach et al., [2022](#bib.bib31)），在文本到图像生成方面取得了前所未有的成功。因此，它们为各种编辑任务提供了稳健的先验，包括文本引导的图像操作（Brooks
    et al., [2023](#bib.bib6); Cao et al., [2023](#bib.bib7); Hertz et al., [2022](#bib.bib11);
    Kawar et al., [2023](#bib.bib19); Parmar et al., [2023](#bib.bib27); Tumanyan
    et al., [2023](#bib.bib39)）。**Prompt-to-Prompt**（Hertz et al., [2022](#bib.bib11)）和**Plug-and-Play**（Tumanyan
    et al., [2023](#bib.bib39)）利用交叉注意力或空间特征，通过直接修改文本提示来编辑图像的全局和局部方面。**MasaCtrl**（Cao
    et al., [2023](#bib.bib7)）和**Imagic**（Kawar et al., [2023](#bib.bib19)）可以处理非刚性变换，如更改对象姿势。特别是，**Plug-and-Play**（Tumanyan
    et al., [2023](#bib.bib39)）考虑了文本引导的图像到图像翻译任务，旨在估计从源领域到目标领域的图像映射，其中目标领域不是通过图像数据集指定的，而是通过目标文本提示来实现的。然而，大多数这些方法直接生成最终编辑的图像，对连续动画（如图像形态变化）的探索有限。'
- en: Image Morphing. Image morphing is a task in computer graphics and image processing
    that aims to obtain reasonable intermediate images in the smooth transition between
    two images (Aloraibi, [2023](#bib.bib3); Zope and Zope, [2017](#bib.bib54)). With
    the advent of deep learning, neural networks have been used for image morphing,
    learning to identify correspondences and generate intermediate frames through
    latent interpolations. For instance, in the works on GANs (Karras et al., [2021](#bib.bib16),
    [2019](#bib.bib17), [2020](#bib.bib18); Sauer et al., [2023](#bib.bib33), [2022](#bib.bib34)),
    it has been demonstrated that their latent embedding space is highly continuous,
    and linear interpolation between two latent codes yields impressive image morphing
    results. Recent studies on diffusion models have also indicated the feasibility
    of generating plausible intermediate images through latent noise interpolation
    and text embedding interpolation (Bao et al., [2023](#bib.bib4); Song et al.,
    [2020b](#bib.bib37); Wang and Golland, [2023](#bib.bib40)). Impus (Yang et al.,
    [2023](#bib.bib43)) explored the application of diffusion models in image morphing
    tasks, performing interpolation in the locally linear continuous text embedding
    space and Gaussian latent space. DiffMorpher (Zhang et al., [2023c](#bib.bib46))
    utilizes pre-trained diffusion models to achieve smooth and natural image interpolation
    and morphing. It performs spherical linear interpolation on the latent noise obtained
    through DDIM inversion for two images and combines it with text-conditioned linear
    interpolation, thus addressing the limitations of smooth interpolation between
    two image samples within the unstructured latent space of diffusion models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图像变形。图像变形是计算机图形学和图像处理中的一项任务，旨在在两个图像之间平滑过渡中获得合理的中间图像（Aloraibi，[2023](#bib.bib3)；Zope
    和 Zope，[2017](#bib.bib54)）。随着深度学习的出现，神经网络已被用于图像变形，通过潜在插值学习识别对应关系并生成中间帧。例如，在 GANs
    的研究中（Karras 等，[2021](#bib.bib16)，[2019](#bib.bib17)，[2020](#bib.bib18)；Sauer 等，[2023](#bib.bib33)，[2022](#bib.bib34)），已证明其潜在嵌入空间高度连续，两个潜在编码之间的线性插值产生了令人印象深刻的图像变形结果。最近关于扩散模型的研究也表明，通过潜在噪声插值和文本嵌入插值生成合理的中间图像是可行的（Bao
    等，[2023](#bib.bib4)；Song 等，[2020b](#bib.bib37)；Wang 和 Golland，[2023](#bib.bib40)）。Impus（Yang
    等，[2023](#bib.bib43)）探索了扩散模型在图像变形任务中的应用，在局部线性连续文本嵌入空间和高斯潜在空间中进行插值。DiffMorpher（Zhang
    等，[2023c](#bib.bib46)）利用预训练的扩散模型实现平滑自然的图像插值和变形。它对通过 DDIM 反演获得的两个图像的潜在噪声进行球面线性插值，并将其与文本条件线性插值结合，从而解决了扩散模型中不规则潜在空间内两个图像样本之间平滑插值的局限性。
- en: 3\. Methodology
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法学
- en: Given a user-defined descriptor $P_{*}$ and an initial image $I_{0}$ (provided
    or generated), our method generates the animation sequence $\{x_{0}^{(\alpha)},x_{1}^{(\alpha)},\ldots,x_{n}^{(\alpha)}\}$,
    where $\alpha$ varies from 0 to 1\. The length of the $x_{i}^{(\alpha)}$ sequence
    is set by $n_{f}$ and the number of sequences $x_{i}^{(\alpha)}$ corresponds to
    the transformation stages $n_{t}$. The resulting animation is expected to visually
    manifest the smooth transitions of $I_{0}$ to $I_{n}$ and characteristics as described
    by $P_{*}$. To guide this generative process, a series of descriptive prompts
    $\{P_{0},P_{1},\ldots,P_{n_{t}}\}$ are derived to anchor each keyframe in the
    animation’s continuity.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定用户定义的描述符 $P_{*}$ 和初始图像 $I_{0}$（提供或生成），我们的方法生成动画序列 $\{x_{0}^{(\alpha)},x_{1}^{(\alpha)},\ldots,x_{n}^{(\alpha)}\}$，其中
    $\alpha$ 从 0 到 1 变化。$x_{i}^{(\alpha)}$ 序列的长度由 $n_{f}$ 设置，序列 $x_{i}^{(\alpha)}$
    的数量对应于变换阶段 $n_{t}$。生成的动画预计将视觉上表现出 $I_{0}$ 到 $I_{n}$ 的平滑过渡以及 $P_{*}$ 描述的特征。为了引导这一生成过程，衍生出一系列描述性提示
    $\{P_{0},P_{1},\ldots,P_{n_{t}}\}$，以锚定动画连续性中的每个关键帧。
- en: '![Refer to caption](img/920aeba3eedc46dc25a0e854bd2bd169.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/920aeba3eedc46dc25a0e854bd2bd169.png)'
- en: Figure 2\. Overview of proposed LASER. (a) The LLM-driven Controller first parses
    the descriptive prompts to generate the descriptive prompts for corresponding
    frames of animation. (b) By doing so, the LLM analyzes these prompts to the feature
    and attention injection control signals, to facilitate the simultaneous portrayal
    of both texture and non-rigid editing. (c) The animation generator leverages spherical
    linear interpolation and adaptive instance normalization to generate the intermediate
    images between keyframes, accessing smooth animation generation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 提出的 LASER 概述。 (a) 由 LLM 驱动的控制器首先解析描述性提示，以生成相应动画帧的描述性提示。 (b) 通过这样做，LLM
    分析这些提示生成特征和注意力注入控制信号，以便同时描绘纹理和非刚性编辑。 (c) 动画生成器利用球面线性插值和自适应实例归一化，在关键帧之间生成中间图像，实现平滑的动画生成。
- en: 3.1\. Preliminary for Diffusion Models
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 扩散模型的初步介绍
- en: 'Diffusion models (Ho et al., [2020](#bib.bib13)) (Song et al., [2020a](#bib.bib36)) (Nichol
    and Dhariwal, [2021](#bib.bib25)) are a series of probabilistic generative models
    that produce images by gradual denoising from a noise distribution, e.g., Gaussian
    distribution. The generation process consists of two main phases: the forward
    (diffusion) process and reverse (denoising) process.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型（Ho 等，[2020](#bib.bib13)）（Song 等，[2020a](#bib.bib36)）（Nichol 和 Dhariwal，[2021](#bib.bib25)）是一系列概率生成模型，通过逐渐去噪噪声分布（例如，高斯分布）来生成图像。生成过程包括两个主要阶段：正向（扩散）过程和反向（去噪）过程。
- en: 'The forward process gradually adds noise to initial data $x_{0}$ to generate
    a noisy data $x_{t}$ given variance schedule $\alpha_{t}\in(0,1)$ at time-step
    $t$:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正向过程逐渐向初始数据 $x_{0}$ 添加噪声，以生成给定方差调度 $\alpha_{t}\in(0,1)$ 在时间步 $t$ 的噪声数据 $x_{t}$：
- en: '| (1) |  | $q(x_{t}&#124;x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})\mathbf{I}),$
    |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $q(x_{t}&#124;x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})\mathbf{I}),$
    |  |'
- en: where $\bar{\alpha}_{t}=\prod_{i=1}^{t}\alpha_{i}$. After $T$ steps, we obtain
    noise $x_{T}\sim\mathcal{N}(0,1)$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{\alpha}_{t}=\prod_{i=1}^{t}\alpha_{i}$。经过 $T$ 步骤后，我们得到噪声 $x_{T}\sim\mathcal{N}(0,1)$。
- en: 'The reverse process aims to gradually clean the noise. By utilizing the Bayzes’
    rules and Markov property, we can intuitively express the conditional probabilities
    as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 反向过程的目标是逐渐清理噪声。通过利用贝叶斯规则和马尔可夫性质，我们可以直观地表示条件概率如下：
- en: '| (2) |  | $1$2 |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $1$2 |  |'
- en: where $\tilde{\beta}_{t}$ is a time-dependent constant and added noise $\epsilon$
    can be predicted by a neural network $\epsilon_{\theta}$. By sampling $x_{t-1}$
    iteratively, we finally get a clean image $x_{0}$ from initial Gaussian noise
    $x_{T}$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{\beta}_{t}$ 是时间相关常数，添加的噪声 $\epsilon$ 可以通过神经网络 $\epsilon_{\theta}$
    预测。通过迭代地采样 $x_{t-1}$，我们最终从初始高斯噪声 $x_{T}$ 中得到清晰图像 $x_{0}$。
- en: We employ a text-conditioned Stable Diffusion (SD) (Rombach et al., [2022](#bib.bib31)),
    which operates within lower-dimensional latent space rather than pixel space.
    It begins with encoding images to latent representation by a variational auto-encoder
    (VAE) (Kingma and Welling, [2013](#bib.bib20)), followed by a diffusion-denoising
    process within the latent space. After denoising, the latent representation is
    decoded back into the image space via a decoder network, culminating in the final
    generated image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了一种基于文本的稳定扩散（SD）（Rombach 等，[2022](#bib.bib31)），该方法在低维潜在空间中运行，而不是像素空间。它从通过变分自编码器（VAE）（Kingma
    和 Welling，[2013](#bib.bib20)）对图像进行编码开始，然后在潜在空间中进行扩散去噪过程。去噪后，潜在表示通过解码器网络解码回图像空间，最终生成图像。
- en: 'In the noise-predicting network $\epsilon_{\theta}$, residual blocks process
    image features to generate intermediate features $f_{t}^{l}$, which are then used
    in the self-attention module to produce $Q$, $K$, $V$ for capturing long-range
    interactions. Subsequently, cross-attention integrates textual prompt $P$ input,
    merging text and image semantics. The attention mechanism can be formulated as
    follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在噪声预测网络 $\epsilon_{\theta}$ 中，残差块处理图像特征以生成中间特征 $f_{t}^{l}$，这些特征随后在自注意力模块中用于生成
    $Q$、$K$、$V$ 以捕获长距离交互。随后，交叉注意力将文本提示 $P$ 输入，融合文本和图像语义。注意力机制可以表示如下：
- en: '| (3) |  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V,$ |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V,$ |  |'
- en: where $Q$, $K$, and $V$ represent queries, keys, and values, respectively, with
    $d_{k}$ denoting the key/query dimension for scaling dot product. In this model,
    $Q$ originates from spatial features, while $K$ and $V$ come from spatial features
    and text embeddings for self and cross-attention, respectively. Leveraging attention
    layers within the SD model significantly affects image composition and development (Hertz
    et al., [2022](#bib.bib11)) (Tumanyan et al., [2023](#bib.bib39)), guiding image
    editing and synthesis by manipulating attention-related information during denoising (Cao
    et al., [2023](#bib.bib7)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`$Q$`、`$K$`和`$V$`分别代表查询、键和值，`$d_{k}$`表示用于缩放点积的键/查询维度。在这个模型中，`$Q$`来源于空间特征，而`$K$`和`$V$`分别来自空间特征和文本嵌入，用于自注意力和交叉注意力。利用SD模型中的注意力层对图像的组成和发展有显著影响（Hertz等，[2022](#bib.bib11)）（Tumanyan等，[2023](#bib.bib39)），通过在去噪过程中操控与注意力相关的信息来指导图像编辑和合成（Cao等，[2023](#bib.bib7)）。
- en: 3.2\. LLM-driven Controller
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. LLM驱动的控制器
- en: 'In this section, we first utilize LLM to extract aligned textual prompts for
    each key animation stage. Our approach supports two input modalities: text-image
    pairs and text-only inputs. If the user provides an image, it is directly utilized
    as the initial image $I_{0}$. In cases where the initial image is absent, we leverage
    pre-trained Stable Diffusion models to generate $I_{0}$.To generate animations
    that adhere to the semantics of a specified text description $P_{*}$, we require
    text prompts $\{P_{0},P_{1},\ldots,P_{n_{t}}\}$ for each key animation stage,
    as these prompts directly guide the animation process. High-quality, detailed
    text prompts are crucial when no initial image is provided, as the model generates
    $I_{0}$ based on $P_{0}$’s semantic cues. Prompts for Stable Diffusion should
    be richly descriptive to accurately produce a high-quality starting image.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先利用LLM提取每个关键动画阶段的对齐文本提示。我们的方法支持两种输入方式：文本-图像对和仅文本输入。如果用户提供了图像，则直接使用该图像作为初始图像`$I_{0}`。如果初始图像缺失，我们利用预训练的Stable
    Diffusion模型生成`$I_{0}`。为了生成符合指定文本描述`$P_{*}`语义的动画，我们需要为每个关键动画阶段提供文本提示`$\{P_{0},P_{1},\ldots,P_{n_{t}}\}$`，因为这些提示直接指导动画过程。当没有提供初始图像时，高质量、详细的文本提示至关重要，因为模型根据`$P_{0}$`的语义线索生成`$I_{0}`。Stable
    Diffusion的提示应富有描述性，以准确生成高质量的起始图像。
- en: 'To enhance the quality and stability of the process, we introduce two agents
    based on large language models: the “Stage Image Text Prompt Agent” (SIA) and
    the “Stable Diffusion Prompt Generator Agent” (PGA). Initially, SIA generates
    text prompts that guide the image generation for each key stage, as illustrated
    in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Methodology ‣ LASER: Tuning-Free LLM-Driven
    Attention Control for Efficient Text-conditioned Image-to-Animation") (a). SIA
    generates text prompts based on two fundamental principles: i) By decomposing
    the animation descriptor $P_{*}$ into multiple independent processes, SIA reduces
    semantic differences between adjacent prompts, enhancing the overall quality of
    the results. ii) The prompts must be highly aligned to facilitate high-quality
    intermediate results through linear interpolation. Given the local linearity within
    the CLIP text embedding space (Kawar et al., [2023](#bib.bib19)), minimizing the
    gap between adjacent embeddings is essential. A practical method involves using
    consistent sentence structures across prompts, such as “A cat [action] on the
    ground” and “A [animal] jumping on the ground” (Yang et al., [2023](#bib.bib43)).
    This approach ensures that while the prompts are semantically distinct, they share
    a common categorical root, thus streamlining the generation process. This generation
    method successfully mitigates the non-linearity and discontinuity commonly encountered
    between text embeddings. With the deployment of the Stage Image Text Prompt Agent
    (SIA), we significantly bolster our model’s capacity to generate semantically
    coherent and high-quality images. The Stable Diffusion Prompt Generator Agent
    (PGA) converts broad, high-level concepts from the SIA into richly detailed and
    vividly descriptive prompts specifically crafted for Stable Diffusion. As depicted
    in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Methodology ‣ LASER: Tuning-Free LLM-Driven
    Attention Control for Efficient Text-conditioned Image-to-Animation") (b), once
    PGA receives the initial text prompt from SIA, it refines this input to craft
    a more detailed prompt. This enhanced prompt not only delineates the subject and
    action but also enriches the scene with specific elements like texture, lighting,
    and artistic style, which instructs Stable Diffusion to produce images of higher
    fidelity and complexity (Lian et al., [2024](#bib.bib23)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提高过程的质量和稳定性，我们引入了基于大型语言模型的两个代理：**“阶段图像文本提示代理”**（SIA）和**“稳定扩散提示生成代理”**（PGA）。最初，SIA
    生成指导每个关键阶段图像生成的文本提示，如图 [2](#S3.F2 "Figure 2 ‣ 3\. Methodology ‣ LASER: Tuning-Free
    LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation")（a）所示。SIA
    根据两个基本原则生成文本提示：i) 通过将动画描述符 $P_{*}$ 分解为多个独立的过程，SIA 减少了相邻提示之间的语义差异，从而提高了结果的整体质量。ii)
    提示必须高度对齐，以通过线性插值实现高质量的中间结果。鉴于 CLIP 文本嵌入空间的局部线性（Kawar 等， [2023](#bib.bib19)），最小化相邻嵌入之间的差距至关重要。一个实际的方法是使用一致的句子结构，例如
    “A cat [action] on the ground” 和 “A [animal] jumping on the ground”（Yang 等， [2023](#bib.bib43)）。这种方法确保了虽然提示在语义上有所不同，但它们共享一个共同的分类根源，从而简化了生成过程。这种生成方法成功减轻了文本嵌入之间常见的非线性和不连续性。通过部署阶段图像文本提示代理（SIA），我们显著提升了模型生成语义一致且高质量图像的能力。稳定扩散提示生成代理（PGA）将
    SIA 的广泛、高层次概念转换为丰富详细且生动描述的提示，专门为稳定扩散量身定制。如图 [2](#S3.F2 "Figure 2 ‣ 3\. Methodology
    ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation")（b）所示，一旦 PGA 收到 SIA 的初始文本提示，它会对这个输入进行优化，制作出更详细的提示。这个增强的提示不仅描绘了主题和动作，还通过具体的元素如纹理、光照和艺术风格丰富了场景，这指导稳定扩散生成更高保真度和复杂性的图像（Lian
    等， [2024](#bib.bib23)）。'
- en: 3.3\. Hybrid Prompt-aware Editor
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 混合提示感知编辑器
- en: This section utilizes the aligned textual prompts to obtain keyframe images.
    During the editing process, $Z^{*}_{1,T}$ is a direct copy of $Z^{*}_{0,T}$. For
    $i\geq 1$, each keyframe $x_{i}$ undergoes DDIM inversion to produce $Z_{i,T}$,
    which is then cloned to form $Z^{*}_{i+1,T}$ for the subsequent keyframe. Despite
    using aligned prompts for text-guiding image editing, we still observe a marked
    discrepancy in semantic identity between the images, which results in animations
    that do not transition smoothly. To overcome this challenge, we draw inspiration
    from previous image editing techniques (Tumanyan et al., [2023](#bib.bib39); Cao
    et al., [2023](#bib.bib7)) and propose a feature and attention injection method
    controlled by the LLM, tailored to query semantically similar content from the
    previous keyframes according to the changing nature of the corresponding stage.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节利用对齐的文本提示来获取关键帧图像。在编辑过程中，$Z^{*}_{1,T}$ 是 $Z^{*}_{0,T}$ 的直接副本。对于 $i\geq 1$，每个关键帧
    $x_{i}$ 经过 DDIM 反演产生 $Z_{i,T}$，然后克隆形成 $Z^{*}_{i+1,T}$ 用于后续关键帧。尽管使用对齐的提示进行文本引导的图像编辑，但我们仍然观察到图像之间的语义身份存在明显差异，这导致动画过渡不流畅。为了解决这一挑战，我们借鉴了之前的图像编辑技术（Tumanyan
    等，[2023](#bib.bib39)；Cao 等，[2023](#bib.bib7)），并提出了一种由 LLM 控制的特征和注意力注入方法，根据相应阶段的变化从先前关键帧中查询语义上类似的内容。
- en: Utilizing DDIM inversion on the prior keyframe, we obtain the initial state
    $Z_{i,T}$. Past work (Tumanyan et al., [2023](#bib.bib39)) has demonstrated that
    injecting features $f_{t}^{l}$ within residual blocks and self-attention projections
    $q_{t}^{l}$, $k_{t}^{l}$ significantly boosts text-guided image edition tasks.
    The encoding in the fourth layer $f_{t}^{4}$ specifically captures shared semantics
    necessary for structure retention during generation. Moreover, the injections
    of self-attention are underpinned by the attention scores, which arise from the
    product of query and key vectors, exhibiting a profound connection to the well-established
    self-referential paradigms within neural attention schemas. By injecting specific
    features $f_{t}^{4}$ into the fourth layer of residual blocks and introducing
    self-attention elements $q_{t}^{l}$ and $k_{t}^{l}$ throughout all decoder layers,
    we have successfully achieved texture variations between keyframes. We refer to
    this injection strategy as “Feature and Association Injection” (FAI).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对先前关键帧应用 DDIM 反演，我们获得了初始状态 $Z_{i,T}$。以往的工作（Tumanyan 等，[2023](#bib.bib39)）已经证明，在残差块和自注意力投影
    $q_{t}^{l}$、$k_{t}^{l}$ 中注入特征 $f_{t}^{l}$ 可以显著提升文本引导的图像编辑任务。第四层中的编码 $f_{t}^{4}$
    专门捕获在生成过程中保持结构所需的共享语义。此外，自注意力的注入基于注意力分数，这些分数来源于查询向量和键向量的乘积，与神经注意力模式中已建立的自指涉范式有着深刻的联系。通过将特定特征
    $f_{t}^{4}$ 注入残差块的第四层，并在所有解码器层中引入自注意力元素 $q_{t}^{l}$ 和 $k_{t}^{l}$，我们成功实现了关键帧之间的纹理变化。我们将这种注入策略称为“特征和关联注入”（FAI）。
- en: '![Refer to caption](img/1443618354d2a7944e15ca48d69ff05f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1443618354d2a7944e15ca48d69ff05f.png)'
- en: Figure 3\. Overview of Feature and Association Injection.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 特征和关联注入概述。
- en: 'However, the aforementioned method struggles with non-rigid keyframe modifications.
    The usual solution, limiting injection range to reflect rigid changes from prompts,
    risks losing image identity. To navigate this, especially for non-rigid edits,
    we avoid injecting into residual blocks, thereby maintaining the image’s structural
    integrity without being obscured by local semantics. Our strategy uses targeted
    attention injections. As image layout solidifies early in denoising and self-attention
    queries align semantically (Cao et al., [2023](#bib.bib7)), they can extract content
    from various objects. Post-denoising, we inject keys $k_{t}^{l}$ and values $v_{t}^{l}$
    from the previous keyframe’s self-attention block, as shown in Fig. [4](#S3.F4
    "Figure 4 ‣ 3.3\. Hybrid Prompt-aware Editor ‣ 3\. Methodology ‣ LASER: Tuning-Free
    LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation").
    This process forms objects’ outlines following text prompts and then enriches
    the generative structure with detailed content from the source image. Consequently,
    we achieve semantically coherent images that also support non-rigid transitions.
    We refer to this injection strategy as “Key-Value Attention Injection” (KVAI).
    Up to this point, the model has acquired the capability to generate diverse keyframes,
    enabling the production of the expected animations.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，上述方法在处理非刚性关键帧修改时存在困难。通常的解决方案是将注入范围限制为反映来自提示的刚性变化，这有可能导致图像身份的丧失。为了应对这一问题，特别是针对非刚性编辑，我们避免对残差块进行注入，从而保持图像的结构完整性，不被局部语义所遮蔽。我们的策略使用有针对性的注意力注入。由于图像布局在去噪早期就已固化，自注意力查询在语义上对齐（Cao
    et al., [2023](#bib.bib7)），它们能够从不同对象中提取内容。在去噪后，我们从前一个关键帧的自注意力块中注入键$k_{t}^{l}$和值$v_{t}^{l}$，如图[4](#S3.F4
    "Figure 4 ‣ 3.3\. Hybrid Prompt-aware Editor ‣ 3\. Methodology ‣ LASER: Tuning-Free
    LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation")所示。此过程根据文本提示形成对象轮廓，然后用源图像中的详细内容丰富生成结构。因此，我们实现了在语义上连贯的图像，并支持非刚性过渡。我们将这种注入策略称为“键值注意力注入”（KVAI）。到目前为止，模型已具备生成多样化关键帧的能力，从而能够生产预期的动画。'
- en: 'Recognizing the need for a systematic approach to select the optimal injection
    strategy for each stage of the animation generation, we have developed the Injection
    Control Agent (ICA), as showcased in Fig. [2](#S3.F2 "Figure 2 ‣ 3\. Methodology
    ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation") (a). ICA’s primary role is to process the text prompts from
    the Stage Image Text Prompt Agent (SIA), which performs an in-depth analysis of
    semantic differences between these text prompts at consecutive key stages. This
    analysis enables SIA to issue tailored control signals: “0” signals ICA to deploy
    the injection strategy for stages where texture changes are dominant, and “1”
    signals the use of the KVAI strategy for stages with non-rigid transformations.
    By precisely managing the type of attention injection at each stage, ICA ensures
    that the generated animations are both visually coherent and closely aligned with
    the textual descriptors.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '认识到需要一种系统的方法来选择每个动画生成阶段的最佳注入策略，我们开发了注入控制代理（ICA），如图[2](#S3.F2 "Figure 2 ‣ 3\.
    Methodology ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation")（a）所示。ICA的主要角色是处理来自阶段图像文本提示代理（SIA）的文本提示，SIA对这些文本提示在连续关键阶段之间的语义差异进行深入分析。这种分析使SIA能够发出量身定制的控制信号：“0”信号指示ICA在纹理变化主导的阶段部署注入策略，而“1”信号指示在非刚性变换阶段使用KVAI策略。通过精确管理每个阶段的注意力注入类型，ICA确保生成的动画在视觉上既一致又与文本描述紧密对齐。'
- en: '![Refer to caption](img/5d062f9069d7208f7feae3490fdc36a4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5d062f9069d7208f7feae3490fdc36a4.png)'
- en: Figure 4\. Overview of Key-Value Attention Injection.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 键值注意力注入概述
- en: 3.4\. Animation Generator
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 动画生成器
- en: 'In this section, we generate intermediate images between keyframe images to
    obtain consistent and smooth animations. After generating the text prompts corresponding
    to each key stage ${P_{0},P_{1},\ldots,P_{n_{t}}}$ in section [3.2](#S3.SS2 "3.2\.
    LLM-driven Controller ‣ 3\. Methodology ‣ LASER: Tuning-Free LLM-Driven Attention
    Control for Efficient Text-conditioned Image-to-Animation"), we obtain the respective
    text embeddings ${e_{0},e_{1},\ldots,e_{n_{t}}}$. When generating intermediate
    images, we perform a simple linear interpolation between the text embeddings of
    two adjacent key stages to obtain the corresponding text embedding $e$.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们在关键帧图像之间生成中间图像，以获得一致且平滑的动画。在生成对应于每个关键阶段的文本提示 ${P_{0},P_{1},\ldots,P_{n_{t}}}$
    后，我们获得了相应的文本嵌入 ${e_{0},e_{1},\ldots,e_{n_{t}}}$。在生成中间图像时，我们在两个相邻关键阶段的文本嵌入之间进行简单的线性插值，以获得相应的文本嵌入
    $e$。
- en: '| (4) |  | $e_{\alpha}^{i}=(1-\alpha)e_{i}+\alpha e_{i+1}$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $e_{\alpha}^{i}=(1-\alpha)e_{i}+\alpha e_{i+1}$ |  |'
- en: In the construction of the animation sequence, the interpolation parameter $\alpha$
    is discretized into a series of values that facilitate a smooth transition between
    frames. This discretization is achieved by defining a set of equidistant points
    within the closed interval $[0,1]$, where the number of points corresponds to
    the intended number of frames in an animation stage, denoted as $n_{f}$. Thus,
    $\alpha$ takes on values $\alpha_{0},\alpha_{1},\ldots,\alpha_{n_{f}-1}$, where
    $\alpha_{0}=0$ represents the starting frame, and $\alpha_{n_{f}-1}=1$ indicates
    the ending frame. The intermediate values of $\alpha$ correspond to proportionally
    spaced frames within the animation sequence, ensuring linear spacing. This arrangement
    guarantees that each frame represents a weighted blend of the preceding and subsequent
    key stage embeddings, facilitating a smooth and continuous transformation across
    the animation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在动画序列的构建中，插值参数 $\alpha$ 被离散化为一系列值，以促进帧之间的平滑过渡。这种离散化是通过在闭区间 $[0,1]$ 内定义一组等距点来实现的，其中点的数量对应于动画阶段的预期帧数，记作
    $n_{f}$。因此，$\alpha$ 取值为 $\alpha_{0},\alpha_{1},\ldots,\alpha_{n_{f}-1}$，其中 $\alpha_{0}=0$
    代表起始帧，而 $\alpha_{n_{f}-1}=1$ 表示结束帧。$\alpha$ 的中间值对应于动画序列中按比例间隔的帧，确保线性间隔。这种安排保证了每一帧都是前后关键阶段嵌入的加权混合，从而实现动画的平滑连续转换。
- en: To ensure visual continuity in the sequence of intermediate images, we interpolate
    the latent noise of these images using the latent noise from adjacent key stages.
    However, standard linear interpolation may introduce artifacts. To address this,
    we adopt spherical linear interpolation (slerp)  (Shoemake, [1985](#bib.bib35)),
    which effectively minimizes artifacts and enhances the smoothness of transitions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保中间图像序列的视觉连贯性，我们使用邻近关键阶段的潜在噪声对这些图像的潜在噪声进行插值。然而，标准线性插值可能会引入伪影。为了解决这个问题，我们采用了球面线性插值（slerp）
    (Shoemake, [1985](#bib.bib35))，它有效地减少了伪影并增强了过渡的平滑性。
- en: '| (5) |  | $\mathbf{z}_{T\alpha}^{i}=\frac{\sin((1-\alpha)\xi)}{\sin\xi}Z_{i,T}+\frac{\sin(\alpha\xi)}{\sin\xi}Z_{i+1,T}$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\mathbf{z}_{T\alpha}^{i}=\frac{\sin((1-\alpha)\xi)}{\sin\xi}Z_{i,T}+\frac{\sin(\alpha\xi)}{\sin\xi}Z_{i+1,T}$
    |  |'
- en: where $\xi=\arccos\left(\frac{Z_{i,T}Z_{i+1,T}}{\|Z_{i,T}\|\|Z_{i+1,T}\|}\right)$
    .
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\xi=\arccos\left(\frac{Z_{i,T}Z_{i+1,T}}{\|Z_{i,T}\|\|Z_{i+1,T}\|}\right)$。
- en: 'To maintain consistency in the color and luminance aspects of both generated
    and source images, we implement a variant of Adaptive Instance Normalization (AdaIN) (Huang
    and Belongie, [2017](#bib.bib15)) for the pre-denoising stage adjustment of the
    interpolated latent noise $\mathbf{z}_{0\alpha}^{i}$. We calculate and then interpolate
    the means ($\mu$) and standard deviations ($\sigma$) of the latent noises for
    each channel:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在生成图像和源图像的颜色及亮度方面保持一致性，我们对插值潜在噪声 $\mathbf{z}_{0\alpha}^{i}$ 的前去噪阶段调整实施了一种自适应实例归一化（AdaIN）的变体
    (Huang and Belongie, [2017](#bib.bib15))。我们计算并插值每个通道潜在噪声的均值（$\mu$）和标准差（$\sigma$）：
- en: '| (6) |  | $\mu_{\alpha}^{i}=(1-\alpha)\mu_{i}+\alpha\mu_{i+1}$ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\mu_{\alpha}^{i}=(1-\alpha)\mu_{i}+\alpha\mu_{i+1}$ |  |'
- en: '| (7) |  | $\sigma_{\alpha}^{i}=(1-\alpha)\sigma_{i}+\alpha\sigma_{i+1}$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\sigma_{\alpha}^{i}=(1-\alpha)\sigma_{i}+\alpha\sigma_{i+1}$ |  |'
- en: '| (8) |  | $\tilde{\mathbf{z}}_{0\alpha}^{i}=\sigma_{\alpha}\left(\frac{\mathbf{z}_{0\alpha}^{i}-\mu(\mathbf{z}_{0\alpha}^{i})}{\sigma(\mathbf{z}_{0\alpha}^{i})}\right)+\mu_{\alpha}^{i}$
    |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\tilde{\mathbf{z}}_{0\alpha}^{i}=\sigma_{\alpha}\left(\frac{\mathbf{z}_{0\alpha}^{i}-\mu(\mathbf{z}_{0\alpha}^{i})}{\sigma(\mathbf{z}_{0\alpha}^{i})}\right)+\mu_{\alpha}^{i}$
    |  |'
- en: Subsequently, adjusted latent noise $\tilde{\mathbf{z}}_{0\alpha}^{i}$ supplants
    the original $\mathbf{z}_{0\alpha}^{i}$ during the denoising steps, thereby improving
    the brightness and color consistency of the resulting images.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，调整后的潜在噪声 $\tilde{\mathbf{z}}_{0\alpha}^{i}$ 取代了去噪步骤中的原始 $\mathbf{z}_{0\alpha}^{i}$，从而提高了生成图像的亮度和颜色一致性。
- en: '![Refer to caption](img/64e1511823946c65bd8493f59a804ed5.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/64e1511823946c65bd8493f59a804ed5.png)'
- en: Figure 5\. Qualitative evaluation. Our method produces animations that significantly
    outperform previous methods in terms of quality, smoothness, and alignment with
    user input.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 质性评价。我们的方法在质量、平滑度以及与用户输入的对齐方面显著超越了之前的方法。
- en: 'Finally, during the denoising process of each intermediate image $x_{\alpha}^{i}$,
    we also perform feature and self-attention injection. When the stage number $n_{t}$
    is not “-1”, we implement the standard injection strategy, wherein, while generating
    $x_{\alpha}^{i}$, the injection is obtained from $x_{i}$. Furthermore, when SIA’s
    feedback on the “$n_{t}$” is “-1”, it indicates a special request from the user
    for a “single-stage generation,” which involves both texture changes and non-rigid
    transformations within a single animation stage. In such cases, ICA leads the
    model to execute the Hybrid Attention Injection (HAI) strategy. HAI solves the
    issue that when using the normal injection strategies, the model is unable to
    produce animations that simultaneously exhibit changes in texture and structure
    within a single phase. This phenomenon will be further discussed in the [4](#S4
    "4\. Experiments ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient
    Text-conditioned Image-to-Animation"). The HAI process initiates by editing $x_{0}$
    to produce $x_{1}$ using the Feature and Association Injection (FAI), and subsequently
    $x_{2}$ is edited from $x_{1}$ utilizing the KVAI. Following these edits, DDIM
    Inversion is applied to extract the latent representations $Z_{0,T}$ and $Z_{2,T}$,
    which are then interpolated to construct the intermediate latent representation
    $\mathbf{z}_{T\alpha}^{i}$. During the denoising phase, injections are strategically
    administered based on the interpolation parameter $\alpha$; specifically, injections
    from $\{k_{t}^{l},v_{t}^{l}\}$ corresponding to $x_{0}$ are applied in the initial
    (1-$\alpha$)T steps, and those corresponding to $x_{2}$ in the subsequent $\alpha$T
    steps. This method effectively conveys the semantic and structural information
    of significantly transformed images, ensuring smooth and consistent animations
    by querying local structures and textures from input images throughout the denoising
    process.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，在每个中间图像 $x_{\alpha}^{i}$ 的去噪过程中，我们还进行特征和自注意力注入。当阶段编号 $n_{t}$ 不是“-1”时，我们实施标准注入策略，其中，在生成
    $x_{\alpha}^{i}$ 时，注入来自 $x_{i}$。此外，当 SIA 对“$n_{t}$”的反馈为“-1”时，这表示用户对“单阶段生成”的特殊请求，这涉及在单一动画阶段内进行纹理变化和非刚性变换。在这种情况下，ICA
    引导模型执行混合注意力注入（HAI）策略。HAI 解决了当使用正常注入策略时，模型无法在单一阶段内同时展示纹理和结构变化的问题。这个现象将在[4](#S4
    "4\. 实验 ‣ LASER: 无需调整的 LLM 驱动的注意力控制以实现高效的文本条件图像到动画")中进一步讨论。HAI 过程从使用特征和关联注入（FAI）编辑
    $x_{0}$ 以生成 $x_{1}$ 开始，随后从 $x_{1}$ 使用 KVAI 编辑 $x_{2}$。这些编辑之后，应用 DDIM 反演以提取潜在表示
    $Z_{0,T}$ 和 $Z_{2,T}$，然后进行插值以构建中间潜在表示 $\mathbf{z}_{T\alpha}^{i}$。在去噪阶段，注入根据插值参数
    $\alpha$ 战略性地施加；具体来说，来自 $x_{0}$ 的 $\{k_{t}^{l},v_{t}^{l}\}$ 在初始 (1-$\alpha$)T
    步骤中应用，来自 $x_{2}$ 的注入在随后的 $\alpha$T 步骤中应用。这种方法通过在去噪过程中从输入图像中查询局部结构和纹理，有效传达显著变换图像的语义和结构信息，确保平滑一致的动画效果。'
- en: 4\. Experiments
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: 'We employ the publicly available Stable Diffusion v2.1-base (Rombach et al.,
    [2022](#bib.bib31)) as our diffusion model and use GPT-4 8k (OpenAI et al., [2024](#bib.bib26))
    as the LLM in our experiments. For generating the initial image $I_{0}$, we utilize
    two pre-trained models: the real-style model dreamshaper-8 and the anime-style
    model MeinaMix, to assess our model’s capability in producing animations across
    diverse styles. In creating intermediate images, we apply DDIM deterministic sampling.
    For keyframe synthesis, aiming to optimize the balance between efficiency and
    quality, we perform deterministic DDIM inversion with 100 forward steps followed
    by deterministic DDIM sampling with 100 backward steps. When implementing Feature
    and Association Injection (FAI), we inject features and self-attention within
    the first 25 of the 50-step sampling process, specifically targeting layers 4
    to 10 of the U-Net decoder. In the case of Key-Value Attention Injection (KVAI),
    injections commence after the initial five sampling steps and are applied within
    layers 6 to 10 of the decoder. The Hybrid Attention Injection (HAI) method follows
    the same timing and targets the same layers as KVAI. These injection strategies
    can be customized to align with the different input images $I_{0}$. For sampling,
    the classifier-free guidance scale is set at 7.5\. Runtime evaluations are performed
    on an NVIDIA RTX 4090 GPU.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中采用公开的Stable Diffusion v2.1-base（Rombach等，[2022](#bib.bib31)）作为我们的扩散模型，并使用GPT-4
    8k（OpenAI等，[2024](#bib.bib26)）作为大语言模型。为了生成初始图像$I_{0}$，我们使用两个预训练模型：真实风格模型dreamshaper-8和动漫风格模型MeinaMix，以评估我们的模型在生成多样风格动画方面的能力。在创建中间图像时，我们应用DDIM确定性采样。为了优化效率和质量之间的平衡，在关键帧合成中，我们执行确定性DDIM反演，进行100次前向步骤，然后进行100次反向步骤的确定性DDIM采样。当实现特征和关联注入（FAI）时，我们在50步采样过程中的前25步注入特征和自注意力，特别针对U-Net解码器的第4到10层。在进行键值注意力注入（KVAI）时，注入在初始五个采样步骤后开始，并在解码器的第6到10层中进行。混合注意力注入（HAI）方法与KVAI具有相同的时机和目标层。这些注入策略可以根据不同的输入图像$I_{0}$进行定制。对于采样，分类器自由引导尺度设置为7.5。运行时评估在NVIDIA
    RTX 4090 GPU上进行。
- en: 4.1\. Text-conditioned Image-to-Animation Benchmark
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 文本条件图像到动画基准
- en: 'Our method enables text-guided image-to-animation transitions, leveraging either
    image-textual or solely textual descriptions through pre-trained text-to-image
    diffusion models. Due to the lack of benchmarks for such configurations, we have
    proposed a new mini dataset: Text-conditioned Image-to-Animation Benchmark, which
    consists of 100 sets of textual descriptions. The collection comprises 100 sets,
    categorized as follows: 20 sets of animal actions and appearance transformations,
    20 sets focused on animal appearance and species changes, 20 sets depicting transitions
    in natural landscapes and objects, 20 sets related to human figures and alterations
    in painting styles, 10 sets featuring character identity transformations, and
    10 sets concerning changes in object colors and materials. Our model utilizes
    these textual prompts to generate 100 corresponding animation sequences. This
    benchmark serves as a preliminary evaluation of our model’s performance, and we
    hope it will facilitate further research in this direction.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法实现了文本引导的图像到动画过渡，通过预训练的文本到图像扩散模型利用图像文本描述或仅文本描述。由于缺乏此类配置的基准，我们提出了一个新的小型数据集：文本条件图像到动画基准，它包含100组文本描述。该集合包括100组，分类如下：20组动物动作和外观变换，20组关注动物外观和物种变化，20组描绘自然风景和物体过渡，20组涉及人类形象和绘画风格变化，10组涉及角色身份变换，10组涉及物体颜色和材质变化。我们的模型利用这些文本提示生成100个相应的动画序列。这个基准作为我们模型性能的初步评估，希望能够促进该方向的进一步研究。
- en: 4.2\. Qualitative Evaluation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 质量评估
- en: 'We present a visual comparison of our method against prior approaches to underscore
    its superiority. Although there are no other tuning-free methods for text-controlled
    image-to-animation currently available, we draw a detailed comparison with state-of-the-art
    baselines promising for text-controlled image morphing. These include: 1) Diffusion-based
    deep interpolation methods such as DDIM (Song et al., [2020a](#bib.bib36)), Diff.Interp
    (Wang and Golland, [2023](#bib.bib40)), and DiffMorpher (Zhang et al., [2023c](#bib.bib46)),
    all utilizing Stable Diffusion v2.1-base; 2) Text-driven, tuning-free image editing
    methods like PnP (Tumanyan et al., [2023](#bib.bib39)) and MasaCtrl (Cao et al.,
    [2023](#bib.bib7)). For the first category of methods, which depend on multiple
    pre-existing image inputs and lack the capability to generate content directly
    from text, our experimental procedure includes: i) Utilizing LLM control for generating
    consistent outputs, which involves creating initial images and key stage prompts
    through stable diffusion prompt generation, similar to our method. ii) Generating
    initial images using the same stable diffusion checkpoint as employed in our experiments.
    iii) Producing subsequent key stage images via DDIM Inversion. For the second
    category, which also leverages LLM control, we maintain consistent application
    of text embedding interpolation rules to generate intermediate images, aligning
    with our approach.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了我们的方法与之前方法的视觉比较，以突出其优越性。尽管目前没有其他无需调优的文本控制图像到动画的方法可用，但我们与现有的先进基准进行了详细比较，这些基准对文本控制的图像变形有前景。包括：1)
    基于扩散的深度插值方法，如 DDIM（Song et al., [2020a](#bib.bib36)）、Diff.Interp（Wang and Golland,
    [2023](#bib.bib40)）和 DiffMorpher（Zhang et al., [2023c](#bib.bib46)），这些方法均使用 Stable
    Diffusion v2.1-base；2) 文本驱动、无需调优的图像编辑方法，如 PnP（Tumanyan et al., [2023](#bib.bib39)）和
    MasaCtrl（Cao et al., [2023](#bib.bib7)）。对于第一类方法，这些方法依赖于多个现有图像输入且无法直接从文本生成内容，我们的实验程序包括：i)
    利用 LLM 控制生成一致的输出，这涉及通过稳定扩散提示生成创建初始图像和关键阶段提示，与我们的方法类似。ii) 使用与我们实验中相同的稳定扩散检查点生成初始图像。iii)
    通过 DDIM 反演生成后续关键阶段图像。对于第二类方法，这些方法也利用 LLM 控制，我们保持一致应用文本嵌入插值规则来生成中间图像，与我们的方法一致。
- en: Generation Results
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成结果
- en: 'As illustrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4\. Animation Generator ‣ 3\.
    Methodology ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation"), our method outperforms previous approaches in alignment
    with user input, transition smoothness, semantic coherence, and maintaining the
    animation subject’s semantic identity. Previous methods have often failed to accurately
    respond to user input changes in appearance and motion. These approaches typically
    struggle to generate the intended motions accurately or introduce noticeable artifacts
    post-motion changes, often resulting in a significant loss of primary subject
    information in the images. Compared to previous methods, our approach consistently
    generates coherent animations that closely align with the semantic content of
    the user input, resulting in visually satisfactory outputs. For additional examples
    of generated results, we encourage readers to consult the appendix.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [5](#S3.F5 "图 5 ‣ 3.4. 动画生成器 ‣ 3. 方法 ‣ LASER: 无需调优的 LLM 驱动的注意力控制用于高效的文本条件图像到动画")
    所示，我们的方法在用户输入对齐、过渡平滑性、语义一致性以及保持动画主题的语义身份方面优于之前的方法。之前的方法往往未能准确响应用户输入在外观和运动上的变化。这些方法通常难以准确生成预期的动作，或者在动作变化后引入明显的伪影，常常导致图像中的主要主题信息显著丢失。与之前的方法相比，我们的方法始终生成与用户输入的语义内容紧密对齐的连贯动画，产生视觉上令人满意的输出。有关生成结果的更多示例，我们鼓励读者查阅附录。'
- en: Generation Diversity
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成多样性
- en: 'Furthermore, the extensive prior knowledge and generative capabilities of the
    LLM enhance our model’s ability to produce diverse outputs, as shown in Fig. [6](#S4.F6
    "Figure 6 ‣ Generation Efficiency ‣ 4.3\. Quantitative Evaluation ‣ 4\. Experiments
    ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation"). When users request multiple distinct results, our model
    meets this demand by generating high-quality, varied animations, significantly
    broadening its creative potential.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，LLM 的广泛先验知识和生成能力增强了我们模型产生多样化输出的能力，如图 [6](#S4.F6 "图 6 ‣ 生成效率 ‣ 4.3. 定量评估
    ‣ 4. 实验 ‣ LASER: 无需调优的 LLM 驱动的注意力控制用于高效的文本条件图像到动画") 所示。当用户请求多个不同的结果时，我们的模型通过生成高质量、多样化的动画来满足这一需求，显著拓宽了其创意潜力。'
- en: 4.3\. Quantitative Evaluation
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 定量评估
- en: 'Drawing from established objectives in prior research (Yang et al., [2023](#bib.bib43);
    Zhang et al., [2023c](#bib.bib46); Cao et al., [2023](#bib.bib7)), we quantitatively
    evaluate the models using these metrics:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从先前研究中确立的目标出发（Yang 等，[2023](#bib.bib43)；Zhang 等，[2023c](#bib.bib46)；Cao 等，[2023](#bib.bib7)），我们使用这些指标对模型进行定量评估：
- en: (1)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Learned Perceptual Image Patch Similarity (LPIPS, $\downarrow$) (Zhang et al.,
    [2018a](#bib.bib48)): LPIPS is employed to assess the perceptual deviation within
    an animation sequence in our work. We compute the total LPIPS (LPIPS[T]) to quantify
    the overall perceptual variance throughout the sequence, highlighting the dynamic
    range of visual changes. Additionally, the maximum LPIPS to the nearest endpoint
    (LPIPS[M]) is determined to identify the maximum perceptual variance, providing
    insights into the most significant changes within the animation. These measurements
    are crucial for assessing directness of the animation, ensuring finding the most
    efficient transition to generate animation.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习感知图像补丁相似度（LPIPS，$\downarrow$）（Zhang 等，[2018a](#bib.bib48)）：在我们的工作中，LPIPS 用于评估动画序列中的感知偏差。我们计算总
    LPIPS（LPIPS[T]）来量化整个序列中的总体感知差异，突出视觉变化的动态范围。此外，还确定到最近端点的最大 LPIPS（LPIPS[M]），以识别最大感知差异，为动画中的最显著变化提供见解。这些测量对于评估动画的直接性至关重要，确保找到生成动画的最有效过渡。
- en: (2)
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'CLIP Score ($\uparrow$) (Hessel et al., [2021](#bib.bib12)): The CLIP score
    is a metric that quantifies the alignment between images and textual descriptions,
    serving as a powerful tool for evaluating the coherence and relevance of generated
    images about their specified textual prompts. For an intermediate image, we describe
    its CLIP score by calculating its average similarity with the prompt before editing
    and the prompt after editing (e.g., $x_{\alpha}^{0}$ with $P_{0}$ and $P_{1}$,
    $x_{\alpha}^{1}$ with $P_{1}$ and $P_{2}$, etc.).'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CLIP 分数（$\uparrow$）（Hessel 等，[2021](#bib.bib12)）：CLIP 分数是一种量化图像与文本描述之间对齐程度的指标，作为评估生成图像与其指定文本提示之间的连贯性和相关性的强大工具。对于中间图像，我们通过计算其在编辑前与编辑后的提示之间的平均相似度来描述其
    CLIP 分数（例如，$x_{\alpha}^{0}$ 与 $P_{0}$ 和 $P_{1}$，$x_{\alpha}^{1}$ 与 $P_{1}$ 和 $P_{2}$
    等）。
- en: (3)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Perceptual Path Length (PPL, $\downarrow$) (Karras et al., [2020](#bib.bib18)):
    To evaluate smoothness, i.e., transitions within the generated animation sequence
    should be seamless between any two consecutive images, we compute PPL: $\operatorname{PPL}_{\epsilon}=\mathbb{E}_{\alpha\sim
    U(0,1)}[\frac{1}{\epsilon^{2}}\operatorname{LPIPS}(\boldsymbol{x}^{(\alpha)},\boldsymbol{x}^{(\alpha+\varepsilon)})]$,
    where $\epsilon$ is a small constant and we set it to $\frac{1}{n_{f}-1}$. It
    is worth noting that we regard the entire sequence as a single animation process,
    despite consisting of multiple stages.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 感知路径长度（PPL，$\downarrow$）（Karras 等，[2020](#bib.bib18)）：为了评估平滑性，即生成的动画序列中任何两个连续图像之间的过渡应无缝，我们计算
    PPL：$\operatorname{PPL}_{\epsilon}=\mathbb{E}_{\alpha\sim U(0,1)}[\frac{1}{\epsilon^{2}}\operatorname{LPIPS}(\boldsymbol{x}^{(\alpha)},\boldsymbol{x}^{(\alpha+\varepsilon)})]$，其中
    $\epsilon$ 是一个小常数，我们将其设置为 $\frac{1}{n_{f}-1}$。值得注意的是，我们将整个序列视为一个单一的动画过程，尽管它由多个阶段组成。
- en: 'The quantitative evaluation results of all methods are presented in Table [1](#S4.T1
    "Table 1 ‣ Generation Efficiency ‣ 4.3\. Quantitative Evaluation ‣ 4\. Experiments
    ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation").'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '所有方法的定量评估结果显示在表 [1](#S4.T1 "Table 1 ‣ Generation Efficiency ‣ 4.3\. Quantitative
    Evaluation ‣ 4\. Experiments ‣ LASER: Tuning-Free LLM-Driven Attention Control
    for Efficient Text-conditioned Image-to-Animation") 中。'
- en: Generation Quality
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成质量
- en: 'Our method achieves a leading Clip Score, demonstrating its semantic alignment
    with user input. While DDIM may excel in CLIP Score, it often compromises structural
    coherence in favour of textual alignment, an issue our approach adeptly avoids
    as demonstrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4\. Animation Generator ‣ 3\.
    Methodology ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation"). Due to PnP’s inability to effectively perform non-rigid
    edits, its generated animations often exhibit only appearance changes, thereby
    achieving higher levels of smoothness. This limitation hinders its capability
    to handle a diverse range of animation generation tasks. Similarly, Masacontrol,
    which struggles with texture transformations, also falls short in producing a
    diverse range of animations. Even when benchmarked against deep interpolation
    techniques that require fine-tuning, our results consistently exhibit superior
    smoothness. This performance not only underscores the effectiveness of our method
    but also affirms its suitability for adapting to a wide range of text-conditioned
    image-to-animation generation scenarios.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法在 Clip Score 上取得了领先，展示了其与用户输入的语义一致性。虽然 DDIM 在 CLIP Score 上可能表现突出，但它常常为了文本对齐而牺牲结构一致性，我们的方法巧妙地避免了这个问题，如图
    [5](#S3.F5 "Figure 5 ‣ 3.4\. Animation Generator ‣ 3\. Methodology ‣ LASER: Tuning-Free
    LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation")
    所示。由于 PnP 无法有效进行非刚性编辑，其生成的动画往往仅展示外观变化，因此能够达到更高的平滑度。这一限制阻碍了其处理多样化动画生成任务的能力。类似地，Masacontrol
    在纹理转换方面表现不佳，也难以生成多样化的动画。即使与需要微调的深度插值技术相比，我们的结果也 consistently 展现出更优越的平滑度。这一性能不仅突显了我们方法的有效性，还确认了其适应各种文本条件图像到动画生成场景的适用性。'
- en: Generation Efficiency
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成效率
- en: In assessing efficiency, our method uniquely blends quality with speed, setting
    it apart from other deep interpolation techniques. This superior performance primarily
    stems from our operation without the need for fine-tuning. Although it may not
    lead in all image editing benchmarks, our model excels at managing a diverse array
    of edits, enabling it to adeptly tackle a broad spectrum of generative tasks.
    Given this versatility, the efficiency of our method is exceptionally high.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在效率评估方面，我们的方法独特地将质量与速度结合起来，使其与其他深度插值技术不同。这一优越性能主要源于我们的方法无需微调。虽然它可能在所有图像编辑基准中不领先，但我们的模型在管理各种编辑方面表现出色，使其能够熟练地处理广泛的生成任务。鉴于这一多样性，我们的方法的效率异常高。
- en: '![Refer to caption](img/394c25aaca56f4b1d9cb490da93dd880.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/394c25aaca56f4b1d9cb490da93dd880.png)'
- en: Figure 6\. The rich prior knowledge of the LLM grants the model the ability
    to generate diverse outcomes from the same input text and image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. LLM 的丰富先验知识赋予了模型从相同的输入文本和图像生成多样化结果的能力。
- en: Table 1\. Comparison of current methods. Superscript ^($\clubsuit$) indicates
    that the model employs an external network (*e.g.*, ControlNet (Zhang et al.,
    [2023b](#bib.bib47))) to generate intermediate images and ^† indicates that the
    model fine-tunes with training LoRA (Hu et al., [2021](#bib.bib14)). “TE” stands
    for texture editing, a process that involves altering the surface appearance of
    objects within an image to match a specific texture style, while preserving the
    underlying structure and layout of the scene. “NRIE” refers to non-rigid image
    editing which involves altering the shape and structure of objects in images,
    like changing facial expressions or body poses. “AG”, standing for animation generation,
    refers to producing intermediate images between keyframes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 当前方法的比较。上标 ^($\clubsuit$) 表示模型使用外部网络（*例如*，ControlNet (Zhang et al., [2023b](#bib.bib47)))
    生成中间图像，^† 表示模型通过训练 LoRA (Hu et al., [2021](#bib.bib14)) 进行微调。“TE” 代表纹理编辑，即改变图像中物体的表面外观以匹配特定纹理风格，同时保持场景的基本结构和布局。“NRIE”
    指的是非刚性图像编辑，即改变图像中物体的形状和结构，如改变面部表情或身体姿势。“AG” 代表动画生成，即在关键帧之间生成中间图像。
- en: '|   Method | Characteristics | Metrics | Runtime$\downarrow$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 特性 | 评估指标 | 运行时间$\downarrow$ |'
- en: '| 4  |  |  |  |  |  |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 4 | | | | | | | |'
- en: '| 8  |  |  |  |  |  |  |  |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 8 | | | | | | | | |'
- en: '|  | TE | NRIE | AG | CLIP Score $\uparrow$ | LPIPS[$T$] $\downarrow$ | LPIPS[$M$]
    $\downarrow$ | PPL $\downarrow$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| | TE | NRIE | AG | CLIP Score $\uparrow$ | LPIPS[$T$] $\downarrow$ | LPIPS[$M$]
    $\downarrow$ | PPL $\downarrow$ | |'
- en: '|   DDIM(Song et al., [2020a](#bib.bib36)) | ✓ |  |  | 27.37 | 3.13 | 0.49
    | 36.91 | 32s |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| DDIM(Song et al., [2020a](#bib.bib36)) | ✓ | | | 27.37 | 3.13 | 0.49 | 36.91
    | 32s |'
- en: '| PnP(Hertz et al., [2022](#bib.bib11)) | ✓ |  |  | 26.57 | 0.90 | 0.23 | 10.51
    | 2min |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| PnP(Hertz et al., [2022](#bib.bib11)) | ✓ |  |  | 26.57 | 0.90 | 0.23 | 10.51
    | 2分钟 |'
- en: '| MasaCtrl(Cao et al., [2023](#bib.bib7)) |  | ✓ |  | 26.56 | 1.54 | 0.28 |
    17.96 | 37s |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| MasaCtrl(Cao et al., [2023](#bib.bib7)) |  | ✓ |  | 26.56 | 1.54 | 0.28 |
    17.96 | 37秒 |'
- en: '| Diff.Interp^($\clubsuit$)(Wang and Golland, [2023](#bib.bib40)) |  |  | ✓
    | 20.05 | 5.14 | 0.58 | 72.29 | 2min6s |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Diff.Interp^($\clubsuit$)(Wang and Golland, [2023](#bib.bib40)) |  |  | ✓
    | 20.05 | 5.14 | 0.58 | 72.29 | 2分钟6秒 |'
- en: '| DiffMorpher^†(Zhang et al., [2023c](#bib.bib46)) |  |  | ✓ | 26.94 | 0.99
    | 0.40 | 14.78 | 1min46s |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| DiffMorpher^†(Zhang et al., [2023c](#bib.bib46)) |  |  | ✓ | 26.94 | 0.99
    | 0.40 | 14.78 | 1分46秒 |'
- en: '| Ours | ✓ | ✓ | ✓ | 26.99 | 1.22 | 0.25 | 14.14 | 41s |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | ✓ | ✓ | ✓ | 26.99 | 1.22 | 0.25 | 14.14 | 41秒 |'
- en: '|   |  |  |  |  |  |  |  |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |'
- en: 4.4\. Ablation Study
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 消融研究
- en: 'We have conducted an ablation study to evaluate the effectiveness of the proposed
    components, with experimental results shown in Table [2](#S4.T2 "Table 2 ‣ 4.4\.
    Ablation Study ‣ 4\. Experiments ‣ LASER: Tuning-Free LLM-Driven Attention Control
    for Efficient Text-conditioned Image-to-Animation") and Fig.2 (in Appendix). The
    findings demonstrate that using DDIM alone cannot accurately restore the structure
    of the input image. In contrast, our feature and self-attention injections address
    the loss of texture and structural information during the DDIM generation process,
    significantly enhancing the quality of the generated animations. However, remnants
    of structural features from the initial state are still noticeable in the generated
    animations, including in the intermediate segments. The implementation of Latent
    Interpolation addresses this issue. While it may slightly elevate the $LPIPS_{T}$
    and PPL metrics, it ensures that subsequent frames more accurately reflect the
    semantic information of the transformed state. After applying the AdaIN adjustment
    to the latent noise, the consistency of brightness and color across the image
    sequence has improved.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了消融研究以评估所提出组件的有效性，实验结果如表 [2](#S4.T2 "Table 2 ‣ 4.4\. Ablation Study ‣ 4\.
    Experiments ‣ LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned
    Image-to-Animation")和图2（附录中）所示。结果表明，仅使用DDIM无法准确恢复输入图像的结构。相比之下，我们的特征和自注意力注入解决了DDIM生成过程中纹理和结构信息的丧失，显著提高了生成动画的质量。然而，生成的动画中仍然能看到来自初始状态的结构特征残留，包括中间片段。潜在插值的实施解决了这一问题。虽然它可能略微提高了
    $LPIPS_{T}$ 和 PPL 指标，但它确保后续帧更准确地反映了变换状态的语义信息。在对潜在噪声应用AdaIN调整后，图像序列的亮度和颜色一致性得到了改善。'
- en: 'To demonstrate the effectiveness of Hybrid Attention Injection (HAI) in producing
    single-stage animations that incorporate both texture changes and non-rigid transformations,
    we conducted qualitative experiments. We generated animations using basic injection
    strategies (FAI and KVAI) and HAI, with the results displayed in Fig. [7](#S4.F7
    "Figure 7 ‣ 4.4\. Ablation Study ‣ 4\. Experiments ‣ LASER: Tuning-Free LLM-Driven
    Attention Control for Efficient Text-conditioned Image-to-Animation"). When employing
    only FAI, the images failed to respond to non-rigid changes; using KVAI alone
    did not result in significant texture modifications. Our proposed HAI strategy
    successfully handles both texture and non-rigid changes, effectively fulfilling
    the task of single-stage animation generation.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示**混合注意力注入（HAI）**在生成包含纹理变化和非刚性变换的单阶段动画中的有效性，我们进行了定性实验。我们使用基本注入策略（FAI和KVAI）以及HAI生成动画，结果如图
    [7](#S4.F7 "Figure 7 ‣ 4.4\. Ablation Study ‣ 4\. Experiments ‣ LASER: Tuning-Free
    LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation")所示。当仅使用FAI时，图像未能对非刚性变化做出反应；单独使用KVAI则未能显著修改纹理。我们提出的HAI策略成功处理了纹理和非刚性变化，有效地完成了单阶段动画生成的任务。'
- en: '![Refer to caption](img/0f5cdcc473a8a4a3b027a9d33675d2b9.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f5cdcc473a8a4a3b027a9d33675d2b9.png)'
- en: Figure 7\. The comparative effects of different injection strategies given the
    textual description “A sitting cat turns into a jumping dog”.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 给定文本描述“坐着的猫变成跳跃的狗”不同注入策略的比较效果。
- en: Table 2\. Ablation study results. Injection, Latent Interp, and AdaIN represent
    different components studied in the ablation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 消融研究结果。注入、潜在插值和AdaIN代表了在消融中研究的不同组件。
- en: '|   Method | Components | Metrics |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|   方法 | 组件 | 指标 |'
- en: '| 4  |  |  |  |  |  |  |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 4  |  |  |  |  |  |  |  |'
- en: '| 8  |  |  |  |  |  |  |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 8  |  |  |  |  |  |  |  |'
- en: '|  | Injection | Latent Interp | AdaIN | Clip Score $\uparrow$ | LPIPS[$T$]
    $\downarrow$ | LPIPS[$M$] $\downarrow$ | PPL $\downarrow$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 注入 | 潜在插值 | AdaIN | Clip Score $\uparrow$ | LPIPS[$T$] $\downarrow$ |
    LPIPS[$M$] $\downarrow$ | PPL $\downarrow$ |'
- en: '|   DDIM(Song et al., [2020a](#bib.bib36)) |  |  |  | 27.37 | 3.13 | 0.49 |
    36.91 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|   DDIM(Song et al., [2020a](#bib.bib36)) |  |  |  | 27.37 | 3.13 | 0.49 |
    36.91 |'
- en: '| - | ✓ |  |  | 26.73 | 1.09 | 0.26 | 12.85 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| - | ✓ |  |  | 26.73 | 1.09 | 0.26 | 12.85 |'
- en: '| - | ✓ | ✓ |  | 26.81 | 1.22 | 0.26 | 14.03 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| - | ✓ | ✓ |  | 26.81 | 1.22 | 0.26 | 14.03 |'
- en: '| Ours | ✓ | ✓ | ✓ | 26.99 | 1.22 | 0.25 | 14.14 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | ✓ | ✓ | ✓ | 26.99 | 1.22 | 0.25 | 14.14 |'
- en: '|   |  |  |  |  |  |  |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |'
- en: 5\. Conclusion
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: We introduce LASER, a tuning-free LLM-driven attention control framework that
    utilizes pre-trained text-to-image models to generate high-quality and smooth
    animations from multimodal inputs. Experimental results validate the superior
    performance of our method, which consistently produces diverse and high-quality
    animations. We believe our approach demonstrates significant potential and serves
    as an inspiration for future research in this field.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 LASER，一种无需调优的 LLM 驱动的注意力控制框架，利用预训练的文本到图像模型，从多模态输入生成高质量和平滑的动画。实验结果验证了我们方法的优越性能，能够持续产生多样化和高质量的动画。我们相信，我们的方法展示了显著的潜力，并为未来的研究提供了灵感。
- en: References
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*
    (2023).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等. 2023. GPT-4 技术报告。*arXiv 预印本 arXiv:2303.08774* (2023).
- en: 'Aloraibi (2023) Alyaa Qusay Aloraibi. 2023. Image morphing techniques: A review.
    (2023).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aloraibi (2023) Alyaa Qusay Aloraibi. 2023. 图像变形技术：综述。 (2023).
- en: Bao et al. (2023) Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole
    Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. 2023. One transformer fits all
    distributions in multi-modal diffusion at scale. In *International Conference
    on Machine Learning*. PMLR, 1692–1717.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao et al. (2023) Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole
    Wang, Gang Yue, Yue Cao, Hang Su, 和 Jun Zhu. 2023. 单一变换器适配所有分布的多模态扩散。*国际机器学习会议论文集*。PMLR,
    1692–1717.
- en: Brock et al. (2018) Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large
    scale GAN training for high fidelity natural image synthesis. *arXiv preprint
    arXiv:1809.11096* (2018).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brock et al. (2018) Andrew Brock, Jeff Donahue, 和 Karen Simonyan. 2018. 大规模
    GAN 训练用于高保真自然图像合成。*arXiv 预印本 arXiv:1809.11096* (2018).
- en: 'Brooks et al. (2023) Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2023.
    Instructpix2pix: Learning to follow image editing instructions. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 18392–18402.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brooks et al. (2023) Tim Brooks, Aleksander Holynski, 和 Alexei A Efros. 2023.
    Instructpix2pix：学习遵循图像编辑指令。*IEEE/CVF 计算机视觉与模式识别会议论文集*。18392–18402.
- en: 'Cao et al. (2023) Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu
    Qie, and Yinqiang Zheng. 2023. Masactrl: Tuning-free mutual self-attention control
    for consistent image synthesis and editing. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 22560–22570.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2023) Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu
    Qie, 和 Yinqiang Zheng. 2023. Masactrl：无需调优的互相自注意力控制用于一致的图像合成与编辑。*IEEE/CVF 国际计算机视觉会议论文集*。22560–22570.
- en: 'Crowson et al. (2022) Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell
    Stander, Eric Hallahan, Louis Castricato, and Edward Raff. 2022. Vqgan-clip: Open
    domain image generation and editing with natural language guidance. In *European
    Conference on Computer Vision*. Springer, 88–105.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crowson et al. (2022) Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell
    Stander, Eric Hallahan, Louis Castricato, 和 Edward Raff. 2022. Vqgan-clip：开放域图像生成与自然语言指导的编辑。*欧洲计算机视觉会议论文集*。Springer,
    88–105.
- en: Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion
    models beat gans on image synthesis. *Advances in neural information processing
    systems* 34 (2021), 8780–8794.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhariwal 和 Nichol (2021) Prafulla Dhariwal 和 Alexander Nichol. 2021. 扩散模型在图像合成上超越
    GAN。*神经信息处理系统进展* 34 (2021), 8780–8794.
- en: Esser et al. (2021) Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming
    transformers for high-resolution image synthesis. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*. 12873–12883.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser et al. (2021) Patrick Esser, Robin Rombach, 和 Bjorn Ommer. 2021. 驯化变换器用于高分辨率图像合成。*IEEE/CVF
    计算机视觉与模式识别会议论文集*。12873–12883.
- en: Hertz et al. (2022) Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael
    Pritch, and Daniel Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention
    control. *arXiv preprint arXiv:2208.01626* (2022).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hertz 等（2022）阿米尔·赫茨、罗恩·莫卡迪、杰伊·特嫩鲍姆、基弗·阿伯曼、雅艾尔·普里奇和丹尼尔·科恩-奥尔。2022。基于交叉注意力控制的
    prompt-to-prompt 图像编辑。*arXiv 预印本 arXiv:2208.01626*（2022）。
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. Clipscore: A reference-free evaluation metric for image
    captioning. *arXiv preprint arXiv:2104.08718* (2021).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hessel 等（2021）杰克·赫塞尔、阿里·霍尔茨曼、麦克斯韦·福布斯、罗南·勒布拉斯和叶进。2021。Clipscore：一种无参考的图像字幕评价指标。*arXiv
    预印本 arXiv:2104.08718*（2021）。
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    diffusion probabilistic models. *Advances in neural information processing systems*
    33 (2020), 6840–6851.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等（2020）乔纳森·霍、阿贾伊·贾因和皮特·阿贝尔。2020。去噪扩散概率模型。*神经信息处理系统进展* 33（2020），6840–6851。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685* (2021).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）爱德华·J·胡、沈也龙、菲利普·沃利斯、泽元·艾伦-朱、李源志、王山、王璐和陈维柱。2021。Lora：大规模语言模型的低秩适配。*arXiv
    预印本 arXiv:2106.09685*（2021）。
- en: Huang and Belongie (2017) Xun Huang and Serge Belongie. 2017. Arbitrary style
    transfer in real-time with adaptive instance normalization. In *Proceedings of
    the IEEE international conference on computer vision*. 1501–1510.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 和 Belongie（2017）黄勋和塞尔日·贝隆吉。2017。实时任意风格转换与自适应实例归一化。发表于 *IEEE 国际计算机视觉会议论文集*。1501–1510。
- en: Karras et al. (2021) Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen,
    Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. Alias-free generative adversarial
    networks. *Advances in neural information processing systems* 34 (2021), 852–863.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等（2021）特罗·卡拉斯、米卡·艾塔拉、萨穆利·莱恩、埃里克·赫尔科宁、贾内·赫尔斯滕、贾科·莱赫廷和蒂莫·艾拉。2021。无别名生成对抗网络。*神经信息处理系统进展*
    34（2021），852–863。
- en: Karras et al. (2019) Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based
    generator architecture for generative adversarial networks. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*. 4401–4410.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等（2019）特罗·卡拉斯、萨穆利·莱恩和蒂莫·艾拉。2019。用于生成对抗网络的基于风格的生成器架构。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*。4401–4410。
- en: Karras et al. (2020) Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
    Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality
    of stylegan. In *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*. 8110–8119.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras 等（2020）特罗·卡拉斯、萨穆利·莱恩、米卡·艾塔拉、贾内·赫尔斯滕、贾科·莱赫廷和蒂莫·艾拉。2020。分析和改进 StyleGAN
    的图像质量。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*。8110–8119。
- en: 'Kawar et al. (2023) Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
    Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real
    image editing with diffusion models. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 6007–6017.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kawar 等（2023）巴赫贾特·卡瓦尔、希兰·扎达、奥兰·朗、奥梅尔·托夫、惠文·张、塔利·德克尔、因巴尔·莫塞里和米哈尔·伊拉尼。2023。Imagic：基于文本的真实图像编辑与扩散模型。发表于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*。6007–6017。
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114* (2013).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Welling（2013）迪德里克·P·金马和马克斯·威林。2013。自编码变分贝叶斯。*arXiv 预印本 arXiv:1312.6114*（2013）。
- en: 'Li et al. (2020) Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS Torr.
    2020. Manigan: Text-guided image manipulation. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*. 7880–7889.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）李博文、齐晓娟、托马斯·卢卡谢维奇和菲利普·HS·托尔。2020。Manigan：文本引导的图像操控。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*。7880–7889。
- en: Li et al. (2023) Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao
    Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. 2023. Fine-tuning
    multimodal llms to follow zero-shot demonstrative instructions. In *The Twelfth
    International Conference on Learning Representations*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）李俊程、潘凯航、葛志奇、高铭赫、季伟、张文乔、蔡达生、唐思良、张汉旺和庄越廷。2023。微调多模态 llms 以遵循零-shot 示范指令。发表于
    *第十二届国际学习表征会议*。
- en: 'Lian et al. (2024) Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. 2024.
    LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
    Models with Large Language Models. arXiv:2305.13655 [cs.CV]'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lian 等人（2024）Long Lian、Boyi Li、Adam Yala 和 Trevor Darrell。2024年。LLM-基础扩散：利用大型语言模型提升文本到图像扩散模型的提示理解。arXiv:2305.13655
    [cs.CV]
- en: 'Nam et al. (2018) Seonghyeon Nam, Yunji Kim, and Seon Joo Kim. 2018. Text-adaptive
    generative adversarial networks: manipulating images with natural language. *Advances
    in neural information processing systems* 31 (2018).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nam 等人（2018）Seonghyeon Nam、Yunji Kim 和 Seon Joo Kim。2018年。文本自适应生成对抗网络：使用自然语言操控图像。*神经信息处理系统进展*
    31 (2018)。
- en: Nichol and Dhariwal (2021) Alexander Quinn Nichol and Prafulla Dhariwal. 2021.
    Improved denoising diffusion probabilistic models. In *International conference
    on machine learning*. PMLR, 8162–8171.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol 和 Dhariwal（2021）Alexander Quinn Nichol 和 Prafulla Dhariwal。2021年。改进的去噪扩散概率模型。发表于
    *国际机器学习大会*。PMLR, 8162–8171。
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等（2024）OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul
    Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine,
    Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine
    Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button,
    Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory
    Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby
    Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung,
    Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah
    Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,
    Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
    Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian
    Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon,
    Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei
    Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke,
    Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny
    Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang,
    Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn,
    Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish
    Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim,
    Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute
    Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael Pokorny, Michelle
    Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth
    Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis
    Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario
    Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David
    Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica
    Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan
    Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk 和 Barret
    Zoph. 2024. GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: Parmar et al. (2023) Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
    Li, Jingwan Lu, and Jun-Yan Zhu. 2023. Zero-shot image-to-image translation. In
    *ACM SIGGRAPH 2023 Conference Proceedings*. 1–11.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parmar et al. (2023) Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
    Li, Jingwan Lu, 和 Jun-Yan Zhu. 2023. 零-shot 图像到图像翻译。见 *ACM SIGGRAPH 2023 会议论文集*。1–11。
- en: 'Patashnik et al. (2021) Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
    and Dani Lischinski. 2021. Styleclip: Text-driven manipulation of stylegan imagery.
    In *Proceedings of the IEEE/CVF international conference on computer vision*.
    2085–2094.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patashnik et al. (2021) Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
    和 Dani Lischinski. 2021. Styleclip: 基于文本的 Stylegan 图像操控。见 *IEEE/CVF 国际计算机视觉会议论文集*。2085–2094。'
- en: 'Podell et al. (2023) Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,
    Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving
    latent diffusion models for high-resolution image synthesis. *arXiv preprint arXiv:2307.01952*
    (2023).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Podell et al. (2023) Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,
    Tim Dockhorn, Jonas Müller, Joe Penna, 和 Robin Rombach. 2023. Sdxl: 改进潜在扩散模型以进行高分辨率图像合成。*arXiv
    预印本 arXiv:2307.01952* (2023)。'
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. Learning transferable visual models from natural language
    supervision. In *International conference on machine learning*. PMLR, 8748–8763.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. 从自然语言监督中学习可转移的视觉模型。见 *国际机器学习会议*。PMLR, 8748–8763。
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion
    models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*. 10684–10695.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, 和 Björn Ommer. 2022. 使用潜在扩散模型进行高分辨率图像合成。见 *IEEE/CVF 计算机视觉与模式识别会议论文集*。10684–10695。
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,
    Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with
    deep language understanding. *Advances in neural information processing systems*
    35 (2022), 36479–36494.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol
    Ayan, Tim Salimans, et al. 2022. 具有深度语言理解的逼真文本到图像扩散模型。*神经信息处理系统进展* 35 (2022),
    36479–36494。
- en: 'Sauer et al. (2023) Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger,
    and Timo Aila. 2023. Stylegan-t: Unlocking the power of gans for fast large-scale
    text-to-image synthesis. In *International conference on machine learning*. PMLR,
    30105–30118.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sauer et al. (2023) Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger,
    和 Timo Aila. 2023. Stylegan-t: 解锁 GANs 的力量以实现快速大规模文本到图像合成。见 *国际机器学习会议*。PMLR, 30105–30118。'
- en: 'Sauer et al. (2022) Axel Sauer, Katja Schwarz, and Andreas Geiger. 2022. Stylegan-xl:
    Scaling stylegan to large diverse datasets. In *ACM SIGGRAPH 2022 conference proceedings*.
    1–10.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sauer et al. (2022) Axel Sauer, Katja Schwarz, 和 Andreas Geiger. 2022. Stylegan-xl:
    扩展 Stylegan 以处理大型多样化数据集。见 *ACM SIGGRAPH 2022 会议论文集*。1–10。'
- en: Shoemake (1985) Ken Shoemake. 1985. Animating rotation with quaternion curves.
    In *Proceedings of the 12th annual conference on Computer graphics and interactive
    techniques*. 245–254.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shoemake (1985) Ken Shoemake. 1985. 使用四元数曲线进行旋转动画。见 *第12届计算机图形学与交互技术年会论文集*。245–254。
- en: Song et al. (2020a) Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020a. Denoising
    diffusion implicit models. *arXiv preprint arXiv:2010.02502* (2020).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2020a) Jiaming Song, Chenlin Meng, 和 Stefano Ermon. 2020a. 去噪扩散隐式模型。*arXiv
    预印本 arXiv:2010.02502* (2020)。
- en: Song et al. (2020b) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. 2020b. Score-based generative modeling through
    stochastic differential equations. *arXiv preprint arXiv:2011.13456* (2020).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2020b) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek
    Kumar, Stefano Ermon, 和 Ben Poole. 2020b. 基于评分的生成建模通过随机微分方程。*arXiv 预印本 arXiv:2011.13456*
    (2020)。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: 开放和高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*
    (2023)。'
- en: Tumanyan et al. (2023) Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel.
    2023. Plug-and-play diffusion features for text-driven image-to-image translation.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    1921–1930.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tumanyan 等人 (2023) Narek Tumanyan、Michal Geyer、Shai Bagon 和 Tali Dekel. 2023.
    插件式扩散特征用于基于文本的图像到图像转换. 见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，1921–1930。
- en: Wang and Golland (2023) Clinton Wang and Polina Golland. 2023. Interpolating
    between images with diffusion models. (2023).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Golland (2023) Clinton Wang 和 Polina Golland. 2023. 使用扩散模型在图像之间插值. (2023)。
- en: 'Xia et al. (2021) Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. 2021.
    Tedigan: Text-guided diverse face image generation and manipulation. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*. 2256–2265.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia 等人 (2021) Weihao Xia、Yujiu Yang、Jing-Hao Xue 和 Baoyuan Wu. 2021. Tedigan:
    基于文本的多样化人脸图像生成与操控. 见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2256–2265。'
- en: 'Xu et al. (2018) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation
    with attentional generative adversarial networks. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*. 1316–1324.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 (2018) Tao Xu、Pengchuan Zhang、Qiuyuan Huang、Han Zhang、Zhe Gan、Xiaolei
    Huang 和 Xiaodong He. 2018. Attngan: 使用注意力生成对抗网络进行细粒度文本到图像生成. 见于 *IEEE 计算机视觉与模式识别会议论文集*，1316–1324。'
- en: 'Yang et al. (2023) Zhaoyuan Yang, Zhengyang Yu, Zhiwei Xu, Jaskirat Singh,
    Jing Zhang, Dylan Campbell, Peter Tu, and Richard Hartley. 2023. Impus: Image
    morphing with perceptually-uniform sampling using diffusion models. *arXiv preprint
    arXiv:2311.06792* (2023).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2023) Zhaoyuan Yang、Zhengyang Yu、Zhiwei Xu、Jaskirat Singh、Jing Zhang、Dylan
    Campbell、Peter Tu 和 Richard Hartley. 2023. Impus: 使用扩散模型的感知均匀采样图像变形. *arXiv 预印本
    arXiv:2311.06792* (2023)。'
- en: 'Zhang et al. (2017) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. In *Proceedings
    of the IEEE international conference on computer vision*. 5907–5915.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2017) Han Zhang、Tao Xu、Hongsheng Li、Shaoting Zhang、Xiaogang Wang、Xiaolei
    Huang 和 Dimitris N Metaxas. 2017. Stackgan: 基于堆叠生成对抗网络的文本到照片级真实图像合成. 见于 *IEEE
    国际计算机视觉会议论文集*，5907–5915。'
- en: 'Zhang et al. (2018b) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. 2018b. Stackgan++: Realistic image
    synthesis with stacked generative adversarial networks. *IEEE transactions on
    pattern analysis and machine intelligence* 41, 8 (2018), 1947–1962.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2018b) Han Zhang、Tao Xu、Hongsheng Li、Shaoting Zhang、Xiaogang Wang、Xiaolei
    Huang 和 Dimitris N Metaxas. 2018b. Stackgan++: 使用堆叠生成对抗网络进行真实感图像合成. *IEEE 模式分析与机器智能期刊*
    41, 8 (2018), 1947–1962。'
- en: 'Zhang et al. (2023c) Kaiwen Zhang, Yifan Zhou, Xudong Xu, Xingang Pan, and
    Bo Dai. 2023c. DiffMorpher: Unleashing the Capability of Diffusion Models for
    Image Morphing. *arXiv preprint arXiv:2312.07409* (2023).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2023c) Kaiwen Zhang、Yifan Zhou、Xudong Xu、Xingang Pan 和 Bo Dai. 2023c.
    DiffMorpher: 发掘扩散模型在图像变形中的能力. *arXiv 预印本 arXiv:2312.07409* (2023)。'
- en: Zhang et al. (2023b) Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023b. Adding
    conditional control to text-to-image diffusion models. In *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*. 3836–3847.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2023b) Lvmin Zhang、Anyi Rao 和 Maneesh Agrawala. 2023b. 为文本到图像的扩散模型添加条件控制.
    见于 *IEEE/CVF 国际计算机视觉会议论文集*，3836–3847。
- en: Zhang et al. (2018a) Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
    and Oliver Wang. 2018a. The unreasonable effectiveness of deep features as a perceptual
    metric. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. 586–595.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2018a) Richard Zhang、Phillip Isola、Alexei A Efros、Eli Shechtman 和
    Oliver Wang. 2018a. 深度特征作为感知度量的非凡有效性. 见于 *IEEE 计算机视觉与模式识别会议论文集*，586–595。
- en: 'Zhang et al. (2024) Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan
    Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, et al. 2024. HyperLLaVA:
    Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models.
    *arXiv preprint arXiv:2403.13447* (2024).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2024) Wenqiao Zhang、Tianwei Lin、Jiang Liu、Fangxun Shu、Haoyuan Li、Lei
    Zhang、He Wanggui、Hao Zhou、Zheqi Lv、Hao Jiang 等. 2024. HyperLLaVA: 多模态大语言模型的动态视觉与语言专家调优.
    *arXiv 预印本 arXiv:2403.13447* (2024)。'
- en: Zhang et al. (2023a) Wenqiao Zhang, Zheqi Lv, Hao Zhou, Jia-Wei Liu, Juncheng
    Li, Mengze Li, Siliang Tang, and Yueting Zhuang. 2023a. Revisiting the Domain
    Shift and Sample Uncertainty in Multi-source Active Domain Transfer. *arXiv preprint
    arXiv:2311.12905* (2023).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023a) Wenqiao Zhang, Zheqi Lv, Hao Zhou, Jia-Wei Liu, Juncheng Li,
    Mengze Li, Siliang Tang 和 Yueting Zhuang. 2023a. 重新审视多源主动领域迁移中的领域偏移和样本不确定性。*arXiv
    预印本 arXiv:2311.12905* (2023)。
- en: Zhang et al. (2019) Wenqiao Zhang, Siliang Tang, Yanpeng Cao, Shiliang Pu, Fei
    Wu, and Yueting Zhuang. 2019. Frame augmented alternating attention network for
    video question answering. *IEEE Transactions on Multimedia* 22, 4 (2019), 1032–1041.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019) Wenqiao Zhang, Siliang Tang, Yanpeng Cao, Shiliang Pu, Fei Wu
    和 Yueting Zhuang. 2019. 用于视频问答的帧增强交替注意力网络。*IEEE 多媒体学报* 22, 4 (2019), 1032–1041。
- en: 'Zhang et al. (2022) Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu Zhang,
    Andrew Makmur, Qingpeng Cai, and Beng Chin Ooi. 2022. Boostmis: Boosting medical
    image semi-supervised learning with adaptive pseudo labeling and informative active
    annotation. In *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*. 20666–20676.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2022) Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu Zhang, Andrew
    Makmur, Qingpeng Cai 和 Beng Chin Ooi. 2022. Boostmis: 利用自适应伪标签和信息性主动标注提升医学图像半监督学习。在
    *IEEE/CVF 计算机视觉与模式识别大会论文集*。20666–20676。'
- en: 'Zhu et al. (2019) Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019. Dm-gan:
    Dynamic memory generative adversarial networks for text-to-image synthesis. In
    *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*.
    5802–5810.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等 (2019) Minfeng Zhu, Pingbo Pan, Wei Chen 和 Yi Yang. 2019. Dm-gan: 动态记忆生成对抗网络用于文本到图像的合成。在
    *IEEE/CVF 计算机视觉与模式识别大会论文集*。5802–5810。'
- en: Zope and Zope (2017) Bhushan Zope and Soniya B Zope. 2017. A Survey of Morphing
    Techniques. *International Journal of Advanced Engineering, Management and Science*
    3, 2 (2017), 239773.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zope 和 Zope (2017) Bhushan Zope 和 Soniya B Zope. 2017. 一项形态变化技术的调查。*国际先进工程、管理与科学杂志*
    3, 2 (2017), 239773。
