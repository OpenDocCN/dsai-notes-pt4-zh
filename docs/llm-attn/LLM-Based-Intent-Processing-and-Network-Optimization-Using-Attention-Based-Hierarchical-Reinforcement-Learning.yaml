- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:53:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的意图处理和使用基于注意力的层次强化学习进行网络优化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06059](https://ar5iv.labs.arxiv.org/html/2406.06059)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06059](https://ar5iv.labs.arxiv.org/html/2406.06059)
- en: Md Arafat Habib¹, Pedro Enrique Iturria Rivera¹, Yigit Ozcan², Medhat Elsayed²,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Md Arafat Habib¹, Pedro Enrique Iturria Rivera¹, Yigit Ozcan², Medhat Elsayed²,
- en: Majid Bavand², Raimundus Gaigalas² and Melike Erol-Kantarci¹, ²Ericsson Inc.,
    Ottawa, Canada Emails:{mhabi050, pitur008, melike.erolkantarci}@uottawa.ca,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Majid Bavand², Raimundus Gaigalas² 和 Melike Erol-Kantarci¹, ²爱立信公司，加拿大渥太华 邮箱：{mhabi050,
    pitur008, melike.erolkantarci}@uottawa.ca,
- en: '{yigit.ozcan, medhat.elsayed, majid.bavand, raimundas.gaigalas }@ericsson.com
    ¹School of Electrical Engineering and Computer Science, University of Ottawa,
    Ottawa, Canada'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{yigit.ozcan, medhat.elsayed, majid.bavand, raimundas.gaigalas }@ericsson.com
    ¹渥太华大学电气工程与计算机科学学院，加拿大渥太华'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Intent-based network automation is a promising tool to enable easier network
    management however certain challenges need to be effectively addressed. These
    are: 1) processing intents, i.e., identification of logic and necessary parameters
    to fulfill an intent, 2) validating an intent to align it with current network
    status, and 3) satisfying intents via network optimizing functions like xApps
    and rApps in O-RAN. This paper addresses these points via a three-fold strategy
    to introduce intent-based automation for O-RAN. First, intents are processed via
    a lightweight Large Language Model (LLM). Secondly, once an intent is processed,
    it is validated against future incoming traffic volume profiles (high or low).
    Finally, a series of network optimization applications (rApps and xApps) have
    been developed. With their machine learning-based functionalities, they can improve
    certain key performance indicators such as throughput, delay, and energy efficiency.
    In this final stage, using an attention-based hierarchical reinforcement learning
    algorithm, these applications are optimally initiated to satisfy the intent of
    an operator. Our simulations show that the proposed method can achieve at least
    $12\%$ increase in energy efficiency, and $26.5\%$ decrease in network delay compared
    to the baseline algorithms.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基于意图的网络自动化是实现更简便网络管理的有前景工具，但仍需有效解决一些挑战。这些挑战包括：1）处理意图，即识别逻辑和实现意图所需的参数，2）验证意图以使其与当前网络状态对齐，以及3）通过O-RAN中的网络优化功能如xApps和rApps来满足意图。本文通过三重策略介绍O-RAN的基于意图的自动化。首先，使用轻量级的大型语言模型（LLM）处理意图。其次，一旦意图被处理，它会根据未来的流量量级（高或低）进行验证。最后，开发了一系列网络优化应用（rApps和xApps）。借助其基于机器学习的功能，它们能够改善某些关键性能指标，如吞吐量、延迟和能效。在最后阶段，使用基于注意力的层次强化学习算法，最优化地启动这些应用以满足操作员的意图。我们的模拟表明，所提出的方法相较于基线算法可以实现至少$12\%$的能效提升和$26.5\%$的网络延迟减少。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Intent-based network automation, Attention-based hierarchical reinforcement
    learning, network optimization
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于意图的网络自动化、基于注意力的层次强化学习、网络优化
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Achieving intent-based automation can be highly challenging in modern-day 5G
    paradigms like Open Radio Access Network (O-RAN). Instead of tweaking specific
    parameters through vendor-specified interfaces, Mobile Network Operators (MNOs)
    can now express their intentions in natural language. This approach introduces
    a huge challenge in the telecommunication industry since intents can vary widely
    based on network conditions, customer demands, and industry-specific terminology
    [[1](#bib.bib1)]. Therefore, a contextual understanding of the intents is vital
    for accurate intent processing. Large Language Models (LLMs) excel at capturing
    such context and understanding the underlying meaning of sentences (intents) within
    a broader spectrum. For instance, Wang et al. propose a Transformer model based
    on an LLM where various forms of network data (packet size, time stamps, and end-to-end
    delay) are integrated into a unified feature space for refining network control
    policies to align with an MNO’s intent [[2](#bib.bib2)]. However, modern-day communication
    systems require continuous monitoring of the Key Performance Indicators (KPIs)
    to serve users having different kinds of Quality-of-Service (QoS) requirements.
    Existing LLM-based approaches like [[2](#bib.bib2)] are not specially tailored
    to extract crucial information associated with what performance metric the operator
    is aiming to improve and by how much.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实现基于意图的自动化在现代5G范式如Open Radio Access Network（O-RAN）中可能非常具有挑战性。移动网络运营商（MNO）现在可以用自然语言表达他们的意图，而不是通过供应商指定的接口调整特定参数。这种方法在电信行业引入了巨大的挑战，因为意图可能因网络条件、客户需求和行业特定术语而有所不同[[1](#bib.bib1)]。因此，对意图的上下文理解对于准确处理意图至关重要。大型语言模型（LLMs）擅长捕捉这种上下文并理解句子的潜在含义（意图）。例如，Wang等人提出了一种基于LLM的Transformer模型，其中各种形式的网络数据（如包大小、时间戳和端到端延迟）被整合到一个统一的特征空间中，以优化网络控制策略，以符合MNO的意图[[2](#bib.bib2)]。然而，现代通信系统需要持续监控关键性能指标（KPIs），以服务于具有不同QoS需求的用户。现有的基于LLM的方法如[[2](#bib.bib2)]并未特别针对提取运营商希望改善的性能指标及其改善幅度的重要信息。
- en: 'Processing intents expressed in natural language by the MNO is crucial for
    deriving policy for network performance optimization. Accurate processing of intents
    can provide MNOs with feedback on how their intents might impact crucial performance
    metrics like energy efficiency or throughput. For example, an intent: “Increase
    energy efficiency by $30\%$” during peak hours can degrade performance due to
    high traffic volume. This kind of scenario requires a sophisticated method of
    intent validation using contemporary network status to ensure a seamless user
    experience without compromising any QoS.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由MNO处理自然语言表达的意图对于制定网络性能优化政策至关重要。准确处理意图可以为MNO提供有关这些意图如何影响关键性能指标（如能源效率或吞吐量）的反馈。例如，在高峰时段，意图“将能源效率提高$30\%$”可能会由于流量大而降低性能。这种情况需要一种使用现代网络状态的复杂意图验证方法，以确保在不妥协任何QoS的情况下提供无缝的用户体验。
- en: The introduction of paradigm-shifting concepts like O-RAN paves the way for
    deploying network-optimizing applications known as xApps and rApps in RAN Intelligent
    Controllers (RICs). These apps can be initiated based on MNO’s processed intents.
    Based on this idea, in [[1](#bib.bib1)], intents are converted into goals for
    a Hierarchical Reinforcement Learning (HRL) algorithm to initiate xApps or rApps
    for intent fulfillment. This decision-making process is computationally intensive,
    given the vast number of potential rApp or xApp combinations that can optimize
    performance. An AI agent, therefore, must not only choose from numerous options
    but also identify the most appropriate combination of network-optimizing apps
    that align with an MNO’s intent.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 引入像O-RAN这样的颠覆性概念为在RAN智能控制器（RICs）中部署网络优化应用程序（即xApps和rApps）铺平了道路。这些应用程序可以基于MNO的处理意图进行启动。基于这一理念，在[[1](#bib.bib1)]中，意图被转换为Hierarchical
    Reinforcement Learning（HRL）算法的目标，以启动xApps或rApps以实现意图。这一决策过程计算量巨大，因为有大量的rApp或xApp组合可以优化性能。因此，AI代理不仅必须从众多选项中进行选择，还需要确定与MNO的意图相符的最合适的网络优化应用组合。
- en: To this end, we propose a three-fold strategy to perform intent processing,
    validation, and RIC application initiation based on intents. First, we process
    intents using Bidirectional Encoder Representations from Transformers (BERT) which
    is an LLM model that can be fine-tuned for specific tasks like intent extraction,
    which allows it to learn task-specific patterns of the data [[3](#bib.bib3)].
    In the next step, intent validation is performed using a Transformer-based time
    series predictor for upcoming traffic volume [[4](#bib.bib4)]. This ensures that
    the intended adjustments to the network are not only theoretically sound but also
    practical and feasible given expected future traffic patterns. Lastly, to overcome
    the computational burden of selecting the appropriate combination of xApps and
    rApps from a high number of possible choices, we utilize an attention-based HRL
    framework that takes the processed intents (magnitude of the intended performance
    metric by an MNO) as goals. By filtering out only the feasible options [[5](#bib.bib5)],
    via an attention mechanism, the system can significantly reduce the computational
    burden
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们提出了一个三重策略，以基于意图进行处理、验证和RIC应用启动。首先，我们使用双向编码器表示模型（BERT）来处理意图，这是一种可以针对特定任务（如意图提取）进行微调的LLM模型，使其能够学习任务特定的数据模式[[3](#bib.bib3)]。接下来，使用基于Transformer的时间序列预测器对未来流量进行意图验证[[4](#bib.bib4)]。这确保了对网络的预期调整不仅在理论上是合理的，而且在考虑到预期的未来流量模式后也是实际可行的。最后，为了克服从大量可能选择中选择适当的xApps和rApps组合的计算负担，我们利用基于注意力的HRL框架，将处理后的意图（由MNO定义的预期性能指标）作为目标。通过注意力机制过滤出仅可行的选项[[5](#bib.bib5)]，系统能够显著减少计算负担。
- en: 'Unlike the previous works, the proposed scheme provides complete intent-based
    automation from an O-RAN perspective via intent processing, validation, and HRL-based
    performance optimization using xApps and rApps. The simulation results demonstrate
    that the proposed scheme yields significant improvements compared to two baseline
    approaches: HRL without attention and intent validation, and a single-application
    scenario developed using Deep RL (DRL). The proposed approach surpasses the HRL
    baseline in terms of throughput, delay, and energy efficiency by $12.02\%$, and
    $17.1\%$, $48.6\%$ respectively.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的工作不同，所提出的方案通过意图处理、验证和基于HRL的性能优化（使用xApps和rApps）从O-RAN的角度提供了完全的意图驱动自动化。模拟结果表明，与两种基线方法（无注意力和意图验证的HRL，及使用深度RL（DRL）开发的单应用场景）相比，所提出的方案带来了显著改进。在吞吐量、延迟和能源效率方面，所提出的方法分别超越了HRL基线$12.02\%$、$17.1\%$和$48.6\%$。
- en: 'The organization of this manuscript is outlined as follows: Section [II](#S2
    "II Related work ‣ LLM-Based Intent Processing and Network Optimization Using
    Attention-Based Hierarchical Reinforcement Learning") offers a short review of
    the existing literature associated with this research. This is succeeded by Section
    [III](#S3 "III System Model ‣ LLM-Based Intent Processing and Network Optimization
    Using Attention-Based Hierarchical Reinforcement Learning"), which elaborates
    on the system model in depth. Section [IV](#S4 "IV Proposed Intent Processing
    and Network Optimization Scheme ‣ LLM-Based Intent Processing and Network Optimization
    Using Attention-Based Hierarchical Reinforcement Learning") is specifically devoted
    to elaborately presenting our proposed approach. Performance assessment and a
    comparative examination of our approach against the baseline algorithms are detailed
    in Section [V](#S5 "V Performance Evaluation ‣ LLM-Based Intent Processing and
    Network Optimization Using Attention-Based Hierarchical Reinforcement Learning").
    Lastly, Section [VI](#S6 "VI Conclusions ‣ LLM-Based Intent Processing and Network
    Optimization Using Attention-Based Hierarchical Reinforcement Learning") presents
    the concluding remarks of the paper.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本手稿的组织结构如下：第[II](#S2 "II Related work ‣ LLM-Based Intent Processing and Network
    Optimization Using Attention-Based Hierarchical Reinforcement Learning")节简要回顾了与本研究相关的现有文献。接下来是第[III](#S3
    "III System Model ‣ LLM-Based Intent Processing and Network Optimization Using
    Attention-Based Hierarchical Reinforcement Learning")节，该节深入阐述了系统模型。第[IV](#S4 "IV
    Proposed Intent Processing and Network Optimization Scheme ‣ LLM-Based Intent
    Processing and Network Optimization Using Attention-Based Hierarchical Reinforcement
    Learning")节专门用来详细介绍我们提出的方法。第[V](#S5 "V Performance Evaluation ‣ LLM-Based Intent
    Processing and Network Optimization Using Attention-Based Hierarchical Reinforcement
    Learning")节详细评估了我们的方法性能，并与基线算法进行了比较。最后，第[VI](#S6 "VI Conclusions ‣ LLM-Based Intent
    Processing and Network Optimization Using Attention-Based Hierarchical Reinforcement
    Learning")节总结了论文的结论。
- en: II Related work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: Even though using LLMs for intent processing for network optimization is a relatively
    new concept, a few works have been found in the literature. Wang et al.’s development
    of a transformer model based on an LLM is one such example, where multi-modal
    representation learning is employed to integrate various forms of network data
    into a unified feature space [[2](#bib.bib2)]. Similarly, Kristina et al. have
    introduced a pipeline that utilizes an LLM to process intents into structured,
    policy-based abstractions, linking them to APIs for execution [[6](#bib.bib6)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用 LLM 进行意图处理以优化网络是一个相对较新的概念，但在文献中已经发现了一些相关工作。例如，Wang 等人开发了一种基于 LLM 的变换器模型，采用多模态表示学习将各种网络数据整合到一个统一的特征空间中
    [[2](#bib.bib2)]。类似地，Kristina 等人介绍了一种管道，利用 LLM 将意图处理成结构化的、基于策略的抽象，并将其链接到 APIs
    进行执行 [[6](#bib.bib6)]。
- en: To manage RIC applications based on operator intents, an approach based on HRL
    is proposed in [[7](#bib.bib7)] where intents are processed into goals for the
    control algorithm to optimize performance. Polese et al. propose a framework to
    collect control requests (intents) from an MNO and select the optimal Machine
    Learning (ML) models to achieve the operator’s goals to avoid conflicts [[8](#bib.bib8)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理基于运营商意图的 RIC 应用，提出了一种基于 HRL 的方法，如 [[7](#bib.bib7)] 所示，该方法将意图处理成控制算法的目标以优化性能。Polese
    等人提出了一个框架，用于从 MNO 收集控制请求（意图）并选择最佳的机器学习（ML）模型，以实现运营商的目标，避免冲突 [[8](#bib.bib8)]。
- en: Compared with existing literature, the main contribution of this work lies in
    deploying an LLM to interpret MNO’s intent and extract specific parameters for
    further network performance optimization. Processed intents are validated against
    future wireless traffic conditions. Intents lead HRL agents to initiate and orchestrate
    multiple network functions with an efficient exploration of action space via an
    attention mechanism.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有文献相比，本工作的主要贡献在于部署 LLM 解释 MNO 的意图并提取特定参数，以进一步优化网络性能。处理后的意图根据未来无线流量条件进行验证。意图引导
    HRL 代理发起并协调多个网络功能，通过注意力机制有效探索动作空间。
- en: III System Model
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 系统模型
- en: A cellular system based on O-RAN architecture employing downlink orthogonal
    frequency division multiplexing is considered in this work. There are $B$ users.
    Within the macro cell’s coverage area, multiple small cells are deployed. The
    system supports $K$, can facilitate various technologies like LTE, 5G.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作考虑了基于 O-RAN 架构的下行正交频分复用的蜂窝系统。系统中有 $B$ 个用户。在宏小区的覆盖区域内，部署了多个小小区。系统支持 $K$，可以支持如
    LTE、5G 等各种技术。
- en: The system model, depicted in Figure [1](#S3.F1 "Figure 1 ‣ III System Model
    ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning"), incorporates RAN Intelligent Controllers (RICs), comprising
    Non-Real-Time (non-RT)-RIC and Near-Real-Time (Near-RT)-RIC. These controllers
    host both rApps and xApps, focusing on control and optimization tasks across different
    time scales. We deploy BSs capable of switching bands from 3.5 GHz to mmWave frequencies.
    This deployment enables us to cater to high-throughput traffic by employing intelligent
    beamforming techniques. An xApp can be developed to regulate power based on UE
    location, utilizing minimal transmission power for enhanced energy efficiency.
    The system facilitates analog beamforming, with each BS employing a uniform linear
    array of $\mu$, from which beamforming vectors are selected [[9](#bib.bib9)].
    Each BS $b$, where $P$ represents the set of candidate transmit powers. The energy
    consumption model for the BS is obtained from [[10](#bib.bib10)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 系统模型如图 [1](#S3.F1 "图 1 ‣ III 系统模型 ‣ 基于 LLM 的意图处理和使用基于注意力的层次强化学习的网络优化") 所示，包括
    RAN 智能控制器（RIC），包含非实时（non-RT）-RIC 和近实时（Near-RT）-RIC。这些控制器托管了 rApps 和 xApps，专注于不同时间尺度上的控制和优化任务。我们部署了能够在
    3.5 GHz 和毫米波频率之间切换的 BS。这种部署使我们能够通过智能波束成形技术满足高吞吐量的流量需求。可以开发一个 xApp 来根据 UE 位置调节功率，利用最小的传输功率以提高能效。该系统支持模拟波束成形，每个
    BS 采用均匀线性阵列 $\mu$，从中选择波束成形向量 [[9](#bib.bib9)]。每个 BS $b$，其中 $P$ 表示候选传输功率集合。BS 的能耗模型取自
    [[10](#bib.bib10)]。
- en: '![Refer to caption](img/adc9b92a310bbaad124521afeab6fc72.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/adc9b92a310bbaad124521afeab6fc72.png)'
- en: 'Figure 1: System model designed with macro cell and small cells based on O-RAN
    architecture.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：基于 O-RAN 架构设计的宏小区和小小区的系统模型。
- en: III-A Problem Formulation
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 问题表述
- en: Based on the system and network model discussed so far, we introduce multiple
    xApps and rApps. There are two sets of these RIC applications. The first set consists
    of intent translation, validation, and meta-controller rApps and the second set
    consists of traffic steering, beamforming, cell sleeping, power allocation, and
    handover management applications that can be directly controlled by the first
    set.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于迄今为止讨论的系统和网络模型，我们引入了多个 xApps 和 rApps。这些 RIC 应用有两个集合。第一个集合包括意图翻译、验证和元控制器 rApps，第二个集合包括流量引导、波束形成、基站休眠、电力分配和切换管理应用，可以由第一个集合直接控制。
- en: 'Let us define $\Psi$ represent the subset of $\Psi$ as the set of potential
    KPIs that an application might improve, and $Q_{s}$ as the set of QoS requirements
    that must be met by the system. Given these definitions, the challenge of orchestrating
    RIC applications based on operator intent can be formulated as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\Psi$ 表示应用可能改善的潜在 KPI 子集，$Q_{s}$ 表示系统必须满足的 QoS 要求集合。基于这些定义，基于操作员意图协调 RIC
    应用的挑战可以表述如下：
- en: '|  | $\begin{split}\max\sum_{\rho\in\varrho}\sum_{q_{s}\in Q_{s}}(C_{\rho}-\varpi\gamma_{q_{s}}),\quad\quad\quad\\
    \text{s.t.}\quad\forall(\Psi)\exists(\sigma):\zeta(O)=1,\quad\quad\quad\end{split}$
    |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\max\sum_{\rho\in\varrho}\sum_{q_{s}\in Q_{s}}(C_{\rho}-\varpi\gamma_{q_{s}}),\quad\quad\quad\\
    \text{s.t.}\quad\forall(\Psi)\exists(\sigma):\zeta(O)=1,\quad\quad\quad\end{split}$
    |  | (1) |'
- en: where $C$ is the penalty parameter for QoS requirement violation, and $\gamma_{q_{s}}$
    is the proposition that “An xApp can improve a performance metric”, which is either
    ‘0’ or ‘1’.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C$ 是 QoS 要求违反的惩罚参数，$\gamma_{q_{s}}$ 是“一个 xApp 可以改善性能指标”的命题，其值为‘0’或‘1’。
- en: IV Proposed Intent Processing and Network Optimization Scheme
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 提议的意图处理和网络优化方案
- en: In this section, first, we discuss processing intents using an LLM followed
    by intent validation. The last part of this section consists of an HRL-based attention
    mechanism for RIC application initiation and orchestration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先讨论使用 LLM 处理意图，接着是意图验证。本节的最后部分包括基于 HRL 的注意力机制，用于 RIC 应用的启动和协调。
- en: IV-A Intent Processing using LLMs
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 使用 LLM 处理意图
- en: Intent-based networking simplifies network and administrative tasks by automating
    system operations through the comprehension and execution of operator-defined
    intents, such as “Increase overall energy efficiency by 10%”, “Boost system throughput
    by 15%”, or “Reduce network delay by 13%”. These intents focus on improving metrics
    like throughput, energy efficiency, and network delay. We developed a dataset
    containing various intents related to 5G and O-RAN performance metrics and annotated
    it with details like action direction (increase or decrease), change percentage,
    and targeted metric.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于意图的网络通过理解和执行操作员定义的意图（例如“将整体能源效率提高 10%”、“将系统吞吐量提高 15%”或“将网络延迟减少 13%”）来简化网络和管理任务，从而自动化系统操作。这些意图专注于改善吞吐量、能源效率和网络延迟等指标。我们开发了一个数据集，包含各种与
    5G 和 O-RAN 性能指标相关的意图，并注释了如行动方向（增加或减少）、变化百分比和目标指标等详细信息。
- en: We utilized a lightweight version of BERT, known as ALBERT, for efficient intent
    processing. This model is particularly suitable for real-time inference in O-RAN
    scenarios due to its balance of performance and computational efficiency [[11](#bib.bib11)].
    Using pre-trained ALBERT, we trained our system to predict the specified elements
    from intent expressions. This intent processing functionality is presented as
    User Interface (UI) and intent translation rApp in Fig. [2](#S4.F2 "Figure 2 ‣
    IV-B Intent Validation Using Transformer ‣ IV Proposed Intent Processing and Network
    Optimization Scheme ‣ LLM-Based Intent Processing and Network Optimization Using
    Attention-Based Hierarchical Reinforcement Learning"). The job of this rApp is
    to work as a UI for an MNO and process the intents with crucial information to
    feed the intent validation rApp and the meta-controller rApp.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了一种轻量级的 BERT 版本，称为 ALBERT，以实现高效的意图处理。由于其性能和计算效率的平衡，该模型特别适合于 O-RAN 场景中的实时推断
    [[11](#bib.bib11)]。使用预训练的 ALBERT，我们训练系统以预测意图表达中的指定元素。这种意图处理功能在图 [2](#S4.F2 "Figure
    2 ‣ IV-B Intent Validation Using Transformer ‣ IV Proposed Intent Processing and
    Network Optimization Scheme ‣ LLM-Based Intent Processing and Network Optimization
    Using Attention-Based Hierarchical Reinforcement Learning") 中作为用户界面（UI）和意图翻译 rApp
    展示。该 rApp 的工作是作为 MNO 的用户界面，并处理含有关键信息的意图，以供意图验证 rApp 和元控制器 rApp 使用。
- en: IV-B Intent Validation Using Transformer
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 使用 Transformer 验证意图
- en: 'The second step of this work consists of intent validation. QoS parameters
    deviate from the originally defined QoS metrics over time due to performance degradation
    in the wireless networks causing QoS drifts. By deviation, we mean when a QoS
    parameter is having lesser value than the required. For a specific traffic class,
    $k$ and associated performance metrics $C$, we can formulate QoS drift as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的第二步是意图验证。由于无线网络性能下降导致QoS偏移，QoS参数随着时间的推移偏离了最初定义的QoS指标。偏离指的是QoS参数的实际值低于所需值。对于特定的流量类别，$k$及相关性能指标$C$，我们可以将QoS偏移公式化如下：
- en: '|  | $$\begin{split}\min\sum_{q_{j}\in Q_{s}}\sum_{c\in C}\{D_{QoS}-A_{QoS}\},\quad\quad\\
    \text{s.t.}\quad A_{QoS}\leq D_{QoS}\quad\text{and}\quad A_{QoS}> |  |
    (2) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\begin{split}\min\sum_{q_{j}\in Q_{s}}\sum_{c\in C}\{D_{QoS}-A_{QoS}\},\quad\quad\\
    \text{约束条件：}\quad A_{QoS}\leq D_{QoS}\quad\text{且}\quad A_{QoS}> |  | (2)
    |'
- en: In eq.[2](#S4.E2 $$ based on eq. [2](#S4.E2 "In IV-B Intent Validation Using
    Transformer ‣ IV Proposed Intent Processing and Network Optimization Scheme ‣
    LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning").
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式[2](#S4.E2 $$ 基于公式 [2](#S4.E2 "在IV-B使用Transformer进行意图验证 ‣ IV提出的意图处理和网络优化方案
    ‣ 基于LLM的意图处理和基于注意力的层次强化学习进行网络优化")中。
- en: To perform intent validation, we utilize one of our previous works [[4](#bib.bib4)]
    where traffic data are collected so that it can be fed to the transformer-based
    time series predictor for future traffic volume prediction. The traffic predictor
    resides in the non-RT-RIC as an rApp (named as intent validation rApp, as in Fig
    [2](#S4.F2 "Figure 2 ‣ IV-B Intent Validation Using Transformer ‣ IV Proposed
    Intent Processing and Network Optimization Scheme ‣ LLM-Based Intent Processing
    and Network Optimization Using Attention-Based Hierarchical Reinforcement Learning").).
    The forecast for traffic volume is performed for successive time intervals, with
    two distinct thresholds derived from historical data. The higher threshold, denoted
    as $Th_{p}$, RIC applications such as traffic steering and power allocation can
    be activated to enhance or sustain high throughput, depending on the intent of
    an MNO. The lower threshold, $Th_{t}$, is established based on low traffic volume.
    In the event that the anticipated traffic decreases to less than this threshold,
    an intent on increasing energy efficiency can be validated leading to the initiation
    of energy-saving rApp like cell sleeping due to the expected lower traffic levels.
    The algorithmic process of predictive traffic validation is presented in Algorithm
    1 for the overall representation of the process. Note that the intent validation
    can also be performed by predicting other crucial network parameters or values
    like congestion levels, device connectivity, status, and so on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行意图验证，我们利用了我们之前的一项工作[[4](#bib.bib4)]，该工作中收集了流量数据，以便可以输入到基于Transformer的时间序列预测器中进行未来流量预测。流量预测器作为rApp（称为意图验证rApp，如图[2](#S4.F2
    "图 2 ‣ IV-B使用Transformer进行意图验证 ‣ IV提出的意图处理和网络优化方案 ‣ 基于LLM的意图处理和基于注意力的层次强化学习进行网络优化")中所示）驻留在非实时RIC中。流量预测是在连续时间间隔内进行的，基于历史数据得出两个不同的阈值。较高的阈值，记为$Th_{p}$，可以激活如流量引导和功率分配等RIC应用，以提高或维持高吞吐量，具体取决于MNO的意图。较低的阈值$Th_{t}$是基于低流量水平建立的。如果预期流量下降到低于该阈值，验证提高能效的意图将导致启动节能rApp，如由于预期流量较低而进行的蜂窝休眠。预测流量验证的算法过程在算法1中展示，以总体表示该过程。请注意，意图验证也可以通过预测其他关键网络参数或值（如拥塞水平、设备连接状态等）来执行。
- en: Algorithm 1 Intent Validation Based on Traffic Prediction
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 基于流量预测的意图验证
- en: 1:Predict future traffic $T_{p}$ using Autoformer2:Extract intent ($I_{MNO}$,
    and Change Percentage (%) using LLM3:Define traffic classes $K=\{k_{1},k_{2},\ldots,k_{i}\}$5:if ($T_{p}>
     $Th_{p}$) 7:     如果 $I_{MNO}$
    则 8:         对每个 QoS 参数 $Q_{s_{j}}$ 的配置文件 9:             计算 QoS 漂移 $I_{Q}(Q_{s_{j}})$
    然后 11:                 意图无效 12:                 中断 13:             结束 如果 14:         结束
    循环 15:         如果 $I_{Q}$ 是 0 $||$ 则 16:             验证意图 17:         结束 如果 18:
        结束 如果 19: 否则 20:     重新计算 $Th_{p}$ 21: 结束 如果 22: 返回验证结果'
- en: '![Refer to caption](img/5d51f9feb44a94654eedffabdd9e62ae.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5d51f9feb44a94654eedffabdd9e62ae.png)'
- en: 'Figure 2: Three-step methodology for intent processing, validation, and performance
    optimization.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 意图处理、验证和性能优化的三步方法。'
- en: IV-C Network Optimization Using Attention-based HRL
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 基于注意力的 HRL 网络优化
- en: 'Once the intent is validated, it allows us to go to the final stage of this
    work which is optimizing the network performance based on the operator intent
    using an RL algorithm. To do that, we opt for hierarchical Deep-Q-Network (h-DQN),
    an HRL algorithm having a two-level hierarchy with a meta-controller on top [[12](#bib.bib12)].
    Meta-controller can take in a network state (e.g., traffic class) and a goal (desired
    change of a performance metric extracted from an intent) to achieve and the controller
    in the lower level takes the action of choosing an RIC application or combinations
    of them based on the state and the goal. The meta-controller is designed as an
    rApp in the non-RT-RIC (see Fig. [2](#S4.F2 "Figure 2 ‣ IV-B Intent Validation
    Using Transformer ‣ IV Proposed Intent Processing and Network Optimization Scheme
    ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning")). RIC applications are controlled by the meta-controller
    that can directly optimize system performance are summarized as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦意图被验证，它允许我们进入工作的最终阶段，即基于操作员意图优化网络性能，使用 RL 算法。为此，我们选择了分层深度 Q 网络 (h-DQN)，这是一种具有两个层次的
    HRL 算法，顶部是元控制器 [[12](#bib.bib12)]。元控制器可以接受网络状态（例如流量类别）和目标（从意图中提取的期望性能指标变化）来实现，底层的控制器根据状态和目标选择
    RIC 应用或它们的组合。元控制器被设计为非实时 RIC 中的 rApp（参见图 [2](#S4.F2 "Figure 2 ‣ IV-B Intent Validation
    Using Transformer ‣ IV Proposed Intent Processing and Network Optimization Scheme
    ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning")）。由元控制器直接优化系统性能的 RIC 应用程序总结如下：
- en: IV-C1 Traffic Steering xApp
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 流量引导 xApp
- en: 'The traffic steering xApp is engineered to concurrently uphold QoS across various
    traffic types by employing a steering mechanism utilizing a DQN. To ensure optimal
    performance, the xApp’s state and reward functions are designed with a focus on
    two KPIs: network latency and overall system throughput. For more detailed insights
    into this xApp, please refer to [[13](#bib.bib13)].'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 流量引导 xApp 被设计用来通过采用利用 DQN 的引导机制同时维持各种流量类型的 QoS。为了确保最佳性能，xApp 的状态和奖励函数的设计重点关注两个
    KPI：网络延迟和整体系统吞吐量。有关此 xApp 的详细信息，请参阅 [[13](#bib.bib13)]。
- en: IV-C2 Cell Sleeping rApp
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 小区休眠 rApp
- en: The cell sleeping rApp is based on DQN and aims to reduce network power consumption
    by deactivating idle or less busy BSs based on traffic load and queue length (set
    of states). Higher rewards are provided for taking actions that increase energy
    efficiency without overloading the active BSs. Technical details of this DQN-based
    rApp can be found in our previous work [[7](#bib.bib7)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 小区休眠 rApp 基于 DQN，旨在通过根据流量负载和队列长度（状态集合）停用空闲或负荷较少的基站来减少网络功耗。对于采取提高能效的行动而不超载活跃基站的情况，会提供更高的奖励。有关此
    DQN 基于 rApp 的技术细节，请参阅我们以前的工作 [[7](#bib.bib7)]。
- en: IV-C3 Beamforming xApp
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C3 波束成形 xApp
- en: Similar to the past two apps, we also employ DQN to develop the beamforming
    xApp where UE coordinates are used as a set of states. The set of actions comprises
    $\alpha(\chi_{n})$. Here, $\chi$ denotes the array steering vector corresponding
    to the direction $\chi_{n}$ element in the codebook. $\delta_{n}$, and ii) the
    ratio of the energy efficiency linked with the base station’s throughput and transmission
    power to the maximum theoretical energy efficiency.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与过去两个应用类似，我们还使用DQN开发了波束成形xApp，其中UE坐标被用作状态集合。动作集合包括$\alpha(\chi_{n})$。这里，$\chi$表示对应于代码本中方向$\chi_{n}$的阵列引导向量。$\delta_{n}$，以及
    ii) 与基站吞吐量和传输功率相关的能效与最大理论能效的比率。
- en: IV-C4 Power Allocation xApp
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C4 功率分配xApp
- en: The power allocation xApp tries to maximize the total throughput. The set of
    states includes transmission rate and channel state information. We divide the
    transmission power levels into $P_{L}$. The goal is to improve throughput. The
    reward function is formulated to increase the transmission rate of a BS on a Resource
    Block Group (RBG).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 功率分配xApp尝试最大化总吞吐量。状态集合包括传输速率和信道状态信息。我们将传输功率水平分为$P_{L}$。目标是提高吞吐量。奖励函数被制定为提高资源块组（RBG）上基站的传输速率。
- en: IV-C5 Handover decision making xApp for energy saving
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C5 节能的切换决策xApp
- en: 'This xApp is specifically tailored with DQN to achieve energy efficiency via
    optimally tailored handover policies. To accommodate the extra processing demands
    and signaling exchanges involved in handover executions at both serving and target
    base stations, we establish a handover energy consumption function to incorporate
    the additional energy overheads:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个xApp专门与DQN配合使用，通过优化的切换策略实现能效。为了适应在服务和目标基站的切换执行中涉及的额外处理需求和信令交换，我们建立了一个切换能耗函数来纳入额外的能耗：
- en: '|  | $$E_{u}^{HO}(t)=\begin{cases}E^{HO}(t)&amp;\text{if handover occurs at
    time $t$}\\ 0&amp;\text{otherwise,}\\'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$E_{u}^{HO}(t)=\begin{cases}E^{HO}(t)&amp;\text{如果在时间$t$发生切换}\\ 0&amp;\text{否则，}\\'
- en: \end{cases}$$ |  | (3) |
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}$$ |  | (3) |
- en: 'where $E^{HO}\geq 0$ at time slot $t$ is expressed as follows: $E_{total}=E_{u,b}^{Tx}+E^{HO}$
    is the transmission energy consumption when a $u$. The objective of this xApp
    is to reduce average energy consumption. More details on this xApp can be found
    in [[14](#bib.bib14)].'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$E^{HO}\geq 0$在时间槽$t$的表达式如下：$E_{total}=E_{u,b}^{Tx}+E^{HO}$是一个$u$的传输能耗。这个xApp的目标是减少平均能耗。更多关于这个xApp的详细信息可以在[[14](#bib.bib14)]中找到。
- en: At this stage of the work, we define an MDP to solve the problem formulated
    in eq.[1](#S3.E1 "In III-A Problem Formulation ‣ III System Model ‣ LLM-Based
    Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning").
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作的这一阶段，我们定义一个MDP来解决公式[1](#S3.E1 "在 III-A 问题表述 ‣ III 系统模型 ‣ 基于LLM的意图处理和基于注意力的分层强化学习的网络优化")中提出的问题。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'State and Action: The set of states consists of traffic flow types of different
    users in the network. $S=\{T_{1},..,T_{2},...,T_{3},...,T_{4},..\}$.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态和行动：状态集合由网络中不同用户的流量类型组成。$S=\{T_{1},..,T_{2},...,T_{3},...,T_{4},..\}$。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intrinsic reward: The intrinsic reward function ($r_{in}$.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内在奖励：内在奖励函数($r_{in}$)。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Goal for the controller: Increased level of a performance metric that can satisfy
    operator intent is passed to the controller as goals. For example, $G=\{TP_{1},TP_{2},...,TP_{n}\}$
    for throughput increasing intents.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 控制器的目标：将能满足操作员意图的性能指标的提高水平传递给控制器作为目标。例如，$G=\{TP_{1},TP_{2},...,TP_{n}\}$用于吞吐量增加的意图。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Extrinsic reward: Summation of the intrinsic reward over $\tau$ steps.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外部奖励：在$\tau$步内的内在奖励总和。
- en: 'The process of selecting RIC applications for network optimization using HRL
    can be summarized as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HRL进行网络优化时选择RIC应用的过程可以总结如下：
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 1: An input is provided by the MNO associated with a performance metric.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 1：MNO提供与性能指标相关的输入。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 2: Intent is processed and crucial information like Action: “Increase/Decrease”.
    “Which metric” and by how much (in percentage) are extracted using pre-trained
    ALBERT.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 2：意图被处理，关键信息如行动：“增加/减少”，“哪个指标”以及增加多少（百分比）通过预训练的ALBERT提取。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 3: Extracted information is passed to the intent validation rApp.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 3：提取的信息被传递到意图验证rApp。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 4: With a valid intent, a target associated with the performance metrics
    (goals) and feasible action set for the intent are provided to the controller
    in near-RT-RIC by the meta controller rApp in the non-RT-RIC.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 4：通过有效意图，关联性能指标（目标）和意图的可行动作集被提供给近实时 RIC 的控制器，由非实时 RIC 中的元控制器 rApp 提供。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 5: The controller selects an RIC application or a combination of them
    to reach the target performance as close as possible.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 5：控制器选择一个 RIC 应用程序或它们的组合，以尽可能接近目标性能。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 6: Selected RIC applications optimize the performance of the network as
    a response to the intent of the operator.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 6：选择的 RIC 应用程序优化网络性能，以响应操作员的意图。
- en: We employ an attention mechanism [[5](#bib.bib5)] to reduce the action space
    for our h-DQN agent, enhancing efficiency and decision-making while expediting
    convergence. A critical component of this process is the Markovian option, $\omega$,
    a termination condition $\beta$. Actions are chosen based on $\pi$ terminates
    the option. Our approach uses hard attention [[5](#bib.bib5)], selecting a specific
    part of the input for focus. When an option $\omega$, the intent completion function
    assesses if the option’s policy $\pi_{\omega}$, and using values of $\epsilon$,
    we can efficiently narrow down the action space with a supervised learning algorithm.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了注意力机制 [[5](#bib.bib5)] 来减少 h-DQN 代理的动作空间，从而提高效率和决策能力，同时加快收敛速度。此过程的一个关键组件是马尔可夫选项，$\omega$，终止条件
    $\beta$。动作的选择基于 $\pi$ 终止选项。我们的方法使用硬注意力 [[5](#bib.bib5)]，选择输入的特定部分进行关注。当选项 $\omega$
    被激活时，意图完成函数评估选项策略 $\pi_{\omega}$，并利用 $\epsilon$ 的值，我们可以通过监督学习算法有效地缩小动作空间。
- en: V Performance Evaluation
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 性能评估
- en: In this section, first, we introduce the simulation settings followed by the
    simulation results showing the effectiveness and superiority of the proposed method.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍仿真设置，接着展示仿真结果，显示所提方法的有效性和优越性。
- en: V-A Simulation setup
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 仿真设置
- en: '![Refer to caption](img/d52ffd71d46f0dd5f1c14ac9e8448b3a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d52ffd71d46f0dd5f1c14ac9e8448b3a.png)'
- en: 'Figure 3: Impact of intent validation on energy efficiency.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：意图验证对能效的影响。
- en: '![Refer to caption](img/887b4f6fef832a4f390d28b0850a923c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/887b4f6fef832a4f390d28b0850a923c.png)'
- en: 'Figure 4: Impacts of operator intents on throughput.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：操作员意图对吞吐量的影响。
- en: '![Refer to caption](img/db306c90a18df7dea896078e95f4fd43.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/db306c90a18df7dea896078e95f4fd43.png)'
- en: 'Figure 5: Performance analysis of the proposed method: (a) energy efficiency,
    (b) network delay, and (c) throughput.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：所提方法的性能分析：（a）能效，（b）网络延迟，（c）吞吐量。
- en: The simulation environment in this work consists of one macro-cell surrounded
    by a couple of densely deployed small cells having a multi-RAT environment with
    60 users in total. For 5G NR, the bandwidth is set at $60$ GHz and $30$ KHz, and
    maximum transmission power of $43$ MHz bandwidth, $800$ KHz subcarrier spacing,
    and a maximum transmission power of $38$, $40$, and $0.5$, $120$, and $32$ bytes,
    corresponding to the specified traffic classes. These values are fixed based on
    [[16](#bib.bib16)]. We refer to traffic steering, cell sleeping, beamforming,
    handover management, and power allocation applications as xApp1, rApp2, xApp3,
    xApp4, and xApp5 respectively in the next section to present our results.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中的仿真环境包括一个宏小区，周围有几个密集部署的小区，具有多 RAT 环境，总共 60 个用户。对于 5G NR，带宽设置为 $60$ GHz 和
    $30$ KHz，最大传输功率为 $43$ MHz 带宽，$800$ KHz 子载波间隔，以及最大传输功率分别为 $38$、$40$ 和 $0.5$、$120$
    和 $32$ 字节，对应于指定的流量类别。这些值基于 [[16](#bib.bib16)] 固定。我们在下一节中将流量引导、蜂窝休眠、波束赋形、切换管理和功率分配应用称为
    xApp1、rApp2、xApp3、xApp4 和 xApp5，以呈现我们的结果。
- en: V-B Simulation Results
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 仿真结果
- en: The first simulation result is associated with intent validation. When there
    is no intent validation, conflicting intents from an operator can severely degrade
    performance. Fig. [3](#S5.F3 "Figure 3 ‣ V-A Simulation setup ‣ V Performance
    Evaluation ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based
    Hierarchical Reinforcement Learning") presents such an example where we have a
    low traffic load of 5Mbps. At $2200^{th}$ time slot, there is a sharp decrease
    in energy efficiency. This is because of the unwanted intent of increasing throughput
    even though the cell sleeping application is active due to low traffic demand.
    An intent of increasing throughput by an MNO would result in activating more cells
    even though not necessary and decrease overall energy efficiency. Similar impacts
    can be observed for other KPIs like delay, and throughput also.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模拟结果与意图验证相关。当没有意图验证时，操作员的冲突意图可能严重影响性能。图[3](#S5.F3 "Figure 3 ‣ V-A Simulation
    setup ‣ V Performance Evaluation ‣ LLM-Based Intent Processing and Network Optimization
    Using Attention-Based Hierarchical Reinforcement Learning")展示了一个例子，在此例中，我们的流量负载为5Mbps。在第$2200^{th}$时间段，能效出现了急剧下降。这是因为尽管由于流量需求低，单元睡眠应用仍处于活动状态，但增加吞吐量的意图仍然存在。MNO增加吞吐量的意图会导致激活更多单元，即使不必要，从而降低整体能效。类似的影响也可以观察到其他KPI，如延迟和吞吐量。
- en: As shown in Fig. [4](#S5.F4 "Figure 4 ‣ V-A Simulation setup ‣ V Performance
    Evaluation ‣ LLM-Based Intent Processing and Network Optimization Using Attention-Based
    Hierarchical Reinforcement Learning"), the operator’s goal to “increase throughput”
    leads to specific RIC application selections. For instance, aiming for a $5\%$
    increase involves both xApp1 and xApp3\. Conversely, reducing power consumption
    significantly decreases throughput around the $1250^{th}$ increase, xApp1, xApp3,
    and xApp4 together produce a sharp throughput rise.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[4](#S5.F4 "Figure 4 ‣ V-A Simulation setup ‣ V Performance Evaluation ‣ LLM-Based
    Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning")所示，操作员“提高吞吐量”的目标会导致特定的RIC应用选择。例如，追求$5\%$的增幅涉及xApp1和xApp3。相反，显著减少功耗会导致在第$1250^{th}$次增幅时吞吐量显著下降，此时xApp1、xApp3和xApp4一起产生了急剧的吞吐量上升。
- en: 'The proposed method’s effectiveness is compared with two baselines: an HRL-based
    RIC application initiation without an attention mechanism or intent validation,
    and a single DRL-based application scenario. The proposed method employs a hard
    attention mechanism, focusing on the subset of RIC applications most relevant
    for the current traffic type and operator’s intent. This focus reduces conflicting
    actions and shows superior performance over the HRL baseline (improvements of
    $12.02\%$ in delay, and $17.1\%$, $48.6\%$ in the same metrics), as observed in
    Fig. [5](#S5.F5 "Figure 5 ‣ V-A Simulation setup ‣ V Performance Evaluation ‣
    LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical
    Reinforcement Learning").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的方法的有效性与两个基线进行比较：一种是没有注意力机制或意图验证的基于HRL的RIC应用启动，另一种是单一的DRL应用场景。所提出的方法采用了硬注意力机制，专注于当前流量类型和操作员意图最相关的RIC应用子集。这种关注减少了冲突行为，并在延迟（$12.02\%$的改进）以及其他相同指标（$17.1\%$、$48.6\%$）上显示出优于HRL基线的性能，如图[5](#S5.F5
    "Figure 5 ‣ V-A Simulation setup ‣ V Performance Evaluation ‣ LLM-Based Intent
    Processing and Network Optimization Using Attention-Based Hierarchical Reinforcement
    Learning")所示。
- en: VI Conclusions
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this paper, first, we process intents using an LLM and extract crucial information
    regarding which performance metric to optimize and by how much. Next, a Transformer-based
    intent validation technique has been used to rule out intents that conflict with
    the contemporary network state to avoid performance degradation. Lastly, we use
    an attention-based HRL to initiate and orchestrate RIC applications for performance
    optimization. The proposed method outperforms the HRL baseline in throughput,
    delay, and energy efficiency by $12.02\%$, and $17.1\%$, $48.6\%$ respectively.
    In the future, we plan to perform highly complex intent translation from the MNO
    and perform intent validation predicting other parameters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，首先，我们使用LLM处理意图，并提取关于优化哪些性能指标及其程度的关键信息。接下来，采用基于Transformer的意图验证技术，排除与当前网络状态冲突的意图，以避免性能下降。最后，我们使用基于注意力的HRL来启动和协调RIC应用以进行性能优化。所提出的方法在吞吐量、延迟和能效上分别超越了HRL基线$12.02\%$、$17.1\%$和$48.6\%$。未来，我们计划对MNO进行高度复杂的意图翻译，并进行意图验证以预测其他参数。
- en: Acknowledgement
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work has been supported by MITACS, Ericsson Canada, and NSERC Canada Research
    Chairs program.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了MITACS、爱立信加拿大和NSERC加拿大研究主席计划的支持。
- en: References
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] ETSI TS 128 312: “Management and Orchestration; Intent Driven Management
    Services for Mobile Networks”. [Online]. Available: https://www.etsi.org/deliver/etsi_ts/128300_128399/128312/17.00.01_60/ts_128312v170001p.pdf'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] ETSI TS 128 312：“管理和编排；面向移动网络的意图驱动管理服务”。 [在线]。可用链接：https://www.etsi.org/deliver/etsi_ts/128300_128399/128312/17.00.01_60/ts_128312v170001p.pdf'
- en: '[2] J. Wang, L. Zhang, Y. Yang, Z. Zhuang, Q. Qi, H. Sun, L. Lu, J. Feng, and
    J. Liao, “Network Meets ChatGPT: Intent Autonomous Management, Control and Operation,”
    *Journal of Communications and Information Networks*, vol. 8, no. 3, pp. 239–255,
    2023.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Wang, L. Zhang, Y. Yang, Z. Zhuang, Q. Qi, H. Sun, L. Lu, J. Feng, 和
    J. Liao，“网络与ChatGPT相遇：意图自主管理、控制和操作”， *通信与信息网络期刊*，第8卷，第3期，第239–255页，2023年。'
- en: '[3] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep
    Bidirectional Transformers for Language Understanding,” *CoRR*, vol. abs/1810.04805,
    2018\. [Online]. Available: http://arxiv.org/abs/1810.04805'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Devlin, M. Chang, K. Lee, 和 K. Toutanova，“BERT：用于语言理解的深度双向变换器预训练”，*CoRR*，第abs/1810.04805卷，2018年。
    [在线]。可用链接：http://arxiv.org/abs/1810.04805'
- en: '[4] M. A. Habib, P. E. Iturria-Rivera, Y. Ozcan, M. Elsayed, M. Bavand, R. Gaigalas,
    and M. Erol-Kantarci, “Transformer-Based Wireless Traffic Prediction and Network
    Optimization in O-RAN,” *arXiv preprint arXiv:2403.10808*, Mar. 2024\. [Online].
    Available: https://arxiv.org/abs/2403.10808'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. A. Habib, P. E. Iturria-Rivera, Y. Ozcan, M. Elsayed, M. Bavand, R.
    Gaigalas, 和 M. Erol-Kantarci，“基于变换器的无线流量预测与O-RAN中的网络优化”，*arXiv预印本arXiv:2403.10808*，2024年3月。
    [在线]。可用链接：https://arxiv.org/abs/2403.10808'
- en: '[5] A. C. Nica, K. Khetarpal, and D. Precup, “The paradox of choice: Using
    attention in hierarchical reinforcement learning,” *CoRR*, vol. abs/2201.09653,
    2022\. [Online]. Available: https://arxiv.org/abs/2201.09653'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. C. Nica, K. Khetarpal, 和 D. Precup，“选择的悖论：在层次强化学习中使用注意力”，*CoRR*，第abs/2201.09653卷，2022年。
    [在线]。可用链接：https://arxiv.org/abs/2201.09653'
- en: '[6] K. Dzeparoska, J. Lin, A. Tizghadam, and A. Leon-Garcia, “LLM-Based Policy
    Generation for Intent-Based Management of Applications,” in *2023 19th International
    Conference on Network and Service Management (CNSM)*, 2023, pp. 1–7.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] K. Dzeparoska, J. Lin, A. Tizghadam, 和 A. Leon-Garcia， “基于LLM的应用意图管理策略生成”，发表于
    *2023年第19届国际网络与服务管理会议（CNSM）*，2023年，第1–7页。'
- en: '[7] M. A. Habib, H. Zhou, P. E. Iturria-Rivera, M. Elsayed, M. Bavand, R. Gaigalas,
    Y. Ozcan, and M. Erol-Kantarci, “Intent-driven Intelligent Control and Orchestration
    in O-RAN Via Hierarchical Reinforcement Learning,” in *2023 IEEE 20th International
    Conference on Mobile Ad Hoc and Smart Systems (MASS)*, 2023, pp. 55–61.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. A. Habib, H. Zhou, P. E. Iturria-Rivera, M. Elsayed, M. Bavand, R. Gaigalas,
    Y. Ozcan, 和 M. Erol-Kantarci，“通过层次强化学习在O-RAN中实现意图驱动的智能控制和编排”，发表于 *2023 IEEE第20届国际移动自组网与智能系统会议（MASS）*，2023年，第55–61页。'
- en: '[8] S. D’Oro, L. Bonati, M. Polese, and T. Melodia, “OrchestRAN: Network Automation
    Through Orchestrated Intelligence in the Open RAN,” in *IEEE INFOCOM 2022 - IEEE
    Conference on Computer Communications*, 2022, pp. 270–279.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. D’Oro, L. Bonati, M. Polese, 和 T. Melodia，“OrchestRAN：通过在开放RAN中协调智能实现网络自动化”，发表于
    *IEEE INFOCOM 2022 - IEEE计算机通信会议*，2022年，第270–279页。'
- en: '[9] F. B. Mismar, B. L. Evans, and A. Alkhateeb, “Deep Reinforcement Learning
    for 5G Networks: Joint Beamforming, Power Control, and Interference Coordination,”
    *IEEE Transactions on Communications*, vol. 68, no. 3, pp. 1581–1592, 2020.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] F. B. Mismar, B. L. Evans, 和 A. Alkhateeb，“5G网络中的深度强化学习：联合波束成形、功率控制和干扰协调”，*IEEE通信学报*，第68卷，第3期，第1581–1592页，2020年。'
- en: '[10] P. Ren and M. Tao, “A Decentralized Sleep Mechanism in Heterogeneous Cellular
    Networks with QoS Constraints,” *IEEE Wireless Communications Letters*, vol. 3,
    no. 5, pp. 509–512, Oct. 2014.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] P. Ren 和 M. Tao，“在具有QoS约束的异构蜂窝网络中的去中心化睡眠机制”，*IEEE无线通信快报*，第3卷，第5期，第509–512页，2014年10月。'
- en: '[11] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT:
    A lite BERT for self-supervised learning of language representations,” *CoRR*,
    vol. abs/1909.11942, 2019\. [Online]. Available: http://arxiv.org/abs/1909.11942'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, 和 R. Soricut，“ALBERT：用于自监督语言表示学习的轻量BERT”，*CoRR*，第abs/1909.11942卷，2019年。
    [在线]。可用链接：http://arxiv.org/abs/1909.11942'
- en: '[12] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. B. Tenenbaum, “Hierarchical
    Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,”
    *CoRR*, vol. abs/1604.06057, 2016\. [Online]. Available: http://arxiv.org/abs/1604.06057'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] T. D. Kulkarni, K. Narasimhan, A. Saeedi 和 J. B. Tenenbaum, “层次化深度强化学习：整合时间抽象和内在动机，”
    *CoRR*, vol. abs/1604.06057, 2016。 [在线]。可用： http://arxiv.org/abs/1604.06057'
- en: '[13] M. A. Habib, H. Zhou, P. E. Iturria-Rivera, M. Elsayed, M. Bavand, R. Gaigalas,
    S. Furr, and M. Erol-Kantarci, “Traffic Steering for 5G Multi-RAT Deployments
    using Deep Reinforcement Learning,” in *2023 IEEE 20th Consumer Communications
    & Networking Conference*, 2023, pp. 164–169.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. A. Habib, H. Zhou, P. E. Iturria-Rivera, M. Elsayed, M. Bavand, R.
    Gaigalas, S. Furr 和 M. Erol-Kantarci, “使用深度强化学习进行5G多无线接入技术部署的流量引导，” 见 *2023 IEEE第20届消费者通信与网络会议*,
    2023年，第164–169页。'
- en: '[14] Y. Song, S. H. Lim, and S.-W. Jeon, “Handover Decision Making for Dense
    HetNets: A Reinforcement Learning Approach,” *IEEE Access*, vol. 11, pp. 24 737–24 751,
    2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Song, S. H. Lim 和 S.-W. Jeon, “密集HetNets的切换决策：一种强化学习方法，” *IEEE Access*,
    第11卷，第24,737–24,751页，2023年。'
- en: '[15] E. Dahlman, S. Parkvall, and J. Skold, *4G, LTE-Advanced Pro and The Road
    to 5G, Third Edition*, 3rd ed.   USA: Academic Press, Inc., 2016.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] E. Dahlman, S. Parkvall 和 J. Skold, *4G, LTE-Advanced Pro 和通往5G之路，第三版*,
    第3版。 美国：Academic Press, Inc., 2016。'
- en: '[16] J. Navarro-Ortiz, P. Romero-Diaz, S. Sendra, P. Ameigeiras, J. J. Ramos-Munoz,
    and J. M. Lopez-Soler, “A Survey on 5G Usage Scenarios and Traffic Models,” *IEEE
    Communications Surveys & Tutorials*, vol. 22, no. 2, pp. 905–929, 2020.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Navarro-Ortiz, P. Romero-Diaz, S. Sendra, P. Ameigeiras, J. J. Ramos-Munoz
    和 J. M. Lopez-Soler, “关于5G使用场景和流量模型的调查，” *IEEE Communications Surveys & Tutorials*,
    第22卷，第2期，第905–929页，2020年。'
