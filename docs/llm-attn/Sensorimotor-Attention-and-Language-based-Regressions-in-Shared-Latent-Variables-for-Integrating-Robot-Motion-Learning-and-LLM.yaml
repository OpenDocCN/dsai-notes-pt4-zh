- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:52:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Sensorimotor Attention and Language-based Regressions in Shared Latent Variables
    for Integrating Robot Motion Learning and LLM
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享潜变量中的传感器运动注意力和基于语言的回归，用于集成机器人运动学习和LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.09044](https://ar5iv.labs.arxiv.org/html/2407.09044)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.09044](https://ar5iv.labs.arxiv.org/html/2407.09044)
- en: Kanata Suzuki^(1,2) and Tetsuya Ogata^(1,3) All authors are affiliated with
    Faculty of Science and Engineering, Waseda University, Tokyo 169-8050, Japan.
    ^(1,2)Kanata Suzuki is also at Artificial Intelligence Laboratories, Fujitsu Limited.,
    Kanagawa 211-8588, Japan. ^(1,3)Tetsuya Ogata is also at the National Institute
    of Advanced Industrial Science and Technology, Tokyo 100-8921, Japan. E-mail:suzuki.kanata@fujitsu.com
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kanata Suzuki^(1,2) 和 Tetsuya Ogata^(1,3) 所有作者均隶属于日本东京早稻田大学科学与工程学院。 ^(1,2)Kanata
    Suzuki 还在日本神奈川县富士通有限公司人工智能实验室工作。 ^(1,3)Tetsuya Ogata 还在日本东京先进工业科学技术研究院工作。电子邮件：suzuki.kanata@fujitsu.com
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In recent years, studies have been actively conducted on combining large language
    models (LLM) and robotics; however, most have not considered end-to-end feedback
    in the robot-motion generation phase. The prediction of deep neural networks must
    contain errors, it is required to update the trained model to correspond to the
    real environment to generate robot motion adaptively. This study proposes an integration
    method that connects the robot-motion learning model and LLM using shared latent
    variables. When generating robot motion, the proposed method updates shared parameters
    based on prediction errors from both sensorimotor attention points and task language
    instructions given to the robot. This allows the model to search for latent parameters
    appropriate for the robot task efficiently. Through simulator experiments on multiple
    robot tasks, we demonstrated the effectiveness of our proposed method from two
    perspectives: position generalization and language instruction generalization
    abilities.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，结合大型语言模型（LLM）与机器人技术的研究正在积极进行；然而，大多数研究没有考虑在机器人运动生成阶段的端到端反馈。深度神经网络的预测必然存在误差，因此需要更新训练模型以适应真实环境，从而使机器人运动能够自适应生成。本研究提出了一种集成方法，通过共享潜变量将机器人运动学习模型与LLM连接起来。在生成机器人运动时，所提出的方法根据来自传感器运动注意点和任务语言指令的预测误差更新共享参数。这使得模型能够高效地寻找适合机器人任务的潜在参数。通过对多个机器人任务的仿真实验，我们从位置泛化和语言指令泛化能力两个角度验证了所提方法的有效性。
- en: I INTRODUCTION
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: The application of large language models (LLMs) to robotics is actively being
    explored in many studies [[1](#bib.bib1)][[2](#bib.bib2)][[3](#bib.bib3)][[4](#bib.bib4)].
    Many practical systems designed the LLM and robot-motion control unit independently
    and demonstrated high generalization ability in zero-shot inference. However,
    in such cascade-type configurations, it is difficult to incorporate end-to-end
    feedback for updating the model from the robot at the raw sensor level. The predictions
    of deep neural networks (DNNs) trained with offline data must contain errors,
    and thus, to generate robot motion adaptively, it is necessary to update the model
    to correspond to the real environment. Although there have been studies that used
    dialogue with collaborators or foundation models to determine the success or failure
    of robot tasks [[5](#bib.bib5)][[6](#bib.bib6)][[7](#bib.bib7)], they did not
    consider the feedback necessary for online control at the joint motor level.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在机器人技术中的应用在许多研究中得到了积极探索[[1](#bib.bib1)][[2](#bib.bib2)][[3](#bib.bib3)][[4](#bib.bib4)]。许多实际系统独立设计了LLM和机器人运动控制单元，并展示了零-shot
    推理中的高泛化能力。然而，在这种级联型配置中，很难将端到端反馈纳入模型更新，以便从原始传感器级别的机器人进行更新。使用离线数据训练的深度神经网络（DNNs）的预测必然存在误差，因此，为了自适应地生成机器人运动，必须更新模型以适应真实环境。尽管已有研究利用与协作者或基础模型的对话来判断机器人任务的成功或失败[[5](#bib.bib5)][[6](#bib.bib6)][[7](#bib.bib7)]，但它们没有考虑在关节电机级别上进行在线控制所需的反馈。
- en: On the other hand, there are some methods for building the DNN model from scratch
    by learning multimodal data including robot motion sequences [[3](#bib.bib3)][[4](#bib.bib4)].
    These methods collect paired data of motion and other modalities (language, images,
    etc.) by operating multiple robots over long periods. By fine-tuning additional
    parameters while fixing some or all of the learned weights of the LLM, the model
    can connect the modality representation with the real environment. Although these
    are promising efforts to overcome the limitations of LLMs, they do not consider
    online feedback because of the high cost of data collection and model training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，也有一些方法通过学习多模态数据从头构建 DNN 模型，包括机器人运动序列[[3](#bib.bib3)][[4](#bib.bib4)]。这些方法通过在长时间内操作多个机器人来收集运动与其他模态（语言、图像等）的配对数据。通过在固定部分或全部已学习的
    LLM 权重的同时微调额外的参数，该模型可以将模态表示与真实环境连接起来。虽然这些都是克服 LLM 限制的有希望的努力，但由于数据收集和模型训练的高成本，它们没有考虑在线反馈。
- en: To summarize, integrated learning methods for LLMs and robot motion need to
    be able to adaptively predict using a small training cost. In the field of natural
    language processing, past studies changed the behaviors of their models without
    extensive training by partially fine-tuning the LLM weights [[8](#bib.bib8)] or
    attaching external storage [[9](#bib.bib9)]. The approach of connecting the motion
    learning model and language model using shared parameters has also been explored
    in a variety of fields, including cognitive robotics [[10](#bib.bib10)][[11](#bib.bib11)][[12](#bib.bib12)].
    Considering the grounding of language and robot motion [[13](#bib.bib13)], it
    is important to update the connecting parameters using both language and sensor
    information.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，LLM 和机器人运动的集成学习方法需要能够以较低的训练成本自适应地进行预测。在自然语言处理领域，以往的研究通过部分微调 LLM 权重[[8](#bib.bib8)]或附加外部存储[[9](#bib.bib9)]，在没有大量训练的情况下改变模型行为。利用共享参数连接运动学习模型和语言模型的方法也在认知机器人[[10](#bib.bib10)][[11](#bib.bib11)][[12](#bib.bib12)]等多个领域得到探索。考虑到语言和机器人运动的基础[[13](#bib.bib13)]，使用语言和传感器信息更新连接参数非常重要。
- en: In this study, we propose an integration framework that connects the motion
    learning model and LLM using shared latent variables. We use a Spatial Attention
    Transformer RNN (SATrRNN), which predicts attention points for robot task learning,
    and a Receptance Weighted Key Value (RWKV [[14](#bib.bib14)]), which is an LLM
    with a linear attention mechanism. When generating robot motions, the proposed
    method updates shared parameters based on prediction errors from both attention
    points for sensor information and task instruction sentences given to the robot
    (Fig. [1](#S1.F1 "Figure 1 ‣ I INTRODUCTION ‣ Sensorimotor Attention and Language-based
    Regressions in Shared Latent Variables for Integrating Robot Motion Learning and
    LLM")). This allows the model to efficiently search for parameters appropriate
    for the target robot task in its latent space. We conducted learning experiments
    for manipulation tasks on a simulator and demonstrated the effectiveness of our
    proposed method.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一个集成框架，通过共享潜在变量将运动学习模型和 LLM 连接起来。我们使用了一个空间注意力变换器 RNN（SATrRNN），它预测用于机器人任务学习的注意力点，以及一个具有线性注意力机制的
    LLM——接收加权关键值（RWKV[[14](#bib.bib14)]）。在生成机器人运动时，所提出的方法根据来自传感器信息的注意力点和给定给机器人的任务指令句子的预测误差来更新共享参数（见图[1](#S1.F1
    "图 1 ‣ I 引言 ‣ 基于共享潜在变量的传感-运动注意力和语言回归的机器人运动学习与 LLM 集成")）。这使得模型能够在其潜在空间中有效地搜索适合目标机器人任务的参数。我们在模拟器上进行了操控任务的学习实验，并展示了我们所提方法的有效性。
- en: '![Refer to caption](img/0305bcdf99bc579cc8546cbe3adf4553.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0305bcdf99bc579cc8546cbe3adf4553.png)'
- en: 'Figure 1: Overview of this study. In the proposed method, latent variables
    related to robot tasks are updated based on prediction errors for instruction
    sentences and sensorimotor attention.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：本研究的概述。在所提方法中，与机器人任务相关的潜在变量基于指令句子和传感-运动注意力的预测误差进行更新。
- en: II RELATED WORK
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: '![Refer to caption](img/8a860b80b9586bf8e23b35e8e3ff5afb.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8a860b80b9586bf8e23b35e8e3ff5afb.png)'
- en: 'Figure 2: Overview of the proposed method, consisting of three modules: SATrRNN
    with mask predictor, RWKV, and shared latent variables.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：所提方法的概述，包括三个模块：带掩模预测器的 SATrRNN、RWKV 和共享潜在变量。
- en: The generalization ability of foundation models has greatly increased the possibility
    of real-world applications in robotics. In particular, task planning [[2](#bib.bib2)][[15](#bib.bib15)][[16](#bib.bib16)][[17](#bib.bib17)]
    and object recognition methods [[18](#bib.bib18)][[19](#bib.bib19)][[20](#bib.bib20)]
    have shown higher accuracies than those of conventional methods for connecting
    natural language and our daily environment. However, the prediction results of
    DNN models trained with offline data may be inconsistent with the behavior of
    a real robot. To address this problem, Brohan et al. performed language-conditioned
    robot tasks in a real environment by connecting LLM’s task planning with a motion
    learning model [[3](#bib.bib3)]. They developed the system for determining the
    next action by linking a model that predicts the probability of successfully executing
    a skill with an LLM that evaluates the probability of selecting each skill. However,
    because the skills are predefined, the model tends to lack versatility in terms
    of the tasks that it can accomplish and requires a large amount of data for its
    training.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的泛化能力大大提高了其在机器人领域实际应用的可能性。特别是，任务规划[[2](#bib.bib2)][[15](#bib.bib15)][[16](#bib.bib16)][[17](#bib.bib17)]和物体识别方法[[18](#bib.bib18)][[19](#bib.bib19)][[20](#bib.bib20)]在将自然语言与我们日常环境连接方面显示出了比传统方法更高的准确性。然而，使用离线数据训练的DNN模型的预测结果可能与真实机器人行为不一致。为了解决这个问题，Brohan等人通过将LLM的任务规划与运动学习模型相连接，在真实环境中执行了语言条件的机器人任务[[3](#bib.bib3)]。他们开发了一个系统，通过将预测成功执行技能的概率模型与评估每个技能选择概率的LLM连接起来，以确定下一步动作。然而，由于技能是预定义的，该模型在能够完成的任务方面往往缺乏多样性，并且需要大量的数据进行训练。
- en: On the other hand, approaches to modifying prediction results have also been
    actively studied [[5](#bib.bib5)][[6](#bib.bib6)][[7](#bib.bib7)][[21](#bib.bib21)][[22](#bib.bib22)].
    Huang et al. incorporated closed-loop verbal feedback based on action success
    signals into task planning and performed robot tasks with high-level commands [[5](#bib.bib5)].
    Ren et al. proposed a method that presents actions with confidence ratings during
    task planning using an LLM and poses questions to collaborators when uncertainty
    is high [[7](#bib.bib7)]. Hori et al. attempted to use an LLM to analyze the uncertainty
    of task planning and improve long-horizon planning results through a question-and-answer
    approach [[22](#bib.bib22)]. However, these past studies did not consider sending
    feedback to the LLMs at the sensor information level, which is important for online
    robot-motion generation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，修改预测结果的方法也得到了积极研究[[5](#bib.bib5)][[6](#bib.bib6)][[7](#bib.bib7)][[21](#bib.bib21)][[22](#bib.bib22)]。Huang等人将基于动作成功信号的闭环语言反馈融入任务规划中，并用高级命令执行了机器人任务[[5](#bib.bib5)]。Ren等人提出了一种方法，在任务规划过程中使用LLM呈现带有置信度评分的动作，并在不确定性较高时向协作者提出问题[[7](#bib.bib7)]。Hori等人尝试使用LLM分析任务规划的不确定性，并通过问答方法改善长期规划结果[[22](#bib.bib22)]。然而，这些过去的研究并未考虑在传感器信息层面向LLM发送反馈，而这对于在线机器人运动生成非常重要。
- en: Studies on connecting language and robot-motion learning have been conducted
    since before the LLM era [[23](#bib.bib23)][[24](#bib.bib24)][[25](#bib.bib25)][[26](#bib.bib26)][[12](#bib.bib12)].
    Particularly in DNN-based methods, there has been a study that integrated language
    and motion representations by bringing them closer together in latent space [[23](#bib.bib23)].
    It has also been reported that the bidirectional transformation of modalities
    can be accomplished using a common latent space [[24](#bib.bib24)][[25](#bib.bib25)][[26](#bib.bib26)].
    In this way, learning constraints regarding multiple modalities on the latent
    space are effective for interactive robot control.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 连接语言和机器人运动学习的研究在LLM时代之前就已经开始了[[23](#bib.bib23)][[24](#bib.bib24)][[25](#bib.bib25)][[26](#bib.bib26)][[12](#bib.bib12)]。特别是在基于DNN的方法中，有一项研究通过将语言和运动表示在潜在空间中靠近整合起来[[23](#bib.bib23)]。还报告了使用共同潜在空间可以实现模态的双向转换[[24](#bib.bib24)][[25](#bib.bib25)][[26](#bib.bib26)]。通过这种方式，关于潜在空间的多模态学习约束对于交互式机器人控制是有效的。
- en: The approach that we propose in this study is to perform error regressions at
    the motion generation phase for latent parameters that connect an LLM and a motion
    learning model. This approach is based on the concept of predictive coding [[27](#bib.bib27)],
    which controls model behavior based on minimizing prediction errors, and we apply
    it to robot learning [[28](#bib.bib28)]. By using language and attention information
    based on robot motion learning as a prediction error, it is expected that parameters
    are updated to reflect more important grounding information in the robot task.
    To the best of our knowledge, the integration of an LLM and the robot learning
    model using this approach has not been performed in past studies and is an important
    contribution of this study.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本研究中提出的方法是在运动生成阶段对连接 LLM 和运动学习模型的潜在参数进行误差回归。这种方法基于预测编码的概念 [[27](#bib.bib27)]，该概念基于最小化预测误差来控制模型行为，我们将其应用于机器人学习
    [[28](#bib.bib28)]。通过使用基于机器人运动学习的语言和注意力信息作为预测误差，预计参数会更新，以反映在机器人任务中更重要的基础信息。据我们了解，使用这种方法将
    LLM 和机器人学习模型整合起来的工作在过去的研究中尚未进行，这是本研究的重要贡献。
- en: III PROPOSED METHOD
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 提议的方法
- en: The proposed model consists of a spatial attention transformer RNN (SATrRNN)
    that predicts time sequences of sensorimotor information; and RWKV [[14](#bib.bib14)],
    a lightweight LLM. This model expands a past study on language-motion integrated
    learning [[12](#bib.bib12)]. We prepare common latent variables that are connected
    to the hidden layer of the two models and simultaneously optimize the entire model
    during the training phase. An overview of the proposed model is shown in Fig. [2](#S2.F2
    "Figure 2 ‣ II RELATED WORK ‣ Sensorimotor Attention and Language-based Regressions
    in Shared Latent Variables for Integrating Robot Motion Learning and LLM"). Table
    I shows a specific configuration of the model, including its layer types and parameters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的模型包括一个预测传感运动信息时间序列的空间注意力变换器 RNN（SATrRNN）和一个轻量级的 LLM RWKV [[14](#bib.bib14)]。该模型扩展了关于语言-运动整合学习的过去研究
    [[12](#bib.bib12)]。我们准备了与两个模型的隐藏层相连的共同潜在变量，并在训练阶段同时优化整个模型。图 [2](#S2.F2 "图 2 ‣
    II 相关工作 ‣ 传感运动注意力和基于语言的回归在共享潜在变量中用于整合机器人运动学习和 LLM") 显示了提议模型的概述。表 I 显示了模型的具体配置，包括其层类型和参数。
- en: The key idea of the proposed method is to construct a common latent space by
    learning together with the motion learning model while keeping the weight parameters
    of the LLM fixed. This makes it possible to update task-appropriate latent variables
    during the motion generation phase via the backpropagation of raw sensor information
    and language prediction without extensive learning (Fig. [3](#S3.F3 "Figure 3
    ‣ III PROPOSED METHOD ‣ Sensorimotor Attention and Language-based Regressions
    in Shared Latent Variables for Integrating Robot Motion Learning and LLM")).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的方法的关键思想是通过与运动学习模型共同学习来构建一个共同的潜在空间，同时保持 LLM 的权重参数不变。这使得在运动生成阶段通过原始传感器信息和语言预测的反向传播更新任务适用的潜在变量成为可能，而无需进行广泛的学习（图 [3](#S3.F3
    "图 3 ‣ III 提议的方法 ‣ 传感运动注意力和基于语言的回归在共享潜在变量中用于整合机器人运动学习和 LLM")）。
- en: '![Refer to caption](img/b8670716d5e74aa0ceccd31da29ded72.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b8670716d5e74aa0ceccd31da29ded72.png)'
- en: 'Figure 3: Overview of the proposed error regression method. The SLV is optimized
    from the reconstruction error for language instruction and MSE between extracted
    attention points (blue circle marks) and predicted attention points (red cross
    marks).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：提议的误差回归方法概述。SLV 是从语言指令的重建误差和提取的注意点（蓝色圆圈标记）与预测的注意点（红色交叉标记）之间的均方误差中优化的。
- en: III-A Model Architecture
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 模型架构
- en: III-A1 SATrRNN
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 SATrRNN
- en: 'The SATrRNN consists of three units: an encoder unit (Fig. [2](#S2.F2 "Figure
    2 ‣ II RELATED WORK ‣ Sensorimotor Attention and Language-based Regressions in
    Shared Latent Variables for Integrating Robot Motion Learning and LLM")-i), a
    recurrent unit (Fig. [2](#S2.F2 "Figure 2 ‣ II RELATED WORK ‣ Sensorimotor Attention
    and Language-based Regressions in Shared Latent Variables for Integrating Robot
    Motion Learning and LLM")-ii), and a decoder unit (Fig. [2](#S2.F2 "Figure 2 ‣
    II RELATED WORK ‣ Sensorimotor Attention and Language-based Regressions in Shared
    Latent Variables for Integrating Robot Motion Learning and LLM")-iii). The encoder
    part has one image encoder (i-a) and two attention point encoders (i-b/c). From
    a 64$\times$3-pixel camera image, the image encoder extracts a feature map, whereas
    each attention point encoder extracts three position coordinates using spatial
    softmax [[29](#bib.bib29)]. Because two attention point encoders are used, a total
    of 6 coordinates are extracted. In this study, to enable robust feature extraction
    for the background or similar objects, we used a mask image predicted by MobileSAMv2 [[30](#bib.bib30)]
    as input to the attention point encoders. We used CLIP [[31](#bib.bib31)] to predict
    the degree of similarity between the instruction sentence and each mask region,
    and adopted the top two that were above the threshold. In cases wherein only one
    region is detected, the same one is input into the attention point encoders. It
    is expected that through the use of multiple encoders to extract coordinates,
    attention points will be predicted for each object.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SATrRNN 由三个单元组成：一个编码器单元（图 [2](#S2.F2 "图 2 ‣ II 相关工作 ‣ 用于整合机器人运动学习和大语言模型的传感运动注意力和基于语言的回归的共享潜在变量")-i），一个递归单元（图 [2](#S2.F2
    "图 2 ‣ II 相关工作 ‣ 用于整合机器人运动学习和大语言模型的传感运动注意力和基于语言的回归的共享潜在变量")-ii），和一个解码器单元（图 [2](#S2.F2
    "图 2 ‣ II 相关工作 ‣ 用于整合机器人运动学习和大语言模型的传感运动注意力和基于语言的回归的共享潜在变量")-iii）。编码器部分有一个图像编码器（i-a）和两个注意点编码器（i-b/c）。从一个
    64$\times$3 像素的相机图像中，图像编码器提取特征图，而每个注意点编码器使用空间 Softmax [[29](#bib.bib29)] 提取三个位置坐标。由于使用了两个注意点编码器，因此总共提取了
    6 个坐标。在本研究中，为了实现对背景或类似对象的稳健特征提取，我们使用了 MobileSAMv2 [[30](#bib.bib30)] 预测的掩码图像作为注意点编码器的输入。我们使用
    CLIP [[31](#bib.bib31)] 预测指令句子与每个掩码区域之间的相似度，并选择了超过阈值的前两个区域。在只检测到一个区域的情况下，将该区域输入到注意点编码器中。预计通过使用多个编码器提取坐标，将为每个对象预测注意点。
- en: The extracted 6 position coordinates and joint angles are used as input tokens
    for a 4-layer transformer encoder to learn the relationships between modalities.
    The self-attention layers of the transformer encoder predict feature vectors useful
    for task learning from redundantly extracted position coordinates. Then, a 3-layer
    LSTM network predicts the next state of the sensorimotor sequence. The output
    of the LSTM is converted into joint angles and position coordinates for the next
    step through the linear layer. The position information predicted by the LSTM
    is converted into heatmaps, and the image decoder outputs the next step image
    based on the feature map and heatmap. Image encoders and decoders extract information
    about the colors and shapes of objects by reconstructing images. This allows the
    SATrRNN to generalize object position information, learn their relationship with
    joint angles, and generate appropriate motions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的 6 个位置坐标和关节角度被用作 4 层 Transformer 编码器的输入，以学习模态之间的关系。Transformer 编码器的自注意力层从冗余提取的位置坐标中预测对任务学习有用的特征向量。然后，3
    层 LSTM 网络预测传感运动序列的下一个状态。LSTM 的输出通过线性层转换为下一个步骤的关节角度和位置坐标。LSTM 预测的位置信息被转换为热图，图像解码器基于特征图和热图输出下一步的图像。图像编码器和解码器通过重建图像提取对象的颜色和形状信息。这使得
    SATrRNN 能够概括对象位置的信息，学习它们与关节角度的关系，并生成适当的动作。
- en: '| TABLE I: Structure of Networks |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 表 I: 网络结构 |'
- en: '|  |  | Layer type | Parameter | Output shape |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 层类型 | 参数 | 输出形状 |'
- en: '|  | (i-a) | Conv2D 1 | k=3, s=1, ch=18 | 62$\times$18 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | (i-a) | Conv2D 1 | k=3, s=1, ch=18 | 62$\times$18 |'
- en: '|  | Conv2D 2 | k=3, s=1, ch=36 | 60$\times$36 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | Conv2D 2 | k=3, s=1, ch=36 | 60$\times$36 |'
- en: '|  | Conv2D 3 | k=3, s=1, ch=6 | 58$\times$6 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | Conv2D 3 | k=3, s=1, ch=6 | 58$\times$6 |'
- en: '| \hdashline | (i-b/c) | Conv2D 4, 7 | k=3, s=1, ch=9 | 62$\times$9 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline | (i-b/c) | Conv2D 4, 7 | k=3, s=1, ch=9 | 62$\times$9 |'
- en: '|  | Conv2D 5, 8 | k=3, s=1, ch=18 | 60$\times$18 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | Conv2D 5, 8 | k=3, s=1, ch=18 | 60$\times$18 |'
- en: '|  | Conv2D 6, 9 | k=3, s=1, ch=3 | 58$\times$3 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | Conv2D 6, 9 | k=3, s=1, ch=3 | 58$\times$3 |'
- en: '|  | Spatial Softmax | - | 3$\times$2 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | 空间 Softmax | - | 3$\times$2 |'
- en: '| \hdashline(a) | (ii) | Linears 1 | layer=1, hid=20 | 20 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline(a) | (ii) | 线性 1 | layer=1, hid=20 | 20 |'
- en: '| Linears 2 | layer=1, hid=20 | 20 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 线性 2 | layer=1, hid=20 | 20 |'
- en: '| Transformer Enc. | layer=4, head=4 | 20$\times$7 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Transformer Enc. | layer=4, head=4 | 20$\times$7 |'
- en: '| LSTMs | layer=3, hid=100 | 100 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | layer=3, hid=100 | 100 |'
- en: '| Linears 3 | layer=1, hid=12 | 12 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 线性 3 | layer=1, hid=12 | 12 |'
- en: '| Linears 4 | layer=1, hid=8 | 8 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 线性 4 | layer=1, hid=8 | 8 |'
- en: '| \hdashline | (iii) | Generate heatmap | - | 58$\times$6 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline | (iii) | 生成热图 | - | 58$\times$6 |'
- en: '|  | Multiply | - | 58$\times$6 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | 乘法 | - | 58$\times$6 |'
- en: '|  | Conv2DTrans. 1 | k=3, s=1, ch=18 | 60$\times$18 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | Conv2DTrans. 1 | k=3, s=1, ch=18 | 60$\times$18 |'
- en: '|  | Conv2DTrans. 2 | k=3, s=1, ch=36 | 62$\times$36 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | Conv2DTrans. 2 | k=3, s=1, ch=36 | 62$\times$36 |'
- en: '|  | Conv2DTrans. 3 | k=3, s=1, ch=3 | 64$\times$3 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | Conv2DTrans. 3 | k=3, s=1, ch=3 | 64$\times$3 |'
- en: '| (b) |  | RWKV block | layer=12, hid=768 | 768 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| (b) |  | RWKV 块 | layer=12, hid=768 | 768 |'
- en: '|  | RWKV head | hid=50277 | 50277 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | RWKV 头 | hid=50277 | 50277 |'
- en: '| (c) |  | Variable | dim=5 | 5 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| (c) |  | 变量 | dim=5 | 5 |'
- en: '|  | Linears 5–7 | layer=2, hid=100 | 100 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | 线性 5–7 | layer=2, hid=100 | 100 |'
- en: '|  | Linears 8–19 | layer=3, hid=768 | 768 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 线性 8–19 | layer=3, hid=768 | 768 |'
- en: '* All layers have leaky ReLU activation.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '* 所有层均使用泄漏 ReLU 激活函数。'
- en: '*** Linear and LSTM layers have layer normalization.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*** 线性层和 LSTM 层具有层归一化。'
- en: '**** k: kernel, s: stride, ch: channel dim, layer: layer num, hid: hidden dim.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**** k: 卷积核，s: 步幅，ch: 通道维度，layer: 层数，hid: 隐藏维度。'
- en: III-A2 RWKV
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 RWKV
- en: We use RWKV as a unit that accepts language instructions from collaborators
    and sends them to robots (Fig. [2](#S2.F2 "Figure 2 ‣ II RELATED WORK ‣ Sensorimotor
    Attention and Language-based Regressions in Shared Latent Variables for Integrating
    Robot Motion Learning and LLM")b). The RWKV has a structure in which time-mixing
    blocks, which extend the linear attention mechanism, and channel-mixing blocks
    are stacked alternately. It is possible to parallelize the training process and
    maintain the cost in terms of amount of calculation when performing inference.
    The RWKV consists of 12 blocks, each of which has a hidden state dimension of
    768. We adopted a particularly lightweight model because memory efficiency is
    important for robot motion generation. The instruction sentences are converted
    into tokens by a tokenizer, and the RWKV makes predictions to restore the input.
    The weight parameters of the RWKV are initialized with trained weights and are
    not updated during both the training and inference phases. Therefore, the prediction
    of the RWKV is modified only by shared parameters, which are described in the
    next subsection.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 RWKV 作为一个单元，接收来自协作者的语言指令并将其发送到机器人（见图 [2](#S2.F2 "Figure 2 ‣ II RELATED
    WORK ‣ Sensorimotor Attention and Language-based Regressions in Shared Latent
    Variables for Integrating Robot Motion Learning and LLM")b）。RWKV 的结构包括时间混合块（扩展了线性注意机制）和通道混合块交替堆叠。这样可以并行化训练过程，并在进行推理时保持计算成本不变。RWKV
    由 12 个块组成，每个块的隐藏状态维度为 768。由于内存效率对于机器人运动生成至关重要，我们采用了特别轻量的模型。指令句子通过分词器转换为标记，RWKV
    进行预测以恢复输入。RWKV 的权重参数使用训练好的权重进行初始化，在训练和推理阶段都不会更新。因此，RWKV 的预测仅由共享参数修改，这些共享参数将在下一小节中描述。
- en: III-A3 Shared Latent Variables
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 共享潜在变量
- en: We incorporate parametric bias [[10](#bib.bib10)] as the shared latent variable
    (SLV) into the model to obtain integration representations of language and motion
    (Fig. [2](#S2.F2 "Figure 2 ‣ II RELATED WORK ‣ Sensorimotor Attention and Language-based
    Regressions in Shared Latent Variables for Integrating Robot Motion Learning and
    LLM")c). The SLV is a 5-dimensional vector and is initialized to zero. Unlike
    other input values, the SLV does not depend on the time series and is always a
    constant value. The SATrRNN and RWKV are independent models, and each model is
    connected through multiple linear layers such that the SLV is shared. The SLVs
    are prepared for each motion sequence in the dataset and learn the correspondence
    between language and motion as a variable by updating individual parameters for
    each training sequence. Because the SATrRNN and RWKV use common weights for all
    sequences in the training phase, the SLV strongly maps the relationship between
    language and motions. Therefore, the SATrRNN can predict appropriate task motions
    based on bottom-up recognition of object position coordinates using the SLV as
    input to LSTM layers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将参数偏置 [[10](#bib.bib10)] 作为共享潜在变量 (SLV) 纳入模型，以获得语言和运动的整合表示（图 [2](#S2.F2 "图
    2 ‣ II 相关工作 ‣ 基于传感器运动注意力和语言的回归在共享潜在变量中，用于整合机器人运动学习和LLM")c）。SLV 是一个 5 维向量，初始化为零。与其他输入值不同，SLV
    不依赖于时间序列，始终是一个常量值。SATrRNN 和 RWKV 是独立模型，每个模型通过多个线性层连接，使 SLV 可以共享。为数据集中每个运动序列准备
    SLV，并通过更新每个训练序列的单独参数来学习语言与运动之间的对应关系。由于 SATrRNN 和 RWKV 在训练阶段对所有序列使用共同的权重，因此 SLV
    强烈映射语言与动作之间的关系。因此，SATrRNN 可以基于对物体位置坐标的自下而上的识别，使用 SLV 作为 LSTM 层的输入来预测适当的任务动作。
- en: III-B Training and Regression Phases
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 训练和回归阶段
- en: During the training phase, the models are optimized using an imitation learning
    scheme. The SATrRNN extracts attention points $pt^{enc}_{t}$ using the image encoders.
    Then, using $pt^{enc}_{t}$ as input, the LSTM predicts the attention points $pt^{dec}_{t+1}$
    for the next step. The image decoder receives $pt^{dec}_{t+1}$. The RWKV is configured
    to reconstruct the tokenized input sentences $s$ is expressed as follows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，模型使用模仿学习方案进行优化。SATrRNN 使用图像编码器提取注意点 $pt^{enc}_{t}$。然后，使用 $pt^{enc}_{t}$
    作为输入，LSTM 预测下一个步骤的注意点 $pt^{dec}_{t+1}$。图像解码器接收 $pt^{dec}_{t+1}$。RWKV 被配置为重建标记化的输入句子
    $s$，其表示如下。
- en: '|  | $\displaystyle L_{train}=\alpha L_{ja}+\beta(L_{img}+L_{pt})+\gamma L_{dsc}$
    |  | (1) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{train}=\alpha L_{ja}+\beta(L_{img}+L_{pt})+\gamma L_{dsc}$
    |  | (1) |'
- en: '|  | $\displaystyle L_{ja}=\frac{1}{T_{s}-1}\sum_{t=1}^{T_{s}-1}\left[(j_{t+1}-\hat{j}_{t+1})^{2}\right]$
    |  | (2) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{ja}=\frac{1}{T_{s}-1}\sum_{t=1}^{T_{s}-1}\left[(j_{t+1}-\hat{j}_{t+1})^{2}\right]$
    |  | (2) |'
- en: '|  | $\displaystyle L_{img}=\frac{1}{T_{s}-1}\sum_{t=1}^{T_{s}-1}\left[(i_{t+1}-\hat{i}_{t+1})^{2}\right]$
    |  | (3) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{img}=\frac{1}{T_{s}-1}\sum_{t=1}^{T_{s}-1}\left[(i_{t+1}-\hat{i}_{t+1})^{2}\right]$
    |  | (3) |'
- en: '|  | $1$2 |  | (4) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: '|  | $1$2 |  | (5) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: where $L_{ja}$ the MSE of the predicted image; $L_{pt}$; $L_{dsc}$ the motion
    sequence length; $T_{d}$ the target value obtained from the training dataset;
    and $W$, $\beta$ represent the loss contribution, of which only $\alpha$ and $\gamma$
    were set to 0.1. RAdam [[32](#bib.bib32)] was used as the optimization method,
    and the model was trained for a total of 5000 epochs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{ja}$ 是预测图像的均方误差；$L_{pt}$；$L_{dsc}$ 是运动序列长度；$T_{d}$ 是从训练数据集中获得的目标值；$W$
    和 $\beta$ 代表损失贡献，其中仅 $\alpha$ 和 $\gamma$ 设置为 0.1。使用 RAdam [[32](#bib.bib32)] 作为优化方法，模型训练了总共
    5000 个周期。
- en: 'In robot-motion generation, the SLV value is initialized to zero and a regression
    phase is provided to search for an appropriate SLV value before starting the task
    (Fig. [3](#S3.F3 "Figure 3 ‣ III PROPOSED METHOD ‣ Sensorimotor Attention and
    Language-based Regressions in Shared Latent Variables for Integrating Robot Motion
    Learning and LLM")). The regression loss $L_{ER}$ and optimization for the SLV,
    the behavior of the model is changed to predict the object position and the corresponding
    task motion appropriately. The weight parameters other than the SLV are fixed,
    and the SLV is updated to minimize the following loss function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人运动生成中，SLV 值初始化为零，并提供回归阶段以在任务开始之前寻找合适的 SLV 值（图 [3](#S3.F3 "图 3 ‣ III 提出的办法
    ‣ 基于传感器运动注意力和语言的回归在共享潜在变量中，用于整合机器人运动学习和LLM")）。回归损失 $L_{ER}$ 和 SLV 的优化使模型行为发生变化，以适当地预测物体位置和相应的任务运动。除了
    SLV 以外的权重参数是固定的，SLV 被更新以最小化以下损失函数：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: Here, $L_{pt}$ targets only the initial environment state. We use RAdam as the
    optimization method and update the parameters for 10 iterations. Using the SLV
    values updated through the regression phase, the model predicts the next state
    from the current joint angles and camera images. The target posture of the robot
    is defined based on the predicted joint angles, and the joints are controlled
    to generate motion online.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$L_{pt}$ 仅针对初始环境状态。我们使用RAdam作为优化方法，并更新参数10次。通过回归阶段更新的SLV值，模型从当前的关节角度和相机图像中预测下一个状态。机器人目标姿态基于预测的关节角度定义，关节被控制以生成在线运动。
- en: '![Refer to caption](img/b8720101be6d6cc8a5d4a599b5e6e082.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b8720101be6d6cc8a5d4a599b5e6e082.png)'
- en: 'Figure 4: Robot task setup in our experiments.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：我们实验中的机器人任务设置。
- en: IV EXPERIMENTS
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: '![Refer to caption](img/7a986042b085e808760c2f8b42f6030f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7a986042b085e808760c2f8b42f6030f.png)'
- en: 'Figure 5: Examples of generated Lift, Roll, and Stack task sequences in case
    2 (test position).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在案例2（测试位置）中生成的拿起、滚动和叠放任务序列示例。
- en: '![Refer to caption](img/93e09df4bc2f6062963e4468ea1aeb81.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/93e09df4bc2f6062963e4468ea1aeb81.png)'
- en: 'Figure 6: Visualized transition of internal states of LSTM layer in generated
    Lift, Roll, and Stack tasks.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：LSTM层内部状态在生成的拿起、滚动和叠放任务中的可视化过渡。
- en: IV-A Dataset
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 数据集
- en: IV-A1 Robot Task
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 机器人任务
- en: 'To verify the effectiveness of the proposed method, we conducted learning experiments
    on the cube manipulation task based on language instructions. We used Robosuite [[33](#bib.bib33)],
    a dynamics simulator built on the physics engine MuJoCo [[34](#bib.bib34)]. For
    the robot model, we used Panda, which has a total of 8 degrees of freedom (7 for
    the joints and 1 for the opening/closing gripper). Three cubes, of colors red,
    blue, and green, were placed on the table in front of the robot. The robot then
    performed three tasks: “Lift a cube,” “Stack a cube on another one,” and “Roll
    a cube.” Examples of each task and the instruction sentences are shown in Fig. [4](#S3.F4
    "Figure 4 ‣ III-B Training and Regression Phases ‣ III PROPOSED METHOD ‣ Sensorimotor
    Attention and Language-based Regressions in Shared Latent Variables for Integrating
    Robot Motion Learning and LLM"). The cubes were placed at any of the training
    positions on the table during training dataset collection. We obtained 10 patterns
    for the Lift and Roll tasks and 20 patterns for the Stack task by changing the
    arrangement of the cubes. During data collection, we controlled the robot’s posture
    using a 3D mouse, and the joint angles and camera image sequences were recorded
    at 10 Hz. The average data lengths of the tasks were 61 steps for Lift, 75 steps
    for Roll, and 133 steps for Stack.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证所提出方法的有效性，我们基于语言指令在立方体操作任务上进行了学习实验。我们使用了Robosuite [[33](#bib.bib33)]，这是一个基于物理引擎MuJoCo [[34](#bib.bib34)]构建的动态模拟器。对于机器人模型，我们使用了Panda，它具有8个自由度（7个关节自由度和1个开合夹具自由度）。三个颜色分别为红色、蓝色和绿色的立方体被放置在机器人面前的桌子上。然后，机器人执行了三个任务：“拿起一个立方体”、“将一个立方体叠放到另一个立方体上”和“滚动一个立方体”。每个任务的示例和指令句子如图 [4](#S3.F4
    "图 4 ‣ III-B 训练和回归阶段 ‣ III 提出的 方法 ‣ 感知运动注意力和基于语言的回归，用于整合机器人运动学习和大语言模型")所示。在训练数据集收集期间，立方体被放置在桌子上的任何训练位置。通过改变立方体的排列，我们获得了10种“拿起”和“滚动”任务模式，以及20种“叠放”任务模式。在数据收集过程中，我们使用3D鼠标控制机器人的姿态，关节角度和相机图像序列以10
    Hz的频率记录。任务的平均数据长度为：拿起任务61步，滚动任务75步，叠放任务133步。
- en: IV-A2 Training
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 训练
- en: We trained the model using the collected motion sequences. The input value to
    the model was normalized to be in the range [0.1, 0.9] and was augmented with
    Gaussian noise. The instruction sentences for each task were prepared as “Lift
    the [color] cube” for the Lift task, “Roll the [color] cube” for the Roll task,
    and “Stack the [color] cube to the [color]” for the Stack task. In these sentences,
    the word corresponding to the color of the cube was inserted in place of [color].
    In the RWKV training, the input sentences were augmented by randomly paraphrasing
    them into a synonymous sentence using ChatGPT. The SLV had a total of 30 patterns,
    and each was trained for individual sequences.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用收集到的运动序列对模型进行了训练。模型的输入值被规范化到[0.1, 0.9]范围内，并且添加了高斯噪声。每个任务的指令句子被准备为“拿起[颜色]立方体”用于拿起任务，“滚动[颜色]立方体”用于滚动任务，以及“将[颜色]立方体叠放到[颜色]”用于叠放任务。在这些句子中，与立方体颜色对应的词被插入到[颜色]的位置。在RWKV训练中，通过使用ChatGPT将输入句子随机改写为同义句来增强数据。SLV共有30种模式，每种模式针对单独的序列进行训练。
- en: IV-B Evaluation Metrics
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 评估指标
- en: 'During the test phase, we evaluated the proposed method from two viewpoints:
    position generalization ability and language generalization ability. To evaluate
    the former, the cube was placed at random positions based on the test position
    range shown in Fig. [4](#S3.F4 "Figure 4 ‣ III-B Training and Regression Phases
    ‣ III PROPOSED METHOD ‣ Sensorimotor Attention and Language-based Regressions
    in Shared Latent Variables for Integrating Robot Motion Learning and LLM"). To
    evaluate the latter, we used unseen paraphrased sentences (Fig. [4](#S3.F4 "Figure
    4 ‣ III-B Training and Regression Phases ‣ III PROPOSED METHOD ‣ Sensorimotor
    Attention and Language-based Regressions in Shared Latent Variables for Integrating
    Robot Motion Learning and LLM")). Specific examples of instruction sentences in
    the test case will be introduced in the next section.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试阶段，我们从两个角度评估了提出的方法：位置泛化能力和语言泛化能力。为了评估前者，将立方体放置在图[4](#S3.F4 "Figure 4 ‣ III-B
    Training and Regression Phases ‣ III PROPOSED METHOD ‣ Sensorimotor Attention
    and Language-based Regressions in Shared Latent Variables for Integrating Robot
    Motion Learning and LLM")中显示的测试位置范围内的随机位置。为了评估后者，我们使用了未见过的同义句（图[4](#S3.F4 "Figure
    4 ‣ III-B Training and Regression Phases ‣ III PROPOSED METHOD ‣ Sensorimotor
    Attention and Language-based Regressions in Shared Latent Variables for Integrating
    Robot Motion Learning and LLM")）。测试案例中的指令句子具体示例将在下一节中介绍。
- en: V RESULTS AND DISCUSSION
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结果与讨论
- en: '| TABLE II: Success Rates in Generated Robot Tasks |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 表II：生成机器人任务中的成功率 |'
- en: '| --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|  |  | Case 1 (Training position) | Case 2 (Test position) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 案例1（训练位置） | 案例2（测试位置） |'
- en: '| Lift | no ER | 25% (5/20) | 25% (5/20) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 提升 | 无ER | 25% (5/20) | 25% (5/20) |'
- en: '| w/ ER | 100% (20/20) | 90% (18/20) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 有ER | 100% (20/20) | 90% (18/20) |'
- en: '| Roll | no ER | 20% (4/20) | 15% (3/20) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 滚动 | 无ER | 20% (4/20) | 15% (3/20) |'
- en: '| w/ ER | 95% (19/20) | 90% (18/20) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 有ER | 95% (19/20) | 90% (18/20) |'
- en: '| Stack | no ER | 5% (1/20) | 0% (0/20) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 堆叠 | 无ER | 5% (1/20) | 0% (0/20) |'
- en: '| w/ ER | 85% (17/20) | 70% (14/20) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 有ER | 85% (17/20) | 70% (14/20) |'
- en: '![Refer to caption](img/0c9cb2e76215bbab664cbc90c9fc3235.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0c9cb2e76215bbab664cbc90c9fc3235.png)'
- en: 'Figure 7: Visualized transition of shared latent variables by PCA, and loss
    curve of $L_{ER}$.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：通过PCA可视化的共享潜在变量的过渡，以及$L_{ER}$的损失曲线。
- en: V-A Task Performance
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 任务表现
- en: Table II shows the success rates in the generated tasks with and without error
    regression (ER). Three colored cubes were placed at the training position in case
    1 or the test position in case 2. Note that although the object positions of case
    1 have been included in the training dataset, the arrangement and color combinations
    were not included. The results showed that the proposed method performed high
    success rates in all Lift, Roll, and Stack tasks at the training positions. The
    proposed method also performed high success rates at the test positions, even
    though slightly lower. Examples of the generated tasks for case 2 are shown in
    Fig. [5](#S4.F5 "Figure 5 ‣ IV EXPERIMENTS ‣ Sensorimotor Attention and Language-based
    Regressions in Shared Latent Variables for Integrating Robot Motion Learning and
    LLM"). The red cross marks in the figure indicate attention points predicted by
    the LSTM. In the Lift and Roll tasks, the model predicted attention points only
    for the target cube. On the other hand, in the Stack task, the model predicted
    attention points for each of the two cubes to be operated. This allowed the robot
    to perform tasks according to the position of the object.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表II显示了在生成任务中有无误差回归（ER）的成功率。三个彩色立方体被放置在案例1的训练位置或案例2的测试位置。请注意，尽管案例1的物体位置已包含在训练数据集中，但排列和颜色组合并未包括在内。结果显示，提出的方法在训练位置的所有提升、滚动和堆叠任务中都表现出高成功率。尽管在测试位置的成功率稍低，但提出的方法在测试位置也表现出高成功率。案例2的生成任务示例如图[5](#S4.F5
    "Figure 5 ‣ IV EXPERIMENTS ‣ Sensorimotor Attention and Language-based Regressions
    in Shared Latent Variables for Integrating Robot Motion Learning and LLM")所示。图中的红色十字标记表示由LSTM预测的关注点。在提升和滚动任务中，模型仅对目标立方体预测了关注点。另一方面，在堆叠任务中，模型对要操作的两个立方体分别预测了关注点。这使得机器人能够根据物体的位置执行任务。
- en: When the tasks described earlier were generated, paraphrases based on the instruction
    sentences shown in Fig. [4](#S3.F4 "Figure 4 ‣ III-B Training and Regression Phases
    ‣ III PROPOSED METHOD ‣ Sensorimotor Attention and Language-based Regressions
    in Shared Latent Variables for Integrating Robot Motion Learning and LLM") were
    used. Examples of paraphrases included patterns in which verbs change, nouns and
    adjectives change, and adverbs change (shown in Fig. [5](#S4.F5 "Figure 5 ‣ IV
    EXPERIMENTS ‣ Sensorimotor Attention and Language-based Regressions in Shared
    Latent Variables for Integrating Robot Motion Learning and LLM")). By estimating
    a common task expression through the ER phase, the proposed method could accept
    language instructions in various expressions. On the other hand, failure examples
    included cases where MobileSAMv2 detection failed and cases where the wrong task
    was selected in the shared latent space. These cases can be expected to be improved
    by changing the LLM or the prompt for the model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成前述任务时，使用了基于图 [4](#S3.F4 "图 4 ‣ III-B 训练和回归阶段 ‣ III 提出的方案 ‣ 传感运动注意力和基于语言的回归在共享潜在变量中的整合机器人运动学习和大语言模型")
    所示指令句子的同义表达。同义表达的示例包括动词变化、名词和形容词变化以及副词变化的模式（见图 [5](#S4.F5 "图 5 ‣ IV 实验 ‣ 传感运动注意力和基于语言的回归在共享潜在变量中的整合机器人运动学习和大语言模型")）。通过
    ER 阶段估计一个通用任务表达，所提方法可以接受各种表达形式的语言指令。另一方面，失败的例子包括 MobileSAMv2 检测失败的情况和在共享潜在空间中选择了错误任务的情况。这些情况可以通过更改
    LLM 或模型的提示来改进。
- en: V-B Analysis of Model
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 模型分析
- en: V-B1 Internal State of RNN
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 RNN 的内部状态
- en: Next, we verified the behavior of the proposed method by analyzing its internal
    state. Fig. [6](#S4.F6 "Figure 6 ‣ IV EXPERIMENTS ‣ Sensorimotor Attention and
    Language-based Regressions in Shared Latent Variables for Integrating Robot Motion
    Learning and LLM") shows the results of principal component analysis (PCA) visualization
    up to the third principal component in the hidden state of the third layer of
    the LSTM. Each subfigure shows the internal state transition in the corresponding
    generated task sequence outlined in the previous subsection. For all tasks, the
    internal state of the LSTM corresponds to the cube positions, indicating that
    the proposed method generalizes the position information of the object. Additionally,
    in the Stack task, the PCA space for picking objects and moving and placing objects
    are separate. This indicates that the proposed method embedded the approach trajectories
    to the object position into its internal state, and changed this prediction by
    changing the SLV.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过分析所提方法的内部状态来验证其行为。图 [6](#S4.F6 "图 6 ‣ IV 实验 ‣ 传感运动注意力和基于语言的回归在共享潜在变量中的整合机器人运动学习和大语言模型")
    显示了在 LSTM 第三层的隐藏状态中，主成分分析 (PCA) 视觉化的结果，直至第三主成分。每个子图展示了在上一小节概述的相应生成任务序列中的内部状态转变。对于所有任务，LSTM
    的内部状态与立方体的位置相对应，这表明所提方法对物体的位置进行了泛化。此外，在 Stack 任务中，捡取物体和移动与放置物体的 PCA 空间是分开的。这表明所提方法将接近物体的位置的轨迹嵌入了其内部状态，并通过改变
    SLV 改变了这一预测。
- en: V-B2 Shared Latent Space
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 共享潜在空间
- en: The results of visualizing the SLV transition using PCA are shown on the left
    of Fig. [7](#S5.F7 "Figure 7 ‣ V RESULTS AND DISCUSSION ‣ Sensorimotor Attention
    and Language-based Regressions in Shared Latent Variables for Integrating Robot
    Motion Learning and LLM"), and the loss curve of $L_{ER}$ for each task is shown
    on the right of Fig. [7](#S5.F7 "Figure 7 ‣ V RESULTS AND DISCUSSION ‣ Sensorimotor
    Attention and Language-based Regressions in Shared Latent Variables for Integrating
    Robot Motion Learning and LLM"). The black point in the figure indicates the initial
    state (zero vector), whereas the colored points represent the updated states in
    the iterations during the ER. It can be observed that for each task, the SLVs
    cluster together as the loss function decreases. This indicates that the proposed
    method has an embedded representation suitable for each task in the SLV, and is
    updated to an appropriate value by the ER of the predicted attention points and
    language. This language generalization ability allows the proposed method to accept
    diverse language instructions. The large variation in the SLVs for the Roll task
    is due to the wide range of expressions for the instruction sentences. For example,
    unnatural words such as “spin” and “twist” were used in the instructions for cube
    manipulation. The aforementioned is a future problem, and it may be solved by
    introducing uncertainty prediction [[7](#bib.bib7)].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用主成分分析（PCA）可视化SLV过渡的结果显示在图[7](#S5.F7 "Figure 7 ‣ V RESULTS AND DISCUSSION ‣
    Sensorimotor Attention and Language-based Regressions in Shared Latent Variables
    for Integrating Robot Motion Learning and LLM")的左侧，$L_{ER}$的每个任务的损失曲线显示在图[7](#S5.F7
    "Figure 7 ‣ V RESULTS AND DISCUSSION ‣ Sensorimotor Attention and Language-based
    Regressions in Shared Latent Variables for Integrating Robot Motion Learning and
    LLM")的右侧。图中的黑色点表示初始状态（零向量），而彩色点表示ER过程中的迭代更新状态。可以观察到，对于每个任务，SLV随着损失函数的减少而聚集在一起。这表明，所提出的方法具有适用于SLV中每个任务的嵌入表示，并且通过预测的注意点和语言的ER更新到适当的值。这种语言泛化能力使得所提出的方法能够接受多样的语言指令。Roll任务中SLV的较大变化是由于指令句子的表达范围广泛。例如，在立方体操作的指令中使用了“spin”和“twist”等不自然的词汇。上述问题是未来的一个挑战，可能通过引入不确定性预测[[7](#bib.bib7)]得到解决。
- en: V-B3 Self-Attention of Transformer Encoder
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3 Transformer编码器的自注意力
- en: Finally, by visualizing the self-attention layer of the Transformer encoder,
    we check how the relationship between the attention points and joint angles was
    learned. Fig. [8](#S5.F8 "Figure 8 ‣ V-B3 Self-Attention of Transformer Encoder
    ‣ V-B Analysis of Model ‣ V RESULTS AND DISCUSSION ‣ Sensorimotor Attention and
    Language-based Regressions in Shared Latent Variables for Integrating Robot Motion
    Learning and LLM") shows the activity of self-attention layers 1–4 in the Stack
    task shown in Fig. [5](#S4.F5 "Figure 5 ‣ IV EXPERIMENTS ‣ Sensorimotor Attention
    and Language-based Regressions in Shared Latent Variables for Integrating Robot
    Motion Learning and LLM"). The ja columns and rows in the figure indicate joint
    angles, pt1–pt3 columns and rows the output of the attention point encoder (i-b),
    and pt4–pt6 columns and rows another attention point encoder (i-c). In layer 1,
    attention is strongly expressed between the joint angles and each attention point,
    but as we move to higher layers, the influence of the attention points becomes
    stronger. Based on the time series, as the task approaches its end, the attention
    points gradually become weaker, and eventually, the joint angle information becomes
    the most important. This result shows that the transformer encoder layers dynamically
    change the representation as the values of the attention points and joint angles
    change. It also shows that the recurrent unit can handle cases wherein the extracted
    attention points are redundant.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过可视化Transformer编码器的自注意力层，我们检查了注意点与关节角度之间的关系是如何学习的。图[8](#S5.F8 "Figure 8
    ‣ V-B3 Self-Attention of Transformer Encoder ‣ V-B Analysis of Model ‣ V RESULTS
    AND DISCUSSION ‣ Sensorimotor Attention and Language-based Regressions in Shared
    Latent Variables for Integrating Robot Motion Learning and LLM")展示了图[5](#S4.F5
    "Figure 5 ‣ IV EXPERIMENTS ‣ Sensorimotor Attention and Language-based Regressions
    in Shared Latent Variables for Integrating Robot Motion Learning and LLM")中Stack任务的自注意力层1–4的活动。图中的ja列和行表示关节角度，pt1–pt3列和行表示第一个注意点编码器（i-b）的输出，pt4–pt6列和行表示另一个注意点编码器（i-c）。在层1中，注意力在关节角度和每个注意点之间表现得很强，但随着层数的增加，注意点的影响变得更强。根据时间序列，当任务接近结束时，注意点逐渐变弱，最终关节角度信息变得最重要。这个结果表明，transformer编码器层在注意点和关节角度的值变化时动态地改变表示。它还表明，循环单元能够处理提取的注意点冗余的情况。
- en: '![Refer to caption](img/8ea1a72b9b1ee09faf7a7eaf5e48cd4b.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8ea1a72b9b1ee09faf7a7eaf5e48cd4b.png)'
- en: 'Figure 8: Visualized self-attention layer of Transformer encoder.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Transformer 编码器的可视化自注意力层。
- en: VI CONCLUSION
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: 'In this paper, we proposed an integrated learning method to efficiently connect
    LLM and robot-motion learning. The proposed method adopted SATrRNN, which predicts
    attention points for motion prediction, and RWKV as the LLM. We then connected
    the models using shared latent variables. During the process of robot-motion generation,
    an appropriate SLV value is determined based on backpropagation with the prediction
    errors from the predicted attention points and generated sentences. To evaluate
    the proposed method, we conducted a learning experiment on a simulator for three
    tasks: Lift, Roll, and Stack, on cubes of three colors. Our experimental results
    showed that the proposed method was able to generate appropriate motions through
    the error regression phase. Analysis of the internal state of the model showed
    the effectiveness of our method from two perspectives: position generalization
    and language instruction generalization. We plan to extend the proposed method
    to more complex tasks [[35](#bib.bib35)][[36](#bib.bib36)] and to integrate with
    LLM task planning [[22](#bib.bib22)].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种集成学习方法，以高效地连接大语言模型（LLM）和机器人运动学习。提出的方法采用了 SATrRNN，它用于预测运动预测的注意点，并使用
    RWKV 作为 LLM。然后，我们通过共享潜在变量连接了这些模型。在机器人运动生成过程中，根据从预测的注意点和生成的句子中得到的预测误差进行反向传播，确定一个合适的
    SLV 值。为了评估该方法，我们在模拟器上进行了三项任务的学习实验：Lift、Roll 和 Stack，使用三种颜色的立方体。我们的实验结果表明，提出的方法能够通过误差回归阶段生成适当的动作。对模型内部状态的分析从位置泛化和语言指令泛化两个角度证明了我们方法的有效性。我们计划将提出的方法扩展到更复杂的任务[[35](#bib.bib35)][[36](#bib.bib36)]，并与
    LLM 任务规划[[22](#bib.bib22)]进行集成。
- en: ACKNOWLEDGMENT
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This work was supported by JST Moonshot R&D Grant Number JPMJMS2031 and JSPS
    Grant-in-Aid for Early-Career Scientists (Grant Number: 24K20877), Japan.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了 JST Moonshot R&D 资助编号 JPMJMS2031 和 JSPS 青年科学家资助（资助编号：24K20877），日本的支持。
- en: References
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where pathways
    for robotic manipulation,” in *Proc. of the 5th Conf. on Robot Learning*, 2021.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Shridhar, L. Manuelli, 和 D. Fox, “Cliport：机器人操作的什么和哪里路径，” 在 *第5届机器人学习会议论文集*，2021。'
- en: '[2] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    and A. Zeng, “Code as policies: Language model programs for embodied control,”
    2022.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    和 A. Zeng, “代码作为策略：语言模型程序用于体现控制，” 2022。'
- en: '[3] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan,
    K. Hausman, A. Herzog, J. Hsu *et al.*, “Rt-1: Robotics transformer for real-world
    control at scale,” 2022.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan,
    K. Hausman, A. Herzog, J. Hsu *等*, “Rt-1：用于大规模现实世界控制的机器人变换器，” 2022。'
- en: '[4] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid,
    J. Tompson, Q. Vuong, T. Yu *et al.*, “Palm-e: An embodied multimodal language
    model,” *arXiv preprint arXiv:2303.03378*, 2023.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A.
    Wahid, J. Tompson, Q. Vuong, T. Yu *等*, “Palm-e：一种体现的多模态语言模型，” *arXiv 预印本 arXiv:2303.03378*,
    2023。'
- en: '[5] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson,
    I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine,
    K. Hausman, and B. Ichter, “Inner monologue: Embodied reasoning through planning
    with language models,” *arXiv preprint arXiv:2207.05608*, 2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J.
    Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu,
    S. Levine, K. Hausman, 和 B. Ichter, “内心独白：通过与语言模型的规划实现体现推理，” *arXiv 预印本 arXiv:2207.05608*,
    2022。'
- en: '[6] W. Huang, F. Xia, D. Shah, D. Driess, A. Zeng, Y. Lu, P. Florence, I. Mordatch,
    S. Levine, K. Hausman, and B. Ichter, “Grounded decoding: Guiding text generation
    with grounded models for embodied agents,” *arXiv preprint arXiv:2303.00855*,
    2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] W. Huang, F. Xia, D. Shah, D. Driess, A. Zeng, Y. Lu, P. Florence, I. Mordatch,
    S. Levine, K. Hausman, 和 B. Ichter, “基于地面的解码：通过基于地面的模型指导文本生成以实现体现体代理，” *arXiv
    预印本 arXiv:2303.00855*, 2023。'
- en: '[7] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama,
    F. Xia, J. Varley, Z. Xu, D. Sadigh, A. Zeng, and A. Majumdar, “Robots That Ask
    For Help: Uncertainty Alignment for Large Language Model Planners,” *arXiv preprint
    arXiv:2307.01928*, 2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama,
    F. Xia, J. Varley, Z. Xu, D. Sadigh, A. Zeng, 和 A. Majumdar, “寻求帮助的机器人：大语言模型规划者的不确定性对齐，”
    *arXiv 预印本 arXiv:2307.01928*, 2023。'
- en: '[8] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “LoRA: Low-rank adaptation of large language models,” in *Proc. of the
    Int. Conf. on Learning Representations*, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen，“LoRA：大规模语言模型的低秩适应，” 见 *国际学习表示会议论文集*，2022。'
- en: '[9] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler,
    M. Lewis, W.-t. Yih, T. Rocktäschel *et al.*, “Retrieval-augmented generation
    for knowledge-intensive nlp tasks,” *Advances in Neural Information Processing
    Systems*, vol. 33, pp. 9459–9474, 2020.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler,
    M. Lewis, W.-t. Yih, T. Rocktäschel *等*，“用于知识密集型NLP任务的检索增强生成，” *神经信息处理系统进展*，第33卷，第9459–9474页，2020。'
- en: '[10] J. Tani, “Self-organization of behavioral primitives as multiple attractor
    dynamics: a robot experiment,” in *Proc. of the Int. Joint Conf. on Neural Networks*,
    vol. 1, 2002, pp. 489–494.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Tani，“行为原型的自组织作为多重吸引子动态：一项机器人实验，” 见 *国际联合神经网络会议论文集*，第1卷，2002，第489–494页。'
- en: '[11] T. Ogata, M. Murase, J. Tani, K. Komatani, and H. G. Okuno, “Two-way translation
    of compound sentences and arm motions by recurrent neural networks,” in *Proc.
    of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems*, 2007, pp. 1858–1863.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] T. Ogata, M. Murase, J. Tani, K. Komatani, 和 H. G. Okuno，“复合句和手臂动作的双向翻译：基于递归神经网络，”
    见 *IEEE/RSJ国际智能机器人与系统会议论文集*，2007，第1858–1863页。'
- en: '[12] H. Ito, H. Ichiwara, K. Yamamoto, H. Mori, and T. Ogata, “Integrated learning
    of robot motion and sentences: Real-time prediction of grasping motion and attention
    based on language instructions,” in *Proc. of the IEEE Int. Conf. on Robotics
    and Automation*, 2022, pp. 5404–5410.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] H. Ito, H. Ichiwara, K. Yamamoto, H. Mori, 和 T. Ogata，“机器人运动与句子的集成学习：基于语言指令的抓取动作和注意力的实时预测，”
    见 *IEEE国际机器人与自动化会议论文集*，2022，第5404–5410页。'
- en: '[13] T. Yoshida, A. Masumori, and T. Ikegami, “From text to motion: Grounding
    gpt-4 in a humanoid robot ”alter3”,” *arXiv preprint arXiv:2312.06571*, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] T. Yoshida, A. Masumori, 和 T. Ikegami，“从文本到动作：在类人机器人‘alter3’中实现GPT-4，”
    *arXiv 预印本 arXiv:2312.06571*，2023。'
- en: '[14] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman,
    H. Cao, X. Cheng, M. Chung, M. Grella *et al.*, “Rwkv: Reinventing rnns for the
    transformer era,” *arXiv preprint arXiv:2305.13048*, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman,
    H. Cao, X. Cheng, M. Chung, M. Grella *等*，“Rwkv：为变换器时代重新定义RNN，” *arXiv 预印本 arXiv:2305.13048*，2023。'
- en: '[15] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz,
    A. Irpan, E. Jang, R. Julian *et al.*, “Do as i can, not as i say: Grounding language
    in robotic affordances,” in *Proc. of the Conf. on Robot Learning*, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz,
    A. Irpan, E. Jang, R. Julian *等*，“做我所能，不做我所说：将语言与机器人能力对接，” 见 *机器人学习会议论文集*，2022。'
- en: '[16] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot
    planners: Extracting actionable knowledge for embodied agents,” in *Proc. of the
    Int. Conf. on Machine Learning*, 2022, pp. 9118–9147.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. Huang, P. Abbeel, D. Pathak, 和 I. Mordatch，“语言模型作为零样本规划者：为具身体代理提取可操作知识，”
    见 *国际机器学习会议论文集*，2022，第9118–9147页。'
- en: '[17] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, “Chatgpt for robotics:
    Design principles and model abilities,” 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Vemprala, R. Bonatti, A. Bucker, 和 A. Kapoor，“Chatgpt在机器人学中的应用：设计原则和模型能力，”
    2023。'
- en: '[18] X. Zhou, R. Girdhar, A. Joulin, P. Krähenbühl, and I. Misra, “Detecting
    twenty-thousand classes using image-level supervision,” in *Proc. of the European
    Conf. on Computer Vision*, 2022, pp. 350–368.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] X. Zhou, R. Girdhar, A. Joulin, P. Krähenbühl, 和 I. Misra，“使用图像级监督检测两万类，”
    见 *欧洲计算机视觉会议论文集*，2022，第350–368页。'
- en: '[19] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo *et al.*, “Segment anything,” 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo *等*，“Segment anything，” 2023。'
- en: '[20] T. Cheng, L. Song, Y. Ge, W. Liu, X. Wang, and Y. Shan, “Yolo-world: Real-time
    open-vocabulary object detection,” 2024.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T. Cheng, L. Song, Y. Ge, W. Liu, X. Wang, 和 Y. Shan，“Yolo-world: 实时开放词汇对象检测，”
    2024。'
- en: '[21] M. Skreta, Z. Zhou, J. L. Yuan, K. Darvish, A. Aspuru-Guzik, and A. Garg,
    “Replan: Robotic replanning with perception and language models,” *arXiv preprint
    arXiv:2401.04157*, 2024.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] M. Skreta, Z. Zhou, J. L. Yuan, K. Darvish, A. Aspuru-Guzik, 和 A. Garg，“Replan:
    具有感知和语言模型的机器人重新规划，” *arXiv 预印本 arXiv:2401.04157*，2024。'
- en: '[22] K. Hori, K. Suzuki, and T. Ogata, “Interactively robot action planning
    with uncertainty analysis and active questioning by large language model,” in
    *Proc. of the IEEE/SICE Int. Symp. on System Integrations*, 2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] K. Hori, K. Suzuki, 和 T. Ogata，“通过大型语言模型的互动机器人动作规划、分析不确定性和主动提问”，发表于*IEEE/SICE国际系统集成研讨会论文集*，2024年。'
- en: '[23] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine,
    and C. Finn, “BC-z: Zero-shot task generalization with robotic imitation learning,”
    in *Proc. of the 5th Annual Conf. on Robot Learning*, 2021.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine,
    和 C. Finn，“BC-z：通过机器人模仿学习进行零样本任务泛化”，发表于*第五届机器人学习年会论文集*，2021年。'
- en: '[24] M. Toyoda, K. Suzuki, H. Mori, Y. Hayashi, and T. Ogata, “Embodying pre-trained
    word embeddings through robot actions,” *IEEE Robotics and Automation Letters*,
    vol. 6, no. 2, pp. 4225–4232, 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] M. Toyoda, K. Suzuki, H. Mori, Y. Hayashi, 和 T. Ogata，“通过机器人动作体现预训练词嵌入”，*IEEE机器人与自动化快报*，第6卷，第2期，第4225–4232页，2021年。'
- en: '[25] M. Toyoda, K. Suzuki, Y. Hayashi, and T. Ogata, “Learning bidirectional
    translation between descriptions and actions with small paired data,” *IEEE Robotics
    and Automation Letters*, vol. 7, no. 4, pp. 10 930–10 937, 2022.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Toyoda, K. Suzuki, Y. Hayashi, 和 T. Ogata，“利用小规模配对数据学习描述与动作之间的双向翻译”，*IEEE机器人与自动化快报*，第7卷，第4期，第10,930–10,937页，2022年。'
- en: '[26] B. Jiang, X. Chen, W. Liu, J. Yu, G. Yu, and T. Chen, “Motiongpt: Human
    motion as a foreign language,” *arXiv preprint arXiv:2306.14795*, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] B. Jiang, X. Chen, W. Liu, J. Yu, G. Yu, 和 T. Chen，“Motiongpt：将人类动作作为外语”，*arXiv预印本
    arXiv:2306.14795*，2023年。'
- en: '[27] K. Friston, J. Kilner, and L. Harrison, “A free energy principle for the
    brain,” *Journal of Physiology-Paris*, vol. 100, no. 1, pp. 70–87, 2006.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] K. Friston, J. Kilner, 和 L. Harrison，“大脑的自由能原理”，*巴黎生理学杂志*，第100卷，第1期，第70–87页，2006年。'
- en: '[28] K. Suzuki, H. Ito, T. Yamada, K. Kase, and T. Ogata, “Deep predictive
    learning : Motion learning concept inspired by cognitive robotics,” *arXiv preprint
    arXiv:2306.14714*, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] K. Suzuki, H. Ito, T. Yamada, K. Kase, 和 T. Ogata，“深度预测学习：一种受认知机器人启发的运动学习概念”，*arXiv预印本
    arXiv:2306.14714*，2023年。'
- en: '[29] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel, “Deep
    spatial autoencoders for visuomotor learning,” in *Proc. of the IEEE Int. Conf.
    on Robotics and Automation*, 2016, pp. 512–519.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, 和 P. Abbeel，“用于视觉运动学习的深度空间自编码器”，发表于*IEEE国际机器人与自动化会议论文集*，2016年，第512–519页。'
- en: '[30] C. Zhang, D. Han, S. Zheng, J. Choi, T.-H. Kim, and C. S. Hong, “Mobilesamv2:
    Faster segment anything to everything,” *arXiv preprint arXiv:2312.09579*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] C. Zhang, D. Han, S. Zheng, J. Choi, T.-H. Kim, 和 C. S. Hong，“Mobilesamv2：更快的从任何到一切的分割”，*arXiv预印本
    arXiv:2312.09579*，2023年。'
- en: '[31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *Proc. of the Int. Conf. on Machine Learning*,
    2021, pp. 8748–8763.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *等*，“从自然语言监督中学习可转移的视觉模型”，发表于*国际机器学习会议论文集*，2021年，第8748–8763页。'
- en: '[32] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, “On the
    variance of the adaptive learning rate and beyond,” in *Proc. of the Eighth Int.
    Conf. on Learning Representations*, 2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, 和 J. Han，“关于自适应学习率的方差及其他”，发表于*第八届国际学习表示会议论文集*，2020年。'
- en: '[33] Y. Zhu, J. Wong, A. Mandlekar, R. Martín-Martín, A. Joshi, S. Nasiriany,
    and Y. Zhu, “robosuite: A modular simulation framework and benchmark for robot
    learning,” *arXiv preprint arXiv:2009.12293*, 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Zhu, J. Wong, A. Mandlekar, R. Martín-Martín, A. Joshi, S. Nasiriany,
    和 Y. Zhu，“robosuite：一个模块化的机器人学习模拟框架和基准”，*arXiv预印本 arXiv:2009.12293*，2020年。'
- en: '[34] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-based
    control,” in *Proc. of the IEEE/RSJ Int. Conf. on Intelligent Robots and Systems*,
    2012, pp. 5026–5033.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] E. Todorov, T. Erez, 和 Y. Tassa，“Mujoco：一种用于基于模型控制的物理引擎”，发表于*IEEE/RSJ国际智能机器人与系统会议论文集*，2012年，第5026–5033页。'
- en: '[35] W. Fujii, K. Suzuki, T. Ando, A. Tateishi, H. Mori, and T. Ogata, “Buttoning
    task with a dual-arm robot: An exploratory study on a marker-based algorithmic
    method and marker-less machine learning methods,” in *Proc. of the 2022 IEEE/SICE
    Int. Symp. on System Integration*, 2022, pp. 682–689.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] W. Fujii, K. Suzuki, T. Ando, A. Tateishi, H. Mori, 和 T. Ogata，“双臂机器人按钮任务：基于标记的算法方法与无标记机器学习方法的探索研究”，发表于*2022
    IEEE/SICE国际系统集成研讨会论文集*，2022年，第682–689页。'
- en: '[36] N. Saito, M. Hiramoto, A. Kubo, K. Suzuki, H. Ito, S. Sugano, and T. Ogata,
    “Realtime motion generation with active perception using attention mechanism for
    cooking robot,” *arXiv preprint arXiv:2309.14837*, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] N. Saito, M. Hiramoto, A. Kubo, K. Suzuki, H. Ito, S. Sugano, 和 T. Ogata,
    “使用注意力机制进行实时运动生成的主动感知应用于烹饪机器人，” *arXiv 预印本 arXiv:2309.14837*，2023。'
