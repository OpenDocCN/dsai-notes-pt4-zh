- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
- en: Huiqiang Jiang¹¹1Equal contribution. ^◆Work during internship at Microsoft.,
    Yucheng Li^◆¹¹1Equal contribution. ^◆Work during internship at Microsoft., Chengruidong
    Zhang¹¹1Equal contribution. ^◆Work during internship at Microsoft., Qianhui Wu,
    Xufang Luo,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 姜辉强¹¹1等贡献。 ^◆微软实习期间工作。李宇程^◆¹¹1等贡献。 ^◆微软实习期间工作。张成瑞栋¹¹1等贡献。 ^◆微软实习期间工作。吴千辉，罗旭芳，
- en: Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang,
    Lili Qiu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 安素仁，韩振华，阿米尔·H·阿卜迪，李东生，林镇耀，杨钰清，邱莉莉
- en: Microsoft Corporation, ^◆University of Surrey {hjiang,chengzhang,yuqyang}@microsoft.com,yucheng.li@surrey.ac.uk
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软公司，^◆萨里大学 {hjiang,chengzhang,yuqyang}@microsoft.com，yucheng.li@surrey.ac.uk
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The computational challenges of Large Language Model (LLM) inference remain
    a significant barrier to their widespread deployment, especially as prompt lengths
    continue to increase. Due to the quadratic complexity of the attention computation,
    it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the
    pre-filling stage) on a single A100 GPU. Existing methods for speeding up pre-filling
    often fail to maintain acceptable accuracy or efficiency when applied to long-context
    LLMs. To address this gap, we introduce MInference (Million-tokens Inference),
    a sparse calculation method designed to accelerate pre-filling of long-sequence
    processing. Specifically, we identify three unique patterns in long-context attention
    matrices—the A-shape, Vertical-Slash, and Block-Sparse—that can be leveraged for
    efficient sparse computation on GPUs. We determine the optimal pattern for each
    attention head offline and dynamically build sparse indices based on the assigned
    pattern during inference. With the pattern and sparse indices, we perform efficient
    sparse attention calculations via our optimized GPU kernels to significantly reduce
    the latency in the pre-filling stage of long-context LLMs. Our proposed technique
    can be directly applied to existing LLMs without any modifications to the pre-training
    setup or additional fine-tuning. By evaluating on a wide range of downstream tasks,
    including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including
    LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that
    MInference effectively reduces inference latency by up to $10\times$ for pre-filling
    on an A100, while maintaining accuracy. Our code is available at [https://aka.ms/MInference](https://aka.ms/MInference).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）推理的计算挑战仍然是其广泛部署的重大障碍，特别是当提示长度不断增加时。由于注意力计算的二次复杂性，8B LLM在单个A100 GPU上处理1M
    tokens的提示（即预填充阶段）需要30分钟。现有的加速预填充的方法通常在应用于长上下文LLM时，无法保持令人满意的准确性或效率。为了解决这一问题，我们介绍了MInference（百万token推理），一种稀疏计算方法，旨在加速长序列处理的预填充。具体而言，我们在长上下文注意力矩阵中识别出三种独特的模式——A形、垂直斜线和块稀疏——可以用于在GPU上进行高效的稀疏计算。我们离线确定每个注意力头的最佳模式，并在推理过程中根据分配的模式动态构建稀疏索引。利用模式和稀疏索引，我们通过优化的GPU内核执行高效的稀疏注意力计算，从而显著降低长上下文LLM预填充阶段的延迟。我们提出的技术可以直接应用于现有LLM，无需对预训练设置进行任何修改或额外的微调。通过在包括InfiniteBench、RULER、PG-19和Needle
    In A Haystack在内的广泛下游任务以及LLaMA-3-1M、GLM-4-1M、Yi-200K、Phi-3-128K和Qwen2-128K等模型上进行评估，我们证明MInference在A100上有效地将推理延迟降低了高达$10\times$，同时保持了准确性。我们的代码可以在[https://aka.ms/MInference](https://aka.ms/MInference)获得。
- en: \doparttoc\faketableofcontents![Refer to caption](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontents![参见说明](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
- en: (a) Needle In A Haystack
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 针对大海捞针
- en: '![Refer to caption](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
- en: (b) Latency Speedup
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 延迟加速
- en: 'Figure 1: Attention weights, especially in long-context LLMs, exhibit up to
    96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic
    sparse attention to accelerate the pre-filling stage of long-context LLM inference.
    It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b),
    and matches or surpasses baselines, as demonstrated by Needle In A Haystack [[35](#bib.bib35)]
    in (a) on LLaMA-3-8B-1M [[24](#bib.bib24)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：注意力权重，尤其是在长上下文 LLMs 中，在 128K 上下文中表现出高达 96.8% 的稀疏性。我们提出了 MInference，利用动态稀疏注意力加速长上下文
    LLM 推理的预填充阶段。它在单个 A100 上对 1M 上下文实现了最高 10 倍的加速，如图(b)所示，并且在 LLaMA-3-8B-1M [[24](#bib.bib24)]
    上的 Needle In A Haystack [[35](#bib.bib35)] 实验中，结果匹配或超越了基线。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have entered the era of long-context processing,
    with some of them supporting context windows ranging from 128K to 10M tokens [[24](#bib.bib24),
    [67](#bib.bib67), [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)].
    These extended context windows enable LLMs to unlock a multitude of complex real-world
    applications, such as repository-level code understanding [[7](#bib.bib7), [34](#bib.bib34),
    [58](#bib.bib58)], long-document question-answering [[9](#bib.bib9), [51](#bib.bib51)],
    extreme-label in-context learning [[51](#bib.bib51)], and long-horizon agent tasks [[79](#bib.bib79)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经进入了长上下文处理的时代，其中一些支持从 128K 到 10M tokens 的上下文窗口 [[24](#bib.bib24),
    [67](#bib.bib67), [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)]。这些扩展的上下文窗口使
    LLMs 能够解锁大量复杂的现实世界应用，如仓库级代码理解 [[7](#bib.bib7), [34](#bib.bib34), [58](#bib.bib58)]，长文档问答
    [[9](#bib.bib9), [51](#bib.bib51)]，极端标签上下文学习 [[51](#bib.bib51)] 和长时地平线代理任务 [[79](#bib.bib79)]。
- en: 'However, due to the quadratic complexity of attention, it can take several
    minutes for the model to process the input prompt (i.e., the pre-filling stage)
    and then start to produce the first token, which leads to unacceptable Time To
    First Token experience, thus greatly hinders the wide application of long-context
    LLMs. As shown in Fig. [2a](#S2.F2.sf1 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), when serving LLaMA-3-8B on a single A100
    machine, the model would keep users waiting for 6 minutes to finish the pre-filling
    stage given a prompt of 300K tokens, and this number increases to 30 minutes for
    a prompt of 1M tokens. The overhead of self-attention computation exceeds 90%
    of the total pre-filling latency, which makes it the major bottleneck in long-context
    processing of LLMs. Previous research has shown that the attention matrices are
    highly sparse [[47](#bib.bib47), [17](#bib.bib17)], which has led to the development
    of fixed sparse attention methods such as Longformer [[6](#bib.bib6)] and BigBird [[87](#bib.bib87)].
    However, prior studies have also noted that attention distributions vary significantly
    across different inputs [[39](#bib.bib39), [47](#bib.bib47)]. This dynamic nature
    prevents prior sparse methods from being used directly on long-context LLMs without
    expensive training or fine-tuning. But if the dynamic sparse attention patterns
    could be efficiently predicted online, the pre-filling latency of long-context
    LLMs could be significantly reduced by calculating only the most important part
    of the attention weights.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于注意力的二次复杂性，模型处理输入提示（即预填充阶段）并开始生成第一个 token 可能需要几分钟，这导致了不可接受的首次 token 时间体验，从而极大地阻碍了长上下文
    LLMs 的广泛应用。如图 [2a](#S2.F2.sf1 "在图 2 ‣ 2 注意力头：动态、稀疏和特性 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文
    LLMs 的预填充") 所示，在单个 A100 机器上服务 LLaMA-3-8B 时，给定 300K tokens 的提示，模型会让用户等待 6 分钟才能完成预填充阶段，而对于
    1M tokens 的提示，这一时间增加到 30 分钟。自注意力计算的开销超过了总预填充延迟的 90%，这使其成为 LLMs 长上下文处理的主要瓶颈。以前的研究表明，注意力矩阵高度稀疏
    [[47](#bib.bib47), [17](#bib.bib17)]，这导致了如 Longformer [[6](#bib.bib6)] 和 BigBird
    [[87](#bib.bib87)] 等固定稀疏注意力方法的发展。然而，先前的研究还注意到，不同输入的注意力分布差异显著 [[39](#bib.bib39),
    [47](#bib.bib47)]。这种动态特性使得以前的稀疏方法无法直接用于长上下文 LLMs，除非进行昂贵的训练或微调。但是，如果可以高效地在线预测动态稀疏注意力模式，则通过仅计算最重要的注意力权重部分，可以显著减少长上下文
    LLMs 的预填充延迟。
- en: 'Building upon this idea, we present MInference, a technique that reduces 95%
    of FLOPs in the attention computation to significantly accelerate the pre-filling
    stage of long-context LLM inference via dynamic sparse attention. Unlike existing
    dynamic sparse attention methods that introduce large computational overhead to
    estimate attention patterns with low-rank hidden dimensions [[47](#bib.bib47),
    [63](#bib.bib63)], our method is designed specifically for long-context scenarios
    with minimal overhead in estimation. Specifically, we conduct extensive analysis
    and identify three general patterns of sparse attention in long-context LLMs:
    A-shape pattern, Vertical-Slash pattern, and Block-Sparse pattern. Based on these
    findings, we introduce a kernel-aware search method to assign the optimal attention
    pattern for each head. Importantly, instead of fixed attention masks in prior
    studies, we perform an efficient online approximation to build a dynamic sparse
    mask for each head according to their assigned pattern and particular inputs.
    For example, to build a dynamic sparse mask for a specific prompt on one Vertical-Slash
    head, we use a partial of attention weight consisting of the last last_q query
    and key vectors (i.e. $\bm{Q}_{[-\text{last\_q}:]}$) to estimate the most important
    indices of the vertical and slash lines globally on the attention matrix. For
    Block-Sparse heads, we perform mean pooling on both query and key vectors in blocks
    of 64 and calculate the block-level attention weights to determine the most important
    blocks and thereby obtain a block-sparse dynamic mask. After obtaining the dynamic
    sparse mask, three optimized GPU kernels are used, which we developed for the
    above three sparse patterns. These kernels are based on the dynamic sparse compilers
    PIT [[88](#bib.bib88)], Triton [[75](#bib.bib75)] and FlashAttention [[13](#bib.bib13)],
    which enable extremely efficient computation of dynamic sparse attention.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们提出了 MInference，这是一种通过动态稀疏注意力显著加速长上下文 LLM 推理的预填充阶段的技术，减少了注意力计算中 95% 的
    FLOPs。与现有的动态稀疏注意力方法不同，这些方法通过低秩隐藏维度估计注意力模式，带来大量计算开销 [[47](#bib.bib47), [63](#bib.bib63)]，我们的方法专门为长上下文场景设计，估计开销最小。具体而言，我们进行了广泛的分析，并识别出长上下文
    LLM 中稀疏注意力的三种通用模式：A 形模式、垂直斜杠模式和块稀疏模式。基于这些发现，我们引入了一种内核感知搜索方法，为每个注意力头分配最佳注意力模式。重要的是，与先前研究中的固定注意力掩码不同，我们进行高效的在线近似，为每个头部根据其分配的模式和特定输入构建动态稀疏掩码。例如，为了为一个垂直斜杠头部的特定提示构建动态稀疏掩码，我们使用部分注意力权重，由最后的
    `last_q` 查询和键向量（即 $\bm{Q}_{[-\text{last\_q}:]}$）组成，以估计在注意力矩阵上垂直和斜线的最重要的索引。对于块稀疏头部，我们对查询和键向量进行
    64 的块均值池化，并计算块级别的注意力权重，以确定最重要的块，从而获得块稀疏动态掩码。获得动态稀疏掩码后，我们使用三种优化的 GPU 内核，这些内核是我们为上述三种稀疏模式开发的。这些内核基于动态稀疏编译器
    PIT [[88](#bib.bib88)]，Triton [[75](#bib.bib75)] 和 FlashAttention [[13](#bib.bib13)]，使动态稀疏注意力的计算极为高效。
- en: Extensive experiments are conducted on various Long-context LLMs, including
    LLaMA-3-8B-1M [[24](#bib.bib24)], GLM-4-9B-1M [[26](#bib.bib26)], and Yi-9B-200K [[84](#bib.bib84)],
    across benchmarks with context lengths over 1M tokens, such as InfiniteBench [[86](#bib.bib86)],
    RULER [[28](#bib.bib28)], Needle In A Haystack [[35](#bib.bib35)], and PG-19 [[65](#bib.bib65)].
    Needle In A Haystack was also tested on Phi-3-Mini-128K [[2](#bib.bib2)] and Qwen-2-7B-128K [[5](#bib.bib5)].
    Results show that MInference speeds up the pre-filling stage by up to $10\times$
    for 1M contexts with LLaMA-3-8B on a single A100, reducing latency from 30 minutes
    to 3 minutes per prompt, while maintaining or improving accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种长上下文 LLM 上进行广泛实验，包括 LLaMA-3-8B-1M [[24](#bib.bib24)]，GLM-4-9B-1M [[26](#bib.bib26)]
    和 Yi-9B-200K [[84](#bib.bib84)]，涵盖了上下文长度超过 1M tokens 的基准测试，例如 InfiniteBench [[86](#bib.bib86)]，RULER [[28](#bib.bib28)]，Needle
    In A Haystack [[35](#bib.bib35)] 和 PG-19 [[65](#bib.bib65)]。Needle In A Haystack
    还在 Phi-3-Mini-128K [[2](#bib.bib2)] 和 Qwen-2-7B-128K [[5](#bib.bib5)] 上进行了测试。结果表明，MInference
    将 1M 上下文的预填充阶段加速高达 $10\times$，在单个 A100 上使用 LLaMA-3-8B，将延迟从每个提示的 30 分钟减少到 3 分钟，同时保持或提高了准确性。
- en: '2 Attention Heads: Dynamic, Sparse, and Characteristic'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 注意力头：动态、稀疏和特征
- en: '![Refer to caption](img/10ccd929778db2d7ea98214ca2a9092c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10ccd929778db2d7ea98214ca2a9092c.png)'
- en: (a) Attention incurs heavy cost.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力会产生很大的成本。
- en: '![Refer to caption](img/84a510695ce9529c4982da1844ae2d16.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/84a510695ce9529c4982da1844ae2d16.png)'
- en: (b) Attention is sparse.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力是稀疏的。
- en: '![Refer to caption](img/804139198ed227c4ddf295cd89c1f3cd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/804139198ed227c4ddf295cd89c1f3cd.png)'
- en: (c) Sparsity of attention is dynamic.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 注意力的稀疏性是动态的。
- en: 'Figure 2: (a) Latency breakdown of the pre-filling stage. (b) How much attention
    scores can top-k (k=4096) columns cover in a 128k context. (c) Less attention
    scores are retrieved when reusing the top-k indices from another examples, indicating
    its dynamic nature. Visualizations are based on LLaMa-3-8B with a single A100.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：(a) 预填充阶段的延迟细分。(b) 在128k上下文中，前k（k=4096）列能够覆盖多少注意力分数。(c) 当重用来自其他示例的前k索引时，检索到的注意力分数较少，表明其动态特性。可视化基于单个A100的LLaMa-3-8B。
- en: 2.1 Attention is Dynamically Sparse
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 注意力是动态稀疏的
- en: 'The sparsity of attention weights in pre-trained LLMs, especially in long-context
    scenarios, has been well-documented [[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]. As shown in Fig. [2b](#S2.F2.sf2 "In Figure 2 ‣ 2 Attention
    Heads: Dynamic, Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), for an attention matrix
    of size $128k\times 128k$, retaining only the top 4k columns recalls 96.8% of
    the total attention. In other words, each token is attending to a limit number
    of tokens despite the long sequence it is processing.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练LLM中的注意力权重的稀疏性，尤其是在长上下文场景中，已被充分记录[[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]。如图[2b](#S2.F2.sf2 "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")所示，对于大小为$128k\times
    128k$的注意力矩阵，仅保留前4k列可以回忆96.8%的总注意力。换句话说，每个令牌尽管处理的是长序列，但关注的令牌数量是有限的。
- en: 'On the other hand, although the sparse nature of attention matrices is shared
    across different inputs, the exact distributions of sparse pattern are highly
    dynamic. That is to say, a token at a given position only attends to a subset
    of the sequence in self-attention, and the exact tokens it attends to are highly
    context-dependent and vary significantly across different prompts. This dynamism
    has been mathematically demonstrated in prior studies [[39](#bib.bib39), [40](#bib.bib40)].
    As depicted in Fig. [2c](#S2.F2.sf3 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), if we take the top 4k columns found in Fig. [2b](#S2.F2.sf2
    "In Figure 2 ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and apply it on another prompt of 128k, the recall of attention would drop largely
    to 83.7%.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，尽管注意力矩阵的稀疏特性在不同输入间是共享的，但稀疏模式的具体分布是高度动态的。也就是说，给定位置的令牌在自注意力中仅关注序列的一部分，它所关注的具体令牌高度依赖于上下文，并且在不同的提示中变化显著。这种动态性在先前的研究中已被数学证明[[39](#bib.bib39),
    [40](#bib.bib40)]。如图[2c](#S2.F2.sf3 "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")所示，如果我们在图[2b](#S2.F2.sf2
    "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")中找到前4k列，并将其应用于另一个128k的提示，注意力的回忆率将大幅下降至83.7%。
- en: 2.2 Attention Sparsity Exhibits Patterns
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 注意力稀疏性表现出模式
- en: 'Table 1: Comparison of different sparse patterns.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同稀疏模式的比较。
- en: '| Patterns | A-shape | Vertical-Slash | Block-Sparse | Top-K |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | A形 | 垂直斜杠 | 块稀疏 | 前K个 |'
- en: '| Spatial Distribution | Static structured | Dynamic structured | Dynamic structured
    | Dynamic fine-grained |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 空间分布 | 静态结构 | 动态结构 | 动态结构 | 动态细粒度 |'
- en: '| Latency on GPU | Low | Medium | Low | High |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| GPU上的延迟 | 低 | 中等 | 低 | 高 |'
- en: '| Time to build the index | Zero | Small | Small | High | ![Refer to caption](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '| 构建索引的时间 | 零 | 小 | 小 | 高 | ![参见说明](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
- en: (a) Attention patterns
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力模式
- en: '![Refer to caption](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
- en: (b) Attention is spatial clustering
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力是空间聚类
- en: '![Refer to caption](img/155b7edb1253cb7cf43bccc0db737600.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/155b7edb1253cb7cf43bccc0db737600.png)'
- en: (c) Attention pattern recall
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 注意力模式回忆
- en: 'Figure 3: (a) Visualization of attention weights from different attention heads.
    For different prompts and tasks, the pattern of the same head is relatively consistent,
    but the sparse indices are dynamically changing.(b) Distance of the top-10 nearest
    non-zero element in the attention matrix. (c) Attention recall distribution using
    our identified patterns, where FLOPs in the kernel refer to the real FLOPs required
    for sparse attention computing using on GPUs. Here, a 1x64 block size is used
    for the Vertical-Slash pattern, and a 64x64 block size is used for others on GPUs.
    All visualization are based on LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：（a）来自不同注意力头的注意力权重可视化。对于不同的提示和任务，相同头部的模式相对一致，但稀疏索引动态变化。（b）注意力矩阵中前10个最近非零元素的距离。（c）使用我们识别的模式的注意力回忆分布，其中内核中的FLOPs指的是使用GPU进行稀疏注意力计算所需的实际FLOPs。这里，对于Vertical-Slash模式使用1x64的块大小，对于其他模式在GPU上使用64x64的块大小。所有可视化均基于LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)]。
- en: 'Although the sparsity distribution of attention matrix is dynamic, previous
    works [[82](#bib.bib82), [29](#bib.bib29)] have shown that they exhibit certain
    patterns in the two-dimensional space such as spatial clustering. Through our
    analysis of long-context prompts of various lengths and tasks, we have categorized
    such attention sparse patterns into the A-shape, Vertical-Slash (VS), and Block-Sparse
    patterns, as shown in Fig. [3a](#S2.F3.sf1 "In Figure 3 ‣ 2.2 Attention Sparsity
    Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and Fig. [4](#S3.F4 "Figure 4 ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"). Table [1](#S2.T1
    "Table 1 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") details the characteristics and differences
    between these three patterns.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力矩阵的稀疏分布是动态的，之前的研究[[82](#bib.bib82), [29](#bib.bib29)]表明，它们在二维空间中表现出某些模式，如空间聚类。通过对各种长度和任务的长上下文提示进行分析，我们将这种注意力稀疏模式归类为A形、Vertical-Slash
    (VS) 和Block-Sparse模式，如图 [3a](#S2.F3.sf1 "在图3中 ‣ 2.2 注意力稀疏性表现出模式 ‣ 2 注意力头：动态、稀疏和特征
    ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充") 和图 [4](#S3.F4 "图4 ‣ 3 MInference 1.0
    ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")。表 [1](#S2.T1 "表1 ‣ 2.2 注意力稀疏性表现出模式
    ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充") 详细说明了这三种模式的特征和差异。
- en: A-shape pattern The attention weights of these types of heads are concentrated
    on initial tokens and local windows [[82](#bib.bib82), [29](#bib.bib29)], exhibiting
    relatively higher stability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: A形模式 这些类型的头部的注意力权重集中在初始标记和局部窗口[[82](#bib.bib82), [29](#bib.bib29)]，表现出相对较高的稳定性。
- en: Vertical-Slash (VS) pattern The attention weights are concentrated on specific
    tokens (vertical lines) and tokens at fixed intervals (slash lines). The positions
    of vertical and slash lines in this pattern dynamically change with the context
    content and exhibit a certain sparsity, making them difficult to be encompassed
    by local windows and A-shape patterns.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Vertical-Slash (VS)模式 注意力权重集中在特定标记（垂直线）和固定间隔的标记（斜线）。这种模式中的垂直线和斜线的位置随着上下文内容动态变化，并表现出一定的稀疏性，使其难以被局部窗口和A形模式所涵盖。
- en: 'Block-Sparse pattern This sparsity pattern is the most dynamic, exhibiting
    a more dispersed distribution. Despite its dynamism, the attention weights maintain
    some characteristics of spatial clustering, which we identify as the block-sparse
    pattern. We analyzed the distances between non-zero attention weights and their
    top-k nearest non-zero neighbors within a 128k prompt as shown in Fig. [3b](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"). The results indicate that across layers and
    heads, the distances between nearest non-zero values are generally concentrated
    around 5, suggesting a strong spatial clustering of the attention weights.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Block-Sparse** 模式 这种稀疏模式最具动态性，表现出更分散的分布。尽管其动态性较强，注意力权重仍保持一些空间聚类的特征，我们将其识别为**块稀疏模式**。我们分析了在128k提示内非零注意力权重及其前k个最近非零邻居之间的距离，如图[3b](#S2.F3.sf2
    "在图3 ‣ 2.2 注意力稀疏性展示模式 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")所示。结果表明，在不同的层和头之间，最近非零值之间的距离通常集中在5左右，表明注意力权重有较强的空间聚类。'
- en: 'The point of these three patterns is that we can leverage them to perform highly
    efficient sparse computing for the attention matrix in long-context LLMs. In Fig. [3c](#S2.F3.sf3
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), we test how efficient is our indentified
    patterns retrieving attention scores with limit computing cost on GPU (FLOPs).
    First, attention heads are labeled with one of the sparse pattern (detail see
    §[3.2](#S3.SS2 "3.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention
    ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")). Then we demonstrate our patterns are significantly
    more efficient compared to other sparse methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)]. Specifically, with the same amount of FLOPs, our patterns achieve
    a notable higher recall on attention scores, which can potentially lead to better
    accuracy. For example, previous Top-K methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)] struggle with the Block-Sparse pattern as they focus on specific
    tokens globally, while our pattern retrieves attention scores more efficiently
    and accurately. We example how we use these patterns on long-context LLMs and
    how we implement optimized GPU kernels for these patterns in §[3](#S3 "3 MInference
    1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种模式的重点是我们可以利用它们对长上下文LLMs中的注意力矩阵执行高效的稀疏计算。在图[3c](#S2.F3.sf3 "在图3 ‣ 2.2 注意力稀疏性展示模式
    ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")中，我们测试了我们识别的模式在有限的计算成本下（FLOPs）检索注意力分数的效率。首先，注意力头被标记为某种稀疏模式（详细见§[3.2](#S3.SS2
    "3.2 通过动态稀疏注意力加速长上下文LLM推理 ‣ 3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")）。然后我们演示了我们的模式相较于其他稀疏方法[[63](#bib.bib63),
    [82](#bib.bib82), [59](#bib.bib59)]显著更高效。具体而言，在相同的FLOPs量下，我们的模式在注意力分数上获得了显著更高的召回率，这可能会导致更好的准确性。例如，以前的Top-K方法[[63](#bib.bib63),
    [82](#bib.bib82), [59](#bib.bib59)]在处理**块稀疏模式**时存在困难，因为它们全球性地关注特定令牌，而我们的模式能够更高效、更准确地检索注意力分数。我们在§[3](#S3
    "3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")中示例了我们如何在长上下文LLMs中使用这些模式，以及如何为这些模式实现优化的GPU内核。
- en: 3 MInference 1.0
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 MInference 1.0
- en: 'Following the analysis in §[2](#S2 "2 Attention Heads: Dynamic, Sparse, and
    Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs
    via Dynamic Sparse Attention"), we propose MInference to accelerate the pre-filling
    stage of long-context LLMs, consisting of three steps: 1) Offline attention pattern
    identification for each head; 2) Dynamic build of sparse indices w.r.t. the pattern;
    3) Sparse attention calculation with optimized GPU kernels.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据§[2](#S2 "2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")中的分析，我们提出了MInference以加速长上下文LLMs的预填充阶段，该过程包括三个步骤：1）离线识别每个头的注意力模式；2）根据模式动态构建稀疏索引；3）使用优化的GPU内核计算稀疏注意力。
- en: '![Refer to caption](img/717aebceb4eff85c06852353dc924b36.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/717aebceb4eff85c06852353dc924b36.png)'
- en: 'Figure 4: The three sparse methods in MInference.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MInference中的三种稀疏方法。
- en: 3.1 Problem Formulation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: 'When accelerating the pre-filling stage of long-context LLMs with sparse attention
    computing, the attention matrix can be formulated as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当加速长上下文LLMs的稀疏注意力计算的预填充阶段时，注意力矩阵可以表示如下：
- en: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
- en: where $M_{i,j}\in\{0,1\}$ of the attention matrix. Here, $c$ have values approaching
    zero after the softmax, i.e., $A_{i,j}\approx 0$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，注意力矩阵中的 $M_{i,j}\in\{0,1\}$。这里，$c$ 在 softmax 之后的值接近零，即 $A_{i,j}\approx 0$。
- en: 'The goal of the dynamic sparse attention system is to achieve greater speedup
    with minimal overhead while retaining as much of the attention weights as possible.
    Formally, this can be expressed as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 动态稀疏注意力系统的目标是在保留尽可能多的注意力权重的同时，实现更大的加速并保持最小的开销。正式地，这可以表示为：
- en: '|  | $\displaystyle\min$ |  | (2) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ |  | (2) |'
- en: '|  | $\displaystyle\min$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ |  |'
- en: where $t_{\text{sparse}}$ represent the time spent on dynamic sparse attention
    computation and estimation of the approximate dynamic sparse pattern, respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{\text{sparse}}$ 分别表示动态稀疏注意力计算和近似动态稀疏模式估计所花费的时间。
- en: Algorithm 1 Kernel-Aware Sparse Pattern Search
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 核心感知稀疏模式搜索
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$, search space $\rho$,
    initialized search space $\sigma$ to $|\sigma|$     while $|t_{i}-t|>                90%). However,
    according to the ablation study, using only the Vertical-Slash pattern significantly
    impacts performance in highly dynamic tasks like KV retrieval. Secondly, the Block-Sparse
    pattern is primarily distributed in several intermediate to later layers, while
    the A-shape pattern is found in the middle layers. Although the optimal patterns
    vary slightly across different models, they generally align with these observations.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#A5.F11 "Figure 11 ‣ Appendix E Pattern Distribution ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")展示了通过我们的搜索获得的最优头配置的分布。首先，大多数模式是纵向斜杠模式（>90%）。然而，根据消融研究，单独使用纵向斜杠模式会显著影响在KV检索等高度动态任务中的性能。其次，块稀疏模式主要分布在几个中间到后期层，而A形模式则出现在中间层。尽管不同模型中的最优模式略有不同，但通常与这些观察结果一致。'
- en: Additionally, we used the same configuration for two versions of LLaMA in our
    experiments, and the results show that the 1M model also performs very well, with
    nearly perfect results in the Needle In A Haystack task. This demonstrates the
    generalizability of the optimal sparse pattern.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在实验中对两个版本的LLaMA使用了相同的配置，结果显示1M模型也表现非常好，在针筒中的针任务中取得了近乎完美的结果。这证明了最优稀疏模式的通用性。
- en: Appendix F Sparsity in Kernel Distribution
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 核心分布中的稀疏性
- en: '![Refer to caption](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
- en: 'Figure 12: The distribution of sparsity in the kernel across different context
    windows refers to the proportion of the kernel that is actually computed after
    block coverage, compared to the sparsity rate when using FlashAttention with a
    causal mask.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：核心中不同上下文窗口的稀疏性分布指的是在块覆盖后实际计算的核心部分的比例，相对于使用带有因果掩码的FlashAttention时的稀疏率。
- en: 'As shown in Fig. [12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), the sparsity distribution of the three patterns during the actual
    kernel computation process is displayed. It can be seen that when the context
    windows exceed 200k, the actual sparsity of all three patterns surpasses 90%.
    Even considering a 20% index-building overhead, this ensures that the kernel achieves
    a speedup of over 8$\times$.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")所示，实际核心计算过程中三种模式的稀疏分布情况被展示出来。可以看出，当上下文窗口超过200k时，所有三种模式的实际稀疏度均超过90%。即使考虑到20%的索引构建开销，这也确保了核心实现了超过8$\times$的加速。'
- en: Appendix G Does This Dynamic Sparse Attention Pattern Exist Only in Auto-Regressive
    LLMs or RoPE-Based LLMs?
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 这种动态稀疏注意力模式仅存在于自回归LLM或基于RoPE的LLM中吗？
- en: '![Refer to caption](img/6b22e182a2b67446c71bc01d753a7101.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6b22e182a2b67446c71bc01d753a7101.png)'
- en: 'Figure 13: The sparse pattern in T5-style Encoder Attention using Flan-UL2 [[74](#bib.bib74)]
    on the Summarization dataset [[86](#bib.bib86)].'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：使用Flan-UL2 [[74](#bib.bib74)]在Summarization数据集[[86](#bib.bib86)]上进行T5风格的编码器注意力中的稀疏模式。
- en: 'Similar vertical and slash line sparse patterns have been discovered in BERT [[72](#bib.bib72)]
    and multi-modal LLMs [[80](#bib.bib80)]. Additionally, as shown in Fig. [13](#A7.F13
    "Figure 13 ‣ Appendix G Does This Dynamic Sparse Attention Pattern Exist Only
    in Auto-Regressive LLMs or RoPE-Based LLMs? ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), we analyzed the distribution
    of attention patterns in T5 across different heads. It is evident that there are
    vertical and slash sparse patterns even in bidirectional attention.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BERT [[72](#bib.bib72)] 和多模态 LLMs [[80](#bib.bib80)] 中已经发现了类似的垂直和斜线稀疏模式。此外，如图
    [13](#A7.F13 "图 13 ‣ 附录 G 这种动态稀疏注意力模式是否只存在于自回归 LLMs 或基于 RoPE 的 LLMs？ ‣ MInference
    1.0：通过动态稀疏注意力加速长上下文 LLMs 的预填充") 所示，我们分析了 T5 在不同头部的注意力模式分布。显然，即使在双向注意力中，也存在垂直和斜线稀疏模式。
- en: Recent studies [[80](#bib.bib80)] have analyzed sparse attention patterns in
    multi-modal LLMs, revealing the presence of vertical and slash patterns in models
    like LLaVA [[45](#bib.bib45)] and InternVL [[11](#bib.bib11)]. Using MInference
    for pre-filling stage inference acceleration holds great promise.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究 [[80](#bib.bib80)] 分析了多模态 LLMs 中的稀疏注意力模式，揭示了像 LLaVA [[45](#bib.bib45)]
    和 InternVL [[11](#bib.bib11)] 这样的模型中存在垂直和斜线模式。使用 MInference 进行预填充阶段的推理加速具有很大的前景。
- en: Appendix H Case Study
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 案例研究
- en: 'Table [8](#A8.T8 "Table 8 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") presents a comparison
    of the generation performance for various methods on the EN.SUM task (200K input
    length) from InfiniteBench based on the LLaMA-3-8B-262K model. The original summary
    provides a comprehensive and coherent narrative, detailing the Bronwyn family’s
    trip to the Kindergarten and touching on themes such as nostalgia, loss, and the
    passage of time. StreamingLLM’s summary, although looks coherent, introduces elements
    that are not present in the original story, leading to serious factual errors.
    For example, it mentions a boat trip to a school for boys and specific details
    like fishermen, sandwiches, and a spot where men were drowned. These details deviate
    from the original story, which is about the Bronwyn family preparing for a trip
    to the Kindergarten. In addition, the summaries generated by StreamingLLM with
    dilated and strided techniques are largely incoherent, consisting primarily of
    repetitive and nonsensical characters, indicating a failure to produce meaningful
    content. In stark contrast, the summary generated by our proposed method offers
    a detailed and coherent narrative, comparable to the original, with a clear depiction
    of the story’s main events and themes. This includes the preparation of the Bronwyn
    family for their trip, the characterization of family members and guests, and
    the exploration of deeper themes such as love, marriage, and the search for meaning.
    The results demonstrate the superiority of our proposed method in generating high-quality,
    human-like summaries over the baseline methods.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [8](#A8.T8 "表 8 ‣ 附录 H 案例研究 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLMs 的预填充") 展示了基于
    LLaMA-3-8B-262K 模型的 InfiniteBench 对 EN.SUM 任务（200K 输入长度）不同方法的生成性能比较。原始总结提供了一个全面且连贯的叙述，详细讲述了
    Bronwyn 家庭的幼儿园之行，并触及了怀旧、失落和时间流逝等主题。尽管 StreamingLLM 的总结看似连贯，但引入了原故事中没有的元素，导致了严重的事实错误。例如，它提到了前往男子学校的船上旅行以及像渔民、三明治和一个男人溺水的地方等具体细节。这些细节偏离了原故事，原故事是关于
    Bronwyn 家庭为幼儿园旅行做准备。此外，StreamingLLM 生成的 dilated 和 strided 技术的总结大多不连贯，主要由重复和无意义的字符组成，表明未能生成有意义的内容。相比之下，我们提出的方法生成的总结提供了详细且连贯的叙述，能够与原文相媲美，清晰地描绘了故事的主要事件和主题。这包括
    Bronwyn 家庭为旅行做准备、家庭成员和客人的描绘，以及对爱、婚姻和寻求意义等深层主题的探索。结果显示，我们提出的方法在生成高质量、类人总结方面优于基线方法。
- en: 'Table [9](#A8.T9 "Table 9 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") compares the
    performance of various methods on the Retrieve.KV task (200K input length) using
    the LLaMA-3-8B-262K model. The original method demonstrates perfect retrieval,
    correctly predicting the exact strings of the ground truth for both examples.
    StreamingLLM, again, generates predictions that looks coherent and real, but factually
    incorrect. In addition, StreamingLLM with dilated and strided techniques, and
    our method with a static pattern, fail significantly, producing outputs that are
    either repetitive sequences of characters or nonsensical strings, indicating their
    inability to accurately retrieve the required key-value pairs. Our method, however,
    performs on par with the original, accurately retrieving and predicting the exact
    key-value pairs for both examples. This demonstrates the superior capability of
    our method in handling KV retrieval tasks, providing precise and reliable outputs
    consistent with the ground truth. The results highlight our method’s effectiveness
    and robustness compared to the baselines, making it a reliable choice for such
    tasks.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '表[9](#A8.T9 "Table 9 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") 比较了使用 LLaMA-3-8B-262K
    模型在 Retrieve.KV 任务（200K 输入长度）上各种方法的性能。原始方法展示了完美的检索，正确预测了两个示例的地面真实值的确切字符串。StreamingLLM
    再次生成了看似连贯和真实的预测，但在事实层面上不正确。此外，使用扩张和步长技术的 StreamingLLM 以及我们的静态模式方法表现差劲，产生了重复的字符序列或无意义的字符串，表明它们无法准确检索所需的键值对。然而，我们的方法与原始方法表现相当，准确地检索和预测了两个示例的确切键值对。这展示了我们的方法在处理
    KV 检索任务中的卓越能力，提供了与地面真实值一致的精确可靠的输出。结果突出显示了我们方法的有效性和稳健性，相较于基线方法，使其成为此类任务的可靠选择。'
- en: Algorithm 4 Vertical-Slash Index
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 垂直-斜线索引
- en: 'Input: vertical indexes $\bm{i}_{v}\in\mathbb{N}^{k_{v}}$ # Sort vertical and
    slash indexes  $\bm{i}_{v}\leftarrow\mathrm{IncrementalSort}\left(\bm{i}_{v}\right)$  #
    Calculate block number (block_size $B$  # Initialize outputsblock count $\bm{c}_{\text{blk}}\in\mathbb{N}^{N}$,
    column count $\bm{c}_{\text{col}}\mathbb{N}^{N}$  # Parallelized in GPU  for $i\leftarrow
    1$ do     $j_{v}\leftarrow 1$  # Define the range by slash index     $r_{\text{start}}\leftarrow(i-1)\times
    B-\bm{i}_{s}^{j_{s}}$  # Merge points (vertical indexes) and ranges (slash indexes)     while $s_{v}\leq
    k_{s}$  and  $\bm{i}_{v}^{j_{v}}              while $ <math id=$ while <math
    id=$ $\bm{i}_{\text{blk}}^{i,\bm{c}_{\text{blk}}^{i}}\leftarrow s$ end while #
    计算新范围 $r_{\text{start}}\leftarrow(i-1)\times B-\bm{i}_{s}^{j_{s}}$ else # 扩展范围
    $r_{\text{end}}\leftarrow r_{\text{end}}+B$ while $s<r_{\text{end}}$ $\bm{i}_{\text{blk}}^{i,c_{blk}^{i}}\leftarrow
    s$ end while end for $\mathrm{return}\,\,\,\bm{c}_{\text{blk}},\bm{i}_{\text{blk}},\bm{c}_{\text{col}},\bm{i}_{\text{col}}$'
- en: Algorithm 5 Vertical-Slash Flash Attention
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 5 垂直-斜线闪存注意力
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$, block index $\bm{i}_{\text{blk}}\in\mathbb{N}^{N\times
    k_{s}}$, column index $\bm{i}_{\text{col}}\in\mathbb{N}^{N\times k_{v}}$  Initialize
    $\bm{O}\leftarrow(0)^{S\times d_{h}}\in\mathbb{R}^{S\times d_{h}}$ to $N$Initialize
    $\bm{O}_{\text{chip}}\leftarrow(0)^{B\times d_{h}}\in\mathbb{R}^{B\times d_{h}}$Initialize
    $\bm{l}\leftarrow(0)^{B}\in\mathbb{R}^{B}$ to $\bm{c}_{\text{blk}}^{i}$Load $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$        $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$     while $j<\bm{c}_{\text{col}}^{j}$Load
    $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{\bm{cols}}\in\mathbb{R}^{B\times d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$        $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$     end while#
    Write outputs     $\bm{O}_{\text{chip}}\leftarrow\mathrm{diag}(\bm{l}^{i})^{-1}\bm{O}_{\text{chip}}$  end for'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$，块索引 $\bm{i}_{\text{blk}}\in\mathbb{N}^{N\times
    k_{s}}$，列索引 $\bm{i}_{\text{col}}\in\mathbb{N}^{N\times k_{v}}$ 初始化 $\bm{O}\leftarrow(0)^{S\times
    d_{h}}\in\mathbb{R}^{S\times d_{h}}$ 到 $N$ 初始化 $\bm{O}_{\text{chip}}\leftarrow(0)^{B\times
    d_{h}}\in\mathbb{R}^{B\times d_{h}}$ 初始化 $\bm{l}\leftarrow(0)^{B}\in\mathbb{R}^{B}$
    到 $\bm{c}_{\text{blk}}^{i}$ 加载 $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$
           $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$
           $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$
           $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$
       while $j<\bm{c}_{\text{col}}^{j}$ 加载 $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{\bm{cols}}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$
           $\bm{m}^{i}_{new}\leftarrow\mathrm{max}(\bm{m}^{i},\mathrm{rowmax}(\bm{S}))\in\mathbb{R}^{B}$
           $\bm{P}\leftarrow\mathrm{exp}(\bm{S})$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$
           $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$
       end while# 写出结果    $\bm{O}_{\text{chip}}\leftarrow\mathrm{diag}(\bm{l}^{i})^{-1}\bm{O}_{\text{chip}}$
       end for
- en: 'Table 8: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in summarization task [[86](#bib.bib86)].'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：使用 LLaMA-3-8B-Instruct-262K 在摘要任务中不同方法生成结果的比较 [[86](#bib.bib86)]。
- en: '| Methods | Summary |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 总结 |'
- en: '| Original | The story follows the Bronwyn family and their guests as they
    prepare for a day trip to the Kindergarten, a mysterious destination that holds
    great significance for them. The story is told from multiple perspectives, including
    that of Pete Lucy, who is staying with the family and is struggling to find her
    place in their lives. As the day progresses, the family members prepare for their
    trip, and Pete reflects on her own past experiences with the Bronwyns. The story
    ends with the family setting off on their journey, leaving Pete alone on the lawn
    to contemplate her own feelings and the meaning of life. The Kindergarten is a
    symbol of the family’s past and their memories, and the story explores themes
    of nostalgia, loss, and the passage of time. Ultimately, the story suggests that
    even though the past cannot be changed, it can still be celebrated and remembered.
    |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 原文 | 故事讲述了布朗温家族及其客人准备前往幼儿园的短途旅行，这个神秘的目的地对他们意义重大。故事从多个视角讲述，包括与家人一起住的皮特·露西，她在努力寻找自己在他们生活中的位置。随着一天的进展，家人们为旅行做准备，皮特回顾了自己与布朗温家族的过去经历。故事以家人们启程的场景结束，留下皮特独自在草坪上沉思自己的感受和生命的意义。幼儿园象征着家族的过去和他们的记忆，故事探讨了怀旧、失落和时间流逝的主题。最终，故事暗示尽管过去无法改变，但仍可以庆祝和铭记。
    |'
- en: '| StreamingLLM | The story follows a family on a boat trip to the Kindergarten,
    a school for boys. The father, Mr Bronwyn, is reading a book and seems to be in
    a trance. The children, Seb and Annalisa, are excited about the trip but also
    a little scared. They pass by a group of fishermen and Mr Bronwyn shares out sandwiches.
    They come across a spot where three men were drowned and Mr Bronwyn takes out
    his watch, making a calculation. They reach the Kindergarten and Mr Bronwyn jumps
    out of the boat, holding a parcel. The story ends with Pete Lucy, who has been
    watching the family from the shore, feeling relieved that the trip is over and
    that Mr Bronwyn has reached his destination. The story is told from multiple perspectives,
    including Pete Lucy’s, who is watching the family from the shore and reflecting
    on his own life and relationships. |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 故事讲述了一个家庭在前往一个为男孩设立的幼儿园的船上旅行的经历。父亲布朗温先生正在阅读一本书，似乎进入了恍惚状态。孩子们，塞布和安娜丽莎，对这次旅行感到兴奋但也有些害怕。他们经过一群渔民，布朗温先生分发了三明治。他们来到一个三人溺水的地方，布朗温先生拿出手表进行计算。他们抵达幼儿园，布朗温先生跳下船，拿着一个包裹。故事以皮特·露西为结尾，他一直在岸边观察这个家庭，感到旅行结束了，布朗温先生也达到了他的目的地。故事从多个视角讲述，包括皮特·露西的视角，他在岸边观察这个家庭，反思自己的生活和关系。
    |'
- en: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
- en: '| StreamingLLM w/ strided | The GraphicsUnit36.OOAKA new ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | The GraphicsUnit36.OOAKA new ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( (请确认一下，您希望对这些文本的翻译是否有特定的格式要求，比如是否需要保留表格结构或特殊标记？'
- en: '| Ours w/ static | def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 w/ static | def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
- en: '| Ours | The story follows the Bronwyn family and their guests as they prepare
    for a trip to the Kindergarten, a remote island off the coast of Scotland. The
    family is made up of Mr. Bronwyn, a philosopher, and his wife, Mrs. Bronwyn, who
    is known for her beauty and her ability to surround and protect her family. The
    guests include Kevin Emma, a young man who is studying for a fellowship, and Castor
    Kiki, a botanist who is also a widower. The story explores themes of love, marriage,
    and the search for meaning in life. It also touches on the idea of the other,
    or the person who is different from oneself and cannot be understood or judged
    by conventional standards. Ultimately, the story is about the search for truth
    and the desire to connect with others on a deeper level. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 故事讲述了布朗温一家及其客人准备前往幼儿园的经历，幼儿园是苏格兰海岸附近的一个偏远岛屿。家庭成员包括布朗温先生，一位哲学家，和他的妻子布朗温女士，她以美貌和保护家庭的能力而闻名。客人包括凯文·艾玛，一位正在为奖学金学习的年轻人，以及卡斯特·基基，一位也为寡妇的植物学家。故事探讨了爱情、婚姻和对生活意义的探索等主题。它还涉及了他者的概念，即与自己不同且不能用传统标准来理解或评判的人。最终，故事讲述了对真理的追寻以及与他人建立更深层次联系的愿望。
    |'
- en: 'Table 9: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in KV Retrieval task [[86](#bib.bib86)].'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：不同方法生成结果的比较，使用 LLaMA-3-8B-Instruct-262K 在 KV 检索任务中 [[86](#bib.bib86)]。
- en: '| Methods | Summary |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 总结 |'
- en: '| Original | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '| 原始 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值："5e6b7b90-710d-4953-9b18-3e96b2cadbf2"
    2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："f1ec3dd8-b6e7-4af5-a589-fda0b7693f93"（长度为32的字符串）。 |
- en: '| StreamingLLM | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant 2\. Ground Truth: bbf3fa73-6217-4e6e-923c-8349bd286c3d'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值：2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant
    2\. 实际值：bbf3fa73-6217-4e6e-923c-8349bd286c3d'
- en: 'Prediction: "d829ce18-6339-4f77-8c04-31fc7ec33619". |'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："d829ce18-6339-4f77-8c04-31fc7ec33619"。 |
- en: '| StreamingLLM w/ dilated | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: 5 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM 使用扩张 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值：5 (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( 2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    |'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 |
- en: '| StreamingLLM w/ strided | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM 使用步幅 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值："def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |
- en: '| Ours w/ static | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "def solverome2 def solverome2 def solverome2 def solverome2 def 2\. Ground Truth:
    f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '| 我们的静态 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值："def solverome2
    def solverome2 def solverome2 def solverome2 def 2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    |'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："def solverome2 def solverome2 def solverome2 def solverome2 def |
- en: '| Ours | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '| 我们的 | 1\. 实际值：5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值："5e6b7b90-710d-4953-9b18-3e96b2cadbf2"
    2\. 实际值：f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值："f1ec3dd8-b6e7-4af5-a589-fda0b7693f93"（长度为32的字符串）。 |
