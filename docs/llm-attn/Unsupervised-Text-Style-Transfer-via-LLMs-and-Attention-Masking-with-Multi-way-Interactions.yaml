- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 19:02:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:02:48'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way
    Interactions
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督文本风格迁移通过LLMs和注意力掩蔽与多途径交互
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13647](https://ar5iv.labs.arxiv.org/html/2402.13647)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13647](https://ar5iv.labs.arxiv.org/html/2402.13647)
- en: Lei Pan¹, Yunshi Lan¹, Yang Li², Weining Qian¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 雷潘¹、云石兰¹、杨力²、魏宁乾¹
- en: ¹ East China Normal University, ² Alibaba Group
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 华东师范大学, ² 阿里巴巴集团
- en: leipan@stu.ecnu.edu.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: leipan@stu.ecnu.edu.cn
- en: ly200170@alibaba-inc.com, {yslan,wnqian}@dase.ecnu.edu.cn *Corresponding author
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ly200170@alibaba-inc.com, {yslan,wnqian}@dase.ecnu.edu.cn *通讯作者
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Unsupervised Text Style Transfer (UTST) has emerged as a critical task within
    the domain of Natural Language Processing (NLP), aiming to transfer one stylistic
    aspect of a sentence into another style without changing its semantics, syntax,
    or other attributes. This task is especially challenging given the intrinsic lack
    of parallel text pairings. Among existing methods for UTST tasks, attention masking
    approach and Large Language Models (LLMs) are deemed as two pioneering methods.
    However, they have shortcomings in generating unsmooth sentences and changing
    the original contents, respectively. In this paper, we investigate if we can combine
    these two methods effectively. We propose four ways of interactions, that are
    pipeline framework with tuned orders; knowledge distillation from LLMs to attention
    masking model; in-context learning with constructed parallel examples. We empirically
    show these multi-way interactions can improve the baselines in certain perspective
    of style strength, content preservation and text fluency. Experiments also demonstrate
    that simply conducting prompting followed by attention masking-based revision
    can consistently surpass the other systems, including supervised text style transfer
    systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best
    mean metric by $0.5$ absolute percentages respectively, and achieves new SOTA
    results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督文本风格迁移（UTST）在自然语言处理（NLP）领域中成为了一项关键任务，旨在将句子的一种风格特征转换为另一种风格，而不改变其语义、句法或其他属性。鉴于缺乏平行文本对，这一任务尤其具有挑战性。在现有的UTST方法中，注意力掩蔽方法和大型语言模型（LLMs）被认为是两种开创性的技术。然而，它们分别在生成不流畅句子和改变原始内容方面存在缺陷。本文探讨了这两种方法能否有效结合。我们提出了四种交互方式，包括调整顺序的管道框架；从LLMs到注意力掩蔽模型的知识蒸馏；使用构造的平行示例进行上下文学习。我们实证表明，这些多途径交互可以在风格强度、内容保留和文本流畅性等某些方面改善基线。实验还表明，简单地进行提示，然后通过注意力掩蔽基础的修订可以始终超越其他系统，包括监督文本风格迁移系统。在Yelp-clean和Amazon-clean数据集上，它分别提高了之前最佳均值指标$0.5$个绝对百分点，并达到了新的SOTA结果。
- en: Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way
    Interactions
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督文本风格迁移通过LLMs和注意力掩蔽与多途径交互
- en: 'Lei Pan¹, Yunshi Lan¹^†^†thanks: *Corresponding author, Yang Li², Weining Qian¹
    ¹ East China Normal University, ² Alibaba Group leipan@stu.ecnu.edu.cn ly200170@alibaba-inc.com,
    {yslan,wnqian}@dase.ecnu.edu.cn'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '雷潘¹、云石兰¹^†^†感谢: *通讯作者、杨力²、魏宁乾¹ ¹ 华东师范大学, ² 阿里巴巴集团 leipan@stu.ecnu.edu.cn ly200170@alibaba-inc.com,
    {yslan,wnqian}@dase.ecnu.edu.cn'
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Text Style Transfer (TST) is a widely investigated NLP task that aims to transfer
    one stylistic aspect of a piece of text (e.g., sentiment polarity, formality,
    politeness, etc.) into another style without changing its semantics, syntax, or
    other attributes. Although TST has attracted increased interest from scholars
    and a number of methods have been developed to solve TST problems Shang et al.
    ([2019](#bib.bib28)); Zhang et al. ([2015](#bib.bib42)); Rao and Tetreault ([2018](#bib.bib22));
    Zhang et al. ([2020](#bib.bib43)), these approaches hold an impractical hypothesis
    that a substantial amount of parallel training instances are well-annotated. In
    the absence of a parallel corpus, traditional approaches for supervised learning
    are inapplicable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文本风格迁移（TST）是一个广泛研究的NLP任务，旨在将文本的一种风格特征（如情感极性、正式性、礼貌性等）转换为另一种风格，而不改变其语义、句法或其他属性。尽管TST受到了学者们的广泛关注，并且已经开发了许多解决TST问题的方法 Shang
    et al. ([2019](#bib.bib28)); Zhang et al. ([2015](#bib.bib42)); Rao and Tetreault
    ([2018](#bib.bib22)); Zhang et al. ([2020](#bib.bib43))，这些方法假设有大量平行训练实例已被很好标注，但这一假设并不切实际。在没有平行语料库的情况下，传统的监督学习方法是不适用的。
- en: '![Refer to caption](img/7381c6d300fb18063171a48184bfd677.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7381c6d300fb18063171a48184bfd677.png)'
- en: 'Figure 1: UTST system via LLMs and Attention Masking (AM) with four-way interactions:
    Prompt-then-AM, AM-then-prompt, knowledge distillation using LLM outputs as signals,
    and in-context learning using AM outputs as demonstrations. The details of attention
    masking module and LLM-based module are displayed at the left side, where the
    black arrow denotes propagation and the red arrow denotes back-propagation.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：通过LLMs和注意力遮蔽（AM）的UTST系统，包含四种交互方式：Prompt-then-AM、AM-then-prompt、利用LLM输出作为信号的知识蒸馏，以及使用AM输出作为示例的上下文学习。注意力遮蔽模块和基于LLM的模块的详细信息显示在左侧，其中黑色箭头表示传播，红色箭头表示反向传播。
- en: 'Recently, researchers also took efforts to develop a set of systems for Unsupervised
    Text Style Transfer Goyal et al. ([2020](#bib.bib4)); Lewis ([2022](#bib.bib9));
    Luo et al. ([2023](#bib.bib15)); Suzgun et al. ([2022a](#bib.bib32)). Among these
    approaches, we spotlight two types of pioneering methods: attention masking and
    LLM-based methods. Attention masking methods utilize attention mechanisms to distinguish
    the stylistic words that contribute most to the style prediction of a sentence
    via a fine-tuned classifier and then substitute these identified words to transfer
    the sentence polarity. LLM-based methods prompt a general-purpose Large Language
    Model to transform a sentence into an arbitrary style without additional training
    or fine-tuning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员也努力开发了一套无监督文本风格迁移系统 Goyal 等人 ([2020](#bib.bib4))；Lewis ([2022](#bib.bib9))；Luo
    等人 ([2023](#bib.bib15))；Suzgun 等人 ([2022a](#bib.bib32))。在这些方法中，我们重点关注了两种开创性方法：注意力遮蔽方法和基于LLM的方法。注意力遮蔽方法利用注意力机制通过精调分类器区分对句子风格预测贡献最大的风格词，然后替换这些识别出的词以转移句子极性。基于LLM的方法提示通用大语言模型将句子转化为任意风格，无需额外训练或微调。
- en: 'However, they both have shortcomings. The former methods enable modification
    with controllable edits but the edit to the text is restricted to token-level
    substitution, which easily generates unnatural and unsmooth expressions. The latter
    one enables more flexible text generation but it has a high risk of dramatically
    changing the content of the original text. Therefore, an intuitively appealing
    idea is to combine them together. But we encounter the core questions: How to
    effectively combine attention masking and LLM prompting for UTST tasks? Are there
    any keys to note when we combine them?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它们都有不足之处。前者方法允许通过可控的编辑进行修改，但文本的编辑限制在词汇级别的替换，这容易生成不自然和不流畅的表达。后者方法允许更灵活的文本生成，但有很高的风险显著改变原文本的内容。因此，一个直观的吸引人的想法是将它们结合起来。但我们遇到核心问题：如何有效地将注意力遮蔽和LLM提示结合用于UTST任务？在结合时有何需要注意的关键点？
- en: In this paper, we go deep to explore the possible interactions of LLMs and attention
    masking, including (1) pipeline framework with tuned orders; (2) knowledge distillation
    from LLMs to attention masking model; (3) in-context learning with constructed
    parallel examples. In particular, we first show the baseline methods to solve
    UTST tasks. Then we introduce the multi-way interactions to combine LLMs and attention
    masking with detailed implementation. We conduct experiments with three unparalleled
    datasets and six UTST challenges and conclude that these multi-way interactions
    can improve the baselines in certain perspectives of style strength, content preservation,
    and text fluency. Especially, the one-way pipeline of prompting followed by attention
    masking can achieve the SOTA results on UTST tasks, even compared with supervised
    TST systems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们深入探讨了LLMs和注意力遮蔽的可能交互，包括（1）调优顺序的管道框架；（2）从LLMs到注意力遮蔽模型的知识蒸馏；（3）利用构建的平行示例进行上下文学习。特别是，我们首先展示了解决UTST任务的基准方法。然后我们介绍了将LLMs和注意力遮蔽结合的多种交互方式，并详细实施。我们在三个独特的数据集和六个UTST挑战上进行实验，并得出结论，这些多种交互方式可以在风格强度、内容保留和文本流畅性等某些方面改进基准方法。特别是，Prompt-then-AM的单向管道可以在UTST任务上实现SOTA结果，甚至与监督TST系统相比。
- en: 'In summary, the contributions of this study are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结起来，本研究的贡献如下：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We innovatively take the interactions of LLMs and attention masking method into
    the spotlight and discuss the efficient way to combine them for UTST tasks, which
    may benefit general text generation tasks.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们创新性地将LLMs的交互和注意力遮蔽方法聚焦讨论，并探讨了它们在UTST任务中的有效结合方式，这可能对一般的文本生成任务有所裨益。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We empirically show UTST systems with combining LLMs and attention masking in
    four ways can improve the baselines on different evaluation metrics like style
    strength, content preservation, and text fluency.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过四种方式结合LLMs和注意力遮蔽，实证显示UTST系统可以在风格强度、内容保持和文本流畅性等不同评估指标上提高基线。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We achieve the SOTA results of the mean evaluation metric on two commonly-used
    style transfer datasets, namely Yelp-clean and Amazon-clean, via prompting followed
    by attention masking-based revision.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过提示和基于注意力遮蔽的修订，在两个常用的风格转换数据集上实现了均值评估指标的SOTA结果，即Yelp-clean和Amazon-clean。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: For text style transfer, most existing studies hypothesize the existence of
    parallel transferred text. In the task of text style transfer with supervision,
    these parallel corpora are leveraged to learn a model that can convert text from
    one style to another while preserving the original content Shen et al. ([2017](#bib.bib29));
    Prabhumoye et al. ([2018](#bib.bib21)); Fu et al. ([2018](#bib.bib3)); Li et al.
    ([2018](#bib.bib11)); Luo et al. ([2019](#bib.bib14)); Reif et al. ([2021](#bib.bib24)).
    These end-to-end approaches typically involve training on pairs of sentences that
    are semantically equivalent but stylistically distinct, allowing the model to
    capture the nuances of each style. However, in real-world scenarios, we frequently
    encounter cases where the parallel data is unavailable. To solve such unsupervised
    text style transfer tasks, there are two lines of mainstream approaches, that
    aim to implicitly or explicitly model the style-related words in the text respectively
    and then generate a text in the target style Madaan et al. ([2020](#bib.bib16));
    Malmi et al. ([2022](#bib.bib18)); Reid and Zhong ([2021](#bib.bib23)); Mallinson
    et al. ([2022](#bib.bib17)); Wang et al. ([2022a](#bib.bib37)); Vincent et al.
    ([2008](#bib.bib36)); Hu et al. ([2017](#bib.bib6)); Kingma and Welling ([2013](#bib.bib8));
    Fu et al. ([2018](#bib.bib3)); John et al. ([2018](#bib.bib7)); Li et al. ([2020](#bib.bib12)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本风格转换，大多数现有研究假设存在平行转换文本。在有监督的文本风格转换任务中，这些平行语料库被用来学习一个可以在保持原始内容的同时，将文本从一种风格转换为另一种风格的模型 Shen
    et al. ([2017](#bib.bib29)); Prabhumoye et al. ([2018](#bib.bib21)); Fu et al.
    ([2018](#bib.bib3)); Li et al. ([2018](#bib.bib11)); Luo et al. ([2019](#bib.bib14));
    Reif et al. ([2021](#bib.bib24))。这些端到端的方法通常涉及对语义上等价但风格上不同的句子对进行训练，从而使模型能够捕捉每种风格的细微差别。然而，在现实世界中，我们经常遇到平行数据不可用的情况。为了解决这些无监督文本风格转换任务，主流方法有两条路线，分别旨在隐式或显式地建模文本中的风格相关词汇，然后生成目标风格的文本
    Madaan et al. ([2020](#bib.bib16)); Malmi et al. ([2022](#bib.bib18)); Reid and
    Zhong ([2021](#bib.bib23)); Mallinson et al. ([2022](#bib.bib17)); Wang et al.
    ([2022a](#bib.bib37)); Vincent et al. ([2008](#bib.bib36)); Hu et al. ([2017](#bib.bib6));
    Kingma and Welling ([2013](#bib.bib8)); Fu et al. ([2018](#bib.bib3)); John et
    al. ([2018](#bib.bib7)); Li et al. ([2020](#bib.bib12))。
- en: Recent advance in unsupervised text style transfer have leveraged deep learning
    methods like Variational Autoencoders (VAE) and Denoising Autoencoders (DAE) to
    modify textual styles while preserving the original content. A notable approach
    involves learning a latent representation that separates style from content, facilitating
    the generation of new text in the target style. [Hu et al.](#bib.bib6) ([2017](#bib.bib6))
    utilize the VAE framework to learn the latent representation of text and employ
    a style classifier to discern the style attribute vector. Similarly, [Fu et al.](#bib.bib3)
    ([2018](#bib.bib3)) employ an adversarial network to train a content encoder,
    with the encoded content vector being transformed by a style-specific decoder.
    [John et al.](#bib.bib7) ([2018](#bib.bib7)) further refined this approach by
    using VAEs to segregate style and content representations, with the decoder combining
    both to generate the desired output.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督文本风格转换方面的最新进展利用了诸如变分自编码器（VAE）和去噪自编码器（DAE）等深度学习方法，以修改文本风格同时保持原始内容。一种显著的方法涉及学习一个将风格与内容分离的潜在表示，从而促进生成目标风格的新文本。
    [Hu et al.](#bib.bib6) ([2017](#bib.bib6)) 利用VAE框架学习文本的潜在表示，并使用风格分类器来识别风格属性向量。同样，
    [Fu et al.](#bib.bib3) ([2018](#bib.bib3)) 使用对抗网络训练内容编码器，并通过风格特定的解码器转换编码的内容向量。
    [John et al.](#bib.bib7) ([2018](#bib.bib7)) 通过使用VAE来分离风格和内容表示，进一步改进了这种方法，其中解码器将两者结合以生成所需的输出。
- en: Another line of stream explicitly modeling the style-related information by
    identifying the stylistic spans from the text then conducting edits to the identified
    spans. Attention masking methods have been explored extensively across various
    tasks, such as sentiment alteration, gender bias mitigation, and political slant
    adjustmentLi et al. ([2018](#bib.bib11)); Sudhakar et al. ([2019](#bib.bib31)).
    These methods typically involve disassociating the original style from the content
    and then amalgamating the sanitized content with the intended style to synthesize
    new sentences. A pioneering approach in this domain is the “Delete, Retrieve,
    Generate” framework proposed by Li et al. ([2018](#bib.bib11)), which has been
    further enhanced by subsequent studies like Sudhakar et al. ([2019](#bib.bib31)).
    These methodologies excel in retaining substantial portions of the original content
    while transitioning to the target style. However, they have drawbacks, such as
    compromised fluency in the generated text and a dependence on the retrieval of
    analogous content across both source and target styles during the training phase.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流派明确建模样式相关信息，通过识别文本中的样式跨度并对识别出的跨度进行编辑来实现。注意力遮蔽方法在各种任务中得到了广泛探索，例如情感变化、性别偏见缓解和政治倾斜调整Li
    et al. ([2018](#bib.bib11)); Sudhakar et al. ([2019](#bib.bib31))。这些方法通常涉及将原始样式与内容分离，然后将清理过的内容与目标样式结合以合成新句子。该领域的一个开创性方法是Li
    et al. ([2018](#bib.bib11))提出的“删除、检索、生成”框架，随后研究如Sudhakar et al. ([2019](#bib.bib31))进一步提升了这一方法。这些方法在过渡到目标样式的同时，能够保留原始内容的
    substantial 部分。然而，它们也存在缺陷，如生成文本的流畅性受损以及在训练阶段依赖于源样式和目标样式之间的相似内容检索。
- en: Currently, multiple studies attempt to leverage LLMs to solve unsupervised text
    style transfer Tian et al. ([2023](#bib.bib35)); Luo et al. ([2023](#bib.bib15));
    Suzgun et al. ([2022a](#bib.bib32)); Reif et al. ([2022](#bib.bib25)). [Suzgun
    et al.](#bib.bib32) ([2022a](#bib.bib32)) first elaborately design the instruction
    for text style transfer, which achieves promising results on USTS tasks under
    zero or few-shot setting. [Reif et al.](#bib.bib25) ([2022](#bib.bib25)) prompt
    LLMs to acquire a collection of candidate transferred text in the target style,
    then a re-ranking process is conducted to further rank this candidate text to
    produce the final prediction. However, these methods rely solely on the LLMs,
    which has a high risk of changing the semantics of the original sentence. In contrast,
    our method combines the LLMs with the attention masking method and discusses the
    multi-way interactions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，多项研究尝试利用LLMs解决无监督文本样式转换 Tian et al. ([2023](#bib.bib35)); Luo et al. ([2023](#bib.bib15));
    Suzgun et al. ([2022a](#bib.bib32)); Reif et al. ([2022](#bib.bib25))。[Suzgun
    et al.](#bib.bib32) ([2022a](#bib.bib32)) 首次详细设计了文本样式转换的指令，在零样本或少样本设置下在USTS任务中取得了有希望的结果。[Reif
    et al.](#bib.bib25) ([2022](#bib.bib25)) 让LLMs获得一组目标样式下的候选转换文本，然后进行重新排序过程以进一步排名这些候选文本，以产生最终预测。然而，这些方法完全依赖LLMs，这有很高的改变原始句子语义的风险。相比之下，我们的方法将LLMs与注意力遮蔽方法相结合，并讨论了多方位的互动。
- en: 3 Task Definition
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 任务定义
- en: UTST is a NLP task aiming to convert the style of the given text without parallel
    text pairs. Specifically, we are given two non-parallel corpus $\mathcal{D}^{s_{x}}=\{X_{i}\}$
    respectively. The key of text style transfer is to train a model that enables
    to transfer a text from $X$ with preserving the original content.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: UTST是一个NLP任务，旨在在没有平行文本对的情况下转换给定文本的样式。具体来说，我们分别给出两个非平行语料库$\mathcal{D}^{s_{x}}=\{X_{i}\}$。文本样式转换的关键是训练一个能够在保留原始内容的情况下将文本从$X$转移的模型。
- en: 4 Baseline UTST System
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基准UTST系统
- en: We highlight two cutting-edge baseline methods for UTST systems, namely Attention
    masking method Wang et al. ([2022b](#bib.bib38)) and LLM-based method Reif et al.
    ([2022](#bib.bib25)). The former conducts style transfer with attention masking
    and styled filling, where the attention is derived from a classifier trained on
    unparalleled data. The latter prompts LLMs to transfer the text in source style
    to target style, which relies on the highly adaptive behaviour of LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们突出了UTST系统的两种前沿基准方法，即注意力遮蔽方法 Wang et al. ([2022b](#bib.bib38)) 和基于LLM的方法 Reif
    et al. ([2022](#bib.bib25))。前者通过注意力遮蔽和样式填充进行样式转换，其中注意力来源于在独特数据上训练的分类器。后者则利用LLMs将文本从源样式转换为目标样式，这依赖于LLMs的高度适应性行为。
- en: 4.1 Attention Masking Method
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 注意力遮蔽方法
- en: Attention masking method consists of a mask predictor and a styled filling model,
    which take charge of identifying the positions of stylistic words in a text and
    predicting new stylistic words at the masked positions, respectively. We display
    the method in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Unsupervised Text
    Style Transfer via LLMs and Attention Masking with Multi-way Interactions").
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力掩码方法包括一个掩码预测器和一个风格化填充模型，分别负责识别文本中风格化词汇的位置，并在掩码位置预测新的风格化词汇。我们在图 [1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ 通过LLMs和注意力掩码进行无监督文本风格迁移与多方式交互")中展示了该方法。
- en: 'Mask predictor. A RoBERTa-based mask predictor takes a source text $X=\{w_{1},w_{2},...,w_{n}\}$
    to judge the style of the text, where the objective is designed as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码预测器。一个基于RoBERTa的掩码预测器接受源文本$X=\{w_{1},w_{2},...,w_{n}\}$来判断文本风格，目标设计为：
- en: '|  | $\displaystyle\mathcal{L}=-($ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=-($ |  |'
- en: '|  |  | $\displaystyle+\sum_{Y\sim\mathcal{D}^{s_{y}}}s_{y}\cdot\log P_{\text{MaskPredictor}}(Y)).$
    |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{Y\sim\mathcal{D}^{s_{y}}}s_{y}\cdot\log P_{\text{MaskPredictor}}(Y)).$
    |  | (1) |'
- en: 'Regarding inference, following existing UTST studies Lewis ([2022](#bib.bib9)),
    we utilize attention score in the classifier as the style feature to decide the
    mask position, where the scaled attention score is higher than a threshold $\alpha$.
    For simplicity, we denote the above procedure as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 关于推理，参考现有的UTST研究Lewis（[2022](#bib.bib9)），我们利用分类器中的注意力分数作为风格特征来决定掩码位置，其中缩放后的注意力分数高于阈值$\alpha$。为简便起见，我们将上述过程表示为：
- en: '|  | $\displaystyle\hat{X}^{mask}=\text{MaskPredictor}(X).$ |  | (2) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{X}^{mask}=\text{MaskPredictor}(X).$ |  | (2) |'
- en: 'Filling model. A BART-based model fine-tuned on processed $\mathcal{D}^{s_{y}}$
    as the target for fine-tuning. We denote this procedure as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 填充模型。一个基于BART的模型，在处理后的$\mathcal{D}^{s_{y}}$上进行微调，作为微调的目标。我们将此过程表示为：
- en: '|  | $\displaystyle\hat{Y}=\text{FillingModel}(\hat{X}^{mask}).$ |  | (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{Y}=\text{FillingModel}(\hat{X}^{mask}).$ |  | (3) |'
- en: 4.2 LLM-based Method
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于LLM的方法
- en: We leverage LLMs to perform text style transfer. Following existing study Reif
    et al. ([2022](#bib.bib25)), we frame style transfer as a sentence rewriting task
    and prompts with only a natural language instruction.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用大型语言模型（LLMs）进行文本风格迁移。参考现有的研究Reif等人（[2022](#bib.bib25)），我们将风格迁移框架化为一个句子重写任务，并用自然语言指令进行提示。
- en: 'Instruction: Rewrite the following text in a [$s_{y}$].'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令：用[$s_{y}$]重写以下文本。
- en: 'Where [$s_{y}$] are filled with their instantiation. With the instruction,
    no parallel data is required for prompting. We denote this procedure as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中[$s_{y}$]用其实例化填充。根据指令，无需并行数据即可进行提示。我们将此过程表示为：
- en: '|  | $\displaystyle\hat{Y}=\text{LLM}(X)$ |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{Y}=\text{LLM}(X)$ |  |'
- en: 5 Pipeline UTST System
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 流水线UTST系统
- en: The motivation behind is that LLMs have shown outstanding capability of arbitrary
    text transfer even under few-shot setting. But it has the disadvantage of producing
    less controllability in the properties of the style-transferred text than models
    trained on the task-specific training data Reif et al. ([2022](#bib.bib25)). Therefore,
    we propose intuitive pipeline to edit text by attention mask models and LLMs successively,
    which could not only leverage LLMs to conduct arbitrary text transfer at absence
    of parallel text pairs, but also ensure controlability to the semantics of the
    text by trained models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其背后的动机是，LLMs在少量样本设置下展现出了出色的任意文本转移能力。但它的缺点是，相比于在任务特定训练数据上训练的模型，风格迁移文本的属性控制性较差Reif等人（[2022](#bib.bib25)）。因此，我们提出了直观的流水线，通过注意力掩码模型和LLMs依次编辑文本，这不仅可以在缺少并行文本对的情况下利用LLMs进行任意文本迁移，还可以通过训练模型确保对文本语义的控制。
- en: 5.1 Prompt-then-AM
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 Prompt-then-AM
- en: We first conduct text style transfer by prompting the LLMs as illustrated in
    Section [4.2](#S4.SS2 "4.2 LLM-based Method ‣ 4 Baseline UTST System ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions"),
    which results in a sentence with the target style. This would serve as an intermediate
    prediction, which might encounter over-transfer of text. Hence, we apply attention
    masking, which are pre-trained on the non-parallel text pairs, to the intermediate
    prediction. To preserve the content of the intermediate prediction, we tune the
    threshold $\alpha$ to keep more words in the sentences and request the attention
    masking model to predict masked tokens.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过提示LLMs进行文本风格迁移，如第[4.2](#S4.SS2 "4.2 基于LLM的方法 ‣ 4 基线UTST系统 ‣ 通过LLMs和注意力掩码进行无监督文本风格迁移")节所示，这会生成具有目标风格的句子。这将作为一个中间预测，可能会遇到过度转移文本的问题。因此，我们应用在非平行文本对上预训练的注意力掩码到中间预测上。为了保持中间预测的内容，我们调整阈值$\alpha$以保留句子中的更多单词，并请求注意力掩码模型预测掩码词。
- en: Take the sentence “It is awful.” as an example, after prompting the LLMs, the
    input has been transferred into “It is unpleasant.” as the intermediate prediction.
    After further applying the attention masking model to the intermediate prediction,
    the sentence is further rewritten as “It is wonderful.” as the final prediction.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以句子“It is awful.”为例，在提示LLMs之后，输入被转化为“ It is unpleasant.”作为中间预测。进一步应用注意力掩码模型于中间预测后，句子被重新改写为“It
    is wonderful.”作为最终预测。
- en: 5.2 AM-then-prompt
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 AM-then-prompt
- en: 'Alternatively, we first apply attention masking model to edit the original
    text, which is inherently reliable at producing text that looks like the training
    corpus. This serves as an intermediate prediction. Then we prompt LLMs to rewrite
    the intermediate prediction with keeping the semantics unchanged, the prompt of
    which is displayed as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们首先应用注意力掩码模型来编辑原始文本，该模型在生成看似训练语料库的文本时具有内在的可靠性。这作为一个中间预测。然后，我们提示LLMs将中间预测重写，保持语义不变，其提示如下所示：
- en: 'Instruction: Refine the following text without changing its semantic: [$X$].'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令：在不改变语义的情况下，润色以下文本：[$X$]。
- en: Then the outputs of LLMs are extracted as the final prediction. We still take
    the sentence “It is awful.” as an example, the attention masking model transfers
    the text to “It is good.” then we request the LLMs to paraphrase the sentence
    and obtain the final prediction “It is wonderful.”.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，LLMs的输出被提取为最终预测。我们仍以句子“It is awful.”为例，注意力掩码模型将文本转化为“It is good.”，然后我们请求LLMs对句子进行改写，得到最终预测“It
    is wonderful.”。
- en: 6 Using LLM Outputs as Signals
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 使用LLM输出作为信号
- en: To further fuse the knowledge from LLMs with trained model, we propose to conduct
    knowledge distillation for text style transfer. We utilize a LLM as a teacher
    to generate teaching data and improve the performance of a smaller student model,
    that is attention masking model, by the generated teaching data. Instead of directly
    considering the outputs of LLMs as prediction, distilling knowledge from the LLMs
    as training signals, which can be considered as a type of label smoothing regularization Hu
    et al. ([2022](#bib.bib5)), makes the student model learns less about the noisy
    edits to the text.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步融合LLMs的知识与训练模型，我们建议进行文本风格迁移的知识蒸馏。我们利用LLM作为教师生成教学数据，并通过生成的教学数据提升较小学生模型（即注意力掩码模型）的性能。与直接将LLMs的输出视为预测不同，从LLMs中提取知识作为训练信号（这可以视为一种标签平滑正则化
    Hu et al. ([2022](#bib.bib5))），使学生模型减少对文本噪声编辑的学习。
- en: To this end, we first sample data from $\mathcal{D}^{s_{x}}$. Take the sentence
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Unsupervised Text Style Transfer
    via LLMs and Attention Masking with Multi-way Interactions") as an example, comparing
    the original sentence “It is awful” and the transferred sentence “It is wonderful”
    produced via LLM, we annotate “awful” with a mask. Hence the supervision signal
    is “[0, 0, 1]”, where “0” indicates the current token is not a stylistic token
    or is already a stylistic token in $s_{y}$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首先从$\mathcal{D}^{s_{x}}$中采样数据。以图[1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 通过LLMs和注意力掩码进行无监督文本风格迁移的多重交互")中的句子为例，比对原句“It
    is awful”和通过LLM生成的转化句子“It is wonderful”，我们用掩码标记“awful”。因此监督信号为“[0, 0, 1]”，其中“0”表示当前词不是风格词或已经是$s_{y}$中的风格词。
- en: 'In this way, we can fine-tune mask predictor with supervision signals $\mathcal{D}^{1}_{par}=\{X^{s_{x}}_{i}\rightarrow
    X^{mask}_{i}\}$ where the objective of Equation ([1](#S4.E1 "In 4.1 Attention
    Masking Method ‣ 4 Baseline UTST System ‣ Unsupervised Text Style Transfer via
    LLMs and Attention Masking with Multi-way Interactions")) becomes:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以通过监督信号 $\mathcal{D}^{1}_{par}=\{X^{s_{x}}_{i}\rightarrow X^{mask}_{i}\}$
    来微调掩码预测器，其中方程 ([1](#S4.E1 "在 4.1 注意力掩码方法 ‣ 4 基线 UTST 系统 ‣ 通过LLMs和多途径交互的注意力掩码进行无监督文本风格转移"))
    的目标变为：
- en: '|  | $\displaystyle\mathcal{L}=-\sum_{X\sim\mathcal{D}^{1}_{par}}\sum_{w_{k}\sim
    X;w_{k}\text{is masked}}\log P_{\text{MaskPredictor}}(w_{k}).$ |  | (4) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=-\sum_{X\sim\mathcal{D}^{1}_{par}}\sum_{w_{k}\sim
    X;w_{k}\text{is masked}}\log P_{\text{MaskPredictor}}(w_{k}).$ |  | (4) |'
- en: Instead of training a classifier to identify the style of $X$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是训练分类器来识别 $X$ 的风格。
- en: The filling model follows the original implementation but we enhance the fine-tuning
    by augmenting data $\mathcal{D}^{2}_{par}=\{X_{i}^{mask}\rightarrow X^{s_{y}}_{i}\}$
    as the transferred sentence. Comparing with the original filling model, this procedure
    strengthens it by involving the annotations from LLMs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 填充模型遵循原始实现，但我们通过增强数据 $\mathcal{D}^{2}_{par}=\{X_{i}^{mask}\rightarrow X^{s_{y}}_{i}\}$
    来增强微调，作为转移句子。与原始填充模型相比，此过程通过引入来自LLMs的注释来增强模型。
- en: 7 Using AM Outputs as Demonstrations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 使用 AM 输出作为示例
- en: To inject the knowledge of $\mathcal{D}^{s_{x}}$ to LLMs and guide it to produce
    more stylistic sentences as shown in the corpus. In-Context Learning (ICL), as
    a significant prompting strategy, effectively incorporates a small number of demonstrations
    as part of the input Ruis et al. ([2022](#bib.bib27)) to help LLMs comprehend
    the tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将$\mathcal{D}^{s_{x}}$的知识注入LLMs，并指导其生成更多具有语料库中风格的句子。上下文学习（ICL），作为一种重要的提示策略，利用少量示例作为输入的一部分 Ruis
    et al. ([2022](#bib.bib27)) 来帮助LLMs理解任务。
- en: 'Multiple studies Su et al. ([2022](#bib.bib30)); Rubin et al. ([2022](#bib.bib26))
    indicate that a good demonstration should share similarity to the current query
    in the perspectives of semantic pattern. Hence, we first encode the current query
    as well as the text in $\mathcal{D}^{s_{x}}$ via bge-base-en-v1.5 Xiao et al.
    ([2023](#bib.bib39)), which is a commonly used sentence-level encoder. Then, we
    extract the final hidden state as the vectorized representations and compute the
    cosine similarity between the current query and sentences in corpus. We denote
    the above procedure as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究 Su et al. ([2022](#bib.bib30)); Rubin et al. ([2022](#bib.bib26)) 指出，好的示例应该在语义模式的角度上与当前查询共享相似性。因此，我们首先通过
    bge-base-en-v1.5 Xiao et al. ([2023](#bib.bib39)) 对当前查询以及$\mathcal{D}^{s_{x}}$中的文本进行编码，这是一个常用的句子级别编码器。然后，我们提取最终隐藏状态作为向量表示，并计算当前查询与语料库中句子的余弦相似度。我们将上述过程称为：
- en: '|  | $\displaystyle s_{i}=\text{CosSim}_{X_{i}\in\mathcal{D}^{s_{x}}}(X,X_{i}),$
    |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s_{i}=\text{CosSim}_{X_{i}\in\mathcal{D}^{s_{x}}}(X,X_{i}),$
    |  |'
- en: 'where $s_{i}$. Next, we prepend the demonstrations to the prompt, which is
    shown as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{i}$。接下来，我们将演示附加到提示中，具体如下所示：
- en: 'Instruction: ‘‘[$s_{x}$]. ... ‘‘[$s_{x}$ Text’’: [$X^{s_{y}}_{k}$].'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指令：‘‘[$s_{x}$]. ... ‘‘[$s_{x}$ 文本’’：[$X^{s_{y}}_{k}$]。
- en: 'Please rewrite the following text into a [$s_{y}$] Text’’: [$X$] Text’’:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请将以下文本重写为[$s_{y}$] 文本’’：[$X$] 文本’’：
- en: As a result, the outputs of LLMs are extracted as the prediction, which is featured
    with the style of text corpus from $\mathcal{D}^{s_{y}}$.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，LLMs 的输出被提取为预测，其特点是具有来自$\mathcal{D}^{s_{y}}$的文本语料库的风格。
- en: 8 Experimental Setup
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 实验设置
- en: 8.1 Dataset
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 数据集
- en: Following existing UTST studies Reid and Zhong ([2021](#bib.bib23)); Suzgun
    et al. ([2022a](#bib.bib32)), we choose three datasets for our experiments.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 根据现有的UTST研究 Reid 和 Zhong ([2021](#bib.bib23)); Suzgun et al. ([2022a](#bib.bib32))，我们选择了三个数据集进行实验。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Yelp²²2[https://github.com/shentianxiao/language-style-transfer/blob/master/data/yelp/sentiment.dev.0](https://github.com/shentianxiao/language-style-transfer/blob/master/data/yelp/sentiment.dev.0)
    Shen et al. ([2017](#bib.bib29)) is a commonly used sentiment polarity classification
    dataset consisting of review data for Yelp. It has $270$K negative sentences as
    non-parallel corpus.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Yelp²²2[https://github.com/shentianxiao/language-style-transfer/blob/master/data/yelp/sentiment.dev.0](https://github.com/shentianxiao/language-style-transfer/blob/master/data/yelp/sentiment.dev.0)
    Shen et al. ([2017](#bib.bib29)) 是一个常用的情感极性分类数据集，包含Yelp的评论数据。它有 $270$K 条负面句子作为非平行语料。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Amazon³³3[https://github.com/lijuncen/Sentiment-and-Style-Transfer](https://github.com/lijuncen/Sentiment-and-Style-Transfer)
    Li et al. ([2018](#bib.bib11)) is a sentiment classification dataset consisting
    of Amazon reviews. It comprises $277$K negative sentences as non-parallel corpus.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Amazon³³3[https://github.com/lijuncen/Sentiment-and-Style-Transfer](https://github.com/lijuncen/Sentiment-and-Style-Transfer)
    Li等人（[2018](#bib.bib11)）是一个情感分类数据集，包含亚马逊评论。它由$277$K负面句子组成，作为非平行语料库。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Politeness⁴⁴4[https://github.com/tag-and-generate/politeness-dataset](https://github.com/tag-and-generate/politeness-dataset)
    Madaan et al. ([2020](#bib.bib16)) is a dataset designed for classifying text
    as impolite or polite. It is derived from the Enron Email corpus and has $200$K
    test sentences.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Politeness⁴⁴4[https://github.com/tag-and-generate/politeness-dataset](https://github.com/tag-and-generate/politeness-dataset)
    Madaan等人（[2020](#bib.bib16)）是一个用于将文本分类为不礼貌或礼貌的数据集。它源自Enron Email语料库，包含$200$K测试句子。
- en: We employ Yelp-clean and Amazon-clean from existing study⁵⁵5[https://github.com/suzgunmirac/prompt-and-rerank/tree/main/datasets](https://github.com/suzgunmirac/prompt-and-rerank/tree/main/datasets) Suzgun
    et al. ([2022b](#bib.bib33)) as test data, which are the pre-processed data from
    the original text. They both contain $500$ sentences, which are evenly split between
    positive and negative styles.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自现有研究的Yelp-clean和Amazon-clean⁵⁵5[https://github.com/suzgunmirac/prompt-and-rerank/tree/main/datasets](https://github.com/suzgunmirac/prompt-and-rerank/tree/main/datasets)
    Suzgun等人（[2022b](#bib.bib33)）作为测试数据，这些数据是从原始文本中预处理得到的。它们都包含$500$个句子，正负风格均匀分配。
- en: 8.2 Comparable Methods
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 可比方法
- en: We take attention masking (AM) method and LLM-based method as baselines. And
    we further explore the effect of the multi-way interactions of LLMs and attention
    masking as described in Section [5](#S5 "5 Pipeline UTST System ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions"),
    [6](#S6 "6 Using LLM Outputs as Signals ‣ Unsupervised Text Style Transfer via
    LLMs and Attention Masking with Multi-way Interactions"), and [7](#S7 "7 Using
    AM Outputs as Demonstrations ‣ Unsupervised Text Style Transfer via LLMs and Attention
    Masking with Multi-way Interactions"), namely Prompt-then-AM, AM-then-prompt,
    LLM-as-signal, and AM-as-demo.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以注意力掩码（AM）方法和基于LLM的方法作为基线。我们进一步探讨了LLM与注意力掩码的多方式交互的效果，如第[5](#S5 "5 Pipeline
    UTST System ‣ Unsupervised Text Style Transfer via LLMs and Attention Masking
    with Multi-way Interactions")、[6](#S6 "6 Using LLM Outputs as Signals ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions")和[7](#S7
    "7 Using AM Outputs as Demonstrations ‣ Unsupervised Text Style Transfer via LLMs
    and Attention Masking with Multi-way Interactions")节所述，即Prompt-then-AM、AM-then-prompt、LLM-as-signal和AM-as-demo。
- en: 8.3 Evaluation metrics
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 评估指标
- en: 'We follow the prior studies Post ([2018](#bib.bib20)) and measure the UTST
    methods with the following evaluation metrics: We employ a RoBERTa model to independently
    train binary style classifiers on the Yelp, Amazon, and Politeness datasets respectively.
    And we leverage the predicted polarity probability to judge if the generated sentence
    matches the desired styles, namely accuracy (ACC). We also employ reference-BLEU
    (r-sBLEU) to measure the overlap between the generated text and the references
    if there is any, and self-BLEU (s-sBLEU) to measure the extent to which the system
    merely replicates the source text. We further include GPT-2 to measure the smoothness
    and naturalness of the generated text by computing PPL.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循之前的研究Post（[2018](#bib.bib20)）并使用以下评估指标来测量UTST方法：我们使用RoBERTa模型分别在Yelp、Amazon和Politeness数据集上独立训练二分类风格分类器。我们利用预测的极性概率来判断生成的句子是否符合预期风格，即准确率（ACC）。我们还使用参考-BLEU（r-sBLEU）来测量生成文本与参考文本之间的重叠程度（如果有的话），并使用自我-BLEU（s-sBLEU）来衡量系统仅仅复制源文本的程度。我们进一步包括GPT-2，通过计算PPL来测量生成文本的流畅性和自然性。
- en: Following Narasimhan et al. ([2023](#bib.bib19)); Wang et al. ([2022a](#bib.bib37)),
    we also propose a comprehensive Mean metric, combining ACC, s-sBLEU, and a scaled
    PPL through a geometric mean. When calculating the Mean score, the PPL score was
    exponentially scaled to align its lower-is-better nature with the higher-is-better
    orientation of the other metrics, normalizing its range to $[0,100]$..
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Narasimhan等人（[2023](#bib.bib19)）；Wang等人（[2022a](#bib.bib37)），我们还提出了一个综合的Mean指标，结合了ACC、s-sBLEU和通过几何平均得到的缩放PPL。在计算Mean分数时，PPL分数被指数缩放，以使其较低值更优的特性与其他指标的较高值更优的方向一致，将其范围归一化为$[0,100]$。
- en: 8.4 Implementation Details
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 实施细节
- en: We fine-tuned the pre-trained RoBERTa Liu et al. ([2019](#bib.bib13)) and BART
    Lewis et al. ([2019](#bib.bib10)) models in Equation ([2](#S4.E2 "In 4.1 Attention
    Masking Method ‣ 4 Baseline UTST System ‣ Unsupervised Text Style Transfer via
    LLMs and Attention Masking with Multi-way Interactions")) and ([3](#S4.E3 "In
    4.1 Attention Masking Method ‣ 4 Baseline UTST System ‣ Unsupervised Text Style
    Transfer via LLMs and Attention Masking with Multi-way Interactions")) with specific
    configurations tailored to their respective tasks and datasets. The $\alpha$.
    For RoBERTa, we set the batch size to $64$ epochs using the AdamW optimizer with
    a learning rate of $1e-5$, running $10$ epochs for the Politeness dataset, which
    allowed for quicker convergence given its distinct content. We incorporate ChatGLM2-6B Du
    et al. ([2022](#bib.bib2)) as the backbone of the LLMs used in our methods, due
    to the wide usage and outstanding performance in style transferring Xuanfan and
    Piji ([2023](#bib.bib41)); Tao et al. ([2024](#bib.bib34)). More details can be
    found in Appendix [A](#A1 "Appendix A Appendix ‣ Unsupervised Text Style Transfer
    via LLMs and Attention Masking with Multi-way Interactions").
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对预训练的 RoBERTa Liu et al. ([2019](#bib.bib13)) 和 BART Lewis et al. ([2019](#bib.bib10))
    模型进行了微调，配置如方程 ([2](#S4.E2 "In 4.1 Attention Masking Method ‣ 4 Baseline UTST System
    ‣ Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way
    Interactions")) 和 ([3](#S4.E3 "In 4.1 Attention Masking Method ‣ 4 Baseline UTST
    System ‣ Unsupervised Text Style Transfer via LLMs and Attention Masking with
    Multi-way Interactions")) 所示，具体配置根据各自的任务和数据集进行了调整。对于 RoBERTa，我们将批量大小设置为 $64$ 轮次，使用
    AdamW 优化器，学习率为 $1e-5$，在 Politeness 数据集上运行 $10$ 轮，这使得其在独特内容下能够更快地收敛。我们将 ChatGLM2-6B
    Du et al. ([2022](#bib.bib2)) 作为我们方法中使用的 LLM 的主干，因其在风格转移中的广泛使用和出色表现 Xuanfan 和
    Piji ([2023](#bib.bib41)); Tao et al. ([2024](#bib.bib34))。更多细节见附录 [A](#A1 "Appendix
    A Appendix ‣ Unsupervised Text Style Transfer via LLMs and Attention Masking with
    Multi-way Interactions")。
- en: 9 Results and Analysis
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结果与分析
- en: 9.1 Comparison of Different Interactions
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 不同交互的比较
- en: 'The experimental results presented in Table [1](#S9.T1 "Table 1 ‣ 9.1 Comparison
    of Different Interactions ‣ 9 Results and Analysis ‣ Unsupervised Text Style Transfer
    via LLMs and Attention Masking with Multi-way Interactions") provide a comprehensive
    overview of the performance of various methods for text style transfer across
    multiple datasets. We present a summary of our key findings:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S9.T1 "Table 1 ‣ 9.1 Comparison of Different Interactions ‣ 9 Results
    and Analysis ‣ Unsupervised Text Style Transfer via LLMs and Attention Masking
    with Multi-way Interactions") 中展示的实验结果提供了对多数据集中文本风格转移各种方法性能的全面概述。我们总结了我们的关键发现：
- en: (1) For UTST tasks, in comparison, LLMs are more likely to produce more fluent
    transferred sentences and AM methods have advantages in transferring sentences
    to targeted style. Therefore, it is intuitive to combine these two paradigms together.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 对于 UTST 任务，相比之下，LLMs 更有可能生成更流畅的转换句子，而 AM 方法在将句子转化为目标风格方面具有优势。因此，将这两种范式结合在一起是直观的。
- en: (2) Considering the different interactions between AM and LLMs, there is no
    absolute agreement on the different interactions of LLMs and AM methods regarding
    the performance. But on five out of six settings, Prompt-then-AM method outperforms
    the other methods by mean metric. This indicates that applying prompting followed
    by AM could effectively balance the fluency and style strength.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 考虑到 AM 和 LLM 之间的不同交互，对 LLM 和 AM 方法性能的不同交互没有绝对的一致意见。但在六个设置中的五个上，Prompt-then-AM
    方法通过平均度量超越了其他方法。这表明，应用提示然后进行 AM 可以有效平衡流畅度和风格强度。
- en: (3) AM-as-demo method exhibits the lowest PPL in major settings. And it improves
    ACC of LLM-based baseline with observable margin, suggesting that by adding valid
    demonstrations could help LLMs learn better about the target styles though the
    upper bound heavily relies on the capability of the LLM.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (3) AM-as-demo 方法在主要设置中表现出最低的 PPL，并且它提高了 LLM 基线模型的 ACC，差距显著，这表明通过添加有效的演示可以帮助
    LLM 更好地学习目标风格，尽管上限严重依赖于 LLM 的能力。
- en: (4) LLM-as-signal has more advantage in preserving contents of original text
    than AM method. It achieves the best r-sBLEU and s-sBLEU in majority of settings
    but usually shows poor ACC. This may because the signals generated via LLMs are
    initially have poor style strength, distillation from such signals affects the
    attention masking in a negative way.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: (4) LLM-as-signal 在保留原文内容方面比 AM 方法更具优势。在大多数设置中，它实现了最佳的 r-sBLEU 和 s-sBLEU，但通常显示出较差的
    ACC。这可能是因为通过 LLM 生成的信号最初具有较差的风格强度，从这些信号提取的蒸馏影响了注意力遮罩的负面效果。
- en: '| Dataset | Method | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | Mean ($\uparrow$)
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法 | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | 平均 ($\uparrow$) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| AMAZON | LLM-based | $29$ | $\underline{34.0}$ | $32.0$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| AMAZON | 基于LLM | $29$ | $\underline{34.0}$ | $32.0$ |'
- en: '| N$\rightarrow$ | $23.0$ | $178$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| N$\rightarrow$ | $23.0$ | $178$ |'
- en: '|  | Prompt-then-AM | $\mathbf{87}$ | $22.2$ | $\mathbf{45.2}$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 提示-然后-AM | $\mathbf{87}$ | $22.2$ | $\mathbf{45.2}$ |'
- en: '|  | AM-then-prompt | $\underline{83}$ | $13.5$ | $42.5$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-然后提示 | $\underline{83}$ | $13.5$ | $42.5$ |'
- en: '|  | LLM-as-signal | $27$ | $\mathbf{70.9}$ | $34.7$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-作为信号 | $27$ | $\mathbf{70.9}$ | $34.7$ |'
- en: '|  | AM-as-demo | $56$ | $20.5$ | $40.3$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-作为示例 | $56$ | $20.5$ | $40.3$ |'
- en: '| AMAZON | LLM-based | $75$ | $34.7$ | $49.7$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| AMAZON | 基于LLM | $75$ | $34.7$ | $49.7$ |'
- en: '| P$\rightarrow$ | $\underline{35.3}$ | $123$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| P$\rightarrow$ | $\underline{35.3}$ | $123$ |'
- en: '|  | Prompt-then-AM | $\mathbf{97}$ | $32.7$ | $\mathbf{55.4}$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | 提示-然后-AM | $\mathbf{97}$ | $32.7$ | $\mathbf{55.4}$ |'
- en: '|  | AM-then-prompt | $75$ | $17.5$ | $45.2$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-然后提示 | $75$ | $17.5$ | $45.2$ |'
- en: '|  | LLM-as-signal | $55$ | $\mathbf{74.9}$ | $48.3$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-作为信号 | $55$ | $\mathbf{74.9}$ | $48.3$ |'
- en: '|  | AM-as-demo | $\underline{85}$ | $36.1$ | $49.8$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-作为示例 | $\underline{85}$ | $36.1$ | $49.8$ |'
- en: '| YELP | LLM-based | $67$ | $33.1$ | 51.9 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| YELP | 基于LLM | $67$ | $33.1$ | 51.9 |'
- en: '| N$\rightarrow$ | $\underline{38.7}$ | $128$ | 51.7 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| N$\rightarrow$ | $\underline{38.7}$ | $128$ | 51.7 |'
- en: '|  | Prompt-then-AM | $\mathbf{93}$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 提示-然后-AM | $\mathbf{93}$ |'
- en: '|  | AM-then-prompt | $\underline{84}$ | $36.4$ | 55.2 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-然后提示 | $\underline{84}$ | $36.4$ | 55.2 |'
- en: '|  | LLM-as-signal | $31$ | $\mathbf{64.2}$ | 37.3 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-作为信号 | $31$ | $\mathbf{64.2}$ | 37.3 |'
- en: '|  | AM-as-demo | 81 | 32.5 | 38.8 | 68 | 52.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-作为示例 | 81 | 32.5 | 38.8 | 68 | 52.0 |'
- en: '| YELP | LLM-based | $80$ | $37.6$ | 51.6 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| YELP | 基于LLM | $80$ | $37.6$ | 51.6 |'
- en: '| P$\rightarrow$ | $\underline{37.4}$ | $108$ | 51.6 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| P$\rightarrow$ | $\underline{37.4}$ | $108$ | 51.6 |'
- en: '|  | Prompt-then-AM | $\mathbf{97}$ | $36.8$ | $\mathbf{57.2}$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | 提示-然后-AM | $\mathbf{97}$ | $36.8$ | $\mathbf{57.2}$ |'
- en: '|  | AM-then-prompt | $74$ | $23.3$ | 47.9 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-然后提示 | $74$ | $23.3$ | 47.9 |'
- en: '|  | LLM-as-signal | $42$ | $\mathbf{86.0}$ | 49.2 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-作为信号 | $42$ | $\mathbf{86.0}$ | 49.2 |'
- en: '|  | AM-as-demo | $\underline{94}$ | $15.3$ | 51.9 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-作为示例 | $\underline{94}$ | $15.3$ | 51.9 |'
- en: '| Politeness | LLM-based | $66$ | $46.7$ | 50.9 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 礼貌性 | 基于LLM | $66$ | $46.7$ | 50.9 |'
- en: '| I$\rightarrow$ | $-$ | $181$ | 53.6 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| I$\rightarrow$ | $-$ | $181$ | 53.6 |'
- en: '|  | Prompt-then-AM | $\mathbf{93}$ | $42.4$ | 55.2 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 提示-然后-AM | $\mathbf{93}$ | $42.4$ | 55.2 |'
- en: '|  | AM-then-prompt | $80$ | $44.5$ | $\mathbf{55.7}$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-然后提示 | $80$ | $44.5$ | $\mathbf{55.7}$ |'
- en: '|  | LLM-as-signal | $54$ | $\underline{55.6}$ | 38.7 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-作为信号 | $54$ | $\underline{55.6}$ | 38.7 |'
- en: '|  | AM-as-demo | $78$ | $40.7$ | 55.1 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-作为示例 | $78$ | $40.7$ | 55.1 |'
- en: '| Politeness | LLM-based | $53$ | $46.2$ | 61 | 46.4 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 礼貌性 | 基于LLM | $53$ | $46.2$ | 61 | 46.4 |'
- en: '| P$\rightarrow$ | $-$ | $171$ | 55.2 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| P$\rightarrow$ | $-$ | $171$ | 55.2 |'
- en: '|  | Prompt-then-AM | $\mathbf{95}$ | $44.2$ | $\mathbf{57.4}$ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | 提示-然后-AM | $\mathbf{95}$ | $44.2$ | $\mathbf{57.4}$ |'
- en: '|  | AM-then-prompt | $74$ | $48.2$ | 55.3 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-然后提示 | $74$ | $48.2$ | 55.3 |'
- en: '|  | LLM-as-signal | $47$ | $\underline{63.2}$ | 39.1 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-作为信号 | $47$ | $\underline{63.2}$ | 39.1 |'
- en: '|  | AM-as-demo | $67$ | $34.2$ | 47.3 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | AM-作为示例 | $67$ | $34.2$ | 47.3 |'
- en: 'Table 1: Results of text style transfer with multi-level interactions on several
    UTST datasets. “P$\rightarrow$ P” denotes positive style to negative style and
    negative style to positive style, respectively. “P$\rightarrow$ P” denotes polite
    style to impolite style and impolite style to polite style, respectively.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在多个UTST数据集上具有多级交互的文本风格转移结果。“P$\rightarrow$ P”表示从正面风格到负面风格以及从负面风格到正面风格。“P$\rightarrow$
    P”表示从礼貌风格到无礼风格以及从无礼风格到礼貌风格。
- en: 9.2 Comparison with Other TST Systems
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 与其他TST系统的比较
- en: We further compare our methods based on LLMs and attention masking with other
    text style transfer systems with supervision or without supervision. Table [2](#S9.T2
    "Table 2 ‣ 9.2 Comparison with Other TST Systems ‣ 9 Results and Analysis ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions")
    displays the results on Yelp-clean. As we can see, even though under supervised
    text style transfer setting, a system can be trained using the parallel corpus,
    it is still short in producing fluent transferred sentences, which results in
    a relatively low PPL value. Unsupervised methods with LLMs can achieve impressive
    results even without any training or fine-tuning on the parallel corpus, but it
    shows flaws in preserving the original semantics. Among all the UTST systems,
    Prompt-then-AM surpasses the other systems on ACC and obtains the highest results
    of mean metric.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步将基于 LLMs 和注意力掩码的方法与其他文本风格转移系统进行比较，无论是有监督还是无监督的。表格 [2](#S9.T2 "表 2 ‣ 9.2
    与其他 TST 系统的比较 ‣ 9 结果与分析 ‣ 无监督文本风格转移通过 LLMs 和注意力掩码与多方交互") 显示了 Yelp-clean 上的结果。正如我们所见，即使在有监督的文本风格转移设置下，系统可以利用平行语料库进行训练，但在生成流畅的转移句子方面仍然不足，导致
    PPL 值相对较低。即便无监督方法使用 LLMs 没有经过任何平行语料库的训练或微调，也能取得令人印象深刻的结果，但在保留原始语义方面存在缺陷。在所有 UTST
    系统中，Prompt-then-AM 在 ACC 上超过了其他系统，并获得了最高的平均指标结果。
- en: We have the similar observation based on Table [3](#S9.T3 "Table 3 ‣ 9.2 Comparison
    with Other TST Systems ‣ 9 Results and Analysis ‣ Unsupervised Text Style Transfer
    via LLMs and Attention Masking with Multi-way Interactions"), which includes the
    results on Amazon-clean. The supervised TST systems have relatively high style
    strength but low fluency. The unsupervised TST systems collaborating with diverse
    LLMs usually generate fluent target sentences but are short in target style. Overall,
    our method of Prompt-then-AM can achieve the best mean score with highest style
    strength.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据表格 [3](#S9.T3 "表 3 ‣ 9.2 与其他 TST 系统的比较 ‣ 9 结果与分析 ‣ 无监督文本风格转移通过 LLMs 和注意力掩码与多方交互")
    也有类似的观察，包括了 Amazon-clean 上的结果。监督 TST 系统具有相对较高的风格强度，但流畅度较低。与多样化 LLMs 协作的无监督 TST
    系统通常生成流畅的目标句子，但目标风格较弱。总体而言，我们的方法 Prompt-then-AM 能够取得最佳的平均分数，并具有最高的风格强度。
- en: '| Method | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | Mean ($\uparrow$) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | 平均 ($\uparrow$) |'
- en: '| Supervised Text Style Transfer |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 有监督文本风格转移 |'
- en: '| $[1]$ CrossAlignment | 73 | 7.8 | 18.3 | 217 | 31.7 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| $[1]$ CrossAlignment | 73 | 7.8 | 18.3 | 217 | 31.7 |'
- en: '| $[2]$ BackTrans | 95 | 2.0 | 46.5 | 158 | 50.3 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| $[2]$ BackTrans | 95 | 2.0 | 46.5 | 158 | 50.3 |'
- en: '| $[3]$ MultiDecoder | 46 | 13.0 | 39.4 | 373 | 28.6 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| $[3]$ MultiDecoder | 46 | 13.0 | 39.4 | 373 | 28.6 |'
- en: '| $[4]$ DeleteOnly | 85 | 13.4 | 33.9 | 182 | 41.8 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| $[4]$ DeleteOnly | 85 | 13.4 | 33.9 | 182 | 41.8 |'
- en: '| $[4]$ DeleteAndRetrieve | 90 | 14.7 | 36.4 | 180 | 44.4 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| $[4]$ DeleteAndRetrieve | 90 | 14.7 | 36.4 | 180 | 44.4 |'
- en: '| $[5]$ UnpairedRL | 49 | 16.8 | 45.7 | 385 | 31.7 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| $[5]$ UnpairedRL | 49 | 16.8 | 45.7 | 385 | 31.7 |'
- en: '| $[6]$ DualRL | 88 | 25.9 | 58.9 | 133 | 53.5 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| $[6]$ DualRL | 88 | 25.9 | 58.9 | 133 | 53.5 |'
- en: '| $[7]$ ST (Multi-Class) | 86 | 26.4 | 63.0 | 175 | 52.0 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| $[7]$ ST（多类别） | 86 | 26.4 | 63.0 | 175 | 52.0 |'
- en: '| $[7]$ ST (Conditional) | 93 | 22.9 | 52.8 | 223 | 49.8 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| $[7]$ ST（条件） | 93 | 22.9 | 52.8 | 223 | 49.8 |'
- en: '| $[8]$ B-GST | 81 | 21.6 | 46.5 | 158 | 45.6 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| $[8]$ B-GST | 81 | 21.6 | 46.5 | 158 | 45.6 |'
- en: '| Unsupervised Text Style Transfer |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 无监督文本风格转移 |'
- en: '| $[9]$ Prompt-and-Rerank (GPT2) | 87 | 14.8 | 28.7 | 65 | 51.1 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| $[9]$ Prompt-and-Rerank (GPT2) | 87 | 14.8 | 28.7 | 65 | 51.1 |'
- en: '| $[9]$ Prompt-and-Rerank (GPT-J) | 87 | 23.0 | 47.7 | 80 | 54.9 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| $[9]$ Prompt-and-Rerank (GPT-J) | 87 | 23.0 | 47.7 | 80 | 54.9 |'
- en: '| Prompt-then-AM | 93 | 26.7 | 31.9 | 59 | $\mathbf{55.4}$ |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Prompt-then-AM | 93 | 26.7 | 31.9 | 59 | $\mathbf{55.4}$ |'
- en: '| AM-then-prompt | $84$ | $36.4$ | 55.2 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| AM-then-prompt | $84$ | $36.4$ | 55.2 |'
- en: '| AM-as-demo | 81 | 32.5 | 38.8 | 68 | 52.0 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| AM-as-demo | 81 | 32.5 | 38.8 | 68 | 52.0 |'
- en: '| LLM-as-signal | 31 | 42.6 | 64.2 | 119 | 37.3 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| LLM-as-signal | 31 | 42.6 | 64.2 | 119 | 37.3 |'
- en: 'Table 2: A comparison of existing methods on Yelp-clean (N$\rightarrow$ P).
    References: [1] Shen et al. ([2017](#bib.bib29)), [2] Prabhumoye et al. ([2018](#bib.bib21))
    , [3] Fu et al. ([2018](#bib.bib3)), [4] Li et al. ([2018](#bib.bib11)), [5] Xu
    et al. ([2018](#bib.bib40)), [6] Luo et al. ([2019](#bib.bib14)), [7] Dai et al.
    ([2019](#bib.bib1)), [8] Sudhakar et al. ([2019](#bib.bib31)) [9] Reif et al.
    ([2021](#bib.bib24)). The results of other systems are copied from prior study Suzgun
    et al. ([2022a](#bib.bib32)).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 现有方法在 Yelp-clean (N$\rightarrow$ P) 上的比较。参考文献: [1] Shen 等 ([2017](#bib.bib29)),
    [2] Prabhumoye 等 ([2018](#bib.bib21)), [3] Fu 等 ([2018](#bib.bib3)), [4] Li 等
    ([2018](#bib.bib11)), [5] Xu 等 ([2018](#bib.bib40)), [6] Luo 等 ([2019](#bib.bib14)),
    [7] Dai 等 ([2019](#bib.bib1)), [8] Sudhakar 等 ([2019](#bib.bib31)) [9] Reif 等
    ([2021](#bib.bib24))。其他系统的结果来自于先前的研究 Suzgun 等 ([2022a](#bib.bib32))。'
- en: '| Method | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | Mean ($\uparrow$) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) | 平均 ($\uparrow$) |'
- en: '| Supervised Text Style Transfer |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 有监督文本风格转换 |'
- en: '| $[1]$ Style-Embedding | 47 | 13.1 | 29.0 | 287 | 25.8 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| $[1]$ 风格嵌入 | 47 | 13.1 | 29.0 | 287 | 25.8 |'
- en: '| $[1]$ CrossAligned | 74 | 1.7 | 2.4 | 96 | 33.4 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| $[1]$ 跨对齐 | 74 | 1.7 | 2.4 | 96 | 33.4 |'
- en: '| $[1]$ DeleteAndRetrieve | 51 | 26.7 | 53.5 | 113 | 41.0 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| $[1]$ 删除并检索 | 51 | 26.7 | 53.5 | 113 | 41.0 |'
- en: '| $[1]$ TemplateBased | 56 | 31.0 | 65.7 | 200 | 42.2 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| $[1]$ 基于模板 | 56 | 31.0 | 65.7 | 200 | 42.2 |'
- en: '| Unsupervised Text Style Transfer |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 无监督文本风格转换 |'
- en: '| $[2]$ GPT-2-Small | 18 | 17.7 | 38.1 | 48 | 34.9 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| $[2]$ GPT-2-Small | 18 | 17.7 | 38.1 | 48 | 34.9 |'
- en: '| $[2]$ GPT-2-Medium | 32 | 20.1 | 38.0 | 57 | 37.5 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| $[2]$ GPT-2-Medium | 32 | 20.1 | 38.0 | 57 | 37.5 |'
- en: '| $[2]$ GPT-2-Large | 28 | 26.0 | 51.2 | 55 | 41.0 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| $[2]$ GPT-2-Large | 28 | 26.0 | 51.2 | 55 | 41.0 |'
- en: '| $[2]$ GPT-2-XL | 32 | 22.3 | 41.4 | 70 | 36.1 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| $[2]$ GPT-2-XL | 32 | 22.3 | 41.4 | 70 | 36.1 |'
- en: '| $[2]$ GPT-Neo-1.3B | 31 | 10.9 | 20.5 | 35 | 36.9 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| $[2]$ GPT-Neo-1.3B | 31 | 10.9 | 20.5 | 35 | 36.9 |'
- en: '| $[2]$ GPT-Neo-2.7B | 28 | 23.7 | 45.9 | 57 | 38.8 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| $[2]$ GPT-Neo-2.7B | 28 | 23.7 | 45.9 | 57 | 38.8 |'
- en: '| $[2]$ GPT-J-6B | 33 | 27.1 | 47.7 | 72 | 38.2 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| $[2]$ GPT-J-6B | 33 | 27.1 | 47.7 | 72 | 38.2 |'
- en: '| Prompt-then-AM | 87 | 17.0 | 22.2 | 89 | $\mathbf{45.2}$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 提示-再AM | 87 | 17.0 | 22.2 | 89 | $\mathbf{45.2}$ |'
- en: '| AM-then-prompt | 83 | 10.0 | 13.5 | 78 | 42.5 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| AM-再提示 | 83 | 10.0 | 13.5 | 78 | 42.5 |'
- en: '| AM-as-demo | 56 | 19.2 | 20.5 | 54 | 40.3 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| AM-作为示例 | 56 | 19.2 | 20.5 | 54 | 40.3 |'
- en: '| LLM-as-signal | 27 | 44.2 | 70.9 | 187 | 34.7 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| LLM-作为信号 | 27 | 44.2 | 70.9 | 187 | 34.7 |'
- en: 'Table 3: A comparison of existing methods on Amazon-clean (N$\rightarrow$ P).
    References: [1] Li et al. ([2018](#bib.bib11)), [2] Suzgun et al. ([2022a](#bib.bib32)).
    The results of other systems are copied from prior study Suzgun et al. ([2022a](#bib.bib32)).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 现有方法在 Amazon-clean (N$\rightarrow$ P) 上的比较。参考文献: [1] Li 等 ([2018](#bib.bib11)),
    [2] Suzgun 等 ([2022a](#bib.bib32))。其他系统的结果来自于先前的研究 Suzgun 等 ([2022a](#bib.bib32))。'
- en: 9.3 Effect of $\alpha$ in Prompt-then-AM
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3 提示-再AM中 $\alpha$ 的影响
- en: '![Refer to caption](img/0af9b92c8c7d4bb5d26afa42c2929c11.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0af9b92c8c7d4bb5d26afa42c2929c11.png)'
- en: 'Figure 2: ACC and s-sBLEU of Prompt-then-AM on Yelp-clean (N$\rightarrow$.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 提示-再AM在 Yelp-clean (N$\rightarrow$ 上的 ACC 和 s-sBLEU。'
- en: '![Refer to caption](img/06986d172db97225aa1ec5f4cf612bdb.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/06986d172db97225aa1ec5f4cf612bdb.png)'
- en: 'Figure 3: ACC and PPL of Prompt-then-AM on Yelp-clean (N$\rightarrow$.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 提示-再AM在 Yelp-clean (N$\rightarrow$ 上的 ACC 和 PPL。'
- en: '| Methods | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ACC ($\uparrow$) | s-sBLEU ($\uparrow$) |'
- en: '| ChatGLM2-6B |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B |'
- en: '| LLM-based | 67 | 27.7 | 33.1 | 39 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 基于LLM | 67 | 27.7 | 33.1 | 39 |'
- en: '| Prompt-then-AM | 93 | 26.7 | 31.9 | 59 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 提示-再AM | 93 | 26.7 | 31.9 | 59 |'
- en: '| AM-then-prompt | $84$ | $36.4$ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| AM-再提示 | $84$ | $36.4$ |'
- en: '| GPT2-XL |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-XL |'
- en: '| LLM-based | 22 | 12.7 | 15.1 | 83 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 基于LLM | 22 | 12.7 | 15.1 | 83 |'
- en: '| Prompt-then-AM | 81 | 13.3 | 14.9 | 172 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 提示-再AM | 81 | 13.3 | 14.9 | 172 |'
- en: '| AM-then-prompt | 52 | 8.4 | 9.4 | 51 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| AM-再提示 | 52 | 8.4 | 9.4 | 51 |'
- en: '| GPT-J-6B |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| GPT-J-6B |'
- en: '| LLM-based | 46 | 12.8 | 28.9 | 49 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 基于LLM | 46 | 12.8 | 28.9 | 49 |'
- en: '| Prompt-then-AM | 87 | 12.4 | 27.6 | 78 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 提示-再AM | 87 | 12.4 | 27.6 | 78 |'
- en: '| AM-then-prompt | 69 | 10.2 | 15.8 | 46 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| AM-再提示 | 69 | 10.2 | 15.8 | 46 |'
- en: 'Table 4: A comparison of LLM-based methods with different backbone LLMs on
    Yelp-clean (N$\rightarrow$ P).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 不同基础LLM上的基于LLM的方法在 Yelp-clean (N$\rightarrow$ P) 上的比较。'
- en: From the above experiments, we conclude that Prompt-then-AM shows a superior
    effect among the multiple interactions. In Prompt-then-AM method, $\alpha$ value
    signifies more aggressive edits while a lower value maintains the outputs of LLMs.
    We draw the ACC and s-sBLEU of Prompt-then-AM on Yelp-clean (N$\rightarrow$ in
    Figure [3](#S9.F3 "Figure 3 ‣ 9.3 Effect of 𝛼 in Prompt-then-AM ‣ 9 Results and
    Analysis ‣ Unsupervised Text Style Transfer via LLMs and Attention Masking with
    Multi-way Interactions"). When $\alpha$, prompt-then-AM degrades into LLM-based
    method and when $\alpha$, prompt-then-AM masks out all the intermediate outputs
    and re-prediction using AM model. We observe an negative correlation between ACC
    and s-sBLEU with the increasing $\alpha$. This indicates that with the increasing
    number of edits on the outputs of LLMs, we can enhance the style strength of the
    transferred sentences at the expense of the changing the original semantics.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述实验中，我们得出结论，提示-然后-AM 在多种交互中表现优越。在提示-然后-AM 方法中，$\alpha$ 值表示更积极的编辑，而较低的值保持 LLM
    的输出。我们在图 [3](#S9.F3 "图 3 ‣ 9.3 提示-然后-AM 中 𝛼 的影响 ‣ 9 结果与分析 ‣ 通过 LLM 和注意力遮罩进行的无监督文本风格转换")
    上绘制了提示-然后-AM 的 ACC 和 s-sBLEU。当 $\alpha$ 时，提示-然后-AM 退化为基于 LLM 的方法，而当 $\alpha$ 时，提示-然后-AM
    遮蔽所有中间输出并使用 AM 模型重新预测。我们观察到 ACC 和 s-sBLEU 随着 $\alpha$ 的增加而呈负相关。这表明，随着对 LLM 输出进行的编辑数量的增加，我们可以在改变原始语义的代价下增强转换句子的风格强度。
- en: A positive correlation trend between ACC and PPL can be observed in Figure [3](#S9.F3
    "Figure 3 ‣ 9.3 Effect of 𝛼 in Prompt-then-AM ‣ 9 Results and Analysis ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions").
    With the increase of $\alpha$.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 [3](#S9.F3 "图 3 ‣ 9.3 提示-然后-AM 中 𝛼 的影响 ‣ 9 结果与分析 ‣ 通过 LLM 和注意力遮罩进行的无监督文本风格转换")
    中可以观察到 ACC 和 PPL 之间的正相关趋势。随着 $\alpha$ 的增加。
- en: 9.4 Different LLMs as Backbones
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4 不同的 LLM 作为核心
- en: We further try different LLMs as the backbone for the multi-ways interaction.
    From Table [4](#S9.T4 "Table 4 ‣ Figure 3 ‣ 9.3 Effect of 𝛼 in Prompt-then-AM
    ‣ 9 Results and Analysis ‣ Unsupervised Text Style Transfer via LLMs and Attention
    Masking with Multi-way Interactions"), we observe the similar trends for various
    baselines. Involving AM is able to improve the performance of LLM-based methods.
    And Prompt-then-AM interaction is more efficient in generating comprehensively
    good outputs. It is worth noting that different backbones may decide the upper
    bound of the UTST systems. Methods with ChatGLM2 generally surpass the other systems,
    which indicates it is vital to select a good LLM as the collaborator for unsupervised
    text style transfer.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步尝试将不同的 LLM 作为多方式交互的核心。从表 [4](#S9.T4 "表 4 ‣ 图 3 ‣ 9.3 提示-然后-AM 中 𝛼 的影响 ‣
    9 结果与分析 ‣ 通过 LLM 和注意力遮罩进行的无监督文本风格转换") 中，我们观察到各种基线的类似趋势。涉及 AM 可以提高基于 LLM 的方法的性能。并且提示-然后-AM
    交互在生成全面良好的输出方面更有效。值得注意的是，不同的核心可能决定 UTST 系统的上限。使用 ChatGLM2 的方法通常优于其他系统，这表明选择一个好的
    LLM 作为无监督文本风格转换的协作者是至关重要的。
- en: 9.5 Case Study
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5 案例研究
- en: '| Methods | Sentence |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 句子 |'
- en: '| Source text | there are no smiles and no customer |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 源文本 | 没有笑容和没有客户 |'
- en: '|  | service. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | 服务。 |'
- en: '| AM | there are always smiles and great |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| AM | 总是有笑容和优质的 |'
- en: '|  | customer service. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | 客户服务。 |'
- en: '| LLM-based | there is a lack of smiles and a lack of |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 基于 LLM 的 | 缺乏笑容和缺乏 |'
- en: '|  | customer service. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | 客户服务。 |'
- en: '| Prompt-then-AM | there are plenty of smiles and great |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 提示-然后-AM | 有很多笑容和优质的 |'
- en: '|  | customer service. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | 客户服务。 |'
- en: '| AM-then-prompt | there is always a smile and exceptional |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| AM-然后提示 | 总是有微笑和出色的 |'
- en: '|  | customer service. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | 客户服务。 |'
- en: '| AM-as-demo | They make you feel at ease and their |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| AM-作为演示 | 他们让你感到舒适，他们的 |'
- en: '|  | smiles are contagious. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 笑容是有感染力的。 |'
- en: '| LLM-as-signal | delightful smiles and customer service. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| LLM-作为信号 | 令人愉悦的笑容和客户服务。 |'
- en: '| Target text | there were plenty of smiles and cus- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 目标文本 | 有很多笑容和客户- |'
- en: '|  | tomer service. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | 服务。 |'
- en: 'Table 5: Generated outputs of different methods on Yelp-clean (N$\rightarrow$
    P). The improper transferring is annotated with red color.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同方法在 Yelp-clean (N$\rightarrow$ P) 上生成的输出。 不适当的转换用红色标注。
- en: In Table [5](#S9.T5 "Table 5 ‣ 9.5 Case Study ‣ 9 Results and Analysis ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions"),
    we examine the outputs generated by different methods. AM model is able to generate
    a sentence that is close to the target sentence. LLM-based approach generates
    a sentence that conveys a slightly negative sentiment, which fails to produce
    a positive sentence. AM-then-prompt method is able to partially change the sentiment
    but the unexpected revision by LLMs leads to a token “exceptional”, which cannot
    match the original semantic meaning. LLM-as-signal largely changes the syntax
    of the sentence, leading to an undesirable prediction. Unexpectedly, AM-as-demo
    dramatically change the source sentence. This could happen when the demonstration
    for in-context learning misguides the semantic transfer to the LLMs. In comparison,
    Prompt-then-AM is more controllable in the perspective of preserving the semantics
    and transferring the polarity with rational edits.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [5](#S9.T5 "Table 5 ‣ 9.5 Case Study ‣ 9 Results and Analysis ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions")中，我们检查了不同方法生成的输出。AM
    模型能够生成一个接近目标句子的句子。基于 LLM 的方法生成的句子传达了略微负面的情感，这未能产生积极的句子。AM-then-prompt 方法能够部分改变情感，但
    LLMs 的意外修订导致了一个“exceptional”标记，这与原始语义意义不符。LLM-as-signal 在很大程度上改变了句子的语法，导致了不理想的预测。意外地，AM-as-demo
    显著改变了源句子。这可能发生在上下文学习的演示误导了 LLMs 的语义转换时。相比之下，Prompt-then-AM 在保留语义和通过合理的编辑转移极性方面更具可控性。
- en: 10 Conclusion
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 结论
- en: 'In this paper, we investigate multi-way interaction of LLMs and attention masking
    for solving UTST tasks. Specifically, we consider: pipeline framework with tuned
    orders; knowledge distillation from LLMs to attention masking model; in-context
    learning with constructed parallel examples. We show UTST systems with combining
    LLMs and attention masking in four ways can improve the baselines on different
    evaluation metrics like style strength, content preservation, and text fluency.
    We further show that simply conduct prompting and attention masking-based revision
    can consistently surpasses the other interactions and achieve SOTA results in
    Yelp-clean and Amazon-clean datasets even compared with supervised text style
    transfer systems.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了 LLMs 和注意力掩码的多向交互以解决 UTST（无监督文本风格转换）任务。具体来说，我们考虑了：调整顺序的管道框架；从 LLMs
    到注意力掩码模型的知识蒸馏；通过构造的平行示例进行上下文学习。我们展示了结合 LLMs 和注意力掩码的 UTST 系统在风格强度、内容保留和文本流畅性等不同评估指标上能够改进基准。我们进一步展示了，仅通过提示和基于注意力掩码的修订可以
    consistently 超过其他交互，并在 Yelp-clean 和 Amazon-clean 数据集上取得 SOTA（最先进）结果，即便与监督文本风格转换系统相比也是如此。
- en: Limitations
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: LLMs have hallucinations. We have noticed that LLMs sometimes get confused by
    the instructions in a prompt and sometimes generate content that is completely
    unrelated to the input. They mix up the text that needs to be transformed with
    the instructions themselves, especially during In-Context Learning methods. This
    happens more when the prompts are too long and it is hard for the model to differentiate
    between instructions and the actual content. In the future, we will dive into
    the research of easing hallucination issue in text style transfer and make more
    controllable text style transfer with LLMs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大规模语言模型）存在幻觉现象。我们注意到，LLMs 有时会对提示中的指令感到困惑，有时生成的内容与输入完全无关。它们将需要转换的文本与指令本身混淆，尤其是在上下文学习方法期间。当提示过长时，这种情况更为严重，因为模型很难区分指令和实际内容。未来，我们将深入研究缓解文本风格转换中的幻觉问题，并使
    LLMs 实现更可控的文本风格转换。
- en: Ethics Statement
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: In our study on Unsupervised Text Style Transfer, no personal information was
    collected, ensuring the ethical integrity of our research. We emphasize that this
    endeavor involved no risk, as participants were neither exposed to harmful materials
    nor engaged in hazardous activities. Our commitment to ethical standards is unwavering.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对无监督文本风格转换的研究中，没有收集个人信息，确保了我们研究的伦理完整性。我们强调，这项工作没有风险，因为参与者既没有接触有害材料，也没有从事危险活动。我们对伦理标准的承诺坚定不移。
- en: References
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Dai et al. (2019) Ning Dai, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. 2019.
    Style transformer: Unpaired text style transfer without disentangled latent representation.
    *arXiv preprint arXiv:1905.05621*.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2019）戴宁、梁建泽、邱希鹏和黄宣静。2019年。风格变换器：没有解耦潜在表示的未配对文本风格转换。*arXiv 预印本 arXiv:1905.05621*。
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with
    autoregressive blank infilling. In *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    320–335.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等（2022）郑晓·杜、郁杰·钱、肖·刘、明·丁、杰中·邱、志林·杨和杰·唐。2022年。《Glm：具有自回归空白填充的通用语言模型预训练》。收录于*第60届计算语言学协会年会（第一卷：长篇论文）*，第320–335页。
- en: 'Fu et al. (2018) Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui
    Yan. 2018. Style transfer in text: Exploration and evaluation. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 32.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 傅等（2018）振欣·傅、晓叶·谭、南云·彭、东岩·赵和瑞燕。2018年。《文本风格转换：探索与评估》。收录于*AAAI人工智能会议论文集*，第32卷。
- en: Goyal et al. (2020) Navita Goyal, Balaji Vasan Srinivasan, Anandhavelu Natarajan,
    and Abhilasha Sancheti. 2020. Multi-style transfer with discriminative feedback
    on disjoint corpus. *arXiv preprint arXiv:2010.11578*.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 戈亚尔等（2020）纳维塔·戈亚尔、巴拉吉·瓦桑·斯里尼瓦桑、安南达维鲁·纳塔拉詹和阿比拉莎·桑切蒂。2020年。《多风格转换与不相交语料库上的区分性反馈》。*arXiv预印本
    arXiv:2010.11578*。
- en: 'Hu et al. (2022) Chengming Hu, Xuan Li, Dan Liu, Xi Chen, Ju Wang, and Xue
    Liu. 2022. Teacher-student architecture for knowledge learning: A survey. *arXiv
    preprint arXiv:2210.17332*.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等（2022）成名·胡、轩·李、丹·刘、喜·陈、巨·王和薛·刘。2022年。《知识学习的师生架构：一项综述》。*arXiv预印本 arXiv:2210.17332*。
- en: Hu et al. (2017) Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov,
    and Eric P Xing. 2017. Toward controlled generation of text. In *International
    conference on machine learning*, pages 1587–1596\. PMLR.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等（2017）智婷·胡、自超·杨、晓丹·梁、鲁斯兰·萨拉赫丁诺夫和埃里克·P·邢。2017年。《朝向受控的文本生成》。收录于*国际机器学习会议*，第1587–1596页。PMLR。
- en: John et al. (2018) Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova.
    2018. Disentangled representation learning for non-parallel text style transfer.
    *arXiv preprint arXiv:1808.04339*.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约翰等（2018）维尼特·约翰、莉莉·穆、哈里什·巴胡莱扬和奥尔加·韦赫托莫娃。2018年。《用于非平行文本风格转换的解耦表示学习》。*arXiv预印本
    arXiv:1808.04339*。
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114*.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金马和韦林（2013）迪德里克·P·金马和马克斯·韦林。2013年。《自编码变分贝叶斯》。*arXiv预印本 arXiv:1312.6114*。
- en: 'Lewis (2022) Armanda Lewis. 2022. [Multimodal large language models for inclusive
    collaboration learning tasks](https://doi.org/10.18653/v1/2022.naacl-srw.26).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies: Student Research Workshop*,
    pages 202–210, Hybrid: Seattle, Washington + Online. Association for Computational
    Linguistics.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘易斯（2022）阿曼达·刘易斯。2022年。[多模态大型语言模型用于包容性协作学习任务](https://doi.org/10.18653/v1/2022.naacl-srw.26)。收录于*2022年北美计算语言学协会年会：人类语言技术：学生研究研讨会会议录*，第202–210页，混合形式：华盛顿州西雅图
    + 在线。计算语言学协会。
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461*.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘易斯等（2019）迈克·刘易斯、尹汉·刘、纳曼·戈亚尔、马尔扬·加兹维尼贾德、阿卜杜勒拉赫曼·穆罕默德、奥梅尔·列维、韦斯·斯托亚诺夫和卢克·泽特尔莫耶。2019年。《Bart：用于自然语言生成、翻译和理解的去噪序列到序列预训练》。*arXiv预印本
    arXiv:1910.13461*。
- en: 'Li et al. (2018) Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete,
    retrieve, generate: a simple approach to sentiment and style transfer. *arXiv
    preprint arXiv:1804.06437*.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2018）俊岑·李、罗宾·贾、何赫和珀西·梁。2018年。《删除、检索、生成：一种简单的情感和风格转换方法》。*arXiv预印本 arXiv:1804.06437*。
- en: 'Li et al. (2020) Xiao Li, Guanyi Chen, Chenghua Lin, and Ruizhe Li. 2020. Dgst:
    a dual-generator network for text style transfer. *arXiv preprint arXiv:2010.14557*.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2020）肖·李、关艺·陈、成华·林和瑞哲·李。2020年。《Dgst：一种用于文本风格转换的双生成器网络》。*arXiv预印本 arXiv:2010.14557*。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2019）尹汉·刘、迈尔·奥特、纳曼·戈亚尔、静飞·杜、曼达尔·乔希、丹奇·陈、奥梅尔·列维、迈克·刘易斯、卢克·泽特尔莫耶和韦塞林·斯托亚诺夫。2019年。《Roberta：一种稳健优化的BERT预训练方法》。*arXiv预印本
    arXiv:1907.11692*。
- en: Luo et al. (2019) Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang,
    Zhifang Sui, and Xu Sun. 2019. A dual reinforcement learning framework for unsupervised
    text style transfer. *arXiv preprint arXiv:1905.10060*.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2019) Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang,
    Zhifang Sui, and Xu Sun. 2019. 用于无监督文本风格迁移的双重强化学习框架。*arXiv 预印本 arXiv:1905.10060*。
- en: Luo et al. (2023) Guoqing Luo, Yu Tong Han, Lili Mou, and Mauajama Firdaus.
    2023. Prompt-based editing for text style transfer. *arXiv preprint arXiv:2301.11997*.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2023) Guoqing Luo, Yu Tong Han, Lili Mou, and Mauajama Firdaus.
    2023. 基于提示的文本风格迁移编辑。*arXiv 预印本 arXiv:2301.11997*。
- en: 'Madaan et al. (2020) Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos,
    Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W Black, and Shrimai Prabhumoye.
    2020. Politeness transfer: A tag and generate approach. *arXiv preprint arXiv:2004.14257*.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan et al. (2020) Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos,
    Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W Black, and Shrimai Prabhumoye.
    2020. 礼貌迁移：标记和生成方法。*arXiv 预印本 arXiv:2004.14257*。
- en: 'Mallinson et al. (2022) Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei
    Severyn. 2022. Edit5: Semi-autoregressive text-editing with t5 warm-start. *arXiv
    preprint arXiv:2205.12209*.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mallinson et al. (2022) Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei
    Severyn. 2022. Edit5: 使用 t5 热启动的半自回归文本编辑。*arXiv 预印本 arXiv:2205.12209*。'
- en: 'Malmi et al. (2022) Eric Malmi, Yue Dong, Jonathan Mallinson, Aleksandr Chuklin,
    Jakub Adamek, Daniil Mirylenka, Felix Stahlberg, Sebastian Krause, Shankar Kumar,
    and Aliaksei Severyn. 2022. [Text generation with text-editing models](https://doi.org/10.18653/v1/2022.naacl-tutorials.1).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies: Tutorial Abstracts*,
    pages 1–7, Seattle, United States. Association for Computational Linguistics.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malmi et al. (2022) Eric Malmi, Yue Dong, Jonathan Mallinson, Aleksandr Chuklin,
    Jakub Adamek, Daniil Mirylenka, Felix Stahlberg, Sebastian Krause, Shankar Kumar,
    and Aliaksei Severyn. 2022. [文本生成与文本编辑模型](https://doi.org/10.18653/v1/2022.naacl-tutorials.1)。在
    *2022年北美计算语言学协会年会：人类语言技术：教程摘要*，第1–7页，西雅图，美国。计算语言学协会。
- en: Narasimhan et al. (2023) Sharan Narasimhan, H Pooja, Suvodip Dey, and Maunendra Sankar
    Desarkar. 2023. On text style transfer via style-aware masked language models.
    In *Proceedings of the 16th International Natural Language Generation Conference*,
    pages 362–374.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narasimhan et al. (2023) Sharan Narasimhan, H Pooja, Suvodip Dey, and Maunendra
    Sankar Desarkar. 2023. 通过风格感知的掩码语言模型进行文本风格迁移。在 *第16届国际自然语言生成会议论文集*，第362–374页。
- en: Post (2018) Matt Post. 2018. A call for clarity in reporting bleu scores. *arXiv
    preprint arXiv:1804.08771*.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Post (2018) Matt Post. 2018. 对蓝色评分报告的明确性呼吁。*arXiv 预印本 arXiv:1804.08771*。
- en: Prabhumoye et al. (2018) Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov,
    and Alan W Black. 2018. Style transfer through back-translation. *arXiv preprint
    arXiv:1804.09000*.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prabhumoye et al. (2018) Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov,
    and Alan W Black. 2018. 通过回译进行风格迁移。*arXiv 预印本 arXiv:1804.09000*。
- en: 'Rao and Tetreault (2018) Sudha Rao and Joel Tetreault. 2018. Dear sir or madam,
    may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality
    style transfer. *arXiv preprint arXiv:1803.06535*.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rao and Tetreault (2018) Sudha Rao and Joel Tetreault. 2018. 亲爱的先生或女士，我来介绍一下
    gyafc 数据集：正式风格迁移的语料库、基准和指标。*arXiv 预印本 arXiv:1803.06535*。
- en: 'Reid and Zhong (2021) Machel Reid and Victor Zhong. 2021. Lewis: Levenshtein
    editing for unsupervised text style transfer. *arXiv preprint arXiv:2105.08206*.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid and Zhong (2021) Machel Reid and Victor Zhong. 2021. Lewis: 用于无监督文本风格迁移的
    Levenshtein 编辑。*arXiv 预印本 arXiv:2105.08206*。'
- en: Reif et al. (2021) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris
    Callison-Burch, and Jason Wei. 2021. A recipe for arbitrary text style transfer
    with large language models. *arXiv preprint arXiv:2109.03910*.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reif et al. (2021) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris
    Callison-Burch, and Jason Wei. 2021. 使用大型语言模型进行任意文本风格迁移的配方。*arXiv 预印本 arXiv:2109.03910*。
- en: 'Reif et al. (2022) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris
    Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer
    with large language models. In *Proceedings of the 60th Annual Meeting of the
    Association for Computational Linguistics (Volume 2: Short Papers)*, pages 837–848.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reif et al. (2022) Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris
    Callison-Burch, and Jason Wei. 2022. 使用大型语言模型进行任意文本风格迁移的配方。在 *第60届计算语言学协会年会（第2卷：短论文）论文集*，第837–848页。
- en: 'Rubin et al. (2022) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.
    Learning to retrieve prompts for in-context learning. In *Proceedings of the 2022
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 2655–2671, Seattle, United States.
    Association for Computational Linguistics.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin 等（2022）Ohad Rubin、Jonathan Herzig 和 Jonathan Berant。2022。学习从上下文学习中检索提示。在
    *2022年北美计算语言学协会：人类语言技术会议论文集*，第2655–2671页，美国西雅图。计算语言学协会。
- en: Ruis et al. (2022) Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim
    Rocktäschel, and Edward Grefenstette. 2022. Large language models are not zero-shot
    communicators. *arXiv preprint arXiv:2210.14986*.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruis 等（2022）Laura Ruis、Akbir Khan、Stella Biderman、Sara Hooker、Tim Rocktäschel
    和 Edward Grefenstette。2022。大型语言模型不是零样本交流者。*arXiv 预印本 arXiv:2210.14986*。
- en: 'Shang et al. (2019) Mingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing, Dongyan
    Zhao, Shuming Shi, and Rui Yan. 2019. Semi-supervised text style transfer: Cross
    projection in latent space. *arXiv preprint arXiv:1909.11493*.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang 等（2019）Mingyue Shang、Piji Li、Zhenxin Fu、Lidong Bing、Dongyan Zhao、Shuming
    Shi 和 Rui Yan。2019。半监督文本风格转移：潜在空间中的交叉投影。*arXiv 预印本 arXiv:1909.11493*。
- en: Shen et al. (2017) Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola.
    2017. Style transfer from non-parallel text by cross-alignment. *Advances in neural
    information processing systems*, 30.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等（2017）Tianxiao Shen、Tao Lei、Regina Barzilay 和 Tommi Jaakkola。2017。从非平行文本中通过交叉对齐进行风格转移。*神经信息处理系统进展*，30。
- en: Su et al. (2022) Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu
    Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al.
    2022. Selective annotation makes language models better few-shot learners. *arXiv
    pr arXiv:2209.01975*.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等（2022）Hongjin Su、Jungo Kasai、Chen Henry Wu、Weijia Shi、Tianlu Wang、Jiayi
    Xin、Rui Zhang、Mari Ostendorf、Luke Zettlemoyer、Noah A Smith 等。2022。选择性注释使语言模型成为更好的少样本学习者。*arXiv
    预印本 arXiv:2209.01975*。
- en: Sudhakar et al. (2019) Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Maheswaran.
    2019. Transforming delete, retrieve, generate approach for controlled text style
    transfer. *arXiv preprint arXiv:1908.09368*.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sudhakar 等（2019）Akhilesh Sudhakar、Bhargav Upadhyay 和 Arjun Maheswaran。2019。用于受控文本风格转移的删除、检索、生成方法的转换。*arXiv
    预印本 arXiv:1908.09368*。
- en: 'Suzgun et al. (2022a) Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022a.
    Prompt-and-rerank: A method for zero-shot and few-shot arbitrary textual style
    transfer with small language models. *arXiv preprint arXiv:2205.11503*.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzgun 等（2022a）Mirac Suzgun、Luke Melas-Kyriazi 和 Dan Jurafsky。2022a。Prompt-and-rerank：一种用于小型语言模型的零样本和少样本任意文本风格转移的方法。*arXiv
    预印本 arXiv:2205.11503*。
- en: 'Suzgun et al. (2022b) Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2022b.
    [Prompt-and-rerank: A method for zero-shot and few-shot arbitrary textual style
    transfer with small language models](https://doi.org/10.18653/v1/2022.emnlp-main.141).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 2195–2222, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzgun 等（2022b）Mirac Suzgun、Luke Melas-Kyriazi 和 Dan Jurafsky。2022b。[Prompt-and-rerank：一种用于小型语言模型的零样本和少样本任意文本风格转移的方法](https://doi.org/10.18653/v1/2022.emnlp-main.141)。在
    *2022年自然语言处理经验方法会议论文集*，第2195–2222页，阿布扎比，阿联酋。计算语言学协会。
- en: 'Tao et al. (2024) Zhen Tao, Dinghao Xi, Zhiyu Li, Liumin Tang, and Wei Xu.
    2024. Cat-llm: Prompting large language models with text style definition for
    chinese article-style transfer. *arXiv preprint arXiv:2401.05707*.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao 等（2024）Zhen Tao、Dinghao Xi、Zhiyu Li、Liumin Tang 和 Wei Xu。2024。Cat-llm：通过文本风格定义提示大型语言模型以进行中文文章风格转移。*arXiv
    预印本 arXiv:2401.05707*。
- en: 'Tian et al. (2023) Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, and Yunshi
    Lan. 2023. R ³ prompting: Review, rephrase and resolve for chain-of-thought reasoning
    in large language models under noisy context. *arXiv preprint arXiv:2310.16535*.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等（2023）Qingyuan Tian、Hanlun Zhu、Lei Wang、Yang Li 和 Yunshi Lan。2023。R³ 提示：在嘈杂上下文下，大型语言模型中的链式思维推理的回顾、重述和解决。*arXiv
    预印本 arXiv:2310.16535*。
- en: Vincent et al. (2008) Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine
    Manzagol. 2008. Extracting and composing robust features with denoising autoencoders.
    In *Proceedings of the 25th international conference on Machine learning*, pages
    1096–1103.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent 等（2008）Pascal Vincent、Hugo Larochelle、Yoshua Bengio 和 Pierre-Antoine
    Manzagol。2008。使用去噪自编码器提取和组合鲁棒特征。在 *第25届国际机器学习会议论文集*，第1096–1103页。
- en: Wang et al. (2022a) Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, and
    Yongyi Mao. 2022a. Text style transferring via adversarial masking and styled
    filling. In *Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing*, pages 7654–7663.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, 和 Yongyi
    Mao. 2022a. 通过对抗掩码和风格填充进行文本风格转换。在*2022年自然语言处理经验方法会议论文集*，第7654-7663页。
- en: Wang et al. (2022b) Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, and
    Yongyi Mao. 2022b. Text style transferring via adversarial masking and styled
    filling. In *Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing*, pages 7654–7663.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022b) Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, 和 Yongyi
    Mao. 2022b. 通过对抗掩码和风格填充进行文本风格转换。在*2022年自然语言处理经验方法会议论文集*，第7654-7663页。
- en: 'Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof.
    2023. C-pack: Packaged resources to advance general chinese embedding. *arXiv
    preprint arXiv:2309.07597*.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, 和 Niklas Muennighof.
    2023. C-pack：推进通用中文嵌入的打包资源。*arXiv预印本 arXiv:2309.07597*。
- en: 'Xu et al. (2018) Jingjing Xu, Xu Sun, Qi Zeng, Xuancheng Ren, Xiaodong Zhang,
    Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation:
    A cycled reinforcement learning approach. *arXiv preprint arXiv:1805.05181*.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2018) Jingjing Xu, Xu Sun, Qi Zeng, Xuancheng Ren, Xiaodong Zhang,
    Houfeng Wang, 和 Wenjie Li. 2018. 无配对情感到情感的翻译：一种循环强化学习方法。*arXiv预印本 arXiv:1805.05181*。
- en: 'Xuanfan and Piji (2023) Ni Xuanfan and Li Piji. 2023. A systematic evaluation
    of large language models for natural. In *Proceedings of the 22nd Chinese National
    Conference on Computational Linguistics (Volume 2: Frontier Forum)*, pages 40–56.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xuanfan 和 Piji (2023) Ni Xuanfan 和 Li Piji. 2023. 大型语言模型在自然语言处理中的系统评估。在*第22届中国计算语言学大会论文集（第2卷：前沿论坛）*，第40-56页。
- en: Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level
    convolutional networks for text classification. *Advances in neural information
    processing systems*, 28.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2015) Xiang Zhang, Junbo Zhao, 和 Yann LeCun. 2015. 面向文本分类的字符级卷积网络。*神经信息处理系统进展*，28。
- en: Zhang et al. (2020) Yi Zhang, Tao Ge, and Xu Sun. 2020. Parallel data augmentation
    for formality style transfer. *arXiv preprint arXiv:2005.07522*.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) Yi Zhang, Tao Ge, 和 Xu Sun. 2020. 形式风格转移的并行数据增强。*arXiv预印本
    arXiv:2005.07522*。
- en: Appendix A Appendix
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: 'For training models, we use 2 NVIDIA V100 GPUs on a machine running Ubuntu
    20.04 on a 10-Core Intel CPU. We choose RoBERTa-base as the baseline model for
    the classifier and trained on three datasets: Yelp, Amazon, and Politeness, respectively.
    For each training session, we set the batch size to 64, the number of epochs to
    10, and the learning rate to 1e-5\. We choose BART-base as our filling model and
    trained on different datasets. To prepare the training data for BART on the Yelp
    dataset, we set the mask predictor’s $\alpha$ to 0.5\. For the Politeness dataset,
    we set the mask predictor’s $\alpha$ to 0.35. the detailed implementation inforamtion
    can be found in Table [6](#A1.T6 "Table 6 ‣ Appendix A Appendix ‣ Unsupervised
    Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions").'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型训练，我们使用2个NVIDIA V100 GPU，在运行Ubuntu 20.04的10核Intel CPU机器上。我们选择RoBERTa-base作为分类器的基准模型，并在三个数据集上进行训练：Yelp、Amazon和Politeness。对于每次训练，我们将批量大小设置为64，训练轮数设置为10，学习率设置为1e-5。我们选择BART-base作为填充模型，并在不同的数据集上进行训练。为了准备BART在Yelp数据集上的训练数据，我们将掩码预测器的$\alpha$设置为0.5。对于Politeness数据集，我们将掩码预测器的$\alpha$设置为0.35。详细的实施信息可以在表格[6](#A1.T6
    "Table 6 ‣ Appendix A Appendix ‣ Unsupervised Text Style Transfer via LLMs and
    Attention Masking with Multi-way Interactions")中找到。
- en: '| Dataset | Parameters | Settings |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 参数 | 设置 |'
- en: '| RoBERTa-base | Batch size | 64 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa-base | 批量大小 | 64 |'
- en: '|  | Epochs | 20 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | 训练轮数 | 20 |'
- en: '|  | Learning rate | 1e-5 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | 学习率 | 1e-5 |'
- en: '|  | $\alpha$ | 0.5 for Yelp and Amazon |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha$ | Yelp和Amazon为0.5 |'
- en: '|  |  | 0.35 for Politeness |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Politeness为0.35 |'
- en: '| BART-base | Batch size | 16 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| BART-base | 批量大小 | 16 |'
- en: '|  | Epochs | 10 for Yelp and Amazon |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | 训练轮数 | Yelp和Amazon为10 |'
- en: '|  |  | 5 for Politeness |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Politeness为5 |'
- en: '| ChatGLM2-6B | Temperature | 0 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B | 温度 | 0 |'
- en: 'Table 6: Implementation details.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：实施细节。
