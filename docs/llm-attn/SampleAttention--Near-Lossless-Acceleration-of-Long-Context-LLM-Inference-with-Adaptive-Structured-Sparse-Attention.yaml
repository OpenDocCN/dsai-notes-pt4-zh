- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SampleAttention：通过自适应结构化稀疏注意力加速长上下文LLM推理的近乎无损方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15486](https://ar5iv.labs.arxiv.org/html/2406.15486)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15486](https://ar5iv.labs.arxiv.org/html/2406.15486)
- en: Qianchao Zhu^†, Jiangfei Duan^‡, Chang Chen^†, Siran Liu^†, Xiuhong Li^†, Guanyu
    Feng^§
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 朱乾超^†、段江飞^‡、陈畅^†、刘思然^†、李秀宏^†、冯观宇^§
- en: Xin Lv^§, Huanqi Cao^∪, Chuanfu Xiao^†, Xingcheng Zhang^∩, Dahua Lin^(‡∩), Chao
    Yang^†
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 辛露^§、曹欢奇^∪、肖传福^†、张兴成^∩、林大华^(‡∩)、杨超^†
- en: ^†Peking University  ^‡The Chinese University of Hong Kong
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^†北京大学  ^‡香港中文大学
- en: ^§Zhipu.AI  ^∪Tsinghua University  ^∩Shanghai AI Lab
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^§智谱.AI  ^∪清华大学  ^∩上海人工智能实验室
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) now support extremely long context windows, but
    the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token
    (TTFT) latency. Existing approaches to address this complexity require additional
    pretraining or finetuning, and often sacrifice model accuracy. In this paper,
    we first provide both theoretical and empirical foundations for near-lossless
    sparse attention. We find dynamically capturing head-specific sparse patterns
    at runtime with low overhead is crucial. To address this, we propose SampleAttention,
    an adaptive structured and near-lossless sparse attention. Leveraging observed
    significant sparse patterns, SampleAttention attends to a fixed percentage of
    adjacent tokens to capture local window patterns, and employs a two-stage query-guided
    key-value filtering approach, which adaptively select a minimum set of key-values
    with low overhead, to capture column stripe patterns. Comprehensive evaluations
    show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf
    LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$ compared
    with FlashAttention.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）现在支持极长的上下文窗口，但普通注意力机制的二次复杂性导致了显著的首次标记时间（TTFT）延迟。现有的方法为了应对这种复杂性，需要额外的预训练或微调，并且通常会牺牲模型的准确性。本文首先提供了近乎无损的稀疏注意力的理论和实证基础。我们发现，在运行时以低开销动态捕捉头部特定的稀疏模式至关重要。为了解决这个问题，我们提出了SampleAttention，一种自适应结构化且近乎无损的稀疏注意力。SampleAttention利用观察到的显著稀疏模式，关注于固定比例的相邻标记以捕捉局部窗口模式，并采用一种两阶段的查询引导键值过滤方法，该方法自适应地选择一组低开销的键值来捕捉列条纹模式。综合评估显示，SampleAttention可以无缝替代现有的注意力机制，几乎没有准确性损失，并且与FlashAttention相比，将TTFT减少了多达$2.42\times$。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Recent advances [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)] race to scale the context window of large language models (LLMs) [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)] for more complex applications, including document
    analysis [[9](#bib.bib9)], code copilot [[10](#bib.bib10), [11](#bib.bib11)],
    and prolonged conversations [[12](#bib.bib12), [13](#bib.bib13)]. Popular LLMs
    like Gemini [[14](#bib.bib14)], Claude [[15](#bib.bib15)] and Kimi [[16](#bib.bib16)]
    now support context lengths exceeding 1 million tokens. However, the increase
    in context length makes it challenging to support live interactions due to the
    quadratic complexity of attention mechanism. As illustrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ SampleAttention: Near-Lossless Acceleration of Long
    Context LLM Inference with Adaptive Structured Sparse Attention"), the attention
    computation time increases quadratically with sequence length, quickly dominating
    the Time to First Token (TTFT) latency (i.e. prefill latency). For example, in
    a 1 million token context, the attention of ChatGLM-6B [[17](#bib.bib17)] takes
    $1555$ seconds, constituting over 90% of the TTFT when evaluated on an A100 GPU.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的进展[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]
    竞相扩展大型语言模型（LLMs）的上下文窗口[[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]，以支持更复杂的应用，包括文档分析[[9](#bib.bib9)]、代码助手[[10](#bib.bib10),
    [11](#bib.bib11)]和长期对话[[12](#bib.bib12), [13](#bib.bib13)]。像Gemini[[14](#bib.bib14)]、Claude[[15](#bib.bib15)]和Kimi[[16](#bib.bib16)]这样的流行LLM现在支持超过100万标记的上下文长度。然而，上下文长度的增加使得支持实时交互变得具有挑战性，因为注意力机制的二次复杂性。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ SampleAttention: Near-Lossless Acceleration of Long
    Context LLM Inference with Adaptive Structured Sparse Attention")所示，注意力计算时间随着序列长度的增加呈二次增长，迅速主导首次标记时间（TTFT）延迟（即预填充延迟）。例如，在一个100万标记的上下文中，ChatGLM-6B[[17](#bib.bib17)]的注意力计算耗时$1555$秒，占TTFT的90%以上，当在A100
    GPU上评估时。'
- en: Various solutions have been proposed to address the quadratic complexity of
    attention, but none of them can be seamlessly and practically applied to pretrained
    LLMs without finetuning or pretraining and sacrificing model accuracy. Prior approaches
    explore to approximate dense attention with static or dynamic sparse attention [[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)], low-rank matrices [[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)], and unified sparse and low-rank attention [[30](#bib.bib30),
    [31](#bib.bib31)]. Recurrent states [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]
    and external memory [[35](#bib.bib35), [36](#bib.bib36)] are also investigated
    to mitigate the complexity. However, these approaches require pretraining from
    scratch or additional finetuning, and cannot achieve the same accuracy of full
    attention. StreamingLLM [[37](#bib.bib37)] offers a tuning-free sparse attention
    for infinite generation scenarios, but it cannot effectively reduce TTFT without
    accuracy loss. Therefore, we ask the question,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 针对注意力机制的二次复杂性，已经提出了各种解决方案，但没有一种能够在不进行微调或重新训练且不牺牲模型准确性的情况下，无缝地实际应用于预训练的LLM。之前的方法探讨了通过静态或动态稀疏注意力来逼近密集注意力[[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]、低秩矩阵[[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)]以及统一的稀疏和低秩注意力[[30](#bib.bib30), [31](#bib.bib31)]。递归状态[[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)]和外部记忆[[35](#bib.bib35), [36](#bib.bib36)]也被研究以缓解复杂性。然而，这些方法需要从头开始预训练或额外的微调，无法实现完全注意力的相同准确性。StreamingLLM[[37](#bib.bib37)]提供了一种免调优的稀疏注意力，用于无限生成场景，但它无法有效减少TTFT而不损失准确性。因此，我们提出了一个问题，
- en: '![Refer to caption](img/9b92eaacd2ebefcc5134639c7bc7a79a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b92eaacd2ebefcc5134639c7bc7a79a.png)'
- en: 'Figure 1: Comparison of sparse attention pattern and TTFT latency speedup.
    SampleAttention features adaptive structured sparse, compared with previous static
    and dynamic sparse attention. It achieves significant reduction in TTFT compared
    with FlashAttention.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：稀疏注意力模式与TTFT延迟加速的比较。SampleAttention具有自适应结构稀疏性，与之前的静态和动态稀疏注意力相比，显著减少了TTFT，相较于FlashAttention。
- en: How can we reduce the TTFT for off-the-shelf long context LLMs with near-lossless¹¹1Near-lossless
    refers to that model accuracy stays above $99\%$ of the baseline according to
    MLPerf[[38](#bib.bib38)]. model accuracy?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何减少现成长上下文LLM的TTFT，同时保持近乎无损¹¹¹近乎无损指的是模型准确度保持在基准值的$99\%$以上，依据MLPerf[[38](#bib.bib38)]。的模型准确性？
- en: 'In this paper, we first provide both theoretical and empirical foundations
    for near-lossless sparse attention. We find that the sparsity of intermediate
    score matrix in long-context attention is inherently-high, head-specific, and
    content-aware. Specifically, for a given long context prompt, some attention heads
    focus on only $0.2\%$ of the tokens, while others may need to attend to over half.
    From the dynamic sparse patterns, we also demonstrate some inherent local window
    and column stripe patterns as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"). Except for adjacent tokens in the local
    window, some dynamic column stripes appear to be critical for near-lossless attention.
    This flexible sparsity indicates that sparse attention should dynamically capture
    the head-specific sparse patterns at runtime to be near-lossless. However, adaptive
    selection of essential elements involves significant overhead. The trade-off between
    efficiency and accuracy is a permanent topic in sparse attention design.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首先提供了近乎无损稀疏注意力的理论和实证基础。我们发现，长上下文注意力中间分数矩阵的稀疏性本质上很高、特定头部且内容感知。具体来说，对于给定的长上下文提示，一些注意力头仅关注$0.2\%$的标记，而其他的可能需要关注超过一半。从动态稀疏模式中，我们还展示了一些固有的局部窗口和列条纹模式，如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理的近乎无损")所示。除了局部窗口中的相邻标记外，一些动态列条纹对于近乎无损的注意力显得尤为重要。这种灵活的稀疏性表明，稀疏注意力应动态捕捉特定头部的稀疏模式以实现近乎无损。然而，关键元素的自适应选择涉及显著的开销。在稀疏注意力设计中，效率与准确性之间的权衡是一个永恒的话题。
- en: To address these challenges, we propose SampleAttention, an adaptive structured
    sparse attention that can be seamlessly integrated into off-the-shelf long context
    LLMs with near-lossless model accuracy. SampleAttention leverages the significant
    window and stripe sparse patterns, thus achieves structured sparse and is hardware-efficient.
    To resolve the adaptive sparsity, SampleAttention attends to a fixed percentage
    of adjacent tokens to capture local window patterns, and employs a two-stage query-guided
    key-value filtering approach, which adaptively select a minimum set of key-values
    with low overhead, to focus on column stripe patterns. SampleAttention significantly
    accelerates vanilla attention by reducing both I/O and computation requirements.
    We also implement hardware-efficient kernels. Notably, SampleAttention aims to
    reduce the computation overhead of attention, and is orthogonal and can be combined
    with existing KV cache eviction approaches [[39](#bib.bib39), [40](#bib.bib40),
    [41](#bib.bib41)] to further reduce memory consumption.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，我们提出了SampleAttention，一种自适应结构化稀疏注意力机制，它可以无缝集成到现成的长上下文LLM中，且几乎不损失模型准确性。SampleAttention利用显著的窗口和条纹稀疏模式，从而实现结构化稀疏，并且硬件高效。为了解决自适应稀疏性，SampleAttention关注固定比例的相邻标记，以捕捉局部窗口模式，并采用两阶段查询引导的键值过滤方法，该方法自适应地选择最小的一组低开销键值，以关注列条纹模式。SampleAttention通过减少I/O和计算需求显著加速了原始注意力。我们还实现了硬件高效的内核。值得注意的是，SampleAttention旨在减少注意力的计算开销，并且是正交的，可以与现有的KV缓存驱逐方法[[39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)]结合，以进一步减少内存消耗。
- en: We evaluate SampleAttention on ChatGLM2 and InternLM2 with a suite of popular
    benchmarks covering various generative tasks across different sequence lengths.
    Experimental results show that SampleAttention achieves nearly no accuracy loss
    for different LLMs, significantly outperforming prior works, and reduces the TTFT
    by up to $2.42\times$ compared with FlashAttention.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在ChatGLM2和InternLM2上评估了SampleAttention，使用了一系列涵盖不同序列长度的流行基准测试。实验结果表明，SampleAttention对不同LLM几乎没有准确性损失，显著超越了之前的工作，并且与FlashAttention相比，TTFT减少了多达$2.42\times$。
- en: 2 Related Work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Approximate Attention. Plenty of works have been proposed to approximate quadratic
    attention with lower complexity[[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [42](#bib.bib42), [40](#bib.bib40), [25](#bib.bib25)]. For example,
    BigBird [[20](#bib.bib20)] combines window-, global- and random-attention to capture
    long range dependency. Reformer [[21](#bib.bib21)] reduces computional cost via
    locality-sensitive hashing. LongNet [[22](#bib.bib22)] replaces full attention
    with dilated attention. Linformer [[27](#bib.bib27)] employs low-rank matrix to
    approximate attention. HyperAttention [[26](#bib.bib26)] utilizes locality sensitive
    hashing to identify important entries on attention map. However, these approaches
    uses either static or coarse-grained sparse pattern, and often overlook the head-specific
    sparsity pattern. They cannot be losslessly applied in pretrained LLMs without
    additional finetuning or training.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 近似注意力。许多研究已提出用更低复杂度来近似二次注意力[[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [42](#bib.bib42), [40](#bib.bib40), [25](#bib.bib25)]。例如，BigBird[[20](#bib.bib20)]结合了窗口、全局和随机注意力以捕捉长距离依赖关系。Reformer[[21](#bib.bib21)]通过局部敏感哈希来减少计算成本。LongNet[[22](#bib.bib22)]用扩张注意力替代了全注意力。Linformer[[27](#bib.bib27)]利用低秩矩阵来近似注意力。HyperAttention[[26](#bib.bib26)]利用局部敏感哈希来识别注意力图上的重要条目。然而，这些方法使用的要么是静态的，要么是粗粒度的稀疏模式，并且往往忽视了特定头部的稀疏模式。它们不能在不额外微调或训练的情况下无损地应用于预训练LLM。
- en: KV Cache Compression. Long sequence comes with substantial KV cache memory consumption.
    StreamingLLM [[37](#bib.bib37)] keeps attention sinks and several recent tokens
    for infinite length generation. H2O [[39](#bib.bib39)] dynamically retains a balance
    of recent and heavy hitter tokens according to attention score during decoding.
    FastGen [[43](#bib.bib43)] adaptively construct KV cache according to observed
    head-specific policies. Recent efforts also quantize KV cache to lower precision
    to reduce memory consumption [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)].
    These works target on reducing the memory consumption of KV cache, while SampleAttention
    focuses on mitigating the long context computation overhead. SampleAttention can
    be combined with these approaches to further reduce memory consumption of KV cache.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: KV 缓存压缩。长序列带来了大量的KV缓存内存消耗。StreamingLLM [[37](#bib.bib37)] 保留注意力沉没和几个最近的令牌以进行无限长度生成。H2O [[39](#bib.bib39)]
    根据解码期间的注意力得分动态保留最近的和重击令牌的平衡。FastGen [[43](#bib.bib43)] 根据观察到的头特定策略自适应构建KV缓存。最近的工作还将KV缓存量化为较低精度以减少内存消耗 [[44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46)]。这些工作致力于减少KV缓存的内存消耗，而SampleAttention则专注于减轻长上下文计算开销。SampleAttention可以与这些方法结合，以进一步减少KV缓存的内存消耗。
- en: 3 Foundation of Near-Lossless Sparse Attention
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 接近无损稀疏注意力的基础
- en: We start with a regular full attention mechanism for one attention head, while
    the following contents can be seamlessly applied to multiple attention heads.
    Let $\textbf{Q}\in\mathbb{R}^{S_{q}\times d}$ be the query and key-value tensor
    of one head, where $S_{q},S_{k}$ is the head dimension. The full attention output
    $\textbf{O}\in\mathbb{R}^{S_{q}\times d}$ can be formulated as,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个常规的全注意力机制开始，针对一个注意力头，以下内容可以无缝地应用于多个注意力头。设$\textbf{Q}\in\mathbb{R}^{S_{q}\times
    d}$为一个头的查询和键值张量，其中$S_{q},S_{k}$是头维度。全注意力输出$\textbf{O}\in\mathbb{R}^{S_{q}\times
    d}$可以表示为，
- en: '|  | $\textbf{P}=\texttt{softmax}(\frac{\textbf{QK}^{T}}{\sqrt{d}})\in[0,1]^{S_{q}\times
    S_{k}},\quad\textbf{O}=\textbf{PV}\in\mathbb{R}^{S_{q}\times d},$ |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{P}=\texttt{softmax}(\frac{\textbf{QK}^{T}}{\sqrt{d}})\in[0,1]^{S_{q}\times
    S_{k}},\quad\textbf{O}=\textbf{PV}\in\mathbb{R}^{S_{q}\times d},$ |  | (1) |'
- en: where softmax is applied in row-wise, and P is the attention score. We find,
    in long context LLMs, the attention score matrix P becomes extremely large, leading
    to inefficiencies. Moreover, applying softmax over long sequences tends to reduce
    the influence of smaller elements, making them less significant. This insight
    motivates us to investigate the inherent sparsity in the attention scores, which
    can potentially accelerate the attention mechanism without compromising accuracy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 软最大（softmax）被应用于按行处理，而P是注意力得分。我们发现，在长上下文的LLMs中，注意力得分矩阵P变得非常大，导致效率低下。此外，对长序列应用软最大化往往会降低较小元素的影响，使它们不那么重要。这一见解促使我们研究注意力得分中的固有稀疏性，这可能在不影响准确性的情况下加速注意力机制。
- en: 3.1 Theoretical Foundation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 理论基础
- en: We first present a theoretical foundation to explore the attention score sparsity.
    Suppose we apply an attention mask $\textbf{M}\in\{0,1\}^{S_{q}\times S_{k}}$
    can be formulated as,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提出一个理论基础来探讨注意力得分的稀疏性。假设我们应用一个注意力掩码$\textbf{M}\in\{0,1\}^{S_{q}\times S_{k}}$，其形式为，
- en: '|  | $\tilde{\textbf{P}}=\textbf{M}*\textbf{P}\in[0,1]^{S_{q}\times S_{k}},\quad\tilde{\textbf{O}}=\tilde{\textbf{P}}\textbf{V}\in\mathbb{R}^{S_{q}\times
    d},$ |  | (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\textbf{P}}=\textbf{M}*\textbf{P}\in[0,1]^{S_{q}\times S_{k}},\quad\tilde{\textbf{O}}=\tilde{\textbf{P}}\textbf{V}\in\mathbb{R}^{S_{q}\times
    d},$ |  | (2) |'
- en: where $*$ represents the element-wise product. We give a theorem for near-lossless
    sparse attention.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$*$表示逐元素乘积。我们给出一个接近无损稀疏注意力的定理。
- en: Theorem 1.
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1.
- en: '(near-lossless sparse attention) Assume that $L_{1}$. Given $\epsilon>,
    and the following holds: .
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 。
- en: 'Lemma [1](#Thmlemma1 "Lemma 1\. ‣ 3.1 Theoretical Foundation ‣ 3 Foundation
    of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") can
    be easily proved since $||\tilde{\textbf{P}}-\textbf{P}||_{1}=1-\textbf{CRA}(\textbf{M})$.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 引理 [1](#Thmlemma1 "引理 1\. ‣ 3.1 理论基础 ‣ 3 近乎无损稀疏注意力基础 ‣ SampleAttention：通过自适应结构稀疏注意力加速长上下文LLM推理")
    可以很容易证明，因为 $||\tilde{\textbf{P}}-\textbf{P}||_{1}=1-\textbf{CRA}(\textbf{M})$。
- en: 'Takeaway: By discovering an effective attention mask M that meets a desired
    CRA threshold $\alpha$) brings greater acceleration.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结论：通过发现一个有效的注意力掩码 M，使其满足所需的CRA阈值 $\alpha$，可以带来更大的加速。
- en: '![Refer to caption](img/a48dddd8b02b99e54e295f725bb7a652.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a48dddd8b02b99e54e295f725bb7a652.png)'
- en: 'Figure 2: Statistics of ChatGLM-6B (Model1, 28 layers$\times$32 heads), evaluated
    over different tasks at the prefill stage. (a) The trend of SD($\alpha$=0.95)
    as the sequence length extends in the "Needle in a Haystack" task, indicating
    that an increase in sequence length intensifies the sparsity. (c) The variation
    of SD($\alpha$=0.95) across different heads under a 90K sequence, indicating significant
    disparities in sparsity among the heads. (d) Different contexts cause the same
    head to display varied sparse structures, while numerous attention heads follow
    two primary patterns: column stripe and local window. (e) Relationship between
    the ratio of selected top-k strips and CRA. The high row-wise numerical distribution
    similarity enables a small amount of critical column stripes to cover the majority
    values of the full attention score matrix. Further details are presented in Appendix [A.3](#A1.SS3
    "A.3 Visualization of attention ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"),
    [A.4](#A1.SS4 "A.4 Sparisty analysis ‣ Appendix A Appendix ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：ChatGLM-6B（Model1，28层$\times$32头）的统计数据，在预填充阶段对不同任务进行评估。(a) 在“针在稻草堆中”任务中，随着序列长度的延长，SD($\alpha$=0.95)的趋势，表明序列长度的增加会加剧稀疏性。(c)
    在90K序列下，不同头的SD($\alpha$=0.95)的变化，显示出头部之间稀疏性的显著差异。(d) 不同的上下文使得相同的头部显示出不同的稀疏结构，同时许多注意力头遵循两种主要模式：列条带和局部窗口。(e)
    选择的top-k条带与CRA的关系。高行级数值分布相似性使得少量关键列条带能够覆盖全注意力分数矩阵的大多数值。更多细节请参见附录[A.3](#A1.SS3
    "A.3 Visualization of attention ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")，[A.4](#A1.SS4
    "A.4 Sparisty analysis ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")。'
- en: 3.2 Empirical Foundation of Adaptive Sparsity in Attention
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 自适应稀疏性的经验基础
- en: 'Section [3.1](#S3.SS1 "3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless
    Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention") uncovers the possibility
    of approximating the attention output using near-lossless sparse attention, with
    the key lying in finding an effective attention mask. In this section, we present
    our empirical findings that reveal the inherently-high, head-specific, and content-aware
    adaptive sparsity and the significant patterns. These can be leveraged to achieve
    efficient near-lossless sparse attention.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '部分[3.1](#S3.SS1 "3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless
    Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention")揭示了使用接近无损稀疏注意力来近似注意力输出的可能性，关键在于找到有效的注意力掩码。在本节中，我们展示了我们的经验发现，这些发现揭示了天然高稀疏度、特定头部和内容感知的自适应稀疏性及其显著模式。这些可以被利用来实现高效的接近无损稀疏注意力。'
- en: 'Inherently-High Sparsity Degree. Our observations reveal that LLMs inherently
    exhibit a significant sparsity degree when using near-lossless sparse attention.
    In Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of
    Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of
    Long Context LLM Inference with Adaptive Structured Sparse Attention")(a), the
    average sparsity degree across different layers of various LLMs is depicted, with
    a threshold of $\alpha=0.95$ for near-lossless model accuracy. We find that most
    layers exhibit remarkably high sparsity degree, surpassing 90%, regardless of
    the input length. Notably, the first layer has a lower sparsity degree.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '天然的高稀疏度。我们的观察表明，LLM在使用接近无损的稀疏注意力时，天然地表现出显著的稀疏度。在图[2](#S3.F2 "Figure 2 ‣ 3.1
    Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse Attention ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention")(a)中，描绘了不同LLM的不同层的平均稀疏度，其中接近无损的模型准确率的阈值为$\alpha=0.95$。我们发现，大多数层的稀疏度显著高于90%，无论输入长度如何。值得注意的是，第一层的稀疏度较低。'
- en: 'To further quantify the variation in sparsity degree with increasing sequence
    length, we conduct a scaling evaluation on the "Needle in a Haystack" [[47](#bib.bib47)]
    task, as illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation
    ‣ 3 Foundation of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")(b).
    Our findings indicate that as the context becomes longer, there is a corresponding
    increase in the sparsity degree.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步量化随着序列长度增加的稀疏度变化，我们在“针在干草堆中”[[47](#bib.bib47)]任务上进行规模评估，如图[2](#S3.F2 "Figure
    2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse Attention
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention")(b)所示。我们的发现表明，随着上下文变长，稀疏度也相应增加。'
- en: 'Adaptive Sparsity. The attention sparsity is head-specific and content-aware.
    The sparsity degree and structure varies across different attention heads and
    input contexts. Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation
    of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")(c) demonstrates
    that certain heads in most layers exhibit lower SD($\alpha$, while the highest
    degree can reach $99.8\%$. This suggests that different heads may have distinct
    roles in processing long sequences, indicating that uniform compression across
    all heads may not be optimal. Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation
    ‣ 3 Foundation of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")(d)
    shows that different contents of similar length result in noticeable variations
    in sparse patterns within the same layer and head. This indicates that regions
    with higher attention scores change significantly based on the given scenario,
    such as different user prompts.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '自适应稀疏度。注意力稀疏度是特定头部且内容感知的。不同注意力头部和输入上下文中的稀疏度和结构各不相同。图[2](#S3.F2 "Figure 2 ‣
    3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse Attention ‣
    SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention")(c)显示，大多数层中的某些头部表现出较低的SD($\alpha$，而最高稀疏度可达$99.8\%$。这表明不同的头部在处理长序列时可能有不同的角色，表明在所有头部之间进行均匀压缩可能不是最佳的。图[2](#S3.F2
    "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse
    Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention")(d)显示，相似长度的不同内容在同一层和头部中导致稀疏模式的明显变化。这表明，具有较高注意力分数的区域会根据给定的场景（如不同的用户提示）发生显著变化。'
- en: 'Significant Window and Stripe Patterns. We identify two significant sparse
    patterns that substantially contribute to the attention score, as depicted in
    Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless
    Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention")(d). The local window
    pattern captures recent context information, while column stripe pattern embodies
    the key global contextual information. By adaptively combining these two patterns,
    LLMs can effectively handle both fine-grained information and key contextual cues.
    Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless
    Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention")(e) demonstrates that
    selecting a small amount of critical column strips is able to cover the majority
    values of the full attention score matrix, thus achieving a high CRA. This indicates
    the high numerical distribution similarity across rows.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '显著的窗口和条纹模式。我们识别出两种显著的稀疏模式，它们对注意力分数有重要贡献，如图[2](#S3.F2 "Figure 2 ‣ 3.1 Theoretical
    Foundation ‣ 3 Foundation of Near-Lossless Sparse Attention ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention")(d)所示。局部窗口模式捕捉最近的上下文信息，而列条纹模式体现了关键的全局上下文信息。通过自适应地结合这两种模式，LLMs能够有效处理细粒度信息和关键上下文线索。图[2](#S3.F2
    "Figure 2 ‣ 3.1 Theoretical Foundation ‣ 3 Foundation of Near-Lossless Sparse
    Attention ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention")(e)表明，选择少量关键列条带能够覆盖完整注意力分数矩阵的大部分值，从而实现高CRA。这表明行间数值分布的相似性很高。'
- en: Although similar patterns have been observed in recent works [[43](#bib.bib43),
    [37](#bib.bib37), [39](#bib.bib39)], they focus on reducing KV cache memory consumption
    during decoding. Directly migrating these approaches to accelerate prefill attention
    requires computing full attention score, which is unaffordable in long context.
    How to effectively explore these patterns for near-lossless acceleration of prefill
    is remain challenging.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近的工作中观察到了类似的模式[[43](#bib.bib43), [37](#bib.bib37), [39](#bib.bib39)]，它们专注于减少解码过程中的KV缓存内存消耗。直接将这些方法迁移以加速预填充注意力需要计算完整的注意力分数，这在长上下文中是无法承受的。如何有效探索这些模式以实现近乎无损的预填充加速仍然是一个挑战。
- en: 'Takeaway: Attention sparsity is inherently-high, head-specific, and content-aware,
    and exhibits significant local window and column stripe patterns. This adaptive
    sparsity indicates that sparse attention should dynamically capture the adaptive
    sparse patterns at runtime to be near-lossless.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 收获：注意力稀疏性本质上很高，特定于头，并且内容感知，表现出显著的局部窗口和列条纹模式。这种自适应稀疏性表明，稀疏注意力应动态捕捉运行时的自适应稀疏模式，以达到近乎无损的效果。
- en: 4 SampleAttention
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 样本注意力
- en: In this section, we introduce our approach to efficiently discover effective
    attention masks with observed significant sparse patterns and accelerate the attention
    with near-lossless sparse attention.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的方法，以高效发现具有观察到的显著稀疏模式的有效注意力掩码，并以近乎无损的稀疏注意力加速注意力计算。
- en: 4.1 Problem Formulation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 问题表述
- en: 'As discussed, the key to utilizing near-lossless sparse attention is to find
    an attention mask M with the following properties to achieve superior performance:
    1) near-lossless: meets a desired CRA threshold $\alpha$, 2) adaptive: varies
    across different heads, layers and contents, 3) hardware-efficient: maximizes
    hardware efficiency, 4) efficiently discoverable: can be found with minimal overhead.
    A static mask clearly cannot meet these criteria, and these properties pose significant
    challenges.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如讨论所述，利用近乎无损稀疏注意力的关键是找到一个具有以下属性的注意力掩码M，以实现卓越的性能：1）近乎无损：满足期望的CRA阈值$\alpha$，2）自适应：在不同的头、层和内容中变化，3）硬件高效：最大化硬件效率，4）高效可发现：可以在最小开销下找到。静态掩码显然无法满足这些标准，这些属性带来了显著的挑战。
- en: Selecting an attention mask $\textbf{M}\in\{0,1\}^{S_{q}\times S_{k}}$ attention
    score grid during runtime is hardware-inefficient and incurs high overhead due
    to the grid size and potential random pattern. Thus, we first utilize the observed
    significant sparse patterns to simplify and reformulate the problem, aiming to
    discover a hardware-efficient structured sparse pattern mask $\hat{\textbf{M}}$,
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时选择一个注意力掩码$\textbf{M}\in\{0,1\}^{S_{q}\times S_{k}}$的注意力分数网格在硬件上效率低且开销高，因为网格大小和潜在的随机模式。因此，我们首先利用观察到的显著稀疏模式来简化和重新表述问题，旨在发现一个硬件高效的结构化稀疏模式掩码$\hat{\textbf{M}}$。
- en: '|  | $1$2 |  | (5) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 'where $w$, and $I_{KV}$ (as illustrated in Figure [3](#S4.F3 "Figure 3 ‣ 4.2
    Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration of Long
    Context LLM Inference with Adaptive Structured Sparse Attention")). We leverage
    the fixed sparse pattern, which is hardware-efficient, and adaptively determine
    the size and indices during runtime according to the context. Moreover, the structured
    attention mask $\hat{\textbf{M}}$ maintains near-lossless property,'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$w$，以及$I_{KV}$（如图[3](#S4.F3 "图 3 ‣ 4.2 方法 ‣ 4 样本注意力 ‣ 样本注意力：通过自适应结构化稀疏注意力加速长上下文LLM推理")所示）。我们利用固定的稀疏模式，该模式在硬件上高效，并在运行时根据上下文自适应地确定大小和索引。此外，结构化注意力掩码$\hat{\textbf{M}}$保持近乎无损的属性。
- en: Theorem 2.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理2
- en: The hardware-efficient structured sparse pattern mask $\hat{\textbf{M}}$ maintains
    near-lossless sparse.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件高效的结构化稀疏模式掩码$\hat{\textbf{M}}$保持近乎无损稀疏。
- en: 'The proof of Theorem [2](#Thmtheorem2 "Theorem 2\. ‣ 4.1 Problem Formulation
    ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention") please refer to Appendix [A.1](#A1.SS1
    "A.1 Proof of Theorems ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention").
    Given the formulation, the problem now is to find $w$ for each head to meet the
    required properties during runtime.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '关于定理[2](#Thmtheorem2 "定理 2\. ‣ 4.1 问题表述 ‣ 4 SampleAttention ‣ SampleAttention:
    基于自适应结构化稀疏注意力的长上下文 LLM 推理的几乎无损加速")的证明，请参见附录[A.1](#A1.SS1 "A.1 定理证明 ‣ 附录 A 附录 ‣
    SampleAttention: 基于自适应结构化稀疏注意力的长上下文 LLM 推理的几乎无损加速")。给定这个公式，问题现在是找到每个头的 $w$ 以满足运行时所需的属性。'
- en: 4.2 Method
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 方法
- en: Tuned Window Size $w$ as a fixed percentage of sequence length ($\lceil r_{w}\%\times
    S_{k}\rceil$ is the sequence length of the input request. The percentage is tuned
    to be enough large to capture important local windows, and it also accommodates
    dynamic window sizes across various context lengths. While previous works have
    explored window attention [[20](#bib.bib20), [39](#bib.bib39), [37](#bib.bib37)],
    they typically rely on a fixed window size, which cannot adequately capture local
    dependencies across various context lengths.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的窗口大小 $w$ 是序列长度的一个固定百分比（$\lceil r_{w}\%\times S_{k}\rceil$ 是输入请求的序列长度）。这个百分比被调整得足够大以捕捉重要的局部窗口，同时也适应了不同上下文长度的动态窗口大小。虽然之前的工作已经探讨了窗口注意力
    [[20](#bib.bib20), [39](#bib.bib39), [37](#bib.bib37)]，但它们通常依赖于固定的窗口大小，这不能充分捕捉不同上下文长度下的局部依赖。
- en: KV Indices of Interest $I_{KV}$ for the input prompt and desired CRA threshold
    $\alpha$,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输入提示的感兴趣的 KV 索引 $I_{KV}$ 和所需的 CRA 阈值 $\alpha$，
- en: '|  | $1$2 |  | (6) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: 'Ideally, computing the entire attention score matrix P and then selecting $I_{KV}$
    would be optimal, but this incurs unaffordable quadratic overhead in both computation
    and memory consumption. Fortunately, the similar distribution of large numerical
    values across rows, as observed in Section [3.2](#S3.SS2 "3.2 Empirical Foundation
    of Adaptive Sparsity in Attention ‣ 3 Foundation of Near-Lossless Sparse Attention
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"), can be leveraged to simplify the indices
    selection process. SampleAttention introduces a two-stage query-guided key-value
    filtering approach to approximate the solution. The PyTorch-style algorithm refers
    to Appendix [A.7](#A1.SS7 "A.7 PyTorch-Style Implementation Algorithm ‣ Appendix
    A Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention"). Our evaluations show that the approximation
    performs pretty well (Section [5](#S5 "5 Experiments ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '理想情况下，计算整个注意力得分矩阵 P 并选择 $I_{KV}$ 将是最优的，但这在计算和内存消耗上都会带来难以承受的二次开销。幸运的是，可以利用第[3.2节](#S3.SS2
    "3.2 自适应稀疏注意力的实证基础 ‣ 几乎无损稀疏注意力的基础 ‣ SampleAttention: 基于自适应结构化稀疏注意力的长上下文 LLM 推理的几乎无损加速")中观察到的类似的数值大范围分布来简化索引选择过程。SampleAttention
    引入了一种两阶段的查询引导的键值过滤方法来近似求解。PyTorch 风格的算法详见附录[A.7](#A1.SS7 "A.7 PyTorch 风格实现算法 ‣
    附录 A 附录 ‣ SampleAttention: 基于自适应结构化稀疏注意力的长上下文 LLM 推理的几乎无损加速")。我们的评估显示，这种近似方法表现得相当不错（第[5节](#S5
    "5 实验 ‣ SampleAttention: 基于自适应结构化稀疏注意力的长上下文 LLM 推理的几乎无损加速")）。'
- en: 'Stage-1: Query-Guided Attention Sampling. SampleAttention first samples the
    attention score matrix by computing exact scores for a few queries (Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")①). This
    is motivated by the significant column stripe sparse pattern: a high score for
    $\textbf{P}_{ik}$ is also high. Therefore we can select a minimum subset of queries
    $\{i_{1},\cdots,i_{l}\}\subseteq\{0,\cdots,S_{q}-1\}$. Experiments show that this
    simple approach is effective: sampling a small amount of rows can accurately approximate
    the real CRA, further details can be found in Appendix [A.5](#A1.SS5 "A.5 Effectiveness
    of sampling ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '阶段1：查询引导的注意力采样。SampleAttention 首先通过计算少量查询的精确得分来采样注意力得分矩阵（图 [3](#S4.F3 "Figure
    3 ‣ 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")①）。这是由显著的列条纹稀疏模式激发的：$\textbf{P}_{ik}$
    的高得分也是高的。因此，我们可以选择最小的查询子集 $\{i_{1},\cdots,i_{l}\}\subseteq\{0,\cdots,S_{q}-1\}$。实验表明，这种简单的方法是有效的：采样少量行可以准确地近似真实的
    CRA，进一步的细节可以在附录 [A.5](#A1.SS5 "A.5 Effectiveness of sampling ‣ Appendix A Appendix
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention") 中找到。'
- en: 'Stage-2: Score-Based Key-Value Filtering. SampleAttention then filters key-values
    indices of interest base on the sampled attention score. Exactly solve Equation [6](#S4.E6
    "In 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") for
    sampled queries is inefficient due to long sequence length. To resolve this, SampleAttention
    filters key-values based on the accumulated attention scores along column (Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")②), which
    is more statistical approximation of attention score. After column-wise reduction,
    SampleAttention separately select top-k key-value indices that can meet the desired
    CRA threshold $\alpha$ for each head. Attention sinks can also be discovered in
    this way.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '阶段2：基于得分的键值过滤。SampleAttention 然后基于采样的注意力得分过滤感兴趣的键值索引。由于序列长度较长，精确解方程 [6](#S4.E6
    "In 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") 对于采样查询来说效率低下。为了解决这个问题，SampleAttention
    基于沿列的累计注意力得分（图 [3](#S4.F3 "Figure 3 ‣ 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention")②）过滤键值，这是一种对注意力得分的更统计性近似。在按列减少之后，SampleAttention 为每个头分别选择能够满足所需的
    CRA 阈值 $\alpha$ 的 top-k 键值索引。也可以通过这种方式发现注意力沉降。'
- en: '![Refer to caption](img/13e1b338ba1218fd30490c4ec57e9e11.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/13e1b338ba1218fd30490c4ec57e9e11.png)'
- en: 'Figure 3: SampleAttention replaces the original full attention with a two-stage
    implementation. In the first stage, attention scores are computed by performing
    stride sampling across multiple rows and accumulating the scores along the column.
    In the second stage, the indices $I_{KV}$ are selected via top-k operation for
    each head. The obtained $I_{KV}$ is then merged with the masks of the local window
    and bottom area to enable sparse computation of the attention.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：SampleAttention 用两阶段实现替换了原始的全注意力。在第一阶段，通过在多行上执行步幅采样并沿列累计得分来计算注意力得分。在第二阶段，通过
    top-k 操作为每个头选择索引 $I_{KV}$。获得的 $I_{KV}$ 然后与局部窗口和底部区域的掩码合并，以实现注意力的稀疏计算。
- en: 'Table 1: The meaning of hyperparameters and tuning approach.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：超参数的含义和调整方法。
- en: '| Hyperparameter | Description | Tuning |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 描述 | 调整 |'
- en: '| $\alpha$ | The desired CRA threshold | Offline profiling separately |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ | 所需的 CRA 阈值 | 离线分析单独进行 |'
- en: '| $r_{row}$ | The sampling ratio in stage-1 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| $r_{row}$ | 阶段1中的采样比率 |'
- en: '| $r_{w}\%$ | The ratio of local window size |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| $r_{w}\%$ | 局部窗口大小的比率 |'
- en: 'Hyperparameter Tuning. SampleAttention needs to tune several hyperparameters
    as listed in Table [1](#S4.T1 "Table 1 ‣ 4.2 Method ‣ 4 SampleAttention ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention"). These hyperparameters affects both model accuracy and inference
    latency. For example, a large $\alpha$ increases the sampling overhead but reduces
    the attention approximation error. We find that fixed hyperparameter, obtained
    by lightweight offline profiling, for an LLM performs well across different tasks.
    Thus we use a small dataset that contains 22 requests ranging from 25K-96K context
    length to determine these hyperparameters. The detailed effects of varying these
    hyperparameters are studied in Section [5.3](#S5.SS3 "5.3 Hyperparameter Ablation
    Study ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention").'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '超参数调整。SampleAttention 需要调整多个超参数，如表格 [1](#S4.T1 "Table 1 ‣ 4.2 Method ‣ 4 SampleAttention
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention") 所示。这些超参数影响模型的准确性和推理延迟。例如，大的 $\alpha$ 增加了采样开销，但减少了注意力近似误差。我们发现，通过轻量级的离线分析获得的固定超参数在不同任务中表现良好。因此，我们使用包含
    22 个请求的较小数据集，这些请求的上下文长度范围从 25K 到 96K，以确定这些超参数。关于这些超参数变化的详细影响在第 [5.3](#S5.SS3 "5.3
    Hyperparameter Ablation Study ‣ 5 Experiments ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")
    节中进行了研究。'
- en: 4.3 Hardware-efficient Implementation
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 硬件高效实现
- en: To achieve substantial speedup in wall-clock time, SampleAttention is implemented
    with IO-awareness to maximize hardware-efficiency. First, the query-guided key-value
    filtering involves a series of small operators (bmm, softmax, reduction) that
    read and write large intermediate results. SampleAttention significantly reduces
    IO overhead by fusing these operators. Second, SampleAttention implements an efficient
    adaptive structured sparse attention kernel by modifying FlashAttention [[48](#bib.bib48)].
    These hardware-aware optimizations enhance speed performance significantly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在实际时间上实现显著加速，SampleAttention 实现了 IO 感知以最大化硬件效率。首先，查询引导的键值过滤涉及一系列小型操作符（bmm、softmax、reduction），这些操作符读写大量中间结果。SampleAttention
    通过融合这些操作符显著减少了 IO 开销。其次，SampleAttention 通过修改 FlashAttention [[48](#bib.bib48)]
    实现了高效的自适应结构稀疏注意力内核。这些硬件感知优化显著提高了速度性能。
- en: 5 Experiments
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Setup
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: 'Backbones. We evaluate our method on two widely used open-source LLM variants:
    ChatGLM2-6B with a 96K context window based on GLM [[17](#bib.bib17)], and internLM2-7B [[49](#bib.bib49)]
    with a 200K context window based on LLAMA2 [[8](#bib.bib8)]. All utilized models
    are decoder-only transformers [[50](#bib.bib50)], and are pre-trained via causal
    language modeling. They encompass similar architectural components, such as rotary
    positional encoding [[51](#bib.bib51)], and grouped-query attention [[52](#bib.bib52)].
    Simultaneously, there are notable differences, e.g., the former augments the context
    window capacity via continued training with an extended sequence length, whereas
    the latter achieves length extrapolation through rope scaling. We only replace
    the full attention implementation during the prompt prefill stage with SampleAttention
    and various baselines, while maintaining an uncompressed KV cache in the decode
    phase.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型。我们在两个广泛使用的开源 LLM 变体上评估我们的方法：基于 GLM 的 ChatGLM2-6B，具有 96K 上下文窗口 [[17](#bib.bib17)]，以及基于
    LLAMA2 的 internLM2-7B [[49](#bib.bib49)]，具有 200K 上下文窗口 [[8](#bib.bib8)]。所有使用的模型都是仅解码器的变换器
    [[50](#bib.bib50)]，并通过因果语言建模进行预训练。它们包含类似的架构组件，例如旋转位置编码 [[51](#bib.bib51)] 和分组查询注意力
    [[52](#bib.bib52)]。同时，也存在显著差异，例如，前者通过延长序列长度进行持续训练以增强上下文窗口容量，而后者通过绳索缩放实现长度外推。我们仅在提示预填充阶段用
    SampleAttention 和各种基线替换全注意力实现，同时在解码阶段保持未压缩的 KV 缓存。
- en: 'Tasks. We evaluate SampleAttention and other methods’ understanding capabilities
    in long-context scenarios on three distinct tasks: LongBench [[53](#bib.bib53)],
    BABILong [[54](#bib.bib54)], and Needle in a Haystack [[47](#bib.bib47)]. LongBench,
    a multi-task benchmark, comprises single and multi-document QA, summarization,
    few-shot learning, synthetic tasks, and code completion. It offers over 4,750
    test cases with task lengths from 4K-35K. BABILong is a generative benchmark test
    designed to assess long-context inferencing capability, consisting of 20 different
    tasks. Given its generative nature, task lengths can be flexibly set from 4K-88K.
    Additionally, the "Needle in a Haystack" stress test challenges models to accurately
    extract information from a specific sentence buried within a lengthy document
    at a random position. We have set the number of depth intervals at 32, with lengths
    ranging from 10K-96K. Note that in these tasks, each case is evaluated after the
    model provides an output. This output is compared against a standard answer or
    judged by more advanced models, such as GPT-4 [[55](#bib.bib55)], for scoring.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 任务。我们在三个不同的任务上评估 SampleAttention 和其他方法在长上下文场景中的理解能力：LongBench [[53](#bib.bib53)]、BABILong
    [[54](#bib.bib54)] 和 Needle in a Haystack [[47](#bib.bib47)]。LongBench 是一个多任务基准测试，包含单文档和多文档
    QA、摘要、少样本学习、合成任务和代码补全。它提供了超过 4,750 个测试案例，任务长度从 4K 到 35K。BABILong 是一个生成性基准测试，旨在评估长上下文推理能力，由
    20 个不同任务组成。由于其生成性特征，任务长度可以灵活设置为 4K 到 88K。此外，“针在干草堆中”压力测试挑战模型准确提取埋藏在长文档中的特定句子信息。我们将深度区间的数量设置为
    32，长度范围从 10K 到 96K。请注意，在这些任务中，每个案例在模型提供输出后进行评估。该输出与标准答案进行比较，或由更高级的模型（如 GPT-4 [[55](#bib.bib55)]）进行评分。
- en: 5.2 Accuracy Results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 准确性结果
- en: 'Table 2: Accuracy comparison across various sparse methods on LongBench and
    BABILong. The best results are highlighted in Bold while the second best results
    are marked with an Underline.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 LongBench 和 BABILong 上各种稀疏方法的准确性比较。最佳结果以**粗体**突出显示，而第二最佳结果则用*下划线*标记。
- en: Model Baseline LongBench BABILong Single- Doc QA Multi- Doc QA Summari- zation
    Few-shot Learning Synthetic Tasks Code Completion Total Scores Total Scores ChatGLM2
    6B Full Attention 161.15 147.76 98.64 243.66 87.00 99.20 837.40 30.20 SampleAttention($\alpha=0.95$)
    77.53 76.01 98.52 254.95 53.02 126.83 686.86 36.88 BigBrid 72.55 73.16 95.59 254.87
    19.88 120.99 637.04 34.12 Streaming LLM 31.49 26.44 35.32 133.53 3.33 89.44 319.55
    5.96 HyperAttention 87.98 33.40 38.52 95.78 3.09 77.80 336.57 16.64 Hash-Sparse
    20.12 11.37 24.32 49.88 5.87 45.28 156.84 2.82
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 基线 LongBench BABILong 单文档 QA 多文档 QA 摘要 少样本学习 合成任务 代码补全 总分 总分 ChatGLM2 6B
    全注意力 161.15 147.76 98.64 243.66 87.00 99.20 837.40 30.20 SampleAttention($\alpha=0.95$)
    77.53 76.01 98.52 254.95 53.02 126.83 686.86 36.88 BigBrid 72.55 73.16 95.59 254.87
    19.88 120.99 637.04 34.12 Streaming LLM 31.49 26.44 35.32 133.53 3.33 89.44 319.55
    5.96 HyperAttention 87.98 33.40 38.52 95.78 3.09 77.80 336.57 16.64 Hash-Sparse
    20.12 11.37 24.32 49.88 5.87 45.28 156.84 2.82
- en: '![Refer to caption](img/efba74be3dba7153eab3657454f4e773.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/efba74be3dba7153eab3657454f4e773.png)'
- en: 'Figure 4: Scores of different methods on the "Needle in a Haystack" task at
    various lengths.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同方法在“针在干草堆中”任务中的得分，任务长度各异。
- en: Baselines and settings. We consider the full attention (as the gold baseline),
    BigBrid [[20](#bib.bib20)], Streaming-LLM [[37](#bib.bib37)], HyperAttention [[26](#bib.bib26)]
    and Hash-Sparse [[24](#bib.bib24)] as baselines to compare model accuracy across
    different tasks. To maintain consistency, we assign the same window size ratio
    $8\%$. StreamingLLM sets its initail attention sink at 4 tokens. HyperAttention
    set both bucket size and the number of sampled columns to 256, and Hash-Sparse
    uses a bucket number of 16. The sampling ratio $r_{row}$ for SampleAttention are
    set to 5% and 0.95, respectively, through offline profiling.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基线和设置。我们将全注意力（作为黄金基线）、BigBrid [[20](#bib.bib20)]、Streaming-LLM [[37](#bib.bib37)]、HyperAttention
    [[26](#bib.bib26)] 和 Hash-Sparse [[24](#bib.bib24)] 作为基线，以比较不同任务中的模型准确性。为了保持一致性，我们设置了相同的窗口大小比例
    $8\%$。StreamingLLM 将其初始注意力阈值设置为 4 个标记。HyperAttention 将桶大小和采样列数都设置为 256，而 Hash-Sparse
    使用了 16 个桶。SampleAttention 的采样比例 $r_{row}$ 分别设置为 5% 和 0.95，通过离线分析得出。
- en: 'Main results. Table [2](#S5.T2 "Table 2 ‣ 5.2 Accuracy Results ‣ 5 Experiments
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention") and Figure [4](#S5.F4 "Figure 4 ‣ 5.2 Accuracy
    Results ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of Long
    Context LLM Inference with Adaptive Structured Sparse Attention") display the
    accuracy results of the models on three downstream tasks. Detailed results are
    listed in Appendix [A.2](#A1.SS2 "A.2 Detailed results ‣ Appendix A Appendix ‣
    SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"). The results show that:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '主要结果。表格 [2](#S5.T2 "表格 2 ‣ 5.2 准确性结果 ‣ 5 实验 ‣ SampleAttention: 通过自适应结构化稀疏注意力实现长上下文LLM推理的近无损加速")和图 [4](#S5.F4
    "图 4 ‣ 5.2 准确性结果 ‣ 5 实验 ‣ SampleAttention: 通过自适应结构化稀疏注意力实现长上下文LLM推理的近无损加速")展示了模型在三个下游任务上的准确性结果。详细结果列在附录 [A.2](#A1.SS2
    "A.2 详细结果 ‣ 附录 A 附录 ‣ SampleAttention: 通过自适应结构化稀疏注意力实现长上下文LLM推理的近无损加速")中。结果显示：'
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The performance in accuracy of SampleAttention is consistently robust across
    all benchmarks (including subdomains), various models, and diverse sequence lengths.
    When compared to full attention, which serves as the gold standard, SampleAttention
    consistently achieves scores above 99% of full attention, demonstrating near-lossless
    efficiency.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SampleAttention在所有基准测试（包括子领域）、各种模型和不同序列长度下的准确性表现始终稳健。与作为黄金标准的全注意力相比，SampleAttention的得分始终超过全注意力的99%，显示出近乎无损的效率。
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BigBrid exhibits varying degrees of performance degradation across different
    tasks, with "Synthetic Task" presenting a significant challenge. Nonetheless,
    on average, BigBrid still attains scores that are approximately 91% of those achieved
    by full attention.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BigBrid在不同任务中表现出不同程度的性能下降，其中“合成任务”带来了显著的挑战。然而，平均而言，BigBrid的得分仍约为全注意力得分的91%。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: StreamingLLM, HyperAttention and Hash-Sparse result in performance degradation
    across all tasks, demonstrating that these techniques fail to capture critical
    KV elements in long sequences at the prefill stage.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: StreamingLLM、HyperAttention和Hash-Sparse在所有任务中都表现出性能下降，表明这些技术在预填充阶段未能捕捉到长序列中的关键KV元素。
- en: 5.3 Hyperparameter Ablation Study
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 超参数消融研究
- en: 'We conducted further tests on the impact of three critical hyperparameters
    in SampleAttention on the accuracy of downstream tasks. These experiments adhered
    to the settings outlined in Section [5.2](#S5.SS2 "5.2 Accuracy Results ‣ 5 Experiments
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"), with only one hyperparameter changed at
    a time. Detailed results under different hyperparameter configurations are provided
    in Table [5.2](#S5.SS2 "5.2 Accuracy Results ‣ 5 Experiments ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention").'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对SampleAttention中三个关键超参数对下游任务准确性的影响进行了进一步测试。这些实验遵循了第 [5.2](#S5.SS2 "5.2 准确性结果
    ‣ 5 实验 ‣ SampleAttention: 通过自适应结构化稀疏注意力实现长上下文LLM推理的近无损加速")节中概述的设置，每次只改变一个超参数。不同超参数配置下的详细结果见表格 [5.2](#S5.SS2
    "5.2 准确性结果 ‣ 5 实验 ‣ SampleAttention: 通过自适应结构化稀疏注意力实现长上下文LLM推理的近无损加速")。'
- en: 'Table 3: Results of varying the three hyperparameters in the SampleAttention
    on the ChatGLM2-6B. The best results are highlighted in Bold while the second
    best results are marked with an Underline.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：SampleAttention在ChatGLM2-6B上的三个超参数变化结果。最佳结果用粗体标出，第二最佳结果用下划线标记。
- en: Task full attention CRA threshold $\alpha$ sample ratio $r_{row}$ $\alpha=0.90$
    $\alpha=0.98$ $r_{w}=8$ $5\%$ LongBench 837.40 820.30 824.98 833.00 829.80 792.87
    833.00 809.34 833.00 831.14 BABILong 30.20 27.28 29.08 31.04 31.16 31.12 31.04
    28.92 31.04 30.64 Needle in a Haystack 2235 2130 2090 2239 2231 2084 2239 2106
    2239 2231
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 任务 全注意力 CRA 阈值 $\alpha$ 样本比率 $r_{row}$ $\alpha=0.90$ $\alpha=0.98$ $r_{w}=8$
    $5\%$ LongBench 837.40 820.30 824.98 833.00 829.80 792.87 833.00 809.34 833.00
    831.14 BABILong 30.20 27.28 29.08 31.04 31.16 31.12 31.04 28.92 31.04 30.64 Needle
    in a Haystack 2235 2130 2090 2239 2231 2084 2239 2106 2239 2231
- en: CRA threshold $\alpha$ too low leads to performance degradation due to the excessive
    filtering of KV elements. This represents a clear trade-off between performance
    and speedup. However, even with $\alpha$ reaches a sufficiently high threshold.
    Thus, conducting a profiling to determine an appropriate $\alpha$ for a given
    model is essential.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: CRA 阈值 $\alpha$ 过低会导致由于 KV 元素过度过滤而导致性能下降。这代表了性能与加速之间的明显权衡。然而，即使 $\alpha$ 达到足够高的阈值，因此，进行分析以确定给定模型的适当
    $\alpha$ 是至关重要的。
- en: Local window size and sampling ratio. Additionally, setting excessively small
    local window ratios or sampling ratios also results in performance decreases.
    Specifically, halving the ratio of the local window size (k=4) results in a performance
    decline of over 6% in the LongBench and "Needle-in-a-Haystack" tasks. This confirms
    the high significance of KV elements within the local window area. Additionally,
    reducing the sampling ratio to 2% results in an approximate 4.5% performance loss.
    However, performance stabilizes once the sampling ratio reaches a certain threshold,
    as the top-k results for the approximate attention becomes stable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 局部窗口大小和采样比例。此外，设置过小的局部窗口比例或采样比例也会导致性能下降。具体来说，将局部窗口大小（k=4）的比例减半会导致 LongBench
    和“针在干草堆中”任务的性能下降超过 6%。这确认了 KV 元素在局部窗口区域内的重要性。此外，将采样比例减少到 2% 会导致约 4.5% 的性能损失。然而，一旦采样比例达到某个阈值，性能会稳定下来，因为近似注意力的
    top-k 结果变得稳定。
- en: 5.4 Acceleration Speedup Benchmarking
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 加速加速基准测试
- en: 'We conducted micro-benchmarks on a single NVIDIA-A100 GPU (80GB) to evaluate
    performance in speed of attention operation during the prefill and TTFT metrics.
    The baselines selected were PyTorch’s scaled_dot_product_attention (noted as SDPA)
    and FlashAttention2. All tests were conducted using the configuration from ChatGLM2-6B:
    32 heads, and $d=128$, with synthetic data from the "Needle-in-a-Haystack" benchmark
    as input. We standardize the batch size of the input data to 1 to support longer
    sequence lengths.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在单个 NVIDIA-A100 GPU（80GB）上进行了微基准测试，以评估在预填充和 TTFT 评测期间的注意力操作速度。选择的基线是 PyTorch
    的 scaled_dot_product_attention（简称 SDPA）和 FlashAttention2。所有测试均使用来自 ChatGLM2-6B
    的配置进行：32 个头，$d=128$，输入数据来自“针在干草堆中”基准测试。我们将输入数据的批量大小标准化为 1 以支持更长的序列长度。
- en: 'Speedup and sampling overhead Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Acceleration
    Speedup Benchmarking ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") displays
    the profiling results conducted on the model’s full 28 layers using generated
    data ranging from 8K to 96K. Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Acceleration Speedup
    Benchmarking ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of
    Long Context LLM Inference with Adaptive Structured Sparse Attention")(a), focusing
    on the GPU performance of the attention module, indicates that both SampleAttention
    ($\alpha=0.95$) do not exhibit a speed advantage over FlashAttention2 at shorter
    lengths, due to sampling overhead and small batch sizes. However, for longer sequences
    such as 96K, substantial savings in KV memory-transfers enable the attention operations
    of SampleAttention($\alpha=0.95$) to achieve accelerations of $2.20\times$ over
    FlashAttention2, while reducing the TTFT metric by $1.62\times$, respectively.
    Furthermore, Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Acceleration Speedup Benchmarking
    ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention")(c) demonstrates that
    as sequence lengths increase, the proportion of sampling overhead decreases, suggesting
    that SampleAttention can offer greater acceleration benefits for longer sequences.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '加速和采样开销 图 [5](#S5.F5 "图 5 ‣ 5.4 加速加速基准 ‣ 5 实验 ‣ SampleAttention: 近乎无损的长上下文LLM推断的自适应结构稀疏注意力")
    显示了在模型的全部28层上使用生成的数据（范围从8K到96K）进行的剖析结果。 图 [5](#S5.F5 "图 5 ‣ 5.4 加速加速基准 ‣ 5 实验
    ‣ SampleAttention: 近乎无损的长上下文LLM推断的自适应结构稀疏注意力")(a)，专注于注意力模块的GPU性能，表明SampleAttention
    ($\alpha=0.95$) 在较短的长度上没有表现出比FlashAttention2更快的速度，原因是采样开销和小批量尺寸。然而，对于96K这样的较长序列，由于KV内存传输的大幅节省，使SampleAttention($\alpha=0.95$)
    的注意力操作相对于FlashAttention2实现了 $2.20\times$ 的加速，同时将TTFT指标降低了 $1.62\times$。此外，图 [5](#S5.F5
    "图 5 ‣ 5.4 加速加速基准 ‣ 5 实验 ‣ SampleAttention: 近乎无损的长上下文LLM推断的自适应结构稀疏注意力")(c) 展示了随着序列长度的增加，采样开销的比例减少，表明SampleAttention可以为更长的序列提供更大的加速效益。'
- en: '![Refer to caption](img/f9e963ec7e334475f3a5ada2944586a3.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f9e963ec7e334475f3a5ada2944586a3.png)'
- en: 'Figure 5: (a) Latency comparison for the self-attention module. (b)The proportion
    of time spent on sampling and sparse computation in SampleAttention. (c) Comparison
    for the TTFT metric.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图5: (a) 自注意力模块的延迟比较。 (b) SampleAttention中用于采样和稀疏计算的时间比例。 (c) TTFT指标的比较。'
- en: 'Scaling the sequence length to 1M. We conducted GPU performance evaluations
    scalable to a sequence length of 1 million, based on profiling results from the
    first layer. Since SampleAttention is content-aware, for sequences longer than
    128K, we derived the average attention latency per layer from the first layer
    results of SampleAttention combined with model sparsity analysis to avoid memory
    issues. Figure [6](#S5.F6 "Figure 6 ‣ 5.4 Acceleration Speedup Benchmarking ‣
    5 Experiments ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM
    Inference with Adaptive Structured Sparse Attention") illustrates that at a sequence
    scaling to 1M, thresholds of 0.95 and 0.80 respectively achieve reductions in
    the TTFT metric by $2.27\times$.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '扩展序列长度到1M。我们基于第一层的剖析结果进行了可扩展到1百万序列长度的GPU性能评估。由于SampleAttention具有内容感知能力，对于超过128K的序列，我们从SampleAttention的第一层结果和模型稀疏性分析中推导出每层的平均注意力延迟，以避免内存问题。图 [6](#S5.F6
    "图 6 ‣ 5.4 加速加速基准 ‣ 5 实验 ‣ SampleAttention: 近乎无损的长上下文LLM推断的自适应结构稀疏注意力") 说明，在序列扩展到1M时，0.95和0.80的阈值分别将TTFT指标降低了
    $2.27\times$。'
- en: '![Refer to caption](img/b530a455793848fe404fb6954930da39.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b530a455793848fe404fb6954930da39.png)'
- en: 'Figure 6: (a) and (b) compare the latency of attention and TTFT metrics as
    the sequence scales from 8K to 1M, respectively. The numbers represent the speedup
    compared with FlashAttention2.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图6: (a) 和 (b) 比较了注意力的延迟和TTFT指标，随着序列从8K扩展到1M，数字表示与FlashAttention2相比的加速。'
- en: 6 Conclusion
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this paper, we first present both theoretical and empirical foundation for
    near-lossless sparse attention, and then leverage observed significant patterns
    to design SampleAttention, an adaptive structured sparse attention that can seamlessly
    replace FlashAttention in long context LLMs without accuracy loss. SampleAttention
    significantly reduces the TTFT of long context requests. Limitations and future
    work are discussed in Appendix [A.6](#A1.SS6 "A.6 Limitations and Future Work
    ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention").'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们首先展示了近乎无损稀疏注意力的理论和实证基础，然后利用观察到的显著模式设计了 SampleAttention，一种自适应结构稀疏注意力，可以无缝替代长上下文LLMs中的
    FlashAttention，而不损失准确性。SampleAttention 显著减少了长上下文请求的 TTFT。限制和未来工作在附录 [A.6](#A1.SS6
    "A.6 限制与未来工作 ‣ 附录 A 附录 ‣ SampleAttention: 通过自适应结构稀疏注意力加速长上下文 LLM 推断的近乎无损方法") 中讨论。'
- en: References
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava,
    Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
    et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039,
    2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava,
    Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz
    等。基础模型的有效长上下文扩展。arXiv 预印本 arXiv:2309.16039, 2023。'
- en: '[2] Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin.
    Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, 和 Dahua Lin.
    基于绳索的外推的规模法则。arXiv 预印本 arXiv:2310.05209, 2023。'
- en: '[3] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han,
    and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language
    models. In The Twelfth International Conference on Learning Representations, 2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han,
    和 Jiaya Jia. Longlora: 高效微调长上下文大型语言模型。在第十二届国际学习表征会议上，2023年。'
- en: '[4] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E.
    Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms
    truly promise on context length?, June 2023.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E.
    Gonzalez, Ion Stoica, Xuezhe Ma, 和 Hao Zhang. 开源LLMs在上下文长度上的真正承诺有多长？，2023年6月。'
- en: '[5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation. arXiv preprint
    arXiv:2306.15595, 2023.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong Tian. 通过位置插值扩展大型语言模型的上下文窗口。arXiv
    预印本 arXiv:2306.15595, 2023。'
- en: '[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
    Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato,
    Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario Amodei.
    语言模型是少量样本学习者。在 Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
    Balcan 和 Hsuan-Tien Lin 主编的《神经信息处理系统进展 33: 神经信息处理系统年会 2020》中，NeurIPS 2020，2020年12月6-12日，虚拟会议，2020年。'
- en: '[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力机制才是你所需要的。在《神经信息处理系统进展》,
    30, 2017年。'
- en: '[8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等。Llama 2: 开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288, 2023。'
- en: '[9] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown,
    and Tatsunori Hashimoto. Benchmarking large language models for news summarization.
    Transactions of the Association for Computational Linguistics, 12, 2024.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown
    和 Tatsunori Hashimoto。Benchmarking large language models for news summarization。计算语言学协会会刊，12，2024。'
- en: '[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
    Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
    et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,
    2021.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
    Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman
    等。Evaluating large language models trained on code。arXiv 预印本 arXiv:2107.03374，2021。'
- en: '[11] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
    Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code
    llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
    Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin 等。Code llama:
    Open foundation models for code。arXiv 预印本 arXiv:2308.12950, 2023。'
- en: '[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica 和
    Eric P. Xing。Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality，2023年3月。'
- en: '[13] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An
    instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang 和 Tatsunori B. Hashimoto。Stanford alpaca: An instruction-following
    llama model。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023。'
- en: '[14] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
    Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
    2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth 等。Gemini:
    a family of highly capable multimodal models。arXiv 预印本 arXiv:2312.11805，2023。'
- en: '[15] Anthropic. Claude. [https://www.anthropic.com/claude](https://www.anthropic.com/claude),
    2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Anthropic。Claude。 [https://www.anthropic.com/claude](https://www.anthropic.com/claude)，2023。'
- en: '[16] Moonshot. Kimi chat. [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/),
    2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Moonshot。Kimi chat。 [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/)，2023。'
- en: '[17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang,
    and Jie Tang. Glm: General language model pretraining with autoregressive blank
    infilling. arXiv preprint arXiv:2103.10360, 2021.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang
    和 Jie Tang。Glm: General language model pretraining with autoregressive blank infilling。arXiv
    预印本 arXiv:2103.10360，2021。'
- en: '[18] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary
    Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC:
    Encoding long and structured inputs in transformers. In Bonnie Webber, Trevor
    Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP), pages 268–284, Online, November
    2020\. Association for Computational Linguistics.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary
    Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang 和 Li Yang。ETC:
    Encoding long and structured inputs in transformers。在 Bonnie Webber, Trevor Cohn,
    Yulan He 和 Yang Liu 编辑的《2020年自然语言处理经验方法会议论文集（EMNLP）》中，页268–284，在线，2020年11月。计算语言学协会。'
- en: '[19] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document
    transformer. arXiv preprint arXiv:2004.05150, 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Iz Beltagy, Matthew E Peters 和 Arman Cohan。Longformer: The long-document
    transformer。arXiv 预印本 arXiv:2004.05150，2020。'
- en: '[20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
    Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al.
    Big bird: Transformers for longer sequences. Advances in neural information processing
    systems, 33:17283–17297, 2020.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
    Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang 等。Big
    bird: Transformers for longer sequences。神经信息处理系统进展，33:17283–17297，2020。'
- en: '[21] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. arXiv preprint arXiv:2001.04451, 2020.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Nikita Kitaev, Łukasz Kaiser 和 Anselm Levskaya。Reformer: The efficient
    transformer。arXiv 预印本 arXiv:2001.04451，2020。'
- en: '[22] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui
    Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. arXiv preprint arXiv:2307.02486, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Jiayu Ding、Shuming Ma、Li Dong、Xingxing Zhang、Shaohan Huang、Wenhui Wang、Nanning
    Zheng 和 Furu Wei。Longnet：将变换器扩展到 1,000,000,000 个标记。arXiv 预印本 arXiv:2307.02486，2023年。'
- en: '[23] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Rewon Child、Scott Gray、Alec Radford 和 Ilya Sutskever。使用稀疏变换器生成长序列。arXiv
    预印本 arXiv:1904.10509，2019年。'
- en: '[24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and François Fleuret.
    Faster causal attention over large sequences through sparse flash attention. arXiv
    preprint arXiv:2306.01160, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Matteo Pagliardini、Daniele Paliotta、Martin Jaggi 和 François Fleuret。通过稀疏闪电注意力加快对大序列的因果注意力。arXiv
    预印本 arXiv:2306.01160，2023年。'
- en: '[25] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient
    content-based sparse attention with routing transformers. Transactions of the
    Association for Computational Linguistics, 9:53–68, 2021.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Aurko Roy、Mohammad Saffar、Ashish Vaswani 和 David Grangier。高效的基于内容的稀疏注意力与路由变换器。《计算语言学协会会刊》，9:53–68，2021年。'
- en: '[26] Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David Woodruff,
    and Amir Zandieh. Hyperattention: Long-context attention in near-linear time.
    In The Twelfth International Conference on Learning Representations, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Insu Han、Rajesh Jayaram、Amin Karbasi、Vahab Mirrokni、David Woodruff 和 Amir
    Zandieh。Hyperattention：在接近线性时间内的长上下文注意力。在《第十二届国际学习表征会议》，2023年。'
- en: '[27] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
    Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Sinong Wang、Belinda Z Li、Madian Khabsa、Han Fang 和 Hao Ma。Linformer：具有线性复杂度的自注意力。arXiv
    预印本 arXiv:2006.04768，2020年。'
- en: '[28] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
    Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
    Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference
    on Learning Representations, 2020.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Krzysztof Marcin Choromanski、Valerii Likhosherstov、David Dohan、Xingyou
    Song、Andreea Gane、Tamas Sarlos、Peter Hawkins、Jared Quincy Davis、Afroz Mohiuddin、Lukasz
    Kaiser 等人。重新思考带有表演者的注意力。在《国际学习表征会议》，2020年。'
- en: '[29] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
    Transformers are rnns: Fast autoregressive transformers with linear attention.
    In International conference on machine learning, pages 5156–5165\. PMLR, 2020.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Angelos Katharopoulos、Apoorv Vyas、Nikolaos Pappas 和 François Fleuret。变换器是
    RNN：带有线性注意力的快速自回归变换器。在《国际机器学习会议》，页5156–5165。PMLR，2020年。'
- en: '[30] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher
    Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information
    Processing Systems, 34:17413–17426, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Beidi Chen、Tri Dao、Eric Winsor、Zhao Song、Atri Rudra 和 Christopher Ré。Scatterbrain：统一稀疏和低秩注意力。《神经信息处理系统进展》，34:17413–17426，2021年。'
- en: '[31] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra,
    and Christopher Re. Pixelated butterfly: Simple and efficient sparse training
    for neural network models. In International Conference on Learning Representations,
    2021.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Beidi Chen、Tri Dao、Kaizhao Liang、Jiaming Yang、Zhao Song、Atri Rudra 和 Christopher
    Re。Pixelated butterfly：简单高效的稀疏神经网络模型训练。在《国际学习表征会议》，2021年。'
- en: '[32] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective
    state spaces. arXiv preprint arXiv:2312.00752, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Albert Gu 和 Tri Dao。Mamba：具有选择状态空间的线性时间序列建模。arXiv 预印本 arXiv:2312.00752，2023年。'
- en: '[33] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al.
    Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048,
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Bo Peng、Eric Alcaide、Quentin Anthony、Alon Albalak、Samuel Arcadinho、Huanqi
    Cao、Xin Cheng、Michael Chung、Matteo Grella、Kranthi Kiran GV 等人。Rwkv：为变换器时代重新发明
    RNN。arXiv 预印本 arXiv:2305.13048，2023年。'
- en: '[34] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.
    Advances in Neural Information Processing Systems, 35:11079–11091, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Aydar Bulatov、Yury Kuratov 和 Mikhail Burtsev。递归记忆变换器。《神经信息处理系统进展》，35:11079–11091，2022年。'
- en: '[35] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing
    transformers. arXiv preprint arXiv:2203.08913, 2022.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Yuhuai Wu、Markus N Rabe、DeLesley Hutchins 和 Christian Szegedy。记忆变换器。arXiv
    预印本 arXiv:2203.08913，2022年。'
- en: '[36] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
    Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau,
    Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from
    trillions of tokens. In International conference on machine learning, pages 2206–2240\.
    PMLR, 2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
    Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau,
    Bogdan Damoc, Aidan Clark 等人。通过从万亿 tokens 中检索来改进语言模型。国际机器学习会议，页码 2206–2240。PMLR，2022。'
- en: '[37] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks. arXiv preprint arXiv:2309.17453,
    2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han 和 Mike Lewis。具有注意力汇聚的高效流式语言模型。arXiv
    预印本 arXiv:2309.17453，2023。'
- en: '[38] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther
    Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois,
    William Chou, et al. Mlperf inference benchmark. In 2020 ACM/IEEE 47th Annual
    International Symposium on Computer Architecture (ISCA), pages 446–459\. IEEE,
    2020.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther
    Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark Charlebois,
    William Chou 等人。Mlperf 推理基准。发表于 2020 年 ACM/IEEE 第 47 届计算机体系结构国际年会（ISCA），第 446–459
    页。IEEE，2020。'
- en: '[39] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter
    oracle for efficient generative inference of large language models. Advances in
    Neural Information Processing Systems, 36, 2024.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett 等人。H2o：用于大规模语言模型高效生成推理的重型预言机。神经信息处理系统进展，36，2024。'
- en: '[40] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
    Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv
    preprint arXiv:2312.04985, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
    Luschi 和 Douglas Orr。Sparq 注意力：带宽高效的 LLM 推理。arXiv 预印本 arXiv:2312.04985，2023。'
- en: '[41] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with
    gist tokens. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jesse Mu, Xiang Li 和 Noah Goodman。利用 gist tokens 学习压缩提示。神经信息处理系统进展，36，2024。'
- en: '[42] Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, and Rynson WH Lau. Biformer:
    Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pages 10323–10333, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang 和 Rynson WH Lau。Biformer：具有双级路由注意力的视觉变换器。IEEE/CVF
    计算机视觉与模式识别会议论文集，页码 10323–10333，2023。'
- en: '[43] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng
    Gao. Model tells you what to discard: Adaptive kv cache compression for llms.
    arXiv preprint arXiv:2310.01801, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han 和 Jianfeng
    Gao。模型告诉你要丢弃什么：LLM 的自适应 KV 缓存压缩。arXiv 预印本 arXiv:2310.01801，2023。'
- en: '[44] Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang,
    and Dahua Lin. Skvq: Sliding-window key and value cache quantization for large
    language models. arXiv preprint arXiv:2405.06219, 2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang
    和 Dahua Lin。Skvq：大规模语言模型的滑动窗口键值缓存量化。arXiv 预印本 arXiv:2405.06219，2024。'
- en: '[45] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth 和 Song
    Han。Smoothquant：大规模语言模型的准确且高效的后训练量化。国际机器学习会议，页码 38087–38099。PMLR，2023。'
- en: '[46] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng,
    Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit
    quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng,
    Luis Ceze, Arvind Krishnamurthy, Tianqi Chen 和 Baris Kasikci。Atom：用于高效且准确的 LLM
    服务的低比特量化。arXiv 预印本 arXiv:2310.19102，2023。'
- en: '[47] G Kamradt. Needle in a haystack–pressure testing llms, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] G Kamradt。大海捞针——压力测试 LLM，2023。'
- en: '[48] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. Advances in Neural
    Information Processing Systems, 35:16344–16359, 2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra 和 Christopher Ré。Flashattention：具有
    IO 兼容性的快速内存高效准确注意力。神经信息处理系统进展，35:16344–16359，2022。'
- en: '[49] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen,
    Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv
    preprint arXiv:2403.17297, 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen,
    Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, 等. Internlm2 技术报告. arXiv 预印本 arXiv:2403.17297,
    2024.'
- en: '[50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. 2018.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 等. 通过生成预训练提高语言理解能力.
    2018.'
- en: '[51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing,
    568:127063, 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, 和 Yunfeng Liu.
    Roformer: 带有旋转位置嵌入的增强型变换器. Neurocomputing, 568:127063, 2024.'
- en: '[52] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models
    from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebrón, 和 Sumit Sanghai. Gqa: 从多头检查点训练通用多查询变换器模型. arXiv 预印本 arXiv:2305.13245,
    2023.'
- en: '[53] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian
    Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual,
    multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508,
    2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian
    Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, 等. Longbench: 一个双语的、多任务的长期上下文理解基准.
    arXiv 预印本 arXiv:2308.14508, 2023.'
- en: '[54] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin,
    and Mikhail Burtsev. In search of needles in a 10m haystack: Recurrent memory
    finds what llms miss. arXiv preprint arXiv:2402.10790, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin,
    和 Mikhail Burtsev. 在 10 米干草堆中寻找针: 循环记忆找到 LLM 遗漏的内容. arXiv 预印本 arXiv:2402.10790,
    2024.'
- en: '[55] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat, 等. Gpt-4 技术报告. arXiv 预印本 arXiv:2303.08774, 2023.'
- en: Appendix A Appendix
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Proof of Theorems
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 定理证明
- en: 'For Theorem [1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 Theoretical Foundation ‣ 3
    Foundation of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"),'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '对于定理 [1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 Theoretical Foundation ‣ 3 Foundation
    of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention"),'
- en: Proof.
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: In the worst case, we can set the attention mask M to all ones, ensuring that
    the sparse attention score $\tilde{\textbf{P}}$. With that attention mask,
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在最坏的情况下，我们可以将注意力掩码 M 设置为全 1，确保稀疏注意力得分 $\tilde{\textbf{P}}$。使用该注意力掩码，
- en: '|  | $1$2 |  | (7) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: Therefore, $||\tilde{\textbf{O}}-\textbf{O}||_{1}\leq\epsilon$, completing the
    proof. ∎
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$||\tilde{\textbf{O}}-\textbf{O}||_{1}\leq\epsilon$，完成证明。∎
- en: 'For Theorem [2](#Thmtheorem2 "Theorem 2\. ‣ 4.1 Problem Formulation ‣ 4 SampleAttention
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"),'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '对于定理 [2](#Thmtheorem2 "Theorem 2\. ‣ 4.1 Problem Formulation ‣ 4 SampleAttention
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention"),'
- en: Proof.
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'In the worst case, we can also set the attention mask $\hat{\textbf{M}}$ is
    identical to the original attention score P. In this case, $\hat{\textbf{M}}$
    maintains the decomposed structured sparse pattern. Therefore, following the proof
    of Theorem [1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 Theoretical Foundation ‣ 3 Foundation
    of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention") completes
    the proof. ∎'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '在最坏的情况下，我们还可以将注意力掩码 $\hat{\textbf{M}}$ 设置为与原始注意力得分 P 相同。在这种情况下，$\hat{\textbf{M}}$
    维持了分解的结构稀疏模式。因此，按照定理 [1](#Thmtheorem1 "Theorem 1\. ‣ 3.1 Theoretical Foundation
    ‣ 3 Foundation of Near-Lossless Sparse Attention ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")
    的证明完成证明。∎'
- en: A.2 Detailed results
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 详细结果
- en: 'Figure [7](#A1.F7 "Figure 7 ‣ A.2 Detailed results ‣ Appendix A Appendix ‣
    SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention") and Figure [8](#A1.F8 "Figure 8 ‣ A.2 Detailed
    results ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration of
    Long Context LLM Inference with Adaptive Structured Sparse Attention") report
    the detailed scores of the two evaluated models on the BABILong and "Needle in
    a Haystack" tasks across different sequence lengths, respectively. For settings
    of the baselines and overall score statistics, please refer to Section [5.2](#S5.SS2
    "5.2 Accuracy Results ‣ 5 Experiments ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention").'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#A1.F7 "图 7 ‣ A.2 详细结果 ‣ 附录 A 附录 ‣ SampleAttention: 近乎无损的长上下文 LLM 推理加速与自适应结构稀疏注意力")
    和图 [8](#A1.F8 "图 8 ‣ A.2 详细结果 ‣ 附录 A 附录 ‣ SampleAttention: 近乎无损的长上下文 LLM 推理加速与自适应结构稀疏注意力")
    报告了两个评估模型在 BABILong 和 “干草堆里的针” 任务中的详细分数，分别针对不同的序列长度。有关基线设置和总体分数统计，请参见第 [5.2](#S5.SS2
    "5.2 准确度结果 ‣ 5 实验 ‣ SampleAttention: 近乎无损的长上下文 LLM 推理加速与自适应结构稀疏注意力") 节。'
- en: '![Refer to caption](img/6a664e699e17a6e46f892e18d9eb612f.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6a664e699e17a6e46f892e18d9eb612f.png)'
- en: (a) Full attention
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 全注意力
- en: '![Refer to caption](img/96c642ce2ce0f643ada0083e3ff84360.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/96c642ce2ce0f643ada0083e3ff84360.png)'
- en: (b) SampleAttention
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: (b) SampleAttention
- en: '![Refer to caption](img/f45265c1e9ddbeff4442964f222545f2.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f45265c1e9ddbeff4442964f222545f2.png)'
- en: (c) BigBrid
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: (c) BigBrid
- en: '![Refer to caption](img/4137b667ea4139c114a129ee52c3d275.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4137b667ea4139c114a129ee52c3d275.png)'
- en: (d) StreamingLLM
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (d) StreamingLLM
- en: '![Refer to caption](img/6589c94d1f697e0074ac13f3cc529397.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6589c94d1f697e0074ac13f3cc529397.png)'
- en: (e) Full attention
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 全注意力
- en: '![Refer to caption](img/7173e90e339c5096e6d36c8a7e17d8a6.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7173e90e339c5096e6d36c8a7e17d8a6.png)'
- en: (f) SampleAttention
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: (f) SampleAttention
- en: '![Refer to caption](img/0f7c5e0e57849a20f3b380e9b6ae0e65.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f7c5e0e57849a20f3b380e9b6ae0e65.png)'
- en: (g) BigBrid
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: (g) BigBrid
- en: '![Refer to caption](img/35449fdcf1b2776869be8d4cedd3ff4a.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/35449fdcf1b2776869be8d4cedd3ff4a.png)'
- en: (h) StreamingLLM
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: (h) StreamingLLM
- en: 'Figure 7: Detailed results of the evaluations on the BABILong benchmark: (a),
    (b), (c), and (d) are based on the ChatGLM-6B model, while (e), (f), (g), and
    (h) are based on the InternLM2-7B model.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '图7: 在 BABILong 基准上的评估详细结果: (a)、(b)、(c) 和 (d) 基于 ChatGLM-6B 模型，而 (e)、(f)、(g)
    和 (h) 基于 InternLM2-7B 模型。'
- en: '![Refer to caption](img/1966e7a2f8140fbc8a3591100f8cf142.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1966e7a2f8140fbc8a3591100f8cf142.png)'
- en: (a) Full attention
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 全注意力
- en: '![Refer to caption](img/0af63c2d7bf59bb38cf7a05455ce6ac8.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0af63c2d7bf59bb38cf7a05455ce6ac8.png)'
- en: (b) SampleAttention
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: (b) SampleAttention
- en: '![Refer to caption](img/7a210177c85a5e5e141a07347dbb8709.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7a210177c85a5e5e141a07347dbb8709.png)'
- en: (c) BigBrid
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: (c) BigBrid
- en: '![Refer to caption](img/c14d0e1d3596339e21f7a0593b1d9513.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c14d0e1d3596339e21f7a0593b1d9513.png)'
- en: (d) StreamingLLM
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: (d) StreamingLLM
- en: '![Refer to caption](img/8020b6765a75891d4ae5523c54525af7.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8020b6765a75891d4ae5523c54525af7.png)'
- en: (e) Full attention
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 全注意力
- en: '![Refer to caption](img/b06f917260f8a343e6675e8e2155f6b0.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b06f917260f8a343e6675e8e2155f6b0.png)'
- en: (f) SampleAttention
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: (f) SampleAttention
- en: '![Refer to caption](img/66907bf7832bbab35d729504ead3a866.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66907bf7832bbab35d729504ead3a866.png)'
- en: (g) BigBrid
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (g) BigBrid
- en: '![Refer to caption](img/202bac61cc8d0eb3ae69cb9973610382.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/202bac61cc8d0eb3ae69cb9973610382.png)'
- en: (h) StreamingLLM
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (h) StreamingLLM
- en: 'Figure 8: Detailed results of the evaluations on the "Need in a Haystack" task:
    (a), (b), (c), and (d) are based on the ChatGLM-6B model, while (e), (f), (g),
    and (h) are based on the InternLM2-7B model.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图8: 在“干草堆里的针”任务上的评估详细结果: (a)、(b)、(c) 和 (d) 基于 ChatGLM-6B 模型，而 (e)、(f)、(g) 和
    (h) 基于 InternLM2-7B 模型。'
- en: 'Table [4](#A1.T4 "Table 4 ‣ A.2 Detailed results ‣ Appendix A Appendix ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention") displays the sequence scaling results at the prefill stage
    based on the text-generation-interface serving framework, using the ChatGLM2-6B
    model with 8$\times$NVIDIA A100 GPUs. The parallelism configuration employed is
    TP=4 and PP=2, and a chunking implementation on the sequence length has been used
    for memory-efficiency. The profiled TTFT (Time To First Token) metric and the
    proportion of self-attention modules demonstrate the influence of the attention
    mechanism’s quadratic complexity. As sequence lengths increase, this complexity
    causes a significant rise in the latency of the attention module, which can approach
    around 90% at sequence lengths of 1 million.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表[4](#A1.T4 "Table 4 ‣ A.2 Detailed results ‣ Appendix A Appendix ‣ SampleAttention:
    Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured
    Sparse Attention")显示了基于文本生成接口服务框架的预填阶段的序列扩展结果，使用了8$\times$NVIDIA A100 GPU的ChatGLM2-6B模型。所采用的并行配置为TP=4和PP=2，并且使用了序列长度的分块实现以提高内存效率。分析的TTFT（首次标记时间）指标和自注意力模块的比例展示了注意力机制的二次复杂性影响。随着序列长度的增加，这种复杂性导致了注意力模块的延迟显著上升，在序列长度达到100万时，这一延迟可接近90%。'
- en: 'Table 4: Latency breakdown at the prefill stage (Based on the ChatGLM-6B).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：预填阶段的延迟分解（基于ChatGLM-6B）。
- en: '| Sequence Length | TTFT (ms) | Full Attention (ms) | Precent (%) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | TTFT（毫秒） | 完整注意力（毫秒） | 百分比（%） |'
- en: '| 32K | 1273.4 | 410.4 | 32.2 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 32K | 1273.4 | 410.4 | 32.2 |'
- en: '| 64K | 2917.3 | 1538.1 | 52.7 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 64K | 2917.3 | 1538.1 | 52.7 |'
- en: '| 128K | 7756.5 | 4403.9 | 56.8 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 128K | 7756.5 | 4403.9 | 56.8 |'
- en: '| 256K | 23403.7 | 16839.5 | 72.0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 256K | 23403.7 | 16839.5 | 72.0 |'
- en: '| 512K | 51084.3 | 43477.0 | 85.1 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 512K | 51084.3 | 43477.0 | 85.1 |'
- en: '| 1M | 169653.0 | 148774.1 | 87.7 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 1M | 169653.0 | 148774.1 | 87.7 |'
- en: A.3 Visualization of attention
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 注意力的可视化
- en: 'Figures [9](#A1.F9 "Figure 9 ‣ A.3 Visualization of attention ‣ Appendix A
    Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference
    with Adaptive Structured Sparse Attention") and Figures [10](#A1.F10 "Figure 10
    ‣ A.3 Visualization of attention ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention")
    present the sparse patterns across various heads in the ChatGLM2-6B model (28
    layers x 32 heads) under a sequence length of 61K. We conducted row-by-row filtering
    based on the full attention softmax weight, using a CRA threshold of $\alpha=0.95$,
    and randomly selected four heads from different layers for display.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '图[9](#A1.F9 "Figure 9 ‣ A.3 Visualization of attention ‣ Appendix A Appendix
    ‣ SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with
    Adaptive Structured Sparse Attention")和图[10](#A1.F10 "Figure 10 ‣ A.3 Visualization
    of attention ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration
    of Long Context LLM Inference with Adaptive Structured Sparse Attention")展示了ChatGLM2-6B模型（28层
    x 32头）在61K序列长度下的稀疏模式。我们基于完整的注意力softmax权重进行了逐行过滤，使用了CRA阈值$\alpha=0.95$，并随机选择了来自不同层的四个头进行展示。'
- en: 'According to the visualization results on the majority of heads, we observed
    two distinct and prominent patterns prevalent in the heatmap of attention weight:
    column stripes and local windows. Column stripe patterns embody the global contextual
    information whereas diagonal window patterns capture local information.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 根据大多数头部的可视化结果，我们观察到在注意力权重的热图中存在两种明显的模式：列条纹和局部窗口。列条纹模式体现了全局上下文信息，而对角线窗口模式则捕捉了局部信息。
- en: '![Refer to caption](img/48a1119234718bd8c23ea5f110251adc.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/48a1119234718bd8c23ea5f110251adc.png)'
- en: (a) Layer0
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Layer0
- en: '![Refer to caption](img/5edf9121db3b91a9f0b2c3eb3b9e75e2.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5edf9121db3b91a9f0b2c3eb3b9e75e2.png)'
- en: (b) Layer0
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Layer0
- en: '![Refer to caption](img/fd2d1b8d0ff441ac8be57cd1decb0e77.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fd2d1b8d0ff441ac8be57cd1decb0e77.png)'
- en: (c) Layer0
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Layer0
- en: '![Refer to caption](img/1c9f30c8082e33fa1bf9cab84ab9180f.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c9f30c8082e33fa1bf9cab84ab9180f.png)'
- en: (d) Layer0
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Layer0
- en: '![Refer to caption](img/bacd1e1e177c4869233c8b4cdf719e81.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bacd1e1e177c4869233c8b4cdf719e81.png)'
- en: (e) Layer4
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Layer4
- en: '![Refer to caption](img/2b2316233bfceba83adbe98e6f9c099b.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b2316233bfceba83adbe98e6f9c099b.png)'
- en: (f) Layer4
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Layer4
- en: '![Refer to caption](img/5c490c0f971bae8736f99f565c138d36.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5c490c0f971bae8736f99f565c138d36.png)'
- en: (g) Layer4
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: (g) Layer4
- en: '![Refer to caption](img/2796bab26799d43ac594b341ba9c06eb.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2796bab26799d43ac594b341ba9c06eb.png)'
- en: (h) Layer4
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: (h) Layer4
- en: '![Refer to caption](img/f79f3c0f87fdc0d78d6ed47ef07bb5ac.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f79f3c0f87fdc0d78d6ed47ef07bb5ac.png)'
- en: (i) Layer8
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: (i) Layer8
- en: '![Refer to caption](img/590bdf18e7169d3697249e6469c6da83.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/590bdf18e7169d3697249e6469c6da83.png)'
- en: (j) Layer8
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: (j) Layer8
- en: '![Refer to caption](img/4784729f933c22669afb106c859ff4e3.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4784729f933c22669afb106c859ff4e3.png)'
- en: (k) Layer8
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: (k) Layer8
- en: '![Refer to caption](img/e9d9899d0753fed264bfdaac05b77243.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e9d9899d0753fed264bfdaac05b77243.png)'
- en: (l) Layer8
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: (l) Layer8
- en: '![Refer to caption](img/d647f6d6de405e6a7d40c8571f3fbe7c.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d647f6d6de405e6a7d40c8571f3fbe7c.png)'
- en: (m) Layer12
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: (m) Layer12
- en: '![Refer to caption](img/b3dd88a44d218553ed7dc812b2ca22bf.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b3dd88a44d218553ed7dc812b2ca22bf.png)'
- en: (n) Layer12
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: (n) Layer12
- en: '![Refer to caption](img/fe20981bbf6d142a2a462252fd6f2f8a.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fe20981bbf6d142a2a462252fd6f2f8a.png)'
- en: (o) Layer12
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: (o) Layer12
- en: '![Refer to caption](img/fcbce890d2f8b58c1486e17b13135ad3.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fcbce890d2f8b58c1486e17b13135ad3.png)'
- en: (p) Layer12
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: (p) Layer12
- en: 'Figure 9: The visualization attention based on a content length of 61K, displays
    the sparse patterns for randomly chosen heads from layers 0, 4, 8 and 12.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：基于内容长度为 61K 的可视化注意力，展示了来自层 0、4、8 和 12 的随机选择的头部的稀疏模式。
- en: '![Refer to caption](img/6150a3c749f4934a0f798c309bb8f503.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6150a3c749f4934a0f798c309bb8f503.png)'
- en: (a) Layer16
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Layer16
- en: '![Refer to caption](img/a678f3c2e8e301b1121309351075c50e.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a678f3c2e8e301b1121309351075c50e.png)'
- en: (b) Layer16
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Layer16
- en: '![Refer to caption](img/f9bf6a4e8dd1b124f165a1f760b12b88.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f9bf6a4e8dd1b124f165a1f760b12b88.png)'
- en: (c) Layer16
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Layer16
- en: '![Refer to caption](img/39167f8a9fa50c6f371bcb597bd2158e.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/39167f8a9fa50c6f371bcb597bd2158e.png)'
- en: (d) Layer16
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Layer16
- en: '![Refer to caption](img/f51fc7fdf1c62522a8a324bebe8f907a.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f51fc7fdf1c62522a8a324bebe8f907a.png)'
- en: (e) Layer20
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Layer20
- en: '![Refer to caption](img/79a78c1b96773367e3332ae86fc643f4.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/79a78c1b96773367e3332ae86fc643f4.png)'
- en: (f) Layer20
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Layer20
- en: '![Refer to caption](img/74a3fc0c2031ba3981b9dbc5596ebe18.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/74a3fc0c2031ba3981b9dbc5596ebe18.png)'
- en: (g) Layer20
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: (g) Layer20
- en: '![Refer to caption](img/27a2a3b5924f3f29b9dcbd49c852d42f.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/27a2a3b5924f3f29b9dcbd49c852d42f.png)'
- en: (h) Layer20
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: (h) Layer20
- en: '![Refer to caption](img/c6c4fdacda76faa23dfa792dab202cac.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c6c4fdacda76faa23dfa792dab202cac.png)'
- en: (i) Layer24
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: (i) Layer24
- en: '![Refer to caption](img/d65b8c695d2d2005d0ed9c50fa99a24c.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d65b8c695d2d2005d0ed9c50fa99a24c.png)'
- en: (j) Layer24
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: (j) Layer24
- en: '![Refer to caption](img/00a2f27a4fdc0ba501140116c84acb17.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/00a2f27a4fdc0ba501140116c84acb17.png)'
- en: (k) Layer24
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: (k) Layer24
- en: '![Refer to caption](img/76e853c55fa04011d11f2acdb5fcbbc3.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/76e853c55fa04011d11f2acdb5fcbbc3.png)'
- en: (l) Layer24
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: (l) Layer24
- en: 'Figure 10: The visualization attention based on a content length of 61K, displays
    the sparse patterns for randomly chosen heads from layers 16, 20 and 24.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：基于内容长度为 61K 的可视化注意力，展示了来自层 16、20 和 24 的随机选择的头部的稀疏模式。
- en: A.4 Sparisty analysis
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 稀疏性分析
- en: 'To further quantify the degree of sparsity exposed as sequence lengths increase,
    we conducted scalability tests on the ChatGLM2-6B model using the "Needle-in-a-Haystack"
    task to evaluate sparsity. The results are presented in Table [5](#A1.T5 "Table
    5 ‣ A.4 Sparisty analysis ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention").
    According to the results, the increase in sequence length introduces more apparent
    sparsity. With each doubling of length, the proportion of KV elements needed to
    maintain the same threshold $\alpha$ dimension for heads exhibiting different
    degrees of sparsity.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步量化序列长度增加所暴露出的稀疏程度，我们在 ChatGLM2-6B 模型上进行了可扩展性测试，使用了“针在干草堆中”任务来评估稀疏性。结果展示在表 [5](#A1.T5
    "表 5 ‣ A.4 稀疏性分析 ‣ 附录 A 附录 ‣ 示例注意力：使用自适应结构稀疏注意力的长上下文 LLM 推理的近无损加速")中。根据结果，序列长度的增加引入了更明显的稀疏性。每次长度翻倍，需要的
    KV 元素比例保持相同阈值 $\alpha$ 维度，以应对具有不同稀疏程度的头部。
- en: 'Table 5: Sparsity analysis for ChatGLM2-6B model as sequence length scales.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：ChatGLM2-6B 模型在序列长度缩放下的稀疏性分析。
- en: '| Sequence length | Average SD ($\alpha$=0.95) | Average SD ($\alpha$=0.98)
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 平均 SD ($\alpha$=0.95) | 平均 SD ($\alpha$=0.98) |'
- en: '| 4K | 91.27% | 88.00% | 79.17% |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 4K | 91.27% | 88.00% | 79.17% |'
- en: '| 8K | 93.68% | 90.74% | 83.43% |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 8K | 93.68% | 90.74% | 83.43% |'
- en: '| 16K | 95.84% | 92.52% | 86.37% |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 16K | 95.84% | 92.52% | 86.37% |'
- en: '| 32K | 96.34% | 93.88% | 88.68% |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 32K | 96.34% | 93.88% | 88.68% |'
- en: '| 64K | 96.91% | 94.89% | 90.70% |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 64K | 96.91% | 94.89% | 90.70% |'
- en: '| 128K | 97.44% | 95.84% | 92.43% |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 128K | 97.44% | 95.84% | 92.43% |'
- en: '![Refer to caption](img/b30a4ec87c459fb4169bb63ecda2e861.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b30a4ec87c459fb4169bb63ecda2e861.png)'
- en: 'Figure 11: The frequency reduction results for the retained KV elements in
    the Sk dimension on two randomly selected heads. The SD ($\alpha=0.95$) for the
    left head under sequence length of 61K is 41.2%, while 97.5% for the right head.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在两个随机选择的头部中 Sk 维度上保留的 KV 元素的频率减少结果。在序列长度为 61K 的左侧头部的 SD ($\alpha=0.95$)
    为 41.2%，而右侧头部为 97.5%。
- en: A.5 Effectiveness of sampling
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 采样的有效性
- en: 'To verify the efficiency of this sampling method, we conducted tests on different
    heads using two distinct sampling ratios $r_{w}$. We applied different ratios
    of top-k stripes combined with a tuned window mask to the full attention matrices
    to observe the changes in CRA. The results, as shown in Table [6](#A1.T6 "Table
    6 ‣ A.5 Effectiveness of sampling ‣ Appendix A Appendix ‣ SampleAttention: Near-Lossless
    Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention"),
    indicate that the CRA achieved by selecting top-k stripes at a 5% sampling ratio
    is remarkably close to that obtained from the full attention score. This confirms
    that SampleAttention’s simple sampling method is highly efficient.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这种采样方法的效率，我们在不同的头部上使用了两种不同的采样比例 $r_{w}$ 进行了测试。我们将不同比例的 top-k 条纹与调优后的窗口掩码结合应用于完整的注意力矩阵，以观察
    CRA 的变化。结果如表[6](#A1.T6 "表 6 ‣ A.5 采样的有效性 ‣ 附录 A 附录 ‣ SampleAttention：具有自适应结构稀疏注意力的长上下文
    LLM 推理的近无损加速") 所示，选择 top-k 条纹并使用 5% 采样比例获得的 CRA 与从完整注意力分数中获得的 CRA 非常接近。这证实了 SampleAttention
    的简单采样方法具有很高的效率。
- en: 'Table 6: The CRA percentages that can be achieved by selecting different ratios
    of top-k stripes under different sampling ratios for each head. The sequence length
    of tested content is 61K.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：通过选择不同的 top-k 条纹比例在不同采样比例下每个头部可以实现的 CRA 百分比。测试内容的序列长度为 61K。
- en: ratio of top-k stripes 2.5% 5% 10% 20% 40% 80% sampling ratio 100% 5% 100% 5%
    100% 5% 100% 5% 100% 5% 100% 5% Layer0-Head0 10.60% 10.31% 17.85% 17.74% 29.49%
    28.83% 47.09% 46.14% 71.19% 70.15% 97.12% 96.65% Layer13-Head0 75.29% 65.62% 80.57%
    74.89% 86.33% 81.58% 92.09% 89.98% 97.07% 95.21% 99.85% 98.68% Layer13-Head13
    98.24% 97.85% 98.63% 98.29% 99.02% 98.73% 99.41% 99.12% 99.76% 99.66% 100.00%
    99.80a%
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: top-k 条纹的比例 2.5% 5% 10% 20% 40% 80% 采样比例 100% 5% 100% 5% 100% 5% 100% 5% 100%
    5% 100% 5% Layer0-Head0 10.60% 10.31% 17.85% 17.74% 29.49% 28.83% 47.09% 46.14%
    71.19% 70.15% 97.12% 96.65% Layer13-Head0 75.29% 65.62% 80.57% 74.89% 86.33% 81.58%
    92.09% 89.98% 97.07% 95.21% 99.85% 98.68% Layer13-Head13 98.24% 97.85% 98.63%
    98.29% 99.02% 98.73% 99.41% 99.12% 99.76% 99.66% 100.00% 99.80%
- en: A.6 Limitations and Future Work
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6 局限性和未来工作
- en: We discuss limitations of SampleAttention and future directions in this subsection.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一小节中讨论了 SampleAttention 的局限性和未来方向。
- en: Other pattern and sampling.
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他模式和采样。
- en: We also identified additional diagonal structures in heads with lower sparsity
    levels. Although SampleAttention is capable of covering these areas by selecting
    an adequate proportion of KVs, accurately capturing these patterns could potentially
    lead to further performance enhancements. Additionally, considering the time overhead
    associated with sampling, how to further improve sampling efficiency to achieve
    acceleration even at shorter sequence lengths remains an important challenge for
    future research.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在稀疏度较低的头部中识别出了额外的对角结构。尽管 SampleAttention 能够通过选择适当比例的 KVs 来覆盖这些区域，但准确捕捉这些模式可能会进一步提升性能。此外，考虑到与采样相关的时间开销，如何进一步提高采样效率以在较短序列长度下实现加速仍然是未来研究中的一个重要挑战。
- en: Hyperparameter tuning.
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数调整。
- en: The experimental results demonstrate that hyperparameters substantially influence
    the trade-off between task performance and speedup. Consequently, swiftly determining
    efficient hyperparameters for a specific model emerges as a critical challenge.
    In the future, we aim to implement autotuning of these hyperparameters during
    task runtime, enabling SampleAttention to consistently achieve high accuracy and
    low latency across diverse sequence lengths and scenarios.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，超参数对任务性能与加速之间的权衡有显著影响。因此，快速确定特定模型的有效超参数成为一个关键挑战。在未来，我们旨在实现这些超参数在任务运行时的自动调优，使得
    SampleAttention 能够在各种序列长度和场景下始终实现高准确性和低延迟。
- en: Serving.
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 服务。
- en: After integrating SampleAttention into the distributed serving framework, we
    found that requests with ultra-long sequences (>=128K) or large batch sizes will
    cause memory issues. More engineering efforts are required to achieve memory efficiency,
    potentially through strategies like implementing pipeline or sequence parallelism
    and chunking along the sequence dimension.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 将 SampleAttention 集成到分布式服务框架中后，我们发现请求超长序列（>=128K）或大批量大小会导致内存问题。需要更多的工程工作来实现内存效率，可能需要通过实现流水线或序列并行以及沿序列维度进行分块等策略来解决。
- en: A.7 PyTorch-Style Implementation Algorithm
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7 PyTorch风格的实现算法
- en: 'Algorithm [1](#algorithm1 "In A.7 PyTorch-Style Implementation Algorithm ‣
    Appendix A Appendix ‣ SampleAttention: Near-Lossless Acceleration of Long Context
    LLM Inference with Adaptive Structured Sparse Attention") presents a succinct
    pseudo-code of the SampleAttention’s implementation in the PyTorch style. Link
    to the source code based on PyTorch and Triton, along with scripts to reproduce
    the main experimental results, will be provided in the camera-ready version.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [1](#algorithm1 "在 A.7 PyTorch风格的实现算法 ‣ 附录 A 附录 ‣ SampleAttention：通过自适应结构化稀疏注意力实现长上下文
    LLM 推理的近乎无损加速") 展示了 PyTorch 风格下 SampleAttention 实现的简洁伪代码。基于 PyTorch 和 Triton 的源代码链接以及重现主要实验结果的脚本将在最终版本中提供。
- en: 'Input:$\textbf{Q}\in\mathbb{R}^{Sq\times d},\textbf{K}\in\mathbb{R}^{Sk\times
    d},\textbf{V}\in\mathbb{R}^{Sk\times d},\alpha\in(0,1),r_{row}\in(0,1),r_{w}\in\mathbb{N}$)SortedWeight
    = SampleWeight.sort(dim=-1)WeightSum = SortedWeight.sum(dim=-1)# Stage2: Score-Based
    Key-Value Filtering# example prefixsum_sample_list=[0.0125, 0.025,0.05,0.1,0.2,0.4,0.8,1.0]
    * SkSD_sample_list = SortedWeight[::,:prefixsum_sample_list].sum()/WeightSumKV_ratio_per_head
    = searchsorted(SD_sample_list, $\alpha$_per_head = gather_KV_Index(SortedWeight.idx,
    KV_ratio_per_head)# Sparse computation of the attention# combined mask of $I_{KV}$M_Merged
    = merge_mask( $I_{KV}$)Output = sparse_flash_attn(Q, K, V, M_Merged)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：$\textbf{Q}\in\mathbb{R}^{Sq\times d},\textbf{K}\in\mathbb{R}^{Sk\times
    d},\textbf{V}\in\mathbb{R}^{Sk\times d},\alpha\in(0,1),r_{row}\in(0,1),r_{w}\in\mathbb{N}$)SortedWeight
    = SampleWeight.sort(dim=-1)WeightSum = SortedWeight.sum(dim=-1)# Stage2: 基于评分的键值过滤#
    例如 prefixsum_sample_list=[0.0125, 0.025,0.05,0.1,0.2,0.4,0.8,1.0] * SkSD_sample_list
    = SortedWeight[::,:prefixsum_sample_list].sum()/WeightSumKV_ratio_per_head = searchsorted(SD_sample_list,
    $\alpha$_per_head = gather_KV_Index(SortedWeight.idx, KV_ratio_per_head)# 稀疏计算注意力#
    $I_{KV}$ 的合并掩码 M_Merged = merge_mask( $I_{KV}$)输出 = sparse_flash_attn(Q, K, V,
    M_Merged)'
- en: Algorithm 1 Implementation of SampleAttention
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 SampleAttention的实现
