- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:03:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:03:24'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种快速优化视角：基于张量和支持向量机技巧的单层注意力重构及其矩阵乘法时间解决方案
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.07418](https://ar5iv.labs.arxiv.org/html/2309.07418)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.07418](https://ar5iv.labs.arxiv.org/html/2309.07418)
- en: Yeqi Gao a916755226@gmail.com. The University of Washington.    Zhao Song zsong@adobe.com.
    Adobe Research.    Weixin Wang wwang176@jh.edu. Johns Hopkins University.    Junze
    Yin junze@bu.edu. Boston University.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yeqi Gao a916755226@gmail.com. 华盛顿大学。    Zhao Song zsong@adobe.com. Adobe Research.
       Weixin Wang wwang176@jh.edu. 约翰斯·霍普金斯大学。    Junze Yin junze@bu.edu. 波士顿大学。
- en: Large language models have played a pivotal role in revolutionizing various
    facets of our daily existence. Serving as the cornerstone of virtual assistants,
    they have seamlessly streamlined information retrieval and task automation. Spanning
    domains from healthcare to education, these models have made an enduring impact,
    elevating productivity, decision-making processes, and accessibility, thereby
    influencing and, to a certain extent, reshaping the lifestyles of people.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在彻底改变我们日常生活的各个方面中发挥了关键作用。作为虚拟助手的基石，它们无缝地简化了信息检索和任务自动化。从医疗保健到教育，这些模型产生了持久的影响，提升了生产力、决策过程和可及性，从而影响并在某种程度上重塑了人们的生活方式。
- en: Solving attention regression is a fundamental task in optimizing LLMs. In this
    work, we focus on giving a provable guarantee for the one-layer attention network
    objective function
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 解决注意力回归是优化大型语言模型（LLMs）的一个基本任务。在这项工作中，我们专注于为单层注意力网络的目标函数提供可证明的保证。
- en: '|  | $1$2 |  |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Here $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$ and $A_{2}\in\mathbb{R}^{n\times
    d}$ is a matrix in $\mathbb{R}^{n\times d}$ is the $j_{0}$. The $X,Y\in\mathbb{R}^{d\times
    d}$ and $b_{j_{0},i_{0}}\in\mathbb{R}$-th row and $i_{0}$, $Y_{*,i_{0}}\in\mathbb{R}^{d}$-column
    vector of $Y$ is the vectorization of $X$.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$ 和 $A_{2}\in\mathbb{R}^{n\times
    d}$ 是 $\mathbb{R}^{n\times d}$ 中的一个矩阵，是 $j_{0}$。$X,Y\in\mathbb{R}^{d\times d}$
    和 $b_{j_{0},i_{0}}\in\mathbb{R}$-th 行和 $i_{0}$，$Y_{*,i_{0}}\in\mathbb{R}^{d}$-列向量
    $Y$ 是 $X$ 的向量化。
- en: In a multi-layer LLM network, the matrix $B\in\mathbb{R}^{n\times d}$ can be
    viewed as the input of a layer. The matrix version of $x$ and $Y$. We provide
    an iterative greedy algorithm to train loss function $L(X,Y)$ that runs in $\widetilde{O}(({\cal
    T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon))$
    denotes the time of multiplying $a\times b$ matrix, and $\omega\approx 2.37$ denotes
    the exponent of matrix multiplication.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在多层 LLM 网络中，矩阵 $B\in\mathbb{R}^{n\times d}$ 可以被视为一层的输入。矩阵版本的 $x$ 和 $Y$。我们提供了一种迭代贪婪算法来训练损失函数
    $L(X,Y)$，其运行时间为 $\widetilde{O}(({\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon))$，其中
    $\omega\approx 2.37$ 表示矩阵乘法的指数。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) like GPT-1 [[149](#bib.bib149)], BERT [[49](#bib.bib49)],
    GPT-2 [[154](#bib.bib154)], GPT-3 [[24](#bib.bib24)], ChatGPT [[35](#bib.bib35)],
    GPT-4 [[134](#bib.bib134)], OPT [[209](#bib.bib209)], Llama [[174](#bib.bib174)],
    and Llama 2 [[176](#bib.bib176)] have demonstrated impressive capabilities in
    natural language processing (NLP). These models understand and generate complex
    language, enabling a wide range of applications such as sentiment analysis [[200](#bib.bib200)],
    language translation [[1](#bib.bib1)], question answering [[23](#bib.bib23)],
    and text summarization [[137](#bib.bib137)]. Despite their high-quality performance,
    there remains untapped potential in optimizing and training these massive models,
    making it a challenging endeavor in the present day.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-1 [[149](#bib.bib149)]、BERT [[49](#bib.bib49)]、GPT-2 [[154](#bib.bib154)]、GPT-3
    [[24](#bib.bib24)]、ChatGPT [[35](#bib.bib35)]、GPT-4 [[134](#bib.bib134)]、OPT [[209](#bib.bib209)]、Llama
    [[174](#bib.bib174)] 和 Llama 2 [[176](#bib.bib176)] 这样的语言模型在自然语言处理（NLP）领域展示了令人印象深刻的能力。这些模型理解并生成复杂的语言，使得情感分析
    [[200](#bib.bib200)]、语言翻译 [[1](#bib.bib1)]、问答 [[23](#bib.bib23)] 和文本摘要 [[137](#bib.bib137)]
    等广泛应用成为可能。尽管它们的表现非常优秀，但在优化和训练这些大型模型方面仍有未开发的潜力，使得这一工作在当今仍然具有挑战性。
- en: The primary technical foundation supporting the capabilities of LLMs is the
    attention matrix [[149](#bib.bib149), [179](#bib.bib179), [24](#bib.bib24), [49](#bib.bib49)].
    The central concept of attention is to learn representations that emphasize the
    most relevant parts of the input. To be more specific, the attention mechanism
    compares the query vectors (the output tokens) with the key vectors (the input
    tokens). The attention weights are then determined based on the similarity of
    this comparison, indicating the relative importance of each input token. These
    attention weights are used to compute weighted averages of the value vectors,
    resulting in the output representation. By leveraging attention, LLMs acquire
    the ability to focus on the crucial aspects of the input, allowing them to gather
    pertinent information more efficiently and precisely. This capability enables
    LLMs to process longer texts effectively and comprehend intricate semantic relationships.
    Notably, the self-attention mechanism enables LLMs to establish connections between
    various segments of the input sequence, enhancing their contextual understanding.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 支撑 LLM 功能的主要技术基础是注意力矩阵 [[149](#bib.bib149), [179](#bib.bib179), [24](#bib.bib24),
    [49](#bib.bib49)]。注意力的核心概念是学习强调输入中最相关部分的表示。更具体地说，注意力机制将查询向量（输出标记）与键向量（输入标记）进行比较。然后，根据这种比较的相似性来确定注意力权重，表示每个输入标记的相对重要性。这些注意力权重用于计算值向量的加权平均，从而得到输出表示。通过利用注意力，LLMs
    能够专注于输入的关键方面，使它们能够更有效和准确地收集相关信息。这一能力使 LLMs 能够有效处理更长的文本，并理解复杂的语义关系。值得注意的是，自注意力机制使
    LLMs 能够在输入序列的各个部分之间建立联系，增强其上下文理解能力。
- en: We start with defining the general Attention forward layer,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从定义一般的注意力前向层开始，
- en: Definition 1.1  ($\ell$-th layer forward computation).
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1.1 （$\ell$-层前向计算）。
- en: 'Let ${\bf 1}_{n}$-dimensional vector whose entries are all $1$ be a function:
    each entry of the vector in $\mathbb{R}^{n}$ and other entries of this matrix
    are all $0$, let $X_{\ell}\in\mathbb{R}^{n\times d}$-th layer input and $X_{\ell+1}\in\mathbb{R}^{n\times
    d}$'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 设 ${\bf 1}_{n}$-维向量，其所有条目均为 $1$ 为一个函数：向量在 $\mathbb{R}^{n}$ 中的每个条目及该矩阵的其他条目均为
    $0$，设 $X_{\ell}\in\mathbb{R}^{n\times d}$-层输入和 $X_{\ell+1}\in\mathbb{R}^{n\times
    d}$
- en: '|  | $\displaystyle X_{\ell+1}\leftarrow D^{-1}\exp(X_{\ell}QK^{\top}X_{\ell}^{\top})X_{\ell}V$
    |  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle X_{\ell+1}\leftarrow D^{-1}\exp(X_{\ell}QK^{\top}X_{\ell}^{\top})X_{\ell}V$
    |  |'
- en: where $D:=\operatorname{diag}(\exp(X_{\ell}QK^{\top}X_{\ell}^{\top}){\bf 1}_{n})$
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D:=\operatorname{diag}(\exp(X_{\ell}QK^{\top}X_{\ell}^{\top}){\bf 1}_{n})$
- en: 'Mathematically, a general optimization with respect to attention computation
    is defined as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，关于注意力计算的一般优化定义为：
- en: Definition 1.2  (Attention optimization).
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1.2 （注意力优化）。
- en: 'Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$. The attention computation
    is defined as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$。注意力计算定义为：
- en: '|  | $1$2 |  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $D(X)\in\mathbb{R}^{n\times n}$.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D(X)\in\mathbb{R}^{n\times n}$。
- en: '![Refer to caption](img/4299be57d79d62ad6bdc597ce50480f9.png)![Refer to caption](img/00d76d08bdd817b3626571f58ccdc732.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4299be57d79d62ad6bdc597ce50480f9.png)![参见标题](img/00d76d08bdd817b3626571f58ccdc732.png)'
- en: 'Figure 1: The visualization of the attention optimization (see Definition [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$. We first get $\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times n}$, $X$. Then,
    we have $D(X)\in\mathbb{R}^{n\times n}$. After that, we multiply $D(X)^{-1}$,
    $A_{3}$ and subtract $B$ matrices, the purple rectangle represents the $n$ matrices,
    and the green squares represent the $n\times n$ diagonal matrices.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：注意力优化的可视化（见定义 [1.2](#S1.Thmtheorem2 "定义 1.2（注意力优化）。 ‣ 1 引言 ‣ 快速优化视角：基于张量和支持向量机技巧的单层注意力重构及矩阵乘法时间求解"））。设
    $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$。我们首先得到 $\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times
    n}$，$X$。然后，我们得到 $D(X)\in\mathbb{R}^{n\times n}$。接着，我们对 $D(X)^{-1}$、$A_{3}$ 进行乘法操作，并减去
    $B$ 矩阵，紫色矩形表示 $n$ 矩阵，绿色方块表示 $n\times n$ 对角矩阵。
- en: '![Refer to caption](img/e77a6fab7b9b8d8aa84a3ca6114da1a5.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e77a6fab7b9b8d8aa84a3ca6114da1a5.png)'
- en: 'Figure 2: The visualization of a variation of Definition [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$, $D(X)\in\mathbb{R}^{n\times n}$. $\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times
    n}$, and $\operatorname{vec}=\mathrm{mat}^{-1}$ and multiply $\operatorname{\mathsf{A}}$.
    Then, we multiply $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times n^{2}}$,
    which gives us a vector in $\mathbb{R}^{n^{2}}$ to transform that into a matrix
    in $\mathbb{R}^{n\times n}$. Finally, we compute the minimum of the Frobenius
    norm of $\mathrm{mat}((D(X)\otimes I_{n})^{-1}\cdot\exp(\operatorname{\mathsf{A}}\operatorname{vec}(X)))A_{3}Y-B$:
    in the matrix $D(X)\otimes I_{n}$. The red rectangle represents the matrix in
    $\mathbb{R}^{d\times d}$.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：定义 [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization). ‣ 1
    Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") 的变体的可视化。令 $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$，$D(X)\in\mathbb{R}^{n\times
    n}$。$\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times n}$，$\operatorname{vec}=\mathrm{mat}^{-1}$
    并乘以 $\operatorname{\mathsf{A}}$。然后，我们乘以 $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times
    n^{2}}$，这给我们一个向量 $\mathbb{R}^{n^{2}}$，将其转换为矩阵 $\mathbb{R}^{n\times n}$。最后，我们计算
    $\mathrm{mat}((D(X)\otimes I_{n})^{-1}\cdot\exp(\operatorname{\mathsf{A}}\operatorname{vec}(X)))A_{3}Y-B$
    在矩阵 $D(X)\otimes I_{n}$ 中的弗罗贝纽斯范数的最小值。红色矩形表示 $\mathbb{R}^{d\times d}$ 中的矩阵。'
- en: 'Here $X=QK^{\top},Y=V$ are the input of a layer $X_{\ell}$ are the output layer
    $X_{\ell+1}$. Attention computation has been analyzed in many recent works [[203](#bib.bib203),
    [11](#bib.bib11), [29](#bib.bib29), [75](#bib.bib75), [57](#bib.bib57), [171](#bib.bib171),
    [175](#bib.bib175), [139](#bib.bib139), [125](#bib.bib125), [208](#bib.bib208),
    [140](#bib.bib140), [159](#bib.bib159)], but none of them give a complete analysis
    of the full version of the attention computation problem. They all simplify this
    problem by different strategies (see details in Table [1](#S4.T1 "Table 1 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '这里 $X=QK^{\top},Y=V$ 是层的输入，$X_{\ell}$ 是输出层，$X_{\ell+1}$。注意力计算在许多近期的工作中进行了分析
    [[203](#bib.bib203), [11](#bib.bib11), [29](#bib.bib29), [75](#bib.bib75), [57](#bib.bib57),
    [171](#bib.bib171), [175](#bib.bib175), [139](#bib.bib139), [125](#bib.bib125),
    [208](#bib.bib208), [140](#bib.bib140), [159](#bib.bib159)]，但其中没有一个给出完整的注意力计算问题的分析。它们都通过不同的策略简化了这个问题（详细信息见表 [1](#S4.T1
    "Table 1 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")）。'
- en: However, simplifying this problem may lead to a significant decrease in the
    model performance, which may require extra model training or fine-tuning. This
    results in deployment obstacles.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，简化这个问题可能会导致模型性能显著下降，这可能需要额外的模型训练或微调。这导致了部署障碍。
- en: 'In this paper, our focus is on optimizing the attention mechanism. Our goal
    is to present a complete, un-simplified analysis of the attention problem defined
    in Definition [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), a task that, to the best of our knowledge, has not been done before. We
    provide a provable guarantee for optimizing the attention function in the case
    of a single-layer attention network. Our motivation stems from the critical role
    of the attention optimization problem in the functionality of LLMs, and we firmly
    believe that our theoretical analysis will significantly influence the development
    of LLMs.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的重点是优化注意力机制。我们的目标是对定义在定义 [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention
    optimization). ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") 中的注意力问题进行完整、未简化的分析，这项任务在我们看来尚未完成。我们为单层注意力网络中的注意力函数优化提供了可证明的保证。我们的动机来源于注意力优化问题在
    LLM 功能中的关键作用，我们坚信我们的理论分析将对 LLM 的发展产生重大影响。'
- en: As [[11](#bib.bib11)], they show that one step forward computation of attention
    can be done in $o(n^{2})$ matrix. However, it is still an open problem about how
    fast we optimize the loss function via the iterative method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 [[11](#bib.bib11)] 所示，他们表明，注意力的单步前向计算可以在 $o(n^{2})$ 矩阵中完成。然而，如何通过迭代方法优化损失函数仍然是一个悬而未决的问题。
- en: 'How fast can we optimize the training process of attention matrix (See Definition [1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"))?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '我们能多快优化注意力矩阵的训练过程（参见定义 [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")）？'
- en: In this study, we make progress towards this fundamental question.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们在这个基本问题上取得了进展。
- en: To establish the correctness of our algorithm, we conduct a comprehensive analysis
    of the positive semi-definite (PSD) property and the Lipschitz continuity of the
    Hessian matrix constructed from the attention matrix. These two properties provide
    the necessary assurance for employing TensorSRHT and Newton’s method, ensuring
    both fast computation and convergence, respectively.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们算法的正确性，我们对从注意力矩阵构造的海森矩阵的正半定（PSD）性质和 Lipschitz 连续性进行了全面分析。这两个性质为使用 TensorSRHT
    和牛顿法提供了必要的保证，确保了快速计算和收敛。
- en: Now, we will present our main result as follows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将展示我们的主要结果如下。
- en: Theorem 1.3  (Informal version of our main theorem).
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1.3（我们主要定理的非正式版本）。
- en: Let $A_{1},A_{2},A_{2}\in\mathbb{R}^{n\times d}$ solves to the attention problem
    up to $\epsilon$. Here $\omega\approx 2.37$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $A_{1},A_{2},A_{2}\in\mathbb{R}^{n\times d}$ 解决注意力问题至 $\epsilon$。这里 $\omega\approx
    2.37$。
- en: 'Here $\omega$ denotes the time of multiplying an $a\times b$ size matrix, and
    ${\cal T}_{\mathrm{mat}}(n,n,n)=n^{\omega}$. See more details of matrix multiplication
    notation in Section [4.7](#S4.SS7 "4.7 Fast Matrix Multiplication ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '这里 $\omega$ 表示乘法一个 $a\times b$ 大小矩阵的时间，而 ${\cal T}_{\mathrm{mat}}(n,n,n)=n^{\omega}$。有关矩阵乘法符号的更多细节请参见第
    [4.7](#S4.SS7 "4.7 Fast Matrix Multiplication ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 节。'
- en: Relationship with the Softmax Regression Problem
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 Softmax 回归问题的关系
- en: 'Moreover, the attention weight can be viewed as the output of a softmax regression
    model, which is defined as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意力权重可以视为 softmax 回归模型的输出，定义如下：
- en: Definition 1.4  (Single softmax regression [[55](#bib.bib55)] and multiple softmax
    regression [[72](#bib.bib72)]).
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1.4（单一 softmax 回归 [[55](#bib.bib55)] 和多重 softmax 回归 [[72](#bib.bib72)]）。
- en: Given a matrix $A\in\mathbb{R}^{n\times d}$, the single softmax regression problem
    is defined as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个矩阵 $A\in\mathbb{R}^{n\times d}$，单一 softmax 回归问题定义为
- en: '|  | $1$2 |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Let $D(X)\in\mathbb{R}^{n\times n}$ and $X\in\mathbb{R}^{d\times d}$
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $D(X)\in\mathbb{R}^{n\times n}$ 和 $X\in\mathbb{R}^{d\times d}$
- en: '|  | $1$2 |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'On the one hand, due to the observation in [[72](#bib.bib72), [73](#bib.bib73)],
    the equation in Part 1 of Definition [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single
    softmax regression [55] and multiple softmax regression [72]). ‣ Relationship
    with the Softmax Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") can be viewed as one row of the equation
    in Part 2 of Definition [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single softmax
    regression [55] and multiple softmax regression [72]). ‣ Relationship with the
    Softmax Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '一方面，由于在 [[72](#bib.bib72), [73](#bib.bib73)] 的观察，定义 [1.4](#S1.Thmtheorem4 "Definition
    1.4 (Single softmax regression [55] and multiple softmax regression [72]). ‣ Relationship
    with the Softmax Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") 第1部分的方程可以被视为定义 [1.4](#S1.Thmtheorem4
    "Definition 1.4 (Single softmax regression [55] and multiple softmax regression
    [72]). ‣ Relationship with the Softmax Regression Problem ‣ 1 Introduction ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") 第2部分方程的一行。'
- en: 'On the other hand, due to the well-known tensor trick¹¹1Given matrices $A_{1},A_{2}\in\mathbb{R}^{n\times
    d}$, the well-known tensor-trick suggests that $\operatorname{vec}(A_{1}XA_{2}^{\top})=(A_{1}\otimes
    A_{2})\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$. (see [[58](#bib.bib58), [52](#bib.bib52)]
    as an example), the Part 2 equation Definition [1.4](#S1.Thmtheorem4 "Definition
    1.4 (Single softmax regression [55] and multiple softmax regression [72]). ‣ Relationship
    with the Softmax Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") is equivalent to'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于著名的张量技巧¹¹1 给定矩阵 $A_{1},A_{2}\in\mathbb{R}^{n\times d}$，著名的张量技巧建议 $\operatorname{vec}(A_{1}XA_{2}^{\top})=(A_{1}\otimes
    A_{2})\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$。（参见 [[58](#bib.bib58), [52](#bib.bib52)]
    作为例子），第二部分方程定义 [1.4](#S1.Thmtheorem4 "定义 1.4（单一 softmax 回归 [55] 和多重 softmax 回归
    [72]）。 ‣ 与 softmax 回归问题的关系 ‣ 1 引言 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决")
    相当于
- en: '|  | $1$2 |  | (1) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'which can be a slightly more complicated version of the Part 1 equation in
    Definition [1.4](#S1.Thmtheorem4 "Definition 1.4 (Single softmax regression [55]
    and multiple softmax regression [72]). ‣ Relationship with the Softmax Regression
    Problem ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). In particular, instead of one re-scaling factor, we will have $n$ into
    $n$. For each chunk, we use the same rescaling factor.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以是定义 [1.4](#S1.Thmtheorem4 "定义 1.4（单一 softmax 回归 [55] 和多重 softmax 回归 [72]）。
    ‣ 与 softmax 回归问题的关系 ‣ 1 引言 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决") 中第一部分方程的稍微复杂的版本。特别是，我们将从一个重新缩放因子改为
    $n$。对于每个块，我们使用相同的重新缩放因子。
- en: '![Refer to caption](img/bfca81b3da52836ce3d9be7b8fb0c63c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bfca81b3da52836ce3d9be7b8fb0c63c.png)'
- en: 'Figure 3: The visualization of Eq. ([1](#S1.E1 "In Relationship with the Softmax
    Regression Problem ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). Let $A_{1},A_{2}\in\mathbb{R}^{n\times d}$, $C,D(X)\in\mathbb{R}^{n\times
    n}$. We first get that $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times n^{2}}$
    with $\operatorname{vec}(X)$ with $\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$
    and subtract it from $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))$
    norm of $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))-c$:
    in the matrix $D(X)\otimes I_{n}$.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：方程 ([1](#S1.E1 "与 softmax 回归问题的关系 ‣ 1 引言 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决"))
    的可视化。令 $A_{1},A_{2}\in\mathbb{R}^{n\times d}$，$C,D(X)\in\mathbb{R}^{n\times n}$。我们首先得到
    $(D(X)\otimes I_{n})^{-1}\in\mathbb{R}^{n^{2}\times n^{2}}$ 和 $\operatorname{vec}(X)$
    与 $\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X)\in\mathbb{R}^{n^{2}}$ 的运算，然后从
    $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))$
    中减去 $(D(X)\otimes I_{n})^{-1}\exp(\operatorname{\mathsf{A}}\cdot\operatorname{vec}(X))-c$
    的范数：在矩阵 $D(X)\otimes I_{n}$ 中。
- en: 'Note that the multiple softmax regression problem is a simplified version of
    what we study in Definition [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). We believe that our work can also support the study of softmax regression.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，多重 softmax 回归问题是我们在定义 [1.2](#S1.Thmtheorem2 "定义 1.2（注意力优化）。 ‣ 1 引言 ‣ 快速优化视角：基于张量和
    SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决") 中研究的简化版本。我们相信我们的工作也可以支持 softmax 回归的研究。
- en: Relatinship with Support Vector Machines (SVM)
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与支持向量机（SVM）的关系
- en: The usual SVM [[91](#bib.bib91), [37](#bib.bib37), [77](#bib.bib77), [175](#bib.bib175)]
    objective function in optimization can be viewed as a product of a summation of
    a batch of inner product. Inspired by that, we can define $n$ for each $j_{0}\in[n]$
    functions $h(Y)_{i_{0}}\in\mathbb{R}^{n}$ is the vectorization of $X$ is the vectorization
    of $Y$ can be turned into
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的 SVM [[91](#bib.bib91), [37](#bib.bib37), [77](#bib.bib77), [175](#bib.bib175)]
    优化中的目标函数可以看作是若干内积的求和的乘积。受此启发，我们可以为每个 $j_{0}\in[n]$ 定义 $n$ 个函数 $h(Y)_{i_{0}}\in\mathbb{R}^{n}$，其中
    $X$ 的向量化可以转换为 $Y$ 的向量化。
- en: '|  | $1$2 |  | (2) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $b_{j_{0},i_{0}}$. We call this formulation SVM-inspired formulation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{j_{0},i_{0}}$。我们称这种形式为 SVM 灵感的公式。
- en: '![Refer to caption](img/d73e25196093f8ef6e4aad48d830bd59.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d73e25196093f8ef6e4aad48d830bd59.png)'
- en: 'Figure 4: The visualization of Eq. ([2](#S1.E2 "In Relatinship with Support
    Vector Machines (SVM) ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). Let $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times
    d}$. We have $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ is the $j_{0}$. $x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$ (see Definition [4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")) and $h(Y)_{i_{0}}\in\mathbb{R}^{n}$
    at $j_{0}$-column from the inner produce. Finally, we compute the square of this
    difference and add all of them from $i_{0}=1$ and from $j_{0}=1$. In this figure,
    we use blue rectangles to represent vectors, where the dark blue represents $f(x)_{j_{0}}$,
    and the light blue represents the terms used to compute $f(x)_{j_{0}}$. The green
    square represents the scalar. The red rectangle represents the matrix.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：方程的可视化（[2](#S1.E2 "与支持向量机 (SVM) 的关系 ‣ 1 引言 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间中求解"）。设
    $A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$。我们有 $\operatorname{\mathsf{A}}=A_{1}\otimes
    A_{2}\in\mathbb{R}^{n^{2}\times d^{2}}$ 是 $j_{0}$。$x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$（见定义 [4.10](#S4.Thmtheorem10
    "定义 4.10. ‣ 4.3 与 𝑋 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间中求解"））和
    $h(Y)_{i_{0}}\in\mathbb{R}^{n}$ 在 $j_{0}$ 列来自内积。最后，我们计算这个差异的平方，并将 $i_{0}=1$ 和
    $j_{0}=1$ 的所有结果相加。在此图中，我们用蓝色矩形表示向量，其中深蓝色表示 $f(x)_{j_{0}}$，浅蓝色表示用于计算 $f(x)_{j_{0}}$
    的项。绿色方块表示标量。红色矩形表示矩阵。
- en: Roadmap
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 路线图
- en: 'In Section [2](#S2 "2 Related Work ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we introduce related research work. In Section [3](#S3
    "3 Technique Overview ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we provide an overview of the techniques we will use throughout the rest
    of the paper. In Section [4](#S4 "4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we present the basic notations we use, some mathematical
    facts, and helpful definitions that support the following proof. In Section [5](#S5
    "5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we compute the gradients of the helpful functions defined earlier. In Section [6](#S6
    "6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we define the Hessian for further discussion. In Section [7](#S7 "7 Hessian for
    𝑋 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    the Hessian matrix with respect to $X$ is Lipschitz. In Section [9](#S9 "9 Hessian
    for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we show that the Hessian matrix with respect to $X$ and show that it is
    Lipschitz and positive semidefinite (PSD). In Section [11](#S11 "11 Hessian for
    𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we compute the Hessian matrix with respect to both $X$. In Section [12](#S12 "12
    Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we demonstrate that the Hessian matrix with respect to
    both $X$ is Lipschitz. In Section [13](#S13 "13 Generating a Spectral Sparsifier
    via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we introduce some tensor sketch techniques to obtain fast approximations
    of the Hessian. In Section [14](#S14 "14 Analysis Of Algorithm 1 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we introduce the Newton step.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2](#S2 "2 相关工作 ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们介绍了相关的研究工作。在第[3](#S3
    "3 技术概述 ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们提供了我们将在论文其余部分中使用的技术概述。在第[4](#S4
    "4 初步 ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们介绍了我们使用的基本符号，一些数学事实，以及支持后续证明的有用定义。在第[5](#S5
    "5 梯度 ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们计算了之前定义的有用函数的梯度。在第[6](#S6
    "6 Hessian ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们定义了Hessian矩阵以供进一步讨论。在第[7](#S7
    "7 关于 $X$ 的 Hessian ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们计算了关于$X$的Hessian矩阵，且其为Lipschitz。
    在第[9](#S9 "9 关于 $X$ 的Hessian为PSD ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们展示了关于$X$的Hessian矩阵，并证明其为Lipschitz且为正半定（PSD）。在第[11](#S11
    "11 关于 $X$ 和 $Y$ 的Hessian ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们计算了关于$X$和$Y$的Hessian矩阵。在第[12](#S12
    "12 关于 $x, y$ 的Hessian Lipschitz ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们证明了关于$X$和$Y$的Hessian矩阵为Lipschitz。在第[13](#S13
    "13 通过TensorSketch生成光谱稀疏化器 ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们介绍了一些张量草图技术以获得Hessian的快速近似。在第[14](#S14
    "14 算法 1 的分析 ‣ 快速优化视角：基于张量和支持向量机技巧重构单层注意力，并在矩阵乘法时间内解决")节中，我们介绍了牛顿步骤。
- en: 2 Related Work
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Attention
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力
- en: '[[18](#bib.bib18)] represents one of the earliest works that employed attention
    in NLP. They assumed that a fixed-length vector could enhance the performance
    of the encoder-decoder design by incorporating an attention mechanism. This mechanism
    allows the decoder to focus on relevant words in the source sentence while generating
    translations. Consequently, this approach significantly improves the performance
    of machine translation models compared to those without an attention mechanism.
    Subsequently, [[113](#bib.bib113)] explained two variants of attention: local
    attention, which considers a subset of source words at a time, and global attention,
    which attends to all source words.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18](#bib.bib18)] 代表了最早采用注意力机制进行自然语言处理的研究之一。他们假设一个固定长度的向量通过引入注意力机制可以提高编码器-解码器设计的性能。该机制允许解码器在生成翻译时专注于源句子中的相关词语。因此，与没有注意力机制的模型相比，这种方法显著提高了机器翻译模型的性能。随后，[[113](#bib.bib113)]
    解释了注意力的两种变体：局部注意力，它一次考虑源词语的一个子集，以及全局注意力，它关注所有源词语。'
- en: Attention finds extensive applications across various domains. In image captioning,
    [[192](#bib.bib192)] utilizes attention matrices to align specific parts of an
    image with words in a caption. In the context of the Transformer model [[179](#bib.bib179)],
    attention matrices capture differences between words in a sentence. In the realm
    of graph neural networks, [[177](#bib.bib177)] investigates these neural network
    architectures designed for graph-structured data, computing attention matrices
    between each node and its neighbors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在各个领域中得到了广泛应用。在图像描述中，[[192](#bib.bib192)] 利用注意力矩阵将图像的特定部分与描述中的词语对齐。在 Transformer
    模型的背景下，[[179](#bib.bib179)] 注意力矩阵捕捉了句子中词语之间的差异。在图神经网络领域，[[177](#bib.bib177)] 研究了这些为图结构数据设计的神经网络架构，计算了每个节点与其邻居之间的注意力矩阵。
- en: On the theoretical side, after the emergence of LLMs, there has been a substantial
    body of work dedicated to studying attention computation [[57](#bib.bib57), [11](#bib.bib11),
    [203](#bib.bib203), [40](#bib.bib40), [118](#bib.bib118), [29](#bib.bib29), [99](#bib.bib99)].
    Notably, recent research by [[203](#bib.bib203), [40](#bib.bib40), [99](#bib.bib99)]
    employs Locality Sensitive Hashing (LSH) techniques to approximate attention mechanisms.
    In particular, [[203](#bib.bib203)] introduces $\mathsf{KDEformer}$, $\sinh$.
    Lastly, [[57](#bib.bib57)] proposes randomized and deterministic algorithms for
    reducing the dimensionality of attention matrices in LLMs, achieving high accuracy
    while significantly reducing feature dimensions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在理论方面，随着大型语言模型（LLMs）的出现，已经有大量的工作致力于研究注意力计算[[57](#bib.bib57), [11](#bib.bib11),
    [203](#bib.bib203), [40](#bib.bib40), [118](#bib.bib118), [29](#bib.bib29), [99](#bib.bib99)]。特别是，[[203](#bib.bib203),
    [40](#bib.bib40), [99](#bib.bib99)] 采用局部敏感哈希（LSH）技术来近似注意力机制。特别是，[[203](#bib.bib203)]
    介绍了 $\mathsf{KDEformer}$ 和 $\sinh$。最后，[[57](#bib.bib57)] 提出了随机和确定性算法来减少 LLMs 中注意力矩阵的维度，同时在显著降低特征维度的同时实现高精度。
- en: Additionally, numerous studies have attempted to analyze theoretical attention
    from the perspectives of optimization and convergence [[111](#bib.bib111), [69](#bib.bib69),
    [172](#bib.bib172), [205](#bib.bib205)]. [[111](#bib.bib111)] investigated how
    transformers acquire knowledge about word co-occurrence patterns. [[69](#bib.bib69)]
    focused on studying regression problems inspired by neural networks that employ
    exponential activation functions. [[172](#bib.bib172)] analyzed why models occasionally
    prioritize significant words and explained how the attention mechanism evolves
    during the training process. [[205](#bib.bib205)] demonstrated that the presence
    of a heavy-tailed noise distribution contributes to the bad performance of stochastic
    gradient descent (SGD) compared to adaptive methods.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多研究试图从优化和收敛的角度分析理论上的注意力[[111](#bib.bib111), [69](#bib.bib69), [172](#bib.bib172),
    [205](#bib.bib205)]。[[111](#bib.bib111)] 研究了变换器如何获取词语共现模式的知识。[[69](#bib.bib69)]
    重点研究了受神经网络启发的回归问题，这些神经网络使用指数激活函数。[[172](#bib.bib172)] 分析了模型为何偶尔优先考虑重要词语，并解释了注意力机制在训练过程中的演变。[[205](#bib.bib205)]
    证明了重尾噪声分布的存在导致随机梯度下降（SGD）的性能差于自适应方法。
- en: Theoretical LLMs
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理论 LLMs
- en: There are numerous amount of works focusing on the theoretical aspects of LLMs.
    In [[155](#bib.bib155)], the syntactic representations of the attention matrix
    and the individual word embeddings are presented, together with the mathematical
    justification of elucidating the geometrical properties of these representations.
    [[84](#bib.bib84)] introduces a structural probe that analyzes, under the linear
    transformation of a word representation space of a neural network, whether or
    not syntax trees are embedded.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工作专注于 LLM 的理论方面。 在 [[155](#bib.bib155)] 中，展示了注意力矩阵和单词嵌入的句法表示，并提供了阐明这些表示几何属性的数学证明。
    [[84](#bib.bib84)] 介绍了一个结构探测器，该探测器分析在神经网络的单词表示空间的线性变换下，语法树是否被嵌入。
- en: '[[39](#bib.bib39), [110](#bib.bib110), [151](#bib.bib151), [100](#bib.bib100)]
    study the optimization of LLMs. [[39](#bib.bib39)] proposes a new algorithm called
    ZO-BCD. It has favorable overall query complexity and a smaller computational
    complexity in each iteration. [[110](#bib.bib110)] creates a simple scalable second-order
    optimizer, called Sophia. In different parts of the parameter, Sophia adapts to
    the curvature. This may be strongly heterogeneous for language modeling tasks.
    The bound of the running time does not rely on the condition number of the loss.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[[39](#bib.bib39), [110](#bib.bib110), [151](#bib.bib151), [100](#bib.bib100)]
    研究了 LLM 的优化问题。 [[39](#bib.bib39)] 提出了一个新算法，称为 ZO-BCD。该算法具有良好的整体查询复杂度，并且在每次迭代中计算复杂度较小。
    [[110](#bib.bib110)] 创建了一个简单的可扩展二阶优化器，称为 Sophia。在参数的不同部分，Sophia 适应曲率。这对于语言建模任务可能具有强烈的异质性。运行时间的界限不依赖于损失的条件数。'
- en: Other theoretical LLM papers study the knowledge and skills of LLMs. [[188](#bib.bib188)]
    analyzes distinct “skill” neurons, which are regarded as robust indicators of
    downstream tasks when employing the process of soft prompt-tuning, as discussed
    in [[108](#bib.bib108)], for language models. [[50](#bib.bib50)] find a positive
    relationship between the activation of these neurons and the expression of their
    corresponding facts, through analyzing BERT. Simultaneously, [[32](#bib.bib32)]
    employs a fully unsupervised approach to extract latent knowledge from a language
    model’s internal activations. In addition, [[79](#bib.bib79)] and [[124](#bib.bib124)]
    show that in the feed-forward layers of pre-trained models, language models localize
    knowledge. [[194](#bib.bib194)] explores the feasibility of selecting a specific
    subset of layers for modification and determining the optimal location for integrating
    a classifier. [[123](#bib.bib123)] demonstrate that large trained transformers
    exhibit sparsity in their feedforward activations. Zero-th order algorithm for
    training LLM has been analyzed [[125](#bib.bib125), [54](#bib.bib54), [204](#bib.bib204)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其他理论 LLM 论文研究了 LLM 的知识和技能。 [[188](#bib.bib188)] 分析了不同的“技能”神经元，这些神经元被视为在采用软提示调优过程时，作为下游任务的稳健指标，如
    [[108](#bib.bib108)] 中讨论的语言模型。 [[50](#bib.bib50)] 通过分析 BERT 发现这些神经元的激活与其对应事实的表达之间存在正相关关系。同时，[[32](#bib.bib32)]
    采用完全无监督的方法从语言模型的内部激活中提取潜在知识。此外，[[79](#bib.bib79)] 和 [[124](#bib.bib124)] 表明，在预训练模型的前馈层中，语言模型局部化知识。
    [[194](#bib.bib194)] 探索了选择特定层子集进行修改和确定整合分类器的最佳位置的可行性。 [[123](#bib.bib123)] 证明了大型训练变换器在其前馈激活中表现出稀疏性。零阶算法用于训练
    LLM 已被分析 [[125](#bib.bib125), [54](#bib.bib54), [204](#bib.bib204)]。
- en: LLMs Application and Evaluation
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs 应用与评估
- en: Recently, there has been much interest in developing LLM-based systems for conversational
    AI and task-oriented dialogue, like Google’s Meena chatbot [[148](#bib.bib148)],
    Microsoft 365 Copilot [[161](#bib.bib161)], Adobe firefly, Adobe Photoshop, GPT
    series [[149](#bib.bib149), [154](#bib.bib154), [24](#bib.bib24), [35](#bib.bib35),
    [134](#bib.bib134)], and BERT [[49](#bib.bib49)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，开发基于 LLM 的对话 AI 和任务导向对话系统引起了广泛关注，如谷歌的 Meena 聊天机器人 [[148](#bib.bib148)]、Microsoft
    365 Copilot [[161](#bib.bib161)]、Adobe Firefly、Adobe Photoshop、GPT 系列 [[149](#bib.bib149),
    [154](#bib.bib154), [24](#bib.bib24), [35](#bib.bib35), [134](#bib.bib134)] 和
    BERT [[49](#bib.bib49)]。
- en: Moreover, LLM evaluation is also a popular research area. Within the field of
    NLP, LLMs are evaluated based on natural language understanding [[20](#bib.bib20),
    [102](#bib.bib102), [103](#bib.bib103), [45](#bib.bib45)], reasoning [[23](#bib.bib23),
    [187](#bib.bib187), [193](#bib.bib193)], natural language generation [[183](#bib.bib183),
    [147](#bib.bib147), [137](#bib.bib137), [36](#bib.bib36), [47](#bib.bib47)], and
    multilingual tasks [[10](#bib.bib10), [5](#bib.bib5), [112](#bib.bib112), [199](#bib.bib199)].
    Robustness [[109](#bib.bib109), [181](#bib.bib181), [207](#bib.bib207)], ethics
    [[48](#bib.bib48)], biases [[65](#bib.bib65)], and trustworthiness [[80](#bib.bib80)]
    are also important aspects. More specifically, the abilities of LLMs in social
    science [[51](#bib.bib51), [66](#bib.bib66), [131](#bib.bib131)], mathematics
    [[12](#bib.bib12), [53](#bib.bib53), [184](#bib.bib184), [19](#bib.bib19)], science
    [[42](#bib.bib42), [67](#bib.bib67)], engineering [[19](#bib.bib19), [122](#bib.bib122),
    [138](#bib.bib138), [160](#bib.bib160)], and medical applications [[38](#bib.bib38),
    [87](#bib.bib87)] are evaluated.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLM 评估也是一个热门的研究领域。在 NLP 领域，LLM 的评估基于自然语言理解 [[20](#bib.bib20), [102](#bib.bib102),
    [103](#bib.bib103), [45](#bib.bib45)]，推理 [[23](#bib.bib23), [187](#bib.bib187),
    [193](#bib.bib193)]，自然语言生成 [[183](#bib.bib183), [147](#bib.bib147), [137](#bib.bib137),
    [36](#bib.bib36), [47](#bib.bib47)]，以及多语言任务 [[10](#bib.bib10), [5](#bib.bib5),
    [112](#bib.bib112), [199](#bib.bib199)]。鲁棒性 [[109](#bib.bib109), [181](#bib.bib181),
    [207](#bib.bib207)]，伦理 [[48](#bib.bib48)]，偏见 [[65](#bib.bib65)]，以及可信度 [[80](#bib.bib80)]
    也是重要方面。更具体地说，LLM 在社会科学 [[51](#bib.bib51), [66](#bib.bib66), [131](#bib.bib131)]，数学
    [[12](#bib.bib12), [53](#bib.bib53), [184](#bib.bib184), [19](#bib.bib19)]，科学
    [[42](#bib.bib42), [67](#bib.bib67)]，工程 [[19](#bib.bib19), [122](#bib.bib122),
    [138](#bib.bib138), [160](#bib.bib160)]，和医学应用 [[38](#bib.bib38), [87](#bib.bib87)]
    的能力也在评估之中。
- en: Sketching
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**草图**'
- en: 'Sketching is a powerful tool that is used to accelerate the performance of
    machine learning algorithms and optimization processes. The fundamental concept
    of sketching is to partition a large input matrix into a significantly smaller
    sketching matrix but still preserve the main characteristics of the original matrix.
    Therefore, the algorithms may work with the smaller matrix instead of the huge
    original, which leads to a substantial reduction in processing time. Many previous
    works have studied sketching, proposed sketching algorithms, and supported these
    algorithms with robust theoretical guarantees. For example, the Johnson-Lindenstrauss
    lemma is proposed by [[89](#bib.bib89)]: it shows that under a certain high-dimensional
    space, projecting points to a lower-dimensional subspace may preserve the pairwise
    distances between these points. This mathematical property becomes the foundation
    of the development of faster algorithms for tasks such as nearest neighbor search.
    In addition, as explained in [[2](#bib.bib2)], the Fast Johnson-Lindenstrauss
    Transform (FJLT) introduces a specific family of structured random projections
    that can be applied to a matrix in input sparsity time.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**草图** 是一个强大的工具，用于加速机器学习算法和优化过程的性能。草图的基本概念是将一个大输入矩阵划分为一个显著较小的草图矩阵，但仍然保留原始矩阵的主要特征。因此，算法可以使用较小的矩阵而不是巨大的原始矩阵，从而大幅减少处理时间。许多之前的工作研究了草图技术，提出了草图算法，并用坚实的理论保证支持这些算法。例如，**Johnson-Lindenstrauss
    引理** 由 [[89](#bib.bib89)] 提出：它表明，在某些高维空间下，将点投影到低维子空间可能保留这些点之间的对距离。这一数学性质成为了发展更快算法的基础，如最近邻搜索。此外，如
    [[2](#bib.bib2)] 所解释的，**快速 Johnson-Lindenstrauss 变换（FJLT）** 引入了一类特定的结构化随机投影，这些投影可以在输入稀疏时间内应用于矩阵。'
- en: More recently, sketching has been applied to many numerical linear algebra tasks,
    such as linear regression [[46](#bib.bib46), [133](#bib.bib133)], dynamic kernel
    estimation [[143](#bib.bib143)], submodular maximization [[144](#bib.bib144)],
    matrix sensing [[145](#bib.bib145)], gradient-based algorithm [[195](#bib.bib195)],
    clustering [[59](#bib.bib59), [64](#bib.bib64)], convex programming [[167](#bib.bib167),
    [146](#bib.bib146), [93](#bib.bib93), [90](#bib.bib90), [117](#bib.bib117)], online
    optimization problems [[150](#bib.bib150)], training neural networks [[173](#bib.bib173),
    [197](#bib.bib197), [169](#bib.bib169), [70](#bib.bib70), [25](#bib.bib25)], reinforcement
    learning [[191](#bib.bib191), [196](#bib.bib196)], tensor decomposition [[165](#bib.bib165),
    [60](#bib.bib60)], relational database [[141](#bib.bib141)], low-rank approximation
    [[30](#bib.bib30), [128](#bib.bib128), [126](#bib.bib126), [8](#bib.bib8), [164](#bib.bib164)],
    distributed problems [[31](#bib.bib31), [190](#bib.bib190)], weighted low rank
    approximation [[152](#bib.bib152), [76](#bib.bib76), [168](#bib.bib168)], CP decomposition
    [[129](#bib.bib129)], regression inspired by softmax [[118](#bib.bib118), [74](#bib.bib74),
    [162](#bib.bib162), [55](#bib.bib55)], matrix sensing [[145](#bib.bib145)], and
    Kronecker product regression [[153](#bib.bib153)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，素描技术已被应用于许多数值线性代数任务，例如线性回归 [[46](#bib.bib46), [133](#bib.bib133)]，动态核估计 [[143](#bib.bib143)]，子模量最大化
    [[144](#bib.bib144)]，矩阵感知 [[145](#bib.bib145)]，基于梯度的算法 [[195](#bib.bib195)]，聚类
    [[59](#bib.bib59), [64](#bib.bib64)]，凸编程 [[167](#bib.bib167), [146](#bib.bib146),
    [93](#bib.bib93), [90](#bib.bib90), [117](#bib.bib117)]，在线优化问题 [[150](#bib.bib150)]，神经网络训练
    [[173](#bib.bib173), [197](#bib.bib197), [169](#bib.bib169), [70](#bib.bib70),
    [25](#bib.bib25)]，强化学习 [[191](#bib.bib191), [196](#bib.bib196)]，张量分解 [[165](#bib.bib165),
    [60](#bib.bib60)]，关系数据库 [[141](#bib.bib141)]，低秩近似 [[30](#bib.bib30), [128](#bib.bib128),
    [126](#bib.bib126), [8](#bib.bib8), [164](#bib.bib164)]，分布式问题 [[31](#bib.bib31),
    [190](#bib.bib190)]，加权低秩近似 [[152](#bib.bib152), [76](#bib.bib76), [168](#bib.bib168)]，CP分解
    [[129](#bib.bib129)]，受softmax启发的回归 [[118](#bib.bib118), [74](#bib.bib74), [162](#bib.bib162),
    [55](#bib.bib55)]，矩阵感知 [[145](#bib.bib145)]，以及Kronecker积回归 [[153](#bib.bib153)]。
- en: Second-order Method
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 二阶方法
- en: Second-order method have been used for solving many convex optimization and
    non-convex optimization problems, such as linear programming [[41](#bib.bib41),
    [26](#bib.bib26), [93](#bib.bib93), [167](#bib.bib167), [71](#bib.bib71), [83](#bib.bib83)],
    empirical risk minimization [[117](#bib.bib117), [146](#bib.bib146)], support
    vector machines [[77](#bib.bib77)], cutting plan method [[116](#bib.bib116), [90](#bib.bib90)],
    semi-definite programming [[88](#bib.bib88), [81](#bib.bib81), [71](#bib.bib71),
    [170](#bib.bib170)], hyperbolic programming/polynomials [[61](#bib.bib61), [211](#bib.bib211)],
    streaming algorithm [[119](#bib.bib119), [27](#bib.bib27), [170](#bib.bib170)],
    federated learning [[28](#bib.bib28)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶方法已被用于解决许多凸优化和非凸优化问题，例如线性规划 [[41](#bib.bib41), [26](#bib.bib26), [93](#bib.bib93),
    [167](#bib.bib167), [71](#bib.bib71), [83](#bib.bib83)]，经验风险最小化 [[117](#bib.bib117),
    [146](#bib.bib146)]，支持向量机 [[77](#bib.bib77)]，剪切平面方法 [[116](#bib.bib116), [90](#bib.bib90)]，半正定规划
    [[88](#bib.bib88), [81](#bib.bib81), [71](#bib.bib71), [170](#bib.bib170)]，双曲规划/多项式
    [[61](#bib.bib61), [211](#bib.bib211)]，流式算法 [[119](#bib.bib119), [27](#bib.bib27),
    [170](#bib.bib170)]，联邦学习 [[28](#bib.bib28)]。
- en: Convergence and Deep Neural Network Optimization
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 收敛性与深度神经网络优化
- en: Many works focus on analyzing optimization, convergence guarantees, and training
    improvement. [[107](#bib.bib107)] shows that stochastic gradient descent optimizes
    over-parameterized neural networks on structured data, while [[63](#bib.bib63)]
    demonstrates that gradient descent optimizes over-parameterized neural networks.
    In [[16](#bib.bib16)], a convergence theory for over-parameterized deep neural
    networks via gradient descent is developed. [[17](#bib.bib17)] analyzes the convergence
    rate of training recurrent neural networks. [[3](#bib.bib3)] provides a fine-grained
    analysis of optimization and generalization for over-parameterized two-layer neural
    networks. [[4](#bib.bib4)] studies exact computation with an infinitely wide neural
    network. [[33](#bib.bib33)] proposes a Gram-Gauss-Newton method for optimizing
    over-parameterized neural networks. [[201](#bib.bib201)] improves the analysis
    of the global convergence of stochastic gradient descent when training deep neural
    networks, requiring a milder over-parameterization compared to prior research.
    Other research, such as [[135](#bib.bib135), [96](#bib.bib96), [206](#bib.bib206)],
    focuses on optimization and generalization, while [[69](#bib.bib69), [118](#bib.bib118)]
    emphasize the convergence rate and stability. Works like [[25](#bib.bib25), [173](#bib.bib173),
    [9](#bib.bib9), [127](#bib.bib127), [202](#bib.bib202)] concentrate on specialized
    optimization algorithms and techniques for training neural networks, and [[115](#bib.bib115),
    [82](#bib.bib82)] concentrate on leveraging neural network structure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究专注于分析优化、收敛保证和训练改进。[[107](#bib.bib107)] 显示了随机梯度下降在结构化数据上的优化效果，而 [[63](#bib.bib63)]
    证明了梯度下降在过参数化神经网络上的优化效果。在 [[16](#bib.bib16)] 中，开发了一种通过梯度下降对过参数化深度神经网络的收敛理论。[[17](#bib.bib17)]
    分析了训练递归神经网络的收敛速度。[[3](#bib.bib3)] 提供了对过参数化双层神经网络的优化和泛化的细致分析。[[4](#bib.bib4)] 研究了具有无限宽度神经网络的精确计算。[[33](#bib.bib33)]
    提出了优化过参数化神经网络的Gram-Gauss-Newton方法。[[201](#bib.bib201)] 改进了在训练深度神经网络时随机梯度下降的全局收敛分析，与之前的研究相比，要求的过参数化程度较低。其他研究，如
    [[135](#bib.bib135), [96](#bib.bib96), [206](#bib.bib206)]，专注于优化和泛化，而 [[69](#bib.bib69),
    [118](#bib.bib118)] 强调收敛速度和稳定性。像 [[25](#bib.bib25), [173](#bib.bib173), [9](#bib.bib9),
    [127](#bib.bib127), [202](#bib.bib202)] 这样的研究集中于训练神经网络的专门优化算法和技术，而 [[115](#bib.bib115),
    [82](#bib.bib82)] 则专注于利用神经网络结构。
- en: Algorithmic Regularization
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法正则化
- en: There is a significant body of research exploring the latent bias inherent in
    gradient descent when applied to separable classification tasks. This research
    typically employs logistic or exponentially-tailed loss functions to maximize
    margins, as demonstrated in previous studies [[97](#bib.bib97), [68](#bib.bib68),
    [101](#bib.bib101), [98](#bib.bib98), [158](#bib.bib158), [130](#bib.bib130),
    [132](#bib.bib132)]. These novel findings have also been applied to non-separable
    data through the utilization of gradient-based techniques [[86](#bib.bib86), [95](#bib.bib95),
    [94](#bib.bib94)]. Analysis of implicit bias in regression problems and associated
    loss functions is carried out using methods such as mirror descent [[198](#bib.bib198),
    [13](#bib.bib13), [14](#bib.bib14), [178](#bib.bib178), [157](#bib.bib157), [180](#bib.bib180),
    [7](#bib.bib7), [68](#bib.bib68)] and stochastic gradient descent [[85](#bib.bib85),
    [120](#bib.bib120), [114](#bib.bib114), [210](#bib.bib210), [56](#bib.bib56),
    [121](#bib.bib121), [22](#bib.bib22)]. These findings extend to the implicit bias
    of adaptive and momentum-based optimization methods [[92](#bib.bib92), [185](#bib.bib185),
    [186](#bib.bib186), [142](#bib.bib142)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量研究探讨了在可分离分类任务中应用梯度下降时固有的潜在偏差。这些研究通常使用逻辑回归或指数尾损失函数来最大化边界，如先前的研究所示 [[97](#bib.bib97),
    [68](#bib.bib68), [101](#bib.bib101), [98](#bib.bib98), [158](#bib.bib158), [130](#bib.bib130),
    [132](#bib.bib132)]。这些新发现还通过利用基于梯度的技术应用于不可分数据 [[86](#bib.bib86), [95](#bib.bib95),
    [94](#bib.bib94)]。对回归问题及相关损失函数中隐含偏差的分析使用了如镜像下降 [[198](#bib.bib198), [13](#bib.bib13),
    [14](#bib.bib14), [178](#bib.bib178), [157](#bib.bib157), [180](#bib.bib180),
    [7](#bib.bib7), [68](#bib.bib68)] 和随机梯度下降 [[85](#bib.bib85), [120](#bib.bib120),
    [114](#bib.bib114), [210](#bib.bib210), [56](#bib.bib56), [121](#bib.bib121),
    [22](#bib.bib22)] 等方法。这些发现还扩展到自适应和基于动量的优化方法的隐含偏差 [[92](#bib.bib92), [185](#bib.bib185),
    [186](#bib.bib186), [142](#bib.bib142)]。
- en: 3 Technique Overview
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 技术概述
- en: 'In this section, we will introduce the primary technique employed in this paper.
    The notations used in this section are presented in Preliminary (Section [4](#S4
    "4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将介绍本文采用的主要技术。本节中使用的符号在初步部分（第[4](#S4 "4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")节）中给出。'
- en: 3.1 Analysis
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 分析
- en: Split Hessian into blocks ($X,Y$)
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将Hessian分割成块（$X,Y$）
- en: 'In the fast approximation and convergence guarantee of the training process
    for the attention matrix, the positive semi-definite property is a key focus in
    Section [6](#S6 "6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). In comparison to single/multiple softmax regression, both the weights
    $X$ (refer to Definition [1.2](#S1.Thmtheorem2 "Definition 1.2 (Attention optimization).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")) need to be considered. Therefore, our Hessian matrix discussed in Section [6](#S6
    "6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    has the following format'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '在训练过程中对注意力矩阵的快速近似和收敛保证中，正半定性质是第[6](#S6 "6 Hessian ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")节的关键关注点。与单一/多重softmax回归相比，需要考虑权重$X$（参见定义[1.2](#S1.Thmtheorem2
    "Definition 1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")）。因此，我们在第[6](#S6 "6 Hessian ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")节讨论的Hessian矩阵具有以下格式'
- en: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
- en: To establish the positive semi-definite property, we will examine the properties
    of the matrix above individually.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立正半定性质，我们将逐个检查上述矩阵的性质。
- en: Positive Semi-Definite For Hessian $H_{x,x}$
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对于Hessian $H_{x,x}$的正半定性
- en: 'The positive semi-definite of the Hessian, denoted as ${H_{x,x},H_{y,y}}$,
    constitutes a crucial initial step in the proof outlined in Lemma [6.1](#S6.Thmtheorem1
    "Lemma 6.1\. ‣ 6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). These Hessian are discussed in detail in Section [9](#S9 "9 Hessian for
    𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    and Section [10](#S10 "10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hessian的正半定性，记作${H_{x,x},H_{y,y}}$，构成了引理[6.1](#S6.Thmtheorem1 "Lemma 6.1\.
    ‣ 6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")中证明的关键初步步骤。这些Hessian在第[9](#S9
    "9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")节和第[10](#S10 "10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")节中详细讨论。'
- en: 'Leveraging Lemma [10.1](#S10.Thmtheorem1 "Lemma 10.1\. ‣ 10.1 Hessian Property
    ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [9.1](#S9.Thmtheorem1 "Lemma 9.1\. ‣ 9.1 Main Result ‣ 9 Hessian
    for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we can establish the following results if the regularization weight sufficiently
    large (see Section [9](#S9 "9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") in details), then'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '利用引理 [10.1](#S10.Thmtheorem1 "Lemma 10.1\. ‣ 10.1 Hessian Property ‣ 10 Hessian
    for 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    和引理 [9.1](#S9.Thmtheorem1 "Lemma 9.1\. ‣ 9.1 Main Result ‣ 9 Hessian for 𝑋 Is
    PSD ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，如果正则化权重足够大（详见第
    [9](#S9 "9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") 节），我们可以建立以下结果：'
- en: '|  | $\displaystyle H(x)\succeq l\cdot I_{d^{2}}~{}~{}\text{and}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H(x)\succeq l\cdot I_{d^{2}}~{}~{}\text{and}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
- en: Spectral upper bound for $H_{x,y}$
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $H_{x,y}$ 的光谱上界
- en: To establish the spectral upper bound of $H_{x,y}$ into $\{{G_{i}}\}_{i=1}^{4}$.
    The spectral upper bound for $H_{x,y}$.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立 $H_{x,y}$ 的光谱上界为 $\{{G_{i}}\}_{i=1}^{4}$。$H_{x,y}$ 的光谱上界。
- en: Given this upper bound, our final focus in the proof of the positive semi-definite
    property (PSD) will be as follows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在此上界下，我们在正半定性质 (PSD) 证明中的最终关注点如下。
- en: PSD for Hessian $H$
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hessian $H$ 的 PSD
- en: 'The Hessian matrix $H$ in Section [9](#S9 "9 Hessian for 𝑋 Is PSD ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") and $H_{y,y}$ and
    $H_{y,x}$, $a_{2}$ as the bound of the matrix above respectively in Lemma [6.1](#S6.Thmtheorem1
    "Lemma 6.1\. ‣ 6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we have the following result'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '第 [9](#S9 "9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") 节中的 Hessian 矩阵 $H$ 和 $H_{y,y}$ 及 $H_{y,x}$，$a_{2}$
    作为上界矩阵的界限，分别在引理 [6.1](#S6.Thmtheorem1 "Lemma 6.1\. ‣ 6 Hessian ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 中，我们得出以下结果：'
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
- en: Given the relationship of $\{a_{i}\}_{i=1}^{3}$ as discussed above, the positive
    semi-definite property of the Hessian matrix is established.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述讨论的 $\{a_{i}\}_{i=1}^{3}$ 关系，Hessian 矩阵的正半定性质被建立。
- en: Lipschitz property for Hessian
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hessian 的 Lipschitz 性质
- en: 'The Lipschitz property of the Hessian is determined by the upper bound and
    Lipschitz property of the basic functions that constitute the Hessian matrix $H$
    has three parts $H_{x,x}$ and $H_{y,y}$ is independent of $y$, the Lipschitz property
    can be easily established. For details of others, we refer the readers to read
    Section [12](#S12 "12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hessian 的 Lipschitz 性质由构成 Hessian 矩阵 $H$ 的基本函数的上界和 Lipschitz 性质决定，其中 $H_{x,x}$
    和 $H_{y,y}$ 与 $y$ 无关，Lipschitz 性质可以很容易地建立。有关其他细节，我们请读者查阅第 [12](#S12 "12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") 节。'
- en: 'To compute the Lipschitz continuity of $H_{x,x}$, $c(x)$ in Lemma [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), which together form the matrix $H_{x,x}$
    into four distinct parts denoted as $\{G_{k}\}_{k=1}^{4}$), we want to bound'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '为了计算 $H_{x,x}$ 的 Lipschitz 连续性，$c(x)$ 在引理 [8.4](#S8.Thmtheorem4 "Lemma 8.4
    (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic
    Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") 中，结合形成矩阵 $H_{x,x}$ 的四个不同部分 $\{G_{k}\}_{k=1}^{4}$，我们希望界定'
- en: '|  | $\displaystyle&#124;\prod_{i=1}^{t}\beta_{i}(x)-\prod_{i=1}^{t}\beta_{i}(\widetilde{x})&#124;,$
    |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\prod_{i=1}^{t}\beta_{i}(x)-\prod_{i=1}^{t}\beta_{i}(\widetilde{x})&#124;,$
    |  |'
- en: which can be upper bounded by
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被上界
- en: '|  | $1$2 |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where assume that $\beta_{0}(x)=1$ for convenient. We will then proceed to establish
    the Lipschitz continuity of $H_{x,x}$
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在方便的情况下，我们假设 $\beta_{0}(x)=1$。然后我们将继续建立 $H_{x,x}$ 的 Lipschitz 连续性。
- en: '|  | $1$2 |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 3.2 Algorithm
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 算法
- en: Forward Computation
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前向计算
- en: 'To simplify the computation of the attention matrix, we can decompose the computation
    process into three components: $f$, and $h$ time, as stated in Lemma [5.3](#S5.Thmtheorem3
    "Lemma 5.3\. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '为了简化注意力矩阵的计算，我们可以将计算过程分解为三个部分：$f$ 和 $h$ 时间，如引理 [5.3](#S5.Thmtheorem3 "Lemma
    5.3\. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") 中所述。'
- en: Gradient Computation
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度计算
- en: 'We can compute the gradient in Section [5](#S5 "5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以在第 [5](#S5 "5 Gradient ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") 节计算梯度，如下所示：'
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
- en: for some matrix $p(x,y)\in\mathbb{R}^{n\times n}$ can be computed in ${\cal
    T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(d,n,d)$ time. Similarly,
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些矩阵 $p(x,y)\in\mathbb{R}^{n\times n}$ 可以在 ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal
    T}_{\mathrm{mat}}(d,n,d)$ 时间内计算。类似地，
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$ |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$ |  |'
- en: which also takes ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    and $g(y(t))$ time.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这也需要 ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$ 和 $g(y(t))$
    时间。
- en: Straightforward Hessian Computation
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直接计算 Hessian 矩阵
- en: Computing the Hessian in straightforward way would take ${\cal T}_{\mathrm{mat}}(d^{2},n^{2},d^{2})$
    where $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$. This is too
    slow, we will use sketching ideas to speed up this running time. Using sketching
    matrices to speed up the Hessian computation has been extensively studied in convex
    and non-convex optimization [[93](#bib.bib93), [117](#bib.bib117), [167](#bib.bib167),
    [71](#bib.bib71), [77](#bib.bib77), [146](#bib.bib146)].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 直接计算 Hessian 将需要 ${\cal T}_{\mathrm{mat}}(d^{2},n^{2},d^{2})$ 时间，其中 $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times
    d^{2}}$。这太慢了，我们将使用草图方法来加快这一运行时间。使用草图矩阵加速 Hessian 计算已经在凸优化和非凸优化中得到了广泛研究 [[93](#bib.bib93),
    [117](#bib.bib117), [167](#bib.bib167), [71](#bib.bib71), [77](#bib.bib77), [146](#bib.bib146)]。
- en: TensorSRHT Fast Approximation for Hessian
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorSRHT 快速近似 Hessian
- en: 'Building upon the aforementioned properties, we can apply the Newton Method
    in Section [14](#S14 "14 Analysis Of Algorithm 1 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") to establish convergence for the regression problem.
    Now, let’s delve into the primary contribution of this paper. Given that $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ down to $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$
    in the paper which is the most common setting in practice because $n$ is feature
    dimension).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述特性，我们可以应用第 [14](#S14 "14 算法 1 的分析 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决")
    节中的牛顿法来建立回归问题的收敛性。现在，让我们深入探讨本文的主要贡献。鉴于 $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times
    d^{2}}$ 在本文中下限为 $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$，这是实际中最常见的设置，因为
    $n$ 是特征维度）。
- en: Overall Time
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总体时间
- en: In Summary, we know that
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们知道
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computing forward function ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    time (Lemma [5.3](#S5.Thmtheorem3 "Lemma 5.3\. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5
    Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"))'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算前向函数的时间为 ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$（引理
    [5.3](#S5.Thmtheorem3 "引理 5.3 ‣ 5.3 计算 𝑐,𝑓,ℎ ‣ 5 梯度 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决")）。
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computing gradient takes ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$
    time (Lemma [5.4](#S5.Thmtheorem4 "Lemma 5.4\. ‣ 5.4 Reformulating Gradient (𝑥)
    in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [5.5](#S5.Thmtheorem5 "Lemma 5.5\. ‣ 5.5 Reformulating Gradient
    (𝑦) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"))'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算梯度的时间为 ${\cal T}_{\mathrm{mat}}(n,n,d)+{\cal T}_{\mathrm{mat}}(n,d,d)$（引理
    [5.4](#S5.Thmtheorem4 "引理 5.4 ‣ 5.4 在矩阵视角中重新表述梯度（𝑥） ‣ 5 梯度 ‣ 快速优化视角：基于张量和 SVM
    技巧重新表述单层注意力，并在矩阵乘法时间内解决") 和引理 [5.5](#S5.Thmtheorem5 "引理 5.5 ‣ 5.5 在矩阵视角中重新表述梯度（𝑦）
    ‣ 5 梯度 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决")）。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Compute Hessian takes $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$
    (Lemma [13.6](#S13.Thmtheorem6 "Lemma 13.6\. ‣ 13.4 Fast Approximation for Hessian
    via Sketching ‣ 13 Generating a Spectral Sparsifier via TensorSketch ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"))'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算 Hessian 的时间复杂度为 $\widetilde{O}(nd)+{\cal T}_{\mathrm{mat}}(d^{2},d^{2},d^{2})$（引理
    [13.6](#S13.Thmtheorem6 "引理 13.6 ‣ 13.4 通过 sketching 快速逼近 Hessian ‣ 13 通过 TensorSketch
    生成谱稀疏化器 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决")）。
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compute $g$
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算 $g$
- en: The total time can be expressed as $\widetilde{O}({\cal T}_{\mathrm{mat}}(n,d,n)+{\cal
    T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon)$ is the exponent of matrix
    multiplication.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 总时间可以表示为 $\widetilde{O}({\cal T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)+d^{2\omega})\log(1/\epsilon)$，其中
    $\omega$ 是矩阵乘法的指数。
- en: 4 Preliminary
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 初步
- en: '| Previous works | Simplified version of Def. [1.2](#S1.Thmtheorem2 "Definition
    1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") | How Def. [1.2](#S1.Thmtheorem2 "Definition 1.2
    (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") is simplified |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 先前的工作 | 定义 [1.2](#S1.Thmtheorem2 "定义 1.2（注意力优化）。 ‣ 1 介绍 ‣ 快速优化视角：基于张量和 SVM
    技巧重新表述单层注意力，并在矩阵乘法时间内解决") 的简化版本 | 定义 [1.2](#S1.Thmtheorem2 "定义 1.2（注意力优化）。 ‣ 1
    介绍 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决") 是如何简化的 |'
- en: '| --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [[203](#bib.bib203), [11](#bib.bib11), [29](#bib.bib29)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})A_{3}Y$,
    $K=A_{2}$, both $X,Y$ are not considered |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[203](#bib.bib203), [11](#bib.bib11), [29](#bib.bib29)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})A_{3}Y$，$K=A_{2}$，$X$
    和 $Y$ 都不考虑 |'
- en: '| [[55](#bib.bib55)] | $(D^{-1}\exp(A_{1}XA_{2}^{\top}))_{i,*}$ are not considered
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | $(D^{-1}\exp(A_{1}XA_{2}^{\top}))_{i,*}$ 不被考虑 |'
- en: '| [[72](#bib.bib72), [73](#bib.bib73)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ are
    not considered |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#bib.bib72), [73](#bib.bib73)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ 未被考虑
    |'
- en: '| [[75](#bib.bib75)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ is not considered and
    need the symmetric assumption for matrix |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] | $D^{-1}\exp(A_{1}XA_{2}^{\top})$ 未被考虑，需要对矩阵进行对称假设 |'
- en: '| [[57](#bib.bib57)] | $D^{-1}\exp(A_{2}A_{2}^{\top})$ is not considered |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[57](#bib.bib57)] | $D^{-1}\exp(A_{2}A_{2}^{\top})$ 未被考虑 |'
- en: '| [[171](#bib.bib171)] | $A_{1}XA_{2}^{\top}A_{3}$ and $\exp$ is not considered
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[171](#bib.bib171)] | $A_{1}XA_{2}^{\top}A_{3}$ 和 $\exp$ 未被考虑 |'
- en: 'Table 1: Here $D:=\operatorname{diag}(\exp(A_{1}XA_{2}^{\top}){\bf 1}_{n})$.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：这里 $D:=\operatorname{diag}(\exp(A_{1}XA_{2}^{\top}){\bf 1}_{n})$。
- en: 'In Section [4.1](#S4.SS1 "4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we present the basic mathematical
    properties of vectors, norms and matrices. In section [4.2](#S4.SS2 "4.2 General
    Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we provide a definition of $L(X,Y)$. In section [4.4](#S4.SS4 "4.4 A Helpful
    Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we define a series of helpful functions with respect
    to $Y$ and $Y$. In Section [4.6](#S4.SS6 "4.6 Regularization ‣ 4 Preliminary ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we define
    the regularization function. In Section [4.7](#S4.SS7 "4.7 Fast Matrix Multiplication
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we introduce facts related to fast matrix multiplication.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在章节 [4.1](#S4.SS1 "4.1 基础事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间中求解")
    中，我们介绍了向量、范数和矩阵的基本数学属性。在章节 [4.2](#S4.SS2 "4.2 一般定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间中求解")
    中，我们提供了 $L(X,Y)$ 的定义。在章节 [4.4](#S4.SS4 "4.4 关于 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM
    技巧重构单层注意力，并在矩阵乘法时间中求解") 中，我们定义了一系列与 $Y$ 和 $Y$ 相关的有用函数。在章节 [4.6](#S4.SS6 "4.6 正则化
    ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间中求解") 中，我们定义了正则化函数。在章节 [4.7](#S4.SS7
    "4.7 快速矩阵乘法 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间中求解") 中，我们介绍了与快速矩阵乘法相关的事实。
- en: Notation
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 符号
- en: Now we define the basic notations we use in this paper.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义了本文中使用的基本符号。
- en: First, we define the notations related to the sets. We use $\mathbb{N}$. Let
    $n$ be in $\mathbb{N}$. We use $\mathbb{R},\mathbb{R}^{n},\mathbb{R}^{n\times
    d}$-dimensional vectors, and $n\times d$. We use $\mathbb{R}_{+}$ to denote the
    set containing all positive real numbers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义与集合相关的符号。我们使用 $\mathbb{N}$。设 $n$ 属于 $\mathbb{N}$。我们使用 $\mathbb{R},\mathbb{R}^{n},\mathbb{R}^{n\times
    d}$ 维向量和 $n\times d$。我们使用 $\mathbb{R}_{+}$ 表示包含所有正实数的集合。
- en: Then, we define the notations related to vectors. Let $x,y\in\mathbb{R}^{d}$,
    we define $x_{i}\in\mathbb{R}$-th entry of $x$ as $\langle x,y\rangle:=\sum_{i=1}^{d}x_{i}y_{i}$
    and $y$ as $(x\circ y)_{i}:=x_{i}\cdot y_{i}$. For all $p\in\{1,2,\infty\}$, which
    is the $\ell_{p}$. We use ${\bf 1}_{d}$ to denote the $d$’s and $0$’s, respectively.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了与向量相关的符号。设 $x,y\in\mathbb{R}^{d}$，我们定义 $x_{i}\in\mathbb{R}$-th 项为 $\langle
    x,y\rangle:=\sum_{i=1}^{d}x_{i}y_{i}$，$y$ 为 $(x\circ y)_{i}:=x_{i}\cdot y_{i}$。对于所有
    $p\in\{1,2,\infty\}$，即 $\ell_{p}$。我们用 ${\bf 1}_{d}$ 表示 $d$ 个 $1$ 和 $0$。
- en: After that, we define the notations related to matrices. Let $A\in\mathbb{R}^{n\times
    d}$ and $j\in[d]$ to denote the entry of $A$-th row and $j$ and $A_{*,j}\in\mathbb{R}^{n}$.
    We use $A^{\top}\in\mathbb{R}^{d\times n}$, where $A_{i,j}^{\top}=A_{j,i}$, we
    define $x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$. For $x\in\mathbb{R}^{d}$
    as $\operatorname{diag}(x)_{i,i}=x_{i}$ and other entries of $\operatorname{diag}(x)$’s.
    $\|A\|_{F}\in\mathbb{R}$ denote the Frobenius norm and the spectral norm of $A\in\mathbb{R}^{n\times
    d}$ and $\|A\|:=\max_{x\in\mathbb{R}^{d}}\|Ax\|_{2}/\|x\|_{2}$. For each $j_{1}\in[n]$
    to denote one $n\times d^{2}$. Let $C,D\in\mathbb{R}^{d\times d}$ if for all $y\in\mathbb{R}^{d}$.
    $C$. We use $I_{d}$ identity matrix. $\operatorname{nnz}(A)$ that are not equal
    to zero. ${\bf 0}_{n\times n}\in\mathbb{R}^{n\times n}$, $({\bf 0}_{n\times n})_{i,j}=0$.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义与矩阵相关的符号。设 $A\in\mathbb{R}^{n\times d}$ 和 $j\in[d]$ 表示 $A$ 第 $j$ 列的条目，以及
    $A_{*,j}\in\mathbb{R}^{n}$。我们使用 $A^{\top}\in\mathbb{R}^{d\times n}$，其中 $A_{i,j}^{\top}=A_{j,i}$，我们定义
    $x=\operatorname{vec}(X)\in\mathbb{R}^{d^{2}}$。对于 $x\in\mathbb{R}^{d}$，定义 $\operatorname{diag}(x)_{i,i}=x_{i}$
    和 $\operatorname{diag}(x)$ 的其他条目。$\|A\|_{F}\in\mathbb{R}$ 表示 Frobenius 范数和 $A\in\mathbb{R}^{n\times
    d}$ 的谱范数，$\|A\|:=\max_{x\in\mathbb{R}^{d}}\|Ax\|_{2}/\|x\|_{2}$。对于每个 $j_{1}\in[n]$，表示一个
    $n\times d^{2}$。设 $C,D\in\mathbb{R}^{d\times d}$，如果对于所有 $y\in\mathbb{R}^{d}$ 都成立
    $C$。我们使用 $I_{d}$ 作为单位矩阵。$\operatorname{nnz}(A)$ 表示不等于零的条目。${\bf 0}_{n\times n}\in\mathbb{R}^{n\times
    n}$，$({\bf 0}_{n\times n})_{i,j}=0$。
- en: Let $n_{1},n_{2},d_{1},d_{2}$ and $B\in\mathbb{R}^{n_{2}\times d_{2}}$ and $B$,
    as $(A\otimes B)_{(i_{1}-1)n_{2}+i_{2},(j_{1}-1)d_{2}+j_{2}}$, where $i_{1}\in[n_{1}],j_{1}\in[d_{1}],i_{2}\in[n_{2}],j_{2}\in[d_{2}]$
    is defined by $X_{i,j}=\mathrm{mat}(x)_{i,j}:=x_{(i-1)\cdot n+j}$.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $n_{1},n_{2},d_{1},d_{2}$ 和 $B\in\mathbb{R}^{n_{2}\times d_{2}}$，以及 $B$，其中
    $(A\otimes B)_{(i_{1}-1)n_{2}+i_{2},(j_{1}-1)d_{2}+j_{2}}$，其中 $i_{1}\in[n_{1}],j_{1}\in[d_{1}],i_{2}\in[n_{2}],j_{2}\in[d_{2}]$
    由 $X_{i,j}=\mathrm{mat}(x)_{i,j}:=x_{(i-1)\cdot n+j}$ 定义。
- en: '![Refer to caption](img/6c1c1e971b27b856ec67768788f9b1ad.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6c1c1e971b27b856ec67768788f9b1ad.png)'
- en: 'Figure 5: The visualization of the functions $\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times
    n}$. We have $x\in\mathbb{R}^{n^{2}}$. In this figure, we give an example of $n=3$,
    the first three entries of the vector $x$, $X_{1,2}$ respectively, the second
    three entries of the vector $x$, $X_{2,2}$ respectively, and the third three entries
    of the vector $x$, $X_{3,2}$ respectively. For the right figure, every entry in
    $X$ by $\operatorname{vec}$.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：函数 $\mathrm{mat}:\mathbb{R}^{n^{2}}\to\mathbb{R}^{n\times n}$ 的可视化。我们有 $x\in\mathbb{R}^{n^{2}}$。在此图中，我们给出了
    $n=3$ 的示例，向量 $x$ 的前三个条目，分别是 $X_{1,2}$，向量 $x$ 的第二组三个条目，分别是 $X_{2,2}$，以及向量 $x$ 的第三组三个条目，分别是
    $X_{3,2}$。对于右侧的图形，$X$ 中的每个条目都由 $\operatorname{vec}$ 表示。
- en: 4.1 Basic Facts
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基本事实
- en: In this section, we will introduce the basic mathematical facts.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将介绍基本的数学事实。
- en: Fact 4.1.
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 事实 4.1。
- en: Let $a,b\in\mathbb{R}$.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $a,b\in\mathbb{R}$。
- en: For all vectors $u,v,w\in\mathbb{R}^{n}$, we have
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有向量 $u,v,w\in\mathbb{R}^{n}$，我们有
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\langle u,v\rangle=\langle u\circ v,{\bf 1}_{n}\rangle=u^{\top}\mathrm{diag}(v){\bf
    1}_{n}$
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u,v\rangle=\langle u\circ v,{\bf 1}_{n}\rangle=u^{\top}\mathrm{diag}(v){\bf
    1}_{n}$
- en: •
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\langle u\circ v,w\rangle=\langle u\circ w,v\rangle$
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u\circ v,w\rangle=\langle u\circ w,v\rangle$
- en: •
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\langle u\circ v,w\rangle=\langle u\circ v\circ w,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v)w$
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u\circ v,w\rangle=\langle u\circ v\circ w,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v)w$
- en: •
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\langle u\circ v\circ w\circ z,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v\circ
    w)z$
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u\circ v\circ w\circ z,{\bf 1}_{n}\rangle=u^{\top}\operatorname{diag}(v\circ
    w)z$
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $u\circ v=v\circ u=\operatorname{diag}(u)\cdot v=\operatorname{diag}(v)\cdot
    u$
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $u\circ v=v\circ u=\operatorname{diag}(u)\cdot v=\operatorname{diag}(v)\cdot
    u$
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\operatorname{diag}(u)^{\top}=\operatorname{diag}(u)$
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\operatorname{diag}(u)^{\top}=\operatorname{diag}(u)$
- en: •
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\operatorname{diag}(u)\cdot\operatorname{diag}(v)\cdot{\bf 1}_{n}=\operatorname{diag}(u)v$
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\operatorname{diag}(u)\cdot\operatorname{diag}(v)\cdot{\bf 1}_{n}=\operatorname{diag}(u)v$
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\operatorname{diag}(u\circ v)=\operatorname{diag}(u)\operatorname{diag}(v)$
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\operatorname{diag}(u\circ v)=\operatorname{diag}(u)\operatorname{diag}(v)$
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\operatorname{diag}(u)+\operatorname{diag}(v)=\operatorname{diag}(u+v)$
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\operatorname{diag}(u)+\operatorname{diag}(v)=\operatorname{diag}(u+v)$
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\langle u,v\rangle=\langle v,u\rangle$
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u,v\rangle=\langle v,u\rangle$
- en: •
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\langle u,v\rangle=u^{\top}v=v^{\top}u$
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\langle u,v\rangle=u^{\top}v=v^{\top}u$
- en: •
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2。
- en: Fact 4.2.
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 事实 4.2。
- en: Let .
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 设 。
- en: For vectors $x,y\in\mathbb{R}^{n}$ we have
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于向量 $x,y\in\mathbb{R}^{n}$，我们有
- en: •
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|x\circ y\|_{2}\leq\|x\|_{\infty}\cdot\|y\|_{2}$
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|x\circ y\|_{2}\leq\|x\|_{\infty}\cdot\|y\|_{2}$
- en: •
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|x\|_{\infty}\leq\|x\|_{2}\leq\sqrt{n}\|x\|_{\infty}$
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|x\|_{\infty}\leq\|x\|_{2}\leq\sqrt{n}\|x\|_{\infty}$
- en: •
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|\exp(x)\|_{\infty}\leq\exp(\|x\|_{2})$
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\exp(x)\|_{\infty}\leq\exp(\|x\|_{2})$
- en: •
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|x+y\|_{2}\leq\|x\|_{2}+\|y\|_{2}$
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|x+y\|_{2}\leq\|x\|_{2}+\|y\|_{2}$
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|\alpha x\|_{2}\leq|\alpha|\cdot\|x\|_{2}$
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\alpha x\|_{2}\leq|\alpha|\cdot\|x\|_{2}$
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For any $\|x\|_{2},\|y\|_{2}\leq R$
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于任何 $\|x\|_{2},\|y\|_{2}\leq R$
- en: Fact 4.3.
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 事实 4.3。
- en: For matrices $X,Y\in\mathbb{R}^{n\times n}$, we have
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于矩阵 $X,Y\in\mathbb{R}^{n\times n}$，我们有
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|X^{\top}\|=\|X\|$
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|X^{\top}\|=\|X\|$
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|X\|\geq\|Y\|-\|X-Y\|$
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|X\|\geq\|Y\|-\|X-Y\|$
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|X+Y\|\leq\|X\|+\|Y\|$
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|X+Y\|\leq\|X\|+\|Y\|$
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|X\cdot Y\|\leq\|X\|\cdot\|Y\|$
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|X\cdot Y\|\leq\|X\|\cdot\|Y\|$
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If $X\preceq\alpha\cdot Y$
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 $X\preceq\alpha\cdot Y$
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|Yx\|_{2}\leq\|Y\|\cdot\|x\|_{2}$
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|Yx\|_{2}\leq\|Y\|\cdot\|x\|_{2}$
- en: Fact 4.4.
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 事实 4.4。
- en: For any vectors $u,v\in\mathbb{R}^{n}$, we have
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意向量 $u,v\in\mathbb{R}^{n}$，我们有
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. $uu^{\top}\preceq\|u\|_{2}^{2}\cdot I_{n}$
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分 $uu^{\top}\preceq\|u\|_{2}^{2}\cdot I_{n}$
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2\. $\operatorname{diag}(u)\preceq\|u\|_{2}\cdot I_{n}$
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分 $\operatorname{diag}(u)\preceq\|u\|_{2}\cdot I_{n}$
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3\. $\operatorname{diag}(u\circ u)\preceq\|u\|_{2}^{2}\cdot I_{n}$
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3部分 $\operatorname{diag}(u\circ u)\preceq\|u\|_{2}^{2}\cdot I_{n}$
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4\. $uv^{\top}+vu^{\top}\preceq uu^{\top}+vv^{\top}$
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4部分 $uv^{\top}+vu^{\top}\preceq uu^{\top}+vv^{\top}$
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 5\. $uv^{\top}+vu^{\top}\succeq-(uu^{\top}+vv^{\top})$
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5部分 $uv^{\top}+vu^{\top}\succeq-(uu^{\top}+vv^{\top})$
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 6\. $(v\circ u)(v\circ u)^{\top}\preceq\|v\|^{2}_{\infty}uu^{\top}$
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第6部分 $(v\circ u)(v\circ u)^{\top}\preceq\|v\|^{2}_{\infty}uu^{\top}$
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 7\. $\operatorname{diag}(u\circ v)\preceq\|u\|_{2}\|v\|_{2}\cdot I_{n}$
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第7部分 $\operatorname{diag}(u\circ v)\preceq\|u\|_{2}\|v\|_{2}\cdot I_{n}$
- en: Fact 4.5.
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 事实 4.5。
- en: Let $g,f:\mathbb{R}^{d}\to\mathbb{R}^{n}$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $g,f:\mathbb{R}^{d}\to\mathbb{R}^{n}$。
- en: Let $x\in\mathbb{R}^{d}$ be an arbitrary vector.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $x\in\mathbb{R}^{d}$ 为任意向量。
- en: Let $a\in\mathbb{R}$ be an arbitrary real number.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $a\in\mathbb{R}$ 为任意实数。
- en: Then, we have
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\frac{\mathrm{d}q(x)^{a}}{\mathrm{d}x}=a\cdot q(x)^{a-1}\cdot\frac{\mathrm{d}q(x)}{\mathrm{d}x}$
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\frac{\mathrm{d}q(x)^{a}}{\mathrm{d}x}=a\cdot q(x)^{a-1}\cdot\frac{\mathrm{d}q(x)}{\mathrm{d}x}$
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\frac{\mathrm{d}\|f(x)\|^{2}_{2}}{\mathrm{d}t}=2\langle f(x),\frac{\mathrm{d}f(x)}{\mathrm{d}t}\rangle$
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\frac{\mathrm{d}\|f(x)\|^{2}_{2}}{\mathrm{d}t}=2\langle f(x),\frac{\mathrm{d}f(x)}{\mathrm{d}t}\rangle$
- en: •
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2 (product rule for Hadamard product)
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2（Hadamard乘积的乘法规则）
- en: 4.2 General Definitions
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 一般定义
- en: In this section, we introduce some general definitions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们引入一些一般定义。
- en: Definition 4.6  (Index summary).
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.6（索引总结）。
- en: We use $i$ range, and $j$ range.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 $i$ 范围和 $j$ 范围。
- en: We use $i_{0},i_{1},i_{2}$, and $j_{0},j_{1},j_{2}$.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 $i_{0},i_{1},i_{2}$ 和 $j_{0},j_{1},j_{2}$。
- en: Definition 4.7.
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.7。
- en: If the following conditions hold
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $A_{1}\in\mathbb{R}^{n\times d}$
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A_{1}\in\mathbb{R}^{n\times d}$
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $A_{2}\in\mathbb{R}^{n\times d}$
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A_{2}\in\mathbb{R}^{n\times d}$
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\mathsf{A}\in\mathbb{R}^{n^{2}\times d^{2}}$
- en: –
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: For each $j_{0}\in[n]$ to be one $n\times d^{2}$
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个 $j_{0}\in[n]$，是一个 $n\times d^{2}$
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $A_{3}\in\mathbb{R}^{n\times d}$
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A_{3}\in\mathbb{R}^{n\times d}$
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $B\in\mathbb{R}^{n\times d}$ denote the $(j_{0},i_{0})$ for each $j_{0}\in[n]$
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B\in\mathbb{R}^{n\times d}$ 表示每个 $j_{0}\in[n]$ 的 $(j_{0},i_{0})$
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $X\in\mathbb{R}^{d\times d}$
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $X\in\mathbb{R}^{d\times d}$
- en: 'Our final goal is to study the loss function, defined as:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终目标是研究损失函数，定义为：
- en: '|  | $1$2 |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: we define $D(X)\in\mathbb{R}^{n\times n}$
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们定义 $D(X)\in\mathbb{R}^{n\times n}$
- en: •
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For each $j_{0}\in[n]$ to be $\langle\exp(\operatorname{\mathsf{A}}_{j_{0}}x),{\bf
    1}_{n}\rangle$ is the $j_{0}$ and $x\in\mathbb{R}^{d^{2}}$
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个 $j_{0}\in[n]$，使得 $\langle\exp(\operatorname{\mathsf{A}}_{j_{0}}x),{\bf
    1}_{n}\rangle$ 是 $j_{0}$ 和 $x\in\mathbb{R}^{d^{2}}$
- en: 'Further, for each $j_{0}\in[n],i_{0}\in[d]$ as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于每个 $j_{0}\in[n],i_{0}\in[d]$ 如下：
- en: '|  | $1$2 |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Using tensor-trick in [[72](#bib.bib72), [73](#bib.bib73)], we can see that
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [[72](#bib.bib72), [73](#bib.bib73)] 中的张量技巧，我们可以看到
- en: '|  | $\displaystyle L(X,Y)=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}L(X,Y)_{j_{0},i_{0}}.$
    |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(X,Y)=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}L(X,Y)_{j_{0},i_{0}}.$
    |  |'
- en: 4.3 Helpful Definitions With Respect to $X$
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 关于 $X$ 的有用定义
- en: Now, we introduce a few helpful definitions related to $X\in\mathbb{R}^{d\times
    d}$.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们引入一些与 $X\in\mathbb{R}^{d\times d}$ 相关的有用定义。
- en: Definition 4.8.
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.8。
- en: Let $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$, and $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$
    block from $\operatorname{\mathsf{A}}$.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times d^{2}}$，且
    $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$ 是 $\operatorname{\mathsf{A}}$
    的一个块。
- en: 'We define $u(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$ as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $u(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$ 如下：
- en: '|  | $\displaystyle u(x)_{j_{0}}:=\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1}.$ |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle u(x)_{j_{0}}:=\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1}.$ |  |'
- en: Definition 4.9.
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.9。
- en: Let $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times
    d^{2}}$, and $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$
    block from $\operatorname{\mathsf{A}}$.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\operatorname{\mathsf{A}}=A_{1}\otimes A_{2}\in\mathbb{R}^{n^{2}\times d^{2}}$，而
    $\operatorname{\mathsf{A}}_{j_{0}}\in\mathbb{R}^{n\times d^{2}}$ 是从 $\operatorname{\mathsf{A}}$
    中的一个块。
- en: 'We define $\alpha(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$ as:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $\alpha(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$ 如下：
- en: '|  | $\displaystyle\alpha(x)_{j_{0}}:=\langle\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1},\underbrace{{\bf 1}_{n}}_{n\times 1}\rangle.$ |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\alpha(x)_{j_{0}}:=\langle\underbrace{\exp(\operatorname{\mathsf{A}}_{j_{0}}x)}_{n\times
    1},\underbrace{{\bf 1}_{n}}_{n\times 1}\rangle.$ |  |'
- en: Definition 4.10.
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.10。
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as in Definition [4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 按定义 [4.9](#S4.Thmtheorem9 "Definition 4.9\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 定义。'
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as in Definition [4.8](#S4.Thmtheorem8
    "Definition 4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '设 $u(x)_{j_{0}}\in\mathbb{R}^{n}$ 按定义 [4.8](#S4.Thmtheorem8 "Definition 4.8\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 定义。'
- en: We define $f(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $f(x)_{j_{0}}:\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}^{n}$
- en: '|  | $\displaystyle f(x)_{j_{0}}:=\underbrace{\alpha(x)_{j_{0}}^{-1}}_{\mathrm{scalar}}\underbrace{u(x)_{j_{0}}}_{n\times
    1}.$ |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(x)_{j_{0}}:=\underbrace{\alpha(x)_{j_{0}}^{-1}}_{\mathrm{scalar}}\underbrace{u(x)_{j_{0}}}_{n\times
    1}.$ |  |'
- en: 4.4 A Helpful Definition With Respect to $Y$
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 关于 $Y$ 的有用定义
- en: In this section, we introduce a helpful definition related to $Y\in\mathbb{R}^{d\times
    d}$.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了与 $Y\in\mathbb{R}^{d\times d}$ 相关的有用定义。
- en: Definition 4.11.
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.11。
- en: 'For each $i_{0}\in[d]$ as:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 $i_{0}\in[d]$，如下：
- en: '|  | $\displaystyle h(Y)_{i_{0}}:=\underbrace{A_{3}}_{n\times d}\underbrace{Y_{*,i_{0}}}_{d\times
    1}.$ |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h(Y)_{i_{0}}:=\underbrace{A_{3}}_{n\times d}\underbrace{Y_{*,i_{0}}}_{d\times
    1}.$ |  |'
- en: 4.5 Helpful Definitions With Respect to Both $X$
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 关于 $X$ 的有用定义
- en: In this section, we introduce some helpful definitions related to both $X\in\mathbb{R}^{d\times
    d}$.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一些与 $X\in\mathbb{R}^{d\times d}$ 相关的有用定义。
- en: Definition 4.12.
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.12。
- en: 'We define $c(x,y)_{j_{0},i_{0}}:\mathbb{R}^{d^{2}}\times\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$
    as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $c(x,y)_{j_{0},i_{0}}:\mathbb{R}^{d^{2}}\times\mathbb{R}^{d^{2}}\rightarrow\mathbb{R}$
    如下：
- en: '|  | $\displaystyle c(x,y)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}.$
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(x,y)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}.$
    |  |'
- en: Furthermore, we define $c(x,:)_{j_{0},i_{0}}$ as follows
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们定义 $c(x,:)_{j_{0},i_{0}}$ 如下
- en: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  |'
- en: for some fixed vector $v\in\mathbb{R}^{n}$ and also doesn’t depend on $y$.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个固定向量 $v\in\mathbb{R}^{n}$，且也不依赖于 $y$。
- en: Similarly, we also define $c(:,y)_{j_{0},i_{0}}$ as follows
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们还定义 $c(:,y)_{j_{0},i_{0}}$ 如下
- en: '|  | $\displaystyle c(:,y)_{j_{0},i_{0}}:=\langle v,h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}$
    |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(:,y)_{j_{0},i_{0}}:=\langle v,h(y)_{i_{0}}\rangle-b_{j_{0},i_{0}}$
    |  |'
- en: for some fixed vector $v\in\mathbb{R}^{n}$ and also doesn’t depend on $y$.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个固定向量 $v\in\mathbb{R}^{n}$，且也不依赖于 $y$。
- en: Definition 4.13.
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.13。
- en: We define
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}:=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}:=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |'
- en: and
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\displaystyle L(:,y)_{j_{0},i_{0}}:=0.5c(:,y)_{j_{0},i_{0}}^{2}$ |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(:,y)_{j_{0},i_{0}}:=0.5c(:,y)_{j_{0},i_{0}}^{2}$ |  |'
- en: and
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\displaystyle L(x,y)_{j_{0},i_{0}}:=0.5c(x,y)_{j_{0},i_{0}}^{2}$ |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(x,y)_{j_{0},i_{0}}:=0.5c(x,y)_{j_{0},i_{0}}^{2}$ |  |'
- en: 4.6 Regularization
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 正则化
- en: In this section, we define the regularization loss we use.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们定义我们使用的正则化损失。
- en: Definition 4.14.
  id: totrans-314
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.14。
- en: Let $W\in\mathbb{R}^{n\times n}$ denote a positive diagonal matrix. We use the
    following regularization loss
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $W\in\mathbb{R}^{n\times n}$ 为正对角矩阵。我们使用以下正则化损失
- en: '|  | $\displaystyle\&#124;(W\otimes I)(A_{1}\otimes A_{2})x\&#124;_{2}^{2}+\&#124;WA_{3}y\&#124;_{F}^{2}$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;(W\otimes I)(A_{1}\otimes A_{2})x\&#124;_{2}^{2}+\&#124;WA_{3}y\&#124;_{F}^{2}$
    |  |'
- en: Note that $1$2.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 $1$2。
- en: 4.7 Fast Matrix Multiplication
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 快速矩阵乘法
- en: We use ${\cal T}_{\mathrm{mat}}(a,b,c)$ matrix with another $b\times c$ matrix.
    Fast matrix multiplication [[44](#bib.bib44), [182](#bib.bib182), [105](#bib.bib105),
    [78](#bib.bib78), [34](#bib.bib34), [15](#bib.bib15), [62](#bib.bib62), [106](#bib.bib106),
    [189](#bib.bib189)] is a fundamental tool in theoretical computer science.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ${\cal T}_{\mathrm{mat}}(a,b,c)$ 矩阵与另一个 $b\times c$ 矩阵。快速矩阵乘法 [[44](#bib.bib44),
    [182](#bib.bib182), [105](#bib.bib105), [78](#bib.bib78), [34](#bib.bib34), [15](#bib.bib15),
    [62](#bib.bib62), [106](#bib.bib106), [189](#bib.bib189)] 是理论计算机科学中的一个基本工具。
- en: Fact 4.15.
  id: totrans-320
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 事实 4.15。
- en: ${\cal T}_{\mathrm{mat}}(a,b,c)=O({\cal T}_{\mathrm{mat}}(b,a,c))=O({\cal T}_{\mathrm{mat}}(a,c,b))$.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ${\cal T}_{\mathrm{mat}}(a,b,c)=O({\cal T}_{\mathrm{mat}}(b,a,c))=O({\cal T}_{\mathrm{mat}}(a,c,b))$。
- en: For $k\in\mathbb{R}_{+}$ to be the value such that $\forall n\in\mathbb{N}$.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $k\in\mathbb{R}_{+}$，使得 $\forall n\in\mathbb{N}$。
- en: For convenience, we define three special values of $\omega(k)$ to be the fast
    matrix multiplication exponent, i.e., $\omega:=\omega(1)$ to be the dual exponent
    of matrix multiplication, i.e., $\omega(\alpha)=2$.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们定义 $\omega(k)$ 的三个特殊值为快速矩阵乘法的指数，即 $\omega:=\omega(1)$ 作为矩阵乘法的对偶指数，即
    $\omega(\alpha)=2$。
- en: The following fact can be found in Lemma 3.6 of [[88](#bib.bib88)], also see
    [[21](#bib.bib21)].
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的事实可以在 [[88](#bib.bib88)] 的引理 3.6 中找到，也参见 [[21](#bib.bib21)]。
- en: Fact 4.16  (Convexity of $\omega(k)$).
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 事实 4.16  (函数 $\omega(k)$ 的凸性)。
- en: The function $\omega(k)$ is convex.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\omega(k)$ 是凸的。
- en: 5 Gradient
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 梯度
- en: 'In Section [5.1](#S5.SS1 "5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we show the gradient with respect
    to variables $x$. In Section [5.3](#S5.SS3 "5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    running time of $c,f,h$ to compute time complexity. In Section [5.5](#S5.SS5 "5.5
    Reformulating Gradient (𝑦) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), we reformulate the gradient with respect
    to $Y$ to compute time complexity.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 [5.1](#S5.SS1 "5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") 节中，我们展示了关于变量 $x$ 的梯度。在第 [5.3](#S5.SS3
    "5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") 节中，我们计算了 $c,f,h$ 的运行时间以计算时间复杂度。在第 [5.5](#S5.SS5 "5.5
    Reformulating Gradient (𝑦) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") 节中，我们重新表述了关于 $Y$ 的梯度以计算时间复杂度。'
- en: 5.1 Gradient for $x$
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 对 $x$ 的梯度
- en: In this section, we compute the gradient for $x$. Most of the following gradient
    computations can be found in [[72](#bib.bib72), [73](#bib.bib73)].
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们计算 $x$ 的梯度。大多数后续的梯度计算可以在 [[72](#bib.bib72), [73](#bib.bib73)] 中找到。
- en: Lemma 5.1  (Gradient with respect to $x$).
  id: totrans-331
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 5.1  (关于 $x$ 的梯度)。
- en: If the following conditions hold
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For each $i\in[d^{2}]$ denote the $i$
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个 $i\in[d^{2}]$，表示 $i$
- en: •
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.8](#S4.Thmtheorem8
    "Definition 4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $u(x)_{j_{0}}\in\mathbb{R}^{n}$ 如定义[4.8](#S4.Thmtheorem8 "Definition 4.8\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 定义。'
- en: •
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 如定义[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 定义。'
- en: •
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 如定义 [4.10](#S4.Thmtheorem10 "定义 4.10。 ‣ 4.3
    与 𝑋 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新构建 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义 [4.12](#S4.Thmtheorem12 "定义 4.12。
    ‣ 4.5 与 𝑋 和 𝑌 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新构建 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $L(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.13](#S4.Thmtheorem13
    "Definition 4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $L(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义 [4.13](#S4.Thmtheorem13 "定义 4.13。
    ‣ 4.5 与 𝑋 和 𝑌 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新构建 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: Then, for each $i\in[d^{2}]$, we have
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个 $i\in[d^{2}]$，我们有
- en: •
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 1 部分。
- en: '|  | $\displaystyle\frac{\mathrm{d}u(x)_{j_{0}}}{\mathrm{d}x_{i}}=u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}$
    |  |'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}u(x)_{j_{0}}}{\mathrm{d}x_{i}}=u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}$
    |  |'
- en: •
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 2 部分。
- en: '|  | $\displaystyle\frac{\mathrm{d}\alpha(x)_{j_{0}}}{\mathrm{d}x_{i}}=\langle
    u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{i_{0},i},{\bf 1}_{n}\rangle$ |  |'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\alpha(x)_{j_{0}}}{\mathrm{d}x_{i}}=\langle
    u(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{i_{0},i},{\bf 1}_{n}\rangle$ |  |'
- en: •
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 3 部分。
- en: '|  | $1$2 |  |'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4. For a fixed vector $v\in\mathbb{R}^{n}$), we have
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 4 部分。对于固定向量 $v\in\mathbb{R}^{n}$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '![Refer to caption](img/1863170410eedf795b70b046b0768e9c.png)'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考标题](img/1863170410eedf795b70b046b0768e9c.png)'
- en: 'Figure 6: The visualization of Part 4 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $v$. For the right-hand side, we have three steps. Step 1: we compute the
    Hadamard product of $f(x)_{j_{0}}$. Step 2: We find the inner product of this
    Hadamard product and $v$ and $v$ and $\operatorname{\mathsf{A}}_{j_{0},i}$. The
    red rectangles represent the vector $v$.'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 6: 引用定理 [5.1](#S5.Thmtheorem1 "定理 5.1（相对于 𝑥 的梯度）。 ‣ 5.1 𝑥 的梯度 ‣ 5 梯度 ‣ 快速优化视角：基于张量和
    SVM 技巧重新构建 LLM 中的单层注意力，并在矩阵乘法时间内求解") 的第 4 部分可视化。我们给定 $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    和 $v$。对于右侧，有三个步骤。步骤 1：计算 $f(x)_{j_{0}}$ 的 Hadamard 乘积。步骤 2：找到该 Hadamard 乘积与 $v$
    的内积，以及 $v$ 和 $\operatorname{\mathsf{A}}_{j_{0},i}$ 的内积。红色矩形表示向量 $v$。'
- en: •
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 5. For each $i_{0}\in[d]$
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 5 部分。对于每个 $i_{0}\in[d]$
- en: '|  | $1$2 |  |'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 6.
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 6 部分。
- en: '|  | $1$2 |  |'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 7. (for hessian diagonal term)
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 7 部分。（对于 Hessian 对角线项）
- en: '|  | $1$2 |  |'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '![Refer to caption](img/506215cafa5031425b4de08e4ebc4f1b.png)'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考标题](img/506215cafa5031425b4de08e4ebc4f1b.png)'
- en: 'Figure 7: The visualization of Part 7 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $\operatorname{\mathsf{A}}_{j_{0},i}$ with respect to $x_{i}\in\mathbb{R}$
    and $v$ and $v$ and $\operatorname{\mathsf{A}}_{j_{0},i}$. The red rectangles
    represent the vector $v$.'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 7: 引用定理 [5.1](#S5.Thmtheorem1 "定理 5.1（相对于 𝑥 的梯度）。 ‣ 5.1 𝑥 的梯度 ‣ 5 梯度 ‣ 快速优化视角：基于张量和
    SVM 技巧重新构建 LLM 中的单层注意力，并在矩阵乘法时间内求解") 的第 7 部分可视化。我们给定 $f(x)_{j_{0}},v,\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    和 $\operatorname{\mathsf{A}}_{j_{0},i}$ 关于 $x_{i}\in\mathbb{R}$ 和 $v$ 以及 $v$ 和
    $\operatorname{\mathsf{A}}_{j_{0},i}$。红色矩形表示向量 $v$。'
- en: •
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 8. (for hessian off-diagonal term)
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 8 部分。（对于 Hessian 非对角线项）
- en: '|  | $1$2 |  |'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 9 (for hessian diagonal term, this can be obtained by using Part 4 as a
    black-box)
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第9部分（对于 Hessian 对角线项，这可以通过使用第4部分作为黑箱获得）
- en: '|  | $1$2 |  |'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '![Refer to caption](img/7789ea533c8b22818e08c27ec76bbfba.png)'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见标题](img/7789ea533c8b22818e08c27ec76bbfba.png)'
- en: 'Figure 8: The visualization of Part 9 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). We are given $f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    and $\operatorname{\mathsf{A}}_{j_{0},i}$. For the right-hand side, we have three
    steps. Step 1: we compute the Hadamard product of $\operatorname{\mathsf{A}}_{j_{0},i}$.
    Step 2: We find the inner product of $f(x)_{j_{0}}$ and $\operatorname{\mathsf{A}}_{j_{0},i}$.
    The green rectangles represent the vector $\operatorname{\mathsf{A}}_{j_{0},i}$.'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图8：引理 [5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to 𝑥). ‣ 5.1
    Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") 第9部分的可视化。我们给出 $f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\in\mathbb{R}^{n}$
    和 $\operatorname{\mathsf{A}}_{j_{0},i}$。对于右侧，有三个步骤。步骤1：我们计算 $\operatorname{\mathsf{A}}_{j_{0},i}$
    的 Hadamard 乘积。步骤2：我们找到 $f(x)_{j_{0}}$ 和 $\operatorname{\mathsf{A}}_{j_{0},i}$
    的内积。绿色矩形表示向量 $\operatorname{\mathsf{A}}_{j_{0},i}$。'
- en: •
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 10 (for hessian off-diagonal term, this can be obtained by using Part 4
    as a black-box)
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第10部分（对于 Hessian 非对角线项，这可以通过使用第4部分作为黑箱获得）
- en: '|  | $1$2 |  |'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-382
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1. See Part 4 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (Page
    14).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的证明。参见 [[72](#bib.bib72)] 的第4部分的引理 5.18 的证明（第14页）。
- en: Proof of Part 2. See Part 5 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (Page
    14).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分的证明。参见 [[72](#bib.bib72)] 的第5部分的引理 5.18 的证明（第14页）。
- en: Proof of Part 3. See Part 9 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (page
    15).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 第3部分的证明。参见 [[72](#bib.bib72)] 的第9部分的引理 5.18 的证明（第15页）。
- en: Proof of Part 4. See Part 14 of Proof of Lemma 5.18 in [[72](#bib.bib72)] (page
    15).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 第4部分的证明。参见 [[72](#bib.bib72)] 的第14部分的引理 5.18 的证明（第15页）。
- en: Proof of Part 5.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 第5部分的证明。
- en: 'Note that by Definition [4.12](#S4.Thmtheorem12 "Definition 4.12\. ‣ 4.5 Helpful
    Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we have'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，根据定义 [4.12](#S4.Thmtheorem12 "Definition 4.12\. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，我们有'
- en: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  | (3) |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(x,:)_{j_{0},i_{0}}:=\langle f(x)_{j_{0}},v\rangle-b_{j_{0},i_{0}}$
    |  | (3) |'
- en: Therefore, we have
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '|  | $\displaystyle\frac{\mathrm{d}c(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from Eq. ([3](#S5.E3 "In Proof. ‣ 5.1 Gradient for
    𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the second step follows from $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=0$,
    and the third step is due to Part 4.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步来自于 Eq. ([3](#S5.E3 "In Proof. ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"))，第二步是因为 $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=0$，第三步是由于第4部分。'
- en: 'Proof of Part 6. Noted that by Definition [4.13](#S4.Thmtheorem13 "Definition
    4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we have'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '第6部分的证明。注意，根据定义 [4.13](#S4.Thmtheorem13 "Definition 4.13\. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，我们有'
- en: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |
    (4) |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(x,:)_{j_{0},i_{0}}=0.5c(x,:)_{j_{0},i_{0}}^{2}$ |  |
    (4) |'
- en: Therefore, we have
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,:)_{j_{0},i_{0}}}{\mathrm{d}x_{i}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to Eq. ([4](#S5.E4 "In Proof. ‣ 5.1 Gradient for
    𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the second step is because of chain rule of derivative, the last step
    comes from Part 5.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步是由于公式 ([4](#S5.E4 "在证明 ‣ 5.1 梯度对于 𝑥 ‣ 5 梯度 ‣ 快速优化视角：基于张量和支持向量机技巧的单层注意力重构及矩阵乘法时间解决方案")),
    第二步是由于导数链式法则，最后一步来自第5部分。
- en: Proof of Part 7.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 第7部分的证明。
- en: We have
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{i}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step comes from Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the third step is because of Part 4, the fourth step is owing to simple
    algebra, the fifth step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step comes from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步是由于事实 [4.5](#S4.Thmtheorem5 "事实 4.5\. ‣ 4.1 基本事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和支持向量机技巧的单层注意力重构及矩阵乘法时间解决方案"),
    第二步来自事实 [4.5](#S4.Thmtheorem5 "事实 4.5\. ‣ 4.1 基本事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和支持向量机技巧的单层注意力重构及矩阵乘法时间解决方案"),
    第三步是由于第4部分，第四步是由于简单的代数，第五步由事实 [4.1](#S4.Thmtheorem1 "事实 4.1\. ‣ 4.1 基本事实 ‣ 4 初步
    ‣ 快速优化视角：基于张量和支持向量机技巧的单层注意力重构及矩阵乘法时间解决方案")，最后一步来自事实 [4.1](#S4.Thmtheorem1 "事实
    4.1\. ‣ 4.1 基本事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和支持向量机技巧的单层注意力重构及矩阵乘法时间解决方案")。
- en: Proof of Part 8.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 第8部分的证明。
- en: We have
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{l}}=$
    |  |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle}{\mathrm{d}x_{l}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is because of Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the third step follows from Part 4, the fourth step is
    due to simple algebra, the fifth step is owing to Fact [4.1](#S4.Thmtheorem1 "Fact
    4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the last step comes from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步来源于事实[4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第二步因为事实[4.5](#S4.Thmtheorem5
    "Fact 4.5\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第三步来自第4部分，第四步由于简单的代数，第五步由于事实[4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，最后一步来源于事实[4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")。'
- en: Proof of Part 9.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 第9部分的证明。
- en: We have
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{i}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step comes from Part 4, and the last step is because of Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步由于事实[4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第二步来源于第4部分，而最后一步因为事实[4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")。'
- en: Proof of Part 10. We have
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 第10部分的证明。我们有
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{l}}=$
    |  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle}{\mathrm{d}x_{l}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is owing to Part 4, and the last step is due to Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"). ∎'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步来源于事实[4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第二步由于第4部分，而最后一步由于事实[4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")。∎'
- en: 5.2 Gradient With Respect to $y$
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 关于$y$的梯度
- en: In this section, we compute the gradient with respect to $y$.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们计算关于$y$的梯度。
- en: Lemma 5.2.
  id: totrans-433
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理5.2。
- en: If the following conditions hold
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $v\in\mathbb{R}^{n}$ which doesn’t depend on x and also doesn’t depend on
    y.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$v\in\mathbb{R}^{n}$不依赖于$x$，也不依赖于$y$。
- en: •
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 关于 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和 SVM 技巧的单层注意力的重新表述，以及在矩阵乘法时间中的解决")。
- en: •
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $L(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.13](#S4.Thmtheorem13
    "Definition 4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $L(:,y)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义[4.13](#S4.Thmtheorem13 "定义 4.13\.
    ‣ 4.5 关于 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和 SVM 技巧的单层注意力的重新表述，以及在矩阵乘法时间中的解决")。
- en: •
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $h(y_{i_{0}}):=\underbrace{A_{3}}_{n\times d}\underbrace{y_{i_{0}}}_{d\times
    1}.$
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $h(y_{i_{0}}):=\underbrace{A_{3}}_{n\times d}\underbrace{y_{i_{0}}}_{d\times
    1}.$
- en: •
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $h(y_{i_{0}})=h(y)_{i_{0}}$ for convenient
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为方便起见，设 $h(y_{i_{0}})=h(y)_{i_{0}}$
- en: •
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $A_{3,*,i_{2}}\in\mathbb{R}^{n}$-th column of matrix $A_{3}\in\mathbb{R}^{n\times
    d}$
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $A_{3,*,i_{2}}\in\mathbb{R}^{n}$ 是矩阵 $A_{3}\in\mathbb{R}^{n\times d}$ 的第 $n$
    列
- en: Then, we have
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到
- en: •
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1. If $i_{1}=i_{0}$
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分。如果 $i_{1}=i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=A_{3,*,i_{2}}$
    |  |'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=A_{3,*,i_{2}}$
    |  |'
- en: •
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2. If $i_{1}\neq i_{0}$
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。如果 $i_{1}\neq i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}={\bf
    0}_{n}$ |  |'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}={\bf
    0}_{n}$ |  |'
- en: •
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3. If $i_{1}=i_{0}$
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3部分。如果 $i_{1}=i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
- en: •
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4. If $i_{1}\neq i_{0}$
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4部分。如果 $i_{1}\neq i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
- en: •
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 5. If $i_{1}=i_{0}$
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5部分。如果 $i_{1}=i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=\langle
    v,A_{3,*,i_{2}}\rangle$ |  |'
- en: •
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 6. If $i_{1}\neq i_{0}$
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第6部分。如果 $i_{1}\neq i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
- en: •
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 7. If $i_{1}=i_{0}$
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第7部分。如果 $i_{1}=i_{0}$
- en: '|  | $1$2 |  |'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 8. If $i_{1}\neq i_{0}$
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第8部分。如果 $i_{1}\neq i_{0}$
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$
    |  |'
- en: Proof.
  id: totrans-472
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: where the first step is due to the definition of $h(y_{i_{0}})$.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步是由于 $h(y_{i_{0}})$ 的定义。
- en: Proof of Part 2.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}h(y_{i_{0}})}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: where the first step is due to $i_{1}\neq i_{2}$.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步是由于 $i_{1}\neq i_{2}$。
- en: Proof of Part 3.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 第3部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the second step is due to the result of Part 1.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步来源于事实[4.5](#S4.Thmtheorem5 "事实 4.5\. ‣ 4.1 基本事实 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和
    SVM 技巧的单层注意力的重新表述，以及在矩阵乘法时间中的解决")，第二步是由于第1部分的结果。
- en: Proof of Part 4.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 第4部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}\langle v,h(y)_{i_{0}}\rangle}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is becaues of Fact [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step comes from the result of Part 2.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '首先的步骤是因为事实 [4.5](#S4.Thmtheorem5 "Fact 4.5\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第二步来自于第2部分的结果。'
- en: Proof of Part 5.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 第5部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle\ =$ |  |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ =$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from the Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is because of $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$,
    and the last step is due to Part 3.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '首先的步骤来自于定义 [4.12](#S4.Thmtheorem12 "Definition 4.12\. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第二步是因为 $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$，最后一步是由于第3部分。'
- en: Proof of Part 6.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 第6部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}c(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle\ =$ |  |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\ =$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to the Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step comes from $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$,
    and the last step is owing to Part 4.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '首先的步骤源自于定义 [4.12](#S4.Thmtheorem12 "Definition 4.12\. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第二步来自于 $\frac{\mathrm{d}b_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=0$，最后一步是由于第4部分。'
- en: Proof of Part 7.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 第7部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to the Definition [4.13](#S4.Thmtheorem13 "Definition
    4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step comes from the chain rule of derivative, and the last step is owing to Part
    5.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '首先的步骤源自于定义 [4.13](#S4.Thmtheorem13 "Definition 4.13\. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第二步来自于导数的链式法则，最后一步是由于第5部分。'
- en: Proof of Part 8.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 第8部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(:,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is because of the Definition [4.13](#S4.Thmtheorem13 "Definition
    4.13\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is due to the chain rule of derivative, and the last step comes from Part
    6. ∎'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '首先的步骤是由于定义 [4.13](#S4.Thmtheorem13 "Definition 4.13\. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第二步是由于导数的链式法则，最后一步来自于第6部分。∎'
- en: 5.3 Computation of $c,f,h$
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 计算 $c,f,h$
- en: In this section, we explain how to compute $c(x,y),f(x),h(y)$.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们解释如何计算 $c(x,y),f(x),h(y)$。
- en: Lemma 5.3.
  id: totrans-510
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理5.3。
- en: If the following conditions hold
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For each $j_{0}\in[n]$, let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ as an $n\times
    d$ matrix)
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个 $j_{0}\in[n]$，设 $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ 作为一个 $n\times d$ 矩阵
- en: •
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'For each $j_{0}\in[n]$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). (We can view $f(x)$ matrix)'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '对于每个$j_{0}\in[n]$，定义为定义[4.10](#S4.Thmtheorem10 "Definition 4.10\. ‣ 4.3 Helpful
    Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")。 （我们可以查看$f(x)$矩阵）'
- en: •
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'For each $i_{0}\in[d]$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"). (We can view $h(y)$ matrix)'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '对于每个$i_{0}\in[d]$，定义为定义[4.11](#S4.Thmtheorem11 "Definition 4.11\. ‣ 4.4 A Helpful
    Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")。 （我们可以查看$h(y)$矩阵）'
- en: •
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $A_{3}\in\mathbb{R}^{n\times d}$
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令$A_{3}\in\mathbb{R}^{n\times d}$
- en: •
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We can view $y$ matrix
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以查看$y$矩阵
- en: Then, we can compute $f,h,c$ time.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算$f,h,c$的时间。
- en: Proof.
  id: totrans-523
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'By definition [4.11](#S4.Thmtheorem11 "Definition 4.11\. ‣ 4.4 A Helpful Definition
    With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we have'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '根据定义[4.11](#S4.Thmtheorem11 "Definition 4.11\. ‣ 4.4 A Helpful Definition With
    Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")，我们有'
- en: '|  | $\displaystyle\underbrace{h(y)}_{n\times d}=\underbrace{A_{3}}_{n\times
    d}\underbrace{y}_{d\times d}.$ |  | (5) |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underbrace{h(y)}_{n\times d}=\underbrace{A_{3}}_{n\times
    d}\underbrace{y}_{d\times d}.$ |  | (5) |'
- en: First $h(y)\in\mathbb{R}^{n\times d}$ matrix ($A_{3}$ matrix ($y$.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是$h(y)\in\mathbb{R}^{n\times d}$矩阵（$A_{3}$矩阵（$y$。
- en: '![Refer to caption](img/2d3e4154621daaadac8b25aad9a865c1.png)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2d3e4154621daaadac8b25aad9a865c1.png)'
- en: 'Figure 9: The visualization of Eq. ([5](#S5.E5 "In Proof. ‣ 5.3 Computation
    of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). We have $A_{3}\in\mathbb{R}^{n\times d}$ is a function, which maps the
    matrix $y\in\mathbb{R}^{d\times d}$ by multiplying $A_{3}$. The red rectangles
    represent matrices which are the factors, and the blue rectangle represents the
    matrix which is the product.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: Eq. ([5](#S5.E5 "In Proof. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"))的可视化。我们有$A_{3}\in\mathbb{R}^{n\times
    d}$是一个函数，它通过乘以$A_{3}$来映射矩阵$y\in\mathbb{R}^{d\times d}$。红色矩形表示因子矩阵，蓝色矩形表示乘积矩阵。'
- en: We also have
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以
- en: '|  | $1$2 |  | (6) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: Then the computation of $f(x)\in\mathbb{R}^{n\times n}$.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算$f(x)\in\mathbb{R}^{n\times n}$。
- en: '![Refer to caption](img/c05b86816f745fcf485d105c5f191095.png)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c05b86816f745fcf485d105c5f191095.png)'
- en: 'Figure 10: The visualization of Eq. ([6](#S5.E6 "In Proof. ‣ 5.3 Computation
    of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). We have $A_{1},A_{2}\in\mathbb{R}^{n\times d}$, and $D(X)\in\mathbb{R}^{n\times
    n}$ and compute $\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times n}$ and $\exp(A_{1}XA_{2}^{\top})$.
    The green squares represent the square matrices in $\mathbb{R}^{n\times n}$ (the
    dark blue denotes the transpose of the matrix in $\mathbb{R}^{n\times d}$.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: Eq. ([6](#S5.E6 "In Proof. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"))的可视化。我们有$A_{1},A_{2}\in\mathbb{R}^{n\times
    d}$，以及$D(X)\in\mathbb{R}^{n\times n}$，并计算$\exp(A_{1}XA_{2}^{\top})\in\mathbb{R}^{n\times
    n}$和$\exp(A_{1}XA_{2}^{\top})$。绿色的方块表示$\mathbb{R}^{n\times n}$中的方阵（深蓝色表示$\mathbb{R}^{n\times
    d}$中的矩阵的转置）。'
- en: Given that
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 给定
- en: '|  | $\displaystyle\underbrace{c(x,y)}_{n\times d}=\underbrace{f(x)}_{n\times
    n}\underbrace{h(y)}_{n\times d}-\underbrace{B}_{n\times d}$ |  | (7) |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underbrace{c(x,y)}_{n\times d}=\underbrace{f(x)}_{n\times
    n}\underbrace{h(y)}_{n\times d}-\underbrace{B}_{n\times d}$ |  | (7) |'
- en: Then $c$.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是$c$。
- en: '![Refer to caption](img/1858c5740c54c01100bd045fbd858479.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1858c5740c54c01100bd045fbd858479.png)'
- en: 'Figure 11: The visualization of Eq. ([7](#S5.E7 "In Proof. ‣ 5.3 Computation
    of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). Let $f(x)\in\mathbb{R}^{n\times n}$ (see Figure [9](#S5.F9 "Figure 9
    ‣ Proof. ‣ 5.3 Computation of 𝑐,𝑓,ℎ ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")). We have $B\in\mathbb{R}^{n\times d}$ with $h(y)$
    from their product to get $c(x,y)\in\mathbb{R}^{n\times d}$. The blue rectangles
    represent the matrix in $\mathbb{R}^{n\times d}$.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：方程 ([7](#S5.E7 "在证明 ‣ 5.3 计算 𝑐,𝑓,ℎ ‣ 5 梯度 ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力的重新表述，并在矩阵乘法时间内解决"))
    的可视化。令 $f(x)\in\mathbb{R}^{n\times n}$（见图 [9](#S5.F9 "图 9 ‣ 证明 ‣ 5.3 计算 𝑐,𝑓,ℎ
    ‣ 5 梯度 ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力的重新表述，并在矩阵乘法时间内解决")）。我们有 $B\in\mathbb{R}^{n\times
    d}$，通过它们的乘积得到 $c(x,y)\in\mathbb{R}^{n\times d}$。蓝色矩形表示 $\mathbb{R}^{n\times d}$
    中的矩阵。
- en: ∎
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: 5.4 Reformulating Gradient ($x$) in Matrix View
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 从矩阵视角重新表述梯度（$x$）
- en: In this section, we reformulate the gradient $x$ in the matrix’s view.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从矩阵的视角重新表述梯度 $x$。
- en: Lemma 5.4.
  id: totrans-542
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 5.4。
- en: If the following conditions hold
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $c(x,y)\in\mathbb{R}^{n\times d}$
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $c(x,y)\in\mathbb{R}^{n\times d}$
- en: •
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $f(x)_{j_{0}}\in\mathbb{R}^{n}$
- en: •
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $h(y)_{i_{0}}\in\mathbb{R}^{n}$
- en: •
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $1$2
- en: •
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令
- en: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}$
    |  |'
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}$
    |  |'
- en: then, we have
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: •
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 1 部分。
- en: '|  | $1$2 |  |'
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2. Suppose $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ can be computed
    in $O(nd^{2})$ time.
  id: totrans-562
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 2 部分。假设 $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ 可以在 $O(nd^{2})$ 时间内计算。
- en: •
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3.
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 3 部分。
- en: '|  | $1$2 |  |'
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4. Suppose $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ can be computed
    in ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)$ time
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 4 部分。假设 $c(x,y),\operatorname{\mathsf{A}},f(x),h(y)$ 可以在 ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal
    T}_{\mathrm{mat}}(n,d,d)$ 时间内计算。
- en: Proof.
  id: totrans-568
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 部分的证明。
- en: 'Note that by Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we have'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于事实 [4.1](#S4.Thmtheorem1 "事实 4.1。 ‣ 4.1 基本事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM
    技巧的单层注意力的重新表述，并在矩阵乘法时间内解决")，我们有
- en: '|  | $1$2 |  |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $1$2 |  |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Thus, we complete the proof.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们完成了证明。
- en: Proof of Part 2.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 部分的证明。
- en: We first compute $(\operatorname{diag}(f(x)_{j_{0}})-f(x)_{j_{0}}f(x)_{j_{0}}^{\top})h(y)_{i_{0}}$
    time.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算 $(\operatorname{diag}(f(x)_{j_{0}})-f(x)_{j_{0}}f(x)_{j_{0}}^{\top})h(y)_{i_{0}}$
    时间。
- en: Then we can compute the rest, it takes $O(nd^{2})$ time.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以计算其余部分，这需要 $O(nd^{2})$ 时间。
- en: Proof of Part 3 and Part 4.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 部分和第 4 部分的证明。
- en: Firstly, we can compute $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以计算 $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$。
- en: Recall from the Lemma statement, we have
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 从引理陈述中回顾，我们有
- en: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}.$
    |  | (8) |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q(x,y)_{j_{0}}=\sum_{i_{0}=1}^{d}c(x,y)_{j_{0},i_{0}}h(y)_{i_{0}}.$
    |  | (8) |'
- en: Let $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$-th column of $q(x,y)$.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $q(x,y)_{j_{0}}\in\mathbb{R}^{n}$ 为 $q(x,y)$ 的第 $j_{0}$ 列。
- en: Then we have
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有
- en: '|  | $\displaystyle q(x,y)=\underbrace{h(y)}_{n\times d}\underbrace{c(x,y)^{\top}}_{d\times
    n}$ |  |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle q(x,y)=\underbrace{h(y)}_{n\times d}\underbrace{c(x,y)^{\top}}_{d\times
    n}$ |  |'
- en: This takes ${\cal T}_{\mathrm{mat}}(n,d,n)$ time.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要 ${\cal T}_{\mathrm{mat}}(n,d,n)$ 时间。
- en: Then, we compute
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算
- en: '|  | $1$2 |  | (9) |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: This takes $O(n^{2})$ time in total.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 这总共需要 $O(n^{2})$ 时间。
- en: We can show that
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以证明
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\frac{\mathrm{d}L(x,y)}{\mathrm{d}x}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is based on Definition [4.7](#S4.Thmtheorem7 "Definition
    4.7\. ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the second step is because of Part 1, the third
    step is due to Eq. ([8](#S5.E8 "In Proof. ‣ 5.4 Reformulating Gradient (𝑥) in
    Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")), the fourth step follows from Eq. ([9](#S5.E9 "In Proof. ‣ 5.4 Reformulating
    Gradient (𝑥) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")), and the last step due to tensor-trick.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步基于定义 [4.7](#S4.Thmtheorem7 "定义 4.7。‣ 4.2 一般定义 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和支持向量机技巧的单层注意力的重构，并在矩阵乘法时间内求解")，第二步由于第
    1 部分，第三步依据公式 ([8](#S5.E8 "证明中。‣ 5.4 在矩阵视角下重构梯度 (𝑥) ‣ 5 梯度 ‣ 一种快速优化视角：基于张量和支持向量机技巧的单层注意力的重构，并在矩阵乘法时间内求解"))，第四步来自公式
    ([9](#S5.E9 "证明中。‣ 5.4 在矩阵视角下重构梯度 (𝑥) ‣ 5 梯度 ‣ 一种快速优化视角：基于张量和支持向量机技巧的单层注意力的重构，并在矩阵乘法时间内求解"))，最后一步由于张量技巧。
- en: Note that $A_{1}^{\top}p(x,y)A_{2}$ time. ∎
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 $A_{1}^{\top}p(x,y)A_{2}$ 时间。∎
- en: 5.5 Reformulating Gradient ($y$) in Matrix View
  id: totrans-598
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 在矩阵视角下重构梯度 ($y$)
- en: In this section, we reformulate the gradient $y$ in the matrix’s view.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从矩阵视角重新构造梯度 $y$。
- en: Lemma 5.5.
  id: totrans-600
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 5.5。
- en: If the following conditions hold
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: if $i_{1}=i_{0}$
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 $i_{1}=i_{0}$
- en: •
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: if $i_{1}\neq i_{0}$
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 $i_{1}\neq i_{0}$
- en: •
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $1$2
- en: •
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\widetilde{q}(x,y)_{i_{0}}=\sum_{j_{0}=1}^{n}f(x)_{j_{0}}c(x,y)_{j_{0},i_{0}}$
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\widetilde{q}(x,y)_{i_{0}}=\sum_{j_{0}=1}^{n}f(x)_{j_{0}}c(x,y)_{j_{0},i_{0}}$
- en: Then we have
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有
- en: •
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 1 部分。
- en: '|  | $1$2 |  |'
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 2 部分。
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=A_{3,*,i_{2}}^{\top}\widetilde{q}(x,y)_{i_{0}}$
    |  |'
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=A_{3,*,i_{2}}^{\top}\widetilde{q}(x,y)_{i_{0}}$
    |  |'
- en: •
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3.
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 3 部分。
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(\underbrace{A_{3}^{\top}}_{d\times
    n}\underbrace{\widetilde{q}(x,y)}_{n\times d})$ |  |'
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(\underbrace{A_{3}^{\top}}_{d\times
    n}\underbrace{\widetilde{q}(x,y)}_{n\times d})$ |  |'
- en: •
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4. Computing $\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 4 部分。计算 $\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}$
- en: Proof.
  id: totrans-622
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}}=$
    |  |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step comes from the assumption from the Lemma statement and
    the second step is based on Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步来自引理声明的假设，第二步基于事实 [4.1](#S4.Thmtheorem1 "事实 4.1。‣ 4.1 基本事实 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和支持向量机技巧的单层注意力的重构，并在矩阵乘法时间内求解")。
- en: Proof of Part 2.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=$ |  |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y_{i_{0},i_{2}}}=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to the assumption from the Lemma statement, the
    second step is because of Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the last step comes from the definition of $\widetilde{q}(x,y)_{i_{0}}$
    (see from the Lemma statement).'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步来自引理声明的假设，第二步由于事实 [4.1](#S4.Thmtheorem1 "事实 4.1。‣ 4.1 基本事实 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和支持向量机技巧的单层注意力的重构，并在矩阵乘法时间内求解")，最后一步来自
    $\widetilde{q}(x,y)_{i_{0}}$ 的定义（见引理声明）。
- en: Proof of Part 3.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(A_{3}^{\top}\widetilde{q}(x,y))$
    |  |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}L(x,y)}{\mathrm{d}y}=\operatorname{vec}(A_{3}^{\top}\widetilde{q}(x,y))$
    |  |'
- en: where the first step comes from tensor trick based on Part 2.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步来自基于第 2 部分的张量技巧。
- en: Proof of Part 4. Computing $\widetilde{q}(x,y)\in\mathbb{R}^{n\times d}$ time.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 部分的证明。计算 $\widetilde{q}(x,y)\in\mathbb{R}^{n\times d}$ 时间。
- en: Computing $A_{3}^{\top}\widetilde{q}(x,y)$ time.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 计算$A_{3}^{\top}\widetilde{q}(x,y)$的时间。
- en: ∎
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: 6 Hessian
  id: totrans-638
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 Hessian
- en: In this section, we provide more details related to Hessian.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供与Hessian相关的更多细节。
- en: Finally the hessian $H\in\mathbb{R}^{2d^{2}\times 2d^{2}}$ which can be written
    as
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的Hessian矩阵$H\in\mathbb{R}^{2d^{2}\times 2d^{2}}$可以写作
- en: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H=\begin{bmatrix}H_{x,x}&amp;H_{x,y}\\ H_{y,x}&amp;H_{y,y}\end{bmatrix}$
    |  |'
- en: where
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: •
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$H_{x,x}\in\mathbb{R}^{d^{2}\times d^{2}}$ (see details in Section [7](#S7
    "7 Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  id: totrans-644
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$H_{x,x}\in\mathbb{R}^{d^{2}\times d^{2}}$（详见第[7](#S7 "7 Hessian for 𝑋 ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")节）'
- en: •
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$H_{x,y}$ is $\frac{\mathrm{d}^{2}L}{\mathrm{d}x\mathrm{d}y}$ (see details
    in Section [11](#S11 "11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"))'
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$H_{x,y}$是$\frac{\mathrm{d}^{2}L}{\mathrm{d}x\mathrm{d}y}$（详见第[11](#S11 "11
    Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")节）'
- en: •
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$H_{y,y}\in\mathbb{R}^{d^{2}\times d^{2}}$ (see details in Section [10](#S10
    "10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  id: totrans-648
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$H_{y,y}\in\mathbb{R}^{d^{2}\times d^{2}}$（详见第[10](#S10 "10 Hessian for 𝑌 ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")节）'
- en: –
  id: totrans-649
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: We can view $$H_{y,y}=\begin{bmatrix}H_{y,y,1,1}&amp;0&amp;0&amp;\cdots&amp;0\\
  id: totrans-650
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到$$H_{y,y}=\begin{bmatrix}H_{y,y,1,1}&amp;0&amp;0&amp;\cdots&amp;0\\
- en: 0&amp;H_{y,y,2,2}&amp;0&amp;\cdots&amp;0\\
  id: totrans-651
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;H_{y,y,2,2}&amp;0&amp;\cdots&amp;0\\
- en: 0&amp;0&amp;H_{y,y,3,3}&amp;\cdots&amp;0\\
  id: totrans-652
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;H_{y,y,3,3}&amp;\cdots&amp;0\\
- en: \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-653
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
- en: 0&amp;0&amp;0&amp;\cdots&amp;H_{y,y,d,d}\end{bmatrix}$$
  id: totrans-654
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;0&amp;\cdots&amp;H_{y,y,d,d}\end{bmatrix}$$
- en: –
  id: totrans-655
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: where $H_{y,y,i_{0},i_{0}}=\sum_{j_{0}=1}^{n}\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},*}\mathrm{d}y_{i_{0},*}}\in\mathbb{R}^{d\times
    d}$
  id: totrans-656
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$H_{y,y,i_{0},i_{0}}=\sum_{j_{0}=1}^{n}\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},*}\mathrm{d}y_{i_{0},*}}\in\mathbb{R}^{d\times
    d}$
- en: Lemma 6.1.
  id: totrans-657
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 6.1.
- en: If the following conditions hold
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $H_{x,x}\succeq\alpha_{1}I_{d^{2}}$
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $H_{x,x}\succeq\alpha_{1}I_{d^{2}}$
- en: •
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $H_{y,y}\succeq\alpha_{2}I_{d^{2}}$
  id: totrans-662
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $H_{y,y}\succeq\alpha_{2}I_{d^{2}}$
- en: •
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|H_{x,y}\|\leq\alpha_{3}$
  id: totrans-664
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|H_{x,y}\|\leq\alpha_{3}$
- en: •
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|H_{y,x}\|\leq\alpha_{3}$
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|H_{y,x}\|\leq\alpha_{3}$
- en: •
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let 
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设
- en: Then we have
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
- en: Proof.
  id: totrans-671
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Let $u,v\in\mathbb{R}^{d^{2}}$, then we have
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 设$u,v\in\mathbb{R}^{d^{2}}$，则我们有
- en: '|  | $\displaystyle\begin{bmatrix}u^{\top}&amp;v^{\top}\end{bmatrix}H\begin{bmatrix}u\\
    v\end{bmatrix}=$ |  |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{bmatrix}u^{\top}&amp;v^{\top}\end{bmatrix}H\begin{bmatrix}u\\
    v\end{bmatrix}=$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: '|  | $\displaystyle\geq$ |  |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\geq$ |  |'
- en: 'where the first step is based on the expansion of $H$, the third step comes
    from Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and Fact [4.3](#S4.Thmtheorem3
    "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") , the fourth step is because of $\|H_{x,y}\|\leq\alpha_{3},\|H_{y,x}\|\leq\alpha_{3}$,
    and the last step is based on the simple algebra.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步基于$H$的展开，第三步来自于事实[4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣
    4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")和事实[4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第四步由于$\|H_{x,y}\|\leq\alpha_{3},\|H_{y,x}\|\leq\alpha_{3}$，最后一步基于简单的代数运算。'
- en: Thus, it implies
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它意味着
- en: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H\succeq\{\alpha_{1}-\alpha_{3},\alpha_{2}-\alpha_{3}\}\cdot
    I_{2d^{2}}$ |  |'
- en: ∎
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Algorithm 1 Our Algorithm
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 我们的算法
- en: '1:procedure TrainingAlgorithm($A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$
    Theorem [1.3](#S1.Thmtheorem3 "Theorem 1.3 (Informal version of our main theorem).
    ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")2:     Let $x(0),y(0)\in\mathbb{R}^{d^{2}}$ do4:          /*Forward*/5:         Compute
    $h(y(t))\in\mathbb{R}^{n\times d}$ ${\cal T}_{\mathrm{mat}}(n,d,d)$ $\triangleright$
    time7:         Compute $c(x(t),y(t))\in\mathbb{R}^{n\times d}$, $h(y(t))$ ${\cal
    T}_{\mathrm{mat}}(n,d,d)$ based on Lemma [5.4](#S5.Thmtheorem4 "Lemma 5.4\. ‣
    5.4 Reformulating Gradient (𝑥) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") $\triangleright$ time10:         Compute
    $g(y(t))$ ${\cal T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)$ via
    TensorSRHT $\triangleright$13:          /*Update*/14:         $$\begin{bmatrix}x(t+1)\\'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '1:procedure TrainingAlgorithm($A_{1},A_{2},A_{3},B\in\mathbb{R}^{n\times d}$
    定理 [1.3](#S1.Thmtheorem3 "定理 1.3 (我们主要定理的非正式版本). ‣ 1 介绍 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")2:     令 $x(0),y(0)\in\mathbb{R}^{d^{2}}$
    执行4:          /*前向*/5:         计算 $h(y(t))\in\mathbb{R}^{n\times d}$ ${\cal T}_{\mathrm{mat}}(n,d,d)$
    $\triangleright$ 时间7:         计算 $c(x(t),y(t))\in\mathbb{R}^{n\times d}$, $h(y(t))$
    ${\cal T}_{\mathrm{mat}}(n,d,d)$ 基于引理 [5.4](#S5.Thmtheorem4 "引理 5.4\. ‣ 5.4 Reformulating
    Gradient (𝑥) in Matrix View ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") $\triangleright$ 时间10:         计算 $g(y(t))$ ${\cal
    T}_{\mathrm{mat}}(n,d,n)+{\cal T}_{\mathrm{mat}}(n,d,d)$ 通过TensorSRHT $\triangleright$13:         
    /*更新*/14:         $$\begin{bmatrix}x(t+1)\\'
- en: y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\
- en: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
- en: g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ $\triangleright$15:     end for16:end procedure
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ $\triangleright$15:     结束 for16:结束 procedure
- en: 7 Hessian for $X$
  id: totrans-688
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 Hessian 对于 $X$
- en: 'In Section [7.1](#S7.SS1 "7.1 Hessian ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we compute the Hessian matrix
    with respect to $x$, representing the Hessian.'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[7.1](#S7.SS1 "7.1 Hessian ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")节中，我们计算了关于$x$的Hessian矩阵，表示Hessian。'
- en: 7.1 Hessian
  id: totrans-690
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 Hessian
- en: Now, we start to compute the Hessian matrix with respect to $x$.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们开始计算关于$x$的Hessian矩阵。
- en: Lemma 7.1.
  id: totrans-692
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 7.1.
- en: If the following conditions hold
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}:=\langle f(x)_{j_{0}},v\rangle$ (We define this notation
    for easy of writing proofs.)
  id: totrans-695
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\gamma(x)_{j_{0}}:=\langle f(x)_{j_{0}},v\rangle$（我们定义这个符号以便于书写证明。）
- en: Then we have for each $i\in[d^{2}]$
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 然后对于每个$i\in[d^{2}]$我们有
- en: •
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. $i=l$ Hessian diagonal term
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分\. $i=l$ Hessian的对角线项
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-699
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{i}}=$
    |  |'
- en: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
- en: '|  |  | $\displaystyle~{}($ |  |'
  id: totrans-701
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}($ |  |'
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle(1-\gamma_{j_{0}}(x))$
    |  |'
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle(1-\gamma_{j_{0}}(x))$
    |  |'
- en: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
- en: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\gamma_{j_{0}}(x)$
    |  |'
  id: totrans-704
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\gamma_{j_{0}}(x)$
    |  |'
- en: '|  |  | $\displaystyle~{})$ |  |'
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{})$ |  |'
- en: •
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2\. $i\neq l$ Hessian off-diagonal term
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分\. $i\neq l$ Hessian的非对角线项
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{l}}=$
    |  |'
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x_{i}\mathrm{d}x_{l}}=$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-709
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+c(x,:)_{j_{0},i_{0}}\cdot$ |  |'
- en: '|  |  | $\displaystyle~{}($ |  |'
  id: totrans-711
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}($ |  |'
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle(1-\langle
    f(x)_{j_{0}},v\rangle))$ |  |'
  id: totrans-712
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle(1-\langle
    f(x)_{j_{0}},v\rangle))$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-713
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-714
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle~{})$ |  |'
  id: totrans-715
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{})$ |  |'
- en: Proof.
  id: totrans-716
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 证明第1部分。
- en: At first, we have
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有
- en: '|  |  | $1$2 |  |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}-2\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle$ |  |'
- en: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+2\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle^{2}\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
- en: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
- en: 'where the first step is based on the product rule of derivative, the second
    step comes from Part 4, Part 7, and Part 9 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step is due to simple
    algebra.'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步基于导数的乘积规则，第二步来自引理[5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect
    to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")的第4部分、第7部分和第9部分，最后一步由于简单的代数。'
- en: Then we can show that
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以证明
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{i}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{i}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'where the first step comes from Part 6 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and the second step is due to Part
    5 of Lemma [5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to 𝑥). ‣ 5.1
    Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步来自引理[5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to 𝑥). ‣ 5.1
    Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")的第6部分，第二步由于引理[5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect to
    𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")的第5部分。'
- en: Combining the above two equations, we complete the proof.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 结合上述两个方程，我们完成了证明。
- en: Proof of Part 2.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 证明第2部分。
- en: Firstly, we can show that
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以证明
- en: '|  |  | $1$2 |  |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}-\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle\cdot\langle
    f(x)_{j_{0}},v\rangle$ |  |'
- en: 'where the first step is owing to the product rule of derivative, the second
    step is based on Part 4, Part 8, and Part 10 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step comes from simple
    algebra.'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是由于导数的乘积规则，第二步基于引理[5.1](#S5.Thmtheorem1 "Lemma 5.1 (Gradient with respect
    to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")的第4部分、第8部分和第10部分，最后一步来自简单的代数。'
- en: We have
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{l}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}x_{l}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Combining the above two equations, we complete the proof. ∎
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 结合上述两个方程，我们完成了证明。∎
- en: 7.2 A Helpful Lemma
  id: totrans-759
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 有用的引理
- en: In this section, we present a helpful Lemma.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了一个有用的引理。
- en: Lemma 7.2.
  id: totrans-761
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 7.2。
- en: We have
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: •
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-764
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分。
- en: '|  | $1$2 |  |'
  id: totrans-765
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-767
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。
- en: '|  |  | $1$2 |  |'
  id: totrans-768
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-769
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: •
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3.
  id: totrans-771
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3部分。
- en: '|  | $1$2 |  |'
  id: totrans-772
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4.
  id: totrans-774
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4部分。
- en: '|  | $1$2 |  |'
  id: totrans-775
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-776
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1. We have
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的证明。我们有
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$
    |  |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$
    |  |'
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步来自于事实[4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Proof of Part 2. We have
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分的证明。我们有
- en: '|  |  | $1$2 |  |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ v,\operatorname{\mathsf{A}}_{j_{0},l}\rangle\cdot\operatorname{\mathsf{A}}_{j_{0},i}^{\top}\cdot
    f(x)_{j_{0}}$ |  |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+\langle f(x)_{j_{0}}\circ v,\operatorname{\mathsf{A}}_{j_{0},l}\rangle\cdot\operatorname{\mathsf{A}}_{j_{0},i}^{\top}\cdot
    f(x)_{j_{0}}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle~{}+\operatorname{\mathsf{A}}_{j_{0},i}^{\top}f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}\operatorname{\mathsf{A}}_{j_{0},l}$ |  |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+\operatorname{\mathsf{A}}_{j_{0},i}^{\top}f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}\operatorname{\mathsf{A}}_{j_{0},l}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle~{}+f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top})\operatorname{\mathsf{A}}_{j_{0},l}$
    |  |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top})\operatorname{\mathsf{A}}_{j_{0},l}$
    |  |'
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the last step follows from the simple algebra.'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步来自于事实[4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第二步来自于事实[4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，最后一步来自于简单的代数运算。'
- en: Proof of Part 3. We have
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 第3部分的证明。我们有
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$ |  |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},v\rangle\cdot\langle
    f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},l},v\rangle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步来自于事实[4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，最后一步来自于事实[4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")。'
- en: Proof of Part 4. We have
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 第4部分的证明。我们有
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},l}\rangle=$ |  |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},l}\rangle=$ |  |'
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"). ∎'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步遵循事实 [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。∎'
- en: 7.3 Defining $B(x)$
  id: totrans-796
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 定义 $B(x)$
- en: In this section, we formally define $B(x)$.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们正式定义 $B(x)$。
- en: Definition 7.3.
  id: totrans-798
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 7.3。
- en: If the following conditions hold
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma_{j_{0}}(x)=\langle f(x)_{j_{0}},v\rangle$
  id: totrans-801
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\gamma_{j_{0}}(x)=\langle f(x)_{j_{0}},v\rangle$
- en: We define $B(x)\in\mathbb{R}^{n\times n}$ as follows
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $B(x)\in\mathbb{R}^{n\times n}$ 如下
- en: '|  | $\displaystyle B(x):=$ |  |'
  id: totrans-803
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B(x):=$ |  |'
- en: '|  |  | $\displaystyle~{}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{3}$
    |  |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{3}$
    |  |'
- en: where
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 在哪里
- en: •
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-807
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: and
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: •
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-810
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-812
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  id: totrans-814
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
- en: Lemma 7.4.
  id: totrans-815
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 7.4。
- en: 'Let $B(x)$ be defined as Definition [7.3](#S7.Thmtheorem3 "Definition 7.3\.
    ‣ 7.3 Defining 𝐵⁢(𝑥) ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), then we have'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '令 $B(x)$ 如定义 [7.3](#S7.Thmtheorem3 "Definition 7.3\. ‣ 7.3 Defining 𝐵⁢(𝑥) ‣
    7 Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") 所定义，则我们有'
- en: '|  | $1$2 |  |'
  id: totrans-817
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-818
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'The proof follows by combining Lemma [7.1](#S7.Thmtheorem1 "Lemma 7.1\. ‣ 7.1
    Hessian ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [7.2](#S7.Thmtheorem2 "Lemma 7.2\. ‣ 7.2 A Helpful Lemma ‣ 7
    Hessian for 𝑋 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). ∎'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '证明通过结合引理 [7.1](#S7.Thmtheorem1 "Lemma 7.1\. ‣ 7.1 Hessian ‣ 7 Hessian for 𝑋
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") 和引理 [7.2](#S7.Thmtheorem2
    "Lemma 7.2\. ‣ 7.2 A Helpful Lemma ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")。∎'
- en: 8 Lipschitz Property of $H_{x,x}$
  id: totrans-820
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 Lipschitz 性质的 $H_{x,x}$
- en: 'In Section [8.1](#S8.SS1 "8.1 Main Result ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥}
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we present
    the main results of the Lipschitz property of $H_{x,x}$. In Section [8.6](#S8.SS6
    "8.6 Calculation: Step 2 Lipschitz for Matrix Function -𝛾_𝑗₀⁢(𝑥)⋅𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣)
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the second step of Lipschitz function $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$. In Section [8.8](#S8.SS8
    "8.8 Calculation: Step 4 Lipschitz for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the fourth step of Lipschitz function $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$. In Section [8.10](#S8.SS10 "8.10 Calculation: Step 6 Lipschitz
    for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    we analyze the sixth step of Lipschitz function $-c(x,:)_{j_{0},i_{0}})\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}$. In Section [8.12](#S8.SS12 "8.12 Calculation: Step 8 Lipschitz for
    Matrix Function 𝛾_𝑗₀⁢(𝑥)²⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥}
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we analyze
    the eighth step of Lipschitz function $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[8.1](#S8.SS1 "8.1 主要结果 ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧的 LLM
    单层注意力重构及其在矩阵乘法时间中的解决")节中，我们展示了 $H_{x,x}$ 的 Lipschitz 性质的主要结果。在第[8.6](#S8.SS6 "8.6
    计算：步骤 2 Lipschitz 对矩阵函数 -𝛾_𝑗₀⁢(𝑥)⋅𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣) ‣ 8 Lipschitz
    性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧的 LLM 单层注意力重构及其在矩阵乘法时间中的解决")节中，我们分析了 Lipschitz
    函数 $-\gamma_{j_{0}}(x)\cdot c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$ 的第二步。在第[8.8](#S8.SS8 "8.8 计算：步骤 4 Lipschitz 对矩阵函数 -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧的 LLM 单层注意力重构及其在矩阵乘法时间中的解决")节中，我们分析了
    Lipschitz 函数 $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$
    的第四步。在第[8.10](#S8.SS10 "8.10 计算：步骤 6 Lipschitz 对矩阵函数 -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧的 LLM 单层注意力重构及其在矩阵乘法时间中的解决")节中，我们分析了
    Lipschitz 函数 $-c(x,:)_{j_{0},i_{0}})\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$
    的第六步。在第[8.12](#S8.SS12 "8.12 计算：步骤 8 Lipschitz 对矩阵函数 𝛾_𝑗₀⁢(𝑥)²⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧的 LLM 单层注意力重构及其在矩阵乘法时间中的解决")节中，我们分析了
    Lipschitz 函数 $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$ 的第八步。
- en: 8.1 Main Result
  id: totrans-822
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 主要结果
- en: In this section, we present the main result of the Lipschitz property.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了 Lipschitz 性质的主要结果。
- en: Lemma 8.1.
  id: totrans-824
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.1。
- en: If the following conditions hold
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-827
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $1$2
- en: •
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$)
  id: totrans-829
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$)
- en: •
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")'
  id: totrans-831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ 如定义[4.8](#S4.Thmtheorem8
    "定义 4.8\. ‣ 4.3 与 𝑋 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧的 LLM 单层注意力重构及其在矩阵乘法时间中的解决")所定义。
- en: •
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-833
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 如定义[4.9](#S4.Thmtheorem9 "定义 4.9\. ‣ 4.3
    与 𝑋 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧的 LLM 单层注意力重构及其在矩阵乘法时间中的解决")所定义。
- en: •
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-835
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$f(x)_{j_{0}}\in\mathbb{R}^{n}$定义为定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣ 4.3
    关于 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内解决")
- en: •
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-837
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$定义为定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 关于 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内解决")。
- en: •
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-839
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-841
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$，$\|x\|_{2}\leq R$，$\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-843
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$R\geq 4$
- en: •
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $M:=\exp(O(R^{2}+\log(nd)))$
  id: totrans-845
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$M:=\exp(O(R^{2}+\log(nd)))$
- en: Then, we have for all $x,\widetilde{x}\in\mathbb{R}^{d^{2}}$
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，对于所有$x,\widetilde{x}\in\mathbb{R}^{d^{2}}$我们有
- en: •
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. For each $j_{0}\in[n]$
  id: totrans-848
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分\. 对于每个$j_{0}\in[n]$
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq
    M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-849
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq
    M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: •
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-851
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。
- en: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-852
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq M\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-853
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1. We have
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的证明。我们有
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq$
    |  |'
  id: totrans-855
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)-H_{j_{0},i_{0}}(\widetilde{x})\&#124;\leq$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-856
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-857
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from definition of $H_{j_{0},i_{0}}(x)$, the second
    step follows from Lemma [8.2](#S8.Thmtheorem2 "Lemma 8.2\. ‣ 8.2 Summary of Nine
    Steps ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and last step follows from simple algebra.'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步由$H_{j_{0},i_{0}}(x)$的定义得出，第二步由引理[8.2](#S8.Thmtheorem2 "引理 8.2\. ‣ 8.2
    九步总结 ‣ 8 𝐻_{𝑥,𝑥}的Lipschitz性质 ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内解决")得出，最后一步由简单代数得出。
- en: Proof of Part 2.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分的证明。
- en: Then, we have
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有
- en: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq$ |  |'
  id: totrans-861
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x)-H(\widetilde{x})\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-862
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: where the first step follows from triangle inequality and $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$,
    and the second step follows from Part 1. ∎
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步由三角不等式得出，$H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$，第二步由第1部分得出。∎
- en: 8.2 Summary of Nine Steps
  id: totrans-864
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 九步总结
- en: In this section, we provide a summary of the nine-step calculation of Lipschitz
    for different matrix functions.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了不同矩阵函数Lipschitz的九步计算方法。
- en: Lemma 8.2.
  id: totrans-866
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.2。
- en: If the following conditions hold
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
  id: totrans-869
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
- en: •
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-871
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-873
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-875
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{5}(x)=-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
  id: totrans-877
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{5}(x)=-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
- en: •
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{6}(x)=-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
  id: totrans-879
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{6}(x)=-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$)
- en: •
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-881
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{8}(x)=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-883
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{8}(x)=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: •
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  id: totrans-885
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
- en: Then, we have
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有
- en: '|  | $\displaystyle\max_{k\in[9]}\&#124;G_{k}(x)-G_{k}(\widetilde{x})\&#124;\leq
    n^{1.5}\exp(20R^{2}).$ |  |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{k\in[9]}\&#124;G_{k}(x)-G_{k}(\widetilde{x})\&#124;\leq
    n^{1.5}\exp(20R^{2}).$ |  |'
- en: Proof.
  id: totrans-888
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'The proof follows from Lemma [8.7](#S8.Thmtheorem7 "Lemma 8.7\. ‣ 8.5 Calculation:
    Step 1 Lipschitz for Matrix Function 𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣) ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), Lemma [8.8](#S8.Thmtheorem8 "Lemma 8.8\. ‣ 8.6 Calculation: Step 2 Lipschitz
    for Matrix Function -𝛾_𝑗₀⁢(𝑥)⋅𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣) ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.9](#S8.Thmtheorem9 "Lemma 8.9\. ‣ 8.7 Calculation: Step 3 Lipschitz for
    Matrix Function -2⁢𝛾_𝑗₀⁢(𝑥)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz Property of
    𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.10](#S8.Thmtheorem10 "Lemma 8.10\. ‣ 8.8 Calculation: Step 4 Lipschitz
    for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.11](#S8.Thmtheorem11 "Lemma 8.11\. ‣ 8.9 Calculation: Step 5 Lipschitz
    for Matrix Function -2⁢𝛾_𝑗₀⁢(𝑥)⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.12](#S8.Thmtheorem12 "Lemma 8.12\. ‣ 8.10 Calculation: Step 6 Lipschitz
    for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    Lemma [8.13](#S8.Thmtheorem13 "Lemma 8.13\. ‣ 8.11 Calculation: Step 7 Lipschitz
    for Matrix Function 2⁢𝛾_𝑗₀⁢(𝑥)⁢𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), Lemma [8.14](#S8.Thmtheorem14 "Lemma 8.14\. ‣ 8.12 Calculation: Step 8
    Lipschitz for Matrix Function 𝛾_𝑗₀⁢(𝑥)²⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and Lemma [8.15](#S8.Thmtheorem15 "Lemma 8.15\. ‣ 8.13 Calculation: Step 9 Lipschitz
    for Matrix Function (𝑓⁢(𝑥)_𝑗₀∘𝑣)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤ ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥}
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"). ∎'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 证明来自引理 [8.7](#S8.Thmtheorem7 "引理 8.7\. ‣ 8.5 计算：步骤 1 矩阵函数 𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣)
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")，引理
    [8.8](#S8.Thmtheorem8 "引理 8.8\. ‣ 8.6 计算：步骤 2 矩阵函数 -𝛾_𝑗₀⁢(𝑥)⋅𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅diag(𝑓⁢(𝑥)_𝑗₀∘𝑣)
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")，引理
    [8.9](#S8.Thmtheorem9 "引理 8.9\. ‣ 8.7 计算：步骤 3 矩阵函数 -2⁢𝛾_𝑗₀⁢(𝑥)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")，引理
    [8.10](#S8.Thmtheorem10 "引理 8.10\. ‣ 8.8 计算：步骤 4 矩阵函数 -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")，引理
    [8.11](#S8.Thmtheorem11 "引理 8.11\. ‣ 8.9 计算：步骤 5 矩阵函数 -2⁢𝛾_𝑗₀⁢(𝑥)⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")，引理
    [8.12](#S8.Thmtheorem12 "引理 8.12\. ‣ 8.10 计算：步骤 6 矩阵函数 -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")，引理
    [8.13](#S8.Thmtheorem13 "引理 8.13\. ‣ 8.11 计算：步骤 7 矩阵函数 2⁢𝛾_𝑗₀⁢(𝑥)⁢𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")，引理
    [8.14](#S8.Thmtheorem14 "引理 8.14\. ‣ 8.12 计算：步骤 8 矩阵函数 𝛾_𝑗₀⁢(𝑥)²⋅𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")，以及引理
    [8.15](#S8.Thmtheorem15 "引理 8.15\. ‣ 8.13 计算：步骤 9 矩阵函数 (𝑓⁢(𝑥)_𝑗₀∘𝑣)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)^⊤
    ‣ 8 Lipschitz 性质的 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决")。∎
- en: '8.3 A Core Tool: Upper Bound for Several Basic Functions'
  id: totrans-890
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 核心工具：几个基本函数的上界
- en: In this section, we analyze the upper bound of several basic functions.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析几个基本函数的上界。
- en: Lemma 8.3  ([[55](#bib.bib55), [72](#bib.bib72)]).
  id: totrans-892
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.3  ([[55](#bib.bib55), [72](#bib.bib72)])。
- en: Provided that the subsequent requirements are satisfied
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 只要满足以下要求
- en: •
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$
  id: totrans-895
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$
- en: •
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $x\in\mathbb{R}^{d^{2}}$
  id: totrans-897
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$x\in\mathbb{R}^{d^{2}}$
- en: •
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We define $u(x)$ as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3
    Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")'
  id: totrans-899
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们定义$u(x)$见定义[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 关于𝑋的有用定义 ‣ 4 初步 ‣
    快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内解决")
- en: •
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\beta$
  id: totrans-901
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$\beta$
- en: Then we have
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有
- en: '|  | $\displaystyle\beta\geq\exp(-R^{2}).$ |  |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\beta\geq\exp(-R^{2}).$ |  |'
- en: Lemma 8.4  (Basic Functions Upper Bound).
  id: totrans-904
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.4  (基本函数上界)。
- en: If the following conditions hold,
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件，
- en: •
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $u(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.8](#S4.Thmtheorem8
    "Definition 4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-907
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$u(x)_{j_{0}}\in\mathbb{R}^{n}$的定义见定义[4.8](#S4.Thmtheorem8 "Definition 4.8\.
    ‣ 4.3 关于𝑋的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内解决")
- en: •
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-909
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$\alpha(x)_{j_{0}}\in\mathbb{R}$的定义见定义[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    ‣ 4.3 关于𝑋的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内解决")
- en: •
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-911
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$f(x)_{j_{0}}\in\mathbb{R}^{n}$的定义见定义[4.10](#S4.Thmtheorem10 "Definition 4.10\.
    ‣ 4.3 关于𝑋的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内解决")
- en: •
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-913
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$的定义见定义[4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 关于𝑋和𝑌的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内解决")
- en: •
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-915
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\beta$
  id: totrans-917
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$\beta$
- en: •
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$
  id: totrans-919
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$
- en: •
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-921
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: •
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|x\|_{2}\leq R$
  id: totrans-923
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|x\|_{2}\leq R$
- en: •
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $|b_{j_{0},i_{0}}|\leq R$
  id: totrans-925
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|b_{j_{0},i_{0}}|\leq R$
- en: •
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-927
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$R\geq 4$
- en: •
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|v\|_{2}\leq R^{2}$
  id: totrans-929
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|v\|_{2}\leq R^{2}$
- en: 'Then we have: for all $x\in\mathbb{R}^{d^{2}}$'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到：对于所有$x\in\mathbb{R}^{d^{2}}$
- en: •
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. $\|u(x)_{j_{0}}\|_{2}\leq\sqrt{n}\cdot\exp(R^{2})$
  id: totrans-932
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分．$\|u(x)_{j_{0}}\|_{2}\leq\sqrt{n}\cdot\exp(R^{2})$
- en: •
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2\. $|\alpha(x)_{j_{0}}|\leq n\exp(R^{2})$
  id: totrans-934
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分．$|\alpha(x)_{j_{0}}|\leq n\exp(R^{2})$
- en: •
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3\. $|\alpha(x)_{j_{0}}|^{-1}\leq\exp(R^{2})$
  id: totrans-936
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3部分．$|\alpha(x)_{j_{0}}|^{-1}\leq\exp(R^{2})$
- en: •
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4\. $\|f(x)_{j_{0}}\|_{2}\leq 1$
  id: totrans-938
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4部分．$\|f(x)_{j_{0}}\|_{2}\leq 1$
- en: •
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 5\. $|\gamma(x)_{j_{0}}|\leq R^{2}$
  id: totrans-940
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5部分．$|\gamma(x)_{j_{0}}|\leq R^{2}$
- en: •
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 6\. $|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
  id: totrans-942
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第6部分．$|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
- en: Proof.
  id: totrans-943
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We present our proof as follows.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的证明如下。
- en: Proof of Part 1. We have
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的证明。我们有
- en: '|  | $\displaystyle\&#124;u(x)_{j_{0}}\&#124;_{2}=$ |  |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|u(x)_{j_{0}}\|_{2}=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from Definition [4.8](#S4.Thmtheorem8 "Definition
    4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on Fact [4.2](#S4.Thmtheorem2
    "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the third step follows from Fact [4.2](#S4.Thmtheorem2
    "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the fourth step is because of $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq
    R$ (see from the Lemma statement).'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步来自定义[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")，第二步基于事实[4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步来自事实[4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第四步是因为$\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq
    R$（见引理声明）。'
- en: Proof of Part 2. We have
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分的证明。我们有
- en: '|  | $\displaystyle&#124;\alpha(x)_{j_{0}}&#124;=$ |  |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\| \alpha(x)_{j_{0}} \|=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-955
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step is due to Definition [4.9](#S4.Thmtheorem9 "Definition
    4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second is based on Fact [4.2](#S4.Thmtheorem2
    "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), the third step follows from Part 1. and the forth
    step follows from simple algebra.'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是根据定义[4.9](#S4.Thmtheorem9 "Definition 4.9\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")，第二步基于事实[4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步来自第1部分，第四步来自简单的代数。'
- en: Proof of Part 3.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 第3部分的证明。
- en: We have
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  | $\displaystyle&#124;\alpha^{-1}(x)_{j_{0}}&#124;=$ |  |'
  id: totrans-959
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\| \alpha^{-1}(x)_{j_{0}} \|=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-960
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-961
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of Definition [4.9](#S4.Thmtheorem9 "Definition
    4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step follows from the
    definition of $\beta$ and the third step is due to Lemma [8.3](#S8.Thmtheorem3
    "Lemma 8.3 ([55, 72]). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是因为定义[4.9](#S4.Thmtheorem9 "Definition 4.9\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")，第二步来自$\beta$的定义，第三步是由于引理[8.3](#S8.Thmtheorem3 "Lemma 8.3
    ([55, 72]). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")。'
- en: Proof of Part 4. We have
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 第4部分的证明。我们有
- en: '|  | $\displaystyle\&#124;f(x)_{j_{0}}\&#124;_{2}\leq$ |  |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\| f(x)_{j_{0}} \|_{2} \leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the second step is due to Definition [4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步来自于事实 [4.2](#S4.Thmtheorem2 "Fact 4.2. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第二步由于定义
    [4.10](#S4.Thmtheorem10 "Definition 4.10. ‣ 4.3 Helpful Definitions With Respect
    to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")'
- en: Proof of Part 5. We have
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 部分的证明。我们有
- en: '|  | $\displaystyle&#124;\gamma(x)_{j_{0}}&#124;=$ |  |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\gamma(x)_{j_{0}}&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: where the first step follows from the definition of $\gamma(x)_{j_{0}}$ norm
    of $v$ (from the Lemma statement), and the last step follows from simple algebra.
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步由 $v$ 的 $\gamma(x)_{j_{0}}$ 范数定义得出（来自引理陈述），最后一步由简单的代数得出。
- en: Proof of Part 6. We have
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 第 6 部分的证明。我们有
- en: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}&#124;=$ |  |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is because of the definition of $\gamma_{j_{0}}(x)$ (see from the Lemma statement),
    and the last step follows from $R\geq 1$. ∎'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步基于定义 [4.12](#S4.Thmtheorem12 "Definition 4.12. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第二步是由于 $\gamma_{j_{0}}(x)$ 的定义（见引理陈述），最后一步由于 $R\geq
    1$。∎'
- en: '8.4 A Core Tool: Lipschitz Property for Several Basic Functions'
  id: totrans-980
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 一个核心工具：多个基本函数的利普希茨性质
- en: In this section, we analyze the Lipschitz property of several basic functions.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了几个基本函数的利普希茨性质。
- en: Lemma 8.5  (Basic Functions Lipschitz Property).
  id: totrans-982
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.5（基本函数的利普希茨性质）。
- en: If the following conditions hold,
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件，
- en: •
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|v\|\leq R^{2}$
  id: totrans-985
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|v\|\leq R^{2}$
- en: •
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-987
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: •
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\beta$
  id: totrans-989
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\beta$
- en: •
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\beta^{-1}\leq\exp(R^{2})$
  id: totrans-991
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\beta^{-1}\leq\exp(R^{2})$
- en: •
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-993
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: 'Then, we have: for all $x,\widetilde{x}\in\mathbb{R}^{d^{2}}$'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有：对所有 $x,\widetilde{x}\in\mathbb{R}^{d^{2}}$
- en: •
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. $1$2
  id: totrans-996
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 1 部分。$1$2
- en: •
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2\. $|\alpha(x)^{-1}-\alpha^{-1}(\widetilde{x})|\leq n\exp(4R^{2})\cdot\|x-\widetilde{x}\|_{2}$
  id: totrans-998
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 2 部分。$|\alpha(x)^{-1}-\alpha^{-1}(\widetilde{x})|\leq n\exp(4R^{2})\cdot\|x-\widetilde{x}\|_{2}$
- en: •
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3\. $1$2
  id: totrans-1000
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 3 部分。$1$2
- en: •
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4\. $1$2
  id: totrans-1002
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 4 部分。$1$2
- en: •
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 5\. $1$2
  id: totrans-1004
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 5 部分。$1$2
- en: Proof.
  id: totrans-1005
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 部分的证明。
- en: '|  | $\displaystyle\&#124;u(x)_{j_{0}}-u(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;u(x)_{j_{0}}-u(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1012
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to Definition [4.8](#S4.Thmtheorem8 "Definition
    4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is because of
    Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), the third step
    is based on Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the fourth
    step follows from Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    , fifth step is due to $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$.'
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步由于定义 [4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")，第二步因为事实 [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步基于事实 [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第四步跟随事实
    [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")，第五步由于 $\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq
    R$。'
- en: Proof of Part 2 We have
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: Part 2 的证明我们有
- en: '|  | $\displaystyle&#124;\alpha(x)^{-1}_{j_{0}}-\alpha(\widetilde{x})^{-1}_{j_{0}}&#124;\leq$
    |  |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\alpha(x)^{-1}_{j_{0}}-\alpha(\widetilde{x})^{-1}_{j_{0}}&#124;\leq$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1016
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1018
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1019
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to simple algebra, the second step is due to $\beta\geq\langle
    u(x)_{j_{0}},{\bf 1}_{n}\rangle$ (see Definition [4.9](#S4.Thmtheorem9 "Definition
    4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")), the fourth step is based on Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), the fifth step is because of Part 1, and the sixth step follows from .'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步由于简单代数，第二步由于 $\beta\geq\langle u(x)_{j_{0}},{\bf 1}_{n}\rangle$（见定义 [4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")），第四步基于事实
    [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 和事实 [4.2](#S4.Thmtheorem2 "Fact
    4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第五步因为 Part 1，第六步跟随 。'
- en: Proof of Part 3. We have
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: Part 3 的证明我们有
- en: '|  | $\displaystyle\&#124;f(x)_{j_{0}}-f(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;f(x)_{j_{0}}-f(\widetilde{x})_{j_{0}}\&#124;_{2}=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1024
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1025
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to Definition [4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on triangle
    inequality, the third step follows from Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\.
    ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), the fourth follows from combination of Part 1, Part 2 and
    Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A
    Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥}
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步基于定义 [4.10](#S4.Thmtheorem10 "定义 4.10. ‣ 4.3 关于 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和支持向量机技巧重新构建单层注意力，并在矩阵乘法时间内求解")，第二步基于三角不等式，第三步依据事实 [4.2](#S4.Thmtheorem2
    "事实 4.2. ‣ 4.1 基本事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和支持向量机技巧重新构建单层注意力，并在矩阵乘法时间内求解")，第四步则源于第
    1 部分、第 2 部分和引理 [8.4](#S8.Thmtheorem4 "引理 8.4 (基本函数上界). ‣ 8.3 核心工具：若干基本函数的上界 ‣
    8 𝐻_{𝑥,𝑥} 的 Lipschitz 性质 ‣ 快速优化视角：基于张量和支持向量机技巧重新构建单层注意力，并在矩阵乘法时间内求解") 的组合。
- en: Proof of Part 4. We have
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 部分的证明。我们有
- en: '|  | $\displaystyle&#124;\gamma_{j_{0}}(x)-\gamma_{j_{0}}(\widetilde{x})&#124;=$
    |  |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;\gamma_{j_{0}}(x)-\gamma_{j_{0}}(\widetilde{x})&#124;=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: where the first step is based on the definition of $\gamma_{j_{0}}(x)$ and $R\geq
    4$.
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步基于 $\gamma_{j_{0}}(x)$ 的定义和 $R\geq 4$。
- en: Proof of Part 5. We have
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 部分的证明。我们有
- en: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}-c(\widetilde{x},:)_{j_{0},i_{0}}&#124;=$
    |  |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,:)_{j_{0},i_{0}}-c(\widetilde{x},:)_{j_{0},i_{0}}&#124;=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is based on the definition of $\gamma_{j_{0}}(x)$ and the last step follows
    from Part 4. ∎'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步基于定义 [4.12](#S4.Thmtheorem12 "定义 4.12. ‣ 4.5 关于 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和支持向量机技巧重新构建单层注意力，并在矩阵乘法时间内求解")，第二步基于
    $\gamma_{j_{0}}(x)$ 的定义，最后一步来自第 4 部分。∎
- en: For convenient, we define
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们定义
- en: Definition 8.6.
  id: totrans-1040
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 8.6。
- en: We define $R_{0}$ as follows
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $R_{0}$ 如下
- en: '|  | $\displaystyle R_{0}:=n^{1.5}\exp(10R^{2}).$ |  |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R_{0}:=n^{1.5}\exp(10R^{2}).$ |  |'
- en: '8.5 Calculation: Step 1 Lipschitz for Matrix Function $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$'
  id: totrans-1043
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5 计算：步骤 1 Lipschitz 对于矩阵函数 $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$
- en: In this section, we introduce our calculation of Lipschitz for $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍对 $c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
    的 Lipschitz 计算。
- en: Lemma 8.7.
  id: totrans-1045
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.7。
- en: If the following conditions
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$
  id: totrans-1048
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $G_{1}(x)=c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$
- en: •
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")'
  id: totrans-1050
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $R_{0}$ 定义如定义 [8.6](#S8.Thmtheorem6 "定义 8.6. ‣ 8.4 核心工具：若干基本函数的 Lipschitz
    性质 ‣ 8 𝐻_{𝑥,𝑥} 的 Lipschitz 性质 ‣ 快速优化视角：基于张量和支持向量机技巧重新构建单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    as Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")'
  id: totrans-1052
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ 定义为定义[4.8](#S4.Thmtheorem8
    "Definition 4.8\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
- en: •
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1054
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 定义为定义[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: •
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1056
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 定义为定义[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: •
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1058
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 定义为定义[4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
- en: •
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1060
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1062
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1064
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $R\geq 4$
- en: Then, we have
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $\displaystyle\&#124;G_{1}(x)-G_{1}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1066
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1}(x)-G_{1}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1067
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{1,1}=$ |  |'
  id: totrans-1069
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,1}=$ |  |'
- en: '|  | $\displaystyle G_{1,2}=$ |  |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,2}=$ |  |'
- en: we have
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1075
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on definition $G_{1,1}$, the second step is due
    to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，第一步基于定义 $G_{1,1}$，第二步基于事实 [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步来源于引理 [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第四步则是由于引理 [8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Additionally, we have
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们有
- en: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1079
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1081
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of definition of $G_{1,2}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是由于 $G_{1,2}$ 的定义，第二步是由于事实 [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步来自引理 [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第四步是由于引理
    [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property). ‣ 8.4 A
    Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Combining the above two equations, we complete the proof. ∎
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: 结合上述两个方程，我们完成了证明。∎
- en: '8.6 Calculation: Step 2 Lipschitz for Matrix Function $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$'
  id: totrans-1084
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 计算：第2步 矩阵函数 $-\gamma_{j_{0}}(x)\cdot c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$ 的 Lipschitz 常数
- en: In this section, we introduce our calculation of Lipschitz for $-\gamma_{j_{0}}(x)\cdot
    c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ v)$.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍 $-\gamma_{j_{0}}(x)\cdot c(x,:)_{j_{0},i_{0}}\cdot\operatorname{diag}(f(x)_{j_{0}}\circ
    v)$ 的 Lipschitz 常数计算。
- en: Lemma 8.8.
  id: totrans-1086
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.8。
- en: If the following conditions hold
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-1089
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $1$2
- en: •
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1091
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 如定义[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 所定义'
- en: •
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1093
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 如定义[4.10](#S4.Thmtheorem10 "Definition 4.10\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 所定义'
- en: •
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1095
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义[4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") 所定义'
- en: •
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1097
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1099
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: Then, we have
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $\displaystyle\&#124;G_{2}(x)-G_{2}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2}(x)-G_{2}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{2,1}=$ |  |'
  id: totrans-1104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,1}=$ |  |'
- en: '|  | $\displaystyle G_{2,2}=$ |  |'
  id: totrans-1105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,2}=$ |  |'
- en: '|  | $\displaystyle G_{2,3}=$ |  |'
  id: totrans-1106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,3}=$ |  |'
- en: We have
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
  id: totrans-1108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of definition of $G_{2,1}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是因为 $G_{2,1}$ 的定义，第二步是由于事实 [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步是根据引理 [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第四步是因为引理 [8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Additionally, we have
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们有
- en: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of definition of $G_{2,2}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the fourth step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5
    (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for
    Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是因为 $G_{2,2}$ 的定义，第二步是由于事实 [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步是根据引理 [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第四步是因为引理 [8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Additionally, we have
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们有
- en: '|  | $\displaystyle\&#124;G_{2,3}\&#124;=$ |  |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,3}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of definition of $G_{2,3}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步是由于 $G_{2,3}$ 的定义，第二步是由于事实 [4.3](#S4.Thmtheorem3 "事实 4.3\. ‣ 4.1 基本事实 ‣
    4 初步 ‣ 一种快速优化视角：基于张量和支持向量机技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决它")，第三步来自引理 [8.4](#S8.Thmtheorem4
    "引理 8.4（基本函数上界）。 ‣ 8.3 核心工具：几个基本函数的上界 ‣ 8 Lipschitz 属性 𝐻_{𝑥,𝑥} ‣ 一种快速优化视角：基于张量和支持向量机技巧重新表述
    LLM 中的单层注意力，并在矩阵乘法时间内解决它") 和引理 [8.5](#S8.Thmtheorem5 "引理 8.5（基本函数 Lipschitz 属性）。
    ‣ 8.4 核心工具：几个基本函数的 Lipschitz 属性 ‣ 8 Lipschitz 属性 𝐻_{𝑥,𝑥} ‣ 一种快速优化视角：基于张量和支持向量机技巧重新表述
    LLM 中的单层注意力，并在矩阵乘法时间内解决它")。
- en: Combining all the above equations finish the proof. ∎
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述所有方程结合起来完成证明。∎
- en: '8.7 Calculation: Step 3 Lipschitz for Matrix Function $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$'
  id: totrans-1125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 计算：步骤 3 Lipschitz 对矩阵函数 $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$
- en: In this section, we introduce our calculation of Lipschitz for $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$.
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了对 $-2\gamma_{j_{0}}(x)\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$
    的 Lipschitz 计算。
- en: Lemma 8.9.
  id: totrans-1127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.9。
- en: If the following conditions hold
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2.
  id: totrans-1130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $1$2。
- en: •
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R_{0}$ 根据定义[8.6](#S8.Thmtheorem6 "定义 8.6\. ‣ 8.4 核心工具：几个基本函数的 Lipschitz 属性
    ‣ 8 Lipschitz 属性 𝐻_{𝑥,𝑥} ‣ 一种快速优化视角：基于张量和支持向量机技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决它")
    定义。
- en: •
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 根据定义[4.9](#S4.Thmtheorem9 "定义 4.9\. ‣ 4.3
    关于 𝑋 的有用定义 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和支持向量机技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决它") 定义。
- en: •
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$f(x)_{j_{0}}\in\mathbb{R}^{n}$ 根据定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣ 4.3
    关于 𝑋 的有用定义 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和支持向量机技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决它")
- en: •
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 根据定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 关于 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和支持向量机技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内解决它")
- en: •
  id: totrans-1139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: Then, we have
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $\displaystyle\&#124;G_{3}(x)-G_{3}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{3}(x)-G_{3}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{3,1}=$ |  |'
  id: totrans-1149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,1}=$ |  |'
- en: '|  | $\displaystyle G_{3,2}=$ |  |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,2}=$ |  |'
- en: '|  | $\displaystyle G_{3,3}=$ |  |'
  id: totrans-1151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,3}=$ |  |'
- en: For $G_{3,1}$, we have
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $G_{3,1}$，我们有
- en: '|  | $\displaystyle\&#124;G_{3,1}\&#124;\leq$ |  |'
  id: totrans-1153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|G_{3,1}\|\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1
    Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and the second step is due to Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic
    Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") and Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步基于事实[4.3](#S4.Thmtheorem3 "事实 4.3. ‣ 4.1 基本事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内求解")，第二步由于引理[8.4](#S8.Thmtheorem4
    "引理 8.4（基本函数上界）。 ‣ 8.3 核心工具：若干基本函数的上界 ‣ 8 Lipschitz属性 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内求解")和引理[8.5](#S8.Thmtheorem5
    "引理 8.5（基本函数Lipschitz属性）。 ‣ 8.4 核心工具：若干基本函数的Lipschitz属性 ‣ 8 Lipschitz属性 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内求解")。
- en: Similarly, we have
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们有
- en: '|  | $\displaystyle\&#124;G_{3,2}\&#124;\leq 2R_{0}\cdot R^{4}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1157
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|G_{3,2}\|\leq 2R_{0}\cdot R^{4}\|x-\widetilde{x}\|_{2}$
    |  |'
- en: and
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\displaystyle\&#124;G_{3,3}\&#124;\leq 2R_{0}\cdot R^{4}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|G_{3,3}\|\leq 2R_{0}\cdot R^{4}\|x-\widetilde{x}\|_{2}$
    |  |'
- en: ∎
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '8.8 Calculation: Step 4 Lipschitz for Matrix Function $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$'
  id: totrans-1161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8 计算：步骤 4 对矩阵函数$-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$的Lipschitz计算
- en: In this section, we introduce our calculation of Lipschitz for $-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ
    v)f(x)_{j_{0}}^{\top}$.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了对$-c(x,:)_{j_{0},i_{0}}\cdot(f(x)_{j_{0}}\circ v)f(x)_{j_{0}}^{\top}$的Lipschitz计算。
- en: Lemma 8.10.
  id: totrans-1163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.10。
- en: If the following conditions hold
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 如定义[4.9](#S4.Thmtheorem9 "定义 4.9. ‣ 4.3 关于𝑋的有用定义
    ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 如定义[4.10](#S4.Thmtheorem10 "定义 4.10. ‣ 4.3
    关于𝑋的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义[4.12](#S4.Thmtheorem12 "定义 4.12. ‣
    4.5 关于𝑋和𝑌的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重新表述LLM中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $R\geq 4$
- en: •
  id: totrans-1177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-1178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $1$2
- en: Then, we have
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $\displaystyle\&#124;G_{4}(x)-G_{4}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\|G_{4}(x)-G_{4}(\widetilde{x})\|\leq 10R^{4}\cdot R_{0}\|x-\widetilde{x}\|_{2}$
    |  |'
- en: Proof.
  id: totrans-1181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{4,1}=$ |  |'
  id: totrans-1183
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,1}=$ |  |'
- en: '|  | $\displaystyle G_{4,2}=$ |  |'
  id: totrans-1184
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,2}=$ |  |'
- en: '|  | $\displaystyle G_{4,3}=$ |  |'
  id: totrans-1185
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,3}=$ |  |'
- en: For $G_{4,1}$, we have
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $G_{4,1}$，我们有
- en: '|  | $\displaystyle\&#124;G_{4,1}\&#124;\leq R^{2}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,1}\&#124;\leq R^{2}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: For $G_{4,2}$, we have
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $G_{4,2}$，我们有
- en: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: For $G_{4,3}$, we have
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $G_{4,3}$，我们有
- en: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq 2R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: ∎
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '8.9 Calculation: Step 5 Lipschitz for Matrix Function $-2\gamma_{j_{0}}(x)\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$'
  id: totrans-1193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.9 计算：步骤 5 对矩阵函数 $-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$
    的 Lipschitz 计算
- en: In this section, we introduce our calculation of Lipschitz for $-2\gamma_{j_{0}}(x)\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$.
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了对 $-2\gamma_{j_{0}}(x)\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$
    的 Lipschitz 计算。
- en: Lemma 8.11.
  id: totrans-1195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.11。
- en: If the following conditions hold
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")'
  id: totrans-1198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R_{0}$ 按照定义[8.6](#S8.Thmtheorem6 "定义 8.6\. ‣ 8.4 一个核心工具：多个基本函数的 Lipschitz
    属性 ‣ 8 Lipschitz 属性 ‣ 快速优化视角：基于张量和 SVM 技巧对 LLM 中单层注意力进行重新表述，并在矩阵乘法时间内解决") 定义。
- en: •
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 按照定义[4.9](#S4.Thmtheorem9 "定义 4.9\. ‣ 4.3
    与 𝑋 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧对 LLM 中单层注意力进行重新表述，并在矩阵乘法时间内解决") 定义。
- en: •
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 按照定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣ 4.3
    与 𝑋 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧对 LLM 中单层注意力进行重新表述，并在矩阵乘法时间内解决") 定义。
- en: •
  id: totrans-1203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 按照定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 与 𝑋 和 𝑌 相关的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧对 LLM 中单层注意力进行重新表述，并在矩阵乘法时间内解决")
    定义。
- en: •
  id: totrans-1205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: •
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-1212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $1$2
- en: Then, we have
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'This proof is similar to the proof of Lemma [8.9](#S8.Thmtheorem9 "Lemma 8.9\.
    ‣ 8.7 Calculation: Step 3 Lipschitz for Matrix Function -2⁢𝛾_𝑗₀⁢(𝑥)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), so we omit it here. ∎'
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个证明类似于引理 [8.9](#S8.Thmtheorem9 "引理 8.9\. ‣ 8.7 计算：步骤 3 对矩阵函数 -2⁢𝛾_𝑗₀⁢(𝑥)⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz 属性 ‣ 快速优化视角：基于张量和 SVM 技巧对 LLM 中单层注意力进行重新表述，并在矩阵乘法时间内解决") 的证明，因此在这里省略。∎
- en: '8.10 Calculation: Step 6 Lipschitz for Matrix Function $-c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$'
  id: totrans-1217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.10 计算：步骤 6 对矩阵函数 $-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ
    v)^{\top}$ 的 Lipschitz 计算
- en: In this section, we introduce our calculation of Lipschitz for $-c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$.
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了对 $-c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}(f(x)_{j_{0}}\circ v)^{\top}$
    的 Lipschitz 计算。
- en: Lemma 8.12.
  id: totrans-1219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.12。
- en: If the following conditions hold
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 定义为定义[4.9](#S4.Thmtheorem9 "定义 4.9\. ‣ 4.3
    有关 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并以矩阵乘法时间求解")
- en: •
  id: totrans-1223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 定义为定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣
    4.3 有关 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并以矩阵乘法时间求解")
- en: •
  id: totrans-1225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 定义为定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 有关 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并以矩阵乘法时间求解")
- en: •
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $R\geq 4$
- en: •
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-1234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $1$2
- en: Then, we have
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有
- en: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
  id: totrans-1236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{5}(x)-G_{5}(\widetilde{x})\&#124;\leq 10R^{4}\cdot
    R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$ |  |'
- en: Proof.
  id: totrans-1237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'This proof is similar to the proof of Lemma [8.10](#S8.Thmtheorem10 "Lemma
    8.10\. ‣ 8.8 Calculation: Step 4 Lipschitz for Matrix Function -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), so we omit it here. ∎'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个证明类似于引理 [8.10](#S8.Thmtheorem10 "引理 8.10\. ‣ 8.8 计算：步骤 4 Lipschitz 对矩阵函数 -𝑐⁢(𝑥,:)_{𝑗₀,𝑖₀}⋅(𝑓⁢(𝑥)_𝑗₀∘𝑣)⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 8 Lipschitz 性质 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并以矩阵乘法时间求解"), 因此我们在此省略。
    ∎
- en: '8.11 Calculation: Step 7 Lipschitz for Matrix Function $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  id: totrans-1239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.11 计算：步骤 7 Lipschitz 对矩阵函数 $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: In this section, we introduce our calculation of Lipschitz for $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍对 $2\gamma_{j_{0}}(x)c(x,:)_{j_{0},i_{0}}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
    的 Lipschitz 计算。
- en: Lemma 8.13.
  id: totrans-1241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.13。
- en: If the following conditions hold
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 定义为定义[4.9](#S4.Thmtheorem9 "定义 4.9\. ‣ 4.3
    有关 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并以矩阵乘法时间求解")
- en: •
  id: totrans-1245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 定义为定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣
    4.3 有关 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并以矩阵乘法时间求解")
- en: •
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 定义为定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 有关 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并以矩阵乘法时间求解")
- en: •
  id: totrans-1249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: •
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-1256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $1$2
- en: Then, we have
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $\displaystyle\&#124;G_{7}(x)-G_{7}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7}(x)-G_{7}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-1259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{7,1}=$ |  |'
  id: totrans-1261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{7,1}=$ |  |'
- en: '|  | $\displaystyle G_{7,2}=$ |  |'
  id: totrans-1262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{7,2}=$ |  |'
- en: '|  | $\displaystyle G_{7,3}=$ |  |'
  id: totrans-1263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{7,3}=$ |  |'
- en: '|  | $\displaystyle G_{7,4}=$ |  |'
  id: totrans-1264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{7,4}=$ |  |'
- en: For $G_{7,1}$, we have
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $G_{7,1}$，我们有
- en: '|  | $\displaystyle\&#124;G_{7,1}\&#124;=$ |  |'
  id: totrans-1266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to the definition of $G_{7,1}$, the second step
    is because of Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the third
    step is based on Part 4 of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") and Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and the last step comes from Part 4 and Part 6 of Lemma [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步源于 $G_{7,1}$ 的定义，第二步是由于事实 [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步基于引理 [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") 第 4 部分和事实 [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，最后一步来自引理
    [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool:
    Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") 第 4 部分和第 6 部分。'
- en: Similarly, for $G_{7,2}$, we have
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于 $G_{7,2}$，我们有
- en: '|  | $\displaystyle\&#124;G_{7,2}\&#124;\leq 2R_{0}\cdot R^{2}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7,2}\&#124;\leq 2R_{0}\cdot R^{2}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: For $G_{7,3}$, we have
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $G_{7,3}$，我们有
- en: '|  | $\displaystyle\&#124;G_{7,3}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1274
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7,3}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: For $G_{7,4}$, we have
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $G_{7,4}$，我们有
- en: '|  | $\displaystyle\&#124;G_{7,4}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{7,4}\&#124;\leq 2R_{0}\cdot 2R^{4}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: ∎
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '8.12 Calculation: Step 8 Lipschitz for Matrix Function $\gamma_{j_{0}}(x)^{2}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  id: totrans-1278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.12 计算：步骤 8 矩阵函数 $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
    的 Lipschitz
- en: In this section, we introduce our calculation of Lipschitz for $\gamma_{j_{0}}(x)^{2}\cdot
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了对 $\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$ 的
    Lipschitz 计算。
- en: Lemma 8.14.
  id: totrans-1280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.14。
- en: If the following conditions hold
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-1282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 定义如定义[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: •
  id: totrans-1284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 如定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣ 4.3
    关于 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 关于 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$，$\|x\|_{2}\leq R$，$\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: •
  id: totrans-1294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $G_{8,1}=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $G_{8,1}=\gamma_{j_{0}}(x)^{2}\cdot f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Then, we have
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到
- en: '|  | $\displaystyle\&#124;G_{8}(x)-G_{8}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1297
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{8}(x)-G_{8}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-1298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{8,1}=$ |  |'
  id: totrans-1300
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{8,1}=$ |  |'
- en: '|  | $\displaystyle G_{8,2}=$ |  |'
  id: totrans-1301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{8,2}=$ |  |'
- en: '|  | $\displaystyle G_{8,3}=$ |  |'
  id: totrans-1302
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{8,3}=$ |  |'
- en: '|  | $\displaystyle G_{8,4}=$ |  |'
  id: totrans-1303
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{8,4}=$ |  |'
- en: We can show that
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以证明
- en: '|  | $\displaystyle\max_{i\in[4]}\&#124;G_{8,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1305
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{i\in[4]}\&#124;G_{8,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: ∎
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '8.13 Calculation: Step 9 Lipschitz for Matrix Function $(f(x)_{j_{0}}\circ
    v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$'
  id: totrans-1307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.13 计算：第 9 步 Lipschitz 对矩阵函数 $(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ
    v)^{\top}$
- en: In this section, we introduce our calculation of Lipschitz for $(f(x)_{j_{0}}\circ
    v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$.
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍对 $(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$ 的 Lipschitz
    计算。
- en: Lemma 8.15.
  id: totrans-1309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 8.15。
- en: If the following conditions hold
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 如定义[4.9](#S4.Thmtheorem9 "定义 4.9\. ‣ 4.3
    关于 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 如定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣ 4.3
    关于 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(x,:)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 关于 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$，$\|x\|_{2}\leq R$，$\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: •
  id: totrans-1323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  id: totrans-1324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $G_{9}(x)=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
- en: Then, we have
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到
- en: '|  | $\displaystyle\&#124;G_{9}(x)-G_{9}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{9}(x)-G_{9}(\widetilde{x})\&#124;\leq 10R^{4}R_{0}\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-1327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{9,1}=$ |  |'
  id: totrans-1329
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{9,1}=$ |  |'
- en: '|  | $\displaystyle G_{9,2}=$ |  |'
  id: totrans-1330
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{9,2}=$ |  |'
- en: We can show that
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以证明
- en: '|  | $\displaystyle\max_{i\in[2]}\&#124;G_{9,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
  id: totrans-1332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{i\in[2]}\&#124;G_{9,i}\&#124;\leq R^{4}\cdot R_{0}\cdot\&#124;x-\widetilde{x}\&#124;_{2}$
    |  |'
- en: ∎
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: 9 Hessian for $X$ Is PSD
  id: totrans-1334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 Hessian 对 $X$ 是 PSD
- en: 'In Section [9.1](#S9.SS1 "9.1 Main Result ‣ 9 Hessian for 𝑋 Is PSD ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we present the
    main result of PSD bound for Hessian. In Section [9.2](#S9.SS2 "9.2 PSD Bound
    ‣ 9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), we show the PSD bound for $B(x)$. Throughout this section, we will use
    the symbol $H$ for the sake of simplicity.'
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[9.1](#S9.SS1 "9.1 主要结果 ‣ 9 Hessian 对 𝑋 是 PSD ‣ 快速优化视角：基于张量和支持向量机技巧重新构建单层注意力，并在矩阵乘法时间中解决它")节中，我们展示了Hessian的PSD界限。在第[9.2](#S9.SS2
    "9.2 PSD 界限 ‣ 9 Hessian 对 𝑋 是 PSD ‣ 快速优化视角：基于张量和支持向量机技巧重新构建单层注意力，并在矩阵乘法时间中解决它")节中，我们展示了$B(x)$的PSD界限。在本节中，为了简便起见，我们将使用符号$H$。
- en: 9.1 Main Result
  id: totrans-1336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 主要结果
- en: In this section, we introduce the main result of the PSD bound for Hessian.
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了Hessian的PSD界限的主要结果。
- en: Lemma 9.1.
  id: totrans-1338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 9.1。
- en: If the following conditions hold
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $j_{0}\in[n]$
  id: totrans-1341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$j_{0}\in[n]$
- en: •
  id: totrans-1342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $i_{0}\in[d]$
  id: totrans-1343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$i_{0}\in[d]$
- en: •
  id: totrans-1344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H_{j_{0},i_{0}}=\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}x}\in\mathbb{R}^{d^{2}\times
    d^{2}}$
  id: totrans-1345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$H_{j_{0},i_{0}}=\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}x}\in\mathbb{R}^{d^{2}\times
    d^{2}}$
- en: •
  id: totrans-1346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $B_{j_{0},i_{0}}(x)\in\mathbb{R}^{n\times n}$ be defined as Definition[7.3](#S7.Thmtheorem3
    "Definition 7.3\. ‣ 7.3 Defining 𝐵⁢(𝑥) ‣ 7 Hessian for 𝑋 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$B_{j_{0},i_{0}}(x)\in\mathbb{R}^{n\times n}$的定义见定义[7.3](#S7.Thmtheorem3 "定义
    7.3\. ‣ 7.3 定义 𝐵⁢(𝑥) ‣ 7 Hessian 对 𝑋 ‣ 快速优化视角：基于张量和支持向量机技巧重新构建单层注意力，并在矩阵乘法时间中解决它")。
- en: –
  id: totrans-1348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Therefore, $1$2
  id: totrans-1349
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，$1$2
- en: •
  id: totrans-1350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-1351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: •
  id: totrans-1352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\sigma_{\min}$.
  id: totrans-1353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$\sigma_{\min}$。
- en: •
  id: totrans-1354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$
  id: totrans-1355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$H=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{j_{0},i_{0}}$
- en: •
  id: totrans-1356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H_{\operatorname{reg},j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}(B_{j_{0},i_{0}}(x)+W^{2})\operatorname{\mathsf{A}}_{j_{0}}$
    is a positive diagonal matrix.
  id: totrans-1357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$H_{\operatorname{reg},j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}(B_{j_{0},i_{0}}(x)+W^{2})\operatorname{\mathsf{A}}_{j_{0}}$是一个正对角矩阵。
- en: •
  id: totrans-1358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H_{\operatorname{reg}}=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{\operatorname{reg},j_{0},i_{0}}$
  id: totrans-1359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$H_{\operatorname{reg}}=\sum_{j_{0}=1}^{n}\sum_{i_{0}=1}^{d}H_{\operatorname{reg},j_{0},i_{0}}$
- en: •
  id: totrans-1360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $C_{0}:=30R^{8}$ (be a local parameter in this lemma)
  id: totrans-1361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设$C_{0}:=30R^{8}$（在本引理中的局部参数）
- en: •
  id: totrans-1362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let  (denote
    the strongly convex parameter for hessian)
  id: totrans-1363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设（表示Hessian的强凸参数）
- en: Then, we have
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: •
  id: totrans-1365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1. For each $j_{0}\in[n]$
  id: totrans-1366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分。对于每个$j_{0}\in[n]$
- en: '|  | $\displaystyle-C_{0}I_{n}\preceq B_{j_{0},i_{0}}(x)\preceq C_{0}I_{n}$
    |  |'
  id: totrans-1367
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle-C_{0}I_{n}\preceq B_{j_{0},i_{0}}(x)\preceq C_{0}I_{n}$
    |  |'
- en: •
  id: totrans-1368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2. For each $j_{0}\in[n]$
  id: totrans-1369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。对于每个$j_{0}\in[n]$
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)\&#124;\leq C_{0}R^{2}.$ |  |'
  id: totrans-1370
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}(x)\&#124;\leq C_{0}R^{2}.$ |  |'
- en: •
  id: totrans-1371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+C_{0}$,
    then we have
  id: totrans-1372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3部分。对于每个$j_{0}\in[n]$，如果$\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+C_{0}$，则我们有
- en: '|  | $\displaystyle H_{\operatorname{reg},j_{0},i_{0}}(x)\succeq l\cdot I_{d^{2}}$
    |  |'
  id: totrans-1373
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{\operatorname{reg},j_{0},i_{0}}(x)\succeq l\cdot I_{d^{2}}$
    |  |'
- en: •
  id: totrans-1374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+100\cdot
    C_{0}$, then we have
  id: totrans-1375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4部分。对于每个$j_{0}\in[n]$，如果$\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{\sigma_{\min}(\operatorname{\mathsf{A}}_{j_{0}})^{2}}+100\cdot
    C_{0}$，则我们有
- en: '|  | $\displaystyle 1.1\cdot(B(x)_{j_{0},i_{0}}+W^{2})\succeq W^{2}\succeq
    0.9\cdot(B(x)_{j_{0},i_{0}}+W^{2})$ |  |'
  id: totrans-1376
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 1.1\cdot(B(x)_{j_{0},i_{0}}+W^{2})\succeq W^{2}\succeq
    0.9\cdot(B(x)_{j_{0},i_{0}}+W^{2})$ |  |'
- en: and
  id: totrans-1377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\displaystyle 1.1H_{j_{0},i_{0}}\succeq H_{\operatorname{reg},j_{0},i_{0}}\succeq
    0.9H_{j_{0},i_{0}}$ |  |'
  id: totrans-1378
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 1.1H_{j_{0},i_{0}}\succeq H_{\operatorname{reg},j_{0},i_{0}}\succeq
    0.9H_{j_{0},i_{0}}$ |  |'
- en: •
  id: totrans-1379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 5. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+C_{0}$,
    then we have
  id: totrans-1380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 5 部分。对于每个 $j_{0}\in[n]$，如果 $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+C_{0}$，那么我们有
- en: '|  | $\displaystyle H_{\operatorname{reg}}(x)\succeq l\cdot I_{d^{2}}$ |  |'
  id: totrans-1381
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{\operatorname{reg}}(x)\succeq l\cdot I_{d^{2}}$ |  |'
- en: •
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 6. For each $j_{0}\in[n]$, if $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+100\cdot
    C_{0}$, then we have
  id: totrans-1383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 6 部分。对于每个 $j_{0}\in[n]$，如果 $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}\geq\frac{l}{nd\sigma_{\min}(\operatorname{\mathsf{A}}_{\min})^{2}}+100\cdot
    C_{0}$，那么我们有
- en: '|  | $\displaystyle 1.1H\succeq H_{\operatorname{reg}}\succeq 0.9H$ |  |'
  id: totrans-1384
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 1.1H\succeq H_{\operatorname{reg}}\succeq 0.9H$ |  |'
- en: Proof.
  id: totrans-1385
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 部分的证明。
- en: 'It directly follows from Lemma [9.2](#S9.Thmtheorem2 "Lemma 9.2\. ‣ 9.2 PSD
    Bound ‣ 9 Hessian for 𝑋 Is PSD ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: 直接来自于引理 [9.2](#S9.Thmtheorem2 "引理 9.2. ‣ 9.2 PSD 界限 ‣ 9 Hessian 对于 𝑋 是 PSD ‣
    基于张量和 SVM 技巧的 LLM 单层注意力的快速优化视角，并在矩阵乘法时间内求解")。
- en: Proof of Part 2. We have
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 部分的证明。我们有
- en: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}\&#124;=$ |  |'
  id: totrans-1389
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H_{j_{0},i_{0}}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1390
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1391
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1392
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: where the first step follows from the $H_{j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B_{j_{0},i_{0}}(x)\operatorname{\mathsf{A}}_{j_{0}}$,
    and the last step follow from Part 1.
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步来自 $H_{j_{0},i_{0}}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B_{j_{0},i_{0}}(x)\operatorname{\mathsf{A}}_{j_{0}}$，最后一步来自第
    1 部分。
- en: Proof of Part 3.
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 部分的证明。
- en: The proof is similar to [[55](#bib.bib55)].
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 证明与[[55](#bib.bib55)]类似。
- en: Proof of Part 4.
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 部分的证明。
- en: The proof is similar to [[55](#bib.bib55)].
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: 证明与[[55](#bib.bib55)]类似。
- en: Proof of Part 5 and Part 6. It is because we can write $H$ terms $H_{j_{0},i_{0}}$,
    $i_{0}\in[d]$. ∎
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 部分和第 6 部分的证明。因为我们可以写 $H$ 项 $H_{j_{0},i_{0}}$，$i_{0}\in[d]$。∎
- en: 9.2 PSD Bound
  id: totrans-1399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 PSD 界限
- en: In this section, we analyze the PSD bound for each of the $B_{\operatorname{rank}}$.
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了每个 $B_{\operatorname{rank}}$ 的 PSD 界限。
- en: Lemma 9.2.
  id: totrans-1401
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 9.2。
- en: If the following condition holds
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-1403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-1404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-1405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-1406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-1407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-1408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-1409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
  id: totrans-1410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{rank}}^{3}:=(f(x)_{j_{0}}\circ v)\cdot(f(x)_{j_{0}}\circ v)^{\top}$
- en: •
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $|\gamma(x)_{j_{0}}|\leq R^{2}$
  id: totrans-1412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|\gamma(x)_{j_{0}}|\leq R^{2}$
- en: •
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
  id: totrans-1414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|c(x,:)_{j_{0},i_{0}}|\leq 2R^{2}$
- en: •
  id: totrans-1415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|v\|_{2}\leq R^{2}$
  id: totrans-1416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|v\|_{2}\leq R^{2}$
- en: Then, we have
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: •
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-1419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 1 部分。
- en: '|  | $\displaystyle-8R^{6}\cdot I_{n}\preceq B_{\operatorname{diag}}^{1}\preceq
    8R^{6}\cdot I_{n}$ |  |'
  id: totrans-1420
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle-8R^{6}\cdot I_{n}\preceq B_{\operatorname{diag}}^{1}\preceq
    8R^{6}\cdot I_{n}$ |  |'
- en: •
  id: totrans-1421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-1422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 2 部分。
- en: '|  | $\displaystyle-16R^{8}\cdot I_{n}\preceq B_{\operatorname{rank}}^{1}\preceq
    16R^{8}\cdot I_{n}$ |  |'
  id: totrans-1423
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle-16R^{8}\cdot I_{n}\preceq B_{\operatorname{rank}}^{1}\preceq
    16R^{8}\cdot I_{n}$ |  |'
- en: •
  id: totrans-1424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3.
  id: totrans-1425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 3 部分。
- en: '|  | $\displaystyle-8R^{4}\cdot I_{n}\preceq B_{\operatorname{rank}}^{2}\preceq
    8R^{4}\cdot I_{n}$ |  |'
  id: totrans-1426
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle-8R^{4}\cdot I_{n}\preceq B_{\operatorname{rank}}^{2}\preceq
    8R^{4}\cdot I_{n}$ |  |'
- en: •
  id: totrans-1427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4.
  id: totrans-1428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 4 部分。
- en: '|  | $\displaystyle 0\cdot I_{n}\preceq B_{\operatorname{rank}}^{3}\preceq
    8R^{4}\cdot I_{n}$ |  |'
  id: totrans-1429
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0\cdot I_{n}\preceq B_{\operatorname{rank}}^{3}\preceq
    8R^{4}\cdot I_{n}$ |  |'
- en: Proof.
  id: totrans-1430
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 部分的证明。
- en: '|  | $\displaystyle B_{\operatorname{diag}}^{1}=$ |  |'
  id: totrans-1432
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{\operatorname{diag}}^{1}=$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1433
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1434
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: where the first step follows from the definition of $B_{\operatorname{diag}}^{1}$,
    and $\|v\|_{2}\leq R^{2}$.
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步源自 $B_{\operatorname{diag}}^{1}$ 的定义，并且 $\|v\|_{2}\leq R^{2}$。
- en: Proof of Part 2.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 部分的证明。
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{1}=$ |  |'
  id: totrans-1437
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{\operatorname{rank}}^{1}=$ |  |'
- en: '|  | $\displaystyle\succeq$ |  |'
  id: totrans-1438
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\succeq$ |  |'
- en: '|  | $\displaystyle\succeq$ |  |'
  id: totrans-1439
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\succeq$ |  |'
- en: '|  | $\displaystyle\succeq$ |  |'
  id: totrans-1440
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\succeq$ |  |'
- en: '|  | $\displaystyle\succeq$ |  |'
  id: totrans-1441
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\succeq$ |  |'
- en: 'where the first step follows from the definition of $B_{\operatorname{rank}}^{1}$
    and Fact [4.4](#S4.Thmtheorem4 "Fact 4.4\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the fourth
    step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and last step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and $\|v\|_{2}\leq R^{2}$.'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步由 $B_{\operatorname{rank}}^{1}$ 的定义和事实 [4.4](#S4.Thmtheorem4 "Fact 4.4\.
    ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") 得出，第四步由事实 [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") 得出，最后一步由引理 [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    和 $\|v\|_{2}\leq R^{2}$ 得出。'
- en: Proof of Part 3.
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分的证明。
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{2}=$ |  |'
  id: totrans-1444
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{\operatorname{rank}}^{2}=$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1445
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1446
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: 'where the first step follows from definition of $B_{\operatorname{rank}}^{2}$
    and Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3
    A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of
    𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: '第一部分的证明由 $B_{\operatorname{rank}}^{2}$ 的定义和引理 [8.4](#S8.Thmtheorem4 "Lemma
    8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") 得出。'
- en: Proof of Part 4.
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: 第四部分的证明。
- en: '|  | $\displaystyle B_{\operatorname{rank}}^{3}=$ |  |'
  id: totrans-1449
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B_{\operatorname{rank}}^{3}=$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1450
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1451
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: '|  | $\displaystyle\preceq$ |  |'
  id: totrans-1452
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\preceq$ |  |'
- en: 'where the first step follows from definition of $B_{\operatorname{rank}}^{3}$
    and Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3
    A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of
    𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: '第一部分的证明由 $B_{\operatorname{rank}}^{3}$ 的定义和引理 [8.4](#S8.Thmtheorem4 "Lemma
    8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time") 得出。'
- en: ∎
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: 10 Hessian for $Y$
  id: totrans-1455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 Hessian矩阵对于 $Y$
- en: 'In Section [10.1](#S10.SS1 "10.1 Hessian Property ‣ 10 Hessian for 𝑌 ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we present the
    hessian property with respect to $Y$ for one $j_{0},i_{0}$.'
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 [10.1](#S10.SS1 "10.1 Hessian Property ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 节中，我们展示了相对于 $Y$ 的Hessian矩阵性质，对于一个
    $j_{0},i_{0}$。'
- en: 10.1 Hessian Property
  id: totrans-1457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 Hessian矩阵性质
- en: In this section, we analyze the Hessian properties.
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析Hessian矩阵的性质。
- en: Lemma 10.1.
  id: totrans-1459
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 10.1。
- en: If the following conditions hold
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-1461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $B_{j_{0}}(x)=f(x)_{j_{0}}f(x)_{j_{0}}^{\top}\in\mathbb{R}^{n\times n}$
    (because of Lemma[10.2](#S10.Thmtheorem2 "Lemma 10.2\. ‣ 10.2 Hessian for One
    𝑗₀,𝑖₀ ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  id: totrans-1462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $B_{j_{0}}(x)=f(x)_{j_{0}}f(x)_{j_{0}}^{\top}\in\mathbb{R}^{n\times n}$（因为引理
    [10.2](#S10.Thmtheorem2 "Lemma 10.2\. ‣ 10.2 Hessian for One 𝑗₀,𝑖₀ ‣ 10 Hessian
    for 𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")）'
- en: •
  id: totrans-1463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $B(x)=\sum_{j_{0}=1}^{n}B_{j_{0}}(x)$
  id: totrans-1464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $B(x)=\sum_{j_{0}=1}^{n}B_{j_{0}}(x)$
- en: •
  id: totrans-1465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-1466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $1$2
- en: •
  id: totrans-1467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H_{i_{0}}\in\mathbb{R}^{d\times d}$
  id: totrans-1468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $H_{i_{0}}\in\mathbb{R}^{d\times d}$
- en: •
  id: totrans-1469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H_{\operatorname{reg},i_{0}}=A_{3}^{\top}(B(x)+W^{2})A_{3}$ is a positive
    diagonal matrix
  id: totrans-1470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $H_{\operatorname{reg},i_{0}}=A_{3}^{\top}(B(x)+W^{2})A_{3}$ 是一个正对角矩阵
- en: •
  id: totrans-1471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H(y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be $$H(y)=\begin{bmatrix}H_{1}&amp;0&amp;\cdots&amp;0\\
  id: totrans-1472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $H(y)\in\mathbb{R}^{d^{2}\times d^{2}}$ 为 $$H(y)=\begin{bmatrix}H_{1}&amp;0&amp;\cdots&amp;0\\
- en: 0&amp;H_{2}&amp;\cdots&amp;0\\
  id: totrans-1473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;H_{2}&amp;\cdots&amp;0\\
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-1474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
- en: 0&amp;0&amp;\cdots&amp;H_{d}\end{bmatrix}$$
  id: totrans-1475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;\cdots&amp;H_{d}\end{bmatrix}$$
- en: Then, we have
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: •
  id: totrans-1477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-1478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分。
- en: '|  | $\displaystyle 0\preceq B_{j_{0}}(x)\preceq I_{n}$ |  |'
  id: totrans-1479
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0\preceq B_{j_{0}}(x)\preceq I_{n}$ |  |'
- en: •
  id: totrans-1480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-1481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。
- en: '|  | $\displaystyle 0\preceq B(x)\preceq n\cdot I_{n}$ |  |'
  id: totrans-1482
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0\preceq B(x)\preceq n\cdot I_{n}$ |  |'
- en: •
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3. If $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}$
  id: totrans-1484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3部分。如果 $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}$
- en: '|  | $\displaystyle H_{\operatorname{reg},i_{0}}\succeq l\cdot I_{d},~{}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
  id: totrans-1485
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{\operatorname{reg},i_{0}}\succeq l\cdot I_{d},~{}~{}~{}H(y)\succeq
    l\cdot I_{d^{2}}$ |  |'
- en: •
  id: totrans-1486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4. If $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}+100n$
  id: totrans-1487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4部分。如果 $\min_{j_{1}\in[n]}w_{j_{1},j_{1}}^{2}\geq\frac{l}{\sigma_{\min}(A_{3})^{2}}+100n$
- en: '|  | $\displaystyle 0.9(W^{2}+B(x))\preceq W^{2}\preceq 1.1(W^{2}+B(x))$ |  |'
  id: totrans-1488
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0.9(W^{2}+B(x))\preceq W^{2}\preceq 1.1(W^{2}+B(x))$ |  |'
- en: •
  id: totrans-1489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 5. Lipschitz, Due to $H(y)$, then
  id: totrans-1490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5部分。Lipschitz，由于 $H(y)$，则
- en: '|  | $\displaystyle\&#124;H(y)-H(\widetilde{y})\&#124;\leq\&#124;y-\widetilde{y}\&#124;_{2}$
    |  |'
  id: totrans-1491
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(y)-H(\widetilde{y})\&#124;\leq\&#124;y-\widetilde{y}\&#124;_{2}$
    |  |'
- en: Proof.
  id: totrans-1492
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'For hessian closed-form, we can obtain them from Lemma [10.2](#S10.Thmtheorem2
    "Lemma 10.2\. ‣ 10.2 Hessian for One 𝑗₀,𝑖₀ ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 Hessian 的闭式形式，我们可以从引理 [10.2](#S10.Thmtheorem2 "引理 10.2\. ‣ 10.2 Hessian
    for One 𝑗₀,𝑖₀ ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time") 中获得。'
- en: The proofs are straightforward, so we omit the details here. ∎
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: 证明过程很简单，因此在此省略详细步骤。∎
- en: 10.2 Hessian for One $j_{0},i_{0}$
  id: totrans-1495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2 一个 $j_{0},i_{0}$ 的 Hessian
- en: In this section, we analyze the Hessian for the matrix $Y$.
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们分析矩阵 $Y$ 的 Hessian。
- en: Lemma 10.2.
  id: totrans-1497
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 10.2。
- en: If the following conditions hold
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-1499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We define a temporary notation here $v:=f(x)_{j_{0}}$ in the statement. Note
    that $v$ could have different meaning in other sections.)
  id: totrans-1500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在此定义一个临时符号 $v:=f(x)_{j_{0}}$。注意，$v$ 在其他部分可能有不同的含义。
- en: •
  id: totrans-1501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $f(x)_{j_{0}}$ 定义为定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")。'
- en: •
  id: totrans-1503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,:)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1504
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $c(x,:)_{j_{0},i_{0}}$ 定义为定义[4.12](#S4.Thmtheorem12 "定义 4.12\. ‣ 4.5 Helpful
    Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")。'
- en: •
  id: totrans-1505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $h(y)_{i_{0}}$ 定义为定义[4.11](#S4.Thmtheorem11 "定义 4.11\. ‣ 4.4 A Helpful Definition
    With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")。'
- en: •
  id: totrans-1507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $L_{j_{0},i_{0}}$ 定义为定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")。'
- en: Then, we have
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: •
  id: totrans-1510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1. For $i_{1}=i_{2}$, the diagonal case
  id: totrans-1511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分。对于 $i_{1}=i_{2}$，对角线情况
- en: '|  | $1$2 |  |'
  id: totrans-1512
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-1513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2. For $i_{1}\neq i_{2}$, the off-diagonal case
  id: totrans-1514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。对于 $i_{1}\neq i_{2}$，非对角线情况
- en: '|  | $1$2 |  |'
  id: totrans-1515
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-1516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3. The $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}\in\mathbb{R}^{d\times
    d}$
  id: totrans-1517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三部分. $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}\in\mathbb{R}^{d\times
    d}$
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}=A_{3}^{\top}vv^{\top}A_{3}$
    |  |'
  id: totrans-1518
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}y_{i_{0}}}=A_{3}^{\top}vv^{\top}A_{3}$
    |  |'
- en: Proof.
  id: totrans-1519
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{1}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
  id: totrans-1521
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{1}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1522
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1523
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1524
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from simple algebra, the second step follows from
    Lemma [5.2](#S5.Thmtheorem2 "Lemma 5.2\. ‣ 5.2 Gradient With Respect to 𝑦 ‣ 5
    Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    the third step follows from Lemma [5.2](#S5.Thmtheorem2 "Lemma 5.2\. ‣ 5.2 Gradient
    With Respect to 𝑦 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步由简单代数得出，第二步由引理 [5.2](#S5.Thmtheorem2 "引理 5.2\. ‣ 5.2 关于 𝑦 的梯度 ‣ 5 梯度 ‣
    快速优化视角：基于张量和 SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内解决它") 得出，第三步由引理 [5.2](#S5.Thmtheorem2
    "引理 5.2\. ‣ 5.2 关于 𝑦 的梯度 ‣ 5 梯度 ‣ 快速优化视角：基于张量和 SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内解决它")
    得出，最后一步由事实 [4.1](#S4.Thmtheorem1 "事实 4.1\. ‣ 4.1 基本事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM
    技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内解决它") 得出。
- en: Proof of Part 2.
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分的证明。
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
  id: totrans-1527
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}y_{i_{0},i_{1}}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1528
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1529
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1530
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from simple algebra, the second step follows from
    Lemma [5.2](#S5.Thmtheorem2 "Lemma 5.2\. ‣ 5.2 Gradient With Respect to 𝑦 ‣ 5
    Gradient ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    the third step follows from Lemma [5.2](#S5.Thmtheorem2 "Lemma 5.2\. ‣ 5.2 Gradient
    With Respect to 𝑦 ‣ 5 Gradient ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the last step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步由简单代数得出，第二步由引理 [5.2](#S5.Thmtheorem2 "引理 5.2\. ‣ 5.2 关于 𝑦 的梯度 ‣ 5 梯度 ‣
    快速优化视角：基于张量和 SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内解决它") 得出，第三步由引理 [5.2](#S5.Thmtheorem2
    "引理 5.2\. ‣ 5.2 关于 𝑦 的梯度 ‣ 5 梯度 ‣ 快速优化视角：基于张量和 SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内解决它")
    得出，最后一步由事实 [4.1](#S4.Thmtheorem1 "事实 4.1\. ‣ 4.1 基本事实 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM
    技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内解决它") 得出。
- en: Proof of Part 3.
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分的证明。
- en: It follows by combining above two parts directly. ∎
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
  zh: 通过直接结合以上两部分得出。 ∎
- en: 11 Hessian for $X$
  id: totrans-1534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 对 $X$ 的 Hessian
- en: 'In Section [11.1](#S11.SS1 "11.1 Computing Hessian ‣ 11 Hessian for 𝑋 and 𝑌
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we compute
    the Hessian matrix with respect to both $X$. In Section [11.2](#S11.SS2 "11.2
    A Helpful Lemma ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we present several helpful lemmas for the following
    proof. In Section [11.2](#S11.SS2 "11.2 A Helpful Lemma ‣ 11 Hessian for 𝑋 and
    𝑌 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we create
    $B(x)$ for the further analysis.'
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
  zh: 在章节 [11.1](#S11.SS1 "11.1 计算 Hessian ‣ 11 𝑋 和 𝑌 的 Hessian ‣ 快速优化视角：基于张量和 SVM
    技巧重构单层注意力，并在矩阵乘法时间内解决") 中，我们计算了相对于 $X$ 的 Hessian 矩阵。在章节 [11.2](#S11.SS2 "11.2
    一个有用的引理 ‣ 11 𝑋 和 𝑌 的 Hessian ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内解决") 中，我们展示了几个对后续证明有帮助的引理。在章节
    [11.2](#S11.SS2 "11.2 一个有用的引理 ‣ 11 𝑋 和 𝑌 的 Hessian ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内解决")
    中，我们创建了用于进一步分析的 $B(x)$。
- en: 11.1 Computing Hessian
  id: totrans-1536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1 计算 Hessian
- en: In this section, we compute the Hessian matrix for $X$.
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们计算了 $X$ 的 Hessian 矩阵。
- en: Lemma 11.1.
  id: totrans-1538
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 11.1。
- en: If the following conditions hold
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}$ be defined as Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $f(x)_{j_{0}}$ 的定义参见定义 [4.10](#S4.Thmtheorem10 "定义 4.10 ‣ 4.3 关于 𝑋 的有用定义 ‣
    4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内解决")。
- en: •
  id: totrans-1542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,y)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(x,y)_{j_{0},i_{0}}$ 的定义参见定义 [4.12](#S4.Thmtheorem12 "定义 4.12 ‣ 4.5 关于 𝑋
    和 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内解决")。
- en: •
  id: totrans-1544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1545
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $h(y)_{i_{0}}$ 的定义参见定义 [4.11](#S4.Thmtheorem11 "定义 4.11 ‣ 4.4 关于 𝑌 的有用定义 ‣
    4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内解决")。
- en: •
  id: totrans-1546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.7](#S4.Thmtheorem7 "Definition
    4.7\. ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $L_{j_{0},i_{0}}$ 的定义参见定义 [4.7](#S4.Thmtheorem7 "定义 4.7 ‣ 4.2 一般定义 ‣ 4 初步
    ‣ 快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内解决")。
- en: Then, we have
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有
- en: •
  id: totrans-1549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-1550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一部分。
- en: '|  | $\displaystyle\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})=$
    |  |'
  id: totrans-1551
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})=$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1552
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1553
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Proof.
  id: totrans-1554
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We can show
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以证明
- en: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
  id: totrans-1556
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle~{}\frac{\mathrm{d}}{\mathrm{d}y_{i_{0},i_{1}}}(\frac{\mathrm{d}}{\mathrm{d}x_{i}}L_{j_{0},i_{0}})$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1557
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1558
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1559
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1560
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1561
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1562
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1563
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1564
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'where the first step is due to Part 6 of Lemma [5.1](#S5.Thmtheorem1 "Lemma
    5.1 (Gradient with respect to 𝑥). ‣ 5.1 Gradient for 𝑥 ‣ 5 Gradient ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step comes from the
    product rule of derivative, the third step is based on Lemma [10.2](#S10.Thmtheorem2
    "Lemma 10.2\. ‣ 10.2 Hessian for One 𝑗₀,𝑖₀ ‣ 10 Hessian for 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), and the last step follows from
    simple algebra.'
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步基于引理 [5.1](#S5.Thmtheorem1 "引理 5.1（关于 𝑥 的梯度）. ‣ 5.1 关于 𝑥 的梯度 ‣ 5 梯度 ‣ 快速优化视角：基于张量和
    SVM 技巧的单层注意力重构及其矩阵乘法时间求解") 第6部分，第二步来自导数的乘积法则，第三步基于引理 [10.2](#S10.Thmtheorem2 "引理
    10.2\. ‣ 10.2 关于一个 𝑗₀,𝑖₀ 的 Hessian ‣ 10 关于 𝑌 的 Hessian ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力重构及其矩阵乘法时间求解")，最后一步则是简单的代数运算。
- en: Thus, we complete the proof. ∎
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们完成了证明。 ∎
- en: 11.2 A Helpful Lemma
  id: totrans-1567
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2 一个有用的引理
- en: In this section, we provide a helpful Lemma.
  id: totrans-1568
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一个有用的引理。
- en: Lemma 11.2.
  id: totrans-1569
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 11.2。
- en: If the following conditions hold
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}$ be defined in Definition[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1572
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $f(x)_{j_{0}}$ 按照定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣ 4.3 关于 𝑋 的有用定义 ‣ 4
    初步 ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力重构及其矩阵乘法时间求解") 定义。
- en: •
  id: totrans-1573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ be defined
    in Definition[4.8](#S4.Thmtheorem8 "Definition 4.8\. ‣ 4.3 Helpful Definitions
    With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1574
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\operatorname{\mathsf{A}}\in\mathbb{R}^{n^{2}\times d^{2}}$ 按照定义[4.8](#S4.Thmtheorem8
    "定义 4.8\. ‣ 4.3 关于 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力重构及其矩阵乘法时间求解") 定义。
- en: •
  id: totrans-1575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,y)_{j_{0},i_{0}}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1576
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $c(x,y)_{j_{0},i_{0}}$ 按照定义[4.12](#S4.Thmtheorem12 "定义 4.12\. ‣ 4.5 有关 𝑋 和
    𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力重构及其矩阵乘法时间求解") 定义。
- en: •
  id: totrans-1577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $h(y)_{i_{0}}$ be defined as Definition[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1578
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $h(y)_{i_{0}}$ 按照定义[4.11](#S4.Thmtheorem11 "定义 4.11\. ‣ 4.4 关于 𝑌 的有用定义 ‣ 4
    初步 ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力重构及其矩阵乘法时间求解") 定义。
- en: •
  id: totrans-1579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $L_{j_{0},i_{0}}$ be defined as Definition[4.7](#S4.Thmtheorem7 "Definition
    4.7\. ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1580
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $L_{j_{0},i_{0}}$ 按照定义[4.7](#S4.Thmtheorem7 "定义 4.7\. ‣ 4.2 一般定义 ‣ 4 初步 ‣
    快速优化视角：基于张量和 SVM 技巧的单层注意力重构及其矩阵乘法时间求解") 定义。
- en: Then, we have
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到
- en: •
  id: totrans-1582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-1583
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分。
- en: '|  | $1$2 |  |'
  id: totrans-1584
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-1585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-1586
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。
- en: '|  | $1$2 |  |'
  id: totrans-1587
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-1588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3.
  id: totrans-1589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3部分。
- en: '|  | $1$2 |  |'
  id: totrans-1590
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-1591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4.
  id: totrans-1592
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4部分。
- en: '|  | $1$2 |  |'
  id: totrans-1593
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1594
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的证明。
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},h(y)_{i_{0}}\rangle=$
    |  |'
  id: totrans-1596
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},h(y)_{i_{0}}\rangle=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1597
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and the second step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步来自于事实 [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，第二步来自于事实
    [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")。'
- en: Proof of Part 2.
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分的证明。
- en: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle=$ |  |'
  id: totrans-1600
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}},A_{3,*,i_{1}}\rangle\cdot\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle\cdot\langle
    f(x)_{j_{0}},\operatorname{\mathsf{A}}_{j_{0},i}\rangle=$ |  |'
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步来自于事实 [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Proof of Part 3.
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分的证明。
- en: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},A_{3,*,i_{1}}\rangle=$
    |  |'
  id: totrans-1603
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\langle f(x)_{j_{0}}\circ\operatorname{\mathsf{A}}_{j_{0},i},A_{3,*,i_{1}}\rangle=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1604
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-1605
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'where the first, second, and last step follows from Fact [4.1](#S4.Thmtheorem1
    "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步、第二步和最后一步来自于事实 [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Proof of Part 4.
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
  zh: 第四部分的证明。
- en: '|  | $1$2 |  |'
  id: totrans-1608
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'where the first step follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣
    4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步来自于事实 [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: ∎
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: 11.3 Creating $B(x,y)$
  id: totrans-1611
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3 创建 $B(x,y)$
- en: In this section, we give a formal definition of $B(x,y)$.
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们给出 $B(x,y)$ 的正式定义。
- en: Definition 11.3.
  id: totrans-1613
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 11.3。
- en: We define $B(x,y)$
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $B(x,y)$
- en: '|  | $\displaystyle B(x,y)=B_{\operatorname{diag}}^{1}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{1}$
    |  |'
  id: totrans-1615
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B(x,y)=B_{\operatorname{diag}}^{1}+B_{\operatorname{rank}}^{1}+B_{\operatorname{rank}}^{2}+B_{\operatorname{rank}}^{1}$
    |  |'
- en: where
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: •
  id: totrans-1617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $B_{\operatorname{rank}}^{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  id: totrans-1618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{rank}}^{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: •
  id: totrans-1619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-1620
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-1621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $B_{\operatorname{diag}}^{1}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  id: totrans-1622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{diag}}^{1}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: •
  id: totrans-1623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $B_{\operatorname{rank}}^{3}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $B_{\operatorname{rank}}^{3}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Lemma 11.4.
  id: totrans-1625
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 11.4。
- en: If the following conditions
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $B(x,y)$ be defined as Definition[11.3](#S11.Thmtheorem3 "Definition 11.3\.
    ‣ 11.3 Creating 𝐵⁢(𝑥,𝑦) ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time").'
  id: totrans-1628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '让 $B(x,y)$ 被定义为定义 [11.3](#S11.Thmtheorem3 "Definition 11.3\. ‣ 11.3 Creating
    𝐵⁢(𝑥,𝑦) ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")。'
- en: Then, we have
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: •
  id: totrans-1630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-1631
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一部分。
- en: '|  | $1$2 |  |'
  id: totrans-1632
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-1633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.  $i_{1}\neq i_{0}$
  id: totrans-1634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二部分。 $i_{1}\neq i_{0}$
- en: '|  | $1$2 |  |'
  id: totrans-1635
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1636
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1. We have
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分的证明。我们有
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-1638
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
- en: 'where the first step follows from combining Lemma [11.1](#S11.Thmtheorem1 "Lemma
    11.1\. ‣ 11.1 Computing Hessian ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and Lemma [11.2](#S11.Thmtheorem2
    "Lemma 11.2\. ‣ 11.2 A Helpful Lemma ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步来自于结合引理[11.1](#S11.Thmtheorem1 "引理 11.1. ‣ 11.1 计算 Hessian ‣ 11 Hessian
    for 𝑋 和 𝑌 ‣ 基于张量和 SVM 技巧的 LLM 单层注意力的快速优化视图，并在矩阵乘法时间内求解")和引理[11.2](#S11.Thmtheorem2
    "引理 11.2. ‣ 11.2 一个有用的引理 ‣ 11 Hessian for 𝑋 和 𝑌 ‣ 基于张量和 SVM 技巧的 LLM 单层注意力的快速优化视图，并在矩阵乘法时间内求解")。
- en: Then, we can have
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们可以得到
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}x}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B(x,y)A_{3}$
    |  |'
  id: totrans-1641
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{0}}\mathrm{d}x}=\operatorname{\mathsf{A}}_{j_{0}}^{\top}B(x,y)A_{3}$
    |  |'
- en: Proof of Part 2. We have
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分的证明如下。
- en: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
  id: totrans-1643
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}y_{i_{1},i_{2}}\mathrm{d}x_{i}}=$
    |  |'
- en: 'where the first step follows from combining Lemma [11.1](#S11.Thmtheorem1 "Lemma
    11.1\. ‣ 11.1 Computing Hessian ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and Lemma [11.2](#S11.Thmtheorem2
    "Lemma 11.2\. ‣ 11.2 A Helpful Lemma ‣ 11 Hessian for 𝑋 and 𝑌 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步来自于结合引理[11.1](#S11.Thmtheorem1 "引理 11.1. ‣ 11.1 计算 Hessian ‣ 11 Hessian
    for 𝑋 和 𝑌 ‣ 基于张量和 SVM 技巧的 LLM 单层注意力的快速优化视图，并在矩阵乘法时间内求解")和引理[11.2](#S11.Thmtheorem2
    "引理 11.2. ‣ 11.2 一个有用的引理 ‣ 11 Hessian for 𝑋 和 𝑌 ‣ 基于张量和 SVM 技巧的 LLM 单层注意力的快速优化视图，并在矩阵乘法时间内求解")。
- en: Then, we can have
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们可以得到
- en: '|  | $1$2 |  |'
  id: totrans-1646
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: ∎
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: 12 Lipschitz for Hessian of $x,y$
  id: totrans-1648
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12 对 $x, y$ 的 Hessian 的 Lipschitz 条件
- en: 'In Section [12.1](#S12.SS1 "12.1 Main Results ‣ 12 Lipschitz for Hessian of
    𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), we present
    the main results of the Lipschitz property of $H_{x,y}$. In Section [12.6](#S12.SS6
    "12.6 Calculation: Step 2 Lipschitz for Matrix Function -⟨𝑓⁢(𝑥)_𝑗₀,ℎ⁢(𝑦)_𝑖₀⟩⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the second step of Lipschitz function $-\langle
    f(x)_{j_{0}},h(y)_{i_{0}}\rangle f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$. In Section [12.8](#S12.SS8
    "12.8 Calculation: Step 4 Lipschitz for Matrix Function 𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), we analyze the fourth step of Lipschitz function $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.'
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[12.1节](#S12.SS1 "12.1 主要结果 ‣ 12 对 𝑥,𝑦 的 Hessian 的 Lipschitz 条件 ‣ 基于张量和 SVM
    技巧的 LLM 单层注意力的快速优化视图，并在矩阵乘法时间内求解")中，我们展示了 $H_{x,y}$ 的 Lipschitz 性质的主要结果。在第[12.6节](#S12.SS6
    "12.6 计算: 第 2 步对矩阵函数的 Lipschitz ‣ ⟨𝑓⁢(𝑥)_𝑗₀,ℎ⁢(𝑦)_𝑖₀⟩⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12
    对 𝑥,𝑦 的 Hessian 的 Lipschitz 条件 ‣ 基于张量和 SVM 技巧的 LLM 单层注意力的快速优化视图，并在矩阵乘法时间内求解")中，我们分析了
    Lipschitz 函数 $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
    的第二步。在第[12.8节](#S12.SS8 "12.8 计算: 第 4 步对矩阵函数的 Lipschitz ‣ 𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 12 对 𝑥,𝑦 的 Hessian 的 Lipschitz 条件 ‣ 基于张量和 SVM 技巧的 LLM 单层注意力的快速优化视图，并在矩阵乘法时间内求解")中，我们分析了
    Lipschitz 函数 $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$ 的第四步。'
- en: 12.1 Main Results
  id: totrans-1650
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1 主要结果
- en: 'In this section, we present the main result of Section [12](#S12 "12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了第[12节](#S12 "12 对 𝑥,𝑦 的 Hessian 的 Lipschitz 条件 ‣ 基于张量和 SVM 技巧的 LLM
    单层注意力的快速优化视图，并在矩阵乘法时间内求解")的主要结果。
- en: Lemma 12.1.
  id: totrans-1652
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.1。
- en: If the following conditions hold
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-1655
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: •
  id: totrans-1656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
  id: totrans-1657
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
- en: •
  id: totrans-1658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
  id: totrans-1659
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
- en: •
  id: totrans-1660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be
  id: totrans-1661
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ 为
- en: '|  | $1$2 |  |'
  id: totrans-1662
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Then we have
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有
- en: •
  id: totrans-1664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. For $j_{0}\in[d],i_{0}\in[n]$
  id: totrans-1665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 1\. 对于 $j_{0}\in[d],i_{0}\in[n]$
- en: '|  | $1$2 |  |'
  id: totrans-1666
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-1667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-1668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 2。
- en: '|  | $1$2 |  |'
  id: totrans-1669
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1670
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Proof of Part 1. It follows from Lemma [12.2](#S12.Thmtheorem2 "Lemma 12.2\.
    ‣ 12.2 Summary of Four Steps on Lipschitz for Matrix Functions ‣ 12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1671
  prefs: []
  type: TYPE_NORMAL
  zh: 部分 1 的证明。它来自于引理 [12.2](#S12.Thmtheorem2 "引理 12.2\. ‣ 12.2 矩阵函数 Lipschitz 连续性的四个步骤总结
    ‣ 12 𝑥,𝑦 的 Hessian 的 Lipschitz 连续性 ‣ 快速优化视角：基于张量和 SVM 技巧重新构造 LLM 中的单层注意力，并在矩阵乘法时间内求解")。
- en: Proof of Part 2. We can show that
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: 部分 2 的证明。我们可以展示
- en: '|  | $1$2 |  |'
  id: totrans-1673
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where the first step follows from that we can write $H$ terms $H_{j_{0},i_{0}}$,
    $i_{0}\in[d]$. ∎
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: 首先的步骤来自于我们可以写出 $H$ 项 $H_{j_{0},i_{0}}$，$i_{0}\in[d]$。∎
- en: 12.2 Summary of Four Steps on Lipschitz for Matrix Functions
  id: totrans-1675
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2 矩阵函数 Lipschitz 连续性的四个步骤总结
- en: In this section, we summarize the four steps for analyzing the Lipschitz for
    different matrix functions.
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了分析不同矩阵函数 Lipschitz 连续性的四个步骤。
- en: Lemma 12.2.
  id: totrans-1677
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.2。
- en: If the following conditions hold
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  id: totrans-1680
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: •
  id: totrans-1681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-1682
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-1683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  id: totrans-1684
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: •
  id: totrans-1685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1686
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Then, we have
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1688
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1689
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'The proof follows from Lemma [12.5](#S12.Thmtheorem5 "Lemma 12.5\. ‣ 12.5 Calculation:
    Step 1 Lipschitz for Matrix Function (𝑓⁢(𝑥)_𝑗₀∘ℎ⁢(𝑦)_𝑖₀)⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), Lemma [12.6](#S12.Thmtheorem6 "Lemma 12.6\. ‣ 12.6 Calculation: Step 2
    Lipschitz for Matrix Function -⟨𝑓⁢(𝑥)_𝑗₀,ℎ⁢(𝑦)_𝑖₀⟩⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), Lemma [12.7](#S12.Thmtheorem7 "Lemma 12.7\. ‣ 12.7 Calculation: Step 3
    Lipschitz for Matrix Function -𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢diag(𝑓⁢(𝑥)_𝑗₀) ‣ 12 Lipschitz for
    Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"), and Lemma [12.8](#S12.Thmtheorem8 "Lemma 12.8\. ‣ 12.8 Calculation: Step
    4 Lipschitz for Matrix Function 𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12 Lipschitz
    for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"). ∎'
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: 证明来自于引理 [12.5](#S12.Thmtheorem5 "引理 12.5\. ‣ 12.5 计算：步骤 1 矩阵函数的 Lipschitz 连续性
    (𝑓⁢(𝑥)_𝑗₀∘ℎ⁢(𝑦)_𝑖₀)⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12 𝑥,𝑦 的 Hessian 的 Lipschitz 连续性 ‣ 快速优化视角：基于张量和
    SVM 技巧重新构造 LLM 中的单层注意力，并在矩阵乘法时间内求解")、引理 [12.6](#S12.Thmtheorem6 "引理 12.6\. ‣ 12.6
    计算：步骤 2 矩阵函数的 Lipschitz 连续性 -⟨𝑓⁢(𝑥)_𝑗₀,ℎ⁢(𝑦)_𝑖₀⟩⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤ ‣ 12 𝑥,𝑦
    的 Hessian 的 Lipschitz 连续性 ‣ 快速优化视角：基于张量和 SVM 技巧重新构造 LLM 中的单层注意力，并在矩阵乘法时间内求解")、引理
    [12.7](#S12.Thmtheorem7 "引理 12.7\. ‣ 12.7 计算：步骤 3 矩阵函数的 Lipschitz 连续性 -𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢diag(𝑓⁢(𝑥)_𝑗₀)
    ‣ 12 𝑥,𝑦 的 Hessian 的 Lipschitz 连续性 ‣ 快速优化视角：基于张量和 SVM 技巧重新构造 LLM 中的单层注意力，并在矩阵乘法时间内求解")
    和引理 [12.8](#S12.Thmtheorem8 "引理 12.8\. ‣ 12.8 计算：步骤 4 矩阵函数的 Lipschitz 连续性 𝑐⁢(𝑥,𝑦)_{𝑗₀,𝑖₀}⁢𝑓⁢(𝑥)_𝑗₀⁢𝑓⁢(𝑥)_𝑗₀^⊤
    ‣ 12 𝑥,𝑦 的 Hessian 的 Lipschitz 连续性 ‣ 快速优化视角：基于张量和 SVM 技巧重新构造 LLM 中的单层注意力，并在矩阵乘法时间内求解")。∎
- en: '12.3 A Core Tool: Upper Bound for Several Basic Functions'
  id: totrans-1691
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3 核心工具：几个基本函数的上界
- en: In this section, we give an upper bound for each of the basic functions.
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们给出了每个基本函数的上界。
- en: Lemma 12.3.
  id: totrans-1693
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.3。
- en: If the following conditions hold
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(y)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1696
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $f(y)_{j_{0}}\in\mathbb{R}^{n}$ 如定义[4.10](#S4.Thmtheorem10 "Definition 4.10\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 所定义。'
- en: •
  id: totrans-1697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.11](#S4.Thmtheorem11
    "Definition 4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $h(y)_{i_{0}}\in\mathbb{R}^{n}$ 如定义[4.11](#S4.Thmtheorem11 "Definition 4.11\.
    ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 所定义。'
- en: •
  id: totrans-1699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ 如定义[4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") 所定义。'
- en: •
  id: totrans-1701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1702
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: •
  id: totrans-1703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{3}\|\leq R$
  id: totrans-1704
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{3}\|\leq R$
- en: •
  id: totrans-1705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|y_{i_{0}}\|\leq R$
  id: totrans-1706
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|y_{i_{0}}\|\leq R$
- en: •
  id: totrans-1707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
  id: totrans-1708
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
- en: Then, we have
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到
- en: •
  id: totrans-1710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. $\|h(y)_{i_{0}}\|_{2}\leq R^{2}$
  id: totrans-1711
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分\. $\|h(y)_{i_{0}}\|_{2}\leq R^{2}$
- en: •
  id: totrans-1712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2\. $|c(x,y)_{j_{0},i_{0}}|\leq 2R^{2}$
  id: totrans-1713
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分\. $|c(x,y)_{j_{0},i_{0}}|\leq 2R^{2}$
- en: Proof.
  id: totrans-1714
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的证明。
- en: '|  | $\displaystyle\&#124;h(y)_{i_{0}}\&#124;_{2}=$ |  |'
  id: totrans-1716
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;h(y)_{i_{0}}\&#124;_{2}=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1717
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1718
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to Definition [4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on Fact [4.3](#S4.Thmtheorem3
    "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and the third step is because of Lemma [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步由于定义 [4.11](#S4.Thmtheorem11 "Definition 4.11\. ‣ 4.4 A Helpful Definition
    With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")，第二步基于事实 [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步由于引理 [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Proof of Part 2.
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分的证明。
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}&#124;=$ |  |'
  id: totrans-1721
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1722
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1723
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1724
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is because of Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is based on triangle inequality and Cauchy–Schwarz inequality, the third
    step is due to Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and the last step follows from $R\geq 4$. ∎'
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是根据定义[4.12](#S4.Thmtheorem12 "Definition 4.12\. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第二步基于三角不等式和柯西–施瓦兹不等式，第三步是由于引理[8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time")，最后一步跟随于 $R\geq 4$。∎'
- en: '12.4 A Core Tool: Lipschitz Property for Several Basic Functions'
  id: totrans-1726
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4 核心工具：几种基本函数的 Lipschitz 性质
- en: In this section, we introduce the Lipschitz property for several basic functions.
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了几种基本函数的 Lipschitz 性质。
- en: Lemma 12.4.
  id: totrans-1728
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.4。
- en: If the following conditions hold
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-1730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(y)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1731
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $f(y)_{j_{0}}\in\mathbb{R}^{n}$ 定义为定义[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")。'
- en: •
  id: totrans-1732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $h(y)_{i_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.11](#S4.Thmtheorem11
    "Definition 4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1733
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $h(y)_{i_{0}}\in\mathbb{R}^{n}$ 定义为定义[4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")。'
- en: •
  id: totrans-1734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ 定义为定义[4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: •
  id: totrans-1736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $R\geq 4$
- en: •
  id: totrans-1738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{3}\|\leq R$
  id: totrans-1739
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{3}\|\leq R$
- en: •
  id: totrans-1740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|y_{i_{0}}\|\leq R$
  id: totrans-1741
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|y_{i_{0}}\|\leq R$
- en: •
  id: totrans-1742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
  id: totrans-1743
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|b_{j_{0},i_{0}}\|_{2}\leq R$
- en: •
  id: totrans-1744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1745
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '令 $R_{0}$ 定义为定义[8.6](#S8.Thmtheorem6 "Definition 8.6\. ‣ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Then, we have
  id: totrans-1746
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: •
  id: totrans-1747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. $\|h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\|_{2}\leq R\|y-\widetilde{y}\|_{2}$
  id: totrans-1748
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一部分。$\|h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\|_{2}\leq R\|y-\widetilde{y}\|_{2}$
- en: •
  id: totrans-1749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2. $1$2
  id: totrans-1750
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二部分。$1$2
- en: •
  id: totrans-1751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3. $|c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}})|\leq R\|y-\widetilde{y}\|_{2}$
  id: totrans-1752
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三部分。$|c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}}|\leq R\|y-\widetilde{y}\|_{2}$
- en: Proof.
  id: totrans-1753
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分的证明。
- en: '|  | $\displaystyle\&#124;h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\&#124;_{2}=$
    |  |'
  id: totrans-1755
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;h(y)_{i_{0}}-h(\widetilde{y})_{i_{0}}\&#124;_{2}=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1756
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1757
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from Definition [4.11](#S4.Thmtheorem11 "Definition
    4.11\. ‣ 4.4 A Helpful Definition With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), the second step is based on Fact [4.3](#S4.Thmtheorem3
    "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), and the third step is due to Lemma [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several
    Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time").'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是根据定义 [4.11](#S4.Thmtheorem11 "Definition 4.11\. ‣ 4.4 A Helpful Definition
    With Respect to 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time")，第二步基于事实 [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步由于引理 [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Proof of Part 2.
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分的证明。
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(\widetilde{x},y_{j_{0},i_{0}})&#124;=$
    |  |'
  id: totrans-1760
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(\widetilde{x},y_{j_{0},i_{0}})&#124;=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1761
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1762
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step follows from Cauchy–Schwarz inequality, and the third step is because of
    Part 1 of Lemma [12.3](#S12.SS3 "12.3 A Core Tool: Upper Bound for Several Basic
    Functions ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") and Part 3 of Lemma [8.5](#S8.Thmtheorem5 "Lemma
    8.5 (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property
    for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是根据定义 [4.12](#S4.Thmtheorem12 "Definition 4.12\. ‣ 4.5 Helpful Definitions
    With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")，第二步来自于柯西–施瓦兹不等式，第三步由于引理 [12.3](#S12.SS3 "12.3 A Core
    Tool: Upper Bound for Several Basic Functions ‣ 12 Lipschitz for Hessian of 𝑥,𝑦
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")的第一部分和引理 [8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")的第三部分。'
- en: Proof of Part 3.
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分的证明。
- en: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}})&#124;=$
    |  |'
  id: totrans-1765
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;c(x,y)_{j_{0},i_{0}}-c(x,\widetilde{y})_{j_{0},i_{0}})&#124;=$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1766
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1767
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from Definition [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), the second
    step is due to Cauchy–Schwarz inequality and the third step is because of Part
    4 of Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound). ‣ 8.3
    A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property of
    𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")
    and Part 1 of this Lemma. ∎'
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分的证明。
- en: '12.5 Calculation: Step 1 Lipschitz for Matrix Function $(f(x)_{j_{0}}\circ
    h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$'
  id: totrans-1769
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5 计算：步骤 1 矩阵函数的 Lipschitz 性质 $(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: In this section, we calculate the Lipschitz for $(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$.
  id: totrans-1770
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们计算 $(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$ 的 Lipschitz
    性质。
- en: Lemma 12.5.
  id: totrans-1771
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.5。
- en: If the following conditions
  id: totrans-1772
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-1773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  id: totrans-1774
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: •
  id: totrans-1775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1776
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R_{0}$ 定义见定义[8.6](#S8.Thmtheorem6 "定义 8.6\. ‣ 8.4 基本函数的 Lipschitz 性质 ‣ 8
    Lipschitz 性质 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内求解")。
- en: •
  id: totrans-1777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1778
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 定义见定义[4.9](#S4.Thmtheorem9 "定义 4.9\. ‣ 4.3
    关于 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1780
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 定义见定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣
    4.3 关于 𝑋 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1782
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ 定义见定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 关于 𝑋 和 𝑌 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1784
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1786
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1788
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: Then, we have
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1790
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1791
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{1,1}=$ |  |'
  id: totrans-1793
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,1}=$ |  |'
- en: '|  | $\displaystyle G_{1,2}=$ |  |'
  id: totrans-1794
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,2}=$ |  |'
- en: '|  | $\displaystyle G_{1,3}=$ |  |'
  id: totrans-1795
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{1,3}=$ |  |'
- en: 'where the first step follows from definition of $G_{1,1}$, the second step
    is based on Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step is due to Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper
    Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1796
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步依据 $G_{1,1}$ 的定义，第二步基于事实 [4.2](#S4.Thmtheorem2 "事实 4.2\. ‣ 4.1 基本事实 ‣ 4
    初步 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内求解")，第三步由引理 [8.4](#S8.Thmtheorem4
    "引理 8.4（基本函数上界）。 ‣ 8.3 核心工具：几个基本函数的上界 ‣ 8 Lipschitz 性质 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和
    SVM 技巧重新表述 LLM 中的单层注意力，并在矩阵乘法时间内求解") 推导而来。
- en: We have
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
  id: totrans-1798
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1799
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1800
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from definition of $G_{1,1}$, the second step
    is due to Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is based on combining Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic
    Functions Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time"), and Lemma [12.3](#S12.SS3 "12.3 A Core Tool: Upper Bound
    for Several Basic Functions ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time").'
  id: totrans-1801
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步遵循$G_{1,1}$的定义，第二步由于事实[4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步基于结合引理[8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions Upper Bound).
    ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz Property
    of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")、引理[8.5](#S8.Thmtheorem5
    "Lemma 8.5 (Basic Functions Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") 和引理[12.3](#S12.SS3
    "12.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 12 Lipschitz for
    Hessian of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")。'
- en: Also, we have
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们有
- en: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
  id: totrans-1803
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,2}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1804
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1805
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on definition of $G_{1,2}$, the second step is
    because of Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step follows from Lemma [12.4](#S12.Thmtheorem4 "Lemma 12.4\. ‣ 12.4 A Core
    Tool: Lipschitz Property for Several Basic Functions ‣ 12 Lipschitz for Hessian
    of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步基于$G_{1,2}$的定义，第二步由于事实[4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步遵循引理[12.4](#S12.Thmtheorem4 "Lemma 12.4\. ‣ 12.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Additionally,
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，
- en: '|  | $\displaystyle\&#124;G_{1,3}\&#124;=$ |  |'
  id: totrans-1808
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{1,3}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1809
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1810
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from the definition of $G_{1,3}$, the second step
    follows from Fact [4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is because of Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions
    Lipschitz Property). ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions
    ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single
    Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix
    Multiplication Time").'
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
  zh: '第一步遵循$G_{1,3}$的定义，第二步基于事实[4.3](#S4.Thmtheorem3 "Fact 4.3\. ‣ 4.1 Basic Facts
    ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步由于引理[8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")。'
- en: Combining all the above equations we complete the proof. ∎
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
  zh: 综合以上所有方程，我们完成了证明。∎
- en: '12.6 Calculation: Step 2 Lipschitz for Matrix Function $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  id: totrans-1813
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6 计算：步骤 2 对矩阵函数的 Lipschitz 常数 $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: In this section, we calculate the Lipschitz for $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle
    f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  id: totrans-1814
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们计算 $-\langle f(x)_{j_{0}},h(y)_{i_{0}}\rangle f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
    的 Lipschitz 常数。
- en: Lemma 12.6.
  id: totrans-1815
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.6。
- en: If the following conditions
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件
- en: •
  id: totrans-1817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1818
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 定义为定义 [4.9](#S4.Thmtheorem9 "Definition 4.9\.
    ‣ 4.3 对 $𝑋$ 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1820
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 定义为定义 [4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 对 $𝑋$ 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1822
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ 定义为定义 [4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 对 $𝑋$ 和 $𝑌$ 的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1824
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1826
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1828
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: •
  id: totrans-1829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $1$2
  id: totrans-1830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $1$2
- en: Then, we have
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1832
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1833
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{2,1}=$ |  |'
  id: totrans-1835
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,1}=$ |  |'
- en: '|  | $\displaystyle G_{2,2}=$ |  |'
  id: totrans-1836
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,2}=$ |  |'
- en: '|  | $\displaystyle G_{2,3}=$ |  |'
  id: totrans-1837
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,3}=$ |  |'
- en: '|  | $\displaystyle G_{2,4}=$ |  |'
  id: totrans-1838
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{2,4}=$ |  |'
- en: We have
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有
- en: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
  id: totrans-1840
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1841
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1842
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is based on the definition of $G_{2,1}$, the second step
    follows from Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step is because of Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步基于 $G_{2,1}$ 的定义，第二步来自于事实 [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 基本事实
    ‣ 4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内求解")，第三步由于引理 [8.4](#S8.Thmtheorem4
    "Lemma 8.4 (基本函数上界). ‣ 8.3 核心工具：几个基本函数的上界 ‣ 8 Lipschitz 属性 𝐻_{𝑥,𝑥} ‣ 快速优化视角：基于张量和
    SVM 技巧重构 LLM 中的单层注意力，并在矩阵乘法时间内求解")。
- en: and
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
  id: totrans-1845
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,2}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1846
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1847
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to the definition of $G_{2,1}$, the second step
    is based on Fact [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"), and the
    third step follows from Lemma [12.4](#S12.Thmtheorem4 "Lemma 12.4\. ‣ 12.4 A Core
    Tool: Lipschitz Property for Several Basic Functions ‣ 12 Lipschitz for Hessian
    of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1848
  prefs: []
  type: TYPE_NORMAL
  zh: '其中第一步是基于 $G_{2,1}$ 的定义，第二步基于事实 [4.1](#S4.Thmtheorem1 "Fact 4.1\. ‣ 4.1 Basic
    Facts ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")，第三步则来自引理 [12.4](#S12.Thmtheorem4 "Lemma 12.4\. ‣ 12.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: Similarly, we have
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们有
- en: '|  | $\displaystyle\&#124;G_{2,3}\&#124;\leq$ |  |'
  id: totrans-1850
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,3}\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\&#124;G_{2,4}\&#124;\leq$ |  |'
  id: totrans-1851
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{2,4}\&#124;\leq$ |  |'
- en: Combining all the above equations we complete the proof. ∎
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
  zh: 结合上述所有方程，我们完成了证明。∎
- en: '12.7 Calculation: Step 3 Lipschitz for Matrix Function $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$'
  id: totrans-1853
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.7 计算：步骤 3 Lipschitz 对矩阵函数 $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: In this section, we calculate the Lipschitz for $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$.
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们计算 $-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$ 的 Lipschitz
    常数。
- en: Lemma 12.7.
  id: totrans-1855
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.7。
- en: If the following conditions
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件
- en: •
  id: totrans-1857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1858
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $\alpha(x)_{j_{0}}\in\mathbb{R}$ 定义为定义[4.9](#S4.Thmtheorem9 "Definition 4.9\.
    ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: •
  id: totrans-1859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1860
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $f(x)_{j_{0}}\in\mathbb{R}^{n}$ 定义为定义[4.10](#S4.Thmtheorem10 "Definition
    4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
- en: •
  id: totrans-1861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1862
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ 定义为定义[4.12](#S4.Thmtheorem12 "Definition
    4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
- en: •
  id: totrans-1863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1864
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1866
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$，$\|x\|_{2}\leq R$，$\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1868
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $R\geq 4$
- en: •
  id: totrans-1869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $R_{0}$ be defined as Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1870
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $R_{0}$ 定义为定义[8.6](#S8.Thmtheorem6 "Definition 8.6\. ‣ 8.4 A Core Tool: Lipschitz
    Property for Several Basic Functions ‣ 8 Lipschitz Property of 𝐻_{𝑥,𝑥} ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time")。'
- en: •
  id: totrans-1871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  id: totrans-1872
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: Then, we have
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1874
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1875
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{3,1}=$ |  |'
  id: totrans-1877
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,1}=$ |  |'
- en: '|  | $\displaystyle G_{3,2}=$ |  |'
  id: totrans-1878
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,2}=$ |  |'
- en: '|  | $\displaystyle G_{3,3}=$ |  |'
  id: totrans-1879
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{3,3}=$ |  |'
- en: For $G_{3,1}$, we have
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $G_{3,1}$，我们有
- en: '|  | $\displaystyle\&#124;G_{3,1}\&#124;=$ |  |'
  id: totrans-1881
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{3,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1882
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1883
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step follows from definition of $G_{3,1}$, the second step
    is based on Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step is because of Lemma [12.4](#S12.Thmtheorem4 "Lemma 12.4\. ‣ 12.4 A
    Core Tool: Lipschitz Property for Several Basic Functions ‣ 12 Lipschitz for Hessian
    of 𝑥,𝑦 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM
    Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，第一步遵循$G_{3,1}$的定义，第二步基于事实[4.2](#S4.Thmtheorem2 "事实 4.2\. ‣ 4.1 基本事实 ‣ 4 初步
    ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内求解")，第三步由于引理[12.4](#S12.Thmtheorem4 "引理
    12.4\. ‣ 12.4 核心工具：几种基本函数的Lipschitz性质 ‣ 12 对𝑥,𝑦的Hessian的Lipschitz ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内求解")。
- en: Similarly, we have
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们有
- en: '|  | $\displaystyle\&#124;G_{3,2}\&#124;\leq$ |  |'
  id: totrans-1886
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{3,2}\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\&#124;G_{3,3}\&#124;\leq$ |  |'
  id: totrans-1887
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{3,3}\&#124;\leq$ |  |'
- en: Combining all the above equations we complete the proof. ∎
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
  zh: 结合以上所有方程，我们完成了证明。 ∎
- en: '12.8 Calculation: Step 4 Lipschitz for Matrix Function $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$'
  id: totrans-1889
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.8 计算：步骤 4 矩阵函数 $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$ 的Lipschitz常数
- en: In this section, we calculate the Lipschitz for $c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$.
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们计算$c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$的Lipschitz常数。
- en: Lemma 12.8.
  id: totrans-1891
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.8。
- en: If the following conditions
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件
- en: •
  id: totrans-1893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\alpha(x)_{j_{0}}\in\mathbb{R}$ be defined as Definition[4.9](#S4.Thmtheorem9
    "Definition 4.9\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1894
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令$\alpha(x)_{j_{0}}\in\mathbb{R}$，其定义见定义[4.9](#S4.Thmtheorem9 "定义 4.9\. ‣ 4.3
    关于𝑋的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $f(x)_{j_{0}}\in\mathbb{R}^{n}$ be defined as Definition[4.10](#S4.Thmtheorem10
    "Definition 4.10\. ‣ 4.3 Helpful Definitions With Respect to 𝑋 ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1896
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令$f(x)_{j_{0}}\in\mathbb{R}^{n}$，其定义见定义[4.10](#S4.Thmtheorem10 "定义 4.10\. ‣
    4.3 关于𝑋的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$ be defined as Definition[4.12](#S4.Thmtheorem12
    "Definition 4.12\. ‣ 4.5 Helpful Definitions With Respect to Both 𝑋 and 𝑌 ‣ 4
    Preliminary ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-1898
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令$c(x,y)_{j_{0},i_{0}}\in\mathbb{R}$，其定义见定义[4.12](#S4.Thmtheorem12 "定义 4.12\.
    ‣ 4.5 关于𝑋和𝑌的有用定义 ‣ 4 初步 ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内求解")
- en: •
  id: totrans-1899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
  id: totrans-1900
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令$\gamma(x)_{j_{0}}=\langle f(x)_{j_{0}},v\rangle\in\mathbb{R}$
- en: •
  id: totrans-1901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$, $\|x\|_{2}\leq R$, $\|v\|_{2}\leq R^{2}$
  id: totrans-1902
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\|A_{1}\|,\|A_{2}\|,\|A_{3}\|\leq R$，$\|x\|_{2}\leq R$，$\|v\|_{2}\leq R^{2}$
- en: •
  id: totrans-1903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $R\geq 4$
  id: totrans-1904
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令$R\geq 4$
- en: •
  id: totrans-1905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $R_{0}$ be defined in Definition[8.6](#S8.Thmtheorem6 "Definition 8.6\.
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1906
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令$R_{0}$，其定义见定义[8.6](#S8.Thmtheorem6 "定义 8.6\. ‣ 8.4 核心工具：几种基本函数的Lipschitz性质
    ‣ 8 𝐻_{𝑥,𝑥}的Lipschitz性质 ‣ 快速优化视角：基于张量和SVM技巧重构LLM中的单层注意力，并在矩阵乘法时间内求解")。
- en: •
  id: totrans-1907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1908
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令$G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Then, we have
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1910
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1911
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We define
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义
- en: '|  | $\displaystyle G_{4,1}=$ |  |'
  id: totrans-1913
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,1}=$ |  |'
- en: '|  | $\displaystyle G_{4,2}=$ |  |'
  id: totrans-1914
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,2}=$ |  |'
- en: '|  | $\displaystyle G_{4,3}=$ |  |'
  id: totrans-1915
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,3}=$ |  |'
- en: '|  | $\displaystyle G_{4,4}=$ |  |'
  id: totrans-1916
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{4,4}=$ |  |'
- en: For $G_{4,1}$, we have
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
  zh: 对于$G_{4,1}$，我们有
- en: '|  | $\displaystyle\&#124;G_{4,1}\&#124;=$ |  |'
  id: totrans-1918
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,1}\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1919
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1920
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: 'where the first step is due to definition of $G_{4,1}$, the second step is
    because of Fact [4.2](#S4.Thmtheorem2 "Fact 4.2\. ‣ 4.1 Basic Facts ‣ 4 Preliminary
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") and the
    third step follows from Lemma [8.4](#S8.Thmtheorem4 "Lemma 8.4 (Basic Functions
    Upper Bound). ‣ 8.3 A Core Tool: Upper Bound for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time") and Lemma [8.5](#S8.Thmtheorem5 "Lemma 8.5 (Basic Functions Lipschitz Property).
    ‣ 8.4 A Core Tool: Lipschitz Property for Several Basic Functions ‣ 8 Lipschitz
    Property of 𝐻_{𝑥,𝑥} ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步是由于 $G_{4,1}$ 的定义，第二步是因为事实 [4.2](#S4.Thmtheorem2 "事实 4.2\. ‣ 4.1 基本事实 ‣
    4 初步 ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力重构及在矩阵乘法时间内解决")，第三步来自引理 [8.4](#S8.Thmtheorem4
    "引理 8.4 (基本函数的上界). ‣ 8.3 核心工具：几个基本函数的上界 ‣ 8 𝐻_{𝑥,𝑥} 的 Lipschitz 性质 ‣ 快速优化视角：基于张量和
    SVM 技巧的单层注意力重构及在矩阵乘法时间内解决") 和引理 [8.5](#S8.Thmtheorem5 "引理 8.5 (基本函数的 Lipschitz
    性质). ‣ 8.4 核心工具：几个基本函数的 Lipschitz 性质 ‣ 8 𝐻_{𝑥,𝑥} 的 Lipschitz 性质 ‣ 快速优化视角：基于张量和
    SVM 技巧的单层注意力重构及在矩阵乘法时间内解决")。
- en: Similarly, we have
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们得到
- en: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq$ |  |'
  id: totrans-1923
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,2}\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq$ |  |'
  id: totrans-1924
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,3}\&#124;\leq$ |  |'
- en: '|  | $\displaystyle\&#124;G_{4,4}\&#124;\leq$ |  |'
  id: totrans-1925
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;G_{4,4}\&#124;\leq$ |  |'
- en: Combining all the above equations we complete the proof. ∎
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: 结合以上所有方程，我们完成了证明。∎
- en: 12.9 PSD Upper Bound for Hessian $x,y$
  id: totrans-1927
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.9 关于 Hessian $x,y$ 的 PSD 上界
- en: In this section, we analyze the PSD upper bound for Hessian.
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了 Hessian 的 PSD 上界。
- en: Lemma 12.9.
  id: totrans-1929
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.9。
- en: If the following conditions hold
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-1931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
  id: totrans-1932
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\max_{j_{0}\in[n]}\|\operatorname{\mathsf{A}}_{j_{0}}\|\leq R$
- en: •
  id: totrans-1933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
  id: totrans-1934
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $H(x,y)_{j_{0},i_{0}}\in\mathbb{R}^{d^{2}\times d}$
- en: •
  id: totrans-1935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
  id: totrans-1936
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\frac{\mathrm{d}^{2}L_{j_{0},i_{0}}}{\mathrm{d}x\mathrm{d}y_{i_{1}}}={\bf 0}_{d^{2}\times
    d}$
- en: •
  id: totrans-1937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ be
  id: totrans-1938
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $H(x,y)\in\mathbb{R}^{d^{2}\times d^{2}}$ 为
- en: '|  | $1$2 |  |'
  id: totrans-1939
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Then we have
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到
- en: •
  id: totrans-1941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. For $j_{0}\in[d],i_{0}\in[n]$
  id: totrans-1942
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分。对于 $j_{0}\in[d],i_{0}\in[n]$
- en: '|  | $\displaystyle\&#124;H(x,y)_{j_{0},i_{0}}\&#124;\leq 10R^{2}$ |  |'
  id: totrans-1943
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x,y)_{j_{0},i_{0}}\&#124;\leq 10R^{2}$ |  |'
- en: •
  id: totrans-1944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2.
  id: totrans-1945
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。
- en: '|  | $\displaystyle\&#124;H(x,y)\&#124;\leq nd\cdot 10R^{2}$ |  |'
  id: totrans-1946
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x,y)\&#124;\leq nd\cdot 10R^{2}$ |  |'
- en: Proof.
  id: totrans-1947
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Proof of Part 1. It follows from Lemma [12.10](#S12.Thmtheorem10 "Lemma 12.10\.
    ‣ 12.10 Upper Bound on Hessian Spectral Norms ‣ 12 Lipschitz for Hessian of 𝑥,𝑦
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: 第1部分的证明。它来自引理 [12.10](#S12.Thmtheorem10 "引理 12.10\. ‣ 12.10 关于 Hessian 谱范数的上界
    ‣ 12 对于 𝑥,𝑦 的 Hessian 的 Lipschitz ‣ 快速优化视角：基于张量和 SVM 技巧的单层注意力重构及在矩阵乘法时间内解决")。
- en: Proof of Part 2. We can show that
  id: totrans-1949
  prefs: []
  type: TYPE_NORMAL
  zh: 第2部分的证明。我们可以展示
- en: '|  | $\displaystyle\&#124;H(x,y)\&#124;=$ |  |'
  id: totrans-1950
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;H(x,y)\&#124;=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-1951
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: where the first step is due to the assumption of $H(x,y)$, and the second step
    comes from Part 1. ∎
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步是由于 $H(x,y)$ 的假设，第二步来自第1部分。∎
- en: 12.10 Upper Bound on Hessian Spectral Norms
  id: totrans-1953
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.10 关于 Hessian 谱范数的上界
- en: In this section, we find the upper bound for the Hessian spectral norms.
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们找到了 Hessian 谱范数的上界。
- en: Lemma 12.10.
  id: totrans-1955
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12.10。
- en: If the following conditions hold
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下条件成立
- en: •
  id: totrans-1957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
  id: totrans-1958
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{1}(x,y)=(f(x)_{j_{0}}\circ h(y)_{i_{0}})f(x)_{j_{0}}^{\top}$
- en: •
  id: totrans-1959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $1$2
  id: totrans-1960
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $1$2
- en: •
  id: totrans-1961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
  id: totrans-1962
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{3}(x,y)=-c(x,y)_{j_{0},i_{0}}\operatorname{diag}(f(x)_{j_{0}})$
- en: •
  id: totrans-1963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
  id: totrans-1964
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G_{4}(x,y)=c(x,y)_{j_{0},i_{0}}f(x)_{j_{0}}f(x)_{j_{0}}^{\top}$
- en: Then, we have
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到
- en: •
  id: totrans-1966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1\. $\|G_{1}(x,y)\|\leq R^{2}$
  id: totrans-1967
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1部分。$\|G_{1}(x,y)\|\leq R^{2}$
- en: •
  id: totrans-1968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2\. $\|G_{2}(x,y)\|\leq R^{2}$
  id: totrans-1969
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2部分。$\|G_{2}(x,y)\|\leq R^{2}$
- en: •
  id: totrans-1970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3\. $\|G_{3}(x,y)\|\leq 2R^{2}$
  id: totrans-1971
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第3部分。$\|G_{3}(x,y)\|\leq 2R^{2}$
- en: •
  id: totrans-1972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 4\. $\|G_{4}(x,y)\|\leq 2R^{2}$
  id: totrans-1973
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4部分。$\|G_{4}(x,y)\|\leq 2R^{2}$
- en: •
  id: totrans-1974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 5.
  id: totrans-1975
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5部分。
- en: '|  | $\displaystyle\sum_{k=1}^{4}\&#124;G_{k}(x,y)\&#124;\leq 10R^{2}$ |  |'
  id: totrans-1976
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{k=1}^{4}\&#124;G_{k}(x,y)\&#124;\leq 10R^{2}$ |  |'
- en: Proof.
  id: totrans-1977
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: The proof is straightforward by using upper bound on each term ∎
  id: totrans-1978
  prefs: []
  type: TYPE_NORMAL
  zh: 证明通过对每项使用上界来得出，∎
- en: 13 Generating a Spectral Sparsifier via TensorSketch
  id: totrans-1979
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13 通过 TensorSketch 生成谱稀疏化器
- en: 'Tensor type sketching has been widely used in problems [[165](#bib.bib165),
    [58](#bib.bib58), [52](#bib.bib52), [6](#bib.bib6), [163](#bib.bib163), [173](#bib.bib173),
    [166](#bib.bib166), [202](#bib.bib202), [170](#bib.bib170)]. Section [13.1](#S13.SS1
    "13.1 Oblivious Subspace Embedding ‣ 13 Generating a Spectral Sparsifier via TensorSketch
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time") presents
    the definition of oblivious subspace embedding. In Section [13.2](#S13.SS2 "13.2
    TensorSRHT ‣ 13 Generating a Spectral Sparsifier via TensorSketch ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we give an overview of $\mathsf{TensorSRHT}$.
    In Section [13.4](#S13.SS4 "13.4 Fast Approximation for Hessian via Sketching
    ‣ 13 Generating a Spectral Sparsifier via TensorSketch ‣ A Fast Optimization View:
    Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and
    Solving It in Matrix Multiplication Time"), we introduce the fast approximation
    for hessian via sketching.'
  id: totrans-1980
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor 类型的草图在问题中被广泛使用 [[165](#bib.bib165), [58](#bib.bib58), [52](#bib.bib52),
    [6](#bib.bib6), [163](#bib.bib163), [173](#bib.bib173), [166](#bib.bib166), [202](#bib.bib202),
    [170](#bib.bib170)]。第 [13.1](#S13.SS1 "13.1 无关子空间嵌入 ‣ 13 通过 TensorSketch 生成谱稀疏化器
    ‣ 快速优化视图：基于 Tensor 和 SVM 技巧的单层注意力重新构造，并以矩阵乘法时间解决") 节介绍了无关子空间嵌入的定义。在第 [13.2](#S13.SS2
    "13.2 TensorSRHT ‣ 13 通过 TensorSketch 生成谱稀疏化器 ‣ 快速优化视图：基于 Tensor 和 SVM 技巧的单层注意力重新构造，并以矩阵乘法时间解决")
    节中，我们概述了 $\mathsf{TensorSRHT}$。在第 [13.4](#S13.SS4 "13.4 通过草图进行 Hessian 的快速近似 ‣
    13 通过 TensorSketch 生成谱稀疏化器 ‣ 快速优化视图：基于 Tensor 和 SVM 技巧的单层注意力重新构造，并以矩阵乘法时间解决")
    节中，我们介绍了通过草图进行 Hessian 的快速近似。
- en: 13.1 Oblivious Subspace Embedding
  id: totrans-1981
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1 无关子空间嵌入
- en: We define oblivious subspace embedding,
  id: totrans-1982
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了无关子空间嵌入，
- en: Definition 13.1  (Oblivious subspace embedding, [[156](#bib.bib156)]).
  id: totrans-1983
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 13.1  (无关子空间嵌入，[[156](#bib.bib156)])。
- en: We define $(\epsilon,\delta,d,n)$ is a distribution on $m\times n$, where $m$,
    and $\delta$, for any fixed $n\times d$, a matrix $S$ has the property that the
    singular values of $SU$.
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $(\epsilon,\delta,d,n)$ 是 $m\times n$ 的一个分布，其中 $m$ 和 $\delta$，对于任何固定的 $n\times
    d$，矩阵 $S$ 具有 $SU$ 的奇异值性质。
- en: 13.2 TensorSRHT
  id: totrans-1985
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2 TensorSRHT
- en: We define a well-known sketching matrix family called TensorSRHT [[104](#bib.bib104),
    [6](#bib.bib6)]. It has been used in many optimization literature [[163](#bib.bib163),
    [173](#bib.bib173), [166](#bib.bib166)].
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一种称为 TensorSRHT 的著名草图矩阵族 [[104](#bib.bib104), [6](#bib.bib6)]。它已被广泛用于许多优化文献
    [[163](#bib.bib163), [173](#bib.bib173), [166](#bib.bib166)]。
- en: Definition 13.2  (Tensor subsampled randomized Hadamard transform (TensorSRHT)
    [[6](#bib.bib6), [163](#bib.bib163)]).
  id: totrans-1987
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 13.2  (Tensor 子样本随机 Hadamard 变换 (TensorSRHT) [[6](#bib.bib6), [163](#bib.bib163)])。
- en: The $\mathsf{TensorSRHT}$ is defined as
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathsf{TensorSRHT}$ 定义为
- en: '|  | $\displaystyle S:=\frac{1}{\sqrt{m}}P\cdot(HD_{1}\otimes HD_{2}),$ |  |'
  id: totrans-1989
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S:=\frac{1}{\sqrt{m}}P\cdot(HD_{1}\otimes HD_{2}),$ |  |'
- en: where each row of $P\in\{0,1\}^{m\times n^{2}}$ at a random coordinate and one
    can view $P$ is a $n\times n$, $D_{2}$ independent diagonal matrices with diagonals
    that are each independently set to be a Rademacher random variable (uniform in
    $\{-1,1\}$).
  id: totrans-1990
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P\in\{0,1\}^{m\times n^{2}}$ 的每一行在随机坐标下，$P$ 可以视作是一个 $n\times n$ 的矩阵，$D_{2}$
    是独立的对角矩阵，其对角线上的元素独立地设置为 Rademacher 随机变量（在 $\{-1,1\}$ 中均匀分布）。
- en: It is known [[6](#bib.bib6)] that TensorSRHT matrices imply the OSE.
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
  zh: 已知[[6](#bib.bib6)] TensorSRHT 矩阵暗示 OSE。
- en: Lemma 13.3  ([[6](#bib.bib6), [163](#bib.bib163)] , see for example, Lemma 2.12
    in [[163](#bib.bib163)]).
  id: totrans-1992
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 13.3  ([[6](#bib.bib6), [163](#bib.bib163)] ，例如，见[[163](#bib.bib163)]中的引理
    2.12)。
- en: 'Let $S$ be a TensorSRHT matrix defined in Definition [13.2](#S13.Thmtheorem2
    "Definition 13.2 (Tensor subsampled randomized Hadamard transform (TensorSRHT)
    [6, 163]). ‣ 13.2 TensorSRHT ‣ 13 Generating a Spectral Sparsifier via TensorSketch
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"). If'
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $S$ 是在定义 [13.2](#S13.Thmtheorem2 "定义 13.2 (张量子采样随机 Hadamard 变换 (TensorSRHT)
    [6, 163])。 ‣ 13.2 TensorSRHT ‣ 13 通过 TensorSketch 生成谱稀疏化器 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决")
    中定义的 TensorSRHT 矩阵。如果
- en: '|  | $\displaystyle m=O(\epsilon^{-2}d^{2}\log^{3}(nd/\epsilon\delta)),$ |  |'
  id: totrans-1994
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m=O(\epsilon^{-2}d^{2}\log^{3}(nd/\epsilon\delta)),$ |  |'
- en: then $S$-OSE for degree-$2$ tensors.
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 $S$-OSE 对于度数-$2$ 张量。
- en: Further for matrices $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ can be computed
    in $\widetilde{O}(nd+md^{2})$ time.
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
  zh: 对于矩阵 $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ 可以在 $\widetilde{O}(nd+md^{2})$ 时间内计算。
- en: 13.3 TensorSparse
  id: totrans-1997
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3 TensorSparse
- en: '[[166](#bib.bib166)] define TensorSparse by compose Sparse embedding [[133](#bib.bib133),
    [43](#bib.bib43)] with tensor operation [[136](#bib.bib136)].'
  id: totrans-1998
  prefs: []
  type: TYPE_NORMAL
  zh: '[[166](#bib.bib166)] 通过将稀疏嵌入 [[133](#bib.bib133), [43](#bib.bib43)] 与张量操作 [[136](#bib.bib136)]
    组合来定义 TensorSparse。'
- en: Definition 13.4  (TensorSparse, see Definition 7.6 in [[166](#bib.bib166)]).
  id: totrans-1999
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 13.4  （TensorSparse，见定义 7.6 在 [[166](#bib.bib166)]）。
- en: 'Let $h_{1},h_{2}:[n]\times[s]\rightarrow[m/s]$-wise independent hash functions
    and let $\sigma_{1},\sigma_{2}:[n]\times[s]\rightarrow\{\pm 1\}$-wise independent
    random sign functions. Then, the degree two tensor sparse transform, $S:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
    is given as:'
  id: totrans-2000
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $h_{1},h_{2}:[n]\times[s]\rightarrow[m/s]$-wise 独立哈希函数，$\sigma_{1},\sigma_{2}:[n]\times[s]\rightarrow\{\pm
    1\}$-wise 独立随机符号函数。然后，度数二张量稀疏变换 $S:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
    由以下公式给出：
- en: '|  | $\displaystyle R_{r,(i,j)}=$ |  |'
  id: totrans-2001
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R_{r,(i,j)}=$ |  |'
- en: Lemma 13.5  (Theorem 7.10 in [[166](#bib.bib166)]).
  id: totrans-2002
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 13.5  （定理 7.10 在 [[166](#bib.bib166)]）。
- en: 'Let $\epsilon\in(0,1)$ be success probability. Let $S\in\mathbb{R}^{m\times
    n^{2}}$ matrix (Def. [13.4](#S13.Thmtheorem4 "Definition 13.4 (TensorSparse, see
    Definition 7.6 in [166]). ‣ 13.3 TensorSparse ‣ 13 Generating a Spectral Sparsifier
    via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")). Suppose $m=\Omega(\epsilon^{-2}d^{2}\log(n/\delta))$, then TensorSparse
    provides $(\epsilon,\delta,d^{2},n^{2})$-OSE.'
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\epsilon\in(0,1)$ 为成功概率。设 $S\in\mathbb{R}^{m\times n^{2}}$ 矩阵（定义 [13.4](#S13.Thmtheorem4
    "定义 13.4 (TensorSparse，见定义 7.6 在 [166] 中)。 ‣ 13.3 TensorSparse ‣ 13 通过 TensorSketch
    生成谱稀疏化器 ‣ 快速优化视角：基于张量和 SVM 技巧重新表述单层注意力，并在矩阵乘法时间内解决")）。假设 $m=\Omega(\epsilon^{-2}d^{2}\log(n/\delta))$，则
    TensorSparse 提供 $(\epsilon,\delta,d^{2},n^{2})$-OSE。
- en: Further for matrices $A_{1},A_{2}\in\mathbb{R}^{n\times d}$ can be computed
    in $O((\operatorname{nnz}(A_{1})+\operatorname{nnz}(A_{2}))s+md^{2})$ time
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
  zh: 对于矩阵 $A_{1},A_{2}\in\mathbb{R}^{n\times d}$，可以在 $O((\operatorname{nnz}(A_{1})+\operatorname{nnz}(A_{2}))s+md^{2})$
    时间内计算。
- en: 13.4 Fast Approximation for Hessian via Sketching
  id: totrans-2005
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4 通过草图进行 Hessian 的快速近似
- en: In this section, we present the fast approximation for hessian via sketching.
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍通过草图进行 Hessian 的快速近似。
- en: Lemma 13.6.
  id: totrans-2007
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 13.6。
- en: If the following conditions hold
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-2009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $A_{1}\in\mathbb{R}^{n\times d}$
  id: totrans-2010
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $A_{1}\in\mathbb{R}^{n\times d}$
- en: •
  id: totrans-2011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\operatorname{\mathsf{A}}=(A_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
  id: totrans-2012
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\operatorname{\mathsf{A}}=(A_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
- en: •
  id: totrans-2013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $W\in\mathbb{R}^{n\times n}$ denote a positive diagonal matrix
  id: totrans-2014
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $W\in\mathbb{R}^{n\times n}$ 表示一个正对角矩阵
- en: •
  id: totrans-2015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\overline{A}_{1}=WA_{1}$
  id: totrans-2016
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\overline{A}_{1}=WA_{1}$
- en: •
  id: totrans-2017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\overline{\operatorname{\mathsf{A}}}=(\overline{A}_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
  id: totrans-2018
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\overline{\operatorname{\mathsf{A}}}=(\overline{A}_{1}\otimes A_{2})\in\mathbb{R}^{n^{2}\times
    d^{2}}$
- en: Then, we have
  id: totrans-2019
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到
- en: •
  id: totrans-2020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 1.
  id: totrans-2021
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一部分。
- en: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  id: totrans-2022
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
- en: •
  id: totrans-2023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 2. For any constant $\epsilon\in(0,0.1)$ time to compute $S\overline{\operatorname{\mathsf{A}}}$
    such that
  id: totrans-2024
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二部分。对于任何常数 $\epsilon\in(0,0.1)$，计算 $S\overline{\operatorname{\mathsf{A}}}$
    的时间使得
- en: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  id: totrans-2025
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
- en: holds with probability $1-\delta$.
  id: totrans-2026
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以概率$1-\delta$成立。
- en: •
  id: totrans-2027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Part 3. For any $\epsilon\in(0,0.1)$ time to compute $S\overline{\operatorname{\mathsf{A}}}$
    such that
  id: totrans-2028
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三部分。对于任何 $\epsilon\in(0,0.1)$，计算 $S\overline{\operatorname{\mathsf{A}}}$ 的时间，使得
- en: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
  id: totrans-2029
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle(1-\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}\preceq\overline{\operatorname{\mathsf{A}}}^{\top}S^{\top}S\overline{\operatorname{\mathsf{A}}}\preceq(1+\epsilon)\cdot\overline{\operatorname{\mathsf{A}}}^{\top}\overline{\operatorname{\mathsf{A}}}$
    |  |'
- en: holds with probability $1-\delta$.
  id: totrans-2030
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以概率$1-\delta$成立。
- en: Proof.
  id: totrans-2031
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Proof of Part 1.
  id: totrans-2032
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分的证明。
- en: We can show
  id: totrans-2033
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示
- en: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=$
    |  |'
  id: totrans-2034
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{\mathsf{A}}^{\top}(W^{2}\otimes I_{n})\operatorname{\mathsf{A}}=$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-2035
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-2036
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-2037
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: where the first step follows from $(W^{2}\otimes I)=(W\otimes I_{n})\cdot(W\otimes
    I_{n})$ operation and $W$, the third step follows from the definition of $\overline{A}_{1}$.
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一步来自$(W^{2}\otimes I)=(W\otimes I_{n})\cdot(W\otimes I_{n})$ 操作和 $W$，第三步来自
    $\overline{A}_{1}$ 的定义。
- en: Proof of Part 2.
  id: totrans-2039
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分的证明。
- en: 'It follows from using Lemma [13.3](#S13.Thmtheorem3 "Lemma 13.3 ([6, 163] ,
    see for example, Lemma 2.12 in [163]). ‣ 13.2 TensorSRHT ‣ 13 Generating a Spectral
    Sparsifier via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time").'
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以从引理 [13.3](#S13.Thmtheorem3 "引理 13.3 ([6, 163]，例如，见[163]中的引理 2.12)。 ‣ 13.2
    TensorSRHT ‣ 13 通过 TensorSketch 生成谱稀疏器 ‣ 快速优化视角：基于 Tensor 和 SVM 技巧的单层注意力的重新表述，并在矩阵乘法时间内解决它。")
    得出。
- en: Proof of Part 3.
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分的证明。
- en: 'It follows from using Lemma [13.5](#S13.Thmtheorem5 "Lemma 13.5 (Theorem 7.10
    in [166]). ‣ 13.3 TensorSparse ‣ 13 Generating a Spectral Sparsifier via TensorSketch
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-2042
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以从引理 [13.5](#S13.Thmtheorem5 "引理 13.5 (定理 7.10 在 [166] 中)。 ‣ 13.3 TensorSparse
    ‣ 13 通过 TensorSketch 生成谱稀疏器 ‣ 快速优化视角：基于 Tensor 和 SVM 技巧的单层注意力的重新表述，并在矩阵乘法时间内解决它。")
    得出。
- en: ∎
  id: totrans-2043
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '14 Analysis Of Algorithm [1](#alg1 "Algorithm 1 ‣ 6 Hessian ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time")'
  id: totrans-2044
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14 算法分析 [1](#alg1 "算法 1 ‣ 6 Hessian ‣ 快速优化视角：基于 Tensor 和 SVM 技巧的单层注意力的重新表述，并在矩阵乘法时间内解决它")
- en: 'We introduce the concept of a $(l,M)$-good function in Section [14.1](#S14.SS1
    "14.1 (𝑙,𝑀)-Good Loss Function ‣ 14 Analysis Of Algorithm 1 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") and discuss the notion of a well-initialized
    point. Subsequently, we will present our approximation and update rule methods
    in Section [14.2](#S14.SS2 "14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"). In light of the
    optimization problem introduced in Definition [1.2](#S1.Thmtheorem2 "Definition
    1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"), we put forward Algorithm [1](#alg1 "Algorithm 1
    ‣ 6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"),
    and in this section, we establish the correctness and convergence of the algorithm.'
  id: totrans-2045
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第[14.1](#S14.SS1 "14.1 (𝑙,𝑀)-Good Loss Function ‣ 14 Analysis Of Algorithm
    1 ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")节介绍了 $(l,M)$-良好函数的概念，并讨论了良好初始化点的概念。随后，我们将在第[14.2](#S14.SS2
    "14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")节中介绍我们的近似和更新规则方法。鉴于定义[1.2](#S1.Thmtheorem2 "Definition
    1.2 (Attention optimization). ‣ 1 Introduction ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")中引入的优化问题，我们提出了算法[1](#alg1 "Algorithm 1 ‣ 6 Hessian
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")，在本节中，我们建立了该算法的正确性和收敛性。'
- en: 14.1 $(l,M)$-Good Loss Function
  id: totrans-2046
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1 $(l,M)$-良好损失函数
- en: 'We will now introduce the definition of a $(l,M)$-Good Loss Function. Next,
    let’s revisit the optimization problem defined in Definition [4.7](#S4.Thmtheorem7
    "Definition 4.7\. ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") as follows:'
  id: totrans-2047
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在介绍 $(l,M)$-良好损失函数的定义。接下来，我们回顾在定义[4.7](#S4.Thmtheorem7 "Definition 4.7\.
    ‣ 4.2 General Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")中定义的优化问题，如下所示：'
- en: '|  | $1$2 |  |'
  id: totrans-2048
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: We will now demonstrate that our optimization function possesses the following
    properties.
  id: totrans-2049
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示我们的优化函数具有以下特性。
- en: Definition 14.1  ($(l,M)$-good Loss function).
  id: totrans-2050
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 14.1 ($(l,M)$-良好损失函数)。
- en: For a function $L:\mathbb{R}^{d}\rightarrow\mathbb{R}$, if the following conditions
    hold,
  id: totrans-2051
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个函数 $L:\mathbb{R}^{d}\rightarrow\mathbb{R}$，如果满足以下条件，
- en: •
  id: totrans-2052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hessian is $M$ such that
  id: totrans-2053
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hessian 是 $M$，满足以下条件：
- en: '|  | $1$2 |  |'
  id: totrans-2054
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: •
  id: totrans-2055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $l$ as a positive scalar. If there exists a vector $x^{*}\in\mathbb{R}^{d^{2}}$
    such that the following holds
  id: totrans-2056
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $l$ 为一个正标量。如果存在一个向量 $x^{*}\in\mathbb{R}^{d^{2}}$，使得以下条件成立：
- en: –
  id: totrans-2057
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: $\nabla L(x^{*},y^{*})={\bf 0}_{d}$.
  id: totrans-2058
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\nabla L(x^{*},y^{*})={\bf 0}_{d}$。
- en: –
  id: totrans-2059
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: $\nabla^{2}L(x^{*},y^{*})\succeq l\cdot I_{2d^{2}}$.
  id: totrans-2060
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\nabla^{2}L(x^{*},y^{*})\succeq l\cdot I_{2d^{2}}$。
- en: •
  id: totrans-2061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Good Initialization Point. Let $x_{0}$ denote the initialization point. If $r_{0}:=(\|x_{0}-x_{*}\|_{2}+\|y_{0}-y_{*}\|_{2})$
    satisfies
  id: totrans-2062
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 良好的初始化点。设 $x_{0}$ 为初始化点。如果 $r_{0}:=(\|x_{0}-x_{*}\|_{2}+\|y_{0}-y_{*}\|_{2})$
    满足
- en: '|  | $\displaystyle r_{0}M\leq 0.1l.$ |  |'
  id: totrans-2063
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{0}M\leq 0.1l.$ |  |'
- en: we say $L$-good
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称 $L$ 为 $L$-良好
- en: 'Drawing upon Lemma [6.1](#S6.Thmtheorem1 "Lemma 6.1\. ‣ 6 Hessian ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time") and Lemma [12.1](#S12.Thmtheorem1
    "Lemma 12.1\. ‣ 12.1 Main Results ‣ 12 Lipschitz for Hessian of 𝑥,𝑦 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time"), we can establish that our loss
    function (See Definition [4.7](#S4.Thmtheorem7 "Definition 4.7\. ‣ 4.2 General
    Definitions ‣ 4 Preliminary ‣ A Fast Optimization View: Reformulating Single Layer
    Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")) satisfies the aforementioned assumption.'
  id: totrans-2065
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴引理[6.1](#S6.Thmtheorem1 "引理 6.1\. ‣ 6 Hessian ‣ 一种快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内求解")和引理[12.1](#S12.Thmtheorem1
    "引理 12.1\. ‣ 12.1 主要结果 ‣ 12 Lipschitz 对 𝑥,𝑦 的 Hessian ‣ 一种快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内求解")，我们可以确定我们的损失函数（见定义[4.7](#S4.Thmtheorem7
    "定义 4.7\. ‣ 4.2 一般定义 ‣ 4 初步 ‣ 一种快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内求解")）满足上述假设。
- en: 14.2 Convergence
  id: totrans-2066
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2 收敛性
- en: 'After introducing the approximation method ’Sparsifier via TensorSketch’ in
    Section [13](#S13 "13 Generating a Spectral Sparsifier via TensorSketch ‣ A Fast
    Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time"), we will now proceed
    to introduce the update method employed in Algorithm [1](#alg1 "Algorithm 1 ‣
    6 Hessian ‣ A Fast Optimization View: Reformulating Single Layer Attention in
    LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time").
    In this section, we demonstrate the concept of approximate update and present
    an induction hypothesis.'
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[13](#S13 "13 通过 TensorSketch 生成谱稀疏化器 ‣ 一种快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内求解")节介绍了近似方法‘通过
    TensorSketch 生成稀疏化器’后，我们将介绍在算法[1](#alg1 "算法 1 ‣ 6 Hessian ‣ 一种快速优化视角：基于张量和 SVM
    技巧重构单层注意力，并在矩阵乘法时间内求解")中使用的更新方法。在本节中，我们演示了近似更新的概念，并提出了一个归纳假设。
- en: Definition 14.2  (Approximate Update).
  id: totrans-2068
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 14.2（近似更新）。
- en: The following process is considered by us
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑以下过程
- en: '|  | $$\displaystyle\begin{bmatrix}x(t+1)\\ y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\'
  id: totrans-2070
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle\begin{bmatrix}x(t+1)\\ y(t+1)\end{bmatrix}\leftarrow\begin{bmatrix}x(t)\\'
- en: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: y(t)\end{bmatrix}-\begin{bmatrix}g(x(t))\\
- en: g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ |  |
  id: totrans-2072
  prefs: []
  type: TYPE_NORMAL
  zh: $g(y(t))\end{bmatrix}\widetilde{H}^{-1}$$ |  |
- en: A tool from previous work is presented by us now.
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在介绍一个来自先前工作的工具。
- en: Lemma 14.3  (Iterative shrinking, a variation of Lemma 6.9 on page 32 of [[118](#bib.bib118)]).
  id: totrans-2074
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 14.3（迭代收缩，见 [[118](#bib.bib118)] 第 32 页的引理 6.9 的变体）。
- en: If the following conditions hold
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-2076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loss Function $L$-good (see Definition [14.1](#S14.Thmtheorem1 "Definition
    14.1 ((𝑙,𝑀)-good Loss function). ‣ 14.1 (𝑙,𝑀)-Good Loss Function ‣ 14 Analysis
    Of Algorithm 1 ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  id: totrans-2077
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 损失函数 $L$-良好（见定义[14.1](#S14.Thmtheorem1 "定义 14.1（(𝑙,𝑀)-良好损失函数）。 ‣ 14.1 (𝑙,𝑀)-良好损失函数
    ‣ 14 算法 1 的分析 ‣ 一种快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内求解")）。
- en: •
  id: totrans-2078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $\epsilon\in(0,0.1)$ (see Lemma[13.6](#S13.Thmtheorem6 "Lemma 13.6\. ‣
    13.4 Fast Approximation for Hessian via Sketching ‣ 13 Generating a Spectral Sparsifier
    via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time")).'
  id: totrans-2079
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $\epsilon\in(0,0.1)$（见引理[13.6](#S13.Thmtheorem6 "引理 13.6\. ‣ 13.4 通过绘图快速近似
    Hessian ‣ 13 通过 TensorSketch 生成谱稀疏化器 ‣ 一种快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内求解")）。
- en: •
  id: totrans-2080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $x^{*},y^{*}$ be defined in Definition[14.2](#S14.Thmtheorem2 "Definition
    14.2 (Approximate Update). ‣ 14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-2081
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $x^{*},y^{*}$ 如定义[14.2](#S14.Thmtheorem2 "定义 14.2（近似更新）。 ‣ 14.2 收敛性 ‣ 14 算法
    1 的分析 ‣ 一种快速优化视角：基于张量和 SVM 技巧重构单层注意力，并在矩阵乘法时间内求解")中定义。
- en: •
  id: totrans-2082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$.
  id: totrans-2083
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$。
- en: •
  id: totrans-2084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $\overline{r}_{t}:=M\cdot r_{t}$
  id: totrans-2085
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 令 $\overline{r}_{t}:=M\cdot r_{t}$
- en: It follows that
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '|  | $\displaystyle r_{t+1}\leq 2\cdot(\epsilon_{0}+\overline{r}_{t}/(l-\overline{r}_{t}))\cdot
    r_{t}.$ |  |'
  id: totrans-2087
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{t+1}\leq 2\cdot(\epsilon_{0}+\overline{r}_{t}/(l-\overline{r}_{t}))\cdot
    r_{t}.$ |  |'
- en: 'In this context, where $T$ denotes the total number of iterations in the algorithm,
    we require the following lemma based on the induction hypothesis to apply Lemma [14.3](#S14.Thmtheorem3
    "Lemma 14.3 (Iterative shrinking, a variation of Lemma 6.9 on page 32 of [118]).
    ‣ 14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time"). This lemma is a well-established concept in the
    literature, and for further details, you can refer to [[118](#bib.bib118)].'
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个上下文中，$T$ 表示算法中的总迭代次数，我们需要基于归纳假设的以下引理来应用引理 [14.3](#S14.Thmtheorem3 "Lemma
    14.3 (Iterative shrinking, a variation of Lemma 6.9 on page 32 of [118]). ‣ 14.2
    Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time")。这个引理是文献中的一个成熟概念，更多细节请参考 [[118](#bib.bib118)]。'
- en: Lemma 14.4  (Induction hypothesis, Lemma 6.10 on page 34 of [[118](#bib.bib118)]).
  id: totrans-2089
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 14.4  （归纳假设，引理 6.10 在 [[118](#bib.bib118)] 的第 34 页）。
- en: If the following condition hold
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足以下条件
- en: •
  id: totrans-2091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\epsilon=0.01$ (see Lemma[13.6](#S13.Thmtheorem6 "Lemma 13.6\. ‣ 13.4 Fast
    Approximation for Hessian via Sketching ‣ 13 Generating a Spectral Sparsifier
    via TensorSketch ‣ A Fast Optimization View: Reformulating Single Layer Attention
    in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication
    Time"))'
  id: totrans-2092
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\epsilon=0.01$（见引理[13.6](#S13.Thmtheorem6 "Lemma 13.6\. ‣ 13.4 Fast Approximation
    for Hessian via Sketching ‣ 13 Generating a Spectral Sparsifier via TensorSketch
    ‣ A Fast Optimization View: Reformulating Single Layer Attention in LLM Based
    on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")）'
- en: •
  id: totrans-2093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $x^{*},y^{*}$ be defined in Definition[14.2](#S14.Thmtheorem2 "Definition
    14.2 (Approximate Update). ‣ 14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A
    Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor
    and SVM Trick, and Solving It in Matrix Multiplication Time").'
  id: totrans-2094
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $x^{*},y^{*}$ 在定义[14.2](#S14.Thmtheorem2 "Definition 14.2 (Approximate Update).
    ‣ 14.2 Convergence ‣ 14 Analysis Of Algorithm 1 ‣ A Fast Optimization View: Reformulating
    Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in
    Matrix Multiplication Time") 中定义。'
- en: •
  id: totrans-2095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$.
  id: totrans-2096
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设 $r_{t}:=\|x_{t}-x^{*}\|_{2}+\|y_{t}-y^{*}\|_{2}$。
- en: •
  id: totrans-2097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For each $i\in[T]$, for all $i\in[t]$
  id: totrans-2098
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个 $i\in[T]$，对所有 $i\in[t]$
- en: •
  id: totrans-2099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Let $l$ be Defined in Definition[14.1](#S14.Thmtheorem1 "Definition 14.1 ((𝑙,𝑀)-good
    Loss function). ‣ 14.1 (𝑙,𝑀)-Good Loss Function ‣ 14 Analysis Of Algorithm 1 ‣
    A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on
    Tensor and SVM Trick, and Solving It in Matrix Multiplication Time")'
  id: totrans-2100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '设 $l$ 在定义[14.1](#S14.Thmtheorem1 "Definition 14.1 ((𝑙,𝑀)-good Loss function).
    ‣ 14.1 (𝑙,𝑀)-Good Loss Function ‣ 14 Analysis Of Algorithm 1 ‣ A Fast Optimization
    View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick,
    and Solving It in Matrix Multiplication Time") 中定义'
- en: •
  id: totrans-2101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $M\cdot r_{i}\leq 0.1l$.
  id: totrans-2102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $M\cdot r_{i}\leq 0.1l$。
- en: It follows that
  id: totrans-2103
  prefs: []
  type: TYPE_NORMAL
  zh: 由此得出
- en: •
  id: totrans-2104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $r_{t+1}\leq 0.4r_{t}$
  id: totrans-2105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $r_{t+1}\leq 0.4r_{t}$
- en: •
  id: totrans-2106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $M\cdot r_{t+1}\leq 0.1l$
  id: totrans-2107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $M\cdot r_{t+1}\leq 0.1l$
- en: References
  id: totrans-2108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'AAA^+ [23] Zaid Alyafeai, Maged S Alshaibani, Badr AlKhamissi, Hamzah Luqman,
    Ebrahim Alareqi, and Ali Fadel. Taqyim: Evaluating arabic nlp tasks using chatgpt
    models. arXiv preprint arXiv:2306.16322, 2023.'
  id: totrans-2109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AAA^+ [23] Zaid Alyafeai, Maged S Alshaibani, Badr AlKhamissi, Hamzah Luqman,
    Ebrahim Alareqi, 和 Ali Fadel. Taqyim: 使用 chatgpt 模型评估阿拉伯语 NLP 任务。arXiv 预印本 arXiv:2306.16322,
    2023。'
- en: AC [06] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the
    fast johnson-lindenstrauss transform. In Proceedings of the thirty-eighth annual
    ACM symposium on Theory of computing, pages 557–563, 2006.
  id: totrans-2110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AC [06] Nir Ailon 和 Bernard Chazelle. 近似最近邻和快速的 johnson-lindenstrauss 变换。发表于第三十八届
    ACM 理论计算机学年会议论文集，第 557–563 页，2006 年。
- en: '[3] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained
    analysis of optimization and generalization for overparameterized two-layer neural
    networks. In International Conference on Machine Learning, pages 322–332\. PMLR,
    2019.'
  id: totrans-2111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, 和 Ruosong Wang. 对过参数化两层神经网络的优化和泛化进行细粒度分析。发表于国际机器学习会议，第
    322–332 页。PMLR，2019。'
- en: '[4] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and
    Ruosong Wang. On exact computation with an infinitely wide neural net. Advances
    in neural information processing systems, 32, 2019.'
  id: totrans-2112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, 和
    Ruosong Wang. 关于无限宽神经网络的精确计算。神经信息处理系统进展，32，2019。'
- en: 'AHO^+ [23] Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita
    Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, et al.
    Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528,
    2023.'
  id: totrans-2113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AHO^+ [23] Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita
    Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali 等人。Mega：生成
    AI 的多语言评估。arXiv 预印本 arXiv:2303.12528，2023。
- en: AKK^+ [20] Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya
    Velingker, David P Woodruff, and Amir Zandieh. Oblivious sketching of high-degree
    polynomial kernels. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium
    on Discrete Algorithms, pages 141–160\. SIAM, 2020.
  id: totrans-2114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AKK^+ [20] Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya
    Velingker, David P Woodruff 和 Amir Zandieh. 高度多项式核的无意识草图。发表于《第十四届 ACM-SIAM 离散算法年会论文集》，第
    141–160 页。SIAM，2020。
- en: ALH [21] Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent
    on overparameterized nonlinear models. IEEE Transactions on Neural Networks and
    Learning Systems, 33(12):7717–7727, 2021.
  id: totrans-2115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALH [21] Navid Azizan, Sahin Lale 和 Babak Hassibi. 针对过参数化非线性模型的随机镜面下降。发表于《IEEE
    神经网络与学习系统汇刊》，33(12):7717–7727，2021。
- en: ALS^+ [18] Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi
    Zhong. Subspace embedding and linear regression with orlicz norm. In International
    Conference on Machine Learning, pages 224–233\. PMLR, 2018.
  id: totrans-2116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALS^+ [18] Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, 和 Ruiqi Zhong.
    子空间嵌入与 Orlicz 范数下的线性回归。发表于《国际机器学习会议》，第 224–233 页。PMLR，2018。
- en: 'ALS^+ [22] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, and Danyang Zhuo.
    Bypass exponential time preprocessing: Fast neural network training via weight-data
    correlation preprocessing. arXiv preprint arXiv:2211.14227, 2022.'
  id: totrans-2117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALS^+ [22] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang 和 Danyang Zhuo.
    绕过指数时间预处理：通过权重-数据相关预处理加速神经网络训练。arXiv 预印本 arXiv:2211.14227，2022。
- en: AMC^+ [23] Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury, Maram Hasanain,
    Basel Mousi, Sabri Boughorbel, Yassine El Kheir, Daniel Izham, Fahim Dalvi, Majd
    Hawasly, et al. Benchmarking arabic ai with large language models. arXiv preprint
    arXiv:2305.14982, 2023.
  id: totrans-2118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AMC^+ [23] Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury, Maram Hasanain,
    Basel Mousi, Sabri Boughorbel, Yassine El Kheir, Daniel Izham, Fahim Dalvi, Majd
    Hawasly 等人。使用大语言模型进行阿拉伯语 AI 基准测试。arXiv 预印本 arXiv:2305.14982，2023。
- en: '[11] Josh Alman and Zhao Song. Fast attention requires bounded entries. arXiv
    preprint arXiv:2302.13214, 2023.'
  id: totrans-2119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Josh Alman 和 Zhao Song. 快速注意力需要有界条目。arXiv 预印本 arXiv:2302.13214，2023。'
- en: '[12] Daman Arora, Himanshu Gaurav Singh, et al. Have llms advanced enough?
    a challenging problem solving benchmark for large language models. arXiv preprint
    arXiv:2305.15074, 2023.'
  id: totrans-2120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Daman Arora, Himanshu Gaurav Singh 等人。大型语言模型是否足够先进？针对大型语言模型的挑战性问题解决基准。arXiv
    预印本 arXiv:2305.15074，2023。'
- en: '[13] Ehsan Amid and Manfred K Warmuth. Winnowing with gradient descent. In
    Conference on Learning Theory, pages 163–182\. PMLR, 2020.'
  id: totrans-2121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ehsan Amid 和 Manfred K Warmuth. 使用梯度下降的筛选。发表于《学习理论会议》，第 163–182 页。PMLR，2020。'
- en: '[14] Ehsan Amid and Manfred KK Warmuth. Reparameterizing mirror descent as
    gradient descent. Advances in Neural Information Processing Systems, 33:8430–8439,
    2020.'
  id: totrans-2122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Ehsan Amid 和 Manfred KK Warmuth. 将镜面下降重新参数化为梯度下降。发表于《神经信息处理系统进展》，33:8430–8439，2020。'
- en: AW [21] Josh Alman and Virginia Vassilevska Williams. A refined laser method
    and faster matrix multiplication. In Proceedings of the 2021 ACM-SIAM Symposium
    on Discrete Algorithms (SODA), pages 522–539\. SIAM, 2021.
  id: totrans-2123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AW [21] Josh Alman 和 Virginia Vassilevska Williams. 精细化的激光方法与更快的矩阵乘法。发表于《2021
    年 ACM-SIAM 离散算法研讨会（SODA）》论文集，第 522–539 页。SIAM，2021。
- en: '[16] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for
    deep learning via over-parameterization. In International conference on machine
    learning, pages 242–252\. PMLR, 2019.'
  id: totrans-2124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Zeyuan Allen-Zhu, Yuanzhi Li 和 Zhao Song. 通过过参数化的深度学习收敛理论。发表于《国际机器学习会议》，第
    242–252 页。PMLR，2019。'
- en: '[17] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of
    training recurrent neural networks. Advances in neural information processing
    systems, 32, 2019.'
  id: totrans-2125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Zeyuan Allen-Zhu, Yuanzhi Li 和 Zhao Song. 循环神经网络训练的收敛速度。发表于《神经信息处理系统进展》，32，2019。'
- en: BCB [14] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine
    translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473,
    2014.
  id: totrans-2126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BCB [14] Dzmitry Bahdanau, Kyunghyun Cho 和 Yoshua Bengio. 通过共同学习对齐和翻译进行神经机器翻译。arXiv
    预印本 arXiv:1409.0473，2014。
- en: 'BCE^+ [23] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,
    Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  id: totrans-2127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BCE^+ [23] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,
    Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg，等。人工通用智能的火花：关于gpt-4的早期实验。arXiv
    预印本 arXiv:2303.12712，2023年。
- en: BCL^+ [23] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su,
    Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask,
    multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and
    interactivity. arXiv preprint arXiv:2302.04023, 2023.
  id: totrans-2128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BCL^+ [23] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su,
    Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung，等。对ChatGPT在推理、幻觉和互动性的多任务、多语言、多模态评估。arXiv
    预印本 arXiv:2302.04023，2023年。
- en: BCS [97] Peter Bürgisser, Michael Clausen, and Mohammad A Shokrollahi. Algebraic
    complexity theory, volume 315. Springer Science & Business Media, 1997.
  id: totrans-2129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BCS [97] Peter Bürgisser, Michael Clausen, 和 Mohammad A Shokrollahi。代数复杂性理论，第315卷。Springer
    Science & Business Media，1997年。
- en: BGVV [20] Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit
    regularization for deep neural networks driven by an ornstein-uhlenbeck like process.
    In Conference on learning theory, pages 483–513\. PMLR, 2020.
  id: totrans-2130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BGVV [20] Guy Blanc, Neha Gupta, Gregory Valiant, 和 Paul Valiant。由奥恩斯坦-乌伦贝克过程驱动的深度神经网络的隐式正则化。载于学习理论会议论文集，第483–513页。PMLR，2020年。
- en: 'BHS^+ [23] Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and Ben He.
    Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense
    problem in large language models. arXiv preprint arXiv:2303.16421, 2023.'
  id: totrans-2131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BHS^+ [23] Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, 和 Ben He。Chatgpt是一个知识丰富但经验不足的求解者：对大型语言模型中常识问题的调查。arXiv
    预印本 arXiv:2303.16421，2023年。
- en: BMR^+ [20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.
  id: totrans-2132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BMR^+ [20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell，等。语言模型是少样本学习者。神经信息处理系统进展，33:1877–1901，2020年。
- en: BPSW [20] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training
    (overparametrized) neural networks in near-linear time. arXiv preprint arXiv:2006.11648,
    2020.
  id: totrans-2133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BPSW [20] Jan van den Brand, Binghui Peng, Zhao Song, 和 Omri Weinstein。在近线性时间内训练（过参数化的）神经网络。arXiv
    预印本 arXiv:2006.11648，2020年。
- en: Bra [20] Jan van den Brand. A deterministic linear program solver in current
    matrix multiplication time. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium
    on Discrete Algorithms (SODA), pages 259–278\. SIAM, 2020.
  id: totrans-2134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bra [20] Jan van den Brand。在当前矩阵乘法时间内的确定性线性程序求解器。载于第十四届ACM-SIAM离散算法年会论文集（SODA），第259–278页。SIAM，2020年。
- en: BS [23] Jan den van Brand and Zhao Song. A $\sqrt{n}$ passes streaming algorithm
    for solving bipartite matching exactly. Manuscript, 2023.
  id: totrans-2135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BS [23] Jan den van Brand 和 Zhao Song。一个$\sqrt{n}$次流式算法用于精确求解二分匹配。手稿，2023年。
- en: BSY [23] Song Bian, Zhao Song, and Junze Yin. Federated empirical risk minimization
    via second-order method. arXiv preprint arXiv:2305.17482, 2023.
  id: totrans-2136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BSY [23] Song Bian, Zhao Song, 和 Junze Yin。通过二阶方法进行联邦经验风险最小化。arXiv 预印本 arXiv:2305.17482，2023年。
- en: BSZ [23] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness
    for dynamic attention maintenance in large language models. arXiv preprint arXiv:2304.02207,
    2023.
  id: totrans-2137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BSZ [23] Jan van den Brand, Zhao Song, 和 Tianyi Zhou。大型语言模型中动态注意力维护的算法与难度。arXiv
    预印本 arXiv:2304.02207，2023年。
- en: BW [14] Christos Boutsidis and David P Woodruff. Optimal cur matrix decompositions.
    In Proceedings of the forty-sixth annual ACM symposium on Theory of computing
    (STOC), pages 353–362, 2014.
  id: totrans-2138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BW [14] Christos Boutsidis 和 David P Woodruff。最佳CUR矩阵分解。载于第46届ACM计算理论年会论文集（STOC），第353–362页，2014年。
- en: BWZ [16] Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal
    component analysis in distributed and streaming models. In Proceedings of the
    forty-eighth annual ACM symposium on Theory of Computing, pages 236–249, 2016.
  id: totrans-2139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BWZ [16] Christos Boutsidis, David P Woodruff, 和 Peilin Zhong。在分布式和流模型中进行最佳主成分分析。载于第48届ACM计算理论年会论文集，第236–249页，2016年。
- en: BYKS [22] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering
    latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827,
    2022.
  id: totrans-2140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BYKS [22] Collin Burns, Haotian Ye, Dan Klein, 和 Jacob Steinhardt。无监督下发现语言模型中的潜在知识。arXiv
    预印本 arXiv:2212.03827，2022年。
- en: 'CGH^+ [19] Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua
    Zhang, and Liwei Wang. Gram-gauss-newton method: Learning overparameterized neural
    networks for regression problems. arXiv preprint arXiv:1905.11675, 2019.'
  id: totrans-2141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CGH^+ [19] Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua
    Zhang 和 Liwei Wang. Gram-Gauss-Newton 方法：学习过参数化神经网络以解决回归问题。arXiv 预印本 arXiv:1905.11675,
    2019。
- en: CGLZ [20] Matthias Christandl, François Le Gall, Vladimir Lysikov, and Jeroen
    Zuiddam. Barriers for rectangular matrix multiplication. In arXiv preprint, 2020.
  id: totrans-2142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CGLZ [20] Matthias Christandl, François Le Gall, Vladimir Lysikov 和 Jeroen Zuiddam.
    矩形矩阵乘法的障碍。发表于 arXiv 预印本，2020。
- en: Cha [22] ChatGPT. Optimizing language models for dialogue. OpenAI Blog, November
    2022.
  id: totrans-2143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cha [22] ChatGPT. 优化对话的语言模型。OpenAI 博客，2022年11月。
- en: 'CHBP [23] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval:
    Towards holistic evaluation of instruction-tuned large language models. arXiv
    preprint arXiv:2306.04757, 2023.'
  id: totrans-2144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CHBP [23] Yew Ken Chia, Pengfei Hong, Lidong Bing 和 Soujanya Poria. Instructeval：朝着整体评估指令调整的大型语言模型迈进。arXiv
    预印本 arXiv:2306.04757, 2023。
- en: 'CL [01] Chih-Chung Chang and Chih-Jen Lin. Training v-support vector classifiers:
    theory and algorithms. Neural computation, 13(9):2119–2147, 2001.'
  id: totrans-2145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CL [01] Chih-Chung Chang 和 Chih-Jen Lin. 训练 v-支持向量分类器：理论与算法。神经计算，13(9):2119–2147,
    2001。
- en: 'CLBBJ [23] Joseph Chervenak, Harry Lieman, Miranda Blanco-Breindel, and Sangita
    Jindal. The promise and peril of using a large language model to obtain clinical
    information: Chatgpt performs strongly as a fertility counseling tool with limitations.
    Fertility and Sterility, 2023.'
  id: totrans-2146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLBBJ [23] Joseph Chervenak, Harry Lieman, Miranda Blanco-Breindel 和 Sangita
    Jindal. 使用大型语言模型获取临床信息的承诺与风险：ChatGPT 作为生育咨询工具表现出色但有限。生育与不孕期刊，2023。
- en: CLMY [21] HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order
    block coordinate descent algorithm for huge-scale black-box optimization. In International
    Conference on Machine Learning, pages 1193–1203\. PMLR, 2021.
  id: totrans-2147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLMY [21] HanQin Cai, Yuchen Lou, Daniel McKenzie 和 Wotao Yin. 用于大规模黑箱优化的零阶块坐标下降算法。发表于国际机器学习会议，页面
    1193–1203\. PMLR, 2021。
- en: 'CLP^+ [21] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie
    Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A
    learnable lsh framework for efficient neural network training. In International
    Conference on Learning Representations, 2021.'
  id: totrans-2148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLP^+ [21] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie
    Li, Tri Dao, Zhao Song, Anshumali Shrivastava 和 Christopher Re. Mongoose：一个可学习的
    LSH 框架，用于高效的神经网络训练。发表于国际学习表征会议，2021。
- en: CLS [19] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs
    in the current matrix multiplication time. In STOC, 2019.
  id: totrans-2149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLS [19] Michael B Cohen, Yin Tat Lee 和 Zhao Song. 在当前矩阵乘法时间内求解线性程序。发表于 STOC,
    2019。
- en: CNP [23] Cayque Monteiro Castro Nascimento and André Silva Pimentel. Do large
    language models understand chemistry? a conversation with chatgpt. Journal of
    Chemical Information and Modeling, 63(6):1649–1655, 2023.
  id: totrans-2150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNP [23] Cayque Monteiro Castro Nascimento 和 André Silva Pimentel. 大型语言模型是否理解化学？与
    ChatGPT 的对话。化学信息与建模期刊，63(6):1649–1655, 2023。
- en: Coh [16] Michael B Cohen. Nearly tight oblivious subspace embeddings by trace
    inequalities. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on
    Discrete algorithms, pages 278–287\. SIAM, 2016.
  id: totrans-2151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coh [16] Michael B Cohen. 通过迹不等式获得几乎紧凑的盲目子空间嵌入。发表于第二十七届 ACM-SIAM 离散算法年会，页面 278–287\.
    SIAM, 2016。
- en: Cop [82] Don Coppersmith. Rapid multiplication of rectangular matrices. SIAM
    Journal on Computing, 11(3):467–471, 1982.
  id: totrans-2152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cop [82] Don Coppersmith. 矩形矩阵的快速乘法。SIAM 计算期刊，11(3):467–471, 1982。
- en: CPK^+ [23] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens.
    Do llms understand social knowledge? evaluating the sociability of large language
    models with socket benchmark. arXiv preprint arXiv:2305.14938, 2023.
  id: totrans-2153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPK^+ [23] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu 和 David Jurgens. 大型语言模型是否理解社会知识？通过
    Socket 基准评估大型语言模型的社交能力。arXiv 预印本 arXiv:2305.14938, 2023。
- en: CW [13] Kenneth L Clarkson and David P Woodruff. Low-rank approximation and
    regression in input sparsity time. In STOC, 2013.
  id: totrans-2154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CW [13] Kenneth L Clarkson 和 David P Woodruff. 输入稀疏时间中的低秩近似与回归。发表于 STOC, 2013。
- en: 'CWJ^+ [23] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring
    the use of large language models for reference-free text quality evaluation: A
    preliminary empirical study. arXiv preprint arXiv:2304.00723, 2023.'
  id: totrans-2155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CWJ^+ [23] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi 和 Ruifeng Xu. 探索大型语言模型在无参考文本质量评估中的应用：初步实证研究。arXiv
    预印本 arXiv:2304.00723, 2023。
- en: 'CZL^+ [23] Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel
    Hershcovich. Assessing cross-cultural alignment between chatgpt and human societies:
    An empirical study. arXiv preprint arXiv:2303.17466, 2023.'
  id: totrans-2156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CZL^+ [23] 曹勇、周力、李瑶华、劳拉·卡贝洛、陈敏和丹尼尔·赫什科维奇。评估 ChatGPT 和人类社会之间的跨文化对齐：一项实证研究。arXiv
    预印本 arXiv:2303.17466，2023年。
- en: 'DCLT [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    Bert: Pre-training of deep bidirectional transformers for language understanding.
    arXiv preprint arXiv:1810.04805, 2018.'
  id: totrans-2157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DCLT [18] 雅各布·德夫林、张敏伟、李肯顿和克里斯蒂娜·托塔诺娃。BERT：用于语言理解的深度双向变换器的预训练。arXiv 预印本 arXiv:1810.04805，2018年。
- en: DDH^+ [21] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu
    Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696,
    2021.
  id: totrans-2158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDH^+ [21] 戴达迈、董丽、郝雅如、隋志芳、常宝宝和魏伏如。预训练变换器中的知识神经元。arXiv 预印本 arXiv:2104.08696，2021年。
- en: DGG [23] Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. How ready are
    pre-trained abstractive models and llms for legal case judgement summarization?
    arXiv preprint arXiv:2306.01248, 2023.
  id: totrans-2159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DGG [23] 阿尼凯特·德罗伊、克里帕班德胡·戈什和萨普塔尔希·戈什。预训练抽象模型和大型语言模型在法律案件判决总结中的准备程度如何？arXiv 预印本
    arXiv:2306.01248，2023年。
- en: DJS^+ [19] Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff.
    Optimal sketching for kronecker product regression and low rank approximation.
    Advances in neural information processing systems, 32, 2019.
  id: totrans-2160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DJS^+ [19] 迪昂·华、拉杰什·贾亚拉姆、宋钊、孙雯和大卫·伍德鲁夫。克罗内克乘积回归和低秩逼近的最优草图。神经信息处理系统进展，32，2019年。
- en: 'DL [23] Xuan-Quy Dao and Ngoc-Bich Le. Investigating the effectiveness of chatgpt
    in mathematical reasoning and problem solving: Evidence from the vietnamese national
    high school graduation examination. arXiv preprint arXiv:2306.06331, 2023.'
  id: totrans-2161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL [23] 阮久奇和黎玉碧。研究 ChatGPT 在数学推理和问题解决中的有效性：来自越南国家高中毕业考试的证据。arXiv 预印本 arXiv:2306.06331，2023年。
- en: DLMS [23] Yichuan Deng, Zhihang Li, Sridhar Mahadevan, and Zhao Song. Zero-th
    order algorithm for softmax attention optimization. arXiv preprint arXiv:2307.08352,
    2023.
  id: totrans-2162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLMS [23] 邓一川、李智航、斯里达·马哈德万和宋钊。用于软最大注意力优化的零阶算法。arXiv 预印本 arXiv:2307.08352，2023年。
- en: DLS [23] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired
    softmax regression. arXiv preprint arXiv:2304.10411, 2023.
  id: totrans-2163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLS [23] 邓一川、李智航和宋钊。受启发的注意力机制软最大回归。arXiv 预印本 arXiv:2304.10411，2023年。
- en: DML [21] Alex Damian, Tengyu Ma, and Jason D Lee. Label noise sgd provably prefers
    flat global minimizers. Advances in Neural Information Processing Systems, 34:27449–27461,
    2021.
  id: totrans-2164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DML [21] 亚历克斯·达米安、滕昱马和杰森·D·李。标签噪声 SGD 可证明偏好平坦的全局最小值。神经信息处理系统进展，34:27449–27461，2021年。
- en: DMS [23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic
    attention sparsification algorithms for over-parameterized feature dimension.
    arXiv preprint arXiv:2304.04397, 2023.
  id: totrans-2165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DMS [23] 邓一川、斯里达·马哈德万和宋钊。用于过参数特征维度的随机和确定性注意力稀疏化算法。arXiv 预印本 arXiv:2304.04397，2023年。
- en: DSSW [18] Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for
    kronecker product regression and p-splines. In International Conference on Artificial
    Intelligence and Statistics, pages 1299–1308\. PMLR, 2018.
  id: totrans-2166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSSW [18] 迪昂·华、宋钊、孙雯和大卫·伍德鲁夫。用于克罗内克乘积回归和 p-splines 的草图。在国际人工智能与统计会议上，1299–1308页。PMLR，2018年。
- en: DSWY [22] Yichuan Deng, Zhao Song, Yitan Wang, and Yuanyuan Yang. A nearly optimal
    size coreset algorithm with nearly linear time. arXiv preprint arXiv:2210.08361,
    2022.
  id: totrans-2167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSWY [22] 邓一川、宋钊、王怡坦和杨媛媛。几乎最优大小的核心集算法，时间复杂度接近线性。arXiv 预印本 arXiv:2210.08361，2022年。
- en: DSY [23] Yichuan Deng, Zhao Song, and Junze Yin. Faster robust tensor power
    method for arbitrary order. arXiv preprint arXiv:2306.00406, 2023.
  id: totrans-2168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSY [23] 邓一川、宋钊和尹军泽。用于任意阶的更快鲁棒张量幂方法。arXiv 预印本 arXiv:2306.00406，2023年。
- en: DSZZ [23] Yichuan Deng, Zhao Song, Lichen Zhang, and Ruizhe Zhang. Efficient
    algorithm for solving hyperbolic programs. arXiv preprint arXiv:2306.07587, 2023.
  id: totrans-2169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSZZ [23] 邓一川、宋钊、张立臣和张瑞哲。求解双曲程序的高效算法。arXiv 预印本 arXiv:2306.07587，2023年。
- en: DWZ [23] Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication
    via asymmetric hashing. In FOCS, 2023.
  id: totrans-2170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DWZ [23] 段冉、吴宏勋和周任飞。通过非对称哈希加速矩阵乘法。在 FOCS，2023年。
- en: DZPS [18] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient
    descent provably optimizes over-parameterized neural networks. arXiv preprint
    arXiv:1810.02054, 2018.
  id: totrans-2171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DZPS [18] 西蒙·S·杜、翟熙宇、巴纳巴斯·波科斯和阿尔蒂·辛格。梯度下降可证明优化过参数神经网络。arXiv 预印本 arXiv:1810.02054，2018年。
- en: EMZ [21] Hossein Esfandiari, Vahab Mirrokni, and Peilin Zhong. Almost linear
    time density level set estimation via dbscan. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 35, pages 7349–7357, 2021.
  id: totrans-2172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMZ [21] Hossein Esfandiari, Vahab Mirrokni, 和 Peilin Zhong. 通过 DBSCAN 进行近似线性时间密度水平集估计.
    见于 AAAI 人工智能会议论文集, 第 35 卷, 页码 7349–7357, 2021.
- en: Fer [23] Emilio Ferrara. Should chatgpt be biased? challenges and risks of bias
    in large language models. arXiv preprint arXiv:2304.03738, 2023.
  id: totrans-2173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fer [23] Emilio Ferrara. ChatGPT 是否应有偏见？大型语言模型中的偏见挑战与风险. arXiv 预印本 arXiv:2304.03738,
    2023.
- en: Fra [23] Michael C Frank. Baby steps in evaluating the capacities of large language
    models. Nature Reviews Psychology, pages 1–2, 2023.
  id: totrans-2174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fra [23] Michael C Frank. 评估大型语言模型能力的初步步骤. 《自然评论心理学》, 页码 1–2, 2023.
- en: GGL^+ [23] Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, Nitesh V Chawla,
    Olaf Wiest, Xiangliang Zhang, et al. What indeed can gpt models do in chemistry?
    a comprehensive benchmark on eight tasks. arXiv preprint arXiv:2305.18365, 2023.
  id: totrans-2175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GGL^+ [23] Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, Nitesh V Chawla,
    Olaf Wiest, Xiangliang Zhang, 等. GPT 模型在化学中究竟能做什么？八项任务的综合基准测试. arXiv 预印本 arXiv:2305.18365,
    2023.
- en: GLSS [18] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing
    implicit bias in terms of optimization geometry. In International Conference on
    Machine Learning, pages 1832–1841\. PMLR, 2018.
  id: totrans-2176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GLSS [18] Suriya Gunasekar, Jason Lee, Daniel Soudry, 和 Nathan Srebro. 从优化几何的角度描述隐式偏见.
    见于国际机器学习会议, 页码 1832–1841. PMLR, 2018.
- en: GMS [23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential
    regression. arXiv preprint arXiv:2303.16504, 2023.
  id: totrans-2177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMS [23] Yeqi Gao, Sridhar Mahadevan, 和 Zhao Song. 一种过参数化的指数回归. arXiv 预印本 arXiv:2303.16504,
    2023.
- en: GQSW [22] Yeqi Gao, Lianke Qin, Zhao Song, and Yitan Wang. A sublinear adversarial
    training algorithm. arXiv preprint arXiv:2208.05395, 2022.
  id: totrans-2178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GQSW [22] Yeqi Gao, Lianke Qin, Zhao Song, 和 Yitan Wang. 一种次线性的对抗训练算法. arXiv
    预印本 arXiv:2208.05395, 2022.
- en: GS [22] Yuzhou Gu and Zhao Song. A faster small treewidth sdp solver. arXiv
    preprint arXiv:2211.06033, 2022.
  id: totrans-2179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GS [22] Yuzhou Gu 和 Zhao Song. 更快的小树宽 SDP 求解器. arXiv 预印本 arXiv:2211.06033, 2022.
- en: 'GSX [23] Yeqi Gao, Zhao Song, and Shenghao Xie. In-context learning for attention
    scheme: from single softmax regression to multiple softmax regression via a tensor
    trick. arXiv preprint arXiv:2307.02419, 2023.'
  id: totrans-2180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GSX [23] Yeqi Gao, Zhao Song, 和 Shenghao Xie. 注意力机制的上下文学习：从单一 softmax 回归到通过张量技巧的多重
    softmax 回归. arXiv 预印本 arXiv:2307.02419, 2023.
- en: '[73] Yeqi Gao, Zhao Song, and Junze Yin. Gradientcoin: A peer-to-peer decentralized
    large language models. arXiv preprint arXiv:2308.10502, 2023.'
  id: totrans-2181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Yeqi Gao, Zhao Song, 和 Junze Yin. Gradientcoin: 一个点对点去中心化的大型语言模型. arXiv
    预印本 arXiv:2308.10502, 2023.'
- en: '[74] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled
    hyperbolic functions regression. arXiv preprint arXiv:2305.00660, 2023.'
  id: totrans-2182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Yeqi Gao, Zhao Song, 和 Junze Yin. 一种用于缩放双曲函数回归的迭代算法. arXiv 预印本 arXiv:2305.00660,
    2023.'
- en: '[75] Yeqi Gao, Zhao Song, Xin Yang, and Ruizhe Zhang. Fast quantum algorithm
    for attention computation. arXiv preprint arXiv:2307.08045, 2023.'
  id: totrans-2183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Yeqi Gao, Zhao Song, Xin Yang, 和 Ruizhe Zhang. 快速量子算法用于注意力计算. arXiv 预印本
    arXiv:2307.08045, 2023.'
- en: '[76] Yuzhou Gu, Zhao Song, Junze Yin, and Lichen Zhang. Low rank matrix completion
    via robust alternating minimization in nearly linear time. arXiv preprint arXiv:2302.11068,
    2023.'
  id: totrans-2184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Yuzhou Gu, Zhao Song, Junze Yin, 和 Lichen Zhang. 通过强健的交替最小化在近线性时间内完成低秩矩阵.
    arXiv 预印本 arXiv:2302.11068, 2023.'
- en: GSZ [23] Yuzhou Gu, Zhao Song, and Lichen Zhang. A nearly-linear time algorithm
    for structured support vector machines. arXiv preprint arXiv:2307.07735, 2023.
  id: totrans-2185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GSZ [23] Yuzhou Gu, Zhao Song, 和 Lichen Zhang. 一种用于结构支持向量机的近线性时间算法. arXiv 预印本
    arXiv:2307.07735, 2023.
- en: GU [18] François Le Gall and Florent Urrutia. Improved rectangular matrix multiplication
    using powers of the coppersmith-winograd tensor. In Proceedings of the Twenty-Ninth
    Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2018.
  id: totrans-2186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GU [18] François Le Gall 和 Florent Urrutia. 使用 Coppersmith-Winograd 张量的幂改进的矩形矩阵乘法.
    见于第二十九届年度 ACM-SIAM 离散算法研讨会 (SODA) 论文集, 2018.
- en: HBKG [23] Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization
    inform editing? surprising differences in causality-based localization vs. knowledge
    editing in language models. arXiv preprint arXiv:2301.04213, 2023.
  id: totrans-2187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBKG [23] Peter Hase, Mohit Bansal, Been Kim, 和 Asma Ghandeharioun. 本地化是否有助于编辑？在语言模型中基于因果关系的本地化与知识编辑之间的惊人差异.
    arXiv 预印本 arXiv:2301.04213, 2023.
- en: HF [23] Thilo Hagendorff and Sarah Fabi. Human-like intuitive behavior and reasoning
    biases emerged in language models–and disappeared in gpt-4. arXiv preprint arXiv:2306.07622,
    2023.
  id: totrans-2188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HF [23] Thilo Hagendorff 和 Sarah Fabi. 人类般的直观行为和推理偏差在语言模型中出现，并在 GPT-4 中消失. arXiv
    预印本 arXiv:2306.07622, 2023.
- en: 'HJS^+ [22] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang.
    Solving sdp faster: A robust ipm framework and efficient implementation. In 2022
    IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 233–244\.
    IEEE, 2022.'
  id: totrans-2189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HJS^+ [22] 白鹤·黄，蒋顺华，赵松，润洲·陶，和瑞哲·张。《更快地解决SDP：一个强健的IPM框架和高效实现》。在2022年IEEE第63届计算机科学基础年会（FOCS），页码233–244。IEEE，2022年。
- en: 'HLSY [21] Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural
    tangent kernel-based framework for federated learning analysis. In International
    Conference on Machine Learning, pages 4423–4434\. PMLR, 2021.'
  id: totrans-2190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HLSY [21] 白鹤·黄，肖肖·李，赵松，和辛·杨。《FL-NTK：一个基于神经切线核的联邦学习分析框架》。在国际机器学习会议论文集，页码4423–4434。PMLR，2021年。
- en: HLZ [23] Sophie Huiberts, Yin Tat Lee, and Xinzhi Zhang. Upper and lower bounds
    on the smoothed complexity of the simplex method. In Proceedings of the 55th Annual
    ACM Symposium on Theory of Computing, pages 1904–1917, 2023.
  id: totrans-2191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HLZ [23] 索菲·霍伊伯茨，尹·塔特·李，和辛智·张。《单纯形法平滑复杂度的上下界》。在第55届ACM理论计算研讨会论文集，页码1904–1917，2023年。
- en: 'HM [19] John Hewitt and Christopher D Manning. A structural probe for finding
    syntax in word representations. In Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, 2019.'
  id: totrans-2192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HM [19] 约翰·休伊特和克里斯托弗·D·曼宁。《寻找词表示中的句法结构的结构探测器》。在2019年北美计算语言学协会人类语言技术会议论文集第1卷（长篇和短篇论文），页码4129–4138，2019年。
- en: 'HWLM [21] Jeff Z HaoChen, Colin Wei, Jason Lee, and Tengyu Ma. Shape matters:
    Understanding the implicit bias of the noise covariance. In Conference on Learning
    Theory, pages 2315–2357\. PMLR, 2021.'
  id: totrans-2193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HWLM [21] 杰夫·Z·郝晨，科林·韦，杰森·李，和滕宇·马。《形状很重要：理解噪声协方差的隐含偏差》。在学习理论会议，页码2315–2357。PMLR，2021年。
- en: JDST [20] Ziwei Ji, Miroslav Dudik, Robert E Schapire, and Matus Telgarsky.
    Gradient descent follows the regularization path for general losses. In Conference
    on Learning Theory, pages 2109–2136\. PMLR, 2020.
  id: totrans-2194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDST [20] 纪子伟，米罗斯拉夫·杜迪克，罗伯特·E·沙皮尔，和马图斯·特尔加尔斯基。《梯度下降遵循一般损失的正则化路径》。在学习理论会议，页码2109–2136。PMLR，2020年。
- en: 'JGP^+ [23] Douglas Johnson, Rachel Goodman, J Patrinely, Cosby Stone, Eli Zimmerman,
    Rebecca Donald, Sam Chang, Sean Berkowitz, Avni Finn, Eiman Jahangir, et al. Assessing
    the accuracy and reliability of ai-generated medical responses: an evaluation
    of the chat-gpt model. ., 2023.'
  id: totrans-2195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JGP^+ [23] 道格拉斯·约翰逊，瑞秋·古德曼，J·帕特里内利，科斯比·斯通，埃利·齐默曼，丽贝卡·唐纳德，萨姆·张，肖恩·伯科维茨，阿夫尼·芬，艾曼·贾汉吉尔等。《评估AI生成的医疗回应的准确性和可靠性：对Chat-GPT模型的评估》。2023年。
- en: JKL^+ [20] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and
    Zhao Song. A faster interior point method for semidefinite programming. In 2020
    IEEE 61st annual symposium on foundations of computer science (FOCS), pages 910–918\.
    IEEE, 2020.
  id: totrans-2196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JKL^+ [20] 乔浩天，塔伦·卡图里亚，尹·塔特·李，斯瓦蒂·帕德马纳班，和赵松。《一种更快的半正定规划内部点法》。在2020年IEEE第61届计算机科学基础年会（FOCS），页码910–918。IEEE，2020年。
- en: JL [84] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings
    into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.
  id: totrans-2197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JL [84] 威廉·B·约翰逊和约拉姆·林登斯特劳斯。《映射到希尔伯特空间的利普希茨映射的扩展》。现代数学，26(189-206):1，1984年。
- en: JLSW [20] Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved
    cutting plane method for convex optimization, convex-concave games, and its applications.
    In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,
    pages 944–953, 2020.
  id: totrans-2198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JLSW [20] 乔浩天，尹·塔特·李，赵松，和萨姆·邱伟·黄。《用于凸优化、凸凹游戏及其应用的改进切割平面法》。在第52届ACM SIGACT理论计算研讨会论文集，页码944–953，2020年。
- en: Joa [06] Thorsten Joachims. Training linear svms in linear time. In Proceedings
    of the 12th ACM SIGKDD international conference on Knowledge discovery and data
    mining, pages 217–226, 2006.
  id: totrans-2199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joa [06] 托尔斯滕·乔阿希姆斯。《在线性时间内训练线性支持向量机》。在第12届ACM SIGKDD国际知识发现与数据挖掘会议论文集，页码217–226，2006年。
- en: JST [21] Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization
    via dual acceleration. In International Conference on Machine Learning, pages
    4860–4869\. PMLR, 2021.
  id: totrans-2200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JST [21] 纪子伟，内森·斯雷布罗，和马图斯·特尔加尔斯基。《通过双重加速实现快速边际最大化》。在国际机器学习会议论文集，页码4860–4869。PMLR，2021年。
- en: JSWZ [21] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster
    algorithm for solving general lps. In Proceedings of the 53rd Annual ACM SIGACT
    Symposium on Theory of Computing, pages 823–832, 2021.
  id: totrans-2201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSWZ [21] 蒋顺华，赵松，奥姆里·温斯坦，和恒杰·张。《解决一般LPS的更快算法》。在第53届ACM SIGACT理论计算研讨会论文集，页码823–832，2021年。
- en: JT [18] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic
    regression. arXiv preprint arXiv:1803.07300, 2018.
  id: totrans-2202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JT [18] Ziwei Ji 和 Matus Telgarsky. 逻辑回归的风险和参数收敛。arXiv 预印本 arXiv:1803.07300，2018年。
- en: '[95] Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on
    nonseparable data. In Conference on Learning Theory, pages 1772–1798\. PMLR, 2019.'
  id: totrans-2203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Ziwei Ji 和 Matus Telgarsky. 梯度下降在不可分数据上的隐含偏差。收录于学习理论会议，第1772–1798页，PMLR，2019年。'
- en: '[96] Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient
    descent to achieve arbitrarily small test error with shallow relu networks. arXiv
    preprint arXiv:1909.12292, 2019.'
  id: totrans-2204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Ziwei Ji 和 Matus Telgarsky. 多对数宽度足以使梯度下降在浅层 relu 网络中实现任意小的测试误差。arXiv 预印本
    arXiv:1909.12292，2019年。'
- en: JT [20] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment
    in deep learning. Advances in Neural Information Processing Systems, 33:17176–17186,
    2020.
  id: totrans-2205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JT [20] Ziwei Ji 和 Matus Telgarsky. 深度学习中的方向性收敛与对齐。收录于《神经信息处理系统进展》，33:17176–17186，2020年。
- en: JT [21] Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a
    primal-dual analysis. In Algorithmic Learning Theory, pages 772–804\. PMLR, 2021.
  id: totrans-2206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JT [21] Ziwei Ji 和 Matus Telgarsky. 通过原始-对偶分析刻画隐含偏差。收录于《算法学习理论》，第772–804页，PMLR，2021年。
- en: 'KKL [20] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer. arXiv preprint arXiv:2001.04451, 2020.'
  id: totrans-2207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KKL [20] Nikita Kitaev, Łukasz Kaiser, 和 Anselm Levskaya. Reformer：高效的 transformer。arXiv
    预印本 arXiv:2001.04451，2020年。
- en: KMH^+ [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling
    laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
  id: totrans-2208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KMH^+ [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和 Dario Amodei. 神经语言模型的规模定律。arXiv
    预印本 arXiv:2001.08361，2020年。
- en: KPOT [21] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos
    Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization.
    Advances in Neural Information Processing Systems, 34:18970–18983, 2021.
  id: totrans-2209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KPOT [21] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, 和 Christos
    Thrampoulidis. 在过参数化下的标签不平衡和群体敏感分类。收录于《神经信息处理系统进展》，34:18970–18983，2021年。
- en: LBL^+ [22] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
    et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110,
    2022.
  id: totrans-2210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LBL^+ [22] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar
    等人。语言模型的整体评估。arXiv 预印本 arXiv:2211.09110，2022年。
- en: LBR^+ [23] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen
    Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. A systematic study and comprehensive
    evaluation of chatgpt on benchmark datasets. arXiv preprint arXiv:2305.18486,
    2023.
  id: totrans-2211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LBR^+ [23] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran
    Hossen Bhuiyan, Shafiq Joty, 和 Jimmy Xiangji Huang. 对 ChatGPT 在基准数据集上的系统研究和综合评估。arXiv
    预印本 arXiv:2305.18486，2023年。
- en: LDFU [13] Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster
    ridge regression via the subsampled randomized hadamard transform. Advances in
    neural information processing systems, 26, 2013.
  id: totrans-2212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDFU [13] Yichao Lu, Paramveer Dhillon, Dean P Foster, 和 Lyle Ungar. 通过子样本随机
    Hadamard 变换加速岭回归。收录于《神经信息处理系统进展》，26，2013年。
- en: LG [14] François Le Gall. Powers of tensors and fast matrix multiplication.
    In Proceedings of the 39th international symposium on symbolic and algebraic computation,
    pages 296–303, 2014.
  id: totrans-2213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LG [14] François Le Gall. 张量的幂与快速矩阵乘法。收录于第39届国际符号与代数计算研讨会论文集，第296–303页，2014年。
- en: LG [23] François Le Gall. Faster rectangular matrix multiplication by combination
    loss analysis. arXiv preprint arXiv:2307.06535, 2023.
  id: totrans-2214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LG [23] François Le Gall. 通过组合损失分析加速矩形矩阵乘法。arXiv 预印本 arXiv:2307.06535，2023年。
- en: LL [18] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks
    via stochastic gradient descent on structured data. Advances in neural information
    processing systems, 31, 2018.
  id: totrans-2215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LL [18] Yuanzhi Li 和 Yingyu Liang. 通过对结构化数据进行随机梯度下降来学习过参数化的神经网络。收录于《神经信息处理系统进展》，31，2018年。
- en: 'LL [21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous
    prompts for generation. arXiv preprint arXiv:2101.00190, 2021.'
  id: totrans-2216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LL [21] Xiang Lisa Li 和 Percy Liang. 前缀调优：优化生成的连续提示。arXiv 预印本 arXiv:2101.00190，2021年。
- en: LLGB [23] Xinzhe Li, Ming Liu, Shang Gao, and Wray Buntine. A survey on out-of-distribution
    evaluation of neural nlp models. arXiv preprint arXiv:2306.15261, 2023.
  id: totrans-2217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLGB [23] Xinzhe Li, Ming Liu, Shang Gao, 和 Wray Buntine. 神经 NLP 模型的分布外评估调查。arXiv
    预印本 arXiv:2306.15261，2023年。
- en: 'LLH^+ [23] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia:
    A scalable stochastic second-order optimizer for language model pre-training.
    arXiv preprint arXiv:2305.14342, 2023.'
  id: totrans-2218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLH^+ [23] 洪·刘，志远·李，大卫·霍尔，佩尔西·梁，和滕宇·马。Sophia：一种可扩展的随机二阶优化器，用于语言模型预训练。arXiv预印本
    arXiv:2305.14342, 2023。
- en: 'LLR [23] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn
    topic structure: Towards a mechanistic understanding. arXiv preprint arXiv:2303.04245,
    2023.'
  id: totrans-2219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLR [23] 郁辰·李，袁智·李，和安德烈·里斯特斯基。变压器如何学习主题结构：迈向机制理解。arXiv预印本 arXiv:2303.04245,
    2023。
- en: 'LNV^+ [23] Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man,
    Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. Chatgpt beyond english: Towards
    a comprehensive evaluation of large language models in multilingual learning.
    arXiv preprint arXiv:2304.05613, 2023.'
  id: totrans-2220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LNV^+ [23] 越德·赖，义中·吴，阿米尔·普朗·本·维赛，希欧·曼，弗朗克·德农库特，中·布，和天·吴·阮。Chatgpt超越英语：迈向对多语言学习的大语言模型的综合评估。arXiv预印本
    arXiv:2304.05613, 2023。
- en: LPM [15] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches
    to attention-based neural machine translation. arXiv preprint arXiv:1508.04025,
    2015.
  id: totrans-2221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LPM [15] 明塘·良，希欧·范，和克里斯托弗·D·曼宁。基于注意力的神经机器翻译的有效方法。arXiv预印本 arXiv:1508.04025,
    2015。
- en: 'LR [20] Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless”
    regression can generalize. ., 2020.'
  id: totrans-2222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LR [20] 滕元·梁和亚历山大·拉赫林。仅插值：核“无岭”回归可以推广。., 2020。
- en: LSS^+ [20] Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, et al. Generalized
    leverage score sampling for neural networks. Advances in Neural Information Processing
    Systems, 33:10775–10787, 2020.
  id: totrans-2223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSS^+ [20] 杰森·D·李，若琦·沈，赵宋，蒙迪·王，等。神经网络的广义杠杆得分采样。《神经信息处理系统进展》，33:10775–10787,
    2020。
- en: LSW [15] Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting
    plane method and its implications for combinatorial and convex optimization. In
    2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 1049–1065\.
    IEEE, 2015.
  id: totrans-2224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSW [15] 阴达特·李，亚伦·西德福德，和山姆·赵伟·黄。一种更快的切割平面方法及其对组合优化和凸优化的影响。在2015年IEEE第56届计算机科学基础年会，页码1049–1065\.
    IEEE, 2015。
- en: LSZ [19] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization
    in the current matrix multiplication time. In Conference on Learning Theory, pages
    2140–2157\. PMLR, 2019.
  id: totrans-2225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSZ [19] 阴达特·李，赵宋，和秋怡·张。在当前矩阵乘法时间中解决经验风险最小化。在学习理论会议，页码2140–2157\. PMLR, 2019。
- en: '[118] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh
    and sinh regression problems. arXiv preprint arXiv:2303.15725, 2023.'
  id: totrans-2226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] 智航·李，赵宋，和田奕·周。解决正则化的exp，cosh和sinh回归问题。arXiv预印本 arXiv:2303.15725, 2023。'
- en: '[119] S. Cliff Liu, Zhao Song, Hengjie Zhang, Lichen Zhang, and Tianyi Zhou.
    Space-efficient interior point method, with applications to linear programming
    and maximum weight bipartite matching. In International Colloquium on Automata,
    Languages and Programming (ICALP), pages 88:1–88:14, 2023.'
  id: totrans-2227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] S. 克里夫·刘，赵宋，恒杰·张，李辰·张，和田奕·周。空间高效的内点方法及其在线性规划和最大权重二分匹配中的应用。在国际自动机、语言与编程研讨会（ICALP），页码88:1–88:14,
    2023。'
- en: LWA [21] Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd
    reaches zero loss?–a mathematical framework. arXiv preprint arXiv:2110.06914,
    2021.
  id: totrans-2228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LWA [21] 志远·李，田浩·王，和桑吉夫·阿罗拉。SGD达到零损失后会发生什么？–一个数学框架。arXiv预印本 arXiv:2110.06914,
    2021。
- en: LWM [19] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization
    effect of initial large learning rate in training neural networks. Advances in
    Neural Information Processing Systems, 32, 2019.
  id: totrans-2229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LWM [19] 袁智·李，科林·魏，和滕宇·马。解释神经网络训练中初始大学习率的正则化效应。《神经信息处理系统进展》，32，2019。
- en: LXWZ [23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is
    your code generated by chatgpt really correct? rigorous evaluation of large language
    models for code generation. arXiv preprint arXiv:2305.01210, 2023.
  id: totrans-2230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LXWZ [23] 刘家伟，春秋·斯蒂文·夏，余瑶·王，和凌铭·张。你的代码是由chatgpt生成的，真的正确吗？对大语言模型生成代码的严格评估。arXiv预印本
    arXiv:2305.01210, 2023。
- en: 'LYB^+ [22] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh
    Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. Large
    models are parsimonious learners: Activation sparsity in trained transformers.
    arXiv preprint arXiv:2210.06313, 2022.'
  id: totrans-2231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LYB^+ [22] 宗林·李，崇·游，斯里纳德·博贾纳帕利，大亮·李，安基特·辛格·拉瓦特，萨尚克·J·雷迪，柯·叶，费利克斯·陈，费利克斯·于，瑞奇·郭，等。大模型是节俭的学习者：训练的变压器中的激活稀疏性。arXiv预印本
    arXiv:2210.06313, 2022。
- en: MBAB [22] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating
    and editing factual associations in gpt. Advances in Neural Information Processing
    Systems, 35:17359–17372, 2022.
  id: totrans-2232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MBAB [22] Kevin Meng、David Bau、Alex Andonian 和 Yonatan Belinkov。定位和编辑 GPT 中的事实关联。《神经信息处理系统进展》，35:17359–17372，2022
    年。
- en: MGN^+ [23] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D
    Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning language models with just forward
    passes. arXiv preprint arXiv:2305.17333, 2023.
  id: totrans-2233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MGN^+ [23] Sadhika Malladi、Tianyu Gao、Eshaan Nichani、Alex Damian、Jason D Lee、Danqi
    Chen 和 Sanjeev Arora。仅通过前向传递微调语言模型。arXiv 预印本 arXiv:2305.17333，2023 年。
- en: MM [13] Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings
    in input-sparsity time and applications to robust linear regression. In Proceedings
    of the forty-fifth annual ACM symposium on Theory of computing, pages 91–100,
    2013.
  id: totrans-2234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MM [13] Xiangrui Meng 和 Michael W Mahoney。在输入稀疏时间内低失真子空间嵌入及其在鲁棒线性回归中的应用。发表于第
    45 届 ACM 计算理论年会，第 91–100 页，2013 年。
- en: MOSW [22] Alexander Munteanu, Simon Omlor, Zhao Song, and David Woodruff. Bounding
    the width of neural networks via coupled initialization a worst case analysis.
    In International Conference on Machine Learning, pages 16083–16122\. PMLR, 2022.
  id: totrans-2235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MOSW [22] Alexander Munteanu、Simon Omlor、Zhao Song 和 David Woodruff。通过耦合初始化来限制神经网络的宽度：最坏情况分析。发表于国际机器学习会议，第
    16083–16122 页。PMLR，2022 年。
- en: MRS [20] Konstantin Makarychev, Aravind Reddy, and Liren Shan. Improved guarantees
    for k-means++ and k-means++ parallel. Advances in Neural Information Processing
    Systems, 33:16142–16152, 2020.
  id: totrans-2236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MRS [20] Konstantin Makarychev、Aravind Reddy 和 Liren Shan。改进的 k-means++ 和 k-means++
    并行算法的保证。《神经信息处理系统进展》，33:16142–16152，2020 年。
- en: MS [21] Linjian Ma and Edgar Solomonik. Fast and accurate randomized algorithms
    for low-rank tensor decompositions. Advances in Neural Information Processing
    Systems, 34:24299–24312, 2021.
  id: totrans-2237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MS [21] Linjian Ma 和 Edgar Solomonik。用于低秩张量分解的快速且准确的随机算法。《神经信息处理系统进展》，34:24299–24312，2021
    年。
- en: 'MWG^+ [20] Edward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee,
    Nati Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization
    scale vs training accuracy. Advances in neural information processing systems,
    33:22182–22193, 2020.'
  id: totrans-2238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MWG^+ [20] Edward Moroshko、Blake E Woodworth、Suriya Gunasekar、Jason D Lee、Nati
    Srebro 和 Daniel Soudry。深度线性分类中的隐性偏差：初始化规模与训练准确性。《神经信息处理系统进展》，33:22182–22193，2020
    年。
- en: 'NKL^+ [23] John J Nay, David Karamardian, Sarah B Lawsky, Wenting Tao, Meghana
    Bhat, Raghav Jain, Aaron Travis Lee, Jonathan H Choi, and Jungo Kasai. Large language
    models as tax attorneys: A case study in legal capabilities emergence. arXiv preprint
    arXiv:2306.07075, 2023.'
  id: totrans-2239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NKL^+ [23] John J Nay、David Karamardian、Sarah B Lawsky、Wenting Tao、Meghana Bhat、Raghav
    Jain、Aaron Travis Lee、Jonathan H Choi 和 Jungo Kasai。大型语言模型作为税务律师：法律能力出现的案例研究。arXiv
    预印本 arXiv:2306.07075，2023 年。
- en: NLG^+ [19] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona
    Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on
    separable data. In The 22nd International Conference on Artificial Intelligence
    and Statistics, pages 3420–3428\. PMLR, 2019.
  id: totrans-2240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLG^+ [19] Mor Shpigel Nacson、Jason Lee、Suriya Gunasekar、Pedro Henrique Pamplona
    Savarese、Nathan Srebro 和 Daniel Soudry。梯度下降在可分数据上的收敛性。发表于第 22 届国际人工智能与统计会议，第 3420–3428
    页。PMLR，2019 年。
- en: 'NN [13] Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra
    algorithms via sparser subspace embeddings. In 2013 ieee 54th annual symposium
    on foundations of computer science, pages 117–126\. IEEE, 2013.'
  id: totrans-2241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NN [13] Jelani Nelson 和 Huy L Nguyên。Osnap：通过稀疏子空间嵌入加速数值线性代数算法。发表于 2013 IEEE
    第 54 届计算机科学基础年会，第 117–126 页。IEEE，2013 年。
- en: Ope [23] OpenAI. Gpt-4 technical report, 2023.
  id: totrans-2242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ope [23] OpenAI。GPT-4 技术报告，2023 年。
- en: 'OS [20] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization:
    Global convergence guarantees for training shallow neural networks. IEEE Journal
    on Selected Areas in Information Theory, 1(1):84–105, 2020.'
  id: totrans-2243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OS [20] Samet Oymak 和 Mahdi Soltanolkotabi。适度过参数化的方向：浅层神经网络训练的全局收敛保证。《IEEE 信息理论精选期刊》，1(1):84–105，2020
    年。
- en: Pag [13] Rasmus Pagh. Compressed matrix multiplication. ACM Transactions on
    Computation Theory (TOCT), 5(3):1–17, 2013.
  id: totrans-2244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pag [13] Rasmus Pagh。压缩矩阵乘法。《ACM 计算理论期刊》（TOCT），5(3):1–17，2013 年。
- en: 'PD [23] Dongqi Pu and Vera Demberg. Chatgpt vs human-authored text: Insights
    into controllable text summarization and sentence style transfer. arXiv preprint
    arXiv:2306.07799, 2023.'
  id: totrans-2245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PD [23] Dongqi Pu 和 Vera Demberg。ChatGPT 与人工撰写文本的比较：对可控文本摘要和句子风格转移的见解。arXiv
    预印本 arXiv:2306.07799，2023 年。
- en: PMM^+ [23] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca
    Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, and Andrea Loreggia.
    Understanding the capabilities of large language models for automated planning.
    arXiv preprint arXiv:2305.16151, 2023.
  id: totrans-2246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PMM^+ [23] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca
    Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, 和 Andrea Loreggia. 理解大型语言模型在自动规划中的能力。arXiv
    预印本 arXiv:2305.16151, 2023。
- en: PMXA [23] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, and Sanjeev Arora.
    Trainable transformer in transformer. arXiv preprint arXiv:2307.01189, 2023.
  id: totrans-2247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PMXA [23] Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, 和 Sanjeev Arora.
    可训练的 transformer in transformer。arXiv 预印本 arXiv:2307.01189, 2023。
- en: PSZA [23] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora.
    Task-specific skill localization in fine-tuned language models. arXiv preprint
    arXiv:2302.06600, 2023.
  id: totrans-2248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PSZA [23] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, 和 Sanjeev Arora. 在微调语言模型中的任务特定技能定位。arXiv
    预印本 arXiv:2302.06600, 2023。
- en: 'QJS^+ [22] Lianke Qin, Rajesh Jayaram, Elaine Shi, Zhao Song, Danyang Zhuo,
    and Shumo Chu. Adore: Differentially oblivious relational database operators.
    In VLDB, 2022.'
  id: totrans-2249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'QJS^+ [22] Lianke Qin, Rajesh Jayaram, Elaine Shi, Zhao Song, Danyang Zhuo,
    和 Shumo Chu. Adore: 差分隐私关系数据库操作符。发表于 VLDB, 2022。'
- en: QQ [19] Qian Qian and Xiaoyuan Qian. The implicit bias of adagrad on separable
    data. Advances in Neural Information Processing Systems, 32, 2019.
  id: totrans-2250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QQ [19] Qian Qian 和 Xiaoyuan Qian. Adagrad 在可分数据上的隐含偏差。神经信息处理系统进展, 32, 2019。
- en: QRS^+ [22] Lianke Qin, Aravind Reddy, Zhao Song, Zhaozhuo Xu, and Danyang Zhuo.
    Adaptive and dynamic multi-resolution hashing for pairwise summations. In BigData,
    2022.
  id: totrans-2251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QRS^+ [22] Lianke Qin, Aravind Reddy, Zhao Song, Zhaozhuo Xu, 和 Danyang Zhuo.
    自适应和动态多分辨率哈希用于成对求和。发表于 BigData, 2022。
- en: QSW [23] Lianke Qin, Zhao Song, and Yitan Wang. Fast submodular function maximization.
    arXiv preprint arXiv:2305.08367, 2023.
  id: totrans-2252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QSW [23] Lianke Qin, Zhao Song, 和 Yitan Wang. 快速子模函数最大化。arXiv 预印本 arXiv:2305.08367,
    2023。
- en: QSZ [23] Lianke Qin, Zhao Song, and Ruizhe Zhang. A general algorithm for solving
    rank-one matrix sensing. arXiv preprint arXiv:2303.12298, 2023.
  id: totrans-2253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QSZ [23] Lianke Qin, Zhao Song, 和 Ruizhe Zhang. 一种解决秩一矩阵感知的通用算法。arXiv 预印本 arXiv:2303.12298,
    2023。
- en: QSZZ [23] Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and
    unified algorithm for projection matrix vector multiplication with application
    to empirical risk minimization. In International Conference on Artificial Intelligence
    and Statistics (AISTATS), pages 101–156\. PMLR, 2023.
  id: totrans-2254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QSZZ [23] Lianke Qin, Zhao Song, Lichen Zhang, 和 Danyang Zhuo. 一种用于投影矩阵向量乘法的在线统一算法及其在经验风险最小化中的应用。发表于国际人工智能与统计会议
    (AISTATS), 第 101–156 页. PMLR, 2023。
- en: QZZ^+ [23] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing
    task solver? arXiv preprint arXiv:2302.06476, 2023.
  id: totrans-2255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QZZ^+ [23] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, 和 Diyi Yang. ChatGPT 是一个通用自然语言处理任务求解器吗？arXiv 预印本 arXiv:2302.06476, 2023。
- en: Rat [20] Kovid Rathee. Meet google meena, 2020.
  id: totrans-2256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rat [20] Kovid Rathee. 认识 Google Meena, 2020。
- en: RNS^+ [18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. ., 2018.
  id: totrans-2257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNS^+ [18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 等.
    通过生成预训练提高语言理解能力。., 2018。
- en: RRS^+ [21] Aravind Reddy, Ryan A Rossi, Zhao Song, Anup Rao, Tung Mai, Nedim
    Lipka, Gang Wu, Eunyee Koh, and Nesreen Ahmed. Online map inference and learning
    for nonsymmetric determinantal point processes. arXiv preprint arXiv:2111.14674,
    2021.
  id: totrans-2258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RRS^+ [21] Aravind Reddy, Ryan A Rossi, Zhao Song, Anup Rao, Tung Mai, Nedim
    Lipka, Gang Wu, Eunyee Koh, 和 Nesreen Ahmed. 非对称行列式点过程的在线映射推断和学习。arXiv 预印本 arXiv:2111.14674,
    2021。
- en: 'RSM^+ [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D
    Manning, and Chelsea Finn. Direct preference optimization: Your language model
    is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.'
  id: totrans-2259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RSM^+ [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher
    D Manning, 和 Chelsea Finn. 直接偏好优化: 你的语言模型实际上是一个奖励模型。arXiv 预印本 arXiv:2305.18290,
    2023。'
- en: RSW [16] Ilya Razenshteyn, Zhao Song, and David P Woodruff. Weighted low rank
    approximations with provable guarantees. In Proceedings of the forty-eighth annual
    ACM symposium on Theory of Computing, pages 250–263, 2016.
  id: totrans-2260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSW [16] Ilya Razenshteyn, Zhao Song, 和 David P Woodruff. 具有可证明保证的加权低秩近似。发表于第四十八届年度
    ACM 计算理论研讨会, 第 250–263 页, 2016。
- en: RSZ [22] Aravind Reddy, Zhao Song, and Lichen Zhang. Dynamic tensor product
    regression. In NeurIPS, 2022.
  id: totrans-2261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSZ [22] Aravind Reddy, Zhao Song, 和 Lichen Zhang. 动态张量积回归。发表于 NeurIPS, 2022。
- en: RWC^+ [19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.
  id: totrans-2262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RWC^+ [19] 亚历克·拉德福德、杰弗里·吴、雷旺·查德、大卫·鲁安、达里奥·阿莫代、伊利亚·苏茨克维尔等。语言模型是无监督的多任务学习者。OpenAI
    博客，1(8):9，2019年。
- en: RYW^+ [19] Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy
    Coenen, Adam Pearce, and Been Kim. Visualizing and measuring the geometry of bert.
    Advances in Neural Information Processing Systems, 32, 2019.
  id: totrans-2263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RYW^+ [19] 艾米莉·瑞夫、安·袁、马丁·瓦滕伯格、费尔南达·B·维戈斯、安迪·科南、亚当·皮尔斯和宾·金。可视化和测量 BERT 的几何结构。《神经信息处理系统进展》，32，2019年。
- en: Sar [06] Tamas Sarlos. Improved approximation algorithms for large matrices
    via random projections. In 2006 47th annual IEEE symposium on foundations of computer
    science (FOCS’06), pages 143–152\. IEEE, 2006.
  id: totrans-2264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sar [06] 塔玛斯·萨洛什。通过随机投影改进大矩阵的近似算法。发表于2006年第47届IEEE计算机科学基础研讨会（FOCS’06），页码 143–152。IEEE，2006年。
- en: SATA [22] Haoyuan Sun, Kwangjun Ahn, Christos Thrampoulidis, and Navid Azizan.
    Mirror descent maximizes generalized margin and can be implemented efficiently.
    Advances in Neural Information Processing Systems, 35:31089–31101, 2022.
  id: totrans-2265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SATA [22] 孙浩源、安光俊、克里斯托斯·斯拉姆普利迪斯和纳维德·阿齐赞。镜像下降最大化广义边际，并且可以高效实现。《神经信息处理系统进展》，35:31089–31101，2022年。
- en: SHN^+ [18] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar,
    and Nathan Srebro. The implicit bias of gradient descent on separable data. The
    Journal of Machine Learning Research, 19:2822–2878, 2018.
  id: totrans-2266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHN^+ [18] 丹尼尔·索德里、埃拉德·霍弗、摩尔·施皮戈尔·纳克森、苏利亚·古纳塞卡尔和内森·斯雷布罗。梯度下降在可分数据上的隐式偏差。《机器学习研究杂志》，19:2822–2878，2018年。
- en: SHT [23] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational
    strengths and limitations of transformers. arXiv preprint arXiv:2306.02896, 2023.
  id: totrans-2267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHT [23] 克莱顿·桑福德、丹尼尔·许和马图斯·特尔加斯基。变换器的表征强度和局限性。arXiv 预印本 arXiv:2306.02896，2023年。
- en: 'SM^+ [23] Giriprasad Sridhara, Sourav Mazumdar, et al. Chatgpt: A study on
    its utility for ubiquitous software engineering tasks. arXiv preprint arXiv:2305.16837,
    2023.'
  id: totrans-2268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SM^+ [23] 吉里普拉萨德·斯里达拉、苏拉夫·马祖姆达等。ChatGPT：关于其在普遍软件工程任务中的效用的研究。arXiv 预印本 arXiv:2305.16837，2023年。
- en: Spa [23] Jared Spataro. Introducing microsoft 365 copilot – your copilot for
    work, 2023.
  id: totrans-2269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spa [23] 贾里德·斯帕塔罗。介绍微软 365 Copilot——你的工作助手，2023年。
- en: SSZ [23] Ritwik Sinha, Zhao Song, and Tianyi Zhou. A mathematical abstraction
    for balancing the trade-off between creativity and reality in large language models.
    arXiv preprint arXiv:2306.02295, 2023.
  id: totrans-2270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSZ [23] 里特维克·辛哈、赵松和田一周。大语言模型中平衡创造力与现实之间权衡的数学抽象。arXiv 预印本 arXiv:2306.02295，2023年。
- en: SWYZ [21] Zhao Song, David Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching
    of polynomial kernels of polynomial degree. In International Conference on Machine
    Learning, pages 9812–9823\. PMLR, 2021.
  id: totrans-2271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SWYZ [21] 赵松、大卫·伍德拉夫、郑宇和李晨张。多项式核的多项式度的快速草图。发表于国际机器学习大会，页码 9812–9823。PMLR，2021年。
- en: SWZ [17] Zhao Song, David P Woodruff, and Peilin Zhong. Low rank approximation
    with entrywise l1-norm error. In Proceedings of the 49th Annual ACM SIGACT Symposium
    on Theory of Computing, pages 688–701, 2017.
  id: totrans-2272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SWZ [17] 赵松、大卫·P·伍德拉夫和钟佩林。带有逐项 l1 范数误差的低秩近似。发表于第49届年度 ACM SIGACT 理论计算机科学研讨会，页码
    688–701，2017年。
- en: SWZ [19] Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor
    low rank approximation. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
    on Discrete Algorithms, pages 2772–2789\. SIAM, 2019.
  id: totrans-2273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SWZ [19] 赵松、大卫·P·伍德拉夫和钟佩林。相对误差张量低秩近似。发表于第三十届年度 ACM-SIAM 离散算法研讨会，页码 2772–2789。SIAM，2019年。
- en: SXZ [22] Zhao Song, Zhaozhuo Xu, and Lichen Zhang. Speeding up sparsification
    using inner product search data structures. arXiv preprint arXiv:2204.03209, 2022.
  id: totrans-2274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SXZ [22] 赵松、赵卓旭和李晨张。利用内积搜索数据结构加速稀疏化。arXiv 预印本 arXiv:2204.03209，2022年。
- en: SY [21] Zhao Song and Zheng Yu. Oblivious sketching-based central path method
    for solving linear programming problems. In 38th International Conference on Machine
    Learning (ICML), 2021.
  id: totrans-2275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SY [21] 赵松和郑宇。基于隐式草图的中心路径方法解决线性规划问题。发表于第38届国际机器学习大会（ICML），2021年。
- en: SYYZ [23] Zhao Song, Mingquan Ye, Junze Yin, and Lichen Zhang. Efficient alternating
    minimization with applications to weighted low rank approximation. arXiv preprint
    arXiv:2306.04169, 2023.
  id: totrans-2276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SYYZ [23] 赵松、明全叶、君泽尹和李晨张。带有加权低秩近似应用的高效交替最小化。arXiv 预印本 arXiv:2306.04169，2023年。
- en: SYZ [21] Zhao Song, Shuo Yang, and Ruizhe Zhang. Does preprocessing help training
    over-parameterized neural networks? Advances in Neural Information Processing
    Systems, 34:22890–22904, 2021.
  id: totrans-2277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SYZ [21] 赵松、杨硕和张瑞哲。预处理是否有助于训练过参数化神经网络？《神经信息处理系统进展》，34:22890–22904，2021年。
- en: '[170] Zhao Song, Mingquan Ye, and Lichen Zhang. Streaming semidefinite programs:
    $o(\sqrt{n})$ passes, small space and fast runtime. Manuscript, 2023.'
  id: totrans-2278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] 赵松、叶名泉和张立辰。流式半正定程序：$o(\sqrt{n})$ 次通过、小空间和快速运行时间。手稿，2023年。'
- en: '[171] Zhao Song, Junze Yin, and Lichen Zhang. Solving attention kernel regression
    problem via pre-conditioner. arXiv preprint arXiv:2308.14304, 2023.'
  id: totrans-2279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] 赵松、尹俊泽和张立辰。通过预条件器解决注意力核回归问题。arXiv 预印本 arXiv:2308.14304，2023年。'
- en: SZKS [21] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating
    how single head attention learns. arXiv preprint arXiv:2103.07601, 2021.
  id: totrans-2280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SZKS [21] 查理·斯内尔、钟瑞奇、丹·克莱因和雅各布·斯坦赫特。近似单头注意力如何学习。arXiv 预印本 arXiv:2103.07601，2021年。
- en: SZZ [21] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized
    neural network in subquadratic time. arXiv preprint arXiv:2112.07628, 2021.
  id: totrans-2281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SZZ [21] 赵松、张立辰和张瑞哲。以亚平方时间训练多层过参数化神经网络。arXiv 预印本 arXiv:2112.07628，2021年。
- en: 'TLI^+ [23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv
    preprint arXiv:2302.13971, 2023.'
  id: totrans-2282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TLI^+ [23] 休戈·图夫龙、提博·拉夫里尔、戈蒂埃·伊扎卡尔、泽维尔·马尔蒂内、玛丽-安·拉肖、蒂莫泰·拉克鲁瓦、巴蒂斯特·罗兹耶、纳曼·戈亚尔、埃里克·汉布罗、费萨尔·阿扎尔等。Llama：开放和高效的基础语言模型。arXiv
    预印本 arXiv:2302.13971，2023年。
- en: TLTO [23] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet
    Oymak. Transformers as support vector machines. arXiv preprint arXiv:2308.16898,
    2023.
  id: totrans-2283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TLTO [23] 达乌德·阿塔伊·塔尔扎纳赫、李颖聪、克里斯托斯·萨姆普利迪斯和萨梅特·奥伊马克。将变换器作为支持向量机。arXiv 预印本 arXiv:2308.16898，2023年。
- en: 'TMS^+ [23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-2284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TMS^+ [23] 休戈·图夫龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马哈伊里、雅斯敏·巴巴埃、尼古拉·巴什利科夫、苏姆亚·巴特拉、普拉吉瓦尔·巴尔加瓦、舒尔提·博萨尔等。Llama
    2：开放基础和微调的聊天模型。arXiv 预印本 arXiv:2307.09288，2023年。
- en: VCC^+ [17] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
    Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903,
    2017.
  id: totrans-2285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VCC^+ [17] 佩塔尔·维利奇科维奇、吉列姆·库库鲁、阿兰莎·卡萨诺瓦、阿德里安娜·罗梅罗、皮特罗·利奥和约书亚·本吉奥。图注意力网络。arXiv
    预印本 arXiv:1710.10903，2017年。
- en: VKR [19] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization
    for optimal sparse recovery. Advances in Neural Information Processing Systems,
    32, 2019.
  id: totrans-2286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VKR [19] 托马斯·瓦斯凯维修斯、瓦伦·卡纳德和帕特里克·雷贝什尼。用于最优稀疏恢复的隐式正则化。《神经信息处理系统进展》，32，2019年。
- en: VSP^+ [17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you
    need. Advances in neural information processing systems, 30, 2017.
  id: totrans-2287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VSP^+ [17] 阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马尔、雅各布·乌斯科雷特、利昂·琼斯、艾丹·N·戈麦斯、卢卡斯·凯瑟和伊利亚·波洛苏金。注意力是你所需要的一切。《神经信息处理系统进展》，30，2017年。
- en: WGL^+ [20] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko,
    Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich
    regimes in overparametrized models. In Conference on Learning Theory, pages 3635–3673\.
    PMLR, 2020.
  id: totrans-2288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WGL^+ [20] 布雷克·伍德沃斯、苏里亚·古纳塞卡、杰森·D·李、爱德华·莫罗什科、佩德罗·萨瓦雷斯、伊泰·戈兰、丹尼尔·苏德里和内森·斯雷布罗。过参数化模型中的核和丰富机制。在学习理论会议上，第3635–3673页。PMLR，2020年。
- en: 'WHH^+ [23] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong
    Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of
    chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095,
    2023.'
  id: totrans-2289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WHH^+ [23] 汪进东、胡希旭、侯文鑫、陈浩、郑润凯、王亦东、杨林毅、黄浩俊、叶威、耿西博等。关于 ChatGPT 的鲁棒性：一种对抗性和分布外的视角。arXiv
    预印本 arXiv:2302.12095，2023年。
- en: Wil [12] Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd.
    In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,
    pages 887–898, 2012.
  id: totrans-2290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wil [12] 弗吉尼亚·瓦西列夫斯卡·威廉姆斯。比 Coppersmith-Winograd 更快的矩阵乘法。在第四十四届年度 ACM 计算理论研讨会论文集中，第887–898页，2012年。
- en: WLJ^+ [23] Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming
    Shi, and Zhaopeng Tu. Document-level machine translation with large language models.
    arXiv preprint arXiv:2304.02210, 2023.
  id: totrans-2291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WLJ^+ [23] 王龙越、吕晨阳、季天博、张志瑞、余电和石树铭。使用大型语言模型进行文档级机器翻译。arXiv预印本 arXiv:2304.02210，2023年。
- en: 'WLL^+ [23] Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath:
    Can your language model pass chinese elementary school math test? arXiv preprint
    arXiv:2306.16636, 2023.'
  id: totrans-2292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WLL^+ [23] 田文伟、段剑、刘伟、董爽和王斌。Cmath：你的语言模型能通过中文小学数学测试吗？arXiv预印本 arXiv:2306.16636，2023年。
- en: WMCL [21] Bohan Wang, Qi Meng, Wei Chen, and Tie-Yan Liu. The implicit bias
    for adaptive optimization algorithms on homogeneous neural networks. In International
    Conference on Machine Learning, pages 10849–10858\. PMLR, 2021.
  id: totrans-2293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WMCL [21] 王博涵、孟琪、陈伟和刘铁岩。均质神经网络上自适应优化算法的隐性偏差。在国际机器学习大会上，页10849–10858。PMLR，2021年。
- en: WMZ^+ [21] Bohan Wang, Qi Meng, Huishuai Zhang, Ruoyu Sun, Wei Chen, and Zhi-Ming
    Ma. Momentum doesn’t change the implicit bias. ., 2021.
  id: totrans-2294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WMZ^+ [21] 王博涵、孟琪、张惠帅、孙若愚、陈伟和马志铭。动量并不会改变隐性偏差。。，2021年。
- en: WQR^+ [23] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin
    Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring
    the capabilities and limitations of language models through counterfactual tasks.
    arXiv preprint arXiv:2307.02477, 2023.
  id: totrans-2295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WQR^+ [23] 吴兆峰、邱琳璐、亚历克西斯·罗斯、Ekin Akyürek、陈博远、王白林、金娜荣、雅各布·安德烈亚斯 和 金云。推理还是背诵？通过反事实任务探索语言模型的能力和局限性。arXiv预印本
    arXiv:2307.02477，2023年。
- en: WWZ^+ [22] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and
    Juanzi Li. Finding skill neurons in pre-trained transformer-based language models.
    arXiv preprint arXiv:2211.07349, 2022.
  id: totrans-2296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WWZ^+ [22] 王晓智、温凯岳、张正岩、侯磊、刘志远和李娟子。寻找预训练变换器模型中的技能神经元。arXiv预印本 arXiv:2211.07349，2022年。
- en: 'WXXZ [23] Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei
    Zhou. New bounds for matrix multiplication: from alpha to omega, 2023.'
  id: totrans-2297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WXXZ [23] Virginia Vassilevska Williams、许寅瞻、许子轩和周仁飞。矩阵乘法的新界限：从alpha到omega，2023年。
- en: WZ [16] David P Woodruff and Peilin Zhong. Distributed low rank approximation
    of implicit functions of a matrix. In 2016 IEEE 32nd International Conference
    on Data Engineering (ICDE), pages 847–858\. IEEE, 2016.
  id: totrans-2298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WZ [16] David P Woodruff 和钟培林。矩阵隐函数的分布式低秩近似。在2016 IEEE第32届数据工程国际会议（ICDE）上，页847–858。IEEE，2016年。
- en: 'WZD^+ [20] Ruosong Wang, Peilin Zhong, Simon S Du, Russ R Salakhutdinov, and
    Lin Yang. Planning with general objective functions: Going beyond total rewards.
    Advances in Neural Information Processing Systems, 33:14486–14497, 2020.'
  id: totrans-2299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WZD^+ [20] 王若松、钟培林、Simon S Du、Russ R Salakhutdinov 和杨林。使用通用目标函数进行规划：超越总奖励。神经信息处理系统进展，33:14486–14497，2020年。
- en: 'XBK^+ [15] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,
    Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural
    image caption generation with visual attention. In International conference on
    machine learning, pages 2048–2057\. PMLR, 2015.'
  id: totrans-2300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XBK^+ [15] 许凯文、Jimmy Ba、Ryan Kiros、Cho Kyunghyun、Aaron Courville、Ruslan Salakhudinov、Rich
    Zemel 和 Yoshua Bengio。展示、关注并讲述：具有视觉注意的神经图像描述生成。在国际机器学习大会上，页2048–2057。PMLR，2015年。
- en: XLH^+ [23] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik
    Cambria. Are large language models really good logical reasoners? a comprehensive
    evaluation from deductive, inductive and abductive views. arXiv preprint arXiv:2306.09841,
    2023.
  id: totrans-2301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLH^+ [23] 许方之、林启卡、韩嘉伟、赵天哲、刘军 和 Erik Cambria。大型语言模型真的很擅长逻辑推理吗？从演绎、归纳和溯因视角的全面评估。arXiv预印本
    arXiv:2306.09841，2023年。
- en: XQP^+ [22] Shuo Xie, Jiahao Qiu, Ankita Pasad, Li Du, Qing Qu, and Hongyuan
    Mei. Hidden state variability of pretrained language models can guide computation
    reduction for transfer learning. arXiv preprint arXiv:2210.10041, 2022.
  id: totrans-2302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XQP^+ [22] 谢硕、邱家浩、Ankita Pasad、杜丽、曲青和梅洪源。预训练语言模型的隐藏状态变异性可以指导迁移学习中的计算减少。arXiv预印本
    arXiv:2210.10041，2022年。
- en: XSS [21] Zhaozhuo Xu, Zhao Song, and Anshumali Shrivastava. Breaking the linear
    iteration cost barrier for some well-known conditional gradient methods using
    maxip data-structures. Advances in Neural Information Processing Systems, 34:5576–5589,
    2021.
  id: totrans-2303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XSS [21] 许兆卓、宋赵和安舒马利·施里瓦斯塔瓦。使用maxip数据结构打破一些著名条件梯度方法的线性迭代成本障碍。神经信息处理系统进展，34:5576–5589，2021年。
- en: XSS [23] Zhaozhuo Xu, Zhao Song, and Anshumali Shrivastava. A tale of two efficient
    value iteration algorithms for solving linear mdps with large action space. In
    AISTATS, 2023.
  id: totrans-2304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XSS [23] Zhaozhuo Xu、Zhao Song 和 Anshumali Shrivastava。解决具有大动作空间的线性 MDPs 的两个高效价值迭代算法的故事。AISTATS,
    2023。
- en: 'XZZ [18] Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan: Generative networks
    with metric embeddings. Advances in neural information processing systems, 31,
    2018.'
  id: totrans-2305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XZZ [18] Chang Xiao、Peilin Zhong 和 Changxi Zheng。Bourgan：具有度量嵌入的生成网络。神经信息处理系统进展，31,
    2018。
- en: YKM [20] Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view
    on implicit bias in training linear neural networks. arXiv preprint arXiv:2010.02501,
    2020.
  id: totrans-2306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YKM [20] Chulhee Yun、Shankar Krishnan 和 Hossein Mobahi。关于训练线性神经网络中的隐性偏差的统一视角。arXiv
    预印本 arXiv:2010.02501, 2020。
- en: 'ZAG^+ [23] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia,
    and Lidong Bing. M3exam: A multilingual, multimodal, multilevel benchmark for
    examining large language models. arXiv preprint arXiv:2306.05179, 2023.'
  id: totrans-2307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZAG^+ [23] Wenxuan Zhang、Sharifah Mahani Aljunied、Chang Gao、Yew Ken Chia 和 Lidong
    Bing。M3exam：一个多语言、多模态、多层次的基准，用于检验大语言模型。arXiv 预印本 arXiv:2306.05179, 2023。
- en: 'ZDL^+ [23] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong
    Bing. Sentiment analysis in the era of large language models: A reality check.
    arXiv preprint arXiv:2305.15005, 2023.'
  id: totrans-2308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZDL^+ [23] Wenxuan Zhang、Yue Deng、Bing Liu、Sinno Jialin Pan 和 Lidong Bing。大语言模型时代的情感分析：现实检验。arXiv
    预印本 arXiv:2305.15005, 2023。
- en: ZG [19] Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized
    deep neural networks. Advances in neural information processing systems, 32, 2019.
  id: totrans-2309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZG [19] Difan Zou 和 Quanquan Gu。训练超参数化深度神经网络的改进分析。神经信息处理系统进展，32, 2019。
- en: 'Zha [22] Lichen Zhang. Speeding up optimizations via data structures: Faster
    search, sample and maintenance. Master’s thesis, Carnegie Mellon University, 2022.'
  id: totrans-2310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zha [22] Lichen Zhang。通过数据结构加速优化：更快的搜索、采样和维护。硕士论文，卡内基梅隆大学，2022。
- en: 'ZHDK [23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer:
    Accelerating transformers via kernel density estimation. arXiv preprint arXiv:2302.02451,
    2023.'
  id: totrans-2311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZHDK [23] Amir Zandieh、Insu Han、Majid Daliri 和 Amin Karbasi。Kdeformer：通过核密度估计加速变换器。arXiv
    预印本 arXiv:2302.02451, 2023。
- en: 'ZHL^+ [23] Eric Zelikman, Qian Huang, Percy Liang, Nick Haber, and Noah D Goodman.
    Just one byte (per gradient): A note on low-bandwidth decentralized language model
    finetuning using shared randomness. arXiv preprint arXiv:2306.10015, 2023.'
  id: totrans-2312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZHL^+ [23] Eric Zelikman、Qian Huang、Percy Liang、Nick Haber 和 Noah D Goodman。每个梯度只有一个字节：关于使用共享随机性进行低带宽分布式语言模型微调的说明。arXiv
    预印本 arXiv:2306.10015, 2023。
- en: ZKV^+ [20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon
    Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good
    for attention models? Advances in Neural Information Processing Systems, 33:15383–15393,
    2020.
  id: totrans-2313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZKV^+ [20] Jingzhao Zhang、Sai Praneeth Karimireddy、Andreas Veit、Seungyeon Kim、Sashank
    Reddi、Sanjiv Kumar 和 Suvrit Sra。为什么自适应方法对注意力模型有好处？神经信息处理系统进展，33:15383–15393, 2020。
- en: 'ZPD^+ [20] Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song,
    and Sanjeev Arora. Over-parameterized adversarial training: An analysis overcoming
    the curse of dimensionality. Advances in Neural Information Processing Systems,
    33:679–688, 2020.'
  id: totrans-2314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZPD^+ [20] Yi Zhang、Orestis Plevrakis、Simon S Du、Xingguo Li、Zhao Song 和 Sanjeev
    Arora。超参数化对抗训练：一种克服维度灾难的分析。神经信息处理系统进展，33:679–688, 2020。
- en: ZPD^+ [23] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man
    Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language
    models. arXiv preprint arXiv:2305.16934, 2023.
  id: totrans-2315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZPD^+ [23] Yunqing Zhao、Tianyu Pang、Chao Du、Xiao Yang、Chongxuan Li、Ngai-Man
    Cheung 和 Min Lin。评估大型视觉语言模型的对抗鲁棒性。arXiv 预印本 arXiv:2305.16934, 2023。
- en: ZPGA [23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers
    parse while predicting the masked word? arXiv preprint arXiv:2303.08117, 2023.
  id: totrans-2316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZPGA [23] Haoyu Zhao、Abhishek Panigrahi、Rong Ge 和 Sanjeev Arora。变换器在预测掩码词时是否进行解析？arXiv
    预印本 arXiv:2303.08117, 2023。
- en: 'ZRG^+ [22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,
    Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt:
    Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,
    2022.'
  id: totrans-2317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZRG^+ [22] Susan Zhang、Stephen Roller、Naman Goyal、Mikel Artetxe、Moya Chen、Shuohui
    Chen、Christopher Dewan、Mona Diab、Xian Li、Xi Victoria Lin 等。Opt：开放的预训练变换器语言模型。arXiv
    预印本 arXiv:2205.01068, 2022。
- en: ZWB^+ [21] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster,
    and Sham Kakade. The benefits of implicit regularization from sgd in least squares
    problems. Advances in neural information processing systems, 34:5456–5468, 2021.
  id: totrans-2318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZWB^+ [21] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean P Foster
    和 Sham Kakade. SGD 在最小二乘问题中隐式正则化的好处。《神经信息处理系统进展》，34:5456–5468，2021。
- en: ZZ [23] Ruizhe Zhang and Xinzhi Zhang. A hyperbolic extension of kadison-singer
    type results. In ICALP, 2023.
  id: totrans-2319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZZ [23] Ruizhe Zhang 和 Xinzhi Zhang. 一种超曲面扩展的 Kadison-Singer 类型结果。发表于 ICALP，2023。
