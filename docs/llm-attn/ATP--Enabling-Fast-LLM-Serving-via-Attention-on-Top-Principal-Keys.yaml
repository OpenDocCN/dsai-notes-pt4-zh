- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:02:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:02:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ATP：通过关注顶部主键实现快速 LLM 服务
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02352](https://ar5iv.labs.arxiv.org/html/2403.02352)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02352](https://ar5iv.labs.arxiv.org/html/2403.02352)
- en: Yue Niu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 岳牛
- en: University of Southern California
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 南加州大学
- en: Los Angeles, CA, US
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 洛杉矶，加州，美国
- en: yueniu@usc.edu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: yueniu@usc.edu
- en: '&Saurav Prakash'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&萨乌拉夫·普拉卡什'
- en: University of Illinois Urbana-Champaign
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 伊利诺伊大学厄本那-香槟
- en: Urbana, IL, US
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 厄本那，伊利诺伊，美国
- en: sauravp2@illinois.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: sauravp2@illinois.edu
- en: '&Salman Avestimehr'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&萨尔曼·阿维斯特马赫'
- en: University of Southern California
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 南加州大学
- en: Los Angeles, CA, US
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 洛杉矶，加州，美国
- en: avestime@usc.edu
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: avestime@usc.edu
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We propose a new attention mechanism with linear complexity, ATP, that fixates
    Attention on Top Principal keys, rather than on each individual token. Particularly,
    ATP is driven by an important observation that input sequences are typically low-rank,
    i.e., input sequences can be represented by a few principal bases. Therefore,
    instead of directly iterating over all the input tokens, ATP transforms inputs
    into an orthogonal space and computes attention only on the top principal bases
    (keys). Owing to the observed low-rank structure in input sequences, ATP is able
    to capture semantic relationships in input sequences with a few principal keys.
    Furthermore, the attention complexity is reduced from *quadratic* to *linear*
    without incurring a noticeable performance drop. ATP further reduces complexity
    for other linear layers with low-rank inputs, leading to more speedup compared
    to prior works that solely target the attention module. Our evaluations on various
    models (e.g., BERT and Llama) demonstrate that ATP achieves comparable accuracy
    with much lower computation and memory complexity than the standard attention
    mechanism. In particular, ATP barely loses accuracy with only $1/2$ principal
    keys, and only incurs around $2\%$ accuracy drops with $1/4$ principal keys.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种具有线性复杂度的新注意力机制，ATP，它将注意力集中在顶部主键上，而不是每个单独的标记上。特别地，ATP 是基于一个重要的观察，即输入序列通常是低秩的，即输入序列可以由少数主基表示。因此，ATP
    不直接遍历所有输入标记，而是将输入转换为正交空间，仅对顶部主基（键）进行计算。由于输入序列中观察到的低秩结构，ATP 能够通过少数主键捕捉输入序列中的语义关系。此外，注意力复杂度从*二次*降低到*线性*，而不会显著影响性能。ATP
    进一步减少了对低秩输入的其他线性层的复杂度，与仅针对注意力模块的先前工作相比，带来了更多的加速。我们对各种模型（例如 BERT 和 Llama）的评估表明，ATP
    实现了与标准注意力机制相当的准确性，但计算和内存复杂度更低。特别地，ATP 在只有 $1/2$ 主键的情况下几乎没有准确性损失，而在只有 $1/4$ 主键的情况下仅造成约
    $2\%$ 的准确性下降。
- en: 'ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ATP：通过关注顶部主键实现快速 LLM 服务
- en: Yue Niu University of Southern California Los Angeles, CA, US yueniu@usc.edu
                           Saurav Prakash University of Illinois Urbana-Champaign
    Urbana, IL, US sauravp2@illinois.edu                        Salman Avestimehr
    University of Southern California Los Angeles, CA, US avestime@usc.edu
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 岳牛 南加州大学 洛杉矶，加州，美国 yueniu@usc.edu                        萨乌拉夫·普拉卡什 伊利诺伊大学厄本那-香槟
    厄本那，伊利诺伊，美国 sauravp2@illinois.edu                        萨尔曼·阿维斯特马赫 南加州大学 洛杉矶，加州，美国
    avestime@usc.edu
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transformers with self-attention have become a mainstream model architecture
    in many machine-learning tasks on natural language processing Wolf et al. ([2020](#bib.bib32)),
    and computer vision Khan et al. ([2022](#bib.bib13)); Dosovitskiy et al. ([2020](#bib.bib6)).
    In particular, owing to the attention mechanism, transformers have been demonstrated
    to be more effective in learning semantic relationships from input sequences.
    This drives transformers to become the backbone of current large language models
    (LLMs) like ChatGPT [OpenAI](#bib.bib19) and Copilot [Microsoft](#bib.bib16) .
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 带有自注意力的变换器已经成为许多自然语言处理任务中的主流模型架构（Wolf 等，([2020](#bib.bib32))，计算机视觉（Khan 等，([2022](#bib.bib13))；Dosovitskiy
    等，([2020](#bib.bib6))）。特别是，由于注意力机制，变换器在从输入序列中学习语义关系方面被证明更为有效。这推动变换器成为当前大型语言模型（LLMs）的核心，例如
    ChatGPT [OpenAI](#bib.bib19) 和 Copilot [Microsoft](#bib.bib16)。
- en: Despite their remarkable utility in real-world applications, transformers with
    standard self-attention, however, incur *quadratic* complexity in terms of sequence
    length (Vaswani et al., [2017](#bib.bib28)). To be specific, considering an input
    sequence with length $L$ (i.e., $L$ tokens), each attention layer needs $\mathcal{O}(L^{2})$
    computation and memory complexity on attention operations. Such a quadratic degree
    of complexity renders transformers difficult to scale with long input sequences.
    As a result, most LLM services at scale backed by transformers incur significant
    computation and memory footprints, which can only be afforded by large companies
    with sufficient computing power Samsi et al. ([2023](#bib.bib23)). To meet memory
    and computation resource constraints during deployment, some transformer models
    Devlin et al. ([2018](#bib.bib5)); Lan et al. ([2020](#bib.bib14)); Radford et al.
    ([2018](#bib.bib20)); Touvron et al. ([2023](#bib.bib27)) usually come with a
    hard constraint on sequence length. However, in many real-world tasks such as
    question-answering Wang et al. ([2019](#bib.bib30)), text summarization El-Kassas
    et al. ([2021](#bib.bib7)), enabling long sequence length is crucial for capturing
    semantic relationships in a broader context, and improving models’ performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管变压器在实际应用中具有显著的效用，但标准自注意力的变压器在序列长度上却会产生*平方*复杂度 (Vaswani 等人，[2017](#bib.bib28))。具体来说，考虑一个长度为
    $L$（即 $L$ 个标记）的输入序列，每个注意力层需要 $\mathcal{O}(L^{2})$ 的计算和内存复杂度。如此平方级别的复杂度使得变压器难以处理长输入序列。因此，基于变压器的大型语言模型服务通常会产生显著的计算和内存开销，这仅能由具有足够计算能力的大公司承担（Samsi
    等人，[2023](#bib.bib23)）。为了应对部署中的内存和计算资源限制，一些变压器模型（Devlin 等人，[2018](#bib.bib5)；Lan
    等人，[2020](#bib.bib14)；Radford 等人，[2018](#bib.bib20)；Touvron 等人，[2023](#bib.bib27)）通常对序列长度有严格限制。然而，在许多实际任务中，如问答（Wang
    等人，[2019](#bib.bib30)）和文本摘要（El-Kassas 等人，[2021](#bib.bib7)），启用较长的序列长度对于捕捉更广泛上下文中的语义关系及提升模型性能至关重要。
- en: Therefore, at the core of transformers and LLM services, lightweight self-attention
    mechanisms play a key role in improving model performance with longer sequences,
    as well as computation and memory efficiency in deployment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在变压器和大语言模型服务的核心，轻量级自注意力机制在提高模型性能、处理更长序列以及提升部署中的计算和内存效率方面发挥着关键作用。
- en: Current works on reducing the complexities of transformers can be categorized
    in two ways. The first line of research usually exploits redundancy in query/key/value
    matrices or attention maps, while the second approximates the Softmax-based attention
    with linear complexity.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 目前对减少变压器复杂性的研究可以分为两类。第一类研究通常利用查询/键/值矩阵或注意力图中的冗余，而第二类则通过线性复杂度来近似基于 Softmax 的注意力。
- en: Along the first line of works, Vyas et al. ([2020](#bib.bib29)) reduces attention
    complexity via clustering queries and only computes attention output for each
    cluster. Its performance hinges on the performance of clustering as well as the
    dimension in queries. On the other hand, Linformer Wang et al. ([2020](#bib.bib31))
    chooses to reduce the number of keys/values via a low-dimensional projection.
    A pre-defined or learnable projection layer is inserted into each attention layer.
    However, such a projection layer lacks a rigorous guarantee to preserve information
    in inputs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在首个研究方向中，Vyas 等人 ([2020](#bib.bib29)) 通过对查询进行聚类来减少注意力复杂性，仅计算每个簇的注意力输出。其性能依赖于聚类的效果以及查询的维度。另一方面，Linformer
    Wang 等人 ([2020](#bib.bib31)) 选择通过低维投影来减少键/值的数量。在每个注意力层中插入一个预定义的或可学习的投影层。然而，这种投影层缺乏严格的信息保留保证。
- en: Compared to simply approximating queries, keys, or values, another line of work
    approximates the Softmax-based attention using randomized feature mappingRahimi
    and Recht ([2007](#bib.bib21)). In these works, standard attention is regarded
    as a kernel method, and can be approximated with low-dimensional kernels. For
    instance, Performers (Choromanski et al., [2020](#bib.bib3)) shows that Softmax
    attention can be converted to a Gaussian kernel function. Therefore, self-attention
    can be potentially calculated at a lower dimension with linear complexity, as
    done in a Gaussian kernel function. Following such an idea, several works explore
    different kernels to approximate Softmax attention Katharopoulos et al. ([2020a](#bib.bib11)).
    However, the approximation methods with randomized feature mapping need to trade
    off approximation accuracy and the number of random features. A low approximation
    error needs more random features but increases approximation complexity Choromanski
    et al. ([2020](#bib.bib3)). Furthermore, while the aforementioned works reduce
    the complexity of the attention mechanism to linear, they still have not been
    seen in large-scale language models such as Llama Touvron et al. ([2023](#bib.bib27)).
    One reason is that these methods cannot effectively preserve information when
    performing attention in low dimensions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单近似查询、键或值不同，另一类工作使用随机特征映射来近似基于 Softmax 的注意力 Rahimi 和 Recht ([2007](#bib.bib21))。在这些工作中，标准注意力被视为一种核方法，可以用低维核进行近似。例如，Performers（Choromanski
    等，[2020](#bib.bib3)）显示 Softmax 注意力可以转化为高斯核函数。因此，自注意力可以在低维度下以线性复杂度进行计算，就像在高斯核函数中一样。根据这种思路，一些工作探索了不同的核来近似
    Softmax 注意力 Katharopoulos 等 ([2020a](#bib.bib11))。然而，使用随机特征映射的近似方法需要在近似准确性和随机特征数量之间进行权衡。较低的近似误差需要更多的随机特征，但会增加近似复杂度
    Choromanski 等 ([2020](#bib.bib3))。此外，虽然上述工作将注意力机制的复杂度降低到线性，但在像 Llama 这样的规模大型语言模型中仍未见到。一个原因是这些方法在低维度下进行注意力操作时无法有效保留信息。
- en: In this paper, we propose a new attention mechanism with linear complexity,
    that maintains model performance with significantly reduced computation and memory
    costs. The new attention mechanism, called ATP, is the first work that adapts
    self-attention with a low-rank structure in input embeddings. In particular, ATP
    first analyzes an input sequence’s structure and obtains orthogonal bases. We
    observe that input sequences usually exhibit a high correlation among tokens,
    with a few orthogonal bases being more important than the rest. Then ATP computes
    attention only on the top principal bases ( we call them *principal keys*), rather
    than iterating over all keys as in the standard attention layer. Owing to the
    low-rank structure in input sequences, the new self-attention mechanism with few
    principal keys/values is sufficient to capture semantic relationships among tokens.
    As a result, compute and memory complexity is reduced from quadratic to linear
    in terms of sequence length.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种具有线性复杂度的新注意力机制，能够在显著降低计算和内存成本的同时保持模型性能。这个新的注意力机制称为ATP，它是首个将自注意力与低秩结构相适应的输入嵌入方法。特别地，ATP首先分析输入序列的结构，并获得正交基。我们观察到输入序列通常在标记之间表现出较高的相关性，其中少数正交基比其余基更为重要。然后，ATP仅对顶级主基（我们称之为*主键*）计算注意力，而不是像标准注意力层那样遍历所有键。由于输入序列中的低秩结构，具有少量主键/值的新自注意力机制足以捕捉标记之间的语义关系。因此，计算和内存复杂度在序列长度方面从二次方降至线性。
- en: Furthermore, by exploiting low-rank structure in inputs, not only is the complexity
    of attention reduced, but also the complexity of other linear layers. Hence, ATP
    achieves further computation reductions compared to prior works focusing solely
    on the Softmax operation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过利用输入中的低秩结构，不仅注意力的复杂度降低了，其他线性层的复杂度也得到了降低。因此，与仅关注 Softmax 操作的先前工作相比，ATP 实现了进一步的计算减少。
- en: Our evaluations on various models (e.g., BERT and Llama) demonstrate ATP still
    maintains comparable accuracy with small fractional principal keys/values. In
    particular, with only $1/4$ principal keys, ATP achieves accuracy almost as the
    original model. With only $1/4$ principal keys, ATP only incurs around $2\%$ accuracy
    drop on BERT-base and Llama2 models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对各种模型（例如，BERT 和 Llama）的评估表明，ATP在使用少量主键/值时仍能保持相当的准确性。特别是，仅使用 $1/4$ 的主键时，ATP的准确性几乎与原始模型相同。仅使用
    $1/4$ 的主键时，ATP在 BERT-base 和 Llama2 模型上的准确性仅下降约 $2\%$。
- en: 2 Preliminaries and Related Works
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步研究与相关工作
- en: 2.1 Standard Self-Attention
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 标准自注意力
- en: 'Standard self-attention consists of three matrices: queries $Q$, keys $K$,
    and values $V\in\mathbb{R}^{L\times d^{\prime}}$, where $L$ is sequence length
    and $d^{\prime}$ is the hidden dimension. For each query vector $\bm{q}\in Q$,
    the self-attention applies dot-product with all keys, followed by a Softmax op
    to compute a score on each key. Each score denotes a weight on the corresponding
    values. Then the attention output $A(\bm{q})$ is obtained as a weighted average
    of all values:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 标准自注意力包括三个矩阵：查询$Q$、键$K$和值$V\in\mathbb{R}^{L\times d^{\prime}}$，其中$L$是序列长度，$d^{\prime}$是隐藏维度。对于每个查询向量$\bm{q}\in
    Q$，自注意力对所有键执行点积，然后通过Softmax操作计算每个键的得分。每个得分表示对应值的权重。然后，注意力输出$A(\bm{q})$作为所有值的加权平均获得：
- en: '|  | $A(\bm{q})=\texttt{Softmax}(\bm{q}\cdot K^{T}/\sqrt{d})\cdot V.$ |  |
    (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $A(\bm{q})=\texttt{Softmax}(\bm{q}\cdot K^{T}/\sqrt{d})\cdot V.$ |  |
    (1) |'
- en: The query/key/value matrices are obtained by projecting input $X\in\mathbb{R}^{L\times
    d}$ with parameter $W^{Q},W^{K},W^{V}\in\mathbb{R}^{d\times d^{\prime}}$ as
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 查询/键/值矩阵是通过用参数$W^{Q},W^{K},W^{V}\in\mathbb{R}^{d\times d^{\prime}}$投影输入$X\in\mathbb{R}^{L\times
    d}$获得的。
- en: '|  | $Q,K,V=X\cdot\{W^{Q},W^{K},W^{V}\}.$ |  | (2) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q,K,V=X\cdot\{W^{Q},W^{K},W^{V}\}.$ |  | (2) |'
- en: In essence, the self-attention mechanism finds the relations between a query
    and all keys, which are measured by probability after Softmax. Then, it averages
    corresponding values with the notion that a key closer to the query should be
    assigned larger weights (i.e., probability after Softmax).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，自注意力机制寻找查询与所有键之间的关系，这些关系通过Softmax后的概率进行度量。然后，它根据一个键离查询更近的观念，对相应的值进行加权平均，赋予更大的权重（即，Softmax后的概率）。
- en: 2.2 Related Works on Efficient Self-Attention
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 高效自注意力的相关工作
- en: Given an input sequence with length $L$, the standard self-attention needs to
    perform $L^{2}$ dot-products for all token vectors to get the whole attention
    map. As a result, it incurs complexity of $\mathcal{O}(L^{2})$ on computations
    and memory, which makes it difficult to scale with long inputs in many tasks.
    As a result, current LLM services usually require a significant amount of memory
    and computing power, in order to support long sequences. In some cases, facing
    actual resource constraints, some LLMs may need to limit the sequence length.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个长度为$L$的输入序列，标准自注意力需要对所有标记向量执行$L^{2}$次点积操作，以获取整个注意力图。因此，它在计算和内存上产生了$\mathcal{O}(L^{2})$的复杂度，这使得在许多任务中难以扩展到长输入。因此，目前的LLM服务通常需要大量内存和计算能力，以支持长序列。在某些情况下，面对实际资源限制，一些LLM可能需要限制序列长度。
- en: Current literature typically mitigates the limitation via exploiting sparsity
    or redundancy in attention matrices Roy et al. ([2021](#bib.bib22)); Sun et al.
    ([2021](#bib.bib26)); Vyas et al. ([2020](#bib.bib29)); Katharopoulos et al. ([2020a](#bib.bib11));
    Wang et al. ([2020](#bib.bib31)), or approximating the self-attention operation.
    For sparsity and redundancy in attention, exploiting low-rank structures in query/key/value
    and attention maps shows great potential. For instance, by exploring redundancy
    in input query vectors, Vyas et al. ([2020](#bib.bib29)) propose to first cluster
    query vectors, and use *cluster centroid vectors* to represent all query vectors
    of the same cluster. Hence, for all queries in the same cluster, it only needs
    to compute the attention score once on the centroid vector. With a reduced number
    of vectors when performing self-attention, it reduces the complexity of self-attention
    from quadratic to linear. However, the cost is a noticeable error by approximating
    many queries with the same cluster, thereby leading to performance degradation.
    On the other hand, Wang et al. ([2020](#bib.bib31)) project key and value matrices
    into a low-dimensional space. Specifically, with $r$ keys and values in the low-dimensional
    space, the method only needs to perform an attention op on $r$ keys rather than
    $L$ keys as in the standard self-attention mechanism. However, due to the fact
    that the projection matrix is pre-defined and learned from scratch, it is not
    guaranteed that the low-dimensional projection is effective in preserving information
    in the original key and value matrices Wang et al. ([2020](#bib.bib31)). Besides
    query/key/value matrices, Nguyen et al. ([2021](#bib.bib17)); Han et al. ([2023](#bib.bib8))
    directly exploit redundancy in the attention map, and approximate the attention
    map with low-rank and sparse matrices. Therefore, computation and memory costs
    of self-attention can also be reduced.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当前文献通常通过利用注意力矩阵中的稀疏性或冗余性来缓解这种限制（Roy et al. ([2021](#bib.bib22)); Sun et al.
    ([2021](#bib.bib26)); Vyas et al. ([2020](#bib.bib29)); Katharopoulos et al. ([2020a](#bib.bib11));
    Wang et al. ([2020](#bib.bib31))），或者通过近似自注意力操作来解决。对于注意力中的稀疏性和冗余性，利用查询/键/值及注意力图中的低秩结构显示出巨大的潜力。例如，通过探索输入查询向量中的冗余性，Vyas
    et al. ([2020](#bib.bib29)) 提出首先对查询向量进行聚类，并使用*聚类质心向量*来表示同一簇中的所有查询向量。因此，对于同一簇中的所有查询，只需在质心向量上计算一次注意力分数。通过减少自注意力时的向量数量，它将自注意力的复杂性从二次降低到线性。然而，由于通过将许多查询近似为同一簇，导致了明显的误差，从而造成了性能下降。另一方面，Wang
    et al. ([2020](#bib.bib31)) 将键和值矩阵投影到低维空间。具体而言，在低维空间中有 $r$ 个键和值时，该方法只需在 $r$ 个键上执行一次注意力操作，而不是像标准自注意力机制中那样在
    $L$ 个键上执行。然而，由于投影矩阵是预定义的并从头开始学习，因此不能保证低维投影在保留原始键和值矩阵中的信息方面是有效的（Wang et al. ([2020](#bib.bib31))）。除了查询/键/值矩阵外，Nguyen
    et al. ([2021](#bib.bib17)); Han et al. ([2023](#bib.bib8)) 直接利用注意力图中的冗余性，并使用低秩和稀疏矩阵来近似注意力图。因此，自注意力的计算和内存成本也可以减少。
- en: 'Besides removing redundancy in self-attention operations, current works along
    the second line of research attack the problem via approximating Softmax operations
    with kernelization. Typically, Choromanski et al. ([2020](#bib.bib3)) regard self-attention
    as Softmax kernels: $\texttt{exp}(\bm{q}\cdot\bm{k}^{T})$ with query $\bm{q}$
    and key $\bm{k}$, and approximate it with the Gaussian kernel functionRahimi and
    Recht ([2007](#bib.bib21)). Specifically, it estimates Softmax as: $\texttt{exp}(\bm{q}\cdot\bm{k}^{T})\to\mathbb{E}\left[\phi(\bm{q})\cdot\phi(\bm{k})^{T}\right]$,
    where kernel function $\phi(\cdot)$ maps a vector to a low-dimensional space.
    Therefore, the dimension after kernelization is reduced, leading to a reduction
    in self-attention operations. Along this line, other works (Katharopoulos et al.,
    [2020b](#bib.bib12); Nguyen et al., [2021](#bib.bib17)) explore different kernel
    functions to approximate the self-attention function. While the complexity is
    reduced, these kernel-based approximations still incur large Softmax approximation
    errors given large hidden dimensions in large models.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了去除自注意力操作中的冗余外，当前的研究还通过用核化方法逼近 Softmax 操作来解决问题。通常，Choromanski 等人 ([2020](#bib.bib3))
    将自注意力视为 Softmax 核：$\texttt{exp}(\bm{q}\cdot\bm{k}^{T})$，其中查询为 $\bm{q}$，键为 $\bm{k}$，并用高斯核函数（Rahimi
    和 Recht，[2007](#bib.bib21)）进行逼近。具体而言，它将 Softmax 估计为：$\texttt{exp}(\bm{q}\cdot\bm{k}^{T})\to\mathbb{E}\left[\phi(\bm{q})\cdot\phi(\bm{k})^{T}\right]$，其中核函数
    $\phi(\cdot)$ 将向量映射到低维空间。因此，核化后的维度减少，从而减少自注意力操作。在这方面，其他工作（Katharopoulos 等人，[2020b](#bib.bib12)；Nguyen
    等人，[2021](#bib.bib17)）探索了不同的核函数以近似自注意力函数。虽然复杂度有所降低，但考虑到大型模型中的隐藏维度较大，这些基于核的近似方法仍然会产生较大的
    Softmax 近似误差。
- en: Therefore, *a lightweight self-attention mechanism with linear complexity is
    still needed, especially for current large models with huge computation and memory
    footprints.*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*尽管目前已经有许多进展，仍然需要一种具有线性复杂度的轻量级自注意力机制，尤其是在计算和内存开销巨大的大型模型中。*
- en: 3 Lowrank Structure in Sequences
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 序列中的低秩结构
- en: Low-rank structures in inputs of language models are an essential component,
    that, surprisingly, is rarely exploited in current models for better computation
    and memory efficiency. Compared to model parameters Hu et al. ([2021](#bib.bib10)),
    inputs and internal hidden states are usually more correlated, which can be potentially
    exploited. Such a property has also been observed in vision problems Niu et al.
    ([2022](#bib.bib18)); Andriushchenko et al. ([2023](#bib.bib1)) and used to reduce
    the complexity of convolution operations. This paper is the first work that investigates
    the low-rank structure of input sequences in language models, and, importantly,
    its potential to computation and memory saving. In this section, we first analyze
    low-rank structures in transformers’ input sequence. Then, in the next section,
    we present ATP that leverages low-rank structures in inputs and performs self-attention
    with significantly reduced computation and memory footprints.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型输入中的低秩结构是一个基本组成部分，但令人惊讶的是，当前的模型很少利用这一点来提高计算和内存效率。与模型参数相比，Hu 等人 ([2021](#bib.bib10))
    发现输入和内部隐藏状态通常更加相关，这一特性有待挖掘。这样的特性在视觉问题中也有观察到（Niu 等人，[2022](#bib.bib18)；Andriushchenko
    等人，[2023](#bib.bib1)），并且被用于减少卷积操作的复杂度。本文是首个研究语言模型中输入序列的低秩结构的工作，重要的是，它还探讨了这一结构在计算和内存节省方面的潜力。在本节中，我们首先分析了
    Transformer 输入序列中的低秩结构。接下来，在下一节中，我们介绍了 ATP，该方法利用输入中的低秩结构并显著减少计算和内存开销来执行自注意力。
- en: Transformer models comprise a stack of self-attention layers. Each self-attention
    layer takes input state $X\in\mathbb{R}^{L\times d}$, and computes output state
    $Y\in\mathbb{R}^{L\times d}$, where $L$ denotes the sequence length, $d$ is the
    dimension of each hidden state vector. Each state vector corresponds to a token
    in the input sequence. Owning to the semantic relationships among tokens, these
    vectors are also correlated. To formally measure such correlations, we adopt a
    metric called SVD-Entropy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型由一系列自注意力层组成。每个自注意力层接收输入状态 $X\in\mathbb{R}^{L\times d}$，并计算输出状态
    $Y\in\mathbb{R}^{L\times d}$，其中 $L$ 表示序列长度，$d$ 是每个隐藏状态向量的维度。每个状态向量对应于输入序列中的一个令牌。由于令牌之间的语义关系，这些向量也是相关的。为了正式测量这种相关性，我们采用了一种叫做
    SVD-Entropy 的度量。
- en: In detail, we apply singular value decomposition (SVD) to the hidden state as
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们对隐藏状态应用奇异值分解（SVD），如下所示：
- en: '|  | $X\quad\xrightarrow[]{\texttt{SVD}}\quad\sum_{i=1}^{L}\sigma_{i}\cdot\bm{u}_{i}\cdot\bm{v}_{i}^{T}.$
    |  | (3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $X\quad\xrightarrow[]{\texttt{SVD}}\quad\sum_{i=1}^{L}\sigma_{i}\cdot\bm{u}_{i}\cdot\bm{v}_{i}^{T}.$
    |  | (3) |'
- en: 'We assume $L\leq d$ without loss of generality. With Eq([3](#S3.E3 "In 3 Lowrank
    Structure in Sequences ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys")), we attains singular values $\{\sigma_{i}\}$ and corresponding principal
    components $\{\bm{v}_{i}\}$. Then, based on Niu et al. ([2022](#bib.bib18)), we
    compute SVD-Entropy as the “low-rankness" of $X$,'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们假设 $L\leq d$，不失一般性。通过公式([3](#S3.E3 "In 3 Lowrank Structure in Sequences ‣
    ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys"))，我们获得了奇异值
    $\{\sigma_{i}\}$ 和相应的主成分 $\{\bm{v}_{i}\}$。然后，基于 Niu 等人 ([2022](#bib.bib18))，我们计算了
    SVD-Entropy 作为 $X$ 的“低秩性”。'
- en: '|  | $\mu=-\log\left(\sum\limits_{i=1}^{L}\bar{\sigma}_{i}^{2}\right),$ |  |
    (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu=-\log\left(\sum\limits_{i=1}^{L}\bar{\sigma}_{i}^{2}\right),$ |  |
    (4) |'
- en: where $\bar{\sigma}_{i}=\frac{\sigma_{i}}{\sum\limits_{i^{\prime}=1}^{L}\sigma_{i^{\prime}}}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{\sigma}_{i}=\frac{\sigma_{i}}{\sum\limits_{i^{\prime}=1}^{L}\sigma_{i^{\prime}}}$。
- en: According to Niu et al. ([2022](#bib.bib18)), $\left\lceil 2^{\mu}\right\rceil$
    can denote the number of necessary principal components to sufficiently approximate
    input $X$. $\left\lceil 2^{\mu}\right\rceil\ll L$ implies that input state vectors
    in $X$ are highly correlated such that only a few principal components are sufficient
    to represent $X$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Niu 等人 ([2022](#bib.bib18))，$\left\lceil 2^{\mu}\right\rceil$ 可以表示足够近似输入
    $X$ 所需的主成分数量。$\left\lceil 2^{\mu}\right\rceil\ll L$ 意味着 $X$ 中的输入状态向量高度相关，因此只需少量主成分即可表示
    $X$。
- en: 'With such a measure, we analyze the low-rank structure of hidden states in
    language models. Figure [1](#S3.F1 "Figure 1 ‣ 3 Lowrank Structure in Sequences
    ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys") shows the
    distribution of low-rankness after Llama-2’s embedding layer on BoolQ and MMLU
    datasets, measured by ratio $\left\lceil 2^{\mu}\right\rceil/L$. A small ratio
    implies that the embedding of a sequence is more low-rank. We can easily observe
    that embeddings of all sequences are highly low-rank, where $50\%$ or even fewer
    principal components are sufficient to approximate embedding vectors without error.
    Moreover, longer sequences usually exhibit more low-rank structure compared to
    shorter sequences. Note that the observation implies that exploiting the low-rankness
    of input data can be more effective compared to the low-rankness of models Hu
    et al. ([2021](#bib.bib10)). Such a crucial observation presents great potential
    for reducing the dimension of inputs, thereby leading to more efficient self-attention
    with reduced computation and memory complexities, especially for long sequences.
    Low-rankness analysis of other models is deferred to Appendix [A](#A1 "Appendix
    A Lowrank Structure in Other Model ‣ ATP: Enabling Fast LLM Serving via Attention
    on Top Principal Keys").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '使用这种度量，我们分析了语言模型中隐状态的低秩结构。图 [1](#S3.F1 "Figure 1 ‣ 3 Lowrank Structure in Sequences
    ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys") 显示了在 Llama-2
    的嵌入层后，BoolQ 和 MMLU 数据集上的低秩性分布，测量方法为比例 $\left\lceil 2^{\mu}\right\rceil/L$。较小的比例意味着序列的嵌入更具低秩性。我们可以很容易地观察到，所有序列的嵌入都具有高度的低秩性，其中
    $50\%$ 或更少的主成分足以在没有误差的情况下近似嵌入向量。此外，相较于较短的序列，较长的序列通常表现出更多的低秩结构。值得注意的是，这一观察结果表明，相比于模型的低秩性，利用输入数据的低秩性可能更加有效
    Hu 等人 ([2021](#bib.bib10))。这一重要观察结果展示了降低输入维度的巨大潜力，从而实现更高效的自注意力，减少计算和内存复杂度，尤其是对于长序列。其他模型的低秩性分析推迟到附录
    [A](#A1 "Appendix A Lowrank Structure in Other Model ‣ ATP: Enabling Fast LLM
    Serving via Attention on Top Principal Keys")。'
- en: 0.20.30.40.510203040$\left\lceil
    2^{\mu}\right\rceil/L$probability density$L\in[0,300]$$L\in[300,600]$$L\in[600,]$
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 0.20.30.40.510203040$\left\lceil
    2^{\mu}\right\rceil/L$probability density$L\in[0,300]$$L\in[300,600]$$L\in[600,]$
- en: (a) MMLU
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MMLU
- en: 0.10.20.30.40.551015$\left\lceil
    2^{\mu}\right\rceil/L$probability density$L\in[0,100]$$L\in[100,200]$$L\in[200,]$
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 0.10.20.30.40.551015$\left\lceil
    2^{\mu}\right\rceil/L$probability density$L\in[0,100]$$L\in[100,200]$$L\in[200,]$
- en: (b) BoolQ
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (b) BoolQ
- en: 'Figure 1: Distribution of low-rankness of Llama-2’s embedding on MMLU and BoolQ
    dataset, measured by ratio $\left\lceil 2^{\mu}\right\rceil/L$. Almost all sequences
    can be sufficiently approximated with less than half principal components without
    incurring error. Longer sequences exhibit a more low-rank structure.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: Llama-2 在 MMLU 和 BoolQ 数据集上的嵌入低秩性分布，测量方法为比例 $\left\lceil 2^{\mu}\right\rceil/L$。几乎所有序列都可以用不到一半的主成分进行充分近似而不会产生误差。较长的序列表现出更多的低秩结构。'
- en: 4 ATP Methodology
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 ATP 方法论
- en: In this section, we introduce ATP, a generic transformer architecture with a
    new efficient self-attention. ATP introduces a rank-aware self-attention mechanism
    that reduces the complexity of self-attention to linear given the low-rank structure
    in input sequence embeddings.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了 ATP，这是一种具有新型高效自注意力的通用 Transformer 架构。ATP 引入了一种基于秩的自注意力机制，使得在输入序列嵌入中低秩结构的情况下，自注意力的复杂度降至线性。
- en: 4.1 Self-Attention with Low-Rank Inputs
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 具有低秩输入的自注意力
- en: Given low-rank input $X\in\mathbb{R}^{L\times d}$ with $r$ principal components,
    we write it as
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 给定低秩输入 $X\in\mathbb{R}^{L\times d}$ 以及 $r$ 个主成分，我们将其写作
- en: '|  | $X=U\cdot X^{\prime},$ |  | (5) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $X=U\cdot X^{\prime},$ |  | (5) |'
- en: where $U\in\mathbb{R}^{L\times r}$, and $X^{\prime}\in\mathbb{R}^{r\times d}$
    denotes the principal components. Since $X$ is low-rank, query/key/values matrices
    obtained by projecting $X$ are also low-rank. That is,
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U\in\mathbb{R}^{L\times r}$，$X^{\prime}\in\mathbb{R}^{r\times d}$ 表示主成分。由于
    $X$ 是低秩的，通过将 $X$ 投影得到的查询/键/值矩阵也是低秩的。即，
- en: '|  | $\begin{split}Q,K,V&amp;=U\cdot X^{\prime}\cdot\left\{W^{Q},W^{K},W^{V}\right\}\\
    &amp;=U\cdot\left\{Q^{\prime},K^{\prime},V^{\prime}\right\}.\end{split}$ |  |
    (6) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}Q,K,V&amp;=U\cdot X^{\prime}\cdot\left\{W^{Q},W^{K},W^{V}\right\}\\
    &amp;=U\cdot\left\{Q^{\prime},K^{\prime},V^{\prime}\right\}.\end{split}$ |  |
    (6) |'
- en: By the matrix rank inequality Banerjee and Roy ([2014](#bib.bib2)), we have
    $\texttt{rank}(\left\{Q,K,V\right\})\leq\texttt{rank}(X^{\prime})=r$.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Banerjee 和 Roy 的矩阵秩不等式 ([2014](#bib.bib2))，我们有 $\texttt{rank}(\left\{Q,K,V\right\})\leq\texttt{rank}(X^{\prime})=r$。
- en: Then we start from the standard self-attention, and show the computations can
    be significantly reduced with low-rank keys/values. We omit the normalization
    in Softmax and write self-attention with query $\bm{q}$ on all keys/values as
    $\texttt{exp}(\bm{q},K^{T})\cdot V$. With low-rankness of input $X$, we can break
    down the self-attention as
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从标准自注意力开始，展示了使用低秩键/值可以显著减少计算量。我们省略了 Softmax 中的归一化，并将查询 $\bm{q}$ 在所有键/值上的自注意力写作
    $\texttt{exp}(\bm{q},K^{T})\cdot V$。利用输入 $X$ 的低秩性，我们可以将自注意力分解为
- en: '|  | $\begin{split}\texttt{exp}(\bm{q}\cdot K^{T})\cdot V=\texttt{exp}(\bm{q}\cdot
    K^{\prime T}\cdot U^{T})\cdot U\cdot V^{\prime}\end{split}$ |  | (7) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\texttt{exp}(\bm{q}\cdot K^{T})\cdot V=\texttt{exp}(\bm{q}\cdot
    K^{\prime T}\cdot U^{T})\cdot U\cdot V^{\prime}\end{split}$ |  | (7) |'
- en: By the Taylor expansion on the exp function on ech value, we have the following
    approximation,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对每个值的 exp 函数进行泰勒展开，我们得到以下近似，
- en: '|  | $$\begin{split}&amp;\texttt{exp}(\bm{q},K^{T})\cdot V\\ &amp;\simeq\bm{1}\cdot
    U\cdot V^{\prime}+\bm{q}\cdot K^{\prime T}\cdot U^{T}\cdot U\cdot V^{\prime}\\'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{split}&amp;\texttt{exp}(\bm{q},K^{T})\cdot V\\ &amp;\simeq\bm{1}\cdot
    U\cdot V^{\prime}+\bm{q}\cdot K^{\prime T}\cdot U^{T}\cdot U\cdot V^{\prime}\\'
- en: '&amp;=\bm{1}\cdot U\cdot V^{\prime}+\bm{q}\cdot K^{\prime T}\cdot V^{\prime}\\'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=\bm{1}\cdot U\cdot V^{\prime}+\bm{q}\cdot K^{\prime T}\cdot V^{\prime}\\'
- en: '&amp;=(\bm{1}\cdot U+\bm{q}\cdot K^{\prime T})\cdot V^{\prime}=A^{\prime}\cdot
    V^{\prime},\end{split}$$ |  | (8) |'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=(\bm{1}\cdot U+\bm{q}\cdot K^{\prime T})\cdot V^{\prime}=A^{\prime}\cdot
    V^{\prime},\end{split}$$ |  | (8) |'
- en: where $\bm{1}\in\mathbb{R}^{1\times L}$, and $U^{T}\cdot U=I$. Similar as Softmax,
    normalization is applied row-wise on the new attention map $A^{\prime}$.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{1}\in\mathbb{R}^{1\times L}$，且 $U^{T}\cdot U=I$。与 Softmax 类似，对新注意力图
    $A^{\prime}$ 应用按行归一化。
- en: 'Eq([8](#S4.E8 "In 4.1 Self-Attention with Low-Rank Inputs ‣ 4 ATP Methodology
    ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys")) shows
    that self-attention on all token vectors $X$ can be converted to attention on
    all principal keys $K^{\prime}$. More importantly, different from the standard
    self-attention where each key corresponds to a token in the input sequence, these
    principal keys denote all principal bases drawn from $X^{\prime}$. That is, *ATP
    converts the attention operation from individual token vectors to principal basis
    vectors.* The observation is very crucial given low-rank input $X$. The reason
    is that, given low-rank input with $r\ll L$, based on Eq([8](#S4.E8 "In 4.1 Self-Attention
    with Low-Rank Inputs ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via
    Attention on Top Principal Keys")), for each query, we only need to perform dot-product
    on $r$ vectors, rather than $L$ vectors as in the standard self-attention. Therefore,
    the self-attention does not incur $\mathcal{O}(L^{2})$ computation and memory
    costs. Instead, the costs scale linearly with sequence length, $L$, and the number
    of principal components, $r$. Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Self-Attention
    with Low-Rank Inputs ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via
    Attention on Top Principal Keys") shows a point-to-point comparison between the
    standard self-attention and the low-rank self-attention. The low-rank self-attention
    shares a similar procedure as the stand self-attention, while the difference is
    that the low-rank self-attention performs dot-product on $r$ principal keys.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Eq([8](#S4.E8 "在4.1节 自注意力与低秩输入 ‣ 4 ATP方法论 ‣ ATP：通过对主要主键的注意力实现快速LLM服务")) 显示，所有token向量
    $X$ 的自注意力可以转换为对所有主要主键 $K^{\prime}$ 的注意力。更重要的是，与每个键对应于输入序列中的token的标准自注意力不同，这些主要主键表示从
    $X^{\prime}$ 中提取的所有主要基。也就是说，*ATP将注意力操作从单个token向量转换为主要基向量。* 鉴于低秩输入 $X$，这一观察非常关键。原因是，鉴于低秩输入且
    $r\ll L$，根据 Eq([8](#S4.E8 "在4.1节 自注意力与低秩输入 ‣ 4 ATP方法论 ‣ ATP：通过对主要主键的注意力实现快速LLM服务"))，对于每个查询，我们只需在
    $r$ 个向量上执行点积，而不是像标准自注意力中那样在 $L$ 个向量上执行。因此，自注意力不会产生 $\mathcal{O}(L^{2})$ 的计算和内存开销。相反，开销与序列长度
    $L$ 和主要成分数量 $r$ 线性相关。图 [2](#S4.F2 "图2 ‣ 4.1节 自注意力与低秩输入 ‣ 4 ATP方法论 ‣ ATP：通过对主要主键的注意力实现快速LLM服务")
    展示了标准自注意力和低秩自注意力之间的逐点比较。低秩自注意力与标准自注意力有类似的过程，但不同之处在于，低秩自注意力在 $r$ 个主要主键上执行点积。
- en: '$i$-th
    Query: $\bm{q}\in\mathbb{R}^{d^{\prime}}$ $K_{1}$$K_{2}$$\vdots$$K_{L}$Key$A_{1}$$A_{2}$$\vdots$$A_{L}$$A_{L}$Attention$V_{1}$$V_{2}$$\cdots$$V_{L}$Value +
    Query
    output'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对不起，您提供的文本似乎是 SVG 格式的图像代码，而不是直接的文本内容。请问您是否有特定的文本内容需要翻译？
- en: (a) Standard self-attention.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 标准自注意力。
- en: '$i$-th
    Query: $\bm{q}\in\mathbb{R}^{d^{\prime}}$ $K^{\prime}_{1}$$K^{\prime}_{2}$$\vdots$$K^{\prime}_{r}$Principal
    Key$A^{\prime}_{1}$$A^{\prime}_{2}$$\vdots$$A^{\prime}_{r}$$A^{\prime}_{r}$Attention$V^{\prime}_{1}$$V^{\prime}_{2}$$\cdots$$V^{\prime}_{r}$Principal
    Value +
    Query
    output'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`$i$-th
    Query: $\bm{q}\in\mathbb{R}^{d^{\prime}}$ $K^{\prime}_{1}$$K^{\prime}_{2}$$\vdots$$K^{\prime}_{r}$Principal
    Key$A^{\prime}_{1}$$A^{\prime}_{2}$$\vdots$$A^{\prime}_{r}$$A^{\prime}_{r}$Attention$V^{\prime}_{1}$$V^{\prime}_{2}$Low-Rank
    Attention $W^{Q}$
    $W^{K}$ $W^{V}$
    SVD Norm
    Feedforward $X$$Y$$X^{\prime}$Projection
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该文本包含一个复杂的 SVG 图形，没有文字内容需要翻译。请提供其他文本或说明。
- en: 'Figure 3: Transformer encoder/decoder with low-rank self-attention. Input $X$
    is first fed to SVD to attain the principal components, $X^{\prime}$. Then, $X^{\prime}$
    is fed to an encoder/decoder layer with low-rank self-attention.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：具有低秩自注意力的Transformer编码器/解码器。输入$X$首先经过SVD以获得主成分$X^{\prime}$。然后，将$X^{\prime}$输入到具有低秩自注意力的编码器/解码器层中。
- en: 'Then, principal components $X^{\prime}$ are fed into a self-attention layer
    to attain principal keys and values as in Eq([6](#S4.E6 "In 4.1 Self-Attention
    with Low-Rank Inputs ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via
    Attention on Top Principal Keys")). With the principal keys and values, ATP performs
    attention as in Eq([8](#S4.E8 "In 4.1 Self-Attention with Low-Rank Inputs ‣ 4
    ATP Methodology ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys")), and feedforward to obtain output states $Y$. The next encoder/decoder
    layer follows the same procedure first to attain principal components of $Y$ and
    perform low-rank self-attention.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将主成分$X^{\prime}$输入到自注意力层中，以获得主键和值，如公式([6](#S4.E6 "在4.1低秩输入的自注意力 ‣ 4 ATP方法
    ‣ ATP:通过主键上的注意力实现快速LLM服务"))。使用主键和值，ATP按照公式([8](#S4.E8 "在4.1低秩输入的自注意力 ‣ 4 ATP方法
    ‣ ATP:通过主键上的注意力实现快速LLM服务"))执行注意力操作，并通过前馈网络获得输出状态$Y$。接下来的编码器/解码器层遵循相同的过程，首先获取$Y$的主成分，并执行低秩自注意力。
- en: Combine with Position Encoding. For absolution or relative position encoding
    vectors $P$ for a sequence Devlin et al. ([2018](#bib.bib5)); Lan et al. ([2020](#bib.bib14));
    Shaw et al. ([2018](#bib.bib24)), they are added to token embeddings before an
    encoder/decoder layer. Therefore, we can still directly apply SVD to the input
    vectors, $X+P$, and obtain principal components for low-rank self-attention.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结合位置编码。对于序列的绝对或相对位置编码向量$P$，Devlin等（[2018](#bib.bib5)）；Lan等（[2020](#bib.bib14)）；Shaw等（[2018](#bib.bib24)），它们在编码器/解码器层之前被添加到令牌嵌入中。因此，我们仍然可以直接对输入向量$X+P$应用SVD，获得低秩自注意力的主成分。
- en: 'For rotatory position embedding Su et al. ([2021](#bib.bib25)); Touvron et al.
    ([2023](#bib.bib27)), the position encoding vectors are added after query/key
    projection. That is, $\bm{k}=\bm{x}\cdot W^{K}\cdot R_{i}$, where $R_{i}$ is a
    rotary matrix corresponding to a transformation for a token at position $i$, $\bm{x}$
    denotes one input vector in $X$. While the low-rank structure might change during
    the rotation, we can still attain a low-rank key matrix by projecting the key
    matrix into a low-dimension space with $U$ as in Eq([8](#S4.E8 "In 4.1 Self-Attention
    with Low-Rank Inputs ‣ 4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via
    Attention on Top Principal Keys")).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旋转位置嵌入，Su等（[2021](#bib.bib25)）；Touvron等（[2023](#bib.bib27)），位置编码向量在查询/键投影之后添加。即，$\bm{k}=\bm{x}\cdot
    W^{K}\cdot R_{i}$，其中$R_{i}$是对应于位置$i$的令牌转换的旋转矩阵，$\bm{x}$表示$X$中的一个输入向量。虽然旋转过程中低秩结构可能会发生变化，但我们仍然可以通过将键矩阵投影到低维空间中来获得低秩键矩阵，如公式([8](#S4.E8
    "在4.1低秩输入的自注意力 ‣ 4 ATP方法 ‣ ATP:通过主键上的注意力实现快速LLM服务"))。
- en: Therefore, the low-rank self-attention mechanism is compatible with current
    position encoding methods.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，低秩自注意力机制与当前的位置编码方法兼容。
- en: '| Mechanism | Standard | Low-rank |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 机制 | 标准 | 低秩 |'
- en: '| --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Computation | Complexity | Memory | Computation | Complexity | Memory
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 计算 | 复杂度 | 内存 | 计算 | 复杂度 | 内存 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Projection | $X\cdot W$ | $\mathcal{O}(Ldd^{\prime})$ | $\mathcal{O}(Ld^{\prime})$
    | $X^{\prime}\cdot W$ | $\mathcal{O}(rdd^{\prime})$ | $\mathcal{O}(rd^{\prime})$
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Projection | $X\cdot W$ | $\mathcal{O}(Ldd^{\prime})$ | $\mathcal{O}(Ld^{\prime})$
    | $X^{\prime}\cdot W$ | $\mathcal{O}(rdd^{\prime})$ | $\mathcal{O}(rd^{\prime})$
    |'
- en: '| Attention | $Q\cdot K^{T}$ | $\mathcal{O}(L^{2}d^{\prime})$ | $\mathcal{O}(L^{2})$
    | $QK^{\prime T}$ | $\mathcal{O}(rLd^{\prime})$ | $\mathcal{O}(rL)$ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 注意力 | $Q\cdot K^{T}$ | $\mathcal{O}(L^{2}d^{\prime})$ | $\mathcal{O}(L^{2})$
    | $QK^{\prime T}$ | $\mathcal{O}(rLd^{\prime})$ | $\mathcal{O}(rL)$ |'
- en: 'Table 1: Computation and memory complexity with low-rank input. Low-rank self-attention
    reduces the complexity of attention from quadratic to linear. It also reduces
    complexities for other linear layers ($L$: sequence length, $r$: rank, $d$: dimension
    of $X$, $d^{\prime}$: dimension of hidden state).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：低秩输入的计算和内存复杂度。低秩自注意力将注意力的复杂度从平方级降低到线性级。它还减少了其他线性层的复杂度（$L$：序列长度，$r$：秩，$d$：$X$的维度，$d^{\prime}$：隐藏状态的维度）。
- en: 4.3 Complexity Analysis
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 复杂性分析
- en: 'Self-attention with low-rank inputs not only relieves computation and memory
    pressure for attention operations, but also reduces complexity for other linear
    layers. Table [1](#S4.T1 "Table 1 ‣ 4.2 Tansformers with Low-Rank Attention ‣
    4 ATP Methodology ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys") lists computations and the corresponding complexity of the standard and
    low-rank self-attention. Due to the reduced number of components in $X^{\prime}$,
    query/key/value projection only needs to project $r$ vectors rather than $L$ token
    vectors as the standard self-attention, thereby resulting in $r$ keys and value
    vectors with dimension $d^{\prime}$. Hence, both the computation and memory during
    the projection are reduced by $L/r$. On the other hand, when performing attention,
    the low-rank attention only needs to compute the attention score on $r$ principal
    keys, rather than $L$ token keys as the standard self-attention. Therefore, the
    computation and memory complexities are also reduced by $L/r$. Note that the additional
    SVD only incurs computation complexity of $\mathcal{O}(rLd)$, which is linear
    in term of $L$, and is relatively small compared to computations in the standard
    self-attention. In addition, more computation saving can be achieved by decomposing
    hidden state vectors to the FeedForward layer. In this paper, we mainly focus
    on self-attention layers.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '低秩输入的自注意力不仅减轻了注意力操作的计算和内存压力，还减少了其他线性层的复杂性。表[1](#S4.T1 "表 1 ‣ 4.2 低秩注意力的变换器
    ‣ 4 ATP 方法论 ‣ ATP: 通过关注主要键实现快速 LLM 服务") 列出了标准自注意力和低秩自注意力的计算量及相应复杂性。由于$X^{\prime}$中组件数量的减少，查询/键/值投影只需要投影$r$个向量，而不是像标准自注意力那样投影$L$个标记向量，从而得到$r$个键和值向量，维度为$d^{\prime}$。因此，投影过程中的计算和内存减少了$L/r$。另一方面，在进行注意力计算时，低秩注意力只需要计算$r$个主要键上的注意力得分，而不是像标准自注意力那样计算$L$个标记键。因而，计算和内存复杂度也减少了$L/r$。请注意，额外的SVD仅带来$\mathcal{O}(rLd)$的计算复杂度，这在$L$方面是线性的，相较于标准自注意力中的计算要小得多。此外，通过将隐藏状态向量分解到前馈层中，可以实现更多的计算节省。本文主要集中在自注意力层上。'
- en: 5121024204840968192.2.4.6.8sequence
    lengthnormalized timelow-rankstandard
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 5121024204840968192.2.4.6.8sequence
    lengthnormalized timelow-rankstandard
- en: 'Figure 4: Actual running time of low-rank self-attention compared to the standard
    mechanism with different sequence lengths ($r$=128). The running time of the standard
    self-attention increases quadratically with the sequence length. Low-rank self-attention
    reduces the running time to almost linear.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：低秩自注意力与标准机制在不同序列长度下的实际运行时间（$r$=128）。标准自注意力的运行时间随序列长度呈二次增长。低秩自注意力将运行时间减少到接近线性。
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Complexity Analysis ‣ 4 ATP Methodology ‣
    ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys") shows actual
    speedups of low-rank self-attention compared to the standard self-attention given
    different sequence lengths. Note that the standard self-attention, as expected,
    incurs quadratic running time with increasing input sequence length. On the other
    hand, the running time of the low-rank self-attention scales almost linearly with
    sequence length. The time gap between them grows rapidly with long sequences.
    This shows that the standard self-attention indeed comes with a severe bottleneck
    on real performance with long sequences, while the low-rank self-attention significantly
    reduces actual running time.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S4.F4 "图 4 ‣ 4.3 复杂性分析 ‣ 4 ATP 方法论 ‣ ATP: 通过关注主要键实现快速 LLM 服务") 显示了低秩自注意力与标准自注意力在不同序列长度下的实际加速效果。请注意，正如预期的那样，标准自注意力随着输入序列长度的增加会产生二次运行时间。另一方面，低秩自注意力的运行时间几乎线性地随序列长度变化。它们之间的时间差距在长序列下迅速增长。这表明，标准自注意力在长序列的实际性能上确实存在严重瓶颈，而低秩自注意力显著减少了实际运行时间。'
- en: 5 Empirical Evaluation
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实证评估
- en: In this section, we evaluate the low-rank attention on benchmark models and
    datasets. To investigate the applicability of low-rank attention in a wide range
    of applications, we choose models with different sizes. For datasets, we focus
    on long sequences, which usually incur significant computation and memory pressure
    during inference.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了基准模型和数据集上的低秩注意力。为了研究低秩注意力在广泛应用中的适用性，我们选择了不同规模的模型。对于数据集，我们专注于长序列，这通常在推断过程中会产生显著的计算和内存压力。
- en: 'Model. We choose BERT-base (encoders only) as the small model Devlin et al.
    ([2018](#bib.bib5)), Llama2-7B (decoder only) as the medium model, and Llama2-13B
    as the large model Touvron et al. ([2023](#bib.bib27)). Table [2](#S5.T2 "Table
    2 ‣ 5 Empirical Evaluation ‣ ATP: Enabling Fast LLM Serving via Attention on Top
    Principal Keys") lists their detailed architecture parameters. Note that all three
    models adopt the standard self-attention mechanism.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '模型。我们选择BERT-base（仅编码器）作为小型模型 Devlin等（[2018](#bib.bib5)），Llama2-7B（仅解码器）作为中型模型，Llama2-13B作为大型模型
    Touvron等（[2023](#bib.bib27)）。表[2](#S5.T2 "表 2 ‣ 5 实证评估 ‣ ATP: 通过注意力在主成分键上实现快速LLM服务")列出了它们的详细架构参数。注意，所有三个模型均采用标准自注意力机制。'
- en: '|  | BERT | Llama2-7B | Llama2-13B |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | BERT | Llama2-7B | Llama2-13B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| # att layers | 12 | 32 | 40 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| # att layers | 12 | 32 | 40 |'
- en: '| # heads/layer | 12 | 32 | 40 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| # heads/layer | 12 | 32 | 40 |'
- en: '| # head dim | 64 | 128 | 128 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| # head dim | 64 | 128 | 128 |'
- en: 'Table 2: Architecture parameters of BERT-base, Llama2-7b and Llama2-13B.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：BERT-base、Llama2-7b和Llama2-13B的架构参数。
- en: 'Datasets. For BERT-base, we choose SST-2, Squad Wang et al. ([2019](#bib.bib30)),
    and IMDBMaas et al. ([2011](#bib.bib15)). In particular, the IMDB dataset consists
    of long sequences that exhibit more low-rank structures. For Llama2-7B and Llama2-13B,
    we choose two of the official benchmark datasets: MMLU Hendrycks et al. ([2021](#bib.bib9))
    and BoolQ Clark et al. ([2019](#bib.bib4)).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。对于BERT-base，我们选择了SST-2、Squad Wang等（[2019](#bib.bib30)）和IMDB Maas等（[2011](#bib.bib15)）。特别地，IMDB数据集包含长序列，这些序列显示出更多的低秩结构。对于Llama2-7B和Llama2-13B，我们选择了两个官方基准数据集：MMLU
    Hendrycks等（[2021](#bib.bib9)）和BoolQ Clark等（[2019](#bib.bib4)）。
- en: 5.1 BERT-base
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 BERT-base
- en: 'For all datasets, we start from a pre-trained model, replace each self-attention
    layer with the low-rank self-attention, and finetune the model. Owing to the model
    size, we finetune full parameters. Training details are provided in Appendix [C](#A3
    "Appendix C Finetune Hyperparameters ‣ ATP: Enabling Fast LLM Serving via Attention
    on Top Principal Keys"). Table [3](#S5.T3 "Table 3 ‣ 5.1 BERT-base ‣ 5 Empirical
    Evaluation ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys")
    lists the final model accuracy on SST-2, Squad, and IMDB. We can observe that
    BERT-base with low-rank self-attention preserves models’ performance. In particular,
    with $1/2$ principal keys used, the model with low-rank self-attention barely
    loses accuracy. This indicates that owing to the low-rank structure in sequences,
    $1/2$ principal keys preserve most information in inputs. Surprisingly, we can
    further see that even only keeping $1/8$ principal keys, the model still achieves
    a comparable accuracy as the model with standard self-attention.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '对于所有数据集，我们从一个预训练模型开始，将每个自注意力层替换为低秩自注意力，并对模型进行微调。由于模型的大小，我们微调了所有参数。训练细节见附录[C](#A3
    "附录 C 微调超参数 ‣ ATP: 通过注意力在主成分键上实现快速LLM服务")。表[3](#S5.T3 "表 3 ‣ 5.1 BERT-base ‣ 5
    实证评估 ‣ ATP: 通过注意力在主成分键上实现快速LLM服务")列出了SST-2、Squad和IMDB上的最终模型准确率。我们可以观察到，BERT-base使用低秩自注意力可以保持模型的性能。特别是，使用$1/2$的主键时，低秩自注意力模型几乎没有失去准确性。这表明，由于序列中的低秩结构，$1/2$的主键保留了输入中的大部分信息。令人惊讶的是，即使只保留$1/8$的主键，模型的准确率仍与标准自注意力模型相当。'
- en: '| Model | Original | 1/2 | 1/4 | 1/8 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 原始 | 1/2 | 1/4 | 1/8 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SST-2 | $92.32\pm 0.2$ | $92.1\pm 0.17$ | $91.0\pm 0.23$ | $89.2\pm 0.26$
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | $92.32\pm 0.2$ | $92.1\pm 0.17$ | $91.0\pm 0.23$ | $89.2\pm 0.26$
    |'
- en: '| Squad | $88.15\pm 0.3$ | $87.93\pm 0.2$ | $87.23\pm 0.34$ | $84.94\pm 0.28$
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Squad | $88.15\pm 0.3$ | $87.93\pm 0.2$ | $87.23\pm 0.34$ | $84.94\pm 0.28$
    |'
- en: '| IMDB | $91.45\pm 0.2$ | $90.97\pm 0.19$ | $89.65\pm 0.3$ | $87.28\pm 0.3$
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| IMDB | $91.45\pm 0.2$ | $90.97\pm 0.19$ | $89.65\pm 0.3$ | $87.28\pm 0.3$
    |'
- en: 'Table 3: BERT-base accuracy on SST-2, Squad, and IMDB using low-rank self-attention.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：使用低秩自注意力的BERT-base在SST-2、Squad和IMDB上的准确率。
- en: 1/81/41/23/4full809010060complexityenergy
    ratio(%)SST-2SquadIMDB
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 1/81/41/23/4full809010060complexityenergy
    ratio(%)SST-2SquadIMDB
- en: 'Figure 5: Energy ratio ($\left\|X^{\prime}\right\|^{2}_{F}/\left\|X\right\|^{2}_{F}$)
    in low-rank hidden representations. Embeddings of all three datasets exhibit highly
    low-rank structures, with $1/2$ principal components preserving almost all energy.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：低秩隐藏表示中的能量比（$\left\|X^{\prime}\right\|^{2}_{F}/\left\|X\right\|^{2}_{F}$）。所有三种数据集的嵌入显示出高度的低秩结构，$1/2$的主成分几乎保留了所有能量。
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.1 BERT-base ‣ 5 Empirical Evaluation ‣ ATP:
    Enabling Fast LLM Serving via Attention on Top Principal Keys") shows the relative
    energy kept in the low-rank keys. We observe that for $1/2$ principal keys are
    sufficient to keep almost all energy in inputs, which is aligned with model accuracy
    in Table [3](#S5.T3 "Table 3 ‣ 5.1 BERT-base ‣ 5 Empirical Evaluation ‣ ATP: Enabling
    Fast LLM Serving via Attention on Top Principal Keys"). On the other hand, compared
    to Squad and IMDB, SST-2 exhibits a more low-rank structure, with even $1/8$ principal
    keys still preserving near $90\%$ energy. The observation explains BERT-base’s
    performance on SST-2 that even low-rank self-attention with only $1/8$ principal
    keys only incurs a $\sim 3\%$ accuracy drop.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#S5.F5 "Figure 5 ‣ 5.1 BERT-base ‣ 5 Empirical Evaluation ‣ ATP: Enabling
    Fast LLM Serving via Attention on Top Principal Keys") 显示了低秩键中保留的相对能量。我们观察到，对于
    $1/2$ 的主键足以保留输入中的几乎所有能量，这与表 [3](#S5.T3 "Table 3 ‣ 5.1 BERT-base ‣ 5 Empirical
    Evaluation ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys")
    中的模型准确性一致。另一方面，与 Squad 和 IMDB 相比，SST-2 显示了更低秩的结构，即使 $1/8$ 的主键仍能保留接近 $90\%$ 的能量。这一观察解释了
    BERT-base 在 SST-2 上的表现，即使低秩自注意力只有 $1/8$ 的主键，也仅导致约 $3\%$ 的准确性下降。'
- en: 5.2 Llama2
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 Llama2
- en: 'We obtain pre-trained Llama2-7B/13B models from the Hugging Face repo ¹¹1[https://huggingface.co/meta-llama](https://huggingface.co/meta-llama).
    Starting from the pre-trained models, we replace their attention layers with low-rank
    self-attention. For MMLU and BoolQ, since they have different formats, we will
    first finetune the model on the datasets for a few iterations (See Appendix [C](#A3
    "Appendix C Finetune Hyperparameters ‣ ATP: Enabling Fast LLM Serving via Attention
    on Top Principal Keys") for more finetuning parameters), and then evaluate their
    performance on the validation dataset. Appendix [D](#A4 "Appendix D Prompt Format
    for MMLU and BoolQ ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal
    Keys") provides prompt formats for MMLU and BoolQ during training and validation.
    To reduce training workload, we use LoRA Hu et al. ([2021](#bib.bib10)) to finetune
    the projection matrix for queries/keys/values with rank of $32$, and fix other
    layers.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从 Hugging Face 仓库获取了预训练的 Llama2-7B/13B 模型 [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama)。从这些预训练模型开始，我们用低秩自注意力替换它们的注意力层。对于
    MMLU 和 BoolQ 这两个数据集，由于它们的格式不同，我们首先会在这些数据集上对模型进行几轮微调（更多微调参数请参见附录 [C](#A3 "Appendix
    C Finetune Hyperparameters ‣ ATP: Enabling Fast LLM Serving via Attention on Top
    Principal Keys")），然后在验证数据集上评估它们的性能。附录 [D](#A4 "Appendix D Prompt Format for MMLU
    and BoolQ ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys")
    提供了 MMLU 和 BoolQ 在训练和验证过程中的提示格式。为了减少训练工作量，我们使用 LoRA Hu 等人 ([2021](#bib.bib10))
    对查询/键/值的投影矩阵进行微调，秩为 $32$，并固定其他层。'
- en: 'For MMLU, we obtain the first predicted logit vector from the model given an
    input sequence, and compute the probability on the four tokens: *A, B, C, D*.
    The token with the highest probability will be the predicted answer. For BoolQ,
    we adopt a similar procedure but compute the probability on the two tokens: *Yes,
    No*, and output the token with the highest probability. Note that we ignore other
    tokens that might have the highest probability.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MMLU，我们从模型中获得第一个预测的 logit 向量，并计算四个标记的概率：*A, B, C, D*。概率最高的标记将作为预测答案。对于 BoolQ，我们采用类似的程序，但计算两个标记的概率：*Yes,
    No*，并输出概率最高的标记。请注意，我们忽略了可能具有最高概率的其他标记。
- en: 'Figure [6](#S5.F6 "Figure 6 ‣ 5.2 Llama2 ‣ 5 Empirical Evaluation ‣ ATP: Enabling
    Fast LLM Serving via Attention on Top Principal Keys") shows the accuracy of Llama2-7B
    and 13B on MMLU using ATP. We can observe that on all categories, ATP achieves
    accuracy close to original Llama2-7B and 13B with standard self-attention. In
    particular, owing to the highly low-rank structure in input sequences, with $1/2$
    principal keys, the model performance with ATP is almost identical to the original
    model. Furthermore, even with only $1/4$ principal keys, ATP still does not incur
    a significant accuracy drop. Similar performance of LLama2-7B and 13B with the
    low-rank self-attention holds on the BoolQ dataset, as listed in Table [4](#S5.T4
    "Table 4 ‣ 5.2 Llama2 ‣ 5 Empirical Evaluation ‣ ATP: Enabling Fast LLM Serving
    via Attention on Top Principal Keys"). Therefore, ATP effectively leverages low-rank
    structure in input sequences and performs self-attention with a few top principal
    keys, leading to performance close to the original model but with significantly
    reduced complexities.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#S5.F6 "图 6 ‣ 5.2 Llama2 ‣ 5 实证评估 ‣ ATP：通过关注主要键实现快速 LLM 服务") 显示了使用 ATP
    的 Llama2-7B 和 13B 在 MMLU 上的准确性。我们可以观察到，在所有类别中，ATP 的准确性接近原始的 Llama2-7B 和 13B 使用标准自注意力的水平。特别是由于输入序列中的高度低秩结构，使用
    $1/2$ 主要键时，ATP 的模型性能几乎与原始模型相同。此外，即使只有 $1/4$ 主要键，ATP 仍然不会导致显著的准确性下降。Llama2-7B 和
    13B 在 BoolQ 数据集上的类似表现，如表 [4](#S5.T4 "表 4 ‣ 5.2 Llama2 ‣ 5 实证评估 ‣ ATP：通过关注主要键实现快速
    LLM 服务") 所示。因此，ATP 有效地利用了输入序列中的低秩结构，并通过少量主要键进行自注意力处理，导致性能接近原始模型，但复杂性大大降低。
- en: STEMhumanitiessocialother0.30.40.50.25AccOrig1/21/31/4
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: STEMhumanitiessocialother0.30.40.50.25AccOrig1/21/31/4
- en: (a) Llama2-7B.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama2-7B。
- en: STEMhumanitiessocialother0.30.40.50.60.25Acc
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: STEMhumanitiessocialother0.30.40.50.60.25Acc
- en: (b) Llama2-13B.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama2-13B。
- en: 'Figure 6: LLama2 on MMLU (random guess: 0.25). Low-rank self-attention effectively
    preserves performance on all subjects, even with $1/4$ principal keys.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：LLama2 在 MMLU 上（随机猜测：0.25）。低秩自注意力在所有主题上的性能都得到了有效保留，即使只有 $1/4$ 主要键。
- en: '| Model | Orig | 1/2 | 1/3 | 1/4 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 原始 | 1/2 | 1/3 | 1/4 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 7B | 0.795 | 0.791 | 0.789 | 0.763 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 7B | 0.795 | 0.791 | 0.789 | 0.763 |'
- en: '| 13B | 0.839 | 0.836 | 0.819 | 0.816 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 13B | 0.839 | 0.836 | 0.819 | 0.816 |'
- en: 'Table 4: Llama2 on BoolQ with low-rank self-attention. Performance is not greatly
    affected even a small fraction of principal keys/values are used in attention
    layers.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：Llama2 在 BoolQ 上使用低秩自注意力。即使在注意力层中只使用少量主要键/值，性能也不会受到很大影响。
- en: 6 Conclusion
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we propose a low-rank self-attention mechanism, ATP, significantly
    reducing computation and memory complexity for transformers and LLMs. ATP leverages
    low-rank structures in input sequences and sufficiently represents each input
    sequence with a few top principal components. Then, ATP designs a low-rank self-attention
    layer that first attains principal keys/values given a low-rank input. Then, it
    performs attention only on top principal keys/values, rather than on each individual
    token embedding. Therefore, ATP reduces the attention complexity from quadratic
    to linear in terms of sequence length. Owing to low-rank structures in input sequences,
    a few top principal keys/values are sufficient to preserve information in input
    sequences. Evaluation of BERT and Llama models shows ATP achieves performance
    close to original models with much-reduced computation and memory footprints.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种低秩自注意力机制 ATP，大大减少了变换器和 LLM 的计算和内存复杂性。ATP 利用输入序列中的低秩结构，并用少量主要成分充分表示每个输入序列。然后，ATP
    设计了一个低秩自注意力层，首先在低秩输入下获得主要键/值。接着，它只在主要键/值上执行注意力，而不是在每个单独的 token 嵌入上。因此，ATP 将注意力复杂性从二次降到线性，取决于序列长度。由于输入序列中的低秩结构，少量主要键/值足以保留输入序列中的信息。对
    BERT 和 Llama 模型的评估表明，ATP 在计算和内存占用大大减少的情况下，性能接近原始模型。
- en: 7 Limitations
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: Limitations. One of the limitations of this work is that we evaluate ATP on
    BERT and Llama2 models. While performance on other models may differ. We will
    evaluate more models and datasets in future works.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 限制。本工作的一个限制是我们仅在 BERT 和 Llama2 模型上评估了 ATP。其他模型的性能可能有所不同。我们将在未来的工作中评估更多的模型和数据集。
- en: Potential Risk. While this work is aimed at lowering the barrier of deploying
    LLMs, it may be misused by malicious parties to quickly deploy and run adverse
    LLM services for their purposes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在风险。虽然本工作旨在降低部署 LLM 的门槛，但恶意方可能会滥用这一点，以快速部署和运行不利的 LLM 服务以达成其目的。
- en: References
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Andriushchenko et al. (2023) Maksym Andriushchenko, Dara Bahri, Hossein Mobahi,
    and Nicolas Flammarion. 2023. Sharpness-aware minimization leads to low-rank features.
    *arXiv preprint arXiv:2305.16292*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andriushchenko et al. (2023) Maksym Andriushchenko, Dara Bahri, Hossein Mobahi,
    和 Nicolas Flammarion. 2023. Sharpness-aware minimization leads to low-rank features.
    *arXiv 预印本 arXiv:2305.16292*。
- en: Banerjee and Roy (2014) Sudipto Banerjee and Anindya Roy. 2014. *Linear algebra
    and matrix analysis for statistics*. Crc Press.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee and Roy (2014) Sudipto Banerjee 和 Anindya Roy. 2014. *统计学中的线性代数和矩阵分析*。Crc
    Press。
- en: Choromanski et al. (2020) Krzysztof Marcin Choromanski, Valerii Likhosherstov,
    David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy
    Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with
    performers. In *International Conference on Learning Representations*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski et al. (2020) Krzysztof Marcin Choromanski, Valerii Likhosherstov,
    David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy
    Davis, Afroz Mohiuddin, Lukasz Kaiser, 等。2020. 重新思考使用 performers 的注意力。在 *国际学习表征会议*。
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. In *NAACL*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. 2019. Boolq: 探索自然是/否问题的惊人难度。在 *NAACL*。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2018. Bert: 预训练的深度双向变换器用于语言理解。*arXiv 预印本 arXiv:1810.04805*。'
- en: 'Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words:
    Transformers for image recognition at scale. In *International Conference on Learning
    Representations*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, 等。2020. 一张图像值 16x16 个词：用于大规模图像识别的变换器。在
    *国际学习表征会议*。
- en: 'El-Kassas et al. (2021) Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea,
    and Hoda K Mohamed. 2021. Automatic text summarization: A comprehensive survey.
    *Expert systems with applications*, 165:113679.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: El-Kassas et al. (2021) Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, 和
    Hoda K Mohamed. 2021. 自动文本摘要：全面调查。*应用专家系统*，165:113679。
- en: 'Han et al. (2023) Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P.
    Woodruff, and Amir Zandieh. 2023. [Hyperattention: Long-context attention in near-linear
    time](http://arxiv.org/abs/2310.05869).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2023) Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David
    P. Woodruff, 和 Amir Zandieh. 2023. [Hyperattention: Long-context attention in
    near-linear time](http://arxiv.org/abs/2310.05869)。'
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask
    language understanding. *Proceedings of the International Conference on Learning
    Representations (ICLR)*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, 和 Jacob Steinhardt. 2021. 测量大规模多任务语言理解。*国际学习表征会议（ICLR）论文集*。
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large
    language models. In *International Conference on Learning Representations*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, 等。2021. Lora: 大型语言模型的低秩适应。在 *国际学习表征会议*。'
- en: 'Katharopoulos et al. (2020a) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. 2020a. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *International conference on machine learning*, pages
    5156–5165\. PMLR.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katharopoulos et al. (2020a) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    和 François Fleuret. 2020a. Transformers are rnns: 快速自回归变换器与线性注意力。在 *国际机器学习会议*，第5156–5165页。PMLR。'
- en: 'Katharopoulos et al. (2020b) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. 2020b. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *International conference on machine learning*, pages
    5156–5165\. PMLR.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katharopoulos et al. (2020b) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    和 François Fleuret. 2020b. Transformers are rnns: 快速自回归变换器与线性注意力。在 *国际机器学习会议*，第5156–5165页。PMLR。'
- en: 'Khan et al. (2022) Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas
    Zamir, Fahad Shahbaz Khan, and Mubarak Shah. 2022. Transformers in vision: A survey.
    *ACM computing surveys (CSUR)*, 54(10s):1–41.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等人 (2022) Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir,
    Fahad Shahbaz Khan, 和 Mubarak Shah. 2022. 视觉中的转换器：一项调查。*ACM 计算机调查 (CSUR)*，54(10s):1–41。
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised
    learning of language representations.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan 等人 (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, 和 Radu Soricut. 2020. Albert: 用于自监督学习语言表征的轻量级 bert。'
- en: 'Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. 2011. [Learning word vectors for sentiment
    analysis](http://www.aclweb.org/anthology/P11-1015). In *Proceedings of the 49th
    Annual Meeting of the Association for Computational Linguistics: Human Language
    Technologies*, pages 142–150, Portland, Oregon, USA. Association for Computational
    Linguistics.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maas 等人 (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew
    Y. Ng, 和 Christopher Potts. 2011. [情感分析的词向量学习](http://www.aclweb.org/anthology/P11-1015)。在
    *第 49 届计算语言学协会年会：人类语言技术会议论文集*，页 142–150，俄勒冈州波特兰市，美国。计算语言学协会。
- en: (16) Microsoft. [Microsoft copilot](https://adoption.microsoft.com/en-us/copilot/).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Microsoft. [微软 Copilot](https://adoption.microsoft.com/en-us/copilot/)。
- en: 'Nguyen et al. (2021) Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and
    Bao Wang. 2021. Fmmformer: Efficient and flexible transformer via decomposed near-field
    and far-field attention. *Advances in neural information processing systems*,
    34:29449–29463.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen 等人 (2021) Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, 和 Bao Wang.
    2021. Fmmformer: 通过分解近场和远场注意力实现高效和灵活的 transformer。*神经信息处理系统进展*，34:29449–29463。'
- en: 'Niu et al. (2022) Yue Niu, Ramy E Ali, and Salman Avestimehr. 2022. 3legrace:
    Privacy-preserving dnn training over tees and gpus. *Proceedings on Privacy Enhancing
    Technologies*, 4:183–203.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Niu 等人 (2022) Yue Niu, Ramy E Ali, 和 Salman Avestimehr. 2022. 3legrace: 保护隐私的
    dnn 训练在 tees 和 gpus 上。*隐私增强技术会议论文集*，4:183–203。'
- en: (19) OpenAI. [Introducing chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) OpenAI. [介绍 ChatGPT](https://openai.com/blog/chatgpt)。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    等人. 2018. 通过生成性预训练提高语言理解。
- en: Rahimi and Recht (2007) Ali Rahimi and Benjamin Recht. 2007. Random features
    for large-scale kernel machines. *Advances in neural information processing systems*,
    20.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahimi 和 Recht (2007) Ali Rahimi 和 Benjamin Recht. 2007. 用于大规模核机器的随机特征。*神经信息处理系统进展*，20。
- en: Roy et al. (2021) Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
    2021. Efficient content-based sparse attention with routing transformers. *Transactions
    of the Association for Computational Linguistics*, 9:53–68.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 等人 (2021) Aurko Roy, Mohammad Saffar, Ashish Vaswani, 和 David Grangier.
    2021. 基于内容的稀疏注意力与路由转换器的高效实现。*计算语言学学会会刊*，9:53–68。
- en: 'Samsi et al. (2023) Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li,
    Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari,
    and Vijay Gadepally. 2023. From words to watts: Benchmarking the energy costs
    of large language model inference. *arXiv preprint arXiv:2310.03003*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samsi 等人 (2023) Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam
    Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, 和 Vijay
    Gadepally. 2023. 从词到瓦特：大型语言模型推理的能耗基准。*arXiv 预印本 arXiv:2310.03003*。
- en: 'Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention
    with relative position representations. In *Proceedings of the 2018 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 2 (Short Papers)*, pages 464–468.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaw 等人 (2018) Peter Shaw, Jakob Uszkoreit, 和 Ashish Vaswani. 2018. 带有相对位置表示的自注意力。在
    *2018 年北美计算语言学协会年会：人类语言技术会议论文集，第 2 卷（短论文）*，页 464–468。
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.
    *arXiv preprint arXiv:2104.09864*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su 等人 (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, 和 Yunfeng
    Liu. 2021. Roformer: 使用旋转位置嵌入增强的 transformer。*arXiv 预印本 arXiv:2104.09864*。'
- en: Sun et al. (2021) Zhiqing Sun, Yiming Yang, and Shinjae Yoo. 2021. Sparse attention
    with learning to hash. In *International Conference on Learning Representations*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2021) Zhiqing Sun, Yiming Yang, 和 Shinjae Yoo. 2021. 稀疏注意力与学习哈希。在 *国际学习表征会议*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等. 2023. Llama 2: 开放基础和微调聊天模型. *arXiv 预印本 arXiv:2307.09288*.'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力即你所需.
    *神经信息处理系统进展*, 30.
- en: Vyas et al. (2020) Apoorv Vyas, Angelos Katharopoulos, and François Fleuret.
    2020. Fast transformers with clustered attention. *Advances in Neural Information
    Processing Systems*, 33:21665–21674.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vyas et al. (2020) Apoorv Vyas, Angelos Katharopoulos, 和 François Fleuret. 2020.
    具有集群注意力的快速变换器. *神经信息处理系统进展*, 33:21665–21674.
- en: 'Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue:
    A stickier benchmark for general-purpose language understanding systems. *Advances
    in neural information processing systems*, 32.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet
    Singh, Julian Michael, Felix Hill, Omer Levy, 和 Samuel Bowman. 2019. Superglue:
    一个更具挑战性的通用语言理解基准. *神经信息处理系统进展*, 32.'
- en: 'Wang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
    Hao Ma. 2020. Linformer: Self-attention with linear complexity. *arXiv preprint
    arXiv:2006.04768*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, 和 Hao
    Ma. 2020. Linformer: 具有线性复杂度的自注意力. *arXiv 预印本 arXiv:2006.04768*.'
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. 2020. Transformers: State-of-the-art natural language processing. In *Proceedings
    of the 2020 conference on empirical methods in natural language processing: system
    demonstrations*, pages 38–45.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    等. 2020. 变换器: 先进的自然语言处理技术. 在 *2020 年经验方法自然语言处理会议: 系统演示文稿* 中, 第 38–45 页.'
- en: Appendix A Lowrank Structure in Other Model
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 其他模型中的低秩结构
- en: 'Figure [7](#A1.F7 "Figure 7 ‣ Appendix A Lowrank Structure in Other Model ‣
    ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys") shows the
    low-rankness of BERT model on IMDB dataset. We can also observe that most sequences
    exhibists low-rank structures. In particular, long sequences are more low-rank,
    which is aligned with the observation in Sec [3](#S3 "3 Lowrank Structure in Sequences
    ‣ ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#A1.F7 "图 7 ‣ 附录 A 其他模型中的低秩结构 ‣ ATP: 通过注意力在主键上启用快速 LLM 服务") 展示了 BERT
    模型在 IMDB 数据集上的低秩性。我们还可以观察到大多数序列表现出低秩结构。特别是，长序列的低秩性更明显，这与第 [3](#S3 "3 序列中的低秩结构
    ‣ ATP: 通过注意力在主键上启用快速 LLM 服务") 节中的观察结果一致。'
- en: 0.30.40.50.60.70.836912$\left\lceil
    2^{\mu}\right\rceil/L$probability density$L\in[0,200]$$L\in[200,350]$$L\in[350,]$
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 0.30.40.50.60.70.836912$\left\lceil
    2^{\mu}\right\rceil/L$probability density$L\in[0,200]$$L\in[200,350]$$L\in[350,]$
- en: 'Figure 7: Distribution of low-rankness of BERT-base’s embedding on IMDB dataset,
    measured by ratio $\left\lceil 2^{\mu}\right\rceil/L$.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: BERT-base 在 IMDB 数据集上的低秩分布，通过比例 $\left\lceil 2^{\mu}\right\rceil/L$ 测量。'
- en: Appendix B Alternating Optimization
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 交替优化
- en: 'Data: $r,X,\left\{\bm{u}_{i}^{0},\bm{v}_{i}^{0}\right\}_{i=1}^{r}$Result: $\left\{\bm{u}_{i},\bm{v}_{i}\right\}_{i=1}^{r}$for *$i$
    in $1,\cdots,r$* do       for *$j$ in $1,\cdots,2$* do             /* Alternating
    optimization */             $\bm{u}_{i}^{j}=\frac{X\cdot\bm{v}_{i}^{j-1}}{\left\|\bm{v}_{i}^{j-1}\right\|_{F}^{2}}$;            
    $\bm{v}_{i}^{j}=\frac{X^{T}\cdot\bm{u}_{i}^{j}}{\left\|\bm{u}_{i}^{j}\right\|_{F}^{2}}$;      
    end for      $\bm{u}_{i},\bm{v}_{i}=\bm{u}_{i}^{j},{\bm{v}_{i}^{j^{T}}}$;      
    $X=X-\bm{u}_{i}^{j}\cdot{\bm{v}_{i}^{j^{T}}}$;end for'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '数据: $r,X,\left\{\bm{u}_{i}^{0},\bm{v}_{i}^{0}\right\}_{i=1}^{r}$ 结果: $\left\{\bm{u}_{i},\bm{v}_{i}\right\}_{i=1}^{r}$
    对于 *$i$* 在 $1,\cdots,r$ 中执行    对于 *$j$* 在 $1,\cdots,2$ 中执行           /* 交替优化 */
              $\bm{u}_{i}^{j}=\frac{X\cdot\bm{v}_{i}^{j-1}}{\left\|\bm{v}_{i}^{j-1}\right\|_{F}^{2}}$;
              $\bm{v}_{i}^{j}=\frac{X^{T}\cdot\bm{u}_{i}^{j}}{\left\|\bm{u}_{i}^{j}\right\|_{F}^{2}}$;
       结束    $\bm{u}_{i},\bm{v}_{i}=\bm{u}_{i}^{j},{\bm{v}_{i}^{j^{T}}}$;    $X=X-\bm{u}_{i}^{j}\cdot{\bm{v}_{i}^{j^{T}}}$;
    结束'
- en: Algorithm 1 Alternating Opt for SVD.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 交替优化用于SVD.
- en: Appendix C Finetune Hyperparameters
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 微调超参数
- en: 'For BERT-base and Llama2 models, we conduct a grid search on learning rate
    (1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 5e-4), and weight decay (1e-3, 5e-3, 1e-2, 5e-2).
    Table [5](#A3.T5 "Table 5 ‣ Appendix C Finetune Hyperparameters ‣ ATP: Enabling
    Fast LLM Serving via Attention on Top Principal Keys") and [6](#A3.T6 "Table 6
    ‣ Appendix C Finetune Hyperparameters ‣ ATP: Enabling Fast LLM Serving via Attention
    on Top Principal Keys") list the best hyperparameters found during fine-tuning.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 BERT-base 和 Llama2 模型，我们对学习率 (`1e-5`, `2e-5`, `5e-5`, `1e-4`, `2e-4`, `5e-4`)
    和权重衰减 (`1e-3`, `5e-3`, `1e-2`, `5e-2`) 进行了网格搜索。表格 [5](#A3.T5 "Table 5 ‣ Appendix
    C Finetune Hyperparameters ‣ ATP: Enabling Fast LLM Serving via Attention on Top
    Principal Keys") 和 [6](#A3.T6 "Table 6 ‣ Appendix C Finetune Hyperparameters ‣
    ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys") 列出了在微调过程中找到的最佳超参数。 '
- en: '| max len | batch size | epochs | $lr$ | $wd$ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 最大长度 | 批量大小 | 纪元数 | $lr$ | $wd$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 512 | 32 | 20 | 5e-5 | 1e-2 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 32 | 20 | `5e-5` | `1e-2` |'
- en: 'Table 5: Finetuning hyperparameters for BERT-base on SST-2, Squad, and IMDB.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: BERT-base 在 SST-2、Squad 和 IMDB 上的微调超参数。'
- en: '| max len | batch size | iters | $lr$ | $wd$ |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 最大长度 | 批量大小 | 迭代次数 | $lr$ | $wd$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 2048 | 32 | 400 | 2e-4 | 1e-2 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 2048 | 32 | 400 | `2e-4` | `1e-2` |'
- en: 'Table 6: Finetuning hyperparameters for Llama 2-7B/13B on MMLU and BoolQ.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: MMLU 和 BoolQ 上的 Llama 2-7B/13B 微调超参数。'
- en: Appendix D Prompt Format for MMLU and BoolQ
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D MMLU 和 BoolQ 提示格式
- en: 'Table [7](#A4.T7 "Table 7 ‣ Appendix D Prompt Format for MMLU and BoolQ ‣ ATP:
    Enabling Fast LLM Serving via Attention on Top Principal Keys") and [8](#A4.T8
    "Table 8 ‣ Appendix D Prompt Format for MMLU and BoolQ ‣ ATP: Enabling Fast LLM
    Serving via Attention on Top Principal Keys") list the prompt format for MMLU
    and BoolQ dataset.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [7](#A4.T7 "Table 7 ‣ Appendix D Prompt Format for MMLU and BoolQ ‣ ATP:
    Enabling Fast LLM Serving via Attention on Top Principal Keys") 和 [8](#A4.T8 "Table
    8 ‣ Appendix D Prompt Format for MMLU and BoolQ ‣ ATP: Enabling Fast LLM Serving
    via Attention on Top Principal Keys") 列出了 MMLU 和 BoolQ 数据集的提示格式。'
- en: '| The following are multiple choice questions (with answers). |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 以下是选择题（带答案）。 |'
- en: '| One of the reasons that the government discourages |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 政府阻碍垄断的原因之一是 |'
- en: '| and regulates monopolies is that |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 并且调节垄断的原因是 |'
- en: '| A. producer surplus is lost and consumer surplus is gained. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| A. 生产者剩余丧失，消费者剩余获得。 |'
- en: '| B. monopoly prices ensure productive efficiency but cost |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| B. 垄断价格确保生产效率但成本 |'
- en: '| society allocative efficiency. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 社会分配效率。 |'
- en: '| C. monopoly firms do not engage in significant research |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| C. 垄断企业不从事重要的研究 |'
- en: '| and development. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 和开发。 |'
- en: '| D. consumer surplus is lost with higher prices and lower |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| D. 消费者剩余在价格上涨和产量下降时会丧失。 |'
- en: '| levels of output. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| |'
- en: '| Answer: |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 答案： |'
- en: '| C |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| C |'
- en: 'Table 7: MMLU prompt format'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: MMLU 提示格式'
- en: '| Below is an instruction that describes a task. Write a response |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 以下是描述任务的指令。请写出响应 |'
- en: '| that appropriately completes the request. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 适当完成请求的内容。 |'
- en: '| ### Instruction: |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| ### 指令： |'
- en: '| is harry potter and the escape from gringotts a roller coaster ride |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 哈利·波特与逃离古灵阁是一场过山车体验 |'
- en: '| ### Input: |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| ### 输入： |'
- en: '| Harry Potter and the Escape from Gringotts is an indoor steel |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 哈利·波特与逃离古灵阁是一种室内钢制 |'
- en: '| roller coaster *** |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 过山车 *** |'
- en: '| ### Answer: |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ### 答案： |'
- en: '| Yes |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 是的 |'
- en: 'Table 8: BoolQ prompt format'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: BoolQ 提示格式'
