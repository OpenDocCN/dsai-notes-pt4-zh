- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:02:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:02:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free
    Attention'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NoMAD-Attention：通过无乘加操作的注意力机制在CPU上高效推理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01273](https://ar5iv.labs.arxiv.org/html/2403.01273)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01273](https://ar5iv.labs.arxiv.org/html/2403.01273)
- en: Tianyi Zhang    Jonah Wonkyu Yi    Bowen Yao    Zhaozhuo Xu    Anshumali Shrivastava
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 张天一    乔纳·翁奎·伊    鲍文·姚    赵卓熙    安舒玛利·施里瓦斯塔瓦
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language model inference on Central Processing Units (CPU) is challenging
    due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in
    the attention computations. In this paper, we argue that there is a rare gem in
    modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers, which allow for
    ultra-low-latency lookups in batch. We leverage this unique capability of CPUs
    to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD
    operations with in-register lookups. Through hardware-aware algorithmic designs,
    NoMAD-Attention achieves the computation of attention scores using repeated fast
    accesses to SIMD registers despite their highly limited sizes. Moreover, NoMAD-Attention
    works with pre-trained attention-based LLMs without model finetuning. Empirical
    evaluations demonstrate that NoMAD-Attention maintains the quality of the original
    LLMs well, and speeds up the 4-bit quantized LLaMA-7B-based model by up to $2\times$
    at 16k context length. Our results are reproducible at [https://github.com/tonyzhang617/nomad-dist](https://github.com/tonyzhang617/nomad-dist).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在中央处理单元（CPU）上进行大语言模型推理具有挑战性，因为注意力计算中包含大量昂贵的乘加（MAD）矩阵操作。在本文中，我们认为现代CPU中有一个稀有的宝石，即单指令多数据（SIMD）寄存器，这些寄存器允许在批处理中进行超低延迟查找。我们利用这一CPU的独特能力，提出了NoMAD-Attention，这是一种高效的注意力算法，用寄存器查找代替了MAD操作。通过硬件感知的算法设计，NoMAD-Attention在SIMD寄存器的极限尺寸下，通过重复快速访问实现了注意力分数的计算。此外，NoMAD-Attention能够在不进行模型微调的情况下，与预训练的基于注意力的LLM一起使用。实证评估表明，NoMAD-Attention保持了原始LLM的质量，并在16k上下文长度下使4-bit量化的LLaMA-7B模型加速了多达$2\times$。我们的结果可以在[https://github.com/tonyzhang617/nomad-dist](https://github.com/tonyzhang617/nomad-dist)上重复。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Auto-regressive transformer-based Large Language Models (LLM) have demonstrated
    remarkable abilities across a wide range of natural language processing tasks
    including reading comprehension, translation, and question answering (Radford
    et al., [2019](#bib.bib23)). LLMs exhibit emergent abilities (Wei et al., [2022](#bib.bib35))
    in solving complex tasks without fine-tuning. These capabilities give LLMs immense
    potential for impactful applications in diverse fields such as medicine (Thirunavukarasu
    et al., [2023](#bib.bib30)), law (Xiao et al., [2021](#bib.bib37)), and robotics
    (Kaddour et al., [2023](#bib.bib14)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归变换器基础的大型语言模型（LLM）在包括阅读理解、翻译和问答等广泛的自然语言处理任务中展现了显著的能力（Radford et al., [2019](#bib.bib23)）。LLM展现了在解决复杂任务中无需微调的*突现能力*（Wei
    et al., [2022](#bib.bib35)）。这些能力为LLM在医学（Thirunavukarasu et al., [2023](#bib.bib30)）、法律（Xiao
    et al., [2021](#bib.bib37)）和机器人技术（Kaddour et al., [2023](#bib.bib14)）等多个领域的*影响力应用*提供了巨大潜力。
- en: The Need for Deploying LLM on CPUs. Despite the promising potential of LLMs,
    their deployment is extremely expensive (Lin et al., [2023](#bib.bib17)). Serving
    LLMs with billion-scale parameters requires specialized hardware such as Nvidia
    A100 Graphics Processing Units (GPUs) (Zhang et al., [2023a](#bib.bib40)). However,
    mainstream personal devices, such as laptops, are predominately equipped with
    Central Processing Units (CPUs) (Sun et al., [2019](#bib.bib28)). As a result,
    making LLM-related services accessible to everyone remains a major challenge.
    Reducing the LLM inference latency on CPUs, beyond doubt, has significant implications
    for its accessibility and adoption.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上部署LLM的必要性。尽管LLM具有令人鼓舞的潜力，但其部署成本极高（Lin et al., [2023](#bib.bib17)）。服务拥有十亿规模参数的LLM需要专用硬件，例如Nvidia
    A100图形处理单元（GPU）（Zhang et al., [2023a](#bib.bib40)）。然而，主流个人设备，如笔记本电脑，主要配备中央处理单元（CPU）（Sun
    et al., [2019](#bib.bib28)）。因此，使LLM相关服务对每个人都可访问仍然是一个重大挑战。无疑，降低CPU上LLM推理的延迟对其可访问性和采纳具有重要意义。
- en: Expensive Multiply-add Operations for Attention in LLM Inference. LLM inference
    on CPUs is compute-bound and the primary computational bottleneck is the calculation
    of attention scores (Han et al., [2023](#bib.bib12)). Attention, a mechanism that
    models token interactions through all-pair dot products, heavily relies on the
    multiply-add (MAD) kernel on processors. The MAD operation involves computing
    the product of two numbers and adding that product to an accumulator (Sung et al.,
    [2023](#bib.bib29)). Within the attention mechanism, MAD plays a crucial role
    in determining the attention score between tokens and subsequently blending their
    embeddings based on these scores. The computational cost of attention grows quadratically
    with the sequence length due to the cumulative MAD operations. Since CPUs have
    limited parallel cores, they are inefficient for handling highly repetitive and
    parallel workloads. The extensive MAD operations required by the attention mechanism
    thus become the primary bottleneck during inference.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 推理中的昂贵乘加运算。在 CPU 上的 LLM 推理受限于计算，主要的计算瓶颈是注意力得分的计算（Han 等，[2023](#bib.bib12)）。注意力机制通过全对点积建模标记之间的交互，严重依赖于处理器上的乘加（MAD）内核。MAD
    操作涉及计算两个数字的乘积，并将该乘积加到累加器中（Sung 等，[2023](#bib.bib29)）。在注意力机制中，MAD 在确定标记之间的注意力得分并根据这些得分融合它们的嵌入方面发挥着至关重要的作用。由于累积的
    MAD 操作，注意力的计算成本随着序列长度的增加而呈二次增长。由于 CPU 的并行核心有限，它们在处理高度重复和并行的工作负载时效率低下。因此，注意力机制所需的大量
    MAD 操作成为推理过程中主要的瓶颈。
- en: 'Opportunities and Challenges from Modern CPUs: In-Register Lookups. The memory
    hierarchy of modern CPUs has undergone significant evolution, introducing a new
    type of registers optimized for Single-Instruction-Multiple-Data (SIMD) operations.
    The SIMD registers vary in size, ranging from 128 bits to 512 bits (Shin et al.,
    [2019](#bib.bib26)), and support specialized SIMD instructions for high-throughput
    parallel processing (Zhang et al., [2019](#bib.bib41)). Nowadays, SIMD registers
    have become a standard feature in commodity hardware, including laptops and mobile
    devices (Dasika et al., [2010](#bib.bib9)). In this context, in-register lookup
    refers to the low-latency retrieval of information stored within SIMD registers.
    Specifically, storing information such as dot-product lookup tables (LUT) within
    SIMD registers as opposed to cache memory has the potential of accelerating LLM
    inference (André et al., [2017](#bib.bib2)). Despite these opportunities, the
    limited size of SIMD registers poses a great challenge to fitting the computational
    paradigm of existing models.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 CPU 带来的机会和挑战：寄存器内查找。现代 CPU 的内存层次结构经历了显著的演变，引入了一种针对单指令多数据（SIMD）操作优化的新类型寄存器。SIMD
    寄存器的大小从 128 位到 512 位不等（Shin 等，[2019](#bib.bib26)），并支持用于高吞吐量并行处理的专用 SIMD 指令（Zhang
    等，[2019](#bib.bib41)）。如今，SIMD 寄存器已成为包括笔记本电脑和移动设备在内的商品硬件的标准配置（Dasika 等，[2010](#bib.bib9)）。在这种背景下，寄存器内查找指的是低延迟地检索存储在
    SIMD 寄存器中的信息。具体来说，将点积查找表（LUT）等信息存储在 SIMD 寄存器中而不是缓存内存中，有可能加速 LLM 推理（André 等，[2017](#bib.bib2)）。尽管存在这些机会，SIMD
    寄存器的有限大小对现有模型的计算范式形成了巨大挑战。
- en: 'Our Proposal: MAD-Free Attention with In-Register Lookups. In this paper, we
    demonstrate a new approach for speeding up LLM inference by leveraging the unique
    hardware capability of CPUs. We show how the vast quantities of MAD operations
    in attention computation can be replaced with in-register lookups to mitigate
    the quadratic computational bottleneck of LLM inference on CPUs. NoMAD-Attention
    significantly speeds up LLM inference without sacrificing model quality and is
    compatible with pre-trained attention-based transformers without finetuning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提议：使用寄存器内查找实现无 MAD 注意力。在本文中，我们展示了一种通过利用 CPU 独特硬件能力来加速 LLM 推理的新方法。我们展示了如何用寄存器内查找替代注意力计算中大量的
    MAD 操作，从而缓解 CPU 上 LLM 推理的二次计算瓶颈。NoMAD-Attention 显著加速了 LLM 推理，而不牺牲模型质量，并且与预训练的基于注意力的变换器兼容，无需微调。
- en: 'We summarize our contributions as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了我们的贡献如下：
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We identify the extensive MAD operations in attention as the bottleneck of CPU
    LLM inference and explore the opportunity of mitigating it through replacing MAD
    in attention with fast in-register lookups.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将注意力中的广泛 MAD 操作识别为 CPU LLM 推理的瓶颈，并探索通过用快速的寄存器内查找替代注意力中的 MAD 来缓解这一瓶颈的机会。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We introduce NoMAD-Attention, a MAD-free framework of attention computation
    for pre-trained attention-based LLMs. NoMAD-Attention leverages hardware-aware
    algorithmic designs to enable accurate and fast in-register lookup-based estimations
    of query-key dot products despite the limited capacity of SIMD registers. NoMAD-Attention
    preserves model quality while yielding considerable speedups over MAD-based attention.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了 NoMAD-Attention，一个不使用 MAD 的注意力计算框架，用于预训练的基于注意力的 LLMs。NoMAD-Attention 利用硬件感知的算法设计，即使在
    SIMD 寄存器容量有限的情况下，也能实现准确且快速的基于寄存器查找的查询-键点积估计。NoMAD-Attention 保持了模型质量，同时相较于基于 MAD
    的注意力方法实现了显著的加速。
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Through extensive experiments, we demonstrate that NoMAD-Attention achieves
    up to $2\times$ speedup on 4-bit quantized LLaMA-7B-based models at a context
    length of 16k, while maintaining the predictive performance of the original model.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过大量实验，我们证明了 NoMAD-Attention 在 16k 上下文长度的 4 位量化 LLaMA-7B 模型中实现了最高 $2\times$
    的加速，同时保持了原始模型的预测性能。
- en: 2 LLM Inference on CPUs
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 LLM 在 CPU 上的推理
- en: In this section, we introduce the attention mechanism used in LLMs and the key-value
    (KV) caching technique for avoiding redundant attention computations. We also
    discuss the CPU memory hierarchy, which serves as the motivation for performing
    fast in-register lookups.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了 LLMs 中使用的注意力机制以及用于避免冗余注意力计算的键值（KV）缓存技术。我们还讨论了 CPU 内存层次结构，它作为执行快速寄存器内查找的动机。
- en: 2.1 LLM Attention
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 注意力
- en: Most LLMs are decoder-only attention-based models that are pre-trained on a
    next token prediction objective. LLMs use masked self-attention, in which the
    attention output of each token is only dependent on previous tokens and itself,
    and unaffected by future tokens. Masked self-attention allows LLMs to cache key
    and value embeddings, avoiding future recomputations. However, this comes at the
    cost of memory overhead. The autoregressive generation of LLMs consists of two
    phases 1. prompt processing, in which the sequence of token embeddings in the
    prompt is fed through by the model, and their key-value embeddings are cached
    by the model, 2. decoding, in which a new token is sampled based on the output
    embedding of the last token, and the embedding of the new token is fed through
    the model, the output of which becomes the basis for sampling the next token.
    The decoding process continues until an end-of-sequence token  is sampled.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 LLM 是仅解码器的基于注意力的模型，预训练于下一个标记预测目标。LLM 使用掩码自注意力，其中每个标记的注意力输出仅依赖于之前的标记和自身，而不受未来标记的影响。掩码自注意力允许
    LLM 缓存键和值嵌入，避免了未来的重新计算。然而，这会带来内存开销。LLM 的自回归生成包括两个阶段：1. 提示处理，其中提示中的标记嵌入序列通过模型处理，其键值嵌入由模型缓存；2.
    解码，其中基于最后一个标记的输出嵌入采样一个新标记，新标记的嵌入通过模型处理，输出成为采样下一个标记的基础。解码过程持续进行，直到采样到序列结束标记 。
- en: At the decoding step $t$ is transformed into key, query, and value embeddings
    through distinct transformations,
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码步骤 $t$ 中，通过不同的转换将其转化为键、查询和值嵌入，
- en: '|  | $\displaystyle k^{t}=f_{K}(e^{t}),q^{t}=f_{Q}(e^{t}),v^{t}=f_{V}(e^{t})$
    |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle k^{t}=f_{K}(e^{t}),q^{t}=f_{Q}(e^{t}),v^{t}=f_{V}(e^{t})$
    |  |'
- en: Then, the key and value embedding of the current token are appended to the key
    and value cache, respectively. The KV cache $K^{t-1}_{\mathrm{cache}},V^{t-1}_{\mathrm{cache}}$
    contains the key/value embeddings of all previous tokens, and after appending,
    the KV cache become
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当前标记的键值和数值嵌入分别附加到键值缓存中。KV 缓存 $K^{t-1}_{\mathrm{cache}},V^{t-1}_{\mathrm{cache}}$
    包含所有之前标记的键/值嵌入，附加后，KV 缓存变为
- en: '|  | $$\displaystyle K^{t}_{\mathrm{cache}}=\begin{bmatrix}K^{t-1}_{\mathrm{cache}}\\
    k^{t}\end{bmatrix}=\begin{bmatrix}k^{1}\\'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle K^{t}_{\mathrm{cache}}=\begin{bmatrix}K^{t-1}_{\mathrm{cache}}\\
    k^{t}\end{bmatrix}=\begin{bmatrix}k^{1}\\'
- en: k^{2}\\
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: k^{2}\\
- en: \dots\\
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \dots\\
- en: k^{t}\end{bmatrix},V^{t}_{\mathrm{cache}}=\begin{bmatrix}V^{t-1}_{\mathrm{cache}}\\
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: k^{t}\end{bmatrix},V^{t}_{\mathrm{cache}}=\begin{bmatrix}V^{t-1}_{\mathrm{cache}}\\
- en: v^{t}\end{bmatrix}=\begin{bmatrix}v^{1}\\
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: v^{t}\end{bmatrix}=\begin{bmatrix}v^{1}\\
- en: v^{2}\\
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: v^{2}\\
- en: \dots\\
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \dots\\
- en: v^{t}\end{bmatrix}$$ |  |
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: v^{t}\end{bmatrix}$$ |  |
- en: Finally, the attention output is computed as
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，注意力输出计算为
- en: '|  | $\displaystyle\mathrm{attention}(e^{t})=\mathrm{softmax}\left(\frac{q^{t}(K_{\mathrm{cache}}^{t})^{\top}}{\sqrt{d}}\right)V_{\mathrm{cache}}^{t}$
    |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{attention}(e^{t})=\mathrm{softmax}\left(\frac{q^{t}(K_{\mathrm{cache}}^{t})^{\top}}{\sqrt{d}}\right)V_{\mathrm{cache}}^{t}$
    |  |'
- en: where $d$. We will refer to the result of $\mathrm{softmax}(\frac{qK^{\top}}{\sqrt{d}})$
    as the attention scores since they dictate how much “attention” each token pays
    to other tokens. Computations in the prompt processing phase are similar to the
    decoding phase, except all the prompt tokens are computed in batch. LLMs use multi-head
    attention, which transforms the concatenation of the outputs of multiple single-head
    attentions to form an output embedding.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d$。我们将 $\mathrm{softmax}(\frac{qK^{\top}}{\sqrt{d}})$ 的结果称为注意力分数，因为它们决定了每个令牌对其他令牌的“注意”程度。提示处理阶段的计算类似于解码阶段，只是所有提示令牌都是批量计算的。LLM
    使用多头注意力，它将多个单头注意力的输出连接起来形成输出嵌入。
- en: MAD-based Attention. The attention mechanism models the interaction between
    tokens by performing all-pair dot products, where each dot product is computed
    via $d$ Multiply-Add (MAD) operations. Since attention computes the interaction
    between all pairs of tokens exhaustively, the amount of MAD operations scales
    quadratically with the sequence length, quickly overwhelming the computing capability
    of CPUs. CPUs are designed to handle complex workloads with granular control,
    while GPUs are optimized for processing simple and repetitive tasks in high throughput.
    Hence the success of attention has largely been fueled by the development of highly
    parallel throughput-oriented processors such as GPUs (Dao et al., [2022](#bib.bib8)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 MAD 的注意力。注意力机制通过执行所有对的点积来建模令牌之间的交互，其中每个点积通过 $d$ 次乘加（MAD）操作计算。由于注意力计算了所有令牌对之间的交互，因此
    MAD 操作的数量随着序列长度的平方增加，迅速超出 CPU 的计算能力。CPU 设计用于处理具有细粒度控制的复杂工作负载，而 GPU 优化用于处理高吞吐量中的简单和重复任务。因此，注意力的成功在很大程度上得益于如
    GPU（Dao et al., [2022](#bib.bib8)）等高度并行的吞吐量导向处理器的发展。
- en: Algorithm 1 Attention Score Computation in LLM
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 LLM 中的注意力分数计算
- en: Input:query $q^{t}$, key cache $K_{\mathrm{cache}}^{t-1}$let $$K_{\mathrm{cache}}^{t}\leftarrow\begin{bmatrix}K_{\mathrm{cache}}^{t-1}\\
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：查询 $q^{t}$，键缓存 $K_{\mathrm{cache}}^{t-1}$，设 $$K_{\mathrm{cache}}^{t}\leftarrow\begin{bmatrix}K_{\mathrm{cache}}^{t-1}\\
- en: k^{t}\end{bmatrix}$$Append the current key to key cachereturn $\mathrm{softmax}(\frac{q^{t}(K_{\mathrm{cache}}^{t})^{\top}}{\sqrt{d}})$\State\State\Comment\State
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: k^{t}\end{bmatrix}$$将当前键追加到键缓存中返回 $\mathrm{softmax}(\frac{q^{t}(K_{\mathrm{cache}}^{t})^{\top}}{\sqrt{d}})$\State\State\Comment\State
- en: 'MAD-based Attention as Bottleneck of LLM Inference. The computation of attention
    scores becomes the bottleneck of LLM inference as the sequence length increases.
    At the $t$ due to $t$. We will focus on optimizing the efficiency of attention
    score computations in our proposed approach. Algorithm [1](#alg1 "Algorithm 1
    ‣ 2.1 LLM Attention ‣ 2 LLM Inference on CPUs ‣ NoMAD-Attention: Efficient LLM
    Inference on CPUs Through Multiply-add-free Attention") presents the pseudocode
    for attention score computation, including key caching, for a single-head masked
    self-attention in LLM. This algorithm will serve as a point of comparison in our
    proposed approach.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '基于 MAD 的注意力作为 LLM 推理的瓶颈。随着序列长度的增加，注意力分数的计算成为 LLM 推理的瓶颈。在 $t$ 由于 $t$。我们将重点优化我们提出的方法中注意力分数计算的效率。算法
    [1](#alg1 "Algorithm 1 ‣ 2.1 LLM Attention ‣ 2 LLM Inference on CPUs ‣ NoMAD-Attention:
    Efficient LLM Inference on CPUs Through Multiply-add-free Attention") 提出了注意力分数计算的伪代码，包括键缓存，用于
    LLM 中的单头掩蔽自注意力。该算法将作为我们提出的方法的比较点。'
- en: 2.2 Memory Hierarchy of Modern CPUs
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 现代 CPU 的内存层次结构
- en: 'Memory of the CPU is organized into a pyramidal hierarchy as shown in Figure
    [1](#S3.F1 "Figure 1 ‣ 3 Methodology ‣ NoMAD-Attention: Efficient LLM Inference
    on CPUs Through Multiply-add-free Attention"), with faster memory being significantly
    smaller than slower memory. The memory unit with the fastest access speed is registers.
    Each compute core can access its dedicated registers in just 1-2 CPU cycles, but
    these registers are highly limited in size, usually not exceeding 64 bits. Modern
    processors have a new type of registers optimized for Single-Instruction-Multiple-Data
    (SIMD) operations. These SIMD registers range from 128 bits to 512 bits in size
    and support specialized SIMD instructions for throughput-oriented parallel processing.
    SIMD registers are common on commodity hardware, including laptops and mobile
    devices. Using SIMD operations can speed up deep learning models on CPUs by parallelizing
    matrix multiplications. However, due to the limited number of cores in a CPU,
    its efficiency in deep learning is still considerably worse than GPU. Prior works
    (Spring & Shrivastava, [2017](#bib.bib27); Chen et al., [2020](#bib.bib5)) have
    resorted to sparsity and sampling-based approaches to reduce the number of computations
    for efficient deep learning on CPU, but they require training models from scratch
    and may not apply to all architectures. In this work, we exploit the SIMD registers
    to shift the computation paradigm from MAD to in-register lookups, which demonstrates
    significant speedup over MAD-based models.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 的内存被组织成如图 [1](#S3.F1 "图 1 ‣ 3 方法论 ‣ NoMAD-Attention：通过无乘加操作的注意力在 CPU 上高效推理")
    所示的金字塔层次结构，其中更快的内存比较慢的内存小得多。访问速度最快的内存单元是寄存器。每个计算核心可以在仅 1-2 个 CPU 周期内访问其专用寄存器，但这些寄存器的大小非常有限，通常不超过
    64 位。现代处理器有一种新型寄存器，专门针对单指令多数据（SIMD）操作进行了优化。这些 SIMD 寄存器的大小从 128 位到 512 位不等，并支持用于吞吐量导向并行处理的专用
    SIMD 指令。SIMD 寄存器在包括笔记本电脑和移动设备在内的消费级硬件上很常见。使用 SIMD 操作可以通过并行化矩阵乘法来加速 CPU 上的深度学习模型。然而，由于
    CPU 核心数量有限，其在深度学习中的效率仍然明显低于 GPU。先前的工作（Spring & Shrivastava, [2017](#bib.bib27);
    Chen et al., [2020](#bib.bib5)）采用了稀疏性和基于采样的方法来减少 CPU 上高效深度学习的计算量，但这些方法需要从头训练模型，并且可能不适用于所有架构。在这项工作中，我们利用
    SIMD 寄存器将计算范式从 MAD 转变为寄存器内查找，这在速度上显著优于基于 MAD 的模型。
- en: When describing our proposed algorithm, we assume SIMD registers are 128 bits
    wide. There exist systems with wider SIMD registers that support more parallelism,
    e.g. 256-bit registers in AVX-2 and 512-bit registers in AVX-512\. However, the
    most universal form of SIMD registers uses 128 bits, which is supported by Arm
    NEON and AVX-compatible processors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述我们提出的算法时，我们假设 SIMD 寄存器宽度为 128 位。确实存在支持更多并行性的更宽 SIMD 寄存器的系统，例如 AVX-2 中的 256
    位寄存器和 AVX-512 中的 512 位寄存器。然而，最通用的 SIMD 寄存器形式使用 128 位，这被 Arm NEON 和兼容 AVX 的处理器所支持。
- en: 3 Methodology
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'In this section, we describe our proposed approach NoMAD-Attention, which replaces
    MAD operations with in-register lookups to enable fast attention computations
    on CPUs. NoMAD utilizes three techniques to enable lookup-based attention: 1.
    transforming dot product computations to memory lookups through product quantization,
    2. compressing lookup tables into SIMD registers for low-latency access, 3. reorganizing
    the memory layout of key cache for batch parallel dot product lookups.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了我们提出的方法 NoMAD-Attention，该方法用寄存器内查找替代 MAD 操作，以实现 CPU 上的快速注意力计算。NoMAD
    利用三种技术实现基于查找的注意力：1. 通过产品量化将点积计算转换为内存查找，2. 将查找表压缩到 SIMD 寄存器中以实现低延迟访问，3. 重新组织关键缓存的内存布局以进行批量并行点积查找。
- en: '![Refer to caption](img/9d2b766543dc3aa7dbd4019ae01b0003.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9d2b766543dc3aa7dbd4019ae01b0003.png)'
- en: 'Figure 1: An illustrative comparison of memory layouts of the key cache of
    LLM attention and the key-code cache of NoMAD-Attention, and an illustration of
    how attention scores are computed through in-register lookups in NoMAD.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示了 LLM 注意力的关键缓存和 NoMAD-Attention 的关键代码缓存的内存布局比较，并说明了 NoMAD 中如何通过寄存器内查找计算注意力分数。
- en: 3.1 Transforming Dot-products into Lookups
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 将点积转换为查找
- en: Previous works have shown that inexact attention scores in transformers work
    well for sequence modeling (Zaheer et al., [2020](#bib.bib39)). Based on this
    insight, NoMAD leverages Product Quantization (PQ) (Jegou et al., [2010](#bib.bib13))
    to compute high-quality estimations of dot products through register lookups.
    PQ, originally designed for compressing high-dimensional vectors to enable efficient
    nearest-neighbor search, quantizes a floating-point vector into discrete codes.
    It makes use of sub-quantizers; for a $d$ sub-vectors, where each sub-vector has
    dimension $d_{\mathrm{sub}}=\frac{d}{S}$, where $s\in\{1\dots S\}$-dimensional
    vector $e$-dimensional sub-vector of the $s$ to denote the $c$-th sub-quantizer.
    For a given vector $e$, denoted $c_{1},\dots,c_{S}$, are the indexes of the nearest
    centroid of each sub-quantizer, i.e.,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的研究表明，变换器中的不精确注意力分数在序列建模中效果良好（Zaheer et al., [2020](#bib.bib39)）。基于这一见解，NoMAD利用产品量化（PQ）（Jegou
    et al., [2010](#bib.bib13)）通过寄存器查找来计算高质量的点积估计。PQ最初设计用于压缩高维向量以实现高效的最近邻搜索，将浮点向量量化为离散代码。它利用子量化器；对于一个$d$维子向量，每个子向量的维度为$d_{\mathrm{sub}}=\frac{d}{S}$，其中$s\in\{1\dots
    S\}$表示第$s$个子量化器的维度向量$e$。对于给定的向量$e$，用$c_{1},\dots,c_{S}$表示每个子量化器的最近质心的索引，即，
- en: '|  | $\displaystyle\mathrm{PQ}(e)=[c_{1}\dots c_{S}],\text{where }c_{s}=\operatorname*{arg\,min}_{c}\big{\lVert}\pi_{s}(v)-b_{s,c}\big{\rVert}$
    |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{PQ}(e)=[c_{1}\dots c_{S}],\text{where }c_{s}=\operatorname*{arg\,min}_{c}\big{\lVert}\pi_{s}(v)-b_{s,c}\big{\rVert}$
    |  |'
- en: Once base vectors have been product-quantized to codes, PQ leverages asymmetric
    distance computation to keep the estimation error low. In the computed distances,
    the original query vector is used while the quantized base vectors are used, hence
    the asymmetry. For a given query $q$ and the $c$-th sub-quantizer using $\mathrm{LUT}_{s}[c]=\mathrm{dist}\big{(}\pi_{s}(q),b_{s,c}\big{)}$
    and a product-quantized base vector $e$, is
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦基向量被量化为代码，PQ利用非对称距离计算来保持估计误差较低。在计算的距离中，使用了原始查询向量，同时使用量化的基向量，因此存在非对称性。对于给定的查询$q$和第$c$个子量化器，使用$\mathrm{LUT}_{s}[c]=\mathrm{dist}\big{(}\pi_{s}(q),b_{s,c}\big{)}$和一个产品量化的基向量$e$，
- en: '|  | $\widetilde{\mathrm{dist}}(q,e)=\sum_{s=1}^{S}\mathrm{LUT}_{s}[c_{s}]$
    |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widetilde{\mathrm{dist}}(q,e)=\sum_{s=1}^{S}\mathrm{LUT}_{s}[c_{s}]$
    |  |'
- en: We extend PQ, which works for metric distances, to estimate dot products for
    attention. We propose to product-quantize the key vectors in attention to produce
    key codes, which will be stored in place of the key cache in LLM attention. The
    codebooks are learned by performing clustering on a set of key vectors from a
    training set. The key vectors are quantized to the nearest centroid with respect
    to L2 distance. For a given query, the query-dependent LUT is computed to hold
    dot products with respect to centroids. Dot products of sub-vectors are retrieved
    from the LUT based on key codes and accumulated to produce the final dot product
    estimates. This procedure allows us to compute attention scores through lookups.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将PQ扩展到度量距离，以估算注意力的点积。我们提出将注意力中的键向量进行产品量化以生成键代码，这些代码将替代LLM注意力中的键缓存。通过对训练集中一组键向量进行聚类来学习代码簿。键向量根据L2距离被量化到最接近的质心。对于给定的查询，计算查询相关的LUT以保存相对于质心的点积。基于键代码从LUT中检索子向量的点积，并累积以生成最终的点积估计。这一过程使我们能够通过查找计算注意力分数。
- en: 3.2 Compressing Lookup Tables into SIMD Registers
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 将查找表压缩到SIMD寄存器中
- en: 'Estimating dot products through PQ mostly eliminates the use of MAD kernels
    in the computation of attention scores. However, this approach yields limited
    speedup over dot-product attention since a high proportion of the CPU cycles are
    wasted due to cache/memory access stalling (Figure [5](#S4.F5 "Figure 5 ‣ 4.4
    Ablation Study ‣ 4 Experiments ‣ NoMAD-Attention: Efficient LLM Inference on CPUs
    Through Multiply-add-free Attention") offers a comparison between the speed of
    PQ and dot product operations). It has been shown that even L1-cache-resident
    LUT is not enough to offer high-performance PQ (André et al., [2016](#bib.bib1)).
    The full potential of lookup-based attention can only be unlocked by having the
    LUT stored in registers, which take only 1-2 CPU cycles to access. However, the
    highly limited size of registers poses a challenge to fitting the LUT. In PQ,
    each sub-quantizer commonly uses 256 centroids, which translates to 8-bit codes.
    Combined with 32-bit floating-point (FP32) dot products, the LUT for each sub-quantizer
    consumes 8192 bits of memory while the SIMD registers are only 128 bits wide.
    To circumvent this limitation in register size, we leverage hardware-aware techniques
    proposed by André et al. ([2017](#bib.bib2)) to enable low-latency retrieval from
    register-resident LUT.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '通过 PQ 估计点积在计算注意力分数时大大减少了 MAD 内核的使用。然而，由于 CPU 周期中的大部分时间被缓存/内存访问延迟所浪费，这种方法在点积注意力上的加速效果有限（图
    [5](#S4.F5 "Figure 5 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ NoMAD-Attention: Efficient
    LLM Inference on CPUs Through Multiply-add-free Attention") 提供了 PQ 和点积操作速度的对比）。研究表明，即使是
    L1 缓存驻留的 LUT 也不足以提供高性能的 PQ（André 等，[2016](#bib.bib1)）。只有将 LUT 存储在寄存器中，才能充分发挥基于查找的注意力的潜力，这样访问只需
    1-2 个 CPU 周期。然而，寄存器的极限大小使得放置 LUT 成为挑战。在 PQ 中，每个子量化器通常使用 256 个质心，这转换为 8 位编码。结合
    32 位浮点（FP32）点积，每个子量化器的 LUT 消耗 8192 位内存，而 SIMD 寄存器仅宽 128 位。为绕过寄存器大小的限制，我们利用 André
    等（[2017](#bib.bib2)）提出的硬件感知技术，以实现从寄存器驻留的 LUT 中低延迟检索。'
- en: 8-bit Quantized Dot Products in LUT Due to the mere 128-bit width of SIMD registers,
    the FP32 representation of dot product is too costly to store. Adopting FP32 dot
    products in LUT implies that each codebook can only contain up to 4 centroids,
    which will no doubt lead to significant quantization errors. Therefore, we adopt
    the 8-bit dynamically quantized representation of dot products. Compressing beyond
    8-bit is infeasible since many SIMD instruction sets do not support parallel lookups
    below 8 bits. The quantization is done dynamically for each query to minimize
    quantization errors. For a given query and sub-quantizer, dot products to centroids
    are first computed in full FP32 precision. Then the quantization range is determined
    by the minimum and maximum dot products to the centroids. Finally, the range is
    evenly divided into $2^{8}$ and $\mathrm{dp}_{\max}=\max_{c}(\pi_{s}(q)\cdot b_{s,c})$
    to the centroids of the $s$ as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 SIMD 寄存器仅有 128 位宽，FP32 表示的点积存储成本过高。在 LUT 中采用 FP32 点积意味着每个代码本只能包含最多 4 个质心，这无疑会导致显著的量化误差。因此，我们采用
    8 位动态量化的点积表示。进一步压缩超过 8 位是不可行的，因为许多 SIMD 指令集不支持低于 8 位的并行查找。量化是针对每个查询动态进行的，以最小化量化误差。对于给定的查询和子量化器，首先以全
    FP32 精度计算点积到质心的值。然后通过点积到质心的最小值和最大值来确定量化范围。最后，将范围均匀划分为 $2^{8}$ 和 $\mathrm{dp}_{\max}=\max_{c}(\pi_{s}(q)\cdot
    b_{s,c})$ 到质心 $s$ 如下：
- en: '|  | $\mathrm{LUT}_{s}[c]=\Big{\lfloor}\frac{(\pi_{s}(q)\cdot b_{s,c})-\mathrm{dp}_{\min}}{(\mathrm{dp}_{\max}-\mathrm{dp}_{\min})/2^{8}}\Big{\rfloor}$
    |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{LUT}_{s}[c]=\Big{\lfloor}\frac{(\pi_{s}(q)\cdot b_{s,c})-\mathrm{dp}_{\min}}{(\mathrm{dp}_{\max}-\mathrm{dp}_{\min})/2^{8}}\Big{\rfloor}$
    |  | (1) |'
- en: The quantization and de-quantization process can be done efficiently without
    much computational overhead, and the quantization error is kept low thanks to
    dynamic query-dependent quantization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 量化和反量化过程可以高效地完成，计算开销很小，并且由于动态查询相关量化，量化误差保持在较低水平。
- en: Constrained Codebook Size By adopting 8-bit quantized dot products in LUT, we
    can fit 16 dot products on 128-bit SIMD registers. This implies that the codebook
    size of each sub-quantizer is constrained to 16 centroids. Although the codebook
    seems limited in size, some evidences suggest it may work well with attention.
    It has been shown that the output of attention loses rank extremely quickly (Dong
    et al., [2021](#bib.bib10)), implying that the intermediate embeddings of transformers
    may exhibit clear clustering structures.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用在 LUT 中量化为 8 位的点积，我们可以在 128 位 SIMD 寄存器中适配 16 个点积。这意味着每个子量化器的代码簿大小被限制为 16
    个质心。虽然代码簿的大小看似有限，但一些证据表明它可能与注意力机制配合得很好。研究表明，注意力的输出会极其快速地丧失秩（Dong et al., [2021](#bib.bib10)），这暗示了变换器的中间嵌入可能表现出明显的聚类结构。
- en: Algorithm 2 NoMAD-Attention Score Computation
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 NoMAD-注意力得分计算
- en: 'Input: query $q^{t}$, key-code cache $K_{\mathrm{code}}^{t-1}$ for $s=1\dots
    S$ \Statex\CommentInsert codes of the current key into the key-code cache \Statelet
    $\mathrm{LUT}_{s}[c]\leftarrow\mathrm{quantize}(\pi_{s}(q^{t})\cdot b_{s,c})$
    \Statex\CommentStore 8-bit quantized dot products (Equation [1](#S3.E1 "Equation
    1 ‣ 3.2 Compressing Lookup Tables into SIMD Registers ‣ 3 Methodology ‣ NoMAD-Attention:
    Efficient LLM Inference on CPUs Through Multiply-add-free Attention")) in LUT
    \Statelet $\mathrm{accu}[1\dots t]\leftarrow 0$ \CommentPerform in-register lookups
    in batch of 32 keys \For$s\leftarrow 1\dots S$ \CommentLoad LUT into registers
    \State$\mathrm{accu}[32i-31\dots 32i]\leftarrow$, \Statex           simd_shuffle$(\mathrm{LUT}_{s},K_{\mathrm{code}}^{32i-31\dots
    32i,s})$\State'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：查询 $q^{t}$，键码缓存 $K_{\mathrm{code}}^{t-1}$ 对于 $s=1\dots S$ \Statex\Comment将当前键的代码插入键码缓存
    \Statelet $\mathrm{LUT}_{s}[c]\leftarrow\mathrm{quantize}(\pi_{s}(q^{t})\cdot
    b_{s,c})$ \Statex\Comment在 LUT 中存储 8 位量化点积（方程 [1](#S3.E1 "方程 1 ‣ 3.2 将查找表压缩到 SIMD
    寄存器 ‣ 3 方法 ‣ NoMAD-注意力：通过无乘加注意力在 CPU 上高效推理")) \Statelet $\mathrm{accu}[1\dots
    t]\leftarrow 0$ \Comment对 32 个键的批量执行寄存器内查找 \For$s\leftarrow 1\dots S$ \Comment将
    LUT 加载到寄存器 \State$\mathrm{accu}[32i-31\dots 32i]\leftarrow$, \Statex           simd_shuffle$(\mathrm{LUT}_{s},K_{\mathrm{code}}^{32i-31\dots
    32i,s})$\State
- en: 3.3 Reorganizing Key Cache Memory Layout
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 重新组织键缓存内存布局
- en: 'Quantized dot products and constrained codebooks enable LUT to be stored in
    SIMD registers, but the layout format of the key cache needs to be reorganized
    to take advantage of SIMD instructions. The original key cache in LLM attention
    stores each key vector contiguously in a row to optimize single vector reads.
    NoMAD-Attention uses the key-code cache in place of the key cache, which stores
    the quantized codes of keys. To allow fast lookups of LUT entries based on key
    codes, we store the key codes in a transposed blocked format. An illustrative
    comparison between the LLM key cache and the NoMAD key-code cache is given in
    Figure [1](#S3.F1 "Figure 1 ‣ 3 Methodology ‣ NoMAD-Attention: Efficient LLM Inference
    on CPUs Through Multiply-add-free Attention").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 量化点积和受限代码簿使 LUT 能够存储在 SIMD 寄存器中，但键缓存的布局格式需要重新组织以利用 SIMD 指令。LLM 注意力中的原始键缓存将每个键向量连续地存储在一行中，以优化单个向量读取。NoMAD-注意力使用键码缓存代替键缓存，键码缓存存储键的量化代码。为了快速查找基于键码的
    LUT 条目，我们将键码存储在转置的块格式中。图 [1](#S3.F1 "图 1 ‣ 3 方法 ‣ NoMAD-注意力：通过无乘加注意力在 CPU 上高效推理")
    给出了 LLM 键缓存和 NoMAD 键码缓存之间的说明性比较。
- en: 'The storage format of the NoMAD key-code cache is transposed: stored in column-major
    order instead of row-major, and blocked: with 32 keys as a block. The SIMD instruction
    shuffle, which we leverage for performing low-latency batch lookups, takes a batch
    of byte-size integers as input and retrieves the values held in the registers
    corresponding to the integer indices. The original storage format of the key cache
    stores all dimensions of a key contiguously, which does not allow efficient use
    of shuffle. To maximize the usage of the LUT held in registers, we store key codes
    belonging to the same sub-quantizer contiguously in rows of 32 codes. Since shuffle
    performs lookups in a batch size of 16, the keys within the same block are stored
    in alternating order. This is because each quantized code occupies half a byte
    as there are 16 centroids in a codebook, while the shuffle instruction uses each
    byte as an input argument. By performing SIMD bit-shifting and bit-masking on
    a block of alternating keys, we obtain the key codes in the original order, ready
    for use with shuffle. More details on how shuffle is performed on each block of
    key-code cache and pseudocode can be found in the appendix.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: NoMAD 键码缓存的存储格式被转置：以列主序而非行主序存储，并且进行块状存储：每块 32 个键。我们利用的 SIMD 指令 shuffle 将字节大小的整数作为输入，并检索寄存器中与整数索引对应的值。键缓存的原始存储格式将键的所有维度连续存储，这不允许高效使用
    shuffle。为了最大化寄存器中 LUT 的使用，我们将属于同一子量化器的键码连续存储在 32 个代码的行中。由于 shuffle 以 16 的批量大小执行查找，因此同一块中的键按交替顺序存储。这是因为每个量化代码占用半个字节，因为代码簿中有
    16 个质心，而 shuffle 指令使用每个字节作为输入参数。通过对一块交替键进行 SIMD 位移和位掩码操作，我们以原始顺序获得键码，准备好与 shuffle
    一起使用。有关如何对每块键码缓存执行 shuffle 的更多细节和伪代码，请参见附录。
- en: 3.4 NoMAD-Attention
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 NoMAD-Attention
- en: 'By combining these three techniques, NoMAD-Attention achieves fast MAD-free
    attention score computations through SIMD in-register lookups. For a given query,
    first, LUTs with 8-bit quantized dot products are computed for each sub-quantizer.
    Then, a LUT is loaded into registers, followed by SIMD shuffle instructions to
    retrieve dot products in the LUT in batch based on key codes. The loading and
    lookup are repeated for all sub-quantizers, and the retrieved dot products are
    accumulated in batch through SIMD add. Finally, the quantized dot products accumulated
    over all sub-quantizers are de-quantized, scaled, and fed through softmax to produce
    the attention scores. The pseudocode for NoMAD-Attention score computations is
    given in Algorithm [2](#alg2 "Algorithm 2 ‣ 3.2 Compressing Lookup Tables into
    SIMD Registers ‣ 3 Methodology ‣ NoMAD-Attention: Efficient LLM Inference on CPUs
    Through Multiply-add-free Attention").'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这三种技术，NoMAD-Attention 实现了通过 SIMD 寄存器查找进行快速的无 MAD 注意力分数计算。对于给定的查询，首先，计算每个子量化器的
    8 位量化点积的 LUT。然后，将一个 LUT 加载到寄存器中，接着使用 SIMD shuffle 指令根据键码批量检索 LUT 中的点积。加载和查找对所有子量化器重复进行，检索到的点积通过
    SIMD 加法批量累加。最后，将所有子量化器累加的量化点积去量化、缩放，并通过 softmax 生成注意力分数。NoMAD-Attention 分数计算的伪代码见算法
    [2](#alg2 "算法 2 ‣ 3.2 压缩查找表到 SIMD 寄存器 ‣ 3 方法 ‣ NoMAD-Attention：通过无乘加注意力在 CPU 上高效推理的
    LLM")。
- en: '![Refer to caption](img/589b29c895df2f366219bffe9bb23f9e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/589b29c895df2f366219bffe9bb23f9e.png)'
- en: 'Figure 2: Value distributions of attention key embeddings of the LLaMA-2-7B
    model on samples of the WikiText-2 dataset. The first 4 attention heads in 4 different
    layers are shown, and all 128 dimensions of the key embeddings are used. Key embeddings
    have different distributions in value across different layers and heads, making
    it necessary for codebooks to be learned independently for each layer and head
    to minimize quantization error.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLaMA-2-7B 模型在 WikiText-2 数据集样本上的注意力键嵌入的值分布。展示了 4 个不同层中的前 4 个注意力头，并使用了键嵌入的所有
    128 个维度。由于不同层和头中的键嵌入值分布不同，因此需要为每个层和头独立学习代码簿以最小化量化误差。
- en: '![Refer to caption](img/cb0762df2147284846414932929d4cb9.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/cb0762df2147284846414932929d4cb9.png)'
- en: 'Figure 3: NoMAD-Attention-based LLMs maintain model quality with negligible
    degradation in perplexity compared to the original model at $8\times$. Dimensionality
    reduction-based PCA-Attention leads to significant model quality degradation even
    at $2\times$ key cache compression.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于 NoMAD-Attention 的 LLM 在 $8\times$ 的情况下，相较于原始模型，保持了模型质量而几乎没有困惑度的降级。基于降维的
    PCA-Attention 即使在 $2\times$ 键缓存压缩下也会导致显著的模型质量降级。
- en: 3.5 Learning Key Compression
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 学习关键压缩
- en: 'Compressing each segment of the attention key into a 4-bit code requires careful
    initialization of centroids to avoid high quantization errors, which can lead
    to degradation in model quality. Ideally, centroids in the codebooks should have
    low L2 distance to the attention key sub-vectors of the corresponding sub-quantizer.
    Empirically, we observe that the value distributions in attention key embeddings
    vary significantly for each attention head and layer. Figure [2](#S3.F2 "Figure
    2 ‣ 3.4 NoMAD-Attention ‣ 3 Methodology ‣ NoMAD-Attention: Efficient LLM Inference
    on CPUs Through Multiply-add-free Attention") illustrates the value distributions
    in attention key embeddings of the LLaMA-2-7B model on samples from the WikiText-2
    dataset. Distinct attention heads have different value ranges and distributional
    skew. Hence, we propose learning codebooks for key compression in the following
    way: we first perform LLM inference with the original attention on a learning
    set of data and record the attention key embeddings for each layer and head. Subsequently,
    we learn codebook centroids for key compression by clustering the key embeddings
    through a K-Means-based algorithm on each attention head independently. These
    centroids become the codebooks for compressing the key-code cache.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '将每个注意力键段压缩为 4 位代码需要仔细初始化质心，以避免高量化误差，这可能导致模型质量下降。理想情况下，代码本中的质心应与相应子量化器的注意力键子向量之间有较低的
    L2 距离。我们观察到注意力键嵌入中的值分布在每个注意力头和层之间差异显著。图 [2](#S3.F2 "图 2 ‣ 3.4 NoMAD-Attention
    ‣ 3 方法论 ‣ NoMAD-Attention: 通过免乘加的 Attention 在 CPU 上高效推理 LLM") 说明了来自 WikiText-2
    数据集的 LLaMA-2-7B 模型注意力键嵌入中的值分布。不同的注意力头有不同的值范围和分布偏斜。因此，我们提出以下方法来学习关键压缩的代码本：首先，在学习数据集上进行原始注意力的
    LLM 推理，并记录每层和每个头的注意力键嵌入。随后，通过基于 K-Means 的算法独立地对每个注意力头的键嵌入进行聚类，学习代码本质心。这些质心将成为压缩键代码缓存的代码本。'
- en: 4 Experiments
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we evaluate the effectiveness of our proposed NoMAD-Attention
    in maintaining model quality and achieving efficient LLM inference on CPUs. In
    particular, we aim to evaluate 1. the model quality of NoMAD-Attention-based LLMs
    compared to the original LLMs, 2. the efficiency of NoMAD-Attention-based LLMs
    compared to attention-based LLMs. We first introduce the software implementation
    and testbed hardware, then detail the experiment setup and baseline methods, and
    finally report experimental results.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们提出的 NoMAD-Attention 在保持模型质量和实现 CPU 上高效 LLM 推理方面的有效性。特别是，我们旨在评估 1.
    基于 NoMAD-Attention 的 LLM 相较于原始 LLM 的模型质量，2. 基于 NoMAD-Attention 的 LLM 相较于基于 Attention
    的 LLM 的效率。我们首先介绍软件实现和测试平台硬件，然后详细说明实验设置和基线方法，最后报告实验结果。
- en: Software Implementation The software system is built in C and C++, based on
    the open-source projects llama.cpp ¹¹1[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
    and FAISS (Douze et al., [2024](#bib.bib11)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 软件实现 该软件系统基于开源项目 llama.cpp ¹¹1[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
    和 FAISS (Douze et al., [2024](#bib.bib11))，使用 C 和 C++ 构建。
- en: Testbed Hardware Experiments are performed on a server running Linux Ubuntu
    20.04, equipped with $2\times$ Intel Xeon E5-2695 V3 14-core CPUs, 512GB of DDR4
    RAM, and 1TB of SSD. The processors used support AVX2 SIMD instructions, which
    we leverage to perform in-register lookups for NoMAD-Attention.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 测试平台硬件 实验在运行 Linux Ubuntu 20.04 的服务器上进行，该服务器配备了 $2\times$ Intel Xeon E5-2695
    V3 14 核 CPU、512GB DDR4 RAM 和 1TB SSD。所用处理器支持 AVX2 SIMD 指令，我们利用这些指令进行 NoMAD-Attention
    的寄存器内查找。
- en: '![Refer to caption](img/90eb62bc24d8c135d79936e435ecb8a7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/90eb62bc24d8c135d79936e435ecb8a7.png)'
- en: 'Figure 4: The efficiency of Attention-based and NoMAD-Attention-based CodeLLaMA-7B
    models on prompt processing and decoding. NoMAD-Attention-based models achieve
    significant speedup over Attention-based counterparts. At the context length of
    16k, NoMAD-Attention-based CodeLlama-7B (4-bit weights) achieves $2\times$ speedup
    over the original CodeLlama-7B (4-bit weights).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：基于 Attention 和 NoMAD-Attention 的 CodeLLaMA-7B 模型在提示处理和解码方面的效率。基于 NoMAD-Attention
    的模型相比于基于 Attention 的模型显著加快。在 16k 的上下文长度下，基于 NoMAD-Attention 的 CodeLlama-7B（4 位权重）比原始的
    CodeLlama-7B（4 位权重）快 $2\times$。
- en: 4.1 Experiment Setup
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 4.1.1 Measuring LLM Quality
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 测量 LLM 质量
- en: We first perform a set of experiments to measure the model quality of NoMAD-Attention-based
    LLMs against baselines. Model quality is measured using the perplexity metric
    (lower the better), which is defined as
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先进行一组实验，以衡量NoMAD-Attention基础的LLMs与基准线的模型质量。模型质量使用困惑度度量（越低越好），其定义为
- en: '|  | $\displaystyle\mathrm{PPL}(x)=\exp\left(-\frac{1}{T}\sum_{i=1}^{T}\log
    P(x_{i}&#124;x_{<i})\right)$ |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{PPL}(x)=\exp\left(-\frac{1}{T}\sum_{i=1}^{T}\log
    P(x_{i}&#124;x_{<i})\right)$ |  |'
- en: where $x=\{x_{i}\}_{1\leq i\leq T}$ is the probability of token $x_{i}$ as context.
    Perplexity is measured on the test set of two datasets, WikiText-2 (Merity et al.,
    [2016](#bib.bib20)) and Penn Treebank (PTB) (Marcus et al., [1993](#bib.bib19)),
    in chunks of length 512\. The LLMs employed in perplexity testing are LLaMA-2-7B
    (Touvron et al., [2023](#bib.bib31)) (with the original 16-bit and quantized 4-bit
    weights) and StableLM-3B-4E1T ([Tow et al.,](#bib.bib32) ) (with 8-bit and 4-bit
    quantized weights).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x=\{x_{i}\}_{1\leq i\leq T}$ 是作为上下文的标记 $x_{i}$ 的概率。困惑度在两个数据集的测试集上进行测量，WikiText-2（Merity等，[2016](#bib.bib20)）和Penn
    Treebank（PTB）（Marcus等，[1993](#bib.bib19)），每次长度为512。困惑度测试中使用的LLMs是LLaMA-2-7B（Touvron等，[2023](#bib.bib31)）（具有原始16位和量化4位权重）和StableLM-3B-4E1T（[Tow等，](#bib.bib32)）（具有8位和4位量化权重）。
- en: Baselines We use LLMs with the original dot-product attention as a baseline,
    and evaluate the model quality of NoMAD-Attention-based LLMs and PCA-Attention-based
    LLMs. Principal Component Analysis (PCA) (Wold et al., [1987](#bib.bib36)) is
    a well-used and studied dimensionality reduction technique. By reducing the dimensionality
    of query and key embeddings via PCA, the efficiency of attention score computation
    can be improved. NoMAD and PCA require codebook learning and projection learning,
    respectively, in which we use the key embeddings of the first 100 samples from
    the training set of WikiText-2 and PTB datasets for learning. This avoids the
    train-test overlap and ensures that the codebooks learned can generalize to unseen
    data. For the same model, the codebooks and projection are learned only once and
    used for different quantization schemes. For the original dot-product attention,
    we vary the compression of the key cache from FP32 to q4_0 and q8_0 (each float
    uses 4.5 and 8.5 bits respectively). For StableLM models, q4_0 and q8_0 key cache
    compression is not supported by the llama.cpp library, hence omitted. For NoMAD
    and PCA, all attention heads in the LLM are replaced with their attention variant.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线 我们使用具有原始点积注意力的LLMs作为基准，并评估NoMAD-Attention基础的LLMs和PCA-Attention基础的LLMs的模型质量。主成分分析（PCA）（Wold等，[1987](#bib.bib36)）是一种常用且研究广泛的降维技术。通过PCA减少查询和键嵌入的维度，可以提高注意力分数计算的效率。NoMAD和PCA分别需要代码簿学习和投影学习，我们使用WikiText-2和PTB数据集的前100个样本的键嵌入进行学习。这避免了训练-测试重叠，并确保学到的代码簿能够推广到未见的数据。对于相同的模型，代码簿和投影只学习一次，并用于不同的量化方案。对于原始点积注意力，我们将键缓存的压缩从FP32变更为q4_0和q8_0（每个浮点数分别使用4.5和8.5位）。对于StableLM模型，llama.cpp库不支持q4_0和q8_0键缓存压缩，因此被省略。对于NoMAD和PCA，LLM中的所有注意力头都被替换为其注意力变体。
- en: 4.1.2 Measuring LLM Efficiency
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 测量LLM效率
- en: For measuring the model efficiency, we use CodeLlama-7B (Roziere et al., [2023](#bib.bib24))
    (with 16-bit and 4-bit weights), a variant of the Llama LLM that supports a long
    context length of 16384\. We sample 10 sequences of varying lengths up to 16K
    from the stack-overflow-questions dataset (Annamoradnejad et al., [2022](#bib.bib3)),
    and use them as prompts to generate 4096 tokens. The baseline implementation is
    based on the llama.cpp implementation. The experiments are run with all available
    28 CPU cores. For efficiency comparisons, we report the time to the first token
    (time to finish prompt processing), decoding time for each token, and decoded
    tokens per second.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量模型效率，我们使用CodeLlama-7B（Roziere等，[2023](#bib.bib24)）（具有16位和4位权重），这是支持16384长上下文长度的Llama
    LLM的一个变体。我们从stack-overflow-questions数据集（Annamoradnejad等，[2022](#bib.bib3)）中采样10个长度各异的序列，最多至16K，并使用它们作为提示生成4096个标记。基准实现基于llama.cpp实现。实验在所有可用的28个CPU核心上运行。为了效率比较，我们报告了第一个标记的时间（完成提示处理的时间）、每个标记的解码时间，以及每秒解码标记数。
- en: 4.2 LLM Quality
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LLM 质量
- en: 'The results of the model quality comparison are shown in Figure [3](#S3.F3
    "Figure 3 ‣ 3.4 NoMAD-Attention ‣ 3 Methodology ‣ NoMAD-Attention: Efficient LLM
    Inference on CPUs Through Multiply-add-free Attention"). Overall, the NoMAD-Attention-based
    LLM maintains model quality at $8\times$) with a negligible loss in perplexity
    (consistently less than a 4% increase). In contrast, the dimensionality-reduction-based
    method PCA fails to maintain model quality at $2\times$ key cache compression,
    or $d_{\mathrm{sub}}=1$ key cache compression, NoMAD-Attention-based LLMs drop
    in quality with increasing compression factor. However, NoMAD-Attention is significantly
    better than PCA-Attention in maintaining model quality. The quality of attention
    drops catastrophically when dimensionality reduction is applied. This is likely
    because dimensionality reduction strategies use symmetric dot-product computations,
    while NoMAD uses asymmetric dot-product computations.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '模型质量比较的结果见图 [3](#S3.F3 "图 3 ‣ 3.4 NoMAD-Attention ‣ 3 方法论 ‣ NoMAD-Attention:
    通过无乘加注意力在 CPU 上高效推理 LLM")。总体而言，基于 NoMAD-Attention 的 LLM 在 $8\times$ 设定下保持了模型质量，困惑度损失几乎可以忽略（始终低于
    4% 的增加）。相比之下，基于降维的方法 PCA 无法在 $2\times$ 键缓存压缩或 $d_{\mathrm{sub}}=1$ 键缓存压缩下保持模型质量，NoMAD-Attention
    基于的 LLM 随着压缩因子的增加而质量下降。然而，NoMAD-Attention 在保持模型质量方面明显优于 PCA-Attention。当应用降维时，注意力质量急剧下降。这可能是因为降维策略使用对称点积计算，而
    NoMAD 使用非对称点积计算。'
- en: 4.3 LLM Efficiency
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 LLM 效率
- en: 'The experimental results of the model efficiency comparison are given in Figure
    [4](#S4.F4 "Figure 4 ‣ 4 Experiments ‣ NoMAD-Attention: Efficient LLM Inference
    on CPUs Through Multiply-add-free Attention"). Since NoMAD-Attention maintains
    model quality well at $8\times$, we explore the speedup of NoMAD-Attention-based
    models at this configuration. We also include results of NoMAD-Attention at $d_{\mathrm{sub}}=2$
    speedup over the original CodeLlama-7B (4-bit weights) at 16k sequence length.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '模型效率比较的实验结果见图 [4](#S4.F4 "图 4 ‣ 4 实验 ‣ NoMAD-Attention: 通过无乘加注意力在 CPU 上高效推理
    LLM")。由于 NoMAD-Attention 在 $8\times$ 设定下保持了良好的模型质量，我们探索了在该配置下基于 NoMAD-Attention
    的模型的加速效果。我们还包括了 $d_{\mathrm{sub}}=2$ 的 NoMAD-Attention 相对于原始的 CodeLlama-7B（4 位权重）在
    16k 序列长度下的加速结果。'
- en: 4.4 Ablation Study
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: 'We study the efficiency of attention score computation and key caching for
    single-head attention, NoMAD-Attention, and PQ-Attention to study the effectiveness
    of our proposed hardware-aware strategies. PQ-Attention quantizes keys into 8-bit
    codes in each sub-quantizer and performs asymmetric dot product computations.
    To match the key compression factor, we use PQ-Attention at $d_{\mathrm{sub}}=2$.
    We perform 16k queries at a context length of 16k and measure the latency of each
    attention in attention score computation and key caching using a single thread.
    The results of the ablation study are given in Figure [5](#S4.F5 "Figure 5 ‣ 4.4
    Ablation Study ‣ 4 Experiments ‣ NoMAD-Attention: Efficient LLM Inference on CPUs
    Through Multiply-add-free Attention"). Despite replacing MADs with lookups, PQ-Attention
    yields limited speedup as compared to dot-product attention. NoMAD-Attention achieves
    $8.3\times$ more than NoMAD-Attention, since finding the code for each sub-quantizer
    takes 256 distance computations as opposed to 16.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究了单头注意力、NoMAD-Attention 和 PQ-Attention 的注意力分数计算和键缓存的效率，以研究我们提出的硬件感知策略的有效性。PQ-Attention
    将键量化为每个子量化器中的 8 位代码，并执行非对称点积计算。为了匹配键压缩因子，我们在 $d_{\mathrm{sub}}=2$ 使用 PQ-Attention。我们在
    16k 的上下文长度下执行了 16k 次查询，并使用单线程测量每次注意力在注意力分数计算和键缓存中的延迟。消融研究的结果见图 [5](#S4.F5 "图 5
    ‣ 4.4 消融研究 ‣ 4 实验 ‣ NoMAD-Attention: 通过无乘加注意力在 CPU 上高效推理 LLM")。尽管用查找替换了 MADs，但与点积注意力相比，PQ-Attention
    提供的加速效果有限。NoMAD-Attention 比 NoMAD-Attention 提供了 $8.3\times$ 的加速，因为每个子量化器寻找代码需要
    256 次距离计算，而不是 16 次。'
- en: '![Refer to caption](img/82e15a79a75c0f92f2eca1328802c6ce.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/82e15a79a75c0f92f2eca1328802c6ce.png)'
- en: 'Figure 5: The latency per query of Attention, PQ-Attention (8-bit code, $d_{\mathrm{sub}}=2$)
    in computing attention scores and key caching for 16k queries at 16k context length.
    PQ-Attention yields limited speedup compared to Attention and incur the most overhead
    in key caching due to the large size of codebooks. NoMAD-Attention significantly
    reduces the latency of attention score computations over Attention.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在计算注意力得分和关键缓存时，Attention、PQ-Attention（8 位代码，$d_{\mathrm{sub}}=2$）每次查询的延迟，针对
    16k 查询和 16k 上下文长度。与 Attention 相比，PQ-Attention 的加速效果有限，并且由于代码本的巨大尺寸，在关键缓存中产生了最大的开销。NoMAD-Attention
    显著减少了注意力得分计算的延迟。
- en: 5 Related Works
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Efficient and Approximate Attention
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高效与近似注意力
- en: Since the introduction of attention in transformers (Vaswani et al., [2017](#bib.bib33)),
    there has been a body of work on approximating the attention mechanism for efficient
    training and inference of transformers. For example, dynamically sparse attention
    has been achieved using LSH (Kitaev et al., [2020](#bib.bib15)), Nyström method
    (Xiong et al., [2021](#bib.bib38)), and random sampling (Zaheer et al., [2020](#bib.bib39)).
    Furthermore, low-rank attention has been extensively explored (Wang et al., [2020](#bib.bib34);
    Choromanski et al., [2020](#bib.bib7); Chen et al., [2021](#bib.bib6)) and shown
    to have compute- and memory-efficiency advantages over regular transformers. Attention
    mechanisms with hardware-aware designs such as FlashAttention (Dao et al., [2022](#bib.bib8))
    have been proposed to mitigate the IO bottleneck in GPUs. In large language models,
    multiple approaches (Zhang et al., [2023b](#bib.bib42); Liu et al., [2023](#bib.bib18))
    have been proposed to reduce the high memory overhead of the KV cache. For CPU-only
    environments, Shen et al. ([2023](#bib.bib25)) proposes to speed up LLM inference
    through weight quantization.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 自从在变换器中引入注意力机制（Vaswani 等，[2017](#bib.bib33)）以来，已经有大量的研究致力于近似注意力机制，以提高变换器的训练和推理效率。例如，通过
    LSH（Kitaev 等，[2020](#bib.bib15)）、Nyström 方法（Xiong 等，[2021](#bib.bib38)）和随机采样（Zaheer
    等，[2020](#bib.bib39)）实现了动态稀疏注意力。此外，低秩注意力（Wang 等，[2020](#bib.bib34)；Choromanski
    等，[2020](#bib.bib7)；Chen 等，[2021](#bib.bib6)）也得到了广泛探索，并显示出相较于普通变换器的计算和内存效率优势。具有硬件感知设计的注意力机制，如
    FlashAttention（Dao 等，[2022](#bib.bib8)），被提出以缓解 GPU 的 I/O 瓶颈。在大型语言模型中，提出了多种方法（Zhang
    等，[2023b](#bib.bib42)；Liu 等，[2023](#bib.bib18)）以减少 KV 缓存的高内存开销。对于仅使用 CPU 的环境，Shen
    等（[2023](#bib.bib25)）提出通过权重量化来加速 LLM 推理。
- en: Matrix Multiplication Optimization and Compression
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 矩阵乘法优化与压缩
- en: Approximate matrix multiplication is applicable in a wide range of computational
    problems from statistical analysis to image compression, and its optimization
    has been a topic of interest for years (Pagh, [2013](#bib.bib22)). Using novel
    compression techniques and specialized hardware, modern researchers have begun
    optimizing matrix multiplication around the specific limitations of computers
    including their memory capacity and traffic between the CPU and main memory (Krishna
    et al., [2021](#bib.bib16)). Compression techniques were developed to multiply
    billion-scale matrices, fully utilize the DSP, and use learning-based algorithms
    on computers (Nelson et al., [2019](#bib.bib21); Blalock & Guttag, [2021](#bib.bib4);
    Krishna et al., [2021](#bib.bib16)). However, many of these algorithms still had
    limitations ranging from training on a matrix to megabytes of hardware resources
    and computation that made the matrix multiplication of LLMs nearly impossible
    to compute without prior training or significant hardware resources (Blalock &
    Guttag, [2021](#bib.bib4); Krishna et al., [2021](#bib.bib16)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 近似矩阵乘法在从统计分析到图像压缩的广泛计算问题中都有应用，其优化多年来一直是一个研究课题（Pagh，[2013](#bib.bib22)）。通过使用新颖的压缩技术和专用硬件，现代研究人员已开始围绕计算机的特定限制（包括内存容量和
    CPU 与主内存之间的流量）优化矩阵乘法（Krishna 等，[2021](#bib.bib16)）。开发了压缩技术以实现十亿规模矩阵的乘法，充分利用 DSP，并在计算机上使用基于学习的算法（Nelson
    等，[2019](#bib.bib21)；Blalock & Guttag，[2021](#bib.bib4)；Krishna 等，[2021](#bib.bib16)）。然而，许多这些算法仍存在限制，从训练一个矩阵到兆字节的硬件资源和计算，使得在没有先前训练或大量硬件资源的情况下，LLM
    的矩阵乘法几乎不可能计算（Blalock & Guttag，[2021](#bib.bib4)；Krishna 等，[2021](#bib.bib16)）。
- en: 6 Conclusion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In conclusion, this study aims to address the challenges of large language model
    inference on Central Processing Units (CPUs), particularly the difficulties associated
    with the expensive Multiply-Add (MAD) matrix operations in attention mechanisms.
    The investigation highlighted the untapped potential of Single-Instruction-Multiple-Data
    (SIMD) registers and their fast in-register lookup capabilities within CPUs. The
    proposed NoMAD-Attention algorithm was introduced as an efficient alternative
    to traditional MAD-based approaches, leveraging in-register lookups and optimizing
    memory access to SIMD registers. Consequently, the implementation of NoMAD-Attention
    resulted in a significant acceleration of LLaMA-7B-based model inference, achieving
    up to a $2\times$ speedup on CPUs. This research underscores the importance of
    exploring novel approaches, such as NoMAD-Attention, to enhance the efficiency
    of large language model inference on CPU architectures.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本研究旨在解决大型语言模型在中央处理单元（CPUs）上的推理挑战，特别是与注意力机制中昂贵的乘加（MAD）矩阵操作相关的困难。调查突显了单指令多数据（SIMD）寄存器及其在CPU中快速的寄存器内查找能力的未开发潜力。提出了NoMAD-Attention算法，作为传统MAD方法的高效替代方案，利用寄存器内查找和优化对SIMD寄存器的内存访问。因此，NoMAD-Attention的实现显著加速了基于LLaMA-7B模型的推理，在CPU上实现了高达$2\times$的加速。本研究强调了探索新方法如NoMAD-Attention的重要性，以提升大型语言模型在CPU架构上的推理效率。
- en: 7 Impact Statement
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 影响声明
- en: This paper aims to democratize large language models (LLMs) by enabling their
    operation on CPU cores, making them accessible to a broader audience. By successfully
    demonstrating the implementation of an LLM on CPU, our study contributes to fostering
    innovation and expansion of cutting-edge LLM technologies to a wider user base.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文旨在通过使大型语言模型（LLMs）在CPU核心上运行，从而使其更加普及，使更广泛的受众能够访问它们。通过成功展示LLM在CPU上的实现，我们的研究有助于促进创新并将前沿LLM技术扩展到更广泛的用户群体。
- en: References
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'André et al. (2016) André, F., Kermarrec, A.-M., and Le Scouarnec, N. Cache
    locality is not enough: High-performance nearest neighbor search with product
    quantization fast scan. In *42nd International Conference on Very Large Data Bases*,
    volume 9, pp.  12, 2016.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: André等（2016）André, F., Kermarrec, A.-M., 和 Le Scouarnec, N. 缓存局部性是不够的：具有产品量化快速扫描的高性能最近邻搜索。在*第42届国际超大规模数据库会议*，第9卷，第12页，2016年。
- en: André et al. (2017) André, F., Kermarrec, A.-M., and Le Scouarnec, N. Accelerated
    nearest neighbor search with quick adc. In *Proceedings of the 2017 ACM on International
    Conference on Multimedia Retrieval*, pp.  159–166, 2017.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: André等（2017）André, F., Kermarrec, A.-M., 和 Le Scouarnec, N. 通过快速adc加速最近邻搜索。在*2017年ACM国际多媒体检索会议论文集*，第159–166页，2017年。
- en: 'Annamoradnejad et al. (2022) Annamoradnejad, I., Habibi, J., and Fazli, M.
    Multi-view approach to suggest moderation actions in community question answering
    sites. *Information Sciences*, 600:144–154, 2022. ISSN 0020-0255. doi: https://doi.org/10.1016/j.ins.2022.03.085.
    URL [https://www.sciencedirect.com/science/article/pii/S0020025522003127](https://www.sciencedirect.com/science/article/pii/S0020025522003127).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Annamoradnejad等（2022）Annamoradnejad, I., Habibi, J., 和 Fazli, M. 多视角方法建议社区问答网站的调节操作。*信息科学*，600:144–154，2022年。ISSN
    0020-0255。doi: https://doi.org/10.1016/j.ins.2022.03.085。网址 [https://www.sciencedirect.com/science/article/pii/S0020025522003127](https://www.sciencedirect.com/science/article/pii/S0020025522003127)。'
- en: Blalock & Guttag (2021) Blalock, D. and Guttag, J. Multiplying matrices without
    multiplying. In Meila, M. and Zhang, T. (eds.), *Proceedings of the 38th International
    Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning
    Research*, pp.  992–1004\. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/blalock21a.html](https://proceedings.mlr.press/v139/blalock21a.html).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blalock & Guttag（2021）Blalock, D. 和 Guttag, J. 不进行矩阵乘法的矩阵乘法。在Meila, M. 和 Zhang,
    T.（编），*第38届国际机器学习会议论文集*，第139卷，*机器学习研究论文集*，第992–1004页。PMLR，2021年7月18–24日。网址 [https://proceedings.mlr.press/v139/blalock21a.html](https://proceedings.mlr.press/v139/blalock21a.html)。
- en: 'Chen et al. (2020) Chen, B., Medini, T., Farwell, J., Tai, C., Shrivastava,
    A., et al. Slide: In defense of smart algorithms over hardware acceleration for
    large-scale deep learning systems. *Proceedings of Machine Learning and Systems*,
    2:291–306, 2020.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen等（2020）Chen, B., Medini, T., Farwell, J., Tai, C., Shrivastava, A., 等。Slide:
    支持智能算法而非硬件加速的大规模深度学习系统。*机器学习与系统会议论文集*，2:291–306，2020年。'
- en: 'Chen et al. (2021) Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and
    Ré, C. Scatterbrain: Unifying sparse and low-rank attention. *Advances in Neural
    Information Processing Systems*, 34:17413–17426, 2021.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2021) Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., 和 Ré, C.
    Scatterbrain：统一稀疏和低秩注意力。*神经信息处理系统进展*，34:17413–17426，2021。
- en: Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song,
    X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al.
    Rethinking attention with performers. *arXiv preprint arXiv:2009.14794*, 2020.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski 等人 (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song, X.,
    Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., 等. 重新思考有表演者的注意力。*arXiv
    预印本 arXiv:2009.14794*，2020。
- en: 'Dao et al. (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. *Advances in Neural
    Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等人 (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., 和 Ré, C. Flashattention：具备
    IO 感知的快速且节省内存的精确注意力。*神经信息处理系统进展*，35:16344–16359，2022。
- en: Dasika et al. (2010) Dasika, G., Woh, M., Seo, S., Clark, N., Mudge, T., and
    Mahlke, S. Mighty-morphing power-simd. In *Proceedings of the 2010 international
    conference on Compilers, architectures and synthesis for embedded systems*, pp. 
    67–76, 2010.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasika 等人 (2010) Dasika, G., Woh, M., Seo, S., Clark, N., Mudge, T., 和 Mahlke,
    S. 强大的变形 Power-SIMD。在 *2010 年国际嵌入式系统编译器、架构和综合会议论文集*，第 67–76 页，2010。
- en: 'Dong et al. (2021) Dong, Y., Cordonnier, J.-B., and Loukas, A. Attention is
    not all you need: Pure attention loses rank doubly exponentially with depth. In
    *International Conference on Machine Learning*, pp.  2793–2803\. PMLR, 2021.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人 (2021) Dong, Y., Cordonnier, J.-B., 和 Loukas, A. 注意力并不是你所需要的一切：纯注意力在深度上的秩以双指数级速率下降。在
    *国际机器学习会议*，第 2793–2803 页。PMLR，2021。
- en: Douze et al. (2024) Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy,
    G., Mazaré, P.-E., Lomeli, M., Hosseini, L., and Jégou, H. The faiss library.
    2024.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Douze 等人 (2024) Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G.,
    Mazaré, P.-E., Lomeli, M., Hosseini, L., 和 Jégou, H. FAISS 库。2024。
- en: 'Han et al. (2023) Han, I., Jayaram, R., Karbasi, A., Mirrokni, V., Woodruff,
    D. P., and Zandieh, A. Hyperattention: Long-context attention in near-linear time.
    *arXiv preprint arXiv:2310.05869*, 2023.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人 (2023) Han, I., Jayaram, R., Karbasi, A., Mirrokni, V., Woodruff, D.
    P., 和 Zandieh, A. Hyperattention：近线性时间中的长上下文注意力。*arXiv 预印本 arXiv:2310.05869*，2023。
- en: Jegou et al. (2010) Jegou, H., Douze, M., and Schmid, C. Product quantization
    for nearest neighbor search. *IEEE transactions on pattern analysis and machine
    intelligence*, 33(1):117–128, 2010.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jegou 等人 (2010) Jegou, H., Douze, M., 和 Schmid, C. 用于最近邻搜索的产品量化。*IEEE 模式分析与机器智能学报*，33(1):117–128，2010。
- en: Kaddour et al. (2023) Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,
    R., and McHardy, R. Challenges and applications of large language models. *arXiv
    preprint arXiv:2307.10169*, 2023.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaddour 等人 (2023) Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,
    R., 和 McHardy, R. 大型语言模型的挑战与应用。*arXiv 预印本 arXiv:2307.10169*，2023。
- en: 'Kitaev et al. (2020) Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The
    efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitaev 等人 (2020) Kitaev, N., Kaiser, Ł., 和 Levskaya, A. Reformer：高效的变换器。*arXiv
    预印本 arXiv:2001.04451*，2020。
- en: Krishna et al. (2021) Krishna, S. G., Narasimhan, A., Radhakrishnan, S., and
    Veras, R. On large-scale matrix-matrix multiplication on compressed structures.
    In *2021 IEEE International Conference on Big Data (Big Data)*, pp.  2976–2985\.
    IEEE, 2021.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等人 (2021) Krishna, S. G., Narasimhan, A., Radhakrishnan, S., 和 Veras,
    R. 大规模矩阵-矩阵乘法在压缩结构上的应用。在 *2021 IEEE 国际大数据会议 (Big Data)*，第 2976–2985 页。IEEE，2021。
- en: 'Lin et al. (2023) Lin, Z., Qu, G., Chen, Q., Chen, X., Chen, Z., and Huang,
    K. Pushing large language models to the 6g edge: Vision, challenges, and opportunities.
    *arXiv preprint arXiv:2309.16739*, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2023) Lin, Z., Qu, G., Chen, Q., Chen, X., Chen, Z., 和 Huang, K. 将大型语言模型推向
    6g 边缘：愿景、挑战和机遇。*arXiv 预印本 arXiv:2309.16739*，2023。
- en: 'Liu et al. (2023) Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z.,
    Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of
    importance hypothesis for llm kv cache compression at test time. *arXiv preprint
    arXiv:2305.17118*, 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis,
    A., 和 Shrivastava, A. Scissorhands：利用重要性假设在测试时进行 LLM KV 缓存压缩。*arXiv 预印本 arXiv:2305.17118*，2023。
- en: 'Marcus et al. (1993) Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A.
    Building a large annotated corpus of English: The Penn Treebank. *Computational
    Linguistics*, 19(2):313–330, 1993. URL [https://www.aclweb.org/anthology/J93-2004](https://www.aclweb.org/anthology/J93-2004).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcus 等人（1993）Marcus, M. P., Santorini, B., 和 Marcinkiewicz, M. A. 建立一个大型英语标注语料库：Penn
    Treebank。*计算语言学*，19(2):313–330，1993年。网址 [https://www.aclweb.org/anthology/J93-2004](https://www.aclweb.org/anthology/J93-2004)。
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models, 2016.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人（2016）Merity, S., Xiong, C., Bradbury, J., 和 Socher, R. 指针哨兵混合模型，2016年。
- en: Nelson et al. (2019) Nelson, M., Radhakrishnan, S., and Sekharan, C. N. Billion-scale
    matrix compression and multiplication with implications in data mining. In *2019
    IEEE 20th International Conference on Information Reuse and Integration for Data
    Science (IRI)*, pp.  395–402\. IEEE, 2019.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nelson 等人（2019）Nelson, M., Radhakrishnan, S., 和 Sekharan, C. N. 十亿规模的矩阵压缩与乘法及其在数据挖掘中的应用。在*2019
    IEEE 第20届信息重用与集成国际会议（IRI）*，第395–402页，IEEE，2019年。
- en: Pagh (2013) Pagh, R. Compressed matrix multiplication. *ACM Transactions on
    Computation Theory (TOCT)*, 5(3):1–17, 2013.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pagh（2013）Pagh, R. 压缩矩阵乘法。*ACM 计算理论事务（TOCT）*，5(3):1–17，2013年。
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I., et al. Language models are unsupervised multitask learners. *OpenAI
    blog*, 1(8):9, 2019.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人（2019）Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever,
    I., 等人。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9，2019年。
- en: 'Roziere et al. (2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open
    foundation models for code. *arXiv preprint arXiv:2308.12950*, 2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere 等人（2023）Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,
    Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., 等人。代码 llama：开源代码基础模型。*arXiv
    预印本 arXiv:2308.12950*，2023年。
- en: Shen et al. (2023) Shen, H., Chang, H., Dong, B., Luo, Y., and Meng, H. Efficient
    llm inference on cpus. *arXiv preprint arXiv:2311.00502*, 2023.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人（2023）Shen, H., Chang, H., Dong, B., Luo, Y., 和 Meng, H. 高效的 CPU 上的 llm
    推理。*arXiv 预印本 arXiv:2311.00502*，2023年。
- en: Shin et al. (2019) Shin, S.-R., Choo, S.-Y., and Park, J.-S. Accelerating random
    network coding using 512-bit simd instructions. In *2019 International Conference
    on Information and Communication Technology Convergence (ICTC)*, pp.  1099–1103\.
    IEEE, 2019.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin 等人（2019）Shin, S.-R., Choo, S.-Y., 和 Park, J.-S. 使用 512 位 SIMD 指令加速随机网络编码。在*2019年国际信息与通信技术融合会议（ICTC）*，第1099–1103页，IEEE，2019年。
- en: Spring & Shrivastava (2017) Spring, R. and Shrivastava, A. Scalable and sustainable
    deep learning via randomized hashing. In *Proceedings of the 23rd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining*, pp.  445–454, 2017.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spring & Shrivastava（2017）Spring, R. 和 Shrivastava, A. 通过随机哈希实现可扩展和可持续的深度学习。在*第23届
    ACM SIGKDD 国际知识发现与数据挖掘会议论文集*，第445–454页，2017年。
- en: Sun et al. (2019) Sun, Y., Agostini, N. B., Dong, S., and Kaeli, D. Summarizing
    cpu and gpu design trends with product data. *arXiv preprint arXiv:1911.11313*,
    2019.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2019）Sun, Y., Agostini, N. B., Dong, S., 和 Kaeli, D. 使用产品数据总结 CPU 和 GPU
    设计趋势。*arXiv 预印本 arXiv:1911.11313*，2019年。
- en: 'Sung et al. (2023) Sung, S., Hur, S., Kim, S., Ha, D., Oh, Y., and Ro, W. W.
    Mad macce: Supporting multiply-add operations for democratizing matrix-multiplication
    accelerators. In *Proceedings of the 56th Annual IEEE/ACM International Symposium
    on Microarchitecture*, pp.  367–379, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung 等人（2023）Sung, S., Hur, S., Kim, S., Ha, D., Oh, Y., 和 Ro, W. W. Mad macce：支持多重加法操作以促进矩阵乘法加速器的普及。在*第56届
    IEEE/ACM 国际微体系结构年会论文集*，第367–379页，2023年。
- en: Thirunavukarasu et al. (2023) Thirunavukarasu, A. J., Ting, D. S. J., Elangovan,
    K., Gutierrez, L., Tan, T. F., and Ting, D. S. W. Large language models in medicine.
    *Nature medicine*, 29(8):1930–1940, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thirunavukarasu 等人（2023）Thirunavukarasu, A. J., Ting, D. S. J., Elangovan, K.,
    Gutierrez, L., Tan, T. F., 和 Ting, D. S. W. 医学中的大语言模型。*自然医学*，29(8):1930–1940，2023年。
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,
    Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等人。Llama 2：开源基础模型和微调聊天模型。*arXiv
    预印本 arXiv:2307.09288*，2023年。
- en: (32) Tow, J., Bellagente, M., Mahan, D., and Riquelme, C. Stablelm 3b 4e1t.
    URL [[https://huggingface.co/stabilityai/stablelm-3b-4e1t](https://huggingface.co/stabilityai/stablelm-3b-4e1t)](%5Bhttps://huggingface.co/stabilityai/stablelm-3b-4e1t%5D(https://huggingface.co/stabilityai/stablelm-3b-4e1t)).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) Tow, J., Bellagente, M., Mahan, D., 和 Riquelme, C. Stablelm 3b 4e1t。网址
    [[https://huggingface.co/stabilityai/stablelm-3b-4e1t](https://huggingface.co/stabilityai/stablelm-3b-4e1t)](https://huggingface.co/stabilityai/stablelm-3b-4e1t)。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. *Advances
    in neural information processing systems*, 30, 2017.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I. 注意力即你所需。*神经信息处理系统进展*，30，2017年。
- en: 'Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer:
    Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*, 2020.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2020）Wang, S., Li, B. Z., Khabsa, M., Fang, H., 和 Ma, H. Linformer:
    具有线性复杂度的自注意力。*arXiv 预印本 arXiv:2006.04768*，2020年。'
- en: Wei et al. (2022) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities
    of large language models. *arXiv preprint arXiv:2206.07682*, 2022.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022）Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., 等。大型语言模型的突现能力。*arXiv 预印本 arXiv:2206.07682*，2022年。
- en: Wold et al. (1987) Wold, S., Esbensen, K., and Geladi, P. Principal component
    analysis. *Chemometrics and intelligent laboratory systems*, 2(1-3):37–52, 1987.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wold 等（1987）Wold, S., Esbensen, K., 和 Geladi, P. 主成分分析。*化学计量学与智能实验室系统*，2(1-3):37–52，1987年。
- en: 'Xiao et al. (2021) Xiao, C., Hu, X., Liu, Z., Tu, C., and Sun, M. Lawformer:
    A pre-trained language model for chinese legal long documents. *AI Open*, 2:79–84,
    2021.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等（2021）Xiao, C., Hu, X., Liu, Z., Tu, C., 和 Sun, M. Lawformer: 一种用于中文法律长文档的预训练语言模型。*AI
    Open*，2:79–84，2021年。'
- en: 'Xiong et al. (2021) Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G.,
    Li, Y., and Singh, V. Nyströmformer: A nyström-based algorithm for approximating
    self-attention. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 35, pp.  14138–14148, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiong 等（2021）Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y.,
    和 Singh, V. Nyströmformer: 一种基于 Nyström 的自注意力近似算法。见 *AAAI 人工智能会议论文集*，第35卷，第 14138–14148
    页，2021年。'
- en: 'Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J.,
    Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big
    bird: Transformers for longer sequences. *Advances in neural information processing
    systems*, 33:17283–17297, 2020.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等（2020）Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti,
    C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., 等。Big bird: 用于更长序列的变换器。*神经信息处理系统进展*，33:17283–17297，2020年。'
- en: Zhang et al. (2023a) Zhang, H., Ning, A., Prabhakar, R., and Wentzlaff, D. A
    hardware evaluation framework for large language model inference. *arXiv preprint
    arXiv:2312.03134*, 2023a.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023a）Zhang, H., Ning, A., Prabhakar, R., 和 Wentzlaff, D. 大型语言模型推理的硬件评估框架。*arXiv
    预印本 arXiv:2312.03134*，2023a。
- en: Zhang et al. (2019) Zhang, W., Yan, Z., Lin, Y., Zhao, C., and Peng, L. A high
    throughput b+ tree for simd architectures. *IEEE Transactions on Parallel and
    Distributed Systems*, 31(3):707–720, 2019.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2019）Zhang, W., Yan, Z., Lin, Y., Zhao, C., 和 Peng, L. 一种用于 SIMD 架构的高吞吐量
    b+ 树。*IEEE 并行与分布式系统汇刊*，31(3):707–720，2019年。
- en: 'Zhang et al. (2023b) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. H $\_2$ o: Heavy-hitter oracle
    for efficient generative inference of large language models. *arXiv preprint arXiv:2306.14048*,
    2023b.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2023b）Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R.,
    Song, Z., Tian, Y., Ré, C., Barrett, C., 等。H $\_2$ o: 大型语言模型高效生成推理的重击者预言机。*arXiv
    预印本 arXiv:2306.14048*，2023b。'
- en: Appendix
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Details Regarding SIMD Instructions
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A SIMD 指令的详细信息
- en: 'The SIMD shuffle presented in Algorithm [2](#alg2 "Algorithm 2 ‣ 3.2 Compressing
    Lookup Tables into SIMD Registers ‣ 3 Methodology ‣ NoMAD-Attention: Efficient
    LLM Inference on CPUs Through Multiply-add-free Attention") is a simplification
    of the actual hardware implementation. We give full details of lines 5 to 11 in
    Algorithm [2](#alg2 "Algorithm 2 ‣ 3.2 Compressing Lookup Tables into SIMD Registers
    ‣ 3 Methodology ‣ NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free
    Attention") in Algorithm [3](#alg3 "Algorithm 3 ‣ Appendix A Details Regarding
    SIMD Instructions ‣ NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free
    Attention"). Keys are stored in blocks of 32, in which keys are stored in an alternating
    order (see Figure [1](#S3.F1 "Figure 1 ‣ 3 Methodology ‣ NoMAD-Attention: Efficient
    LLM Inference on CPUs Through Multiply-add-free Attention") for an illustration).
    After a LUT is loaded into registers, the row of key codes in the block corresponding
    to the sub-quantizer is used to perform shuffle. First, each byte in the row is
    bit-shifted to the right by 4 bits via a SIMD instruction, which produces the
    codes of the first 16 keys in the block. The codes are fed to shuffle to retrieve
    the quantized dot products of the first 16 keys from the LUT. Then, the first
    4 bits of each byte in the row are masked out via a SIMD instruction, which produces
    the code of the last 16 keys in the block. They are similarly used to retrieve
    the quantized dot products from the LUT. The retrieved quantized dot products
    of 32 keys are accumulated in the accumulator. Since quantized dot products are
    8 bits wide, accumulating them in 8-bit accumulators easily results in overflows.
    Therefore, 16-bit accumulators are used to accumulate quantized dot products.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [2](#alg2 "算法 2 ‣ 3.2 将查找表压缩到 SIMD 寄存器 ‣ 3 方法 ‣ NoMAD-Attention：通过乘加自由注意力在
    CPU 上高效推断") 中呈现的 SIMD 洗牌是实际硬件实现的简化版。我们在算法 [2](#alg2 "算法 2 ‣ 3.2 将查找表压缩到 SIMD 寄存器
    ‣ 3 方法 ‣ NoMAD-Attention：通过乘加自由注意力在 CPU 上高效推断") 的第 5 行到第 11 行中详细说明了这些细节，在算法 [3](#alg3
    "算法 3 ‣ 附录 A 关于 SIMD 指令的详细信息 ‣ NoMAD-Attention：通过乘加自由注意力在 CPU 上高效推断") 中进行了补充。键以
    32 个为一块存储，其中键按交替顺序存储（参见图 [1](#S3.F1 "图 1 ‣ 3 方法 ‣ NoMAD-Attention：通过乘加自由注意力在 CPU
    上高效推断") 以获取示意图）。在将 LUT 加载到寄存器中后，使用与子量化器对应的块中的键码行进行洗牌。首先，每个字节在行中通过 SIMD 指令向右移位
    4 位，得到块中前 16 个键的代码。这些代码被送入洗牌操作以从 LUT 中检索前 16 个键的量化点积。然后，行中每个字节的前 4 位通过 SIMD 指令被屏蔽，得到块中最后
    16 个键的代码。这些代码被类似地用于从 LUT 中检索量化点积。32 个键的量化点积被累加到累加器中。由于量化点积宽度为 8 位，将它们累加到 8 位累加器中容易导致溢出。因此，使用
    16 位累加器来累加量化点积。
- en: Algorithm 3 NoMAD Dot-Product Lookup Accumulation Loop
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 NoMAD 点积查找累加循环
- en: let $\mathrm{accu}[1\dots t]\leftarrow 0$ \For$s\leftarrow 1\dots S$) \CommentLoad
    LUT into registers \Statelet $K_{\mathrm{cache}}^{32i-31\dots 32i-16,s}\leftarrow$
    \CommentObtain the first 15 key codes through bit shifting \State$\mathrm{accu}[32i-31\dots
    32i-16]\leftarrow$, \Statex           simd_shuffle$(\mathrm{LUT}_{s},K_{\mathrm{cache}}^{32i-31\dots
    32i-16,s})$simd_bitwise_and$(K_{\mathrm{cache}}^{32i-15\dots 32i,s},\texttt{0xf})$
    simd_add( \Statex           $\mathrm{accu}[32i-15\dots 32i]$) \EndFor\EndFor\State
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: let $\mathrm{accu}[1\dots t]\leftarrow 0$ \For$s\leftarrow 1\dots S$) \Comment将
    LUT 加载到寄存器 \Statelet $K_{\mathrm{cache}}^{32i-31\dots 32i-16,s}\leftarrow$ \Comment通过位移获得前
    15 个键码 \State$\mathrm{accu}[32i-31\dots 32i-16]\leftarrow$, \Statex simd_shuffle$(\mathrm{LUT}_{s},K_{\mathrm{cache}}^{32i-31\dots
    32i-16,s})$simd_bitwise_and$(K_{\mathrm{cache}}^{32i-15\dots 32i,s},\texttt{0xf})$
    simd_add( \Statex $\mathrm{accu}[32i-15\dots 32i]$) \EndFor\EndFor\State
- en: '![Refer to caption](img/6f85d7c01789744a7fc265abd333ca2f.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6f85d7c01789744a7fc265abd333ca2f.png)'
- en: 'Figure 6: Illustration demonstrating the mapping of an input key $k^{t}$-th
    sub-quantizer using $\pi_{s}(k^{t})$. Subsequently, each sub-quantizer maps to
    its closest centroid $c_{i}^{t}$, and the results are stored in the key cache
    $K_{\mathrm{cache}}^{t}$.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：示意图展示了使用 $\pi_{s}(k^{t})$ 映射输入键 $k^{t}$-th 子量化器。随后，每个子量化器映射到其最近的质心 $c_{i}^{t}$，结果存储在键缓存
    $K_{\mathrm{cache}}^{t}$ 中。
- en: '![Refer to caption](img/402fb9e9dbfaa33b75c5eef39548c49f.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/402fb9e9dbfaa33b75c5eef39548c49f.png)'
- en: 'Figure 7: Illustration depicting the mapping of a query vector $q$-th sub-quantizer
    using $\pi_{s}(q)$. Subsequently, the distance between $\pi_{s}(q)$.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：示意图描绘了使用 $\pi_{s}(q)$ 映射查询向量 $q$-th 子量化器。随后，计算 $\pi_{s}(q)$ 之间的距离。
- en: Appendix B Visual Explanations on $\boldsymbol{K_{\mathrm{cache}}}$ and Lookup
    Table (LUT) Construction
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B $\boldsymbol{K_{\mathrm{cache}}}$ 和查找表（LUT）构建的可视化解释
- en: 'Figure [6](#A1.F6 "Figure 6 ‣ Appendix A Details Regarding SIMD Instructions
    ‣ NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention")
    illustrates the process of mapping and compressing key vector $k^{t}$. For an
    input key vector $k^{t}$, where $s\in{1\dots S}$. Subsequently, each sub-quantizer
    $\pi_{s}(k^{t})$ by referencing the codebook $b_{s}$, among 16 centroids in the
    codebook. The resulting values are then stored in the key cache $K_{\mathrm{cache}}^{t}$.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [6](#A1.F6 "图 6 ‣ 附录 A SIMD 指令的详细信息 ‣ NoMAD-Attention：通过免乘加注意力在 CPU 上高效推理")
    说明了键向量 $k^{t}$ 的映射和压缩过程。对于输入键向量 $k^{t}$，其中 $s\in{1\dots S}$。随后，每个子量化器 $\pi_{s}(k^{t})$
    通过参考代码本 $b_{s}$，从代码本中的 16 个质心中进行量化。结果值随后存储在键缓存 $K_{\mathrm{cache}}^{t}$ 中。
- en: 'Similarly, Figure [7](#A1.F7 "Figure 7 ‣ Appendix A Details Regarding SIMD
    Instructions ‣ NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free
    Attention") illustrates the process of mapping and compressing query vector $q^{t}$,
    functions $\pi_{s}$, first split the query into sub-queries $q=(\pi_{1}(q^{t}),\pi_{2}(q^{t}),\pi_{S}(q^{t}))$
    and the 16 centroids from the codebook $b_{s}$.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，图 [7](#A1.F7 "图 7 ‣ 附录 A SIMD 指令的详细信息 ‣ NoMAD-Attention：通过免乘加注意力在 CPU 上高效推理")
    说明了查询向量 $q^{t}$ 的映射和压缩过程，函数 $\pi_{s}$ 首先将查询拆分为子查询 $q=(\pi_{1}(q^{t}),\pi_{2}(q^{t}),\pi_{S}(q^{t}))$
    以及来自代码本 $b_{s}$ 的 16 个质心。
- en: '![Refer to caption](img/da802b1987d384fe090ef6a753b6567b.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da802b1987d384fe090ef6a753b6567b.png)'
- en: 'Figure 8: A speed comparison of NoMAD-Attention-based CodeLlama-7B (4-bit quantized
    weights) at $d_{\mathrm{sub}}=1$ with the original LLM.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：基于 NoMAD-Attention 的 CodeLlama-7B（4 位量化权重）在 $d_{\mathrm{sub}}=1$ 下与原始 LLM
    的速度比较。
- en: Appendix C Efficiency of NoMAD-Attention at Different Compression Rates
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C NoMAD-Attention 在不同压缩率下的效率
- en: 'The results of the model efficiency comparison are depicted in Figure [8](#A2.F8
    "Figure 8 ‣ Appendix B Visual Explanations on 𝑲_𝐜𝐚𝐜𝐡𝐞 and Lookup Table (LUT) Construction
    ‣ NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention").
    Concerning the time required to finish prompt processing, the original model with
    4-bit quantized weights takes $2.8\times 10^{6}$ ms for models with both $d_{\text{sub}}=1$,
    achieving over a $1.5\times$ increase at a 16k prompt length.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 模型效率比较的结果如图 [8](#A2.F8 "图 8 ‣ 附录 B $\boldsymbol{K_{\mathrm{cache}}}$ 和查找表（LUT）构建的可视化解释
    ‣ NoMAD-Attention：通过免乘加注意力在 CPU 上高效推理") 所示。关于完成提示处理所需的时间，原始模型的 4 位量化权重在模型中 $d_{\text{sub}}=1$
    时需要 $2.8\times 10^{6}$ 毫秒，在 16k 提示长度下实现了超过 $1.5\times$ 的增加。
- en: Regarding the decoding time for each token, the original model with 4-bit quantized
    weights takes $450-600$ and $200$ and $d_{\text{sub}}=2$ increase at a 16k prompt
    length.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 关于每个 token 的解码时间，原始模型的 4 位量化权重在 16k 提示长度下需要 $450-600$ 毫秒和 $200$ 毫秒，并且 $d_{\text{sub}}=2$
    的增加。
- en: In terms of throughput measured by tokens per second, at a context length of
    16k, the NoMAD-Attention-based CodeLlama-7B can achieve speeds of $4$ tokens per
    second on models with 16-bit and 4-bit quantized weights, respectively. In contrast,
    the original model only manages $2$ tokens per second, demonstrating more than
    a $2\times$ tokens per second, while the NoMAD-Attention-based CodeLlama-7B can
    achieve speeds of up to $4$ tokens per second for models with $d_{\text{sub}}=1$
    respectively, also achieving over a $2\times$ increase at a 16k prompt length.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 就每秒 token 数量（吞吐量）而言，在 16k 的上下文长度下，基于 NoMAD-Attention 的 CodeLlama-7B 在具有 16 位和
    4 位量化权重的模型上分别可以达到每秒 $4$ 个 token 的速度。相比之下，原始模型仅能达到每秒 $2$ 个 token，表现出超过 $2\times$
    的每秒 token 增加，同时，对于 $d_{\text{sub}}=1$ 的模型，NoMAD-Attention 基于 CodeLlama-7B 可以达到每秒
    $4$ 个 token 的速度，也在 16k 提示长度下实现了超过 $2\times$ 的增加。
- en: Overall, the NoMAD-Attention-based CodeLlama-7B (4-bit quantized weights) achieves
    a $2\times$ speedup over the original CodeLlama-7B (4-bit quantized weights) at
    long prompt and context lengths (e.g., 16k).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，基于 NoMAD-Attention 的 CodeLlama-7B（4 位量化权重）在长提示和上下文长度（例如 16k）下，相较于原始的 CodeLlama-7B（4
    位量化权重）实现了 $2\times$ 的加速。
