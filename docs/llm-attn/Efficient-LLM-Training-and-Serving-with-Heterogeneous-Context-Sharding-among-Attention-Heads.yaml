- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:52:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:52:27'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Efficient LLM Training and Serving with Heterogeneous Context Sharding among
    Attention Heads
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用异质上下文分片在注意力头之间实现高效的LLM训练和服务
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17678](https://ar5iv.labs.arxiv.org/html/2407.17678)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17678](https://ar5iv.labs.arxiv.org/html/2407.17678)
- en: Xihui Lin¹, Yunan Zhang^(1∗), Suyu Ge², Barun Patra¹, Vishrav Chaudhary¹, Xia
    Song¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xihui Lin¹, Yunan Zhang^(1∗), Suyu Ge², Barun Patra¹, Vishrav Chaudhary¹, Xia
    Song¹
- en: ¹Microsoft, ²UIUC
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹Microsoft, ²UIUC
- en: '{xihlin,yunanzhang}@microsoft.com Leading authors. Xihui Lin is the major contributor
    of the kernel. Code is available is at https://github.com/linxihui/dkernel'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{xihlin,yunanzhang}@microsoft.com 主要作者。Xihui Lin是内核的主要贡献者。代码可在 https://github.com/linxihui/dkernel
    获取'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Existing LLM training and inference frameworks struggle in boosting efficiency
    with sparsity while maintaining the integrity of context and model architecture.
    Inspired by the sharding concept in database and the fact that attention parallelizes
    over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention
    algorithm that allocates heterogeneous context partitions for different attention
    heads to divide and conquer. S2-Attention enforces each attention head to only
    attend to a partition of contexts following a strided sparsity pattern, while
    the full context is preserved as the union of all the shards. As attention heads
    are processed in separate thread blocks, the context reduction for each head can
    thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained
    with S2-Attention can then take the KV cache reduction as free meals with guaranteed
    model quality preserve. In experiments, we show S2-Attentioncan provide as much
    as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in
    6X reduction in end-to-end training time and 10X inference latency, (2) on-par
    model training quality compared to default attention, (3)perfect needle retrieval
    accuracy over 32K context window. On top of the algorithm, we build DKernel, an
    LLM training and inference kernel library that allows users to customize sparsity
    patterns for their own models. We open-sourced DKerneland make it compatible with
    Megatron, Pytorch, and vLLM.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的LLM训练和推理框架在提升稀疏性的效率的同时，难以保持上下文和模型架构的完整性。受数据库中的分片概念和注意力机制在加速器上的并行化启发，我们提出了稀疏分片（S2）注意力算法，这是一种将异质上下文分区分配给不同注意力头以实现分而治之的注意力算法。S2-注意力强制每个注意力头仅关注按照步幅稀疏模式的上下文分区，同时保留完整的上下文作为所有分片的联合体。由于注意力头在不同的线程块中处理，因此每个头的上下文减少可以实现端到端的加速和内存减少。在推理阶段，使用S2-注意力训练的LLM可以将KV缓存减少作为免费餐，保证模型质量保持。在实验中，我们展示了S2-注意力可以提供多达（1）25.3倍相对于FlashAttention-2的时钟速度加速，从而使端到端训练时间减少6倍和推理延迟减少10倍，（2）与默认注意力相比的相当的模型训练质量，（3）在32K上下文窗口中的完美检索准确性。在此算法的基础上，我们构建了DKernel，一个允许用户为其模型自定义稀疏模式的LLM训练和推理内核库。我们开源了DKernel，并使其与Megatron、Pytorch和vLLM兼容。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The emergence of transformer-based large language models (OpenAI, [2023](#bib.bib18);
    Touvron et al., [2023](#bib.bib22)) has brought about revolutionary opportunities
    to research and user experience. However, the remarkable success of these models
    across diverse applications also underscores the pressing need for training and
    serving these models economically.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的大型语言模型（OpenAI, [2023](#bib.bib18); Touvron et al., [2023](#bib.bib22)）的出现为研究和用户体验带来了革命性的机遇。然而，这些模型在各种应用中的显著成功也突显了以经济方式训练和服务这些模型的迫切需求。
- en: The core of the transformer architecture is the self-attention mechanism. However,
    self-attention costs quadratic time and memory complexity with respect to the
    context length, making it frustratingly expensive to scale to longer context.
    For example, with 4096 context length, training Llama 2 70b on 2 trillion tokens
    Touvron et al. ([2023](#bib.bib22)) needs 23 days to finish with 2048 A100 GPUs
    Rucinski ([2024](#bib.bib21)), which is estimated to cost 2 million dollars.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构的核心是自注意力机制。然而，自注意力机制在上下文长度方面的时间和内存复杂度是平方级的，这使得扩展到更长的上下文变得令人沮丧地昂贵。例如，使用4096上下文长度，在2万亿个标记上训练Llama
    2 70b，Touvron et al. ([2023](#bib.bib22)) 需要23天才能完成，使用2048个A100 GPU，Rucinski ([2024](#bib.bib21))
    估计成本为200万美元。
- en: '![Refer to caption](img/ff67a29dcda139a7140f7fb2c8f34346.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ff67a29dcda139a7140f7fb2c8f34346.png)'
- en: (a) Speedup over FlashAttention-2\.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 相对于FlashAttention-2的加速。
- en: '![Refer to caption](img/5383b371b0aefad9bed9c2752e1eb793.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5383b371b0aefad9bed9c2752e1eb793.png)'
- en: (b) Needle in a haystack evaluation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 针对针在干草堆中的评估。
- en: 'Figure 1: Training Efficiency and long-context analysis of S2-Attention.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：S2-注意力的训练效率和长上下文分析。
- en: During inference, the forward pass of LLMs typically involves storing the previously
    computed key and value vectors (KV Cache) to avoid recomputing. For instance,
    serving Llama 2 70b requires a staggering 43 GB of KV cache when setting the batch
    size to 16 and the context length to 2048.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，LLM的前向传播通常涉及存储之前计算的键和值向量（KV缓存）以避免重新计算。例如，当批量大小设置为16且上下文长度为2048时，服务Llama
    2 70b需要惊人的43 GB KV缓存。
- en: 'Our study aims to improve the training and inference efficiency of LLM. We
    starts from the observation of the sparsity nature of transformers (Wu et al.,
    [2024](#bib.bib24); Olsson et al., [2022](#bib.bib17); Ge et al., [2024](#bib.bib8)).
    As illustrated in Figure [3](#S3.F3 "Figure 3 ‣ 3 Observation ‣ Efficient LLM
    Training and Serving with Heterogeneous Context Sharding among Attention Heads"),
    despite densely trained, the majority of attention mass still concentrates on
    a few tokens. For most cases, over $95\%$ attention mass can be recalled with
    less than $1\%$ tokens. Also, different attention heads could have dynamically
    different attention mass distribution patterns. As inference works with a sparse
    subset of contexts for each head, can we accelerate training by enforcing this
    sparsity on each head? More specifically, we want to examine how to maintain the
    model quality while maximally reducing the context load of every attention head
    in training. From a hardware perspective, as each attention head is processed
    independently by a distinct thread block, assigning a subset of the context to
    each thread block is similar to the sharding concept in database. Thus, we design
    Sparsely-Sharded (S2) Attention built on three key premises:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究旨在提高LLM的训练和推理效率。我们从观察变换器的稀疏性特征开始（Wu等， [2024](#bib.bib24)；Olsson等， [2022](#bib.bib17)；Ge等，
    [2024](#bib.bib8)）。如图 [3](#S3.F3 "图 3 ‣ 3 观察 ‣ 使用异质上下文分片提高LLM训练和服务效率") 所示，尽管经过密集训练，但大部分注意力质量仍集中在少数几个令牌上。在大多数情况下，超过
    $95\%$ 的注意力质量可以通过少于 $1\%$ 的令牌来回忆。此外，不同的注意力头可能具有动态不同的注意力质量分布模式。由于推理是针对每个头部的稀疏上下文子集进行的，我们是否可以通过在每个头部强制执行这种稀疏性来加速训练？更具体地说，我们想要研究如何在最大限度地减少每个注意力头的上下文负载的同时保持模型质量。从硬件的角度来看，由于每个注意力头由不同的线程块独立处理，将上下文的一个子集分配给每个线程块类似于数据库中的分片概念。因此，我们设计了基于三个关键前提的稀疏分片（S2）注意力：
- en: '![Refer to caption](img/ce4fca41947767c5477b12dfb1f89825.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ce4fca41947767c5477b12dfb1f89825.png)'
- en: 'Figure 2: Illustration of S2-Attentionwith four attention heads on a hypothetical
    GPU with 4 thread blocks. Each attention head is allocated with a shard of the
    context.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：展示了在假设的GPU上具有四个注意力头的S2-注意力，每个注意力头分配了一个上下文分片。
- en: 'P1: In training, we can get equivalent model quality while allocating only
    a partition of the context to each attention head, as long as the union of all
    the shards equals the complete context.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: P1：在训练中，只要所有分片的并集等于完整的上下文，我们可以在仅将上下文的一部分分配给每个注意力头的情况下获得等效的模型质量。
- en: 'P2: Following P1, attention heads should attend to diverse shards. Under the
    same compute budget, models trained with heterogeneous sparsity pattern on attention
    heads would have a better quality over those trained with homogeneous sparsity
    pattern.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: P2：继P1之后，注意力头应关注不同的分片。在相同的计算预算下，使用异质稀疏模式训练的模型比使用同质稀疏模式训练的模型质量更高。
- en: 'P3: Dense layers at certain point are necessary for preserving model quality.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: P3：在某些情况下，密集层对于保持模型质量是必要的。
- en: In this way, S2-Attentioncan be trained with significant fewer FLOPs while maintaining
    the model quality, including long context capabilities, compared to those trained
    with default attention. Compared to previous sparsity method (Child et al., [2019](#bib.bib2);
    Ho et al., [2019](#bib.bib9); Zaheer et al., [2020](#bib.bib27)), sharding the
    context among numerous attention head can achieve higher sparsity per head, while
    explicitly maintaining the full context. Also, the heterogeneous assignment could
    better utilize the multi-head expressiveness compared to previous homogeneous
    designs. During serving, S2-Attentioncan then take KV Cache saving as a free meal
    while having a better quality guarantee, as each attention head is originally
    trained on reduced contexts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，与使用默认注意力训练的模型相比，S2-Attention可以在保持模型质量（包括长上下文能力）的同时，训练所需的FLOPs显著减少。与之前的稀疏方法（Child
    et al., [2019](#bib.bib2); Ho et al., [2019](#bib.bib9); Zaheer et al., [2020](#bib.bib27)）相比，在众多注意力头之间分片上下文可以实现更高的每头稀疏性，同时明确保持完整的上下文。此外，与之前的同质设计相比，异质分配可以更好地利用多头表达能力。在服务过程中，S2-Attention可以在保证更好质量的同时，将KV缓存节省作为免费的收益，因为每个注意力头最初在减少的上下文上进行训练。
- en: In this paper, we provide an instantiation of S2-Attentionthat satisfies all
    the arguments. We first chunk the sequence of tokens into blocks. For an attention
    head $h$, it only attends to the blocks that are adjacent to the current token,
    and blocks whose ids are divisible by a stride after offset by $h$ . A larger
    stride means each attention head receives a smaller, and more sparse shard. The
    stride size is between 1 and number of attention head, which secure the union
    can preserve full context. Lastly, we keep the first two dense layers dense while
    the rest of layers sparse, as we find it’s crucial to maintain the initial layers
    intact.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了一个满足所有论点的S2-Attention的实例。我们首先将令牌序列划分为块。对于一个注意力头 $h$，它仅关注与当前令牌相邻的块，以及其ID在经过
    $h$ 偏移后能被步长整除的块。较大的步长意味着每个注意力头接收到的片段更小且更稀疏。步长大小在1和注意力头数量之间，这确保了联合体可以保持完整的上下文。最后，我们保持前两层稠密层不变，而其余层为稀疏层，因为我们发现保持初始层的完整性至关重要。
- en: We validate the effectiveness of S2-Attentionby evaluating the model quality
    on a wide range of tasks covering context retrieval, common sense reasoning, and
    question answering. Across benchmarks, S2-Attentionachieves on par, or even better
    performance compared to densely trained models. For training efficiency benefit,
    compared to Flash-Attention 2, S2-Attentioncan achieve as much as 8.8X wall-clock
    speed-up at 128K sequence length for 1.3B model. The boost increases as the model
    and context length scales up. On a 70B model with 64 heads, S2-Attentioncan bring
    25.3X speed-up. For inference speed-up, S2-Attentioncan achieves up to 12X speed
    up over FlashAttention-2 on a 1M context window. Lastly, we back S2-Attentionwith
    an optimized kernel library, DKernel, designed to be a plug-in-and-play replacement
    of the popular FlashAttnetion-2 and PyTorch SPDA used in most modern training
    frameworks. It allows users to freely customized the interested pattern they want
    to study in both training and inference. Lastly, we’ve integrated DKernelinto
    vLLM for easy adoption in serving.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在各种任务上评估模型质量来验证S2-Attention的有效性，这些任务涵盖了上下文检索、常识推理和问答。与密集训练模型相比，S2-Attention在基准测试中表现相当，甚至更好。为了训练效率的收益，与Flash-Attention
    2相比，对于1.3B模型在128K序列长度下，S2-Attention可以实现多达8.8倍的实际时间加速。随着模型和上下文长度的增加，这种提升也会增加。在64个头的70B模型上，S2-Attention可以带来25.3倍的加速。对于推理速度提升，S2-Attention在1M上下文窗口上可以比FlashAttention-2快12倍。最后，我们通过优化的内核库DKernel支持S2-Attention，DKernel设计为流行的FlashAttention-2和PyTorch
    SPDA的插件替代品，用于大多数现代训练框架。它允许用户自由自定义他们在训练和推理中希望研究的模式。最后，我们将DKernel集成到vLLM中，方便服务的采用。
- en: 2 Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Efficient Transformers Numerous of attempts have been made to make the training
    and serving of LLMs more efficient. FlashAttention family (Dao et al., [2022](#bib.bib5);
    Dao, [2023](#bib.bib4)) are the most widely-adopted attention acceleration framework.
    Dao et al. ([2022](#bib.bib5)) breaks down the softmax operation into smaller
    blockwise computation to reduce the IO between SRAM and HBM. Dao ([2023](#bib.bib4))
    further improve the performance by reducing the non-matmul operations, extra parallelization
    over sequence length, and better warp organization. Our implementation is based
    on the FlashAttention kernel, and leverage its parallelization over the number
    of heads. Liu et al. ([2023a](#bib.bib13)) further scales the blockwise computation
    into multi-accelerator setting by using each GPU as cache. However, these algorithm
    does not tackle the redundancy in the attention structure, which leaves great
    headroom to further improve efficiency. The FlashAttention family calculates the
    exact self-attention. On the other hand, computation-efficient attention approximation
    mainly leverages low-rank and sparsity approximation to reduce the computational
    complexity of self-attention (Child et al., [2019](#bib.bib2); Katharopoulos et al.,
    [2020](#bib.bib10); Kitaev et al., [2020](#bib.bib11); Zaheer et al., [2020](#bib.bib27);
    Beltagy et al., [2020](#bib.bib1)). Zaheer et al. ([2020](#bib.bib27)) replaces
    the one-to-all attention in transformers with sliding window attention and random
    attention. Qiu et al. ([2020](#bib.bib20)) chunks the sequence into blocks, which
    reduces FLOPs by performing attention on a larger granularity. However, many of
    these methods can’t bring wall-clock speed-up due to ignorance of the IO overhead
    Dao et al. ([2022](#bib.bib5)). Also, restricted by receptive field and the homogeneous
    sparsity patterns among attention heads, these methods under-utilize the multi-head
    structure and limit the sparsity level. Random attention on the other hand, makes
    KV cache undecidable during decoding for generative models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的 Transformers 许多尝试已经致力于提高 LLMs 的训练和服务效率。FlashAttention 系列 (Dao 等人，[2022](#bib.bib5)；Dao，[2023](#bib.bib4))
    是最广泛采用的注意力加速框架。Dao 等人 ([2022](#bib.bib5)) 将 softmax 操作分解为更小的块计算，以减少 SRAM 和 HBM
    之间的 IO。Dao ([2023](#bib.bib4)) 通过减少非 matmul 操作、在序列长度上进行额外的并行化以及更好的 warp 组织来进一步提高性能。我们的实现基于
    FlashAttention 内核，并利用其在头数量上的并行化。Liu 等人 ([2023a](#bib.bib13)) 通过使用每个 GPU 作为缓存，将块计算进一步扩展到多加速器设置。然而，这些算法并未解决注意力结构中的冗余问题，这为进一步提高效率留下了很大的空间。FlashAttention
    系列计算准确的自注意力。另一方面，计算高效的注意力近似主要利用低秩和稀疏近似来降低自注意力的计算复杂度 (Child 等人，[2019](#bib.bib2)；Katharopoulos
    等人，[2020](#bib.bib10)；Kitaev 等人，[2020](#bib.bib11)；Zaheer 等人，[2020](#bib.bib27)；Beltagy
    等人，[2020](#bib.bib1))。Zaheer 等人 ([2020](#bib.bib27)) 用滑动窗口注意力和随机注意力替代了 Transformers
    中的全对一注意力。Qiu 等人 ([2020](#bib.bib20)) 将序列分块，从而通过在更大粒度上进行注意力计算来减少 FLOPs。然而，由于忽视了
    IO 开销 Dao 等人 ([2022](#bib.bib5))，许多这些方法无法带来实时时间的加速。此外，受限于感受野和注意力头之间的同质稀疏模式，这些方法未能充分利用多头结构，并限制了稀疏水平。另一方面，随机注意力在生成模型的解码过程中使
    KV 缓存变得不可决定。
- en: Attention Pattern Analysis Several studies have sought to understand the roles
    individual attention heads play in the Transformer. For encoder-only models, Voita
    et al. ([2019](#bib.bib23)); Michel et al. ([2019](#bib.bib16)); Clark et al.
    ([2019](#bib.bib3)) analyze the self-attention heads in BERT Devlin et al. ([2019](#bib.bib6))
    and find a significant portion of heads attends separator tokens, next or previous
    tokens and a hybrid of them. Even heads in the same layer could have different
    patterns and impact the downstream tasks in a diverse way. Ge et al. ([2024](#bib.bib8));
    Fu ([2024](#bib.bib7)); Wu et al. ([2024](#bib.bib24)); Olsson et al. ([2022](#bib.bib17))
    study the attention head behavior for decoder-only models. They reveal most of
    the attention heads concentrate on a few tokens and categorize these patterns
    to facilitate KV Cache compression.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模式分析 许多研究试图理解在 Transformer 中单个注意力头所扮演的角色。对于仅编码器模型，Voita 等人 ([2019](#bib.bib23))；Michel
    等人 ([2019](#bib.bib16))；Clark 等人 ([2019](#bib.bib3)) 分析了 BERT Devlin 等人 ([2019](#bib.bib6))
    中的自注意力头，发现大量的注意力头关注分隔符标记、下一个或上一个标记以及它们的混合。即使在同一层中的注意力头也可能具有不同的模式，并以多样的方式影响下游任务。Ge
    等人 ([2024](#bib.bib8))；Fu ([2024](#bib.bib7))；Wu 等人 ([2024](#bib.bib24))；Olsson
    等人 ([2022](#bib.bib17)) 研究了仅解码器模型的注意力头行为。他们揭示了大多数注意力头集中在少数几个标记上，并将这些模式分类以便于 KV
    缓存压缩。
- en: Infer-time KV Cache management Along with these studies, a line of inference
    optimization work emerges which evict part of the KV cache at inference based
    on attention score. (Zhang et al., [2023](#bib.bib28); Liu et al., [2023b](#bib.bib14))
    proposes an eviction policy to retain a combination of recent tokens and important
    tokens decided by frequency statistics. Xiao et al. ([2023](#bib.bib25)) proposes
    to only keep the first quarter and local contexts as they accounts for the predominantly
    amount of attention scores. Ge et al. ([2024](#bib.bib8)) studies the head level
    attention pattern at inference, and proposed to use an adaptive hybrid eviction
    policies to better present the attention pattern for each attention head. However,
    these methods cannot guarantee performance preserving, especially long context
    capabilities Li et al. ([2024](#bib.bib12)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 推理时的KV缓存管理。随着这些研究的进展，出现了一系列推理优化工作，这些工作基于注意力分数在推理时逐出部分KV缓存。（Zhang 等，[2023](#bib.bib28)；Liu
    等，[2023b](#bib.bib14)）提出了一种逐出策略，以保留由频率统计决定的近期标记和重要标记的组合。Xiao 等（[2023](#bib.bib25)）建议仅保留前四分之一和局部上下文，因为它们占据了主要的注意力分数。Ge
    等（[2024](#bib.bib8)）研究了推理时的头级注意力模式，并提出使用自适应混合逐出策略，以更好地呈现每个注意力头的注意力模式。然而，这些方法不能保证性能保持，特别是在长上下文能力方面
    Li 等（[2024](#bib.bib12)）。
- en: 3 Observation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 观察
- en: '![Refer to caption](img/92e6c03f63dda17419ef626a64fbfb74.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/92e6c03f63dda17419ef626a64fbfb74.png)'
- en: 'Figure 3: Left: Attention mass sparsity in different heads during decoding
    the same sequence. Right: Sparsity in different layers. Except for the bottom
    layer, all other layers display significant attention sparsity. Experiments are
    done on Llama3-8B with different task prompts, including needle retrieval, summarization
    and math.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：左侧：在解码相同序列时不同头部的注意力稀疏性。右侧：不同层中的稀疏性。除底层外，所有其他层显示出显著的注意力稀疏性。实验在 Llama3-8B
    上进行，使用了不同的任务提示，包括针检索、总结和数学。
- en: In this section, we present an empirical study to see what each attention head
    actually attends to at inference time. We conduct experiments on Llama-3 8B (Meta,
    [2024](#bib.bib15)) using a 2048 context length. We first start with the overall
    layer by layer pattern, then showcase specific sparsity patterns we find.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行了一项实证研究，以查看每个注意力头在推理时实际关注的内容。我们在 Llama-3 8B（Meta，[2024](#bib.bib15)）上进行了实验，使用了2048上下文长度。我们首先从总体层级模式开始，然后展示我们发现的具体稀疏模式。
- en: Scattered Distribution in Initial Layers. In the initial layers, attention score
    is generally uniform for all heads. We call this pattern Uniform Scatter, which
    is illustrated at the right of Figure [3](#S3.F3 "Figure 3 ‣ 3 Observation ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads").
    Such pattern suggests the model does not heavily prioritize any specific part
    of the input. Thus, it’s risky to reduce the context when the head show such patterns.
    Intuitively, these attention layers should be kept dense.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 初始层的分散分布。在初始层中，注意力分数通常在所有头之间均匀分布。我们称这种模式为均匀分散，如图[3](#S3.F3 "图 3 ‣ 3 观察 ‣ 通过在注意力头之间进行异构上下文分片来高效训练和服务
    LLM")右侧所示。这种模式表明模型不会过度优先考虑输入的任何特定部分。因此，当头部显示此类模式时，减少上下文是有风险的。直观上，这些注意力层应保持稠密。
- en: Sparse Patterns in Middle Layers. As we move deeper into the network, the attention
    mechanisms exhibit a marked departure from uniformity. We start with identifying
    simpler patterns. Attention Sink Xiao et al. ([2023](#bib.bib25)) and Locality
    can be seen in a small subset of heads. For these heads, nearly all attention
    probability concentrate on the first few tokens or tokens adjacent to the current
    position, as shown in the upper left and upper right of Figure [3](#S3.F3 "Figure
    3 ‣ 3 Observation ‣ Efficient LLM Training and Serving with Heterogeneous Context
    Sharding among Attention Heads").
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 中层的稀疏模式。随着我们深入网络，注意力机制显著偏离均匀性。我们首先识别更简单的模式。注意力沉淀 Xiao 等（[2023](#bib.bib25)）和局部性可以在少数几个头中看到。对于这些头，几乎所有的注意力概率集中在前几个标记或当前位置信息相邻的标记上，如图[3](#S3.F3
    "图 3 ‣ 3 观察 ‣ 通过在注意力头之间进行异构上下文分片来高效训练和服务 LLM")的左上角和右上角所示。
- en: Middle Spikes, as shown in Figure [3](#S3.F3 "Figure 3 ‣ 3 Observation ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads"),
    can also be identified in a few heads. This pattern puts significant probability
    mass on a few tokens that are in the middle of the context.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3](#S3.F3 "图 3 ‣ 3 观察 ‣ 通过注意力头的异质上下文分片实现高效的LLM训练和服务")所示，Middle Spikes也可以在一些头中识别出。这种模式在上下文中间的几个令牌上分配了大量的概率质量。
- en: However, the vast majority of attention heads actually show a more complex hybrid
    pattern that could be decomposed into patterns mentioned above. For example, hybrids
    like Attention Sink + Local, or Attention Sink + Middle Spikes + Local are the
    most prevalent patterns, which is also demonstrated in Ge et al. ([2024](#bib.bib8));
    Fu ([2024](#bib.bib7)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，绝大多数注意力头实际上显示出更复杂的混合模式，这些模式可以分解为上述模式。例如，像Attention Sink + Local，或者Attention
    Sink + Middle Spikes + Local这样的混合模式是最普遍的，这一点也在Ge等人（[2024](#bib.bib8)）；Fu（[2024](#bib.bib7)）的研究中得到验证。
- en: From the observation, we can conclude that (1) individual attention heads do
    not necessarily take the full context to function, (2) the actual context needed
    by different heads can be quite different, and (3) the initial attention layers
    are usually denser than any other layer. This inspires us to design S2-Attention,
    where the context is diversely sharded and distributed to each attention head.
    We will discuss the details of our method in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从观察中我们可以得出结论：(1) 个别注意力头不一定需要完整的上下文才能正常工作，(2) 不同头所需的实际上下文可以非常不同，(3) 初始注意力层通常比其他层更密集。这启发我们设计S2-Attention，其中上下文被多样地分片并分配给每个注意力头。我们将在下一节中讨论我们方法的细节。
- en: 4 Methodology
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: 4.1 Preliminary
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 初步
- en: In this section, we first present formulation of the context sharding process
    for Sparsely Sharded Attention (S2-Attention). Then we discuss context-sharding
    policies we designed to instantiate S2-Attention. Lastly, we analyze why S2-Attentioncan
    achieve significant wall-clock speed-up for both training and inference.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍了Sparsely Sharded Attention（S2-Attention）上下文分片过程的公式。然后我们讨论了我们设计的上下文分片策略，以实例化S2-Attention。最后，我们分析了为什么S2-Attention能够在训练和推理中实现显著的墙上时钟加速。
- en: 4.2 Context Sharding
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 上下文分片
- en: 'Let $\mathcal{X}=\{x_{1},x_{2},\dots,x_{N}\}$ be the input sequence where $N$
    is the length of the sequence, $\mathcal{H}=\{h_{1},h_{2},\dots,h_{K}\}$, where
    $K$ is the number of attention heads. Each attention head $h_{k}$ is associated
    with a shard $S_{k}$ of the sequence $\mathcal{X}$, determined by a context sharding
    policy $f$. The policy can be viewed as a mapping function $f:\mathcal{H}\to 2^{\mathcal{X}}$,
    where $2^{\mathcal{X}}$ is the power set of $\mathcal{X}$. Each head receives
    a subset of the sequence elements: $f(h_{k})=S_{k}$.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\mathcal{X}=\{x_{1},x_{2},\dots,x_{N}\}$为输入序列，其中$N$是序列的长度，$\mathcal{H}=\{h_{1},h_{2},\dots,h_{K}\}$，其中$K$是注意力头的数量。每个注意力头$h_{k}$与序列$\mathcal{X}$的片段$S_{k}$相关联，该片段由上下文分片策略$f$确定。该策略可以视为映射函数$f:\mathcal{H}\to
    2^{\mathcal{X}}$，其中$2^{\mathcal{X}}$是$\mathcal{X}$的幂集。每个头接收序列元素的一个子集：$f(h_{k})=S_{k}$。
- en: 'We design the sharding process to meet the following properties:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了分片过程以满足以下属性：
- en: '1.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'The union of all the shards equals the full context. Namely, each element $x_{i}$
    in $\mathcal{X}$ must be assigned to at least one shard. Formally, this can be
    expressed as:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有片段的并集等于完整的上下文。即，$\mathcal{X}$中的每个元素$x_{i}$必须分配给至少一个片段。正式地，这可以表示为：
- en: '|  | $\bigcup_{k=1}^{K}S_{k}=\mathcal{X}$ |  |'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\bigcup_{k=1}^{K}S_{k}=\mathcal{X}$ |  |'
- en: '2.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Different attention heads receive heterogeneous context shards. At least two
    heads should receive different subsets of $\mathcal{X}$. Specifically:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同的注意力头接收异质的上下文片段。至少有两个头应该接收不同的$\mathcal{X}$子集。具体来说：
- en: '|  | $\exists i,j\in\{1,\dots,K\},\,i\neq j:S_{i}\neq S_{j}$ |  |'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\exists i,j\in\{1,\dots,K\},\,i\neq j:S_{i}\neq S_{j}$ |  |'
- en: '3.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Initial layers should receive full context for optimal performances. For simplicity,
    we set the initial two layers to be dense and sparsely shared contexts among the
    rest layers.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始层应该接收完整的上下文以实现最佳性能。为了简便起见，我们将初始的两层设置为密集的并且与其余层共享稀疏的上下文。
- en: 4.3 Context Sharding Policy Choices
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 上下文分片策略选择
- en: From our experiments in Figure [3](#S3.F3 "Figure 3 ‣ 3 Observation ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads")
    and previous studies (Ge et al., [2024](#bib.bib8); Wu et al., [2024](#bib.bib24)),
    we summarize 6 basic attention patterns in trained transformers detailed in Section
    [3](#S3 "3 Observation ‣ Efficient LLM Training and Serving with Heterogeneous
    Context Sharding among Attention Heads"). As we found the Local pattern is most
    common across attention heads, we first ensure each shard includes local context.
    For contexts out of this local window, termed as remote context, we would like
    the sharding policy to cover the Attention Sink, Middle Spikes and their hybrid
    combinations Ge et al. ([2024](#bib.bib8)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们在图[3](#S3.F3 "图 3 ‣ 3 观察 ‣ 高效的LLM训练与服务通过注意力头之间的异质上下文分片")中的实验和之前的研究（Ge等，[2024](#bib.bib8);
    Wu等，[2024](#bib.bib24)），我们总结了在训练变换器中6种基本注意力模式，详见第[3](#S3 "3 观察 ‣ 高效的LLM训练与服务通过注意力头之间的异质上下文分片")节。由于我们发现局部模式在注意力头中最为常见，我们首先确保每个分片包含局部上下文。对于超出此局部窗口的上下文，称为远程上下文，我们希望分片策略涵盖注意力沉降、中间峰值及其混合组合Ge等（[2024](#bib.bib8)）。
- en: To capture the huge combinations while maintaining sparsity, we take advantage
    of the inherent expressiveness in the multi-head architecture. Each attention
    head is assigned with strided partitions of the remote contexts, with the starting
    points of these partitions uniquely offset by the index of the corresponding head.
    The heterogeneous assignment allows for a nuanced and scalable coverage of remote
    contexts. By combining the two designs, we introduce a heterogeneous strided sharding
    policy as formulated below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉巨大的组合同时保持稀疏性，我们利用了多头架构中的固有表现力。每个注意力头被分配到远程上下文的步幅分区，这些分区的起始点根据相应头的索引唯一偏移。异质分配允许对远程上下文进行细致而可扩展的覆盖。通过结合这两种设计，我们引入了如下所示的异质步幅分片策略。
- en: 4.4 Heterogeneous Strided Sharding
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 异质步幅分片
- en: KV-cache efficient sharding has a vertical pattern. For $l\geq 1$ and $j\geq
    i$, we say a sharding $S$ is KV cache efficient if $k_{i},v_{i}$ is attended by
    $q_{j+l}$, then it must be also attended by $q_{j}$. Otherwise, $k_{i},v_{i}$
    has to be stored at token $j$ for generating token $j+l$, but not used to generate
    token $j$. In this case, we can just let $q_{j}$ also attend to $k_{i},v_{i}$
    without additional memory for KV cache. However, KV cache can be dropped after
    generating certain tokens. This also means that the sharding $S$ should be based
    on absolute token positions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: KV-cache高效分片具有垂直模式。对于$l\geq 1$和$j\geq i$，我们说分片$S$是KV缓存高效的，如果$k_{i},v_{i}$被$q_{j+l}$关注，那么它也必须被$q_{j}$关注。否则，$k_{i},v_{i}$必须存储在令牌$j$中以生成令牌$j+l$，但不会用于生成令牌$j$。在这种情况下，我们可以让$q_{j}$也关注$k_{i},v_{i}$而无需额外的KV缓存内存。然而，KV缓存可以在生成某些令牌后被丢弃。这也意味着分片$S$应基于绝对令牌位置。
- en: '![Refer to caption](img/d98657c3ee6cf3329e2a02f9c17d52cd.png)![Refer to caption](img/b81ae8d636694001e626dc6747e585e9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d98657c3ee6cf3329e2a02f9c17d52cd.png)![参见标题](img/b81ae8d636694001e626dc6747e585e9.png)'
- en: 'Figure 4: Left: KV-cache efficient sharding. Right: sharding that is not KV-cache
    efficient since token 2 is not attended by tokens 2 - 11, but needs to be stored
    at token 12 to generate token 13 and onward. The upper right corner is masked
    due in case of causal attention.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：左：KV-cache高效分片。右：不是KV-cache高效的分片，因为令牌2没有被令牌2-11关注，但需要在令牌12处存储以生成令牌13及以后。右上角因因果注意力被屏蔽。
- en: Vertical pattern with one fixed stride and a local window. For a transformer
    with $H$ heads, given a sequence of $N$ tokens, we first chunk context into a
    list of blocks of size $S$, which gives us $B=ceil(N/S)$ blocks. We use $q_{i}$
    and $k_{i}$ to denote the query block and key block index. Note that $q_{i}$ corresponds
    to the $i$ th indexed block, instead of the $i$ th token, and one block includes
    $S$ tokens.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直模式具有一个固定步幅和一个局部窗口。对于一个具有$H$个头的变换器，给定一个$N$个令牌的序列，我们首先将上下文分块成大小为$S$的块列表，这样我们就得到$B=ceil(N/S)$块。我们用$q_{i}$和$k_{i}$来表示查询块和键块索引。注意$q_{i}$对应于第$i$个索引块，而不是第$i$个令牌，并且一个块包含$S$个令牌。
- en: Among the total $B$ blocks, We take the most recent $N_{l}$ blocks as local
    blocks and set the rest as remote blocks. Since local blocks are more frequently
    attended, we use different stride sizes for local and remote blocks for fine-grained
    context modeling. Denote the local stride as $v_{l}$ and remote stride as $v_{r}$.
    For simplicity, we use full attention for local blocks by setting $v_{l}=1$, since
    local blocks only accounts for a small portion of the total context and brings
    negligible change to the overall efficiency. In practice, $v_{r}$ is set to be
    a factor number of attention heads to satisfy the “union as full context” constraint.
    For each head $h\in\{1,...,H\}$, we set a unique position offset $o_{h}$. For
    simplicity, we set $o_{h}=h$ here, which means the offset of each head equals
    its head index.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在总共 $B$ 个块中，我们将最新的 $N_{l}$ 个块视为局部块，其余的块视为远程块。由于局部块的注意力频率较高，我们对局部和远程块使用不同的步幅大小，以实现更精细的上下文建模。将局部步幅记为
    $v_{l}$，远程步幅记为 $v_{r}$。为简便起见，我们对局部块使用全注意力，通过设置 $v_{l}=1$，因为局部块只占总上下文的一小部分，对整体效率的影响微乎其微。在实际操作中，$v_{r}$
    设置为注意力头的一个倍数，以满足“联合作为完整上下文”的约束。对于每个头 $h\in\{1,...,H\}$，我们设置一个唯一的位置偏移量 $o_{h}$。为简便起见，我们在此设置
    $o_{h}=h$，这意味着每个头的偏移量等于其头索引。
- en: 'Then for each head $h$, its block attention mask $M^{h}$ in $B\times B$ dimension
    is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个头 $h$，其在 $B\times B$ 维度的块注意力掩码 $M^{h}$ 如下：
- en: '|  | $$M^{h}_{i,j}=\left\{\begin{array}[]{ll}1&amp;q_{i}-k_{j}<N_{l},\\ 1&amp;(k_{j}-o_{h})\in
    v_{r}\mathbb{Z}_{\geq 0}\,\&amp;\,q_{i}-k_{j}\in[N_{l},B)\\'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$M^{h}_{i,j}=\left\{\begin{array}[]{ll}1\&q_{i}-k_{j}<N_{l},\\ 1\&(k_{j}-o_{h})\in
    v_{r}\mathbb{Z}_{\geq 0}\,\&\&\,q_{i}-k_{j}\in[N_{l},B)\\'
- en: 0&amp;\text{otherwise},\end{array}\right.$$ |  | (1) |
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 0\&\text{否则},\end{array}\right.$$ |  | (1) |
- en: where $x\in m\mathbb{Z}_{\geq 0}$ mean $x$ is 0 or a positive multiple of $m$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x\in m\mathbb{Z}_{\geq 0}$ 表示 $x$ 是 0 或 $m$ 的正整数倍。
- en: The first constraint determines the shards within the local window. The second
    constraint controls what shard each head attends to outside the local window.
    Note that if the distance is beyond $B$, the attention is dropped, thus extends
    similar to a sliding window. As an example, setting $B=8,N_{l}=2,v_{l}=1,v_{r}=3,o_{h}=h$
    will give us the sharding as presented in the left part of Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Efficient LLM Training and Serving with Heterogeneous Context
    Sharding among Attention Heads"). $B=8,N_{l}=3,v_{l}=2,v_{r}=3,o_{h}=h$ will give
    us the right part of Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads").
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个约束确定了局部窗口中的分片。第二个约束控制每个头在局部窗口外关注哪个分片。注意，如果距离超出 $B$，则注意力会被丢弃，从而类似于滑动窗口。举例来说，设置
    $B=8,N_{l}=2,v_{l}=1,v_{r}=3,o_{h}=h$ 将得到图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ 通过在注意力头之间的异构上下文分片实现高效
    LLM 训练和服务") 左侧部分的分片。$B=8,N_{l}=3,v_{l}=2,v_{r}=3,o_{h}=h$ 将得到图 [2](#S1.F2 "图 2
    ‣ 1 介绍 ‣ 通过在注意力头之间的异构上下文分片实现高效 LLM 训练和服务") 右侧部分的分片。
- en: '![Refer to caption](img/3f855cd63af99bba10a4b4ebf8de3468.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3f855cd63af99bba10a4b4ebf8de3468.png)'
- en: 'Figure 5: Example Sparsity Patterns.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 示例稀疏模式。'
- en: We pick this neat instantiation but the formulation is quite general. The constraints
    can also be met by choosing different offset $o_{h}$, or different vertical stride
    $v_{i}$, or local attention window size $N_{l}$ for each head, which are all supported
    by our library.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了这个简洁的实例，但其公式是相当通用的。约束条件也可以通过选择不同的偏移量 $o_{h}$，或不同的垂直步幅 $v_{i}$，或每个头的局部注意力窗口大小
    $N_{l}$ 来满足，这些都在我们的库中得到了支持。
- en: Vertical pattern with multiple fixed strides and a local window. This local-stride
    pattern can be easily extended to have multiple strides in different remote blocks.
    With blocks $N_{l}<N_{r_{1}}<N_{r_{2}}<B$, and $v_{r_{1}}<v_{r_{2}}$, we define
    block attention mask as
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直模式具有多个固定步幅和一个局部窗口。这个局部步幅模式可以很容易地扩展到不同远程块中的多个步幅。对于 $N_{l}<N_{r_{1}}<N_{r_{2}}<B$，且
    $v_{r_{1}}<v_{r_{2}}$，我们定义块注意力掩码为
- en: '|  | $$M^{h}_{i,j}=\left\{\begin{array}[]{llll}1&amp;q_{i}-k_{j}\in[0,N_{l}),&amp;&amp;\\
    1&amp;q_{i}-k_{j}\in[N_{l},N_{r_{1}})&amp;\&amp;&amp;(k_{j}-o^{1}_{h})\in v_{r_{1}}\mathbb{Z}_{\geq
    0},\\'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$M^{h}_{i,j}=\left\{\begin{array}[]{llll}1\&q_{i}-k_{j}\in[0,N_{l}),\&\&\\
    1\&q_{i}-k_{j}\in[N_{l},N_{r_{1}})\&\&\&\&(k_{j}-o^{1}_{h})\in v_{r_{1}}\mathbb{Z}_{\geq
    0},\\'
- en: 1&amp;q_{i}-k_{j}\in[N_{r_{2}},B)&amp;\&amp;&amp;(k_{j}-o^{2}_{h})\in v_{r_{2}}\mathbb{Z}_{\geq
    0},\\
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 1\&q_{i}-k_{j}\in[N_{r_{2}},B)\&\&\&\&(k_{j}-o^{2}_{h})\in v_{r_{2}}\mathbb{Z}_{\geq
    0},\\
- en: 0&amp;\text{otherwise}.\end{array}\right.$$ |  | (2) |
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 0\&\text{否则}.\end{array}\right.$$ |  | (2) |
- en: To ensure that $M_{i,j}^{h}$ is KV-cache efficient, we requires that $v_{r_{2}}\in
    v_{r_{1}}\mathbb{Z}_{\geq 0}$ and $(o^{2}_{h}-0^{1}_{h})\in v_{r_{1}}\mathbb{Z}_{\geq
    0}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保 $M_{i,j}^{h}$ 在 KV-cache 中的高效性，我们要求 $v_{r_{2}}\in v_{r_{1}}\mathbb{Z}_{\geq
    0}$ 和 $(o^{2}_{h}-0^{1}_{h})\in v_{r_{1}}\mathbb{Z}_{\geq 0}$。
- en: 4.5 Training and Inference Advantages
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 训练和推理的优势
- en: With the formulation of S2-Attention, we can achieve significant FLOPs reduction
    in training. To train a default self-attention layer, the total FLOPs needed is
    $O(TN)$, where $T$ is the number of tokens to train, $N$ is the max sequence length.
    When using S2-Attention, in case where$v_{l}=1$, we have a dense local window
    of $L$ tokens, and the vertical stride $v_{r}=v$ for the remote context. For each
    attention head, $L+(N-L)/v$ is number of tokens attention to, i.e. the dense equivalent
    sequence length. Then the total FLOPS reduced from $O(TN)$ to $O(T(L+(N-L)/v)$
    at an attention layer. For a 1.3B Llama-like model with 16 attention heads, trained
    on 8192 sequence length, set $L=64$ and $v=16$ will generate a 14.32X FLOPs reduction.
    As the stride $v$ is bounded by the number of heads for the union constraint,
    we can have larger v choices as the model size increases, which will further boost
    the reduction. It can be derived that the upper bound of the speed-up grows linearly
    with the number of attention heads. For example, for a 70B model with 64 attention
    heads, S2-Attentioncan generate 42.89X reduction.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 S2-Attention 的形式化方法，我们可以在训练中显著减少 FLOPs。训练一个默认的自注意力层所需的总 FLOPs 为 $O(TN)$，其中
    $T$ 是训练的标记数量，$N$ 是最大序列长度。当使用 S2-Attention 时，假设 $v_{l}=1$，我们将有一个包含 $L$ 个标记的稠密局部窗口，并且远程上下文的垂直步幅
    $v_{r}=v$。对于每个注意力头，$L+(N-L)/v$ 是关注的标记数量，即稠密等效序列长度。这样，总 FLOPs 从 $O(TN)$ 减少到 $O(T(L+(N-L)/v))$。对于一个具有
    16 个注意力头的 1.3B Llama-like 模型，在 8192 的序列长度上训练，设置 $L=64$ 和 $v=16$ 将产生 14.32 倍的 FLOPs
    减少。由于步幅 $v$ 受限于注意力头数量的并集约束，随着模型规模的增加，我们可以选择更大的 $v$，这将进一步提升减少效果。可以推导出，加速的上限与注意力头的数量呈线性增长。例如，对于一个具有
    64 个注意力头的 70B 模型，S2-Attention 可以实现 42.89 倍的减少。
- en: During inference, the prompt prefilling stage has the same FLOPs saving as the
    forward pass in training. In the decoding phase, despite the FLOPs saving, S2-Attentionwill
    have extra memory footprint reduction. As each attention head is processed in
    one Streaming Multiprocessor(SM), skipping the blocks outside the shard of a given
    head can thus reduce the memory usage, which further boosts throughput.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，提示预填充阶段具有与训练中的前向传播相同的 FLOPs 节省。在解码阶段，尽管有 FLOPs 节省，S2-Attention 还将有额外的内存占用减少。由于每个注意力头在一个
    Streaming Multiprocessor (SM) 中处理，因此跳过给定头部的分片外的块可以减少内存使用，从而进一步提高吞吐量。
- en: 4.6 System Implementation
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 系统实现
- en: Our S2-Attentiontraining kernel is developed upon the Triton flash attention
    code. We’ve compared it to the CUDA version of FlashAttention-2. It accepts a
    sparse sparse pattern tensors specified the per head sparsity pattern on a shard
    level. The underline algorithm turn the sparse matrix to compressed sparse row
    (CSR) format as input, which contains k shard indices for each q shard, then scan
    over the k shard indices to do the FlashAttention in a thread block. We see that
    when vertical stride is set to 1, e.g., normal dense attention, the latency is
    comparable to the original Triton flash attention. Compared to the CUDA version
    of Flash Attention 2, the forward pass has similar latency while the backward
    pass lags slightly. When the vertical stride increases, the performance increase
    almost linearly as shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.6 System Implementation
    ‣ 4 Methodology ‣ Efficient LLM Training and Serving with Heterogeneous Context
    Sharding among Attention Heads"). The memory usage in S2-attention is the same
    as FlashAttention-2 in training regardless of the vertical stride. However at
    inference, during the decoding phase, the KV cache is about $1/v$ of a dense attention
    if local window is considerably small compared to the current sequence length.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的S2-注意力训练内核是基于Triton闪存注意力代码开发的。我们将其与CUDA版本的FlashAttention-2进行了比较。它接受一个指定每个头部稀疏模式的稀疏模式张量。底层算法将稀疏矩阵转换为压缩稀疏行（CSR）格式作为输入，该格式包含每个q
    shard的k shard索引，然后在线程块中扫描k shard索引以进行闪存注意力。我们发现，当垂直步幅设置为1时，例如正常的稠密注意力，延迟与原始Triton闪存注意力相当。与CUDA版本的Flash
    Attention 2相比，前向传递具有类似的延迟，而反向传递稍微滞后。当垂直步幅增加时，性能几乎线性增长，如图[6](#S4.F6 "图 6 ‣ 4.6
    系统实现 ‣ 4 方法 ‣ 使用异构上下文分片的高效LLM训练和服务")所示。无论垂直步幅如何，S2-注意力的内存使用量与FlashAttention-2在训练时相同。然而在推理阶段，如果局部窗口相对于当前序列长度非常小，则KV缓存约为稠密注意力的$1/v$。
- en: Both the CUDA version and the Triton version of flash attention load the whole
    head dimension(D) at once to SRAM, which is intuitive as the whole vector is needed
    to compute the attention scores. However, in our experiment, we found that split
    at the D dimension in many cases is beneficial. We suspect that this is because
    D-split reduce SRAM usage and enable a larger degree of software pipelining. Interestingly,
    we also find that with shard size of 64, D-split only helps when head dimension
    is 128, the most commonly used case, while has no additional benefit when head
    dimension is 64 or 256. We suspect that this is due to under-tuned hyper-parameters
    like kernel block sizes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA版本和Triton版本的闪存注意力都会一次性将整个头部维度（D）加载到SRAM中，这很直观，因为计算注意力分数需要整个向量。然而，在我们的实验中，我们发现D维度的分裂在许多情况下是有益的。我们怀疑这是因为D-split减少了SRAM使用，并且允许更大的软件流水线程度。有趣的是，我们还发现，当分片大小为64时，D-split仅在头部维度为128时（最常用的情况）有帮助，而在头部维度为64或256时没有额外的好处。我们怀疑这是由于像内核块大小这样的超参数未经过调优。
- en: Algorithm 1 Scratch of D-split at the forward pass in a thread block. See Dao
    ([2023](#bib.bib4)) for more detail
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 D-split在线程块中的前向传递草图。有关更多细节，请参见Dao ([2023](#bib.bib4))。
- en: Init SRAM $b,p,q_{0},q_{1},o_{1},o_{2}$Load $q_{0}\leftarrow q[...,:D/2]$, $q_{1}\leftarrow
    q[...,D/2:]$for j in attn_shard_indices do     Load $b\leftarrow k_{0}^{j}$     $p\leftarrow
    q_{0}@b$     Load $b\leftarrow k_{1}^{j}$     $p\leftarrow p+q_{1}@b$     $p\leftarrow\mathbf{softmax}(p)$     Load
    $b\leftarrow v_{0}^{j}$     $o_{1}=update(o_{1},p@b)$     Load $b\leftarrow v_{1}^{j}$     $o_{2}=update(o_{2},p@b)$end for![Refer
    to caption](img/fabc2d7513bccd1e94928ff6c84bc7c4.png)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化SRAM $b,p,q_{0},q_{1},o_{1},o_{2}$ 加载 $q_{0}\leftarrow q[...,:D/2]$, $q_{1}\leftarrow
    q[...,D/2:]$ 对于 j 在 attn_shard_indices 中     加载 $b\leftarrow k_{0}^{j}$     $p\leftarrow
    q_{0}@b$     加载 $b\leftarrow k_{1}^{j}$     $p\leftarrow p+q_{1}@b$     $p\leftarrow\mathbf{softmax}(p)$     加载
    $b\leftarrow v_{0}^{j}$     $o_{1}=update(o_{1},p@b)$     加载 $b\leftarrow v_{1}^{j}$     $o_{2}=update(o_{2},p@b)$
    结束 对于![参见说明](img/fabc2d7513bccd1e94928ff6c84bc7c4.png)
- en: 'Figure 6: Left: Comparison between dense and S2-attention with different sparsity.
    All experiment use (batch, heads, head_dim) = (4, 16, 128) on A100 80GB-SXM. Right:
    Impact of D-split. All experiments use (batch, heads, head_dim) = (4, 16, 128)
    on A100 80GB-SXM. We see D-Split=2 consistently outperforms D-Split=1 in most
    implementation of flash attention on the forward pass. However. the impact on
    the backward pass is negligible.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：左侧：不同稀疏度下，稠密注意力与S2-注意力的比较。所有实验在A100 80GB-SXM上使用(batch, heads, head_dim) =
    (4, 16, 128)。右侧：D-split的影响。所有实验在A100 80GB-SXM上使用(batch, heads, head_dim) = (4,
    16, 128)。我们看到在大多数闪存注意力实现中，D-Split=2在前向传递时始终优于D-Split=1。然而，反向传递的影响可以忽略不计。
- en: 5 Experiment
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: We conduct comprehensive experiments to validate the significant efficiency
    advantage and quality preserving S2-Attentioncan achieve. We first show S2-Attentioncan
    achieve on par, if not better, downstream task performance and context retrieval
    capability compared to default attention in Section [5.1](#S5.SS1 "5.1 Benchmarking
    Model Training Quality ‣ 5 Experiment ‣ Efficient LLM Training and Serving with
    Heterogeneous Context Sharding among Attention Heads") and Section [5.2](#S5.SS2
    "5.2 Context Retrieval ‣ 5 Experiment ‣ Efficient LLM Training and Serving with
    Heterogeneous Context Sharding among Attention Heads"). We then demonstrate the
    wall-clock speed-up of S2-Attentionover FlashAttention-2 across different context
    length settings in Section [5.3](#S5.SS3 "5.3 Training Speed-up ‣ 5 Experiment
    ‣ Efficient LLM Training and Serving with Heterogeneous Context Sharding among
    Attention Heads"). We then present the end-to-end throughput and latency improvement
    of S2-Attentionover the default attention implementation and infer-time KV cache
    management in Section [5.4](#S5.SS4 "5.4 Inference Speed-up ‣ 5 Experiment ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads").
    Lastly, we test our hypotheses for S2-Attentionwith comprehensive ablation studies.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行全面的实验，以验证 S2-Attention 所能实现的显著效率优势和质量保持。我们首先展示 S2-Attention 在下游任务性能和上下文检索能力上能与默认注意力相媲美，甚至更好，如第
    [5.1](#S5.SS1 "5.1 Benchmarking Model Training Quality ‣ 5 Experiment ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads")
    节和第 [5.2](#S5.SS2 "5.2 Context Retrieval ‣ 5 Experiment ‣ Efficient LLM Training
    and Serving with Heterogeneous Context Sharding among Attention Heads") 节所示。接着，我们展示了
    S2-Attention 在不同上下文长度设置下相较于 FlashAttention-2 的实时时钟加速，如第 [5.3](#S5.SS3 "5.3 Training
    Speed-up ‣ 5 Experiment ‣ Efficient LLM Training and Serving with Heterogeneous
    Context Sharding among Attention Heads") 节所示。然后，我们展示了 S2-Attention 相较于默认注意力实现和推理时
    KV 缓存管理的端到端吞吐量和延迟改进，如第 [5.4](#S5.SS4 "5.4 Inference Speed-up ‣ 5 Experiment ‣
    Efficient LLM Training and Serving with Heterogeneous Context Sharding among Attention
    Heads") 节所示。最后，我们通过全面的消融研究测试了我们对 S2-Attention 的假设。
- en: 'Table 1: Training quality evaluation. SWA refers to sliding window attention,
    while S2 refers to S2-Attention. L refers to number of local blocks. V refers
    to the vertical stride size. Dense refers to the idex of dense attention layers.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：训练质量评估。SWA 指滑动窗口注意力，而 S2 指 S2-Attention。L 指本地块的数量。V 指垂直步幅大小。Dense 指密集注意力层的索引。
- en: '| Model | KV reduction | Passkey | WinoGrande | piqa | race | wikitext103(ppl)
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | KV reduction | Passkey | WinoGrande | piqa | race | wikitext103(ppl)
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Dense | 0% | 0.865 | 0.592 | 0.733 | 0.403 | 15.88 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Dense | 0% | 0.865 | 0.592 | 0.733 | 0.403 | 15.88 |'
- en: '| SWA | 92.9% | 0.334 | 0.566 | 0.721 | 0.380 | 21.03 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SWA | 92.9% | 0.334 | 0.566 | 0.721 | 0.380 | 21.03 |'
- en: '| SWA + Dense 1,2 | 85.4% | 0.771 | 0.577 | 0.728 | 0.381 | 17.45 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SWA + Dense 1,2 | 85.4% | 0.771 | 0.577 | 0.728 | 0.381 | 17.45 |'
- en: '| S2-L1V15 | 92.70% | 0.782 | 0.571 | 0.724 | 0.361 | 19.55 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| S2-L1V15 | 92.70% | 0.782 | 0.571 | 0.724 | 0.361 | 19.55 |'
- en: '| S2-L1V15 + Dense 1,2 | 85.0% | 0.941 | 0.587 | 0.725 | 0.397 | 17.18 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| S2-L1V15 + Dense 1,2 | 85.0% | 0.941 | 0.587 | 0.725 | 0.397 | 17.18 |'
- en: '| S2-L8V15 | 87.5% | 0.740 | 0.586 | 0.721 | 0.379 | 17.87 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| S2-L8V15 | 87.5% | 0.740 | 0.586 | 0.721 | 0.379 | 17.87 |'
- en: '| S2-L8V15 + Dense 1,2 | 80.4% | 0.884 | 0.586 | 0.721 | 0.379 | 17.49 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| S2-L8V15 + Dense 1,2 | 80.4% | 0.884 | 0.586 | 0.721 | 0.379 | 17.49 |'
- en: 5.1 Benchmarking Model Training Quality
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基准测试模型训练质量
- en: Experiment Settings. To study the S2-Attention, we train a range of 1.3B models
    with 24 layers, 2048 hidden size with 16 heads, with max sequence length as 8192.
    We use the FineWeb-Edu-350B Penedo et al. ([2024](#bib.bib19)) as the pre-training
    corpus. An OpenAI Tiktoken tokenizer with 100K vocabulary size is used to process
    the raw text. All model variations use batch size of 4M tokens for all sequence
    lengths and train for a total of 300 billion tokens. For hyperparameters, we use
    $\mu$P Yang et al. ([2022](#bib.bib26)) with a base shape of 256. A $\mu$P learning
    rate of 0.02 is used with linear decay and 0.5% of total training tokens for warmup.
    All models are evaluated after training on the total 300B tokens for one epoch.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。为了研究 S2-Attention，我们训练了一系列 1.3B 模型，包含 24 层，2048 隐藏层大小，16 个头，最大序列长度为 8192。我们使用
    FineWeb-Edu-350B Penedo 等 ([2024](#bib.bib19)) 作为预训练语料库。使用了一个 100K 词汇量的 OpenAI
    Tiktoken 分词器来处理原始文本。所有模型变体在所有序列长度下使用 4M tokens 的批量大小，并训练总计 3000 亿 tokens。对于超参数，我们使用
    $\mu$P Yang 等 ([2022](#bib.bib26))，基本形状为 256。使用 0.02 的 $\mu$P 学习率，带有线性衰减和 0.5%
    的总训练 tokens 用于预热。所有模型在训练完总计 300B tokens 的一个周期后进行评估。
- en: We use a model whose attention layers are dense, denoted as Dense, as our baseline.
    We use the standard dense attention as our baseline, which is implemented as flash
    attention in To study the effect of union constraint and heterogeneous sharding,
    we include sliding window attention (SWA) for comparison. For SWA, we control
    the FLOPs to be equivalent to S2-Attention. For S2-Attentionwith 1 local block
    and a vertical stride of 15, the total FLOPs can be translated to SWA whose local
    window size is 9*64=576. To study the effect of initial dense layers, we also
    train a SWA with the first two layers dense.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个注意力层为密集层的模型，称为Dense，作为基线。我们使用标准密集注意力作为基线，该模型实现为To study the effect of
    union constraint and heterogeneous sharding，我们包括滑动窗口注意力（SWA）进行比较。对于SWA，我们控制FLOPs以与S2-Attention相当。对于具有1个本地块和垂直步幅15的S2-Attention，总FLOPs可以转换为SWA，其本地窗口大小为9*64=576。为了研究初始密集层的效果，我们还训练了一个前两层为密集的SWA。
- en: From Table [1](#S5.T1 "Table 1 ‣ 5 Experiment ‣ Efficient LLM Training and Serving
    with Heterogeneous Context Sharding among Attention Heads"), we can observe S2-Attentionshows
    promising results. Compared to fully dense models, S2-Attentioncan achieve on
    par performance for all the downstream tasks. With a delta below 0.008, S2-Attentioncan
    achieves $95.0\%$ KV reduction compared to Dense. The minor gap can be explained
    by the scaling law, as S2-Attentiononly has around $10\%$ attention FLOPs compared
    to Dense. Notably, in the Passkey Retrieval task, S2-Attentioncan achieve much
    better performance compared to Dense, despite each head only has a reduced context.
    This observation demonstrate the context understanding ability of the S2-Attentiondesign.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[1](#S5.T1 "表1 ‣ 5 实验 ‣ 利用异质上下文分片提高LLM训练和服务效率")中，我们可以观察到S2-Attention表现出良好的结果。与完全密集模型相比，S2-Attention在所有下游任务中都能达到相当的性能。与Dense相比，S2-Attention实现了$95.0\%$的KV减少，差异小于0.008。这一小差距可以通过规模法则解释，因为S2-Attention的注意力FLOPs大约只有Dense的$10\%$。值得注意的是，在Passkey
    Retrieval任务中，尽管每个头只有减少的上下文，S2-Attention的表现远胜于Dense。这一观察结果展示了S2-Attention设计的上下文理解能力。
- en: Compared to SWA, we can see S2-Attentionwith the same FLOPs, can consistently
    achieve better performance across all the downstream tasks. Among all the tasks,
    the Passkey Retrieval task is where S2-Attentionand SWA have the largest performance
    margin. The superior performance indicate the benefit of securing the union constraint
    and heterogeneous constraint. Moreover, we can see that for both S2-Attentionand
    SWA can benefit from the initial dense layers. If changing initial layers to sparse,
    all downstream tasks will have a significant downgradation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与SWA相比，我们可以看到具有相同FLOPs的S2-Attention在所有下游任务中始终能够实现更好的性能。在所有任务中，Passkey Retrieval任务是S2-Attention和SWA性能差距最大的任务。优越的性能表明了约束和异质约束的好处。此外，我们还可以看到S2-Attention和SWA都能从初始密集层中获益。如果将初始层更改为稀疏层，所有下游任务的性能都会显著下降。
- en: 5.2 Context Retrieval
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 上下文检索
- en: In this section, we evaluate the long context understanding capability on the
    Needle in a Haystack task. In this task, a factual sentence, “needle”, is inserted
    into texts of 512, 1024, 2048, 4096, 8192, 16384, 32768 tokens. The needle is
    positioned at 0-$100\%$ depth of the original context, where 0 is the start of
    the context and $100\%$ being the end of the context.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了在Needle in a Haystack任务中的长上下文理解能力。在此任务中，一个事实句子“needle”被插入到512、1024、2048、4096、8192、16384、32768个标记的文本中。needle位于原始上下文的0-$100\%$深度位置，其中0为上下文开始，$100\%$为上下文结束。
- en: Following the settings in previous section, we pretrain an 1.3B model on a 350B
    subset of FineWeb-Edu. The first two attention layers are dense while the rest
    of layers deploy S2-Attention, with vertical stride as 15 and local window as
    1. We modified the RoPE base to 1,00,000 to adapt to the longer context window,
    and continue train the model on BookS3.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前面章节的设置，我们在FineWeb-Edu的350B子集上预训练了一个1.3B模型。前两层注意力层为密集层，其余层部署S2-Attention，垂直步幅为15，本地窗口为1。我们将RoPE基数修改为1,00,000以适应更长的上下文窗口，并继续在BookS3上训练模型。
- en: As shown in Figure[1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads"),
    S2-Attentioncan achieves perfect recall on the 32k context window. The results
    validate the long context understanding capability of S2-Attentiondesign.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1(b)](#S1.F1.sf2 "在图1 ‣ 1 引言 ‣ 利用异质上下文分片提高LLM训练和服务效率")所示，S2-Attention在32k上下文窗口上可以实现完美的回忆。结果验证了S2-Attention设计的长上下文理解能力。
- en: 5.3 Training Speed-up
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 训练加速
- en: Benchmark Settings We measure the attention runtime of S2-Attentionand FlashAttention-2
    on an A100 80GB GPU for different context length, number of head, and head dimension
    settings.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基准设置 我们测量了在 A100 80GB GPU 上，S2-Attention 和 FlashAttention-2 在不同上下文长度、头数和头维度设置下的注意力运行时间。
- en: '![Refer to caption](img/19a966e693ce7291cebf3b04f3e0d401.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/19a966e693ce7291cebf3b04f3e0d401.png)'
- en: 'Figure 7: End-to-end latency speed-up for 1.3B Llama architecture models.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：1.3B Llama 架构模型的端到端延迟加速。
- en: We benchmark 3 model sizes to showcase the scalability of our system. Figure
    [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ Efficient LLM Training and Serving
    with Heterogeneous Context Sharding among Attention Heads") shows the 1.3B model
    results. Figure [7](#S5.F7 "Figure 7 ‣ 5.3 Training Speed-up ‣ 5 Experiment ‣
    Efficient LLM Training and Serving with Heterogeneous Context Sharding among Attention
    Heads") shows the 70B speedup. For all the model sizes, S2-Attentioncan achieve
    multiple times of speed-up over FlashAttention-2. For 1.3B models with 16 heads,
    S2-Attentioncan achieve as much as 8.79X speed-up as the max sequence length grows
    longer. For 70B models with 64 heads, S2-Attentioncan give 25.3X end-to-end speed-up.
    Taken forward pass alone, S2-Attentioncan generate 37.58X reduction. The overall
    boost is hedged due to our less optimized backward kernel, which leaves room for
    further improvement.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基准测试了 3 种模型大小，以展示系统的可扩展性。图 [1(a)](#S1.F1.sf1 "图 1 ‣ 1 引言 ‣ 使用异构上下文分片进行高效 LLM
    训练和服务") 显示了 1.3B 模型的结果。图 [7](#S5.F7 "图 7 ‣ 5.3 训练加速 ‣ 5 实验 ‣ 使用异构上下文分片进行高效 LLM
    训练和服务") 显示了 70B 的加速。对于所有模型大小，S2-Attention 能够实现比 FlashAttention-2 多倍的加速。对于 1.3B
    模型（16 个头），S2-Attention 随着最大序列长度增加，能实现高达 8.79 倍的加速。对于 70B 模型（64 个头），S2-Attention
    可以实现 25.3 倍的端到端加速。单独考虑前向传递，S2-Attention 可以实现 37.58 倍的减少。由于我们的反向内核优化不足，总体提升有限，还有进一步改进的空间。
- en: 5.4 Inference Speed-up
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 推理加速
- en: '![Refer to caption](img/62785c96c016417efbd58e0065fa827f.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/62785c96c016417efbd58e0065fa827f.png)'
- en: 'Figure 8: End-to-end latency speed-up for 7B size Llama architecture models.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：7B 大小 Llama 架构模型的端到端延迟加速。
- en: In order to demonstrate the inference improvments of S2-Attention, we measure
    the end-to-end prefilling and decoding latency over different context length settings
    in Figure [8](#S5.F8 "Figure 8 ‣ 5.4 Inference Speed-up ‣ 5 Experiment ‣ Efficient
    LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads").
    We choose the FlashAttention-2 backend in vLLM as baseline for fair comparison.
    Both methods are deployed on a single node with 8 A100 80GPU, with tensor parallel
    size setting as 8. For prefilling speed-up, we set output length as 1, and vary
    input length between 16k to 512k. As shown in Figure [8](#S5.F8 "Figure 8 ‣ 5.4
    Inference Speed-up ‣ 5 Experiment ‣ Efficient LLM Training and Serving with Heterogeneous
    Context Sharding among Attention Heads"), S2-Attentioncan achieves 1.2X, 2.7X,
    3X, 5.7X speed-up on 16k, 128K, 256K, and 512K context.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 S2-Attention 的推理改进，我们测量了不同上下文长度设置下的端到端预填充和解码延迟，如图 [8](#S5.F8 "图 8 ‣ 5.4
    推理加速 ‣ 5 实验 ‣ 使用异构上下文分片进行高效 LLM 训练和服务") 所示。我们选择了 vLLM 中的 FlashAttention-2 后端作为公平比较的基线。两种方法都在配置为
    8 的张量并行设置下部署在单个节点上，使用 8 个 A100 80GPU。对于预填充加速，我们将输出长度设置为 1，并在 16k 到 512k 之间变化输入长度。如图
    [8](#S5.F8 "图 8 ‣ 5.4 推理加速 ‣ 5 实验 ‣ 使用异构上下文分片进行高效 LLM 训练和服务") 所示，S2-Attention
    在 16k、128K、256K 和 512K 上下文下分别实现了 1.2 倍、2.7 倍、3 倍和 5.7 倍的加速。
- en: 6 Conclusion
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we introduce S2-Attention, a framework that sparsely shards the
    context for different attention heads to divide-and-conquer. Experimental results
    show model trained with S2-Attentioncan achieve promising performance on long
    context tasks with reduced context for each head. We back S2-Attentionwith a highly
    optimized kernel library, we can get equivalent model quality while achieving
    speed-up over FlashAttention-2 linearly increasing over the number of attention
    heads. We open-sourced our kernel library and make it a plug-in-and-play alternative
    for FlashAttention-2 module in popular training frameworks like Megatron and Pytorch.
    We also integrated S2-Attentioninto vLLM backend for instant serving. Both the
    training and inference kernels allow users to freely customize their sparsity
    pattern, facilitating the whole community to study the topic in the future.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 S2-Attention，一个将上下文稀疏分割给不同注意力头以进行分而治之的框架。实验结果显示，使用 S2-Attention 训练的模型在长上下文任务中可以取得良好的性能，每个头的上下文量减少。我们通过一个高度优化的内核库支持
    S2-Attention，可以在保持模型质量的同时，在线性增加的注意力头数量上获得相对于 FlashAttention-2 的加速。我们开源了我们的内核库，并将其作为
    FlashAttention-2 模块在流行的训练框架如 Megatron 和 Pytorch 中的即插即用替代品。我们还将 S2-Attention 集成到
    vLLM 后端以进行即时服务。训练和推理内核允许用户自由定制其稀疏模式，促进整个社区未来对该主题的研究。
- en: 7 Acknowledgement
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: We would like to thank Liyuan Liu and Yao Fu from Microsoft Research and University
    of Edinburgh for insightful discussions. We also would like to thank Zhuohan Li,
    Simon Mo, and Kaichao You from UC Berkeley and Tsinghua University for sharing
    expertise on the vLLM codebase.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢微软研究院和爱丁堡大学的刘丽媛和傅耀的深刻讨论。我们还要感谢加州大学伯克利分校和清华大学的李卓翰、莫西蒙和游开超，他们在 vLLM 代码库上的专业指导。
- en: References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer:
    The long-document transformer. *CoRR*, abs/2004.05150, 2020. URL [https://arxiv.org/abs/2004.05150](https://arxiv.org/abs/2004.05150).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy 等（2020）Iz Beltagy, Matthew E. Peters 和 Arman Cohan. Longformer：长文档变换器。*CoRR*，abs/2004.05150，2020
    年。网址 [https://arxiv.org/abs/2004.05150](https://arxiv.org/abs/2004.05150)。
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. *CoRR*, abs/1904.10509, 2019.
    URL [http://arxiv.org/abs/1904.10509](http://arxiv.org/abs/1904.10509).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等（2019）Rewon Child, Scott Gray, Alec Radford 和 Ilya Sutskever. 使用稀疏变换器生成长序列。*CoRR*，abs/1904.10509，2019
    年。网址 [http://arxiv.org/abs/1904.10509](http://arxiv.org/abs/1904.10509)。
- en: 'Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. What does BERT look at? an analysis of BERT’s attention. In *Proceedings
    of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks
    for NLP*, pp.  276–286, Florence, Italy, August 2019\. Association for Computational
    Linguistics. doi: 10.18653/v1/W19-4828. URL [https://aclanthology.org/W19-4828](https://aclanthology.org/W19-4828).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等（2019）Kevin Clark, Urvashi Khandelwal, Omer Levy 和 Christopher D. Manning.
    BERT 关注什么？对 BERT 注意力的分析。载于 *2019 年 ACL 研讨会 BlackboxNLP：分析和解释 NLP 神经网络*，第 276–286
    页，意大利佛罗伦萨，2019 年 8 月。计算语言学协会。doi: 10.18653/v1/W19-4828。网址 [https://aclanthology.org/W19-4828](https://aclanthology.org/W19-4828)。'
- en: 'Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *CoRR*, abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691.
    URL [https://doi.org/10.48550/arXiv.2307.08691](https://doi.org/10.48550/arXiv.2307.08691).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao（2023）Tri Dao. Flashattention-2：更快的注意力，具有更好的并行性和工作划分。*CoRR*，abs/2307.08691，2023
    年。doi: 10.48550/ARXIV.2307.08691。网址 [https://doi.org/10.48550/arXiv.2307.08691](https://doi.org/10.48550/arXiv.2307.08691)。'
- en: 'Dao et al. (2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh
    (eds.), *Advances in Neural Information Processing Systems 35: Annual Conference
    on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,
    USA, November 28 - December 9, 2022*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等（2022）Tri Dao、Daniel Y. Fu、Stefano Ermon、Atri Rudra 和 Christopher Ré。Flashattention：具有IO意识的快速且内存高效的精确注意力。在
    Sanmi Koyejo、S. Mohamed、A. Agarwal、Danielle Belgrave、K. Cho 和 A. Oh（编辑），*神经信息处理系统进展
    35：2022年神经信息处理系统年会，NeurIPS 2022，美国路易斯安那州新奥尔良，2022年11月28日-12月9日*，2022年。网址 [http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html)。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: pre-training of deep bidirectional transformers for language
    understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
    June 2-7, 2019, Volume 1 (Long and Short Papers)*, pp.  4171–4186\. Association
    for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等（2019）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。BERT：用于语言理解的深度双向变换器的预训练。在
    Jill Burstein、Christy Doran 和 Thamar Solorio（编辑），*2019年北美计算语言学协会人类语言技术会议论文集：NAACL-HLT
    2019，美国明尼阿波利斯，2019年6月2-7日，第1卷（长文和短文）*，页码 4171–4186。计算语言学协会，2019年。doi: 10.18653/v1/n19-1423。网址
    [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)。'
- en: Fu (2024) Yao Fu. How do language models put attention weights over long context?
    *Yao Fu’s Notion*, Mar 2024. URL [https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context](https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu（2024）Yao Fu。语言模型如何在长上下文上分配注意力权重？*Yao Fu的Notion*，2024年3月。网址 [https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context](https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context)。
- en: 'Ge et al. (2024) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han,
    and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression
    for llms. In *The Twelfth International Conference on Learning Representations,
    ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net, 2024. URL [https://openreview.net/pdf?id=88nT0j5jAn](https://openreview.net/pdf?id=88nT0j5jAn).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等（2024）Suyu Ge、Yunan Zhang、Liyuan Liu、Minjia Zhang、Jiawei Han 和 Jianfeng
    Gao。模型告诉你该丢弃什么：对大型语言模型的自适应KV缓存压缩。在 *第十二届国际学习表征会议，ICLR 2024，奥地利维也纳，2024年5月7-11日*。OpenReview.net，2024。网址
    [https://openreview.net/pdf?id=88nT0j5jAn](https://openreview.net/pdf?id=88nT0j5jAn)。
- en: Ho et al. (2019) Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans.
    Axial attention in multidimensional transformers. *CoRR*, abs/1912.12180, 2019.
    URL [http://arxiv.org/abs/1912.12180](http://arxiv.org/abs/1912.12180).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等（2019）Jonathan Ho、Nal Kalchbrenner、Dirk Weissenborn 和 Tim Salimans。多维变换器中的轴向注意力。*CoRR*，abs/1912.12180，2019。网址
    [http://arxiv.org/abs/1912.12180](http://arxiv.org/abs/1912.12180)。
- en: 'Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *Proceedings of the 37th International Conference on
    Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event*, volume 119 of *Proceedings
    of Machine Learning Research*, pp.  5156–5165\. PMLR, 2020. URL [http://proceedings.mlr.press/v119/katharopoulos20a.html](http://proceedings.mlr.press/v119/katharopoulos20a.html).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katharopoulos 等（2020）Angelos Katharopoulos、Apoorv Vyas、Nikolaos Pappas 和 François
    Fleuret。变换器即RNN：具有线性注意力的快速自回归变换器。在 *第37届国际机器学习会议，ICML 2020，2020年7月13-18日，虚拟会议*，*机器学习研究论文集*第119卷，页码
    5156–5165。PMLR，2020年。网址 [http://proceedings.mlr.press/v119/katharopoulos20a.html](http://proceedings.mlr.press/v119/katharopoulos20a.html)。
- en: 'Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. In *8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020. URL
    [https://openreview.net/forum?id=rkgNKkHtvB](https://openreview.net/forum?id=rkgNKkHtvB).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kitaev et al. (2020) 尼基塔·基塔耶夫、卢卡斯·凯泽和安塞姆·列夫斯卡亚。Reformer: 高效的变压器。在 *第八届国际学习表示大会
    (ICLR 2020)，埃蒂俄比亚亚的斯亚贝巴，2020年4月26-30日*。OpenReview.net，2020。网址 [https://openreview.net/forum?id=rkgNKkHtvB](https://openreview.net/forum?id=rkgNKkHtvB)。'
- en: 'Li et al. (2024) Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr
    Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: LLM
    knows what you are looking for before generation. *CoRR*, abs/2404.14469, 2024.
    doi: 10.48550/ARXIV.2404.14469. URL [https://doi.org/10.48550/arXiv.2404.14469](https://doi.org/10.48550/arXiv.2404.14469).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024) 余宏李、英冰黄、博文杨、巴拉特·文基特什、阿希尔·洛卡泰利、汉臣·叶、天乐·蔡、帕特里克·刘易斯和德明·陈。Snapkv:
    LLM 在生成之前知道你在寻找什么。*CoRR*，abs/2404.14469，2024。doi: 10.48550/ARXIV.2404.14469。网址
    [https://doi.org/10.48550/arXiv.2404.14469](https://doi.org/10.48550/arXiv.2404.14469)。'
- en: 'Liu et al. (2023a) Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention
    with blockwise transformers for near-infinite context. *CoRR*, abs/2310.01889,
    2023a. doi: 10.48550/ARXIV.2310.01889. URL [https://doi.org/10.48550/arXiv.2310.01889](https://doi.org/10.48550/arXiv.2310.01889).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023a) 郝刘、马泰·扎哈里亚和彼得·阿贝尔。具有块状变压器的环形注意力用于近乎无限的上下文。*CoRR*，abs/2310.01889，2023a。doi:
    10.48550/ARXIV.2310.01889。网址 [https://doi.org/10.48550/arXiv.2310.01889](https://doi.org/10.48550/arXiv.2310.01889)。'
- en: 'Liu et al. (2023b) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands:
    Exploiting the persistence of importance hypothesis for LLM KV cache compression
    at test time. *CoRR*, abs/2305.17118, 2023b. doi: 10.48550/arXiv.2305.17118. URL
    [https://doi.org/10.48550/arXiv.2305.17118](https://doi.org/10.48550/arXiv.2305.17118).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023b) 资昌·刘、阿迪提亚·德赛、方硕·廖、韦涛·王、维克托·谢、赵卓·徐、阿纳斯塔西奥斯·基里迪斯和安舒玛利·施里瓦斯塔瓦。Scissorhands:
    利用重要性持久性假设进行 LLM KV 缓存压缩。*CoRR*，abs/2305.17118，2023b。doi: 10.48550/arXiv.2305.17118。网址
    [https://doi.org/10.48550/arXiv.2305.17118](https://doi.org/10.48550/arXiv.2305.17118)。'
- en: 'Meta (2024) AI Meta. Introducing meta llama 3: The most capable openly available
    llm to date. *Meta AI*, 2024.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Meta (2024) AI Meta。介绍 Meta Llama 3: 迄今为止最强大的公开 LLM。*Meta AI*，2024。'
- en: Michel et al. (2019) Paul Michel, Omer Levy, and Graham Neubig. Are sixteen
    heads really better than one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
    E. Fox, and R. Garnett (eds.), *Advances in Neural Information Processing Systems*,
    volume 32\. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michel et al. (2019) 保罗·米歇尔、奥梅尔·莱维和格雷厄姆·纽比格。十六个头真的比一个头更好吗？在 H. Wallach、H. Larochelle、A.
    Beygelzimer、F. d'Alché-Buc、E. Fox 和 R. Garnett (编)，*神经信息处理系统进展*，第 32 卷。Curran
    Associates, Inc.，2019。网址 [https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf)。
- en: 'Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
    Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna
    Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,
    Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
    Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context
    learning and induction heads. *CoRR*, abs/2209.11895, 2022. doi: 10.48550/ARXIV.2209.11895.
    URL [https://doi.org/10.48550/arXiv.2209.11895](https://doi.org/10.48550/arXiv.2209.11895).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Olsson et al. (2022) 凯瑟琳·奥尔森、尼尔森·埃尔哈奇、尼尔·南达、尼古拉斯·约瑟夫、诺瓦·达萨尔玛、汤姆·赫宁汉、本·曼、阿曼达·阿斯凯尔、云涛·白、安娜·陈、汤姆·科纳利、道恩·德雷恩、迪普·甘古利、扎克·哈特菲尔德-多兹、丹尼·埃尔南德斯、斯科特·约翰斯顿、安迪·琼斯、杰克逊·科尔尼昂、莉安·洛维特、卡马尔·恩杜斯、达里奥·阿莫德伊、汤姆·布朗、杰克·克拉克、贾雷德·卡普兰、萨姆·麦坎利什和克里斯·奥拉。上下文学习和归纳头。*CoRR*，abs/2209.11895，2022。doi:
    10.48550/ARXIV.2209.11895。网址 [https://doi.org/10.48550/arXiv.2209.11895](https://doi.org/10.48550/arXiv.2209.11895)。'
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。GPT-4 技术报告，2023。
- en: 'Penedo et al. (2024) Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton
    Lozhkov, Margaret Mitchell, Colin Raffel, Leandro von Werra, and Thomas Wolf.
    The fineweb datasets: Decanting the web for the finest text data at scale. *CoRR*,
    abs/2406.17557, 2024. doi: 10.48550/ARXIV.2406.17557. URL [https://doi.org/10.48550/arXiv.2406.17557](https://doi.org/10.48550/arXiv.2406.17557).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Penedo 等人（2024）**Guilherme Penedo**、**Hynek Kydlícek**、**Loubna Ben Allal**、**Anton
    Lozhkov**、**Margaret Mitchell**、**Colin Raffel**、**Leandro von Werra** 和 **Thomas
    Wolf**。精细网页数据集：从网络中提炼出最优质的文本数据。*CoRR*，abs/2406.17557，2024。doi: 10.48550/ARXIV.2406.17557。URL
    [https://doi.org/10.48550/arXiv.2406.17557](https://doi.org/10.48550/arXiv.2406.17557)。'
- en: 'Qiu et al. (2020) Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang,
    and Jie Tang. Blockwise self-attention for long document understanding. In Trevor
    Cohn, Yulan He, and Yang Liu (eds.), *Findings of the Association for Computational
    Linguistics: EMNLP 2020, Online Event, 16-20 November 2020*, volume EMNLP 2020
    of *Findings of ACL*, pp.  2555–2565\. Association for Computational Linguistics,
    2020. doi: 10.18653/V1/2020.FINDINGS-EMNLP.232. URL [https://doi.org/10.18653/v1/2020.findings-emnlp.232](https://doi.org/10.18653/v1/2020.findings-emnlp.232).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu 等人（2020）**Jiezhong Qiu**、**Hao Ma**、**Omer Levy**、**Wen-tau Yih**、**Sinong
    Wang** 和 **Jie Tang**。块级自注意力用于长文档理解。在 **Trevor Cohn**、**Yulan He** 和 **Yang Liu**（编），*计算语言学协会会议论文集：EMNLP
    2020，在线活动，2020年11月16-20日*，*ACL会议论文集 EMNLP 2020* 第2555-2565页。计算语言学协会，2020。doi:
    10.18653/V1/2020.FINDINGS-EMNLP.232。URL [https://doi.org/10.18653/v1/2020.findings-emnlp.232](https://doi.org/10.18653/v1/2020.findings-emnlp.232)。'
- en: 'Rucinski (2024) Szymon Rucinski. Efficient language adaptive pre-training:
    Extending state-of-the-art large language models for polish. *CoRR*, abs/2402.09759,
    2024. doi: 10.48550/ARXIV.2402.09759. URL [https://doi.org/10.48550/arXiv.2402.09759](https://doi.org/10.48550/arXiv.2402.09759).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rucinski（2024）**Szymon Rucinski**。高效的语言适应预训练：扩展最新的大型语言模型以支持波兰语。*CoRR*，abs/2402.09759，2024。doi:
    10.48550/ARXIV.2402.09759。URL [https://doi.org/10.48550/arXiv.2402.09759](https://doi.org/10.48550/arXiv.2402.09759)。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023）**Hugo Touvron**、**Louis Martin**、**Kevin Stone**、**Peter Albert**、**Amjad
    Almahairi**、**Yasmine Babaei**、**Nikolay Bashlykov**、**Soumya Batra**、**Prajjwal
    Bhargava**、**Shruti Bhosale**、**Dan Bikel**、**Lukas Blecher**、**Cristian Canton
    Ferrer**、**Moya Chen**、**Guillem Cucurull**、**David Esiobu**、**Jude Fernandes**、**Jeremy
    Fu**、**Wenyin Fu**、**Brian Fuller**、**Cynthia Gao**、**Vedanuj Goswami**、**Naman
    Goyal**、**Anthony Hartshorn**、**Saghar Hosseini**、**Rui Hou**、**Hakan Inan**、**Marcin
    Kardas**、**Viktor Kerkez**、**Madian Khabsa**、**Isabel Kloumann**、**Artem Korenev**、**Punit
    Singh Koura**、**Marie-Anne Lachaux**、**Thibaut Lavril**、**Jenya Lee**、**Diana
    Liskovich**、**Yinghai Lu**、**Yuning Mao**、**Xavier Martinet**、**Todor Mihaylov**、**Pushkar
    Mishra**、**Igor Molybog**、**Yixin Nie**、**Andrew Poulton**、**Jeremy Reizenstein**、**Rashi
    Rungta**、**Kalyan Saladi**、**Alan Schelten**、**Ruan Silva**、**Eric Michael Smith**、**Ranjan
    Subramanian**、**Xiaoqing Ellen Tan**、**Binh Tang**、**Ross Taylor**、**Adina Williams**、**Jian
    Xiang Kuan**、**Puxin Xu**、**Zheng Yan**、**Iliyan Zarov**、**Yuchen Zhang**、**Angela
    Fan**、**Melanie Kambadur**、**Sharan Narang**、**Aurelien Rodriguez**、**Robert Stojnic**、**Sergey
    Edunov** 和 **Thomas Scialom**。Llama 2：开放基础和微调聊天模型，2023。
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned, July 2019. URL [https://aclanthology.org/P19-1580](https://aclanthology.org/P19-1580).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita 等人（2019）**Elena Voita**、**David Talbot**、**Fedor Moiseev**、**Rico Sennrich**
    和 **Ivan Titov**。分析多头自注意力：专业化头部进行重负担，其余部分可以剪枝，2019年7月。URL [https://aclanthology.org/P19-1580](https://aclanthology.org/P19-1580)。
- en: 'Wu et al. (2024) Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao
    Fu. Retrieval head mechanistically explains long-context factuality. *CoRR*, abs/2404.15574,
    2024. doi: 10.48550/ARXIV.2404.15574. URL [https://doi.org/10.48550/arXiv.2404.15574](https://doi.org/10.48550/arXiv.2404.15574).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人（2024）**Wenhao Wu**、**Yizhong Wang**、**Guangxuan Xiao**、**Hao Peng** 和
    **Yao Fu**。检索头从机制上解释了长上下文的真实性。*CoRR*，abs/2404.15574，2024。doi: 10.48550/ARXIV.2404.15574。URL
    [https://doi.org/10.48550/arXiv.2404.15574](https://doi.org/10.48550/arXiv.2404.15574)。'
- en: 'Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. Efficient streaming language models with attention sinks. *CoRR*,
    abs/2309.17453, 2023. doi: 10.48550/ARXIV.2309.17453. URL [https://doi.org/10.48550/arXiv.2309.17453](https://doi.org/10.48550/arXiv.2309.17453).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等（2023）Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike Lewis.
    具有注意力池的高效流式语言模型。*CoRR*，abs/2309.17453，2023年。doi: 10.48550/ARXIV.2309.17453。URL
    [https://doi.org/10.48550/arXiv.2309.17453](https://doi.org/10.48550/arXiv.2309.17453)。'
- en: 'Yang et al. (2022) Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor,
    David Farhi, Jakub Pachocki, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. Tensor
    programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
    In *NeurIPS 2021*, March 2022. URL [https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/](https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等（2022）Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi,
    Jakub Pachocki, Xiaodong Liu, Weizhu Chen, 和 Jianfeng Gao. Tensor programs v:
    通过零-shot 超参数转移调优大规模神经网络。在*NeurIPS 2021*，2022年3月。URL [https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/](https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/)。'
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
    *Advances in Neural Information Processing Systems 33: Annual Conference on Neural
    Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*,
    2020. URL [https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等（2020）Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie,
    Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
    和 Amr Ahmed. Big bird: 用于更长序列的 Transformers。在 Hugo Larochelle, Marc’Aurelio Ranzato,
    Raia Hadsell, Maria-Florina Balcan, 和 Hsuan-Tien Lin（编），*神经信息处理系统进展 33: 2020年神经信息处理系统年会，NeurIPS
    2020，2020年12月6-12日，虚拟会议*，2020年。URL [https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html)。'
- en: 'Zhang et al. (2023) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett,
    Zhangyang Wang, and Beidi Chen. H${}_{\mbox{2}}$o: Heavy-hitter oracle for efficient
    generative inference of large language models. *CoRR*, abs/2306.14048, 2023. doi:
    10.48550/arXiv.2306.14048. URL [https://doi.org/10.48550/arXiv.2306.14048](https://doi.org/10.48550/arXiv.2306.14048).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2023）Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett,
    Zhangyang Wang, 和 Beidi Chen. H${}_{\mbox{2}}$o: 高效生成推理的大型语言模型的重击预言机。*CoRR*，abs/2306.14048，2023年。doi:
    10.48550/arXiv.2306.14048。URL [https://doi.org/10.48550/arXiv.2306.14048](https://doi.org/10.48550/arXiv.2306.14048)。'
- en: Appendix A Appendix
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: You may include other additional sections here.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处添加其他附加部分。
