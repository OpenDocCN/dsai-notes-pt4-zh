- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:53:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'DeFT: 带有 IO 感知的闪存树注意力算法，用于高效树搜索基础的 LLM 推理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00242](https://ar5iv.labs.arxiv.org/html/2404.00242)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00242](https://ar5iv.labs.arxiv.org/html/2404.00242)
- en: Jinwei Yao^(1,4,)  Kaiqi Chen^(2,∗)  Kexun Zhang^(3,∗)  Jiaxuan You ⁴  Binhang
    Yuan⁵  Zeke Wang^(2,†)  Tao Lin^(1,)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jinwei Yao^(1,4,)  Kaiqi Chen^(2,∗)  Kexun Zhang^(3,∗)  Jiaxuan You ⁴  Binhang
    Yuan⁵  Zeke Wang^(2,†)  Tao Lin^(1,)
- en: jinwei.yao1114@gmail.com;  {chiaki_cage,wangzeke}@zju.edu.cn;
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: jinwei.yao1114@gmail.com;  {chiaki_cage,wangzeke}@zju.edu.cn;
- en: kexunz@andrew.cmu.edu;  jiaxuan@illinois.edu;
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: kexunz@andrew.cmu.edu;  jiaxuan@illinois.edu;
- en: biyuan@ust.hk;  lintao@westlake.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: biyuan@ust.hk;  lintao@westlake.edu.cn
- en: ¹Westlake University  ²Zhejiang University  ³Carnegie Mellon University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹西湖大学  ²浙江大学  ³卡内基梅隆大学
- en: ⁴University of Illinois Urbana-Champaign  ⁵Hong Kong University of Science and
    Technology Equal contribution. Work was done during Jinwei’s visit to Westlake
    University.Corresponding author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴伊利诺伊大学厄本那-香槟分校  ⁵香港科技大学 平等贡献。此工作在 Jinwei 访问西湖大学期间完成。通讯作者。
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Decoding using tree search can greatly enhance the inference quality for transformer-based
    Large Language Models (LLMs). Depending on the guidance signal, it searches for
    the best path from root to leaf in the tree by forming LLM outputs to improve
    controllability, reasoning ability, alignment, et cetera. However, current tree
    decoding strategies and their inference systems do not suit each other well due
    to redundancy in computation, memory footprints, and memory access, resulting
    in inefficient inference. To address this issue, we propose DeFT, an IO-aware
    tree attention algorithm that maintains memory-efficient attention calculation
    with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided
    Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction
    of memory reads/writes for the KV cache between GPU global memory and on-chip
    shared memory as much as possible; (2) Attention Calculation: we calculate partial
    attention of each QKV groups in a fused kernel then apply a Tree-topology-aware
    Global Reduction strategy to get final attention. Thanks to a reduction in KV
    cache IO by 3.6-4.5$\times$, along with an additional reduction in IO for $\mathbf{Q}\mathbf{K}^{\top}$
    and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup
    of 1.7-2.4$\times$ in end-to-end latency across two practical reasoning tasks
    over the SOTA attention algorithms.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用树搜索进行解码可以显著提升基于变换器的语言模型（LLMs）的推理质量。根据指导信号，它通过形成 LLM 输出，搜索从根到叶的最佳路径，以提高可控性、推理能力、对齐等。然而，当前的树解码策略及其推理系统由于计算冗余、内存占用和内存访问的不匹配，导致推理效率低下。为了解决这一问题，我们提出了
    DeFT，一种 IO 感知的树注意力算法，它在两个阶段内保持内存高效的注意力计算： (1) QKV 准备：我们提出了一种 KV 引导的树分割策略，以明智地分组
    QKV，尽可能高效利用 GPU，并减少 GPU 全局内存与片上共享内存之间的 KV 缓存的内存读写； (2) 注意力计算：我们在一个融合的内核中计算每个 QKV
    组的部分注意力，然后应用树拓扑感知的全局归约策略以获取最终的注意力。得益于 KV 缓存 IO 减少 3.6-4.5$\times$，以及 $\mathbf{Q}\mathbf{K}^{\top}$
    和 Softmax 额外减少了相当于总 KV 缓存 IO 的 25%，DeFT 在两个实际推理任务中相比最先进的注意力算法实现了 1.7-2.4$\times$
    的端到端延迟加速。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) (Achiam et al., [2023](#bib.bib1); Touvron et al.,
    [2023a](#bib.bib26); [b](#bib.bib27)) are extensively utilized across a range
    of tasks like chatbot (Roller et al., [2020](#bib.bib23)), code generation (Mark
    et al., [2021](#bib.bib19)), reasoning (Yao et al., [2023](#bib.bib33); Besta
    et al., [2023](#bib.bib3); Ning et al., [2023](#bib.bib21)), etc. Tree search
    algorithms (Graves, [2012](#bib.bib8); Lu et al., [2022](#bib.bib18); Liu et al.,
    [2023](#bib.bib15)) are frequently integrated with LLMs to meet Service-Level-Objectives
    (SLOs) (Yao et al., [2023](#bib.bib33); Liu et al., [2023](#bib.bib15); Anderson
    et al., [2017](#bib.bib2); Post & Vilar, [2018](#bib.bib22); Hokamp & Liu, [2017](#bib.bib9)).
    In order to cover a large search space, numerous tokens will be generated with
    significant computational and memory overhead, resulting in greater latency during
    inference.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型 (LLMs) (Achiam et al., [2023](#bib.bib1); Touvron et al., [2023a](#bib.bib26);
    [b](#bib.bib27)) 被广泛应用于聊天机器人 (Roller et al., [2020](#bib.bib23))、代码生成 (Mark et
    al., [2021](#bib.bib19))、推理 (Yao et al., [2023](#bib.bib33); Besta et al., [2023](#bib.bib3);
    Ning et al., [2023](#bib.bib21)) 等任务。树搜索算法 (Graves, [2012](#bib.bib8); Lu et al.,
    [2022](#bib.bib18); Liu et al., [2023](#bib.bib15)) 经常与 LLMs 集成，以满足服务水平目标 (SLOs)
    (Yao et al., [2023](#bib.bib33); Liu et al., [2023](#bib.bib15); Anderson et al.,
    [2017](#bib.bib2); Post & Vilar, [2018](#bib.bib22); Hokamp & Liu, [2017](#bib.bib9))。为了覆盖较大的搜索空间，将生成大量令牌，导致显著的计算和内存开销，从而在推理过程中产生更大的延迟。
- en: 'Sequence-based decoding and tree-based decoding represent two prominent approaches
    in handling LLM inference (Yao et al., [2023](#bib.bib33)). Sequence-based decoding
    samples a single sequence of tokens every time, while tree-based decoding maintains
    multiple sequences with common prefixes as a tree structure, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeFT: Flash Tree-attention with IO-Awareness for
    Efficient Tree-search-based LLM Inference"). Since nodes in the forms of the tree
    can be shared computationally and in memory while that of the sequence cannot,
    applying tree-search-based tasks directly to sequence-based decoding causes three
    levels of redundancy: (1) *memory storage*, especially the KV cache (Kwon et al.,
    [2023](#bib.bib14); Zheng et al., [2023](#bib.bib35)); (2) *computation*, especially
    the computation for common prompts among sequences in a batch (Zheng et al., [2023](#bib.bib35));
    (3) *memory access*.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '基于序列的解码和基于树的解码代表了处理 LLM 推理的两种主要方法 (Yao et al., [2023](#bib.bib33))。基于序列的解码每次采样一个令牌序列，而基于树的解码则将具有公共前缀的多个序列维护为树结构，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeFT: Flash Tree-attention with IO-Awareness for
    Efficient Tree-search-based LLM Inference")所示。由于树的节点可以在计算和内存中共享，而序列则不能，直接将树搜索任务应用于基于序列的解码会导致三种冗余：
    (1) *内存存储*，尤其是 KV 缓存 (Kwon et al., [2023](#bib.bib14); Zheng et al., [2023](#bib.bib35));
    (2) *计算*，尤其是批量中序列之间的公共提示的计算 (Zheng et al., [2023](#bib.bib35)); (3) *内存访问*。'
- en: 'Existing work of tree-based decoding focuses on the first two levels while
    largely ignoring the third yet the most important one–*memory access*, given the
    nature of memory-bounded LLM inference (Shazeer, [2019](#bib.bib24); Cai et al.,
    [2024](#bib.bib4); Kim et al., [2023](#bib.bib13)). As for sequence-based decoding
    methods optimize the memory access for the aspects of partial results (i.e., $\mathbf{Q}\mathbf{K}^{\top}$)
    during attention calculations (Dao et al., [2022](#bib.bib5); [2023](#bib.bib6);
    Hong et al., [2023](#bib.bib11)). However, their effectiveness in tree-based decoding
    is limited. In particular, these optimizations are unable to address the potential
    bottleneck posed by the KV cache IO when dealing with a large number of tokens,
    as illustrated in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的基于树的解码工作主要集中在前两级，而忽视了第三个也是最重要的——*内存访问*，考虑到内存受限的 LLM 推理的性质 (Shazeer, [2019](#bib.bib24);
    Cai et al., [2024](#bib.bib4); Kim et al., [2023](#bib.bib13))。至于基于序列的解码方法，它们优化了在注意力计算期间部分结果的内存访问（即，$\mathbf{Q}\mathbf{K}^{\top}$）
    (Dao et al., [2022](#bib.bib5); [2023](#bib.bib6); Hong et al., [2023](#bib.bib11))。然而，这些优化在基于树的解码中效果有限。特别是，当处理大量令牌时，这些优化无法解决
    KV 缓存 IO 可能带来的瓶颈，如表[1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference")所示。'
- en: '![Refer to caption](img/b950ee8c74fc244dc1a32617cc044171.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/b950ee8c74fc244dc1a32617cc044171.png)'
- en: 'Figure 1: Comparison of Sequence-based decoding and Tree-based decoding. An
    illustration of Sequence-based decoding and Tree-based decoding with the example
    of Chain-of-thoughts (CoT) (Wei et al., [2022](#bib.bib29)) and Tree-of-thoughts
    (ToT) (Yao et al., [2023](#bib.bib33)) in Besta et al. ([2023](#bib.bib3)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 基于序列的解码与基于树的解码的比较。基于序列的解码和基于树的解码的示例，以思维链 (CoT)（Wei 等，[2022](#bib.bib29)）和思维树
    (ToT)（Yao 等，[2023](#bib.bib33)）为例，来自 Besta 等人 ([2023](#bib.bib3))。'
- en: As a remedy, in this paper, we resort to the key attention component during
    the decoding process. Orthogonal to the traditional attention mechanisms in sequence-based
    decoding, tree attention (Miao et al., [2023](#bib.bib20); Cai et al., [2024](#bib.bib4))—specifically
    designed to handle hierarchical or tree-structured tokens in tasks such as parallel
    decoding—can reduce the kernel launching, computation and KV cache storage overheads
    for attention calculations. However, this line of research does not further leverage
    the tree topology to reduce IO when calculating attention, and thus still not
    fully IO-aware for both (i) partial result (i.e., $\mathbf{Q}\mathbf{K}^{\top}$) (Cai
    et al., [2024](#bib.bib4)) due to the lack of tiling and kernel fusion (Dao et al.,
    [2022](#bib.bib5)); and (ii) KV cache in a tree structure (Miao et al., [2023](#bib.bib20)).
    These limitations hinder their effectiveness in optimizing memory access during
    tree-based decoding.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为补救措施，本文在解码过程中借助了关键的注意力组件。与传统的基于序列的解码机制不同，树注意力（Miao 等，[2023](#bib.bib20)；Cai
    等，[2024](#bib.bib4)）——专门设计用于处理层次或树结构的令牌，例如并行解码——可以减少注意力计算的内核启动、计算和 KV 缓存存储开销。然而，这一研究方向并未进一步利用树的拓扑结构来减少计算注意力时的
    IO，因此仍未完全具备 IO 感知能力，（i）部分结果（即 $\mathbf{Q}\mathbf{K}^{\top}$）（Cai 等，[2024](#bib.bib4)）由于缺乏分块和内核融合（Dao
    等，[2022](#bib.bib5)）；（ii）树结构中的 KV 缓存（Miao 等，[2023](#bib.bib20)）。这些限制阻碍了其在优化树结构解码期间的内存访问效率。
- en: 'Table 1: Comparison of efficiency in CoT and ToT. The task is document merging
    from Besta et al. ([2023](#bib.bib3)). CoT is implemented with Sequence-based
    decoding while ToT is with Tree-based decoding. The total generated tokens of
    CoT is only 525 while 24,026 in ToT, resulting in inefficiency in end-to-end latency
    (second) and IO (TB). IO mainly consists of three parts as follows. (i) KV cache:
    IO-KV; (ii) $QK^{T}$: IO-$QK^{T}$; (iii) Softmax$(QK^{T})$: IO-$Softmax$. Baselines:
    (i) Flash-Decoding: attention in Flash-Decoding (Dao et al., [2023](#bib.bib6));
    (ii) Tree Attention: tree attention in Medusa (Cai et al., [2024](#bib.bib4)).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: CoT 和 ToT 的效率比较。任务是文档合并，来自 Besta 等人 ([2023](#bib.bib3))。CoT 实现为基于序列的解码，而
    ToT 实现为基于树的解码。CoT 的总生成令牌仅为 525，而 ToT 为 24,026，导致在端到端延迟（秒）和 IO（TB）方面效率低下。IO 主要由以下三部分组成：（i）KV
    缓存：IO-KV；（ii）$QK^{T}$：IO-$QK^{T}$；（iii）Softmax$(QK^{T})$：IO-$Softmax$。基线：（i）Flash-Decoding：Flash-Decoding
    中的注意力（Dao 等，[2023](#bib.bib6)）；（ii）Tree Attention：Medusa 中的树注意力（Cai 等，[2024](#bib.bib4)）。'
- en: '|  | Metrics |  |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | 指标 |  |  |'
- en: '|  | Latency | IO-KV | IO-$QK^{T}$ | IO-$Softmax$ |  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | 延迟 | IO-KV | IO-$QK^{T}$ | IO-$Softmax$ |  |'
- en: '| Flash-Decoding + CoT | 21 | 0.6 | 0 | 0 |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Flash-Decoding + CoT | 21 | 0.6 | 0 | 0 |  |'
- en: '| Flash-Decoding + ToT | 450 | 30.7 | 0 | 0 |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Flash-Decoding + ToT | 450 | 30.7 | 0 | 0 |  |'
- en: '| Tree Attention + ToT | 660 | 8.6 | 0.7 | 1.3 |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Tree Attention + ToT | 660 | 8.6 | 0.7 | 1.3 |  |'
- en: '| DeFT(ours) + ToT | 272 | 8.6 | 0 | 0 |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| DeFT(我们的方法) + ToT | 272 | 8.6 | 0 | 0 |  |'
- en: '| Speed up over best baseline | $1.66\times$ | - | - | - |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 比最佳基线加速 | $1.66\times$ | - | - | - |  |'
- en: To bridge the above gap, we propose DeFT, an IO-aware tree attention algorithm
    with two key insights. First, the IO workload for queries (Q) is negligible compared
    to that of KV cache, primarily because the maximum query length typically corresponds
    to root-to-leaf paths in the tree, resulting in relatively short queries (e.g. dozens
    of tokens) compared with KV cache length in each node (e.g. hundreds/thousands
    of tokens). Second, in sequence-based decoding, each KV cache entry corresponds
    to a unique query, whereas in tree-based decoding, multiple queries can share
    their common ancestor’s KV cache during attention calculation, benefiting not
    only in terms of KV cache storage but also in reducing IOs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥合上述差距，我们提出了 DeFT，一种具备 IO 感知的树注意力算法，具有两个关键见解。首先，与 KV 缓存相比，查询（Q）的 IO 工作负载可以忽略不计，主要是因为最大查询长度通常对应于树中的根到叶路径，导致相对于每个节点的
    KV 缓存长度（例如数百/数千个令牌），查询较短（例如几十个令牌）。其次，在基于序列的解码中，每个 KV 缓存条目对应于一个唯一查询，而在基于树的解码中，多个查询可以在计算注意力时共享其共同祖先的
    KV 缓存，这不仅有利于 KV 缓存存储，还能减少 IO。
- en: 'Building upon these two insights, in the first phase of DeFT—QKV Preparation,
    inspired by tiling technique in Dao et al. ([2022](#bib.bib5); [2023](#bib.bib6)),
    we split the decoding tree by nodes as each node has sequence-granularity of tokens
    and KV cache. Then we group the KV cache of each node with all queries that share
    it in the decoding tree, to minimize the IO of KV cache with negligible IO overhead
    of queries. In the second phase of DeFT—Attention Calculation, we adopt a fused
    kernel to get partial attention with LogSumExp of QKV groups calculated in phase
    1, and conduct the tree-topology-aware global reduction inspired by Flash-Decoding (Dao
    et al., [2023](#bib.bib6)). We summarize our contributions as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这两点见解，在DeFT的第一阶段—QKV准备中，受Dao等（[2022](#bib.bib5)；[2023](#bib.bib6)）的平铺技术启发，我们按节点拆分解码树，因为每个节点具有序列粒度的标记和KV缓存。然后，我们将每个节点的KV缓存与解码树中共享该缓存的所有查询进行分组，以最小化KV缓存的IO，同时查询的IO开销可以忽略不计。在DeFT的第二阶段—注意力计算中，我们采用了融合内核，通过阶段1中计算的QKV组的LogSumExp获取部分注意力，并进行受Flash-Decoding（Dao等，[2023](#bib.bib6)）启发的树拓扑感知全局归约。我们总结我们的贡献如下：
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a simple but hardware-efficient tree attention algorithm–DeFT, which
    is IO-aware for both KV cache in a tree structure and partial results (i.e., $\mathbf{Q}\mathbf{K}^{\top}$
    and Softmax).
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种简单但硬件高效的树注意力算法—DeFT，它对树结构中的KV缓存和部分结果（即，$\mathbf{Q}\mathbf{K}^{\top}$和Softmax）具有IO感知。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We implement DeFT on OpenAI Triton (Tillet et al., [2019](#bib.bib25)) to gain
    precise management over memory access and fuse all attention operations into a
    single GPU kernel.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在OpenAI Triton（Tillet等，[2019](#bib.bib25)）上实现了DeFT，以精确管理内存访问，并将所有注意力操作融合到单个GPU内核中。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We theoretically justify the superiority of DeFT over the existing attention
    algorithms (Wolf et al., [2019](#bib.bib31); Dao et al., [2023](#bib.bib6); Cai
    et al., [2024](#bib.bib4); Miao et al., [2023](#bib.bib20)) in terms of IO complexity.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从理论上证明了DeFT在IO复杂性方面优于现有注意力算法（Wolf等，[2019](#bib.bib31)；Dao等，[2023](#bib.bib6)；Cai等，[2024](#bib.bib4)；Miao等，[2023](#bib.bib20)）的优势。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We empirically verify its effectiveness on practical reasoning tasks. DeFT can
    achieve a speedup of 1.7-2.4 times across two practical reasoning tasks compared
    with the SOTA attention algorithms.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在实际推理任务中实证验证了其有效性。与SOTA注意力算法相比，DeFT可以在两个实际推理任务中实现1.7-2.4倍的加速。
- en: 2 Related Work
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Tree-based Decoding.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于树的解码。
- en: Tree-based decoding, exemplified by beam search (Graves, [2012](#bib.bib8)),
    has been pivotal in NLP, handling lexical and logical constraints (Anderson et al.,
    [2017](#bib.bib2); Post & Vilar, [2018](#bib.bib22); Hokamp & Liu, [2017](#bib.bib9)),
    mitigating gender bias (Lu et al., [2021](#bib.bib17)), achieving communicative
    goals (Holtzman et al., [2018](#bib.bib10)), and improving alignment (Liu et al.,
    [2023](#bib.bib15)). Recent strategies enhance LLM reasoning (Yao et al., [2023](#bib.bib33);
    Besta et al., [2023](#bib.bib3); Ning et al., [2023](#bib.bib21)), using search
    trees with parallel hypothesis generation and selection based on scoring functions.
    Some score candidates per token (Dathathri et al., [2019](#bib.bib7); Lu et al.,
    [2021](#bib.bib17); [2022](#bib.bib18)), others per reasoning step (Welleck et al.,
    [2022](#bib.bib30); Uesato et al., [2022](#bib.bib28); Xie et al., [2023](#bib.bib32)).
    Efficiency in tree decoding remains underexplored despite various search algorithms’
    application, such as A* (Lu et al., [2022](#bib.bib18)) and Monte-Carlo Tree Search
    (Liu et al., [2023](#bib.bib15)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以束搜索（Graves，[2012](#bib.bib8)）为例的基于树的解码在自然语言处理（NLP）中发挥了关键作用，处理了词汇和逻辑约束（Anderson等，[2017](#bib.bib2)；Post
    & Vilar，[2018](#bib.bib22)；Hokamp & Liu，[2017](#bib.bib9)），缓解了性别偏见（Lu等，[2021](#bib.bib17)），实现了交流目标（Holtzman等，[2018](#bib.bib10)），并改善了对齐（Liu等，[2023](#bib.bib15)）。最近的策略通过使用带有并行假设生成和基于评分函数的选择的搜索树来增强LLM推理（Yao等，[2023](#bib.bib33)；Besta等，[2023](#bib.bib3)；Ning等，[2023](#bib.bib21)）。一些按标记的分数候选（Dathathri等，[2019](#bib.bib7)；Lu等，[2021](#bib.bib17)；[2022](#bib.bib18)），另一些按推理步骤（Welleck等，[2022](#bib.bib30)；Uesato等，[2022](#bib.bib28)；Xie等，[2023](#bib.bib32)）。尽管各种搜索算法如A*（Lu等，[2022](#bib.bib18)）和蒙特卡洛树搜索（Liu等，[2023](#bib.bib15)）已被应用，但树解码的效率仍然没有得到充分探讨。
- en: Memory-efficient Attention Algorithms.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存高效的注意力算法。
- en: Existing memory-efficient attention algorithms target sequence-based decoding.
    FlashAttention (Dao et al., [2022](#bib.bib5)) improves self-attention computation
    in LLM training via tiling and kernel fusion, reducing IOs. Flash-Decoding (Dao
    et al., [2023](#bib.bib6)) extends this, enhancing parallelism by dividing K and
    V and introducing global reduction to gather partial attention results, enabling
    efficient decoding for long sequences. Unluckily, applying these memory-efficient
    algorithms to the tree-based decoding overlooks redundancy in IO of tree-structured
    KV cache, which is the focus of DeFT.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的内存高效注意力算法针对基于序列的解码。FlashAttention (Dao et al., [2022](#bib.bib5)) 通过分块和内核融合改进了LLM训练中的自注意力计算，减少了IO操作。Flash-Decoding
    (Dao et al., [2023](#bib.bib6)) 扩展了这一方法，通过划分K和V并引入全局归约来收集部分注意力结果，从而增强了并行性，实现了对长序列的高效解码。不幸的是，将这些内存高效算法应用于基于树的解码时忽略了树结构KV缓存中的IO冗余，而这正是DeFT的重点。
- en: Tree Attention.
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 树注意力。
- en: Integrated into LLM inference, tree attention reduces computation, storage,
    and kernel launching overheads (Miao et al., [2023](#bib.bib20)). Tree-structured
    token candidates undergo parallel decoding, with SpecInfer (Miao et al., [2023](#bib.bib20))
    introducing a topology-aware causal masked tree attention algorithm, dynamically
    updating a causal mask to capture relationships among tokens. Medusa (Cai et al.,
    [2024](#bib.bib4)) uses a similar mechanism with a static causal mask, while other
    works (Zhao et al., [2023](#bib.bib34); Liu et al., [2024](#bib.bib16)) adopt
    analogous approaches to enhance attention calculation efficiency. However, unlike
    DeFT, these existing works utilizing tree attention do not take memory access
    into consideration.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 集成到LLM推理中，树注意力减少了计算、存储和内核启动开销 (Miao et al., [2023](#bib.bib20))。树结构的标记候选项经历并行解码，其中SpecInfer (Miao
    et al., [2023](#bib.bib20)) 引入了一种拓扑感知的因果掩蔽树注意力算法，动态更新因果掩蔽以捕捉标记之间的关系。Medusa (Cai
    et al., [2024](#bib.bib4)) 使用类似机制，但具有静态因果掩蔽，而其他工作 (Zhao et al., [2023](#bib.bib34);
    Liu et al., [2024](#bib.bib16)) 采用类似的方法来提高注意力计算效率。然而，与DeFT不同，这些现有利用树注意力的工作没有考虑内存访问。
- en: Discussion on Tree-based Decoding.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于树的解码讨论。
- en: 'To improve inference efficiency by generating multiple tokens in each iteration,
    tree-based decoding (Miao et al., [2023](#bib.bib20); Cai et al., [2024](#bib.bib4);
    Zhao et al., [2023](#bib.bib34); Liu et al., [2024](#bib.bib16)) could have sequential
    past KV cache with tree-structured queries. In DeFT, we propose another tree-based
    decoding with tree-structured past KV cache. A general tree-based decoding could
    have both tree-structured past KV and queries by combining the two aforementioned
    tree-decoding paradigms mentioned. Details are discussed in Appendix [A.2](#A1.SS2
    "A.2 Discussion of Tree-based Decoding ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '为了通过在每次迭代中生成多个标记来提高推理效率，基于树的解码 (Miao et al., [2023](#bib.bib20); Cai et al.,
    [2024](#bib.bib4); Zhao et al., [2023](#bib.bib34); Liu et al., [2024](#bib.bib16))
    可能会具有带有树结构查询的顺序过去KV缓存。在DeFT中，我们提出了另一种基于树的解码方法，具有树结构的过去KV缓存。一般的基于树的解码可以通过结合上述两种树解码范式来同时具有树结构的过去KV和查询。详细信息请参见附录
    [A.2](#A1.SS2 "A.2 Discussion of Tree-based Decoding ‣ Appendix A Appendix ‣ DeFT:
    Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference")。'
- en: Storage Optimization of Tree-based Decoding.
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于树的解码存储优化。
- en: LLM frameworks optimized for tree-based decoding (Kwon et al., [2023](#bib.bib14);
    Zheng et al., [2023](#bib.bib35)) focus on memory storage efficiency. vLLM (Kwon
    et al., [2023](#bib.bib14)) enhances GPU memory utilization, allowing sequences
    from the same parent to share KV cache storage. SGLang (Zheng et al., [2023](#bib.bib35))
    supports dynamic KV cache management during multi-round interactions with LLMs,
    improving memory efficiency.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 针对基于树的解码优化的LLM框架 (Kwon et al., [2023](#bib.bib14); Zheng et al., [2023](#bib.bib35))
    侧重于内存存储效率。vLLM (Kwon et al., [2023](#bib.bib14)) 增强了GPU内存利用率，允许来自相同父节点的序列共享KV缓存存储。SGLang (Zheng
    et al., [2023](#bib.bib35)) 支持在与LLM的多轮交互过程中动态管理KV缓存，提高了内存效率。
- en: 3 DeFT
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 DeFT
- en: In this section, we start by introducing the background knowledge of LLM inference,
    upon which we outline the system overview of DeFT. As a key component of DeFT,
    we present DeFT Attention Kernel, which not only reduces memory access of tree
    KV but also adopts a fused kernel to eliminate the memory access of partial results
    like $\mathbf{Q}\mathbf{K}^{\top}$ and Softmax operations. We further theoretically
    analyze DeFT’s IO with existing attention algorithms to justify its advances.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍 LLM 推理的背景知识，然后概述 DeFT 的系统概况。作为 DeFT 的一个关键组件，我们介绍 DeFT Attention
    Kernel，它不仅减少了树 KV 的内存访问，还采用了融合内核来消除对部分结果如 $\mathbf{Q}\mathbf{K}^{\top}$ 和 Softmax
    操作的内存访问。我们进一步从理论上分析 DeFT 的 IO 与现有注意力算法的比较，以证明其进步。
- en: 3.1 Preliminary
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初步
- en: LLM inference and its bottleneck.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 推理及其瓶颈。
- en: LLM inference consists of two stages, namely the (1) prefill stage and (2) decoding
    stage. In the prefill stage, a prompt is tokenized as the initial input of LLM.
    Upon receiving the prefill stage’s outputs from LLM, a new token then serves as
    the input for the decoding stage. The decoding stage is auto-regressive, where
    each output token from the previous step will be used as the input token for the
    next decoding step.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 推理包括两个阶段，即（1）预填充阶段和（2）解码阶段。在预填充阶段，提示被标记化为 LLM 的初始输入。在接收到 LLM 预填充阶段的输出后，一个新的令牌将作为解码阶段的输入。解码阶段是自回归的，其中来自前一步的每个输出令牌将作为下一个解码步骤的输入令牌。
- en: Due to the sequential process of auto-regressive decoding, LLM inference is
    memory-bound (Shazeer, [2019](#bib.bib24); Kim et al., [2023](#bib.bib13); Cai
    et al., [2024](#bib.bib4)), wherein every forward pass requires transferring all
    model parameters and KV cache from slower but larger High-Bandwidth Memory (HBM)
    to the faster but much smaller shared memory of the GPU (Jia & Van Sandt, [2021](#bib.bib12))
    ¹¹1 A100’s HBM has 1.5-2TB/s bandwidth and 40-80GB; its shared memory has 19TB/s
    bandwidth and 20MB. .
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自回归解码的顺序过程，LLM 推理受限于内存（Shazeer，[2019](#bib.bib24)；Kim 等，[2023](#bib.bib13)；Cai
    等，[2024](#bib.bib4)），每次前向传递都需要将所有模型参数和 KV 缓存从较慢但更大的高带宽内存（HBM）传输到更快但小得多的 GPU 共享内存（Jia
    & Van Sandt，[2021](#bib.bib12)）。A100 的 HBM 带宽为 1.5-2TB/s，容量为 40-80GB；其共享内存带宽为
    19TB/s，容量为 20MB。
- en: Motivation for DeFT.
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DeFT 的动机。
- en: To improve efficiency, boosting the arithmetic intensity—the ratio of total
    floating-point operations (FLOPs) to total memory access—of the decoding process
    is essential. Parallel decoding frameworks (Cai et al., [2024](#bib.bib4); Miao
    et al., [2023](#bib.bib20)) tend to achieve this goal by introducing more calculations
    to generate more tokens in each decoding step, while keeping memory access nearly
    the same²²2 Medusa (Cai et al., [2024](#bib.bib4)) only introduces negligible
    memory access of KV cache for token candidates in the tree. in each decoding step.
    A sequence of tokens will be generated as token candidates by draft models (Miao
    et al., [2023](#bib.bib20)) or fine-tuned heads (Cai et al., [2024](#bib.bib4)),
    which is then refined by the LLM for acceptable continuation. This line of approach
    reduces the total number of decoding steps as well as the total amount of memory
    access.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，提升解码过程的算术强度——即总浮点运算（FLOPs）与总内存访问的比率——至关重要。并行解码框架（Cai 等，[2024](#bib.bib4)；Miao
    等，[2023](#bib.bib20)）往往通过引入更多计算来生成更多令牌，以实现这一目标，同时保持内存访问几乎不变。Medusa（Cai 等，[2024](#bib.bib4)）仅引入了对树中令牌候选的
    KV 缓存的微不足道的内存访问。由草稿模型（Miao 等，[2023](#bib.bib20)）或微调头（Cai 等，[2024](#bib.bib4)）生成的一系列令牌将作为令牌候选，然后由
    LLM 进行精炼以获得可接受的续写。这种方法减少了总解码步骤数以及总内存访问量。
- en: In the meanwhile, tree-based decoding, leveraging the *decoding tree* defined
    below, enables efficient parallel decoding. The tree attention is further introduced
    to reduce redundant KV storage, calculation, and kernel launching overheads when
    calculating the attention.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，基于下文定义的*解码树*，树形解码实现了高效的并行解码。引入了树注意力，以减少计算注意力时冗余的 KV 存储、计算和内核启动开销。
- en: Definition 3.1  (Decoding tree).
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.1 （解码树）。
- en: A decoding tree $\mathcal{T}$ is a rooted tree where the root node corresponds
    to the prompt and each non-root node $u$ represents a sequence of generated tokens
    $\mathcal{S}_{u}$. For each node $u$, $\mathcal{B}_{u}$ is the path from root
    node to $u$ (without $u$) and $P_{\mathcal{B}_{u}}$ is the concatenation of tokens
    in sequences of nodes in path $\mathcal{B}_{u}$ by the sequential order. For each
    token $n\in u$, $s_{u,n}\in\mathcal{S}_{u}$ represents the sequence from the first
    token of node $u$ to $n$ (including $n$). The last token of each leaf node represents
    the input token for the next decoding iteration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 解码树 $\mathcal{T}$ 是一棵有根树，其中根节点对应于提示，每个非根节点 $u$ 表示生成的令牌序列 $\mathcal{S}_{u}$。对于每个节点
    $u$，$\mathcal{B}_{u}$ 是从根节点到 $u$ 的路径（不包括 $u$），$P_{\mathcal{B}_{u}}$ 是路径 $\mathcal{B}_{u}$
    中节点序列的令牌的连接。对于每个令牌 $n\in u$，$s_{u,n}\in\mathcal{S}_{u}$ 表示从节点 $u$ 的第一个令牌到 $n$
    的序列（包括 $n$）。每个叶节点的最后一个令牌代表下一个解码迭代的输入令牌。
- en: Definition 3.2  (Tree-Attention).
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.2（树注意力）。
- en: 'For each token $n\in u$, where $u$ is any non-root node in the decoding tree
    $\mathcal{T}$, its tree attention is defined as the output of original Transformer-based
    sequence attention ($\text{Attention}(\cdot)$) on $P_{\text{root}\rightarrow n}$,
    where $P_{\text{root}\rightarrow n}$ is the concatenation of $P_{B_{u}}$ and $s_{u,n}$:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解码树 $\mathcal{T}$ 中的每个令牌 $n\in u$（其中 $u$ 是任何非根节点），其树注意力定义为在 $P_{\text{root}\rightarrow
    n}$ 上应用原始基于 Transformer 的序列注意力（$\text{Attention}(\cdot)$）的输出，其中 $P_{\text{root}\rightarrow
    n}$ 是 $P_{B_{u}}$ 和 $s_{u,n}$ 的连接：
- en: '|  | $\displaystyle\textstyle\text{Tree-Attention}(n)=\text{Attention}(P_{\text{root}\rightarrow
    n})\,.$ |  | (1) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textstyle\text{Tree-Attention}(n)=\text{Attention}(P_{\text{root}\rightarrow
    n})\,.$ |  | (1) |'
- en: 'The existing solution of tree attention omits the potential IO optimization
    brought by the tree topology itself, thus motivating the DeFT we will explore
    in this paper. DeFT optimizes LLM efficiency from another perspective: It leverages
    the characteristics of node sharing in decoding trees to reduce the redundancy
    of KV cache IO from HBM to on-chip shared memory. Together with the IO-awareness
    DeFT tree attention for KV cache and partial results (i.e., $\mathbf{Q}\mathbf{K}^{\top}$),
    the whole arithmetic intensity will be improved with less memory access and nearly
    the same FLOPs.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的树注意力解决方案忽略了树拓扑本身带来的潜在 IO 优化，从而激发了我们将在本文中探讨的 DeFT。DeFT 从另一个角度优化 LLM 效率：它利用解码树中节点共享的特性来减少从
    HBM 到片上共享内存的 KV 缓存 IO 的冗余。结合 IO 感知 DeFT 树注意力用于 KV 缓存和部分结果（即 $\mathbf{Q}\mathbf{K}^{\top}$），整体算术强度将在减少内存访问和几乎相同
    FLOPs 的情况下得到改善。
- en: '![Refer to caption](img/672cb3bbd1ff01588958fb88a657a67f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/672cb3bbd1ff01588958fb88a657a67f.png)'
- en: 'Figure 2: Illustration of DeFT. (Left) System overview. (Right) The data flow
    using a decoding tree example.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：DeFT 的说明。（左）系统概述。（右）使用解码树示例的数据流。
- en: 3.2 System Overview of DeFT
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 DeFT 系统概述
- en: 'We outline the key component of DeFT, namely the DeFT Attention Kernel, in
    [Figure 2](#S3.F2 "Figure 2 ‣ Motivation for DeFT. ‣ 3.1 Preliminary ‣ 3 DeFT
    ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference"). As will be elaborated in Section [3.3](#S3.SS3 "3.3 An Efficient
    Attention Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash
    Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference"),
    the DeFT Attention Kernel requires 1) Query (tokens), 2) KV (KV cache of decoding
    tree), and 3) Tree Topo (the topology of decoding tree to map Query and KV), which
    are prepared by *Branch Controller*, *KV cache Manager*, and *Sequence Tree Manager*,
    respectively. The details of each component are in Appendix [A.1](#A1.SS1 "A.1
    Components of system support for DeFT ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 [图 2](#S3.F2 "图 2 ‣ DeFT 动机 ‣ 3.1 初步 ‣ 3 DeFT ‣ DeFT: 具有 IO 感知的快速树注意力用于高效的基于树的
    LLM 推理") 中概述了 DeFT 的关键组件，即 DeFT 注意力内核。如 [3.3](#S3.SS3 "3.3 高效的注意力算法，通过平铺和重用树 KV
    缓存 ‣ 3 DeFT ‣ DeFT: 具有 IO 感知的快速树注意力用于高效的基于树的 LLM 推理") 节中详细说明的那样，DeFT 注意力内核需要 1)
    查询（令牌），2) KV（解码树的 KV 缓存），以及 3) 树拓扑（用于映射查询和 KV 的解码树拓扑），这些由 *分支控制器*、*KV 缓存管理器* 和
    *序列树管理器* 准备。每个组件的详细信息见附录 [A.1](#A1.SS1 "A.1 DeFT 系统支持组件 ‣ 附录 A ‣ DeFT: 具有 IO 感知的快速树注意力用于高效的基于树的
    LLM 推理")。'
- en: 'The right part of [Figure 2](#S3.F2 "Figure 2 ‣ Motivation for DeFT. ‣ 3.1
    Preliminary ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient
    Tree-search-based LLM Inference") further showcases the key data flow of the system
    through a decoding tree example: input metadata will be extracted by tree components
    we mentioned above, then loaded from HBM to shared memory in a group manner during
    the QKV Preparation phase discussed later. Then QKV groups will be processed by
    DeFT Attention Kernel in Section [3.3](#S3.SS3 "3.3 An Efficient Attention Algorithm
    With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with
    IO-Awareness for Efficient Tree-search-based LLM Inference").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2](#S3.F2 "图 2 ‣ DeFT 的动机 ‣ 3.1 初步 ‣ 3 DeFT ‣ DeFT：具有 IO 认识的 Flash 树形注意力，用于高效的树形搜索基础
    LLM 推理") 的右侧部分通过解码树示例进一步展示了系统的关键数据流：输入元数据将由我们前面提到的树组件提取，然后在稍后讨论的 QKV 准备阶段以分组方式从
    HBM 加载到共享内存中。然后，QKV 组将由第 [3.3](#S3.SS3 "3.3 一种高效的注意力算法，通过切片和重用树形 KV 缓存 ‣ 3 DeFT
    ‣ DeFT：具有 IO 认识的 Flash 树形注意力，用于高效的树形搜索基础 LLM 推理") 节中描述的 DeFT 注意力内核处理。'
- en: 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV cache
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 一种高效的注意力算法，通过切片和重用树形 KV 缓存
- en: '![Refer to caption](img/80e4a0388413e8cee3b538a81414243d.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/80e4a0388413e8cee3b538a81414243d.png)'
- en: 'Figure 3: Comparison of memory access from HBM to shared memory for different
    attention algorithms in QKV Preparation Phase, where the amount of IO required
    by each is enclosed in red rectangles. Tiling is adopted in Flash Decoding (Dao
    et al., [2023](#bib.bib6)), Tree Attention-SpecInfer (Miao et al., [2023](#bib.bib20)),
    and DeFT to fit in small shared memory with a fused kernel. Sequence-based attention
    algorithms like Flash Decoding are not aware of the tree topology of KV cache,
    so $KV_{0}$ will be loaded twice; Tree Attention in Medusa (Cai et al., [2024](#bib.bib4))
    groups all queries and KV cache in a tree, with additional IO of the causal mask,
    then the QKV group will be allocated to streaming multiprocessors in GPU by Pytorch
    primitives; Tree Attention in SpecInfer will load KV cache of the whole tree for
    each query with the causal mask; DeFT groups QKV based on KV with tree topology
    information (e.g. from the decoding tree at the left, we can know $Q_{1}$ and
    $Q_{2}$ both share KV cache of sequence node $S_{0}$), which reduces IO of $KV_{0}$
    at the expense of negligible query overhead.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在 QKV 准备阶段，比较了不同注意力算法从 HBM 到共享内存的内存访问，其中每种算法所需的 IO 量用红色矩形框起来。Flash 解码（Dao
    et al., [2023](#bib.bib6)）、树形注意力-规约推理（Miao et al., [2023](#bib.bib20)）和 DeFT 采用切片以适应小型共享内存与融合内核。基于序列的注意力算法，如
    Flash 解码，并不考虑 KV 缓存的树形拓扑，因此 $KV_{0}$ 将被加载两次；Medusa 中的树形注意力（Cai et al., [2024](#bib.bib4)）将所有查询和
    KV 缓存分组到一个树中，并增加了因果掩码的额外 IO，然后 QKV 组将由 Pytorch 原语分配到 GPU 的流处理器中；SpecInfer 中的树形注意力将为每个查询加载整个树的
    KV 缓存及其因果掩码；DeFT 基于具有树形拓扑信息的 KV 对 QKV 进行分组（例如，从左侧的解码树中，我们可以知道 $Q_{1}$ 和 $Q_{2}$
    都共享序列节点 $S_{0}$ 的 KV 缓存），这减少了 $KV_{0}$ 的 IO，同时查询开销可以忽略不计。
- en: 'We can separate the execution of attention algorithms into two main phases:
    (1) QKV Preparation Phase: load Query, Key, and Value (QKV) into shared memory
    and group them logically to calculate attention; (2) Attention Calculation Phase:
    apply attention algorithms to QKV groups for final attention results.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将注意力算法的执行分为两个主要阶段：（1）QKV 准备阶段：将查询、键和值（QKV）加载到共享内存中并进行逻辑分组以计算注意力；（2）注意力计算阶段：对
    QKV 组应用注意力算法以获得最终的注意力结果。
- en: DeFT aims to be a memory-efficient algorithm in both aforementioned phases to
    get exact attention. Motivated by the heterogeneous GPU memory hierarchy (Dao
    et al., [2022](#bib.bib5))—namely, HBM is large but slower while the shared memory
    is much smaller but much faster—minimizing memory access between HBM and shared
    memory for memory-bound computations (e.g., attention) during the attention computation
    is crucial.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: DeFT 旨在在上述两个阶段中实现内存高效算法，以获得准确的注意力。受到异构 GPU 内存层次结构（Dao et al., [2022](#bib.bib5)）的启发——即
    HBM 较大但较慢，而共享内存较小但速度更快——在注意力计算过程中最小化 HBM 和共享内存之间的内存访问对内存绑定计算（例如注意力）至关重要。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In the QKV Preparation Phase, we introduce a KV-Guided Tree Split strategy with
    tree-topology awareness to minimize the IO of QKV.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 QKV 准备阶段，我们引入了一种具有树形拓扑意识的 KV 引导树分裂策略，以最小化 QKV 的 IO。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: During the Attention Calculation Phase, we propose a Tree-Topology-Aware Global
    Reduction strategy combined with the established techniques (Kernel Fusion and
    Tiling), to eliminate the IO of partial results (i.e.. $\mathbf{Q}\mathbf{K}^{\top}$
    and Softmax).
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力计算阶段，我们提出了一种树拓扑感知全局归约策略，结合了已有技术（内核融合和切片），以消除部分结果的IO（即 $\mathbf{Q}\mathbf{K}^{\top}$
    和 Softmax）。
- en: QKV Preparation Phase of DeFT.
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DeFT的QKV准备阶段。
- en: In sequence-based decoding, split strategy—namely splitting the inputs QKV into
    blocks—is commonly deployed to generate enough QKV groups for full utilization
    of the GPU (Dao et al., [2023](#bib.bib6)). This technique is crucial when the
    parallelism (usually limited by the batch size(Dao et al., [2023](#bib.bib6)))
    is much smaller than the number of streaming multiprocessors (SMs) on the GPU
    (108 for an A100), where the operation will only utilize a small portion of the
    GPU. Similarly, for tree-based decoding—where a decoding tree consists of multiple
    nodes and each node is a sequence of tokens—the batch size of trees may also be
    insufficient to fully utilize the GPU when the number of tokens in the tree is
    large, due to memory capacity limitations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于序列的解码中，分裂策略，即将输入QKV分割成块，通常用于生成足够的QKV组以充分利用GPU（Dao等，[2023](#bib.bib6)）。当并行度（通常受批量大小限制（Dao等，[2023](#bib.bib6)））远小于GPU上的流处理器（SMs）（A100为108）时，这种技术至关重要，因为操作仅会利用GPU的一小部分。类似地，在基于树的解码中——其中解码树由多个节点组成，每个节点是一系列标记——当树中的标记数量很大时，由于内存容量限制，树的批量大小也可能不足以充分利用GPU。
- en: 'Unfortunately, split the tree is not as easy as split the sequence: applying
    the existing split strategy (Dao et al., [2023](#bib.bib6)) in sequence-based
    decoding to tree-based decoding could cause redundancy of KV cache IO when grouping
    QKV based on Q without tree topology awareness, as illustrated in the top left
    of [Figure 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient Attention Algorithm With Tiling
    and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference"). To bridge this gap, DeFT splits
    the tree by sequence nodes³³3When the sequence length of a node is substantial,
    it can be split into blocks similar to Flash-Decoding (Dao et al., [2023](#bib.bib6))
    to ensure the more balanced KV lengths among QKV groups., then group the KV of
    each node with all queries that share it based on tree topology.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '不幸的是，分裂树不像分裂序列那样简单：在基于树的解码中应用现有的分裂策略（Dao等，[2023](#bib.bib6)）可能会导致KV缓存IO的冗余，当基于Q而不考虑树拓扑来分组QKV时，如[图
    3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing
    Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient
    Tree-search-based LLM Inference")左上角所示。为了解决这一问题，DeFT通过序列节点³³3当节点的序列长度很大时，可以将其分割成类似于Flash-Decoding（Dao等，[2023](#bib.bib6)）的块，以确保QKV组之间的KV长度更均衡。来分裂树，然后根据树拓扑将每个节点的KV与所有共享它的查询进行分组。'
- en: Remark 3.3  (Properties of KV-Guided Tree Split strategy).
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 3.3（KV引导的树分裂策略的属性）。
- en: 'This KV-Guided Tree Split strategy, with KV as the indicator for grouping,
    eliminates redundant IO operations for KV with negligible query IO cost, as illustrated
    in the bottom right of [Figure 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient Attention
    Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '这种KV引导的树分裂策略，以KV作为分组指标，消除了对KV的冗余IO操作，查询IO成本微不足道，如[图 3](#S3.F3 "Figure 3 ‣ 3.3
    An Efficient Attention Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT
    ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference")右下角所示。'
- en: 'The additional IO cost of Q in DeFT is negligible because the length of the
    KV often surpasses that of the Q during tree decoding, primarily due to two reasons:
    (1) the auto-regressive decoding pattern dictates that each query in the decoding
    stage has a length of 1, which means the maximum query length of a decoding tree
    is determined by the number of branches; (2) In tree-search tasks, the token length
    of each sequence node in the decoding tree ("thought" in reasoning) is typically
    not exceedingly small (Yao et al., [2023](#bib.bib33)), implying that the KV sequence
    length of a node is often much larger than the total length of all queries that
    share it.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DeFT 中，Q 的额外 IO 成本可以忽略不计，因为 KV 的长度通常超过 Q 的长度，主要有两个原因：(1) 自回归解码模式规定解码阶段的每个查询长度为
    1，这意味着解码树的最大查询长度由分支数量决定；(2) 在树搜索任务中，解码树中每个序列节点的标记长度（推理中的“思想”）通常不会非常小（Yao 等， [2023](#bib.bib33)），这意味着一个节点的
    KV 序列长度通常远大于所有共享该节点的查询的总长度。
- en: 'Besides, DeFT is extreme simple yet effective: during decoding, there is no
    need for DeFT to utilize a causal mask⁴⁴4In Appendix [A.4](#A1.SS4 "A.4 DeFT-Subtree
    Algorithm ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference"), we also provide a variant of
    the DeFT algorithm with split-granularity of subtrees, which could have more balanced
    QKV groups by split the decoding tree to evenly subtrees, with the cost of introducing
    causal masks. (Miao et al., [2023](#bib.bib20); Cai et al., [2024](#bib.bib4))
    to calculate the attention in the incoming Attention Calculation Phase. Instead,
    only the tree topology among sequence nodes in the decoding tree is required.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，DeFT 极其简单但有效：在解码过程中，DeFT 不需要使用因果掩码⁴⁴4 在附录 [A.4](#A1.SS4 "A.4 DeFT-Subtree
    Algorithm ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference") 中，我们还提供了一个 DeFT 算法的变体，通过将解码树拆分成均匀的子树，以实现更平衡的
    QKV 组，代价是引入因果掩码。(Miao 等， [2023](#bib.bib20)；Cai 等， [2024](#bib.bib4)) 在即将到来的注意力计算阶段中计算注意力。相反，只需要解码树中序列节点之间的树拓扑。'
- en: Attention Calculation Phase of DeFT.
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DeFT 的注意力计算阶段。
- en: GPUs boast an extensive array of threads to execute an operation, known as a
    kernel. Each kernel retrieves inputs from HBM to registers and shared memory,
    processes them, and subsequently saves the outputs back to HBM.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: GPUs 拥有广泛的线程阵列来执行操作，称为内核。每个内核从 HBM 中检索输入到寄存器和共享内存，处理这些输入，然后将输出保存回 HBM。
- en: In this phase, we design DeFT kernel to load QKV splits in a memory efficient
    way, which are logically grouped by the QKV Preparation Phase, then to perform
    the attention calculation. We achieve DeFT kernel by using Kernel Fusion with
    Tiling strategy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们设计 DeFT 内核以内存高效的方式加载 QKV 拆分，这些拆分由 QKV 准备阶段逻辑分组，然后执行注意力计算。我们通过使用 Tiling
    策略的内核融合实现 DeFT 内核。
- en: 'Kernel Fusion is a common technique of IO reduction: if multiple operations
    are performed on the same input, it is more efficient to load the input once from
    HBM rather than loading it multiple times for each operation; Similarly, the same
    principle applies when transferring output from shared memory to HBM. To fuse
    all the attention operations into one GPU kernel with the limited size of shared
    memory, we further utilize the commonly employed Tiling strategy (Dao et al.,
    [2022](#bib.bib5); [2023](#bib.bib6)): split queries and KV cache within each
    QKV group to small blocks to prevent materialization of attention matrix in HBM
    by computing attention within the limited size of shared memory, then incrementally
    performing the softmax reduction to reconstruct the attention.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 内核融合是一种常见的 IO 减少技术：如果对同一输入执行多个操作，从 HBM 中加载输入一次比为每个操作多次加载更高效；类似地，从共享内存到 HBM 的输出传输也适用相同的原则。为了将所有注意力操作融合到一个
    GPU 内核中，同时共享内存的大小有限，我们进一步利用常用的 Tiling 策略 (Dao 等， [2022](#bib.bib5)； [2023](#bib.bib6))：在每个
    QKV 组内将查询和 KV 缓存拆分为小块，以防止在 HBM 中生成注意力矩阵，通过在有限的共享内存中计算注意力，然后逐步执行 softmax 减少以重建注意力。
- en: 'DeFT kernel consists of two stages, as shown in [Figure 5](#S3.F5 "Figure 5
    ‣ Attention Calculation Phase of DeFT. ‣ 3.3 An Efficient Attention Algorithm
    With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with
    IO-Awareness for Efficient Tree-search-based LLM Inference"):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeFT 内核由两个阶段组成，如 [图 5](#S3.F5 "Figure 5 ‣ Attention Calculation Phase of DeFT.
    ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV cache ‣
    3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference") 所示：'
- en: '1.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Stage 1–calculate partial attentions. Based on the QKV grouping method of DeFT
    mentioned above, each QKV group will be allocated to a thread block for Flash
    Attention (Dao et al., [2022](#bib.bib5)) calculation with Tiling strategy. Similar
    to Flash-Decoding (Dao et al., [2023](#bib.bib6)), we not only get partial attention
    but also return “LogSumExp” as a weight parameter for the next stage’s reduction.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阶段1–计算部分注意力。根据上述DeFT的QKV分组方法，每个QKV组将分配给一个线程块进行Flash Attention（Dao等，[2022](#bib.bib5)）计算，采用平铺策略。类似于Flash-Decoding（Dao等，[2023](#bib.bib6)），我们不仅获取部分注意力，还返回“LogSumExp”作为下一阶段归约的权重参数。
- en: '2.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Stage 2–global reduction. Upon receiving partial attention and LogSumExps for
    all QKV groups—recall that we grouped QKV based on KV before attention calculation—DeFT
    now performs a Tree-Topology-Aware Global Reduction. Guided by the tree topology
    among sequence nodes of KV in the decoding tree, DeFT remaps the partial results
    of attention and LogSumExp based on a query for the execution of global reduction.
    The grouped partial attention and LogSumExp will be passed to “Global_reduction_kernel”
    on the right side of [Figure 5](#S3.F5 "Figure 5 ‣ Attention Calculation Phase
    of DeFT. ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV
    cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference"). To point out, there is no memory movement during the group of
    partial attention and LogSumExp as we group them logically by recording offsets
    of partial results required for a query.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阶段2–全局归约。在接收到所有QKV组的部分注意力和LogSumExps后——请回忆，我们在注意力计算前基于KV对QKV进行分组——DeFT现在执行树拓扑感知全局归约。在解码树的KV序列节点之间的树拓扑指导下，DeFT根据查询重新映射注意力和LogSumExp的部分结果，以执行全局归约。分组的部分注意力和LogSumExp将传递到[图5](#S3.F5
    "图 5 ‣ DeFT的注意力计算阶段 ‣ 3.3 一种高效的注意力算法，具有平铺和重用树KV缓存 ‣ 3 DeFT ‣ DeFT：带有IO感知的闪存树注意力，用于高效的树搜索基础LLM推理")右侧的“Global_reduction_kernel”。需要指出的是，在分组部分注意力和LogSumExp的过程中，没有发生内存移动，因为我们通过记录所需查询的部分结果的偏移量来逻辑地进行分组。
- en: '![Refer to caption](img/716f2b4dbe375a85f133a8d766ba2838.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/716f2b4dbe375a85f133a8d766ba2838.png)'
- en: 'Figure 4: Operations of Tree Attention-Medusa (Cai et al., [2024](#bib.bib4)).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Tree Attention-Medusa的操作（Cai等，[2024](#bib.bib4)）。
- en: '![Refer to caption](img/730654551b49471d23cbc19ddb94b460.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/730654551b49471d23cbc19ddb94b460.png)'
- en: '(a) Left: Illustration of DeFT kernel with two stages. Right: Global reduction
    kernel called in DeFT stage 2 illustrated in Figure [5b](#S3.F5.sf2 "In Figure
    5 ‣ Attention Calculation Phase of DeFT. ‣ 3.3 An Efficient Attention Algorithm
    With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with
    IO-Awareness for Efficient Tree-search-based LLM Inference"). QKV Groups $G_{0}$,$G_{1}$
    and $G_{2}$ are from DeFT QKV groups in [Figure 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient
    Attention Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash
    Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 左：带有两个阶段的DeFT内核示意图。右：在[图5b](#S3.F5.sf2 "在图 5 ‣ DeFT的注意力计算阶段 ‣ 3.3 一种高效的注意力算法，具有平铺和重用树KV缓存
    ‣ 3 DeFT ‣ DeFT：带有IO感知的闪存树注意力，用于高效的树搜索基础LLM推理")中说明的DeFT阶段2调用的全局归约内核。QKV组$G_{0}$，$G_{1}$和$G_{2}$来自[图3](#S3.F3
    "图 3 ‣ 3.3 一种高效的注意力算法，具有平铺和重用树KV缓存 ‣ 3 DeFT ‣ DeFT：带有IO感知的闪存树注意力，用于高效的树搜索基础LLM推理")中的DeFT
    QKV组。
- en: '![Refer to caption](img/d1e196d92ac3bb70185442ed046085c2.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1e196d92ac3bb70185442ed046085c2.png)'
- en: '(b) Stage 2 of DeFT: Global Reduction. Based on tree topology in [Figure 3](#S3.F3
    "Figure 3 ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree
    KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient
    Tree-search-based LLM Inference"), we can group LogSumExp and Partial Attention
    based on Query, then we call the Global reduction kernel in the right of Figure [5a](#S3.F5.sf1
    "In Figure 5 ‣ Attention Calculation Phase of DeFT. ‣ 3.3 An Efficient Attention
    Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference") to get the final
    attention.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: (b) DeFT的阶段2：全局归约。根据[图3](#S3.F3 "图 3 ‣ 3.3 一种高效的注意力算法，具有平铺和重用树KV缓存 ‣ 3 DeFT
    ‣ DeFT：带有IO感知的闪存树注意力，用于高效的树搜索基础LLM推理")中的树拓扑结构，我们可以根据查询将LogSumExp和部分注意力进行分组，然后调用[图5a](#S3.F5.sf1
    "在图5 ‣ DeFT的注意力计算阶段 ‣ 3.3 一种高效的注意力算法，具有平铺和重用树KV缓存 ‣ 3 DeFT ‣ DeFT：带有IO感知的闪存树注意力，用于高效的树搜索基础LLM推理")右侧的全局归约内核来获得最终的注意力。
- en: 'Figure 5: Attention operations of DeFT kernel. Based on the same decoding tree
    in [Figure 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient Attention Algorithm With Tiling
    and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5：DeFT 内核的注意力操作。基于 [图 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient Attention Algorithm
    With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with
    IO-Awareness for Efficient Tree-search-based LLM Inference") 中相同的解码树。'
- en: Remark 3.4  (The effects of QKV grouping strategy in the QKV Preparation Phase).
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 3.4 （QKV 准备阶段 QKV 分组策略的效果）。
- en: 'In the QKV Preparation Phase, how QKVs are grouped logically results in different
    memory access of QKV for tree decoding, as shown in [Figure 3](#S3.F3 "Figure
    3 ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV cache
    ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '在 QKV 准备阶段，QKVs 的逻辑分组会导致树解码的 QKV 内存访问不同，如 [图 3](#S3.F3 "Figure 3 ‣ 3.3 An Efficient
    Attention Algorithm With Tiling and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash
    Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference")
    所示。'
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Flash-Decoding (Dao et al., [2023](#bib.bib6)), splits long KV and group QKV
    based on Q without tree topology awareness, which will bring redundant KV cache
    IO from GPU global memory to shared memory;
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Flash-Decoding（Dao et al., [2023](#bib.bib6)），基于 Q 分割长 KV 并分组 QKV，而没有树拓扑感知，这将导致从
    GPU 全局内存到共享内存的冗余 KV 缓存 IO；
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tree Attention-Medusa (Cai et al., [2024](#bib.bib4)) groups the QKV of the
    entire decoding tree together with a tree topology-aware causal mask for tree
    attention computation based on Pytorch primitives, resulting in no redundancy
    of loading KV cache and Query with the cost of additional IO for the causal mask;
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Tree Attention-Medusa （Cai et al., [2024](#bib.bib4)）将整个解码树的 QKV 与树拓扑感知的因果掩码结合用于树注意力计算，基于
    Pytorch 原语，避免了加载 KV 缓存和 Query 的冗余，只需额外的 IO 代价用于因果掩码；
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tree Attention-SpecInfer (Miao et al., [2023](#bib.bib20)) groups each query
    with the KV of the entire tree with a causal mask for tree attention calculation,
    which has great redundancy in KV cache IO.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Tree Attention-SpecInfer （Miao et al., [2023](#bib.bib20)）将每个查询与整个树的 KV 和因果掩码结合用于树注意力计算，这在
    KV 缓存 IO 中具有很大的冗余。
- en: Remark 3.5  (On the importance of tiling and fused kernel during Attention Calculation
    Phase).
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 3.5 （关于在注意力计算阶段切分和融合内核的重要性）。
- en: 'Methods in this phase can be roughly divided into two categories: (1) without
    tiling and kernel fusion: Tree Attention in Medusa (Cai et al., [2024](#bib.bib4));
    (2) with tiling and a fused kernel: Flash Decoding (Dao et al., [2023](#bib.bib6)),
    Tree Attention in SpecInfer (Miao et al., [2023](#bib.bib20)) and our DeFT.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本阶段的方法大致可分为两类：（1）无切分和内核融合：Medusa 中的树注意力（Cai et al., [2024](#bib.bib4)）；（2）有切分和融合内核：Flash
    Decoding（Dao et al., [2023](#bib.bib6)），SpecInfer 中的树注意力（Miao et al., [2023](#bib.bib20)）和我们的
    DeFT。
- en: 'Note that the method with no tiling and a fused kernel is inefficient during
    attention computation due to the materialization of the attention matrix in HBM.
    In detail, this approach may lead to significant IO operations for the causal
    mask (CM) and partial results (i.e.. $\mathbf{Q}\mathbf{K}^{\top}$ and Softmax),
    as illustrated in [Figure 4](#S3.F4 "Figure 4 ‣ Attention Calculation Phase of
    DeFT. ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV cache
    ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，无切分和融合内核的方法在注意力计算期间效率低下，因为在 HBM 中材料化的注意力矩阵。具体来说，这种方法可能会导致因果掩码（CM）和部分结果（即
    $\mathbf{Q}\mathbf{K}^{\top}$ 和 Softmax）的大量 IO 操作，如 [图 4](#S3.F4 "Figure 4 ‣ Attention
    Calculation Phase of DeFT. ‣ 3.3 An Efficient Attention Algorithm With Tiling
    and Reusing Tree KV cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference") 所示。'
- en: DeFT is designed and implemented with a fused kernel to eliminate the IO cost
    of partial results mentioned above.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: DeFT 设计并实现了一个融合内核，以消除上述部分结果的 IO 成本。
- en: Remark 3.6  (The effects of introducing a causal mask).
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 3.6 （引入因果掩码的效果）。
- en: 'Causal mask brings two parts of redundancy:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因果掩码带来了两个部分的冗余：
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Access. Medusa (Cai et al., [2024](#bib.bib4)) materializes the causal
    mask in HBM to record the causal information between $n_{q}$ tokens in queries
    and $n_{kv}$ tokens in the KV cache, thereby introducing a significant IO cost
    for loading this $n_{q}\times n_{kv}$-sized mask to shared memory. SpecInfer (Miao
    et al., [2023](#bib.bib20)) introduces a 64-bit integer as a bit mask to record
    the causal information among up to 64 tokens, which incurs minimal IO cost from
    HBM to shared memory but is not suitable for decoding trees with more than 64
    tokens. Details regarding the design of the bit mask in SpecInfer are discussed
    in Appendix [A.2](#A1.SS2 "A.2 Discussion of Tree-based Decoding ‣ Appendix A
    Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '内存访问。Medusa (Cai et al., [2024](#bib.bib4)) 将因果掩码物化到 HBM 中，以记录查询中的 $n_{q}$
    个令牌和 KV 缓存中的 $n_{kv}$ 个令牌之间的因果信息，从而引入了将 $n_{q}\times n_{kv}$ 大小的掩码加载到共享内存中的显著
    IO 成本。SpecInfer (Miao et al., [2023](#bib.bib20)) 引入了一个 64 位整数作为位掩码来记录最多 64 个令牌之间的因果信息，这在从
    HBM 到共享内存的 IO 成本上最小，但不适合解码超过 64 个令牌的树。有关 SpecInfer 中位掩码设计的详细信息，请参见附录 [A.2](#A1.SS2
    "A.2 Discussion of Tree-based Decoding ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference")。'
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computation. In addition to the computational cost of generating the causal
    mask itself, there is an additional redundancy in computation: many of the matrix
    multiplication results of $\mathbf{Q}\mathbf{K}^{\top}$ are masked out and never
    utilized. Both Medusa and SpecInfer have this issue.'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算。除了生成因果掩码本身的计算成本外，还有额外的计算冗余：许多 $\mathbf{Q}\mathbf{K}^{\top}$ 的矩阵乘法结果被掩盖并未被利用。Medusa
    和 SpecInfer 都有这个问题。
- en: DeFT does not require a causal mask and there is no IO and calculation redundancy
    caused by masking.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: DeFT 不需要因果掩码，并且没有因掩码造成的 IO 和计算冗余。
- en: Implementation details.
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节。
- en: 'We implement the DeFT attention kernel by OpenAI Triton (Tillet et al., [2019](#bib.bib25)),
    which enables us to control memory access from global memory to shared memory
    and attention calculations in a thread block granularity. DeFT algorithm with
    two phases in a Python style can be found in Appendix [A.3](#A1.SS3 "A.3 DeFT-Node
    Algorithm ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference").'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过 OpenAI Triton (Tillet et al., [2019](#bib.bib25)) 实现了 DeFT 注意力内核，这使我们能够控制从全局内存到共享内存的内存访问以及线程块粒度的注意力计算。Python
    风格的 DeFT 算法的两个阶段可以在附录 [A.3](#A1.SS3 "A.3 DeFT-Node Algorithm ‣ Appendix A Appendix
    ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference") 中找到。'
- en: '3.4 Analysis: IO Complexity of DeFT'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 分析：DeFT 的 IO 复杂度
- en: 'Table 2: Notations.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：符号。
- en: '| $\displaystyle l_{n}$ | Number of leaf nodes in a decoding tree, which means
    how many queries in this decoding iteration. |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| $\displaystyle l_{n}$ | 解码树中的叶子节点数量，即此解码迭代中的查询数量。 |'
- en: '| $\displaystyle N_{i}$ | Total token length from the root node to leaf node
    i |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| $\displaystyle N_{i}$ | 从根节点到叶子节点 i 的总令牌长度 |'
- en: '| $\displaystyle N_{tree}$ | Total token length the entire tree. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| $\displaystyle N_{tree}$ | 整棵树的总令牌长度。 |'
- en: '| $\displaystyle d_{head}$ | Head dimension of LLM. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| $\displaystyle d_{head}$ | LLM 的头维度。 |'
- en: '| $\displaystyle F_{s}$ | Shared factor of reusing prefixes in tree attention,
    which means to which extent we can reduce IOs of KV cache: $F_{s}=(\sum_{i=1}^{ln}N_{i})/N_{tree}$.
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $\displaystyle F_{s}$ | 树注意力中复用前缀的共享因子，表示我们可以减少 KV 缓存 IO 的程度：$F_{s}=(\sum_{i=1}^{ln}N_{i})/N_{tree}$。
    |'
- en: 'Table 3: IO complexity breakdown for various methods. $\mathcal{O}(1)$ denotes
    the IO cost for a single data in the tensor across all layers and heads, which
    is equivalent to $\#heads*\#layer*dtype\_size$. Red ticks mean the best among
    all methods in the table, while red crosses mean the (potential) worst.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：各种方法的 IO 复杂度细分。$\mathcal{O}(1)$ 表示在所有层和头中单个数据的 IO 成本，相当于 $\#heads*\#layer*dtype\_size$。红色勾号表示表中所有方法中的最佳，红色叉号表示（潜在的）最差。
- en: '| Method | Query | KV cache | $\mathbf{Q}\mathbf{K}^{\top}$ | $\text{Softmax}(\mathbf{Q}\mathbf{K}^{\top})$
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 查询 | KV 缓存 | $\mathbf{Q}\mathbf{K}^{\top}$ | $\text{Softmax}(\mathbf{Q}\mathbf{K}^{\top})$
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Naive Attention | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}\sum_{i=1}^{l_{n}}N_{i})$$\times$
    | $\mathcal{O}(2\sum_{i=1}^{l_{n}}N_{i})$ | $\mathcal{O}(2\sum_{i=1}^{l_{n}}N_{i})$
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Naive Attention | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}\sum_{i=1}^{l_{n}}N_{i})$$\times$
    | $\mathcal{O}(2\sum_{i=1}^{l_{n}}N_{i})$ | $\mathcal{O}(2\sum_{i=1}^{l_{n}}N_{i})$
    |'
- en: '| Flash-Decoding | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}\sum_{i=1}^{l_{n}}N_{i})$
    $\times$ | $0$ ✓ | $0$ ✓ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Flash-Decoding | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}\sum_{i=1}^{l_{n}}N_{i})$
    $\times$ | $0$ ✓ | $0$ ✓ |'
- en: '| Tree Attention-Medusa | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree})$
    ✓ | $\mathcal{O}(2l_{n}N_{tree})$ $\times$ | $\mathcal{O}(2l_{n}N_{tree})$ $\times$
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Tree Attention-Medusa | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree})$
    ✓ | $\mathcal{O}(2l_{n}N_{tree})$ $\times$ | $\mathcal{O}(2l_{n}N_{tree})$ $\times$
    |'
- en: '| Tree Attention-SpecInfer | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree}l_{n})$
    $\times$ | $0$ ✓ | $0$ ✓ |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Tree Attention-SpecInfer | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree}l_{n})$
    $\times$ | $0$ ✓ | $0$ ✓ |'
- en: '| DeFT (Ours) | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree})$
    ✓ | $0$ ✓ | $0$ ✓ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| DeFT (我们的) | $\mathcal{O}(l_{n}d_{head})$ | $\mathcal{O}(d_{head}N_{tree})$
    ✓ | $0$ ✓ | $0$ ✓ |'
- en: This section analyzes the IO complexity of DeFT, showing a significant reduction
    in HBM accesses compared to existing attention algorithms. Note that it is non-trivial
    to summarize the IO cost of the entire tree decoding process, thus we only compare
    IOs based on the decoding tree snapshot in a single iteration.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本节分析了 DeFT 的 IO 复杂性，并显示了与现有注意力算法相比，HBM 访问显著减少。请注意，汇总整个树解码过程的 IO 成本并非易事，因此我们仅基于单次迭代中的解码树快照来比较
    IO。
- en: 'Consider a decoding tree with the features outlined in [Table 2](#S3.T2 "Table
    2 ‣ 3.4 Analysis: IO Complexity of DeFT ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference"), and we summarize
    the corresponding IO breakdown in [Table 3](#S3.T3 "Table 3 ‣ 3.4 Analysis: IO
    Complexity of DeFT ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for
    Efficient Tree-search-based LLM Inference"). It can be observed that *due to the
    lack of tree-topology awareness, sequence-based decoding methods, such as naive
    attention and Flash-Decoding, incur $F_{s}$ times more memory access overheads
    for KV cache compared to DeFT and Tree Attention-Medusa (Cai et al., [2024](#bib.bib4))*.
    However, Tree Attention-Medusa entails higher IO overheads for partial results
    like $\mathbf{Q}\mathbf{K}^{\top}$ and Softmax due to the lack of tiling and kernel
    fusion⁵⁵5 Note that both $\mathbf{Q}\mathbf{K}^{\top}$ and Softmax will load and
    write, so the IO cost contains a round-trip of memory access between HBM and shared
    memory, as shown in [Figure 4](#S3.F4 "Figure 4 ‣ Attention Calculation Phase
    of DeFT. ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV
    cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference"). . When the number of leaf nodes/queries $ln$ is sufficiently
    large, the IO cost of partial results might become comparable to that of the KV
    cache. For instance, in the Llama models (Touvron et al., [2023a](#bib.bib26);
    [b](#bib.bib27)), where $d_{head}\!=\!128$, with $l_{n}\!=\!32$, the total IO
    cost of $\mathbf{Q}\mathbf{K}^{T}$ and Softmax matches that of the KV cache.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '参考 [表 2](#S3.T2 "Table 2 ‣ 3.4 Analysis: IO Complexity of DeFT ‣ 3 DeFT ‣ DeFT:
    Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference")
    中概述的解码树特征，我们在 [表 3](#S3.T3 "Table 3 ‣ 3.4 Analysis: IO Complexity of DeFT ‣ 3
    DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference") 中总结了相应的 IO 细分。可以观察到 *由于缺乏树拓扑感知，基于序列的解码方法，如原始注意力和 Flash-Decoding，相比于
    DeFT 和 Tree Attention-Medusa (Cai et al., [2024](#bib.bib4))，对 KV 缓存产生了 $F_{s}$
    倍的更多内存访问开销*。然而，由于缺乏切片和内核融合，Tree Attention-Medusa 在部分结果（如 $\mathbf{Q}\mathbf{K}^{\top}$
    和 Softmax）上产生了更高的 IO 开销⁵⁵5。请注意，$\mathbf{Q}\mathbf{K}^{\top}$ 和 Softmax 都会进行加载和写入，因此
    IO 成本包括 HBM 和共享内存之间的往返内存访问，如 [图 4](#S3.F4 "Figure 4 ‣ Attention Calculation Phase
    of DeFT. ‣ 3.3 An Efficient Attention Algorithm With Tiling and Reusing Tree KV
    cache ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference") 所示。当叶子节点/查询数 $ln$ 足够大时，部分结果的 IO 成本可能会与 KV 缓存的成本相当。例如，在 Llama 模型中
    (Touvron et al., [2023a](#bib.bib26); [b](#bib.bib27))，当 $d_{head}\!=\!128$ 和
    $l_{n}\!=\!32$ 时，$\mathbf{Q}\mathbf{K}^{T}$ 和 Softmax 的总 IO 成本与 KV 缓存的成本匹配。'
- en: 'Remark. Though similar to DeFT, SpecInfer (Miao et al., [2023](#bib.bib20))
    also employs a fused kernel for tree attention. No IO is sharing for KV cache
    among queries in SpecInfer: instead, each query will load the entire KV cache
    of the tree independently, bringing significant IOs of the KV cache as in [Table 3](#S3.T3
    "Table 3 ‣ 3.4 Analysis: IO Complexity of DeFT ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference").'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '备注。尽管与DeFT类似，SpecInfer (Miao et al., [2023](#bib.bib20))也使用了用于树注意力的融合内核。SpecInfer中KV缓存的IO没有在查询之间共享：相反，每个查询将独立加载整个树的KV缓存，这带来了与[表3](#S3.T3
    "表3 ‣ 3.4 分析：DeFT的IO复杂性 ‣ 3 DeFT ‣ DeFT: 面向高效树搜索的闪存树注意力与IO感知")中相同的显著IO开销。'
- en: 4 Experiments
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we evaluate DeFT and other methods on tree-search-based tasks
    like reasoning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了DeFT和其他方法在基于树搜索的任务（如推理）上的表现。
- en: 4.1 Experimental Setup
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'To ensure a fair and feasible comparison, we use tree KV management illustrated
    in Section [3.2](#S3.SS2 "3.2 System Overview of DeFT ‣ 3 DeFT ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference") throughout our
    evaluations. We omit results with sequence-based KV cache management, due to the
    inefficient memory footprint and out-of-memory issue⁶⁶6 When applying sequence-based
    KV cache management (no sharing in storage for prefixes), we find sequence-based
    decoding algorithms—e.g., i) naive sequence-based attention in HuggingFace Transformers
    (Wolf et al., [2019](#bib.bib31)), and ii) original Flash-Decoding implementation—meet
    out-of-memory in the middle of decoding ($\sim 2{,}000$ iterations for workloads
    in this section) even for A100 with 80GB memory capacity. .'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '为了确保公平和可行的比较，我们在评估中使用第[3.2节](#S3.SS2 "3.2 DeFT系统概述 ‣ 3 DeFT ‣ DeFT: 面向高效树搜索的闪存树注意力与IO感知")中展示的树KV管理。我们省略了使用基于序列的KV缓存管理的结果，因为其内存开销低效且存在内存溢出问题⁶⁶6
    当应用基于序列的KV缓存管理（前缀存储不共享）时，我们发现基于序列的解码算法——例如，i) HuggingFace Transformers中的朴素序列注意力（Wolf
    et al., [2019](#bib.bib31)），以及 ii) 原始Flash-Decoding实现——在解码过程中（本节工作负载约$2{,}000$次迭代）即使在具有80GB内存容量的A100上也会遇到内存溢出。'
- en: Baselines.
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准线。
- en: 'We evaluate the performance of DeFT in NVIDIA A100 (80GB) in Llama2-7B-HF model (Touvron
    et al., [2023b](#bib.bib27)) with SOTA attention algorithms in sequence-based
    and tree-based decoding: (1) *Flash-Decoding* (Dao et al., [2023](#bib.bib6)),
    using the CUDA implementation; (2) *Tree Attention-Medusa*, where we extend it
    to the PyTorch implementation suitable for general tree attention, inspired by
    the Medusa (Cai et al., [2024](#bib.bib4)) with fixed tree masks. ⁷⁷7We didn’t
    include the tree attention operator in SpecInfer (Miao et al., [2023](#bib.bib20))
    because its kernel only support at most 64 tokens in the decoding tree. Details
    in Appendix [A.2](#A1.SS2 "A.2 Discussion of Tree-based Decoding ‣ Appendix A
    Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在NVIDIA A100 (80GB)上评估了DeFT在Llama2-7B-HF模型 (Touvron et al., [2023b](#bib.bib27))中的性能，与基于序列和基于树的解码中的SOTA注意力算法进行比较：（1）*Flash-Decoding* (Dao
    et al., [2023](#bib.bib6))，使用CUDA实现；（2）*Tree Attention-Medusa*，我们将其扩展到适用于通用树注意力的PyTorch实现，灵感来自于具有固定树掩码的Medusa (Cai
    et al., [2024](#bib.bib4))。⁷⁷7我们没有包括SpecInfer (Miao et al., [2023](#bib.bib20))中的树注意力操作符，因为其内核仅支持解码树中最多64个令牌。详情见附录[A.2](#A1.SS2
    "A.2 基于树的解码讨论 ‣ 附录 A 附录 ‣ DeFT: 面向高效树搜索的闪存树注意力与IO感知")。'
- en: '![Refer to caption](img/3ea8a7bd6c5c31c6b73ebb75616b7734.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3ea8a7bd6c5c31c6b73ebb75616b7734.png)'
- en: 'Figure 6: The detailed procedure of reconstructing tree templates. (Left) Reconstructing
    reasoning trees from practical reasoning records as outlined in Besta et al. ([2023](#bib.bib3))
    involves capturing the following aspects: (1) the structure of trees, characterized
    by their depth $d$ and width $w$; (2) the token length associated with each thought;
    and (3) the best thought at each depth along with its corresponding score. For
    the task of document merging, the tree depth is set to $d=3$, with a width of
    $w=10$ at each depth. For sorting 128 numbers, the depth is reduced to $d=10$,
    while maintaining the same width of $w=10$. (Right) Utilizing the extracted thought
    information from Left, we can generate tree templates for decoding, encompassing
    *branch records* and *prune records*. These records are instrumental in guiding
    the tree decoding process to produce decoding trees that faithfully replicate
    the structure of the tree-of-thoughts.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：重建树模板的详细过程。 （左）根据 Besta 等（[2023](#bib.bib3)）的描述，从实际推理记录中重建推理树涉及捕捉以下方面：（1）树的结构，特点是其深度
    $d$ 和宽度 $w$；（2）与每个思维相关的标记长度；（3）每个深度的最佳思维及其相应的得分。对于文档合并任务，树的深度设置为 $d=3$，每个深度的宽度为
    $w=10$。对于排序 128 个数字，深度减少为 $d=10$，同时保持相同的宽度 $w=10$。 （右）利用左侧提取的思维信息，我们可以生成用于解码的树模板，包括
    *分支记录* 和 *剪枝记录*。这些记录在指导树解码过程中非常重要，有助于生成准确复现思维树结构的解码树。
- en: '![Refer to caption](img/322e56f3e610828868b49e1bd0db1e12.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/322e56f3e610828868b49e1bd0db1e12.png)'
- en: (a) Thought token length distribution in sorting 128. $79.5\%$ of thoughts’
    KV cache length is larger than the maximum query numbers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 排序 128 中的思维标记长度分布。$79.5\%$ 的思维 KV 缓存长度大于最大查询数。
- en: '![Refer to caption](img/7044a43330458c7e37d45183e7448341.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7044a43330458c7e37d45183e7448341.png)'
- en: (b) Thought token length distribution in document merging. $100\%$ of thoughts’
    KV cache length is larger than the maximum query numbers.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 文档合并中的思维标记长度分布。$100\%$ 的思维 KV 缓存长度大于最大查询数。
- en: 'Figure 7: The distribution of thought token lengths over $100$ trees across
    various depths for two tasks. We compare the thought token length with maximum
    query numbers during decoding for all tree templates and find that the query length
    during tree decoding is significantly smaller than the thought token length of
    KV. This observation highlights that $79.5\%$ and $100\%$ of thoughts’ KV cache
    in the tasks of sorting 128 and document merging respectively, have a larger length
    than the maximum query numbers among 100 trees during tree decoding. Note that
    the reason some thoughts are smaller than the maximum query length is attributed
    to GPT3.5’s output not adhering to the prompt regulations, which may cause the
    graph-of-thought (Besta et al., [2023](#bib.bib3)) parser to inaccurately parse
    the results.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在两项任务中，跨越 $100$ 棵树的不同深度下思维标记长度的分布。我们将思维标记长度与解码过程中所有树模板的最大查询数进行比较，发现树解码期间的查询长度显著小于
    KV 的思维标记长度。这一观察表明，在排序 128 和文档合并任务中，$79.5\%$ 和 $100\%$ 的思维 KV 缓存长度分别大于在树解码过程中 100
    棵树中的最大查询数。需要注意的是，一些思维长度小于最大查询长度的原因归因于 GPT3.5 的输出未能遵循提示规范，这可能导致图形思维（Besta 等，[2023](#bib.bib3)）解析器对结果解析不准确。
- en: Workloads generation.
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 工作负载生成。
- en: 'Using the interaction records of Tree-of-Thoughts methods with GPT 3.5 Turbo
    from Besta et al. ([2023](#bib.bib3)), we reconstruct the decoding trees for two
    tasks: (1) sorting 128—sorting 128 numbers; (2) doc merging—document merging.
    We elaborate on the procedure of extracting decoding trees from interaction records
    with LLM from Besta et al. ([2023](#bib.bib3)) in [Figure 6](#S4.F6 "Figure 6
    ‣ Baselines. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference"), which could
    be tree templates⁸⁸8 The decoding trees would be forced to branch and prune in
    certain iterations to get exactly the same shape of the decoding tree as the original
    tree-of-thoughts process, to ensure fairness for workloads of different baselines.
    to guide the tree decoding process with the same function as the tree-search algorithm.
    For the sake of simplicity, we ignore the evaluation prompt template and its overhead
    to select the best thought when generating the tree of thoughts. See workload
    analysis in [Figure 7](#S4.F7 "Figure 7 ‣ Baselines. ‣ 4.1 Experimental Setup
    ‣ 4 Experiments ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference").'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Besta 等人（[2023](#bib.bib3)）与 GPT 3.5 Turbo 的 Tree-of-Thoughts 方法的交互记录，我们重构了两个任务的解码树：（1）排序
    128—对 128 个数字进行排序；（2）文档合并—文档合并。我们在 [图 6](#S4.F6 "图 6 ‣ 基线。 ‣ 4.1 实验设置 ‣ 4 实验 ‣
    DeFT：具有 IO 感知的闪存树注意力用于高效的树搜索基础 LLM 推理") 中详细阐述了从 Besta 等人（[2023](#bib.bib3)）的交互记录中提取解码树的过程，这些解码树会在某些迭代中强制分支和修剪，以确保解码树的形状与原始的
    Tree-of-Thoughts 过程完全相同，从而确保不同基线的工作负载的公平性。以相同的功能引导树解码过程，与树搜索算法相同。为了简化，我们忽略了评估提示模板及其选择最佳思路的开销。见
    [图 7](#S4.F7 "图 7 ‣ 基线。 ‣ 4.1 实验设置 ‣ 4 实验 ‣ DeFT：具有 IO 感知的闪存树注意力用于高效的树搜索基础 LLM
    推理") 中的工作负载分析。
- en: 'Table 4: Average end-to-end latency (second) and IO breakdown (TB) when decoding
    with 100 trees of two reasoning tasks from (Besta et al., [2023](#bib.bib3)):
    (i) (Left) sorting 128 numbers; (ii) (Right) document merging. IO mainly consists
    of three parts as follows. (i) KV cache: IO-KV; (ii) $QK^{T}$: IO-$QK^{T}$; (iii)
    Softmax$(QK^{T})$: IO-$Softmax$. Baselines: (i) Flash-Decoding: attention in Flash-Decoding (Dao
    et al., [2023](#bib.bib6)); (ii) Tree Attention-Medusa: causal masked tree attention
    in Medusa (Cai et al., [2024](#bib.bib4)).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：解码时平均端到端延迟（秒）和 IO 分解（TB），使用 100 棵树进行两种推理任务（Besta 等人，[2023](#bib.bib3)）：（i）（左）对
    128 个数字进行排序；（ii）（右）文档合并。IO 主要由以下三部分组成：（i）KV 缓存：IO-KV；（ii）$QK^{T}$：IO-$QK^{T}$；（iii）Softmax$(QK^{T})$：IO-$Softmax$。基线：（i）Flash-Decoding：Flash-Decoding
    中的注意力（Dao 等人，[2023](#bib.bib6)）；（ii）Tree Attention-Medusa：Medusa 中的因果掩蔽树注意力（Cai
    等人，[2024](#bib.bib4)）。
- en: '|  | Task of sorting 128 numbers | Task of document merging |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | 排序 128 个数字的任务 | 文档合并的任务 |  |'
- en: '|  | Latency | IO-KV | IO-$QK^{T}$ | IO-$Softmax$ | Latency | IO-KV | IO-$QK^{T}$
    | IO-$Softmax$ |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | 延迟 | IO-KV | IO-$QK^{T}$ | IO-$Softmax$ | 延迟 | IO-KV | IO-$QK^{T}$ | IO-$Softmax$
    |  |'
- en: '| Flash-Decoding | 548 | 35.1 | 0 | 0 | 450 | 30.7 | 0 | 0 |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Flash-Decoding | 548 | 35.1 | 0 | 0 | 450 | 30.7 | 0 | 0 |  |'
- en: '| Tree Attention-Medusa | 653 | 7.9 | 0.6 | 1.3 | 660 | 8.6 | 0.7 | 1.3 |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Tree Attention-Medusa | 653 | 7.9 | 0.6 | 1.3 | 660 | 8.6 | 0.7 | 1.3 |  |'
- en: '| DeFT(ours) | 305 | 7.9 | 0 | 0 | 272 | 8.6 | 0 | 0 |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| DeFT（我们的） | 305 | 7.9 | 0 | 0 | 272 | 8.6 | 0 | 0 |  |'
- en: '| Speed up over best baseline | $1.80\times$ | - | - | - | $1.66\times$ | -
    | - | - |  | ![Refer to caption](img/54dd33ff73a55a9a7b634d83d446a72d.png)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '| 相对于最佳基线的加速 | $1.80\times$ | - | - | - | $1.66\times$ | - | - | - |  | ![参见标题](img/54dd33ff73a55a9a7b634d83d446a72d.png)'
- en: (a) Average latency and query length when decoding with 100 trees of sorting
    128 numbers per iteration.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 解码时每次迭代使用 100 棵树对 128 个数字进行排序的平均延迟和查询长度。
- en: '![Refer to caption](img/b973efa851b6f5658f4a69e06af48433.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b973efa851b6f5658f4a69e06af48433.png)'
- en: (b) Average latency and query length when decoding with 100 trees of document
    merging per iteration.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 解码时每次迭代使用 100 棵树进行文档合并的平均延迟和查询长度。
- en: 'Figure 8: Average per iteration results for 100 trees in each reasoning task.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：每个推理任务中 100 棵树的平均每次迭代结果。
- en: 4.2 Results
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: 'End-to-end behaviors: latency and IOs.'
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 端到端行为：延迟和 IO。
- en: 'We compare DeFT with Flash-Decoding and Tree Attention-Medusa in average latency
    and IOs for tasks of sorting 128 and doc merging in [Table 4](#S4.T4 "Table 4
    ‣ Workloads generation. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash
    Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference").
    The optimization in KV cache results in a significant reduction of IOs, achieving
    a $3.6{-}4.5\times$ reduction compared to Flash-Decoding’s KV cache IO. Additionally,
    the reduction of IOs in partial results during attention calculation leads to
    a reduction of $1.9-2\text{TB}$ (equivalent to 25% of the total KV cache IO) compared
    to Tree Attention-Medusa. *These optimizations jointly contribute to a notable
    speedup of our DeFT, achieving $\mathbf{1.7{-}1.8\times}$ improvements over Flash-Decoding
    and $\mathbf{2.2{-}2.4\times}$ improvements over Tree Attention-Medusa, respectively.*
    We will discuss why the reduction of KV cache IO does not have a significant acceleration
    effect on causal masked tree attention below.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将 DeFT 与 Flash-Decoding 和 Tree Attention-Medusa 在平均延迟和 IO 上进行了比较，任务包括排序 128
    和文档合并，见[表 4](#S4.T4 "Table 4 ‣ Workloads generation. ‣ 4.1 Experimental Setup
    ‣ 4 Experiments ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference")。KV 缓存中的优化显著减少了 IO，相较于 Flash-Decoding 的 KV 缓存 IO 实现了 $3.6{-}4.5\times$
    的减少。此外，在注意力计算过程中，部分结果的 IO 减少导致了与 Tree Attention-Medusa 相比的 $1.9-2\text{TB}$（相当于总
    KV 缓存 IO 的 25%）减少。*这些优化共同促成了 DeFT 的显著加速，使其相较于 Flash-Decoding 提高了 $\mathbf{1.7{-}1.8\times}$，相较于
    Tree Attention-Medusa 提高了 $\mathbf{2.2{-}2.4\times}$。* 我们将在下面讨论为什么 KV 缓存 IO 的减少对因果遮蔽树注意力没有显著的加速效果。'
- en: 'Dynamic behaviors: latency per iteration.'
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态行为：每次迭代的延迟。
- en: 'The size of the decoding tree dynamically changes due to branch and prune operations
    among iterations. To capture this dynamic behavior, we track the latency changes
    across tree decoding iterations in [Figure 8](#S4.F8 "Figure 8 ‣ Workloads generation.
    ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference") for DeFT, Flash-Decoding, and
    Tree Attention-Medusa. Notably, Tree Attention-Medusa exhibits a strong sensitivity
    to query length: as evident in iteration $1{,}000$ of Figure [8a](#S4.F8.sf1 "In
    Figure 8 ‣ Workloads generation. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT:
    Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference"),
    the sensitivity arises as the size of the partial result is directly proportional
    to the query number. Consequently, this not only leads to increased IO but also
    results in a larger memory footprint, with the GPU’s peak memory usage reaching
    95% compared to that of 40% in DeFT. However, when the tree size is relatively
    small due to pruning, Tree Attention-Medusa can outperform Flash-Decoding, as
    observed in iteration $2{,}500$ of Figure [8a](#S4.F8.sf1 "In Figure 8 ‣ Workloads
    generation. ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference"). *DeFT significantly
    outperforms these two baselines and exhibits stable performance across a variety
    of tree sizes.*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '解码树的大小因每次迭代中的分支和修剪操作而动态变化。为了捕捉这种动态行为，我们跟踪了 DeFT、Flash-Decoding 和 Tree Attention-Medusa
    在[图 8](#S4.F8 "Figure 8 ‣ Workloads generation. ‣ 4.1 Experimental Setup ‣ 4 Experiments
    ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference")中的树解码迭代的延迟变化。值得注意的是，Tree Attention-Medusa 对查询长度具有较强的敏感性：如[图 8a](#S4.F8.sf1
    "In Figure 8 ‣ Workloads generation. ‣ 4.1 Experimental Setup ‣ 4 Experiments
    ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference")中的迭代 $1{,}000$ 所示，这种敏感性出现是因为部分结果的大小与查询数量直接成正比。因此，这不仅导致了 IO 增加，还导致了更大的内存占用，GPU
    的峰值内存使用率达到了 95%，而 DeFT 的峰值内存使用率为 40%。然而，当树的大小由于修剪而相对较小时，Tree Attention-Medusa
    可以超越 Flash-Decoding，正如[图 8a](#S4.F8.sf1 "In Figure 8 ‣ Workloads generation. ‣
    4.1 Experimental Setup ‣ 4 Experiments ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference")中的迭代 $2{,}500$ 所观察到的那样。*DeFT 显著优于这两个基线，并且在各种树大小下表现稳定。*'
- en: 5 Discussion
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: In conclusion, we have proposed DeFT, a novel IO-aware tree attention algorithm
    to accelerate large language models combined with tree search algorithms. DeFT
    can aware topology of decoding tree to ease the great IO of KV cache, and a fused
    kernel is adopted to eliminate the IO of partial results during attention calculations.
    We have demonstrated in two practical reasoning tasks, DeFT can obtain $\mathbf{1.7{-}2.4\times}$
    speedup thanks to $3.6{-}4.5\times$ reduction of KV cache IO and $1.9-2\text{TB}$
    reduction (equivalent to 25% of the total KV cache IO) of $\mathbf{Q}\mathbf{K}^{\top}$
    and Softmax.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们提出了 DeFT，这是一种新颖的 IO 感知树注意力算法，用于加速结合树搜索算法的大型语言模型。DeFT 能够感知解码树的拓扑结构，以缓解 KV
    缓存的大量 IO，并采用融合内核以消除注意力计算过程中的部分结果 IO。我们在两个实际推理任务中演示了 DeFT 的效果，得益于 KV 缓存 IO 降低了
    $3.6{-}4.5\times$，以及 $\mathbf{Q}\mathbf{K}^{\top}$ 和 Softmax 的 IO 降低了 $1.9-2\text{TB}$（相当于
    KV 缓存 IO 总量的 25%），DeFT 可获得 $\mathbf{1.7{-}2.4\times$ 的加速。
- en: Key advantages of DeFT are IO-aware and simple, as no tree attention mask is
    required during the decoding. DeFT is also not sensitive to query numbers of the
    tree, which shows great potential to support a large search space with multiple
    branches.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: DeFT 的主要优势是对 IO 感知且简单，因为在解码过程中不需要树注意力掩码。DeFT 对树的查询数量也不敏感，这显示了支持具有多个分支的大搜索空间的巨大潜力。
- en: References
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等人。Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*，2023。
- en: 'Anderson et al. (2017) Peter Anderson, Basura Fernando, Mark Johnson, and Stephen
    Gould. Guided open vocabulary image captioning with constrained beam search. In
    Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), *Proceedings of the 2017
    Conference on Empirical Methods in Natural Language Processing*, pp.  936–945,
    Copenhagen, Denmark, September 2017. Association for Computational Linguistics.
    doi: 10.18653/v1/D17-1098. URL [https://aclanthology.org/D17-1098](https://aclanthology.org/D17-1098).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Anderson 等（2017）Peter Anderson, Basura Fernando, Mark Johnson 和 Stephen Gould。受限束搜索的引导开放词汇图像字幕生成。收录于
    Martha Palmer, Rebecca Hwa 和 Sebastian Riedel（编辑），*2017年自然语言处理实证方法会议论文集*，第936–945页，丹麦哥本哈根，2017年9月。计算语言学协会。doi:
    10.18653/v1/D17-1098。网址 [https://aclanthology.org/D17-1098](https://aclanthology.org/D17-1098)。'
- en: 'Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large
    language models. *arXiv preprint arXiv:2308.09687*, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 等（2023）Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas
    Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk 等人。思维图：使用大型语言模型解决复杂问题。*arXiv 预印本 arXiv:2308.09687*，2023。
- en: 'Cai et al. (2024) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D
    Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework
    with multiple decoding heads. *arXiv preprint arXiv:2401.10774*, 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等（2024）Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee,
    Deming Chen 和 Tri Dao。Medusa：具有多个解码头的简单 LLM 推理加速框架。*arXiv 预印本 arXiv:2401.10774*，2024。
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等（2022）Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra 和 Christopher Ré。Flashattention：快速且内存高效的确切注意力，具有
    IO 感知。*神经信息处理系统进展*，35:16344–16359，2022。
- en: Dao et al. (2023) Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov.
    Flash-decoding for long-context inference, 2023. URL [https://pytorch.org/blog/flash-decoding/](https://pytorch.org/blog/flash-decoding/).
    PyTorch Blog.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等（2023）Tri Dao, Daniel Haziza, Francisco Massa 和 Grigory Sizov。长上下文推理的 Flash-decoding，2023。网址
    [https://pytorch.org/blog/flash-decoding/](https://pytorch.org/blog/flash-decoding/)。PyTorch
    博客。
- en: 'Dathathri et al. (2019) Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
    Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play
    language models: A simple approach to controlled text generation. In *International
    Conference on Learning Representations*, 2019.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dathathri 等（2019）Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric
    Frank, Piero Molino, Jason Yosinski 和 Rosanne Liu。即插即用语言模型：一种控制文本生成的简单方法。收录于 *国际学习表征会议*，2019。
- en: Graves (2012) Alex Graves. Sequence transduction with recurrent neural networks.
    *arXiv preprint arXiv:1211.3711*, 2012.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves (2012) Alex Graves. 使用递归神经网络进行序列转导。*arXiv 预印本 arXiv:1211.3711*，2012。
- en: 'Hokamp & Liu (2017) Chris Hokamp and Qun Liu. Lexically constrained decoding
    for sequence generation using grid beam search. In Regina Barzilay and Min-Yen
    Kan (eds.), *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  1535–1546, Vancouver, Canada, July
    2017\. Association for Computational Linguistics. doi: 10.18653/v1/P17-1141. URL
    [https://aclanthology.org/P17-1141](https://aclanthology.org/P17-1141).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hokamp & Liu (2017) Chris Hokamp 和 Qun Liu. 使用网格束搜索的词汇约束解码进行序列生成。在 Regina Barzilay
    和 Min-Yen Kan (编)，*第55届计算语言学协会年会论文集（第1卷：长篇论文）*，第1535–1546页，加拿大温哥华，2017年7月。计算语言学协会。doi:
    10.18653/v1/P17-1141。网址 [https://aclanthology.org/P17-1141](https://aclanthology.org/P17-1141)。'
- en: 'Holtzman et al. (2018) Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut,
    David Golub, and Yejin Choi. Learning to write with cooperative discriminators.
    In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  1638–1649, 2018.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman et al. (2018) Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut,
    David Golub, 和 Yejin Choi. 学习通过合作鉴别器进行写作。在*第56届计算语言学协会年会论文集（第1卷：长篇论文）*，第1638–1649页，2018年。
- en: 'Hong et al. (2023) Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li,
    Jun Liu, Kangdi Chen, Hanyu Dong, and Yu Wang. Flashdecoding++: Faster large language
    model inference on gpus. *arXiv preprint arXiv:2311.01282*, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong et al. (2023) Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun
    Liu, Kangdi Chen, Hanyu Dong, 和 Yu Wang. Flashdecoding++：在 GPUs 上更快的大型语言模型推理。*arXiv
    预印本 arXiv:2311.01282*，2023。
- en: Jia & Van Sandt (2021) Zhe Jia and Peter Van Sandt. Dissecting the ampere gpu
    architecture via microbenchmarking. In *GPU Technology Conference*, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia & Van Sandt (2021) Zhe Jia 和 Peter Van Sandt. 通过微基准测试解剖安培 GPU 架构。在*GPU 技术大会*，2021。
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, 和 Kurt Keutzer. Squeezellm: 密集与稀疏量化。*arXiv
    预印本 arXiv:2306.07629*，2023。'
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. *arXiv preprint
    arXiv:2309.06180*, 2023.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, 和 Ion Stoica. 大型语言模型服务的高效内存管理与分页注意力。*arXiv
    预印本 arXiv:2309.06180*，2023。
- en: 'Liu et al. (2023) Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi,
    Hannaneh Hajishirzi, and Asli Celikyilmaz. Making ppo even better: Value-guided
    monte-carlo tree search decoding. *arXiv preprint arXiv:2309.15028*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi,
    Hannaneh Hajishirzi, 和 Asli Celikyilmaz. 使 PPO 更加出色：值指导的蒙特卡洛树搜索解码。*arXiv 预印本 arXiv:2309.15028*，2023。
- en: 'Liu et al. (2024) Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang,
    and Yuxiao Dong. Apar: Llms can do auto-parallel auto-regressive decoding. *arXiv
    preprint arXiv:2401.06761*, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2024) Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang,
    和 Yuxiao Dong. Apar: Llms 可以进行自动并行自回归解码。*arXiv 预印本 arXiv:2401.06761*，2024。'
- en: 'Lu et al. (2021) Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra
    Bhagavatula, and Yejin Choi. Neurologic decoding:(un) supervised neural text generation
    with predicate logic constraints. In *Proceedings of the 2021 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies*, pp.  4288–4299, 2021.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2021) Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra
    Bhagavatula, 和 Yejin Choi. 神经解码：（非）监督神经文本生成与谓词逻辑约束。在*2021年北美计算语言学协会年会：人类语言技术会议论文集*，第4288–4299页，2021年。
- en: 'Lu et al. (2022) Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai,
    Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, et al.
    Neurologic a* esque decoding: Constrained text generation with lookahead heuristics.
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pp.  780–799, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2022) Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai,
    Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers，等。神经解码：具有前瞻启发式的约束文本生成。在*2022年北美计算语言学协会年会：人类语言技术会议论文集*，第780–799页，2022年。
- en: Mark et al. (2021) Chen Mark, Tworek Jerry, Jun Heewoo, Yuan Qiming, Pinto Henrique Ponde
    de Oliveira, Kaplan Jared, Edwards Harrison, Burda Yuri, Joseph Nicholas, Brockman
    Greg, et al. Carr andrew n. *Leike Jan, Achiam Joshua, Misra Vedant, Morikawa
    Evan, Radford Alec, Knight Matthew, Brundage Miles, Murati Mira, Mayer Katie,
    Welinder Peter, McGrew Bob, Amodei Dario, McCandlish Sam, Sutskever Ilya, and
    Zaremba Wojciech*, 2021.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mark 等人 (2021) **陈·马克**、**特沃雷克·杰瑞**、**郑·希沃**、**袁·启明**、**品托·亨里克·庞德·德·奥利维拉**、**卡普兰·贾里德**、**爱德华兹·哈里森**、**布尔达·尤里**、**约瑟夫·尼古拉斯**、**布罗克曼·格雷格**
    等人。**莱克·简**、**阿基姆·约书亚**、**米斯拉·维丹特**、**森川·埃文**、**拉德福德·亚历克**、**奈特·马修**、**布伦达奇·迈尔斯**、**穆拉提·米拉**、**迈耶·凯蒂**、**维林德·彼得**、**麦格鲁·鲍勃**、**阿莫迪·达里奥**、**麦坎德利什·萨姆**、**苏茨克弗·伊利亚**
    和 **扎伦巴·沃伊切赫**，2021年。
- en: 'Miao et al. (2023) Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,
    Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar,
    and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative
    inference and token tree verification. *arXiv preprint arXiv:2305.09781*, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miao 等人 (2023) **肖鹏·苗**、**加布里埃尔·奥利亚罗**、**志豪·张**、**欣浩·程**、**泽宇·王**、**瑞·英·易·黄**、**周梦辰**、**戴安·阿尔芬**、**雷娜·阿比扬卡**
    和 **志豪·贾**。Specinfer: 利用推测推理和标记树验证加速生成式 LLM 服务。*arXiv 预印本 arXiv:2305.09781*，2023年。'
- en: 'Ning et al. (2023) Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and
    Yu Wang. Skeleton-of-thought: Large language models can do parallel decoding.
    *arXiv preprint arXiv:2307.15337*, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ning 等人 (2023) **徐飞 宁**、**林自南**、**周子轩**、**杨华中** 和 **王宇**。Skeleton-of-thought:
    大型语言模型可以进行并行解码。*arXiv 预印本 arXiv:2307.15337*，2023年。'
- en: 'Post & Vilar (2018) Matt Post and David Vilar. Fast lexically constrained decoding
    with dynamic beam allocation for neural machine translation. In Marilyn Walker,
    Heng Ji, and Amanda Stent (eds.), *Proceedings of the 2018 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long Papers)*, pp.  1314–1324, New Orleans, Louisiana,
    June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1119.
    URL [https://aclanthology.org/N18-1119](https://aclanthology.org/N18-1119).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Post & Vilar (2018) **马特·波斯特** 和 **大卫·维拉尔**。具有动态光束分配的快速词汇约束解码用于神经机器翻译。在 **玛丽莲·沃克**、**亨·季**
    和 **阿曼达·斯滕**（编），*2018年北美计算语言学协会年会论文集：人类语言技术，第1卷（长篇论文）*，第1314–1324页，路易斯安那州新奥尔良，2018年6月。计算语言学协会。doi:
    10.18653/v1/N18-1119。网址 [https://aclanthology.org/N18-1119](https://aclanthology.org/N18-1119)。'
- en: Roller et al. (2020) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson,
    Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for
    building an open-domain chatbot. *arXiv preprint arXiv:2004.13637*, 2020.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roller 等人 (2020) **斯蒂芬·罗勒**、**艾米莉·迪南**、**纳曼·戈亚尔**、**大·朱**、**玛丽·威廉姆森**、**尹汉·刘**、**晶·许**、**迈尔·奥特**、**库尔特·舒斯特**、**埃里克·M·史密斯**
    等人。构建开放域聊天机器人的食谱。*arXiv 预印本 arXiv:2004.13637*，2020年。
- en: 'Shazeer (2019) Noam Shazeer. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shazeer (2019) **诺姆·沙泽尔**。快速变压器解码: 一只写头就够了。*arXiv 预印本 arXiv:1911.02150*，2019年。'
- en: 'Tillet et al. (2019) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton:
    an intermediate language and compiler for tiled neural network computations. In
    *Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning
    and Programming Languages*, pp.  10–19, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tillet 等人 (2019) **菲利普·蒂莱**、**匡向宗** 和 **大卫·考克斯**。Triton: 一种用于平铺神经网络计算的中间语言和编译器。在
    *第三届 ACM SIGPLAN 国际机器学习与编程语言研讨会论文集*，第10–19页，2019年。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023a) **雨果·图弗龙**、**提博·拉夫里尔**、**戈蒂埃·伊扎卡德**、**泽维尔·马蒂内特**、**玛丽-安·拉肖**、**蒂莫泰·拉克鲁瓦**、**巴普蒂斯特·罗齐埃尔**、**纳曼·戈亚尔**、**埃里克·汉布罗**、**法伊萨尔·阿扎尔**
    等人。Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年a。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023b) **雨果·图弗龙**、**路易斯·马丁**、**凯文·斯通**、**彼得·阿尔伯特**、**阿姆贾德·阿尔马海里**、**雅斯敏·巴巴伊**、**尼古拉·巴什利科夫**、**索姆雅·巴特拉**、**普拉吉瓦尔·巴尔戈瓦**、**舒提·博萨尔**
    等人。Llama 2: 开放的基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023年b。'
- en: Uesato et al. (2022) Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song,
    Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.
    Solving math word problems with process-and outcome-based feedback. *arXiv preprint
    arXiv:2211.14275*, 2022.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uesato 等人 (2022) **乔纳森·乌萨托**、**内特·库什曼**、**拉马纳·库马尔**、**弗朗西斯·宋**、**诺亚·西格尔**、**丽莎·王**、**安东尼亚·克雷斯韦尔**、**杰弗里·欧文**
    和 **伊琳娜·希金斯**。通过过程和结果反馈解决数学文字问题。*arXiv 预印本 arXiv:2211.14275*，2022年。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2022）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed Chi、Quoc
    V Le、Denny Zhou 等人。链式思维提示激发大语言模型的推理。*神经信息处理系统进展*，35:24824–24837，2022年。
- en: 'Welleck et al. (2022) Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi,
    and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language
    models. *Advances in Neural Information Processing Systems*, 35:4913–4927, 2022.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck 等人（2022）Sean Welleck、Jiacheng Liu、Ximing Lu、Hannaneh Hajishirzi 和 Yejin
    Choi。Naturalprover：基于语言模型的实证数学证明生成。*神经信息处理系统进展*，35:4913–4927，2022年。
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*, 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等人（2019）Thomas Wolf、Lysandre Debut、Victor Sanh、Julien Chaumond、Clement
    Delangue、Anthony Moi、Pierric Cistac、Tim Rault、Rémi Louf、Morgan Funtowicz 等人。Huggingface
    的 transformers：最先进的自然语言处理。*arXiv 预印本 arXiv:1910.03771*，2019年。
- en: Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan,
    Junxian He, and Qizhe Xie. Self-evaluation guided beam search for reasoning. In
    *Thirty-seventh Conference on Neural Information Processing Systems*, 2023. URL
    [https://openreview.net/forum?id=Bw82hwg5Q3](https://openreview.net/forum?id=Bw82hwg5Q3).
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人（2023）Yuxi Xie、Kenji Kawaguchi、Yiran Zhao、Xu Zhao、Min-Yen Kan、Junxian
    He 和 Qizhe Xie。自我评估指导的束搜索推理。发表于 *第三十七届神经信息处理系统会议*，2023年。网址 [https://openreview.net/forum?id=Bw82hwg5Q3](https://openreview.net/forum?id=Bw82hwg5Q3)。
- en: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. *arXiv preprint arXiv:2305.10601*, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人（2023）Shunyu Yao、Dian Yu、Jeffrey Zhao、Izhak Shafran、Thomas L Griffiths、Yuan
    Cao 和 Karthik Narasimhan。思维树：与大语言模型进行深思熟虑的问题解决。*arXiv 预印本 arXiv:2305.10601*，2023年。
- en: 'Zhao et al. (2023) Yao Zhao, Zhitian Xie, Chenyi Zhuang, and Jinjie Gu. Lookahead:
    An inference acceleration framework for large language model with lossless generation
    accuracy. *arXiv preprint arXiv:2312.12728*, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2023）Yao Zhao、Zhitian Xie、Chenyi Zhuang 和 Jinjie Gu。Lookahead：一种用于大语言模型的无损生成精度的推理加速框架。*arXiv
    预印本 arXiv:2312.12728*，2023年。
- en: Zheng et al. (2023) Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang,
    Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez,
    et al. Efficiently programming large language models using sglang. *arXiv preprint
    arXiv:2312.07104*, 2023.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2023）Lianmin Zheng、Liangsheng Yin、Zhiqiang Xie、Jeff Huang、Chuyue Sun、Cody
    Hao Yu、Shiyi Cao、Christos Kozyrakis、Ion Stoica、Joseph E Gonzalez 等人。使用 sglang
    高效编程大语言模型。*arXiv 预印本 arXiv:2312.07104*，2023年。
- en: Appendix A Appendix
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Components of system support for DeFT
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 DeFT 系统支持的组件
- en: 'The details of functions for system components of DeFT are as below:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: DeFT 系统组件功能的详细信息如下：
- en: '1.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Branch Controller: It makes the tree decoding process forced by a user-defined
    function (e.g. branch to two children every $3$ iterations, as the example shown
    in the right of [Figure 2](#S3.F2 "Figure 2 ‣ Motivation for DeFT. ‣ 3.1 Preliminary
    ‣ 3 DeFT ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based
    LLM Inference")). Tree-search-based algorithms can be applied here using the decoding
    tree’s topology information.'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '分支控制器：它使树解码过程受到用户定义函数的强制（例如，每 $3$ 次迭代分支到两个子节点，如 [图 2](#S3.F2 "图 2 ‣ DeFT 动机
    ‣ 3.1 初步 ‣ 3 DeFT ‣ DeFT: 面向 IO 认知的高效树搜索 LLM 推理") 右侧示例所示）。在这里可以应用基于树解码的算法，使用解码树的拓扑信息。'
- en: '2.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Sequence Tree Manager: It maintains the topology of the decoding tree based
    on the tree operations and tokens from the Branch Controller. The tree operations
    like pruning and branching will be executed by Tree Handler in this component.
    Branch Result Storage will record token generation results of all branches in
    the decoding tree, and output when the decoding stops.'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 序列树管理器：它根据来自分支控制器的树操作和令牌维护解码树的拓扑结构。像剪枝和分支这样的树操作将在这个组件中由树处理器执行。分支结果存储将记录解码树中所有分支的令牌生成结果，并在解码停止时输出。
- en: '3.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'KV cache Manager: It will maintain KV cache with a tree structure. A map between
    sequence IDs in the decoding tree and KV cache index is kept, which will be updated
    based on KV operations⁹⁹9 e.g. when a node is pruned in the decoding tree, its
    KV space will be evicted using a Remove operation. from the Sequence Tree Manager.'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: KV 缓存管理器：它将维护具有树结构的 KV 缓存。一个序列 ID 与 KV 缓存索引之间的映射被保持，这将根据 KV 操作进行更新⁹⁹9 例如，当解码树中的一个节点被剪枝时，它的
    KV 空间将通过移除操作从序列树管理器中逐出。
- en: '4.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Model Interface: pass input metadata to DeFT Attention kernel and MLP module,
    then return logits and memory pointers of updated KV cache.'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型接口：将输入元数据传递给 DeFT 注意力内核和 MLP 模块，然后返回 logits 和更新后的 KV 缓存的内存指针。
- en: A.2 Discussion of Tree-based Decoding
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 基于树的解码讨论
- en: 'Tree-based decoding could have tree-structured KV cache for storage with awareness
    of shared prefixes (Zheng et al., [2023](#bib.bib35)), or tree-structured queries
    in parallel decoding (Miao et al., [2023](#bib.bib20); Cai et al., [2024](#bib.bib4)),
    as shown in [Figure 9](#A1.F9 "Figure 9 ‣ A.2 Discussion of Tree-based Decoding
    ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness for Efficient
    Tree-search-based LLM Inference").'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '基于树的解码可以拥有树状结构的 KV 缓存来存储共享前缀（Zheng 等， [2023](#bib.bib35)），或在并行解码中使用树状结构的查询（Miao
    等， [2023](#bib.bib20)；Cai 等， [2024](#bib.bib4)），如[图 9](#A1.F9 "图 9 ‣ A.2 基于树的解码讨论
    ‣ 附录 A 附录 ‣ DeFT: 具有 IO 感知的闪电树注意力用于高效树搜索 LLM 推理")所示。'
- en: 'In SpecInfer(Miao et al., [2023](#bib.bib20)), as shown in Figure [9b](#A1.F9.sf2
    "9b ‣ Figure 9 ‣ A.2 Discussion of Tree-based Decoding ‣ Appendix A Appendix ‣
    DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM
    Inference"), a bit mask is utilized to record the causal information among queries
    of a token tree. Each token $t_{i}$ in queries will have a 64-bit Int as a bit
    mask, where j-th bit means the causal relationship between query of $t_{i}$ and
    KV cache of $t_{j}$. The advantage of this mask design is that it greatly reduces
    IO, but it results in the maximum number of tree tokens being only 64, which is
    not practical for scenarios with tree-structured KV cache.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '在 SpecInfer（Miao 等， [2023](#bib.bib20)）中，如图 [9b](#A1.F9.sf2 "9b ‣ 图 9 ‣ A.2
    基于树的解码讨论 ‣ 附录 A 附录 ‣ DeFT: 具有 IO 感知的闪电树注意力用于高效树搜索 LLM 推理") 所示，位掩码被用来记录令牌树中查询之间的因果信息。查询中的每个令牌
    $t_{i}$ 将有一个 64 位的 Int 作为位掩码，其中第 j 位表示 $t_{i}$ 的查询与 $t_{j}$ 的 KV 缓存之间的因果关系。此掩码设计的优点是极大地减少了
    IO，但结果是树状令牌的最大数量仅为 64，对于具有树状结构 KV 缓存的场景并不实用。'
- en: A general decoding could both do with tree KV and tree queries, which could
    reduce redundancy (e.g. IO, storage, computation, etc) of shared prefixes, as
    well as increase the generated tokens per decoding iteration.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的解码可以同时使用树 KV 和树查询，这可以减少共享前缀的冗余（例如 IO、存储、计算等），并且增加每次解码迭代生成的令牌数量。
- en: '![Refer to caption](img/7f8a319cf56d91f50b949fca55bf758f.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7f8a319cf56d91f50b949fca55bf758f.png)'
- en: '(a) (Left) Sequence KV with queries in a tree for parallel decoding (Miao et al.,
    [2023](#bib.bib20)), where a bit mask in Figure [9b](#A1.F9.sf2 "In Figure 9 ‣
    A.2 Discussion of Tree-based Decoding ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention
    with IO-Awareness for Efficient Tree-search-based LLM Inference") is applied to
    record the causal information among queries in a tree of tokens. (Right) Tree
    KV with parallel queries for shared prefixes in DeFT.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '(a) (左) 带有树状结构查询的序列 KV 用于并行解码（Miao 等， [2023](#bib.bib20)），其中图 [9b](#A1.F9.sf2
    "在图 9 ‣ A.2 基于树的解码讨论 ‣ 附录 A 附录 ‣ DeFT: 具有 IO 感知的闪电树注意力用于高效树搜索 LLM 推理") 中的位掩码用于记录令牌树中查询之间的因果信息。
    (右) DeFT 中用于共享前缀的并行查询的树 KV。'
- en: '![Refer to caption](img/e4f73ba0e6a206089f38dd755b410e99.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e4f73ba0e6a206089f38dd755b410e99.png)'
- en: (b) Bit Mask in SpecInfer (Miao et al., [2023](#bib.bib20))to record the causal
    information between query tokens in a tree structure.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: (b) SpecInfer 中的位掩码（Miao 等， [2023](#bib.bib20)）用于记录树结构中查询令牌之间的因果信息。
- en: 'Figure 9: Discussion of tree-based decoding with tree queries (Miao et al.,
    [2023](#bib.bib20)) and tree KV.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：讨论树状查询（Miao 等， [2023](#bib.bib20)）和树 KV 的基于树的解码。
- en: A.3 DeFT-Node Algorithm
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 DeFT-Node 算法
- en: 'Algorithm 1 DeFT-Node Algorithm-Phase 1: QKV Preparation.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 DeFT-Node 算法-阶段 1：QKV 准备。
- en: 'Input: query $Q\in R^{(b_{q},d)}$, Key cache list $KL=(K_{0},...K_{N-1})$,
    Value cache list $VL=(V_{0},...V_{N-1})$ for each sequence node in the tree, where
    $N$ is the total number of sequences in a tree, and Tree $T$ with its topology
    information.  for each $q$ in $Q$ with its global index $idx$ do     /*Get KV
    indices of all prefixes’ for a query.*/     $QMapKV[idx]$=GetPrefixKVIndices($q,KL,VL,T$)  end for  for each
    seq’s KV cache $K_{i},V_{i}$ in $KL,VL$ with its KV indice $i$ do     /*Group
    each sequence’s KV with all queries that share it.*/     $Q_{i}$= GroupQueryToKV($Q,K_{i},V_{i},T$)
    $\in R^{b_{i},d}\subset Q$     $KVMapQ[i]=Q_{i}$  end for  Return QMapKV, KVMapQ'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：查询 $Q\in R^{(b_{q},d)}$，键缓存列表 $KL=(K_{0},...K_{N-1})$，值缓存列表 $VL=(V_{0},...V_{N-1})$，适用于树中的每个序列节点，其中
    $N$ 是树中的序列总数，以及具有拓扑信息的树 $T$。对 $Q$ 中的每个 $q$ 及其全局索引 $idx$ 执行     /*获取所有前缀的KV索引。*/     $QMapKV[idx]$=GetPrefixKVIndices($q,KL,VL,T$)   结束对每个序列的KV缓存
    $K_{i},V_{i}$ 在 $KL,VL$ 中，其KV索引 $i$ 执行     /*将每个序列的KV与共享的所有查询分组。*/     $Q_{i}$=
    GroupQueryToKV($Q,K_{i},V_{i},T$) $\in R^{b_{i},d}\subset Q$     $KVMapQ[i]=Q_{i}$   结束返回
    QMapKV, KVMapQ
- en: DeFT-Node^(10)^(10)10If there is no special explanation of the DeFT Attention
    algorithm in the text, it refers to DeFT-Node Algorithm. has two phases-Phase
    1-QKV Preparation and Phase 2-Attention Calculation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: DeFT-Node^(10)^(10)10如果文本中没有特别说明DeFT注意力算法，则指的是DeFT-Node算法。包含两个阶段——第1阶段-QKV准备和第2阶段-注意力计算。
- en: Phase 2-Attention Calculation of DeFT has two stages.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: DeFT的Phase 2-注意力计算分为两个阶段。
- en: '1.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Stage 1: Calculate Partial Attentions. We will apply Flash Attention of all
    QKV groups obtained after Phase 1-QKV Preparation of DeFT, to get partial attention
    and LogSumExp.'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第1阶段：计算部分注意力。我们将应用在DeFT的Phase 1-QKV准备阶段获得的所有QKV组的Flash Attention，以获取部分注意力和LogSumExp。
- en: '2.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Stage 2: Global Reduction. We will remap partial attention and LogSumExp based
    on each query, and get final attention based on global reduction similar to Flash-Decoding
    (Dao et al., [2023](#bib.bib6)).'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第2阶段：全局缩减。我们将基于每个查询重新映射部分注意力和LogSumExp，并基于类似于Flash-Decoding (Dao等，[2023](#bib.bib6))的全局缩减获取最终注意力。
- en: 'Algorithm 2 DeFT-Node Algorithm-Phase 2: Attention Calculation.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2 DeFT-Node算法-第2阶段：注意力计算。
- en: 'Input: query $Q\in R^{(b_{q},d)}$, Key cache list $KL=(K_{0},...K_{N-1})$,
    Value cache list $VL=(V_{0},...V_{N-1})$ for each sequence node in the tree, where
    $N$ is the total number of sequences in a tree, and Tree $T$ with its topology
    information. QKV group information $QMapKV$, $KVMapQ$ from QKV Preparation Phase.  for each
    $q$ in $Q$ with its global index $idx$ do     /*Allocate to store LogSumExp of
    $Q@K^{T}$ grouped by query.*/     $LogSumExp[idx]=\{\}$     /*Allocate to store
    partial results of $SoftMax(Q@K^{T})V$ for each query.*/     $O[idx]=\{\}$  end for  /*Allocate
    space for output after reduction.*/  $FO=(0)_{b_{q}\times d}\in R^{(b_{q},d)}$  for each
    seq’s KV cache $K_{i},V_{i}\in R^{(b_{kv},d)},R^{(b_{kv},d)}$ in $KL,VL$ with
    its KV indice $i$ do     # Unroll for loop to SMs     $Q_{i}$= $KVMapQ[i]\in R^{(b_{i},d)}$     /*Get
    partial attention $o_{i}$ for each QKV group, LogSumExp $lse_{i}$ of $Q@K^{T}$
    in row for reduction.*/     $o_{i},lse_{i}$ = FlashAttention($Q_{i},K_{i},V_{i}$)     $\in
    R^{(b_{i},d)},R^{b_{i}}$     /*Map the partial results back to each query for
    reduction.*/     for each query $q$ in $Q_{i}$ with its group index $gp\_idx$
    and global index $idx$ in $Q$ do        if $i\in QMapKV[idx]$ then           $LogSumExp[idx].append(lse_{i}[gp\_idx])$        end if     end for  end for  for each
    $q$ in $Q$ with its global index $idx$ do     # Unroll for loop to SMs     if len($O[idx]$)==len($QMapKV[idx]$) then        /*Global
    reduction after collecting all partial results from QKV groups that contains $q$.*/        $LSE_{cat}$=
    CatTensor($LogSumExp[idx]$)        $LSE_{max}$=RowMax($LSE_{cat}$)        $Mid\_L=0,Mid\_O=0^{(1,d)}$        for each
    $lse_{j}$ in $LogSumExp[idx]$ do           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_L=Mid\_L+new\_exp$        end for        for each
    $lse_{j},o_{j}$ in $LogSumExp[idx],O[idx]$ do           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_O=Mid\_O+new\_exp@o_{j}/Mid\_L$        end for        $FO[idx]=Mid\_O$     end if  end for  Return
    $FO$'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：查询 $Q\in R^{(b_{q},d)}$，每个序列节点的键缓存列表 $KL=(K_{0},...K_{N-1})$，值缓存列表 $VL=(V_{0},...V_{N-1})$，其中
    $N$ 是树中序列的总数，以及具有拓扑信息的树 $T$。QKV 组信息 $QMapKV$、$KVMapQ$ 来自 QKV 准备阶段。  对每个 $q$ 在
    $Q$ 中及其全局索引 $idx$ 执行     /*分配以存储按查询分组的 $Q@K^{T}$ 的 LogSumExp。*/     $LogSumExp[idx]=\{\}$     /*分配以存储每个查询的
    $SoftMax(Q@K^{T})V$ 的部分结果。*/     $O[idx]=\{\}$  结束 对每个序列的 KV 缓存 $K_{i},V_{i}\in
    R^{(b_{kv},d)},R^{(b_{kv},d)}$ 在 $KL,VL$ 中及其 KV 索引 $i$ 执行     # 展开循环到 SMs     $Q_{i}$=
    $KVMapQ[i]\in R^{(b_{i},d)}$     /*获取每个 QKV 组的部分注意力 $o_{i}$，按行进行 $Q@K^{T}$ 的 LogSumExp
    $lse_{i}$ 以进行归约。*/     $o_{i},lse_{i}$ = FlashAttention($Q_{i},K_{i},V_{i}$)     $\in
    R^{(b_{i},d)},R^{b_{i}}$     /*将部分结果映射回每个查询以进行归约。*/     对每个查询 $q$ 在 $Q_{i}$ 中及其组索引
    $gp\_idx$ 和全局索引 $idx$ 在 $Q$ 中执行        如果 $i\in QMapKV[idx]$ 则           $LogSumExp[idx].append(lse_{i}[gp\_idx])$        结束 如果     结束 结束 对每个
    $q$ 在 $Q$ 中及其全局索引 $idx$ 执行     # 展开循环到 SMs     如果 len($O[idx]$)==len($QMapKV[idx]$) 则        /*在收集所有包含
    $q$ 的 QKV 组的部分结果后进行全局归约。*/        $LSE_{cat}$= CatTensor($LogSumExp[idx]$)        $LSE_{max}$=RowMax($LSE_{cat}$)        $Mid\_L=0,Mid\_O=0^{(1,d)}$        对每个
    $lse_{j}$ 在 $LogSumExp[idx]$ 中执行           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_L=Mid\_L+new\_exp$        结束 对每个
    $lse_{j},o_{j}$ 在 $LogSumExp[idx],O[idx]$ 中执行           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_O=Mid\_O+new\_exp@o_{j}/Mid\_L$        结束        $FO[idx]=Mid\_O$     结束 如果 结束 返回
    $FO$
- en: A.4 DeFT-Subtree Algorithm
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 DeFT-Subtree 算法
- en: 'The algorithm (noted as DeFT-Node) in Appendix [A.3](#A1.SS3 "A.3 DeFT-Node
    Algorithm ‣ Appendix A Appendix ‣ DeFT: Flash Tree-attention with IO-Awareness
    for Efficient Tree-search-based LLM Inference") adopts a node-granularity split
    strategy, which is quite simple. However, when the token lengths of different
    nodes in a decoding tree are very unbalanced, it might introduce inefficient calculation
    due to the unbalanced workload in on-chip SMs of GPUs.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '附录[A.3](#A1.SS3 "A.3 DeFT-Node 算法 ‣ 附录 A 附录 ‣ DeFT: Flash Tree-attention with
    IO-Awareness for Efficient Tree-search-based LLM Inference")中记作 DeFT-Node 的算法采用了节点粒度的拆分策略，这相当简单。然而，当解码树中不同节点的令牌长度非常不均衡时，由于
    GPU 的片上 SMs 中的工作负载不均衡，这可能引入低效的计算。'
- en: Therefore, we can split the decoding tree in a more balanced way– in subtree-granularity.
    We show the DeFT-Subtree algorithm as follows, which also consists of two stages
    similar to DeFT-Node^(11)^(11)11We are testing DeFT-Subtree Algorithm and will
    add performance comparison in subsequent versions..
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以以更均衡的方式拆分解码树——以子树粒度的方式。我们展示了如下的 DeFT-Subtree 算法，它也包括两个阶段，类似于 DeFT-Node^(11)^(11)11我们正在测试
    DeFT-Subtree 算法，并将在后续版本中添加性能比较。。
- en: 'Algorithm 3 DeFT-Subtree Algorithm-Phase 1: QKV Preparation.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 DeFT-Subtree 算法-阶段 1：QKV 准备。
- en: 'Input: query $Q\in R^{(b_{q},d)}$, Key cache list $KL=(K_{0},...K_{N-1})$,
    Value cache list $VL=(V_{0},...V_{N-1})$ for each sequence node in the tree, where
    $N$ is the total number of sequences in a tree, and Tree $T$ with its topology
    information. Subtree size $S_{t}$, which means each subtree after tiling contains
    at most $S_{t}$ tokens.  /*Evenly slice/blockwise the Tree KV cache (with $n_{T}$
    tokens in the tree ) to subtrees.*/  SubInfo, KSub, VSub =Slice( KL, VL, $S_{t}$,
    $T$)  /*Notes: (1) subtree number $m=Ceil(n_{T}/S_{t})$;  (2) subtrees’ KV cache
    $KSub=(Kb_{0},...,Kb_{m-1})$, $VSub=(Vb_{0},...,Vb_{m-1})$;  (3) subtree information
    $SubInfo=(Sb_{0},...,Sb_{m-1})$, where each subtree i has $Sb_{i}=(ofs_{0},...ofs_{n_{b_{i}}-1})$
    to record the offset of each node in the subtree KV cache, with $n_{b_{i}}$ as
    the total number of nodes in subtree $i$. */  for each subtree’s KV cache $Kb_{i},Vb_{i}$
    in $KSub,VSub$ with its subtree ID $i$ do     /*Group each subtree’s KV with all
    queries that share it.*/     $Q_{i}$= GroupQueryToKV($Q,Kb_{i},Vb_{i},T$) $\in
    R^{b_{i},d}\subset Q$     $KVMapQ[i]=Q_{i}$     for each query $q$ in $Q_{i}$
    with a global index $idx$ in $Q$ do        $QMapKV[idx].append(i)$     end for     /*Add
    a causal mask as different nodes in a subtree could be shared by different queries.*/     $CausalMask[i]=GetBitMask(Q_{i},Kb_{i},Vb_{i},T)$=$(CM_{0},...CM_{n_{b_{i}}-1})$     where
    $n_{b_{i}}$ is the total number of nodes in the subtree, and $CM_{i}$ is a 64-bit
    int bit mask for node i.     /*E.g, $100....00$ with 1 in bit 0, means the $Q_{i}[0]$
    does not share KV cache of node i in the subtree.*/  end for  Return QMapKV, KVMapQ,
    CausalMask,SubInfo'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：查询 $Q\in R^{(b_{q},d)}$，键缓存列表 $KL=(K_{0},...K_{N-1})$，值缓存列表 $VL=(V_{0},...V_{N-1})$
    适用于树中的每个序列节点，其中 $N$ 是树中的序列总数，树 $T$ 及其拓扑信息。子树大小 $S_{t}$，意味着每个子树在切片后包含最多 $S_{t}$
    个令牌。/*将树 KV 缓存（树中有 $n_{T}$ 个令牌）均匀切片/块分割成子树。*/ SubInfo, KSub, VSub = Slice(KL,
    VL, $S_{t}$, $T$) /*注释：（1）子树数量 $m=Ceil(n_{T}/S_{t})$；（2）子树的 KV 缓存 $KSub=(Kb_{0},...,Kb_{m-1})$，$VSub=(Vb_{0},...,Vb_{m-1})$；（3）子树信息
    $SubInfo=(Sb_{0},...,Sb_{m-1})$，其中每个子树 i 有 $Sb_{i}=(ofs_{0},...ofs_{n_{b_{i}}-1})$
    用于记录子树 KV 缓存中每个节点的偏移量，$n_{b_{i}}$ 为子树 $i$ 中节点的总数。*/ 对于每个子树的 KV 缓存 $Kb_{i},Vb_{i}$
    在 $KSub,VSub$ 中，其子树 ID 为 $i$，执行 /*将每个子树的 KV 与所有共享该子树的查询分组。*/ $Q_{i}$= GroupQueryToKV($Q,Kb_{i},Vb_{i},T$)
    $\in R^{b_{i},d}\subset Q$ $KVMapQ[i]=Q_{i}$ 对于每个查询 $q$ 在 $Q_{i}$ 中，其在 $Q$ 中的全局索引为
    $idx$，执行 $QMapKV[idx].append(i)$ 结束循环 /*添加因果掩码，因为子树中的不同节点可能被不同的查询共享。*/ $CausalMask[i]=GetBitMask(Q_{i},Kb_{i},Vb_{i},T)$=$(CM_{0},...CM_{n_{b_{i}}-1})$
    其中 $n_{b_{i}}$ 为子树中的节点总数，$CM_{i}$ 为节点 i 的 64 位整数位掩码。/*例如，$100....00$ 其中位 0 为 1，表示
    $Q_{i}[0]$ 不共享子树中节点 i 的 KV 缓存。*/ 结束循环 返回 QMapKV, KVMapQ, CausalMask, SubInfo
- en: 'Algorithm 4 DeFT-Subtree Algorithm-Phase 2: Attention Calculation.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 DeFT-Subtree 算法-阶段 2：注意力计算。
- en: 'Input: query $Q\in R^{(b_{q},d)}$, Key cache list in subtree-granularity KSub=($Kb_{0}$,…,$Kb_{m-1}$),
    Value cache list in subtree VSub = ($Vb_{0}$,…,$Vb_{m-1}$ for $m$ subtrees after
    tiling based on Tree $T$ with its topology information. QKV group information
    $QMapKV$, $KVMapQ$, causal mask $CausalMask$ and subtree information $SubInfo$
    from QKV Preparation Phase.  for each $q$ in $Q$ with its global index $idx$ do     /*Allocate
    to store LogSumExp of $Q@K^{T}$ grouped by query.*/     $LogSumExp[idx]=\{\}$     /*Allocate
    to store partial results of $SoftMax(Q@K^{T})V$ for each query.*/     $O[idx]=\{\}$  end for  /*Allocate
    space for output after reduction.*/  $FO=(0)_{b_{q}\times d}\in R^{(b_{q},d)}$  for each
    subtree’s KV cache $Kb_{i},Vb_{i}\in R^{(b_{kv},d)},R^{(b_{kv},d)}$ in $KSub,VSub$
    with subtree ID $i$ do     # Unroll for loop to SMs     $Q_{i}$= $KVMapQ[i]\in
    R^{(b_{i},d)}$     /*Reconstruct mask for attention calculation based on $CausalMask$
    and $SubInfo$*/     $bitmask=CausalMask[i]\in R^{n_{b_{i}}}$,where $n_{b_{i}}$
    is the total number of nodes for subtree i.     $SubOfst=SubInfo[i]\in R^{n_{b_{i}}}$     $mask=ReconstructMask(bitmask,SubOfst)\in
    R^{(b_{i},b_{kv})}$     /*Get partial attention $o_{i}$ for each QKV group, LogSumExp
    $lse_{i}$ of $Q@K^{T}$ in row for reduction.*/     $o_{i},lse_{i}$ = FlashAttention($Q_{i},Kb_{i},Vb_{i},mask$)     $\in
    R^{(b_{i},d)},R^{b_{i}}$     /*Map the partial results back to each query for
    reduction.*/     for each query $q$ in $Q_{i}$ with its group index $gp\_idx$
    and global index $idx$ in $Q$ do        if $i\in QMapKV[idx]$ then           $LogSumExp[idx].append(lse_{i}[gp\_idx])$        end if     end for  end for  for each
    $q$ in $Q$ with its global index $idx$ do     # Unroll for loop to SMs     if len($O[idx]$)==len($QMapKV[idx]$) then        /*Global
    reduction after collecting all partial results from QKV groups that contains $q$.*/        $LSE_{cat}$=
    CatTensor($LogSumExp[idx]$)        $LSE_{max}$=RowMax($LSE_{cat}$)        $Mid\_L=0,Mid\_O=0^{(1,d)}$        for each
    $lse_{j}$ in $LogSumExp[idx]$ do           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_L=Mid\_L+new\_exp$        end for        for each
    $lse_{j},o_{j}$ in $LogSumExp[idx],O[idx]$ do           $new\_exp=e^{lse_{j}-LSE_{max}}$           $Mid\_O=Mid\_O+new\_exp@o_{j}/Mid\_L$        end for        $FO[idx]=Mid\_O$     end if  end for  Return
    $FO$'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：查询 $Q\in R^{(b_{q},d)}$，子树粒度的键缓存列表 KSub=($Kb_{0}$,…,$Kb_{m-1}$)，值缓存列表 VSub
    = ($Vb_{0}$,…,$Vb_{m-1}$)，其中 $m$ 个子树在基于树 $T$ 及其拓扑信息的平铺后。QKV 组信息 $QMapKV$、$KVMapQ$、因果掩码
    $CausalMask$ 和子树信息 $SubInfo$ 来自 QKV 准备阶段。对于 $Q$ 中的每个 $q$ 及其全局索引 $idx$，执行：  '
