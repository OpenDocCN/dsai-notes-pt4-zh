- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CHAI: Clustered Head Attention for Efficient LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**CHAI**: 高效LLM推理的聚类头注意力'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.08058](https://ar5iv.labs.arxiv.org/html/2403.08058)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.08058](https://ar5iv.labs.arxiv.org/html/2403.08058)
- en: Saurabh Agarwal    Bilge Acun    Basil Homer    Mostafa Elhoushi    Yejin Lee
       Shivaram Venkataraman    Dimitris Papailiopoulos    Carole-Jean Wu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Saurabh Agarwal    Bilge Acun    Basil Homer    Mostafa Elhoushi    Yejin Lee
       Shivaram Venkataraman    Dimitris Papailiopoulos    Carole-Jean Wu
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) with hundreds of billions of parameters have transformed
    the field of machine learning. However, serving these models at inference time
    is both compute and memory intensive, where a single request can require multiple
    GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components
    of LLMs, which can account for over 50% of LLMs memory and compute requirement.
    We observe that there is a high amount of redundancy across heads on which tokens
    they pay attention to. Based on this insight, we propose Clustered Head Attention
    (CHAI). CHAI combines heads with a high amount of correlation for self-attention
    at runtime, thus reducing both memory and compute. In our experiments, we show
    that CHAI is able to reduce the memory requirements for storing K,V cache by up
    to 21.4% and inference time latency by up to $1.73\times$ without any fine-tuning
    required. CHAI achieves this with a maximum 3.2% deviation in accuracy across
    3 different models (i.e. OPT-66B, LLaMa-7B, LLaMa-33B) and 5 different evaluation
    datasets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有数百亿参数的大型语言模型（LLMs）已经彻底改变了机器学习领域。然而，在推理时服务这些模型不仅计算和内存需求极高，单次请求可能需要多个GPU和数十GB的内存。多头注意力是LLMs的关键组件之一，它可以占据LLMs内存和计算需求的50%以上。我们观察到在不同头之间对关注的tokens有很高的冗余。基于这一观察，我们提出了**聚类头注意力（CHAI）**。CHAI在运行时结合了具有高相关性的注意力头，从而减少了内存和计算需求。在我们的实验中，我们展示了CHAI能够将存储K,V缓存的内存需求减少多达21.4%，并且将推理时间延迟减少多达$1.73\times$，且无需任何微调。CHAI在3种不同模型（即OPT-66B、LLaMa-7B、LLaMa-33B）和5个不同评估数据集上，能够实现最多3.2%的准确度偏差。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: LLMs have demonstrated remarkable performance on language modelling tasks ranging
    from question answering, text summarizing, language translation. However, such
    performance has been achieved by scaling models to trillions of parameters, and
    existing works (Hoffmann et al., [2022](#bib.bib22); Touvron et al., [2023a](#bib.bib45);
    Kaplan et al., [2020](#bib.bib26)) show that increasing the model size may lead
    to even higher model quality.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在语言建模任务（如问答、文本总结、语言翻译）上展现了卓越的性能。然而，这种性能是通过将模型扩展到数万亿参数实现的，现有研究（Hoffmann等，[2022](#bib.bib22)；Touvron等，[2023a](#bib.bib45)；Kaplan等，[2020](#bib.bib26)）显示，增加模型规模可能会带来更高的模型质量。
- en: Inference on LLMs introduce several new challenges. Beyond just the quadratic
    computation cost of self-attention (Vaswani et al., [2017](#bib.bib47)) with increasing
    context and large model sizes, LLMs also store intermediate Key (K) and Value
    (V) pairs for subsequent next word prediction. This K,V caching introduces additional
    memory related challenges as K,V cache size increases with increase in sequence
    length. The architecture of widely used LLMs like GPT (Brown et al., [2020](#bib.bib4))
    and LLaMa (Touvron et al., [2023a](#bib.bib45), [b](#bib.bib46)) use Multi-Head
    Attention (MHA) (Vaswani et al., [2017](#bib.bib47)). MHA uses several attention
    heads to look at a sequence. As models grow bigger, the number of heads increases
    as well. For example, LLaMa-7B uses 32 attention heads in each layer, while LLaMa-65B
    uses 64 attention heads per layer (Touvron et al., [2023a](#bib.bib45)). The use
    of MHA exacerbates bottlenecks for serving LLMs. First, it increases compute pressure
    due to repeated application of the attention operation. Second, it increases the
    memory pressure due to requiring storage of Key (K), Value (V) caches that comes
    with the additional attention heads. To alleviate these bottlenecks, prior works
    have introduced primarily two types of methods - (i) pruning of LLMs to utilize
    sparsity based on the input context (Liu et al., [2023b](#bib.bib33); Voita et al.,
    [2019](#bib.bib48)) and (ii) Co-designing of the Attention module to reuse components
    across multiple heads like Multi-Query Attention (MQA) (Shazeer, [2019](#bib.bib41))
    and Grouped-Query Attention (GQA) (Ainslie et al., [2023](#bib.bib2)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大语言模型（LLMs）的推理带来了几个新的挑战。除了随着上下文和模型大小增加而自注意力的二次计算成本（Vaswani 等人，[2017](#bib.bib47)）外，LLMs
    还存储中间的键（K）和值（V）对以进行后续的下一个词预测。K,V 缓存引入了额外的内存相关挑战，因为 K,V 缓存的大小随着序列长度的增加而增加。像 GPT（Brown
    等人，[2020](#bib.bib4)）和 LLaMa（Touvron 等人，[2023a](#bib.bib45)，[b](#bib.bib46)）这样的广泛使用的
    LLMs 使用多头注意力（MHA）（Vaswani 等人，[2017](#bib.bib47)）。MHA 使用多个注意力头来查看一个序列。随着模型规模的扩大，头的数量也增加。例如，LLaMa-7B
    在每一层使用 32 个注意力头，而 LLaMa-65B 在每层使用 64 个注意力头（Touvron 等人，[2023a](#bib.bib45)）。MHA
    的使用加剧了服务 LLMs 的瓶颈。首先，由于重复应用注意力操作，它增加了计算压力。其次，由于需要存储带有额外注意力头的键（K）和值（V）缓存，它增加了内存压力。为了缓解这些瓶颈，先前的工作主要提出了两种方法
    - (i) 剪枝 LLMs 以利用基于输入上下文的稀疏性（Liu 等人，[2023b](#bib.bib33)；Voita 等人，[2019](#bib.bib48)）和
    (ii) 共同设计注意力模块，以在多个头之间重用组件，如多查询注意力（MQA）（Shazeer，[2019](#bib.bib41)）和分组查询注意力（GQA）（Ainslie
    等人，[2023](#bib.bib2)）。
- en: 'Figure 1: Accuracy vs Flops: We study various methods of clustering attention
    heads, and plot the runtime for sequence length of 2048\. For random head selection
    we randomly choose heads to combine in increasing number of 4, 8, 16 and 24\.
    For Static Head Selection, we choose the heads to combine based on activations.
    CHAI is our proposed method.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 精度与 Flops 的关系：我们研究了各种聚类注意力头的方法，并绘制了序列长度为 2048 的运行时间。对于随机头选择，我们随机选择头进行组合，数量依次增加为
    4、8、16 和 24。对于静态头选择，我们根据激活选择需要组合的头。CHAI 是我们提出的方法。'
- en: 'Pruning LLMs can potentially ease the compute bottleneck, however it is challenging
    as the classical methods for pruning (Frankle & Carbin, [2018](#bib.bib18); Chen
    et al., [2020b](#bib.bib6); You et al., [2019](#bib.bib58); Waleffe & Rekatsinas,
    [2020](#bib.bib49)) require fine-tuning or iterative training which is prohibitively
    expensive for LLMs due to massive memory and compute cost. There have been recent
    pruning works such as DejaVu (Liu et al., [2023b](#bib.bib33)) which perform pruning
    based on the context at inference time without requiring fine-tuning. However,
    we observe that methods like DejaVu are primarily designed for large parameter-inefficient
    models such as OPT (Zhang et al., [2022](#bib.bib60)) and the insights used to
    build DejaVu are not directly applicable on newer parameter efficient models like
    LLaMa-7B (Section [2](#S2 "2 Background and Related Work ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference")). In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference"), we show that CHAI
    achieves the best trade-off between flops and accuracy compared to the state-of-the-art
    methods. Furthermore, runtime pruning methods like DejaVu only reduce the compute
    cost and have no effect on the large memory requirements of K,V cache.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '剪枝LLM（大语言模型）可能有助于缓解计算瓶颈，但这是一项挑战，因为传统的剪枝方法（Frankle & Carbin, [2018](#bib.bib18);
    Chen et al., [2020b](#bib.bib6); You et al., [2019](#bib.bib58); Waleffe & Rekatsinas,
    [2020](#bib.bib49)）需要微调或迭代训练，这对于LLM而言因为巨大的内存和计算成本而极为昂贵。最近有一些剪枝工作，例如DejaVu（Liu
    et al., [2023b](#bib.bib33)），其在推理时基于上下文进行剪枝而不需要微调。然而，我们观察到像DejaVu这样的方式主要是针对大型参数效率低的模型（如OPT（Zhang
    et al., [2022](#bib.bib60)））设计的，而构建DejaVu所使用的见解并不直接适用于更新的参数高效模型，如LLaMa-7B（第[2](#S2
    "2 Background and Related Work ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference")节）。在图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference")中，我们展示了CHAI在flops与准确性之间达成的最佳折衷。进一步地，像DejaVu这样的运行时剪枝方法只减少了计算成本，对K,V缓存的大内存需求没有影响。'
- en: The Attention module co-design methods like GQA (Ainslie et al., [2023](#bib.bib2))
    require re-training of LLMs, e.g., LLaMa-2 (Touvron et al., [2023b](#bib.bib46))
    trained the models from scratch to utilize the benefits of GQA, making it quite
    expensive. Even in the case where users are willing to perform retraining, accuracy
    trade-off between GQA and MHA will not be known prior to multiple rounds of training.
    Further, Attention module co-design methods only reduce the K,V cache size and
    do not reduce computational complexity. Therefore, there is a need for a method,
    which can reduce both the compute and K,V cache overhead for attention and is
    - (i) Applicable on a wide range of models (from LLaMa–7B to OPT-66B). (ii) Does
    not require any fine-tuning or re-training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模块共同设计方法如GQA（Ainslie et al., [2023](#bib.bib2)）需要重新训练LLM，例如，LLaMa-2（Touvron
    et al., [2023b](#bib.bib46)）从头开始训练模型以利用GQA的优势，这使得它相当昂贵。即使在用户愿意进行重新训练的情况下，GQA与MHA之间的准确性折衷在多轮训练之前也是未知的。此外，注意力模块共同设计方法仅减少了K,V缓存的大小，而未减少计算复杂度。因此，需要一种方法，能够同时减少注意力的计算和K,V缓存开销，并且-
    (i) 适用于各种模型（从LLaMa–7B到OPT-66B）。(ii) 不需要任何微调或重新训练。
- en: 'In this work we present Clustered Head Attention for efficient LLM Inference
    (CHAI), a dynamic inference time method for efficient LLM inference that does
    not require fine-tuning. CHAI is inspired by two observations. First, several
    heads in multi-head attention give similar weight to each token in a given sequence,
    indicating redundant compute. In Figure [2(a)](#S1.F2.sf1 "Figure 2(a) ‣ Figure
    2 ‣ 1 Introduction ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    we show attention scores for a single layer of LLaMa-7B for an auto-regressive
    decoding step of a sentence. We observe that several heads output similar scores,
    i.e., giving similar weight to each token in the sequence. Figure [2(b)](#S1.F2.sf2
    "Figure 2(b) ‣ Figure 2 ‣ 1 Introduction ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference") highlights the similarity in attention score by plotting
    correlation for the activation for LLaMa-7B. In Figure [2(b)](#S1.F2.sf2 "Figure
    2(b) ‣ Figure 2 ‣ 1 Introduction ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference") we observe that there are three clusters and within these clusters
    the correlation is greater than 0.95.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们提出了用于高效 LLM 推断的聚类头注意力（CHAI），这是一种动态推断时间方法，用于高效的 LLM 推断，无需微调。CHAI 的灵感来源于两个观察结果。首先，多头注意力中的几个头对给定序列中的每个标记赋予类似的权重，这表明计算冗余。在图
    [2(a)](#S1.F2.sf1 "Figure 2(a) ‣ Figure 2 ‣ 1 Introduction ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference") 中，我们展示了 LLaMa-7B 的单层注意力分数，用于句子的自回归解码步骤。我们观察到几个头输出了类似的分数，即对序列中的每个标记赋予了类似的权重。图
    [2(b)](#S1.F2.sf2 "Figure 2(b) ‣ Figure 2 ‣ 1 Introduction ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference") 通过绘制 LLaMa-7B 的激活相关性来突出注意力分数的相似性。在图 [2(b)](#S1.F2.sf2
    "Figure 2(b) ‣ Figure 2 ‣ 1 Introduction ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference") 中，我们观察到存在三个簇，在这些簇内的相关性大于 0.95。'
- en: 'This indicates that by identifying attention heads with similar attention scores
    and clustering them together we can reduce the number of self-attention operations
    for MHA by calculating self-attention only for a single head within a cluster.
    Secondly, we observe that for each request to an LLM we can accurately determine
    the heads which are going to give similar (attention) weight to the tokens in
    a sequence after running a few decoding steps on the sequence (Section [3.3](#S3.SS3
    "3.3 Determination of Cluster Membership ‣ 3 CHAI ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference")). Schematic in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference") depicts both Multi-Head
    and Clustered-Head Attention.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '这表明，通过识别具有相似注意力分数的注意力头并将其聚类在一起，我们可以通过仅对簇中的一个头计算自注意力，从而减少 MHA 的自注意力操作次数。其次，我们观察到，对于每个对
    LLM 的请求，我们可以准确确定在对序列运行几个解码步骤后，将对序列中的标记赋予类似（注意力）权重的头（第 [3.3](#S3.SS3 "3.3 Determination
    of Cluster Membership ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference") 节）。图 [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference") 的示意图描绘了多头和聚类头注意力。'
- en: '![Refer to caption](img/cbaae28fcfba70d02f3d784537d4b4fe.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cbaae28fcfba70d02f3d784537d4b4fe.png)'
- en: '(a) Activations of Multi Head Attention: Figure shows activation scores for
    each token for each head. We observe that several heads give similar scores to
    the sequence.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 多头注意力的激活：图中显示了每个头对每个标记的激活分数。我们观察到几个头对序列给出了类似的分数。
- en: '![Refer to caption](img/63404e9ac502d63d43e2f03ca3f88a9a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/63404e9ac502d63d43e2f03ca3f88a9a.png)'
- en: '(b) Pairwise cross correlation: Pairwise cross-correlations show existence
    of three clusters- Heads [12,26] show strong correlation forming one cluster,
    Heads [20,25] form another, and the remaining heads form a third cluster.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 成对的交叉相关：成对的交叉相关显示存在三个簇——头 [12,26] 显示出强相关性形成一个簇，头 [20,25] 形成另一个簇，其余的头形成第三个簇。
- en: 'Figure 2: Redundancy across heads for LLaMa–7B'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLaMa–7B 中的头部冗余
- en: '![Refer to caption](img/99217df1baddd78971fb016d55476380.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/99217df1baddd78971fb016d55476380.png)'
- en: (a) Multi-Head Attention
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 多头注意力
- en: '![Refer to caption](img/bba42e362b2f2efbfcd25d14f0cbfe33.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bba42e362b2f2efbfcd25d14f0cbfe33.png)'
- en: (b) Clustered Head Attention
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 聚类头注意力
- en: 'Figure 3: Clustered Head Attention: Schematic of clustered head attention,
    comparing it with popular Multi-Head Attention. In clustered head attention, we
    remove the query and key vectors which produce similar attention scores.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：聚类头注意力：聚类头注意力的示意图，与流行的多头注意力进行比较。在聚类头注意力中，我们移除了生成类似注意力分数的查询和键向量。
- en: 'Our contributions in this paper are as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中的贡献如下：
- en: $\bullet$
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We show that there is high level of redundancy across several different heads
    of multi head attention, and the redundancy varies differently across layers with
    increasing redundancy towards later layers.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了在多个不同的多头注意力头之间存在高度冗余，并且这种冗余在层之间的变化也不同，随着层数的增加，冗余程度也会增加。
- en: $\bullet$
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We introduce CHAI, a practical and principled inference time pruning method
    which clusters attention heads that have similar output together with dynamic
    determination of clusters. CHAI reduces both compute and K,V cache size for self
    attention.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了 CHAI，一种实用且有原则的推理时间剪枝方法，它将具有相似输出的注意力头聚类在一起，并动态确定这些聚类。CHAI 降低了自注意力的计算和 K,V
    缓存大小。
- en: $\bullet$
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: We show that CHAI is capable of reducing the inference time by up to 1.73$\times$
    and K,V cache memory size by up to 21.4% compared to MHA for LLaMa models with
    minimal accuracy trade-off (maximum of 3.2%).
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了 CHAI 能够将推理时间减少高达 1.73$\times$，将 K,V 缓存内存大小减少高达 21.4%，相比于 MHA 对 LLaMa 模型的准确度影响最小（最大
    3.2%）。
- en: $\bullet$
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Compared to other runtime pruning methods like DejaVu, which only works well
    for OPT models, CHAI outperforms DejaVu and performs well for wider class of models.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与其他运行时剪枝方法如 DejaVu 相比，DejaVu 仅适用于 OPT 模型，CHAI 超越了 DejaVu，并且在更广泛的模型类别中表现良好。
- en: 2 Background and Related Work
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景和相关工作
- en: We first provide background on inference process for decoder only transformers
    like GPT (Radford et al., [2019](#bib.bib37); Brown et al., [2020](#bib.bib4)),
    LLaMa (Touvron et al., [2023a](#bib.bib45), [b](#bib.bib46)) and the bottlenecks
    in performing inference. Further, we discussed several prior lines of work which
    have tried to tackle the inference bottlenecks for transformer based model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍了类似 GPT（Radford 等人，[2019](#bib.bib37); Brown 等人，[2020](#bib.bib4)）、LLaMa（Touvron
    等人，[2023a](#bib.bib45)，[b](#bib.bib46)）的仅解码器 Transformer 的推理过程的背景，以及在执行推理时的瓶颈。此外，我们讨论了若干之前的工作，这些工作尝试解决基于
    Transformer 模型的推理瓶颈。
- en: Decoder-only Transformer A decoder-only transformer forms the building block
    of popular LLMs. A single decoder block consists of a self attention layer and
    a MLP. An input token is fed into the decoder block, to perform next-word prediction.
    The self attention block uses prior query (Q), key (K) and value (V) vectors associated
    with current token. These tokens are extracted by performing a linear projection
    with query, key and value weight matrices associated with a transformer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 仅解码器 Transformer 仅解码器 Transformer 是流行的 LLM 的构建块。单个解码器块由一个自注意力层和一个 MLP 组成。一个输入标记被送入解码器块，以执行下一个词的预测。自注意力块使用与当前标记相关的先前查询（Q）、键（K）和值（V）向量。这些标记通过对与
    Transformer 相关的查询、键和值权重矩阵执行线性投影来提取。
- en: 'To precisely define Multi-Head Attention (MHA), let $H$, $T$, $d$ be positive
    integers, where $H$ denotes number of heads, $T$ denotes sequence length, $d$
    denotes model dimension. Let $x\in{}^{T\times d}$ be input to the MHA layer. For
    a single head $h$, then $\mathbf{K}^{h}=x\mathbf{W}_{K}^{h}$, $\mathbf{Q}^{h}=x\mathbf{W}_{Q}^{h}$
    and $\mathbf{V}^{h}=x\mathbf{W}_{V}^{h}$ denote the corresponding key, query and
    value vector. The attention matrix for head $h$ is calculated as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了精确定义多头注意力（MHA），设 $H$、$T$、$d$ 为正整数，其中 $H$ 表示头的数量，$T$ 表示序列长度，$d$ 表示模型维度。设 $x\in{}^{T\times
    d}$ 为 MHA 层的输入。对于单个头 $h$，则 $\mathbf{K}^{h}=x\mathbf{W}_{K}^{h}$、$\mathbf{Q}^{h}=x\mathbf{W}_{Q}^{h}$
    和 $\mathbf{V}^{h}=x\mathbf{W}_{V}^{h}$ 表示相应的键、查询和值向量。头 $h$ 的注意力矩阵计算如下：
- en: '|  | $A_{h}=\sigma(\frac{1}{\sqrt{d}}Q^{h}K{{}^{h}}{{}^{T}})$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{h}=\sigma(\frac{1}{\sqrt{d}}Q^{h}K{{}^{h}}{{}^{T}})$ |  |'
- en: 'Output of MHA is denoted by:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MHA 的输出表示为：
- en: '|  | $y=A_{0}V_{0}\oplus A_{1}V_{1}\oplus A_{2}V_{2}\oplus\cdot\cdot\cdot\oplus
    A_{H}V_{H}$ |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=A_{0}V_{0}\oplus A_{1}V_{1}\oplus A_{2}V_{2}\oplus\cdot\cdot\cdot\oplus
    A_{H}V_{H}$ |  |'
- en: For performing inference, self attention needs access to the query, key and
    values associated with prior tokens. In order to avoid re-computation, inference
    serving systems cache the prior tokens in a sequence.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行推理时，自注意力需要访问与先前标记相关的查询、键和值。为了避免重新计算，推理服务系统会将先前的标记缓存到序列中。
- en: Compute cost required for multiple attention heads and memory capacity required
    for storing key and value vectors associated with each head during inference form
    two primary bottlenecks for LLM inference. In this work, we focus on reducing
    both memory and compute requirements via clustering multiple attention heads with
    similar output.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多个注意力头所需的计算成本和存储与每个头相关的键和值向量所需的内存容量形成了 LLM 推理的两个主要瓶颈。在这项工作中，我们重点通过将多个具有相似输出的注意力头进行聚类来减少内存和计算需求。
- en: Building Efficient Transformers. Improving efficiency of transformer models
    has been of major focus in recent years. Prior work can be broadly categorized
    in the following fields - (i) Hardware-software co-design (Dao et al., [2022](#bib.bib11);
    Dao, [2023](#bib.bib10); Ham et al., [2020](#bib.bib20), [2021](#bib.bib21); Tambe
    et al., [2021](#bib.bib43); Fang et al., [2022](#bib.bib17); Qin et al., [2023](#bib.bib36);
    Wang et al., [2021b](#bib.bib51)), (ii) Knowledge distillation (Hsieh et al.,
    [2023](#bib.bib24); Jiao et al., [2019](#bib.bib25); Sanh et al., [2019](#bib.bib40);
    Wang et al., [2020](#bib.bib53)) (iii) Neural Architecture Search (NAS) (Zhou
    et al., [2023](#bib.bib62); Kitaev et al., [2020](#bib.bib28); Lagunas et al.,
    [2021](#bib.bib30)) and (iv) Pruning (Voita et al., [2019](#bib.bib48); Liu et al.,
    [2023b](#bib.bib33)) and Quantization (Frantar et al., [2022](#bib.bib19); Xiao
    et al., [2023](#bib.bib56); Kim et al., [2021](#bib.bib27); Shen et al., [2020](#bib.bib42);
    Dettmers et al., [2022](#bib.bib14); Dettmers, [2015](#bib.bib12); Dettmers &
    Zettlemoyer, [2023](#bib.bib13)). In this work our focus is on pruning , which
    we discuss next.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 构建高效的变压器。提高变压器模型的效率近年来成为了主要关注点。以前的工作可以大致分为以下几个领域 - (i) 硬件-软件共同设计 (Dao et al.,
    [2022](#bib.bib11); Dao, [2023](#bib.bib10); Ham et al., [2020](#bib.bib20), [2021](#bib.bib21);
    Tambe et al., [2021](#bib.bib43); Fang et al., [2022](#bib.bib17); Qin et al.,
    [2023](#bib.bib36); Wang et al., [2021b](#bib.bib51)), (ii) 知识蒸馏 (Hsieh et al.,
    [2023](#bib.bib24); Jiao et al., [2019](#bib.bib25); Sanh et al., [2019](#bib.bib40);
    Wang et al., [2020](#bib.bib53)) (iii) 神经架构搜索 (NAS) (Zhou et al., [2023](#bib.bib62);
    Kitaev et al., [2020](#bib.bib28); Lagunas et al., [2021](#bib.bib30)) 和 (iv)
    剪枝 (Voita et al., [2019](#bib.bib48); Liu et al., [2023b](#bib.bib33)) 和量化 (Frantar
    et al., [2022](#bib.bib19); Xiao et al., [2023](#bib.bib56); Kim et al., [2021](#bib.bib27);
    Shen et al., [2020](#bib.bib42); Dettmers et al., [2022](#bib.bib14); Dettmers,
    [2015](#bib.bib12); Dettmers & Zettlemoyer, [2023](#bib.bib13))。在这项工作中，我们的重点是剪枝，接下来将讨论这个主题。
- en: LLM Quantization. Recently several methods have been proposed to perform post
    training quantization allowing models to be quantized to a lower precision (Frantar
    et al., [2022](#bib.bib19); Xiao et al., [2023](#bib.bib56); Dettmers & Zettlemoyer,
    [2023](#bib.bib13)). The goal of these methods is to perform quantization so as
    to minimize the error, CHAI is orthogonal to quantization based mechanisms as
    it depends on the insight of several attention heads focusing on the same tokens.
    The goal of quantization methods is to keep the same properties of original models,
    therefore we believe CHAI can be used to further accelerate post training quantized
    neural networks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 量化。最近提出了几种方法来进行训练后的量化，使模型能够量化为更低的精度 (Frantar et al., [2022](#bib.bib19);
    Xiao et al., [2023](#bib.bib56); Dettmers & Zettlemoyer, [2023](#bib.bib13))。这些方法的目标是进行量化以最小化误差，CHAI
    与基于量化的机制是正交的，因为它依赖于多个注意力头集中于相同的标记。量化方法的目标是保持原始模型的相同特性，因此我们相信 CHAI 可以用于进一步加速训练后量化的神经网络。
- en: LLM Pruning. Pruning is a widely studied method to improve inference time by
    removing unused weights post training. Several prior works have looked at pruning
    for language models (Chen et al., [2020b](#bib.bib6); Prasanna et al., [2020](#bib.bib35);
    Chen et al., [2020a](#bib.bib5)). For example, oBERT is a second order method
    to reduce the number of weights (Kurtic et al., [2022](#bib.bib29)). Although
    these approaches can compress a model, they rarely yield inference speedups due
    to lack of hardware support for sparse operations on modern GPUs. To overcome
    the challenges, low rank decomposition methods (Wang et al., [2023](#bib.bib52),
    [2021a](#bib.bib50), [2019](#bib.bib54)), attention head pruning (Michel et al.,
    [2019](#bib.bib34); Voita et al., [2019](#bib.bib48)), layer dropping (Sajjad
    et al., [2023](#bib.bib39); Fan et al., [2019](#bib.bib16); Dai et al., [2023](#bib.bib9))
    were proposed. However, these methods are infeasible for LLMs due to the use of
    iterative gradient calculations or fine-tuning leading to high resource requirements.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 剪枝。剪枝是一种广泛研究的方法，通过去除未使用的权重来提高推理时间。几个先前的工作研究了语言模型的剪枝 (Chen et al., [2020b](#bib.bib6);
    Prasanna et al., [2020](#bib.bib35); Chen et al., [2020a](#bib.bib5))。例如，oBERT
    是一种二阶方法，用于减少权重数量 (Kurtic et al., [2022](#bib.bib29))。尽管这些方法可以压缩模型，但由于现代 GPU 对稀疏操作的硬件支持不足，它们很少带来推理速度的提升。为了解决这些挑战，提出了低秩分解方法
    (Wang et al., [2023](#bib.bib52), [2021a](#bib.bib50), [2019](#bib.bib54))，注意力头剪枝
    (Michel et al., [2019](#bib.bib34); Voita et al., [2019](#bib.bib48))，以及层丢弃 (Sajjad
    et al., [2023](#bib.bib39); Fan et al., [2019](#bib.bib16); Dai et al., [2023](#bib.bib9))。然而，由于使用迭代梯度计算或微调导致高资源需求，这些方法对于
    LLM 来说是不可行的。
- en: 'To overcome these issues, a recently proposed method, DejaVu (Liu et al., [2023b](#bib.bib33)),
    identifies portions of the model which are unused for a given context. To reduce
    the overhead of self-attention, DejaVu prunes attention heads which give *uniform
    weight across tokens*. We plot the activations for an exemplary sentence used
    by DejaVu for both OPT-66B and LLaMa-7B in Figure [4](#S2.F4 "Figure 4 ‣ 2 Background
    and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM Inference").
    We observe that while there are heads which give uniform weight to each token
    in OPT-66B model, there are no such heads in more parameter efficient models like
    LLaMa-7B, indicating that for smaller parameter efficient models like LLaMa DejaVu
    might not be applicable. (Additional plots for different layers can be found in
    Appendix-[A](#A1 "Appendix A Additional Plots ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference").) The primary difference between OPT and LLaMa activation
    patterns could be attributed to the fact that LLaMa models are trained significantly
    longer and with more data.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '为了克服这些问题，最近提出了一种方法DejaVu（Liu et al., [2023b](#bib.bib33)），该方法识别在给定上下文中未使用的模型部分。为了减少自注意力的开销，DejaVu
    修剪了对*所有令牌赋予均匀权重*的注意力头。我们在图[4](#S2.F4 "Figure 4 ‣ 2 Background and Related Work
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")中绘制了 DejaVu 对 OPT-66B
    和 LLaMa-7B 使用的示例句子的激活图。我们观察到，虽然在 OPT-66B 模型中有一些头对每个令牌赋予均匀权重，但在像 LLaMa-7B 这样的更高效的模型中没有这种头，这表明对于像
    LLaMa 这样的参数更少的高效模型，DejaVu 可能不适用。（不同层的附加图可以在附录-[A](#A1 "Appendix A Additional Plots
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference") 中找到。）OPT 和 LLaMa
    激活模式的主要差异可能归因于 LLaMa 模型的训练时间显著更长且使用了更多的数据。'
- en: 'We observe that CHAI’s insight about redundancy in the output of multiple heads
    in the attention holds across both OPT and LLaMa family of models. In our evaluation
    (Section [4](#S4 "4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference")), we perform quantitative comparison between CHAI and DejaVu.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到，CHAI 对多个头输出冗余的洞察在 OPT 和 LLaMa 系列模型中均成立。在我们的评估（第[4](#S4 "4 Evaluation
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")节）中，我们对 CHAI 和 DejaVu
    进行了定量比较。'
- en: '![Refer to caption](img/4b3cdad209b7499347acb3d84dbe5791.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b3cdad209b7499347acb3d84dbe5791.png)'
- en: '(a) OPT-66B: For several heads the activation scores are uniform, i.e., the
    heads given close to equal importance to each input token.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (a) OPT-66B：对于几个头部，激活分数是均匀的，即这些头部对每个输入令牌赋予了几乎相等的重要性。
- en: '![Refer to caption](img/8f5a1d05769f4b62c1d7034dc38536aa.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f5a1d05769f4b62c1d7034dc38536aa.png)'
- en: '(b) LLaMa-7B: Heads in LLaMa-7B specifically pay attention to a specific token.
    However, multiple heads are attending to same token, in this case the first token.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LLaMa-7B：LLaMa-7B 中的头部特别关注某个特定的令牌。然而，多个头部正在关注同一个令牌，在这种情况下是第一个令牌。
- en: 'Figure 4: Activations for OPT-66B and LLaMa-7B for an exemplary sentence: We
    observe that OPT-66B has several heads which give uniform attention scores to
    tokens whereas LLaMa-7B does not. However, both models have redundancies across
    heads, i.e., groups of heads are give similar attention to each token.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：OPT-66B 和 LLaMa-7B 的示例句子的激活：我们观察到，OPT-66B 有几个头对令牌赋予均匀的注意力分数，而 LLaMa-7B 则没有。然而，这两个模型在头部之间都有冗余，即头部组对每个令牌给予类似的注意力。
- en: '![Refer to caption](img/a64a66ef858b612b35e8856914be2a01.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a64a66ef858b612b35e8856914be2a01.png)'
- en: 'Figure 5: CHAI Flow: In the offline phase, we run clustering and perform elbow
    plot analysis for each new model. Then, for each new inference request we only
    perform cluster membership identification based on online performance.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：CHAI 流程：在离线阶段，我们对每个新模型进行聚类并进行肘部图分析。然后，对于每个新的推理请求，我们仅根据在线性能进行集群成员识别。
- en: '![Refer to caption](img/ec9cede6a32483d5f70e13247f07df7b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec9cede6a32483d5f70e13247f07df7b.png)'
- en: (a) Layer 1
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第 1 层
- en: '![Refer to caption](img/cbf27f89ca9783dee7f07a08f2a17b72.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cbf27f89ca9783dee7f07a08f2a17b72.png)'
- en: (b) Layer 5
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第 5 层
- en: '![Refer to caption](img/95493ecb394e0b95c57c79818ac72465.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/95493ecb394e0b95c57c79818ac72465.png)'
- en: (c) Layer 17
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 第 17 层
- en: '![Refer to caption](img/4ac86c86de2806832cc294844294e2df.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ac86c86de2806832cc294844294e2df.png)'
- en: (d) Layer 30
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 第 30 层
- en: 'Figure 6: Average Correlation for 1024 Samples of C4 on LLaMa-7B: The above
    figure shows two interesting observations. First, there exists high amount of
    correlation across several heads of attention. Second, the correlation is not
    uniform across layers, with later layers having higher correlation, i.e., , first
    layer has very little correlation but correlation increases in later layers.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：LLaMa-7B上的C4 1024个样本的平均相关性：上图显示了两个有趣的观察结果。首先，多个注意力头之间存在高度相关性。其次，相关性在各层之间并不均匀，后层的相关性更高，即第一层相关性很小，但在后续层中相关性增加。
- en: '![Refer to caption](img/7c5538b8e859b96bbcc953f5bbb79d69.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c5538b8e859b96bbcc953f5bbb79d69.png)'
- en: (a) Layer 1
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第1层
- en: '![Refer to caption](img/805fc10824da6ce393f7834dac56bc6b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/805fc10824da6ce393f7834dac56bc6b.png)'
- en: (b) Layer 5
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第5层
- en: '![Refer to caption](img/bb5c0a0611b0300a882c440b97b30124.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bb5c0a0611b0300a882c440b97b30124.png)'
- en: (c) Layer 17
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 第17层
- en: '![Refer to caption](img/5ff599164cd2909b87a139557fe2b70b.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ff599164cd2909b87a139557fe2b70b.png)'
- en: (d) Layer 30
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 第30层
- en: 'Figure 7: Correlation on a randomly selected single sample of LLaMa-7B.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在随机选取的单个LLaMa-7B样本上的相关性。
- en: K,V Cache Compression. Prior works which have tried to reduce the K,V cache
    size (Liu et al., [2023a](#bib.bib32); Zhang et al., [2023](#bib.bib61)) by storing
    the K,V cache values for the most recent important tokens. However, they can not
    directly improve the latency of generating the next token, as they still perform
    the full transformer compute before finally deciding which K,V pairs should be
    stored. On the other hand, CHAI reduces not just the K,V cache size, it is also
    able to reduce the latency of next word prediction.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: K,V缓存压缩。之前的研究尝试通过存储最近重要令牌的K,V缓存值来减少K,V缓存的大小（Liu等，[2023a](#bib.bib32); Zhang等，[2023](#bib.bib61)）。然而，它们不能直接改善生成下一个令牌的延迟，因为它们仍然需要执行完整的变换器计算，然后才决定应该存储哪些K,V对。另一方面，CHAI不仅减少了K,V缓存的大小，还能减少下一个词预测的延迟。
- en: Speculative Decoding. Speculative decoding (Leviathan et al., [2023](#bib.bib31);
    Yang et al., [2023](#bib.bib57); Xia et al., [2023](#bib.bib55)) is a popular
    method where a draft model is used to cheaply generate a sequence of draft tokens
    which can be efficiently verified by a target LLM. Speculative decoding can significantly
    reduce the latency of LLM serving, however it further exacerbates the compute
    and memory requirements as it requires additional resources to run both the draft
    and target model. CHAI on the other hand is focused on reducing the resource required
    for inference.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 推测解码。推测解码（Leviathan等，[2023](#bib.bib31); Yang等，[2023](#bib.bib57); Xia等，[2023](#bib.bib55)）是一种流行的方法，其中使用草稿模型来廉价地生成一系列草稿令牌，这些令牌可以由目标LLM有效地验证。推测解码可以显著减少LLM服务的延迟，但由于需要额外的资源来运行草稿模型和目标模型，它进一步加剧了计算和内存要求。另一方面，CHAI则专注于减少推理所需的资源。
- en: 3 CHAI
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CHAI
- en: 'Next, we describe CHAI. We first describe the key insights which have been
    used to build CHAI. Then, we detail CHAI’s runtime pruning algorithm which is
    inspired by our insights and discuss how we perform inference using CHAI. Figure [5](#S2.F5
    "Figure 5 ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference") provides a high level overview of inference using CHAI,
    which includes offline and online components.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们描述CHAI。我们首先介绍用于构建CHAI的关键见解。然后，我们详细说明CHAI的运行时修剪算法，这些算法受到了我们的见解的启发，并讨论如何使用CHAI进行推理。图[5](#S2.F5
    "Figure 5 ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference")提供了使用CHAI进行推理的高级概述，包括离线和在线组件。'
- en: 3.1 Observations
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 观察
- en: 'Our primary insight stems from the observation that there is a high amount
    of correlation across the output of various attention heads in MHA, i.e., the
    output of several attention heads focuses on the same tokens. In Figure [6](#S2.F6
    "Figure 6 ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference"), we plot the average correlation across the 32 heads
    of LLaMa-7B for 1024 samples of the C4 (Raffel et al., [2020](#bib.bib38)) dataset
    for different layers and in Figure [7](#S2.F7 "Figure 7 ‣ 2 Background and Related
    Work ‣ CHAI: Clustered Head Attention for Efficient LLM Inference"), we plot correlation
    for a single sample of the dataset. These show us two insights - (i) Several heads
    output similar attention scores for each example and (ii) The amount of correlation
    increases in later layers, with heads in later layers with having higher correlation.
    This indicates that there is an opportunity to cluster attention heads with similar
    output and only run the self-attention operation for one of the representative
    attention heads within each cluster, thus reducing the amount of computation as
    well as the size of K,V cache.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要见解来源于观察到 MHA 中各注意力头输出之间存在高度相关性，即多个注意力头的输出集中在相同的令牌上。在图 [6](#S2.F6 "图 6 ‣
    2 背景和相关工作 ‣ CHAI：高效 LLM 推理的聚类头注意力") 中，我们绘制了 LLaMa-7B 的 32 个头部在 C4 数据集（Raffel 等人，[2020](#bib.bib38)）的
    1024 个样本中的平均相关性，对于不同的层，而在图 [7](#S2.F7 "图 7 ‣ 2 背景和相关工作 ‣ CHAI：高效 LLM 推理的聚类头注意力")
    中，我们绘制了数据集单个样本的相关性。这些图展示了两个见解 - (i) 多个头部为每个示例输出类似的注意力分数，(ii) 相关性在后续层中增加，后续层的头部具有更高的相关性。这表明有机会对输出相似的注意力头进行聚类，并仅对每个聚类中的一个代表性注意力头运行自注意力操作，从而减少计算量和
    K、V 缓存的大小。
- en: '![Refer to caption](img/688b440026282afe4f3a8e7e687e1fc7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/688b440026282afe4f3a8e7e687e1fc7.png)'
- en: 'Figure 8: Clustering Error: We plot the clustering error on 1024 samples of
    C4-dataset. The markers represent the number of clusters we choose for a layer.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：聚类误差：我们在 C4 数据集的 1024 个样本上绘制了聚类误差。标记表示我们为每一层选择的聚类数。
- en: '![Refer to caption](img/eaa6f1ed7b63d8655146e5a0194c4dd6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eaa6f1ed7b63d8655146e5a0194c4dd6.png)'
- en: 'Figure 9: Cluster Membership Evaluation: We evaluate the number of times the
    cluster membership changes for performing next token prediction. We observed that
    if clustering is performed beyond the fifth token the number of times cluster
    membership changes is quite small.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：聚类成员评估：我们评估了在进行下一个令牌预测时聚类成员变化的次数。我们观察到，如果在第五个令牌之后进行聚类，聚类成员变化的次数相当少。
- en: '![Refer to caption](img/4dece796bd1047e5b3ce1166802c70ef.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4dece796bd1047e5b3ce1166802c70ef.png)'
- en: '(a) Offline Cluster Identification: For each new model we run an offline cluster
    identification phase. We collect the activations and perform Elbow-plot analysis
    to decide number of clusters.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 离线聚类识别：对于每个新模型，我们运行离线聚类识别阶段。我们收集激活值并进行肘部图分析以决定聚类数。
- en: '![Refer to caption](img/41f11c67f462c4c06f67129b6fb31e10.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/41f11c67f462c4c06f67129b6fb31e10.png)'
- en: '(b) Cluster Membership Identification: For each new request, we initial run
    with multi-head attention for first five tokens. Using this we determine the number
    of clusters in each layer.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 聚类成员识别：对于每个新请求，我们在前五个令牌上使用多头注意力进行初始运行。通过这些，我们确定每层中的聚类数量。
- en: '![Refer to caption](img/f4d63d3b5244e91283abe2d75a0669af.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f4d63d3b5244e91283abe2d75a0669af.png)'
- en: '(c) CHAI Inference: Post cluster membership identification we substitute MHA
    with Clustered Head Attention.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (c) CHAI 推理：在识别聚类成员之后，我们用聚类头注意力替代 MHA。
- en: 'Figure 10: Schematic of CHAI detailing three phases of the system.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：CHAI 系统三阶段的示意图。
- en: Problem Formulation. Next, we formally define the problem of finding heads whose
    attention score is similar. Let $H$ be the total number of attention heads, let
    $S=\{\langle K^{1},Q^{1}\rangle,\langle K^{2},Q^{2}\rangle,\langle K^{3},Q^{3}\rangle,\cdot\cdot\cdot,\langle
    K^{H},V^{H}\rangle\}$ be the set of $Q,K$ pairs associated with each head $h$.
    Our goal is to find $k$ subsets, $S_{1}\subset S,S_{2}\subset S,S_{3}\subset S,\cdot\cdot\cdot
    S_{k}\subset S$ such that $ pairs in each subset $ 对在函数 $f$ 下产生相似的输出。其中函数 $f$
    是自注意力操作，$f(Q,K)=\sigma(QK{{}^{T}})$。此外，我们希望 $\cup_{i=1}^{k}S_{i}=S$。
- en: Formally, we want to find $S_{i}$,
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，我们要找到 $S_{i}$，
- en: '|  | 
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | 
    |  |'
- en: s.t.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: s.t.
- en: '|  | $f(K^{n},Q^{n})\approx f(K^{m},Q^{m})$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(K^{n},Q^{n})\approx f(K^{m},Q^{m})$ |  |'
- en: Informally, we want subset of heads, where within each subset the self attention
    operation gives similar outcome.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式地，我们希望找到头部的子集，其中每个子集内的自注意力操作产生相似的结果。
- en: In order to solve this problem we need to determine $k$ which represents the
    number of such subsets, and the membership of such subset $S_{i}$. Our observations
    empirically demonstrate the existence of such a solution. We can potentially solve
    this problem using clustering, where determining the number of subsets translates
    to determining number of clusters and determining cluster membership becomes determination
    of cluster membership.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要确定 $k$，它表示这些子集的数量，并确定这样的子集 $S_{i}$ 的成员。我们的观察实验证明了这样的解决方案的存在。我们可以通过聚类方法潜在地解决这个问题，其中确定子集的数量等同于确定聚类的数量，而确定聚类成员变成了确定聚类成员的过程。
- en: To observe memory and compute savings, we need an accurate and efficient method
    to determine the number of clusters and their membership *without having access
    to activations*. Solving this forms a core contribution of our work.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察内存和计算节省，我们需要一种准确而高效的方法来确定聚类的数量及其成员*而不需访问激活值*。解决这一问题是我们工作的核心贡献。
- en: 3.2 Determination of Number of Clusters
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 聚类数量的确定
- en: Challenges.
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 挑战。
- en: 'Figure [6](#S2.F6 "Figure 6 ‣ 2 Background and Related Work ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference") and Figure [7](#S2.F7 "Figure 7 ‣
    2 Background and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference") indicate that the number of clusters varies widely per layer in a
    LLM. Specifically, the last few layers in the LLM exhibit a very low number of
    clusters (high redundancy), whereas the early layers demonstrate a high degree
    of variance across the output of heads resulting in large number of clusters.
    This observation suggests that the method used to determine number of clusters
    needs to make decisions for each layer independently. Additionally, widely used
    methods such as Elbow plot method (Thorndike, [1953](#bib.bib44)) for determining
    number of clusters entail manual effort making cluster number determination impractical
    at inference time.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [6](#S2.F6 "图 6 ‣ 2 背景和相关工作 ‣ CHAI: 集群头部注意力以提高 LLM 推理效率") 和图 [7](#S2.F7 "图
    7 ‣ 2 背景和相关工作 ‣ CHAI: 集群头部注意力以提高 LLM 推理效率") 表明 LLM 中每层的聚类数量差异很大。具体而言，LLM 中的最后几层表现出非常低的聚类数量（高冗余），而早期层展示了头部输出的高度方差，导致大量的聚类。这一观察表明，用于确定聚类数量的方法需要对每一层独立做出决策。此外，像肘部图方法（Thorndike,
    [1953](#bib.bib44)）这样的广泛使用的方法来确定聚类数量需要手动操作，使得在推理时确定聚类数量不切实际。'
- en: 'Design. To determine the number of clusters, we propose an offline strategy
    we run once for each model. In our case, we sample a small number of samples (1024)
    from the C4 (Raffel et al., [2020](#bib.bib38)) dataset and perform elbow-plot
    analysis by plotting clustering error (i.e. sum of squared distance from the closest
    cluster) as a function of number of clusters. Figure [8](#S3.F8 "Figure 8 ‣ 3.1
    Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    shows the clustering error for LLaMa-7B for the samples selected. Based on the
    Elbow-plot analysis we choose the number of clusters when the error plateaus.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '设计。为了确定簇的数量，我们提出了一种离线策略，针对每个模型运行一次。在我们的情况下，我们从 C4 (Raffel 等， [2020](#bib.bib38))
    数据集中抽取少量样本 (1024)，通过绘制聚类误差（即距离最近簇的平方和）与簇数量的函数关系图进行肘部图分析。图 [8](#S3.F8 "Figure 8
    ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference") 显示了为所选样本的 LLaMa-7B 计算的聚类误差。根据肘部图分析，我们选择误差平稳时的簇数量。'
- en: The offline analysis is performed once for each network by using the C4 (Raffel
    et al., [2020](#bib.bib38)) dataset. We do not change the number of clusters determined
    for a new dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 离线分析是针对每个网络使用 C4 (Raffel 等， [2020](#bib.bib38)) 数据集进行一次的。我们不会更改为新数据集确定的簇数量。
- en: 3.3 Determination of Cluster Membership
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 簇成员的确定
- en: 'Challenges. Having determined number of clusters, we need to determine the
    membership of these clusters, i.e., which heads belong to which cluster in each
    layer. For Figure [6](#S2.F6 "Figure 6 ‣ 2 Background and Related Work ‣ CHAI:
    Clustered Head Attention for Efficient LLM Inference"), [7](#S2.F7 "Figure 7 ‣
    2 Background and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference") and  [8](#S3.F8 "Figure 8 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference"), we perform clustering based on activations
    obtained by performing the forward pass. However, for each decoding step, performing
    clustering on output of self attention post forward pass will not yield any performance
    benefit as we will still be performing the original compute and using the full
    K,V cache. In order to utilize the insights observed in Section [3.1](#S3.SS1
    "3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference"), we will need to decide the cluster members without having access
    to the output of the self attention.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '挑战。在确定簇的数量后，我们需要确定这些簇的成员，即每层中哪些头属于哪个簇。对于图 [6](#S2.F6 "Figure 6 ‣ 2 Background
    and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")、[7](#S2.F7
    "Figure 7 ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference") 和 [8](#S3.F8 "Figure 8 ‣ 3.1 Observations ‣ 3 CHAI ‣
    CHAI: Clustered Head Attention for Efficient LLM Inference")，我们根据执行前向传播获得的激活值进行聚类。然而，对于每个解码步骤，在前向传播后对自注意力的输出进行聚类不会带来任何性能提升，因为我们仍然会执行原始计算并使用完整的
    K,V 缓存。为了利用在第 [3.1](#S3.SS1 "3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference") 节中观察到的见解，我们需要在无法访问自注意力输出的情况下决定簇的成员。'
- en: Design. A simple strategy would have been keeping the cluster membership static
    across the tokens and independent of input context, e.g., we use the same cluster
    membership found during offline analysis with C4 data in the previous section.
    For evaluation purposes we call this version of head selection CHAI-static.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 设计。一种简单的策略是保持簇的成员在所有标记中静态且与输入上下文无关，例如，我们使用在上一节的 C4 数据中找到的相同簇成员。为了评估目的，我们将这种头部选择版本称为
    CHAI-static。
- en: 'However, we observed that the cluster membership does not remain static and
    varies based on context. When comparing Figure [7](#S2.F7 "Figure 7 ‣ 2 Background
    and Related Work ‣ CHAI: Clustered Head Attention for Efficient LLM Inference"),
    which plots correlation for a single example, with Figure [6](#S2.F6 "Figure 6
    ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference"), which plots correlation for 1024 samples, we observe that the
    correlation across heads varies with varying context. Therefore, the correlation
    across the output of the heads depends on the context (input prompt), i.e., *a
    solution to determine the membership of each cluster has to account for context.*
    To understand the effects of accounting for context while clustering heads, we
    analysed the change in cluster membership changes and clustering with different
    context. In Figure [9](#S3.F9 "Figure 9 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference"), we observed an interesting phenomenon,
    after determining cluster membership by accounting for five tokens, the cluster
    membership does not change frequently. A direct outcome of this observation is
    that for each new sequence we can perform clustering based on the output of self-attention
    after the first five tokens. We observe that *activation from first five tokens
    of a new sequence are enough to accurately predict the cluster membership.* This
    dynamic version of head selection further allows us to improve accuracy over CHAI-static.
    Figure [10(b)](#S3.F10.sf2 "Figure 10(b) ‣ Figure 10 ‣ 3.1 Observations ‣ 3 CHAI
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference") shows an illustration
    of the membership identification step. Furthermore, evaluation results in Section [4](#S4
    "4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient LLM Inference") compare
    CHAI-static and CHAI performance.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，我们观察到簇成员资格并非静态，而是基于上下文变化。当将图 [7](#S2.F7 "Figure 7 ‣ 2 Background and Related
    Work ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")中绘制的单个示例的相关性与图 [6](#S2.F6
    "Figure 6 ‣ 2 Background and Related Work ‣ CHAI: Clustered Head Attention for
    Efficient LLM Inference")中绘制的1024个样本的相关性进行比较时，我们发现各个头之间的相关性随上下文变化。因此，各个头的输出之间的相关性依赖于上下文（输入提示），即，*确定每个簇的成员资格的解决方案必须考虑上下文。*为了理解在聚类头部时考虑上下文的影响，我们分析了簇成员资格的变化以及不同上下文下的聚类。在图 [9](#S3.F9
    "Figure 9 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference")中，我们观察到一个有趣的现象，在考虑了五个标记后确定簇成员资格时，簇成员资格不会频繁变化。这个观察的直接结果是，对于每个新序列，我们可以基于前五个标记后的自注意力输出进行聚类。我们观察到*新序列前五个标记的激活足以准确预测簇成员资格。*这种动态版本的头选择进一步使我们能够在准确度上超越
    CHAI-static。图 [10(b)](#S3.F10.sf2 "Figure 10(b) ‣ Figure 10 ‣ 3.1 Observations
    ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")展示了成员资格识别步骤的示意图。此外，第 [4](#S4
    "4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")节的评估结果比较了
    CHAI-static 和 CHAI 的性能。'
- en: 3.4 Clustered Head Attention
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 聚类头部注意力
- en: Once we have decided which heads have similar attention output, we can than
    use Clustered Head Attention to combine key and query vectors for the heads.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定了哪些头具有相似的注意力输出，我们就可以使用聚类头部注意力来结合这些头的键和值向量。
- en: 3.5 Inference using CHAI
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 使用 CHAI 进行推理
- en: 'Next we, discuss the inference flow of CHAI, illustrated in detail in Figure [10](#S3.F10
    "Figure 10 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference"). For each new model we first perform offline cluster identification
    (Figure [10(a)](#S3.F10.sf1 "Figure 10(a) ‣ Figure 10 ‣ 3.1 Observations ‣ 3 CHAI
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")). Then for each
    new request, we determine the cluster membership using K-Means clustering once
    we have processed five tokens, using the observed activations (Figure [10(b)](#S3.F10.sf2
    "Figure 10(b) ‣ Figure 10 ‣ 3.1 Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference")). After this step, we keep the clustered heads same
    throughout inference (Figure [10(c)](#S3.F10.sf3 "Figure 10(c) ‣ Figure 10 ‣ 3.1
    Observations ‣ 3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论CHAI的推理流程，详细见图[10](#S3.F10 "图10 ‣ 3.1 观察 ‣ 3 CHAI ‣ CHAI：用于高效LLM推理的聚类头注意力")。对于每个新模型，我们首先进行离线集群识别（图[10(a)](#S3.F10.sf1
    "图10(a) ‣ 图10 ‣ 3.1 观察 ‣ 3 CHAI ‣ CHAI：用于高效LLM推理的聚类头注意力")）。然后，对于每个新请求，我们在处理了五个标记后，使用K-Means聚类来确定集群成员资格，利用观察到的激活（图[10(b)](#S3.F10.sf2
    "图10(b) ‣ 图10 ‣ 3.1 观察 ‣ 3 CHAI ‣ CHAI：用于高效LLM推理的聚类头注意力")）。在这一步之后，我们在整个推理过程中保持集群头不变（图[10(c)](#S3.F10.sf3
    "图10(c) ‣ 图10 ‣ 3.1 观察 ‣ 3 CHAI ‣ CHAI：用于高效LLM推理的聚类头注意力")。
- en: There are two direct outcomes of CHAI’s design. First, we directly reduce the
    amount of computation by removing redundant heads. Secondly, after a pre-determined
    token we fix the heads which are going to be pruned, this also allows us to remove
    the corresponding *Key* tokens associated, which significantly reduces the K,V
    cache size. Therefore, CHAI allows us to reduce both the inference compute as
    well as the size of the K,V cache required.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: CHAI设计有两个直接结果。首先，我们通过去除冗余的头来直接减少计算量。其次，在预定的标记之后，我们固定要修剪的头，这也使我们能够去除相应的*Key*标记，从而显著减少K,V缓存的大小。因此，CHAI使我们能够减少推理计算以及所需的K,V缓存的大小。
- en: 'Table 1: Accuracy on OPT-66B'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：OPT-66B上的准确性
- en: '| Method | PIQA | Hellaswag | Arc-Challenge | Arc-Easy | Boolq |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PIQA | Hellaswag | Arc-Challenge | Arc-Easy | Boolq |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MHA | 78.4 | 71.1 | 41.6 | 64.7 | 65.4 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MHA | 78.4 | 71.1 | 41.6 | 64.7 | 65.4 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DejaVu-50% | -0.25 | -0.7 | -0.6 | -0.2 | -4.0 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| DejaVu-50% | -0.25 | -0.7 | -0.6 | -0.2 | -4.0 |'
- en: '| CHAI-static | -1.35 | -1.7 | -0.7 | -0.7 | -0.7 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| CHAI-static | -1.35 | -1.7 | -0.7 | -0.7 | -0.7 |'
- en: '| CHAI | -0.15 | 0.1 | 0.1 | -0.1 | -0.6 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| CHAI | -0.15 | 0.1 | 0.1 | -0.1 | -0.6 |'
- en: 4 Evaluation
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估
- en: 'We experimentally verify the performance of CHAI and compare it to DejaVu (Liu
    et al., [2023b](#bib.bib33)) and SpAtten (Wang et al., [2021b](#bib.bib51)) on
    three different models of various sizes LLaMa-7B (Touvron et al., [2023a](#bib.bib45)),
    LLaMa-33B and OPT-66B (Zhang et al., [2022](#bib.bib60)). We evaluate the models
    on five commonly used NLP tasks: PIQA (Bisk et al., [2020](#bib.bib3)), HellaSwag (Zellers
    et al., [2019](#bib.bib59)), Arc-Challenge and Arc-Easy (Clark et al., [2018](#bib.bib8))
    and BoolQA (Clark et al., [2019](#bib.bib7)).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过实验验证了CHAI的性能，并将其与DejaVu（Liu et al., [2023b](#bib.bib33)）和SpAtten（Wang et
    al., [2021b](#bib.bib51)）在三种不同大小的模型上进行了比较：LLaMa-7B（Touvron et al., [2023a](#bib.bib45)）、LLaMa-33B和OPT-66B（Zhang
    et al., [2022](#bib.bib60)）。我们在五个常用的NLP任务上评估了这些模型：PIQA（Bisk et al., [2020](#bib.bib3)）、HellaSwag（Zellers
    et al., [2019](#bib.bib59)）、Arc-Challenge和Arc-Easy（Clark et al., [2018](#bib.bib8)）以及BoolQA（Clark
    et al., [2019](#bib.bib7)）。
- en: 4.1 Experimental Setup
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: All our experiments are performed on servers with NVIDIA V100 GPUs. For OPT-66B
    we used eight GPUs on a single node, for LLaMa-33B we used four GPUs, and for
    LLaMa-7B, we used a single GPU for inference. CHAI is built on top of Meta’s xFormers (facebookresearch,
    [2023](#bib.bib15)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的实验都在配备NVIDIA V100 GPU的服务器上进行。对于OPT-66B，我们在单个节点上使用了八个GPU，对于LLaMa-33B，我们使用了四个GPU，而对于LLaMa-7B，我们使用了一个GPU进行推理。CHAI建立在Meta的xFormers（facebookresearch，[2023](#bib.bib15)）之上。
- en: 4.2 Accuracy Evaluation
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 准确性评估
- en: In our evaluation, we compare CHAI with Multi-Head Attention as baseline, static
    version of CHAI, as well two other state-of-the-art prior pruning methods; DejaVu
    and SpAtten. For DejaVu, we try different sparsity ratios, in order to try to
    match the accuracy number to MHA. We also compare CHAI to SpAtten, a method which
    removes unimportant tokens and heads.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的评估中，我们将CHAI与多头注意力（MHA）作为基准，静态版本的CHAI，以及两种其他先进的剪枝方法：DejaVu和SpAtten进行了比较。对于DejaVu，我们尝试了不同的稀疏比，以期准确性数值匹配MHA。我们还将CHAI与SpAtten进行比较，SpAtten是一种去除不重要标记和头的方法。
- en: 'In Table [1](#S3.T1 "Table 1 ‣ 3.5 Inference using CHAI ‣ 3 CHAI ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference"), we first verify that we are able
    to reproduce the performance numbers reported by DejaVu. To perform this, we took
    the OPT-66B and evaluated both DejaVu, CHAI and CHAI-static. We used DejaVu with
    50% sparsity as reported by the authors. We used the author provided code to train
    their MLP predictor layers and incorporate their scheme in our setup. In Table [1](#S3.T1
    "Table 1 ‣ 3.5 Inference using CHAI ‣ 3 CHAI ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference"), we observe that we were able to replicate results
    for OPT-66B. Furthermore, CHAI is also able to match the accuracy of MHA for OPT-66B.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [1](#S3.T1 "Table 1 ‣ 3.5 Inference using CHAI ‣ 3 CHAI ‣ CHAI: Clustered
    Head Attention for Efficient LLM Inference") 中，我们首先验证了我们是否能够复现 DejaVu 报告的性能数据。为此，我们使用了
    OPT-66B，并评估了 DejaVu、CHAI 和 CHAI-static。我们使用了作者报告的 50% 稀疏性 DejaVu。我们使用了作者提供的代码来训练他们的
    MLP 预测器层，并将他们的方案纳入我们的设置中。在表格 [1](#S3.T1 "Table 1 ‣ 3.5 Inference using CHAI ‣
    3 CHAI ‣ CHAI: Clustered Head Attention for Efficient LLM Inference") 中，我们观察到我们能够复制
    OPT-66B 的结果。此外，CHAI 还能够匹配 OPT-66B 的 MHA 准确率。'
- en: 'Next, we compare CHAI, CHAI-static and DejaVu with the pre-trained MHA network,
    using LLaMa-7B on 5 different datasets. For DejaVu we used three configurations,
    50% sparsity, 30% sparsity and 10% sparsity. In Table [2](#S4.T2 "Table 2 ‣ 4.2
    Accuracy Evaluation ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient
    LLM Inference"), we observe that when we use DejaVu with more 10% sparsity we
    see significant decrease in accuracy (by 18.6% for DejaVu-30%). On the other hand,
    our method based on our close analysis of the behaviour of layers of LLaMa-7B
    is able to recover accuracy. We observe a maximum accuracy degradation of 3.7%
    for CHAI. Similarly for LLaMa-33B using sparsity for more than 10% leads to significant
    accuracy drop, meanwhile CHAI closely matches the accuracy of the pre-trained
    model using MHA with maximum degradation in accuracy by 0.14%. This shows that
    CHAI is widely applicable across multiple datasets and models. We also want to
    highlight that we do not perform any dataset specific tuning.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们将 CHAI、CHAI-static 和 DejaVu 与预训练的 MHA 网络进行比较，使用 LLaMa-7B 在 5 个不同的数据集上。对于
    DejaVu，我们使用了三种配置：50% 稀疏性、30% 稀疏性和 10% 稀疏性。在表格 [2](#S4.T2 "Table 2 ‣ 4.2 Accuracy
    Evaluation ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    中，我们观察到当 DejaVu 使用超过 10% 的稀疏性时，准确率显著下降（DejaVu-30% 下降了 18.6%）。另一方面，我们的方法基于对 LLaMa-7B
    层行为的详细分析，能够恢复准确率。我们观察到 CHAI 的最大准确率下降为 3.7%。同样，对于 LLaMa-33B，使用超过 10% 的稀疏性会导致准确率显著下降，而
    CHAI 的准确率与使用 MHA 的预训练模型非常接近，最大准确率下降为 0.14%。这表明 CHAI 广泛适用于多个数据集和模型。我们还要强调的是，我们没有进行任何数据集特定的调优。'
- en: 'Table 2: Accuracy on LLaMa-7B'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: LLaMa-7B 的准确率'
- en: '| Method | PIQA | HellaSwag | Arc-Challenge | Arc-Easy | BoolQ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PIQA | HellaSwag | Arc-Challenge | Arc-Easy | BoolQ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MHA | 79.8 | 76.1 | 47.5 | 72.8 | 76.0 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| MHA | 79.8 | 76.1 | 47.5 | 72.8 | 76.0 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DejaVu-10% | -3.9 | -4.7 | -5.78 | -3.18 | -7.4 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| DejaVu-10% | -3.9 | -4.7 | -5.78 | -3.18 | -7.4 |'
- en: '| DejaVu-30% | -13.3 | -18.6 | -18.75 | -4.2 | -20.2 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| DejaVu-30% | -13.3 | -18.6 | -18.75 | -4.2 | -20.2 |'
- en: '| DejaVu-50% | -24.6 | -50.7 | -19.35 | -46.3 | -21.6 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| DejaVu-50% | -24.6 | -50.7 | -19.35 | -46.3 | -21.6 |'
- en: '| SpAtten | -41.4 | -42.5 | -18.0 | -40.2 | -27.1 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| SpAtten | -41.4 | -42.5 | -18.0 | -40.2 | -27.1 |'
- en: '| CHAI-static | -4.0 | -4.3 | -3.7 | -2.5 | -0.8 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| CHAI-static | -4.0 | -4.3 | -3.7 | -2.5 | -0.8 |'
- en: '| CHAI | -2.0 | -3.2 | -0.5 | 0.3 | 0.1 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| CHAI | -2.0 | -3.2 | -0.5 | 0.3 | 0.1 |'
- en: 'Table 3: Accuracy on LLaMa-33B'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: LLaMa-33B 的准确率'
- en: '| Method | PIQA | HellaSwag | Arc-Challenge | Arc-Easy | BoolQ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PIQA | HellaSwag | Arc-Challenge | Arc-Easy | BoolQ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MHA | 82.1 | 82.8 | 57.8 | 80.0 | 83.1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MHA | 82.1 | 82.8 | 57.8 | 80.0 | 83.1 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| DejaVu-10% | -0.7 | 0.1 | -0.2 | -0.6 | -0.2 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| DejaVu-10% | -0.7 | 0.1 | -0.2 | -0.6 | -0.2 |'
- en: '| DejaVu-30% | -9.3 | -24.4 | -17.91 | -12.4 | -12.2 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| DejaVu-30% | -9.3 | -24.4 | -17.91 | -12.4 | -12.2 |'
- en: '| DejaVu-50% | -27.6 | -43.2 | -24.6 | -37.6 | -21.2 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| DejaVu-50% | -27.6 | -43.2 | -24.6 | -37.6 | -21.2 |'
- en: '| SpAtten | -31.9 | -44.1 | -26.4 | -40.3 | -34.55 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| SpAtten | -31.9 | -44.1 | -26.4 | -40.3 | -34.55 |'
- en: '| CHAI-static | -0.5 | -0.2 | -1.3 | -3.7 | -1.5 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| CHAI-static | -0.5 | -0.2 | -1.3 | -3.7 | -1.5 |'
- en: '| CHAI | 0 | -0.14 | -0.21 | 0.9 | -0.04 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CHAI | 0 | -0.14 | -0.21 | 0.9 | -0.04 |'
- en: 4.3 Reduction in K,V Cache Memory Requirement
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 K,V 缓存内存需求的减少
- en: 'In this section, we study the memory capacity reduction achieved by use of
    CHAI due to reduction in K,V cache size. In Figure [11](#S4.F11 "Figure 11 ‣ 4.3
    Reduction in K,V Cache Memory Requirement ‣ 4 Evaluation ‣ CHAI: Clustered Head
    Attention for Efficient LLM Inference"), we show that for LLaMa-7B CHAI reduces
    the size of K,V cache by up to 21.4% compared to MHA. Even for comparatively small
    models like LLaMa-7B, the size of the K,V cache for a sequence length of 2048
    is around 1.2 GB, while around 12 GB is used for the model weights. A reduction
    in K,V cache size can enable use of larger context length or serving more requests.
    We would also like to note that as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ CHAI: Clustered Head Attention for Efficient LLM Inference"), CHAI only removes
    the keys associated with redundant heads and keeps all the value vectors.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们研究了由于K,V缓存大小减少而通过使用CHAI实现的内存容量减少。在图[11](#S4.F11 "图11 ‣ 4.3 K,V缓存内存需求的减少
    ‣ 4 评估 ‣ CHAI: 高效LLM推理的聚类头注意力")中，我们展示了对于LLaMa-7B，CHAI相比MHA减少了最多21.4%的K,V缓存大小。即使对于像LLaMa-7B这样的相对较小的模型，在序列长度为2048时，K,V缓存的大小大约为1.2
    GB，而模型权重则使用了约12 GB。K,V缓存大小的减少可以使得使用更大的上下文长度或服务更多的请求。我们还要指出，如图[3](#S1.F3 "图3 ‣
    1 介绍 ‣ CHAI: 高效LLM推理的聚类头注意力")所示，CHAI仅移除了与冗余头相关的键，并保留了所有的值向量。'
- en: '![Refer to caption](img/eba81434fda3380d27d4cd93b10de364.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/eba81434fda3380d27d4cd93b10de364.png)'
- en: 'Figure 11: Memory Savings: We observed that for LLaMa-7B CHAI provides memory
    savings of up to 21.4%.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：内存节省：我们观察到对于LLaMa-7B，CHAI提供了最高21.4%的内存节省。
- en: '![Refer to caption](img/49c944ca7c0968245e6c2707c4e6dca6.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/49c944ca7c0968245e6c2707c4e6dca6.png)'
- en: '(a) Time to first token: We observe speedups of upto 1.73$\times$ for sequence
    length of 2048\.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第一个标记的时间：我们观察到在序列长度为2048时，速度提高了最高1.73$\times$。
- en: '![Refer to caption](img/2bcba4a4e13f4a53f0875ac412258e50.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2bcba4a4e13f4a53f0875ac412258e50.png)'
- en: '(b) Time to next token: We observe a speedup of upto $5\times$ for sequence
    length of 2048\.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 下一标记的时间：我们观察到在序列长度为2048时，速度提高了最高可达$5\times$。
- en: 'Figure 12: Latency Analysis: We observe that the speedups provided by CHAI
    increases as the sequence length becomes larger. Even for a comparatively small
    model like LLaMa-7B we observe speedups of up to 1.73$\times$ for a large sequence
    length.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：延迟分析：我们观察到随着序列长度的增加，CHAI提供的加速也在增加。即使对于像LLaMa-7B这样相对较小的模型，我们也观察到了在较大序列长度下最高1.73$\times$的加速。
- en: 4.4 End-to-End Latency
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 端到端延迟
- en: Next, we evaluate time to first token and time to next token comparing it with
    MHA. These are two standard metrics used for evaluation of an LLM. Time to first
    token evaluates the time for generating a first token given a new context. Time
    to first token accounts for generating K,V caches for all the tokens in the context.
    Whereas time to next token evaluates the time for generating the next token, assuming
    the K,V caches for all internal tokens is available.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较了第一个标记的时间和下一标记的时间与MHA的差异。这两个是用于评估LLM的标准指标。第一个标记的时间评估了在给定新上下文时生成第一个标记的时间。第一个标记的时间包括生成所有上下文中标记的K,V缓存。而下一标记的时间评估了生成下一个标记的时间，假设所有内部标记的K,V缓存是可用的。
- en: 'Time to first token. Next, in our experiments we compare the speedups provided
    by CHAI. In Figure [12](#S4.F12 "Figure 12 ‣ 4.3 Reduction in K,V Cache Memory
    Requirement ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient LLM
    Inference")-(a) for LLaMa-7B we show that our method provides speedup of up to
    $1.72\times$ on a sequence length of 2048. The execution times represented in
    this figure accounts for the overhead of clustering in CHAI.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个标记的时间。接下来，在我们的实验中，我们比较了CHAI提供的速度提升。在图[12](#S4.F12 "图12 ‣ 4.3 K,V缓存内存需求的减少
    ‣ 4 评估 ‣ CHAI: 高效LLM推理的聚类头注意力")-(a)中，我们展示了对于LLaMa-7B，我们的方法在序列长度为2048时提供了最高$1.72\times$的加速。此图中表示的执行时间考虑了CHAI中的聚类开销。'
- en: 'Time to next token. Another metric for evaluation of LLMs is time to next token.
    We do not account for the overhead of clustering in the case of time to next token.
    Our primary wins come from reducing compute and reducing memory bandwidth requirement
    for performing time to next token. Figure [12](#S4.F12 "Figure 12 ‣ 4.3 Reduction
    in K,V Cache Memory Requirement ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference")-(b) shows time to predict the next token for different
    sequence lengths. We observe that CHAI provides a speedup of over $5\times$ for
    a sequence length of 2048.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个标记的时间。另一个评估LLM的指标是下一个标记的时间。我们在计算下一个标记的时间时没有考虑集群的开销。我们主要的优势来自于减少计算和减少执行下一个标记所需的内存带宽。图[12](#S4.F12
    "图 12 ‣ 4.3 K,V缓存内存需求的减少 ‣ 4 评估 ‣ CHAI：用于高效LLM推理的集群头注意力")-(b)显示了不同序列长度下预测下一个标记的时间。我们观察到，对于2048的序列长度，CHAI提供了超过$5\times$的加速。
- en: Unfortunately, we are not able to compare times with DejaVu as the authors have
    not released the specialized kernels used for realizing the speedups on hardware (git,
    [2024](#bib.bib1)), thus inhibiting a runtime comparison. However, we believe
    it is unlikely that at less than 10% sparsity which is needed by DejaVu to get
    comparable accuracy to MHA, it will yield high speedups (Hooker, [2021](#bib.bib23)).
    We would like to highlight that because of performing dense computations, unlike
    DejaVu, CHAI does not need custom GPU kernels. Further, CHAI’s speedup benefits
    are independent of the framework used, because irrespective of implementation,
    CHAI directly reduces the complexity of MHA.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们无法与DejaVu进行时间比较，因为作者未发布用于在硬件上实现加速的专用内核（git，[2024](#bib.bib1)），因此无法进行运行时比较。然而，我们认为在DejaVu需要的低于10%的稀疏度下，它不太可能获得与MHA相当的高加速（Hooker，[2021](#bib.bib23)）。我们想强调的是，由于执行密集计算，与DejaVu不同，CHAI不需要自定义GPU内核。此外，CHAI的加速收益与所使用的框架无关，因为无论实现如何，CHAI直接减少了MHA的复杂性。
- en: '![Refer to caption](img/04b86cf12f20160be9ac4380f53aef3f.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04b86cf12f20160be9ac4380f53aef3f.png)'
- en: 'Figure 13: Cluster Distribution: We observe that number of heads within the
    cluster is quite skewed. We often observe one or two large clusters, while the
    remaining heads in the cluster.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：集群分布：我们观察到集群内的头部数量相当不均衡。我们经常会观察到一个或两个大的集群，而其余的头部则分布在集群中。
- en: 4.5 Additional Experiments
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 额外实验
- en: Next we perform additional studies on our algorithm.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对我们的算法进行额外的研究。
- en: 'Pruning K, Q and V. In CHAI, we prune only the Key and Query portion of an
    attention head leaving the Value vector intact. Next, we study how accuracy changes
    if we remove the value vector as well. To perform this experiment we chose to
    reuse the value vector generated by the chosen head. In Table [4](#S4.T4 "Table
    4 ‣ 4.5 Additional Experiments ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention
    for Efficient LLM Inference"), we show how reusing the full head (Query, Key and
    Value vector) lead to additional loss in accuracy. This shows that for smaller
    networks like LLaMa it might be hard to remove the whole head in Multi-Head Attention.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪K、Q和V。在CHAI中，我们仅修剪注意力头的Key和Query部分，保持Value向量不变。接下来，我们研究如果我们也移除Value向量，准确性会如何变化。为了进行这个实验，我们选择重用由选择的头生成的Value向量。在表[4](#S4.T4
    "表 4 ‣ 4.5 额外实验 ‣ 4 评估 ‣ CHAI：用于高效LLM推理的集群头注意力")中，我们展示了重用完整头部（Query、Key和Value向量）如何导致额外的准确性损失。这表明对于像LLaMa这样较小的网络，可能很难在多头注意力中移除整个头部。
- en: 'Table 4: Pruning Both Q,K,V'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：修剪Q、K、V
- en: '|  | CHAI | CHAI-QKV | MHA |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | CHAI | CHAI-QKV | MHA |'
- en: '| Arc-Challenge | 47.0 | 41.29 | 47.5 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Arc-Challenge | 47.0 | 41.29 | 47.5 |'
- en: '| PIQA | 77.8 | 61.93 | 79.8 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 77.8 | 61.93 | 79.8 |'
- en: 'Cluster Distribution. Figure [13](#S4.F13 "Figure 13 ‣ 4.4 End-to-End Latency
    ‣ 4 Evaluation ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    shows the distribution across clusters for Layer-18 on LLaMa-7B for different
    1024 samples of C4 dataset. We observe that typically for LLMs majority of heads
    can be grouped into a single head.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 集群分布。图[13](#S4.F13 "图 13 ‣ 4.4 端到端延迟 ‣ 4 评估 ‣ CHAI：用于高效LLM推理的集群头注意力")显示了不同1024样本的C4数据集中Layer-18的集群分布。我们观察到通常对于LLM，大多数头部可以被分组到一个单一的头部中。
- en: 5 Conclusion
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we present CHAI, an efficient runtime method which identifies
    attention heads giving similar scores. Using this method we reduce overhead of
    Multi-Head Attention by clustering the correlated heads and computing attention
    scores only for heads which lead to disparate attention scores. Our evaluation
    shows that with minor accuracy loss system can speedup inference by up to $1.73\times$.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了 CHAI，这是一种高效的运行时方法，用于识别给出相似分数的注意力头。使用这种方法，我们通过聚类相关的头部并仅计算导致不同注意力分数的头部的注意力分数，从而减少了多头注意力的开销。我们的评估表明，系统在略微降低准确度的情况下，可以将推理速度提高到
    $1.73\times$。
- en: References
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'git (2024) Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time.
    [https://github.com/FMInference/DejaVu](https://github.com/FMInference/DejaVu),
    2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'git (2024) Deja Vu: 上下文稀疏性以提高 LLM 推理时的效率。 [https://github.com/FMInference/DejaVu](https://github.com/FMInference/DejaVu)，2024。'
- en: 'Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy,
    Y., Lebrón, F., and Sanghai, S. Gqa: Training generalized multi-query transformer
    models from multi-head checkpoints. *arXiv preprint arXiv:2305.13245*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy,
    Y., Lebrón, F., 和 Sanghai, S. GQA：从多头检查点训练通用多查询变换器模型。*arXiv 预印本 arXiv:2305.13245*，2023。
- en: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning
    about physical commonsense in natural language. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 34, pp.  7432–7439, 2020.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., 等. PIQA: 关于自然语言中的物理常识推理。在*AAAI人工智能会议论文集*，第34卷，第7432–7439页，2020。'
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Advances in neural information processing systems*,
    33:1877–1901, 2020.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.
    D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., 等. 语言模型是少量示例学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: Chen et al. (2020a) Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang,
    Z., and Carbin, M. The lottery ticket hypothesis for pre-trained bert networks.
    *Advances in neural information processing systems*, 33:15834–15846, 2020a.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020a) Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang,
    Z., 和 Carbin, M. 预训练 BERT 网络的抽签票假设。*神经信息处理系统进展*，33:15834–15846，2020a。
- en: 'Chen et al. (2020b) Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., and Liu,
    J. Earlybert: Efficient bert training via early-bird lottery tickets. *arXiv preprint
    arXiv:2101.00063*, 2020b.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2020b) Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., 和 Liu,
    J. EarlyBERT: 通过早鸟抽签票进行高效的 BERT 训练。*arXiv 预印本 arXiv:2101.00063*，2020b。'
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., 和 Toutanova, K. BoolQ: 探索自然是/否问题的惊人难度。*arXiv 预印本 arXiv:1905.10044*，2019。'
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*, 2018.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., 和 Tafjord, O. 认为你已经解决了问题回答？试试 ARC，AI2 推理挑战。*arXiv 预印本 arXiv:1803.05457*，2018。
- en: Dai et al. (2023) Dai, S., Genc, H., Venkatesan, R., and Khailany, B. Efficient
    transformer inference with statically structured sparse attention. In *2023 60th
    ACM/IEEE Design Automation Conference (DAC)*, pp.  1–6\. IEEE, 2023.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2023) Dai, S., Genc, H., Venkatesan, R., 和 Khailany, B. 具有静态结构稀疏注意力的高效变换器推理。在*2023年第60届ACM/IEEE设计自动化会议（DAC）*，第1–6页。IEEE，2023。
- en: 'Dao (2023) Dao, T. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *arXiv preprint arXiv:2307.08691*, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao (2023) Dao, T. FlashAttention-2: 更快的注意力机制，具有更好的并行性和工作分配。*arXiv 预印本 arXiv:2307.08691*，2023。'
- en: 'Dao et al. (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. *Advances in Neural
    Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao et al. (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., 和 Ré, C. FlashAttention:
    快速且内存高效的确切注意力，具有 IO 认知。*神经信息处理系统进展*，35:16344–16359，2022。'
- en: Dettmers (2015) Dettmers, T. 8-bit approximations for parallelism in deep learning.
    *arXiv preprint arXiv:1511.04561*, 2015.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers (2015) Dettmers, T. 深度学习中的并行性8位近似。*arXiv 预印本 arXiv:1511.04561*，2015。
- en: 'Dettmers & Zettlemoyer (2023) Dettmers, T. and Zettlemoyer, L. The case for
    4-bit precision: k-bit inference scaling laws. In *International Conference on
    Machine Learning*, pp.  7750–7774\. PMLR, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers & Zettlemoyer (2023) Dettmers, T. 和 Zettlemoyer, L. 4位精度的理由：k位推理扩展定律。在
    *国际机器学习大会*，第7750–7774页。PMLR，2023年。
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*, 2022.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer,
    L. Llm. int8 (): 大规模变换器的8位矩阵乘法。 *arXiv 预印本 arXiv:2208.07339*，2022年。'
- en: 'facebookresearch (2023) facebookresearch. xformers - toolbox to accelerate
    research on transformers. [https://github.com/facebookresearch/xformers](https://github.com/facebookresearch/xformers),
    2023. Accessed: December 12, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: facebookresearch (2023) facebookresearch. xformers - 加速变换器研究的工具箱。 [https://github.com/facebookresearch/xformers](https://github.com/facebookresearch/xformers)，2023。访问日期：2023年12月12日。
- en: Fan et al. (2019) Fan, A., Grave, E., and Joulin, A. Reducing transformer depth
    on demand with structured dropout. *arXiv preprint arXiv:1909.11556*, 2019.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan et al. (2019) Fan, A., Grave, E., 和 Joulin, A. 根据需求减少变换器深度的结构性丢弃。 *arXiv
    预印本 arXiv:1909.11556*，2019年。
- en: 'Fang et al. (2022) Fang, C., Zhou, A., and Wang, Z. An algorithm–hardware co-optimized
    framework for accelerating n: M sparse transformers. *IEEE Transactions on Very
    Large Scale Integration (VLSI) Systems*, 30(11):1573–1586, 2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang et al. (2022) Fang, C., Zhou, A., 和 Wang, Z. 一个算法–硬件共同优化的框架，用于加速 n: M
    稀疏变换器。 *IEEE 大规模集成 (VLSI) 系统汇刊*，30(11):1573–1586，2022年。'
- en: 'Frankle & Carbin (2018) Frankle, J. and Carbin, M. The lottery ticket hypothesis:
    Finding sparse, trainable neural networks. *arXiv preprint arXiv:1803.03635*,
    2018.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle & Carbin (2018) Frankle, J. 和 Carbin, M. 彩票假设：发现稀疏、可训练的神经网络。 *arXiv
    预印本 arXiv:1803.03635*，2018年。
- en: 'Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D.
    Gptq：生成预训练变换器的准确后训练量化。 *arXiv 预印本 arXiv:2210.17323*，2022年。
- en: 'Ham et al. (2020) Ham, T. J., Jung, S. J., Kim, S., Oh, Y. H., Park, Y., Song,
    Y., Park, J.-H., Lee, S., Park, K., Lee, J. W., et al. A^ 3: Accelerating attention
    mechanisms in neural networks with approximation. In *2020 IEEE International
    Symposium on High Performance Computer Architecture (HPCA)*, pp.  328–341\. IEEE,
    2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ham et al. (2020) Ham, T. J., Jung, S. J., Kim, S., Oh, Y. H., Park, Y., Song,
    Y., Park, J.-H., Lee, S., Park, K., Lee, J. W., 等。A^ 3：通过近似加速神经网络中的注意力机制。在 *2020
    IEEE 国际高性能计算机体系结构研讨会 (HPCA)*，第328–341页。IEEE，2020年。
- en: 'Ham et al. (2021) Ham, T. J., Lee, Y., Seo, S. H., Kim, S., Choi, H., Jung,
    S. J., and Lee, J. W. Elsa: Hardware-software co-design for efficient, lightweight
    self-attention mechanism in neural networks. In *2021 ACM/IEEE 48th Annual International
    Symposium on Computer Architecture (ISCA)*, pp.  692–705\. IEEE, 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ham et al. (2021) Ham, T. J., Lee, Y., Seo, S. H., Kim, S., Choi, H., Jung,
    S. J., 和 Lee, J. W. Elsa：用于高效、轻量级自注意力机制的硬件-软件协同设计。在 *2021 ACM/IEEE 第48届年度计算机体系结构国际研讨会
    (ISCA)*，第692–705页。IEEE，2021年。
- en: Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya,
    E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark,
    A., et al. Training compute-optimal large language models. *arXiv preprint arXiv:2203.15556*,
    2022.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya,
    E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark,
    A., 等。训练计算最优的大型语言模型。 *arXiv 预印本 arXiv:2203.15556*，2022年。
- en: Hooker (2021) Hooker, S. The hardware lottery. *Communications of the ACM*,
    64(12):58–65, 2021.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hooker (2021) Hooker, S. 硬件彩票。 *ACM 通讯*，64(12):58–65，2021年。
- en: Hsieh et al. (2023) Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii,
    Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T. Distilling step-by-step!
    outperforming larger language models with less training data and smaller model
    sizes. *arXiv preprint arXiv:2305.02301*, 2023.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh et al. (2023) Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii,
    Y., Ratner, A., Krishna, R., Lee, C.-Y., 和 Pfister, T. 一步步提炼！用更少的训练数据和更小的模型尺寸超越更大的语言模型。
    *arXiv 预印本 arXiv:2305.02301*，2023年。
- en: 'Jiao et al. (2019) Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L.,
    Wang, F., and Liu, Q. Tinybert: Distilling bert for natural language understanding.
    *arXiv preprint arXiv:1909.10351*, 2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao et al. (2019) Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L.,
    Wang, F., 和 Liu, Q. Tinybert：为自然语言理解提炼bert。 *arXiv 预印本 arXiv:1909.10351*，2019年。
- en: Kaplan et al. (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
    Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws
    for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan 等（2020）Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess,
    B., Child, R., Gray, S., Radford, A., Wu, J., 和 Amodei, D. 神经语言模型的扩展规律。*arXiv
    预印本 arXiv:2001.08361*，2020。
- en: 'Kim et al. (2021) Kim, S., Gholami, A., Yao, Z., Mahoney, M. W., and Keutzer,
    K. I-bert: Integer-only bert quantization. In *International conference on machine
    learning*, pp.  5506–5518\. PMLR, 2021.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等（2021）Kim, S., Gholami, A., Yao, Z., Mahoney, M. W., 和 Keutzer, K. I-bert:
    仅整数的 bert 量化。发表于 *国际机器学习会议*，第5506–5518页。PMLR，2021。'
- en: 'Kitaev et al. (2020) Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The
    efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kitaev 等（2020）Kitaev, N., Kaiser, Ł., 和 Levskaya, A. Reformer: 高效的变换器。*arXiv
    预印本 arXiv:2001.04451*，2020。'
- en: 'Kurtic et al. (2022) Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz,
    M., Fineran, B., Goin, M., and Alistarh, D. The optimal bert surgeon: Scalable
    and accurate second-order pruning for large language models. *arXiv preprint arXiv:2203.07259*,
    2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic 等（2022）Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M., Fineran,
    B., Goin, M., 和 Alistarh, D. 最优 bert 外科医生：适用于大语言模型的可扩展且准确的二阶修剪。*arXiv 预印本 arXiv:2203.07259*，2022。
- en: Lagunas et al. (2021) Lagunas, F., Charlaix, E., Sanh, V., and Rush, A. M. Block
    pruning for faster transformers. *arXiv preprint arXiv:2109.04838*, 2021.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagunas 等（2021）Lagunas, F., Charlaix, E., Sanh, V., 和 Rush, A. M. 更快的变换器的块修剪。*arXiv
    预印本 arXiv:2109.04838*，2021。
- en: Leviathan et al. (2023) Leviathan, Y., Kalman, M., and Matias, Y. Fast inference
    from transformers via speculative decoding. In *International Conference on Machine
    Learning*, pp.  19274–19286\. PMLR, 2023.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan 等（2023）Leviathan, Y., Kalman, M., 和 Matias, Y. 通过推测解码实现变换器的快速推理。在
    *国际机器学习会议*，第19274–19286页。PMLR，2023。
- en: 'Liu et al. (2023a) Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z.,
    Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of
    importance hypothesis for llm kv cache compression at test time. *arXiv preprint
    arXiv:2305.17118*, 2023a.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023a）Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis,
    A., 和 Shrivastava, A. Scissorhands: 在测试时利用重要性假设的持久性进行 LLM KV 缓存压缩。*arXiv 预印本 arXiv:2305.17118*，2023a。'
- en: 'Liu et al. (2023b) Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,
    Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity
    for efficient llms at inference time. In *International Conference on Machine
    Learning*, pp.  22137–22176\. PMLR, 2023b.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023b）Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava,
    A., Zhang, C., Tian, Y., Re, C., 等. Deja vu: 推理时高效 LLM 的上下文稀疏性。在 *国际机器学习会议*，第22137–22176页。PMLR，2023b。'
- en: Michel et al. (2019) Michel, P., Levy, O., and Neubig, G. Are sixteen heads
    really better than one? *Advances in neural information processing systems*, 32,
    2019.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michel 等（2019）Michel, P., Levy, O., 和 Neubig, G. 十六个头真的比一个头好吗？ *神经信息处理系统进展*，32，2019。
- en: Prasanna et al. (2020) Prasanna, S., Rogers, A., and Rumshisky, A. When bert
    plays the lottery, all tickets are winning. *arXiv preprint arXiv:2005.00561*,
    2020.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prasanna 等（2020）Prasanna, S., Rogers, A., 和 Rumshisky, A. 当 bert 参与彩票时，所有票据都是中奖的。*arXiv
    预印本 arXiv:2005.00561*，2020。
- en: 'Qin et al. (2023) Qin, Y., Wang, Y., Deng, D., Zhao, Z., Yang, X., Liu, L.,
    Wei, S., Hu, Y., and Yin, S. Fact: Ffn-attention co-optimized transformer architecture
    with eager correlation prediction. In *Proceedings of the 50th Annual International
    Symposium on Computer Architecture*, pp.  1–14, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等（2023）Qin, Y., Wang, Y., Deng, D., Zhao, Z., Yang, X., Liu, L., Wei, S.,
    Hu, Y., 和 Yin, S. FACT: FFN-注意力共同优化的变换器架构与急切的相关性预测。在 *第50届年度国际计算机架构研讨会*，第1–14页，2023。'
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    Sutskever, I., et al. Language models are unsupervised multitask learners. *OpenAI
    blog*, 1(8):9, 2019.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever,
    I., 等. 语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9，2019。
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485–5551, 2020.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J. 探索统一文本到文本变换器的迁移学习极限。*机器学习研究期刊*，21(1):5485–5551，2020。
- en: Sajjad et al. (2023) Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the
    effect of dropping layers of pre-trained transformer models. *Computer Speech
    & Language*, 77:101429, 2023.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sajjad 等（2023）Sajjad, H., Dalvi, F., Durrani, N., 和 Nakov, P. 预训练变换器模型去除层数的效果。*计算机语音与语言*，77:101429，2023。
- en: 'Sanh et al. (2019) Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. *arXiv preprint
    arXiv:1910.01108*, 2019.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh et al. (2019) Sanh, V., Debut, L., Chaumond, J., 和 Wolf, T. Distilbert，BERT的蒸馏版：更小、更快、更便宜、更轻量。*arXiv
    预印本 arXiv:1910.01108*，2019。
- en: 'Shazeer (2019) Shazeer, N. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer (2019) Shazeer, N. 快速变压器解码：一个写头就够了。*arXiv 预印本 arXiv:1911.02150*，2019。
- en: 'Shen et al. (2020) Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A.,
    Mahoney, M. W., and Keutzer, K. Q-bert: Hessian based ultra low precision quantization
    of bert. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34,
    pp.  8815–8821, 2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2020) Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A.,
    Mahoney, M. W., 和 Keutzer, K. Q-bert：基于 Hessian 的超低精度 BERT 量化。*AAAI 人工智能会议论文集*，第34卷，第8815–8821页，2020。
- en: 'Tambe et al. (2021) Tambe, T., Hooper, C., Pentecost, L., Jia, T., Yang, E.-Y.,
    Donato, M., Sanh, V., Whatmough, P., Rush, A. M., Brooks, D., et al. Edgebert:
    Sentence-level energy optimizations for latency-aware multi-task nlp inference.
    In *MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture*,
    pp.  830–844, 2021.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tambe et al. (2021) Tambe, T., Hooper, C., Pentecost, L., Jia, T., Yang, E.-Y.,
    Donato, M., Sanh, V., Whatmough, P., Rush, A. M., Brooks, D., 等. Edgebert：面向延迟感知的多任务
    NLP 推理的句子级能效优化。*MICRO-54：第54届年度 IEEE/ACM 国际微架构研讨会*，第830–844页，2021。
- en: Thorndike (1953) Thorndike, R. L. Who belongs in the family? *Psychometrika*,
    18(4):267–276, 1953.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thorndike (1953) Thorndike, R. L. 谁属于这个家庭？*心理测量学*，18(4)：267–276，1953。
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023a.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等. Llama：开放且高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023a。
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023b.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等. Llama
    2：开放的基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. *Advances
    in neural information processing systems*, 30, 2017.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I. 注意力即一切。*神经信息处理系统进展*，30，2017。
- en: 'Voita et al. (2019) Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov,
    I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting,
    the rest can be pruned. *arXiv preprint arXiv:1905.09418*, 2019.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita et al. (2019) Voita, E., Talbot, D., Moiseev, F., Sennrich, R., 和 Titov,
    I. 分析多头自注意力：专门化头部承担主要工作，其余可以被修剪。*arXiv 预印本 arXiv:1905.09418*，2019。
- en: 'Waleffe & Rekatsinas (2020) Waleffe, R. and Rekatsinas, T. Principal component
    networks: Parameter reduction early in training. *arXiv preprint arXiv:2006.13347*,
    2020.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Waleffe & Rekatsinas (2020) Waleffe, R. 和 Rekatsinas, T. 主成分网络：训练初期的参数减少。*arXiv
    预印本 arXiv:2006.13347*，2020。
- en: 'Wang et al. (2021a) Wang, H., Agarwal, S., and Papailiopoulos, D. Pufferfish:
    Communication-efficient models at no extra cost. *Proceedings of Machine Learning
    and Systems*, 3:365–386, 2021a.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) Wang, H., Agarwal, S., 和 Papailiopoulos, D. Pufferfish：无额外成本的通信高效模型。*机器学习与系统会议论文集*，3:365–386，2021a。
- en: 'Wang et al. (2021b) Wang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse
    attention architecture with cascade token and head pruning. In *2021 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA)*, pp.  97–110\. IEEE,
    2021b.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021b) Wang, H., Zhang, Z., 和 Han, S. Spatten：具有级联令牌和头部修剪的高效稀疏注意力架构。*2021
    IEEE 国际高性能计算机架构研讨会 (HPCA)*，第97–110页，IEEE，2021b。
- en: 'Wang et al. (2023) Wang, H., Agarwal, S., Tanaka, Y., Xing, E., Papailiopoulos,
    D., et al. Cuttlefish: Low-rank model training without all the tuning. *Proceedings
    of Machine Learning and Systems*, 5, 2023.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Wang, H., Agarwal, S., Tanaka, Y., Xing, E., Papailiopoulos,
    D., 等. Cutlefish：无需所有调优的低秩模型训练。*机器学习与系统会议论文集*，5，2023。
- en: 'Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer:
    Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*, 2020.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., 和 Ma, H. Linformer：具有线性复杂度的自注意力。*arXiv
    预印本 arXiv:2006.04768*，2020。
- en: Wang et al. (2019) Wang, Z., Wohlwend, J., and Lei, T. Structured pruning of
    large language models. *arXiv preprint arXiv:1910.04732*, 2019.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2019) Wang, Z., Wohlwend, J., 和 Lei, T. 大型语言模型的结构化剪枝。*arXiv 预印本 arXiv:1910.04732*，2019。
- en: 'Xia et al. (2023) Xia, H., Ge, T., Wang, P., Chen, S.-Q., Wei, F., and Sui,
    Z. Speculative decoding: Exploiting speculative execution for accelerating seq2seq
    generation. In *Findings of the Association for Computational Linguistics: EMNLP
    2023*, pp.  3909–3925, 2023.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等 (2023) Xia, H., Ge, T., Wang, P., Chen, S.-Q., Wei, F., 和 Sui, Z. 投机解码：利用投机执行加速
    seq2seq 生成。在 *计算语言学协会发现：EMNLP 2023*，第 3909–3925 页，2023。
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In *International Conference on Machine Learning*, pp.  38087–38099\.
    PMLR, 2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等 (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han, S.
    Smoothquant: 对大型语言模型进行准确且高效的后训练量化。在 *国际机器学习会议*，第 38087–38099 页。PMLR，2023。'
- en: 'Yang et al. (2023) Yang, S., Lee, G., Cho, J., Papailiopoulos, D., and Lee,
    K. Predictive pipelined decoding: A compute-latency trade-off for exact llm decoding.
    *arXiv preprint arXiv:2307.05908*, 2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 (2023) Yang, S., Lee, G., Cho, J., Papailiopoulos, D., 和 Lee, K. 预测性流水线解码：精确
    LLM 解码的计算与延迟权衡。*arXiv 预印本 arXiv:2307.05908*，2023。
- en: 'You et al. (2019) You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk,
    R. G., Wang, Z., and Lin, Y. Drawing early-bird tickets: Towards more efficient
    training of deep networks. *arXiv preprint arXiv:1909.11957*, 2019.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等 (2019) You, H., Li, C., Xu, P., Fu, Y., Wang, Y., Chen, X., Baraniuk,
    R. G., Wang, Z., 和 Lin, Y. 提前购票：朝着更高效的深度网络训练迈进。*arXiv 预印本 arXiv:1909.11957*，2019。
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等 (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., 和 Choi,
    Y. Hellaswag: 机器真的能完成你的句子吗？ *arXiv 预印本 arXiv:1905.07830*，2019。'
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained
    transformer language models. *arXiv preprint arXiv:2205.01068*, 2022.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen,
    S., Dewan, C., Diab, M., Li, X., Lin, X. V., 等. Opt: 开放预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022。'
- en: 'Zhang et al. (2023) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. H $\_2$ o: Heavy-hitter oracle
    for efficient generative inference of large language models. *arXiv preprint arXiv:2306.14048*,
    2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2023) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R.,
    Song, Z., Tian, Y., Ré, C., Barrett, C., 等. H $\_2$ o: 大型语言模型高效生成推理的重击预言者。*arXiv
    预印本 arXiv:2306.14048*，2023。'
- en: 'Zhou et al. (2023) Zhou, Y., Du, N., Huang, Y., Peng, D., Lan, C., Huang, D.,
    Shakeri, S., So, D., Dai, A. M., Lu, Y., et al. Brainformers: Trading simplicity
    for efficiency. In *International Conference on Machine Learning*, pp.  42531–42542\.
    PMLR, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 (2023) Zhou, Y., Du, N., Huang, Y., Peng, D., Lan, C., Huang, D., Shakeri,
    S., So, D., Dai, A. M., Lu, Y., 等. Brainformers: 以简单换取效率。在 *国际机器学习会议*，第 42531–42542
    页。PMLR，2023。'
- en: Appendix A Additional Plots
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 额外图表
- en: A.1 Accuracy and Inference Time Trade-off
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 准确率与推理时间权衡
- en: '![Refer to caption](img/46fda07b9f008d1d2c31329c351008a5.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/46fda07b9f008d1d2c31329c351008a5.png)'
- en: 'Figure 14: Accuracy vs Inference Time for LLaMa-7B: We study various methods
    of clustering attention heads, and plot the runtime for sequence length of 2048\.
    For random head selection we randomly choose heads to combine together in increasing
    number of 4, 8, 16 and 24\. For Static Head selection we choose the heads in increasing
    order of 4,8,16, and 24 based on activation analysis of activation on C4 dataset (Raffel
    et al., [2020](#bib.bib38)).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：LLaMa-7B 的准确率与推理时间：我们研究了各种聚类注意力头的方法，并绘制了序列长度为 2048 的运行时间。对于随机头选择，我们随机选择头进行组合，数量依次增加为
    4、8、16 和 24。对于静态头选择，我们根据 C4 数据集的激活分析，选择头的数量依次增加为 4、8、16 和 24 (Raffel 等，[2020](#bib.bib38))。
- en: A.2 OPT-66B Activation Plots
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 OPT-66B 激活图
- en: 'From Figure [15](#A1.F15 "Figure 15 ‣ A.3 LLaMa-7B Activation Plots ‣ Appendix
    A Additional Plots ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    to Figure [20](#A1.F20 "Figure 20 ‣ A.3 LLaMa-7B Activation Plots ‣ Appendix A
    Additional Plots ‣ CHAI: Clustered Head Attention for Efficient LLM Inference")
    shows the activation plots for all layers of OPT. We consistently observe that
    for this examples there is high amount of.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '从图 [15](#A1.F15 "图 15 ‣ A.3 LLaMa-7B 激活图 ‣ 附录 A 额外图 ‣ CHAI: 集群头注意力以实现高效的 LLM
    推理") 到图 [20](#A1.F20 "图 20 ‣ A.3 LLaMa-7B 激活图 ‣ 附录 A 额外图 ‣ CHAI: 集群头注意力以实现高效的
    LLM 推理") 显示了 OPT 所有层的激活图。我们一致观察到，对于这些例子，存在大量的。'
- en: A.3 LLaMa-7B Activation Plots
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 LLaMa-7B 激活图
- en: '![Refer to caption](img/d5faeab550796e6b4d64a88e449aae0a.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d5faeab550796e6b4d64a88e449aae0a.png)'
- en: i Layer 0
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: i 层 0
- en: '![Refer to caption](img/8bcb529b04994632372fbb38c9af2b3c.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8bcb529b04994632372fbb38c9af2b3c.png)'
- en: ii Layer 1
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ii 层 1
- en: '![Refer to caption](img/0113163bc21cd9204d54e9f8bd617052.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0113163bc21cd9204d54e9f8bd617052.png)'
- en: iii Layer 2
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: iii 层 2
- en: '![Refer to caption](img/8e305dce167e961667d654f728a9e5b8.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8e305dce167e961667d654f728a9e5b8.png)'
- en: iv Layer 3
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: iv 层 3
- en: '![Refer to caption](img/c5ddb779307fd4b39528098dac608732.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c5ddb779307fd4b39528098dac608732.png)'
- en: v Layer 4
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: v 层 4
- en: '![Refer to caption](img/efc312eaf99d435f722c19f356e40827.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/efc312eaf99d435f722c19f356e40827.png)'
- en: vi Layer 5
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: vi 层 5
- en: 'Figure 15: Activations for OPT-66B'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: OPT-66B 的激活'
- en: i Layer 6
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: i 层 6
- en: '![Refer to caption](img/c5270bbcbc5cf76234b1338ddca1b2d5.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c5270bbcbc5cf76234b1338ddca1b2d5.png)'
- en: ii Layer 7
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ii 层 7
- en: '![Refer to caption](img/ef8bfc3317956cb5eed9c6f7a2ba9ab1.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ef8bfc3317956cb5eed9c6f7a2ba9ab1.png)'
- en: iii Layer 8
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: iii 层 8
- en: '![Refer to caption](img/f0b8a11cd4e306d244faddbeeeeb9bac.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f0b8a11cd4e306d244faddbeeeeb9bac.png)'
- en: iv Layer 9
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: iv 层 9
- en: '![Refer to caption](img/8651a746aefcae3c9f2d094f327b6a4f.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8651a746aefcae3c9f2d094f327b6a4f.png)'
- en: v Layer 10
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: v 层 10
- en: '![Refer to caption](img/ca2288f357270c5b3ac22f34343b453f.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ca2288f357270c5b3ac22f34343b453f.png)'
- en: vi Layer 11
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: vi 层 11
- en: 'Figure 16: Activations for OPT-66B'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: OPT-66B 的激活'
- en: '![Refer to caption](img/70b21497f5ffc7165594ac941e6392db.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/70b21497f5ffc7165594ac941e6392db.png)'
- en: i Layer 12
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: i 层 12
- en: '![Refer to caption](img/ec324385f35a2d87e86ebb55cd680729.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ec324385f35a2d87e86ebb55cd680729.png)'
- en: ii Layer 13
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ii 层 13
- en: '![Refer to caption](img/74844e074ff373f427229974deffe570.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/74844e074ff373f427229974deffe570.png)'
- en: iii Layer 14
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: iii 层 14
- en: '![Refer to caption](img/604f894ba2f2b9e2e3b662eb14824cb7.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/604f894ba2f2b9e2e3b662eb14824cb7.png)'
- en: iv Layer 15
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: iv 层 15
- en: '![Refer to caption](img/599bbc196b9f8ca142ee2f66a0213a7d.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/599bbc196b9f8ca142ee2f66a0213a7d.png)'
- en: v Layer 16
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: v 层 16
- en: '![Refer to caption](img/99580ae39c520deef478256a365f24e4.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/99580ae39c520deef478256a365f24e4.png)'
- en: vi Layer 17
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: vi 层 17
- en: '![Refer to caption](img/f968b9159f4030974781c1a01178773b.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f968b9159f4030974781c1a01178773b.png)'
- en: vii Layer 18
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: vii 层 18
- en: '![Refer to caption](img/c2fd6bc254e5f174fc04ce648d2834f6.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c2fd6bc254e5f174fc04ce648d2834f6.png)'
- en: viii Layer 19
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: viii 层 19
- en: '![Refer to caption](img/6af48198c9353beaf33b91a91a38dac4.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6af48198c9353beaf33b91a91a38dac4.png)'
- en: ix Layer 20
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ix 层 20
- en: '![Refer to caption](img/9a58bf5a0fa0676e451bc7f5e2234853.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9a58bf5a0fa0676e451bc7f5e2234853.png)'
- en: x Layer 21
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: x 层 21
- en: '![Refer to caption](img/7d72e2c08485f8e3a1ec28d052135f5e.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7d72e2c08485f8e3a1ec28d052135f5e.png)'
- en: xi Layer 22
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: xi 层 22
- en: '![Refer to caption](img/f742addb45cf76b51053382c5d58d092.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f742addb45cf76b51053382c5d58d092.png)'
- en: xii Layer 20
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: xii 层 20
- en: 'Figure 17: Activations of OPT-66B'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '图 17: OPT-66B 的激活'
- en: '![Refer to caption](img/280d6649bf0b9f09353c5ee02e22cc8e.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/280d6649bf0b9f09353c5ee02e22cc8e.png)'
- en: i Layer 24
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: i 层 24
- en: '![Refer to caption](img/2351a71d7e37385c420d0e8570b25d22.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2351a71d7e37385c420d0e8570b25d22.png)'
- en: ii Layer 25
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ii 层 25
- en: '![Refer to caption](img/b926becfd34ce5f68cf42afdb976fae8.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b926becfd34ce5f68cf42afdb976fae8.png)'
- en: iii Layer 25
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: iii 层 25
- en: '![Refer to caption](img/7bcfc0feb4e089584041f848f01f53c7.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7bcfc0feb4e089584041f848f01f53c7.png)'
- en: iv Layer 26
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: iv 层 26
- en: '![Refer to caption](img/75dd531de91cd4dc22030e4dfbc91006.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/75dd531de91cd4dc22030e4dfbc91006.png)'
- en: v Layer 27
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: v 层 27
- en: '![Refer to caption](img/f82060942ff0110c12c1e67cff41621b.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f82060942ff0110c12c1e67cff41621b.png)'
- en: vi Layer 28
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: vi 层 28
- en: '![Refer to caption](img/d28065dc29fa3ca634c80d69d9d4b4e6.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d28065dc29fa3ca634c80d69d9d4b4e6.png)'
- en: vii Layer 29
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: vii 层 29
- en: '![Refer to caption](img/81c215cdc6b18d517d148c84cb099306.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/81c215cdc6b18d517d148c84cb099306.png)'
- en: viii Layer 30
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: viii 层 30
- en: '![Refer to caption](img/3e8710d1635d50379e573dbd41af9fa7.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3e8710d1635d50379e573dbd41af9fa7.png)'
- en: ix Layer 31
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ix 层 31
- en: '![Refer to caption](img/65d676fc0f6bf7f063b0861b4be8c399.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/65d676fc0f6bf7f063b0861b4be8c399.png)'
- en: x Layer 32
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: x 层 32
- en: '![Refer to caption](img/063ed09cbae7488e495f961069755da0.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/063ed09cbae7488e495f961069755da0.png)'
- en: xi Layer 33
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: xi 层 33
- en: '![Refer to caption](img/5a02b59a5f7cf2633c488f91eaf81f2b.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5a02b59a5f7cf2633c488f91eaf81f2b.png)'
- en: xii Layer 34
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: xii 层 34
- en: '![Refer to caption](img/d0e2b6da2d4ce8d56d81e796f8455371.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d0e2b6da2d4ce8d56d81e796f8455371.png)'
- en: xiii Layer 35
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: xiii 层 35
- en: '![Refer to caption](img/79eb27ba08d0a183d9464970edd1fae2.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/79eb27ba08d0a183d9464970edd1fae2.png)'
- en: xiv Layer 36
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: xiv 层 36
- en: '![Refer to caption](img/3106b04699c877833a72716a14c74d27.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3106b04699c877833a72716a14c74d27.png)'
- en: xv Layer 37
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: xv 层 37
- en: 'Figure 18: Activations of OPT-66B'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18: OPT-66B 的激活情况'
- en: '![Refer to caption](img/47c31c9da943d43b3dedcb77285fd600.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/47c31c9da943d43b3dedcb77285fd600.png)'
- en: i Layer 38
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: i 层 38
- en: '![Refer to caption](img/54a661b6c0348392da9e2b70c1aac569.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/54a661b6c0348392da9e2b70c1aac569.png)'
- en: ii Layer 39
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ii 层 39
- en: '![Refer to caption](img/cc405149beedd86c3f6229394cafade2.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cc405149beedd86c3f6229394cafade2.png)'
- en: iii Layer 41
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: iii 层 41
- en: '![Refer to caption](img/16c68e8951360bbfcfb44a02733f3623.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/16c68e8951360bbfcfb44a02733f3623.png)'
- en: iv Layer 42
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: iv 层 42
- en: '![Refer to caption](img/f2e20919a103b81b0828bf4b010e5aa8.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f2e20919a103b81b0828bf4b010e5aa8.png)'
- en: v Layer 43
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: v 层 43
- en: '![Refer to caption](img/ca14720beca69a24f43ffacec664c4f3.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ca14720beca69a24f43ffacec664c4f3.png)'
- en: vi Layer 44
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: vi 层 44
- en: '![Refer to caption](img/bfbc79c3c7cb538a8e55e76639bdfec5.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bfbc79c3c7cb538a8e55e76639bdfec5.png)'
- en: vii Layer 45
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: vii 层 45
- en: '![Refer to caption](img/4796f509c4cd4b31911e961acb330ede.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4796f509c4cd4b31911e961acb330ede.png)'
- en: viii Layer 46
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: viii 层 46
- en: '![Refer to caption](img/dcdcdd4a14bb9dfdb96cca1b0b2c8241.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dcdcdd4a14bb9dfdb96cca1b0b2c8241.png)'
- en: ix Layer 47
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ix 层 47
- en: '![Refer to caption](img/6efec26a798cb616c5bdedee57ef32f9.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6efec26a798cb616c5bdedee57ef32f9.png)'
- en: x Layer 48
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: x 层 48
- en: '![Refer to caption](img/78c11f4ba43b9f05d811c6368fdace45.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/78c11f4ba43b9f05d811c6368fdace45.png)'
- en: xi Layer 49
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: xi 层 49
- en: '![Refer to caption](img/9803fa2268b105e3529233172fa77399.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9803fa2268b105e3529233172fa77399.png)'
- en: xii Layer 50
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: xii 层 50
- en: '![Refer to caption](img/ccff2e4a1ff0282bd004eb2246a931ec.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ccff2e4a1ff0282bd004eb2246a931ec.png)'
- en: xiii Layer 51
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: xiii 层 51
- en: '![Refer to caption](img/963565d19758efc5509476df6d162358.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/963565d19758efc5509476df6d162358.png)'
- en: xiv Layer 52
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: xiv 层 52
- en: '![Refer to caption](img/ba2cd9194b1bb8585b116458e8d1110f.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ba2cd9194b1bb8585b116458e8d1110f.png)'
- en: xv Layer 53
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: xv 层 53
- en: 'Figure 19: Activations of OPT-66B'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '图 19: OPT-66B 的激活情况'
- en: '![Refer to caption](img/cb46ba7fdb48452bfaff9ac7de68a16c.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cb46ba7fdb48452bfaff9ac7de68a16c.png)'
- en: i Layer 54
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: i 层 54
- en: '![Refer to caption](img/7852dd629cc90a6e515b6cbb50d4fd6a.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7852dd629cc90a6e515b6cbb50d4fd6a.png)'
- en: ii Layer 55
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ii 层 55
- en: '![Refer to caption](img/a57df8454fae53ce8404956233ee23df.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a57df8454fae53ce8404956233ee23df.png)'
- en: iii Layer 56
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: iii 层 56
- en: '![Refer to caption](img/5e4fdbcfd6b31ad441e2bd3a9d44d026.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5e4fdbcfd6b31ad441e2bd3a9d44d026.png)'
- en: iv Layer 57
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: iv 层 57
- en: '![Refer to caption](img/d1c6f0f66877e7787afda1980c977673.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d1c6f0f66877e7787afda1980c977673.png)'
- en: v Layer 58
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: v 层 58
- en: '![Refer to caption](img/7e02ad09e2fca7889579cba759af42d1.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7e02ad09e2fca7889579cba759af42d1.png)'
- en: vi Layer 59
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: vi 层 59
- en: '![Refer to caption](img/d779705eab57a83d6935bde8c787cf7b.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d779705eab57a83d6935bde8c787cf7b.png)'
- en: vii Layer 60
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: vii 层 60
- en: '![Refer to caption](img/949c7dd052c2900e4c4255b203257cc0.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/949c7dd052c2900e4c4255b203257cc0.png)'
- en: viii Layer 61
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: viii 层 61
- en: '![Refer to caption](img/7ecccfdb11821a1280815f1e4f7fde5e.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7ecccfdb11821a1280815f1e4f7fde5e.png)'
- en: ix Layer 62
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ix 层 62
- en: '![Refer to caption](img/03d452a78c30af96caa0642aa4aee368.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/03d452a78c30af96caa0642aa4aee368.png)'
- en: x Layer 63
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: x 层 63
- en: 'Figure 20: Layer Map OPT-66B'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '图 20: 层级图 OPT-66B'
- en: '![Refer to caption](img/8ebf1bf7640fa5915853470331b8ca79.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8ebf1bf7640fa5915853470331b8ca79.png)'
- en: (a) Layer 0
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 层 0
- en: '![Refer to caption](img/ad5b68a9228a73eee11006363aadd240.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ad5b68a9228a73eee11006363aadd240.png)'
- en: (b) Layer 1
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 层 1
- en: '![Refer to caption](img/38678d818734dc5f5b18cf3d6721323b.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/38678d818734dc5f5b18cf3d6721323b.png)'
- en: (c) Layer 2
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 层 2
- en: '![Refer to caption](img/d835812846e4281937220cde3a1197cb.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d835812846e4281937220cde3a1197cb.png)'
- en: (d) Layer 3
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 层 3
- en: '![Refer to caption](img/31ca48fa846ca66a21752f68b55fba53.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/31ca48fa846ca66a21752f68b55fba53.png)'
- en: (e) Layer 4
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 层 4
- en: '![Refer to caption](img/c57d56479793341b7abe26c88d926254.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c57d56479793341b7abe26c88d926254.png)'
- en: (f) Layer 5
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 层 5
- en: '![Refer to caption](img/9b2d6635be18c54c7c55379b6d3b674e.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9b2d6635be18c54c7c55379b6d3b674e.png)'
- en: (g) Layer 6
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 层 6
- en: '![Refer to caption](img/dcc7498c6a717ede854b215e2fa07e56.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dcc7498c6a717ede854b215e2fa07e56.png)'
- en: (h) Layer 7
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 层 7
- en: '![Refer to caption](img/1ea199eb0ff05646ee582eaec8ad1c88.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1ea199eb0ff05646ee582eaec8ad1c88.png)'
- en: (i) Layer 8
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 层 8
- en: '![Refer to caption](img/12b744eca52c6f69db8ff5f7def18732.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/12b744eca52c6f69db8ff5f7def18732.png)'
- en: (j) Layer 9
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: (j) 层 9
- en: '![Refer to caption](img/6d4ad1f86bb080d9b79c96c459074508.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6d4ad1f86bb080d9b79c96c459074508.png)'
- en: (k) Layer 10
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: (k) 层 10
- en: '![Refer to caption](img/8707a3679c12a725ce2aa0946968cf51.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8707a3679c12a725ce2aa0946968cf51.png)'
- en: (l) Layer 11
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: (l) 层 11
- en: '![Refer to caption](img/7f9641a8ccbc754066182b748bcd2e07.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7f9641a8ccbc754066182b748bcd2e07.png)'
- en: (m) Layer 12
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: (m) 层 12
- en: '![Refer to caption](img/800dafc7ca0d100b4aaae8cdae78049d.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/800dafc7ca0d100b4aaae8cdae78049d.png)'
- en: (n) Layer 13
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: (n) 层 13
- en: '![Refer to caption](img/6d6c0dfa0a7a98a038b5f11bf7497ce5.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6d6c0dfa0a7a98a038b5f11bf7497ce5.png)'
- en: (o) Layer 14
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: (o) 层 14
- en: 'Figure 21: Activations of LLaMa-7B'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '图 21: LLaMa-7B 的激活'
- en: '![Refer to caption](img/36de8fd8686b083eeb7f3b8b25310dcd.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/36de8fd8686b083eeb7f3b8b25310dcd.png)'
- en: i Layer 15
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: i 层 15
- en: '![Refer to caption](img/417334cdc34afea9cc850e6dfae5a394.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/417334cdc34afea9cc850e6dfae5a394.png)'
- en: ii Layer 16
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ii 层 16
- en: '![Refer to caption](img/2bc658786ca7ff49b2e47d8cb10aa877.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2bc658786ca7ff49b2e47d8cb10aa877.png)'
- en: iii Layer 17
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: iii 层 17
- en: '![Refer to caption](img/fe44a9191d7a6d3d2da24bb8a5d8bce3.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fe44a9191d7a6d3d2da24bb8a5d8bce3.png)'
- en: iv Layer 18
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: iv 层 18
- en: '![Refer to caption](img/092085b055556e63b4e80a674a3cc3c4.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/092085b055556e63b4e80a674a3cc3c4.png)'
- en: v Layer 19
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: v 层 19
- en: '![Refer to caption](img/848a981eae331776c290954676a571bf.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/848a981eae331776c290954676a571bf.png)'
- en: vi Layer 20
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: vi 层 20
- en: vii Layer 21
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: vii 层 21
- en: '![Refer to caption](img/e9faf90fdd3f7eef35b3a80e2993592b.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e9faf90fdd3f7eef35b3a80e2993592b.png)'
- en: viii Layer 22
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: viii 层 22
- en: '![Refer to caption](img/bc013e92d7f183cae974578843101d7d.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bc013e92d7f183cae974578843101d7d.png)'
- en: ix Layer 20
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ix 层 20
- en: '![Refer to caption](img/1396fde67b8c1b3f536560e3993d40e0.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1396fde67b8c1b3f536560e3993d40e0.png)'
- en: x Layer 24
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: x 层 24
- en: '![Refer to caption](img/30ce8509ed2391764cf3df2a0f835902.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/30ce8509ed2391764cf3df2a0f835902.png)'
- en: xi Layer 25
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: xi 层 25
- en: '![Refer to caption](img/f704fc2f9f058a13872c2a709c3cb425.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f704fc2f9f058a13872c2a709c3cb425.png)'
- en: xii Layer 26
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: xii 层 26
- en: '![Refer to caption](img/d643c9c0a857fbf82e8de078c17487cf.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d643c9c0a857fbf82e8de078c17487cf.png)'
- en: xiii Layer 27
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: xiii 层 27
- en: '![Refer to caption](img/1a467fb66a8bef84c7b1f4f7e36c08bc.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1a467fb66a8bef84c7b1f4f7e36c08bc.png)'
- en: xiv Layer 28
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: xiv 层 28
- en: '![Refer to caption](img/ac1b96fed00f4361d58c03f62d443ccd.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ac1b96fed00f4361d58c03f62d443ccd.png)'
- en: xv Layer 29
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: xv 层 29
- en: 'Figure 22: Activations of LLaMa-7B'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '图 22: LLaMa-7B 的激活'
- en: '![Refer to caption](img/99ac4bc0c30914c60af6e211ec6ad173.png)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/99ac4bc0c30914c60af6e211ec6ad173.png)'
- en: i Layer 30
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: i 层 30
- en: '![Refer to caption](img/cd4f801419e27cc79efa0c20d5a61ce0.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cd4f801419e27cc79efa0c20d5a61ce0.png)'
- en: ii Layer 31
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ii 层 31
- en: 'Figure 23: Activations of LLaMa-7B'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '图 23: LLaMa-7B 的激活'
