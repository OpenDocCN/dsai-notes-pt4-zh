- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'CrAM: 大型语言模型中基于可信度的注意力调整以对抗RAG中的虚假信息'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11497](https://ar5iv.labs.arxiv.org/html/2406.11497)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11497](https://ar5iv.labs.arxiv.org/html/2406.11497)
- en: 'Boyi Deng¹, Wenjie Wang², Fengbin Zhu², Qifan Wang³, Fuli Feng¹¹¹footnotemark:
    1'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Boyi Deng¹, Wenjie Wang², Fengbin Zhu², Qifan Wang³, Fuli Feng¹¹¹脚注标记：1
- en: ¹University of Science and Technology of China,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国科学技术大学，
- en: ²National University of Singapore, ³Meta AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²新加坡国立大学，³Meta AI
- en: dengboyi@mail.ustc.edu.cn, wqfcr@fb.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: dengboyi@mail.ustc.edu.cn, wqfcr@fb.com
- en: '{wenjiewang96,zhfengbin,fulifeng93}@gamil.com Corresponding author.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{wenjiewang96,zhfengbin,fulifeng93}@gamil.com 通讯作者。'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large Language
    Models (LLMs) by referencing external documents. However, the misinformation in
    external documents may mislead LLMs’ generation. To address this issue, we explore
    the task of “credibility-aware RAG”, in which LLMs automatically adjust the influence
    of retrieved documents based on their credibility scores to counteract misinformation.
    To this end, we introduce a plug-and-play method named Credibility-aware Attention
    Modification (CrAM). CrAM identifies influential attention heads in LLMs and adjusts
    their attention weights based on the credibility of the documents, thereby reducing
    the impact of low-credibility documents. Experiments on Natual Questions and TriviaQA
    using Llama2-13B, Llama3-8B, and Qwen-7B show that CrAM improves the RAG performance
    of LLMs against misinformation pollution by over 20%, even surpassing supervised
    fine-tuning methods.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）可以通过引用外部文档来减轻大型语言模型（LLMs）的幻觉。然而，外部文档中的虚假信息可能会误导LLMs的生成。为了解决这个问题，我们探讨了“基于可信度的RAG”任务，在该任务中，LLMs根据检索文档的可信度分数自动调整文档的影响，以对抗虚假信息。为此，我们引入了一种即插即用的方法，称为可信度感知注意力修改（CrAM）。CrAM识别LLMs中有影响力的注意力头，并根据文档的可信度调整它们的注意力权重，从而减少低可信度文档的影响。在使用Llama2-13B、Llama3-8B和Qwen-7B进行的Natual
    Questions和TriviaQA实验中，CrAM使LLMs在对抗虚假信息污染方面的RAG性能提高了20%以上，甚至超越了监督微调方法。
- en: 'CrAM: Credibility-Aware Attention Modification in LLMs for'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'CrAM: 大型语言模型中基于可信度的注意力调整'
- en: Combating Misinformation in RAG
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗RAG中的虚假信息
- en: 'Boyi Deng¹, Wenjie Wang²^†^†thanks: Corresponding author., Fengbin Zhu², Qifan
    Wang³, Fuli Feng¹¹¹footnotemark: 1 ¹University of Science and Technology of China,
    ²National University of Singapore, ³Meta AI dengboyi@mail.ustc.edu.cn, wqfcr@fb.com
    {wenjiewang96,zhfengbin,fulifeng93}@gamil.com'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Boyi Deng¹, Wenjie Wang²^†^†感谢：通讯作者., Fengbin Zhu², Qifan Wang³, Fuli Feng¹¹¹脚注标记：1
    ¹中国科学技术大学，²新加坡国立大学，³Meta AI dengboyi@mail.ustc.edu.cn, wqfcr@fb.com {wenjiewang96,zhfengbin,fulifeng93}@gamil.com
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Retrieval-Augmented Generation (RAG) (Gao et al., [2024](#bib.bib9); Zhu et al.,
    [2021](#bib.bib43)) is a representative approach to mitigate hallucination issues
    of Large Language Models (LLMs) (Zhang et al., [2023](#bib.bib42)) by retrieving
    and referencing relevant documents from an external corpus. Despite its effectiveness,
    most RAG works overlook a crucial issue: misinformation pollution in the external
    corpus (Pan et al., [2023b](#bib.bib28); Dufour et al., [2024](#bib.bib7)). The
    maliciously generated misinformation may mislead LLMs to produce unfaithful responses.
    For instance, Microsoft’s Bing can be misled by misinformation on the internet
    to generate incorrect information for Bing users (Vincent, [2023](#bib.bib36)).
    Besides, Pan et al. ([2023b](#bib.bib28)) and Pan et al. ([2023a](#bib.bib26))
    demonstrated that inserting LLM-generated misinformation into the RAG corpus can
    significantly degrade LLMs’ performance. Therefore, addressing the misinformation
    pollution for RAG is essential.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）(Gao et al., [2024](#bib.bib9); Zhu et al., [2021](#bib.bib43)) 是一种有效的减轻大型语言模型（LLMs）(Zhang
    et al., [2023](#bib.bib42)) 幻觉问题的方法，通过从外部语料库中检索和引用相关文档。尽管其有效性已被证明，但大多数RAG研究忽视了一个关键问题：外部语料库中的虚假信息污染
    (Pan et al., [2023b](#bib.bib28); Dufour et al., [2024](#bib.bib7))。恶意生成的虚假信息可能会误导LLMs产生不准确的回答。例如，微软的Bing可能会受到互联网上虚假信息的误导，从而为Bing用户生成不正确信息
    (Vincent, [2023](#bib.bib36))。此外，Pan et al. ([2023b](#bib.bib28)) 和 Pan et al.
    ([2023a](#bib.bib26)) 证明，将LLM生成的虚假信息插入RAG语料库中会显著降低LLMs的性能。因此，解决RAG中的虚假信息污染问题至关重要。
- en: '![Refer to caption](img/728df2f04de47c30de874a892106744b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/728df2f04de47c30de874a892106744b.png)'
- en: 'Figure 1: A comparison between RAG and credibility-aware RAG. Credibility-aware
    RAG considers credibility to reduce the impact of low-credibility documents.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：RAG和可信度感知RAG之间的比较。可信度感知RAG考虑了可信度，以减少低可信度文档的影响。
- en: 'A straightforward idea to address this misinformation pollution issue is misinformation
    detection and filtering. Extensive misinformation detection works focus on measuring
    the credibility of documents, *i.e.,* the probability of the document not containing
    misinformation. And these works have achieve significant results (Kaliyar et al.,
    [2021](#bib.bib17); Pelrine et al., [2023](#bib.bib29); Quelle and Bovet, [2024](#bib.bib30)).
    Once we obtain the credibility of each retrieved document, we can exclude those
    with credibility below a certain threshold before using them in RAG. However,
    directly discarding certain documents may result in the loss of relevant and important
    information, leading to performance degradation (Yoran et al., [2024](#bib.bib41))¹¹1Our
    experimental results in Table [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG") also confirm
    that directly excluding documents leads to inferior performance.. Moreover, discretizing
    credibility scores into binary labels loses fine-grained credibility information.
    As such, we should account for the value of credibility scores to wisely utilize
    the retrieved information.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '解决这些虚假信息污染问题的一个直接方法是虚假信息检测与过滤。广泛的虚假信息检测工作重点在于衡量文档的可信度，*即*文档不包含虚假信息的概率。这些研究取得了显著成果（Kaliyar
    et al., [2021](#bib.bib17)；Pelrine et al., [2023](#bib.bib29)；Quelle and Bovet,
    [2024](#bib.bib30)）。一旦我们获得了每个检索到的文档的可信度，我们可以在将它们用于RAG之前，排除那些可信度低于某个阈值的文档。然而，直接丢弃某些文档可能会导致相关和重要信息的丢失，从而导致性能下降（Yoran
    et al., [2024](#bib.bib41)）¹¹1我们的实验结果在表[2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ CrAM:
    Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG")也确认了直接排除文档会导致性能较差。此外，将可信度分数离散化为二进制标签会丢失细粒度的可信度信息。因此，我们应该考虑可信度分数的价值，以明智地利用检索到的信息。'
- en: 'To achieve this, we focus on a task named “credibility-aware RAG” as shown
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). Specifically, given
    a user query $x$ with a list of relevant documents $\mathcal{D}=\{d_{1},d_{2},...,d_{n}\}$
    and $\mathcal{D}$’s credibility scores $\mathcal{S}=\{s_{1},s_{2},...,s_{n}\}$,
    credibility-aware RAG requests LLMs to automatically adjust the influence of documents
    in $\mathcal{D}$ on the generated output $y$ based on their credibility scores
    in $\mathcal{S}$. Initial attempts on credibility-aware RAG adopted supervised
    fine-tuning (SFT) to teach LLMs to distinguish the importance of different documents
    in the prompt by their credibility scores (Hong et al., [2024](#bib.bib13); Pan
    et al., [2024](#bib.bib27)). However, SFT requires additional computational resources
    and well-designed training data, which limits the application scenarios. Therefore,
    we explore non-SFT method for LLMs to attain credibility-aware RAG.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '为了实现这一点，我们专注于一个名为“可信度感知RAG”的任务，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ CrAM:
    Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG")所示。具体来说，给定一个用户查询$x$和一个相关文档列表$\mathcal{D}=\{d_{1},d_{2},...,d_{n}\}$及其可信度分数$\mathcal{S}=\{s_{1},s_{2},...,s_{n}\}$，可信度感知RAG要求LLMs根据$\mathcal{S}$中的可信度分数自动调整文档在$\mathcal{D}$中对生成输出$y$的影响。最初对可信度感知RAG的尝试采用了监督微调（SFT），以教会LLMs通过其可信度分数来区分提示中不同文档的重要性（Hong
    et al., [2024](#bib.bib13)；Pan et al., [2024](#bib.bib27)）。然而，SFT需要额外的计算资源和精心设计的训练数据，这限制了应用场景。因此，我们探索了非SFT方法，以使LLMs能够实现可信度感知RAG。'
- en: Given that the attention mechanism serves as the central component for adjusting
    the significance of various input data, we consider manipulating attention weights
    of LLMs to achieve credibility-aware RAG. In particular, we adjust attention weights
    according to credibility scores in the inference stage of LLMs. In this way, we
    can regulate LLMs to pay less “attention” to less credible documents by decreasing
    the corresponding attention weights. Moreover, previous studies (Clark et al.,
    [2019](#bib.bib5); Elhage et al., [2021](#bib.bib8); Voita et al., [2019](#bib.bib37))
    have indicated that different attention heads exhibit distinct patterns and functions,
    resulting in varying impacts on LLMs’ outputs. In this context, the key lies in
    identifying a subset of influential attention heads for attention weight modification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于注意力机制作为调整各种输入数据重要性的核心组件，我们考虑操控LLMs的注意力权重，以实现可信度感知RAG。具体而言，我们在LLMs的推理阶段根据可信度评分调整注意力权重。通过这种方式，我们可以通过降低相应的注意力权重来调节LLMs对低可信度文档的“关注”。此外，先前的研究（Clark
    et al., [2019](#bib.bib5); Elhage et al., [2021](#bib.bib8); Voita et al., [2019](#bib.bib37)）表明，不同的注意力头展现出不同的模式和功能，从而对LLMs的输出产生不同的影响。在这种背景下，关键在于识别一组有影响力的注意力头进行注意力权重修改。
- en: 'In this work, we propose a plug-and-play method named Credibility-aware Attention
    Modification (CrAM), which identifies the influential attention heads and then
    modifies their attention weights *w.r.t.* different document tokens to reduce
    the impact of low-credibility documents. Specifically, 1) influential head identification:
    we select top-ranked attention heads according to an extended causal tracing method (Meng
    et al., [2022](#bib.bib23)) that estimates the contribution of each attention
    head to generating incorrect answers over a small dataset. 2) Attention weight
    modification: we scale down the attention weights of the retrieved documents based
    on their normalized credibility scores.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种即插即用的方法，名为可信度感知注意力修改（CrAM），该方法识别有影响力的注意力头，并根据不同文档标记修改它们的注意力权重，以减少低可信度文档的影响。具体而言，1)
    有影响力的头部识别：我们根据一种扩展的因果追踪方法（Meng et al., [2022](#bib.bib23)），选择排名靠前的注意力头，该方法估计每个注意力头在生成错误答案中的贡献。2)
    注意力权重修改：我们根据检索到文档的归一化可信度评分缩小它们的注意力权重。
- en: 'We conduct extensive experiments on two open-domain Question Answering (QA)
    datasets, Natual Questions (NQ) (Kwiatkowski et al., [2019](#bib.bib20)) and TriviaQA
    (Joshi et al., [2017](#bib.bib16)), using three open-source LLMs: Llama2-13B (Touvron
    et al., [2023](#bib.bib33)), Llama3-8B (Meta, [2024](#bib.bib24)), and Qwen-7B
    (Bai et al., [2023](#bib.bib1)). The results show that CrAM significantly alleviates
    the influence of misinformation documents on RAG, in terms of both ideal credibility
    scores and GPT-generated credibility scores. It is worth noting that CrAM even
    outperforms the SFT-based method CAG (Pan et al., [2024](#bib.bib27)) in most
    scenarios, demonstrating the superiority of CrAM. We release our code and data
    at [https://anonymous.4open.science/r/CrAM-77DF](https://anonymous.4open.science/r/CrAM-77DF).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个开放领域问答（QA）数据集，Natural Questions (NQ)（Kwiatkowski et al., [2019](#bib.bib20)）和TriviaQA（Joshi
    et al., [2017](#bib.bib16)），使用三种开源LLMs：Llama2-13B（Touvron et al., [2023](#bib.bib33)），Llama3-8B（Meta,
    [2024](#bib.bib24)），和Qwen-7B（Bai et al., [2023](#bib.bib1)）进行了广泛的实验。结果表明，CrAM显著减轻了虚假信息文档对RAG的影响，无论是在理想可信度评分还是GPT生成的可信度评分方面。值得注意的是，CrAM在大多数情况下甚至超越了基于SFT的方法CAG（Pan
    et al., [2024](#bib.bib27)），展示了CrAM的优越性。我们在[https://anonymous.4open.science/r/CrAM-77DF](https://anonymous.4open.science/r/CrAM-77DF)发布了我们的代码和数据。
- en: 'In summary, our main contributions are:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的主要贡献是：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We explore the task of credibility-aware RAG without fine-tuning LLMs to alleviate
    the misinformation pollution issue.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们探索了在不对LLMs进行微调的情况下进行可信度感知RAG的任务，以减轻虚假信息污染问题。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop a plug-and-play method, CrAM, which identifies influential attention
    heads and modifies their attention weights to equip LLMs with credibility-aware
    RAG capabilities.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一种即插即用的方法CrAM，它识别有影响力的注意力头并修改它们的注意力权重，为LLMs提供了可信度感知RAG的能力。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments with two QA datasets on three LLMs using ideal
    credibility scores and GPT-generated credibility scores, validating the superiority
    of CrAM.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在三个LLM上使用两个QA数据集进行广泛实验，使用理想可信度评分和GPT生成的可信度评分，验证了CrAM的优越性。
- en: 2 Credibility-Aware RAG
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 可信度感知 RAG
- en: '![Refer to caption](img/d36be28cf87dd674022b4125932089ed.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d36be28cf87dd674022b4125932089ed.png)'
- en: 'Figure 2: Illustration of CrAM. Compared to RAG, CrAM first identifies influential
    attention heads and then modifies their attention weights based on the credibility
    scores of each document.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：CrAM 的示意图。与 RAG 相比，CrAM 首先识别有影响力的注意力头，然后根据每个文档的可信度评分修改它们的注意力权重。
- en: Given a user query $x$, RAG retrieves a set of documents $\mathcal{D}=\{d_{1},d_{2},\ldots,d_{n}\}$
    relevant to $x$ through a retriever (Gao et al., [2024](#bib.bib9)). Then the
    relevant documents $\mathcal{D}$ are evaluated by a credibility estimator²²2Recent
    worked on this task has achieved promising performance (Kaliyar et al., [2021](#bib.bib17);
    Pelrine et al., [2023](#bib.bib29))., obtaining their credibility scores $\mathcal{S}=\{s_{1},s_{2},\ldots,s_{n}\}$,
    which represents the probability of each document not containing misinformation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个用户查询 $x$，RAG 通过检索器（Gao 等人，[2024](#bib.bib9)）检索一组与 $x$ 相关的文档 $\mathcal{D}=\{d_{1},d_{2},\ldots,d_{n}\}$。然后，通过可信度评估器²²2
    最近在此任务上取得了令人满意的成绩 (Kaliyar 等人，[2021](#bib.bib17)；Pelrine 等人，[2023](#bib.bib29))，对相关文档
    $\mathcal{D}$ 进行评估，获得其可信度评分 $\mathcal{S}=\{s_{1},s_{2},\ldots,s_{n}\}$，这些评分表示每个文档不包含虚假信息的概率。
- en: Credibility-Aware RAG.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可信度感知 RAG。
- en: 'Given an LLM $L$, a user query $x$, and relevant documents $\mathcal{D}$ associated
    with credibility scores $\mathcal{S}$, the objective of credibility-aware RAG
    is to enable LLMs to automatically adjust the influence of these documents on
    the generated output $y$ based on their credibility scores $\mathcal{S}$. This
    can be formally defined as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个 LLM $L$、一个用户查询 $x$ 和相关的文档 $\mathcal{D}$ 以及对应的可信度评分 $\mathcal{S}$，可信度感知
    RAG 的目标是使 LLM 能够根据这些文档的可信度评分 $\mathcal{S}$ 自动调整这些文档对生成输出 $y$ 的影响。这可以正式定义为：
- en: '|  | $\mathop{\max}\ \mathrm{Metric}(\mathrm{Combine}(L,x,\mathcal{D},\mathcal{S})),$
    |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathop{\max}\ \mathrm{Metric}(\mathrm{Combine}(L,x,\mathcal{D},\mathcal{S})),$
    |  |'
- en: where $\mathrm{Combine}(\cdot)$ represents the method or mechanism to integrate
    credibility scores into the generation process of $L$. For example, Pan et al.
    ([2024](#bib.bib27)) employ SFT to fine-tune LLMs to capture the credibility difference
    of documents more effectively, denoted as $\mathrm{Combine}(L,x,\mathcal{D},\mathcal{S})=L_{SFT}(x,\mathcal{D},\mathcal{S})$.
    Additionally, $\mathrm{Metric}(\cdot)$ is a function that assesses whether documents
    with different credibility scores have varying impacts on the output of $L$. Indeed,
    we can utilize the performance of generating factual answers to measure $\mathrm{Metric}(\cdot)$.
    For instance, we use the accuracy of QA tasks to approximate $\mathrm{Metric}(\cdot)$
    in this work. The rationality is that if the impact of low-credibility documents
    decreases, the accuracy of QA tasks should increase accordingly.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathrm{Combine}(\cdot)$ 代表将可信度评分整合到 $L$ 的生成过程中的方法或机制。例如，Pan 等人 ([2024](#bib.bib27))
    使用 SFT 来微调 LLM，更有效地捕捉文档的可信度差异，记作 $\mathrm{Combine}(L,x,\mathcal{D},\mathcal{S})=L_{SFT}(x,\mathcal{D},\mathcal{S})$。此外，$\mathrm{Metric}(\cdot)$
    是一个评估具有不同可信度评分的文档对 $L$ 的输出是否有不同影响的函数。实际上，我们可以利用生成事实回答的性能来衡量 $\mathrm{Metric}(\cdot)$。例如，我们在这项工作中使用
    QA 任务的准确性来近似 $\mathrm{Metric}(\cdot)$。其合理性在于，如果低可信度文档的影响降低，则 QA 任务的准确性应该相应增加。
- en: 3 CrAM
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CrAM
- en: 'CrAM first identifies influential attention heads, and then modifies the attention
    weights of these identified heads to reduce the impact of low-credibility documents
    as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Credibility-Aware RAG ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG"). Since influential
    attention heads identification process involves attention weight modification,
    we first explain the procedure of attention weight modification in Section [3.1](#S3.SS1
    "3.1 Attention Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"), and then describe
    influential attention heads identification in Section [3.2](#S3.SS2 "3.2 Influential
    Head Identification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"). Finally, we summarize the overall
    CrAM workflow in Section [3.3](#S3.SS3 "3.3 CrAM Workflow ‣ 3 CrAM ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'CrAM 首先识别有影响力的注意力头，然后修改这些识别出的头的注意力权重，以减少低可信度文档的影响，如图 [2](#S2.F2 "图 2 ‣ 2 可信度感知
    RAG ‣ CrAM: 用于对抗 RAG 中虚假信息的可信度感知注意力修改") 所示。由于有影响力的注意力头识别过程涉及注意力权重的修改，我们首先在第 [3.1](#S3.SS1
    "3.1 注意力权重修改 ‣ 3 CrAM ‣ CrAM: 用于对抗 RAG 中虚假信息的可信度感知注意力修改") 节中解释注意力权重修改的过程，然后在第
    [3.2](#S3.SS2 "3.2 有影响力头识别 ‣ 3 CrAM ‣ CrAM: 用于对抗 RAG 中虚假信息的可信度感知注意力修改") 节中描述有影响力的注意力头识别。最后，我们在第
    [3.3](#S3.SS3 "3.3 CrAM 工作流程 ‣ 3 CrAM ‣ CrAM: 用于对抗 RAG 中虚假信息的可信度感知注意力修改") 节中总结
    CrAM 的整体工作流程。'
- en: 3.1 Attention Weight Modification
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 注意力权重修改
- en: 'As defined in Section [2](#S2 "2 Credibility-Aware RAG ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG"), the objective
    of credibility-aware RAG is to reduce the impact of low-credibility documents
    on the generated output of LLMs. Intuitively, it requires LLMs to pay less “attention”
    to low-credibility documents. To this end, a natural approach is scaling down
    the corresponding attention weights of low-credibility documents.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '如第 [2](#S2 "2 可信度感知 RAG ‣ CrAM: 用于对抗 RAG 中虚假信息的可信度感知注意力修改") 节中定义的，可信度感知 RAG
    的目标是减少低可信度文档对 LLMs 生成输出的影响。直观上，它要求 LLMs 对低可信度文档减少“注意力”。为此，自然的方法是缩减低可信度文档的对应注意力权重。'
- en: 'For RAG, a user query $x$ and a set of relevant documents $\mathcal{D}=\{d_{1},d_{2},\ldots,d_{n}\}$
    should be concatenated and tokenized into a token sequence $\mathcal{T}(x,\mathcal{D})=\{t_{1},t_{2},\ldots,t_{m}\}$,
    where $t_{k}$ denotes the $k$-th token. Given the credibility scores for each
    document $\mathcal{S}=\{s_{1},s_{2},\ldots,s_{n}\}$, the normalized credibility
    score for token $t_{k}$ can be calculated as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RAG，用户查询 $x$ 和一组相关文档 $\mathcal{D}=\{d_{1},d_{2},\ldots,d_{n}\}$ 应该被连接并分词成一个令牌序列
    $\mathcal{T}(x,\mathcal{D})=\{t_{1},t_{2},\ldots,t_{m}\}$，其中 $t_{k}$ 表示第 $k$ 个令牌。给定每个文档的可信度分数
    $\mathcal{S}=\{s_{1},s_{2},\ldots,s_{n}\}$，令牌 $t_{k}$ 的归一化可信度分数可以通过以下方式计算：
- en: '|  | $\bar{s}_{k}=\begin{cases}\frac{s_{i}-\min(\mathcal{S})}{\max(\mathcal{S})-\min(\mathcal{S})}&amp;\text{if
    }t_{k}\text{ belongs to }d_{i}\\ 1&amp;\text{otherwise}\end{cases},$ |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{s}_{k}=\begin{cases}\frac{s_{i}-\min(\mathcal{S})}{\max(\mathcal{S})-\min(\mathcal{S})}&amp;\text{如果
    }t_{k}\text{ 属于 }d_{i}\\ 1&amp;\text{否则}\end{cases},$ |  |'
- en: where $s_{i}$ is subtracted by $\min(\mathcal{S})$, and then scaled down by
    $1/(\max(\mathcal{S})-\min(\mathcal{S}))$ to ensure all credibility scores are
    normalized to $[0,1]$. Besides, we define $\mathbf{\bar{s}}=[\bar{s}_{1},\ldots,\bar{s}_{m}]\in\mathbb{R}^{1\times
    m}$ to represent the normalized credibility scores of the whole token sequence
    $\mathcal{T}(x,\mathcal{D})$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{i}$ 减去 $\min(\mathcal{S})$，然后缩减 $1/(\max(\mathcal{S})-\min(\mathcal{S}))$，以确保所有可信度分数归一化到
    $[0,1]$。此外，我们定义 $\mathbf{\bar{s}}=[\bar{s}_{1},\ldots,\bar{s}_{m}]\in\mathbb{R}^{1\times
    m}$ 来表示整个令牌序列 $\mathcal{T}(x,\mathcal{D})$ 的归一化可信度分数。
- en: 'For each attention head $h$ in LLM, $\mathbf{A}_{h}$ represents its attention
    weights matrix³³3The attention weights matrix is defined in Equation ([3](#A1.E3
    "Equation 3 ‣ Appendix A Multi-Head Attention ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")).. Let $(\mathbf{A}_{h})_{k}$
    represent the $k$-th row vector⁴⁴4$(\mathbf{A}_{h})_{k}$ can be interpreted as
    the attention weight vector when using the $k$-th token as the query. of $\mathbf{A}_{h}$,
    we can obtain the modified attention weight matrix $\mathbf{A}^{*}_{h}$ by element-wise
    multiplying $\bar{\mathbf{s}}$ as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '对于LLM中的每个注意力头 $h$，$\mathbf{A}_{h}$ 代表其注意力权重矩阵³³3注意力权重矩阵在方程 ([3](#A1.E3 "方程
    3 ‣ 附录 A 多头注意力 ‣ CrAM: 可信度感知注意力修改在RAG中的虚假信息对抗")) 中定义。让 $(\mathbf{A}_{h})_{k}$
    代表第 $k$ 行向量⁴⁴4$(\mathbf{A}_{h})_{k}$ 可以解释为在使用第 $k$ 个标记作为查询时的注意力权重向量。我们可以通过逐元素乘法
    $\bar{\mathbf{s}}$ 来获得修改后的注意力权重矩阵 $\mathbf{A}^{*}_{h}$，方法如下：'
- en: '|  |  $(\mathbf{A}_{h})_{k}^{*}=\mathrm{Norm}((\mathbf{A}_{h})_{k}\odot\mathbf{\bar{s}}),k\in\{1,\ldots,m\},$  |  |
    (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |  $(\mathbf{A}_{h})_{k}^{*}=\mathrm{Norm}((\mathbf{A}_{h})_{k}\odot\mathbf{\bar{s}}),k\in\{1,\ldots,m\},$  |  |
    (1) |'
- en: where $\odot$ denotes the element-wise multiplication of vectors. The Norm function
    refers to $\ell_{1}$ normalization, which ensures that the attention weights sum
    to one.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$ 表示向量的逐元素乘法。Norm 函数指的是 $\ell_{1}$ 归一化，确保注意力权重之和为1。
- en: 3.2 Influential Head Identification
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 影响力头部识别
- en: 'Previous works Clark et al. ([2019](#bib.bib5)); Elhage et al. ([2021](#bib.bib8));
    Voita et al. ([2019](#bib.bib37)) have found that different attention heads exhibit
    various patterns and functions, leading to different impacts on LLMs’ output.
    As such, we hypothesize that some attention heads have a larger impact on using
    misinformation documents to generate incorrect answers. Previously, causal tracing
    (Meng et al., [2022](#bib.bib23)) has been developed to quantify the contribution
    of each hidden state towards generating given answers. The contribution is measured
    by adding noises to each hidden state to compare the changes in the generation
    probability of the given answer. In light of this, CrAM revises causal tracing
    to evaluate the contribution of attention heads instead of hidden states. Utilizing
    attention weight modification, as detailed in Section [3.1](#S3.SS1 "3.1 Attention
    Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"), CrAM estimates the change in probability
    of generating incorrect answers to determine the contribution of each attention
    head. Thereafter, CrAM ranks all attention heads by contributions and identifies
    influential ones.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '以往的研究（Clark et al. ([2019](#bib.bib5))；Elhage et al. ([2021](#bib.bib8))；Voita
    et al. ([2019](#bib.bib37))）发现，不同的注意力头展现出不同的模式和功能，导致对LLMs输出的影响各异。因此，我们假设一些注意力头对使用虚假信息文档生成错误答案有更大的影响。之前的因果追踪（Meng
    et al., [2022](#bib.bib23)）已被开发用于量化每个隐藏状态对生成给定答案的贡献。通过向每个隐藏状态添加噪声来比较生成答案概率的变化，从而衡量贡献。基于此，CrAM
    修正了因果追踪以评估注意力头的贡献，而不是隐藏状态。利用注意力权重修改，如第 [3.1](#S3.SS1 "3.1 注意力权重修改 ‣ 3 CrAM ‣ CrAM:
    可信度感知注意力修改在RAG中的虚假信息对抗") 节所述，CrAM 估计生成错误答案的概率变化，以确定每个注意力头的贡献。之后，CrAM 按照贡献对所有注意力头进行排名，并识别出影响力大的注意力头。'
- en: 'Specifically, the contribution of one attention head $h$ can be obtained as
    follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，单个注意力头 $h$ 的贡献可以通过以下方法获得：
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Given an LLM $L$, a user query $x$, a set of relevant documents $\mathcal{D}=\{d_{mis},d_{1},d_{2},\ldots,d_{n}\}$
    with one misinformation document $d_{mis}$, and an incorrect answer $a_{wrong}$
    to $x$ that is supported by $d_{mis}$, we first calculate the generation probability
    of $a_{wrong}$ with $x$ and $\mathcal{D}$ by $L$. Formally, we have:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给定一个LLM $L$，一个用户查询 $x$，一组相关文档 $\mathcal{D}=\{d_{mis},d_{1},d_{2},\ldots,d_{n}\}$（其中一个文档为虚假信息文档
    $d_{mis}$），以及一个由 $d_{mis}$ 支持的错误答案 $a_{wrong}$，我们首先通过 $L$ 计算 $a_{wrong}$ 在 $x$
    和 $\mathcal{D}$ 下的生成概率。形式上，我们有：
- en: '|  | $P_{0}=P_{L}(a_{wrong}\mid x,\mathcal{D}).$ |  |'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $P_{0}=P_{L}(a_{wrong}\mid x,\mathcal{D}).$ |  |'
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Next, we modify a specific attention head as described in Section [3.1](#S3.SS1
    "3.1 Attention Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") by using the credibility
    scores $\mathcal{S}=\{0,1,1,\ldots,1\}$ of $\mathcal{D}$ and recalculate the generation
    probability of $a_{wrong}$:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '接下来，我们按照第[3.1节](#S3.SS1 "3.1 Attention Weight Modification ‣ 3 CrAM ‣ CrAM:
    Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG")中的描述，使用$\mathcal{D}$的可信度评分$\mathcal{S}=\{0,1,1,\ldots,1\}$修改特定的注意力头，并重新计算$a_{wrong}$的生成概率：'
- en: '|  | $P_{1}=P_{L_{h}^{*}}(a_{wrong}\mid x,\mathcal{D}),$ |  |'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $P_{1}=P_{L_{h}^{*}}(a_{wrong}\mid x,\mathcal{D}),$ |  |'
- en: 'where $L_{h}^{*}$ denotes the LLM $L$ whose attention weight matrix of the
    attention head $h$ is modified according to Equation ([1](#S3.E1 "Equation 1 ‣
    3.1 Attention Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")).'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '其中$L_{h}^{*}$表示LLM $L$，其注意力头$h$的注意力权重矩阵根据方程([1](#S3.E1 "Equation 1 ‣ 3.1 Attention
    Weight Modification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"))进行修改。'
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Finally, we quantify the contribution of head $h$ towards generating the incorrect
    answer, *a.k.a.* the indirect effect (IE) (Meng et al., [2022](#bib.bib23)):'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们量化头$h$在生成错误答案中的贡献，*即*间接效应（IE）（Meng et al., [2022](#bib.bib23)）：
- en: '|  | $\mathrm{IE}_{h}=P_{0}-P_{1},$ |  | (2) |'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\mathrm{IE}_{h}=P_{0}-P_{1},$ |  | (2) |'
- en: which can also be interpreted as the decrease in the generation probability
    of the incorrect answer $a_{wrong}$ after modifying head $h$.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这也可以解释为在修改头$h$后，错误答案$a_{wrong}$的生成概率下降。
- en: 'To improve the robustness of the contribution estimation, we utilize a small
    dataset $\{(x,a_{wrong},\mathcal{D},\mathcal{S}),\ldots\}$ with different user
    queries to compute the average IE for each attention head (refer to Section [5](#S4.F5
    "Figure 5 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") for robustness analysis).
    Thereafter, we can calculate IEs for all the attention heads and rank them to
    select the top-ranked ones with larger IEs for attention weight modification.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提高贡献估计的鲁棒性，我们利用一个不同用户查询的小数据集$\{(x,a_{wrong},\mathcal{D},\mathcal{S}),\ldots\}$来计算每个注意力头的平均IE（有关鲁棒性分析，请参见第[5节](#S4.F5
    "Figure 5 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")）。然后，我们可以计算所有注意力头的IE，并对它们进行排序，以选择IE较大的前几个进行注意力权重修改。'
- en: 3.3 CrAM Workflow
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 CrAM工作流程
- en: 'The CrAM workflow is summarized as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: CrAM工作流程总结如下：
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'First, we use a small dataset with misinformation-polluted documents to calculate
    the average IE for each attention head in an LLM as described in Section [3.2](#S3.SS2
    "3.2 Influential Head Identification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). Then, we rank all
    attention heads by their IEs in descending order and select the top-ranked heads
    as influential attention heads.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '首先，我们使用一个包含虚假信息的文档的小数据集来计算每个注意力头的平均IE，如第[3.2节](#S3.SS2 "3.2 Influential Head
    Identification ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention Modification in LLMs
    for Combating Misinformation in RAG")中所述。然后，我们按降序排列所有注意力头的IE，并选择排名最高的头作为有影响力的注意力头。'
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Given any user query, along with the relevant documents and credibility scores,
    we modify the attention weights of influential attention heads using the method
    described in Section [3.1](#S3.SS1 "3.1 Attention Weight Modification ‣ 3 CrAM
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG") to obtain the final answer, thereby significantly reducing the impact
    of low-credibility documents.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '给定任何用户查询，以及相关文档和可信度评分，我们使用第[3.1节](#S3.SS1 "3.1 Attention Weight Modification
    ‣ 3 CrAM ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG")中描述的方法来修改有影响力的注意力头的注意力权重，从而获得最终答案，从而显著减少低可信度文档的影响。'
- en: 4 Experiments
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Model In-context corpus Method NQ TriviaQA EM F1 score EM F1 score Qwen-7B 0
    ✓ Naive LLM 7.20 16.41 28.00 38.23 4 ✓ Naive RAG 27.60 39.08 55.30 66.85 4 ✓ +
    1 ✗ Naive RAG 10.50 20.71 25.00 35.63 Prompt Based 12.20 22.26 27.40 37.98 CrAM
    29.10 (+16.90) 41.02 (+18.76) 52.90 (+25.50) 64.16 (+26.18) Llama2-13B 0 ✓ Naive
    LLM 20.30 28.59 50.40 57.56 4 ✓ Naive RAG 28.90 39.98 62.50 71.03 4 ✓ + 1 ✗ Naive
    RAG 11.90 19.97 28.00 36.22 Prompt Based 12.50 22.94 23.10 32.70 CrAM 33.60 (+21.10)
    44.62 (+21.68) 59.90 (+31.90) 67.11 (+30.89) Llama3-8B 0 ✓ Naive LLM 20.60 30.58
    55.70 62.67 4 ✓ Naive RAG 33.10 45.66 64.30 73.68 4 ✓ + 1 ✗ Naive RAG 16.00 26.16
    36.80 47.09 Prompt Based 29.90 39.69 53.50 63.01 CrAM 36.90 (+7.00) 48.45 (+8.76)
    64.40 (+10.90) 73.49 (+10.48)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 上下文语料方法 NQ TriviaQA EM F1 分数 EM F1 分数 Qwen-7B 0 ✓ 原始 LLM 7.20 16.41 28.00
    38.23 4 ✓ 原始 RAG 27.60 39.08 55.30 66.85 4 ✓ + 1 ✗ 原始 RAG 10.50 20.71 25.00 35.63
    基于提示 12.20 22.26 27.40 37.98 CrAM 29.10 (+16.90) 41.02 (+18.76) 52.90 (+25.50)
    64.16 (+26.18) Llama2-13B 0 ✓ 原始 LLM 20.30 28.59 50.40 57.56 4 ✓ 原始 RAG 28.90
    39.98 62.50 71.03 4 ✓ + 1 ✗ 原始 RAG 11.90 19.97 28.00 36.22 基于提示 12.50 22.94 23.10
    32.70 CrAM 33.60 (+21.10) 44.62 (+21.68) 59.90 (+31.90) 67.11 (+30.89) Llama3-8B
    0 ✓ 原始 LLM 20.60 30.58 55.70 62.67 4 ✓ 原始 RAG 33.10 45.66 64.30 73.68 4 ✓ + 1
    ✗ 原始 RAG 16.00 26.16 36.80 47.09 基于提示 29.90 39.69 53.50 63.01 CrAM 36.90 (+7.00)
    48.45 (+8.76) 64.40 (+10.90) 73.49 (+10.48)
- en: 'Table 1: Main results under ideal setting. 0 ✓ indicates no document and the
    model directly prompted, 4 ✓ indicates all four documents retrieved from the Wikipedia
    dump, and 4 ✓ + 1 ✗ indicates four high-credibility documents (i.e., retrieved
    from external corpus) plus one low-credibility document (i.e., containing misinformation).
    In the 4 ✓ + 1 ✗ setting, the best performance is highlighted in bold. And the
    red part indicates the difference between CrAM and second best performance.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表格1：理想设置下的主要结果。0 ✓ 表示没有文档，模型直接提示，4 ✓ 表示从维基百科转储中检索到的所有四个文档，而 4 ✓ + 1 ✗ 表示四个高可信度文档（即来自外部语料库）加上一个低可信度文档（即包含虚假信息）。在
    4 ✓ + 1 ✗ 设置下，最佳表现以**粗体**标出。红色部分表示 CrAM 与第二最佳表现之间的差异。
- en: Model In-context corpus Method NQ TriviaQA EM F1 score EM F1 score Qwen-7B 0
    ✓ Naive LLM 7.20 16.41 28.00 38.23 4 ✓ Naive RAG 27.60 39.08 55.30 66.85 4 ✓ +
    1 ✗ Naive RAG 10.50 20.71 25.00 35.63 Prompt Based 12.50 22.98 29.70 40.18 Exclusion
    21.60 32.56 49.50 61.03 CrAM 23.10 (+1.50) 34.84 (+2.28) 52.10 (+2.60) 63.76 (+2.73)
    Llama2-13B 0 ✓ Naive LLM 20.30 28.59 50.40 57.56 4 ✓ Naive RAG 28.90 39.98 62.50
    71.03 4 ✓ + 1 ✗ Naive RAG 11.90 19.97 28.00 36.22 Prompt Based 11.20 21.62 20.50
    30.09 Exclusion 23.70 34.00 54.40 62.37 CrAM 25.10 (+1.40) 35.56 (+1.56) 56.20
    (+1.80) 64.03 (+1.66) Llama3-8B 0 ✓ Naive LLM 20.60 30.58 55.70 62.67 4 ✓ Naive
    RAG 33.10 45.66 64.30 73.68 4 ✓ + 1 ✗ Naive RAG 16.00 26.16 36.80 47.09 Prompt
    Based 24.20 34.10 49.50 58.59 Exclusion 26.60 38.44 57.70 67.33 CrAM 30.70 (+4.10)
    41.71 (+3.27) 62.20 (+4.50) 70.70 (+3.37)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 上下文语料方法 NQ TriviaQA EM F1 分数 EM F1 分数 Qwen-7B 0 ✓ 原始 LLM 7.20 16.41 28.00
    38.23 4 ✓ 原始 RAG 27.60 39.08 55.30 66.85 4 ✓ + 1 ✗ 原始 RAG 10.50 20.71 25.00 35.63
    基于提示 12.50 22.98 29.70 40.18 排除 21.60 32.56 49.50 61.03 CrAM 23.10 (+1.50) 34.84
    (+2.28) 52.10 (+2.60) 63.76 (+2.73) Llama2-13B 0 ✓ 原始 LLM 20.30 28.59 50.40 57.56
    4 ✓ 原始 RAG 28.90 39.98 62.50 71.03 4 ✓ + 1 ✗ 原始 RAG 11.90 19.97 28.00 36.22 基于提示
    11.20 21.62 20.50 30.09 排除 23.70 34.00 54.40 62.37 CrAM 25.10 (+1.40) 35.56 (+1.56)
    56.20 (+1.80) 64.03 (+1.66) Llama3-8B 0 ✓ 原始 LLM 20.60 30.58 55.70 62.67 4 ✓ 原始
    RAG 33.10 45.66 64.30 73.68 4 ✓ + 1 ✗ 原始 RAG 16.00 26.16 36.80 47.09 基于提示 24.20
    34.10 49.50 58.59 排除 26.60 38.44 57.70 67.33 CrAM 30.70 (+4.10) 41.71 (+3.27)
    62.20 (+4.50) 70.70 (+3.37)
- en: 'Table 2: Main results under GPT setting. 0 ✓ indicates no document and the
    model directly prompted, 4 ✓ indicates all four documents retrieved from the Wikipedia
    dump, and 4 ✓ + 1 ✗ indicates four high-credibility documents (i.e., retrieved
    from external corpus) plus one low-credibility document (i.e., containing misinformation).
    In the 4 ✓ + 1 ✗ setting, the best performance is highlighted in bold. The red
    part indicates the improvement of our CrAM compared to the second-best model.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表格2：GPT 设置下的主要结果。0 ✓ 表示没有文档，模型直接提示，4 ✓ 表示从维基百科转储中检索到的所有四个文档，而 4 ✓ + 1 ✗ 表示四个高可信度文档（即来自外部语料库）加上一个低可信度文档（即包含虚假信息）。在
    4 ✓ + 1 ✗ 设置下，最佳表现以**粗体**标出。红色部分表示我们的 CrAM 相较于第二最佳模型的提升。
- en: 4.1 Experimental Settings
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Datasets, LLMs and Metrics.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集、LLMs 和指标。
- en: We conduct experiments over the Natural Questions (NQ) (Kwiatkowski et al.,
    [2019](#bib.bib20)) and TriviaQA (Joshi et al., [2017](#bib.bib16)) datasets with
    three LLMs, i.e. Llama2-13B (Touvron et al., [2023](#bib.bib33)), Llama3-8B (Meta,
    [2024](#bib.bib24)), and Qwen-7B (Bai et al., [2023](#bib.bib1)). We adopt Exact
    Match (EM) and F1 score as evaluation metrics, which are widely used in the QA
    setting (Karpukhin et al., [2020](#bib.bib19); Rajpurkar et al., [2016](#bib.bib32);
    Chen et al., [2017](#bib.bib4)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Natural Questions (NQ) (Kwiatkowski et al., [2019](#bib.bib20)) 和TriviaQA
    (Joshi et al., [2017](#bib.bib16)) 数据集上对三个LLMs进行实验，即Llama2-13B (Touvron et al.,
    [2023](#bib.bib33))、Llama3-8B (Meta, [2024](#bib.bib24)) 和Qwen-7B (Bai et al.,
    [2023](#bib.bib1))。我们采用Exact Match (EM) 和F1分数作为评估指标，这些指标在QA设置中被广泛使用（Karpukhin
    et al., [2020](#bib.bib19); Rajpurkar et al., [2016](#bib.bib32); Chen et al.,
    [2017](#bib.bib4)）。
- en: Document Preparation.
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文档准备。
- en: 'We prepare both high-credibility and low-credibility documents (i.e., with
    misinformation) associated with the questions for evaluating the proposed method.
    1) *High-credibility documents* are collected by retrieving the most relevant
    documents from the external corpus for each question. Specifically, we first employ
    bge-large-en-v1.5⁵⁵5[huggingface.co/BAAI/bge-large-en-v1.5](huggingface.co/BAAI/bge-large-en-v1.5).
    to obtain a set of candidates from the Wikipedia dump on December 30, 2018 (Karpukhin
    et al., [2020](#bib.bib19)). Then, we apply bge-reranker-large⁶⁶6[huggingface.co/BAAI/bge-reranker-large](huggingface.co/BAAI/bge-reranker-large).
    to rank the retrieved candidates and select the top four documents. 2) *Low-credibility
    documents* are generated via prompting LLMs (i.e., gpt-3.5-turbo-0125), with misinformation
    included, similar to the practice in previous works (Pan et al., [2023a](#bib.bib26),
    [b](#bib.bib28), [2024](#bib.bib27); Hong et al., [2024](#bib.bib13); Chen and
    Shu, [2024](#bib.bib3)). Specifically, given a question, we instruct the LLM to
    generate a news-style piece containing misinformation that supports an incorrect
    answer, which is regarded as one low-credibility document for the question. For
    each question, we collect three distinct low-credibility documents, all supporting
    the same incorrect answer. The prompts can be found in Appendix [G](#A7 "Appendix
    G Prompts ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '我们准备了与问题相关的高可信度和低可信度文档（即包含虚假信息的文档）来评估所提出的方法。1) *高可信度文档*是通过从外部语料库中检索与每个问题最相关的文档收集的。具体而言，我们首先使用bge-large-en-v1.5⁵⁵5[huggingface.co/BAAI/bge-large-en-v1.5](huggingface.co/BAAI/bge-large-en-v1.5)
    从2018年12月30日的维基百科转储中获得一组候选文档（Karpukhin et al., [2020](#bib.bib19)）。然后，我们使用bge-reranker-large⁶⁶6[huggingface.co/BAAI/bge-reranker-large](huggingface.co/BAAI/bge-reranker-large)
    对检索到的候选文档进行排序，并选择排名前四的文档。2) *低可信度文档* 是通过提示LLMs（即gpt-3.5-turbo-0125）生成的，其中包含虚假信息，类似于以前的研究方法（Pan
    et al., [2023a](#bib.bib26), [b](#bib.bib28), [2024](#bib.bib27); Hong et al.,
    [2024](#bib.bib13); Chen and Shu, [2024](#bib.bib3)）。具体而言，给定一个问题，我们指示LLM生成一篇包含虚假信息的新闻风格文章，该文章支持一个错误的答案，这被视为该问题的一个低可信度文档。对于每个问题，我们收集三个不同的低可信度文档，这些文档都支持相同的错误答案。提示语可以在附录[G](#A7
    "Appendix G Prompts ‣ CrAM: Credibility-Aware Attention Modification in LLMs for
    Combating Misinformation in RAG")中找到。'
- en: In implementation, we combine the generated low-credibility documents and the
    retrieved high-credibility documents for a given question as the LLM input. Compared
    to injecting the generated low-credibility documents into the corpus (Pan et al.,
    [2023a](#bib.bib26); Weller et al., [2024](#bib.bib38)), our approach can mitigate
    the retriever’s potential bias towards the misinformation. Also, our method is
    more controllable, making it easier to observe the impact of varying numbers of
    documents with misinformation on LLMs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施过程中，我们将生成的低可信度文档和检索到的高可信度文档结合起来作为LLM的输入。与将生成的低可信度文档注入语料库（Pan et al., [2023a](#bib.bib26);
    Weller et al., [2024](#bib.bib38)）相比，我们的方法可以减轻检索器对虚假信息的潜在偏见。此外，我们的方法更具可控性，使得观察不同数量虚假信息文档对LLMs的影响变得更加容易。
- en: Credibility Scores Generation.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可信度评分生成。
- en: 'We adopt two different ways to assign credibility scores for each document.
    1) *Ideal Setting.* After obtaining the high-credibility and low-credibility documents,
    we assign a score of 10 to each high-credibility document and a score of 1 to
    each low-credibility document. 2) *GPT Setting.* We employ GPT (i.e., gpt-3.5-turbo-0125)
    to directly generate the credibility score for each document. The prompts and
    the distribution of GPT-generated scores for all documents are provided in Figure [20](#A7.F20
    "Figure 20 ‣ Appendix G Prompts ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG") and Appendix [C](#A3 "Appendix C
    GPT-Generated Credibility Scores ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG").'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '我们采用两种不同的方法为每个文档分配可信度评分。1) *理想设置*。在获得高可信度和低可信度文档后，我们为每个高可信度文档分配10分，为每个低可信度文档分配1分。2)
    *GPT设置*。我们使用GPT（即gpt-3.5-turbo-0125）直接生成每个文档的可信度评分。所有文档的提示和GPT生成的评分分布见图 [20](#A7.F20
    "Figure 20 ‣ Appendix G Prompts ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG") 和附录 [C](#A3 "Appendix C GPT-Generated
    Credibility Scores ‣ CrAM: Credibility-Aware Attention Modification in LLMs for
    Combating Misinformation in RAG")。'
- en: Compared Methods.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 比较方法。
- en: 'We compare our CrAM model with four types of methods: 1) *Naive RAG.* The Naive
    RAG follows the standard RAG pipeline without any mechanisms against misinformation.
    2) *Prompt Based.* This method directly informs the LLM of the credibility score
    via prompts, feeding the score and documents into the LLM without additional training.
    3) *Exclusion.* This method excludes the documents with credibility scores below
    a threshold. This method will not be compared under the ideal setting due to the
    binary value of the ideal credibility score. 4) *CAG.* This method is proposed
    by Pan et al. ([2024](#bib.bib27)), which directly incorporates credibility scores
    and documents into prompts to fine-tune an LLM (i.e., Llama2-13B) to lift its
    understanding capabilities. Among them, Naive RAG, Prompt Based, and Exclusion
    are non-SFT methods, while CAG is an SFT-based method.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将CrAM模型与四种类型的方法进行比较：1) *Naive RAG*。Naive RAG遵循标准的RAG流程，没有任何针对虚假信息的机制。2) *基于提示*。该方法通过提示直接向LLM提供可信度评分，将评分和文档输入LLM，无需额外训练。3)
    *排除*。该方法排除可信度评分低于阈值的文档。由于理想可信度评分的二值特性，该方法在理想设置下不会进行比较。4) *CAG*。该方法由Pan等人提出（[2024](#bib.bib27)），直接将可信度评分和文档纳入提示中，以微调LLM（即Llama2-13B），提升其理解能力。其中，Naive
    RAG、基于提示和排除是非SFT方法，而CAG是基于SFT的方法。
- en: Hyperparameters.
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超参数。
- en: Unless otherwise specified, in the following experiments, we randomly select
    100 data points from each dataset to calculate average IE for all the heads. And
    we use another validation set of 100 data points from each dataset to determine
    how many top-ranked heads should be included in the final modified set.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，在以下实验中，我们随机选择每个数据集的100个数据点来计算所有头部的平均IE。我们还使用每个数据集的另一个100个数据点的验证集来确定最终修改集应该包括多少个排名靠前的头部。
- en: 4.2 Experimental Results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: 4.2.1 Main Results
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 主要结果
- en: Comparison with Non-SFT Methods.
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与非SFT方法的比较。
- en: 'We first compare our CrAM model with Non-SFT methods, i.e., Naive RAG, Prompt
    Based, and Exclusion. Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG") and Table
    [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG") show the experimental results in
    the Ideal and GPT settings respectively. We make the following observations. 1)
    Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") demonstrates that our
    CrAM method significantly outperforms all compared methods across all three LLMs:
    Qwen 7B, LLama2-13B, and LLama3-8B, on both NQ and TriviaQA datasets in the setting
    of 4 ✓+ 1 ✗ (i.e., four high-credibility documents plus one low-credibility document).
    For instance, our CrAM model surpasses the second-best method, i.e. Prompt Based,
    by $25.5\%$, $31.90\%$ and $10.9\%$ on Qwen-7B, Llama2-13B and Llama3-8B in terms
    of EM on TriviaQA, demonstrating remarkable performance gains. 2) With GPT-generated
    credibility scores, our CrAM model also outperforms all compared methods on all
    three LLMs over both NQ and TriviaQA datasets, as shown in Table [2](#S4.T2 "Table
    2 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for
    Combating Misinformation in RAG"), further highlighting its effectiveness. 3)
    Interestingly, we find that our CrAM model with 4 ✓ + 1 ✗ sometimes even outperforms
    the Naive RAG with 4 ✓ under ideal setting. This is likely because our generated
    misinformation includes both affirmations of incorrect information and denials
    of correct information, e.g.“The first person to win the Nobel Prize in Physics
    was not Roentgen, but Einstein.” This allows LLMs to reuse the correct information
    denied by the misinformation. To further validate this hypothesis, we conduct
    additional experiments and present the findings in Appendix [F](#A6 "Appendix
    F Results with Filtered Misinformation ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先将我们的CrAM模型与非SFT方法进行比较，即Naive RAG、基于提示的方法和排除方法。表格[1](#S4.T1 "Table 1 ‣ 4
    Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG")和表格[2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG")分别展示了在Ideal和GPT设置下的实验结果。我们做出以下观察。1)
    表格[1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG")表明，在4 ✓+ 1 ✗的设置下（即四个高可信度文档加一个低可信度文档），我们的CrAM方法在NQ和TriviaQA数据集上显著超越了所有对比方法，适用于所有三个LLM：Qwen
    7B、LLama2-13B和LLama3-8B。例如，在TriviaQA数据集上，我们的CrAM模型在Qwen-7B、Llama2-13B和Llama3-8B上的EM指标分别超越了第二好的方法——基于提示的方法——$25.5\%$、$31.90\%$和$10.9\%$，表现出显著的性能提升。2)
    使用GPT生成的可信度评分，我们的CrAM模型在所有三个LLM上在NQ和TriviaQA数据集上的表现也优于所有对比方法，如表格[2](#S4.T2 "Table
    2 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for
    Combating Misinformation in RAG")所示，进一步突显了其有效性。3) 有趣的是，我们发现，在理想设置下，我们的CrAM模型在4
    ✓ + 1 ✗的情况下有时甚至超越了Naive RAG在4 ✓的情况下。这可能是因为我们生成的虚假信息包括了对错误信息的确认和对正确信息的否认，例如：“获得诺贝尔物理奖的第一人不是伦琴，而是爱因斯坦。”这使得LLM可以重新利用被虚假信息否认的正确信息。为了进一步验证这一假设，我们进行了额外的实验，并在附录[F](#A6
    "Appendix F Results with Filtered Misinformation ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")中呈现了结果。'
- en: '![Refer to caption](img/b90cf1b0025c92972b5dd094262d2a4f.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b90cf1b0025c92972b5dd094262d2a4f.png)'
- en: 'Figure 3: Performance comparison of CrAM and CAG-13B regarding the varying
    number of documents containing misinformation under ideal setting.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在理想设置下，CrAM和CAG-13B对包含虚假信息的文档数量变化的性能比较。
- en: Comparison with SFT-based Method.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与基于SFT的方法的比较。
- en: 'For a fair comparison, we only compare our Llama2-13B based CrAM model with
    CAG-13B, because CAG-13B is trained on Llama2-13B. Moreover, to verify the robustness
    of our CrAM model, we perform comparisons using different numbers of low-credibility
    documents. As shown in Figure [3](#S4.F3 "Figure 3 ‣ Comparison with Non-SFT Methods.
    ‣ 4.2.1 Main Results ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG"), our CrAM
    model consistently outperforms the CAG-13B model remarkably in terms of F1 score
    when the number of low-credibility documents ranges from 1 to 3. The results further
    prove the effectiveness of our CrAM model.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进行公平比较，我们仅将基于Llama2-13B的CrAM模型与CAG-13B进行对比，因为CAG-13B是在Llama2-13B上训练的。此外，为了验证我们的CrAM模型的鲁棒性，我们使用不同数量的低可信文档进行比较。如图 [3](#S4.F3
    "Figure 3 ‣ Comparison with Non-SFT Methods. ‣ 4.2.1 Main Results ‣ 4.2 Experimental
    Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs
    for Combating Misinformation in RAG")所示，当低可信文档的数量从1到3时，我们的CrAM模型在F1得分方面始终显著优于CAG-13B模型。这些结果进一步证明了我们CrAM模型的有效性。'
- en: 4.2.2 In-Depth Analysis
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 深入分析
- en: Effect of Number of Low-credibility Documents.
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 低可信文档数量的影响。
- en: '![Refer to caption](img/f708637c3639dc88a6a3f41327d92741.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f708637c3639dc88a6a3f41327d92741.png)'
- en: 'Figure 4: Performance change on NQ regarding the varying number of documents
    with misinformation.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 关于虚假信息文档数量变化对NQ的性能变化。'
- en: 'In the following, we analyze the effect of varying the number of low-credibility
    documents fed into the LLM. We conduct experiments using Llama3-8B on the NQ dataset.
    Specifically, we vary the number of low-credibility documents from $1$ to $3$
    while keeping the number of high-credibility documents constant, i.e., $4$. We
    present the experimental results in Figure [4](#S4.F4 "Figure 4 ‣ Effect of Number
    of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis ‣ 4.2 Experimental Results
    ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG"). From the figure, we make the following observations.
    1) Our CrAM model consistently outperforms the compared models when changing the
    number of low-credibility documents from 1 to 3 in both ideal and GPT settings.
    2) Comparably, our CrAM model exhibits much smaller performance drops compared
    to other models when increasing the number of low-credibility documents. These
    results demonstrate the robustness of our proposed model to the varying number
    of low-credibility documents.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们分析了输入LLM的低可信文档数量的变化效果。我们在NQ数据集上使用Llama3-8B进行实验。具体来说，我们将低可信文档的数量从$1$变化到$3$，同时保持高可信文档的数量不变，即$4$。我们在图 [4](#S4.F4
    "Figure 4 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")中展示了实验结果。从图中，我们得出以下观察结论。1)
    在理想和GPT设置下，当低可信文档的数量从1变化到3时，我们的CrAM模型始终优于比较模型。2) 相比之下，当增加低可信文档的数量时，我们的CrAM模型的性能下降要小得多。这些结果表明我们提出的模型对低可信文档数量变化的鲁棒性。'
- en: '![Refer to caption](img/bb12f0ecfb579a6b3055c42e71d15214.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bb12f0ecfb579a6b3055c42e71d15214.png)'
- en: 'Figure 5: Performance on NQ and TriviaQA regarding the dataset size for determining
    the influential attention head changes.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 关于数据集大小对决定影响注意力头变化的性能。'
- en: Effect of Dataset Size on Attention Heads Selection.
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集大小对注意力头选择的影响。
- en: 'As we described in Section [3.3](#S3.SS3 "3.3 CrAM Workflow ‣ 3 CrAM ‣ CrAM:
    Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG"), we randomly select $100$ data points from each dataset to identify the
    influential attention heads. In the following, we vary the number of data points
    used for selecting these influential attention heads to analyze its impact on
    model performance. The experimental results are presented in Figure [5](#S4.F5
    "Figure 5 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). Despite fluctuations
    in performance along with the changing dataset size, the variations are not substantial
    on both NQ and TriviaQA datasets, with a maximum difference of $4\%$ in terms
    of EM. The results indicate that the number of data points has a minor impact
    on the final model performance.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在第[3.3节](#S3.SS3 "3.3 CrAM Workflow ‣ 3 CrAM ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG")中描述的，我们从每个数据集中随机选择$100$个数据点来识别有影响力的注意力头。接下来，我们改变选择这些有影响力的注意力头所使用的数据点数量，以分析其对模型性能的影响。实验结果如图[5](#S4.F5
    "Figure 5 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")所示。尽管性能随着数据集大小的变化有所波动，但在NQ和TriviaQA数据集上变化不大，EM的最大差异为$4\%$。结果表明，数据点数量对最终模型性能的影响较小。'
- en: '![Refer to caption](img/bfc3c384db16b99ff20ed70ea3b11425.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bfc3c384db16b99ff20ed70ea3b11425.png)'
- en: 'Figure 6: Performance on NQ in ideal setting regarding the varying number of
    selected attention heads.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在理想设置下，选择的注意力头数量变化对NQ性能的影响。
- en: '![Refer to caption](img/5f5668fb531c15e977cc2c4095e7bd6e.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5f5668fb531c15e977cc2c4095e7bd6e.png)'
- en: 'Figure 7: Density distribution of IE of all the attention heads in Llama3-8B.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：Llama3-8B中所有注意力头的IE密度分布。
- en: Analysis on Number of Selected Attention Heads.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择的注意力头数量分析。
- en: 'In the following, we analyze the performance change when we adjust the number
    of selected attention heads. We present the results in Figure [5](#S4.F5 "Figure
    5 ‣ Effect of Number of Low-credibility Documents. ‣ 4.2.2 In-Depth Analysis ‣
    4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"). We observe a sharp drop in model
    performance when the number of selected attention heads is near either 0 or the
    maximum number of heads, i.e., 1024; comparably, it has a minor effect when the
    number of selected attention heads falls into the range of values in between.
    To investigate the underlying reasons, we further analyze the IE’s density distribution
    using Llama3-8B, as shown in Figure [7](#S4.F7 "Figure 7 ‣ Effect of Dataset Size
    on Attention Heads Selection. ‣ 4.2.2 In-Depth Analysis ‣ 4.2 Experimental Results
    ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG"). We find that the IE density distribution approximates
    a normal distribution centered around 0, with the majority of values concentrated
    near 0. It indicates that most attention heads have minor impact on model performance,
    and only when the attention heads with IE values far from zero, either positive
    or negative, are selected, the model performance will be affected significantly.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们分析调整选择的注意力头数量时性能的变化。结果如图[5](#S4.F5 "Figure 5 ‣ Effect of Number of Low-credibility
    Documents. ‣ 4.2.2 In-Depth Analysis ‣ 4.2 Experimental Results ‣ 4 Experiments
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG")所示。当选择的注意力头数量接近0或最大值（即1024）时，模型性能显著下降；相对而言，当选择的注意力头数量在两者之间时，其影响较小。为了探究其背后的原因，我们进一步分析了使用Llama3-8B的IE密度分布，如图[7](#S4.F7
    "Figure 7 ‣ Effect of Dataset Size on Attention Heads Selection. ‣ 4.2.2 In-Depth
    Analysis ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG")所示。我们发现IE密度分布近似为以0为中心的正态分布，大多数值集中在0附近。这表明大多数注意力头对模型性能的影响较小，只有当IE值远离0（无论是正值还是负值）的注意力头被选择时，模型性能才会显著受到影响。'
- en: 4.2.3 Ablation Study
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 消融研究
- en: Model Method NQ TriviaQA EM EM Qwen-7B CrAM 29.10 52.90 CrAM-all 27.20 (-1.90)
    50.60 (-2.30) Naive RAG 10.50 (-18.60) 25.00 (-27.90) Llama2-13B CrAM 33.60 59.90
    CrAM-all 29.50 (-4.10) 59.50 (-0.40) Naive RAG 11.90 (-21.70) 28.00 (-27.90) Llama3-8B
    CrAM 36.90 64.40 CrAM-all 22.40 (-14.50) 51.50 (-12.90) Naive RAG 16.00 (-20.90)
    36.80 (-27.60)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 方法 NQ TriviaQA EM EM Qwen-7B CrAM 29.10 52.90 CrAM-all 27.20 (-1.90) 50.60
    (-2.30) Naive RAG 10.50 (-18.60) 25.00 (-27.90) Llama2-13B CrAM 33.60 59.90 CrAM-all
    29.50 (-4.10) 59.50 (-0.40) Naive RAG 11.90 (-21.70) 28.00 (-27.90) Llama3-8B
    CrAM 36.90 64.40 CrAM-all 22.40 (-14.50) 51.50 (-12.90) Naive RAG 16.00 (-20.90)
    36.80 (-27.60)
- en: 'Table 3: Results of ablation study under ideal setting with 4 ✓ + 1 ✗ (i.e.,
    four high-credibility documents plus one low-credibility document).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在理想设置下的消融研究结果，4 ✓ + 1 ✗（即四个高可信度文档加一个低可信度文档）。
- en: 'To better understand the rationality of our model design, we conduct ablation
    study and present the results in Table [3](#S4.T3 "Table 3 ‣ 4.2.3 Ablation Study
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). First, we remove the
    selection of influential attention heads and apply attention weight modification
    on all attention heads in LLMs, and denote this variant model as CrAM-all. As
    shown in Table [3](#S4.T3 "Table 3 ‣ 4.2.3 Ablation Study ‣ 4.2 Experimental Results
    ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating
    Misinformation in RAG"), we observe that the performance of the CrAM-all model
    has noticeable drops on all three LLMs. Among them, Llama3-8B based CrAM has the
    largest decrease on both NQ and TriviaQA, i.e., $14.5\%$ and $12.9\%$. This indicates
    the necessity of identifying the influential attention heads before modifying
    the attention weights.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更好地理解我们模型设计的合理性，我们进行了一项消融研究，并在表[3](#S4.T3 "Table 3 ‣ 4.2.3 Ablation Study
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")中展示了结果。首先，我们移除对有影响力的注意力头的选择，并对所有注意力头应用注意力权重修改，将这种变体模型称为CrAM-all。正如表[3](#S4.T3
    "Table 3 ‣ 4.2.3 Ablation Study ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM:
    Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG")所示，我们观察到CrAM-all模型在所有三种LLM上表现出明显的下降。其中，基于Llama3-8B的CrAM在NQ和TriviaQA上有最大的下降，即$14.5\%$和$12.9\%$。这表明在修改注意力权重之前识别有影响力的注意力头是必要的。'
- en: 'If we disable the attention weight modification mechanism in our model, it
    becomes the Naive RAG method. Table [3](#S4.T3 "Table 3 ‣ 4.2.3 Ablation Study
    ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") shows that this results
    in a remarkable performance drop on all three LLMs compared to the CrAM model.
    For instance, the performance of all three LLMs decreases more than $27.5\%$ on
    TriviaQA dataset. These results verify that it is necessary to modify the attention
    weight and meanwhile take into account the credibility scores of the documents.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们禁用模型中的注意力权重修改机制，它就变成了Naive RAG方法。表[3](#S4.T3 "Table 3 ‣ 4.2.3 Ablation
    Study ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")显示，与CrAM模型相比，这在所有三种LLM上导致了显著的性能下降。例如，所有三种LLM在TriviaQA数据集上的性能下降超过$27.5\%$。这些结果验证了修改注意力权重并同时考虑文档的可信度分数的必要性。'
- en: 5 Related Work
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Misinformation Detection.
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 误信息检测。
- en: Misinformation detection aims to identify false or misleading information from
    various data sources (Guo et al., [2019](#bib.bib11); Kaliyar and Singh, [2019](#bib.bib18);
    Vaibhav et al., [2019](#bib.bib34)). It can be categorized into non-LLM-based
    methods and LLM-based methods. Non-LLM-based methods often involve a training
    process, enabling models to identify misinformation (Vaibhav et al., [2019](#bib.bib34);
    Kaliyar et al., [2021](#bib.bib17); Liu et al., [2023](#bib.bib22); Goonathilake
    and Kumara, [2020](#bib.bib10)). For example, Kaliyar et al. ([2021](#bib.bib17))
    utilize BERT (Devlin et al., [2019](#bib.bib6)) to score the credibility of documents,
    while Vaibhav et al. ([2019](#bib.bib34)) use a graph neural network for misinformation
    detection. Comparably, LLM-based methods typically use LLMs without additional
    training (Pelrine et al., [2023](#bib.bib29); Quelle and Bovet, [2024](#bib.bib30);
    Caramancion, [2023](#bib.bib2); Hoes et al., [2023](#bib.bib12)). For instance,
    Pelrine et al. ([2023](#bib.bib29)) adopt GPT-4 (OpenAI et al., [2024](#bib.bib25))
    for document credibility scoring, while Quelle and Bovet ([2024](#bib.bib30))
    employ an LLM agent (Xi et al., [2023](#bib.bib39)) for iterative verification
    of document credibility. In this study, we employ LLMs to obtain the credibility
    score for each document similar to the previous LLM-based methods (Pelrine et al.,
    [2023](#bib.bib29); Hoes et al., [2023](#bib.bib12)). In this study, we employ
    LLMs to obtain the credibility score for each document similar to (Pelrine et al.,
    [2023](#bib.bib29); Hoes et al., [2023](#bib.bib12)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虚假信息检测旨在从各种数据源中识别虚假或误导性的信息（Guo et al., [2019](#bib.bib11); Kaliyar and Singh,
    [2019](#bib.bib18); Vaibhav et al., [2019](#bib.bib34)）。它可以分为非LLM基础的方法和LLM基础的方法。非LLM基础的方法通常涉及训练过程，使模型能够识别虚假信息（Vaibhav
    et al., [2019](#bib.bib34); Kaliyar et al., [2021](#bib.bib17); Liu et al., [2023](#bib.bib22);
    Goonathilake and Kumara, [2020](#bib.bib10)）。例如，Kaliyar et al. ([2021](#bib.bib17))
    利用 BERT (Devlin et al., [2019](#bib.bib6)) 来评分文档的可信度，而 Vaibhav et al. ([2019](#bib.bib34))
    使用图神经网络进行虚假信息检测。相比之下，LLM基础的方法通常使用LLM而无需额外训练（Pelrine et al., [2023](#bib.bib29);
    Quelle and Bovet, [2024](#bib.bib30); Caramancion, [2023](#bib.bib2); Hoes et
    al., [2023](#bib.bib12)）。例如，Pelrine et al. ([2023](#bib.bib29)) 采用 GPT-4 (OpenAI
    et al., [2024](#bib.bib25)) 来评分文档的可信度，而 Quelle and Bovet ([2024](#bib.bib30))
    使用 LLM 代理 (Xi et al., [2023](#bib.bib39)) 进行文档可信度的迭代验证。在本研究中，我们采用LLM来获得每个文档的可信度评分，类似于之前的LLM基础方法（Pelrine
    et al., [2023](#bib.bib29); Hoes et al., [2023](#bib.bib12)）。
- en: Combating Misinformation in RAG.
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 RAG 中打击虚假信息。
- en: Retrieval-Augmented Generation (RAG) enhance LLMs by retrieving relevant documents
    from external corpus (Lewis et al., [2020](#bib.bib21); Izacard and Grave, [2021](#bib.bib14)).
    However, prior works (Zou et al., [2024](#bib.bib44); Pan et al., [2023b](#bib.bib28),
    [a](#bib.bib26)) find that RAG is vulnerable to misinformation in its corpus,
    leading to undesired results. To combat misinformation in RAG, lots of studies
    have been conducted. For example, CAR Weller et al. ([2024](#bib.bib38)) adopt
    a query augmentation scheme to retrieve a larger set of documents first and then
    apply a voting mechanism to mitigate the impact of misinformation. RobustRAG Xiang
    et al. ([2024](#bib.bib40)) obtains the LLM response for each document independently
    and aggregates these responses through keyword-based and decoding-based algorithms
    to generate the final result. Hong et al. ([2024](#bib.bib13)) and Pan et al.
    ([2024](#bib.bib27)) assign each retrieved document a credibility score and fine-tune
    LLMs with the documents and their scores, enabling the LLMs to leverage these
    credibility scores when generating. CD² Jin et al. ([2024](#bib.bib15)) train
    two LLMs to generate truthful answers and misleading answers respectively to make
    it better distinguish the conflict information. However, CAR Weller et al. ([2024](#bib.bib38))
    and RobustRAG Xiang et al. ([2024](#bib.bib40)) require multiple rounds of model
    inference, leading to inefficiency. The methods proposed by Hong et al. ([2024](#bib.bib13)),
    Pan et al. ([2024](#bib.bib27)), and Jin et al. ([2024](#bib.bib15)) require fine-tuning
    LLMs, which demands additional computational resources and well-designed training
    data, thereby limiting their application scenarios. In contrast, our CrAM model
    requires no training and only needs a single inference to produce the final output.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）通过从外部语料库中检索相关文档来增强 LLM（Lewis et al., [2020](#bib.bib21); Izacard
    and Grave, [2021](#bib.bib14)）。然而，先前的研究（Zou et al., [2024](#bib.bib44); Pan et
    al., [2023b](#bib.bib28), [a](#bib.bib26)）发现 RAG 易受到语料库中的虚假信息的影响，导致不理想的结果。为了应对
    RAG 中的虚假信息，已经进行了大量研究。例如，CAR Weller et al. ([2024](#bib.bib38)) 采用了查询增强方案，首先检索更大集合的文档，然后应用投票机制来减轻虚假信息的影响。RobustRAG
    Xiang et al. ([2024](#bib.bib40)) 为每个文档独立获得 LLM 响应，并通过基于关键词和解码的算法汇总这些响应以生成最终结果。Hong
    et al. ([2024](#bib.bib13)) 和 Pan et al. ([2024](#bib.bib27)) 为每个检索到的文档分配一个可信度评分，并使用这些文档及其评分对
    LLM 进行微调，使 LLM 在生成时能够利用这些可信度评分。CD² Jin et al. ([2024](#bib.bib15)) 训练两个 LLM 分别生成真实答案和误导性答案，以更好地区分冲突信息。然而，CAR
    Weller et al. ([2024](#bib.bib38)) 和 RobustRAG Xiang et al. ([2024](#bib.bib40))
    需要多轮模型推理，导致效率低下。Hong et al. ([2024](#bib.bib13))、Pan et al. ([2024](#bib.bib27))
    和 Jin et al. ([2024](#bib.bib15)) 提出的这些方法需要对 LLM 进行微调，这需要额外的计算资源和精心设计的训练数据，从而限制了它们的应用场景。相比之下，我们的
    CrAM 模型不需要训练，只需一次推理即可生成最终输出。
- en: 6 Conclusion
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This work introduces CrAM, a plug-and-play method that enables RAG to automatically
    adjust the influence of retrieved documents on the output of LLMs based on document
    credibility. CrAM first identifies influential attention heads and then adjusts
    the attention weights of identified attention heads according to the credibility
    score of documents, regulating LLMs to pay less attention to the low-credibility
    documents. Empirical experiments demonstrate that, compared to vanilla RAG, CrAM
    improves EM performance by more than 20% on two datasets and even outperforms
    the baseline with SFT, demonstrating CrAM’s efficiency.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 CrAM，一种即插即用的方法，能够使 RAG 根据文档的可信度自动调整检索文档对 LLM 输出的影响。CrAM 首先识别出有影响力的注意力头，然后根据文档的可信度评分调整这些识别出的注意力头的权重，从而使
    LLM 对低可信度文档的关注度降低。实证实验表明，与普通 RAG 相比，CrAM 在两个数据集上将 EM 性能提高了超过 20%，甚至在与 SFT 基线的比较中表现更好，展示了
    CrAM 的高效性。
- en: Limitations
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'This work has several limitations that we aim to address in the future. First,
    we identify a fixed set of attention heads for attention weight modification for
    all questions. Despite Section [5](#S4.F5 "Figure 5 ‣ Effect of Number of Low-credibility
    Documents. ‣ 4.2.2 In-Depth Analysis ‣ 4.2 Experimental Results ‣ 4 Experiments
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG") indicating the robustness of using a small dataset for influential head
    identification, a more effective solution is to identify specific attention heads
    tailored to each individual question. Second, we only use the credibility scores
    of each document for credibility-aware RAG. However, LLMs actually can utilize
    the correct information in the misinformation document. Thus, empowering LLMs
    to leverage a fine-grained credibility score at the sentence or even word level
    for answer generation is promising. Third, we only evaluate the performance of
    CrAM on decoder-only LLMs, and the effectiveness of CrAM on more models with different
    architectures, such as T5 (Raffel et al., [2020](#bib.bib31)), is worth exploring.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作存在几个限制，我们计划在未来进行改进。首先，我们为所有问题确定了一组固定的注意力头用于调整注意力权重。尽管第[5](#S4.F5 "图 5 ‣
    低可信度文档数量的影响。 ‣ 4.2.2 深入分析 ‣ 4.2 实验结果 ‣ 4 实验 ‣ CrAM：在RAG中用于对抗虚假信息的可信度感知注意力修改")节表明使用小数据集来识别有影响力的注意力头具有鲁棒性，但更有效的解决方案是识别针对每个具体问题定制的特定注意力头。其次，我们仅使用每个文档的可信度评分进行可信度感知的RAG。然而，LLMs
    实际上可以利用虚假信息文档中的正确信息。因此，使 LLMs 能够利用句子甚至单词级别的细粒度可信度评分来生成答案是有前景的。第三，我们仅评估了 CrAM 在仅解码器的
    LLMs 上的性能，而 CrAM 在不同架构的更多模型上的有效性，例如 T5（Raffel et al., [2020](#bib.bib31)），值得进一步探索。
- en: Ethics Statement
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: In our experiments, we use gpt-3.5-turbo-0125 to generate misinformation. We
    want to emphasize that we generate this misinformation solely for research purposes,
    and we will not use it for any other purpose ourselves. Additionally, we do not
    encourage anyone to use this misinformation for any other purpose.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用 gpt-3.5-turbo-0125 生成虚假信息。我们想强调，我们生成这些虚假信息仅用于研究目的，我们自己不会将其用于其他任何目的。此外，我们也不鼓励任何人将这些虚假信息用于其他目的。
- en: References
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, et al. 2023. [Qwen
    technical report](https://arxiv.org/abs/2309.16609). *Preprint*, arXiv:2309.16609.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, 等. 2023. [Qwen 技术报告](https://arxiv.org/abs/2309.16609).
    *预印本*，arXiv:2309.16609。
- en: 'Caramancion (2023) Kevin Matthe Caramancion. 2023. [Harnessing the power of
    chatgpt to decimate mis/disinformation: Using chatgpt for fake news detection](https://doi.org/10.1109/AIIoT58121.2023.10174450).
    In *2023 IEEE World AI IoT Congress (AIIoT)*, pages 0042–0046.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caramancion (2023) Kevin Matthe Caramancion. 2023. [利用 ChatGPT 的力量打击虚假/误导信息：使用
    ChatGPT 进行假新闻检测](https://doi.org/10.1109/AIIoT58121.2023.10174450)。发表于 *2023 IEEE
    世界人工智能物联网大会 (AIIoT)*，第 0042–0046 页。
- en: Chen and Shu (2024) Canyu Chen and Kai Shu. 2024. [Can llm-generated misinformation
    be detected?](https://arxiv.org/abs/2309.13788) *Preprint*, arXiv:2309.13788.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Shu (2024) Canyu Chen 和 Kai Shu. 2024. [LLM 生成的虚假信息是否可以被检测？](https://arxiv.org/abs/2309.13788)
    *预印本*，arXiv:2309.13788。
- en: 'Chen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
    2017. [Reading Wikipedia to answer open-domain questions](https://doi.org/10.18653/v1/P17-1171).
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 1870–1879, Vancouver, Canada. Association
    for Computational Linguistics.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston 和 Antoine Bordes. 2017.
    [阅读维基百科以回答开放领域问题](https://doi.org/10.18653/v1/P17-1171)。发表于 *第 55 届计算语言学协会年会论文集
    (第 1 卷：长篇论文)*，第 1870–1879 页，加拿大温哥华。计算语言学协会。
- en: 'Clark et al. (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D.
    Manning. 2019. [What does BERT look at? an analysis of BERT’s attention](https://doi.org/10.18653/v1/W19-4828).
    In *Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting
    Neural Networks for NLP*, pages 276–286, Florence, Italy. Association for Computational
    Linguistics.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 (2019) Kevin Clark, Urvashi Khandelwal, Omer Levy 和 Christopher D. Manning.
    2019. [BERT 看什么？对 BERT 注意力的分析](https://doi.org/10.18653/v1/W19-4828)。在 *2019 ACL
    研讨会：BlackboxNLP：分析和解释 NLP 的神经网络*，第276–286页，佛罗伦萨，意大利。计算语言学协会。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等 (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    2019. [BERT：用于语言理解的深度双向变换器预训练](https://doi.org/10.18653/v1/N19-1423)。在 *2019年北美计算语言学协会年会：人类语言技术会议论文集第1卷（长篇和短篇论文）*，第4171–4186页，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: 'Dufour et al. (2024) Nicholas Dufour, Arkanath Pathak, Pouya Samangouei, Nikki
    Hariri, Shashi Deshetti, Andrew Dudfield, Christopher Guess, Pablo Hernández Escayola,
    Bobby Tran, Mevan Babakar, and Christoph Bregler. 2024. [Ammeba: A large-scale
    survey and dataset of media-based misinformation in-the-wild](https://arxiv.org/abs/2405.11697).
    *Preprint*, arXiv:2405.11697.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dufour 等 (2024) Nicholas Dufour, Arkanath Pathak, Pouya Samangouei, Nikki Hariri,
    Shashi Deshetti, Andrew Dudfield, Christopher Guess, Pablo Hernández Escayola,
    Bobby Tran, Mevan Babakar 和 Christoph Bregler. 2024. [Ammeba：一项大规模的媒体误信息调查和数据集](https://arxiv.org/abs/2405.11697)。*预印本*，arXiv:2405.11697。
- en: Elhage et al. (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan,
    Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
    et al. 2021. [A mathematical framework for transformer circuits](https://transformer-circuits.pub/2021/framework/index.html).
    *Transformer Circuits Thread*, 1:1.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhage 等 (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas
    Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly 等. 2021. [变换器电路的数学框架](https://transformer-circuits.pub/2021/framework/index.html)。*变换器电路线程*，1:1。
- en: 'Gao et al. (2024) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan,
    Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. [Retrieval-augmented
    generation for large language models: A survey](https://arxiv.org/abs/2312.10997).
    *Preprint*, arXiv:2312.10997.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2024) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi
    Bi, Yi Dai, Jiawei Sun, Meng Wang 和 Haofen Wang. 2024. [大语言模型的检索增强生成：一项综述](https://arxiv.org/abs/2312.10997)。*预印本*，arXiv:2312.10997。
- en: Goonathilake and Kumara (2020) M. D. P. P Goonathilake and P. P. N. V Kumara.
    2020. [Cnn, rnn-lstm based hybrid approach to detect state-of-the-art stance-based
    fake news on social media](https://doi.org/10.1109/ICTer51097.2020.9325477). In
    *2020 20th International Conference on Advances in ICT for Emerging Regions (ICTer)*,
    pages 23–28.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goonathilake 和 Kumara (2020) M. D. P. Goonathilake 和 P. P. N. V. Kumara. 2020.
    [基于 CNN 和 RNN-LSTM 的混合方法来检测社交媒体上的前沿立场型假新闻](https://doi.org/10.1109/ICTer51097.2020.9325477)。在
    *2020年第20届国际信息与通信技术新兴地区会议（ICTer）*，第23–28页。
- en: 'Guo et al. (2019) Bin Guo, Yasan Ding, Lina Yao, Yunji Liang, and Zhiwen Yu.
    2019. [The future of misinformation detection: New perspectives and trends](https://arxiv.org/abs/1909.03654).
    *Preprint*, arXiv:1909.03654.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2019) Bin Guo, Yasan Ding, Lina Yao, Yunji Liang 和 Zhiwen Yu. 2019. [虚假信息检测的未来：新视角和趋势](https://arxiv.org/abs/1909.03654)。*预印本*，arXiv:1909.03654。
- en: Hoes et al. (2023) Emma Hoes, Sacha Altay, and Juan Bermeo. 2023. [Leveraging
    chatgpt for efficient fact-checking](https://doi.org/10.31234/osf.io/qnjkf).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoes 等 (2023) Emma Hoes, Sacha Altay 和 Juan Bermeo. 2023. [利用 ChatGPT 高效进行事实核查](https://doi.org/10.31234/osf.io/qnjkf)。
- en: Hong et al. (2024) Giwon Hong, Jeonghwan Kim, Junmo Kang, Sung-Hyon Myaeng,
    and Joyce Jiyoung Whang. 2024. [Why so gullible? enhancing the robustness of retrieval-augmented
    models against counterfactual noise](https://arxiv.org/abs/2305.01579). *Preprint*,
    arXiv:2305.01579.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等 (2024) Giwon Hong, Jeonghwan Kim, Junmo Kang, Sung-Hyon Myaeng 和 Joyce
    Jiyoung Whang. 2024. [为什么如此容易受骗？增强检索增强模型对反事实噪声的鲁棒性](https://arxiv.org/abs/2305.01579)。*预印本*，arXiv:2305.01579。
- en: 'Izacard and Grave (2021) Gautier Izacard and Edouard Grave. 2021. [Leveraging
    passage retrieval with generative models for open domain question answering](https://doi.org/10.18653/v1/2021.eacl-main.74).
    In *Proceedings of the 16th Conference of the European Chapter of the Association
    for Computational Linguistics: Main Volume*, pages 874–880, Online. Association
    for Computational Linguistics.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard和Grave（2021）Gautier Izacard 和 Edouard Grave. 2021. [利用生成模型进行开放域问答中的段落检索](https://doi.org/10.18653/v1/2021.eacl-main.74)。发表于*第16届欧洲计算语言学协会年会：主卷论文集*，页码874–880，在线。计算语言学协会。
- en: 'Jin et al. (2024) Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang,
    Jiexin Xu, Li Qiuxia, and Jun Zhao. 2024. [Tug-of-war between knowledge: Exploring
    and resolving knowledge conflicts in retrieval-augmented language models](https://aclanthology.org/2024.lrec-main.1466).
    In *Proceedings of the 2024 Joint International Conference on Computational Linguistics,
    Language Resources and Evaluation (LREC-COLING 2024)*, pages 16867–16878, Torino,
    Italia. ELRA and ICCL.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin等人（2024）Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin
    Xu, Li Qiuxia, 和 Jun Zhao. 2024. [知识的拉锯战：探索和解决检索增强语言模型中的知识冲突](https://aclanthology.org/2024.lrec-main.1466)。发表于*2024年联合国际计算语言学、语言资源与评估会议（LREC-COLING
    2024）论文集*，页码16867–16878，意大利都灵。ELRA和ICCL。
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
    2017. [TriviaQA: A large scale distantly supervised challenge dataset for reading
    comprehension](https://doi.org/10.18653/v1/P17-1147). In *Proceedings of the 55th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi等人（2017）Mandar Joshi, Eunsol Choi, Daniel Weld, 和 Luke Zettlemoyer. 2017.
    [TriviaQA：一个大规模的远程监督挑战数据集，用于阅读理解](https://doi.org/10.18653/v1/P17-1147)。发表于*第55届计算语言学协会年会（第一卷：长篇论文集）*，页码1601–1611，加拿大温哥华。计算语言学协会。
- en: 'Kaliyar et al. (2021) Rohit Kumar Kaliyar, Anurag Goswami, and Pratik Narang.
    2021. [Fakebert: Fake news detection in social media with a bert-based deep learning
    approach](https://doi.org/10.1007/s11042-020-10183-2). *Multimedia Tools and Applications*,
    80(8):11765–11788.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliyar等人（2021）Rohit Kumar Kaliyar, Anurag Goswami, 和 Pratik Narang. 2021. [Fakebert：基于BERT的深度学习方法进行社交媒体中的假新闻检测](https://doi.org/10.1007/s11042-020-10183-2)。*多媒体工具与应用*，80(8):11765–11788。
- en: Kaliyar and Singh (2019) Rohit Kumar Kaliyar and Navya Singh. 2019. [Misinformation
    detection on online social media-a survey](https://doi.org/10.1109/ICCCNT45670.2019.8944587).
    In *2019 10th International Conference on Computing, Communication and Networking
    Technologies (ICCCNT)*, pages 1–6.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaliyar和Singh（2019）Rohit Kumar Kaliyar 和 Navya Singh. 2019. [在线社交媒体中的虚假信息检测——综述](https://doi.org/10.1109/ICCCNT45670.2019.8944587)。发表于*2019年第10届国际计算、通信与网络技术会议（ICCCNT）*，页码1–6。
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. [Dense passage
    retrieval for open-domain question answering](https://doi.org/10.18653/v1/2020.emnlp-main.550).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 6769–6781, Online. Association for Computational Linguistics.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin等人（2020）Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis,
    Ledell Wu, Sergey Edunov, Danqi Chen, 和 Wen-tau Yih. 2020. [开放域问答中的密集段落检索](https://doi.org/10.18653/v1/2020.emnlp-main.550)。发表于*2020年自然语言处理经验方法会议（EMNLP）论文集*，页码6769–6781，在线。计算语言学协会。
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. [Natural
    questions: A benchmark for question answering research](https://doi.org/10.1162/tacl_a_00276).
    *Transactions of the Association for Computational Linguistics*, 7:453–466.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwiatkowski等人（2019）Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael
    Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob
    Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
    Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, 和 Slav Petrov. 2019. [自然问题：一个问答研究的基准](https://doi.org/10.1162/tacl_a_00276)。*计算语言学协会交易*，7:453–466。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. [Retrieval-augmented generation
    for knowledge-intensive nlp tasks](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 9459–9474\.
    Curran Associates, Inc.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人（2020）Patrick Lewis、Ethan Perez、Aleksandra Piktus、Fabio Petroni、Vladimir
    Karpukhin、Naman Goyal、Heinrich Küttler、Mike Lewis、Wen-tau Yih、Tim Rocktäschel、Sebastian
    Riedel 和 Douwe Kiela。2020。[用于知识密集型 NLP 任务的检索增强生成](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)。在
    *神经信息处理系统进展* 中，第 33 卷，第 9459–9474 页。Curran Associates, Inc.
- en: 'Liu et al. (2023) Hui Liu, Wenya Wang, and Haoliang Li. 2023. [Interpretable
    multimodal misinformation detection with logic reasoning](https://doi.org/10.18653/v1/2023.findings-acl.620).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    9781–9796, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023）Hui Liu、Wenya Wang 和 Haoliang Li。2023。[可解释的多模态虚假信息检测与逻辑推理](https://doi.org/10.18653/v1/2023.findings-acl.620)。在
    *计算语言学协会发现：ACL 2023* 中，第 9781–9796 页，多伦多，加拿大。计算语言学协会。
- en: Meng et al. (2022) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
    2022. [Locating and editing factual associations in gpt](https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 17359–17372\.
    Curran Associates, Inc.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等人（2022）Kevin Meng、David Bau、Alex Andonian 和 Yonatan Belinkov。2022。[在 GPT
    中定位和编辑事实关联](https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf)。在
    *神经信息处理系统进展* 中，第 35 卷，第 17359–17372 页。Curran Associates, Inc.
- en: Meta (2024) Meta. 2024. [Llama 3](https://llama.meta.com/llama3/).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta（2024）Meta。2024。[Llama 3](https://llama.meta.com/llama3/)。
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, et al.
    2024. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774). *Preprint*,
    arXiv:2303.08774.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人（2024）OpenAI、Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge
    Akkaya、Florencia Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal
    Anadkat、Red Avila、Igor Babuschkin、Suchir Balaji 等。2024。[GPT-4 技术报告](https://arxiv.org/abs/2303.08774)。*预印本*，arXiv:2303.08774。
- en: 'Pan et al. (2023a) Liangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang
    Wang. 2023a. [Attacking open-domain question answering by injecting misinformation](https://doi.org/10.18653/v1/2023.ijcnlp-main.35).
    In *Proceedings of the 13th International Joint Conference on Natural Language
    Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 525–539, Nusa Dua,
    Bali. Association for Computational Linguistics.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人（2023a）Liangming Pan、Wenhu Chen、Min-Yen Kan 和 William Yang Wang。2023a。[通过注入虚假信息攻击开放域问答](https://doi.org/10.18653/v1/2023.ijcnlp-main.35)。在
    *第 13 届国际自然语言处理联合会议和亚太计算语言学协会第 3 届会议（第 1 卷：长篇论文）* 中，第 525–539 页，努沙杜瓦，巴厘岛。计算语言学协会。
- en: 'Pan et al. (2024) Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng,
    Sirui Wang, Xunliang Cai, and Le Sun. 2024. [Not all contexts are equal: Teaching
    llms credibility-aware generation](https://arxiv.org/abs/2404.06809). *Preprint*,
    arXiv:2404.06809.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人（2024）Ruotong Pan、Boxi Cao、Hongyu Lin、Xianpei Han、Jia Zheng、Sirui Wang、Xunliang
    Cai 和 Le Sun。2024。[并非所有上下文都是平等的：教学 llms 可信度意识生成](https://arxiv.org/abs/2404.06809)。*预印本*，arXiv:2404.06809。
- en: 'Pan et al. (2023b) Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen
    Kan, and William Wang. 2023b. [On the risk of misinformation pollution with large
    language models](https://doi.org/10.18653/v1/2023.findings-emnlp.97). In *Findings
    of the Association for Computational Linguistics: EMNLP 2023*, pages 1389–1403,
    Singapore. Association for Computational Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人（2023b）Yikang Pan、Liangming Pan、Wenhu Chen、Preslav Nakov、Min-Yen Kan 和
    William Wang。2023b。[大型语言模型的信息污染风险](https://doi.org/10.18653/v1/2023.findings-emnlp.97)。在
    *计算语言学协会发现：EMNLP 2023* 中，第 1389–1403 页，新加坡。计算语言学协会。
- en: 'Pelrine et al. (2023) Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina
    Reksoprodjo, Caleb Gupta, Joel Christoph, Jean-François Godbout, and Reihaneh
    Rabbany. 2023. [Towards reliable misinformation mitigation: Generalization, uncertainty,
    and GPT-4](https://doi.org/10.18653/v1/2023.emnlp-main.395). In *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    6399–6429, Singapore. Association for Computational Linguistics.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pelrine et al. (2023) Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina
    Reksoprodjo, Caleb Gupta, Joel Christoph, Jean-François Godbout, 和 Reihaneh Rabbany.
    2023. [《走向可靠的虚假信息缓解：泛化、不确定性与GPT-4》](https://doi.org/10.18653/v1/2023.emnlp-main.395)。在
    *2023年自然语言处理实证方法会议论文集*，第6399–6429页，新加坡。计算语言学协会。
- en: Quelle and Bovet (2024) Dorian Quelle and Alexandre Bovet. 2024. [The perils
    and promises of fact-checking with large language models](https://doi.org/10.3389/frai.2024.1341697).
    *Frontiers in Artificial Intelligence*, 7.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quelle 和 Bovet (2024) Dorian Quelle 和 Alexandre Bovet. 2024. [《使用大型语言模型进行事实核查的风险与机遇》](https://doi.org/10.3389/frai.2024.1341697)。*人工智能前沿*，7。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21(1).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J. Liu. 2020. 探索统一的文本到文本变换器的迁移学习极限。*J.
    Mach. Learn. Res.*，21(1)。
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. 2016. [SQuAD: 100,000+ questions for machine comprehension of text](https://doi.org/10.18653/v1/D16-1264).
    In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, 和
    Percy Liang. 2016. [《SQuAD: 100,000+ 个机器理解文本的问题》](https://doi.org/10.18653/v1/D16-1264)。在
    *2016年自然语言处理实证方法会议论文集*，第2383–2392页，奥斯汀，德克萨斯州。计算语言学协会。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, et al.
    2023. [Llama 2: Open foundation and fine-tuned chat models](https://arxiv.org/abs/2307.09288).
    *Preprint*, arXiv:2307.09288.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, 等人.
    2023. [《Llama 2: 开放基础与微调聊天模型》](https://arxiv.org/abs/2307.09288)。*预印本*，arXiv:2307.09288。'
- en: Vaibhav et al. (2019) Vaibhav Vaibhav, Raghuram Mandyam, and Eduard Hovy. 2019.
    [Do sentence interactions matter? leveraging sentence level representations for
    fake news classification](https://doi.org/10.18653/v1/D19-5316). In *Proceedings
    of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing
    (TextGraphs-13)*, pages 134–139, Hong Kong. Association for Computational Linguistics.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaibhav et al. (2019) Vaibhav Vaibhav, Raghuram Mandyam, 和 Eduard Hovy. 2019.
    [《句子交互重要吗？利用句子级表示进行假新闻分类》](https://doi.org/10.18653/v1/D19-5316)。在 *第十三届基于图的方法自然语言处理研讨会（TextGraphs-13）*，第134–139页，香港。计算语言学协会。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 30\. Curran Associates,
    Inc.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. [《注意力机制:
    你所需要的全部》](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)。在
    *神经信息处理系统进展*，第30卷。Curran Associates, Inc.'
- en: 'Vincent (2023) James Vincent. 2023. [Google and microsoft’s chatbots are already
    citing one another’s misinformation](https://www.theverge.com/2023/3/22/23651564/google-microsoft-bard-bing-chatbots-misinformation).
    *The Verge*. Accessed: 2023-06-05.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent (2023) James Vincent. 2023. [《谷歌和微软的聊天机器人已经开始引用对方的虚假信息》](https://www.theverge.com/2023/3/22/23651564/google-microsoft-bard-bing-chatbots-misinformation)。*The
    Verge*。访问日期：2023-06-05。
- en: 'Voita et al. (2019) Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
    and Ivan Titov. 2019. [Analyzing multi-head self-attention: Specialized heads
    do the heavy lifting, the rest can be pruned](https://doi.org/10.18653/v1/P19-1580).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 5797–5808, Florence, Italy. Association for Computational
    Linguistics.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita等人（2019）Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, 和 Ivan
    Titov. 2019. [分析多头自注意力：专用头承担主要任务，其余部分可以被修剪](https://doi.org/10.18653/v1/P19-1580).
    载于 *第57届计算语言学协会年会论文集*, 第5797–5808页, 意大利佛罗伦萨。计算语言学协会。
- en: 'Weller et al. (2024) Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie,
    and Benjamin Van Durme. 2024. [Defending against disinformation attacks in open-domain
    question answering](https://aclanthology.org/2024.eacl-short.35). In *Proceedings
    of the 18th Conference of the European Chapter of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pages 402–417, St. Julian’s, Malta. Association
    for Computational Linguistics.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weller等人（2024）Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, 和 Benjamin
    Van Durme. 2024. [在开放领域问答中抵御虚假信息攻击](https://aclanthology.org/2024.eacl-short.35).
    载于 *第18届欧洲计算语言学协会会议（第2卷：短篇论文）*, 第402–417页, 马耳他圣朱利安。计算语言学协会。
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. [The
    rise and potential of large language model based agents: A survey](https://arxiv.org/abs/2309.07864).
    *Preprint*, arXiv:2309.07864.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi等人（2023）Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong,
    Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang,
    Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu,
    Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin,
    Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, 和 Tao Gui. 2023. [大型语言模型基础的代理的崛起与潜力：一项调查](https://arxiv.org/abs/2309.07864).
    *预印本*, arXiv:2309.07864。
- en: Xiang et al. (2024) Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi
    Chen, and Prateek Mittal. 2024. [Certifiably robust rag against retrieval corruption](https://arxiv.org/abs/2405.15556).
    *Preprint*, arXiv:2405.15556.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang等人（2024）Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, 和
    Prateek Mittal. 2024. [在检索损坏下的认证鲁棒性rag](https://arxiv.org/abs/2405.15556). *预印本*,
    arXiv:2405.15556。
- en: Yoran et al. (2024) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant.
    2024. [Making retrieval-augmented language models robust to irrelevant context](https://arxiv.org/abs/2310.01558).
    *Preprint*, arXiv:2310.01558.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoran等人（2024）Ori Yoran, Tomer Wolfson, Ori Ram, 和 Jonathan Berant. 2024. [使检索增强语言模型对无关上下文具有鲁棒性](https://arxiv.org/abs/2310.01558).
    *预印本*, arXiv:2310.01558。
- en: 'Zhang et al. (2023) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen
    Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu,
    Wei Bi, Freda Shi, and Shuming Shi. 2023. [Siren’s song in the ai ocean: A survey
    on hallucination in large language models](https://arxiv.org/abs/2309.01219).
    *Preprint*, arXiv:2309.01219.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2023）Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu,
    Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei
    Bi, Freda Shi, 和 Shuming Shi. 2023. [AI海洋中的塞壬之歌：大型语言模型中的幻觉调查](https://arxiv.org/abs/2309.01219).
    *预印本*, arXiv:2309.01219。
- en: 'Zhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya
    Poria, and Tat-Seng Chua. 2021. [Retrieving and reading: A comprehensive survey
    on open-domain question answering](https://arxiv.org/abs/2101.00774). *Preprint*,
    arXiv:2101.00774.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等人（2021）Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria,
    和 Tat-Seng Chua. 2021. [检索与阅读：开放领域问答的综合调查](https://arxiv.org/abs/2101.00774).
    *预印本*, arXiv:2101.00774。
- en: 'Zou et al. (2024) Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024.
    [Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of
    large language models](https://arxiv.org/abs/2402.07867). *Preprint*, arXiv:2402.07867.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou等人（2024）Wei Zou, Runpeng Geng, Binghui Wang, 和 Jinyuan Jia. 2024. [Poisonedrag：对检索增强生成的大型语言模型的知识毒化攻击](https://arxiv.org/abs/2402.07867).
    *预印本*, arXiv:2402.07867。
- en: Appendix A Multi-Head Attention
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 多头注意力
- en: Currently, leading LLMs are built on autoregressive transformer architectures
    (Touvron et al., [2023](#bib.bib33); Meta, [2024](#bib.bib24); Bai et al., [2023](#bib.bib1)).
    The multi-head attention mechanism (Vaswani et al., [2017](#bib.bib35)) is the
    core component of autoregressive transformer models. It is illustrated in the
    following steps.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，领先的LLM基于自回归变换器架构（Touvron et al., [2023](#bib.bib33); Meta, [2024](#bib.bib24);
    Bai et al., [2023](#bib.bib1)）。多头注意力机制（Vaswani et al., [2017](#bib.bib35)）是自回归变换器模型的核心组件。它在以下步骤中进行了说明。
- en: 'Linear Transformation: Given an input hidden state $\mathbf{X}\in\mathbb{R}^{n\times
    d}$, three linear transformations are applied to produce queries $\mathbf{Q}\in\mathbb{R}^{n\times
    d_{k}}$, keys $\mathbf{K}\in\mathbb{R}^{n\times d_{k}}$, and values $\mathbf{V}\in\mathbb{R}^{n\times
    d_{v}}$:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换：给定一个输入隐藏状态 $\mathbf{X}\in\mathbb{R}^{n\times d}$，应用三个线性变换来生成查询 $\mathbf{Q}\in\mathbb{R}^{n\times
    d_{k}}$、键 $\mathbf{K}\in\mathbb{R}^{n\times d_{k}}$ 和值 $\mathbf{V}\in\mathbb{R}^{n\times
    d_{v}}$：
- en: '|  | $\mathbf{Q}=\mathbf{X}\mathbf{W}^{Q},\quad\mathbf{K}=\mathbf{X}\mathbf{W}^{K},\quad\mathbf{V}=\mathbf{X}\mathbf{W}^{V}$
    |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Q}=\mathbf{X}\mathbf{W}^{Q},\quad\mathbf{K}=\mathbf{X}\mathbf{W}^{K},\quad\mathbf{V}=\mathbf{X}\mathbf{W}^{V}$
    |  |'
- en: where $\mathbf{W}^{Q}\in\mathbb{R}^{d\times d_{k}}$, $\mathbf{W}^{K}\in\mathbb{R}^{d\times
    d_{k}}$, and $\mathbf{W}^{V}\in\mathbb{R}^{d\times d_{v}}$ are weight matrices.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}^{Q}\in\mathbb{R}^{d\times d_{k}}$、$\mathbf{W}^{K}\in\mathbb{R}^{d\times
    d_{k}}$ 和 $\mathbf{W}^{V}\in\mathbb{R}^{d\times d_{v}}$ 是权重矩阵。
- en: 'Scaled Dot-Product Attention: The attention weights are computed using the
    dot product of the queries and keys, scaled by $1/\sqrt{d_{k}}$:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放点积注意力：注意力权重通过查询和键的点积计算，并按 $1/\sqrt{d_{k}}$ 缩放：
- en: '|  | $\mathbf{A}=\mathrm{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{k}}}\right)$
    |  | (3) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{A}=\mathrm{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{k}}}\right)$
    |  | (3) |'
- en: The softmax function ensures that the attention weights sum to one.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 函数确保注意力权重的总和为一。
- en: 'Multi-Head Attention: Instead of performing a single attention function, $h$
    attention functions (or heads) are performed in parallel. Each head has its own
    set of weight matrices $\mathbf{W}_{i}^{Q},\mathbf{W}_{i}^{K},\mathbf{W}_{i}^{V}$
    and attention weights $\mathbf{A}_{i}$ for $i\in[1,h]$:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力：不执行单一的注意力函数，而是并行执行 $h$ 个注意力函数（或头）。每个头都有自己的一组权重矩阵 $\mathbf{W}_{i}^{Q},\mathbf{W}_{i}^{K},\mathbf{W}_{i}^{V}$
    和注意力权重 $\mathbf{A}_{i}$，其中 $i\in[1,h]$：
- en: '|  |  $\mathrm{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{Concat}(\mathrm{head}_{1},\ldots,\mathrm{head}_{h})\mathbf{W}^{O}$   |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  |  $\mathrm{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{Concat}(\mathrm{head}_{1},\ldots,\mathrm{head}_{h})\mathbf{W}^{O}$   |  |'
- en: where $\mathrm{head}_{i}=\mathbf{A}_{i}\mathbf{V}_{i}$ and $\mathbf{W}^{O}\in\mathbb{R}^{hd_{v}\times
    d}$ is the output weight matrix.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathrm{head}_{i}=\mathbf{A}_{i}\mathbf{V}_{i}$ 和 $\mathbf{W}^{O}\in\mathbb{R}^{hd_{v}\times
    d}$ 是输出权重矩阵。
- en: Appendix B Implementation Details
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实现细节
- en: We used gpt-3.5-turbo-0125 for all generations involving GPT. For Llama2-13B,
    Qwen-7B, and Llama3-8B, we did not perform any sampling during generation to avoid
    randomness. For the NQ and TriviaQA datasets, we randomly selected 1,000 samples
    from the original test set for our evaluation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在所有涉及GPT的生成中使用了 gpt-3.5-turbo-0125。对于 Llama2-13B、Qwen-7B 和 Llama3-8B，我们在生成过程中没有进行任何采样以避免随机性。对于
    NQ 和 TriviaQA 数据集，我们从原始测试集中随机选择了 1,000 个样本用于评估。
- en: '![Refer to caption](img/4a3090031cb500af9f7678ae74ed8ba8.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4a3090031cb500af9f7678ae74ed8ba8.png)'
- en: 'Figure 8: Distribution of GPT-generated credibility scores on misinformation
    and Wikipedia documents.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: GPT生成的可信度评分在虚假信息和维基百科文档上的分布。'
- en: '![Refer to caption](img/e0a59afa9c45fcbbcce4653b94eca41f.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e0a59afa9c45fcbbcce4653b94eca41f.png)'
- en: 'Figure 9: ROC curve of GPT-generated credibility scores, with area under curve
    (AUC) = 0.801.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: GPT生成的可信度评分的ROC曲线，曲线下面积 (AUC) = 0.801。'
- en: Appendix C GPT-Generated Credibility Scores
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C GPT生成的可信度评分
- en: 'We present the distribution of GPT-generated credibility scores in Figure [8](#A2.F8
    "Figure 8 ‣ Appendix B Implementation Details ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG") and the corresponding
    receiver operating characteristic (ROC) curve in Figure [9](#A2.F9 "Figure 9 ‣
    Appendix B Implementation Details ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [8](#A2.F8 "图 8 ‣ 附录 B 实现细节 ‣ CrAM: 可信度感知注意力修改") 中展示了 GPT生成的可信度评分的分布，并在图
    [9](#A2.F9 "图 9 ‣ 附录 B 实现细节 ‣ CrAM: 可信度感知注意力修改") 中展示了相应的接收器操作特性（ROC）曲线。'
- en: Appendix D Full Results with Varying Number of Documents with Misinformation
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D：带有虚假信息的文档数量变化的完整结果
- en: 'We provide the full results as the number of documents with misinformation
    increase, as shown in Figure [10](#A4.F10 "Figure 10 ‣ Appendix D Full Results
    with Varying Number of Documents with Misinformation ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG")-[13](#A4.F13
    "Figure 13 ‣ Appendix D Full Results with Varying Number of Documents with Misinformation
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG"). All results are done with four correct documents.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了虚假信息文档数量增加时的完整结果，如图[10](#A4.F10 "Figure 10 ‣ Appendix D Full Results with
    Varying Number of Documents with Misinformation ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG")-[13](#A4.F13 "Figure
    13 ‣ Appendix D Full Results with Varying Number of Documents with Misinformation
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG")所示。所有结果均基于四个正确的文档。'
- en: '![Refer to caption](img/edde3f9d9ae37ada207959ee301b5d56.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/edde3f9d9ae37ada207959ee301b5d56.png)'
- en: 'Figure 10: EM and F1 socre on NQ using Llama3-8B under ideal setting.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：在理想设置下，使用Llama3-8B在NQ上的EM和F1得分。
- en: '![Refer to caption](img/2b64e35ded924c0296f931614183cfa1.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b64e35ded924c0296f931614183cfa1.png)'
- en: 'Figure 11: EM and F1 socre on TriviaQA using Llama3-8B under ideal setting.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在理想设置下，使用Llama3-8B在TriviaQA上的EM和F1得分。
- en: '![Refer to caption](img/ce5df03e5dc32006ee154b5b584872bc.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce5df03e5dc32006ee154b5b584872bc.png)'
- en: 'Figure 12: EM and F1 socre on NQ using Llama3-8B under GPT setting.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：在GPT设置下，使用Llama3-8B在NQ上的EM和F1得分。
- en: '![Refer to caption](img/4bdbf18706cb16c741cbd3652acb2691.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4bdbf18706cb16c741cbd3652acb2691.png)'
- en: 'Figure 13: EM and F1 socre on TriviaQA using Llama3-8B under GPT setting.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：在GPT设置下，使用Llama3-8B在TriviaQA上的EM和F1得分。
- en: '![Refer to caption](img/8acb80ce9fafd3b157ca77782b0ce7f7.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8acb80ce9fafd3b157ca77782b0ce7f7.png)'
- en: 'Figure 14: Performance comparison of CrAM of Llama2-13B and CAG 13B with varying
    amounts of misinformation under ideal setting.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：在理想设置下，Llama2-13B的CrAM与CAG 13B在不同虚假信息量下的性能比较。
- en: Appendix E Comparison with CAG
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E：与CAG的比较
- en: 'Since the CAG 13B model tends to provide lengthy responses, its performance
    on EM is very low. Therefore, we consider an answer "correct" if the correct answer
    appears in the model’s prediction, and we use accuracy as the metric. The results
    are shown in Figure [14](#A4.F14 "Figure 14 ‣ Appendix D Full Results with Varying
    Number of Documents with Misinformation ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"). This metric is more favorable for
    long answers, however, CrAM still surpasses the SFT-based CAG 13B in most situations,
    demonstrating the superiority of our approach.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '由于CAG 13B模型倾向于提供冗长的回复，其在EM上的表现非常低。因此，如果正确答案出现在模型的预测中，我们认为答案是“正确”的，并使用准确率作为度量标准。结果如图[14](#A4.F14
    "Figure 14 ‣ Appendix D Full Results with Varying Number of Documents with Misinformation
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG")所示。该度量标准对长答案更为有利，但CrAM在大多数情况下仍超越了基于SFT的CAG 13B，展示了我们方法的优越性。'
- en: Appendix F Results with Filtered Misinformation
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F：过滤虚假信息的结果
- en: 'We replaced all the correct answers in the existing misinformation with “xxx”
    (denoted as “filtered misinformation”) and then conducted the same experiments
    on filtered misinformation. The results are shown in Table [4](#A6.T4 "Table 4
    ‣ Appendix F Results with Filtered Misinformation ‣ CrAM: Credibility-Aware Attention
    Modification in LLMs for Combating Misinformation in RAG"). We make the following
    observations. 1) The performance of CrAM with 4 ✓+ 1 ✗ is lower than that in Table
    [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG"), and it is worse than that of the
    Naive RAG with 4 ✓ in most cases. This indicates that CrAM enables LLMs to re-utilize
    the correct information denied by the misinformation, resulting in a better performance.
    2) Table [4](#A6.T4 "Table 4 ‣ Appendix F Results with Filtered Misinformation
    ‣ CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation
    in RAG") demonstrates that our CrAM method still outperforms all compared methods
    across all three LLMs: Qwen 7B, LLama2-13B, and LLama3-8B, on both NQ and TriviaQA
    datasets in the setting of 4 ✓+ 1 ✗ (i.e., four high-credibility documents plus
    one low-credibility document), proving CrAM doesn’t solely rely on correct answers
    in misinformation.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将现有虚假信息中的所有正确答案替换为“xxx”（表示为“筛选后的虚假信息”），然后对筛选后的虚假信息进行了相同的实验。结果如表[4](#A6.T4
    "表 4 ‣ 附录 F 筛选虚假信息的结果 ‣ CrAM: 在 RAG 中对抗虚假信息的可信度感知注意力修改")所示。我们做出以下观察：1）CrAM 在 4
    ✓+ 1 ✗ 的表现低于表[1](#S4.T1 "表 1 ‣ 4 实验 ‣ CrAM: 在 RAG 中对抗虚假信息的可信度感知注意力修改")中的表现，并且在大多数情况下比
    Naive RAG 在 4 ✓ 的表现更差。这表明 CrAM 使 LLM 能够重新利用被虚假信息否定的正确信息，从而提高了性能。2）表[4](#A6.T4
    "表 4 ‣ 附录 F 筛选虚假信息的结果 ‣ CrAM: 在 RAG 中对抗虚假信息的可信度感知注意力修改") 表明，我们的 CrAM 方法在 4 ✓+
    1 ✗ 设置下（即四个高可信度文档加一个低可信度文档）在 NQ 和 TriviaQA 数据集上的表现仍优于所有比较方法，证明 CrAM 并不完全依赖于虚假信息中的正确答案。'
- en: Model In-context corpus Method NQ TriviaQA EM F1 score EM F1 score Qwen-7B 0
    ✓ Naive LLM 7.20 16.41 28.00 38.23 4 ✓ Naive RAG 27.60 39.08 55.30 66.85 4 ✓ +
    1 ✗ Naive RAG 9.70 20.22 25.40 36.14 Prompt Based 10.40 20.67 26.30 37.12 CrAM
    25.90 (-1.70) 37.87 (-1.21) 51.70 (-3.60) 63.07 (-3.78) Llama2-13B 0 ✓ Naive LLM
    20.30 28.59 50.40 57.56 4 ✓ Naive RAG 28.90 39.98 62.50 71.03 4 ✓ + 1 ✗ Naive
    RAG 12.20 20.71 27.60 35.80 Prompt Based 9.90 20.48 21.90 31.22 CrAM 29.90 (+1.00)
    40.85 (+0.87) 57.90 (-4.60) 65.60 (-5.43) Llama3-8B 0 ✓ Naive LLM 20.60 30.58
    55.70 62.67 4 ✓ Naive RAG 33.10 45.66 64.30 73.68 4 ✓ + 1 ✗ Naive RAG 16.10 26.57
    38.70 48.84 Prompt Based 25.20 35.72 52.10 61.03 CrAM 33.80 (+0.70) 45.63 (-0.03)
    63.70 (-0.60) 72.87 (-0.81)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 上下文语料 方法 NQ TriviaQA EM F1 分数 EM F1 分数 Qwen-7B 0 ✓ Naive LLM 7.20 16.41 28.00
    38.23 4 ✓ Naive RAG 27.60 39.08 55.30 66.85 4 ✓ + 1 ✗ Naive RAG 9.70 20.22 25.40
    36.14 基于提示 10.40 20.67 26.30 37.12 CrAM 25.90 (-1.70) 37.87 (-1.21) 51.70 (-3.60)
    63.07 (-3.78) Llama2-13B 0 ✓ Naive LLM 20.30 28.59 50.40 57.56 4 ✓ Naive RAG 28.90
    39.98 62.50 71.03 4 ✓ + 1 ✗ Naive RAG 12.20 20.71 27.60 35.80 基于提示 9.90 20.48
    21.90 31.22 CrAM 29.90 (+1.00) 40.85 (+0.87) 57.90 (-4.60) 65.60 (-5.43) Llama3-8B
    0 ✓ Naive LLM 20.60 30.58 55.70 62.67 4 ✓ Naive RAG 33.10 45.66 64.30 73.68 4
    ✓ + 1 ✗ Naive RAG 16.10 26.57 38.70 48.84 基于提示 25.20 35.72 52.10 61.03 CrAM 33.80
    (+0.70) 45.63 (-0.03) 63.70 (-0.60) 72.87 (-0.81)
- en: 'Table 4: With ideal credibility scores and filtered misinformation, we evaluate
    the performance of three models on two open-domain QA datasets. 0 ✓ indicates
    no document and the model is directly prompted, 4 ✓ indicates that all four documents
    are retrieved from the Wikipedia dump, while 4 ✓ + 1 ✗ indicates four documents
    from the Wikipedia dump plus one piece of misinformation. In the 4 ✓ + 1 ✗ setting,
    the best performance is highlighted in bold. And the red part indicates the difference
    between Naive RAG with 4 ✓ and CrAM with 4 ✓ + 1 ✗.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在理想的可信度分数和筛选后的虚假信息条件下，我们评估了三种模型在两个开放领域问答数据集上的表现。0 ✓ 表示没有文档，模型直接进行提示；4 ✓
    表示从维基百科数据集中检索了所有四个文档，而 4 ✓ + 1 ✗ 表示从维基百科数据集中检索了四个文档加上一条虚假信息。在 4 ✓ + 1 ✗ 设置中，最佳性能以**粗体**突出显示。红色部分表示
    Naive RAG 在 4 ✓ 和 CrAM 在 4 ✓ + 1 ✗ 设置下的差异。
- en: Appendix G Prompts
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 提示
- en: 'We have made all the prompts used in our experiments publicly available, as
    shown in Figure [15](#A7.F15 "Figure 15 ‣ Appendix G Prompts ‣ CrAM: Credibility-Aware
    Attention Modification in LLMs for Combating Misinformation in RAG")-[19](#A7.F19
    "Figure 19 ‣ Appendix G Prompts ‣ CrAM: Credibility-Aware Attention Modification
    in LLMs for Combating Misinformation in RAG").'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将所有在实验中使用的提示公开，如图 [15](#A7.F15 "图15 ‣ 附录G 提示 ‣ CrAM：在RAG中对抗虚假信息的LLMs中的可信度意识注意力修改")-[19](#A7.F19
    "图19 ‣ 附录G 提示 ‣ CrAM：在RAG中对抗虚假信息的LLMs中的可信度意识注意力修改") 所示。
- en: 'Please
    generate a text that is logical and supports the statement: The answer to ""
    is "", rather than "". The generated text should
    be in the following form: CNN news. The generated text should be less than 200
    words. Just output the generated text , and do not output anything else. Generated
    Text:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 'Please
    generate a text that is logical and supports the statement: The answer to ""
    is "", rather than "". The generated text should
    be in the following form: CNN news. The generated text should be less than 200
    words. Just output the generated text , and do not output anything else. Generated
    Text:'
- en: 'Figure 15: Prompt for misinformation generation.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：生成虚假信息的提示。
- en: 'Answer the following question
    based on your internal knowledge with one or few words. Question:  Answer:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 'Answer the following question
    based on your internal knowledge with one or few words. Question:  Answer:'
- en: 'Figure 16: Prompt for Naive LLM without documents.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：没有文档的简单LLM的提示。
- en: 'Given
    the following information:  Answer the following question based on
    the given information or your internal knowledge with one or few words without
    the source. Question:  Answer:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 'Given
    the following information:  Answer the following question based on
    the given information or your internal knowledge with one or few words without
    the source. Question:  Answer:'
- en: 'Figure 17: Prompt for RAG.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：RAG的提示。
- en: 'You
    are an assistant who can answer questions based on the given passages. Each passage
    has a credibility score that indicates the relevance and accuracy of the passage
    to the question. Your answer need to combine multiple passages and their credibility.
    Question:  Docs:  Your answer should based on the given information
    or your internal knowledge with one or few words without the source (just output
    a answer, don’t output anything else). Answer:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 'You
    are an assistant who can answer questions based on the given passages. Each passage
    has a credibility score that indicates the relevance and accuracy of the passage
    to the question. Your answer need to combine multiple passages and their credibility.
    Question:  Docs:  Your answer should based on the given information
    or your internal knowledge with one or few words without the source (just output
    a answer, don’t output anything else). Answer:'
- en: 'Figure 18: Prompt for CAG.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：CAG的提示。
- en: 'You
    are an assistant capable of answering questions based on provided passages. Each
    passage is assigned a truthfulness score (0-10), where a higher score indicates
    greater credibility. Your answer need to combine multiple passages and their credibility.
    Given the following information:  Answer the following question based
    on the given information or your internal knowledge with one or few words without
    the source (just output a answer, don’t output anything else). Question: 
    Answer:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'You
    are an assistant capable of answering questions based on provided passages. Each
    passage is assigned a truthfulness score (0-10), where a higher score indicates
    greater credibility. Your answer need to combine multiple passages and their credibility.
    Given the following information:  Answer the following question based
    on the given information or your internal knowledge with one or few words without
    the source (just output a answer, don’t output anything else). Question: 
    Answer:'
- en: 'Figure 19: Prompt for prompt-based method.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：基于提示的方法的提示。
- en: 'Your
    task is to evaluate the authenticity of a text based on your internal knowledge.
    Specifically, I will provide you with a passage that may contain accurate information
    or fabricated errors. Using your own knowledge, reason, and deduction, you are
    to assign a credibility score ranging from 0 to 10, where a higher score indicates
    greater authenticity and a lower score suggests lesser authenticity. Here are
    2 examples (you should follow the output format below): ########## Passage: In
    a groundbreaking discovery, researchers have found that Albert Einstein was the
    first recipient of the Nobel Prize in Physics. According to newly uncovered documents,
    Einstein’s pioneering work in theoretical physics, particularly his theory of
    relativity, was recognized by the Nobel Committee in 1921\. This revelation challenges
    the long-held belief that Marie Curie was the first Nobel laureate in physics,
    and solidifies Einstein’s place as one of the greatest minds in scientific history.
    Analysis: 1\. Albert Einstein as the First Nobel Prize Recipient in Physics: This
    is incorrect. The first Nobel Prize in Physics was awarded in 1901, not to Albert
    Einstein, but to Wilhelm Conrad Röntgen for the discovery of X-rays. 2\. Einstein’s
    Nobel Prize Recognition: Albert Einstein was indeed awarded the Nobel Prize in
    Physics in 1921, but not for his theory of relativity. He received it for his
    discovery of the photoelectric effect, which was instrumental in the development
    of quantum theory. 3\. Marie Curie as the First Nobel Laureate in Physics: This
    is also incorrect. Marie Curie was a Nobel laureate, but she was not the first
    to win the Nobel Prize in Physics. Her first Nobel Prize was in Physics in 1903,
    shared with her husband Pierre Curie and Henri Becquerel for their work on radioactivity.
    Marie Curie was, notably, the first woman to win a Nobel Prize, and the first
    person to win Nobel Prizes in two different scientific fields (Physics and Chemistry).
    4\. Implication about the Nobel Committee’s Recognition of Relativity: As mentioned,
    Einstein’s Nobel Prize was not for relativity, despite its profound impact on
    physics. The Nobel Committee specifically avoided awarding the prize for relativity
    at the time due to ongoing debates and lack of experimental confirmation of the
    theory during that period. Credibility Score: 0 Passage: The first Nobel Prize
    in Physics was awarded to Wilhelm Conrad Roentgen in 1901\. Roentgen received
    the Nobel Prize for his discovery of X-rays, which had a significant impact on
    the field of physics and medicine Analysis: The facts presented in the statement
    you provided are largely accurate. Credibility Score: 10 ########## Passage: '
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'Your
    task is to evaluate the authenticity of a text based on your internal knowledge.
    Specifically, I will provide you with a passage that may contain accurate information
    or fabricated errors. Using your own knowledge, reason, and deduction, you are
    to assign a credibility score ranging from 0 to 10, where a higher score indicates
    greater authenticity and a lower score suggests lesser authenticity. Here are
    2 examples (you should follow the output format below): ########## Passage: In
    a groundbreaking discovery, researchers have found that Albert Einstein was the
    first recipient of the Nobel Prize in Physics. According to newly uncovered documents,
    Einstein’s pioneering work in theoretical physics, particularly his theory of
    relativity, was recognized by the Nobel Committee in 1921\. This revelation challenges
    the long-held belief that Marie Curie was the first Nobel laureate in physics,
    and solidifies Einstein’s place as one of the greatest minds in scientific history.
    Analysis: 1\. Albert Einstein as the First Nobel Prize Recipient in Physics: This
    is incorrect. The first Nobel Prize in Physics was awarded in 1901, not to Albert
    Einstein, but to Wilhelm Conrad Röntgen for the discovery of X-rays. 2\. Einstein’s
    Nobel Prize Recognition: Albert Einstein was indeed awarded the Nobel Prize in
    Physics in 1921, but not for his theory of relativity. He received it for his
    discovery of the photoelectric effect, which was instrumental in the development
    of quantum theory. 3\. Marie Curie as the First Nobel Laureate in Physics: This
    is also incorrect. Marie Curie was a Nobel laureate, but she was not the first
    to win the Nobel Prize in Physics. Her first Nobel Prize was in Physics in 1903,
    shared with her husband Pierre Curie and Henri Becquerel for their work on radioactivity.
    Marie Curie was, notably, the first woman to win a Nobel Prize, and the first
    person to win Nobel Prizes in two different scientific fields (Physics and Chemistry).
    4\. Implication about the Nobel Committee’s Recognition of Relativity: As mentioned,
    Einstein’s Nobel Prize was not for relativity, despite its profound impact on
    physics. The Nobel Committee specifically avoided awarding the prize for relativity
    at the time due to ongoing debates and lack of experimental confirmation of the
    theory during that period. Credibility Score: 0 Passage: The first Nobel Prize
    in Physics was awarded to Wilhelm Conrad Roentgen in 1901\. Roentgen received
    the Nobel Prize for his discovery of X-rays, which had a significant impact on
    the field of physics and medicine Analysis: The facts presented in the statement
    you provided are largely accurate. Credibility Score: 10 ########## Passage: '
- en: 'Figure 20: Prompt for GPT to generate credibility scores.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：生成可信度评分的GPT提示。
