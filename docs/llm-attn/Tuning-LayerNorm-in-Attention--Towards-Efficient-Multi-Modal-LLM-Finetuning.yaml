- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:03:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:03:01'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整Attention中的LayerNorm：迈向高效的多模态LLM微调
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.11420](https://ar5iv.labs.arxiv.org/html/2312.11420)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.11420](https://ar5iv.labs.arxiv.org/html/2312.11420)
- en: Bingchen Zhao*¹ Haoqin Tu*² Chen Wei³  Jieru Mei³  Cihang Xie⁴
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Bingchen Zhao*¹ Haoqin Tu*² Chen Wei³  Jieru Mei³  Cihang Xie⁴
- en: '*equal contribution'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*平等贡献'
- en: ¹ University of Edinburgh    ² University of Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 爱丁堡大学    ² 中国科学院大学
- en: ³ Johns Hopkins University    ⁴ UC Santa Cruz
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 约翰·霍普金斯大学    ⁴ 加州大学圣克鲁斯分校
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper introduces an efficient strategy to transform Large Language Models
    (LLMs) into Multi-Modal Large Language Models. By conceptualizing this transformation
    as a domain adaptation process, *i.e*., transitioning from text understanding
    to embracing multiple modalities, we intriguingly note that, within each attention
    block, tuning LayerNorm suffices to yield strong performance. Moreover, when benchmarked
    against other tuning approaches like full parameter finetuning or LoRA, its benefits
    on efficiency are substantial. For example, when compared to LoRA on a 13B model
    scale, performance can be enhanced by an average of over 20% across five multi-modal
    tasks, and meanwhile, results in a significant reduction of trainable parameters
    by 41.9% and a decrease in GPU memory usage by 17.6%. On top of this LayerNorm
    strategy, we showcase that selectively tuning only with conversational data can
    improve efficiency further. Beyond these empirical outcomes, we provide a comprehensive
    analysis to explore the role of LayerNorm in adapting LLMs to the multi-modal
    domain and improving the expressive power of the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一种有效的策略，将大语言模型（LLMs）转变为多模态大语言模型。通过将这种转变概念化为领域适应过程，即从文本理解过渡到接受多种模态，我们有趣地注意到，在每个注意力块中，调整LayerNorm足以产生强大的性能。此外，与全参数微调或LoRA等其他调优方法相比，其在效率上的优势显著。例如，与LoRA在13B模型规模上的比较，性能可以在五个多模态任务中平均提高20%以上，同时，训练参数减少41.9%，GPU内存使用减少17.6%。在这种LayerNorm策略之上，我们展示了仅用对话数据进行选择性调优可以进一步提高效率。除了这些经验结果，我们提供了全面的分析，以探讨LayerNorm在将LLMs适应多模态领域和提高模型表达能力中的作用。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Large Language Models (LLMs) have had many application scenarios since their
    debut. In particular, extending LLMs to handle multiple modalities has gathered
    much interest from both academia and industry. Such models, termed Multi-modal
    Large Language Models (MLLMs), are typically derived by finetuning a pretrained
    LLM on multi-modal data (Liu et al., [2023](#bib.bib21); Ye et al., [2023](#bib.bib38)).
    However, this process typically poses a substantial computational challenge (Liu
    et al., [2023](#bib.bib21)), particularly for exceptionally large-scale models.
    While Su et al. ([2023](#bib.bib29)); Zhang et al. ([2023](#bib.bib41)) employ
    low-rank adapters (LoRA) (Hu et al., [2022](#bib.bib12)) or soft prompts (Li &
    Liang, [2021a](#bib.bib16)) for more parameter-efficient tuning, this often comes
    at the cost of compromised performance on multi-modal tasks. This challenge prompts
    the pivotal question: *how can we make this process more efficient?*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自大语言模型（LLMs）问世以来，已拥有许多应用场景。特别是，将LLMs扩展到处理多种模态引起了学术界和工业界的广泛关注。这些模型被称为多模态大语言模型（MLLMs），通常通过在多模态数据上微调预训练的LLM而得出（Liu等，[2023](#bib.bib21)；Ye等，[2023](#bib.bib38)）。然而，这一过程通常会带来巨大的计算挑战（Liu等，[2023](#bib.bib21)），尤其是对于极大规模的模型。尽管Su等（[2023](#bib.bib29)）；Zhang等（[2023](#bib.bib41)）采用低秩适配器（LoRA）（Hu等，[2022](#bib.bib12)）或软提示（Li
    & Liang，[2021a](#bib.bib16)）进行更具参数效率的调优，但这通常以多模态任务性能的妥协为代价。这一挑战引出了一个关键问题：*我们如何使这一过程更高效？*
- en: 'In response to this challenge, we introduce a simple and effective strategy
    for MLLM finetuning: as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")(a),
    within each attention block, we adjust only the weights of the LayerNorm (Ba et al.,
    [2016](#bib.bib2)). This strategy is underpinned by the understanding that the
    evolution from LLMs to MLLMs can be conceptualized as a domain adaptation process,
    *i.e*., transitioning from text-centric to multi-modal understanding. Adjusting
    normalization layers, as suggested by prior research, emerges as a particularly
    effective technique in such domain shifts (Li et al., [2016](#bib.bib18)). Empirically,
    this straightforward technique can surprisingly yield comparable or even better
    performance than the strong baseline of finetuning all parameters offer about
    $10\times$ more parameter efficiency than LoRA.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这一挑战，我们提出了一种简单而有效的 MLLM 微调策略：如图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 在注意力中调整 LayerNorm：迈向高效的多模态
    LLM 微调") (a) 所示，在每个注意力块内，我们仅调整 LayerNorm 的权重（Ba et al., [2016](#bib.bib2)）。这一策略基于这样一种理解：从
    LLMs 到 MLLMs 的演变可以被概念化为一个领域适应过程，*即*，从以文本为中心转变为多模态理解。调整规范化层，如先前研究所建议的，在这样的领域转换中是一种特别有效的技术（Li
    et al., [2016](#bib.bib18)）。从经验来看，这种简单的技术可以出奇地提供与微调所有参数相当甚至更好的性能，提供比 LoRA 多约 $10\times$
    的参数效率。
- en: '![Refer to caption](img/b80eddc09ed01d3c8e932181049a04ce.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b80eddc09ed01d3c8e932181049a04ce.png)'
- en: 'Figure 1: (left) Different tuning methods for MLLMs. Trainable components are
    in blue, while frozen parameters are in gray. Within the attention blocks, (a)
    only activates LayerNorm parameters. Note that vision-language connector, word
    embedding, and output head paramters are by default activated for all three options.
    (right) Comparison on trainable parameters and GPU memory. Tuning LayerNorm achieves
    significant reductions in trainable parameters and GPU memory usages.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1： (左) MLLMs 的不同调整方法。可训练的组件用蓝色标出，而冻结的参数用灰色标出。在注意力块内，(a) 仅激活 LayerNorm 参数。请注意，视觉-语言连接器、词嵌入和输出头参数默认对所有三个选项均已激活。
    (右) 可训练参数和 GPU 内存的比较。调整 LayerNorm 显著减少了可训练参数和 GPU 内存的使用。
- en: 'By delving deeper, we note that the process can be further simplified by designating
    LayerNorm as the sole trainable component within the entire model. This means,
    in contrast to the typical configurations depicted in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal
    LLM Finetuning")(a)-(c), we now freeze the standardly activated elements, including
    the vision-language connector, word embedding, and the output head. We term it
    as LayerNorm-simple. Impressively, despite constituting a mere 0.004% of trainable
    parameters, this configuration surpasses the performance of LoRA, registering
    an average enhancement of 4.3% across five benchmarks.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入探讨，我们注意到可以通过将 LayerNorm 设定为整个模型中唯一可训练组件来进一步简化这一过程。这意味着，与图 [1](#S1.F1 "图
    1 ‣ 1 介绍 ‣ 在注意力中调整 LayerNorm：迈向高效的多模态 LLM 微调") (a)-(c) 所示的典型配置相比，我们现在冻结标准激活的元素，包括视觉-语言连接器、词嵌入和输出头。我们将其称为
    LayerNorm-simple。令人印象深刻的是，尽管仅占可训练参数的 0.004%，这一配置的表现超过了 LoRA，在五个基准测试中平均提升了 4.3%。
- en: 'On top of this LayerNorm strategy, we further improve the finetuning efficiency
    from the data perspective. Specifically, we assess the performance implications
    of different types of finetuning data, including conversational data, detailed
    description data, and complex reasoning data. Our results offer a crucial insight:
    not all data are created equal for the task of MLLM finetuning. Remarkably, we
    find that MLLMs finetuned on conversational data consistently outperform those
    finetuned on other data types. Specifically, conversational data improves the
    model performance by an average of 50% compared to other data types. This observation
    interestingly opens up avenues for more targeted data collection and curation
    strategies, thereby further optimizing the efficiency of MLLMs finetuning. Furthermore,
    by combining the LayerNorm strategy and this data perspective, we can achieve
    on average 10.0% performance improvement over full parameter finetuning on traditional
    VQA benchmarks with an LLaMA2 13B model while using significantly less parameters
    and data.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在此LayerNorm策略的基础上，我们从数据角度进一步提高了微调效率。具体而言，我们评估了不同类型微调数据的性能影响，包括对话数据、详细描述数据和复杂推理数据。我们的结果提供了一个关键的见解：并非所有数据对于MLLM微调任务都是平等的。值得注意的是，我们发现对话数据微调的MLLMs在性能上始终优于其他数据类型微调的模型。具体来说，对话数据相比于其他数据类型提高了模型性能平均50%。这一观察有趣地开启了更多有针对性的数据显示和策划策略，从而进一步优化MLLMs微调的效率。此外，通过结合LayerNorm策略和这一数据视角，我们可以在传统VQA基准上使用LLaMA2
    13B模型时，平均提高10.0%的性能，同时使用显著更少的参数和数据。
- en: Beyond the empirical outcomes above, we conduct an investigation into the expressive
    power of LayerNorm tuning. Our analysis reveals that LayerNorm-tuned MLLMs exhibit
    lower cross-layer similarity compared to models all of which parameters are finetuned.
    This lowered similarity is indicative of a more expressive model, since the model
    incorporates anisotropic layer presentations can capture a wider range of learning
    patterns (Pires et al., [2023](#bib.bib26)). It stands to reason that this amplified
    expressiveness is a key factor underpinning the efficiency and superior performance
    we noted, granting the model enhanced adaptability to novel multi-modal datasets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述的经验结果之外，我们还对LayerNorm调优的表达能力进行了调查。我们的分析揭示，LayerNorm调优的MLLMs相比于所有参数都经过微调的模型表现出较低的跨层相似性。这种降低的相似性表明模型更具表达能力，因为模型通过各向异性的层表示可以捕捉到更广泛的学习模式（Pires等人，[2023](#bib.bib26)）。可以推测，这种增强的表达能力是我们注意到的效率和优越性能的关键因素，使得模型能够更好地适应新颖的多模态数据集。
- en: In essence, our findings illuminate the profound influence of LayerNorm tuning,
    suggesting its potential to adeptly harness the intrinsic properties of LLMs.
    We hope that this study will catalyze subsequent research endeavors focused on
    efficient multi-modal finetuning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们的发现揭示了LayerNorm调优的深远影响，表明其有潜力熟练地利用LLMs的内在属性。我们希望这项研究能推动后续的研究工作，重点关注高效的多模态微调。
- en: 2 Related Works
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Multi-Modal and Large Language Models.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多模态和大语言模型。
- en: Multi-modality has been extensively studied in the literature. Starting from
    learning aligned representation across image and text modalities like CLIP (Radford
    et al., [2021](#bib.bib27)), many works have proposed more techniques (Yu et al.,
    [2022](#bib.bib40); Mu et al., [2022](#bib.bib24); Zhao et al., [2023](#bib.bib42)).
    Given the interest in developing instruction-tuned LLMs (Ouyang et al., [2022](#bib.bib25)),
    the studies of multi-modal models have also shifted the focus to instruction-tuned
    MLLMs. For example, LLaVA (Liu et al., [2023](#bib.bib21)) pioneers the development
    of instruction-tuned MLLMs by designing instruction-tuning data with the help
    of GPT-4. The concurrent work MiniGPT4 (Zhu et al., [2023](#bib.bib45)) is built
    using QFormers (Li et al., [2023a](#bib.bib15)) and Vicuna (Zheng et al., [2023](#bib.bib43)),
    but with only a linear layer activated for tuning. Similar to MiniGPT4, Su et al.
    ([2023](#bib.bib29)) devises PandaGPT with a more advanced vision encoder and
    LoRA-tuned LLM as its base models. mPLUG-Owl (Ye et al., [2023](#bib.bib38)) mixes
    text-only and multi-modal instruction data for finetuning LLaMA model. InstructBLIP (Dai
    et al., [2023](#bib.bib5)) builds on the BLIP2 model (Li et al., [2023a](#bib.bib15))
    with additional finetuning on instruction tuning datasets.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态在文献中已经得到了广泛的研究。从学习图像和文本模态对齐表示的方法如CLIP （Radford et al., [2021](#bib.bib27)）开始，许多研究工作提出了更多技术 （Yu
    et al., [2022](#bib.bib40)；Mu et al., [2022](#bib.bib24)；Zhao et al., [2023](#bib.bib42)）。鉴于对开发指令调优LLM的兴趣 （Ouyang
    et al., [2022](#bib.bib25)），多模态模型的研究也将重点转向了指令调优的MLLMs。例如，LLaVA （Liu et al., [2023](#bib.bib21)）通过借助GPT-4设计指令调优数据，开创了指令调优MLLM的发展。与此同时，MiniGPT4 （Zhu
    et al., [2023](#bib.bib45)）则使用了QFormers （Li et al., [2023a](#bib.bib15)）和Vicuna （Zheng
    et al., [2023](#bib.bib43)），但仅激活了一个线性层进行调优。类似于MiniGPT4，Su et al.（[2023](#bib.bib29)）设计了PandaGPT，其基础模型为更先进的视觉编码器和LoRA调优LLM。mPLUG-Owl （Ye
    et al., [2023](#bib.bib38)）混合了仅文本和多模态指令数据用于微调LLaMA模型。InstructBLIP （Dai et al.,
    [2023](#bib.bib5)）在BLIP2模型 （Li et al., [2023a](#bib.bib15)）基础上进行了额外的指令调优数据集微调。
- en: Parameter-Efficient Finetuning.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 参数高效微调。
- en: The parameter-efficient finetuning (PEFT) technique has been widely applied
    and studied because of huge resource consumption of larger and larger deep learning
    models. Adapter (Houlsby et al., [2019](#bib.bib11)) additionally adds trainable
    adapter components in the LM, which proved to achieve comparable results in NLP
    tasks with less than 10% trainable parameters. Prefix tuning (Li & Liang, [2021b](#bib.bib17))
    only inserts trainable parameters to the attention head in LMs LoRA (Hu et al.,
    [2022](#bib.bib12)), as the most widely employed PEFT method recently, injects
    trainable low rank decomposition matrices into a model to reduce the number of
    training parameters. Following the same line, QLoRA (Dettmers et al., [2023](#bib.bib7))
    achieves further reduction in the memory usage for finetuning LLMs with quantized
    4-bits parameters. These techniques have been widely utilized for the tuning of
    both LLMs (He et al., [2022](#bib.bib10); Tu et al., [2022](#bib.bib31)) and MLLMs (Zhu
    et al., [2023](#bib.bib45); Tu et al., [2023](#bib.bib32)). In this paper, we
    show that tuning LayerNorm in the LLM of an MLLM can achieve better results than
    tuning other components in the model and LoRA tuning, while requiring less resource
    for computation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）技术因大型深度学习模型消耗巨大的资源而被广泛应用和研究。Adapter （Houlsby et al., [2019](#bib.bib11)）在语言模型中额外添加了可训练的适配器组件，证明在NLP任务中用不到10%的可训练参数也能获得可比的结果。Prefix
    tuning （Li & Liang, [2021b](#bib.bib17)）仅在语言模型的注意力头中插入可训练参数。LoRA （Hu et al., [2022](#bib.bib12)），作为最近最广泛采用的PEFT方法，将可训练的低秩分解矩阵注入模型中，以减少训练参数的数量。沿着同样的思路，QLoRA （Dettmers
    et al., [2023](#bib.bib7)）通过量化的4位参数进一步减少了微调LLM的内存使用。这些技术已被广泛用于LLM （He et al.,
    [2022](#bib.bib10)；Tu et al., [2022](#bib.bib31)）和MLLM （Zhu et al., [2023](#bib.bib45)；Tu
    et al., [2023](#bib.bib32)）的调优。在本文中，我们展示了在MLLM的LLM中调优LayerNorm可以获得比调优模型中的其他组件和LoRA调优更好的结果，同时计算资源需求更少。
- en: The Normalization Studies.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正常化研究。
- en: The normalization layer in neural networks has long been a subject for debate
    and study. The foundational work on batch normalization (Ioffe & Szegedy, [2015](#bib.bib13))
    first introduces normalization as an important part of neural network architectures,
    and argues the effectiveness of normalization comes from alleviating the internal
    covariant shifting problem. Later works have proposed many variants of normalization,
    such as InstanceNorm (Ulyanov et al., [2016](#bib.bib33)), GroupNorm (Wu & He,
    [2018](#bib.bib36)), and LayerNorm (Ba et al., [2016](#bib.bib2)). LayerNorm has
    been the design choice of LLMs for normalization, and its effectiveness in LLM
    pretraining has also been explored and discussed (Xu et al., [2019](#bib.bib37)).
    In this work, we explore the effectiveness of finetuning LayerNorm in MLLMs as
    well as the reason behind the effectiveness.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的归一化层长期以来一直是研究和讨论的主题。批量归一化的基础工作 (Ioffe & Szegedy, [2015](#bib.bib13)) 首次将归一化引入神经网络架构中，论证了归一化的有效性在于缓解内部协方差偏移问题。后续工作提出了许多归一化的变体，例如实例归一化
    (Ulyanov 等, [2016](#bib.bib33))、组归一化 (Wu & He, [2018](#bib.bib36)) 和层归一化 (Ba 等,
    [2016](#bib.bib2))。层归一化一直是LLM选择的归一化设计方案，其在LLM预训练中的有效性也得到探索和讨论 (Xu 等, [2019](#bib.bib37))。本研究探讨了在MLLM中微调层归一化的有效性及其背后的原因。
- en: 'Table 1: Model performance on five multi-modal benchmarks with different components
    tuned in the LLM. We mark the best results with bold and the second best scores
    with underline. ‘-’ means the model cannot follow the required output format on
    captioning tasks.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：模型在五个多模态基准上的性能，显示了不同组件在LLM中的调整效果。我们用**粗体**标记最佳结果，用*下划线*标记第二最佳结果。‘-’表示模型无法满足字幕任务所需的输出格式。
- en: '| Models | MME $\uparrow$ | MSCOCO $\uparrow$ | POPE $\uparrow$ |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | MME $\uparrow$ | MSCOCO $\uparrow$ | POPE $\uparrow$ |'
- en: '| Baseline Models |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 基准模型 |'
- en: '| mPLUG-Owl | 967.4/276.1 | 21.38 | 70.70 | 41.78 | 50.9/54.0/50.7 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| mPLUG-Owl | 967.4/276.1 | 21.38 | 70.70 | 41.78 | 50.9/54.0/50.7 |'
- en: '| MiniGPT4 | 866.6/292.1 | 17.30 | - | - | 69.7/79.7/65.2 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT4 | 866.6/292.1 | 17.30 | - | - | 69.7/79.7/65.2 |'
- en: '| LLaVA-v0 | 502.8/214.6 | 15.06 | 58.89 | 23.02 | 70.5/74.6/66.0 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA-v0 | 502.8/214.6 | 15.06 | 58.89 | 23.02 | 70.5/74.6/66.0 |'
- en: '| MM-Vicuna-7B |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| MM-Vicuna-7B |'
- en: '| Finetune | 625.2/270.7 | 15.40 | 67.50 | 34.61 | 73.8/76.5/66.5 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Finetune | 625.2/270.7 | 15.40 | 67.50 | 34.61 | 73.8/76.5/66.5 |'
- en: '| LoRA | 552.3/217.5 | 15.00 | 63.93 | 34.13 | 50.4/51.6/50.4 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 552.3/217.5 | 15.00 | 63.93 | 34.13 | 50.4/51.6/50.4 |'
- en: '| Attn. QV Proj. | 678.0/277.5 | 15.51 | 72.63 | 32.24 | 72.0/77.1/65.3 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Attn. QV Proj. | 678.0/277.5 | 15.51 | 72.63 | 32.24 | 72.0/77.1/65.3 |'
- en: '| Attn. MLP | 637.3/268.2 | 15.37 | 65.22 | 37.47 | 60.0/68.2/56.6 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Attn. MLP | 637.3/268.2 | 15.37 | 65.22 | 37.47 | 60.0/68.2/56.6 |'
- en: '| LayerNorm | 723.2/253.2 | 17.06 | 80.89 | 48.01 | 76.1/81.1/70.8 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm | 723.2/253.2 | 17.06 | 80.89 | 48.01 | 76.1/81.1/70.8 |'
- en: '| LayerNorm-simp. | 720.9/251.8 | 23.46 | 79.75 | 46.18 | 61.1/72.3/58.5 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-simp. | 720.9/251.8 | 23.46 | 79.75 | 46.18 | 61.1/72.3/58.5 |'
- en: '| MM-LLaMA2-7B |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-7B |'
- en: '| Finetune | 661.3/237.1 | 16.09 | 65.08 | 31.64 | 56.3/65.0/55.4 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Finetune | 661.3/237.1 | 16.09 | 65.08 | 31.64 | 56.3/65.0/55.4 |'
- en: '| LoRA | 395.0/200.0 | 14.87 | 61.97 | 26.17 | 51.9/54.7/51.3 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 395.0/200.0 | 14.87 | 61.97 | 26.17 | 51.9/54.7/51.3 |'
- en: '| Attn. QV Proj. | 584.0/222.9 | 16.39 | 76.05 | 42.93 | 55.7/63.0/56.8 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Attn. QV Proj. | 584.0/222.9 | 16.39 | 76.05 | 42.93 | 55.7/63.0/56.8 |'
- en: '| Attn. MLP | 413.1/203.6 | 15.29 | 58.35 | 29.04 | 53.7/59.6/53.9 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Attn. MLP | 413.1/203.6 | 15.29 | 58.35 | 29.04 | 53.7/59.6/53.9 |'
- en: '| LayerNorm | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9 |'
- en: '| LayerNorm-simp. | 542.6/205.0 | 14.98 | 65.10 | 46.88 | 51.6/52.5/51.1 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-simp. | 542.6/205.0 | 14.98 | 65.10 | 46.88 | 51.6/52.5/51.1 |'
- en: '| MM-LLaMA2-chat-7B |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-chat-7B |'
- en: '| Finetune | 805.4/234.6 | 15.29 | 66.33 | 26.70 | 60.3/69.8/57.9 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Finetune | 805.4/234.6 | 15.29 | 66.33 | 26.70 | 60.3/69.8/57.9 |'
- en: '| LoRA | 709.8/228.6 | 15.28 | 57.27 | 25.49 | 59.2/65.9/56.8 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 709.8/228.6 | 15.28 | 57.27 | 25.49 | 59.2/65.9/56.8 |'
- en: '| Attn. QV Proj. | 926.5/220.7 | 15.88 | 58.49 | 31.10 | 68.5/77.3/65.0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Attn. QV Proj. | 926.5/220.7 | 15.88 | 58.49 | 31.10 | 68.5/77.3/65.0 |'
- en: '| Attn. MLP | 840.0/240.0 | 15.20 | 54.42 | 24.89 | 56.9/67.3/56.8 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Attn. MLP | 840.0/240.0 | 15.20 | 54.42 | 24.89 | 56.9/67.3/56.8 |'
- en: '| LayerNorm | 651.3/219.3 | 16.60 | 75.34 | 43.75 | 71.3/72.4/67.8 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm | 651.3/219.3 | 16.60 | 75.34 | 43.75 | 71.3/72.4/67.8 |'
- en: '| LayerNorm-simp. | 372.0/169.3 | 18.42 | 59.99 | 41.63 | 52.0/54.6/52.3 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-simp. | 372.0/169.3 | 18.42 | 59.99 | 41.63 | 52.0/54.6/52.3 |'
- en: '| MM-LLaMA2-13B |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-13B |'
- en: '| Finetune | 402.3/199.3 | 18.33 | 73.88 | 45.33 | 51.6/51.1/52.2 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Finetune | 402.3/199.3 | 18.33 | 73.88 | 45.33 | 51.6/51.1/52.2 |'
- en: '| LoRA | 400.8/189.3 | 16.08 | 68.83 | 43.70 | 50.5/51.2/50.5 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 400.8/189.3 | 16.08 | 68.83 | 43.70 | 50.5/51.2/50.5 |'
- en: '| Attn. QV Proj. | 489.0/200.4 | 15.12 | 63.07 | 32.81 | 51.1/52.9/52.5 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Attn. QV Proj. | 489.0/200.4 | 15.12 | 63.07 | 32.81 | 51.1/52.9/52.5 |'
- en: '| Attn. MLP | 387.5/167.1 | 25.19 | 64.19 | 44.06 | 50.7/52.2/51.9 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Attn. MLP | 387.5/167.1 | 25.19 | 64.19 | 44.06 | 50.7/52.2/51.9 |'
- en: '| LayerNorm | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9 |'
- en: '| LayerNorm-simp. | 403.3/185.4 | 18.62 | 68.28 | 43.04 | 55.3/58.0/57.2 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-简化 | 403.3/185.4 | 18.62 | 68.28 | 43.04 | 55.3/58.0/57.2 |'
- en: '| MM-LLaMA2-chat-13B |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-chat-13B |'
- en: '| Finetune | 623.3/221.4 | 15.17 | 64.19 | 41.82 | 67.6/64.8/64.5 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Finetune | 623.3/221.4 | 15.17 | 64.19 | 41.82 | 67.6/64.8/64.5 |'
- en: '| LoRA | 516.7/214.3 | 14.39 | 66.33 | 43.09 | 66.9/64.1/63.8 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 516.7/214.3 | 14.39 | 66.33 | 43.09 | 66.9/64.1/63.8 |'
- en: '| Attn. QV Proj. | 624.5/250.4 | 14.91 | 60.96 | 34.90 | 66.3/66.0/61.8 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 注意事项 QV Proj. | 624.5/250.4 | 14.91 | 60.96 | 34.90 | 66.3/66.0/61.8 |'
- en: '| Attn. MLP | 456.7/211.4 | 14.67 | 62.19 | 40.39 | 56.8/56.9/56.5 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Attn. MLP | 456.7/211.4 | 14.67 | 62.19 | 40.39 | 56.8/56.9/56.5 |'
- en: '| LayerNorm | 929.3/254.3 | 16.10 | 74.96 | 42.79 | 78.9/83.9/74.3 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm | 929.3/254.3 | 16.10 | 74.96 | 42.79 | 78.9/83.9/74.3 |'
- en: '| LayerNorm-simp. | 824.3/221.1 | 13.29 | 52.70 | 40.20 | 73.3/76.0/69.0 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-简化 | 824.3/221.1 | 13.29 | 52.70 | 40.20 | 73.3/76.0/69.0 |'
- en: 3 Tuning and Evaluation Settings
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 调优与评估设置
- en: In this section, we first introduce the comment structure of MLLMs, different
    tuning strategies of MLLMs, and then present the evaluation benchmarks employed
    in the paper.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先介绍 MLLMs 的评论结构、不同的 MLLMs 调优策略，然后介绍论文中使用的评估基准。
- en: Architecture of MLLMs.
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MLLMs 的架构。
- en: 'A typical MLLM usually contains three parts: 1) A vision encoder for extracting
    visual features; 2) An LLM decoder for generating plausible texts from both text
    instructions and visual information; 3) A vision-language connector for bridging
    between the vision encoder and the LLM. We follow Liu et al. ([2023](#bib.bib21))
    to set up the model, where the vision encoder is a CLIP-pretrained ViT-L (Radford
    et al., [2021](#bib.bib27)) and the vision-language connector is a simple linear
    projector.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 MLLM 通常包含三个部分：1) 用于提取视觉特征的视觉编码器；2) 用于从文本指令和视觉信息生成合理文本的 LLM 解码器；3) 用于在视觉编码器和
    LLM 之间建立桥梁的视觉语言连接器。我们按照刘等人 ([2023](#bib.bib21)) 的方法建立模型，其中视觉编码器是 CLIP 预训练的 ViT-L
    (Radford et al., [2021](#bib.bib27))，视觉语言连接器是一个简单的线性投影器。
- en: 'We experiment with a range of LLMs for the language decoder. Specifically,
    we choose three types of LLM with 7B and 13B scales: Vicuna-7B (v1.1) (Zheng et al.,
    [2023](#bib.bib43)), LLaMA2-7B&13B, and LLaMA2-chat-7B&13B (Touvron et al., [2023](#bib.bib30)).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在语言解码器上实验了多种 LLM。具体来说，我们选择了三种规模为 7B 和 13B 的 LLM 类型：Vicuna-7B (v1.1) (Zheng
    et al., [2023](#bib.bib43))，LLaMA2-7B&13B，以及 LLaMA2-chat-7B&13B (Touvron et al.,
    [2023](#bib.bib30))。
- en: Baseline Models.
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准模型。
- en: 'Apart from our trained MLLMs, we showcase the performances of three publicly
    available models: mPLUG-Owl (Ye et al., [2023](#bib.bib38)), MiniGPT4 (Zhu et al.,
    [2023](#bib.bib45)), and LLaVA-v0 (Liu et al., [2023](#bib.bib21)). The LLaVA-v0
    here represents the initial release of LLaVA. These baseline results are obtained
    from existing literature (Fu et al., [2023](#bib.bib9); Li et al., [2023b](#bib.bib19))
    or tested using their released checkpoints.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们训练的 MLLMs，我们还展示了三种公开可用模型的性能：mPLUG-Owl (Ye et al., [2023](#bib.bib38))，MiniGPT4
    (Zhu et al., [2023](#bib.bib45))，以及 LLaVA-v0 (Liu et al., [2023](#bib.bib21))。这里的
    LLaVA-v0 代表 LLaVA 的初始版本。这些基准结果来自现有文献 (Fu et al., [2023](#bib.bib9)；Li et al.,
    [2023b](#bib.bib19)) 或使用其发布的检查点进行测试。
- en: Tuning Modules.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调优模块。
- en: 'To analyze the effects of different tuning components in the MLLMs, we employ
    five different tuning paradigms on the same training corpus. (1) finetune: activates
    all the parameters in LLM for MLLMs tuning; (2) LoRA: inserts LoRA component (Hu
    et al., [2022](#bib.bib12)) with rank 32 between all linear structure in the LLM;
    (3) Attn. QV Proj.: activates Q and V linear projection in attention of the LLM,
    as they are proved to be especially effective for tuning LLMs (Hu et al., [2022](#bib.bib12));
    (4) Attn. MLP: activates MLP layers in attention of the LLM; (5) LayerNorm: both
    input and post LayerNorm in attention blocks of the LLM. Note that all tuning
    methods activate vision-language connector for training.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析不同调优组件在 MLLMs 中的效果，我们在相同的训练语料库上应用了五种不同的调优范式。(1) finetune：激活 LLM 中的所有参数进行
    MLLMs 调优；(2) LoRA：在 LLM 中所有线性结构之间插入具有 32 排序的 LoRA 组件 (Hu et al., [2022](#bib.bib12))；(3)
    Attn. QV Proj.：激活 LLM 中的 Q 和 V 线性投影，因为它们被证明对调优 LLM 特别有效 (Hu et al., [2022](#bib.bib12))；(4)
    Attn. MLP：激活 LLM 中注意力机制中的 MLP 层；(5) LayerNorm：激活 LLM 注意力模块中的输入和后置 LayerNorm。请注意，所有调优方法都激活视觉语言连接器进行训练。
- en: Training Details.
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练细节。
- en: We pre-train the vision-language connector for 3 epochs on CC3M (Changpinyo
    et al., [2021](#bib.bib3)), and conduct the finetuning stage on 80K filtered image-text
    pairs collected by Liu et al. ([2023](#bib.bib21)) for 1 epoch. For the first
    stage, we set the learning rate to 2e-3 for all variants. During the second stage,
    we search the learning rate from 2e-3 to 1e-7 with 11 options for all tuning strategies
    and pick the best learning rate based on their performances on Flickr30k task.
    We set the weight decay (Loshchilov & Hutter, [2019](#bib.bib23)) to 0 and a warmup
    ratio to 0.03 with the cosine learning rate scheduler (Loshchilov & Hutter, [2017](#bib.bib22)).
    Moreover, we employ the gradient checkpointing (Chen et al., [2016](#bib.bib4)),
    DeepSpeed technique (Rajbhandari et al., [2020](#bib.bib28)), and a data precision
    of TensorFloat32 for models training. We conduct all of our experiments on 4 80G
    A100 GPUs on the same node.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在CC3M (Changpinyo等，[2021](#bib.bib3))上对视觉-语言连接器进行了3轮预训练，并在刘等（[2023](#bib.bib21)）收集的80K筛选后的图像-文本对上进行了一轮微调。在第一阶段，我们将所有变体的学习率设置为2e-3。在第二阶段，我们从2e-3到1e-7范围内搜索学习率，提供了11种选项，并根据它们在Flickr30k任务上的表现选择最佳学习率。我们将权重衰减（Loshchilov
    & Hutter，[2019](#bib.bib23)）设置为0，warmup比例设置为0.03，并使用余弦学习率调度器（Loshchilov & Hutter，[2017](#bib.bib22)）。此外，我们使用了梯度检查点技术（Chen等，[2016](#bib.bib4)）、DeepSpeed技术（Rajbhandari等，[2020](#bib.bib28)）和TensorFloat32的数据精度进行模型训练。我们在同一节点上的4块80G
    A100 GPU上进行了所有实验。
- en: Multi-Modal Benchmarks.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多模态基准测试。
- en: 'We test the visual-instruction tuned models on recent multi-modal evaluation
    benchmarks, where five multi-modal benchmarks are deployed: MME (Fu et al., [2023](#bib.bib9))
    consists of two evaluation aspects, *i.e*., cognition (CS) and perception (PS)
    with total 14 VQA tasks; VQAv2 (Antol et al., [2015](#bib.bib1)), MSCOCO (Lin
    et al., [2014](#bib.bib20)) and Flickr30k (Young et al., [2014](#bib.bib39)) captioning
    tasks are commonly used benchmarks in the field of VQA and captioning. The former
    two benchmarks are based on MSCOCO-2017 dataset (Lin et al., [2014](#bib.bib20)).
    For the latter two captioning tasks, we report the zero-shot CIDEr (Vedantam et al.,
    [2015](#bib.bib34)) scores (with three text-only QA examples) on the test set
    from Karpathy & Fei-Fei ([2015](#bib.bib14)). POPE (Li et al., [2023b](#bib.bib19))
    is used to evaluate the level of object hallucinations in MLLMs, which consists
    of three versions of balanced yes/no VQA tasks (*i.e*., Popular/Random/Adversarial)
    considering objects in the given image.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在近期的多模态评估基准上测试了视觉-指令微调模型，其中部署了五个多模态基准：MME（Fu等，[2023](#bib.bib9)）包括两个评估方面，即认知（CS）和感知（PS），总共14个VQA任务；VQAv2（Antol等，[2015](#bib.bib1)）、MSCOCO（Lin等，[2014](#bib.bib20)）和Flickr30k（Young等，[2014](#bib.bib39)）的图像描述任务是VQA和图像描述领域常用的基准。前两个基准基于MSCOCO-2017数据集（Lin等，[2014](#bib.bib20)）。对于后两个图像描述任务，我们报告了来自Karpathy
    & Fei-Fei（[2015](#bib.bib14)）测试集上的零样本CIDEr（Vedantam等，[2015](#bib.bib34)）得分（以三个仅文本QA示例）。POPE（Li等，[2023b](#bib.bib19)）用于评估MLLM中的对象幻觉水平，包含三个版本的平衡是/否VQA任务（即，流行/随机/对抗），考虑给定图像中的对象。
- en: 'Table 2: Memory consumption and percentages of trainable parameters tested
    on a single A100 GPU.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在单个A100 GPU上测试的内存消耗和可训练参数的百分比。
- en: '| Model | 7B scale | 13B scale |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 7B规模 | 13B规模 |'
- en: '| Mem. (GB) | #param. | Mem. (GB) | #param. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 内存 (GB) | 参数数量 | 内存 (GB) | 参数数量 |'
- en: '| Finetune | OOM | 95.70% | OOM | 97.72% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | OOM | 95.70% | OOM | 97.72% |'
- en: '| LoRA | 29.4 | 5.92% | 46.5 | 4.30% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 29.4 | 5.92% | 46.5 | 4.30% |'
- en: '| Attn. QV Proj. | 57.0 | 19.02% | OOM | 18.24% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Attn. QV Proj. | 57.0 | 19.02% | OOM | 18.24% |'
- en: '| Attn. MLP | OOM | 65.21% | OOM | 66.24% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Attn. MLP | OOM | 65.21% | OOM | 66.24% |'
- en: '| LayerNorm | 24.2 | 3.78% | 38.3 | 2.50% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm | 24.2 | 3.78% | 38.3 | 2.50% |'
- en: '| LayerNorm-simp. | 18.9 | 0.004% | 31.7 | 0.003% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm简化版 | 18.9 | 0.004% | 31.7 | 0.003% |'
- en: 4 Experimental Results
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验结果
- en: 4.1 Tuning LayerNorm
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 调整LayerNorm
- en: Tuning LayerNorm in Attention Blocks.
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在注意力块中调整LayerNorm。
- en: 'In [table 1](#S2.T1 "In The Normalization Studies. ‣ 2 Related Works ‣ Tuning
    LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"), it is
    noteworthy that activating only the LayerNorm yields the least activated parameters,
    yet the model performances are surprisingly impressive when compared to tuning
    other modules. Specifically, in two captioning tasks, the VQAv2 task, and the
    challenging hallucination benchmark POPE, models with only the LayerNorm activated
    consistently outperform all other competitors by at least 8.2%. On the comprehensively
    evaluated benchmark MME, while tuning LayerNorm outperforms finetuning the intact
    language model by an average of 6.6% on the Perception aspect, it lags behind
    finetuning by an average of 6.3% on the Cognition score. It is vital to note,
    however, that the LayerNorm only accounts for approximately 2.5% of the training
    parameters in the whole model.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 1](#S2.T1 "在归一化研究中。 ‣ 2 相关工作 ‣ 在注意力中调整 LayerNorm：朝向高效的多模态 LLM 微调")中，值得注意的是，仅激活
    LayerNorm 会产生最少的激活参数，但与调整其他模块相比，模型性能令人惊讶地出色。具体而言，在两个标注任务、VQAv2 任务以及具有挑战性的幻觉基准
    POPE 中，仅激活 LayerNorm 的模型 consistently 比所有其他竞争对手高出至少 8.2%。在综合评估的 MME 基准中，虽然调整 LayerNorm
    在感知方面的表现比完全微调高出平均 6.6%，但在认知得分上却比完全微调低了平均 6.3%。然而，值得注意的是，LayerNorm 仅占整个模型训练参数的约
    2.5%。
- en: In addition to tuning modules, another observation is that MLLMs incorporating
    human-aligned LLMs (such as LLaMA2-chat) exhibit superior performance in complex
    and demanding tasks such as POPE and MME compared to their unaligned counterparts.
    This underscores the importance of utilizing aligned LLMs to construct a more
    powerful MLLMs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调整模块，另一个观察是，融入人类对齐的 LLM（如 LLaMA2-chat）的 MLLMs 在处理复杂且要求高的任务（如 POPE 和 MME）时，比其未对齐的对手表现更优。这突显了使用对齐
    LLMs 构建更强大的 MLLMs 的重要性。
- en: Tuning LayerNorm and Only LayerNorm.
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调整 LayerNorm 仅调整 LayerNorm。
- en: 'As the above LayerNorm method finetunes (1) vision-language connector, (2)
    word embedding, (3) output head, and (4) LayerNorm component in the LLM simultaneously,
    a pertinent question arises: *Is it possible for (4) LayerNorm alone to generalize
    effectively in training MLLMs?* To address this query, we take a step further
    and solely finetune LayerNorm in MLLMs, which is denoted as LayerNorm-simp. in [table 1](#S2.T1
    "In The Normalization Studies. ‣ 2 Related Works ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"). The results are intriguing, demonstrating
    that even with a mere 0.004% parameter finetuning in the whole model, LayerNorm-simp.
    surpasses full parameter finetuning on three conventional vision-language tasks
    (i.e., two captioning and one VQA tasks) by 10%, and only lags behind full finetuning
    by 7.9% on the MME benchmark. This intriguing discovery suggests that the transition
    from LLM to MLLMs probably involves a domain adaptation process as the LayerNorm
    takes the most credits in tuning a well-behaved MLLMs. The LayerNorm alone may
    be also capable of integrating vision information with language tokens seamlessly.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述 LayerNorm 方法同时微调 (1) 视觉语言连接器、(2) 词嵌入、(3) 输出头和 (4) LLM 中的 LayerNorm 组件，因此出现了一个相关问题：*仅凭
    (4) LayerNorm 是否能有效地在 MLLMs 训练中推广？* 为了回答这个问题，我们进一步仅微调 MLLMs 中的 LayerNorm，简称为 LayerNorm-simp.，见 [表
    1](#S2.T1 "在归一化研究中。 ‣ 2 相关工作 ‣ 在注意力中调整 LayerNorm：朝向高效的多模态 LLM 微调")。结果颇具趣味，表明即使在整个模型中仅微调了
    0.004% 的参数，LayerNorm-simp. 也比在三个传统视觉语言任务（即两个标注任务和一个 VQA 任务）上完全微调提高了 10%，且在 MME
    基准测试中仅落后完全微调 7.9%。这一有趣发现表明，从 LLM 到 MLLMs 的过渡可能涉及一个领域适应过程，因为 LayerNorm 在调优一个表现良好的
    MLLMs 中起到了最大作用。LayerNorm 单独也可能能够无缝地将视觉信息与语言标记结合起来。
- en: Memory Consumption and Parameter Efficiency.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存消耗和参数效率。
- en: 'In table [2](#S3.T2 "Table 2 ‣ Multi-Modal Benchmarks. ‣ 3 Tuning and Evaluation
    Settings ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"),
    we present the total memory consumption and the percentage of trainable parameters
    of each MLLMs finetuning method across 7B and 13B scales. Methods like full parameter
    finetuning and finetuning MLPs in attention modules face out-of-memory (OOM) issue
    even on a high-capacity 80GB A100 GPU, while LayerNorm based methods stand out
    for their efficiency. Specifically, LayerNorm tuning requires only 24.2 GB and
    38.3 GB memory at 7B and 13B scales respectively. Remarkably, LayerNorm-simp.
    further reduces the memory to 18.9 GB and 31.7 GB. In terms of trainable parameters,
    LayerNorm based methods also show remarkable efficiency, LayerNorm utilizes only
    3.78% and 2.50% of the total parameters at the 7B and 13B scales, and LayerNorm-simp.
    takes efficiency to an extreme, involving only 0.004% and 0.003% of the parameters
    at these scales. These results demonstrate the efficiency advantage of LayerNorm
    tuning, compared with existing methods like LoRA or full parameter finetuning.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [2](#S3.T2 "表2 ‣ 多模态基准测试 ‣ 3 调优和评估设置 ‣ 在注意力中的调优LayerNorm: 朝向高效的多模态LLM微调")中，我们展示了在7B和13B规模下，每种MLLM微调方法的总内存消耗和可训练参数的百分比。诸如全参数微调和在注意力模块中微调MLP的方法，即使在高容量的80GB
    A100 GPU上也会面临内存不足（OOM）问题，而基于LayerNorm的方法因其效率而脱颖而出。具体来说，LayerNorm调优在7B和13B规模下仅需24.2
    GB和38.3 GB的内存。值得注意的是，LayerNorm-simp. 进一步将内存减少到18.9 GB和31.7 GB。在可训练参数方面，基于LayerNorm的方法也表现出显著的效率，LayerNorm在7B和13B规模下仅利用了总参数的3.78%和2.50%，而LayerNorm-simp.
    的效率更高，仅涉及0.004%和0.003%的参数。这些结果展示了LayerNorm调优的效率优势，相比于现有方法如LoRA或全参数微调。'
- en: '![Refer to caption](img/0718d25657b340190137f6e469a263bf.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0718d25657b340190137f6e469a263bf.png)'
- en: 'Figure 2: Performances of models that are finetuned on different datasets on
    four multi-modal benchmarks. The MME score is the sum of both Cognition and Perception
    scores on the benchmark.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在四个多模态基准测试上，针对不同数据集微调的模型性能。MME得分是基准测试中Cognition和Perception得分的总和。
- en: 'Table 3: Model performance on different data types. Methods with 80K and Conv.20K
    suffix are tuned on the full 80K data and the 20K conversational data, respectively.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：不同数据类型下模型的性能。带有80K和Conv.20K后缀的方法分别在完整的80K数据和20K对话数据上进行调优。
- en: '| Method | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
- en: '| MM-Vicuna-7B |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| MM-Vicuna-7B |'
- en: '| Finetune-80K | 625.2/270.7 | 15.40 | 67.50 | 34.61 | 73.8/76.5/66.5 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Finetune-80K | 625.2/270.7 | 15.40 | 67.50 | 34.61 | 73.8/76.5/66.5 |'
- en: '| LayerNorm-80K | 723.2/253.2 | 17.06 | 80.89 | 48.01 | 76.1/81.1/70.8 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-80K | 723.2/253.2 | 17.06 | 80.89 | 48.01 | 76.1/81.1/70.8 |'
- en: '| LayerNorm-Conv. 20K | 777.1/231.4 | 15.39 | 67.30 | 40.33 | 75.2/79.2/68.8
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-Conv. 20K | 777.1/231.4 | 15.39 | 67.30 | 40.33 | 75.2/79.2/68.8
    |'
- en: '| MM-LLaMA2-7B |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-7B |'
- en: '| Finetune-80K | 661.3/237.1 | 16.09 | 65.08 | 31.64 | 56.3/65.0/55.4 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Finetune-80K | 661.3/237.1 | 16.09 | 65.08 | 31.64 | 56.3/65.0/55.4 |'
- en: '| LayerNorm-80K | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-80K | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9 |'
- en: '| LayerNorm-Conv. 20K | 376.2/157.5 | 16.19 | 86.80 | 44.88 | 50.5/50.7/50.3
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-Conv. 20K | 376.2/157.5 | 16.19 | 86.80 | 44.88 | 50.5/50.7/50.3
    |'
- en: '| MM-LLaMA2-chat-7B |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-chat-7B |'
- en: '| Finetune-80K | 805.4/234.6 | 15.29 | 57.40 | 26.70 | 60.3/69.8/57.9 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Finetune-80K | 805.4/234.6 | 15.29 | 57.40 | 26.70 | 60.3/69.8/57.9 |'
- en: '| LayerNorm-80K | 651.3/219.3 | 16.60 | 75.34 | 43.75 | 71.3/72.4/67.8 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-80K | 651.3/219.3 | 16.60 | 75.34 | 43.75 | 71.3/72.4/67.8 |'
- en: '| LayerNorm-Conv. 20K | 482.9/172.1 | 13.88 | 66.85 | 41.95 | 62.7/71.7/61.3
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-Conv. 20K | 482.9/172.1 | 13.88 | 66.85 | 41.95 | 62.7/71.7/61.3
    |'
- en: '| MM-LLaMA2-13B |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-13B |'
- en: '| Finetune-80K | 402.3/199.3 | 18.33 | 73.88 | 45.33 | 51.6/51.1/52.2 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Finetune-80K | 402.3/199.3 | 18.33 | 73.88 | 45.33 | 51.6/51.1/52.2 |'
- en: '| LayerNorm-80K | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-80K | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9 |'
- en: '| LayerNorm-Conv. 20K | 646.0/242.9 | 16.01 | 76.50 | 44.86 | 70.0/76.9/68.6
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-Conv. 20K | 646.0/242.9 | 16.01 | 76.50 | 44.86 | 70.0/76.9/68.6
    |'
- en: '| MM-LLaMA2-chat-13B |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-chat-13B |'
- en: '| Finetune-80K | 623.3/221.4 | 15.17 | 64.19 | 41.82 | 67.6/64.8/64.5 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Finetune-80K | 623.3/221.4 | 15.17 | 64.19 | 41.82 | 67.6/64.8/64.5 |'
- en: '| LayerNorm-80K | 929.3/254.3 | 16.10 | 74.96 | 42.79 | 78.9/83.9/74.3 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-80K | 929.3/254.3 | 16.10 | 74.96 | 42.79 | 78.9/83.9/74.3 |'
- en: '| LayerNorm-Conv. 20K | 769.7/227.5 | 15.57 | 73.30 | 43.08 | 68.2/72.8/65.3
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-Conv. 20K | 769.7/227.5 | 15.57 | 73.30 | 43.08 | 68.2/72.8/65.3
    |'
- en: 4.2 ‘Less is More’ on Both Data and Parameter Sides
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据和参数方面的‘少即是多’
- en: 'Efficiency in training can also be improved by considering the data used in
    LLMs and MLLMs (Zhou et al., [2023](#bib.bib44); Wei et al., [2023](#bib.bib35)).
    To this end, we conducted experiments using LLaMA2-7B and LLaMA2-7B-chat, where
    we divided the training data into three categories, each comprising 20K data points:
    image-grounded conversation, image detail descriptions, and image-based complex
    reasoning, as previously deployed in Liu et al. ([2023](#bib.bib21)). Based on
    the results presented in [fig. 2](#S4.F2 "In Memory Consumption and Parameter
    Efficiency. ‣ 4.1 Tuning LayerNorm ‣ 4 Experimental Results ‣ Tuning LayerNorm
    in Attention: Towards Efficient Multi-Modal LLM Finetuning"), we observe that
    the image-grounded conversation data is the most effective in enhancing the multi-modal
    capabilities of the model, with an average improvement of over 50% compared to
    other data types. This highlights the potential benefits of a targeted approach
    that leverages the strengths of specific data types to facilitate more nuanced
    and effective multi-modal tuning for MLLMs.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 训练效率也可以通过考虑 LLMs 和 MLLMs 中使用的数据来提高 [Zhou et al. (2023)](#bib.bib44); [Wei et
    al. (2023)](#bib.bib35)。为此，我们进行了使用 LLaMA2-7B 和 LLaMA2-7B-chat 的实验，在这些实验中，我们将训练数据分为三个类别，每个类别包含
    20K 数据点：基于图像的对话、图像细节描述和基于图像的复杂推理，正如 [Liu et al. (2023)](#bib.bib21) 所描述的。根据 [图
    2](#S4.F2 "内存消耗与参数效率。 ‣ 4.1 调优 LayerNorm ‣ 4 实验结果 ‣ 在注意力中调优 LayerNorm：走向高效的多模态
    LLM 微调") 中呈现的结果，我们观察到基于图像的对话数据在提升模型的多模态能力方面最为有效，与其他数据类型相比，平均提高超过 50%。这突显了针对性方法的潜在好处，这种方法利用特定数据类型的优势来促进
    MLLMs 更加细致和有效的多模态调优。
- en: 'To validate ‘Less is More’ on both the data and parameter sides, we present
    results of MLLMs with LayerNorm activated in LLM and tuned on 20k conversational
    data in [table 3](#S4.T3 "In Memory Consumption and Parameter Efficiency. ‣ 4.1
    Tuning LayerNorm ‣ 4 Experimental Results ‣ Tuning LayerNorm in Attention: Towards
    Efficient Multi-Modal LLM Finetuning"). Our experimental results indicate that
    even with a smaller dataset and the use of LayerNorm tuning, the model outperforms
    the full parameter finetuning approach on the full 80K dataset by 18.4% on two
    captioning tasks, and only falls short in MME by a tolerable 2.5%. It is noteworthy
    that LayerNorm with 20K data is only 7.6% and 7.4% behind LayerNorm on the full
    80K dataset for two captioning tasks and MME task, respectively. These findings
    demonstrate that ‘Less is More’ for both the parameter and data perspectives beyond
    language domain Zhou et al. ([2023](#bib.bib44)), but for multi-modal tuning.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证“少即是多”在数据和参数方面的有效性，我们展示了激活 LayerNorm 的 MLLMs 在 LLM 中的结果，并在 20k 对话数据上进行了调优，见
    [表 3](#S4.T3 "内存消耗与参数效率。 ‣ 4.1 调优 LayerNorm ‣ 4 实验结果 ‣ 在注意力中调优 LayerNorm：走向高效的多模态
    LLM 微调")。我们的实验结果表明，即使在较小的数据集和使用 LayerNorm 调优的情况下，该模型在两个标注任务上比全参数微调方法在 80K 数据集上表现优越
    18.4%，且在 MME 上的差距也只有 2.5%，这是可以接受的。值得注意的是，使用 20K 数据的 LayerNorm 在两个标注任务和 MME 任务上的表现分别比在
    80K 数据集上的 LayerNorm 少 7.6% 和 7.4%。这些发现表明，“少即是多”在数据和参数方面超越了语言领域 [Zhou et al. (2023)](#bib.bib44)，但对于多模态调优。
- en: 5 Intuitions Behind LayerNorm Tuning
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 LayerNorm 调优的直觉
- en: In this section, driven by the empirical success of LayerNorm tuning, we explore
    the intuitions behind LayerNorm from three perspectives, domain adaptation, expressive
    power, and gradient variance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们受 LayerNorm 调优的经验成功驱动，从领域适应性、表达能力和梯度方差三个角度探讨 LayerNorm 的直觉。
- en: 5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 LayerNorm 调优使 LLMs 适应多模态
- en: Influence of the Vision-Language Connector
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 视觉-语言连接器的影响
- en: 'The vision-language connector serves as the converter to project features from
    the vision encoder to the LLM domain. In our previous experiments, we focused
    on finetuning the LLM component of the MLLMs while keeping the vision-language
    connector activated by default. To determine which component plays a more important
    role for domain adaptation of LLM to multi-modal domain, we performed an ablation
    study by activating the two components separately. Results are presented in [table 4](#S5.T4
    "In Influence of the Vision-Language Connector ‣ 5.1 LayerNorm Tuning Adapts LLMs
    to Multi-Modal ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), tuning LayerNorm in attention
    blocks without activating the vision-language connector resulted in only a 4.2%
    and 5.4% decrease in performance on three traditional multi-modal tasks and the
    MME benchmark, respectively. This decrease is significantly lower than the 15.6%
    and 9.2% downgrade observed when only activating the Connector on the same tasks.
    This observation highlights the vital role LayerNorm plays in transforming knowledge
    from the vision domain to language, indicating LayerNorm as a strong domain adaptor
    for the LLM architecture.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉-语言连接器作为将特征从视觉编码器投影到 LLM 领域的转换器。在我们之前的实验中，我们专注于微调 MLLM 的 LLM 组件，同时默认启用视觉-语言连接器。为了确定哪个组件在
    LLM 的多模态领域适应中发挥了更重要的作用，我们通过分别激活这两个组件进行了消融研究。结果见 [表 4](#S5.T4 "在视觉-语言连接器的影响 ‣ 5.1
    LayerNorm 调整使 LLM 适应多模态 ‣ 5 LayerNorm 调整的直觉 ‣ 在注意力中调整 LayerNorm：实现高效的多模态 LLM 微调")，在未激活视觉-语言连接器的情况下调整注意力块中的
    LayerNorm，仅在三个传统的多模态任务和 MME 基准上分别减少了 4.2% 和 5.4% 的性能。这一下降显著低于在相同任务中仅激活连接器时观察到的
    15.6% 和 9.2% 的降级。这一观察突出了 LayerNorm 在将知识从视觉领域转化到语言中的重要作用，表明 LayerNorm 是 LLM 架构的强大领域适配器。
- en: 'Table 4: Results of models with LayerNorm and/or vision-language Connector
    activated.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：激活了 LayerNorm 和/或视觉-语言连接器的模型结果。
- en: '| Method | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
- en: '| MM-LLaMA2-7B |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-7B |'
- en: '| LayerNorm + Connector | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm + 连接器 | 583.2/200.7 | 16.78 | 88.85 | 49.24 | 66.6/68.5/64.9 |'
- en: '| Connector | 311.1/105.4 | 12.72 | 60.43 | 35.91 | 67.9/73.7/66.9 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 连接器 | 311.1/105.4 | 12.72 | 60.43 | 35.91 | 67.9/73.7/66.9 |'
- en: '| LayerNorm | 395.0/191.4 | 18.18 | 80.13 | 41.68 | 50.3/51.3/50.2 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm | 395.0/191.4 | 18.18 | 80.13 | 41.68 | 50.3/51.3/50.2 |'
- en: '| MM-LLaMA2-13B |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-13B |'
- en: '| LayerNorm + Connector | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm + 连接器 | 526.0/177.5 | 15.31 | 82.92 | 48.42 | 60.0/69.1/58.9 |'
- en: '| Connector | 507.0/187.9 | 15.22 | 62.60 | 25.13 | 60.9/66.8/60.1 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 连接器 | 507.0/187.9 | 15.22 | 62.60 | 25.13 | 60.9/66.8/60.1 |'
- en: '| LayerNorm | 405.0/188.6 | 16.51 | 70.41 | 39.86 | 50.9/52.7/51.0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm | 405.0/188.6 | 16.51 | 70.41 | 39.86 | 50.9/52.7/51.0 |'
- en: '![Refer to caption](img/e055dff09de54808c5a1860de0cf7ace.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e055dff09de54808c5a1860de0cf7ace.png)'
- en: 'Figure 3: Layer similarities between different LLM layers in (a) Finetuned
    and (b) LayerNorm-tuned MM-Vicuna-7B. The average layer similarity of two models
    are 0.624 and 0.585, respectively.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在（a）微调后的和（b）LayerNorm 调整后的 MM-Vicuna-7B 中，不同 LLM 层之间的层相似度。两个模型的平均层相似度分别为
    0.624 和 0.585。
- en: Switching Visual Features.
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 切换视觉特征。
- en: 'We employ the ViT encoder from CLIP (Radford et al., [2021](#bib.bib27)) by
    default in our previous experiments. CLIP (Radford et al., [2021](#bib.bib27))
    models are trained with image-text contrastive loss, thus its feature space is
    already aligned with language. Since LayerNorm has shown its effectiveness as
    a domain adaptor, we are interested in testing whether or not LayerNorm tuning
    can adapt a LLM to image features that are not pretrained to align with language.
    The vision encoder is switched to a ViT model that was pretrained on ImageNet (Dosovitskiy
    et al., [2021](#bib.bib8); Deng et al., [2009](#bib.bib6)). Results in [table 5](#S5.T5
    "In Switching Visual Features. ‣ 5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal
    ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention: Towards
    Efficient Multi-Modal LLM Finetuning") demonstrate that both LayerNorm and finetuning
    approaches can yield high performance. Interestingly, we observe that by LayerNorm
    tuning with ImageNet trained ViT, which has not been aligned with language, the
    model is able to achieve comparable performance to full parameter finetuning ,
    *i.e*., results show that LayerNorm tuning outperforms finetuning by 12.0% on
    captioning tasks, but performs slightly worse by 5.0% on the MME benchmark. These
    results again indicates the domain adaptor role of the LayerNorm , hinting the
    reason behind the empircal success of LayerNorm tuning. Furthermore, it is worth
    noting that the performance of MLLMs incorporating ViT pretrained on ImageNet
    is generally inferior to that of CLIP’s vision encoder. This observation provides
    compelling evidence that, despite differences in tokenizer and training paradigm
    between CLIP’s text encoder and LLaMA’s, ViT from CLIP has the capacity to learn
    general patterns of language formulation during pre-training. Thus, significantly
    enhance MLLM abilities.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的实验中默认使用了 CLIP 的 ViT 编码器（Radford et al., [2021](#bib.bib27)）。CLIP（Radford
    et al., [2021](#bib.bib27)）模型是通过图像-文本对比损失进行训练的，因此其特征空间已经与语言对齐。由于 LayerNorm 已显示其作为领域适配器的有效性，我们对测试
    LayerNorm 调优是否能够使 LLM 适应那些未经过预训练以对齐语言的图像特征感兴趣。视觉编码器被切换为一个在 ImageNet（Dosovitskiy
    et al., [2021](#bib.bib8); Deng et al., [2009](#bib.bib6)）上进行过预训练的 ViT 模型。结果在
    [表 5](#S5.T5 "在切换视觉特征中。‣ 5.1 LayerNorm 调优使 LLM 适应多模态 ‣ 5.0 LayerNorm 调优背后的直觉 ‣
    在注意力中调优 LayerNorm：迈向高效的多模态 LLM 微调") 中展示了 LayerNorm 和微调方法均能获得高性能。有趣的是，我们观察到，通过使用未与语言对齐的
    ImageNet 训练的 ViT 进行 LayerNorm 调优，模型能够实现与完全参数微调相当的性能，即结果显示 LayerNorm 调优在图像描述任务上比微调表现提高了
    12.0%，但在 MME 基准测试上略逊 5.0%。这些结果再次表明 LayerNorm 的领域适配器作用，暗示了 LayerNorm 调优的经验成功背后的原因。此外，值得注意的是，融入在
    ImageNet 上进行预训练的 ViT 的 MLLM 性能通常低于 CLIP 的视觉编码器。这一观察提供了有力的证据，尽管 CLIP 的文本编码器和 LLaMA
    的训练范式存在差异，但 CLIP 的 ViT 具有在预训练期间学习语言表达的通用模式的能力。因此，显著提升了 MLLM 的能力。
- en: 'Table 5: Results of models with LLaMA2 Finetuned/LayerNorm-tuned with ViT pre-trained
    on ImageNet (Deng et al., [2009](#bib.bib6)), which have not been aligned with
    the language domain.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在 ImageNet（Deng et al., [2009](#bib.bib6)）上预训练的 ViT 上微调/LayerNorm 调优的 LLaMA2
    模型的结果，这些模型尚未与语言领域对齐。
- en: '|  | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | MME | VQAv2 | MSCOCO | Flickr30k | POPE |'
- en: '| Finetune-7B | 406.79/182.5 | 15.05 | 47.75 | 18.97 | 50.0/51.6/50.1 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 微调-7B | 406.79/182.5 | 15.05 | 47.75 | 18.97 | 50.0/51.6/50.1 |'
- en: '| LayerNorm-7B | 301.51/127.14 | 15.48 | 66.22 | 31.73 | 50.0/50.1/50.1 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-7B | 301.51/127.14 | 15.48 | 66.22 | 31.73 | 50.0/50.1/50.1 |'
- en: '| Finetune-13B | 375.41/171.79 | 25.38 | 51.26 | 25.96 | 50.3/51.1/51.0 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 微调-13B | 375.41/171.79 | 25.38 | 51.26 | 25.96 | 50.3/51.1/51.0 |'
- en: '| LayerNorm-13B | 445.98/150.0 | 15.59 | 64.63 | 32.17 | 51.2/53.0/50.8 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| LayerNorm-13B | 445.98/150.0 | 15.59 | 64.63 | 32.17 | 51.2/53.0/50.8 |'
- en: 5.2 LayerNorm Tuning Improves the Expressive Power
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 LayerNorm 调优提升了表达能力
- en: 'It is shown in Pires et al. ([2023](#bib.bib26)) that a Transformer model incorporating
    anisotropic layer representation can capture a wider range of learning patterns.
    By computing the cosine similarities between all layers in the LLM of a finetuned
    MLLM, we aim to investigate whether the improved efficiency is the results of
    the improved expressive power. In [table 6](#S5.T6 "In 5.2 LayerNorm Tuning Improves
    the Expressive Power ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm
    in Attention: Towards Efficient Multi-Modal LLM Finetuning"), we present the average
    layer similarity of three 7B scale MLLMs, and in [fig. 3](#S5.F3 "In Influence
    of the Vision-Language Connector ‣ 5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal
    ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention: Towards
    Efficient Multi-Modal LLM Finetuning") we present the visualization of per layer
    similarity scores of MM-Vicuna-7B. Our analysis reveals that the transformer layers
    in the MLLMs with LayerNorm tuning exhibit a clear distinction from one another
    (*i.e*., an average 10.6% lower layer similarities comparing finetuning), indicating
    superior generalization ability and expressive power compared to finetuning. This
    finding sheds light on why tuning LayerNorm is effective for multi-modal LLM training.
    For additional visualizations, please refer to the Appendix [A.2.1](#A1.SS2.SSS1
    "A.2.1 Visualization Examples of Layer Similarities ‣ A.2 Insights of LayerNorm
    Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient
    Multi-Modal LLM Finetuning").'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Pires 等人（[2023](#bib.bib26)）展示了一个结合了各向异性层表示的 Transformer 模型能够捕捉更广泛的学习模式。通过计算微调
    MLLM 的 LLM 中所有层的余弦相似性，我们旨在研究是否改进的效率是由于表达能力的提升。在 [表 6](#S5.T6 "在 5.2 LayerNorm
    调整提升表达能力 ‣ 5 LayerNorm 调整的直觉 ‣ 在注意力中调整 LayerNorm：迈向高效的多模态 LLM 微调") 中，我们展示了三种 7B
    规模 MLLMs 的平均层相似性，在 [图 3](#S5.F3 "在视觉-语言连接器的影响 ‣ 5.1 LayerNorm 调整使 LLM 适应多模态 ‣
    5 LayerNorm 调整的直觉 ‣ 在注意力中调整 LayerNorm：迈向高效的多模态 LLM 微调") 中，我们展示了 MM-Vicuna-7B 的每层相似性得分的可视化。我们的分析揭示，采用
    LayerNorm 调整的 MLLMs 中的 Transformer 层彼此之间有明显的区别（*即*，相比微调，平均层相似性低 10.6%），表明相比微调具有更强的泛化能力和表达能力。这一发现揭示了为什么调整
    LayerNorm 对于多模态 LLM 训练是有效的。有关更多可视化，请参见附录 [A.2.1](#A1.SS2.SSS1 "A.2.1 层相似性的可视化示例
    ‣ A.2 LayerNorm 调整的见解 ‣ 附录 A 附录 ‣ 在注意力中调整 LayerNorm：迈向高效的多模态 LLM 微调")。
- en: 'Table 6: Layer representation similarity of LayerNorm and finetuning methods
    on three 7B MLLMs. Lower the similarity is, the better expressive power a model
    possesses.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：三种 7B MLLMs 中 LayerNorm 和微调方法的层表示相似性。相似性越低，模型的表达能力越强。
- en: '| Model | LayerNorm Sim. | Finetuning Sim. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LayerNorm 相似性 | 微调相似性 |'
- en: '| MM-Vicuna | 0.585 | 0.624 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| MM-Vicuna | 0.585 | 0.624 |'
- en: '| MM-LLaMA2 | 0.504 | 0.591 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2 | 0.504 | 0.591 |'
- en: '| MM-LLaMA2-chat | 0.550 | 0.617 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2-chat | 0.550 | 0.617 |'
- en: 5.3 LayerNorm Tuning has Smaller Gradient Variance
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 LayerNorm 调整具有更小的梯度方差
- en: 'A well accepted view about LayerNorm is that, as the neural network goes deeper,
    the mean of LayerNorm gradients should goes to zero as the LayerNorm itself is
    designed to normalize all training parameters. In the meantime, the variance of
    LayerNorm gradients should be small to ensure a better generalization ability
    of the model (Xu et al., [2019](#bib.bib37)) (See the proof in Appendix [A.2.2](#A1.SS2.SSS2
    "A.2.2 Gradients of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A
    Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")).
    As we presented in [fig. 4](#S5.F4 "In 5.3 LayerNorm Tuning has Smaller Gradient
    Variance ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), MLLM with LayerNorm tuning method
    has a more concentrated LayerNorm gradients than fine-tuning during the training
    process. This result gives another view on the effectiveness of LayerNorm from
    the optimization perspective. More visualizations are listed in Appendix [A.2.2](#A1.SS2.SSS2
    "A.2.2 Gradients of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A
    Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '对LayerNorm的一个广泛接受的观点是，随着神经网络的加深，LayerNorm梯度的均值应该趋近于零，因为LayerNorm本身旨在规范化所有训练参数。同时，LayerNorm梯度的方差应较小，以确保模型的更好泛化能力
    (Xu et al., [2019](#bib.bib37)) (参见附录[A.2.2](#A1.SS2.SSS2 "A.2.2 Gradients of
    LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm
    in Attention: Towards Efficient Multi-Modal LLM Finetuning")中的证明)。如我们在[图4](#S5.F4
    "In 5.3 LayerNorm Tuning has Smaller Gradient Variance ‣ 5 Intuitions Behind LayerNorm
    Tuning ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")中所示，具有LayerNorm调整方法的MLLM在训练过程中拥有比微调更集中化的LayerNorm梯度。这一结果从优化角度提供了对LayerNorm有效性的另一种观点。更多可视化结果列在附录[A.2.2](#A1.SS2.SSS2
    "A.2.2 Gradients of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A
    Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")中。'
- en: '![Refer to caption](img/f1e85df7d096077efd3c21ffe99bf1a4.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f1e85df7d096077efd3c21ffe99bf1a4.png)'
- en: 'Figure 4: Gradients of the input LayerNorm in the 11th layer of the MM-Vicuna
    as training proceeds. LayerNorm-tuned model has lower gradient variance than full
    parameter finetuning.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：MM-Vicuna第11层输入LayerNorm的梯度随训练过程的变化。与全参数微调相比，LayerNorm调整的模型具有更低的梯度方差。
- en: 6 Conclusion and Discussions
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与讨论
- en: LayerNorm is effective and sufficient built upon MLLM pre-training.
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于MLLM预训练，LayerNorm是有效且足够的。
- en: 'MLLM training typically involves pre-training on image-text pairs followed
    by finetuning on visual instruction data. While the second stage of training receives
    more attention, it is worth noting that the function of the first stage pre-training
    is non-negligible for training a competent MLLM. We have presented in the paper
    only a small portion of parameter activation is sufficient to tune a well-behaved
    MLLM. However, other models such as InstructBLIP (Dai et al., [2023](#bib.bib5))
    and MiniGPT4 (Zhu et al., [2023](#bib.bib45)) only tune the vision-language connector,
    leaving the LLM untouched during the second stage of training. These models have
    yielded strong performances when given a large-scale finetuning dataset. In Sec. [5.1](#S5.SS1
    "5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal ‣ 5 Intuitions Behind LayerNorm
    Tuning ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"),
    we demonstrate that tuning LayerNorm may be a more effective means for the second
    stage training, especially when compared to existing parameter-efficient methods
    for training MLLMs.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'MLLM训练通常包括在图像-文本对上进行预训练，随后在视觉指令数据上进行微调。尽管第二阶段的训练受到更多关注，但值得注意的是，第一阶段的预训练在训练一个高效的MLLM时作用不可忽视。我们在论文中展示了仅需少量参数激活即可调优一个表现良好的MLLM。然而，其他模型如InstructBLIP
    (Dai et al., [2023](#bib.bib5)) 和 MiniGPT4 (Zhu et al., [2023](#bib.bib45)) 仅调整视觉-语言连接器，而在第二阶段训练中保持LLM不变。这些模型在提供大规模微调数据集时表现出色。在第[5.1](#S5.SS1
    "5.1 LayerNorm Tuning Adapts LLMs to Multi-Modal ‣ 5 Intuitions Behind LayerNorm
    Tuning ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")节中，我们展示了调整LayerNorm可能是一种更有效的第二阶段训练手段，尤其是与现有的参数高效方法相比。'
- en: Limitations.
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 限制。
- en: 'One shortcoming of these parameter-efficient finetuning methods is that they
    are more sensitive to hyper-parameters (*e.g*., learning rate, training epoch)
    than finetuning. Since the number of trainable parameters of LayerNorm is small,
    the model performance of LayerNorm method also varies when twitching the training
    hyper-parameters. This drawback calls for potential future investigations on the
    LayerNorm tuning method. In the Appendix [A.1](#A1.SS1 "A.1 Training Details ‣
    Appendix A Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal
    LLM Finetuning"), we give a hint for the grid search range of learning rate on
    both 7B and 13B scaled models using LayerNorm tuning based on our experimental
    results.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '这些参数高效微调方法的一个缺点是它们对超参数（*例如*，学习率，训练轮次）比微调更敏感。由于 LayerNorm 的可训练参数数量较少，当调整训练超参数时，LayerNorm
    方法的模型性能也会有所变化。这一缺点需要对 LayerNorm 调优方法进行潜在的未来研究。在附录[A.1](#A1.SS1 "A.1 Training Details
    ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal
    LLM Finetuning")中，我们根据实验结果为 LayerNorm 调优下 7B 和 13B 规模模型的学习率网格搜索范围提供了一个提示。'
- en: Conclusion.
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结论。
- en: Our studies demonstrate LayerNorm tuning as a simple yet effective tuning method
    for adapting LLMs comprehend multi-modal content across various model variants.
    Compared to LoRA tuning or full parameter finetuning, LayerNorm tuning reduces
    the trainable parameters by a significant 41.9%, enabling efficient finetuning
    of MLLMs on consumer-grade GPUs. Moreover, we demonstrate that MLLMs can achieve
    exceptional performance with minimal “right” data and parameters, showcasing the
    potential of LayerNorm tuning method in real-world applications. Given the empirical
    success of LayerNorm tuning, we revisited the MLLM finetuning from a domain adaptation
    perspective and showed that LayerNorm plays a critical role in adapting LLMs to
    the multi-modal domain. Additionally, our research illustrates the expressive
    power and optimization potential of LayerNorm tuning from layer similarities and
    the gradient variance. We hope that our work could inspire future works on designing
    improved PEFT methods that enable more diverse application scenarios for MLLMs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究表明 LayerNorm 调优是一种简单而有效的调优方法，能够帮助 LLM 理解多模态内容并适用于各种模型变体。与 LoRA 调优或全参数微调相比，LayerNorm
    调优将可训练参数减少了 41.9%，使得在消费级 GPU 上有效微调 MLLM 成为可能。此外，我们展示了 MLLM 在最少的“正确”数据和参数下可以实现卓越的性能，展示了
    LayerNorm 调优方法在实际应用中的潜力。鉴于 LayerNorm 调优的实证成功，我们从领域适应的角度重新审视了 MLLM 微调，并表明 LayerNorm
    在将 LLM 适应到多模态领域中发挥了关键作用。此外，我们的研究从层相似性和梯度方差的角度说明了 LayerNorm 调优的表达能力和优化潜力。我们希望我们的工作能激发未来在设计改进的
    PEFT 方法方面的研究，以实现 MLLM 更多样化的应用场景。
- en: References
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question
    answering. In *ICCV*, 2015.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C Lawrence Zitnick, 和 Devi Parikh. Vqa: 视觉问答。发表于*ICCV*，2015年。'
- en: Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer
    Normalization. *arXiv:1607.06450*, 2016.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, 和 Geoffrey E. Hinton. 层归一化。*arXiv:1607.06450*，2016年。
- en: 'Changpinyo et al. (2021) Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
    Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize
    long-tail visual concepts. In *CVPR*, 2021.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Changpinyo et al. (2021) Soravit Changpinyo, Piyush Sharma, Nan Ding, 和 Radu
    Soricut. Conceptual 12m: 推动网页规模的图像-文本预训练以识别长尾视觉概念。发表于*CVPR*，2021年。'
- en: Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*,
    2016.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, 和 Carlos Guestrin. 用次线性内存成本训练深度网络。*arXiv
    预印本 arXiv:1604.06174*，2016年。
- en: 'Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning. *NeurIPS*,
    2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, 和 Steven Hoi. Instructblip:
    朝向通用视觉语言模型的指令调优。发表于*NeurIPS*，2023年。'
- en: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In *CVPR*, 2009.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
    Li Fei-Fei. ImageNet: 一个大规模层次化的图像数据库。发表于*CVPR*，2009年。'
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023) Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。QLORA：量化大语言模型的高效微调。*arXiv
    preprint arXiv:2305.14314*，2023年。
- en: 'Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
    Transformers for image recognition at scale. *ICLR*, 2021.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy et al. (2021) Alexey Dosovitskiy、Lucas Beyer、Alexander Kolesnikov、Dirk
    Weissenborn、Xiaohua Zhai、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg
    Heigold、Sylvain Gelly 等。一张图片值16x16个词：大规模图像识别的变换器。*ICLR*，2021年。
- en: 'Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan
    Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive
    evaluation benchmark for multimodal large language models. *arXiv preprint arXiv:2306.13394*,
    2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2023) Chaoyou Fu、Peixian Chen、Yunhang Shen、Yulei Qin、Mengdan Zhang、Xu
    Lin、Zhenyu Qiu、Wei Lin、Jinrui Yang、Xiawu Zheng 等。Mme：多模态大语言模型的综合评估基准。*arXiv preprint
    arXiv:2306.13394*，2023年。
- en: He et al. (2022) Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick,
    and Graham Neubig. Towards a unified view of parameter-efficient transfer learning.
    *ICLR*, 2022.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2022) Junxian He、Chunting Zhou、Xuezhe Ma、Taylor Berg-Kirkpatrick
    和 Graham Neubig。迈向参数高效迁移学习的统一视角。*ICLR*，2022年。
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *ICML*, 2019.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby et al. (2019) Neil Houlsby、Andrei Giurgiu、Stanislaw Jastrzebski、Bruna
    Morrone、Quentin De Laroussilhe、Andrea Gesmundo、Mona Attariyan 和 Sylvain Gelly。用于自然语言处理的参数高效迁移学习。发表于
    *ICML*，2019年。
- en: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *ICLR*, 2022.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2022) Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi
    Li、Shean Wang、Lu Wang 和 Weizhu Chen。LORA：大型语言模型的低秩适应。*ICLR*，2022年。
- en: 'Ioffe & Szegedy (2015) Sergey Ioffe and Christian Szegedy. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. In *ICML*,
    2015.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe & Szegedy (2015) Sergey Ioffe 和 Christian Szegedy。批量归一化：通过减少内部协变量偏移加速深度网络训练。发表于
    *ICML*，2015年。
- en: Karpathy & Fei-Fei (2015) Andrej Karpathy and Li Fei-Fei. Deep visual-semantic
    alignments for generating image descriptions. In *CVPR*, 2015.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy & Fei-Fei (2015) Andrej Karpathy 和 Li Fei-Fei。生成图像描述的深度视觉-语义对齐。发表于
    *CVPR*，2015年。
- en: 'Li et al. (2023a) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models. *ICML*, 2023a.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Junnan Li、Dongxu Li、Silvio Savarese 和 Steven Hoi。BLIP-2：利用冻结的图像编码器和大型语言模型进行语言-图像预训练。*ICML*，2023年。
- en: 'Li & Liang (2021a) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In *ACL*, 2021a.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Liang (2021a) Xiang Lisa Li 和 Percy Liang。前缀调整：优化生成的连续提示。发表于 *ACL*，2021年。
- en: 'Li & Liang (2021b) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *ACL*, 2021b.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li & Liang (2021b) Xiang Lisa Li 和 Percy Liang。前缀调整：优化生成的连续提示。*ACL*，2021年。
- en: Li et al. (2016) Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi
    Hou. Revisiting batch normalization for practical domain adaptation. *arXiv preprint
    arXiv:1603.04779*, 2016.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2016) Yanghao Li、Naiyan Wang、Jianping Shi、Jiaying Liu 和 Xiaodi Hou。重新审视批量归一化在实际领域适应中的应用。*arXiv
    preprint arXiv:1603.04779*，2016年。
- en: Li et al. (2023b) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao,
    and Ji-Rong Wen. Evaluating object hallucination in large vision-language models.
    *arXiv preprint arXiv:2305.10355*, 2023b.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Yifan Li、Yifan Du、Kun Zhou、Jinpeng Wang、Wayne Xin Zhao 和 Ji-Rong
    Wen。评估大型视觉-语言模型中的对象幻觉。*arXiv preprint arXiv:2305.10355*，2023年。
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *ECCV*, 2014.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2014) Tsung-Yi Lin、Michael Maire、Serge Belongie、James Hays、Pietro
    Perona、Deva Ramanan、Piotr Dollár 和 C Lawrence Zitnick。Microsoft COCO：上下文中的常见对象。发表于
    *ECCV*，2014年。
- en: Liu et al. (2023) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual
    instruction tuning. *NeurIPS*, 2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Haotian Liu、Chunyuan Li、Qingyang Wu 和 Yong Jae Lee。视觉指令调整。*NeurIPS*，2023年。
- en: 'Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic
    gradient descent with warm restarts. *ICLR*, 2017.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter (2017) Ilya Loshchilov 和 Frank Hutter。SGDR：带有热启动的随机梯度下降。*ICLR*，2017年。
- en: Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. *ICLR*, 2019.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov & Hutter (2019) Ilya Loshchilov 和 Frank Hutter。解耦权重衰减正则化。*ICLR*，2019。
- en: 'Mu et al. (2022) Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
    Slip: Self-supervision meets language-image pre-training. In *ECCV*, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等 (2022) Norman Mu、Alexander Kirillov、David Wagner 和 Saining Xie。Slip：自监督遇见语言-图像预训练。在
    *ECCV*，2022。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *NeurIPS*, 2022.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 (2022) Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray 等。训练语言模型以遵循人类反馈的指令。*NeurIPS*，2022。
- en: Pires et al. (2023) Telmo Pessoa Pires, António V Lopes, Yannick Assogba, and
    Hendra Setiawan. One wide feedforward is all you need. *arXiv preprint arXiv:2309.01826*,
    2023.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pires 等 (2023) Telmo Pessoa Pires、António V Lopes、Yannick Assogba 和 Hendra Setiawan。一个宽的前馈网络就够了。*arXiv
    预印本 arXiv:2309.01826*，2023。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *ICML*, 2021.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2021) Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel
    Goh、Sandhini Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark 等。从自然语言监督中学习可迁移的视觉模型。在
    *ICML*，2021。
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC*, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajbhandari 等 (2020) Samyam Rajbhandari、Jeff Rasley、Olatunji Ruwase 和 Yuxiong
    He。Zero：针对训练万亿参数模型的内存优化。在 *SC*，2020。
- en: 'Su et al. (2023) Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng
    Cai. Pandagpt: One model to instruction-follow them all. *arXiv preprint arXiv:2305.16355*,
    2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等 (2023) Yixuan Su、Tian Lan、Huayang Li、Jialu Xu、Yan Wang 和 Deng Cai。Pandagpt：一个模型统领所有指令。*arXiv
    预印本 arXiv:2305.16355*，2023。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023) Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。Llama
    2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023。
- en: 'Tu et al. (2022) Haoqin Tu, Zhongliang Yang, Jinshuai Yang, and Yongfeng Huang.
    Adavae: Exploring adaptive gpt-2s in variational auto-encoders for language modeling.
    *arXiv preprint arXiv:2205.05862*, 2022.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等 (2022) Haoqin Tu、Zhongliang Yang、Jinshuai Yang 和 Yongfeng Huang。Adavae：在变分自编码器中探索自适应
    GPT-2 进行语言建模。*arXiv 预印本 arXiv:2205.05862*，2022。
- en: 'Tu et al. (2023) Haoqin Tu, Bingchen Zhao, Chen Wei, and Cihang Xie. Sight
    beyond text: Multi-modal training enhances llms in truthfulness and ethics. *arXiv
    preprint arXiv:2309.07120*, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等 (2023) Haoqin Tu、Bingchen Zhao、Chen Wei 和 Cihang Xie。超越文本的视野：多模态训练提升 LLM
    的真实性和伦理。*arXiv 预印本 arXiv:2309.07120*，2023。
- en: 'Ulyanov et al. (2016) Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
    Instance normalization: The missing ingredient for fast stylization. *arXiv preprint
    arXiv:1607.08022*, 2016.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ulyanov 等 (2016) Dmitry Ulyanov、Andrea Vedaldi 和 Victor Lempitsky。实例归一化：快速风格化的缺失成分。*arXiv
    预印本 arXiv:1607.08022*，2016。
- en: 'Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    Cider: Consensus-based image description evaluation. In *CVPR*, 2015.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vedantam 等 (2015) Ramakrishna Vedantam、C Lawrence Zitnick 和 Devi Parikh。Cider：基于共识的图像描述评估。在
    *CVPR*，2015。
- en: 'Wei et al. (2023) Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4:
    A 200-instruction paradigm for fine-tuning minigpt-4. *arXiv preprint arXiv:2308.12067*,
    2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2023) Lai Wei、Zihao Jiang、Weiran Huang 和 Lichao Sun。Instructiongpt-4：一种用于微调
    miniGPT-4 的 200 指令范式。*arXiv 预印本 arXiv:2308.12067*，2023。
- en: Wu & He (2018) Yuxin Wu and Kaiming He. Group normalization. In *ECCV*, 2018.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu & He (2018) Yuxin Wu 和 Kaiming He。组归一化。在 *ECCV*，2018。
- en: Xu et al. (2019) Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang
    Lin. Understanding and improving layer normalization. *NeurIPS*, 2019.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2019) Jingjing Xu、Xu Sun、Zhiyuan Zhang、Guangxiang Zhao 和 Junyang Lin。理解和改进层归一化。*NeurIPS*，2019。
- en: 'Ye et al. (2023) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang
    Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization
    empowers large language models with multimodality. *arXiv preprint arXiv:2304.14178*,
    2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等（2023）Qinghao Ye、Haiyang Xu、Guohai Xu、Jiabo Ye、Ming Yan、Yiyang Zhou、Junyang
    Wang、Anwen Hu、Pengcheng Shi、Yaya Shi 等。mplug-owl：模块化赋能大语言模型的多模态能力。*arXiv 预印本 arXiv:2304.14178*，2023年。
- en: 'Young et al. (2014) Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
    From image descriptions to visual denotations: New similarity metrics for semantic
    inference over event descriptions. *TACL*, 2014.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young 等（2014）Peter Young、Alice Lai、Micah Hodosh 和 Julia Hockenmaier。从图像描述到视觉指称：事件描述语义推断的新相似性度量。*TACL*，2014年。
- en: 'Yu et al. (2022) Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba
    Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation
    models. *TMLR*, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2022）Jiahui Yu、Zirui Wang、Vijay Vasudevan、Legg Yeung、Mojtaba Seyedhosseini
    和 Yonghui Wu。Coca：对比性字幕生成模型是图像-文本基础模型。*TMLR*，2022年。
- en: 'Zhang et al. (2023) Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin
    Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning
    of language models with zero-init attention. *arXiv preprint arXiv:2303.16199*,
    2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Renrui Zhang、Jiaming Han、Aojun Zhou、Xiangfei Hu、Shilin Yan、Pan
    Lu、Hongsheng Li、Peng Gao 和 Yu Qiao。Llama-adapter：使用零初始化注意力的高效语言模型微调。*arXiv 预印本
    arXiv:2303.16199*，2023年。
- en: Zhao et al. (2023) Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang,
    and Oisin Mac Aodha. Vision learners meet web image-text pairs. *arXiv preprint
    arXiv:2301.07088*, 2023.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2023）Bingchen Zhao、Quan Cui、Hao Wu、Osamu Yoshie、Cheng Yang 和 Oisin Mac
    Aodha。视觉学习者遇见网络图像-文本对。*arXiv 预印本 arXiv:2301.07088*，2023年。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena, 2023.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2023）Lianmin Zheng、Wei-Lin Chiang、Ying Sheng、Siyuan Zhuang、Zhanghao
    Wu、Yonghao Zhuang、Zi Lin、Zhuohan Li、Dacheng Li、Eric. P Xing、Hao Zhang、Joseph E.
    Gonzalez 和 Ion Stoica。通过 mt-bench 和 chatbot arena 评估 llm-as-a-judge，2023年。
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more
    for alignment. *arXiv preprint arXiv:2305.11206*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023）Chunting Zhou、Pengfei Liu、Puxin Xu、Srini Iyer、Jiao Sun、Yuning Mao、Xuezhe
    Ma、Avia Efrat、Ping Yu、Lili Yu 等。Lima：少即是多用于对齐。*arXiv 预印本 arXiv:2305.11206*，2023年。
- en: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2023）Deyao Zhu、Jun Chen、Xiaoqian Shen、Xiang Li 和 Mohamed Elhoseiny。Minigpt-4：通过先进的大型语言模型提升视觉-语言理解。*arXiv
    预印本 arXiv:2304.10592*，2023年。
- en: Appendix A Appendix
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Training Details
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 训练细节
- en: For the first stage, we set the learning rate to 2e-3 for all variants. During
    the second stage, we search learning the learning rate from [2e-3, 1e-3, 6e-4,
    3e-4, 1e-4, 5e-5, 2e-5, 1e-5, 6e-6, 1e-6, 1e-7] for all models and pick the best
    learning rate based on their performances on the CIDEr score on the Flickr30k
    task.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一阶段，我们将所有变体的学习率设置为 2e-3。在第二阶段，我们从 [2e-3, 1e-3, 6e-4, 3e-4, 1e-4, 5e-5, 2e-5,
    1e-5, 6e-6, 1e-6, 1e-7] 中搜索学习率，并根据它们在 Flickr30k 任务上的 CIDEr 评分表现选择最佳学习率。
- en: 'According to our tryouts based on Flickr30k results in Table [A1](#A1.T1 "Table
    A1 ‣ A.1 Training Details ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), the recommended learning rate
    for 7B scale is between 6e-4 to 2e-3, while on the 13B, the learning rate should
    be searched in the range of 3e-6 to 6e-5.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们基于 Flickr30k 结果的尝试（见表 [A1](#A1.T1 "表 A1 ‣ A.1 训练细节 ‣ 附录 A 附录 ‣ 调整注意力中的 LayerNorm：迈向高效的多模态
    LLM 微调")），7B 规模的推荐学习率在 6e-4 到 2e-3 之间，而 13B 规模的学习率应在 3e-6 到 6e-5 的范围内搜索。
- en: 'Table A1: Performance of MLLMs (LayerNorm-simp.) trained with different learning
    rates and scales on the Flickr30k task.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A1：在 Flickr30k 任务上，使用不同学习率和规模训练的 MLLMs（LayerNorm-simp.）的性能。
- en: '| Learning Rate | 3e-6 | 1e-5 | 3e-5 | 6e-5 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 3e-6 | 1e-5 | 3e-5 | 6e-5 |'
- en: '| MM-LLaMA2 7B | 21.42 | 32.45 | 43.04 | 28.24 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2 7B | 21.42 | 32.45 | 43.04 | 28.24 |'
- en: '| Learning Rate | 6e-4 | 1e-3 | 2e-3 | - |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 6e-4 | 1e-3 | 2e-3 | - |'
- en: '| MM-LLaMA2 13B | 37.35 | 46.88 | 44.15 | - |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| MM-LLaMA2 13B | 37.35 | 46.88 | 44.15 | - |'
- en: A.2 Insights of LayerNorm Tuning
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 层归一化调优的洞察
- en: A.2.1 Visualization Examples of Layer Similarities
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 层相似性的可视化示例
- en: 'Lower similarities between different layers of the transformer indicates more
    expressive power (Pires et al., [2023](#bib.bib26)). In [section 5.2](#S5.SS2
    "5.2 LayerNorm Tuning Improves the Expressive Power ‣ 5 Intuitions Behind LayerNorm
    Tuning ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"),
    we have shown the computed cosine similarity between layers on a Vicuna model,
    here we show the layer similarities between layers on LLaMA2 and LLaMA2 chat models
    in [fig. A1](#A1.F1 "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients
    of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning
    LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning") and [fig. A2](#A1.F2
    "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients of LayerNorm ‣
    A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"). It is clear that, LayerNorm tuning
    again allows the model to learn dissimilar layer representations, improving the
    expressive power of the model.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer不同层之间的相似度较低表明模型具有更强的表达能力（Pires等，[2023](#bib.bib26)）。在[section 5.2](#S5.SS2
    "5.2 LayerNorm Tuning Improves the Expressive Power ‣ 5 Intuitions Behind LayerNorm
    Tuning ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")中，我们展示了在Vicuna模型中计算的层间余弦相似度，此处我们展示了在LLaMA2和LLaMA2
    chat模型中的层间相似度，如[fig. A1](#A1.F1 "In Proof of smaller variance in LayerNorm . ‣
    A.2.2 Gradients of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix
    ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")和[fig.
    A2](#A1.F2 "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients of LayerNorm
    ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm in
    Attention: Towards Efficient Multi-Modal LLM Finetuning")。显而易见，LayerNorm调优再次使模型能够学习到不同的层表示，从而提高了模型的表达能力。'
- en: A.2.2 Gradients of LayerNorm
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 LayerNorm的梯度
- en: Visualization examples of LayerNorm gradients.
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LayerNorm梯度的可视化示例。
- en: 'In [fig. A3](#A1.F3 "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients
    of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning
    LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning") and [fig. A4](#A1.F4
    "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients of LayerNorm ‣
    A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), we present the gradients of the
    LayerNorm parameters during the training process. Similar to the one we have shown
    in the main text, LayerNorm tuning demonstrates a smaller gradient variance which
    is important for converging to a better local minimum (Xu et al., [2019](#bib.bib37)).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '在[fig. A3](#A1.F3 "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients
    of LayerNorm ‣ A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning
    LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning")和[fig. A4](#A1.F4
    "In Proof of smaller variance in LayerNorm . ‣ A.2.2 Gradients of LayerNorm ‣
    A.2 Insights of LayerNorm Tuning ‣ Appendix A Appendix ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning")中，我们展示了训练过程中LayerNorm参数的梯度。与我们在正文中展示的相似，LayerNorm调优表现出较小的梯度方差，这对收敛到更好的局部最小值很重要（Xu等，[2019](#bib.bib37)）。'
- en: Proof of smaller variance in LayerNorm .
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LayerNorm中较小方差的证明。
- en: 'As stated in Sec. [5.3](#S5.SS3 "5.3 LayerNorm Tuning has Smaller Gradient
    Variance ‣ 5 Intuitions Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention:
    Towards Efficient Multi-Modal LLM Finetuning"), deeper the network is, the variance
    of LayerNorm in the model should be naturally smaller (Xu et al., [2019](#bib.bib37)).
    We first let $\mathbf{y}=(y_{1},y_{2},...,y_{N})$ is $0$, respectively. We can
    then formulate the standard LayerNorm as follow:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '如[5.3](#S5.SS3 "5.3 LayerNorm Tuning has Smaller Gradient Variance ‣ 5 Intuitions
    Behind LayerNorm Tuning ‣ Tuning LayerNorm in Attention: Towards Efficient Multi-Modal
    LLM Finetuning")节所述，网络越深，模型中LayerNorm的方差应该自然较小（Xu等，[2019](#bib.bib37)）。我们首先令$\mathbf{y}=(y_{1},y_{2},...,y_{N})$分别为$0$。然后我们可以将标准LayerNorm表示如下：'
- en: '|  | $1$2 |  | (1) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $\mathbf{x}=(x_{1},x_{2},...,x_{N})$ is the dimension of $\mathbf{x}$
    and $\sigma$.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{x}=(x_{1},x_{2},...,x_{N})$是$\mathbf{x}$的维度和$\sigma$。
- en: 'We first define $\mathbf{1}_{N}=\underbrace{(1,1,...,1)^{\intercal}}_{N}$,
    we first simulate the backward propagation regarding the loss $\ell$:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义$\mathbf{1}_{N}=\underbrace{(1,1,...,1)^{\intercal}}_{N}$，然后我们模拟关于损失$\ell$的反向传播：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'Here we define $\frac{\partial\ell}{\partial\mathbf{x}}=(a_{1},a_{2},...,a_{N})$
    and standard deviation $D_{a}$ with mean $\bar{b}$. We set $W_{1}=I-\frac{\mathbf{y}\mathbf{y}^{\intercal}}{N}-\frac{\mathbf{1}_{N}\mathbf{1}^{\intercal}_{N}}{N}$,
    we can verify that:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义$\frac{\partial\ell}{\partial\mathbf{x}}=(a_{1},a_{2},...,a_{N})$和标准差$D_{a}$，均值为$\bar{b}$。我们设定$W_{1}=I-\frac{\mathbf{y}\mathbf{y}^{\intercal}}{N}-\frac{\mathbf{1}_{N}\mathbf{1}^{\intercal}_{N}}{N}$，我们可以验证：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: Therefore, we can easily proof that $N\bar{a}\propto\mathbf{1}_{N}^{\intercal}W_{1}\bar{b}=0$
    should be zero.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以轻松证明 $N\bar{a}\propto\mathbf{1}_{N}^{\intercal}W_{1}\bar{b}=0$ 应该为零。
- en: Then we dive into proofing the variance of LayerNorm gradients should be small
    when the number of network parameters $N$ becomes large.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们深入证明，当网络参数 $N$ 增大时，LayerNorm 梯度的方差应该很小。
- en: '|  | $\displaystyle D_{a}$ |  | (4) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle D_{a}$ |  | (4) |'
- en: '|  |  | $\displaystyle=\left\&#124;\left(a_{1},a_{2},\ldots,a_{N}\right)^{\intercal}\right\&#124;^{2}/N$
    |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\left\&#124;\left(a_{1},a_{2},\ldots,a_{N}\right)^{\intercal}\right\&#124;^{2}/N$
    |  |'
- en: '|  |  | $\displaystyle=\left\&#124;W_{1}\left(b_{1},b_{2},\ldots,b_{N}\right)^{\intercal}\right\&#124;^{2}/N$
    |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\left\&#124;W_{1}\left(b_{1},b_{2},\ldots,b_{N}\right)^{\intercal}\right\&#124;^{2}/N$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle\leq W_{1}^{2}\sum_{i=1}^{N}(b_{i}-\bar{b})^{2}/N$ |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq W_{1}^{2}\sum_{i=1}^{N}(b_{i}-\bar{b})^{2}/N$ |  |'
- en: Since the projection matrix $W_{1}$. That is to say, when $N$. As a consequence,
    when the network parameter $N$ is large, the gradient variance of LayerNorm should
    be small.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 由于投影矩阵 $W_{1}$。也就是说，当 $N$。因此，当网络参数 $N$ 很大时，LayerNorm 的梯度方差应该很小。
- en: '![Refer to caption](img/3163909fa9dd121e3f39e2b9ee31d4e1.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3163909fa9dd121e3f39e2b9ee31d4e1.png)'
- en: 'Figure A1: Layer similarities between different LLM layers in (a) Finetuned
    and (b) LayerNorm-tuned MM-LLaMA2-7B.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '图 A1: (a) 微调与 (b) LayerNorm 调优的 MM-LLaMA2-7B 中不同 LLM 层之间的层相似性。'
- en: '![Refer to caption](img/0307e4ccce72e8dbb3aa8b2caa848a70.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0307e4ccce72e8dbb3aa8b2caa848a70.png)'
- en: 'Figure A2: Layer similarities between different LLM layers in (a) Finetuned
    and (b) LayerNorm-tuned MM-LLaMA2-7B chat.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '图 A2: (a) 微调与 (b) LayerNorm 调优的 MM-LLaMA2-7B 聊天中不同 LLM 层之间的层相似性。'
- en: '![Refer to caption](img/ceb0018641e70a6cba398976e59572a3.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ceb0018641e70a6cba398976e59572a3.png)'
- en: 'Figure A3: The gradients of both input and post LayerNorm in 21st layer of
    the MM-Vicuna as the training proceeds.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '图 A3: 随着训练的进行，MM-Vicuna 的第 21 层中输入和 LayerNorm 后的梯度。'
- en: '![Refer to caption](img/07e0691789eb105c2fee7699e09c39dd.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/07e0691789eb105c2fee7699e09c39dd.png)'
- en: 'Figure A4: The gradients of both input and post LayerNorm in 11th layer of
    the MM-Vicuna as the training proceeds.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '图 A4: 随着训练的进行，MM-Vicuna 的第 11 层中输入和 LayerNorm 后的梯度。'
