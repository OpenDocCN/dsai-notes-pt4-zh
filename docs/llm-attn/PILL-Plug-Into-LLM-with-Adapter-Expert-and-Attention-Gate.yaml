- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:03:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:03:10'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: PILL:Plug Into LLM with Adapter Expert and Attention Gate
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'PILL: 插件化 LLM 与适配器专家和注意力门控'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02126](https://ar5iv.labs.arxiv.org/html/2311.02126)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.02126](https://ar5iv.labs.arxiv.org/html/2311.02126)
- en: Fangyuan Zhang¹  Tingting Liang¹  Zhengyuan Wu¹  Yuyu Yin¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 方圆·张¹  婷婷·梁¹  郑元·吴¹  于雨·尹¹
- en: ¹ School of Computer Science and Technology Yuyu Yin is the corresponding author
       Hangzhou Dianzi University    China.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 计算机科学与技术学院 于雨·尹为通讯作者    杭州电子科技大学    中国。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Due to the remarkable capabilities of powerful Large Language Models (LLMs)
    in effectively following instructions, there has been a growing number of assistants
    in the community to assist humans. Recently, significant progress has been made
    in the development of Vision Language Models (VLMs), expanding the capabilities
    of LLMs and enabling them to execute more diverse instructions. However, it is
    foreseeable that models will likely need to handle tasks involving additional
    modalities such as speech, video, and others. This poses a particularly prominent
    challenge of dealing with the complexity of mixed modalities. To address this,
    we introduce a novel architecture called PILL: Plug Into LLM with adapter expert
    and attention gate to better decouple these complex modalities and leverage efficient
    fine-tuning. We introduce two modules: Firstly, utilizing Mixture-of-Modality-Adapter-Expert
    to independently handle different modalities, enabling better adaptation to downstream
    tasks while preserving the expressive capability of the original model. Secondly,
    by introducing Modality-Attention-Gating, which enables adaptive control of the
    contribution of modality tokens to the overall representation. In addition, we
    have made improvements to the Adapter to enhance its learning and expressive capabilities.
    Experimental results demonstrate that our approach exhibits competitive performance
    compared to other mainstream methods for modality fusion. For researchers interested
    in our work, we provide free access to the code and models at [https://github.com/DsaltYfish/PILL](https://github.com/DsaltYfish/PILL).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '由于强大的大型语言模型（LLMs）在有效执行指令方面的卓越能力，社区中出现了越来越多的助手来帮助人类。最近，视觉语言模型（VLMs）的发展取得了显著进展，扩展了LLMs的能力，使其能够执行更多样化的指令。然而，预计模型将需要处理涉及额外模态的任务，如语音、视频等。这带来了混合模态复杂性特别突出的挑战。为了解决这一问题，我们引入了一种新颖的架构，称为
    PILL: 插件化 LLM 与适配器专家和注意力门控，以更好地解耦这些复杂模态并利用高效的微调。我们介绍了两个模块：首先，利用模态混合适配器专家（Mixture-of-Modality-Adapter-Expert）独立处理不同的模态，实现更好的下游任务适应，同时保留原模型的表达能力。其次，通过引入模态注意力门控（Modality-Attention-Gating），实现对模态标记对整体表示贡献的自适应控制。此外，我们对适配器进行了改进，以增强其学习和表达能力。实验结果表明，我们的方法在模态融合方面表现出与其他主流方法的竞争力。对于对我们工作感兴趣的研究人员，我们提供了[https://github.com/DsaltYfish/PILL](https://github.com/DsaltYfish/PILL)上的代码和模型的免费访问。'
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The long-term goal of artificial intelligence is to enable it to utilize knowledge
    in a human-like manner for tasks such as reasoning, thinking, analysis, and decision-making.
    With the remarkable instruction-following ability and astonishing comprehension
    of human language exhibited by large language models [[31](#bib.bib31), [3](#bib.bib3),
    [6](#bib.bib6)], Universal Visual Language Models (VLMs) [[32](#bib.bib32), [5](#bib.bib5),
    [18](#bib.bib18)] have made significant progress in the field of AI’s multimodal
    capabilities. In order to achieve modular processing of multimodal tasks, recent
    VLMs primarily employ visual prompt generators to provide visual information to
    large language models, showcasing impressive zero-shot performance across various
    visual tasks [[12](#bib.bib12), [19](#bib.bib19), [8](#bib.bib8)].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的长期目标是使其能够以类似人类的方式利用知识进行推理、思考、分析和决策等任务。大型语言模型表现出的卓越指令跟随能力和惊人的人类语言理解能力，已使通用视觉语言模型（VLMs）在AI的多模态能力领域取得了显著进展。为了实现多模态任务的模块化处理，近期VLMs主要使用视觉提示生成器向大型语言模型提供视觉信息，展示了在各种视觉任务中的令人印象深刻的零样本性能。
- en: However, despite having modality information aligned with language, the internal
    structure of the underlying large language models still lacks the ability to process
    entangled modalities, resulting in most VLMs still struggling to comprehend complex
    multimodal prompts. One straightforward approach is to fully fine-tune the large
    language model or utilize the Parameter-Efficient Fine-Tuning (PEFT) method to
    enable the LLMs to learn how to handle other modal information. Yet, this may
    result in the mixture of multiple modality information, leading to interference
    with the original knowledge learned by the LLM.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管具有与语言对齐的模态信息，底层大型语言模型的内部结构仍然缺乏处理纠缠模态的能力，导致大多数 VLMs 仍然难以理解复杂的多模态提示。一种简单的方法是对大型语言模型进行全面微调或使用参数高效微调（PEFT）方法，使
    LLMs 学会处理其他模态信息。然而，这可能会导致多个模态信息的混合，从而干扰 LLM 原有的知识。
- en: '![Refer to caption](img/61ace491ebd5f13533fb870cd0199169.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/61ace491ebd5f13533fb870cd0199169.png)'
- en: 'Figure 1: Comparison between recent VLMs methods and our approach is shown
    in Figure. (a) represents the recent VL-LLMs method. The Processing Module represents
    the modules used for image processing, including ViT, Qformer, Resampler, and
    others. These modules are employed to process image information and transfer it
    to the LLMs. (b) represents our approach. We keep the image processing modules
    consistent with other methods and introduce a dedicated module within the LLMs
    to handle image information.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：图中展示了最近的 VLMs 方法与我们的方法的比较。（a）表示最近的 VL-LLMs 方法。处理模块表示用于图像处理的模块，包括 ViT、Qformer、Resampler
    和其他模块。这些模块用于处理图像信息并将其传递给 LLMs。（b）表示我们的方法。我们保持图像处理模块与其他方法一致，并在 LLMs 中引入一个专用模块来处理图像信息。
- en: 'In this paper, we propose PILL: Plug Into LLM with Adapter Expert and Attention
    Gate, to address the aforementioned issues. Our method consists of three modules.
    Firstly, inspired by the Mixture-of-Modality-Experts (MoME) approach [[1](#bib.bib1),
    [23](#bib.bib23)], we employ the Mixture-of-Modality-Adapter-Expert (MoMAE) to
    handle different modality inputs. In our experimental setup, MoMAE includes the
    V-Adapter and L-Adapter, which respectively handle the visual and textual modalities.
    Secondly, to ensure that the model produces results consistent with the original
    language model during initial training, we introduce the Modality-Attention-Gating
    (MAG) module. This stabilizes the training process and prevents visual information
    from interfering with the LLM’s text modeling phase. Thirdly, recognizing that
    an enhanced adapter can lead to improved performance, we refine the classical
    adapter by introducing the GLU-Adapter.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 PILL：Plug Into LLM with Adapter Expert and Attention Gate，以解决上述问题。我们的方法包含三个模块。首先，受到多模态专家（MoME）方法的启发[[1](#bib.bib1),
    [23](#bib.bib23)]，我们采用了多模态适配器专家（MoMAE）来处理不同的模态输入。在我们的实验设置中，MoMAE 包括 V-Adapter
    和 L-Adapter，分别处理视觉和文本模态。其次，为了确保模型在初始训练过程中产生与原语言模型一致的结果，我们引入了模态注意力门控（MAG）模块。这有助于稳定训练过程，并防止视觉信息干扰
    LLM 的文本建模阶段。第三，认识到增强的适配器可以提高性能，我们通过引入 GLU-Adapter 来改进经典适配器。
- en: To ensure stable training of our model, we adopt a two-stage training approach.
    In the first stage, we train the projection layer and V-Adapter separately to
    align visual and textual information, and enable the LLM to learn to process visual
    knowledge. In the second stage, we unlock all the proposed modules for downstream
    fine-tuning. Throughout both stages, the LLM and VPG module remain frozen. A notable
    advantage of our training method is its efficiency in training since we only train
    the inserted modules. Our method can be efficiently trained on a single A6000
    GPU, allowing for rapid training completion.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保模型的稳定训练，我们采用了两阶段训练方法。在第一阶段，我们分别训练投影层和 V-Adapter，以对齐视觉和文本信息，并使 LLM 学会处理视觉知识。在第二阶段，我们解锁所有提出的模块进行下游微调。在这两个阶段中，LLM
    和 VPG 模块保持冻结。我们训练方法的一个显著优点是训练效率高，因为我们只训练插入的模块。我们的方法可以在单个 A6000 GPU 上高效训练，从而快速完成训练。
- en: The experimental results demonstrate that our model achieves competitive performance
    compared to other methods. On the ScienceQA dataset, our model achieves a 1.4%
    accuracy improvement over models of equivalent scale. Compared to the best-performing
    LLaMA-based model, we achieve a 0.31% accuracy improvement.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，我们的模型在性能上与其他方法具有竞争力。在ScienceQA数据集上，我们的模型在同等规模的模型中提高了1.4%的准确率。与表现最佳的LLaMA基础模型相比，我们提高了0.31%的准确率。
- en: 'In summary, our contributions can be summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的贡献可以总结如下：
- en: '1.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose PILL, which addresses modality-specific information processing using
    MoMAE, dynamically adjusts modality injection into the LLM with MAG, and enhances
    performance with an improved adapter structure.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了PILL，它通过MoMAE解决了模态特定信息处理问题，使用MAG动态调整模态注入到LLM中，并通过改进的适配器结构提升性能。
- en: '2.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Experimental results demonstrate that PILL exhibits superior efficiency and
    competitive performance compared to existing multimodal LLMs. It also showcases
    the significant potential for handling multimodal instructions while maintaining
    satisfactory performance on general VQA benchmarks.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果表明，PILL在效率和性能方面优于现有的多模态LLM。它还展示了在处理多模态指令时的显著潜力，同时在通用VQA基准上保持了令人满意的性能。
- en: '3.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: PILL demonstrates superior training and computational efficiency, as our experiments
    can be completed on a single A6000 GPU.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PILL展现了优越的训练和计算效率，因为我们的实验可以在单个A6000 GPU上完成。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Multi-Modal Pre-Training
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多模态预训练
- en: Large-scale pretraining has played a crucial role in the field of multimodal
    learning. Methods such as VL-BERT[[36](#bib.bib36)], ViL-BERT[[26](#bib.bib26)],
    MCAN[[46](#bib.bib46)], LXMERT[[37](#bib.bib37)], and Rosita[[7](#bib.bib7)] extract
    key information from images using object detectors and process it together with
    text in transformer[[40](#bib.bib40)] blocks. Following the impressive success
    of CLIP[[34](#bib.bib34)] in image-text retrieval, contrastive learning has gained
    attention in the multimodal domain. Methods like ALBEF[[21](#bib.bib21)], BLIP[[20](#bib.bib20)],
    VLMO[[1](#bib.bib1)], BEiT-3[[41](#bib.bib41)], and CoCa[[45](#bib.bib45)] utilize
    contrastive learning to align image and text features, demonstrating the benefits
    of such alignment. Subsequently, with advancements in large-scale model techniques,
    methods like PaLi[[4](#bib.bib4)], PaLM[[5](#bib.bib5)], KOSMOS[[18](#bib.bib18),
    [33](#bib.bib33), [29](#bib.bib29)], BLIP-2[[19](#bib.bib19)] build upon ClipCap[[30](#bib.bib30)]
    by incorporating features from images specifically processed by ViT[[9](#bib.bib9)]
    as prompts in the LM’s input. More recently, with the rise of LLMs, researchers
    have focused on leveraging the powerful capabilities of LLMs and combining them
    with visual information. Mini-GPT4[[50](#bib.bib50)] and LLaVA[[25](#bib.bib25),
    [24](#bib.bib24)] have discovered that a projection layer can effectively project
    visual information into the textual space of LLMs, and they train only this projection
    layer using large-scale pretraining data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模预训练在多模态学习领域中发挥了重要作用。VL-BERT[[36](#bib.bib36)]、ViL-BERT[[26](#bib.bib26)]、MCAN[[46](#bib.bib46)]、LXMERT[[37](#bib.bib37)]和Rosita[[7](#bib.bib7)]等方法通过对象检测器从图像中提取关键信息，并与文本一起在transformer[[40](#bib.bib40)]块中处理。继CLIP[[34](#bib.bib34)]在图像-文本检索中的显著成功之后，对比学习在多模态领域获得了关注。像ALBEF[[21](#bib.bib21)]、BLIP[[20](#bib.bib20)]、VLMO[[1](#bib.bib1)]、BEiT-3[[41](#bib.bib41)]和CoCa[[45](#bib.bib45)]等方法利用对比学习对齐图像和文本特征，展示了这种对齐的好处。随后，随着大规模模型技术的进步，PaLi[[4](#bib.bib4)]、PaLM[[5](#bib.bib5)]、KOSMOS[[18](#bib.bib18),
    [33](#bib.bib33), [29](#bib.bib29)]、BLIP-2[[19](#bib.bib19)]等方法在ClipCap[[30](#bib.bib30)]的基础上，通过将ViT[[9](#bib.bib9)]处理过的图像特征作为提示整合到LM的输入中。最近，随着LLM的崛起，研究人员关注于利用LLM的强大能力并将其与视觉信息结合。Mini-GPT4[[50](#bib.bib50)]和LLaVA[[25](#bib.bib25),
    [24](#bib.bib24)]发现一个投影层可以有效地将视觉信息投射到LLM的文本空间中，并且仅使用大规模预训练数据来训练这个投影层。
- en: 2.2 Multi-Modal Instruction-Tuning in PEFT
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多模态指令调优在PEFT中的应用
- en: To address the high cost associated with full fine-tuning, the Parameter-Efficient
    Fine-Tuning (PEFT) technique has emerged as an alternative. We highlight three
    commonly used PEFT techniques and discuss their applications in various methods.
    Firstly, the Adapter[[16](#bib.bib16)] approach has been explored in VL-Adapter[[43](#bib.bib43)].
    Extensive experiments have demonstrated the advantages of adapters over other
    PEFT methods in the multimodal setting. MAGMA[[11](#bib.bib11)] introduces adapters
    within the LM based on a frozen model. LAVIN[[28](#bib.bib28)] adopts a similar
    approach by treating single-modal and multimodal tasks as separate tasks and utilizing
    MMA (Mixture-of-Modality Adapter) for each task. The key distinction between our
    MoMAE approach and LAVIN is that we focus on modality tokens, while LAVIN focuses
    on tasks. Next, the application of LoRA[[17](#bib.bib17)] in the multimodal context
    is noteworthy. MultiModal-GPT[[14](#bib.bib14)] adopts a similar architecture
    to Flamingo[[12](#bib.bib12)] and incorporates LoRA for LM fine-tuning. Visual-ChatGLM[[10](#bib.bib10)],
    mPLUG-Owl[[42](#bib.bib42)], and LAMM[[44](#bib.bib44)] also employ LoRA for LM
    fine-tuning after pretraining with VPG (Visual Prompt Generation). Finally, the
    prefix-tuning[[22](#bib.bib22)] technique, exemplified by LLAMA-Adapter[[48](#bib.bib48),
    [13](#bib.bib13)], involves adding the image representation from the visual encoder
    to the prefix tokens and processing it along with the text tokens in the LM layers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决完全微调的高成本问题，参数高效微调（PEFT）技术作为一种替代方案应运而生。我们突出了三种常用的PEFT技术，并讨论了它们在各种方法中的应用。首先，Adapter[[16](#bib.bib16)]
    方法已在VL-Adapter[[43](#bib.bib43)] 中得到了探索。大量实验表明，在多模态设置中，适配器相比于其他PEFT方法具有优势。MAGMA[[11](#bib.bib11)]
    在基于冻结模型的语言模型中引入了适配器。LAVIN[[28](#bib.bib28)] 采用了类似的方法，将单模态和多模态任务视为独立任务，并为每个任务使用MMA（混合模态适配器）。我们MoMAE方法与LAVIN的主要区别在于，我们关注于模态标记，而LAVIN关注于任务。接下来，在多模态背景下，LoRA[[17](#bib.bib17)]
    的应用值得注意。MultiModal-GPT[[14](#bib.bib14)] 采用了类似于Flamingo[[12](#bib.bib12)] 的架构，并结合LoRA进行语言模型微调。Visual-ChatGLM[[10](#bib.bib10)]、mPLUG-Owl[[42](#bib.bib42)]
    和LAMM[[44](#bib.bib44)] 也在经过VPG（视觉提示生成）预训练后，使用LoRA进行语言模型微调。最后，前缀微调[[22](#bib.bib22)]
    技术，以LLAMA-Adapter[[48](#bib.bib48), [13](#bib.bib13)] 为例，通过将视觉编码器中的图像表示添加到前缀标记中，并与文本标记一起在语言模型层中处理。
- en: '![Refer to caption](img/dcd9b2d53e7f9e1fc1f7982b45835ffd.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dcd9b2d53e7f9e1fc1f7982b45835ffd.png)'
- en: 'Figure 2: The overview of the architecture of PILL and two module of the PILL:
    Mixture-of-Modality-Adapter-Expert (MoMAE) and Modality-Attention-Gating (MAG).
    In PILL, the novel MoMAE are employed to handle tokens from different modalities.
    During finetuning, MAG is used for coordinate the weights of other modalities'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：PILL架构的概述以及PILL的两个模块：混合模态适配器专家（MoMAE）和模态注意力门控（MAG）。在PILL中，采用了新颖的MoMAE来处理来自不同模态的标记。在微调过程中，MAG用于协调其他模态的权重。
- en: 3 Method
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: In this section, we will present the technical details of PILL, including the
    model architecture (Sec.[3.1](#S3.SS1 "3.1 Model Architecture ‣ 3 Method ‣ PILL:Plug
    Into LLM with Adapter Expert and Attention Gate")), Mixture-of-Modality-Adapter-Expert
    (Sec.[3.2](#S3.SS2 "3.2 Mixture-of-Modality-Adapter-Expert ‣ 3 Method ‣ PILL:Plug
    Into LLM with Adapter Expert and Attention Gate")) for handling different modalities
    of information, Modality-Attention-Gating (Sec.[3.3](#S3.SS3 "3.3 Modality-Attention-Gating
    ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter Expert and Attention Gate")) enables
    adaptive control of the contribution of modality tokens to the overall representation,
    and the SwiGLU-Adapter improves in adapter learning and expressive capability
    (Sec.[3.4](#S3.SS4 "3.4 SwiGLU-Adapter ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter
    Expert and Attention Gate")). After a detailed introduction of these modules,
    we will proceed to discuss the training process and objectives of the model (Sec.[3.5](#S3.SS5
    "3.5 Stagewise Training ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter Expert and
    Attention Gate")). An overview of the PILL framework is depicted in Figure [2](#S2.F2
    "Figure 2 ‣ 2.2 Multi-Modal Instruction-Tuning in PEFT ‣ 2 Related Work ‣ PILL:Plug
    Into LLM with Adapter Expert and Attention Gate").
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 PILL 的技术细节，包括模型架构（Sec.[3.1](#S3.SS1 "3.1 Model Architecture ‣ 3 Method
    ‣ PILL:Plug Into LLM with Adapter Expert and Attention Gate")），模态混合适配器专家（Sec.[3.2](#S3.SS2
    "3.2 Mixture-of-Modality-Adapter-Expert ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter
    Expert and Attention Gate")）用于处理不同模态的信息，模态注意力门控（Sec.[3.3](#S3.SS3 "3.3 Modality-Attention-Gating
    ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter Expert and Attention Gate")）实现对模态标记对整体表示贡献的自适应控制，SwiGLU-Adapter
    改进了适配器学习和表达能力（Sec.[3.4](#S3.SS4 "3.4 SwiGLU-Adapter ‣ 3 Method ‣ PILL:Plug Into
    LLM with Adapter Expert and Attention Gate")）。在详细介绍这些模块之后，我们将继续讨论模型的训练过程和目标（Sec.[3.5](#S3.SS5
    "3.5 Stagewise Training ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter Expert and
    Attention Gate")）。PILL 框架的概述见图 [2](#S2.F2 "Figure 2 ‣ 2.2 Multi-Modal Instruction-Tuning
    in PEFT ‣ 2 Related Work ‣ PILL:Plug Into LLM with Adapter Expert and Attention
    Gate")。
- en: 3.1 Model Architecture
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 模型架构
- en: Given a sample containing a set of images and text, we first process each image
    in the image set using the Q-former module from BLIP-2 to extract image features.
    Q-former takes a fixed number of $K$, which represents the visual prompts for
    the $j$, which represents the $i$. We define that $H^{v}=\{v_{11},...,v_{1K},...,v_{j1},...,v_{jK}\}$
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含一组图像和文本的样本，我们首先使用 BLIP-2 的 Q-former 模块处理图像集中的每张图像，以提取图像特征。Q-former 采用固定数量的
    $K$，表示视觉提示，用于 $j$，表示 $i$。我们定义 $H^{v}=\{v_{11},...,v_{1K},...,v_{j1},...,v_{jK}\}$
- en: 3.2 Mixture-of-Modality-Adapter-Expert
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模态混合适配器专家
- en: 'We propose a universal multimodal module, named MoMAE, which is inspired by
    MoME [[1](#bib.bib1), [23](#bib.bib23)] and Adapter. MoMAE introduces Mixture-of-Modality-Adapter-Expert
    as an alternative solution to the standard Transformer feed-forward networks for
    encoding different modalities. As is shown in [2](#S2.F2 "Figure 2 ‣ 2.2 Multi-Modal
    Instruction-Tuning in PEFT ‣ 2 Related Work ‣ PILL:Plug Into LLM with Adapter
    Expert and Attention Gate")(b), after leveraging the output vectors from the previous
    layer and adapter-based multi-head self-attention (MSA), MoMAE are able to capture
    and process modality-specific information by switching to different modality adapter
    experts. We use vision adapter expert for encoding images $H^{v}$, which can be
    formulated as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一个通用的多模态模块，命名为 MoMAE，它受到了 MoME [[1](#bib.bib1), [23](#bib.bib23)] 和 Adapter
    的启发。MoMAE 引入了模态混合适配器专家，作为编码不同模态的标准 Transformer 前馈网络的替代方案。如 [2](#S2.F2 "Figure
    2 ‣ 2.2 Multi-Modal Instruction-Tuning in PEFT ‣ 2 Related Work ‣ PILL:Plug Into
    LLM with Adapter Expert and Attention Gate")(b) 所示，在利用来自前一层的输出向量和基于适配器的多头自注意力（MSA）后，MoMAE
    能够通过切换到不同的模态适配器专家来捕捉和处理特定模态的信息。我们使用视觉适配器专家来编码图像 $H^{v}$，其公式化为：
- en: '|  | $\displaystyle{H_{out}^{v}}$ |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{H_{out}^{v}}$ |  | (1) |'
- en: '|  | $\displaystyle{H_{out}^{t}}$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{H_{out}^{t}}$ |  |'
- en: Previous research has demonstrated that knowledge is preserved in the feed-forward
    networks (FFNs) within the Transformer. Therefore, we employ MoMAE to enable LLM
    to learn knowledge for handling visual features. Additionally, in our experiments,
    we observed that the variance of visual tokens is typically significantly larger
    than that of textual tokens. Such distribution inconsistency can potentially lead
    to bad training results. Employing a dedicated module to handle visual tokens
    may be a prudent choice. Furthermore, due to the adoption of the Adapter strategy
    for learning, the weights of the original LLaMA are retained, allowing the alignment
    of visual and textual modalities in the same representation space. The alignment
    has been proved to be crucial for multi-modal tasks in previous work.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究已证明，知识在 Transformer 内的前馈网络（FFNs）中得以保留。因此，我们采用 MoMAE 使 LLM 学习处理视觉特征的知识。此外，在我们的实验中，我们观察到视觉标记的方差通常显著大于文本标记的方差。这种分布不一致可能导致训练效果不佳。采用专门的模块处理视觉标记可能是明智的选择。此外，由于采用了
    Adapter 策略进行学习，原始 LLaMA 的权重得以保留，从而实现视觉和文本模态在同一表示空间中的对齐。此前的工作已证明，这种对齐对多模态任务至关重要。
- en: 3.3 Modality-Attention-Gating
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 模态-注意力-门控
- en: 'Regarding modal fusion, both Prefix-Tuning and Cross-Attn methods have demonstrated
    excellent performance. In the experimental findings of Lynx [[47](#bib.bib47)],
    Prefix-Tuning generally achieves better performance, while the Cross-Attn model
    may require more hyper-parameter searching to achieve superior results. Is it
    possible to combine the advantages of these two methods? To address this, we propose
    MAG: Modality-Attention-Gating. Specifically, we apply MAG to filter and adjust
    the attention weights of other modality information, aiming to allow the modal
    information to play its respective role in different transformer layers.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '关于模态融合，Prefix-Tuning 和 Cross-Attn 方法都表现出了优异的性能。在 Lynx [[47](#bib.bib47)] 的实验结果中，Prefix-Tuning
    通常表现更好，而 Cross-Attn 模型可能需要更多的超参数搜索才能获得更好的结果。是否可以结合这两种方法的优点？为此，我们提出了 MAG: Modality-Attention-Gating。具体而言，我们应用
    MAG 来过滤和调整其他模态信息的注意力权重，旨在使模态信息在不同的 Transformer 层中发挥各自的作用。'
- en: 'We specify $G:\mathbb{R}^{d_{dim}}\rightarrow\mathbb{R}^{n_{heads}}$. Given
    vision modality tokens, our MAG module is computed as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定 $G:\mathbb{R}^{d_{dim}}\rightarrow\mathbb{R}^{n_{heads}}$。给定视觉模态标记，我们的
    MAG 模块的计算如下：
- en: '|  | $\displaystyle G_{v}(H)$ |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{v}(H)$ |  | (2) |'
- en: '|  | $\displaystyle G_{t}(H)$ |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle G_{t}(H)$ |  |'
- en: '|  | $\displaystyle S(H)$ |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S(H)$ |  |'
- en: '|  | $\displaystyle MAG(H)$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle MAG(H)$ |  |'
- en: In attention layer, $H$. $G_{v}(H)$ is to multiply the attention heads of each
    modality by a gating weight, which is initialized to 0\. Note that for every visual
    token within the same layer, they undergo the same gate. This allows visual information
    to have distinct effects across different layers while filtering out irrelevant
    information.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力层中，$H$。$G_{v}(H)$ 是通过门控权重将每种模态的注意力头相乘，初始值为 0。注意，对于同一层中的每个视觉标记，它们经历相同的门控。这使得视觉信息在不同层之间具有不同的效果，同时过滤掉不相关的信息。
- en: Based on previous empirical experience in LM, modules closer to the input layer
    primarily focus on language modeling, while modules near the output layer are
    responsible for knowledge injection. The pure use of Prefix-Tuning might introduce
    interference in the language modeling stage due to the injection of other modal
    information, as observed in the experiments with Flamingo [[12](#bib.bib12)],
    where the gate weights near the input approach zero. On the other hand, the benefit
    of Prefix-Tuning lies in injecting different information at different transformer
    layers, effectively adapting the modal information at each layer. This may be
    a contributing factor to the generally higher performance of Prefix-Tuning compared
    to Cross-Attn. Therefore, based on the above assumptions, with the inclusion of
    the MAG and MoMAE module, the model architecture resembles Prefix-Tuning, while
    the details resemble Cross-Attn. In fact, our results of gating weight is similar
    to Flamingo [[12](#bib.bib12)]
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前在语言模型中的经验，靠近输入层的模块主要关注语言建模，而靠近输出层的模块负责知识注入。纯粹使用 Prefix-Tuning 可能会由于注入其他模态信息而干扰语言建模阶段，正如在
    Flamingo [[12](#bib.bib12)] 实验中观察到的那样，其中靠近输入的门权重接近于零。另一方面，Prefix-Tuning 的好处在于可以在不同的
    Transformer 层注入不同的信息，有效地调整每层的模态信息。这可能是 Prefix-Tuning 相比于 Cross-Attn 通常具有更高性能的一个原因。因此，基于上述假设，随着
    MAG 和 MoMAE 模块的加入，模型架构类似于 Prefix-Tuning，而细节则类似于 Cross-Attn。实际上，我们的门控权重结果与 Flamingo [[12](#bib.bib12)]
    类似。
- en: 3.4 SwiGLU-Adapter
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 SwiGLU-Adapter
- en: '![Refer to caption](img/3759c132cd3e42848d86a4f8ee99c294.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3759c132cd3e42848d86a4f8ee99c294.png)'
- en: 'Figure 3: Architecture of SwiGLU-Adapter'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：SwiGLU-Adapter 架构
- en: 'The classical Adapter architecture consists of a downsampling operation, activation
    function, upsampling, and residual connection. With the introduction of the Gated
    Linear Unit(GLU) [[35](#bib.bib35)] structure, it has gained wide application
    in LLMs such as LLaMA [[38](#bib.bib38), [39](#bib.bib39)] and PaLM [[5](#bib.bib5)].
    In a corresponding manner, can we also transform the architecture of Adapter into
    GLU? To address this, we propose the SwiGLU-Adapter. As shown in [3](#S3.F3 "Figure
    3 ‣ 3.4 SwiGLU-Adapter ‣ 3 Method ‣ PILL:Plug Into LLM with Adapter Expert and
    Attention Gate"), we modify the structure of the Adapter by incorporating two
    downsampling operations, where one of the downsampling outputs is passed through
    an activation function. The outputs are then multiplied element-wise and followed
    by an upsampling operation. We represent this transformation using the following
    equation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '经典的 Adapter 架构包括下采样操作、激活函数、上采样和残差连接。引入了 Gated Linear Unit (GLU) [[35](#bib.bib35)]
    结构后，它在 LLM（如 LLaMA [[38](#bib.bib38), [39](#bib.bib39)] 和 PaLM [[5](#bib.bib5)]）中得到了广泛应用。以此类推，我们是否也可以将
    Adapter 的架构转换为 GLU？为了解决这个问题，我们提出了 SwiGLU-Adapter。如 [3](#S3.F3 "图 3 ‣ 3.4 SwiGLU-Adapter
    ‣ 3 方法 ‣ PILL: 用 Adapter 专家和注意力门插入 LLM") 所示，我们通过引入两个下采样操作来修改 Adapter 的结构，其中一个下采样输出经过激活函数处理。然后将这些输出逐元素相乘，并进行上采样操作。我们使用以下方程来表示这种转换：'
- en: '|  | $\displaystyle H_{down1}$ |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{down1}$ |  | (3) |'
- en: '|  | $\displaystyle H_{down2}$ |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{down2}$ |  |'
- en: '|  | $\displaystyle H_{down}$ |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{down}$ |  |'
- en: '|  | $\displaystyle H_{up}$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{up}$ |  |'
- en: '|  | $\displaystyle H_{out}$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{out}$ |  |'
- en: 3.5 Stagewise Training
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 分阶段训练
- en: 'Our training process consists of the classic pre-training and downstream task
    fine-tuning stages, where different modules are trained at each stage. For training
    objective, both stage train on loss which is an auto-regressive manner. Given
    input $X_{text}$ and $X_{answer}$, we we express our training objective using
    the following equation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练过程包括经典的预训练和下游任务微调阶段，在每个阶段中训练不同的模块。对于训练目标，两个阶段都训练在一个自回归的损失上。给定输入 $X_{text}$
    和 $X_{answer}$，我们使用以下方程表示我们的训练目标：
- en: '|  |  | $1$2 |  | (4) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (4) |'
- en: Here, $p_{t}$, $\theta$ with trainable parameters $\theta$ in both training
    stage. The following sections will provide a detailed description of each stage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$p_{t}$ 和 $\theta$ 在训练阶段具有可训练的参数 $\theta$。接下来的部分将详细描述每个阶段。
- en: 'Stage 1: Pre-training for Modality Alignment. In stage 1, similar to VLMO [[1](#bib.bib1),
    [23](#bib.bib23)] and LLaVA [[25](#bib.bib25)], we set $\theta=\{A_{v},P\}$ is
    visual adapter and $P$ in the last layer does not have a connection to the training
    objective, rendering it untrainable. Therefore, we only utilize the V-Adapter
    as a T-Adapter in this case, effectively reversing the positional information
    of the input VL (Visual-Language) tokens.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段 1：模态对齐的预训练。在阶段 1，类似于 VLMO [[1](#bib.bib1), [23](#bib.bib23)] 和 LLaVA [[25](#bib.bib25)]，我们设置
    $\theta=\{A_{v},P\}$ 作为视觉适配器，并且最后一层的 $P$ 与训练目标没有连接，因此无法训练。因此，在这种情况下，我们只使用 V-Adapter
    作为 T-Adapter，有效地逆转了输入 VL（视觉-语言）标记的位置信息。
- en: 'Stage 2: Downstream Task Fine-tuning. In stage 2, we aim to train the model
    to respond to instructions and images, resembling a real AI assistant. Therefore,
    we train all the parameters of the Adapters, attn-gates, and projections, where
    $\theta=\{A_{v},A_{t},A_{attn},G,P\}$. For quantitative experimental analysis,
    we conduct ablation experiments on the ScienceQA [[27](#bib.bib27)], a small-scale
    dataset. To achieve generalized instruction following, we train the model on the
    LLaVA1.5 665k [[24](#bib.bib24)] dataset.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段 2：下游任务微调。在阶段 2，我们的目标是训练模型以响应指令和图像，类似于真正的 AI 助手。因此，我们训练 Adapters、attn-gates
    和 projections 的所有参数，其中 $\theta=\{A_{v},A_{t},A_{attn},G,P\}$。为了进行定量实验分析，我们在小规模数据集
    ScienceQA [[27](#bib.bib27)] 上进行消融实验。为了实现通用的指令跟随，我们在 LLaVA1.5 665k [[24](#bib.bib24)]
    数据集上训练模型。
- en: 4 Experiment
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: '| Method | #T-Param | Subject | Context Modality | Grade | Average |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #T-参数 | 学科 | 上下文模式 | 年级 | 平均 |'
- en: '| NAT | SOC | LAN | TXT | IMG | NO | G1-6 | G7-12 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| NAT | SOC | LAN | TXT | IMG | NO | G1-6 | G7-12 |'
- en: '| Zero- & few-shot methods |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 零样本和少样本方法 |  |'
- en: '| Human [[27](#bib.bib27)] | - | 90.23 | 84.97 | 87.48 | 89.60 | 87.50 | 88.10
    | 91.59 | 82.42 | 88.40 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 人类 [[27](#bib.bib27)] | - | 90.23 | 84.97 | 87.48 | 89.60 | 87.50 | 88.10
    | 91.59 | 82.42 | 88.40 |'
- en: '| GPT-3.5 [[27](#bib.bib27)] | - | 74.64 | 69.74 | 76.00 | 74.44 | 67.28 |
    77.42 | 76.80 | 68.89 | 73.97 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 [[27](#bib.bib27)] | - | 74.64 | 69.74 | 76.00 | 74.44 | 67.28 |
    77.42 | 76.80 | 68.89 | 73.97 |'
- en: '| GPT-3.5 (CoT) [[27](#bib.bib27)] | - | 75.44 | 70.87 | 78.09 | 74.68 | 67.43
    | 79.93 | 78.23 | 69.68 | 75.17 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 (CoT) [[27](#bib.bib27)] | - | 75.44 | 70.87 | 78.09 | 74.68 | 67.43
    | 79.93 | 78.23 | 69.68 | 75.17 |'
- en: '| GPT-4 [[32](#bib.bib32)] | - | 84.06 | 73.45 | 87.36 | 81.87 | 70.75 | 90.73
    | 84.69 | 79.10 | 82.69 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 [[32](#bib.bib32)] | - | 84.06 | 73.45 | 87.36 | 81.87 | 70.75 | 90.73
    | 84.69 | 79.10 | 82.69 |'
- en: '| Representative & SoTA models |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 代表性和 SoTA 模型 |  |'
- en: '| UnifiedQA [[27](#bib.bib27)] | 223M | 71.00 | 76.04 | 78.91 | 66.42 | 66.53
    | 81.81 | 77.06 | 68.82 | 74.11 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| UnifiedQA [[27](#bib.bib27)] | 223M | 71.00 | 76.04 | 78.91 | 66.42 | 66.53
    | 81.81 | 77.06 | 68.82 | 74.11 |'
- en: '| MM-CoT[Base] [[49](#bib.bib49)] | 223M | 87.52 | 77.17 | 85.82 | 87.88 |
    82.90 | 86.83 | 84.65 | 85.37 | 84.91 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| MM-CoT[Base] [[49](#bib.bib49)] | 223M | 87.52 | 77.17 | 85.82 | 87.88 |
    82.90 | 86.83 | 84.65 | 85.37 | 84.91 |'
- en: '| MM-CoT[Large] [[49](#bib.bib49)] | 738M | 95.91 | 82.00 | 90.82 | 95.26 |
    88.80 | 92.89 | 92.44 | 90.31 | 91.68 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| MM-CoT[Large] [[49](#bib.bib49)] | 738M | 95.91 | 82.00 | 90.82 | 95.26 |
    88.80 | 92.89 | 92.44 | 90.31 | 91.68 |'
- en: '| LLaMA-based methods |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 基于LLaMA的方法 |  |'
- en: '| LLaMA-Adapter [[48](#bib.bib48)] | 1.8M | 84.37 | 88.30 | 84.36 | 83.72 |
    80.32 | 86.90 | 85.83 | 84.05 | 85.19 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter [[48](#bib.bib48)] | 1.8M | 84.37 | 88.30 | 84.36 | 83.72 |
    80.32 | 86.90 | 85.83 | 84.05 | 85.19 |'
- en: '| LaVIN [[28](#bib.bib28)] | 5.4M | 89.88 | 94.49 | 89.82 | 88.95 | 87.61 |
    91.85 | 91.45 | 89.72 | 90.83 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LaVIN [[28](#bib.bib28)] | 5.4M | 89.88 | 94.49 | 89.82 | 88.95 | 87.61 |
    91.85 | 91.45 | 89.72 | 90.83 |'
- en: '| LLaVA [[25](#bib.bib25)] | 13B | 90.36 | 95.95 | 88.00 | 89.49 | 88.00 |
    90.66 | 90.93 | 90.90 | 90.92 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA [[25](#bib.bib25)] | 13B | 90.36 | 95.95 | 88.00 | 89.49 | 88.00 |
    90.66 | 90.93 | 90.90 | 90.92 |'
- en: '| LLaMA-SciTune [[15](#bib.bib15)] | 13B | 89.30 | 95.61 | 87.00 | 93.08 |
    86.67 | 91.75 | 84.37 | 91.30 | 90.03 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-SciTune [[15](#bib.bib15)] | 13B | 89.30 | 95.61 | 87.00 | 93.08 |
    86.67 | 91.75 | 84.37 | 91.30 | 90.03 |'
- en: '| PILL (ours) | 45M | 90.36 | 95.84 | 89.27 | 89.39 | 88.65 | 91.71 | 92.11
    | 89.65 | 91.23 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| PILL（我们的） | 45M | 90.36 | 95.84 | 89.27 | 89.39 | 88.65 | 91.71 | 92.11 |
    89.65 | 91.23 |'
- en: 'Table 1: Comparison on ScienceQA test set. Question classes: NAT = natural
    science, SOC = social science, LAN = language science, TXT = text context, IMG
    = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12\. #T-Params
    denotes that the number of trainable parameters.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在 ScienceQA 测试集上的比较。问题类别：NAT = 自然科学，SOC = 社会科学，LAN = 语言科学，TXT = 文本上下文，IMG
    = 图像上下文，NO = 无上下文，G1-6 = 年级 1-6，G7-12 = 年级 7-12。#T-Params 表示可训练参数的数量。
- en: 4.1 Dataset
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: CC595K. CC595K was filtered by LLaVA [[25](#bib.bib25), [24](#bib.bib24)]. LLaVa
    extracted noun phrases from each caption in the entire CC3M dataset and calculated
    the frequency of each unique noun phrase. Noun phrases with a frequency below
    3 were skipped, as they typically represent rare combinations of concepts and
    attributes. For noun phrases with a frequency exceeding 100, a random subset of
    100 samples was selected from all captions. This resulted in approximately 595k
    image-text pairs. We use CC595K to pretrain our model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: CC595K。CC595K由LLaVA[[25](#bib.bib25), [24](#bib.bib24)]过滤。LLaVa从整个CC3M数据集中提取了名词短语，并计算了每个独特名词短语的频率。频率低于3的名词短语被跳过，因为它们通常代表稀有的概念和属性组合。对于频率超过100的名词短语，从所有字幕中随机选择了100个样本。这导致了大约595k的图像-文本对。我们使用CC595K来预训练我们的模型。
- en: ScienceQA. ScienceQA [[27](#bib.bib27)] consists of 21,000 data samples, including
    multiple-choice questions with multimodal content. It covers 3 subjects, 26 topics,
    127 categories and 379 skills. We utilized the training split of the ScienceQA
    dataset, which comprised 12,726 samples, to further optimize our model. Additionally,
    we employed the test split, consisting of 4,241 samples, to conduct an initial
    evaluation of our model’s performance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ScienceQA。ScienceQA[[27](#bib.bib27)]包含21,000个数据样本，包括具有多模态内容的多项选择题。它涵盖了3个学科、26个主题、127个类别和379个技能。我们利用了ScienceQA数据集的训练集，其中包含12,726个样本，以进一步优化我们的模型。此外，我们使用了测试集，包含4,241个样本，以对模型的性能进行初步评估。
- en: 4.2 Implementation Details
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实施细节
- en: For the image feature extraction part, we utilized the Q-former module from
    BLIP2 [[19](#bib.bib19)], which has been thoroughly pre-trained on Flan-T5-XXL [[6](#bib.bib6)].
    For the LLM, we opted for the LLaMA-2-chat [[39](#bib.bib39)] model. The default
    setting for the intermediate hidden layer dimension in our adapter is 32\. We
    employed AdamW as the optimizer with a cosine learning rate decay strategy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像特征提取部分，我们使用了来自BLIP2的Q-former模块[[19](#bib.bib19)]，该模块已经在Flan-T5-XXL[[6](#bib.bib6)]上进行了彻底的预训练。对于LLM，我们选择了LLaMA-2-chat[[39](#bib.bib39)]模型。我们适配器的默认设置为中间隐藏层维度为32。我们使用AdamW作为优化器，并采用了余弦学习率衰减策略。
- en: During the pre-training phase, we solely trained the projection layer and v-adapter
    for 3 epochs, with a learning rate of 1e-3\. The language model input length was
    set to 128, and the batch size was set to 32\. In the fine-tuning stage, we trained
    the projection layer, v-adapter, t-adapter, attn-adapter, and attn-gate layer
    for 20 epochs, with a learning rate of 2e-3\. The language model input length
    was increased to 512, and the batch size was reduced to 4.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，我们仅训练了投影层和v-adapter，训练了3个周期，学习率为1e-3。语言模型的输入长度设置为128，批量大小设置为32。在微调阶段，我们训练了投影层、v-adapter、t-adapter、attn-adapter和attn-gate层，训练了20个周期，学习率为2e-3。语言模型的输入长度增加到512，批量大小减少到4。
- en: Under our experimental settings, the total number of trainable parameters amounts
    to 45M. The training of our model was conducted on a single A6000, which is a
    hardware configuration widely acceptable among researchers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验设置下，可训练参数的总数达到45M。我们的模型训练是在一台单独的A6000上进行的，这是一种在研究人员中广泛接受的硬件配置。
- en: 4.3 Experimental Results
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实验结果
- en: 4.3.1 Results on ScienceQA
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 ScienceQA上的结果
- en: As shown in Table [1](#S4.T1 "Table 1 ‣ 4 Experiment ‣ PILL:Plug Into LLM with
    Adapter Expert and Attention Gate"), we compare the performance of several existing
    methods on the ScienceQA [[27](#bib.bib27)]. Firstly, we consider zero-shot or
    few-shot approaches. Human performance in question answering achieves an accuracy
    of 88.40%. Remarkably, GPT-4 achieves an accuracy of 82.69% without any specific
    training on ScienceQA. Among other LLMs, MM-CoT[Large] [[49](#bib.bib49)] achieves
    an accuracy of 91.68%. We speculate that this performance difference may be due
    to variations in the base models. Notably, there is a significant discrepancy
    in the NAT and SOC metrics between MM-COT and llama-based models. Furthermore,
    it is expected that MM-COT performs exceptionally well, as it focuses on the chain-of-thought,
    a technique that has been proven to significantly enhance LLM capabilities. Therefore,
    the outstanding performance of MM-COT comes as no surprise. Among the LLaMA-based
    methods, our method consistently ranks first or second across all metrics, ultimately
    achieving the best overall performance in terms of average metrics. Our method
    achieves comparable performance to the state-of-the-art methods, even without
    employing any extravagant techniques. In terms of parameter count, our method
    significantly reduces training overhead compared to LLaVA as we only need to train
    the adapter components of the model. Despite our method having significantly more
    training parameters compared to LaVIN, our training speed is faster due to the
    absence of gradient backpropagation to ViT during training. In fact, under the
    same experimental settings, our method requires only 90% of the training time
    compared to LaVIN. Overall, these results validate the efficiency and design of
    PILL.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [1](#S4.T1 "Table 1 ‣ 4 Experiment ‣ PILL:Plug Into LLM with Adapter Expert
    and Attention Gate") 所示，我们比较了几种现有方法在 ScienceQA [[27](#bib.bib27)] 上的表现。首先，我们考虑零样本或少样本的方法。人类在问答中的表现达到
    88.40% 的准确率。值得注意的是，GPT-4 在没有针对 ScienceQA 进行特定训练的情况下，达到了 82.69% 的准确率。在其他 LLM 中，MM-CoT[Large] [[49](#bib.bib49)]
    达到了 91.68% 的准确率。我们推测，这种表现差异可能与基础模型的不同有关。值得注意的是，MM-COT 和基于 llama 的模型在 NAT 和 SOC
    指标上存在显著差异。此外，MM-COT 表现特别优秀，因为它关注于链式思维，这是一种已被证明能显著提升 LLM 能力的技术。因此，MM-COT 的卓越表现并不令人意外。在基于
    LLaMA 的方法中，我们的方法在所有指标上始终排名第一或第二，最终在平均指标方面取得了最佳整体表现。我们的方法在不采用任何奢侈技术的情况下，达到了与最先进方法相媲美的性能。在参数数量方面，我们的方法相比于
    LLaVA 显著减少了训练开销，因为我们只需要训练模型的适配器组件。尽管我们的方法相比于 LaVIN 具有显著更多的训练参数，但由于训练过程中没有进行对 ViT
    的梯度反向传播，我们的训练速度更快。实际上，在相同实验设置下，我们的方法仅需 LaVIN 训练时间的 90%。总体而言，这些结果验证了 PILL 的效率和设计。
- en: '| Settings | w/o pretrain | w pretrain |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | 无预训练 | 有预训练 |'
- en: '| LLaMA-Adapter [[48](#bib.bib48)] | 85.19 | - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter [[48](#bib.bib48)] | 85.19 | - |'
- en: '| LLaVA [[25](#bib.bib25)] | 85.81 | 89.84 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA [[25](#bib.bib25)] | 85.81 | 89.84 |'
- en: '| LaVIN [[28](#bib.bib28)] | 89.41 | - |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| LaVIN [[28](#bib.bib28)] | 89.41 | - |'
- en: '| LLaMA-SciTune [[15](#bib.bib15)] | - | 86.11 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-SciTune [[15](#bib.bib15)] | - | 86.11 |'
- en: '| PILL(ours) | 90.24 | 91.23 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| PILL(ours) | 90.24 | 91.23 |'
- en: 'Table 2: Results of existing multimodal LLMs in 7B model scaling on ScienceQA
    test set.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：现有多模态 LLM 在 ScienceQA 测试集上的 7B 模型扩展结果。
- en: In Table [2](#S4.T2 "Table 2 ‣ 4.3.1 Results on ScienceQA ‣ 4.3 Experimental
    Results ‣ 4 Experiment ‣ PILL:Plug Into LLM with Adapter Expert and Attention
    Gate"), we compare the performance of PILL with other multimodal methods using
    the LLaMA-7B model specification. Without pretraining, LLaMA-Adapter[[48](#bib.bib48)],
    which was one of the pioneering methods to employ PEFT techniques, enabled the
    LLM to possess visual recognition capabilities and achieved impressive results
    on ScienceQA[[27](#bib.bib27)] at that time. LAVIN[[28](#bib.bib28)] further improved
    upon this performance by approximately 4%. We speculate that the Adapter structure
    may be particularly suitable for VL-LLM, as evidenced by the experimental results
    of VL-Adapter[[43](#bib.bib43)]. On the other hand, since LLaVA[[25](#bib.bib25)]
    requires full fine-tuning, a well-initialized visual projection layer is likely
    to stabilize the subsequent fine-tuning process. As a result, LLaVA achieves a
    4% improvement with pretraining. However, SciTune[[15](#bib.bib15)], despite its
    focus on analyzing figures and tables in scientific papers, performs poorly on
    some common-sense VQA tasks. Importantly, our method consistently achieves the
    best results regardless of whether pretraining is employed, demonstrating the
    effectiveness of our proposed approach.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[2](#S4.T2 "表 2 ‣ 4.3.1 ScienceQA 结果 ‣ 4.3 实验结果 ‣ 4 实验 ‣ PILL:Plug Into LLM
    with Adapter Expert and Attention Gate")中，我们比较了使用LLaMA-7B模型规格的PILL与其他多模态方法的表现。在没有预训练的情况下，LLaMA-Adapter[[48](#bib.bib48)]作为一种开创性的PEFT技术应用方法，使LLM具备了视觉识别能力，并在当时的ScienceQA[[27](#bib.bib27)]中取得了令人印象深刻的结果。LAVIN[[28](#bib.bib28)]在此基础上进一步提高了约4%的性能。我们推测，Adapter结构可能特别适合VL-LLM，这一点通过VL-Adapter[[43](#bib.bib43)]的实验结果得到了证明。另一方面，由于LLaVA[[25](#bib.bib25)]需要完全微调，一个初始化良好的视觉投影层可能会稳定后续的微调过程。因此，LLaVA通过预训练实现了4%的改进。然而，尽管SciTune[[15](#bib.bib15)]专注于分析科学论文中的图表，但在一些常识VQA任务上的表现却不尽如人意。重要的是，无论是否使用预训练，我们的方法始终能取得最佳结果，证明了我们提出的方法的有效性。
- en: '![Refer to caption](img/25c8504a2831ea4b1fe5fa060e872969.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25c8504a2831ea4b1fe5fa060e872969.png)'
- en: 'Figure 4: Evolution of the absolute value of the attention gate at different
    layers of PILL'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：PILL中不同层级注意力门的绝对值演变
- en: As is shown in figure [4](#S4.F4 "Figure 4 ‣ 4.3.1 Results on ScienceQA ‣ 4.3
    Experimental Results ‣ 4 Experiment ‣ PILL:Plug Into LLM with Adapter Expert and
    Attention Gate"), We also plotted the absolute values of the MAG at different
    layers of the PILL model, which consists of 32 LM layers. It appears that all
    layers of the frozen LM stack utilize visual information to some extent. We also
    observed that the absolute values tend to increase with depth. We attribute this
    to the roles played by the LLM at different layers. In the layers closer to the
    input, the LLM focuses on language modeling and does not heavily rely on visual
    information. However, as the layers get closer to the output, the LLM has accumulated
    sufficient knowledge and needs to extract information from the images. As a result,
    the absolute values of the tanh gate activations start from near-zero values near
    the input layers and rapidly increase, approaching 1 near the output layers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[4](#S4.F4 "图 4 ‣ 4.3.1 ScienceQA 结果 ‣ 4.3 实验结果 ‣ 4 实验 ‣ PILL:Plug Into LLM
    with Adapter Expert and Attention Gate")所示，我们还绘制了PILL模型在不同层级上的MAG绝对值，PILL模型包含32层语言模型（LM）。结果显示，所有层级的冻结LM堆栈在一定程度上都利用了视觉信息。我们还观察到绝对值随着层级的深入而增加。我们将这一现象归因于LLM在不同层级所扮演的角色。在接近输入的层级，LLM专注于语言建模，并不依赖于视觉信息。然而，随着层级接近输出，LLM已经积累了足够的知识，并需要从图像中提取信息。因此，tanh门控激活的绝对值从接近输入层的接近零值开始，迅速增加，在输出层附近接近1。
- en: '| Settings | NAT | SOC | LAN | TXT | IMG | NO | G1-6 | G7-12 | Avg. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | NAT | SOC | LAN | TXT | IMG | NO | G1-6 | G7-12 | 平均值 |'
- en: '| w/o pretrain |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 无预训练 |'
- en: '| MoMAE + L-A | 89.25 | 91.79 | 87.00 | 88.51 | 85.57 | 89.48 | 89.79 | 88.13
    | 89.20 (+0.00) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MoMAE + L-A | 89.25 | 91.79 | 87.00 | 88.51 | 85.57 | 89.48 | 89.79 | 88.13
    | 89.20 (+0.00) |'
- en: '| MoMAE + MAG + L-A | 88.99 | 92.80 | 88.27 | 88.22 | 86.07 | 90.38 | 90.49
    | 88.00 | 89.60 (+0.40) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MoMAE + MAG + L-A | 88.99 | 92.80 | 88.27 | 88.22 | 86.07 | 90.38 | 90.49
    | 88.00 | 89.60 (+0.40) |'
- en: '| MoMAE + MAG + G-A | 89.83 | 90.55 | 88.36 | 89.20 | 86.51 | 89.97 | 90.60
    | 87.80 | 89.60 (+0.40) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| MoMAE + MAG + G-A | 89.83 | 90.55 | 88.36 | 89.20 | 86.51 | 89.97 | 90.60
    | 87.80 | 89.60 (+0.40) |'
- en: '| MoMAE + MAG + SG-A | 90.01 | 93.36 | 88.18 | 89.15 | 87.16 | 90.59 | 91.12
    | 88.66 | 90.24 (+1.04) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MoMAE + MAG + SG-A | 90.01 | 93.36 | 88.18 | 89.15 | 87.16 | 90.59 | 91.12
    | 88.66 | 90.24 (+1.04) |'
- en: '| MoMAE + MAG + SG-A + 0.1 wrong answer | 90.19 | 93.36 | 89.09 | 89.30 | 87.41
    | 91.22 | 91.45 | 88.99 | 90.57 (+1.37) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| MoMAE + MAG + SG-A + 0.1 错误答案 | 90.19 | 93.36 | 89.09 | 89.30 | 87.41 | 91.22
    | 91.45 | 88.99 | 90.57 (+1.37) |'
- en: '| w pretrain epochs=1 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| w 预训练轮次=1 |'
- en: '| MAG + SG-A | 90.36 | 94.38 | 87.45 | 89.83 | 88.10 | 89.83 | 90.90 | 89.65
    | 90.45 (+1.25) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| MAG + SG-A | 90.36 | 94.38 | 87.45 | 89.83 | 88.10 | 89.83 | 90.90 | 89.65
    | 90.45 (+1.25) |'
- en: '| MoMAE + MAG + SG-A | 89.96 | 95.05 | 88.82 | 89.30 | 88.25 | 90.73 | 91.48
    | 89.39 | 90.73 (+1.53) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MoMAE + MAG + SG-A | 89.96 | 95.05 | 88.82 | 89.30 | 88.25 | 90.73 | 91.48
    | 89.39 | 90.73 (+1.53) |'
- en: '| MoMAE + MAG + SG-A + 0.1 wrong answer | 89.43 | 95.61 | 89.09 | 88.66 | 87.85
    | 91.01 | 91.34 | 89.39 | 90.64 (+1.44) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MoMAE + MAG + SG-A + 0.1 错误答案 | 89.43 | 95.61 | 89.09 | 88.66 | 87.85 | 91.01
    | 91.34 | 89.39 | 90.64 (+1.44) |'
- en: '| w pretrain epochs=3 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| w 预训练轮次=3 |'
- en: '| MoMAE + MAG + SG-A | 90.36 | 95.84 | 89.36 | 89.93 | 88.80 | 91.15 | 92.33
    | 89.32 | 91.23 (+2.05) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MoMAE + MAG + SG-A | 90.36 | 95.84 | 89.36 | 89.93 | 88.80 | 91.15 | 92.33
    | 89.32 | 91.23 (+2.05) |'
- en: 'Table 3: Ablation studies on ScienceQA test set. “L-a” denotes Linear Adapter.
    “G-a” denotes GELU Adapter. “SG-A” denotes SwiGLU Adapter. “0.1 wrong answer”
    indicates that during the training process, we randomly replace the ground truth
    answer with another option with a probability of 0.1.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在ScienceQA测试集上的消融研究。“L-a”表示线性适配器。“G-a”表示GELU适配器。“SG-A”表示SwiGLU适配器。“0.1 错误答案”表示在训练过程中，我们以0.1的概率随机将真实答案替换为其他选项。
- en: Ablation study. In Table  [3](#S4.T3 "Table 3 ‣ 4.3.1 Results on ScienceQA ‣
    4.3 Experimental Results ‣ 4 Experiment ‣ PILL:Plug Into LLM with Adapter Expert
    and Attention Gate"), we present the contributions of various components and training
    processes of PILL on the ScienceQA test set. MoMAE and Linear Adapter serve as
    our baselines, achieving an accuracy of 89.20%. Building upon this baseline, we
    incorporate the MAG module, resulting in a performance improvement of 0.4%. To
    investigate the impact of different activation functions on the Adapter, we initially
    apply the GELU activation function, which does not lead to any improvement. However,
    when we replace the activation function with our proposed SwiGLU activation function,
    we achieve a further improvement of 0.64%. Additionally, inspired by [[2](#bib.bib2)],
    we explore the effect of LLM hallucination on our model. During training, we randomly
    replace the answer with another option with a probability of 0.1, leading to an
    additional improvement of 0.33%.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究。在表格 [3](#S4.T3 "Table 3 ‣ 4.3.1 Results on ScienceQA ‣ 4.3 Experimental
    Results ‣ 4 Experiment ‣ PILL:Plug Into LLM with Adapter Expert and Attention
    Gate") 中，我们展示了PILL在ScienceQA测试集上的各种组件和训练过程的贡献。MoMAE和线性适配器作为我们的基线，准确率为89.20%。在此基线基础上，我们加入了MAG模块，性能提高了0.4%。为了研究不同激活函数对适配器的影响，我们最初应用了GELU激活函数，但没有带来改进。然而，当我们将激活函数替换为我们提出的SwiGLU激活函数时，进一步提高了0.64%。此外，受[[2](#bib.bib2)]的启发，我们探讨了LLM幻觉对模型的影响。在训练过程中，我们以0.1的概率随机将答案替换为另一个选项，进一步提高了0.33%。
- en: To further enhance PILL’s ability to recognize images, we conducted image captioning
    training on the model. Initially, we attempted to remove the V-Adapter, training
    only a projection layer during pretraining and using a single adapter for fine-tuning.
    This approach achieved an accuracy of 90.45%. When incorporating the MoMAE method,
    we observed an additional improvement of 0.28%. We also explored the use of random
    wrong answers but encountered a decline in performance. We believe this is due
    to the lack of pretraining, which could result in insufficient image recognition
    capabilities and lead to hallucinations, similar to the findings in MM-COT[[49](#bib.bib49)].
    After pretraining, the model exhibits improved image understanding, partially
    mitigating the hallucination issue. Therefore, employing the random wrong answer
    approach at this stage resulted in a decline in performance. Finally, by increasing
    the number of pretraining epochs to 3, we achieved the best results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提升PILL的图像识别能力，我们在模型上进行了图像描述训练。最初，我们尝试去除V-Adapter，仅在预训练阶段训练投影层，并使用单个适配器进行微调。这种方法达到了90.45%的准确率。结合MoMAE方法后，我们观察到额外提高了0.28%。我们还探索了使用随机错误答案的方法，但性能下降。我们认为这是由于缺乏预训练，可能导致图像识别能力不足，从而引发幻觉现象，类似于MM-COT中的发现[[49](#bib.bib49)]。经过预训练后，模型展现了更好的图像理解能力，部分缓解了幻觉问题。因此，在这个阶段使用随机错误答案的方法导致了性能下降。最终，通过将预训练轮次增加到3，我们达到了最佳结果。
- en: 5 Limitation
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 限制
- en: Due to our utilization of LLM as the underlying model, PILL inevitably inherits
    some of the limitations of LLM. As a result, the image-to-text generation process
    may yield unsatisfactory results, including erroneous knowledge and reasoning
    or hallucinations. Additionally, although our work only involves the fusion of
    the image modality, our method can naturally incorporate other modalities such
    as speech. In future work, we aspire to create a genuine multimodal AI assistant
    by further exploring the integration of multiple modalities and scaling up the
    LLM.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了LLM作为基础模型，PILL不可避免地继承了LLM的一些局限性。因此，图像到文本生成过程可能会产生不满意的结果，包括错误的知识和推理或幻觉。此外，虽然我们的工作仅涉及图像模态的融合，但我们的方法可以自然地融入其他模态，例如语音。在未来的工作中，我们希望通过进一步探索多模态的集成和扩大LLM的规模，创造一个真正的多模态AI助手。
- en: 6 Conclusion
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we propose the PILL method to address the challenge of decoupling
    complex multimodal interactions. We leverage the MoMAE (Mixture-of-Modality-Adapter-Expert)
    module, which is specifically designed to handle image tokens, and the MAG (Modality-Attention-Gating)
    module for dynamic fusion of modalities. Additionally, we introduce the SwiGLU-Adapter
    to further enhance performance. Our experimental results demonstrate the effectiveness
    of our proposed method. With the advantage of fine-tuning only a small number
    of parameters, our approach offers a cost-effective solution that can be trained
    on a single A6000 GPU. This enables us to achieve visual language instruction
    following capabilities while maintaining efficient computation and training speed.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了PILL方法，以应对复杂多模态交互的解耦挑战。我们利用了MoMAE（混合模态适配器专家）模块，专门设计用于处理图像令牌，以及MAG（模态注意力门控）模块用于动态融合模态。此外，我们引入了SwiGLU-Adapter以进一步提升性能。我们的实验结果展示了我们提出方法的有效性。凭借仅需微调少量参数的优势，我们的方法提供了一种经济高效的解决方案，可以在单个A6000
    GPU上训练。这使我们能够实现视觉语言指令跟随能力，同时保持高效的计算和训练速度。
- en: References
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti
    Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. VLMo: Unified vision-language
    pre-training with mixture-of-modality-experts. In Advances in Neural Information
    Processing Systems, 2022.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti
    Aggarwal, Subhojit Som, Songhao Piao, 和 Furu Wei. VLMo: 通过混合模态专家的统一视觉语言预训练。在神经信息处理系统进展，2022年。'
- en: '[2] Ali Furkan Biten, Lluís Gómez, and Dimosthenis Karatzas. Let there be a
    clock on the beach: Reducing object hallucination in image captioning. In Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1381–1390,
    2022.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Ali Furkan Biten, Lluís Gómez, 和 Dimosthenis Karatzas. 让海滩上有一个时钟：减少图像描述中的对象幻觉。在IEEE/CVF冬季计算机视觉应用会议论文集，页面1381–1390，2022年。'
- en: '[3] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E
    Hinton. Big self-supervised models are strong semi-supervised learners. Advances
    in neural information processing systems (NeurIPS), 33:22243–22255, 2020.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, 和 Geoffrey
    E Hinton. 大型自监督模型是强大的半监督学习者。神经信息处理系统进展（NeurIPS），33:22243–22255，2020年。'
- en: '[4] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski,
    Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al.
    Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794,
    2022.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski,
    Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer等. Pali:
    一种联合缩放的多语言图像模型。arXiv 预印本 arXiv:2209.06794，2022年。'
- en: '[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann等. Palm: 通过路径扩展语言建模。arXiv 预印本 arXiv:2204.02311，2022年。'
- en: '[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
    language models. arXiv preprint arXiv:2210.11416, 2022.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma等. 扩展指令微调语言模型。arXiv 预印本
    arXiv:2210.11416，2022年。'
- en: '[7] Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji Zhang, Meng Wang, and
    Jun Yu. Rosita: Enhancing vision-and-language semantic alignments via cross- and
    intra-modal knowledge integration. In Proceedings of the 29th ACM International
    Conference on Multimedia, 2021.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Yuhao Cui、Zhou Yu、Chunqi Wang、Zhongzhou Zhao、Ji Zhang、Meng Wang 和 Jun Yu。Rosita：通过跨模态和内部模态知识整合增强视觉与语言的语义对齐。在第29届
    ACM 国际多媒体会议上，2021 年。'
- en: '[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,
    Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards
    general-purpose vision-language models with instruction tuning, 2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Wenliang Dai、Junnan Li、Dongxu Li、Anthony Meng Huat Tiong、Junqi Zhao、Weisheng
    Wang、Boyang Li、Pascale Fung 和 Steven Hoi。Instructblip：朝着具有指令调整的通用视觉语言模型，2023 年。'
- en: '[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929, 2020.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Alexey Dosovitskiy、Lucas Beyer、Alexander Kolesnikov、Dirk Weissenborn、Xiaohua
    Zhai、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg Heigold、Sylvain
    Gelly 等。一张图像价值 16x16 个词：用于大规模图像识别的变换器。arXiv 预印本 arXiv:2010.11929，2020 年。'
- en: '[10] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang,
    and Jie Tang. Glm: General language model pretraining with autoregressive blank
    infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 320–335, 2022.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zhengxiao Du、Yujie Qian、Xiao Liu、Ming Ding、Jiezhong Qiu、Zhilin Yang 和
    Jie Tang。Glm：具有自回归空白填充的通用语言模型预训练。在第60届计算语言学协会年会上（第1卷：长论文），页码 320–335，2022 年。'
- en: '[11] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu,
    and Anette Frank. Magma–multimodal augmentation of generative models through adapter-based
    finetuning. arXiv preprint arXiv:2112.05253, 2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Constantin Eichenberg、Sidney Black、Samuel Weinbach、Letitia Parcalabescu
    和 Anette Frank。Magma–通过基于适配器的微调增强生成模型的多模态增强。arXiv 预印本 arXiv:2112.05253，2021 年。'
- en: '[12] Jean-Baptiste Alayrac et al. Flamingo: a visual language model for few-shot
    learning. 2022.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Jean-Baptiste Alayrac 等。Flamingo：一种用于少样本学习的视觉语言模型。2022 年。'
- en: '[13] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou,
    Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter
    v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010,
    2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Peng Gao、Jiaming Han、Renrui Zhang、Ziyi Lin、Shijie Geng、Aojun Zhou、Wei
    Zhang、Pan Lu、Conghui He、Xiangyu Yue、Hongsheng Li 和 Yu Qiao。Llama-adapter v2：参数高效的视觉指令模型。arXiv
    预印本 arXiv:2304.15010，2023 年。'
- en: '[14] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,
    Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and
    language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Tao Gong、Chengqi Lyu、Shilong Zhang、Yudong Wang、Miao Zheng、Qian Zhao、Kuikun
    Liu、Wenwei Zhang、Ping Luo 和 Kai Chen。Multimodal-gpt：一种用于与人类对话的视觉和语言模型。arXiv 预印本
    arXiv:2305.04790，2023 年。'
- en: '[15] Sameera Horawalavithana, Sai Munikoti, Ian Stewart, and Henry Kvinge.
    Scitune: Aligning large language models with scientific multimodal instructions,
    2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Sameera Horawalavithana、Sai Munikoti、Ian Stewart 和 Henry Kvinge。Scitune：将大型语言模型与科学多模态指令对齐，2023
    年。'
- en: '[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
    De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient
    transfer learning for nlp. In International Conference on Machine Learning, pages
    2790–2799\. PMLR, 2019.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Neil Houlsby、Andrei Giurgiu、Stanislaw Jastrzebski、Bruna Morrone、Quentin
    De Laroussilhe、Andrea Gesmundo、Mona Attariyan 和 Sylvain Gelly。针对 NLP 的参数高效迁移学习。在国际机器学习会议上，页码
    2790–2799。PMLR，2019 年。'
- en: '[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean
    Wang、Lu Wang 和 Weizhu Chen。Lora：大语言模型的低秩适配。arXiv 预印本 arXiv:2106.09685，2021 年。'
- en: '[18] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming
    Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not
    all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045,
    2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Shaohan Huang、Li Dong、Wenhui Wang、Yaru Hao、Saksham Singhal、Shuming Ma、Tengchao
    Lv、Lei Cui、Owais Khan Mohammed、Qiang Liu 等。语言并非你所需的一切：将感知与语言模型对齐。arXiv 预印本 arXiv:2302.14045，2023
    年。'
- en: '[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping
    language-image pre-training with frozen image encoders and large language models.
    In ICML, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Junnan Li、Dongxu Li、Silvio Savarese 和 Steven Hoi。BLIP-2：通过冻结的图像编码器和大型语言模型进行语言-图像预训练的引导。在
    ICML 上，2023 年。'
- en: '[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping
    language-image pre-training for unified vision-language understanding and generation.
    In International Conference on Machine Learning, pages 12888–12900\. PMLR, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Junnan Li, Dongxu Li, Caiming Xiong, 和 Steven Hoi. Blip: 用于统一视觉-语言理解和生成的自举语言-图像预训练。在国际机器学习大会上，页
    12888–12900。PMLR，2022。'
- en: '[21] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming
    Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation
    learning with momentum distillation. Advances in neural information processing
    systems, 34:9694–9705, 2021.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming
    Xiong, 和 Steven Chu Hong Hoi. 对齐再融合：带有动量蒸馏的视觉和语言表示学习。神经信息处理系统进展，34:9694–9705，2021。'
- en: '[22] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
    for generation, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Xiang Lisa Li 和 Percy Liang. Prefix-tuning: 优化生成的连续提示，2021。'
- en: '[23] Yunshui Li, Binyuan Hui, ZhiChao Yin, Min Yang, Fei Huang, and Yongbin
    Li. Pace: Unified multi-modal dialogue pre-training with progressive and compositional
    experts. arXiv preprint arXiv:2305.14839, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yunshui Li, Binyuan Hui, ZhiChao Yin, Min Yang, Fei Huang, 和 Yongbin Li.
    Pace: 具有渐进式和组合专家的统一多模态对话预训练。arXiv 预印本 arXiv:2305.14839，2023。'
- en: '[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines
    with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Haotian Liu, Chunyuan Li, Yuheng Li, 和 Yong Jae Lee. 改进的基准与视觉指令微调。arXiv
    预印本 arXiv:2310.03744，2023。'
- en: '[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee. 视觉指令微调，2023。'
- en: '[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining
    task-agnostic visiolinguistic representations for vision-and-language tasks. In
    Advances in Neural Information Processing Systems, pages 13–23, 2019.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Jiasen Lu, Dhruv Batra, Devi Parikh, 和 Stefan Lee. Vilbert: 预训练任务无关的视觉语言表示，用于视觉和语言任务。在神经信息处理系统进展中，页
    13–23，2019。'
- en: '[27] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun
    Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal
    reasoning via thought chains for science question answering. In The 36th Conference
    on Neural Information Processing Systems (NeurIPS), 2022.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun
    Zhu, Oyvind Tafjord, Peter Clark, 和 Ashwin Kalyan. 学会解释：通过思维链进行多模态推理以回答科学问题。在第36届神经信息处理系统会议（NeurIPS）上，2022。'
- en: '[28] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong
    Ji. Cheap and quick: Efficient vision-language instruction tuning for large language
    models. arXiv preprint arXiv:2305.15023, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, 和 Rongrong
    Ji. 便宜且快速：针对大型语言模型的高效视觉-语言指令微调。arXiv 预印本 arXiv:2305.15023，2023。'
- en: '[29] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang,
    Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al. Kosmos-2.5: A multimodal
    literate model. arXiv preprint arXiv:2309.11419, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang,
    Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, 等. Kosmos-2.5: 一个多模态素养模型。arXiv
    预印本 arXiv:2309.11419，2023。'
- en: '[30] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image
    captioning. arXiv preprint arXiv:2111.09734, 2021.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Ron Mokady, Amir Hertz, 和 Amit H Bermano. Clipcap: 用于图像描述的 Clip 前缀。arXiv
    预印本 arXiv:2111.09734，2021。'
- en: '[31] OpenAI. Chatgpt, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] OpenAI. Chatgpt，2023。'
- en: '[32] OpenAI. Gpt-4 technical report, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] OpenAI. Gpt-4 技术报告，2023。'
- en: '[33] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming
    Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the
    world. arXiv preprint arXiv:2306.14824, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming
    Ma, 和 Furu Wei. Kosmos-2: 将多模态大型语言模型与现实世界对接。arXiv 预印本 arXiv:2306.14824，2023。'
- en: '[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
    Learning transferable visual models from natural language supervision. In International
    conference on machine learning, pages 8748–8763\. PMLR, 2021.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, 等.
    从自然语言监督中学习可转移的视觉模型。在国际机器学习大会上，页 8748–8763。PMLR，2021。'
- en: '[35] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202,
    2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Noam Shazeer. Glu 变体改进变换器。arXiv 预印本 arXiv:2002.05202，2020。'
- en: '[36] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng
    Dai. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv
    preprint arXiv:1908.08530, 2019.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, 和 Jifeng Dai.
    Vl-bert: 通用视觉-语言表示的预训练。arXiv 预印本 arXiv:1908.08530，2019年。'
- en: '[37] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations
    from transformers. arXiv preprint arXiv:1908.07490, 2019.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Hao Tan 和 Mohit Bansal. Lxmert: 从变换器学习跨模态编码表示。arXiv 预印本 arXiv:1908.07490，2019年。'
- en: '[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等。Llama: 开放和高效的基础语言模型。arXiv 预印本 arXiv:2302.13971，2023年。'
- en: '[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等。Llama 2: 开放的基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288，2023年。'
- en: '[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力即你所需。神经信息处理系统进展，第30卷，2017年。'
- en: '[41] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu,
    Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image
    as a foreign language: Beit pretraining for all vision and vision-language tasks.
    arXiv preprint arXiv:2208.10442, 2022.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu,
    Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som 等。图像作为外语: Beit
    预训练用于所有视觉和视觉-语言任务。arXiv 预印本 arXiv:2208.10442，2022年。'
- en: '[42] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
    Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers
    large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
    Wang, Anwen Hu, Pengcheng Shi, Yaya Shi 等。mplug-owl: 模块化赋能大语言模型的多模态能力。arXiv 预印本
    arXiv:2304.14178，2023年。'
- en: '[43] Mohit Bansal Yi-Lin Sung, Jaemin Cho. Vl-adapter: Parameter-efficient
    transfer learning for vision-and-language tasks. In CVPR, 2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Mohit Bansal, Yi-Lin Sung, 和 Jaemin Cho. Vl-adapter: 针对视觉和语言任务的参数高效迁移学习。在
    CVPR，2022年。'
- en: '[44] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai
    Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et al. Lamm: Language-assisted
    multi-modal instruction-tuning dataset, framework, and benchmark. arXiv preprint
    arXiv:2306.06687, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai
    Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang 等。Lamm: 语言辅助的多模态指令调优数据集、框架和基准。arXiv
    预印本 arXiv:2306.06687，2023年。'
- en: '[45] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini,
    and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models.
    arXiv preprint arXiv:2205.01917, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini,
    和 Yonghui Wu. Coca: 对比性标题生成器是图像-文本基础模型。arXiv 预印本 arXiv:2205.01917，2022年。'
- en: '[46] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention
    networks for visual question answering. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pages 6281–6290, 2019.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, 和 Qi Tian. 深度模块化协同注意网络用于视觉问答。在
    IEEE 计算机视觉与模式识别会议（CVPR）论文集，页6281–6290，2019年。'
- en: '[47] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei,
    Yuchen Zhang, and Tao Kong. What matters in training a gpt4-style language model
    with multimodal inputs? arXiv preprint arXiv:2307.02469, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei,
    Yuchen Zhang, 和 Tao Kong. 在使用多模态输入训练 GPT-4 风格语言模型时，什么因素最为重要？arXiv 预印本 arXiv:2307.02469，2023年。'
- en: '[48] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning
    of language models with zero-init attention. arXiv preprint arXiv:2303.16199,
    2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, 和 Yu Qiao. Llama-adapter: 使用零初始化注意力的高效语言模型微调。arXiv
    预印本 arXiv:2303.16199，2023年。'
- en: '[49] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex
    Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint
    arXiv:2302.00923, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 张卓生、张阿斯顿、穆李、赵海和乔治·卡里皮斯。语言模型中的多模态链式思维推理。arXiv 预印本 arXiv:2302.00923，2023年。'
- en: '[50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:
    Enhancing vision-language understanding with advanced large language models. arXiv
    preprint arXiv:2304.10592, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 朱德尧、陈军、沈晓前、李翔和穆罕默德·艾尔霍塞尼。Minigpt-4：通过先进的大型语言模型增强视觉-语言理解。arXiv 预印本 arXiv:2304.10592，2023年。'
