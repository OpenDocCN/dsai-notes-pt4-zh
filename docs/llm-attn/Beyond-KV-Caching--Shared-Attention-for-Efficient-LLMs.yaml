- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:52:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:52:29'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Beyond KV Caching: Shared Attention for Efficient LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越KV缓存：用于高效LLMs的共享注意力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12866](https://ar5iv.labs.arxiv.org/html/2407.12866)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12866](https://ar5iv.labs.arxiv.org/html/2407.12866)
- en: Liao Bingli¹, Danilo Vasconcellos Vargas¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 廖冰立¹，达尼洛·瓦斯孔塞洛斯·瓦尔加斯¹
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The efficiency of large language models (LLMs) remains a critical challenge,
    particularly in contexts where computational resources are limited. Traditional
    attention mechanisms in these models, while powerful, require significant computational
    and memory resources due to the necessity of recalculating and storing attention
    weights across different layers. This paper introduces a novel Shared Attention
    (SA) mechanism, designed to enhance the efficiency of LLMs by directly sharing
    computed attention weights across multiple layers. Unlike previous methods that
    focus on sharing intermediate Key-Value (KV) caches, our approach utilizes the
    isotropic tendencies of attention distributions observed in advanced LLMs post-pretraining
    to reduce both the computational flops and the size of the KV cache required during
    inference. We empirically demonstrate that implementing SA across various LLMs
    results in minimal accuracy loss on standard benchmarks. Our findings suggest
    that SA not only conserves computational resources but also maintains robust model
    performance, thereby facilitating the deployment of more efficient LLMs in resource-constrained
    environments. Code: https://github.com/metacarbon/shareAtt'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）的效率仍然是一个关键挑战，特别是在计算资源有限的情况下。这些模型中的传统注意力机制虽然强大，但由于需要重新计算和存储跨不同层的注意力权重，要求大量的计算和内存资源。本文介绍了一种新颖的共享注意力（SA）机制，旨在通过直接在多个层之间共享计算得到的注意力权重来提高LLMs的效率。与以往专注于共享中间键值（KV）缓存的方法不同，我们的方法利用了在高级LLMs预训练后观察到的注意力分布的各向同性趋势，以减少推理过程中所需的计算量和KV缓存的大小。我们通过实验证明，在各种LLMs中实现SA会导致标准基准测试上的准确性损失最小。我们的发现表明，SA不仅节省了计算资源，还保持了模型的强大性能，从而促进了在资源受限环境中部署更高效的LLMs。代码:
    https://github.com/metacarbon/shareAtt'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: The rapid growth of large language models (LLM) has brought forth significant
    challenges in terms of computational and memory efficiency during inference. Traditional
    approaches, such as Multi-Query Attention (MQA) (Shazeer [2019](#bib.bib13)) and
    Grouped-Query Attention (GQA) (Ainslie et al. [2023](#bib.bib1)), have made strides
    in reducing the key-value (KV) cache size by sharing keys and values across multiple
    heads within a layer. More recently, Cross-Layer Attention (CLA) has extended
    this concept by sharing keys and values across adjacent layers, further reducing
    memory requirements without substantially impacting model performance (Brandon
    et al. [2024](#bib.bib3)). Despite these advancements, the need for more efficient
    methods continues to grow, particularly as models scale and are deployed in resource-constrained
    environments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）的快速增长在推理过程中带来了计算和内存效率方面的重大挑战。传统方法，如多查询注意力（MQA）（Shazeer [2019](#bib.bib13)）和分组查询注意力（GQA）（Ainslie
    et al. [2023](#bib.bib1)），通过在层内的多个头之间共享键和值，已经在减少键值（KV）缓存大小方面取得了进展。最近，跨层注意力（CLA）通过在相邻层之间共享键和值，进一步减少了内存需求而没有显著影响模型性能（Brandon
    et al. [2024](#bib.bib3)）。尽管这些进展存在，但随着模型规模的扩大和在资源受限环境中的部署，对更高效方法的需求仍在增长。
- en: In this paper, we introduce a novel method termed Shared Attention (SA), which
    significantly reduces the KV cache requirements and computational load during
    inference for LLMs. Unlike previous methods that focused on sharing KV caches
    either within the same layer or between adjacent layers, our approach inspired
    by the inherent similarity of attention weights distribution across layers, and
    sharing these weights directly could further reduce the need for repeated key
    and value computations. This innovative approach not only reduces the KV cache
    size but also circumvents the need for the computationally expensive softmax operation,
    leading to a more efficient inference process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们引入了一种新颖的方法，称为共享注意力（SA），该方法显著减少了LLMs在推理期间对KV缓存的需求和计算负担。与以前集中于在同一层内或相邻层之间共享KV缓存的方法不同，我们的方法受到跨层注意力权重分布固有相似性的启发，直接共享这些权重可以进一步减少重复计算键和值的需求。这一创新方法不仅减少了KV缓存的大小，还避免了计算开销较大的softmax操作，从而使推理过程更高效。
- en: 'The key contributions of our work are summarized as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献总结如下：
- en: '1.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose a novel Shared Attention mechanism that reduces computational and
    memory overhead by directly sharing pre-computed attention weights across multiple
    layers in LLMs.
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的**共享注意力**机制，通过在大型语言模型（LLMs）中跨多个层直接共享预计算的注意力权重，减少了计算和内存开销。
- en: '2.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We empirically validate the effectiveness of Shared Attention by implementing
    it across various benchmarks and demonstrate that it achieves comparable accuracy.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过在各种基准测试中实现共享注意力，经验性地验证了其有效性，并展示了它达到了可比的准确性。
- en: '3.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Our analysis of attention isotropy across pretrained LLMs provides insights
    into how attention mechanisms stabilize and become more uniform across layers
    as training progresses. This understanding informs the optimal layer ranges for
    applying Shared Attention.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对预训练LLMs中注意力各向同性的分析提供了关于注意力机制如何在训练过程中稳定并在层间变得更加均匀的见解。这一理解为应用共享注意力的最佳层范围提供了指导。
- en: Shared Attention
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共享注意力
- en: In this section we demonstrate motivation, Shared Attention (SA) method, and
    the comparison to existed KV-sharing mechanisms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了动机、共享注意力（SA）方法及其与现有KV共享机制的比较。
- en: '![Refer to caption](img/041b3e3ed4dd89c3e02b7d78e2f5e148.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/041b3e3ed4dd89c3e02b7d78e2f5e148.png)'
- en: 'Figure 1: Illustration of various sharing algorithms. The MQA and GQA methods
    share the Key and Value caches with the Query within the same layer to reduce
    memory usage. The CLA method extends this by sharing the Key and Value caches
    across different layers. Our method, Shared Attention, advances this concept further
    by sharing the attention weights across multiple layers.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：各种共享算法的示意图。MQA和GQA方法在同一层内共享查询的键和值缓存，以减少内存使用。CLA方法通过在不同层之间共享键和值缓存扩展了这一点。我们的方法——共享注意力——进一步推动了这一概念，通过跨多个层共享注意力权重。
- en: Motivation
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动机
- en: The self-attention mechanism in transformer models is typically defined as $\text{softmax}(\frac{QK^{T}}{\sqrt{d}})V$,
    where $Q$, $K$, and $V$ represent the query, key, and value matrices respectively,
    and $d$ is the dimension of the key vectors. This formulation necessitates the
    recomputation of attention weights at each layer, a computationally intensive
    task, particularly when the model is deployed in inference mode. To mitigate this,
    the concept of a KV-cache is employed, reducing the need to recompute $K$ and
    $V$ matrices for previously encountered tokens.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型中的自注意力机制通常定义为$\text{softmax}(\frac{QK^{T}}{\sqrt{d}})V$，其中$Q$、$K$和$V$分别表示查询、键和值矩阵，$d$是键向量的维度。该公式需要在每一层重新计算注意力权重，这是一项计算密集型任务，特别是在模型部署于推理模式时。为了缓解这一问题，引入了KV缓存的概念，减少了对已遇到的令牌重新计算$K$和$V$矩阵的需求。
- en: 'While prior methodologies have focused on sharing KV caches at different levels
    to minimize memory overhead, they predominantly operate under the assumption that
    attention weights differ significantly across layers, thereby necessitating individual
    computations to capture diverse contextual dependencies effectively. This assumption
    prompts a critical inquiry: Are the attention weights indeed markedly different
    across layers, or is this variation minimal enough to allow for a unified approach
    across multiple layers?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然先前的方法集中于在不同层级共享 KV 缓存以减少内存开销，但它们主要基于注意力权重在各层之间存在显著差异的假设，因此需要单独计算以有效捕捉不同的上下文依赖。这一假设引发了一个关键问题：注意力权重在各层之间确实存在明显差异，还是这种变化足够小，可以在多个层之间采用统一的方法？
- en: To explore this, we conducted an empirical analysis on the distribution of attention
    weights across different layers of the model. Based on the Llama2-7B-chat model,
    we processed the Massive Multitask Language Understanding (MMLU) dataset (Hendrycks
    et al. [2020](#bib.bib6)) to extract the attention matrices, $\text{softmax}(\frac{QK^{T}}{\sqrt{d}})$,
    for each layer. Given the variability in sequence lengths, we standardized these
    matrices to a uniform size by applying zero-padding to align them to a consistent
    shape of $\text{maxlen}\times\text{maxlen}$.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探讨这一点，我们对模型中不同层的注意力权重分布进行了实证分析。基于 Llama2-7B-chat 模型，我们处理了大规模多任务语言理解（MMLU）数据集（Hendrycks
    et al. [2020](#bib.bib6)），以提取每层的注意力矩阵，$\text{softmax}(\frac{QK^{T}}{\sqrt{d}})$。由于序列长度的变异性，我们通过应用零填充将这些矩阵标准化为统一的尺寸，以对齐到一致的
    $\text{maxlen}\times\text{maxlen}$ 形状。
- en: Our analysis employed the cosine similarity metric to compare the attention
    matrices of all layers, revealing a notable high degree of similarity across most
    of layers, particularly from indices 3 to 30\. Contrastingly, the initial layers
    (0 and 1) and the final output layer (31) exhibited substantially lower similarity
    scores to middle layers. This observation is intuitive as the early layers are
    closer to the input token embeddings, requiring frequent adjustments to their
    attention distribution to accurately abstract semantic meanings from diverse inputs.
    Similarly, the final layer’s unique role in predicting the next token justifies
    its distinct attention pattern.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析使用了余弦相似度指标来比较所有层的注意力矩阵，结果显示大多数层之间存在显著的高相似度，特别是在索引 3 到 30 之间。相反，初始层（0 和
    1）以及最终输出层（31）与中间层的相似度得分显著较低。这一观察结果是直观的，因为早期层更接近输入令牌嵌入，需要频繁调整它们的注意力分布，以准确地从不同的输入中抽象语义。类似地，最终层在预测下一个令牌中的独特角色也解释了其独特的注意力模式。
- en: Inspired by these findings, we hypothesize that the high similarity in attention
    weights across the majority of layers could allow for a shared representation
    of these weights, thus eliminating the need for separate softmax computations
    in each layer and reducing the key cache size. Such a strategy could not only
    streamline the inference process but also enhance computational efficiency significantly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些发现，我们假设大多数层之间的高相似度可能允许这些权重的共享表示，从而消除每层单独进行 softmax 计算的需要，并减少关键缓存的大小。这种策略不仅可以简化推理过程，还可以显著提高计算效率。
- en: 'Based on the observed uniformity in attention weights, we propose a novel algorithm
    as shown in Algorithm [1](#alg1 "Algorithm 1 ‣ Motivation ‣ Shared Attention ‣
    Beyond KV Caching: Shared Attention for Efficient LLMs"), Shared Attention, which
    utilizes a single shared attention matrix across multiple layers. This approach
    fundamentally redefines the operational paradigm by maintaining a consistent attention
    mechanism across various contextual layers, thereby reducing redundancy and enhancing
    inference speed.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '基于观察到的注意力权重的均匀性，我们提出了一种新颖的算法，如算法 [1](#alg1 "Algorithm 1 ‣ Motivation ‣ Shared
    Attention ‣ Beyond KV Caching: Shared Attention for Efficient LLMs")所示，即共享注意力，该算法在多个层之间使用单一共享的注意力矩阵。这种方法从根本上重新定义了操作范式，通过在不同的上下文层之间保持一致的注意力机制，从而减少了冗余并提高了推理速度。'
- en: Algorithm 1 Shared Attention Algorithm
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 共享注意力算法
- en: 'Input: Set of layers $L$, input tokens $X$'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：层集合 $L$，输入令牌 $X$
- en: 'Parameters: Attention span $S$ (e.g., layers 23 to 30)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：注意力范围 $S$（例如，层 23 到 30）
- en: 'Output: Updated attention weights across specified layers'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：指定层的更新注意力权重
- en: 1:  Initialize attention weights $A\leftarrow\emptyset$2:  for each layer $l\in
    S$ do3:     if first layer in $S$ then4:        Compute initial attention weights
    $A_{l}\leftarrow\text{softmax}(\frac{Q_{l}K_{l}^{T}}{\sqrt{d_{k}}})$5:        Set
    $A\leftarrow A_{l}$6:     else7:        Share attention weights $A_{l}\leftarrow
    A$8:     end if9:     Apply shared attention to compute outputs $O_{l}\leftarrow
    A_{l}\cdot V_{l}$10:  end for11:  Adjust subsequent layers’ inputs using outputs
    from $S$12:  Continue processing remaining layers with standard attention13:  return
    Final output after processing all layers![Refer to caption](img/e3a005dc9a9dd42d1d02a6dd8fee59f7.png)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  初始化注意力权重 $A\leftarrow\emptyset$2:  对于每一层 $l\in S$，执行以下操作3:      如果是 $S$
    中的第一层4:        计算初始注意力权重 $A_{l}\leftarrow\text{softmax}(\frac{Q_{l}K_{l}^{T}}{\sqrt{d_{k}}})$5:        设置
    $A\leftarrow A_{l}$6:      否则7:        共享注意力权重 $A_{l}\leftarrow A$8:      结束 if9:      应用共享注意力计算输出
    $O_{l}\leftarrow A_{l}\cdot V_{l}$10:  结束 for11:  使用 $S$ 的输出调整后续层的输入12:  使用标准注意力继续处理剩余层13:  返回处理所有层后的最终输出![参考图片](img/e3a005dc9a9dd42d1d02a6dd8fee59f7.png)'
- en: 'Figure 2: Layer-wise similarity of attention weights across various LLMs. The
    x-axis and y-axis represent the layer indices, while the z-axis depicts the cosine
    similarity values. The distinct similarity patterns are indicative of the specific
    functional roles each group of layers plays within the overall architecture.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 各种 LLM 中的注意力权重的层级相似性。x 轴和 y 轴表示层级索引，而 z 轴则显示余弦相似性值。不同的相似性模式表明每组层在整体架构中扮演的特定功能角色。'
- en: Comparison with Existing Approaches
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与现有方法的比较
- en: The original self-attention mechanism in Transformers, characterized by the
    Multi-Head Attention (MHA) model, necessitates caching the keys ($K$) and values
    ($V$) in each head and layer to accelerate inference (Vaswani et al. [2017](#bib.bib16)).
    This requirement has historically imposed a significant memory overhead, prompting
    a series of innovations aimed at reducing this burden.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 中的原始自注意力机制，以多头注意力（MHA）模型为特征，需要在每个头和层中缓存键（$K$）和值（$V$）以加速推断（Vaswani
    等人 [2017](#bib.bib16)）。这一要求在历史上造成了显著的内存开销，因此促使了一系列旨在减少这一负担的创新。
- en: Among these, Multi-Query Attention (MQA) and its more generalized counterpart,
    Grouped-Query Attention (GQA), consolidate the KV cache by allowing multiple query
    heads within the same layer to share a singular set of K and V matrices. This
    approach effectively reduces the number of unique key and value pairs that must
    be stored and retrieved during the computation process. Subsequently, Cross-Layer
    Attention (CLA) extends this concept by facilitating the sharing of K and V matrices
    across different layers, thereby offering further reductions in the memory footprint
    required for KV storage.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，多查询注意力（MQA）及其更广泛的对应方法，分组查询注意力（GQA），通过允许同一层内的多个查询头共享一组 K 和 V 矩阵来整合 KV
    缓存。这种方法有效减少了在计算过程中必须存储和检索的唯一键值对的数量。随后，跨层注意力（CLA）通过促进不同层之间的 K 和 V 矩阵的共享，进一步减少了
    KV 存储所需的内存占用。
- en: Our method, however, introduces a fundamentally different paradigm in addressing
    the challenges of self-attention. While previous methods have focused on reducing
    the redundancy in storing K and V matrices, our approach centers on the optimization
    of the computation of attention weights themselves. In standard practice, the
    cached keys ($K$) are primarily utilized to compute attention weights in conjunction
    with the queries ($Q$). Instead of indirectly facilitating this interaction through
    shared KV matrices, our method proposes the direct sharing of the resultant attention
    weights—specifically, the softmax-normalized scores.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的方法在解决自注意力挑战方面引入了根本不同的范式。虽然以前的方法侧重于减少存储 K 和 V 矩阵的冗余，我们的方法则集中于优化注意力权重本身的计算。在标准实践中，缓存的键（$K$）主要用于与查询（$Q$）一起计算注意力权重。我们的办法是直接共享计算出的注意力权重——具体来说，是
    softmax 标准化后的分数，而不是通过共享 KV 矩阵间接促进这种交互。
- en: This not only diminishes the memory requirements by obviating the need to store
    separate sets of keys for each layer but also significantly reduces the computational
    complexity. By sharing the pre-computed softmax results across layers, our approach
    circumvents the repeated calculation of softmax, which is often one of the most
    computationally intensive operations in the attention mechanism. This efficiency
    gain is reflected in a substantial reduction in the number of floating-point operations
    (FLOPs) required during model inference, enhancing both the speed and scalability
    of Transformer deployments.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅通过消除存储每层单独键集的需求来减少内存需求，还显著降低了计算复杂性。通过在层间共享预计算的softmax结果，我们的方法避免了softmax的重复计算，而softmax通常是注意力机制中最计算密集的操作之一。这种效率的提高反映在模型推理过程中所需的浮点运算（FLOPs）显著减少，提升了Transformer部署的速度和可扩展性。
- en: Unlike traditional methods that optimize memory use by sharing physical keys
    and values across layers or heads, our Shared Attention model innovates on the
    computational process itself, exploiting the consistent patterns in attention
    weights to streamline operations across multiple layers of the Transformer architecture.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统方法通过跨层或头部共享物理键值来优化内存使用不同，我们的共享注意力模型在计算过程本身上进行了创新，利用注意力权重中的一致模式来简化Transformer架构中多个层的操作。
- en: Isotropic Attention Distribution
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 各向同性注意力分布
- en: In an extensive analysis of layer-specific attention weights across a spectrum
    of LLMs, we explored the attention dynamics within models such as Llama2-7B-chat,
    Llama3-8B-instruct, Llama3-70B-instruct, Baichuan2-7B-chat, Qwen2-7B-instruct,
    and Qwen2-72B-instruct (Touvron et al. [2023](#bib.bib15); Yang et al. [2023](#bib.bib19);
    Bai et al. [2023](#bib.bib2)). These models were evaluated using the MMLU.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在对一系列大语言模型（LLMs）的层特定注意力权重进行深入分析时，我们探讨了模型如Llama2-7B-chat、Llama3-8B-instruct、Llama3-70B-instruct、Baichuan2-7B-chat、Qwen2-7B-instruct和Qwen2-72B-instruct中的注意力动态（Touvron
    et al. [2023](#bib.bib15); Yang et al. [2023](#bib.bib19); Bai et al. [2023](#bib.bib2)）。这些模型使用了MMLU进行评估。
- en: 'Our investigations reveal a self-organization pattern in the attention weights
    across these diverse models. As depicted in Figure [2](#Sx2.F2 "Figure 2 ‣ Motivation
    ‣ Shared Attention ‣ Beyond KV Caching: Shared Attention for Efficient LLMs"),
    there exists a consistent global similarity pattern in the layers’ attention weights
    across all tested models. This pattern suggests an inherent structural characteristic
    in the way LLMs process information, which can be broadly segmented into four
    distinct groups:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的研究揭示了这些不同模型中注意力权重的自组织模式。如图 [2](#Sx2.F2 "Figure 2 ‣ Motivation ‣ Shared Attention
    ‣ Beyond KV Caching: Shared Attention for Efficient LLMs") 所示，所有测试模型的层注意力权重中存在一致的全球相似性模式。这种模式表明，LLMs处理信息的方式存在固有的结构特征，可以广泛地分为四个不同的组：'
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Group 1: Comprising the initial layers (indices 0 and 1), this group is situated
    closest to the input tokens and primarily focuses on abstracting token-level semantic
    information. These layers exhibit data-dependent attention patterns that are crucial
    for the initial semantic processing of the inputs.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 组 1：包括初始层（索引0和1），该组最接近输入标记，主要专注于抽象化标记级别的语义信息。这些层展示了对数据依赖的注意力模式，对于输入的初步语义处理至关重要。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Group 2: This group includes layers immediately following the first group and
    extends up to layer index 5\. Layers in this segment demonstrate high internal
    similarity in attention weights but are markedly different from those in other
    groups. These layers likely serve as transitional zones where intermediate semantic
    features are refined.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 组 2：该组包括紧接第一组的层，并延伸到第5层索引。该段中的层在注意力权重上表现出高度的内部相似性，但与其他组的层显著不同。这些层可能作为过渡区域，其中中间语义特征得以精炼。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Group 3: Encompassing layers post-Group 2 and extending to the penultimate
    layer, this is the largest group both in terms of the number of layers and their
    role within the architecture. The layers within this group display a high degree
    of similarity, suggesting an isotropy in the attention mechanism where the refined
    features are consistently utilized to inform the model’s deeper contextual understanding.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 组 3：包括组 2 之后的层，并延伸到倒数第二层，这是数量最多的组，无论是在层数还是在架构中的角色方面。该组中的层表现出高度的相似性，表明注意力机制具有各向同性，精炼的特征被一致地用于模型的更深层次的上下文理解。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Group 4: The final group, consisting solely of the output layer, distinctively
    processes the aggregated contextual information to generate outputs. This layer’s
    attention weights diverge from those observed in other layers, underscoring its
    specialized role in the final decision-making process.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第4组：最后一组仅由输出层组成，独特地处理聚合的上下文信息以生成输出。该层的注意力权重与其他层所观察到的不同，突显了其在最终决策过程中的专门角色。
- en: The distinct attention weight patterns identified across these groups reinforce
    the concept of functional specialization within LLMs. This segmentation not only
    highlights the diverse roles of different layers in processing inputs but also
    supports the potential for optimizing computational strategies, such as our proposed
    Shared Attention method, by manipulating these inherent patterns to reduce computational
    redundancy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组之间识别出的不同注意力权重模式强化了LLM中功能专门化的概念。这种分段不仅突显了不同层在处理输入中的多样化角色，还支持通过操控这些固有模式来优化计算策略，如我们提出的共享注意力方法，从而减少计算冗余。
- en: '![Refer to caption](img/f9c814a598a02f00745ee6b9a373ac50.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f9c814a598a02f00745ee6b9a373ac50.png)'
- en: 'Figure 3: Evolution of layer attention weights similarity throughout the pretraining
    phase of the Baichuan2 7B model, as it processes trained tokens from 220 billion
    to 2.6 trillion. The color gradient in the visualization represents cosine similarity,
    effectively illustrating the transition in attention patterns from the initial
    to the advanced stages of pretraining.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Baichuan2 7B模型在预训练阶段注意力权重相似性的演变，处理从220亿到2.6万亿的训练令牌。可视化中的颜色渐变表示余弦相似性，有效地展示了从初始阶段到高级阶段的注意力模式的过渡。
- en: Dynamics During Pretraining
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练期间的动态
- en: To elucidate the formation and evolution of attention weight patterns during
    the pretraining phase of LLMs, we utilized intermediate checkpoints of the Baichuan
    7B model, provided by the model developers. These checkpoints, spanning from 0.2T
    to 2.6T tokens processed, offer a unique point of view to observe the dynamic
    shifts in attention mechanisms as the model gains exposure to an increasing volume
    of data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明在LLM预训练阶段注意力权重模式的形成和演变，我们利用了由模型开发者提供的Baichuan 7B模型的中间检查点。这些检查点涵盖了从0.2T到2.6T的处理令牌，提供了一个独特的视角来观察随着模型接触越来越多的数据，注意力机制的动态变化。
- en: 'We applied a consistent metric for measuring the similarity of attention weights
    across layers at each pretraining checkpoint. Additionally, the final chat model,
    fine-tuned to align with human reference responses, was included to benchmark
    the evolution against practical application outcomes. The dynamics of these attention
    weights are visualized in Figure [3](#Sx3.F3 "Figure 3 ‣ Isotropic Attention Distribution
    ‣ Beyond KV Caching: Shared Attention for Efficient LLMs"), which illustrates
    the progressive differentiation and stabilization of attention patterns across
    the model’s layers.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每个预训练检查点应用了一致的度量标准来衡量各层注意力权重的相似性。此外，还包括了最终的聊天模型，这一模型经过微调以对齐人类参考响应，用于与实际应用结果进行对比。注意力权重的动态在图[3](#Sx3.F3
    "图3 ‣ 各向同性注意力分布 ‣ 超越KV缓存：高效LLM的共享注意力")中进行了可视化，展示了模型各层注意力模式的逐步差异化和稳定化。
- en: As observed in the early pretraining stage at 0.2T tokens, Groups 1 and 2 appear
    merged, indicating a less differentiated processing strategy across these initial
    layers. This combination suggests that early in training, the model does not distinctly
    separate token-level semantic processing from intermediate semantic refinement.
    However, as the model progresses to 1.0T tokens, a clear division emerges between
    Groups 1 and 2\. This separation aligns with the model beginning to form more
    specialized and efficient strategies for handling different types of information
    across its architecture.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在0.2T令牌的早期预训练阶段观察到，第1组和第2组似乎合并，这表明这些初始层的处理策略不够差异化。这种结合表明在训练初期，模型没有明显区分令牌级别的语义处理和中间语义细化。然而，随着模型进展到1.0T令牌，第1组和第2组之间出现了明显的分隔。这一分隔与模型开始形成更专业和高效的信息处理策略相一致。
- en: The similarity within Group 3, which encompasses the bulk of the model’s layers,
    shows a marked improvement from a similarity score of 0.8 to 0.9\. This increase
    is indicative of the model’s attention mechanism stabilizing and becoming more
    consistent in its approach to processing the bulk of contextual information.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Group 3内的相似度从0.8提升至0.9，显示出显著改进。这一增长表明模型的注意机制正在稳定，并在处理大量上下文信息时变得更加一致。
- en: The training advancements observed across the pretraining checkpoints not only
    demonstrate significant shifts in the internal structure of the model’s attention
    mechanisms but also correlate positively with performance improvements on multiple
    benchmarks. This includes results on the MMLU, CMMLU (Li et al. [2023](#bib.bib10)),
    and C-Eval (Huang et al. [2024](#bib.bib8)) 5-shot accuracy tests, which have
    reportedly improved from a baseline accuracy of 0.25 to 0.50 (Yang et al. [2023](#bib.bib19)).
    This notable enhancement underscores the intrinsic link between the refinement
    of attention mechanisms within LLMs and their enhanced capabilities in natural
    language understanding tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练检查点观察到的训练进展不仅展示了模型注意机制内部结构的显著变化，而且与多个基准上的性能改进正相关。这包括MMLU、CMMLU（Li等，[2023](#bib.bib10)）和C-Eval（Huang等，[2024](#bib.bib8)）的5-shot准确率测试，报告显示从基线准确率0.25提升至0.50（Yang等，[2023](#bib.bib19)）。这一显著提升突显了LLM内部注意机制的精炼与其在自然语言理解任务中能力增强之间的内在联系。
- en: Moreover, further examination of the model’s development, as observed in supplementary
    material, reveals that the similarity within Group 3—comprising the core contextual
    processing layers of the model—continues to enhance after the alignment stage.
    This observation suggests that the alignment process, typically aimed at fine-tuning
    the model to more closely mirror human-like understanding and response generation,
    also contributes to the stabilization of the model’s attention mechanisms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，进一步检查模型的发展（如补充材料所示）揭示Group 3（包括模型的核心上下文处理层）的相似度在对齐阶段后仍在提升。这一观察结果表明，对齐过程通常旨在使模型更贴近人类理解和响应生成，也有助于模型注意机制的稳定。
- en: Experiments and Discussion
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验与讨论
- en: To validate the efficacy of our proposed Shared Attention (SA) method, we conducted
    series of experiments. These experiments were designed to test the robustness
    of SA under various configurations and to evaluate its performance on widely recognized
    benchmarks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们提出的Shared Attention（SA）方法的有效性，我们进行了系列实验。这些实验旨在测试SA在各种配置下的鲁棒性，并评估其在广泛认可的基准上的性能。
- en: Initially, we applied the SA mechanism directly to advanced LLMs without any
    prior training to assess its impact on pre-trained models. This experiment aimed
    to understand the immediate effects of SA when integrated into existing model
    architectures. We evaluated the performance of these models on standard LLM benchmarks,
    including GLUE (General), GSM8k (Arithmetic), HellaSwag (Reasoning), and MMLU
    (Knowledge) (Wang et al. [2018](#bib.bib17); Cobbe et al. [2021](#bib.bib5); Zellers
    et al. [2019](#bib.bib20)). As anticipated, the direct application of SA resulted
    in a loss of accuracy on some benchmarks. This outcome is consistent with our
    expectations given the lack of retraining to adapt the models fully to the nuances
    of the Shared Attention mechanism. Due to computational constraints, it was impractical
    for our team to pretrain an LLM from scratch incorporating SA.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们将SA机制直接应用于先进的LLM，而没有任何事先训练，以评估其对预训练模型的影响。这个实验旨在理解SA在集成到现有模型架构中的即时效果。我们在标准LLM基准测试上评估了这些模型的性能，包括GLUE（通用）、GSM8k（算术）、HellaSwag（推理）和MMLU（知识）（Wang等，[2018](#bib.bib17)；Cobbe等，[2021](#bib.bib5)；Zellers等，[2019](#bib.bib20)）。正如预期，SA的直接应用在一些基准测试上导致了准确度的下降。鉴于缺乏重新训练以完全适应Shared
    Attention机制的细微差别，这一结果与我们的预期一致。由于计算限制，我们的团队无法从头开始预训练一个包含SA的LLM。
- en: To further probe the capabilities of SA under a training regimen, we fine-tuned
    base LLMs equipped with Shared Attention on the publicly available Instruct dataset
    (Taori et al. [2023](#bib.bib14)). Post fine-tuning, these models were tested
    against the same benchmarks to find out any performance changes. This approach
    allowed us to measure the adaptability of SA when models are trained to accommodate
    its dynamics.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探究 SA 在训练过程中的能力，我们在公开的 Instruct 数据集（Taori et al. [2023](#bib.bib14)）上微调了配备了
    Shared Attention 的基础 LLMs。微调后，这些模型在相同的基准测试中进行了测试，以发现任何性能变化。这种方法使我们能够衡量当模型被训练以适应
    SA 的动态时的适应性。
- en: These experiments collectively demonstrate the potential of Shared Attention
    to modify the traditional attention mechanism in LLMs, showing a promising avenue
    for reducing computational demands while maintaining, and in some cases enhancing,
    model performance. The detailed results and further discussion on each benchmark
    and dataset are provided in the subsequent sections.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验共同展示了 Shared Attention 在 LLM 中修改传统注意力机制的潜力，展示了在保持甚至在某些情况下提升模型性能的同时减少计算需求的有希望的途径。详细的结果和对每个基准测试和数据集的进一步讨论在后续部分提供。
- en: Experimental Setup
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验设置
- en: For the fine-tuning experiments, we utilized the Llama2-7B and Llama3-8B base
    models. These experiments were conducted on a robust hardware configuration consisting
    of two NVIDIA A100 80GB GPUs. Optimization of the models was carried out using
    the AdamW optimizer, with an initial learning rate set at $2\times 10^{-5}$. We
    employed the bf16 datatype for model parameters, which enhances the numeric range
    and stability during backpropagation, crucial for maintaining precision in large
    model training.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于微调实验，我们使用了 Llama2-7B 和 Llama3-8B 基础模型。这些实验在由两块 NVIDIA A100 80GB GPU 组成的强大硬件配置上进行。模型的优化使用了
    AdamW 优化器，初始学习率设置为 $2\times 10^{-5}$。我们采用了 bf16 数据类型来增强模型参数的数值范围和在反向传播过程中的稳定性，这对大模型训练中的精度保持至关重要。
- en: Each GPU handled a micro-batch size of 16, leveraging gradient accumulation
    techniques to effectively manage the computational load. Additionally, we utilized
    DeepSpeed Zero Stage 3 to optimize the distribution of model and optimizer parameters
    and enhance memory management across the GPUs, ensuring efficient use of available
    resources. The fine-tuning process spanned two epochs and employed the standard
    Alpaca instruction format, which is designed to improve the responsiveness and
    accuracy of the models in handling instruction-based tasks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 GPU 处理的微批量大小为 16，利用梯度累积技术有效管理计算负载。此外，我们使用了 DeepSpeed Zero Stage 3 来优化模型和优化器参数的分配，并改善
    GPU 之间的内存管理，以确保资源的高效利用。微调过程历时两个周期，并采用了标准的 Alpaca 指令格式，旨在提高模型处理基于指令任务的响应性和准确性。
- en: Direct Application of Shared Attention
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Shared Attention 的直接应用
- en: 'The application of SA was tested across discrete segments of layers within
    the Llama2-7B and Llama3-8B models, each comprising 32 layers in total. To evaluate
    the robustness and adaptability of SA as shown in Figure [4](#Sx4.F4 "Figure 4
    ‣ Direct Application of Shared Attention ‣ Experiments and Discussion ‣ Beyond
    KV Caching: Shared Attention for Efficient LLMs"), it was implemented in varying
    layer segments, ranging from narrower spans such as four layers (e.g., SA:15$\sim$18)
    to broader spans such as eight layers (e.g., SA:23$\sim$30).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'SA 的应用在 Llama2-7B 和 Llama3-8B 模型中进行了测试，每个模型总共有 32 层。为了评估 SA 的稳健性和适应性，如图 [4](#Sx4.F4
    "Figure 4 ‣ Direct Application of Shared Attention ‣ Experiments and Discussion
    ‣ Beyond KV Caching: Shared Attention for Efficient LLMs") 所示，SA 在不同的层段中进行了实现，从四层（例如，SA:15$\sim$18）到八层（例如，SA:23$\sim$30）等更宽的跨度。'
- en: '![Refer to caption](img/01b4e238fb5505d468fc9c281c2a87f4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/01b4e238fb5505d468fc9c281c2a87f4.png)'
- en: 'Figure 4: The figure illustrates the implementation of Shared Attention within
    specific layer segments of the model. Shared Attention spans from layer 27 to
    30 for a four-layer segment and from layer 23 to 30 for an eight-layer segment.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：该图示意了 Shared Attention 在模型的特定层段中的实现。Shared Attention 从第 27 层到第 30 层跨越四层段，从第
    23 层到第 30 层跨越八层段。
- en: 'Preliminary assessments of SA in the earlier layers of Llama2-7B (e.g., layers
    3 to 6) resulted in an explosion of perplexity, indicating significant disruptions
    in the model’s ability to predict subsequent tokens accurately. This phenomenon
    underscores the crucial role that attention score variances play in the model’s
    early stages of processing, which are essential for initial context setting and
    feature extraction. To quantitatively assess the impact of attention variance
    throughout the model, we conducted a detailed variance analysis. We applied the
    same computational method used to obtain attention mean scores to calculate the
    variance of attention weights in Llama2-7B and Llama3-8B while processing the
    MMLU dataset. We further explored the potential influence of attention variance
    in downstream layers by computing a weighted cumulative variance. This metric
    aggregates the variances of all downstream layers starting from each specific
    layer, weighted by the average of these summed variances. As illustrated in Figure
    [5](#Sx4.F5 "Figure 5 ‣ Direct Application of Shared Attention ‣ Experiments and
    Discussion ‣ Beyond KV Caching: Shared Attention for Efficient LLMs"), the analysis
    revealed that early layers exhibited significantly higher weighted variances compared
    to latter layers. This variance tends to decrease as one progresses through the
    model’s architecture, suggesting a stabilization of attention mechanisms in the
    latter layers. Given these results, our experiments predominantly focused on the
    application of SA in the latter layers, where such variances appear to stabilize.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对Llama2-7B早期层（例如，第3到第6层）进行的初步评估显示出困惑度的爆炸，表明模型在准确预测后续标记方面存在显著干扰。这一现象强调了注意力得分方差在模型早期处理阶段的重要作用，这对初始上下文设置和特征提取至关重要。为了定量评估注意力方差在整个模型中的影响，我们进行了详细的方差分析。我们采用了与获取注意力均值分数相同的计算方法，计算了Llama2-7B和Llama3-8B在处理MMLU数据集时的注意力权重方差。我们进一步探讨了注意力方差在下游层的潜在影响，通过计算加权累积方差。该指标汇总了从每个特定层开始的所有下游层的方差，并按这些总方差的平均值加权。如图[5](#Sx4.F5
    "图5 ‣ 共享注意力的直接应用 ‣ 实验与讨论 ‣ 超越KV缓存：高效LLMs的共享注意力")所示，分析揭示了早期层的加权方差显著高于后期层。这种方差随着模型架构的推进而减少，表明后期层中的注意力机制趋于稳定。鉴于这些结果，我们的实验主要集中在后期层的SA应用上，这些层的方差似乎已趋于稳定。
- en: '| Model | GLUE |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GLUE |'
- en: '&#124; GSM8K &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GSM8K &#124;'
- en: '&#124; 5-shot &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5-shot &#124;'
- en: '| HellaSwag | MMLU |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag | MMLU |'
- en: '| Llama2-7B | 0.4050 $\pm$ 0.0019 | 0.1395 $\pm$ 0.0095 | 0.5713 $\pm$ 0.0049
    | 0.4119 $\pm$ 0.0041 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | 0.4050 $\pm$ 0.0019 | 0.1395 $\pm$ 0.0095 | 0.5713 $\pm$ 0.0049
    | 0.4119 $\pm$ 0.0041 |'
- en: '| Llama2-7B${}_{\text{SA}:23\sim 30}$ | 0.3819 $\pm$ 0.0019 | 0.0728 $\pm$
    0.0072 | 0.5575 $\pm$ 0.0050 | 0.3794 $\pm$ 0.0040 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B${}_{\text{SA}:23\sim 30}$ | 0.3819 $\pm$ 0.0019 | 0.0728 $\pm$
    0.0072 | 0.5575 $\pm$ 0.0050 | 0.3794 $\pm$ 0.0040 |'
- en: '| Llama2-7B${}_{\text{SA}:27\sim 30}$ | 0.3882 $\pm$ 0.0019 | 0.1243 $\pm$
    0.0091 | 0.5616 $\pm$ 0.0050 | 0.4056 $\pm$ 0.0041 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B${}_{\text{SA}:27\sim 30}$ | 0.3882 $\pm$ 0.0019 | 0.1243 $\pm$
    0.0091 | 0.5616 $\pm$ 0.0050 | 0.4056 $\pm$ 0.0041 |'
- en: '| Llama2-7B${}_{\text{SA}:23\sim 26}$ | 0.4351 $\pm$ 0.0019 | 0.1122 $\pm$
    0.0087 | 0.5681 $\pm$ 0.0049 | 0.3994 $\pm$ 0.0040 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B${}_{\text{SA}:23\sim 26}$ | 0.4351 $\pm$ 0.0019 | 0.1122 $\pm$
    0.0087 | 0.5681 $\pm$ 0.0049 | 0.3994 $\pm$ 0.0040 |'
- en: '| Llama2-7B${}_{\text{SA}:19\sim 22}$ | 0.3996 $\pm$ 0.0019 | 0.0834 $\pm$
    0.0076 | 0.5553 $\pm$ 0.0050 | 0.3926 $\pm$ 0.0040 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B${}_{\text{SA}:19\sim 22}$ | 0.3996 $\pm$ 0.0019 | 0.0834 $\pm$
    0.0076 | 0.5553 $\pm$ 0.0050 | 0.3926 $\pm$ 0.0040 |'
- en: '| Llama2-7B${}_{\text{SA}:15\sim 18}$ | 0.3731 $\pm$ 0.0019 | 0.0220 $\pm$
    0.0040 | 0.4790 $\pm$ 0.0050 | 0.3378 $\pm$ 0.0047 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B${}_{\text{SA}:15\sim 18}$ | 0.3731 $\pm$ 0.0019 | 0.0220 $\pm$
    0.0040 | 0.4790 $\pm$ 0.0050 | 0.3378 $\pm$ 0.0047 |'
- en: '| Llama2-7B-Instruct-SFT | 0.5372 $\pm$ 0.0019 | 0.1440 $\pm$ 0.0097 | 0.5772
    $\pm$ 0.0049 | 0.3722 $\pm$ 0.0040 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-Instruct-SFT | 0.5372 $\pm$ 0.0019 | 0.1440 $\pm$ 0.0097 | 0.5772
    $\pm$ 0.0049 | 0.3722 $\pm$ 0.0040 |'
- en: '| Llama2-7B-Instruct-SFT${}_{\text{SA}:23\sim 30}$ | 0.5401 $\pm$ 0.0019 |
    0.0758 $\pm$ 0.0073 | 0.5671 $\pm$ 0.0049 | 0.3717 $\pm$ 0.0040 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-Instruct-SFT${}_{\text{SA}:23\sim 30}$ | 0.5401 $\pm$ 0.0019 |
    0.0758 $\pm$ 0.0073 | 0.5671 $\pm$ 0.0049 | 0.3717 $\pm$ 0.0040 |'
- en: '| Llama3-8B | 0.4804 $\pm$ 0.0019 | 0.5155 $\pm$ 0.0138 | 0.6009 $\pm$ 0.0049
    | 0.6198 $\pm$ 0.0038 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B | 0.4804 $\pm$ 0.0019 | 0.5155 $\pm$ 0.0138 | 0.6009 $\pm$ 0.0049
    | 0.6198 $\pm$ 0.0038 |'
- en: '| Llama3-8B${}_{\text{SA}:23\sim 30}$ | 0.5595 $\pm$ 0.0019 | 0.3275 $\pm$
    0.0129 | 0.6011 $\pm$ 0.0049 | 0.6122 $\pm$ 0.0038 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B${}_{\text{SA}:23\sim 30}$ | 0.5595 $\pm$ 0.0019 | 0.3275 $\pm$
    0.0129 | 0.6011 $\pm$ 0.0049 | 0.6122 $\pm$ 0.0038 |'
- en: '| Llama3-8B${}_{\text{SA}:27\sim 30}$ | 0.5532 $\pm$ 0.0019 | 0.4526 $\pm$
    0.0137 | 0.6060 $\pm$ 0.0049 | 0.6163 $\pm$ 0.0038 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B${}_{\text{SA}:27\sim 30}$ | 0.5532 $\pm$ 0.0019 | 0.4526 $\pm$
    0.0137 | 0.6060 $\pm$ 0.0049 | 0.6163 $\pm$ 0.0038 |'
- en: '| Llama3-8B${}_{\text{SA}:23\sim 26}$ | 0.5024 $\pm$ 0.0019 | 0.4556 $\pm$
    0.0137 | 0.5993 $\pm$ 0.0049 | 0.6189 $\pm$ 0.0038 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B${}_{\text{SA}:23\sim 26}$ | 0.5024 $\pm$ 0.0019 | 0.4556 $\pm$
    0.0137 | 0.5993 $\pm$ 0.0049 | 0.6189 $\pm$ 0.0038 |'
- en: '| Llama3-8B${}_{\text{SA}:19\sim 22}$ | 0.5115 $\pm$ 0.0019 | 0.3745 $\pm$
    0.0133 | 0.5829 $\pm$ 0.0049 | 0.6181 $\pm$ 0.0038 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B${}_{\text{SA}:19\sim 22}$ | 0.5115 $\pm$ 0.0019 | 0.3745 $\pm$
    0.0133 | 0.5829 $\pm$ 0.0049 | 0.6181 $\pm$ 0.0038 |'
- en: '| Llama3-8B${}_{\text{SA}:15\sim 18}$ | 0.4685 $\pm$ 0.0019 | 0.0136 $\pm$
    0.0032 | 0.5307 $\pm$ 0.0050 | 0.3019 $\pm$ 0.0038 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B${}_{\text{SA}:15\sim 18}$ | 0.4685 $\pm$ 0.0019 | 0.0136 $\pm$
    0.0032 | 0.5307 $\pm$ 0.0050 | 0.3019 $\pm$ 0.0038 |'
- en: 'Table 1: Performance metrics for different models across tasks'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同模型在任务中的性能指标
- en: 'Figure 5: The figure displays the weighted cumulative variance for the Llama2-7B-chat
    and Llama3-8B-instruct models. The two lower axes represent the model’s structure:
    the left axis details the 32 layers, and the right axis shows the 32 heads within
    each layer. The z-axis represents the variance values.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：图中显示了 Llama2-7B-chat 和 Llama3-8B-instruct 模型的加权累积方差。两个下轴表示模型的结构：左轴详细描述了
    32 层，右轴显示了每层内的 32 个头。z 轴表示方差值。
- en: 'The outcomes of these experiments, as summarized in Table [1](#Sx4.T1 "Table
    1 ‣ Direct Application of Shared Attention ‣ Experiments and Discussion ‣ Beyond
    KV Caching: Shared Attention for Efficient LLMs"), reveal interesting patterns.
    For the Llama2-7B model, implementing SA in the latter layers (e.g., SA:23$\sim$26
    and SA:27$\sim$30) maintained relatively stable performance across a variety of
    benchmarks, including GLUE and MMLU. Conversely, extending the scope of SA to
    encompass more layers, particularly mid-level layers such as SA:15$\sim$18, led
    to a noticeable degradation in tasks requiring mathematical reasoning (GSM8K).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验的结果，如表[1](#Sx4.T1 "表 1 ‣ 共享注意力的直接应用 ‣ 实验与讨论 ‣ 超越 KV 缓存：高效 LLM 的共享注意力")所总结的，揭示了有趣的模式。对于
    Llama2-7B 模型，在后层（例如 SA:23$\sim$26 和 SA:27$\sim$30）实施 SA 使得在多种基准测试中，包括 GLUE 和 MMLU，性能保持相对稳定。相反，将
    SA 扩展到更多层，特别是像 SA:15$\sim$18 这样的中层，导致在需要数学推理的任务（GSM8K）中性能显著下降。
- en: In comparison, the Llama3-8B model, which inherently showed higher layer-wise
    attention similarity as discussed in the previous sections, exhibited less performance
    deterioration when SA was applied. After implementing SA in the layers closer
    to the model’s output (e.g., SA:27$\sim$30), the Llama3-8B even outperformed its
    original configuration on the GLUE benchmark, suggesting that strategic placement
    of SA can potentially enhance the model’s performance in complex natural language
    understanding tasks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，Llama3-8B 模型在之前部分讨论中显示了较高的层级注意力相似性，在应用 SA 时表现出较少的性能退化。在将 SA 实施于靠近模型输出的层（例如
    SA:27$\sim$30）后，Llama3-8B 在 GLUE 基准测试中甚至超越了其原始配置，这表明 SA 的战略性放置可能会提升模型在复杂自然语言理解任务中的表现。
- en: Fine-Tuning on Instruct Dataset
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Instruct 数据集上的微调
- en: Given the computational constraints that preclude the pretraining of LLMs with
    SA from scratch, we adopted to fine-tune existing LLMs to evaluate whether fine-tuning
    could ameliorate the performance deficits observed with the direct application
    of SA. This approach was particularly aimed at understanding the adaptability
    of SA under a more controlled learning regimen.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于计算限制使得无法从头开始对 LLMs 进行 SA 预训练，我们选择微调现有的 LLMs，以评估微调是否能够改善直接应用 SA 观察到的性能不足。该方法特别旨在了解
    SA 在更受控的学习环境下的适应性。
- en: Fine-tuning was conducted on the publicly available Instruct dataset, which
    is designed to evaluate models on tasks that require following complex instructions.
    This dataset was chosen because it challenges the models to utilize their learned
    representations effectively, making it an ideal benchmark for testing the efficacy
    of modifications like SA.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是在公开可用的 Instruct 数据集上进行的，该数据集旨在评估模型在需要遵循复杂指令的任务上的表现。选择这个数据集是因为它挑战模型有效利用其学习的表征，使其成为测试
    SA 等修改效果的理想基准。
- en: 'The results, as summarized in Table [1](#Sx4.T1 "Table 1 ‣ Direct Application
    of Shared Attention ‣ Experiments and Discussion ‣ Beyond KV Caching: Shared Attention
    for Efficient LLMs"), demonstrate a narrowed performance gap between the original
    models and those modified with SA. For instance, while the original Llama2-7B
    model outperformed the SA version in direct application tests, the fine-tuned
    Llama2-7B${}_{\text{SA}:23\sim 30}$ showed significant improvements across multiple
    metrics. This suggests that fine-tuning enables the model to better integrate
    and leverage the Shared Attention mechanism, effectively regaining some of the
    lost performance noted in the initial application of SA.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](#Sx4.T1 "Table 1 ‣ Direct Application of Shared Attention ‣ Experiments
    and Discussion ‣ Beyond KV Caching: Shared Attention for Efficient LLMs")中总结的结果，原始模型与SA修改版之间的性能差距有所缩小。例如，尽管原始Llama2-7B模型在直接应用测试中优于SA版本，但微调后的Llama2-7B${}_{\text{SA}:23\sim
    30}$在多个指标上显示出显著的改进。这表明，微调使模型能够更好地集成和利用共享注意力机制，从而有效地恢复了在SA初始应用中所记录的部分性能损失。'
- en: These findings indicate the potential of fine-tuning as a viable method for
    integrating new architectural changes like SA into existing models. The recovery
    in performance indicates that with adequate training, the initial disadvantages
    of directly applying SA can be mitigated, leading to enhanced model capabilities
    that more closely align with or even exceed their original configurations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现表明，微调作为将SA等新架构更改集成到现有模型中的一种可行方法具有潜力。性能的恢复表明，通过充分的训练，可以减轻直接应用SA的初始缺点，从而提升模型能力，使其更接近甚至超越其原始配置。
- en: Future Directions
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未来方向
- en: Our experimental investigations have demonstrated that implementing Shared Attention
    (SA) across multiple latter layers in LLMs arouses minimal accuracy loss, making
    it a promising approach for enhancing model efficiency. Furthermore, our analysis
    reveals a trend towards isotropic attention patterns during the pretraining process,
    indicating that the models’ attention mechanisms tend to stabilize as they process
    more data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验研究表明，在LLMs的多个后续层中实施共享注意力（SA）引起的准确度损失最小，这使其成为一种提升模型效率的有前景的方法。此外，我们的分析揭示了在预训练过程中出现的各向同性注意力模式的趋势，表明模型的注意力机制在处理更多数据时趋于稳定。
- en: Given these insights, integrating SA from the pretraining appears to be a particularly
    beneficial strategy. This early integration could allow models to better adapt
    to the streamlined attention mechanism, potentially improving performance and
    efficiency across various tasks. The foundational embedding of SA might simplify
    later adaptations and inherently supports efficient attention dynamics.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些见解，将SA集成到预训练中似乎是一种特别有益的策略。这种早期集成可以使模型更好地适应精简的注意力机制，从而可能在各种任务中提高性能和效率。SA的基础嵌入可能简化后续的调整，并本质上支持高效的注意力动态。
- en: Another promising research direction involves exploring combinations between
    SA and other attention-sharing strategies like Cross-Layer Attention (CLA). Combining
    SA with methods such as CLA could exploit the strengths of both approaches, leading
    to a more robust and flexible attention mechanism. This holistic approach to attention
    management could provide a comprehensive solution that maximizes both computational
    efficiency and model scalability.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有前景的研究方向是探索SA与其他注意力共享策略（如跨层注意力（CLA））的组合。将SA与CLA等方法相结合，可以发挥这两种方法的优点，从而形成更强大和灵活的注意力机制。这种整体的方法可能提供一个综合解决方案，最大化计算效率和模型可扩展性。
- en: By pursuing these avenues, future research can not only refine the application
    of Shared Attention within LLMs but also explore its full potential in enhancing
    the architectural and operational efficiency of next-generation language models.
    These efforts could lead to models that are better equipped to handle the increasing
    complexity and diversity of tasks in natural language processing.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过追求这些方向，未来的研究不仅可以改进在LLMs中应用共享注意力，还可以探索其在提升下一代语言模型的架构和操作效率方面的全部潜力。这些努力可能导致更好地应对自然语言处理任务日益增加的复杂性和多样性的模型。
- en: Related Work
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Efficient memory management in transformers is a critical area of research with
    diverse objectives ranging from reducing memory bandwidth and storage requirements
    to optimizing computational costs during both training and inference phases. Notably,
    our work focuses on minimizing the size of the inference Key-Value (KV) cache
    that persists between model passes, thereby enhancing model efficiency without
    a significant compromise in performance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的内存管理是变换器中的一个关键研究领域，其目标包括减少内存带宽和存储需求，以及优化训练和推理阶段的计算成本。值得注意的是，我们的工作专注于最小化模型传递间持续存在的推理Key-Value（KV）缓存的大小，从而在不显著妥协性能的情况下提高模型效率。
- en: Memory Efficiency in Attention Mechanisms
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力机制中的内存效率
- en: Significant efforts have been made to address the efficiency of the KV cache
    post-training. Techniques such as KV cache compression have been explored extensively.
    For instance, methods like KVQuant (Hooper et al. [2024](#bib.bib7)) and KIVI
    (Liu et al. [2024b](#bib.bib12)) employ quantization strategies to reduce the
    memory footprint of KV pairs to just a few bits. Moreover, works such as AttentionSink
    (Xiao et al. [2023](#bib.bib18)) and Scissorhands (Liu et al. [2024a](#bib.bib11))
    introduce sparsity into the KV cache by selectively storing elements based on
    their proximity or importance to the generation token, thus reducing the overall
    storage requirements.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 针对KV缓存效率的显著努力已在训练后展开。诸如KV缓存压缩等技术已被广泛探索。例如，像KVQuant（Hooper et al. [2024](#bib.bib7)）和KIVI（Liu
    et al. [2024b](#bib.bib12)）的方法采用量化策略将KV对的内存占用减少到仅几位。此外，像AttentionSink（Xiao et
    al. [2023](#bib.bib18)）和Scissorhands（Liu et al. [2024a](#bib.bib11)）等工作通过基于与生成令牌的接近性或重要性选择性地存储元素，引入了KV缓存的稀疏性，从而减少了整体存储需求。
- en: Architectural Innovations for Reducing KV Cache
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减少KV缓存的结构性创新
- en: Architectural modifications aimed at reducing the KV cache size are pivotal
    in enhancing the efficiency of large language models. Such strategies include
    limiting the effective sequence length, as seen in Sparse Attention (Child et al.
    [2019](#bib.bib4)), which constrain attention to local windows to reduce both
    computational load and memory overhead. Another approach involves replacing traditional
    softmax attention with scalable alternatives like linear attention (Katharopoulos
    et al. [2020](#bib.bib9)), which maintains constant space complexity and offers
    more graceful scaling with respect to the token count. Additionally, methods such
    as Grouped-Query Attention (GQA) (Ainslie et al. [2023](#bib.bib1)) and Multi-Query
    Attention (MQA) (Shazeer [2019](#bib.bib13)) aggregate attention across multiple
    queries, significantly decreasing the memory footprint by sharing KV pairs across
    attention heads. These innovations collectively contribute to reducing the redundancy
    in attention calculations and are directly relevant to our work, informing our
    development of the Shared Attention mechanism that further optimizes memory usage
    by sharing attention weights across layers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 针对减少KV缓存大小的结构性修改对于提高大型语言模型的效率至关重要。这些策略包括限制有效序列长度，如在稀疏注意力中所见（Child et al. [2019](#bib.bib4)），它通过将注意力限制在局部窗口中以减少计算负荷和内存开销。另一种方法是用可扩展的替代方案如线性注意力（Katharopoulos
    et al. [2020](#bib.bib9)）替代传统的softmax注意力，这种方法保持常数空间复杂度，并且在令牌数量上提供更优的扩展性。此外，像分组查询注意力（GQA）（Ainslie
    et al. [2023](#bib.bib1)）和多查询注意力（MQA）（Shazeer [2019](#bib.bib13)）等方法通过在多个查询之间汇聚注意力，通过在注意力头之间共享KV对显著减少了内存占用。这些创新共同有助于减少注意力计算中的冗余，与我们的工作直接相关，为我们开发共享注意力机制提供了信息，从而通过在层间共享注意力权重进一步优化内存使用。
- en: Conclusion
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this paper, we explored the attention dynamics within advanced LLMs and observed
    that the attention distribution across layers tends to isotropize following extensive
    pretraining. This isotropic pattern of attention, where layers exhibit similar
    attention mechanisms, inspired a novel approach to attention sharing that departs
    from conventional methods.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探索了先进LLMs中的注意力动态，并观察到在广泛预训练后，层之间的注意力分布趋向于各向同性。这种各向同性的注意力模式，即层之间表现出类似的注意力机制，启发了一种新颖的注意力共享方法，突破了传统方法。
- en: Traditionally, methods like MQA and CLA have focused on sharing KV caches to
    reduce memory overheads but still required the computation of attention weights
    independently across each layer. Our proposed Shared Attention (SA) method bypasses
    this redundancy by directly sharing the computed attention weights across multiple
    layers. This approach not only significantly reduces the size of the KV cache
    but also decreases the computational FLOPs required during model inference.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，MQA 和 CLA 等方法专注于共享 KV 缓存以减少内存开销，但仍需独立计算每一层的注意力权重。我们提出的共享注意力 (SA) 方法通过直接在多层之间共享计算出的注意力权重来绕过这种冗余。这种方法不仅显著减少了
    KV 缓存的大小，还降低了模型推断时所需的计算 FLOP。
- en: The introduction of Shared Attention represents a paradigm shift in the design
    of attention mechanisms in neural networks, emphasizing efficiency without compromising
    the model’s performance. By reducing both the computational burden and memory
    requirements, SA enables more scalable and efficient deployment of LLMs, particularly
    in environments where resources are constrained.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 共享注意力的引入代表了神经网络中注意力机制设计的范式转变，强调在不妥协模型性能的情况下提高效率。通过减少计算负担和内存需求，SA 实现了更具规模性和高效性的
    LLM 部署，特别是在资源受限的环境中。
- en: This research paves the way for further explorations into efficient model architectures
    and opens up new possibilities for the application of LLMs across a broader spectrum
    of tasks and datasets. Future work will focus on expanding the applicability of
    Shared Attention, exploring its integration during the initial phases of model
    training, and combining it with other optimization techniques to maximize the
    operational efficiency of LLMs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究为进一步探索高效模型架构铺平了道路，并为 LLM 在更广泛的任务和数据集中的应用打开了新可能。未来的工作将集中在扩展共享注意力的适用性、探索其在模型训练初期的集成，以及将其与其他优化技术结合以最大化
    LLM 的操作效率。
- en: References
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ainslie et al. (2023) Ainslie, J.; Lee-Thorp, J.; de Jong, M.; Zemlyanskiy,
    Y.; Lebrón, F.; and Sanghai, S. 2023. Gqa: Training generalized multi-query transformer
    models from multi-head checkpoints. *arXiv preprint arXiv:2305.13245*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ainslie 等人 (2023) Ainslie, J.; Lee-Thorp, J.; de Jong, M.; Zemlyanskiy, Y.;
    Lebrón, F.; 和 Sanghai, S. 2023. Gqa: 从多头检查点训练通用多查询变换器模型。*arXiv 预印本 arXiv:2305.13245*。'
- en: Bai et al. (2023) Bai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan,
    Y.; Ge, W.; Han, Y.; Huang, F.; et al. 2023. Qwen technical report. *arXiv preprint
    arXiv:2309.16609*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 (2023) Bai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan, Y.;
    Ge, W.; Han, Y.; Huang, F.; 等人. 2023. Qwen 技术报告。*arXiv 预印本 arXiv:2309.16609*。
- en: Brandon et al. (2024) Brandon, W.; Mishra, M.; Nrusimha, A.; Panda, R.; and
    Kelly, J. R. 2024. Reducing Transformer Key-Value Cache Size with Cross-Layer
    Attention. *arXiv preprint arXiv:2405.12981*.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandon 等人 (2024) Brandon, W.; Mishra, M.; Nrusimha, A.; Panda, R.; 和 Kelly,
    J. R. 2024. 使用跨层注意力减少 Transformer 的键值缓存大小。*arXiv 预印本 arXiv:2405.12981*。
- en: Child et al. (2019) Child, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.
    Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等人 (2019) Child, R.; Gray, S.; Radford, A.; 和 Sutskever, I. 2019. 使用稀疏变换器生成长序列。*arXiv
    预印本 arXiv:1904.10509*。
- en: Cobbe et al. (2021) Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.;
    Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021. Training
    verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人 (2021) Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser,
    L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; 等人. 2021. 训练验证器解决数学文字问题。*arXiv
    预印本 arXiv:2110.14168*。
- en: Hendrycks et al. (2020) Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,
    M.; Song, D.; and Steinhardt, J. 2020. Measuring massive multitask language understanding.
    *arXiv preprint arXiv:2009.03300*.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 (2020) Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,
    M.; Song, D.; 和 Steinhardt, J. 2020. 测量大规模多任务语言理解。*arXiv 预印本 arXiv:2009.03300*。
- en: 'Hooper et al. (2024) Hooper, C.; Kim, S.; Mohammadzadeh, H.; Mahoney, M. W.;
    Shao, Y. S.; Keutzer, K.; and Gholami, A. 2024. Kvquant: Towards 10 million context
    length llm inference with kv cache quantization. *arXiv preprint arXiv:2401.18079*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hooper 等人 (2024) Hooper, C.; Kim, S.; Mohammadzadeh, H.; Mahoney, M. W.; Shao,
    Y. S.; Keutzer, K.; 和 Gholami, A. 2024. Kvquant: 通过 kv 缓存量化实现 1000 万上下文长度的 LLM
    推断。*arXiv 预印本 arXiv:2401.18079*。'
- en: 'Huang et al. (2024) Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su,
    T.; Liu, J.; Lv, C.; Zhang, Y.; Fu, Y.; et al. 2024. C-eval: A multi-level multi-discipline
    chinese evaluation suite for foundation models. *Advances in Neural Information
    Processing Systems*, 36.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等人（2024）Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,
    J.; Lv, C.; Zhang, Y.; Fu, Y.; 等. 2024. C-eval: 一个多层次、多学科的中文评估套件用于基础模型。*神经信息处理系统进展*,
    36。'
- en: 'Katharopoulos et al. (2020) Katharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret,
    F. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention.
    In *International conference on machine learning*, 5156–5165\. PMLR.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katharopoulos 等人（2020）Katharopoulos, A.; Vyas, A.; Pappas, N.; 和 Fleuret, F.
    2020. Transformers 是 RNNs: 快速自回归 transformers 和线性注意力。发表于*国际机器学习会议*, 5156–5165。PMLR。'
- en: 'Li et al. (2023) Li, H.; Zhang, Y.; Koto, F.; Yang, Y.; Zhao, H.; Gong, Y.;
    Duan, N.; and Baldwin, T. 2023. Cmmlu: Measuring massive multitask language understanding
    in chinese. *arXiv preprint arXiv:2306.09212*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2023）Li, H.; Zhang, Y.; Koto, F.; Yang, Y.; Zhao, H.; Gong, Y.; Duan,
    N.; 和 Baldwin, T. 2023. Cmmlu: 测量中文的巨大多任务语言理解能力。*arXiv 预印本 arXiv:2306.09212*。'
- en: 'Liu et al. (2024a) Liu, Z.; Desai, A.; Liao, F.; Wang, W.; Xie, V.; Xu, Z.;
    Kyrillidis, A.; and Shrivastava, A. 2024a. Scissorhands: Exploiting the persistence
    of importance hypothesis for llm kv cache compression at test time. *Advances
    in Neural Information Processing Systems*, 36.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人（2024a）Liu, Z.; Desai, A.; Liao, F.; Wang, W.; Xie, V.; Xu, Z.; Kyrillidis,
    A.; 和 Shrivastava, A. 2024a. Scissorhands: 利用重要性假设的持久性进行 llm kv 缓存压缩。*神经信息处理系统进展*,
    36。'
- en: Liu et al. (2024b) Liu, Z.; Yuan, J.; Jin, H.; Zhong, S.; Xu, Z.; Braverman,
    V.; Chen, B.; and Kivi, X. H. 2024b. A tuning-free asymmetric 2bit quantization
    for kv cache. *arXiv preprint arXiv:2402.02750*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2024b）Liu, Z.; Yuan, J.; Jin, H.; Zhong, S.; Xu, Z.; Braverman, V.; Chen,
    B.; 和 Kivi, X. H. 2024b. 无需调优的非对称 2bit 量化用于 kv 缓存。*arXiv 预印本 arXiv:2402.02750*。
- en: 'Shazeer (2019) Shazeer, N. 2019. Fast transformer decoding: One write-head
    is all you need. *arXiv preprint arXiv:1911.02150*.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shazeer（2019）Shazeer, N. 2019. 快速 transformer 解码: 只需要一个写头。*arXiv 预印本 arXiv:1911.02150*。'
- en: 'Taori et al. (2023) Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.;
    Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpaca: A strong, replicable
    instruction-following model. *Stanford Center for Research on Foundation Models.
    https://crfm. stanford. edu/2023/03/13/alpaca. html*, 3(6): 7.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori 等人（2023）Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin,
    C.; Liang, P.; 和 Hashimoto, T. B. 2023. Alpaca: 一个强大的、可复制的指令跟随模型。*斯坦福大学基础模型研究中心.
    https://crfm.stanford.edu/2023/03/13/alpaca.html*, 3(6): 7。'
- en: 'Touvron et al. (2023) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023.
    Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023）Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; 等. 2023.
    Llama 2: 开放的基础模型和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: Vaswani et al. (2017) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
    L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need.
    *Advances in neural information processing systems*, 30.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017）Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
    L.; Gomez, A. N.; Kaiser, Ł.; 和 Polosukhin, I. 2017. 注意力即一切。*神经信息处理系统进展*, 30。
- en: 'Wang et al. (2018) Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
    Bowman, S. R. 2018. GLUE: A multi-task benchmark and analysis platform for natural
    language understanding. *arXiv preprint arXiv:1804.07461*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2018）Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; 和 Bowman,
    S. R. 2018. GLUE: 一个用于自然语言理解的多任务基准和分析平台。*arXiv 预印本 arXiv:1804.07461*。'
- en: Xiao et al. (2023) Xiao, G.; Tian, Y.; Chen, B.; Han, S.; and Lewis, M. 2023.
    Efficient streaming language models with attention sinks. *arXiv preprint arXiv:2309.17453*.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人（2023）Xiao, G.; Tian, Y.; Chen, B.; Han, S.; 和 Lewis, M. 2023. 高效的流式语言模型与注意力池。*arXiv
    预印本 arXiv:2309.17453*。
- en: 'Yang et al. (2023) Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin,
    C.; Lv, C.; Pan, D.; Wang, D.; Yan, D.; et al. 2023. Baichuan 2: Open large-scale
    language models. *arXiv preprint arXiv:2309.10305*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人（2023）Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.; Lv,
    C.; Pan, D.; Wang, D.; Yan, D.; 等. 2023. Baichuan 2: 开放的大规模语言模型。*arXiv 预印本 arXiv:2309.10305*。'
- en: 'Zellers et al. (2019) Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and
    Choi, Y. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等人（2019）Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; 和 Choi, Y.
    2019. Hellaswag: 机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*。'
