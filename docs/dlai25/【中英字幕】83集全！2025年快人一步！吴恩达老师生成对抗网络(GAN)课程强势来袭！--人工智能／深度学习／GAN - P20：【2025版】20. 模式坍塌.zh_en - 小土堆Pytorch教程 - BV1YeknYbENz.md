# P20：【2025版】20. 模式坍塌.zh_en - 小土堆Pytorch教程 - BV1YeknYbENz

在这周里，你将看到传统GAN在训练时面临的一些问题，这些问题是由二元交叉熵或熵损失引起的。这些问题包括模式崩溃和梯度消失。我将向你展示对GAN架构进行一个小的修改以及一个新的损失函数。

这将帮助你克服这些问题。在这段视频中，你将了解概率密度分布中的一个模式是什么，以及模式崩溃的直观理解。在GAN训练期间。



![](img/26a2ac54c493cdedf7db3bf4c6331573_1.png)

在数据分布中，模式只是一个观察值高度集中的区域。例如。

![](img/26a2ac54c493cdedf7db3bf4c6331573_3.png)

例如，例如，例如，正态分布中的平均值是该分布的单一模式，当然存在有多个模式的分布，平均值不一定是其中之一，因此，在这个双峰分布中有两个模式或多模式，意味着它有多个模式。



![](img/26a2ac54c493cdedf7db3bf4c6331573_5.png)

所以更直观地说，特征上概率密度分布的任何峰值都是该分布的模式。

![](img/26a2ac54c493cdedf7db3bf4c6331573_7.png)

例如，以手写数字为例，它们由特征x1和x2表示，这意味着这些是您可以用来表示这些数字的维度。

![](img/26a2ac54c493cdedf7db3bf4c6331573_9.png)

你喜欢的值可以用来表示不同的手写数字，因此，在这种情况下的概率密度分布将是一个具有许多对应于每个数字的峰值的表面，这绝对是多模式的，有十个不同的模态和数字七的不同观察，例如，将由相似的x1和x2对表示。

因此，这两个值都将是7，而标记为红色的那个，在那里在均值将是平均7将是一个看起来平均的7，你可以想象这些峰值在3D表示中向你伸出，更高的海拔用更深的圆圈表示。



![](img/26a2ac54c493cdedf7db3bf4c6331573_11.png)

不同的特征对x1和x2会产生这些手写五在这里，以及七和五之间的值，这里非常，非常低的密度，所以七和五之间的中间值生成的概率非常低。



![](img/26a2ac54c493cdedf7db3bf4c6331573_13.png)

在真实数据集中可能是七和五的混合。

![](img/26a2ac54c493cdedf7db3bf4c6331573_15.png)

但你看到x1 x2对的概率非常低，在这个产生中间五七看起来的数字之间的空间。

![](img/26a2ac54c493cdedf7db3bf4c6331573_17.png)

所以同一数字的不同观察会在这个特征空间中被分组，在这个区域内，浓度很高，最常见的写法是那个数字，或者那个平均七在那里，当然，平均五在五模峰的中心，所以这个概率密度分布于这些特征，x1和x2会有十个模。

每个数字都有一个模。

![](img/26a2ac54c493cdedf7db3bf4c6331573_19.png)

许多真实的生活分布，用于训练GANs，是多模态的，像这个一样。

![](img/26a2ac54c493cdedf7db3bf4c6331573_21.png)

我将继续使手写数字的例子，来展示模态崩溃可能意味着什么，直观上，当你刚开始时，听起来像是东西崩溃到一个模态或更少的模态，听起来像是东西崩溃到一个模态或更少的模态。



![](img/26a2ac54c493cdedf7db3bf4c6331573_23.png)

一些模式正在消失。

![](img/26a2ac54c493cdedf7db3bf4c6331573_25.png)

所以，一个已经学会识别，手写数字中的假数字。

![](img/26a2ac54c493cdedf7db3bf4c6331573_27.png)

除非生成的图像看起来像1和7。

![](img/26a2ac54c493cdedf7db3bf4c6331573_29.png)

这可能意味着判别器在其成本函数中处于局部最小值，所以判别器正确分类了大多数数字。

![](img/26a2ac54c493cdedf7db3bf4c6331573_31.png)

除了那些看起来像1和7的数字。

![](img/26a2ac54c493cdedf7db3bf4c6331573_33.png)

然后这些信息传递给生成器，现在，生成器看到这一点，并查看判别器的反馈。

![](img/26a2ac54c493cdedf7db3bf4c6331573_35.png)

并了解到如何在下一轮中愚弄判别器。

![](img/26a2ac54c493cdedf7db3bf4c6331573_37.png)

似乎所有图像都被判别器错误分类，看起来像1或7。

![](img/26a2ac54c493cdedf7db3bf4c6331573_39.png)

所以它生成大量看起来像那些数字的图像。

![](img/26a2ac54c493cdedf7db3bf4c6331573_41.png)

然后这些生成的图像在下一轮传递给判别器。

![](img/26a2ac54c493cdedf7db3bf4c6331573_43.png)

然后判别器错误分类了每张图片，除了那张看起来更像7的图片。

![](img/26a2ac54c493cdedf7db3bf4c6331573_45.png)

生成器得到这些反馈，并看到判别器的弱点是。

![](img/26a2ac54c493cdedf7db3bf4c6331573_47.png)

那些看起来像手写1的图像，这次它产生的所有图像都看起来像那个数字。

![](img/26a2ac54c493cdedf7db3bf4c6331573_49.png)

坍塌到所有可能的手写数字的分布中的单一模式。

![](img/26a2ac54c493cdedf7db3bf4c6331573_51.png)

最终，判别器可能会意识到，并学会捕捉生成器伪造的手写数字1，通过摆脱那个局部最小值，但生成器也可能迁移到分布的另一个模式，然后GAN又会坍塌到另一个模式，或者生成器可能无法找出其他地方多样化。



![](img/26a2ac54c493cdedf7db3bf4c6331573_53.png)

总之，模式是特征上的概率分布的峰值，现实世界数据集包含许多与每个可能的类相关的模式，就像这个数据集中的手写数字，模式坍塌发生在生成器学会愚弄判别器。



![](img/26a2ac54c493cdedf7db3bf4c6331573_55.png)

通过产生单一类的示例，来自整个训练数据集，如手写数字1。

![](img/26a2ac54c493cdedf7db3bf4c6331573_57.png)

这是很不幸的，因为当生成器优化以愚弄判别器时。

![](img/26a2ac54c493cdedf7db3bf4c6331573_59.png)