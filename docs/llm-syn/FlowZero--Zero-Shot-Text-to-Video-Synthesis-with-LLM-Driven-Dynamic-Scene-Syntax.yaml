- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FlowZero：基于LLM驱动的动态场景语法的零样本文本到视频合成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.15813](https://ar5iv.labs.arxiv.org/html/2311.15813)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.15813](https://ar5iv.labs.arxiv.org/html/2311.15813)
- en: Yu Lu¹ Linchao Zhu² Hehe Fan² Yi Yang²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 余璐¹ 林超² 何赫² 易杨²
- en: ¹ReLER Lab, University of Technology Sydney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹悉尼科技大学 ReLER 实验室
- en: ²CCAI, Zhejiang University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²CCAI，浙江大学
- en: aniki.yulu@gmail.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: aniki.yulu@gmail.com
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Text-to-video (T2V) generation is a rapidly growing research area that aims
    to translate the scenes, objects, and actions within complex video text into a
    sequence of coherent visual frames. We present FlowZero, a novel framework that
    combines Large Language Models (LLMs) with image diffusion models to generate
    temporally-coherent videos. FlowZero uses LLMs to understand complex spatio-temporal
    dynamics from text, where LLMs can generate a comprehensive dynamic scene syntax (DSS)
    containing scene descriptions, object layouts, and background motion patterns.
    These elements in DSS are then used to guide the image diffusion model for video
    generation with smooth object motions and frame-to-frame coherence. Moreover,
    FlowZero incorporates an iterative self-refinement process, enhancing the alignment
    between the spatio-temporal layouts and the textual prompts for the videos. To
    enhance global coherence, we propose enriching the initial noise of each frame
    with motion dynamics to control the background movement and camera motion adaptively.
    By using spatio-temporal syntaxes to guide the diffusion process, FlowZero achieves
    improvement in zero-shot video synthesis, generating coherent videos with vivid
    motion. Project page: [https://flowzero-video.github.io/](https://flowzero-video.github.io/)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到视频生成（T2V）是一个快速发展的研究领域，旨在将复杂视频文本中的场景、物体和动作转换为一系列连贯的视觉帧。我们提出了FlowZero，一个新颖的框架，它将大型语言模型（LLMs）与图像扩散模型相结合，以生成时间上连贯的视频。FlowZero利用LLMs从文本中理解复杂的时空动态，LLMs可以生成包含场景描述、物体布局和背景运动模式的全面动态场景语法（DSS）。DSS中的这些元素随后用于引导图像扩散模型进行视频生成，以实现平滑的物体运动和帧与帧之间的连贯性。此外，FlowZero还包括一个迭代自我精炼过程，增强了时空布局与视频文本提示之间的一致性。为了提高全球连贯性，我们建议通过动态运动来丰富每一帧的初始噪声，以自适应地控制背景运动和摄像机运动。通过使用时空语法来指导扩散过程，FlowZero在零样本视频合成中取得了改进，生成了具有生动运动的连贯视频。项目页面：[https://flowzero-video.github.io/](https://flowzero-video.github.io/)
- en: '![Refer to caption](img/a8d7df2a9f04bdcadc6e6b20f0badf42.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a8d7df2a9f04bdcadc6e6b20f0badf42.png)'
- en: 'Figure 1: Zero-shot text-to-video generation. We present a new framework for
    text-to-video generation with exceptional temporal coherence, featuring realistic
    object movements, transformations, and background motion within the generated
    videos.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：零样本文本到视频生成。我们提出了一种具有卓越时间连贯性的新框架，能够在生成的视频中实现逼真的物体运动、变换和背景运动。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: In the field of AI-generated content, there has been growing interest in expanding
    the generative capabilities of pre-trained text-to-image (T2I) models to text-to-video
    (T2V) generation [[5](#bib.bib5), [11](#bib.bib11), [10](#bib.bib10), [12](#bib.bib12),
    [9](#bib.bib9), [33](#bib.bib33), [14](#bib.bib14), [20](#bib.bib20), [27](#bib.bib27)].
    Recent studies have introduced zero-shot T2V [[14](#bib.bib14), [12](#bib.bib12),
    [10](#bib.bib10)], which aims to adapt image diffusion models for video generation
    without additional training. These methods utilize the ability of image diffusion
    models, originally trained on static images, to generate frame sequences from
    video text prompts. However, generating coherent dynamic visual scenes in videos
    remains challenging due to the succinct and abstract nature of video text prompts.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI生成内容领域，对扩展预训练的文本到图像（T2I）模型到文本到视频（T2V）生成的兴趣日益增加[[5](#bib.bib5)，[11](#bib.bib11)，[10](#bib.bib10)，[12](#bib.bib12)，[9](#bib.bib9)，[33](#bib.bib33)，[14](#bib.bib14)，[20](#bib.bib20)，[27](#bib.bib27)]。近期的研究介绍了零样本T2V[[14](#bib.bib14)，[12](#bib.bib12)，[10](#bib.bib10)]，旨在将图像扩散模型调整用于视频生成，而无需额外的训练。这些方法利用了原本在静态图像上训练的图像扩散模型生成来自视频文本提示的帧序列。然而，由于视频文本提示的简洁和抽象性质，生成连贯的动态视觉场景仍然是一个挑战。
- en: Meanwhile, Large Language Models (LLMs) demonstrated their capability to generate
    layouts to control visual modules, especially image generation models [[32](#bib.bib32),
    [3](#bib.bib3), [19](#bib.bib19)]. These capabilities indicate a potential for
    LLMs to understand complex video prompts and generate fine-grained spatio-temporal
    layouts to guide video synthesis. However, generating spatio-temporal layouts
    for videos is more intricate, necessitating the LLMs to comprehend and illustrate
    how objects move and transform over time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，大型语言模型（LLMs）展示了其生成布局以控制视觉模块，尤其是图像生成模型的能力[[32](#bib.bib32)、[3](#bib.bib3)、[19](#bib.bib19)]。这些能力表明LLMs有潜力理解复杂的视频提示，并生成细粒度的时空布局以指导视频合成。然而，为视频生成时空布局更加复杂，要求LLMs理解并展示物体随时间的运动和变换。
- en: Furthermore, recent research [[12](#bib.bib12), [10](#bib.bib10)] in zero-shot
    T2V proposes utilizing LLMs to break down video text into frame-level descriptions.
    These descriptions are crafted to represent each moment or event within the video,
    guiding image diffusion models to generate semantic-coherent videos. However,
    these frame-level descriptions only capture the basic temporal semantics of video
    prompts, lacking detailed spatio-temporal information necessary for ensuring smooth
    object motion and consistent frame-to-frame coherence in videos. Additionally,
    representing global background movement to depict camera motion is crucial for
    immersive video generation [[30](#bib.bib30), [8](#bib.bib8)], which further complicates
    video generation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最近的研究[[12](#bib.bib12)、[10](#bib.bib10)] 提出了利用LLMs将视频文本分解为帧级描述。这些描述旨在表示视频中的每一个时刻或事件，指导图像扩散模型生成语义连贯的视频。然而，这些帧级描述仅捕捉了视频提示的基本时间语义，缺乏确保平滑物体运动和一致帧间连贯性的详细时空信息。此外，表示全局背景运动以描绘相机运动对于沉浸式视频生成至关重要[[30](#bib.bib30)、[8](#bib.bib8)]，这进一步复杂了视频生成过程。
- en: In this paper, we introduce FlowZero, a novel framework that integrates LLMs
    with image diffusion models to generate temporally-coherent videos from text prompts.
    FlowZero utilizes LLMs for comprehensive analysis and translating the video text
    prompt into a proposed structured Dynamic Scene Syntax (DSS). Unlike previous
    methods that only provide basic semantic descriptions, the DSS contains scene
    descriptions, layouts for foreground objects, and background motion patterns.
    Foreground layouts contain a series of bounding boxes that define each frame’s
    spatial arrangement and track changes in the positions and sizes of objects. This
    ensures that the coherent object motion and transformation align with the textual
    prompt. Additionally, FlowZero incorporates an iterative self-refinement process.
    This process effectively enhances the alignment between the generated layouts
    and the textual descriptions, specifically addressing inaccuracies such as spatial
    and temporal errors. In the self-refinement process, the generated layouts are
    iteratively compared and adjusted against the text through a feedback loop, ensuring
    a high fidelity and coherence of the spatio-temporal layouts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了FlowZero，这是一种将LLMs与图像扩散模型结合的全新框架，以从文本提示生成时间一致的视频。FlowZero利用LLMs进行全面分析，将视频文本提示转化为建议的结构化动态场景语法（DSS）。与之前仅提供基本语义描述的方法不同，DSS包含场景描述、前景物体的布局和背景运动模式。前景布局包含一系列边界框，定义了每一帧的空间排列，并跟踪物体位置和尺寸的变化。这确保了物体运动和变换与文本提示一致。此外，FlowZero还引入了迭代自我修正过程。该过程有效地提升了生成布局与文本描述之间的一致性，特别是针对空间和时间错误等不准确之处。在自我修正过程中，生成的布局通过反馈循环与文本进行迭代比较和调整，确保时空布局的高保真度和一致性。
- en: FlowZero prompts LLMs to predict background motion patterns to enhance temporal
    coherence and consistency, which can be used to control global scenes and camera
    motion in video frames. For instance, consider a text that describes a horse running
    from right to left, as shown in the middle example of Figure 1. The LLMs predict
    a corresponding camera motion, making the background move from left to right,
    enhancing the video’s immersiveness [[30](#bib.bib30), [8](#bib.bib8)]. The background
    motion pattern includes specific directions and speeds. We introduce a motion-guided
    noise shifting (MNS) technique, shifting the initial noise of each frame according
    to the predicted background motion direction and speed, leading to smoother video
    synthesis.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: FlowZero 促使 LLMs 预测背景运动模式，以增强时间一致性和一致性，这可以用来控制视频帧中的全局场景和摄像机运动。例如，考虑一个描述马从右向左跑的文本，如图
    1 中间示例所示。LLMs 预测出相应的摄像机运动，使背景从左向右移动，增强视频的沉浸感 [[30](#bib.bib30), [8](#bib.bib8)]。背景运动模式包括特定的方向和速度。我们引入了一种运动引导噪声位移（MNS）技术，根据预测的背景运动方向和速度调整每帧的初始噪声，从而实现更平滑的视频合成。
- en: FlowZero achieves a significant advancement in zero-shot text-to-video synthesis,
    utilizing the spatio-temporal planning ability of LLMs to generate detailed frame-by-frame
    syntax to enhance text-to-video generation. The fusion of these technologies within
    the FlowZero framework enables the generation of temporally-coherent, visually
    appealing videos directly from textual prompts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: FlowZero 在零-shot 文本到视频合成方面取得了显著进展，利用 LLMs 的时空规划能力生成详细的逐帧语法，以增强文本到视频的生成。FlowZero
    框架内这些技术的融合使得可以直接从文本提示生成时间上连贯、视觉上吸引人的视频。
- en: 'Our contributions are summarised as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce FlowZero, which uses LLMs to convert text into Dynamic Scene Syntax,
    leading to accurate frame-by-frame video instructions. The framework’s iterative
    self-refinement process ensures better alignment of spatio-temporal layouts with
    text prompts, enhancing video synthesis coherence and fidelity.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了 FlowZero，它利用大型语言模型（LLMs）将文本转换为动态场景语法，从而生成准确的逐帧视频指令。该框架的迭代自我优化过程确保了时空布局与文本提示的更好对齐，提高了视频合成的一致性和真实性。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The framework improves the global coherence of videos with adaptively controlled
    background motion through motion-guided noise shifting, increasing the realism
    of scene and camera motion.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该框架通过运动引导噪声位移，适应性地控制背景运动，从而改善视频的全局一致性，提高场景和摄像机运动的真实感。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Through extensive experiments and evaluations, we demonstrate FlowZero’s capability
    to generate temporally-coherent videos that accurately depict complex motions
    and transformations as described in textual prompts.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过大量实验和评估，我们展示了 FlowZero 生成时间上连贯的视频的能力，这些视频准确地描绘了文本提示中描述的复杂运动和变换。
- en: '![Refer to caption](img/646b3ad0f55a38544db3328ded07d12e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/646b3ad0f55a38544db3328ded07d12e.png)'
- en: 'Figure 2: Overview of FlowZero. Starting from a video prompt, we first instruct
    the LLMs (*i.e*., GPT4) to generate serial frame-by-frame syntax, including scene
    descriptions, foreground layouts, and background motion patterns. We employ an
    iterative self-refinement process to improve the generated spatio-temporal layouts.
    This process includes implementing a feedback loop where the LLM autonomously
    verifies and rectifies the spatial and temporal errors of the initial layouts.
    The loop continues until the confidence score $C$ coherent video frames.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：FlowZero 的概述。从视频提示开始，我们首先指示 LLMs（*例如*，GPT4）生成串行逐帧语法，包括场景描述、前景布局和背景运动模式。我们采用迭代自我优化过程来改进生成的时空布局。这个过程包括实施一个反馈循环，其中
    LLM 自主验证和纠正初始布局的空间和时间错误。循环会持续进行，直到置信度评分 $C$ 达到一致的视频帧。
- en: 2 Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Text-to-Video Generation.
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本到视频生成。
- en: Text-to-video (T2V) generation has evolved from initial variational autoencoders [[16](#bib.bib16),
    [17](#bib.bib17)] and GANs [[4](#bib.bib4)] to advanced diffusion-based techniques [[11](#bib.bib11),
    [5](#bib.bib5), [7](#bib.bib7), [33](#bib.bib33), [27](#bib.bib27), [12](#bib.bib12),
    [10](#bib.bib10), [14](#bib.bib14)], signifying a major advancement in synthesis
    methods. Although video diffusion models create high-quality visuals, training
    T2V models is often computationally expensive. This has led to the exploration
    of alternative approaches that balance efficiency and quality. Recent advancements [[14](#bib.bib14),
    [12](#bib.bib12), [10](#bib.bib10)] have explored leveraging image diffusion models
    pre-trained on static images [[19](#bib.bib19)] to sidestep the demanding training
    process for T2V. For example, Text-to-Video Zero [[14](#bib.bib14)] uses linear
    transformations and attention mechanisms to maintain video coherence. Free-Bloom [[12](#bib.bib12)]
    and DirecT2V [[10](#bib.bib10)], use Large Language Models (LLMs) to guide image
    diffusion models with descriptive scene prompts for sequential frames. However,
    these approaches struggle to capture the intricate object dynamics and background
    motion of videos, often leading to less expressive and coherent video generation.
    In contrast to previous methods that only provide semantic descriptions for each
    frame, FlowZero utilizes LLMs to reason a more comprehensive Dynamic Scene Syntax,
    delivering detailed, frame-by-frame guidance to enhance the temporal coherence
    and realism of T2V outputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到视频（T2V）生成已经从最初的变分自编码器[[16](#bib.bib16), [17](#bib.bib17)]和GANs[[4](#bib.bib4)]发展到先进的基于扩散的技术[[11](#bib.bib11),
    [5](#bib.bib5), [7](#bib.bib7), [33](#bib.bib33), [27](#bib.bib27), [12](#bib.bib12),
    [10](#bib.bib10), [14](#bib.bib14)]，标志着合成方法的重大进步。尽管视频扩散模型能创建高质量的视觉效果，但训练 T2V 模型通常计算成本高。这促使探索在效率和质量之间取得平衡的替代方法。最近的进展[[14](#bib.bib14),
    [12](#bib.bib12), [10](#bib.bib10)]探索了利用在静态图像上预训练的图像扩散模型[[19](#bib.bib19)]来规避
    T2V 的高昂训练过程。例如，Text-to-Video Zero[[14](#bib.bib14)]使用线性变换和注意力机制来保持视频的一致性。Free-Bloom[[12](#bib.bib12)]和DirecT2V[[10](#bib.bib10)]，利用大型语言模型（LLMs）通过描述性场景提示来指导图像扩散模型生成序列帧。然而，这些方法难以捕捉视频中的复杂物体动态和背景运动，通常导致生成的视频表现力和连贯性较差。与之前仅为每帧提供语义描述的方法不同，FlowZero
    利用 LLMs 推理出更全面的动态场景语法，提供详细的逐帧指导，以增强 T2V 输出的时间一致性和现实感。
- en: Visual Planning with Large Language Models.
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 利用大型语言模型进行视觉规划
- en: Advancements in text-to-image synthesis show that using an intermediate representation,
    such as a layout or segmentation map, greatly improves the alignment between generated
    images and their text descriptions [[18](#bib.bib18), [31](#bib.bib31)]. Various
    methodologies [[3](#bib.bib3), [32](#bib.bib32), [19](#bib.bib19), [29](#bib.bib29)]
    harness the vast world knowledge embedded in LLMs to craft spatial layouts to
    guide the image generation process. This has resulted in the creation of images
    with a reasonable spatial arrangement that closely matches the given textual prompts.
    For instance, LMD [[19](#bib.bib19)] introduces a novel, training-free approach,
    guiding a diffusion model with a unique controller to generate images based on
    layouts from LLMs. Similarly, LayoutGPT [[3](#bib.bib3)] employs a program-guided
    strategy, adapting LLMs to cater to layout-driven visual planning across diverse
    fields. Differing from these methods, FlowZero explores the spatio-temporal planning
    ability of LLMs for temporally-coherent video generation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像合成的进展表明，使用中间表示（如布局或分割图）大大改善了生成图像与文本描述之间的对齐[[18](#bib.bib18), [31](#bib.bib31)]。各种方法[[3](#bib.bib3),
    [32](#bib.bib32), [19](#bib.bib19), [29](#bib.bib29)]利用嵌入在 LLMs 中的丰富世界知识来创建空间布局以指导图像生成过程。这导致生成的图像具有合理的空间布局，紧密匹配给定的文本提示。例如，LMD[[19](#bib.bib19)]引入了一种新颖的、无训练的方法，通过独特的控制器引导扩散模型根据
    LLMs 的布局生成图像。类似地，LayoutGPT[[3](#bib.bib3)]采用程序引导策略，调整 LLMs 以适应各种领域的布局驱动视觉规划。与这些方法不同，FlowZero
    探索 LLMs 的时空规划能力，用于时间一致的视频生成。
- en: 3 Method
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'As shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ FlowZero: Zero-Shot
    Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax"), FlowZero initially
    leverages LLMs (e.g., GPT-4) to process a video prompt $\mathcal{T}$ are generated.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ FlowZero: Zero-Shot Text-to-Video
    Synthesis with LLM-Driven Dynamic Scene Syntax")所示，FlowZero 最初利用 LLMs（例如，GPT-4）处理视频提示$\mathcal{T}$并生成。'
- en: 3.1 Dynamic Scene Syntax Generation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 动态场景语法生成
- en: In this stage, we aim to use the LLMs, *i.e*., GPT-4 [[21](#bib.bib21)] to convert
    textual prompts into structured syntaxes for guiding the generation of temporally-coherent
    videos. These syntaxes include frame-by-frame descriptions, foreground object
    layouts, and background motion patterns.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们的目标是使用LLMs，即GPT-4 [[21](#bib.bib21)]，将文本提示转换为结构化语法，以指导生成时间一致的视频。这些语法包括逐帧描述、前景对象布局和背景运动模式。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scene Descriptions: Videos often depict a series of continuous events, such
    as the sunrise, beginning with the “lighting in the edge” and gradually “rising
    from the horizon”. We propose using LLMs to break down the video text prompt into
    detailed frame descriptions to depict these events. Given a video text prompt $\mathcal{T}$.
    These descriptions maintain consistent linguistic structures, ensuring that each
    prompt accurately conveys the visual content in a detailed manner. By providing
    a description for each frame, we can capture the temporal semantics of the video
    prompt.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景描述：视频通常描绘一系列连续事件，例如日出，开始时是“边缘的光线”，逐渐“从地平线上升起”。我们建议使用LLMs将视频文本提示分解为详细的帧描述，以描绘这些事件。给定一个视频文本提示 $\mathcal{T}$。这些描述保持一致的语言结构，确保每个提示准确传达视觉内容的详细信息。通过为每一帧提供描述，我们可以捕捉视频提示的时间语义。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Foreground Layout: While scene descriptions provide semantic details for each
    frame, these high-level constraints are not sufficient to accurately depict specific
    object motion and transformations. To achieve coherent object motion, we prompt
    LLMs to generate a sequence of frame-specific layouts $\{{L_{1},L_{2},\ldots,L_{N}}\}$
    denote the coordinates for the top-left and bottom-right vertices of the bounding
    box. These layouts provide more fine-grained conditions to ensure the foreground
    objects adhere to the visual and spatio-temporal cues the text provides.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前景布局：虽然场景描述为每一帧提供了语义细节，但这些高层约束不足以准确描绘特定对象的运动和变换。为了实现连贯的对象运动，我们提示LLMs生成一系列帧特定的布局
    $\{{L_{1},L_{2},\ldots,L_{N}}\}$，表示边界框的左上角和右下角顶点的坐标。这些布局提供了更精细的条件，以确保前景对象遵循文本提供的视觉和时空线索。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Background Motion: Background motion plays a crucial role in enhancing the
    global coherence of videos, especially when dynamic foreground objects are involved.
    For example, in a video showing a horse running to the left, synchronizing the
    camera motion with the horse’s direction can create a visually smooth effect,
    making the video more immersive and engaging [[30](#bib.bib30), [8](#bib.bib8)].
    To effectively simulate this, we first categorize potential background motion
    into eight moving directions: {left, right, up, down, left_up, left_down, right_up,
    right_down}, and include a “random” option for non-directional movement. We also
    define a motion speed that ranges from 0 (no movement) to 1.0 (rapid movement).
    We use LLMs to determine the most appropriate background motion direction and
    speed for each frame. This helps us align it with the foreground movements as
    described in the scene. By integrating background motions, we ensure global coherence
    and consistency in video sequences.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景运动：背景运动在提升视频的全球一致性方面起着至关重要的作用，尤其是在涉及动态前景对象时。例如，在展示马向左奔跑的视频中，将相机运动与马的方向同步可以创造出视觉上的流畅效果，使视频更加沉浸和引人入胜 [[30](#bib.bib30),
    [8](#bib.bib8)]。为了有效模拟这一点，我们首先将潜在的背景运动分为八个移动方向：{left, right, up, down, left_up,
    left_down, right_up, right_down}，并包括一个“随机”选项用于无方向的运动。我们还定义了一个运动速度范围，从0（无运动）到1.0（快速运动）。我们使用LLMs来确定每帧最合适的背景运动方向和速度。这帮助我们与场景中描述的前景运动对齐。通过整合背景运动，我们确保视频序列中的全球一致性和连贯性。
- en: Based on previous studies [[3](#bib.bib3), [29](#bib.bib29), [32](#bib.bib32),
    [12](#bib.bib12), [10](#bib.bib10)], we instruct LLMs to generate these syntaxes
    through direct commands. For example, we use prompts like “describe each frame”
    to create descriptions and “generate layouts for each scene” to generate foreground
    layouts. We provide an example in context to enhance the stability and effectiveness
    of LLMs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于以前的研究 [[3](#bib.bib3), [29](#bib.bib29), [32](#bib.bib32), [12](#bib.bib12),
    [10](#bib.bib10)]，我们通过直接命令来指导LLMs生成这些语法。例如，我们使用诸如“描述每一帧”的提示来创建描述，并使用“为每个场景生成布局”的提示来生成前景布局。我们提供一个上下文中的示例，以增强LLMs的稳定性和有效性。
- en: Iterative Self-Refinement.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 迭代自我优化。
- en: 'Due to the complex nature of reasoning in spatio-temporal dynamics, there may
    be discrepancies between the generated spatio-temporal layouts and the textual
    prompts. As illustrated in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ FlowZero:
    Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax"), the
    sun initially moves downward over time, which contradicts the video prompt “sun
    gradually rises”. Previous research has shown that LLMs can verify and correct
    generated texts or codes [[26](#bib.bib26), [2](#bib.bib2), [28](#bib.bib28),
    [15](#bib.bib15)]. Inspired by this, we propose an iterative self-refinement process
    to address potential misalignments between the initial spatio-temporal layouts
    $\{L_{1},L_{2},\ldots,L_{N}\}$.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '由于时空动态推理的复杂性，生成的时空布局与文本提示之间可能存在差异。如图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣
    FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax")所示，太阳最初向下移动，这与视频提示“太阳逐渐升起”相矛盾。以往的研究表明，LLMs可以验证和纠正生成的文本或代码[[26](#bib.bib26),
    [2](#bib.bib2), [28](#bib.bib28), [15](#bib.bib15)]。受此启发，我们提出了一种迭代自我修正过程，以解决初始时空布局$\{L_{1},L_{2},\ldots,L_{N}\}$之间可能的错位问题。'
- en: 3.2 Video Synthesis from Dynamic Scene Syntax
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 动态场景语法的视频合成
- en: 'In this section, we seek to generate coherent video frames based on the generated
    DSS. As shown in the right part of Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax"),
    beginning with a noise $x_{T}^{1}$ to match the background motion direction and
    speed predicted in the DSS generation stage. We will provide detailed information
    on Motion-guided noise shifting below. We employ a modified U-Net with cross attention,
    gated attention, and cross-frame attention mechanisms [[14](#bib.bib14), [10](#bib.bib10),
    [12](#bib.bib12)]. The cross-attention mechanism within the U-Net is designed
    to input scene descriptions, enabling the capture of diverse semantics for each
    frame. Simultaneously, the gated attention [[18](#bib.bib18)] inputs foreground
    layouts into the U-Net, managing the arrangement of objects across different frames.
    We then convert the self-attention layer in the U-Net of the image diffusion model
    into cross-frame attention [[12](#bib.bib12), [10](#bib.bib10), [14](#bib.bib14)],
    which performs attention between the query frame and previous frames.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们旨在基于生成的DSS生成连贯的视频帧。如图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ FlowZero:
    Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax")右侧所示，从噪声$x_{T}^{1}$开始，以匹配在DSS生成阶段预测的背景运动方向和速度。我们将在下面提供关于运动引导噪声移动的详细信息。我们采用了具有交叉注意力、门控注意力和跨帧注意力机制的改进U-Net[[14](#bib.bib14),
    [10](#bib.bib10), [12](#bib.bib12)]。U-Net中的交叉注意力机制旨在输入场景描述，捕捉每帧的多样化语义。同时，门控注意力[[18](#bib.bib18)]将前景布局输入U-Net，管理不同帧之间的对象排列。然后，我们将图像扩散模型中U-Net的自注意力层转换为跨帧注意力[[12](#bib.bib12),
    [10](#bib.bib10), [14](#bib.bib14)]，该机制在查询帧和前一帧之间执行注意力。'
- en: Motion-guided Noise Shifting.
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运动引导噪声移动。
- en: Previous method [[14](#bib.bib14)] performs a linear transformation on initial
    noises with fixed direction and speed to model global motion dynamics in video
    frames. In contrast, our approach allows LLMs to predict the background motion
    direction and speed adaptively for transforming noises, thereby significantly
    enhancing the global temporal coherence of videos.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方法[[14](#bib.bib14)]对初始噪声进行固定方向和速度的线性变换，以建模视频帧中的全局运动动态。相比之下，我们的方法允许LLMs自适应地预测背景运动方向和速度以转换噪声，从而显著提高视频的全局时间一致性。
- en: Given the predicted background motion $d$, a straightforward method is directly
    shifting the noise spatially for each frame. However, this often results in abrupt
    changes in low-level visual effects, such as color and lighting alterations in
    the video frames. To address this problem, we propose a technique to shift the
    phase of noises in the frequency domain [[13](#bib.bib13)]. This method preserves
    the amplitude component to maintain low-level visual effects while modulating
    the phase component to simulate spatial noise shifting [[6](#bib.bib6), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)], achieves smoother video frames.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 给定预测的背景运动$d$，一种直接的方法是为每一帧空间地移动噪声。然而，这常常导致低级视觉效果的突然变化，比如视频帧中的颜色和光照变化。为了解决这个问题，我们提出了一种在频率域内移动噪声相位的技术[[13](#bib.bib13)]。该方法保留了幅度分量，以维持低级视觉效果，同时调制相位分量以模拟空间噪声移动[[6](#bib.bib6),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]，实现更平滑的视频帧。
- en: 'Specifically, for each frame $i$ means the total diffusion step. The mathematical
    formulation is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对于每一帧 $i$，表示总的扩散步骤。数学公式如下：
- en: '|  | $1$2 |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\mathcal{F}$ would be set to {0,1}. When the background motion direction
    remains the same across frames, the index will increase linearly, resulting in
    smooth motion effects.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当背景运动方向在帧之间保持不变时，$\mathcal{F}$ 将设置为 {0,1}。此时索引将线性增加，从而产生平滑的运动效果。
- en: In scenarios with non-directional movements ($d=random$), such as “a goldfish
    swimming in a fish bowl,” setting a static background for all frames can be unrealistic.
    Our method addresses this by adding random disturbances at the phase components
    of all frequencies, simulating natural scene variability, and enhancing video
    realism.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在无方向运动的场景中（$d=random$），例如“金鱼在鱼缸中游动”，为所有帧设置静态背景可能不切实际。我们的方法通过在所有频率的相位分量中添加随机干扰，模拟自然场景的变化，增强视频的现实感。
- en: 'We perform the noise-shifting technique in the frequency domain has several
    advantages:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在频域中实施噪声平移技术具有若干优点：
- en: '1.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Our method allows for easy modification of moving directions by adjusting the
    direction multipliers, offering greater flexibility than direct space shifting.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的方法允许通过调整方向倍增因子来轻松修改移动方向，比直接空间平移提供了更大的灵活性。
- en: '2.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Since our technique operates in the frequency domain, it is more efficient and
    computationally less intensive than spatial domain transformations, particularly
    for handling high-resolution and long videos.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们的方法在频域中操作，它比空间域变换更高效、计算量更小，特别是在处理高分辨率和长视频时。
- en: '3.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We can simulate realistic motionless scenes by adding random disturbances to
    the noises.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过在噪声中添加随机干扰来模拟逼真的静止场景。
- en: 4 Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: '![Refer to caption](img/93744ae5ac180b14c82b16f1e0a32fdd.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/93744ae5ac180b14c82b16f1e0a32fdd.png)'
- en: 'Figure 3: Qualitative comparison. Our method can capture detailed object motion
    to generate temporally coherent frame sequences.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：定性比较。我们的方法能够捕捉详细的物体运动，以生成时间上连贯的帧序列。
- en: 4.1 Implementation Details
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实施细节
- en: We utilize the GLIGEN [[18](#bib.bib18)] as the base image diffusion model,
    which is pre-trained to generate images adhering to a layout. We employ GPT-4 [[21](#bib.bib21)]
    to reason Dynamic Scene Syntax (DSS). In our tests, we generate $N$ of self-refinement
    as 3 and the maximum iteration as 5. All experiments are conducted on a single
    NVIDIA V100 GPU.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 GLIGEN [[18](#bib.bib18)] 作为基础图像扩散模型，该模型经过预训练以生成符合布局的图像。我们使用 GPT-4 [[21](#bib.bib21)]
    推理动态场景语法 (DSS)。在我们的测试中，我们生成 $N$ 次自我优化，设置为 3 次，最大迭代次数为 5 次。所有实验均在单个 NVIDIA V100
    GPU 上进行。
- en: 4.2 Comparisons with Baseline Methods
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 与基准方法的比较
- en: Qualitative Comparison
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定性比较
- en: '![Refer to caption](img/d8a0e46c91a46cec3618f110b80a6559.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d8a0e46c91a46cec3618f110b80a6559.png)'
- en: 'Figure 4: Qualitative comparison. Our method can model intricate object transformations
    representing narrative structures in the video prompt.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：定性比较。我们的方法能够建模复杂的物体变换，代表视频提示中的叙事结构。
- en: 'Table 1: Quantitative Results. We perform automatic metrics,   i.e., CLIP score
    and user study, to validate the effectiveness.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：定量结果。我们进行自动化指标评估，即 CLIP 分数和用户研究，以验证效果。
- en: '|  |  | Automatic Metric | User Study |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 自动指标 | 用户研究 |  |'
- en: '| Method | Training-Free | CLIP Score$\uparrow$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 无需训练 | CLIP 分数$\uparrow$ |'
- en: '| AnimateDiff [[5](#bib.bib5)] |  | 0.244 | 3.15 | 2.75 | 2.97 | 3.42 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| AnimateDiff [[5](#bib.bib5)] |  | 0.244 | 3.15 | 2.75 | 2.97 | 3.42 |'
- en: '| VideoFusion [[20](#bib.bib20)] |  | 0.264 | 3.38 | 2.92 | 3.11 | 3.17 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| VideoFusion [[20](#bib.bib20)] |  | 0.264 | 3.38 | 2.92 | 3.11 | 3.17 |'
- en: '| T2V-Z [[14](#bib.bib14)] | ✓ | 0.245 | 3.29 | 2.99 | 3.03 | 3.19 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| T2V-Z [[14](#bib.bib14)] | ✓ | 0.245 | 3.29 | 2.99 | 3.03 | 3.19 |'
- en: '| DirecT2V [[10](#bib.bib10)] | ✓ | 0.244 | 3.39 | 3.29 | 2.52 | 2.97 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| DirecT2V [[10](#bib.bib10)] | ✓ | 0.244 | 3.39 | 3.29 | 2.52 | 2.97 |'
- en: '| Ours | ✓ | 0.267 | 4.57 | 4.58 | 4.40 | 2.00 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | ✓ | 0.267 | 4.57 | 4.58 | 4.40 | 2.00 |'
- en: '![Refer to caption](img/2d3c3d6bae2c5940526b96e20a13f595.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2d3c3d6bae2c5940526b96e20a13f595.png)'
- en: 'Figure 5: Ablation studies of the effectiveness of FlowZero. (A) cross-frame
    attention, (B) scene descriptions, (C) foreground layouts, (D) motion-guided noise
    shifting.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：FlowZero 效果的消融研究。 (A) 跨帧注意力，(B) 场景描述，(C) 前景布局，(D) 运动引导噪声平移。
- en: '![Refer to caption](img/93aec0bd11bb3a24ff1503dcd24a5b83.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/93aec0bd11bb3a24ff1503dcd24a5b83.png)'
- en: 'Figure 6: Analysis of the self-refinement process. The self-refinement mechanism
    verifies and rectifies spatial and temporal alignment between layouts and video
    prompts.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：自我精炼过程分析。自我精炼机制验证和纠正布局与视频提示之间的时空对齐。
- en: 'In our qualitative comparative analysis, we compare videos generated using
    our FlowZero with several benchmark methods: zero-shot based methods, T2V-Z [[14](#bib.bib14)],
    DirecT2V [[10](#bib.bib10)], and training-based methods AnimateDiff [[5](#bib.bib5)],
    VideoFusion [[20](#bib.bib20)]. We assess the performance in three scenarios:
    basic object motion rendering, multiple object motion depiction, and complex object
    transformations.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的定性比较分析中，我们将FlowZero生成的视频与几种基准方法进行比较：零-shot基础方法T2V-Z [[14](#bib.bib14)]、DirecT2V
    [[10](#bib.bib10)]，以及基于训练的方法AnimateDiff [[5](#bib.bib5)]、VideoFusion [[20](#bib.bib20)]。我们评估了三种场景中的性能：基本物体运动渲染、多个物体运动描绘和复杂物体变换。
- en: 'In our initial assessment, shown in Figure [3](#S4.F3 "Figure 3 ‣ 4 Experiments
    ‣ FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax"),
    we analyze videos generated from prompts featuring basic object motion. FlowZero
    effectively demonstrates the ability to depict smooth object motion, particularly
    showcasing a butterfly’s departure from a flower. However, other zero-shot techniques,
    such as T2V-Z and DirecT2V, only capture temporal semantics and struggle to model
    the coherent object motion. AnimateDiff and VideoFusion were trained on extensive
    video-text data [[1](#bib.bib1)] and exhibit temporal frame coherence. However,
    they fall short in rendering nuanced motion details, resulting in slightly stilted
    animations. In scenarios involving multiple objects with designated movements,
    also presented in the right of Figure [3](#S4.F3 "Figure 3 ‣ 4 Experiments ‣ FlowZero:
    Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax"), FlowZero
    continues to excel, accurately animating specific objects and motion defined in
    the text prompts. Other methods struggle to precisely replicate the specified
    objects and movements, often resulting in a less accurate portrayal. In Figure [4](#S4.F4
    "Figure 4 ‣ Qualitative Comparison ‣ 4.2 Comparisons with Baseline Methods ‣ 4
    Experiments ‣ FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic
    Scene Syntax"), we compare all methods of generating videos from prompts that
    describe complex object transformations. FlowZero distinguishes itself by vividly
    rendering transformations, such as a tranquil volcano erupting. Other methods
    do not effectively translate these temporal dynamics, leading to less coherent
    visual transformations.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的初步评估中，如图[3](#S4.F3 "Figure 3 ‣ 4 Experiments ‣ FlowZero: Zero-Shot Text-to-Video
    Synthesis with LLM-Driven Dynamic Scene Syntax")所示，我们分析了从基本物体运动提示生成的视频。FlowZero有效地展示了平滑物体运动的能力，尤其是蝴蝶离开花朵的场景。然而，其他零-shot技术，如T2V-Z和DirecT2V，仅捕捉时间语义，难以模拟连贯的物体运动。AnimateDiff和VideoFusion在大量视频-文本数据[[1](#bib.bib1)]上训练，并展现了时间帧的连贯性。然而，它们在渲染细致的运动细节上有所不足，导致动画稍显僵硬。在涉及多个物体和指定运动的场景中，如图[3](#S4.F3
    "Figure 3 ‣ 4 Experiments ‣ FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven
    Dynamic Scene Syntax")右侧所示，FlowZero依然表现出色，准确地动画了文本提示中定义的特定物体和运动。其他方法在精确复制指定物体和运动方面表现不佳，常导致表现不够准确。在图[4](#S4.F4
    "Figure 4 ‣ Qualitative Comparison ‣ 4.2 Comparisons with Baseline Methods ‣ 4
    Experiments ‣ FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic
    Scene Syntax")中，我们比较了所有基于提示生成复杂物体变换的视频的方法。FlowZero通过生动地渲染变换（如宁静的火山喷发）脱颖而出。其他方法未能有效地转化这些时间动态，导致视觉变换不够连贯。'
- en: By utilizing LLMs to plan the spatio-temporal syntax as guidance for the diffusion
    model, FlowZero surpasses other methods in text-prompted video generation. Its
    superior performance is particularly noticeable in the accurate motion of multiple
    objects and intricate object transformations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用LLMs来规划时空语法作为扩散模型的指导，FlowZero 在文本提示的视频生成中超越了其他方法。其卓越的性能在多个物体的准确运动和复杂物体变换中尤为明显。
- en: Quantitative Comparison
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定量比较
- en: 'As shown in Table [1](#S4.T1 "Table 1 ‣ Qualitative Comparison ‣ 4.2 Comparisons
    with Baseline Methods ‣ 4 Experiments ‣ FlowZero: Zero-Shot Text-to-Video Synthesis
    with LLM-Driven Dynamic Scene Syntax"), we first compare our methods with other
    four baseline methods, *i.e*., AnimateDiff [[5](#bib.bib5)], VideoFusion [[20](#bib.bib20)],
    T2V-Z [[14](#bib.bib14)], DirecT2V [[10](#bib.bib10)] using CLIP score metrics [[25](#bib.bib25)].
    The CLIP metrics measure the semantic similarity between the text and video frames.
    Our method achieves the highest performance by prompting LLMs to deduce more semantics
    from both spatial and temporal dimensions.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](#S4.T1 "Table 1 ‣ Qualitative Comparison ‣ 4.2 Comparisons with Baseline
    Methods ‣ 4 Experiments ‣ FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven
    Dynamic Scene Syntax")所示，我们首先将我们的方法与另外四种基准方法进行比较，即*AnimateDiff* [[5](#bib.bib5)]、*VideoFusion*
    [[20](#bib.bib20)]、*T2V-Z* [[14](#bib.bib14)]、*DirecT2V* [[10](#bib.bib10)]，使用CLIP分数指标[[25](#bib.bib25)]。CLIP指标测量文本与视频帧之间的语义相似性。通过促使LLMs从空间和时间维度推导更多的语义，我们的方法表现出最高的性能。'
- en: Due to the complexity of quantitatively evaluating videos with intricate temporal
    dynamics, we conducted a user study to validate the effectiveness of our method.
    We recruited 20 people from academia and industry to conduct this survey. We ask
    users to provide feedback on the semantic accuracy, temporal coherence, and video
    quality of the videos generated by five different methods. It is evident that
    users prefer our method and achieve better results, surpassing even training-based
    methods, *e.g*., AnimateDiff [[5](#bib.bib5)] and VideoFusio [[20](#bib.bib20)].
    Furthermore, our methods achieve significant improvement over other zero-shot
    methods, *e.g*., T2V-Zero [[14](#bib.bib14)] and DirecT2V [[10](#bib.bib10)] on
    temporal coherence, which validates the effectiveness of our approach.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对具有复杂时间动态的视频进行定量评估的复杂性，我们进行了用户研究以验证我们方法的有效性。我们从学术界和工业界招募了20名参与者来进行此调查。我们要求用户对五种不同方法生成的视频的语义准确性、时间一致性和视频质量提供反馈。显然，用户更喜欢我们的方法，并且取得了更好的结果，甚至超越了基于训练的方法，如*AnimateDiff*
    [[5](#bib.bib5)]和*VideoFusion* [[20](#bib.bib20)]。此外，我们的方法在时间一致性上相较于其他零样本方法，如*T2V-Zero*
    [[14](#bib.bib14)]和*DirecT2V* [[10](#bib.bib10)]，取得了显著的改进，这验证了我们方法的有效性。
- en: 4.3 Ablation Study
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: Effectiveness of FlowZero
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FlowZero的有效性
- en: 'In Figure [5](#S4.F5 "Figure 5 ‣ Qualitative Comparison ‣ 4.2 Comparisons with
    Baseline Methods ‣ 4 Experiments ‣ FlowZero: Zero-Shot Text-to-Video Synthesis
    with LLM-Driven Dynamic Scene Syntax"), we conduct a comprehensive ablation study
    to validate the effectiveness of key components in FlowZero. This includes cross-frame
    attention, scene descriptions, foreground object layouts, and background motion
    for noise shifting. We begin with a baseline model that employs cross-frame attention
    to adapt U-Net, feeding it original video prompts alongside independent random
    noise for each frame. Row #1 of Figure [5](#S4.F5 "Figure 5 ‣ Qualitative Comparison
    ‣ 4.2 Comparisons with Baseline Methods ‣ 4 Experiments ‣ FlowZero: Zero-Shot
    Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax") demonstrates that
    the baseline generates videos with basic semantics like Ironman and surfing but
    fails to capture detailed object motion. In row#2, we replace the video prompt
    with generated scene descriptions for each frame, similar to previous methods
    Free-Bloom [[12](#bib.bib12)] and DirecT2V [[10](#bib.bib10)]. However, we found
    that merely using temporal semantics resulted in a lack of coherent object motion,
    resulting in inconsistencies across frames. Instead, in row#3, by adding the layout
    to constrain the arrangement of foreground objects, we can clearly capture the
    coherent motion of the main object, such as “from left to right” and “rises”.
    However, the video frames still display temporal inconsistency, such as in color,
    lighting, and global scene. In row#4, we experimented with removing the cross-frame
    attention from U-Net, which means relying solely on a pure image diffusion model [[18](#bib.bib18)].
    This modification resulted in a lack of inconsistencies in the representation
    of objects and backgrounds across frames, even though the layouts guide the object
    motion. Finally, by utilizing our motion-guided noise shifting technique in row#5,
    we can smoothly control the motion direction and speed in the background, resulting
    in a coherent global scene.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [5](#S4.F5 "图 5 ‣ 质量比较 ‣ 4.2 与基线方法的比较 ‣ 4 实验 ‣ FlowZero: 零样本文本到视频合成与 LLM
    驱动的动态场景语法") 中，我们进行了一项全面的消融研究，以验证 FlowZero 中关键组件的有效性。这包括跨帧注意力、场景描述、前景物体布局和用于噪声转移的背景运动。我们从一个基线模型开始，该模型利用跨帧注意力对
    U-Net 进行适配，将原始视频提示与每帧独立的随机噪声一起输入。图 [5](#S4.F5 "图 5 ‣ 质量比较 ‣ 4.2 与基线方法的比较 ‣ 4 实验
    ‣ FlowZero: 零样本文本到视频合成与 LLM 驱动的动态场景语法") 的第1行展示了基线生成的具有基本语义的视频，例如钢铁侠和冲浪，但未能捕捉详细的物体运动。在第2行中，我们将视频提示替换为每帧生成的场景描述，类似于之前的方法
    Free-Bloom [[12](#bib.bib12)] 和 DirecT2V [[10](#bib.bib10)]。然而，我们发现仅仅使用时间语义会导致物体运动不连贯，从而导致帧间不一致。相反，在第3行中，通过添加布局来约束前景物体的排列，我们可以清晰地捕捉到主物体的一致运动，例如“从左到右”和“上升”。然而，视频帧仍显示出时间上的不一致，例如颜色、光照和整体场景。在第4行中，我们尝试从
    U-Net 中移除跨帧注意力，这意味着完全依赖于纯图像扩散模型 [[18](#bib.bib18)]。这种修改虽然使得物体和背景在帧间的表示一致，但即使布局指导了物体运动，仍然存在不一致。最后，通过在第5行中利用我们的运动引导噪声转移技术，我们可以平滑地控制背景中的运动方向和速度，从而实现一致的全球场景。'
- en: Effectiveness of Self-Refinement Process
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自我完善过程的有效性
- en: 'We present two examples in Figure [6](#S4.F6 "Figure 6 ‣ Qualitative Comparison
    ‣ 4.2 Comparisons with Baseline Methods ‣ 4 Experiments ‣ FlowZero: Zero-Shot
    Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax") to illustrate the
    effectiveness of our self-refinement process in correcting spatial and temporal
    errors of initial layouts. In the first example, the video prompt describes “a
    boat moving in the river.” However, the initial generated layout incorrectly places
    the boat above the river. The boat is correctly positioned within the river through
    self-refinement, aligning with the prompt’s spatial arrangement. The second example
    describes a plane ascending into the sky. Initially, the size of the plane remains
    constant across all layouts over time, contradicting the expectation that it should
    appear smaller as it ascends. After refinement, the size of the plane decreases
    in later frames, accurately reflecting the prompt’s temporal dynamics.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[6](#S4.F6 "图 6 ‣ 定性比较 ‣ 4.2 与基准方法的比较 ‣ 4 实验 ‣ FlowZero: 基于LLM驱动的动态场景语法的零-shot文本到视频合成")中展示了两个例子，以说明我们自我修正过程在纠正初始布局的空间和时间错误方面的有效性。在第一个例子中，视频提示描述了“一只船在河中移动”。然而，初始生成的布局错误地将船放置在了河上方。通过自我修正，船被正确地放置在河中，与提示的空间排列一致。第二个例子描述了一架飞机升向天空。最初，飞机在所有布局中大小保持不变，这与其应在上升时显得更小的预期相悖。经过修正后，飞机在后续帧中的大小减小，准确反映了提示的时间动态。'
- en: 'To quantitatively evaluate the effectiveness of the self-refinement process,
    we propose a benchmark comprising four spatio-temporal layout generation tasks.
    These tasks include multiple objects, object movements (left, right, up, down),
    size changes (big to small or small to big), and visibility variations (half or
    quarter visibility). Each task includes 20 programmatically generated prompts,
    assessed using a rule-based metric. For instance, we calculate the change in object
    area across frames to evaluate size changes. The results are displayed in Table [2](#S4.T2
    "Table 2 ‣ Effectiveness of Self-Refinement Process ‣ 4.3 Ablation Study ‣ 4 Experiments
    ‣ FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax").
    We observe LLMs initially struggle to generate precise results that accurately
    reflect specific temporal changes, including object movement, size variation,
    and visibility. Moreover, through our self-refinement process, we noted a notable
    improvement in accuracy, particularly in tasks temporal visibility (from 61% to
    78%). The self-refinement mechanism consistently enhances spatial-temporal layout
    generation, effectively aligning the generated content with specific temporal
    requirements. These experiments confirm the effectiveness of our self-refinement
    process in improving the spatial-temporal coherence of the generated scenes.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '为了定量评估自我修正过程的有效性，我们提出了一个包含四个时空布局生成任务的基准。这些任务包括多个对象、对象运动（左、右、上、下）、大小变化（大到小或小到大）以及可见性变化（半可见或四分之一可见）。每个任务包括20个程序生成的提示，通过基于规则的指标进行评估。例如，我们计算帧间对象区域的变化以评估大小变化。结果显示在表[2](#S4.T2
    "表 2 ‣ 自我修正过程的有效性 ‣ 4.3 消融研究 ‣ 4 实验 ‣ FlowZero: 基于LLM驱动的动态场景语法的零-shot文本到视频合成")中。我们观察到LLMs最初在生成准确反映特定时间变化（包括对象运动、大小变化和可见性）的结果方面存在困难。此外，通过我们的自我修正过程，我们注意到准确性有了显著提高，特别是在时间可见性任务（从61%提升至78%）。自我修正机制持续提升时空布局生成的准确性，有效对齐生成内容与特定时间要求。这些实验验证了我们的自我修正过程在提高生成场景的时空一致性方面的有效性。'
- en: 'Table 2: Quantitative analysis of the self-refinement process.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：自我修正过程的定量分析。
- en: '| Method | Objects$\uparrow$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 对象$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| w/o self-refine | 90% | 83% | 80% | 61% |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 不使用自我修正 | 90% | 83% | 80% | 61% |'
- en: '| w/ self-refine | 96% | 93% | 93% | 78% |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 使用自我修正 | 96% | 93% | 93% | 78% |'
- en: 5 Conclusion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we have investigated leveraging the spatial-temporal planning
    ability of Large Language Models to guide temporally-coherent text-to-video generation
    with image diffusion models. We prompt LLMs to generate comprehensive Dynamic
    Scene Syntax, including scene descriptions, layouts for foreground objects, and
    background motion patterns. The foreground layouts ensure coherent object motions
    and object transformations described in the prompt. Furthermore, the introduced
    iterative self-refinement can enhance the alignment between the generated spatio-temporal
    layouts and the textual descriptions, specifically addressing inaccuracies such
    as spatial and temporal errors. The background motion can be controlled by motion-guided
    noise shifting, leading to smoother video synthesis and a coherent global scene.
    We have performed extensive qualitative and quantitative experiments along with
    ablation studies to validate the effectiveness of our FlowZero framework. These
    experiments validate that FlowZero can generate temporally-coherent videos from
    complex video prompts.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了利用大型语言模型的时空规划能力，指导具有图像扩散模型的时间一致性文本到视频生成。我们促使LLMs生成全面的动态场景语法，包括场景描述、前景对象的布局以及背景运动模式。前景布局确保了对象运动和提示中描述的对象变换的一致性。此外，引入的迭代自我精炼可以增强生成的时空布局与文本描述之间的对齐，特别是解决空间和时间误差等不准确之处。背景运动可以通过运动引导的噪声移动进行控制，从而实现更平滑的视频合成和一致的全局场景。我们进行了广泛的定性和定量实验以及消融研究，以验证我们的FlowZero框架的有效性。这些实验验证了FlowZero可以从复杂的视频提示中生成时间一致的视频。
- en: References
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bain et al. [2021] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.
    Frozen in time: A joint video and image encoder for end-to-end retrieval. In *2021
    IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC,
    Canada, October 10-17, 2021*, pages 1708–1718\. IEEE, 2021.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bain等人 [2021] 麦克斯·贝恩、阿尔沙·纳格拉尼、古尔·瓦罗尔和安德鲁·齐瑟曼。定格在时间中：用于端到端检索的联合视频和图像编码器。在*2021
    IEEE/CVF 国际计算机视觉大会，ICCV 2021，蒙特利尔，QC，加拿大，2021年10月10-17日*，第1708–1718页。IEEE, 2021。
- en: Dhuliawala et al. [2023] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta
    Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces
    hallucination in large language models. *arXiv preprint arXiv:2309.11495*, 2023.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhuliawala等人 [2023] 谢赫扎德·杜利亚瓦拉、莫赫塔巴·科梅利、徐静、罗伯塔·雷利安努、李娴、阿斯利·切利基尔马兹和杰森·韦斯顿。验证链减少大型语言模型中的幻觉。*arXiv
    预印本 arXiv:2309.11495*, 2023。
- en: 'Feng et al. [2023] Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun R.
    Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt:
    Compositional visual planning and generation with large language models. *CoRR*,
    abs/2305.15393, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng等人 [2023] 冯伟熙、朱婉容、傅子睿、贾普伦、阿尔君·R·阿库拉、何学海、苏加托·巴苏、王欣·艾瑞克和威廉·杨·王。LayoutGPT：利用大型语言模型进行组合视觉规划和生成。*CoRR*,
    abs/2305.15393, 2023。
- en: Goodfellow et al. [2014] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    Generative adversarial networks. *CoRR*, abs/1406.2661, 2014.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow等人 [2014] 伊恩·J·古德费洛、让·普热特-阿巴迪、梅赫迪·米尔扎、邢炳、戴维·沃德-法利、谢尔吉尔·奥扎尔、亚伦·C·库维尔和约书亚·本吉奥。生成对抗网络。*CoRR*,
    abs/1406.2661, 2014。
- en: 'Guo et al. [2023] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua
    Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion
    models without specific tuning. *CoRR*, abs/2307.04725, 2023.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人 [2023] 余伟国、杨策元、饶安逸、王耀辉、乔宇、大华林和戴博。Animatediff：无需特定调整即可对个性化文本到图像扩散模型进行动画化。*CoRR*,
    abs/2307.04725, 2023。
- en: Hansen and Hess [2007] Bruce C Hansen and Robert F Hess. Structural sparseness
    and spatial phase alignment in natural scenes. *JOSA A*, 24(7):1873–1885, 2007.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansen和Hess [2007] 布鲁斯·C·汉森和罗伯特·F·赫斯。自然场景中的结构稀疏性和空间相位对齐。*JOSA A*, 24(7):1873–1885,
    2007。
- en: He et al. [2022] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng
    Chen. Latent video diffusion models for high-fidelity video generation with arbitrary
    lengths. *arXiv preprint arXiv:2211.13221*, 2022.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人 [2022] 余颖青、杨天宇、张勇、单颖和陈启峰。用于高保真视频生成的潜在视频扩散模型，支持任意长度。*arXiv 预印本 arXiv:2211.13221*,
    2022。
- en: 'Heimann et al. [2019] Katrin Heimann, Sebo Uithol, Marta Calbi, Maria Alessandra
    Umiltà, Michele Guerra, Joerg Fingerhut, and Vittorio Gallese. Embodying the camera:
    An eeg study on the effect of camera movements on film spectators sensorimotor
    cortex activation. *PloS one*, 14(3):e0211026, 2019.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heimann et al. [2019] 卡特琳·海曼、塞博·乌伊索尔、玛尔塔·卡尔比、玛利亚·亚历山德拉·乌米尔塔、米歇尔·古埃拉、约尔格·芬戈特、和维托里奥·加列斯。体现摄像机：关于摄像机移动对电影观众传感运动皮层激活影响的脑电图研究。*PloS
    one*，14(3)：e0211026，2019年。
- en: 'Ho et al. [2022] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi
    Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J.
    Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion
    models. *CoRR*, abs/2210.02303, 2022.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho et al. [2022] 乔纳森·霍、威廉·陈、奇特万·萨哈里亚、杰伊·黄、瑞奇·高、阿列克谢·A·格里岑科、迪德里克·P·金马、本·普尔、穆罕默德·诺鲁兹、戴维·J·弗利特、和蒂姆·萨利曼斯。Imagen
    video：使用扩散模型生成高清晰度视频。*CoRR*，abs/2210.02303，2022年。
- en: Hong et al. [2023a] Susung Hong, Junyoung Seo, Sunghwan Hong, Heeseong Shin,
    and Seungryong Kim. Large language models are frame-level directors for zero-shot
    text-to-video generation. *CoRR*, abs/2305.14330, 2023a.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong et al. [2023a] 洪寿星、徐俊英、洪成焕、申熙成、和金胜龙。大型语言模型是零样本文本到视频生成的帧级导演。*CoRR*，abs/2305.14330，2023a。
- en: 'Hong et al. [2023b] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie
    Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers.
    In *The Eleventh International Conference on Learning Representations, ICLR 2023,
    Kigali, Rwanda, May 1-5, 2023*. OpenReview.net, 2023b.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong et al. [2023b] 洪文逸、丁明、郑文迪、刘兴汉、和唐洁。Cogvideo：通过变换器进行大规模预训练以生成文本到视频。在*第十一届国际学习表示会议，ICLR
    2023，基加利，卢旺达，2023年5月1-5日*。OpenReview.net，2023b。
- en: 'Huang et al. [2023] Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu,
    and Sibei Yang. Free-bloom: Zero-shot text-to-video generator with LLM director
    and LDM animator. *CoRR*, abs/2309.14494, 2023.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2023] 黄汉卓、冯宇凡、石成、徐兰、余景怡、和杨思贝。Free-bloom：具有LLM导演和LDM动画师的零样本文本到视频生成器。*CoRR*，abs/2309.14494，2023年。
- en: Jähne [2005] Bernd Jähne. *Digital image processing*. Springer Science & Business
    Media, 2005.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jähne [2005] 贝恩德·耶内。*数字图像处理*。施普林格科学与商业媒体，2005年。
- en: 'Khachatryan et al. [2023] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan,
    Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero:
    Text-to-image diffusion models are zero-shot video generators. *CoRR*, abs/2303.13439,
    2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khachatryan et al. [2023] 列冯·赫恰特里安、安德拉尼克·莫夫西扬、瓦赫拉姆·塔代沃斯扬、罗伯托·亨施尔、张阳·王、尚特·纳瓦萨尔迪安、和汉弗雷·石。Text2video-zero：文本到图像扩散模型是零样本视频生成器。*CoRR*，abs/2303.13439，2023年。
- en: Kim et al. [2023] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models
    can solve computer tasks. *CoRR*, abs/2303.17491, 2023.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2023] 金根宇、皮埃尔·巴尔迪、和斯蒂芬·麦克艾勒。语言模型可以解决计算机任务。*CoRR*，abs/2303.17491，2023年。
- en: Li et al. [2017] Yitong Li, Martin Renqiang Min, Dinghan Shen, David E. Carlson,
    and Lawrence Carin. Video generation from text. *CoRR*, abs/1710.00421, 2017.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2017] 李怡通、任仁强、沈丁汉、戴维·E·卡尔森、和劳伦斯·卡林。基于文本的视频生成。*CoRR*，abs/1710.00421，2017年。
- en: 'Li et al. [2019] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin
    Wu, Lawrence Carin, David E. Carlson, and Jianfeng Gao. Storygan: A sequential
    conditional GAN for story visualization. In *IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019*, pages
    6329–6338\. Computer Vision Foundation / IEEE, 2019.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2019] 李怡通、甘哲、沈也龙、刘静静、程宇、吴跃新、劳伦斯·卡林、戴维·E·卡尔森、和高剑锋。Storygan：用于故事可视化的序列条件GAN。发表于*IEEE计算机视觉与模式识别会议，CVPR
    2019，长滩，加利福尼亚，美国，2019年6月16-20日*，第6329–6338页。计算机视觉基金会 / IEEE，2019年。
- en: 'Li et al. [2023] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei
    Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. GLIGEN: open-set grounded text-to-image
    generation. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023*, pages 22511–22521\. IEEE,
    2023.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023] 李宇恒、刘浩天、吴青阳、穆方舟、杨建伟、高剑锋、李春远、和李永宰。GLIGEN：开放集基础文本到图像生成。发表于*IEEE/CVF计算机视觉与模式识别会议，CVPR
    2023，温哥华，加拿大，2023年6月17-24日*，第22511–22521页。IEEE，2023年。
- en: 'Lian et al. [2023] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded
    diffusion: Enhancing prompt understanding of text-to-image diffusion models with
    large language models. *CoRR*, abs/2305.13655, 2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lian et al. [2023] 连龙、李博义、亚当·雅拉、和特雷弗·达雷尔。基于语言模型的扩散：通过大型语言模型增强文本到图像扩散模型的提示理解。*CoRR*，abs/2305.13655，2023年。
- en: 'Luo et al. [2023] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang
    Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed
    diffusion models for high-quality video generation. In *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June
    17-24, 2023*, pages 10209–10218\. IEEE, 2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. [2023] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang
    Wang, Yujun Shen, Deli Zhao, Jingren Zhou, 和 Tieniu Tan. Videofusion：用于高质量视频生成的分解扩散模型。在*IEEE/CVF计算机视觉与模式识别会议，CVPR
    2023，加拿大温哥华，2023年6月17-24日*，第10209–10218页。IEEE，2023年。
- en: OpenAI [2023] OpenAI. GPT-4 technical report. *CoRR*, abs/2303.08774, 2023.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. GPT-4技术报告。*CoRR*，abs/2303.08774，2023年。
- en: Oppenheim et al. [1979] A Oppenheim, Jae Lim, Gary Kopec, and SC Pohlig. Phase
    in speech and pictures. In *ICASSP’79\. IEEE International Conference on Acoustics,
    Speech, and Signal Processing*, pages 632–637\. IEEE, 1979.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oppenheim et al. [1979] A Oppenheim, Jae Lim, Gary Kopec, 和 SC Pohlig. 语音和图片中的相位。在*ICASSP’79.
    IEEE国际声学、语音和信号处理会议*，第632–637页。IEEE，1979年。
- en: Oppenheim and Lim [1981] Alan V Oppenheim and Jae S Lim. The importance of phase
    in signals. *Proceedings of the IEEE*, 69(5):529–541, 1981.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oppenheim and Lim [1981] Alan V Oppenheim 和 Jae S Lim. 信号中相位的重要性。*IEEE汇刊*，69(5):529–541，1981年。
- en: Piotrowski and Campbell [1982] Leon N Piotrowski and Fergus W Campbell. A demonstration
    of the visual importance and flexibility of spatial-frequency amplitude and phase.
    *Perception*, 11(3):337–346, 1982.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piotrowski and Campbell [1982] Leon N Piotrowski 和 Fergus W Campbell. 空间频率幅度和相位的视觉重要性和灵活性的演示。*知觉*，11(3):337–346，1982年。
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, 等. 从自然语言监督中学习可转移的视觉模型。在*国际机器学习会议*，第8748–8763页。PMLR，2021年。
- en: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning. In *Thirty-seventh Conference on Neural Information Processing Systems*,
    2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    R Narasimhan, 和 Shunyu Yao. Reflexion: 具有语言强化学习的语言代理。在*第三十七届神经信息处理系统会议*，2023年。'
- en: 'Singer et al. [2023] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
    Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal
    Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video
    data. In *The Eleventh International Conference on Learning Representations, ICLR
    2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singer et al. [2023] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
    Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal
    Gupta, 和 Yaniv Taigman. Make-a-video：无需文本-视频数据的文本到视频生成。在*第十一届国际学习表示会议，ICLR 2023，卢旺达基加利，2023年5月1-5日*。OpenReview.net，2023年。
- en: Weng et al. [2023] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang
    Liu, and Jun Zhao. Large language models are better reasoners with self-verification,
    2023.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng et al. [2023] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang
    Liu, 和 Jun Zhao. 大型语言模型通过自我验证更擅长推理，2023年。
- en: 'Xie et al. [2023] Jinheng Xie, Kai Ye, Yudong Li, Yuexiang Li, Kevin Qinghong
    Lin, Yefeng Zheng, Linlin Shen, and Mike Zheng Shou. Visorgpt: Learning visual
    prior via generative pre-training. *CoRR*, abs/2305.13777, 2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al. [2023] Jinheng Xie, Kai Ye, Yudong Li, Yuexiang Li, Kevin Qinghong
    Lin, Yefeng Zheng, Linlin Shen, 和 Mike Zheng Shou. Visorgpt: 通过生成预训练学习视觉先验。*CoRR*，abs/2305.13777，2023年。'
- en: 'Yilmaz et al. [2023] Mehmet Burak Yilmaz, Elen Lotman, Andres Karjus, and Pia
    Tikka. An embodiment of the cinematographer: emotional and perceptual responses
    to different camera movement techniques. *Frontiers in Neuroscience*, 17, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yilmaz et al. [2023] Mehmet Burak Yilmaz, Elen Lotman, Andres Karjus, 和 Pia
    Tikka. 摄影师的体现：对不同相机运动技巧的情感和感知反应。*前沿神经科学*，17，2023年。
- en: Zhang and Agrawala [2023] Lvmin Zhang and Maneesh Agrawala. Adding conditional
    control to text-to-image diffusion models. *CoRR*, abs/2302.05543, 2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang and Agrawala [2023] Lvmin Zhang 和 Maneesh Agrawala. 向文本到图像扩散模型中添加条件控制。*CoRR*，abs/2302.05543，2023年。
- en: Zhang et al. [2023] Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and
    Xin Wang. Controllable text-to-image generation with GPT-4. *CoRR*, abs/2305.18583,
    2023.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2023] Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, 和 Xin
    Wang. 使用GPT-4的可控文本到图像生成。*CoRR*，abs/2305.18583，2023年。
- en: 'Zhou et al. [2022] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu,
    and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion
    models. *CoRR*, abs/2211.11018, 2022.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. [2022] 周等人，2022年，达全·周、伟民·王、汉舒·严、伟伟·吕、怡哲·朱和佳世·冯。《Magicvideo: Efficient
    video generation with latent diffusion models》。*CoRR*，abs/2211.11018。'
