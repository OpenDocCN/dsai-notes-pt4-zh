- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:35
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过LLMs-AIGCs合作进行系统视觉适应的交互数据合成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.12799](https://ar5iv.labs.arxiv.org/html/2305.12799)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.12799](https://ar5iv.labs.arxiv.org/html/2305.12799)
- en: Qifan Yu¹¹¹1Equal Contribution.   Juncheng Li¹¹¹1Equal Contribution.   Wentao
    Ye¹   Siliang Tang¹   Yueting Zhuang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 余祺凡¹¹¹1等贡献。   李峻成¹¹¹1等贡献。   叶文涛¹   唐思亮¹   庄月婷¹
- en: ¹Zhejiang University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹浙江大学
- en: '{yuqifan, junchengli, 22121058, siliang, yzhuang}@zju.edu.cn'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{yuqifan, junchengli, 22121058, siliang, yzhuang}@zju.edu.cn'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent text-to-image generation models have shown promising results in generating
    high-fidelity photo-realistic images. In parallel, the problem of data scarcity
    has brought a growing interest in employing AIGC technology for high-quality data
    expansion. However, this paradigm requires well-designed prompt engineering that
    cost-less data expansion and labeling remain under-explored. Inspired by LLM’s
    powerful capability in task guidance, we propose a new paradigm of annotated data
    expansion named as ChatGenImage. The core idea behind it is to leverage the complementary
    strengths of diverse models to establish a highly effective and user-friendly
    pipeline for interactive data augmentation. In this work, we extensively study
    how LLMs communicate with AIGC model to achieve more controllable image generation
    and make the first attempt to collaborate them for automatic data augmentation
    for a variety of downstream tasks. Finally, we present fascinating results obtained
    from our ChatGenImage framework and demonstrate the powerful potential of our
    synthetic data for systematic vision adaptation. Our codes are available at [https://github.com/Yuqifan1117/Labal-Anything-Pipeline](https://github.com/Yuqifan1117/Labal-Anything-Pipeline).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的文本到图像生成模型在生成高保真度的真实照片图像方面表现出令人期待的结果。同时，数据稀缺的问题引发了对利用AIGC技术进行高质量数据扩展的日益关注。然而，这种模式需要精心设计的提示工程，而成本较低的数据扩展和标注仍未得到充分探索。受LLM强大任务指导能力的启发，我们提出了一种新的标注数据扩展范式，称为ChatGenImage。其核心思想是利用多种模型的互补优势，建立一个高效且用户友好的交互数据增强管道。在这项工作中，我们广泛研究了LLMs如何与AIGC模型进行沟通，以实现更可控的图像生成，并首次尝试将它们协作用于各种下游任务的自动数据增强。最后，我们展示了从ChatGenImage框架中获得的令人惊叹的结果，并展示了我们合成数据在系统视觉适应中的强大潜力。我们的代码可以在[https://github.com/Yuqifan1117/Labal-Anything-Pipeline](https://github.com/Yuqifan1117/Labal-Anything-Pipeline)上获得。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In the past decade, deep learning techniques have demonstrated promising performance
    across diverse tasks, owing to the availability of large-scale annotated data [[12](#bib.bib12),
    [19](#bib.bib19), [10](#bib.bib10)]. However, it is time-consuming and expensive
    to manually collect a large-scale annotated dataset containing every possible
    domain for robust training. Besides, the problem of cross-domain and long-tail
    distributions within existing datasets have a detrimental effect on the performance
    and robustness of vision models, thereby impeding their generalization ability
    to novel categories or unseen domains. This promotes us to explore a less labor-intensive
    way to harvest labeled data containing multiple domains in one step for robust
    vision tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，深度学习技术在各种任务中展现了良好的性能，这得益于大规模标注数据的可用性[[12](#bib.bib12), [19](#bib.bib19),
    [10](#bib.bib10)]。然而，手动收集包含所有可能领域的大规模标注数据集既耗时又昂贵。此外，现有数据集中存在的跨领域和长尾分布问题对视觉模型的性能和鲁棒性产生了不利影响，从而阻碍了它们对新类别或未见领域的泛化能力。这促使我们探索一种劳动力消耗较少的方式，在一次性步骤中收集包含多个领域的标注数据，以应对鲁棒的视觉任务。
- en: One effective strategy to improve generalization and robustness is to enlarge
    the scale of training data by intricate augmentations [[14](#bib.bib14)]. There
    are several GAN-based models [[7](#bib.bib7), [17](#bib.bib17)] generating images
    for vision tasks, but their applicability remains constrained by their narrow
    focus on specific settings or small scales. Recently, AIGC models[[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)] have emerged as promising candidates for generating
    high-quality synthetic data, with the ability to address the limitations of the
    existing dataset. There are several early attempts at exploring synthetic data
    from generative models for data augmentation [[13](#bib.bib13), [23](#bib.bib23),
    [3](#bib.bib3), [37](#bib.bib37)]. Albeit promising, early works usually produce
    simple scenarios or object-centric images only by global constraints (i.e., “airplane"
    or “a white airplane hovering over a beach and a city".), which limits downstream
    models’ perception of intricate scenes and fine-grained attributes. Additionally,
    these methods concentrate on generating images under typical scenarios (e.g.,
    daylight, field), while neglecting less common but predictable circumstances (e.g.,
    snow, forest, night). This limitation may impede the ability of deep learning
    models to generalize when deployed in real-world environments that exhibit unseen
    test distributions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 提高泛化能力和鲁棒性的一种有效策略是通过复杂的数据增强来扩大训练数据的规模[[14](#bib.bib14)]。虽然有几种基于GAN的模型[[7](#bib.bib7),
    [17](#bib.bib17)]用于生成视觉任务的图像，但它们的适用性仍受到仅关注特定设置或小规模的限制。最近，AIGC模型[[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)]作为生成高质量合成数据的有希望的候选者出现，能够解决现有数据集的局限性。对于数据增强，有几个早期尝试探索来自生成模型的合成数据[[13](#bib.bib13),
    [23](#bib.bib23), [3](#bib.bib3), [37](#bib.bib37)]。尽管前景可期，早期工作通常仅通过全局约束生成简单场景或以对象为中心的图像（即，“飞机”或“一架白色飞机悬停在海滩和城市上空”），这限制了下游模型对复杂场景和细粒度属性的感知。此外，这些方法集中于生成典型场景下的图像（例如，白天，田野），而忽视了较少见但可预测的情况（例如，雪天，森林，夜晚）。这种局限性可能阻碍深度学习模型在现实世界环境中进行泛化的能力，因为这些环境展示了未见过的测试分布。
- en: In this paper, we present a novel approach named ChatGenImage that facilitates
    more controllabel data augmentation. ChatGenImage harnesses the collaborative
    power of the LLM and AIGC models, enabling iterative communication between them
    in a cost-effective and controllable manner. This automatically iterative process
    facilitates the generation of high-quality synthetic images depicting complex
    scenes and diverse domains, along with fine-grained annotations. Our fundamental
    intuition is that large language models have remarkable capabilities to perform
    new tasks in a zero-shot manner when presented with well-crafted instruction prompts[[34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [11](#bib.bib11)]. We discover that these
    LLMs like ChatGPT possess the capability to autonomously navigate image editing
    processes. By strategically designing appropriate prompts, LLMs can leverage the
    inherent knowledge within the system and effectively guide the AIGC models to
    produce highly controllable and intricate images. While ChatGPT contains diverse
    world knowledge for simulating the human brain’s efficient processing, it is non-trival
    to elicit this knowledge from it for data augmentation with automatic labeling
    because ChatGPT is a pure language model that lacks the ability to visually perceive
    any information. We explore this issue in the context of generative data augmentation,
    showing that language can act as a bridge connecting LLMs and AIGC models, producing
    elaborate images for downstream tasks by globally controllable prompts and iteratively
    local editing instructions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的方法，称为ChatGenImage，旨在促进更可控的数据增强。ChatGenImage利用LLM和AIGC模型的协同能力，以一种成本效益高且可控的方式实现它们之间的迭代通信。这个自动迭代的过程有助于生成描绘复杂场景和多样领域的高质量合成图像及其细粒度注释。我们基本的直觉是，大型语言模型在提供精心设计的指令提示时，具有以零样本方式执行新任务的显著能力[[34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [11](#bib.bib11)]。我们发现这些LLM如ChatGPT具有自主导航图像编辑过程的能力。通过战略性地设计适当的提示，LLM可以利用系统内在的知识，有效地引导AIGC模型生成高度可控且复杂的图像。虽然ChatGPT包含了多样的世界知识来模拟人脑的高效处理，但从中引出这种知识以进行数据增强和自动标注并非易事，因为ChatGPT是纯语言模型，缺乏视觉感知任何信息的能力。我们在生成数据增强的背景下探讨了这个问题，展示了语言如何作为LLMs和AIGC模型之间的桥梁，通过全局可控的提示和迭代的局部编辑指令生成复杂的图像以用于下游任务。
- en: To this end, we demonstrate three key findings. First, we find that the LLM
    such as ChatGPT contains a wealth of conceptual knowledge and can imagine vivid
    descriptions even with only one label word (e.g. A dog playing in a lush green
    park, with a frisbee in its mouth. The dog should have a shiny coat of fur.) [[33](#bib.bib33),
    [6](#bib.bib6)]. We further obverse that the existing AIGC models can only generate
    simple image with few objects and backgrounds, which are not diverse for domain
    generalization [[20](#bib.bib20)]. Thus, we establish the iterative pipeline to
    repair missing details and refine generated images with the help of label foundation
    toolkits and local editing prompts. Finally, we demonstrate our method flow to
    produce large amounts of high-quality synthetic data with fine-grained labels
    in a scalable manner for data augmentation in data scarcity scenarios.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们展示了三个关键发现。首先，我们发现LLM如ChatGPT包含了丰富的概念知识，即使只有一个标签词也能想象出生动的描述（例如，一只狗在郁郁葱葱的公园里玩耍，嘴里叼着飞盘。狗的毛发应该光亮。）[[33](#bib.bib33),
    [6](#bib.bib6)]。我们进一步观察到，现有的AIGC模型只能生成简单的图像，具有较少的对象和背景，这些图像在领域泛化方面不够多样化[[20](#bib.bib20)]。因此，我们建立了一个迭代流程，通过标签基础工具包和局部编辑提示修复缺失的细节和细化生成的图像。最后，我们演示了我们的方法流程，以可扩展的方式生成大量高质量的合成数据及其细粒度标签，以应对数据稀缺场景的数据增强。
- en: '![Refer to caption](img/ea7fa710666718ed49448995d640d64f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ea7fa710666718ed49448995d640d64f.png)'
- en: 'Figure 1: Language as a bridge for LLMs (e.g. ChatGPT) and AIGC models (e.g.
    Stable Diffusion) can iteratively control image generation and automatic labeling.
    The LLM first generates global prompts to guide AIGC models in generating initialization
    images, then iteratively refines them using automatically generated fine-grained
    annotations as local constraint prompts to produce diverse and complex scenes.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：语言作为LLMs（例如ChatGPT）和AIGC模型（例如Stable Diffusion）的桥梁，可以迭代地控制图像生成和自动标注。LLM首先生成全局提示，以指导AIGC模型生成初始化图像，然后使用自动生成的细粒度注释作为局部约束提示，迭代地对其进行细化，以生成多样化和复杂的场景。
- en: 2 Related Work
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Large Language Models
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型
- en: In recent years, the field of natural language processing has been revolutionized
    by the emergence of large language models (LLMs), exemplified by models such as
    PaLM [[8](#bib.bib8)], ChatGPT, and LLaMa [[33](#bib.bib33)]. Moreover, the remarkable
    performance of LLMs in zero-shot and few-shot generalization has sparked a growing
    trend in utilizing autoregressive language models for vision-language tasks [[21](#bib.bib21),
    [26](#bib.bib26)]. However, the generalization of LLMs does not translate well
    to visual tasks[[20](#bib.bib20), [9](#bib.bib9)]. Unlike previous works, we utilize
    LLMs to enrich training data for fine-tuning in downstream tasks instead of directly
    transferring by contrastive learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着大型语言模型（LLMs）的出现，如PaLM [[8](#bib.bib8)]、ChatGPT和LLaMa [[33](#bib.bib33)]，自然语言处理领域经历了革命性的变化。此外，LLMs在零-shot和少-shot泛化中的卓越表现激发了利用自回归语言模型进行视觉语言任务的日益增长的趋势 [[21](#bib.bib21),
    [26](#bib.bib26)]。然而，LLMs的泛化能力在视觉任务中表现不佳[[20](#bib.bib20), [9](#bib.bib9)]。与以往的研究不同，我们利用LLMs来丰富训练数据，以便在下游任务中进行微调，而不是通过对比学习直接转移。
- en: 2.2 Text-to-Image Diffusion Models
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 文本到图像扩散模型
- en: Recently, diffusion models have become a promising generative modeling framework,
    achieving state-of-the-art performance on image generation tasks [[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)]. GLIDE [[25](#bib.bib25)] studies diffusion
    models for the text-conditional image synthesis by classifier-free guidance strategies.
    InstructPix2Pix [[5](#bib.bib5)] proposes a effective framework to edit images
    with human instructions, which opens up new opportunities for controllable image
    creation by user-written instructions. However, existing SOTA text-to-image models
    require longer and more complex prompts to yield impressive outcomes, which is
    less user-friendly. Thus, we provide a powerful and user-friendly pipeline to
    generate more elaborate images through iterative refinement with the aid of large
    language models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，扩散模型已成为一种有前景的生成建模框架，在图像生成任务中取得了最先进的表现 [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)]。GLIDE [[25](#bib.bib25)]研究了通过无分类器引导策略的文本条件图像合成。InstructPix2Pix [[5](#bib.bib5)]提出了一个有效的框架，通过人类指令编辑图像，这为通过用户编写的指令进行可控图像创作开辟了新的机会。然而，现有的SOTA文本到图像模型需要更长且更复杂的提示才能产生令人印象深刻的结果，这不够用户友好。因此，我们提供了一种强大且用户友好的管道，通过大型语言模型的帮助，迭代改进生成更精细的图像。
- en: 2.3 Synthetic Data for Visual Tasks
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 视觉任务的合成数据
- en: Recently, there has been an increasing interest in using high-quality synthetic
    data to augment training data for downstream tasks [[23](#bib.bib23), [13](#bib.bib13),
    [4](#bib.bib4), [3](#bib.bib3)]. PET [[31](#bib.bib31)] primarily focuses on a
    semi-supervised situation to automatically generate abundant labeled data for
    augmentation. [[13](#bib.bib13)] use GLIDE to generate abundant class-conditioned
    images and explore the effectiveness of synthetic data for image recognition tasks
    in data-scarce settings. For the task of few-shot object detection, a method proposed
    in [[23](#bib.bib23)] involves selecting representative samples from a large-scale
    synthetic dataset to potentially enhance the performance of FSOD models. Here
    we present a novel approach for generating high-quality synthetic data by leveraging
    state-of-the-art text-to-image models and LLMs. Our method eliminates the need
    for expensive prompt engineering by introducing a unified framework that produces
    abundant and elaborate images with annotations in a single pipeline.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，越来越多的研究对使用高质量的合成数据来增强下游任务的训练数据产生了兴趣 [[23](#bib.bib23), [13](#bib.bib13),
    [4](#bib.bib4), [3](#bib.bib3)]。PET [[31](#bib.bib31)]主要关注半监督情况，以自动生成丰富的标记数据用于增强。[[13](#bib.bib13)]使用GLIDE生成丰富的类条件图像，并探讨了在数据稀缺环境下合成数据在图像识别任务中的有效性。对于少-shot目标检测任务，[[23](#bib.bib23)]提出了一种从大规模合成数据集中选择代表性样本的方法，以潜在提高FSOD模型的性能。在这里，我们提出了一种新颖的方法，通过利用最先进的文本到图像模型和LLMs来生成高质量的合成数据。我们的方法通过引入统一的框架，在单一管道中生成丰富且详尽的带注释图像，从而消除了昂贵的提示工程需求。
- en: 3 Method
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'ChatGenImage is a labeling collaboration framework that involves a mentor large
    language model (LLM) and numerous AIGC models as controllable image creators,
    and labeling foundation models for executing the labeling task. The workflow of
    Label Anything consists of two stages: Language Enhancement Image Initialization
    and Iteratively Local Refinement and Labeling, as shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Interactive Data Synthesis for Systematic Vision Adaptation
    via LLMs-AIGCs Collaboration"). 1) During the first stage, An LLM (e.g., ChatGPT)
    analyze the label word from the user input, and generate complex scene descriptions
    and object-centric descriptions in Global Prompts Brainstorming. Then, AIGC generators
    initialize controllable images based on the global constraints from the LLM. 2)
    During the second stage, the LLM produces Label Editing Prompts based on the high-quality
    pseudo labels automatically obtained from the Label Foundation Toolkit and employs
    them to iteratively control the process of local image editing. Based on that,
    AIGC models can perform Controllable Image Editing from both background and foreground
    to obtain more diversified synthetic images. Through our approach, the generated
    images are modified to align with the complex annotations, resulting in high-quality
    synthetic data suitable for data augmentation in downstream tasks.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGenImage是一个标签协作框架，涉及一个导师大语言模型（LLM）和众多AIGC模型作为可控图像生成器，以及用于执行标注任务的标注基础模型。Label
    Anything的工作流程包括两个阶段：语言增强图像初始化和迭代局部细化及标注，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration")所示。1)
    在第一个阶段，LLM（如ChatGPT）分析用户输入的标签词，并生成复杂的场景描述和以对象为中心的描述，进行全球提示头脑风暴。然后，AIGC生成器基于LLM的全球约束初始化可控图像。2)
    在第二个阶段，LLM基于从标注基础工具包自动获得的高质量伪标签生成标签编辑提示，并使用这些提示迭代控制局部图像编辑过程。基于此，AIGC模型可以从背景和前景进行可控图像编辑，以获得更多样化的合成图像。通过我们的方法，生成的图像被修改以符合复杂的注释，从而生成高质量的合成数据，适用于下游任务中的数据增强。
- en: 3.1 Global Prompts Brainstorming
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 全球提示头脑风暴
- en: Due to the limited knowledge of the large language model about the AIGC model,
    it is not capable of providing appropriate prompts for the AIGC model. Therefore,
    ChatGenImage utilizes a hybrid approach that combines both specification-based
    instruction and demonstration-based learning to effectively guide the AIGC model
    in generating high-quality synthetic data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大语言模型对AIGC模型的知识有限，因此无法提供适当的提示给AIGC模型。因此，ChatGenImage采用了结合规范化指令和示范学习的混合方法，以有效指导AIGC模型生成高质量的合成数据。
- en: Specification-based Instruction. The prompt specification can serve as a standard
    template for large language models to comprehend visual attributes of the specific
    concept, thereby facilitating the sensible scene imagination for a given word
    through slot filling. However, using category names alone in AIGC models may limit
    their ability to perceive visual features, leading to ambiguous image generation[[13](#bib.bib13)].
    To help the large language model imagine effective scene descriptions, ChatGenImage
    prompts focus on descriptive features rather than broad categories. In the first
    stage of ChatGenImage, the large language model take the Label Word from the user
    and construct several relevant descriptions as its Visual Feature for global prompt
    brainstorming. Moreover, we propose to automatically obtain appropriate visual
    features by prompting the LLM to describe the visual features that distinguish
    that category in a photograph. We demonstrate the prompt process for visual feature
    descriptions and controllable generation in Table [1](#S3.T1 "Table 1 ‣ 3.1 Global
    Prompts Brainstorming ‣ 3 Method ‣ Interactive Data Synthesis for Systematic Vision
    Adaptation via LLMs-AIGCs Collaboration").
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化指令。提示规范可以作为大语言模型理解特定概念的视觉属性的标准模板，从而通过槽填充促进对给定词汇的合理场景想象。然而，单独使用类别名称在AIGC模型中可能会限制其感知视觉特征的能力，导致生成模糊的图像[[13](#bib.bib13)]。为了帮助大语言模型想象有效的场景描述，ChatGenImage提示侧重于描述性特征而非广泛的类别。在ChatGenImage的第一阶段，大语言模型从用户处获取标签词，并构建多个相关描述作为其视觉特征，以便进行全球提示头脑风暴。此外，我们建议通过提示LLM描述在照片中区分该类别的视觉特征，自动获取适当的视觉特征。我们在表[1](#S3.T1
    "Table 1 ‣ 3.1 Global Prompts Brainstorming ‣ 3 Method ‣ Interactive Data Synthesis
    for Systematic Vision Adaptation via LLMs-AIGCs Collaboration")中展示了视觉特征描述和可控生成的提示过程。
- en: Demonstration-based learning. ChatGenImage utilizes the in-context learning
    capability of LLMs and injects several demonstrations into the prompt learning,
    helping large language models to better understand the parameter criteria for
    conditional image generation. Each demonstration is a group of input and output
    on scene prompts brainstorming——the user’s request in standard templates and the
    expected image descriptions for AIGC models. Furthermore, these demonstrations
    consist of complex scene descriptions and object-centric descriptions, as shown
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Interactive Data Synthesis for
    Systematic Vision Adaptation via LLMs-AIGCs Collaboration"), effectively aid ChatGenImage
    in understanding the given label’s attributes in various environments and imagining
    reasonable prompts for high-quality image synthesis.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于演示的学习。ChatGenImage 利用大型语言模型的上下文学习能力，将多个演示注入提示学习，帮助大语言模型更好地理解条件图像生成的参数标准。每个演示是一组场景提示头脑风暴的输入和输出——用户在标准模板中的请求以及AIGC模型期望的图像描述。此外，这些演示包括复杂的场景描述和以对象为中心的描述，如图
    [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 通过LLMs-AIGCs合作的系统视觉适应的互动数据合成") 所示，有效帮助 ChatGenImage 理解给定标签在各种环境中的属性，并为高质量图像合成想象合理的提示。
- en: '| Visual Descriptor | Prompt |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 视觉描述符 | 提示 |'
- en: '| "role": "system", "content": "You are an expert in the field of vision and
    graphics, please fully consider the input concept or topic, give me the most important
    fine-grained visual features of the input concept or category based on the Wikipedia.
    Only give me several phrases or keywords as more as possible." "role": "user",
    "content": "Q: What are useful visual features for distinguishing a {category
    name} in a photo? A: There are several useful visual features to tell there is
    a {category name} in a photo:" |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| "role": "system", "content": "你是视觉和图形领域的专家，请充分考虑输入的概念或主题，根据维基百科提供输入概念或类别的最重要的细粒度视觉特征。只需给出尽可能多的短语或关键词。"
    "role": "user", "content": "问：在照片中区分{category name}有哪些有用的视觉特征？答：在照片中识别{category
    name}有几个有用的视觉特征：" |'
- en: '| AIGC Creator | Prompt |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| AIGC 创作者 | 提示 |'
- en: '| "role": "system", "content": "The AI assistant is a professional data specialist.
    The AI assistant can imagine different real scenes according to the input caption
    and return the background and closely related foreground objects in a specific
    format. Note that the background should be described with a single word or phrase,
    and each background contains five main objects." {Demonstration} "role": "user",
    "content": "Give me {prompt number} high quality prompts for text-to-image models
    about the amazing close-up realistic scene containing the {label word}. Scene
    prompts should also contain other objects as more as possible. The {label word}
    is with {visual feature}." |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| "role": "system", "content": "AI助手是一名专业数据专家。AI助手可以根据输入的描述想象不同的真实场景，并以特定格式返回背景和紧密相关的前景对象。请注意，背景应以一个单词或短语描述，每个背景包含五个主要对象。"
    {演示} "role": "user", "content": "给我{prompt number}个高质量的文本到图像模型的提示，关于包含{label word}的惊人逼真特写场景。场景提示也应尽可能包含其他对象。{label
    word}的视觉特征是{visual feature}。" |'
- en: '| Demonstration |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 演示 |'
- en: '| Sure, here are 1 high quality prompts for text-to-image models about the
    amazing close-up realistic scene containing the red panda: Bring a photo-realistic
    close-up scene of a red panda to life, set against a backdrop of a rustic temple
    in a mountainous region. The panda should be situated in front of the temple,
    with its round ears, short legs, and bushy tail with alternating red and white
    rings visible. The temple should have ornate carvings and architecture, with stone
    lanterns and a small pond nearby. The panda’s white paws with black claws should
    be visible as it sits atop a stone pedestal, perhaps with some cherry blossom
    petals scattered around it. The overall scene should have a peaceful and serene
    atmosphere. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 当然，这里有1个高质量的文本到图像模型的提示，关于包含红熊猫的惊人逼真特写场景：展现一个红熊猫的照片级逼真特写场景，背景是山区的一个古老寺庙。熊猫应位于寺庙前面，圆耳、短腿和带有交替红白环的蓬松尾巴应可见。寺庙应有华丽的雕刻和建筑风格，附近有石灯笼和一个小池塘。熊猫坐在石座上时，其白色的爪子和黑色的爪子应可见，周围可能散落着一些樱花花瓣。整体场景应具有宁静和安详的氛围。
    |'
- en: '| Visual Feature |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 视觉特征 |'
- en: '| Label word: "Lemur" | - Large, forward-facing eyes with binocular vision;
    Soft, thick fur covering the body - Long, bushy tail; Striking coloration patterns
    such as black and white rings - Slender fingers and toes with opposable thumbs
    for grasping and climbing; Prominent ears that can be pointed or rounded with
    tufts of fur - Wet, reflective nose; Rounded head with a shortened snout and large
    ears - Relatively small body size, typically weighing between 2 and 5 kilograms;
    Distinctive vocalization or call that can vary between species and subspecies
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 标签词： "狐猴" | - 大而向前的双眼具有双目视觉；身体覆盖柔软、厚实的毛发 - 长而蓬松的尾巴；显眼的颜色模式，如黑白相间的环 - 细长的手指和脚趾，具有相对拇指用于抓握和攀爬；显著的耳朵，可以是尖的或圆的，带有毛发的小簇
    - 湿润、反光的鼻子；圆头，短吻和大耳朵 - 相对较小的体型，通常体重大约在2到5公斤之间；具有特征性的叫声或鸣叫，可能因物种和亚种而异 |'
- en: '| Scene Imagination | Prompt |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 场景想象 | 提示 |'
- en: '| "role": "system", "content": "The AI assistant is a professional data specialist.
    The AI assistant can imagine different real scenes according to the input caption
    and return the background and closely related foreground objects in a specific
    format. Note that the background should be described with a single word or phrase,
    and each background contains five main objects." "role": "user", "content": "Give
    me scene number real scene descriptions based on the context caption. The scene
    objects should consist exist objects, and also contain five additional objects
    associated with the background. Each scene description is a complex sentence containing
    the above objects. Return the result in the following format: ’background’:[],
    ’objects’:[], ’description’:[]. Only return the result." |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| "role": "system", "content": "AI助手是一名专业的数据专家。AI助手可以根据输入的描述想象不同的实际场景，并以特定格式返回背景和紧密相关的前景物体。请注意，背景应使用单个词或短语进行描述，并且每个背景包含五个主要物体。"
    "role": "user", "content": "根据上下文描述给我场景编号的真实场景描述。场景中的物体应包括现有物体，并且还包含与背景相关的五个附加物体。每个场景描述是一个复杂的句子，包含上述物体。以以下格式返回结果：’background’:[],
    ’objects’:[], ’description’:[]. 仅返回结果。" |'
- en: '| Box Candidates Generation | Prompt |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 框候选生成 | 提示 |'
- en: '| "Please make {prompt number} possible prediction of the remaining box coordinates
    with different box size based on the dense description "{caption}". Note that
    the image size is (512,512), and the existing box coordinates are [{existing boxes
    info}]. Based on the layout of the objects, predict the possible number reasonable
    box coordinates of the following objects {target objects}. The size of the {target
    objects} box should be based on the category and the size of other object boxes,
    and the width and height of the box should be greater than 75 and less than 300\.
    Only return each result in the following format: "label":, "box":, "relationship":"
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| "请基于密集描述"{caption}"，对剩余框坐标进行{prompt number}次可能的预测，框大小不同。请注意，图像大小为(512,512)，现有框坐标为[{existing
    boxes info}]。根据物体的布局，预测以下物体{target objects}的合理数量框坐标。{target objects}框的大小应根据类别和其他物体框的大小来确定，框的宽度和高度应大于75且小于300。仅以以下格式返回每个结果："label":,
    "box":, "relationship":" |'
- en: '| Demonstration |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 演示 |'
- en: '| "{caption}": ’there is a dog sitting on a bench in a field.’ "{existing boxes
    info}": "value": 1, "label": "bench", "logit": 0.84, "box": [33.93, 224.34, 463.20,
    491.01], "value": 2, "label": "dog", "logit": 0.43, "box": [175.71, 116.29, 311.58,
    367.13] Return Results: "label": ’cat’, "box": [343.23, 176.29, 467.23, 353.13],
    "relationship": ’sitting next to the dog.’ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| "{caption}": ’一个狗坐在一个田野中的长椅上。’ "{existing boxes info}": "value": 1, "label":
    "长椅", "logit": 0.84, "box": [33.93, 224.34, 463.20, 491.01], "value": 2, "label":
    "狗", "logit": 0.43, "box": [175.71, 116.29, 311.58, 367.13] 返回结果： "label": ’猫’,
    "box": [343.23, 176.29, 467.23, 353.13], "relationship": ’坐在狗旁边。’ |'
- en: 'Table 1: The details of the prompt design in ChatGenImage. There are injectable
    slots in the prompts, such as Caption, Visual Feature, and Existing Boxes Info.
    These slots imply visual perceptions that help LLM building multimodal awareness
    and are uniformly replaced with the contents from visual foundation models before
    being fed into the LLM.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：ChatGenImage中的提示设计详细信息。提示中有可注入的槽位，如描述、视觉特征和现有框信息。这些槽位暗示了视觉感知，有助于LLM建立多模态意识，并在喂入LLM之前被统一替换为视觉基础模型中的内容。
- en: 3.2 Label Foundation Toolkit
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 标签基础工具包
- en: Since ChatGPT is a pure language model and cannot “see” any visual information,
    we present the initialized images to several powerful label foundation toolkits (i.e.,
    Segment Anything Model [[18](#bib.bib18)], Grounding DINO [[24](#bib.bib24)],
    and BLIP2 [[22](#bib.bib22)]) and serve them as sensors in the system to provide
    perceptual information to the ChatGPT.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ChatGPT 是一个纯语言模型，无法“看到”任何视觉信息，我们将初始化图像呈现给几个强大的标签基础工具包（即 Segment Anything
    Model [[18](#bib.bib18)]、Grounding DINO [[24](#bib.bib24)] 和 BLIP2 [[22](#bib.bib22)]），并将它们作为系统中的传感器，以向
    ChatGPT 提供感知信息。
- en: Segment Anything Model (SAM) is a large ViT-based model trained on the large
    visual corpus (SA-1B) [[18](#bib.bib18)], which has demonstrated promising zero-shot
    segmentation capabilities in various scenarios and the great potential for data
    labeling in visual tasks. But it needs precise prompts (like boxes/points) to
    generate accurate masks and lacks category predictions or annotations for each
    mask.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything Model（SAM）是一个基于大型 ViT 的模型，经过在大型视觉语料库（SA-1B）[[18](#bib.bib18)]
    上训练，展示了在各种场景下有前景的零样本分割能力，并在视觉任务的数据标注中具有很大潜力。但它需要精确的提示（如框/点）来生成准确的掩膜，并且缺乏对每个掩膜的类别预测或注释。
- en: Grounding DINO is a strong zero-shot detector which is capable of to generate
    high quality boxes and labels with free-form text [[24](#bib.bib24)], which can
    also serves as box prompts generator for SAM. Our approach combines the strengths
    of Grounding DINO and SAM to detect and segment comprehensive regions in each
    synthetic image. This builds a powerful pipeline for complex visual scene labeling
    and produces abundant fine-grained pseudo labels for training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Grounding DINO 是一个强大的零样本检测器，能够通过自由格式文本生成高质量的框和标签[[24](#bib.bib24)]，同时也可以作为 SAM
    的框提示生成器。我们的方法结合了 Grounding DINO 和 SAM 的优势，以检测和分割每个合成图像中的全面区域。这构建了一个强大的管道，用于复杂视觉场景标注，并为训练生成了丰富的细致伪标签。
- en: BLIP2 is a language-vision model that seamlessly integrates visual input into
    text sequences to facilitate overall visual perception of LLMs [[22](#bib.bib22)].
    By combining BLIP2 with the aforementioned visual models, our approach can automatically
    generate high-quality text descriptions for synthetic images. The LLMs then use
    these descriptions to understand image regions and return local editing prompts
    for controllable and diverse image refinement.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP2 是一种语言-视觉模型，它将视觉输入无缝地整合到文本序列中，以促进 LLMs 的整体视觉感知[[22](#bib.bib22)]。通过将 BLIP2
    与上述视觉模型结合，我们的方法可以自动生成合成图像的高质量文本描述。然后，LLMs 使用这些描述来理解图像区域，并返回用于可控和多样化图像细化的局部编辑提示。
- en: 3.3 Local Editing Prompt
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 局部编辑提示
- en: Despite the ability of AIGC models to generate labeled images through prompt-based
    techniques with the aid of LLMs, their effectiveness in depicting intricate scenes
    and fine-grained attributes remains limited due to their inclination to create
    object-centric images with only global constraints. Besides, we observe that the
    generated images contain fewer objects, which poses a challenge in constructing
    complex scenes for demanding downstream tasks (e.g., scene graph generation and
    visual question answering). Thus we further introduce local editing prompt in
    the iterative pipeline for fine-grained image refinement.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AIGC 模型能够通过基于提示的技术生成标记图像，并借助 LLMs 提高效果，但由于它们倾向于生成仅有全局约束的以物体为中心的图像，因此在描绘复杂场景和细致属性方面的效果仍然有限。此外，我们观察到生成的图像包含的对象较少，这在构建复杂场景以满足高要求的下游任务（例如场景图生成和视觉问答）时带来了挑战。因此，我们进一步引入了局部编辑提示，以便在迭代流程中进行细致的图像细化。
- en: In detail, we design a iterative communication that encourages ChatGPT to provide
    a series of informative feedbacks based on the generated images from AIGC models
    and corresponding labels from label foundation toolkits. Since language models
    are blind to the initialized image, ChatGPT cannot partially edited the initial
    image directly. Hence, we employ a predefined prompt template and populate the
    slots with the corresponding caption and object box coordinates identified by
    the visual foundation models. This template is subsequently utilized by the LLMs
    to produce novel scenes that comprise new backgrounds and additional objects.
    It is worth noting that ChatGPT can voluntarily select a reasonable location for
    editing based on human instructions and its underlying knowledge, autonomously
    generating accurate local editing prompts. Then the AIGC model use the resulting
    prompts to edit the images and improve their quality by adding missing details.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 详细来说，我们设计了一种迭代通信，鼓励ChatGPT基于AIGC模型生成的图像及标签基础工具包中的相应标签提供一系列信息反馈。由于语言模型对初始化的图像是盲目的，ChatGPT无法直接部分编辑初始图像。因此，我们使用预定义的提示模板，并用视觉基础模型识别的相应字幕和物体框坐标填充槽位。随后，LLMs使用此模板生成包含新背景和附加对象的新场景。值得注意的是，ChatGPT可以根据人类指令和其基础知识自愿选择合适的位置进行编辑，自动生成准确的局部编辑提示。然后，AIGC模型使用生成的提示编辑图像，通过添加缺失的细节来提高图像质量。
- en: Algorithm 1 ChatGenImage Pipeline
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 ChatGenImage 流程
- en: '1:  Initialization: Label $w_{0}$= Filling$(I_{0},{\rm Scene}[^{\prime}objects^{\prime}])$'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始化: 标签 $w_{0}$= 填充$(I_{0},{\rm Scene}[^{\prime}objects^{\prime}])$'
- en: 3.4 Controllable Image Editing
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 可控图像编辑
- en: To efficiently generate a significant amount of images with complex scenes and
    rich annotations in a low-resource manner, It is necessary to collaborate with
    various controllable editing models that can perform controllable image editing
    based on both global and local prompts. The total process is shown in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.3 Local Editing Prompt ‣ 3 Method ‣ Interactive Data Synthesis
    for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"). Besides, we use
    image filtering rules to figure out those representative samples as valid results
    and utilize the label foundation toolkit to get high-quality annotations for downstream
    tasks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以低资源的方式高效生成大量复杂场景和丰富注释的图像，需要与各种可控编辑模型合作，这些模型能够基于全球和局部提示执行可控图像编辑。整个过程如算法[1](#alg1
    "算法 1 ‣ 3.3 局部编辑提示 ‣ 3 方法 ‣ 通过LLMs-AIGCs协作的系统化视觉适应的互动数据合成")所示。此外，我们使用图像过滤规则来筛选出那些代表性样本作为有效结果，并利用标签基础工具包为下游任务获取高质量注释。
- en: Background Imagination. We notice that retrieved or directly generated images
    are usually restricted to to a single domain, thereby leading to a constraint
    in the development of robust visual models [[9](#bib.bib9)]. Furthermore, it is
    impractical to obtain labeled data for all possible anticipated settings at once
    is often impractical due to the significant expense involved. However, acquiring
    linguistic knowledge of the anticipated domain shift is a more cost-effective
    and accessible approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 背景想象。我们注意到，检索或直接生成的图像通常局限于单一领域，从而导致在开发鲁棒视觉模型时的限制[[9](#bib.bib9)]。此外，由于涉及的费用很高，一次性获取所有可能的预期设置的标记数据通常是不切实际的。然而，获取预期领域转移的语言知识是一种更具成本效益和可及的方法。
- en: Hence, we leverage the ChatGPT to generate novel backgrounds for the original
    image and employ InstructPix2Pix [[5](#bib.bib5)] to substitute different backgrounds,
    generating a vast collection of composite images across various domains in a cost-effective
    manner. To preserve the foreground semantics of images after background modification,
    we perform target detection on both the original and modified images, and apply
    filter rules to exclude images with missing objects.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们利用ChatGPT为原始图像生成新背景，并采用InstructPix2Pix [[5](#bib.bib5)]来替换不同的背景，从而以具有成本效益的方式生成大量涵盖各种领域的合成图像。为了在背景修改后保持图像的前景语义，我们对原始图像和修改后的图像进行目标检测，并应用过滤规则排除缺少对象的图像。
- en: 'Foreground Object Filling. To avoid altering the semantic information of the
    source image, we propose a method to increase the complexity of the image scene
    by filling foreground objects. The necessary object labels, coordinates, and their
    interactions with the scene (i.e., {label: ‘rocks’, box: [200, 50, 300, 150],
    relationship: ‘near the cabin and surrounded by trees’}) can be obtained by filtering
    the local editing prompts automatically. Once collect sufficient possible boxes
    through ChatGPT, we can use Blended Latent Diffusion [[2](#bib.bib2), [1](#bib.bib1)]
    to fill novel objects in the specific position of the foreground. It is worth
    noting that we filter out the overlapping bounding boxes to ensure that the original
    semantics are preserved. In this way, we greatly enrich the spatial interaction
    of objects in the generated image.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '前景对象填充。为了避免改变源图像的语义信息，我们提出了一种通过填充前景对象来增加图像场景复杂性的方法。必要的对象标签、坐标及其与场景的交互（例如，{label:
    ‘rocks’, box: [200, 50, 300, 150], relationship: ‘near the cabin and surrounded
    by trees’}）可以通过自动过滤本地编辑提示获得。一旦通过 ChatGPT 收集到足够的可能框，我们可以使用 Blended Latent Diffusion
    [[2](#bib.bib2), [1](#bib.bib1)] 在前景的特定位置填充新对象。值得注意的是，我们会过滤掉重叠的边界框，以确保原始语义得到保留。通过这种方式，我们大大丰富了生成图像中对象的空间互动。'
- en: '![Refer to caption](img/04672a06c1b79e4e47c3a72e9b3039f4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04672a06c1b79e4e47c3a72e9b3039f4.png)'
- en: 'Figure 2: Visualization results of Iteratively Local Refinement and Labeling.
    It contains three steps: 1) Background Imagination, 2) Iteratively Object Filling,
    3) Label anything in the image via visual foundation models.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：迭代局部细化和标记的可视化结果。它包括三个步骤：1) 背景想象，2) 迭代对象填充，3) 通过视觉基础模型标记图像中的任何内容。
- en: 'Figure 3: Qualitative analysis of object-centric image generation and complex
    scene description with multiple background and objects.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于对象的图像生成和复杂场景描述的定性分析，包括多个背景和对象。
- en: '![Refer to caption](img/c96cc6e5367168cc16aa4c4524f1024d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c96cc6e5367168cc16aa4c4524f1024d.png)'
- en: 'Figure 4: Visualization results of ChatGenImage for object-centric image generation.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：ChatGenImage 在基于对象的图像生成中的可视化结果。
- en: 3.5 Image Filtering Rules
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 图像过滤规则
- en: 'Though Most state-of-the-art AIGC methods generate astonishing images, they
    may have several visual and textual issues: 1) boundary incoherence, 2) semantic
    mismatch and 3) object missing. It is essential to establish robust image filtering
    rules that can effectively evaluate the synthetic images and filter out those
    low-quality results. To address above challenges, we introduce a Pixel Checking (PC)
    and a Semantic Checking (SC) strategy for the generated images from the perspective
    of visual pixels and textual semantics.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数最先进的 AIGC 方法生成了惊人的图像，但它们可能存在若干视觉和文本问题：1) 边界不一致，2) 语义不匹配和 3) 对象缺失。建立能够有效评估合成图像并过滤低质量结果的鲁棒图像过滤规则至关重要。为了应对上述挑战，我们从视觉像素和文本语义的角度引入了像素检查
    (PC) 和语义检查 (SC) 策略。
- en: Pixel Checking. To ensure the boundary consistency of the edited image, we evaluate
    the fidelity of the generated image. IS [[30](#bib.bib30)], FID [[15](#bib.bib15)]
    SceneFID[[32](#bib.bib32)] are common metrics to evaluate the fidelity of general
    images in different scales. However, all of these metrics rely on ground truth
    labels, which are not suitable for assessing images generated by stable diffusion
    models [[27](#bib.bib27)]. Therefore, we exploit the SSIM and PSNR [[16](#bib.bib16)]
    to explore structural similarity and pixel similarity between the locally edited
    image and the original image for pixel checking. We employ a threshold strategy
    of PSNR and SSIM between the original and edited images, minimizing artifacts
    and incoherence at the editing boundary to preserve the global coherence of the
    image during the local editing process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 像素检查。为了确保编辑图像的边界一致性，我们评估生成图像的保真度。IS [[30](#bib.bib30)]、FID [[15](#bib.bib15)]
    和 SceneFID [[32](#bib.bib32)] 是评估不同尺度下图像保真度的常见指标。然而，这些指标都依赖于真实标签，这对于评估由稳定扩散模型生成的图像并不适用
    [[27](#bib.bib27)]。因此，我们利用 SSIM 和 PSNR [[16](#bib.bib16)] 探索本地编辑图像与原始图像之间的结构相似性和像素相似性以进行像素检查。我们采用
    PSNR 和 SSIM 阈值策略，最小化编辑边界处的伪影和不一致性，以在本地编辑过程中保持图像的全局一致性。
- en: Semantic Checking. Considering that local image editing may introduce undesired
    items to destroy the semantics of the original image, we evaluate the semantics
    and object detection of the generated image during semantic checking. Specifically,
    we generate a set of image candidates based on scene descriptions of specific
    label words during both global initialization and local image editing. Then, we
    use the CLIP similarity score [[26](#bib.bib26)] to evaluate the semantic alignment
    between the image candidates and textual constraints. We rank the image candidates
    based on the score and filter out the low-confidence images to obtain most matching
    ones as the final result. Besides, as the we employ open vocabulary object detection
    on the images after background editing. We only retain those images that can identify
    the novel background and original foreground objects to keep the original semantics
    and enhance the downstream utility of the edited images.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 语义检查。考虑到局部图像编辑可能会引入不希望出现的项，破坏原始图像的语义，我们在语义检查过程中评估生成图像的语义和对象检测。具体来说，我们在全局初始化和局部图像编辑过程中，根据特定标签词的场景描述生成一组图像候选。然后，我们使用
    CLIP 相似度分数[[26](#bib.bib26)]来评估图像候选与文本约束之间的语义对齐。我们根据分数对图像候选进行排名，并筛选出低置信度图像，以获取匹配度最高的图像作为最终结果。此外，在背景编辑后，我们对图像进行开放词汇对象检测。我们仅保留那些能够识别新背景和原始前景对象的图像，以保持原始语义并增强编辑图像的下游实用性。
- en: '![Refer to caption](img/60156ea3d78587da8f296c85e9f5a9aa.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/60156ea3d78587da8f296c85e9f5a9aa.png)'
- en: 'Figure 5: Visualization results of ChatGenImage for complex scene imagination.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：ChatGenImage 在复杂场景想象中的可视化结果。
- en: 4 Experiments
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this version, we discuss our main experiments setup and several results.
    We demonstrate the effectiveness of our ChatGenImage on interactive data synthesis
    through qualitative results, showing the potential of using ChatGenImage for systematic
    vision adaptation. In the next release, we will further explore how to better
    leverage the synthetic data obtained from our ChatGenImage framework for better
    downstream task generalization.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一版本中，我们讨论了主要实验设置和一些结果。我们通过质性结果展示了 ChatGenImage 在互动数据合成上的有效性，展示了利用 ChatGenImage
    进行系统化视觉适配的潜力。在下一个版本中，我们将进一步探讨如何更好地利用从 ChatGenImage 框架中获得的合成数据，以提升下游任务的泛化能力。
- en: 4.1 Setting
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: In our experiments, we employed the gpt-3.5-turbo variants of the GPT models
    as the large language models (i.e., ChatGPT), which are publicly accessible through
    the OpenAI API¹¹1[https://platform.openai.com/](https://platform.openai.com/).
    To make the LLM output more stable, we set the decoding temperature to 0\. For
    AIGC models, we uniformly set the pixel of the picture to 512×512 to save the
    memory overhead. Also to adapt to the whole system, we use stable diffusion v1.5
    as the AIGC base model with the same default parameters as the original setting [[28](#bib.bib28)].
    We provide detailed prompts designed for the Visual Descriptor, AIGC Creator,
    Scene Imagination, Box Candidates Generation in the step of Global Prompts Brainstorming
    and Local Editing Prompts in Table [1](#S3.T1 "Table 1 ‣ 3.1 Global Prompts Brainstorming
    ‣ 3 Method ‣ Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs
    Collaboration"), where {variable} indicates that the slot needs to be populated
    with the corresponding text before the prompt can be fed into the LLM. This label
    pipeline is based on a single Nvidia RTX 3090 GPU, which is affordable for most
    people.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用了 GPT 模型的 gpt-3.5-turbo 变体作为大型语言模型（即 ChatGPT），这些模型可以通过 OpenAI API¹¹1[https://platform.openai.com/](https://platform.openai.com/)
    公开访问。为了使 LLM 输出更稳定，我们将解码温度设置为 0。对于 AIGC 模型，我们将图片的像素统一设置为 512×512，以节省内存开销。同时，为了适应整个系统，我们使用了稳定扩散
    v1.5 作为 AIGC 基础模型，并保持原始设置的默认参数[[28](#bib.bib28)]。我们在表[1](#S3.T1 "Table 1 ‣ 3.1
    Global Prompts Brainstorming ‣ 3 Method ‣ Interactive Data Synthesis for Systematic
    Vision Adaptation via LLMs-AIGCs Collaboration")中提供了为视觉描述符、AIGC 创作者、场景想象、框选候选生成设计的详细提示，其中
    {variable} 表示需要在提示输入到 LLM 之前填入相应的文本。该标签管道基于单张 Nvidia RTX 3090 GPU，这对于大多数人来说是可承受的。
- en: 4.2 Qualitative Results
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 质性结果
- en: We evaluate the generated image and fine-grained annotations in our ChatGenImage
    in two cases (i.e., complex scene description and object-centric image generation).
    In Figures [3](#S3.F3 "Figure 3 ‣ 3.4 Controllable Image Editing ‣ 3 Method ‣
    Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"),
    we show several dialogue demonstrations and qualitative analysis for above two
    different cases of requirement data, respectively. We collect several label words
    of rare and endangered species for testing, which have few photos in the web and
    are unfamiliar to most image classifiers. We compare our approach with LLM descriptions
    and original images generated by naive AIGC models. The result is shown in Figure [4](#S3.F4
    "Figure 4 ‣ 3.4 Controllable Image Editing ‣ 3 Method ‣ Interactive Data Synthesis
    for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"). The experimental
    result indicates that the proposed ChatGenImage is both general and controllable,
    effectively creating robust images even for unfamiliar and rare concepts. Through
    interactive communication with the LLM, the AIGC can learn the specific descriptions
    of novel concepts and complete controllable generation in different domains via
    iterative image editing.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 ChatGenImage 中评估生成的图像和细粒度注释，涉及两种情况（即复杂场景描述和以对象为中心的图像生成）。在图[3](#S3.F3 "图
    3 ‣ 3.4 可控图像编辑 ‣ 3 方法 ‣ 通过 LLM 和 AIGC 协作的系统视觉适配的交互数据合成")中，我们分别展示了几种对话演示和对这两种不同要求数据的定性分析。我们收集了几种稀有和濒危物种的标签词进行测试，这些物种在网络上的照片很少，大多数图像分类器对此并不熟悉。我们将我们的方法与
    LLM 描述和原始图像（由简单 AIGC 模型生成）进行了比较。结果显示在图[4](#S3.F4 "图 4 ‣ 3.4 可控图像编辑 ‣ 3 方法 ‣ 通过
    LLM 和 AIGC 协作的系统视觉适配的交互数据合成")中。实验结果表明，提出的 ChatGenImage 既通用又可控，能够有效地生成即使对于不熟悉和稀有概念也能表现出强大的图像。通过与
    LLM 的交互沟通，AIGC 可以学习新概念的具体描述，并通过迭代图像编辑完成不同领域的可控生成。
- en: Besides, we explore the effectiveness of the ChatGenImage for complex scene
    descriptions and whether the LLM help AIGC models iteratively fill accurate objects
    in the foreground. In Figure [5](#S3.F5 "Figure 5 ‣ 3.5 Image Filtering Rules
    ‣ 3 Method ‣ Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs
    Collaboration"), we show that the LLM provides several information comprising
    of object coordinates and relation interactions, which is then used by Stable
    Diffusion to generate diverse backgrounds (e.g., mountain, forest) and incorporate
    relevant objects (e.g., snow trees, stream and rocks) to produce a rich image
    depicting a complex scene.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们探讨了 ChatGenImage 在复杂场景描述中的有效性，以及 LLM 是否能帮助 AIGC 模型逐步填充前景中的准确对象。在图[5](#S3.F5
    "图 5 ‣ 3.5 图像过滤规则 ‣ 3 方法 ‣ 通过 LLM 和 AIGC 协作的系统视觉适配的交互数据合成")中，我们展示了 LLM 提供的包括对象坐标和关系交互的信息，然后由
    Stable Diffusion 用于生成多样的背景（如山脉、森林）并融入相关对象（如雪树、小溪和岩石），以产生描绘复杂场景的丰富图像。
- en: 4.3 Applications
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 应用
- en: ChatGenImage, as an interactive data synthesis framework, provides two generation
    modes and fine-grained images with various domains to easily meet the requirements
    of different field. Moreover, the usage of synthetic data from ChatGenImage enables
    more generalizable of downstream tasks in complex application scenarios.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGenImage 作为一个交互式数据合成框架，提供了两种生成模式和各种领域的细粒度图像，以轻松满足不同领域的需求。此外，ChatGenImage
    的合成数据使用使下游任务在复杂应用场景中具有更好的泛化能力。
- en: Since ChatGenImage can iteratively generate large amount of diverse images via
    LLM-AIGC collaboration, it can provide extra unseen data domains for systematic
    vision adapation. We will evaluate ChatGenImgae on several domain adaptation benchmarks
    to investigate how to enrich the existing dataset with synthetic data and construct
    adaptive visual perception system in a cost-less manner.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ChatGenImage 可以通过 LLM 和 AIGC 协作迭代生成大量多样化的图像，它可以为系统视觉适配提供额外的未见数据领域。我们将评估 ChatGenImage
    在几个领域适配基准上的表现，以研究如何用合成数据丰富现有数据集，并以低成本构建自适应视觉感知系统。
- en: 5 Conclusion
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: ChatGenImage is a versatile tool that combines the capabilities of LLMs, AIGC
    models and powerful label foundation toolkits. Based on the collaboration of diverse
    models, ChatGenImage enables to generate fine-grained images with rich annotations
    for data augmentation. In the future, we will further develop our approach to
    support more complex and challengeable scenarios, like fine-grained human-object
    interaction, action editing, etc., and apply it to more realistic applications
    for systematic vision adaptation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGenImage 是一个多功能工具，结合了 LLM、AIGC 模型和强大的标签基础工具包。基于不同模型的协作，ChatGenImage 能够生成具有丰富注释的细粒度图像用于数据增强。未来，我们将进一步开发我们的方案，以支持更多复杂和具有挑战性的场景，如细粒度的人物-物体互动、动作编辑等，并将其应用于更现实的应用中，以实现系统化的视觉适配。
- en: References
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: '[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion.
    arXiv preprint arXiv:2206.02779, 2022.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Omri Avrahami, Ohad Fried, 和 Dani Lischinski. 混合潜在扩散. arXiv 预印本 arXiv:2206.02779,
    2022 年。'
- en: '[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven
    editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 18208–18218, 2022.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Omri Avrahami, Dani Lischinski, 和 Ohad Fried. 用于文本驱动自然图像编辑的混合扩散. 见于 IEEE/CVF
    计算机视觉与模式识别会议论文集，第 18208–18218 页，2022 年。'
- en: '[3] Hritik Bansal and Aditya Grover. Leaving reality to imagination: Robust
    classification via generated datasets. arXiv preprint arXiv:2302.02503, 2023.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Hritik Bansal 和 Aditya Grover. 离开现实进入想象: 通过生成数据集进行稳健分类. arXiv 预印本 arXiv:2302.02503,
    2023 年。'
- en: '[4] Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, and Stéphane
    Lathuilière. One-shot unsupervised domain adaptation with personalized diffusion
    models. arXiv preprint arXiv:2303.18080, 2023.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, 和 Stéphane
    Lathuilière. 一次性无监督领域适配与个性化扩散模型. arXiv 预印本 arXiv:2303.18080, 2023 年。'
- en: '[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning
    to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Tim Brooks, Aleksander Holynski, 和 Alexei A Efros. Instructpix2pix: 学习遵循图像编辑指令.
    arXiv 预印本 arXiv:2211.09800, 2022 年。'
- en: '[6] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, 等. 人工通用智能的火花:
    关于 GPT-4 的早期实验. arXiv 预印本 arXiv:2303.12712, 2023 年。'
- en: '[7] Jaehoon Choi, Taekyung Kim, and Changick Kim. Self-ensembling with gan-based
    data augmentation for domain adaptation in semantic segmentation. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 6830–6840,
    2019.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Jaehoon Choi, Taekyung Kim, 和 Changick Kim. 使用基于 GAN 的数据增强进行语义分割领域适配的自我集成.
    见于 IEEE/CVF 国际计算机视觉会议论文集，第 6830–6840 页，2019 年。'
- en: '[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, 等. Palm: 扩展语言建模的路径. arXiv 预印本 arXiv:2204.02311, 2022 年。'
- en: '[9] Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E
    Gonzalez, Aditi Raghunanthan, and Anja Rohrbach. Using language to extend to unseen
    domains. arXiv preprint arXiv:2210.09520, 2022.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph
    E Gonzalez, Aditi Raghunanthan, 和 Anja Rohrbach. 使用语言扩展到未见领域. arXiv 预印本 arXiv:2210.09520,
    2022 年。'
- en: '[10] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference
    on computer vision, pages 1440–1448, 2015.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ross Girshick. Fast R-CNN. 见于 IEEE 国际计算机视觉会议论文集，第 1440–1448 页，2015 年。'
- en: '[11] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional
    visual reasoning without training. arXiv preprint arXiv:2211.11559, 2022.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Tanmay Gupta 和 Aniruddha Kembhavi. 视觉编程: 无需训练的组合视觉推理. arXiv 预印本 arXiv:2211.11559,
    2022 年。'
- en: '[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 图像识别的深度残差学习. 见于 IEEE
    计算机视觉与模式识别会议论文集，第 770–778 页，2016 年。'
- en: '[13] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr,
    Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for
    image recognition? arXiv preprint arXiv:2210.07574, 2022.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr,
    Song Bai, 和 Xiaojuan Qi。生成模型中的合成数据是否已准备好用于图像识别？arXiv 预印本 arXiv:2210.07574, 2022。'
- en: '[14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang,
    Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many
    faces of robustness: A critical analysis of out-of-distribution generalization.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    8340–8349, 2021.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang,
    Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo 等。鲁棒性的多重面貌：对分布外泛化的批判性分析。在IEEE/CVF国际计算机视觉会议论文集，页8340–8349,
    2021。'
- en: '[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
    and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to
    a local nash equilibrium. Advances in neural information processing systems, 30,
    2017.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
    和 Sepp Hochreiter。通过双时间尺度更新规则训练的生成对抗网络收敛到局部纳什均衡。神经信息处理系统进展，30, 2017。'
- en: '[16] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010
    20th international conference on pattern recognition, pages 2366–2369\. IEEE,
    2010.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Alain Hore 和 Djemel Ziou。图像质量度量：Psnr 与 ssim。在2010年第20届国际模式识别会议论文集，页2366–2369。IEEE,
    2010。'
- en: '[17] Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola. Generative
    models as a data source for multiview representation learning. arXiv preprint
    arXiv:2106.05258, 2021.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Ali Jahanian, Xavier Puig, Yonglong Tian, 和 Phillip Isola。生成模型作为多视图表示学习的数据来源。arXiv
    预印本 arXiv:2106.05258, 2021。'
- en: '[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
    Segment anything. arXiv preprint arXiv:2304.02643, 2023.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo 等。Segment
    Anything。arXiv 预印本 arXiv:2304.02643, 2023。'
- en: '[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. Communications of the ACM, 60(6):84–90,
    2017.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E Hinton。使用深度卷积神经网络进行ImageNet分类。ACM通讯，60(6):84–90,
    2017。'
- en: '[20] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.
    Fine-tuning can distort pretrained features and underperform out-of-distribution.
    arXiv preprint arXiv:2202.10054, 2022.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, 和 Percy Liang。微调可能扭曲预训练特征并在分布外表现不佳。arXiv
    预印本 arXiv:2202.10054, 2022。'
- en: '[21] Juncheng Li, XIN HE, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie,
    Yueting Zhuang, Qi Tian, and Siliang Tang. Fine-grained semantically aligned vision-language
    pre-training. In Advances in Neural Information Processing Systems, 2022.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Juncheng Li, XIN HE, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie,
    Yueting Zhuang, Qi Tian, 和 Siliang Tang。细粒度语义对齐的视觉语言预训练。在神经信息处理系统进展，2022。'
- en: '[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping
    language-image pre-training with frozen image encoders and large language models.
    arXiv preprint arXiv:2301.12597, 2023.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Junnan Li, Dongxu Li, Silvio Savarese, 和 Steven Hoi。BLIP-2：通过冻结图像编码器和大型语言模型引导语言-图像预训练。arXiv
    预印本 arXiv:2301.12597, 2023。'
- en: '[23] Shaobo Lin, Kun Wang, Xingyu Zeng, and Rui Zhao. Explore the power of
    synthetic data on few-shot object detection. arXiv preprint arXiv:2303.13221,
    2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Shaobo Lin, Kun Wang, Xingyu Zeng, 和 Rui Zhao。探索合成数据在少样本目标检测中的强大作用。arXiv
    预印本 arXiv:2303.13221, 2023。'
- en: '[24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang,
    Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino
    with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499,
    2023.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang,
    Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu 等。Grounding DINO：将DINO与基础预训练结合用于开放集目标检测。arXiv
    预印本 arXiv:2303.05499, 2023。'
- en: '[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
    Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image
    generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741,
    2021.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
    Bob McGrew, Ilya Sutskever, 和 Mark Chen。GLIDE：通过文本引导的扩散模型实现逼真的图像生成和编辑。arXiv 预印本
    arXiv:2112.10741, 2021。'
- en: '[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
    Learning transferable visual models from natural language supervision. In International
    conference on machine learning, pages 8748–8763\. PMLR, 2021.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 亚历克·拉德福德、钟旭金、克里斯·哈拉西、阿迪提亚·拉梅什、加布里埃尔·戈、桑迪尼·阿加瓦尔、吉里什·萨斯特里、阿曼达·阿斯克尔、帕梅拉·米什金、杰克·克拉克等。从自然语言监督中学习可迁移的视觉模型。发表于国际机器学习会议，页码
    8748–8763。PMLR，2021年。'
- en: '[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
    Hierarchical text-conditional image generation with clip latents. arXiv preprint
    arXiv:2204.06125, 2022.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 阿迪提亚·拉梅什、普拉夫拉·达里瓦尔、亚历克斯·尼科尔、凯西·朱和马克·陈。基于 CLIP 潜变量的层次文本条件图像生成。arXiv 预印本
    arXiv:2204.06125，2022年。'
- en: '[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695,
    2022.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 罗宾·隆巴赫、安德烈亚斯·布拉特曼、多米尼克·洛伦茨、帕特里克·埃瑟和比约恩·奥默。高分辨率图像合成与潜在扩散模型。发表于 IEEE/CVF
    计算机视觉与模式识别会议论文集，页码 10684–10695，2022年。'
- en: '[29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
    Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
    et al. Photorealistic text-to-image diffusion models with deep language understanding.
    Advances in Neural Information Processing Systems, 35:36479–36494, 2022.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 奇特万·萨哈里亚、威廉·陈、索拉布·萨克塞纳、拉拉·李、杰伊·黄、艾米莉·L·登顿、卡姆亚尔·加塞米普尔、拉斐尔·贡蒂霍·洛佩斯、布尔库·卡拉戈尔·阿扬、蒂姆·萨利曼斯等。具有深度语言理解的
    photorealistic 文本到图像扩散模型。神经信息处理系统进展，第 35 卷：36479–36494，2022年。'
- en: '[30] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    and Xi Chen. Improved techniques for training gans. Advances in neural information
    processing systems, 29, 2016.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 蒂姆·萨利曼斯、伊恩·古德费洛、沃伊切赫·扎伦巴、维基·张、亚历克·拉德福德和谢辰。改进的 GAN 训练技术。神经信息处理系统进展，第 29
    卷，2016年。'
- en: '[31] Timo Schick and Hinrich Schütze. It’s not just size that matters: Small
    language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 蒂莫·希克和欣里希·施图策。不仅仅是规模重要：小型语言模型也是少样本学习者。arXiv 预印本 arXiv:2009.07118，2020年。'
- en: '[32] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, and Shikhar
    Sharma. Object-centric image generation from layouts. In Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 35, pages 2647–2655, 2021.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 特里斯坦·西尔万、彭川张、约书亚·本吉奥、R·德文·赫尔姆和希卡尔·夏尔马。基于布局的面向对象图像生成。发表于 AAAI 人工智能会议论文集，第
    35 卷，页码 2647–2655，2021年。'
- en: '[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 雨果·图弗龙、提博·拉夫里尔、戈蒂埃·伊扎卡德、泽维尔·马尔坦、玛丽-安娜·拉肖、蒂莫泰·拉克鲁瓦、巴普蒂斯特·罗齐埃、纳曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔等。Llama：开放且高效的基础语言模型。arXiv
    预印本 arXiv:2302.13971，2023年。'
- en: '[34] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
    Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot
    learners. arXiv preprint arXiv:2109.01652, 2021.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 杰森·魏、马滕·博斯玛、文森特·Y·赵、凯尔文·古、亚当斯·魏·余、布赖恩·莱斯特、南段、安德鲁·M·戴和阮克·李。微调的语言模型是零样本学习者。arXiv
    预印本 arXiv:2109.01652，2021年。'
- en: '[35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le,
    and Denny Zhou. Chain of thought prompting elicits reasoning in large language
    models. arXiv preprint arXiv:2201.11903, 2022.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 杰森·魏、薛之旺、戴尔·舒尔曼斯、马滕·博斯玛、埃德·池、阮克·李和丹尼·周。思维链提示引发大型语言模型的推理。arXiv 预印本 arXiv:2201.11903，2022年。'
- en: '[36] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and
    Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation
    models. arXiv preprint arXiv:2303.04671, 2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 陈飞吴、盛名尹、魏震齐、肖东王、泽城唐和南段。视觉 ChatGPT：与视觉基础模型进行对话、绘图和编辑。arXiv 预印本 arXiv:2303.04671，2023年。'
- en: '[37] Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche,
    Adela Barriuso, Antonio Torralba, and Sanja Fidler. Datasetgan: Efficient labeled
    data factory with minimal human effort. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 10145–10155, 2021.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 郁轩张、欢灵、俊高、康雪尹、让-弗朗索瓦·拉夫莱什、阿德拉·巴里乌索、安东尼奥·托拉尔巴和桑贾·费德勒。DatasetGAN：最小人工干预的高效标记数据工厂。发表于
    IEEE/CVF 计算机视觉与模式识别会议论文集，页码 10145–10155，2021年。'
