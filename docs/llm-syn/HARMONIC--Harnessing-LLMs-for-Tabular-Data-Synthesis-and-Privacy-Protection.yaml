- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'HARMONIC: 利用LLM进行表格数据合成和隐私保护'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02927](https://ar5iv.labs.arxiv.org/html/2408.02927)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02927](https://ar5iv.labs.arxiv.org/html/2408.02927)
- en: Yuxin Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yuxin Wang
- en: Sichuan University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 四川大学
- en: Chengdu, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 成都，中国
- en: wangyuxin1st@gmail.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: wangyuxin1st@gmail.com
- en: '&Duanyu Feng'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Duanyu Feng'
- en: Sichuan University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 四川大学
- en: Chengdu, China
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 成都，中国
- en: fengduanyuscu@stu.scu.edu.cn
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: fengduanyuscu@stu.scu.edu.cn
- en: '&Yongfu Dai'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yongfu Dai'
- en: Sichuan University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 四川大学
- en: Chengdu, China
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 成都，中国
- en: wal.daishen@gmail.com
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: wal.daishen@gmail.com
- en: '&Zhengyu Chen'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&Zhengyu Chen'
- en: Wuhan University
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 武汉大学
- en: Wuhan, China
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 武汉，中国
- en: 2019302120293@whu.edu.cn
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2019302120293@whu.edu.cn
- en: '&Jimin Huang'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '&Jimin Huang'
- en: The Fin AI
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: The Fin AI
- en: Singapore
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 新加坡
- en: jimin.huang@thefin.ai
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: jimin.huang@thefin.ai
- en: '&Sophia Ananiadou'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&Sophia Ananiadou'
- en: The University of Manchester
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 曼彻斯特大学
- en: Manchester, UK
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 曼彻斯特，英国
- en: sophia.ananiadou@manchester.ac.uk
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: sophia.ananiadou@manchester.ac.uk
- en: '&Qianqian Xie^∗'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&Qianqian Xie^∗'
- en: The Fin AI
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: The Fin AI
- en: Singapore
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 新加坡
- en: qianqian.xie@thefin.ai
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: qianqian.xie@thefin.ai
- en: '&Hao Wang'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '&Hao Wang'
- en: Sichuan University
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 四川大学
- en: Chengdu, China
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 成都，中国
- en: wangh@scu.edu.cn Co-Corresponding Author.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: wangh@scu.edu.cn 共同通讯作者。
- en: Abstract
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Data serves as the fundamental foundation for advancing deep learning, particularly
    tabular data presented in a structured format, which is highly conducive to modeling.
    However, even in the era of LLM, obtaining tabular data from sensitive domains
    remains a challenge due to privacy or copyright concerns. Hence, exploring how
    to effectively use models like LLMs to generate realistic and privacy-preserving
    synthetic tabular data is urgent. In this paper, we take a step forward to explore
    LLMs for tabular data synthesis and privacy protection, by introducing a new framework
    HARMONIC for tabular data generation and evaluation. In the tabular data generation
    of our framework, unlike previous small-scale LLM-based methods that rely on continued
    pre-training, we explore the larger-scale LLMs with fine-tuning to generate tabular
    data and enhance privacy. Based on idea of the k-nearest neighbors algorithm,
    an instruction fine-tuning dataset is constructed to inspire LLMs to discover
    inter-row relationships. Then, with fine-tuning, LLMs are trained to remember
    the format and connections of the data rather than the data itself, which reduces
    the risk of privacy leakage. In the evaluation part of our framework, we develop
    specific privacy risk metrics DLT for LLM synthetic data generation, as well as
    performance evaluation metrics LLE for downstream LLM tasks. Our experiments find
    that this tabular data generation framework achieves equivalent performance to
    existing methods with better privacy, which also demonstrates our evaluation framework
    for the effectiveness of synthetic data and privacy risks in LLM scenarios.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据作为推动深度学习的基础，尤其是以结构化格式呈现的表格数据，这对于建模非常有利。然而，即使在LLM时代，从敏感领域获取表格数据仍然是一个挑战，因为隐私或版权问题。因此，探索如何有效利用LLM等模型生成真实且保护隐私的合成表格数据显得尤为迫切。在本文中，我们迈出了一步，通过引入一个新的框架HARMONIC来探索表格数据合成和隐私保护。与之前依赖持续预训练的小规模LLM方法不同，我们在表格数据生成方面探索了更大规模的LLM，并通过微调来生成表格数据并增强隐私。基于k近邻算法的思想，构建了一个指令微调数据集，以激发LLM发现行间关系。然后，通过微调，使LLM能够记住数据的格式和连接，而不是数据本身，从而减少隐私泄露的风险。在我们框架的评估部分，我们开发了专门的隐私风险度量DLT，用于LLM合成数据生成，以及性能评估度量LLE，用于下游LLM任务。我们的实验发现，这一表格数据生成框架在隐私保护方面表现优于现有方法，并且也展示了我们评估框架对LLM场景下合成数据和隐私风险的有效性。
- en: 1 Introduction
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In the age of deep learning, tabular data is a predominant data format and a
    key element for building more effective algorithms to solve specific applications
    in various fields [[1](#bib.bib1), [2](#bib.bib2)]. However, in many sensitive
    domains such as business [[3](#bib.bib3)], healthcare [[4](#bib.bib4)], and governmental
    operations [[5](#bib.bib5)], there are significant limitations on the acquisition
    and use of tabular data. Tabular data in these domains involves personal privacy,
    business secrets, or state secrets. The collection and use of such data are strictly
    regulated by laws and regulations, and compliance with relevant data protection
    requirements is necessary. Unauthorized use or disclosure may result in serious
    privacy infringement or business losses. Therefore, generating data that ensures
    the effectiveness in modeling these data while preserving privacy in tabular data
    synthesis has always been a critical research area [[6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习时代，表格数据是一种主要的数据格式，也是构建更有效算法以解决各个领域特定应用的关键元素 [[1](#bib.bib1), [2](#bib.bib2)]。然而，在许多敏感领域，如商业
    [[3](#bib.bib3)]、医疗 [[4](#bib.bib4)] 和政府运营 [[5](#bib.bib5)]，对表格数据的获取和使用有显著的限制。这些领域的表格数据涉及个人隐私、商业秘密或国家秘密。此类数据的收集和使用受到法律法规的严格监管，并且需要遵守相关的数据保护要求。未经授权的使用或披露可能会导致严重的隐私侵犯或商业损失。因此，生成能够在保证模型有效性的同时保护隐私的表格数据，一直是一个关键的研究领域
    [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]。
- en: Traditionally, tabular data synthesis relied on methods like GANs [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)], VAEs [[12](#bib.bib12), [13](#bib.bib13)],
    and Diffusion Models [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)].
    These techniques, built on mathematical foundations and complex frameworks, significantly
    advanced the field. However, the rise of Large Language Models (LLMs) with their
    impressive ability to generate realistic data has shifted the paradigm. Methods
    like GReaT [[18](#bib.bib18)] and TabuLa [[6](#bib.bib6)] leverage LLMs for faster
    synthesis by converting tables to natural language and predicting the next data
    token. They often utilize smaller pre-trained models like GPT-2 [[19](#bib.bib19)]
    for efficiency. Despite their advantages, LLMs introduce significant privacy concerns
    [[20](#bib.bib20), [21](#bib.bib21)]. These models may potentially leak sensitive
    information from the training data they are exposed to. Therefore, a crucial area
    of exploration lies in developing strategies to mitigate these privacy risks while
    harnessing the power of LLMs for tabular data synthesis.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，表格数据合成依赖于诸如GANs [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]、VAEs
    [[12](#bib.bib12), [13](#bib.bib13)] 和扩散模型 [[14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17)] 等方法。这些技术建立在数学基础和复杂框架之上，极大地推动了这一领域的发展。然而，随着大语言模型（LLMs）的兴起，其生成逼真数据的卓越能力改变了这一范式。像GReaT
    [[18](#bib.bib18)] 和TabuLa [[6](#bib.bib6)] 这样的技术利用LLMs通过将表格转换为自然语言并预测下一个数据令牌来实现更快的合成。它们通常利用较小的预训练模型，如GPT-2
    [[19](#bib.bib19)]，以提高效率。尽管有这些优势，LLMs引入了显著的隐私问题 [[20](#bib.bib20), [21](#bib.bib21)]。这些模型可能会泄露它们所接触到的训练数据中的敏感信息。因此，开发减轻这些隐私风险的策略，同时利用LLMs在表格数据合成中的力量，是一个至关重要的研究领域。
- en: 'To Harness LLMs fOr Tabular Data SyNthesis and PrIvacy ProteCtion, we develop
    a new framework, HARMONIC¹¹1[https://github.com/Wendy619/HARMONIC](https://github.com/Wendy619/HARMONIC).,
    with tabular data generation and evaluation on LLMs. For the tabular data generation
    framework, we use existing larger-scale LLMs to leverage their understanding abilities
    for generating tabular data while ensuring privacy. It is based on the idea of
    k-nearest neighbor classification (kNN) [[22](#bib.bib22)], which lets the LLMs
    see the relationship between multiple similar rows and construct the structural
    tabular synthetic data format. With this format, we obtain the instruction-tuning
    datasets that retain more structural information for LLMs to enhance the ability
    to generate synthetic data through fine-tuning, while avoiding the forced memorization
    of data with pre-training. Meanwhile, to comprehensively assess the effectiveness
    and privacy of synthetic data generated by LLMs, our framework introduces two
    novel metrics: DLT (Data Leakage Test) and LLE (LLM Efficiency). DLT quantifies
    the privacy risk of the synthesized data by LLMs. Conversely, LLE evaluates the
    effectiveness of the synthetic data in downstream LLM tasks. The evaluation of
    the effectiveness of downstream LLM tasks is based on the increasing application
    of LLMs in various fields. It is important to note that machine learning-based
    evaluations are no longer sufficient.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用LLM进行表格数据合成和隐私保护，我们开发了一个新的框架HARMONIC¹¹1[https://github.com/Wendy619/HARMONIC](https://github.com/Wendy619/HARMONIC)，用于LLM的表格数据生成和评估。在表格数据生成框架中，我们使用现有的大规模LLM，利用它们的理解能力来生成表格数据，同时确保隐私。该框架基于k近邻分类（kNN）[[22](#bib.bib22)]的思想，使LLM能够看到多个相似行之间的关系，并构建结构化的表格合成数据格式。通过这种格式，我们获得了保留更多结构信息的指令调优数据集，以增强LLM通过微调生成合成数据的能力，同时避免了预训练数据的强制记忆。同时，为了全面评估LLM生成的合成数据的有效性和隐私，我们的框架引入了两个新颖的指标：DLT（数据泄漏测试）和LLE（LLM效率）。DLT量化了LLM合成数据的隐私风险，而LLE评估合成数据在下游LLM任务中的有效性。下游LLM任务有效性的评估基于LLM在各个领域应用的增加。需要注意的是，基于机器学习的评估已不再足够。
- en: Using our evaluation framework, we assess four datasets commonly used for classification
    tasks in tabular data synthesis. The results show that synthetic data generated
    with HARMONIC performs comparably to existing methods in machine learning and
    excels in downstream tasks and privacy assessments in LLMs. Crucially, HARMONIC’s
    evaluation suggests that traditional synthetic data methods may be unsuitable
    for downstream LLM tasks and that pretraining-based synthetic data poses significant
    privacy risks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的评估框架，我们评估了四个常用于分类任务的表格数据集。结果显示，使用HARMONIC生成的合成数据在机器学习中表现与现有方法相当，并在下游任务和LLM隐私评估中表现出色。重要的是，HARMONIC的评估表明，传统的合成数据方法可能不适合下游LLM任务，而基于预训练的合成数据存在显著的隐私风险。
- en: 'The main contributions of this study can be summarized as follows: 1) We recognize
    that it is crucial to not only focus on the strong data generation ability of
    LLM in this era, but also pay attention to the potential privacy risks it may
    bring. 2) We develop a framework, HARMONIC, for synthesizing tabular data based
    on LLM. The framework aims to minimize the risk of data leakage while ensuring
    the effectiveness of data synthesis using LLM. 3) Under the HARMONIC framework,
    a set of metrics is proposed for the effectiveness in downstream LLMs tasks and
    privacy risk evaluation of synthetic tabular data.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的主要贡献可以总结如下：1）我们认识到，在这个时代，不仅要关注大规模语言模型（LLM）强大的数据生成能力，还需要关注它可能带来的潜在隐私风险。2）我们开发了一个基于LLM的数据表合成框架HARMONIC。该框架旨在最小化数据泄漏的风险，同时确保使用LLM进行数据合成的有效性。3）在HARMONIC框架下，提出了一套用于下游LLM任务有效性和合成数据隐私风险评估的指标。
- en: 2 Related work
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Tabular Data Synthesis. Prior to the rise of Large Language Models (LLMs),
    synthetic tabular data generation primarily relied on machine learning or classical
    neural network frameworks. These methods can be broadly categorized into three
    groups: Simple Augmentation, Generative Adversarial Networks (GANs), and Diffusion
    Models. Techniques like SMOTE [[23](#bib.bib23)] exemplify Simple Augmentation,
    leveraging linear interpolation for data resampling. While effective for structured
    data, SMOTE overlooks semantic information. Building on GANs, CTGAN [[9](#bib.bib9)]
    introduces a conditional generator and adapts a Variational Autoencoder (VAE)
    for tabular data (TVAE). CTAB-GAN [[10](#bib.bib10)] tackles data imbalance and
    long-tail issues. TabDDPM [[14](#bib.bib14)] serves as a prominent benchmark for
    Diffusion-based methods, with TABSYN [[15](#bib.bib15)] offering faster synthesis
    compared to other such techniques. However, most of these methods utilize one-hot
    encoding for categorical data, which can exacerbate the "curse of dimensionality"
    for high-cardinality variables and fail to capture contextual information [[18](#bib.bib18),
    [6](#bib.bib6)].'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据合成。在大型语言模型（LLMs）崛起之前，合成表格数据的生成主要依赖于机器学习或经典神经网络框架。这些方法大致可分为三类：简单增强、生成对抗网络（GANs）和扩散模型。像SMOTE
    [[23](#bib.bib23)]这样的技术示例了简单增强，利用线性插值进行数据重采样。尽管对结构化数据有效，但SMOTE忽略了语义信息。基于GANs，CTGAN
    [[9](#bib.bib9)]引入了条件生成器，并将变分自编码器（VAE）适配于表格数据（TVAE）。CTAB-GAN [[10](#bib.bib10)]解决了数据不平衡和长尾问题。TabDDPM
    [[14](#bib.bib14)]作为扩散方法的一个重要基准，TABSYN [[15](#bib.bib15)]提供了比其他类似技术更快的合成速度。然而，大多数这些方法使用独热编码处理分类数据，这可能加剧高基数变量的“维度诅咒”并且无法捕捉上下文信息
    [[18](#bib.bib18), [6](#bib.bib6)]。
- en: LLMs have emerged as a compelling approach for synthetic data generation due
    to their exceptional capabilities in producing high-quality, human-like data.
    LLM-based methods commonly employ a pre-training paradigm. Real tabular data is
    converted into text format and fed into the LLM for learning. GreaT [[18](#bib.bib18)]
    exemplifies this approach, converting each tabular feature into the format "X
    is Y" and feeding the text into GPT-2 [[19](#bib.bib19)] for fine-tuning. Tabula
    [[6](#bib.bib6)] introduces a tabular data synthesizer leveraging an LLM framework
    without pre-trained weights. It prioritizes faster training speed by simplifying
    token sequences to "X Y". REaLTabFormer [[24](#bib.bib24)] presents a transformer-based
    framework for generating both non-relational and relational tabular data. It treats
    each tabular sample as a sequence with dependencies, akin to a sentence, learning
    conditional distributions to sequentially generate complete samples. While LLM-based
    methods often outperform machine learning approaches due to their ability to leverage
    contextual information in text entries, limitations exist. Processing table data
    entry-by-entry hinders LLMs from fully exploiting relational information between
    samples. Furthermore, inherent security risks associated with data leakage plague
    LLMs [[20](#bib.bib20)]. Pre-training-like fine-tuning can make them vulnerable,
    potentially allowing an attacker with knowledge of one or two feature values in
    a real entry to retrieve the entire real data record.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs作为一种有吸引力的合成数据生成方法出现，因其在生成高质量、人类类似数据方面具有卓越能力。基于LLM的方法通常采用预训练范式。真实表格数据被转换成文本格式并输入到LLM中进行学习。GreaT
    [[18](#bib.bib18)]就是这种方法的一个示例，将每个表格特征转换为“X是Y”的格式，并将文本输入到GPT-2 [[19](#bib.bib19)]进行微调。Tabula
    [[6](#bib.bib6)]介绍了一种利用LLM框架的表格数据合成器，没有预训练权重。它通过简化令牌序列为“X Y”来优先考虑更快的训练速度。REaLTabFormer
    [[24](#bib.bib24)]呈现了一个基于变换器的框架，用于生成非关系型和关系型表格数据。它将每个表格样本视为一个具有依赖关系的序列，类似于句子，通过学习条件分布来顺序生成完整的样本。虽然基于LLM的方法由于能够利用文本条目的上下文信息通常优于机器学习方法，但仍存在局限性。逐条处理表格数据使LLM无法充分利用样本之间的关系信息。此外，与数据泄露相关的固有安全风险困扰着LLM
    [[20](#bib.bib20)]。类似于预训练的微调可能使其变得脆弱，可能使攻击者凭借对一个或两个真实条目特征值的了解就能够检索整个真实数据记录。
- en: Tabular Data Synthesis Evaluate. Existing evaluation methods for synthetic data,
    such as the MLE benchmarking system proposed by Xu et al. [[9](#bib.bib9)], primarily
    focus on assessing its performance as training data for machine learning models.
    However, as Kotelnikov et al. [[14](#bib.bib14)] argue, relying on weak classifiers
    for evaluation becomes outdated in light of the capabilities of advanced models
    like CatBoost [[25](#bib.bib25)]. This underscores the need for more sophisticated
    evaluation techniques, especially considering the widespread adoption of LLMs
    in downstream applications [[26](#bib.bib26)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据合成评估。现有的合成数据评估方法，如Xu等人提出的MLE基准系统[[9](#bib.bib9)]，主要关注评估其作为机器学习模型训练数据的性能。然而，正如Kotelnikov等人[[14](#bib.bib14)]所指出的，依赖于弱分类器的评估在面对像CatBoost[[25](#bib.bib25)]这样的高级模型时已经过时。这突显了需要更复杂的评估技术，特别是考虑到LLMs在下游应用中的广泛采用[[26](#bib.bib26)]。
- en: Current privacy metrics for synthetic data, such as Distance to Closest Record
    (DCR) [[10](#bib.bib10)] and the NewRowSynthesis metric from SDMetrics [[27](#bib.bib27)],
    solely analyze the distance between synthetic data and real data. While these
    distance-based approaches provide valuable insights, they fall short when dealing
    with Large Language Models (LLMs). LLMs are particularly susceptible to data leakage
    due to their complex nature and training on massive datasets [[20](#bib.bib20)].
    However, existing privacy metrics based solely on tabular data feature distances
    fail to capture the unique learning and inference mechanisms of LLMs, which operate
    at the semantic and generative probability levels of embeddings. Consequently,
    these methods lack intuitive indicators of privacy leakage specific to LLMs [[28](#bib.bib28)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的合成数据隐私度量指标，如距离最近记录（DCR）[[10](#bib.bib10)]和来自SDMetrics的新行合成度量[[27](#bib.bib27)]，仅分析合成数据与真实数据之间的距离。虽然这些基于距离的方法提供了有价值的见解，但在处理大型语言模型（LLMs）时存在不足。由于LLMs的复杂性质和对大量数据集的训练，它们特别容易发生数据泄露[[20](#bib.bib20)]。然而，现有的仅基于表格数据特征距离的隐私度量无法捕捉LLMs的独特学习和推理机制，这些机制在嵌入的语义和生成概率层面操作。因此，这些方法缺乏针对LLMs特定隐私泄露的直观指标[[28](#bib.bib28)]。
- en: '3 Our Framework: HARMONIC'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 我们的框架：HARMONIC
- en: This chapter delves into the HARMONIC framework for tabular data synthesis powered
    by LLMs, encompassing both generation and evaluation modules.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了由LLMs驱动的表格数据合成的HARMONIC框架，包括生成和评估模块。
- en: 3.1 Synthetic Tabular Data Generation Framework
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 合成表格数据生成框架
- en: 'In this section, we present our approach to fine-tuning pre-trained LLMs for
    the generation of synthetic tabular data, including three key stages: (1) Construct
    instruction dataset: Construct an instruction fine-tuning dataset designed to
    fine-tune the generator model and a prompt dataset to facilitate data generation.
    (2) Instruction tuning: Fed the instruction fine-tuning dataset into a pre-trained
    LLM for fine-tuning, as illustrated in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Synthetic
    Tabular Data Generation Framework ‣ 3 Our Framework: HARMONIC ‣ HARMONIC: Harnessing
    LLMs for Tabular Data Synthesis and Privacy Protection"); (3) Sampling: Synthetic
    tabular data is generated by sampling from the fine-tuned LLM, with the sampling
    process depicted in Figure [2](#S3.F2 "Figure 2 ‣ 3.1.1 Construct Instruction
    Dataset ‣ 3.1 Synthetic Tabular Data Generation Framework ‣ 3 Our Framework: HARMONIC
    ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection").
    Below, we will provide a comprehensive description of the entire process, encompassing
    the construction of the instruction dataset, model fine-tuning, and the implementation
    of sampling.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们介绍了针对合成表格数据生成的预训练LLMs微调方法，包括三个关键阶段：（1）构建指令数据集：构建一个指令微调数据集，旨在微调生成模型，以及一个提示数据集，以便于数据生成。（2）指令微调：将指令微调数据集输入到预训练LLM中进行微调，如图[1](#S3.F1
    "Figure 1 ‣ 3.1 Synthetic Tabular Data Generation Framework ‣ 3 Our Framework:
    HARMONIC ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection")所示；（3）采样：通过从微调后的LLM中采样生成合成表格数据，采样过程如图[2](#S3.F2
    "Figure 2 ‣ 3.1.1 Construct Instruction Dataset ‣ 3.1 Synthetic Tabular Data Generation
    Framework ‣ 3 Our Framework: HARMONIC ‣ HARMONIC: Harnessing LLMs for Tabular
    Data Synthesis and Privacy Protection")所示。下面，我们将对整个过程进行全面描述，包括指令数据集的构建、模型微调以及采样的实现。'
- en: '![Refer to caption](img/d38f048e78eac5d881a6987787c1a1fa.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d38f048e78eac5d881a6987787c1a1fa.png)'
- en: 'Figure 1: After applying the kNN algorithm to the original table, we obtain
    $n$ data points. Each set is structured according to the template shown in the
    gray table at the bottom left. These datasets are then encoded into a single instruction
    using text encoding, with the features of each table data shuffled, as shown in
    the white box above (a). Finally, the encoded fine-tuning dataset is input into
    the pre-trained LLM for fine-tuning (b).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：在对原始表格应用kNN算法后，我们得到$n$个数据点。每个集合按照底部左侧灰色表格中显示的模板进行结构化。这些数据集随后通过文本编码编码成一个指令，其中每个表格数据的特征被随机打乱，如上方白色框（a）所示。最后，将编码后的微调数据集输入到预训练LLM中进行微调（b）。
- en: 3.1.1 Construct Instruction Dataset
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 构建指令数据集
- en: Construct a fine-tuning dataset using kNN. Our approach leverages kNN to enable
    LLMs to generate synthetic data resembling limited real data. This is expected
    to use the in-context learning ability of LLM (few-shots) to mine information
    from the most relevant table samples.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用kNN构建微调数据集。我们的方法利用kNN使LLM生成类似于有限真实数据的合成数据。这预计将利用LLM的上下文学习能力（少量样本）从最相关的表格样本中挖掘信息。
- en: Specifically, this process involves finding the $k$ was used throughout our
    experimental setup.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，这一过程涉及到在我们的实验设置中使用的$k$。
- en: Data format engineering. Since LLMs are designed as sequence-to-sequence models,
    feeding tabular data into an LLM requires converting the structured data into
    a textual format. A straightforward approach would be to directly input a programming
    language readable data structure, such as Pandas DataFrame Loader for Python,
    line-separated JSON-file format, HTML code reflecting tables, etc. [[1](#bib.bib1)]
    In our method, each table entry is converted into JSON dictionary format, preserving
    the original table structure and enabling the model to understand the semantics
    of each value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据格式工程。由于LLM被设计为序列到序列模型，将表格数据输入LLM需要将结构化数据转换为文本格式。一种简单的方法是直接输入可由编程语言读取的数据结构，例如Python的Pandas
    DataFrame加载器、行分隔的JSON文件格式、反映表格的HTML代码等。[[1](#bib.bib1)] 在我们的方法中，每个表格条目被转换为JSON字典格式，保留了原始表格结构，使模型能够理解每个值的语义。
- en: 'For a table entry $s_{i}$ is defined as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个表格条目 $s_{i}$ 的定义如下：
- en: '|  | $\displaystyle t_{i,j}=[f_{j}:v_{i,j}]\qquad$ |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle t_{i,j}=[f_{j}:v_{i,j}]\qquad$ |  | (1) |'
- en: '|  | $\displaystyle\textbf{t}_{i}=\{t_{i,1},t_{i,2},\ldots,t_{i,m}\}\qquad$
    |  | (2) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{t}_{i}=\{t_{i,1},t_{i,2},\ldots,t_{i,m}\}\qquad$
    |  | (2) |'
- en: 'We concatenate $k$ using a permutation. This operation results in a new sequence
    where the order of features is randomized, ensuring that the model learns to be
    invariant to feature order. Therefore, a template for this instruction fine-tuning
    dataset is shown as Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Synthetic Tabular Data Generation
    Framework ‣ 3 Our Framework: HARMONIC ‣ HARMONIC: Harnessing LLMs for Tabular
    Data Synthesis and Privacy Protection") ²²2For illustrative examples, please refer
    to Appendix [A.5](#A1.SS5 "A.5 Data Instance ‣ Appendix A Datasets Details ‣ HARMONIC:
    Harnessing LLMs for Tabular Data Synthesis and Privacy Protection")..'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过排列组合的方式连接$k$。这个操作生成了一个新的序列，其中特征的顺序被随机化，确保模型学习到对特征顺序的不变性。因此，这个指令微调数据集的模板如图[1](#S3.F1
    "图1 ‣ 3.1 合成表格数据生成框架 ‣ 3 我们的框架：HARMONIC ‣ HARMONIC：利用LLM进行表格数据合成和隐私保护") ²²2有关示例，请参见附录[A.5](#A1.SS5
    "A.5 数据实例 ‣ 附录A 数据集详细信息 ‣ HARMONIC：利用LLM进行表格数据合成和隐私保护")..
- en: '![Refer to caption](img/8eb75756a937593a83fc71bcd73fdff3.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8eb75756a937593a83fc71bcd73fdff3.png)'
- en: 'Figure 2: The sampling step involves inputting a prompt, shown within the white
    box in the upper left corner (a), into the fine-tuned pretrained LLM. This results
    in a textual output (b), which is then converted into a table using pattern matching
    (c).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：采样步骤包括将一个提示（如图中左上角的白色框（a）所示）输入到经过微调的预训练LLM中。这会生成一个文本输出（b），然后通过模式匹配（c）将其转换为表格。
- en: 'Construct prompt dataset for generation. To generate synthetic data, we need
    to construct a prompt dataset consistent in format with the fine-tuning dataset.
    There are three key differences between the prompt dataset and the fine-tuning
    dataset: (1) The prompts used for generating data remain consistent with those
    used during fine-tuning, with the exception of the $\mathrm{OUTPUT}$ real data
    points in the prompt dataset is randomly resampled from the real data, unlike
    in the fine-tuning dataset, preventing the model from reproducing the original
    real data, and it does not require filtering operations. (3) The size of the prompt
    dataset should be larger than the number of synthetic data samples required, as
    each prompt can generate only one piece of synthetic data.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 构建生成的提示数据集。为了生成合成数据，我们需要构建一个与微调数据集格式一致的提示数据集。提示数据集与微调数据集之间有三个关键区别：（1）用于生成数据的提示与微调过程中使用的提示保持一致，唯一的例外是
    $\mathrm{OUTPUT}$ 真实数据点在提示数据集中是从真实数据中随机重新采样的，而不是在微调数据集中，以防止模型再现原始真实数据，并且不需要过滤操作。（3）提示数据集的大小应大于所需的合成数据样本数量，因为每个提示只能生成一条合成数据。
- en: 3.1.2 Instruction Tuning
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 指令调优
- en: 'We then fine-tune the LLM for the synthetic data generation task using the
    instruction dataset we constructed. Unlike pretraining-based LLMs for this task,
    our aim is to prevent the LLM from memorizing the original tabular data in the
    dataset. After tokenizing our instruction dataset, the resulting token embeddings
    of one sample for the $\mathrm{INPUT}$ input real data points. This objective
    function is formulated as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用构建的指令数据集对 LLM 进行合成数据生成任务的微调。与基于预训练的 LLM 不同，我们的目标是防止 LLM 记忆数据集中原始的表格数据。在对我们的指令数据集进行标记化后，得到的标记嵌入是
    $\mathrm{INPUT}$ 输入真实数据点的一个样本。这个目标函数的形式为：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: The LLM is trained by optimizing the parameters to maximize the probability
    $\prod_{\textbf{ft}\in FT}p(\textbf{ft})$ to protect privacy.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 通过优化参数来最大化概率 $\prod_{\textbf{ft}\in FT}p(\textbf{ft})$ 以保护隐私。
- en: 3.1.3 Sampling
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 采样
- en: We denote the fine-tuned LLM as the generator G. Each data point in the prompt
    dataset is fed into G, yielding the distribution of subsequent tokens conditioned
    on the known input sequence. To generate the next token with more diversity and
    protect privacy, we adopt a weighted sampling strategy incorporating a temperature
    coefficient $T$ to 0.7\. After generation, we utilize pattern-matching algorithms,
    as described in [[30](#bib.bib30)], to reconvert the generated textual feature
    representations into a dataframe format, resulting in the final synthetic tabular
    dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将微调后的 LLM 称为生成器 G。将提示数据集中的每个数据点输入到 G 中，生成基于已知输入序列的后续标记的分布。为了生成更多样化的下一个标记并保护隐私，我们采用了一个加权采样策略，温度系数
    $T$ 设为 0.7\. 生成后，我们使用模式匹配算法，如 [[30](#bib.bib30)] 所述，将生成的文本特征表示重新转换为数据框格式，从而得到最终的合成表格数据集。
- en: 3.2 Synthetic Tabular Data Evaluation Framework
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 合成表格数据评估框架
- en: 'We introduce two new metrics to evaluate the quality and privacy of synthetic
    data for LLM-based synthesis methods: LLM Efficacy (LLE) and Data Leakage Test
    (DLT).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了两个新的指标来评估基于 LLM 的合成数据生成方法的质量和隐私性：LLM 效能（LLE）和数据泄漏测试（DLT）。
- en: '3.2.1 LLE: LLM Efficacy'
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 LLE：LLM 效能
- en: With the development of LLMs, we believe that evaluating the quality of synthetic
    data using weak classifiers is losing its practical value and credibility. More
    and more pepole are concerned with the performance of synthetic data as a training
    set for state-of-the-art methods [[14](#bib.bib14)]. Recent research exploring
    the application of LLMs to tabular data processing has yielded significant advancements,
    with potential to rival or even surpass state-of-the-art machine learning approaches
    [[31](#bib.bib31)]. Therefore, we propose using synthetic data to fine-tune a
    pretrained LLM and then evaluate the fine-tuned LLM on the real test set. We refer
    to this as LLM Efficacy (LLE). We choose LLaMA-2-7b-chat [[32](#bib.bib32)] as
    the base model to compute the LLE.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 的发展，我们认为使用弱分类器评估合成数据的质量已经失去了实际价值和可信度。越来越多的人关注合成数据作为最先进方法的训练集的表现 [[14](#bib.bib14)]。近期研究探索了将
    LLM 应用于表格数据处理，取得了显著进展，有可能与最先进的机器学习方法相媲美甚至超越 [[31](#bib.bib31)]。因此，我们建议使用合成数据来微调预训练的
    LLM，然后在真实测试集上评估微调后的 LLM。我们称之为 LLM 效能（LLE）。我们选择 LLaMA-2-7b-chat [[32](#bib.bib32)]
    作为基础模型来计算 LLE。
- en: '3.2.2 DLT: Data Leakage Test'
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 DLT：数据泄漏测试
- en: The metrics Distance to Closest Record (DCR) [[10](#bib.bib10)] and SDMetrics
    [[27](#bib.bib27)] focus on measuring the "distance" between synthetic data and
    real data, without taking into account the extent to which the generator itself
    leaks data. Research indicates that LLMs are susceptible to data leakage issues
    to varying degrees [[20](#bib.bib20)]. Attacks on LLMs of synthetic data generator
    can potentially extract complete training data, leading to severe privacy breaches.
    To address this, we propose a new metric for quantifying privacy protection, named
    the Data Leakage Test (DLT), inspired by the work of Skywork [[33](#bib.bib33)].
    This metric measures the extent to which a generator leaks real data, thereby
    reflecting the privacy level of the synthetic data. The $\mathrm{DLT}$ computes
    the perplexity of the generator on both synthetic and real data to determine its
    data generation tendencies.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量合成数据与真实数据“距离”的指标包括距离最近记录（DCR）[[10](#bib.bib10)]和SDMetrics [[27](#bib.bib27)]，它们关注合成数据与真实数据之间的“距离”，而未考虑生成器自身泄漏数据的程度。研究表明，LLM对数据泄漏问题的敏感程度各不相同[[20](#bib.bib20)]。对LLM的合成数据生成器的攻击可能会提取完整的训练数据，导致严重的隐私泄露。为此，我们提出了一种新的隐私保护量化指标，名为数据泄漏测试（DLT），灵感来源于Skywork的工作[[33](#bib.bib33)]。该指标衡量生成器泄漏真实数据的程度，从而反映合成数据的隐私水平。$\mathrm{DLT}$计算生成器在合成数据和真实数据上的困惑度，以确定其数据生成倾向。
- en: To compute the $\mathrm{DLT}$ denotes the probability of generating a sentence.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 计算$\mathrm{DLT}$表示生成一个句子的概率。
- en: '|  | $\displaystyle\mathrm{DLT}$ |  | (4) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{DLT}$ |  | (4) |'
- en: '|  | $1$2 |  | (5) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 4 Experiment
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we select four real-world datasets to compare the performance
    of HARMONIC with various types of data synthesis methods. The comparison is conducted
    from two perspectives: the effectiveness of the synthesized data and its privacy.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们选择了四个真实世界的数据集，以比较HARMONIC与各种数据合成方法的性能。比较从两个角度进行：合成数据的有效性及其隐私性。
- en: 4.1 Experimental Setup
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Datasets. To evaluate the proposed method, we utilized four real-world datasets
    from various domains, namely GM (German [[34](#bib.bib34)]), AD (Adult Income
    [[35](#bib.bib35)]), DI (Diabetes)³³3[https://www.openml.org/search?type=data&sort=runs&id=37](https://www.openml.org/search?type=data&sort=runs&id=37),
    BU (Buddy)⁴⁴4[https://www.kaggle.com/datasets/akash14/adopt-a-buddy](https://www.kaggle.com/datasets/akash14/adopt-a-buddy),
    which are all open source datasets and don’t contain any personal information
    such as names, phone numbers, addresses, or other sensitive data. These datasets
    differ in size, feature types, and the number of features, ranging from fewer
    than 1,000 to tens of thousands of samples. Some datasets include only numerical
    features, while others contain both numerical and categorical features. We divided
    each dataset into training, validation, and test sets in approximately a 7:1:2
    ratio. All models were trained on the same training data samples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。为了评估所提方法，我们利用了来自不同领域的四个真实世界数据集，即GM（德国[[34](#bib.bib34)]）、AD（成人收入[[35](#bib.bib35)]）、DI（糖尿病）³³3[https://www.openml.org/search?type=data&sort=runs&id=37](https://www.openml.org/search?type=data&sort=runs&id=37)、BU（Buddy）⁴⁴4[https://www.kaggle.com/datasets/akash14/adopt-a-buddy](https://www.kaggle.com/datasets/akash14/adopt-a-buddy)，这些数据集都是开源的，不包含个人信息，如姓名、电话号码、地址或其他敏感数据。这些数据集在大小、特征类型和特征数量上有所不同，从不到1000个样本到几万个样本不等。一些数据集仅包含数值特征，而其他数据集包含数值和分类特征。我们将每个数据集分为训练集、验证集和测试集，比例大约为7:1:2。所有模型均在相同的训练数据样本上进行训练。
- en: Baselines. There are numerous synthetic methods for generating tabular data.
    Based on the classification approach discussed previously, we selected the most
    representative methods as our baselines. SMOTE [[23](#bib.bib23)] is a simple
    interpolation method proposed for oversampling minority classes and can also be
    used for generating synthetic data. TVAE [[9](#bib.bib9)] is a state-of-the-art
    method for tabular data generation based on VAE. CTABGAN [[10](#bib.bib10)] is
    a GAN-based model that performs exceptionally well across a diverse set of benchmarks.
    TabDDPM [[14](#bib.bib14)] serves as a famous benchmark for Diffusion-based Methods.
    TABSYN [[15](#bib.bib15)] achieves faster synthesis compared to other diffusion-based
    techniques. GReaT [[18](#bib.bib18)] and REaLTabFormer [[24](#bib.bib24)] are
    SOTA tabular data synthesizers based on LLMs, to be precise, both are based on
    GPT-2 [[19](#bib.bib19)]. The code for these methods can be found on GitHub.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。生成表格数据的合成方法有很多。基于前面讨论的分类方法，我们选择了最具代表性的方法作为我们的基线。SMOTE [[23](#bib.bib23)]
    是一种简单的插值方法，旨在过采样少数类，也可用于生成合成数据。TVAE [[9](#bib.bib9)] 是一种基于 VAE 的先进表格数据生成方法。CTABGAN
    [[10](#bib.bib10)] 是一种基于 GAN 的模型，在各种基准测试中表现优异。TabDDPM [[14](#bib.bib14)] 是 Diffusion-based
    Methods 的著名基准。TABSYN [[15](#bib.bib15)] 实现了比其他基于扩散的技术更快的合成。GReaT [[18](#bib.bib18)]
    和 REaLTabFormer [[24](#bib.bib24)] 是基于 LLM 的 SOTA 表格数据合成器，具体来说，两者都基于 GPT-2 [[19](#bib.bib19)]。这些方法的代码可以在
    GitHub 上找到。
- en: Metrics. For the effectiveness of synthetic data, we evaluate it using our proposed
    LLE metric. Specifically, we convert the synthetic tabular data into the text
    format required for specific classification tasks, then feed this data into a
    pre-trained LLM for fine-tuning. The fine-tuned model is then tested using a test
    set that has been similarly converted into the corresponding text format, and
    the weighted average F1 score is obtained. We also fine-tune the pre-trained LLM
    using the training set of real data and evaluate it on the test set. The synthetic
    data is considered to have practical value if the performance of the fine-tuned
    model using synthetic data is on par with or better than that using real data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。为了评估合成数据的有效性，我们使用了我们提出的 LLE 指标。具体来说，我们将合成的表格数据转换为特定分类任务所需的文本格式，然后将这些数据输入到预训练的
    LLM 中进行微调。然后使用相同转换为相应文本格式的测试集对微调后的模型进行测试，并获得加权平均 F1 分数。我们还使用真实数据的训练集对预训练 LLM 进行微调，并在测试集上进行评估。如果使用合成数据的微调模型的性能与使用真实数据的模型相当或更好，则认为合成数据具有实际价值。
- en: 'For the privacy of synthetic data, we use three different metrics to evaluate
    privacy: DCR [[10](#bib.bib10)] and NRS($\mathrm{NewRowSynthesis}$) [[27](#bib.bib27)],
    and our proposed DLT metric. All three metrics are positively correlated with
    privacy, meaning that higher values indicate stronger privacy.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保护合成数据的隐私，我们使用了三种不同的指标来评估隐私：DCR [[10](#bib.bib10)] 和 NRS($\mathrm{NewRowSynthesis}$)
    [[27](#bib.bib27)]，以及我们提出的 DLT 指标。这三种指标都与隐私正相关，意味着值越高隐私越强。
- en: 'Implementation Details. ⁵⁵5For detailed configurations of the fine-tuning,
    please refer to Appendix [B](#A2 "Appendix B Experimental Details ‣ HARMONIC:
    Harnessing LLMs for Tabular Data Synthesis and Privacy Protection"). Our approach
    allows for the selection of any pre-trained generative LLM that supports fine-tuning,
    such as GPT-2 [[19](#bib.bib19)], LLaMA-2-7b-chat [[32](#bib.bib32)], Mistral
    [[36](#bib.bib36)], etc., as the base model. By default, our method opts for LLaMA-2-7b-chat
    [[32](#bib.bib32)] as the base model due to its rich pre-training corpus, resulting
    in a stronger language understanding capability compared to GPT-2 [[19](#bib.bib19)].
    This enables LLaMA-2-7b-chat [[32](#bib.bib32)] to learn fine-tuning tasks more
    efficiently. However, users also have the flexibility to switch base models according
    to their specific requirements. Considering the time cost of the entire experiment,
    we choose lora [[37](#bib.bib37)] efficient fine-tuning instead of full parameter
    adjustment.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '实施细节。⁵⁵5有关微调的详细配置，请参见附录[B](#A2 "附录 B 实验细节 ‣ HARMONIC: 利用 LLM 进行表格数据合成与隐私保护")。我们的方法允许选择任何支持微调的预训练生成
    LLM，例如 GPT-2 [[19](#bib.bib19)]、LLaMA-2-7b-chat [[32](#bib.bib32)]、Mistral [[36](#bib.bib36)]
    等作为基础模型。默认情况下，由于 LLaMA-2-7b-chat [[32](#bib.bib32)] 具有丰富的预训练语料库，其语言理解能力比 GPT-2
    [[19](#bib.bib19)] 更强，因此我们选择 LLaMA-2-7b-chat [[32](#bib.bib32)] 作为基础模型。这使得 LLaMA-2-7b-chat
    [[32](#bib.bib32)] 能够更高效地学习微调任务。然而，用户也可以根据具体需求灵活更换基础模型。考虑到整个实验的时间成本，我们选择了 lora
    [[37](#bib.bib37)] 高效微调而非全面参数调整。'
- en: 4.2 The Effectiveness of Synthetic Data
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 合成数据的有效性
- en: Our experimental results offer compelling evidence that the synthetic data generated
    by our method can effectively serve as a substitute for real data in downstream
    tasks. This finding aligns with the growing recognition that traditional Machine
    Learning Efficacy (MLE) metrics may not be well-suited for evaluating the effectiveness
    of synthetic data used with modern LLMs. Relying solely on MLE metrics can be
    misleading when evaluating LLMs, potentially leading to inaccurate conclusions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验结果提供了有力的证据，证明我们方法生成的合成数据可以有效地替代真实数据用于下游任务。这一发现与越来越多的认识一致，即传统的机器学习有效性（MLE）指标可能不适合用于评估用于现代LLM的合成数据的有效性。仅依赖MLE指标进行LLM评估可能会产生误导，导致不准确的结论。
- en: 'Table 1: The results for effectiveness. The best results are marked in bold,
    the second-best results are underlined. All results are averages over 3 trials
    with different random seeds.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 有效性结果。最佳结果以**粗体**标记，第二最佳结果下划线标记。所有结果都是基于不同随机种子进行的3次试验的平均值。'
- en: '| Dataset | Metric | Original | HARMONIC | SMOTE | TVAE | CTAB | TabDDPM |
    TABSYN | GReaT | RTF |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 指标 | 原始 | HARMONIC | SMOTE | TVAE | CTAB | TabDDPM | TABSYN | GReaT
    | RTF |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GM | MLE | $0.50_{\pm 0.00}$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| GM | MLE | $0.50_{\pm 0.00}$ |'
- en: '|  | LLE | $0.71_{\pm 0.00}$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | LLE | $0.71_{\pm 0.00}$ |'
- en: '| AD | MLE | $0.61_{\pm 0.00}$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| AD | MLE | $0.61_{\pm 0.00}$ |'
- en: '|  | LLE | $0.81_{\pm 0.00}$ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | LLE | $0.81_{\pm 0.00}$ |'
- en: '| DI | MLE | $0.56_{\pm 0.00}$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| DI | MLE | $0.56_{\pm 0.00}$ |'
- en: '|  | LLE | $0.70_{\pm 0.00}$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | LLE | $0.70_{\pm 0.00}$ |'
- en: '| BU | MLE | $0.38_{\pm 0.00}$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| BU | MLE | $0.38_{\pm 0.00}$ |'
- en: '|  | LLE | $0.88_{\pm 0.00}$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | LLE | $0.88_{\pm 0.00}$ |'
- en: 'Therefore, we primarily base our analysis and evaluation on the LLM Efficacy
    (LLE) metric. This metric provides a more nuanced assessment of the quality and
    effectiveness of synthetic data specifically for LLM-based tasks. Table [1](#S4.T1
    "Table 1 ‣ 4.2 The Effectiveness of Synthetic Data ‣ 4 Experiment ‣ HARMONIC:
    Harnessing LLMs for Tabular Data Synthesis and Privacy Protection") summarizes
    the weighted average F1 scores achieved on classification tasks using the LLaMA-2-7b-chat
    model. Each value in the table represents the average F1 score obtained across
    three independent runs of the synthetic data generation process, using different
    random seeds to ensure robustness. The results presented in Table [1](#S4.T1 "Table
    1 ‣ 4.2 The Effectiveness of Synthetic Data ‣ 4 Experiment ‣ HARMONIC: Harnessing
    LLMs for Tabular Data Synthesis and Privacy Protection") (LLE) demonstrate the
    effectiveness of our method. While our method surpasses the real training set
    on the DI dataset, its performance on the remaining three datasets falls slightly
    short. However, the average decrease compared to the real data benchmark is less
    than 5%, which falls within an acceptable range for practical applications. Notably,
    even TABSYN , boasting the best overall performance among the compared methods,
    only outperforms the real training set on two datasets (GM and DI). Furthermore,
    our method exhibits a distinct advantage in terms of stability. Compared to other
    prominent LLM-based methods like GReaT and RTF (REaLTabFormer), our synthetic
    data generation process produces results with a significantly lower standard deviation.
    This indicates that our method generates data with greater consistency and reliability,
    leading to more predictable performance in downstream LLM tasks.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们主要基于LLM有效性（LLE）指标来进行分析和评估。该指标提供了对合成数据质量和有效性的更为细致的评估，特别是针对基于LLM的任务。表[1](#S4.T1
    "表 1 ‣ 4.2 合成数据的有效性 ‣ 4 实验 ‣ HARMONIC: 利用LLMs进行表格数据合成和隐私保护")总结了使用LLaMA-2-7b-chat模型在分类任务中获得的加权平均F1分数。表中的每个值表示在合成数据生成过程中的三次独立运行中获得的平均F1分数，使用不同的随机种子以确保稳健性。表[1](#S4.T1
    "表 1 ‣ 4.2 合成数据的有效性 ‣ 4 实验 ‣ HARMONIC: 利用LLMs进行表格数据合成和隐私保护")（LLE）中展示的结果表明了我们方法的有效性。虽然我们的方法在DI数据集上超越了真实训练集，但在其余三个数据集上的表现略显不足。然而，与真实数据基准相比，平均下降幅度不足5%，这一点对于实际应用来说是可以接受的。值得注意的是，即使是整体表现最佳的TABSYN，在两个数据集（GM和DI）上也仅超越了真实训练集。此外，我们的方法在稳定性方面具有明显优势。与其他主要的基于LLM的方法如GReaT和RTF（REaLTabFormer）相比，我们的合成数据生成过程产生的结果具有显著更低的标准差。这表明我们的方法生成的数据具有更高的一致性和可靠性，从而在下游LLM任务中表现更为可预测。'
- en: In conclusion, while our method may not achieve the absolute highest performance
    on every dataset, the results presented in this section overwhelmingly support
    its potential as a viable substitute for real data. The synthetic data generated
    by our method demonstrates both effectiveness and stability, making it a valuable
    tool for various LLM-based applications.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，尽管我们的方法可能在每个数据集上未能达到绝对最高的性能，但本节中展示的结果极力支持其作为真实数据的可行替代品的潜力。我们的方法生成的合成数据展示了其有效性和稳定性，使其成为各种基于LLM的应用中的宝贵工具。
- en: 4.3 The Privacy of Synthetic Data
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 合成数据的隐私
- en: The experimental results demonstrate that our method prioritizes privacy in
    the synthetic data generation. This is particularly beneficial in situations where
    disclosing real data is not feasible due to privacy concerns. In such scenarios,
    our synthetic data serves as a reliable and secure substitute for real data, allowing
    downstream tasks to proceed without compromising sensitive information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，我们的方法在合成数据生成中优先考虑隐私。这在由于隐私问题无法披露真实数据的情况下尤为有利。在这种情况下，我们的合成数据作为真实数据的可靠且安全的替代品，使下游任务可以在不泄露敏感信息的情况下进行。
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.3 The Privacy of Synthetic Data ‣ 4 Experiment
    ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection")
    presents three key privacy metric scores to quantify the effectiveness of our
    method. Analyzing the results in Table [2](#S4.T2 "Table 2 ‣ 4.3 The Privacy of
    Synthetic Data ‣ 4 Experiment ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis
    and Privacy Protection"), it’s evident that our method surpasses or comes in a
    close second for almost all datasets across all three metrics. This translates
    to demonstrably stronger privacy protection compared to existing methods.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S4.T2 "Table 2 ‣ 4.3 The Privacy of Synthetic Data ‣ 4 Experiment ‣
    HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection")展示了三个关键隐私度量分数，以量化我们方法的有效性。从表 [2](#S4.T2
    "Table 2 ‣ 4.3 The Privacy of Synthetic Data ‣ 4 Experiment ‣ HARMONIC: Harnessing
    LLMs for Tabular Data Synthesis and Privacy Protection")的结果分析可以明显看出，我们的方法在几乎所有数据集的所有三个指标上均超越或接近第二名。这意味着相比于现有方法，我们的隐私保护显著更强。'
- en: 'Table 2: The results for privacy. The best results are marked in bold, the
    second-best results are underlined. Each dataset has three metrics, and in all
    cases, higher values are better.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 隐私结果。最佳结果用粗体标记，第二最佳结果用下划线标记。每个数据集有三个度量，在所有情况下，更高的值更好。'
- en: '| Dataset | Metric | HARMONIC | SMOTE | TVAE | CTAB | TabDDPM | TABSYN | GReaT
    | RTF |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 度量 | HARMONIC | SMOTE | TVAE | CTAB | TabDDPM | TABSYN | GReaT | RTF
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GM | NRS | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GM | NRS | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |'
- en: '|  | DCR | 8.08 | 2.77 | 4.09 | 5.36 | 2.21 | 3.98 | 5.84 | 4.60 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | DCR | 8.08 | 2.77 | 4.09 | 5.36 | 2.21 | 3.98 | 5.84 | 4.60 |'
- en: '|  | DLT | -0.16 | — | — | — | — | — | -2.14 | -22.04 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | DLT | -0.16 | — | — | — | — | — | -2.14 | -22.04 |'
- en: '| AD | NRS | 1.00 | 0.95 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| AD | NRS | 1.00 | 0.95 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |'
- en: '|  | DCR | 2.47 | 0.16 | 0.49 | 0.82 | 0.50 | 0.86 | 1.51 | 0.57 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | DCR | 2.47 | 0.16 | 0.49 | 0.82 | 0.50 | 0.86 | 1.51 | 0.57 |'
- en: '|  | DLT | -0.98 | — | — | — | — | — | -0.67 | -163.71 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | DLT | -0.98 | — | — | — | — | — | -0.67 | -163.71 |'
- en: '| DI | NRS | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DI | NRS | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |'
- en: '|  | DCR | 0.44 | 0.28 | 0.33 | 0.72 | 0.21 | 1.37 | 1.36 | 0.36 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | DCR | 0.44 | 0.28 | 0.33 | 0.72 | 0.21 | 1.37 | 1.36 | 0.36 |'
- en: '|  | DLT | -0.37 | — | — | — | — | — | -0.44 | -42.46 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | DLT | -0.37 | — | — | — | — | — | -0.44 | -42.46 |'
- en: '| BU | NRS | 1.00 | 0.93 | 1.00 | 1.00 | 0.99 | 1.00 | 1.00 | 1.00 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| BU | NRS | 1.00 | 0.93 | 1.00 | 1.00 | 0.99 | 1.00 | 1.00 | 1.00 |'
- en: '|  | DCR | 2.52 | 0.15 | 0.66 | 0.70 | 0.18 | 1.38 | 8.30 | 0.38 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | DCR | 2.52 | 0.15 | 0.66 | 0.70 | 0.18 | 1.38 | 8.30 | 0.38 |'
- en: '|  | DLT | -0.34 | — | — | — | — | — | -2.22 | -41.13 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | DLT | -0.34 | — | — | — | — | — | -2.22 | -41.13 |'
- en: However, the privacy benefits go beyond the quantitative metrics. The design
    of our method inherently offers superior security. An attacker attempting to reconstruct
    a single real data record would need knowledge of nearly the entire set of k real
    data records (typically set to 5). This includes knowing the sequence of each
    feature within a record and the specific order of these k samples. This significantly
    raises the bar for attackers compared to methods like GReaT, which exposes a vulnerability
    where an attacker with knowledge of just one or two feature values in a real record
    can potentially reconstruct the entire record.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，隐私收益超越了定量指标。我们方法的设计固有地提供了更高的安全性。一个攻击者若想重建一个真实的数据记录，需要了解几乎整个 k 个真实数据记录的集合（通常设置为
    5）。这包括了解记录中每个特征的序列以及这些 k 个样本的具体顺序。这显著提高了攻击者的难度，相比于像 GReaT 这样的方式，后者暴露了一个漏洞，即攻击者只需了解真实记录中的一两个特征值就有可能重建整个记录。
- en: 5 Conclusion
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we introduce HARMONIC, a novel framework that leverages the power
    of LLMs for synthesizing tabular data and privacy concerns. HARMONIC enables LLMs
    to capture both the internal feature relationships within individual data points
    and the broader connections between samples by instruction fine-tuning. Recognizing
    the crucial importance of privacy, we have proposed DLT specifically for detecting
    data privacy in LLM synthesis. Extensive evaluations across four real-world datasets
    for classification tasks showcase HARMONIC’s ability to achieve this crucial balance
    of effectiveness and privacy. HARMONIC demonstrably offers robust privacy protection
    while preserving the effectiveness of the synthetic data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 HARMONIC，这是一个新颖的框架，利用大语言模型（LLMs）的力量来综合表格数据和隐私问题。HARMONIC 使 LLMs 通过指令微调来捕捉单个数据点内部特征之间的关系以及样本之间的广泛联系。鉴于隐私的重要性，我们提出了
    DLT 专门用于检测 LLM 综合中的数据隐私。通过在四个真实世界数据集上的广泛评估，展示了 HARMONIC 在效果和隐私之间取得这一关键平衡的能力。HARMONIC
    显著提供了强大的隐私保护，同时保持了合成数据的有效性。
- en: Limitations. Compared to other methods, our approach requires a longer processing
    time for larger LLMs. In addition, because LLMs are less sensitive to numerical
    data and are better suited for classification tasks rather than regression tasks.
    As a result, our current work focuses solely on tabular data used for classification
    tasks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 限制。与其他方法相比，我们的方法需要更长的处理时间来处理较大的 LLMs。此外，由于 LLMs 对数值数据的敏感性较低，更适合分类任务而非回归任务。因此，我们目前的工作仅专注于用于分类任务的表格数据。
- en: References
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi,
    Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, and Christos Faloutsos.
    Large language models on tabular data–a survey. arXiv preprint arXiv:2402.17944,
    2024.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi,
    Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, 和 Christos Faloutsos.
    大语言模型在表格数据上的应用综述。arXiv 预印本 arXiv:2402.17944, 2024。'
- en: '[2] Weizheng Lu, Jiaming Zhang, Jing Zhang, and Yueguo Chen. Large language
    model for table processing: A survey, 2024.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Weizheng Lu, Jiaming Zhang, Jing Zhang, 和 Yueguo Chen. 大语言模型用于表格处理：综述，2024。'
- en: '[3] Alejandro Mottini, Alix Lheritier, and Rodrigo Acuna-Agost. Airline passenger
    name record generation using generative adversarial networks. arXiv preprint arXiv:1807.06657,
    2018.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Alejandro Mottini, Alix Lheritier, 和 Rodrigo Acuna-Agost. 使用生成对抗网络生成航空旅客姓名记录。arXiv
    预印本 arXiv:1807.06657, 2018。'
- en: '[4] Richard J Chen, Ming Y Lu, Tiffany Y Chen, Drew FK Williamson, and Faisal
    Mahmood. Synthetic data in machine learning for medicine and healthcare. Nature
    Biomedical Engineering, 5(6):493–497, 2021.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Richard J Chen, Ming Y Lu, Tiffany Y Chen, Drew FK Williamson, 和 Faisal
    Mahmood. 医学和医疗保健中的合成数据。Nature Biomedical Engineering, 5(6):493–497, 2021。'
- en: '[5] Chaeyoon Jeong, Sundong Kim, Jaewoo Park, and Yeonsoo Choi. Customs import
    declaration datasets. arXiv preprint arXiv:2208.02484, 2022.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Chaeyoon Jeong, Sundong Kim, Jaewoo Park, 和 Yeonsoo Choi. 海关进口申报数据集。arXiv
    预印本 arXiv:2208.02484, 2022。'
- en: '[6] Zilong Zhao, Robert Birke, and Lydia Chen. Tabula: Harnessing language
    models for tabular data synthesis. arXiv preprint arXiv:2310.12746, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Zilong Zhao, Robert Birke, 和 Lydia Chen. Tabula：利用语言模型进行表格数据合成。arXiv 预印本
    arXiv:2310.12746, 2023。'
- en: '[7] Qinyi Liu, Mohammad Khalil, Jelena Jovanovic, and Ronas Shakya. Scaling
    while privacy preserving: A comprehensive synthetic tabular data generation and
    evaluation in learning analytics. In Proceedings of the 14th Learning Analytics
    and Knowledge Conference, pages 620–631, 2024.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Qinyi Liu, Mohammad Khalil, Jelena Jovanovic, 和 Ronas Shakya. 在隐私保护的同时扩展：学习分析中的综合合成表格数据生成与评估。发表于第14届学习分析与知识会议论文集，页面
    620–631, 2024。'
- en: '[8] Alycia N Carey, Karuna Bhaila, Kennedy Edemacu, and Xintao Wu. Dp-tabicl:
    In-context learning with differentially private tabular data. arXiv preprint arXiv:2403.05681,
    2024.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Alycia N Carey, Karuna Bhaila, Kennedy Edemacu, 和 Xintao Wu. Dp-tabicl:
    使用差分隐私表格数据进行上下文学习。arXiv 预印本 arXiv:2403.05681, 2024。'
- en: '[9] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni.
    Modeling tabular data using conditional gan. Advances in neural information processing
    systems, 32, 2019.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, 和 Kalyan Veeramachaneni.
    使用条件 gan 对表格数据进行建模。神经信息处理系统进展, 32, 2019。'
- en: '[10] Zilong Zhao, Aditya Kunar, Robert Birke, and Lydia Y Chen. Ctab-gan: Effective
    table data synthesizing. In Asian Conference on Machine Learning, pages 97–112\.
    PMLR, 2021.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zilong Zhao, Aditya Kunar, Robert Birke, 和 Lydia Y Chen. Ctab-gan: 高效的表格数据合成。发表于亚洲机器学习会议，页面
    97–112。PMLR, 2021。'
- en: '[11] Bingyang Wen, Yupeng Cao, Fan Yang, Koduvayur Subbalakshmi, and Rajarathnam
    Chandramouli. Causal-tgan: Modeling tabular data using causally-aware gan. In
    ICLR Workshop on Deep Generative Models for Highly Structured Data, 2022.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Bingyang Wen, Yupeng Cao, Fan Yang, Koduvayur Subbalakshmi, 和 Rajarathnam
    Chandramouli. Causal-tgan: 使用因果感知 gan 对表格数据进行建模。发表于 ICLR 工作坊关于高度结构化数据的深度生成模型,
    2022。'
- en: '[12] Syed Mahir Tazwar, Max Knobbout, Enrique Hortal Quesada, and Mirela Popa.
    Tab-vae: A novel vae for generating synthetic tabular data.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Syed Mahir Tazwar, Max Knobbout, Enrique Hortal Quesada, 和 Mirela Popa.
    Tab-vae: 一种新颖的 vae 用于生成合成表格数据。'
- en: '[13] Patricia A Apellániz, Juan Parras, and Santiago Zazo. An improved tabular
    data generator with vae-gmm integration. arXiv preprint arXiv:2404.08434, 2024.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Patricia A Apellániz, Juan Parras, 和 Santiago Zazo. 改进的表格数据生成器与 vae-gmm
    集成。arXiv 预印本 arXiv:2404.08434, 2024。'
- en: '[14] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm:
    Modelling tabular data with diffusion models. In International Conference on Machine
    Learning, pages 17564–17579\. PMLR, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, 和 Artem Babenko. Tabddpm:
    使用扩散模型对表格数据进行建模。发表于国际机器学习会议，页面 17564–17579。PMLR, 2023。'
- en: '[15] Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen,
    Xiao Qin, Christos Faloutsos, Huzefa Rangwala, and George Karypis. Mixed-type
    tabular data synthesis with score-based diffusion in latent space. arXiv preprint
    arXiv:2310.09656, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen,
    Xiao Qin, Christos Faloutsos, Huzefa Rangwala, 和 George Karypis. 使用基于评分的扩散在潜在空间中进行混合类型表格数据合成。arXiv
    预印本 arXiv:2310.09656, 2023。'
- en: '[16] Tongyu Liu, Ju Fan, Nan Tang, Guoliang Li, and Xiaoyong Du. Controllable
    tabular data synthesis using diffusion models. Proceedings of the ACM on Management
    of Data, 2(1):1–29, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Tongyu Liu, Ju Fan, Nan Tang, Guoliang Li, 和 Xiaoyong Du. 使用扩散模型进行可控表格数据合成。ACM
    数据管理会议论文集, 2(1):1–29, 2024。'
- en: '[17] Timur Sattarov, Marco Schreyer, and Damian Borth. Findiff: Diffusion models
    for financial tabular data generation. In Proceedings of the Fourth ACM International
    Conference on AI in Finance, pages 64–72, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Timur Sattarov, Marco Schreyer, 和 Damian Borth. Findiff: 用于金融表格数据生成的扩散模型。发表于第四届
    ACM 国际金融人工智能会议论文集，页面 64–72, 2023。'
- en: '[18] Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji
    Kasneci. Language models are realistic tabular data generators. arXiv preprint
    arXiv:2210.06280, 2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, 和 Gjergji
    Kasneci. 语言模型是现实的表格数据生成器。arXiv 预印本 arXiv:2210.06280, 2022。'
- en: '[19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint
    arXiv:1910.01108, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Victor Sanh, Lysandre Debut, Julien Chaumond, 和 Thomas Wolf. Distilbert，BERT
    的精简版本：更小、更快、更便宜且更轻便。arXiv 预印本 arXiv:1910.01108, 2019。'
- en: '[20] Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and
    Xiuzheng Cheng. On protecting the data privacy of large language models (llms):
    A survey. arXiv preprint arXiv:2403.05156, 2024.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, 和
    Xiuzheng Cheng. 保护大型语言模型（LLMs）数据隐私的研究综述。arXiv 预印本 arXiv:2403.05156, 2024。'
- en: '[21] Bishwas Mandal, George Amariucai, and Shuangqing Wei. Initial exploration
    of zero-shot privacy utility tradeoffs in tabular data using gpt-4. arXiv preprint
    arXiv:2404.05047, 2024.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Bishwas Mandal, George Amariucai, 和 Shuangqing Wei. 初步探索在表格数据中使用 gpt-4
    的零-shot 隐私效用权衡。arXiv 预印本 arXiv:2404.05047, 2024。'
- en: '[22] Thomas Cover and Peter Hart. Nearest neighbor pattern classification.
    IEEE transactions on information theory, 13(1):21–27, 1967.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Thomas Cover 和 Peter Hart. 最近邻模式分类. IEEE信息理论学报, 13(1):21–27, 1967年。'
- en: '[23] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.
    Smote: synthetic minority over-sampling technique. Journal of artificial intelligence
    research, 16:321–357, 2002.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, 和W Philip Kegelmeyer.
    Smote: 合成少数类过采样技术. 人工智能研究杂志, 16:321–357, 2002年。'
- en: '[24] Aivin V Solatorio and Olivier Dupriez. Realtabformer: Generating realistic
    relational and tabular data using transformers. arXiv preprint arXiv:2302.02041,
    2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Aivin V Solatorio和Olivier Dupriez. Realtabformer: 使用变换器生成逼真的关系型和表格数据.
    arXiv预印本arXiv:2302.02041, 2023年。'
- en: '[25] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush,
    and Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances
    in neural information processing systems, 31, 2018.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush,
    和Andrey Gulin. Catboost: 带有分类特征的无偏增强. 神经信息处理系统进展, 31, 2018年。'
- en: '[26] Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang
    Han, Alejandro Lopez-Lira, and Hao Wang. Empowering many, biasing a few: Generalist
    credit scoring through large language models. arXiv preprint arXiv:2310.00566,
    2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang
    Han, Alejandro Lopez-Lira, 和Hao Wang. 授权众多，偏向少数：通过大型语言模型进行通用信用评分. arXiv预印本arXiv:2310.00566,
    2023年。'
- en: '[27] DataCebo, Inc. Synthetic Data Metrics, 10 2023. Version 0.12.0.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] DataCebo, Inc. 合成数据度量, 2023年10月. 版本0.12.0。'
- en: '[28] Jeffrey G Wang, Jason Wang, Marvin Li, and Seth Neel. Pandora’s white-box:
    Increased training data leakage in open llms. arXiv preprint arXiv:2402.17012,
    2024.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Jeffrey G Wang, Jason Wang, Marvin Li, 和Seth Neel. 潘多拉的白盒子：开放LLMs中增加的训练数据泄漏.
    arXiv预印本arXiv:2402.17012, 2024年。'
- en: '[29] V Borisov, T Leemann, K Seßler, J Haug, M Pawelczyk, and G Kasneci. Deep
    neural networks and tabular data: A survey. arxiv 2021. arXiv preprint arXiv:2110.01889.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] V Borisov, T Leemann, K Seßler, J Haug, M Pawelczyk, 和G Kasneci. 深度神经网络与表格数据：综述.
    arxiv 2021. arXiv预印本arXiv:2110.01889。'
- en: '[30] Alfred V Aho and AJ van Leeuwen. Algorithms for finding patterns in strings,
    handbook of theoretical computer science vol a. A, ed. J. van Leeuwen, ElsevierSciencePublishersB,
    1990:257–297, 1990.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Alfred V Aho和AJ van Leeuwen. 字符串模式发现算法, 理论计算机科学手册第a卷. A卷, 编者J. van Leeuwen,
    ElsevierSciencePublishersB, 1990:257–297, 1990年。'
- en: '[31] Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun,
    Jian Wu, and Jintai Chen. Making pre-trained language models great on tabular
    prediction. arXiv preprint arXiv:2403.01841, 2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun,
    Jian Wu, 和Jintai Chen. 使预训练语言模型在表格预测中表现出色. arXiv预印本arXiv:2403.01841, 2024年。'
- en: '[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    等人. Llama 2: 开放基础和微调聊天模型. arXiv预印本arXiv:2307.09288, 2023年。'
- en: '[33] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang,
    Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, et al. Skywork: A more open bilingual
    foundation model. arXiv preprint arXiv:2310.19341, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang,
    Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, 等人. Skywork: 更开放的双语基础模型. arXiv预印本arXiv:2310.19341,
    2023年。'
- en: '[34] Hans Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository,
    1994. DOI: https://doi.org/10.24432/C5NC77.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Hans Hofmann. Statlog（德国信用数据）. UCI机器学习库, 1994年。DOI: https://doi.org/10.24432/C5NC77。'
- en: '[35] Ron Kohavi et al. Scaling up the accuracy of naive-bayes classifiers:
    A decision-tree hybrid. In Kdd, volume 96, pages 202–207, 1996.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ron Kohavi等人. 提升朴素贝叶斯分类器的准确性：决策树混合体. 见于Kdd, 第96卷, 页202–207, 1996年。'
- en: '[36] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre
    Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El
    Sayed. Mistral 7b, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre
    Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和William
    El Sayed. Mistral 7b, 2023年。'
- en: '[37] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适应。arXiv预印本arXiv:2106.09685,
    2021.'
- en: Appendix A Datasets Details
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 数据集详细信息
- en: A.1 Data Source
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数据来源
- en: 'We list the sources of our datasets in Table [3](#A1.T3 "Table 3 ‣ A.1 Data
    Source ‣ Appendix A Datasets Details ‣ HARMONIC: Harnessing LLMs for Tabular Data
    Synthesis and Privacy Protection"), all of which are obtained from publicly accessible
    and reputable websites.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[3](#A1.T3 "表3 ‣ A.1 数据来源 ‣ 附录A 数据集详细信息 ‣ HARMONIC: 利用LLMs进行表格数据合成和隐私保护")中列出了数据集的来源，这些数据集均来自公开可访问的信誉网站。'
- en: '| Dataset | URL |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | URL |'
- en: '| --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| German | [https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data](https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data)
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 德国 | [https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data](https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data)
    |'
- en: '| Adult Income | [https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult)
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 成人收入 | [https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult)
    |'
- en: '| Diabetes | [https://www.openml.org/search?type=data&sort=runs&id=37&status=active](https://www.openml.org/search?type=data&sort=runs&id=37&status=active)
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 糖尿病 | [https://www.openml.org/search?type=data&sort=runs&id=37&status=active](https://www.openml.org/search?type=data&sort=runs&id=37&status=active)
    |'
- en: '| Buddy | [https://www.kaggle.com/datasets/akash14/adopt-a-buddy](https://www.kaggle.com/datasets/akash14/adopt-a-buddy)
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Buddy | [https://www.kaggle.com/datasets/akash14/adopt-a-buddy](https://www.kaggle.com/datasets/akash14/adopt-a-buddy)
    |'
- en: 'Table 3: URLs for real-world datasets of the experiments'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '表3: 实验中真实世界数据集的URLs'
- en: A.2 Data Description
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 数据描述
- en: 'Additionally, we record various statistical details for each dataset in Table
    [4](#A1.T4 "Table 4 ‣ A.2 Data Description ‣ Appendix A Datasets Details ‣ HARMONIC:
    Harnessing LLMs for Tabular Data Synthesis and Privacy Protection").'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们在表[4](#A1.T4 "表4 ‣ A.2 数据描述 ‣ 附录A 数据集详细信息 ‣ HARMONIC: 利用LLMs进行表格数据合成和隐私保护")中记录了每个数据集的各种统计细节。'
- en: German. The German dataset classifies people as good or bad credit risks described
    by a set of attributes including status of existing checking account, duration
    in month, credit history, purpose and more.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 德国。德国数据集将人们分类为良好或不良信用风险，描述了一组属性，包括现有支票账户的状态、持续时间（月）、信用历史、用途等。
- en: Adult Income. The US Adult income dataset was extracted by Barry Becker from
    the 1994 US Census Database. The dataset consists of anonymous information such
    as occupation, age, native country, race, capital gain, capital loss, education,
    work class and more. Each row is labelled as either having a salary greater than
    ">50K" or "<=50K".
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 成人收入。美国成人收入数据集由Barry Becker从1994年美国人口普查数据库中提取。数据集包含匿名信息，如职业、年龄、出生国、种族、资本收益、资本损失、教育、工作类别等。每行标记为收入是否高于">50K"或"<=50K"。
- en: Diabetes. The Diabetes dataset originates from the National Institute of Diabetes
    and Digestive and Kidney Diseases. This dataset comprises medical features including
    the number of times pregnant, diastolic blood pressure, body mass index, age,
    among other variables. The label indicates whether the individual has diabetes
    or not.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病。糖尿病数据集来源于国家糖尿病、消化系统和肾脏疾病研究所。该数据集包括医疗特征，如怀孕次数、舒张压、体重指数、年龄等。标签指示个体是否患有糖尿病。
- en: 'Buddy. The Buddy dataset originates from the HackerEarth Machine Learning Challenge—Adopt
    a Buddy. The dataset consists of parameters such as: a unique ID assigned to each
    animal that is up for adoption, date on which they arrived at the shelter, their
    physical attributes such as color, length and height, among other factors. The
    labels in this dataset denote the breed of the animals.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Buddy。Buddy数据集来源于HackerEarth机器学习挑战——领养一个Buddy。数据集包括参数，如：分配给每个待领养动物的唯一ID、它们到达收容所的日期、其物理属性如颜色、长度和高度等。该数据集中的标签表示动物的品种。
- en: '| Dataset | Domain | # Samples | # Num | # Cat | Tasks | # Classes |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 领域 | 样本数 | 数量 | 类别 | 任务 | 类别数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| German | Financial | 1000 | 7 | 13 | Classification | 2 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 德国 | 财务 | 1000 | 7 | 13 | 分类 | 2 |'
- en: '| Adult Income | Social | 32561 | 6 | 8 | Classification | 2 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 成人收入 | 社会 | 32561 | 6 | 8 | 分类 | 2 |'
- en: '| Diabetes | Medical | 768 | 8 | 0 | Classification | 2 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 糖尿病 | 医疗 | 768 | 8 | 0 | 分类 | 2 |'
- en: '| Buddy | Nature | 18834 | 4 | 5 | Multi-Class | 3 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Buddy | Nature | 18834 | 4 | 5 | 多类 | 3 |'
- en: 'Table 4: Dataset Statistics. # Samples denotes the number of samples in each
    dataset. # Num and # Cat columns indicate numbers of numerical and categorical
    features in each dataset.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '表4：数据集统计信息。# 样本表示每个数据集中的样本数量。# Num 和 # Cat 列指示每个数据集中的数值特征和分类特征的数量。'
- en: A.3 Data Preprocessing
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 数据预处理
- en: 'To maintain consistency in formatting, we converted all four datasets into
    CSV files. Additionally, the other datasets underwent the following preprocessing
    steps:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持格式的一致性，我们将所有四个数据集转换为CSV文件。此外，其他数据集经过了以下预处理步骤：
- en: German. The original label "status" with a value of "1" was converted to "0",
    and the original label "status" with a value of "2" was converted to "1".
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 德语。原始标签“status”值为“1”被转换为“0”，原始标签“status”值为“2”被转换为“1”。
- en: Adult Income. The original label "class" with a value of "<=50K" was converted
    to "0", and the original label "class" with a value of ">50K" was converted to
    "1".
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 成人收入。原始标签“class”值为“<=50K”被转换为“0”，原始标签“class”值为“>50K”被转换为“1”。
- en: Diabetes. The diabetes dataset was used without any additional preprocessing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病。糖尿病数据集在没有任何额外预处理的情况下使用。
- en: Buddy. The original "issue_date" and "listing_date," which were represented
    in the "date_time" format, have been replaced with a timestamp format.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Buddy。原来的“issue_date”和“listing_date”，以“date_time”格式表示，已被替换为时间戳格式。
- en: A.4 Data Field
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 数据字段
- en: 'The instruction fine-tunnig dataset is provided in json format and contains
    the following attributes. And a specific instance of INPUT and OUTPUT can be found
    in [A.5](#A1.SS5 "A.5 Data Instance ‣ Appendix A Datasets Details ‣ HARMONIC:
    Harnessing LLMs for Tabular Data Synthesis and Privacy Protection").'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '指令微调数据集以json格式提供，包含以下属性。特定的INPUT和OUTPUT实例可在[A.5](#A1.SS5 "A.5 Data Instance
    ‣ Appendix A Datasets Details ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis
    and Privacy Protection")中找到。'
- en: '{'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: 'id: [integer] The unique identifier for each instance'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'id: [integer] 每个实例的唯一标识符'
- en: 'conversations: ['
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 'conversations: ['
- en: '{'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: 'from: [string] "human"'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 'from: [string] "human"'
- en: 'value: [string] the INPUT text for LLM fine-tuning'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'value: [string] 用于LLM微调的输入文本'
- en: '},'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: '{'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: 'from: [string] "assistant"'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 'from: [string] "assistant"'
- en: 'value: [string] the OUTPUT text for LLM fine-tunnig'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 'value: [string] 用于LLM微调的输出文本'
- en: '}'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ']'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: '}'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: A.5 Data Instance
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 数据实例
- en: 'To illustrate the data format used for fine-tuning both the generator and downstream
    tasks, we present a complete data instance from the German dataset as an example,
    shown in Table [5](#A1.T5 "Table 5 ‣ A.5 Data Instance ‣ Appendix A Datasets Details
    ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection")
    and Table [6](#A1.T6 "Table 6 ‣ A.5 Data Instance ‣ Appendix A Datasets Details
    ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection")
    respectively.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示用于微调生成器和下游任务的数据格式，我们以德语数据集中的一个完整数据实例作为示例，见表[5](#A1.T5 "Table 5 ‣ A.5 Data
    Instance ‣ Appendix A Datasets Details ‣ HARMONIC: Harnessing LLMs for Tabular
    Data Synthesis and Privacy Protection")和表[6](#A1.T6 "Table 6 ‣ A.5 Data Instance
    ‣ Appendix A Datasets Details ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis
    and Privacy Protection")。'
- en: '| INPUT: Here are 5 tabular data about user credit scores, each containing
    20 columns of features and 1 column of labels, where the ’status’ column is a
    binary classification label. I will transmit the data to you in JSON format. Please
    generate an approximate sample based on these 5 examples.\n Example one: {"Present
    employment since": "A75", "Credit amount": "11816", "Credit history": "A30", "Purpose":
    "A49", "Duration in month": "45", "Other installment plans": "A143", "Age in years":
    "29", "Savings account/bonds": "A61", "status": "1", "foreign worker": "A201",
    "Number of people being liable to provide maintenance for": "1", "Number of existing
    credits at this bank": "2", "Installment rate in percentage of disposable income":
    "2", "Housing": "A151", "Property": "A123", "Present residence since": "4", "Telephone":
    "A191", "Other debtors / guarantors": "A101", "Job": "A173", "Status of existing
    checking account": "A11", "Personal status and sex": "A93"}.\n Example two: {"Housing":
    "A151", "Personal status and sex": "A92", "Credit amount": "6416", "Job": "A173",
    "Property": "A124", "Purpose": "A49", "status": "1", "Number of people being liable
    to provide maintenance for": "1", "Number of existing credits at this bank": "1",
    "Present employment since": "A75", "Other installment plans": "A143", "Installment
    rate in percentage of disposable income": "4", "Present residence since": "3",
    "Status of existing checking account": "A12", "Savings account/bonds": "A61",
    "Telephone": "A191", "Other debtors / guarantors": "A101", "Age in years": "59",
    "Duration in month": "48", "Credit history": "A31", "foreign worker": "A201"}.\n
    Example three: {"Housing": "A151", "Installment rate in percentage of disposable
    income": "4", "Age in years": "31", "Duration in month": "24", "foreign worker":
    "A201", "Number of people being liable to provide maintenance for": "1", "Other
    installment plans": "A143", "Savings account/bonds": "A61", "Present employment
    since": "A73", "Credit history": "A31", "Status of existing checking account":
    "A11", "Job": "A173", "Telephone": "A192", "Number of existing credits at this
    bank": "1", "status": "1", "Personal status and sex": "A93", "Credit amount":
    "3161", "Other debtors / guarantors": "A101", "Purpose": "A49", "Property": "A122",
    "Present residence since": "2"}.\n Example four: {"Purpose": "A49", "Number of
    people being liable to provide maintenance for": "1", "Housing": "A151", "Age
    in years": "26", "Savings account/bonds": "A62", "Other installment plans": "A143",
    "Present employment since": "A73", "Telephone": "A191", "Installment rate in percentage
    of disposable income": "4", "Duration in month": "30", "Number of existing credits
    at this bank": "2", "Personal status and sex": "A92", "Present residence since":
    "4", "Status of existing checking account": "A12", "Job": "A172", "Credit history":
    "A30", "Property": "A123", "Other debtors / guarantors": "A101", "status": "1",
    "Credit amount": "4280", "foreign worker": "A201"}.\n Example five: {"Present
    employment since": "A74", "Credit amount": "3566", "Duration in month": "48",
    "foreign worker": "A201", "Other debtors / guarantors": "A101", "Other installment
    plans": "A143", "Number of existing credits at this bank": "1", "Number of people
    being liable to provide maintenance for": "1", "Credit history": "A31", "Housing":
    "A152", "Present residence since": "2", "Installment rate in percentage of disposable
    income": "4", "Savings account/bonds": "A62", "Telephone": "A191", "status": "0",
    "Job": "A173", "Purpose": "A49", "Age in years": "30", "Personal status and sex":
    "A93", "Property": "A123", "Status of existing checking account": "A12"}.\n Generate
    one sample: |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 输入：这里有5个关于用户信用评分的表格数据，每个数据包含20列特征和1列标签，其中“status”列是二元分类标签。我将以JSON格式传输数据。请根据这5个示例生成一个近似的样本。\n
    示例一：{"目前就业时间": "A75", "信用金额": "11816", "信用历史": "A30", "用途": "A49", "月份持续时间": "45",
    "其他分期计划": "A143", "年龄（年）": "29", "储蓄账户/债券": "A61", "状态": "1", "外籍工人": "A201",
    "需要提供赡养的人数": "1", "在这家银行的现有信用数量": "2", "可支配收入百分比的分期付款率": "2", "住房": "A151", "财产":
    "A123", "目前居住时间": "4", "电话": "A191", "其他债务人/担保人": "A101", "工作": "A173", "现有支票账户状态":
    "A11", "个人状态和性别": "A93"}。\n 示例二：{"住房": "A151", "个人状态和性别": "A92", "信用金额": "6416",
    "工作": "A173", "财产": "A124", "用途": "A49", "状态": "1", "需要提供赡养的人数": "1", "在这家银行的现有信用数量":
    "1", "目前就业时间": "A75", "其他分期计划": "A143", "可支配收入百分比的分期付款率": "4", "目前居住时间": "3",
    "现有支票账户状态": "A12", "储蓄账户/债券": "A61", "电话": "A191", "其他债务人/担保人": "A101", "年龄（年）":
    "59", "月份持续时间": "48", "信用历史": "A31", "外籍工人": "A201"}。\n 示例三：{"住房": "A151", "可支配收入百分比的分期付款率":
    "4", "年龄（年）": "31", "月份持续时间": "24", "外籍工人": "A201", "需要提供赡养的人数": "1", "其他分期计划":
    "A143", "储蓄账户/债券": "A61", "目前就业时间": "A73", "信用历史": "A31", "现有支票账户状态": "A11", "工作":
    "A173", "电话": "A192", "在这家银行的现有信用数量": "1", "状态": "1", "个人状态和性别": "A93", "信用金额":
    "3161", "其他债务人/担保人": "A101", "用途": "A49", "财产": "A122", "目前居住时间": "2"}。\n 示例四：{"用途":
    "A49", "需要提供赡养的人数": "1", "住房": "A151", "年龄（年）": "26", "储蓄账户/债券": "A62", "其他分期计划":
    "A143", "目前就业时间": "A73", "电话": "A191", "可支配收入百分比的分期付款率": "4", "月份持续时间": "30",
    "在这家银行的现有信用数量": "2", "个人状态和性别": "A92", "目前居住时间": "4", "现有支票账户状态": "A12", "工作":
    "A172", "信用历史": "A30", "财产": "A123", "其他债务人/担保人": "A101", "状态": "1", "信用金额": "4280",
    "外籍工人": "A201"}。\n 示例五：{"目前就业时间": "A74", "信用金额": "3566", "月份持续时间": "48", "外籍工人":
    "A201", "其他债务人/担保人": "A101", "其他分期计划": "A143", "在这家银行的现有信用数量": "1", "需要提供赡养的人数":
    "1", "信用历史": "A31", "住房": "A152", "目前居住时间": "2", "可支配收入百分比的分期付款率": "4", "储蓄账户/债券":
    "A62", "电话": "A191", "状态": "0", "工作": "A173", "用途": "A49", "年龄（年）": "30", "个人状态和性别":
    "A93", "财产": "A123", "现有支票账户状态": "A12"}。\n 生成一个样本：|'
- en: '| OUTPUT: {"Present residence since": "4", "Credit amount": "7685", "Age in
    years": "37", "Other installment plans": "A143", "Status of existing checking
    account": "A11", "Housing": "A151", "Credit history": "A31", "Duration in month":
    "48", "Property": "A123", "Purpose": "A49", "Other debtors / guarantors": "A103",
    "Present employment since": "A74", "Installment rate in percentage of disposable
    income": "2", "Job": "A173", "Savings account/bonds": "A61", "Telephone": "A191",
    "Number of people being liable to provide maintenance for": "1", "Number of existing
    credits at this bank": "1", "Personal status and sex": "A92", "foreign worker":
    "A201", "status": "1"}. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| OUTPUT: {"Present residence since": "4", "Credit amount": "7685", "Age in
    years": "37", "Other installment plans": "A143", "Status of existing checking
    account": "A11", "Housing": "A151", "Credit history": "A31", "Duration in month":
    "48", "Property": "A123", "Purpose": "A49", "Other debtors / guarantors": "A103",
    "Present employment since": "A74", "Installment rate in percentage of disposable
    income": "2", "Job": "A173", "Savings account/bonds": "A61", "Telephone": "A191",
    "Number of people being liable to provide maintenance for": "1", "Number of existing
    credits at this bank": "1", "Personal status and sex": "A92", "foreign worker":
    "A201", "status": "1"}. |'
- en: 'Table 5: An instance of the instruction data for the generator'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：生成器的指令数据的一个实例
- en: '| INPUT: Evaluate the creditworthiness of a customer with the following financial
    profile. Respond with only either ’good’ or ’bad’. \n Text: ’The state of Status
    of existing checking account is bigger than 0 DM but smaller than 200 DM, The
    state of Duration in month is 36, The state of Credit history is delay in paying
    off in the past, The state of Purpose is car (new), The state of Credit amount
    is 1873, The state of Savings account or bonds is bigger than 100 smaller than
    500 DM, The state of Present employment since is bigger than 1 smaller than 4
    years, The state of Installment rate in percentage of disposable income is 2,
    The state of Personal status and sex is male and single, The state of Other debtors
    or guarantors is none, The state of Present residence since is 2, The state of
    Property is unknown or no property, The state of Age in years is 29, The state
    of Other installment plans is none, The state of Housing is for free, The state
    of Number of existing credits at this bank is 1.0, The state of Job is management
    or self-employed or highly qualified employee or officer, The state of Number
    of people being liable to provide maintenance for is 1, The state of Telephone
    is yes, registered under the customers name, The state of foreign worker is yes.’\n
    Answer: |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| INPUT: 评估具有以下财务状况的客户的信用worthiness。仅用‘good’或‘bad’作答。 \n Text: ’Status of existing
    checking account 的状态大于 0 DM 但小于 200 DM，Duration in month 的状态为 36，Credit history
    的状态为过去的延迟还款，Purpose 的状态为新车，Credit amount 的状态为 1873，Savings account or bonds 的状态大于
    100 小于 500 DM，Present employment since 的状态大于 1 小于 4 年，Installment rate in percentage
    of disposable income 的状态为 2，Personal status and sex 的状态为男性且单身，Other debtors or
    guarantors 的状态为无，Present residence since 的状态为 2，Property 的状态为未知或无财产，Age in years
    的状态为 29，Other installment plans 的状态为无，Housing 的状态为免费，Number of existing credits
    at this bank 的状态为 1.0，Job 的状态为管理或自雇或高学历员工或官员，Number of people being liable to
    provide maintenance for 的状态为 1，Telephone 的状态为是，登记在客户名下，foreign worker 的状态为是。’\n
    Answer: |'
- en: '| OUTPUT: "bad" |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| OUTPUT: "bad" |'
- en: 'Table 6: An instance of the instruction data for downstream tasks'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：下游任务指令数据的一个实例
- en: Appendix B Experimental Details
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实验细节
- en: B.1 Parameter Selection
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 参数选择
- en: Considering the time cost of the entire experiment, we did not adjust the best
    hyperparameters for different dataset. By conducting experiments on the validation
    set and combining empirical settings, we unified the hyperparameters of the fine-tuning
    process. In the fine-tuning stage, we choose lora[[37](#bib.bib37)] efficient
    fine-tuning instead of full parameter adjustment.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到整个实验的时间成本，我们没有为不同的数据集调整最佳超参数。通过在验证集上进行实验并结合经验设置，我们统一了微调过程的超参数。在微调阶段，我们选择了
    lora[[37](#bib.bib37)] 高效微调，而不是全参数调整。
- en: We fine-tune the LLaMA-2-7b-chat model for each dataset for 5 epochs with a
    batch size of 16\. We utilize the AdamW optimizer for the proposed generative
    models, with the learning rate $3\times 10^{-4}$.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个数据集的 LLaMA-2-7b-chat 模型进行了 5 个周期的微调，批量大小为 16。我们为所提议的生成模型使用了 AdamW 优化器，学习率为
    $3\times 10^{-4}$。
- en: 'For the sampling step, we use 3 random seeds in the data generation stage for
    each dataset, specifically 1234, 1235, and 1236\. We set the temperature parameter
    T to 0.7 for all experiments and datasets. We sample new synthetic data using
    the prompt dataset for generation (Sec [3.1.1](#S3.SS1.SSS1 "3.1.1 Construct Instruction
    Dataset ‣ 3.1 Synthetic Tabular Data Generation Framework ‣ 3 Our Framework: HARMONIC
    ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection")),
    starting with task description and five random real samples(see an example in
    Appendix [A.5](#A1.SS5 "A.5 Data Instance ‣ Appendix A Datasets Details ‣ HARMONIC:
    Harnessing LLMs for Tabular Data Synthesis and Privacy Protection")). We generated
    synthetic datasets for German and Diabetes with the same number of samples as
    their respective training sets. For the Adult Income and Buddy datasets, where
    the training sets are larger, exceeding 10,000 samples, we generated 5,000 samples
    due to the extended time required for sampling with our method.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '对于采样步骤，我们在数据生成阶段对每个数据集使用了3个随机种子，具体为1234、1235和1236。我们将温度参数T设置为0.7用于所有实验和数据集。我们使用提示数据集进行合成数据采样（参见[3.1.1](#S3.SS1.SSS1
    "3.1.1 Construct Instruction Dataset ‣ 3.1 Synthetic Tabular Data Generation Framework
    ‣ 3 Our Framework: HARMONIC ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis
    and Privacy Protection")），从任务描述和五个随机真实样本开始（参见附录[A.5](#A1.SS5 "A.5 Data Instance
    ‣ Appendix A Datasets Details ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis
    and Privacy Protection")）。我们为德语和糖尿病数据集生成了与其各自训练集相同数量的合成数据集。对于成人收入和Buddy数据集，训练集较大，超过10,000个样本，我们生成了5,000个样本，因为我们的采样方法需要较长时间。'
- en: For the MLE metric, we employ logistic regression, decision tree, mlp and random
    forest models.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MLE指标，我们使用逻辑回归、决策树、MLP和随机森林模型。
- en: 'For the LLE metric, the epoch set for fine-tuning the downstream LLaMA-2-7b-chat
    model is 5, the learning rate is $1\times 10^{-4}$, and the batch size is 32\.
    The random seed is fixed when fine-tuning the downstream model. See an example
    of instruction data for downstream tasks in Appendix [A.5](#A1.SS5 "A.5 Data Instance
    ‣ Appendix A Datasets Details ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis
    and Privacy Protection")).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '对于LLE指标，用于微调下游LLaMA-2-7b-chat模型的epoch设置为5，学习率为$1\times 10^{-4}$，批量大小为32。微调下游模型时随机种子是固定的。请参见附录[A.5](#A1.SS5
    "A.5 Data Instance ‣ Appendix A Datasets Details ‣ HARMONIC: Harnessing LLMs for
    Tabular Data Synthesis and Privacy Protection")中的下游任务指令数据示例。'
- en: B.2 Experimental Environment
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 实验环境
- en: Our hardware setup includes 4 NVIDIA A100-40GB GPUs. The system has 1 TB system
    RAM, and runs on an AMD EPYC 7742 processor with 64 cores, using the Ubuntu 22.04
    operating system.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的硬件配置包括4个NVIDIA A100-40GB GPU。系统具有1 TB的系统RAM，运行在具有64个核心的AMD EPYC 7742处理器上，使用Ubuntu
    22.04操作系统。
- en: Appendix C Additional results
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 额外结果
- en: The following presents the results of the ablation study. We conducted comparative
    experiments using the German and Diabetes datasets.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了消融研究的结果。我们使用德语和糖尿病数据集进行了对比实验。
- en: C.1 Filter operation
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 筛选操作
- en: 'Experimental results demonstrate that the filtering step can enhance the quality
    of synthetic data. As shown in Table [7](#A3.T7 "Table 7 ‣ C.1 Filter operation
    ‣ Appendix C Additional results ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis
    and Privacy Protection"), the LLE values decrease without filtering, particularly
    for the German dataset. This is likely due to incorrect labels in the generated
    synthetic data. Additionally, privacy slightly diminishes without the filtering
    step, though the difference is minimal. These findings indicate that the filtering
    step is effective.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果表明，筛选步骤可以提高合成数据的质量。如表[7](#A3.T7 "Table 7 ‣ C.1 Filter operation ‣ Appendix
    C Additional results ‣ HARMONIC: Harnessing LLMs for Tabular Data Synthesis and
    Privacy Protection")所示，在没有筛选的情况下，LLE值下降，尤其是对于德语数据集。这可能是由于生成的合成数据中标签不正确造成的。此外，没有筛选步骤的情况下隐私略有下降，尽管差异很小。这些发现表明筛选步骤是有效的。'
- en: 'Table 7: The results of whether to filter data after kNN, where "w/o fil" means
    not to filter data, and "with fil" means to filter data, which is our original
    method. Each dataset has five metrics, and in all cases, higher values are better.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：kNN后是否筛选数据的结果，其中“w/o fil”表示不筛选数据，而“with fil”表示筛选数据，即我们的原始方法。每个数据集有五个指标，在所有情况下，值越高越好。
- en: '| Dataset | Filter | MLE | LLE | NRS | DCR | DLT |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 筛选 | MLE | LLE | NRS | DCR | DLT |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GM | w/o fil | $0.56_{\pm 0.06}$ | 1.00 | 7.97 | -0.17 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| GM | w/o fil | $0.56_{\pm 0.06}$ | 1.00 | 7.97 | -0.17 |'
- en: '|  | with fil | $0.55_{\pm 0.03}$ | 1.00 | 8.08 | -0.16 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | 带有 fil | $0.55_{\pm 0.03}$ | 1.00 | 8.08 | -0.16 |'
- en: '| DI | w/o fil | $0.56_{\pm 0.06}$ | 1.00 | 0.44 | -0.38 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| DI | 不带 fil | $0.56_{\pm 0.06}$ | 1.00 | 0.44 | -0.38 |'
- en: '|  | with fil | $0.46_{\pm 0.02}$ | 1.00 | 0.44 | -0.37 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | 带有 fil | $0.46_{\pm 0.02}$ | 1.00 | 0.44 | -0.37 |'
- en: C.2 Random feature order permutation
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 随机特征顺序排列
- en: 'Experiments indicate that permuting features can enhance the privacy of synthetic
    data. As shown in the last two columns of Table [8](#A3.T8 "Table 8 ‣ C.2 Random
    feature order permutation ‣ Appendix C Additional results ‣ HARMONIC: Harnessing
    LLMs for Tabular Data Synthesis and Privacy Protection"), there is a significant
    reduction in both the DCR and DLT values when features are not permuted. Concurrently,
    the generated numerical columns tend to produce repeated values, which may also
    contribute to the decrease in the LLE metric. Overall, these results underscore
    the necessity of shuffling features.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '实验表明，特征排列可以增强合成数据的隐私保护。如表 [8](#A3.T8 "表 8 ‣ C.2 随机特征顺序排列 ‣ 附录 C 额外结果 ‣ HARMONIC:
    利用 LLM 进行表格数据合成与隐私保护") 最后两列所示，当特征未打乱时，DCR 和 DLT 值显著下降。同时，生成的数值列往往会产生重复值，这也可能导致
    LLE 指标的降低。总体来看，这些结果突显了打乱特征的必要性。'
- en: 'Table 8: The results of whether to shuffle features, where "w/o pm" means not
    to shuffle the features, and "with pm" means to shuffle the features, which is
    our original method. Each dataset has five metrics, and in all cases, higher values
    are better.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 特征是否打乱的结果，其中“w/o pm”表示不打乱特征，“with pm”表示打乱特征，这也是我们的原始方法。每个数据集有五个指标，在所有情况下，较高的值更好。'
- en: '| Dataset | Permutation | MLE | LLE | NRS | DCR | DLT |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 排列方式 | MLE | LLE | NRS | DCR | DLT |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GM | w/o pm | $0.56_{\pm 0.04}$ | 1.00 | 7.20 | -0.58 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| GM | 不带 pm | $0.56_{\pm 0.04}$ | 1.00 | 7.20 | -0.58 |'
- en: '|  | with pm | $0.55_{\pm 0.03}$ | 1.00 | 8.08 | -0.16 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | 带有 pm | $0.55_{\pm 0.03}$ | 1.00 | 8.08 | -0.16 |'
- en: '| DI | w/o pm | $0.50_{\pm 0.06}$ | 1.00 | 0.42 | -0.67 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| DI | 不带 pm | $0.50_{\pm 0.06}$ | 1.00 | 0.42 | -0.67 |'
- en: '|  | with pm | $0.46_{\pm 0.02}$ | 1.00 | 0.44 | -0.37 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | 带有 pm | $0.46_{\pm 0.02}$ | 1.00 | 0.44 | -0.37 |'
- en: Appendix D Ethics Statement
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 伦理声明
- en: The dataset used in this study is based on open-source data and can be further
    modified. We thoroughly reviewed and verified the data to ensure it does not contain
    any personally identifiable information or offensive content. Additionally, we
    conducted manual audits to ensure there are no sensitive details. Therefore, we
    believe the dataset is secure and its use in the research is ethically sound and
    appropriate for the purposes of this study.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用的数据集基于开源数据，可以进一步修改。我们彻底审查并验证了数据，以确保其中不包含任何可识别个人的信息或冒犯性内容。此外，我们进行了手动审计，以确保没有敏感细节。因此，我们认为数据集是安全的，其在研究中的使用在伦理上是合理的，并且适用于本研究的目的。
