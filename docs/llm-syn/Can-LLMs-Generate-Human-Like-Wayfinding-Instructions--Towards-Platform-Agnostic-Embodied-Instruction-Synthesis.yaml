- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM能生成类人的路径指引吗？面向平台无关的具身指令合成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.11487](https://ar5iv.labs.arxiv.org/html/2403.11487)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.11487](https://ar5iv.labs.arxiv.org/html/2403.11487)
- en: Vishnu Sashank Dorbala    Sanjoy Chowdhury    Dinesh Manocha
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Vishnu Sashank Dorbala    Sanjoy Chowdhury    Dinesh Manocha
- en: University of Maryland, College Park
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 马里兰大学，帕克分校
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We present a novel approach to automatically synthesize “wayfinding instructions"
    for an embodied robot agent. In contrast to prior approaches that are heavily
    reliant on human-annotated datasets designed exclusively for specific simulation
    platforms, our algorithm uses in-context learning to condition an LLM to generate
    instructions using just a few references. Using an LLM-based Visual Question Answering
    strategy, we gather detailed information about the environment which is used by
    the LLM for instruction synthesis. We implement our approach on multiple simulation
    platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating
    its platform-agnostic nature. We subjectively evaluate our approach via a user
    study and observe that $83.3\%$ change in SR), quantifying the viability of generated
    instructions in replacing human-annotated data. We finally discuss the applicability
    of our approach in enabling a generalizable evaluation of embodied navigation
    policies. To the best of our knowledge, ours is the first LLM-driven approach
    capable of generating “human-like" instructions in a platform-agnostic manner,
    without training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的方法来自动合成“路径指引”以供具身机器人代理使用。与依赖于专为特定仿真平台设计的人类标注数据集的先前方法不同，我们的算法使用上下文学习来对LLM进行条件设置，只需少量参考即可生成指令。通过LLM基础的视觉问答策略，我们收集了有关环境的详细信息，这些信息被LLM用于指令合成。我们在多个仿真平台上实现了我们的方法，包括Matterport3D、AI
    Habitat和ThreeDWorld，从而展示了其平台无关的特性。我们通过用户研究主观评估了我们的方法，并观察到$83.3\%$的SR变化，量化了生成的指令替代人工标注数据的可行性。最后，我们讨论了我们的方法在实现具身导航策略可泛化评估方面的适用性。据我们所知，我们的方法是第一种能够以平台无关的方式生成“类人”指令的LLM驱动方法，无需训练。
- en: Can LLMs Generate Human-Like Wayfinding Instructions?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLM能生成类人的路径指引吗？
- en: Towards Platform-Agnostic Embodied Instruction Synthesis
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 面向平台无关的具身指令合成
- en: Vishnu Sashank Dorbala  and Sanjoy Chowdhury  and Dinesh Manocha University
    of Maryland, College Park
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Vishnu Sashank Dorbala 和 Sanjoy Chowdhury 和 Dinesh Manocha 马里兰大学，帕克分校
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/828bba82fe61521561dd7e30a809d697.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/828bba82fe61521561dd7e30a809d697.png)'
- en: 'Figure 1: Overview: We use in-context learning with an LLM to generate multiple
    styles of wayfinding instructions for embodied navigation. Given any environment,
    we first gather a set of egocentric images along a path (white arrows), and obtain
    spatial knowledge via Visual Question Answering. We then condition an LLM on different
    styles of instructional language (coarse as well as fine grained) via reference
    texts. The figure highlights wayfinding instructions for this environment generated
    without training on any datasets.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：概述：我们使用上下文学习与LLM生成多种风格的路径指引用于具身导航。给定任何环境，我们首先沿路径收集一组自我中心的图像（白色箭头），并通过视觉问答获取空间知识。然后，我们通过参考文本对LLM进行不同风格的指令语言（粗略及细化）的条件设置。该图展示了在没有对任何数据集进行训练的情况下，为此环境生成的路径指引。
- en: In embodied navigation tasks, language is primarily used to convey wayfinding
    instructions to an agent operating in a simulation platform. These instructions
    convey the path that the agent should take to reach a target location. Generating
    these instructions usually takes place in the form of creating datasets that require
    several human annotation hours Qi et al. ([2020a](#bib.bib32)); Anderson et al.
    ([2018a](#bib.bib1)); Padmakumar et al. ([2022](#bib.bib29)). In addition, the
    current datasets are exclusive to the embodied simulation platform in which the
    agent operates, preventing the transfer of instruction-following approaches across
    platforms. For instance, an embodied agent trained to follow instructions present
    in the R2R Anderson et al. ([2018a](#bib.bib1)) or REVERIE Qi et al. ([2020a](#bib.bib32))
    datasets is limited to scenarios (object arrangements and scene layouts) in the
    Matterport3D  Chang et al. ([2017](#bib.bib4)); Ramakrishnan et al. ([2021](#bib.bib36))
    environment, the most commonly used platform for indoor datasets Gu et al. ([2022](#bib.bib10)).
    The scenarios themselves are also limited (around $90$ real-world scans). If its
    performance needs to be evaluated on another simulation environment such as TDW
    Gan et al. ([2020](#bib.bib9)) or ProcTHOR Deitke et al. ([2022](#bib.bib6)),
    the corresponding REVERIE or R2R-style instructions simply do not exist, posing
    a major hurdle for researchers conducting generalizability experiments to assess
    the adaptability of their navigation models. As such, to alleviate these issues,
    it is important to design an approach for synthesizing wayfinding instructions
    that are platform-agnostic, and is not cumbersome to generate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在具身导航任务中，语言主要用于向在仿真平台上操作的代理传达路径寻找指令。这些指令传达了代理应采取的路径，以到达目标位置。生成这些指令通常以创建需要若干人工注释小时的数据集的形式进行
    Qi 等 ([2020a](#bib.bib32))；Anderson 等 ([2018a](#bib.bib1))；Padmakumar 等 ([2022](#bib.bib29))。此外，当前的数据集仅限于代理操作的具身仿真平台，这限制了指令跟随方法在不同平台之间的迁移。例如，训练以遵循
    R2R 数据集 Anderson 等 ([2018a](#bib.bib1)) 或 REVERIE 数据集 Qi 等 ([2020a](#bib.bib32))
    中指令的具身代理仅限于 Matterport3D Chang 等 ([2017](#bib.bib4))；Ramakrishnan 等 ([2021](#bib.bib36))
    环境，这也是最常用的室内数据集平台 Gu 等 ([2022](#bib.bib10))。这些场景本身也有限（大约 $90$ 个现实世界扫描）。如果其性能需要在另一个仿真环境中进行评估，如
    TDW Gan 等 ([2020](#bib.bib9)) 或 ProcTHOR Deitke 等 ([2022](#bib.bib6))，对应的 REVERIE
    或 R2R 风格的指令根本不存在，这对研究人员进行泛化实验以评估其导航模型的适应性构成了重大障碍。因此，为了缓解这些问题，设计一种平台无关且生成过程不繁琐的路径寻找指令合成方法非常重要。
- en: Some recent works have looked at synthesizing instructions from input visual
    landmarks Wang et al. ([2022b](#bib.bib42)); Kurita and Cho ([2020](#bib.bib20));
    Tan et al. ([2019](#bib.bib39)). These approaches however are not easily generalizable
    and require training a separate model for each instruction dataset to infer synthetic
    instructions. Moreover, they only focus on the Matterport3D environment, as indoor
    instruction datasets are scarce on other platforms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最近的研究探讨了从输入视觉标志中合成指令 Wang 等 ([2022b](#bib.bib42))；Kurita 和 Cho ([2020](#bib.bib20))；Tan
    等 ([2019](#bib.bib39))。然而，这些方法不容易推广，并且需要为每个指令数据集训练一个单独的模型来推断合成指令。此外，它们仅关注 Matterport3D
    环境，因为其他平台上室内指令数据集稀缺。
- en: 'Main Results: We present a novel approach to synthesize wayfinding instructions
    for an embodied robot agent. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can
    LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied
    Instruction Synthesis") presents an overview of our approach. Given a set of egocentric
    images captured from a simulator, we perform Visual Question Answering to gather
    information about the scene, and use this to condition an LLM with reference texts
    to generate different styles of instructions. The novel components of our work
    include:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 主要结果：我们提出了一种新颖的方法来合成用于具身机器人代理的路径寻找指令。图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis") 展示了我们方法的概述。给定一组从模拟器中捕获的自我中心图像，我们进行视觉问答以收集关于场景的信息，并利用这些信息来调整大语言模型（LLM），结合参考文本生成不同风格的指令。我们工作的创新部分包括：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present a novel platform-agnostic, non-training based approach to synthesize
    wayfinding instructions of multiple styles.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的平台无关的、非训练基础的方法来合成多种风格的路径寻找指令。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We use the in-context learning capabilities of LLMs to perform instruction synthesis
    in a few-shot manner. Our method only requires a few samples of reference wayfinding
    text to produce human-like instructions in multiple simulation platforms.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们利用LLMs的上下文学习能力以少量样本的方式进行指令合成。我们的方法只需要少量的参考路径文本样本，即可在多个模拟平台上生成类人的指令。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We subjectively validate generated instructions across multiple simulation platforms
    via a user study and infer that $83.3\%$ of users find the instructions accurately
    capture details of the environment, and exhibit human-like characteristics.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过用户研究主观验证了在多个模拟平台上生成的指令，并推断出$83.3\%$的用户认为这些指令准确捕捉了环境细节，并具有类人特征。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Finally, we evaluate the effectiveness of our generated instructions on the
    REVERIE vision-and-language navigation (VLN) task. The performance of three zero-shot
    VLN approaches, evaluated using standard VLN success metrics, was comparable to
    established baselines, highlighting the efficacy and practical utility of LLM-generated
    instructions in navigation tasks.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们评估了在REVERIE视觉与语言导航（VLN）任务中生成指令的有效性。三种零样本VLN方法的性能使用标准VLN成功指标进行评估，与已建立的基线相当，突显了LLM生成指令在导航任务中的有效性和实际效用。
- en: In contrast to prior work which is limited to a single simulation platform and
    instruction style, we use in-context learning in LLMs to achieve instruction synthesis
    of multiple styles on different embodied simulation platforms, including Matterport3D,
    AI Habitat and ThreeDWorld. Our evaluation both via a user study and navigation
    performance indicates that the synthesized instructions are sufficiently representative
    of human-like texts for them to be used as a scalable alternative for generating
    instructions for embodied navigation tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的工作仅限于单一模拟平台和指令风格不同，我们利用LLMs的上下文学习来实现多种风格的指令合成，适用于不同的具身模拟平台，包括Matterport3D、AI
    Habitat和ThreeDWorld。我们通过用户研究和导航性能的评估表明，合成的指令足够代表人类文本，可作为生成具身导航任务指令的可扩展替代方案。
- en: 2 Approach
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: Our approach consists of two components. First, we perform Visual Question Answering
    (VQA) on egocentric images taken along an agent’s path in a simulation environment.
    This gives us spatial knowledge about the scene. Next, we combine this spatial
    knowledge with a few reference wayfinding instructions in an in-context learning
    Liu et al. ([2023b](#bib.bib25)) prompt to condition an LLM for synthesizing instructions
    that would lead the agent to the target location.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法包括两个组件。首先，我们在模拟环境中对代理路径上的自我中心图像进行视觉问答（VQA），从而获得关于场景的空间知识。接下来，我们将这些空间知识与少量参考路径指令结合，在上下文学习Liu等人（[2023b](#bib.bib25)）的提示中，以条件化LLM生成能引导代理到达目标位置的指令。
- en: '2.1 Extracting Spatial Knowledge: LLM + BLIP'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 提取空间知识：LLM + BLIP
- en: '![Refer to caption](img/396f144088537bdbd4d9fbce90e68602.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考字幕](img/396f144088537bdbd4d9fbce90e68602.png)'
- en: 'Figure 2: Extracting Spatial Knowledge: We use the GPT-3.5-turbo along with
    BLIP to maximize knowledge captured from an image, similar to ChatCaptioner Zhu
    et al. ([2023](#bib.bib50)). We notice that adding more detail to the captions
    helps improve the quality the final instruction by filtering out unnecessary information.
    More details about this are in Appendix [A](#A1 "Appendix A In-Context Learning
    Strategies ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：提取空间知识：我们使用GPT-3.5-turbo和BLIP来最大化从图像中捕获的知识，类似于ChatCaptioner Zhu等人（[2023](#bib.bib50)）。我们注意到，向字幕中添加更多细节有助于通过过滤掉不必要的信息来提高最终指令的质量。更多细节见附录[A](#A1
    "附录A 上下文学习策略 ‣ LLMs能否生成类人路径指令？面向平台无关的具身指令合成")。
- en: Paths in simulated environments describe a navigable route for an embodied agent
    to get from one point to another. In our approach, given any embodied simulator,
    we first generate random paths. We then obtain a discrete set of egocentric images
    $\mathcal{I}$ uniformly sampled on this path.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟环境中，路径描述了一个具身代理从一个点到另一个点的可导航路线。在我们的方法中，给定任何具身模拟器，我们首先生成随机路径。然后，我们获取在此路径上均匀采样的一组离散的自我中心图像$\mathcal{I}$。
- en: 'We then perform VQA on the images in $\mathcal{I}$, to gather information about
    the environmental artifacts on the path. Following a similar approach presented
    in ChatCaptioner Zhu et al. ([2023](#bib.bib50)), we maximize the knowledge obtained
    from each image by gathering insights via a conversation in a Chain of Thought
    manner Wei et al. ([2022](#bib.bib44)) between GPT-3.5 OpenAI ([2020](#bib.bib28))
    and BLIP Li et al. ([2023](#bib.bib22)) (Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Extracting
    Spatial Knowledge: LLM + BLIP ‣ 2 Approach ‣ Can LLMs Generate Human-Like Wayfinding
    Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis")). We
    notice that this gives us more detailed descriptions of each image, improving
    the quality of the generated instruction.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对$\mathcal{I}$中的图像进行视觉问答（VQA），以收集关于路径上环境因素的信息。遵循ChatCaptioner Zhu等人（[2023](#bib.bib50)）提出的类似方法，我们通过GPT-3.5
    OpenAI（[2020](#bib.bib28)）与BLIP Li等人（[2023](#bib.bib22)）之间的连贯思维对话（Chain of Thought）方式来最大化从每张图像中获取的知识（图
    [2](#S2.F2 "图 2 ‣ 2.1 提取空间知识：LLM + BLIP ‣ 2 方法 ‣ LLM 能生成类似人类的导航指令吗？面向平台无关的具身指令合成")）。我们注意到，这为每张图像提供了更详细的描述，提升了生成指令的质量。
- en: 2.2 Synthesizing Wayfinding Instructions via In-Context Learning
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 通过上下文学习合成导航指令
- en: '![Refer to caption](img/f7128d5bd32622399a829c7223045f96.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f7128d5bd32622399a829c7223045f96.png)'
- en: 'Figure 3: Given any embodied simulator, we synthesize multiple styles of wayfinding
    instructions for agents. Spatial knowledge is first mined from egocentric images
    $\mathcal{I}$ captured using the LLM and BLIP. These captions are fed into a prompt
    along with a few reference examples representing the desired instruction style.
    Finally, the LLM is conditioned with this prompt to generate a human-like instruction
    in the style of the reference text, using the captioned information.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：给定任何具身模拟器，我们为代理合成多种风格的导航指令。空间知识首先从使用LLM和BLIP捕捉的自我中心图像$\mathcal{I}$中提取。这些标题与几个代表所需指令风格的参考示例一起输入提示。最后，LLM使用这些提示生成类似人类的指令，风格与参考文本相符，利用已标注的信息。
- en: We condition GPT-3.5-turbo-instruct to generate suitable wayfinding instructions
    for navigation. Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Synthesizing Wayfinding Instructions
    via In-Context Learning ‣ 2 Approach ‣ Can LLMs Generate Human-Like Wayfinding
    Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis") illustrates
    this approach. Captions obtained for images in $\mathcal{I}$ along with reference
    texts providing context on the desired instruction style are used to create a
    prompt for the LLM. We experiment with reference instructions taken from two datasets
    with contrasting styles; R2R Anderson et al. ([2018a](#bib.bib1)), which has more
    detailed, fine-grained human annotations, and REVERIE Qi et al. ([2020a](#bib.bib32)),
    which has instructions that are abstract and coarse-grained.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将GPT-3.5-turbo-instruct调整为生成适合导航的指令。图 [3](#S2.F3 "图 3 ‣ 2.2 通过上下文学习合成导航指令
    ‣ 2 方法 ‣ LLM 能生成类似人类的导航指令吗？面向平台无关的具身指令合成") 展示了这种方法。我们使用$\mathcal{I}$中图像获得的标题以及提供所需指令风格上下文的参考文本来创建LLM的提示。我们尝试了来自两个风格对比的数据集的参考指令；R2R
    Anderson等人（[2018a](#bib.bib1)），其具有更详细、更精细的人类注释，和REVERIE Qi等人（[2020a](#bib.bib32)），其指令抽象且粗略。
- en: We also observe that adding more information about the instruction style itself
    helps further finetune the outcome. For instance, in the REVERIE dataset Qi et al.
    ([2020a](#bib.bib32)), almost all instructions end by describing a task with the
    target object (‘turn the faucet’ for example). Adding this information as an additional
    constraint helps further finetune the LLM output. More details about this are
    provided in appendix [A](#A1 "Appendix A In-Context Learning Strategies ‣ Can
    LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied
    Instruction Synthesis").
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还观察到，增加关于指令风格本身的更多信息有助于进一步优化结果。例如，在REVERIE数据集 Qi等人（[2020a](#bib.bib32)）中，几乎所有指令都以描述目标对象的任务结束（例如“转动水龙头”）。将这些信息作为额外约束有助于进一步优化LLM输出。更多详细信息请参见附录
    [A](#A1 "附录 A 上下文学习策略 ‣ LLM 能生成类似人类的导航指令吗？面向平台无关的具身指令合成")。
- en: 3 Evaluation & Results
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 评估与结果
- en: '| Approach | Original | Generated (Central) | Generated (Panoramic) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 原始 | 生成（中央） | 生成（全景） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SR $\uparrow$ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SR $\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Clip-Nav | 6.57 | 28.68 | 0.06 | 5.98 | 26.69 | 0.05 | 5.57 | 26.09 | 0.05
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Clip-Nav | 6.57 | 28.68 | 0.06 | 5.98 | 26.69 | 0.05 | 5.57 | 26.09 | 0.05
    |'
- en: '| Seq-CLIPNav | 14.92 | 24.46 | 0.15 | 13.94 | 21.51 | 0.14 | 11.35 | 23.10
    | 0.13 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Seq-CLIPNav | 14.92 | 24.46 | 0.15 | 13.94 | 21.51 | 0.14 | 11.35 | 23.10
    | 0.13 |'
- en: '| GLIP-Nav | 16.87 | 32.56 | 0.18 | 16.32 | 33.23 | 0.18 | 14.18 | 29.87 |
    0.15 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| GLIP-Nav | 16.87 | 32.56 | 0.18 | 16.32 | 33.23 | 0.18 | 14.18 | 29.87 |
    0.15 |'
- en: 'Results: We evaluate zero-shot VLN models by replacing REVERIE’s human-annotated
    instructions with instructions generated by our approach. Notice the similar performance
    on each VLN model across all metrics. There is a noticeable drop in using panoramic
    frames over central frames, and this could be attributed to condensing copious
    amounts of scene information into a single sentence (See Appendix [B.3.2](#A2.SS3.SSS2
    "B.3.2 Matterport3D: Frame Selection ‣ B.3 Quantitative Study - Zero-Shot Embodied
    Navigation ‣ Appendix B Evaluation Details ‣ Can LLMs Generate Human-Like Wayfinding
    Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis")). We
    can positively infer from the minimal difference in SR, OSR, and SPL values that
    our approach can generate instructions that can indeed serve as a good replacement
    to human-annotated data.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '结果：我们通过将REVERIE的人工标注指令替换为我们方法生成的指令来评估零样本VLN模型。请注意所有指标中每个VLN模型的类似性能。在使用全景帧而非中央帧时出现明显下降，这可能归因于将大量场景信息浓缩成一句话（见附录[B.3.2](#A2.SS3.SSS2
    "B.3.2 Matterport3D: Frame Selection ‣ B.3 Quantitative Study - Zero-Shot Embodied
    Navigation ‣ Appendix B Evaluation Details ‣ Can LLMs Generate Human-Like Wayfinding
    Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis")）。从SR、OSR和SPL值的最小差异中，我们可以积极推断，我们的方法确实可以生成能够很好替代人工标注数据的指令。'
- en: In this section, we discuss our evaluation strategy and present results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论我们的评估策略并展示结果。
- en: '3.1 Qualitative: User Study'
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 定性：用户研究
- en: We conduct a user study to evaluate the quality of the generated instructions.
    Participants are first shown a video of a random path taken from one of 3 different
    simulators (Matterport3D, AI Habitat, ThreeDWorld). Using an instruction of either
    a REVERIE or R2R style as reference they are asked to come up with a stylistically
    similar instruction for the video. We then show them the generated instruction,
    and ask them a few questions about correlation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项用户研究以评估生成指令的质量。参与者首先观看一段来自3种不同模拟器（Matterport3D、AI Habitat、ThreeDWorld）之一的随机路径视频。以REVERIE或R2R风格的指令作为参考，他们被要求为该视频编写一份风格类似的指令。然后，我们展示生成的指令，并向他们提出一些关于相关性的问题。
- en: We infer that $83.3\%$ of participants believed the instructions were different
    from what they wrote. This indicates that the vocabulary people use to describe
    a path may significantly vary from the vocabulary used in the generated instruction.
    This however is not an indicator of instruction quality, as the difference is
    in alternate landmarks being used guide the agent along the same path. This is
    further highlighted in the navigation results presented below. More details are
    in Appendix [B.2](#A2.SS2 "B.2 Qualitative Analysis - User Study Details ‣ Appendix
    B Evaluation Details ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
    Platform-Agnostic Embodied Instruction Synthesis").
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推断出$83.3\%$的参与者认为这些指令与他们编写的不同。这表明，人们用来描述路径的词汇可能与生成指令中使用的词汇大相径庭。然而，这并不表示指令的质量，因为差异在于使用不同的地标引导代理沿相同路径。这在下面呈现的导航结果中进一步突出。更多细节见附录[B.2](#A2.SS2
    "B.2 Qualitative Analysis - User Study Details ‣ Appendix B Evaluation Details
    ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis")。
- en: '3.2 Quantitative: Embodied Navigation'
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 定量：体现导航
- en: Our evaluation setup is simple. We first implement a zero-shot navigation scheme
    using the original instructions provided in REVERIE, a popular VLN dataset. We
    then replace the original instructions with instructions generated by our approach,
    and run the navigation scheme again. A similar performance would indicate that
    the generated instructions can indeed serve as a replacement to human-annotated
    data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估设置很简单。我们首先使用REVERIE（一种流行的VLN数据集）提供的原始指令实现一个零样本导航方案。然后，我们将原始指令替换为我们方法生成的指令，再次运行导航方案。类似的性能表明生成的指令确实可以替代人工标注的数据。
- en: REVERIE is based on the Matterport3D simulator, which contains real-world captures
    of household environments. We look at 3 zero-shot VLN approaches - 1) CLIP-Nav
    Dorbala et al. ([2022](#bib.bib8)), which uses CLIP Radford et al. ([2021](#bib.bib35))
    to ground target instructions to a scene to drive the agent’s navigation policy,
    2) Seq-CLIP-Nav, an extension of this approach that also performs backtracking
    (see Appendix [B.3](#A2.SS3 "B.3 Quantitative Study - Zero-Shot Embodied Navigation
    ‣ Appendix B Evaluation Details ‣ Can LLMs Generate Human-Like Wayfinding Instructions?
    Towards Platform-Agnostic Embodied Instruction Synthesis")), and 3) GLIP-Nav,
    which we introduce as a GLIP Li* et al. ([2022](#bib.bib23)) based variant of
    Seq-CLIP-Nav. More details about these approaches are in Appendix [B.3](#A2.SS3
    "B.3 Quantitative Study - Zero-Shot Embodied Navigation ‣ Appendix B Evaluation
    Details ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis").
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: REVERIE 基于 Matterport3D 模拟器，该模拟器包含了现实世界的家庭环境捕捉。我们考察了三种零样本 VLN 方法 - 1) CLIP-Nav
    Dorbala 等人 ([2022](#bib.bib8))，使用 CLIP Radford 等人 ([2021](#bib.bib35)) 将目标指令与场景对接，以驱动代理的导航策略；2)
    Seq-CLIP-Nav，该方法的扩展版本，还进行回溯（见附录 [B.3](#A2.SS3 "B.3 Quantitative Study - Zero-Shot
    Embodied Navigation ‣ Appendix B Evaluation Details ‣ Can LLMs Generate Human-Like
    Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis")）；3)
    GLIP-Nav，我们介绍的基于 GLIP Li* 等人 ([2022](#bib.bib23)) 的 Seq-CLIP-Nav 变体。这些方法的更多细节见附录
    [B.3](#A2.SS3 "B.3 Quantitative Study - Zero-Shot Embodied Navigation ‣ Appendix
    B Evaluation Details ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
    Platform-Agnostic Embodied Instruction Synthesis")。
- en: 'As Matterport3D provides panoramic images, we consider two possibilities for
    extracting spatial knowledge (see Appendix [B.3.2](#A2.SS3.SSS2 "B.3.2 Matterport3D:
    Frame Selection ‣ B.3 Quantitative Study - Zero-Shot Embodied Navigation ‣ Appendix
    B Evaluation Details ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
    Platform-Agnostic Embodied Instruction Synthesis")); The Central Caption, where
    only the images in the direction of the agent’s heading are captioned, and the
    Panoramic Caption, where the entire panorama ($4$ images) is captioned and summarized
    to obtain an instruction.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 Matterport3D 提供了全景图像，我们考虑了两种提取空间知识的可能性（见附录 [B.3.2](#A2.SS3.SSS2 "B.3.2 Matterport3D:
    Frame Selection ‣ B.3 Quantitative Study - Zero-Shot Embodied Navigation ‣ Appendix
    B Evaluation Details ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
    Platform-Agnostic Embodied Instruction Synthesis")）；中央标题，其中仅对代理视线方向上的图像进行标注；全景标题，其中对整个全景（$4$
    张图像）进行标注和总结，以获得指令。'
- en: 'Experiment Details: We employ $3$ standard VLN evaluation metrics Zhao et al.
    ([2021](#bib.bib47)) to measure performance across each navigation approach -
    1) SR, which is the Success Rate determining when the agent has successfully reached
    the target location; 2) OSR, the Oracle Success Rate, for when the agent successfully
    reached the target location once, but overshot and stopped elsewhere, and 3) SPL,
    which measures efficiency of Success weighted by Path Length. The results table
    compares the performance of the generated instructions with the original ones
    on the zero-shot VLN approaches.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 实验细节：我们采用 $3$ 种标准 VLN 评估指标 Zhao 等人 ([2021](#bib.bib47)) 来衡量每种导航方法的表现 - 1) SR，即成功率，决定代理何时成功到达目标位置；2)
    OSR，即预期成功率，当代理成功到达目标位置一次，但超出并停在其他地方；3) SPL，即成功的效率按路径长度加权。结果表比较了生成的指令与原始指令在零样本
    VLN 方法中的表现。
- en: We make the following key inferences -
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做出了以下关键推断 -
- en: 'Automated Instruction Generation: A key observation is that embodied agents
    equipped with LLM-generated instructions perform almost equally well compared
    to when they are provided with human annotated instruction. This has practical
    implications for researchers working on embodied navigation, where such instruction
    data is limited and hard to annotate. Creating large-scale instruction datasets
    is challenging, often needing simulator-specific annotation tools, which cannot
    be easily transferred. To this end, our study presents a good alternative in leveraging
    off-the-shelf LLMs as a wayfinding instruction generation tool.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化指令生成：一个关键观察是，配备 LLM 生成的指令的具身代理，其表现几乎与提供人工标注指令时相同。这对从事具身导航的研究人员具有实际意义，因为此类指令数据有限且难以标注。创建大规模指令数据集具有挑战性，通常需要特定于模拟器的标注工具，这些工具无法轻易转移。因此，我们的研究提供了一个利用现成
    LLM 作为指令生成工具的良好替代方案。
- en: 'Central vs. Panoramic Captions in MP3D: We observe that the performance of
    the central caption approach is generally higher than that of the panoramic caption
    approach. We believe this to be due to instruction quality being affected by two
    reasons *—* 1) Captioning each image of the panorama and summarizing it leads
    to excess information at each step and 2) The central caption approach implicitly
    contains the information in the heading of the target, leading to more direct
    instructions.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MP3D中的中央与全景标注：我们观察到中央标注方法的性能通常高于全景标注方法。我们认为这是由于指令质量受两个原因的影响 *—* 1）对全景中的每个图像进行标注并进行总结会导致每一步的信息过多，2）中央标注方法隐含了目标的标题信息，导致指令更为直接。
- en: 'Cross-Platform Scalability: Our approach is platform-agnostic, and can be applied
    to generate instructions across embodied simulation platforms, whether they are
    discrete, continuous, photorealistic, or not. The user study validates this, where
    users across simulator types believed that the generated instructions captured
    details of the environment and could lead the agent to the target location. We
    believe that the embodied navigation community can significantly benefit from
    this, enabling researchers to conduct cross-platform generalizability experiments
    without relying on the availability of platform-specific human-annotated data.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 跨平台可扩展性：我们的方法与平台无关，可以应用于生成跨具身模拟平台的指令，无论这些平台是离散的、连续的、真实感的，还是其他类型的。用户研究验证了这一点，用户们认为生成的指令能够捕捉到环境的细节，并能够引导代理到达目标位置。我们相信，具身导航社区可以从中获得显著的好处，使研究人员能够在不依赖于特定平台人工标注数据的情况下进行跨平台的普遍性实验。
- en: 'Improved Instruction Quality: We notice that human-annotated instructions in
    REVERIE sometimes tend to be unnatural and lacking in terms of sentence construction.
    As these annotations are crowdsourced, this can be attributed to human error.
    It is often in these cases that the embodied agent fails to reach it’s target
    location, due to poor annotation leading to inferior grounding scores. LLM-generated
    instructions on the other hand are almost always well structured, containing specific
    objects and waypoints leading up to a target location; a direct consequence of
    our prompting strategy. Some of these cases are discussed in appendix [B.3.3](#A2.SS3.SSS3
    "B.3.3 Inferences on Generated Instructions ‣ B.3 Quantitative Study - Zero-Shot
    Embodied Navigation ‣ Appendix B Evaluation Details ‣ Can LLMs Generate Human-Like
    Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的指令质量：我们注意到，REVERIE中的人工标注指令有时可能显得不自然，句子结构也欠缺。由于这些标注是众包的，这可以归因于人为错误。正是在这些情况下，具身代理往往无法到达目标位置，因为差的标注导致较差的基础评分。另一方面，LLM生成的指令几乎总是结构良好，包含特定的物体和途经点，直达目标位置；这是我们提示策略的直接结果。这些情况的一些讨论见附录
    [B.3.3](#A2.SS3.SSS3 "B.3.3 对生成指令的推断 ‣ B.3 定量研究 - 零样本具身导航 ‣ 附录 B 评估细节 ‣ LLM能否生成类似人类的路线指示？面向平台无关的具身指令合成")。
- en: '4 Discussion: Evaluating Generalizability of Embodied Navigation Policies'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论：评估具身导航策略的普遍性
- en: The overarching motive of our work is to construct a generalist navigation agent
    that performs consistently irrespective of the environment that it is present
    in. Current approaches to solve this task are limited to evaluation on human-annotated
    datasets created specifically for a particular simulator, be it MP3D Chang et al.
    ([2017](#bib.bib4)), AI Habitat Ramakrishnan et al. ([2021](#bib.bib36)), RoboThor
    Kolve et al. ([2017](#bib.bib17)) etc.. While some methods claim generalizability
    Park and Kim ([2023](#bib.bib30)), they back their claims by showing improved
    performance on unseen subsets of a dataset on the same simulator, rather than
    measuring performance across simulators. For a true measure of generalizability,
    we believe it is necessary to measure the navigation performance of agents that
    aren’t bounded to a particular dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要动机是构建一个通用的导航代理，无论环境如何，它都能保持一致的表现。目前解决这一任务的方法限于对专门为特定模拟器创建的人类标注数据集进行评估，如MP3D
    Chang等（[2017](#bib.bib4)）、AI Habitat Ramakrishnan等（[2021](#bib.bib36)）、RoboThor
    Kolve等（[2017](#bib.bib17)）等。虽然一些方法声称具有普遍性 Park和Kim（[2023](#bib.bib30)），但他们通过在同一模拟器上的数据集未见子集上的性能改进来支持他们的主张，而不是测量跨模拟器的性能。我们认为，为了真正衡量普遍性，有必要测量不局限于特定数据集的代理的导航性能。
- en: In this direction, our approach solves a crucial data procurement problem in
    providing a simple method to generate human-like instructions across simulation
    platforms. In doing so, we empower resource-constrained researchers to create
    their own datasets for generalizable experiments on their navigation models; therein
    presenting the true novelty of our work.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，我们的方法解决了一个关键的数据获取问题，提供了一种简单的方法来生成跨模拟平台的人类指令。通过这样做，我们使资源有限的研究人员能够创建自己的数据集，用于对他们的导航模型进行通用性实验，这体现了我们工作的真正创新。
- en: Current datasets cover a wide range of language-guided navigation scenarios,
    ranging from initial-instruction based guidance (fine and coarse-grained) to oracle
    and dialogue based navigation that provide verbal human assistance Gu et al. ([2022](#bib.bib10)).
    There also exist several outdoor datasets including Touchdown Chen et al. ([2019](#bib.bib5)),
    Talk2Nav Vasudevan et al. ([2021](#bib.bib40)) and StreetNav Jain et al. ([2023](#bib.bib15)),
    where the beyond the instruction, the structure and semantics of the scene are
    drastically different from indoors. To account for the diversity and measure true
    generalizability, we propose integrating our scheme for synthesis to measure the
    robustness of navigation policies in two ways as follows:-
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的数据集涵盖了广泛的语言引导导航场景，包括基于初始指令的指导（细粒度和粗粒度）以及提供口头人类协助的oracle和对话式导航Gu等人（[2022](#bib.bib10)）。还存在几个户外数据集，如Touchdown
    Chen等人（[2019](#bib.bib5)）、Talk2Nav Vasudevan等人（[2021](#bib.bib40)）和StreetNav Jain等人（[2023](#bib.bib15)），这些数据集的场景结构和语义与室内环境有很大不同。为了考虑多样性并衡量真正的通用性，我们提出了将我们的合成方案集成到两种方式来测量导航策略的鲁棒性：
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cross-Platform Generalizability: In the first experiment, we gather a set of
    instruction-path pairs across simulators to train a cross-platform model for a
    generalist navigation agent. Consistent performance on each simulator present
    in the dataset during inference would indicate that the navigation policy is globally
    robust with low bias towards a specific simulator.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨平台的通用性：在第一个实验中，我们收集了一组跨模拟器的指令路径对，以训练一个通用导航代理的跨平台模型。在推理过程中，数据集中每个模拟器上的一致表现表明导航策略在全球范围内是鲁棒的，对特定模拟器有较低的偏见。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intra-Platform Generalizability: In the second experiment, we measure the agent’s
    performance within different generated datasets on the same simulator. Unlike
    data augmentation approaches in the past Li et al. ([2022](#bib.bib21)) that seek
    to improve the agent’s performance with generated instruction-path data, our objective
    is measure consistency in performance across multiple instruction-path “datasets”
    generated in the same environment. This consistency would indicate that the navigation
    policy is locally robust, with low bias towards a specific type of scene or region
    within the simulator.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平台内的通用性：在第二个实验中，我们测量了代理在同一模拟器上不同生成的数据集中的表现。不同于过去Li等人（[2022](#bib.bib21)）通过生成指令路径数据来提高代理表现的数据增强方法，我们的目标是测量在同一环境中生成的多个指令路径“数据集”之间的表现一致性。这种一致性表明导航策略在局部是鲁棒的，对模拟器中的特定场景或区域有较低的偏见。
- en: A generalist navigation agent would have a policy that is both globally and
    locally robust. Our approach paves the way to measure this robustness for a fair
    evaluation of state-of-the-art embodied navigation policies.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的导航代理需要具备全球和局部的鲁棒性。我们的方法为衡量这种鲁棒性提供了途径，从而对最先进的具身导航策略进行公平评估。
- en: 5 Conclusion
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present a simple, cross-platform approach to synthesize multiple styles of
    wayfinding instructions for embodied navigation. Our approach requires no training
    and instead utilizes an LLM with in-context learning to produce instructions across
    multiple simulation platforms. We verify the quality of the instructions generated
    both via a user study and by evaluating zero-shot VLN performance. From these
    evaluations, we positively infer that our LLM-generated instructions are a good
    replacement to human-annotated ones, and further, that our approach provides for
    a scalable and accessible solution for creating wayfinding instructions. We finally
    touch upon how our approach can be used for measuring the key quality of robustness
    while evaluating language-guided navigation policies; a defining metric to evaluate
    a generalist navigation agent.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种简单的跨平台方法，用于合成多种风格的导航指令。我们的方法无需训练，而是利用具有上下文学习能力的 LLM 来生成多个模拟平台上的指令。我们通过用户研究和零样本
    VLN 性能评估验证了生成指令的质量。从这些评估中，我们积极推断我们的 LLM 生成的指令可以很好地替代人工标注的指令，并且我们的方法为创建导航指令提供了一个可扩展且易于访问的解决方案。最后，我们提到我们的方法如何用于评估语言指导导航策略的关键质量，即评估通用导航代理的定义指标。
- en: 6 Limitations and Future Work
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制与未来工作
- en: While our approach is platform-agnostic, the quality of the generated instructions
    is very sensitive to the individual modules that drive our scheme. Poor spatial
    knowledge extracted from performing VQA would directly affect the quality of the
    caption. In some preliminary experiments, we notice this behavior on some images
    taken from the VirtualHome Puig et al. ([2018](#bib.bib31)) embodied simulator,
    which has non-photorealistic environments. Using LLaVA Liu et al. ([2023a](#bib.bib24))
    for VQA seems to create ghost objects and artifacts when asked to describe a scene
    leading to poor instructions. In contrast, it performs well with real world images
    taken from Matterport3D. We believe this poor performance might be because large
    captioning models such as LLaVA are trained on an abundance of real world data,
    and may contain fewer if not any simulation or non-photorealistic images. Secondly,
    during the synthesis stage, we present the LLM with examples from the instruction
    style that we wish to obtain. The generated instructions can sometimes contain
    the direct words or language used in these reference examples. As such, we believe
    it is necessary to explicitly specify in the prompt that the LLM uses only the
    captions and not the reference texts for generation. In the future, we intend
    to use our approach to implement a generalist navigation agent and study its performance
    in terms of consistency across various embodied simulation platforms.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的方法与平台无关，但生成的指令质量对驱动我们方案的各个模块非常敏感。从 VQA 中提取的空间知识较差会直接影响说明的质量。在一些初步实验中，我们注意到这种行为出现在一些从
    VirtualHome Puig 等（[2018](#bib.bib31)）体现模拟器中拍摄的图像上，该模拟器具有非照片级真实感的环境。使用 LLaVA Liu
    等（[2023a](#bib.bib24)）进行 VQA 时，当要求描述场景时，似乎会产生虚幻对象和伪影，从而导致指令质量差。相比之下，它在使用 Matterport3D
    拍摄的真实世界图像时表现良好。我们认为，这种表现不佳可能是因为像 LLaVA 这样的巨大标注模型是基于大量真实世界数据训练的，可能包含较少或没有模拟或非照片级真实感的图像。其次，在合成阶段，我们向
    LLM 提供了我们希望获得的指令风格的示例。生成的指令有时可能包含这些参考示例中直接使用的词汇或语言。因此，我们认为有必要在提示中明确指定 LLM 仅使用标题而不是参考文本来生成内容。未来，我们打算利用我们的方法实现一个通用导航代理，并研究其在不同体现模拟平台上的一致性表现。
- en: 7 Ethics Statement
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 伦理声明
- en: Equipping embodied agent with LLM-generated instructions to perform navigational
    tasks is a step towards cohesive human-robot collaboration. While the end goal
    is to make such systems fault-tolerant and error-free, we may not want an agent
    to perform certain actions that it is unsure of. However, currently there seems
    to be a gap in the language interpretation capabilities of the agent especially
    in complex scenarios.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为体现代理配备 LLM 生成的指令以执行导航任务是迈向紧密的人机协作的一步。虽然最终目标是使这些系统具有容错性和无错误，但我们可能不希望代理执行其不确定的某些动作。然而，目前似乎存在代理在复杂场景中的语言解释能力差距。
- en: Our user study protocol was approved by Institutional Review Board and we do
    not collect, share or store any personal information of the participants.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用户研究协议已获得机构审查委员会的批准，我们不收集、分享或存储参与者的任何个人信息。
- en: 8 Acknowledgments
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 致谢
- en: This work was supported in part by ARO Grants W911NF2110026, W911NF2310046,
    W911NF2310352 and Army Cooperative Agreement W911NF2120076\. We would also like
    thank Niall L. Williams for his creative insights.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分由 ARO 资助（W911NF2110026, W911NF2310046, W911NF2310352）和陆军合作协议（W911NF2120076）支持。我们还要感谢
    Niall L. Williams 的创意见解。
- en: References
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Anderson et al. (2018a) Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
    Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. 2018a.
    Vision-and-language navigation: Interpreting visually-grounded navigation instructions
    in real environments. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, pages 3674–3683.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等人（2018a）Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson,
    Niko Sünderhauf, Ian Reid, Stephen Gould 和 Anton Van Den Hengel. 2018a. 视觉和语言导航：在实际环境中解释视觉基础的导航指令。发表于
    *IEEE 计算机视觉与模式识别会议论文集*，第 3674–3683 页。
- en: 'Anderson et al. (2018b) Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
    Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. 2018b.
    Vision-and-language navigation: Interpreting visually-grounded navigation instructions
    in real environments. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, pages 3674–3683.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等人（2018b）Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson,
    Niko Sünderhauf, Ian Reid, Stephen Gould 和 Anton Van Den Hengel. 2018b. 视觉和语言导航：在实际环境中解释视觉基础的导航指令。发表于
    *IEEE 计算机视觉与模式识别会议论文集*，第 3674–3683 页。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等. 2020. 语言模型是少-shot 学习者。*神经信息处理系统进展*，33:1877–1901。
- en: 'Chang et al. (2017) Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber,
    Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. 2017.
    Matterport3d: Learning from rgb-d data in indoor environments. *arXiv preprint
    arXiv:1709.06158*.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人（2017）Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias
    Niessner, Manolis Savva, Shuran Song, Andy Zeng 和 Yinda Zhang. 2017. Matterport3d：从室内环境中的
    RGB-D 数据中学习。*arXiv 预印本 arXiv:1709.06158*。
- en: 'Chen et al. (2019) Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and
    Yoav Artzi. 2019. Touchdown: Natural language navigation and spatial reasoning
    in visual street environments. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 12538–12547.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2019）Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely 和 Yoav Artzi.
    2019. Touchdown：视觉街道环境中的自然语言导航和空间推理。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 12538–12547
    页。
- en: 'Deitke et al. (2022) Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,
    Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and
    Roozbeh Mottaghi. 2022. Procthor: Large-scale embodied ai using procedural generation.
    *Advances in Neural Information Processing Systems*, 35:5982–5994.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deitke 等人（2022）Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana
    Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi 和 Roozbeh Mottaghi.
    2022. Procthor：使用程序生成的大规模具身 AI。*神经信息处理系统进展*，35:5982–5994。
- en: Dorbala et al. (2023) Vishnu Sashank Dorbala, James F Mullen Jr, and Dinesh
    Manocha. 2023. Can an embodied agent find your" cat-shaped mug"? llm-based zero-shot
    object navigation. *arXiv preprint arXiv:2303.03480*.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorbala 等人（2023）Vishnu Sashank Dorbala, James F Mullen Jr 和 Dinesh Manocha.
    2023. 具身代理能找到你的“猫形杯”吗？基于 LLM 的零-shot 物体导航。*arXiv 预印本 arXiv:2303.03480*。
- en: 'Dorbala et al. (2022) Vishnu Sashank Dorbala, Gunnar Sigurdsson, Robinson Piramuthu,
    Jesse Thomason, and Gaurav S Sukhatme. 2022. Clip-nav: Using clip for zero-shot
    vision-and-language navigation. *arXiv preprint arXiv:2211.16649*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorbala 等人（2022）Vishnu Sashank Dorbala, Gunnar Sigurdsson, Robinson Piramuthu,
    Jesse Thomason 和 Gaurav S Sukhatme. 2022. Clip-nav：使用 CLIP 进行零-shot 视觉和语言导航。*arXiv
    预印本 arXiv:2211.16649*。
- en: 'Gan et al. (2020) Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin
    Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar,
    Nick Haber, et al. 2020. Threedworld: A platform for interactive multi-modal physical
    simulation. *arXiv preprint arXiv:2007.04954*.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gan 等人（2020）Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf,
    James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber
    等. 2020. Threedworld：一个用于互动多模态物理仿真的平台。*arXiv 预印本 arXiv:2007.04954*。
- en: 'Gu et al. (2022) Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin Wang.
    2022. [Vision-and-language navigation: A survey of tasks, methods, and future
    directions](https://doi.org/10.18653/v1/2022.acl-long.524). In *Proceedings of
    the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*. Association for Computational Linguistics.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2022）晶·顾、艾莉安娜·斯特凡尼、齐·吴、杰西·托马森和辛·王。2022。[视觉和语言导航：任务、方法和未来方向的调查](https://doi.org/10.18653/v1/2022.acl-long.524)。在*第60届计算语言学协会年会论文集（第1卷：长篇论文）*。计算语言学协会。
- en: 'Guhur et al. (2021) Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan
    Laptev, and Cordelia Schmid. 2021. Airbert: In-domain pretraining for vision-and-language
    navigation. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, pages 1634–1643.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guhur 等（2021）皮埃尔-路易斯·古赫、马卡兰德·塔帕斯维、施哲·陈、伊万·拉普捷夫和科尔德利亚·施密德。2021。Airbert: 视觉和语言导航的领域内预训练。在*IEEE/CVF
    国际计算机视觉大会论文集*，第1634–1643页。'
- en: 'Hong et al. (2021) Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo,
    and Stephen Gould. 2021. Vln bert: A recurrent vision-and-language bert for navigation.
    In *Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition*,
    pages 1643–1653.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong 等（2021）宜聪·洪、齐·吴、元凯·齐、克里斯蒂安·罗德里格斯-奥帕佐和斯蒂芬·古尔德。2021。Vln bert: 一种用于导航的递归视觉和语言
    BERT。在*IEEE/CVF 计算机视觉与模式识别会议论文集*，第1643–1653页。'
- en: Huang et al. (2022a) Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard.
    2022a. Visual language maps for robot navigation. *arXiv preprint arXiv:2210.05714*.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2022a）程光·黄、奥伊尔·梅斯、安迪·曾和沃尔夫拉姆·布尔加德。2022a。用于机器人导航的视觉语言地图。*arXiv 预印本 arXiv:2210.05714*。
- en: 'Huang et al. (2022b) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    2022b. Inner monologue: Embodied reasoning through planning with language models.
    *arXiv preprint arXiv:2207.05608*.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2022b）文龙·黄、费·夏、泰德·肖、哈里斯·陈、杰基·梁、皮特·弗洛伦斯、安迪·曾、乔纳森·汤普森、伊戈尔·莫达奇、耶夫根·切博塔尔等。2022b。内心独白：通过语言模型进行具身推理规划。*arXiv
    预印本 arXiv:2207.05608*。
- en: 'Jain et al. (2023) Gaurav Jain, Basel Hindi, Zihao Zhang, Koushik Srinivasula,
    Mingyu Xie, Mahshid Ghasemi, Daniel Weiner, Sophie Ana Paris, Xin Yi Therese Xu,
    Michael Malcolm, et al. 2023. Streetnav: Leveraging street cameras to support
    precise outdoor navigation for blind pedestrians. *arXiv preprint arXiv:2310.00491*.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jain 等（2023）高拉夫·贾恩、巴塞尔·辛迪、子浩·张、库什克·斯里尼瓦苏拉、明瑜·谢、马赫希德·加塞米、丹尼尔·温纳、索非亚·安娜·巴黎、辛·伊·特蕾丝·徐、迈克尔·马尔科姆等。2023。Streetnav:
    利用街道摄像头支持盲人行人的精确户外导航。*arXiv 预印本 arXiv:2310.00491*。'
- en: 'Kamath et al. (2023) Aishwarya Kamath, Peter Anderson, Su Wang, Jing Yu Koh,
    Alexander Ku, Austin Waters, Yinfei Yang, Jason Baldridge, and Zarana Parekh.
    2023. A new path: Scaling vision-and-language navigation with synthetic instructions
    and imitation learning. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 10813–10823.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamath 等（2023）艾什瓦利亚·卡马斯、彼得·安德森、苏·王、晶·余·科赫、亚历山大·库、奥斯汀·沃特斯、尹菲·杨、杰森·巴尔德里奇和扎拉纳·帕雷克。2023。新路径：通过合成指令和模仿学习扩展视觉和语言导航。在*IEEE/CVF
    计算机视觉与模式识别会议论文集*，第10813–10823页。
- en: 'Kolve et al. (2017) Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,
    Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu,
    et al. 2017. Ai2-thor: An interactive 3d environment for visual ai. *arXiv preprint
    arXiv:1712.05474*.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kolve 等（2017）埃里克·科尔夫、鲁兹贝赫·莫塔吉、温森·汉、埃利·范德比尔特、卢卡·魏斯、阿尔瓦罗·赫拉斯提、马特·戴特克、基亚娜·艾赫萨尼、丹尼尔·戈登、俞克·朱等。2017。Ai2-thor:
    一个用于视觉 AI 的交互式 3D 环境。*arXiv 预印本 arXiv:1712.05474*。'
- en: 'Krantz et al. (2020) Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra,
    and Stefan Lee. 2020. Beyond the nav-graph: Vision-and-language navigation in
    continuous environments. In *Computer Vision–ECCV 2020: 16th European Conference,
    Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII 16*, pages 104–120\.
    Springer.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Krantz 等（2020）雅各布·克兰茨、埃里克·威曼斯、阿尔君·马朱姆达、德鲁夫·巴特拉和斯特凡·李。2020。超越导航图：连续环境中的视觉和语言导航。在*计算机视觉–ECCV
    2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议录，第28部分*，第104–120页。施普林格。'
- en: 'Ku et al. (2020) Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason
    Baldridge. 2020. Room-across-room: Multilingual vision-and-language navigation
    with dense spatiotemporal grounding. *arXiv preprint arXiv:2010.07954*.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ku 等（2020）亚历山大·库、彼得·安德森、罗马·帕特尔、尤金·伊伊和杰森·巴尔德里奇。2020。Room-across-room: 多语言视觉和语言导航与密集的时空基础。*arXiv
    预印本 arXiv:2010.07954*。'
- en: Kurita and Cho (2020) Shuhei Kurita and Kyunghyun Cho. 2020. Generative language-grounded
    policy in vision-and-language navigation with bayes’ rule. *arXiv preprint arXiv:2009.07783*.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurita和Cho（2020）Shuhei Kurita和Kyunghyun Cho。2020年。《Generative language-grounded
    policy in vision-and-language navigation with bayes’ rule》。*arXiv预印本 arXiv:2009.07783*。
- en: 'Li et al. (2022) Jialu Li, Hao Tan, and Mohit Bansal. 2022. Envedit: Environment
    editing for vision-and-language navigation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 15407–15417.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等（2022）Jialu Li、Hao Tan和Mohit Bansal。2022年。《Envedit: Environment editing
    for vision-and-language navigation》。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，第15407–15417页。'
- en: 'Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.
    Blip-2: Bootstrapping language-image pre-training with frozen image encoders and
    large language models. *arXiv preprint arXiv:2301.12597*.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等（2023）Junnan Li、Dongxu Li、Silvio Savarese和Steven Hoi。2023年。《Blip-2: Bootstrapping
    language-image pre-training with frozen image encoders and large language models》。*arXiv预印本
    arXiv:2301.12597*。'
- en: Li* et al. (2022) Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei
    Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang,
    Kai-Wei Chang, and Jianfeng Gao. 2022. Grounded language-image pre-training. In
    *CVPR*.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li*等（2022）Liunian Harold Li*、Pengchuan Zhang*、Haotian Zhang*、Jianwei Yang、Chunyuan
    Li、Yiwu Zhong、Lijuan Wang、Lu Yuan、Lei Zhang、Jenq-Neng Hwang、Kai-Wei Chang和Jianfeng
    Gao。2022年。《Grounded language-image pre-training》。发表于*CVPR*。
- en: Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2023a. [Visual instruction tuning](http://arxiv.org/abs/2304.08485).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2023a）Haotian Liu、Chunyuan Li、Qingyang Wu和Yong Jae Lee。2023a年。[Visual instruction
    tuning](http://arxiv.org/abs/2304.08485)。
- en: 'Liu et al. (2023b) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023b. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等（2023b）Pengfei Liu、Weizhe Yuan、Jinlan Fu、Zhengbao Jiang、Hiroaki Hayashi和Graham
    Neubig。2023b年。《Pre-train, prompt, and predict: A systematic survey of prompting
    methods in natural language processing》。*ACM计算机调查*，55(9):1–35。'
- en: 'Lu et al. (2019) Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.
    Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language
    tasks. *Advances in neural information processing systems*, 32.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu等（2019）Jiasen Lu、Dhruv Batra、Devi Parikh和Stefan Lee。2019年。《Vilbert: Pretraining
    task-agnostic visiolinguistic representations for vision-and-language tasks》。*神经信息处理系统进展*，32。'
- en: 'Mu et al. (2023) Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding,
    Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. 2023. Embodiedgpt: Vision-language
    pre-training via embodied chain of thought. *arXiv preprint arXiv:2305.15021*.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mu等（2023）Yao Mu、Qinglong Zhang、Mengkang Hu、Wenhai Wang、Mingyu Ding、Jun Jin、Bin
    Wang、Jifeng Dai、Yu Qiao和Ping Luo。2023年。《Embodiedgpt: Vision-language pre-training
    via embodied chain of thought》。*arXiv预印本 arXiv:2305.15021*。'
- en: OpenAI (2020) OpenAI. 2020. Language models are unsupervised multitask learners.
    *OpenAI Blog*, 23(6). [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2020）OpenAI。2020年。《Language models are unsupervised multitask learners》。*OpenAI博客*，23(6)。
    [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)。
- en: 'Padmakumar et al. (2022) Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava,
    Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan
    Tur, and Dilek Hakkani-Tur. 2022. Teach: Task-driven embodied agents that chat.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 36,
    pages 2017–2025.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Padmakumar等（2022）Aishwarya Padmakumar、Jesse Thomason、Ayush Shrivastava、Patrick
    Lange、Anjali Narayan-Chen、Spandana Gella、Robinson Piramuthu、Gokhan Tur和Dilek Hakkani-Tur。2022年。《Teach:
    Task-driven embodied agents that chat》。发表于*AAAI人工智能会议论文集*，第36卷，第2017–2025页。'
- en: 'Park and Kim (2023) Sang-Min Park and Young-Gab Kim. 2023. Visual language
    navigation: A survey and open challenges. *Artificial Intelligence Review*, 56(1):365–427.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park和Kim（2023）Sang-Min Park和Young-Gab Kim。2023年。《Visual language navigation:
    A survey and open challenges》。*人工智能评论*，56(1):365–427。'
- en: 'Puig et al. (2018) Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang,
    Sanja Fidler, and Antonio Torralba. 2018. [Virtualhome: Simulating household activities
    via programs](http://arxiv.org/abs/1806.07011).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Puig等（2018）Xavier Puig、Kevin Ra、Marko Boben、Jiaman Li、Tingwu Wang、Sanja Fidler和Antonio
    Torralba。2018年。[Virtualhome: Simulating household activities via programs](http://arxiv.org/abs/1806.07011)。'
- en: 'Qi et al. (2020a) Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang
    Wang, Chunhua Shen, and Anton van den Hengel. 2020a. Reverie: Remote embodied
    visual referring expression in real indoor environments. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9982–9991.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi等（2020a）Yuankai Qi、Qi Wu、Peter Anderson、Xin Wang、William Yang Wang、Chunhua
    Shen和Anton van den Hengel。2020a年。《Reverie: Remote embodied visual referring expression
    in real indoor environments》。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，第9982–9991页。'
- en: 'Qi et al. (2020b) Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang
    Wang, Chunhua Shen, and Anton van den Hengel. 2020b. Reverie: Remote embodied
    visual referring expression in real indoor environments. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9982–9991.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi 等 (2020b) Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang,
    Chunhua Shen 和 Anton van den Hengel. 2020b. Reverie: 在真实室内环境中的远程具身视觉指代表达。发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，第 9982–9991 页。'
- en: 'Qiao et al. (2023) Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, and Qi Wu.
    2023. March in chat: Interactive prompting for remote embodied referring expression.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pages 15758–15767.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiao 等 (2023) Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu 和 Qi Wu. 2023. March
    in chat: 远程具身指代表达的互动提示。发表于 *IEEE/CVF 国际计算机视觉会议论文集*，第 15758–15767 页。'
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger, and Ilya Sutskever. 2021. [Learning transferable visual
    models from natural language supervision](http://arxiv.org/abs/2103.00020).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger 和 Ilya Sutskever. 2021. [从自然语言监督中学习可转移的视觉模型](http://arxiv.org/abs/2103.00020)。
- en: 'Ramakrishnan et al. (2021) Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans,
    Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba,
    Andrew Westbury, Angel X Chang, et al. 2021. Habitat-matterport 3d dataset (hm3d):
    1000 large-scale 3d environments for embodied ai. *arXiv preprint arXiv:2109.08238*.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramakrishnan 等 (2021) Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans,
    Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba,
    Andrew Westbury, Angel X Chang 等. 2021. Habitat-matterport 3d 数据集 (hm3d): 1000
    个大型 3D 环境用于具身 AI。*arXiv 预印本 arXiv:2109.08238*。'
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. [Sentence-bert:
    Sentence embeddings using siamese bert-networks](http://arxiv.org/abs/1908.10084).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing*. Association for Computational Linguistics.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reimers 和 Gurevych (2019) Nils Reimers 和 Iryna Gurevych. 2019. [Sentence-bert:
    使用 Siamese BERT 网络的句子嵌入](http://arxiv.org/abs/1908.10084)。发表于 *2019 年自然语言处理实证方法会议论文集*。计算语言学协会。'
- en: 'Shah et al. (2023) Dhruv Shah, Błażej Osiński, Sergey Levine, et al. 2023.
    Lm-nav: Robotic navigation with large pre-trained models of language, vision,
    and action. In *Conference on Robot Learning*, pages 492–504\. PMLR.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shah 等 (2023) Dhruv Shah, Błażej Osiński, Sergey Levine 等. 2023. Lm-nav: 使用大型预训练的语言、视觉和动作模型进行机器人导航。发表于
    *机器人学习会议*，第 492–504 页。PMLR。'
- en: 'Tan et al. (2019) Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Learning to
    navigate unseen environments: Back translation with environmental dropout. *arXiv
    preprint arXiv:1904.04195*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan 等 (2019) Hao Tan, Licheng Yu 和 Mohit Bansal. 2019. 学习导航未见环境: 通过环境丢失进行反向翻译。*arXiv
    预印本 arXiv:1904.04195*。'
- en: 'Vasudevan et al. (2021) Arun Balajee Vasudevan, Dengxin Dai, and Luc Van Gool.
    2021. Talk2nav: Long-range vision-and-language navigation with dual attention
    and spatial memory. *International Journal of Computer Vision*, 129:246–266.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vasudevan 等 (2021) Arun Balajee Vasudevan, Dengxin Dai 和 Luc Van Gool. 2021.
    Talk2nav: 具有双重注意力和空间记忆的长距离视觉语言导航。*计算机视觉国际期刊*，129:246–266。'
- en: Wang et al. (2022a) Hanqing Wang, Wei Liang, Jianbing Shen, Luc Van Gool, and
    Wenguan Wang. 2022a. Counterfactual cycle-consistent learning for instruction
    following and generation in vision-language navigation. In *Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition*, pages 15471–15481.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2022a) Hanqing Wang, Wei Liang, Jianbing Shen, Luc Van Gool 和 Wenguan
    Wang. 2022a. 反事实循环一致学习用于视觉语言导航中的指令跟随和生成。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 15471–15481
    页。
- en: 'Wang et al. (2022b) Su Wang, Ceslee Montgomery, Jordi Orbay, Vighnesh Birodkar,
    Aleksandra Faust, Izzeddin Gur, Natasha Jaques, Austin Waters, Jason Baldridge,
    and Peter Anderson. 2022b. Less is more: Generating grounded navigation instructions
    from landmarks. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 15428–15438.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2022b) Su Wang, Ceslee Montgomery, Jordi Orbay, Vighnesh Birodkar,
    Aleksandra Faust, Izzeddin Gur, Natasha Jaques, Austin Waters, Jason Baldridge
    和 Peter Anderson. 2022b. 少即是多: 从地标生成有根据的导航指令。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第
    15428–15438 页。'
- en: 'Wang et al. (2023) Xiaohan Wang, Wenguan Wang, Jiayi Shao, and Yi Yang. 2023.
    Lana: A language-capable navigator for instruction following and generation. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 19048–19058.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Xiaohan Wang, Wenguan Wang, Jiayi Shao, 和 Yi Yang. 2023.
    Lana: 一个具备语言能力的导航系统，用于指令跟随和生成。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 19048–19058 页。'
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed
    Chi, Quoc Le, 和 Denny Zhou. 2022. 思维链提示激发大型语言模型的推理能力。*arXiv 预印本 arXiv:2201.11903*。
- en: 'Yu et al. (2023) Bangguo Yu, Hamidreza Kasaei, and Ming Cao. 2023. L3mvn: Leveraging
    large language models for visual target navigation. *arXiv preprint arXiv:2304.05501*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2023) Bangguo Yu, Hamidreza Kasaei, 和 Ming Cao. 2023. L3mvn: 利用大型语言模型进行视觉目标导航。*arXiv
    预印本 arXiv:2304.05501*。'
- en: 'Zhang and Kordjamshidi (2023) Yue Zhang and Parisa Kordjamshidi. 2023. Vln-trans:
    Translator for the vision and language navigation agent. *arXiv preprint arXiv:2302.09230*.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 和 Kordjamshidi (2023) Yue Zhang 和 Parisa Kordjamshidi. 2023. Vln-trans:
    视觉与语言导航代理的翻译器。*arXiv 预印本 arXiv:2302.09230*。'
- en: Zhao et al. (2021) Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alexander
    Ku, Jason Baldridge, and Eugene Ie. 2021. On the evaluation of vision-and-language
    navigation instructions. *arXiv preprint arXiv:2101.10504*.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2021) Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alexander
    Ku, Jason Baldridge, 和 Eugene Ie. 2021. 关于视觉与语言导航指令的评估。*arXiv 预印本 arXiv:2101.10504*。
- en: 'Zhou et al. (2023a) Gengze Zhou, Yicong Hong, and Qi Wu. 2023a. Navgpt: Explicit
    reasoning in vision-and-language navigation with large language models. *arXiv
    preprint arXiv:2305.16986*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023a) Gengze Zhou, Yicong Hong, 和 Qi Wu. 2023a. Navgpt: 在视觉和语言导航中使用大型语言模型进行明确推理。*arXiv
    预印本 arXiv:2305.16986*。'
- en: 'Zhou et al. (2023b) Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia
    Jin, Lise Getoor, and Xin Eric Wang. 2023b. Esc: Exploration with soft commonsense
    constraints for zero-shot object navigation. *arXiv preprint arXiv:2301.13166*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023b) Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia
    Jin, Lise Getoor, 和 Xin Eric Wang. 2023b. Esc: 带有软常识约束的探索以实现零样本对象导航。*arXiv 预印本
    arXiv:2301.13166*。'
- en: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan
    Zhang, and Mohamed Elhoseiny. 2023. Chatgpt asks, blip-2 answers: Automatic questioning
    towards enriched visual descriptions. *arXiv preprint arXiv:2303.06594*.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen, Wenxuan
    Zhang, 和 Mohamed Elhoseiny. 2023. Chatgpt 提问，blip-2 回答: 自动提问以丰富视觉描述。*arXiv 预印本
    arXiv:2303.06594*。'
- en: '![Refer to caption](img/99c8f3d53ca0bd755434cb59007fef14.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/99c8f3d53ca0bd755434cb59007fef14.png)'
- en: 'Figure 4: Egocentric Image Sequence from a path in ThreeDWorld Gan et al. ([2020](#bib.bib9))'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 来自 ThreeDWorld 路径的自我中心图像序列 Gan et al. ([2020](#bib.bib9))'
- en: '![Refer to caption](img/59ba49aa52503e4900f6eb4bfb744ce4.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/59ba49aa52503e4900f6eb4bfb744ce4.png)'
- en: 'Figure 5: Egocentric Image Sequence from a path in AI Habitat Ramakrishnan
    et al. ([2021](#bib.bib36))'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 来自 AI Habitat 路径的自我中心图像序列 Ramakrishnan et al. ([2021](#bib.bib36))'
- en: '![Refer to caption](img/1f3687edd334ff63404ceb1f3670f35f.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1f3687edd334ff63404ceb1f3670f35f.png)'
- en: 'Figure 6: Egocentric Image Sequence from a path in Matterport3D Chang et al.
    ([2017](#bib.bib4))'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 来自 Matterport3D 路径的自我中心图像序列 Chang et al. ([2017](#bib.bib4))'
- en: Appendix A In-Context Learning Strategies
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 上下文学习策略
- en: In this section, we discuss some strategies we employ to get the best possible
    wayfinding instruction. A prompt template is presented to the LLM as -
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了一些策略，以获得最佳的路径指引。一个提示模板被呈现给LLM，如 -
- en: '"A robot agent at home sees a sequence of egocentric images with the following
    frame descriptions.'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “家中的机器人代理看到一系列自我中心图像，具有以下帧描述。
- en: 'Frame 0: '
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '帧 0:'
- en: 'Frame 1: '
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '帧 1:'
- en: …
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …
- en: 'Frame n: '
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '帧 n:'
- en: 'Reference Texts: [’Go to …’, ’Move past …’, Walk ahead … ]'
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '参考文本: [‘前往 …’, ‘经过 …’, 向前走 … ]'
- en: Write an concise instruction in the style of the Reference Texts that would
    get the robot from Frame 0 to Frame n. "
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 编写一条简明的指令，按照参考文本的风格，将机器人从帧 0 导航到帧 n。"
- en: 'Each caption in this template is obtained using the LLM + BLIP strategy outlined
    in section [2.1](#S2.SS1 "2.1 Extracting Spatial Knowledge: LLM + BLIP ‣ 2 Approach
    ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis").'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '该模板中的每个标题是使用第 [2.1](#S2.SS1 "2.1 提取空间知识: LLM + BLIP ‣ 2 方法 ‣ LLM 能否生成类似人类的路径指引？迈向平台无关的体现式指令合成")
    节中概述的LLM + BLIP策略获得的。'
- en: A.1 Influence of LLM + BLIP
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 LLM + BLIP的影响
- en: 'An example egocentric image sequence of a path taken in the TDW simulator Gan
    et al. ([2020](#bib.bib9)) is shown in figure [4](#A0.F4 "Figure 4 ‣ Can LLMs
    Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied
    Instruction Synthesis"). Using the LLM + BLIP approach discussed in section [2.1](#S2.SS1
    "2.1 Extracting Spatial Knowledge: LLM + BLIP ‣ 2 Approach ‣ Can LLMs Generate
    Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction
    Synthesis"), we get the following captions for each image in [4](#A0.F4 "Figure
    4 ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#A0.F4 "图4 ‣ 大型语言模型能否生成类人导航指令？走向平台无关的具身指令合成")展示了在TDW模拟器中走过的路径的自我中心图像序列（Gan等，[2020](#bib.bib9)）。使用在[2.1](#S2.SS1
    "2.1 提取空间知识：LLM + BLIP ‣ 2 方法 ‣ 大型语言模型能否生成类人导航指令？走向平台无关的具身指令合成")节中讨论的LLM + BLIP方法，我们为[4](#A0.F4
    "图4 ‣ 大型语言模型能否生成类人导航指令？走向平台无关的具身指令合成")中的每张图像获得了以下说明。
- en: '1.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The image depicts a computer screen showing a colorful video of a man that is
    being displayed on a television. There is also a chair visible in the image besides
    the television.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像展示了一台计算机屏幕，上面显示着一个男人的彩色视频，视频通过电视播放。图像中除了电视外，还有一把椅子可见。
- en: '2.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The image contains a small chair made of fabric, in colors of red, white and
    gray. There is another object present in the image, but it is not clear what it
    is.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像包含一把由红色、白色和灰色面料制成的小椅子。图像中还有另一个物体，但不清楚那是什么。
- en: '3.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The image is of a living room with brown furniture and no decorations on the
    walls. There are no people present in the living room.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像显示的是一个有棕色家具且墙上没有装饰的客厅。客厅内没有人。
- en: '4.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: The image depicts a room with a gray couch located against a wall. There is
    a small television mounted on the wall.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像展示了一个房间，墙上靠着一张灰色的沙发。墙上还挂着一台小电视。
- en: '5.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: The image features a computer screen displaying a website, with a couch visible
    in the background. A plant is placed on a table next to the computer. No other
    objects are visible on the table.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像展示了一台显示网站的计算机屏幕，背景中可见一张沙发。计算机旁边的桌子上放着一盆植物。桌子上没有其他物品。
- en: Passing these Frame descriptions to the prompt in [A](#A1 "Appendix A In-Context
    Learning Strategies ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
    Platform-Agnostic Embodied Instruction Synthesis") gives us a REVERIE-like instruction
    as,
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些帧描述传递给[A](#A1 "附录 A 上下文学习策略 ‣ 大型语言模型能否生成类人导航指令？走向平台无关的具身指令合成")，我们得到类似REVERIE的指令，如下：
- en: Go to the living room, then move to the room with the gray couch and turn off
    the television mounted on the wall.
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 前往客厅，然后移步到灰色沙发的房间，关闭墙上的电视。
- en: and R2R-like instruction as,
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 和类似R2R的指令如下：
- en: Go from the computer screen to the chair, then past the object in the background
    and into the living room. Walk past the blue furniture and turn right towards
    the gray couch. Finally, stop in front of the table with the plant and view the
    website on the computer screen.
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从计算机屏幕走到椅子，然后经过背景中的物体，进入客厅。经过蓝色家具，右转到灰色沙发处。最后，在带植物的桌子前停下，查看计算机屏幕上的网站。
- en: Notice the level of detail added by insights that LLM gathers by conversing
    with BLIP. This inturn gives more information for GPT-3.5-Turbo-Instruct to use
    for generating a final instruction from the frames.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意LLM通过与BLIP对话所获得的见解所增加的细节。这反过来为GPT-3.5-Turbo-Instruct生成最终指令提供了更多信息。
- en: '![Refer to caption](img/94fba0e36b1dff1fc3e2961bdfec3543.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/94fba0e36b1dff1fc3e2961bdfec3543.png)'
- en: 'Figure 7: LLM + BLIP: Notice the initial BLIP caption mentions a television
    in the image, even when it is not present. When the LLM asks for the presence
    of electronic items in the room, BLIP answers no, which leads to the refined caption
    preventing misinformation.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：LLM + BLIP：注意最初的BLIP说明提到图像中有电视，即使实际上没有电视。当LLM询问房间内是否有电子设备时，BLIP回答没有，这样可以通过改进说明来防止信息错误。
- en: We also experiment with using only BLIP captions with object and room queries,
    without the LLM. The REVERIE-like instruction in this case looks like,
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还尝试了仅使用BLIP说明结合物体和房间查询，而不使用LLM。在这种情况下，类似REVERIE的指令如下：
- en: Go to the living room on level 0 and turn off the television by the couch and
    the table.
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 前往0层的客厅，关闭沙发和桌子旁的电视。
- en: and the R2R-like instruction is,
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 和类似R2R的指令是：
- en: Start in the kitchen and go up the stairs on the left. Turn right at the top
    of the stairs and then go past the round table and chairs and stairs. Keep walking
    until you see the two small tables on the rug and then turn left. Go down the
    hallway keeping the wall on your left and stop in front of the door on your right
    with the treadmill. Turn left and you will see the living room with a computer
    screen containing a picture of a couch and a table.
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从厨房开始，沿左侧的楼梯上楼。到达楼梯顶端时右转，然后经过圆桌、椅子和楼梯。继续前行，直到看到地毯上的两张小桌子，然后左转。沿着走廊走，左侧靠着墙，停在右侧有跑步机的门前。左转，你会看到客厅，里面有一台显示器，显示一张沙发和桌子的图片。
- en: While the REVERIE-like instruction is still usable, notice the R2R-like instruction
    tends to be nonsensical with ghost objects such as stairs and treadmill in the
    caption. It also contains bad directions. We observe this phenomenon in multiple
    cases, and Figure [7](#A1.F7 "Figure 7 ‣ A.1 Influence of LLM + BLIP ‣ Appendix
    A In-Context Learning Strategies ‣ Can LLMs Generate Human-Like Wayfinding Instructions?
    Towards Platform-Agnostic Embodied Instruction Synthesis") showcases how the conversation
    with the LLM improves the initial captions to remove ghost objects and prevent
    misinformation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 REVERIE 类似的指令仍然可用，但请注意 R2R 类似的指令往往不合逻辑，例如在说明中出现虚假对象，如楼梯和跑步机。它们也包含不良指令。我们在多个案例中观察到这一现象，图
    [7](#A1.F7 "Figure 7 ‣ A.1 Influence of LLM + BLIP ‣ Appendix A In-Context Learning
    Strategies ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis") 展示了与 LLM 的对话如何改善初始说明，去除虚假对象并防止误导信息。
- en: Thus, we infer that using an LLM with BLIP to provide more detail about the
    environment is important when it comes to finally generating more meaningful instructions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们推断使用带有 BLIP 的 LLM 提供更多关于环境的细节在生成更有意义的指令时非常重要。
- en: A.2 Empirical Information on Instruction Styles
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 关于指令风格的实证信息
- en: We utilize factual knowledge about R2R and REVERIE instruction styles to finetune
    the LLM prompt.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用有关 R2R 和 REVERIE 指令风格的事实知识来微调 LLM 提示。
- en: A.2.1 Additional Constraints for R2R
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 R2R 的附加约束条件
- en: Upon inspection, we observe that R2R instructions are usually 2 or more sentences
    long, attributed to longer path lengths. Further, in the R2R paper, the authors
    mention that they ask annotators to “write directions so that a smart robot can
    find the goal location after starting from the same start location", and are told
    that it is not necessary to follow the path, but only to reach the goal. We incorporate
    this information to append our prompt:-
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 经过检查，我们观察到 R2R 指令通常由两句或更多句子组成，这与较长的路径长度有关。此外，在 R2R 论文中，作者提到他们要求标注员“编写指令，以便智能机器人在从相同的起始位置出发后能够找到目标位置”，并且被告知不必按照路径行进，只需到达目标即可。我们将这些信息纳入以附加我们的提示：-
- en: “Write directions so a smart robot can find the final frame after starting from
    the same starting frame. You do not have to use information in the frames, and
    just need to reach the goal location."
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “编写指令，以便智能机器人能够在从相同的起始框架出发后找到最终框架。你不需要使用框架中的信息，只需到达目标位置。”
- en: A.2.2 Additional Constraints for REVERIE
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 REVERIE 的附加约束条件
- en: REVERIE instructions are concise, and talk only about the goal location. Clip-Nav
    Dorbala et al. ([2022](#bib.bib8)) studies REVERIE in detail and empirically deduces
    that most instructions can be broken down into navigation and activity components,
    with the conjunction and between them. We utilize this information to add the
    following to our prompt:-
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: REVERIE 指令简洁，仅讨论目标位置。Clip-Nav Dorbala 等人 ([2022](#bib.bib8)) 详细研究了 REVERIE，并从实证中推断出大多数指令可以分解为导航和活动组件，并且它们之间有连词。我们利用这些信息向我们的提示中添加以下内容：-
- en: '"The instruction must be a single sentence long, ending with a task related
    to an object in the final frame, and must be less than 20 words."'
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “指令必须是一句话长，结尾包含与最终框架中的对象相关的任务，并且必须少于 20 个单词。”
- en: Appendix B Evaluation Details
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 评估细节
- en: B.1 Simulator Implementations
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 模拟器实现
- en: We implement our approach on 3 different simulation platforms, namely AI Habitat
    Ramakrishnan et al. ([2021](#bib.bib36)), Matterport3D Chang et al. ([2017](#bib.bib4))
    and ThreeDWorld (TDW) Gan et al. ([2020](#bib.bib9)). Egocentric image sequences
    for these simulators are presented in Figure [4](#A0.F4 "Figure 4 ‣ Can LLMs Generate
    Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction
    Synthesis"), Figure [5](#A0.F5 "Figure 5 ‣ Can LLMs Generate Human-Like Wayfinding
    Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis") and Figure
    [6](#A0.F6 "Figure 6 ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
    Platform-Agnostic Embodied Instruction Synthesis") respectively. Depending on
    the type of simulator, we revise our strategy for extracting sequences as listed
    below -
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个不同的模拟平台上实现了我们的方法，即AI Habitat Ramakrishnan 等 ([2021](#bib.bib36))、Matterport3D
    Chang 等 ([2017](#bib.bib4)) 和 ThreeDWorld (TDW) Gan 等 ([2020](#bib.bib9))。这些模拟器的自我中心图像序列分别展示在图
    [4](#A0.F4 "Figure 4 ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
    Platform-Agnostic Embodied Instruction Synthesis")、图 [5](#A0.F5 "Figure 5 ‣ Can
    LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied
    Instruction Synthesis") 和图 [6](#A0.F6 "Figure 6 ‣ Can LLMs Generate Human-Like
    Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis")
    中。根据模拟器的类型，我们会修订提取序列的策略，如下所示 -
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Environments in the Matterport3D simulator are taken from real world scenes
    and provide fully connected graphs whose nodes represent 360 panoramas. Given
    two nodes from the connected graph, we compute a path between them as a sequence
    of nodes. To compute captions, we either consider the central frame or the entire
    panorama (described in Appendix [B.3.2](#A2.SS3.SSS2 "B.3.2 Matterport3D: Frame
    Selection ‣ B.3 Quantitative Study - Zero-Shot Embodied Navigation ‣ Appendix
    B Evaluation Details ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards
    Platform-Agnostic Embodied Instruction Synthesis")). The path contains discrete
    “hops" of in the form of images, which gives us our image sequence.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Matterport3D 模拟器中的环境来自现实世界场景，并提供完全连接的图形，其中的节点代表 360 度全景。给定图中两个节点，我们计算它们之间的路径作为节点序列。为了计算说明，我们要么考虑中心帧，要么考虑整个全景（详见附录
    [B.3.2](#A2.SS3.SSS2 "B.3.2 Matterport3D: Frame Selection ‣ B.3 Quantitative Study
    - Zero-Shot Embodied Navigation ‣ Appendix B Evaluation Details ‣ Can LLMs Generate
    Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction
    Synthesis")）。路径包含形式为图像的离散“跳跃”，这给我们提供了图像序列。'
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: AI Habitat has continuous 3D reconstructions of real world household environments.
    To obtain a path, we first sample two navigable points in the environment and
    compute the shortest distance between them. Then, to obtain a discrete sequence
    of images, we sample images at a uniform interval along the path.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: AI Habitat 具有现实世界家庭环境的连续 3D 重建。为了获得路径，我们首先在环境中采样两个可导航的点，并计算它们之间的最短距离。然后，为了获得离散的图像序列，我们沿路径以均匀的间隔采样图像。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: TDW is a photorealistic simulator that is capable of procedurally generating
    new environments. We make use of this simulator to test the robustness of our
    approach in non-real world environments. We obtain our image sequence in the same
    manner as AI Habitat.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TDW 是一个光线真实模拟器，能够程序性地生成新的环境。我们利用这个模拟器来测试我们的方法在非真实环境中的鲁棒性。我们以与 AI Habitat 相同的方式获得我们的图像序列。
- en: For the user study, we sample 100 paths of varying lengths from each of these
    simulators, randomly choosing from environments they offer. We then use our approach
    on these paths to generate instructions in a platform-agnostic manner.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用户研究，我们从每个模拟器中采样 100 条长度不同的路径，随机选择它们提供的环境。然后，我们在这些路径上应用我们的方法，以平台无关的方式生成指令。
- en: B.2 Qualitative Analysis - User Study Details
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 定性分析 - 用户研究细节
- en: Each user is presented with a random image sequence chosen from a bank of sequences
    gathered from the 3 different environments. This allows for us to evaluate the
    generated instruction across multiple platforms. We observe a consistent performance
    across simulators, leading us to establish the platform-agnostic nature of our
    instruction synthesizer.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用户都会被展示一个从三个不同环境中收集的图像序列库中随机选择的图像序列。这使我们能够在多个平台上评估生成的指令。我们观察到在各个模拟器上表现一致，这使我们确认了我们指令合成器的平台无关性。
- en: Our study was aimed at quantifying the usability of generated instructions in
    guiding an embodied agent in the environment. In this direction, we first presented
    the user with video of an egocentric image sequence chosen from a random simulation
    platform. After being shown examples of fine or coarse grained instructions, the
    users were asked to provide an instruction describing the robot’s path in that
    style. Finally, the participant is shown the synthesized instruction for the same
    sequence and is asked comparative questions highlighted in figure below.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究旨在量化生成的指令在指导环境中的具身体的可用性。为此，我们首先向用户展示了从随机仿真平台选择的自我中心图像序列的视频。在展示了细粒度或粗粒度指令的示例后，要求用户提供描述机器人路径的指令。最后，参与者会看到同一序列的合成指令，并被询问下图中突出显示的比较问题。
- en: '![Refer to caption](img/7f00776d571abd938e0666ffda490910.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7f00776d571abd938e0666ffda490910.png)'
- en: Our User Study. The participant is asked questions on the quality of the generated
    instructions and about how much it compares with the instruction that they wrote.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用户研究。参与者被询问生成的指令的质量以及与他们所写的指令的比较程度。
- en: Each question aims to tackle a different comparative perspective. The first
    question seeks to find out if the generated instructions are similar to what the
    user has written down. The second question asks if the generated instructions
    accurately capture details of the environment. The third queries about the robustness
    of generation by asking if the participant has noticed any ghost objects or artifacts.
    Finally, we ask if the user thinks an embodied agent could reach the target location
    by following the generated instruction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 每个问题旨在处理不同的比较视角。第一个问题旨在找出生成的指令是否与用户所写的相似。第二个问题询问生成的指令是否准确捕捉了环境的细节。第三个问题查询生成的稳健性，询问参与者是否注意到任何幽灵对象或伪影。最后，我们询问用户是否认为一个具身体可以通过遵循生成的指令到达目标位置。
- en: Out of a total of 30 participants, $83.3\%$) reported seeing ghost objects,
    which indicates either that some people may have missed objects in the video,
    or that the generated instruction is sensitive to the captioning scheme.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在30名参与者中，有$83.3\%$报告看到幽灵对象，这表明一些人可能在视频中遗漏了对象，或者生成的指令对标题方案非常敏感。
- en: Conversely, $43.3\%$ of participants believed that the instructions generated
    were either very different from what they wrote, or had minor overlaps. We can
    infer from this that the vocabulary people use to describe a path may significantly
    vary from the generated instruction. However, this does not necessarily mean that
    the agent would not be able to follow the generated instruction to reach the target
    location, as it would use alternate references or landmarks to get there.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，$43.3\%$的参与者认为生成的指令与他们所写的指令要么非常不同，要么有细微的重叠。我们可以从中推断出，人们用来描述路径的词汇可能与生成的指令显著不同。然而，这并不一定意味着具身体无法遵循生成的指令到达目标位置，因为它可能会使用替代的参考点或地标来到达那里。
- en: Our study was determined exempt by our institution’s IRB. All of the participants
    voluntarily chose to participate in it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究被我们机构的IRB认定为豁免。所有参与者都是自愿选择参与的。
- en: B.3 Quantitative Study - Zero-Shot Embodied Navigation
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 定量研究 - 零样本具身导航
- en: B.3.1 Dataset and Navigation Setup Details
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.1 数据集和导航设置细节
- en: We run navigation experiments on the REVERIE dataset, which tackles vision-and-language
    navigation (VLN) using coarse-grained instructions. Instructions in REVERIE have
    been human-annotated, where the annotator is asked to write a high-level instruction
    describing how to get to the target location after being shown a path in the Matterport3D
    environment. Each path is discrete, i.e., it consists of a set of panoramic images
    or nodes along which the agent “hops". The nodes inturn consist of $4$ degree
    view of the agent.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在REVERIE数据集上进行导航实验，该数据集使用粗粒度指令处理视觉-语言导航（VLN）。REVERIE中的指令经过人工注释，注释员在观看Matterport3D环境中的路径后，被要求编写描述如何到达目标位置的高层指令。每条路径是离散的，即它由一组全景图像或节点组成，具身体沿这些节点“跳跃”。这些节点反过来由具身体的$4$度视图组成。
- en: We consider a generalizable, zero-shot case, where the agent is dropped in an
    environment that it has no knowledge of, and is given an instruction that it must
    follow to get to a target location. This setting is in line with our ultimate
    goal of developing a generalist embodied navigation agent, which is able to function
    without any supervision in an unseen environment. We opt to use the unseen validation
    split of the REVERIE dataset for evaluation, which contains environments that
    the agent would not see in the training split. It contains $504$ paths, which
    was deemed sufficient for showcasing zero-shot navigation prowess using the generated
    instructions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一种可泛化的零样本情况，其中智能体被放置在一个完全陌生的环境中，并且给出一个必须遵循的指令，以达到目标位置。这个设置与我们最终的目标一致，即开发一个能够在未知环境中无监督操作的通用智能体导航系统。我们选择使用
    REVERIE 数据集的未知验证集进行评估，其中包含智能体在训练集中未见过的环境。该数据集包含 $504$ 条路径，这被认为足以展示使用生成指令进行零样本导航的能力。
- en: CLIP-Nav Dorbala et al. ([2022](#bib.bib8)) uses CLIP to make grounding decisions
    for navigation. The instruction is first broken down into a Navigation Component
    (NC) and an Activity Component (AC). The NC contains information about getting
    to the target location, while the AC containing the activity that the agent is
    expected to perform is disregarded. The NC is further broken down into noun phrases
    using GPT-3.5-turbo, which are then grounded using CLIP with each of the 4 images
    captured by the agent from its panoramic view. The agent takes the direction of
    the highest CLIP grounding score.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP-Nav Dorbala 等人 ([2022](#bib.bib8)) 使用 CLIP 进行导航的基础决策。指令首先被分解为导航组件（NC）和活动组件（AC）。NC
    包含到达目标位置的信息，而 AC 包含智能体预计要执行的活动，则被忽略。NC 进一步使用 GPT-3.5-turbo 被分解为名词短语，然后使用 CLIP
    和智能体从全景视图捕捉的 4 张图像进行基础化。智能体选择 CLIP 基础评分最高的方向。
- en: Seq-CLIP-Nav extends this to incorporate backtracking. Backtracking refers to
    when the agent falls back or “backtracks" a few nodes when it determines that
    it has taken the wrong path.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Seq-CLIP-Nav 扩展了回溯功能。回溯指的是当智能体发现自己走错了路径时，会退回或“回溯”几个节点。
- en: We also ablate with GLIP-Nav, a variant of Seq-CLIP-Nav we introduce, where
    CLIP is replaced with GLIP Li* et al. ([2022](#bib.bib23)) for obtaining grounding
    scores.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还用 GLIP-Nav 进行消融实验，GLIP-Nav 是我们引入的 Seq-CLIP-Nav 的变体，其中 CLIP 被 GLIP Li* 等人
    ([2022](#bib.bib23)) 替代，用于获取基础评分。
- en: 'B.3.2 Matterport3D: Frame Selection'
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.2 Matterport3D：帧选择
- en: REVERIE provides a set of panoramic images taken from Matterport3D that forms
    a path corresponding to each instruction. The annotator is provided with this
    whole panoramic view at each step. To incorporate our generation approach here,
    we consider two variations.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: REVERIE 提供了一组来自 Matterport3D 的全景图像，这些图像形成了与每个指令对应的路径。注释者在每一步都可以查看整个全景视图。为了在这里结合我们的生成方法，我们考虑了两种变体。
- en: 'Central Caption: We hypothesize that the central frame contains the most immediate
    and critical information required for the embodied agent to perform its next set
    of actions. To this end, we caption only the central frames (i.e., the image in
    the direction of the agent’s heading) of the entire path sequence to generate
    the instruction.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 中央字幕：我们假设中央帧包含智能体执行下一组动作所需的最直接和关键的信息。为此，我们仅为整个路径序列中的中央帧（即智能体前进方向上的图像）添加字幕，以生成指令。
- en: 'Panoramic Caption: Here we caption each image of the entire panorama (4 frames),
    and summarize the individual captions using the LLM. We perform this over the
    entire path sequence to generate the instruction. Although the panoramic sequence
    contains more semantic information over the single (central) frame, note that
    each instruction is only a single sentence, and compressing all the information
    of a scene (be it the target or an image along the path) is non-trivial, if the
    instruction has to be of a suitable length.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 全景字幕：我们为全景图像（4帧）中的每一幅图像添加字幕，并使用大型语言模型（LLM）总结每个字幕。我们在整个路径序列上执行这一操作，以生成指令。虽然全景序列包含比单一（中央）帧更多的语义信息，但请注意，每个指令只有一句话，并且如果指令长度要适中，将场景中的所有信息（无论是目标还是沿路径的图像）压缩并非易事。
- en: 'During the panoramic-frame case, we use the LLM to summarize the set of captions
    obtained $4$ degree views around the agent. Each caption in this set is obtained
    using the LLM + BLIP approach discussed in section [2.1](#S2.SS1 "2.1 Extracting
    Spatial Knowledge: LLM + BLIP ‣ 2 Approach ‣ Can LLMs Generate Human-Like Wayfinding
    Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis"). The
    prompt for this is -'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '在全景框架情况下，我们使用LLM来总结围绕代理的$4$度视图获得的一组标题。该组标题中的每个标题都是使用第[2.1](#S2.SS1 "2.1 Extracting
    Spatial Knowledge: LLM + BLIP ‣ 2 Approach ‣ Can LLMs Generate Human-Like Wayfinding
    Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis")节中讨论的LLM
    + BLIP方法获得的。其提示为 -'
- en: '"I see a panoramic view with the following descriptions.'
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我看到一个全景视图，其描述如下。
- en: 'North: '
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 北：
- en: 'East: '
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 东：
- en: 'South: '
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 南：
- en: 'West: '
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 西：
- en: Summarize these descriptions into a single description using less than $20$
    words."
  id: totrans-217
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将这些描述总结成一个少于$20$个单词的描述。
- en: B.3.3 Inferences on Generated Instructions
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.3 生成指令的推论
- en: 'In addition to the results presented in section [3.2](#S3.SS2 "3.2 Quantitative:
    Embodied Navigation ‣ 3 Evaluation & Results ‣ Can LLMs Generate Human-Like Wayfinding
    Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis"), we also
    measure the average pairwise cosine similarity using MiniLM-V6 Reimers and Gurevych
    ([2019](#bib.bib37)) between the human-annotated instructions and the generated
    instructions.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '除了[3.2](#S3.SS2 "3.2 Quantitative: Embodied Navigation ‣ 3 Evaluation & Results
    ‣ Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis")节中呈现的结果外，我们还使用MiniLM-V6 Reimers和Gurevych ([2019](#bib.bib37))测量了人工标注指令与生成指令之间的平均成对余弦相似度。'
- en: For the central-caption case, we get a score of $0.476$. From the overall positive
    correlation, we can infer that the generated instructions tend to be similar to
    the human-annotated ones on average. Some individual cases of extreme difference
    are discussed below.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于中心标题的情况，我们得到一个$0.476$的分数。从整体的正相关性来看，我们可以推断生成的指令在平均上趋于类似于人工标注的指令。下面讨论了一些极端差异的个别情况。
- en: In a low cosine similarity example, consider
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在低余弦相似度的例子中，请考虑
- en: 'Human-Annotated: "Walk to the bottom of the stairs leading to the level 1 hallway
    and find the bottommost stair" Generated: "Move from bedroom to kitchen, turn
    off faucet."'
  id: totrans-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人工标注：“走到通往1楼走廊的楼梯底部并找到最底层的楼梯。” 生成：“从卧室移动到厨房，关闭水龙头。”
- en: 'Similarity: $0.0850$'
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 相似度：$0.0850$
- en: Notice that the human-annotated instruction presents a unique situation to the
    agent where it is expected to find the bottommost stair. In contrast, the generated
    instruction asks the agent to move to the kitchen, which is near the vicinity
    of the staircase in this environment. While the cosine similarity might be low,
    a generalist agent would still be able to reach the target location with the given
    instruction since it references other elements (“the faucet" here) in the scene.
    Note that VLN tasks deal with the agent reaching a target location, and not with
    what it needs to do once it gets there.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到人工标注的指令为代理提供了一个独特的情况，期望它找到最底层的楼梯。相比之下，生成的指令要求代理移动到厨房，该位置在环境中的楼梯附近。虽然余弦相似度可能较低，但通用代理仍然能够按照给定的指令到达目标位置，因为它参考了场景中的其他元素（这里是“水龙头”）。注意，VLN任务处理的是代理到达目标位置，而不是到达后需要做什么。
- en: In a high cosine-similarity example, consider,
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在高余弦相似度的例子中，请考虑，
- en: 'Human-Annotated: "Go through the nearest bedroom to the bathroom on the first
    floor and turn on the faucet on the rightmost" Generated: "Go to the bedroom and
    turn off faucet."'
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人工标注：“通过最近的卧室到达一楼的浴室，并打开最右边的水龙头。” 生成：“去卧室并关闭水龙头。”
- en: 'Similarity: $0.820$'
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 相似度：$0.820$
- en: Observe that a high cosine similarity does not necessarily mean that the generated
    instruction is of good quality. In this example, notice that the human annotator
    asks the agent to enter the bathroom after going through the bedroom to turn off
    the faucet. The generated instruction however entirely misses out on entering
    the bathroom, which would cause an agent to incorrectly look for a faucet in the
    bedroom.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，高余弦相似度并不一定意味着生成的指令质量很好。在这个例子中，注意到人工标注者要求代理在经过卧室后进入浴室以关闭水龙头。然而，生成的指令完全忽略了进入浴室，这将导致代理错误地在卧室寻找水龙头。
- en: These are however one-off cases; we observe that most generated instructions
    tend to closely follow or paraphrase human-annotations. For instance, consider,
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然而这些只是个别情况；我们观察到大多数生成的指令倾向于紧密跟随或改述人工标注。例如，请考虑，
- en: 'Human-Annotated: "Go to the bathroom on level 1 and wipe off the faucet" Generated:
    "Go to the wooden room on level 1, turn off faucet in the bathroom."'
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人工标注： “去1层的厕所并擦拭水龙头” 生成的： “去1层的木制房间，关闭厕所中的水龙头。”
- en: 'Similarity: $0.885$'
  id: totrans-231
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 相似度： $0.885$
- en: Both these instructions ask the agent to go to the bathroom on level 1 to execute
    a task.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个指令都要求代理去1层的厕所执行任务。
- en: Appendix C Related Work
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 相关工作
- en: C.1 Embodied Instruction Synthesis
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 具身指令合成
- en: Embodied or Vision-and-Language Navigation deals with the problem of navigating
    an agent in unseen photorealistic environments and adhering to language instructions.
    These wayfinding instructions are usually human annotated as part of datasets
    Ku et al. ([2020](#bib.bib19)); Qi et al. ([2020b](#bib.bib33)); Anderson et al.
    ([2018b](#bib.bib2)); Krantz et al. ([2020](#bib.bib18)), and can roughly be categorized
    into coarse and fine-grained Gu et al. ([2022](#bib.bib10)) based on their level
    of detail. As these datasets are exclusive to the environments that they are created
    in, generalizing them to other new or procedurally generated environments presents
    a unique challenge. Most prior work on instructions synthesis Li et al. ([2022](#bib.bib21))
    has mostly been tailored toward data augmentation. Wang et al. ([2022a](#bib.bib41))
    presents a counterfactual reasoning approach to generate instructions, but ultimately
    requires the model to be trained on the R2R Anderson et al. ([2018a](#bib.bib1))
    dataset. Wang et al. ([2022b](#bib.bib42)); Kamath et al. ([2023](#bib.bib16))
    present imitation learning models that are trained on datasets, and use the augmented
    instructions to improve navigation performance. More recently Wang et al. ([2023](#bib.bib43))
    presents a navigation agent which is able to not only execute human-written navigation
    commands, but also provide route descriptions to humans. These approaches are
    limited to a few datasets and have cumbersome training procedures. In contrast,
    our approach can generalize over multiple styles of instructions, over multiple
    simulation platforms without requiring a dataset.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 具身或视觉与语言导航涉及在未见过的逼真环境中导航代理并遵循语言指令的问题。这些路径指示通常作为数据集的一部分由人工标注，如 Ku 等（[2020](#bib.bib19)）；Qi
    等（[2020b](#bib.bib33)）；Anderson 等（[2018b](#bib.bib2)）；Krantz 等（[2020](#bib.bib18)），并且可以根据细节水平大致分为粗略和细粒度两类
    Gu 等（[2022](#bib.bib10)）。由于这些数据集仅适用于其创建的环境，将其推广到其他新的或程序生成的环境中是一个独特的挑战。大多数先前关于指令合成的工作
    Li 等（[2022](#bib.bib21)）主要针对数据增强。Wang 等（[2022a](#bib.bib41)）提出了一种反事实推理方法来生成指令，但最终需要在
    R2R Anderson 等（[2018a](#bib.bib1)）数据集上训练模型。Wang 等（[2022b](#bib.bib42)）；Kamath
    等（[2023](#bib.bib16)）提出了在数据集上训练的模仿学习模型，并利用增强的指令提高导航性能。最近，Wang 等（[2023](#bib.bib43)）提出了一种导航代理，能够不仅执行人工编写的导航命令，还能向人类提供路线描述。这些方法局限于少数数据集，并且训练过程繁琐。相比之下，我们的方法可以在多种指令风格和多种模拟平台上进行泛化，无需数据集。
- en: C.2 LLMs for Embodied Robot Navigation
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 具身机器人导航中的大型语言模型
- en: Vision-and-Language Navigation (VLN) has been a popular task in Embodied AI,
    with several pre-LLM era approaches using BERT features, such as VLN-BERT Hong
    et al. ([2021](#bib.bib12)); Zhang and Kordjamshidi ([2023](#bib.bib46)), VilBERT
    Lu et al. ([2019](#bib.bib26)), and Airbert Guhur et al. ([2021](#bib.bib11)).
    Recent work has used LLMs being for this task Huang et al. ([2022a](#bib.bib13));
    Zhou et al. ([2023a](#bib.bib48)), especially in a zero-shot setting Yu et al.
    ([2023](#bib.bib45)); Dorbala et al. ([2022](#bib.bib8)). While Shah et al. ([2023](#bib.bib38))
    leverage GPT-3.5 Brown et al. ([2020](#bib.bib3)) to identify landmarks, Zhou
    et al. ([2023b](#bib.bib49)) and Dorbala et al. ([2023](#bib.bib7)) use an LLM
    for commonsense reasoning between objects and targets to facilitate navigation.
    With LLMs being increasingly used in several embodied AI frameworks beyond navigation
    Mu et al. ([2023](#bib.bib27)); Huang et al. ([2022b](#bib.bib14)), utilizing
    them for instruction generation allows for easier integration and testing at a
    system level. Finally, March-in-Chat (MiC) Qiao et al. ([2023](#bib.bib34)) can
    talk to the LLM on the fly and plan the navigation trajectory dynamically.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉与语言导航（VLN）一直是具身人工智能中的一个热门任务，在LLM（大规模语言模型）之前，有几个方法使用了BERT特征，比如VLN-BERT Hong
    et al. ([2021](#bib.bib12))；Zhang和Kordjamshidi ([2023](#bib.bib46))，VilBERT Lu
    et al. ([2019](#bib.bib26))，以及Airbert Guhur et al. ([2021](#bib.bib11))。最近的工作则采用了LLM进行这项任务，如Huang
    et al. ([2022a](#bib.bib13))；Zhou et al. ([2023a](#bib.bib48))，特别是在零样本设置中，Yu et
    al. ([2023](#bib.bib45))；Dorbala et al. ([2022](#bib.bib8))。Shah et al. ([2023](#bib.bib38))
    利用GPT-3.5 Brown et al. ([2020](#bib.bib3)) 来识别地标，而Zhou et al. ([2023b](#bib.bib49))
    和Dorbala et al. ([2023](#bib.bib7)) 则使用LLM进行对象与目标之间的常识推理以促进导航。随着LLM在多个具身AI框架中被越来越多地使用，Mu
    et al. ([2023](#bib.bib27))；Huang et al. ([2022b](#bib.bib14))，将它们用于指令生成，使得系统级的集成和测试更加便捷。最后，March-in-Chat
    (MiC) Qiao et al. ([2023](#bib.bib34)) 可以即时与LLM对话并动态规划导航轨迹。
