- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:53:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:53:38'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的MOFs合成条件提取使用少样本示例
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.04665](https://ar5iv.labs.arxiv.org/html/2408.04665)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.04665](https://ar5iv.labs.arxiv.org/html/2408.04665)
- en: Lei Shi Lei Shi (shijim@gmail.com) and Zhimeng Liu contribute equally in this
    work. Yue Zhang (yue.zhang@wias.org.cn) and Ge Wang (gewang@ustb.edu.cn) are corresponding
    authors. Beihang University Zhimeng Liu University of Science and Technology Beijing
    Yi Yang Beihang University Weize Wu Beihang University Yuyang Zhang Beihang University
    Hongbo Zhang Westlake University Jing Lin University of Science and Technology
    Beijing Siyu Wu Beihang University Zihan Chen Beihang University Ruiming Li Beihang
    University Nan Wang Beihang University Zipeng Liu Beihang University Huobin Tan
    Beihang University Hongyi Gao University of Science and Technology Beijing Yue
    Zhang Westlake University Ge Wang University of Science and Technology Beijing
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Lei Shi Lei Shi (shijim@gmail.com) 和 Zhimeng Liu 在本工作中贡献均等。Yue Zhang (yue.zhang@wias.org.cn)
    和 Ge Wang (gewang@ustb.edu.cn) 是通讯作者。北京航空航天大学 Zhimeng Liu 北京科技大学 Yi Yang 北京航空航天大学
    Weize Wu 北京航空航天大学 Yuyang Zhang 北京航空航天大学 Hongbo Zhang 西湖大学 Jing Lin 北京科技大学 Siyu
    Wu 北京航空航天大学 Zihan Chen 北京航空航天大学 Ruiming Li 北京航空航天大学 Nan Wang 北京航空航天大学 Zipeng Liu
    北京航空航天大学 Huobin Tan 北京航空航天大学 Hongyi Gao 北京科技大学 Yue Zhang 西湖大学 Ge Wang 北京科技大学
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The extraction of Metal-Organic Frameworks (MOFs) synthesis conditions from
    literature text has been challenging but crucial for the logical design of new
    MOFs with desirable functionality. The recent advent of large language models
    (LLMs) provides disruptively new solution to this long-standing problem and latest
    researches have reported over 90% F1 in extracting correct conditions from MOFs
    literature. We argue in this paper that most existing synthesis extraction practices
    with LLMs stay with the primitive zero-shot learning, which could lead to downgraded
    extraction and application performance due to the lack of specialized knowledge.
    This work pioneers and optimizes the few-shot in-context learning paradigm for
    LLM extraction of material synthesis conditions. First, we propose a human-AI
    joint data curation process to secure high-quality ground-truth demonstrations
    for few-shot learning. Second, we apply a BM25 algorithm based on the retrieval-augmented
    generation (RAG) technique to adaptively select few-shot demonstrations for each
    MOF’s extraction. Over a dataset randomly sampled from 84,898 well-defined MOFs,
    the proposed few-shot method achieves much higher average F1 performance (0.93
    vs. 0.81, +14.8%) than the native zero-shot LLM using the same GPT-4 model, under
    fully automatic evaluation that are more objective than the previous human evaluation.
    The proposed method is further validated through real-world material experiments:
    compared with the baseline zero-shot LLM, the proposed few-shot approach increases
    the MOFs structural inference performance ($R^{2}$) by 29.4% in average.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从文献文本中提取金属有机框架（MOFs）合成条件一直具有挑战性，但对于逻辑设计具有理想功能的新MOFs至关重要。近期大型语言模型（LLMs）的出现为这一长期问题提供了颠覆性的新解决方案，最新研究报告显示从MOFs文献中提取正确条件的F1值超过90%。我们在本文中指出，大多数现有的LLMs合成提取实践仍停留在原始的零样本学习阶段，这可能由于缺乏专业知识而导致提取和应用性能下降。本研究开创并优化了LLM材料合成条件的少样本上下文学习范式。首先，我们提出了一种人机联合数据策划流程，以确保高质量的真实示例用于少样本学习。其次，我们应用基于检索增强生成（RAG）技术的BM25算法，针对每个MOF的提取自适应选择少样本示例。在从84,898个定义明确的MOFs中随机抽样的数据集上，所提少样本方法在完全自动评估下（比以往的人为评估更客观）比使用相同GPT-4模型的原生零样本LLM取得了更高的平均F1性能（0.93
    vs. 0.81，+14.8%）。通过实际材料实验进一步验证了所提方法：与基准零样本LLM相比，所提少样本方法将MOFs结构推断性能（$R^{2}$）提高了29.4%。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Metal-Organic Frameworks (MOFs), a class of high performance porous material,
    have been widely applied to catalysis, gas storage, and groundwater remediation
    [[5](#bib.bib5)] for its prestige in structural tunability and functional versatility
    [[2](#bib.bib2)]. These advantages are deeply rooted in the flexible yet logical
    synthesis configuration of MOFs. Herein, precise and comprehensive knowledge of
    MOFs synthesis conditions becomes extremely important to fully understand its
    structural mechanism and discover new MOFs or sub-types, posing a fundamental
    challenge to the whole discipline of MOFs and reticular chemistry [[23](#bib.bib23)].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 金属有机框架（MOFs）是一类高性能多孔材料，因其在结构可调性和功能多样性方面的声誉，已广泛应用于催化、气体储存和地下水修复[[5](#bib.bib5)]。这些优点深深植根于MOFs灵活而合理的合成配置中。因此，对MOFs合成条件的准确和全面了解对于充分理解其结构机制并发现新的MOFs或亚型变得极为重要，这对MOFs及其网状化学的整个学科提出了基本挑战[[23](#bib.bib23)]。
- en: Currently, there have been 100k+ MOFs successfully synthesized in the laboratory.
    Their detailed synthesis conditions are often recorded by academic literature
    in various textual or tabular formats. Machine learning methods, in particular,
    text mining algorithms, are normally applied to the literature text to automatically
    extract synthesis conditions. However, the complexity and volatility of free text
    limits the accuracy of synthesis condition extraction [[13](#bib.bib13)], which
    could jeopardize the effectiveness of downstream material applications over extracted
    synthesis data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，实验室已经成功合成了超过100,000种MOFs。这些MOFs的详细合成条件通常通过各种文本或表格格式记录在学术文献中。机器学习方法，特别是文本挖掘算法，通常应用于文献文本中以自动提取合成条件。然而，自由文本的复杂性和波动性限制了合成条件提取的准确性[[13](#bib.bib13)]，这可能危害到基于提取数据的下游材料应用的有效性。
- en: The emergence of large language models (LLMs) to some extent resolves the problem
    of synthesis condition extraction from disparate forms of scientific texts, due
    to their well-known expertise in the whole-spectrum of text mining tasks [[3](#bib.bib3)].
    Recently, Zheng et al. [[24](#bib.bib24)], Dagdelen et al. [[7](#bib.bib7)], Polak
    and Morgan [[16](#bib.bib16)] have applied zero-shot or fine-tuned LLMs to extract
    synthesis conditions from experimental MOFs literature. They reported extraction
    performance of close to 0.9 in F1 metric, but mostly over small datasets and evaluated
    by subjective evaluations. It should be pointed out that the baseline zero-shot
    LLMs are notorious for their poor performance on sparse scenarios like MOFs synthesis,
    which are infrequently covered by the general-purpose LLM training data [[6](#bib.bib6)].
    Therefore, evaluating the MOFs condition extraction performance with large-scale,
    real-life datasets become crucial for improving both the quantity and quality
    of MOFs synthesis knowledgebase. In addition, guided material experiments over
    extracted synthesis conditions, which are rarely conducted in previous works,
    should also be an important norm to evaluate the effectiveness of targeted synthesis
    condition extraction task.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现在一定程度上解决了从不同形式的科学文本中提取合成条件的问题，因其在全谱文本挖掘任务中的专业能力[[3](#bib.bib3)]。最近，郑等人[[24](#bib.bib24)]、Dagdelen等人[[7](#bib.bib7)]、Polak和Morgan[[16](#bib.bib16)]将零样本或微调的LLMs应用于从实验MOFs文献中提取合成条件。他们报告了接近0.9的F1度量提取性能，但大多是在小数据集上进行的，并通过主观评估进行评估。应该指出的是，基线零样本LLMs以其在如MOFs合成这样稀疏场景中的较差表现而闻名，这些场景在通用LLM训练数据中很少涉及[[6](#bib.bib6)]。因此，使用大规模的真实数据集评估MOFs条件提取性能对于提升MOFs合成知识库的数量和质量至关重要。此外，对提取的合成条件进行有指导的材料实验，这在以往的工作中很少进行，也应成为评估目标合成条件提取任务效果的重要规范。
- en: In this work, we set out to overcome the notable limitations when applying primitive
    zero-shot LLMs to the problem of MOFs synthesis condition extraction from scientific
    texts. The main theme of this paper is to introduce the few-shot in-context learning
    paradigm as the standard approach to augment general-purpose LLMs on the material
    synthesis condition extraction problem. As shown by our experiment results of
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations"), in a dataset randomly sampled from
    84,898 well-defined MOFs, the proposed few-shot method achieves much higher average
    F1 performance (0.93 vs. 0.81, +14.8%) than the native zero-shot LLMs, both using
    the state-of-the-art GPT-4 Turbo model^†^††The latest GPT-4v model has enhanced
    video and image analysis capability, but not for text analysis. [[1](#bib.bib1)],
    as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations").
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们致力于克服在将原始零样本LLM应用于MOFs合成条件提取时遇到的显著限制。本文的主要主题是将少样本上下文学习范式引入作为增强通用LLM在材料合成条件提取问题上的标准方法。正如图[1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ 基于LLM的MOFs合成条件提取使用少样本示例")中的实验结果所示，在从84,898个定义良好的MOFs中随机抽样的数据集中，所提出的少样本方法在F1平均性能（0.93对0.81，+14.8%）上明显优于原生零样本LLM，两者都使用了最先进的GPT-4
    Turbo模型^†^††最新的GPT-4v模型增强了视频和图像分析能力，但不适用于文本分析。[[1](#bib.bib1)]，如图[1](#S1.F1 "图
    1 ‣ 1 介绍 ‣ 基于LLM的MOFs合成条件提取使用少样本示例")所示。
- en: Nevertheless, deploying few-shot LLMs to solve the current problem still faces
    multiple nontrivial challenges. First, the superiority of few-shot LLMs depends
    on the data quality of their ground-truth demonstrations. In the scenario of MOFs
    synthesis extraction, obtaining ground-truth textual conditions scattered in scientific
    literature in numerous formats remains a daunting task. It would be extremely
    costly to apply traditional human annotation approach given that a change of material
    would require a totally new demonstration dataset. Second, the quantity of ground-truth
    demonstrations selected for each LLM extraction is also critical as high-performance
    LLMs are mostly commercial and charged by input size. For example, the fine-tuning
    technology is known to greatly improve the LLM performance [[7](#bib.bib7)], but
    will normally require hundreds of examples and a locally-stored large set of model
    weights. The application overheads to new synthesis extraction scenarios are quite
    high, thus reducing the adaptability of fine-tuning methods. In our case, minimizing
    the number of few-shot demonstrations would require an elaborate algorithm to
    select the demonstrations adaptively for each MOF’s raw synthesis text.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，部署少样本LLM（大语言模型）来解决当前的问题仍然面临多个非平凡的挑战。首先，少样本LLM的优越性依赖于其真实示例的数据质量。在MOFs（多孔有机框架）合成提取的场景中，从科学文献中获得分散在各种格式中的真实文本条件仍然是一项艰巨的任务。鉴于材料变化会需要全新的示例数据集，采用传统的人力标注方法成本极高。其次，为每个LLM提取选择的真实示例的数量也至关重要，因为高性能LLM大多数是商业化的，并按输入大小收费。例如，微调技术已知可以大幅提升LLM性能[[7](#bib.bib7)]，但通常需要数百个示例和一个本地存储的大量模型权重集。应用于新合成提取场景的开销相当高，从而降低了微调方法的适应性。在我们的案例中，最小化少样本示例的数量将需要一个复杂的算法，以自适应地选择每个MOF原始合成文本的示例。
- en: 'In this paper, we introduce two new methods to resolve the above challenges.
    First, on the preparation of ground-truth demonstrations, to our surprise, human
    annotation and AI annotation show complementary advantages, not only in the annotation
    cost, but also in their output data quality. We then propose a human-AI joint
    data curation process, which enjoys the best of both worlds and offers the highest
    data quality in ground-truth demonstrations produced. Second, based on the popular
    retrieval-augmented generation (RAG) technique, we propose to apply the BM25 algorithm
    to adaptively select the best combination of few-shot demonstrations for each
    MOF’s synthesis extraction, whose performance significantly outruns the baseline
    random selection method. Our experiment results also suggest the most appropriate
    number of demonstrations for the trade-off between performance and cost. It is
    shown that a small overhead of 4-shots could already achieve the optimal performance,
    contrasting to tens to a hundred shots in other domains. In addition, we study
    the utility of different kinds of knowledge on our task when incorporated by LLM:
    the background knowledge on retrieved synthesis conditions, their application
    constraints on the numerical/textual format, and the few-shot demonstrations.
    Notably, the few-shot examples are shown to be the most critical. To our knowledge,
    we are the first to apply and optimize few-shot in-context learning LLM methods
    for the material synthesis condition extraction problem from scientific text.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了两种新方法来解决上述挑战。首先，在真实数据示例的准备上，令我们惊讶的是，人类标注和AI标注展现了互补的优势，不仅在标注成本上，还在其输出数据质量上。我们提出了一种人类-AI联合数据策划过程，充分利用了两者的优势，提供了最高质量的真实数据示例。其次，基于流行的检索增强生成（RAG）技术，我们建议应用BM25算法来自适应选择每个MOF合成提取的最佳少量示例组合，其性能显著优于基线的随机选择方法。我们的实验结果还表明，在性能与成本之间的权衡中，最合适的示例数量是4-shot，远远少于其他领域的几十到上百个示例。此外，我们研究了LLM在任务中结合不同类型知识的效用：检索的合成条件的背景知识、其在数值/文本格式上的应用约束，以及少量示例。值得注意的是，少量示例被证明是最关键的。据我们所知，我们是第一个将少量上下文学习LLM方法应用于从科学文本中提取材料合成条件问题并进行优化的。
- en: '![Refer to caption](img/923c6735402c7f65318d282f31a3ebf5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/923c6735402c7f65318d282f31a3ebf5.png)'
- en: 'Figure 1: Key indicators (F1, ACC, Precision, Recall) of the synthesis condition
    extraction performance on 123 MOFs with ground-truth data: (a) our 4-shot RAG
    algorithm; (b) zero-shot LLM as the baseline; (c) confusion matrix definition
    for evaluation.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：在123种MOF上使用真实数据的合成条件提取性能的关键指标（F1、ACC、精准度、召回率）：（a）我们的4-shot RAG算法；（b）作为基线的零-shot
    LLM；（c）用于评估的混淆矩阵定义。
- en: Moreover, we have considered the scalability issue for high-throughput synthesis
    extraction. The additional overhead includes the labor cost to acquire external
    knowledge (e.g., expert annotations on the literature text), the financial cost
    to request LLM APIs, and the computational cost to potentially train in-house
    LLMs. For example, by the latest GPT-4 pricing model (10$ per 1M tokens), a single
    pass over all the 100k available MOFs synthesis literature (est. 10k words per
    literature) sums up to a non-negligible cost of 10k$, while performance tuning
    normally requires several passes. Three techniques adapted to large-scale material
    data are proposed. First, we learn an offline model to detect the most relevant
    synthesis paragraphs out of each literature, with an overall accuracy of 98.9%.
    In this way, the financial cost in using commercial LLMs is reduced by 94% (the
    average word count of a literature and its synthesis paragraphs are est. 15,000
    and 900, respectively). The fully-tuned high-throughput synthesis extraction workflow
    now processes over 500 millions of scientific texts from all available MOFs literature
    within 7 hours. Second, we conduct experiments to quantify the size of demonstration
    pool as material data scales. Though it is shown that larger example pools almost
    always contribute to the performance, the margin quickly drops as more annotations
    are available. In the extreme, a pool of size $K$) LLMs is the most cost-effective.
    Third, we develop a LLM-based coreference resolution method to restore proxy words
    like “L” or “H2L” into its entirety. Though only a small portion of extracted
    synthesis condition on organic linker suffer from the use of proxy words, it mounts
    to a big number and affects the downstream material tasks on large-scale data.
    By applying our method, only 2.3% linkers remain unresolved.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们已经考虑了高通量合成提取的可扩展性问题。额外的开销包括获取外部知识的劳动成本（例如，专家对文献文本的注释）、请求LLM API的财务成本，以及可能需要训练内部LLM的计算成本。例如，根据最新的GPT-4定价模型（每百万个令牌10美元），对所有100k个可用MOFs合成文献（每篇文献估计10k个词）进行单次处理的成本高达10k美元，而性能调优通常需要多次处理。提出了三种适应大规模材料数据的技术。首先，我们学习一个离线模型，从每篇文献中检测最相关的合成段落，整体准确率为98.9%。通过这种方式，使用商业LLM的财务成本减少了94%（文献及其合成段落的平均字数分别为15,000和900）。现在，完全调优的高通量合成提取工作流程可以在7小时内处理来自所有可用MOFs文献的超过5亿篇科学文本。其次，我们进行实验以量化示范池的规模随材料数据扩展的变化。尽管显示较大的示例池几乎总是有助于性能，但随着更多注释的可用，收益迅速下降。在极端情况下，大小为$K$的池是最具成本效益的。第三，我们开发了一种基于LLM的共指消解方法，将诸如“L”或“H2L”等代理词恢复为其完整形式。尽管仅有一小部分提取的有机连接体的合成条件因使用代理词而受到影响，但数量庞大，并影响大规模数据的下游材料任务。通过应用我们的方法，仅有2.3%的连接体未得到解决。
- en: To validate the importance of our proposal, we set up a real-world MOFs synthesis-structure
    inference experiment, in which the proposed few-shot LLM method is compared with
    the existing LLM application in material synthesis extraction scenario (e.g.,
    zero-shot[[24](#bib.bib24)][[16](#bib.bib16)]). On a set of 5,269 MOFs curated
    from the CSD database, we manage to build machine learning models to predict MOFs
    microstructure properties (framework density, cavity diameter, etc.) with the
    synthesis conditions obtained by LLM. By 6 off-the-shelf machine learning models,
    the inference performance ($R^{2}$) by the proposed few-shot method is consistently
    higher than the benchmark zero-shot approach, with an average improvement of 29.4%.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证我们提议的重要性，我们设置了一个真实世界的MOFs合成-结构推断实验，其中将提出的少样本LLM方法与现有的材料合成提取场景中的LLM应用（例如，零样本[[24](#bib.bib24)][[16](#bib.bib16)]）进行了比较。在从CSD数据库中整理出的5,269个MOFs的数据集上，我们成功建立了机器学习模型，以LLM获取的合成条件预测MOFs微观结构属性（框架密度、孔径等）。通过6种现成的机器学习模型，提出的少样本方法的推断性能（$R^{2}$）始终高于基准零样本方法，平均提升了29.4%。
- en: We also make available an online visual database showing the extracted synthesis
    conditions of all the 36,177 MOFs with literature available from the CSD database
    (see the supplemental material for more details).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个在线可视化数据库，展示了CSD数据库中所有36,177个MOFs的提取合成条件（有关更多细节，请参见补充材料）。
- en: 2 Results
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 结果
- en: '![Refer to caption](img/ebfd14548bd2aee89ab2e8950b8bf498.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ebfd14548bd2aee89ab2e8950b8bf498.png)'
- en: 'Figure 2: The overall pipeline of the few-shot in-context learning method for
    synthesis condition extraction from MOFs literature.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：少样本上下文学习方法用于从 MOFs 文献中提取合成条件的整体流程。
- en: 'As shown in our technology pipeline of Figure [2](#S2.F2 "Figure 2 ‣ 2 Results
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations"),
    the MOFs literature dataset are first collected and pre-processed into compatible
    input format for LLMs (see Sec. [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations") for details).
    The latest high-performance LLM (i.e., GPT-4) is employed to extract 10 essential
    conditions for the synthesis of each MOF: metal precursor name & amount, organic
    linker name & amount, solvent name & amount, modulator name & amount, and synthesis
    reaction duration & temperature. The synthesis extraction result is first evaluated
    on their literal accuracy with respect to an expert-curated ground-truth dataset,
    and then tested on the real-world scenarios of material structure inference and
    design. On the randomly sampled 123 MOFs synthesis literature from all the 36177
    MOFs, the extraction of 1230 synthesis conditions using the proposed few-shot
    LLM model achieves a best average F1 metric of 0.93 (ACC = 0.90), using a few-shot
    demonstration of only 4 examples. The full performance result is illustrated in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")(a), in comparison to the baseline zero-shot
    approach with an average F1 of 0.81 (ACC = 0.77) in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
    Demonstrations")(b). The dataset statistics is listed in Figure [4](#S2.F4 "Figure
    4 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在图 [2](#S2.F2 "Figure 2 ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations") 的技术流程图中所示，MOFs 文献数据集首先被收集并预处理为适合 LLM
    的输入格式（详情见第 [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")节）。最新的高性能 LLM（即 GPT-4）被用于提取每种
    MOF 合成的 10 个关键条件：金属前驱体名称及数量、有机配体名称及数量、溶剂名称及数量、调节剂名称及数量，以及合成反应的持续时间和温度。合成提取结果首先在其字面准确性上进行评估，基于专家整理的真实数据集，然后在材料结构推断和设计的实际场景中进行测试。在从
    36177 种 MOFs 中随机抽样的 123 种 MOFs 合成文献中，使用提出的少样本 LLM 模型提取的 1230 个合成条件在仅用 4 个示例的少样本演示下，达到了最佳平均
    F1 值 0.93（ACC = 0.90）。完整的性能结果见图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(a)，与基线零样本方法的平均
    F1 值 0.81（ACC = 0.77）相比，结果如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b) 所示。数据集统计信息列于图
    [4](#S2.F4 "Figure 4 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge
    ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")。
- en: 2.1 Human-AI Joint Data Curation
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 人工智能联合数据策展
- en: '![Refer to caption](img/f0b9405bec6e64faecdbb7599e3b4911.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f0b9405bec6e64faecdbb7599e3b4911.png)'
- en: 'Figure 3: The human-AI reflection procedure to improve few-shot examples.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：人类与 AI 反思程序以改进少样本示例。
- en: 'To introduce the few-shot LLM method, a prerequisite is to obtain a high-quality
    demonstration pool on the synthesis condition extraction task, i.e., the ground-truth
    annotations. Traditionally, human annotations are the sole means to collect these
    examples for the few-shot learning. In this work, we also start with a standard
    annotation protocol which includes three steps: 1) *pilot annotations* on 20 typical
    literature by the leading experts to reach consensus on the rigorous format of
    MOFs synthesis conditions; 2) *batch annotations* conducted by 6 experts over
    180 MOFs synthesis paragraphs randomly chosen from the entire dataset. Each paragraph
    is double annotated by two experts to ensure reliability; 3) *finalized annotations*
    by only keeping the MOFs synthesis conditions that are agreeing between the two
    experts, while removing annotated paragraphs that are inappropriate as examples
    (e.g., having more than one suite of MOFs synthesis conditions in the same paragraph).
    Eventually, we obtain a ground-truth human annotation dataset composed of 147
    suites of MOFs synthesis conditions. The full detail of our annotation approach
    and an online software to assist the process is described in Sec. [5.2](#S5.SS2
    "5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions ‣
    5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 引入少样本LLM方法的前提是获得高质量的演示池用于合成条件提取任务，即真实标注。传统上，人类标注是收集这些少样本学习示例的唯一手段。在这项工作中，我们也从一个标准的标注协议开始，该协议包括三个步骤：1)
    对20篇典型文献进行*试点标注*，由领先的专家达成关于MOFs合成条件严格格式的共识；2) 由6位专家对从整个数据集中随机选择的180篇MOFs合成段落进行*批量标注*。每个段落由两位专家进行双重标注以确保可靠性；3)
    通过仅保留两位专家一致的MOFs合成条件来进行*最终标注*，同时去除那些不适合作为示例的标注段落（例如，在同一段落中包含多个MOFs合成条件套件）。最终，我们获得了由147套MOFs合成条件组成的真实人类标注数据集。我们的标注方法的详细信息和辅助过程的在线软件描述见第[5.2](#S5.SS2
    "5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions ‣
    5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")节。
- en: Using the human annotations developed above as examples, the performance of
    few-shot LLM models is depicted by the solid orange+triangle lines of Figure [6](#S2.F6
    "Figure 6 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").
    The average F1 metric rises from 0.81 (zero-shot) to the peak of 0.86 ($K=2$,
    both metrics drop. The explanation might be that more information without ground-truth
    distracts the LLM, rather than coaches it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述开发的人类标注作为示例，少样本LLM模型的性能通过图[6](#S2.F6 "Figure 6 ‣ 2.2 Few-Shot Large Language
    Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")中的实线橙色+三角形线条展示。平均F1指标从0.81（零样本）上升到0.86的峰值（$K=2$），然后两个指标都下降。解释可能是因为更多的信息而没有真实标注会使LLM分心，而不是帮助它。
- en: The above results indicate that neither human annotations nor purely AI-generated
    examples achieve optimal data quality for LLM few-shot learning. To delve deeper
    into the issue, several leading MOF experts were consulted to evaluate all errors
    produced by the few-shot LLM method when using human annotations as the sole examples
    and ground-truths. Out of 261 potential errors, 103 LLM outputs (39.5%) were identified
    as correct, 38 (14.6%) had certain issues but contributed to refining the corresponding
    ground-truth, and only 120 (45.9%) were true errors. The experts then compiled
    a revised set of ground-truth annotations, including the synthesis conditions
    for 123 individual MOFs. The remaining 23 annotated conditions were deemed inappropriate
    because they either involved chiral MOFs with duplicate synthesis conditions and
    paragraphs or contained multiple MOF synthesis processes within a single paragraph.
    Although our technical framework can deal with the case of having multiple MOFs
    in a single paragraph, we chose the paragraphs describing the synthesis of only
    one MOF for more precise demonstrations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果表明，既没有人工标注也没有纯AI生成的示例能实现LLM少量学习的最佳数据质量。为了*深入探讨*这一问题，我们咨询了几位领先的MOF专家，以评估当仅使用人工标注作为示例和真实数据时，少量LLM方法产生的所有错误。在261个潜在错误中，103个LLM输出（39.5%）被识别为正确，38个（14.6%）存在一定问题但有助于完善相应的真实数据，只有120个（45.9%）是真正的错误。然后，专家们编制了一套修订的真实数据标注，包括123个个体MOF的合成条件。其余23个标注条件被认为不合适，因为它们涉及到具有重复合成条件和段落的手性MOF，或在单一段落中包含多个MOF合成过程。尽管我们的技术框架可以处理单一段落中包含多个MOF的情况，但我们选择了仅描述一种MOF合成的段落以获得更精确的示例。
- en: With this empirical experience, we propose a human-AI joint data curation process
    for the data quality optimization of ground-truth demonstrations in LLM-based
    few-shot learning paradigm. As shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Human-AI
    Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations")(a), raw synthesis paragraphs are first processed
    by LLM in a zero-shot mode. Human experts then work on the initial AI annotation
    to achieve a best-effort human annotation (Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Human-AI
    Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations")(b)), which is the first round of reflection. After
    that, these human annotations are used as demonstrations in a LLM-based few-shot
    synthesis condition extraction Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Human-AI Joint
    Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using
    Few-Shot Demonstrations")(c), which is the second round of reflection and generates
    few-shot AI annotations. Lastly, human experts combines human annotations and
    few-shot AI annotations into the human-AI joint annotation (Figure [3](#S2.F3
    "Figure 3 ‣ 2.1 Human-AI Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(d)), in the final round of
    reflection. We apply the few-shot LLM model over the final demonstrations with
    the highest-level of data quality. The best performance (F1=0.93 and ACC=0.9)
    is achieved at the point of most appropriate few-shot quantity ($K=4$, as shown
    by the solid blue lines in Figure [6](#S2.F6 "Figure 6 ‣ 2.2 Few-Shot Large Language
    Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations").
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些经验，我们提出了一种人机联合数据整理流程，以优化LLM基础的少量示例学习范式中的真实数据展示的质量。如图[3](#S2.F3 "Figure 3
    ‣ 2.1 Human-AI Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")(a)所示，原始合成段落首先由LLM在零-shot模式下处理。然后，人工专家对初步的AI标注进行处理，以实现最佳的人类标注（图[3](#S2.F3
    "Figure 3 ‣ 2.1 Human-AI Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(b)），这是第一次反思。之后，这些人工标注被用作LLM基础的少量示例合成条件提取中的示例（图[3](#S2.F3
    "Figure 3 ‣ 2.1 Human-AI Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(c)），这是第二次反思，生成少量的AI标注。最后，人工专家将人工标注和少量AI标注结合成最终的人机联合标注（图[3](#S2.F3
    "Figure 3 ‣ 2.1 Human-AI Joint Data Curation ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(d)），这是最终的反思轮次。我们将少量LLM模型应用于数据质量最高的最终展示。最佳性能（F1=0.93
    和 ACC=0.9）是在最合适的少量示例数量（$K=4$，如图[6](#S2.F6 "Figure 6 ‣ 2.2 Few-Shot Large Language
    Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")中的实线蓝色线条所示）下达到的。
- en: 'We ascribe the superiority of human-AI joint data curation to three reasons,
    all due to the complementary nature of human expertise and AI’s capacity. First,
    though human are excellent in flexible usage of material knowledge, they often
    fail to strictly follow pre-defined annotation rules. For example, to standardize
    the solvent condition, it is required to leave out all modifiers of a common solvent.
    Human annotators sometimes extract “hot water” instead of “water”, because his/her
    focus is on the knowledge extraction and neglects the rules. Human are poor multi-objective
    task executors compared with AI, who will not introduce error if only these rules
    are provided in either background prompt or examples. Second, human often suffer
    from fatigue issue when working with a large set of annotation tasks as ours.
    Random errors are then generated, i.e., missing or adding a few characters/words.
    Though redundant annotation by more than one expert can eliminate these random
    errors, it is at the cost of excluding many useful annotations when the redundant
    outputs are different. Here AI is applied to alleviate this issue: an initial
    zero-shot LLM annotation reduces human efforts, their fatigue, and the resulting
    random errors in the first round of reflection; the few-shot LLM output also works
    as a caliber to help resolve differences between redundant human annotations in
    the second round of reflection.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将人类与 AI 联合数据整理的优越性归因于三个原因，这些原因都归功于人类专业知识与 AI 能力的互补性。首先，虽然人类在灵活使用材料知识方面表现出色，但他们往往无法严格遵循预定义的注释规则。例如，为了标准化溶剂条件，要求排除所有常见溶剂的修饰词。人类注释者有时会提取“热水”而不是“水”，因为他们的关注点在于知识提取，忽视了规则。与
    AI 相比，人类在多目标任务执行方面较差，如果只在背景提示或示例中提供这些规则，AI 将不会引入错误。其次，人类在处理大量注释任务时常常受到疲劳问题的困扰。随机错误因此产生，即遗漏或添加一些字符/词汇。尽管通过多个专家的冗余注释可以消除这些随机错误，但当冗余输出不同时，必须排除许多有用的注释。这里应用
    AI 来缓解这一问题：初始的零-shot LLM 注释减少了人类的工作量、疲劳感和第一次反思中产生的随机错误；少-shot LLM 输出也作为一种标准，帮助解决第二次反思中冗余人类注释之间的差异。
- en: Finally, on the other hand, the current general-purpose LLM alone is not the
    ultimate solution to our task. According to the medium performance of zero-shot
    LLM, it lacks specialized knowledge on MOFs synthesis conditions. Though the few-shot
    demonstrations mitigate this deficiency through in-context learning, the scalability
    issue makes it very hard to achieve a close to 100% accuracy. For example, retrieving
    one synthesis condition from a paragraph may require several demonstrations to
    cover all the lexical and syntactic pattern around the target condition. Retrieving
    all 10 conditions then demands tens of examples, inducing a cost magnitudes more
    than the current setting of $K=4$. Customized few-shot algorithms will be needed
    to achieve such goal. Therefore, in the final round of reflection, human experts
    are hired to generate the best-quality ground-truth demonstrations over existing
    human-AI efforts.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一方面，当前的通用 LLM 单独并不是我们任务的**终极解决方案**。根据零-shot LLM 的中等性能，它在 MOFs 合成条件上缺乏专业知识。尽管少-shot
    演示通过上下文学习缓解了这一缺陷，但可扩展性问题使得达到接近 100% 的准确率非常困难。例如，从一段文字中检索一个合成条件可能需要多个演示来覆盖目标条件周围的所有词汇和句法模式。检索所有
    10 个条件则需要数十个示例，从而使成本比当前设置的 $K=4$ 高出几个数量级。将需要定制的少-shot 算法来实现这一目标。因此，在最终的反思阶段，聘请人类专家来生成最佳质量的真实演示，以便对现有的人类-AI
    努力进行补充。
- en: 2.2 Few-Shot Large Language Model with Material Knowledge
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 带有材料知识的少-shot 大语言模型
- en: '![Refer to caption](img/97c7a018b9e04cce270e6543c8c8da88.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/97c7a018b9e04cce270e6543c8c8da88.png)'
- en: 'Figure 4: Optimization of general-purpose LLMs for the MOFs synthesis extraction:
    few-shot in-context learning and RAG.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MOFs 合成提取的通用 LLM 优化：少-shot 上下文学习与 RAG。
- en: Few-shot in-context learning with random examples
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随机示例的少-shot 上下文学习
- en: In the research area of natural language processing (NLP), few-shot in-context
    learning (FS-ICL) [[6](#bib.bib6)] generally refers to one typical learning paradigm
    to adapt the task-agnostic language models to various downstream tasks while achieving
    optimized performance on each task. In more detail, FS-ICL takes a few prompted
    examples as input (known as shots), each composed of a context and a labeled completion,
    in addition to background prompts such as task description (Figure [4](#S2.F4
    "Figure 4 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")).
    In the task of MOFs synthesis extraction for instance, a context refers to a paragraph
    containing all the synthesis conditions of a MOF and the labeled completion refers
    to the ground-truth synthesis conditions annotated and curated by human experts
    in our work. The top-right part of Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Few-Shot
    Large Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations") gives an example of the labeled
    completion. FS-ICL is often discussed in comparison to the fine-tuning (FT) paradigm,
    which updates the pre-trained language models by incorporating a set of labeled
    examples via supervised learning. In both FS-ICL and FT, the final prediction
    is made by prompting a new context and asking the language model to complete it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）的研究领域，少量示例的上下文学习（FS-ICL）[[6](#bib.bib6)] 通常指一种典型的学习范式，用于将任务无关的语言模型适应到各种下游任务，同时在每个任务上实现优化性能。更具体来说，FS-ICL
    将少量提示示例作为输入（称为“shots”），每个示例由一个上下文和一个标记完成组成，此外还有如任务描述等背景提示（见图 [4](#S2.F4 "Figure
    4 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")）。例如，在MOFs合成提取任务中，上下文指的是包含MOF所有合成条件的段落，标记完成指的是由人工专家在我们工作中注释和整理的真实合成条件。图
    [4](#S2.F4 "Figure 4 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge
    ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")
    右上角部分给出了标记完成的示例。FS-ICL 经常与微调（FT）范式进行比较，后者通过监督学习将一组标记示例纳入来更新预训练语言模型。在 FS-ICL 和
    FT 中，最终的预测都是通过提示一个新的上下文，并要求语言模型完成它来进行的。
- en: The main advantage of FS-ICL vs. FT lies in its versatility to work on many
    tasks (e.g., synthesis extraction of various materials) without the need to re-train
    the model, as shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Few-Shot Large Language
    Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations"). In comparison, FT requires gradient-based
    training for each new task to update the model weights and a considerable number
    of labeled examples for supervision. Though the latest technology of few-shot
    fine-tuning (FS-FT) has reduced the requirement of examples to the same level
    of FS-ICL [[11](#bib.bib11)][[15](#bib.bib15)], training and maintaining a small
    fraction of updated language model weights can still be costly for lightweight
    LLM usage scenarios such as synthesis extraction in this work. FT also suffers
    from spurious correlations due to the overfitting effect [[15](#bib.bib15)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: FS-ICL 相对于 FT 的主要优势在于其在无需重新训练模型的情况下，可以在多个任务（例如各种材料的合成提取）上发挥作用，如图 [4](#S2.F4
    "Figure 4 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")
    所示。相比之下，FT 需要对每个新任务进行基于梯度的训练以更新模型权重，并且需要大量标记示例进行监督。尽管最新的少量示例微调（FS-FT）技术已将示例需求降低到与
    FS-ICL 相同的水平 [[11](#bib.bib11)][[15](#bib.bib15)]，但在轻量级 LLM 使用场景（如本工作中的合成提取）中，训练和维护少量更新的语言模型权重仍然可能会很昂贵。FT
    还因过拟合效应而遭受虚假的相关性 [[15](#bib.bib15)]。
- en: Despite the superiority of FS-ICT in our scenario, the paradigm also draws concerns
    due to its disadvantages. First, the inclusion of few-shots in the prompt brings
    additional computation cost to the language model. In mainstream implementations,
    the number of shots, denoted as $K$ (known as zero-shot). Second, in previous
    studies, the format of prompts in FS-ICT (e.g., the wording and ordering of examples)
    can have unpredictable influence on the final performance. In some cases, FS-ICL
    even performs well on incorrect examples. We have investigate these issues and
    demonstrate that our approach can achieve very low variance by fixing the prompt
    format and example orders according to the algorithm. The data quality of few-shot
    examples in our task is also shown to be an important factor to the synthesis
    extraction performance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在我们的场景中FS-ICT具有优势，但该范式也因其缺点而受到关注。首先，提示中包含的少量样本会给语言模型带来额外的计算成本。在主流实现中，样本数量表示为$K$（也称为零样本）。其次，在以前的研究中，FS-ICT中的提示格式（例如示例的措辞和排序）可能对最终性能产生不可预测的影响。在某些情况下，FS-ICL在错误示例上甚至表现良好。我们已经调查了这些问题，并展示了通过根据算法固定提示格式和示例顺序，我们的方法可以实现非常低的方差。我们任务中的少量样本的数据质量也被证明是合成提取性能的重要因素。
- en: It is also previously believed that FT can achieve better performance than FS-ICT,
    but the latest study reveals that under the same size of shots, both paradigms
    obtain similar performance and exhibit large variance depending on the task specification
    [[15](#bib.bib15)]. In our scenario, FS-ICT reaches an excellent performance of
    F10.9, which is enough for real-life deployment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 之前也有人认为FT可以比FS-ICT获得更好的性能，但最新的研究表明，在相同的样本量下，两种范式的性能相似，并且根据任务规格表现出较大的差异[[15](#bib.bib15)]。在我们的场景中，FS-ICT达到了F10.9的优异性能，这足以支持实际部署。
- en: Using RAG to enhance few-shot data quantity and quality
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RAG来提升少量样本的数据量和质量
- en: '![Refer to caption](img/6b1dc0927be3fb4156cd781f85cd6e2d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/6b1dc0927be3fb4156cd781f85cd6e2d.png)'
- en: 'Figure 5: Comparison of different RAG algorithms and their configurations.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同RAG算法及其配置的比较。
- en: '![Refer to caption](img/24652863105bf03a22f07051a1f93c37.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/24652863105bf03a22f07051a1f93c37.png)'
- en: 'Figure 6: The impact of example data quality on extraction performance, with
    varying number of shots.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：示例数据质量对提取性能的影响，样本数量不同。
- en: At the core of FS-ICL approach, we introduce the RAG algorithm which retrieves
    the aforementioned $K$.001 in F1 comparison), showcasing the effectiveness of
    RAG mechanism. On the best BM25 algorithm, we further test the impact of few shots’
    input order as in the LLM prompt. As shown in Figure [5](#S2.F5 "Figure 5 ‣ 2.2
    Few-Shot Large Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(c), the differences
    are not significant among most ordering strategies.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在FS-ICL方法的核心，我们引入了RAG算法，该算法在F1比较中检索到上述$K$.001，展示了RAG机制的有效性。在最佳BM25算法上，我们进一步测试了少量样本输入顺序对LLM提示的影响。如图[5](#S2.F5
    "Figure 5 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge ‣ 2 Results
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(c)所示，大多数排序策略之间的差异不显著。
- en: Here we note that the F1 and ACC (overall accuracy) metrics used follow the
    standard definition computed from TP (true positive), FP (false positive), TN
    (true negative), FN (false negative), throughout this work. The LLM output on
    each synthesis condition of a MOF will be classified into one of TP/FP/TN/FN by
    comparing with the predefined ground-truth annotation, as described in the confusion
    matrix of Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(c). Note that our definition
    is different from the previous research in Zheng et al. [[24](#bib.bib24)] where
    the TP/FP/TN/FN classification is evaluated by human experts case by case. We
    argue that the subjective human evaluation may introduce bias while the fully
    objective classification will ensure a consistent format in retrieved synthesis
    conditions, which is beneficial for the follow-up material applications (see Sec.
    [5.3](#S5.SS3 "5.3 Post-processing of Synthesis Conditions ‣ 5 Appendix ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations") for more details).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在此我们注意到，本文使用的F1和ACC（总体准确率）指标遵循从TP（真正例）、FP（假正例）、TN（真负例）、FN（假负例）计算的标准定义。MOF每种合成条件的LLM输出将通过与预定义的真实标签进行比较，被分类为TP/FP/TN/FN之一，如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 基于LLM的MOFs合成条件提取使用少量示例")(c)的混淆矩阵所示。注意，我们的定义不同于郑等人[[24](#bib.bib24)]的先前研究，其中TP/FP/TN/FN分类是由人工专家逐案评估的。我们认为，主观的人工评估可能会引入偏差，而完全客观的分类将确保检索的合成条件格式的一致性，这有利于后续的材料应用（有关更多细节，请参见第[5.3](#S5.SS3
    "5.3 合成条件的后处理 ‣ 5 附录 ‣ 基于LLM的MOFs合成条件提取使用少量示例")节）。
- en: A key parameter of the RAG algorithm lies in the number of example shots used
    in the LLM prompt, i.e., $K$ increases. However, its performance metrics are consistently
    below the BM25 algorithm, mostly having gaps more than 0.05 on F1\. This result
    again demonstrates the effectiveness of the proposed RAG algorithm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: RAG算法的一个关键参数在于LLM提示中使用的示例数量，即$K$的增加。然而，其性能指标始终低于BM25算法，大多数情况下F1指标差距超过0.05。这一结果再次证明了所提出的RAG算法的有效性。
- en: Material knowledge augmentation via prompt engineering
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提示工程进行材料知识增强
- en: 'In addition to few-shot examples, another way to augment the domain knowledge
    of general-purpose LLM is through the fixed background prompt [[9](#bib.bib9)].
    The previous LLM adaptations on MOFs synthesis extraction by Zheng et al. [[24](#bib.bib24)]
    introduce a preliminary prompt engineering approach, which include the task description
    of MOFs synthesis extraction and the output format specification. In our work,
    based on the latest prompt engineering expertise [[21](#bib.bib21)], we propose
    to further incorporate two types of material knowledge into the background prompt:
    definition of each MOFs synthesis condition, and deterministic constraints on
    each condition’s numerical/textual value or structure (if any). As shown in Figure
    [5](#S2.F5 "Figure 5 ‣ 2.2 Few-Shot Large Language Model with Material Knowledge
    ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(d),
    by integrating the new material knowledge, the F1 metric increases from 0.91 to
    0.93\. However, when the few-shot examples are not incorporated, the background
    material knowledge will not lead to significant improvement by itself.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了少量示例外，另一种增强通用大语言模型（LLM）领域知识的方法是通过固定背景提示[[9](#bib.bib9)]。郑等人对MOFs合成提取的先前LLM适配[[24](#bib.bib24)]介绍了一种初步的提示工程方法，其中包括MOFs合成提取的任务描述和输出格式规范。在我们的工作中，基于最新的提示工程专业知识[[21](#bib.bib21)]，我们建议进一步将两类材料知识融入背景提示中：每种MOFs合成条件的定义，以及对每个条件数值/文本值或结构（如有）的确定性约束。如图[5](#S2.F5
    "图 5 ‣ 2.2 少量示例大语言模型与材料知识 ‣ 2 结果 ‣ 基于LLM的MOFs合成条件提取使用少量示例")(d)所示，通过整合新的材料知识，F1指标从0.91提高到0.93。然而，当未加入少量示例时，背景材料知识本身不会带来显著的改进。
- en: 'The details of newly introduced MOFs synthesis definitions and constraints
    as background prompts are listed in Table [1](#S2.T1 "Table 1 ‣ 2.2 Few-Shot Large
    Language Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations"). Notably, we summarize three
    types of constraints on synthesis conditions: *numerical* that the value of a
    condition should fall into certain range according to prior knowledge, *textual*
    that an extracted condition by text should adhere to certain format to speedup
    follow-up material application, and *structural* that certain rules related to
    the condition are followed in all MOFs synthesis process.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 新引入的MOFs合成定义和约束的详细信息作为背景提示列在表[1](#S2.T1 "Table 1 ‣ 2.2 Few-Shot Large Language
    Model with Material Knowledge ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")中。值得注意的是，我们总结了对合成条件的三种类型的约束：*数值*，即条件的值应根据先前知识落入特定范围；*文本*，即提取的条件应遵循特定格式以加速后续材料应用；*结构*，即所有MOFs合成过程都遵循与条件相关的特定规则。
- en: 'Conditions Definition Constraints by Type Metal Precursor (name & amount) The
    precursor compound(s) containing metal ions … Textual: only include adjectives
    modifying the metal precursor itself… Organic Linker (name & amount) The organic
    precursor linking metal ions or clusters … N/A Solvent (name & amount) The liquid
    medium in which reactants are dissolved … Textual: include “solution” if the solvent
    contains water … Modulator (name & amount) The substance to adjust reaction conditions
    (e.g., pH value) … Structural: the elements of modulator will not become part
    of the backbone of MOF structure … Reaction process (duration & temperature) The
    synthesis process producing MOFs … Numerical: The reaction duration will last
    several minutes to hours …'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 条件定义 按类型约束 金属前驱体（名称和数量） 含有金属离子的前驱体化合物 … 文本：仅包括修饰金属前驱体本身的形容词 … 有机配体（名称和数量） 连接金属离子或簇的有机前驱体
    … 不适用 溶剂（名称和数量） 溶解反应物的液体介质 … 文本：如果溶剂含有水，则包括“溶液” … 调节剂（名称和数量） 调整反应条件的物质（例如，pH值）
    … 结构：调节剂的元素不会成为MOF结构的主链的一部分 … 反应过程（持续时间和温度） 生产MOFs的合成过程 … 数值：反应持续时间从几分钟到几小时不等
    …
- en: 'Structural: Crystallization is not a reaction process …'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结构：结晶不是反应过程 …
- en: 'Table 1: Material knowledge as background prompts: synthesis condition definition
    and numerical/textual/structural constraints.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：材料知识作为背景提示：合成条件定义和数值/文本/结构约束。
- en: 2.3 Optimization for High-Throughput MOFs Synthesis Extraction
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 高通量MOFs合成提取的优化
- en: Though the proposed LLM-based synthesis extraction method achieves state-of-the-art
    performance in our medium-scale validation set, scalability issues arise when
    the method is deployed to high-throughput scenarios involving thousands of real-world
    literature and millions of material texts. The challenges include but not limited
    to the large bill from calling commercial LLM APIs, the high cost to annotate
    enough examples for few-shot learning, and the pragmatic issues in material application.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提出的基于LLM的合成提取方法在我们的中等规模验证集上取得了最先进的性能，但在将该方法应用于涉及数千篇真实文献和数百万材料文本的高通量场景时，仍然会出现可扩展性问题。挑战包括但不限于调用商业LLM
    API的高费用、注释足够示例以进行少量学习的高成本，以及材料应用中的实际问题。
- en: Synthesis paragraph detection
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 合成段落检测
- en: To train the machine learning model for synthesis paragraph detection, we first
    annotate a dataset of 440 papers randomly sampled from the large dataset of Sec.
    [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations"). Details can be accessed in Sec. [5.2](#S5.SS2
    "5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions ‣
    5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").
    Finally, this process yields 1,349 synthesis paragraphs as positive samples. To
    train the classifier, negative samples by non-synthesis paragraphs are obtained
    after removing all annotated paragraphs from a paper, leading to 11,783 negative
    samples. We employed the standard BERT model, specifically the pre-trained bert-base-uncased
    model from HuggingFace, for training. The training and validation processes utilized
    a 5-fold cross-validation method. Given the imbalance dataset, we used stratified
    5-fold cross-validation to ensure that the ratio of positive to negative samples
    remained consistent in each split. The final classification performance is quite
    high, with an ACC of 0.989, precision of 0.955, recall of 0.947, and F1 = 0.951.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练机器学习模型以检测合成段落，我们首先对从第[5.1节](#S5.SS1 "5.1 MOFs数据 ‣ 5附录 ‣ 基于LLM的MOFs合成条件提取使用少样本演示")中随机抽取的大数据集中的440篇论文进行了标注。详细信息可以在第[5.2节](#S5.SS2
    "5.2 合成段落和合成条件的标注程序 ‣ 5附录 ‣ 基于LLM的MOFs合成条件提取使用少样本演示")中获取。最终，这一过程产生了1,349个合成段落作为正样本。为了训练分类器，从每篇论文中去除所有标注段落后获得了负样本，共11,783个负样本。我们使用了标准的BERT模型，具体为HuggingFace的预训练bert-base-uncased模型进行训练。训练和验证过程中采用了5折交叉验证方法。鉴于数据集的不平衡，我们使用了分层5折交叉验证，以确保每个分割中的正负样本比例保持一致。最终分类性能相当高，ACC为0.989，精度为0.955，召回率为0.947，F1为0.951。
- en: Paragraphs related to the synthesis process constitute only about 6% of an article’s
    total length but concentrate the main synthesis condition. Extracting condition
    from synthesized paragraphs, rather than the entire text, can significantly reduce
    the overhead of LLM-based approach and increase the density of synthesis conditions
    in text, thereby enhancing extraction performance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与合成过程相关的段落仅占文章总长度的约6%，但集中体现了主要的合成条件。从合成段落中提取条件，而不是从整个文本中提取，可以显著减少基于LLM的方法的开销，并提高文本中合成条件的密度，从而增强提取性能。
- en: Sizing the few-shot example pool
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本示例池的大小
- en: '![Refer to caption](img/bba32a80b47198b8000db09213c09016.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bba32a80b47198b8000db09213c09016.png)'
- en: 'Figure 7: Synthesis extraction performance with varying sizes of example pool:
    (a) average F1 and its 95% CI of 123-paragraph and 60-paragraph datasets; (b)
    123-paragraph dataset with different $K$-shots.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在不同示例池大小下的合成提取性能：（a）123段落和60段落数据集的平均F1及其95%置信区间；（b）123段落数据集的不同$K$-shots。
- en: In this study, we have discussed both the quantity and quality of few-shot examples.
    Yet, it is still unknown how many ground-truth annotations, namely the example
    pool where few-shots are selected from, are required for high-throughput synthesis
    extraction over thousands of MOFs literature or more. To answer this question,
    we design an experiment that assumes the entire dataset to be 123 synthesis paragraphs
    (all with ground-truth synthesis conditions known), and the example pool size
    (# of annotations) to increase from zero. The example pool in each setting is
    randomly chosen from the entire dataset. To alleviate the uncertainty from randomness,
    we give 5 trials on each example pool size setting. Also, to further understand
    the scalability of example pool sizing, we create a new dataset with 60 synthesis
    paragraphs from the entire data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们讨论了少样本示例的数量和质量。然而，仍然不清楚高通量合成提取需要多少真实标注，即从中选择少样本的示例池，以应对成千上万的MOFs文献或更多。为回答这个问题，我们设计了一个实验，假设整个数据集为123个合成段落（所有段落的合成条件已知），示例池大小（标注数量）从零开始增加。每种设置下的示例池是从整个数据集中随机选择的。为减轻随机性的影响，我们在每种示例池大小设置下进行了5次试验。此外，为了进一步了解示例池大小的可扩展性，我们从整个数据中创建了一个包含60个合成段落的新数据集。
- en: 'Figure [7](#S2.F7 "Figure 7 ‣ 2.3 Optimization for High-Throughput MOFs Synthesis
    Extraction ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
    Demonstrations")(a) illustrates the result on the effect of example pool size.
    First, the first few annotations (from 0 to 5 in the figure) contribute the most
    performance gain, regardless of the size of entire dataset. This is coherent with
    the effect observed on zero-shot learning vs. one-shot. Second, smaller datasets
    (i.e., 60-paragraph by the green line) require fewer # of annotations than larger
    datasets (i.e., 123-paragraph by the red line), while achieving the same level
    of performance. The green line stays above the red line, especially under smaller
    example pool size. Third, more annotations will almost always bring performance
    gains and less uncertainty, though most boosts happen at initial few annotations.
    On the 60-paragraph data, the performance peak (F1=0.92) appears at the pool size
    of 40, 66.7% of the data size; on the 123-paragraph data, the peak (F1=0.93) does
    not happen before the pool size of 65, thus at least larger than 52.8% of the
    data size. A future work would be studying the active learning mechanism, which
    may help to reduce the required example pool in few-shot learning of LLM.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#S2.F7 "Figure 7 ‣ 2.3 Optimization for High-Throughput MOFs Synthesis
    Extraction ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
    Demonstrations")(a) 说明了示例池大小对效果的影响。首先，前几个标注（图中从0到5）贡献了最大的性能提升，不论整个数据集的大小。这与零-shot学习与一-shot学习的效果是一致的。其次，较小的数据集（即绿色线条的60段落）所需的标注数量少于较大的数据集（即红色线条的123段落），同时实现相同水平的性能。绿色线条始终高于红色线条，尤其是在较小的示例池大小下。第三，更多的标注几乎总能带来性能提升和更少的不确定性，尽管大部分提升发生在初始的少数标注上。在60段落的数据上，性能峰值（F1=0.92）出现在池大小为40，即数据大小的66.7%；在123段落的数据上，峰值（F1=0.93）出现时池大小至少为65，即数据大小的52.8%以上。未来的工作可以研究主动学习机制，这可能有助于减少LLM的少-shot学习所需的示例池。
- en: Figure [7](#S2.F7 "Figure 7 ‣ 2.3 Optimization for High-Throughput MOFs Synthesis
    Extraction ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
    Demonstrations")(b) further demonstrates the effect of both example pool size
    and $K$-shots. As shown in the figure, when the labeled pool size increases from
    0 to 5, the performance metrics improve rapidly, indicating that a labeled dataset
    is much more effective than an unlabeled one. Subsequently, the performance metrics
    increase slowly until the labeled pool size reaches the range of 40-55\. The performance
    metrics then stabilize, with the F1 score fluctuating slightly around 0.91 and
    ACC around 0.88.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#S2.F7 "Figure 7 ‣ 2.3 Optimization for High-Throughput MOFs Synthesis
    Extraction ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
    Demonstrations")(b) 进一步展示了示例池大小和$K$-shots的效果。如图所示，当标注池大小从0增加到5时，性能指标迅速改善，表明标注数据集比未标注数据集更为有效。随后，性能指标缓慢提高，直到标注池大小达到40-55的范围。性能指标随后稳定，F1分数在0.91左右略有波动，ACC在0.88左右。
- en: Coreference Resolution
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 核指代解析
- en: For convenience of writing, proxy words like “L” or “H2L” are frequently used
    in the MOFs literature to represent specific organic linkers, which are called
    coreference in NLP. In all the extracted synthesis conditions from 5269 paragraphs,
    578 coreference cases are identified. These proxy words could refer to substances
    defined far in the same article, which makes it difficult to use the extraction
    results in downstream material application.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了书写方便，MOFs文献中常使用像“L”或“H2L”这样的代理词来代表特定的有机连接体，在自然语言处理（NLP）中称为核心指代。在从5269段落中提取的合成条件中，识别出578个核心指代案例。这些代理词可能指代文章中较远定义的物质，这使得在下游材料应用中使用提取结果变得困难。
- en: Due to different writing styles, regular expression can not be employed as the
    sole method to resolve the coreference of these proxy words. We introduce a hybrid
    method combining LLM and regular expression for coreference resolution. The resolving
    of proxy word coreference is done in three steps. First, the synthesis paragraph
    is located in the literature and all the text before the paragraph is input to
    LLM. The LLM is asked to extract all anaphoric references and the original words.
    Second, a regular expression is designed to identify coreference proxy words from
    all the extracted conditions by LLM. Finally, these proxy words in the synthesis
    condition are matched with the detected anaphoric reference. If a match exist,
    the proxy word is resolved into the original words discovered by LLM in the second
    step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同的写作风格，常规表达式不能作为解决这些代理词共指的唯一方法。我们引入了一种结合LLM和常规表达式的混合方法来解决共指问题。代理词共指的解析分为三个步骤。首先，定位文献中的合成段落，并将段落之前的所有文本输入LLM。LLM被要求提取所有的指代引用和原始词。其次，设计一种常规表达式从LLM提取的条件中识别共指代理词。最后，将合成条件中的这些代理词与检测到的指代引用进行匹配。如果存在匹配，代理词将被解析为在第二步中由LLM发现的原始词。
- en: Overall, in all the 578 organic linker conditions using coreference, 79% of
    them can be resolved by our method. Only 0.023 linkers per paragraph remain unresolved.
    As shown in Table [2](#S2.T2 "Table 2 ‣ 2.3 Optimization for High-Throughput MOFs
    Synthesis Extraction ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations"), The five most appearing coreference words are
    “L”, “H2L”, “HL”, “L1”, and “H4L”, with all the resolution rates over 85%.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，在所有578个使用共指的有机连接条件中，我们的方法可以解析其中79%。每段剩余0.023个未解析的连接词。如表[2](#S2.T2 "表 2
    ‣ 2.3 高通量MOFs合成提取的优化 ‣ 2 结果 ‣ 基于LLM的MOFs合成条件提取使用少量示例")所示，出现频率最高的五个共指词是“L”、“H2L”、“HL”、“L1”和“H4L”，所有这些词的解析率都超过了85%。
- en: '| Proxy Words | Occurrence Count | Resolution Count | Resolution Ratio |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 代理词 | 出现次数 | 解析次数 | 解析比例 |'
- en: '| L | 106 | 92 | 86.8% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| L | 106 | 92 | 86.8% |'
- en: '| H2L | 64 | 58 | 90.6% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| H2L | 64 | 58 | 90.6% |'
- en: '| HL | 45 | 45 | 100% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| HL | 45 | 45 | 100% |'
- en: '| L1 | 39 | 37 | 94.9% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| L1 | 39 | 37 | 94.9% |'
- en: '| H4L | 38 | 33 | 86.8% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| H4L | 38 | 33 | 86.8% |'
- en: 'Table 2: Five most frequently used proxy words and their resolution results.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：五个最常用的代理词及其解析结果。
- en: 2.4 MOFs Structure Inference
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 MOFs结构推断
- en: 'To better validate the accuracy and potential of few-shot synthesis extraction
    method in downstream tasks, we set up a real-world MOFs synthesis-structure inference
    task and compared it with existing benchmark methods (zero-shot LLM). The specific
    task is to predict the microscopic property of MOFs: global cavity diameter, pore
    limiting diameter, largest cavity diameter, and framework density, using the synthesis
    conditions including metals, organic links, solvents, and reaction duration/temperature.
    We evaluate the task performance using coefficient of determination ($R^{2}$ metric
    effectively quantifies a model’s explanatory power regarding the actual data variation
    and the model accuracy. Therefore, it can be used to reflect the impact of different
    synthesis conditions on MOFs microstructure.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地验证少量示例合成提取方法在下游任务中的准确性和潜力，我们设置了一个真实世界的MOFs合成-结构推断任务，并与现有的基准方法（零样本LLM）进行了比较。具体任务是预测MOFs的微观性质：全局孔径、孔限制直径、最大孔径和框架密度，使用包括金属、有机连接、溶剂和反应时间/温度的合成条件。我们使用决定系数（$R^{2}$度量有效地量化了模型对实际数据变化的解释能力和模型准确性。因此，它可以用来反映不同合成条件对MOFs微观结构的影响。
- en: The evaluation data is a subset of the CSD database [[14](#bib.bib14)], which
    encompasses 5269 MOFs. As detailed in Sec. [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations"),
    these MOFs are carefully selected so that each MOF is described by only one scientific
    literature and the literature will only have one synthesis paragraph. The resulting
    dataset ensures the validity of evaluation by exact correspondence between a MOF’s
    microscopic structure and its extracted synthesis conditions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 评估数据是CSD数据库[[14](#bib.bib14)]的一个子集，其中包含5269种MOFs。如第[5.1](#S5.SS1 "5.1 MOFs数据
    ‣ 5 附录 ‣ 基于LLM的MOFs合成条件提取使用少量示例")节所述，这些MOFs经过精心挑选，每个MOF由唯一的科学文献描述，而该文献仅包含一个合成段落。生成的数据集通过MOF的微观结构与提取的合成条件之间的精确对应性确保了评估的有效性。
- en: Using the few-shot/zero-shot LLMs and other benchmark methods, the 10 synthesis
    conditions under study are extracted from a unique synthesis paragraph linked
    to each of the 5269 MOFs. The raw textual conditions extracted are post-processed
    to improve data quality, such as synonym merging and standardization of temperature/time
    scales (Sec. [5.3](#S5.SS3 "5.3 Post-processing of Synthesis Conditions ‣ 5 Appendix
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")).
    On the LLM output by the few-shot method, the top 100, 135, and 20 precursor names
    of metals, linkers, and solvents are selected, which leads to a smaller dataset
    of 800 MOFs. On the LLM by zero-shot method, the distribution of conditions are
    less longer-tailed, so that a stricter filter is applied to obtain the same number
    of 800 MOFs. These precursor names are embedded into one length-198 feature vector
    by the methods in Sec. [5.3](#S5.SS3 "5.3 Post-processing of Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations"),
    where serves as the input features in the material inference task. The target
    outcome variables are the four microstructure property of a MOF. Their calculation
    procedure is described in Sec. [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用几-shot/零-shot LLMs 和其他基准方法，从每个5269个MOF相关联的独特合成段落中提取出10个合成条件。提取出的原始文本条件经过后处理以提高数据质量，例如同义词合并和温度/时间尺度的标准化（见
    [5.3](#S5.SS3 "5.3 Post-processing of Synthesis Conditions ‣ 5 Appendix ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")）。在通过几-shot方法的LLM输出中，选择了前100、135和20个金属、连接体和溶剂的前体名称，这导致了一个较小的数据集，总共有800个MOF。在通过零-shot方法的LLM中，条件分布的尾部较短，因此应用了更严格的筛选，以获得相同数量的800个MOF。这些前体名称通过
    [5.3](#S5.SS3 "5.3 Post-processing of Synthesis Conditions ‣ 5 Appendix ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations") 中的方法被嵌入到一个长度为198的特征向量中，作为材料推断任务的输入特征。目标结果变量是MOF的四个微观结构属性。它们的计算过程在
    [5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations") 中描述。
- en: Model Zero-shot R² Few-shot R² Lasso 0.1755 0.2257 Bayesian Ridge 0.1758 0.2318
    AdaBoost 0.2570 0.3298 Random Forest 0.2498 0.3468 Gradient Boosting 0.2919 0.3632
    XGBoost 0.3559 0.4421
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模型  |  Zero-shot R²  |  Few-shot R²
- en: 'Table 3: Performance comparison of few-shot and zero-shot LLMs across different
    machine learning models on the inference MOFs framework density.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：不同机器学习模型在推断MOF框架密度上的几-shot和零-shot LLMs的性能比较。
- en: 'We apply six machine learning models for the inference: Lasso Regression, Bayesian
    Ridge Regression, AdaBoost, Random Forest, Gradient Boosting Regression, and Extreme
    Gradient Boosting (XGBoost). The first five models do not support missing value
    as input, so we use mean imputation instead. With each model, we compare the two
    LLM-based method, few-shot learning vs. zero-shot learning, in a 10-fold cross-validation.
    On the four microstructure properties inferred, the first three lead to negative
    or close to zero $R^{2}$ values larger than 0.2\. This also validates the fact
    that MOFs density is highly correlated with the metal and organic precursors used
    in MOFs material synthesis, as well as the reaction duration and temperature.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了六种机器学习模型进行推断：Lasso回归、贝叶斯岭回归、AdaBoost、随机森林、梯度提升回归和极端梯度提升（XGBoost）。前五种模型不支持缺失值作为输入，因此我们使用均值插补。对于每个模型，我们在10折交叉验证中比较了两种基于LLM的方法，即几-shot学习与零-shot学习。在推断的四个微观结构属性中，前三种方法导致了负值或接近于零的
    $R^{2}$ 值，大于0.2。这也验证了MOF密度与用于MOF材料合成的金属和有机前体，以及反应时间和温度高度相关的事实。
- en: The $R^{2}$ of 0.3559 on the test set for the zero-shot method. We illustrate
    the inference result by XGBoost on the scatterplot of Figure [8](#S2.F8 "Figure
    8 ‣ 2.4 MOFs Structure Inference ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")(a). It shows that the actual vs. predicted
    distribution of the few-shot method (green dots) preserves higher affinity to
    the optimal prediction line (red dashed line), than the predictions by zero-shot
    method (blue dots). The result demonstrates that the proposed few-shot method
    not only extracts more accurate synthesis conditions in comparison to the baseline,
    but also significantly improves the performance of downstream material inference
    tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本方法在测试集上的$R^{2}$为0.3559。我们通过图[8](#S2.F8 "Figure 8 ‣ 2.4 MOFs Structure Inference
    ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(a)的散点图展示了XGBoost的推断结果。图中显示，少样本方法（绿色点）与最佳预测线（红色虚线）相比，实际与预测的分布具有更高的亲和力，而零样本方法（蓝色点）的预测则较低。结果表明，所提出的少样本方法不仅比基准方法提取了更准确的合成条件，还显著提升了下游材料推断任务的性能。
- en: '![Refer to caption](img/333b8395199ee23a4704ce9338fae51e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/333b8395199ee23a4704ce9338fae51e.png)'
- en: 'Figure 8: Performance on MOFs structure inference task: (a) predictive power
    of the best XGBoost model, few-shot vs. zero-shot; (b) comparison of $R^{2}$ values
    and data counts across different data filters.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：MOFs结构推断任务的表现：（a）最佳XGBoost模型的预测能力，少样本 vs. 零样本；（b）不同数据过滤下的$R^{2}$值和数据量对比。
- en: To further showcase the superiority of the few-shot method, we conducted more
    trials using the best-performing model, XGBoost. We gradually reduce the test
    dataset into more densely distributed synthesis conditions, by enforcing stricter
    data filters and selecting only higher-ranked synthesis condition values. The
    XGBoost model is tuned with the best hyperparameters on each dataset following
    the method by Akiba et al. [[4](#bib.bib4)]. As shown in Figure [8](#S2.F8 "Figure
    8 ‣ 2.4 MOFs Structure Inference ‣ 2 Results ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")(b), when the test dataset increases
    with more conditions, the $R^{2}$ of zero-shot method stays stable or rises much
    slowly. The gap between the two methods widens as the dataset includes more unique
    conditions. Meanwhile, the data size by the number of MOFs used remains comparable
    between the two methods in every setting, as indicated by the grouped bar charts
    in Figure [8](#S2.F8 "Figure 8 ‣ 2.4 MOFs Structure Inference ‣ 2 Results ‣ LLM-based
    MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b). The result
    indicates that considering less frequently appearing synthesis conditions will
    significantly improve the accuracy of material structure inference when the few-shot
    method is applied. In contrast, the zero-shot method showed a steady trend in
    predictive performance. This also reveals the superior performance of the few-shot
    method in downstream material inference task compared to the zero-shot method.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步展示少样本方法的优越性，我们使用表现最佳的模型XGBoost进行了更多试验。我们通过施加更严格的数据过滤和选择仅高排名的合成条件值，将测试数据集逐渐减少到更密集分布的合成条件。XGBoost模型在每个数据集上按照Akiba等人的方法[[4](#bib.bib4)]进行了最佳超参数调优。如图[8](#S2.F8
    "Figure 8 ‣ 2.4 MOFs Structure Inference ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(b)所示，当测试数据集的条件增多时，零样本方法的$R^{2}$保持稳定或上升缓慢。随着数据集包含更多独特条件，两种方法之间的差距扩大。与此同时，如图[8](#S2.F8
    "Figure 8 ‣ 2.4 MOFs Structure Inference ‣ 2 Results ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")(b)中的分组条形图所示，两种方法在每个设置下使用的MOFs数量数据大小保持相当。结果表明，考虑不那么频繁出现的合成条件将显著提高材料结构推断的准确性，尤其在应用少样本方法时。相反，零样本方法的预测性能则保持稳定趋势。这也揭示了少样本方法在下游材料推断任务中相比零样本方法的优越性能。
- en: 3 Methods
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Synthesis paragraph detection
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 合成段落检测
- en: To train a machine learning model for binary classification to determine whether
    a paragraph is synthesized, we randomly obtained 440 papers from the database
    in Appendix A for annotation. Each paper was annotated by two different annotators
    to ensure inner annotator agreement. The 880 annotation tasks were assigned to
    four annotators who used our platform, shown in Figure [9](#S5.F9 "Figure 9 ‣
    5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions ‣ 5
    Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations"),
    to annotate synthesis-related paragraphs. After annotation, only paragraphs annotated
    by both annotators were considered valid, while paragraphs annotated by only one
    annotator were discarded. If there was an overlap in the positions of the paragraphs
    annotated by the two annotators, we found that mismatched paragraphs often occurred
    because one annotator noted more synthesis parameters and thus marked a larger
    range. In such cases, the larger annotated paragraph was considered valid. This
    method also resolved minor annotation deviations within a few characters, allowing
    two slightly different synthesis paragraphs to be considered valid. This process
    yielded 1,349 valid annotated paragraphs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个用于二分类的机器学习模型，以确定一个段落是否为合成的，我们从附录A的数据库中随机获得了440篇论文进行标注。每篇论文由两位不同的标注者进行标注，以确保标注者之间的一致性。880个标注任务分配给四位标注者，他们使用我们的平台（见图[9](#S5.F9
    "Figure 9 ‣ 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")）对合成相关段落进行标注。标注后，仅两位标注者都标注的段落被视为有效，而仅由一位标注者标注的段落被丢弃。如果两位标注者标注的段落有重叠，我们发现经常会出现不匹配的段落，因为一个标注者记录了更多的合成参数，因此标记了更大的范围。在这种情况下，较大的标注段落被视为有效。该方法还解决了字符数差异较小的标注偏差，使得两个略有不同的合成段落被视为有效。这个过程产生了1,349个有效标注段落。
- en: 'To train the discrimination model, non-synthesis paragraphs were needed as
    negative samples. After removing all annotated paragraphs from a paper, the remaining
    paragraphs served as negative samples. This method resulted in 11,783 negative
    samples. We employed the standard BERT model, specifically the pre-trained bert-base-uncased
    model from HuggingFace, for training. The training and validation processes utilized
    a 5-fold cross-validation method ($k=5$). Given the imbalance dataset, we used
    stratified k-fold cross-validation to ensure that the ratio of positive to negative
    samples remained consistent in each split. After training and cross-validation
    testing, the model’s evaluation metrics were as follows: Accuracy = 0.989, Precision
    = 0.955, Recall = 0.947, and F1 Score = 0.951\. The trained model achieved high
    overall accuracy and can be used for synthesized paragraph classification in extracted
    paragraphs, which will facilitate subsequent chemical named entity recognition
    tasks.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练判别模型，需要非合成段落作为负样本。在从论文中移除所有标注的段落后，剩余的段落作为负样本。该方法产生了11,783个负样本。我们使用了标准BERT模型，特别是HuggingFace提供的预训练`bert-base-uncased`模型进行训练。训练和验证过程使用了5折交叉验证方法（$k=5$）。鉴于数据集的不平衡性，我们使用了分层k折交叉验证，以确保每个分割中的正负样本比例保持一致。在训练和交叉验证测试之后，模型的评估指标如下：准确率
    = 0.989，精确率 = 0.955，召回率 = 0.947，F1分数 = 0.951。训练后的模型达到了高准确率，并可以用于在提取的段落中进行合成段落分类，这将有助于后续的化学命名实体识别任务。
- en: 'The results of applying the model to our MOFs dataset in Appendix A are as
    follows: According to statistics, the dataset contains a total of 36,233 DOIs,
    corresponding to 78,741 MOF-IDs. Among these, 21,031 DOIs can be used to extract
    synthesis paragraphs, and 22,461 DOIs remain one DOI corresponding to one MOF-ID.
    The intersection of these two sets contains 9,855 DOIs. Further filtering for
    DOIs that contain only one synthesis paragraph results in 5,269 DOIs. In other
    words, we extracted 5,269 valid synthesis paragraphs from the complete dataset.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型应用于附录A中的MOFs数据集的结果如下：根据统计，该数据集包含总共36,233个DOI，对应78,741个MOF-ID。其中，21,031个DOI可以用于提取合成段落，22,461个DOI仍然是一个DOI对应一个MOF-ID。这两个集合的交集包含9,855个DOI。进一步筛选出仅包含一个合成段落的DOI结果为5,269个DOI。换句话说，我们从完整数据集中提取了5,269个有效合成段落。
- en: Paragraphs related to the synthesis process constitute only about 2% of an article’s
    total length but concentrate the main synthesis condition. Extracting condition
    from synthesized paragraphs, rather than the entire text, can significantly reduce
    the model’s extraction overhead and increase the density of field distribution,
    thereby enhancing data quality.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与合成过程相关的段落只占文章总长度的约 2%，但集中体现了主要的合成条件。从合成段落中提取条件，而不是整个文本，可以显著减少模型的提取开销，提高领域分布的密度，从而提升数据质量。
- en: 3.2 Few-Shot RAG Algorithms
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 少样本 RAG 算法
- en: 'To maximize the extraction performance of the model, we provide examples of
    extraction by human-AI annotation as demonstrations. By using a retrieve $K$ similar
    demonstrations are obtained as:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化模型的提取性能，我们提供了人类与 AI 注释的提取示例作为演示。通过使用检索 $K$ 个相似的示例可以得到如下：
- en: '|  | $\text{Top-K}=\text{sort}({(\text{score}(p,d_{i}),d_{i})}^{n}_{i=1})[:k]$
    |  | (1) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Top-K}=\text{sort}({(\text{score}(p,d_{i}),d_{i})}^{n}_{i=1})[:k]$
    |  | (1) |'
- en: Here, the score is used to estimate the similarity between the embeddings of
    document $d_{i}$. The embedding models can be categorized into traditional sparse
    vector encoders (e.g., TF-IDF, BM25 [[18](#bib.bib18)]) and semantic dense vector
    encoders (e.g., SBERT [[17](#bib.bib17), [8](#bib.bib8)]) [[10](#bib.bib10)].
    In our experiments, we compared these two classes of retrieval methods and selected
    the one that performed best as the final approach.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，得分用于估计文档 $d_{i}$ 的嵌入之间的相似性。嵌入模型可以分为传统稀疏向量编码器（例如，TF-IDF、BM25 [[18](#bib.bib18)]）和语义密集向量编码器（例如，SBERT
    [[17](#bib.bib17), [8](#bib.bib8)]] [[10](#bib.bib10)]）。在我们的实验中，我们比较了这两类检索方法，并选择了表现最佳的方法作为最终方案。
- en: 'For the traditional sparse vector retrieval method, we use the BM25 algorithm.
    BM25 is a probabilistic information retrieval model that ranks documents based
    on the frequency of query terms within the documents. It balances term frequency
    (how often a term appears in a document) with inverse document frequency (how
    rare a term is across the entire document set), thus giving more weight to terms
    that are significant. The scoring function is defined as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传统稀疏向量检索方法，我们使用 BM25 算法。BM25 是一种概率信息检索模型，根据文档中查询词的频率对文档进行排序。它平衡了词频（词在文档中出现的频率）与逆文档频率（词在整个文档集中的稀有程度），从而对重要的词给予更多权重。评分函数定义为：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: '|  | $\text{avg\_dl}=\frac{1}{N}\sum_{j=1}^{N}&#124;d_{j}&#124;$ |  | (3) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{avg\_dl}=\frac{1}{N}\sum_{j=1}^{N}&#124;d_{j}&#124;$ |  | (3) |'
- en: where $f(p_{i},d)$.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(p_{i},d)$。
- en: 'For the semantic information-based method, we use the embedding vector representation
    of the text obtained from a pre-trained language model:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于语义信息的方法，我们使用从预训练语言模型中获得的文本嵌入向量表示：
- en: '|  | $\text{Score}(p,d)=\frac{f(p)\cdot f(d)}{&#124;f(p)&#124;&#124;f(d)&#124;}$
    |  | (4) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Score}(p,d)=\frac{f(p)\cdot f(d)}{&#124;f(p)&#124;&#124;f(d)&#124;}$
    |  | (4) |'
- en: where $f(x)=\text{PLM}(x)$ most relevant paragraphs-extraction pairs obtained
    in the previous step before the input as few-shot demonstrations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(x)=\text{PLM}(x)$ 最相关的段落提取对在输入作为少样本示例之前获得。
- en: 4 Conclusion
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: This work studies the new paradigm of applying few-shot in-context learning
    to the popular approach of LLM literature extraction for discovering MOFs synthesis
    conditions. It is shown through experiments that both the quality and the quantity
    of few-shot demonstrations are important in the studied scenario. We introduce
    both a novel process of human-AI joint data curation to enhance few-shot demonstration
    quality and a calibrated BM-25 RAG algorithm to size the optimal few-shot quantity.
    Scalability issues regarding high-throughput MOFs synthesis condition extraction
    are resolved using many practical methods such as offline synthesis paragraph
    detection and LLM-based coreference resolution. Our proposal is thoroughly evaluated
    using large-scale real-life MOFs dataset, on both text extraction performance
    for synthesis condition discovery and the downstream material task on structural
    property inference.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究探讨了将少量示例的上下文学习应用于流行的LLM文献提取方法，以发现MOF合成条件的新范式。通过实验显示，在研究场景中，少量示例的质量和数量都很重要。我们引入了一种新的人机联合数据整理过程来提高少量示例的质量，并提出了一种经过校准的BM-25
    RAG算法来确定最佳少量示例数量。针对高通量MOF合成条件提取的可扩展性问题，使用了许多实际方法，如离线合成段落检测和基于LLM的共指解析。我们的方法在大规模实际MOF数据集上进行了全面评估，既评估了合成条件发现的文本提取性能，也评估了结构属性推断的下游材料任务。
- en: 5 Appendix
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 附录
- en: 5.1 MOFs Data
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 MOF数据
- en: CSD and the retrieved dataset
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: CSD和检索的数据集
- en: We base our work on the MOF subset of Cambridge Structural Database (CSD) [[14](#bib.bib14)]
    retrieved in June 2022, which lists 84,898 MOFs covering the bonding motifs of
    all common MOFs in CSD. The entry of a MOF in the database contains its structure
    in CIF format, the physical properties, a DOI linking to the relevant publication,
    and a unique MOF ID.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作基于剑桥结构数据库（CSD）的MOF子集[[14](#bib.bib14)]，该数据集于2022年6月检索，列出了84,898种MOF，涵盖了CSD中所有常见MOF的配位动机。数据库中的MOF条目包含其CIF格式的结构、物理属性、一个指向相关出版物的DOI链接，以及一个唯一的MOF
    ID。
- en: The dataset is then pre-processed according to the goal of this work. First,
    the full-text describing the MOFs under study should be available. Out of all
    the 84,898 MOFs, 78,741 has non-empty DOIs. Since the same DOI could be linked
    to multiple MOFs (one paper reporting more than one MOFs), there leaves 39,579
    different DOI links after deduplication and 36,177 downloadable paper full-text.
    For the convenience of follow-up processing, we focus on the DOIs where the associated
    publication reports the information of only one MOF in CSD. This leads to a subset
    of 22,461 MOFs, each with a unique publication file in PDF format.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集随后根据本工作的目标进行了预处理。首先，描述所研究MOF的全文本应可用。在所有84,898种MOF中，78,741种具有非空的DOI。由于相同的DOI可能链接到多个MOF（一个论文报告了多个MOF），经去重后留下39,579个不同的DOI链接和36,177个可下载的论文全文。为了方便后续处理，我们专注于那些关联出版物仅报告CSD中一个MOF信息的DOI。这导致了22,461种MOF的子集，每种MOF都有一个唯一的PDF格式出版物文件。
- en: Next, the PDF of each MOF is converted to plain text [[20](#bib.bib20)] and
    segmented into paragraphs. The high performance classification model in Sec. [3.1](#S3.SS1
    "3.1 Synthesis paragraph detection ‣ 3 Methods ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations") is applied to detect synthesis paragraphs
    enclosing the desired synthesis condition information. Again, for the sake of
    convenience and accuracy, we only consider the 5,269 MOFs/publications that contain
    exactly one synthesis paragraph. Another 12,606 publications do not have any synthesis
    paragraph, probably because these papers are not related to MOFs experiments.
    The other 4,586 publications have more than one synthesis paragraphs, as they
    are describing multiple MOFs or synthesis routes. Our pipeline could work with
    papers having more than one suite of synthesis conditions, but the potential MOF-synthesis
    mismatch may downgrade the application performance in evaluation. Therefore, throughout
    this work we stick to the core dataset of 5,269 MOFs/publications and their unique
    synthesis paragraph.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，每种MOF的PDF被转换为纯文本[[20](#bib.bib20)]并分段。第[3.1](#S3.SS1 "3.1 合成段落检测 ‣ 3 方法
    ‣ 基于LLM的MOFs合成条件提取使用少量示范")节中的高性能分类模型被应用于检测包含所需合成条件信息的合成段落。为了方便和准确起见，我们仅考虑包含确切一个合成段落的5,269种MOFs/出版物。其他12,606篇出版物没有任何合成段落，可能因为这些论文与MOFs实验无关。其他4,586篇出版物有多个合成段落，因为它们描述了多种MOFs或合成路线。我们的流程可以处理具有多个合成条件组的论文，但潜在的MOF-合成不匹配可能会降低评估中的应用性能。因此，在整个工作中，我们坚持使用5,269种MOFs/出版物及其唯一的合成段落作为核心数据集。
- en: Microstructure Property Computation
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 微观结构性质计算
- en: 'For material evaluation purpose, we also calculate structural and physical
    properties of the 5,269 MOFs under consideration. The CIF file of each MOF is
    retrieved from CSD and input to the Zeo++ tool [[22](#bib.bib22)]. In total, four
    structural and physical properties are calculated: global cavity diameter, pore
    limiting diameter, largest cavity diameter, and framework density. We set the
    probe radius to 1.29A to simulate helium gas molecules, and the number of Monte
    Carlo samples to 100,000 to ensure the accuracy of calculations. All Zeo++ parameters
    adhere to standard routines, guaranteeing that the computed properties accurately
    represent the behavior of gas molecules within the MOF structure.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了材料评估目的，我们还计算了考虑中的5,269种MOFs的结构和物理性质。每种MOF的CIF文件从CSD中检索并输入到Zeo++工具中[[22](#bib.bib22)]。总共计算了四种结构和物理性质：全球腔体直径、孔限制直径、最大腔体直径和框架密度。我们将探针半径设置为1.29A，以模拟氦气分子，并将Monte
    Carlo样本数设置为100,000，以确保计算的准确性。所有Zeo++参数遵循标准程序，确保计算出的性质准确反映MOF结构内气体分子的行为。
- en: 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 合成段落和合成条件的注释程序
- en: '![Refer to caption](img/c957b7e998bf7159e267e2f3aa419146.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c957b7e998bf7159e267e2f3aa419146.png)'
- en: 'Figure 9: User interface of the annotation platform.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：注释平台的用户界面。
- en: High-quality annotations are the cornerstone of few-shot in-context learning;
    only accurate and highly coherence annotations can improve the precision of extracting.
    Therefore, we enlisted the help of eight experts in materials science and engineering
    to assist with the annotations. Additionally, we developed a batch interactive
    annotation platform to enhance the convenience of the annotation process. During
    the annotation process, we discovered that the task was challenging and had a
    high error rate done by human only, which led to poor model extraction performance
    when using erroneously annotated examples. Consequently, we implemented a comprehensive
    annotation process to improve quality.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量的注释是少量样本上下文学习的基石；只有准确且高度一致的注释才能提高提取的精度。因此，我们邀请了八位材料科学与工程领域的专家来协助注释。此外，我们开发了一个批量交互注释平台，以提高注释过程的便利性。在注释过程中，我们发现任务具有挑战性且仅由人工完成的错误率较高，这导致使用错误注释的示例时模型提取性能较差。因此，我们实施了一整套全面的注释流程以提高质量。
- en: Synthesis Paragraph Annotation
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 合成段落注释
- en: To annotate synthesis paragraphs for offline machine learning, 440 papers were
    randomly obtained from the database in Appendix A. For inner annotator agreement,
    each paper was annotated by two different annotators. The 880 annotation tasks
    were assigned to four annotators, who used our platform shown in Figure [9](#S5.F9
    "Figure 9 ‣ 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")
    to annotate synthesis-related paragraphs. After annotation, only paragraphs annotated
    by both annotators were considered valid, and paragraphs annotated by only one
    annotator were discarded. If there was an overlap in the positions of the paragraphs
    annotated by the two annotators, we found through checking the annotated data
    that the common mismatched paragraphs often occurred because one annotator noted
    more synthesis conditons and thus marked a larger range for the synthesis paragraph.
    In such cases, the paragraph should also be considered valid. Therefore, we treated
    the larger annotated paragraph as a valid synthesis paragraph. This method also
    resolved the issue of minor annotation deviations within a few characters, allowing
    two slightly different synthesis paragraphs to be considered valid. This process
    yielded 1,349 valid annotated paragraphs. To train the discrimination model, non-synthesis
    paragraphs are needed as negative samples. After removing all paragraphs annotated
    by annotators from a paper, the remaining paragraphs serve as negative samples.
    This method resulted in 11,783 negative samples used for training the synthesis
    paragraph discrimination model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对合成段落进行离线机器学习标注，从附录A中的数据库中随机获取了440篇论文。为了确保标注者之间的一致性，每篇论文由两名不同的标注者进行标注。这880个标注任务被分配给四名标注者，他们使用我们在图[9](#S5.F9
    "Figure 9 ‣ 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")中展示的平台进行合成相关段落的标注。标注后，只有两个标注者都标注过的段落才被视为有效，而仅由一个标注者标注的段落则被丢弃。如果两个标注者标注的段落有重叠，通过检查标注数据发现，共同的不匹配段落通常发生在一个标注者标记了更多的合成条件，从而标记了更大的范围。在这种情况下，该段落也应被视为有效。因此，我们将较大的标注段落视为有效合成段落。这种方法也解决了字符数小的标注偏差问题，使得两个略微不同的合成段落可以被视为有效。该过程产生了1,349个有效标注段落。为了训练判别模型，需要将非合成段落作为负样本。通过从论文中删除所有标注的段落，剩余的段落作为负样本。该方法产生了11,783个用于训练合成段落判别模型的负样本。
- en: Synthesis Condition Annotation
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 合成条件标注
- en: 'We randomly selected 200 papers from the paper database constructed in [5.1](#S5.SS1
    "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using
    Few-Shot Demonstrations"), with each paper only contains less than 3 MOFs IDs.
    The annotation process can be divided into five sections: task configuration,
    GPT pre-extraction annotation, pilot annotation, batch annotation, and data curation.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[5.1](#S5.SS1 "5.1 MOFs Data ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition
    Extraction using Few-Shot Demonstrations")构建的论文数据库中随机选择了200篇，每篇论文仅包含少于3个MOFs ID。标注过程可以分为五个部分：任务配置、GPT预提取标注、试点标注、批量标注和数据整理。
- en: 'In the task configuration stage, domain experts define the key synthesis condition
    to be annotated and configure the annotation settings. The core standard for annotation
    configuration is . Due to the diversity of expressions in material papers, consistent
    annotation methods help increase the density of the resulting data, thereby reducing
    the complexity of subsequent data cleaning and enhancing the effectiveness of
    using ML to infer material structure or properties. The selected annotation synthesis
    condition must: 1) be common in synthesis paragraphs of related papers, and 2)
    be beneficial for predicting performance condition. We exclude Active process
    including active temperature and active time after pilot annotation because we
    recognized its low frequency. Also, Molecular formula was excluded for it helps
    little in performance parameter prediction. Once the annotation requirements and
    background knowledge are set by domain experts, the pre-extraction annotation
    phase can begin.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务配置阶段，领域专家定义需要标注的关键合成条件，并配置标注设置。标注配置的核心标准是。由于材料论文中表达的多样性，一致的标注方法有助于增加数据的密度，从而减少后续数据清理的复杂性，并提高使用机器学习推断材料结构或属性的效果。选择的标注合成条件必须：1)
    在相关论文的合成段落中常见，2) 对预测性能条件有利。我们在试点标注后排除了活性过程（包括活性温度和活性时间），因为我们认识到其低频率。同时，分子式也被排除，因为它对性能参数预测帮助不大。一旦领域专家设定了标注要求和背景知识，预提取标注阶段可以开始。
- en: Before the pilot annotation, the GPT pre-extraction method can be used to preliminarily
    locate synthesis paragraphs and relevant condition, assisting experts in annotation.
    The synthesis paragraph discrimination model extracts relevant paragraphs from
    the papers. Using the annotation requirements and domain knowledge configured
    in the task configuration stage, zero-shot prompts are applied for pre-extraction
    to obtain initial extraction data, which is then imported into the annotation
    system. Although the accuracy of this process is limited, it helps locate paragraphs
    and reduces annotation difficulty.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在试点标注之前，可以使用GPT预提取方法初步定位合成段落和相关条件，以帮助专家进行标注。合成段落区分模型从论文中提取相关段落。利用任务配置阶段配置的标注要求和领域知识，应用零-shot提示进行预提取，以获得初步提取数据，然后将其导入标注系统。尽管这一过程的准确性有限，但它有助于定位段落并减少标注难度。
- en: '*Pilot annotation:* twenty papers are randomly selected from the 200 candidate
    papers for pilot annotation to validate and adjust the annotation task settings
    and platform configuration. The annotation platform is shown in Figure [9](#S5.F9
    "Figure 9 ‣ 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations").
    Two annotators independently annotate the 20 papers on the platform. The results
    are used to check inner annotator agreement to ensure accuracy. This process requires
    annotators and researchers to analyze and discuss the following: 1) identify ambiguous
    and unclear parts of the annotation task configuration to clarify specific annotation
    methods, 2) reanalyze the synthesis condition to determine if some field are too
    sparse and need to be removed, or if some field are dense enough to be included
    as synthesis condition for predicting performance, and 3) identify any unreasonable
    designs in the annotation platform and make necessary modifications.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*试点标注：* 从200篇候选论文中随机选择二十篇进行试点标注，以验证和调整标注任务设置和平台配置。标注平台如图[9](#S5.F9 "Figure
    9 ‣ 5.2 Annotation Procedure for Synthesis Paragraphs and Synthesis Conditions
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")所示。两名标注员独立在平台上标注这20篇论文。结果用于检查标注员之间的一致性，以确保准确性。这个过程要求标注员和研究人员分析和讨论以下内容：1)
    识别标注任务配置中的模糊和不清楚的部分，以明确具体的标注方法，2) 重新分析合成条件，以确定是否有些领域过于稀疏需要移除，或者是否有些领域足够密集可以作为预测性能的合成条件，3)
    识别标注平台中的任何不合理设计并进行必要的修改。'
- en: This pilot annotation stage resulted in 20 valid papers. Annotators and researchers
    refined the annotation task configuration to maximize the quality of subsequent
    batch annotations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这一试点标注阶段产生了20篇有效的论文。标注员和研究人员对标注任务配置进行了优化，以最大限度提高后续批量标注的质量。
- en: '*Batch annotations:* the remaining 180 papers will produce 360 annotation tasks,
    assigned to six annotators. Each annotator is randomly assigned 60 papers, ensuring
    each paper is annotated by two different annotators. GPT pre-extraction is also
    used to enhance annotation accuracy and efficiency. Upon completion, the annotation
    data undergo a simple inner annotator agreement check, using Jaccard similarity
    to verify the consistency between the two annotators’ results. For each annotation
    field, a validity threshold of 0.8 overlap between the annotators is required,
    then the result field was the union of two fields, which is better for subsequent
    data cleaning than intersection. For each paper, a verified annotation item overlap
    rate of 80% or higher between the two annotators is considered a valid annotation
    paper. Non valid papers were not used in follow-up steps and can be used as supplementary
    data after manual review.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*批量注释：* 剩余的180篇论文将产生360个注释任务，分配给六位注释员。每位注释员随机分配60篇论文，确保每篇论文由两位不同的注释员注释。还使用GPT预提取以提高注释的准确性和效率。完成后，注释数据经过简单的内部注释员一致性检查，使用Jaccard相似性验证两位注释员结果的一致性。对于每个注释字段，要求注释员之间的重叠有效性阈值为0.8，然后结果字段是两个字段的并集，这比交集更适合后续的数据清理。对于每篇论文，两位注释员之间验证的注释项目重叠率达到80%或更高被视为有效注释论文。无效论文在后续步骤中未使用，并且可以在人工审核后作为补充数据使用。'
- en: This stage resulted in 147 papers with high overlap rates, used in this experiment.
    53 papers are excluded for subsequent process.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本阶段产生了147篇高重叠率的论文，已用于本实验。53篇论文被排除以进行后续处理。
- en: '*Joint Human-AI Data Curation:* to improve annotation quality, we introduce
    a joint human-AI data curation process. Experts finalize the data annotations
    step by double-check the results from both LLM and human annotations. The LLM
    results, using BM25 few-shot extraction of synthesis condition, are compared with
    the annotated results to identify inconsistencies. This step helps detect problems
    in batch annotations and assists in identifying erroneous annotations. Invalid
    papers will be excluded. We detected and excluded chiral MOFs in annotated papers,
    with duplicate synthesis conditions and paragraph. Also, for better sampling,
    we excluded all paragraph with more than one MOF synthesis process. Although our
    resolution framework can handle multiple MOFs in a single paragraph, we chose
    one-to-one paragraphs for better samples.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*联合人类-AI数据整理：* 为了提高注释质量，我们引入了联合人类-AI数据整理过程。专家通过双重检查LLM和人工注释的结果来最终确定数据注释。LLM的结果，使用BM25少样本提取合成条件，与注释结果进行比较，以识别不一致之处。这一步有助于发现批量注释中的问题，并协助识别错误注释。无效论文将被排除。我们在注释的论文中检测并排除了手性MOFs、重复的合成条件和段落。此外，为了更好的抽样，我们排除了所有包含多于一个MOF合成过程的段落。尽管我们的分辨率框架可以处理单个段落中的多个MOFs，但我们选择了逐一段落以获得更好的样本。'
- en: Additionally, common LLM extraction errors by the model are identified, allowing
    for targeted constraint writing in prompts to improve knowledge-based corrections.
    Constraints must be presented in the form of knowledge provision and must not
    contain any examples to avoid overfitting. This human-AI data curation process
    identified 120 annotation errors and added 5 constraints.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型常见的LLM提取错误被识别出来，从而可以在提示中进行有针对性的约束编写，以改善基于知识的修正。约束必须以知识提供的形式呈现，并且不能包含任何示例，以避免过拟合。该人类-AI数据整理过程识别了120个注释错误，并添加了5个约束。
- en: 5.3 Post-processing of Synthesis Conditions
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 合成条件的后处理
- en: '![Refer to caption](img/718fc6f1af4e8427b609c4107ff7d477.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/718fc6f1af4e8427b609c4107ff7d477.png)'
- en: 'Figure 10: Frequencies of occurrence for MOFs synthesis conditions: (a) metal
    precursor; (b) organic linker; (c) solvent.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '图10: MOFs 合成条件的出现频率： (a) 金属前驱体；(b) 有机连接体；(c) 溶剂。'
- en: The raw synthesis conditions extracted by LLM-based method often suffer from
    data quality issue, which potentially affects the downstream material inference
    task. We introduce several data postprocessing methods to improve the quality
    of derived synthesis conditions so that the input data to the inference model
    can be more formatted and densely distributed.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: LLM方法提取的原始合成条件常常存在数据质量问题，这可能影响下游材料推断任务。我们引入了几种数据后处理方法，以提高衍生合成条件的质量，使输入数据更具格式化和密集分布。
- en: 5.3.1 Data Cleansing on Textual Conditions
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 文本条件的数据清理
- en: The synthesis conditions include discrete names for Metal, Organic Linker, Solvent,
    and additives. These names often have different representations for the same substance
    (e.g., ”H2O” and ”Water” both represent water, ”Cd(NO3)2.4H2O” and ”Cd(NO3)2?4H2O”
    both represent cadmium nitrate tetrahydrate). Using unprocessed discrete names
    increases redundancy and noise in the dataset, complicating the embedding process
    and affecting the consistency and performance of the model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 合成条件包括金属、有机连接体、溶剂和添加剂的离散名称。这些名称通常对同一物质有不同的表示（例如，“H2O”和“Water”都表示水，“Cd(NO3)2.4H2O”和“Cd(NO3)2?4H2O”都表示四水合硝酸镉）。使用未经处理的离散名称会增加数据集中的冗余和噪声，使嵌入过程复杂化，并影响模型的一致性和性能。
- en: Similarity Disambiguation
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性消歧
- en: 'First, we use similarity disambiguation to create an initial list of assimilated
    names, eliminating some ambiguities caused by inconsistent spelling, based on
    the Levenshtein distance (edit distance). The specific steps are as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用相似性消歧来创建初步的同化名称列表，基于Levenshtein距离（编辑距离）消除一些因拼写不一致造成的歧义。具体步骤如下：
- en: '1.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Calculate the Levenshtein distance between two strings.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算两个字符串之间的Levenshtein距离。
- en: '2.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Normalize the similarity score by converting the Levenshtein distance to a
    score between 0 and 100 using the formula:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用以下公式将Levenshtein距离转换为0到100之间的分数，从而标准化相似性分数：
- en: '|  | $\text{Similarity\_ratio}=\left(1-\frac{\text{Levenshtein distance}}{\text{maximum
    length of the two strings}}\right)\times 100$ |  | (5) |'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{Similarity\_ratio}=\left(1-\frac{\text{Levenshtein distance}}{\text{maximum
    length of the two strings}}\right)\times 100$ |  | (5) |'
- en: '3.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Set a threshold. We set the threshold at 90 to filter out most unrelated string
    pairs while ensuring that only truly similar strings are identified as the same
    object.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设定一个阈值。我们将阈值设定为90，以过滤掉大多数无关的字符串对，同时确保仅真正相似的字符串被识别为同一对象。
- en: Synonym Merging Using GPT-4
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-4的同义词合并
- en: 'Next, we use GPT-4 for synonym merging. This method leverages the powerful
    capabilities of the GPT-4 model to successfully identify and group different names
    representing the same substance. The system also includes a reflection mechanism
    to ensure the accuracy of the classifications. The detailed workflow is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用GPT-4进行同义词合并。该方法利用GPT-4模型的强大能力，成功识别和分组代表同一物质的不同名称。系统还包括一个反射机制，以确保分类的准确性。详细的工作流程如下：
- en: '1.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Parse the input text to prepare the chemical substance names.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解析输入文本以准备化学物质名称。
- en: '2.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Use a predefined prompt (PROMPT1) to ask the GPT-4 model to classify the chemical
    substances and group identical substances. The model returns a JSON array, each
    item being a list of synonymous substances.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用预定义的提示（PROMPT1）请求GPT-4模型对化学物质进行分类，并将相同的物质分组。模型返回一个JSON数组，每个项目是同义物质的列表。
- en: '3.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Use a reflection prompt (REFLECT_PROMPT1) to re-evaluate the initial classification
    results, ensuring classification accuracy. This step checks if the substances
    within each group belong together and if two groups represent the same substance.
    Finally, output the final classification results.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用反射提示（REFLECT_PROMPT1）重新评估初始分类结果，以确保分类的准确性。此步骤检查每个组中的物质是否属于同一组，并检查两个组是否代表相同的物质。最后，输出最终的分类结果。
- en: This method is suitable for synonym merging tasks in materials chemistry, such
    as Metal Source, Organic Linker, and Solvent. We merged data with frequencies
    of 8/4/5 and above for Metal Source, Organic Linker, and Solvent, respectively,
    instead of merging all data. This reduces potential errors from merging low-frequency
    data and ensures fairness in subsequent comparisons.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法适用于材料化学中的同义词合并任务，例如金属源、 有机连接体和溶剂。我们对金属源、有机连接体和溶剂的数据进行了合并，频率为8/4/5及以上，而不是合并所有数据。这减少了合并低频数据可能带来的错误，并确保了后续比较的公平性。
- en: 5.3.2 Standardization of Numeric Conditions on Time and Temperature
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 数值条件在时间和温度上的标准化
- en: 'After processing the discrete names in the synthesis conditions, we continued
    to parse and capture numerical data for time and temperature. These data may have
    quality issues such as inconsistent units and the presence of special characters.
    To address these issues, we performed the following normalization:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理合成条件中的离散名称后，我们继续解析和捕获时间和温度的数值数据。这些数据可能存在质量问题，如单位不一致和特殊字符的存在。为了解决这些问题，我们进行了以下标准化：
- en: Extracting and Formatting Data
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 提取和格式化数据
- en: Using GPT-4, we extracted and formatted relevant data for time and temperature.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-4，我们提取并格式化了与时间和温度相关的数据。
- en: Unit Standardization
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 单位标准化
- en: We defined standard units for each data type. For example, time was standardized
    to hours, and temperature was standardized to Celsius (room temperature set at
    25^∘C).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每种数据类型定义了标准单位。例如，时间标准化为小时，温度标准化为摄氏度（室温设定为25^∘C）。
- en: Cleaning Special Characters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 清理特殊字符。
- en: Using regular expressions, we cleaned and formatted data that might contain
    special characters (such as spaces, commas, etc.). Through these steps, we ensured
    the integrity and usability of the data, laying a solid foundation for subsequent
    processing and analysis.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式，我们清理并格式化了可能包含特殊字符的数据（如空格、逗号等）。通过这些步骤，我们确保了数据的完整性和可用性，为后续处理和分析奠定了坚实的基础。
- en: 5.3.3 Data Filtering by Synthesis Condition Distributions
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 根据合成条件分布进行数据过滤
- en: After data cleansing and standardization, the distribution of different synthesis
    conditions becomes more centralized. As shown in Figure [10](#S5.F10 "Figure 10
    ‣ 5.3 Post-processing of Synthesis Conditions ‣ 5 Appendix ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations"), the entity lists of both
    metal source and solvent are shortened. The number of unique organic linkers remains
    high due to its long-tailed distribution. In the application of MOFs microstructure
    property inference, we will only select these MOFs synthesized by top entities
    in metal source, organic linker, and solvent. For example, by default we apply
    a filter of (100, 135, 20), which select the MOFs having top-100 metal source
    in the ranked list of Figure [10](#S5.F10 "Figure 10 ‣ 5.3 Post-processing of
    Synthesis Conditions ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations")(a), top-135 organic linker, and top-20 solvent.
    Note that for LLM models in comparison, different filters may be applied to ensure
    the same number of MOFs in the dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据清洗和标准化后，不同合成条件的分布变得更加集中。如图[10](#S5.F10 "图 10 ‣ 5.3 合成条件的后处理 ‣ 5 附录 ‣ 基于LLM的MOFs合成条件提取使用少量示范")所示，金属源和溶剂的实体列表都缩短了。由于其长尾分布，有机连接体的唯一数量保持较高。在MOFs微观结构属性推断的应用中，我们将仅选择由金属源、有机连接体和溶剂中前几个实体合成的这些MOFs。例如，默认情况下，我们应用(100,
    135, 20)的过滤器，选择图[10](#S5.F10 "图 10 ‣ 5.3 合成条件的后处理 ‣ 5 附录 ‣ 基于LLM的MOFs合成条件提取使用少量示范")(a)中排名前100的金属源、前135的有机连接体和前20的溶剂的MOFs。请注意，对于对比的LLM模型，可能会应用不同的过滤器以确保数据集中MOFs的数量相同。
- en: 5.3.4 Feature Embedding for Metal, Organic Linker, and Solvent Data
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4 金属、有机连接体和溶剂数据的特征嵌入
- en: 'After disambiguation and merging, we obtained high-quality precursor/solvent
    data. To build accurate predictive models, we need to perform corresponding feature
    embedding to capture the material/structural characteristics of the precursor/solvent
    data. The specific steps are as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在消歧和合并后，我们获得了高质量的前体/溶剂数据。为了构建准确的预测模型，我们需要执行相应的特征嵌入以捕捉前体/溶剂数据的材料/结构特征。具体步骤如下：
- en: Obtaining Chemical Formulas and SMILES
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 获取化学公式和SMILES
- en: Using GPT-4, we obtained the chemical formulas and SMILES for the top 100 Metals
    and the top 20 Solvents after disambiguation and merging. For Organic Linkers,
    due to the complexity of their naming, GPT-4 could not accurately obtain the corresponding
    SMILES. Therefore, we manually collected the SMILES for the top 135 Organic Linkers
    after disambiguation and merging.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-4，我们在消歧和合并后获得了前100种金属和前20种溶剂的化学公式和SMILES。对于有机连接体，由于其命名的复杂性，GPT-4未能准确获得相应的SMILES。因此，我们手动收集了消歧和合并后的前135种有机连接体的SMILES。
- en: Calculating Molecular Features
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 计算分子特性
- en: Based on the obtained SMILES, we used RDKit to calculate the molecular features
    of Metals, Organic Linkers, and Solvents, including molecular weight, LogP values,
    the number of hydrogen bond donors and acceptors, Labute surface area, maximum
    molecular distance, molecular length, width, height, and topological polar surface
    area (TPSA).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 基于获得的SMILES，我们使用RDKit计算了金属、有机连接体和溶剂的分子特性，包括分子量、LogP值、氢键供体和受体的数量、Labute表面积、最大分子距离、分子长度、宽度、高度和拓扑极性表面积（TPSA）。
- en: Calculating Metal Salt Features
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 计算金属盐特性
- en: Using the Composition class from Pymatgen, we automatically inferred and assigned
    oxidation states for the chemical formulas of metal salts. Using the MultipleFeaturizer
    class from the Matminer library, we calculated a series of chemical features,
    including elemental properties, atomic orbitals, electron affinity, and electronegativity
    differences. Additionally, we included features of the metal elements contained
    in the MOFs, such as atomic mass, atomic radius, thermal conductivity, and detailed
    electronic configuration vector representations. These features provide more comprehensive
    elemental property information for in-depth analysis of the performance and behavior
    of MOFs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pymatgen中的Composition类，我们自动推断并分配了金属盐化学公式的氧化态。使用Matminer库中的MultipleFeaturizer类，我们计算了一系列化学特征，包括元素属性、原子轨道、电子亲和力和电负性差异。此外，我们还包括了MOFs中金属元素的特征，如原子质量、原子半径、热导率和详细的电子配置矢量表示。这些特征提供了更全面的元素属性信息，以便对MOFs的性能和行为进行深入分析。
- en: 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 可视化MOFs综合条件提取引擎和数据库
- en: Database and Engine
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库和引擎
- en: 'To streamline the entire workflow and efficiently organize the extraction results
    from related papers, we developed the Visual MOFs Synthesis Extraction Engine
    and Database. Using our approach, we processed over 30,000 papers and extracted
    57,081 synthesis paragraphs, on which we then performed synthesis condition extraction.
    To better view and analyze the vast amount of extraction results, we built a comprehensive
    database with 2 features: 1) Basic Statistics: The database provides basic statistics
    on all extraction results, including data on synthesis paragraphs and various
    synthesis conditions (Figure [11](#S5.F11 "Figure 11 ‣ 5.4 Visual MOFs Synthesis
    Condition Extraction Engine and Database ‣ 5 Appendix ‣ LLM-based MOFs Synthesis
    Condition Extraction using Few-Shot Demonstrations")). 2) Advanced Search Capabilities:
    This database is designed to support logical expression searches for specific
    fields, allowing users to search for synthesis conditions, paper titles, and synthesis
    paragraph content with precision, and enables visualization of the retrieval results.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化整个工作流程并高效组织相关文献的提取结果，我们开发了可视化MOFs综合提取引擎和数据库。使用我们的方法，我们处理了超过30,000篇文献，提取了57,081个综合段落，然后对这些段落进行了综合条件提取。为了更好地查看和分析大量的提取结果，我们建立了一个具有2个功能的综合数据库：1）基础统计：数据库提供了所有提取结果的基础统计数据，包括综合段落和各种综合条件的数据（图[11](#S5.F11
    "图 11 ‣ 5.4 可视化MOFs综合条件提取引擎和数据库 ‣ 5 附录 ‣ 基于LLM的MOFs综合条件提取使用少样本示范")）。2）高级搜索功能：该数据库支持对特定字段的逻辑表达式搜索，允许用户精准搜索综合条件、文献标题和综合段落内容，并能够可视化检索结果。
- en: '![Refer to caption](img/fd809d8db1f3a9c3c9912f2b5a4caea2.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fd809d8db1f3a9c3c9912f2b5a4caea2.png)'
- en: 'Figure 11: Statistics on the database containing all the extraction results.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：包含所有提取结果的数据库统计信息。
- en: 'An entire process is integrated, from uploading synthesis papers, format conversion,
    paragraph and condition extraction, to the visualization of extraction results:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程集成了从上传综合文献、格式转换、段落和条件提取，到提取结果可视化的所有步骤：
- en: '1.'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Upload and Standardization: Users can upload synthesis papers, which are then
    automatically converted into a standardized format suitable for condition extraction.'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上传和标准化：用户可以上传综合文献，这些文献会自动转换为适合条件提取的标准化格式。
- en: '2.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Automatic Paragraph Extraction: The system will automatically extract synthesis
    paragraphs from the uploaded papers for users to select the paragraphs to process
    and proceed with synthesis condition extraction.'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动段落提取：系统将自动从上传的文献中提取综合段落，以便用户选择处理的段落，并继续进行综合条件提取。
- en: '3.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Configurable Extraction: The engine supports configuration for synthesis condition
    extraction, allowing users to adjust the sample quantity and selection method
    input into the large model.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可配置提取：该引擎支持对综合条件提取的配置，允许用户调整样本数量和输入大模型的选择方法。
- en: '4.'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Organized and Visualized Data: The extracted conditions are systematically
    organized and visualized for data interpretation and analysis.'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 组织和可视化数据：提取的条件被系统地组织和可视化，以便进行数据解释和分析。
- en: Synthesis Visualization
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 综合可视化
- en: '![Refer to caption](img/87e7c83a887b90e53d07a39386b124dc.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/87e7c83a887b90e53d07a39386b124dc.png)'
- en: 'Figure 12: Visualization interface for illustrating the synthesis extraction
    process and result.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：可视化界面，用于说明合成提取过程和结果。
- en: The visualization system we designed can support users in analyzing synthesis
    paragraphs. Initially, users upload batch PDF papers and process through the LLM.
    Once extraction is complete, users can utilize the filtering panel to select specific
    paragraphs for analysis. The overall performance panel (Fig.[12](#S5.F12 "Figure
    12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database ‣ 5 Appendix
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(a))
    then displays four key performance metrics of the LLM resolution, with a default
    HeatMap (Fig.[12](#S5.F12 "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction
    Engine and Database ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction
    using Few-Shot Demonstrations")(b).I) providing a detailed view of entity resolution
    performance across all evaluation metrics. Suppose further detail on specific
    metrics is needed. In that case, users can access the second tab (Fig.[12](#S5.F12
    "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b).II),
    sliding down to the relevant rows to view the distribution of paragraph performance
    across various parameters in bar charts. To explore similarities with other paragraphs
    in the database, users can switch to the third tab (Fig.[12](#S5.F12 "Figure 12
    ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database ‣ 5 Appendix
    ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b).III).
    Here, red dots indicate newly extracted paragraphs; users can look for nearby
    black dots representing similar paragraphs in the database to compare specific
    composite parameters. Should users decide to replace or re-examine certain paragraphs,
    they can reselect them in the filtering panel (Fig.[12](#S5.F12 "Figure 12 ‣ 5.4
    Visual MOFs Synthesis Condition Extraction Engine and Database ‣ 5 Appendix ‣
    LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(c)).
    This action triggers an automatic update of the corresponding performance metrics
    and visual charts, allowing users to repeat the analysis as needed.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计的可视化系统可以支持用户分析合成段落。最初，用户上传批量PDF文献并通过LLM处理。提取完成后，用户可以利用过滤面板选择特定段落进行分析。总体性能面板（图。[12](#S5.F12
    "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(a)）然后显示LLM解析的四个关键性能指标，默认的热图（图。[12](#S5.F12
    "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b).I）提供了一个详细的实体解析性能视图，涵盖所有评估指标。如果需要进一步了解特定指标，用户可以访问第二个标签（图。[12](#S5.F12
    "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b).II），滑动到相关行以查看条形图中不同参数下段落性能的分布。要探索与数据库中其他段落的相似性，用户可以切换到第三个标签（图。[12](#S5.F12
    "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(b).III）。在这里，红点表示新提取的段落；用户可以寻找附近的黑点，代表数据库中相似的段落，以比较特定的综合参数。如果用户决定替换或重新检查某些段落，他们可以在过滤面板中重新选择（图。[12](#S5.F12
    "Figure 12 ‣ 5.4 Visual MOFs Synthesis Condition Extraction Engine and Database
    ‣ 5 Appendix ‣ LLM-based MOFs Synthesis Condition Extraction using Few-Shot Demonstrations")(c)）。此操作将自动更新相应的性能指标和视觉图表，允许用户根据需要重复分析。
- en: References
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] GPT-4, OpenAI. [https://openai.com/index/gpt-4/](https://openai.com/index/gpt-4/).
    Retrieved on 2024-01-25.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] GPT-4，OpenAI。 [https://openai.com/index/gpt-4/](https://openai.com/index/gpt-4/)。检索日期：2024年1月25日。'
- en: '[2] What is a MOF, MOF Commission of the International Zeolite Association.
    [https://www.iza-online.org/MOF/MOFforIZA.pdf](https://www.iza-online.org/MOF/MOFforIZA.pdf).
    Retrieved on 2024-07-10.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 什么是MOF，国际沸石协会MOF委员会。 [https://www.iza-online.org/MOF/MOFforIZA.pdf](https://www.iza-online.org/MOF/MOFforIZA.pdf)。检索日期：2024年7月10日。'
- en: '[3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint
    arXiv:2303.08774, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat, 等等。Gpt-4技术报告。arXiv预印本arXiv:2303.08774，2023年。'
- en: '[4] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: A next-generation
    hyperparameter optimization framework. In The 25th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining, pages 2623–2631, 2019.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Akiba, S. Sano, T. Yanase, T. Ohta, 和 M. Koyama. Optuna：下一代超参数优化框架。在第25届
    ACM SIGKDD 国际知识发现与数据挖掘会议上，第 2623–2631 页，2019。'
- en: '[5] A. H. Alawadhi, S. Chheda, G. D. Stroscio, Z. Rong, D. Kurandina, H. L.
    Nguyen, N. Rampal, Z. Zheng, L. Gagliardi, and O. M. Yaghi. Harvesting water from
    air with high-capacity, stable furan-based metal–organic frameworks. Journal of
    the American Chemical Society, 146(3):2160–2166, 2024.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. H. Alawadhi, S. Chheda, G. D. Stroscio, Z. Rong, D. Kurandina, H. L.
    Nguyen, N. Rampal, Z. Zheng, L. Gagliardi, 和 O. M. Yaghi. 利用高容量、稳定的呋喃基金属有机框架从空气中收集水。《美国化学学会志》，146(3):2160–2166,
    2024。'
- en: '[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners.
    Advances in neural information processing systems, 33:1877–1901, 2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell, 等. 语言模型是少量样本学习者。《神经信息处理系统进展》，33:1877–1901,
    2020。'
- en: '[7] J. Dagdelen, A. Dunn, S. Lee, N. Walker, A. S. Rosen, G. Ceder, K. A. Persson,
    and A. Jain. Structured information extraction from scientific text with large
    language models. Nature Communications, 15(1):1418, 2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Dagdelen, A. Dunn, S. Lee, N. Walker, A. S. Rosen, G. Ceder, K. A. Persson,
    和 A. Jain. 从科学文本中提取结构化信息：利用大型语言模型。《自然通讯》，15(1):1418, 2024。'
- en: '[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of
    deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
    2018.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova. Bert：用于语言理解的深度双向变换器的预训练。arXiv
    预印本 arXiv:1810.04805, 2018。'
- en: '[9] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui.
    A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, 和 Z.
    Sui. 关于上下文学习的综述。arXiv 预印本 arXiv:2301.00234, 2022。'
- en: '[10] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang.
    Retrieval-augmented generation for large language models: A survey. arXiv preprint
    arXiv:2312.10997, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, 和 H.
    Wang. 针对大型语言模型的检索增强生成：综述。arXiv 预印本 arXiv:2312.10997, 2023。'
- en: '[11] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel.
    Few-shot parameter-efficient fine-tuning is better and cheaper than in-context
    learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, 和 C. A. Raffel.
    少量样本参数高效微调优于上下文学习且成本更低。《神经信息处理系统进展》，35:1950–1965, 2022。'
- en: '[12] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes
    good in-context examples for gpt-$3$? arXiv preprint arXiv:2101.06804, 2021.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, 和 W. Chen. 什么样的上下文示例适合
    gpt-$3$？arXiv 预印本 arXiv:2101.06804, 2021。'
- en: '[13] Y. Luo, S. Bag, O. Zaremba, A. Cierpka, J. Andreo, S. Wuttke, P. Friederich,
    and M. Tsotsalas. Mof synthesis prediction enabled by automatic data mining and
    machine learning. Angewandte Chemie International Edition, 61(19):e202200242,
    2022.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Luo, S. Bag, O. Zaremba, A. Cierpka, J. Andreo, S. Wuttke, P. Friederich,
    和 M. Tsotsalas. 基于自动数据挖掘和机器学习的 mof 合成预测。《应用化学国际版》，61(19):e202200242, 2022。'
- en: '[14] P. Z. Moghadam, A. Li, S. B. Wiggin, A. Tao, A. G. Maloney, P. A. Wood,
    S. C. Ward, and D. Fairen-Jimenez. Development of a cambridge structural database
    subset: a collection of metal–organic frameworks for past, present, and future.
    Chemistry of Materials, 29(7):2618–2625, 2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] P. Z. Moghadam, A. Li, S. B. Wiggin, A. Tao, A. G. Maloney, P. A. Wood,
    S. C. Ward, 和 D. Fairen-Jimenez. 剑桥结构数据库子集的开发：过去、现在和未来的金属有机框架集合。《材料化学》，29(7):2618–2625,
    2017。'
- en: '[15] M. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, and Y. Elazar. Few-shot
    fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint
    arXiv:2305.16938, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, 和 Y. Elazar. 少量样本微调与上下文学习：公平比较和评估。arXiv
    预印本 arXiv:2305.16938, 2023。'
- en: '[16] M. P. Polak and D. Morgan. Extracting accurate materials data from research
    papers with conversational language models and prompt engineering. Nature Communications,
    15(1):1569, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. P. Polak 和 D. Morgan. 使用对话语言模型和提示工程从研究论文中提取准确的材料数据。《自然通讯》，15(1):1569,
    2024。'
- en: '[17] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese
    bert-networks. arXiv preprint arXiv:1908.10084, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] N. Reimers 和 I. Gurevych. Sentence-bert：使用 siamese bert 网络的句子嵌入。arXiv
    预印本 arXiv:1908.10084, 2019。'
- en: '[18] S. Robertson, H. Zaragoza, et al. The probabilistic relevance framework:
    Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333–389,
    2009.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Robertson, H. Zaragoza 等人. 概率相关框架：Bm25 及其扩展. 《信息检索基础与趋势》, 3(4):333–389,
    2009.'
- en: '[19] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf,
    L. Zettlemoyer, N. A. Smith, et al. Selective annotation makes language models
    better few-shot learners. arXiv preprint arXiv:2209.01975, 2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf,
    L. Zettlemoyer, N. A. Smith 等人. 选择性注释使语言模型更具少量学习能力. arXiv 预印本 arXiv:2209.01975,
    2022.'
- en: '[20] H. Tian, W. Liu, and other contributors. pdf2htmlex. [https://github.com/pdf2htmlEX/pdf2htmlEX](https://github.com/pdf2htmlEX/pdf2htmlEX),
    2024. Accessed: 2024-07-18.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] H. Tian, W. Liu 及其他贡献者. pdf2htmlex. [https://github.com/pdf2htmlEX/pdf2htmlEX](https://github.com/pdf2htmlEX/pdf2htmlEX),
    2024. 访问日期：2024-07-18.'
- en: '[21] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar,
    J. Spencer-Smith, and D. C. Schmidt. A prompt pattern catalog to enhance prompt
    engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar,
    J. Spencer-Smith 和 D. C. Schmidt. 提升 ChatGPT 提示工程的提示模式目录. arXiv 预印本 arXiv:2302.11382,
    2023.'
- en: '[22] T. F. Willems, C. H. Rycroft, M. Kazi, J. C. Meza, and M. Haranczyk. Algorithms
    and tools for high-throughput geometry-based analysis of crystalline porous materials.
    Microporous and Mesoporous Materials, 149(1):134–141, 2012.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] T. F. Willems, C. H. Rycroft, M. Kazi, J. C. Meza 和 M. Haranczyk. 用于高通量几何分析晶体多孔材料的算法和工具.
    《微孔与中孔材料》, 149(1):134–141, 2012.'
- en: '[23] O. M. Yaghi, M. O’Keeffe, N. W. Ockwig, H. K. Chae, M. Eddaoudi, and J. Kim.
    Reticular synthesis and the design of new materials. Nature, 423(6941):705–714,
    2003.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] O. M. Yaghi, M. O’Keeffe, N. W. Ockwig, H. K. Chae, M. Eddaoudi, 和 J.
    Kim. 网状合成与新材料设计. 《自然》, 423(6941):705–714, 2003.'
- en: '[24] Z. Zheng, O. Zhang, C. Borgs, J. T. Chayes, and O. M. Yaghi. Chatgpt chemistry
    assistant for text mining and the prediction of mof synthesis. Journal of the
    American Chemical Society, 145(32):18048–18062, 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Z. Zheng, O. Zhang, C. Borgs, J. T. Chayes 和 O. M. Yaghi. 用于文本挖掘和 MOF
    合成预测的 ChatGPT 化学助手. 《美国化学学会杂志》, 145(32):18048–18062, 2023.'
