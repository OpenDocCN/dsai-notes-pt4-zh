- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Advanced Large Language Model (LLM) - Driven Verilog Development: Enhancing
    Power, Performance, and Area Optimization in Code Synthesis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级大型语言模型（LLM）驱动的Verilog开发：提升代码合成中的功耗、性能和面积优化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.01022](https://ar5iv.labs.arxiv.org/html/2312.01022)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.01022](https://ar5iv.labs.arxiv.org/html/2312.01022)
- en: Kiran Thorat^($\ast$),
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kiran Thorat^($\ast$),
- en: Hongwu Peng^($\ast$),
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hongwu Peng^($\ast$),
- en: Jeff Zhang^($\dagger$)Arizona State University )
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Jeff Zhang^($\dagger$)亚利桑那州立大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The increasing use of Advanced Language Models (ALMs) in diverse sectors, particularly
    due to their impressive capability to generate top-tier content following linguistic
    instructions, forms the core of this investigation. This study probes into ALMs’
    deployment in electronic hardware design, with a specific emphasis on the synthesis
    and enhancement of Verilog programming. We introduce an innovative framework,
    crafted to assess and amplify ALMs’ productivity in this niche. The methodology
    commences with the initial crafting of Verilog programming via ALMs, succeeded
    by a distinct dual-stage refinement protocol. The premier stage prioritizes augmenting
    the code’s operational and linguistic precision, while the latter stage is dedicated
    to aligning the code with Power-Performance-Area (PPA) benchmarks, a pivotal component
    in proficient hardware design. This bifurcated strategy, merging error remediation
    with PPA enhancement, has yielded substantial upgrades in the caliber of ALM-created
    Verilog programming. Our framework achieves an 81.37% rate in linguistic accuracy
    and 62.0% in operational efficacy in programming synthesis, surpassing current
    leading-edge techniques, such as 73% in linguistic accuracy and 46% in operational
    efficacy. These findings illuminate ALMs’ aptitude in tackling complex technical
    domains and signal a positive shift in the mechanization of hardware design operations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 高级语言模型（ALMs）在各个领域的日益广泛应用，尤其是由于它们在按照语言指令生成顶级内容方面的卓越能力，构成了本研究的核心。本研究探讨了ALMs在电子硬件设计中的应用，特别是对Verilog编程的合成和增强的关注。我们提出了一个创新框架，旨在评估和提升ALMs在这一领域的生产力。该方法论从通过ALMs初步编写Verilog编程开始，然后进行独特的双阶段改进协议。首个阶段优先提高代码的操作和语言精确度，而后阶段则致力于将代码与功耗-性能-面积（PPA）基准对齐，这在高效硬件设计中是关键组成部分。这种将错误修正与PPA提升相结合的双重策略，已在ALM创建的Verilog编程质量上取得了显著提升。我们的框架在语言准确率上达到了81.37%的水平，在编程合成的操作效能上达到了62.0%，超越了目前领先的技术，如73%的语言准确率和46%的操作效能。这些发现揭示了ALMs在处理复杂技术领域的能力，并预示着硬件设计操作机械化的积极变化。
- en: Keywords—LLM, EDA, Hardware Description
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词—LLM、EDA、硬件描述
- en: 1   Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1   引言
- en: With Moore’s law driving increased design complexity and chip capacity, VLSI
    design and verification require more effort. Machine learning (ML) has successfully
    integrated into EDA for logic synthesis [[8](#bib.bib8), [10](#bib.bib10)], placement
    [[25](#bib.bib25)], routing [[12](#bib.bib12), [16](#bib.bib16)], testing [[5](#bib.bib5),
    [24](#bib.bib24), [14](#bib.bib14)], and verification [[7](#bib.bib7), [11](#bib.bib11)].
    The popularity of agile hardware design exploration has been on the rise due to
    the growth of large language models (LLMs). A promising direction is using natural
    language instruction to generate hardware description language (HDL). e.g., Verilog,
    aiming to greatly lower hardware design barriers and increase design productivity,
    especially for users who do not possess extensive expertise in chip design. Despite
    the efforts, Verilog benchmarking has unique challenges in terms of the wide range
    of hardware designs [[13](#bib.bib13)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着摩尔定律推动设计复杂性和芯片容量的增加，VLSI设计和验证需要更多的努力。机器学习（ML）已成功整合进EDA的逻辑合成[[8](#bib.bib8)、[10](#bib.bib10)]，布置[[25](#bib.bib25)]，布线[[12](#bib.bib12)、[16](#bib.bib16)]，测试[[5](#bib.bib5)、[24](#bib.bib24)、[14](#bib.bib14)]，以及验证[[7](#bib.bib7)、[11](#bib.bib11)]。由于大型语言模型（LLMs）的增长，敏捷硬件设计探索的普及也在上升。一个有前途的方向是使用自然语言指令生成硬件描述语言（HDL），例如Verilog，旨在大大降低硬件设计的障碍，并提高设计生产力，尤其是对那些没有广泛芯片设计专业知识的用户。尽管已有努力，Verilog基准测试在广泛的硬件设计范围内仍面临独特挑战[[13](#bib.bib13)]。
- en: Two orthogonal research and development trends have both attracted enormous
    interests [[22](#bib.bib22), [15](#bib.bib15), [13](#bib.bib13), [2](#bib.bib2),
    [4](#bib.bib4)]. The first trend is efficiently finetuning LLMs such as CodeGen [[17](#bib.bib17)],
    with representatives works such as Thakur et al. [[22](#bib.bib22)], Chip-Chat [[2](#bib.bib2)],
    Chip-GPT [[4](#bib.bib4)]. However, due to limited Verilog data sources, these
    works mainly target the scale of simple and small circuits (e.g., <20 designs
    with a medium of <45 HDL lines) [[15](#bib.bib15)]. The relatively low scalability
    and solution quality have propelled the second trend – enrich Verilog source.
    Like oil, data is an immensely valuable resource. One could not generate high
    quality and comprehensive HDL codes without having LLMs trained on vast amount
    of such data. RTLLM [[15](#bib.bib15)] and VerilogEval [[13](#bib.bib13)] introduce
    specialized benchmarking framework (i.e., 30 designs from RTLLm and 156 designs
    from HDLBits [[9](#bib.bib9)] from VerilogEval) to assess the generation quality
    of LLMs. However, they either do not offer Power, Performance, and Area (PPA)
    analysis for the generated codes (e.g., VerilogEval), or the generated Verilog
    codes are directly extracted and synthesized using commercial tools to obtain
    PPA results, without considering PPA feedback (e.g., RTLLM). Thus, their solution
    quality is still limited.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 两个正交的研究和开发趋势都引起了极大的关注 [[22](#bib.bib22), [15](#bib.bib15), [13](#bib.bib13),
    [2](#bib.bib2), [4](#bib.bib4)]。第一个趋势是高效微调 LLMs，如 CodeGen [[17](#bib.bib17)]，代表作包括
    Thakur 等 [[22](#bib.bib22)]、Chip-Chat [[2](#bib.bib2)] 和 Chip-GPT [[4](#bib.bib4)]。然而，由于
    Verilog 数据源有限，这些工作主要针对简单和小型电路的规模（例如，<20 个设计，平均 <45 行 HDL 代码）[[15](#bib.bib15)]。相对较低的可扩展性和解决方案质量促成了第二个趋势——丰富
    Verilog 源数据。像石油一样，数据是一种极其宝贵的资源。如果没有在大量数据上训练的 LLMs，就无法生成高质量和全面的 HDL 代码。RTLLM [[15](#bib.bib15)]
    和 VerilogEval [[13](#bib.bib13)] 引入了专门的基准框架（即 RTLLm 的 30 个设计和 VerilogEval 的 156
    个设计来自 HDLBits [[9](#bib.bib9)]）来评估 LLMs 的生成质量。然而，它们要么没有对生成的代码提供功耗、性能和面积（PPA）分析（例如，VerilogEval），要么直接提取和合成生成的
    Verilog 代码以获得 PPA 结果，而不考虑 PPA 反馈（例如，RTLLM）。因此，它们的解决方案质量仍然有限。
- en: 'In this work, as the first attempt, we integrate power, performance, and area-constraints
    into Verilog generation, and propose VeriPPA, a open-source framework with multi-round
    Verilog generation and error feedback, shown in Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"). We first
    generate initial Verilog codes using LLMs, followed by a unique two-stage refinement
    process. The first stage focuses on improving the syntax and functionality, while
    the second stage aims to optimize the code in line with PPA constraints, an essential
    aspect to ensure hardware design quality. Compared with state-of-the-arts (SOTAs),
    e.g., RTLLM [[15](#bib.bib15)], VerilogEval [[13](#bib.bib13)], our VeriPPA achieves
    a success rate of 62.0% (+16%) for functional accuracy and 81.37% (+8.3%) for
    syntactic correctness in Verilog code generation. Our key contributions are summarized
    here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，作为首次尝试，我们将功耗、性能和面积约束整合到 Verilog 生成中，并提出了 VeriPPA，一个具有多轮 Verilog 生成和错误反馈的开源框架，如图
    [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 基于高级大语言模型 (LLM) 的 Verilog 开发：提升代码综合中的功耗、性能和面积优化") 所示。我们首先使用
    LLMs 生成初步的 Verilog 代码，然后进行独特的两阶段优化过程。第一阶段侧重于改进语法和功能，而第二阶段旨在根据 PPA 约束优化代码，这是确保硬件设计质量的一个关键方面。与先进技术（SOTAs）相比，例如
    RTLLM [[15](#bib.bib15)] 和 VerilogEval [[13](#bib.bib13)]，我们的 VeriPPA 在 Verilog
    代码生成中实现了 62.0% (+16%) 的功能准确率和 81.37% (+8.3%) 的语法正确率。我们的主要贡献总结如下：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We use the detailed error diagnostics from the iverilog simulator [[26](#bib.bib26)],
    and pinpoint the exact location of syntactic or functional discrepancies as indicated
    by testbench failures as new prompts. We use multi-round generation to enhance
    the syntax and functionality correctness.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用来自 iverilog 模拟器 [[26](#bib.bib26)] 的详细错误诊断，并将测试平台失败所指示的语法或功能差异的确切位置作为新的提示。我们使用多轮生成来增强语法和功能的正确性。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To further ensure that the generated Verilog codes are *synthesizable*, and
    design quality (PPA) is sound, we use Synopsys Design Compiler to perform logic
    synthesis (and technology mapping) on the open source ASAP 7nm Predictive PDK,
    and check all designs’ warnings/errors, and PPA report. We then integrate these
    PPA reports and warnings/errors with our PPA goal into the next round prompt for
    further refinement.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为进一步确保生成的Verilog代码是*synthesizable*，且设计质量（PPA）良好，我们使用Synopsys Design Compiler对开源ASAP
    7nm预测PDK进行逻辑综合（和技术映射），并检查所有设计的警告/错误和PPA报告。然后，我们将这些PPA报告和警告/错误与我们的PPA目标整合到下一轮提示中以进行进一步优化。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We incorporate in-context learning (ICL) to significantly improve the LLM performance
    in generating Verilog codes with only a few demonstration examples, especially
    when labeled data are scarce. By carefully selecting diverse text-to-Verilog pairs,
    ICL demonstrates superior performance and generalization capabilities compared
    to fine-tuning in limited example scenarios, thus increasing the performance of
    Verilog code generation.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们采用上下文学习（ICL），以显著提升LLM在仅有少量示例的情况下生成Verilog代码的性能，尤其是在标注数据稀缺时。通过精心选择多样的文本到Verilog对，ICL表现出比在有限示例场景下微调更优越的性能和泛化能力，从而提高了Verilog代码生成的性能。
- en: '![Refer to caption](img/4d6d0d6ae46ec572cdfb3e88c4804aef.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4d6d0d6ae46ec572cdfb3e88c4804aef.png)'
- en: 'Figure 1: This visualization captures the step-by-step process where an LLM
    synthesizes Verilog codes from hardware design prompts, with the ensuing code
    subjected to thorough validation by a Simulator and scrutinized for adherence
    to Power-Performance-Area (PPA) checks.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：此可视化图捕捉了LLM从硬件设计提示中合成Verilog代码的逐步过程，生成的代码经过模拟器的彻底验证，并检查是否符合功率-性能-面积（PPA）要求。
- en: 2   Background and Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: There are mainly two directions related to Verilog generation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与Verilog生成相关的主要有两个方向。
- en: Finetune LLMs. Thakur et al. [[22](#bib.bib22)] advocate for the fine-tuning
    of open-source LLMs such as CodeGen [[17](#bib.bib17)] to specifically generate
    Verilog code tailored for target designs. Subsequently, Chip-Chat [[2](#bib.bib2)]
    delves into the intricacies of hardware design using LLMs, highlighting the markedly
    superior performance of ChatGPT compared to other open-source LLMs. Chip-GPT [[4](#bib.bib4)]
    also focuses on the task of RTL design by leveraging the capabilities of ChatGPT.
    These studies pave the way for a promising future where language models play a
    pivotal role in facilitating and enhancing various aspects of agile hardware design
    exploration. However, these works mainly target the scale of simple and small
    circuits (e.g., ¡20 designs with a medium of ¡45 HDL lines), as pointed out in [[15](#bib.bib15)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLM。Thakur等人 [[22](#bib.bib22)] 主张对开源LLM（如CodeGen [[17](#bib.bib17)]）进行微调，以专门生成针对目标设计的Verilog代码。随后，Chip-Chat [[2](#bib.bib2)]
    深入探讨了使用LLM进行硬件设计的复杂性，强调了ChatGPT相较于其他开源LLM的显著优越性能。Chip-GPT [[4](#bib.bib4)] 也专注于利用ChatGPT能力进行RTL设计。这些研究为语言模型在促进和增强各种灵活硬件设计探索方面的关键作用铺平了道路。然而，这些工作主要针对简单和小规模电路（例如，¡20个设计，平均¡45行HDL），正如 [[15](#bib.bib15)]
    所指出的。
- en: Enrich Verilog Source. Like oil, data is an immensely valuable resource. Recent
    efforts have been focusing on enriching the Verilog data source. RTLLM [[15](#bib.bib15)]
    introduces a benchmarking framework consisting of 30 designs that are specifically
    aimed at enhancing the scalability of benchmark designs. Furthermore, it utilizes
    effective prompt engineering techniques to improve the generation quality. VerilogEval [[13](#bib.bib13)]
    assesses the performance of LLM in the realm of Verilog code generation for hardware
    design and verification. It comprises 156 problems from the Verilog instructional
    website HDLBits. However, VerilogEval [[13](#bib.bib13)] does not offer PPA analysis
    for the generated codes. In RTLLM, the generated Verilog codes are directly extracted
    and synthesized using commercial tools to obtain PPA results, without PPA constraint-based
    feedback. Thus they suffer from limited generation quality.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 丰富 Verilog 源码。数据如同石油，是一种极其宝贵的资源。近期的努力集中在丰富 Verilog 数据源。RTLLM [[15](#bib.bib15)]
    引入了一个由 30 个设计组成的基准测试框架，专门旨在提升基准设计的可扩展性。此外，它利用有效的提示工程技术来提高生成质量。VerilogEval [[13](#bib.bib13)]
    评估了 LLM 在 Verilog 代码生成用于硬件设计和验证领域的表现。它包含了来自 Verilog 教学网站 HDLBits 的 156 个问题。然而，VerilogEval
    [[13](#bib.bib13)] 并未提供生成代码的 PPA 分析。在 RTLLM 中，生成的 Verilog 代码直接提取并使用商业工具综合以获得 PPA
    结果，而没有基于 PPA 约束的反馈。因此，它们在生成质量上受到限制。
- en: 3   VeriPPA FRAMEWORK
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3   VeriPPA 框架
- en: 3.1   Design Overview
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1   设计概述
- en: 'In our VeriPPA framework, as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"), we use
    a text-based Description of Hardware Design, designated as $L$ is then subjected
    to a rigorous validation sequence, beginning with a Simulator that checks both
    syntax and functionality. In the first loop, if unsuccessful, we will input the
    outcomes, along with any syntax and functionality errors, into the LLM for the
    generation of subsequent attempts. If successful, the code undergoes Power-Performance-Area
    (PPA) checks to ensure compliance with constraints. In the second loop, we check
    all designs’ warnings/errors during logic synthesis and PPA reports. If not satisfied,
    both the design and its corresponding PPA report will be fed back to the VeriRectify
    (Section [3.3](#S3.SS3 "3.3 VeriRectify ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large
    Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis")) for refinement. This validation workflow
    ensures that the LLM-generated Verilog codes not only meets functional specifications
    but is also optimized for PPA considerations.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的 VeriPPA 框架中，如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Advanced Large
    Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis") 所示，我们使用基于文本的硬件设计描述，指定为 $L$，然后经过严格的验证序列，首先由一个模拟器检查语法和功能。在第一次循环中，如果验证失败，我们将结果以及任何语法和功能错误输入
    LLM，以生成后续尝试。如果验证成功，代码将接受功率-性能-面积（PPA）检查，以确保符合约束。在第二次循环中，我们检查所有设计在逻辑综合和 PPA 报告期间的警告/错误。如果不满意，设计及其对应的
    PPA 报告将反馈到 VeriRectify（第 [3.3](#S3.SS3 "3.3 VeriRectify ‣ 3 VeriPPA FRAMEWORK
    ‣ Advanced Large Language Model (LLM) - Driven Verilog Development: Enhancing
    Power, Performance, and Area Optimization in Code Synthesis") 节）进行改进。这一验证工作流确保了
    LLM 生成的 Verilog 代码不仅符合功能规范，而且在 PPA 方面也经过优化。'
- en: 3.2   Code Generation and Testing
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2   代码生成和测试
- en: Utilizing design descriptions $L$ are inputs to the ICARUS Verilog simulator
    [[26](#bib.bib26)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用设计描述 $L$ 作为 ICARUS Verilog 模拟器 [[26](#bib.bib26)] 的输入。
- en: that systematically assess the code’s functionality, encompassing a wide array
    of test scenarios.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 系统地评估代码的功能，涵盖广泛的测试场景。
- en: The integration of the ICARUS simulator into in VeriPPA framework enhances the
    verification process, enabling an evaluation function $V$) that provides immediate
    feedback on the code’s syntactical and operational integrity. This integrated
    approach contrasts with frameworks such as RTLLM [[15](#bib.bib15)], where an
    external simulator is used to check the correctness of the generated Verilog codes.
    Since taking out the generated code can not provide instant detailed diagnostics
    of the code. Therefore, we need an integrated approach to get the detailed diagnostics
    of errors. With VeriPPA, detailed error reports from the simulator expedite the
    identification of faults, facilitating their rectification and refining the code
    generation process. This iterative, error-informed development cycle is discussed
    in further detail in the following sections, illustrating how precise error detection
    informs subsequent iterations of code generation within VeriPPA framework.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ICARUS 模拟器的集成到 VeriPPA 框架中增强了验证过程，使得评估函数 $V$) 能够提供对代码的语法和操作完整性的即时反馈。这种集成方法与诸如
    RTLLM [[15](#bib.bib15)] 的框架形成对比，后者使用外部模拟器检查生成的 Verilog 代码的正确性。由于提取生成的代码无法提供即时的详细诊断，因此我们需要一种集成的方法来获得错误的详细诊断。通过
    VeriPPA，来自模拟器的详细错误报告加快了故障的识别，促进了故障的修复，并改进了代码生成过程。这种迭代的、以错误为基础的开发周期将在以下章节中进一步讨论，展示了精确的错误检测如何指导
    VeriPPA 框架内代码生成的后续迭代。
- en: '![Refer to caption](img/eaf75f0e1899e83aad0fb0c83bde4fd0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eaf75f0e1899e83aad0fb0c83bde4fd0.png)'
- en: 'Figure 2: The diagram illustrates the process of syntactic and functional code
    verification.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 该图展示了语法和功能代码验证的过程。'
- en: 3.3   VeriRectify
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 VeriRectify
- en: In our approach, we leverage detailed error diagnostics from the simulator for
    generated codes. This refinement process, termed ”VeriRectify,” is pivotal to
    ensuring the integrity of the generated Verilog codes. The VeriRectify phase ingests
    diagnostics from the iverilog simulator [[26](#bib.bib26)], pinpointing the exact
    location of syntactic discrepancies or functional discrepancies as indicated by
    testbench failures. This diagnostic data, when amalgamated with the antecedent
    code generation attempt in the process where the LLM generates Verilog code. The
    LLM then employs this enriched prompt to rectify and evolve the Verilog code toward
    compliance with specified correctness criteria, following the iterative relation
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方法中，我们利用来自模拟器的详细错误诊断来处理生成的代码。这个精炼过程被称为“VeriRectify”，对确保生成的 Verilog 代码的完整性至关重要。VeriRectify
    阶段处理来自 iverilog 模拟器的诊断 [[26](#bib.bib26)]，精准定位语法或功能差异的位置，如测试基准失败所示。这些诊断数据与前述的代码生成尝试结合在一起，LLM
    然后利用这些丰富的提示来修正和改进 Verilog 代码，以符合指定的正确性标准，遵循迭代关系。
- en: '|  | $V(n+1)=V(n)-E(V(n))$ |  | (1) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(n+1)=V(n)-E(V(n))$ |  | (1) |'
- en: 'where $V(n)$ represents the identified errors. The VeriRectify workflow is
    depicted in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Code Generation and Testing ‣ 3 VeriPPA
    FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"). The figure’s
    top section displays the Syntax Error and Functional Error found in the output
    of our simulation tool (e.g., booth_multiplier), which is the output from our
    simulation tool. During the rectification phase (Error Refinement), we utilize
    a tailored prompt (integrating the Error Details) for the LLMs, enabling it to
    correct the flaws identified in the Verilog codes initially produced. This approach
    allows the LLM to specifically address the encountered issues, which is fundamentally
    different from RTLLM [[15](#bib.bib15)] (using a generalized prompt for all designs).
    As evident in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Code Generation and Testing ‣
    3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"), the LLM
    effectively amends the issues in the Verilog codes for the booth_multiplier design.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $V(n)$ 代表识别出的错误。VeriRectify 工作流程如图 [2](#S3.F2 "图 2 ‣ 3.2 代码生成与测试 ‣ 3 VeriPPA
    框架 ‣ 高级大语言模型（LLM）驱动的 Verilog 开发：在代码综合中提升功耗、性能和面积优化") 所示。图的顶部部分展示了我们仿真工具（如 booth_multiplier）输出的语法错误和功能错误，这是我们仿真工具的输出。在修正阶段（错误细化），我们使用定制的提示（整合错误详情）用于
    LLM，使其能够纠正最初生成的 Verilog 代码中的缺陷。这种方法允许 LLM 针对遇到的问题进行专门处理，这与 RTLLM [[15](#bib.bib15)]（为所有设计使用通用提示）根本不同。如图
    [2](#S3.F2 "图 2 ‣ 3.2 代码生成与测试 ‣ 3 VeriPPA 框架 ‣ 高级大语言模型（LLM）驱动的 Verilog 开发：在代码综合中提升功耗、性能和面积优化")
    所示，LLM 有效地修正了 booth_multiplier 设计中 Verilog 代码的问题。
- en: 3.4   Multi-round Conversation with Error Feedback
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 多轮对话与错误反馈
- en: 'To further refine the generated Verilog codes, we employ a multi-round conversation
    with error feedback loop analogous to human problem-solving techniques. This approach
    can be conceptualized as a function that iteratively refines the output by considering
    the errors of previous steps. Let $V_{i}$. The iterative process can be viewed
    as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步优化生成的 Verilog 代码，我们采用了多轮对话和错误反馈循环的方法，这类似于人类解决问题的技巧。这种方法可以被概念化为一个函数，该函数通过考虑前一步的错误来迭代地优化输出。设
    $V_{i}$。迭代过程可以如下视图：
- en: '|  | $V_{i+1}=R(V_{i},E_{i})\quad\text{and}\quad E_{i+1}=D(V_{i+1})$ |  | (2)
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{i+1}=R(V_{i},E_{i})\quad\text{and}\quad E_{i+1}=D(V_{i+1})$ |  | (2)
    |'
- en: This process repeats until either no errors are detected or a predefined iteration
    limit, $K$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程重复进行，直到检测不到错误或达到预定义的迭代限制 $K$。
- en: 0:  User prompt $P$ then 13:     $V_{final}\leftarrow V_{i}$
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '0: 用户提示 $P$ 然后 13: $V_{final}\leftarrow V_{i}$'
- en: Algorithm 1 Multi-Round Verilog Code Generation using LLM
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 使用 LLM 的多轮 Verilog 代码生成
- en: 3.5   Power Performance & Area (PPA) Checking
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 功耗性能与面积（PPA）检查
- en: The *VeriRectify* process ensures the design to pass both RTL syntax check and
    cycle-accurate functional simulation. However, RTL simulation does not guarantee
    that the design (Verilog code) is *synthesizable*. Furthermore, the quality of
    the hardware design must be measured by its power, performance, and area metrics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*VeriRectify* 过程确保设计通过 RTL 语法检查和周期精确功能仿真。然而，RTL 仿真并不保证设计（Verilog 代码）是 *可综合的*。此外，硬件设计的质量必须通过其功耗、性能和面积指标来衡量。'
- en: 'Our approach takes a step further by inspecting PPA of the design $V$ which
    passes the *VeriRectify* process as the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法进一步检查通过 *VeriRectify* 过程的设计 $V$ 的 PPA，如下所示：
- en: '|  | $V=\left\{\begin{matrix}V&amp;\text{if}~{}PPA(V)~{}\text{satisfies},\\
    VeriRectify(V,PPA(V))&amp;\text{otherwise.}\end{matrix}\right.$ |  | (3) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $V=\left\{\begin{matrix}V&\text{if}~{}PPA(V)~{}\text{satisfies},\\ VeriRectify(V,PPA(V))&\text{otherwise.}\end{matrix}\right.$
    |  | (3) |'
- en: 'In this work, our PPA check calls Synopsys Design Compiler to perform logic
    synthesis (and technology mapping) on the open-source ASAP 7nm Predictive PDK [[23](#bib.bib23)].
    We check all designs’ warning/error messages during the logic synthesis, and the
    power ($nW$) for quality. When the Verilog design can be synthesized and meets
    the PPA goal, it results in a pass. Otherwise, both the design and its corresponding
    PPA report will be fed back to the VeriRectify (Section [3.3](#S3.SS3 "3.3 VeriRectify
    ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis")) for refinement.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们的 PPA 检查调用 Synopsys Design Compiler 对开源 ASAP 7nm 预测 PDK[[23](#bib.bib23)]进行逻辑综合（和技术映射）。我们在逻辑综合过程中检查所有设计的警告/错误信息以及质量的功率（$nW$）。当
    Verilog 设计能够综合并满足 PPA 目标时，结果为通过。否则，设计及其相应的 PPA 报告将反馈到 VeriRectify（第[3.3节](#S3.SS3
    "3.3 VeriRectify ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) -
    Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis")）进行优化。'
- en: '![Refer to caption](img/1edadfb6d87c9c789d3ca32bde6f78da.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1edadfb6d87c9c789d3ca32bde6f78da.png)'
- en: 'Figure 3: Correctness of generated Verilog code with respect to correction
    attempt on RTLLM, using (a) GPT-3.5; (b) GPT-4-v1; (c) GPT-4; (d) GPT-4-4shot'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 使用 (a) GPT-3.5; (b) GPT-4-v1; (c) GPT-4; (d) GPT-4-4shot 对 RTLLM 的修正尝试中生成的
    Verilog 代码的正确性'
- en: 3.6   In-Context Learning
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6   上下文学习
- en: 'LLMs have demonstrated remarkable in-context learning (ICL) capabilities. Given
    a few input-label pairs as demonstrations, they can predict labels for unseen
    inputs without requiring parameter updates [[21](#bib.bib21), [3](#bib.bib3)].
    In-context learning can be viewed as a meta-optimizer and may also be regarded
    as implicit fine-tuning [[6](#bib.bib6)]. While off-the-shelf LLM APIs do not
    offer extensive customization options, such as fine-tuning, ICL can significantly
    improve LLM’s in-domain performance with only a few examples. In situations where
    labeled data is scarce, ICL has been shown to outperform explicit fine-tuning [[20](#bib.bib20)]
    on downstream tasks substantially. The availability of small training set poses
    a challenge in avoiding overfitting; however, it highlights the advantage of ICL
    over fine-tuning in scenarios with limited examples. In these cases, ICL demonstrates
    superior performance and better generalization capabilities, while fine-tuning
    may struggle with overfitting issues and result in poor generalization. A single
    LLM can achieve robust performance on multiple tasks using only its text interface:
    a few task examples are provided to the model as a prompt, accompanied by a query
    input, and the model generates a continuation to produce a predicted output for
    that query.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 已经展示了卓越的上下文学习（ICL）能力。给定少量输入-标签对作为示例，它们可以在不需要参数更新的情况下预测未见输入的标签[[21](#bib.bib21),
    [3](#bib.bib3)]。上下文学习可以被视为一种元优化器，也可以被看作是隐式微调[[6](#bib.bib6)]。尽管现成的 LLM API 不提供广泛的定制选项，如微调，但
    ICL 仅通过少量示例即可显著提高 LLM 在领域内的性能。在标记数据稀缺的情况下，ICL 已被证明在下游任务中明显优于显式微调[[20](#bib.bib20)]。小型训练集的可用性在避免过拟合方面提出了挑战；然而，这突显了
    ICL 在有限示例场景中的优势。在这些情况下，ICL 展示了优越的性能和更好的泛化能力，而微调可能会遇到过拟合问题，导致较差的泛化性能。单个 LLM 仅使用其文本接口即可在多个任务上实现强大的性能：模型接收到少量任务示例作为提示，并附带查询输入，模型生成继续内容以对该查询生成预测输出。
- en: '|  | $q(t&#124;v)=\prod_{k=1}^{K}q(t_{k}&#124;t_{<k}),$ |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $q(t&#124;v)=\prod_{k=1}^{K}q(t_{k}&#124;t_{<k}),$ |  | (4) |'
- en: In this equation, $t_{k}$ is parameterized by an LLM. The equation describes
    the in-context learning of a large language model, where the model learns to predict
    the next token in the sequence by considering the previous demonstration examples.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，$t_{k}$由 LLM 参数化。该方程描述了大型语言模型的上下文学习，其中模型通过考虑之前的示例学习预测序列中的下一个标记。
- en: The selection of few-shot examples [[20](#bib.bib20), [1](#bib.bib1)] also influences
    the generalization capability of LLMs on downstream tasks. In our case, we carefully
    select the text-to-Verilog pairs to ensure that the examples cover a range of
    different Verilog designs, such as addition, multiplication, single-stage design,
    and pipelined design.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 少量示例的选择[[20](#bib.bib20), [1](#bib.bib1)]也会影响 LLM 在下游任务中的泛化能力。在我们的案例中，我们仔细选择文本到
    Verilog 的配对，以确保示例涵盖各种不同的 Verilog 设计，例如加法、乘法、单阶段设计和流水线设计。
- en: 'Table 1: PPA results of generated Verilog code'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 生成的 Verilog 代码的 PPA 结果'
- en: '| Design Name | GPT-4 | GPT-4 (4-shot) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 设计名称 | GPT-4 | GPT-4 (4-shot) |'
- en: '| --- | --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; Clock &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Clock &#124;'
- en: '&#124; (ps) &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (ps) &#124;'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Power &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Power &#124;'
- en: '&#124; ($\scriptstyle\mu$W) &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ($\scriptstyle\mu$W) &#124;'
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Area &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Area &#124;'
- en: '&#124; ($\scriptstyle\mu$m²) &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ($\scriptstyle\mu$m²) &#124;'
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Clock &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Clock &#124;'
- en: '&#124; (ps) &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (ps) &#124;'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Power &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Power &#124;'
- en: '&#124; ($\scriptstyle\mu$W) &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ($\scriptstyle\mu$W) &#124;'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Area &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Area &#124;'
- en: '&#124; ($\scriptstyle\mu$m²) &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ($\scriptstyle\mu$m²) &#124;'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| adder_8bit | 318.5 | 6.3 | 38.5 | 333.1 | 6.1 | 42.9 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| adder_8bit | 318.5 | 6.3 | 38.5 | 333.1 | 6.1 | 42.9 |'
- en: '| adder_16bit | 342.2 | 10.9 | 104.5 | 135.1 | 41.1 | 152.8 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| adder_16bit | 342.2 | 10.9 | 104.5 | 135.1 | 41.1 | 152.8 |'
- en: '| adder_32bit | 500.0 | 14.2 | 211.6 | 500.0 | 14.7 | 213.2 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| adder_32bit | 500.0 | 14.2 | 211.6 | 500.0 | 14.7 | 213.2 |'
- en: '| multi_booth | 409.0 | 112.1 | 526.0 | 409.0 | 112.1 | 526.0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| multi_booth | 409.0 | 112.1 | 526.0 | 409.0 | 112.1 | 526.0 |'
- en: '| right_shifter | 47.5 | 144.3 | 42.9 | 47.5 | 144.3 | 42.9 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| right_shifter | 47.5 | 144.3 | 42.9 | 47.5 | 144.3 | 42.9 |'
- en: '| width_8to16 | 74.1 | 223.2 | 145.8 | 145.6 | 128.7 | 157.2 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| width_8to16 | 74.1 | 223.2 | 145.8 | 145.6 | 128.7 | 157.2 |'
- en: '| edge_detect | 61.5 | 49.0 | 23.3 | 61.5 | 49.0 | 23.3 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| edge_detect | 61.5 | 49.0 | 23.3 | 61.5 | 49.0 | 23.3 |'
- en: '| mux | 54.7 | 215.3 | 86.1 | 54.7 | 215.3 | 86.1 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| mux | 54.7 | 215.3 | 86.1 | 54.7 | 215.3 | 86.1 |'
- en: '| pe | 500.0 | 552.5 | 2546.5 | 500.0 | 541.0 | 2488.6 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| pe | 500.0 | 552.5 | 2546.5 | 500.0 | 541.0 | 2488.6 |'
- en: '| asyn_fifo | 295.2 | 406.4 | 1279.3 | 228.3 | 526.6 | 1295.4 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| asyn_fifo | 295.2 | 406.4 | 1279.3 | 228.3 | 526.6 | 1295.4 |'
- en: '| counter_12 | 134.4 | 33.1 | 40.6 | 124.5 | 34.6 | 36.4 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| counter_12 | 134.4 | 33.1 | 40.6 | 124.5 | 34.6 | 36.4 |'
- en: '| fsm | 88.3 | 32.7 | 31.5 | 68.7 | 49.0 | 50.2 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| fsm | 88.3 | 32.7 | 31.5 | 68.7 | 49.0 | 50.2 |'
- en: '| multi_pipe_4bit | 254.7 | 40.7 | 131.3 | - | - | - |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| multi_pipe_4bit | 254.7 | 40.7 | 131.3 | - | - | - |'
- en: '| pulse_detect | 10.3 | 187.5 | 13.5 | 32.7 | 59.1 | 13.5 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| pulse_detect | 10.3 | 187.5 | 13.5 | 32.7 | 59.1 | 13.5 |'
- en: '| calendar | - | - | - | 208.6 | 86.6 | 199.0 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| calendar | - | - | - | 208.6 | 86.6 | 199.0 |'
- en: 4   Evaluation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4   评估
- en: 4.1   Datasets
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1   数据集
- en: 'In assessing our VeriPPA framework, we utilize two benchmark datasets. Firstly,
    the RTLLLM dataset[[15](#bib.bib15)] includes 29 designs. Notably, it originally
    contained 30 designs, but the risc_cpu design is currently unavailable. Secondly,
    we employ the VerilogEval dataset [[13](#bib.bib13)], which comprises two subsets:
    VerilogEval-human, featuring 156 designs, and VerilogEval-machine, consisting
    of 108 designs.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估我们的 VeriPPA 框架时，我们利用了两个基准数据集。首先，RTLLLM 数据集[[15](#bib.bib15)] 包含 29 个设计。值得注意的是，它原本包含
    30 个设计，但 risc_cpu 设计目前不可用。其次，我们使用 VerilogEval 数据集 [[13](#bib.bib13)]，该数据集包括两个子集：VerilogEval-human，包含
    156 个设计，以及 VerilogEval-machine，包含 108 个设计。
- en: 4.2   Experimental Setup
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2   实验设置
- en: We demonstrate the effectiveness of our VeriPPA framework for generating PPA-optimized
    Verilog code for the given designs. We adopt, GPT-3.5 [[18](#bib.bib18)] and GPT-4
    [[19](#bib.bib19)] as our LLM models. We use n=1, temperature temp = 0.7, and
    a context length of 2048 in our setting. Further, we incorporate the ICARUS Verilog
    simulator [[26](#bib.bib26)] to automate the testing of the generated code. For
    PPA check, we perform the logic synthesis using Synopsys Design Compiler with
    compile_ultra command and we use the ASAP 7nm Predictive PDK [[23](#bib.bib23)].
    We implemented a flow (Python script) that sweeps the timing constraints to find
    the fastest achievable clock frequency for all the generated designs. All experiments
    were conducted on a Linux- based host with AMD EPYC 7543 32-Core Processor and
    an NVIDIA A100-SXM 80 GB.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了 VeriPPA 框架在生成 PPA 优化的 Verilog 代码方面的有效性。我们采用了 GPT-3.5 [[18](#bib.bib18)]
    和 GPT-4 [[19](#bib.bib19)] 作为我们的 LLM 模型。在设置中，我们使用 n=1、温度 temp = 0.7 和上下文长度 2048。此外，我们结合了
    ICARUS Verilog 仿真器 [[26](#bib.bib26)] 来自动化测试生成的代码。对于 PPA 检查，我们使用 Synopsys Design
    Compiler 的 compile_ultra 命令进行逻辑综合，并使用 ASAP 7nm Predictive PDK [[23](#bib.bib23)]。我们实现了一个流（Python
    脚本），该脚本遍历时序约束以找到所有生成设计的最快可实现时钟频率。所有实验均在配备 AMD EPYC 7543 32 核处理器和 NVIDIA A100-SXM
    80 GB 的基于 Linux 的主机上进行。
- en: 4.3   Generation Correctness
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3   生成正确性
- en: 'This study evaluates Verilog code generation accuracy using two primary metrics:
    syntax checking and functionality verification. Figure [3](#S3.F3 "Figure 3 ‣
    3.5 Power Performance & Area (PPA) Checking ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large
    Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis") presents our methodology for improving
    Verilog correctness through successive correction attempts and self-planing [[15](#bib.bib15)].
    We generate five different codes for each design description, attempting up to
    four corrections within each generation. However, after certain attempts, the
    efficiency of these corrections diminishes, as the LLMs tend to provide repetitive
    responses to identical errors.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究使用两个主要指标来评估 Verilog 代码生成的准确性：语法检查和功能验证。图 [3](#S3.F3 "Figure 3 ‣ 3.5 Power
    Performance & Area (PPA) Checking ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language
    Model (LLM) - Driven Verilog Development: Enhancing Power, Performance, and Area
    Optimization in Code Synthesis") 展示了我们通过连续的修正尝试和自我规划来提高 Verilog 正确性的方法[[15](#bib.bib15)]。我们为每个设计描述生成五种不同的代码，在每次生成中尝试最多进行四次修正。然而，在某些尝试之后，这些修正的效率会降低，因为
    LLM 通常对相同的错误提供重复的响应。'
- en: 'In Figure [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking
    ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"), we plot
    syntax and functionality correctness percentages against the number of correction
    attempts. The graph features a solid line for syntax correctness and a dotted
    line for functionality correctness. Functionality is evaluated the same as RTLLM
    [[15](#bib.bib15)], considering a design functionally correct if at least one
    generated code passes the functionality test. We use GPT-3.5 and observe initial
    syntax correctness of 44.13% and functionality correctness of 24.13%, as shown
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣
    3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") (a). After
    applying correction attempts, these figures improved to 65.51% for syntax and
    31.03% for functionality. Compared to RTLLM, which initially scored 24.82% in
    syntax and 27.58% in functionality without corrections. Please note that RTTLM
    uses self panning in prompt. Integrating our correction approach with RTLLM increases
    the maximum syntax and functionality correctness to 49.65% and 34.48%, respectively.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣ 3 VeriPPA
    FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") 中，我们将语法和功能正确率与修正尝试次数进行比较。图表中实线表示语法正确率，虚线表示功能正确率。功能性评估与
    RTLLM [[15](#bib.bib15)] 相同，如果至少一种生成的代码通过功能测试，则认为设计在功能上是正确的。我们使用 GPT-3.5 观察到初始的语法正确率为
    44.13% 和功能正确率为 24.13%，如图 [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA)
    Checking ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven
    Verilog Development: Enhancing Power, Performance, and Area Optimization in Code
    Synthesis")（a）所示。经过修正尝试后，这些数字提高到了 65.51% 的语法正确率和 31.03% 的功能正确率。相比之下，RTLLM 在没有修正的情况下，初始语法正确率为
    24.82%，功能正确率为 27.58%。请注意，RTTLM 在提示中使用自我规划。将我们的修正方法与 RTLLM 结合，可以将最大语法和功能正确率提高到
    49.65% 和 34.48%。'
- en: 'We then evaluate two versions of the GPT-4 model. The first, GPT-4-0314 (v1),
    showed an initial syntax correctness of 56.55% and functionality correctness of
    37.93%. Our correction methods raised these to 71.03% for syntax and 51.72% for
    functionality. Combining RTLLM with our approach, we further enhanced the syntax
    correctness to 75.17% and functionality to 51.72%, as indicated in Figure [3](#S3.F3
    "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣ 3 VeriPPA FRAMEWORK
    ‣ Advanced Large Language Model (LLM) - Driven Verilog Development: Enhancing
    Power, Performance, and Area Optimization in Code Synthesis") (b). For the base
    GPT-4 model, we noticed an increase in syntax correctness from 66.2% to 81.37%
    by the fourth attempt and in functionality from 37.93% to 48.27%. When combined
    with RTLLM and our correction techniques, the syntax correctness further improved
    from 60% to 77.93%, and functionality from 34.48% to 48.27%, as shown in Figure
    [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣ 3 VeriPPA
    FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") (c).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了两个版本的GPT-4模型。第一个，GPT-4-0314（v1），初始的语法正确性为56.55%，功能正确性为37.93%。我们的纠正方法将这些提升到了71.03%的语法和51.72%的功能。将RTLLM与我们的方法结合后，我们进一步将语法正确性提高到了75.17%，功能提高到51.72%，如图[3](#S3.F3
    "图 3 ‣ 3.5 功率性能与面积（PPA）检查 ‣ 3 VeriPPA框架 ‣ 高级大语言模型（LLM）驱动的Verilog开发：在代码综合中提升功率、性能和面积优化")
    (b)所示。对于基础GPT-4模型，我们注意到语法正确性从66.2%提高到81.37%在第四次尝试中，功能从37.93%提高到48.27%。当与RTLLM和我们的纠正技术结合时，语法正确性进一步提高了从60%到77.93%，功能从34.48%到48.27%，如图[3](#S3.F3
    "图 3 ‣ 3.5 功率性能与面积（PPA）检查 ‣ 3 VeriPPA框架 ‣ 高级大语言模型（LLM）驱动的Verilog开发：在代码综合中提升功率、性能和面积优化")
    (c)所示。
- en: 'Finally, testing the GPT-4 model with four-shot learning, we observed an improvement
    in syntax from 70.34% to 79.31% and in functionality from 37.93% to 41.37%. With
    the addition of RTLLM and our correction methods, the functionality correctness
    notably increased from 44.82% to 62.06% after four attempts, as demonstrated in
    Figure [3](#S3.F3 "Figure 3 ‣ 3.5 Power Performance & Area (PPA) Checking ‣ 3
    VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") (d). This
    significant improvement highlights the effectiveness of our methods in enhancing
    the functional accuracy of the designs.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过四次学习测试GPT-4模型，我们观察到语法从70.34%提高到79.31%，功能从37.93%提高到41.37%。在加入RTLLM和我们的纠正方法后，功能正确性显著提高，从44.82%增加到62.06%经过四次尝试，如图[3](#S3.F3
    "图 3 ‣ 3.5 功率性能与面积（PPA）检查 ‣ 3 VeriPPA框架 ‣ 高级大语言模型（LLM）驱动的Verilog开发：在代码综合中提升功率、性能和面积优化")
    (d)所示。这一显著改善突显了我们方法在提升设计功能准确性方面的有效性。
- en: 'In evaluating our VeriPPA framework with VerilogEval data, we found notable
    improvements. For the VerilogEval-Machine dataset (Figure [4](#S4.F4 "Figure 4
    ‣ 4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis") (a)), our method significantly increased syntax accuracy.
    Functionality accuracy also rose from 33.57% to 43.79% using GPT-4, and further
    to 45.25% with GPT-4’s four-shot learning. The VerilogEval-human dataset showed
    similar trends, with functionality accuracy improving from 29.48% to 39.74% through
    the application of GPT-4 and its four-shot learning variant as shown in the Figure
    [4](#S4.F4 "Figure 4 ‣ 4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large
    Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis") (b). This underscores our framework’s
    effectiveness in enhancing both syntax and functionality in Verilog code generation'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用VerilogEval数据评估我们的VeriPPA框架时，我们发现了显著的改善。对于VerilogEval-Machine数据集（图[4](#S4.F4
    "图 4 ‣ 4.3 生成正确性 ‣ 4 评估 ‣ 高级大语言模型（LLM）驱动的Verilog开发：在代码综合中提升功率、性能和面积优化") (a)），我们的方法显著提高了语法准确性。功能准确性也从33.57%提高到43.79%使用GPT-4，并进一步提高到45.25%使用GPT-4的四次学习。VerilogEval-human数据集显示了类似的趋势，通过应用GPT-4及其四次学习变体，功能准确性从29.48%提高到39.74%，如图[4](#S4.F4
    "图 4 ‣ 4.3 生成正确性 ‣ 4 评估 ‣ 高级大语言模型（LLM）驱动的Verilog开发：在代码综合中提升功率、性能和面积优化") (b)所示。这突显了我们框架在增强Verilog代码生成中的语法和功能方面的有效性。
- en: '![Refer to caption](img/5f0955230d71e612049002e58ce7ca71.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5f0955230d71e612049002e58ce7ca71.png)'
- en: 'Figure 4: Correctness of generated Verilog code with respect to correction
    attempt on VerilogEval, using (a) VerilogEval-Machine, and (b) VerilogEval-Human.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：生成的 Verilog 代码的正确性，相对于在 VerilogEval 上的修正尝试，使用 (a) VerilogEval-Machine 和
    (b) VerilogEval-Human。
- en: \captionof
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: \captionof
- en: figureOptimization Flow.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: figureOptimization 流程。
- en: 'Table 2: Optimized results'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：优化结果
- en: '| Design Name | Clock (ps) | Power ($\mu$m) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 设计名称 | 时钟 (ps) | 功耗 ($\mu$m) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| adder_32bit | 180.0 | 587.31 | 1005.67 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| adder_32bit | 180.0 | 587.31 | 1005.67 |'
- en: '| multi_booth | 123.2 | 42.39 | 42.92 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| multi_booth | 123.2 | 42.39 | 42.92 |'
- en: '| pe | 325.0 | 1206.0 | 4863.88 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| pe | 325.0 | 1206.0 | 4863.88 |'
- en: '| asyn_fifo | 114.8 | 988.92 | 1344.86 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| asyn_fifo | 114.8 | 988.92 | 1344.86 |'
- en: '| radix2_div | - | - | - |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| radix2_div | - | - | - |'
- en: 4.4   PPA Optimization
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 PPA 优化
- en: 'In VeriPPA, we use the Synopsis Design Compiler for synthesizing the designs,
    culminating in the production of PPA reports. The PPA results of complex designs
    are encapsulated in Table [1](#S3.T1 "Table 1 ‣ 3.6 In-Context Learning ‣ 3 VeriPPA
    FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"). This
    table, though comprehensive, does not encompass specific design constraints. Similar
    to the ChipGPT approach [[4](#bib.bib4)], where an output manager and enumerative
    search finalize the PPA from multiple reports, our process also generates multiple
    PPA reports for each design. An example is the $pulse\_detect$ design, and we
    selected the most optimized one to include in Table [1](#S3.T1 "Table 1 ‣ 3.6
    In-Context Learning ‣ 3 VeriPPA FRAMEWORK ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在 VeriPPA 中，我们使用 Synopsis Design Compiler 来合成设计，并最终生成 PPA 报告。复杂设计的 PPA 结果封装在表
    [1](#S3.T1 "Table 1 ‣ 3.6 In-Context Learning ‣ 3 VeriPPA FRAMEWORK ‣ Advanced
    Large Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis") 中。尽管该表很全面，但并未涵盖具体的设计约束。与 ChipGPT 方法
    [[4](#bib.bib4)] 类似，其中一个输出管理器和枚举搜索从多个报告中确定最终的 PPA，我们的过程也为每个设计生成多个 PPA 报告。以 $pulse\_detect$
    设计为例，我们选择了最优化的报告以包含在表 [1](#S3.T1 "Table 1 ‣ 3.6 In-Context Learning ‣ 3 VeriPPA
    FRAMEWORK ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") 中。'
- en: 'It is crucial that PPA results do not conform to specialized design requirements,
    a standard practice in industrial applications. To address this disparity, we
    further perform the PPA constraint-based feedback mechanism, integrated with context-based
    learning, as illustrated in Figure [4.3](#S4.SS3 "4.3 Generation Correctness ‣
    4 Evaluation ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis"). This
    approach represents a significant step towards aligning LLM-generated code with
    industry-specific PPA requirements. Figure [4.3](#S4.SS3 "4.3 Generation Correctness
    ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM) - Driven Verilog Development:
    Enhancing Power, Performance, and Area Optimization in Code Synthesis") demonstrates
    our process, starting with the collection of synthesized design outputs that require
    optimization. For example, $adder\_32bit$, we impose a clock constraint, aiming
    for a clock speed of less than 300ps, as outlined in the PPA constraint-based
    prompt in Figure [4.3](#S4.SS3 "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced
    Large Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis"). The framework instructs the LLM to
    consider various optimization strategies, including Pipelining, Clock Gating,
    Parallel Operation, and Hierarchical Design. It also encourages the exploration
    of additional methods to generate Verilog code that meets the defined optimization
    constraints, as illustrated in the context-based learning segment of Figure [4.3](#S4.SS3
    "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis").'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'PPA 结果与专业设计要求不一致是至关重要的，这在工业应用中是一种标准做法。为了解决这种差异，我们进一步执行基于 PPA 约束的反馈机制，并结合基于上下文的学习，如图
    [4.3](#S4.SS3 "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language
    Model (LLM) - Driven Verilog Development: Enhancing Power, Performance, and Area
    Optimization in Code Synthesis") 所示。这种方法代表了将 LLM 生成的代码与行业特定的 PPA 要求对齐的重要一步。图 [4.3](#S4.SS3
    "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis") 展示了我们的过程，从收集需要优化的综合设计输出开始。例如，对于 $adder\_32bit$，我们施加了时钟约束，目标是时钟速度低于
    300ps，如图 [4.3](#S4.SS3 "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large
    Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis") 中的 PPA 约束基于提示所述。框架指示 LLM 考虑各种优化策略，包括流水线化、时钟门控、并行操作和层次化设计。它还鼓励探索额外的方法来生成满足定义优化约束的
    Verilog 代码，如图 [4.3](#S4.SS3 "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced
    Large Language Model (LLM) - Driven Verilog Development: Enhancing Power, Performance,
    and Area Optimization in Code Synthesis") 中的基于上下文的学习部分所示。'
- en: 'Upon providing the PPA-based constraint prompt and context to the LLM, we analyze
    the resultant Verilog code for syntax and functional accuracy, making corrections
    where necessary. If the code passes both checks, we proceed to its final synthesis,
    achieving an optimized Verilog code as shown in Figure [4.3](#S4.SS3 "4.3 Generation
    Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM) - Driven Verilog
    Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis")
    (d), where the $adder\_32bit$ operates at an improved 180ps clock. In Table [2](#S4.T2
    "Table 2 ‣ 4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language
    Model (LLM) - Driven Verilog Development: Enhancing Power, Performance, and Area
    Optimization in Code Synthesis"), we present the results of selected optimized
    designs. Due to the page limit and we only show substantial optimizations, the
    table does not include simpler designs. Notably, no design from the VerilogEval
    [[13](#bib.bib13)] dataset features in Table [2](#S4.T2 "Table 2 ‣ 4.3 Generation
    Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM) - Driven Verilog
    Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis"),
    as those designs did not require complex optimization.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '在向LLM提供基于PPA的约束提示和上下文后，我们分析生成的Verilog代码的语法和功能准确性，必要时进行修正。如果代码通过了这两项检查，我们将进行最终合成，达到优化后的Verilog代码，如图[4.3](#S4.SS3
    "4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis")（d）所示，其中$adder\_32bit$在改进的180ps时钟下运行。在表[2](#S4.T2 "Table 2
    ‣ 4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language Model (LLM)
    - Driven Verilog Development: Enhancing Power, Performance, and Area Optimization
    in Code Synthesis")中，我们展示了选择的优化设计结果。由于页面限制，我们只展示了重要的优化，表中未包括更简单的设计。值得注意的是，表[2](#S4.T2
    "Table 2 ‣ 4.3 Generation Correctness ‣ 4 Evaluation ‣ Advanced Large Language
    Model (LLM) - Driven Verilog Development: Enhancing Power, Performance, and Area
    Optimization in Code Synthesis")中没有来自VerilogEval [[13](#bib.bib13)] 数据集的设计，因为这些设计不需要复杂的优化。'
- en: 5   conlcusion
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5   结论
- en: In this paper, we introduce a novel framework VeriPPA, designed to assess and
    enhance LLM efficiency in this specialized area. Our method includes generating
    initial Verilog code using LLMs, followed by a unique two-stage refinement process.
    The first stage focuses on improving the functional and syntactic integrity of
    the code, while the second stage aims to optimize the code in line with Power-Performance-Area
    (PPA) constraints, an essential aspect of effective hardware design. This dual-phase
    approach of error correction and PPA optimization has led to notable improvements
    in the quality of LLM-generated Verilog code. Our framework schieves 62.0% (+16%)
    for functional accuracy and 81.37% (+8.3%) for syntactic correctness in Verilog
    code generation, compared to SOTAs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们介绍了一个新颖的框架VeriPPA，旨在评估和提高LLM在这一专业领域的效率。我们的方法包括使用LLM生成初始Verilog代码，然后进行独特的两阶段精化过程。第一阶段专注于提高代码的功能和语法完整性，而第二阶段则旨在根据功耗-性能-面积（PPA）约束优化代码，这是有效硬件设计的一个重要方面。这种错误修正和PPA优化的双阶段方法在LLM生成的Verilog代码质量上取得了显著改进。我们的框架在功能准确性上达到了62.0%（+16%）和在语法正确性上达到了81.37%（+8.3%），相比SOTA。
- en: References
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Alon Albalak et al. Improving few-shot generalization by exploring and
    exploiting auxiliary data. arXiv preprint arXiv:2302.00674, 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Alon Albalak等人。通过探索和利用辅助数据来改善少样本泛化。arXiv预印本arXiv:2302.00674，2023年。'
- en: '[2] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. Chip-chat:
    Challenges and opportunities in conversational hardware design. arXiv preprint
    arXiv:2305.13243, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jason Blocklove, Siddharth Garg, Ramesh Karri, 和 Hammond Pearce。Chip-chat：对话硬件设计中的挑战与机遇。arXiv预印本arXiv:2305.13243，2023年。'
- en: '[3] Tom Brown et al. Language models are few-shot learners. Advances in neural
    information processing systems, 33:1877–1901, 2020.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom Brown等人。语言模型是少样本学习者。神经信息处理系统进展，33:1877–1901，2020年。'
- en: '[4] Kaiyan Chang et al. Chipgpt: How far are we from natural language hardware
    design, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Kaiyan Chang等人。Chipgpt：我们距离自然语言硬件设计还有多远，2023年。'
- en: '[5] Wen Chen et al. Novel test detection to improve simulation efficiency:
    A commercial experiment. In ICCAD’12, page 101–108, New York, NY, USA, 2012.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Wen Chen等人。新颖的测试检测以提高仿真效率：一项商业实验。在ICCAD’12，第101–108页，纽约，NY，美国，2012年。'
- en: '[6] Damai Dai et al. Why can gpt learn in-context? language models implicitly
    perform gradient descent as meta-optimizers. In ICLR 2023 Workshop on Mathematical
    and Empirical Understanding of Foundation Models, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Damai Dai 等。为什么 GPT 能够在上下文中学习？语言模型隐式地执行梯度下降作为元优化器。见 ICLR 2023 数学与经验理解基础模型研讨会，2023
    年。'
- en: '[7] Shai Fine and Avi Ziv. Coverage directed test generation for functional
    verification using bayesian networks. In DAC ’03, page 286–291, New York, NY,
    USA, 2003.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Shai Fine 和 Avi Ziv。使用贝叶斯网络进行功能验证的覆盖导向测试生成。见 DAC ’03，页 286–291，美国纽约，2003
    年。'
- en: '[8] Winston Haaswijk et al. Deep learning for logic optimization algorithms.
    In 2018 ISCAS, pages 1–4, 2018.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Winston Haaswijk 等。用于逻辑优化算法的深度学习。见 2018 ISCAS，页 1–4，2018 年。'
- en: '[9] HDLBits. Hdlbits:verilog practice. [https://hdlbits.01xz.net](https://hdlbits.01xz.net),
    2023. Accessed on 11/20/2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] HDLBits。Hdlbits：Verilog 练习。 [https://hdlbits.01xz.net](https://hdlbits.01xz.net)，2023
    年。访问日期：2023 年 11 月 20 日。'
- en: '[10] Abdelrahman Hosny et al. Drills: Deep reinforcement learning for logic
    synthesis. In 2020 25th ASP-DAC, pages 581–586, 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Abdelrahman Hosny 等。Drills：用于逻辑合成的深度强化学习。见 2020 第 25 届 ASP-DAC，页 581–586，2020
    年。'
- en: '[11] Hanbin Hu et al. Hfmv: Hybridizing formal methods and machine learning
    for verification of analog and mixed-signal circuits. In DAC ’18, New York, NY,
    USA, 2018.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Hanbin Hu 等。Hfmv：结合形式方法和机器学习以验证模拟和混合信号电路。见 DAC ’18，美国纽约，2018 年。'
- en: '[12] Rongjian Liang et al. Drc hotspot prediction at sub-10nm process nodes
    using customized convolutional network. In ISPD ’20, page 135–142, New York, NY,
    USA, 2020.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Rongjian Liang 等。在子 10nm 工艺节点上使用定制卷积网络进行 Drc 热点预测。见 ISPD ’20，页 135–142，美国纽约，2020
    年。'
- en: '[13] Mingjie Liu et al. VerilogEval: evaluating large language models for verilog
    code generation. In ICCAD’23, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Mingjie Liu 等。VerilogEval：评估大型语言模型用于 Verilog 代码生成。见 ICCAD’23，2023 年。'
- en: '[14] Zeye Liu et al. Improving test chip design efficiency via machine learning.
    In 2019 ITC, pages 1–10, 2019.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Zeye Liu 等。通过机器学习提高测试芯片设计效率。见 2019 ITC，页 1–10，2019 年。'
- en: '[15] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. Rtllm: An open-source
    benchmark for design rtl generation with large language model, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Yao Lu, Shang Liu, Qijun Zhang 和 Zhiyao Xie。Rtllm：一个用于设计 RTL 生成的开源基准，具有大型语言模型，2023
    年。'
- en: '[16] Dani Maarouf et al. Machine-learning based congestion estimation for modern
    fpgas. In FPL’18, pages 427–4277, 2018.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Dani Maarouf 等。基于机器学习的现代 FPGA 拥塞估计。见 FPL’18，页 427–4277，2018 年。'
- en: '[17] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
    Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for
    code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
    Silvio Savarese 和 Caiming Xiong。Codegen：一个用于代码的开放大型语言模型，支持多轮程序合成。arXiv 预印本 arXiv:2203.13474，2022
    年。'
- en: '[18] OpenAI. Gpt-3.5. [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5),
    2023. Accessed on 15/11/2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] OpenAI。Gpt-3.5。 [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)，2023
    年。访问日期：2023 年 11 月 15 日。'
- en: '[19] OpenAI. Gpt-4. [https://platform.openai.com/docs/models/gpt-4](https://platform.openai.com/docs/models/gpt-4),
    2023. Accessed on 15/11/2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] OpenAI。Gpt-4。 [https://platform.openai.com/docs/models/gpt-4](https://platform.openai.com/docs/models/gpt-4)，2023
    年。访问日期：2023 年 11 月 15 日。'
- en: '[20] Ethan Perez et al. True few-shot learning with language models. Advances
    in neural information processing systems, 34:11054–11070, 2021.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Ethan Perez 等。具有语言模型的真实少样本学习。神经信息处理系统进展，34:11054–11070，2021 年。'
- en: '[21] Alec Radford et al. Language models are unsupervised multitask learners.
    OpenAI blog, 1(8):9, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Alec Radford 等。语言模型是无监督的多任务学习者。OpenAI 博客，1(8):9，2019 年。'
- en: '[22] Shailja Thakur et al. Benchmarking large language models for automated
    verilog rtl code generation. In DATE’23, pages 1–6\. IEEE, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Shailja Thakur 等。基准测试大型语言模型用于自动 Verilog RTL 代码生成。见 DATE’23，页 1–6。IEEE，2023
    年。'
- en: '[23] Vinay Vashishtha, Manoj Vangala, and Lawrence T Clark. Asap7 predictive
    design kit development and cell design technology co-optimization. In IEEE/ACM
    International Conference on Computer-Aided Design (ICCAD), 2017.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Vinay Vashishtha, Manoj Vangala 和 Lawrence T Clark。Asap7 预测设计套件开发和单元设计技术共同优化。见
    IEEE/ACM 计算机辅助设计国际会议 (ICCAD)，2017 年。'
- en: '[24] Fanchao Wang et al. Accelerating coverage directed test generation for
    functional verification: A neural network-based framework. In GLSVLSI’18, page
    207–212, New York, NY, USA, 2018.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Fanchao Wang 等。加速功能验证的覆盖导向测试生成：一种基于神经网络的框架。见 GLSVLSI’18，页 207–212，美国纽约，2018
    年。'
- en: '[25] Samuel Ward, Duo Ding, and David Z Pan. Pade: A high-performance placer
    with automatic datapath extraction and evaluation through high dimensional data
    learning. In Proceedings of the 49th Annual Design Automation Conference, pages
    756–761, 2012.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Samuel Ward, Duo Ding, 和 David Z Pan. Pade: 一种高性能的布置器，通过高维数据学习实现自动数据路径提取和评估。载于第49届年度设计自动化会议论文集，第756–761页，2012年。'
- en: '[26] S. Williams. The icarus verilog compilation system, 2023. [Online]. Available:
    [https://github.com/steveicarus/iverilog](https://github.com/steveicarus/iverilog).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Williams. icarus verilog 编译系统，2023年。 [在线]。可用： [https://github.com/steveicarus/iverilog](https://github.com/steveicarus/iverilog)。'
