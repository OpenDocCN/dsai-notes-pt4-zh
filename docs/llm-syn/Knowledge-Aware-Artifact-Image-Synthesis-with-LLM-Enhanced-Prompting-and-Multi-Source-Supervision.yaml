- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:53:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:53:56'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识驱动的文物图像合成与 LLM 增强的提示及多源监督
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08056](https://ar5iv.labs.arxiv.org/html/2312.08056)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08056](https://ar5iv.labs.arxiv.org/html/2312.08056)
- en: Shengguang Wu Peking UniversityBeijingChina [wushengguang@stu.pku.edu.cn](mailto:wushengguang@stu.pku.edu.cn)
    ,  Zhenglun Chen Peking UniversityBeijingChina [danielchenbj@gmail.com](mailto:danielchenbj@gmail.com)
     and  Qi Su Peking UniversityBeijingChina [sukia@pku.edu.cn](mailto:sukia@pku.edu.cn)(2023)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 吴晟光，北京大学，北京，中国 [wushengguang@stu.pku.edu.cn](mailto:wushengguang@stu.pku.edu.cn)
    ，陈郑伦，北京大学，北京，中国 [danielchenbj@gmail.com](mailto:danielchenbj@gmail.com) 和苏琪，北京大学，北京，中国
    [sukia@pku.edu.cn](mailto:sukia@pku.edu.cn)(2023)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Ancient artifacts are an important medium for cultural preservation and restoration.
    However, many physical copies of artifacts are either damaged or lost, leaving
    a blank space in archaeological and historical studies that calls for artifact
    image generation techniques. Despite the significant advancements in open-domain
    text-to-image synthesis, existing approaches fail to capture the important domain
    knowledge presented in the textual description, resulting in errors in recreated
    images such as incorrect shapes and patterns. In this paper, we propose a novel
    knowledge-aware artifact image synthesis approach that brings lost historical
    objects accurately into their visual forms. We use a pretrained diffusion model
    as backbone and introduce three key techniques to enhance the text-to-image generation
    framework: 1) we construct prompts with explicit archaeological knowledge elicited
    from large language models (LLMs); 2) we incorporate additional textual guidance
    to correlated historical expertise in a contrastive manner; 3) we introduce further
    visual-semantic constraints on edge and perceptual features that enable our model
    to learn more intricate visual details of the artifacts. Compared to existing
    approaches, our proposed model produces higher-quality artifact images that align
    better with the implicit details and historical knowledge contained within written
    documents, thus achieving significant improvements across automatic metrics and
    in human evaluation. Our code and data are available at [https://github.com/danielwusg/artifact_diffusion](https://github.com/danielwusg/artifact_diffusion).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 古代文物是文化保存和修复的重要媒介。然而，许多文物的实物复制品要么受损，要么遗失，这在考古和历史研究中留下了空白，迫切需要文物图像生成技术。尽管开源领域的文本到图像合成取得了显著进展，但现有方法未能捕捉到文本描述中呈现的重要领域知识，导致重建图像中出现错误，如形状和图案不正确。本文提出了一种新颖的知识驱动文物图像合成方法，准确地将失落的历史物体呈现为其视觉形态。我们使用预训练的扩散模型作为骨干，并引入三项关键技术以增强文本到图像生成框架：1)
    我们构造了带有显著考古知识的提示，这些知识从大型语言模型（LLMs）中引出；2) 我们以对比方式将额外的文本指导融入相关的历史专业知识；3) 我们引入了针对边缘和感知特征的进一步视觉-语义约束，使我们的模型能够学习更复杂的文物视觉细节。与现有方法相比，我们提出的模型生成的文物图像质量更高，更好地与书面文件中的隐含细节和历史知识对齐，从而在自动评估指标和人工评价中实现了显著的改进。我们的代码和数据可在
    [https://github.com/danielwusg/artifact_diffusion](https://github.com/danielwusg/artifact_diffusion)
    获得。
- en: 'Ancient Artifact Visualization, Text-to-Image Synthesis, Diffusion Models,
    Multi-Source Supervision, Large Language Models^†^†copyright: acmcopyright^†^†journalyear:
    2023^†^†doi: XXXXXXX.XXXXXXX^†^†conference: arXiv; Preprint; ^†^†submissionid:
    xxxx'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 古代文物可视化、文本到图像合成、扩散模型、多源监督、大型语言模型^†^†版权：acmcopyright^†^†期刊年份：2023^†^†doi：XXXXXXX.XXXXXXX^†^†会议：arXiv；预印本；^†^†提交编号：xxxx
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: '![Refer to caption](img/fe22936b3707e77f5724369a5c5bc81e.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fe22936b3707e77f5724369a5c5bc81e.png)'
- en: Figure 1\. Images of artifacts generated by a vanilla diffusion model, the shape,
    color, pattern, and material differ greatly from the ground truth.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 由普通扩散模型生成的文物图像，其形状、颜色、图案和材料与实际情况差异很大。
- en: Ancient artifacts are crucial for cultural preservation, as they represent tangible
    evidence of the past, offering insights into history. In recent years, innovative
    artifact-related projects have emerged, including the restoration of degraded
    character images (Shi et al., [2022](#bib.bib31)), the generation of captions
    for ancient artwork (Sheng and Moens, [2019](#bib.bib30)), and the deciphering
    of oracle bone inscriptions (Chang et al., [2022](#bib.bib4)). These works have
    opened up new avenues for researchers to study artifacts and gain insights into
    the past. Despite these advancements, there is still much to be explored in artifact-related
    tasks, one of which is to recreate visual images of artifacts from text descriptions
    as many physical copies of artifacts are often damaged or lost, leaving only textual
    records behind. This task could prove immensely invaluable to historical studies
    and cultural preservation, as it provides historians with new visual angles to
    study the past and enables people to connect with their cultural heritage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 古代文物对于文化保存至关重要，因为它们代表了过去的实物证据，提供了对历史的见解。近年来，出现了一些创新的与文物相关的项目，包括恢复退化的字符图像（Shi
    et al., [2022](#bib.bib31)），为古代艺术品生成标题（Sheng and Moens, [2019](#bib.bib30)），以及解读甲骨文（Chang
    et al., [2022](#bib.bib4)）。这些工作为研究人员开辟了新的途径，以研究文物并获得对过去的见解。尽管有这些进展，但在与文物相关的任务中仍有很多待探索的领域，其中之一是从文本描述中重建文物的视觉图像，因为许多文物的实体副本常常被损坏或丢失，只留下文本记录。这一任务可能对历史研究和文化保存具有极大的价值，因为它为历史学家提供了研究过去的新视觉角度，并使人们能够与他们的文化遗产建立联系。
- en: One area that has shown potential to aid in the recreation of visual images
    of ancient artifacts is text-to-image synthesis. This task has been a popular
    area of research, especially in recent years with the introduction of diffusion
    models (Yang et al., [2022](#bib.bib45); Ho et al., [2020](#bib.bib12); Rombach
    et al., [2021](#bib.bib25); Song et al., [2021](#bib.bib36); Nichol and Dhariwal,
    [2021](#bib.bib20); Song et al., [2022](#bib.bib33)) that have demonstrated significant
    capabilities in generating photorealistic images based on a given text prompt
    in open-domain problems (Nichol et al., [2022](#bib.bib21); Rombach et al., [2021](#bib.bib25);
    Saharia et al., [2022](#bib.bib28); Gu et al., [2022](#bib.bib8); Ramesh et al.,
    [2022](#bib.bib24)). However, in the specialized area of archaeological studies,
    where data is often limited and domain knowledge is required, vanilla diffusion
    models struggle to produce promising results even with finetuning, as shown in
    Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Knowledge-Aware Artifact Image
    Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"). The generated
    images often display errors in shape, patterns, and details that fail to match
    the implicit knowledge in the textual information and the underlying historical
    context of the target artifact.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显示出有潜力帮助重建古代文物视觉图像的领域是文本到图像合成。这一任务一直是一个热门的研究领域，尤其是在近年来，随着扩散模型（Yang et al.,
    [2022](#bib.bib45); Ho et al., [2020](#bib.bib12); Rombach et al., [2021](#bib.bib25);
    Song et al., [2021](#bib.bib36); Nichol and Dhariwal, [2021](#bib.bib20); Song
    et al., [2022](#bib.bib33)）的引入，它们在基于给定文本提示生成逼真图像方面展示了显著的能力（Nichol et al., [2022](#bib.bib21);
    Rombach et al., [2021](#bib.bib25); Saharia et al., [2022](#bib.bib28); Gu et
    al., [2022](#bib.bib8); Ramesh et al., [2022](#bib.bib24)）。然而，在考古学这一专业领域中，由于数据通常有限且需要领域知识，普通扩散模型即使经过微调也难以产生令人满意的结果，如图[1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision")所示。生成的图像往往在形状、图案和细节上出现错误，无法匹配文本信息中的隐含知识和目标文物的历史背景。
- en: We have identified a key cause for this problem to be the lack of knowledge
    supervision during the generating process, which can be attributed to two main
    aspects. 1). Current text prompts may not be infused with domain-specific knowledge
    from the archaeological and historical fields, leading to noisiness and lack of
    well-presented knowledge information in the text prompt. 2). The text and visual
    modules in the vanilla diffusion models (Rombach et al., [2021](#bib.bib25); Sohl-Dickstein
    et al., [2015](#bib.bib32); Song et al., [2020](#bib.bib35); Ho et al., [2020](#bib.bib12);
    Song and Ermon, [2020](#bib.bib34)) may be unable to capture domain-specific knowledge
    under the standard training pipeline, resulting in the lack of detailed textual
    and visual signals of ancient artifacts in the generation process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已确定这一问题的关键原因是生成过程中缺乏知识监督，这可以归因于两个主要方面。 1). 当前的文本提示可能没有融入考古和历史领域的领域特定知识，导致文本提示中出现噪声且知识信息呈现不佳。
    2). 原始扩散模型中的文本和视觉模块（Rombach 等，[2021](#bib.bib25)；Sohl-Dickstein 等，[2015](#bib.bib32)；Song
    等，[2020](#bib.bib35)；Ho 等，[2020](#bib.bib12)；Song 和 Ermon，[2020](#bib.bib34)）在标准训练流程下可能无法捕捉领域特定知识，从而导致生成过程中缺乏古代文物的详细文本和视觉信号。
- en: 'To address these challenges, we propose our knowledge-aware artifact image
    synthesis approach with a pretrained Chinese Stable Diffusion model (Rombach et al.,
    [2021](#bib.bib25); Zhang et al., [2022a](#bib.bib47)) as our backbone. Our method
    can generate visualizations of lost artifacts that well align with the underlying
    domain-knowledge presented in their textual records. Specifically: 1). To address
    the issue of noisiness and lack of well-presented knowledge information in the
    text prompt, we propose to use Large Language Models (LLMs) to enhance our text
    prompts in two ways: for one, we use LLMs to extract the core and meaningful information
    in the given text prompt and reorganize them in a more structured way to explicitly
    present the current knowledge information; for another, we use LLMs as an external
    knowledge base to retrieve relevant archaeological knowledge information and augment
    them in the restructured text prompt. 2). To address the lack of both textual
    and visual knowledge supervision in the generation process, we introduce additional
    supervisions in both modalities. Firstly, we introduce a contrastive training
    paradigm that enables the text encoder to make the textual representation of the
    artifact more in line with their archaeological knowledge. Secondly, we apply
    stricter visual constraints using edge loss (Seif and Androutsos, [2018](#bib.bib29))
    and perceptual loss (Johnson et al., [2016](#bib.bib13)) to make the final visual
    output align with the visual domain knowledge of ancient artifacts.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，我们提出了使用预训练的中文稳定扩散模型（Rombach 等，[2021](#bib.bib25)；Zhang 等，[2022a](#bib.bib47)）作为骨干的知识感知文物图像合成方法。我们的方法能够生成与其文本记录中呈现的基础领域知识良好对齐的失落文物的可视化效果。具体而言：1).
    为了解决文本提示中噪声和知识信息呈现不佳的问题，我们建议使用大型语言模型（LLMs）以两种方式增强我们的文本提示：一方面，我们使用LLMs提取给定文本提示中的核心和有意义的信息，并以更结构化的方式重新组织它们，以明确呈现当前的知识信息；另一方面，我们使用LLMs作为外部知识库来检索相关的考古知识信息，并将其增强到重组后的文本提示中。2).
    为了解决生成过程中文本和视觉知识监督的缺乏，我们在两种模式中引入了额外的监督。首先，我们引入了对比训练范式，使文本编码器能够使文物的文本表示与其考古知识更一致。其次，我们使用边缘损失（Seif
    和 Androutsos，[2018](#bib.bib29)）和感知损失（Johnson 等，[2016](#bib.bib13)）应用更严格的视觉约束，以使最终的视觉输出与古代文物的视觉领域知识对齐。
- en: Both quantitative experiments and a user study demonstrate that our knowledge-aware
    artifact image synthesis approach significantly outperforms existing text-to-image
    models and greatly improves the generation quality of historical artifacts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 定量实验和用户研究表明，我们的知识感知文物图像合成方法显著优于现有的文本到图像模型，并极大地提高了历史文物的生成质量。
- en: 'Overall, our main contributions can be summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的主要贡献可以总结如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose to use LLMs as both information extractor and external knowledge
    base to aid better prompt construction in a specialized domain, which in our case
    is archaeological studies.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建议使用LLMs作为信息提取器和外部知识库，以帮助在特定领域（在我们这个案例中是考古研究）中更好地构建提示。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce additional multimodal supervisions to enable our model to learn
    textual representations and visual features that better align with archaeological
    knowledge and historical context, thus improving the current finetuning paradigm
    of diffusion models.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了额外的多模态监督，以使我们的模型能够学习更好地与考古学知识和历史背景对齐的文本表示和视觉特征，从而改进当前的扩散模型微调范式。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To our best knowledge, we are the first to explore text-to-image synthesis etask
    in the archaeological domain as an attempt to recreate lost artifacts, thus aiding
    archaeologists to gain deeper insights into history.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们是首个在考古领域探索文本到图像合成任务的研究者，旨在重建失落的文物，从而帮助考古学家更深入地了解历史。
- en: 2\. Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: Text-to-image Synthesis.
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本到图像合成。
- en: 'Text-to-image synthesis tasks have long been a vital task at the intersection
    between computer vision and natural language processing, of which models are given
    a plain text description to generate the corresponding image. One major architecture
    in this area is GAN (Goodfellow et al., [2014](#bib.bib7)), whose variations (Zhang
    et al., [2016](#bib.bib46); Xu et al., [2017](#bib.bib44); Kang et al., [2023](#bib.bib14))
    have resulted in the state-of-art performance of text-to-image synthesis tasks.
    Recently, diffusion models (Rombach et al., [2021](#bib.bib25); Sohl-Dickstein
    et al., [2015](#bib.bib32); Song et al., [2020](#bib.bib35); Ho et al., [2020](#bib.bib12);
    Song and Ermon, [2020](#bib.bib34)) also have demonstrated their ability to achieve
    new state-of-the-art results (Dhariwal and Nichol, [2021](#bib.bib5)). Diffusion
    models make use of two Markov chains: forward and reverse. The forward chain gradually
    adds noise to the data with Gaussian prior. The reverse chain aims to denoise
    the data gradually. The transition probability at each timestep is learned by
    a deep neural network, which in the case of text-to-image synthesis is usually
    a U-Net (Ronneberger et al., [2015](#bib.bib27)) model.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像合成任务长期以来一直是计算机视觉和自然语言处理交汇处的重要任务，其中模型根据纯文本描述生成相应的图像。这个领域的一个主要架构是GAN（Goodfellow
    et al., [2014](#bib.bib7)），其变体（Zhang et al., [2016](#bib.bib46); Xu et al., [2017](#bib.bib44);
    Kang et al., [2023](#bib.bib14)）已经在文本到图像合成任务中取得了最先进的性能。最近，扩散模型（Rombach et al.,
    [2021](#bib.bib25); Sohl-Dickstein et al., [2015](#bib.bib32); Song et al., [2020](#bib.bib35);
    Ho et al., [2020](#bib.bib12); Song and Ermon, [2020](#bib.bib34)）也展示了它们取得新状态最先进结果的能力（Dhariwal
    and Nichol, [2021](#bib.bib5)）。扩散模型利用两个马尔科夫链：前向链和反向链。前向链逐渐向数据中添加高斯先验噪声。反向链旨在逐渐去噪数据。在每个时间步的转移概率由深度神经网络学习，在文本到图像合成的情况下，这通常是一个U-Net（Ronneberger
    et al., [2015](#bib.bib27)）模型。
- en: Large Language Models.
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型。
- en: Language models are a family of probabilistic models that predict the probability
    of the next word, given a sequence of previous words within a context. The introduction
    of GPT-3 (Brown et al., [2020](#bib.bib2)), which contains 175B parameters, has
    led to the emergence of Large Language Models (LLMs), referring to language models
    with a large number of parameters. These LLMs have demonstrated never-seen-before
    abilities, expanding the frontiers of what is possible with language models. One
    emerging ability of LLMs is in-context learning (Brown et al., [2020](#bib.bib2)),
    where LLMs are able to perform downstream tasks after being prompted with just
    a few examples without further parameter updates. Thus, by providing carefully
    designed examples, we can make use of LLMs as an information extractor given a
    noisy and unstructured text. LLMs have also shown their ability to acquire world
    knowledge from the massive training corpus (Petroni et al., [2019](#bib.bib22);
    Wang et al., [2021](#bib.bib40); Liu et al., [2022](#bib.bib19)). An efficient
    way to extract the implicit knowledge from LLMs is to ask questions with proper
    prompt engineering as LLMs are highly sensitive to the prompt input (Liang et al.,
    [2022](#bib.bib18)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是一类概率模型，它们根据上下文中的一系列前词预测下一个词的概率。GPT-3（Brown et al., [2020](#bib.bib2)）的推出，包含了175B参数，催生了大型语言模型（LLMs），即参数数量庞大的语言模型。这些LLMs展示了前所未见的能力，扩展了语言模型的可能性边界。LLMs的一个新兴能力是上下文学习（Brown
    et al., [2020](#bib.bib2)），即LLMs能够在仅通过几个示例提示后执行下游任务，而无需进一步的参数更新。因此，通过提供精心设计的示例，我们可以利用LLMs作为信息提取器，即使面对嘈杂和非结构化的文本。LLMs还展示了从大规模训练语料库中获取世界知识的能力（Petroni
    et al., [2019](#bib.bib22); Wang et al., [2021](#bib.bib40); Liu et al., [2022](#bib.bib19)）。从LLMs中提取隐性知识的有效方法是通过合适的提示工程提问，因为LLMs对提示输入非常敏感（Liang
    et al., [2022](#bib.bib18)）。
- en: 3\. Background
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 背景
- en: 3.1\. Problem Statement
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 问题陈述
- en: 'As mentioned in Section [1](#S1 "1\. Introduction ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"), for
    many artifacts, text documents are the only available source of information. Hence
    our task is to recreate a visual image $I_{i}^{\prime}$ is the corresponding artifacts
    image. The raw text information available for an artifact - as often cataloged
    in museums - contains roughly four parts: the name or title of the artifact; the
    time period of origin; a raw description of the artifact (often presented in a
    messy way); the physical size of the artifact. Formulated from accessible resources
    of such kind, the task of our work is then to generate accurate artifact images
    based on these textual descriptions of historical objects.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [1](#S1 "1\. 介绍 ‣ 基于知识的文物图像合成与 LLM 增强提示和多源监督") 节中所述，对于许多文物，文本文件是唯一可用的信息来源。因此，我们的任务是重建视觉图像
    $I_{i}^{\prime}$ 作为相应的文物图像。作为博物馆中常见的文物目录，文物的原始文本信息大致包含四个部分：文物的名称或标题；起源时间；文物的原始描述（通常呈现得很凌乱）；文物的物理尺寸。基于这些类型的可获取资源，我们的工作任务是根据这些历史对象的文本描述生成准确的文物图像。
- en: 3.2\. Diffusion Preliminaries
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 扩散初步
- en: To solve the task defined in the above section [3.1](#S3.SS1 "3.1\. Problem
    Statement ‣ 3\. Background ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision"), we propose to build our model upon the
    text-conditioned Stable Diffusion pipeline (Rombach et al., [2021](#bib.bib25)).
    Before diving deeper into our approach, we briefly introduce diffusion models
    and the Stable Diffusion architecture which serves as the backbone of our method.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述第 [3.1](#S3.SS1 "3.1\. 问题陈述 ‣ 3\. 背景 ‣ 基于知识的文物图像合成与 LLM 增强提示和多源监督") 节中定义的任务，我们建议在文本条件的稳定扩散流程
    (Rombach et al., [2021](#bib.bib25)) 上构建我们的模型。在深入探讨我们的方法之前，我们简要介绍扩散模型和作为我们方法骨干的稳定扩散架构。
- en: Diffusion Models
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩散模型
- en: '(Rombach et al., [2021](#bib.bib25); Sohl-Dickstein et al., [2015](#bib.bib32);
    Song et al., [2020](#bib.bib35); Ho et al., [2020](#bib.bib12); Song and Ermon,
    [2020](#bib.bib34)) are a family of probabilistic models which involves two processes:
    forward process and reverse process. Let $p(x_{0})$, where'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (Rombach et al., [2021](#bib.bib25); Sohl-Dickstein et al., [2015](#bib.bib32);
    Song et al., [2020](#bib.bib35); Ho et al., [2020](#bib.bib12); Song 和 Ermon,
    [2020](#bib.bib34)) 是一类概率模型，涉及两个过程：前向过程和反向过程。设 $p(x_{0})$，其中
- en: '| (1) |  | $q(x_{t}&#124;x_{0})=\prod^{T}_{t=1}q(x_{t}&#124;x_{t-1})$ |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $q(x_{t}&#124;x_{0})=\prod^{T}_{t=1}q(x_{t}&#124;x_{t-1})$ |  |'
- en: '| (2) |  | $q(x_{t}&#124;x_{t-1})=\mathcal{N}(x_{t}&#124;\sqrt{1-\beta_{t}}x_{t-1};\beta_{t}\boldsymbol{I})$
    |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $q(x_{t}&#124;x_{t-1})=\mathcal{N}(x_{t}&#124;\sqrt{1-\beta_{t}}x_{t-1};\beta_{t}\boldsymbol{I})$
    |  |'
- en: The reverse process $p_{\theta}$, where
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 反向过程 $p_{\theta}$，其中
- en: '| (3) |  | $p_{\theta}(x_{0})=p(x_{T})\prod^{T}_{t=1}p_{\theta}(x_{t-1}&#124;x_{t})$
    |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $p_{\theta}(x_{0})=p(x_{T})\prod^{T}_{t=1}p_{\theta}(x_{t-1}&#124;x_{t})$
    |  |'
- en: '| (4) |  | $1$2 |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $1$2 |  |'
- en: The original diffusion model (Ho et al., [2020](#bib.bib12)) set $\boldsymbol{\Sigma_{\theta}}(x_{t},t)=\sigma^{2}_{t}\boldsymbol{I}$,
    leading to the loss function
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 原始扩散模型 (Ho et al., [2020](#bib.bib12)) 设置了 $\boldsymbol{\Sigma_{\theta}}(x_{t},t)=\sigma^{2}_{t}\boldsymbol{I}$，导致了损失函数
- en: '| (5) |  | $L(\theta):=\mathbb{E}_{t,x_{0},\epsilon}&#124;&#124;\epsilon-\epsilon_{\theta}(\alpha_{t}x_{0}+\sigma_{t}\epsilon,t)&#124;&#124;^{2}$
    |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $L(\theta):=\mathbb{E}_{t,x_{0},\epsilon}&#124;&#124;\epsilon-\epsilon_{\theta}(\alpha_{t}x_{0}+\sigma_{t}\epsilon,t)&#124;&#124;^{2}$
    |  |'
- en: where $\alpha_{t}=\sqrt{1-\sigma^{2}_{t}}$ is a neural model using U-Net (Ronneberger
    et al., [2015](#bib.bib27)) as the backbone.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{t}=\sqrt{1-\sigma^{2}_{t}}$ 是一个使用 U-Net (Ronneberger et al., [2015](#bib.bib27))
    作为骨干的神经模型。
- en: Stable Diffusion
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稳定扩散
- en: '(SD) (Rombach et al., [2021](#bib.bib25)) introduces perceptual image compression
    which first maps the image to a latent space, then applies diffusion process and
    reverse process on the latent space rather than the pixel space which was used
    in earlier diffusion models. Overall, a Stable Diffusion model is comprised of
    three parts: a VAE (Kingma and Welling, [2013](#bib.bib16)), a text encoder and
    a U-Net (Ronneberger et al., [2015](#bib.bib27)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (SD) (Rombach et al., [2021](#bib.bib25)) 引入了感知图像压缩技术，该技术首先将图像映射到潜在空间，然后在潜在空间上应用扩散过程和反向过程，而不是早期扩散模型中使用的像素空间。总体而言，稳定扩散模型由三部分组成：一个
    VAE (Kingma 和 Welling, [2013](#bib.bib16))、一个文本编码器和一个 U-Net (Ronneberger et al.,
    [2015](#bib.bib27))。
- en: 'VAE (Kingma and Welling, [2013](#bib.bib16)) contains two parts: an encoder
    $\mathcal{E}$ in the forward process during training. The decoder part of VAE
    (Kingma and Welling, [2013](#bib.bib16)) is used to decode the denoised latent
    representation back into an image at inference time.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: VAE (Kingma 和 Welling, [2013](#bib.bib16)) 包含两个部分：一个编码器 $\mathcal{E}$ 在训练过程中用于前向处理。VAE
    (Kingma 和 Welling, [2013](#bib.bib16)) 的解码器部分用于在推理时将去噪的潜在表示解码回图像。
- en: Text Encoder $\mathcal{E}_{text}$. Stable diffusion (Rombach et al., [2021](#bib.bib25))
    uses a pre-trained CLIP (Radford et al., [2021](#bib.bib23)) as the text encoder.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器 $\mathcal{E}_{text}$。稳定扩散 (Rombach 等, [2021](#bib.bib25)) 使用了预训练的 CLIP
    (Radford 等, [2021](#bib.bib23)) 作为文本编码器。
- en: 'U-Net (Ronneberger et al., [2015](#bib.bib27)) contains two parts: an encoder
    and a decoder, with ResNet (He et al., [2015](#bib.bib10)) as the block structure.
    The encoder part projects the image into a low-resolution image presentation and
    the decoder part aims to restore the original image. Similar to the original diffusion
    model, in Stable Diffusion, the U-Net structure serves as $\epsilon_{\theta}$
    during the reverse process, conditioned on the text embedding using cross-attention
    (Vaswani et al., [2017](#bib.bib39)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net (Ronneberger 等, [2015](#bib.bib27)) 包含两个部分：一个编码器和一个解码器，以 ResNet (He 等,
    [2015](#bib.bib10)) 作为块结构。编码器部分将图像投影到低分辨率图像表示中，解码器部分旨在恢复原始图像。与原始扩散模型类似，在稳定扩散中，U-Net
    结构在反向过程中作为 $\epsilon_{\theta}$，通过交叉注意力（Vaswani 等, [2017](#bib.bib39)）对文本嵌入进行条件化。
- en: During finetuning, the training objective for Stable Diffusion can be written
    as
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，稳定扩散的训练目标可以写作
- en: '| (6) |  | $L_{SD}(\theta):=\mathbb{E}_{t,\mathcal{E}(x_{0}),\epsilon}&#124;&#124;\epsilon-\epsilon_{\theta}(z_{t},t,w)&#124;&#124;^{2}$
    |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $L_{SD}(\theta):=\mathbb{E}_{t,\mathcal{E}(x_{0}),\epsilon}&#124;&#124;\epsilon-\epsilon_{\theta}(z_{t},t,w)&#124;&#124;^{2}$
    |  |'
- en: where $\epsilon$ is the latent space encoder.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$ 是潜在空间编码器。
- en: '![Refer to caption](img/3d393f70123ecaae1f28e5d4a71a755a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3d393f70123ecaae1f28e5d4a71a755a.png)'
- en: Figure 2\. Raw artifact descriptions fail to depict the artifact with sufficient
    archaeological information, such as their artifact-“type” which determines important
    aspects of their visual appearance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 原始文物描述未能提供足够的考古信息来描绘文物，例如它们的文物-“类型”，这决定了它们视觉外观的重要方面。
- en: 4\. Our Method
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 我们的方法
- en: '![Refer to caption](img/48bbfd2da627bb2e1aadc8998f4a0a3f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/48bbfd2da627bb2e1aadc8998f4a0a3f.png)'
- en: 'Figure 3\. Our proposed knowledge-aware approach is illustrated in a). It features
    a Chinese Stable Diffusion model as backbone and our proposed three key techniques
    labeled as follows: b) illustrates the way of performing textual contrastive learning,
    which is discussed in Section [4.2](#S4.SS2 "4.2\. Alignment with Domain-Expertise
    via Contrastive Learning ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision"); c) is edge loss, and
    d) is perceptual loss, both of which are part of the additional visual-semantic
    supervision, as discussed in Section [4.3](#S4.SS3 "4.3\. Restoring Visual Details
    via Additional Visual-Semantic Supervision ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 我们提出的知识感知方法如 a) 所示。它以一个中文稳定扩散模型作为骨干，并标注了我们提出的三个关键技术：b) 展示了执行文本对比学习的方法，详见第[4.2节](#S4.SS2
    "4.2\. Alignment with Domain-Expertise via Contrastive Learning ‣ 4\. Our Method
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision")；c) 是边缘损失，d) 是感知损失，这两者都是额外视觉语义监督的一部分，如第[4.3节](#S4.SS3 "4.3\. Restoring
    Visual Details via Additional Visual-Semantic Supervision ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")中所述。
- en: Our proposed approach for knowledge-aware artifact image synthesis is built
    upon a pretrained Stable Diffusion model, which retains its powerful generative
    capability of common domains and is further finetuned to align with the specific
    characteristics of ancient artifacts. The generic Stable Diffusion model, even
    with finetuning, however, struggles to generate visually and historically accurate
    artifact images and shows multifaceted errors, as is demonstrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision").
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的知识感知文物图像合成方法基于一个预训练的稳定扩散模型，该模型保留了其在常见领域的强大生成能力，并进一步微调以适应古代文物的特定特征。然而，即使经过微调，通用的稳定扩散模型在生成视觉和历史上准确的文物图像时仍然存在困难，并表现出多方面的错误，如图[1](#S1.F1
    "图 1 ‣ 1\. 引言 ‣ 知识感知文物图像合成与LLM增强提示及多源监督")所示。
- en: 'To address these issues, we propose specific modifications at three steps in
    the Stable Diffusion system: 1). Given source text information $T_{i}$ and apply
    additional visual-semantic supervision with edge loss (Seif and Androutsos, [2018](#bib.bib29))
    and perceptual loss (Johnson et al., [2016](#bib.bib13)) to steer the generation
    of our model closer towards the ground-truth appearance of artifacts.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们在稳定扩散系统的三个步骤中提出了具体的修改：1). 给定源文本信息 $T_{i}$，并应用额外的视觉-语义监督，包括边缘损失（Seif
    和 Androutsos，[2018](#bib.bib29)）和感知损失（Johnson 等，[2016](#bib.bib13)），以使我们模型生成的图像更接近真实的文物外观。
- en: The overall framework for our approach is illustrated in Figure [3](#S4.F3 "Figure
    3 ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision") and explained in detail in the following
    subsections.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的总体框架如图[3](#S4.F3 "图 3 ‣ 4\. 我们的方法 ‣ 知识感知文物图像合成与LLM增强提示及多源监督")所示，并在以下小节中详细解释。
- en: 4.1\. Prompt-Construction Enhanced by LLM
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 由LLM增强的提示构建
- en: 'We have noticed that the raw description of an artifact accessible in museum
    resources (as mentioned in Section [3.1](#S3.SS1 "3.1\. Problem Statement ‣ 3\.
    Background ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision")) is far from ideal for prompting a text-to-image
    model. It is often incomplete and filled with noisy messages and fails to sufficiently
    depict a historical object. Other than the messiness problem, these off-the-shelf
    descriptions may well lack specific information about an artifact that is essential
    to its visual form, such as its fundamental classification (or: artifact-“type”).
    An example¹¹1To maintain a consistent language usage throughout the paper, we
    translate all Chinese text (e.g., the textual descriptions of artifacts) into
    English via ChatGPT. of this is given in Figure [2](#S3.F2 "Figure 2 ‣ Stable
    Diffusion ‣ 3.2\. Diffusion Preliminaries ‣ 3\. Background ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"). The
    fact that the artifact on the left side is classified as a “Round Ding” rather
    than a “Square Ding” confines its shape to a round body having only three legs
    as opposed to four legs. However, key archaeological information of this kind
    is often missing in the raw description of the artifact, prohibiting a text-to-image
    model from sufficiently understanding the association between the visual appearance
    and the textual prompt.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，博物馆资源中可获得的文物原始描述（如第[3.1](#S3.SS1 "3.1\. 问题陈述 ‣ 3\. 背景 ‣ 知识感知文物图像合成与LLM增强提示及多源监督")节中提到的）远非理想，用于提示文本到图像模型。它们通常是不完整的，充满了噪声信息，无法充分描述历史物体。除了杂乱的问题外，这些现成的描述可能缺乏关于文物的重要信息，这些信息对其视觉形式至关重要，例如其基本分类（或称：文物-“类型”）。一个例子¹¹1为了在整篇论文中保持一致的语言使用，我们通过
    ChatGPT 将所有中文文本（例如文物的文本描述）翻译成英文。见图[2](#S3.F2 "图 2 ‣ 稳定扩散 ‣ 3.2\. 扩散基础 ‣ 3\. 背景
    ‣ 知识感知文物图像合成与LLM增强提示及多源监督")。左侧的文物被分类为“圆鼎”而非“方鼎”，这限制了其形状为只有三条腿的圆形主体，而不是四条腿。然而，这种关键考古信息常常在文物的原始描述中缺失，阻碍了文本到图像模型对视觉外观与文本提示之间关联的充分理解。
- en: To alleviate the problems of noisiness and knowledge deficiency in the original
    text information, we propose to utilize an LLM as both an information extractor
    to retrieve the most useful information, and as an external knowledge base to
    complete any missing important attributes of the artifact.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解原始文本信息中的噪声和知识缺乏问题，我们建议利用LLM作为信息提取器来检索最有用的信息，并作为外部知识库来补充文物的任何缺失的重要属性。
- en: Based on archaeological expertise, we have compiled a list of key attributes
    that are vital for effectively describing artifacts and defining their physical
    forms, see Table [1](#S4.T1 "Table 1 ‣ 4.1\. Prompt-Construction Enhanced by LLM
    ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision"). Examples of these attributes are given
    in Table [5](#A0.T5 "Table 5 ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision") in Appendix [A](#A1 "Appendix A Example
    of the Enhanced Artifact Attributes ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision"). While the “name”,
    “time period” and “size” of an artifact are usually available in museum resources,
    the specific “material”, “shape” and “pattern” need to be extracted or derived
    from the raw description of the object. Further, as explained above, the classified
    “type” of an artifact determines certain fundamental aspects of its looks, which
    are specified by the generic definition of this artifact-type (i.e., “type definition”).
    It requires a general knowledge of archaeology to be able to categorize an ancient
    object into a certain artifact-type and to define the basic appearance of this
    type.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于考古学专业知识，我们编制了一份关键属性列表，这些属性对于有效描述文物和定义其物理形态至关重要，请参见表格[1](#S4.T1 "表 1 ‣ 4.1\.
    由 LLM 增强的提示构建 ‣ 4\. 我们的方法 ‣ 知识驱动的文物图像合成与 LLM 增强的提示和多源监督")。这些属性的示例列在附录[A](#A1 "附录
    A 增强文物属性示例 ‣ 知识驱动的文物图像合成与 LLM 增强的提示和多源监督")中的表格[5](#A0.T5 "表 5 ‣ 知识驱动的文物图像合成与 LLM
    增强的提示和多源监督")。虽然文物的“名称”、“时间段”和“尺寸”通常可以在博物馆资源中找到，但具体的“材料”、“形状”和“图案”需要从文物的原始描述中提取或推导出来。此外，如上所述，文物的分类“类型”决定了其外观的某些基本方面，这些方面由这种文物类型的通用定义（即“类型定义”）指定。需要具备一般的考古学知识才能将古代物品归类为某种文物类型，并定义这种类型的基本外观。
- en: '| Expert Attribute | Definition |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 专家属性 | 定义 |'
- en: '| --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Name | name or title of an artifact |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 文物的名称或标题 |'
- en: '| Material | the material an artifact is made of |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 材料 | 文物所用的材料 |'
- en: '| Time Period | time period of origin |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 时间段 | 起源的时间段 |'
- en: '| Type | classified type of an artifact |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 文物的分类类型 |'
- en: '| Type Definition | general definition of artifact type |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 类型定义 | 文物类型的一般定义 |'
- en: '| Shape | shape and structure of an artifact |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 形状 | 文物的形状和结构 |'
- en: '| Pattern | patterns/motifs on an artifact |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 图案 | 文物上的图案/纹饰 |'
- en: '| Size | physical dimensions of an artifact |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 尺寸 | 文物的物理尺寸 |'
- en: Table 1\. Expert attributes of artifacts that are vital to their visual appearance
    according to archaeological expertise. See Table [5](#A0.T5 "Table 5 ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")
    in Appendix [A](#A1 "Appendix A Example of the Enhanced Artifact Attributes ‣
    Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision") for examples of these attributes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 根据考古学专业知识，影响文物视觉外观的重要专家属性。有关这些属性的示例，请参见附录[A](#A1 "附录 A 增强文物属性示例 ‣ 知识驱动的文物图像合成与
    LLM 增强的提示和多源监督")中的表格[5](#A0.T5 "表 5 ‣ 知识驱动的文物图像合成与 LLM 增强的提示和多源监督")。
- en: 'An LLM is well-suited for fulfilling these two tasks with its ability to obtain
    a certain extent of world knowledge from the massive pretraining corpus (Liang
    et al., [2022](#bib.bib18)) and to learn to perform specialized downstream tasks
    using the in-context learning paradigm (Brown et al., [2020](#bib.bib2)). Specifically,
    we use GPT-3.5-TURBO as our knowledge-base LLM, and the prompt for querying GPT-3.5
    is designed with a similar format following self-instruct (Wang et al., [2022](#bib.bib41)).
    Our prompt template consists of three parts: 1). A task statement that describes
    to GPT-3.5 the task to be done; 2). Two in-context examples of high quality sampled
    from our labeled pool of 54 artifacts written by archaeology experts; 3). The
    target artifacts whose “material”, “shape”, “pattern”, “type” and “type definition”
    are left blank and need to be answered by GPT-3.5\. The former 3 attributes can
    be retrieved from the given “description” and the latter 2 artifact-type related
    features need to be fulfilled via the world knowledge of GPT-3.5 and its in-context
    learning from the given human-labeled examples. An example of our prompt for querying
    artifact information is illustrated in Figure [6](#A4.F6 "Figure 6 ‣ Learned Perceptual
    Image Patch Similarity (LPIPS). ‣ Appendix D More Details on Evaluation Metrics
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision") in Appendix [B](#A2 "Appendix B Example of the Prompt Template for
    Querying GPT-3.5 ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LLM非常适合完成这两个任务，因为它能够从大量预训练语料库中获得一定程度的世界知识（Liang et al., [2022](#bib.bib18)），并通过上下文学习范式学习执行专门的下游任务（Brown
    et al., [2020](#bib.bib2)）。具体来说，我们使用GPT-3.5-TURBO作为我们的知识库LLM，查询GPT-3.5的提示设计遵循类似的格式，采用self-instruct（Wang
    et al., [2022](#bib.bib41)）。我们的提示模板包括三个部分：1）描述给GPT-3.5要完成的任务的任务声明；2）两个来自考古专家编写的54件文物标注池中的高质量上下文示例；3）目标文物，其中的“材料”、“形状”、“图案”、“类型”和“类型定义”留空，需要由GPT-3.5回答。前3个属性可以从给定的“描述”中提取，后2个与文物类型相关的特征需要通过GPT-3.5的世界知识和从给定人工标注示例中获得的上下文学习来完成。我们查询文物信息的提示示例在附录[B](#A2
    "附录 B 查询GPT-3.5的提示模板示例 ‣ 知识感知文物图像合成与LLM增强提示和多源监督")中的图[6](#A4.F6 "图 6 ‣ 学习感知图像补丁相似性
    (LPIPS). ‣ 附录 D 评估指标的更多细节 ‣ 知识感知文物图像合成与LLM增强提示和多源监督")中有所示例。
- en: By leveraging the power of LLM as both an information extractor and external
    knowledge provider, we are able to collect all the key attributes of a given artifact,
    which are then rearranged into the prompt to our diffusion model with a $[SEP]$
    (implemented as a Chinese comma in our work) splitting each key feature, as shown
    by the example in Table [5](#A0.T5 "Table 5 ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision") in Appendix [A](#A1
    "Appendix A Example of the Enhanced Artifact Attributes ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"). Such
    input prompt thus contains enriched text information that provides well-defined
    archaeology-knowledge guidance. It assists the text-to-image diffusion model in
    synthesizing a more realistic result that better corresponds to the ground-truth
    artifacts.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用LLM作为信息提取器和外部知识提供者的能力，我们能够收集给定文物的所有关键属性，这些属性随后被重新排列到我们扩散模型的提示中，并用$[SEP]$（在我们的工作中实现为中文逗号）分隔每个关键特征，如附录[A](#A1
    "附录 A 增强文物属性示例 ‣ 知识感知文物图像合成与LLM增强提示和多源监督")中表[5](#A0.T5 "表 5 ‣ 知识感知文物图像合成与LLM增强提示和多源监督")的示例所示。这样的输入提示包含了丰富的文本信息，提供了明确的考古知识指导。它帮助文本到图像的扩散模型合成一个更真实的结果，更好地对应于实际文物。
- en: 4.2\. Alignment with Domain-Expertise via Contrastive Learning
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 通过对比学习与领域专家知识的对齐
- en: Another issue we identify is that the text encoder might not encode the text
    into a representation that reflects the underlying archaeological knowledge and
    thus needs further finetuning. We observe that the names of ancient artifacts
    are often accurate and concise summarizations of the artifact’s key attributes,
    while the descriptions provide an extended version of the artifact’s features.
    Given that both the names and descriptions are provided by domain experts - as
    written in museum sources, they reflect a high level of expertise in the field.
    Thus, we believe that closely aligning the names and descriptions is essential
    to reflect this domain knowledge.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别出的另一个问题是，文本编码器可能无法将文本编码为反映基础考古知识的表示，因此需要进一步的微调。我们观察到，古代文物的名称通常是文物关键属性的准确而简洁的总结，而描述则提供了文物特征的扩展版本。鉴于名称和描述都是由领域专家提供的——如博物馆资料所述，它们反映了该领域的高水平专业知识。因此，我们认为紧密对齐名称和描述对于反映这一领域知识至关重要。
- en: To achieve this goal, we propose the use of contrastive learning, which aims
    to minimize the distance between positive pairs, consisting of matching $([description]_{i},[name]_{i})$,
    and to maximize the distance between negative ones with mismatching names. However,
    we have also observed that artifacts with similar attributes (i.e., similar description
    contents) and origins share similar names, making it unintuitive to finetune the
    text encoder to differentiate between similar pairs. We believe that such pairs
    should be close to each other in the semantic space. Therefore, we readjust our
    sampling strategy for negative pairs. From the perspective of historical studies,
    “time period” is one of the most determining factors in the style and appearance
    of an artifact, where different artifacts from different eras can be vastly different.
    Therefore, aiming to separate hard negatives rather than slightly different ones,
    we sample our negative samples from artifact names in different eras.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们提出使用对比学习，旨在最小化匹配的正样本对 $([description]_{i},[name]_{i})$ 之间的距离，并最大化负样本对（名称不匹配）之间的距离。然而，我们也观察到，具有相似属性（即相似描述内容）和来源的文物往往具有相似的名称，这使得对文本编码器进行微调以区分相似的对样本变得不直观。我们认为这样的对样本应该在语义空间中彼此接近。因此，我们重新调整了负样本的采样策略。从历史研究的角度来看，“时间段”是决定文物风格和外观的最重要因素之一，不同年代的文物可能有很大不同。因此，为了分离困难的负样本而非仅有些许差异的负样本，我们从不同年代的文物名称中采样负样本。
- en: 'In our approach, we use InfoNCE (van den Oord et al., [2018](#bib.bib38)) to
    penalize the misalignment in the representation encoded by the text encoder. The
    formula for text contrastive learning can be written as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方法中，我们使用 InfoNCE (van den Oord 等人, [2018](#bib.bib38)) 来惩罚文本编码器编码的表示中的不对齐。文本对比学习的公式可以写成：
- en: '| (7) |  | $L_{\text{text}}:=-\mathbb{E}_{X}\log\frac{EXP(x_{i})}{\sum_{x_{j}\in
    X}EXP(x_{j})}$ |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $L_{\text{text}}:=-\mathbb{E}_{X}\log\frac{EXP(x_{i})}{\sum_{x_{j}\in
    X}EXP(x_{j})}$ |  |'
- en: where $x_{i}=\mathcal{E}([description]_{i})\cdot\mathcal{E}([name]_{i})$.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{i}=\mathcal{E}([description]_{i})\cdot\mathcal{E}([name]_{i})$。
- en: 4.3\. Restoring Visual Details via Additional Visual-Semantic Supervision
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 通过额外的视觉-语义监督恢复视觉细节
- en: Artifact images generated by the vanilla Stable Diffusion model suffer from
    blurry edges and false color and patterns under the current setting (see Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision")), implying that stricter visual constraints
    need to be enforced to address these issues. Therefore, we propose to use edge
    loss (Seif and Androutsos, [2018](#bib.bib29)) and perceptual loss (Johnson et al.,
    [2016](#bib.bib13)) that apply additional visual-semantic supervision on images
    generated by our Stable Diffusion model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由普通的 Stable Diffusion 模型生成的文物图像在当前设置下存在模糊的边缘和错误的颜色及图案（见图 [1](#S1.F1 "图 1 ‣ 1\.
    介绍 ‣ 知识感知文物图像合成与 LLM 增强提示和多源监督")），这表明需要施加更严格的视觉约束以解决这些问题。因此，我们提议使用边缘损失 (Seif 和
    Androutsos, [2018](#bib.bib29)) 和感知损失 (Johnson 等人, [2016](#bib.bib13))，在我们 Stable
    Diffusion 模型生成的图像上应用额外的视觉-语义监督。
- en: Edge Loss.
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 边缘损失。
- en: 'Building upon the insights from (Seif and Androutsos, [2018](#bib.bib29)),
    we penalize the differences in contours between two images, by aiming to minimize
    the $L_{2}$ distance between their edge maps, as shown in part c) of Figure [3](#S4.F3
    "Figure 3 ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision"). Since the vanilla Stable Diffusion model
    often produces images that suffer from the problem of incorrect and blurry shape
    compared to the ground-truth artifact, it is necessary to penalize such errors
    as defined here in the edge loss:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基于（Seif 和 Androutsos, [2018](#bib.bib29)）的见解，我们通过尽量减少两个图像的边缘图之间的 $L_{2}$ 距离来惩罚轮廓之间的差异，如图
    [3](#S4.F3 "图 3 ‣ 4\. 我们的方法 ‣ 知识感知伪影图像合成与 LLM 增强提示和多源监督") c) 部分所示。由于原始的 Stable
    Diffusion 模型常常生成与真实伪影相比形状不正确且模糊的图像，因此有必要根据边缘损失对这些错误进行惩罚：
- en: '| (8) |  | $L_{\text{edge}}:=&#124;&#124;EDGE(I_{i})-EDGE(I^{\prime}_{i})&#124;&#124;^{2}$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $L_{\text{edge}}:=\|\|EDGE(I_{i})-EDGE(I^{\prime}_{i})\|\|^{2}$
    |  |'
- en: where $EDGE(\cdot)$ distance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $EDGE(\cdot)$ 距离。
- en: '![Refer to caption](img/68812f7f3ac5389bdab2f12bf3a98164.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/68812f7f3ac5389bdab2f12bf3a98164.png)'
- en: Figure 4\. Canny edge maps of artifacts
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 伪影的 Canny 边缘图
- en: Perceptual Loss.
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 感知损失。
- en: Similar to (Johnson et al., [2016](#bib.bib13)), we also penalize the problem
    of mismatching high-level details between the generated image and the real one.
    As we have also observed on the images generated by vanilla Stable Diffusion model,
    the high-level details (such as colors and patterns) are often misaligned with
    the original ones. Therefore, we incorporate perceptual loss into our training
    process to tackle such issue, as perceptual loss works by mapping the images into
    a semantic space using a pretrained network, and then minimizing the difference
    between the high-level features of the generated image and the original image.
    The formula for perceptual loss is defined as
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于（Johnson 等人, [2016](#bib.bib13)），我们还惩罚生成图像与真实图像之间高层次细节的不匹配。正如我们在原始 Stable
    Diffusion 模型生成的图像中观察到的那样，高层次的细节（如颜色和图案）通常与原始图像不一致。因此，我们在训练过程中引入了感知损失来解决这个问题，因为感知损失通过使用预训练的网络将图像映射到语义空间，然后最小化生成图像和原始图像的高层次特征之间的差异。感知损失的公式定义为
- en: '| (9) |  | $L_{\text{perceptual}}:=&#124;&#124;\phi(I_{i})-\phi(I^{\prime}_{i})&#124;&#124;^{2}$
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $L_{\text{perceptual}}:=\|\|\phi(I_{i})-\phi(I^{\prime}_{i})\|\|^{2}$
    |  |'
- en: where $\phi$ denotes a pretrained image encoder to extract the high-level features
    of an image. This is applied to impose a stricter supervision on color, texture,
    and other high-level features. In our method, we use a CLIP-ViT-L/14 (Radford
    et al., [2021](#bib.bib23)) image encoder to act as our pretrained image encoder
    for perceptual loss.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi$ 表示一个预训练的图像编码器，用于提取图像的高层次特征。这是为了对颜色、纹理和其他高层次特征施加更严格的监督。在我们的方法中，我们使用
    CLIP-ViT-L/14（Radford 等人, [2021](#bib.bib23)）图像编码器作为我们感知损失的预训练图像编码器。
- en: 4.4\. Objective Functions
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 目标函数
- en: 'Combining all the extra multi-source multi-modal supervisions above, the overall
    training objective of our system is:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 综合以上所有额外的多源多模态监督，我们系统的整体训练目标是：
- en: '| (10) |  | $1$2 |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $1$2 |  |'
- en: where $\lambda_{1}$ are the ground-truth and the restored image from our model’s
    prediction.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1}$ 是从我们模型预测中恢复的真实图像。
- en: '| Models | Prompt | $\lambda_{1}$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 提示 | $\lambda_{1}$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Taiyi-SD-finetuned-description | raw description | - | - | - | 0.772 | 0.536
    | 0.608 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Taiyi-SD-finetuned-description | 原始描述 | - | - | - | 0.772 | 0.536 | 0.608
    |'
- en: '| Taiyi-SD-finetuned-attributes | LLM enhanced attribute | - | - | - | 0.792
    | 0.554 | 0.598 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Taiyi-SD-finetuned-attributes | LLM 增强属性 | - | - | - | 0.792 | 0.554 | 0.598
    |'
- en: '| OURS-attributes +text | LLM enhanced attributes | 0.5 | - | - | 0.801 | 0.580
    | 0.552 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| OURS-attributes +text | LLM 增强属性 | 0.5 | - | - | 0.801 | 0.580 | 0.552 |'
- en: '| OURS-attributes +edge+perceptual | LLM enhanced attributes | - | 0.3 | 0.1
    | 0.815 | 0.636 | 0.497 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| OURS-attributes +edge+perceptual | LLM 增强属性 | - | 0.3 | 0.1 | 0.815 | 0.636
    | 0.497 |'
- en: '| OURS-attributes +text+edge+perceptual | LLM enhanced attributes | 0.3 | 0.3
    | 0.1 | 0.831 | 0.594 | 0.536 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| OURS-attributes +text+edge+perceptual | LLM 增强属性 | 0.3 | 0.3 | 0.1 | 0.831
    | 0.594 | 0.536 |'
- en: Table 2\. Quantitative comparison of our models against the finetuned Taiyi-SD
    baselines over CLIP Visual Similarity (CLIP-VS), SSIM and LPIPS.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 我们的模型与微调后的 Taiyi-SD 基准模型在 CLIP 视觉相似度 (CLIP-VS)、SSIM 和 LPIPS 上的定量比较。
- en: '| Prompt | CLIP-VS $\uparrow$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | CLIP-VS $\uparrow$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Raw description | 0.748 | 0.383 | 0.748 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 原始描述 | 0.748 | 0.383 | 0.748 |'
- en: '| LLM enhanced attributes-sequence | 0.765 | 0.413 | 0.730 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LLM增强属性序列 | 0.765 | 0.413 | 0.730 |'
- en: Table 3\. Quantitative comparison between zero-shot Taiyi-SD models using different
    textual prompts over CLIP Visual Similarity (CLIP-VS), SSIM and LPIPS.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 使用不同文本提示对零样本Taiyi-SD模型进行的定量比较，包括CLIP视觉相似度（CLIP-VS）、SSIM和LPIPS。
- en: '| Models | Material $\uparrow$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 材料 $\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Taiyi-SD-finetuned | 2.66 | 1.50 | 1.44 | 1.79 | 2.12 | 1.90 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Taiyi-SD微调版 | 2.66 | 1.50 | 1.44 | 1.79 | 2.12 | 1.90 |'
- en: '| OURS | 3.94 | 3.38 | 3.25 | 3.30 | 3.20 | 3.41 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 我们的模型 | 3.94 | 3.38 | 3.25 | 3.30 | 3.20 | 3.41 |'
- en: Table 4\. Human evaluation of the quality of artifact images generated by the
    finetuned baseline and our model. The images are rated from 5 different aspects
    on a scale of 0 to 5 by 20 archaeology experts from top institutions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表4\. 对微调基线和我们模型生成的工件图像质量的人工评估。20位顶级机构的考古专家从5个不同方面对这些图像进行0到5的评分。
- en: 5\. Experiment
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 实验
- en: 5.1\. Experimental Setup
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 实验设置
- en: Dataset.
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集。
- en: Due to the sparsity of paired text-image data in the ancient artifact domain,
    we build our own text-to-image dataset by collecting artifact information from
    National Palace Museum Open Data Platform (Taipei National Palace Museum, [2019](#bib.bib37)).
    After careful cleansing of available entries, we are left with 16,092 unique artifact
    samples with their descriptions and ground-truth images. We split the data by
    80%/10%/10% for training, validation, and testing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于古代工件领域中配对的文本-图像数据稀缺，我们通过从国立故宫博物院开放数据平台（台北故宫博物院，[2019](#bib.bib37)）收集工件信息，建立了自己的文本到图像数据集。在仔细清理可用条目后，我们得到了16,092个独特的工件样本及其描述和真实图像。我们将数据分为80%/10%/10%用于训练、验证和测试。
- en: Implementation Details.
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施细节。
- en: For our backbone model, we use a pretrained Chinese Stable Diffusion Taiyi-Stable-Diffusion-1B-Chinese-v0.1
    (Zhang et al., [2022a](#bib.bib47)) (dubbed Taiyi-SD) which was trained on 20M
    filtered Chinese image-text pairs. Taiyi-SD inherits the same VAE and U-Net from
    stable-diffusion-v1-4 (Rombach et al., [2022](#bib.bib26)) and trains a Chinese
    text encoder from Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese (Zhang et al., [2022b](#bib.bib48))
    to align Chinese prompts with the images. Further training details are left in
    Appendix [C](#A3 "Appendix C More on Implementation Details ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision").
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的主干模型，我们使用了一个预训练的中文稳定扩散模型Taiyi-Stable-Diffusion-1B-Chinese-v0.1（Zhang et
    al., [2022a](#bib.bib47)）（称为Taiyi-SD），该模型在2000万对筛选后的中文图像-文本对上进行了训练。Taiyi-SD继承了稳定扩散v1-4（Rombach
    et al., [2022](#bib.bib26)）中的相同VAE和U-Net，并从Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese（Zhang
    et al., [2022b](#bib.bib48)）中训练了一个中文文本编码器，以对齐中文提示与图像。进一步的训练细节请参见附录[C](#A3 "附录
    C 实施细节的更多信息 ‣ 知识感知的工件图像合成与LLM增强提示和多源监督")。
- en: Evaluation Metrics.
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标。
- en: 'To comprehensively evaluate our method for text-to-image synthesis quantitatively,
    we employ three commonly used metrics that measure image generation quality: CLIP
    Visual Similarity, Structural Similarity Index (SSIM) (Wang et al., [2004](#bib.bib42))
    and Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., [2018](#bib.bib49)).
    Each of them highlights different aspects of the generated image. Together, they
    provide a thorough judgment of a synthesized artifact image in terms of its overall
    resemblance to the ground truth, the accuracy of its shape and pattern, and its
    perceptual affinity to the target image. We leave an extensive explanation of
    these metrics in Appendix [D](#A4 "Appendix D More Details on Evaluation Metrics
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面定量评估我们的方法用于文本到图像的合成，我们使用了三种常用的度量标准来衡量图像生成质量：CLIP视觉相似度、结构相似性指数（SSIM）（Wang
    et al., [2004](#bib.bib42)）和学习感知图像补丁相似性（LPIPS）（Zhang et al., [2018](#bib.bib49)）。每种度量标准突出生成图像的不同方面。它们共同提供了对合成工件图像的全面判断，包括其与真实图像的整体相似度、形状和模式的准确性，以及与目标图像的感知亲和力。我们在附录[D](#A4
    "附录 D 评估指标的更多细节 ‣ 知识感知的工件图像合成与LLM增强提示和多源监督")中详细解释了这些度量标准。
- en: '![Refer to caption](img/f7ba795988979112df9fa50e6271125a.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f7ba795988979112df9fa50e6271125a.png)'
- en: Figure 5\. Comparison between the finetuned Taiyi-SD baseline model and OUR
    approach trained with additional edge loss and perceptual loss against the ground
    truth. Clearly, objects generated by OUR model display more accurate shape, colors,
    and patterns when compared to the ground truth, whereas these delicate visual
    details are easily neglected by the baseline.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 微调的Taiyi-SD基线模型与我们的方法的比较，后者经过额外的边缘损失和感知损失训练，相对于真实情况。显然，我们模型生成的物体在形状、颜色和图案上更为准确，而基线模型容易忽视这些细腻的视觉细节。
- en: 5.2\. Main Results and Discussion
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 主要结果与讨论
- en: In Table [2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\. Our Method ‣
    Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision"), we compare the quantitative results of our approach with the baselines
    on our test set. The first column denotes the models we experimented with.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")中，我们比较了我们的方法与基线在测试集上的定量结果。第一列表示我们实验的模型。
- en: 'For the baselines, we use Taiyi-SD via two versions: Taiyi-SD-finetuned-description:
    the finetuned Taiyi-SD with the raw description (directly available from museum
    archives) as input prompt; Taiyi-SD-finetuned-attributes: the finetuned Taiyi-SD
    using LLM-enhanced prompt (a sequence of artifact attributes) as designed in Section [4.1](#S4.SS1
    "4.1\. Prompt-Construction Enhanced by LLM ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision").'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基线，我们使用两种版本的Taiyi-SD：Taiyi-SD-finetuned-description：使用原始描述（直接从博物馆档案中获得）作为输入提示的微调Taiyi-SD；Taiyi-SD-finetuned-attributes：使用LLM增强提示（由一系列文物属性组成）的微调Taiyi-SD，如第[4.1](#S4.SS1
    "4.1\. Prompt-Construction Enhanced by LLM ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")节所设计的。
- en: 'For our approach, we apply the LLM-enhanced prompt by default and also explore
    three different versions of extra supervisions in addition to training the Taiyi-SD
    backbone: OURS-attributes +text: finetuning with additional text contrastive loss
    (see Section [4.2](#S4.SS2 "4.2\. Alignment with Domain-Expertise via Contrastive
    Learning ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision")) to align the text representation of
    our model with domain expertise; OURS-attributes +edge+perceptual: finetuning
    with edge loss and perceptual loss (see Section [4.3](#S4.SS3 "4.3\. Restoring
    Visual Details via Additional Visual-Semantic Supervision ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"))
    as additional supervision to enforce more visual-semantic constraints on the image
    generation process; OURS-attributes +text+edge+perceptual: finetuning with both
    text contrastive loss and the edge and perceptual loss as multi-source supervisions.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的方法，我们默认应用LLM增强提示，并且在训练Taiyi-SD主干网络的基础上，还探索了三种不同版本的额外监督：OURS-attributes
    +text：通过额外的文本对比损失进行微调（见第[4.2](#S4.SS2 "4.2\. Alignment with Domain-Expertise via
    Contrastive Learning ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision")节），以将模型的文本表示与领域专业知识对齐；OURS-attributes
    +edge+perceptual：通过边缘损失和感知损失进行微调（见第[4.3](#S4.SS3 "4.3\. Restoring Visual Details
    via Additional Visual-Semantic Supervision ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")节），作为额外的监督，以对图像生成过程施加更多的视觉-语义约束；OURS-attributes
    +text+edge+perceptual：结合文本对比损失、边缘损失和感知损失进行微调，作为多源监督。
- en: Overall, our proposed artifact image synthesis approach significantly outperforms
    the finetuned Taiyi-SD-baselines across all metrics. The improvement on SSIM indicates
    that images generated by our model better preserve the shapes and boundaries of
    the original artifacts. An increase in CLIP Visual Similarity also indicates that
    our approach produces images that are more closely aligned to the ground truths.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们提出的文物图像合成方法在所有指标上明显优于微调的Taiyi-SD基线。SSIM的改善表明，我们模型生成的图像在形状和边界的保留上更佳。CLIP视觉相似性的增加也表明，我们的方法生成的图像与真实情况的对齐程度更高。
- en: Additional visual-semantic constraints in the form of edge loss and perceptual
    loss contribute greatly to boosting the SSIM and LPIPS scores. This can be attributed
    to the fact that edge loss and perceptual loss put a stricter condition on both
    structural details like edge and contour (captured by SSIM) and perceptual-level
    image features like color and texture (captured by LPIPS). These visual details
    are exactly much desired in our case of artifact image synthesis, as the shape,
    pattern, and texture of artifacts are of vital importance for determining their
    historical position and status.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的视觉-语义约束形式，如边缘损失和感知损失，大大提高了SSIM和LPIPS分数。这可以归因于边缘损失和感知损失对结构细节（如边缘和轮廓，由SSIM捕捉）以及感知级别图像特征（如颜色和纹理，由LPIPS捕捉）施加了更严格的条件。在我们所处理的伪影图像合成中，这些视觉细节正是非常重要的，因为伪影的形状、图案和纹理对于确定其历史位置和状态至关重要。
- en: By further incorporating the text contrastive loss into the overall training
    objective, we observe a slight increase in CLIP Visual Similarity, yet a decrease
    in SSIM and LPIPS scores. We believe there are two reasons behind this phenomenon.
    For one, by aligning the text knowledge (descriptions with names) (see Section [4.2](#S4.SS2
    "4.2\. Alignment with Domain-Expertise via Contrastive Learning ‣ 4\. Our Method
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision")), the textual guidance for generating the image is better represented
    and closer to the general visual content of the artifact, thus leading to a higher
    CLIP Visual Similarity. For another, the relative weight of edge and perceptual
    loss is reduced with the additional text contrastive loss, which might compromise
    the strict supervision on structural coherence and perceptual similarity and limit
    the model’s performance on SSIM and LPIPS.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将文本对比损失进一步纳入整体训练目标，我们观察到CLIP视觉相似性略有增加，但SSIM和LPIPS分数有所下降。我们认为这种现象有两个原因。首先，通过对齐文本知识（描述与名称）（见第[4.2](#S4.SS2
    "4.2\. Alignment with Domain-Expertise via Contrastive Learning ‣ 4\. Our Method
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision")节），生成图像的文本指导得到了更好的表征，更接近伪影的一般视觉内容，从而导致更高的CLIP视觉相似性。另一方面，随着额外的文本对比损失的引入，边缘和感知损失的相对权重减少，这可能会妨碍对结构一致性和感知相似性的严格监督，并限制模型在SSIM和LPIPS上的表现。
- en: As is also evidently shown by just comparing the baselines finetuned with different
    prompt formats in Table [2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\.
    Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision"), using LLM to enhance the prompt construction as
    a sequence of important artifact attributes effectively improves the performance
    of the finetuned baseline model across all three metrics. More about the effects
    of our LLM-enhanced prompting method will be discussed in the following subsection [5.3](#S5.SS3
    "5.3\. Ablation Studies ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision").
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")中对不同提示格式微调的基线进行比较可以明显看出，使用LLM增强提示构建作为一系列重要伪影属性，有效地提高了微调基线模型在所有三个指标上的表现。有关我们LLM增强提示方法效果的更多讨论将在以下小节[5.3](#S5.SS3
    "5.3\. Ablation Studies ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision")中进行。
- en: 5.3\. Ablation Studies
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 消融研究
- en: To investigate the contribution of components proposed in our approach and for
    further studies, we conduct extensive ablation studies on two key designs of our
    model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究我们方法中提出的组件的贡献以及进一步的研究，我们对模型的两个关键设计进行了广泛的消融研究。
- en: Effectiveness of LLM enhanced prompt.
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强LLM的提示的有效性。
- en: As is shown in Table [2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\. Our
    Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision"), the finetuned model benefits from LLM-enhanced
    prompting, achieving better scores on all three quantitative metrics. To further
    illustrate the effectiveness of our proposed prompting method, we explore the
    zero-shot setting, where the baseline Taiyi-SD is directly prompted to generate
    artifact images without any training on our artifact dataset. We use either the
    raw description from the museum archives or the sequence of artifact attributes
    enhanced by LLM as prompt. The results, as shown in Table [3](#S4.T3 "Table 3
    ‣ 4.4\. Objective Functions ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image
    Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"), again demonstrate
    the superiority of LLM-enhanced prompting, which excels across all metrics. This
    can be credited to the organized information format in the attribute sequence
    and the additional knowledge provided by LLM (see Section [4.1](#S4.SS1 "4.1\.
    Prompt-Construction Enhanced by LLM ‣ 4\. Our Method ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")所示，经过微调的模型受益于LLM增强的提示，在所有三个定量指标上均取得了更好的成绩。为了进一步说明我们提出的提示方法的有效性，我们探索了零-shot
    设置，其中基线Taiyi-SD直接被提示生成文物图像，而没有对我们的文物数据集进行任何训练。我们使用来自博物馆档案的原始描述或LLM增强的文物属性序列作为提示。结果，如表[3](#S4.T3
    "Table 3 ‣ 4.4\. Objective Functions ‣ 4\. Our Method ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")所示，再次证明了LLM增强提示的优越性，其在所有指标上表现出色。这可以归功于属性序列中有序的信息格式和LLM提供的额外知识（见第[4.1](#S4.SS1
    "4.1\. Prompt-Construction Enhanced by LLM ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")节）。
- en: Effectiveness of Edge Loss and Perceptual Loss.
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 边缘损失和感知损失的有效性。
- en: In Figure [5](#S5.F5 "Figure 5 ‣ Evaluation Metrics. ‣ 5.1\. Experimental Setup
    ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision"), we compare the artifact images generated
    from our model that uses edge loss and perceptual loss against the finetuned Taiyi-SD
    baseline that does not involve these visual semantic constraints. Evidently, the
    shape, colors, and patterns of the artifacts are more accurate and close to the
    ground truth if the model is additionally supervised by edge and perceptual loss.
    On the other hand, the vanilla finetuning paradigm may easily lead to output objects
    either lacking proper shapes and forms or manifesting incorrect motifs and patterns.
    For example, in the second column of Figure [5](#S5.F5 "Figure 5 ‣ Evaluation
    Metrics. ‣ 5.1\. Experimental Setup ‣ 5\. Experiment ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"), the
    “tall-footed” aspect of the target bowl is clearly neglected without edge and
    perceptual constraints. Also, the intricate cracking lines on the “begonia-blossom
    shaped vase” shown in the third column are better simulated with our model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](#S5.F5 "Figure 5 ‣ Evaluation Metrics. ‣ 5.1\. Experimental Setup ‣ 5\.
    Experiment ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision")中，我们比较了使用边缘损失和感知损失的模型生成的文物图像与不涉及这些视觉语义约束的微调Taiyi-SD基线的图像。显然，如果模型额外受到边缘和感知损失的监督，文物的形状、颜色和图案会更准确、更接近真实情况。另一方面，普通的微调范式可能容易导致输出对象缺乏适当的形状和形式或表现出不正确的图案和样式。例如，在图[5](#S5.F5
    "Figure 5 ‣ Evaluation Metrics. ‣ 5.1\. Experimental Setup ‣ 5\. Experiment ‣
    Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision")的第二列中，目标碗的“高脚”方面在没有边缘和感知约束的情况下明显被忽略。此外，第三列中“秋海棠花瓶”上的复杂裂纹线条在我们的模型中得到了更好的模拟。
- en: 5.4\. User Study
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 用户研究
- en: 'In addition to quantitative evaluation, we conducted a user study involving
    archaeology experts to evaluate the generated images. This study is designed to
    assess various aspects of the generated artifacts, as outlined in our prompt design:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定量评估外，我们还进行了用户研究，涉及考古专家评估生成的图像。本研究旨在评估生成文物的各个方面，如我们提示设计中所述：
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Material: How accurately does the generated artifact resemble the actual manufacturing
    material?'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 材料：生成的文物与实际制造材料的相似度有多高？
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Shape: How closely does the generated artifact match the described shape?'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 形状：生成的文物与描述的形状相符的程度如何？
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pattern/Color: How faithful is the representation of patterns and colors on
    the generated artifact?'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图案/颜色：生成的工件在图案和颜色表现上的忠实度如何？
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Size/Ratio: How accurately does the generated artifact maintain the ratio of
    height and width?'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尺寸/比例：生成的工件在保持高度和宽度比例方面的准确性如何？
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dynasty: How well does the generated artifact reflect the characteristics of
    its era?'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 朝代：生成的工件在反映其时代特征方面的效果如何？
- en: Each aspect is rated on a scale of 0 to 5, with higher ratings indicating better
    quality. We randomly select 30 samples from the test set and provide the model-generated
    images to 20 graduate students of archaeology major from top institutions for
    assessment. The average ratings of images generated by our proposed method (OURS)
    are compared with those generated by the baseline Chinese SD model also finetuned
    on our data. The results are presented in Table [4](#S4.T4 "Table 4 ‣ 4.4\. Objective
    Functions ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision").
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 每个方面的评分范围是0到5，评分越高表示质量越好。我们从测试集中随机选择了30个样本，并将模型生成的图像提供给来自顶级机构的20名考古学研究生进行评估。我们提出的方法（OURS）生成的图像的平均评分与基线中文SD模型在我们数据上微调生成的图像进行比较。结果见表 [4](#S4.T4
    "表 4 ‣ 4.4\. 目标函数 ‣ 4\. 我们的方法 ‣ 知识感知工件图像合成与 LLM 增强提示和多源监督")。
- en: Clearly, according to human experts, the artifact images generated by our method
    are much better in quality across all five important rating aspects, especially
    in terms of shape, pattern, and color. These results of our user study resonate
    with the findings from the automatic evaluation metrics and further highlight
    the superior performance of our model in generating artifact images that accurately
    align with history.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，根据人类专家的意见，我们的方法生成的工件图像在所有五个重要评分方面的质量都更高，特别是在形状、图案和颜色方面。这些用户研究结果与自动评估指标的发现相一致，并进一步突显了我们模型在生成与历史准确对齐的工件图像方面的卓越性能。
- en: To offer a richer qualitative demonstration of our model’s capabilities, we
    present a diverse collection of artifact images generated by our model, showcasing
    its remarkable fidelity across a broad spectrum of historical artifacts. Refer
    to Figure [7](#A4.F7 "Figure 7 ‣ Learned Perceptual Image Patch Similarity (LPIPS).
    ‣ Appendix D More Details on Evaluation Metrics ‣ Knowledge-Aware Artifact Image
    Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision") in Appendix [E](#A5
    "Appendix E More Examples of Artifact Images Generated by Our Model ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")
    for a comprehensive visual display.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供我们模型能力的更丰富的定性展示，我们呈现了一组由我们模型生成的工件图像，这些图像展示了模型在广泛历史工件范围内的显著保真度。有关全面的视觉展示，请参阅附录[E](#A5
    "附录 E 更多我们模型生成的工件图像示例 ‣ 知识感知工件图像合成与 LLM 增强提示和多源监督")中的图 [7](#A4.F7 "图 7 ‣ 学习感知图像补丁相似性（LPIPS）。
    ‣ 附录 D 评估指标的更多细节 ‣ 知识感知工件图像合成与 LLM 增强提示和多源监督")。
- en: 6\. Conclusion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: 'In this paper, we present a novel approach to tackle the challenge of artifact
    image synthesis. Our method features three key techniques: 1). Leveraging an LLM
    to infuse textual prompts with archaeological knowledge, 2). Aligning textual
    representations with domain expertise via contrasting learning, and 3). Employing
    stricter visual-semantic constraints (edge and perceptual) to generate images
    with higher fidelity to visual details of historical artifacts. Quantitative experiments
    and our user study confirm the superior performance of our approach compared to
    existing models, significantly advancing the quality of generated artifact images.
    Beyond technological contributions, our work introduces a profound societal impact.
    As the first attempt to restore lost artifacts from the remaining descriptions,
    our work empowers archaeologists and historians with a tool to resurrect lost
    artifacts visually, offering new perspectives on cultural heritage and enriching
    our understanding of history. We also hope that this work will open new avenues
    for further exploration, fostering deeper insights into our past and cultural
    legacy with the help of technical advances.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的方法来应对文物图像合成的挑战。我们的方法包含三个关键技术：1). 利用大型语言模型（LLM）将文本提示与考古知识融合，2).
    通过对比学习将文本表示与领域知识对齐，以及 3). 采用更严格的视觉-语义约束（边缘和感知）生成与历史文物视觉细节高度一致的图像。定量实验和用户研究确认了我们的方法相比于现有模型的优越性能，显著提高了生成文物图像的质量。除了技术贡献外，我们的工作还带来了深远的社会影响。作为首次尝试从剩余描述中恢复丢失文物的工作，我们的研究为考古学家和历史学家提供了一种工具，通过视觉复原丢失文物，提供了文化遗产的新视角，丰富了我们对历史的理解。我们还希望这项工作能够为进一步探索开辟新途径，借助技术进步，促进对我们过去和文化遗产的更深入理解。
- en: Ethics Statement
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: In this project, we have used training data sourced from National Palace Museum
    (Taipei National Palace Museum, [2019](#bib.bib37)). This ensures that the data
    we’ve worked with has already been scrutinized by authorities and is open to the
    public. However, we recognize possible inaccuracies in our model’s generation
    despite our extensive efforts to improve its fidelity. Therefore, anyone using
    our model should be warned of possible mistakes in the generated artifact images
    and we strongly advise all users to verify important content.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本项目中，我们使用了来源于国立故宫博物院（台北国立故宫博物院，[2019](#bib.bib37)）的训练数据。这确保了我们所使用的数据已经过相关部门审查，并对公众开放。然而，尽管我们已经尽力提高模型的准确性，我们仍然认识到模型生成中可能存在的不准确性。因此，任何使用我们模型的人都应警惕生成的文物图像可能存在的错误，我们强烈建议所有用户对重要内容进行核实。
- en: References
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell、Sandhini
    Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon Child、Aditya Ramesh、Daniel
    M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark Chen、Eric Sigler、Mateusz
    Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher Berner、Sam McCandlish、Alec
    Radford、Ilya Sutskever 和 Dario Amodei。2020年。《语言模型是少样本学习者》。arXiv:2005.14165 [cs.CL]
- en: Canny (1986) John Canny. 1986. A Computational Approach to Edge Detection. *IEEE
    Transactions on Pattern Analysis and Machine Intelligence* PAMI-8, 6 (1986), 679–698.
    [https://doi.org/10.1109/TPAMI.1986.4767851](https://doi.org/10.1109/TPAMI.1986.4767851)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Canny（1986）John Canny。1986年。《一种计算边缘检测的方法》。*IEEE 图像处理与机器智能学报* PAMI-8，6（1986），679–698。
    [https://doi.org/10.1109/TPAMI.1986.4767851](https://doi.org/10.1109/TPAMI.1986.4767851)
- en: 'Chang et al. (2022) Xiang Chang, Fei Chao, Changjing Shang, and Qiang Shen.
    2022. Sundial-GAN: A Cascade Generative Adversarial Networks Framework for Deciphering
    Oracle Bone Inscriptions. In *Proceedings of the 30th ACM International Conference
    on Multimedia*. ACM. [https://doi.org/10.1145/3503161.3547925](https://doi.org/10.1145/3503161.3547925)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人（2022）Xiang Chang、Fei Chao、Changjing Shang 和 Qiang Shen。2022年。《Sundial-GAN：一种用于解读甲骨文的级联生成对抗网络框架》。见于
    *第30届 ACM 国际多媒体会议论文集*。ACM。 [https://doi.org/10.1145/3503161.3547925](https://doi.org/10.1145/3503161.3547925)
- en: Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion
    Models Beat GANs on Image Synthesis. In *Advances in Neural Information Processing
    Systems*, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan
    (Eds.), Vol. 34\. Curran Associates, Inc., 8780–8794. [https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhariwal 和 Nichol（2021）普拉弗拉·达里瓦尔和亚历山大·尼科尔。2021年。扩散模型在图像合成中击败了GAN。在*神经信息处理系统进展*中，M.
    Ranzato、A. Beygelzimer、Y. Dauphin、P.S. Liang 和 J. Wortman Vaughan（编），第34卷。Curran
    Associates, Inc.，8780–8794。 [https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf)
- en: 'Gal et al. (2022) Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H
    Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing
    text-to-image generation using textual inversion. *arXiv preprint arXiv:2208.01618*
    (2022).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal 等人（2022）里农·加尔、尤瓦尔·阿拉鲁夫、尤瓦尔·阿茨蒙、奥尔·帕塔什尼克、阿米特·H·伯马诺、加尔·切奇克和丹尼尔·科恩-奥尔。2022年。一张图片值一个词：使用文本反演个性化文本到图像生成。*arXiv预印本arXiv:2208.01618*（2022年）。
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative Adversarial Nets. In *Advances in Neural Information Processing Systems*,
    Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.),
    Vol. 27\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人（2014）伊恩·古德费洛、让·普盖-阿巴迪、梅赫迪·米尔扎、宾·徐、大卫·沃德-法利、谢尔吉尔·奥扎尔、亚伦·库尔维尔和约书亚·本吉奥。2014年。生成对抗网络。在*神经信息处理系统进展*中，Z.
    Ghahramani、M. Welling、C. Cortes、N. Lawrence 和 K.Q. Weinberger（编），第27卷。Curran Associates,
    Inc. [https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
- en: Gu et al. (2022) Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong
    Chen, Lu Yuan, and Baining Guo. 2022. Vector Quantized Diffusion Model for Text-to-Image
    Synthesis. arXiv:2111.14822 [cs.CV]
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2022）舒扬·顾、董晨、建敏·包、方文、博·张、董东·陈、卢元和白宁·郭。2022年。用于文本到图像合成的向量量化扩散模型。arXiv:2111.14822
    [cs.CV]
- en: Hang et al. (2023) Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen,
    Han Hu, Xin Geng, and Baining Guo. 2023. Efficient Diffusion Training via Min-SNR
    Weighting Strategy. (March 2023). arXiv:2303.09556 [cs.CV]
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hang 等人（2023）天凯·杭、舒扬·顾、陈力、建敏·包、董晨、韩虎、辛·耿和白宁·郭。2023年。通过最小信噪比加权策略的高效扩散训练。（2023年3月）。arXiv:2303.09556
    [cs.CV]
- en: He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.
    Deep residual learning for image recognition. (Dec. 2015). arXiv:1512.03385 [cs.CV]
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2015）凯明·赫、向宇·张、邵青·任和建·孙。2015年。用于图像识别的深度残差学习。（2015年12月）。arXiv:1512.03385
    [cs.CV]
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning. In *Proceedings of the 2021 Conference on Empirical Methods in Natural
    Language Processing*. Association for Computational Linguistics, Online and Punta
    Cana, Dominican Republic, 7514–7528. [https://doi.org/10.18653/v1/2021.emnlp-main.595](https://doi.org/10.18653/v1/2021.emnlp-main.595)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hessel 等人（2021）杰克·赫塞尔、阿里·霍尔茨曼、马克斯韦尔·福布斯、罗南·勒布拉斯和叶金·崔。2021年。CLIPScore：一种无参考图像描述评估指标。在*2021年自然语言处理实证方法会议论文集*中。计算语言学协会，在线和多米尼加共和国蓬塔卡纳，7514–7528。
    [https://doi.org/10.18653/v1/2021.emnlp-main.595](https://doi.org/10.18653/v1/2021.emnlp-main.595)
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    Diffusion Probabilistic Models. (June 2020). arXiv:2006.11239 [cs.LG] [https://arxiv.org/pdf/2006.11239.pdf](https://arxiv.org/pdf/2006.11239.pdf)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等人（2020）乔纳森·霍、阿贾伊·简和皮特·阿贝尔。2020年。去噪扩散概率模型。（2020年6月）。arXiv:2006.11239 [cs.LG]
    [https://arxiv.org/pdf/2006.11239.pdf](https://arxiv.org/pdf/2006.11239.pdf)
- en: Johnson et al. (2016) Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.
    Perceptual losses for real-time style transfer and super-resolution. (March 2016).
    arXiv:1603.08155 [cs.CV]
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等人（2016）贾斯汀·约翰逊、亚历山大·阿拉希和李飞飞。2016年。用于实时风格转换和超分辨率的感知损失。（2016年3月）。arXiv:1603.08155
    [cs.CV]
- en: Kang et al. (2023) Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli
    Shechtman, Sylvain Paris, and Taesung Park. 2023. Scaling up GANs for text-to-image
    synthesis. (March 2023). arXiv:2303.05511 [cs.CV]
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等 (2023) Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman,
    Sylvain Paris, 和 Taesung Park. 2023. 扩大 GANs 用于文本到图像合成. (2023年3月). arXiv:2303.05511
    [cs.CV]
- en: 'Kingma and Ba (2014) Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method
    for Stochastic Optimization. arXiv:arXiv:1412.6980'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma 和 Ba (2014) Diederik P. Kingma 和 Jimmy Ba. 2014. Adam: 一种用于随机优化的方法.
    arXiv:arXiv:1412.6980'
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-Encoding
    Variational Bayes. (Dec. 2013). arXiv:1312.6114 [stat.ML]
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Welling (2013) Diederik P Kingma 和 Max Welling. 2013. Auto-Encoding
    Variational Bayes. (2013年12月). arXiv:1312.6114 [stat.ML]
- en: Kumari et al. (2022) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman,
    and Jun-Yan Zhu. 2022. Multi-Concept Customization of Text-to-Image Diffusion.
    *arXiv preprint arXiv:2212.04488* (2022).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumari 等 (2022) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman,
    和 Jun-Yan Zhu. 2022. 文本到图像扩散的多概念定制. *arXiv 预印本 arXiv:2212.04488* (2022).
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,
    Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric
    Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue
    Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,
    Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian
    Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori
    Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen
    Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic Evaluation of Language
    Models. (Nov. 2022). arXiv:2211.09110 [cs.CL]
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等 (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
    Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher
    D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin
    Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam,
    Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha,
    Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael
    Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi
    Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, 和
    Yuta Koreeda. 2022. 语言模型的整体评估. (2022年11月). arXiv:2211.09110 [cs.CL]
- en: 'Liu et al. (2022) Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West,
    Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022. Generated Knowledge
    Prompting for Commonsense Reasoning. In *Proceedings of the 60th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*. Association
    for Computational Linguistics, Dublin, Ireland, 3154–3169. [https://doi.org/10.18653/v1/2022.acl-long.225](https://doi.org/10.18653/v1/2022.acl-long.225)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2022) Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West,
    Ronan Le Bras, Yejin Choi, 和 Hannaneh Hajishirzi. 2022. 用于常识推理的生成知识提示. 在 *第60届计算语言学协会年会论文集
    (第1卷: 长篇论文)* 中. 计算语言学协会, 爱尔兰都柏林, 3154–3169. [https://doi.org/10.18653/v1/2022.acl-long.225](https://doi.org/10.18653/v1/2022.acl-long.225)'
- en: Nichol and Dhariwal (2021) Alex Nichol and Prafulla Dhariwal. 2021. Improved
    Denoising Diffusion Probabilistic Models. arXiv:2102.09672 [cs.LG]
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol 和 Dhariwal (2021) Alex Nichol 和 Prafulla Dhariwal. 2021. 改进的去噪扩散概率模型.
    arXiv:2102.09672 [cs.LG]
- en: 'Nichol et al. (2022) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
    Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE:
    Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion
    Models. arXiv:2112.10741 [cs.CV]'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nichol 等 (2022) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam,
    Pamela Mishkin, Bob McGrew, Ilya Sutskever, 和 Mark Chen. 2022. GLIDE: 通过文本引导扩散模型实现逼真的图像生成与编辑.
    arXiv:2112.10741 [cs.CV]'
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language Models
    as Knowledge Bases?. In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP)*. Association for Computational Linguistics,
    Hong Kong, China, 2463–2473. [https://doi.org/10.18653/v1/D19-1250](https://doi.org/10.18653/v1/D19-1250)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni 等 (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis,
    Anton Bakhtin, Yuxiang Wu, 和 Alexander Miller. 2019. 语言模型作为知识库?. 在 *2019年自然语言处理实证方法会议暨第九届国际联合自然语言处理会议
    (EMNLP-IJCNLP) 论文集* 中. 计算语言学协会, 中国香港, 2463–2473. [https://doi.org/10.18653/v1/D19-1250](https://doi.org/10.18653/v1/D19-1250)
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual
    models from natural language supervision. (Feb. 2021). arXiv:2103.00020 [cs.CV]
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger 和 Ilya Sutskever. 2021. 从自然语言监督中学习可转移的视觉模型。(2021年2月)。arXiv:2103.00020
    [cs.CV]
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. 2022. Hierarchical Text-Conditional Image Generation with CLIP
    Latents. arXiv:2204.06125 [cs.CV]
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh 等 (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu 和 Mark
    Chen. 2022. 基于 CLIP 潜变量的分层文本条件图像生成。arXiv:2204.06125 [cs.CV]
- en: Rombach et al. (2021) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2021. High-resolution image synthesis with latent diffusion
    models. (Dec. 2021). arXiv:2112.10752 [cs.CV]
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach 等 (2021) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser
    和 Björn Ommer. 2021. 使用潜在扩散模型进行高分辨率图像合成。(2021年12月)。arXiv:2112.10752 [cs.CV]
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion
    Models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*. 10684–10695.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach 等 (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser
    和 Björn Ommer. 2022. 基于潜在扩散模型的高分辨率图像合成。发表于 *IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集*。10684–10695。
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. (May 2015).
    arXiv:1505.04597 [cs.CV]'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronneberger 等 (2015) Olaf Ronneberger, Philipp Fischer 和 Thomas Brox. 2015.
    U-Net：用于生物医学图像分割的卷积网络。(2015年5月)。arXiv:1505.04597 [cs.CV]
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara
    Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad
    Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language
    Understanding. arXiv:2205.11487 [cs.CV]
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saharia 等 (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay
    Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara
    Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet 和 Mohammad
    Norouzi. 2022. 具有深度语言理解的逼真文本到图像扩散模型。arXiv:2205.11487 [cs.CV]
- en: Seif and Androutsos (2018) George Seif and Dimitrios Androutsos. 2018. Edge-Based
    Loss Function for Single Image Super-Resolution. In *2018 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*. 1468–1472. [https://doi.org/10.1109/ICASSP.2018.8461664](https://doi.org/10.1109/ICASSP.2018.8461664)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seif 和 Androutsos (2018) George Seif 和 Dimitrios Androutsos. 2018. 基于边缘的单幅图像超分辨率损失函数。发表于
    *2018 IEEE 国际声学、语音与信号处理会议 (ICASSP)*。1468–1472。 [https://doi.org/10.1109/ICASSP.2018.8461664](https://doi.org/10.1109/ICASSP.2018.8461664)
- en: Sheng and Moens (2019) Shurong Sheng and Marie-Francine Moens. 2019. Generating
    Captions for Images of Ancient Artworks. In *Proceedings of the 27th ACM International
    Conference on Multimedia*. ACM. [https://doi.org/10.1145/3343031.3350972](https://doi.org/10.1145/3343031.3350972)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 和 Moens (2019) Shurong Sheng 和 Marie-Francine Moens. 2019. 为古代艺术品图像生成描述。发表于
    *第27届 ACM 国际多媒体会议论文集*。ACM。 [https://doi.org/10.1145/3343031.3350972](https://doi.org/10.1145/3343031.3350972)
- en: 'Shi et al. (2022) Daqian Shi, Xiaolei Diao, Lida Shi, Hao Tang, Yang Chi, Chuntao
    Li, and Hao Xu. 2022. CharFormer: A Glyph Fusion based Attentive Framework for
    High-precision Character Image Denoising. (2022). [https://doi.org/10.1145/3503161.3548208](https://doi.org/10.1145/3503161.3548208)
    arXiv:arXiv:2207.07798'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等 (2022) Daqian Shi, Xiaolei Diao, Lida Shi, Hao Tang, Yang Chi, Chuntao
    Li 和 Hao Xu. 2022. CharFormer：一种基于字形融合的高精度字符图像去噪框架。(2022)。 [https://doi.org/10.1145/3503161.3548208](https://doi.org/10.1145/3503161.3548208)
    arXiv:arXiv:2207.07798
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
    and Surya Ganguli. 2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics.
    arXiv:arXiv:1503.03585
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein 等 (2015) Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan
    和 Surya Ganguli. 2015. 使用非平衡热力学的深度无监督学习。arXiv:arXiv:1503.03585
- en: Song et al. (2022) Jiaming Song, Chenlin Meng, and Stefano Ermon. 2022. Denoising
    Diffusion Implicit Models. arXiv:2010.02502 [cs.LG]
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 (2022) Jiaming Song, Chenlin Meng 和 Stefano Ermon. 2022. 去噪扩散隐式模型。arXiv:2010.02502
    [cs.LG]
- en: Song and Ermon (2020) Yang Song and Stefano Ermon. 2020. Generative Modeling
    by Estimating Gradients of the Data Distribution. arXiv:1907.05600 [cs.LG]
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 和 Ermon (2020) Yang Song 和 Stefano Ermon. 2020. 通过估计数据分布的梯度进行生成建模。arXiv:1907.05600
    [cs.LG]
- en: Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. 2020. Score-Based Generative Modeling through
    Stochastic Differential Equations. arXiv:arXiv:2011.13456
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宋等（2020）宋洋、贾斯查·索尔-迪克斯坦、迪德里克·P·金马、阿比谢克·库马尔、斯特凡诺·厄尔蒙、贝恩·普尔。2020。通过随机微分方程的基于得分的生成建模。arXiv:arXiv:2011.13456
- en: Song et al. (2021) Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling through
    Stochastic Differential Equations. arXiv:2011.13456 [cs.LG]
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宋等（2021）宋洋、贾斯查·索尔-迪克斯坦、迪德里克·P·金马、阿比谢克·库马尔、斯特凡诺·厄尔蒙、贝恩·普尔。2021。通过随机微分方程的基于得分的生成建模。arXiv:2011.13456
    [cs.LG]
- en: Taipei National Palace Museum (2019) Taipei National Palace Museum. 2019. National
    Palace Museum Open Data. data retrieved from National Palace Museum Open Data,
    [https://theme.npm.edu.tw/opendata/index.aspx?lang=2](https://theme.npm.edu.tw/opendata/index.aspx?lang=2).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 台北故宫博物院（2019）台北故宫博物院。2019。故宫博物院开放数据。从故宫博物院开放数据中检索数据，[https://theme.npm.edu.tw/opendata/index.aspx?lang=2](https://theme.npm.edu.tw/opendata/index.aspx?lang=2)。
- en: van den Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
    2018. Representation learning with Contrastive Predictive Coding. (July 2018).
    arXiv:1807.03748 [cs.LG]
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凡登·奥德等（2018）亚伦·凡登·奥德、李雅哲、奥里奥尔·维尼亚尔斯。2018。通过对比预测编码进行表示学习。（2018年7月）。arXiv:1807.03748
    [cs.LG]
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. (June 2017). arXiv:1706.03762 [cs.CL]
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瓦斯瓦尼等（2017）阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔玛尔、雅各布·乌斯科雷特、利昂·琼斯、艾丹·N·戈麦斯、卢卡斯·凯泽、伊利亚·波洛苏金。2017。注意力机制是你所需要的一切。（2017年6月）。arXiv:1706.03762
    [cs.CL]
- en: 'Wang et al. (2021) Cunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can Generative
    Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*. Association for Computational Linguistics, Online, 3241–3251.
    [https://doi.org/10.18653/v1/2021.acl-long.251](https://doi.org/10.18653/v1/2021.acl-long.251)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2021）王存翔、刘派、张跃。2021。生成预训练语言模型能否作为封闭式问答的知识库？在*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长论文）*中。计算语言学协会，在线，3241–3251。
    [https://doi.org/10.18653/v1/2021.acl-long.251](https://doi.org/10.18653/v1/2021.acl-long.251)
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct:
    Aligning Language Model with Self Generated Instructions. arXiv:2212.10560 [cs.CL]'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2022）王艺中、耶加内赫·科尔迪、斯瓦罗普·米什拉、艾莉莎·刘、诺亚·A·史密斯、丹尼尔·哈沙比、哈娜赫·哈吉什尔齐。2022。Self-Instruct：与自生成指令对齐的语言模型。arXiv:2212.10560
    [cs.CL]
- en: 'Wang et al. (2004) Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.
    2004. Image quality assessment: from error visibility to structural similarity.
    *IEEE Transactions on Image Processing* 13, 4 (2004), 600–612. [https://doi.org/10.1109/TIP.2003.819861](https://doi.org/10.1109/TIP.2003.819861)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2004）王舟、A.C.博维克、H.R.谢赫、E.P.西蒙切利。2004。图像质量评估：从误差可见性到结构相似性。*IEEE图像处理汇刊* 13,
    4（2004），600–612。 [https://doi.org/10.1109/TIP.2003.819861](https://doi.org/10.1109/TIP.2003.819861)
- en: 'Wei et al. (2023) Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang,
    and Wangmeng Zuo. 2023. Elite: Encoding visual concepts into textual embeddings
    for customized text-to-image generation. *arXiv preprint arXiv:2302.13848* (2023).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2023）魏宇翔、张亚博、纪志龙、白金锋、张磊、左旺萌。2023。精英：将视觉概念编码为文本嵌入用于定制的图像生成。*arXiv预印本 arXiv:2302.13848*（2023年）。
- en: 'Xu et al. (2017) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, and Xiaodong He. 2017. AttnGAN: Fine-grained text to image generation
    with attentional generative adversarial networks. (Nov. 2017). arXiv:1711.10485 [cs.CV]'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等（2017）徐涛、张鹏川、黄秋远、张汉、甘哲、黄小磊、贺晓东。2017。AttnGAN：通过注意力生成对抗网络进行细粒度文本到图像生成。（2017年11月）。arXiv:1711.10485
    [cs.CV]
- en: 'Yang et al. (2022) Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng
    Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2022. Diffusion models:
    A comprehensive survey of methods and applications. (Sept. 2022). arXiv:2209.00796 [cs.LG]'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2022）杨灵、张志龙、宋洋、洪申达、徐润生、赵越、张文涛、崔斌、杨名轩。2022。扩散模型：方法与应用的全面综述。（2022年9月）。arXiv:2209.00796
    [cs.LG]
- en: 'Zhang et al. (2016) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris Metaxas. 2016. StackGAN: Text to photo-realistic
    image synthesis with Stacked Generative Adversarial Networks. (Dec. 2016). arXiv:1612.03242 [cs.CV]'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2016）汉张，涛徐，洪生李，少廷张，小刚王，小磊黄，和迪米特里斯·梅塔克萨斯。2016。StackGAN：利用堆叠生成对抗网络进行文本到照片级图像合成。（2016年12月）。arXiv:1612.03242
    [cs.CV]
- en: 'Zhang et al. (2022a) Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin
    Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo,
    Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen,
    Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, and Chongpei
    Chen. 2022a. Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence.
    arXiv:arXiv:2209.02970'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2022a）贾兴张，瑞宇甘，俊杰王，玉翔张，林张，平杨，新宇高，子伟吴，小群董，俊青何，建恒卓，琪杨，永峰黄，霞宇李，杨汉吴，君宇陆，新宇朱，伟峰陈，婷婷韩，坤浩潘，瑞王，浩王，小军吴，钟申曾，冲佩陈。2022a。风神榜1.0：作为中国认知智能的基础。arXiv:arXiv:2209.02970
- en: 'Zhang et al. (2022b) Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin
    Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo,
    Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen,
    Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, and Chongpei
    Chen. 2022b. Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence.
    *CoRR* abs/2209.02970 (2022).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2022b）贾兴张，瑞宇甘，俊杰王，玉翔张，林张，平杨，新宇高，子伟吴，小群董，俊青何，建恒卓，琪杨，永峰黄，霞宇李，杨汉吴，君宇陆，新宇朱，伟峰陈，婷婷韩，坤浩潘，瑞王，浩王，小军吴，钟申曾，冲佩陈。2022b。风神榜1.0：作为中国认知智能的基础。*CoRR*
    abs/2209.02970（2022年）。
- en: Zhang et al. (2018) Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,
    and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as a Perceptual
    Metric. arXiv:arXiv:1801.03924
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2018）理查德张，菲利普·伊索拉，亚历克谢·A·埃夫罗斯，伊利·谢赫特曼，和奥利弗·王。2018。深度特征作为感知度量的非凡有效性。arXiv:arXiv:1801.03924
- en: '| Expert Attribute | Example |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 专家属性 | 示例 |'
- en: '| --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Name | Yuhuchun vase in cobalt blue glaze |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 钴蓝釉玉壶春瓶 |'
- en: '| Material | Porcelain |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 材料 | 瓷 |'
- en: '| Time Period | Qing Dynasty, Yongzheng reign, 1723-1735 AD |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 时间段 | 清朝，雍正年间，1723-1735年 |'
- en: '| Type | Yuhuchun vase |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 玉壶春瓶 |'
- en: '| Type Definition | Also known as ”narrow-necked vase,” yuhuchun vase is a
    practical commemorative ceramic widely popular in the northern regions. The vase
    consists of five parts: neck, shoulders, body, foot, and mouth. The neck is long
    and slender, the body is plump, and the foot can be a short circular footring
    or a horseshoe-shaped foot. Yuhuchun vases are created using various clay recipes
    and glaze techniques, resulting in distinct colors and surface effects for each
    piece |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 类型定义 | 也称为“细颈瓶”，玉壶春瓶是一种在北方地区广受欢迎的实用纪念陶瓷。瓶子由颈部、肩部、瓶身、足部和口部五部分组成。颈部细长，瓶身丰满，底部可以是短圆形足环或马蹄形足。玉壶春瓶采用各种泥料配方和釉料技术制作，每件作品呈现出独特的颜色和表面效果
    |'
- en: '| Shape | Flared mouth, slender neck, sloping shoulders, pear-shaped ample
    body, and a circular footring |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 形状 | 喇叭口，细长颈部，倾斜肩部，梨形丰满瓶身，圆形足环 |'
- en: '| Pattern | The body of the vase is adorned with a cobalt blue glaze, which
    shines with a bright indigo color. The interior and the base of the vessel are
    covered in white glaze. The footring reveals the white body of the vase |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 图案 | 瓶身装饰有钴蓝釉，光泽明亮的靛蓝色。容器的内部和底部覆盖白色釉。足环显露出瓶子的白色胎体 |'
- en: '| Size | Height of 30.3 cm, mouth diameter of 8.5 cm, base diameter of 12.0
    cm |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 尺寸 | 高度30.3厘米，口径8.5厘米，底径12.0厘米 |'
- en: '| Enhanced Prompt Example | Yuhuchun vase in cobalt blue glaze [SEP] Porcelain
    [SEP] Qing Dynasty, Yongzheng reign, 1723-1735 AD [SEP] Yuhuchun vase [SEP] Also
    known as ”narrow-necked vase,” yuhuchun vase is a practical commemorative ceramic
    widely popular in the northern regions. The vase consists of five parts: neck,
    shoulders, body, foot, and mouth. The neck is long and slender, the body is plump,
    and the foot can be a short circular footring or a horseshoe-shaped foot. Yuhuchun
    vases are created using various clay recipes and glaze techniques, resulting in
    distinct colors and surface effects for each piece [SEP] Flared mouth, slender
    neck, sloping shoulders, pear-shaped ample body, and a circular footring [SEP]
    The body of the vase is adorned with a cobalt blue glaze, which shines with a
    bright indigo color. The interior and the base of the vessel are covered in white
    glaze. The footring reveals the white body of the vase [SEP] Height of 30.3 cm,
    mouth diameter of 8.5 cm, base diameter of 12.0 cm |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 增强提示示例 | 钴蓝釉御宇春瓶 [SEP] 瓷器 [SEP] 清朝，雍正年间，1723-1735年 [SEP] 御宇春瓶 [SEP] 也称为“细颈瓶”，御宇春瓶是一种在北方地区广泛流行的实用纪念陶瓷。瓶子由五部分组成：颈部、肩部、主体、足部和口部。颈部细长而纤细，主体丰满，足部可为短圆形足圈或马蹄形足。御宇春瓶使用各种泥料配方和釉料技术制作，每件瓶子都有独特的颜色和表面效果
    [SEP] 外翻口，细长颈部，斜肩，梨形丰满的主体和圆形足圈 [SEP] 瓶体装饰有钴蓝釉，呈现出明亮的靛蓝色。瓶的内部和底部覆盖着白色釉料。足圈显示了瓶子的白色胎体
    [SEP] 高30.3厘米，口径8.5厘米，底径12.0厘米 |'
- en: Table 5\. An example of the proposed expert attributes of artifacts. The last
    row forms the LLM-enhanced prompt input to our text-to-image model. The special
    delimiter [SEP] connecting attributes is implemented as a Chinese comma.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 提议的文物专家属性示例。最后一行形成了对我们的文本到图像模型的LLM增强提示输入。连接属性的特殊分隔符[SEP]实现为中文逗号。
- en: Appendix A Example of the Enhanced Artifact Attributes
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 增强版文物属性示例
- en: As a supplement to Section [4.1](#S4.SS1 "4.1\. Prompt-Construction Enhanced
    by LLM ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision") and Table [1](#S4.T1 "Table 1 ‣ 4.1\.
    Prompt-Construction Enhanced by LLM ‣ 4\. Our Method ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision") in
    the main text, we provide a concrete example of the proposed expert attributes
    of the historical artifact “yuhuchun vase in cobalt blue glaze” in Table [5](#A0.T5
    "Table 5 ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision") of this appendix²²2The numbering of tables, figures
    and equations in this appendix follows that of the main text. So the number does
    not start fresh from 1 in this appendix.. The last row is the arranged sequence
    of the artifact attributes separated by [SEP] (implemented as a Chinese comma),
    which forms the LLM-enhanced prompt input to our text-to-image models.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对主文中第[4.1节](#S4.SS1 "4.1\. Prompt-Construction Enhanced by LLM ‣ 4\. Our Method
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision")和表[1](#S4.T1 "Table 1 ‣ 4.1\. Prompt-Construction Enhanced by LLM
    ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision")的补充，我们在本附录的表[5](#A0.T5 "Table 5 ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")中提供了“钴蓝釉御宇春瓶”这一历史文物的提议专家属性的具体示例²²2本附录中的表格、图示和公式编号与主文一致，因此编号不会从1重新开始。最后一行是将文物属性按照[SEP]（用作中文逗号）分隔的排列序列，形成了对我们的文本到图像模型的LLM增强提示输入。
- en: Appendix B Example of the Prompt Template for Querying GPT-3.5
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B GPT-3.5 查询提示模板示例
- en: 'As specified in Section [4.1](#S4.SS1 "4.1\. Prompt-Construction Enhanced by
    LLM ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision") in the main text, we use GPT-3.5-TURBO
    as our knowledge-base LLM. The prompt template for querying GPT-3.5 is designed
    with a similar format following self-instruct (Wang et al., [2022](#bib.bib41)).
    It consists of three parts: 1). A task statement that describes to GPT-3.5 the
    task to be done; 2). Two in-context examples sampled from our labeled pool of
    54 artifacts written by archaeology experts; 3). The target artifacts whose “material”,
    “shape”, “pattern”, “type” and “type definition” are left blank and need to be
    answered by GPT-3.5\. In between the parts and the different artifact samples,
    we use “###” as a separator. Figure [6](#A4.F6 "Figure 6 ‣ Learned Perceptual
    Image Patch Similarity (LPIPS). ‣ Appendix D More Details on Evaluation Metrics
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision") shows an example of our prompt for querying information about the
    artifact “snuff bottle with intertwined floral decoration in fencai polychrome
    enamels on a yellow ground”.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如主文本第[4.1](#S4.SS1 "4.1\. Prompt-Construction Enhanced by LLM ‣ 4\. Our Method
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision")节中所述，我们使用 GPT-3.5-TURBO 作为我们的知识库 LLM。查询 GPT-3.5 的提示模板采用类似自我指导（Wang
    et al., [2022](#bib.bib41)）的格式设计。它由三部分组成：1）描述任务给 GPT-3.5 的任务声明；2）从我们由考古学专家编写的
    54 件文物标注池中抽取的两个上下文示例；3）目标文物，其“材料”、“形状”、“图案”、“类型”和“类型定义”留空，需要由 GPT-3.5 填写。在各部分和不同文物示例之间，我们使用“###”作为分隔符。图[6](#A4.F6
    "Figure 6 ‣ Learned Perceptual Image Patch Similarity (LPIPS). ‣ Appendix D More
    Details on Evaluation Metrics ‣ Knowledge-Aware Artifact Image Synthesis with
    LLM-Enhanced Prompting and Multi-Source Supervision")展示了我们用于查询“花卉装饰的瓷瓶”的文物信息的提示示例。
- en: Appendix C More on Implementation Details
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 更多实施细节
- en: 'Here we provide additional details with regard to the implementation of our
    method as a supplement to the model details specified in Section [5.1](#S5.SS1
    "5.1\. Experimental Setup ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision") of the main text: To
    get the canny edge map of images for the edge loss computation, we implement a
    canny filter (Canny, [1986](#bib.bib3)) with a Gaussian kernel of size 3, Sobel
    filter kernel size of 3, a low threshold on pixel intensity of 0.15. We compute
    perceptual loss (Johnson et al., [2016](#bib.bib13)) using visual features extracted
    by the image encoder of CLIP-ViT-L/14 (Radford et al., [2021](#bib.bib23)). The
    weights of additional losses used in our model training objective defined in (5)
    of the main text are $\lambda_{1}=$ for all our experiments, which run on dual
    NVIDIA A100 GPUs.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了有关我们方法实施的更多细节，以补充主文本第[5.1](#S5.SS1 "5.1\. Experimental Setup ‣ 5\.
    Experiment ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision")节中指定的模型细节：为了获得图像的 Canny 边缘图以计算边缘损失，我们实现了一个 Canny
    滤波器（Canny，[1986](#bib.bib3)），使用了尺寸为 3 的高斯核，Sobel 滤波器的核大小为 3，像素强度的低阈值为 0.15。我们使用
    CLIP-ViT-L/14（Radford et al., [2021](#bib.bib23)）的图像编码器提取的视觉特征来计算感知损失（Johnson
    et al., [2016](#bib.bib13)）。在主文本（5）节中定义的我们模型训练目标中使用的附加损失的权重为所有实验中的$\lambda_{1}=$，这些实验在双
    NVIDIA A100 GPU 上运行。
- en: Appendix D More Details on Evaluation Metrics
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 更多评估指标细节
- en: In this section, we offer a more extensive explanation of the automatic metrics
    (CLIP Visual Similarity, Structural Similarity Index (SSIM) (Wang et al., [2004](#bib.bib42))
    and Learned Perceptual Image Patch Similarity (LPIPS)) employed to quantitatively
    evaluate the performance of artifact image generation models. This serves to provide
    additional technical details to the evaluation metrics stated in Section [5.1](#S5.SS1
    "5.1\. Experimental Setup ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision") in the main text.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了有关自动度量（CLIP 视觉相似性、结构相似性指数 (SSIM)（Wang et al., [2004](#bib.bib42)）和学习感知图像补丁相似性
    (LPIPS)）的更详细解释，用于定量评估文物图像生成模型的性能。这旨在提供主文本第[5.1](#S5.SS1 "5.1\. Experimental Setup
    ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision")节中列出的评估指标的额外技术细节。
- en: CLIP Visual Similarity.
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CLIP 视觉相似性。
- en: Inspired by the CLIP Score (Hessel et al., [2021](#bib.bib11)), which is widely
    employed to assess the similarity between a text-image pair, we compute the visual
    similarity of two images (i.e., the ground-truth image and the generated image)
    with the visual module of a pre-trained CLIP model (specifically, CLIP-ViT-L/14)
    (Radford et al., [2021](#bib.bib23)). We refer to this metric as Clip Visual Similarity,
    which is also utilized in (Gal et al., [2022](#bib.bib6); Kumari et al., [2022](#bib.bib17);
    Wei et al., [2023](#bib.bib43)). Since the CLIP model has shown a strong ability
    in capturing the overall image contents and mapping those into a feature space,
    a higher similarity of the encoded visual features suggests a generally closer
    resemblance of the generated image to the ground truth.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 受 CLIP 分数（Hessel 等人，[2021](#bib.bib11)）的启发，该分数广泛用于评估文本-图像对之间的相似性，我们使用预训练 CLIP
    模型（特别是 CLIP-ViT-L/14）的视觉模块来计算两个图像（即真实图像和生成图像）的视觉相似性（Radford 等人，[2021](#bib.bib23)）。我们将这一度量称为
    Clip Visual Similarity，这在 (Gal 等人，[2022](#bib.bib6); Kumari 等人，[2022](#bib.bib17);
    Wei 等人，[2023](#bib.bib43)) 中也有使用。由于 CLIP 模型在捕捉整体图像内容并将其映射到特征空间方面表现出强大的能力，因此编码视觉特征的相似性较高意味着生成的图像与真实图像的相似度较高。
- en: Structural Similarity Index (SSIM).
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结构相似性指数 (SSIM)。
- en: SSIM (Wang et al., [2004](#bib.bib42)) is designed to primarily measure the
    similarity in terms of structural components between two images. It evaluates
    how well the synthesized output preserves the structural details present in the
    ground truth images. This includes important edges, boundaries, and overall structural
    coherence. By quantifying the degree of structural resemblance, SSIM provides
    valuable insights into the accuracy of artifact image synthesis in terms of preserving
    crucial details related to the formative appearance of artifacts, e.g., their
    shape, and patterns.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: SSIM (Wang 等人，[2004](#bib.bib42)) 主要用于测量两幅图像在结构组件方面的相似性。它评估合成输出保留真实图像中结构细节的程度。这包括重要的边缘、边界和整体结构连贯性。通过量化结构相似度，SSIM
    提供了有关伪像图像合成在保留与伪像的形成外观相关的关键细节（例如形状和模式）方面的准确性的宝贵见解。
- en: Learned Perceptual Image Patch Similarity (LPIPS).
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习的感知图像块相似性 (LPIPS)。
- en: LPIPS (Zhang et al., [2018](#bib.bib49)) judges the perceptual similarities
    between two images. In artifact image synthesis tasks, it is indispensable to
    assess not only the structural similarity but also the perceptual quality of the
    synthesized images. LPIPS is designed to align with human perception of image
    quality which also corresponds to the goal of artifact image synthesis tasks of
    generating historical objects that are visually convincing to human experts.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: LPIPS (Zhang 等人，[2018](#bib.bib49)) 评估两幅图像之间的感知相似性。在伪像图像合成任务中，评估合成图像的结构相似性和感知质量是不可或缺的。LPIPS
    旨在与人类对图像质量的感知对齐，这也符合伪像图像合成任务生成视觉上令人信服的历史对象的目标。
- en: '![Refer to caption](img/20de2713d1b4055d5796e71b4941e6b1.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/20de2713d1b4055d5796e71b4941e6b1.png)'
- en: Figure 6\. An example of our GPT-3.5 querying prompt. We use Chinese by default
    because of the origin language of our data. The right-hand side is an English
    translation also done by the same GPT-3.5-TURBO engine.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 我们 GPT-3.5 查询提示的示例。由于我们数据的来源语言，我们默认使用中文。右侧是同样由 GPT-3.5-TURBO 引擎完成的英文翻译。
- en: '![Refer to caption](img/f2723d590b7d77e0e80ae45bc843a8f6.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f2723d590b7d77e0e80ae45bc843a8f6.png)'
- en: Figure 7\. High-fidelity images of a wide range of artifacts generated by our
    model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 我们模型生成的各种伪像的高保真图像。
- en: Appendix E More Examples of Artifact Images Generated by Our Model
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 我们模型生成的伪像图像更多示例
- en: Our model is capable of synthesizing images of a wide range of artifacts with
    high historical accuracy based on simple textual descriptions, as shown in Figure [7](#A4.F7
    "Figure 7 ‣ Learned Perceptual Image Patch Similarity (LPIPS). ‣ Appendix D More
    Details on Evaluation Metrics ‣ Knowledge-Aware Artifact Image Synthesis with
    LLM-Enhanced Prompting and Multi-Source Supervision") in this appendix.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型能够根据简单的文本描述合成高历史准确度的各种伪像图像，如附录中图 [7](#A4.F7 "图 7 ‣ 学习的感知图像块相似性 (LPIPS)。
    ‣ 附录 D 评估指标的更多细节 ‣ 知识驱动的伪像图像合成与 LLM 增强提示和多源监督") 所示。
