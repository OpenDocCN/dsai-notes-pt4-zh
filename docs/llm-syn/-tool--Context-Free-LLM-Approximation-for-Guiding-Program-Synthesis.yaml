- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 19:03:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:03:55'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '\tool: Context-Free LLM Approximation for Guiding Program Synthesis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '\tool: 用于指导程序合成的上下文无关LLM近似'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.15880](https://ar5iv.labs.arxiv.org/html/2405.15880)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.15880](https://ar5iv.labs.arxiv.org/html/2405.15880)
- en: Shraddha Barke
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shraddha Barke
- en: UC San Diego
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: UC 圣地亚哥
- en: San Diego, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 美国圣地亚哥
- en: sbarke@ucsd.edu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: sbarke@ucsd.edu
- en: '&Emmanuel Anaya Gonzalez'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Emmanuel Anaya Gonzalez'
- en: UC San Diego
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: UC 圣地亚哥
- en: San Diego, USA
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 美国圣地亚哥
- en: fanayagonzalez@ucsd.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: fanayagonzalez@ucsd.edu
- en: '&Saketh Ram Kasibatla'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&Saketh Ram Kasibatla'
- en: UC San Diego
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: UC 圣地亚哥
- en: San Diego, USA
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 美国圣地亚哥
- en: skasibatla@ucsd.edu
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: skasibatla@ucsd.edu
- en: '&Taylor Berg-Kirkpatrick'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&Taylor Berg-Kirkpatrick'
- en: UC San Diego
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: UC 圣地亚哥
- en: San Diego, USA
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 美国圣地亚哥
- en: tbergkirkpatrick@ucsd.edu
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: tbergkirkpatrick@ucsd.edu
- en: '&Nadia Polikarpova'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '&Nadia Polikarpova'
- en: UC San Diego
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: UC 圣地亚哥
- en: San Diego, USA
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 美国圣地亚哥
- en: npolikarpova@ucsd.edu
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: npolikarpova@ucsd.edu
- en: Abstract
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Many structured prediction and reasoning tasks can be framed as program synthesis
    problems, where the goal is to generate a program in a *domain-specific language*
    (DSL) that transforms input data into the desired output. Unfortunately, purely
    neural approaches, such as large language models (LLMs), often fail to produce
    fully correct programs in unfamiliar DSLs, while purely symbolic methods based
    on combinatorial search scale poorly to complex problems. Motivated by these limitations,
    we introduce a hybrid approach, where LLM completions for a given task are used
    to learn a task-specific, context-free surrogate model, which is then used to
    guide program synthesis. We evaluate this hybrid approach on three domains, and
    show that it outperforms both unguided search and direct sampling from LLMs, as
    well as existing program synthesizers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 许多结构化预测和推理任务可以被框定为程序合成问题，其目标是生成一个*领域特定语言*（DSL）中的程序，将输入数据转换为期望的输出。不幸的是，纯粹的神经方法，如大型语言模型（LLMs），在不熟悉的DSL中往往无法生成完全正确的程序，而基于组合搜索的纯符号方法在复杂问题上表现不佳。基于这些限制，我们引入了一种混合方法，其中使用LLM完成的给定任务来学习一个特定任务的上下文无关替代模型，然后用它来指导程序合成。我们在三个领域上评估了这种混合方法，并显示它优于无指导搜索和直接从LLMs采样，以及现有的程序合成器。
- en: 1 Introduction
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large language models (LLMs) demonstrate impressive capabilities in various
    domains, but they continue to struggle with tasks that require precision—e.g. structured
    prediction, reasoning, counting, or data transformation—when direct task examples
    are not prevalent in their training data [[38](#bib.bib38), [45](#bib.bib45),
    [12](#bib.bib12), [8](#bib.bib8), [23](#bib.bib23), [40](#bib.bib40), [31](#bib.bib31)].
    As one example, consider the *Abstraction and Reasoning Corpus* (Arc) [[14](#bib.bib14)],
    which was designed as a benchmark for human-like structured reasoning. Arc tasks
    are grid-based puzzles, such as one depicted in [1(a)](#S1.F1.sf1 "1(a) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"). This puzzle consists of three training examples, which are pairs
    of input and output grids; the goal is to infer the transformation that maps the
    input to the output, and then apply this transformation to the test grid. The
    Arc benchmark’s emphasis on generalization and few-shot learning has rendered
    it challenging to solve with purely machine learning techniques: state-of-the-art
    generative models like GPT-4 hardly solve more than 10% of the tasks in the dataset
    when asked to predict the test output, even with the help of advanced prompting
    techniques [[25](#bib.bib25)].'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）在各个领域展现了令人印象深刻的能力，但它们在需要精确度的任务上仍然存在困难，例如结构化预测、推理、计数或数据转换，当直接的任务示例在训练数据中不常见时
    [[38](#bib.bib38), [45](#bib.bib45), [12](#bib.bib12), [8](#bib.bib8), [23](#bib.bib23),
    [40](#bib.bib40), [31](#bib.bib31)]。例如，*抽象与推理语料库*（Arc）[[14](#bib.bib14)]，它被设计为人类类似的结构化推理基准。Arc任务是基于网格的谜题，如[1(a)](#S1.F1.sf1
    "1(a) ‣ 图 1 ‣ 1 引言 ‣ \tool: 用于指导程序合成的上下文无关LLM近似")中所示。这些谜题由三个训练示例组成，它们是输入和输出网格的对；目标是推断将输入映射到输出的转换，然后将这种转换应用于测试网格。Arc基准强调泛化和少量样本学习，使其仅用纯粹的机器学习技术解决起来具有挑战性：最先进的生成模型如GPT-4在被要求预测测试输出时，几乎无法解决数据集中超过10%的任务，即使借助先进的提示技术
    [[25](#bib.bib25)]。'
- en: '![Refer to caption](img/5b885f8e13c6c66ffbdba1b24b2f4a97.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/5b885f8e13c6c66ffbdba1b24b2f4a97.png)'
- en: (a) Arc
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 弧
- en: '![Refer to caption](img/810e04c85579d0e35e06d0fb43d3b4cb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/810e04c85579d0e35e06d0fb43d3b4cb.png)'
- en: (b) Tensor
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 张量
- en: '![Refer to caption](img/51f93defe020204a37f7b2b1d1f31eb9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/51f93defe020204a37f7b2b1d1f31eb9.png)'
- en: (c) String
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 字符串
- en: 'Figure 1: Example problems from the three domains we evaluate \toolon: grid-based
    puzzles (Arc), tensor manipulation (Tensor), and string manipulation (String).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们评估的三个领域的示例问题：基于网格的谜题（Arc）、张量操作（Tensor）和字符串操作（String）。
- en: 'In fact, the leading entries in the Arc Kaggle competition [[1](#bib.bib1)]
    tackle this task using *Programming-by-Example* (PBE): instead of predicting the
    output directly, they search for a program that captures the transformation occurring
    in the input-output examples. For example, the transformation in [1(a)](#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for
    Guiding Program Synthesis") might be represented as the following program:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '实际上，Arc Kaggle 竞赛中的领先条目 [[1](#bib.bib1)] 采用了*基于示例编程*（PBE）的方法来处理这一任务：它们不是直接预测输出，而是寻找一个能够捕捉输入-输出示例中发生的转换的程序。例如，[1(a)](#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for
    Guiding Program Synthesis")中的转换可以表示为以下程序：'
- en: '|  | $1$2 |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: This particular program is written in a *domain-specific language* (DSL) inspired
    by the Arga tool [[44](#bib.bib44)]. It consists of a single *rule* of the form
    if *filter* then *transform*, which is applied to each object in the grid simultaneously;
    if the filter holds for the focus object self and another object other, then self
    undergoes the transform. In this case, the rule says that any grey object that
    has a neighbor of the grid’s minimum size (here, a single pixel) should be colored
    with the color of that neighbor.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的程序是用一种*领域特定语言*（DSL）编写的，灵感来自 Arga 工具 [[44](#bib.bib44)]。它由一个*规则*组成，形式为 if
    *filter* then *transform*，该规则同时应用于网格中的每个对象；如果滤镜对焦点对象 self 和另一个对象 other 都有效，那么
    self 会经历转换。在这种情况下，规则表示任何灰色对象，如果有一个网格最小尺寸（这里是一个像素）的邻居，则应以该邻居的颜色为该对象上色。
- en: 'Beyond grid puzzles, PBE is a general paradigm for structured reasoning and
    data transformation tasks: for example, it can help spreadsheet users with systematic
    string manipulation [[20](#bib.bib20)], and help programmers use unfamiliar APIs [[18](#bib.bib18),
    [17](#bib.bib17), [36](#bib.bib36)]; [Fig. 1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis") shows
    example PBE tasks from three domains.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '除了网格谜题，PBE 是一种用于结构化推理和数据转换任务的通用范式：例如，它可以帮助电子表格用户进行系统化的字符串操作 [[20](#bib.bib20)]，并帮助程序员使用不熟悉的
    API [[18](#bib.bib18), [17](#bib.bib17), [36](#bib.bib36)]；[图 1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis") 显示了来自三个领域的 PBE 任务示例。'
- en: 'Challenge: Harnessing the Power of LLMs for PBE'
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 挑战：利用 LLM 的力量进行 PBE
- en: 'How can we automatically discover programs from the input-output examples like
    those shown in [Fig. 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")? The traditional *program synthesis*
    approach is based on combinatorial search [[39](#bib.bib39), [2](#bib.bib2), [34](#bib.bib34),
    [7](#bib.bib7), [35](#bib.bib35)], which works well for small programs and restrictive
    DSLs, but becomes infeasible as the program size and the DSL complexity grow.
    At the other end of the spectrum, purely *neural* approaches [[15](#bib.bib15),
    [42](#bib.bib42)] use a neural model to predict the program from input-output
    examples; unfortunately, even state-of-art LLMs like GPT-4o [[33](#bib.bib33)]
    struggle to predict an entire program in an unfamiliar DSL: when we asked GPT-4o
    to generate 10 programs for the running example above, none of them were entirely
    correct.¹¹1A detailed analysis of GPT-4o’s performance on this task is provided
    in [Appendix A](#A1 "Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '我们如何从输入-输出示例中自动发现程序，如[图 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")中所示？传统的*程序合成*方法基于组合搜索 [[39](#bib.bib39),
    [2](#bib.bib2), [34](#bib.bib34), [7](#bib.bib7), [35](#bib.bib35)]，这种方法对于小程序和限制性的
    DSL 工作良好，但随着程序大小和 DSL 复杂度的增加，它变得不可行。在另一端，纯*神经*方法 [[15](#bib.bib15), [42](#bib.bib42)]
    使用神经模型从输入-输出示例中预测程序；不幸的是，即使是像 GPT-4o [[33](#bib.bib33)] 这样的最先进 LLM，也难以预测不熟悉的 DSL
    中的整个程序：当我们让 GPT-4o 为上述运行示例生成 10 个程序时，没有一个完全正确。¹¹1 对 GPT-4o 在此任务上的性能的详细分析见 [附录
    A](#A1 "Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis")。'
- en: In the past, the limitations of both program synthesis and neural techniques
    have motivated a hybrid approach, where combinatorial search is *guided* by a
    learned probabilistic model [[9](#bib.bib9), [24](#bib.bib24), [26](#bib.bib26),
    [32](#bib.bib32), [36](#bib.bib36), [37](#bib.bib37)]. Existing hybrid techniques,
    however, use domain-specific models trained on datasets of similar PBE tasks,
    which limits their generalization to new domains. With the advent of LLMs, can
    we now use a single pre-trained model to guide program synthesis across a wide
    range of domains?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，程序合成和神经技术的局限性促使了一种混合方法，其中组合搜索由学习的概率模型*引导*[[9](#bib.bib9), [24](#bib.bib24),
    [26](#bib.bib26), [32](#bib.bib32), [36](#bib.bib36), [37](#bib.bib37)]。然而，现有的混合技术使用在类似PBE任务的数据集上训练的领域特定模型，这限制了它们对新领域的泛化能力。随着LLMs的出现，我们是否可以使用一个单一的预训练模型来引导程序合成跨越广泛领域？
- en: 'Interestingly, there is some tension in the hybrid approach between the efficiency
    of the search algorithm and the power of the model: a search algorithm is efficient
    when it *factorizes the search space* (*i.e.*, merges many search states into
    one), which often makes it incompatible with a powerful model that requires a
    lot of context to make a prediction. Specifically, one of the most widely used
    program synthesis techniques is *bottom-up search* [[2](#bib.bib2), [39](#bib.bib39),
    [11](#bib.bib11), [36](#bib.bib36), [28](#bib.bib28)], which is a dynamic programming
    algorithm, whose efficiency relies on reusing the work of constructing and evaluating
    subprograms in many different contexts. This essentially precludes using models
    with unlimited left-to-right context—like LLMs–to guide bottom-up search.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，混合方法中存在搜索算法效率与模型能力之间的某种紧张关系：当搜索算法*将搜索空间分解*（*即*，将许多搜索状态合并为一个）时，它是高效的，这往往使其与需要大量上下文才能做出预测的强大模型不兼容。具体来说，最广泛使用的程序合成技术之一是*自下而上的搜索*[[2](#bib.bib2),
    [39](#bib.bib39), [11](#bib.bib11), [36](#bib.bib36), [28](#bib.bib28)]，这是一种动态规划算法，其效率依赖于在许多不同上下文中重用构造和评估子程序的工作。这本质上排除了使用具有无限左右上下文的模型（如LLMs）来引导自下而上的搜索。
- en: 'Our Solution: Context-Free LLM Approximation'
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的解决方案：无上下文LLM近似
- en: 'To bridge this gap and harness the power of LLMs to guide bottom-up search,
    we propose to approximate the LLM’s conditional output distribution *for a given
    task* with a context-free surrogate model. Recent work in NLP [[46](#bib.bib46)]
    has found that a Hidden Markov Model (HMM) trained to match an LLM can be used
    as an efficient surrogate in style-controlled language generation. We extend this
    idea to program synthesis, replacing the HMM with a *probabilistic context-free
    grammar* (PCFG). The benefits of using a PCFG are twofold: (1) PCFGs are context-free,
    which makes them compatible with bottom-up search for PBE [[11](#bib.bib11), [36](#bib.bib36)],
    and (2) while a context-free model may make a poor approximation to an LLM’s full
    joint, in a PBE setting it is able to reasonably approximate an LLM’s conditional
    distribution over output programs *for a given prompt*. The overview of our approach
    is shown in [Fig. 2](#S1.F2 "Figure 2 ‣ Our Solution: Context-Free LLM Approximation
    ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '为了弥合这一差距并利用LLMs的力量来引导自下而上的搜索，我们提出用无上下文的替代模型来*近似LLM的条件输出分布*。最近在NLP领域的工作[[46](#bib.bib46)]发现，训练以匹配LLM的隐马尔可夫模型（HMM）可以作为风格控制语言生成中的高效替代模型。我们将这一思路扩展到程序合成，用*概率上下文无关文法*（PCFG）替代HMM。使用PCFG的好处有两个：（1）PCFG是无上下文的，这使它们与PBE的自下而上搜索兼容[[11](#bib.bib11),
    [36](#bib.bib36)]；（2）虽然无上下文模型可能无法很好地近似LLM的完整联合分布，但在PBE设置中，它能够合理近似LLM在*给定提示*上的条件分布。我们的方法概述见[图2](#S1.F2
    "图2 ‣ 我们的解决方案：无上下文LLM近似 ‣ 1 引言 ‣ \tool: 用于引导程序合成的无上下文LLM近似")。'
- en: '![Refer to caption](img/4e20e29e00396ce3f5fc27a3da8b779d.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4e20e29e00396ce3f5fc27a3da8b779d.png)'
- en: 'Figure 2: An overview of the hybrid program synthesis technique that uses a
    context-free LLM approximation. Programs generated by an LLM are used to learn
    a PCFG, which guides a bottom-up synthesizer to generate programs until a solution
    is found.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用无上下文LLM近似的混合程序合成技术概述。LLM生成的程序用于学习PCFG，从而引导自下而上的合成器生成程序，直到找到解决方案。
- en: Evaluation
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: 'We implemented this technique in a tool \tool²²2The name stands for “HYbrid
    SYNTHesis” and is pronounced like the flower “hyacinth”. and evaluated it on 299
    PBE tasks from three domains: Arc grid-based puzzles [[14](#bib.bib14)], tensor
    manipulation tasks from TFCoder [[36](#bib.bib36)], and string manipulation tasks
    from the SyGuS benchmark [[5](#bib.bib5)], which are inspired by spreadsheet use
    cases. Example problems from these domains are shown in [Fig. 1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"). Our evaluation shows that \tooloutperforms both unguided search and
    LLMs alone, solving 58% of the tasks overall, compared to 40% for unguided search
    and 2% for LLMs without search. Our tool also outperforms baseline program synthesizers
    for these domains—Arga, TFCoder, and Probe, respectively; importantly, in the
    Tensor domain, the guidance from the LLM not only speeds up the search, but also
    frees the user from having to explicitly provide any non-standard *constants*
    that the solution might use, thereby significantly improving the usability of
    the tool.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在工具\tool²²2该名称代表“HYbrid SYNTHesis”，发音类似于花朵“风信子”。中实现了这种技术，并在来自三个领域的 299 个
    PBE 任务上进行了评估：Arc 网格谜题[[14](#bib.bib14)]、来自 TFCoder 的张量操作任务[[36](#bib.bib36)]以及来自
    SyGuS 基准的字符串操作任务[[5](#bib.bib5)]，这些任务受到电子表格用例的启发。这些领域中的示例问题见[图 1](#S1.F1 "图 1
    ‣ 1 引言 ‣ \tool: 用于指导程序合成的上下文无关 LLM 近似")。我们的评估显示，\tool优于无指导搜索和仅使用 LLM，整体上解决了 58%
    的任务，而无指导搜索为 40%，而没有搜索的 LLM 为 2%。我们的工具还优于这些领域的基线程序合成器——分别是 Arga、TFCoder 和 Probe；值得注意的是，在
    Tensor 领域中，LLM 的指导不仅加速了搜索，还使用户无需明确提供任何解决方案可能使用的非标准*常量*，从而显著提高了工具的可用性。'
- en: Contributions
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献
- en: 'In summary, this paper makes the following contributions:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文做出了以下贡献：
- en: '1.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose a hybrid program synthesis approach that integrates LLMs with efficient
    bottom-up search via a task-specific context-free approximation.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种将 LLM 与通过任务特定上下文无关近似的高效自下而上搜索相结合的混合程序合成方法。
- en: '2.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'We implement this approach in a tool \tooland instantiate it on three domains:
    grid-based puzzles (Arc), tensor manipulation (Tensor), and string manipulation
    (String). While the latter two domains reuse off-the-shelf bottom-up synthesizers,
    for Arc we implement a custom synthesizer that uses a divide-and-conquer strategy [[6](#bib.bib6)]
    to leverage the structure of the rule-based DSL to further speed up the search.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在工具\tool中实现了这种方法，并在三个领域上进行了实例化：基于网格的谜题（Arc）、张量操作（Tensor）和字符串操作（String）。虽然后两个领域重用了现成的自下而上合成器，但对于
    Arc，我们实现了一个自定义合成器，该合成器使用分治策略[[6](#bib.bib6)]，利用基于规则的 DSL 的结构进一步加速搜索。
- en: '3.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We evaluate \toolon the three domains and show that it outperforms both the
    LLM alone and existing baseline synthesizers, which are not guided by LLMs.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这三个领域评估了\tool，结果表明，它优于仅使用 LLM 和现有基线合成器（这些合成器没有受到 LLM 的指导）。
- en: 2 Background
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Programming-By-Example
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 通过示例编程
- en: Programming by Example (PBE) [[21](#bib.bib21)] is the task of synthesizing
    programs that satisfy a given set of input-output examples. To restrict the program
    space, the programs are typically drawn from a *domain-specific language* (DSL),
    which is specified by a *context-free grammar* and an *evaluation function*. This
    section provides a formal definition of these concepts.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过示例编程（PBE）[[21](#bib.bib21)] 是一种综合满足给定输入输出示例的程序的任务。为了限制程序空间，这些程序通常来自于*特定领域语言*（DSL），该语言由*上下文无关文法*和*评估函数*定义。本节提供了这些概念的正式定义。
- en: Context-Free Grammars
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文无关文法
- en: A *context-free grammar* (CFG) is a quadruple $\mathcal{G}=(\mathcal{N},\Sigma,\mathcal{S},\mathcal{R})$
    is called (leftmost) *derivation*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*上下文无关文法*（CFG）是一个四元组 $\mathcal{G}=(\mathcal{N},\Sigma,\mathcal{S},\mathcal{R})$，被称为（最左）*推导*。'
- en: '|  | $\displaystyle\mathit{Rule}$ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{Rule}$ |  |'
- en: '|  | $\displaystyle\mathit{Filter}$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{Filter}$ |  |'
- en: '|  | $\displaystyle\mathit{Transform}$ |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{Transform}$ |  |'
- en: '|  | $\displaystyle\mathit{Color}$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{Color}$ |  |'
- en: '|  | $\displaystyle\mathit{Size}$ |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{Size}$ |  |'
- en: '|  | $\displaystyle\mathit{Dir}$ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{Dir}$ |  |'
- en: '|  | $\displaystyle\mathit{Obj}$ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{Obj}$ |  |'
- en: 'Figure 3: A fragment from the context-free grammar of our Arc DSL.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们 Arc DSL 的上下文无关文法的一部分。
- en: Programs
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 程序
- en: A *program*  $P\in\Sigma^{*}$, which maps the values of program variables to
    its output value.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*程序* $P\in\Sigma^{*}$，它将程序变量的值映射到其输出值。
- en: Problem Statement
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题陈述
- en: A PBE problem is defined by a DSL with a grammar $\mathcal{G}$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: PBE 问题由具有文法 $\mathcal{G}$ 的 DSL 定义。
- en: 2.2 Assigning Costs to Programs
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 分配程序成本
- en: Weighted Context-free Grammar
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加权上下文无关文法
- en: A *weighted context-free grammar* (WCFG) $\mathcal{G}_{w}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*加权上下文无关文法*（WCFG）$\mathcal{G}_{w}$。'
- en: For the purposes of search, it is convenient to define a *discrete weight* function
    $w:\mathcal{R}\rightarrow\mathbb{Z}^{+}$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行搜索，定义 *离散权重* 函数 $w:\mathcal{R}\rightarrow\mathbb{Z}^{+}$ 是方便的。
- en: Probabilistic Context-free Grammar
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概率上下文无关文法
- en: A popular way to assign weights to production rules is via a *probabilistic
    context-free grammar* (PCFG). A PCFG $\mathcal{G}_{p}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将权重分配给生成规则的一种流行方法是通过 *概率上下文无关文法*（PCFG）。一个 PCFG $\mathcal{G}_{p}$。
- en: Given a PCFG $(\mathcal{G},p)$.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个 PCFG $(\mathcal{G},p)$。
- en: 2.3 Bottom-up Search
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 自底向上搜索
- en: Algorithm 1 Bottom-Up Search Algorithm
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 自底向上的搜索算法
- en: 1:Input-output examples $\mathcal{E}$ $P$ non-terminals18:         for $(c_{1},\dots,c_{k})\in\left\{\,[1..\textsc{Lvl}-1]^{k}\,\middle|\,\sum
    c_{i}=\textsc{Lvl}-w(\textsc{R})\,\right\}$ Substitute subexpressions into R’s
    RHS
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入输出示例 $\mathcal{E}$ $P$ 非终结符18:         对所有 $(c_{1},\dots,c_{k})\in\left\{\,[1..\textsc{Lvl}-1]^{k}\,\middle|\,\sum
    c_{i}=\textsc{Lvl}-w(\textsc{R})\,\right\}$ 代入子表达式到 R 的右侧'
- en: 'Bottom-up search is a popular search technique in program synthesis [[2](#bib.bib2),
    [39](#bib.bib39), [11](#bib.bib11), [36](#bib.bib36), [28](#bib.bib28)], which
    enumerates programs from the DSL in the order of increasing costs until it finds
    a program that satisfies the given examples. The search is implemented as a dynamic
    programming algorithm (see [Alg. 1](#alg1 "Algorithm 1 ‣ 2.3 Bottom-up Search
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")), which maintains a program
    *bank*  B mapping discrete costs to programs of that cost. Starting with an empty
    bank and current cost level $\textsc{Lvl}=1$, the search iteratively creates all
    programs of cost 1, 2, 3, and so on; to create complex programs, the algorithm
    *reuses* simpler programs already stored in the bank, and combines them using
    the production rules of the grammar.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '自底向上搜索是一种流行的程序合成搜索技术 [[2](#bib.bib2), [39](#bib.bib39), [11](#bib.bib11), [36](#bib.bib36),
    [28](#bib.bib28)]，它按照成本递增的顺序枚举 DSL 中的程序，直到找到满足给定示例的程序。搜索实现为动态规划算法（见 [Alg. 1](#alg1
    "Algorithm 1 ‣ 2.3 Bottom-up Search ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")），它维护一个程序
    *库* B，将离散成本映射到相应的程序。从空库和当前成本级别 $\textsc{Lvl}=1$ 开始，搜索迭代地创建所有成本为 1、2、3 等的程序；为了创建复杂程序，该算法
    *重用* 已经存储在库中的简单程序，并通过文法的生成规则将它们组合起来。'
- en: 'For example, consider the CFG in [Sec. 2.1](#S2.SS1.SSS0.Px1 "Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis"), and assume a uniform weight function
    $w(\cdot)=1$.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，考虑 [Sec. 2.1](#S2.SS1.SSS0.Px1 "Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")
    中的 CFG，并假设均匀权重函数 $w(\cdot)=1$。'
- en: 'During search, each candidate expression is evaluated to see if it satisfies
    the examples (lines 5–7). Importantly, the search maintains a cache of all evaluation
    results E, and discard the newly constructed program if it is *observationally
    equivalent* to a program already in the bank (line 8), *i.e.* if it evaluates
    to the same output for all inputs in the examples. This step is the key to the
    efficiency of the bottom-up search algorithm: it allows the synthesizer to factorize
    the search space by evaluation result, significantly reducing the number of programs
    explored at each cost level.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索过程中，每个候选表达式都被评估以查看其是否满足示例（第 5–7 行）。重要的是，搜索维护所有评估结果 E 的缓存，并且如果新构造的程序 *在观察上等价*
    于库中已经存在的程序（第 8 行），则丢弃该程序，即如果它在所有示例输入下的输出相同。这一步骤是自底向上搜索算法效率的关键：它允许合成器通过评估结果对搜索空间进行因式分解，显著减少在每个成本级别上探索的程序数量。
- en: 3 The \toolApproach
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 \toolApproach
- en: 'A key challenge in program synthesis is the astronomical size of the search
    space the synthesizer has to explore. For example, to find the program [Eq. 1](#S1.E1
    "1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"), the solution to the Arc task from the introduction, bottom-up search
    with a uniform weight function has to enumerate around 450K programs (all programs
    of size $\leq 16$), which takes 4.5 minutes in our experiments.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '程序合成中的一个关键挑战是合成器必须探索的搜索空间的庞大。例如，为了找到程序[方程 1](#S1.E1 "1 ‣ 1 Introduction ‣ \tool:
    Context-Free LLM Approximation for Guiding Program Synthesis")，即引言中的Arc任务的解决方案，自底向上的搜索需要枚举大约450K个程序（所有大小$\leq
    16$的程序），在我们的实验中需要4.5分钟。'
- en: 'On the other hand, sampling solutions to this task from an LLM yields programs
    that are *close* to the desired solution, even if not quite correct. As we show
    in [Appendix A](#A1 "Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis"), GPT-4o uses relevant components
    update_color, color_of, and is_neighbor in nearly all of its solutions (usually
    missing some part of the filter or using the wrong color in the transform), and
    never uses irrelevant components like move or rotate. This suggests that the LLM
    generally has the right intuition about the components the solution needs to use;
    our insight is to leverage this intuition to guide bottom-up search by *assigning
    lower weights to the components that the LLM uses frequently*.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，从LLM中采样该任务的解决方案会产生*接近*期望解决方案的程序，即使不完全正确。正如我们在[附录 A](#A1 "Appendix A GPT4o
    Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")中所示，GPT-4o在几乎所有解决方案中都使用了相关组件update_color、color_of和is_neighbor（通常缺少部分过滤器或在变换中使用了错误的颜色），且从未使用像move或rotate这样的无关组件。这表明LLM通常对解决方案所需组件有正确的直觉；我们的见解是利用这种直觉通过*为LLM频繁使用的组件分配较低的权重*来引导自底向上的搜索。'
- en: 3.1 Guiding Bottom-up Search with Context-Free LLM Approximation
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 使用上下文无关LLM近似引导自底向上的搜索
- en: 'The overview of our approach, \tool, is shown in [Fig. 2](#S1.F2 "Figure 2
    ‣ Our Solution: Context-Free LLM Approximation ‣ 1 Introduction ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis"). Given a PBE problem consisting
    of a DSL with grammar $\mathcal{G}$, \toolproceeds in three steps.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法概述，\tool，如[图 2](#S1.F2 "Figure 2 ‣ Our Solution: Context-Free LLM Approximation
    ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")所示。给定一个包含语法$\mathcal{G}$的DSL的PBE问题，\tool
    分三个步骤进行。'
- en: 'Step 1: Sampling Solutions from an LLM'
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步：从LLM中采样解决方案
- en: \tool
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \tool
- en: starts by creating an LLM prompt that contains $\mathcal{G}$ trades off computational
    cost and the faithfulness of the approximation to the true LLM conditional.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个包含$\mathcal{G}$的LLM提示，这在计算成本和近似于真实LLM条件的忠实性之间做出权衡。
- en: 'Step 2: Learning a PCFG from LLM Solutions'
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2步：从LLM解决方案中学习PCFG
- en: Next, \toolattempts to parse each completion $S_{i}$ is a smoothing parameter
    that ensures that every rule has a non-zero probability (typically set to 1).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，\tool尝试解析每个补全$S_{i}$是一个平滑参数，确保每个规则具有非零概率（通常设为1）。
- en: Our experiments show that some models struggle to generate grammatical completions,
    leading to $N^{\prime}\ll N$ that produce this terminal.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验显示，一些模型在生成语法正确的补全时存在困难，导致产生的终结符数量$N^{\prime}\ll N$。
- en: 'Step 3: Guiding Bottom-up Search with PCFG'
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第3步：使用PCFG引导自底向上的搜索
- en: Finally, \tooluses the PCFG computed in the previous step to derive a weighted
    grammar $\mathcal{G}_{w}$ rules have weight 4; the relevant filter operators color_of
    and is_neighbor are similarly down-weighted. As a result, the search procedure
    only has to enumerate around 220K programs instead of 450K, achieving a 4x speedup,
    and solving the motivating example in just one minute with LLM guidance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，\tool 使用在前一步计算得到的PCFG来推导一个加权语法$\mathcal{G}_{w}$，其中规则的权重为4；相关的过滤操作符color_of和is_neighbor也被相应地降低权重。因此，搜索过程只需枚举大约220K个程序而不是450K，实现了4倍的加速，并在仅用LLM引导的情况下在一分钟内解决了激励示例。
- en: 3.2 Domain-Specific Instantiations
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 领域特定实例化
- en: 'We now describe how the \toolapproach is instantiated in three different domains:
    Arc grid puzzles, Tensor manipulations, and String manipulations.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在描述\tool方法在三个不同领域中的实例化：Arc网格谜题、张量操作和字符串操作。
- en: Arc Domain
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Arc领域
- en: 'An example task from this domain is shown in [1(a)](#S1.F1.sf1 "1(a) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis") and has been used as a running example throughout this paper. There
    is no established DSL for Arc, and arguably, DSL design is the biggest challenge
    when attempting to solve Arc using a PBE approach, since it is hard to capture
    the wide variety of tasks in this domain. Our DSL is inspired by the rule-based
    language of Arga [[44](#bib.bib44)], which we modified slightly to make it more
    compositional.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '本领域的一个示例任务如[1(a)](#S1.F1.sf1 "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")所示，并在本文中作为一个运行示例。Arc 没有一个确定的DSL（领域特定语言），可以说，DSL设计是使用PBE方法解决Arc时面临的最大挑战，因为很难捕捉到该领域任务的广泛变化。我们的DSL灵感来源于Arga
    [[44](#bib.bib44)] 的规则语言，我们稍微修改了它，使其更加可组合。'
- en: 'A program in our DSL is a sequence of rules of the form if  *filter*  then  *transform*.
    A rule refers to the current object self, which is modified by the transform if
    the filter is satisfied in the current state of the grid. The rule can also refer
    to other objects in the grid, such as other in [Eq. 1](#S1.E1 "1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"). This
    program is well-defined because its filter uniquely identifies the object other;
    if the filter is too weak to uniquely determine the effect of the transform, the
    program’s output is considered undefined. The full grammar of our DSL can be found
    in [Appendix H](#A8 "Appendix H The Full Arc DSL ‣ Appendix G The Full String
    Grammar ‣ Appendix F LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor
    Grammar ‣ Appendix D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix
    C Different sample sizes ablation for Arc, Tensor and String domains ‣ Appendix
    B LLM Prompt for the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating
    Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣
    \tool: Context-Free LLM Approximation for Guiding Program Synthesis").'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '我们DSL中的程序是一系列规则，形式为 if *filter* then *transform*。规则指当前对象本身，如果在网格的当前状态下过滤器满足条件，则变换将修改该对象。规则还可以引用网格中的其他对象，例如在[公式
    1](#S1.E1 "1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding
    Program Synthesis")中。这些程序定义明确，因为其过滤器可以唯一地识别其他对象；如果过滤器太弱以至于不能唯一确定变换的效果，则程序的输出被认为是未定义的。我们DSL的完整语法可以在[附录
    H](#A8 "Appendix H The Full Arc DSL ‣ Appendix G The Full String Grammar ‣ Appendix
    F LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")中找到。'
- en: 'Instead of searching for a complete program using [Alg. 1](#alg1 "Algorithm
    1 ‣ 2.3 Bottom-up Search ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"),
    we further optimize our synthesizer using a divide-and-conquer strategy inspired
    by [[6](#bib.bib6)], searching for filters and transforms *separately*. Specifically,
    \tool-Arc first searches for transforms that are correct on some objects in the
    grid; once it has found a set of transforms that collectively describe all grid
    objects, it searches for filters that distinguish between the subsets of objects
    changed by each transform.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '我们没有直接搜索完整的程序使用[算法 1](#alg1 "Algorithm 1 ‣ 2.3 Bottom-up Search ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis")，而是进一步优化了我们的合成器，采用了一种基于[[6](#bib.bib6)]的分而治之策略，*分别*
    搜索过滤器和变换。具体来说，\tool-Arc 首先搜索在网格中对某些对象正确的变换；一旦找到一组变换能够共同描述所有网格对象，它就会搜索区分每个变换所改变的对象子集的过滤器。'
- en: 'Consider once again our running example. When the transform synthesizer enumerates
    the expression update_color(color_of(other)), it detects that this transform works
    for all *grey object*, because for each grey object self there exists a corresponding
    object other whose color can be copied. Now the goal of filter synthesis is to
    find a boolean expression that holds exactly for those pairs of objects ${{(\mbox{\leavevmode\lstinline{{\lst@@@set@language\lst@@@set@numbers\lst@@@set@frame\lst@@@set@rulecolor\lst@@@set@numbers\lst@@@set@language\lst@@@set@numbers\lst@@@set@language{\@listingGroup{ltx_lst_keyword}{self}}}}}},\mbox{\leavevmode\lstinline{{\lst@@@set@language\lst@@@set@numbers\lst@@@set@frame\lst@@@set@rulecolor\lst@@@set@numbers\lst@@@set@language\lst@@@set@numbers\lst@@@set@language{\@listingGroup{ltx_lst_identifier}{other}}}}}})$
    that make the transform work. See [Appendix L](#A12 "Appendix L The Arc Synthesis
    Algorithm ‣ Appendix K Broader Research Impacts ‣ Appendix J Detailed Prompt Settings
    ‣ Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc DSL ‣ Appendix
    G The Full String Grammar ‣ Appendix F LLM Prompt for String ‣ Appendix E LLM
    Prompt for the Tensor Grammar ‣ Appendix D Experimental results with LLMs DeepSeek
    and Gpt3.5 ‣ Appendix C Different sample sizes ablation for Arc, Tensor and String
    domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix A GPT4o
    Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")
    for more details about this algorithm.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '再次考虑我们正在使用的例子。当变换合成器枚举表达式 `update_color(color_of(other))` 时，它检测到该变换适用于所有*灰色对象*，因为每个灰色对象
    self 都存在一个对应的对象 other，其颜色可以被复制。现在，滤波器合成的目标是找到一个布尔表达式，该表达式恰好对这些使变换有效的对象对 ${{(\mbox{\leavevmode\lstinline{{\lst@@@set@language\lst@@@set@numbers\lst@@@set@frame\lst@@@set@rulecolor\lst@@@set@numbers\lst@@@set@language\lst@@@set@numbers\lst@@@set@language{\@listingGroup{ltx_lst_keyword}{self}}}}}},\mbox{\leavevmode\lstinline{{\lst@@@set@language\lst@@@set@numbers\lst@@@set@frame\lst@@@set@rulecolor\lst@@@set@numbers\lst@@@set@language\lst@@@set@numbers\lst@@@set@language{\@listingGroup{ltx_lst_identifier}{other}}}}}})$
    有效。有关该算法的更多详细信息，请参见[附录 L](#A12 "附录 L 弧合成算法 ‣ 附录 K 更广泛的研究影响 ‣ 附录 J 详细提示设置 ‣ 附录
    I 完整的张量语法 ‣ 附录 H 完整的弧 DSL ‣ 附录 G 完整的字符串语法 ‣ 附录 F 字符串的 LLM 提示 ‣ 附录 E 张量语法的 LLM
    提示 ‣ 附录 D 与 LLMs DeepSeek 和 Gpt3.5 的实验结果 ‣ 附录 C 对弧、张量和字符串领域的不同样本量的消融 ‣ 附录 B 动机示例的
    LLM 提示 ‣ 附录 A GPT4o 动机示例的解决方案 ‣ 无上下文文法 ‣ 2.1 通过示例编程 ‣ 2 背景 ‣ \tool: 无上下文 LLM 近似用于指导程序合成")。'
- en: Tensor Domain
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 张量领域
- en: 'This domain originates from the TFCoder synthesizer[[36](#bib.bib36)], which
    takes as input examples of a tensor transformation (with an optional natural language
    description) and synthesizes a TensorFlow program that performs the transformation.
    An example task from this domain is shown in [1(b)](#S1.F1.sf2 "1(b) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"), whose solution is: tf.gather_nd(in1,  tf.stack((in2,  in3),  axis=-1)).
    The main challenge, however, is that the TensorFlow grammar is very large (see
    [Appendix I](#A9 "Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc
    DSL ‣ Appendix G The Full String Grammar ‣ Appendix F LLM Prompt for String ‣
    Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis")), and most importantly, the programs are allowed
    to use an *unbounded* set of constants. The original TFCoder synthesizer requires
    the user to provide any non-standard constants that a task might require, and,
    according to their paper, this is the main barrier to the usability of their tool.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '该领域源于 TFCoder 合成器[[36](#bib.bib36)]，该合成器以张量变换的示例（可选的自然语言描述）为输入，合成执行该变换的 TensorFlow
    程序。此领域的一个示例任务在 [1(b)](#S1.F1.sf2 "1(b) ‣ 图 1 ‣ 1 引言 ‣ \tool: 无上下文 LLM 近似用于引导程序合成")
    中展示，其解决方案为：tf.gather_nd(in1, tf.stack((in2, in3), axis=-1))。然而，主要挑战在于 TensorFlow
    语法非常庞大（参见 [附录 I](#A9 "附录 I 完整的 Tensor 语法 ‣ 附录 H 完整的 Arc DSL ‣ 附录 G 完整的字符串语法 ‣
    附录 F LLM 提示用于字符串 ‣ 附录 E LLM 提示用于张量语法 ‣ 附录 D 使用 LLMs DeepSeek 和 Gpt3.5 的实验结果 ‣
    附录 C Arc、Tensor 和字符串领域的不同样本大小消融 ‣ 附录 B LLM 提示用于动机示例 ‣ 附录 A GPT4o 对动机示例的解决方案 ‣
    无上下文语法 ‣ 2.1 示例编程 ‣ 2 背景 ‣ \tool: 无上下文 LLM 近似用于引导程序合成")），最重要的是，程序允许使用*无限*的常量集合。原始的
    TFCoder 合成器要求用户提供任务可能需要的任何非标准常量，按照他们的论文，这是他们工具可用性的主要障碍。'
- en: For program synthesis in this domain we use the TFCoder synthesizer off the
    shelf. TFCoder performs weighted bottom-up search, using a combination of hand-tuned
    weights and weights derived by two custom-trained neural models. \tool-Tensor
    replaces these weights entirely with weights computed by sampling from an LLM.
    Importantly, our version of the tool does not require the user to provide any
    constants; instead we extract constants from the LLM completions, whereby significantly
    reducing the burden on the user.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域的程序合成中，我们使用现成的 TFCoder 合成器。TFCoder 执行加权自下而上的搜索，结合手动调整的权重和两个定制训练的神经模型导出的权重。\tool-Tensor
    完全用从 LLM 中采样计算的权重替代这些权重。重要的是，我们版本的工具不需要用户提供任何常量；相反，我们从 LLM 完成中提取常量，从而显著减少用户的负担。
- en: String Domain
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 字符串领域
- en: 'Our third domain involves string manipulation tasks from the SyGuS competition[[4](#bib.bib4)],
    which are inspired by spreadsheet use cases. An example task, which requires extracting
    the top-level domain name from a URL, is shown in [1(c)](#S1.F1.sf3 "1(c) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"). In this domain we use the Probe [[11](#bib.bib11)] synthesizer off
    the shelf. Probe performs weighted bottom-up search, starting with a uniform grammar
    and updating the weights on the fly; \tool-String instead initializes Probe’s
    search with weights derived from an LLM, and disables the weight updates during
    search.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的第三个领域涉及来自 SyGuS 竞赛的字符串操作任务[[4](#bib.bib4)]，这些任务灵感来自电子表格的用例。一个示例任务是从 URL
    中提取顶级域名，如 [1(c)](#S1.F1.sf3 "1(c) ‣ 图 1 ‣ 1 引言 ‣ \tool: 无上下文 LLM 近似用于引导程序合成")
    所示。在这个领域，我们使用现成的 Probe [[11](#bib.bib11)] 合成器。Probe 执行加权自下而上的搜索，从均匀的语法开始，并实时更新权重；\tool-String
    则用从 LLM 中导出的权重初始化 Probe 的搜索，并在搜索过程中禁用权重更新。'
- en: 4 Experiments and Results
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验与结果
- en: '![Refer to caption](img/19c8e978c48016bfbb00912dfdd0ef2a.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/19c8e978c48016bfbb00912dfdd0ef2a.png)'
- en: (a) \tool-Arc results with Gpt4o
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (a) \tool-Arc 与 Gpt4o 的结果
- en: '![Refer to caption](img/512148e9d189f3987ac9dacb5397217e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/512148e9d189f3987ac9dacb5397217e.png)'
- en: (b) \tool-String results with Gpt4o
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (b) \tool-String 与 Gpt4o 的结果
- en: '![Refer to caption](img/d30ace6d3931a4668361a616706d13e0.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d30ace6d3931a4668361a616706d13e0.png)'
- en: (c) \tool-Tensor results with Gpt4o
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (c) \tool-Tensor 与 Gpt4o 的结果
- en: '| Domain/Model | % Valid completions |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 领域/模型 | % 有效完成 |'
- en: '| Tensor-Gpt4o | 99.96% |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Tensor-Gpt4o | 99.96% |'
- en: '| Tensor-DeepSeek | 92.8% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Tensor-DeepSeek | 92.8% |'
- en: '| String-Gpt4o | 98.3% |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| String-Gpt4o | 98.3% |'
- en: '| String-DeepSeek | 86% |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| String-DeepSeek | 86% |'
- en: '| Arc-Gpt4o | 78% |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Arc-Gpt4o | 78% |'
- en: (d) Percentage of syntactically valid completions.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: （d）语法上有效的完成百分比。
- en: 'Figure 4: (a,b,c) Number of benchmarks solved by \toolas a function of time
    for the Arc, Tensor, and String domains; timeout is 10 min. (d) Percentage of
    syntactically valid completions per domain.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：（a,b,c）Arc、Tensor和String领域中，\tool随时间变化解决的基准数目；超时为10分钟。（d）每个领域中语法上有效的完成百分比。
- en: 4.1 Experimental Setup
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'We evaluate \toolon 299 PBE tasks from three different domains: Arc (160 tasks),
    String (70 tasks) and Tensor (69 tasks).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在来自Arc（160个任务）、String（70个任务）和Tensor（69个任务）这三个不同领域的299个PBE任务上评估\tool。
- en: Arc Benchmark
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Arc基准
- en: The 160 Arc tasks are taken from the testing set of Arga  [[44](#bib.bib44)].
    This *object-centric* subset of the full Arc corpus is known as Object-Arc, and
    has been used to evaluate other Arc solvers[[27](#bib.bib27)]. Arc specifications
    consist of 2-7 input-output training grids and 1 testing grid. Correctness is
    based on whether the generated solution produces the correct output on the testing
    grid. Our Arc DSL has a total of 20 operations and 50 constants and variables
    across all types.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这160个Arc任务来自Arga的测试集[[44](#bib.bib44)]。这个*以对象为中心*的Arc完整语料库的子集被称为Object-Arc，并已被用于评估其他Arc求解器[[27](#bib.bib27)]。Arc规范由2-7个输入输出训练网格和1个测试网格组成。正确性基于生成的解决方案是否能在测试网格上产生正确的输出。我们的Arc
    DSL总共有20个操作和50个常量及变量。
- en: Tensor Benchmark
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Tensor基准
- en: The 69 Tensor tasks taken from TFCoder focus on tensor manipulation. 49 of them
    are sourced from StackOverflow inquiries, and 20 are from real-world scenarios
    faced by TensorFlow users at Google. The overall benchmark suite consists of 72
    tasks. We use three of these tasks as in-context examples and evaluate on the
    rest. The grammar for this domain consists of 134 Tensorflow operations, primitives
    like 0, 1, -1, True and other task-specific constants.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这69个Tensor任务来自TFCoder，重点关注张量操作。其中49个任务来源于StackOverflow的咨询，20个任务来自Google的TensorFlow用户面临的真实场景。整体基准套件包含72个任务。我们使用其中三个任务作为上下文示例，并对其余任务进行评估。该领域的语法包括134个Tensorflow操作、0、1、-1、True等原始值以及其他任务特定常量。
- en: String Benchmark
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: String基准
- en: The 70 String tasks are taken from testing set of Probe, which is derived them
    from the SyGuS benchmark[[4](#bib.bib4)]. The number of examples ranges from 2
    to 400. The original SyGuS benchmark have custom grammars for each task, but we
    use a union of all the grammars to make the search more challenging; the union
    grammar has 16 operations and 59 constants.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这70个String任务来自Probe的测试集，它们来自SyGuS基准[[4](#bib.bib4)]。示例数量从2到400不等。原始SyGuS基准为每个任务都有自定义语法，但我们使用所有语法的并集来增加搜索的挑战性；并集语法有16个操作和59个常量。
- en: Configurations
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置
- en: 'Our main \toolconfiguration uses Gpt4o as the LLM, with 100 samples per task
    to learn a PCFG in non-strict mode (*i.e.* syntactically invalid completions are
    included in the PCFG learning process, as explained in [Sec. 3.1](#S3.SS1 "3.1
    Guiding Bottom-up Search with Context-Free LLM Approximation ‣ 3 The \toolApproach
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")). For each domain, we compare
    the performance of \toolwith a baseline synthesizer for that domain (Arga  ⁴⁴4At
    the time of writing, Arga is no longer state of the art on the Object-Arc dataset;
    we explain in [Sec. 5](#S5 "5 Related Work ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")
    why the comparison with Arga is still relevant., Probe, and TFCoder), as well
    as two ablations: (1) *no search*, *i.e.* using the 100 samples from the LLM directly,
    and (2) *unguided search*, *i.e.* running the same synthesizer but with a uniform
    weighted grammar. We also analyze the performance of \toolwith different numbers
    of samples used to learn the PCFG (10, 20, and 50), with other LLMs (Gpt3.5 and
    DeepSeek [[22](#bib.bib22)]), as well as in strict mode (which discards syntactically
    invalid LLM completions). Search timeout is set to 10 minutes for all experiments.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的主要 \toolconfiguration 使用 Gpt4o 作为 LLM，每个任务 100 个样本以非严格模式学习 PCFG（*即* 语法上无效的补全被纳入
    PCFG 学习过程，如 [Sec. 3.1](#S3.SS1 "3.1 用上下文无关 LLM 近似引导自底向上搜索 ‣ 3 \tool 方法 ‣ 上下文无关文法
    ‣ 2.1 编程示例 ‣ 2 背景 ‣ \tool: 上下文无关 LLM 近似引导程序合成") 所述）。对于每个领域，我们将 \tool 的性能与该领域的基线合成器（Arga
    ⁴⁴4 在撰写时，Arga 在 Object-Arc 数据集上已不再是最先进的；我们在 [Sec. 5](#S5 "5 相关工作 ‣ 上下文无关文法 ‣ 2.1
    编程示例 ‣ 2 背景 ‣ \tool: 上下文无关 LLM 近似引导程序合成") 解释了为什么与 Arga 的比较仍然相关）、Probe 和 TFCoder）进行比较，以及两个消融实验：（1）*无搜索*，*即*
    直接使用 LLM 的 100 个样本，以及（2）*无指导搜索*，*即* 运行相同的合成器但使用统一加权的文法。我们还分析了 \tool 在学习 PCFG 时使用不同数量样本（10、20
    和 50）的性能，其他 LLM（Gpt3.5 和 DeepSeek [[22](#bib.bib22)]）以及严格模式（即丢弃语法上无效的 LLM 补全）的性能。所有实验的搜索超时时间设定为
    10 分钟。'
- en: 4.2 Results
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: How does \toolcompare to baselines and ablations?
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: \tool 与基线和消融实验相比如何？
- en: 'We compare the time to solution for the main \toolconfiguration, baseline synthesizers,
    and the two ablations; the results for the three domains are shown in [3(a)](#S4.F3.sf1
    "3(a) ‣ Figure 4 ‣ 4 Experiments and Results ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"),
    [3(c)](#S4.F3.sf3 "3(c) ‣ Figure 4 ‣ 4 Experiments and Results ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis"), and [3(b)](#S4.F3.sf2 "3(b) ‣ Figure
    4 ‣ 4 Experiments and Results ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").
    Overall, \toolconsistently outperforms both the baseline synthesizers and ablations,
    solving more tasks across all domains and time scales.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了主要 \toolconfiguration、基线合成器和两个消融实验的解决时间；三个领域的结果分别显示在 [3(a)](#S4.F3.sf1
    "3(a) ‣ 图 4 ‣ 4 实验与结果 ‣ 上下文无关文法 ‣ 2.1 编程示例 ‣ 2 背景 ‣ \tool: 上下文无关 LLM 近似引导程序合成")、[3(c)](#S4.F3.sf3
    "3(c) ‣ 图 4 ‣ 4 实验与结果 ‣ 上下文无关文法 ‣ 2.1 编程示例 ‣ 2 背景 ‣ \tool: 上下文无关 LLM 近似引导程序合成")
    和 [3(b)](#S4.F3.sf2 "3(b) ‣ 图 4 ‣ 4 实验与结果 ‣ 上下文无关文法 ‣ 2.1 编程示例 ‣ 2 背景 ‣ \tool:
    上下文无关 LLM 近似引导程序合成")。总体而言，\tool 一致优于基线合成器和消融实验，在所有领域和时间尺度上解决了更多任务。'
- en: In more detail, direct LLM sampling performs very poorly on all domains, solving
    between 0 and 5 tasks; this confirms our hypothesis that LLMs struggle on PBE
    tasks in domain-specific languages, which are not prevalent in their training
    data. Interestingly, despite not being able to solve *any*  String tasks by itself,
    Gpt4o provides excellent guidance for \toolon that domain, helping it solve 5x
    more tasks than the unguided search!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，直接的 LLM 采样在所有领域表现非常差，仅解决了 0 到 5 个任务；这证实了我们的假设，即 LLM 在领域特定语言的 PBE 任务上表现不佳，因为这些任务在其训练数据中并不普遍。有趣的是，尽管
    Gpt4o 本身无法解决*任何*字符串任务，但它为 \toolon 提供了出色的指导，帮助它解决了比未指导搜索多 5 倍的任务！
- en: In String and Tensor domains, the baseline synthesizers predictably do better
    than unguided search, since both use the same search implementation, but with
    different weights. On Arc, however, our custom synthesizer outperforms Arga  ⁵⁵5[[44](#bib.bib44)]
    report 57 tasks for Arga but we could only reproduce 51 on our hardware with a
    10 minute timeout. even without LLM guidance; this speaks to the efficiency of
    the bottom-up search and the divide-and-conquer strategy we use, which are results
    of years of research in the program synthesis community.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在String和Tensor领域，基线合成器的表现比未指导搜索更好是可以预测的，因为两者使用了相同的搜索实现，但权重不同。然而，在Arc上，我们的定制合成器即使没有LLM指导也优于Arga  ⁵⁵5[[44](#bib.bib44)]报告Arga完成了57个任务，但我们在硬件上只能重现51个任务，并且在10分钟超时情况下。即使没有LLM指导，这也证明了我们使用的自下而上的搜索和分而治之策略的效率，这是程序合成社区多年研究的成果。
- en: How many samples are needed to learn a PCFG?
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习一个PCFG需要多少样本？
- en: 'To better understand how the number of samples affects the quality of PCFG
    guidance, we vary the number of Gpt4o programs used in PCFG learning $N=10,20,50,100$,
    and once again measure the number of tasks solved over time. The results are shown
    in [Fig. 8](#A3.F8 "Figure 8 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") in [Appendix C](#A3 "Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis"). As expected, larger sample
    sizes generally lead to better performance, but the difference is minimal: in
    Arc and Tensor, the difference between the best and worst performing versions
    of \toolis only 2 and 1 problems, respectively, while in String, \toolsolves 5
    fewer problems with 10 samples than with 100. Despite these differences, all versions
    of \toolstill outperform the baseline and unguided search. This suggests that
    fewer samples are sufficient to effectively train a robust surrogate model, thereby
    optimizing costs.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '为了更好地理解样本数量如何影响PCFG指导的质量，我们改变了PCFG学习中使用的Gpt4o程序数量$N=10,20,50,100$，并再次测量了随时间解决的任务数量。结果如[图8](#A3.F8
    "Figure 8 ‣ Appendix C Different sample sizes ablation for Arc, Tensor and String
    domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix A GPT4o
    Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")所示，在[附录C](#A3
    "Appendix C Different sample sizes ablation for Arc, Tensor and String domains
    ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix A GPT4o Solutions
    for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis")中展示。正如预期的，较大的样本数量通常会导致更好的性能，但差异很小：在Arc和Tensor中，\tool表现最佳和最差版本之间的差距仅为2和1个问题，而在String中，\tool在10个样本下比在100个样本下少解决5个问题。尽管存在这些差异，所有版本的\tool仍然优于基线和未指导搜索。这表明较少的样本足以有效地训练一个稳健的替代模型，从而优化成本。'
- en: Do our results generalize to other models?
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的结果是否适用于其他模型？
- en: 'To answer this question, we repeat our experiments on String and Tensor domains
    with Gpt3.5 and the open-source model deepseek-coder-33b-instruct (DeepSeek) [[22](#bib.bib22)].
    The results with these models are detailed in [Fig. 9](#A4.F9 "Figure 9 ‣ Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") in [Appendix D](#A4 "Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis"), and they corroborate the pattern
    observed with Gpt4o, where the guided versions outperform the baseline, unguided
    search, and direct sampling from the LLM.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '为了回答这个问题，我们重复了在 String 和 Tensor 领域上使用 Gpt3.5 和开源模型 deepseek-coder-33b-instruct
    (DeepSeek) 的实验[[22](#bib.bib22)]。这些模型的结果详见 [图 9](#A4.F9 "Figure 9 ‣ Appendix D
    Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") 在 [附录 D](#A4 "Appendix D Experimental
    results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation
    for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating
    Example ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis") 中，它们证实了与 Gpt4o 观察到的模式一致，即引导版本优于基线、未引导的搜索和直接从
    LLM 采样。'
- en: How important is non-strict mode?
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非严格模式有多重要？
- en: '[3(d)](#S4.F3.sf4 "3(d) ‣ Figure 4 ‣ 4 Experiments and Results ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis") shows the percentage of syntactically
    valid completions generated by Gpt4o and DeepSeek (where applicable). You can
    see that while on Tensor almost all completions are valid, this percentage falls
    to 78% for Arc and 86% for String; this is not surprising, given that the former
    are TensorFlow programs, which the model has seen during training, while the latter
    two are custom DSLs. Hence our non-strict mode proves especially helpful for low-resource
    domains, where otherwise we would have to discard a large proportion of completions.
    At the same time, we find that *given the same number of completions to learn
    from*, the PCFGs learned in non-strict mode are just as effective as those learned
    in strict mode: for example, \tool-Tensor with the guidance from 100 DeepSeek
    completions solves 67 tasks *in either mode* (with the difference that strict
    mode has to sample more completions to get 100 valid ones).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[3(d)](#S4.F3.sf4 "3(d) ‣ Figure 4 ‣ 4 Experiments and Results ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis") 显示了 Gpt4o 和 DeepSeek 生成的语法有效完成的百分比（如适用）。你可以看到，尽管在
    Tensor 上几乎所有完成都是有效的，但在 Arc 和 String 上，这一百分比降至 78% 和 86%；考虑到前者是 TensorFlow 程序，模型在训练期间见过这些程序，而后两者是自定义
    DSL，因此这并不令人惊讶。因此，我们的非严格模式对于低资源领域尤其有帮助，否则我们将不得不丢弃大量完成。同时，我们发现 *在相同数量的完成中*，非严格模式下学习到的
    PCFG 与严格模式下的 PCFG 同样有效：例如，\tool-Tensor 在 100 个 DeepSeek 完成的引导下可以解决 67 个任务 *无论哪种模式*（不同之处在于严格模式需要采样更多完成以获得
    100 个有效的）。'
- en: 4.3 Limitations
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 限制
- en: 'The main limitation of our hybrid approach *wrt.* to purely neural approaches
    is that is requires implementing a synthesizer for each DSL of interest; although
    we have shown that the same bottom-up search can be used across different domains,
    some implementation effort is still required. On the other hand, compared to purely
    symbolic approaches, our method requires sampling from an LLM, which is costly;
    additionally, the guidance provided by our approach is only as good as the LLM’s
    completions: if they contain many irrelevant operators, our guided search can
    be *slower* than unguided search. Finally, our experiments are subject to the
    usual threat that the LLMs might have seen our benchmarks in their training data;
    we do not consider it a major issue, however, given that our main result is the
    superior performance of guided search *relative* to using LLMs without search.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的混合方法相对于纯神经方法的主要限制在于，它需要为每个感兴趣的 DSL 实现一个合成器；尽管我们已经证明相同的自底向上搜索可以在不同领域中使用，但仍然需要一些实现工作。另一方面，与纯符号方法相比，我们的方法需要从
    LLM 中采样，这很昂贵；此外，我们的方法提供的指导仅与 LLM 的完成情况相当：如果它们包含许多无关的操作符，我们的指导搜索可能比无指导搜索*更慢*。最后，我们的实验面临
    LLM 可能在其训练数据中见过我们的基准的常见威胁；然而，考虑到我们的主要结果是指导搜索相对于不使用搜索的 LLM 的优越性能，我们不认为这是一个主要问题。
- en: 5 Related Work
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Guiding Program Synthesis with Probabilistic Models
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用概率模型指导程序合成
- en: The traditional approach to *program synthesis* is based on combinatorial search[[7](#bib.bib7)],
    augmented with pruning techniques based on program semantics [[39](#bib.bib39),
    [2](#bib.bib2), [6](#bib.bib6)]. To further speed up the search, researchers have
    proposed *guiding* the search with a learned probabilistic model. Most approaches
    to guided search use special-purpose models that have to be trained on a domain-specific
    corpus of programs[[26](#bib.bib26)] or PBE tasks[[9](#bib.bib9), [24](#bib.bib24),
    [32](#bib.bib32), [37](#bib.bib37)]. Although some of these models can be trained
    on synthetic data, the training process is still expensive and requires manual
    tuning, which makes it hard to apply these techniques to new domains.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的*程序合成*方法基于组合搜索[[7](#bib.bib7)]，并辅以基于程序语义的剪枝技术[[39](#bib.bib39), [2](#bib.bib2),
    [6](#bib.bib6)]。为了进一步加快搜索速度，研究人员提出了用学习的概率模型来指导搜索的方法。大多数指导搜索的方法使用专用模型，这些模型需要在特定领域的程序语料库[[26](#bib.bib26)]或
    PBE 任务[[9](#bib.bib9), [24](#bib.bib24), [32](#bib.bib32), [37](#bib.bib37)]上进行训练。虽然其中一些模型可以在合成数据上进行训练，但训练过程仍然昂贵且需要手动调整，这使得这些技术难以应用于新领域。
- en: 'With the advent of pretrained Large Language Models (LLMs), it seems only natural
    to use them to guide search-based program synthesis, thus alleviating the need
    for domain-specific training data. We are only aware of one other attempt to do
    this: concurrent work by Li et al. [[29](#bib.bib29)], which also extracts a PCFG
    from the LLM’s samples, similarly to \tool. An important difference is that they
    use the PCFG to guide *top-down* A* search, while we use it to guide *bottom-up*
    search, which is known to be more efficient (they also evaluate their tool on
    synthesis from logical formulas as opposed to PBE).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预训练大型语言模型（LLMs）的出现，利用它们来指导基于搜索的程序合成似乎是自然的，从而减轻了对领域特定训练数据的需求。我们仅知道另一个类似尝试：Li
    等人的并行工作 [[29](#bib.bib29)]，他们也从 LLM 的样本中提取了 PCFG，与 \tool 类似。一个重要的区别在于，他们使用 PCFG
    来指导*自顶向下* A* 搜索，而我们使用它来指导*自底向上* 搜索，后者已知更高效（他们还评估了他们的工具在逻辑公式合成而非 PBE 上的表现）。
- en: Solving the Abstraction and Reasoning Corpus
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决抽象和推理语料库
- en: All state-of-the-art solvers for this benchmark have relied on carefully curated
    DSLs for Arc  [[13](#bib.bib13), [43](#bib.bib43), [3](#bib.bib3), [27](#bib.bib27),
    [19](#bib.bib19)]. Xu et al. [[44](#bib.bib44)] proposed the DSL we extend in
    our approach, and the Object-Arc subset we evaluate on. Lei et al. [[27](#bib.bib27)]
    embed their DSL as a subset of PDDL and use a Generalized Planning (GP) algorithm
    as their search component. They have the current best performance on Object-Arc,
    however they encode more domain-knowledge in the form of preconditions and per-abstraction
    restrictions on filters and transforms, to make GP viable. Our approach does not
    require this additional information. [[3](#bib.bib3), [10](#bib.bib10)] use DreamCoder
    [[16](#bib.bib16)], to perform execution-guided search over a DSL for grid manipulations,
    however they only provide proof-of-concept evaluations. [[41](#bib.bib41), [38](#bib.bib38)]
    also use an LLM to generate code given the spec of the task. Both of these approaches
    interact with the model across several rounds, while our technique uses the suggestions
    from the LLM only as a starting point. Our technique also performs a complete
    search guided by the LLM distribution, enabled by the structure of our DSL, whereas
    previous approaches only consider code directly generated by the LLM.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所有针对这个基准测试的最先进求解器都依赖于精心策划的Arc DSL [[13](#bib.bib13), [43](#bib.bib43), [3](#bib.bib3),
    [27](#bib.bib27), [19](#bib.bib19)]。Xu等人[[44](#bib.bib44)] 提出了我们在方法中扩展的DSL，以及我们评估的Object-Arc子集。Lei等人[[27](#bib.bib27)]
    将他们的DSL嵌入PDDL的一个子集中，并使用广义规划（GP）算法作为他们的搜索组件。他们在Object-Arc上的性能最佳，但他们在过滤器和变换上编码了更多的领域知识，以使GP可行。我们的方法不需要这些额外的信息。[[3](#bib.bib3),
    [10](#bib.bib10)] 使用DreamCoder [[16](#bib.bib16)] 在DSL上进行执行指导的搜索以处理网格操作，但他们仅提供了概念验证评估。[[41](#bib.bib41),
    [38](#bib.bib38)] 也使用LLM根据任务规范生成代码。这两种方法都与模型进行多轮交互，而我们的方法仅将LLM的建议作为起点。我们的技术还通过LLM分布进行完整搜索，得益于我们DSL的结构，而之前的方法只考虑LLM直接生成的代码。
- en: 6 Conclusion and Future Work
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: Our approach introduces a robust technique for using both valid and invalid
    completions from an LLM to learn a surrogate model. By incorporating ungrammatical
    completions, we can extract useful insights that would otherwise be discarded.
    Overall, we provide an alternative to the conventional strategy of large-scale
    sampling from LLMs, proposing a more effective use of the available completions
    to guide the search process. An interesting future direction would be to guide
    search with a more expressive context-dependent surrogate model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法引入了一种强大的技术，利用LLM的有效和无效补全来学习替代模型。通过包含语法不正确的补全，我们可以提取出有用的见解，这些见解否则会被丢弃。总体而言，我们提供了一种替代传统大规模采样LLM的策略，提出了一种更有效地利用现有补全来引导搜索过程的方法。一个有趣的未来方向是用更具表达力的上下文依赖替代模型来引导搜索。
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'arc [2020] 2020. [Arc kaggle competition leaderboard](https://www.kaggle.com/competitions/abstraction-and-reasoning-challenge/leaderboard).
    Accessed: 2024-05-19.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: arc [2020] 2020. [Arc kaggle competition leaderboard](https://www.kaggle.com/competitions/abstraction-and-reasoning-challenge/leaderboard).
    访问日期：2024-05-19。
- en: Albarghouthi et al. [2013] Aws Albarghouthi, Sumit Gulwani, and Zachary Kincaid.
    2013. Recursive program synthesis. In *International Conference on Computer Aided
    Verification*, pages 934–950\. Springer.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Albarghouthi et al. [2013] Aws Albarghouthi, Sumit Gulwani, 和 Zachary Kincaid.
    2013. 递归程序合成。在 *International Conference on Computer Aided Verification*，第934–950页。Springer。
- en: 'Alford et al. [2022] Simon Alford, Anshula Gandhi, Akshay Rangamani, Andrzej
    Banburski, Tony Wang, Sylee Dandekar, John Chin, Tomaso Poggio, and Peter Chin.
    2022. Neural-guided, bidirectional program search for abstraction and reasoning.
    In *Complex Networks & Their Applications X: Volume 1, Proceedings of the Tenth
    International Conference on Complex Networks and Their Applications COMPLEX NETWORKS
    2021 10*, pages 657–668\. Springer.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alford et al. [2022] Simon Alford, Anshula Gandhi, Akshay Rangamani, Andrzej
    Banburski, Tony Wang, Sylee Dandekar, John Chin, Tomaso Poggio, 和 Peter Chin.
    2022. 神经指导的双向程序搜索用于抽象和推理。在 *Complex Networks & Their Applications X: Volume 1,
    Proceedings of the Tenth International Conference on Complex Networks and Their
    Applications COMPLEX NETWORKS 2021 10*，第657–668页。Springer。'
- en: Alur et al. [2013] Rajeev Alur, Rastislav Bodík, Garvit Juniwal, Milo M. K.
    Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama,
    Emina Torlak, and Abhishek Udupa. 2013. Syntax-guided synthesis. In *Formal Methods
    in Computer-Aided Design, FMCAD 2013*, pages 1–8.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alur et al. [2013] Rajeev Alur, Rastislav Bodík, Garvit Juniwal, Milo M. K.
    Martin, Mukund Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama,
    Emina Torlak, 和 Abhishek Udupa. 2013. 语法指导合成。在 *计算机辅助设计中的形式方法，FMCAD 2013*，页码 1–8。
- en: 'Alur et al. [2017a] Rajeev Alur, Dana Fisman, Rishabh Singh, and Armando Solar-Lezama.
    2017a. Sygus-comp 2017: Results and analysis. *arXiv preprint arXiv:1711.11438*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alur et al. [2017a] Rajeev Alur, Dana Fisman, Rishabh Singh, 和 Armando Solar-Lezama.
    2017a. Sygus-comp 2017: 结果与分析。*arXiv 预印本 arXiv:1711.11438*。'
- en: Alur et al. [2017b] Rajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. 2017b.
    Scaling enumerative program synthesis via divide and conquer. In *International
    Conference on Tools and Algorithms for the Construction and Analysis of Systems*,
    pages 319–336\. Springer.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alur et al. [2017b] Rajeev Alur, Arjun Radhakrishna, 和 Abhishek Udupa. 2017b.
    通过分治法扩展列举程序合成。在 *系统构建与分析工具国际会议*，页码 319–336。Springer。
- en: Alur et al. [2018] Rajeev Alur, Rishabh Singh, Dana Fisman, and Armando Solar-Lezama.
    2018. Search-based program synthesis. *Communications of the ACM*, 61(12):84–93.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alur et al. [2018] Rajeev Alur, Rishabh Singh, Dana Fisman, 和 Armando Solar-Lezama.
    2018. 基于搜索的程序合成。*ACM 通讯*，61(12):84–93。
- en: Bai et al. [2023] Xuefeng Bai, Jialong Wu, Yulong Chen, Zhongqing Wang, and
    Yue Zhang. 2023. Constituency parsing using llms. *arXiv preprint arXiv:2310.19462*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2023] Xuefeng Bai, Jialong Wu, Yulong Chen, Zhongqing Wang, 和 Yue
    Zhang. 2023. 使用 LLMS 的构 constituency 解析。*arXiv 预印本 arXiv:2310.19462*。
- en: 'Balog et al. [2016] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian
    Nowozin, and Daniel Tarlow. 2016. Deepcoder: Learning to write programs. *arXiv
    preprint arXiv:1611.01989*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Balog et al. [2016] Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian
    Nowozin, 和 Daniel Tarlow. 2016. Deepcoder: 学习编写程序。*arXiv 预印本 arXiv:1611.01989*。'
- en: Banburski et al. [2020] Andrzej Banburski, Anshula Gandhi, Simon Alford, Sylee
    Dandekar, Sang Chin, and tomaso a poggio. 2020. [Dreaming with ARC](https://openreview.net/forum?id=-gjy2V1ko6t).
    In *Learning Meets Combinatorial Algorithms at NeurIPS2020*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banburski et al. [2020] Andrzej Banburski, Anshula Gandhi, Simon Alford, Sylee
    Dandekar, Sang Chin, 和 tomaso a poggio. 2020. [与 ARC 共梦](https://openreview.net/forum?id=-gjy2V1ko6t)。在
    *NeurIPS2020 会议：学习与组合算法的结合*。
- en: Barke et al. [2020] Shraddha Barke, Hila Peleg, and Nadia Polikarpova. 2020.
    Just-in-time learning for bottom-up enumerative synthesis. *Proceedings of the
    ACM on Programming Languages*, 4(OOPSLA):1–29.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barke et al. [2020] Shraddha Barke, Hila Peleg, 和 Nadia Polikarpova. 2020. 适时学习用于自下而上的列举合成。*ACM
    编程语言会议论文集*，4(OOPSLA):1–29。
- en: 'Berglund et al. [2023] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni,
    Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse:
    Llms trained on" a is b" fail to learn" b is a". *arXiv preprint arXiv:2309.12288*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berglund et al. [2023] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni,
    Asa Cooper Stickland, Tomasz Korbak, 和 Owain Evans. 2023. 反转诅咒：训练于“a 是 b”的 LLMS
    无法学习“b 是 a”。*arXiv 预印本 arXiv:2309.12288*。
- en: 'Butt et al. [2023] Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone,
    David W Zhang, Michaël Defferrard, and Taco Cohen. 2023. Codeit: Abstract reasoning
    with iterative policy-guided program synthesis.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Butt et al. [2023] Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone,
    David W Zhang, Michaël Defferrard, 和 Taco Cohen. 2023. Codeit: 通过迭代策略引导的程序合成进行抽象推理。'
- en: Chollet [2019] François Chollet. 2019. On the measure of intelligence. *arXiv
    preprint arXiv:1911.01547*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet [2019] François Chollet. 2019. 关于智能的衡量。*arXiv 预印本 arXiv:1911.01547*。
- en: 'Devlin et al. [2017] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh
    Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017. Robustfill: Neural program
    learning under noisy i/o. In *International conference on machine learning*, pages
    990–998\. PMLR.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. [2017] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh
    Singh, Abdel-rahman Mohamed, 和 Pushmeet Kohli. 2017. Robustfill: 在噪声输入/输出下的神经程序学习。在
    *国际机器学习会议*，页码 990–998。PMLR。'
- en: 'Ellis et al. [2020] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer,
    Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum.
    2020. Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep
    bayesian program learning. *arXiv preprint arXiv:2006.08381*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ellis et al. [2020] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer,
    Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, 和 Joshua B Tenenbaum.
    2020. Dreamcoder: 通过觉醒-睡眠贝叶斯程序学习增长可泛化、可解释的知识。*arXiv 预印本 arXiv:2006.08381*。'
- en: Feng et al. [2018] Yu Feng, Ruben Martins, Osbert Bastani, and Isil Dillig.
    2018. [Program synthesis using conflict-driven learning](https://doi.org/10.1145/3192366.3192382).
    In *Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design
    and Implementation*, PLDI 2018, pages 420–435, New York, NY, USA. Association
    for Computing Machinery.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人 [2018] Yu Feng, Ruben Martins, Osbert Bastani 和 Isil Dillig. 2018. [使用冲突驱动学习的程序合成](https://doi.org/10.1145/3192366.3192382)。发表于*第39届
    ACM SIGPLAN 编程语言设计与实现会议论文集*，PLDI 2018，页码 420–435，美国纽约。计算机协会。
- en: Feng et al. [2017] Yu Feng, Ruben Martins, Yuepeng Wang, Isil Dillig, and Thomas W.
    Reps. 2017. Component-based synthesis for complex apis. In *POPL*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人 [2017] Yu Feng, Ruben Martins, Yuepeng Wang, Isil Dillig 和 Thomas W.
    Reps. 2017. 基于组件的复杂 API 合成。发表于*POPL*。
- en: Fischer et al. [2020] Raphael Fischer, Matthias Jakobs, Sascha Mücke, and Katharina
    Morik. 2020. Solving abstract reasoning tasks with grammatical evolution. In *LWDA*,
    pages 6–10.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischer 等人 [2020] Raphael Fischer, Matthias Jakobs, Sascha Mücke 和 Katharina
    Morik. 2020. 使用语法演化解决抽象推理任务。发表于*LWDA*，页码 6–10。
- en: Gulwani [2011] Sumit Gulwani. 2011. Automating string processing in spreadsheets
    using input-output examples. *ACM Sigplan Notices*, 46(1):317–330.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulwani [2011] Sumit Gulwani. 2011. 使用输入输出示例自动化电子表格中的字符串处理。*ACM Sigplan Notices*，46(1)：317–330。
- en: Gulwani [2016] Sumit Gulwani. 2016. Programming by examples (and its applications
    in data wrangling). In Javier Esparza, Orna Grumberg, and Salomon Sickert, editors,
    *Verification and Synthesis of Correct and Secure Systems*. IOS Press.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gulwani [2016] Sumit Gulwani. 2016. 通过示例编程（及其在数据整理中的应用）。在 Javier Esparza, Orna
    Grumberg 和 Salomon Sickert 编辑的 *正确和安全系统的验证与合成* 中。IOS出版社。
- en: 'Guo et al. [2024] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024. Deepseek-coder: When
    the large language model meets programming–the rise of code intelligence. *arXiv
    preprint arXiv:2401.14196*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2024] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li 等人. 2024. Deepseek-coder：当大型语言模型遇到编程——代码智能的崛起。*arXiv
    预印本 arXiv:2401.14196*。
- en: 'Josifoski et al. [2023] Martin Josifoski, Marija Sakota, Maxime Peyrard, and
    Robert West. 2023. Exploiting asymmetry for synthetic training data generation:
    Synthie and the case of information extraction. *arXiv preprint arXiv:2303.04132*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Josifoski 等人 [2023] Martin Josifoski, Marija Sakota, Maxime Peyrard 和 Robert
    West. 2023. 利用不对称性进行合成训练数据生成：Synthie 和信息提取的案例。*arXiv 预印本 arXiv:2303.04132*。
- en: Kalyan et al. [2018] Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv
    Batra, Prateek Jain, and Sumit Gulwani. 2018. Neural-guided deductive search for
    real-time program synthesis from examples. *arXiv preprint arXiv:1804.01186*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalyan 等人 [2018] Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra,
    Prateek Jain 和 Sumit Gulwani. 2018. 神经引导的演绎搜索用于实时程序合成。*arXiv 预印本 arXiv:1804.01186*。
- en: 'Lee et al. [2024] Seungpil Lee, Woochang Sim, Donghyeon Shin, Sanha Hwang,
    Wongyu Seo, Jiwon Park, Seokki Lee, Sejin Kim, and Sundong Kim. 2024. [Reasoning
    abilities of large language models: In-depth analysis on the abstraction and reasoning
    corpus](http://arxiv.org/abs/2403.11793).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2024] Seungpil Lee, Woochang Sim, Donghyeon Shin, Sanha Hwang, Wongyu
    Seo, Jiwon Park, Seokki Lee, Sejin Kim 和 Sundong Kim. 2024. [大型语言模型的推理能力：对抽象和推理语料库的深入分析](http://arxiv.org/abs/2403.11793)。
- en: Lee et al. [2018] Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. 2018.
    Accelerating search-based program synthesis using learned probabilistic models.
    *ACM SIGPLAN Notices*, 53(4):436–449.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2018] Woosuk Lee, Kihong Heo, Rajeev Alur 和 Mayur Naik. 2018. 使用学习的概率模型加速基于搜索的程序合成。*ACM
    SIGPLAN Notices*，53(4)：436–449。
- en: Lei et al. [2024] Chao Lei, Nir Lipovetzky, and Krista A. Ehinger. 2024. [Generalized
    planning for the abstraction and reasoning corpus](http://arxiv.org/abs/2401.07426).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等人 [2024] Chao Lei, Nir Lipovetzky 和 Krista A. Ehinger. 2024. [针对抽象和推理语料库的广义规划](http://arxiv.org/abs/2401.07426)。
- en: Li et al. [2024a] Xiang Li, Xiangyu Zhou, Rui Dong, Yihong Zhang, and Xinyu
    Wang. 2024a. [Efficient bottom-up synthesis for programs with local variables](https://doi.org/10.1145/3632894).
    *Proc. ACM Program. Lang.*, 8(POPL).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2024a] Xiang Li, Xiangyu Zhou, Rui Dong, Yihong Zhang 和 Xinyu Wang. 2024a.
    [具有局部变量的程序的高效自下而上的合成](https://doi.org/10.1145/3632894)。*Proc. ACM Program. Lang.*，8(POPL)。
- en: Li et al. [2024b] Yixuan Li, Julian Parsert, and Elizabeth Polgreen. 2024b.
    [Guiding enumerative program synthesis with large language models](http://arxiv.org/abs/2403.03997).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2024b] Yixuan Li, Julian Parsert 和 Elizabeth Polgreen. 2024b. [利用大型语言模型指导枚举程序合成](http://arxiv.org/abs/2403.03997)。
- en: McCarthy [1960] John McCarthy. 1960. [Recursive functions of symbolic expressions
    and their computation by machine, part i](https://doi.org/10.1145/367177.367199).
    *Commun. ACM*, 3(4):184–195.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCarthy [1960] John McCarthy. 1960. [符号表达式的递归函数及其通过机器计算，第一部分](https://doi.org/10.1145/367177.367199)。*Commun.
    ACM*, 3(4):184–195。
- en: 'McCoy et al. [2023] R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy,
    and Thomas L Griffiths. 2023. Embers of autoregression: Understanding large language
    models through the problem they are trained to solve. *arXiv preprint arXiv:2309.13638*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCoy et al. [2023] R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy,
    和 Thomas L Griffiths. 2023. 自回归的余烬：通过问题理解大型语言模型。*arXiv 预印本 arXiv:2309.13638*。
- en: 'Odena et al. [2020] Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh,
    Charles Sutton, and Hanjun Dai. 2020. Bustle: bottom-up program synthesis through
    learning-guided exploration. *arXiv preprint arXiv:2007.14381*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Odena et al. [2020] Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh,
    Charles Sutton, 和 Hanjun Dai. 2020. Bustle：通过学习指导探索的自底向上程序合成。*arXiv 预印本 arXiv:2007.14381*。
- en: 'OpenAI [2024] OpenAI. 2024. [Hello gpt-4.0](https://openai.com/index/hello-gpt-4o/).
    Accessed: 2024-05-19.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024] OpenAI. 2024. [Hello gpt-4.0](https://openai.com/index/hello-gpt-4o/)。访问日期：2024-05-19。
- en: Osera and Zdancewic [2015] Peter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed
    program synthesis. *ACM SIGPLAN Notices*, 50(6):619–630.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osera and Zdancewic [2015] Peter-Michael Osera 和 Steve Zdancewic. 2015. 类型和示例引导的程序合成。*ACM
    SIGPLAN Notices*, 50(6):619–630。
- en: 'Reynolds et al. [2019] Andrew Reynolds, Haniel Barbosa, Andres Nötzli, Clark
    Barrett, and Cesare Tinelli. 2019. cvc 4 sy: smart and fast term enumeration for
    syntax-guided synthesis. In *International Conference on Computer Aided Verification*,
    pages 74–83\. Springer.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reynolds et al. [2019] Andrew Reynolds, Haniel Barbosa, Andres Nötzli, Clark
    Barrett, 和 Cesare Tinelli. 2019. cvc 4 sy：用于语法引导合成的智能和快速术语枚举。在 *国际计算机辅助验证会议*，第
    74–83 页。Springer。
- en: 'Shi et al. [2022a] Kensen Shi, David Bieber, and Rishabh Singh. 2022a. [Tf-coder:
    Program synthesis for tensor manipulations](https://doi.org/10.1145/3517034).
    *ACM Trans. Program. Lang. Syst.*, 44(2).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. [2022a] Kensen Shi, David Bieber, 和 Rishabh Singh. 2022a. [Tf-coder：张量操作的程序合成](https://doi.org/10.1145/3517034)。*ACM
    Trans. Program. Lang. Syst.*, 44(2)。
- en: 'Shi et al. [2022b] Kensen Shi, Hanjun Dai, Kevin Ellis, and Charles Sutton.
    2022b. Crossbeam: Learning to search in bottom-up program synthesis. *arXiv preprint
    arXiv:2203.10452*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. [2022b] Kensen Shi, Hanjun Dai, Kevin Ellis, 和 Charles Sutton. 2022b.
    Crossbeam：学习在自底向上的程序合成中进行搜索。*arXiv 预印本 arXiv:2203.10452*。
- en: 'Tan and Motani [2023] John Chong Min Tan and Mehul Motani. 2023. Large language
    model (llm) as a system of multiple expert agents: An approach to solve the abstraction
    and reasoning corpus (arc) challenge. *arXiv preprint arXiv:2310.05146*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan and Motani [2023] John Chong Min Tan 和 Mehul Motani. 2023. 大型语言模型（llm）作为多个专家代理的系统：解决抽象和推理语料库（arc）挑战的方法。*arXiv
    预印本 arXiv:2310.05146*。
- en: 'Udupa et al. [2013] Abhishek Udupa, Arun Raghavan, Jyotirmoy V Deshmukh, Sela
    Mador-Haim, Milo MK Martin, and Rajeev Alur. 2013. Transit: specifying protocols
    with concolic snippets. *ACM SIGPLAN Notices*, 48(6):287–296.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Udupa et al. [2013] Abhishek Udupa, Arun Raghavan, Jyotirmoy V Deshmukh, Sela
    Mador-Haim, Milo MK Martin, 和 Rajeev Alur. 2013. Transit：通过同态片段指定协议。*ACM SIGPLAN
    Notices*, 48(6):287–296。
- en: Ugare et al. [2024] Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic,
    and Gagandeep Singh. 2024. Improving llm code generation with grammar augmentation.
    *arXiv preprint arXiv:2403.01632*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ugare et al. [2024] Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic,
    和 Gagandeep Singh. 2024. 通过语法增强改进 llm 代码生成。*arXiv 预印本 arXiv:2403.01632*。
- en: 'Wang et al. [2023] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu,
    Nick Haber, and Noah D Goodman. 2023. Hypothesis search: Inductive reasoning with
    language models. *arXiv preprint arXiv:2309.05660*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick
    Haber, 和 Noah D Goodman. 2023. 假设搜索：与语言模型的归纳推理。*arXiv 预印本 arXiv:2309.05660*。
- en: Wen et al. [2024] Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski,
    Swarat Chaudhuri, and Alex Polozov. 2024. [Grounding data science code generation
    with input-output specifications](http://arxiv.org/abs/2402.08073).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. [2024] Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski,
    Swarat Chaudhuri, and Alex Polozov. 2024. [基于输入输出规范的数据科学代码生成](http://arxiv.org/abs/2402.08073)。
- en: 'Wind [2020] Johan Sokrates Wind. 2020. [Arc kaggle competition, 1st place](https://github.com/top-quarks/ARC-solution).
    Accessed: 2024-05-19.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wind [2020] Johan Sokrates Wind. 2020. [Arc kaggle 竞赛，第 1 名](https://github.com/top-quarks/ARC-solution)。访问日期：2024-05-19。
- en: Xu et al. [2023a] Yudong Xu, Elias B Khalil, and Scott Sanner. 2023a. Graphs,
    constraints, and search for the abstraction and reasoning corpus. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 37, pages 4115–4122.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023a] Yudong Xu, Elias B Khalil, 和 Scott Sanner. 2023a. 图、约束与抽象和推理语料库的搜索。发表于*AAAI人工智能会议论文集*，第37卷，第4115–4122页。
- en: 'Xu et al. [2023b] Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner,
    and Elias B Khalil. 2023b. Llms and the abstraction and reasoning corpus: Successes,
    failures, and the importance of object-based representations. *arXiv preprint
    arXiv:2305.18354*.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023b] Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, 和
    Elias B Khalil. 2023b. Llms和抽象与推理语料库：成功、失败以及基于对象的表示的重要性。*arXiv预印本arXiv:2305.18354*。
- en: Zhang et al. [2023] Honghua Zhang, Meihua Dang, Nanyun Peng, and Guy Van den
    Broeck. 2023. Tractable control for autoregressive language generation. In *International
    Conference on Machine Learning*, pages 40932–40945\. PMLR.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2023] Honghua Zhang, Meihua Dang, Nanyun Peng, 和 Guy Van den Broeck.
    2023. 自回归语言生成的可处理控制。发表于*国际机器学习会议*，第40932–40945页。PMLR。
- en: Appendix A GPT4o Solutions for the Motivating Example
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A GPT4o解决方案用于激励示例。
- en: //  Solution  1,  occurs  6  timesif  color_of(self)  $=$  BLUE  then  update_color(ORANGE)  ;if  size_of(self)  $=$  YELLOW  then  update_color(CYAN)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: //  解决方案1，出现6次。如果color_of(self) $=$ BLUE，则update_color(ORANGE)；如果size_of(self)
    $=$ YELLOW，则update_color(CYAN)。
- en: 'Figure 5: Ten samples from GPT4o for the motivating example in [1(a)](#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for
    Guiding Program Synthesis")'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '图5：来自GPT4o的10个样本，用于[1(a)](#S1.F1.sf1 "1(a) ‣ 图1 ‣ 1 引言 ‣ \tool: 无上下文LLM近似引导程序合成")的激励示例。'
- en: 'Recall the motivating example in [1(a)](#S1.F1.sf1 "1(a) ‣ Figure 1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis") where
    the task is to update the color of the grey objects to the color of their single-pixel
    neighbor. As a reminder, the smallest correct solution to this task consists of
    the following rule:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '回顾[1(a)](#S1.F1.sf1 "1(a) ‣ 图1 ‣ 1 引言 ‣ \tool: 无上下文LLM近似引导程序合成")中的激励示例，其中任务是将灰色对象的颜色更新为其单像素邻居的颜色。作为提醒，这个任务的最小正确解决方案包含以下规则：'
- en: if  color_of(self)  $=$  MINthen  update_color(color_of(x))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果color_of(self) $=$ MIN，则update_color(color_of(x))
- en: '[Fig. 5](#A1.F5 "Figure 5 ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") shows the programs we obtained
    by deduplicating 10 samples from GPT4o for this task. The syntax of the solutions
    is slightly modified for readability; our implementation uses a LISP-style s-expression
    syntax[[30](#bib.bib30)] to simplify parsing.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5](#A1.F5 "图5 ‣ 附录A GPT4o解决方案用于激励示例 ‣ 无上下文语法 ‣ 2.1 例程编程 ‣ 2 背景 ‣ \tool: 无上下文LLM近似引导程序合成")显示了我们通过去重10个GPT4o样本获得的程序。解决方案的语法稍作修改以提高可读性；我们的实现使用了LISP风格的s表达式语法[[30](#bib.bib30)]来简化解析。'
- en: As you can see, the most frequent solution is almost correct, except that is
    does not constrain the neighbor other to be of size 1; this leads to the constraint
    being ambiguous (since every gray object has multiple neighbors of different colors),
    in which case the program semantics is considered undefined. That said, you can
    observe that the model consistently uses relevant components, such as color_of,
    is_neighbor, and update_color, which enables us to extract a useful PCFG from
    these solutions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，最频繁的解决方案几乎是正确的，只是它没有限制邻居的大小为1；这导致约束条件模糊（因为每个灰色对象有多个不同颜色的邻居），在这种情况下，程序的语义被认为是未定义的。不过，你可以观察到模型一致地使用了相关组件，例如color_of、is_neighbor和update_color，这使我们能够从这些解决方案中提取出有用的PCFG。
- en: 'When we increased the sample size to 125, GPT4o was able to produce one correct
    solution (which is slightly larger than the minimal solution above):'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将样本量增加到125时，GPT4o能够产生一个正确的解决方案（比上面提到的最小解决方案稍大）：
- en: if  color_of(self)  $=$  GREY)then  update_color(color_of(other))
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果color_of(self) $=$ GREY，则update_color(color_of(other))
- en: Appendix B LLM Prompt for the Motivating Example
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B LLM提示词用于激励示例。
- en: B.1 System Prompt
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 系统提示词
- en: 'The system prompt given to the LLM for Arc domain is shown in [Fig. 6](#A2.F6
    "Figure 6 ‣ B.1 System Prompt ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis").'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '提供给LLM的Arc领域系统提示见[图6](#A2.F6 "Figure 6 ‣ B.1 System Prompt ‣ Appendix B LLM
    Prompt for the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating
    Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣
    \tool: Context-Free LLM Approximation for Guiding Program Synthesis")。'
- en: You  are  an  assistant  chatbot  with  human-like  perception,  reasoning  and  learning  capabilities.You  can  solve  tasks  concisely,  efficiently,  and  moreover,  correctly.Let’s  engage  in  perception-  and  logic-based  tasks.You  only  output  source  code.No  explanations  or  any  other  text.Only  code.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个具有人类般感知、推理和学习能力的助理聊天机器人。你能够简洁、高效且正确地解决任务。让我们进行基于感知和逻辑的任务。你只输出源代码。不提供解释或其他文本。仅仅是代码。
- en: 'Figure 6: System prompt for Arc domain.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Arc领域的系统提示。
- en: B.2 User Prompt
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 用户提示
- en: 'The full user prompt for the Arc domain is shown in [Fig. 7](#A2.F7 "Figure
    7 ‣ B.2 User Prompt ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix
    A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").
    It contains the domain-specific language, four in-context examples and the query
    for the test task.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'Arc领域的完整用户提示见[图7](#A2.F7 "Figure 7 ‣ B.2 User Prompt ‣ Appendix B LLM Prompt
    for the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis")。它包含了特定领域的语言、四个上下文示例以及测试任务的查询。'
- en: 'You  are  an  efficient  assistant  for  logical  reasoning  and  code  generation.You  will  help  me  solve  a  visual  perception  and  reasoning  task.I  will  first  provide  you  with  the  definition  of  a  Domain  Specific  Language  you  will  use  for  writing  a  solution  for  the  task.I  will  then  present  you  with  the  description  of  the  task  that  you  will  be  tested  in.You  will  then  respond  the  queries  I  make  regarding  the  solution  of  the  task.This  is  the  definition  of  the  DSL  you  will  use  to  solve  the  task.It  is  given  as  a  context-free  grammar  in  the  EBNF  format  used  by  the  Lark  parser  generator,  with  some  informative  comments  about  the  semantics.You  will  return  a  string  that  is  parseable  by  the  ‘program‘  non-terminal  of  the  grammar.‘‘‘library:  "("  program*  ")"//  Rules  are  executed  one  after  another,  in  the  order  they  appear.//  There  could  be  no  rules,  in  which  case  the  program  does  nothing.program:  "("  "do"  rule*  ")"...$<$‘‘‘Now  we  continue  with  the  visual  perception  and  reasoning  task.The  input  for  the  task  is  a  small  number  of  pairs  of  grids  of  characters.The  value  of  each  of  the  cells  of  the  grids  are  the  colors  defined  in  the  DSL,  so  we  can  think  of  grids  as  images.Each  pair  of  images  correspond  to  an  input-output  example  for  an  unknown  program  P.For  each  pair,  the  program  P  is  evaluated  on  the  image  grid  and  operates  on  the  objects  that  appear  in  it.The  output  of  the  program  is  then  the  output  image.The  objects  in  the  images  are  easy  and  natural  to  identify  for  humans,  so  there  is  no  need  to  define  them  explicitly.However  you  are  able  to  abstract  them  correctly,  and  the  DSL  is  interpreted  with  the  same  correct  abstraction.Now  I  will  show  you  some  demonstration  tasks  along  with  the  output  you  would  be  expected  to  produce  for  each  of  them.##  DEMONSTRATION  TASK  1###  INPUTPAIR  1INPUT  GRID:O  O  O  O  O  O  O  OO  O  O  O  O  R  O  OO  R  O  O  O  R  O  RO  R  R  O  O  R  O  OO  O  O  O  O  O  O  OO  R  R  O  O  O  O  OO  R  R  O  R  R  O  OO  O  O  O  O  O  O  O'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '你是一个高效的逻辑推理和代码生成助手。你将帮助我解决一个视觉感知和推理任务。我将首先向你提供定义领域特定语言的定义，然后呈现你将接受测试的任务描述。接着，你将回应我对任务解决方案的查询。这是你将用来解决任务的DSL定义。它以Lark解析器生成器使用的EBNF格式的上下文无关文法给出，并附有关于语义的一些信息性注释。你将返回一个可以被语法的‘program‘非终结符解析的字符串。‘‘‘library:  "("  program*  ")"//  规则按照出现的顺序一个接一个地执行。//  规则可以为空，这种情况下程序不做任何操作。program:  "("  "do"  rule*  ")"...$<$‘‘‘现在我们继续进行视觉感知和推理任务。任务的输入是一小对字符网格。每个网格单元的值是DSL中定义的颜色，因此我们可以将网格视为图像。每对图像对应于一个未知程序P的输入输出示例。对于每一对，程序P在图像网格上进行评估，并操作其中出现的对象。程序的输出即为输出图像。图像中的对象对于人类来说容易且自然地识别，因此不需要明确定义。然而你能够正确地抽象它们，并且DSL被解释为相同的正确抽象。现在我将展示一些示范任务及你预计为每个任务产生的输出。##
    示范任务 1### 输入对 1输入网格：O  O  O  O  O  O  O  OO  O  O  O  O  R  O  OO  R  O  O  O  R  O  RO  R  R  O  O  R  O  OO  O  O  O  O  O  O  OO  R  R  O  O  O  O  OO  R  R  O  R  R  O  OO  O  O  O  O  O  O  O'
- en: 'Figure 7: User prompt for Arc domain.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Arc领域的用户提示。
- en: OUTPUT  GRID:O  O  O  O  O  O  O  OO  O  O  O  O  Y  O  OO  Y  O  O  O  Y  O  YO  Y  Y  O  O  Y  O  OO  O  O  O  O  O  O  OO  Y  Y  O  O  O  O  OO  Y  Y  O  Y  Y  O  OO  O  O  O  O  O  O  O$<$
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 输出网格：O  O  O  O  O  O  O  OO  O  O  O  O  Y  O  OO  Y  O  O  O  Y  O  YO  Y  Y  O  O  Y  O  OO  O  O  O  O  O  O  OO  Y  Y  O  O  O  O  OO  Y  Y  O  Y  Y  O  OO  O  O  O  O  O  O  O$$
- en: Appendix C Different sample sizes ablation for Arc, Tensor and String domains
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C Arc、Tensor和String领域的不同样本大小消融
- en: '![Refer to caption](img/86cfef06e90217c53c092ca598ee56dd.png)![Refer to caption](img/75df365f95cefe4004015bc3cd0c0c94.png)![Refer
    to caption](img/5bfb8ec75bdd6435d81bbaf265d385ea.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/86cfef06e90217c53c092ca598ee56dd.png)![参考标题](img/75df365f95cefe4004015bc3cd0c0c94.png)![参考标题](img/5bfb8ec75bdd6435d81bbaf265d385ea.png)'
- en: 'Figure 8: \tool-Arc, \tool-Tensor and \tool-String results guided by a PCFG
    learned from different number of Gpt4o samples (n=10, 20, 50, 100).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：\tool-Arc、\tool-Tensor和\tool-String结果，基于从不同数量的Gpt4o样本（n=10, 20, 50, 100）学习到的PCFG。
- en: Appendix D Experimental results with LLMs DeepSeek and Gpt3.5
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 使用LLMs DeepSeek和Gpt3.5的实验结果
- en: '![Refer to caption](img/53f62fdf19f62786669c439592aa435a.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/53f62fdf19f62786669c439592aa435a.png)'
- en: (a) \tool-String results with DeepSeek
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: (a) \tool-String 使用 DeepSeek 的结果
- en: '![Refer to caption](img/657a302b1624b670609fd128c2ce5ad7.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/657a302b1624b670609fd128c2ce5ad7.png)'
- en: (b) \tool-String results with Gpt3.5
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: (b) \tool-String 使用 Gpt3.5 的结果
- en: '![Refer to caption](img/79469f9a9dfff42818d73c99028c593e.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/79469f9a9dfff42818d73c99028c593e.png)'
- en: (c) \tool-Tensor results with DeepSeek
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: (c) \tool-Tensor 使用 DeepSeek 的结果
- en: '![Refer to caption](img/e17b0b32bcdb340cd269cb3f8417185c.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e17b0b32bcdb340cd269cb3f8417185c.png)'
- en: (d) \tool-Tensor results with Gpt3.5
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: (d) \tool-Tensor 使用 Gpt3.5 的结果
- en: 'Figure 9: \tool-String and \tool-Tensor evaluation results with DeepSeek and
    Gpt3.5.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: \tool-String 和 \tool-Tensor 使用 DeepSeek 和 Gpt3.5 的评估结果'
- en: Appendix E LLM Prompt for the Tensor Grammar
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E LLM Tensor 语法提示
- en: 'The system and user prompt for Tensor domain are in [Fig. 10](#A5.F10 "Figure
    10 ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") and [Fig. 11](#A5.F11 "Figure 11 ‣ Appendix E
    LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results with LLMs
    DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for Arc, Tensor
    and String domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix
    A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tensor 领域的系统和用户提示见 [图 10](#A5.F10 "图 10 ‣ 附录 E LLM Tensor 语法提示 ‣ 附录 D LLMs
    DeepSeek 和 Gpt3.5 的实验结果 ‣ 附录 C Arc、Tensor 和 String 领域不同样本大小的消融实验 ‣ 附录 B 激励示例的
    LLM 提示 ‣ 附录 A 激励示例的 GPT4o 解决方案 ‣ 上下文无关文法 ‣ 2.1 例子驱动编程 ‣ 2 背景 ‣ \tool: 上下文无关 LLM
    近似指导程序合成") 和 [图 11](#A5.F11 "图 11 ‣ 附录 E LLM Tensor 语法提示 ‣ 附录 D LLMs DeepSeek
    和 Gpt3.5 的实验结果 ‣ 附录 C Arc、Tensor 和 String 领域不同样本大小的消融实验 ‣ 附录 B 激励示例的 LLM 提示 ‣
    附录 A 激励示例的 GPT4o 解决方案 ‣ 上下文无关文法 ‣ 2.1 例子驱动编程 ‣ 2 背景 ‣ \tool: 上下文无关 LLM 近似指导程序合成")。'
- en: You  are  a  coding  assistant.  Be  precise  and  terse.You  will  be  provided  a  list  of  tensorflow  operators,  a  task  description,  and  some  input/output  examples.Your  task  is  to  generate  the  body  of  a  python  function  that  will  transform  the  input  to  the  output.Only  use  the  operators  provided  in  the  list.Your  answer  should  be  as  short  as  possible  while  still  being  correct.Make  sure  to  only  generate  python  code.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个编码助手。请精确简洁。你将获得一个 TensorFlow 操作符列表、一个任务描述和一些输入/输出示例。你的任务是生成一个 Python 函数的主体，该函数将输入转换为输出。仅使用列表中提供的操作符。你的回答应该尽可能简短，同时确保正确。请确保仅生成
    Python 代码。
- en: 'Figure 10: System prompt for Tensor domain.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: Tensor 领域的系统提示'
- en: '[TENSORFLOW  OPERATORS]$<$[TASK  DESCRIPTION]index  into  the  tensor[INPUTS][[  5.  2.][  1.  3.][  0.  -1.]][OUTPUTS][[[  5.  5.][  1.  1.][  0.  0.]][[  2.  2.][  3.  3.][-1.  -1.]]][PROGRAM]def  transform(in1):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[TENSORFLOW  OPERATORS]$<$[TASK  DESCRIPTION]索引到张量[INPUTS][[  5.  2.][  1.  3.][  0.  -1.]][OUTPUTS][[[  5.  5.][  1.  1.][  0.  0.]][[  2.  2.][  3.  3.][-1.  -1.]]][PROGRAM]def  transform(in1):'
- en: 'Figure 11: User prompt for Tensor domain'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: Tensor 领域的用户提示'
- en: Appendix F LLM Prompt for String
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F LLM 字符串提示
- en: 'The system and user prompt for String domain are in [Fig. 10](#A5.F10 "Figure
    10 ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") and [Fig. 11](#A5.F11 "Figure 11 ‣ Appendix E
    LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results with LLMs
    DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for Arc, Tensor
    and String domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix
    A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '字符串领域的系统和用户提示见 [图 10](#A5.F10 "图 10 ‣ 附录 E 张量语法的 LLM 提示 ‣ 附录 D LLMs DeepSeek
    和 Gpt3.5 的实验结果 ‣ 附录 C Arc、张量和字符串领域的不同样本大小消融 ‣ 附录 B 激励示例的 LLM 提示 ‣ 附录 A GPT4o 对激励示例的解决方案
    ‣ 上下文无关语法 ‣ 2.1 示例编程 ‣ 2 背景 ‣ \tool: 指导程序合成的上下文无关 LLM 近似") 和 [图 11](#A5.F11 "图
    11 ‣ 附录 E 张量语法的 LLM 提示 ‣ 附录 D LLMs DeepSeek 和 Gpt3.5 的实验结果 ‣ 附录 C Arc、张量和字符串领域的不同样本大小消融
    ‣ 附录 B 激励示例的 LLM 提示 ‣ 附录 A GPT4o 对激励示例的解决方案 ‣ 上下文无关语法 ‣ 2.1 示例编程 ‣ 2 背景 ‣ \tool:
    指导程序合成的上下文无关 LLM 近似")。'
- en: You  are  a  coding  assistant.  Be  precise  and  terse.You  will  be  given  a  SyGuS  grammar,  a  natural  language  specification,  and  a  set  of  input-output  examples.Your  task  is  to  complete  the  provided  function  definition  with  an  implementation  that  is  correct  according  to  the  grammar,  specification,  and  examples.Your  answer  should  be  as  short  as  possible  while  still  being  correct.Make  sure  that  your  answer  is  a  valid  s-expression.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个编码助手。要准确简洁。你将获得一个 SyGuS 语法，一个自然语言规范，以及一组输入输出示例。你的任务是根据语法、规范和示例完成所提供的函数定义。你的答案应尽可能简短，同时仍然正确。确保你的答案是一个有效的
    s 表达式。
- en: 'Figure 12: System prompt for String domain'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 字符串领域的系统提示'
- en: '[GRAMMAR](synth-fun  f  ((_arg_0  String))  String  ((Start  String  (ntString))  (ntString  String  (_arg_0  ""  "  "  "BRD"  "DRS"  "LDS"  "Branding"  "Direct  Response"  "Leads"  "="  "/"  "in"  "_"  "9"  "."  "microsoft"  "windows"  "apple"  "mac"  "-"  "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "0"  ","  "$<$  uk[SOLUTION](define-fun  f  (_arg_0  String)  String'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[语法](synth-fun  f  ((_arg_0  String))  String  ((Start  String  (ntString))  (ntString  String  (_arg_0  ""  "  "  "BRD"  "DRS"  "LDS"  "Branding"  "Direct  Response"  "Leads"  "="  "/"  "in"  "_"  "9"  "."  "microsoft"  "windows"  "apple"  "mac"  "-"  "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "0"  ","  "$<$  uk[SOLUTION](define-fun  f  (_arg_0  String)  String'
- en: 'Figure 13: User message for String'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 字符串的用户消息'
- en: Appendix G The Full String Grammar
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 完整的字符串语法
- en: '|  | $\displaystyle\mathit{Start}$ |  |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{Start}$ |  |  |'
- en: '|  | $\displaystyle S$ | string variables |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S$ | 字符串变量 |  |'
- en: '|  |  | $\displaystyle\mid$ | string literals |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | 字符串文字 |  |'
- en: '|  |  | $\displaystyle\mid$ | replace s x y replaces first occurrence of x
    in s with y |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | replace s x y 将 s 中第一个 x 替换为 y |  |'
- en: '|  |  | $\displaystyle\mid$ | concat x y concatenates x and y |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | concat x y 连接 x 和 y |  |'
- en: '|  |  | $\displaystyle\mid$ | substr x y z extracts substring of length z,
    from index y |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | substr x y z 从索引 y 提取长度为 z 的子串 |  |'
- en: '|  |  | $\displaystyle\mid$ | ite x y z returns y if x is true, otherwise z
    |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | ite x y z 如果 x 为真则返回 y，否则返回 z |  |'
- en: '|  |  | $\displaystyle\mid$ | int.to.str x converts int x to a string |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | int.to.str x 将整型 x 转换为字符串 |  |'
- en: '|  |  | $\displaystyle\mid$ | at x y returns the character at index y in string
    x |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | at x y 返回字符串 x 中索引 y 处的字符 |  |'
- en: '|  | $\displaystyle B$ | bool literals |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B$ | 布尔文字 |  |'
- en: '|  |  | $\displaystyle\mid$ | = x y returns true if x equals y |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | = x y 如果 x 等于 y 则返回真 |  |'
- en: '|  |  | $\displaystyle\mid$ | contains x y returns true if x contains y |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | contains x y 如果 x 包含 y 返回真 |  |'
- en: '|  |  | $\displaystyle\mid$ | suffixof x y returns true if x is the suffix
    of y |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | suffixof x y 如果 x 是 y 的后缀则返回真 |  |'
- en: '|  |  | $\displaystyle\mid$ | prefixof x y returns true if x is the prefix
    of y |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | prefixof x y 如果 x 是 y 的前缀则返回真 |  |'
- en: '|  | $\displaystyle I$ | int variables |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I$ | 整型变量 |  |'
- en: '|  |  | $\displaystyle\mid$ | int literals |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | 整数文字 |  |'
- en: '|  |  | $\displaystyle\mid$ | str.to.int x converts string x to a int |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | str.to.int x 将字符串 x 转换为整数 |  |'
- en: '|  |  | $\displaystyle\mid$ | + x y sums x and y |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | + x y 对 x 和 y 求和 |  |'
- en: '|  |  | $\displaystyle\mid$ | - x y subtracts y from x |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | - x y 从 x 中减去 y |  |'
- en: '|  |  | $\displaystyle\mid$ | length x returns length of x |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | length x 返回 x 的长度 |  |'
- en: '|  |  | $\displaystyle\mid$ | ite x y z returns y if x is true, otherwise z
    |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | ite x y z 如果 x 为真返回 y，否则返回 z |  |'
- en: '|  |  | $\displaystyle\mid$ | indexof x y z returns index of y in x, starting
    at index z |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | indexof x y z 返回 x 中 y 的索引，从索引 z 开始 |  |'
- en: 'Figure 14: The full SyGuS String grammar of the Probe benchmark suite. Integer
    and string variables and constants change per benchmark. Some benchmark files
    contain a reduced grammar.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: Probe 基准套件的完整 SyGuS 字符串语法。每个基准的整数和字符串变量及常量不同。某些基准文件包含简化语法。'
- en: Appendix H The Full Arc DSL
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H Arc DSL 完整语法
- en: 'The full grammar of our Arc DSL is shown in [Fig. 15](#A8.F15 "Figure 15 ‣
    Appendix H The Full Arc DSL ‣ Appendix G The Full String Grammar ‣ Appendix F
    LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") (for filters) and [Fig. 16](#A8.F16
    "Figure 16 ‣ Appendix H The Full Arc DSL ‣ Appendix G The Full String Grammar
    ‣ Appendix F LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor Grammar
    ‣ Appendix D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different
    sample sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt
    for the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") (for transforms).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '我们 Arc DSL 的完整语法见 [图 15](#A8.F15 "图 15 ‣ 附录 H Arc DSL 完整语法 ‣ 附录 G 字符串语法 ‣ 附录
    F 字符串 LLM 提示 ‣ 附录 E 张量语法 LLM 提示 ‣ 附录 D LLM DeepSeek 和 Gpt3.5 的实验结果 ‣ 附录 C Arc、张量和字符串领域的不同样本大小消融实验
    ‣ 附录 B 动机示例的 LLM 提示 ‣ 附录 A GPT4o 动机示例的解决方案 ‣ 无上下文语法 ‣ 2.1 编程示例 ‣ 2 背景 ‣ \tool:
    指导程序合成的上下文无关 LLM 近似")（用于过滤器）和 [图 16](#A8.F16 "图 16 ‣ 附录 H Arc DSL 完整语法 ‣ 附录 G
    字符串语法 ‣ 附录 F 字符串 LLM 提示 ‣ 附录 E 张量语法 LLM 提示 ‣ 附录 D LLM DeepSeek 和 Gpt3.5 的实验结果
    ‣ 附录 C Arc、张量和字符串领域的不同样本大小消融实验 ‣ 附录 B 动机示例的 LLM 提示 ‣ 附录 A GPT4o 动机示例的解决方案 ‣ 无上下文语法
    ‣ 2.1 编程示例 ‣ 2 背景 ‣ \tool: 指导程序合成的上下文无关 LLM 近似")（用于变换）。'
- en: '|  | $\displaystyle\mathit{Start}$ |  |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{开始}$ |  |  |'
- en: '|  | $\displaystyle\mathit{Filters}$ |  |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{过滤器}$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  | $\displaystyle\mathit{Filter\_Ops}$ |  |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{过滤器\_操作}$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  |  | $\displaystyle\mid$ |  |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |  |'
- en: '|  | $\displaystyle\mathit{Color}$ |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{颜色}$ |  |'
- en: '|  |  | $\displaystyle\mid$ | object colors in the grid |  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | 网格中的对象颜色 |  |'
- en: '|  | $\displaystyle\mathit{Size}$ | object sizes |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{大小}$ | 对象尺寸 |  |'
- en: '|  | $\displaystyle\mathit{Degree}$ | degrees in the graphs |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{度}$ | 图中的度数 |  |'
- en: '|  | $\displaystyle\mathit{Height}$ | object heights |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{高度}$ | 对象高度 |  |'
- en: '|  | $\displaystyle\mathit{Width}$ | object widths |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{宽度}$ | 对象宽度 |  |'
- en: '|  | $\displaystyle\mathit{Column}$ | columns in the grid |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{列}$ | 网格中的列 |  |'
- en: '|  | $\displaystyle\mathit{Row}$ | rows in the grid |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{行}$ | 网格中的行 |  |'
- en: '|  | $\displaystyle\mathit{Shape}$ | shapes in the grid |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{形状}$ | 网格中的形状 |  |'
- en: '|  | $\displaystyle\mathit{Obj}$ |  |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{对象}$ |  |  |'
- en: 'Figure 15: The modified filter grammar derived from Arga [[44](#bib.bib44)],
    object specific parameters like size, degree, height, width change per benchmark.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：从 Arga [[44](#bib.bib44)] 派生的修改过滤器语法，对象特定参数如大小、度数、高度、宽度在每个基准测试中会有所变化。
- en: '|  | $\displaystyle\mathit{Start}$ |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{开始}$ |  |'
- en: '|  | $\displaystyle\mathit{Transforms}$ |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{变换}$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  | $\displaystyle\mathit{Transform\_Ops}$ |  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{变换\_操作}$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ |  |'
- en: '|  |  | $\displaystyle\mid$ | NoOp |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mid$ | NoOp |  |'
- en: '|  | $\displaystyle\mathit{Color}$ |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{颜色}$ |  |'
- en: '|  | $\displaystyle\mathit{Direction}$ |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{方向}$ |  |'
- en: '|  | $\displaystyle\mathit{Overlap}$ |  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{重叠}$ |  |'
- en: '|  | $\displaystyle\mathit{Angle}$ |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{角度}$ |  |'
- en: '|  | $\displaystyle\mathit{Axis}$ |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{轴}$ |  |'
- en: '|  | $\displaystyle\mathit{Object}$ |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathit{对象}$ |  |'
- en: 'Figure 16: The modified transform grammar derived from Arga [[44](#bib.bib44)],
    parameters like objects change based on the benchmark.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：从 Arga [[44](#bib.bib44)] 派生的修改变换语法，参数如对象根据基准测试会有所变化。
- en: Appendix I The Full Tensor Grammar
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 完整的张量语法
- en: General  TensorFlow  functions:-----------------------------tf.abs(x)tf.add(x,  y)tf.add_n(inputs)tf.argmax(input,  axis)tf.argmin(input,  axis)tf.argsort(values,  axis,  stable=True)tf.argsort(values,  axis,  direction=’DESCENDING’,  stable=True)tf.boolean_mask(tensor,  mask)tf.broadcast_to(input,  shape)tf.cast(x,  dtype)tf.clip_by_value(t,  clip_value_min,  clip_value_max)tf.concat(values,  axis)tf.constant(value)tf.constant(value,  dtype)tf.divide(x,  y)tf.equal(x,  y)tf.exp(x)tf.expand_dims(input,  axis)tf.eye(num_rows)tf.eye(num_rows,  num_columns)tf.eye(num_rows,  dtype)tf.fill(dims,  value)tf.gather(params,  indices)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 一般 TensorFlow 函数：-----------------------------tf.abs(x)tf.add(x, y)tf.add_n(inputs)tf.argmax(input,
    axis)tf.argmin(input, axis)tf.argsort(values, axis, stable=True)tf.argsort(values,
    axis, direction=’DESCENDING’, stable=True)tf.boolean_mask(tensor, mask)tf.broadcast_to(input,
    shape)tf.cast(x, dtype)tf.clip_by_value(t, clip_value_min, clip_value_max)tf.concat(values,
    axis)tf.constant(value)tf.constant(value, dtype)tf.divide(x, y)tf.equal(x, y)tf.exp(x)tf.expand_dims(input,
    axis)tf.eye(num_rows)tf.eye(num_rows, num_columns)tf.eye(num_rows, dtype)tf.fill(dims,
    value)tf.gather(params, indices)
- en: 'Figure 17: List of TensorFlow operations as used in TFCoder.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：在 TFCoder 中使用的 TensorFlow 操作列表。
- en: 'tf.gather(params,  indices,  axis,  batch_dims)tf.gather_nd(params,  indices)tf.gather_nd(params,  indices,  batch_dims)tf.greater(x,  y)tf.greater_equal(x,  y)tf.math.bincount(arr)tf.math.ceil(x)tf.math.count_nonzero(input)tf.math.count_nonzero(input,  axis)tf.math.cumsum(x,  axis)tf.math.cumsum(x,  axis,  exclusive=True)tf.math.divide_no_nan(x,  y)tf.math.floor(x)tf.math.log(x)tf.math.negative(x)tf.math.reciprocal(x)tf.math.reciprocal_no_nan(x)tf.math.segment_max(data,  segment_ids)tf.math.segment_mean(data,  segment_ids)tf.math.segment_min(data,  segment_ids)tf.math.segment_prod(data,  segment_ids)tf.math.segment_sum(data,  segment_ids)tf.math.squared_difference(x,  y)tf.math.top_k(input,  k)tf.math.unsorted_segment_max(data,  segment_ids,  num_segments)tf.math.unsorted_segment_mean(data,  segment_ids,  num_segments)tf.math.unsorted_segment_min(data,  segment_ids,  num_segments)tf.math.unsorted_segment_prod(data,  segment_ids,  num_segments)tf.math.unsorted_segment_sum(data,  segment_ids,  num_segments)tf.matmul(a,  b)tf.maximum(x,  y)tf.minimum(x,  y)tf.multiply(x,  y)tf.not_equal(x,  y)tf.one_hot(indices,  depth)tf.ones(shape)tf.ones_like(input)tf.pad(tensor,  paddings,  mode=’CONSTANT’)tf.pad(tensor,  paddings,  mode=’CONSTANT’,  constant_values)tf.pad(tensor,  paddings,  mode=’REFLECT’)tf.pad(tensor,  paddings,  mode=’SYMMETRIC’)tf.range(start)tf.range(start,  limit,  delta)tf.reduce_any(input_tensor,  axis)tf.reduce_max(input_tensor)tf.reduce_max(input_tensor,  axis)tf.reduce_mean(input_tensor)tf.reduce_mean(input_tensor,  axis)tf.reduce_min(input_tensor)tf.reduce_min(input_tensor,  axis)tf.reduce_prod(input_tensor,  axis)tf.reduce_sum(input_tensor)tf.reduce_sum(input_tensor,  axis)tf.reshape(tensor,  shape)tf.reverse(tensor,  axis)tf.roll(input,  shift,  axis)tf.round(x)tf.searchsorted(sorted_sequence,  values,  side=’left’)tf.searchsorted(sorted_sequence,  values,  side=’right’)tf.sequence_mask(lengths)tf.sequence_mask(lengths,  maxlen)tf.shape(input)tf.sign(x)tf.sort(values,  axis)tf.sort(values,  axis,  direction=’DESCENDING’)tf.sqrt(x)tf.square(x)tf.squeeze(input)tf.squeeze(input,  axis)tf.stack(values,  axis)tf.subtract(x,  y)tf.tensordot(a,  b,  axes)tf.tile(input,  multiples)tf.transpose(a)tf.transpose(a,  perm)tf.unique_with_counts(x)tf.unstack(value,  axis)tf.where(condition)tf.where(condition,  x,  y)tf.zeros(shape)tf.zeros_like(input)SparseTensor  functions:-----------------------tf.SparseTensor(indices,  values,  dense_shape)tf.sparse.add(a,  b)tf.sparse.concat(axis,  sp_inputs)tf.sparse.expand_dims(sp_input,  axis)tf.sparse.from_dense(tensor)tf.sparse.maximum(sp_a,  sp_b)tf.sparse.minimum(sp_a,  sp_b)tf.sparse.reduce_max(sp_input,  axis,  output_is_sparse)tf.sparse.reduce_sum(sp_input,  axis,  output_is_sparse)tf.sparse.reset_shape(sp_input)tf.sparse.reshape(sp_input,  shape)tf.sparse.retain(sp_input,  to_retain)tf.sparse.slice(sp_input,  start,  size)tf.sparse.split(sp_input,  num_split,  axis)tf.sparse.to_dense(sp_input)tf.sparse.to_dense(sp_input,  default_value)tf.sparse.to_indicator(sp_input,  vocab_size)tf.sparse.transpose(sp_input)tf.sparse.transpose(sp_input,  perm)Python-syntax  operations:-------------------------IndexingAxis1Operation:  arg1[:,  arg2]IndexingOperation:  arg1[arg2]PairCreationOperation:  (arg1,  arg2)SingletonTupleCreationOperation:  (arg1,)SlicingAxis0BothOperation:  arg1[arg2:arg3]SlicingAxis0LeftOperation:  arg1[arg2:]SlicingAxis0RightOperation:  arg1[:arg2]SlicingAxis1BothOperation:  arg1[:,  arg2:arg3]SlicingAxis1LeftOperation:  arg1[:,  arg2:]SlicingAxis1RightOperation:  arg1[:,  :arg2]TripleCreationOperation:  (arg1,  arg2,  arg3)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.gather(params,  indices,  axis,  batch_dims)` `tf.gather_nd(params,  indices)`
    `tf.gather_nd(params,  indices,  batch_dims)` `tf.greater(x,  y)` `tf.greater_equal(x,  y)`
    `tf.math.bincount(arr)` `tf.math.ceil(x)` `tf.math.count_nonzero(input)` `tf.math.count_nonzero(input,  axis)`
    `tf.math.cumsum(x,  axis)` `tf.math.cumsum(x,  axis,  exclusive=True)` `tf.math.divide_no_nan(x,  y)`
    `tf.math.floor(x)` `tf.math.log(x)` `tf.math.negative(x)` `tf.math.reciprocal(x)`
    `tf.math.reciprocal_no_nan(x)` `tf.math.segment_max(data,  segment_ids)` `tf.math.segment_mean(data,  segment_ids)`
    `tf.math.segment_min(data,  segment_ids)` `tf.math.segment_prod(data,  segment_ids)`
    `tf.math.segment_sum(data,  segment_ids)` `tf.math.squared_difference(x,  y)`
    `tf.math.top_k(input,  k)` `tf.math.unsorted_segment_max(data,  segment_ids,  num_segments)`
    `tf.math.unsorted_segment_mean(data,  segment_ids,  num_segments)` `tf.math.unsorted_segment_min(data,  segment_ids,  num_segments)`
    `tf.math.unsorted_segment_prod(data,  segment_ids,  num_segments)` `tf.math.unsorted_segment_sum(data,  segment_ids,  num_segments)`
    `tf.matmul(a,  b)` `tf.maximum(x,  y)` `tf.minimum(x,  y)` `tf.multiply(x,  y)`
    `tf.not_equal(x,  y)` `tf.one_hot(indices,  depth)` `tf.ones(shape)` `tf.ones_like(input)`
    `tf.pad(tensor,  paddings,  mode=’CONSTANT’)` `tf.pad(tensor,  paddings,  mode=’CONSTANT’,  constant_values)`
    `tf.pad(tensor,  paddings,  mode=’REFLECT’)` `tf.pad(tensor,  paddings,  mode=’SYMMETRIC’)`
    `tf.range(start)` `tf.range(start,  limit,  delta)` `tf.reduce_any(input_tensor,  axis)`
    `tf.reduce_max(input_tensor)` `tf.reduce_max(input_tensor,  axis)` `tf.reduce_mean(input_tensor)`
    `tf.reduce_mean(input_tensor,  axis)` `tf.reduce_min(input_tensor)` `tf.reduce_min(input_tensor,  axis)`
    `tf.reduce_prod(input_tensor,  axis)` `tf.reduce_sum(input_tensor)` `tf.reduce_sum(input_tensor,  axis)`
    `tf.reshape(tensor,  shape)` `tf.reverse(tensor,  axis)` `tf.roll(input,  shift,  axis)`
    `tf.round(x)` `tf.searchsorted(sorted_sequence,  values,  side=’left’)` `tf.searchsorted(sorted_sequence,  values,  side=’right’)`
    `tf.sequence_mask(lengths)` `tf.sequence_mask(lengths,  maxlen)` `tf.shape(input)`
    `tf.sign(x)` `tf.sort(values,  axis)` `tf.sort(values,  axis,  direction=’DESCENDING’)`
    `tf.sqrt(x)` `tf.square(x)` `tf.squeeze(input)` `tf.squeeze(input,  axis)` `tf.stack(values,  axis)`
    `tf.subtract(x,  y)` `tf.tensordot(a,  b,  axes)` `tf.tile(input,  multiples)`
    `tf.transpose(a)` `tf.transpose(a,  perm)` `tf.unique_with_counts(x)` `tf.unstack(value,  axis)`
    `tf.where(condition)` `tf.where(condition,  x,  y)` `tf.zeros(shape)` `tf.zeros_like(input)`
    稀疏张量函数： ----------------------- `tf.SparseTensor(indices,  values,  dense_shape)`
    `tf.sparse.add(a,  b)` `tf.sparse.concat(axis,  sp_inputs)` `tf.sparse.expand_dims(sp_input,  axis)`
    `tf.sparse.from_dense(tensor)` `tf.sparse.maximum(sp_a,  sp_b)` `tf.sparse.minimum(sp_a,  sp_b)`
    `tf.sparse.reduce_max(sp_input,  axis,  output_is_sparse)` `tf.sparse.reduce_sum(sp_input,  axis,  output_is_sparse)`
    `tf.sparse.reset_shape(sp_input)` `tf.sparse.reshape(sp_input,  shape)` `tf.sparse.retain(sp_input,  to_retain)`
    `tf.sparse.slice(sp_input,  start,  size)` `tf.sparse.split(sp_input,  num_split,  axis)`
    `tf.sparse.to_dense(sp_input)` `tf.sparse.to_dense(sp_input,  default_value)`
    `tf.sparse.to_indicator(sp_input,  vocab_size)` `tf.sparse.transpose(sp_input)`
    `tf.sparse.transpose(sp_input,  perm)` Python 语法操作： -------------------------
    `IndexingAxis1Operation:  arg1[:,  arg2]` `IndexingOperation:  arg1[arg2]` `PairCreationOperation:  (arg1,  arg2)`
    `SingletonTupleCreationOperation:  (arg1,)` `SlicingAxis0BothOperation:  arg1[arg2:arg3]`
    `SlicingAxis0LeftOperation:  arg1[arg2:]` `SlicingAxis0RightOperation:  arg1[:arg2]`
    `SlicingAxis1BothOperation:  arg1[:,  arg2:arg3]` `SlicingAxis1LeftOperation:  arg1[:,  arg2:]`
    `SlicingAxis1RightOperation:  arg1[:,  :arg2]` `TripleCreationOperation:  (arg1,  arg2,  arg3)`'
- en: Appendix J Detailed Prompt Settings
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 J 详细提示设置
- en: For Arc, we sample completions with temperature 1 and 4000 max tokens. For Tensor,
    we use temperature 1 and 300 max tokens. For SyGuS, we use temperature 0.5 and
    200 max tokens. We use the same settings for all 3 LLMs. When prompting Gpt4o
    for Arc, we set response_type to JSON.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Arc，我们采样温度为1，最大令牌数为4000。对于 Tensor，我们使用温度1和最大令牌数300。对于 SyGuS，我们使用温度0.5和最大令牌数200。我们对所有3个LLM使用相同的设置。在对
    Gpt4o 进行 Arc 提示时，我们将 response_type 设置为 JSON。
- en: Appendix K Broader Research Impacts
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 K 更广泛的研究影响
- en: Our method presents a powerful strategy for harnessing both syntactically valid
    and invalid outputs from an LLM to learn a surrogate model. Incorporating hallucinatory
    outputs – often erroneous generated by the model, allows us to extract insights
    that are discarded in standard practices. Our approach mitigates the need for
    large-scale sampling of completions from LLMs, promoting a more efficient and
    effective utilization of these models, saving resources. Our method not only improves
    the cost effectiveness of using LLMs but also opens up new avenues for enhancing
    model robustness and adaptability across different domains.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法提出了一种强大的策略，用于利用LLM生成的语法有效和无效的输出以学习代理模型。结合模型经常产生的错误输出，使我们能够提取标准实践中被丢弃的见解。我们的方法减少了从LLM中大规模采样完成的需求，促进了这些模型的更高效、更有效的利用，从而节省了资源。我们的方法不仅提高了使用LLM的成本效益，还开辟了在不同领域提升模型鲁棒性和适应性的新的途径。
- en: Appendix L The Arc Synthesis Algorithm
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 L Arc 合成算法
- en: Overall Synthesis Algorithm
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总体合成算法
- en: 'The overall synthesis algorithm takes as input a set of input-output grids
    $\mathcal{E}$. Concretely, the first set of transforms that satisfies our running
    example is [update_color(FUCHSIA),  update_color(CYAN),  update_color(RED),  update_color(BLUE),
    update_color(ORANGE),  update_color(YELLOW)] but filters can not be found for
    all these transforms. Eventually, the more concise set of transforms [update_color(color_of(X)]
    is enumerated leading to the filter solution we saw in [Eq. 1](#S1.E1 "1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"). In addition,
    the algorithm described above terminates after the first solution is found, but
    we keep searching for a smaller set of transforms[[6](#bib.bib6)].'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '总体合成算法以一组输入输出网格 $\mathcal{E}$ 作为输入。具体来说，第一个满足我们运行示例的转换集合是 [update_color(FUCHSIA),  update_color(CYAN),  update_color(RED),  update_color(BLUE),
    update_color(ORANGE),  update_color(YELLOW)]，但无法找到这些转换的过滤器。最终，更简洁的转换集合 [update_color(color_of(X)]
    被枚举，得出我们在 [Eq. 1](#S1.E1 "1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") 中看到的过滤器解决方案。此外，上述算法在找到第一个解决方案后终止，但我们继续寻找更小的转换集合[[6](#bib.bib6)]。'
- en: Algorithm 2 Overall Synthesis Algorithm
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 总体合成算法
- en: 1:A set of input-output example grids $\mathcal{E}$ Search for transforms9:         if $\mathcal{T}\neq\mathcal{T}_{p}$
    Found a complete solution! return transform-filter map
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 1:一组输入输出示例网格 $\mathcal{E}$ 搜索转换9:         如果 $\mathcal{T}\neq\mathcal{T}_{p}$
    找到完整解决方案！返回转换-过滤器映射
- en: Transform Search Algorithm
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转换搜索算法
- en: Algorithm 3 Transform Synthesis Algorithm
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 转换合成算法
- en: 1:PCFG $\mathcal{G}_{t}$8:                 $\mathrm{TSol}\leftarrow\mathrm{TSol}\cup\{T\}$  $\triangleright$ then  $\triangleright$
    concrete programs with substituted values33:                             $\mathbf{yield}\
    (t\ P_{1}^{\prime}\ \ldots\ P_{k}^{\prime})$
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 1:PCFG $\mathcal{G}_{t}$8:                 $\mathrm{TSol}\leftarrow\mathrm{TSol}\cup\{T\}$  $\triangleright$ 然后
    $\triangleright$ 具有替代值的具体程序33:                             $\mathbf{yield}\ (t\
    P_{1}^{\prime}\ \ldots\ P_{k}^{\prime})$
- en: 'We first describe our transforms synthesis algorithm in Algorithm [3](#alg3
    "Algorithm 3 ‣ Transform Search Algorithm ‣ Appendix L The Arc Synthesis Algorithm
    ‣ Appendix K Broader Research Impacts ‣ Appendix J Detailed Prompt Settings ‣
    Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc DSL ‣ Appendix G
    The Full String Grammar ‣ Appendix F LLM Prompt for String ‣ Appendix E LLM Prompt
    for the Tensor Grammar ‣ Appendix D Experimental results with LLMs DeepSeek and
    Gpt3.5 ‣ Appendix C Different sample sizes ablation for Arc, Tensor and String
    domains ‣ Appendix B LLM Prompt for the Motivating Example ‣ Appendix A GPT4o
    Solutions for the Motivating Example ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example
    ‣ 2 Background ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis").
    The algorithm takes as input a PCFG $\mathcal{G}_{t}$ or reaches a certain cost
    limit Lim.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在算法 [3](#alg3 "Algorithm 3 ‣ Transform Search Algorithm ‣ Appendix L The
    Arc Synthesis Algorithm ‣ Appendix K Broader Research Impacts ‣ Appendix J Detailed
    Prompt Settings ‣ Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc
    DSL ‣ Appendix G The Full String Grammar ‣ Appendix F LLM Prompt for String ‣
    Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") 中描述了我们的变换合成算法。该算法以 PCFG $\mathcal{G}_{t}$ 作为输入，或者达到某个成本限制
    Lim。'
- en: 'The algorithm starts with the following initial state: 1) a cost level ($\textsc{Lvl}_{0}$.
    At each iteration, the algorithm explores the space of all new transforms generated
    by the New-Transforms procedure for the current cost level.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从以下初始状态开始：1) 成本水平（$\textsc{Lvl}_{0}$）。在每次迭代中，算法会探索由当前成本水平的 New-Transforms
    程序生成的所有新变换的空间。
- en: 'On line 5 in Algorithm [3](#alg3 "Algorithm 3 ‣ Transform Search Algorithm
    ‣ Appendix L The Arc Synthesis Algorithm ‣ Appendix K Broader Research Impacts
    ‣ Appendix J Detailed Prompt Settings ‣ Appendix I The Full Tensor Grammar ‣ Appendix
    H The Full Arc DSL ‣ Appendix G The Full String Grammar ‣ Appendix F LLM Prompt
    for String ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental
    results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation
    for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating
    Example ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free
    Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM
    Approximation for Guiding Program Synthesis"), the enumerated transform $T$ (line
    11).'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '在算法 [3](#alg3 "Algorithm 3 ‣ Transform Search Algorithm ‣ Appendix L The Arc
    Synthesis Algorithm ‣ Appendix K Broader Research Impacts ‣ Appendix J Detailed
    Prompt Settings ‣ Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc
    DSL ‣ Appendix G The Full String Grammar ‣ Appendix F LLM Prompt for String ‣
    Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") 的第5行中，列举的变换 $T$（第11行）。'
- en: 'As discussed before, the grey objects get painted with a variable color which
    corresponds to the color of it’s smallest neighbor, for *e.g.* in the first grid[1(a)](#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for
    Guiding Program Synthesis"), the grey objects get painted with red, fuchsia and
    cyan colors. Whenever we encounter a transform which could have variable values,
    we consider all possible values that could be assigned and yield multiple concrete
    programs corresponding to each of those assignments. Consider the three changed
    objects in [1(a)](#S1.F1.sf1 "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") to be $\omega_{1}$ with its
    corresponding value $v_{i_{j}}$ to get concrete subexpressions in the eventual
    transform that is returned on line 38. In order to do so, the algorithm considers
    all possible values the variable subexpressions could assume on line 37, for *e.g.*
    with variable colors it could be red, green, cyan and variable direction could
    be Up, Down, Right. It then computes the cartesian product of these value sets
    and for each combination of variable assigments, the algorithm substitutes the
    original variable subexpressions with their concrete values, generating a new,
    concrete program on line 38. If there are no variable subexpressions, a new program
    is returned using the selected subexpressions based on rule R on line 40.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，灰色对象会被涂上一个变量颜色，该颜色对应于其最小邻居的颜色，例如，在第一个网格[1(a)](#S1.F1.sf1 "1(a) ‣ 图 1 ‣
    1 引言 ‣ \tool: 无上下文 LLM 近似指导程序合成")中，灰色对象被涂上红色、紫红色和青色。每当遇到可能具有变量值的转换时，我们会考虑所有可能的赋值，并生成多个具体的程序对应于每一个赋值。考虑[1(a)](#S1.F1.sf1
    "1(a) ‣ 图 1 ‣ 1 引言 ‣ \tool: 无上下文 LLM 近似指导程序合成")中的三个变更对象为$\omega_{1}$及其对应的值$v_{i_{j}}$，以在第38行返回的最终转换中获得具体的子表达式。为此，算法在第37行考虑变量子表达式可能取的所有值，例如，变量颜色可以是红色、绿色、青色，变量方向可以是上、下、右。然后，它计算这些值集合的笛卡尔积，对于每一种变量赋值的组合，算法将原始变量子表达式替换为其具体值，在第38行生成一个新的具体程序。如果没有变量子表达式，则使用基于规则R的选择子表达式生成一个新程序，并在第40行返回。'
- en: Algorithm 4 Filter Synthesis Algorithm
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 过滤合成算法
- en: 1:PCFG $\mathcal{G}_{f}$ then  $\triangleright$  $\triangleright$
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 1:PCFG $\mathcal{G}_{f}$ 然后 $\triangleright$ $\triangleright$
- en: Filter Search Algorithm
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过滤搜索算法
- en: 'The filter search algorithm in Algorithm  [4](#alg4 "Algorithm 4 ‣ Transform
    Search Algorithm ‣ Appendix L The Arc Synthesis Algorithm ‣ Appendix K Broader
    Research Impacts ‣ Appendix J Detailed Prompt Settings ‣ Appendix I The Full Tensor
    Grammar ‣ Appendix H The Full Arc DSL ‣ Appendix G The Full String Grammar ‣ Appendix
    F LLM Prompt for String ‣ Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix
    D Experimental results with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample
    sizes ablation for Arc, Tensor and String domains ‣ Appendix B LLM Prompt for
    the Motivating Example ‣ Appendix A GPT4o Solutions for the Motivating Example
    ‣ Context-Free Grammars ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free
    LLM Approximation for Guiding Program Synthesis") takes as input: 1) a filter
    grammar $\mathcal{G}_{f}$, it initiates a new search state to find a filter. Each
    filter at a cost level is evaluated on all the objects in the input grid to return
    the objects for which the filter holds True. In [1(a)](#S1.F1.sf1 "1(a) ‣ Figure
    1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding Program
    Synthesis"), the filter size_Of(obj) == 1 would return the objects which are unchanged
    since all of them have size 1 (line 9). Consider the filter for the other transform
    in our example, update_color(color_of(X)) in [Eq. 1](#S1.E1 "1 ‣ 1 Introduction
    ‣ \tool: Context-Free LLM Approximation for Guiding Program Synthesis"). The evaluation
    result for this filter would be a mapping from the grey objects and the objects
    from where they got their updated color.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '算法[4](#alg4 "Algorithm 4 ‣ Transform Search Algorithm ‣ Appendix L The Arc
    Synthesis Algorithm ‣ Appendix K Broader Research Impacts ‣ Appendix J Detailed
    Prompt Settings ‣ Appendix I The Full Tensor Grammar ‣ Appendix H The Full Arc
    DSL ‣ Appendix G The Full String Grammar ‣ Appendix F LLM Prompt for String ‣
    Appendix E LLM Prompt for the Tensor Grammar ‣ Appendix D Experimental results
    with LLMs DeepSeek and Gpt3.5 ‣ Appendix C Different sample sizes ablation for
    Arc, Tensor and String domains ‣ Appendix B LLM Prompt for the Motivating Example
    ‣ Appendix A GPT4o Solutions for the Motivating Example ‣ Context-Free Grammars
    ‣ 2.1 Programming-By-Example ‣ 2 Background ‣ \tool: Context-Free LLM Approximation
    for Guiding Program Synthesis") 中的滤镜搜索算法以以下输入：1) 滤镜语法$\mathcal{G}_{f}$，它启动一个新的搜索状态以寻找滤镜。每个滤镜在一个成本水平上评估输入网格中的所有对象，以返回滤镜为真的对象。在[1(a)](#S1.F1.sf1
    "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for
    Guiding Program Synthesis")中，filter size_Of(obj) == 1会返回那些未改变的对象，因为它们的大小都是1（第9行）。考虑我们示例中的另一个变换滤镜，update_color(color_of(X))在[Eq.
    1](#S1.E1 "1 ‣ 1 Introduction ‣ \tool: Context-Free LLM Approximation for Guiding
    Program Synthesis")。该滤镜的评估结果将是从灰色对象和它们获取更新颜色的对象的映射。'
- en: If the objects for which the filter is True is same as the objects correctly
    transformed by $\mathrm{TSol}$ which maps all transforms to their filters.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果滤镜为真时的对象与通过$\mathrm{TSol}$正确转换的对象相同，$\mathrm{TSol}$将所有转换映射到它们的滤镜。
