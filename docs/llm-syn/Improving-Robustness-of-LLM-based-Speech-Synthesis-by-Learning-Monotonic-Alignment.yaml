- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:03:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:03:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高LLM-based语音合成的鲁棒性，通过学习单调对齐
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17957](https://ar5iv.labs.arxiv.org/html/2406.17957)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17957](https://ar5iv.labs.arxiv.org/html/2406.17957)
- en: \interspeechcameraready\name
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \interspeechcameraready\name
- en: ^∗PaarthNeekhara \name^∗ShehzeenHussain \nameSubhankarGhosh \nameJasonLi \nameRafaelValle
    \nameRohanBadlani \nameBorisGinsburg
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗PaarthNeekhara \name^∗ShehzeenHussain \nameSubhankarGhosh \nameJasonLi \nameRafaelValle
    \nameRohanBadlani \nameBorisGinsburg
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated
    remarkable capabilities in handling large speech datasets and generating natural
    speech for new speakers. However, LLM-based TTS models are not robust as the generated
    output can contain repeating words, missing words and mis-aligned speech (referred
    to as hallucinations or attention errors), especially when the text contains multiple
    occurrences of the same token. We examine these challenges in an encoder-decoder
    transformer model and find that certain cross-attention heads in such models implicitly
    learn the text and speech alignment when trained for predicting speech tokens
    for a given text. To make the alignment more robust, we propose techniques utilizing
    CTC loss and attention priors that encourage monotonic cross-attention over the
    text tokens. Our guided attention training technique does not introduce any new
    learnable parameters and significantly improves robustness of LLM-based TTS models. ¹¹1
    Audio Examples: [https://t5tts.github.io/](https://t5tts.github.io/)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大语言模型（LLM）的文本到语音（TTS）系统在处理大型语音数据集和为新说话者生成自然语音方面展现了显著的能力。然而，LLM-based TTS模型并不够鲁棒，因为生成的输出可能包含重复的词语、缺失的词语以及对齐不准确的语音（称为幻觉或注意力错误），特别是当文本中包含多个相同的标记时。我们在一个编码器-解码器变换器模型中研究了这些挑战，发现这样的模型中的某些交叉注意力头在训练时会隐式地学习文本和语音的对齐，以预测给定文本的语音标记。为了使对齐更加鲁棒，我们提出了利用CTC损失和注意力先验的技术，这些技术鼓励对文本标记进行单调交叉注意力。我们的指导注意力训练技术不引入任何新的可学习参数，并显著提高了LLM-based
    TTS模型的鲁棒性。¹¹1 音频示例：[https://t5tts.github.io/](https://t5tts.github.io/)
- en: ^∗Denotes equal contribution
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗表示同等贡献
- en: 'keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: speech synthesis, large language modeling, robustness, computational paralinguistics,
    speech text alignments
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语音合成、大语言模型、鲁棒性、计算语言学、语音文本对齐
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have revolutionized the landscape of deep generative
    AI with their unprecedented ability to generate coherent and contextually rich
    content across diverse domains. In LLM-based generative models, data is quantized
    into discrete tokens, which allows the formulation of data synthesis as a language
    modeling task. Transformer architectures such as GPT [[1](#bib.bib1)] (decoder-only)
    and T5 [[2](#bib.bib2)] (encoder-decoder) are trained to autoregressively generate
    discrete tokens given a prompt, leading to a unified architecture that can be
    adapted across various data domains and synthesis tasks. Particularly in the speech
    domain, there has been a recent surge in the use of LLMs for various speech synthesis
    applications such as text-to-speech (TTS) and speech-to-speech translation tasks [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）以其在不同领域生成连贯且富有语境的内容的前所未有的能力，彻底改变了深度生成式人工智能的格局。在基于LLM的生成模型中，数据被量化为离散的标记，这使得数据合成可以被表述为语言建模任务。像GPT
    [[1](#bib.bib1)]（仅解码器）和T5 [[2](#bib.bib2)]（编码器-解码器）这样的变换器架构被训练来根据提示自回归地生成离散标记，从而形成一个可以适用于各种数据领域和合成任务的统一架构。特别是在语音领域，最近出现了使用LLMs进行各种语音合成应用的趋势，如文本到语音（TTS）和语音到语音翻译任务
    [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]。
- en: TTS synthesis has been traditionally treated as a cascaded problem with intermediate
    mel-spectrogram representation that is typically modelled as a regression task [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]. However,
    discrete neural audio codecs [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]
    have emerged as a promising intermediate audio representation, that not only preserve
    audio fidelity at a high compression rate, but are also suitable for training
    autoregressive transformer-based LLMs. Audio LLMs [[3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)] have gained traction for their ability to generate audio seamlessly,
    eliminating the necessity for additional duration and pitch prediction models.
    Moreover, LLM-based speech synthesis models can scale up to large speech datasets
    and be prompted in diverse ways to perform tasks like zero-shot speech synthesis,
    multilingual speech synthesis and other audio generation tasks besides speech.
    Despite their remarkable achievements, LLM-based TTS models suffer from attention
    errors resulting in mis-aligned speech, repeating and missing words, analogous
    to hallucinations [[15](#bib.bib15), [16](#bib.bib16)] exhibited by LLMs in the
    text domain. This issue becomes more prominent when the input text is challenging
    and contains repeating words. For certain inputs, the probabilistic autoregressive
    inference of LLM-based TTS models can result in looping or infinite silences [[17](#bib.bib17)].
    This issue makes LLM-based TTS models unreliable for real-world applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TTS 合成传统上被视为一个级联问题，通常使用中间的 mel-spectrogram 表示，该表示通常被建模为回归任务[[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]。然而，离散神经音频编码器[[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]已经作为一种有前景的中间音频表示出现，不仅在高压缩率下保持音频保真度，而且还适合用于训练自回归
    transformer 基于 LLM 的模型。音频 LLM[[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]因其能够无缝生成音频而受到关注，消除了额外的时长和音高预测模型的必要性。此外，基于
    LLM 的语音合成模型可以扩展到大型语音数据集，并以多种方式进行提示，以执行如零-shot 语音合成、多语言语音合成以及其他音频生成任务。尽管取得了显著成就，基于
    LLM 的 TTS 模型仍然遭受注意力错误，导致语音对齐错误、重复和缺失单词，这类似于 LLM 在文本领域表现出的幻觉[[15](#bib.bib15),
    [16](#bib.bib16)]。当输入文本具有挑战性且包含重复单词时，这个问题尤为突出。对于某些输入，基于 LLM 的 TTS 模型的概率自回归推断可能导致循环或无限沉默[[17](#bib.bib17)]。这一问题使得基于
    LLM 的 TTS 模型在实际应用中不可靠。
- en: In our work, we investigate this hallucination issue and find that attention
    layers of LLM-based TTS models learn an implicit alignment between text and speech
    tokens when trained using the next-token prediction objective. In encoder-decoder
    transformers, the TTS alignment is learned in certain cross-attention heads of
    the decoder; while in decoder-only models, the alignment is learned in the self-attention
    layers. Since the implicitly learned alignment in attention layers is unconstrained
    during training, it is not strictly monotonic which results in mis-aligned synthesis
    during inference. To address this challenge, we propose a learning procedure that
    encourages monotonic alignment in the attention layers of LLM-based TTS models,
    resulting in significantly more robust TTS models without modifying the architecture
    or introducing new parameters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们研究了这个幻觉问题，并发现基于 LLM 的 TTS 模型的注意力层在使用下一个词预测目标进行训练时学习了文本和语音标记之间的隐式对齐。在编码器-解码器
    transformers 中，TTS 对齐是在解码器的某些交叉注意力头中学习的；而在仅解码器模型中，对齐是在自注意力层中学习的。由于训练过程中注意力层中隐式学习的对齐是不受约束的，因此在推断时会导致对齐不一致。为了解决这个挑战，我们提出了一种学习程序，鼓励
    LLM 基于 TTS 模型的注意力层中的单调对齐，从而在不修改架构或引入新参数的情况下显著增强 TTS 模型的鲁棒性。
- en: 'We design a TTS model based on an encoder-decoder T5 [[2](#bib.bib2)] transformer
    architecture, which takes text and audio tokens of a reference audio as input
    and autoregressively predicts the audio tokens of the target audio from the decoder.
    To improve robustness of the TTS model, we propose a technique to guide the cross-attention
    head of the T5 model using a static attention prior and alignment loss that encourages
    monotonic attention over the text input. Our experiments demonstrate that the
    proposed technique significantly improves intelligibility of the synthesized audio
    especially for challenging text inputs. The key contributions of our work are
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了一种基于编码器-解码器 T5 [[2](#bib.bib2)] Transformer 架构的 TTS 模型，该模型以文本和参考音频的音频标记作为输入，并从解码器中自回归地预测目标音频的音频标记。为了提高
    TTS 模型的鲁棒性，我们提出了一种技术，通过静态注意力先验和鼓励在文本输入上进行单调注意的对齐损失来指导 T5 模型的交叉注意力头。我们的实验表明，提出的技术显著提高了合成音频的可懂度，尤其是对于具有挑战性的文本输入。我们工作的主要贡献如下：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose an encoder-decoder transformer model for TTS synthesis. To the best
    of our knowledge, this is the first attempt at synthesizing multi-codebook neural
    audio codecs with an encoder-decoder architecture.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种用于 TTS 合成的编码器-解码器 Transformer 模型。根据我们所知，这是首次尝试使用编码器-解码器架构合成多代码本神经音频编解码器。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop an alignment learning technique to guide the cross-attention heads
    in our TTS model to learn monotonic alignment. Incorporating our proposed technique
    reduces Character Error Rate (CER) of synthesized speech from $9.03\%$ on challenging
    texts.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一种对齐学习技术，以指导我们 TTS 模型中的交叉注意力头学习单调对齐。引入我们提出的技术可以将合成语音的字符错误率（CER）从挑战性文本中的
    $9.03\%$ 降低。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We compare audio codec models based on Residual Vector Quantization and Finite
    Scalar Quantization (FSQ). FSQ codecs not only improve audio quality but also
    simplify the data representation by allowing parallel codebook prediction.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们比较了基于残差向量量化和有限标量量化（FSQ）的音频编解码器模型。FSQ 编解码器不仅改善了音频质量，还通过允许并行代码本预测来简化数据表示。
- en: '![Refer to caption](img/dd609717ab79ecf1bb11818df198c24a.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd609717ab79ecf1bb11818df198c24a.png)'
- en: 'Figure 1: Model Overview: (Left) The T5-TTS model takes as input text tokens
    and acoustic codes of reference audio and predicts the acoustic codes of the target
    audio. The figure shows both context input location options. (Right) The cross-attention
    scores implicitly learn text and speech alignment, but can be guided to learn
    more robust alignment with attention prior and alignment loss $L_{\textit{align}}$'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：模型概览：（左）T5-TTS 模型以文本标记和参考音频的声学代码作为输入，并预测目标音频的声学代码。图中展示了两种上下文输入位置选项。（右）交叉注意力分数隐式地学习文本和语音对齐，但可以通过注意力先验和对齐损失
    $L_{\textit{align}}$ 指导以学习更稳健的对齐。
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: AudioLM [[3](#bib.bib3)] pioneered the task of training a decoder-only LLM on
    discretized audio tokens from a neural codec model, for high-quality speech synthesis.
    Following this, several solutions utilizing decoder-only transformer architectures
    have been proposed such as VALL-E, UniAudio, Bark, SpeechX [[4](#bib.bib4), [18](#bib.bib18),
    [6](#bib.bib6), [5](#bib.bib5)]. They frame audio generation as an autoregressive
    language modeling task using multiple discrete codebooks. Alternatively, SpeechT5 [[19](#bib.bib19)]
    proposes an encoder-decoder architecture for sequence to sequence translation
    using a unified discrete representation of text and speech. However, SpeechT5
    similar to other synthesis models based on SSL representations [[20](#bib.bib20),
    [21](#bib.bib21)], does not utilize multi-codebook audio representations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: AudioLM [[3](#bib.bib3)] 开创了在神经编解码器模型的离散音频标记上训练仅解码器 LLM 的任务，以实现高质量语音合成。在此基础上，提出了几种利用仅解码器
    Transformer 架构的解决方案，如 VALL-E、UniAudio、Bark、SpeechX [[4](#bib.bib4)、[18](#bib.bib18)、[6](#bib.bib6)、[5](#bib.bib5)]。它们将音频生成框定为使用多个离散代码本的自回归语言建模任务。另一个方法是
    SpeechT5 [[19](#bib.bib19)]，它提出了一种用于序列到序列翻译的编码器-解码器架构，使用文本和语音的统一离散表示。然而，SpeechT5
    类似于其他基于 SSL 表示的合成模型 [[20](#bib.bib20)、[21](#bib.bib21)]，并没有利用多代码本音频表示。
- en: In the aforementioned transformer-based TTS models, the alignment between audio
    and phoneme sequences is entirely learned implicitly through the attention mechanisms
    in the transformer. This introduces potential instability in the form of hallucinations,
    since the alignment is not constrained to capture the monotonic dependencies of
    audio and text tokens [[4](#bib.bib4), [17](#bib.bib17)]. Prior research [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)] on non-LLM spectrogram generation models have
    proposed solutions to learn stricter alignment between text and speech tokens
    by constraining the encoder-decoder attention layers in CNN-based TTS models and
    LSTM-based models such as Tacotron [[7](#bib.bib7)] and Flowtron [[25](#bib.bib25)].
    While these techniques show promising results, they cannot be directly applied
    to transformer-based models which contain multiple cross-attention layers and
    multiple heads per layer, and generate discrete codes as opposed to continuous
    spectrograms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述基于变压器的 TTS 模型中，音频和音素序列之间的对齐完全通过变压器中的注意力机制隐式学习。这引入了潜在的不稳定性，如幻觉，因为对齐没有约束来捕捉音频和文本符号的单调依赖关系[[4](#bib.bib4),
    [17](#bib.bib17)]。先前的研究[[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]
    在非 LLM 的声谱图生成模型中提出了解决方案，通过约束 CNN 基于 TTS 模型和 LSTM 基于模型（如 Tacotron [[7](#bib.bib7)]
    和 Flowtron [[25](#bib.bib25)]）中的编码器-解码器注意力层，来学习文本和语音符号之间更严格的对齐。虽然这些技术显示出有前景的结果，但它们不能直接应用于包含多个交叉注意力层和每层多个头的变压器模型，并生成离散代码而非连续声谱图。
- en: 3 Methodology
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: Our TTS model is an encoder-decoder LLM that is trained to predict acoustic
    codes of the target speech given the tokenized text input and acoustic codes of
    a reference audio from the target speaker. In this section, we first describe
    the tokenized representations used for text and speech. Next, we describe our
    model architecture and prompting setup for TTS. Finally, we propose a training
    procedure to learn robust text and speech alignment in the LLM.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 TTS 模型是一个编码器-解码器 LLM，训练以预测目标语音的声学代码，给定标记化的文本输入和目标说话者的参考音频的声学代码。在本节中，我们首先描述用于文本和语音的标记化表示。接下来，我们描述我们的模型架构和
    TTS 的提示设置。最后，我们提出了一种训练过程，以学习 LLM 中稳健的文本和语音对齐。
- en: 3.1 Tokenization
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 分词
- en: 'Speech: We utilize neural audio codec models to convert the raw speech signal
    into a tokenized representation. Given an audio signal $y=y_{1}\dots y_{t}$ codebooks
    and achieve high quality speech synthesis.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 语音：我们利用神经音频编解码器模型将原始语音信号转换为标记化表示。给定音频信号 $y=y_{1}\dots y_{t}$ 代码本，并实现高质量的语音合成。
- en: 'Text: For text, we use two tokenization schemes: sentence-piece [[29](#bib.bib29)]
    and phonemes. Sentence-piece tokens allows us to leverage pretrained text LLMs.
    To allow phoneme tokens as input, we expand the vocabulary and embedding space
    of the pretrained text-LLM to include additional tokens for phonemes. We train
    a single model to perform both phoneme to speech and sentence-piece to speech
    by prepending the text with the task prompt ``Phoneme TTS'''' or ``Text to Speech''''
    respectively.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本：对于文本，我们使用两种分词方案：句子片段 [[29](#bib.bib29)] 和音素。句子片段标记使我们能够利用预训练的文本 LLM。为了允许音素标记作为输入，我们扩展了预训练文本-LLM
    的词汇表和嵌入空间，以包括额外的音素标记。我们训练一个模型，通过分别在文本前加上任务提示 ``Phoneme TTS'' 或 ``Text to Speech''，来执行音素到语音和句子片段到语音的转换。
- en: 3.2 Model Overview
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模型概述
- en: Our model is based on the T5 architecture [[2](#bib.bib2)], with additional
    embedding layers and prediction heads to adapt it for TTS task. T5 is an encoder-decoder
    model, where the encoder is a non autoregressive bi-directional transformer and
    the decoder is an autoregressive transformer. Both the encoder and decoder networks
    contain $N_{l}$ transformer layers. Each layer within the encoder is composed
    of a self-attention module and a fully connected feed-forward network. In the
    decoder network, each layer adds an additional cross-attention module which performs
    multi-headed attention over the encoder's output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型基于 T5 架构 [[2](#bib.bib2)]，增加了额外的嵌入层和预测头，以适应 TTS 任务。T5 是一个编码器-解码器模型，其中编码器是一个非自回归的双向变压器，解码器是一个自回归变压器。编码器和解码器网络都包含
    $N_{l}$ 个变压器层。编码器中的每一层由一个自注意力模块和一个全连接的前馈网络组成。在解码器网络中，每一层增加了一个额外的交叉注意力模块，该模块对编码器的输出执行多头注意力。
- en: 'To perform multi-speaker TTS, the model takes as input the tokenized text (question),
    and the acoustic tokens of a reference audio from the target speaker (context);
    and outputs the acoustic tokens of the target audio (answer). We consider two
    design options in our experiments: feeding the context audio tokens to the encoder
    network with the question, or to the decoder network before the answer. We discuss
    the trade-offs between these two designs in Section [4](#S4 "4 Experiments ‣ Improving
    Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行多说话人TTS，模型以标记化的文本（问题）和来自目标说话人（上下文）的参考音频的声学标记作为输入；并输出目标音频的声学标记（答案）。我们在实验中考虑了两种设计选项：将上下文音频标记输入到编码器网络与问题一起，或者在答案之前输入到解码器网络。我们在第 [4](#S4
    "4 Experiments ‣ Improving Robustness of LLM-based Speech Synthesis by Learning
    Monotonic Alignment") 节中讨论了这两种设计之间的权衡。
- en: 'Note that the context and answer are represented by $N$ is computed as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到上下文和答案由$N$表示，计算方式为：
- en: '|  | $e_{t}=\sum_{i=1}^{N}\texttt{EmbedA}_{i}(C[t,i])$ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{t}=\sum_{i=1}^{N}\texttt{EmbedA}_{i}(C[t,i])$ |  |'
- en: 'Similarly, at the decoder output, predictions for each of the $N$. Finally
    we calculate the cross entropy loss for next-token prediction as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在解码器输出时，对于每个$N$的预测。最后，我们计算下一个标记预测的交叉熵损失为：
- en: '|  | $L=\textit{CE}(\textit{SoftMax}(y),\textit{answer})$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=\textit{CE}(\textit{SoftMax}(y),\textit{answer})$ |  |'
- en: Note that unlike past work [[4](#bib.bib4), [6](#bib.bib6)], our model does
    not use additional networks for handling multiple codebook predictions. Instead,
    we employ the delay pattern for representing RVQ tokens [[28](#bib.bib28)] to
    model codebook dependencies.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与过去的工作 [[4](#bib.bib4), [6](#bib.bib6)]不同，我们的模型没有使用额外的网络来处理多个代码簿预测。相反，我们使用延迟模式来表示RVQ标记 [[28](#bib.bib28)]，以建模代码簿依赖关系。
- en: 3.3 Alignment Learning
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 对齐学习
- en: When the T5 model is trained for TTS task using only the next token prediction
    loss, we observe that the attention-score matrix $A_{T\times M}$ is the number
    of encoder timesteps). That is, if we slice the attention-score matrix to include
    only the question time-steps, we observe higher attention-scores near the diagonal
    indicating the desirable monotonic alignment (Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Improving Robustness of LLM-based Speech Synthesis by Learning
    Monotonic Alignment")). However, attention errors in this implicitly learned alignment
    can cause missing or repeating words during inference, leading to hallucinations
    and inaccurate generations for challenging texts. Moreover, the alignment learning
    using only the next token prediction loss is often unstable and it can take several
    training iterations to learn a reasonable text and speech alignment, especially
    when training utterances are longer [[17](#bib.bib17), [24](#bib.bib24)]. We extend
    prior work [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)] and propose
    an alignment learning framework to guide multiple cross-attention heads of the
    T5 transformer model to learn robust alignment.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当T5模型仅使用下一个标记预测损失进行TTS任务训练时，我们观察到注意力评分矩阵$A_{T\times M}$（这是编码器时间步的数量）。也就是说，如果我们切片注意力评分矩阵，只包含问题的时间步，我们会观察到对角线附近有更高的注意力评分，表明期望的单调对齐（图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Improving Robustness of LLM-based Speech Synthesis
    by Learning Monotonic Alignment")）。然而，这种隐式学习的对齐中的注意力错误可能导致推断过程中缺失或重复单词，从而引发幻觉和对具有挑战性的文本生成不准确。此外，仅使用下一个标记预测损失进行对齐学习通常是不稳定的，可能需要多个训练迭代才能学到合理的文本和语音对齐，特别是当训练语句较长时 [[17](#bib.bib17),
    [24](#bib.bib24)]。我们扩展了之前的工作 [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]，提出了一个对齐学习框架，以引导T5变换器模型的多个交叉注意力头学习鲁棒的对齐。
- en: 3.3.1 Attention Prior
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 注意力先验
- en: To accelerate alignment learning, during initial training we multiply the attention-score
    matrices in the cross-attention heads with a static 2D beta-binomial prior. The
    2D beta-binomial prior is a near-diagonal heuristic matrix that is wider near
    the center and narrower near the corners. Multiplying the initially random attention
    matrices with such a prior, reduces the attention scores that are far-off the
    diagonal, providing a desirable monotonic initialization to the cross-attention
    scores.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速对齐学习，在初始训练期间，我们将交叉注意力头中的注意力评分矩阵与一个静态的2D贝塔-二项分布先验相乘。2D贝塔-二项分布先验是一个接近对角线的启发式矩阵，中心处较宽，角落处较窄。将最初随机的注意力矩阵与这样的先验相乘，可以减少远离对角线的注意力评分，为交叉注意力评分提供期望的单调初始化。
- en: 'Consider the attention-score matrix between the decoder and encoder timesteps
    $A_{T\times M}^{\textit{l,h}}$ is the number of question (text) timesteps. Given
    this prior, we obtain the re-scaled attention scores as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑解码器和编码器时间步之间的注意力分数矩阵 $A_{T\times M}^{\textit{l,h}}$ 是问题（文本）时间步的数量。给定这个先验，我们获得重新缩放的注意力分数如下：
- en: '|  | ${A}_{T\times M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}]\leftarrow A_{T\times
    M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}]\odot P_{T^{\prime}\times M^{\prime}}$
    |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | ${A}_{T\times M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}]\leftarrow A_{T\times
    M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}]\odot P_{T^{\prime}\times M^{\prime}}$
    |  |'
- en: where $q_{s}$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q_{s}$。
- en: 'We apply the prior to all cross-attention heads of each decoder layer. Since
    we do not know the target audio length during inference which is needed to compute
    the prior, we cannot use this prior during inference. Therefore, we apply the
    attention prior for the first $S_{1}$, the prior matrix is obtained as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将先验应用于每个解码器层的所有交叉注意力头。由于在推理过程中我们不知道目标音频长度（这是计算先验所需的），我们无法在推理期间使用这个先验。因此，我们将注意力先验应用于前
    $S_{1}$ 步，先验矩阵如下获得：
- en: '|  | $1$2 |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This annealing procedure is necessary to ensure stability during training. Turning
    off the prior without annealing causes the loss curve to spike, since the decoder
    expects re-scaled attention scores for making valid predictions. In our experiments,
    we set $S_{1}=8000$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个退火过程是确保训练期间稳定性所必需的。如果在没有退火的情况下关闭先验，会导致损失曲线急剧上升，因为解码器期望重新缩放的注意力分数来进行有效的预测。在我们的实验中，我们设置
    $S_{1}=8000$。
- en: 3.3.2 Alignment Loss
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 对齐损失
- en: 'The soft alignment matrix between the text and audio timesteps can be obtained
    by taking softmax of the sliced attention-score matrix over the text dimension:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对文本维度上的切片注意力分数矩阵进行 softmax，可以获得文本和音频时间步之间的软对齐矩阵：
- en: '|  | $A^{\textit{soft}_{l,h}}_{T^{\prime}\times M^{\prime}}=\textit{Softmax}({A}_{T\times
    M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}])$ |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $A^{\textit{soft}_{l,h}}_{T^{\prime}\times M^{\prime}}=\textit{Softmax}({A}_{T\times
    M}^{\textit{l,h}}[a_{s}:a_{e},q_{s}:q_{e}])$ |  |'
- en: An $i^{\textit{th}}$. If we sample a prediction from such a distribution at
    each answer timestep, it is desirable that the resulting sequence of text timesteps
    is monotonic. Since the length of the answer is typically longer than the text,
    there can be multiple valid monotnic reductions of the alignment matrix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第 $i^{\textit{th}}$。如果我们在每个回答时间步从这样的分布中采样一个预测，理想情况下，结果的文本时间步序列是单调的。由于答案的长度通常比文本长，对齐矩阵可以有多个有效的单调缩减。
- en: 'To encourage valid monotonic sampling from the alignment matrix, we calculate
    the likelihood of all possible monotonic reductions using the Connectionist Temporal
    Classification (CTC) algorithm. That is, given the alignment matrix $A^{\textit{soft}_{l,h}}_{T^{\prime}\times
    M^{\prime}}$, we obtain the alignment loss for a decoder layer and head as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励从对齐矩阵中进行有效的单调采样，我们使用 Connectionist Temporal Classification (CTC) 算法计算所有可能单调缩减的似然值。也就是说，给定对齐矩阵
    $A^{\textit{soft}_{l,h}}_{T^{\prime}\times M^{\prime}}$，我们获得解码器层和头的对齐损失如下：
- en: '|  | $L_{\textit{align}}^{l,h}=\textit{CTCLoss}(A^{\textit{soft}_{l,h}}_{T^{\prime}\times
    M^{\prime}},q_{M^{\prime}})$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\textit{align}}^{l,h}=\textit{CTCLoss}(A^{\textit{soft}_{l,h}}_{T^{\prime}\times
    M^{\prime}},q_{M^{\prime}})$ |  |'
- en: where $q_{M^{\prime}}=\{1,2,\dots M^{\prime}\}$ over which we wish to enforce
    monotonic alignment. That is,
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q_{M^{\prime}}=\{1,2,\dots M^{\prime}\}$，我们希望对其施加单调对齐。也就是说，
- en: '|  | $L_{\textit{align}}=\sum_{\begin{subarray}{c}l,h\in\mathbb{{P}}\end{subarray}}L_{\textit{align}}^{l,h}$
    |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\textit{align}}=\sum_{\begin{subarray}{c}l,h\in\mathbb{{P}}\end{subarray}}L_{\textit{align}}^{l,h}$
    |  |'
- en: For set $\mathbb{{P}}$ to all cross-attention heads in each layer in our experiments.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们实验中每层的所有交叉注意力头，设定 $\mathbb{{P}}$。
- en: 4 Experiments
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Datasets and Models
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集和模型
- en: We train our T5-TTS models on a data-blend containing $1.8k$ steps optimized
    with a fixed learning rate of $1e-4$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在包含 $1.8k$ 步的数据混合上训练我们的 T5-TTS 模型，优化使用固定学习率 $1e-4$。
- en: 4.2 Results
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: 'Alignment Learning: To assess the efficacy of our alignment learning method
    (Section [3.3](#S3.SS3 "3.3 Alignment Learning ‣ 3 Methodology ‣ Improving Robustness
    of LLM-based Speech Synthesis by Learning Monotonic Alignment")), we train three
    variants of our T5 TTS model using the spectral codec: 1) T5-TTS (No Prior, No
    $L_{\textit{align}})$, we obtain monotonic but unaligned attention maps. leading
    to no speech synthesis.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐学习：为了评估我们的对齐学习方法（第[3.3节](#S3.SS3 "3.3 Alignment Learning ‣ 3 Methodology ‣
    Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment)")的有效性，我们使用光谱编解码器训练了三种变体的T5
    TTS模型：1）T5-TTS（No Prior, No $L_{\textit{align}})$，我们获得了单调但未对齐的注意力图，从而无法进行语音合成。
- en: 'Table 1: TTS results on seen and unseen speakers for different T5-TTS models.
    Lower CER(%) & WER(%) indicate higher intelligibility. Higher SSIM indicates higher
    speaker similarity to ground-truth audio.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同T5-TTS模型在已知和未知发言人上的TTS结果。CER(%)和WER(%)越低表示可懂度越高。SSIM越高表示与真实音频的发言人相似性越高。
- en: '| Eval Set | Model | Context | CER $\downarrow$ |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 评估集 | 模型 | 上下文 | CER $\downarrow$ |'
- en: '|  | Ground Truth |  | $1.03$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | 真实值 |  | $1.03$ |'
- en: '| LibriTTS | T5-TTS (No Prior, No $L_{\textit{align}})$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| LibriTTS | T5-TTS（No Prior, No $L_{\textit{align}})$ |'
- en: '| (Seen | T5-TTS (W Prior, No $L_{\textit{align}})$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| （已知 | T5-TTS（W Prior, No $L_{\textit{align}})$ |'
- en: '| Speakers) | T5-TTS (W Prior, W $L_{\textit{align}})$ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 发言人） | T5-TTS（W Prior, W $L_{\textit{align}})$ |'
- en: '|  | T5-TTS (W Prior, W $L_{\textit{align}})$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | T5-TTS（W Prior, W $L_{\textit{align}})$ |'
- en: '| VCTK | Ground Truth |  | $0.50$ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| VCTK | 真实值 |  | $0.50$ |'
- en: '| (Unseen | T5-TTS (W Prior, W $L_{\textit{align}})$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| （未知 | T5-TTS（W Prior, W $L_{\textit{align}})$ |'
- en: '| Speakers) | T5-TTS (W Prior, W $L_{\textit{align}})$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 发言人） | T5-TTS（W Prior, W $L_{\textit{align}})$ |'
- en: We evaluate the models on a set of seen and unseen speakers. For seen speakers,
    we use $200$ utterances per speaker. For each utterance, we synthesize two audios
    using either the sentence piece text tokenizer or the phoneme tokenizer. We evaluate
    the synthesized speech on intelligibility and speaker similarity. For intelligibility,
    we transcribe the synthesized audio through a Conformer-Transducer ASR model ²²2[https://hf.co/nvidia/stt_en_conformer_transducer_large](https://hf.co/nvidia/stt_en_conformer_transducer_large)
    and compute the CER and WER between the ASR transcript and the ground-truth text.
    For speaker similarity (SSIM), we compute the cosine similarity between the embeddings
    of the synthesized speech and target ground-truth audio obtained from WavLM [[35](#bib.bib35)]
    speaker verification model ³³3[https://hf.co/microsoft/wavlm-base-plus-sv](https://hf.co/microsoft/wavlm-base-plus-sv).
    We report the results in Table [1](#S4.T1 "Table 1 ‣ 4.2 Results ‣ 4 Experiments
    ‣ Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment").
    While all three models achieve high speaker similarity for seen speakers, the
    intelligibility metrics improve as we incorporate attention prior and alignment
    loss during training. For unseen speakers, we observe a higher speaker similarity
    and intelligibility when the context is fed to the T5 decoder instead of the encoder.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一组已知和未知的发言人上评估模型。对于已知发言人，我们使用每位发言人的$200$个发言。对于每个发言，我们使用句子片段文本标记器或音素标记器合成两个音频。我们在可懂度和发言人相似性上评估合成的语音。对于可懂度，我们通过Conformer-Transducer
    ASR模型²²2[https://hf.co/nvidia/stt_en_conformer_transducer_large](https://hf.co/nvidia/stt_en_conformer_transducer_large)转录合成的音频，并计算ASR转录文本与真实文本之间的CER和WER。对于发言人相似性（SSIM），我们计算合成语音的嵌入与从WavLM[[35](#bib.bib35)]发言人验证模型³³3[https://hf.co/microsoft/wavlm-base-plus-sv](https://hf.co/microsoft/wavlm-base-plus-sv)获得的目标真实音频之间的余弦相似性。我们在表[1](#S4.T1
    "Table 1 ‣ 4.2 Results ‣ 4 Experiments ‣ Improving Robustness of LLM-based Speech
    Synthesis by Learning Monotonic Alignment")中报告了结果。虽然所有三个模型在已知发言人的发言人相似性方面都取得了高分，但随着我们在训练中引入注意力先验和对齐损失，可懂度指标得到了改善。对于未知发言人，当上下文传递给T5解码器而不是编码器时，我们观察到更高的发言人相似性和可懂度。
- en: 'Challenging Texts and Comparison against Prior Work: As shown in Table [2](#S4.T2
    "Table 2 ‣ 4.2 Results ‣ 4 Experiments ‣ Improving Robustness of LLM-based Speech
    Synthesis by Learning Monotonic Alignment"), the improvement in intelligibility
    is even more significant when we consider challenging text inputs with repeating
    words. We compare our models (using decoder context) with three open source LLM-based
    TTS models using the inference code and released checkpoints [[19](#bib.bib19),
    [36](#bib.bib36), [37](#bib.bib37)]. For this evaluation we consider a set of
    $100$ confidence intervals indicates our model outperforms prior LLM-based TTS
    models considered in our study. We encourage readers to listen to audio examples
    linked in the footnote of the first page.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '挑战文本和与先前工作的比较: 如表 [2](#S4.T2 "表 2 ‣ 4.2 结果 ‣ 4 实验 ‣ 通过学习单调对齐来提高基于 LLM 的语音合成的鲁棒性")
    所示，当我们考虑具有重复单词的挑战性文本输入时，智能性的改善更加显著。我们将我们的模型（使用解码器上下文）与三个开源 LLM 基于 TTS 的模型进行比较，这些模型使用推理代码和发布的检查点
    [[19](#bib.bib19), [36](#bib.bib36), [37](#bib.bib37)]。在这次评估中，我们考虑了一组 $100$ 的置信区间，表明我们的模型优于我们研究中考虑的先前
    LLM 基于 TTS 的模型。我们鼓励读者收听第一页脚注中链接的音频示例。'
- en: 'Table 2: Comparison of different T5-TTS models against prior LLM-based TTS
    models. Intelligibility metrics (WER, CER, character insertions, deletions, substitutions
    (%)) are evaluated on $100$ challenging texts. Naturalness MOS is calculated on
    subset of harvard sentences.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 不同 T5-TTS 模型与先前 LLM 基于 TTS 模型的比较。智能性指标（WER、CER、字符插入、删除、替换（%））在 $100$ 个具有挑战性的文本上进行评估。自然度
    MOS 在哈佛句子子集上计算。'
- en: '|  | *Intelligibility* $\downarrow$ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | *智能性* $\downarrow$ |'
- en: '| --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Model | WER | CER | Ins | Del | Subs | MOS |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | WER | CER | 插入 | 删除 | 替换 | MOS |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| VALL-E-X [[18](#bib.bib18)] | $16.8$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| VALL-E-X [[18](#bib.bib18)] | $16.8$ |'
- en: '| Bark [[36](#bib.bib36)] | $19.1$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Bark [[36](#bib.bib36)] | $19.1$ |'
- en: '| SpeechT5 [[19](#bib.bib19)] | $13.5$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SpeechT5 [[19](#bib.bib19)] | $13.5$ |'
- en: '| T5-TTS (No Prior, No $L_{\textit{align}})$ | - |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| T5-TTS（无先验，无 $L_{\textit{align}}$） | - |'
- en: '| T5-TTS (W Prior, No $L_{\textit{align}})$ | - |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| T5-TTS（有先验，无 $L_{\textit{align}}$） | - |'
- en: '| T5-TTS (W Prior, W $\mathbf{L_{\textit{{align}}}}\textbf{)}$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| T5-TTS（有先验，有 $\mathbf{L_{\textit{{align}}}}\textbf{)}$ |'
- en: 'Codec Choice: We train three T5-TTS models with alignment learning on the three
    audio codecs and report results on seen speakers in Table [3](#S4.T3 "Table 3
    ‣ 4.2 Results ‣ 4 Experiments ‣ Improving Robustness of LLM-based Speech Synthesis
    by Learning Monotonic Alignment"). We find that both spectral codec and Dac significantly
    outperform Encodec in terms of audio naturalness. Spectral codec streamlines training
    by independently predicting codebooks in parallel, unlike the delay pattern scheme
    needed for Encodec/Dac. Additionally, spectral codec enhances synthesized speech
    intelligibility, demonstrated by reduced CER/WER.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '编解码器选择: 我们在三个音频编解码器上训练了三个 T5-TTS 模型，进行对齐学习，并在表 [3](#S4.T3 "表 3 ‣ 4.2 结果 ‣ 4
    实验 ‣ 通过学习单调对齐来提高基于 LLM 的语音合成的鲁棒性") 中报告了在已见说话者上的结果。我们发现，无论是频谱编解码器还是 Dac，在音频自然度方面都显著优于
    Encodec。频谱编解码器通过独立预测码本并行化训练，从而简化了训练过程，而 Encodec/Dac 需要延迟模式方案。此外，频谱编解码器通过减少 CER/WER
    提升了合成语音的智能性。'
- en: 'Table 3: Comparison of T5-TTS (W Prior, W $L_{\textit{align}})$ models trained
    with different audio codecs. N: number of codebooks, FPS: Frames Per Second, TPS:
    Tokens Per Second. All codecs use 10-bit tokens.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 使用不同音频编解码器训练的 T5-TTS（有先验，有 $L_{\textit{align}}$）模型的比较。N: 码本数量，FPS: 每秒帧数，TPS:
    每秒令牌数。所有编解码器使用 10 位令牌。'
- en: '| Codec | FPS | N | TPS | CER $\downarrow$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 编解码器 | FPS | N | TPS | CER $\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Encodec | 75 | 8 | 600 | $4.01$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Encodec | 75 | 8 | 600 | $4.01$ |'
- en: '| Dac | 86 | 9 | 774 | $6.72$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Dac | 86 | 9 | 774 | $6.72$ |'
- en: '| Spectral codec | 86 | 8 | 688 | $\mathbf{2.16}$ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 频谱编解码器 | 86 | 8 | 688 | $\mathbf{2.16}$ |'
- en: 5 Conclusion
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present a T5-TTS model that can learn robust text and speech alignment without
    modifying the model architecture or requiring ground-truth text duration. We identify
    that attention heads in LLM-based TTS models implicitly learn text and speech
    alignment and can be guided to monotonically attend over the text input. Our experiments
    demonstrate that our alignment learning procedure improves the reliability of
    TTS synthesis, especially for challenging text inputs and outperforms prior LLM-based
    TTS models on both intelligibility and naturalness.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一个T5-TTS模型，该模型能够在不修改模型架构或不需要真实文本时长的情况下学习稳健的文本和语音对齐。我们发现，基于LLM的TTS模型中的注意力头会隐式学习文本和语音对齐，并且可以引导其单调地关注文本输入。我们的实验表明，我们的对齐学习过程提高了TTS合成的可靠性，特别是在处理具有挑战性的文本输入时，并且在可懂度和自然度方面优于以前的基于LLM的TTS模型。
- en: 6 Acknowledgements
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 致谢
- en: We would also like to thank Ryan Langman for developing the spectral codec model
    that was used in our TTS model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想感谢 Ryan Langman 开发了我们TTS模型中使用的光谱编解码模型。
- en: References
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever *et al.*, ``Improving
    language understanding by generative pre-training,'''' *OpenAI blog*, 2018.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever *等人*，``通过生成性预训练提高语言理解,''''
    *OpenAI blog*, 2018。'
- en: '[2] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. J. Liu, ``Exploring the limits of transfer learning with a unified
    text-to-text transformer,'''' *The Journal of Machine Learning Research*, 2020.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, 和 P. J. Liu，``探索统一文本到文本转换器的迁移学习极限,'''' *The Journal of Machine Learning
    Research*, 2020。'
- en: '[3] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi,
    D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi *et al.*, ``Audiolm: a language
    modeling approach to audio generation,'''' *IEEE/ACM Transactions on Audio, Speech,
    and Language Processing*, 2023.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi,
    D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi *等人*，``Audiolm: 一种语言建模方法用于音频生成,''''
    *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, 2023。'
- en: '[4] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang,
    J. Li *et al.*, ``Neural codec language models are zero-shot text to speech synthesizers,''''
    *arXiv:2301.02111*, 2023.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H.
    Wang, J. Li *等人*，``神经编解码器语言模型作为零样本文本到语音合成器,'''' *arXiv:2301.02111*, 2023。'
- en: '[5] X. Wang, M. Thakker, Z. Chen, N. Kanda, S. E. Eskimez, S. Chen, M. Tang,
    S. Liu, J. Li, and T. Yoshioka, ``Speechx: Neural codec language model as a versatile
    speech transformer,'''' *arXiv:2308.06873*, 2023.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Wang, M. Thakker, Z. Chen, N. Kanda, S. E. Eskimez, S. Chen, M. Tang,
    S. Liu, J. Li, 和 T. Yoshioka，``Speechx: 神经编解码器语言模型作为多功能语音变换器,'''' *arXiv:2308.06873*,
    2023。'
- en: '[6] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao,
    J. Bian, X. Wu *et al.*, ``Uniaudio: An audio foundation model toward universal
    audio generation,'''' *arXiv:2310.00704*, 2023.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao,
    J. Bian, X. Wu *等人*，``Uniaudio: 一种通用音频生成的音频基础模型,'''' *arXiv:2310.00704*, 2023。'
- en: '[7] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang,
    Y. Xiao, Z. Chen, S. Bengio *et al.*, ``Tacotron: Towards end-to-end speech synthesis,''''
    in *INTERSPEECH*, 2017.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z.
    Yang, Y. Xiao, Z. Chen, S. Bengio *等人*，``Tacotron: 实现端到端语音合成,'''' 在 *INTERSPEECH*,
    2017。'
- en: '[8] A. Łańcucki, ``Fastpitch: Parallel text-to-speech with pitch prediction,''''
    in *ICASSP*, 2021.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Łańcucki，``Fastpitch: 基于音高预测的并行文本到语音,'''' 在 *ICASSP*, 2021。'
- en: '[9] P. Neekhara, S. Hussain, S. Dubnov, F. Koushanfar, and J. McAuley, ``Expressive
    neural voice cloning,'''' in *Asian Conference on Machine Learning*.   PMLR, 2021.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. Neekhara, S. Hussain, S. Dubnov, F. Koushanfar, 和 J. McAuley，``表现力神经语音克隆,''''
    在 *Asian Conference on Machine Learning*。PMLR, 2021。'
- en: '[10] R. Valle, J. Li, R. Prenger, and B. Catanzaro, ``Mellotron: Multispeaker
    expressive voice synthesis by conditioning on rhythm, pitch and global style tokens,''''
    *ICASSP*, 2020.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Valle, J. Li, R. Prenger, 和 B. Catanzaro，``Mellotron: 通过条件化节奏、音调和全局风格标记进行多说话人表现力语音合成,''''
    *ICASSP*, 2020。'
- en: '[11] E. Casanova, J. Weber, C. Shulby, A. Junior, E. Gölge, and M. A. Ponti,
    ``Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion
    for everyone,'''' in *ICML*.   PMLR, 2022.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] E. Casanova, J. Weber, C. Shulby, A. Junior, E. Gölge, 和 M. A. Ponti，``Yourtts:
    实现零样本多说话人 TTS 和零样本语音转换,'''' 在 *ICML*。PMLR, 2022。'
- en: '[12] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, ``High fidelity neural
    audio compression,'''' *Transactions on Machine Learning Research*, 2023.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Défossez, J. Copet, G. Synnaeve, 和 Y. Adi，``高保真神经音频压缩,'''' *Transactions
    on Machine Learning Research*, 2023。'
- en: '[13] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar, ``High-fidelity
    audio compression with improved rvqgan,'''' *Advances in Neural Information Processing
    Systems*, 2024.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar 和 K. Kumar，``高保真音频压缩与改进的rvqgan，''''
    *神经信息处理系统进展*，2024年。'
- en: '[14] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, ``Soundstream:
    An end-to-end neural audio codec,'''' *IEEE/ACM Transactions on Audio, Speech,
    and Language Processing*, 2021.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund 和 M. Tagliasacchi，``Soundstream:
    一种端到端的神经音频编解码器，'''' *IEEE/ACM音频、语音与语言处理交易*，2021年。'
- en: '[15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
    A. Madotto, and P. Fung, ``Survey of hallucination in natural language generation,''''
    *Association for Computing Machinery*, 2023.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
    A. Madotto 和 P. Fung，``自然语言生成中的幻觉调查，'''' *计算机协会*，2023年。'
- en: '[16] R. Azamfirei, S. R. Kudchadkar, and J. Fackler, ``Large language models
    and the perils of their hallucinations,'''' *Critical Care*, 2023.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] R. Azamfirei, S. R. Kudchadkar 和 J. Fackler，``大型语言模型及其幻觉的危险，'''' *危重护理*，2023年。'
- en: '[17] Y. Song, Z. Chen, X. Wang, Z. Ma, and X. Chen, ``Ella-v: Stable neural
    codec language modeling with alignment-guided sequence reordering,'''' *arXiv:2401.07333*,
    2024.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Song, Z. Chen, X. Wang, Z. Ma 和 X. Chen，``Ella-v: 具有对齐引导序列重排的稳定神经编解码语言建模，''''
    *arXiv:2401.07333*，2024年。'
- en: '[18] Z. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu, H. Wang,
    J. Li *et al.*, ``Speak foreign languages with your own voice: Cross-lingual neural
    codec language modeling,'''' *arXiv:2303.03926*, 2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Z. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu, H.
    Wang, J. Li *等人*，``用自己的声音说外语：跨语言神经编解码语言建模，'''' *arXiv:2303.03926*，2023年。'
- en: '[19] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko, Q. Li,
    Y. Zhang *et al.*, ``Speecht5: Unified-modal encoder-decoder pre-training for
    spoken language processing,'''' in *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*, 2022,
    pp. 5723–5738.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko, Q. Li,
    Y. Zhang *等人*，``Speecht5: 统一模态编码器-解码器预训练用于口语语言处理，'''' 见 *第60届计算语言学协会年会会议论文集（第1卷：长篇论文）*，2022年，第5723–5738页。'
- en: '[20] W. C. Huang, S. W. Yang, T. Hayashi, H. Y. Lee, S. Watanabe, and T. Toda,
    ``S3prl-vc: Open-source voice conversion framework with self-supervised speech
    representations,'''' in *ICASSP*, 2022.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] W. C. Huang, S. W. Yang, T. Hayashi, H. Y. Lee, S. Watanabe 和 T. Toda，``S3prl-vc:
    具有自监督语音表示的开源语音转换框架，'''' 见 *ICASSP*，2022年。'
- en: '[21] S. Hussain, P. Neekhara, J. Huang, J. Li, and B. Ginsburg, ``Ace-vc: Adaptive
    and controllable voice conversion using explicitly disentangled self-supervised
    speech representations,'''' in *ICASSP*, 2023.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Hussain, P. Neekhara, J. Huang, J. Li 和 B. Ginsburg，``Ace-vc: 使用明确分离的自监督语音表示的自适应和可控语音转换，''''
    见 *ICASSP*，2023年。'
- en: '[22] H. Tachibana, K. Uenoyama, and S. Aihara, ``Efficiently trainable text-to-speech
    system based on deep convolutional networks with guided attention,'''' in *ICASSP*,
    2018.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Tachibana, K. Uenoyama 和 S. Aihara，``基于深度卷积网络与引导注意力的高效可训练文本到语音系统，''''
    见 *ICASSP*，2018年。'
- en: '[23] K. J. Shih, R. Valle, R. Badlani, A. Lancucki, W. Ping, and B. Catanzaro,
    ``Rad-tts: Parallel flow-based tts with robust alignment learning and diverse
    synthesis,'''' in *ICML Workshop on Invertible Neural Networks, Normalizing Flows,
    and Explicit Likelihood Models*, 2021.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] K. J. Shih, R. Valle, R. Badlani, A. Lancucki, W. Ping 和 B. Catanzaro，``Rad-tts:
    基于流的并行TTS，具有强健的对齐学习和多样的合成，'''' 见 *ICML可逆神经网络、归一化流和显式似然模型研讨会*，2021年。'
- en: '[24] R. Badlani, A. Łańcucki, K. J. Shih, R. Valle, W. Ping, and B. Catanzaro,
    ``One tts alignment to rule them all,'''' in *ICASSP*, 2022.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] R. Badlani, A. Łańcucki, K. J. Shih, R. Valle, W. Ping 和 B. Catanzaro，``一个对齐方案统治一切，''''
    见 *ICASSP*，2022年。'
- en: '[25] R. Valle, K. J. Shih, R. Prenger, and B. Catanzaro, ``Flowtron: an autoregressive
    flow-based generative network for text-to-speech synthesis,'''' in *ICLR*, 2020.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] R. Valle, K. J. Shih, R. Prenger 和 B. Catanzaro，``Flowtron: 一种自回归流基生成网络用于文本到语音合成，''''
    见 *ICLR*，2020年。'
- en: '[26] F. Mentzer, D. Minnen, E. Agustsson, and M. Tschannen, ``Finite scalar
    quantization: Vq-vae made simple,'''' in *ICLR*, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] F. Mentzer, D. Minnen, E. Agustsson 和 M. Tschannen，``有限标量量化：简化版VQ-VAE，''''
    见 *ICLR*，2023年。'
- en: '[27] R. Langman, A. Jukić, K. Dhawan, N. R. Koluguri, and B. Ginsburg, ``Spectral
    codecs: Spectrogram-based audio codecs for high quality speech synthesis,''''
    2024.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] R. Langman, A. Jukić, K. Dhawan, N. R. Koluguri 和 B. Ginsburg，``谱编码器：基于频谱图的高质量语音合成编解码器，''''
    2024年。'
- en: '[28] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and
    A. Défossez, ``Simple and controllable music generation,'''' *Advances in Neural
    Information Processing Systems*, vol. 36, 2024.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi 和 A.
    Défossez, ``简单且可控的音乐生成,'''' *神经信息处理系统进展*，第36卷，2024年。'
- en: '[29] T. Kudo and J. Richardson, ``Sentencepiece: A simple and language independent
    subword tokenizer and detokenizer for neural text processing,'''' in *Proceedings
    of the 2018 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, 2018.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] T. Kudo 和 J. Richardson, ``Sentencepiece: 一个简单且语言独立的子词分词器和去分词器用于神经文本处理,''''
    见于 *2018年自然语言处理实证方法会议: 系统演示*，2018年。'
- en: '[30] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and
    Y. Wu, ``Libritts: A corpus derived from librispeech for text-to-speech,'''' *INTERSPEECH*,
    2019.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen 和 Y.
    Wu, ``Libritts: 从LibriSpeech派生的文本到语音语料库,'''' *INTERSPEECH*，2019年。'
- en: '[31] E. Bakhturina, V. Lavrukhin, B. Ginsburg, and Y. Zhang, ``Hi-Fi Multi-Speaker
    English TTS Dataset,'''' in *INTERSPEECH*, 2021.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] E. Bakhturina, V. Lavrukhin, B. Ginsburg 和 Y. Zhang, ``高保真多扬声器英语TTS数据集,''''
    见于 *INTERSPEECH*，2021年。'
- en: '[32] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, ``MLS: A Large-Scale
    Multilingual Dataset for Speech Research,'''' in *INTERSPEECH*, 2020.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve 和 R. Collobert, ``MLS: 一个大规模多语言语音研究数据集,''''
    见于 *INTERSPEECH*，2020年。'
- en: '[33] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy, ``The Pile: An 800gb
    dataset of diverse text for language modeling,'''' *arXiv:2101.00027*, 2020.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, S. Presser 和 C. Leahy, ``The Pile: 一个800GB多样文本数据集用于语言建模,''''
    *arXiv:2101.00027*，2020年。'
- en: '[34] C. Veaux, J. Yamagishi, and K. Macdonald, ``CSTR VCTK corpus: English
    multi-speaker corpus for CSTR voice cloning toolkit,'''' 2017.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C. Veaux, J. Yamagishi 和 K. Macdonald, ``CSTR VCTK语料库: CSTR语音克隆工具包的英语多扬声器语料库,''''
    2017年。'
- en: '[35] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka,
    X. Xiao *et al.*, ``Wavlm: Large-scale self-supervised pre-training for full stack
    speech processing,'''' *IEEE Journal of Selected Topics in Signal Processing*,
    2022.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T.
    Yoshioka, X. Xiao *等*, ``Wavlm: 大规模自监督预训练用于全栈语音处理,'''' *IEEE信号处理选择主题期刊*，2022年。'
- en: '[36] SunoAI, ``Bark audio generation model,'''' 2023\. [Online]. Available:
    [https://github.com/suno-ai/bark](https://github.com/suno-ai/bark)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] SunoAI, ``Bark音频生成模型,'''' 2023\. [在线]. 可用: [https://github.com/suno-ai/bark](https://github.com/suno-ai/bark)'
- en: '[37] Songting, ``VALL-E-X,'''' 2023\. [Online]. Available: [https://github.com/Plachtaa/VALL-E-X](https://github.com/Plachtaa/VALL-E-X)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Songting, ``VALL-E-X,'''' 2023\. [在线]. 可用: [https://github.com/Plachtaa/VALL-E-X](https://github.com/Plachtaa/VALL-E-X)'
- en: '[38] ``IEEE recommended practice for speech quality measurements,'''' *IEEE
    Transactions on Audio and Electroacoustics*, vol. 17, no. 3, pp. 225–246, 1969.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] ``IEEE推荐的语音质量测量实践,'''' *IEEE音频与电声学期刊*，第17卷，第3期，225–246页，1969年。'
