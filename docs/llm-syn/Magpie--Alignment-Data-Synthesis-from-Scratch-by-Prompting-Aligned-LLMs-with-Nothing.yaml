- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:53:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:53:48'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Magpie: 从头开始通过提示对齐的LLM生成对齐数据'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08464](https://ar5iv.labs.arxiv.org/html/2406.08464)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.08464](https://ar5iv.labs.arxiv.org/html/2406.08464)
- en: Zhangchen Xu^♠   Fengqing Jiang ^♠   Luyao Niu^♠   Yuntian Deng ^♢
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 张晨徐^♠   冯庆江 ^♠   陆耀牛^♠   云天邓 ^♢
- en: Radha Poovendran^♠   Yejin Choi^♠^♢   Bill Yuchen Lin^♢
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 拉达·普文德兰^♠   叶进·崔^♠^♢   比尔·宇辰·林^♢
- en: ^♠University of Washington   ^♢Allen Institute for AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^♠华盛顿大学   ^♢艾伦人工智能研究所
- en: '![[Uncaptioned image]](img/32da8907c985a1db6b9511760d9d921c.png) [https://magpie-align.github.io/](https://magpie-align.github.io/)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/32da8907c985a1db6b9511760d9d921c.png) [https://magpie-align.github.io/](https://magpie-align.github.io/)'
- en: '![[Uncaptioned image]](img/23dbbe8c72cb1ed239df9dcac3f93cb1.png) [https://hf.co/magpie-align](https://hf.co/magpie-align)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/23dbbe8c72cb1ed239df9dcac3f93cb1.png) [https://hf.co/magpie-align](https://hf.co/magpie-align)'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: High-quality instruction data is critical for aligning large language models
    (LLMs). Although some models, such as Llama-3-Instruct, have open weights, their
    alignment data remain private, which hinders the democratization of AI. High human
    labor costs and a limited, predefined scope for prompting prevent existing open-source
    data creation methods from scaling effectively, potentially limiting the diversity
    and quality of public alignment datasets. Is it possible to synthesize high-quality
    instruction data at scale by extracting it directly from an aligned LLM? We present
    a self-synthesis method for generating large-scale alignment data named Magpie.
    Our key observation is that aligned LLMs like Llama-3-Instruct can generate a
    user query when we input only the left-side templates up to the position reserved
    for user messages, thanks to their auto-regressive nature. We use this method
    to prompt Llama-3-Instruct and generate 4 million instructions along with their
    corresponding responses. We perform a comprehensive analysis of the extracted
    data and select 300K high-quality instances. To compare Magpie data with other
    public instruction datasets (e.g., ShareGPT, WildChat, Evol-Instruct, UltraChat,
    OpenHermes, Tulu-V2-Mix), we fine-tune Llama-3-8B-Base with each dataset and evaluate
    the performance of the fine-tuned models. Our results indicate that in some tasks,
    models fine-tuned with Magpie perform comparably to the official Llama-3-8B-Instruct,
    despite the latter being enhanced with 10 million data points through supervised
    fine-tuning (SFT) and subsequent feedback learning. We also show that using Magpie
    solely for SFT can surpass the performance of previous public datasets utilized
    for both SFT and preference optimization, such as direct preference optimization
    with UltraFeedback. This advantage is evident on alignment benchmarks such as
    AlpacaEval, ArenaHard, and WildBench, and importantly, it is achieved without
    compromising performance on reasoning tasks like MMLU-Redux, despite the alignment
    tax.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量的指令数据对于对齐大型语言模型（LLMs）至关重要。尽管一些模型，如Llama-3-Instruct，具有公开的权重，但其对齐数据仍然是私密的，这阻碍了AI的民主化。高昂的人力成本和有限的预定义提示范围阻止了现有开源数据创建方法的有效扩展，这可能限制了公共对齐数据集的多样性和质量。是否有可能通过直接从对齐的LLM中提取数据来大规模合成高质量的指令数据？我们提出了一种名为Magpie的大规模对齐数据自我合成方法。我们的关键观察是，由于其自回归特性，像Llama-3-Instruct这样的对齐LLM可以在仅输入左侧模板到预留用户消息的位置时生成用户查询。我们利用这一方法提示Llama-3-Instruct，生成400万条指令及其对应的响应。我们对提取的数据进行了全面分析，并选择了30万个高质量实例。为了将Magpie数据与其他公共指令数据集（例如ShareGPT、WildChat、Evol-Instruct、UltraChat、OpenHermes、Tulu-V2-Mix）进行比较，我们用每个数据集对Llama-3-8B-Base进行了微调，并评估了微调模型的性能。我们的结果表明，在某些任务中，使用Magpie微调的模型在性能上与官方Llama-3-8B-Instruct相当，尽管后者通过监督微调（SFT）和后续反馈学习增强了1000万数据点。我们还展示了，仅使用Magpie进行SFT可以超越以前用于SFT和偏好优化的公共数据集的性能，如使用UltraFeedback进行的直接偏好优化。这一优势在对齐基准（如AlpacaEval、ArenaHard和WildBench）中显而易见，并且在不妥协推理任务（如MMLU-Redux）性能的情况下实现了对齐，尽管存在对齐成本。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/57858315bf45920b829ffc77f47753b1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/57858315bf45920b829ffc77f47753b1.png)'
- en: 'Figure 1: This figure illustrates the process of self-synthesizing instruction
    data from aligned LLMs (e.g., Llama-3-8B-Instruct) to create a high-quality instruction
    dataset. In Step 1, we input only the pre-query template into the aligned LLM
    and generate an instruction along with its response using auto-regressive generation.
    In Step 2, we use a combination of a post-query template and another pre-query
    template to wrap the instruction from Step 1, prompting the LLM to generate the
    query for the second turn. This completes the construction of the instruction
    dataset. Magpie efficiently generates diverse and high-quality instruction data.
    Our experimental results show that Magpie outperforms other public datasets for
    aligning Llama-3-8B-base.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：该图展示了从对齐的 LLM（例如 Llama-3-8B-Instruct）中自我合成指令数据以创建高质量指令数据集的过程。在步骤 1 中，我们将仅预查询模板输入对齐的
    LLM，并使用自回归生成生成指令及其响应。在步骤 2 中，我们使用后查询模板和另一个预查询模板的组合来包装步骤 1 中的指令，促使 LLM 生成第二轮的查询。这完成了指令数据集的构建。Magpie
    高效地生成多样化和高质量的指令数据。我们的实验结果显示，Magpie 在对齐 Llama-3-8B-base 方面优于其他公共数据集。
- en: Large language models (LLMs) such as GPT-4 [[1](#bib.bib1)] and Llama-3 [[40](#bib.bib40)]
    have become integral to AI applications due to their exceptional performance on
    a wide array of tasks by following instructions. The success of LLMs is heavily
    reliant on the data used for instruction fine-tuning, which equips them to handle
    a diverse range of tasks, including those not encountered during training. The
    effectiveness of this instruction tuning depends crucially on access to high-quality
    instruction datasets. However, the alignment datasets used for fine-tuning models
    like Llama-3-Instruct are typically private, even when the model weights are open,
    which impedes the democratization of AI and limits scientific research for understanding
    and enhancing LLM alignment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）如 GPT-4 [[1](#bib.bib1)] 和 Llama-3 [[40](#bib.bib40)] 由于其在遵循指令的各种任务中表现出色，已成为人工智能应用中不可或缺的一部分。LLMs
    的成功在很大程度上依赖于用于指令微调的数据，这些数据使它们能够处理多样化的任务，包括在训练过程中未曾遇到的任务。指令微调的效果在很大程度上取决于高质量指令数据集的获取。然而，像
    Llama-3-Instruct 这样的模型所使用的对齐数据集通常是私有的，即使模型权重是公开的，这也阻碍了人工智能的民主化，并限制了科学研究以理解和增强
    LLM 对齐的能力。
- en: To address the challenges in constructing such datasets, researchers have developed
    two main approaches. The first type of method involves human effort to generate
    and curate instruction data [[14](#bib.bib14), [26](#bib.bib26), [64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66)], which is both *time-consuming* and *labor-intensive*
    [[37](#bib.bib37)]. In contrast, the second type of method uses LLMs to produce
    synthetic instructions [[16](#bib.bib16), [31](#bib.bib31), [46](#bib.bib46),
    [47](#bib.bib47), [53](#bib.bib53), [55](#bib.bib55), [58](#bib.bib58), [59](#bib.bib59)].
    Although these methods reduce human effort, its success heavily depends on prompt
    engineering and the careful selection of initial seed questions. The *diversity*
    of synthetic data tends to decrease as the dataset size grows. Despite ongoing
    efforts, the scalable creation of high-quality and diverse instruction datasets
    continues to be a challenging problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对构建此类数据集的挑战，研究人员开发了两种主要方法。第一种方法涉及人工生成和策划指令数据 [[14](#bib.bib14), [26](#bib.bib26),
    [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)]，这既*耗时*又*劳动密集* [[37](#bib.bib37)]。与此相对的是第二种方法，使用
    LLMs 生成合成指令 [[16](#bib.bib16), [31](#bib.bib31), [46](#bib.bib46), [47](#bib.bib47),
    [53](#bib.bib53), [55](#bib.bib55), [58](#bib.bib58), [59](#bib.bib59)]。虽然这些方法减少了人工工作，但其成功在很大程度上依赖于提示工程和初始种子问题的仔细选择。合成数据的*多样性*往往随着数据集规模的增长而下降。尽管不断努力，高质量和多样化指令数据集的可扩展创建仍然是一个具有挑战性的问题。
- en: 'Is it possible to synthesize high-quality instructions at scale by directly
    extracting data from advanced aligned LLMs themselves? A typical input to an aligned
    LLM contains three key components: the pre-query template, the query, and the
    post-query template. For instance, an input to Llama-2-chat could be “[INST] Hi!
    [/INST]”, where [INST] is the pre-query template and [/INST] is the post-query
    template. These templates are predefined by the creators of the aligned LLMs to
    ensure the correct prompting of the models. We observe that when we only input
    the pre-query template to aligned LLMs such as Llama-3-Instruct, they self-synthesize
    a user query due to their auto-regressive nature. Our preliminary experiments
    indicate that these random user queries are of high quality and great diversity,
    suggesting that the abilities learned during the alignment process are effectively
    utilized.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有可能通过直接从高级对齐的大型语言模型（LLMs）中提取数据来大规模合成高质量的指令？一个典型的对齐LLM的输入包含三个关键组成部分：前查询模板、查询和后查询模板。例如，输入到Llama-2-chat的内容可能是“[INST]
    Hi! [/INST]”，其中[INST]是前查询模板，[/INST]是后查询模板。这些模板由对齐LLM的创建者预定义，以确保模型的正确提示。我们观察到，当仅将前查询模板输入到像Llama-3-Instruct这样的对齐LLM时，由于其自回归特性，它们会自我合成用户查询。我们的初步实验表明，这些随机用户查询质量高且多样性大，表明对齐过程中的学习能力得到了有效利用。
- en: 'Based on these findings, we developed a self-synthesis method to construct
    high-quality instruction datasets at scale, named Magpie (as illustrated in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Magpie: Alignment Data Synthesis from
    Scratch by Prompting Aligned LLMs with Nothing")). Unlike existing methods, our
    approach does not rely on prompt engineering or seed questions. Instead, it directly
    constructs instruction data by prompting aligned LLMs with a pre-query template
    for sampling instructions. We applied this method to the Llama-3-8B-Instruct and
    Llama-3-70B-Instruct models, creating two instruction datasets: Magpie-Air and
    Magpie-Pro, respectively.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '基于这些发现，我们开发了一种自我合成方法来大规模构建高质量的指令数据集，命名为Magpie（如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing") 所示）。与现有方法不同，我们的方法不依赖于提示工程或种子问题。而是通过用前查询模板提示对齐LLM直接构建指令数据。我们将这种方法应用于Llama-3-8B-Instruct和Llama-3-70B-Instruct模型，分别创建了两个指令数据集：Magpie-Air和Magpie-Pro。'
- en: 'Our Magpie-Air and Magpie-Pro datasets were created using 206 and 614 GPU hours,
    respectively, without requiring any human intervention or API access to production
    LLMs like GPT-4\. Additionally, we generated two multi-turn instruction datasets,
    Magpie-Air-MT and Magpie-Pro-MT, which contain sequences of multi-turn instructions
    and responses. The statistics and advantages of our instruction datasets compared
    to existing ones are summarized in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing"). We perform a comprehensive analysis of the generated data, allowing
    practitioners to filter and select data instances from these datasets for fine-tuning
    according to their particular needs.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的Magpie-Air和Magpie-Pro数据集分别使用了206小时和614小时的GPU计算，不需要任何人工干预或对生产LLM如GPT-4的API访问。此外，我们还生成了两个多轮指令数据集Magpie-Air-MT和Magpie-Pro-MT，包含多轮指令和响应的序列。我们总结了这些指令数据集与现有数据集的统计信息和优势，如表
    [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing") 所示。我们对生成的数据进行了全面分析，允许实践者根据其特定需求从这些数据集中筛选和选择数据实例进行微调。'
- en: To compare Magpie data with other public instruction datasets (e.g., ShareGPT
    [[10](#bib.bib10)], WildChat [[64](#bib.bib64)], Evol Instruct [[58](#bib.bib58)],
    UltraChat [[16](#bib.bib16)], OpenHermes [[49](#bib.bib49)], Tulu V2 Mix [[24](#bib.bib24)])
    and various preference tuning strategies with UltraFeedback [[13](#bib.bib13)],
    we fine-tune the Llama-3-8B-Base model with each dataset and assess the performance
    of the resultant models on LLM alignment benchmarks such as AlpacaEval 2 [[33](#bib.bib33)],
    Arena-Hard [[32](#bib.bib32)], and WildBench [[34](#bib.bib34)]. Our results show
    that models fine-tuned with Magpie achieve superior performance, even surpassing
    the official Llama-3-8B-Instruct model on AlpacaEval, which was fine-tuned with
    over 10 million data points for supervised fine-tuning (SFT) and follow-up feedback
    learning. Not only does Magpie excel in SFT alone compared to prior public datasets
    that incorporate both SFT and preference optimization (e.g., direct preference
    optimization with UltraFeedback [[13](#bib.bib13)]), but it also delivers the
    best results when evaluated against six baseline instruction datasets and four
    preference tuning methods (DPO [[44](#bib.bib44)], IPO [[2](#bib.bib2)], KTO [[19](#bib.bib19)],
    and ORPO [[23](#bib.bib23)] with the UltraFeedback dataset). These findings show
    the exceptional quality of instruction data generated by Magpie, enabling it to
    outperform even the official, extensively optimized LLMs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 Magpie 数据与其他公共指令数据集（例如，ShareGPT [[10](#bib.bib10)]、WildChat [[64](#bib.bib64)]、Evol
    Instruct [[58](#bib.bib58)]、UltraChat [[16](#bib.bib16)]、OpenHermes [[49](#bib.bib49)]、Tulu
    V2 Mix [[24](#bib.bib24)]）以及各种与 UltraFeedback [[13](#bib.bib13)] 的偏好调整策略进行比较，我们对每个数据集对
    Llama-3-8B-Base 模型进行微调，并评估结果模型在 LLM 对齐基准测试（如 AlpacaEval 2 [[33](#bib.bib33)]、Arena-Hard
    [[32](#bib.bib32)] 和 WildBench [[34](#bib.bib34)]）上的表现。我们的结果显示，经过 Magpie 微调的模型在性能上优于其他模型，甚至在
    AlpacaEval 上超越了官方的 Llama-3-8B-Instruct 模型，该模型经过了超过 1000 万个数据点的监督微调 (SFT) 和后续反馈学习。Magpie
    在单独的 SFT 中表现出色，相较于包括 SFT 和偏好优化的早期公共数据集（例如，通过 UltraFeedback [[13](#bib.bib13)]
    的直接偏好优化），它在评估六个基准指令数据集和四种偏好调整方法（DPO [[44](#bib.bib44)]、IPO [[2](#bib.bib2)]、KTO
    [[19](#bib.bib19)] 和 ORPO [[23](#bib.bib23)] 与 UltraFeedback 数据集）时也取得了最佳结果。这些发现展示了
    Magpie 生成的指令数据的卓越质量，使其能够超越甚至官方广泛优化的 LLM。
- en: 'Table 1: Statistics of instruction datasets generated by Magpie compared to
    other instruction datasets. Tokens are counted using the tiktoken library [[42](#bib.bib42)].'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: Magpie 生成的指令数据集与其他指令数据集的统计数据。使用 tiktoken 库计算的 Tokens [[42](#bib.bib42)]。'
- en: '| Instruction Source | Dataset Name | #Convs | #Turns | Human Effort | Response
    Generator | #Tokens / Turn | #Total Tokens |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Instruction Source | Dataset Name | #Convs | #Turns | Human Effort | Response
    Generator | #Tokens / Turn | #Total Tokens |'
- en: '| Synthetic | Alpaca [[47](#bib.bib47)] | 52K | 1 | Low | text-davinci-003
    | 67.38[±54.88] | 3.5M |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Synthetic | Alpaca [[47](#bib.bib47)] | 52K | 1 | Low | text-davinci-003
    | 67.38[±54.88] | 3.5M |'
- en: '| Evol Instruct [[58](#bib.bib58)] | 143K | 1 | Low | ChatGPT | 473.33[±330.13]
    | 68M |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Evol Instruct [[58](#bib.bib58)] | 143K | 1 | Low | ChatGPT | 473.33[±330.13]
    | 68M |'
- en: '| UltraChat [[16](#bib.bib16)] | 208K | 3.16 | Low | GhatGPT | 376.58[±177.81]
    | 238M |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| UltraChat [[16](#bib.bib16)] | 208K | 3.16 | Low | GhatGPT | 376.58[±177.81]
    | 238M |'
- en: '| Human | Dolly [[14](#bib.bib14)] | 15K | 1 | High | ChatGPT | 94.61[±135.84]
    | 1.42M |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Human | Dolly [[14](#bib.bib14)] | 15K | 1 | High | ChatGPT | 94.61[±135.84]
    | 1.42M |'
- en: '| ShareGPT [[66](#bib.bib66)] | 112K | 4.79 | High | ChatGPT | 465.38[±368.37]
    | 201M |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| ShareGPT [[66](#bib.bib66)] | 112K | 4.79 | High | ChatGPT | 465.38[±368.37]
    | 201M |'
- en: '| WildChat [[64](#bib.bib64)] | 652K | 2.52 | High | GPT-3.5 & GPT-4 | 727.09[±818.84]
    | 852M |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| WildChat [[64](#bib.bib64)] | 652K | 2.52 | High | GPT-3.5 & GPT-4 | 727.09[±818.84]
    | 852M |'
- en: '| LMSYS-Chat-1M [[65](#bib.bib65)] | 1M | 2.01 | High | Mix | 260.37[±346.97]
    | 496M |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| LMSYS-Chat-1M [[65](#bib.bib65)] | 1M | 2.01 | High | Mix | 260.37[±346.97]
    | 496M |'
- en: '| Mixture | Deita [[38](#bib.bib38)] | 9.5K | 22.02 | - | Mix | 372.78[±182.97]
    | 74M |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Mixture | Deita [[38](#bib.bib38)] | 9.5K | 22.02 | - | Mix | 372.78[±182.97]
    | 74M |'
- en: '| OpenHermes [[49](#bib.bib49)] | 243K | 1 | - | Mix | 297.86[±258.45] | 72M
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| OpenHermes [[49](#bib.bib49)] | 243K | 1 | - | Mix | 297.86[±258.45] | 72M
    |'
- en: '| Tulu V2 Mixture [[24](#bib.bib24)] | 326K | 2.31 | - | Mix | 411.94[±447.48]
    | 285M |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Tulu V2 Mixture [[24](#bib.bib24)] | 326K | 2.31 | - | Mix | 411.94[±447.48]
    | 285M |'
- en: '| Magpie | Llama-3-Magpie-Air | 3M | 1 | No | Llama-3-8B | 426.39[±217.39]
    | 1.28B |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Magpie | Llama-3-Magpie-Air | 3M | 1 | No | Llama-3-8B | 426.39[±217.39]
    | 1.28B |'
- en: '| Llama-3-Magpie-Air-MT | 300K | 2 | No | Llama-3-8B | 610.80[±90.61] | 366M
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-Magpie-Air-MT | 300K | 2 | No | Llama-3-8B | 610.80[±90.61] | 366M
    |'
- en: '| Llama-3-Magpie-Pro | 1M | 1 | No | Llama-3-70B | 478.00[±211.09] | 477M |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-Magpie-Pro | 1M | 1 | No | Llama-3-70B | 478.00[±211.09] | 477M |'
- en: '| Llama-3-Magpie-Pro-MT | 300K | 2 | No | Llama-3-70B | 554.53[±133.64] | 333M
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-Magpie-Pro-MT | 300K | 2 | No | Llama-3-70B | 554.53[±133.64] | 333M
    |'
- en: '2 Magpie: A Scalable Method to Synthesize Instruction Data'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 Magpie：一种可扩展的指令数据合成方法
- en: Overview of Magpie.
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Magpie 概述。
- en: 'In what follows, we describe our method, Magpie, to synthesize instruction
    data for fine-tuning LLMs. An instance of instruction data consists of at least
    one or multiple instruction-response pairs. Each pair specifies the roles of instruction
    provider and follower, along with their instruction and response. As shown in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing"), Magpie consists of two
    steps: (1) instruction generation, and (2) response generation. The pipeline of
    Magpie can be fully *automated without any human intervention*. Given the data
    generated by Magpie, practitioners may customize and build their own personalized
    instruction dataset accordingly (see Section [3](#S3 "3 Dataset Analysis ‣ Magpie:
    Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing")
    and Appendix [B](#A2 "Appendix B Filter Setups ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing") for more details). We detail
    each step in the following.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们描述了我们的方法 Magpie，以合成用于微调 LLM 的指令数据。一个指令数据实例由一个或多个指令-响应对组成。每对指令-响应对指定了指令提供者和追随者的角色，以及他们的指令和响应。如图
    [1](#S1.F1 "图 1 ‣ 1 引言 ‣ Magpie：从头开始通过提示对齐的 LLM 合成对齐数据") 所示，Magpie 包含两个步骤：（1）指令生成，（2）响应生成。Magpie
    的流程可以完全 *自动化，无需任何人工干预*。给定 Magpie 生成的数据，实践者可以根据需要定制并构建自己的个性化指令数据集（有关更多细节，请参见第 [3](#S3
    "3 数据集分析 ‣ Magpie：从头开始通过提示对齐的 LLM 合成对齐数据") 节和附录 [B](#A2 "附录 B 过滤设置 ‣ Magpie：从头开始通过提示对齐的
    LLM 合成对齐数据")）。我们在下文中详细说明每个步骤。
- en: 'Step 1: Instruction Generation.'
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1：指令生成。
- en: The goal of this step is to generate an instruction for each instance of instruction
    data. Given an open-weight aligned LLM (e.g., Llama-3-70B-Instruct), Magpie crafts
    an input query in the format of the predefined instruction template of the LLM.
    This query defines only the role of instruction provider (e.g., user), and does
    not provide any instruction. Note that the auto-regressive LLM has been fine-tuned
    using instruction data in the format of the predefined instruction template. Thus,
    the LLM autonomously generates an instruction when the query crafted by Magpie is
    given as an input. Magpie stops generating the instruction once the LLM produces
    an end-of-sequence token. Sending the crafted query to the LLM multiple times
    leads to a set of instructions. Compared with existing synthetic approaches [[16](#bib.bib16),
    [31](#bib.bib31), [47](#bib.bib47), [53](#bib.bib53), [55](#bib.bib55), [58](#bib.bib58),
    [59](#bib.bib59)], Magpie does not require specific prompt engineering techniques
    since the crafted query follows the format of the predefined instruction template.
    In addition, Magpie autonomously generates instructions without using any seed
    question, ensuring the diversity of generated instructions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步的目标是为每个指令数据实例生成一个指令。给定一个开放权重对齐的 LLM（例如，Llama-3-70B-Instruct），Magpie 制作一个符合
    LLM 预定义指令模板格式的输入查询。这个查询仅定义指令提供者的角色（例如，用户），而不提供任何指令。请注意，自回归 LLM 已使用预定义指令模板格式的指令数据进行了微调。因此，当
    Magpie 制作的查询作为输入提供时，LLM 会自动生成指令。Magpie 会在 LLM 生成结束序列标记时停止生成指令。将制作的查询多次发送到 LLM
    会生成一组指令。与现有的合成方法相比 [[16](#bib.bib16), [31](#bib.bib31), [47](#bib.bib47), [53](#bib.bib53),
    [55](#bib.bib55), [58](#bib.bib58), [59](#bib.bib59)]，Magpie 不需要特定的提示工程技术，因为制作的查询遵循预定义指令模板的格式。此外，Magpie
    在不使用任何种子问题的情况下自动生成指令，确保了生成指令的多样性。
- en: 'Step 2: Response Generation. The goal of this step is to generate responses
    to the instructions obtained from Step 1. Magpie sends these instructions to the
    LLM to generate the corresponding responses. Combining the roles of instruction
    provider and follower, the instructions from Step 1, and the responses generated
    in Step 2 yields the instruction dataset. Detailed discussion on the generation
    configuration can be found in Appendix [D](#A4 "Appendix D Detailed Experimental
    Setups ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs
    with Nothing").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2：响应生成。这一步的目标是生成对步骤 1 中获得的指令的响应。Magpie 将这些指令发送到 LLM，以生成相应的响应。结合指令提供者和追随者的角色、步骤
    1 中的指令以及步骤 2 中生成的响应，形成指令数据集。有关生成配置的详细讨论，请参见附录 [D](#A4 "附录 D 详细实验设置 ‣ Magpie：从头开始通过提示对齐的
    LLM 合成对齐数据")。
- en: Extensions of Magpie.
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Magpie 的扩展。
- en: 'Magpie can be readily extended to generate multi-turn instruction datasets
    and preference datasets. In addition, practitioners can specify the task requested
    by the instructions. We defer the detailed discussion on these extensions to Appendix
    [A](#A1 "Appendix A Magpie Extension ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'Magpie 可以轻松扩展以生成多轮指令数据集和偏好数据集。此外，实践者可以指定指令所请求的任务。我们将这些扩展的详细讨论推迟到附录 [A](#A1
    "附录 A Magpie 扩展 ‣ Magpie: 从头开始通过提示对齐的 LLMs 进行对齐数据合成")。'
- en: 3 Dataset Analysis
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集分析
- en: 'We apply Magpie to the Llama-3-8B-Instruct and Llama-3-70B-Instruct models
    to construct two instruction datasets: Magpie-Air and Magpie-Pro, respectively.
    Examples of instances in both datasets can be found in Appendix [G](#A7 "Appendix
    G Magpie Examples ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing"). In this section, we present a comprehensive statistical
    analysis of the Magpie-Air and Magpie-Pro datasets. An overview of the lengths
    of instructions and responses of the data in Magpie-Air and Magpie-Pro is presented
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Dataset Coverage ‣ 3 Dataset Analysis ‣ Magpie:
    Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing").
    In what follows, we first assess the breadth of Magpie-Pro by analyzing its coverage.
    We then discuss the attributes of Magpie-Pro, including topic coverage, difficulty,
    quality, and similarity of instructions, as well as quality of response. Finally,
    we provide the safety analysis and cost analysis. Using our dataset analysis,
    practitioners can customize and configure their own datasets for fine-tuning LLMs.
    In Appendix [B](#A2 "Appendix B Filter Setups ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing"), we showcase the process
    of customizing and filtering an instruction dataset based on our analysis. Specifically,
    we select 300K instances from Magpie-Pro and Magpie-Air-Filtered, yielding datasets
    Magpie-Pro-300K and Magpie-Air-300K-Filtered, respectively.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将 Magpie 应用于 Llama-3-8B-Instruct 和 Llama-3-70B-Instruct 模型，分别构建了两个指令数据集：Magpie-Air
    和 Magpie-Pro。在附录 [G](#A7 "附录 G Magpie 示例 ‣ Magpie: 从头开始通过提示对齐的 LLMs 进行对齐数据合成")
    中可以找到这两个数据集中实例的示例。在本节中，我们呈现了对 Magpie-Air 和 Magpie-Pro 数据集的综合统计分析。Magpie-Air 和
    Magpie-Pro 数据中指令和响应的长度概览见图 [3](#S3.F3 "图 3 ‣ 3.1 数据集覆盖 ‣ 3 数据集分析 ‣ Magpie: 从头开始通过提示对齐的
    LLMs 进行对齐数据合成")。接下来，我们首先通过分析 Magpie-Pro 的覆盖范围来评估其广度。然后，我们讨论 Magpie-Pro 的属性，包括主题覆盖、难度、质量和指令的相似性，以及响应的质量。最后，我们提供了安全性分析和成本分析。通过我们的数据集分析，实践者可以定制和配置自己的数据集，以便对
    LLMs 进行微调。在附录 [B](#A2 "附录 B 过滤设置 ‣ Magpie: 从头开始通过提示对齐的 LLMs 进行对齐数据合成") 中，我们展示了基于我们的分析定制和过滤指令数据集的过程。具体而言，我们从
    Magpie-Pro 和 Magpie-Air-Filtered 中选择了 30 万个实例，分别生成数据集 Magpie-Pro-300K 和 Magpie-Air-300K-Filtered。'
- en: 3.1 Dataset Coverage
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集覆盖
- en: We follow the approach in [[64](#bib.bib64)] and analyze the coverage of Magpie-Pro
    in the embedding space. Specifically, we use the all-mpnet-base-v2 embedding model¹¹1[https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
    to calculate the input embeddings, and employ t-SNE [[51](#bib.bib51)] to project
    these embeddings into a two-dimensional space. We adopt three synthetic datasets
    as baselines, including Alpaca [[47](#bib.bib47)], Evol Instruct [[58](#bib.bib58)],
    and UltraChat [[16](#bib.bib16)], to demonstrate the coverage of Magpie-Pro.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循 [[64](#bib.bib64)] 中的方法，分析 Magpie-Pro 在嵌入空间中的覆盖情况。具体而言，我们使用 all-mpnet-base-v2
    嵌入模型¹¹1[https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)
    来计算输入嵌入，并采用 t-SNE [[51](#bib.bib51)] 将这些嵌入投影到二维空间中。我们采用三种合成数据集作为基线，包括 Alpaca [[47](#bib.bib47)]、Evol
    Instruct [[58](#bib.bib58)] 和 UltraChat [[16](#bib.bib16)]，以展示 Magpie-Pro 的覆盖情况。
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Dataset Coverage ‣ 3 Dataset Analysis ‣ Magpie:
    Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing")
    presents the t-SNE plots of Magpie-Pro, Alpaca, Evol Instruct, and UltraChat.
    Each t-SNE plot is generated by randomly sampling 10,000 instructions from the
    associated dataset. We observe that the t-SNE plot of Magpie-Pro encompasses the
    area covered by the plots of Alpaca, Evol Instruct, and UltraChat. This suggests
    that Magpie-Pro provides a broader or more diverse range of topics, highlighting
    its extensive coverage across varied themes and subjects. We also follow the practice
    in [[53](#bib.bib53)] and present the most common verbs and their top direct noun
    objects in instructions in Appendix [C](#A3 "Appendix C More Dataset Analysis
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing"), indicating the diverse topic coverage of Magpie dataset. Coverage analysis
    of Magpie-Air can also be found in Appendix [C](#A3 "Appendix C More Dataset Analysis
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S3.F3 "图 3 ‣ 3.1 数据集覆盖 ‣ 3 数据集分析 ‣ Magpie: 从头开始通过提示对齐的 LLM 进行对齐数据合成")
    展示了 Magpie-Pro、Alpaca、Evol Instruct 和 UltraChat 的 t-SNE 图。每个 t-SNE 图都是通过随机抽取 10,000
    条指令生成的。我们观察到 Magpie-Pro 的 t-SNE 图包含了 Alpaca、Evol Instruct 和 UltraChat 图的覆盖区域。这表明
    Magpie-Pro 提供了更广泛或更多样化的主题，突显了其在各种主题和学科上的广泛覆盖。我们还遵循了 [[53](#bib.bib53)] 的做法，并在附录
    [C](#A3 "附录 C 更多数据集分析 ‣ Magpie: 从头开始通过提示对齐的 LLM 进行对齐数据合成") 中展示了指令中最常见的动词及其主要直接名词对象，表明了
    Magpie 数据集的多样化主题覆盖。Magpie-Air 的覆盖分析也可以在附录 [C](#A3 "附录 C 更多数据集分析 ‣ Magpie: 从头开始通过提示对齐的
    LLM 进行对齐数据合成") 中找到。'
- en: '![Refer to caption](img/9b4448d302454a19860a7cf3e741db1d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b4448d302454a19860a7cf3e741db1d.png)'
- en: 'Figure 2: Lengths of instructions and responses in Magpie-Air/Pro.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Magpie-Air/Pro 中指令和响应的长度。
- en: '![Refer to caption](img/1b1ced70e1310e97837552a22263a64d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1b1ced70e1310e97837552a22263a64d.png)'
- en: 'Figure 3: This figure compares the t-SNE plot of Magpie-Pro with those of Alpaca,
    Evol Instruct, and UltraChat, each of which is sampled with 10,000 instructions.
    The t-SNE plot of Magpie-Pro encompasses the area covered by the other plots,
    demonstrating the comprehensive coverage of Magpie-Pro.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：该图将 Magpie-Pro 的 t-SNE 图与 Alpaca、Evol Instruct 和 UltraChat 的图进行比较，每个图都是通过
    10,000 条指令进行采样的。Magpie-Pro 的 t-SNE 图包含了其他图覆盖的区域，展示了 Magpie-Pro 的全面覆盖性。
- en: 3.2 Dataset Attributes
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据集属性
- en: 'Attribute: Task Categories of Instructions.'
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 属性：指令任务类别。
- en: 'We use Llama-3-8B-Instruct to categorize the instances in Magpie-Pro (see Figure
    [7](#A3.F7 "Figure 7 ‣ Task Categories of Magpie-Pro and Magpie-Air. ‣ C.1 Additional
    Analysis on Dataset Coverage and Attributes. ‣ Appendix C More Dataset Analysis
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing") in Appendix [C.1](#A3.SS1 "C.1 Additional Analysis on Dataset Coverage
    and Attributes. ‣ Appendix C More Dataset Analysis ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing") for detail). The prompts
    used to query Llama-3-8B-Instruct can be found in Appendix [F](#A6 "Appendix F
    Prompt Templates ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing"). Our observations indicate that over half of the tasks
    in Magpie-Pro pertain to information seeking, making it the predominant category.
    This is followed by tasks involving creative writing, advice seeking, planning,
    and math. This distribution over the task categories aligns with the practical
    requests from human users [[33](#bib.bib33)].'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 Llama-3-8B-Instruct 对 Magpie-Pro 中的实例进行分类（见附录 [C.1](#A3.SS1 "C.1 数据集覆盖和属性的附加分析
    ‣ 附录 C 更多数据集分析 ‣ Magpie: 从头开始通过提示对齐的 LLM 进行对齐数据合成") 中的图 [7](#A3.F7 "图 7 ‣ Magpie-Pro
    和 Magpie-Air 的任务类别 ‣ C.1 数据集覆盖和属性的附加分析 ‣ 附录 C 更多数据集分析 ‣ Magpie: 从头开始通过提示对齐的 LLM
    进行对齐数据合成") 了解详细信息）。用于查询 Llama-3-8B-Instruct 的提示可以在附录 [F](#A6 "附录 F 提示模板 ‣ Magpie:
    从头开始通过提示对齐的 LLM 进行对齐数据合成") 中找到。我们的观察结果表明，Magpie-Pro 中超过一半的任务与信息获取相关，使其成为主要类别。其次是涉及创作写作、寻求建议、计划和数学的任务。这种任务类别的分布与人类用户的实际需求
    [[33](#bib.bib33)] 相一致。'
- en: '![Refer to caption](img/d1a9a02727238e74ff27ba51dbd838d5.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1a9a02727238e74ff27ba51dbd838d5.png)'
- en: 'Figure 4: The statistics of input difficulty and quality.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：输入难度和质量的统计数据。
- en: 'Attribute: Quality of Instructions.'
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 属性：指令质量。
- en: 'We use the Llama-3-8B-Instruct model to assess the quality of each instruction
    in Magpie-Air and Magpie-Pro, categorizing them as ‘very poor’, ‘poor’, ‘average’,
    ‘good’, and ‘excellent’. We present the histograms of qualities for both datasets
    in Figure [4](#S3.F4 "Figure 4 ‣ Attribute: Task Categories of Instructions. ‣
    3.2 Dataset Attributes ‣ 3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing")-(a). We have the following
    two observations. First, both datasets are of high quality, with the majority
    of instances rated ‘average’ or higher. In addition, the overall quality of Magpie-Pro
    surpasses that of Magpie-Air. We hypothesize that this is due to the enhanced
    capabilities of Llama-3-70B compared with Llama-3-8B.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用Llama-3-8B-Instruct模型评估Magpie-Air和Magpie-Pro中每个指令的质量，将其分类为“非常差”、“差”、“一般”、“好”和“优秀”。图[4](#S3.F4
    "Figure 4 ‣ Attribute: Task Categories of Instructions. ‣ 3.2 Dataset Attributes
    ‣ 3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing")-(a)展示了两个数据集的质量直方图。我们有以下两个观察结果。首先，两个数据集的质量都很高，大多数实例的评级为“中等”或更高。此外，Magpie-Pro的整体质量超过了Magpie-Air。我们推测这可能是由于Llama-3-70B相对于Llama-3-8B的增强能力。'
- en: 'Attribute: Difficulty of Instructions.'
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 属性：指令难度。
- en: 'We use the Llama-3-8B-Instruct model to rate the difficulty of each instruction
    in Magpie-Air and Magpie-Pro. Each instruction can be labeled as ‘very easy’,
    ‘easy’, ‘medium’, ‘hard’, or ‘very hard’. Figure [4](#S3.F4 "Figure 4 ‣ Attribute:
    Task Categories of Instructions. ‣ 3.2 Dataset Attributes ‣ 3 Dataset Analysis
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing")-(b) presents the histograms of the levels of difficulty for Magpie-Air
    and Magpie-Pro. We observe that the distributions across difficulty levels are
    similar for Magpie-Air and Magpie-Pro. Some instructions in Magpie-Pro are more
    challenging than those in Magpie-Air because Magpie-Pro is generated by a more
    capable model (Llama-3-70B-Instruct).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用Llama-3-8B-Instruct模型对Magpie-Air和Magpie-Pro中的每个指令的难度进行评级。每个指令可以标记为“非常容易”、“容易”、“中等”、“困难”或“非常困难”。图[4](#S3.F4
    "Figure 4 ‣ Attribute: Task Categories of Instructions. ‣ 3.2 Dataset Attributes
    ‣ 3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing")-(b)展示了Magpie-Air和Magpie-Pro的难度水平直方图。我们观察到Magpie-Air和Magpie-Pro的难度分布相似。由于Magpie-Pro是由更强大的模型（Llama-3-70B-Instruct）生成的，Magpie-Pro中的一些指令比Magpie-Air中的指令更具挑战性。'
- en: '![Refer to caption](img/434f5ed973a2ec2df67a81c106e20b30.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/434f5ed973a2ec2df67a81c106e20b30.png)'
- en: 'Figure 5: This figure summarizes the minimum neighbor distances and reward
    differences.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：该图总结了最小邻居距离和奖励差异。
- en: 'Attribute: Instruction Similarity.'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 属性：指令相似性。
- en: 'We quantify the similarity among instructions generated by Magpie to remove
    repetitive instructions. We measure the similarity using minimum neighbor distance
    in the embedding space. Specifically, we first represent all instructions in the
    embedding space using the all-mpnet-base-v2 embedding model. For any given instruction,
    we then calculate the minimum distance from the instruction to its nearest neighbors
    in the embedding space using Facebook AI Similarity Search (FAISS) [[17](#bib.bib17)].
    The minimum neighbor distances of instructions in Magpie-Air after removing repetitions
    are summarized in Figure [5](#S3.F5 "Figure 5 ‣ Attribute: Difficulty of Instructions.
    ‣ 3.2 Dataset Attributes ‣ 3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing")-(a).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们量化了Magpie生成的指令之间的相似性，以去除重复的指令。我们使用嵌入空间中的最小邻居距离来衡量相似性。具体而言，我们首先使用all-mpnet-base-v2嵌入模型在嵌入空间中表示所有指令。然后，对于任何给定的指令，我们计算该指令到其在嵌入空间中最近邻的最小距离，使用Facebook
    AI Similarity Search (FAISS) [[17](#bib.bib17)]。去除重复后的Magpie-Air指令的最小邻居距离总结见图[5](#S3.F5
    "Figure 5 ‣ Attribute: Difficulty of Instructions. ‣ 3.2 Dataset Attributes ‣
    3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing")-(a)。'
- en: 'Attribute: Quality of Responses.'
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 属性：响应质量。
- en: 'We assess the quality of responses using a metric named reward difference.
    For each instance in our dataset, the reward difference is calculated as $r^{*}-r_{base}$
    is the reward assigned by the same model to the response generated by the Llama-3
    base model for the same instruction. We use URIAL [[35](#bib.bib35)] to elicit
    responses from the base model. A positive reward difference indicates that the
    response from our dataset is of higher quality, and could potentially benefit
    instruction tuning. In our experiments, we follow [[29](#bib.bib29)] and use FsfairX-LLaMA3-RM-v0.1
    [[57](#bib.bib57)] as our reward model. Our results on the reward difference are
    presented in Figure [5](#S3.F5 "Figure 5 ‣ Attribute: Difficulty of Instructions.
    ‣ 3.2 Dataset Attributes ‣ 3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing")-(b).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用一种名为奖励差异的指标来评估响应的质量。对于数据集中的每个实例，奖励差异计算为 $r^{*}-r_{base}$，其中 $r^{*}$ 是同一模型对由
    Llama-3 基础模型为相同指令生成的响应分配的奖励。我们使用 URIAL [[35](#bib.bib35)] 从基础模型中引出响应。正的奖励差异表示数据集中的响应质量更高，可能有利于指令调整。在我们的实验中，我们遵循
    [[29](#bib.bib29)] 并使用 FsfairX-LLaMA3-RM-v0.1 [[57](#bib.bib57)] 作为我们的奖励模型。我们的奖励差异结果显示在图
    [5](#S3.F5 "Figure 5 ‣ Attribute: Difficulty of Instructions. ‣ 3.2 Dataset Attributes
    ‣ 3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing")-(b)。'
- en: 3.3 Safety Analysis
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 安全分析
- en: 'We use Llama-Guard-2 [[48](#bib.bib48)] to analyze the safety of Magpie-Air
    and Magpie-Pro. Our results indicate that both datasets are predominantly safe,
    with less than 1% of the data potentially containing harmful instructions or responses.
    Please refer to Appendix [C.2](#A3.SS2 "C.2 Additional Safety Analysis ‣ Appendix
    C More Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing") for detailed safety analysis.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 Llama-Guard-2 [[48](#bib.bib48)] 分析 Magpie-Air 和 Magpie-Pro 的安全性。我们的结果表明，这两个数据集主要是安全的，少于
    1% 的数据可能包含有害的指令或响应。有关详细的安全分析，请参见附录 [C.2](#A3.SS2 "C.2 Additional Safety Analysis
    ‣ Appendix C More Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing")。'
- en: 3.4 Cost Analysis
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 成本分析
- en: We perform experiments on a server with four NVIDIA A100-SXM4-80GB GPUs, an
    AMD EPYC 7763 64-Core Processor, and 512 GB of RAM, using the VLLM inference framework
    [[28](#bib.bib28)]. The models are loaded in the bfloat16 format.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在配备四个 NVIDIA A100-SXM4-80GB GPU、一个 AMD EPYC 7763 64 核处理器和 512 GB RAM 的服务器上进行实验，使用
    VLLM 推理框架 [[28](#bib.bib28)]。模型以 bfloat16 格式加载。
- en: When creating the 3M Magpie-Air dataset, our Magpie spent 1.55 and 50 hours
    to generate the instructions (Step 1) and responses (Step 2), respectively. For
    the 1M Magpie-Pro dataset, Magpie used 3.5 and 150 hours to generate the instructions
    (Step 1) and responses (Step 2), respectively. Compared to existing approaches
    to create instruction datasets, the pipeline of Magpie is fully automated without
    any human intervention or API access to advanced commercial models such as GPT-4
    [[1](#bib.bib1)]. Consequently, Magpie is cost-effective and scalable. On average,
    implementing Magpie on a cloud server²²2[https://lambdalabs.com/service/gpu-cloud](https://lambdalabs.com/service/gpu-cloud)
    would incur costs of $0.12 and $1.1 per 1,000 data instances for Magpie-Air and
    Magpie-Pro, respectively.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 3M Magpie-Air 数据集时，我们的 Magpie 花费了 1.55 小时和 50 小时分别生成了指令（步骤 1）和响应（步骤 2）。对于
    1M Magpie-Pro 数据集，Magpie 花费了 3.5 小时和 150 小时分别生成了指令（步骤 1）和响应（步骤 2）。与现有的指令数据集创建方法相比，Magpie
    的流程是完全自动化的，不需要任何人工干预或访问高级商业模型如 GPT-4 的 API [[1](#bib.bib1)]。因此，Magpie 成本效益高且可扩展。平均而言，在云服务器²²2[https://lambdalabs.com/service/gpu-cloud](https://lambdalabs.com/service/gpu-cloud)
    上实施 Magpie 的成本为每 1,000 个数据实例 Magpie-Air 为 $0.12，Magpie-Pro 为 $1.1。
- en: 3.5 Additional Analysis
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 额外分析
- en: 'Additional dataset analysis, including the impact of generation configurations
    on the quality and difficulty of the generated instructions, is detailed in Appendix
    [C.3](#A3.SS3 "C.3 Ablation Analysis on Generation Configurations ‣ Appendix C
    More Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing").'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '附录 [C.3](#A3.SS3 "C.3 Ablation Analysis on Generation Configurations ‣ Appendix
    C More Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing") 中详细介绍了额外的数据集分析，包括生成配置对生成指令的质量和难度的影响。'
- en: 4 Performance Analysis
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 性能分析
- en: In this section, we evaluate the quality of datasets generated by Magpie by
    utilizing them to fine-tune model families including Llama-3 [[40](#bib.bib40)]
    and Qwen1.5 [[3](#bib.bib3)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过利用Magpie生成的数据集对包括Llama-3 [[40](#bib.bib40)]和Qwen1.5 [[3](#bib.bib3)]在内的模型家族进行微调，以评估数据集的质量。
- en: 4.1 Experimental Setups.
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置。
- en: Baselines for Instruction Tuning.
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指令调整的基线。
- en: 'We compare the family of datasets generated by Magpie with six state-of-the-art
    open-source instruction tuning datasets: ShareGPT [[10](#bib.bib10)], WildChat
    [[64](#bib.bib64)], Evol Instruct [[58](#bib.bib58)], UltraChat [[16](#bib.bib16)],
    OpenHermes [[49](#bib.bib49)], and Tulu V2 Mix [[24](#bib.bib24)]. ShareGPT and
    WildChat are representative human-written datasets containing 112K and 652K high-quality
    multi-round conversations between humans and GPT, respectively. Evol Instruct
    and UltraChat are representative open-source synthetic datasets. Following [[39](#bib.bib39)],
    we use the 208K sanitized version of Ultrachat provided by HuggingFace³³3[https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k).
    OpenHermes and Tulu V2 Mix are crowd-sourced datasets consisting of a mix of diverse
    open-source alignment datasets, with 243K and 326K conversations, respectively.
    We note that to ensure fair comparison involving datasets of different sizes,
    we provide the results of Magpie-Pro-200K-Filtered and Magpie-Pro-100K-Filtered,
    which contains the first 200K and 100K conversations from Magpie-Pro-300K-Filtered.
    Detailed discussion on how to generate these datasets can be found in Appendix
    [B](#A2 "Appendix B Filter Setups ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing").'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Magpie生成的数据集家族与六个最先进的开源指令调整数据集进行比较：ShareGPT [[10](#bib.bib10)]、WildChat [[64](#bib.bib64)]、Evol
    Instruct [[58](#bib.bib58)]、UltraChat [[16](#bib.bib16)]、OpenHermes [[49](#bib.bib49)]
    和 Tulu V2 Mix [[24](#bib.bib24)]。ShareGPT和WildChat是代表性的人工编写数据集，分别包含112K和652K高质量的人类与GPT之间的多轮对话。Evol
    Instruct和UltraChat是代表性的开源合成数据集。根据[[39](#bib.bib39)]，我们使用HuggingFace提供的208K净化版Ultrachat³³3[https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)。OpenHermes和Tulu
    V2 Mix是众包数据集，由多种开源对齐数据集混合而成，分别包含243K和326K对话。为了确保涉及不同大小数据集的公平比较，我们提供了Magpie-Pro-200K-Filtered和Magpie-Pro-100K-Filtered的结果，其中包含Magpie-Pro-300K-Filtered的前200K和100K对话。有关如何生成这些数据集的详细讨论，请参见附录[B](#A2
    "附录 B 过滤设置 ‣ Magpie：通过提示对齐的LLMs从头开始进行对齐数据合成")。
- en: Baselines for Instruction and Preference Tuning.
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指令和偏好调整的基线。
- en: We compare the models fine-tuned using data generated by Magpie with preference
    optimization baselines, including DPO [[44](#bib.bib44)], IPO [[2](#bib.bib2)],
    KTO [[19](#bib.bib19)] and ORPO [[23](#bib.bib23)]. Specifically, we follow [[39](#bib.bib39)]
    and use the models fine-tuned with the UltraChat dataset (for instruction tuning)
    and Ultrafeedback dataset (for preference optimization) [[13](#bib.bib13)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Magpie生成的数据进行微调的模型与包括DPO [[44](#bib.bib44)]、IPO [[2](#bib.bib2)]、KTO [[19](#bib.bib19)]
    和 ORPO [[23](#bib.bib23)]在内的偏好优化基线进行比较。具体而言，我们遵循[[39](#bib.bib39)]，使用在UltraChat数据集（用于指令调整）和Ultrafeedback数据集（用于偏好优化）[[13](#bib.bib13)]上微调的模型。
- en: Fine-Tuning Details.
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调细节。
- en: We follow [[50](#bib.bib50)] and use a cosine learning rate schedule with an
    initial learning rate of $2\times 10^{-5}$ when fine-tuning Llama-3 and Qwen1.5
    models. The maximum sequence length is 8192. The fine-tuning process is conducted
    using four NVIDIA A100 GPUs with 80G memory, and the effective batch size is 32.
    The models are fine-tuned for 2 epochs. We follow the official instruction templates
    of each model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照[[50](#bib.bib50)]的做法，使用初始学习率为$2\times 10^{-5}$的余弦学习率调度进行Llama-3和Qwen1.5模型的微调。最大序列长度为8192。微调过程使用四个具有80G内存的NVIDIA
    A100 GPU进行，实际批次大小为32。模型微调进行2个周期。我们遵循每个模型的官方指令模板。
- en: Evaluation Benchmarks.
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估基准。
- en: 'We evaluate the performance of the fine-tuned models using two widely-adopted
    instruction-following benchmarks: AlpacaEval 2 [[33](#bib.bib33)] and Arena-Hard
    [[32](#bib.bib32)]. AlpacaEval 2 consists of 805 representative instructions chosen
    from real user interactions. Arena-Hard is an enhanced version of MT-Bench [[66](#bib.bib66)],
    containing 500 challenging user queries. Both benchmarks employ a GPT evaluator
    to assess responses generated by the model of interest and a baseline model. Specifically,
    we use GPT-4-Turbo (1106) and Llama-3-8B-Instruct as baselines for AlpacaEval
    2. By default, Arena-Hard uses GPT-4 (0314) as its baseline model.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个广泛采用的指令遵循基准测试来评估微调模型的表现：AlpacaEval 2 [[33](#bib.bib33)]和Arena-Hard [[32](#bib.bib32)]。AlpacaEval
    2由805个从真实用户互动中挑选的代表性指令组成。Arena-Hard是MT-Bench [[66](#bib.bib66)]的增强版，包含500个具有挑战性的用户查询。这两个基准测试都使用GPT评估者来评估模型和基线模型生成的响应。具体而言，我们使用GPT-4-Turbo（1106）和Llama-3-8B-Instruct作为AlpacaEval
    2的基线。默认情况下，Arena-Hard使用GPT-4（0314）作为基线模型。
- en: Metrics.
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标。
- en: We adopt two metrics to measure the capabilities of instruction-following of
    fine-tuned models. The first metric is the win rate (WR), which calculates the
    fraction of responses that are favored by the GPT evaluator. This metric is applied
    in both benchmarks including AlpacaEval 2 and Arena-Hard. The second metric is
    the length-controlled win rate (LC) [[18](#bib.bib18)], a debiased version of
    WR.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用两个指标来衡量微调模型的指令遵循能力。第一个指标是胜率（WR），它计算由GPT评估者青睐的响应的比例。这个指标应用于包括AlpacaEval 2和Arena-Hard在内的两个基准测试。第二个指标是长度控制的胜率（LC）[[18](#bib.bib18)]，是WR的去偏版。
- en: '![Refer to caption](img/3badcae0d40ffbbd3814a19e63397198.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3badcae0d40ffbbd3814a19e63397198.png)'
- en: 'Figure 6: This figure shows the performance breakdown by category of Magpie-Pro
    and baselines on WildBench.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：该图展示了Magpie-Pro和基线在WildBench上的性能分类细分。
- en: The GPT evaluator considers the lengths of responses generated by the baseline
    model and model under evaluation when computing LC. By accounting for response
    length, LC reduces its impact on the win rate. This metric is specifically applied
    to the AlpacaEval 2 benchmark [[33](#bib.bib33)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: GPT评估者在计算LC时考虑了基线模型和评估模型生成的响应长度。通过考虑响应长度，LC减少了其对胜率的影响。该指标专门应用于AlpacaEval 2基准测试[[33](#bib.bib33)]。
- en: 'Detailed Experimental Setups. We provide more detailed descriptions of our
    experimental setups, including more fine-tuning details and decoding hyper-parameters
    in Appendix [D](#A4 "Appendix D Detailed Experimental Setups ‣ Magpie: Alignment
    Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '详细实验设置。我们在附录[D](#A4 "Appendix D Detailed Experimental Setups ‣ Magpie: Alignment
    Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing")中提供了我们实验设置的更详细描述，包括更多的微调细节和解码超参数。'
- en: 4.2 Experimental Results
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: 'Table 2: This table compares the performance of models instruction-tuned on
    the Llama-8B base models using our datasets and baseline datasets. We observe
    that models fine-tuned with our datasets significantly outperform those fine-tuned
    with baseline datasets of the same order of magnitude in terms of data size. In
    addition, our fine-tuned models achieve comparable performance to the official
    aligned model, despite only undergoing SFT with a much smaller dataset. Numbers
    in bold indicate that Magpie outperforms the official Llama-3-8B-Instruct model.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：该表比较了使用我们的数据集和基线数据集对Llama-8B基础模型进行指令调优的模型性能。我们观察到，使用我们的数据集微调的模型在数据规模相同级别的基线数据集上显著优于那些经过基线数据集微调的模型。此外，尽管我们的微调模型仅使用了一个更小的数据集进行SFT，但其表现与官方对齐模型相当。粗体数字表明Magpie优于官方Llama-3-8B-Instruct模型。
- en: '| Alignment Setup (Base LLM = Llama-3-8B) | #Convs | AlpacaEval 2 | Arena-Hard
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 对齐设置（基础LLM = Llama-3-8B） | #Convs | AlpacaEval 2 | Arena-Hard |'
- en: '| GPT-4-Turbo (1106) | Llama-3-8B-Instruct |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo (1106) | Llama-3-8B-Instruct |  |'
- en: '| LC (%) | WR (%) | SD | LC (%) | WR (%) | SD | WR(%) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| LC (%) | WR (%) | SD | LC (%) | WR (%) | SD | WR(%) |'
- en: '| SFT | +ShareGPT [[10](#bib.bib10)] | 112K | 9.73 | 7.2 | 0.81 | 27.26 | 18.32
    | 1.18 | 6.5 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SFT | +ShareGPT [[10](#bib.bib10)] | 112K | 9.73 | 7.2 | 0.81 | 27.26 | 18.32
    | 1.18 | 6.5 |'
- en: '|  | +Evol Instruct [[58](#bib.bib58)] | 143K | 8.52 | 6.25 | 0.76 | 20.16
    | 14.98 | 1.1 | 5.1 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | +Evol Instruct [[58](#bib.bib58)] | 143K | 8.52 | 6.25 | 0.76 | 20.16
    | 14.98 | 1.1 | 5.1 |'
- en: '|  | +OpenHermes [[49](#bib.bib49)] | 243K | 9.94 | 6.27 | 0.73 | 29.19 | 17.92
    | 1.16 | 4.4 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | +OpenHermes [[49](#bib.bib49)] | 243K | 9.94 | 6.27 | 0.73 | 29.19 | 17.92
    | 1.16 | 4.4 |'
- en: '|  | +Tulu V2 Mix [[24](#bib.bib24)] | 326K | 9.91 | 7.94 | 0.86 | 24.28 |
    18.64 | 1.18 | 5.4 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | +Tulu V2 Mix [[24](#bib.bib24)] | 326K | 9.91 | 7.94 | 0.86 | 24.28 |
    18.64 | 1.18 | 5.4 |'
- en: '|  | +WildChat[[64](#bib.bib64)] | 652K | 14.62 | 10.58 | 0.92 | 34.85 | 26.57
    | 1.32 | 8.7 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | +WildChat[[64](#bib.bib64)] | 652K | 14.62 | 10.58 | 0.92 | 34.85 | 26.57
    | 1.32 | 8.7 |'
- en: '|  | +UltraChat [[16](#bib.bib16)]\faArrowRight | 208K | 8.29 | 5.44 | 0.71
    | 23.95 | 15.12 | 1.11 | 3.6 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | +UltraChat [[16](#bib.bib16)]\faArrowRight | 208K | 8.29 | 5.44 | 0.71
    | 23.95 | 15.12 | 1.11 | 3.6 |'
- en: '| +*PO |   +UltraFeedback (DPO) [[13](#bib.bib13), [44](#bib.bib44)] | 64K
    | 18.36 | 17.33 | 1.14 | 44.42 | 42.36 | 1.46 | 14.8 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| +*PO |   +UltraFeedback (DPO) [[13](#bib.bib13), [44](#bib.bib44)] | 64K
    | 18.36 | 17.33 | 1.14 | 44.42 | 42.36 | 1.46 | 14.8 |'
- en: '|  |   +UltraFeedback (IPO) [[2](#bib.bib2), [13](#bib.bib13)] | 64K | 17.46
    | 16.13 | 1.11 | 41.66 | 38.45 | 1.43 | 14.2 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |   +UltraFeedback (IPO) [[2](#bib.bib2), [13](#bib.bib13)] | 64K | 17.46
    | 16.13 | 1.11 | 41.66 | 38.45 | 1.43 | 14.2 |'
- en: '|  |   +UltraFeedback (KTO) [[13](#bib.bib13), [19](#bib.bib19)] | 64K | 15.81
    | 14.62 | 1.05 | 41.33 | 38.32 | 1.42 | 12.2 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |   +UltraFeedback (KTO) [[13](#bib.bib13), [19](#bib.bib19)] | 64K | 15.81
    | 14.62 | 1.05 | 41.33 | 38.32 | 1.42 | 12.2 |'
- en: '|  |   +UltraFeedback (ORPO) [[13](#bib.bib13), [23](#bib.bib23)] | 64K | 13.23
    | 12.57 | 0.99 | 30.62 | 28.27 | 1.35 | 10.9 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |   +UltraFeedback (ORPO) [[13](#bib.bib13), [23](#bib.bib23)] | 64K | 13.23
    | 12.57 | 0.99 | 30.62 | 28.27 | 1.35 | 10.9 |'
- en: '| SFT | +Magpie (Ours) |  |  |  |  |  |  |  |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SFT | +Magpie (Ours) |  |  |  |  |  |  |  |  |'
- en: '|  | Air-300K-Raw | 300K | 21.99 | 21.65 | 1.21 | 48.63 | 48.06 | 1.42 | 15.8
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | Air-300K-Raw | 300K | 21.99 | 21.65 | 1.21 | 48.63 | 48.06 | 1.42 | 15.8
    |'
- en: '|  | Air-300K-Filtered | 300K | 22.66 | 23.99 | 1.24 | 49.27 | 50.8 | 1.44
    | 14.9 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | Air-300K-Filtered | 300K | 22.66 | 23.99 | 1.24 | 49.27 | 50.8 | 1.44
    | 14.9 |'
- en: '|  | Pro-300K-Raw | 300K | 21.65 | 22.19 | 1.2 | 49.65 | 50.84 | 1.42 | 15.9
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | Pro-300K-Raw | 300K | 21.65 | 22.19 | 1.2 | 49.65 | 50.84 | 1.42 | 15.9
    |'
- en: '|  | Pro-100K-Filtered | 100K | 20.47 | 24.52 | 1.25 | 47.92 | 52.75 | 1.43
    | 17.2 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | Pro-100K-Filtered | 100K | 20.47 | 24.52 | 1.25 | 47.92 | 52.75 | 1.43
    | 17.2 |'
- en: '|  | Pro-200K-Filtered | 200K | 22.11 | 26.02 | 1.26 | 51.17 | 56.76 | 1.41
    | 15.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | Pro-200K-Filtered | 200K | 22.11 | 26.02 | 1.26 | 51.17 | 56.76 | 1.41
    | 15.9 |'
- en: '|  | Pro-300K-Filtered | 300K | 25.08 | 29.47 | 1.35 | 52.12 | 53.43 | 1.44
    | 18.9 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | Pro-300K-Filtered | 300K | 25.08 | 29.47 | 1.35 | 52.12 | 53.43 | 1.44
    | 18.9 |'
- en: '|    Llama-3-8B-Instruct (SFT+RLHF) | $$>10M⁴⁴4[https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
    | 22.92 | 22.57 | 1.26 | 50 | 50 | - | 20.6 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|    Llama-3-8B-Instruct (SFT+RLHF) | $$>10M⁴⁴4[https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
    | 22.92 | 22.57 | 1.26 | 50 | 50 | - | 20.6 |'
- en: Magpie datasets outperform others.
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Magpie 数据集优于其他数据集。
- en: 'In Table [2](#S4.T2 "Table 2 ‣ 4.2 Experimental Results ‣ 4 Performance Analysis
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing"), we first compare the performance of Llama-3 models fine-tuned with
    datasets generated by Magpie against those fine-tuned with baseline datasets.
    Using the AlpacaEval 2 evaluation benchmark, we observe that both LC and WR of
    our fine-tuned models surpass those fine-tuned with baseline instruction datasets,
    regardless of the choice of the baseline model. This indicates that the datasets
    generated by Magpie are of higher quality, leading to significantly enhanced instruction-following
    capabilities. A similar observation is made when using the Arena-Hard evaluation
    benchmark. We highlight that the Llama-3 models fine-tuned with datasets generated
    by Magpie outperform even those models that have undergone preference optimization
    (e.g., instruction tuning combined with DPO), which emphasizes the high quality
    of data generated by Magpie.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [2](#S4.T2 "Table 2 ‣ 4.2 Experimental Results ‣ 4 Performance Analysis
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing")中，我们首先比较了使用Magpie生成的数据集微调的Llama-3模型与使用基准数据集微调的模型的表现。使用AlpacaEval 2评估基准，我们观察到，无论基准模型的选择如何，我们的微调模型的LC和WR都超越了使用基准指令数据集微调的模型。这表明，Magpie生成的数据集质量更高，显著提升了指令跟随能力。使用Arena-Hard评估基准时也有类似的观察。我们强调，使用Magpie生成的数据集微调的Llama-3模型甚至优于经过偏好优化的模型（例如，结合DPO的指令调整），这突显了Magpie生成数据的高质量。'
- en: 'To investigate the advantages of Magpie across different task categories, we
    also compare the performance of models fine-tuned with Magpie-Pro compared with
    baseline datasets using WildBench benchmark [[34](#bib.bib34)]. This benchmark
    consists of 1024 tasks carefully selected from real-world human-LLM conversation
    logs. The results are demonstrated in Figure [6](#S4.F6 "Figure 6 ‣ Metrics. ‣
    4.1 Experimental Setups. ‣ 4 Performance Analysis ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing"). We observe that Magpie
    consistently outperforms baseline datasets across categories.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '为了调查 Magpie 在不同任务类别中的优势，我们还比较了使用 Magpie-Pro 微调的模型与基线数据集的性能，采用 WildBench 基准
    [[34](#bib.bib34)]。该基准包含了从真实世界人类-LLM 对话记录中精心挑选的 1024 个任务。结果如图 [6](#S4.F6 "Figure
    6 ‣ Metrics. ‣ 4.1 Experimental Setups. ‣ 4 Performance Analysis ‣ Magpie: Alignment
    Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing") 所示。我们观察到
    Magpie 在各类别中始终优于基线数据集。'
- en: 'Models fine-tuned with data generated by Magpie achieve comparable performance
    to the official aligned model, but with fewer data. In Table [2](#S4.T2 "Table
    2 ‣ 4.2 Experimental Results ‣ 4 Performance Analysis ‣ Magpie: Alignment Data
    Synthesis from Scratch by Prompting Aligned LLMs with Nothing"), we compare the
    performance of models fine-tuned with data generated by Magpie against the official
    aligned model (Llama-3-8B-Instruct). We observe that the Llama-3-8B base model
    fine-tuned with data from Magpie outperforms Llama-3-8B-instruct using the AlpacaEval
    2 benchmark. For example, using the Magpie-Pro-300K-Filtered dataset to fine-tune
    the Llama-3-8B base model results in WC 29.47% against GPT-4-Turbo (1106). Furthermore,
    when Llama-3-8B-Instruct is chosen as the baseline model of AlpacaEval 2, we observe
    that WC of Llama-3-8B base models fine-tuned with data from Magpie exceeds 50%,
    indicating a preference for our fine-tuned models over the official aligned model.
    Finally, we highlight that our fine-tuning process uses no more than 300K data,
    whereas the official aligned models are fine-tuned with more than 10M data samples.
    This demonstrates the high quality of the data generated by Magpie. Using the
    Arena-Hard benchmark, we observe that a 1.7% difference between the WR achieved
    using our fine-tuned model and the official aligned model. We attribute this discrepancy
    to the fraction of coding-related instructions in our dataset. We believe that
    this gap could be easily bridged as we increase the size of datasets.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 Magpie 生成的数据微调的模型与官方对齐模型性能相当，但数据量较少。在表 [2](#S4.T2 "Table 2 ‣ 4.2 Experimental
    Results ‣ 4 Performance Analysis ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing") 中，我们比较了使用 Magpie 生成的数据微调的模型与官方对齐模型 (Llama-3-8B-Instruct)
    的性能。我们观察到使用 Magpie 数据微调的 Llama-3-8B 基础模型在 AlpacaEval 2 基准中优于 Llama-3-8B-instruct。例如，使用
    Magpie-Pro-300K-Filtered 数据集对 Llama-3-8B 基础模型进行微调的结果是 WC 29.47% 对 GPT-4-Turbo
    (1106)。此外，当 Llama-3-8B-Instruct 被选为 AlpacaEval 2 的基线模型时，我们观察到使用 Magpie 数据微调的 Llama-3-8B
    基础模型的 WC 超过 50%，表明我们微调的模型相比官方对齐模型更受偏爱。最后，我们强调我们的微调过程使用的数据不超过 300K，而官方对齐模型则使用了超过
    10M 的数据样本。这展示了 Magpie 生成数据的高质量。使用 Arena-Hard 基准时，我们观察到我们的微调模型与官方对齐模型之间的 WR 差异为
    1.7%。我们将此差异归因于我们数据集中编程相关指令的比例。我们相信，随着数据集规模的增加，这一差距可以轻松弥合。'
- en: 'Both data quantity and quality matter to capabilities of instruction-following.
    In what follows, we compare within the family of datasets generated by Magpie
    in Table [2](#S4.T2 "Table 2 ‣ 4.2 Experimental Results ‣ 4 Performance Analysis
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing"). These datasets differ in sizes, deployment of filtering, and models
    used to generate data. We observe that as the size of dataset increases, the performance
    of fine-tuned model improves, indicating that data quantity plays a critical role
    in enhancing instruction-following capabilities. Furthermore, the model fine-tuned
    with Magpie-Pro-300K-Filtered outperform those fine-tuned with the same amount
    of raw data. This demonstrates the effectiveness of our filtering technique, and
    underscores the importance of data quality. Finally, we observe that the models
    fine-tuned with Magpie-Pro consistently outperform those fine-tuned with Magpie-Air.
    The reason is that Magpie-Pro is generated by the more capable model, i.e., Llama-3-70B-Instruct.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的数量和质量对指令跟随能力都很重要。在接下来的内容中，我们在表 [2](#S4.T2 "表 2 ‣ 4.2 实验结果 ‣ 4 性能分析 ‣ Magpie：通过对齐的
    LLMs 进行无干预的对齐数据合成") 中比较了由 Magpie 生成的数据集家族。这些数据集在大小、过滤应用和生成数据的模型上有所不同。我们观察到，随着数据集大小的增加，经过微调的模型性能有所提升，这表明数据数量在提高指令跟随能力方面起着关键作用。此外，使用
    Magpie-Pro-300K-Filtered 进行微调的模型优于使用相同数量原始数据进行微调的模型。这展示了我们过滤技术的有效性，并强调了数据质量的重要性。最后，我们观察到，使用
    Magpie-Pro 微调的模型始终优于使用 Magpie-Air 微调的模型。原因在于 Magpie-Pro 是由更强大的模型生成的，即 Llama-3-70B-Instruct。
- en: 'Magpie can enhance the performance of other backbone models. Table [3](#S4.T3
    "Table 3 ‣ Magpie datasets outperform others. ‣ 4.2 Experimental Results ‣ 4 Performance
    Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned
    LLMs with Nothing") illustrates the efficacy of Magpie when applied to generate
    instruction dataset and fine-tune other backbone models, i.e., Qwen1.5-4B and
    Qwen1.5-7B. The results demonstrate that our fine-tuned models achieve better
    performance than the official aligned models, which have undergone instruction
    and preference tuning. These results underscore the effectiveness of Magpie and
    the quality of its generated instructions.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Magpie 可以增强其他骨干模型的性能。表 [3](#S4.T3 "表 3 ‣ Magpie 数据集优于其他数据集 ‣ 4.2 实验结果 ‣ 4 性能分析
    ‣ Magpie：通过对齐的 LLMs 进行无干预的对齐数据合成") 说明了 Magpie 在生成指令数据集并微调其他骨干模型（即 Qwen1.5-4B 和
    Qwen1.5-7B）时的有效性。结果表明，我们的微调模型表现优于经过指令和偏好调优的官方对齐模型。这些结果突显了 Magpie 的有效性及其生成指令的质量。
- en: 'Table 3: This table compares the performance of models instruction-tuned on
    the Qwen base models using the Magpie-Pro-300K-Filtered dataset and the official
    instruction-tuned models. The Qwen base model enhanced with Magpie consistently
    outperforms the official instruction-tuned model.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：该表比较了使用 Magpie-Pro-300K-Filtered 数据集对 Qwen 基础模型进行指令调优的模型性能与官方指令调优模型的性能。使用
    Magpie 增强的 Qwen 基础模型始终优于官方指令调优模型。
- en: '| Alignment Setup | AlpacaEval 2 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 对齐设置 | AlpacaEval 2 |'
- en: '| GPT-4-Turbo (1106) | Official Aligned Model as Ref. |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo (1106) | 官方对齐模型作为参考 |'
- en: '| LC (%) | WR (%) | SD | LC (%) | WR (%) | SD |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| LC (%) | WR (%) | SD | LC (%) | WR (%) | SD |'
- en: '| Qwen1.5-4B | Qwen1.5-4B-Chat | 5.89 | 4.74 | 0.67 | 50 | 50 | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-4B | Qwen1.5-4B-Chat | 5.89 | 4.74 | 0.67 | 50 | 50 | - |'
- en: '| Base Model + Magpie | 9.1 | 10.96 | 0.93 | 68.09 | 72.42 | 1.42 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型 + Magpie | 9.1 | 10.96 | 0.93 | 68.09 | 72.42 | 1.42 |'
- en: '| Qwen1.5-7B | Qwen1.5-7B-Chat | 14.75 | 11.77 | 0.97 | 50 | 50 | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-7B | Qwen1.5-7B-Chat | 14.75 | 11.77 | 0.97 | 50 | 50 | - |'
- en: '| Base Model + Magpie | 15.10 | 18.51 | 1.14 | 46.28 | 58.53 | 1.44 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型 + Magpie | 15.10 | 18.51 | 1.14 | 46.28 | 58.53 | 1.44 |'
- en: Additional Experimental Results.
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 额外实验结果。
- en: 'We defer additional experimental results and analysis of Magpie-Air-MT and
    Magpie-Pro-MT to Appendix [E.1](#A5.SS1 "E.1 Performance of Magpie-MT ‣ Appendix
    E Additional Experimental Results ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing"). Additionally, the performance of Magpie
    across various other benchmarks is reported in Appendix [E.3](#A5.SS3 "E.3 Performance
    of Magpie on More Benchmarks ‣ Appendix E Additional Experimental Results ‣ Magpie:
    Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing").'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将Magpie-Air-MT和Magpie-Pro-MT的额外实验结果和分析推迟到附录 [E.1](#A5.SS1 "E.1 Magpie-MT性能
    ‣ 附录E 额外实验结果 ‣ Magpie: 从零开始的对齐数据合成，通过提示对齐的LLMs")。此外，Magpie在各种其他基准上的表现报告见附录 [E.3](#A5.SS3
    "E.3 Magpie在更多基准上的性能 ‣ 附录E 额外实验结果 ‣ Magpie: 从零开始的对齐数据合成，通过提示对齐的LLMs")。'
- en: 5 Related Work
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: LLM Alignment.
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM对齐。
- en: Instruction tuning [[56](#bib.bib56)] and preference tuning [[5](#bib.bib5)]
    are widely used to align the responses of LLMs with human values. Instruction
    tuning utilizes an instruction dataset to fine-tune LLMs, where each instruction
    data consists of one turn or multiple turns of instructions and desired responses.
    The performance of instruction tuning heavily relies on the quality of instruction
    data [[47](#bib.bib47), [53](#bib.bib53), [67](#bib.bib67)]. Preference tuning
    further improves responses of LLMs using reinforcement learning human feedback
    (RLHF) [[5](#bib.bib5)] or preference optimization [[2](#bib.bib2), [19](#bib.bib19),
    [23](#bib.bib23), [44](#bib.bib44)] based on a preference dataset.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调优[[56](#bib.bib56)]和偏好调优[[5](#bib.bib5)]广泛用于将LLMs的响应与人类价值观对齐。指令调优利用指令数据集来微调LLMs，每条指令数据包含一轮或多轮指令和期望的响应。指令调优的性能严重依赖于指令数据的质量[[47](#bib.bib47),
    [53](#bib.bib53), [67](#bib.bib67)]。偏好调优进一步利用强化学习人类反馈（RLHF）[[5](#bib.bib5)]或基于偏好数据集的偏好优化[[2](#bib.bib2),
    [19](#bib.bib19), [23](#bib.bib23), [44](#bib.bib44)]来改善LLMs的响应。
- en: Alignment Dataset Construction.
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对齐数据集构建。
- en: 'We classify the existing methods of creating datasets for model alignment into
    two main categories: human interactions with LLMs and synthetic instruction generation.
    To create datasets for alignment, previous studies have collected human interactions
    with LLMs [[14](#bib.bib14), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [26](#bib.bib26)]. However, manually crafting instructions is not only time-consuming
    and labor-intensive, but may also incorporate toxic content [[64](#bib.bib64)].
    Another category of approaches [[53](#bib.bib53), [47](#bib.bib47), [58](#bib.bib58),
    [59](#bib.bib59), [55](#bib.bib55), [46](#bib.bib46)] focus on prompting LLMs
    to generate synthetic instruction datasets, beginning with a small set of human-annotated
    seed instructions and expanding these through few-shot prompting. However, these
    methods face a diversity challenge, as few-shot prompting often results in new
    instructions that are too similar to the original seed questions [[31](#bib.bib31)].
    To enhance coverage, some research [[16](#bib.bib16), [31](#bib.bib31)] summarizes
    world knowledge and employs it to generate synthetic datasets. We note that our
    Magpie dataset also belongs to the synthetic dataset. However, we leverage the
    prompt template with no requirement for seed questions or prompt engineering.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将现有的模型对齐数据集创建方法分为两大类：与LLMs的人工交互和合成指令生成。为了创建对齐数据集，以前的研究收集了与LLMs的人工交互[[14](#bib.bib14),
    [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [26](#bib.bib26)]。然而，手动编写指令不仅耗时且劳动密集，还可能包含有毒内容[[64](#bib.bib64)]。另一类方法[[53](#bib.bib53),
    [47](#bib.bib47), [58](#bib.bib58), [59](#bib.bib59), [55](#bib.bib55), [46](#bib.bib46)]则专注于促使LLMs生成合成指令数据集，从一小部分人工标注的种子指令开始，并通过少量示例提示扩展。然而，这些方法面临多样性挑战，因为少量示例提示往往会生成与原始种子问题过于相似的新指令[[31](#bib.bib31)]。为了增强覆盖范围，一些研究[[16](#bib.bib16),
    [31](#bib.bib31)]总结了世界知识并利用其生成合成数据集。我们注意到我们的Magpie数据集也属于合成数据集。然而，我们利用的提示模板无需种子问题或提示工程。
- en: Compared to the above two main categories, alignment data can also be generated
    by transforming existing data [[54](#bib.bib54), [45](#bib.bib45), [20](#bib.bib20)].
    However, the constrained variety of NLP tasks in these datasets may impede the
    ability of tuned LLMs to generalize in real-world scenarios [[31](#bib.bib31)].
    There are also mixture datasets (e.g., [[24](#bib.bib24), [49](#bib.bib49), [38](#bib.bib38),
    [67](#bib.bib67)]) that combine or select high-quality instruction data from various
    existing open-source instruction datasets to enhance coverage [[24](#bib.bib24),
    [49](#bib.bib49)] and/or improve overall performance [[38](#bib.bib38), [67](#bib.bib67)].
    There are also data construction methods focusing on improving the reasoning and
    math abilities [[61](#bib.bib61), [62](#bib.bib62)], which can be further merged
    with Magpie for creating a better mixture of data for instruction tuning.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述两个主要类别相比，对齐数据也可以通过转换现有数据来生成 [[54](#bib.bib54), [45](#bib.bib45), [20](#bib.bib20)]。然而，这些数据集中的NLP任务的受限多样性可能会妨碍微调LLMs在现实世界场景中的泛化能力
    [[31](#bib.bib31)]。还有一些混合数据集（例如，[[24](#bib.bib24), [49](#bib.bib49), [38](#bib.bib38),
    [67](#bib.bib67)]）结合或选择来自各种现有开源指令数据集的高质量指令数据，以提高覆盖范围 [[24](#bib.bib24), [49](#bib.bib49)]
    和/或改善整体性能 [[38](#bib.bib38), [67](#bib.bib67)]。还有一些数据构建方法专注于提高推理和数学能力 [[61](#bib.bib61),
    [62](#bib.bib62)]，这些方法可以与Magpie进一步结合，为指令调优创建更好的数据混合。
- en: Training Data Extraction.
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据提取。
- en: 'Language models have the capability to memorize examples from their training
    datasets, potentially enabling malicious users to extract private information
    [[8](#bib.bib8), [7](#bib.bib7), [9](#bib.bib9)]. Pioneering work [[27](#bib.bib27),
    [9](#bib.bib9), [41](#bib.bib41)] has demonstrated that it is possible to extract
    private pre-training data from BERT [[15](#bib.bib15)], GPT-2 [[43](#bib.bib43)],
    and ChatGPT [[1](#bib.bib1)], respectively. Yu et al. [[60](#bib.bib60)] propose
    several tricks including adjusting sampling strategies to better extract training
    datasets from language models. Recently, Kassem et. al. [[25](#bib.bib25)] propose
    a black-box prompt optimization method that uses an attacker LLM to extract high
    levels of memorization in a victim LLM. Wang et al. [[52](#bib.bib52)] leverage
    membership inference attack (MIA) to extract fine-tuning datasets from fine-tuned
    language models. Bai et al. [[4](#bib.bib4)] extracts the training dataset of
    production language models via special characters (e.g., structural symbols of
    JSON files, and , # in emails and online posts). Different from the prior work,
    we aim to create publicly available alignment datasets with minimal human effort
    by leveraging the remarkable generation capabilities of LLMs, rather than extracting
    private training data from LLMs.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '语言模型有能力记住训练数据集中的示例，这可能使恶意用户能够提取私人信息 [[8](#bib.bib8), [7](#bib.bib7), [9](#bib.bib9)]。开创性的工作
    [[27](#bib.bib27), [9](#bib.bib9), [41](#bib.bib41)] 已经证明可以从BERT [[15](#bib.bib15)]、GPT-2
    [[43](#bib.bib43)] 和ChatGPT [[1](#bib.bib1)]中提取私人预训练数据。Yu 等人 [[60](#bib.bib60)]
    提出了几种技巧，包括调整采样策略，以更好地从语言模型中提取训练数据集。最近，Kassem 等人 [[25](#bib.bib25)] 提出了一个黑盒提示优化方法，使用攻击者LLM在受害者LLM中提取高水平的记忆。Wang
    等人 [[52](#bib.bib52)] 利用成员推断攻击（MIA）从微调的语言模型中提取微调数据集。Bai 等人 [[4](#bib.bib4)] 通过特殊字符（例如，JSON文件的结构符号、电子邮件和在线帖子中的
    , #）提取生产语言模型的训练数据集。与之前的工作不同，我们旨在利用LLMs卓越的生成能力，创建可公开获取的对齐数据集，以最小的人力投入，而不是从LLMs中提取私人训练数据。'
- en: 6 Limitations and Ethical Considerations
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制和伦理考虑
- en: Limitations.
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制。
- en: In certain scenarios, users may aim to fine-tune LLMs using domain-specific
    instruction data. Investigating how to configure Magpie to efficiently generate
    the desired domain-specific instructions (e.g., math problems) is subject to our
    future work. Also, there is still a gap between Magpie-tuned LLMs and official
    Llama-3-Instruct on datasets such as WildBench and MMLU, which suggest that we
    should focus on producing harder reasoning tasks and feedback learning data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景中，用户可能会通过使用特定领域的指令数据来微调大型语言模型（LLMs）。研究如何配置Magpie以高效生成所需的领域特定指令（例如，数学问题）将是我们未来工作的重点。此外，Magpie微调的LLMs与官方Llama-3-Instruct在WildBench和MMLU等数据集上的表现仍存在差距，这表明我们应关注生成更具挑战性的推理任务和反馈学习数据。
- en: License and Legality.
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 许可和合法性。
- en: The instruction datasets generated by Magpie in this paper are subject to CC
    BY-NC license and Meta Llama 3 Community license. While users are permitted to
    distribute, adapt, and further develop our method Magpie, it is the responsibility
    of the users to apply Magpie to LLMs in compliance with the associated license
    agreement. We hereby disclaim any liability for misuse of data generated by users
    of Magpie.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中由 Magpie 生成的指令数据集受 CC BY-NC 许可证和 Meta Llama 3 社区许可证的约束。虽然允许用户分发、改编和进一步开发我们的方法
    Magpie，但用户有责任将 Magpie 应用于 LLMs 时遵守相关许可证协议。我们在此声明对 Magpie 用户生成数据的误用不承担任何责任。
- en: Societal Impact and Potential Harmful Consequences.
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 社会影响与潜在有害后果。
- en: 'The primary objective of this paper is to develop a scalable method to synthesize
    instruction data to enhance the instruction-following capabilities of LLMs, and
    thus align them with human values. However, the data generated by Magpie may contain
    harmful instructions and/or responses, which may lead to unsafe behaviors if used
    raw in instruction tuning. Our empirical evaluations indicate that such harmful
    data instances constitute less than 1% of the dataset. To mitigate this risk,
    we develop a filtering technique in Appendix [B](#A2 "Appendix B Filter Setups
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing") to identify and remove these instances.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要目标是开发一种可扩展的方法来合成指令数据，以增强大型语言模型（LLMs）的指令跟随能力，从而使其与人类价值观对齐。然而，Magpie 生成的数据可能包含有害的指令和/或响应，如果在指令调整中直接使用，可能会导致不安全的行为。我们的实证评估表明，这种有害数据实例在数据集中所占比例不到
    1%。为了降低这种风险，我们在附录 [B](#A2 "附录 B 过滤设置 ‣ Magpie：从头开始通过提示对齐的 LLMs 进行对齐数据合成") 中开发了一种过滤技术，以识别和去除这些实例。
- en: 7 Conclusion
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we developed a scalable method, Magpie, to synthesize instruction
    data for fine-tuning large language models. Magpie leveraged the predefined instruction
    templates of open-weight LLMs and crafted a prompt specifying only the role of
    instruction provider. Given the crafted prompt, the LLM then generated detailed
    instructions due to their auto-regressive nature. Magpie then sent the generated
    instructions to the LLM to generate corresponding responses. These pairs of instructions
    and responses constituted the instruction dataset. We used Llama-3-8B-instruct
    to label the instruction dataset and developed a filtering technique to select
    effective data instances for instruction tuning. We fine-tuned the Llama-3-8B
    base model using the selected data, and demonstrated that the fine-tuned model
    outperformed those fine-tuned using all baselines. Moreover, our fine-tuned models
    outperformed the official aligned model, Llama-3-8B-Instruct, which has been instruction-tuned
    and preference-optimized using more than 10M data instances. This highlighted
    the quality of the instruction data synthesized by Magpie.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们开发了一种可扩展的方法 Magpie，用于合成指令数据以微调大型语言模型。Magpie 利用了开放权重 LLMs 的预定义指令模板，并创建了一个仅指定指令提供者角色的提示。给定该提示后，LLM
    生成了详细的指令，由于其自回归性质。然后，Magpie 将生成的指令发送到 LLM 以生成相应的响应。这些指令和响应的配对构成了指令数据集。我们使用 Llama-3-8B-instruct
    对指令数据集进行了标注，并开发了过滤技术以选择有效的数据实例进行指令调整。我们使用选定的数据对 Llama-3-8B 基础模型进行了微调，并证明微调后的模型优于使用所有基准进行微调的模型。此外，我们微调的模型优于官方对齐模型
    Llama-3-8B-Instruct，该模型经过了指令调整和使用超过 1000 万数据实例的偏好优化。这突显了 Magpie 合成的指令数据的质量。
- en: 8 Acknowledgement
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 致谢
- en: 'The research of Z. Xu, F. Jiang, L. Niu, and R. Poovendran is partially supported
    by the National Science Foundation (NSF) AI Institute for Agent-based Cyber Threat
    Intelligence and Operation (ACTION) under grant IIS 2229876. The research of Y.
    Choi is partially supported by the National Science Foundation (NSF) under grant
    DMS-2134012 (Scaling Laws of Deep Learning) and the Office of Naval Research (ONR)
    under grant N00014-24-1-2207 (Symbolic Knowledge Distillation of LLMs for All:
    Diverse Scales, Skills, and Values).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Z. Xu、F. Jiang、L. Niu 和 R. Poovendran 的研究部分得到了国家科学基金会（NSF）AI 代理基础设施网络威胁情报与操作（ACTION）研究所的资助，资助编号为
    IIS 2229876。Y. Choi 的研究部分得到了国家科学基金会（NSF）资助，资助编号为 DMS-2134012（深度学习的缩放定律）和海军研究办公室（ONR）资助，资助编号为
    N00014-24-1-2207（LLMs 的符号知识蒸馏：多样化规模、技能和价值观）。
- en: This work is supported in part by funds provided by the National Science Foundation,
    Department of Homeland Security, and IBM. Any opinions, findings, and conclusions
    or recommendations expressed in this material are those of the author(s) and do
    not necessarily reflect the views of the NSF or its federal agency and industry
    partners.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分由国家科学基金会、国土安全部和IBM提供资金支持。材料中表达的任何观点、发现、结论或建议均为作者（们）的意见，不一定反映NSF或其联邦机构和行业合作伙伴的观点。
- en: References
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat 等人。GPT-4技术报告。arXiv预印本
    arXiv:2303.08774, 2023。'
- en: '[2] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark
    Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm
    to understand learning from human preferences. In International Conference on
    Artificial Intelligence and Statistics, pages 4447–4455\. PMLR, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark
    Rowland, Michal Valko, 和 Daniele Calandriello. 理解人类偏好的学习的一般理论范式。在国际人工智能与统计会议上，页码4447–4455。PMLR,
    2024。'
- en: '[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang
    Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
    Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang
    Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang,
    Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng
    Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,
    Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
    Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang
    Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji
    Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang
    Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang,
    Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng
    Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang,
    Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
    Zhu. Qwen技术报告。arXiv预印本 arXiv:2309.16609, 2023。'
- en: '[4] Yang Bai, Ge Pei, Jindong Gu, Yong Yang, and Xingjun Ma. Special characters
    attack: Toward scalable training data extraction from large language models. arXiv
    preprint arXiv:2405.05990, 2024.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Yang Bai, Ge Pei, Jindong Gu, Yong Yang, 和 Xingjun Ma. 特殊字符攻击：向可扩展训练数据提取迈进。arXiv预印本
    arXiv:2405.05990, 2024。'
- en: '[5] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav
    Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds,
    Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
    Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish,
    Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant
    with reinforcement learning from human feedback, 2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav
    Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds,
    Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
    Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish,
    Chris Olah, Ben Mann, 和 Jared Kaplan. 通过人类反馈的强化学习训练一个有用且无害的助手，2022年。'
- en: '[6] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert,
    Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard.
    [https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard),
    2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert,
    Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, 和 Thomas Wolf. Open llm排行榜。 [https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard),
    2023。'
- en: '[7] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf,
    Quentin Anthony, Shivanshu Purohit, and Edward Raff. Emergent and predictable
    memorization in large language models. Advances in Neural Information Processing
    Systems, 36, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf,
    Quentin Anthony, Shivanshu Purohit, 和 Edward Raff. 大型语言模型中的紧急和可预测的记忆。神经信息处理系统进展，36,
    2023。'
- en: '[8] Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and
    Florian Tramèr. What does it mean for a language model to preserve privacy? In
    Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency,
    pages 2280–2292, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri 和
    Florian Tramèr。语言模型保留隐私意味着什么？发表于2022年ACM公平性、问责制和透明度会议论文集，第2280–2292页，2022年。'
- en: '[9] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
    Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,
    et al. Extracting training data from large language models. In 30th USENIX Security
    Symposium (USENIX Security 21), pages 2633–2650, 2021.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
    Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson
    等人。 从大型语言模型中提取训练数据。发表于第30届USENIX安全研讨会（USENIX Security 21），第2633–2650页，2021年。'
- en: '[10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica 和
    Eric P. Xing。Vicuna：一个开源聊天机器人，以 90%* ChatGPT 质量打动 GPT-4，2023年3月。'
- en: '[11] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick 和 Oyvind Tafjord。认为你解决了问题回答？尝试 arc，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457，2018年。'
- en: '[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,
    2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano
    等人。训练验证器解决数学文字问题。arXiv 预印本 arXiv:2110.14168，2021年。'
- en: '[13] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong
    Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with
    high-quality feedback. arXiv preprint arXiv:2310.01377, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong
    Xie, Zhiyuan Liu 和 Maosong Sun。Ultrafeedback：通过高质量反馈提升语言模型。arXiv 预印本 arXiv:2310.01377，2023年。'
- en: '[14] Databricks. Databricks dolly-15k, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Databricks。Databricks dolly-15k，2023年。'
- en: '[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova。BERT：用于语言理解的深度双向变换器预训练。arXiv
    预印本 arXiv:1810.04805，2018年。'
- en: '[16] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
    Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality
    instructional conversations. arXiv preprint arXiv:2305.14233, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
    Liu, Maosong Sun 和 Bowen Zhou。通过扩展高质量的指令对话来增强聊天语言模型。arXiv 预印本 arXiv:2305.14233，2023年。'
- en: '[17] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,
    Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss
    library. 2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,
    Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini 和 Hervé Jégou。Faiss 库。2024年。'
- en: '[18] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto.
    Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv
    preprint arXiv:2404.04475, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Yann Dubois, Balázs Galambosi, Percy Liang 和 Tatsunori B Hashimoto。长度控制的
    alpacaeval：一种简单的去偏自动评估工具的方法。arXiv 预印本 arXiv:2404.04475，2024年。'
- en: '[19] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe
    Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint
    arXiv:2402.01306, 2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky 和 Douwe
    Kiela。Kto：模型对齐作为前景理论优化。arXiv 预印本 arXiv:2402.01306，2024年。'
- en: '[20] Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham
    Neubig. Better synthetic data by retrieving and transforming existing datasets.
    arXiv preprint arXiv:2404.14361, 2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu 和 Graham Neubig。通过检索和转化现有数据集来生成更好的合成数据。arXiv
    预印本 arXiv:2404.14361，2024年。'
- en: '[21] Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto,
    Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad
    Reza Ghasemi Madani, et al. Are we done with mmlu? arXiv preprint arXiv:2406.04127,
    2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto,
    Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad
    Reza Ghasemi Madani 等人。《我们对 MMLU 的研究结束了吗？》。arXiv 预印本 arXiv:2406.04127，2024年。'
- en: '[22] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding.
    arXiv preprint arXiv:2009.03300, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song 和 Jacob Steinhardt。《测量大规模多任务语言理解》。arXiv 预印本 arXiv:2009.03300，2020年。'
- en: '[23] Jiwoo Hong, Noah Lee, and James Thorne. Reference-free monolithic preference
    optimization with odds ratio. arXiv preprint arXiv:2403.07691, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jiwoo Hong, Noah Lee 和 James Thorne。《无参考的整体偏好优化与赔率比》。arXiv 预印本 arXiv:2403.07691，2024年。'
- en: '[24] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew
    Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al.
    Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint
    arXiv:2311.10702, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew
    Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy 等人。《气候变化中的骆驼：使用
    Tulu 2 增强语言模型的适应能力》。arXiv 预印本 arXiv:2311.10702，2023年。'
- en: '[25] Aly M Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia
    Tsvetkov, Yejin Choi, Sherif Saad, and Santu Rana. Alpaca against vicuna: Using
    llms to uncover memorization of llms. arXiv preprint arXiv:2403.04801, 2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Aly M Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia
    Tsvetkov, Yejin Choi, Sherif Saad 和 Santu Rana。《Alpaca 对抗 Vicuna：使用大语言模型揭示大语言模型的记忆》。arXiv
    预印本 arXiv:2403.04801，2024年。'
- en: '[26] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,
    Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd
    Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire,
    Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. Openassistant conversations
    - democratizing large language model alignment. In A. Oh, T. Naumann, A. Globerson,
    K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing
    Systems, volume 36, pages 47669–47681\. Curran Associates, Inc., 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,
    Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd
    Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire,
    Christoph Schuhmann, Huu Nguyen 和 Alexander Mattick。《Openassistant 对话——使大型语言模型对齐民主化》。收录于
    A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt 和 S. Levine 编著的《神经信息处理系统进展》，第36卷，第47669–47681页。Curran
    Associates, Inc.，2023年。'
- en: '[27] Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot,
    and Mohit Iyyer. Thieves on sesame street! model extraction of bert-based apis.
    In International Conference on Learning Representations, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot
    和 Mohit Iyyer。《大街上的小偷！基于 BERT 的 API 模型提取》。收录于国际学习表征会议，2020年。'
- en: '[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management
    for large language model serving with pagedattention. In Proceedings of the ACM
    SIGOPS 29th Symposium on Operating Systems Principles, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
    Hao Yu, Joseph E. Gonzalez, Hao Zhang 和 Ion Stoica。《用于大型语言模型服务的高效内存管理与分页注意力》。收录于
    ACM SIGOPS 第29届操作系统原则研讨会会议录，2023年。'
- en: '[29] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen
    Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench:
    Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787,
    2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen
    Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi 等人。《Rewardbench：评估语言建模的奖励模型》。arXiv
    预印本 arXiv:2403.13787，2024年。'
- en: '[30] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema
    challenge. In Thirteenth international conference on the principles of knowledge
    representation and reasoning, 2012.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Hector Levesque, Ernest Davis 和 Leora Morgenstern。《Winograd 语法挑战》。收录于第十三届国际知识表示与推理原则会议，2012年。'
- en: '[31] Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang,
    Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, et al.
    Synthetic data (almost) from scratch: Generalized instruction tuning for language
    models. arXiv preprint arXiv:2402.13064, 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang,
    Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang 等人。《几乎从头开始生成的数据：语言模型的广义指令调优》。arXiv
    预印本 arXiv:2402.13064，2024年。'
- en: '[32] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E.
    Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard
    pipeline, April 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph
    E. Gonzalez 和 Ion Stoica。从实时数据到高质量基准：Arena-hard 流水线，2024年4月。'
- en: '[33] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani,
    Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic
    evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval),
    2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani,
    Carlos Guestrin, Percy Liang 和 Tatsunori B. Hashimoto。Alpacaeval：一种自动评估指令跟随模型的工具。
    [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)，2023年。'
- en: '[34] Bill Yuchen Lin, Khyathi Chandu, Faeze Brahman, Yuntian Deng, Abhilasha
    Ravichander, Valentina Pyatkin, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking
    language models with challenging tasks from real users in the wild, 2024.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Bill Yuchen Lin, Khyathi Chandu, Faeze Brahman, Yuntian Deng, Abhilasha
    Ravichander, Valentina Pyatkin, Ronan Le Bras 和 Yejin Choi。Wildbench：使用来自实际用户的挑战性任务对语言模型进行基准测试，2024年。'
- en: '[35] Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie
    Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell
    on base llms: Rethinking alignment via in-context learning. arXiv preprint arXiv:2312.01552,
    2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie
    Sclar, Khyathi Chandu, Chandra Bhagavatula 和 Yejin Choi。基础LLMs的解锁咒语：通过上下文学习重新思考对齐。arXiv
    预印本 arXiv:2312.01552，2023年。'
- en: '[36] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how
    models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Stephanie Lin, Jacob Hilton 和 Owain Evans。Truthfulqa：衡量模型如何模仿人类虚假信息。arXiv
    预印本 arXiv:2109.07958，2021年。'
- en: '[37] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao,
    Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons
    learned on synthetic data for language models. arXiv preprint arXiv:2404.07503,
    2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao,
    Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou 等人。语言模型合成数据的最佳实践和经验教训。arXiv 预印本
    arXiv:2404.07503，2024年。'
- en: '[38] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes
    good data for alignment? a comprehensive study of automatic data selection in
    instruction tuning. In The Twelfth International Conference on Learning Representations,
    2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang 和 Junxian He。什么样的数据适合对齐？关于自动数据选择在指令调优中的综合研究。第十二届国际学习表征会议，2024年。'
- en: '[39] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization
    with a reference-free reward. arXiv preprint arXiv:2405.14734, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yu Meng, Mengzhou Xia 和 Danqi Chen。Simpo：一种基于无参考奖励的简单偏好优化方法。arXiv 预印本
    arXiv:2405.14734，2024年。'
- en: '[40] Meta. Llama 3. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/),
    2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Meta. Llama 3. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)，2024年。'
- en: '[41] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder
    Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tramèr,
    and Katherine Lee. Scalable extraction of training data from (production) language
    models. arXiv preprint arXiv:2311.17035, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder
    Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tramèr
    和 Katherine Lee。可扩展的从（生产）语言模型中提取训练数据。arXiv 预印本 arXiv:2311.17035，2023年。'
- en: '[42] OpenAI. Tiktoken. [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken),
    2024.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] OpenAI。Tiktoken。 [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)，2024年。'
- en: '[43] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. 2018.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever 等人。通过生成预训练改善语言理解。2018年。'
- en: '[44] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning,
    Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language
    model is secretly a reward model. In Thirty-seventh Conference on Neural Information
    Processing Systems, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning,
    Stefano Ermon 和 Chelsea Finn。直接偏好优化：你的语言模型实际上是一个奖励模型。第三十七届神经信息处理系统会议，2023年。'
- en: '[45] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika,
    Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful
    Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon
    Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian
    Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel
    Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli,
    Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman,
    Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables
    zero-shot task generalization. In International Conference on Learning Representations,
    2022.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Victor Sanh、Albert Webson、Colin Raffel、Stephen Bach、Lintang Sutawika、Zaid
    Alyafeai、Antoine Chaffin、Arnaud Stiegler、Arun Raja、Manan Dey、M Saiful Bari、Canwen
    Xu、Urmish Thakker、Shanya Sharma Sharma、Eliza Szczechla、Taewoon Kim、Gunjan Chhablani、Nihal
    Nayak、Debajyoti Datta、Jonathan Chang、Mike Tian-Jian Jiang、Han Wang、Matteo Manica、Sheng
    Shen、Zheng Xin Yong、Harshit Pandey、Rachel Bawden、Thomas Wang、Trishala Neeraj、Jos
    Rozen、Abheesht Sharma、Andrea Santilli、Thibault Fevry、Jason Alan Fries、Ryan Teehan、Teven
    Le Scao、Stella Biderman、Leo Gao、Thomas Wolf 和 Alexander M Rush。多任务提示训练实现零样本任务泛化。载于国际学习表示会议，2022年。'
- en: '[46] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen,
    David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language
    models from scratch with minimal human supervision. Advances in Neural Information
    Processing Systems, 36, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Zhiqing Sun、Yikang Shen、Qinhong Zhou、Hongxin Zhang、Zhenfang Chen、David
    Cox、Yiming Yang 和 Chuang Gan。从零开始的原则驱动自对齐语言模型，最少人工监督。神经信息处理系统进展，36，2023年。'
- en: '[47] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An
    instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Rohan Taori、Ishaan Gulrajani、Tianyi Zhang、Yann Dubois、Xuechen Li、Carlos
    Guestrin、Percy Liang 和 Tatsunori B. Hashimoto。斯坦福 alpaca：一种跟随指令的 llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023年。'
- en: '[48] Llama Team. Meta llama guard 2. [https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md](https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md),
    2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Llama Team。Meta llama guard 2。 [https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md](https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md)，2024年。'
- en: '[49] Teknium. Openhermes dataset, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Teknium。Openhermes 数据集，2023年。'
- en: '[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。Llama
    2：开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288，2023年。'
- en: '[51] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne.
    Journal of machine learning research, 9(11), 2008.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Laurens Van der Maaten 和 Geoffrey Hinton。使用 t-sne 可视化数据。机器学习研究杂志，9(11)，2008年。'
- en: '[52] Jeffrey G Wang, Jason Wang, Marvin Li, and Seth Neel. Pandora’s white-box:
    Increased training data leakage in open llms. arXiv preprint arXiv:2402.17012,
    2024.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Jeffrey G Wang、Jason Wang、Marvin Li 和 Seth Neel。Pandora 的白盒：开放 llms 中增加的训练数据泄漏。arXiv
    预印本 arXiv:2402.17012，2024年。'
- en: '[53] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith,
    Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models
    with self-generated instructions. In Proceedings of the 61st Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484–13508,
    Toronto, Canada, 2023\. Association for Computational Linguistics.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Yizhong Wang、Yeganeh Kordi、Swaroop Mishra、Alisa Liu、Noah A. Smith、Daniel
    Khashabi 和 Hannaneh Hajishirzi。Self-instruct：将语言模型与自生成指令对齐。载于《第61届计算语言学协会年会论文集（第1卷：长篇论文）》，页13484–13508，加拿大多伦多，2023年。计算语言学协会。'
- en: '[54] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
    Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar,
    David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani
    Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya
    Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha
    Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur
    Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong
    Shen. Super-NaturalInstructions: Generalization via declarative instructions on
    1600+ NLP tasks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
    Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,
    pages 5085–5109, Abu Dhabi, United Arab Emirates, December 2022\. Association
    for Computational Linguistics.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 王一钟、斯瓦罗普·米什拉、佩加赫·阿利普尔莫拉巴希、耶加内赫·科尔迪、阿米雷扎·米尔扎伊、阿塔尔瓦·奈克、阿尔君·阿肖克、阿鲁特·塞尔万·丹纳塞卡兰、安贾纳·阿伦库马尔、大卫·斯塔普、伊沙恩·帕塔克、吉安尼斯·卡拉马诺拉基斯、赖海志、伊尚·普罗希特、伊莎尼·蒙达尔、雅各布·安德森、柯比·库兹尼亚、克里玛·多希、坎塔尔·库马尔·帕尔、迈特里亚·帕特尔、梅赫拉德·莫拉德沙希、米赫尔·帕尔玛、米拉里·普罗希特、尼拉杰·瓦尔什尼、帕尼·罗希塔·卡扎、普尔基特·维尔玛、拉维谢贾·辛格·普里、鲁尚·卡里亚、萨万·多希、沙伊拉贾·凯尔·桑帕特、西达尔塔·米什拉、苏詹·雷迪·A、苏曼塔·帕特罗、塔奈·迪克西特和沈旭东。Super-NaturalInstructions：通过声明性指令在1600+
    NLP任务中实现泛化。在Yoav Goldberg、佐尔尼察·科扎列娃和岳张（编辑），2022年自然语言处理实证方法会议论文集，页5085–5109，阿布扎比，阿联酋，2022年12月。计算语言学协会。'
- en: '[55] Zifeng Wang, Chun-Liang Li, Vincent Perot, Long T Le, Jin Miao, Zizhao
    Zhang, Chen-Yu Lee, and Tomas Pfister. Codeclm: Aligning language models with
    tailored synthetic data. arXiv preprint arXiv:2404.05875, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 王子峰、李春亮、文森特·佩罗、长T·李、苗晋、张子钊、李辰宇和托马斯·普菲斯特。Codeclm：使用量身定制的合成数据对齐语言模型。arXiv
    预印本 arXiv:2404.05875，2024年。'
- en: '[56] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian
    Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot
    learners. In International Conference on Learning Representations, 2022.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 韦杰森、博斯马滕、赵文森、顾凯文、亚当斯·韦·余、布莱恩·莱斯特、杜楠、安德鲁·M·戴和屈克·V·李。微调语言模型是零样本学习者。在国际学习表征会议，2022年。'
- en: '[57] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang,
    and Tong Zhang. Iterative preference learning from human feedback: Bridging theory
    and practice for rlhf under kl-constraint, 2024.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 熊伟、董汉泽、叶辰璐、王子琪、钟涵、季恒、蒋楠和张童。基于人类反馈的迭代偏好学习：在KL约束下将理论与实践桥接的RLHF，2024年。'
- en: '[58] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang
    Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex
    instructions. arXiv preprint arXiv:2304.12244, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 徐灿、孙庆丰、郑凯、耿修博、赵璞、冯家战、陶崇阳和姜大新。Wizardlm：赋能大型语言模型以遵循复杂指令。arXiv 预印本 arXiv:2304.12244，2023年。'
- en: '[59] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source
    chat model with parameter-efficient tuning on self-chat data. In Houda Bouamor,
    Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, pages 6268–6278, Singapore, December 2023\.
    Association for Computational Linguistics.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 徐灿文、郭大亚、段楠和朱利安·麦考利。Baize：一个在自聊数据上具有参数高效调优的开源聊天模型。在霍达·布阿莫尔、胡安·皮诺和卡利卡·巴利（编辑），2023年自然语言处理实证方法会议论文集，页6268–6278，新加坡，2023年12月。计算语言学协会。'
- en: '[60] Weichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min
    Lin, and Shuicheng Yan. Bag of tricks for training data extraction from language
    models. In International Conference on Machine Learning, pages 40306–40320\. PMLR,
    2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 于伟晨、庞天宇、刘倩、杜超、康冰怡、黄艳、林敏和阎水成。训练数据提取的技巧集合。国际机器学习会议，页40306–40320。PMLR，2023年。'
- en: '[61] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su,
    and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction
    tuning. ArXiv, abs/2309.05653, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 岳翔、曲星伟、张格、傅瑶、黄文浩、孙欢、苏宇和陈文虎。Mammoth：通过混合指令调优构建数学通用模型。ArXiv，abs/2309.05653，2023年。'
- en: '[62] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions
    from the web, 2024.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 岳翔、郑天依、张格和陈文虎。Mammoth2：从网络扩展指令，2024年。'
- en: '[63] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,
    2019.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 罗温·泽勒斯、阿里·霍尔茨曼、约纳坦·比斯克、阿里·法赫拉迪和叶晋·崔。Hellaswag：机器真的能完成你的句子吗？arXiv 预印本 arXiv:1905.07830，2019年。'
- en: '[64] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian
    Deng. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International
    Conference on Learning Representations, 2024.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, 和 Yuntian
    Deng. Wildchat: 100 万条 chatGPT 在真实环境中的互动日志。在第十二届国际学习表征会议，2024年。'
- en: '[65] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica,
    and Hao Zhang. LMSYS-chat-1m: A large-scale real-world LLM conversation dataset.
    In The Twelfth International Conference on Learning Representations, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica,
    和 Hao Zhang. LMSYS-chat-1m: 大规模真实世界 LLM 对话数据集。在第十二届国际学习表征会议，2024年。'
- en: '[66] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E.
    Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena.
    In Thirty-seventh Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E.
    Gonzalez, 和 Ion Stoica. 使用 MT-bench 和聊天机器人竞技场评估 LLM 作为评审的能力。在第三十七届神经信息处理系统会议数据集和基准测试专题，2023年。'
- en: '[67] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning
    Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment.
    Advances in Neural Information Processing Systems, 36, 2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning
    Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, 等等. Lima: 对齐的“少即是多”。神经信息处理系统进展,
    36, 2023年。'
- en: Appendix A Magpie Extension
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A Magpie 扩展
- en: In this section, we explore the extension of Magpie. We first outline the process
    for constructing a multi-turn dataset (Magpie-MT). We then discuss methods for
    controlling instruction tasks using Magpie. Finally, we will briefly discuss how
    to develop a preference optimization dataset based on Magpie.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨 Magpie 的扩展。我们首先概述了构建多轮数据集（Magpie-MT）的过程。然后，我们讨论了使用 Magpie 控制指令任务的方法。最后，我们将简要讨论如何基于
    Magpie 开发偏好优化数据集。
- en: A.1 Building Multi-Turn Magpie
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 构建多轮 Magpie
- en: 'To construct Magpie-MT, we initially follow Steps 1 and 2 to generate the first
    turn of instruction and response. For subsequent turns, we append the pre-query
    template to the end of the full prompt from the previous round of communication.
    We have observed that the model may occasionally forget its role as the user,
    especially for the 8B model. To mitigate this, we employ a system prompt designed
    to control the behavior of the LLM and reinforce its awareness of the multi-round
    conversation context. The full prompt for building the instructions of Magpie-MT
    can be found in Figure [11](#A6.F11 "Figure 11 ‣ F.1 Prompt Templates for Magpie
    Extension ‣ Appendix F Prompt Templates ‣ Magpie: Alignment Data Synthesis from
    Scratch by Prompting Aligned LLMs with Nothing") in Appendix [F](#A6 "Appendix
    F Prompt Templates ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing"). We follow the procedure described in Step 2 of Section
    [2](#S2 "2 Magpie: A Scalable Method to Synthesize Instruction Data ‣ Magpie:
    Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing")
    to generate responses and yield the multi-turn instruction dataset.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建 Magpie-MT，我们最初遵循第 1 步和第 2 步来生成第一轮指令和回应。对于后续轮次，我们将预查询模板附加到上一轮交流的完整提示末尾。我们观察到，模型有时可能会忘记其作为用户的角色，特别是对于
    8B 模型。为了缓解这一问题，我们采用了旨在控制 LLM 行为的系统提示，并加强其对多轮对话上下文的意识。构建 Magpie-MT 指令的完整提示可以在附录
    [F](#A6 "附录 F 提示模板 ‣ Magpie：从头开始通过提示对齐 LLM 进行对齐数据合成") 的图 [11](#A6.F11 "图 11 ‣
    F.1 Magpie 扩展提示模板 ‣ 附录 F 提示模板 ‣ Magpie：从头开始通过提示对齐 LLM 进行对齐数据合成") 中找到。我们按照第 [2](#S2
    "2 Magpie：一种可扩展的方法来合成指令数据 ‣ Magpie：从头开始通过提示对齐 LLM 进行对齐数据合成") 节第 2 步中描述的程序来生成回应，并得到多轮指令数据集。
- en: A.2 Control Instruction Tasks of Magpie
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 Magpie 控制指令任务
- en: 'In some scenarios, users may wish to fine-tune large language models (LLMs)
    using domain-specific instruction data, such as code or mathematical content,
    to enhance performance within specific domains. In this section, we introduce
    a lightweight and effective method to control the task category of generated instructions.
    Our approach involves guiding LLMs through the system prompt by specifying that
    it is a chatbot tailored for a particular domain and outlining the types of user
    queries it might encounter. We provide an example of a system prompt designed
    to control the generation of math-related instructions, as illustrated in Figure
    [12](#A6.F12 "Figure 12 ‣ F.1 Prompt Templates for Magpie Extension ‣ Appendix
    F Prompt Templates ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing") within Appendix [F](#A6 "Appendix F Prompt Templates
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing").'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，用户可能希望使用特定领域的指令数据（如代码或数学内容）来微调大型语言模型（LLMs），以提高在特定领域内的表现。在本节中，我们介绍了一种轻量且有效的方法来控制生成指令的任务类别。我们的方法通过系统提示引导
    LLMs，指定它是一个针对特定领域定制的聊天机器人，并概述可能遇到的用户查询类型。我们提供了一个控制数学相关指令生成的系统提示示例，如附录 [F](#A6
    "附录 F 提示模板 ‣ Magpie：从零开始通过提示对齐 LLMs 的对齐数据合成")中的图 [12](#A6.F12 "图 12 ‣ F.1 Magpie
    扩展的提示模板 ‣ 附录 F 提示模板 ‣ Magpie：从零开始通过提示对齐 LLMs 的对齐数据合成") 所示。
- en: A.3 Building Preference Optimization Dataset with Magpie
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 使用 Magpie 构建偏好优化数据集
- en: Magpie can be readily adapted to create preference datasets by integrating responses
    generated by the instruct model with those from the base model. Specifically,
    utilizing the reward difference outlined in Section [3](#S3 $$. We will soon open-source
    Magpie-PO, a preference optimization dataset to further align LLMs with human
    preferences.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Magpie 可以通过将指令模型生成的响应与基础模型生成的响应进行整合，从而轻松调整以创建偏好数据集。具体来说，利用第 [3](#S3 $$ 节中概述的奖励差异，我们将很快开源
    Magpie-PO，一个偏好优化数据集，以进一步将 LLMs 与人类偏好对齐。
- en: Appendix B Filter Setups
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 过滤设置
- en: 'In this section, we explore potential filter configurations for selecting high-quality
    instructional data for fine-tuning purposes. We provide the following metrics
    to enable users to customize their filtered Magpie dataset:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了用于选择高质量教学数据以进行微调的潜在过滤器配置。我们提供了以下指标，以便用户自定义其过滤后的 Magpie 数据集：
- en: '1.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Input Length: The total number of characters in the instructions.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入长度：指令中的字符总数。
- en: '2.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Output Length: The total number of characters in the responses.'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出长度：响应中的字符总数。
- en: '3.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Task Category: The specific category of the instructions. See Appendix [C.1](#A3.SS1
    "C.1 Additional Analysis on Dataset Coverage and Attributes. ‣ Appendix C More
    Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing") for details.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务类别：指令的具体类别。有关详细信息，请参见附录 [C.1](#A3.SS1 "C.1 数据集覆盖范围和属性的附加分析 ‣ 附录 C 更多数据集分析
    ‣ Magpie：从零开始通过提示对齐 LLMs 的对齐数据合成")。
- en: '4.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Input Quality: The clarity, specificity, and coherence of the instructions,
    rated as ‘very poor’, ‘poor’, ‘average’, ‘good’, and ‘excellent’.'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入质量：指令的清晰度、具体性和连贯性，评级为‘非常差’，‘差’，‘一般’，‘好’，和‘优秀’。
- en: '5.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Input Difficulty: The level of knowledge required to address the task described
    in the instruction, rated as ‘very easy’, ‘easy’, ‘medium’, ‘hard’, or ‘very hard’.'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入难度：解决指令中描述的任务所需的知识水平，评级为‘非常简单’，‘简单’，‘中等’，‘困难’，或‘非常困难’。
- en: '6.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Minimum Neighbor Distance: The embedding distance to the nearest neighbor.
    Can be used for filtering out repetitive or similar instances.'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最小邻居距离：嵌入到最近邻的距离。可用于过滤重复或相似的实例。
- en: '7.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Reward: Denoted as $r^{*}$. See Section [3](#S3 "3 Dataset Analysis ‣ Magpie:
    Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing")
    for details. This metric can be used to filter out low-quality responses, such
    as repetitions or refusals.'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励：表示为 $r^{*}$。有关详细信息，请参见第 [3](#S3 "3 数据集分析 ‣ Magpie：从零开始通过提示对齐 LLMs 的对齐数据合成")
    节。该指标可用于过滤低质量响应，如重复或拒绝的回应。
- en: '8.'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Reward Difference: Denoted as $r^{*}-r_{base}$. See Section [3](#S3 "3 Dataset
    Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned
    LLMs with Nothing") for details.'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励差异：表示为 $r^{*}-r_{base}$。有关详细信息，请参见第 [3](#S3 "3 数据集分析 ‣ Magpie：从零开始通过提示对齐 LLMs
    的对齐数据合成") 节。
- en: 'We provide several off-the-shelf configurations, as demonstrated in Table [4](#A2.T4
    "Table 4 ‣ Appendix B Filter Setups ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing"). We defer the detailed performance analysis
    of each filter configuration for Magpie-Pro to Appendix [E.2](#A5.SS2 "E.2 Ablation
    Analysis on Filter Designs ‣ Appendix E Additional Experimental Results ‣ Magpie:
    Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing").'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了几个现成的配置，如表[4](#A2.T4 "表 4 ‣ 附录 B 过滤器设置 ‣ Magpie: 从零开始通过提示对齐 LLMs 的对齐数据合成")所示。我们将
    Magpie-Pro 各过滤器配置的详细性能分析推迟到附录[E.2](#A5.SS2 "E.2 过滤器设计的消融分析 ‣ 附录 E 额外实验结果 ‣ Magpie:
    从零开始通过提示对齐 LLMs 的对齐数据合成")。'
- en: 'Table 4: Different filter configurations we provide. We note that the Output
    Length filter is applied last. Specifically, this filter selects the $k$.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 我们提供的不同过滤器配置。我们注意到，输出长度过滤器是最后应用的。具体来说，该过滤器选择 $k$。'
- en: '| Source Dataset | Filter Name | #Convs | Input Length | Output Length | Task
    Category | Input Quality | Input Difficulty | Min Neighbor Distance | Reward |
    Reward Difference |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 来源数据集 | 过滤器名称 | #Convs | 输入长度 | 输出长度 | 任务类别 | 输入质量 | 输入难度 | 最小邻近距离 | 奖励 |
    奖励差异 |'
- en: '| Magpie-Air | Filter | 300K | - | Longest | - | $\geq$ |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Air | 过滤器 | 300K | - | 最长 | - | $\geq$ |'
- en: '| Magpie-Pro | Filter | 300K | - | Longest | - | $\geq$ | - |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Pro | 过滤器 | 300K | - | 最长 | - | $\geq$ | - |'
- en: '| Filter2 | 300K | - | Longest | - | $\geq$ | - |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Filter2 | 300K | - | 最长 | - | $\geq$ | - |'
- en: '| Filter3 | 300K | - | Longest | - | - | - |  | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Filter3 | 300K | - | 最长 | - | - | - |  | - |'
- en: '| Filter4 | 300K | - | Longest | - | $\geq$ |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Filter4 | 300K | - | 最长 | - | $\geq$ |'
- en: '| Filter5 | 338K | - | - | - | $\geq$ | - |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Filter5 | 338K | - | - | - | $\geq$ | - |'
- en: '|  | Filter6 | 200K | - | Longest | - | - | $50\%$ | - |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | Filter6 | 200K | - | 最长 | - | - | $50\%$ | - |'
- en: Appendix C More Dataset Analysis
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 更多数据集分析
- en: 'This section provides additional dataset analysis, complementing the discussions
    in Section [3](#S3 "3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis from
    Scratch by Prompting Aligned LLMs with Nothing").'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '本节提供了额外的数据集分析，补充了[3](#S3 "3 数据集分析 ‣ Magpie: 从零开始通过提示对齐 LLMs 的对齐数据合成")节中的讨论。'
- en: C.1 Additional Analysis on Dataset Coverage and Attributes.
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 数据集覆盖和属性的附加分析。
- en: Task Categories of Magpie-Pro and Magpie-Air.
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Magpie-Pro 和 Magpie-Air 的任务类别。
- en: 'Figure [7](#A3.F7 "Figure 7 ‣ Task Categories of Magpie-Pro and Magpie-Air.
    ‣ C.1 Additional Analysis on Dataset Coverage and Attributes. ‣ Appendix C More
    Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing") illustrates the task category distributions for Magpie-Pro
    and Magpie-Air, as labeled by Llama-3-Instruct. We observe that the task category
    distributions of these two datasets are largely similar, however, Magpie-Pro exhibits
    a higher percentage of creative writing tasks.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '图[7](#A3.F7 "图 7 ‣ Magpie-Pro 和 Magpie-Air 的任务类别。 ‣ C.1 数据集覆盖和属性的附加分析。 ‣ 附录
    C 更多数据集分析 ‣ Magpie: 从零开始通过提示对齐 LLMs 的对齐数据合成") 说明了 Magpie-Pro 和 Magpie-Air 的任务类别分布，由
    Llama-3-Instruct 标记。我们观察到这两个数据集的任务类别分布大致相似，但 Magpie-Pro 的创意写作任务所占比例更高。'
- en: '![Refer to caption](img/2e55cfd73f66f5dd72ba40dd8a2e3d77.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2e55cfd73f66f5dd72ba40dd8a2e3d77.png)'
- en: (a) Task categories of Magpie-Pro.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Magpie-Pro 的任务类别。
- en: '![Refer to caption](img/fde4b24f96e6d38772a8bbe3201a7f3c.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fde4b24f96e6d38772a8bbe3201a7f3c.png)'
- en: (b) Task categories of Magpie-Air.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Magpie-Air 的任务类别。
- en: 'Figure 7: This figure visualizes the task category of Magpie-Pro and Magpie-Air
    by topic tags.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 此图通过主题标签可视化了 Magpie-Pro 和 Magpie-Air 的任务类别。'
- en: Visualization of Root Verbs and Their Direct Noun Objects.
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 根动词及其直接名词对象的可视化。
- en: 'Figure [8](#A3.F8 "Figure 8 ‣ Visualization of Root Verbs and Their Direct
    Noun Objects. ‣ C.1 Additional Analysis on Dataset Coverage and Attributes. ‣
    Appendix C More Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing") visualizes the top common root verbs
    and their direct noun objects of Magpie-Air dataset. This indicates the diverse
    topic coverage of MAGPIE-Air.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '图[8](#A3.F8 "图 8 ‣ 根动词及其直接名词对象的可视化。 ‣ C.1 数据集覆盖和属性的附加分析。 ‣ 附录 C 更多数据集分析 ‣ Magpie:
    从零开始通过提示对齐 LLMs 的对齐数据合成") 可视化了 Magpie-Air 数据集中的最常见根动词及其直接名词对象。这表明 MAGPIE-Air 的主题覆盖广泛。'
- en: '![Refer to caption](img/e190185ebbf89675af57ec31f00cf0cf.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e190185ebbf89675af57ec31f00cf0cf.png)'
- en: 'Figure 8: This figure demonstrates the top 20 most common root verbs (shown
    in the inner circle) and their top 5 direct noun objects (shown in the outer circle)
    within the Magpie-Air dataset. This indicates that Magpie encompasses a broad
    range of topics.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：此图展示了 Magpie-Air 数据集中最常见的 20 个根动词（显示在内圈）及其前 5 个直接名词对象（显示在外圈）。这表明 Magpie
    涵盖了广泛的主题。
- en: C.2 Additional Safety Analysis
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 附加安全分析
- en: 'Table [5](#A3.T5 "Table 5 ‣ C.2 Additional Safety Analysis ‣ Appendix C More
    Dataset Analysis ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing") illustrates the percentage of different unsafe categories
    of Magpie-Air and Magpie-Pro, as labeled by Llama-Guard-2 [[48](#bib.bib48)].
    We have two key observations. First, the proportion of data containing potentially
    harmful queries is minimal, with less than 1% for both datasets. Second, the majority
    of unsafe responses fall into the category of specialized advice, which includes
    responses that may offer specialized financial, medical, or legal advice, or suggest
    that dangerous activities or objects are safe.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [5](#A3.T5 "表 5 ‣ C.2 附加安全分析 ‣ 附录 C 更多数据集分析 ‣ Magpie: 从零开始通过提示对齐 LLMs 进行对齐数据合成")
    说明了 Magpie-Air 和 Magpie-Pro 中不同不安全类别的百分比，这些数据由 Llama-Guard-2 [[48](#bib.bib48)]
    标记。我们有两个主要观察结果。首先，包含潜在有害查询的数据比例非常小，两个数据集的比例均低于 1%。其次，大多数不安全的回应都属于专业建议类别，包括可能提供专业的金融、医疗或法律建议的回应，或暗示危险活动或物品是安全的。'
- en: 'Table 5: This table shows the percentage of different unsafe categories of
    Magpie-Air and Magpie-Pro tagged by Llama-Guard-2 [[48](#bib.bib48)] model.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：此表展示了 Llama-Guard-2 [[48](#bib.bib48)] 模型标记的 Magpie-Air 和 Magpie-Pro 不安全类别的百分比。
- en: '| Dataset | Safe | Violent Crimes | Non-Violent Crimes | Sex-Related Crimes
    | Child Sexual Exploitation | Specialized Advice | Privacy | Intellectual Property
    | Indiscriminate Weapons | Hate | Suicide & Self-Harm | Sexual Content | Others
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 安全 | 暴力犯罪 | 非暴力犯罪 | 性别相关犯罪 | 儿童性剥削 | 专业建议 | 隐私 | 知识产权 | 随意武器 | 仇恨 |
    自杀和自残 | 性内容 | 其他 |'
- en: '| Magpie-Air | 99.128% | 0.001% | 0.073% | 0.003% | 0.000% | 0.636% | 0.022%
    | 0.026% | 0.038% | 0.001% | 0.002% | 0.009% | 0.062% |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Air | 99.128% | 0.001% | 0.073% | 0.003% | 0.000% | 0.636% | 0.022%
    | 0.026% | 0.038% | 0.001% | 0.002% | 0.009% | 0.062% |'
- en: '| Magpie-Pro | 99.347% | 0.001% | 0.049% | 0.002% | 0.000% | 0.446% | 0.015%
    | 0.074% | 0.014% | 0.001% | 0.004% | 0.011% | 0.036% |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Pro | 99.347% | 0.001% | 0.049% | 0.002% | 0.000% | 0.446% | 0.015%
    | 0.074% | 0.014% | 0.001% | 0.004% | 0.011% | 0.036% |'
- en: C.3 Ablation Analysis on Generation Configurations
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 关于生成配置的消融分析
- en: '![Refer to caption](img/0acffa44ea16834046f44da7ec16d288.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0acffa44ea16834046f44da7ec16d288.png)'
- en: (a) Average Quality Scores of Magpie-Air
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Magpie-Air 的平均质量评分
- en: '![Refer to caption](img/ae50ce8f2f5ae06392096cbdf3141f6f.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ae50ce8f2f5ae06392096cbdf3141f6f.png)'
- en: (b) Average Difficulty Scores of Magpie-Air
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Magpie-Air 的平均难度评分
- en: '![Refer to caption](img/381f5949e41a0acaa896883bdadcfc05.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/381f5949e41a0acaa896883bdadcfc05.png)'
- en: (c) Average Minimum Neighbor Distances of Magpie-Air
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Magpie-Air 的平均最小邻域距离
- en: 'Figure 9: This figure illustrates the impact of varying decoding parameters
    on the quality, difficulty, and diversity of generated instructions. We observe
    that while higher temperature and top-p values may decrease the overall quality,
    they tend to increase both the difficulty and diversity of the instructions.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：此图展示了不同解码参数对生成指令的质量、难度和多样性的影响。我们观察到，虽然较高的温度和 top-p 值可能会降低整体质量，但它们通常会增加指令的难度和多样性。
- en: 'Ablation Analysis on Decoding Parameters. We conduct an ablation analysis on
    the decoding parameters used in generating instruction with Magpie. Specifically,
    we use three different temperatures (i.e., $1$) during Step 1 of Magpie. We use
    three metrics, Average Quality Score, Average Difficulty Score and Average Minimum
    Neighbor Distance to characterize the quality, difficulty, and diversity of instructions
    using different decoding parameters. The Average Quality Score is calculated by
    averaging the ratings of all data within a specific temperature-top-p pair, on
    a scale from 1 (‘very poor’) to 5 (‘excellent’). Similarly, the Average Difficulty
    Score is rated on a scale from 1 (‘very easy’) to 5 (‘very hard’). The Average
    Minimum Neighbor Distance is calculated by averaging the minimum neighbor distances,
    as defined in Section [3](#S3 "3 Dataset Analysis ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing"), for all data generated
    using the same decoding parameters.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 解码参数的消融分析。我们对使用 Magpie 生成指令的解码参数进行了消融分析。具体而言，我们在 Magpie 的第 1 步中使用了三种不同的温度（即，$1$）。我们使用三个指标，即平均质量评分、平均难度评分和平均最小邻居距离，来表征使用不同解码参数的指令的质量、难度和多样性。平均质量评分是通过对特定温度-top-p
    配对内所有数据的评分取平均值计算得出的，评分范围从 1（‘非常差’）到 5（‘优秀’）。类似地，平均难度评分的评分范围从 1（‘非常简单’）到 5（‘非常困难’）。平均最小邻居距离是通过对所有使用相同解码参数生成的数据的最小邻居距离进行平均计算得出的，最小邻居距离的定义见第
    [3](#S3 "3 数据集分析 ‣ Magpie：通过提示对齐 LLM 从头开始对齐数据合成") 节。
- en: '![Refer to caption](img/d9af376537ffbc156e31ab41bf4f1732.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d9af376537ffbc156e31ab41bf4f1732.png)'
- en: 'Figure 10: This figure compares the input quality and difficulty with and without
    system prompts.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：此图比较了使用和不使用系统提示的输入质量和难度。
- en: 'The findings are summarized in Figure [9](#A3.F9 "Figure 9 ‣ C.3 Ablation Analysis
    on Generation Configurations ‣ Appendix C More Dataset Analysis ‣ Magpie: Alignment
    Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"). We observe
    that higher temperature and top-p values may slightly decrease the overall quality
    of instructions, while simultaneously increasing the difficulty and remarkably
    enhancing the diversity of the instructions generated. The selection of these
    hyper-parameters should be tailored to the user’s specific requirements, balancing
    the trade-offs between quality, difficulty, and diversity.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 结果总结见图 [9](#A3.F9 "图 9 ‣ C.3 消融分析：生成配置 ‣ 附录 C 更多数据集分析 ‣ Magpie：通过提示对齐 LLM 从头开始对齐数据合成")。我们观察到较高的温度和
    top-p 值可能会略微降低整体指令质量，同时增加难度并显著提高生成指令的多样性。这些超参数的选择应根据用户的具体需求进行调整，以平衡质量、难度和多样性之间的权衡。
- en: 'Ablation Analysis on the System Prompt. Figure [10](#A3.F10 "Figure 10 ‣ C.3
    Ablation Analysis on Generation Configurations ‣ Appendix C More Dataset Analysis
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing") compares the use of system prompt compared with not using it in Step
    1 of Magpie. Since the Llama-3 model does not have an official system prompt,
    we use the default system prompt from Vicuna [[10](#bib.bib10)]: A chat between
    a curious user and an artificial intelligence assistant. The assistant gives helpful,
    detailed, and polite answers to the user’s questions. We observe that using a
    system prompt generally results in a decrease in the overall quality of instructions,
    and the instructions are easier. Consequently, we recommend not appending system
    prompts in default settings.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示的消融分析。图 [10](#A3.F10 "图 10 ‣ C.3 消融分析：生成配置 ‣ 附录 C 更多数据集分析 ‣ Magpie：通过提示对齐
    LLM 从头开始对齐数据合成") 比较了在 Magpie 的第 1 步中使用系统提示与不使用系统提示的效果。由于 Llama-3 模型没有官方系统提示，我们使用
    Vicuna 的默认系统提示 [[10](#bib.bib10)]：一个好奇的用户与人工智能助手之间的对话。助手对用户的问题提供有帮助、详细和礼貌的回答。我们观察到，使用系统提示通常会导致整体指令质量下降，并且指令变得更容易。因此，我们建议在默认设置中不附加系统提示。
- en: Appendix D Detailed Experimental Setups
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 详细实验设置
- en: D.1 Experimental Setups for Generating Magpie-Air and Magpie-Pro
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 生成 Magpie-Air 和 Magpie-Pro 的实验设置
- en: 'As detailed in Appendix [C.3](#A3.SS3 "C.3 Ablation Analysis on Generation
    Configurations ‣ Appendix C More Dataset Analysis ‣ Magpie: Alignment Data Synthesis
    from Scratch by Prompting Aligned LLMs with Nothing"), varying decoding parameters
    in Step 1 can significantly influence the quality, difficulty, and diversity of
    the generated instructions. To optimize the trade-offs among these attributes,
    we employ diverse decoding parameters for the generation of Magpie-Air and Magpie-Pro.
    Table [6](#A4.T6 "Table 6 ‣ D.1 Experimental Setups for Generating Magpie-Air
    and Magpie-Pro ‣ Appendix D Detailed Experimental Setups ‣ Magpie: Alignment Data
    Synthesis from Scratch by Prompting Aligned LLMs with Nothing") presents the configurations
    of Magpie-Air and Magpie-Pro, showcasing how diverse decoding parameters shape
    each dataset.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '如附录 [C.3](#A3.SS3 "C.3 生成配置的消融分析 ‣ 附录 C 更多数据集分析 ‣ Magpie: 从零开始通过提示对齐 LLMs 的对齐数据合成")
    中详细说明，在步骤1中变化的解码参数可以显著影响生成指令的质量、难度和多样性。为了优化这些属性之间的权衡，我们采用了多样的解码参数来生成 Magpie-Air
    和 Magpie-Pro。表 [6](#A4.T6 "表 6 ‣ D.1 生成 Magpie-Air 和 Magpie-Pro 的实验设置 ‣ 附录 D 详细实验设置
    ‣ Magpie: 从零开始通过提示对齐 LLMs 的对齐数据合成") 展示了 Magpie-Air 和 Magpie-Pro 的配置，展示了多样的解码参数如何塑造每个数据集。'
- en: We employ greedy decoding to generate responses in Step 2 for Magpie-Air and
    Magpie-Pro. The intuition is that the word with the highest probability is more
    likely to originate from the model’s training dataset.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用贪心解码生成 Magpie-Air 和 Magpie-Pro 的回应。直觉是，概率最高的词更有可能来自模型的训练数据集。
- en: 'Table 6: This table demonstrates the configurations of generating instructions
    of Magpie-Air and Magpie-Pro datasets with varying decoding parameters.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：此表演示了使用不同解码参数生成 Magpie-Air 和 Magpie-Pro 数据集的配置。
- en: '| Dataset | Decoding Parameters | Total #Convs |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 解码参数 | 总 #Convs |'
- en: '| Temperature | Top-p | #Convs |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | Top-p | #Convs |'
- en: '| Magpie-Air | 1.0 | 1.00 | 300K | 3M |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Air | 1.0 | 1.00 | 300K | 3M |'
- en: '| 1.0 | 0.995 | 300K |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 0.995 | 300K |'
- en: '| 1.0 | 0.990 | 300K |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | 0.990 | 300K |'
- en: '| 1.1 | 1.00 | 300K |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 1.1 | 1.00 | 300K |'
- en: '| 1.1 | 0.995 | 300K |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 1.1 | 0.995 | 300K |'
- en: '| 1.1 | 0.990 | 300K |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 1.1 | 0.990 | 300K |'
- en: '| 1.2 | 1.00 | 300K |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 | 1.00 | 300K |'
- en: '| 1.2 | 0.995 | 300K |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 | 0.995 | 300K |'
- en: '| 1.2 | 0.990 | 300K |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 | 0.990 | 300K |'
- en: '| 1.25 | 1.00 | 100K |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 1.25 | 1.00 | 100K |'
- en: '| 1.25 | 0.995 | 100K |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 1.25 | 0.995 | 100K |'
- en: '| 1.25 | 0.990 | 100K |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 1.25 | 0.990 | 100K |'
- en: '| Magpie-Pro | 1.0 | 1.00 | 300K | 1M |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Pro | 1.0 | 1.00 | 300K | 1M |'
- en: '| 1.1 | 0.995 | 300K |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 1.1 | 0.995 | 300K |'
- en: '| 1.2 | 0.995 | 300K |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 | 0.995 | 300K |'
- en: '| 1.25 | 0.990 | 100K |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 1.25 | 0.990 | 100K |'
- en: D.2 Experimental Setups for Instruction Tuning
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 指令调优的实验设置
- en: 'Supervised Fine-Tuning Hyper-parameters. Table [7](#A4.T7 "Table 7 ‣ D.2 Experimental
    Setups for Instruction Tuning ‣ Appendix D Detailed Experimental Setups ‣ Magpie:
    Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing")
    demonstrates the detailed supervised fine-tuning hyper-parameters. These experiments
    were conducted using Axolotl⁵⁵5[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '监督微调超参数。表 [7](#A4.T7 "表 7 ‣ D.2 指令调优的实验设置 ‣ 附录 D 详细实验设置 ‣ Magpie: 从零开始通过提示对齐
    LLMs 的对齐数据合成") 演示了详细的监督微调超参数。这些实验使用了 Axolotl⁵⁵5[https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
    进行。'
- en: 'Table 7: This table shows the hyper-parameters for supervised fine-tuning.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：此表展示了监督微调的超参数。
- en: '| Hyper-parameter | Value |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| Learning Rate | $2\times 10^{-5}$ |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | $2\times 10^{-5}$ |'
- en: '| Number of Epochs | $2$ |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 纪元数 | $2$ |'
- en: '| Number of Devices | $4$ |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 设备数量 | $4$ |'
- en: '| Per-device Batch Size | $1$ |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 每设备批量大小 | $1$ |'
- en: '| Gradient Accumulation Steps | $8$ |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 梯度累积步骤 | $8$ |'
- en: '| Effective Batch Size | $32$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 有效批量大小 | $32$ |'
- en: '| Optimizer | Adamw with $\beta s=(0.9,0.999)$ |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | Adamw，$\beta s=(0.9,0.999)$ |'
- en: '| Learning Rate Scheduler | cosine |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度器 | cosine |'
- en: '| Warmup Steps | $100$ |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 预热步骤 | $100$ |'
- en: '| Max Sequence Length | $8192$ |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 最大序列长度 | $8192$ |'
- en: Decoding parameters for evaluation benchmarks. For Arena-Hard [[32](#bib.bib32)]
    and WildBench [[34](#bib.bib34)], we follow its default setting and use greedy
    decoding for all settings. For AlpacaEval 2 [[33](#bib.bib33)] which allows the
    model provider to specify decoding parameters, we also employ greedy decoding
    in all experiments with a slightly increased repetition penalty ($RP=1.2$) to
    mitigate the potential repetitive outputs during the generation.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 解码参数用于评估基准。对于 Arena-Hard [[32](#bib.bib32)] 和 WildBench [[34](#bib.bib34)]，我们遵循其默认设置，所有设置中都使用贪婪解码。对于允许模型提供者指定解码参数的
    AlpacaEval 2 [[33](#bib.bib33)]，我们在所有实验中也使用贪婪解码，并略微增加了重复惩罚（$RP=1.2$），以减少生成过程中的潜在重复输出。
- en: Appendix E Additional Experimental Results
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 附加实验结果
- en: E.1 Performance of Magpie-MT
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 Magpie-MT 性能
- en: 'Table [8](#A5.T8 "Table 8 ‣ E.1 Performance of Magpie-MT ‣ Appendix E Additional
    Experimental Results ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing") compares the performance of Magpie-Air-MT and Magpie-Pro-MT
    with their respective single-turn counterparts. We observe that the multi-turn
    datasets have enhanced performance, particularly in the Arena-Hard benchmark.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [8](#A5.T8 "表 8 ‣ E.1 Magpie-MT 性能 ‣ 附录 E 附加实验结果 ‣ Magpie: 从头开始通过提示对齐 LLMs
    的对齐数据合成") 比较了 Magpie-Air-MT 和 Magpie-Pro-MT 与其各自单轮对应版本的性能。我们观察到多轮数据集在性能上有所提升，特别是在
    Arena-Hard 基准上。'
- en: 'Table 8: This table compares the performance of the multi-turn versions, Magpie-Air-MT
    and Magpie-Pro-MT, with their single-turn counterparts. All models are instruction-tuned
    on the Llama-8B base models.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：此表比较了多轮版本 Magpie-Air-MT 和 Magpie-Pro-MT 与其单轮对应版本的性能。所有模型均在 Llama-8B 基础模型上进行指令调优。
- en: '| Dataset | AlpacaEval 2 | Arena-Hard |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | AlpacaEval 2 | Arena-Hard |'
- en: '| GPT-4-Turbo (1106) | Llama-3-8B-Instruct |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo (1106) | Llama-3-8B-Instruct |  |'
- en: '| LC (%) | WR (%) | SD | LC (%) | WR (%) | SD | WR (%) |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| LC (%) | WR (%) | SD | LC (%) | WR (%) | SD | WR (%) |'
- en: '| Magpie-Air | Single-Turn | 22.66 | 23.99 | 1.24 | 49.27 | 50.80 | 1.44 |
    14.9 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Air | 单轮 | 22.66 | 23.99 | 1.24 | 49.27 | 50.80 | 1.44 | 14.9 |'
- en: '| MT | 22.98 | 24.02 | 1.27 | 49.63 | 51.42 | 1.40 | 15.5 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| MT | 22.98 | 24.02 | 1.27 | 49.63 | 51.42 | 1.40 | 15.5 |'
- en: '| Magpie-Pro | Single-Turn | 25.15 | 26.50 | 1.30 | 50.52 | 52.98 | 1.43 |
    18.9 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Pro | 单轮 | 25.15 | 26.50 | 1.30 | 50.52 | 52.98 | 1.43 | 18.9 |'
- en: '| MT | 24.21 | 25.19 | 1.28 | 52.92 | 54.80 | 1.41 | 20.4 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| MT | 24.21 | 25.19 | 1.28 | 52.92 | 54.80 | 1.41 | 20.4 |'
- en: E.2 Ablation Analysis on Filter Designs
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 过滤器设计的消融分析
- en: 'We conduct an ablation analysis on various filter designs within Magpie-Pro
    to assess their impact on the performance of supervised fine-tuned models. The
    results are presented in Table [9](#A5.T9 "Table 9 ‣ E.2 Ablation Analysis on
    Filter Designs ‣ Appendix E Additional Experimental Results ‣ Magpie: Alignment
    Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"). We observe
    that different filtering strategies yield optimal performance on different benchmarks,
    and no single filter consistently achieves the best performance across all benchmarks.
    Therefore, determining how to select instructional data to enhance the performance
    in supervised fine-tuning is an interesting topic for future research.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对 Magpie-Pro 中的各种过滤器设计进行消融分析，以评估其对监督微调模型性能的影响。结果见表 [9](#A5.T9 "表 9 ‣ E.2
    过滤器设计的消融分析 ‣ 附录 E 附加实验结果 ‣ Magpie: 从头开始通过提示对齐 LLMs 的对齐数据合成")。我们观察到，不同的过滤策略在不同基准上产生最佳性能，并且没有单一的过滤器在所有基准上始终表现最佳。因此，如何选择指令数据以提升监督微调中的性能是未来研究的一个有趣话题。'
- en: 'Table 9: This table compares the performance of different filter designs within
    Magpie-Pro. All models are instruction-tuned on the Llama-8B base models.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：此表比较了 Magpie-Pro 中不同过滤器设计的性能。所有模型均在 Llama-8B 基础模型上进行指令调优。
- en: '| Dataset and Filter | AlpacaEval 2 | Arena-Hard |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 数据集和过滤器 | AlpacaEval 2 | Arena-Hard |'
- en: '| GPT-4-Turbo (1106) | Llama-3-8B-Instruct |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo (1106) | Llama-3-8B-Instruct |  |'
- en: '| LC (%) | WR (%) | SD | LC (%) | WR (%) | SD | WR (%) |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| LC (%) | WR (%) | SD | LC (%) | WR (%) | SD | WR (%) |'
- en: '| Magpie-Pro | Filter | 25.08 | 29.47 | 1.35 | 52.12 | 53.43 | 1.44 | 18.9
    |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Pro | 过滤器 | 25.08 | 29.47 | 1.35 | 52.12 | 53.43 | 1.44 | 18.9 |'
- en: '| Filter 2 | 25.15 | 26.50 | 1.30 | 50.52 | 52.98 | 1.43 | 18.9 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 过滤器 2 | 25.15 | 26.50 | 1.30 | 50.52 | 52.98 | 1.43 | 18.9 |'
- en: '| Filter 3 | 23.90 | 25.21 | 1.25 | 51.45 | 53.64 | 1.41 | 18.3 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 过滤器 3 | 23.90 | 25.21 | 1.25 | 51.45 | 53.64 | 1.41 | 18.3 |'
- en: '| Filter 4 | 24.20 | 25.33 | 1.27 | 52.43 | 54.34 | 1.43 | 17.9 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 过滤器 4 | 24.20 | 25.33 | 1.27 | 52.43 | 54.34 | 1.43 | 17.9 |'
- en: '| Filter 5 | 24.85 | 25.12 | 1.26 | 52.12 | 53.43 | 1.44 | 18.4 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 过滤器 5 | 24.85 | 25.12 | 1.26 | 52.12 | 53.43 | 1.44 | 18.4 |'
- en: '| Filter 6 | 23.20 | 28.43 | 1.26 | 51.34 | 57.29 | 1.41 | 17.9 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Filter 6 | 23.20 | 28.43 | 1.26 | 51.34 | 57.29 | 1.41 | 17.9 |'
- en: E.3 Performance of Magpie on More Benchmarks
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 Magpie 在更多基准上的性能
- en: 'We report the performance of models fine-tuned using Magpie-Air and Magpie-Pro,
    evaluated across a range of tasks featured on the Huggingface Open LLM Leaderboard
    [[6](#bib.bib6)] in Table [10](#A5.T10 "Table 10 ‣ E.3 Performance of Magpie on
    More Benchmarks ‣ Appendix E Additional Experimental Results ‣ Magpie: Alignment
    Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"). The tasks
    includes MMLU [[22](#bib.bib22)], ARC [[11](#bib.bib11)], HellaSwag [[63](#bib.bib63)],
    TruthfulQA [[36](#bib.bib36)], Winogard [[30](#bib.bib30)], and GSM8K [[12](#bib.bib12)].
    We also perform experiments on MMLU-Redux [[21](#bib.bib21)] with zero-shot prompting.
    We use the default greedy decoding with $RP=1$ for all setups. Our experimental
    results demonstrate that models fine-tuned with Magpie-Air and Magpie-Pro achieve
    comparable performance to the official instruct model and other baselines despite
    the alignment tax.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '我们报告了使用 Magpie-Air 和 Magpie-Pro 微调的模型的性能，这些模型在 Huggingface Open LLM Leaderboard
    [[6](#bib.bib6)] 中的任务上进行了评估，如表 [10](#A5.T10 "表 10 ‣ E.3 Magpie 在更多基准上的性能 ‣ 附录
    E 额外实验结果 ‣ Magpie: 从头开始通过提示对齐 LLM 的对齐数据合成") 所示。任务包括 MMLU [[22](#bib.bib22)]、ARC
    [[11](#bib.bib11)]、HellaSwag [[63](#bib.bib63)]、TruthfulQA [[36](#bib.bib36)]、Winogard
    [[30](#bib.bib30)] 和 GSM8K [[12](#bib.bib12)]。我们还对 MMLU-Redux [[21](#bib.bib21)]
    进行了零-shot 提示实验。我们对所有设置使用默认的贪婪解码，$RP=1$。我们的实验结果表明，使用 Magpie-Air 和 Magpie-Pro 微调的模型在对齐税的影响下仍能达到与官方指令模型及其他基线相当的性能。'
- en: 'Table 10: This table compares the performance of models instruction-tuned on
    Magpie-Air and Magpie-Pro against baselines and official instruct model across
    various downstream benchmarks. All models are instruction-tuned on the Llama-8B
    base models.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：该表比较了在 Magpie-Air 和 Magpie-Pro 上进行指令调优的模型与基线模型及官方指令模型在各种下游基准上的性能。所有模型均在
    Llama-8B 基础模型上进行指令调优。
- en: '| Alignment Setup | MMLU (5) | ARC (25) | HellaSwag (10) | TruthfulQA (0) |
    Winograd (5) | GSM8K (5) | MMLU-Redux (0) |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 对齐设置 | MMLU (5) | ARC (25) | HellaSwag (10) | TruthfulQA (0) | Winograd (5)
    | GSM8K (5) | MMLU-Redux (0) |'
- en: '| ShareGPT | 66.03 | 58.45 | 81.50 | 52.34 | 74.03 | 48.67 | 50.68 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| ShareGPT | 66.03 | 58.45 | 81.50 | 52.34 | 74.03 | 48.67 | 50.68 |'
- en: '| Evol Instruct | 65.62 | 60.75 | 82.70 | 52.87 | 76.16 | 42.91 | 52.73 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Evol Instruct | 65.62 | 60.75 | 82.70 | 52.87 | 76.16 | 42.91 | 52.73 |'
- en: '| OpenHermes | 65.42 | 62.29 | 82.15 | 50.85 | 75.61 | 47.16 | 46.07 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| OpenHermes | 65.42 | 62.29 | 82.15 | 50.85 | 75.61 | 47.16 | 46.07 |'
- en: '| Tulu V2 Mix | 66.34 | 59.22 | 82.80 | 47.99 | 76.16 | 58.07 | 46.97 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Tulu V2 Mix | 66.34 | 59.22 | 82.80 | 47.99 | 76.16 | 58.07 | 46.97 |'
- en: '| WildChat | 65.95 | 59.22 | 81.39 | 53.18 | 75.30 | 48.75 | 52.59 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| WildChat | 65.95 | 59.22 | 81.39 | 53.18 | 75.30 | 48.75 | 52.59 |'
- en: '| UltraChat | 65.23 | 62.12 | 81.68 | 52.76 | 75.53 | 50.57 | 50.75 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| UltraChat | 65.23 | 62.12 | 81.68 | 52.76 | 75.53 | 50.57 | 50.75 |'
- en: '| Magpie-Air-300K-Filtered | 64.45 | 61.01 | 79.90 | 53.48 | 72.38 | 52.24
    | 52.34 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Air-300K-Filtered | 64.45 | 61.01 | 79.90 | 53.48 | 72.38 | 52.24
    | 52.34 |'
- en: '| Magpie-Pro-100K-Filtered | 65.31 | 60.32 | 81.18 | 51.11 | 73.32 | 50.42
    | 52.56 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Pro-100K-Filtered | 65.31 | 60.32 | 81.18 | 51.11 | 73.32 | 50.42
    | 52.56 |'
- en: '| Magpie-Pro-200K-Filtered | 64.98 | 61.26 | 80.71 | 51.82 | 73.16 | 47.76
    | 51.44 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Pro-200K-Filtered | 64.98 | 61.26 | 80.71 | 51.82 | 73.16 | 47.76
    | 51.44 |'
- en: '| Magpie-Pro-300K-Filtered | 64.25 | 60.41 | 80.52 | 52.46 | 73.32 | 47.92
    | 52.16 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Magpie-Pro-300K-Filtered | 64.25 | 60.41 | 80.52 | 52.46 | 73.32 | 47.92
    | 52.16 |'
- en: '| Llama-3-8B-Instruct | 67.82 | 61.52 | 78.67 | 52.47 | 72.14 | 71.72 | 58.60
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B-Instruct | 67.82 | 61.52 | 78.67 | 52.47 | 72.14 | 71.72 | 58.60
    |'
- en: Appendix F Prompt Templates
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 提示模板
- en: F.1 Prompt Templates for Magpie Extension
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 Magpie 扩展提示模板
- en: 'This section presents the prompt template used to generate Magpie-MT and control
    instruction tasks, as detailed in Figure [11](#A6.F11 "Figure 11 ‣ F.1 Prompt
    Templates for Magpie Extension ‣ Appendix F Prompt Templates ‣ Magpie: Alignment
    Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing") and Figure
    [12](#A6.F12 "Figure 12 ‣ F.1 Prompt Templates for Magpie Extension ‣ Appendix
    F Prompt Templates ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting
    Aligned LLMs with Nothing"), respectively.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '本节展示了用于生成 Magpie-MT 和控制指令任务的提示模板，如图 [11](#A6.F11 "图 11 ‣ F.1 Magpie 扩展提示模板
    ‣ 附录 F 提示模板 ‣ Magpie: 从头开始通过提示对齐 LLM 的对齐数据合成") 和图 [12](#A6.F12 "图 12 ‣ F.1 Magpie
    扩展提示模板 ‣ 附录 F 提示模板 ‣ Magpie: 从头开始通过提示对齐 LLM 的对齐数据合成") 所示。'
- en: Prompt for generating Magpie-MT <|begin_of_text|><|start_header_id|>system<|end_header_id|>
    You  are  a  helpful  Al  assistant.  The  user  will  engage  in  a  multi-round  conversation  with  you,  asking  initial  questions  and  following  up  with  additional  related  questions.  Your  goal  is  to  provide  thorough,  relevant  and  insightful  responses  to  help  the  user  with  their  queries.<|eot_id|><|start_header_id|>user<|end_header_id|>
    {instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|> {response}<|eot_id|><|start_header_id|>user<|end_header_id|>
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 生成 Magpie-MT 的提示 <|begin_of_text|><|start_header_id|>system<|end_header_id|>
    你是一个有用的 AI 助手。用户将与你进行多轮对话，提出初步问题，并跟进额外相关问题。你的目标是提供全面、相关和有见地的回答，以帮助用户解决他们的疑问。<|eot_id|><|start_header_id|>user<|end_header_id|>
    {instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|> {response}<|eot_id|><|start_header_id|>user<|end_header_id|>
- en: 'Figure 11: Prompt for generating Magpie-MT. The placeholder {instruction} and
    {response} are from the first turn.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：生成 Magpie-MT 的提示。占位符 {instruction} 和 {response} 来自第一次对话。
- en: Prompt for controlling instruction tasks<|begin_of_text|><|start_header_id|>system<|end_header_id|>
    You  are  an  AI  assistant  designed  to  provide  helpful,  step-by-step  guidance  on  solving  math  problems.  The  user  will  ask  you  a  wide  range  of  complex  mathematical  questions.  Your  purpose  is  to  assist  users  in  understanding  mathematical  concepts,  working  through  equations,  and  arriving  at  the  correct  solutions.<|eot_id|><|start_header_id|>user<|end_header_id|>
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 控制指令任务的提示<|begin_of_text|><|start_header_id|>system<|end_header_id|> 你是一个 AI
    助手，设计用于提供解决数学问题的有用、逐步指导。用户将问你一系列复杂的数学问题。你的目的是帮助用户理解数学概念、解决方程，并得出正确的答案。<|eot_id|><|start_header_id|>user<|end_header_id|>
- en: 'Figure 12: Prompt for controlling instruction tasks. In this example, we control
    LLMs to generate instructions related to math.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：控制指令任务的提示。在此示例中，我们控制 LLMs 生成与数学相关的指令。
- en: F.2 Prompt Templates for Evaluation
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 评估提示模板
- en: 'Here, we present the prompt template employed to generate task categories,
    quality, and difficulty, as detailed in Figure [13](#A6.F13 "Figure 13 ‣ F.2 Prompt
    Templates for Evaluation ‣ Appendix F Prompt Templates ‣ Magpie: Alignment Data
    Synthesis from Scratch by Prompting Aligned LLMs with Nothing"), Figure [14](#A6.F14
    "Figure 14 ‣ F.2 Prompt Templates for Evaluation ‣ Appendix F Prompt Templates
    ‣ Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with
    Nothing"), and Figure [15](#A6.F15 "Figure 15 ‣ F.2 Prompt Templates for Evaluation
    ‣ Appendix F Prompt Templates ‣ Magpie: Alignment Data Synthesis from Scratch
    by Prompting Aligned LLMs with Nothing"), respectively. The placeholder input
    represents the instructions to be evaluated.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们展示了用于生成任务类别、质量和难度的提示模板，如图 [13](#A6.F13 "图 13 ‣ F.2 评估提示模板 ‣ 附录 F 提示模板
    ‣ Magpie: 从零开始通过提示对齐 LLMs 进行对齐数据合成")、图 [14](#A6.F14 "图 14 ‣ F.2 评估提示模板 ‣ 附录 F
    提示模板 ‣ Magpie: 从零开始通过提示对齐 LLMs 进行对齐数据合成") 和图 [15](#A6.F15 "图 15 ‣ F.2 评估提示模板 ‣
    附录 F 提示模板 ‣ Magpie: 从零开始通过提示对齐 LLMs 进行对齐数据合成")，占位符输入表示待评估的指令。'
- en: 'Prompt for generating task categories#  Instruction Please  label  the  task  tags  for  the  user  query.
    ##  User  Query “‘{input}“‘ ##  Tagging  the  user  input Please  label  the  task  tags  for  the  user  query.  You  will  need  to  analyze  the  user  query  and  select  the  most  relevant  task  tag  from  the  list  below.
    all_task_tags  =  [ "Information  seeking",  #  Users  ask  for  specific  information  or  facts  about  various  topics.
    "Reasoning",  #  Queries  require  logical  thinking,  problem-solving,  or  processing  of  complex  ideas.
    "Planning",  #  Users  need  assistance  in  creating  plans  or  strategies  for  activities  and  projects.
    "Editing",  #  Involves  editing,  rephrasing,  proofreading,  or  other  tasks  related  to  the  composition  of  general  written  content.
    "Coding  &  Debugging",  #  Users  seek  help  with  writing,  reviewing,  or  fixing  code  in  programming.
    "Math",  #  Queries  related  to  mathematical  concepts,  problems,  and  calculations.
    "Role  playing",  #  Users  engage  in  scenarios  requiring  ChatGPT  to  adopt  a  character  or  persona.
    "Data  analysis",  #  Requests  involve  interpreting  data,  statistics,  or  performing  analytical  tasks.
    "Creative  writing",  #  Users  seek  assistance  with  crafting  stories,  poems,  or  other  creative  texts.
    "Advice  seeking",  #  Users  ask  for  recommendations  or  guidance  on  various  personal  or  professional  issues.
    "Brainstorming",  #  Involves  generating  ideas,  creative  thinking,  or  exploring  possibilities.
    "Others"  #  Any  queries  that  do  not  fit  into  the  above  categories  or  are  of  a  miscellaneous  nature.
    ] ##  Output  Format: Note  that  you  can  only  select  a  single  primary  tag.  Other  applicable  tags  can  be  added  to  the  list  of  other  tags.
    Now,  please  output  your  tags  below  in  a  json  format  by  filling  in  the  placeholders  in  <…>:
    “‘ {{ "primary_tag":  "", "other_tags":  ["",  "",  …  ] }} “‘'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '生成任务类别的提示 # 说明 请为用户查询标记任务标签。 ## 用户查询 “‘{input}“‘ ## 标记用户输入 请为用户查询标记任务标签。你需要分析用户查询，并从下面的列表中选择最相关的任务标签。
    all_task_tags = [ "信息查找",  # 用户询问有关各种主题的具体信息或事实。 "推理",  # 查询需要逻辑思维、问题解决或复杂思想的处理。
    "规划",  # 用户需要帮助制定活动和项目的计划或策略。 "编辑",  # 涉及编辑、改述、校对或其他与一般书面内容编写相关的任务。 "编码与调试",  #
    用户寻求编写、审阅或修复编程代码的帮助。 "数学",  # 与数学概念、问题和计算相关的查询。 "角色扮演",  # 用户参与需要 ChatGPT 采用角色或人格的情景。
    "数据分析",  # 请求涉及数据、统计信息的解释或执行分析任务。 "创意写作",  # 用户寻求帮助撰写故事、诗歌或其他创意文本。 "寻求建议",  #
    用户询问有关个人或职业问题的推荐或指导。 "头脑风暴",  # 涉及生成想法、创意思维或探索可能性。 "其他",  # 任何不符合上述类别或属于杂项的查询。
    ] ## 输出格式：请注意，你只能选择一个主要标签。其他适用的标签可以添加到其他标签列表中。现在，请将你的标签以 json 格式输出，填写 <…> 中的占位符：
    “‘ {{ "primary_tag":  "", "other_tags":  ["",  "",  …  ] }} “‘'
- en: 'Figure 13: Prompt for generating task categories'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：生成任务类别的提示
- en: 'Prompt for generating quality of instructions#  Instruction You  need  to  rate  the  quality  of  the  user  query  based  on  its  clarity,  specificity,  and  coherence.
    The  rating  scale  is  as  follows: -  very  poor:  The  query  is  unclear,  vague,  or  incoherent.  It  lacks  essential  information  and  context.
    -  poor:  The  query  is  somewhat  unclear  or  lacks  important  details.  It  requires  significant  clarification.
    -  average:  The  query  is  moderately  clear  and  specific.  It  may  require  some  additional  information  for  a  complete  understanding.
    -  good:  The  query  is  clear,  specific,  and  mostly  well-formed.  It  provides  sufficient  context  for  understanding  the  user’s  intent.
    -  excellent:  The  query  is  very  clear,  specific,  and  well-articulated.  It  contains  all  the  necessary  information  and  context  for  providing  a  comprehensive  response.
    ##  User  Query “‘{input}“‘ ##  Output  Format Given  the  user  query,  you  first  need  to  give  an  assessment,  highlighting  the  strengths  and/or  weaknesses  of  the  user  query.  Then,  you  need  to  output  a  rating  from  very  poor  to  excellent  by  filling  in  the  placeholders  in  […]:
    “‘ {{ "explanation":  "[…]", "input_quality":  "[very  poor/poor/average/good/excellent]"
    }} “‘ ”’'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '指令生成质量的提示#  指令 你 需要 评估 用户查询的质量，依据其清晰度、具体性和连贯性。评分标准如下： -  很差：查询不清楚、模糊或不连贯，缺乏必要的信息和背景。
    -  较差：查询有些不清楚或缺乏重要细节，需要显著的澄清。 -  一般：查询较为清晰和具体，可能需要一些额外的信息才能完全理解。 -  良好：查询清晰、具体且大部分格式良好，提供了足够的背景以理解用户意图。
    -  优秀：查询非常清晰、具体且表达良好，包含了所有必要的信息和背景，以便提供全面的响应。 ##  用户查询 “‘{input}“‘ ##  输出格式 给定用户查询，你首先需要进行评估，突出用户查询的优点和/或缺点。然后，你需要通过填写[……]中的占位符来输出从“很差”到“优秀”的评分：
    “‘ {{ "explanation":  "[……]", "input_quality":  "[很差/较差/一般/良好/优秀]" }} “‘ ”’'
- en: 'Figure 14: Prompt for generating quality of instructions'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：生成指令质量的提示
- en: 'Prompt for generating difficulty of instructions#  Instruction You  first  need  to  identify  the  given  user  intent  and  then  label  the  difficulty  level  of  the  user  query  based  on  the  content  of  the  user  query.
    ##  User  Query “‘{input}“‘ ##  Output  Format Given  the  user  query,  in  your  output,  you  first  need  to  identify  the  user  intent  and  the  knowledge  needed  to  solve  the  task  in  the  user  query.  Then,  rate  the  difficulty  level  of  the  user  query  as  ‘very  easy‘,  ‘easy‘,  ‘medium‘,  ‘hard‘,  or  ‘very  hard‘.
    Now,  please  output  the  user  intent  and  difficulty  level  below  in  a  json  format  by  filling  in  the  placeholders  in  […]:
    “‘ {{ "intent":  "The  user  wants  to  [….]", "knowledge":  "To  solve  this  problem,  the  models  need  to  know  [….]",
    "difficulty":  "[very  easy/easy/medium/hard/very  hard]" }} “‘'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '指令生成难度的提示#  指令 你首先需要识别给定的用户意图，然后根据用户查询的内容标记用户查询的难度级别。 ##  用户查询 “‘{input}“‘
    ##  输出格式 给定用户查询，在你的输出中，你首先需要识别用户意图和解决用户查询任务所需的知识。然后，将用户查询的难度级别评定为“非常简单”、“简单”、“中等”、“困难”或“非常困难”。现在，请以
    JSON 格式输出用户意图和难度级别，通过填写[……]中的占位符： “‘ {{ "intent":  "用户希望[…]", "knowledge":  "解决此问题，模型需要知道[…]",
    "difficulty":  "[非常简单/简单/中等/困难/非常困难]" }} “‘'
- en: 'Figure 15: Prompt for generating difficulty of instructions'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：生成指令难度的提示
- en: Appendix G Magpie Examples
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G Magpie 示例
- en: In this section, we demonstrate several representative instructions from Magpie-Pro
    with different task categories.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了 Magpie-Pro 中不同任务类别的几个代表性指令。
- en: 'Information Seeking A few days ago, I was at a restaurant and I got a cup of
    coffee. However, when I went to take a sip, I realized it was a little too hot.
    So, I decided to let it cool down for a few minutes. As I waited, I noticed that
    coffee is actually two different colors. The part that was closest to the surface
    of the coffee is a lighter color, and the part that is deeper is a darker color.
    Have you ever observed this phenomenon before?Reasoning You have 3 switches but
    they are not labeled. Each switch corresponds to one of three light bulbs in a
    room. Each light bulb is either on or off. You can turn the switches on and off
    as many times as you want, but you can only enter the room one time to observe
    the bulbs. How can you figure out which switch corresponds to which light bulb?Planning
    You are the Founder of a Financial Planning Company. As a professional financial
    advisor, you are scheduled to meet a new client tomorrow. Specifically, what are
    you planning to do to prepare for this meeting?Editing What is the best way to
    re-write the sentence: "We call this the ‘core’ product or the ‘core’ offering"
    using proper quotation marks and avoiding the word "this"?Coding & Debugging Write
    a Python program that calculates the total cost of a customer’s order. The program
    should ask for the customer’s name, the number of items they want to purchase,
    and the price of each item. It should then calculate the total cost by multiplying
    the number of items by the price of each item and adding 8% sales tax. The program
    should display the customer’s name, the number of items, the price of each item,
    and the total cost, including sales tax.Math In the following problem, please
    use integers to solve it. A water tank has 1000 L of water. On the first day,
    1/5 of the water was drained. On the second day, 3/10 of the remaining water was
    drained. On the third day, 2/5 of the remaining water was drained. On the fourth
    day, 3/4 of the remaining water was drained. How many liters of water are left
    after the fourth day?Role Playing In this game, you will be the host, and I will
    be the contestant. You will ask me a series of questions, and I will try to answer
    them correctly. The questions will be multiple choice, and I will have a 25% chance
    of getting the correct answer if I just randomly guess. However, I am a clever
    contestant, and I will try to use logic and reasoning to increase my chances of
    getting the correct answer.Data Analysis The personnel manager at a company is
    tasked with finding the average salary of new hires. She has collected data on
    the salaries of 13 new hires. She wants to know if there is a statistical difference
    between the average salary of new hires and the national average salary. The national
    average salary is $45,000\. The sample of new hires has a mean salary of $42,800
    and a standard deviation of $4,200.Creative Writing Write a paragraph about a
    mythical creature that you created. The creature is small, no larger than a house
    cat. It has shimmering scales that reflect light, and can emit a soft, pulsing
    glow from its body. It has large, round eyes that seem to see right through you,
    but with a gentle kindness. It has a soft, melodious voice, and can communicate
    with humans through a form of telepathy.Advice Seeking How do you handle stress
    and overwhelm?Brainstorming Can you give me some ideas for a spontaneous, fun
    and memorable birthday celebration for my partner?Others What does "sdrawkcaB"
    mean?'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 信息探索 几天前，我在餐馆里点了一杯咖啡。然而，当我准备喝一口时，我意识到咖啡有点太热了。所以，我决定让它冷却几分钟。在等待的过程中，我注意到咖啡实际上有两种不同的颜色。靠近咖啡表面的是浅色部分，而更深处的是深色部分。你有没有观察到过这种现象？推理
    你有3个开关，但它们没有标签。每个开关对应房间里的一个灯泡。每个灯泡要么是开着的，要么是关着的。你可以随意开关这些开关，但只能进入房间一次来观察灯泡。你怎么能弄清楚哪个开关对应哪个灯泡？计划
    你是一个金融规划公司的创始人。作为一名专业的金融顾问，你明天要见一位新客户。具体来说，你计划如何为这次会议做准备？编辑 用适当的引号重新写句子：“We call
    this the ‘core’ product or the ‘core’ offering”，并避免使用“this”这个词？编码与调试 编写一个Python程序，计算客户订单的总费用。该程序应要求输入客户的姓名、想购买的商品数量以及每件商品的价格。然后，它应通过将商品数量与每件商品的价格相乘并加上8%的销售税来计算总费用。程序应显示客户的姓名、商品数量、每件商品的价格以及包括销售税在内的总费用。数学
    在以下问题中，请使用整数解决。一个水箱里有1000升水。第一天，排出了1/5的水。第二天，排出了剩余水的3/10。第三天，排出了剩余水的2/5。第四天，排出了剩余水的3/4。第四天结束后，剩下多少升水？角色扮演
    在这个游戏中，你将是主持人，我将是参赛者。你将问我一系列问题，我会尽力正确回答。这些问题将是选择题，如果我只是随机猜测，我有25%的概率答对。然而，我是一个聪明的参赛者，我会尝试利用逻辑和推理来增加答对的机会。数据分析
    一家公司的人事经理负责计算新员工的平均工资。她收集了13名新员工的工资数据。她想知道新员工的平均工资与全国平均工资是否有统计学差异。全国平均工资是$45,000。新员工的样本平均工资为$42,800，标准差为$4,200。创意写作
    写一段关于你创造的神话生物的文字。这个生物体积小，不大于一只家猫。它有闪闪发光的鳞片，可以反射光线，并能从身体发出柔和的脉动光辉。它有一双大而圆的眼睛，似乎能透视你，但带有温柔的善意。它有一种柔和的、优美的声音，并能通过一种形式的心灵感应与人类交流。寻求建议
    你是如何应对压力和过度疲劳的？头脑风暴 能给我一些关于为我的伴侣策划一次即兴、有趣且难忘的生日庆祝活动的想法吗？其他 “sdrawkcaB”是什么意思？
