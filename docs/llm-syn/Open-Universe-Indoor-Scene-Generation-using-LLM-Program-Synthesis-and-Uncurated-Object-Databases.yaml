- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated
    Object Databases
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放宇宙室内场景生成：使用LLM程序合成和未整理的物体数据库
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.09675](https://ar5iv.labs.arxiv.org/html/2403.09675)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.09675](https://ar5iv.labs.arxiv.org/html/2403.09675)
- en: \savesymbol
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \savesymbol
- en: zifour@default \savesymbolzifour@scaled
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: zifour@default \savesymbolzifour@scaled
- en: Rio Aguina-Kang [raguinakang@ucsd.edu](mailto:raguinakang@ucsd.edu) UC San DiegoUSA
    ,  Maxim Gumin [maxgumin@gmail.com](mailto:maxgumin@gmail.com) Brown UniversityUSA
    ,  Do Heon Han [do_heon_han@brown.edu](mailto:do_heon_han@brown.edu) Brown UniversityUSA
    ,  Stewart Morris [stewart_morris@brown.edu](mailto:stewart_morris@brown.edu)
    Brown UniversityUSA ,  Seung Jean Yoo [seung_jean_yoo@brown.edu](mailto:seung_jean_yoo@brown.edu)
    Brown UniversityUSA ,  Aditya Ganeshan [aditya_ganeshan@brown.edu](mailto:aditya_ganeshan@brown.edu)
    Brown UniversityUSA ,  R. Kenny Jones [russell˙jones@brown.edu](mailto:russell%CB%99jones@brown.edu)
    Brown UniversityUSA ,  Qiuhong Anna Wei [qiuhong_wei@brown.edu](mailto:qiuhong_wei@brown.edu)
    Brown UniversityUSA ,  Kailiang Fu [kailiang.fu@dymaxion.design](mailto:kailiang.fu@dymaxion.design)
    Dymaxion, LLCUSA  and  Daniel Ritchie [daniel_ritchie@brown.edu](mailto:daniel_ritchie@brown.edu)
    Brown UniversityUSA(2024)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Rio Aguina-Kang [raguinakang@ucsd.edu](mailto:raguinakang@ucsd.edu) UC San DiegoUSA
    ， Maxim Gumin [maxgumin@gmail.com](mailto:maxgumin@gmail.com) Brown UniversityUSA
    ， Do Heon Han [do_heon_han@brown.edu](mailto:do_heon_han@brown.edu) Brown UniversityUSA
    ， Stewart Morris [stewart_morris@brown.edu](mailto:stewart_morris@brown.edu) Brown
    UniversityUSA ， Seung Jean Yoo [seung_jean_yoo@brown.edu](mailto:seung_jean_yoo@brown.edu)
    Brown UniversityUSA ， Aditya Ganeshan [aditya_ganeshan@brown.edu](mailto:aditya_ganeshan@brown.edu)
    Brown UniversityUSA ， R. Kenny Jones [russell˙jones@brown.edu](mailto:russell%CB%99jones@brown.edu)
    Brown UniversityUSA ， Qiuhong Anna Wei [qiuhong_wei@brown.edu](mailto:qiuhong_wei@brown.edu)
    Brown UniversityUSA ， Kailiang Fu [kailiang.fu@dymaxion.design](mailto:kailiang.fu@dymaxion.design)
    Dymaxion, LLCUSA 和 Daniel Ritchie [daniel_ritchie@brown.edu](mailto:daniel_ritchie@brown.edu)
    Brown UniversityUSA(2024)
- en: Abstract.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: We present a system for generating indoor scenes in response to text prompts.
    The prompts are not limited to a fixed vocabulary of scene descriptions, and the
    objects in generated scenes are not restricted to a fixed set of object categories—we
    call this setting *open-universe* indoor scene generation. Unlike most prior work
    on indoor scene generation, our system does not require a large training dataset
    of existing 3D scenes. Instead, it leverages the world knowledge encoded in pre-trained
    large language models (LLMs) to synthesize programs in a domain-specific layout
    language that describe objects and spatial relations between them. Executing such
    a program produces a specification of a constraint satisfaction problem, which
    the system solves using a gradient-based optimization scheme to produce object
    positions and orientations. To produce object geometry, the system retrieves 3D
    meshes from a database. Unlike prior work which uses databases of category-annotated,
    mutually-aligned meshes, we develop a pipeline using vision-language models (VLMs)
    to retrieve meshes from massive databases of un-annotated, inconsistently-aligned
    meshes. Experimental evaluations show that our system outperforms generative models
    trained on 3D data for traditional, closed- universe scene generation tasks; it
    also outperforms a recent LLM-based layout generation method on open-universe
    scene generation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种生成室内场景的系统，以响应文本提示。这些提示不限于固定的场景描述词汇，生成场景中的物体也不受限于固定的物体类别——我们称这种设置为*开放宇宙*室内场景生成。与大多数以往的室内场景生成工作不同，我们的系统不需要大量现有3D场景的训练数据集。相反，它利用编码在预训练的大型语言模型（LLMs）中的世界知识来合成描述物体及其空间关系的特定领域布局语言程序。执行这样的程序会生成一个约束满足问题的规范，系统使用基于梯度的优化方案解决这个问题，以生成物体的位置和方向。为了生成物体几何形状，系统从数据库中检索3D网格。不同于以往使用类别标注、互相对齐的网格数据库的工作，我们开发了一条使用视觉语言模型（VLMs）的管道，从大规模的未标注、不一致对齐的网格数据库中检索网格。实验评估表明，我们的系统在传统的封闭宇宙场景生成任务上优于基于3D数据的生成模型；在开放宇宙场景生成上，它也优于最近的基于LLM的布局生成方法。
- en: 'indoor scene synthesis, program synthesis, layout generation, large language
    models, vision language models, foundation models^†^†copyright: none^†^†journalyear:
    2024^†^†doi: XXXXXXX.XXXXXXX^†^†ccs: Computing methodologies Computer graphics^†^†ccs:
    Computing methodologies Neural networks^†^†ccs: Computing methodologies Natural
    language generation'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 室内场景合成，程序合成，布局生成，大型语言模型，视觉语言模型，基础模型^†^†版权：无^†^†期刊年份：2024^†^†doi：XXXXXXX.XXXXXXX^†^†ccs：计算方法
    计算机图形学^†^†ccs：计算方法 神经网络^†^†ccs：计算方法 自然语言生成
- en: '| “A living room for watching TV” | “A high-end mini restaurant” | “A witch’s
    room with a cauldron’ | “A Japanese living room” |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| “看电视的客厅” | “高档迷你餐厅” | “有锅炉的女巫房间” | “日本风格的客厅” |'
- en: '| ![Refer to caption](img/73293591e997f838dce724a1c23421c5.png)  | ![Refer
    to caption](img/bae307724633e612f02cc86467afd7cc.png)  | ![Refer to caption](img/b2b395aa6179f050ed876b1d386f42a8.png)  |
    ![Refer to caption](img/3c01f4fc6c13f0b0bfb1d8ef9fd72545.png)  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/73293591e997f838dce724a1c23421c5.png) | ![参见说明](img/bae307724633e612f02cc86467afd7cc.png)
    | ![参见说明](img/b2b395aa6179f050ed876b1d386f42a8.png) | ![参见说明](img/3c01f4fc6c13f0b0bfb1d8ef9fd72545.png)
    |'
- en: '| “A living room” | “A dining room for one” | “A bedroom” | “An old-fashioned
    bedroom” |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| “客厅” | “单人用餐室” | “卧室” | “老式卧室” |'
- en: '| ![Refer to caption](img/57200b5e2cdcfafd458b3b42876bfe1f.png)  | ![Refer
    to caption](img/072f3dbaee59a3ee165dd68a7e5ad997.png)  | ![Refer to caption](img/a08d63d0cd180414ef475af9ba03cb56.png)  |
    ![Refer to caption](img/c0e1d6871e012094c06f3a20cf3a1f5a.png)  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/57200b5e2cdcfafd458b3b42876bfe1f.png) | ![参见说明](img/072f3dbaee59a3ee165dd68a7e5ad997.png)
    | ![参见说明](img/a08d63d0cd180414ef475af9ba03cb56.png) | ![参见说明](img/c0e1d6871e012094c06f3a20cf3a1f5a.png)
    |'
- en: Figure 1\. Our method generates 3D indoor scenes from open-ended text prompts.
    Generated scenes are not limited to a fixed set of room types or object categories;
    this “open-universe” capability is enabled by judicious use of pre-trained large
    language models (LLMs) and vision-language models (VLMs).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 我们的方法从开放式文本提示生成 3D 室内场景。生成的场景不限于固定的房间类型或物体类别；这种“开放宇宙”能力是通过明智地使用预训练的大型语言模型（LLMs）和视觉语言模型（VLMs）实现的。
- en: 1\. Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'Many people spend a significant portion of their lives indoors: in their homes,
    workplaces, social gathering spaces, etc. Unsurprisingly, indoor environments
    also feature heavily in virtual depictions of the real world: in games, extended
    reality experiences, and architectural visualizations. Such virtual scenes have
    real-world uses, as well. For example, there are now a variety of free-to-use
    interior design tools online which allow users to explore virtual re-designs of
    their own real spaces (Planner5d, [2024](#bib.bib45); RoomSketcher, [2024](#bib.bib53);
    Target, [2024](#bib.bib61)). In addition, furniture and home product retailers
    are increasingly using renderings of virtual scenes to stage and advertise their
    products, as the process of doing so is easier, less expensive, and more adaptable
    to different regions of the world than taking physical photographs (Hobbs, [2024](#bib.bib26)).
    Finally, virtual indoor scenes have become a critical data source for training
    autonomous embodied agents to perceive and navigate within typical indoor environments (Deitke
    et al., [2022b](#bib.bib13); Puig et al., [2023](#bib.bib47)).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人在室内度过了生活中的大量时间：在他们的家中、工作场所、社交聚会空间等。毫不奇怪，室内环境也在虚拟现实的真实世界描绘中占据了重要地位：在游戏、扩展现实体验和建筑可视化中。这些虚拟场景在现实世界中也有实际应用。例如，现在有多种免费的室内设计工具在线上提供，允许用户探索自己实际空间的虚拟重新设计
    (Planner5d, [2024](#bib.bib45); RoomSketcher, [2024](#bib.bib53); Target, [2024](#bib.bib61))。此外，家具和家居产品零售商越来越多地使用虚拟场景的渲染来布置和宣传他们的产品，因为这种方法比拍摄实际照片更简单、便宜，并且更能适应全球不同地区
    (Hobbs, [2024](#bib.bib26))。最后，虚拟室内场景已成为训练自主体感知和在典型室内环境中导航的关键数据来源 (Deitke et al.,
    [2022b](#bib.bib13); Puig et al., [2023](#bib.bib47))。
- en: Given the importance of virtual indoor scenes to the above applications, computational
    design tools which ease their creation would be valuable. Generative models, i.e.
    systems which can sample novel scenes from a distribution of interest, are a particularly
    promising technology for this purpose. Such models can be used to suggest possible
    placements for new objects in a scene (Zhou et al., [2019](#bib.bib78)), suggest
    completions for partial scene designs (Ritchie et al., [2019](#bib.bib50)), or
    even synthesize entirely new scenes from whole cloth (Paschalidou et al., [2021](#bib.bib44);
    Tang et al., [2023](#bib.bib60); Gao et al., [2023b](#bib.bib21)). These capabilities
    can be used to build tools for interactive design or for the automated creation
    of large-scale virtual worlds.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于虚拟室内场景对上述应用的重要性，简化其创建的计算设计工具将非常有价值。生成模型，即能够从感兴趣的分布中生成新场景的系统，是一个特别有前途的技术。这些模型可以用于建议在场景中放置新对象的位置（Zhou
    等，[2019](#bib.bib78)），建议对部分场景设计的补充（Ritchie 等，[2019](#bib.bib50)），甚至从头合成全新的场景（Paschalidou
    等，[2021](#bib.bib44)；Tang 等，[2023](#bib.bib60)；Gao 等，[2023b](#bib.bib21)）。这些功能可以用于构建交互设计工具或自动创建大规模虚拟世界。
- en: 'The prevailing methodology for building generative models of indoor scenes
    is to train machine learning models on datasets of existing 3D room layouts. It
    is time-consuming and expensive to produce such datasets at the scale required
    by modern machine learning methods; as such, only a handful of these datasets
    exist (Fu et al., [2021](#bib.bib18); Yadav et al., [2023](#bib.bib69)). These
    existing datasets contain room labeled from a finite set of room types (e.g. bedrooms,
    living rooms, offices), and each room is populated with objects from a curated
    set of 3D object models belonging to a small finite set of object categories (e.g.
    tables, chairs, beds). We can think of generative models trained on these datasets
    as being *closed-universe*: they know up-front the small, finite set of object
    and room types that they will ever have to produce.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 构建室内场景生成模型的主要方法是对现有3D房间布局的数据集进行机器学习训练。生产这种规模的数据集既耗时又昂贵；因此，仅存在少数这些数据集（Fu 等，[2021](#bib.bib18)；Yadav
    等，[2023](#bib.bib69)）。这些现有的数据集包含从有限的房间类型（例如卧室、客厅、办公室）中标记的房间，每个房间都充满了来自一个小而有限的对象类别（例如桌子、椅子、床）的3D对象模型的对象。我们可以将基于这些数据集训练的生成模型视为*封闭宇宙*：它们预先知道它们将要生成的对象和房间类型的少量有限集。
- en: 'Could one create an *open-universe* generative model which can synthesize any
    type of indoor scene containing whatever types of objects are needed by that scene?
    It has only recently become possible to contemplate this question with the development
    of so-called “foundation models”: large machine learning models pre-trained on
    enormous datasets of text and/or images (GPT-4, [2023](#bib.bib22); Radford et al.,
    [2021](#bib.bib49); Rombach et al., [2021](#bib.bib51); Mizrahi et al., [2023](#bib.bib42)).
    For example, prior work has shown how to use pre-trained text-to-image generative
    models to synthesize 3D content which satisfies an arbitrary text prompt (Höllein
    et al., [2023](#bib.bib27); Jain et al., [2022](#bib.bib30); Poole et al., [2022](#bib.bib46);
    Yi et al., [2023](#bib.bib72)). While these systems are compelling, they produce
    output in the form of a single unstructured mesh or density/radiance field; these
    representations frequently exhibit artifacts and do not easily support editing
    a scene by moving, swapping, or deleting objects. In contrast, most prior methods
    for indoor scene synthesis produce a layout of individual objects, each of which
    is represented by a high-quality 3D mesh retrieved from a database.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 是否可以创建一个*开放宇宙*生成模型，该模型能够合成包含场景所需的任何类型对象的任何类型的室内场景？随着所谓“基础模型”的发展，这一问题才刚刚开始被考虑：这些大型机器学习模型在巨大的文本和/或图像数据集上进行预训练（GPT-4，[2023](#bib.bib22)；Radford
    等，[2021](#bib.bib49)；Rombach 等，[2021](#bib.bib51)；Mizrahi 等，[2023](#bib.bib42)）。例如，先前的工作展示了如何使用预训练的文本到图像生成模型来合成满足任意文本提示的3D内容（Höllein
    等，[2023](#bib.bib27)；Jain 等，[2022](#bib.bib30)；Poole 等，[2022](#bib.bib46)；Yi 等，[2023](#bib.bib72)）。虽然这些系统很有吸引力，但它们生成的输出是单一的非结构化网格或密度/辐射场，这些表示通常存在伪影，并且不容易通过移动、交换或删除对象来编辑场景。相比之下，大多数先前的室内场景合成方法生成的是单个对象的布局，每个对象都由从数据库中检索到的高质量3D网格表示。
- en: In this paper, we present an open-universe generative model of 3D indoor scenes
    which produces such structured object layouts in response to a text prompt (Fig. [1](#S0.F1
    "Figure 1 ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis
    and Uncurated Object Databases")). Building such a system requires solving several
    subproblems. First, the system must determine the objects that should be in the
    scene and where they should be located. To solve this problem, we leverage the
    commonsense world knowledge encoded by a pre-trained large language model (LLM) (GPT-4,
    [2023](#bib.bib22)). Empirically, we find that tasking an LLM with directly specifying
    the locations of scene objects leads to poor performance, likely due to the mismatch
    between metric location coordinates and the vast majority of natural language
    text contained in its training corpus. Instead, we task the LLM with producing
    a *declarative program* in a domain-specific language that describes the objects
    in the scene and a variety of spatial relation constraints between them. Executing
    these programs produces a constraint satisfaction problem which the system solves
    using a gradient-based optimizer to find one or more object layouts which satisfy
    the specified constraints.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一个开放宇宙的3D室内场景生成模型，该模型响应文本提示生成这样的结构化物体布局（图[1](#S0.F1 "Figure 1 ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases")）。构建这样一个系统需要解决几个子问题。首先，系统必须确定场景中应包含的物体以及它们应放置的位置。为了解决这个问题，我们利用了由预训练的大型语言模型（LLM）（GPT-4，[2023](#bib.bib22)）编码的常识世界知识。经验上，我们发现直接让LLM指定场景物体的位置表现不佳，这可能是由于度量位置坐标与其训练语料库中绝大多数自然语言文本之间的不匹配。相反，我们让LLM生成一个*声明性程序*，以领域特定语言描述场景中的物体及其之间的各种空间关系约束。执行这些程序产生一个约束满足问题，系统使用基于梯度的优化器来找到一个或多个符合指定约束的物体布局。
- en: 'Once the layout of objects is determined, the system must next insert a 3D
    mesh for each object in the layout. One could consider using text-to-3D generative
    models to synthesize these meshes (Jun and Nichol, [2023](#bib.bib31); Nichol
    et al., [2022](#bib.bib43); Poole et al., [2022](#bib.bib46)), but as mentioned
    above, these models can exhibit artifacts and do not (yet) produce outputs with
    comparable quality to human-created 3D meshes. Thus, like prior work on indoor
    scene synthesis, our system retrieves 3D meshes from a 3D object database. However,
    unlike prior work, our object retrieval system is designed for the open-universe
    setting: retrieving from million-scale databases of unlabeled, inconsistently-oriented
    3D meshes (Deitke et al., [2022a](#bib.bib12), [2023](#bib.bib11)). We develop
    a ranking and filtering algorithm using a combination of pre-trained models (GPT-4,
    [2023](#bib.bib22); Zhai et al., [2023a](#bib.bib74)) to retrieve a 3D mesh which
    matches the attributes of an object specified in the object layout. We also leverage
    these models to automatically determine the front-facing direction of each retrieved
    object, allowing the system to correctly orient each retrieved object as specified
    in the layout.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了物体的布局，系统接下来必须为布局中的每个物体插入一个3D网格。可以考虑使用文本到3D生成模型来合成这些网格（Jun和Nichol，[2023](#bib.bib31)；Nichol等，[2022](#bib.bib43)；Poole等，[2022](#bib.bib46)），但如上所述，这些模型可能会出现伪影，并且（尚未）生成的输出质量无法与人工创建的3D网格相媲美。因此，与先前的室内场景合成工作类似，我们的系统从3D物体数据库中检索3D网格。然而，与先前的工作不同，我们的物体检索系统是为开放宇宙环境设计的：从百万规模的未标记、不一致定向的3D网格数据库中检索（Deitke等，[2022a](#bib.bib12)，[2023](#bib.bib11)）。我们开发了一种排名和过滤算法，使用预训练模型的组合（GPT-4，[2023](#bib.bib22)；Zhai等，[2023a](#bib.bib74)）来检索与物体布局中指定的属性匹配的3D网格。我们还利用这些模型自动确定每个检索到的物体的前向方向，从而使系统能够根据布局正确地定向每个检索到的物体。
- en: We evaluate our system by using it to generate a large variety of different
    types of rooms, ranging from common indoor spaces (e.g. “a bedroom”) to rooms
    designed for specific activities (e.g. “a musician’s practice room”) to outlandish/fantastical
    scenes (e.g. “a wizard’s lair”). For generating typical indoor rooms, we compare
    to prior methods for closed-universe scene synthesis which are trained on existing
    3D scene datasets. Our system produces scenes which are preferred to those generated
    by these prior methods in a forced-choice perceptual study. For open-universe
    scene synthesis, we compare to LayoutGPT, a recently-published method for generating
    layouts using large language models. Our system also outperforms it in forced
    choice perceptual study. We also conduct ablation studies on each component of
    our system to validate our design decisions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用系统生成各种不同类型的房间来评估它，从常见的室内空间（例如“卧室”）到为特定活动设计的房间（例如“音乐家的练习室”）再到离奇/幻想场景（例如“巫师的巢穴”）。对于生成典型的室内房间，我们与先前方法进行比较，这些方法在现有3D场景数据集上进行了训练。我们的系统生成的场景在强制选择感知研究中优于这些先前方法。对于开放宇宙场景合成，我们与LayoutGPT进行比较，后者是一种使用大型语言模型生成布局的最近发布的方法。我们的系统在强制选择感知研究中也优于它。我们还对系统的每个组件进行了消融研究，以验证我们的设计决策。
- en: 'In summary, our contributions are:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的贡献包括：
- en: (i)
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: A DSL for specifying indoor scene layouts through declarative constraints and
    a gradient-based executor for this DSL capable of realizing a distribution of
    valid scenes from a single program
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种通过声明性约束指定室内场景布局的DSL，以及一个用于该DSL的基于梯度的执行器，能够从单个程序中实现有效场景的分布
- en: (ii)
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: A robust prompting workflow that leverages LLMs to synthesize programs in our
    DSL from a high-level natural language description of a scene
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种利用LLMs从场景的高层自然语言描述合成我们DSL中程序的稳健提示工作流程
- en: (iii)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (iii)
- en: A pipeline using pretrained vision-language models for retrieving and orienting
    3D meshes from a large, unannotated database to fit object specifications from
    a scene program
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用预训练的视觉-语言模型从大型未标注数据库中检索和定位3D网格，以适应场景程序中的对象规格
- en: (iv)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (iv)
- en: Protocols for evaluating open-universe indoor synthesis systems, including a
    benchmark set of input descriptions covering a wide variety of possible rooms.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于评估开放宇宙室内合成系统的协议，包括涵盖各种可能房间的输入描述基准集。
- en: Our code will be made available as open source upon publication.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码将在出版后作为开源提供。
- en: 2\. Related Work
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 相关工作
- en: Indoor Scene Synthesis
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 室内场景合成
- en: Indoor scene synthesis has been a problem of interest in computer graphics for
    years. In an interesting instance of history repeating itself, some of the earliest
    work in this area formulated the problem as text-to-scene generation, albeit via
    laboriously hand-crafted rules (Coyne and Sproat, [2001](#bib.bib9)). Later, researchers
    built systems for laying out objects to be consistent with a set of manually-defined
    design principles (Merrell et al., [2011](#bib.bib41)), simple statistical relationships
    between objects extracted from a small set of examples (Yu et al., [2011](#bib.bib73)),
    or with programmatically-specified constraints (Yeh et al., [2012](#bib.bib71)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 室内场景合成多年来一直是计算机图形学中的一个关注问题。在历史有趣的重演实例中，该领域的一些早期工作将问题形式化为文本到场景的生成，尽管是通过繁琐的手工规则（Coyne
    和 Sproat，[2001](#bib.bib9)）。后来，研究人员构建了系统，用于布局对象以与一组手动定义的设计原则（Merrell et al., [2011](#bib.bib41)）、从小样本中提取的对象之间的简单统计关系（Yu
    et al., [2011](#bib.bib73)）或程序指定的约束（Yeh et al., [2012](#bib.bib71)）保持一致。
- en: 'After this, indoor scene synthesis research focused on data-driven methods,
    using a variety of machine learning methods: Bayesian networks and Gaussian mixture
    models (Fisher et al., [2012](#bib.bib17)), factor graphs (Kermani et al., [2016](#bib.bib32)),
    topic models (Liang et al., [2017](#bib.bib35)), and stochastic grammars (Qi et al.,
    [2018](#bib.bib48)). Once deep neural networks gained popularity, a wave of research
    applying them to indoor scene synthesis followed: method were proposed using convolutional
    networks (Ritchie et al., [2019](#bib.bib50); Wang et al., [2018](#bib.bib64),
    [2019](#bib.bib63)), tree and graph neural networks (Li et al., [2018](#bib.bib33);
    Wang et al., [2019](#bib.bib63); Zhou et al., [2019](#bib.bib78)), generative
    adversarial networks (Zhang et al., [2018](#bib.bib76)), transformers (Paschalidou
    et al., [2021](#bib.bib44); Wang et al., [2020](#bib.bib65)), and finally denoising
    diffusion models (Tang et al., [2023](#bib.bib60)). All of these prior works present
    closed-universe generative models, and all of them require (in some cases quite
    large) datasets of 3D scenes for training.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之后，室内场景合成的研究集中于数据驱动的方法，使用了多种机器学习方法：贝叶斯网络和高斯混合模型（Fisher 等， [2012](#bib.bib17)），因子图（Kermani
    等， [2016](#bib.bib32)），主题模型（Liang 等， [2017](#bib.bib35)），以及随机语法（Qi 等， [2018](#bib.bib48)）。一旦深度神经网络获得了广泛应用，随之而来的是将其应用于室内场景合成的研究浪潮：提出了使用卷积网络的方法（Ritchie
    等， [2019](#bib.bib50)；Wang 等， [2018](#bib.bib64)， [2019](#bib.bib63)），树形和图神经网络（Li
    等， [2018](#bib.bib33)；Wang 等， [2019](#bib.bib63)；Zhou 等， [2019](#bib.bib78)），生成对抗网络（Zhang
    等， [2018](#bib.bib76)），变换器（Paschalidou 等， [2021](#bib.bib44)；Wang 等， [2020](#bib.bib65)），以及最后的去噪扩散模型（Tang
    等， [2023](#bib.bib60)）。所有这些先前的工作都呈现了封闭宇宙生成模型，并且都需要（在某些情况下相当大的）3D场景数据集进行训练。
- en: 'Recently, the development of pre-trained large language models (LLMs) has raised
    the possibility of a new generation of text-to-scene generative models, more flexible
    and open-ended than the early systems from decades ago. LayoutGPT (Feng et al.,
    [2023](#bib.bib16)) is an LLM-based system designed for generating image layouts
    in an a CSS-like format; the authors also show applications to indoor scene synthesis,
    albeit for the closed-universe case. In work concurrent to ours, the Holodeck
    system shows the ability to use LLMs to generate environments for training embodied
    AI agents (Yang et al., [2023](#bib.bib70)). This system resembles ours in some
    aspects, including supporting general text prompts instead of fixed room types
    and specifying object locations implicitly via relations. It also differs from
    ours in significant ways: using a simpler specification for object relations (we
    use a DSL embedded in Python); lacking mechanisms for automatically correcting
    errors in LLM output; solving for object layouts on a grid, rather than continuously
    (so objects cannot be adjacent to one another). Most importantly, it retrieves
    objects from a curated set of annotated and aligned 3D models, so it cannot be
    considered truly open-universe.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，预训练的大型语言模型（LLMs）的发展提高了新一代文本到场景生成模型的可能性，这些模型比几十年前的早期系统更灵活和开放。LayoutGPT（Feng
    等， [2023](#bib.bib16)）是一个基于LLM的系统，旨在生成类似CSS格式的图像布局；作者还展示了其在室内场景合成中的应用，尽管是针对封闭宇宙的情况。在与我们工作同时进行的研究中，Holodeck系统展示了使用LLMs生成用于训练具身AI代理的环境的能力（Yang
    等， [2023](#bib.bib70)）。该系统在一些方面与我们类似，包括支持通用文本提示而不是固定的房间类型，并通过关系隐式地指定对象位置。它也在重要方面与我们有所不同：使用更简单的对象关系规范（我们使用嵌入Python的DSL）；缺乏自动纠正LLM输出错误的机制；在网格上求解对象布局，而不是连续的（因此对象不能彼此相邻）。最重要的是，它从一个经过精心策划的标注和对齐的3D模型集合中检索对象，因此不能被视为真正的开放宇宙。
- en: Open-vocabulary Text-to-3D
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开放词汇文本到3D
- en: There has been a recent surge in work leveraging pre-trained vision-language
    models (Radford et al., [2021](#bib.bib49); Rombach et al., [2021](#bib.bib51))
    to produce 3D content from arbitrary text prompts without any training data. The
    most prevalent type of such system works via “optimization-based inference,” optimizing
    for a new 3D output in response to each new text prompt (Jain et al., [2022](#bib.bib30);
    Poole et al., [2022](#bib.bib46); Yi et al., [2023](#bib.bib72); Lin et al., [2023](#bib.bib36);
    Chen et al., [2023](#bib.bib7); Wang et al., [2023](#bib.bib66)). Another line
    of work seeks to amortize this inference procedure by training feedforward neural
    networks to produce 3D output from a distribution of text inputs (Sanghi et al.,
    [2022](#bib.bib55), [2023](#bib.bib56); Nichol et al., [2022](#bib.bib43); Jun
    and Nichol, [2023](#bib.bib31); Lorraine et al., [2023](#bib.bib38)). The outputs
    of these methods are either point clouds, unstructured meshes, or isosurfaces
    extracted from density fields, which are (to date) lower-quality than human-created
    3D models. Additionally, since these systems leverage models trained on images
    to synthesize 3D structures, they can also suffer from multiview inconsistency
    artefacts, such as the infamous “Janus face” issue (Poole et al., [2022](#bib.bib46)).
    Their output also cannot be easily modified, because it is not decomposed into
    individual objects.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，利用预训练的视觉语言模型（Radford et al., [2021](#bib.bib49); Rombach et al., [2021](#bib.bib51)）来生成3D内容的工作有所激增，这些模型能够根据任意的文本提示生成3D内容而无需任何训练数据。这类系统中最普遍的类型通过“基于优化的推理”来工作，即针对每个新的文本提示优化生成新的3D输出（Jain
    et al., [2022](#bib.bib30); Poole et al., [2022](#bib.bib46); Yi et al., [2023](#bib.bib72);
    Lin et al., [2023](#bib.bib36); Chen et al., [2023](#bib.bib7); Wang et al., [2023](#bib.bib66)）。另一类工作试图通过训练前馈神经网络，从一组文本输入中生成3D输出，从而摊销这种推理过程（Sanghi
    et al., [2022](#bib.bib55), [2023](#bib.bib56); Nichol et al., [2022](#bib.bib43);
    Jun and Nichol, [2023](#bib.bib31); Lorraine et al., [2023](#bib.bib38)）。这些方法的输出通常是点云、非结构化网格或从密度场中提取的等值面，这些结果（迄今为止）质量低于人工创建的3D模型。此外，由于这些系统利用在图像上训练的模型来合成3D结构，它们也可能遭遇多视角不一致的伪影，如臭名昭著的“贪婪面孔”问题（Poole
    et al., [2022](#bib.bib46)）。它们的输出也无法轻易修改，因为它没有被分解成单独的对象。
- en: These systems are designed with single-object generation in mind but have been
    extended to open-vocabulary scene synthesis. By combining 2D generative image
    out-painting with a depth alignment module, Text2Room (Höllein et al., [2023](#bib.bib27))
    generates textured meshes of 3D rooms for a given text prompt. Other works (Fang
    et al., [2023](#bib.bib15); Schult et al., [2023](#bib.bib57); Bai et al., [2023](#bib.bib5);
    Gao et al., [2023a](#bib.bib20)) allow the specification of a 3D semantic object
    layout, which is then used along with text-to-image models for generating textured
    meshes of scenes/rooms. These method suffer from the same mesh quality drawbacks
    as their single-object counterparts They also assume an object layout as input,
    whereas our method generates one. Our approach could potentially be combined with
    methods in this space; for example, generative re-texturing of retrieved 3D meshes
    to fit a desired style or theme (Huang et al., [2023b](#bib.bib28)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统最初设计时是考虑到单对象生成，但已经扩展到开放词汇场景合成。通过将2D生成图像外延与深度对齐模块相结合，Text2Room（Höllein et
    al., [2023](#bib.bib27)）为给定的文本提示生成带纹理的3D房间网格。其他工作（Fang et al., [2023](#bib.bib15);
    Schult et al., [2023](#bib.bib57); Bai et al., [2023](#bib.bib5); Gao et al.,
    [2023a](#bib.bib20)）允许指定3D语义对象布局，然后与文本到图像模型结合生成带纹理的场景/房间网格。这些方法与其单对象对应物一样，存在网格质量上的缺陷。它们还假设了一个对象布局作为输入，而我们的方法则是生成一个对象布局。我们的方法有可能与这一领域的其他方法结合；例如，生成性的重新纹理化以使检索到的3D网格符合期望的风格或主题（Huang
    et al., [2023b](#bib.bib28)）。
- en: 3D Shape Analysis with Foundation Models
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础模型的3D形状分析
- en: In addition to using pre-trained vision language models (VLMs) for 3D content
    generation, researchers have also explored how to use these models to analyze
    existing 3D content without requiring 3D supervision. Methods have been proposed
    for captioning/annotating 3D objects (Luo et al., [2023](#bib.bib39); Haocheng
    et al., [2023](#bib.bib25)), segmenting 3D shapes into semantic parts or identifying
    regions of interest (Liu et al., [2023](#bib.bib37); Zhou et al., [2023](#bib.bib77);
    Abdelreheem et al., [2023b](#bib.bib3); Decatur et al., [2022](#bib.bib10)), and
    even establishing correspondences between 3D shapes (Abdelreheem et al., [2023a](#bib.bib2))
    Our system uses VLMs to retrieve shapes from a large database, determine their
    category, and determine their front-facing orientation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用预训练的视觉语言模型（VLMs）进行3D内容生成外，研究人员还探索了如何使用这些模型分析现有的3D内容而无需3D监督。已经提出了一些方法，用于对3D物体进行描述/注释（Luo
    等，[2023](#bib.bib39)；Haocheng 等，[2023](#bib.bib25)），将3D形状分割成语义部分或识别兴趣区域（Liu 等，[2023](#bib.bib37)；Zhou
    等，[2023](#bib.bib77)；Abdelreheem 等，[2023b](#bib.bib3)；Decatur 等，[2022](#bib.bib10)），甚至在3D形状之间建立对应关系（Abdelreheem
    等，[2023a](#bib.bib2)）。我们的系统使用VLMs从大型数据库中检索形状，确定其类别，并确定其前向朝向。
- en: Program Synthesis with Large Language Models
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大语言模型的程序合成
- en: One of the key components of our system is using an LLM to generate a declarative
    program which specifies the layout of objects in a scene. Other work has also
    explored the use of LLMs to generate programs. Multiple works (Li et al., [2022](#bib.bib34);
    AlphaCode Team, [2023](#bib.bib4)) explore the use of LLMs for competitive programming,
    demonstrating the prowess of LLMs (augmented with symbolic search) at synthesizing
    programs for a given natural language task description. LLMs’ program synthesis
    abilities have also been employed for solving complex geometric reasoning problems (Trinh
    et al., [2024](#bib.bib62)) and discovering novel mathematical concepts (Romera-Paredes
    et al., [2023](#bib.bib52)). Beyond generating programs, some recent works also
    explore the use of LLMs to improve domain-specific languages automatically (Grand
    et al., [2023](#bib.bib23)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们系统的关键组成部分之一是使用LLM生成一个声明式程序，该程序指定场景中物体的布局。其他研究也探索了使用LLM生成程序的方法。多个研究（Li 等，[2022](#bib.bib34)；AlphaCode团队，[2023](#bib.bib4)）探索了LLM在竞争编程中的应用，展示了LLM（通过符号搜索增强）的编程合成功能。LLM的程序合成功能也被用于解决复杂的几何推理问题（Trinh
    等，[2024](#bib.bib62)）和发现新的数学概念（Romera-Paredes 等，[2023](#bib.bib52)）。除了生成程序之外，一些近期的研究还探索了使用LLM自动改进领域特定语言（Grand
    等，[2023](#bib.bib23)）。
- en: '![Refer to caption](img/8e69cce66f3803b6c6d15fb724ef2480.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8e69cce66f3803b6c6d15fb724ef2480.png)'
- en: Figure 2\. A schematic overview of our system. Given a high-level natural language
    description of a scene (plus optional constraints on the room size and object
    density), an LLM-based program synthesizer produces a scene description program
    which specifies the objects in the scene and their spatial relations. Our layout
    optimizer module then solves the constraint satisfaction problem implied by this
    program to produce a concrete layout of objects in the scene. For each scene object,
    the object retrieval module finds an appropriate 3D mesh from a large, unannotated
    mesh database; the object orientation module then identifies its front-facing
    direction so that it can be correctly inserted into the scene.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 我们系统的示意图。给定场景的高层次自然语言描述（加上对房间大小和物体密度的可选约束），基于LLM的程序合成器生成一个场景描述程序，该程序指定场景中的物体及其空间关系。我们的布局优化模块随后解决该程序所隐含的约束满足问题，以生成场景中物体的具体布局。对于每个场景物体，物体检索模块从一个大型、未标注的网格数据库中找到一个合适的3D网格；物体方向模块随后确定其朝向方向，以便可以正确地将其插入场景中。
- en: Recently, systems have been proposed for visual question answering (VQA) by
    using LLMs to generate programs in a domain-specific language (DSL) designed for
    image reasoning and then executing that program to answer a given question (Surís
    et al., [2023](#bib.bib59); Gupta and Kembhavi, [2023](#bib.bib24)). LLMs have
    also been used for structured image synthesis by equipping them with a DSL which
    helps specify 2D object layouts (Cho et al., [2023](#bib.bib8)). Equipped with
    a library of Python functions in Blender, LLMs have also been used to synthesize
    3D scenes, albeit for a closed set of scenes and objects supported by the library (Sun
    et al., [2023](#bib.bib58)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，已有系统被提出，用于通过使用LLM生成用于图像推理的领域特定语言（DSL）中的程序，然后执行该程序来回答给定的问题，实现视觉问答（VQA）（Surís等，[2023](#bib.bib59)；Gupta和Kembhavi，[2023](#bib.bib24)）。LLM也被用于结构化图像合成，通过为其配备DSL来帮助指定2D物体布局（Cho等，[2023](#bib.bib8)）。配备了Blender中Python函数库的LLM也被用于合成3D场景，尽管仅限于库支持的封闭场景和物体（Sun等，[2023](#bib.bib58)）。
- en: 3\. Overview
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 概述
- en: 'We aim to solve the following problem: given a natural language description
    of a desired 3D room-scale scene, produce a 3D scene composed of positioned and
    oriented 3D meshes retrieved from a database such that the output scene satisfies
    the input description. The input description can be flexible: it could provide
    detailed instructions about the contents of the scene (e.g. “an office with two
    desks, a potted plant, and a sofa”) or be intentionally nebulous (“a serious business
    office”). For additional control, we also support optional inputs in the form
    of desired dimensions (in meters) for the output room and how full the room should
    be (in terms of percentage of floor area occupied by objects). On the output side,
    we assume that the generated room has four walls, and that each object is oriented
    to face along one of the four cardinal directions (north, south, east, west).
    These assumptions are reflective of many, but not all, real-world rooms; in Section [10](#S10
    "10\. Discussion & Future Work ‣ Open-Universe Indoor Scene Generation using LLM
    Program Synthesis and Uncurated Object Databases"), we discuss ideas for removing
    these assumptions.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是解决以下问题：给定一个自然语言描述的期望3D房间规模场景，生成一个由从数据库中检索的定位和定向的3D网格组成的3D场景，使得输出场景符合输入描述。输入描述可以是灵活的：它可以提供关于场景内容的详细指示（例如：“一个有两张桌子、一盆植物和一张沙发的办公室”）或故意模糊的描述（“一个严肃的商务办公室”）。为了进一步控制，我们还支持以期望的尺寸（以米为单位）作为可选输入，指定输出房间的大小以及房间应有的充实程度（即物体占地面积的百分比）。在输出方面，我们假设生成的房间有四面墙，每个物体的方向都面向四个主要方向之一（北、南、东、西）。这些假设反映了许多但不是所有的现实世界房间；在第[10](#S10
    "10\. 讨论与未来工作 ‣ 使用LLM程序合成和未整理的物体数据库生成开放宇宙室内场景")节中，我们讨论了去除这些假设的想法。
- en: Figure [2](#S2.F2 "Figure 2 ‣ Program Synthesis with Large Language Models ‣
    2\. Related Work ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis
    and Uncurated Object Databases") shows a schematic overview of our system. Our
    system specifies the objects which should be in the generated scene and their
    spatial layout via a scene description program written in a declarative domain-specific
    language embedded in Python (Section [4](#S4 "4\. Describing Scenes with Programs
    ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated
    Object Databases")). To produce this program, the system feeds the inputs to a
    program synthesizer module (Section [5](#S5 "5\. Generating Scene Programs ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases"));
    internally, this module uses a series of calls to an LLM to write the scene description
    program. The complete scene description program is then passed to the layout optimizer
    (Section [6](#S6 "6\. Scene Layout Optimization ‣ Open-Universe Indoor Scene Generation
    using LLM Program Synthesis and Uncurated Object Databases")), which converts
    the program into a constraint satisfaction problem which it then solves using
    a gradient-based optimization scheme, producing locations and orientations for
    all objects. In addition, the object declarations in the scene description program
    are sent to an object retrieval module (Section [7](#S7 "7\. Retrieving 3D Objects
    ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated
    Object Databases")), which retrieves from a large, unannotated database the 3D
    model which best matches the description of each object. Finally, the object orientation
    module (Section [8](#S8 "8\. Orienting Retrieved Objects ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases"))
    determines the front-facing direction of each retrieved object, allowing them
    to be inserted in the room according to the optimized layout to produce the final
    output scene.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S2.F2 "图 2 ‣ 使用大型语言模型的程序合成 ‣ 2\. 相关工作 ‣ 使用LLM程序合成和未经整理的对象数据库生成开放宇宙室内场景")
    展示了我们系统的示意概述。我们的系统通过用Python嵌入的声明性领域特定语言编写的场景描述程序来指定生成场景中的对象及其空间布局（第 [4](#S4 "4\.
    使用程序描述场景 ‣ 使用LLM程序合成和未经整理的对象数据库生成开放宇宙室内场景") 节）。为了生成这个程序，系统将输入传递给程序合成模块（第 [5](#S5
    "5\. 生成场景程序 ‣ 使用LLM程序合成和未经整理的对象数据库生成开放宇宙室内场景") 节）；在内部，该模块通过一系列调用LLM来编写场景描述程序。完整的场景描述程序随后传递给布局优化器（第 [6](#S6
    "6\. 场景布局优化 ‣ 使用LLM程序合成和未经整理的对象数据库生成开放宇宙室内场景") 节），该优化器将程序转换为一个约束满足问题，并使用基于梯度的优化方案解决这个问题，从而为所有对象生成位置和方向。此外，场景描述程序中的对象声明被发送到对象检索模块（第 [7](#S7
    "7\. 检索3D对象 ‣ 使用LLM程序合成和未经整理的对象数据库生成开放宇宙室内场景") 节），该模块从一个大型的、未经注释的数据库中检索出最符合每个对象描述的3D模型。最后，对象方向模块（第 [8](#S8
    "8\. 定向检索对象 ‣ 使用LLM程序合成和未经整理的对象数据库生成开放宇宙室内场景") 节）确定每个检索对象的正面方向，使得它们可以根据优化后的布局被插入房间，从而生成最终的输出场景。
- en: 4\. Describing Scenes with Programs
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 使用程序描述场景
- en: '![Refer to caption](img/b5e0ad38159b93523e4b5b4930cea30e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b5e0ad38159b93523e4b5b4930cea30e.png)'
- en: Figure 3\. An example program in our declarative scene description language
    (left) and the object layout produced by running this program through our layout
    optimizer (right). This scene depicts a small, cozy Italian restaurant.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 左侧是我们声明性场景描述语言中的一个示例程序，右侧是通过我们的布局优化器运行该程序后产生的对象布局。这个场景描绘了一个小而温馨的意大利餐厅。
- en: In this section we introduce our declarative domain-specific language (DSL)
    for scene description. The simplest way to describe a scene is to explicitly specify
    all object positions and orientations. However, recent work has shown that even
    the state of the art LLMs such as OpenAI’s GPT4 struggle with accurate placement
    of objects and object parts (Makatura et al., [2023](#bib.bib40)). For example,
    when asked to specify explicit coordinates, GPT4 often creates overlapping objects
    and objects that are floating in the air. Thus, instead of specifying explicit
    coordinates, we describe scenes in a declarative manner using spatial relations.
    Our intuition is that it is easier for an LLM to reason about sentences such as
    “the lamp is on the table” or “the chair is adjacent to the table” than about
    precise numeric values.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了用于场景描述的声明性特定领域语言（DSL）。描述场景的最简单方法是明确指定所有对象的位置和方向。然而，最近的研究表明，即使是最先进的
    LLM，如 OpenAI 的 GPT4，也难以准确地放置对象和对象部件（Makatura 等，[2023](#bib.bib40)）。例如，当被要求指定明确的坐标时，GPT4
    常常创建重叠的对象或悬浮在空中的对象。因此，我们不使用明确的坐标，而是通过空间关系以声明性方式描述场景。我们的直觉是，LLM 更容易推理诸如“灯在桌子上”或“椅子紧挨桌子”这样的句子，而不是精确的数值。
- en: 'Our scene description language is embedded in Python. Using Python allows us
    to capitalize on the expressivity of modern programming languages: features such
    as loops, conditionals, arithmetic, list comprehensions, and many useful built-in
    functions. It also takes advantage of GPT4’s strong Python programming abilities,
    likely due to the large amount of Python code in its training corpus. Our language
    adds several domain-specific functions to Python. The new functions are either
    (1) object constructors, (2) relation functions, or (3) parameter setting functions.
    Appendix [A](#A1 "Appendix A Scene Description Language ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases")
    contains a full listing of the new functions added by our language.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的场景描述语言嵌入在 Python 中。使用 Python 让我们能够利用现代编程语言的表达力：例如循环、条件语句、算术运算、列表推导式以及许多有用的内置函数。它还利用了
    GPT4 强大的 Python 编程能力，这可能是由于其训练语料库中大量的 Python 代码。我们的语言向 Python 添加了几个特定领域的函数。这些新函数包括（1）对象构造函数，（2）关系函数，以及（3）参数设置函数。附录
    [A](#A1 "附录 A 场景描述语言 ‣ Open-Universe 室内场景生成使用 LLM 程序综合和未整理的对象数据库") 包含了我们语言所添加的新函数的完整列表。
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4\. Describing Scenes with Programs ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases")
    shows an example program written in our language and the scene layout that it
    produces when run through our layout optimizer module (Section [6](#S6 "6\. Scene
    Layout Optimization ‣ Open-Universe Indoor Scene Generation using LLM Program
    Synthesis and Uncurated Object Databases")). This program describes a small Italian
    restaurant. Below, we walk through the functionality of each part of this program:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S4.F3 "图 3 ‣ 4\. 使用程序描述场景 ‣ Open-Universe 室内场景生成使用 LLM 程序综合和未整理的对象数据库")
    展示了用我们语言编写的示例程序及其通过我们的布局优化模块（第 [6](#S6 "6\. 场景布局优化 ‣ Open-Universe 室内场景生成使用 LLM
    程序综合和未整理的对象数据库") 节）运行后生成的场景布局。该程序描述了一个小型意大利餐厅。下面，我们将逐步讲解该程序每个部分的功能：
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Line 1 sets the size of the scene in meters.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 1 行设置了场景的大小（单位为米）。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lines 3–8 create tables and chairs in a double loop. Most objects are created
    with an Object(description, width, depth, height, facing) constructor. The width
    of an object is a dimension perpendicular to the object’s front-facing direction;
    the depth is a dimension along this direction. Relation adjacent constrains chairs
    to be next to their corresponding tables (two chairs on each side of the table).
    Relation facing orients each chair to face its corresponding table.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 3 到第 8 行通过双重循环创建了桌子和椅子。大多数对象是通过 Object(description, width, depth, height,
    facing) 构造函数创建的。对象的宽度是垂直于对象正面方向的维度；深度是沿此方向的维度。关系 adjacent 限制了椅子必须在其对应的桌子旁边（每张桌子两侧各两个椅子）。关系
    facing 使每把椅子朝向其对应的桌子。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Line 10: relation aligned constrains all table centers to be on the same line
    running west to east.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第 10 行：关系 aligned 限制所有桌子中心必须在东西方向上的同一条直线上。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lines 12–15 create an L-shaped configuration of a bar and counter. Relation
    adjacent(bar, counter, NORTH, WEST) constrains the bar to be adjacent to the counter
    from the north and aligned with the west side of the counter.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第12–15行创建了一个L形的酒吧和柜台配置。关系adjacent(bar, counter, NORTH, WEST)将酒吧限制在北侧邻近柜台，并与柜台的西侧对齐。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lines 17–18 create a menu board and constrain it to be mounted on the north
    wall above the bar, to be accessible both by customers and by staff.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第17–18行创建了一个菜单板，并将其限制安装在酒吧上方的北墙上，以便客户和工作人员都可以使用。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lines 23–33 create a shelf mounted on the east wall and constrain a row of cheese
    wheels of various sizes to be on top of this shelf.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第23–33行创建了一个挂在东墙上的架子，并将一排不同大小的奶酪轮限制在这个架子上。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lines 35–39 create doors and a window. We constrain the kitchen door to be no
    more than 0.5m from north wall, and the entrance door to be no more than 1m from
    the west wall.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第35–39行创建了门和一个窗户。我们限制厨房门距离北墙不超过0.5米，入口门距离西墙不超过1米。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lines 41–42 create 3 paintings mounted on the north wall. However, unlike chairs,
    tables and cheese wheels, we don’t want the paintings to be represented by the
    same 3d model. That is why we use a unique_objects constructor.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第41–42行创建了3幅挂在北墙上的画作。然而，与椅子、桌子和奶酪轮不同，我们不希望这些画作由相同的3D模型表示。这就是为什么我们使用unique_objects构造函数。
- en: 5\. Generating Scene Programs
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 生成场景程序
- en: '![Refer to caption](img/a06f2e0b881c52ad42d4a4b61b8ba62d.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a06f2e0b881c52ad42d4a4b61b8ba62d.png)'
- en: Figure 4\. Our scene description program synthesizer proceeds in three steps,
    each of which uses a large language model. First, the LLM is asked to generate
    a natural language description of all the objects in the scene, along with how
    and why they are spatially related to one another. Then, a sequence of two LLMs
    translate this description into code which declares objects and relations, respectively.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 我们的场景描述程序合成器分为三个步骤，每个步骤都使用一个大型语言模型。首先，LLM被要求生成对场景中所有对象的自然语言描述，以及它们之间的空间关系和原因。然后，一系列两个LLM将这个描述翻译成声明对象和关系的代码。
- en: 'Given freeform text input, our system must synthesize scene programs like the
    one in Fig. [3](#S4.F3 "Figure 3 ‣ 4\. Describing Scenes with Programs ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases").
    We use a large language model (LLM) to perform this task. Going from a high-level,
    natural language description to a Python program is a challenging task. LLMs have
    a remarkable ability to perform this task, but they are not perfect. In our early
    experiments tasking an LLM to generate scene description programs directly from
    text prompts, it tended to produce undesirably short/sparse programs and make
    some errors (we taxonomize the types of these errors in Section [6.3](#S6.SS3
    "6.3\. Error Correction ‣ 6\. Scene Layout Optimization ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases")).
    To reduce the prevalence of errors and produce richer scene description programs,
    we found it helpful to split the program generation task into a series of smaller
    sub-tasks (an instance of chain-of-thought prompting (Wei et al., [2023](#bib.bib67))):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 给定自由形式的文本输入，我们的系统必须合成类似于图[3](#S4.F3 "图 3 ‣ 4\. 使用程序描述场景 ‣ 使用LLM程序合成和未整理的对象数据库进行开放宇宙室内场景生成")中的场景程序。我们使用大型语言模型（LLM）来执行此任务。从高级自然语言描述到Python程序是一个具有挑战性的任务。LLM在执行此任务方面具有显著的能力，但它们并不完美。在我们早期的实验中，让LLM直接从文本提示生成场景描述程序时，它往往会生成不理想的简短/稀疏程序并出现一些错误（我们在第[6.3节](#S6.SS3
    "6.3\. 错误修正 ‣ 6\. 场景布局优化 ‣ 使用LLM程序合成和未整理的对象数据库进行开放宇宙室内场景生成")中对这些错误进行了分类）。为了减少错误的发生并生成更丰富的场景描述程序，我们发现将程序生成任务分解成一系列较小的子任务（链式思维提示的一个实例（Wei
    et al., [2023](#bib.bib67)））是很有帮助的：
- en: (1)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Generate a detailed natural language description of the complete scene
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成完整场景的详细自然语言描述
- en: (2)
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Generate the code to declare all objects in the scene (ensuring there are enough
    objects to achieve the desired room fullness)
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成代码以声明场景中的所有对象（确保有足够的对象以达到期望的房间充实度）
- en: (3)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Generate the code to specify all object relations
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成代码以指定所有对象关系
- en: Task (1) is better aligned with the LLM’s training data, so it is more reliable
    than directly generating code. Tasks (2) and (3) become easier because they can
    refer to a concrete natural language description—essentially, they are translation
    tasks, rather than synthesis tasks. Figure [4](#S5.F4 "Figure 4 ‣ 5\. Generating
    Scene Programs ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis
    and Uncurated Object Databases") shows a schematic of this pipeline. The remainder
    of this section describes each of these stages in more detail; the complete prompt
    templates for each can be found in the supplemental material. In Section [9](#S9
    "9\. Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM
    Program Synthesis and Uncurated Object Databases"), we conduct an ablation study
    to show the value of this task decomposition.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 任务（1）与LLM的训练数据更为对齐，因此比直接生成代码更为可靠。任务（2）和（3）变得更容易，因为它们可以参考一个具体的自然语言描述——本质上，它们是翻译任务，而非合成任务。图[4](#S5.F4
    "图 4 ‣ 5\. 生成场景程序 ‣ 开放宇宙室内场景生成使用LLM程序合成和未整理对象数据库")展示了这一流程的示意图。本节其余部分更详细地描述了这些阶段；每个阶段的完整提示模板可以在补充材料中找到。在第[9](#S9
    "9\. 结果与评估 ‣ 开放宇宙室内场景生成使用LLM程序合成和未整理对象数据库")节中，我们进行了消融研究以展示这一任务分解的价值。
- en: 5.1\. Describing the Scene in Natural Language
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 使用自然语言描述场景
- en: 'The first stage of the program synthesizer tasks an LLM with describing the
    scene to be generated in natural language. First, if the user has not provided
    an input room size or target object density, the LLM is first asked to produce
    values for those quantities which are appropriate for the input text prompt. The
    texture of the walls and floors are described as well, to be retrieved later in
    the pipeline. Then, the LLM is asked to list and thoroughly describe all the objects
    in the scene: the type of object, its size, and any salient details about its
    appearance. For each object, the LLM also outputs a description of how that object
    is situated in the scene in relation to other objects. Throughout, the LLM is
    asked to explain its reasoning.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 程序合成器的第一阶段要求LLM用自然语言描述待生成的场景。首先，如果用户没有提供输入的房间大小或目标对象密度，则LLM会被要求生成适合输入文本提示的这些数量值。墙壁和地板的纹理也会被描述，以便在后续流程中检索。然后，LLM需要列出并详细描述场景中的所有对象：对象的类型、大小以及外观的任何显著细节。对于每个对象，LLM还会输出该对象在场景中与其他对象的相对位置的描述。在整个过程中，LLM会被要求解释其推理过程。
- en: To guide the LLM, our prompt template for this stage includes two in-context
    examples of the type of output we expect in this stage. The first in-context example
    describes an artist’s one room apartment; the second describes a typical a dining
    room. These two examples span a variety of object types and arrangements; our
    results in Section [9](#S9 "9\. Results and Evaluation ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases")
    show that the system generalizes beyond these two examples to synthesize an even
    wider variety of scenes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指导LLM，我们为这一阶段的提示模板包括了两种期望输出类型的上下文示例。第一个上下文示例描述了一个艺术家的单间公寓；第二个描述了一个典型的餐厅。这两个示例涵盖了多种对象类型和排列；我们在第[9](#S9
    "9\. 结果与评估 ‣ 开放宇宙室内场景生成使用LLM程序合成和未整理对象数据库")节的结果表明，该系统能够超越这两个示例，合成出更多种类的场景。
- en: 5.2\. Declaring Objects
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 声明对象
- en: The next stage of the program synthesizer tasks an LLM with producing Python
    code that declares all the objects in the scene. This stage receives the natural
    language description output by the first stage as its input. In addition to choosing
    the most appropriate constructor for each object (i.e. Object, objects, or unique_objects)
    and the relevant arguments for those constructors (description, dimensions, and
    facing direction information), it also produces its estimate of a ‘category label’
    for the object (e.g. for ‘a sleek dark wood dining table,’ the category label
    might be ‘table’). This category label is used by the later object retrieval and
    orientation stages of the pipeline.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 程序合成器的下一个阶段要求LLM生成声明场景中所有对象的Python代码。此阶段以第一阶段输出的自然语言描述作为输入。除了为每个对象选择最合适的构造函数（即Object、objects或unique_objects）以及这些构造函数的相关参数（描述、尺寸和朝向信息），它还生成对对象的“类别标签”的估计（例如，对于“一个光滑的深色木餐桌”，类别标签可能是“table”）。这个类别标签将在后续的对象检索和方向阶段中使用。
- en: The prompt template for this stage of the program synthesizer also includes
    two in-context examples to help the LLM produce code in the correct output format.
    These in-context examples show object declaration code for the same two scenes
    described in the first stage’s in-context examples (the artist’s apartment and
    the dining room).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本阶段的程序合成器提示模板还包括两个上下文示例，以帮助 LLM 生成正确格式的代码。这些上下文示例展示了与第一个阶段上下文示例中描述的相同两个场景的对象声明代码（艺术家的公寓和餐厅）。
- en: Achieving target object density
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现目标对象密度
- en: Specifying the target occupied room floor area in the input ishelpful for encouraging
    the LLM to generate the right amount of objects, but it does not guarantee that
    the LLM will produce output that satisfies this target. Thus, this stage includes
    logic that checks if the target occupied area has been achieved by the sizes of
    the generated object declarations; if not, it invokes another LLM to generate
    more objects (both their natural language descriptions and their object declaration
    code). This step is iteratively repeated until the actual percentage of occupied
    room floor area meets the set target object density.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入中指定目标占用房间地板面积有助于鼓励 LLM 生成正确数量的对象，但这并不能保证 LLM 产生的输出满足这个目标。因此，本阶段包括逻辑检查生成的对象声明的大小是否达到了目标占用面积；如果没有，它会调用另一个
    LLM 生成更多对象（包括它们的自然语言描述和对象声明代码）。此步骤会反复迭代，直到实际的占用房间地板面积百分比满足设定的目标对象密度。
- en: 5.3\. Specifying Object Relations
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 指定对象关系
- en: The final stage of the program synthesis pipeline receives all previous pipeline
    outputs and completes the scene description program by generating code describing
    object relations. This task boils down to translating the free-text descriptions
    of object arrangement in the Stage 1 output into calls to appropriate relation
    functions in our DSL (which refer to the appropriate objects declared by Stage
    2).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 程序合成流水线的最终阶段接收所有前面阶段的输出，并通过生成描述对象关系的代码来完成场景描述程序。这项任务归结为将第 1 阶段输出中的自由文本描述的对象排列转换为我们
    DSL 中调用适当的关系函数（这些函数指的是第 2 阶段声明的适当对象）的调用。
- en: The prompt template for this stage also includes two in-context examples. These
    in-context examples contain relation specification code based on the same two
    scenes that the previous two stages used as in-context examples. We designed these
    in-context examples to demonstrate certain potentially non-obvious ways to use
    relations to achieve layout goals (e.g. using two next_to_wall relations to place
    an object in a corner).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本阶段的提示模板还包括两个上下文示例。这些上下文示例包含基于前两个阶段使用的相同两个场景的关系规范代码。我们设计这些上下文示例是为了展示使用关系实现布局目标的一些可能不明显的方法（例如，使用两个
    next_to_wall 关系将一个物体放置在角落里）。
- en: 6\. Scene Layout Optimization
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 场景布局优化
- en: In this section, we describe how a scene program is converted into an object
    layout, i.e. a list of objects with locations and orientations. This process consists
    of two stages. First, the Python interpreter converts the program into a geometric
    constraint satisfaction problem, where variables are object positions and object
    directions and constraints are derived from relation functions. Second, the constraint
    problem is solved using an algorithm based on gradient descent. Because LLMs are
    not perfect programmers, scene programs can contain errors. In the last part of
    this section, we describe how the system handles different types of errors.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了如何将场景程序转换为对象布局，即包含位置和方向的对象列表。这个过程包括两个阶段。首先，Python 解释器将程序转换为几何约束满足问题，其中变量是对象位置和对象方向，约束来自关系函数。其次，使用基于梯度下降的算法解决约束问题。由于
    LLM 并非完美的程序员，场景程序可能包含错误。在本节的最后部分，我们描述了系统如何处理不同类型的错误。
- en: 6.1\. Converting Scene Programs into Constraint Problems
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 将场景程序转换为约束问题
- en: First, the Python scene program is executed with a Python interpreter. As described
    in Section [4](#S4 "4\. Describing Scenes with Programs ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases"),
    the new functions in the scene DSL include object constructors and relation functions.
    Standard object constructors define variables of the constraint problem. Relation
    functions define constraints of the constraint problem. Door and window constructors
    define both variables and constraints.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Python 场景程序通过 Python 解释器执行。如第 [4](#S4 "4\. Describing Scenes with Programs
    ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated
    Object Databases") 节所述，场景 DSL 中的新函数包括对象构造函数和关系函数。标准对象构造函数定义约束问题的变量。关系函数定义约束问题的约束。门和窗户构造函数定义变量和约束。
- en: 'While we designed the scene DSL to be most convenient for LLMs to use, we designed
    the constraint set to be most simple mathematically. Each relation function within
    the DSL is translated into one or more of ten defined constraints: ON, NEXTTOWALL,
    HEIGHT, ADJACENT0, ADJACENT1, ADJACENT2, CEILING, ABOVE, ALIGNED and FACING. This
    translation is mostly straightforward. For example, mounted_on_wall(a, wall, height_above_ground,
    above) is translated into'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们设计了场景 DSL 以便 LLM 使用时最方便，但我们将约束集设计得在数学上最简单。DSL 中的每个关系函数都被转换为十个定义约束之一或多个：ON、NEXTTOWALL、HEIGHT、ADJACENT0、ADJACENT1、ADJACENT2、CEILING、ABOVE、ALIGNED
    和 FACING。这种转换大多是直接的。例如，`mounted_on_wall(a, wall, height_above_ground, above)`
    被转换为
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The non-obvious cases are relation functions that take list arguments: aligned
    and surround. Relation function aligned(list, direction) is desugared into a list
    of len(list)-1 constraints:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 非显而易见的情况是接受列表参数的关系函数：aligned 和 surround。关系函数 `aligned(list, direction)` 被转换为一个包含
    len(list)-1 个约束的列表：
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Relation functions surround(objects, centerobj) are handled last. Surrounding
    objects in the objects list are processed one-by-one. Each object is constrained
    to be adjacent to the centerobj from the side of the centerobj that has the most
    free space available, and facing the centerobj. This process respects other adjacencies
    and walls.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关系函数 `surround(objects, centerobj)` 最后处理。将对象列表中的对象逐一处理。每个对象被约束为与 `centerobj`
    相邻，且从 `centerobj` 那一侧有最大空闲空间，并且面向 `centerobj`。此过程会考虑其他相邻关系和墙壁。
- en: 'In addition to the relational constraints, we also add default constraints:
    WITHINBOUNDS and NOOVERLAP. For every object a we add WITHINBOUNDS(a) to ensure
    that the object stays within the bounds of the room. For every unordered pair
    of distinct objects a, b we add NOOVERLAP(a, b, distance_x, distance_z). Here
    distance_x and distance_z are zeros if neither of objects a, b is a door or a
    window, and some nonzero parameter if a or b is door or a window. The meaning
    of these distance arguments is to create ’auras’ for doors and windows that no
    other objects can overlap. This is needed to ensure that doors can be opened,
    and windows are not obstructed by furniture. There is one exception to this rule:
    we allow wide flat objects such as rugs to overlap with anything, to support the
    case (common in real furniture arrangements) where a part of a furniture object
    stands on a rug.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关系约束，我们还添加了默认约束：WITHINBOUNDS 和 NOOVERLAP。对于每个对象 a，我们添加 `WITHINBOUNDS(a)` 以确保该对象保持在房间的边界内。对于每对不同的对象
    a 和 b，我们添加 `NOOVERLAP(a, b, distance_x, distance_z)`。如果 a 和 b 都不是门或窗户，则 `distance_x`
    和 `distance_z` 为零；如果 a 或 b 是门或窗户，则为非零参数。这些距离参数的意义在于为门和窗户创建“气场”，以防其他对象重叠。这是为了确保门可以打开，窗户不被家具遮挡。此规则有一个例外：我们允许宽大的平面物体如地毯与任何物体重叠，以支持家具物体的一部分放在地毯上的情况（这种情况在真实家具安排中很常见）。
- en: 6.2\. Solving the Constraint Problem
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 解决约束问题
- en: The purpose of the layout optimizer is to find a vector of object positions
    and object directions that satisfies all the constraints. Given the geometric
    nature of our constraints, it is natural to formulate our constraint solving problem
    as an optimization of a (mostly) differentiable function. The only non-differentiable
    constraint that we have is a FACING constraint (since orientations are restricted
    to the four cardinal directions); we address this constraint separately.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 布局优化器的目的是找到一个对象位置和对象方向的向量，以满足所有约束。鉴于我们约束的几何特性，将约束求解问题表述为（大多数情况下）可微函数的优化是自然的。我们唯一的非可微约束是
    FACING 约束（因为方向被限制在四个基本方向）；我们单独处理这个约束。
- en: We design differentiable loss functions for each constraint (excluding FACING),
    such that a constraint is satisfied if and only if its loss is zero. For example,
    HEIGHT(a, height) is the squared difference between object’s a bottom vertical
    coordinate and the height parameter, and WITHINBOUNDS(a) is the sum of squares
    of object’s a linear extensions beyond the scene cuboid. We refer the reader to
    Appendix [B](#A2 "Appendix B Layout Optimizer Details ‣ Open-Universe Indoor Scene
    Generation using LLM Program Synthesis and Uncurated Object Databases") for the
    full list of constraint losses. Our system finds a solution to a constraint problem
    by initializing objects in random positions and minimizing the sum of constraint
    losses with gradient descent. Since the initial configuration is random, different
    runs of optimizer can produce different layouts (see Figure [10](#S9.F10 "Figure
    10 ‣ 9.1\. Qualitative Results ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases")).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个约束（不包括FACING）设计了可微损失函数，使得当且仅当约束的损失为零时，约束才被满足。例如，HEIGHT(a, height)是对象a底部垂直坐标与高度参数之间的平方差，而WITHINBOUNDS(a)是对象a超出场景立方体的线性扩展的平方和。我们将完整的约束损失列表提供给读者参考附录[B](#A2
    "Appendix B Layout Optimizer Details ‣ Open-Universe Indoor Scene Generation using
    LLM Program Synthesis and Uncurated Object Databases")。我们的系统通过将对象初始化在随机位置并使用梯度下降最小化约束损失之和来找到约束问题的解决方案。由于初始配置是随机的，因此优化器的不同运行可能会产生不同的布局（参见图[10](#S9.F10
    "Figure 10 ‣ 9.1\. Qualitative Results ‣ 9\. Results and Evaluation ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases")）。
- en: Custom gradients for non-overlap constraints
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非重叠约束的自定义梯度
- en: For constraints of the form NOOVERLAP(a,b), the natural loss formulation is
    a measure of the overlap of the bounding boxes of a and b, and this is indeed
    what we use. However, such functions are flat if one cuboid is inside the other
    along each axis (for example, if two cuboids overlap in a shape similar to the
    $+$ sign, as in the inset figure).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NOOVERLAP(a,b)形式的约束，自然的损失公式是a和b的边界框重叠的度量，这正是我们使用的。然而，如果一个立方体在每个轴上都在另一个立方体内部（例如，如果两个立方体的重叠形状类似于$+$符号，如插图所示），这样的函数是平坦的。
- en: '![[Uncaptioned image]](img/776e1ffa6c48a01e44f07c3496f500ab.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图像]](img/776e1ffa6c48a01e44f07c3496f500ab.png)'
- en: Hence, the gradient can be zero when the loss is not zero, and gradient descent
    fails to minimize the loss. To solve this issue, we define the gradient of the
    NOOVERLAP(a,b) constraint to be proportional to the vector connecting the centroids
    of a and b, where the magnitude of this vector is equal to the minimum side length
    of the cuboidal overlap region between a and b.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当损失不为零时，梯度可以为零，从而导致梯度下降无法最小化损失。为了解决这个问题，我们将NOOVERLAP(a,b)约束的梯度定义为与a和b的质心连接向量成正比，其中该向量的大小等于a和b之间立方体重叠区域的最小边长。
- en: Repel forces
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 排斥力
- en: 'Most scene description programs in our language are underspecifications of
    scenes: there exist many object layouts which can satisfy the specified constraints.
    Are there any priors we can use to inform whether any of these layouts are better
    than others? One reasonable assumption is that since object adjacencies have such
    a strong perceptual impact on the scene (causing objects to be perceived as part
    of some larger group), the optimized layout should not have adjacencies that are
    not explicitly specified in the program. We realize this assumption using repel
    forces, similar to repels in force-directed graph drawing (Battista et al., [1998](#bib.bib6))
    (implementation details in Appendix [B](#A2 "Appendix B Layout Optimizer Details
    ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated
    Object Databases")). Repel forces make the distribution of objects in a scene
    more balanced but do not push away objects that should be together. This makes
    a notable difference in the plausibility of optimized scene layouts; see Figure
    [5](#S6.F5 "Figure 5 ‣ Repel forces ‣ 6.2\. Solving the Constraint Problem ‣ 6\.
    Scene Layout Optimization ‣ Open-Universe Indoor Scene Generation using LLM Program
    Synthesis and Uncurated Object Databases"). However, gradient descent with repel
    vectors added to the gradient does not necessarily converge to a solution of the
    original constraint problem. We describe the solution to this issue in the next
    paragraph.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们语言中的大多数场景描述程序都是场景的欠规范化：存在许多对象布局可以满足指定的约束。我们是否可以使用任何先验知识来判断这些布局是否优于其他布局？一个合理的假设是，由于对象邻接在场景中具有如此强的感知影响（使得对象被视为某个更大组的一部分），优化后的布局不应有程序中未明确指定的邻接关系。我们通过使用排斥力来实现这一假设，类似于力导向图绘制中的排斥力（Battista
    et al., [1998](#bib.bib6)）（实现细节见附录 [B](#A2 "附录 B 布局优化器细节 ‣ 开放宇宙室内场景生成使用LLM程序合成和未整理的对象数据库")）。排斥力使得场景中的对象分布更为均衡，但不会推开应该在一起的对象。这在优化的场景布局的合理性上有显著差异；参见图
    [5](#S6.F5 "图 5 ‣ 排斥力 ‣ 6.2\. 解决约束问题 ‣ 6\. 场景布局优化 ‣ 开放宇宙室内场景生成使用LLM程序合成和未整理的对象数据库")。然而，梯度下降中加入排斥向量并不一定会收敛到原始约束问题的解。我们在下一段中描述了这个问题的解决方案。
- en: '| Without Repel Forces | With Repel Forces |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 无排斥力 | 有排斥力 |'
- en: '| --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ![Refer to caption](img/b6ce4cc4fe55cff94885b394dff851d3.png) | ![Refer to
    caption](img/5832d85ce711af9914c9c3e85a99e0f6.png) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/b6ce4cc4fe55cff94885b394dff851d3.png) | ![参见说明](img/5832d85ce711af9914c9c3e85a99e0f6.png)
    |'
- en: Figure 5\. Adding repel forces to layout optimization allows objects to be appropriately
    spaced without exhaustively specifying explicit relations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 在布局优化中加入排斥力可以使对象适当地分隔，而无需详尽地指定明确的关系。
- en: Determine object orientations
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 确定对象方向
- en: 'In the scene description language, an object can be specified to face either
    one of four cardinal directions or another object. If an object faces some cardinal
    direction, its direction is known up-front and does not change. Situations where
    one object faces another (for example, where a sofa faces a TV) are trickier.
    To know the final direction, the system must know final object positions. However,
    we cannot set directions after position optimization: if we rotate an object,
    its bounding box would change, which could make the object overlap with other
    objects or violate other constraints. Thus, object directions should be optimized
    jointly with object positions. This complicates our gradient descent scheme, because
    direction is a discrete variable.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在场景描述语言中，一个对象可以指定面向四个基本方向之一或另一个对象。如果一个对象面向某个基本方向，那么它的方向是预先确定的且不会改变。一个对象面向另一个对象（例如，沙发面向电视）的情况则更为复杂。要知道最终的方向，系统必须知道最终的对象位置。然而，我们不能在位置优化后设置方向：如果我们旋转一个对象，其边界框会发生变化，这可能会导致对象与其他对象重叠或违反其他约束。因此，对象方向应与对象位置共同优化。这使得我们的梯度下降方案变得复杂，因为方向是一个离散变量。
- en: 'We developed a simple, multi-stage optimization approach that solves this problem
    and also gets around the convergence issue with repel forces mentioned at the
    end of the previous paragraph:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一种简单的多阶段优化方法，它解决了这个问题，并且绕过了前段提到的排斥力的收敛问题：
- en: (1)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Randomly initialize object positions and non-fixed object directions
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机初始化对象位置和非固定对象方向
- en: (2)
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Perform gradient descent with repel forces added to the gradient
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在梯度中加入排斥力后执行梯度下降
- en: (3)
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Set non-fixed objects directions according to the current object positions
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据当前物体位置设置非固定物体的方向
- en: (4)
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （4）
- en: Perform gradient descent again without repel forces
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次执行梯度下降，无需排斥力
- en: Object positions do change in the second descent phase, so in the end, some
    object directions may not agree with object positions. However, the second descent
    phase usually moves objects only slightly, and the mismatch between object positions
    and directions happens very rarely in practice.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次下降阶段，物体位置确实会发生变化，因此最终一些物体方向可能与物体位置不一致。然而，第二次下降阶段通常只是轻微移动物体，实际中物体位置和方向之间的这种不匹配非常少见。
- en: 6.3\. Error Correction
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 错误修正
- en: As mentioned in Section [5](#S5 "5\. Generating Scene Programs ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases"),
    LLMs tasked with synthesizing scene description programs can sometimes produce
    code with errors. Our decomposition of the program synthesis task into stages
    helps reduce these errors (as we show in Section [9](#S9 "9\. Results and Evaluation
    ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated
    Object Databases")), but it does not completely eliminate them. Since these errors
    typically affect only a small part of an otherwise-valid program, the layout optimizer
    includes mechanisms for automatically fixing some of these errors to avoid throwing
    out the entire LLM-generated program.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[5](#S5 "5\. 生成场景程序 ‣ 使用 LLM 程序合成和未经整理的物体数据库进行开放宇宙室内场景生成")节中所述，负责合成场景描述程序的
    LLM 有时会生成带有错误的代码。我们将程序合成任务分解为多个阶段有助于减少这些错误（如第[9](#S9 "9\. 结果和评估 ‣ 使用 LLM 程序合成和未经整理的物体数据库进行开放宇宙室内场景生成")节所示），但并不能完全消除它们。由于这些错误通常只影响程序的一小部分，布局优化器包括自动修复一些错误的机制，以避免丢弃整个
    LLM 生成的程序。
- en: 'We taxonomize the types of errors the program synthesizer makes into four classes:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将程序合成器产生的错误类型分为四类：
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hallucination: calling functions which are not in our language or referring
    to objects which do not exist in the scene (e.g. below(footrest, desk), or on(lamp,
    table) when lamp does not exist)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 幻觉：调用我们语言中不存在的函数或引用场景中不存在的物体（例如，below(footrest, desk) 或 on(lamp, table) 当灯不存在时）
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Misuse: incorrectly using a function in the language (e.g. incorrect argument
    type, missing arguments)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 误用：在语言中不正确地使用函数（例如，参数类型不正确，缺少参数）
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Contradiction: creating relations which are provably in direct conflict with
    one another (e.g. next_to_wall(statue, NORTH) and next_to_wall(statue, SOUTH))'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 矛盾：创建相互明显冲突的关系（例如，next_to_wall(statue, NORTH) 和 next_to_wall(statue, SOUTH)）
- en: •
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unsatisfiability: creating a set of relations which do not have any obvious
    conflicts but which cannot be jointly satisfied (i.e. the layout optimizer converges
    to non-zero loss).'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不可满足性：创建一组没有明显冲突但无法共同满足的关系（即布局优化器收敛到非零损失）。
- en: The layout optimizer catches the first two types of errors by running the Python
    interpreter, catching any raised exceptions, deleting the line which caused the
    exception, and trying to execute the program again. We classify an exception as
    a hallucination if it contains the string ‘‘is not defined’’ and as a misuse otherwise.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 布局优化器通过运行 Python 解释器来捕获前两类错误，捕捉任何引发的异常，删除导致异常的行，并尝试再次执行程序。如果异常包含字符串“is not defined”，我们将其归类为幻觉（hallucination），否则则归类为误用（misuse）。
- en: 'After the program is successfully executed, the layout optimizer identifies
    contradiction-type errors by searching for the following patterns (identifiable
    as subgraphs in the overall constraint graph):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 程序成功执行后，布局优化器通过搜索以下模式来识别矛盾类型的错误（可在整体约束图中识别为子图）：
- en: (1)
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （1）
- en: An object is adjacent to, stands on top of, or faces itself.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个物体与自身相邻、在其上方或面对自身。
- en: (2)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （2）
- en: An object is next to two opposing walls and is not large enough to span the
    room dimension.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个物体靠近两个相对的墙面，并且不够大，无法跨越房间的尺寸。
- en: (3)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （3）
- en: Object a is adjacent to object b from direction d, and also b is adjacent to
    a from any direction other than the opposite of d.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物体 a 从方向 d 相邻于物体 b，并且 b 从除 d 相反方向之外的任何方向与 a 相邻。
- en: (4)
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （4）
- en: Object a stands on top of object b, and also b is (horizontally) adjacent to
    a.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物体 a 站在物体 b 上方，并且 b 在（水平）上与 a 相邻。
- en: (5)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （5）
- en: Object a is next to wall d, but some other object is adjacent to a from direction
    d.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物体 a 靠近墙 d，但另一个物体从方向 d 相邻于 a。
- en: (6)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （6）
- en: The total length of objects adjacent to a from some direction is more than the
    corresponding linear size of a.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从某个方向邻近的对象的总长度大于相应的线性尺寸。
- en: The layout optimizer handles detected contradictory subgraphs by deleting the
    constraint in the subgraph which is declared last in the scene program.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 布局优化器通过删除场景程序中最后声明的子图中的约束来处理检测到的矛盾子图。
- en: If the system does not reach a near zero loss after 10 rounds of layout optimization
    from random initial conditions, we consider the constraint problem to be unsatisfiable
    (the fourth type of error). In this case, the system backtracks to the third stage
    of the program synthesizer and re-generates the relations part of the scene description
    program.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统在 10 轮随机初始条件下的布局优化后没有达到接近零的损失，我们认为约束问题不可满足（第四种错误类型）。在这种情况下，系统会回溯到程序合成器的第三阶段，并重新生成场景描述程序的关系部分。
- en: 7\. Retrieving 3D Objects
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 检索 3D 对象
- en: '![Refer to caption](img/72e9b6a8080b1bd687207028ae8041af.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/72e9b6a8080b1bd687207028ae8041af.png)'
- en: Figure 6\. Our pipeline for open-universe 3D object retrieval. As a preprocess,
    we compute embeddings for each object in our 3D mesh database using a vision language
    model (VLM). Given a description and category of an object (both specified in
    the LLM-generated scene program), our system finds the $k$ nearest neighbors of
    the text description’s VLM embedding in our database. These initial retrieval
    results are then re-ranked to prioritize objects with the correct category and
    further filtered to remove meshes which are the wrong category or which consist
    of multiple objects.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 我们的开放宇宙 3D 对象检索管道。作为预处理，我们使用视觉语言模型（VLM）计算 3D 网格数据库中每个对象的嵌入。给定对象的描述和类别（都在
    LLM 生成的场景程序中指定），我们的系统在数据库中找到文本描述的 VLM 嵌入的 $k$ 个最近邻。这些初步检索结果随后被重新排序，以优先考虑正确类别的对象，并进一步筛选以移除属于错误类别或包含多个对象的网格。
- en: Given a generated scene description program, the system must retrieve relevant
    3D object meshes for each object declared in the program. In this section, we
    describe our object retrieval pipeline specifically designed for retrieving objects
    from massive, un-annotated 3D asset datasets. Figure [6](#S7.F6 "Figure 6 ‣ 7\.
    Retrieving 3D Objects ‣ Open-Universe Indoor Scene Generation using LLM Program
    Synthesis and Uncurated Object Databases") shows a schematic overview of this
    stage.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个生成的场景描述程序，系统必须为程序中声明的每个对象检索相关的 3D 对象网格。在这一部分，我们描述了专门设计用于从大规模未标注的 3D 资产数据集中检索对象的对象检索管道。图 [6](#S7.F6
    "Figure 6 ‣ 7\. Retrieving 3D Objects ‣ Open-Universe Indoor Scene Generation
    using LLM Program Synthesis and Uncurated Object Databases") 显示了这一阶段的示意图。
- en: 'Given an object’s natural language description $T$ as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个对象的自然语言描述 $T$ 如下：
- en: '| (1) |  | $\mathcal{D}^{T}_{k}=\text{TOP}_{k}(f_{t}(T),\mathcal{D}_{E})$ |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\mathcal{D}^{T}_{k}=\text{TOP}_{k}(f_{t}(T),\mathcal{D}_{E})$ |  |'
- en: where $f_{t}$ are also limited, as we only need to store a single embedding
    vector for each 3D asset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f_{t}$ 也受到限制，因为我们只需要为每个 3D 资产存储一个嵌入向量。
- en: While this retrieval algorithm is a good starting point, we found it to be insufficient
    in practice. Due to the large, unstructured, and noisy nature of massive 3D asset
    datasets, we observe and correct for some frequently-occurring failure cases,
    which we describe below.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个检索算法是一个很好的起点，但我们发现它在实际应用中不够充分。由于大规模 3D 资产数据集的庞大、无结构和噪声性质，我们观察并纠正了一些常见的失败案例，下面将对此进行描述。
- en: 7.1\. Retrieving the Correct Category
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 检索正确的类别
- en: We observe that often the retrieved 3D mesh might be visually be similar to
    the text prompt but functionally be of a different category. As the text prompt
    contains information about both the object category (e.g. chair, table etc.) and
    the object style (e.g. wooden, modern etc.), retrieved objects can sometimes have
    a style matching the text description but a different category. To solve this
    problem, we use the category attribute $C$ based on a category-aware embedding
    distance given by
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到检索到的 3D 网格可能在视觉上与文本提示相似，但在功能上属于不同的类别。由于文本提示包含有关对象类别（例如椅子、桌子等）和对象风格（例如木质、现代等）的信息，因此检索到的对象有时可能具有与文本描述匹配的风格但属于不同的类别。为了解决这个问题，我们使用基于类别感知嵌入距离的类别属性
    $C$，其计算公式为
- en: '| (2) |  | $d(f_{t}(T),E(x))+\lambda d(f_{t}(\hat{C}),E(x))$ |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $d(f_{t}(T),E(x))+\lambda d(f_{t}(\hat{C}),E(x))$ |  |'
- en: where $\hat{C}$).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{C}$）。
- en: For indoor scenes, we have found that retrieving an object of the wrong category
    can often seriously disrupt the plausibility of the scene. Therefore, we augment
    our category aware re-ranking by leveraging a multimodal LLM (GPT-4, [2023](#bib.bib22))
    to select an object of the correct category. Specifically, we provide the LLM
    with an image of an $n\times n$ objects of the correct category.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对于室内场景，我们发现检索错误类别的对象往往会严重破坏场景的可信度。因此，我们通过利用多模态LLM（GPT-4，[2023](#bib.bib22)）来选择正确类别的对象，从而增强我们的类别感知重新排序。具体来说，我们向LLM提供正确类别的$n\times
    n$对象的图像。
- en: 7.2\. Retrieving Only Single Objects
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 仅检索单一对象
- en: 'Another mode of failure occurs when the retrieved mesh contains additional
    secondary objects along with the desired object. Objects such as tables and TV
    stands are often modeled alongside adjacent or supported objects such as chairs
    and TVs, respectively. Comparing VLM text embeddings to embeddings of images of
    such objects often produces high similarities despite the presence of the additional
    secondary objects. To solve this problem, we again employ a multimodal LLM to
    perform multi-object filtering. Similarly to our category filtering step, we provide
    the LLM with a set of object renders and task it with filtering out objects based
    on two criteria: (i) if there are other objects on top of the main object of category
    $C$ valid object retrievals.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种失败模式发生在检索的网格包含了除了所需对象之外的额外次要对象时。像桌子和电视架这样的对象通常与相邻或支撑的对象（如椅子和电视）一起建模。尽管存在额外的次要对象，但将VLM文本嵌入与这些对象图像的嵌入进行比较，通常会产生较高的相似度。为了解决这个问题，我们再次使用多模态LLM进行多对象过滤。与我们的类别过滤步骤类似，我们向LLM提供一组对象渲染图，并要求其根据两个标准进行过滤：（i）是否有其他对象在主要对象$C$上方进行有效的对象检索。
- en: '![Refer to caption](img/5b6cfa8abdd7e9c7a8f25d98bfc69796.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5b6cfa8abdd7e9c7a8f25d98bfc69796.png)'
- en: Figure 7\. Our pipeline for orienting retrieved 3D meshes. The system first
    tries a discrete set of rotations to match the upright orientation of the mesh’s
    bounding box to that of the bounding box specified in the object layout. Then,
    a VLM is used to assess how similar each of four orthogonal views of the mesh
    are to the phrase “the front of a $C$ is the object’s estimated category. The
    two most similar views are then given to a multimodal LLM to decide which is the
    best choice for the front face of the object.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 我们的三维网格定位流程。系统首先尝试一组离散的旋转，以使网格的包围框的直立方向与对象布局中指定的包围框的方向相匹配。然后，使用VLM评估网格的四个正交视图与短语“$C$的前面是对象的估计类别”之间的相似度。最相似的两个视图会被交给多模态LLM，以决定哪个视图最适合作为对象的前面。
- en: 7.3\. Matching the Specified Object Size
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 匹配指定的对象大小
- en: 'Not all 3D meshes which match the object category and description specified
    in the program are good candidates, because they must also reasonably match the
    object *size* specified in the program (e.g. a long, thin mesh will not be a good
    candidate for an object which is specified as being small and squarish). Thus,
    we also filter out meshes whose bounding box aspect ratios are too dissimilar
    from those specified by the object dimensions in the generated program. Given
    the bounding boxes $B^{r}$ of the object as specified in the program, we compute
    the minimal Bounding Box Distortion (BBD) as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有与程序中指定的对象类别和描述匹配的3D网格都是合适的候选对象，因为它们还必须合理匹配程序中指定的对象*大小*（例如，一个长而细的网格将不适合作为一个被指定为小型且方形的对象的候选）。因此，我们还会过滤掉那些包围框长宽比与生成程序中指定的对象尺寸差异过大的网格。给定程序中指定的对象的包围框$B^{r}$，我们按如下方式计算最小包围框失真（BBD）：
- en: '| (3) |  | $\displaystyle BBD(B,B_{p})$ |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle BBD(B,B_{p})$ |  |'
- en: '| (4) |  | $\displaystyle mBBD$ |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle mBBD$ |  |'
- en: where $R$ objects are kept).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对象被保存在$R$的位置）。
- en: 8\. Orienting Retrieved Objects
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 定位检索到的对象
- en: 'The final problem we need to solve for end-to-end text to indoor scene generation
    is correctly orientating the retrieved objects in the scene. Specifically, the
    system must determine where the ‘front’ of an object faces, which can be critical
    for objects such as chairs and bookshelves. Full $SO(3)$ to a six-way classification
    problem: which of the six faces of a mesh’s axis-aligned bounding box corresponds
    to its ‘front.’ In this section, we present a simple, training-free approach for
    solving this problem using pre-trained models. Figure [7](#S7.F7 "Figure 7 ‣ 7.2\.
    Retrieving Only Single Objects ‣ 7\. Retrieving 3D Objects ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases")
    shows an overview of our approach.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要解决的最终问题是正确定位场景中检索到的对象的方向。具体来说，系统必须确定对象的“前面”朝向，这对如椅子和书架等物体尤为重要。完整的$SO(3)$到六分类问题：网格的轴对齐包围盒的六个面中哪个对应其“前面”。在这一部分，我们展示了一种简单的、无需训练的方法，利用预训练模型来解决这个问题。图[7](#S7.F7
    "图 7 ‣ 7.2\. 仅检索单个对象 ‣ 7\. 检索 3D 对象 ‣ 使用 LLM 程序合成和未整理的对象数据库进行开放宇宙室内场景生成")展示了我们的方法概述。
- en: 'Our first goal is to ensure that the object is in upright position. Once the
    object is in upright position, the problem further reduces to detecting which
    of the four vertical faces of the object corresponds to its ‘front’. To this end,
    we use the bounding box distortion (BBD) metric from Equation [3](#S7.E3 "Equation
    3 ‣ 7.3\. Matching the Specified Object Size ‣ 7\. Retrieving 3D Objects ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases").
    Empirically, we find that the majority of 3D meshes are modeled in a y-up coordinate
    system, which our system also uses. Thus, we only rotate the object if doing so
    would significantly improve BDD loss w.r.t. the y-up bounding box specified in
    the scene program. Specifically, we check if either of the rotations $(90,0,0)$
    by a margin of 1.0; if so, we re-orient the mesh. At this point, we may be done:
    if the program synthesizer did not declare the object with a facing argument in
    the generated scene program, then we assume that the object does not have a unique
    front-facing direction and does not need to be further oriented (e.g. round tables).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要目标是确保对象处于直立位置。一旦对象处于直立位置，问题进一步简化为检测对象的四个垂直面中哪个对应其“前面”。为此，我们使用了来自方程[3](#S7.E3
    "方程 3 ‣ 7.3\. 匹配指定对象大小 ‣ 7\. 检索 3D 对象 ‣ 使用 LLM 程序合成和未整理的对象数据库进行开放宇宙室内场景生成")的包围盒失真
    (BBD) 指标。经验上，我们发现大多数 3D 网格都以 y 向上的坐标系建模，我们的系统也采用了这一坐标系。因此，我们仅在旋转能够显著改善相对于场景程序中指定的
    y 向上包围盒的 BBD 损失时才进行旋转。具体来说，我们检查旋转 $(90,0,0)$ 的 BBD 损失是否超出 1.0；如果是，我们重新调整网格。此时，我们可能已经完成了：如果程序合成器在生成的场景程序中没有声明对象具有朝向参数，则我们假设对象没有唯一的前向方向，不需要进一步定向（例如圆桌）。
- en: 'If the object does have the facing argument, we next convert the four-way classification
    problem into a two-way classification problem. For objects with a non-square footprint,
    two of the four options can be directly rejected based on the BBD metric: for
    example, for rectangular couches, the side faces can be rejected as mapping them
    to the ‘front’ leads to high BBD loss. Specifically, if the BBDs for the rotations
    $(0,0,0)$?’.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对象确实具有朝向参数，我们接下来将四分类问题转换为二分类问题。对于脚印不是正方形的物体，基于 BBD 指标可以直接排除四个选项中的两个：例如，对于长方形沙发，侧面可以被排除，因为将它们映射到“前面”会导致高
    BBD 损失。具体来说，如果 BBD 在旋转 $(0,0,0)$ 时的损失高于某个阈值，我们会进行调整。
- en: In Section [9](#S9 "9\. Results and Evaluation ‣ Open-Universe Indoor Scene
    Generation using LLM Program Synthesis and Uncurated Object Databases"), we compare
    this multi-step orientation pipeline with several simpler alternatives (including
    approaches using only a VLM and only a multimodal LLM), showing that it performs
    best.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[9](#S9 "9\. 结果与评估 ‣ 使用 LLM 程序合成和未整理的对象数据库进行开放宇宙室内场景生成")节中，我们将这个多步骤的定向流程与几个更简单的替代方法进行比较（包括仅使用
    VLM 和仅使用多模态 LLM 的方法），结果表明它的表现最佳。
- en: 9\. Results and Evaluation
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 结果与评估
- en: '| “A university dorm” | “A boss’s office” | “A vampire’s room” | “A medieval
    knight’s room” |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| “大学宿舍” | “老板办公室” | “吸血鬼的房间” | “中世纪骑士的房间” |'
- en: '| ![Refer to caption](img/1d75ec40f1f720b90a7be70dbe0881eb.png)  | ![Refer
    to caption](img/7038c96caa9515fc560394cb8e7529bd.png)  | ![Refer to caption](img/4a215a56bc10e4570c5f3afd75a2898d.png)  |
    ![Refer to caption](img/514a60f6c3b99d244a4e14849cbba3df.png)  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| ![请参见说明](img/1d75ec40f1f720b90a7be70dbe0881eb.png)  | ![请参见说明](img/7038c96caa9515fc560394cb8e7529bd.png)  |
    ![请参见说明](img/4a215a56bc10e4570c5f3afd75a2898d.png)  | ![请参见说明](img/514a60f6c3b99d244a4e14849cbba3df.png)  |'
- en: Figure 8\. Our method is capable of synthesizing a wide variety of indoor scenes
    that conform to input text prompts.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 我们的方法能够合成各种符合输入文本提示的室内场景。
- en: In this section, we evaluate our scene synthesis system by using it to generate
    scenes in response to a variety of inputs and by comparing it to other scene generation
    methods, both closed- and open-universe.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过使用我们的场景合成系统来生成各种输入响应的场景，并将其与其他场景生成方法（包括封闭和开放宇宙）进行比较，从而评估系统的效果。
- en: Implementation details
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节
- en: We use GPT4 and GPT4V for all language generation and visual question answering
    tasks throughout the system (GPT-4, [2023](#bib.bib22)). For joint text-image
    embedding, we use the SigLIP vision-language model (Zhai et al., [2023b](#bib.bib75)).
    For object retrieval, we use the Objaverse dataset (Deitke et al., [2022a](#bib.bib12))
    as well as multi-view renderings of Objaverse objects provided by the ULIP dataset (Xue
    et al., [2023](#bib.bib68)).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在系统中使用 GPT4 和 GPT4V 进行所有语言生成和视觉问答任务（GPT-4，[2023](#bib.bib22)）。对于联合文本-图像嵌入，我们使用
    SigLIP 视觉-语言模型（Zhai 等，[2023b](#bib.bib75)）。对于对象检索，我们使用 Objaverse 数据集（Deitke 等，[2022a](#bib.bib12)）以及
    ULIP 数据集（Xue 等，[2023](#bib.bib68)）提供的 Objaverse 对象的多视角渲染。
- en: 9.1\. Qualitative Results
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 质性结果
- en: Figs. [1](#S0.F1 "Figure 1 ‣ Open-Universe Indoor Scene Generation using LLM
    Program Synthesis and Uncurated Object Databases") and [8](#S9.F8 "Figure 8 ‣
    9\. Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM Program
    Synthesis and Uncurated Object Databases") show scenes generated by our system
    in response to different text prompts. Our system is able to interpret prompts
    describing typical indoor scenes (e.g. “a bedroom”), rooms for specific purposes
    (e.g. “a dining room for one”), different styles of interiors (e.g. “an old-fashioned
    bedroom”), and non-residential indoor spaces (“a high-end mini restaurant”). It
    can also imagine scenes that don’t exist in the real world (e.g. “a vampire’s
    room” with a coffin and a collection of occult tomes; “a medieval knight’s room”).
    Our system also supports user specification of the desired room size and object
    density/fullness; in Fig. [9](#S9.F9 "Figure 9 ‣ 9.1\. Qualitative Results ‣ 9\.
    Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM Program
    Synthesis and Uncurated Object Databases"), we show examples of how the system
    responds to these optional inputs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S0.F1 "图 1 ‣ 使用 LLM 程序合成和未整理对象数据库的开放宇宙室内场景生成") 和 [8](#S9.F8 "图 8 ‣ 9\.
    结果与评估 ‣ 使用 LLM 程序合成和未整理对象数据库的开放宇宙室内场景生成") 显示了我们的系统响应不同文本提示生成的场景。我们的系统能够解释描述典型室内场景的提示（例如，“一间卧室”）、特定用途的房间（例如，“一个人用的餐厅”）、不同风格的室内（例如，“老式卧室”）以及非住宅室内空间（例如，“一个高档迷你餐厅”）。它还可以想象现实中不存在的场景（例如，“一个吸血鬼的房间”，里面有一具棺材和一系列神秘书籍；“一个中世纪骑士的房间”）。我们的系统还支持用户指定期望的房间大小和对象密度/充实度；在图
    [9](#S9.F9 "图 9 ‣ 9.1\. 质性结果 ‣ 9\. 结果与评估 ‣ 使用 LLM 程序合成和未整理对象数据库的开放宇宙室内场景生成") 中，我们展示了系统如何响应这些可选输入的示例。
- en: '![Refer to caption](img/5885456fbaeaf2bb224a6376ba130344.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/5885456fbaeaf2bb224a6376ba130344.png)'
- en: Figure 9\. When the user specifies the desired room size or target object density
    as part of the input, our system appropriately adjusts its output. *Top:* controlling
    room size; *Bottom:* controlling room fullness.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 当用户将期望的房间大小或目标对象密度作为输入的一部分时，我们的系统会适当调整其输出。*上图：* 控制房间大小；*下图：* 控制房间充实度。
- en: Because we break the scene synthesis problem into declarative program synthesis
    followed by layout optimization, it is possible to optimize multiple layouts for
    the same program, producing multiple design variations. Fig. [10](#S9.F10 "Figure
    10 ‣ 9.1\. Qualitative Results ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases")
    shows an example of this process. Objects placed along walls (such as paintings
    and desks) are free to slide along those walls, in some cases exchanging positions
    (e.g. the desk can appear on both sides of the door).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将场景合成问题分解为声明式程序合成和布局优化，所以可以为同一个程序优化多个布局，产生多个设计变体。图[10](#S9.F10 "Figure 10
    ‣ 9.1\. Qualitative Results ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases")展示了这一过程的一个例子。沿墙放置的物体（如画作和桌子）可以沿墙滑动，在某些情况下可以交换位置（例如，桌子可以出现在门的两侧）。
- en: '| ![Refer to caption](img/e86dd30486d4c2bef707b2601a6b81e3.png) | ![Refer to
    caption](img/65bf88d672f7652f80725fe9678b05bc.png) | ![Refer to caption](img/1e49b88f64a9eab5caf954cc086b98eb.png)
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/e86dd30486d4c2bef707b2601a6b81e3.png) | ![参见说明](img/65bf88d672f7652f80725fe9678b05bc.png)
    | ![参见说明](img/1e49b88f64a9eab5caf954cc086b98eb.png) |'
- en: Figure 10\. Given a single scene description program, we can run the layout
    optimizer multiple times to produce stochastic variations on the same scene.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. 给定一个单一的场景描述程序，我们可以多次运行布局优化器，以产生同一场景的随机变化。
- en: 9.2\. Closed-Universe Scene Synthesis Comparison
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2\. 封闭宇宙场景合成比较
- en: 'Here we evaluate how well our system performs on a closed-universe scene generation
    task when compared to prior methods for this problem which learn from 3D scene
    data. Specifically, we compare against the following methods:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们评估了我们的系统在封闭宇宙场景生成任务中的表现，与之前从3D场景数据中学习的方法相比。具体来说，我们与以下方法进行了比较：
- en: •
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ATISS (Paschalidou et al., [2021](#bib.bib44)): a recent autoregressive Transformer-based
    generative model of indoor scenes.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ATISS （Paschalidou等， [2021](#bib.bib44)）：一种基于自回归Transformer的室内场景生成模型。
- en: •
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DiffuScene (Tang et al., [2023](#bib.bib60)): a recent denoising diffusion-based
    generative model of indoor scenes.'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DiffuScene （Tang等， [2023](#bib.bib60)）：一种基于去噪扩散的室内场景生成模型。
- en: We evaluate each of these methods on generating bedrooms, living rooms, and
    dining rooms (three commonly-occurring room types in closed-universe scene generation
    work). We direct our method to generate object layouts of these types by providing
    it with text prompts of the form “A bedroom.”
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了这些方法在生成卧室、客厅和餐厅（封闭宇宙场景生成工作中常见的三种房间类型）方面的表现。我们通过提供“一个卧室”形式的文本提示，指导我们的方法生成这些类型的对象布局。
- en: To compare the object layouts generated by these different methods, we conducted
    a two-alternative forced-choice perceptual study. We recruited 35 participants
    from a population of university students. Each participant was shown a series
    of 45 comparisons, where each comparison contained a room type label (bedroom,
    living room, or dining room), images of two scenes, and a question asking them
    to choose which scene they thought was a more realistic instance of that type
    of room.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较这些不同方法生成的对象布局，我们进行了一项两选一的强制选择感知研究。我们从大学生群体中招募了35名参与者。每位参与者观看了一系列45个比较，每个比较包含一个房间类型标签（卧室、客厅或餐厅）、两个场景的图像，以及一个问题，要求他们选择哪个场景更符合该房间类型的实际情况。
- en: Table [1](#S9.T1 "Table 1 ‣ 9.2\. Closed-Universe Scene Synthesis Comparison
    ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM
    Program Synthesis and Uncurated Object Databases") shows the results of this study,
    and Fig. [11](#S9.F11 "Figure 11 ‣ 9.2\. Closed-Universe Scene Synthesis Comparison
    ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM
    Program Synthesis and Uncurated Object Databases") shows some of the object layouts
    generated by each method. Participants vastly preferred the object layouts produced
    by our method compared with ATISS (79% overall preference rate) and DiffuScene
    (81% overall preference rate). While we find this trend is consistent across the
    three room types we used, the gap between our approach and these alternatives
    is most pronounced for dining rooms, where the objects layout we produced were
    preferred at rates of 86% and 89% over ATISS and DiffuScene respectively. As seen
    in the last column of Fig. [11](#S9.F11 "Figure 11 ‣ 9.2\. Closed-Universe Scene
    Synthesis Comparison ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor Scene
    Generation using LLM Program Synthesis and Uncurated Object Databases"), our method
    captures relations important to scene fidelity (e.g. surrounding a dining table
    with chairs), all the while avoiding object overlaps and clutter that mar the
    scenes produced by the other two approaches.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S9.T1 "表 1 ‣ 9.2\. 封闭宇宙场景合成比较 ‣ 9\. 结果与评估 ‣ 使用LLM程序合成和未经整理的对象数据库进行的开放宇宙室内场景生成")
    显示了这项研究的结果，图 [11](#S9.F11 "图 11 ‣ 9.2\. 封闭宇宙场景合成比较 ‣ 9\. 结果与评估 ‣ 使用LLM程序合成和未经整理的对象数据库进行的开放宇宙室内场景生成")
    显示了每种方法生成的一些对象布局。参与者普遍更喜欢我们的方法生成的对象布局，相比于 ATISS（总体偏好率 79%）和 DiffuScene（总体偏好率 81%）。虽然我们发现这种趋势在我们使用的三种房间类型中是一致的，但我们的方法与这些替代方案之间的差距在餐厅中最为明显，我们生成的对象布局在
    ATISS 和 DiffuScene 中分别以 86% 和 89% 的比率受到青睐。如图 [11](#S9.F11 "图 11 ‣ 9.2\. 封闭宇宙场景合成比较
    ‣ 9\. 结果与评估 ‣ 使用LLM程序合成和未经整理的对象数据库进行的开放宇宙室内场景生成") 的最后一列所示，我们的方法捕捉了对场景真实性重要的关系（例如，围绕餐桌摆放椅子），同时避免了其他两种方法生成的场景中的对象重叠和杂乱。
- en: '| Ours vs. | Bedroom | Living | Dining | Average |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 我们与 | 卧室 | 客厅 | 餐厅 | 平均 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ATISS | 76% | 74% | 86% | 79% |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ATISS | 76% | 74% | 86% | 79% |'
- en: '| DiffuScene | 75% | 79% | 89% | 81% |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| DiffuScene | 75% | 79% | 89% | 81% |'
- en: Table 1\. Results of a two-alternative forced-choice perceptual study comparing
    scenes generate by our system to those generated by two existing systems for closed-universe
    scene generation. The scenes our method generated were largely preferred over
    those from alternative approaches across typical indoor room types.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 一项两选一强迫选择感知研究的结果，将我们系统生成的场景与两个现有的封闭宇宙场景生成系统生成的场景进行比较。我们的方法生成的场景在典型的室内房间类型中大体上更受欢迎。
- en: '![Refer to caption](img/56b9f41bee07de756e4af75c483b9862.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56b9f41bee07de756e4af75c483b9862.png)'
- en: Figure 11\. Comparing closed-universe scene layouts generated by our system
    to those generated by two existing closed-universe scene generative models (zoom
    in to read object text labels)., Layouts generated by our method are more detailed
    and devoid of object intersection artefacts.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 比较我们系统生成的封闭宇宙场景布局与两种现有封闭宇宙场景生成模型生成的布局（放大以阅读对象文本标签）。我们的方法生成的布局更加详细且没有对象交叉伪影。
- en: 9.3\. Open-Universe Scene Synthesis Comparison
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3\. 开放宇宙场景合成比较
- en: 'We next evaluate our system’s ability to generate open-universe scenes. To
    the best of our knowledge, there is no prior work which solves this exact problem.
    Thus, we compare against the next best thing: an existing method that uses LLMs
    for scene synthesis.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来评估我们系统生成开放宇宙场景的能力。据我们所知，目前没有解决这个精确问题的先前工作。因此，我们与下一个最佳方案进行比较：一种使用 LLM 进行场景合成的现有方法。
- en: Specifically, we compare against LayoutGPT (Feng et al., [2023](#bib.bib16)).
    LayoutGPT was originally only evaluated in the closed-universe setting; we adapt
    it to the open-universe setting by modifying its prompt to remove references to
    fixed sets of room and object types and by providing it the same in-context examples
    that our method sees (converted into LayoutGPT’s scene representation format).
    To convert LayoutGPT’s generated layouts into full 3D scenes, we use the same
    object retrieval and orientation modules as in our system.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将 LayoutGPT（Feng 等人，[2023](#bib.bib16)）进行对比。LayoutGPT 最初仅在封闭宇宙环境下进行评估；我们通过修改其提示以去除对固定房间和对象类型的引用，并提供与我们的方法相同的上下文示例（转换为
    LayoutGPT 的场景表示格式）来将其适配到开放宇宙环境中。为了将 LayoutGPT 生成的布局转换为完整的 3D 场景，我们使用与我们系统相同的对象检索和方向模块。
- en: 'We test how well the two methods can generate scenes in response to a range
    of different types of text prompts, ranging from simple to more complex/subtle:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了这两种方法对不同类型文本提示的场景生成能力，这些提示从简单到更复杂/微妙不等：
- en: (1)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Basic: basic room types such as “a bedroom.”'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基本：基本房间类型，例如“一个卧室”。
- en: (2)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Completion: prompts that describe a subset of a basic scene and ask the system
    to complete it, e.g. “a living room with a sofa, tv, and a coffee table.”'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完成：描述基本场景的子集并要求系统完成它的提示，例如“一个有沙发、电视和咖啡桌的客厅”。
- en: (3)
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Style: basic room types with style descriptors, e.g. “a minimalist living room.”'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 风格：具有风格描述的基本房间类型，例如“一个极简主义的客厅”。
- en: (4)
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Activity: rooms that must accommodate some specific activity, e.g. “a musician’s
    practice room.”'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 活动：必须容纳特定活动的房间，例如“一个音乐家的练习室”。
- en: (5)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: 'Fantastical: fantastical, outlandish, or whimsical rooms that would not exist
    in reality, e.g. “a wizard’s lair.”'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奇幻：幻想、离奇或异想天开的房间，这些房间在现实中不存在，例如“一个巫师的巢穴”。
- en: (6)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (6)
- en: 'Emotion: rooms which should evoke specific emotions, e.g. “A lonely dark jail
    cell.”'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 情感：应唤起特定情感的房间，例如“一个孤独阴暗的监狱牢房”。
- en: We have created 59 prompts across these 6 types; a complete listing of all prompts
    can be found in the supplemental material.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了59个不同类型的提示；所有提示的完整列表可以在补充材料中找到。
- en: '|  | Basic | Completion | Style | Activity | Fantastical | Emotion | Average
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | 基本 | 完成 | 风格 | 活动 | 奇幻 | 情感 | 平均 |'
- en: '| Ours vs. LayoutGPT | 66% | 64% | 76% | 60% | 51% | 66% | 65% |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 vs. LayoutGPT | 66% | 64% | 76% | 60% | 51% | 66% | 65% |'
- en: Table 2\. How often open-universe scenes generated by our method are preferred
    to those generated by LayoutGPT in a two-alternative forced-choice perceptual
    study (higher is better). We report results for the different types of prompts
    in our evaluation set as well as overall results. Our system is preferred over
    LayoutGPT for all prompt types except Fantastical, where there is no clear preference.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 在二选一的感知研究中，我们的方法生成的开放宇宙场景相比 LayoutGPT 生成的场景被更频繁地偏好（数值越高越好）。我们报告了在评估集中的不同类型提示以及整体结果的结果。除奇幻类提示外，我们的方法在所有提示类型中都优于
    LayoutGPT。
- en: '![Refer to caption](img/28f851b32de1b4082fd518d1f8e41993.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/28f851b32de1b4082fd518d1f8e41993.png)'
- en: Figure 12\. Comparing open-universe scenes generated by our system to those
    generated by LayoutGPT and to scenes generated by an ablation of our system using
    a naive object retrieval method. LayoutGPT produces layouts with many overlapping
    objects; the naive retrieval baseline sometimes retrieves unusual and undesirable
    meshes for some objects.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. 比较我们系统生成的开放宇宙场景与 LayoutGPT 生成的场景，以及使用简单对象检索方法的我们系统的消融实验生成的场景。LayoutGPT
    生成的布局有许多重叠的对象；简单检索基线有时会检索到一些异常和不希望出现的网格。
- en: To compare how well the different methods fare on these prompts, we conduct
    a two-alternative forced-choice perceptual study, pitting our method against LayoutGPT.
    We recruited 24 participants from a population of university students. Each participant
    was shown a series of 50 comparisons, where each comparison contains a text prompt,
    images of two scenes, and a question asking them to choose which scene they thought
    was better (taking into account overall scene plausibility and appropriateness
    for the prompt).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较不同方法在这些提示下的表现，我们进行了一项二选一的感知研究，将我们的方法与 LayoutGPT 进行对比。我们从大学生中招募了24名参与者。每位参与者会看到一系列50个比较，其中每个比较包含一个文本提示、两个场景的图像以及一个问题，要求他们选择他们认为更好的场景（考虑到整体场景的可信度和对提示的适宜性）。
- en: Table [2](#S9.T2 "Table 2 ‣ 9.3\. Open-Universe Scene Synthesis Comparison ‣
    9\. Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM Program
    Synthesis and Uncurated Object Databases") shows the results of this experiment.
    In general, participants preferred our method’s scenes over those from LayoutGPT.
    The largest margin between our system and LayoutGPT occurred for prompts in the
    Style category. Since both systems used the same object retrieval method, this
    difference is not attributable to the objects in one condition having more stylistically-appropriate
    appearance; rather, our system does a better job of interpreting which types of
    objects should be in a scene and how they should be arranged to satisfy stylistic
    goals. The Fantastical prompt category, with its unusual prompts, proved to be
    challenging for both methods, with no clear winner emerging.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S9.T2 "表格 2 ‣ 9.3\. 开放宇宙场景合成比较 ‣ 9\. 结果与评价 ‣ 基于LLM程序合成和未整理物体数据库的开放宇宙室内场景生成")
    显示了本实验的结果。总体而言，参与者更喜欢我们方法生成的场景，而非LayoutGPT生成的场景。我们系统与LayoutGPT之间的最大差距发生在风格类别的提示中。由于两个系统使用了相同的物体检索方法，这种差异并非由于某一条件下的物体在风格上更合适；而是我们的系统在解释场景中应包含哪些类型的物体以及如何安排这些物体以满足风格目标方面做得更好。奇幻提示类别，由于其不寻常的提示，对于两种方法都是具有挑战性的，没有明显的赢家。
- en: Fig. [12](#S9.F12 "Figure 12 ‣ 9.3\. Open-Universe Scene Synthesis Comparison
    ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM
    Program Synthesis and Uncurated Object Databases") shows some of the scenes generated
    by each method in this experiment. To demonstrate the value of our object retrieval
    module, we also produce variants of scenes generated by our method where the 3D
    meshes used are retrieved using a naive retrieval method (only the initial VLM-based
    KNN retrieval step from our full pipeline, without re-ranking or filtering). LayoutGPT,
    as it directly generates numerical coordinates for object locations, suffers from
    frequently interpenetrations between objects. Our method avoids these errors by
    construction. Our full retrieval pipeline also helps avoid some erroneous mesh
    retrievals (e.g. for the bookshelf in the second column). The supplemental material
    contains more examples of objects from these scenes where our full retrieval pipeline
    retrieves a mesh of the appropriate category but the naive approach does not.
    Across all the scenes, for every three out of 100 objects, our method retrieved
    a correct-category mesh whereas the naive method did not.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12](#S9.F12 "图 12 ‣ 9.3\. 开放宇宙场景合成比较 ‣ 9\. 结果与评价 ‣ 基于LLM程序合成和未整理物体数据库的开放宇宙室内场景生成")
    显示了本实验中每种方法生成的一些场景。为了展示我们物体检索模块的价值，我们还生成了我们方法生成的场景的变体，其中使用的3D网格通过一种简单的检索方法检索（仅使用我们完整流程中的初始VLM基础KNN检索步骤，不进行重新排序或过滤）。LayoutGPT由于直接生成物体位置的数值坐标，经常出现物体间相互穿透的问题。我们的方法通过构建来避免这些错误。我们的完整检索流程还帮助避免了一些错误的网格检索（例如第二列的书架）。补充材料包含了这些场景中更多的物体示例，其中我们的完整检索流程能够检索到适当类别的网格，而简单的方法则不能。在所有场景中，每100个物体中有三个，我们的方法能够检索到正确类别的网格，而简单方法则不能。
- en: 9.4\. Ablation Studies & Other Evaluations
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4\. 消融研究与其他评估
- en: Here, we discuss several additional experiments we performed to evaluate the
    performance of individual components of our system in isolation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了我们为单独评估系统各个组件的性能所进行的若干额外实验。
- en: '| Synth. & Trans. | Lines$\uparrow$ |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 合成与翻译 | 行数$\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Combined (No stage 1) | 36.2 | 1.16 | 24.28 | 49.71 | 21.97 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 组合（无阶段1） | 36.2 | 1.16 | 24.28 | 49.71 | 21.97 |'
- en: '| Ours (separated) | 49.4 | 9.73 | 16.22 | 31.63 | 14.6 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法（分离） | 49.4 | 9.73 | 16.22 | 31.63 | 14.6 |'
- en: Table 3\. Evaluating how separating synthesis and translation into different
    LLM queries affects the complexity of the generated scene programs (Lines) as
    well as the rates at which the types of errors described in Section [5](#S5 "5\.
    Generating Scene Programs ‣ Open-Universe Indoor Scene Generation using LLM Program
    Synthesis and Uncurated Object Databases") occur (H = Hallucination, M = Misuse,
    C = Contradiction, U = Unsatisfiability). Our full pipeline improves all metrics
    but one (Hallucination).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3\. 评估将合成和翻译分开为不同的LLM查询如何影响生成场景程序的复杂性（行数）以及第 [5](#S5 "5\. 生成场景程序 ‣ 基于LLM程序合成和未整理物体数据库的开放宇宙室内场景生成")
    节中描述的错误类型的发生率（H = 幻觉，M = 滥用，C = 矛盾，U = 不满足）。我们的完整流程改进了所有指标，只有一个（幻觉）未改进。
- en: Scene program synthesis
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 场景程序合成
- en: In Section [5](#S5 "5\. Generating Scene Programs ‣ Open-Universe Indoor Scene
    Generation using LLM Program Synthesis and Uncurated Object Databases"), we discussed
    the benefits of splitting the LLM-based program synthesizer into stages which
    first generate a detailed natural language description of the scene and then translate
    that description into code. Here, we empirically demonstrate those benefits. For
    the prompts from the perceptual study in the previous section, we generate scenes
    using our full program synthesizer and a variant without the natural language
    description stage, i.e. in this variant, the LLM must synthesize the scene and
    translate it to code at the same time. To assess the complexity of the generated
    scene programs, we measure the average number of lines per scene. We also measure
    the frequency at which the four types of errors described in Section [6.3](#S6.SS3
    "6.3\. Error Correction ‣ 6\. Scene Layout Optimization ‣ Open-Universe Indoor
    Scene Generation using LLM Program Synthesis and Uncurated Object Databases")
    occur. Since scenes have a different number of objects & relations (and thus a
    different number of chances to make errors), rather than report the average error
    rate per scene, we instead report the number of errors per 1000 objects.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5节](#S5 "5\. 生成场景程序 ‣ 开放宇宙室内场景生成使用LLM程序合成与未经整理的对象数据库")中，我们讨论了将基于LLM的程序合成器分阶段处理的好处，即首先生成场景的详细自然语言描述，然后将该描述翻译成代码。在这里，我们通过实证方法展示这些好处。对于上一节中感知研究的提示，我们使用完整的程序合成器和一个没有自然语言描述阶段的变体生成场景，即在该变体中，LLM必须同时合成场景并将其翻译成代码。为了评估生成的场景程序的复杂性，我们测量了每个场景的平均行数。我们还测量了在[第6.3节](#S6.SS3
    "6.3\. 错误修正 ‣ 6\. 场景布局优化 ‣ 开放宇宙室内场景生成使用LLM程序合成与未经整理的对象数据库")中描述的四种错误类型发生的频率。由于场景中有不同数量的对象和关系（因此有不同数量的错误机会），我们不报告每个场景的平均错误率，而是报告每1000个对象的错误数量。
- en: 'Table [3](#S9.T3 "Table 3 ‣ 9.4\. Ablation Studies & Other Evaluations ‣ 9\.
    Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM Program
    Synthesis and Uncurated Object Databases") shows the results of this experiment.
    Using our full pipeline results in more complex scene programs and leads to a
    reduction in the rates of all error types except hallucinations. The higher rate
    of hallucinations in our pipeline is not surprising: since the first stage generates
    a free-form natural language description of the scene, the latter stages may “invent”
    new relation functions that correspond to parts of that description. By contrast,
    such errors are less likely to happen when the LLM is instructed to directly generate
    a program with a fixed vocabulary of functions. Nonetheless, the other benefits
    offered by separating synthesis and translation make this trade-off worth it.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[表3](#S9.T3 "表3 ‣ 9.4\. 消融研究及其他评估 ‣ 9\. 结果与评估 ‣ 开放宇宙室内场景生成使用LLM程序合成与未经整理的对象数据库")显示了此实验的结果。使用我们的完整流程会导致更复杂的场景程序，并减少了除幻觉之外的所有错误类型的发生率。我们的流程中幻觉率较高并不意外：由于第一阶段生成了场景的自由形式自然语言描述，后续阶段可能会“发明”出与描述部分相对应的新关系函数。相比之下，当LLM被指示直接生成一个具有固定功能词汇的程序时，这种错误发生的可能性较小。尽管如此，分离合成和翻译所带来的其他好处使得这种权衡是值得的。'
- en: '![Refer to caption](img/f004ab71bbdec55b142c8fcead622147.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f004ab71bbdec55b142c8fcead622147.png)'
- en: Figure 13\. Plotting the retrieval precision (left) and category accuracy (right)
    of different ranking schemes for our open-universe object retrieval module (x
    axis is the number of top $k$ objects considered). Our weighted re-ranking approach
    preserves high category accuracy while incurring only a small hit to precision.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图13\. 绘制了我们开放宇宙对象检索模块不同排名方案的检索精度（左）和类别准确率（右）（x轴为考虑的前$k$个对象数量）。我们加权重排序方法保持了较高的类别准确率，同时仅对精度产生了小幅影响。
- en: Object retrieval
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对象检索
- en: 'We first demonstrate the value of of our category-aware re-ranking scheme for
    object retrieval. We compare this scheme to two alternatives: no re-ranking (naive
    retrieval), and re-ranking purely based on category (i.e. using only the second
    term in Equation [2](#S7.E2 "Equation 2 ‣ 7.1\. Retrieving the Correct Category
    ‣ 7\. Retrieving 3D Objects ‣ Open-Universe Indoor Scene Generation using LLM
    Program Synthesis and Uncurated Object Databases")). We run each of these methods
    on text descriptions from the Cap3D dataset (Luo et al., [2023](#bib.bib39)),
    which contains paired (3D mesh, text description) data with meshes sourced from
    Objaverse. We compute the retrieval Precision@$K$ objects are of the correct category
    while retaining enough information about the overall description of the object
    to suffer only a minor hit to retrieval precision.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先展示了我们类别感知重排序方案在物体检索中的价值。我们将这一方案与两种替代方案进行比较：无重排序（天真检索），以及仅基于类别进行重排序（即仅使用方程
    [2](#S7.E2 "Equation 2 ‣ 7.1\. Retrieving the Correct Category ‣ 7\. Retrieving
    3D Objects ‣ Open-Universe Indoor Scene Generation using LLM Program Synthesis
    and Uncurated Object Databases") 中的第二项）。我们在来自 Cap3D 数据集 (Luo et al., [2023](#bib.bib39))
    的文本描述上运行这些方法，该数据集包含配对的（3D 网格，文本描述）数据，网格来源于 Objaverse。我们计算检索精度@$K$ 对象是否属于正确类别，同时保留足够的对象整体描述信息，以只对检索精度造成轻微影响。
- en: '| Filter | Category | True Positive Rate $\uparrow$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 过滤器 | 类别 | 真正率 $\uparrow$ |'
- en: '| Category | Bookcase | 0.77 | 0.27 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 书架 | 0.77 | 0.27 |'
- en: '| Rug | 0.88 | 0.22 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 地毯 | 0.88 | 0.22 |'
- en: '| Painting | 0.97 | 0.1 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 画作 | 0.97 | 0.1 |'
- en: '| Table | 0.97 | 0.28 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 桌子 | 0.97 | 0.28 |'
- en: '| Average | 0.90 | 0.21 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 0.90 | 0.21 |'
- en: '| Multi-object | Desk | 0.75 | 0.15 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 多物体 | 书桌 | 0.75 | 0.15 |'
- en: '| TV Stand | 0.95 | 0.17 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 电视架 | 0.95 | 0.17 |'
- en: '| Side Table | 0.97 | 0.11 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 边桌 | 0.97 | 0.11 |'
- en: '| Table | 0.92 | 0.2 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 桌子 | 0.92 | 0.2 |'
- en: '| Couch | 0.89 | 0.31 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 沙发 | 0.89 | 0.31 |'
- en: '| Average | 0.86 | 0.2 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 0.86 | 0.2 |'
- en: Table 4\. Performance of our two object retrieval filters when used on a benchmark
    set of of objects with manually-labeled ground truth labels. Our filtration technique
    discards most unsuitable retrievals while retaining a large fraction of suitable
    retrievals.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 我们的两个物体检索过滤器在具有手动标记的基准对象集上的性能。我们的过滤技术丢弃了大多数不合适的检索结果，同时保留了大量合适的检索结果。
- en: We also evaluate the performance of our two retrieval filters (category and
    multi-object). For each filter, we chose a handful of common object categories
    and built benchmark datasets for each by producing a set of two object text descriptions
    and running them through the first part of our retrieval pipeline (initial retrieval
    and re-ranking). For each set of retrieval results, we traverse the top $k$ objects
    and create a set containing 10 meshes which should pass the filter and 5 which
    should not (manually labeled by a human observer). This results in a dataset with
    802 annotated meshes. We then run our filters on all of these sets of meshes three
    times (to account for non-determinism in the LLM) and report their average true
    positive and false positive rates in Table [4](#S9.T4 "Table 4 ‣ Object retrieval
    ‣ 9.4\. Ablation Studies & Other Evaluations ‣ 9\. Results and Evaluation ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases").
    Overall, both the category and multi-object filters achieve around a 90% true
    positive rate and 20% false positive rate. Given the size of our 3D object dataset,
    this true positive rate is more than sufficient to ensure that enough valid candidate
    meshes can be retrieved in almost all cases. This false positive rate means that
    roughly one in five meshes which passes a filter should actually have been rejected—not
    perfect, but a notable improvement over not filtering at all.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还评估了我们的两个检索过滤器（类别和多物体）的性能。对于每个过滤器，我们选择了一些常见的物体类别，并通过生成一组两个物体的文本描述来构建基准数据集，然后将其通过我们检索管道的第一部分（初始检索和重排序）。对于每组检索结果，我们遍历前
    $k$ 个对象，创建一个包含 10 个应该通过过滤器的网格和 5 个应该未通过的网格（由人工观察者手动标记）的数据集。这导致了一个包含 802 个注释网格的数据集。然后我们在所有这些网格集上运行我们的过滤器三次（以考虑
    LLM 的非确定性），并在表 [4](#S9.T4 "Table 4 ‣ Object retrieval ‣ 9.4\. Ablation Studies
    & Other Evaluations ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor Scene
    Generation using LLM Program Synthesis and Uncurated Object Databases") 中报告它们的平均真正率和假正率。总体而言，类别和多物体过滤器的真正率约为
    90%，假正率约为 20%。考虑到我们 3D 物体数据集的规模，这一真正率足以确保在几乎所有情况下都能检索到足够的有效候选网格。这个假正率意味着大约每五个通过过滤器的网格中，就有一个实际上应该被拒绝——虽然不完美，但相比完全不过滤，已是显著改进。
- en: '| Method | Chair | Couch | Desk | Wardrobe | Painting | Average |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 椅子 | 沙发 | 桌子 | 衣柜 | 画作 | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| VLM (front) | 65.3 | 54.0 | 67.3 | 87.3 | 84.3 | 71.6 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| VLM (前) | 65.3 | 54.0 | 67.3 | 87.3 | 84.3 | 71.6 |'
- en: '| VLM (front,back) | 69.3 | 97.0 | 57.4 | 87.3 | 88.2 | 79.8 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| VLM (前、后) | 69.3 | 97.0 | 57.4 | 87.3 | 88.2 | 79.8 |'
- en: '| VLM (front,back,side) | 96.0 | 97.0 | 57.4 | 86.3 | 88.2 | 85.0 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| VLM (前、后、侧) | 96.0 | 97.0 | 57.4 | 86.3 | 88.2 | 85.0 |'
- en: '| LLM | 75.0 | 97.4 | 92.4 | 94.1 | 94.8 | 90.7 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 75.0 | 97.4 | 92.4 | 94.1 | 94.8 | 90.7 |'
- en: '| Ours | 87.8 | 97.0 | 93.7 | 95.8 | 94.8 | 93.8 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 87.8 | 97.0 | 93.7 | 95.8 | 94.8 | 93.8 |'
- en: Table 5\. Classification accuracies for different orientation prediction approaches
    when evaluated on a benchmark set of objects with manually-labeled ground truth
    orientations. Our approach combines a VLM with a multimodal LLM to get the best
    of both worlds, achieving the highest overall accuracy.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 表5\. 在手动标记真实方向的基准数据集上评估不同方向预测方法的分类准确率。我们的方法结合了VLM和多模态LLM，兼具两者的优点，达到了最高的整体准确率。
- en: Object orientation
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 物体方向
- en: 'Finally, we evaluate the performance of our front-facing direction classifier.
    Similarly to the previous experiment, we build a benchmark dataset (with 450 objects
    in total) containing 50-100 objects for each of several common categories, each
    of which has a hand-labeled ground-truth front facing direction. We then evaluate
    how well our method for orientation prediction perform on this dataset, compared
    to the following alternatives we tried:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估了我们的前向方向分类器的性能。与之前的实验类似，我们构建了一个基准数据集（总共有450个物体），包含每个常见类别的50-100个物体，每个物体都有手工标注的真实前向方向。然后，我们评估了我们在该数据集上进行方向预测的方法，与我们尝试的以下替代方法进行比较：
- en: •
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ours: our full orientation prediction method as described in Section [8](#S8
    "8\. Orienting Retrieved Objects ‣ Open-Universe Indoor Scene Generation using
    LLM Program Synthesis and Uncurated Object Databases").'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的方法：如第[8](#S8 "8\. Orienting Retrieved Objects ‣ Open-Universe Indoor Scene
    Generation using LLM Program Synthesis and Uncurated Object Databases")节中描述的完整方向预测方法。
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VLM (front): choosing whichever orientation produces an image whose VLM embedding
    has the highest cosine similarity to that of the text “the front of a $C$ is the
    object category.'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VLM (前)：选择任何一种方向生成图像，其VLM嵌入与文本“$C$的前面是物体类别”的余弦相似度最高。
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VLM (front,back): like the previous method, but where we also render the reverse
    face of the object and add the similarity to “the back of a $C$” to the objective
    we minimize.'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VLM (前、后)：类似于之前的方法，但我们还渲染物体的反面，并将“$C$的后面”的相似度加入我们最小化的目标中。
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VLM (front,back,side): like the previous method, but where we also render one
    of the side faces of the object and add the similarity to “the side of a $C$”
    to the objective we minimize.'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VLM (前、后、侧)：类似于之前的方法，但我们还渲染了物体的一个侧面，并将“$C$的侧面”的相似度加入我们最小化的目标中。
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM: providing renders of all four faces of the object to an LLM and asking
    it to choose which image best represents the front of object.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM：向LLM提供物体的四个面图像，并让其选择哪个图像最好地代表物体的前面。
- en: Table [5](#S9.T5 "Table 5 ‣ Object retrieval ‣ 9.4\. Ablation Studies & Other
    Evaluations ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor Scene Generation
    using LLM Program Synthesis and Uncurated Object Databases") shows the results
    of this experiment. The method which only uses a multimodal LLM performs better
    than the VLM-based methods in general, but it does suffer from large performance
    drops on certain types of objects (e.g. chairs). By using a VLM to filter the
    set of views the LLM must consider down to two (a task likely more prevalent in
    its training data than four-way image comparison), our method improves over the
    LLM-only baseline on nearly all categories.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表[5](#S9.T5 "Table 5 ‣ Object retrieval ‣ 9.4\. Ablation Studies & Other Evaluations
    ‣ 9\. Results and Evaluation ‣ Open-Universe Indoor Scene Generation using LLM
    Program Synthesis and Uncurated Object Databases")展示了这个实验的结果。仅使用多模态LLM的方法在总体上表现优于基于VLM的方法，但在某些类型的物体（如椅子）上性能大幅下降。通过使用VLM将LLM必须考虑的视角数减少到两个（这一任务在其训练数据中可能比四路图像比较更为常见），我们的方法在几乎所有类别上都超越了仅使用LLM的基线。
- en: 9.5\. Timing
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5\. 时间
- en: On a MacBook Pro with an Apple M1 Max processor and 32GB RAM, the median time
    to generate an object layout is about four minutes. Almost all of this time is
    spent querying the LLM (the layout optimizer stage takes under 10 seconds, typically).
    Converting the layout into a full 3D scene is more computationally expensive,
    as our object retrieval and orientation modules can invoke multiple LLM calls
    for each object in the scene; for complex, densely-populated scenes, this cost
    adds up. The median time to retrieve an object is slightly under a minute (51
    seconds); to orient the object, it is 16 seconds. For a set of scenes we generated
    with an average of 17 objects per scene, this led to a median total scene generation
    time of about 25 minutes. While slower than prior systems for closed-universe
    scene synthesis (which often take only seconds), this is still faster than existing
    text-to-3D systems which optimize a VLM-based loss—these systems can take hours
    to produce a single scene. Our approach could also be accelerated by caching information
    computed about retrieved objects (e.g. their front-facing orientations, whether
    they belong to a certain category) to avoid re-computing those quantities if objects
    are encountered again.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在搭载Apple M1 Max处理器和32GB RAM的MacBook Pro上，生成一个物体布局的中位时间约为四分钟。几乎所有时间都用于查询LLM（布局优化阶段通常在10秒内完成）。将布局转换为完整的3D场景在计算上更加昂贵，因为我们的物体检索和方向模块可能会对场景中的每个物体发起多个LLM调用；对于复杂且密集的场景，这种成本会累计。检索一个物体的中位时间略低于一分钟（51秒）；定位物体的时间为16秒。对于我们生成的一组平均每个场景17个物体的场景，这导致了大约25分钟的中位总场景生成时间。虽然比先前的闭合宇宙场景合成系统（通常只需几秒钟）要慢，但这仍然比现有的文本到3D系统要快，这些系统优化基于VLM的损失——这些系统生成一个场景可能需要几个小时。我们的方法也可以通过缓存关于检索物体的信息（例如它们的正面朝向，是否属于某个类别）来加速，以避免如果再次遇到这些物体时重新计算这些量。
- en: 10\. Discussion & Future Work
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10\. 讨论与未来工作
- en: 'We presented a system for open-universe scene generation: taking a text prompt
    as input, our system generates room-scale indoor scenes of any requested type
    composed of whatever relevant objects are needed for that room. Our system leverages
    LLMs to generate scenes by tasking them with generating declarative object-relation
    programs; these programs are then converted to constraint problems which are solved
    with gradient-based optimization to produce object layouts. Finally, our system
    uses multimodal LLMs and vision-language models to retrieve appropriate meshes
    for each object from a massive, unannotated dataset, as well as to estimate the
    front-facing orientation of these retrieved meshes.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了一个用于开放宇宙场景生成的系统：以文本提示作为输入，我们的系统生成任何请求类型的房间规模的室内场景，组成这些场景所需的相关物体。我们的系统利用LLM生成场景，通过生成声明性物体关系程序来实现；这些程序随后被转换为约束问题，通过基于梯度的优化解决以产生物体布局。最后，我们的系统使用多模态LLM和视觉语言模型，从一个庞大且未标注的数据集中检索每个物体的适当网格，并估计这些检索到的网格的正面朝向。
- en: Open-universe scene generation is a complex, challenging task, and our system
    for solving it is not perfect. In the remainder of the paper, we discuss limitations
    and opportunities for improvement.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 开放宇宙场景生成是一个复杂且具有挑战性的任务，我们的系统解决这个问题的能力还不完美。在论文的剩余部分，我们讨论了局限性和改进的机会。
- en: 10.1\. Viability for Interior Design
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1\. 室内设计的可行性
- en: To get a sense for whether the outputs produced by our system could currently
    be used in real-world interior design scenarios, we conducted a small qualitative
    study. We recruited six individuals (“clients”) seeking complimentary interior
    design services through online ads, gathered their design needs through 30-minute
    interviews, and later presented to them designs created by both our system and
    by professional designers. Both the clients and the designers provided feedback
    on the generated scenes. More detail about these interviews and about how the
    scenes were generated can be found in the supplemental material.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们系统生成的输出是否可以在实际室内设计场景中使用，我们进行了一个小规模的定性研究。我们通过在线广告招募了六名寻求免费室内设计服务的“客户”，通过30分钟的访谈收集了他们的设计需求，然后向他们展示了由我们的系统和专业设计师创建的设计。客户和设计师都对生成的场景提供了反馈。有关这些访谈的更多细节以及场景生成的详细信息可以在补充材料中找到。
- en: The clients and the designers appreciated the system’s ability to produce appropriate
    groupings of objects (e.g. dining tables and chairs), correctly place rugs under
    furniture, and ensure adequate space for door openings. They also appreciated
    the color and material coordination in its generated furniture objects. However,
    they found that the system could produce overly cluttered scenes in which it seemed
    to lack an understanding of certain professional interior design principles such
    as maintaining circulation (by e.g. not clustering furniture in corners). Such
    design principles can be expressed computationally (Merrell et al., [2011](#bib.bib41));
    our system could be improved by adding design principles as operations to our
    scene modeling DSL and allowing the program synthesis LLMs to decide which principles
    should be applied to which (parts of) scenes.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 客户和设计师欣赏系统在生成合适的对象分组（例如餐桌和椅子）、正确放置家具下的地毯以及确保门开口的空间充足方面的能力。他们还欣赏生成的家具对象在颜色和材料协调方面的表现。然而，他们发现系统有时会生成过于杂乱的场景，似乎缺乏对某些专业室内设计原则的理解，例如维持流通（例如，不将家具聚集在角落）。这些设计原则可以通过计算方法表达（Merrell
    et al., [2011](#bib.bib41)）；我们可以通过将设计原则作为操作添加到我们的场景建模 DSL 中，并允许程序合成 LLM 决定将哪些原则应用于哪些（部分）场景，从而改进我们的系统。
- en: 10.2\. Other Limitations & Future Work
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2\. 其他局限性与未来工作
- en: 'Our system currently only supports four-walled rooms. There are many ways this
    limitation could be removed: non-rectangular rooms could be subdivided into rectangular
    regions, or arbitrary arrangements of walls could be specified parametrically
    in the system’s input prompt (though reasoning about the resultant wall geometry
    may prove difficult for an LLM; multimodal LLMs which can correlate parametric
    wall objects with images of wall geometry may help). In addition, objects in the
    current system are restricted to one of four cardinal orientations. This discrete
    set could be expanded. Orientations could also be represented as continuous values
    in the layout optimizer (e.g. allowing small angular corrections to maintain FACING
    constraints); this would necessitate a revision to our current multi-step layout
    optimization scheme.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统目前仅支持四面墙的房间。可以通过多种方式消除这一限制：非矩形房间可以被细分为矩形区域，或者可以在系统的输入提示中参数化地指定任意墙的布置（尽管推理结果墙的几何形状对于
    LLM 可能会很困难；可以通过多模态 LLM 来帮助相关参数化墙对象与墙几何形状的图像）。此外，目前系统中的对象受限于四个主要方向之一。这个离散的集合可以扩展。方向也可以在布局优化器中表示为连续值（例如，允许小的角度修正以保持
    FACING 约束）；这将需要对我们当前的多步骤布局优化方案进行修订。
- en: In very rare cases, objects in large databases such as Objaverse are modeled
    with their up axis not aligned with one of the world coordinate axes; this violates
    the assumptions of our orientation prediction module and can result in “tilted”
    objects being inserted into the scene. It may be possible to detect and correct
    (or filter out) such objects using geometric heuristics (Fu et al., [2008](#bib.bib19))
    or carefully-designed queries to multimodal language models.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常少见的情况下，大型数据库中的对象（如 Objaverse）的上轴可能与世界坐标轴之一不对齐；这违反了我们方向预测模块的假设，可能导致“倾斜”的对象被插入到场景中。可以通过几何启发式方法（Fu
    et al., [2008](#bib.bib19)）或精心设计的多模态语言模型查询来检测和修正（或过滤）这些对象。
- en: 'We have introduced some mechanisms for fixing errors produced by our LLM program
    synthesizer, but they are not foolproof. In the future, we are interested in exploring
    LLM self-repair (Huang et al., [2023a](#bib.bib29)) instead of / in addition to
    our existing heuristics: collecting detected errors and tasking the LLM with correcting
    its own prior output to eliminate them.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了一些修正我们 LLM 程序合成器生成的错误的机制，但它们并非万无一失。未来，我们希望探索 LLM 自我修复（Huang et al., [2023a](#bib.bib29)），而不是/以及我们现有的启发式方法：收集检测到的错误，并要求
    LLM 修正其先前的输出以消除这些错误。
- en: The open-ended capabilities of VLMs and LLMs could support myriad approaches
    for open-universe scene generation. In this paper, we have explored one small
    region of this design space; future work is needed to map out its entirety. We
    hope that our work serves as both a springboard and a strong baseline for a new
    line of research on open-universe scene generation.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: VLMs 和 LLMs 的开放式能力可以支持多种开放宇宙场景生成的方法。在本文中，我们探讨了这一设计空间的一个小区域；未来的工作需要绘制其完整范围。我们希望我们的工作既能作为跳板，也能为开放宇宙场景生成的新研究方向提供一个强有力的基准。
- en: References
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Abdelreheem et al. (2023a) Ahmed Abdelreheem, Abdelrahman Eldesokey, Maks Ovsjanikov,
    and Peter Wonka. 2023a. Zero-Shot 3D Shape Correspondence. In *SIGGRAPH Asia*.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelreheem 等 (2023a) Ahmed Abdelreheem, Abdelrahman Eldesokey, Maks Ovsjanikov,
    和 Peter Wonka. 2023a. 零样本3D形状对应。载于 *SIGGRAPH Asia*.
- en: 'Abdelreheem et al. (2023b) Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov,
    and Peter Wonka. 2023b. SATR: Zero-Shot Semantic Segmentation of 3D Shapes. In
    *Proceedings of the International Conference on Computer Vision (ICCV)*.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abdelreheem 等 (2023b) Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov,
    和 Peter Wonka. 2023b. SATR: 零样本语义分割3D形状。载于 *国际计算机视觉大会论文集 (ICCV)*.'
- en: AlphaCode Team (2023) Google DeepMind AlphaCode Team. 2023. AlphaCode 2 Technical
    Report. (2023).
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaCode Team (2023) Google DeepMind AlphaCode Team. 2023. AlphaCode 2 技术报告.
    (2023).
- en: 'Bai et al. (2023) Haotian Bai, Yuanhuiyi Lyu, Lutao Jiang, Sijia Li, Haonan
    Lu, Xiaodong Lin, and Lin Wang. 2023. CompoNeRF: Text-guided Multi-object Compositional
    NeRF with Editable 3D Scene Layout. arXiv:2303.13843 [cs.CV]'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等 (2023) Haotian Bai, Yuanhuiyi Lyu, Lutao Jiang, Sijia Li, Haonan Lu,
    Xiaodong Lin, 和 Lin Wang. 2023. CompoNeRF: 文本指导的多对象组合NeRF与可编辑的3D场景布局. arXiv:2303.13843
    [cs.CV]'
- en: 'Battista et al. (1998) Giuseppe Di Battista, Peter Eades, Roberto Tamassia,
    and Ioannis G. Tollis. 1998. *Graph Drawing: Algorithms for the Visualization
    of Graphs* (1st ed.). Prentice Hall PTR, USA.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Battista 等 (1998) Giuseppe Di Battista, Peter Eades, Roberto Tamassia, 和 Ioannis
    G. Tollis. 1998. *图形绘制: 图形可视化算法* (第1版). Prentice Hall PTR, USA.'
- en: 'Chen et al. (2023) Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023.
    Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D
    Content Creation. In *Proceedings of the IEEE/CVF International Conference on
    Computer Vision (ICCV)*.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2023) Rui Chen, Yongwei Chen, Ningxin Jiao, 和 Kui Jia. 2023. Fantasia3D:
    解析几何和外观以实现高质量的文本到3D内容创建。载于 *IEEE/CVF 国际计算机视觉大会论文集 (ICCV)*.'
- en: Cho et al. (2023) Jaemin Cho, Abhay Zala, and Mohit Bansal. 2023. Visual Programming
    for Text-to-Image Generation and Evaluation. In *NeurIPS*.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等 (2023) Jaemin Cho, Abhay Zala, 和 Mohit Bansal. 2023. 面向文本到图像生成和评估的视觉编程。载于
    *NeurIPS*.
- en: 'Coyne and Sproat (2001) Bob Coyne and Richard Sproat. 2001. WordsEye: an automatic
    text-to-scene conversion system. In *Proceedings of the 28th Annual Conference
    on Computer Graphics and Interactive Techniques* *(SIGGRAPH ’01)*. Association
    for Computing Machinery, New York, NY, USA, 487–496. [https://doi.org/10.1145/383259.383316](https://doi.org/10.1145/383259.383316)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Coyne 和 Sproat (2001) Bob Coyne 和 Richard Sproat. 2001. WordsEye: 一种自动文本到场景转换系统。载于
    *第28届计算机图形学与交互技术年会论文集* *(SIGGRAPH ’01)*. 计算机协会, 纽约, NY, USA, 487–496. [https://doi.org/10.1145/383259.383316](https://doi.org/10.1145/383259.383316)'
- en: 'Decatur et al. (2022) Dale Decatur, Itai Lang, and Rana Hanocka. 2022. 3D Highlighter:
    Localizing Regions on 3D Shapes via Text Descriptions. *CVPR*.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Decatur 等 (2022) Dale Decatur, Itai Lang, 和 Rana Hanocka. 2022. 3D 高亮器: 通过文本描述定位3D形状上的区域。*CVPR*.'
- en: 'Deitke et al. (2023) Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,
    Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak
    Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana
    Ehsani, Ludwig Schmidt, and Ali Farhadi. 2023. Objaverse-XL: A Universe of 10M+
    3D Objects. *arXiv preprint arXiv:2307.05663* (2023).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deitke 等 (2023) Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar
    Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak
    Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana
    Ehsani, Ludwig Schmidt, 和 Ali Farhadi. 2023. Objaverse-XL: 一个包含10M+ 3D对象的宇宙。*arXiv
    预印本 arXiv:2307.05663* (2023).'
- en: 'Deitke et al. (2022a) Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
    Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi,
    and Ali Farhadi. 2022a. Objaverse: A Universe of Annotated 3D Objects. *arXiv
    preprint arXiv:2212.08051* (2022).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deitke 等 (2022a) Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar
    Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, 和 Ali
    Farhadi. 2022a. Objaverse: 一个注释3D对象的宇宙。*arXiv 预印本 arXiv:2212.08051* (2022).'
- en: 'Deitke et al. (2022b) Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,
    Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and
    Roozbeh Mottaghi. 2022b. ProcTHOR: Large-Scale Embodied AI Using Procedural Generation.
    In *Advances in Neural Information Processing Systems*, S. Koyejo, S. Mohamed,
    A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35\. Curran Associates,
    Inc., 5982–5994. [https://proceedings.neurips.cc/paper_files/paper/2022/file/27c546ab1e4f1d7d638e6a8dfbad9a07-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/27c546ab1e4f1d7d638e6a8dfbad9a07-Paper-Conference.pdf)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deitke et al. (2022b) Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,
    Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, 和 Roozbeh
    Mottaghi. 2022b. ProcTHOR: 使用程序生成的大规模具身 AI。收录于 *Neural Information Processing
    Systems 进展*，S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, 和 A. Oh (编辑)，第
    35 卷。Curran Associates, Inc.，5982–5994。 [https://proceedings.neurips.cc/paper_files/paper/2022/file/27c546ab1e4f1d7d638e6a8dfbad9a07-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/27c546ab1e4f1d7d638e6a8dfbad9a07-Paper-Conference.pdf)'
- en: Douze et al. (2024) Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson,
    Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé
    Jégou. 2024. The Faiss library. (2024). arXiv:2401.08281 [cs.LG]
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Douze et al. (2024) Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson,
    Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, 和 Hervé
    Jégou. 2024. 《Faiss 库》。 (2024). arXiv:2401.08281 [cs.LG]
- en: 'Fang et al. (2023) Chuan Fang, Xiaotao Hu, Kunming Luo, and Ping Tan. 2023.
    Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints.
    *arXiv preprint arXiv:2310.03602* (2023).'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang et al. (2023) Chuan Fang, Xiaotao Hu, Kunming Luo, 和 Ping Tan. 2023. Ctrl-Room:
    具有布局约束的可控文本到 3D 房间网格生成。*arXiv 预印本 arXiv:2310.03602* (2023)。'
- en: 'Feng et al. (2023) Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun Reddy
    Akula, Xuehai He, S Basu, Xin Eric Wang, and William Yang Wang. 2023. LayoutGPT:
    Compositional Visual Planning and Generation with Large Language Models. In *Thirty-seventh
    Conference on Neural Information Processing Systems*. [https://openreview.net/forum?id=Xu8aG5Q8M3](https://openreview.net/forum?id=Xu8aG5Q8M3)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng et al. (2023) Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun
    Reddy Akula, Xuehai He, S Basu, Xin Eric Wang, 和 William Yang Wang. 2023. LayoutGPT:
    使用大型语言模型进行组合视觉规划和生成。收录于 *第七十七届神经信息处理系统会议*。 [https://openreview.net/forum?id=Xu8aG5Q8M3](https://openreview.net/forum?id=Xu8aG5Q8M3)'
- en: Fisher et al. (2012) Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser,
    and Pat Hanrahan. 2012. Example-based synthesis of 3D object arrangements. *ACM
    Transactions on Graphics (TOG)* 31, 6 (2012), 135:1–11.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fisher et al. (2012) Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser,
    和 Pat Hanrahan. 2012. 基于示例的 3D 物体排列合成。*ACM Transactions on Graphics (TOG)* 31,
    6 (2012)，135:1–11。
- en: 'Fu et al. (2021) Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang,
    Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 2021. 3d-front:
    3d furnished rooms with layouts and semantics. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*. 10933–10942.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu et al. (2021) Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang,
    Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, 等. 2021. 3d-front:
    具有布局和语义的 3D 布置房间。在 *IEEE/CVF 国际计算机视觉会议论文集*。10933–10942。'
- en: Fu et al. (2008) Hongbo Fu, Daniel Cohen-Or, Gideon Dror, and Alla Sheffer.
    2008. Upright orientation of man-made objects. In *ACM SIGGRAPH 2008 Papers* (Los
    Angeles, California) *(SIGGRAPH ’08)*. Association for Computing Machinery, New
    York, NY, USA, Article 42, 7 pages. [https://doi.org/10.1145/1399504.1360641](https://doi.org/10.1145/1399504.1360641)
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2008) Hongbo Fu, Daniel Cohen-Or, Gideon Dror, 和 Alla Sheffer. 2008.
    人造物体的垂直方向。在 *ACM SIGGRAPH 2008 论文集*（洛杉矶，加州）*(SIGGRAPH ’08)*。计算机协会，纽约，NY，USA，第
    42 篇，7 页。 [https://doi.org/10.1145/1399504.1360641](https://doi.org/10.1145/1399504.1360641)
- en: 'Gao et al. (2023a) Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard
    Schölkopf. 2023a. GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs.
    *arXiv* 2312.00093 (2023).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2023a) Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, 和 Bernhard
    Schölkopf. 2023a. GraphDreamer: 从场景图进行组合性的 3D 场景合成。*arXiv* 2312.00093 (2023)。'
- en: 'Gao et al. (2023b) Lin Gao, Jia-Mu Sun, Kaichun Mo, Yu-Kun Lai, Leonidas J.
    Guibas, and Jie Yang. 2023b. SceneHGN: Hierarchical Graph Networks for 3D Indoor
    Scene Generation with Fine-Grained Geometry. *IEEE Transactions on Pattern Analysis
    and Machine Intelligence* (2023), 1–18. [https://doi.org/10.1109/TPAMI.2023.3237577](https://doi.org/10.1109/TPAMI.2023.3237577)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2023b) Lin Gao, Jia-Mu Sun, Kaichun Mo, Yu-Kun Lai, Leonidas J.
    Guibas, 和 Jie Yang. 2023b. SceneHGN: 用于 3D 室内场景生成的层次图网络，具有细粒度几何。*IEEE 计算机学会模式分析与机器智能汇刊*
    (2023)，1–18。 [https://doi.org/10.1109/TPAMI.2023.3237577](https://doi.org/10.1109/TPAMI.2023.3237577)'
- en: GPT-4 (2023) OpenAI GPT-4\. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4 (2023) OpenAI GPT-4\. 2023. GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: Grand et al. (2023) Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson,
    Muxin Liu, Joshua B. Tenenbaum, and Jacob Andreas. 2023. Learning Interpretable
    Libraries by Compressing and Documenting Code. In *Intrinsically-Motivated and
    Open-Ended Learning Workshop @NeurIPS2023*. [https://openreview.net/forum?id=4gYLottfsf](https://openreview.net/forum?id=4gYLottfsf)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grand 等 (2023) Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson,
    Muxin Liu, Joshua B. Tenenbaum 和 Jacob Andreas. 2023. 通过压缩和记录代码学习可解释的库。发表于 *内在激励和开放式学习研讨会
    @NeurIPS2023*。[https://openreview.net/forum?id=4gYLottfsf](https://openreview.net/forum?id=4gYLottfsf)
- en: 'Gupta and Kembhavi (2023) Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual
    Programming: Compositional Visual Reasoning Without Training. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.
    14953–14962.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 和 Kembhavi (2023) Tanmay Gupta 和 Aniruddha Kembhavi. 2023. 视觉编程：无需训练的组合视觉推理。发表于
    *IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*。14953–14962。
- en: 'Haocheng et al. (2023) Yuan Haocheng, Xu Jing, Pan Hao, Bousseau Adrien, Mitra
    Niloy, and Li Changjian. 2023. CADTalk: An Algorithm and Benchmark for Semantic
    Commenting of CAD Programs. *arXiv preprint arXiv:2311.16703* (2023).'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haocheng 等 (2023) Yuan Haocheng, Xu Jing, Pan Hao, Bousseau Adrien, Mitra Niloy
    和 Li Changjian. 2023. CADTalk：一种用于 CAD 程序语义注释的算法与基准。*arXiv 预印本 arXiv:2311.16703*
    (2023)。
- en: 'Hobbs (2024) Jordan Hobbs. 2024. Why IKEA Uses 3D Renders vs. Photography for
    Their Furniture Catalog. [https://www.cadcrowd.com/blog/why-ikea-uses-3d-renders-vs-photography-for-their-furniture-catalog/](https://www.cadcrowd.com/blog/why-ikea-uses-3d-renders-vs-photography-for-their-furniture-catalog/).
    Accessed: 2024-01-19.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hobbs (2024) Jordan Hobbs. 2024. 为什么 IKEA 使用 3D 渲染而非摄影来制作家具目录。[https://www.cadcrowd.com/blog/why-ikea-uses-3d-renders-vs-photography-for-their-furniture-catalog/](https://www.cadcrowd.com/blog/why-ikea-uses-3d-renders-vs-photography-for-their-furniture-catalog/).
    访问时间：2024-01-19。
- en: 'Höllein et al. (2023) Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson,
    and Matthias Nießner. 2023. Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image
    Models. In *Proceedings of the IEEE/CVF International Conference on Computer Vision
    (ICCV)*. 7909–7920.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Höllein 等 (2023) Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson 和 Matthias
    Nießner. 2023. Text2Room：从 2D 文本到图像模型中提取纹理 3D 网格。发表于 *IEEE/CVF国际计算机视觉会议（ICCV）论文集*。7909–7920。
- en: 'Huang et al. (2023b) Ian Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas
    Guibas. 2023b. Aladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract
    Scene Descriptions. *arXiv preprint arXiv:2306.06212* (2023).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2023b) Ian Huang, Vrishab Krishna, Omoruyi Atekha 和 Leonidas Guibas.
    2023b. Aladdin：从抽象场景描述中零样本生成风格化 3D 资产。*arXiv 预印本 arXiv:2306.06212* (2023)。
- en: Huang et al. (2023a) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven
    Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023a. Large Language Models
    Cannot Self-Correct Reasoning Yet. arXiv:2310.01798 [cs.CL]
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2023a) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng,
    Adams Wei Yu, Xinying Song 和 Denny Zhou. 2023a. 大型语言模型尚不能自我纠正推理。arXiv:2310.01798
    [cs.CL]
- en: Jain et al. (2022) Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel,
    and Ben Poole. 2022. Zero-Shot Text-Guided Object Generation with Dream Fields.
    (2022).
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等 (2022) Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel 和
    Ben Poole. 2022. 零样本文本引导的对象生成与梦境场。 (2022)。
- en: 'Jun and Nichol (2023) Heewoo Jun and Alex Nichol. 2023. Shap-E: Generating
    Conditional 3D Implicit Functions. arXiv:2305.02463 [cs.CV]'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jun 和 Nichol (2023) Heewoo Jun 和 Alex Nichol. 2023. Shap-E：生成条件 3D 隐式函数。arXiv:2305.02463
    [cs.CV]
- en: Kermani et al. (2016) Z Sadeghipour Kermani, Zicheng Liao, Ping Tan, and H Zhang.
    2016. Learning 3D Scene Synthesis from Annotated RGB-D Images. In *Computer Graphics
    Forum*, Vol. 35\. 197–206.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kermani 等 (2016) Z Sadeghipour Kermani, Zicheng Liao, Ping Tan 和 H Zhang. 2016.
    从标注的 RGB-D 图像中学习 3D 场景合成。发表于 *计算机图形学论坛*，第 35 卷。197–206。
- en: 'Li et al. (2018) Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri,
    Owais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen, Daniel Cohen-Or, and Hao Zhang.
    2018. GRAINS: Generative Recursive Autoencoders for INdoor Scenes. *CoRR* arXiv:1807.09193
    (2018).'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2018) Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais
    Khan, Ariel Shamir, Changhe Tu, Baoquan Chen, Daniel Cohen-Or 和 Hao Zhang. 2018.
    GRAINS：用于室内场景的生成递归自编码器。*CoRR* arXiv:1807.09193 (2018)。
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de
    Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation
    with AlphaCode. *Science* 378, 6624 (Dec. 2022), 1092–1097. [https://doi.org/10.1126/science.abq1158](https://doi.org/10.1126/science.abq1158)
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022）Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
    Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas
    Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen,
    Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel
    J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray
    Kavukcuoglu, 和 Oriol Vinyals. 2022. 竞争级代码生成与 AlphaCode. *科学* 378, 6624（2022年12月），1092–1097。
    [https://doi.org/10.1126/science.abq1158](https://doi.org/10.1126/science.abq1158)
- en: Liang et al. (2017) Yuan Liang, Song-Hai Zhang, and Ralph Robert Martin. 2017.
    Automatic Data-Driven Room Design Generation. In *Next Generation Computer Animation
    Techniques*, Jian Chang, Jian Jun Zhang, Nadia Magnenat Thalmann, Shi-Min Hu,
    Ruofeng Tong, and Wencheng Wang (Eds.). Springer International Publishing, Cham,
    133–148.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2017）Yuan Liang, Song-Hai Zhang, 和 Ralph Robert Martin. 2017. 自动数据驱动房间设计生成。在
    *下一代计算机动画技术*，Jian Chang, Jian Jun Zhang, Nadia Magnenat Thalmann, Shi-Min Hu,
    Ruofeng Tong, 和 Wencheng Wang（编辑）。Springer 国际出版公司，Cham，第 133–148 页.
- en: 'Lin et al. (2023) Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui
    Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023.
    Magic3D: High-Resolution Text-to-3D Content Creation. In *IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人（2023）Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui
    Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, 和 Tsung-Yi Lin. 2023.
    Magic3D: 高分辨率文本到 3D 内容创建。在 *IEEE 计算机视觉与模式识别会议（CVPR）*.'
- en: 'Liu et al. (2023) Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling,
    Fatih Porikli, and Hao Su. 2023. Partslip: Low-shot part segmentation for 3d point
    clouds via pretrained image-language models. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 21736–21746.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人（2023）Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih
    Porikli, 和 Hao Su. 2023. Partslip: 基于预训练图像-语言模型的 3D 点云低样本部件分割。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*。21736–21746.'
- en: 'Lorraine et al. (2023) Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan
    Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler,
    and James Lucas. 2023. ATT3D: Amortized Text-To-3D Object Synthesis. *arXiv* (2023).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lorraine 等人（2023）Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin,
    Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, 和 James
    Lucas. 2023. ATT3D: 摊销文本到 3D 对象合成。 *arXiv*（2023年）。'
- en: Luo et al. (2023) Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson.
    2023. Scalable 3D Captioning with Pretrained Models. *arXiv preprint arXiv:2306.07279*
    (2023).
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人（2023）Tiange Luo, Chris Rockwell, Honglak Lee, 和 Justin Johnson. 2023.
    可扩展的 3D 描述生成与预训练模型。 *arXiv 预印本 arXiv:2306.07279*（2023年）。
- en: Makatura et al. (2023) Liane Makatura, Michael Foshey, Bohan Wang, Felix HähnLein,
    Pingchuan Ma, Bolei Deng, Megan Tjandrasuwita, Andrew Spielberg, Crystal Elaine
    Owens, Peter Yichen Chen, Allan Zhao, Amy Zhu, Wil J Norton, Edward Gu, Joshua
    Jacob, Yifei Li, Adriana Schulz, and Wojciech Matusik. 2023. How Can Large Language
    Models Help Humans in Design and Manufacturing? arXiv:2307.14377 [cs.CL]
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makatura 等人（2023）Liane Makatura, Michael Foshey, Bohan Wang, Felix HähnLein,
    Pingchuan Ma, Bolei Deng, Megan Tjandrasuwita, Andrew Spielberg, Crystal Elaine
    Owens, Peter Yichen Chen, Allan Zhao, Amy Zhu, Wil J Norton, Edward Gu, Joshua
    Jacob, Yifei Li, Adriana Schulz, 和 Wojciech Matusik. 2023. 大型语言模型如何帮助人类在设计和制造中？
    arXiv:2307.14377 [cs.CL]
- en: Merrell et al. (2011) Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala,
    and Vladlen Koltun. 2011. Interactive furniture layout using interior design guidelines.
    In *ACM SIGGRAPH 2011 Papers* (Vancouver, British Columbia, Canada) *(SIGGRAPH
    ’11)*. Association for Computing Machinery, New York, NY, USA, Article 87, 10 pages.
    [https://doi.org/10.1145/1964921.1964982](https://doi.org/10.1145/1964921.1964982)
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merrell 等人（2011）Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, 和
    Vladlen Koltun. 2011. 使用室内设计指南的互动家具布局。在 *ACM SIGGRAPH 2011 Papers*（温哥华，不列颠哥伦比亚省，加拿大）*(SIGGRAPH
    ’11)*. 计算机协会，纽约，NY，美国，第 87 号文章，10 页。 [https://doi.org/10.1145/1964921.1964982](https://doi.org/10.1145/1964921.1964982)
- en: 'Mizrahi et al. (2023) David Mizrahi, Roman Bachmann, Oguzhan Fatih Kar, Teresa
    Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 2023. 4M: Massively Multimodal
    Masked Modeling. In *Thirty-seventh Conference on Neural Information Processing
    Systems*. [https://openreview.net/forum?id=TegmlsD8oQ](https://openreview.net/forum?id=TegmlsD8oQ)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mizrahi等（2023）David Mizrahi, Roman Bachmann, Oguzhan Fatih Kar, Teresa Yeo,
    Mingfei Gao, Afshin Dehghan, 和 Amir Zamir。2023年。《4M: Massively Multimodal Masked
    Modeling》。见于*第三十七届神经信息处理系统会议*。[https://openreview.net/forum?id=TegmlsD8oQ](https://openreview.net/forum?id=TegmlsD8oQ)'
- en: 'Nichol et al. (2022) Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin,
    and Mark Chen. 2022. Point-E: A System for Generating 3D Point Clouds from Complex
    Prompts. arXiv:2212.08751 [cs.CV]'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nichol等（2022）Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, 和
    Mark Chen。2022年。《Point-E: A System for Generating 3D Point Clouds from Complex
    Prompts》。arXiv:2212.08751 [cs.CV]'
- en: 'Paschalidou et al. (2021) Despoina Paschalidou, Amlan Kar, Maria Shugrina,
    Karsten Kreis, Andreas Geiger, and Sanja Fidler. 2021. ATISS: Autoregressive Transformers
    for Indoor Scene Synthesis. In *Advances in Neural Information Processing Systems
    (NeurIPS)*.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paschalidou等（2021）Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten
    Kreis, Andreas Geiger, 和 Sanja Fidler。2021年。《ATISS: Autoregressive Transformers
    for Indoor Scene Synthesis》。见于*神经信息处理系统进展（NeurIPS）*。'
- en: 'Planner5d (2024) Planner5d. 2024. Planner5d: House Design Software. [https://planner5d.com](https://planner5d.com).
    Accessed: 2024-01-19.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Planner5d（2024）Planner5d。2024年。《Planner5d: House Design Software》。[https://planner5d.com](https://planner5d.com)。访问时间：2024-01-19。'
- en: 'Poole et al. (2022) Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.
    2022. DreamFusion: Text-to-3D using 2D Diffusion. *arXiv* (2022).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Poole等（2022）Ben Poole, Ajay Jain, Jonathan T. Barron, 和 Ben Mildenhall。2022年。《DreamFusion:
    Text-to-3D using 2D Diffusion》。*arXiv*（2022年）。'
- en: 'Puig et al. (2023) Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire
    Cote, Ruslan Partsey, Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal
    Hlavac, Tiffany Min, Theo Gervet, Vladimir Vondrus, Vincent-Pierre Berges, John
    Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik,
    Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi.
    2023. Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Puig等（2023）Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote,
    Ruslan Partsey, Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal Hlavac,
    Tiffany Min, Theo Gervet, Vladimir Vondrus, Vincent-Pierre Berges, John Turner,
    Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra
    Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, 和 Roozbeh Mottaghi。2023年。《Habitat
    3.0: A Co-Habitat for Humans, Avatars and Robots》。'
- en: Qi et al. (2018) Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and Song-Chun
    Zhu. 2018. Human-centric Indoor Scene Synthesis Using Stochastic Grammar. In *Conference
    on Computer Vision and Pattern Recognition (CVPR)*.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi等（2018）Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, 和 Song-Chun Zhu。2018年。《Human-centric
    Indoor Scene Synthesis Using Stochastic Grammar》。见于*计算机视觉与模式识别会议（CVPR）*。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual
    Models From Natural Language Supervision. In *Proceedings of the 38th International
    Conference on Machine Learning* *(Proceedings of Machine Learning Research, Vol. 139)*,
    Marina Meila and Tong Zhang (Eds.). PMLR, 8748–8763.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等（2021）Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel
    Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
    Gretchen Krueger, 和 Ilya Sutskever。2021年。《Learning Transferable Visual Models
    From Natural Language Supervision》。见于*Proceedings of the 38th International Conference
    on Machine Learning* *(Proceedings of Machine Learning Research, Vol. 139)*，Marina
    Meila 和 Tong Zhang（编）。PMLR, 8748–8763。
- en: Ritchie et al. (2019) Daniel Ritchie, Kai Wang, and Yu an Lin. 2019. Fast and
    Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models. In *CVPR
    2019*.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ritchie等（2019）Daniel Ritchie, Kai Wang, 和 Yu an Lin。2019年。《Fast and Flexible
    Indoor Scene Synthesis via Deep Convolutional Generative Models》。见于*CVPR 2019*。
- en: Rombach et al. (2021) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2021. High-Resolution Image Synthesis with Latent Diffusion
    Models. arXiv:2112.10752 [cs.CV]
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach等（2021）Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser,
    和 Björn Ommer。2021年。《High-Resolution Image Synthesis with Latent Diffusion Models》。arXiv:2112.10752
    [cs.CV]
- en: Romera-Paredes et al. (2023) Bernardino Romera-Paredes, Mohammadamin Barekatain,
    Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R.
    Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein
    Fawzi. 2023. Mathematical discoveries from program search with large language
    models. *Nature* (2023). [https://doi.org/10.1038/s41586-023-06924-6](https://doi.org/10.1038/s41586-023-06924-6)
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Romera-Paredes 等人（2023）Bernardino Romera-Paredes、Mohammadamin Barekatain、Alexander
    Novikov、Matej Balog、M. Pawan Kumar、Emilien Dupont、Francisco J. R. Ruiz、Jordan
    Ellenberg、Pengming Wang、Omar Fawzi、Pushmeet Kohli 和 Alhussein Fawzi。2023年。《从程序搜索中发现的数学发现》。*自然*（2023）。[https://doi.org/10.1038/s41586-023-06924-6](https://doi.org/10.1038/s41586-023-06924-6)
- en: 'RoomSketcher (2024) RoomSketcher. 2024. Create Floor Plans and Home Designs
    Online. [http://www.roomsketcher.com](http://www.roomsketcher.com). Accessed:
    2024-01-19.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoomSketcher（2024）RoomSketcher。2024年。《在线创建平面图和家居设计》。 [http://www.roomsketcher.com](http://www.roomsketcher.com)。访问日期：2024-01-19。
- en: 'Sajnani et al. (2022) Rahul Sajnani, Adrien Poulenard, Jivitesh Jain, Radhika
    Dua, Leonidas J. Guibas, and Srinath Sridhar. 2022. ConDor: Self-Supervised Canonicalization
    of 3D Pose for Partial Shapes. In *The IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sajnani 等人（2022）Rahul Sajnani、Adrien Poulenard、Jivitesh Jain、Radhika Dua、Leonidas
    J. Guibas 和 Srinath Sridhar。2022年。《ConDor：用于部分形状的自监督标准化》。发表于*IEEE计算机视觉与模式识别会议（CVPR）*。
- en: 'Sanghi et al. (2022) Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang,
    Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. 2022. CLIP-Forge: Towards
    Zero-Shot Text-To-Shape Generation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. 18603–18613.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanghi 等人（2022）Aditya Sanghi、Hang Chu、Joseph G. Lambourne、Ye Wang、Chin-Yi Cheng、Marco
    Fumero 和 Kamal Rahimi Malekshan。2022年。《CLIP-Forge：面向零样本文本到形状生成》。发表于*IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，18603–18613。
- en: 'Sanghi et al. (2023) Aditya Sanghi, Rao Fu, Vivian Liu, Karl Willis, Hooman
    Shayani, Amir Hosein Khasahmadi, Srinath Sridhar, and Daniel Ritchie. 2023. CLIP-Sculptor:
    Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language.
    In *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanghi 等人（2023）Aditya Sanghi、Rao Fu、Vivian Liu、Karl Willis、Hooman Shayani、Amir
    Hosein Khasahmadi、Srinath Sridhar 和 Daniel Ritchie。2023年。《CLIP-Sculptor：从自然语言中生成高保真和多样化的形状》。发表于*IEEE计算机视觉与模式识别会议（CVPR）*。
- en: 'Schult et al. (2023) Jonas Schult, Sam Tsai, Lukas Höllein, Bichen Wu, Jialiang
    Wang, Chih-Yao Ma, Kunpeng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, Peizhao
    Zhang, Bastian Leibe, Peter Vajda, and Ji Hou. 2023. ControlRoom3D: Room Generation
    using Semantic Proxy Rooms. *arXiv:2312.05208* (2023).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schult 等人（2023）Jonas Schult、Sam Tsai、Lukas Höllein、Bichen Wu、Jialiang Wang、Chih-Yao
    Ma、Kunpeng Li、Xiaofang Wang、Felix Wimbauer、Zijian He、Peizhao Zhang、Bastian Leibe、Peter
    Vajda 和 Ji Hou。2023年。《ControlRoom3D：使用语义代理房间生成房间》。*arXiv:2312.05208*（2023）。
- en: 'Sun et al. (2023) Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan
    Qin, and Stephen Gould. 2023. 3D-GPT: Procedural 3D Modeling with Large Language
    Models. arXiv:2310.12945 [cs.CV]'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2023）Chunyi Sun、Junlin Han、Weijian Deng、Xinlong Wang、Zishan Qin 和 Stephen
    Gould。2023年。《3D-GPT：利用大型语言模型进行程序化3D建模》。arXiv:2310.12945 [cs.CV]
- en: 'Surís et al. (2023) Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. ViperGPT:
    Visual Inference via Python Execution for Reasoning. *Proceedings of IEEE International
    Conference on Computer Vision (ICCV)* (2023).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Surís 等人（2023）Dídac Surís、Sachit Menon 和 Carl Vondrick。2023年。《ViperGPT：通过Python执行进行视觉推理》。*IEEE国际计算机视觉会议（ICCV）论文集*（2023）。
- en: 'Tang et al. (2023) Jiapeng Tang, Nie Yinyu, Markhasin Lev, Dai Angela, Thies
    Justus, and Matthias Nießner. 2023. DiffuScene: Scene Graph Denoising Diffusion
    Probabilistic Model for Generative Indoor Scene Synthesis. In *arxiv*.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2023）Jiapeng Tang、Nie Yinyu、Markhasin Lev、Dai Angela、Thies Justus 和
    Matthias Nießner。2023年。《DiffuScene：用于生成室内场景合成的场景图去噪扩散概率模型》。发表于*arxiv*。
- en: 'Target (2024) Target. 2024. Home Planner. [https://www.target.com/room-planner/home](https://www.target.com/room-planner/home).
    Accessed: 2024-01-19.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Target（2024）Target。2024年。《家居规划师》。 [https://www.target.com/room-planner/home](https://www.target.com/room-planner/home)。访问日期：2024-01-19。
- en: Trinh et al. (2024) Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong.
    2024. Solving Olympiad Geometry without Human Demonstrations. *Nature* (2024).
    [https://doi.org/10.1038/s41586-023-06747-5](https://doi.org/10.1038/s41586-023-06747-5)
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trinh 等人（2024）Trieu Trinh、Yuhuai Wu、Quoc Le、He He 和 Thang Luong。2024年。《在没有人工示范的情况下解决奥林匹克几何问题》。*自然*（2024）。[https://doi.org/10.1038/s41586-023-06747-5](https://doi.org/10.1038/s41586-023-06747-5)
- en: 'Wang et al. (2019) Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X
    Chang, and Daniel Ritchie. 2019. Planit: Planning and instantiating indoor scenes
    with relation graph and spatial prior networks. *ACM Transactions on Graphics
    (TOG)* 38, 4 (2019), 132.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2019）Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X Chang,
    和 Daniel Ritchie. 2019. Planit: 通过关系图和空间先验网络规划和实例化室内场景. *ACM 图形学交易 (TOG)* 38,
    4（2019），132。'
- en: Wang et al. (2018) Kai Wang, Manolis Savva, Angel X. Chang, and Daniel Ritchie.
    2018. Deep Convolutional Priors for Indoor Scene Synthesis. In *Annual Conference
    on Computer Graphics and Interactive Techniques (SIGGRAPH)*.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2018）Kai Wang, Manolis Savva, Angel X. Chang, 和 Daniel Ritchie. 2018.
    用于室内场景合成的深度卷积先验. 在 *计算机图形学与交互技术年会 (SIGGRAPH)* 中。
- en: 'Wang et al. (2020) Xinpeng Wang, Chandan Yeshwanth, and Matthias Nießner. 2020.
    SceneFormer: Indoor Scene Generation with Transformers. *arXiv preprint arXiv:2012.09793*
    (2020).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2020）Xinpeng Wang, Chandan Yeshwanth, 和 Matthias Nießner. 2020. SceneFormer:
    使用变换器生成室内场景. *arXiv 预印本 arXiv:2012.09793*（2020）。'
- en: 'Wang et al. (2023) Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li,
    Hang Su, and Jun Zhu. 2023. ProlificDreamer: High-Fidelity and Diverse Text-to-3D
    Generation with Variational Score Distillation. In *Advances in Neural Information
    Processing Systems (NeurIPS)*.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023）Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang
    Su, 和 Jun Zhu. 2023. ProlificDreamer: 通过变分分数蒸馏实现高保真和多样的文本到 3D 生成. 在 *神经信息处理系统进展
    (NeurIPS)* 中。'
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models. In *NeurIPS*.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2023）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed Chi, Quoc Le, 和 Denny Zhou. 2023. Chain-of-Thought 提示引发大型语言模型的推理.
    在 *NeurIPS* 中。
- en: 'Xue et al. (2023) Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun
    Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. 2023. ULIP:
    Learning Unified Representation of Language, Image and Point Cloud for 3D Understanding.
    In *CVPR 2023*.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xue 等人（2023）Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu,
    Caiming Xiong, Ran Xu, Juan Carlos Niebles, 和 Silvio Savarese. 2023. ULIP: 语言、图像和点云的统一表示学习用于
    3D 理解. 在 *CVPR 2023* 中。'
- en: Yadav et al. (2023) Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan,
    Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Angel Xuan Chang, Dhruv
    Batra, Manolis Savva, Alexander William Clegg, and Devendra Singh Chaplot. 2023.
    Habitat-Matterport 3D Semantics Dataset. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. 4927–4936.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yadav 等人（2023）Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo
    Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Angel Xuan Chang, Dhruv Batra,
    Manolis Savva, Alexander William Clegg, 和 Devendra Singh Chaplot. 2023. Habitat-Matterport
    3D 语义数据集. 在 *IEEE/CVF 计算机视觉与模式识别会议 (CVPR)* 会议录中。4927–4936。
- en: 'Yang et al. (2023) Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro
    Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris
    Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, and Christopher Clark. 2023.
    Holodeck: Language Guided Generation of 3D Embodied AI Environments. *arXiv preprint
    arXiv:2312.09067* (2023).'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人（2023）Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti,
    Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch,
    Mark Yatskar, Aniruddha Kembhavi, 和 Christopher Clark. 2023. Holodeck: 语言引导的 3D
    具身 AI 环境生成. *arXiv 预印本 arXiv:2312.09067*（2023）。'
- en: Yeh et al. (2012) Yi-Ting Yeh, Lingfeng Yang, Matthew Watson, Noah D. Goodman,
    and Pat Hanrahan. 2012. Synthesizing open worlds with constraints using locally
    annealed reversible jump MCMC. 31, 4, Article 56 (jul 2012), 11 pages. [https://doi.org/10.1145/2185520.2185552](https://doi.org/10.1145/2185520.2185552)
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeh 等人（2012）Yi-Ting Yeh, Lingfeng Yang, Matthew Watson, Noah D. Goodman, 和 Pat
    Hanrahan. 2012. 使用局部退火可逆跳跃 MCMC 合成受约束的开放世界. 31, 4, 文章 56（2012年7月），11 页。[https://doi.org/10.1145/2185520.2185552](https://doi.org/10.1145/2185520.2185552)
- en: 'Yi et al. (2023) Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie,
    Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. 2023. GaussianDreamer:
    Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models.
    *arXiv preprint arXiv:2310.08529* (2023).'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yi 等人（2023）Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng
    Zhang, Wenyu Liu, Qi Tian, 和 Xinggang Wang. 2023. GaussianDreamer: 通过桥接 2D 和 3D
    扩散模型从文本到 3D 高斯体的快速生成. *arXiv 预印本 arXiv:2310.08529*（2023）。'
- en: 'Yu et al. (2011) Lap-Fai Yu, Sai Kit Yeung, Chi-Keung Tang, Demetri Terzopoulos,
    Tony F. Chan, and Stanley Osher. 2011. Make it home: automatic optimization of
    furniture arrangement. *ACM Transactions on Graphics (TOG)* 30, 4 (2011), 86:1–12.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人（2011）Lap-Fai Yu, Sai Kit Yeung, Chi-Keung Tang, Demetri Terzopoulos,
    Tony F. Chan, 和 Stanley Osher. 2011. Make it home: 自动优化家具布局. *ACM 图形学交易 (TOG)*
    30, 4（2011），86:1–12。'
- en: Zhai et al. (2023a) Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas
    Beyer. 2023a. Sigmoid Loss for Language Image Pre-Training. In *Proceedings of
    the IEEE/CVF International Conference on Computer Vision (ICCV)*. 11975–11986.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai等（2023a） Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, 和 Lucas Beyer。2023a。用于语言图像预训练的Sigmoid损失。在*IEEE/CVF计算机视觉国际会议（ICCV）*的论文集中。11975–11986。
- en: Zhai et al. (2023b) Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas
    Beyer. 2023b. Sigmoid Loss for Language Image Pre-Training. In *ICLR 2023*.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai等（2023b） Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, 和 Lucas Beyer。2023b。用于语言图像预训练的Sigmoid损失。在*ICLR
    2023*中。
- en: Zhang et al. (2018) Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander
    Huth, Etienne Vouga, and Qixing Huang. 2018. Deep Generative Modeling for Scene
    Synthesis via Hybrid Representations. *CoRR* abs/1808.02084 (2018). arXiv:1808.02084
    [http://arxiv.org/abs/1808.02084](http://arxiv.org/abs/1808.02084)
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2018） Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander
    Huth, Etienne Vouga, 和 Qixing Huang。2018。通过混合表示进行场景合成的深度生成建模。*CoRR* abs/1808.02084
    (2018)。arXiv:1808.02084 [http://arxiv.org/abs/1808.02084](http://arxiv.org/abs/1808.02084)
- en: 'Zhou et al. (2023) Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao
    Fang, and Hao Su. 2023. PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via
    Multi-View Instance Segmentation and Maximum Likelihood Estimation. arXiv:2312.03015 [cs.CV]'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2023） Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, 和
    Hao Su。2023。PartSLIP++：通过多视图实例分割和最大似然估计增强低样本3D部件分割。arXiv:2312.03015 [cs.CV]
- en: 'Zhou et al. (2019) Yang Zhou, Zachary While, and Evangelos Kalogerakis. 2019.
    SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation. In *IEEE
    Conference on Computer Vision (ICCV)*.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2019） Yang Zhou, Zachary While, 和 Evangelos Kalogerakis。2019。SceneGraphNet：用于3D室内场景增强的神经消息传递。在*IEEE计算机视觉大会（ICCV）*中。
- en: Appendix A Scene Description Language
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 场景描述语言
- en: 'As described in Section [4](#S4 "4\. Describing Scenes with Programs ‣ Open-Universe
    Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases"),
    the domain-specific functions we add to Python are either (1) object constructors,
    (2) relation functions or (3) parameter setting functions. The basic object constructor
    is {spverbatim} Object(description: str, width: float, depth: float, height: float,
    facing: Object — int — None = None).'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[4](#S4 "4\. 描述场景与程序 ‣ 使用LLM程序合成和未经整理的物体数据库进行开放宇宙室内场景生成")节所述，我们添加到Python中的特定领域函数包括（1）对象构造函数，（2）关系函数或（3）参数设置函数。基本的对象构造函数是{spverbatim}
    Object(description: str, width: float, depth: float, height: float, facing: Object
    — int — None = None)。'
- en: An object can face either one of the 4 cardinal directions (EAST=0, NORTH=1,
    WEST=2, SOUTH=3), another object, or not face anything. A programmer (a human
    or an LLM) may want not to specify a facing direction, if this direction is not
    important (for example, for a tablet lying on a sofa). In this case the object
    will appear in a scene in a random orientation.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对象可以面向四个基本方向中的任意一个（EAST=0, NORTH=1, WEST=2, SOUTH=3），另一个对象，或不面向任何东西。如果这个方向不重要（例如，一个平躺在沙发上的平板电脑），程序员（人或LLM）可能希望不指定面向方向。在这种情况下，对象将在场景中以随机方向出现。
- en: 'In addition to the default Object constructor, we have special constructors
    for doors and windows: {spverbatim} Door(description: str, width: float, height:
    float, wall: int)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '除了默认的Object构造函数，我们还有针对门和窗的特殊构造函数：{spverbatim} Door(description: str, width:
    float, height: float, wall: int)'
- en: 'Window(description: str, width: float, height: float, wall: int, height_above_ground:
    float, above: Cuboid = None)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 'Window(description: str, width: float, height: float, wall: int, height_above_ground:
    float, above: Cuboid = None)'
- en: Doors and windows are treated as regular objects in our system. The programmer
    can place additional relations on doors and windows. However, doors and windows
    initialize with additional constraints that ensure that (1) doors are always adjacent
    to walls, and windows are always mounted on walls, and (2) there is enough empty
    space in front of a door, so a door can be opened, windows also require empty
    space in front of them.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的系统中，门和窗被视为普通对象。程序员可以在门和窗上设置额外的关系。然而，门和窗初始化时有额外的约束，确保（1）门总是紧邻墙壁，而窗户总是安装在墙上，（2）门前有足够的空地，以便门可以打开，窗户前也需要空地。
- en: 'For some types of objects such as paintings, books or statues, it makes sense
    to retrieve different 3D meshes even for objects that have the same description.
    For this purpose, we have a list-of-objects constructor unique_objects(amount:
    int, description: str, width: float, depth: float, height: float). For consistency,
    we also have an objects(amount: int, description: str, width: float, depth: float,
    height: float) constructor, although it can be replaced with a list comprehension.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '对于某些类型的物体，如绘画、书籍或雕像，即使物体的描述相同，检索不同的3D网格也是有意义的。为此，我们有一个 list-of-objects 构造函数
    unique_objects(amount: int, description: str, width: float, depth: float, height:
    float)。为了保持一致性，我们还有一个 objects(amount: int, description: str, width: float, depth:
    float, height: float) 构造函数，尽管它可以被列表推导式替代。'
- en: 'DSL relation functions are:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: DSL 关系函数包括：
- en: (1)
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'on(top: Object, bottom: Object) means that the first object stands on top of
    the bottom object. If the top object is smaller, it should not extend beyond the
    bottom object.'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'on(top: Object, bottom: Object) 意味着第一个对象站在底部对象的上面。如果顶部对象较小，它不应超出底部对象的范围。'
- en: (2)
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'next_to_wall(a: Object, wall: int, distance: float = 0.0) means that object
    a stands next to one of the 4 walls. If the optional distance argument is zero,
    the object is touching the wall. Otherwise, it stands no more than distance meters
    from the wall. The optional distance parameter allows to fit a chair between a
    table and a wall when a table is standing next to a wall. Using a pair of next_to_wall
    functions, the programmer can express that some object stands in a corner.'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'next_to_wall(a: Object, wall: int, distance: float = 0.0) 意味着对象 a 靠近四面墙中的一面。如果可选的距离参数为零，则对象与墙接触。否则，它与墙的距离不超过指定的距离。可选的距离参数允许在桌子靠近墙壁时，将椅子放置在桌子和墙之间。使用一对
    next_to_wall 函数，程序员可以表达某个物体位于角落的情况。'
- en: (3)
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'mounted_on_wall(a: Object, wall: int, height: float, above: Object = None)
    means that that object a is mounted on a wall height meters above the ground.
    This relation is useful for paintings, mirrors, wall clocks or whiteboards. When
    the optional above argument is used, object a is mounted above some other object.'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'mounted_on_wall(a: Object, wall: int, height: float, above: Object = None)
    意味着对象 a 被安装在离地面 height 米的墙上。这个关系对于绘画、镜子、挂钟或白板非常有用。当使用可选的 above 参数时，对象 a 被安装在某个其他对象上方。'
- en: (4)
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'mounted_on_ceiling(a: Object, above: Object = None) means that object a is
    mounted on a ceiling, optionally above another object. This relation is useful
    for describing fans, projectors or chandeliers.'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'mounted_on_ceiling(a: Object, above: Object = None) 意味着对象 a 被安装在天花板上，可能在另一个对象的上方。这个关系对于描述风扇、投影仪或吊灯非常有用。'
- en: (5)
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: 'adjacent(a: Object, b: Object, arg1: int | float | None = None, arg2: int |
    float | None = None, arg3: float = 0.0) relation can be used with 0, 1 or 2 direction
    arguments, and an optional distance argument:'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'adjacent(a: Object, b: Object, arg1: int | float | None = None, arg2: int |
    float | None = None, arg3: float = 0.0) 关系可以使用 0、1 或 2 个方向参数，以及一个可选的距离参数：'
- en: (a)
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: adjacent(chair, desk) means that the chair’s bounding box touches the desk’s
    bounding box.
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: adjacent(chair, desk) 意味着椅子的边界框接触到桌子的边界框。
- en: (b)
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: adjacent(chair, desk, NORTH) is the most common variant. It means that the chair
    is adjacent to the desk from the NORTH.
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: adjacent(chair, desk, NORTH) 是最常见的变体。它意味着椅子在桌子的北侧相邻。
- en: (c)
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: adjacent(chair, desk, NORTH, WEST) means that chair is adjacent to the desk
    from the north side but is aligned with the west side of the desk. This version
    of the adjacency relation is useful for describing a nightstand that is adjacent
    to a bed from the side, but is aligned with the head of a bed.
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: adjacent(chair, desk, NORTH, WEST) 意味着椅子在桌子的北侧相邻，但与桌子的西侧对齐。这种邻接关系的版本对于描述床旁的床头柜很有用，它从侧面与床相邻，但与床头对齐。
- en: If an optional distance argument is used, the touching requirement is replaced
    with the distance requirement. For example, adjacent(chair, desk, NORTH, 0.2)
    means that the chair is located to the NORTH of the desk, no more than 20cm away
    from the desk.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了可选的距离参数，则触碰要求被距离要求取代。例如，adjacent(chair, desk, NORTH, 0.2) 意味着椅子位于桌子的北侧，距离桌子不超过
    20 厘米。
- en: (6)
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (6)
- en: 'aligned(cuboids: list[Object], axis: bool) means that the centers of a list
    of objects should be aligned either vertically or horizontally. The second argument
    can either be WESTEAST=False or NORTHSOUTH=True. This relation is useful for describing
    careful arrangements of furniture.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'aligned(cuboids: list[Object], axis: bool) 意味着一组物体的中心应该在垂直或水平方向上对齐。第二个参数可以是
    WESTEAST=False 或 NORTHSOUTH=True。这个关系对于描述家具的精确排列非常有用。'
- en: (7)
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (7)
- en: 'facing(a: Object, direction: Object | int) means that the forward vector of
    object a should face either one of the cardinal directions or another object.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'facing(a: Object, direction: Object | int) 意味着对象 a 的前向向量应朝向一个基本方向或另一个对象。'
- en: (8)
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (8)
- en: 'surround(chairs: list[Object], table: Object) is a syntax sugar relation that
    is implemented with adjacent and facing relations. It adds surrounding objects
    (for example, chairs) one-by-one, and picks the sides of the central object (for
    a example, a table) that have the most free space available. This relation respects
    other adjacencies and walls.'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'surround(chairs: list[Object], table: Object) 是一种语法糖关系，它通过相邻和面对关系来实现。它逐一添加周围的对象（例如，椅子），并选择中央对象（例如，一张桌子）具有最多可用空间的一侧。这个关系尊重其他相邻关系和墙壁。'
- en: 'Finally, the DSL has parameter setting functions for setting the size of the
    scene, the floor texture and the wall texture. These functions are called only
    once per scene:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，DSL 具有用于设置场景大小、地板纹理和墙壁纹理的参数设置函数。这些函数每个场景只调用一次：
- en: '[PRE2]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Appendix B Layout Optimizer Details
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 布局优化器详细信息
- en: Here we provide more implementation details for our system’s layout optimizer
    module.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了系统布局优化器模块的更多实现细节。
- en: B.1\. Constraint Losses
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1\. 约束损失
- en: 'Let $a^{size}$. We define constraint losses in the following way:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $a^{size}$。我们定义约束损失如下：
- en: '|  | $1$2 |  |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\mathcal{L}_{\text{HEIGHT}}(a,height)=(a^{min}_{y}-height)^{2},$ |  |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{HEIGHT}}(a,height)=(a^{min}_{y}-height)^{2},$ |  |'
- en: '|  | $1$2 |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\mathcal{L}_{\text{CEILING}}(a)=(a^{max}_{y}-s^{max}_{y})^{2},$ |  |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{CEILING}}(a)=(a^{max}_{y}-s^{max}_{y})^{2},$ |  |'
- en: '|  | $\mathcal{L}_{\text{ALIGNED}}(a,b,\text{WESTEAST})=(a^{center}_{z}-b^{center}_{z})^{2},$
    |  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{ALIGNED}}(a,b,\text{WESTEAST})=(a^{center}_{z}-b^{center}_{z})^{2},$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $A,B=a,b$ otherwise.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A,B=a,b$ 其他情况。
- en: '|  | $\mathcal{L}_{\text{ADJACENT0}}(a,b,dist)=(d(a,b)-dist)^{2},$ |  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{ADJACENT0}}(a,b,dist)=(d(a,b)-dist)^{2},$ |  |'
- en: '|  | $1$2 |  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $$\begin{split}\mathcal{L}_{\text{A}}(a,b,\text{EAST},dist)=\mathrm{relu}^{2}(a^{min}_{x}-b^{max}_{x}-dist)+\\
    +\mathrm{relu}^{2}(b^{max}_{x}-a^{min}_{x})+\mathrm{relu}^{2}(a^{max}_{z}-b^{max}_{z})-\\'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{split}\mathcal{L}_{\text{A}}(a,b,\text{EAST},dist)=\mathrm{relu}^{2}(a^{min}_{x}-b^{max}_{x}-dist)+\\
    +\mathrm{relu}^{2}(b^{max}_{x}-a^{min}_{x})+\mathrm{relu}^{2}(a^{max}_{z}-b^{max}_{z})-\\'
- en: -\frac{1}{2}\mathrm{relu}^{2}(a^{size}_{z}-b^{size}_{z}).\end{split}$$ |  |
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: -\frac{1}{2}\mathrm{relu}^{2}(a^{size}_{z}-b^{size}_{z}).\end{split}$$ |  |
- en: Generalization from EAST and NORTH to other cardinal directions is straightforward.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 从 EAST 和 NORTH 泛化到其他基本方向是简单的。
- en: B.2\. Repel Force Implementation
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2\. 排斥力实现
- en: 'To implement repel forces, we first define a binary connectivity relation on
    the set of objects and walls in the scene: NEXTTOWALL(a, wall) connects object
    a and wall wall, and ON(a,b) or ADJACENT(a, b) connect objects a and b. This relation
    splits the set of objects and walls into connected components. For every pair
    of objects that belong to different connected components (and for pairs of objects
    and walls from different connected components) we add a repelling vector to the
    optimization gradient with magnitude proportional to $\max(1-d/d_{\text{max}},0)$
    the minimum linear size of the scene. We add a small random noise to repel forces
    to escape local minima.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现排斥力，我们首先在场景中的对象和墙壁集合上定义一个二元连通关系：NEXTTOWALL(a, wall) 连接对象 a 和墙壁 wall，ON(a,b)
    或 ADJACENT(a, b) 连接对象 a 和 b。这个关系将对象和墙壁的集合分成了若干个连通组件。对于每一对属于不同连通组件的对象（以及每一对来自不同连通组件的对象和墙壁），我们在优化梯度中添加一个排斥向量，其大小与
    $\max(1-d/d_{\text{max}},0)$ 场景的最小线性尺寸成正比。我们还在排斥力中添加了小的随机噪声，以逃避局部最小值。
