- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:53:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'HLSPilot: LLM-based High-Level Synthesis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'HLSPilot: 基于LLM的高层次综合'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.06810](https://ar5iv.labs.arxiv.org/html/2408.06810)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.06810](https://ar5iv.labs.arxiv.org/html/2408.06810)
- en: Chenwei Xiong^(1,2), Cheng Liu^(1,2)1, Huawei Li^(1,2), Xiaowei Li^(1,2) 1 Corresponding
    author.This work is supported by the National Key R&D Program of China under Grant
    (2022YFB4500405), and the National Natural Science Foundation of China under Grant
    62174162. ¹SKLP, Institute of Computing Technology, Chinese Academy of Sciences,
    Beijing, China ²Dept. of Computer Science, University of Chinese Academy of Sciences,
    Beijing, China {xiongchenwei22s, liucheng}@ict.ac.cn
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 陈伟雄^(1,2)，刘成^(1,2)，李华为^(1,2)，李晓伟^(1,2) 1 通讯作者。本研究得到了中国国家重点研发计划资助（2022YFB4500405）和中国国家自然科学基金资助（62174162）。¹中国科学院计算技术研究所，SKLP，北京，中国
    ²中国科学院大学计算机科学系，北京，中国 {xiongchenwei22s, liucheng}@ict.ac.cn
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have catalyzed an upsurge in automatic code generation,
    garnering significant attention for register transfer level (RTL) code generation.
    Despite the potential of RTL code generation with natural language, it remains
    error-prone and limited to relatively small modules because of the substantial
    semantic gap between natural language expressions and hardware design intent.
    In response to the limitations, we propose a methodology that reduces the semantic
    gaps by utilizing C/C++ for generating hardware designs via High-Level Synthesis
    (HLS) tools. Basically, we build a set of C-to-HLS optimization strategies catering
    to various code patterns, such as nested loops and local arrays. Then, we apply
    these strategies to sequential C/C++ code through in-context learning, which provides
    the LLMs with exemplary C/C++ to HLS prompts. With this approach, HLS designs
    can be generated effectively. Since LLMs still face problems in determining the
    optimized pragma parameters precisely, we have a design space exploration (DSE)
    tool integrated for pragma parameter tuning. Furthermore, we also employ profiling
    tools to pinpoint the performance bottlenecks within a program and selectively
    convert bottleneck components to HLS code for hardware acceleration. By combining
    the LLM-based profiling, C/C++ to HLS translation, and DSE, we have established
    HLSPilot—the first LLM-enabled high-level synthesis framework, which can fully
    automate the high-level application acceleration on hybrid CPU-FPGA architectures.
    According to our experiments on real-world application benchmarks, HLSPilot achieve
    comparable performance in general and can even outperform manually crafted counterparts,
    thereby underscoring the substantial promise of LLM-assisted hardware designs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）催生了自动代码生成的热潮，引起了对寄存器传输级（RTL）代码生成的广泛关注。尽管自然语言生成RTL代码具有潜力，但由于自然语言表达与硬件设计意图之间存在显著的语义差距，它仍然容易出错且仅限于相对较小的模块。为应对这些限制，我们提出了一种通过高层次综合（HLS）工具利用C/C++生成硬件设计的方法，减少语义差距。基本上，我们构建了一套针对各种代码模式（如嵌套循环和局部数组）的C到HLS优化策略。然后，我们通过上下文学习将这些策略应用于顺序C/C++代码，为LLMs提供示例C/C++到HLS提示。通过这种方法，可以有效生成HLS设计。由于LLMs在精确确定优化的pragma参数方面仍面临问题，我们集成了一个设计空间探索（DSE）工具用于pragma参数调优。此外，我们还使用分析工具来定位程序中的性能瓶颈，并选择性地将瓶颈组件转换为HLS代码以加速硬件。通过结合基于LLM的分析、C/C++到HLS翻译和DSE，我们建立了HLSPilot——首个LLM支持的高层次综合框架，能够全面自动化混合CPU-FPGA架构上的高层次应用加速。根据我们在实际应用基准上的实验，HLSPilot在整体上表现相当，并且可以超越手工制作的对照，显示了LLM辅助硬件设计的巨大前景。
- en: 'Index Terms:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: large language model, high-level synthesis, C-to-HLS, Code Generation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型，高层次综合，C到HLS，代码生成。
- en: I Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Hardware designing is a demanding task requiring a high level of expertise.
    Traditional hardware design involves coding with register transfer level (RTL)
    language. However, as the complexity of hardware increases continuously with the
    computing requirements of applications, RTL coding becomes exceedingly time-consuming
    and labor-intensive. The emergence of High-Level Synthesis (HLS) enables hardware
    design at higher abstraction levels [[1](#bib.bib1)]. HLS typically employs high-level
    languages like C/C++ for hardware description, allowing software engineers to
    also engage in hardware development, which significantly lowering the expertise
    barrier in hardware design. Designers can focus more on the applications and algorithms
    rather than the details of low-level hardware implementations. HLS tools automate
    the design tasks such as concurrent analysis of algorithms, interface design,
    logic unit mapping, and data management, thereby substantially shortening the
    hardware design cycle.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件设计是一项要求很高的任务，需要高水平的专业知识。传统的硬件设计涉及使用寄存器传输级（RTL）语言进行编码。然而，随着应用程序计算需求的不断增加，硬件复杂性也在持续增长，RTL编码变得异常耗时且劳动密集。高级综合（HLS）的出现使得在更高抽象层次上进行硬件设计成为可能[[1](#bib.bib1)]。HLS通常使用高层次的语言，如C/C++来描述硬件，使软件工程师也能参与硬件开发，从而显著降低了硬件设计中的专业门槛。设计师可以更多地关注应用和算法，而不是低级硬件实现的细节。HLS工具自动化了设计任务，如算法的并发分析、接口设计、逻辑单元映射和数据管理，从而大大缩短了硬件设计周期。
- en: While HLS offers numerous advantages such as higher development efficiency and
    lower design barriers [[1](#bib.bib1)] [[2](#bib.bib2)], there are still some
    issues in the real-world HLS-based hardware acceleration workflow [[3](#bib.bib3)].
    Firstly, the overall analysis of the program is of great importance, determining
    the performance bottlenecks of the program and the co-design between CPU and FPGA
    remains a challenging issue. Besides, designs based on HLS still encounter a few
    major performance issues [[4](#bib.bib4)] [[5](#bib.bib5)]. Foremost, it still
    requires substantial optimization experience to craft high-quality HLS code and
    achieve desired performance in practical development processes [[6](#bib.bib6)]
    [[7](#bib.bib7)]. In addition, HLS code often struggles to reach optimality due
    to the large design space of various pragma parameters. Some design space exploration
    (DSE) tools have been proposed [[8](#bib.bib8)] [[9](#bib.bib9)] [[10](#bib.bib10)]
    [[11](#bib.bib11)] to automate the parameter tuning, but these tools do not fundamentally
    optimize the hardware design. High-quality HLS design turns out to be the major
    performance challenge from the perspective of general software designers. Some
    researchers have attempted to address this challenge by using pre-built templates
    for specific domain applications [[12](#bib.bib12)] [[13](#bib.bib13)] [[14](#bib.bib14)].
    For example, ThunderGP [[13](#bib.bib13)] has designed a set of HLS-based templates
    for optimized graph processing accelerator generation, allowing designers to implement
    various graph algorithms by filling in the templates. However, it demands comprehensive
    understanding of both the domain knowledge and the HLS development experience
    from designers and there is still a lack of well-established universal solution
    to obtain optimized HLS code. Bridging the gap between C/C++ and HLS remains a
    formidable challenge requiring further efforts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管HLS提供了许多优势，例如更高的开发效率和较低的设计门槛[[1](#bib.bib1)] [[2](#bib.bib2)]，但在实际的HLS基础硬件加速工作流程中仍然存在一些问题[[3](#bib.bib3)]。首先，程序的整体分析至关重要，确定程序的性能瓶颈以及CPU和FPGA的协同设计仍然是一个具有挑战性的问题。此外，基于HLS的设计仍然面临一些主要的性能问题[[4](#bib.bib4)]
    [[5](#bib.bib5)]。最重要的是，仍然需要大量的优化经验来编写高质量的HLS代码，并在实际开发过程中实现预期的性能[[6](#bib.bib6)]
    [[7](#bib.bib7)]。此外，由于各种pragma参数的设计空间较大，HLS代码通常难以达到最优性。一些设计空间探索（DSE）工具已被提出[[8](#bib.bib8)]
    [[9](#bib.bib9)] [[10](#bib.bib10)] [[11](#bib.bib11)]以自动化参数调整，但这些工具并没有从根本上优化硬件设计。高质量的HLS设计被认为是从通用软件设计师的角度来看最大的性能挑战。一些研究者尝试通过使用特定领域应用的预构建模板来解决这一挑战[[12](#bib.bib12)]
    [[13](#bib.bib13)] [[14](#bib.bib14)]。例如，ThunderGP [[13](#bib.bib13)] 设计了一套用于优化图处理加速器生成的HLS基础模板，允许设计师通过填写模板来实现各种图算法。然而，这需要设计师对领域知识和HLS开发经验有全面的理解，并且仍然缺乏成熟的通用解决方案来获得优化的HLS代码。弥合C/C++和HLS之间的差距仍然是一个巨大的挑战，需要进一步的努力。
- en: Large Language Models (LLMs) have recently exhibited remarkable capabilities
    in various generative tasks, including text generation, machine translation, and
    code generation, underscoring their advanced learning and imitation skills. These
    advancements have opened up possibilities for addressing hardware design challenges.
    Researchers have begun applying LLMs to various hardware design tasks, including
    general-purpose processor designs, domain-specific accelerator designs, and arbitrary
    RTL code generation. Among these applications, it can be observed that neural
    network accelerator generation utilizing a predefined template, as reported in
    [[15](#bib.bib15)], reaches an almost 100% success rate. In contrast, generating
    register transfer level (RTL) code from natural language descriptions, such as
    design specifications, experiences a considerably higher failure rate [[16](#bib.bib16)]
    [[17](#bib.bib17)]. This disparity is largely due to the semantic gap between
    inputs and the anticipated outputs. Despite the imperfections, these work have
    demonstrated the great potential of exploring LLMs for hardware designing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）最近在各种生成任务中展示了显著的能力，包括文本生成、机器翻译和代码生成，突显了它们先进的学习和模仿技能。这些进展为解决硬件设计挑战开辟了新的可能性。研究人员已开始将LLMs应用于各种硬件设计任务，包括通用处理器设计、特定领域加速器设计和任意RTL代码生成。在这些应用中，可以观察到利用预定义模板生成神经网络加速器的成功率几乎达到100%，如[[15](#bib.bib15)]所述。相比之下，从自然语言描述（如设计规格）生成寄存器传输级（RTL）代码的失败率明显更高[[16](#bib.bib16)]
    [[17](#bib.bib17)]。这种差异主要由于输入和预期输出之间的语义差距。尽管存在不完善之处，这些工作仍展示了探索LLMs在硬件设计中巨大潜力的价值。
- en: Inspired by prior works, we introduce HLSPilot, an automated framework that
    utilizes LLMs to generate and optimize HLS code from sequential C/C++ code. Instead
    of generating RTL code from natural language directly, HLSPilot mainly leverages
    LLMs to generate the C-like HLS code from C/C++ with much narrower semantic gap
    and outputs RTL code eventually using established HLS tools. Essentially, HLSPilot
    accomplishes RTL code generation from C/C++ without imposing hardware design tasks
    with broad semantic gap on LLMs. Specifically, HLSPilot initiates the process
    with runtime profiling to pinpoint code segments that are the performance bottleneck
    and require optimization. Subsequently, HLSPilot extracts the kernel code segments
    and applies appropriate HLS optimization strategies to the computing kernels to
    generate optimized HLS code. Then, HLSPilot employs a design space exploration
    (DSE) tool to fine-tune the parameters of the generated HLS design. Finally, HLSPilot
    leverages Xilinx OpenCL APIs to offload the compute kernels to the FPGA, facilitating
    the deployment of the entire algorithm on a hybrid CPU-FPGA architecture. In summary,
    LLMs are utilized for the hardware acceleration throughout the entire hardware
    acceleration workflow ranging from profiling, HW/SW partitioning, HLS code generation,
    HLS code optimization, and tool usage, thereby achieving a high degree of design
    automation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前工作的启发下，我们介绍了HLSPilot，一个利用LLMs从顺序C/C++代码生成和优化HLS代码的自动化框架。HLSPilot不是直接从自然语言生成RTL代码，而是主要利用LLMs从C/C++生成类似C的HLS代码，缩小了语义差距，并最终使用已建立的HLS工具生成RTL代码。本质上，HLSPilot实现了从C/C++生成RTL代码，而无需将具有广泛语义差距的硬件设计任务施加给LLMs。具体而言，HLSPilot通过运行时分析来启动过程，以确定需要优化的性能瓶颈代码段。随后，HLSPilot提取内核代码段，并对计算内核应用适当的HLS优化策略，以生成优化的HLS代码。然后，HLSPilot使用设计空间探索（DSE）工具来微调生成的HLS设计的参数。最后，HLSPilot利用Xilinx
    OpenCL API将计算内核卸载到FPGA，从而促进整个算法在混合CPU-FPGA架构上的部署。总之，LLMs在整个硬件加速工作流中被用于从分析、HW/SW分区、HLS代码生成、HLS代码优化到工具使用，从而实现了高水平的设计自动化。
- en: 'The major contributions of this work are summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose HLSPilot, the first automatic HLS code generation and optimization
    framework from sequential C/C++ code using LLM. This framework investigates the
    use of LLM for HLS design strategy learning and tool learning, and build a complete
    hardware acceleration workflow ranging from runtime profiling, kernel identification,
    automatic HLS code generation, design space exploration, and HW/SW co-design on
    a hybrid CPU-FPGA computing architecture. The framework is open sourced on Github¹¹1[https://github.com/xcw-1010/HLSPilot](https://github.com/xcw-1010/HLSPilot).
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 HLSPilot，首个基于 LLM 从串行 C/C++ 代码生成和优化 HLS 代码的自动化框架。该框架研究了 LLM 在 HLS 设计策略学习和工具学习中的应用，并建立了一个完整的硬件加速工作流程，包括运行时分析、内核识别、自动
    HLS 代码生成、设计空间探索和在混合 CPU-FPGA 计算架构上的 HW/SW 协同设计。该框架已在 Github¹¹1[https://github.com/xcw-1010/HLSPilot](https://github.com/xcw-1010/HLSPilot)
    上开源。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a retrieval based approach to learn the HLS optimization techniques
    and examples from Xilinx user manual and utilize an in-context learning approach
    to apply the learned HLS optimizations on serial C/C++ code and generate optimized
    HLS code with LLM for various computing kernels.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种基于检索的方法来学习来自 Xilinx 用户手册的 HLS 优化技术和示例，并利用上下文学习方法将学习到的 HLS 优化应用于串行 C/C++
    代码，并通过 LLM 为各种计算内核生成优化的 HLS 代码。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: According to our experiments on an HLS benchmark, HLSPilot can generate optimized
    HLS code from sequential C/C++ code and the resulting designs can outperform manual
    optimizations with the assistance of DSE tools in most cases. In addition, we
    also demonstrate the successful use of HLSPilot as a complete hardware acceleration
    workflow on a hybrid CPU-FPGA architecture with a case study.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们在 HLS 基准测试中的实验，HLSPilot 能够从串行 C/C++ 代码生成优化的 HLS 代码，并且在大多数情况下，生成的设计可以超越人工优化，借助
    DSE 工具。此外，我们还展示了 HLSPilot 作为完整硬件加速工作流程在混合 CPU-FPGA 架构上的成功应用案例。
- en: II Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: II-A LLM for Hardware Design
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A LLM 在硬件设计中的应用
- en: Recent works have begun to utilize LLMs to assist the hardware designing from
    different angles [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [16](#bib.bib16), [15](#bib.bib15),
    [25](#bib.bib25)]. Generating RTL code with natural language is a typical approach
    of hardware design with LLMs. For instance, VGen [[18](#bib.bib18)] leverages
    an open-source LLM, CodeGen [[26](#bib.bib26)], fine-tuned with Verilog code corpus
    to generate Verilog code. Similarly, VerilogEval [[19](#bib.bib19)] enhances the
    LLM’s capability to generate Verilog by constructing a supervised fine-tuning
    dataset, it also establishes a benchmark for evaluating LLM’s performance. ChipChat
    [[24](#bib.bib24)] achieves an 8-bit accumulator-based microprocessor design through
    multi-round natural language conversation. ChipGPT [[16](#bib.bib16)] proposes
    a four-stage zero-code logic design framework based on GPT for hardware design.
    These studies have successfully applied LLMs to practical hardware designing.
    However, these methods are mostly limited to small functional modules and the
    success rate drops substantially when the hardware design gets larger. GPT4AIGchip
    proposed in [[15](#bib.bib15)] can also leverage LLMs to generate efficient AI
    accelerators based on a hardware template, but it relies on pre-built hardware
    library that requires intensive understanding of both the domain knowledge and
    the hardware design techniques. which can hinders its use by software developers.Recently,
    a domain-specific LLM for chip design, ChipNeMo [[17](#bib.bib17)], was proposed.
    ChipNeMo employs a series of domain-adaptive techniques to train the LLM capable
    of generating RTL code, writing EDA tool scripts, and summarizing bugs. While
    powerful, domain-specific LLMs face challenges such as high training costs and
    difficulties in data collection.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究已经开始从不同角度利用LLMs来辅助硬件设计 [[18](#bib.bib18)、[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)、[22](#bib.bib22)、[23](#bib.bib23)、[24](#bib.bib24)、[16](#bib.bib16)、[15](#bib.bib15)、[25](#bib.bib25)]。使用自然语言生成RTL代码是硬件设计中利用LLMs的典型方法。例如，VGen
    [[18](#bib.bib18)] 利用开源LLM CodeGen [[26](#bib.bib26)]，通过用Verilog代码语料进行微调来生成Verilog代码。同样，VerilogEval
    [[19](#bib.bib19)] 通过构建一个有监督的微调数据集来增强LLM生成Verilog的能力，它还建立了一个评估LLM性能的基准。ChipChat
    [[24](#bib.bib24)] 通过多轮自然语言对话实现了基于8位累加器的微处理器设计。ChipGPT [[16](#bib.bib16)] 提出了一个基于GPT的四阶段零代码逻辑设计框架用于硬件设计。这些研究已经成功地将LLMs应用于实际的硬件设计。然而，这些方法大多限于小型功能模块，当硬件设计规模增大时，成功率会显著下降。[[15](#bib.bib15)]
    提出的GPT4AIGchip 也可以利用LLMs基于硬件模板生成高效的AI加速器，但它依赖于需要对领域知识和硬件设计技术有深入理解的预构建硬件库，这可能会阻碍其被软件开发人员使用。最近，提出了一个用于芯片设计的领域特定LLM
    ChipNeMo [[17](#bib.bib17)]。ChipNeMo 采用一系列领域自适应技术来训练能够生成RTL代码、编写EDA工具脚本和总结错误的LLM。尽管功能强大，领域特定的LLMs面临着高培训成本和数据收集困难等挑战。
- en: II-B LLM for Code Generation
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B LLM用于代码生成
- en: Code generation is one of the key applications of LLMs. A number of domain-specific
    LLMs such as CodeGen [[26](#bib.bib26)], CodeX [[27](#bib.bib27)], and CodeT5
    [[28](#bib.bib28)] have been proposed to address the programming of popular languages
    such as C/C++, Python, and Java, which have a large number of corpus for pre-training
    and fine-tuning. In contrast, it can be challenging to collect sufficient corpus
    for the less popular languages. VGen [[18](#bib.bib18)] collected and filtered
    Verilog corpus from Github and textbooks, obtaining only hundreds of MB of corpus.
    Hence, prompt engineering in combination with in-context learning provides an
    attractive approach to leverage LLMs to generate code for domain-specific languages.
    For instance, the authors in [[29](#bib.bib29)] augment code generation by providing
    the language’s Backus–Naur form (BNF) grammar within prompts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成是LLMs的关键应用之一。为了处理流行语言如C/C++、Python和Java的编程，已经提出了许多领域特定的LLMs，如CodeGen [[26](#bib.bib26)]、CodeX
    [[27](#bib.bib27)] 和 CodeT5 [[28](#bib.bib28)]，这些语言有大量的语料用于预训练和微调。相比之下，为不那么流行的语言收集足够的语料可能会具有挑战性。VGen
    [[18](#bib.bib18)] 从Github和教科书中收集并过滤了Verilog语料，只获得了数百MB的语料。因此，结合提示工程和上下文学习提供了一种利用LLMs生成领域特定语言代码的有吸引力的方法。例如，[[29](#bib.bib29)]
    的作者通过在提示中提供语言的巴科斯-诺尔范式（BNF）语法来增强代码生成。
- en: III HLSPilot Framework
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III HLSPilot框架
- en: The remarkable achievements of LLMs across a wide domain of applications inspire
    us to create an LLM-driven automatic hardware acceleration design framework tailored
    for a hybrid CPU-FPGA architecture. Unlike previous efforts that primarily focused
    on code generation, our objective is to harness the potential of LLMs to emulate
    the role of an expert engineer in hardware acceleration. Given that hardware acceleration
    on a hybrid CPU-FPGA architecture demands a set of different design tasks such
    as runtime profiling, compute kernel identification, compute kernel acceleration,
    design space exploration, and CPU-FPGA co-design, LLMs must understand the design
    guidelines and manipulate the relevant design tools to achieve the desired design
    objectives, akin to an engineer. Fortunately, LLMs have exhibited powerful capabilities
    in document comprehension, in-context learning, tool learning, and code generation,
    all of which align perfectly with the hardware acceleration design requirements.
    The intended design framework eventually provides an end-to-end high-level synthesis
    of sequential C/C++ code on a hybrid CPU-FPGA architecture, thus named as HLSPilot,
    which will be elaborated in the rest of this section.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 在广泛应用领域的卓越成就激励我们创建一个基于 LLM 的自动硬件加速设计框架，该框架针对混合 CPU-FPGA 架构量身定制。与以前主要集中在代码生成的努力不同，我们的目标是利用
    LLM 的潜力模拟硬件加速领域专家工程师的角色。考虑到混合 CPU-FPGA 架构上的硬件加速需要一系列不同的设计任务，如运行时剖析、计算内核识别、计算内核加速、设计空间探索以及
    CPU-FPGA 协同设计，LLM 必须理解设计指南并操作相关设计工具以实现预期的设计目标，类似于工程师。幸运的是，LLM 展现了强大的文档理解、上下文学习、工具学习和代码生成能力，这些能力与硬件加速设计要求完美契合。最终的设计框架提供了对混合
    CPU-FPGA 架构上顺序 C/C++ 代码的端到端高层次综合，因此命名为 HLSPilot，接下来的部分将详细说明。
- en: III-A HLSPilot Overview
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A HLSPilot 概述
- en: '![Refer to caption](img/209a251f80a60fd19dc73821a00f9bb3.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/209a251f80a60fd19dc73821a00f9bb3.png)'
- en: 'Figure 1: HLSPilot framework'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：HLSPilot 框架
- en: 'HLSPilot as presented in Fig. [1](#S3.F1 "Figure 1 ‣ III-A HLSPilot Overview
    ‣ III HLSPilot Framework ‣ HLSPilot: LLM-based High-Level Synthesis") takes sequential
    C/C++ code as design input and it mainly includes five major processing stages
    to generate optimized hardware acceleration solution on a hybrid CPU-FPGA architecture.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [1](#S3.F1 "图 1 ‣ III-A HLSPilot 概述 ‣ III HLSPilot 框架 ‣ HLSPilot: 基于 LLM
    的高层次综合") 所示，HLSPilot 以顺序 C/C++ 代码作为设计输入，主要包括五个主要处理阶段，以在混合 CPU-FPGA 架构上生成优化的硬件加速解决方案。'
- en: Firstly, HLSPilot conducts runtime profiling on the high-level application code
    to identify the most time-consuming computing kernels, which will be the focus
    of subsequent optimization. In this work, we profile the target algorithm and
    analyze the execution time with gprof on a CPU system. Then, a detailed performance
    report will be generated as needed. With the report, we can obtain the performance
    information such as execution time distribution across the algorithm and the number
    of function calls conveniently. Since LLMs is capable to understand and summarize
    the textual reports, the time-consuming functions can be identified conveniently.
    HLSPilot extracts the computing kernels to be optimized in next stage based on
    these profiling information.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，HLSPilot 对高层应用代码进行运行时剖析，以识别最耗时的计算内核，这些内核将成为后续优化的重点。在这项工作中，我们对目标算法进行剖析，并在
    CPU 系统上使用 gprof 分析执行时间。然后，根据需要生成详细的性能报告。通过报告，我们可以方便地获得如算法执行时间分布和函数调用次数等性能信息。由于
    LLM 能够理解并总结文本报告，因此可以方便地识别耗时的函数。HLSPilot 基于这些剖析信息提取待优化的计算内核。
- en: 'Secondly, the computing kernels are organized as dependent tasks and pipelined
    accordingly. The dependent tasks can be implemented efficiently with the data
    flow mechanism supported by Xilinx HLS. While the compute kernels can be irregular,
    we propose a program-tree-based strategy to refactor the program structure of
    the compute kernels and generate an optimized task flow graph while ensuring equivalent
    code functionality. Details of the automatic task pipelining will be illustrated
    in Section [III-B](#S3.SS2 "III-B Program-Tree-based Task Pipelining ‣ III HLSPilot
    Framework ‣ HLSPilot: LLM-based High-Level Synthesis").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，计算内核被组织为依赖任务，并相应地进行流水线处理。依赖任务可以通过 Xilinx HLS 支持的数据流机制高效实现。虽然计算内核可能是不规则的，我们提出了一种基于程序树的策略来重构计算内核的程序结构，并生成优化的任务流图，同时确保代码功能等效。自动任务流水线的详细信息将在第
    [III-B](#S3.SS2 "III-B Program-Tree-based Task Pipelining ‣ III HLSPilot Framework
    ‣ HLSPilot: LLM-based High-Level Synthesis") 节中说明。'
- en: 'Thirdly, we start to optimize each task with HLS independently. While there
    are many distinct HLS optimization strategies applicable to different high-level
    code patterns, we create a set of HLS optimization strategies based on Xilinx
    HLS user guide and leverage LLMs to select and apply the appropriate optimization
    strategies automatically based on the code patterns in each task. Details of the
    LLM-based automatic HLS optimization will be presented in Section [III-C](#S3.SS3
    "III-C LLM-based Automatic HLS Optimization ‣ III HLSPilot Framework ‣ HLSPilot:
    LLM-based High-Level Synthesis").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '第三，我们开始独立地用 HLS 优化每个任务。虽然有许多适用于不同高级代码模式的独特 HLS 优化策略，我们基于 Xilinx HLS 用户指南创建了一套
    HLS 优化策略，并利用 LLMs 根据每个任务中的代码模式自动选择和应用适当的优化策略。基于 LLM 的自动 HLS 优化的详细信息将在第 [III-C](#S3.SS3
    "III-C LLM-based Automatic HLS Optimization ‣ III HLSPilot Framework ‣ HLSPilot:
    LLM-based High-Level Synthesis") 节中介绍。'
- en: Fourthly, after the code refactoring and the application of various HLS pragmas,
    the HLS code can be obtained, but the parameters such as the initiation interval
    (II) for pipelining, the factors of loop unrolling, and the size for array partitioning
    in the HLS code still needs to be tuned to produce accelerators with higher performance.
    However, it remains rather challenging for LLMs to decide design parameters of
    a complex design precisely. To address this issue, HLSPilot utilizes external
    tools to conduct the design space exploration and decides the optimized solution
    automatically. According to recent research [[30](#bib.bib30)], LLMs is capable
    to learn and utilize external APIs and tools efficiently. Hence, HLSPilot leverages
    LLMs to extract the parameters from HLS code and invoke the DSE tool proposed
    in [[31](#bib.bib31)] by generating the corresponding execution scripts.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，经过代码重构和各种 HLS pragmas 的应用，可以得到 HLS 代码，但 HLS 代码中的诸如流水线的启动间隔 (II)、循环展开因子和数组分区大小等参数仍需调优，以产生性能更高的加速器。然而，对于
    LLM 来说，准确地决定复杂设计的设计参数仍然相当具有挑战性。为了解决这个问题，HLSPilot 利用外部工具进行设计空间探索，并自动决定优化方案。根据最近的研究[[30](#bib.bib30)]，LLMs
    能够有效地学习和利用外部 API 和工具。因此，HLSPilot 利用 LLMs 从 HLS 代码中提取参数，并通过生成相应的执行脚本调用 [[31](#bib.bib31)]
    中提出的 DSE 工具。
- en: Finally, when the compute kernels are optimized with HLS, they can be compiled
    and deployed on FPGAs for hardware acceleration. Nonetheless, these accelerators
    must be integrated with a host processor to provide a holistic hardware acceleration
    solution. The acceleration system has both host code and device code that will
    be executed on CPU side and FPGA side respectively. HLSPilot leverages LLMs to
    learn the APIs provided by Xilinx runtime (XRT) to manage the FPGA-based accelerators
    and perform the data transfer between host memory and FPGA device memory. Then,
    it generates the host code mostly based on the original algorithm code and replaces
    the compute kernels with the compute APIs that will invoke the FPGA accelerators
    and the data movement APIs. The device code is mainly the HLS code generated in
    prior steps. With both the host code and device code, the entire algorithm can
    be deployed on the hybrid CPU-FPGA architecture.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，当计算内核使用HLS优化后，它们可以被编译并部署到FPGA上以实现硬件加速。然而，这些加速器必须与主处理器集成，以提供全面的硬件加速解决方案。加速系统包含主机代码和设备代码，分别在CPU端和FPGA端执行。HLSPilot利用LLMs学习Xilinx运行时（XRT）提供的API，以管理基于FPGA的加速器，并在主机内存和FPGA设备内存之间进行数据传输。然后，它生成主要基于原始算法代码的主机代码，并用计算API替换计算内核，这些API将调用FPGA加速器和数据移动API。设备代码主要是前面步骤生成的HLS代码。通过主机代码和设备代码，整个算法可以在混合CPU-FPGA架构上部署。
- en: III-B Program-Tree-based Task Pipelining
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于程序树的任务流水线
- en: While the compute kernel can be quite complex, it needs to be split into multiple
    tasks for the sake of potential pipelining or parallel processing, which is critical
    to the performance of the generated accelerator. However, it is difficult to split
    the compute kernel appropriately because inappropriate splitting may lead to imbalanced
    pipelining and low performance. In addition, the splitting usually causes code
    refactoring, which may produce code with inconsistent functionality and further
    complicate the problem. To address this problem, we propose a program-tree-based
    strategy to guide LLM to produce fine-grained task splitting and pipelining.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算内核可能非常复杂，但为了潜在的流水线或并行处理，它需要被拆分成多个任务，这对于生成的加速器的性能至关重要。然而，适当地拆分计算内核是困难的，因为不恰当的拆分可能导致流水线不平衡和性能低下。此外，拆分通常会导致代码重构，可能会产生功能不一致的代码，进一步复杂化问题。为了解决这个问题，我们提出了一种基于程序树的策略，以指导LLM进行细粒度的任务拆分和流水线设计。
- en: 'Input: Top-level Function Code COutput: Tasks Collection $T=\{task_{1},task_{2},\dots,task_{n}\}$17      18
    end while'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：顶级函数代码 C 输出：任务集合 $T=\{task_{1},task_{2},\dots,task_{n}\}$17      18 结束循环
- en: Algorithm 1 Program-tree-based Pipelining Strategy
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 基于程序树的流水线策略
- en: 'The proposed program-tree based task pipelining strategy is detailed in Algorithm
    [1](#algorithm1 "In III-B Program-Tree-based Task Pipelining ‣ III HLSPilot Framework
    ‣ HLSPilot: LLM-based High-Level Synthesis"). According to the strategy, LLM iteratively
    decomposes the compute kernel to smaller tasks and form a tree structure eventually.
    An input compute kernel $C$ directly.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '提出的基于程序树的任务流水线策略详见算法 [1](#algorithm1 "在 III-B 程序树基础的任务流水线 ‣ III HLSPilot 框架
    ‣ HLSPilot: 基于LLM的高层次综合")。根据该策略，LLM 迭代地将计算内核分解为更小的任务，并最终形成树结构。输入计算内核 $C$ 直接。'
- en: The major challenge of the program-tree-based task pipelining strategy is the
    task decomposition metric which depends on the code structures and can vary substantially.
    As a result, the metric can be difficult to quantify. Instead of using a determined
    quantitative metric, we leverage LLMs to perform the task decomposition with natural
    language rules and typical decomposition examples. Specifically, for non-loop
    code, we have LLM to analyze the semantics of code statements, recognize the purpose
    of these statements, and group statements performing the same function into a
    single task. For loop code, the decomposition is primarily based on the smallest
    loop granularity that can be executed in parallel. We take advantage of the in-context
    learning capabilities of LLMs and present a few representative decomposition examples
    to guide the task decomposition for general scenarios. These examples as detailed
    as follows.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 基于程序树的任务流水线策略的主要挑战是任务分解度量，这取决于代码结构，并且可能变化很大。因此，度量可能难以量化。我们并不使用确定的定量度量，而是利用 LLM
    执行基于自然语言规则和典型分解示例的任务分解。具体来说，对于非循环代码，我们使用 LLM 分析代码语句的语义，识别这些语句的目的，并将执行相同功能的语句分组为一个任务。对于循环代码，分解主要基于可以并行执行的最小循环粒度。我们利用
    LLM 的上下文学习能力，并提供一些具有代表性的分解示例，以指导一般场景下的任务分解。这些示例如下所示。
- en: III-B1 Each iteration of the loop is considered as a task
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 循环的每次迭代被视为一个任务。
- en: In the original merge sort loop, each iteration processes all intervals of the
    same width. Therefore, each iteration can be regarded as a task. For example,
    $task_{i}$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的归并排序循环中，每次迭代处理所有相同宽度的区间。因此，每次迭代可以视为一个任务。例如，$task_{i}$。
- en: //  before:for  (int  width  =  1;  width  <  SIZE;  width  =  2  *  width)  {for  (int  i1  =  0;  i1  <  SIZE;  i1  =  i1  +  2  *  width)  {int  i2  =  i1  +  width;int  i3  =  i1  +  2  *  width;if  (i2  >=  SIZE)  i2  =  SIZE;if  (i3  >=  SIZE)  i3  =  SIZE;merge(A,  i1,  i2,  i3,  temp);}}//  after:for  (int  stage  =  1;  stage  <  STAGES  -  1;  stage++)  {//  merge  all  equally  wide  intervalsmerge_intervals(temp[stage  -  1],  width,  temp[stage]);width  *=  2;}
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: //  之前:for  (int  width  =  1;  width  <  SIZE;  width  =  2  *  width)  {for  (int  i1  =  0;  i1  <  SIZE;  i1  =  i1  +  2  *  width)  {int  i2  =  i1  +  width;int  i3  =  i1  +  2  *  width;if  (i2  >=  SIZE)  i2  =  SIZE;if  (i3  >=  SIZE)  i3  =  SIZE;merge(A,  i1,  i2,  i3,  temp);}}//  之后:for  (int  stage  =  1;  stage  <  STAGES  -  1;  stage++)  {//  合并  所有  宽度相等  的区间merge_intervals(temp[stage  -  1],  width,  temp[stage]);width  *=  2;}
- en: III-B2 The first and second halves of a loop’s traversal are each considered
    as a task
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 循环遍历的前半部分和后半部分各自被视为一个任务。
- en: In histogram statistics, since the first and second halves of the loop can be
    executed in parallel, they are considered as two tasks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在直方图统计中，由于循环的前半部分和后半部分可以并行执行，因此它们被视为两个任务。
- en: //  before:for  (int  i  =  0;  i  <  INPUT_SIZE;  i++)  {val  =  in[i];hist[val]  =  hist[val]  +  1;}//  after:for  (int  i  =  0;  i  <  INPUT_SIZE  /  2;  i++)  {val  =  in1[i];hist1[val]  =  hist1[val]  +  1;}for  (int  i  =  0;  i  <  INPUT_SIZE  /  2;  i++)  {val  =  in2[i];hist2[val]  =  hist2[val]  +  1;}histogram_reduce(hist1,  hist2,  hist);
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: //  之前:for  (int  i  =  0;  i  <  INPUT_SIZE;  i++)  {val  =  in[i];hist[val]  =  hist[val]  +  1;}//  之后:for  (int  i  =  0;  i  <  INPUT_SIZE  /  2;  i++)  {val  =  in1[i];hist1[val]  =  hist1[val]  +  1;}for  (int  i  =  0;  i  <  INPUT_SIZE  /  2;  i++)  {val  =  in2[i];hist2[val]  =  hist2[val]  +  1;}histogram_reduce(hist1,  hist2,  hist);
- en: III-B3 Each level of a loop is considered as a task
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 循环的每一层被视为一个任务。
- en: In BFS algorithm, there are two loops, with the first loop used to find the
    frontier vertex and read the corresponding rpao data, the second loop used to
    traverse the neighbors of the frontier vertex, which can be divided into two tasks
    based on this.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BFS 算法中，有两个循环，第一个循环用于查找前沿顶点并读取相应的 rpao 数据，第二个循环用于遍历前沿顶点的邻居，根据这一点可以将其分为两个任务。
- en: '//  before:loop1:  for  (int  i  =  0;  i  <  vertex_num;  i++)  {char  d  =  depth[i];if  (d  ==  level)  {start  =  rpao[i];end  =  rpao[i  +  1];loop2:  for  (int  j  =  start;  j  <  end;  j++)  {ngb_vidx  =  ciao[j];ngb_depth  =  depth[ngb_vidx];if  (ngb_depth  ==  -1)  {depth[ngb_vidx]  =  level_plus1;}}}}//  after:void  read_frontier_vertex(int  *depth,  int  vertex_num,  int  level,  int  *rpao,  ...)  {...for  (int  i  =  0;  i  <  vertex_num;  i++)  {if  (depth[i]  ==  level)  {int  start  =  rpao[i];int  end  =  rpao[i  +  1];start_stream  <<  start;end_stream  <<  end;}}}void  traverse(hls::stream&  start_stream,  hls::stream&  end_stream,  ...)  {...while  (!start_stream.empty()  &&  !end_stream.empty())  {int  start  =  start_stream.read();int  end  =  end_stream.read();for  (int  j  =  start;  j  <  end;  j++)  {ngb_vidx  =  ciao[j];ngb_depth  =  depth[ngb_vidx];if  (ngb_depth  ==  -1)  {depth[ngb_vidx]  =  level_plus1;}}}}'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '// before:loop1: for (int i = 0; i < vertex_num; i++) {char d = depth[i];if
    (d == level) {start = rpao[i];end = rpao[i + 1];loop2: for (int j = start; j <
    end; j++) {ngb_vidx = ciao[j];ngb_depth = depth[ngb_vidx];if (ngb_depth == -1)
    {depth[ngb_vidx] = level_plus1;}}}}// after:void read_frontier_vertex(int *depth,
    int vertex_num, int level, int *rpao, ...) {...for (int i = 0; i < vertex_num;
    i++) {if (depth[i] == level) {int start = rpao[i];int end = rpao[i + 1];start_stream
    << start;end_stream << end;}}}void traverse(hls::stream& start_stream, hls::stream&
    end_stream, ...) {...while (!start_stream.empty() && !end_stream.empty()) {int
    start = start_stream.read();int end = end_stream.read();for (int j = start; j
    < end; j++) {ngb_vidx = ciao[j];ngb_depth = depth[ngb_vidx];if (ngb_depth == -1)
    {depth[ngb_vidx] = level_plus1;}}}}'
- en: III-B4 Multiple levels of loops are considered as a task
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 多级循环被视为一个任务
- en: In video frame image convolution, there are a total of 4 layers of loops, where
    loop1 and loop2 are considered as the tasks for reading the pixel, and loop3 and
    loop4 are the tasks for calculating the convolution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频帧图像卷积中，总共有 4 层循环，其中 loop1 和 loop2 被视为读取像素的任务，而 loop3 和 loop4 是计算卷积的任务。
- en: '//  before:loop1:  for(int  line=0;  line=  0  &&  ii  <  img_h  &&  jj  >=  0  &&  jj  <  img_w)  {sum_r  +=  in[(ii  *  img_w)  +  jj].r  *  coeff[(m  *  coeff_size)  +  n];sum_g  +=  in[(ii  *  img_w)  +  jj].g  *  coeff[(m  *  coeff_size)  +  n];sum_b  +=  in[(ii  *  img_w)  +  jj].b  *  coeff[(m  *  coeff_size)  +  n];}...}//  after:void  read_dataflow(hls::stream&  read_stream,  const  RGBPixel  *in,  int  img_w,  int  elements,  int  half)  {int  pixel  =  0;while(elements--)  {read_stream  <<  in[pixel++];}...}void  compute_dataflow(hls::stream&  write_stream,  hls::stream&  read_stream,  const  float*  coefficient,  int  img_width,  int  elements,  int  center)  {static  RGBPixel  window_mem[COEFFICIENT_SIZE][MAX_WIDTH];static  fixed  coef[COEFFICIENT_SIZE  *  COEFFICIENT_SIZE];for(int  i  =  0;  i  <  COEFFICIENT_SIZE*COEFFICIENT_SIZE;  i++)  {coef[i]  =  coefficient[i];}...}![Refer
    to caption](img/dbc01ac6dba733cac57f3bb31cc20af0.png)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '// before:loop1:  for(int line=0; line= 0 && ii < img_h && jj >= 0 && jj <
    img_w) {sum_r += in[(ii * img_w) + jj].r * coeff[(m * coeff_size) + n];sum_g +=
    in[(ii * img_w) + jj].g * coeff[(m * coeff_size) + n];sum_b += in[(ii * img_w)
    + jj].b * coeff[(m * coeff_size) + n];}...}// after:void read_dataflow(hls::stream&
    read_stream, const RGBPixel *in, int img_w, int elements, int half) {int pixel
    = 0;while(elements--) {read_stream << in[pixel++];}...}void compute_dataflow(hls::stream&
    write_stream, hls::stream& read_stream, const float* coefficient, int img_width,
    int elements, int center) {static RGBPixel window_mem[COEFFICIENT_SIZE][MAX_WIDTH];static
    fixed coef[COEFFICIENT_SIZE * COEFFICIENT_SIZE];for(int i = 0; i < COEFFICIENT_SIZE*COEFFICIENT_SIZE;
    i++) {coef[i] = coefficient[i];}...}![参见标题](img/dbc01ac6dba733cac57f3bb31cc20af0.png)'
- en: 'Figure 2: An example of program tree construction. LLM divides BFS with nested
    loop into multiple dependent tasks for the pipelined execution.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：程序树构造的一个示例。LLM 将带有嵌套循环的 BFS 分成多个依赖任务，以便进行流水线执行。
- en: 'In order to demonstrate the proposed task decomposition strategy, we take BFS
    with relatively complex nested loop as an example and present the generated program
    tree in Fig.[2](#S3.F2 "Figure 2 ‣ III-B4 Multiple levels of loops are considered
    as a task ‣ III-B Program-Tree-based Task Pipelining ‣ III HLSPilot Framework
    ‣ HLSPilot: LLM-based High-Level Synthesis"). It shows that the nested loop in
    BFS are effectively identified and extracted as dependent tasks correctly.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '为了演示提出的任务分解策略，我们以相对复杂的嵌套循环 BFS 为例，并在图[2](#S3.F2 "Figure 2 ‣ III-B4 多级循环被视为任务
    ‣ III-B 基于程序树的任务流水线 ‣ III HLSPilot 框架 ‣ HLSPilot: 基于 LLM 的高级合成")中展示了生成的程序树。它显示了
    BFS 中的嵌套循环被有效地识别并正确提取为依赖任务。'
- en: When the tasks are decomposed, the corresponding code segments will be packed
    into a function and the code needs to be refactored accordingly. Before proceeding
    to the HLS acceleration, HLSPilot needs to check the correctness of the refactored
    code. Specifically, we compare the refactored code to the original code by testing
    the execution results to ensure the computing results are consistent. We follow
    a bottom-up testing strategy and start from the leaf nodes of the program tree.
    If an error occurs, it can be traced back to the erroneous leaf node and check
    from its parent node. If errors persist across multiple attempts, the program
    tree is backtracked and the parent node is considered as the final refactored
    result.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务被分解后，相应的代码段将被打包到一个函数中，并且代码需要进行相应的重构。在进行 HLS 加速之前，HLSPilot 需要检查重构后的代码的正确性。具体来说，我们通过测试执行结果来比较重构后的代码与原始代码，以确保计算结果的一致性。我们遵循自下而上的测试策略，从程序树的叶节点开始。如果出现错误，可以追溯到错误的叶节点，并从其父节点检查。如果在多次尝试后错误仍然存在，程序树将回溯，父节点将被视为最终的重构结果。
- en: III-C LLM-based Automatic HLS Optimization
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 基于 LLM 的自动 HLS 优化
- en: After the task pipelining, we continue to apply appropriate HLS optimization
    strategies to these tasks. The HLS optimization strategies are mainly extracted
    from Vendor’s documentation [[32](#bib.bib32)] [[33](#bib.bib33)] [[34](#bib.bib34)]
    by LLM. Since the optimizations are usually limited to specific scenarios or code
    patterns, there are a number of distinct strategies but only a few of them may
    be actually utilized for a specific compute kernel in practice. To facilitate
    the automatic HLS optimization, we build an HLS optimization strategy knowledge
    base and propose a Retrieval-Augmented-Generation-like (RAG-like) strategy to
    select the most suitable optimization strategies from knowledge base. The selected
    optimization strategies will be applied to the target code through in-context
    learning, ensuring optimized HLS code generation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务流水线处理后，我们继续对这些任务应用适当的 HLS 优化策略。这些 HLS 优化策略主要是从供应商的文档中提取的[[32](#bib.bib32)]
    [[33](#bib.bib33)] [[34](#bib.bib34)]，由 LLM 完成。由于优化通常局限于特定的场景或代码模式，因此有许多不同的策略，但在实际应用中可能只有少数策略会被用于特定的计算内核。为了便于自动化
    HLS 优化，我们建立了一个 HLS 优化策略知识库，并提出了一种类似于检索增强生成（RAG-like）的策略，从知识库中选择最适合的优化策略。所选的优化策略将通过上下文学习应用于目标代码，从而确保优化的
    HLS 代码生成。
- en: '![Refer to caption](img/85efc7372ea8d207f613a8cbea21d8ca.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/85efc7372ea8d207f613a8cbea21d8ca.png)'
- en: 'Figure 3: Automatic Optimization Strategies Learning and Application'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：自动优化策略的学习与应用
- en: 'The workflow of HLSPilot’s RAG-like automatic optimization strategy learning
    is illustrated in Fig. [3](#S3.F3 "Figure 3 ‣ III-C LLM-based Automatic HLS Optimization
    ‣ III HLSPilot Framework ‣ HLSPilot: LLM-based High-Level Synthesis"). It uses
    the Xilinx HLS official guide documentation as input and extracts structured pragma
    optimization information from the documents. As shown in fig. [4](#S3.F4 "Figure
    4 ‣ III-C LLM-based Automatic HLS Optimization ‣ III HLSPilot Framework ‣ HLSPilot:
    LLM-based High-Level Synthesis"), the structured information consists of four
    parts: (1) a brief introduction to the optimization strategy; (2) applicable optimization
    scenarios; (3) parameter descriptions; (4) optimization examples. The introduction
    to the optimization strategy and the information on applicable scenarios are primarily
    used to assist in retrieving and matching the optimization strategy with the code,
    thus these information is kept concise and general to enhance retrieval performance.
    Upon retrieving a suitable optimization strategy, the strategy’s parameter description
    information and optimization example information are integrated into the prompt,
    utilizing the LLM’s in-context learning capabilities to generate optimized code.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'HLSPilot 的类似 RAG 的自动优化策略学习工作流程如图 [3](#S3.F3 "Figure 3 ‣ III-C LLM-based Automatic
    HLS Optimization ‣ III HLSPilot Framework ‣ HLSPilot: LLM-based High-Level Synthesis")
    所示。它以 Xilinx HLS 官方指南文档为输入，并从文档中提取结构化的 pragma 优化信息。如图 [4](#S3.F4 "Figure 4 ‣ III-C
    LLM-based Automatic HLS Optimization ‣ III HLSPilot Framework ‣ HLSPilot: LLM-based
    High-Level Synthesis") 所示，结构化信息包括四个部分：(1) 优化策略的简要介绍；(2) 适用的优化场景；(3) 参数描述；(4) 优化示例。优化策略的介绍和适用场景信息主要用于辅助检索和匹配优化策略与代码，因此这些信息保持简洁和通用，以提高检索性能。检索到合适的优化策略后，将策略的参数描述信息和优化示例信息整合到提示中，利用
    LLM 的上下文学习能力生成优化后的代码。'
- en: '![Refer to caption](img/c2eb2d225083ff3651bf6ade19c38e9e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c2eb2d225083ff3651bf6ade19c38e9e.png)'
- en: 'Figure 4: Structured information extracted by HLSPilot. The optimization strategy
    from documents is summarized into four parts: (1) strategy overview and (2) applicable
    scenarios for strategy retrieval; (3) parameter description and (4) examples for
    generating optimization prompt'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：HLSPilot 提取的结构化信息。文档中的优化策略被总结为四个部分：(1) 策略概述和 (2) 策略检索的适用场景；(3) 参数描述和 (4)
    生成优化提示的示例
- en: IV Experiment
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: IV-A Experiment Setting
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 实验设置
- en: In this section, we demonstrate the effectiveness of HLSPilot framework for
    automatically generating and optimizing hardware accelerator based on HLS. We
    utilize GPT-4 [[35](#bib.bib35)] as the default LLM to accomplish tasks such as
    HLS code analysis and optimization within the workflow. For accelerator deployment
    and evaluation, we adopt the Vitis HLS design flow, using the Xilinx Alveo U280
    data center accelerator card. For design space exploration, we utilizes GenHLSOptimizer
    [[31](#bib.bib31)] to tune the parameters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了 HLSPilot 框架在基于 HLS 自动生成和优化硬件加速器方面的有效性。我们利用 GPT-4 [[35](#bib.bib35)]
    作为默认的 LLM 来完成工作流程中的 HLS 代码分析和优化等任务。对于加速器部署和评估，我们采用 Vitis HLS 设计流程，使用 Xilinx Alveo
    U280 数据中心加速卡。对于设计空间探索，我们利用 GenHLSOptimizer [[31](#bib.bib31)] 来调整参数。
- en: IV-B Benchmark Introduction
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 基准介绍
- en: Currently most HLS benchmark suites [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]
    still face sevaral limitations. Firstly, many benchmarks are only comprised of
    some textbook-style function kernels, failing to fully implement the complexity
    of real-world applications. Thus evaluations on these benchmarks lack practical
    value. Secondly, most HLS benchmark suites only include optimized HLS designs,
    lacking corresponding unoptimized versions, which is unfriendly for evaluating
    the effectiveness of HLS optimization strategies.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数 HLS 基准测试套件 [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)] 仍然面临几个限制。首先，许多基准测试仅包含一些教科书式的函数内核，未能完全实现实际应用的复杂性。因此，这些基准测试的评估缺乏实际价值。其次，大多数
    HLS 基准测试套件仅包括优化后的 HLS 设计，缺乏相应的未优化版本，这不利于评估 HLS 优化策略的有效性。
- en: 'To address these issues and accurately evaluate the performance of the accelerators
    generated by our HLSPilot, we designed a benchmark suite that considers both the
    complexity of the designs and the convenience of comparing optimization effects.
    This benchmark suite consists of two parts: modified Rosetta benchmarks [[38](#bib.bib38)]
    and a set of manually collected benchmarks. The Rosetta benchmarks comprise a
    series of complex real-world applications such as 3D rendering, digit recognition,
    and spam filtering. Each application has both a software implementation and a
    corresponding HLS implementation. The original Rosetta benchmarks were implemented
    using SDSoC. We have these designs ported to Vitis and proposed corresponding
    unoptimized HLS designs without any optimization strategies based on the software
    implementations of the applications. Additionally, as a supplement, we collected
    and implemented several other classic algorithm applications. Similarly, these
    applications also include unoptimized versions.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些问题并准确评估我们 HLSPilot 生成的加速器的性能，我们设计了一个基准套件，考虑了设计的复杂性以及优化效果比较的便利性。这个基准套件分为两个部分：修改后的
    Rosetta 基准 [[38](#bib.bib38)] 和一组手动收集的基准。Rosetta 基准包括一系列复杂的现实世界应用，如 3D 渲染、数字识别和垃圾邮件过滤。每个应用都有一个软件实现和一个相应的
    HLS 实现。原始 Rosetta 基准使用 SDSoC 实现。我们将这些设计移植到 Vitis 并提出了相应的未优化 HLS 设计，基于应用程序的软件实现。此外，作为补充，我们收集并实现了几个其他经典算法应用。这些应用也包括未优化版本。
- en: IV-C Experiment Results and Analysis
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 实验结果与分析
- en: 'Experiment results. Table [I](#S4.T1 "TABLE I ‣ IV-C Experiment Results and
    Analysis ‣ IV Experiment ‣ HLSPilot: LLM-based High-Level Synthesis") shows the
    runtime of original unoptimized design, manually optimized design, HLSPilot-generated
    design, and HLSPilot-generated design with DSE for each application in the benchmarks.
    The results indicate a significant improvement in performance compared to the
    unoptimized design when utilizing HLSPilot-generated designs. Overall, HLSPilot-generated
    designs achieve comparable performance to those manually optimized by human experts,
    while greatly reducing labor costs. With the utilization of DSE tools, some HLSPilot-generated
    designs can even outperform human designs.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果。表 [I](#S4.T1 "表 I ‣ IV-C 实验结果与分析 ‣ IV 实验 ‣ HLSPilot: 基于 LLM 的高级综合") 显示了原始未优化设计、手动优化设计、HLSPilot
    生成的设计以及 HLSPilot 生成的设计与 DSE 的运行时间。结果表明，利用 HLSPilot 生成的设计与未优化设计相比，性能显著提高。总体而言，HLSPilot
    生成的设计在性能上可与人类专家手动优化的设计相媲美，同时大大降低了人工成本。借助 DSE 工具，一些 HLSPilot 生成的设计甚至可以超越人类设计。'
- en: 'TABLE I: Benchmark runtime(ms) on Xilinx Alveo U280'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：Xilinx Alveo U280 上的基准运行时间（毫秒）
- en: '| Application | original | handcrafted | HLSPilot | HLSPilot + DSE |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 原始 | 手工优化 | HLSPilot | HLSPilot + DSE |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Fir | 0.413 | 0.279 | 0.245 | 0.227 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Fir | 0.413 | 0.279 | 0.245 | 0.227 |'
- en: '| Merge Sort | 786.618 | 54.878 | 47.580 | 47.460 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 合并排序 | 786.618 | 54.878 | 47.580 | 47.460 |'
- en: '| BFS | 5018.551 | 3973.645 | 4184.273 | 3830.421 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| BFS | 5018.551 | 3973.645 | 4184.273 | 3830.421 |'
- en: '| PageRank | 1862.214 | 1254.833 | 1114.991 | 1050.617 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| PageRank | 1862.214 | 1254.833 | 1114.991 | 1050.617 |'
- en: '| 3D Rendering | 9.177 | 4.918 | 5.375 | 5.146 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 3D 渲染 | 9.177 | 4.918 | 5.375 | 5.146 |'
- en: '| Digit Recognition | 9917.663 | 9.892 | 78.837 | 52.832 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 数字识别 | 9917.663 | 9.892 | 78.837 | 52.832 |'
- en: '| Face Detection | 83.752 | 55.909 | 64.372 | 59.138 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 人脸检测 | 83.752 | 55.909 | 64.372 | 59.138 |'
- en: '| Optical Flow | 101.313 | 54.084 | 71.932 | 63.184 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 光流 | 101.313 | 54.084 | 71.932 | 63.184 |'
- en: '| Spam Filter | 9278.917 | 37.346 | 8013.913 | 7519.317 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾邮件过滤 | 9278.917 | 37.346 | 8013.913 | 7519.317 |'
- en: 'Analysis on the results. Table [II](#S4.T2 "TABLE II ‣ IV-C Experiment Results
    and Analysis ‣ IV Experiment ‣ HLSPilot: LLM-based High-Level Synthesis") shows
    the major optimization strategies adopted by human expert’s designs and HLSPilot-generated
    designs respectively. It can be noted that HLSPilot has selected appropriate optimization
    strategies for different applications, basically covering the optimization selected
    by human expert. The performance gap between HLSPilot and human expert mainly
    comes from the specific implementation methods of optimization. For example, for
    dataflow pipelining optimization, there are various ways to split the same kernel.
    The rich experience of human experts may lead to more reasonable task partitioning.
    In addition, LLM struggles to implement optimizations tailored to specific scenes.
    For instance, in the spam filter application, achieving LUT optimization for sigmoid
    function requires sampling the function and generating a specific lookup table,
    while also considering issues such as quantization precision, which is difficult
    for LLM to implement.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '对结果的分析。表[II](#S4.T2 "表 II ‣ IV-C 实验结果与分析 ‣ IV 实验 ‣ HLSPilot: 基于 LLM 的高层次综合")展示了人类专家设计和
    HLSPilot 生成的设计所采用的主要优化策略。可以看出，HLSPilot 为不同的应用选择了适当的优化策略，基本覆盖了人类专家选择的优化。HLSPilot
    和人类专家之间的性能差距主要来自优化的具体实施方法。例如，对于数据流流水线优化，拆分相同内核的方式有多种。人类专家丰富的经验可能导致更合理的任务划分。此外，LLM
    在实现针对特定场景的优化时存在困难。例如，在垃圾邮件过滤器应用中，实现 sigmoid 函数的 LUT 优化需要对函数进行采样并生成特定的查找表，同时还要考虑量化精度等问题，这对
    LLM 来说很难实现。'
- en: 'TABLE II: Major optimization strategies used in handcrafted design and HLSPilot-generated
    design'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 手工设计和 HLSPilot 生成设计中使用的主要优化策略'
- en: '| Application | manual | HLSPilot |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 应用 | 手动 | HLSPilot |'
- en: '| --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Fir |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| FIR |'
- en: '&#124; Loop unrolling &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环展开 &#124;'
- en: '&#124; Loop pipelining &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环流水线优化 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Loop unrolling &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环展开 &#124;'
- en: '&#124; Loop pipeling &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Merge Sort |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 归并排序 |'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory Optimization &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '&#124; Loop unrolling &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环展开 &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory Optimization &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '&#124; Loop unrolling &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环展开 &#124;'
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| BFS |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| BFS |'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory Optimization &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PageRank |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| PageRank |'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 3D Rendering |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 3D 渲染 |'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Communication optimization &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通信优化 &#124;'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Communication optimization &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通信优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Digit Recognition |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 数字识别 |'
- en: '&#124; Loop unrolling &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环展开 &#124;'
- en: '&#124; Loop pipelining &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环流水线优化 &#124;'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Loop unrolling &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环展开 &#124;'
- en: '&#124; Loop pipelining &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环流水线优化 &#124;'
- en: '&#124; Datatype optimization &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据类型优化 &#124;'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Face Detection |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 人脸检测 |'
- en: '&#124; Memory optimization &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '&#124; Datatype optimization &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据类型优化 &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Optical Flow |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 光流 |'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '&#124; Communication optimization &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通信优化 &#124;'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '&#124; Datatype optimization &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据类型优化 &#124;'
- en: '&#124; Loop pipelining &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 循环流水线优化 &#124;'
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Spam Filter |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾邮件过滤器 |'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '&#124; Communication optimization &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通信优化 &#124;'
- en: '&#124; LUT optimization &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LUT 优化 &#124;'
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dataflow pipelining &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据流流水线优化 &#124;'
- en: '&#124; Memory optimization &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内存优化 &#124;'
- en: '|'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: IV-D Case Study
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 案例研究
- en: To further verify the practicality of HLSPilot in real-world application, we
    selected the L-BFGS algorithm [[39](#bib.bib39)] and performed a complete hardware
    acceleration workflow for it using HLSPilot on the hybrid CPU-FPGA platform.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步验证 HLSPilot 在实际应用中的实用性，我们选择了 L-BFGS 算法 [[39](#bib.bib39)]，并在混合 CPU-FPGA
    平台上使用 HLSPilot 执行了完整的硬件加速工作流程。
- en: Introduction to the L-BFGS algorithm. L-BFGS algorithm is one of the commonly
    used algorithms in machine learning for solving unconstrained optimization problems.
    When solving gradient descent, L-BFGS algorithm approximates the inverse Hessian
    matrix using only a limited amount of past information from the gradients, greatly
    reducing the storage space of data. However, due to its large number of iterations,
    the algorithm performs poorly on the CPU, typically taking several hours for each
    search process.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: L-BFGS 算法简介。L-BFGS 算法是机器学习中常用的解决无约束优化问题的算法之一。在解决梯度下降时，L-BFGS 算法使用有限的历史梯度信息来近似逆
    Hessian 矩阵，从而大大减少了数据存储空间。然而，由于迭代次数较多，该算法在 CPU 上表现不佳，通常每次搜索过程需要几小时。
- en: Complete acceleration workflow of HLSPilot. In this case, we wrote a C++ software
    code for L-BFGS algorithm as the input of HLSPilot. HLSPilot firstly ran the sequential
    C++ code of the algorithm on CPU and generated a profiling report using the gprof
    tool, which includes detailed function runtime and number of calls. According
    to HLSPilot’s analysis, the cost_calculate function in L-BFGS accounts for more
    than 99.1% of the total runtime of the algorithm, which is the performance bottleneck
    of the program. Therefore, this part will be extracted as the kernel for hardware
    acceleration.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: HLSPilot 的完整加速工作流程。在这个案例中，我们编写了一段 C++ 软件代码作为 HLSPilot 的输入。HLSPilot 首先在 CPU 上运行了算法的顺序
    C++ 代码，并使用 gprof 工具生成了一个分析报告，其中包括详细的函数运行时间和调用次数。根据 HLSPilot 的分析，L-BFGS 中的 cost_calculate
    函数占算法总运行时间的 99.1% 以上，是程序的性能瓶颈。因此，这部分将被提取为硬件加速的内核。
- en: 'Next, HLSPilot performed the task pipelining on the kernel code, partitioning
    the cost calculation process into three tasks: cost and convolution calculation,
    reconstruction error gradient calculation, and gradient check. Subsequently, HLSPilot
    applied appropriate optimization strategies to each task. The major optimization
    strategies employed in this stage included local buffer optimization, loop unrolling,
    array partitioning, and others. Particularly, HLSPilot noticed that the cost calculation
    process involved a significant amount of floating-point computations. Therefore,
    it performed floating-point to fixed-point conversion on the code, further optimizing
    the computational performance of the kernel. Finally, HLSPilot determined the
    pragma parameters through DSE tools.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，HLSPilot 在内核代码上执行了任务流水线，将成本计算过程分为三个任务：成本和卷积计算、重建误差梯度计算，以及梯度检查。随后，HLSPilot
    对每个任务应用了适当的优化策略。在这个阶段采用的主要优化策略包括局部缓冲区优化、循环展开、数组分区等。特别是，HLSPilot 注意到成本计算过程涉及大量的浮点计算。因此，它对代码进行了浮点到定点转换，进一步优化了内核的计算性能。最后，HLSPilot
    通过 DSE 工具确定了 pragma 参数。
- en: 'Acceleration result. We evaluated the cost calculation runtime and algorithm’s
    total runtime on both CPU and CPU-FPGA platforms, as shown in Table [III](#S4.T3
    "TABLE III ‣ IV-D Case Study ‣ IV Experiment ‣ HLSPilot: LLM-based High-Level
    Synthesis"). L-BFGS-CPU represents the algorithm program running on the CPU, while
    HLSPilot-FP and HLSPilot-FXP respectively represent the floating-point and fixed-point
    designs generated by HLSPilot. Overall, HLSPilot’s floating-point design and fixed-point
    design have accelerated the end-to-end runtime by 7.79 times and 11.93 times,
    respectively. Notably, for the cost calculation, HLSPilot can accelerate it by
    more than 500 times, which fully demonstrates the effectiveness of HLSPilot’s
    acceleration.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '加速结果。我们在 CPU 和 CPU-FPGA 平台上评估了成本计算运行时间和算法的总运行时间，如表 [III](#S4.T3 "TABLE III
    ‣ IV-D Case Study ‣ IV Experiment ‣ HLSPilot: LLM-based High-Level Synthesis")
    所示。L-BFGS-CPU 代表在 CPU 上运行的算法程序，而 HLSPilot-FP 和 HLSPilot-FXP 分别代表 HLSPilot 生成的浮点和定点设计。总体而言，HLSPilot
    的浮点设计和定点设计分别将端到端的运行时间加速了 7.79 倍和 11.93 倍。特别是，对于成本计算，HLSPilot 可以加速超过 500 倍，这充分展示了
    HLSPilot 加速的有效性。'
- en: 'TABLE III: Acceleration result on L-BFGS algorithm'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：L-BFGS 算法的加速结果
- en: '| Design |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 设计 |'
- en: '&#124; CostCalc. &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CostCalc. &#124;'
- en: '&#124; Runtime(s) &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运行时间(s) &#124;'
- en: '|'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计 &#124;'
- en: '&#124; Runtime(s) &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运行时间 (s) &#124;'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CostCalc. &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CostCalc. &#124;'
- en: '&#124; Speedup &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加速比 &#124;'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; End-to-end &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 端到端 &#124;'
- en: '&#124; Speedup &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加速比 &#124;'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| CPU | 18237 | 18390 | - | - |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| CPU | 18237 | 18390 | - | - |'
- en: '| HLSPilot-FP | 855 | 2365 | 21.33x | 7.78x |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| HLSPilot-FP | 855 | 2365 | 21.33x | 7.78x |'
- en: '| HLSPilot-FXP | 31 | 1541 | 588.29x | 11.93x |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| HLSPilot-FXP | 31 | 1541 | 588.29x | 11.93x |'
- en: 'Table [IV](#S4.T4 "TABLE IV ‣ IV-D Case Study ‣ IV Experiment ‣ HLSPilot: LLM-based
    High-Level Synthesis") shows the resource overhead and runtime of the cost calculation
    kernel in L-BFGS. Runtime in table [IV](#S4.T4 "TABLE IV ‣ IV-D Case Study ‣ IV
    Experiment ‣ HLSPilot: LLM-based High-Level Synthesis") represents the time taken
    to execute one instance of the cost calculation. It is evident that HLSPilot can
    effectively optimize the performance bottlenecks of the algorithm, significantly
    enhancing performance.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [IV](#S4.T4 "TABLE IV ‣ IV-D Case Study ‣ IV Experiment ‣ HLSPilot: LLM-based
    High-Level Synthesis") 显示了 L-BFGS 中成本计算核的资源开销和运行时间。 表 [IV](#S4.T4 "TABLE IV ‣
    IV-D Case Study ‣ IV Experiment ‣ HLSPilot: LLM-based High-Level Synthesis") 中的运行时间表示执行一次成本计算所需的时间。显然，HLSPilot
    可以有效优化算法的性能瓶颈，显著提升性能。'
- en: 'TABLE IV: CostCalc. kernel resource overhead and runtime'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: CostCalc. 核心资源开销与运行时间'
- en: '| Kernels | #LUTs | #FFs | #BRAMs | #DSPs | Runtime(ms) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 核心 | #LUTs | #FFs | #BRAMs | #DSPs | 运行时间 (ms) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| CPU | - | - | - | - | 38529.08 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| CPU | - | - | - | - | 38529.08 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| kernel-FP | 54970 | 66459 | 46 | 107 | 1680.84 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| kernel-FP | 54970 | 66459 | 46 | 107 | 1680.84 |'
- en: '| kernel-FXP | 188294 | 245018 | 270 | 624 | 60.9811 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| kernel-FXP | 188294 | 245018 | 270 | 624 | 60.9811 |'
- en: V Conclusion
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this paper, we have introduced HLSPilot, the first LLM-driven HLS framework
    to automate the generation of hardware accelerators on CPU-FPGA platform. HLSPilot
    focuses on the transformation between sequential C/C++ code and optimized HLS
    code, which greatly reducing the semantic gap between design intent and hardware
    code. Additionally, the integration of profiling tools and DSE tools enables automatic
    hardware/software partition and pragma tuning. Through the combined efforts of
    various modules driven by LLM, HLSPilot automatically generates high-performance
    hardware accelerators. The kernel optimization experiment results on the benchmark
    fully demonstrate the potential of HLSPilot, showing its ability to achieve comparable,
    and in some cases superior, performance relative to manually designed FPGA kernel.
    In addition, we also performed a complete hardware acceleration workflow for a
    real-world algorithm, achieving 11.93x speedup on the hybrid CPU-FPGA platform.
    These results highlight the significant effects of LLM, suggesting a promising
    future for LLM-assisted methodology in hardware design.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 HLSPilot，这是第一个基于 LLM 的 HLS 框架，用于在 CPU-FPGA 平台上自动生成硬件加速器。 HLSPilot
    专注于顺序 C/C++ 代码与优化 HLS 代码之间的转换，极大地减少了设计意图与硬件代码之间的语义差距。此外，分析工具和 DSE 工具的集成使得自动硬件/软件分区和
    pragma 调优成为可能。通过 LLM 驱动的各种模块的联合努力，HLSPilot 自动生成高性能硬件加速器。基准上的核心优化实验结果充分展示了 HLSPilot
    的潜力，表明其能够实现与手动设计的 FPGA 核心相当，甚至在某些情况下优越的性能。此外，我们还为实际算法执行了完整的硬件加速工作流程，实现了在混合 CPU-FPGA
    平台上 11.93 倍的加速。这些结果突显了 LLM 的显著效果，暗示了 LLM 辅助方法在硬件设计中具有光明的前景。
- en: References
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] G. Martin and G. Smith, “High-level synthesis: Past, present, and future,”
    *IEEE Design & Test of Computers*, vol. 26, no. 4, pp. 18–25, 2009.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] G. Martin 和 G. Smith, “高级综合：过去、现在与未来，” *IEEE Design & Test of Computers*,
    vol. 26, no. 4, pp. 18–25, 2009.'
- en: '[2] C. Liu, X. Chen, B. He, X. Liao, Y. Wang, and L. Zhang, “Obfs: Opencl based
    bfs optimizations on software programmable fpgas,” in *2019 International Conference
    on Field-Programmable Technology (ICFPT)*.   IEEE, 2019, pp. 315–318.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] C. Liu, X. Chen, B. He, X. Liao, Y. Wang, 和 L. Zhang, “Obfs: 基于 OpenCL
    的 BFS 在软件可编程 FPGA 上的优化，” 在 *2019 年国际现场可编程技术会议 (ICFPT)*。 IEEE, 2019, pp. 315–318.'
- en: '[3] X. Zhang, Z. Feng, S. Liang, X. Chen, C. Liu, H. Li, and X. Li, “Graphitron:
    A domain specific language for fpga-based graph processing accelerator generation,”
    *arXiv preprint arXiv:2407.12575*, 2024.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. Zhang, Z. Feng, S. Liang, X. Chen, C. Liu, H. Li, 和 X. Li, “Graphitron:
    一种用于 FPGA 的图处理加速器生成的领域特定语言，” *arXiv preprint arXiv:2407.12575*, 2024.'
- en: '[4] S. Lahti, P. Sjövall, J. Vanne, and T. D. Hämäläinen, “Are we there yet?
    a study on the state of high-level synthesis,” *IEEE Transactions on Computer-Aided
    Design of Integrated Circuits and Systems*, vol. 38, no. 5, pp. 898–911, 2018.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Lahti, P. Sjövall, J. Vanne, 和 T. D. Hämäläinen, “我们到了吗？关于高级综合状态的研究，”
    *IEEE集成电路和系统计算机辅助设计汇刊*，第38卷，第5期，第898–911页，2018年。'
- en: '[5] B. C. Schafer and Z. Wang, “High-level synthesis design space exploration:
    Past, present, and future,” *IEEE Transactions on Computer-Aided Design of Integrated
    Circuits and Systems*, vol. 39, no. 10, pp. 2628–2639, 2019.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] B. C. Schafer 和 Z. Wang, “高级综合设计空间探索：过去、现在与未来，” *IEEE集成电路和系统计算机辅助设计汇刊*，第39卷，第10期，第2628–2639页，2019年。'
- en: '[6] J. Zhao, L. Feng, S. Sinha, W. Zhang, Y. Liang, and B. He, “Performance
    modeling and directives optimization for high-level synthesis on fpga,” *IEEE
    Transactions on Computer-Aided Design of Integrated Circuits and Systems*, vol. 39,
    no. 7, pp. 1428–1441, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Zhao, L. Feng, S. Sinha, W. Zhang, Y. Liang, 和 B. He, “FPGA上高级综合的性能建模和指令优化，”
    *IEEE集成电路和系统计算机辅助设计汇刊*，第39卷，第7期，第1428–1441页，2019年。'
- en: '[7] C. Liu, H.-C. Ng, and H. K.-H. So, “Quickdough: A rapid fpga loop accelerator
    design framework using soft cgra overlay,” in *2015 International Conference on
    Field Programmable Technology (FPT)*.   IEEE, 2015, pp. 56–63.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Liu, H.-C. Ng, 和 H. K.-H. So, “Quickdough: 一种快速FPGA循环加速器设计框架，使用软CGRA叠加层，”
    在 *2015国际现场可编程技术会议（FPT）*。 IEEE，2015年，第56–63页。'
- en: '[8] A. Sohrabizadeh, C. H. Yu, M. Gao, and J. Cong, “Autodse: Enabling software
    programmers to design efficient fpga accelerators,” *ACM Transactions on Design
    Automation of Electronic Systems (TODAES)*, vol. 27, no. 4, pp. 1–27, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Sohrabizadeh, C. H. Yu, M. Gao, 和 J. Cong, “Autodse: 使软件程序员能够设计高效的FPGA加速器，”
    *ACM电子系统设计自动化汇刊（TODAES）*，第27卷，第4期，第1–27页，2022年。'
- en: '[9] Y.-k. Choi and J. Cong, “Hls-based optimization and design space exploration
    for applications with variable loop bounds,” in *2018 IEEE/ACM International Conference
    on Computer-Aided Design (ICCAD)*.   IEEE, 2018, pp. 1–8.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Y.-k. Choi 和 J. Cong, “基于HLS的优化与具有可变循环界限的应用设计空间探索，” 在 *2018 IEEE/ACM国际计算机辅助设计会议（ICCAD）*。
    IEEE，2018年，第1–8页。'
- en: '[10] G. Zhong, A. Prakash, S. Wang, Y. Liang, T. Mitra, and S. Niar, “Design
    space exploration of fpga-based accelerators with multi-level parallelism,” in
    *Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017*.   IEEE,
    2017, pp. 1141–1146.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] G. Zhong, A. Prakash, S. Wang, Y. Liang, T. Mitra, 和 S. Niar, “基于FPGA的加速器的设计空间探索与多级并行性，”
    在 *2017年设计、自动化与欧洲测试会议（DATE）*。 IEEE，2017年，第1141–1146页。'
- en: '[11] L. Ferretti, A. Cini, G. Zacharopoulos, C. Alippi, and L. Pozzi, “Graph
    neural networks for high-level synthesis design space exploration,” *ACM Transactions
    on Design Automation of Electronic Systems*, vol. 28, no. 2, pp. 1–20, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] L. Ferretti, A. Cini, G. Zacharopoulos, C. Alippi, 和 L. Pozzi, “图神经网络用于高级综合设计空间探索，”
    *ACM电子系统设计自动化汇刊*，第28卷，第2期，第1–20页，2022年。'
- en: '[12] E. Luo, H. Huang, C. Liu, G. Li, B. Yang, Y. Wang, H. Li, and X. Li, “Deepburning-mixq:
    An open source mixed-precision neural network accelerator design framework for
    fpgas,” in *2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)*.   IEEE,
    2023, pp. 1–9.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] E. Luo, H. Huang, C. Liu, G. Li, B. Yang, Y. Wang, H. Li, 和 X. Li, “Deepburning-mixq:
    一种用于FPGAs的开源混合精度神经网络加速器设计框架，” 在 *2023 IEEE/ACM国际计算机辅助设计会议（ICCAD）*。 IEEE，2023年，第1–9页。'
- en: '[13] X. Chen, H. Tan, Y. Chen, B. He, W.-F. Wong, and D. Chen, “Thundergp:
    Hls-based graph processing framework on fpgas,” in *The 2021 ACM/SIGDA International
    Symposium on Field-Programmable Gate Arrays*, 2021, pp. 69–80.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Chen, H. Tan, Y. Chen, B. He, W.-F. Wong, 和 D. Chen, “Thundergp: 基于HLS的图处理框架在FPGAs上的应用，”
    在 *2021 ACM/SIGDA国际现场可编程门阵列研讨会*，2021年，第69–80页。'
- en: '[14] S. Liang, C. Liu, Y. Wang, H. Li, and X. Li, “Deepburning-gl: an automated
    framework for generating graph neural network accelerators,” in *Proceedings of
    the 39th International Conference on Computer-Aided Design*, 2020, pp. 1–9.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] S. Liang, C. Liu, Y. Wang, H. Li, 和 X. Li, “Deepburning-gl: 一种自动化框架用于生成图神经网络加速器，”
    在 *第39届国际计算机辅助设计会议论文集*，2020年，第1–9页。'
- en: '[15] Y. Fu, Y. Zhang, Z. Yu, S. Li, Z. Ye, C. Li, C. Wan, and Y. C. Lin, “Gpt4aigchip:
    Towards next-generation ai accelerator design automation via large language models,”
    in *2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)*.   IEEE,
    2023, pp. 1–9.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Fu, Y. Zhang, Z. Yu, S. Li, Z. Ye, C. Li, C. Wan, 和 Y. C. Lin, “Gpt4aigchip:
    通过大型语言模型迈向下一代AI加速器设计自动化，” 在 *2023 IEEE/ACM国际计算机辅助设计会议（ICCAD）*。 IEEE，2023年，第1–9页。'
- en: '[16] K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, and X. Li,
    “Chipgpt: How far are we from natural language hardware design,” *arXiv preprint
    arXiv:2305.14019*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, 和 X. Li，“Chipgpt：我们距离自然语言硬件设计还有多远，”
    *arXiv 预印本 arXiv:2305.14019*，2023年。'
- en: '[17] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben,
    H. Anand, S. Banerjee, I. Bayraktaroglu *et al.*, “Chipnemo: Domain-adapted llms
    for chip design,” *arXiv preprint arXiv:2311.00176*, 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben,
    H. Anand, S. Banerjee, I. Bayraktaroglu *等*，“Chipnemo：针对芯片设计的领域适应 llms，” *arXiv
    预印本 arXiv:2311.00176*，2023年。'
- en: '[18] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-Gavitt,
    and S. Garg, “Benchmarking large language models for automated verilog rtl code
    generation,” in *2023 Design, Automation & Test in Europe Conference & Exhibition
    (DATE)*.   IEEE, 2023, pp. 1–6.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-Gavitt,
    和 S. Garg，“基准测试大语言模型在自动 Verilog RTL 代码生成中的表现，” 在 *2023年设计、自动化与测试欧洲会议及展览（DATE）*。
    IEEE，2023年，第1–6页。'
- en: '[19] M. Liu, N. Pinckney, B. Khailany, and H. Ren, “Verilogeval: Evaluating
    large language models for verilog code generation,” in *2023 IEEE/ACM International
    Conference on Computer Aided Design (ICCAD)*.   IEEE, 2023, pp. 1–8.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. Liu, N. Pinckney, B. Khailany, 和 H. Ren，“Verilogeval：评估大语言模型在 Verilog
    代码生成中的表现，” 在 *2023 IEEE/ACM 计算机辅助设计国际会议（ICCAD）*。 IEEE，2023年，第1–8页。'
- en: '[20] Y. Tsai, M. Liu, and H. Ren, “Rtlfixer: Automatically fixing rtl syntax
    errors with large language models,” *arXiv preprint arXiv:2311.16543*, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Tsai, M. Liu, 和 H. Ren，“Rtlfixer：利用大语言模型自动修复 RTL 语法错误，” *arXiv 预印本
    arXiv:2311.16543*，2023年。'
- en: '[21] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and Z. Xie, “Rtlcoder: Outperforming
    gpt-3.5 in design rtl generation with our open-source dataset and lightweight
    solution,” *arXiv preprint arXiv:2312.08617*, 2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, 和 Z. Xie，“Rtlcoder：在设计 RTL
    生成方面超越 gpt-3.5 的开源数据集和轻量级解决方案，” *arXiv 预印本 arXiv:2312.08617*，2023年。'
- en: '[22] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, and R. Karri, “Autochip:
    Automating hdl generation using llm feedback,” *arXiv preprint arXiv:2311.04887*,
    2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, 和 R. Karri，“Autochip：使用
    llm 反馈自动生成 HDL，” *arXiv 预印本 arXiv:2311.04887*，2023年。'
- en: '[23] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, “Rtllm: An open-source benchmark
    for design rtl generation with large language model,” in *2024 29th Asia and South
    Pacific Design Automation Conference (ASP-DAC)*.   IEEE, 2024, pp. 722–727.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Lu, S. Liu, Q. Zhang, 和 Z. Xie，“Rtllm：基于大语言模型的设计 RTL 生成开源基准，” 在 *2024年第29届亚太设计自动化会议（ASP-DAC）*。
    IEEE，2024年，第722–727页。'
- en: '[24] J. Blocklove, S. Garg, R. Karri, and H. Pearce, “Chip-chat: Challenges
    and opportunities in conversational hardware design,” in *2023 ACM/IEEE 5th Workshop
    on Machine Learning for CAD (MLCAD)*.   IEEE, 2023, pp. 1–6.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Blocklove, S. Garg, R. Karri, 和 H. Pearce，“Chip-chat：对话式硬件设计中的挑战与机遇，”
    在 *2023 ACM/IEEE 第五届计算机辅助设计机器学习研讨会（MLCAD）*。 IEEE，2023年，第1–6页。'
- en: '[25] Z. Jiang, Q. Zhang, C. Liu, H. Li, and X. Li, “Iicpilot: An intelligent
    integrated circuit backend design framework using open eda,” *arXiv preprint arXiv:2407.12576*,
    2024.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Z. Jiang, Q. Zhang, C. Liu, H. Li, 和 X. Li，“Iicpilot：一种使用开放EDA的智能集成电路后端设计框架，”
    *arXiv 预印本 arXiv:2407.12576*，2024年。'
- en: '[26] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    and C. Xiong, “Codegen: An open large language model for code with multi-turn
    program synthesis,” *arXiv preprint arXiv:2203.13474*, 2022.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    和 C. Xiong，“Codegen：一种用于代码的开放大语言模型，多轮程序合成，” *arXiv 预印本 arXiv:2203.13474*，2022年。'
- en: '[27] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards,
    Y. Burda, N. Joseph, G. Brockman *et al.*, “Evaluating large language models trained
    on code,” *arXiv preprint arXiv:2107.03374*, 2021.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H.
    Edwards, Y. Burda, N. Joseph, G. Brockman *等*，“评估针对代码训练的大语言模型，” *arXiv 预印本 arXiv:2107.03374*，2021年。'
- en: '[28] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware unified
    pre-trained encoder-decoder models for code understanding and generation,” *arXiv
    preprint arXiv:2109.00859*, 2021.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Wang, W. Wang, S. Joty, 和 S. C. Hoi，“Codet5：面向标识符的统一预训练编码器-解码器模型用于代码理解和生成，”
    *arXiv 预印本 arXiv:2109.00859*，2021年。'
- en: '[29] B. Wang, Z. Wang, X. Wang, Y. Cao, R. A Saurous, and Y. Kim, “Grammar
    prompting for domain-specific language generation with large language models,”
    *Advances in Neural Information Processing Systems*, vol. 36, 2024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] B. Wang, Z. Wang, X. Wang, Y. Cao, R. A Saurous, 和 Y. Kim，“领域特定语言生成的语法提示与大语言模型，”
    *神经信息处理系统进展*，第36卷，2024年。'
- en: '[30] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro,
    L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can
    teach themselves to use tools,” *Advances in Neural Information Processing Systems*,
    vol. 36, 2024.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro,
    L. Zettlemoyer, N. Cancedda, 和 T. Scialom，“Toolformer：语言模型可以自学使用工具”，*神经信息处理系统进展*，第36卷，2024年。'
- en: '[31] aferikoglou, “Genhlsoptimizer,” 2022\. [Online]. Available: [https://github.com/aferikoglou/GenHLSOptimizer](https://github.com/aferikoglou/GenHLSOptimizer)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] aferikoglou，“Genhlsoptimizer”，2022年。 [在线]. 可用链接：[https://github.com/aferikoglou/GenHLSOptimizer](https://github.com/aferikoglou/GenHLSOptimizer)'
- en: '[32] Xilinx, “Vivado design suite user guide: High-level synthesis (ug902),”
    2020\. [Online]. Available: [https://docs.amd.com/v/u/en-US/ug902-vivado-high-level-synthesis](https://docs.amd.com/v/u/en-US/ug902-vivado-high-level-synthesis)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Xilinx，“Vivado 设计套件用户指南：高层综合 (ug902)”，2020年。 [在线]. 可用链接：[https://docs.amd.com/v/u/en-US/ug902-vivado-high-level-synthesis](https://docs.amd.com/v/u/en-US/ug902-vivado-high-level-synthesis)'
- en: '[33] Xilinx, “Vivado hls optimization methodology guide (ug1270),” 2018\. [Online].
    Available: [https://docs.amd.com/v/u/en-US/ug1270-vivado-hls-opt-methodology-guide](https://docs.amd.com/v/u/en-US/ug1270-vivado-hls-opt-methodology-guide)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Xilinx，“Vivado HLS 优化方法指南 (ug1270)”，2018年。 [在线]. 可用链接：[https://docs.amd.com/v/u/en-US/ug1270-vivado-hls-opt-methodology-guide](https://docs.amd.com/v/u/en-US/ug1270-vivado-hls-opt-methodology-guide)'
- en: '[34] Xilinx, “Vitis high-level synthesis user guide (ug1399),” 2023\. [Online].
    Available: [https://docs.amd.com/r/en-US/ug1399-vitis-hls/Navigating-Content-by-Design-Process](https://docs.amd.com/r/en-US/ug1399-vitis-hls/Navigating-Content-by-Design-Process)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Xilinx，“Vitis 高层综合用户指南 (ug1399)”，2023年。 [在线]. 可用链接：[https://docs.amd.com/r/en-US/ug1399-vitis-hls/Navigating-Content-by-Design-Process](https://docs.amd.com/r/en-US/ug1399-vitis-hls/Navigating-Content-by-Design-Process)'
- en: '[35] OpenAI, “Gpt-4,” 2023\. [Online]. Available: [https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] OpenAI，“Gpt-4”，2023年。 [在线]. 可用链接：[https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)'
- en: '[36] Y. Hara, H. Tomiyama, S. Honda, H. Takada, and K. Ishii, “Chstone: A benchmark
    program suite for practical c-based high-level synthesis,” in *2008 IEEE International
    Symposium on Circuits and Systems (ISCAS)*.   IEEE, 2008, pp. 1192–1195.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Y. Hara, H. Tomiyama, S. Honda, H. Takada, 和 K. Ishii，“Chstone：一个用于实际
    C 基高层综合的基准程序套件”，发表于*2008 IEEE 国际电路与系统研讨会 (ISCAS)*。IEEE，2008，第1192–1195页。'
- en: '[37] B. C. Schafer and A. Mahapatra, “S2cbench: Synthesizable systemc benchmark
    suite for high-level synthesis,” *IEEE Embedded Systems Letters*, vol. 6, no. 3,
    pp. 53–56, 2014.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] B. C. Schafer 和 A. Mahapatra，“S2cbench：可综合 SystemC 基准套件，用于高层综合”，*IEEE
    嵌入式系统通讯*，第6卷，第3期，第53–56页，2014年。'
- en: '[38] Y. Zhou, U. Gupta, S. Dai, R. Zhao, N. Srivastava, H. Jin, J. Featherston,
    Y.-H. Lai, G. Liu, G. A. Velasquez *et al.*, “Rosetta: A realistic high-level
    synthesis benchmark suite for software programmable fpgas,” in *Proceedings of
    the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays*,
    2018, pp. 269–278.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Zhou, U. Gupta, S. Dai, R. Zhao, N. Srivastava, H. Jin, J. Featherston,
    Y.-H. Lai, G. Liu, G. A. Velasquez *等*，“Rosetta：一个现实的高层综合基准套件，用于软件可编程 FPGA”，发表于*2018
    ACM/SIGDA 国际现场可编程门阵列研讨会论文集*，2018年，第269–278页。'
- en: '[39] D. C. Liu and J. Nocedal, “On the limited memory bfgs method for large
    scale optimization,” *Mathematical programming*, vol. 45, no. 1, pp. 503–528,
    1989.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] D. C. Liu 和 J. Nocedal，“大规模优化的有限记忆 BFGS 方法”，*数学编程*，第45卷，第1期，第503–528页，1989年。'
