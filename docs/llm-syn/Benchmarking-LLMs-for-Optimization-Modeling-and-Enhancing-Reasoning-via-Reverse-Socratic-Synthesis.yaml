- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:59:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:59:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于优化建模的LLM基准测试与通过反向苏格拉底合成增强推理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.09887](https://ar5iv.labs.arxiv.org/html/2407.09887)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.09887](https://ar5iv.labs.arxiv.org/html/2407.09887)
- en: \pdfcolInitStack
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdfcolInitStack
- en: tcb@breakable
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: tcb@breakable
- en: Zhicheng Yang¹    Yinya Huang⁴    Wei Shi⁷    Liang Feng⁶
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Zhicheng Yang¹    Yinya Huang⁴    Wei Shi⁷    Liang Feng⁶
- en: Linqi Song⁴    Yiwei Wang³    Xiaodan Liang⁵    Jing Tang^(1,2)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Linqi Song⁴    Yiwei Wang³    Xiaodan Liang⁵    Jing Tang^(1,2)
- en: ¹The Hong Kong University of Science and Technology (Guangzhou)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹香港科技大学（广州）
- en: ²The Hong Kong University of Science and Technology
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ²香港科技大学
- en: ³University of California, Los Angeles ⁴City University of Hong Kong
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ³加州大学洛杉矶分校 ⁴香港城市大学
- en: ⁵Sun Yat-sen University    ⁶Chongqing University    ⁷Huawei Noah’s Ark Lab
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵中山大学    ⁶重庆大学    ⁷华为诺亚方舟实验室
- en: yangzhch6@gmail.com, yinya.huang@hotmail.com,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: yangzhch6@gmail.com, yinya.huang@hotmail.com,
- en: shiwei87@huawei.com, liangf@cqu.edu.cn, linqi.song@cityu.edu.hk,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: shiwei87@huawei.com, liangf@cqu.edu.cn, linqi.song@cityu.edu.hk,
- en: wangyw.evan@gmail.com, xdliang328@gmail.com, jingtang@ust.hk
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: wangyw.evan@gmail.com, xdliang328@gmail.com, jingtang@ust.hk
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have exhibited their problem-solving ability in
    mathematical reasoning. Solving realistic optimization (OPT) problems in industrial
    application scenarios requires advanced and applied math ability. However, current
    OPT benchmarks that merely solve linear programming are far from complex realistic
    situations. In this work, we propose E-OPT, a benchmark for end-to-end optimization
    problem-solving with human-readable inputs and outputs. E-OPT contains rich optimization
    problems, including linear/nonlinear programming with/without table data, which
    can comprehensively evaluate LLMs’ solving ability. In our benchmark, LLMs are
    required to correctly understand the problem in E-OPT and call code solver to
    get precise numerical answers. Furthermore, to alleviate the data scarcity for
    optimization problems, and to bridge the gap between open-source LLMs on a small
    scale (e.g., Llama-2-7b and Llama-3-8b) and closed-source LLMs (e.g., GPT-4),
    we further propose a novel data synthesis method namely ReSocratic. Unlike general
    data synthesis methods that proceed from questions to answers, ReSocratic first
    incrementally synthesizes optimization scenarios with mathematical formulations
    step by step and then back-translates the generated scenarios into questions.
    In such a way, we construct the ReSocratic-29k dataset from a small seed sample
    pool with the powerful open-source large model DeepSeek-V2. To demonstrate the
    effectiveness of ReSocratic, we conduct supervised fine-tuning with ReSocratic-29k
    on multiple open-source models. The results show that Llama3-8b is significantly
    improved from 13.6% to 51.7% on E-OPT, while DeepSeek-V2 reaches 61.0%, approaching
    65.5% of GPT-4.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在数学推理方面表现出了其解决问题的能力。在工业应用场景中解决实际的优化（OPT）问题需要高级且应用的数学能力。然而，目前仅能解决线性规划的OPT基准测试远不能应对复杂的实际情况。在这项工作中，我们提出了E-OPT，这是一个用于端到端优化问题求解的基准，具有可读的输入和输出。E-OPT包含丰富的优化问题，包括带有/不带有表格数据的线性/非线性规划，这可以全面评估LLMs的求解能力。在我们的基准测试中，LLMs需要正确理解E-OPT中的问题，并调用代码求解器以获得精确的数值答案。此外，为了缓解优化问题的数据稀缺性，并弥合小规模开源LLMs（例如Llama-2-7b和Llama-3-8b）与闭源LLMs（例如GPT-4）之间的差距，我们进一步提出了一种新颖的数据合成方法，称为ReSocratic。与从问题到答案的通用数据合成方法不同，ReSocratic首先逐步合成具有数学公式的优化场景，然后将生成的场景反向翻译为问题。通过这种方式，我们从一个小的种子样本池和强大的开源大模型DeepSeek-V2构建了ReSocratic-29k数据集。为了展示ReSocratic的有效性，我们对多个开源模型进行ReSocratic-29k的监督微调。结果显示，Llama3-8b在E-OPT上的表现显著提升，从13.6%提高到51.7%，而DeepSeek-V2达到了61.0%，接近GPT-4的65.5%。
- en: \settocdepth
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \settocdepth
- en: part
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 部分
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs), such as GPT-3 [[1](#bib.bib1)], GPT-4 [[2](#bib.bib2)],
    and Llama [[3](#bib.bib3), [4](#bib.bib4)], have demonstrated their emerging capability
    in logical reasoning [[5](#bib.bib5), [6](#bib.bib6)] and mathematical reasoning [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)], such as solving elementary
    [[11](#bib.bib11)] to high-school level [[12](#bib.bib12)] math problems. Yet
    a follow-up curiosity is to what extent LLMs apply their mathematical intelligence
    to practical scenarios. Optimization problem solving is a field of applied mathematics
    that has been proven beneficial in many applications such as supply chain management,
    power energy scheduling, marketing, and quantitative trading. Optimization problem-solving
    is a comprehensive task that evaluates the mathematical and coding capabilities
    of LLMs. To provide the optimal solution to an optimization problem, LLMs are
    not only required to understand and construct the mathematical formulation according
    to the given problem but also to call an optimization solver to get the final
    answers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs），如 GPT-3 [[1](#bib.bib1)]、GPT-4 [[2](#bib.bib2)] 和 Llama [[3](#bib.bib3),
    [4](#bib.bib4)]，已经展示了它们在逻辑推理 [[5](#bib.bib5), [6](#bib.bib6)] 和数学推理 [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)] 方面的新兴能力，例如解决从小学 [[11](#bib.bib11)]
    到高中水平 [[12](#bib.bib12)] 的数学问题。然而，后续的问题是 LLMs 在多大程度上将其数学智能应用于实际场景。优化问题解决是一个应用数学领域，已被证明在许多应用中有益，例如供应链管理、电力能源调度、市场营销和定量交易。优化问题解决是一个综合任务，它评估
    LLMs 的数学和编码能力。为了提供优化问题的最优解，LLMs 不仅需要理解和构建根据给定问题的数学公式，还需要调用优化求解器以获得最终答案。
- en: Previous studies [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)] have
    done some primary exploration of large language models solving operations research
    problems. However, these studies have not yet extended to more generalized scenarios
    regarding practical optimization problems. Specifically, NL4OPT [[13](#bib.bib13),
    [16](#bib.bib16)] uses named entity recognition to extract entity and numerical
    values in the given question text, and then formulate it into mathematical models.
    They only measure the model’s ability to correctly construct mathematical formulations,
    without considering solving the mathematical formulations being constructed. To
    further evaluate models providing the final optimal solution, i.e., the numerical
    values of the variables and the optimization objective, ComplexOR [[14](#bib.bib14)]
    and NLP4LP [[15](#bib.bib15)] benchmark the models to solve a problem with an
    optimization solver in the setting without explicit input numbers. However, due
    to the difficulty of collecting such data, these benchmarks are still on a small
    scale. Moreover, the recent MAMO [[17](#bib.bib17)] proposes to further benchmark
    optimization problem solving with a code solver. Nevertheless, one common pitfall
    of all the aforementioned works is that they merely focus on linear programming,
    whereas nonlinear optimization problems and practical tabular format are not included.
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Benchmarking LLMs for Optimization
    Modeling and Enhancing Reasoning via Reverse Socratic Synthesis") provides a comparison
    of the aforementioned benchmarks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究 [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)] 对大语言模型解决运筹学问题进行了初步探索。然而，这些研究尚未扩展到关于实际优化问题的更广泛场景。具体来说，NL4OPT
    [[13](#bib.bib13), [16](#bib.bib16)] 利用命名实体识别来提取给定问题文本中的实体和数值，然后将其形成数学模型。他们只测量模型正确构建数学公式的能力，而未考虑解决所构建的数学公式。为了进一步评估提供最终最优解的模型，即变量和优化目标的数值，ComplexOR
    [[14](#bib.bib14)] 和 NLP4LP [[15](#bib.bib15)] 将模型基准化以在没有显式输入数字的设置中用优化求解器解决问题。然而，由于收集此类数据的困难，这些基准仍然规模较小。此外，最近的
    MAMO [[17](#bib.bib17)] 提出了使用代码求解器进一步基准化优化问题解决。然而，所有上述工作的一个共同陷阱是它们仅关注线性规划，而未包括非线性优化问题和实际表格格式。表
    [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Benchmarking LLMs for Optimization Modeling
    and Enhancing Reasoning via Reverse Socratic Synthesis") 提供了上述基准的比较。
- en: 'Table 1: Comparison of optimization problem solving benchmarks. The “End2End”
    indicates whether the benchmark requires the model to solve for the optimal values
    of the variables and the optimization objective.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 优化问题解决基准的比较。 “End2End” 表示基准是否要求模型求解变量和优化目标的最优值。'
- en: '| Benchmark | Numbers | End2End | Linear | Nonlinear |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 数量 | End2End | 线性 | 非线性 |'
- en: '| w/ table | w/o table | w/ table | w/o table |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 有表格 | 无表格 | 有表格 | 无表格 |'
- en: '| ComplexOR [[14](#bib.bib14)] | Implicit | $\surd$ |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ComplexOR [[14](#bib.bib14)] | 隐式 | $\surd$ |'
- en: '| NLP4LP [[15](#bib.bib15)] | Implicit | $\surd$ |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| NLP4LP [[15](#bib.bib15)] | 隐式 | $\surd$ |'
- en: '| NL4OPT [[16](#bib.bib16)] | Explicit | $\times$ |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| NL4OPT [[16](#bib.bib16)] | 显式 | $\times$ |'
- en: '| MAMO [[17](#bib.bib17)] | Explicit | $\surd$ |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| MAMO [[17](#bib.bib17)] | 显式 | $\surd$ |'
- en: '| E-OPT (Ours) | Explicit | $\surd$ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| E-OPT (我们的) | 显式 | $\surd$ |'
- en: 'In this work, we propose E-OPT, a new benchmark with high-quality data to evaluate
    LLMs’ end-to-end solving ability in optimization problems. We carefully select
    605 questions and conduct careful manual verification to form the dataset. E-OPT
    contains linear and nonlinear programming with both integer and mixed integer
    variables in the programming problems. E-OPT also includes tabular data, which
    fills the gap in current optimization benchmarks. Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis") demonstrates E-OPT examples in the four problem
    types: linear programming problems without tables, linear programming problems
    with tables, nonlinear programming problems without tables, and nonlinear programming
    problems with tables. A model solves an E-OPT problem by reading the natural language
    input and then generating Python code that solves the problem, where the code
    will be processed to acquire the numerical value of the variables and the objective
    function. We remark that a preliminary subset of E-OPT is released in April 2024
    as a contest track¹¹1[https://www.codabench.org/competitions/2438/](https://www.codabench.org/competitions/2438/)
    of ICML 2024 Challenge on Automated Math Reasoning.²²2[https://sites.google.com/view/ai4mathworkshopicml2024/challenges](https://sites.google.com/view/ai4mathworkshopicml2024/challenges)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了 E-OPT，这是一个具有高质量数据的新基准，用于评估 LLMs 在优化问题中的端到端解决能力。我们仔细选择了 605 个问题，并进行详细的手动验证以形成数据集。E-OPT
    包含线性和非线性编程问题，涉及整数和混合整数变量。E-OPT 还包括表格数据，填补了当前优化基准中的空白。图 [1](#S1.F1 "图 1 ‣ 1 介绍
    ‣ 基准测试 LLMs 的优化建模和通过反向苏格拉底综合提升推理能力") 展示了 E-OPT 在四种问题类型中的示例：没有表格的线性编程问题、带表格的线性编程问题、没有表格的非线性编程问题以及带表格的非线性编程问题。一个模型通过读取自然语言输入来解决
    E-OPT 问题，然后生成解决问题的 Python 代码，该代码将被处理以获取变量和目标函数的数值。我们指出，E-OPT 的初步子集已于 2024 年 4
    月作为 ICML 2024 自动数学推理挑战赛的竞赛跟踪¹¹1[https://www.codabench.org/competitions/2438/](https://www.codabench.org/competitions/2438/)
    发布。²²2[https://sites.google.com/view/ai4mathworkshopicml2024/challenges](https://sites.google.com/view/ai4mathworkshopicml2024/challenges)
- en: Moreover, the data scarcity issue in optimization problems [[14](#bib.bib14),
    [15](#bib.bib15)] cannot be ignored. The data samples of the optimization problem
    are relatively complex. Optimization problem solving requires not only correct
    natural language solutions but also the accurate code of calling optimization
    solvers. Data annotation in this field requires annotators to possess good professional
    knowledge, making the process not only expensive but also time-consuming and labor-intensive.
    In addition, there is a significant performance gap between small open-source
    models (e.g., Llama2-7b, Llama3-8b³³3[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3))
    and large closed-source models (e.g., GPT-4) in many complex reasoning tasks [[7](#bib.bib7),
    [8](#bib.bib8), [5](#bib.bib5), [9](#bib.bib9), [10](#bib.bib10), [6](#bib.bib6)].
    To alleviate the issue of data scarcity and mitigate the performance gap between
    small open-source language models and large models, we propose ReSocratic, a novel
    method for synthesizing diverse and reliable data for optimization problems. Our
    ReSocratic first generates high-quality natural language scenarios step by step
    from a very small number of seed samples. We then back-translate the scenarios
    generated in each step into questions, which can be seen as a reverse Socratic
    approach. We collect 29k samples with ReSocratic, resulting in the ReSocratic-29k
    dataset. Further ablation experiments show that our reverse data synthesis (from
    scenario to question) is more accurate than the forward data synthesis (from question
    to answer), and the step-wise sub-samples manufactured by the ReSocratic further
    improve the performance of the small open-source language models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，优化问题中的数据稀缺问题 [[14](#bib.bib14), [15](#bib.bib15)] 不容忽视。优化问题的数据样本相对复杂。优化问题的解决不仅需要正确的自然语言解答，还需要准确的调用优化求解器的代码。该领域的数据注释要求注释员具备良好的专业知识，使得该过程不仅昂贵，而且耗时且劳动密集。此外，小型开源模型（例如，Llama2-7b，Llama3-8b³³3[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)）与大型闭源模型（例如，GPT-4）在许多复杂推理任务中存在显著的性能差距
    [[7](#bib.bib7), [8](#bib.bib8), [5](#bib.bib5), [9](#bib.bib9), [10](#bib.bib10),
    [6](#bib.bib6)]。为了缓解数据稀缺问题，并减轻小型开源语言模型与大型模型之间的性能差距，我们提出了 ReSocratic，这是一种用于合成多样化和可靠的优化问题数据的新方法。我们的
    ReSocratic 首先从极少量的种子样本中逐步生成高质量的自然语言场景。然后，我们将每一步生成的场景反向翻译成问题，这可以看作是一种逆向苏格拉底方法。我们使用
    ReSocratic 收集了 29k 样本，生成了 ReSocratic-29k 数据集。进一步的消融实验表明，我们的逆向数据合成（从场景到问题）比正向数据合成（从问题到答案）更准确，并且
    ReSocratic 制造的逐步子样本进一步提高了小型开源语言模型的性能。
- en: 'In summary, our contributions are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献如下：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a high-quality benchmark named E-OPT for optimization problems with
    complex samples in multiple forms. As far as we know, this is the first large-scale
    benchmark to measure the model’s end-to-end solving ability in optimization problems
    including nonlinear and tabular data. We evaluate GPT families, Llama families,
    and DeepSeek-V2 [[18](#bib.bib18)] on our proposed E-OPT in few-shot and zero-shot
    settings.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个名为 E-OPT 的高质量基准，用于处理多种形式的复杂样本的优化问题。据我们所知，这是第一个大规模基准，用于测量模型在包括非线性和表格数据在内的优化问题上的端到端解决能力。我们在我们的
    E-OPT 基准上对 GPT 系列、Llama 系列和 DeepSeek-V2 [[18](#bib.bib18)] 进行了少样本和零样本设置的评估。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose ReSocratic, a novel method for generating diverse and reliable data
    for optimization problems. We first create high-quality natural language scenarios
    step by step from a very small number of seed samples. We then back-translate
    the generated scenarios at each step into sub-questions, which can be seen as
    a reverse Socratic approach. Experimental analysis shows that the accuracy of
    reverse data synthesis is higher than that of forward data synthesis, and the
    sub-questions generated in each step of ReSocratic can further improve the model
    performance.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 ReSocratic，这是一种用于生成多样化和可靠的优化问题数据的新方法。我们首先从极少量的种子样本中逐步创建高质量的自然语言场景。然后，我们将每一步生成的场景反向翻译成子问题，这可以看作是一种逆向苏格拉底方法。实验分析表明，逆向数据合成的准确性高于正向数据合成，并且
    ReSocratic 在每一步生成的子问题可以进一步提高模型性能。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We synthesize the ReSocratic-29k dataset with 29k samples by using our ReSocratic.
    Experimental results show that the ReSocratic-29k significantly improves the performance
    of open-source models on E-OPT (Llama2 from 0.0% to 30.6%; Llama3 from 13.6% to
    51.7%), which further demonstrates the validity of our synthetic data.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过使用我们的ReSocratic合成了包含29k样本的ReSocratic-29k数据集。实验结果表明，ReSocratic-29k显著提高了开源模型在E-OPT上的表现（Llama2从0.0%提升到30.6%；Llama3从13.6%提升到51.7%），进一步验证了我们合成数据的有效性。
- en: '![Refer to caption](img/7e98a9e1292ac49f78441e4e5248fcd4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/7e98a9e1292ac49f78441e4e5248fcd4.png)'
- en: 'Figure 1: Our E-OPT contains various types of data (linear, nonlinear, table).
    To enhance readability, we present the table in an excel format and included a
    diagram to illustrate the nonlinear example without a table. However, it should
    be noted that our question comprises text only.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们的E-OPT包含多种类型的数据（线性、非线性、表格）。为了提高可读性，我们以excel格式呈现了表格，并附上了一个示意图来说明没有表格的非线性示例。然而，需要注意的是，我们的问题仅包含文本。
- en: 2 Related Work
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Large Language Models. Recent progress in natural language processing (NLP)
    has led to the development of large language models (LLMs) [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)]. With a large amount of data and computation resources,
    LLMs excel beyond human capability in the domains of writing[[19](#bib.bib19),
    [20](#bib.bib20)], translation [[21](#bib.bib21), [22](#bib.bib22)], and knowledge
    reservoirs [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)].
    Moreover, large models are observed gaining emergent abilities for example in
    multi-step reasoning tasks [[27](#bib.bib27)]. Furthermore, in the realm of multi-step
    reasoning, few-shot prompting techniques [[28](#bib.bib28)] are observed to surpass
    the performance of fine-tuning with the full training set, even when applied to
    the same large model [[29](#bib.bib29)]. In recent years, researchers have focused
    on the performance of language models on complex reasoning tasks, including mathematical
    reasoning [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)] and
    logic reasoning [[5](#bib.bib5), [6](#bib.bib6)]. In this work, we mainly focus
    on optimization problem solving, which has strong practical application value.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型。最近在自然语言处理（NLP）方面的进展促使了大型语言模型（LLMs）的发展[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)]。凭借大量的数据和计算资源，LLMs 在写作[[19](#bib.bib19), [20](#bib.bib20)]、翻译[[21](#bib.bib21),
    [22](#bib.bib22)] 和知识库[[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]等领域超越了人类能力。此外，大型模型在多步骤推理任务中被观察到获得了突现能力[[27](#bib.bib27)]。此外，在多步骤推理领域，少量提示技术[[28](#bib.bib28)]被观察到在性能上超过了使用完整训练集的微调，即使应用于同一个大型模型[[29](#bib.bib29)]。近年来，研究者们专注于语言模型在复杂推理任务上的表现，包括数学推理[[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]和逻辑推理[[5](#bib.bib5), [6](#bib.bib6)]。在这项工作中，我们主要关注优化问题的解决，这具有较强的实际应用价值。
- en: Benchmarks for Optimization Problems. More closely related to our approach,
    the NL4OPT benchmark [[13](#bib.bib13), [16](#bib.bib16)] investigates controlled
    generation techniques to obtain an automatic suggestion of formulation. They first
    use named entity recognition methods to extract a set of entity-typed declarations,
    then they transform it into linear program models. As one can see, NL4OPT only
    evaluates an AI model’s ability to establish mathematical models, while we contribute
    an end-to-end framework in this work. Optimus [[15](#bib.bib15)] and ComplexOR
    [[14](#bib.bib14)] also make significant research in the field of operations research
    with LLMs. However, they provide a very small test set, containing less than 70
    test samples. Recently, MAMO [[17](#bib.bib17)] is proposed to benchmark mathematical
    modeling with code solvers. However, all these works merely focus on linear programming,
    ignoring the nonlinear problems that exist widely in practical applications. In
    addition, these benchmarks are simple in form, ignoring the tabular data that
    often occurs in industrial scenarios. In this work, we contribute E-OPT, which
    is an end-to-end benchmark containing 605 multi-type data samples. E-OPT is a
    comprehensive benchmark that involves linear, non-linear, and tabular data, and
    the types of variables involved in the problems include continuous, integers (IP),
    and mixed integers (MIP). Additionally, we notice a simultaneous work Tang et al.
    [[30](#bib.bib30)] explores synthesizing nonlinear programming via a semi-automated
    process. However, the data are solely applied for LLM fine-tuning. Contrastively,
    in this paper, we aim to benchmark practical optimization modeling with a high-quality
    manually checked test-bed E-OPT and also automatically synthesize more comprehensive
    optimization data including table data and code solutions resulting in ReSocratic-29k.
    Therefore, Tang et al. [[30](#bib.bib30)] is orthogonal to ours regarding the
    benchmark and method purpose.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题的基准测试。与我们的方法更为相关的是，NL4OPT 基准测试 [[13](#bib.bib13), [16](#bib.bib16)] 研究了受控生成技术以获得自动化的公式建议。他们首先使用命名实体识别方法提取一组实体类型声明，然后将其转换为线性规划模型。正如可以看到的，NL4OPT
    仅评估 AI 模型建立数学模型的能力，而我们在这项工作中贡献了一个端到端框架。Optimus [[15](#bib.bib15)] 和 ComplexOR
    [[14](#bib.bib14)] 在运筹学领域也进行了重要的研究，涉及 LLMs。然而，他们提供的测试集非常小，包含不到 70 个测试样本。最近，MAMO
    [[17](#bib.bib17)] 被提出用于通过代码求解器对数学建模进行基准测试。然而，这些工作仅关注线性规划，忽略了实际应用中广泛存在的非线性问题。此外，这些基准测试形式简单，忽视了工业场景中经常出现的表格数据。在这项工作中，我们贡献了
    E-OPT，这是一个包含 605 个多类型数据样本的端到端基准测试。E-OPT 是一个全面的基准测试，涉及线性、非线性和表格数据，问题中涉及的变量类型包括连续型、整数（IP）和混合整数（MIP）。此外，我们注意到
    Tang 等人 [[30](#bib.bib30)] 同时研究了通过半自动化过程合成非线性规划。然而，这些数据仅用于 LLM 的微调。相对而言，在本文中，我们的目标是使用高质量手动检查的测试平台
    E-OPT 对实际优化建模进行基准测试，并自动合成更多全面的优化数据，包括表格数据和代码解决方案，生成 ReSocratic-29k。因此，Tang 等人
    [[30](#bib.bib30)] 在基准测试和方法目的上与我们的工作正交。
- en: Data Synthesis with LLMs for Mathematical Reasoning. One of the important ways
    to improve the performance of language models in mathematical reasoning tasks
    is to upscale the number of fine-tuning data for LLMs. A lot of work [[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)]
    has been done in this area. Rejection sampling fine-tuning (RFT) [[34](#bib.bib34)]
    uses supervised models to generate and collect correct reasoning paths as augmented
    fine-tuning datasets. MAmmoTH [[36](#bib.bib36)] collects both Chain-of-Thoughts
    solutions in natural language and Program-of-Thoughts solutions in formal language
    using RFT with GPT-4\. MetaMath [[31](#bib.bib31)] conducts both data augmentation
    on question and answer text. MathGenie [[35](#bib.bib35)] collects a large amount
    of data through open-source language models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数学推理中的 LLM 数据合成。提高语言模型在数学推理任务中性能的重要方法之一是扩大 LLM 的微调数据量。在这一领域已经做了大量的工作 [[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)]。拒绝采样微调（RFT）
    [[34](#bib.bib34)] 使用监督模型生成和收集正确的推理路径作为扩增的微调数据集。MAmmoTH [[36](#bib.bib36)] 使用
    RFT 和 GPT-4 收集自然语言中的 Chain-of-Thoughts 解决方案以及正式语言中的 Program-of-Thoughts 解决方案。MetaMath
    [[31](#bib.bib31)] 对问题和答案文本进行了数据增强。MathGenie [[35](#bib.bib35)] 通过开源语言模型收集了大量数据。
- en: All of these works are oriented to primary school math word problems, a field
    that already has high-quality data sets (such as GSM8K [[11](#bib.bib11)] and
    SVAMP [[8](#bib.bib8)]), and these work builds on that. However, there is very
    little high-quality data for optimization problems, which poses a great challenge
    to data synthesis. Therefore, these previous approaches can’t transferred directly
    to optimization problems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工作都面向小学数学应用题，这一领域已经拥有高质量的数据集（如GSM8K [[11](#bib.bib11)] 和 SVAMP [[8](#bib.bib8)]），这些工作都建立在此基础上。然而，优化问题的数据仍然非常稀缺，这对数据综合提出了巨大的挑战。因此，这些之前的方法不能直接迁移到优化问题上。
- en: Socratic Large Language Model. The Socratic method [[37](#bib.bib37)] is a critical
    thinking method with dialogic disassembled multi-step subquestions and answers
    cultivating in answering a complex question. This method has been applied by current
    language model techniques for advanced reasoning tasks, such as prompting step-wise
    reasoning [[38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)], multi-agent
    interaction [[41](#bib.bib41)], and discovering math knowledge [[42](#bib.bib42)].
    For example, Qi et al. [[38](#bib.bib38)] proposes a divide-and-conquer style
    algorithm that mimics recursive thinking by asking Socratic questions, it thus
    relieves the reliance on the initial decision as chain-of-thought (CoT) and achieves
    performance improvements on several complex reasoning tasks. Dong et al. [[42](#bib.bib42)]
    prompt GPT-4 with Socratic reasoning to facilitate self-evaluation and refinement
    so that encourages the model to recursively discover, solve, and integrate problems,
    resulting in proving the challenging “$P\neq NP$” problem through 97 dialogue
    turns. Another line of work [[43](#bib.bib43), [11](#bib.bib11)] applies the Socratic
    method for fine-grained dataset construction. GSM8K Socratic dataset⁴⁴4[https://github.com/openai/grade-school-math?tab=readme-ov-file#socratic-dataset](https://github.com/openai/grade-school-math?tab=readme-ov-file#socratic-dataset)
    [[11](#bib.bib11)] is the most related work to our paper. They inject automatically
    generated “Socratic subquestions” before each step, resulting in fine-grained
    math data. To construct a step-by-step benchmark for optimization problem solving
    with intermediate solutions, in this work, we explore the Socratic method to synthesize
    optimization problems. Unlike the previous study, we propose a reverse Socratic
    approach (ReSocratic) that generates optimization problems from the answer back
    to a question, and we demonstrate its superiority to traditional forward Socratic
    synthesis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 苏格拉底大语言模型。苏格拉底方法[[37](#bib.bib37)]是一种批判性思维方法，通过对话式的分解多步骤子问题和答案来培养对复杂问题的回答。这种方法已经被当前的语言模型技术应用于高级推理任务，例如逐步推理[[38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40)]、多智能体互动[[41](#bib.bib41)]以及发现数学知识[[42](#bib.bib42)]。例如，Qi等人[[38](#bib.bib38)]提出了一种模仿递归思维的分而治之风格算法，通过提出苏格拉底问题，从而减轻了对初始决策的依赖，实现了在若干复杂推理任务上的性能提升。Dong等人[[42](#bib.bib42)]利用苏格拉底推理来促进GPT-4的自我评估和改进，鼓励模型递归地发现、解决和整合问题，通过97轮对话证明了具有挑战性的“$P\neq
    NP$”问题。另一项工作[[43](#bib.bib43), [11](#bib.bib11)]将苏格拉底方法应用于细粒度数据集构建。GSM8K苏格拉底数据集⁴⁴4[https://github.com/openai/grade-school-math?tab=readme-ov-file#socratic-dataset](https://github.com/openai/grade-school-math?tab=readme-ov-file#socratic-dataset)
    [[11](#bib.bib11)]是与我们论文最相关的工作。他们在每一步之前注入自动生成的“苏格拉底子问题”，从而产生细粒度的数学数据。为了构建用于优化问题解决的逐步基准，并提供中间解决方案，在这项工作中，我们探讨了使用苏格拉底方法来综合优化问题。与之前的研究不同，我们提出了一种反向苏格拉底方法（ReSocratic），从答案生成优化问题，并展示了其相对于传统前向苏格拉底综合的优越性。
- en: '3 E-OPT: Human-Readable Optimization Problems Benchmark'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 E-OPT：人类可读优化问题基准
- en: 'The benchmark E-OPT is to evaluate the capability of large language models
    to solve end-to-end optimization problems. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis") compares E-OPT and related optimization-problem benchmarks.
    E-OPT covers a substantial number of optimization problems with a wider range
    of problem types. Specifically, E-OPT features linear programming (linear), non-linear
    optimization problems (non-linear), and table content as in industrial use (Table),
    resulting in a comprehensive and versatile benchmark for LLM optimization problem-solving.
    E-OPT is an end-to-end benchmark, which takes natural language as input and numerical
    values of variables and objective as output. One data example of E-OPT is demonstrated
    in Figure [2](#S3.F2 "Figure 2 ‣ 3 E-OPT: Human-Readable Optimization Problems
    Benchmark ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '基准 E-OPT 用于评估大型语言模型解决端到端优化问题的能力。表 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")
    比较了 E-OPT 和相关的优化问题基准。E-OPT 涵盖了大量的优化问题，并且问题类型范围更广。具体来说，E-OPT 包括线性规划（linear）、非线性优化问题（non-linear）以及工业应用中的表格内容（Table），形成了一个全面而多功能的
    LLM 优化问题解决基准。E-OPT 是一个端到端基准，它将自然语言作为输入，并将变量和目标的数值作为输出。E-OPT 的一个数据示例如图 [2](#S3.F2
    "Figure 2 ‣ 3 E-OPT: Human-Readable Optimization Problems Benchmark ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")
    所示。'
- en: '![Refer to caption](img/49f6b182e28a642ed1efce0454f4405e.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/49f6b182e28a642ed1efce0454f4405e.png)'
- en: 'Figure 2: An example of E-OPT. This example is about a mixed integer nonlinear
    optimization problem. We use pyscipopt to solve the given question. Finally, we
    collect the results of the code execution.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：E-OPT 的一个示例。这个示例是关于一个混合整数非线性优化问题。我们使用 pyscipopt 来解决给定的问题。最后，我们收集代码执行的结果。
- en: 'Data Collection and Annotation. In the data annotation stage, we assign workers
    to collect questions from textbooks [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)],
    and a university’s course assignments and examinations. The workers are required
    to collect the questions first and save the text of the question in markdown format.
    If the question contains a table, convert the table to markdown format as well.
    If there is a standard answer to the question, it is also saved; if not, we let
    the worker complete the answer in natural language form. We require our workers
    to write Python code, call the pyscipopt⁵⁵5[https://github.com/scipopt/PySCIPOpt](https://github.com/scipopt/PySCIPOpt)
    solver to solve each problem, and ask them to output the values of the variables
    and optimization targets at the end of the code. These numerical solutions are
    recorded in the dataset as ground truth answers. Figure [2](#S3.F2 "Figure 2 ‣
    3 E-OPT: Human-Readable Optimization Problems Benchmark ‣ Benchmarking LLMs for
    Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")
    shows a mixed integer nonlinear programming sample of E-OPT. For each sample,
    we provide the “Question” and “Results” For every problem, we ask the workers
    to judge the uniqueness of the optimal solution. If the workers determine that
    the solution is unique, the “Results” will contain the description (colored in
    orange in Figure [2](#S3.F2 "Figure 2 ‣ 3 E-OPT: Human-Readable Optimization Problems
    Benchmark ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")) and optimal value (colored in green in Figure
    [2](#S3.F2 "Figure 2 ‣ 3 E-OPT: Human-Readable Optimization Problems Benchmark
    ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis")) of all the variables and optimization objective; otherwise
    “Results” contains only the optimal value of the optimization objective.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '数据收集与标注。在数据标注阶段，我们分配工人从教科书[[44](#bib.bib44)、[45](#bib.bib45)、[46](#bib.bib46)]、大学课程作业和考试中收集问题。工人需首先收集问题并将问题的文本以markdown格式保存。如果问题包含表格，也将表格转换为markdown格式。如果问题有标准答案，也保存该答案；如果没有，我们让工人在自然语言形式下完成答案。我们要求工人编写Python代码，调用pyscipopt⁵⁵5[https://github.com/scipopt/PySCIPOpt](https://github.com/scipopt/PySCIPOpt)求解每个问题，并要求他们在代码末尾输出变量和值和优化目标的值。这些数值解被记录在数据集中作为地面真实答案。图[2](#S3.F2
    "图 2 ‣ 3 E-OPT: 人类可读优化问题基准 ‣ 基准测试 LLMs 优化建模与通过反向苏格拉底合成增强推理")展示了E-OPT的混合整数非线性规划样本。对于每个样本，我们提供“问题”和“结果”。对于每个问题，我们要求工人判断最优解的唯一性。如果工人确定解是唯一的，则“结果”将包含所有变量和优化目标的描述（图[2](#S3.F2
    "图 2 ‣ 3 E-OPT: 人类可读优化问题基准 ‣ 基准测试 LLMs 优化建模与通过反向苏格拉底合成增强推理")中标记为橙色）和最优值（图[2](#S3.F2
    "图 2 ‣ 3 E-OPT: 人类可读优化问题基准 ‣ 基准测试 LLMs 优化建模与通过反向苏格拉底合成增强推理")中标记为绿色）；否则，“结果”仅包含优化目标的最优值。'
- en: 'Data Statistics. In Figure [2](#S3.F2 "Figure 2 ‣ 3 E-OPT: Human-Readable Optimization
    Problems Benchmark ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis"), we show the statistical results of
    four data formats (linear w/ table, linear w/o table, nonlinear w/ table, and
    nonlinear w/o table). Overall, E-OPT is the first optimization modeling benchmark
    with nonlinear data and table format data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '数据统计。在图[2](#S3.F2 "图 2 ‣ 3 E-OPT: 人类可读优化问题基准 ‣ 基准测试 LLMs 优化建模与通过反向苏格拉底合成增强推理")中，我们展示了四种数据格式（有表格的线性、无表格的线性、有表格的非线性、无表格的非线性）的统计结果。总体来看，E-OPT
    是第一个包含非线性数据和表格格式数据的优化建模基准。'
- en: Evaluation. Unlike NL4OPT [[16](#bib.bib16)], which only measures the mathematical
    modeling ability of the language model, we also measure the solving ability of
    the language model to call code solver. In this paper, the evaluation approach
    we adopt is an end-to-end process where natural language text is the input and
    numerical form answers are the output. Given an optimization problem $p$ to calculate
    the accuracy. A problem is considered solved if and only if all the variables
    and objectives are correctly matched. We provide a zero-shot prompt and a few-shot
    prompt to solve the problem in the Appendix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 评估。与仅测量语言模型数学建模能力的 NL4OPT [[16](#bib.bib16)] 不同，我们还测量了语言模型调用代码求解器的解决能力。在本文中，我们采用的评估方法是一个端到端的过程，其中自然语言文本为输入，数值形式的答案为输出。给定一个优化问题
    $p$ 来计算准确度。一个问题被认为解决，只有当所有变量和目标都正确匹配时。我们在附录中提供了零样本提示和少量样本提示来解决该问题。
- en: Challenge on Automated Optimization Problem-Solving with Code. An initial subset
    of E-OPT is released in April 2024 as a contest track¹ of ICML 2024 Challenge
    on Automated Math Reasoning,² where we have collected some information about the
    current model capability in the end-to-end optimization problem-solving setting.
    The challenge has received 784 submissions from 77 participants.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 自动优化问题解决的挑战。E-OPT 的初步子集于 2024 年 4 月作为 ICML 2024 自动数学推理挑战赛¹的比赛赛道发布²，届时我们收集了一些关于当前模型在端到端优化问题解决设置中的能力的信息。该挑战赛共收到来自
    77 位参与者的 784 个提交。
- en: '![Refer to caption](img/fae2d7192f0f663b1d8ecd441b7aa1c6.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fae2d7192f0f663b1d8ecd441b7aa1c6.png)'
- en: 'Figure 3: (a) The forward data synthesis method is to synthesize the question
    first, and then let the LLM generate the answer to the synthetic question. (b)
    In contrast, the reverse data synthesis method we propose, ReSocratic, first synthesizes
    carefully designed formatted scenarios, and then transforms the synthesized scenarios
    into code (answers) and questions. (c) Our carefully designed scenarios are structured
    in a step-by-step manner, with each step containing a natural language description
    as well as the corresponding formalized mathematical content. Starting from the
    third step of the synthetic scenario, each subsequent step is transformed into
    a question-code pair.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：(a) 前向数据合成方法是首先合成问题，然后让 LLM 生成对合成问题的回答。(b) 相比之下，我们提出的反向数据合成方法 ReSocratic，首先合成精心设计的格式化场景，然后将合成的场景转化为代码（答案）和问题。(c)
    我们精心设计的场景是逐步结构化的，每一步包含自然语言描述以及相应的形式化数学内容。从合成场景的第三步开始，每个后续步骤都被转化为问题-代码对。
- en: '4 ReSocratic: Reverse Socratic Data Synthesis'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 ReSocratic：反向苏格拉底数据合成
- en: 'Within the domain of optimization problems, the procurement of data presents
    significant difficulties, and the process of manual annotation is both time-intensive
    and financially burdensome, leading to substantial costs. To alleviate the data
    scarcity in this domain and to improve the optimization problem solving capabilities
    of open-source small models such as (Llama2-7b and Llama3-8b), we introduce ReSocratic,
    a novel data synthesis method for eliciting diverse and reliable data. The ReSocratic
    framework is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3 E-OPT: Human-Readable Optimization
    Problems Benchmark ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")(a). The main idea of ReSocratic is
    to synthesize an optimization problem with step-by-step generation via Socratic
    method [[37](#bib.bib37)] in a reverse manner from our elaborate scenarios to
    questions.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '在优化问题领域，数据获取存在显著困难，手动标注过程既耗时又费用高昂，导致了巨大的成本。为了缓解该领域的数据稀缺问题，并提高开源小模型（如 Llama2-7b
    和 Llama3-8b）的优化问题解决能力，我们引入了 ReSocratic，这是一种用于引出多样且可靠数据的新型数据合成方法。ReSocratic 框架如图 [3](#S3.F3
    "图 3 ‣ 3 E-OPT: 人类可读的优化问题基准 ‣ 基准测试 LLM 的优化建模与通过反向苏格拉底合成增强推理")(a) 所示。ReSocratic
    的主要思想是通过反向苏格拉底方法[[37](#bib.bib37)]从我们详细设计的场景到问题逐步生成一个优化问题。'
- en: 'Optimization Scenario. Figure [3](#S3.F3 "Figure 3 ‣ 3 E-OPT: Human-Readable
    Optimization Problems Benchmark ‣ Benchmarking LLMs for Optimization Modeling
    and Enhancing Reasoning via Reverse Socratic Synthesis")(b) shows a 4-step scenario.
    The scenario is constructed step by step, where each step is clearly delineated
    and builds upon the previous one. Each step consists of three parts:1) A header
    with “##” that introduces the specific aspect of the scenario being addressed,
    such as “Defining Variables”, “Objective Function”, or “Constraints”. 2) A narrative
    description (colored in blue) in natural language that provides context and details
    about the element being introduced. This helps to understand the rationale and
    the requirements of that particular part of the optimization problem. 3) Mathematical
    formalization following “//” that translates the natural language description
    into a precise mathematical expression or constraint.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '优化场景。图[3](#S3.F3 "图 3 ‣ 3 E-OPT: 人工可读的优化问题基准 ‣ 通过逆向苏格拉底综合法对 LLM 进行优化建模和推理增强")(b)展示了一个四步场景。该场景逐步构建，每一步都明确划分，并在前一步的基础上进行扩展。每一步由三部分组成：1）一个以“##”开头的标题，介绍正在处理的场景的具体方面，如“定义变量”、“目标函数”或“约束条件”。2）用自然语言（蓝色标记）提供上下文和细节的叙述，帮助理解优化问题特定部分的理由和要求。3）跟随“//”的数学形式化，将自然语言描述转换为精确的数学表达式或约束。'
- en: Synthesis New Scenarios. We collect 27 elaborate optimization scenarios as a
    seed data pool. We first sample 2 scenarios each time from the pool to form the
    synthesis scenario prompt as shown in the Appendix. The LLM will follow the given
    prompt to generate new scenarios step by step. We set the temperature as 0.7,
    and sample 50 responses for each input. We then deploy a rule filter to detect
    whether each step of the generated scenario contains the above-mentioned three
    parts. If not, the data corresponding to the step is rejected and the generation
    stops. For all generated scenarios, we set a similarity filter, which converts
    all scenario texts into TF-IDF vectors and filters out scenarios with cosine similarity
    higher than a threshold set at 0.7.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 综合新场景。我们收集了27个详细的优化场景作为种子数据池。我们首先从池中每次抽取2个场景，以形成综合场景提示，如附录所示。LLM将根据给定的提示逐步生成新场景。我们将温度设置为0.7，并为每个输入抽取50个响应。然后，我们部署一个规则过滤器，以检测生成的场景的每一步是否包含上述三个部分。如果没有，则拒绝对应步骤的数据，生成过程停止。对于所有生成的场景，我们设置了一个相似性过滤器，将所有场景文本转换为TF-IDF向量，并过滤掉余弦相似度高于设置阈值0.7的场景。
- en: 'Code and Question Translation. We construct a code generation prompt to solve
    the mathematical formulations in the synthetic scenarios and output the optimal
    solution results. If the code runs incorrectly, we delete this scenario. Next,
    to acquire the questions in plain text format and the questions in table format,
    we construct two back-translation prompts. All the prompts are shown in the Appendix.
    Then, for each generated scenario starting from the third step, we translate it
    into a question-code pair, as shown in Figure [3](#S3.F3 "Figure 3 ‣ 3 E-OPT:
    Human-Readable Optimization Problems Benchmark ‣ Benchmarking LLMs for Optimization
    Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")(b).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '代码和问题翻译。我们构建了一个代码生成提示，用于解决合成场景中的数学公式并输出最佳解决结果。如果代码运行不正确，我们将删除该场景。接下来，为了获得纯文本格式和表格格式的问题，我们构建了两个反向翻译提示。所有提示均在附录中展示。然后，从第三步开始，我们将每个生成的场景翻译成问题-代码对，如图[3](#S3.F3
    "图 3 ‣ 3 E-OPT: 人工可读的优化问题基准 ‣ 通过逆向苏格拉底综合法对 LLM 进行优化建模和推理增强")(b)所示。'
- en: The Socratic method guides students to think about themselves through a series
    of sub-questions, gradually approaching the answer to the problem. Our data synthesis
    method, on the other hand, starts with a scenario and progressively synthesizes
    questions step by step, which can be seen as a reverse Socratic method.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 苏格拉底法引导学生通过一系列子问题自我思考，逐步接近问题的答案。我们的数据综合方法则以一个场景为起点，逐步综合问题，这可以看作是逆向苏格拉底法。
- en: Synthetic Dataset. We use DeepSeek-V2 [[18](#bib.bib18)] to apply ReSocratic.
    As an open-source large language model, DeepSeek-V2 [[18](#bib.bib18)] stands
    out due to its competitive performance to GPT-4, while concurrently offering a
    more cost-effective alternative. Furthermore, it exhibits a superior throughput,
    approximately 6 times greater, when contrasted against the existing 70b open-source
    model [[18](#bib.bib18)]. Utilizing the advanced capabilities of DeepSeek-V2,
    we contribute 29k synthetic data, leveraging merely 27 seed scenarios. This results
    in the ReSocratic-29k dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据集。我们使用 DeepSeek-V2 [[18](#bib.bib18)] 应用 ReSocratic。作为开源大型语言模型，DeepSeek-V2
    [[18](#bib.bib18)] 因其与 GPT-4 竞争的性能而脱颖而出，同时提供了更具成本效益的替代方案。此外，与现有的 70b 开源模型相比，它的吞吐量约高出
    6 倍 [[18](#bib.bib18)]。利用 DeepSeek-V2 的先进能力，我们贡献了 29k 合成数据，仅使用了 27 个种子场景。这形成了
    ReSocratic-29k 数据集。
- en: 5 Experiments
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Baselines and Setting
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基线和设置
- en: 'We select GPT-3.5-Turbo [[1](#bib.bib1)], GPT-4 [[2](#bib.bib2)], Llama-2-7b-Chat
    [[4](#bib.bib4)], Llama-3-8b-Instruct and Llama-3-70b-Instruct as our language
    model baselines. The evaluation metric of our E-OPT is the answer accuracy, as
    detailed in Section [2](#S3.F2 "Figure 2 ‣ 3 E-OPT: Human-Readable Optimization
    Problems Benchmark ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis"). We show the solving accuracy of the
    four data types along with the code pass rate. We evaluate LLMs under three settings:
    Zero-shot, Few-shot, and Supervised Fine-Tuning (SFT) setting. We conduct SFT
    experiments on two A800 GPUs, the epoch is set as 3, the learning rate is $2e^{-5}$,
    and the batch size is 128.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '我们选择了 GPT-3.5-Turbo [[1](#bib.bib1)]、GPT-4 [[2](#bib.bib2)]、Llama-2-7b-Chat
    [[4](#bib.bib4)]、Llama-3-8b-Instruct 和 Llama-3-70b-Instruct 作为我们的语言模型基线。我们 E-OPT
    的评估指标是答案准确率，详见第 [2](#S3.F2 "Figure 2 ‣ 3 E-OPT: Human-Readable Optimization Problems
    Benchmark ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis") 节。我们展示了四种数据类型的解题准确率以及代码通过率。我们在三种设置下评估 LLM：零样本、少样本和监督微调
    (SFT) 设置。我们在两台 A800 GPU 上进行 SFT 实验，训练周期设置为 3，学习率为 $2e^{-5}$，批量大小为 128。'
- en: 5.2 Main Results
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: As shown in Table [2](#S5.T2 "Table 2 ‣ 5.2 Main Results ‣ 5 Experiments ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis"),
    GPT-4 has the strongest overall performance and achieves state-of-the-art performance
    on almost all kinds of data formats. The performance of the two open-source models,
    llama3-70b and deepseek-v2, is close to that of GPT-4 in the few-shot setup. In
    addition, open source small models perform extremely poorly on E-OPT, with Llama-2-7B-Chat
    not getting a single question correctly solved and Llama-3-8B-Instruct getting
    only 13.6% accuracy on the few-shot setting. From the perspective of data type,
    the nonlinear data of our E-OPT is more challenging than the linear data, and
    the data with table (w/ table) is more challenging than without table (w/o table).
    Then, to show the validity of ReSocratic, we SFT Llama-2-7B-Chat, and Llama-3-8B-Instruct
    with our synthetic data ReSocratic-29k. We improved the performance of the Llama-2-7B-Chat
    from 0.0% to 30.6%, and the Llama-3-8B-Instruct from 13.6% to 51.1% (+37.5%),
    which is very close to the GPT-3.5-Turbo. In addition, Llama-3-8B-Instruct even
    exceeds GPT-4 in the data type of linear w/table, reaching state-of-the-art performance.
    We present a more detailed dataset performance analysis in the Appendix.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [2](#S5.T2 "Table 2 ‣ 5.2 Main Results ‣ 5 Experiments ‣ Benchmarking LLMs
    for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")
    所示，GPT-4 具有最强的总体性能，在几乎所有数据格式上都达到了最先进的性能。两个开源模型，llama3-70b 和 deepseek-v2，在少样本设置下的表现接近
    GPT-4。此外，开源小模型在 E-OPT 上表现极差，Llama-2-7B-Chat 没有一个问题正确解决，Llama-3-8B-Instruct 在少样本设置下仅有
    13.6% 的准确率。从数据类型的角度来看，我们的 E-OPT 的非线性数据比线性数据更具挑战性，而带表格 (w/ table) 的数据比不带表格 (w/o
    table) 的数据更具挑战性。然后，为了展示 ReSocratic 的有效性，我们用我们的合成数据 ReSocratic-29k 对 Llama-2-7B-Chat
    和 Llama-3-8B-Instruct 进行了 SFT。我们将 Llama-2-7B-Chat 的性能从 0.0% 提高到 30.6%，将 Llama-3-8B-Instruct
    的性能从 13.6% 提高到 51.1%（+37.5%），接近 GPT-3.5-Turbo。此外，Llama-3-8B-Instruct 在带表格的线性数据类型上甚至超过了
    GPT-4，达到了最先进的性能。我们在附录中展示了更详细的数据集性能分析。
- en: 'Table 2: Main results on E-OPT. “Code Pass” indicates the rate of code that
    is successfully executed. Bold indicates the sota in the current setting, and
    underline indicates the sota in the overall situation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：E-OPT 主要结果。“代码通过”表示成功执行的代码比率。粗体表示当前设置中的最佳，下划线表示总体情况中的最佳。
- en: '| Model | Linear | Nonlinear | All | Code Pass |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 线性 | 非线性 | 全部 | 代码通过 |'
- en: '| w/ Table | w/o Table | w/ Table | w/o Table |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 有表格 | 无表格 | 有表格 | 无表格 |'
- en: '| Zero-shot Prompt |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot 提示 |'
- en: '| Llama-3-8B-Instruct | 0.29% | 0.0% | 0.0% | 0.0% | 0.17% | 8.8% |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B-Instruct | 0.29% | 0.0% | 0.0% | 0.0% | 0.17% | 8.8% |'
- en: '| GPT-3.5-Turbo | 37.5% | 68.1% | 16.0% | 19.5% | 49.1% | 85.0% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 37.5% | 68.1% | 16.0% | 19.5% | 49.1% | 85.0% |'
- en: '| Llama-3-70B-Instruct | 50.0% | 76.9% | 32.0% | 30.8% | 59.5% | 86.8% |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B-Instruct | 50.0% | 76.9% | 32.0% | 30.8% | 59.5% | 86.8% |'
- en: '| DeepSeek-V2 | 27.5% | 40.4% | 18.0% | 29.3% | 34.4% | 74.0% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-V2 | 27.5% | 40.4% | 18.0% | 29.3% | 34.4% | 74.0% |'
- en: '| GPT-4 | 62.5% | 75.4% | 32.0% | 42.1% | 62.8% | 88.8% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 62.5% | 75.4% | 32.0% | 42.1% | 62.8% | 88.8% |'
- en: '| Few-shot Prompt |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 少-shot 提示 |'
- en: '| Llama-3-8B-Instruct | 2.5% | 17.8% | 8.0% | 11.3% | 13.6% | 26.9% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B-Instruct | 2.5% | 17.8% | 8.0% | 11.3% | 13.6% | 26.9% |'
- en: '| GPT-3.5-Turbo | 40.0% | 75.4% | 26.0% | 28.6% | 56.4% | 93.2% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 40.0% | 75.4% | 26.0% | 28.6% | 56.4% | 93.2% |'
- en: '| Llama-3-70B-Instruct | 57.5% | 79.2% | 32.0% | 33.8% | 62.5% | 91.2% |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B-Instruct | 57.5% | 79.2% | 32.0% | 33.8% | 62.5% | 91.2% |'
- en: '| DeepSeek-V2 | 56.3% | 79.5% | 32.0% | 27.1% | 61.0% | 85.5% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-V2 | 56.3% | 79.5% | 32.0% | 27.1% | 61.0% | 85.5% |'
- en: '| GPT-4 | 71.3% | 80.7% | 34.0% | 34.6% | 65.5% | 88.3% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 71.3% | 80.7% | 34.0% | 34.6% | 65.5% | 88.3% |'
- en: '| SFT with Synthetic Data |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| SFT 与合成数据 |'
- en: '| Llama-2-7B-Chat | 11.3% | 40.6% | 32.0% | 15.8% | 30.6% | 93.7% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-7B-Chat | 11.3% | 40.6% | 32.0% | 15.8% | 30.6% | 93.7% |'
- en: '| Llama-3-8B-Instruct | 32.5% | 63.5% | 44.0% | 33.0% | 51.1% | 96.3% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B-Instruct | 32.5% | 63.5% | 44.0% | 33.0% | 51.1% | 96.3% |'
- en: 'Table 3: Ablation study on synthetic data.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 3: 合成数据的消融研究。'
- en: '| Model | SFT Data | Linear | Nonlinear | All |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | SFT 数据 | 线性 | 非线性 | 全部 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| w/ Table | w/o Table | w/ Table | w/o Table |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 有表格 | 无表格 | 有表格 | 无表格 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama-2-7B-Chat | ReSocratic w/o step questions | 10.0% | 38.3% | 32.0% |
    15.0% | 28.9% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-7B-Chat | ReSocratic 无步骤问题 | 10.0% | 38.3% | 32.0% | 15.0% | 28.9%
    |'
- en: '| ReSocratic w/o filters | 11.3% | 40.1% | 30.0% | 14.3% | 29.6% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ReSocratic 无过滤器 | 11.3% | 40.1% | 30.0% | 14.3% | 29.6% |'
- en: '| ReSocratic-29k | 11.3% | 40.6% | 32.0% | 15.8% | 30.6% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| ReSocratic-29k | 11.3% | 40.6% | 32.0% | 15.8% | 30.6% |'
- en: '| Llama-3-8B-Instruct | ReSocratic w/o step questions | 32.5% | 62.9% | 42.0%
    | 31.6% | 50.2% |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B-Instruct | ReSocratic 无步骤问题 | 32.5% | 62.9% | 42.0% | 31.6% |
    50.2% |'
- en: '| ReSocratic w/o filters | 31.3% | 62.3% | 36.0% | 32.3% | 49.4% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ReSocratic 无过滤器 | 31.3% | 62.3% | 36.0% | 32.3% | 49.4% |'
- en: '| ReSocratic-29k | 32.5% | 63.5% | 44.0% | 33.0% | 51.1% |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ReSocratic-29k | 32.5% | 63.5% | 44.0% | 33.0% | 51.1% |'
- en: 5.3 Ablation Study on ReSocratic
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 ReSocratic 的消融研究
- en: 'In Table [3](#S5.T3 "Table 3 ‣ 5.2 Main Results ‣ 5 Experiments ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis"),
    we present the performance outcomes of the models with and without the application
    of filters, specifically, the rule filter and similarity filter, which are illustrated
    in Figure [3](#S3.F3 "Figure 3 ‣ 3 E-OPT: Human-Readable Optimization Problems
    Benchmark ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")(a). Additionally, we compare the performance
    when incorporating step questions versus when they are excluded, an aspect that
    is depicted in Figure [3](#S3.F3 "Figure 3 ‣ 3 E-OPT: Human-Readable Optimization
    Problems Benchmark ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")(b). We conduct an ablation study on
    Llama-2-7B-Chat and Llama-3-8B-Instruct. The experimental results show the validity
    of our filters, this conclusion is similar to RFT [[34](#bib.bib34)] that redundant
    data can negatively affect language models. Moreover, the step questions generated
    in ReSocratic can bring positive improvement to the model’s solving ability.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[3](#S5.T3 "表格 3 ‣ 5.2 主要结果 ‣ 5 实验 ‣ 基准测试 LLMs 的优化建模和通过逆费曼合成增强推理")中，我们展示了应用和不应用过滤器（具体是规则过滤器和相似性过滤器）的模型性能结果，这在图[3](#S3.F3
    "图 3 ‣ 3 E-OPT: 人类可读的优化问题基准 ‣ 基准测试 LLMs 的优化建模和通过逆费曼合成增强推理")(a)中有所说明。此外，我们比较了在包含与不包含步骤问题时的性能，这一方面在图[3](#S3.F3
    "图 3 ‣ 3 E-OPT: 人类可读的优化问题基准 ‣ 基准测试 LLMs 的优化建模和通过逆费曼合成增强推理")(b)中有所体现。我们对 Llama-2-7B-Chat
    和 Llama-3-8B-Instruct 进行了消融研究。实验结果表明了我们过滤器的有效性，这一结论与 RFT [[34](#bib.bib34)] 相似，即冗余数据可能对语言模型产生负面影响。此外，ReSocratic
    生成的步骤问题能够对模型的解决能力带来积极改进。'
- en: 5.4 Comparison between Reverse Synthesis and Forward Synthesis
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 逆合成与正向合成的比较
- en: Furthermore, we compared the forward data synthesis approach with the reverse
    data synthesis approach (our ReSocratic method). WizardLM[[47](#bib.bib47)] is
    a typical forward data synthesis method, which first prompts the language model
    to generate questions similar to the seed data and then answer them. Using the
    same seed data, we sample 1000 responses from DeepSeek-v2 with wizardLM and ReSocratic
    respectively, and then fine-tune Llama-2-7B-Chat. The experimental results are
    shown in Table [4](#S5.T4 "Table 4 ‣ 5.4 Comparison between Reverse Synthesis
    and Forward Synthesis ‣ 5 Experiments ‣ Benchmarking LLMs for Optimization Modeling
    and Enhancing Reasoning via Reverse Socratic Synthesis"). The experimental results
    show that our ReSocratic synthesis method is superior to the forward synthesis
    method. Moreover, we sample 30 pieces of data generated by ReSocratic and WizardLM
    respectively, and manually identify the data accuracy, which also shows that the
    data generated by ReSocratic is more accurate.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将前向数据合成方法与反向数据合成方法（我们的ReSocratic方法）进行了比较。WizardLM[[47](#bib.bib47)]是一个典型的前向数据合成方法，它首先提示语言模型生成与种子数据相似的问题，然后回答这些问题。使用相同的种子数据，我们分别从DeepSeek-v2中用wizardLM和ReSocratic采样了1000个响应，然后微调了Llama-2-7B-Chat。实验结果见表[4](#S5.T4
    "Table 4 ‣ 5.4 Comparison between Reverse Synthesis and Forward Synthesis ‣ 5
    Experiments ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")。实验结果表明，我们的ReSocratic合成方法优于前向合成方法。此外，我们分别采样了ReSocratic和WizardLM生成的30条数据，并手动识别数据的准确性，结果也显示ReSocratic生成的数据更为准确。
- en: 'Table 4: Comparision between ReSocratic (Reverse Method) and WizardLM (Forward
    Method). ReSocratic-29k is the synthetic data we contribute.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：ReSocratic（反向方法）与WizardLM（前向方法）的比较。ReSocratic-29k是我们贡献的合成数据。
- en: '| Model | SFT Data | Linear | Nonlinear | All |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | SFT数据 | 线性 | 非线性 | 全部 |'
- en: '| Method | Data Acc | w/ Table | w/o Table | w/ Table | w/o Table |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 数据准确率 | 有表格 | 无表格 | 有表格 | 无表格 |'
- en: '| Llama-2-7B-Chat | Wizard (1k responses) | 76.7% | 7.5% | 15.5% | 6.0% | 3.8%
    | 11.1% |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-7B-Chat | Wizard（1000个响应） | 76.7% | 7.5% | 15.5% | 6.0% | 3.8% |
    11.1% |'
- en: '| ReSocratic (1k responses) | 86.7% | 6.3% | 21.6% | 6.0% | 5.3% | 14.4% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ReSocratic（1000个响应） | 86.7% | 6.3% | 21.6% | 6.0% | 5.3% | 14.4% |'
- en: 6 Conclusion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we propose the E-OPT benchmark, which includes various types
    of data, to evaluate the ability of language models to solve mathematical optimization
    problems end-to-end. Additionally, in order to alleviate the issue of data sparsity
    and mitigate the performance gap between large models and open-source small models,
    we introduce the ReSocratic method, a reverse data synthesis approach. Experimental
    results show that our ReSocratic method outperforms the forward data synthesis
    method. After fine-tuning with our synthetic data, ReSocratic-29k, the performance
    of Llama-2-7B-Chat and Llama-3-8B-Instruct have been significantly improved, demonstrating
    the effectiveness of our synthesis method. In the future, we plan to extend ReSocratic
    to other complex reasoning tasks such as math word problem solving, and evaluate
    more large language models on our proposed E-OPT benchmark.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了E-OPT基准，该基准包括各种类型的数据，用于评估语言模型在端到端解决数学优化问题的能力。此外，为了缓解数据稀疏问题，并减小大型模型与开源小模型之间的性能差距，我们引入了ReSocratic方法，这是一种反向数据合成方法。实验结果显示，我们的ReSocratic方法优于前向数据合成方法。在使用我们的合成数据ReSocratic-29k进行微调后，Llama-2-7B-Chat和Llama-3-8B-Instruct的性能得到了显著提升，证明了我们合成方法的有效性。未来，我们计划将ReSocratic扩展到其他复杂推理任务，如数学应用题求解，并在我们提出的E-OPT基准上评估更多的大型语言模型。
- en: 7 Limitations
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: Due to financial and human resource constraints, we were unable to collect and
    annotate a larger-scale test dataset. Due to computational resource limitations,
    we only explored the application of the ReSocratic method to optimization problems.
    In the future, we plan to extend ReSocratic to other complex reasoning tasks such
    as math word problem solving, and evaluate more large language models on our proposed
    E-OPT benchmark.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于财务和人力资源的限制，我们未能收集和注释更大规模的测试数据集。由于计算资源的限制，我们只探索了ReSocratic方法在优化问题中的应用。未来，我们计划将ReSocratic扩展到其他复杂推理任务，如数学应用题求解，并在我们提出的E-OPT基准上评估更多的大型语言模型。
- en: References
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. [2020] 汤姆·布朗、本杰明·曼、尼克·赖德、梅拉妮·萨比亚、贾雷德·D·卡普兰、普拉夫拉·达里瓦尔、阿尔文·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔等。语言模型是少量学习者。*神经信息处理系统的进展*，33:1877–1901，2020年。
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. [2023] 乔什·阿基亚姆、史蒂文·阿德勒、桑迪尼·阿加瓦尔、拉玛·艾哈迈德、伊尔格·阿卡亚、弗洛伦西亚·莱奥尼·阿勒曼、迪奥戈·阿尔梅达、扬科·阿尔滕施密特、萨姆·阿尔特曼、沙亚马尔·安纳德卡特等。Gpt-4
    技术报告。*arXiv 预印本 arXiv:2303.08774*，2023。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. [2023a] 雨果·图夫龙、提博·拉夫里尔、戈蒂埃·伊扎卡德、克萨维尔·马尔蒂内、玛丽-安·拉肖、蒂莫西·拉克鲁瓦、巴普蒂斯特·罗齐耶、纳曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔等。Llama：开放且高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023a。
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. [2023b] 雨果·图夫龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马赫里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、索姆亚·巴特拉、普拉吉瓦尔·巴尔加瓦、施鲁提·博萨尔等。Llama
    2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。
- en: 'Suzgun et al. [2023] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny
    Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought
    can solve them. In *Findings of the Association for Computational Linguistics:
    ACL 2023*, pages 13003–13051, Toronto, Canada, July 2023\. Association for Computational
    Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL [https://aclanthology.org/2023.findings-acl.824](https://aclanthology.org/2023.findings-acl.824).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Suzgun et al. [2023] 米拉克·苏兹根、内森·斯凯尔斯、纳撒尼尔·沙尔利、塞巴斯蒂安·格尔曼、易·泰、郑亨元、阿坎卡·乔杜里、阮国、埃德·齐、丹尼·周、杰森·韦伊。挑战BIG-bench任务以及链式思维是否能解决它们。发表于*计算语言学协会发现：ACL
    2023*，第13003–13051页，加拿大多伦多，2023年7月。计算语言学协会。doi: 10.18653/v1/2023.findings-acl.824。网址
    [https://aclanthology.org/2023.findings-acl.824](https://aclanthology.org/2023.findings-acl.824)。'
- en: 'Huang et al. [2023a] Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, Zhicheng
    Yang, Dong Yu, Changshui Zhang, Xiaodan Liang, and Linqi Song. Clomo: Counterfactual
    logical modification with large language models. *arXiv preprint arXiv:2311.17438*,
    2023a.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2023a] 黄莹娅、洪瑞鑫、张宏铭、邵伟、杨智诚、董宇、张长水、梁晓丹、宋林祺。Clomo：利用大语言模型进行反事实逻辑修改。*arXiv
    预印本 arXiv:2311.17438*，2023a。
- en: 'Ling et al. [2017] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    Program induction by rationale generation: Learning to solve and explain algebraic
    word problems. In *Proceedings of the 55th Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, pages 158–167, Vancouver,
    Canada, July 2017\. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015.
    URL [https://aclanthology.org/P17-1015](https://aclanthology.org/P17-1015).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ling et al. [2017] 王岭、丹尼·尤加塔马、克里斯·戴尔、菲尔·布伦索姆。通过理由生成的程序归纳：学习解决和解释代数词汇问题。发表于*第55届计算语言学协会年会（第1卷：长篇论文）*，第158–167页，加拿大温哥华，2017年7月。计算语言学协会。doi:
    10.18653/v1/P17-1015。网址 [https://aclanthology.org/P17-1015](https://aclanthology.org/P17-1015)。'
- en: 'Patel et al. [2021] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are
    NLP models really able to solve simple math word problems? In *Proceedings of
    the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 2080–2094, Online, June 2021\.
    Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168.
    URL [https://aclanthology.org/2021.naacl-main.168](https://aclanthology.org/2021.naacl-main.168).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patel et al. [2021] 阿基尔·帕特尔、萨特维克·巴塔米什拉、纳文·戈亚尔。NLP模型真的能够解决简单的数学词汇问题吗？发表于*2021年北美计算语言学协会会议：人类语言技术*，第2080–2094页，在线，2021年6月。计算语言学协会。doi:
    10.18653/v1/2021.naacl-main.168。网址 [https://aclanthology.org/2021.naacl-main.168](https://aclanthology.org/2021.naacl-main.168)。'
- en: 'Yang et al. [2022] Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, and Xiaodan
    Liang. Logicsolver: Towards interpretable math word problem solving with logical
    prompt-enhanced learning. *arXiv preprint arXiv:2205.08232*, 2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等人 [2022] Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, 和 Xiaodan
    Liang. Logicsolver: 通过逻辑提示增强学习实现可解释的数学文字问题求解。*arXiv预印本 arXiv:2205.08232*, 2022。'
- en: 'Yang et al. [2023] Zhicheng Yang, Yiwei Wang, Yinya Huang, Jing Xiong, Xiaodan
    Liang, and Jing Tang. Speak like a native: Prompting large language models in
    a native style. *arXiv preprint arXiv:2311.13538*, 2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang等人 [2023] Zhicheng Yang, Yiwei Wang, Yinya Huang, Jing Xiong, Xiaodan Liang,
    和 Jing Tang. 说得像本地人一样: 用本地风格提示大型语言模型。*arXiv预印本 arXiv:2311.13538*, 2023。'
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems, 2021.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等人 [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman. 训练验证器以解决数学文字问题, 2021。
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*, 2021.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人 [2021] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, 和 Jacob Steinhardt. 使用数学数据集测量数学问题求解能力。*arXiv预印本
    arXiv:2103.03874*, 2021。
- en: 'Ramamonjison et al. [2022a] Rindra Ramamonjison, Haley Li, Timothy T. L. Yu,
    Shiqi He, Vishnu Rengan, Amin Banitalebi-Dehkordi, Zirui Zhou, and Yong Zhang.
    Augmenting operations research with auto-formulation of optimization models from
    problem descriptions. In Yunyao Li and Angeliki Lazaridou, editors, *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing: EMNLP
    2022 - Industry Track, Abu Dhabi, UAE, December 7 - 11, 2022*, pages 29–62\. Association
    for Computational Linguistics, 2022a. doi: 10.18653/V1/2022.EMNLP-INDUSTRY.4.
    URL [https://doi.org/10.18653/v1/2022.emnlp-industry.4](https://doi.org/10.18653/v1/2022.emnlp-industry.4).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramamonjison等人 [2022a] Rindra Ramamonjison, Haley Li, Timothy T. L. Yu, Shiqi
    He, Vishnu Rengan, Amin Banitalebi-Dehkordi, Zirui Zhou, 和 Yong Zhang. 通过从问题描述自动生成优化模型来增强运筹学研究。收录于
    Yunyao Li 和 Angeliki Lazaridou, 编辑，*2022年自然语言处理实证方法会议论文集：EMNLP 2022 - 行业轨道, 阿布扎比,
    阿联酋, 2022年12月7 - 11日*，第29–62页。计算语言学协会, 2022a。doi: 10.18653/V1/2022.EMNLP-INDUSTRY.4。网址
    [https://doi.org/10.18653/v1/2022.emnlp-industry.4](https://doi.org/10.18653/v1/2022.emnlp-industry.4)。'
- en: 'Xiao et al. [2023] Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica
    Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, et al. Chain-of-experts:
    When llms meet complex operations research problems. In *The Twelfth International
    Conference on Learning Representations*, 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等人 [2023] Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica
    Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, 等人. 专家链: 当LLMs遇到复杂的运筹学问题。收录于*第十二届国际学习表征会议*，2023。'
- en: 'AhmadiTeshnizi et al. [2024] Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine
    Udell. Optimus: Scalable optimization modeling with (mi) lp solvers and large
    language models. *arXiv preprint arXiv:2402.10172*, 2024.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AhmadiTeshnizi等人 [2024] Ali AhmadiTeshnizi, Wenzhi Gao, 和 Madeleine Udell.
    Optimus: 使用（mi）lp求解器和大型语言模型的可扩展优化建模。*arXiv预印本 arXiv:2402.10172*, 2024。'
- en: 'Ramamonjison et al. [2022b] Rindranirina Ramamonjison, Timothy Yu, Raymond
    Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh,
    Amin Banitalebi-Dehkordi, Zirui Zhou, and Yong Zhang. Nl4opt competition: Formulating
    optimization problems based on their natural language descriptions. In Marco Ciccone,
    Gustavo Stolovitzky, and Jacob Albrecht, editors, *Proceedings of the NeurIPS
    2022 Competitions Track*, volume 220 of *Proceedings of Machine Learning Research*,
    pages 189–203\. PMLR, 28 Nov–09 Dec 2022b. URL [https://proceedings.mlr.press/v220/ramamonjison23a.html](https://proceedings.mlr.press/v220/ramamonjison23a.html).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramamonjison等人 [2022b] Rindranirina Ramamonjison, Timothy Yu, Raymond Li, Haley
    Li, Giuseppe Carenini, Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-Dehkordi,
    Zirui Zhou, 和 Yong Zhang. Nl4opt竞赛: 基于自然语言描述的优化问题制定。收录于 Marco Ciccone, Gustavo
    Stolovitzky, 和 Jacob Albrecht, 编辑，*NeurIPS 2022竞赛论文集*，第220卷，*机器学习研究论文集*，第189–203页。PMLR,
    2022年11月28日–12月9日。网址 [https://proceedings.mlr.press/v220/ramamonjison23a.html](https://proceedings.mlr.press/v220/ramamonjison23a.html)。'
- en: 'Huang et al. [2024] Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao, and
    Benyou Wang. Mamo: a mathematical modeling benchmark with solvers. *arXiv preprint
    arXiv:2405.13144*, 2024.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang等人 [2024] Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao, 和 Benyou
    Wang. Mamo: 一种带有求解器的数学建模基准。*arXiv预印本 arXiv:2405.13144*, 2024。'
- en: 'DeepSeek-AI et al. [2024] DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan
    Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo,
    Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo
    Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui
    Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong
    Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai
    Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao,
    Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang,
    Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu
    Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi
    Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng
    Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng,
    T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen
    Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang,
    Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen,
    Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan
    Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei,
    Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li,
    Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying
    Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo,
    Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang
    You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang,
    Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu
    Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2: A strong, economical,
    and efficient mixture-of-experts language model, 2024.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DeepSeek-AI 等 [2024] DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang,
    Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian
    Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting
    Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian
    Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni,
    Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige
    Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang,
    Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming
    Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi
    Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li,
    Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong
    Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang,
    Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng
    Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi,
    Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao
    Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi
    Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu,
    Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang,
    Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi
    Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu,
    Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan
    Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda
    Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu
    Li, Zihan Wang, Zihui Gu, Zilin Li, 和 Ziwei Xie. Deepseek-v2: 一种强大、经济高效的专家混合语言模型,
    2024。'
- en: 'Gómez-Rodríguez and Williams [2023] Carlos Gómez-Rodríguez and Paul Williams.
    A confederacy of models: A comprehensive evaluation of llms on creative writing.
    *arXiv preprint arXiv:2310.08433*, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gómez-Rodríguez 和 Williams [2023] Carlos Gómez-Rodríguez 和 Paul Williams. 模型联盟：对
    llms 在创意写作中的全面评估。*arXiv 预印本 arXiv:2310.08433*，2023。
- en: 'Lin et al. [2024] Susan Lin, Jeremy Warner, JD Zamfirescu-Pereira, Matthew G
    Lee, Sauhard Jain, Shanqing Cai, Piyawat Lertvittayakumjorn, Michael Xuelin Huang,
    Shumin Zhai, Björn Hartmann, et al. Rambler: Supporting writing with speech via
    llm-assisted gist manipulation. In *Proceedings of the CHI Conference on Human
    Factors in Computing Systems*, pages 1–19, 2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 [2024] Susan Lin, Jeremy Warner, JD Zamfirescu-Pereira, Matthew G Lee,
    Sauhard Jain, Shanqing Cai, Piyawat Lertvittayakumjorn, Michael Xuelin Huang,
    Shumin Zhai, Björn Hartmann 等. Rambler：通过 llm 辅助要点处理支持写作。在 *CHI 人机交互会议论文集*，第 1–19
    页，2024。
- en: Huang et al. [2023b] Hui Huang, Shuangzhi Wu, Xinnian Liang, Bing Wang, Yanrui
    Shi, Peihao Wu, Muyun Yang, and Tiejun Zhao. Towards making the most of llm for
    translation quality estimation. In *CCF International Conference on Natural Language
    Processing and Chinese Computing*, pages 375–386\. Springer, 2023b.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 [2023b] Hui Huang, Shuangzhi Wu, Xinnian Liang, Bing Wang, Yanrui Shi,
    Peihao Wu, Muyun Yang, 和 Tiejun Zhao. 充分利用 llm 进行翻译质量评估的探讨。在 *CCF 国际自然语言处理与中文计算会议*，第
    375–386 页。Springer, 2023b。
- en: 'Xu et al. [2024] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen,
    Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization:
    Pushing the boundaries of llm performance in machine translation. *arXiv preprint
    arXiv:2401.08417*, 2024.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2024] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen,
    Benjamin Van Durme, Kenton Murray, 和 Young Jin Kim. 对比偏好优化：推动大语言模型在机器翻译中的极限表现。*arXiv
    预印本 arXiv:2401.08417*，2024年。
- en: 'Bai et al. [2024] Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan
    Tan, Shiqi Lou, Tianxing He, and Yulia Tsvetkov. Kgquiz: Evaluating the generalization
    of encoded knowledge in large language models. In *Proceedings of the ACM on Web
    Conference 2024*, pages 2226–2237, 2024.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. [2024] Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan
    Tan, Shiqi Lou, Tianxing He, 和 Yulia Tsvetkov. Kgquiz: 评估大语言模型中编码知识的泛化能力。在 *ACM网络会议论文集
    2024* 中，第2226–2237页，2024年。'
- en: 'Fei et al. [2023] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han,
    Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. Lawbench: Benchmarking
    legal knowledge of large language models. *arXiv preprint arXiv:2309.16289*, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fei et al. [2023] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han,
    Songyang Zhang, Kai Chen, Zongwen Shen, 和 Jidong Ge. Lawbench: 大语言模型法律知识的基准测试。*arXiv
    预印本 arXiv:2309.16289*，2023年。'
- en: 'Rein et al. [2023] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson
    Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman.
    Gpqa: A graduate-level google-proof q&a benchmark. *arXiv preprint arXiv:2311.12022*,
    2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rein et al. [2023] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson
    Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, 和 Samuel R Bowman.
    Gpqa: 一种研究生级别的谷歌抗性问答基准。*arXiv 预印本 arXiv:2311.12022*，2023年。'
- en: 'Cui et al. [2019] Wanyun Cui, Yanghua Xiao, Haixun Wang, Yangqiu Song, Seung-won
    Hwang, and Wei Wang. Kbqa: learning question answering over qa corpora and knowledge
    bases. *arXiv preprint arXiv:1903.02419*, 2019.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cui et al. [2019] Wanyun Cui, Yanghua Xiao, Haixun Wang, Yangqiu Song, Seung-won
    Hwang, 和 Wei Wang. Kbqa: 学习基于问答语料库和知识库的问题回答。*arXiv 预印本 arXiv:1903.02419*，2019年。'
- en: 'Liu et al. [2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *ACM Comput. Surv.*, 55(9),
    jan 2023. ISSN 0360-0300. doi: 10.1145/3560815. URL [https://doi.org/10.1145/3560815](https://doi.org/10.1145/3560815).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, 和 Graham Neubig. 预训练、提示与预测：自然语言处理中的提示方法系统综述。*ACM Comput. Surv.*, 55(9)，2023年1月。ISSN
    0360-0300。doi: 10.1145/3560815。网址 [https://doi.org/10.1145/3560815](https://doi.org/10.1145/3560815)。'
- en: 'Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting
    elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
    Danielle Belgrave, K. Cho, and A. Oh, editors, *Advances in Neural Information
    Processing Systems 35: Annual Conference on Neural Information Processing Systems
    2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022*, 2022.
    URL [http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, 和 Denny Zhou. 链式思考提示引发大语言模型的推理。在 Sanmi
    Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, 和 A. Oh 编辑的 *神经信息处理系统第35届年会：NeurIPS
    2022，2022年11月28日至12月9日，新奥尔良，LA，美国* 中，2022年。网址 [http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)。
- en: Lewkowycz et al. [2022] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan,
    Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil,
    Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and
    Vedant Misra. Solving quantitative reasoning problems with language models. In
    Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, *Advances
    in Neural Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=IFXTZERXdM7](https://openreview.net/forum?id=IFXTZERXdM7).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewkowycz et al. [2022] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan,
    Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil,
    Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, 和 Vedant
    Misra. 利用语言模型解决定量推理问题。在 Alice H. Oh, Alekh Agarwal, Danielle Belgrave, 和 Kyunghyun
    Cho 编辑的 *神经信息处理系统进展* 中，2022年。网址 [https://openreview.net/forum?id=IFXTZERXdM7](https://openreview.net/forum?id=IFXTZERXdM7)。
- en: 'Tang et al. [2024] Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo
    Wang, Dongdong Ge, and Benyou Wang. Orlm: Training large language models for optimization
    modeling. *arXiv preprint arXiv:2405.17743*, 2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 [2024] Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo Wang,
    Dongdong Ge, 和 Benyou Wang。Orlm：为优化建模训练大语言模型。*arXiv 预印本 arXiv:2405.17743*，2024年。
- en: 'Yu et al. [2023] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath:
    Bootstrap your own mathematical questions for large language models. *arXiv preprint
    arXiv:2309.12284*, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2023] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu,
    Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, 和 Weiyang Liu。Metamath：为大语言模型自助生成数学问题。*arXiv
    预印本 arXiv:2309.12284*，2023年。
- en: Liu and Yao [2024] Haoxiong Liu and Andrew Chi-Chih Yao. Augmenting math word
    problems via iterative question composing. *arXiv preprint arXiv:2401.09003*,
    2024.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Yao [2024] Haoxiong Liu 和 Andrew Chi-Chih Yao。通过迭代问题编写增强数学词汇问题。*arXiv
    预印本 arXiv:2401.09003*，2024年。
- en: Li et al. [2023] Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan
    Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. Query and response augmentation cannot
    help out-of-domain math reasoning generalization. *arXiv preprint arXiv:2310.05506*,
    2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023] Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu,
    Chuanqi Tan, Xiang Wang, 和 Chang Zhou。查询和响应增强无法帮助领域外数学推理的泛化。*arXiv 预印本 arXiv:2310.05506*，2023年。
- en: Yuan et al. [2023] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi
    Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with
    large language models. *arXiv preprint arXiv:2308.01825*, 2023.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 [2023] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi
    Tan, 和 Chang Zhou。大语言模型学习数学推理的规模关系。*arXiv 预印本 arXiv:2308.01825*，2023年。
- en: 'Lu et al. [2024] Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting
    Pan, Mingjie Zhan, and Hongsheng Li. Mathgenie: Generating synthetic data with
    question back-translation for enhancing mathematical reasoning of llms. *arXiv
    preprint arXiv:2402.16352*, 2024.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 [2024] Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting
    Pan, Mingjie Zhan, 和 Hongsheng Li。Mathgenie：通过问题反向翻译生成合成数据以增强大语言模型的数学推理能力。*arXiv
    预印本 arXiv:2402.16352*，2024年。
- en: 'Yue et al. [2023] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan
    Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid
    instruction tuning. *arXiv preprint arXiv:2309.05653*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue 等人 [2023] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun,
    Yu Su, 和 Wenhu Chen。Mammoth：通过混合指令调优构建数学通用模型。*arXiv 预印本 arXiv:2309.05653*，2023年。
- en: Wikipedia [2024] Wikipedia. Socratic method — Wikipedia, the free encyclopedia.
    [http://en.wikipedia.org/w/index.php?title=Socratic%20method&oldid=1224318017](http://en.wikipedia.org/w/index.php?title=Socratic%20method&oldid=1224318017),
    2024. [Online; accessed 31-May-2024].
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wikipedia [2024] Wikipedia。苏格拉底方法 — 维基百科，自由百科全书。 [http://en.wikipedia.org/w/index.php?title=Socratic%20method&oldid=1224318017](http://en.wikipedia.org/w/index.php?title=Socratic%20method&oldid=1224318017)，2024年。[在线；访问日期：2024年5月31日]。
- en: 'Qi et al. [2023] Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan
    Wang, and Lifu Huang. The art of SOCRATIC QUESTIONING: Recursive thinking with
    large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,
    *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*,
    pages 4177–4199, Singapore, December 2023\. Association for Computational Linguistics.
    doi: 10.18653/v1/2023.emnlp-main.255. URL [https://aclanthology.org/2023.emnlp-main.255](https://aclanthology.org/2023.emnlp-main.255).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi 等人 [2023] Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan
    Wang, 和 Lifu Huang。苏格拉底提问的艺术：与大语言模型的递归思维。在 Houda Bouamor, Juan Pino, 和 Kalika
    Bali 编辑的*2023年自然语言处理经验方法会议论文集*，第4177–4199页，新加坡，2023年12月。计算语言学协会。doi: 10.18653/v1/2023.emnlp-main.255。网址
    [https://aclanthology.org/2023.emnlp-main.255](https://aclanthology.org/2023.emnlp-main.255)。'
- en: 'Chang [2023] Edward Y. Chang. Prompting large language models with the socratic
    method. In *13th IEEE Annual Computing and Communication Workshop and Conference,
    CCWC 2023, Las Vegas, NV, USA, March 8-11, 2023*, pages 351–360\. IEEE, 2023.
    doi: 10.1109/CCWC57344.2023.10099179. URL [https://doi.org/10.1109/CCWC57344.2023.10099179](https://doi.org/10.1109/CCWC57344.2023.10099179).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang [2023] Edward Y. Chang。使用苏格拉底方法提示大语言模型。在*第13届IEEE年度计算与通信研讨会暨会议，CCWC 2023，拉斯维加斯，NV，美国，2023年3月8-11日*，第351–360页。IEEE，2023年。doi:
    10.1109/CCWC57344.2023.10099179。网址 [https://doi.org/10.1109/CCWC57344.2023.10099179](https://doi.org/10.1109/CCWC57344.2023.10099179)。'
- en: 'Shridhar et al. [2022] Kumar Shridhar, Jakub Macina, Mennatallah El-Assady,
    Tanmay Sinha, Manu Kapur, and Mrinmaya Sachan. Automatic generation of socratic
    subquestions for teaching math word problems. In Yoav Goldberg, Zornitsa Kozareva,
    and Yue Zhang, editors, *Proceedings of the 2022 Conference on Empirical Methods
    in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December
    7-11, 2022*, pages 4136–4149\. Association for Computational Linguistics, 2022.
    doi: 10.18653/V1/2022.EMNLP-MAIN.277. URL [https://doi.org/10.18653/v1/2022.emnlp-main.277](https://doi.org/10.18653/v1/2022.emnlp-main.277).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar等[2022] Kumar Shridhar, Jakub Macina, Mennatallah El-Assady, Tanmay
    Sinha, Manu Kapur, 和 Mrinmaya Sachan. 自动生成苏格拉底子问题用于数学文字问题教学。收录于 Yoav Goldberg,
    Zornitsa Kozareva, 和 Yue Zhang 主编的 *2022年自然语言处理经验方法会议论文集，EMNLP 2022，阿布扎比，阿拉伯联合酋长国，2022年12月7-11日*，第4136-4149页。计算语言学协会，2022年。doi:
    10.18653/V1/2022.EMNLP-MAIN.277。网址 [https://doi.org/10.18653/v1/2022.emnlp-main.277](https://doi.org/10.18653/v1/2022.emnlp-main.277)。'
- en: 'Zeng et al. [2023] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Marcin
    Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S.
    Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic
    models: Composing zero-shot multimodal reasoning with language. In *The Eleventh
    International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
    May 1-5, 2023*. OpenReview.net, 2023. URL [https://openreview.net/pdf?id=G2Q2Mh3avow](https://openreview.net/pdf?id=G2Q2Mh3avow).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng等[2023] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Marcin Choromanski,
    Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S. Ryoo,
    Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, 和 Pete Florence. 苏格拉底模型：将零样本多模态推理与语言组合。在
    *第十一届国际学习表征会议，ICLR 2023，卢旺达基加利，2023年5月1-5日*。OpenReview.net, 2023年。网址 [https://openreview.net/pdf?id=G2Q2Mh3avow](https://openreview.net/pdf?id=G2Q2Mh3avow)。
- en: 'Dong et al. [2023] Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang
    Sui, and Furu Wei. Large language model for science: A study on P vs. NP. *CoRR*,
    abs/2309.05689, 2023. doi: 10.48550/ARXIV.2309.05689. URL [https://doi.org/10.48550/arXiv.2309.05689](https://doi.org/10.48550/arXiv.2309.05689).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong等[2023] Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang
    Sui, 和 Furu Wei. 科学中的大语言模型：P与NP的研究。*CoRR*, abs/2309.05689, 2023年。doi: 10.48550/ARXIV.2309.05689。网址
    [https://doi.org/10.48550/arXiv.2309.05689](https://doi.org/10.48550/arXiv.2309.05689)。'
- en: 'Ang et al. [2023] Beng Heng Ang, Sujatha Das Gollapalli, and See-Kiong Ng.
    Socratic question generation: A novel dataset, models, and evaluation. In Andreas
    Vlachos and Isabelle Augenstein, editors, *Proceedings of the 17th Conference
    of the European Chapter of the Association for Computational Linguistics, EACL
    2023, Dubrovnik, Croatia, May 2-6, 2023*, pages 147–165\. Association for Computational
    Linguistics, 2023. doi: 10.18653/V1/2023.EACL-MAIN.12. URL [https://doi.org/10.18653/v1/2023.eacl-main.12](https://doi.org/10.18653/v1/2023.eacl-main.12).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ang等[2023] Beng Heng Ang, Sujatha Das Gollapalli, 和 See-Kiong Ng. 苏格拉底问题生成：一种新颖的数据集、模型和评估。收录于
    Andreas Vlachos 和 Isabelle Augenstein 主编的 *第17届欧洲计算语言学协会会议论文集，EACL 2023，克罗地亚杜布罗夫尼克，2023年5月2-6日*，第147-165页。计算语言学协会，2023年。doi:
    10.18653/V1/2023.EACL-MAIN.12。网址 [https://doi.org/10.18653/v1/2023.eacl-main.12](https://doi.org/10.18653/v1/2023.eacl-main.12)。'
- en: Bertsimas and Tsitsiklis [1997] Dimitris Bertsimas and John N Tsitsiklis. *Introduction
    to linear optimization*, volume 6. Athena Scientific Belmont, MA, 1997.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertsimas 和 Tsitsiklis [1997] Dimitris Bertsimas 和 John N Tsitsiklis. *线性优化入门*，第6卷。Athena
    Scientific Belmont, MA, 1997年。
- en: Conforti et al. [2014] Michele Conforti, Gérard Cornuéjols, Giacomo Zambelli,
    Michele Conforti, Gérard Cornuéjols, and Giacomo Zambelli. *Integer programming
    models*. Springer, 2014.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conforti等[2014] Michele Conforti, Gérard Cornuéjols, Giacomo Zambelli, Michele
    Conforti, Gérard Cornuéjols, 和 Giacomo Zambelli. *整数规划模型*. Springer, 2014。
- en: Wolsey [2020] Laurence A Wolsey. *Integer programming*. John Wiley & Sons, 2020.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolsey [2020] Laurence A Wolsey. *整数规划*. John Wiley & Sons, 2020.
- en: 'Xu et al. [2023] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models
    to follow complex instructions. *arXiv preprint arXiv:2304.12244*, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等[2023] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng,
    Chongyang Tao, 和 Daxin Jiang. Wizardlm：赋能大型语言模型以遵循复杂指令。*arXiv预印本 arXiv:2304.12244*，2023年。
- en: \resettocdepth
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: \resettocdepth
- en: Appendix
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 附录
- en: '[1 Introduction](#S1 "In Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 引言](#S1 "在基准测试LLMs用于优化建模和通过反向苏格拉底综合提升推理能力")'
- en: '[2 Related Work](#S2 "In Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 相关工作](#S2 "在基准测试LLMs用于优化建模和通过反向苏格拉底综合提升推理能力")'
- en: '[3 E-OPT: Human-Readable Optimization Problems Benchmark](#S3 "In Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 E-OPT: 可读优化问题基准](#S3 "在基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[4 ReSocratic: Reverse Socratic Data Synthesis](#S4 "In Benchmarking LLMs for
    Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 ReSocratic: 逆向苏格拉底数据合成](#S4 "在基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[5 Experiments](#S5 "In Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 实验](#S5 "在基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[5.1 Baselines and Setting](#S5.SS1 "In 5 Experiments ‣ Benchmarking LLMs for
    Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 基线和设置](#S5.SS1 "在5 实验 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[5.2 Main Results](#S5.SS2 "In 5 Experiments ‣ Benchmarking LLMs for Optimization
    Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 主要结果](#S5.SS2 "在5 实验 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[5.3 Ablation Study on ReSocratic](#S5.SS3 "In 5 Experiments ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3 ReSocratic的消融研究](#S5.SS3 "在5 实验 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[5.4 Comparison between Reverse Synthesis and Forward Synthesis](#S5.SS4 "In
    5 Experiments ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4 逆向合成与正向合成的比较](#S5.SS4 "在5 实验 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[6 Conclusion](#S6 "In Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 结论](#S6 "在基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[7 Limitations](#S7 "In Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7 局限性](#S7 "在基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[A More Detail of ReSocratic](#A1 "In Benchmarking LLMs for Optimization Modeling
    and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A ReSocratic的更多细节](#A1 "在基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[A.1 Scenario Pool](#A1.SS1 "In Appendix A More Detail of ReSocratic ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.1 场景库](#A1.SS1 "在附录A ReSocratic的更多细节 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[A.2 Synthesizing Different Data Types](#A1.SS2 "In Appendix A More Detail
    of ReSocratic ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.2 综合不同数据类型](#A1.SS2 "在附录A ReSocratic的更多细节 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[B Analysis](#A2 "In Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B 分析](#A2 "在基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[B.1 Dataset Statistical Results](#A2.SS1 "In Appendix B Analysis ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.1 数据集统计结果](#A2.SS1 "在附录B分析 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[B.2 Analysis of SFT Data and Evaluation Results](#A2.SS2 "In Appendix B Analysis
    ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis")'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.2 SFT数据和评估结果分析](#A2.SS2 "在附录B分析 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[B.2.1 Analysis of Zero-shot and Few-shot Evaluation Results on E-OPT](#A2.SS2.SSS1
    "In B.2 Analysis of SFT Data and Evaluation Results ‣ Appendix B Analysis ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.2.1 E-OPT上零样本和少样本评估结果分析](#A2.SS2.SSS1 "在B.2 SFT数据和评估结果分析 ‣ 附录B分析 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[B.2.2 Analysis of ReSocratic-29k and SFT Results](#A2.SS2.SSS2 "In B.2 Analysis
    of SFT Data and Evaluation Results ‣ Appendix B Analysis ‣ Benchmarking LLMs for
    Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.2.2 ReSocratic-29k与SFT结果分析](#A2.SS2.SSS2 "在B.2 SFT数据和评估结果分析 ‣ 附录B分析 ‣ 基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[C Benchmark and Dataset](#A3 "In Benchmarking LLMs for Optimization Modeling
    and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C 基准和数据集](#A3 "在基准测试LLMs以进行优化建模和通过逆向苏格拉底合成提升推理")'
- en: '[C.1 Data Format](#A3.SS1 "In Appendix C Benchmark and Dataset ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.1 数据格式](#A3.SS1 "在附录 C 基准和数据集 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的 LLM 基准测试")'
- en: '[C.2 Datasheet of E-OPT](#A3.SS2 "In Appendix C Benchmark and Dataset ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.2 E-OPT 数据表](#A3.SS2 "在附录 C 基准和数据集 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的 LLM 基准测试")'
- en: '[C.3 Datasheet of ReSocratic-29k](#A3.SS3 "In Appendix C Benchmark and Dataset
    ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis")'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.3 ReSocratic-29k 数据表](#A3.SS3 "在附录 C 基准和数据集 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的 LLM
    基准测试")'
- en: '[C.4 Data Hosting, Licensing, and Maintenance](#A3.SS4 "In Appendix C Benchmark
    and Dataset ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.4 数据托管、许可和维护](#A3.SS4 "在附录 C 基准和数据集 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的 LLM 基准测试")'
- en: '[D Limitations](#A4 "In Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D 限制](#A4 "在通过逆向苏格拉底综合进行优化建模和增强推理的 LLM 基准测试")'
- en: '[E All Prompts](#A5 "In Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E 所有提示](#A5 "在通过逆向苏格拉底综合进行优化建模和增强推理的 LLM 基准测试")'
- en: '[E.1 Prompts for Evaluation of E-OPT](#A5.SS1 "In Appendix E All Prompts ‣
    Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis")'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.1 E-OPT 评估提示](#A5.SS1 "在附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的 LLM 基准测试")'
- en: '[E.1.1 Zero-Shot Prompt](#A5.SS1.SSS1 "In E.1 Prompts for Evaluation of E-OPT
    ‣ Appendix E All Prompts ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.1.1 零样本提示](#A5.SS1.SSS1 "在 E.1 E-OPT 评估提示 ‣ 附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的
    LLM 基准测试")'
- en: '[E.1.2 Few-Shot Prompt](#A5.SS1.SSS2 "In E.1 Prompts for Evaluation of E-OPT
    ‣ Appendix E All Prompts ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.1.2 少样本提示](#A5.SS1.SSS2 "在 E.1 E-OPT 评估提示 ‣ 附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的
    LLM 基准测试")'
- en: '[E.1.3 Results Extraction Prompt](#A5.SS1.SSS3 "In E.1 Prompts for Evaluation
    of E-OPT ‣ Appendix E All Prompts ‣ Benchmarking LLMs for Optimization Modeling
    and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.1.3 结果提取提示](#A5.SS1.SSS3 "在 E.1 E-OPT 评估提示 ‣ 附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的
    LLM 基准测试")'
- en: '[E.2 Prompts of ReSocratic](#A5.SS2 "In Appendix E All Prompts ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.2 ReSocratic 提示](#A5.SS2 "在附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的 LLM 基准测试")'
- en: '[E.2.1 Linear Scenario Generation](#A5.SS2.SSS1 "In E.2 Prompts of ReSocratic
    ‣ Appendix E All Prompts ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.2.1 线性场景生成](#A5.SS2.SSS1 "在 E.2 ReSocratic 提示 ‣ 附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的
    LLM 基准测试")'
- en: '[E.2.2 Nonlinear Scenario Generation](#A5.SS2.SSS2 "In E.2 Prompts of ReSocratic
    ‣ Appendix E All Prompts ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.2.2 非线性场景生成](#A5.SS2.SSS2 "在 E.2 ReSocratic 提示 ‣ 附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的
    LLM 基准测试")'
- en: '[E.2.3 Question Generation](#A5.SS2.SSS3 "In E.2 Prompts of ReSocratic ‣ Appendix
    E All Prompts ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.2.3 问题生成](#A5.SS2.SSS3 "在 E.2 ReSocratic 提示 ‣ 附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的
    LLM 基准测试")'
- en: '[E.2.4 Code Generation](#A5.SS2.SSS4 "In E.2 Prompts of ReSocratic ‣ Appendix
    E All Prompts ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E.2.4 代码生成](#A5.SS2.SSS4 "在 E.2 ReSocratic 提示 ‣ 附录 E 所有提示 ‣ 通过逆向苏格拉底综合进行优化建模和增强推理的
    LLM 基准测试")'
- en: Appendix A More Detail of ReSocratic
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A ReSocratic 的更多细节
- en: We have already shown the process of our synthesis method in our paper. This
    section adds more detail to our ReSocratic.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在论文中展示了合成方法的过程。本节将添加更多关于 ReSocratic 的细节。
- en: A.1 Scenario Pool
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 场景池
- en: We collect 27 elaborate scenarios (13 linear scenarios and 14 nonlinear scenarios)
    in the scenario pool. An example is shown in the following bellow.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在场景库中收集了27个详细场景（13个线性场景和14个非线性场景）。下面展示了一个示例。
- en: '##  Define  Variables:Chip  Green  is  the  head  groundskeeper  at  Birdie  Valley  Golf  Club.  There  are  four  fertilizers  (Fertilizer  1-4)  available  in  the  market,  Chip  would  like  to  mix  them  together  to  obtain  a  mixture.  Chip  needs  to  determine  the  optimal  proportion  of  each  fertilizer  in  the  mixture.//  {"proportion  of  Fertilizer  1  in  the  compost":  "x1",  "range":  "0  <=  x1  <=  1",  "type":  "continuous"}//  {"proportion  of  Fertilizer  2  in  the  compost":  "x2",  "range":  "0  <=  x2  <=  1",  "type":  "continuous"}//  {"proportion  of  Fertilizer  3  in  the  compost":  "x3",  "range":  "0  <=  x3  <=  1",  "type":  "continuous"}//  {"proportion  of  Fertilizer  4  in  the  compost":  "x4",  "range":  "0  <=  x4  <=  1",  "type":  "continuous"}//  The  sum  of  the  proportions  should  be  1:  x1  +  x2  +  x3  +  x4  =  1##  Define  Objective  Function:The  price  of  Fertilizer  1-4  is  $21.75,  $23.75,  $22.00,  and  $19.50  per  100  pounds,  respectively.Chip  wants  to  minimize  the  cost  of  the  mixture  per  100  pounds.//  Minimize:  21.75*x1  +  23.75*x2  +  22.00*x3  +  19.50*x4##  Generate  Constraint-1:The  Nitrogen  percentage  of  Fertilizer  1-4  is  10%,  8%,  12%,  and  10%;the  Phosphorus  percentage  of  Fertilizer  1-4  is  8%,  11%,  7%,  and  10%;the  Potash  percentage  of  Fertilizer  1-4  is  12,  15%,  12%,  and  10%.Chip  knows  that  the  best  proportion  of  the  chemical  content  should  be  10-8-12  (10%  nitrogen,  8%  phosphorus,  and  12%  potash),  but  no  more  than  0.5%  above  them.  So  the  nitrogen  level  should  be  between  10%  and  10.5%;  the  phosphorus  level  should  be  between  8%  and  8.5%;  the  potash  level  should  be  between  12%  and  12.5%.//  10  <=  10*x1  +  8*x2  +  12*x3  +  10*x4  <=  10.5//  8  <=  8*x1  +  11*x2  +  7*x3  +  10*x4  <=  8.5//  12  <=  12*x1  +  15*x2  +  12*x3  +  10*x4  <=  12.5##  Generate  Constraint-2:The  mixture  should  contain  at  least  20%  Fertilizer  1.//  x1  >=  0.2'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '## 定义 变量：Chip Green 是 Birdie Valley 高尔夫俱乐部的首席场地管理员。市场上有四种肥料（肥料1-4），Chip 希望将它们混合在一起，以获得一种混合物。Chip
    需要确定混合物中每种肥料的最佳比例。// {"肥料1在混合物中的比例": "x1", "范围": "0 <= x1 <= 1", "类型": "连续"}//
    {"肥料2在混合物中的比例": "x2", "范围": "0 <= x2 <= 1", "类型": "连续"}// {"肥料3在混合物中的比例": "x3",
    "范围": "0 <= x3 <= 1", "类型": "连续"}// {"肥料4在混合物中的比例": "x4", "范围": "0 <= x4 <= 1",
    "类型": "连续"}// 各比例之和应为1：x1 + x2 + x3 + x4 = 1## 定义目标函数：肥料1-4的价格分别为每100磅21.75美元、23.75美元、22.00美元和19.50美元。Chip
    想要最小化每100磅混合物的成本。// 最小化：21.75*x1 + 23.75*x2 + 22.00*x3 + 19.50*x4## 生成约束条件-1：肥料1-4的氮含量百分比分别为10%、8%、12%和10%；肥料1-4的磷含量百分比分别为8%、11%、7%和10%；肥料1-4的钾含量百分比分别为12%、15%、12%和10%。Chip
    知道最佳化学成分比例应为10-8-12（10%氮、8%磷和12%钾），但不超过0.5%。所以氮含量应在10%和10.5%之间；磷含量应在8%和8.5%之间；钾含量应在12%和12.5%之间。//
    10 <= 10*x1 + 8*x2 + 12*x3 + 10*x4 <= 10.5// 8 <= 8*x1 + 11*x2 + 7*x3 + 10*x4
    <= 8.5// 12 <= 12*x1 + 15*x2 + 12*x3 + 10*x4 <= 12.5## 生成约束条件-2：混合物中应至少包含20%的肥料1。//
    x1 >= 0.2'
- en: We show some statistical results of our scenario pool in Figure [4](#A1.F4 "Figure
    4 ‣ A.1 Scenario Pool ‣ Appendix A More Detail of ReSocratic ‣ Benchmarking LLMs
    for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis").
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[4](#A1.F4 "Figure 4 ‣ A.1 Scenario Pool ‣ Appendix A More Detail of ReSocratic
    ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis")中展示了我们场景库的一些统计结果。
- en: '![Refer to caption](img/9792562433fb6d5fbea5dc512668fd7e.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9792562433fb6d5fbea5dc512668fd7e.png)'
- en: 'Figure 4: Statistical results of our scenario pool.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：我们场景库的统计结果。
- en: We compare the distribution of the scenario with that of the composite data
    in section [5](#A2.F5 "Figure 5 ‣ B.1 Dataset Statistical Results ‣ Appendix B
    Analysis ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis"). The data distribution of our synthetic dataset
    ReSocratic-29k is similar to the data distribution in our scenario pool.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将场景的分布与第[5](#A2.F5 "Figure 5 ‣ B.1 Dataset Statistical Results ‣ Appendix
    B Analysis ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis")节中的复合数据分布进行了比较。我们合成的数据集ReSocratic-29k的数据分布与我们的场景库中的数据分布类似。
- en: A.2 Synthesizing Different Data Types
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 合成不同数据类型
- en: To facilitate the synthesis of different types of data (linear and nonlinear),
    we produce different prompts, which are shown in Section [E.2.1](#A5.SS2.SSS1
    "E.2.1 Linear Scenario Generation ‣ E.2 Prompts of ReSocratic ‣ Appendix E All
    Prompts ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis") and Section [E.2.2](#A5.SS2.SSS2 "E.2.2 Nonlinear
    Scenario Generation ‣ E.2 Prompts of ReSocratic ‣ Appendix E All Prompts ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis").
    In addition, in the back-translate stage, to get the question text with a table
    and the question text without a table, we construct two different back-translation
    prompts, as shown in Section [E.2.3](#A5.SS2.SSS3 "E.2.3 Question Generation ‣
    E.2 Prompts of ReSocratic ‣ Appendix E All Prompts ‣ Benchmarking LLMs for Optimization
    Modeling and Enhancing Reasoning via Reverse Socratic Synthesis").
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于合成不同类型的数据（线性和非线性），我们制作了不同的提示，这些提示展示在第 [E.2.1](#A5.SS2.SSS1 "E.2.1 线性场景生成
    ‣ E.2 ReSocratic 提示 ‣ 附录 E 所有提示 ‣ 通过反向苏格拉底合成进行 LLMs 优化建模和增强推理") 节和第 [E.2.2](#A5.SS2.SSS2
    "E.2.2 非线性场景生成 ‣ E.2 ReSocratic 提示 ‣ 附录 E 所有提示 ‣ 通过反向苏格拉底合成进行 LLMs 优化建模和增强推理")
    节。此外，在回译阶段，为了获得带表格的题目文本和不带表格的题目文本，我们构建了两个不同的回译提示，如第 [E.2.3](#A5.SS2.SSS3 "E.2.3
    问题生成 ‣ E.2 ReSocratic 提示 ‣ 附录 E 所有提示 ‣ 通过反向苏格拉底合成进行 LLMs 优化建模和增强推理") 节所示。
- en: Appendix B Analysis
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 分析
- en: B.1 Dataset Statistical Results
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 数据集统计结果
- en: 'For E-OPT: We already show some statistical results of the distribution of
    the data types in our paper. In this section, we further show more statistical
    results of our E-OPT benchmark in question length and the number of variables.
    The question length refers to the number of characters in the question text. The
    results are shown in the following Figure [5](#A2.F5 "Figure 5 ‣ B.1 Dataset Statistical
    Results ‣ Appendix B Analysis ‣ Benchmarking LLMs for Optimization Modeling and
    Enhancing Reasoning via Reverse Socratic Synthesis"). The distribution of the
    number of variables involved in our E-OPT is similar to the long-tail distribution,
    with more variables, the smaller the sample size. So is the distribution of the
    question length.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 E-OPT：我们已经在论文中展示了数据类型分布的一些统计结果。在本节中，我们进一步展示了 E-OPT 基准测试在问题长度和变量数量方面的更多统计结果。问题长度指的是问题文本中的字符数。结果显示在下图
    [5](#A2.F5 "图 5 ‣ B.1 数据集统计结果 ‣ 附录 B 分析 ‣ 通过反向苏格拉底合成进行 LLMs 优化建模和增强推理") 中。我们 E-OPT
    涉及的变量数量分布类似于长尾分布，变量越多，样本量越小。问题长度的分布也是如此。
- en: '![Refer to caption](img/0420a0ea2603cde4ca85527d5baec9d0.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0420a0ea2603cde4ca85527d5baec9d0.png)'
- en: 'Figure 5: Statistical results of E-OPT'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：E-OPT 的统计结果
- en: For ReSocratic-29k We show the distribution of each type (linear-notable, linear-table,
    nonlinear-notable, nonlinear-table,) of synthetic data along with the distribution
    of question length and the number of variables in Figure [6](#A2.F6 "Figure 6
    ‣ B.1 Dataset Statistical Results ‣ Appendix B Analysis ‣ Benchmarking LLMs for
    Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis").
    The distribution of our synthesized dataset ReSocratic-29k is slightly different
    from the distribution of E-OPT. First, our data types are more balanced. In addition,
    we generate less data with variables less than or equal to 2, because there is
    only a small proportion (3/27) of samples in our scenario pool with variables
    less than or equal to 2\. The distribution of the length of the problem text we
    generate is also not a long-tail distribution. Overall, the distribution of our
    synthesized data is similar to that of the data in the scenario pool.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ReSocratic-29k，我们展示了每种类型（线性-显著、线性-表格、非线性-显著、非线性-表格）合成数据的分布，以及问题长度和变量数量的分布，如图
    [6](#A2.F6 "图 6 ‣ B.1 数据集统计结果 ‣ 附录 B 分析 ‣ 通过反向苏格拉底合成进行 LLMs 优化建模和增强推理") 所示。我们合成的数据集
    ReSocratic-29k 的分布与 E-OPT 的分布略有不同。首先，我们的数据类型更均衡。此外，我们生成的变量少于或等于 2 的数据较少，因为我们场景库中只有小部分（3/27）的样本具有少于或等于
    2 的变量。我们生成的问题文本长度的分布也不是长尾分布。总体而言，我们合成数据的分布类似于场景库中的数据。
- en: '![Refer to caption](img/3aa1a7df81306e2a539d1d8e42cedd65.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3aa1a7df81306e2a539d1d8e42cedd65.png)'
- en: 'Figure 6: Statistical results of ReSocratic-29k'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：ReSocratic-29k 的统计结果
- en: B.2 Analysis of SFT Data and Evaluation Results
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 SFT 数据分析与评估结果
- en: B.2.1 Analysis of Zero-shot and Few-shot Evaluation Results on E-OPT
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2.1 E-OPT 上零样本和少样本评估结果分析
- en: The relationship between accuracy and the number of variables. We show the evaluation
    results of Zero-shot and Few-shot setting for GPT-4 and GPT-3.5-Turbo in Figure
    [7](#A2.F7 "Figure 7 ‣ B.2.1 Analysis of Zero-shot and Few-shot Evaluation Results
    on E-OPT ‣ B.2 Analysis of SFT Data and Evaluation Results ‣ Appendix B Analysis
    ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis"). We show the accuracy of E-OPT split by data type, number
    of variables, and question length.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性与变量数量之间的关系。我们在图 [7](#A2.F7 "Figure 7 ‣ B.2.1 Analysis of Zero-shot and Few-shot
    Evaluation Results on E-OPT ‣ B.2 Analysis of SFT Data and Evaluation Results
    ‣ Appendix B Analysis ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis")中展示了 GPT-4 和 GPT-3.5-Turbo 在零样本和少样本设置下的评估结果。我们展示了按数据类型、变量数量和问题长度划分的
    E-OPT 的准确性。
- en: '![Refer to caption](img/77be99821281a9874e8c1508d93fa515.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/77be99821281a9874e8c1508d93fa515.png)'
- en: 'Figure 7: Analysis of zero-shot results of GPT-4 and GPT-3.5-Turbo.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：GPT-4 和 GPT-3.5-Turbo 的零样本结果分析。
- en: The relationship between accuracy and the length of question text. We show the
    evaluation results of Zero-shot and Few-shot settings for GPT-4 and GPT-3.5-Turbo
    in Figure [8](#A2.F8 "Figure 8 ‣ B.2.1 Analysis of Zero-shot and Few-shot Evaluation
    Results on E-OPT ‣ B.2 Analysis of SFT Data and Evaluation Results ‣ Appendix
    B Analysis ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis").
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性与问题文本长度之间的关系。我们在图 [8](#A2.F8 "Figure 8 ‣ B.2.1 Analysis of Zero-shot and
    Few-shot Evaluation Results on E-OPT ‣ B.2 Analysis of SFT Data and Evaluation
    Results ‣ Appendix B Analysis ‣ Benchmarking LLMs for Optimization Modeling and
    Enhancing Reasoning via Reverse Socratic Synthesis") 中展示了 GPT-4 和 GPT-3.5-Turbo
    在零样本和少样本设置下的评估结果。
- en: '![Refer to caption](img/13c530d06ee05851146b8f4775b88687.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/13c530d06ee05851146b8f4775b88687.png)'
- en: 'Figure 8: Analysis of few-shot results of GPT-4 and GPT-3.5-Turbo.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：GPT-4 和 GPT-3.5-Turbo 的少样本结果分析。
- en: 'According to the evaluation results on E-OPT, we can find that:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 E-OPT 的评估结果，我们可以发现：
- en: •
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-3.5-Turbo is less accurate on the questions with a higher number of variables
    and longer question length, regardless of the zero-shot or few-shot Setting. However,
    the performance of the GPT-4 is more balanced compared to the GPT-3.5-turbo.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无论是零样本还是少样本设置，GPT-3.5-Turbo 在变量数量较多和问题长度较长的问题上的准确性较低。然而，与 GPT-3.5-Turbo 相比，GPT-4
    的性能更为均衡。
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In addition, we find from the experimental results that for the LLMs we tested,
    it is more difficult to solve table questions than no-table questions, and nonlinear
    questions are more difficult than linear questions.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们从实验结果中发现，对于我们测试的 LLMs，解决表格问题比非表格问题更困难，而非线性问题比线性问题更难。
- en: B.2.2 Analysis of ReSocratic-29k and SFT Results
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2.2 ReSocratic-29k 和 SFT 结果分析
- en: We use the bert-base-uncased model to vectorize question text in E-OPT benchmark
    and ReSocratic-29k. The embedding of the question text is calculated by averaging
    the output of the last layer transformer. Next, we use the t-SNE algorithm to
    reduce the dimension of the embedding of the question texts for each type of data
    (linear-notable, linear-table, nonlinear-notable, nonlinear-table). The visualization
    is shown in Figure [9](#A2.F9 "Figure 9 ‣ B.2.2 Analysis of ReSocratic-29k and
    SFT Results ‣ B.2 Analysis of SFT Data and Evaluation Results ‣ Appendix B Analysis
    ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning via Reverse
    Socratic Synthesis").
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 bert-base-uncased 模型对 E-OPT 基准和 ReSocratic-29k 中的问题文本进行向量化。问题文本的嵌入通过对最后一层变换器的输出进行平均来计算。接下来，我们使用
    t-SNE 算法对每种数据类型（线性-显著、线性-表格、非线性-显著、非线性-表格）的文本嵌入进行降维。可视化结果显示在图 [9](#A2.F9 "Figure
    9 ‣ B.2.2 Analysis of ReSocratic-29k and SFT Results ‣ B.2 Analysis of SFT Data
    and Evaluation Results ‣ Appendix B Analysis ‣ Benchmarking LLMs for Optimization
    Modeling and Enhancing Reasoning via Reverse Socratic Synthesis") 中。
- en: '![Refer to caption](img/5b834dd107979824a18b7f0211fd6faf.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5b834dd107979824a18b7f0211fd6faf.png)'
- en: 'Figure 9: Visualization of the question texts in E-OPT benchmark and ReSocratic-29k
    using t-SNE.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：使用 t-SNE 对 E-OPT 基准和 ReSocratic-29k 中的问题文本进行的可视化。
- en: In the Experiments Section of our paper, we find that after fine-tuning with
    our ReSocratic-29k, the accuracy of Llama3-8B-Instruct and Llama2-7B-Chat on nonlinear-table
    data exceeded the nonlinear-notable table. This is different from our observation
    that "table questions are more difficult than no-table questions".
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们论文的实验部分，我们发现经过用ReSocratic-29k微调后，Llama3-8B-Instruct和Llama2-7B-Chat在非线性表格数据上的准确性超过了非线性显著表格。这与我们观察到的“表格问题比无表格问题更难”有所不同。
- en: '![Refer to caption](img/92ba7ed80a9094901beab510745072fc.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/92ba7ed80a9094901beab510745072fc.png)'
- en: 'Figure 10: The cosine similarity of the question text embedding between synthetic
    data and test dataset.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：合成数据和测试数据集之间问题文本嵌入的余弦相似度。
- en: 'As can be seen in Figure [9](#A2.F9 "Figure 9 ‣ B.2.2 Analysis of ReSocratic-29k
    and SFT Results ‣ B.2 Analysis of SFT Data and Evaluation Results ‣ Appendix B
    Analysis ‣ Benchmarking LLMs for Optimization Modeling and Enhancing Reasoning
    via Reverse Socratic Synthesis"), this is due to the deviation between the distribution
    of our synthetic data ReSocratic-29k and the distribution of E-OPT test data.
    In order to quantify the deviation between the synthesized data and the E-OPT
    test data, we calculate the embedding similarity score of each type of problem
    text. The calculation process is as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[9](#A2.F9 "Figure 9 ‣ B.2.2 Analysis of ReSocratic-29k and SFT Results ‣
    B.2 Analysis of SFT Data and Evaluation Results ‣ Appendix B Analysis ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")所示，这是由于我们合成的数据ReSocratic-29k的分布与E-OPT测试数据的分布之间存在偏差。为了量化合成数据与E-OPT测试数据之间的偏差，我们计算了每种问题文本的嵌入相似度分数。计算过程如下：
- en: '|  | $s={\rm mean}({\rm cos}(f_{i},f_{j})),i\in\texttt{E-OPT},j\in\texttt{ReSocratic-29k}$
    |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $s={\rm mean}({\rm cos}(f_{i},f_{j})),i\in\texttt{E-OPT},j\in\texttt{ReSocratic-29k}$
    |  |'
- en: The similarity score is shown in Figure [10](#A2.F10 "Figure 10 ‣ B.2.2 Analysis
    of ReSocratic-29k and SFT Results ‣ B.2 Analysis of SFT Data and Evaluation Results
    ‣ Appendix B Analysis ‣ Benchmarking LLMs for Optimization Modeling and Enhancing
    Reasoning via Reverse Socratic Synthesis"). The nonlinear-table synthetic data
    has the greatest similarity to test data, but nonlinear-notable synthetic data
    has the least similarity to test data. We believe that this is the reason for
    the fluctuations in the performance of fine-tuned small models (Llama-2-7B-Chat
    and Llama-3-8B-Instruct). Our synthesized nonlinear data is closer to the table
    questions of the nonlinear part in E-OPT and deviates far from the no-table data
    of the nonlinear part in E-OPT.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度分数如图[10](#A2.F10 "Figure 10 ‣ B.2.2 Analysis of ReSocratic-29k and SFT Results
    ‣ B.2 Analysis of SFT Data and Evaluation Results ‣ Appendix B Analysis ‣ Benchmarking
    LLMs for Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis")所示。非线性表格合成数据与测试数据的相似度最高，但非线性显著合成数据与测试数据的相似度最低。我们认为这就是微调小模型（Llama-2-7B-Chat和Llama-3-8B-Instruct）性能波动的原因。我们的合成非线性数据更接近E-OPT中非线性部分的表格问题，而远离E-OPT中非线性部分的无表格数据。
- en: Appendix C Benchmark and Dataset
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 基准和数据集
- en: Our code and data are released on [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码和数据已发布在[https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)。
- en: C.1 Data Format
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 数据格式
- en: Data Format of E-OPT.
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: E-OPT的数据格式。
- en: 'We store E-OPT data in the form of JSON files. A sample of our E-OPT benchmark
    is shown below:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将E-OPT数据存储为JSON文件格式。以下是我们E-OPT基准的样本：
- en: '{"question":  "A  rectangular  garden  is  to  be  constructed  using  a  rock  wall  as  one  side  of  the  garden  and  wire  fencing  for  the  other  three  sides.  Given  100ft  of  wire  fencing,  determine  the  dimensions  that  would  create  a  garden  of  maximum  area.  What  is  the  maximum  area?","results":  {"The  length  of  the  garden":  "50.0","The  width  of  the  garden":  "25.0","The  maximum  area  of  the  garden":  "1250.0"},"type":  "nonlinear-notable","index":  3}'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '{"question":  "一个矩形花园将被建造，利用一面石墙作为花园的一侧，其余三侧使用铁丝网。给定100英尺的铁丝网，确定可以创建最大面积花园的尺寸。最大面积是多少？","results":  {"花园的长度":  "50.0","花园的宽度":  "25.0","花园的最大面积":  "1250.0"},"type":  "nonlinear-notable","index":  3}'
- en: 'We construct samples in dictionary format, and all the data is stored as a
    list in a JSON file. Each sample has the following fields:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以字典格式构造样本，所有数据都作为列表存储在JSON文件中。每个样本具有以下字段：
- en: •
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '“question”: The question text, presented in natural language, contains the
    background as well as the optimization objective and associated constraints. In
    order to solve the question, it is necessary to first find out the variables that
    can be optimized, then build a mathematical model, and then call a code solver
    to get the optimal numerical results of the variables and objective.'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '“question”: 问题文本以自然语言呈现，包含背景信息、优化目标及相关约束。为了解决该问题，需要首先找出可以优化的变量，然后建立数学模型，再调用代码求解器来获得变量和目标的最优数值结果。'
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '“results”: This field is presented in the form of a dictionary, where the key
    is the natural language description of the variables and objectives, followed
    by their optimal values. During the annotation process, if the taggers cannot
    confirm that there is only one optimal solution to the problem, the results only
    contain the description of the optimization objective and its optimal value.'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '“results”: 该字段以字典的形式呈现，其中键是变量和目标的自然语言描述，后跟它们的最佳值。在标注过程中，如果标注者无法确认问题只有一个最优解，结果仅包含优化目标的描述及其最佳值。'
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '“type”: This field records the type of the current sample, and there are four
    types: linear-table, linear-notable, non-linear-table, and nonlinear-notable.'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '“type”: 该字段记录当前样本的类型，共有四种类型：linear-table、linear-notable、non-linear-table 和
    nonlinear-notable。'
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '“index”: The index of the sample.'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '“index”: 样本的索引。'
- en: Data Format of ReSocratic-29k.
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ReSocratic-29k 的数据格式。
- en: We show a sample of our ReSocratic-29k in the following bellow.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面展示了 ReSocratic-29k 的一个示例。
- en: '{"question":  "A  logistics  company  operates  four  different  routes  for  delivering  packages.  They  need  to  determine  the  number  of  trucks  to  allocate  to  each  route  to  optimize  their  operations.  Each  route  has  a  different  cost  and  revenue  structure.  On  route  1,  each  truck  incurs  a  cost  of  $100  per  day  and  generates  a  revenue  of  $150  per  day.  On  route  2,  each  truck  incurs  a  cost  of  $120  per  day  and  generates  a  revenue  of  $180  per  day.  On  route  3,  each  truck  incurs  a  cost  of  $140  per  day  and  generates  a  revenue  of  $210  per  day.  On  route  4,  each  truck  incurs  a  cost  of  $160  per  day  and  generates  a  revenue  of  $240  per  day.  The  company  aims  to  maximize  the  total  daily  profit  across  all  routes.  The  company  has  a  total  of  50  trucks  available.  Please  help  the  company  to  determine  the  optimal  allocation  of  trucks  to  each  route.","code_solution":  "import  math\nimport  pyscipopt\n\n#  Create  a  new  model\nmodel  =  pyscipopt.Model()\n\n#  Define  variables\nT1  =  model.addVar(vtype=\"INTEGER\",  name=\"T1\",  lb=0)  #  number  of  trucks  on  route  1\nT2  =  model.addVar(vtype=\"INTEGER\",  name=\"T2\",  lb=0)  #  number  of  trucks  on  route  2\nT3  =  model.addVar(vtype=\"INTEGER\",  name=\"T3\",  lb=0)  #  number  of  trucks  on  route  3\nT4  =  model.addVar(vtype=\"INTEGER\",  name=\"T4\",  lb=0)  #  number  of  trucks  on  route  4\n\n#  Define  objective  function\nProfit_route1  =  150  *  T1  -  100  *  T1\nProfit_route2  =  180  *  T2  -  120  *  T2\nProfit_route3  =  210  *  T3  -  140  *  T3\nProfit_route4  =  240  *  T4  -  160  *  T4\n#  So,  the  objective  function  is:  Maximize  (Profit_route1  +  Profit_route2  +  Profit_route3  +  Profit_route4)\nobj  =  model.addVar(’obj’)\nmodel.setObjective(obj,  \"maximize\")\nmodel.addCons(obj  ==  Profit_route1  +  Profit_route2  +  Profit_route3  +  Profit_route4)\n\n#  Add  constraints\n#  The  company  has  a  total  of  50  trucks  available.\nmodel.addCons(T1  +  T2  +  T3  +  T4  <=  50)\n\n#  Solve  the  problem\nmodel.optimize()\n\n#  Print  the  optimal  solution  (value  of  the  variables  &  the  objective)\nprint(’-’*10)\nif  model.getStatus()  ==  \"optimal\":\n  print(\"Number  of  Trucks  on  Route  1:  \",  model.getVal(T1))\n  print(\"Number  of  Trucks  on  Route  2:  \",  model.getVal(T2))\n  print(\"Number  of  Trucks  on  Route  3:  \",  model.getVal(T3))\n  print(\"Number  of  Trucks  on  Route  4:  \",  model.getVal(T4))\n  print(\"Maximized  Total  Daily  Profit:  \",  model.getObjVal())\nelse:\n  print(\"The  problem  could  not  be  solved  to  optimality.\")\n"}'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '{"question":  "一家物流公司运营着四条不同的配送路线。他们需要确定每条路线分配的卡车数量，以优化运营。每条路线有不同的成本和收益结构。在路线
    1 上，每辆卡车每天的成本为 $100，每天的收益为 $150。在路线 2 上，每辆卡车每天的成本为 $120，每天的收益为 $180。在路线 3 上，每辆卡车每天的成本为
    $140，每天的收益为 $210。在路线 4 上，每辆卡车每天的成本为 $160，每天的收益为 $240。公司旨在最大化所有路线的总日利润。公司总共有 50
    辆卡车。请帮助公司确定每条路线的卡车最佳分配。","code_solution":  "import  math\nimport  pyscipopt\n\n#  创建一个新的模型\nmodel  =  pyscipopt.Model()\n\n#  定义变量\nT1  =  model.addVar(vtype=\"INTEGER\",  name=\"T1\",  lb=0)  #  路线
    1 上的卡车数量\nT2  =  model.addVar(vtype=\"INTEGER\",  name=\"T2\",  lb=0)  #  路线 2
    上的卡车数量\nT3  =  model.addVar(vtype=\"INTEGER\",  name=\"T3\",  lb=0)  #  路线 3 上的卡车数量\nT4  =  model.addVar(vtype=\"INTEGER\",  name=\"T4\",  lb=0)  #  路线
    4 上的卡车数量\n\n#  定义目标函数\nProfit_route1  =  150  *  T1  -  100  *  T1\nProfit_route2  =  180  *  T2  -  120  *  T2\nProfit_route3  =  210  *  T3  -  140  *  T3\nProfit_route4  =  240  *  T4  -  160  *  T4\n#  因此，目标函数是：最大化
    (Profit_route1  +  Profit_route2  +  Profit_route3  +  Profit_route4)\nobj  =  model.addVar(’obj’)\nmodel.setObjective(obj,  \"maximize\")\nmodel.addCons(obj  ==  Profit_route1  +  Profit_route2  +  Profit_route3  +  Profit_route4)\n\n#  添加约束\n#  公司总共有
    50 辆卡车。\nmodel.addCons(T1  +  T2  +  T3  +  T4  <=  50)\n\n#  解决问题\nmodel.optimize()\n\n#  打印最优解（变量值及目标值）\nprint(’-’*10)\nif  model.getStatus()  ==  \"optimal\":\n  print(\"路线
    1 上的卡车数量： \",  model.getVal(T1))\n  print(\"路线 2 上的卡车数量： \",  model.getVal(T2))\n  print(\"路线
    3 上的卡车数量： \",  model.getVal(T3))\n  print(\"路线 4 上的卡车数量： \",  model.getVal(T4))\n  print(\"最大化的总日利润：
    \",  model.getObjVal())\nelse:\n  print(\"问题无法求解至最优。\")\n"}'
- en: 'We construct samples in dictionary format, and all the data is stored as a
    list in a JSON file. Each sample has the following fields:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以字典格式构建样本，所有数据都存储在 JSON 文件的列表中。每个样本包含以下字段：
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '“question”: The question text, presented in natural language, contains the
    background as well as the optimization objective and associated constraints. In
    order to solve the question, it is necessary to first find out the variables that
    can be optimized, then build a mathematical model, and then call code solver to
    get the optimal numerical results of the variables and objective.'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “question”：问题文本以自然语言呈现，包含背景以及优化目标和相关约束。为了解决问题，首先需要找出可以优化的变量，然后建立数学模型，再调用代码求解器以获取变量和目标的最优数值结果。
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '“code_solution”: The corresponding python code to solve the question.'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: “code_solution”：对应的python代码，用于解决问题。
- en: C.2 Datasheet of E-OPT
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 E-OPT的数据表
- en: We present a datasheet for documentation and responsible usage of E-OPT Benchmark.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了E-OPT基准的文档和负责任使用的数据表。
- en: Motivation.
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动机。
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For what purpose was the dataset created? It was created as a benchmark for
    evaluating LLM’s end-to-end solving ability for optimization modeling. The “end-to-end”
    refers to a measurement approach of text in and numerical values out.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集的创建目的是什么？它被创建为评估LLM优化建模的端到端解决能力的基准。 “端到端”指的是从文本输入到数值输出的测量方法。
- en: •
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who created the dataset (e.g., which team, research group) and on behalf of
    which entity (e.g., company, institution, organization)? It was created by the
    authors of this paper.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集由谁创建（例如，哪个团队、研究小组），代表哪个实体（例如，公司、机构、组织）？它是由本文的作者创建的。
- en: •
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who funded the creation of the dataset? If the paper is accepted, we will point
    this out in our acknowledgments.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谁资助了数据集的创建？如果论文被接受，我们将在致谢中指出这一点。
- en: Composition.
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 组成。
- en: •
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What do the instances that comprise the dataset represent (e.g., documents,
    photos, people, countries)? The dataset consists of questions, solution results
    (natural language descriptions of variables and objective, and their corresponding
    optimal numerical values), question type, and its index.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中的实例代表了什么（例如，文档、照片、人员、国家）？数据集包括问题、解决方案结果（变量和目标的自然语言描述及其对应的最优数值）、问题类型及其索引。
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How many instances are there in total (of each type, if appropriate)? There
    are 605 samples in the E-OPT benchmark. Specifically, we have four categories
    of data, of which nonlinear-notable has 133, nonlinear-table has 50, linear-notable
    has 342, and linear-table has 80
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总共有多少实例（每种类型的，如果适用）？E-OPT基准共有605个样本。具体来说，我们有四类数据，其中非线性-显著有133个，非线性-表格有50个，线性-显著有342个，线性-表格有80个。
- en: •
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Does the dataset contain all possible instances or is it a sample (not necessarily
    random) of instances from a larger set? The dataset is collected by collecting
    questions from text-books, exercises, and examinations in Universities.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集包含所有可能的实例，还是从更大的集合中提取的样本（不一定是随机的）？数据集通过从教科书、练习题和大学考试中收集问题来构建。
- en: •
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What data does each instance consist of? The solution “results” of each sample
    are obtained by coding and solving by annotators with higher education background.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个实例包含哪些数据？每个样本的解决方案“结果”由具有高学历背景的注释员进行编码和求解。
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are relationships between individual instances made explicit? The data instances
    are collected one by one by workers and categorized according to their types.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 个别实例之间的关系是否明确？数据实例由工人一个个收集，并按类型分类。
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are there recommended data splits? Yes, we recommend four data splits according
    to the data type.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否推荐数据拆分？是的，我们根据数据类型推荐四种数据拆分方式。
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are there any errors, sources of noise, or redundancies in the dataset? The
    solution results of each sample are cross-verified by at least two annotators.
    That is, when all the annotators of a sample can reach a consistent solution,
    we will adopt the data; otherwise, we discard the sample.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中是否存在错误、噪音源或冗余？每个样本的解决方案结果由至少两个注释员交叉验证。即，当所有注释员对样本能达成一致解决方案时，我们会采纳这些数据；否则，我们会丢弃该样本。
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Is the dataset self-contained, or does it link to or otherwise rely on external
    resources (e.g., websites, tweets, other datasets)? The dataset is self-contained.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是自包含的，还是链接或依赖于外部资源（例如，网站、推文、其他数据集）？数据集是自包含的。
- en: •
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Does the dataset contain data that might be considered confidential (e.g., data
    that is protected by legal privilege or by doctor-patient confidentiality, data
    that includes the content of individuals’ non-public communications)? No.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中是否包含可能被视为机密的数据（例如，受法律特权或医生-患者保密保护的数据，或包括个人非公开通讯内容的数据）？不。
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Does the dataset contain data that, if viewed directly, might be offensive,
    insulting, threatening, or might otherwise cause anxiety? No.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中是否包含直接查看可能令人反感、侮辱、威胁或可能引起焦虑的数据？不。
- en: Collection Process.
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 收集过程。
- en: •
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How was the data associated with each instance acquired? The data is directly
    observable by opening the JSON file in any Integrated Development Environment
    (IDE).
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何获取与每个实例相关的数据？数据可以通过在任何集成开发环境（IDE）中打开JSON文件直接观察。
- en: •
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What mechanisms or procedures were used to collect the data (e.g., hardware
    apparatuses or sensors, manual human curation, software programs, software APIs)?
    We assign workers to collect the question text and write code to solve those questions,
    finally the solution results (numerical values of variables and objective) are
    saved.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 收集数据时使用了什么机制或程序（例如，硬件装置或传感器、人工策划、软件程序、软件 API）？我们分配工作人员收集问题文本并编写代码解决这些问题，最后将解决方案结果（变量和目标的数值）保存下来。
- en: •
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who was involved in the data collection process (e.g., students, crowd workers,
    contractors), and how were they compensated (e.g., how much were crowd workers
    paid)? We hired six college students with a good knowledge of math and code fundamentals
    to collect and annotate the data.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谁参与了数据收集过程（例如，学生、众包工人、承包商），他们如何获得报酬（例如，众包工人获得了多少报酬）？我们聘用了六名具有良好数学和代码基础知识的大学生来收集和注释数据。
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Over what timeframe was the data collected? The final version of the dataset
    was collected in April 2024.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据是在什么时间范围内收集的？数据集的最终版本是在2024年4月收集的。
- en: Uses.
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用。
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Has the dataset been used for any tasks already? An initial subset of E-OPT
    benchmark is used in April 2024 as a contest track [(https://www.codabench.org/competitions/2438/)](https://www.codabench.org/competitions/2438/)
    of ICML 2024 Challenge on Automated Math Reasoning [(https://sites.google.com/view/ai4mathworkshopicml2024/challenges)](https://sites.google.com/view/ai4mathworkshopicml2024/challenges).
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是否已经用于任何任务？E-OPT基准的初始子集在2024年4月用于ICML 2024挑战赛的比赛轨道 [(https://www.codabench.org/competitions/2438/)](https://www.codabench.org/competitions/2438/)。
- en: •
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Is there a repository that links to any or all papers or systems that use the
    dataset? Yes, we show the our github repo [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic),
    the ICML 2024 workshop [(https://sites.google.com/view/ai4mathworkshopicml2024)](https://sites.google.com/view/ai4mathworkshopicml2024),
    and the ICML 2024 Challenges on Automated Math Reasoning [(https://www.codabench.org/competitions/2438/)](https://www.codabench.org/competitions/2438/).
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有一个库链接到任何或所有使用该数据集的论文或系统？是的，我们展示了我们的GitHub repo [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)，ICML
    2024研讨会 [(https://sites.google.com/view/ai4mathworkshopicml2024)](https://sites.google.com/view/ai4mathworkshopicml2024)，以及ICML
    2024自动数学推理挑战 [(https://www.codabench.org/competitions/2438/)](https://www.codabench.org/competitions/2438/)。
- en: Distribution.
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分发。
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Will the dataset be distributed to third parties outside of the entity (e.g.,
    company, institution, organization) on behalf of which the dataset was created?
    Yes, the dataset is publicly available on the Internet.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是否会分发给实体（例如，公司、机构、组织）以外的第三方？是的，数据集在互联网上公开可用。
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How will the dataset be distributed (e.g., tarball on website, API, GitHub)?
    The dataset can be downloaded on the github repo [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic).
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集将如何分发（例如，网站上的tarball，API，GitHub）？数据集可以在GitHub repo [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)
    上下载。
- en: •
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Will the dataset be distributed under a copyright or other intellectual property
    (IP) license, and/or under applicable terms of use (ToU)? The dataset is distributed
    under CC BY 2.0.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是否会在版权或其他知识产权（IP）许可证下分发，和/或遵循适用的使用条款（ToU）？数据集在CC BY 2.0许可证下分发。
- en: •
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have any third parties imposed IP-based or other restrictions on the data associated
    with the instances? No.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有任何第三方对与实例相关的数据施加了基于知识产权或其他的限制？没有。
- en: •
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Do any export controls or other regulatory restrictions apply to the dataset
    or to individual instances? No.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集或单个实例是否适用任何出口控制或其他监管限制？不适用。
- en: Maintenance.
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 维护。
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who will be supporting/hosting/maintaining the dataset? The authors of this
    paper.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谁将支持/托管/维护数据集？本文的作者。
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How can the owner/curator/manager of the dataset be contacted (e.g., email address)?
    Please contact Zhicheng Yang at yangzhch6@gmail.com.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何联系数据集的所有者/策展人/管理员（例如，电子邮件地址）？请通过电子邮件yangzhch6@gmail.com联系Zhicheng Yang。
- en: •
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Is there an erratum? No.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有勘误？没有。
- en: •
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Will the dataset be updated (e.g., to correct labeling errors, add new instances,
    delete instances)? Please check [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)
    for any update.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是否会更新（例如，纠正标签错误、添加新实例、删除实例）？请查看[https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)以获取任何更新。
- en: •
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If others want to extend/augment/build on/contribute to the dataset, is there
    a mechanism for them to do so? Yes, they can directly contact us through the github
    and email.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果其他人希望扩展/增强/构建或贡献于数据集，是否有机制让他们这样做？是的，他们可以通过github和电子邮件直接联系我们。
- en: C.3 Datasheet of ReSocratic-29k
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 ReSocratic-29k数据表
- en: We present a datasheet for documentation and responsible usage of E-OPT Benchmark.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了E-OPT基准的文档和负责任使用的数据表。
- en: Motivation.
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动机。
- en: •
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For what purpose was the dataset created? It was created as a dataset to fine-tune
    LLMs for optimization problem solving.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集的创建目的是什么？它是为了优化问题解决而对LLM进行微调而创建的。
- en: •
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who created the dataset (e.g., which team, research group) and on behalf of
    which entity (e.g., company, institution, organization)? It was created by the
    authors of this paper.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集由谁创建（例如，哪个团队、研究小组），代表哪个实体（例如，公司、机构、组织）？它是由本文的作者创建的。
- en: •
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who funded the creation of the dataset? If the paper is accepted, we will point
    this out in our acknowledgments.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集的创建由谁资助？如果论文被接受，我们将在致谢中指出这一点。
- en: Composition.
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 组成。
- en: •
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What do the instances that comprise the dataset represent (e.g., documents,
    photos, people, countries)? The dataset consists of optimization questions and
    its corresponding code solutions.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 组成数据集的实例代表了什么（例如，文档、照片、人物、国家）？数据集包含优化问题及其相应的代码解决方案。
- en: •
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How many instances are there in total (of each type, if appropriate)? There
    are 29164 samples in the ReSocratic-29k dataset. Specifically, we have four categories
    of data, of which nonlinear-notable has 7044 samples, nonlinear-table has 4376
    samples, linear-notable has 9286 samples, and linear-table has 8458 samples.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总共有多少实例（如果适用，每种类型的实例数量）？ReSocratic-29k数据集中共有29164个样本。具体而言，我们有四类数据，其中nonlinear-notable有7044个样本，nonlinear-table有4376个样本，linear-notable有9286个样本，linear-table有8458个样本。
- en: •
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Does the dataset contain all possible instances or is it a sample (not necessarily
    random) of instances from a larger set? The dataset is a synthetic dataset.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集包含所有可能的实例，还是从更大的集合中随机抽取的样本？数据集是合成数据集。
- en: •
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What data does each instance consist of? Each instance consists of a question
    text and its code solution.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个实例包含哪些数据？每个实例包括一个问题文本及其代码解决方案。
- en: •
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are relationships between individual instances made explicit? The data instances
    are synthesized by Deepseek-V2-Chat.
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据实例之间的关系是否明确？数据实例由Deepseek-V2-Chat合成。
- en: •
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are there recommended data splits? No
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否推荐数据拆分？没有。
- en: •
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are there any errors, sources of noise, or redundancies in the dataset? This
    dataset is synthesized by an LLM, so there is some noise in it. We measure the
    data noise to some extent in the paper.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中是否有任何错误、噪声源或冗余？这个数据集是由LLM合成的，因此其中有一些噪声。我们在论文中对数据噪声进行了一定程度的测量。
- en: •
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Is the dataset self-contained, or does it link to or otherwise rely on external
    resources (e.g., websites, tweets, other datasets)? The dataset is self-contained.
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是自包含的，还是链接到或依赖于外部资源（例如，网站、推文、其他数据集）？数据集是自包含的。
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Does the dataset contain data that might be considered confidential (e.g., data
    that is protected by legal privilege or by doctor-patient confidentiality, data
    that includes the content of individuals’ non-public communications)? No.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是否包含可能被视为机密的数据（例如，受法律特权保护的数据、医生-患者保密数据、包含个人非公开通信内容的数据）？没有。
- en: •
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Does the dataset contain data that, if viewed directly, might be offensive,
    insulting, threatening, or might otherwise cause anxiety? No.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集中是否包含直接查看可能会冒犯、侮辱、威胁或可能引起焦虑的数据？没有。
- en: Collection Process.
  id: totrans-359
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 收集过程。
- en: •
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How was the data associated with each instance acquired? The data is directly
    observable by opening the JSON file in any Integrated Development Environment
    (IDE).
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何获取与每个实例相关的数据？通过在任何集成开发环境（IDE）中打开 JSON 文件可以直接观察到数据。
- en: •
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What mechanisms or procedures were used to collect the data (e.g., hardware
    apparatuses or sensors, manual human curation, software programs, software APIs)?
    We synthesize the data using our proposed ReSocratic with Deepseek-V2-Chat.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 收集数据时使用了什么机制或程序（例如，硬件设备或传感器、手动人工筛选、软件程序、软件 API）？我们使用提议的 ReSocratic 和 Deepseek-V2-Chat
    合成数据。
- en: •
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who was involved in the data collection process (e.g., students, crowd workers,
    contractors), and how were they compensated (e.g., how much were crowd workers
    paid)? We didn’t hire anyone, we just used Deepseek-V2-Chat, an open source LLM.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谁参与了数据收集过程（例如，学生、众包工人、承包商），他们是如何得到补偿的（例如，众包工人得到了多少报酬）？我们没有雇佣任何人，只是使用了开源 LLM
    Deepseek-V2-Chat。
- en: •
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Over what timeframe was the data collected? The final version of the dataset
    was collected in April 2024.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据是在哪段时间内收集的？数据集的最终版本在 2024 年 4 月收集。
- en: Uses.
  id: totrans-368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用途。
- en: •
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Has the dataset been used for any tasks already? No.
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该数据集是否已经用于任何任务？没有。
- en: •
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Is there a repository that links to any or all papers or systems that use the
    dataset? Yes, we show the github repo [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic),
    the ICML 2024 workshop [https://sites.google.com/view/ai4mathworkshopicml2024](https://sites.google.com/view/ai4mathworkshopicml2024),
    and the ICML 2024 Challenges on Automated Math Reasoning [(https://www.codabench.org/competitions/2438/)](https://www.codabench.org/competitions/2438/).
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有一个链接到使用该数据集的任何或所有论文或系统的存储库？是的，我们展示了 github 仓库 [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)，ICML
    2024 研讨会 [https://sites.google.com/view/ai4mathworkshopicml2024](https://sites.google.com/view/ai4mathworkshopicml2024)，以及
    ICML 2024 自动化数学推理挑战赛 [(https://www.codabench.org/competitions/2438/)](https://www.codabench.org/competitions/2438/)。
- en: Distribution.
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分发。
- en: •
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Will the dataset be distributed to third parties outside of the entity (e.g.,
    company, institution, organization) on behalf of which the dataset was created?
    Yes, the dataset is publicly available on the Internet.
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是否会分发给创建数据集的实体（例如，公司、机构、组织）以外的第三方？是的，该数据集在互联网上公开可用。
- en: •
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How will the dataset be distributed (e.g., tarball on website, API, GitHub)?
    The dataset can be downloaded on the github repo [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic).
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集将如何分发（例如，网站上的 tarball、API、GitHub）？数据集可以在 github 仓库 [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)
    下载。
- en: •
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Will the dataset be distributed under a copyright or other intellectual property
    (IP) license, and/or under applicable terms of use (ToU)? The dataset is distributed
    under CC BY 2.0.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是否会根据版权或其他知识产权（IP）许可和/或适用的使用条款（ToU）进行分发？该数据集在 CC BY 2.0 许可下分发。
- en: •
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have any third parties imposed IP-based or other restrictions on the data associated
    with the instances? No.
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有第三方对与实例相关的数据施加了基于知识产权或其他限制？没有。
- en: •
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Do any export controls or other regulatory restrictions apply to the dataset
    or to individual instances? No.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集或实例的任何部分是否受到出口管制或其他法规限制？没有。
- en: Maintenance.
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 维护。
- en: •
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Who will be supporting/hosting/maintaining the dataset? The authors of this
    paper.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谁将支持/托管/维护该数据集？本文的作者。
- en: •
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How can the owner/curator/manager of the dataset be contacted (e.g., email address)?
    Please contact Zhicheng Yang at yangzhch6@gmail.com.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何联系数据集的所有者/策展人/管理员（例如，电子邮件地址）？请联系 Zhicheng Yang，电子邮件地址为 yangzhch6@gmail.com。
- en: •
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Is there an erratum? No.
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否有勘误表？没有。
- en: •
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Will the dataset be updated (e.g., to correct labeling errors, add new instances,
    delete instances)? Please check [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)
    for any update.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集是否会更新（例如，修正标注错误、添加新实例、删除实例）？请检查 [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)
    以获取任何更新。
- en: •
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If others want to extend/augment/build on/contribute to the dataset, is there
    a mechanism for them to do so? Yes, they can directly contact us through the github
    and email.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果其他人想要扩展/增强/构建或贡献到数据集中，他们是否有机制可以这样做？是的，他们可以通过 github 和电子邮件直接联系我们。
- en: C.4 Data Hosting, Licensing, and Maintenance
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 数据托管、许可与维护
- en: The dataset and code of E-OPT Benchmark and ReSocratic-29k dataset is distributed
    under the CC BY 2.0 license. The data is hosted on [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)
    (a long-term data repository). The code is also released at the github repo under
    the MIT license.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: E-OPT 基准和 ReSocratic-29k 数据集的代码和数据在 CC BY 2.0 许可下发布。数据托管在 [https://github.com/yangzhch6/ReSocratic](https://github.com/yangzhch6/ReSocratic)（一个长期数据存储库）。代码也在
    GitHub 仓库中以 MIT 许可发布。
- en: Appendix D Limitations
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 局限性
- en: Due to financial and human resource constraints, we were unable to collect and
    annotate a larger-scale test dataset. In addition, as we discussed in Section
    [B.2.2](#A2.SS2.SSS2 "B.2.2 Analysis of ReSocratic-29k and SFT Results ‣ B.2 Analysis
    of SFT Data and Evaluation Results ‣ Appendix B Analysis ‣ Benchmarking LLMs for
    Optimization Modeling and Enhancing Reasoning via Reverse Socratic Synthesis"),
    due to the lack of scenario samples, there is some deviation between our synthesized
    data ReSocratic-29k and the benchmark data E-OPT. In the future, we will add more
    diverse scenario samples for data synthesis. We will also explore the application
    of the language model self-refine mechanism to data synthesis to generate more
    accurate data. Due to computational resource limitations, we only explored the
    application of the ReSocratic method to optimization problems. In the future,
    we plan to extend ReSocratic to other complex reasoning tasks such as math word
    problem solving, and evaluate more large language models on our proposed E-OPT
    benchmark.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 由于财务和人力资源限制，我们未能收集和标注更大规模的测试数据集。此外，正如我们在第 [B.2.2](#A2.SS2.SSS2 "B.2.2 ReSocratic-29k
    和 SFT 结果分析 ‣ B.2 SFT 数据和评估结果分析 ‣ 附录 B 分析 ‣ 用于优化建模的基准测试 LLM 和通过反向苏格拉底综合提升推理") 节中讨论的，由于缺乏场景样本，我们合成的数据
    ReSocratic-29k 和基准数据 E-OPT 之间存在一些偏差。未来，我们将增加更多多样化的场景样本用于数据合成。我们还将探索语言模型自我精炼机制在数据合成中的应用，以生成更准确的数据。由于计算资源的限制，我们仅探索了
    ReSocratic 方法在优化问题上的应用。未来，我们计划将 ReSocratic 扩展到其他复杂推理任务，如数学应用题，并在我们提出的 E-OPT 基准上评估更多的大型语言模型。
- en: Appendix E All Prompts
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 所有提示
- en: We show all the prompts we used in this section.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了本节中使用的所有提示。
- en: E.1 Prompts for Evaluation of E-OPT
  id: totrans-401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 E-OPT 评估的提示
- en: E.1.1 Zero-Shot Prompt
  id: totrans-402
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.1.1 零样本提示
- en: '“system”:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: “系统”：
- en: 1Please  use  python  code  to  solve  the  given  question.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 请使用 python 代码解决给定的问题。
- en: '“user”:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: “用户”：
- en: '[Code  Template]:‘‘‘pythonimport  mathimport  pyscipopt#  Create  a  new  modelmodel  =  pyscipopt.Model()#  Define  variables...#  Define  objective  function##  set  objective  as  a  variable  (pyscipopt  does  not  support  non-linear  objective)obj  =  model.addVar(’obj’)model.setObjective(obj,  "...")  #  "maximize"  or  "minimize"model.addCons(obj  ==  ...)  #  obj  function  as  a  constraint#  Add  constraints...#  Solve  the  problemmodel.optimize()#  Print  the  optimal  solution  (value  of  the  variables  &  the  objective)print(’-’*10)if  model.getStatus()  ==  "optimal":...else:print("The  problem  could  not  be  solved  to  optimality.")‘‘‘[Follow  the  code  template  to  solve  the  given  question,  your  code  should  be  enclosed  in  ‘‘‘python\n{}‘‘‘]:‘‘‘question<...  A  testing  question  here  ...>‘‘‘'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码模板]：‘‘‘pythonimport  mathimport  pyscipopt#  创建  一个  新  模型model  =  pyscipopt.Model()#  定义  变量...#  定义  目标  函数##  将  目标  设  为  一个  变量  (pyscipopt  不  支持  非线性  目标)obj  =  model.addVar(’obj’)model.setObjective(obj,  "...")  #  "最大化"  或  "最小化"model.addCons(obj  ==  ...)  #  obj  函数  作为  约束#  添加  约束...#  求解  问题model.optimize()#  打印  最优  解  (变量  的  值  &  目标)print(’-’*10)if  model.getStatus()  ==  "optimal":...else:print("问题  无法  求解  到  最优。")‘‘‘[按照  代码  模板  解决  给定  问题，你的代码  应  包含  在  ‘‘‘python\n{}‘‘‘]:‘‘‘question<...  这里  是  测试  问题
    ...>‘‘‘'
- en: E.1.2 Few-Shot Prompt
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.1.2 少量示例提示
- en: '“system”:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: “系统”：
- en: 1Please  follow  the  given  examples  and  use  python  code  to  solve  the  given  question.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照给定的示例，并使用 python 代码解决给定的问题。
- en: '“user”:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: “用户”：
- en: '[Example-1]:‘‘‘questionA  bakery  specializes  in  producing  two  types  of  cakes:  chocolate  and  vanilla.  The  bakery  needs  to  decide  how  many  of  each  type  of  cake  to  produce  daily  to  maximize  profit  while  considering  the  availability  of  ingredients  and  the  minimum  daily  production  requirement.  The  profit  from  each  chocolate  cake  is  $5,  and  from  each  vanilla  cake  is  $4.  The  bakery  aims  to  maximize  its  daily  profit  from  cake  sales.  Each  chocolate  cake  requires  2  eggs,  and  each  vanilla  cake  requires  1  egg.  The  bakery  has  a  daily  supply  of  100  eggs.  Please  help  the  bakery  determine  the  optimal  number  of  chocolate  and  vanilla  cakes  to  produce  daily.‘‘‘‘‘‘pythonimport  mathimport  pyscipopt#  Create  a  new  modelmodel  =  pyscipopt.Model()#  Define  variables##  The  number  of  each  type  of  cake  to  produce  dailyChoc  =  model.addVar(vtype="INTEGER",  name="Choc",  lb=0)  #  number  of  chocolate  cakesVan  =  model.addVar(vtype="INTEGER",  name="Van",  lb=0)  #  number  of  vanilla  cakes#  Define  objective  function##  set  objective  as  a  variableobj  =  model.addVar(’obj’)model.setObjective(obj,  "maximize")model.addCons(obj  ==  5*Choc  +  4*Van)#  Add  constraints##  Each  chocolate  cake  requires  2  eggs,  and  each  vanilla  cake  requires  1  egg.  The  bakery  has  a  daily  supply  of  100  eggs.model.addCons(2*Choc  +  Van  <=  100)#  Solve  the  problemmodel.optimize()#  Print  the  optimal  solution  (value  of  the  variables  &  the  objective)print(’-’*10)if  model.getStatus()  ==  "optimal":print("Number  of  chocolate  cakes:  ",  model.getVal(Choc))print("Number  of  vanilla  cakes:  ",  model.getVal(Van))print("Maximized  Daily  Profit:  ",  model.getObjVal())else:print("The  problem  could  not  be  solved  to  optimality.")‘‘‘[Example-2]:‘‘‘questionA  company  produces  three  types  of  widgets:  X,  Y,  and  Z.  The  company  needs  to  determine  how  many  units  of  each  widget  to  produce  in  next  week.For  Widget  X,  the  selling  price  is  10$,  the  material  cost  is  5$,  and  the  production  time  is  2  hours.For  Widget  Y,  the  selling  price  is  15$,  the  material  cost  is  7$,  and  the  production  time  is  3  hours.For  Widget  Z,  the  selling  price  is  20$,  the  material  cost  is  9$,  and  the  production  time  is  4  hours.The  company  has  $500  available  for  material  costs  next  week.  The  company  wants  to  produce  at  least  10  units  of  each  widget  next  week.  The  company  wants  to  spend  at  most  200  hours  on  production  next  week.  The  company  has  only  one  production  line  and  can  only  produce  one  widget  at  a  time.  Please  help  the  company  to  maximize  the  rate  at  which  it  earns  profits  (which  is  defined  as  the  sum  of  the  selling  profit  divided  by  the  sum  of  the  production  times).‘‘‘‘‘‘pythonimport  mathimport  pyscipopt#  Create  a  new  modelmodel  =  pyscipopt.Model()#  Define  variables##  The  company  wants  to  produce  at  least  10  units  of  each  widget  next  week.X  =  model.addVar(vtype="INTEGER",  name="X",  lb=10)  #  number  of  units  of  widget  XY  =  model.addVar(vtype="INTEGER",  name="Y",  lb=10)  #  number  of  units  of  widget  YZ  =  model.addVar(vtype="INTEGER",  name="Z",  lb=10)  #  number  of  units  of  widget  Z#  Define  objective  function##  set  objective  as  a  variable  (pyscipopt  does  not  support  non-linear  objective)obj  =  model.addVar(’obj’)model.setObjective(obj,  "maximize")Profit_X  =  (10  -  5)  *  XProfit_Y  =  (15  -  7)  *  YProfit_Z  =  (20  -  9)  *  ZProductionTime  =  2  *  X  +  3  *  Y  +  4  *  Z##  the  objective  function  is:  Maximize  (Profit_X  +  Profit_Y  +  Profit_Z)  /  ProductionTime##  convert  the  division  to  multiplicationmodel.addCons(obj  *  ProductionTime  ==  Profit_X  +  Profit_Y  +  Profit_Z)#  Add  constraints##  The  company  has  $500  available  for  material  costs  next  week.model.addCons(5  *  X  +  7  *  Y  +  9  *  Z  <=  500)##  The  company  wants  to  spend  at  most  200  hours  on  production  next  week.model.addCons(2  *  X  +  3  *  Y  +  4  *  Z  <=  200)#  Solve  the  problemmodel.optimize()#  Print  the  optimal  solution  (value  of  the  variables  &  the  objective)print(’-’*10)if  model.getStatus()  ==  "optimal":print("Number  of  Widget  X:  ",  model.getVal(X))print("Number  of  Widget  Y:  ",  model.getVal(Y))print("Number  of  Widget  Z:  ",  model.getVal(Z))print("Maximized  Profit  Rate:  ",  model.getObjVal())else:print("The  problem  could  not  be  solved  to  optimality.")‘‘‘[Follow  the  examples  to  solve  the  given  question]:‘‘‘question<...  A  testing  question  here  ...>‘‘‘'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例-1]:‘‘‘问题A 一家面包店专注于生产两种类型的蛋糕：巧克力蛋糕和香草蛋糕。面包店需要决定每天生产每种类型蛋糕的数量，以最大化利润，同时考虑原料的供应情况和最低的每日生产要求。每个巧克力蛋糕的利润为$5，每个香草蛋糕的利润为$4。面包店的目标是最大化其蛋糕销售的每日利润。每个巧克力蛋糕需要2个鸡蛋，每个香草蛋糕需要1个鸡蛋。面包店每天供应100个鸡蛋。请帮助面包店确定每天生产的巧克力蛋糕和香草蛋糕的最佳数量。‘‘‘‘‘pythonimport  mathimport  pyscipopt#  创建一个新模型model  =  pyscipopt.Model()#  定义变量##  每天生产的每种类型的蛋糕数量Choc  =  model.addVar(vtype="INTEGER",  name="Choc",  lb=0)  #  巧克力蛋糕的数量Van  =  model.addVar(vtype="INTEGER",  name="Van",  lb=0)  #  香草蛋糕的数量#  定义目标函数##  将目标设置为一个变量obj  =  model.addVar(’obj’)model.setObjective(obj,  "maximize")model.addCons(obj  ==  5*Choc  +  4*Van)#  添加约束##  每个巧克力蛋糕需要2个鸡蛋，每个香草蛋糕需要1个鸡蛋。面包店每天供应100个鸡蛋。model.addCons(2*Choc  +  Van  <=  100)#  解决问题model.optimize()#  打印最优解（变量的值和目标值）print(’-’*10)if  model.getStatus()  ==  "optimal":print("巧克力蛋糕的数量:  ",  model.getVal(Choc))print("香草蛋糕的数量:  ",  model.getVal(Van))print("最大化的每日利润:  ",  model.getObjVal())else:print("问题无法解决到最优解。")‘‘‘[示例-2]:‘‘‘问题A
    一家公司生产三种类型的小工具：X、Y 和 Z。公司需要确定下周每种小工具生产的数量。对于小工具 X，售价为 $10，材料成本为 $5，生产时间为 2 小时。对于小工具
    Y，售价为 $15，材料成本为 $7，生产时间为 3 小时。对于小工具 Z，售价为 $20，材料成本为 $9，生产时间为 4 小时。公司下周有 $500 可用于材料成本。公司希望下周每种小工具至少生产
    10 个。公司希望下周生产的总时间最多为 200 小时。公司只有一条生产线，只能一次生产一种小工具。请帮助公司最大化其利润率（利润率定义为利润总和除以生产时间总和）。‘‘‘‘‘pythonimport  mathimport  pyscipopt#  创建一个新模型model  =  pyscipopt.Model()#  定义变量##  公司希望下周每种小工具至少生产
    10 个X  =  model.addVar(vtype="INTEGER",  name="X",  lb=10)  #  小工具 X 的数量Y  =  model.addVar(vtype="INTEGER",  name="Y",  lb=10)  #  小工具
    Y 的数量Z  =  model.addVar(vtype="INTEGER",  name="Z",  lb=10)  #  小工具 Z 的数量#  定义目标函数##  将目标设置为一个变量（pyscipopt
    不支持非线性目标）obj  =  model.addVar(’obj’)model.setObjective(obj,  "maximize")Profit_X  =  (10  -  5)  *  XProfit_Y  =  (15  -  7)  *  YProfit_Z  =  (20  -  9)  *  ZProductionTime  =  2  *  X  +  3  *  Y  +  4  *  Z##  目标函数是：最大化  (Profit_X  +  Profit_Y  +  Profit_Z)  /  ProductionTime##  将除法转换为乘法model.addCons(obj  *  ProductionTime  ==  Profit_X  +  Profit_Y  +  Profit_Z)#  添加约束##  公司下周有
    $500 可用于材料成本。model.addCons(5  *  X  +  7  *  Y  +  9  *  Z  <=  500)##  公司希望下周生产的总时间最多为
    200 小时。model.addCons(2  *  X  +  3  *  Y  +  4  *  Z  <=  200)#  解决问题model.optimize()#  打印最优解（变量的值和目标值）print(’-’*10)if  model.getStatus()  ==  "optimal":print("小工具
    X 的数量:  ",  model.getVal(X))print("小工具 Y 的数量:  ",  model.getVal(Y))print("小工具
    Z 的数量:  ",  model.getVal(Z))print("最大化的利润率:  ",  model.getObjVal())else:print("问题无法解决到最优解。")‘‘‘[按照示例解决给定问题]:‘‘‘问题<...  这里是一个测试问题
    ...>‘‘‘'
- en: E.1.3 Results Extraction Prompt
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.1.3 结果提取提示
- en: ‘‘‘python<...  solution  code  generated  by  the  LLM  ...>‘‘‘‘‘‘code  output<...  code  execution  result  ...>‘‘‘Accoding  to  the  code  output,  please  give  your  final  answer  for  the  following  query.  (The  answer  should  be  boxed  in  ’\\boxed{}’,  and  only  in  numerical  form,  and  round  it  to  5  decimal  places,  such  as  ’\\boxed{27.00000}’,  ’\\boxed{3.20000}’,  and  ’\\boxed{0.23334}’).<...  query  for  the  variables  and  objective  ...>
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ‘‘‘python<... LLM生成的解决方案代码 ...>‘‘‘‘‘‘代码输出<... 代码执行结果 ...>‘‘‘根据代码输出，请为以下查询给出最终答案。（答案应当用'\\boxed{}'框住，并且仅以数字形式给出，四舍五入到5位小数，例如'\\boxed{27.00000}'，'\\boxed{3.20000}'，以及'\\boxed{0.23334}'）。<...
    变量和目标的查询 ...>
- en: E.2 Prompts of ReSocratic
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 ReSocratic提示
- en: E.2.1 Linear Scenario Generation
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2.1 线性场景生成
- en: '“system”:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '“系统”:'
- en: Please  follow  the  scenario  examples  to  generate  a  [New  Scenario]  with  a  new  background.  The  scenario  should  be  a  real-world  linear  optimization  problem.  Make  sure  that  the  mathematical  logic  in  [New  Scenario]  is  correct.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照场景示例生成一个具有新背景的[新场景]。该场景应为一个现实世界的线性优化问题。确保[新场景]中的数学逻辑是正确的。
- en: '“user”:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '“用户”:'
- en: '[Scenario  Format]:##  Define  Variables:natural  language  description.//  formal  definition  of  variables  (integer,  real,  binary,  etc.)  and  their  domains.##  Define  Objective  Function:natural  language  description.//  formal  definition  of  an  objective  function,  maximize  or  minimize  something.  There  can  only  be  one  objective  function.##  Generate  Constraint-1:natural  language  description.//  formal  definition  of  constraint-1...##  Generate  Constraint-n:natural  language  description.//  formal  definition  of  constraint-n<...  Sample  2  scenarios  in  the  example  pool  ...>[New  Scenario]:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[场景格式]:## 定义变量：自然语言描述。// 变量（整数、实数、二进制等）及其定义的正式定义。## 定义目标函数：自然语言描述。// 目标函数的正式定义，最大化或最小化某个量。只能有一个目标函数。##
    生成约束条件-1：自然语言描述。// 约束条件-1的正式定义...## 生成约束条件-n：自然语言描述。// 约束条件-n的正式定义<... 示例2个场景在示例池中
    ...>[新场景]:'
- en: E.2.2 Nonlinear Scenario Generation
  id: totrans-420
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2.2 非线性场景生成
- en: '“system”:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '“系统”:'
- en: Please  follow  the  scenario  examples  to  generate  a  [New  Scenario]  with  a  new  background.  The  scenario  should  be  a  real-world  **nonlinear**  optimization  problem.  Make  sure  that  the  mathematical  logic  in  [New  Scenario]  is  correct.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照场景示例生成一个具有新背景的[新场景]。该场景应为一个现实世界的**非线性**优化问题。确保[新场景]中的数学逻辑是正确的。
- en: '“user”:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '“用户”:'
- en: '[Scenario  Format]:##  Define  Variables:natural  language  description.//  formal  definition  of  variables  (integer,  real,  binary,  etc.)  and  their  domains.##  Define  Objective  Function:natural  language  description.//  formal  definition  of  a  **nonlinear**  objective  function,  maximize  or  minimize  something.  There  can  only  be  one  objective.##  Generate  Constraint-1:natural  language  description.//  formal  definition  of  constraint-1...##  Generate  Constraint-n:natural  language  description.//  formal  definition  of  constraint-n<...  Sample  2  scenarios  in  the  example  pool  ...>[New  Scenario]:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '[场景格式]:## 定义变量：自然语言描述。// 变量（整数、实数、二进制等）及其定义的正式定义。## 定义目标函数：自然语言描述。// **非线性**目标函数的正式定义，最大化或最小化某个量。只能有一个目标。##
    生成约束条件-1：自然语言描述。// 约束条件-1的正式定义...## 生成约束条件-n：自然语言描述。// 约束条件-n的正式定义<... 示例2个场景在示例池中
    ...>[新场景]:'
- en: E.2.3 Question Generation
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2.3 问题生成
- en: '“system”:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '“系统”:'
- en: You  are  a  mathematical  assistant.  Now,  you  will  be  provided  with  an  optimization  scenario.  Please  follow  the  example  to  convert  the  given  scenario  to  question.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个数学助手。现在，你将会得到一个优化场景。请按照示例将给定的场景转换为问题。
- en: Generating questions without table.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 不使用表格生成问题。
- en: '“user”:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '“用户”:'
- en: '[Task  Description]:You  will  be  given  a  scenario  that  involves  optimization  problem.  The  scenario  is  organized  into  a  few  sections  start  with  "##".Each  section  contains  a  few  lines  of  text  that  describe  the  scenario.  The  mathematical  formal  solution  of  the  scenario  is  provided  in  the  comments  starting  with  "//".Your  job  is  to  convert  the  scenario  into  a  question  without  missing  any  information.  The  question  should  be  clear  and  concise,  and  do  not  expose  the  mathematical  formal  solution  of  the  scenario.[Example  of  converting  a  Scenario  to  a  Question]:‘‘‘scenario##  Define  Variables:A  company  produces  five  types  of  widgets:  X,  Y,  Z,  W,  and  V.  The  company  needs  to  determine  how  many  units  of  each  widget  to  produce  in  next  week.//  {"number  of  units  of  widget  X":  "X",  "range":  "X  >=  0",  "type":  "integer"}//  {"number  of  units  of  widget  Y":  "Y",  "range":  "Y  >=  0",  "type":  "integer"}//  {"number  of  units  of  widget  Z":  "Z",  "range":  "Z  >=  0",  "type":  "integer"}//  {"number  of  units  of  widget  W":  "W",  "range":  "W  >=  0",  "type":  "integer"}//  {"number  of  units  of  widget  V":  "V",  "range":  "V  >=  0",  "type":  "integer"}##  Define  Objective  Function:For  Widget  X,  the  selling  price  is  $10,  the  material  cost  is  $5,  and  the  production  time  is  2  hours.For  Widget  Y,  the  selling  price  is  $15,  the  material  cost  is  $7,  and  the  production  time  is  3  hours.For  Widget  Z,  the  selling  price  is  $20,  the  material  cost  is  $9,  and  the  production  time  is  4  hours.For  Widget  W,  the  selling  price  is  $25,  the  material  cost  is  $11,  and  the  production  time  is  5  hours.For  Widget  V,  the  selling  price  is  $30,  the  material  cost  is  $13,  and  the  production  time  is  6  hours.The  company  has  only  one  production  line  and  can  only  produce  one  widget  at  a  time.  The  company  aims  to  maximize  the  rate  at  which  it  earns  profits  (which  is  defined  as  the  sum  of  the  selling  profit  divided  by  the  sum  of  the  production  times).//  Selling  profit  of  X:  Profit_X  =  (10  -  5)  *  X//  Selling  profit  of  Y:  Profit_Y  =  (15  -  7)  *  Y//  Selling  profit  of  Z:  Profit_Z  =  (20  -  9)  *  Z//  Selling  profit  of  W:  Profit_W  =  (25  -  11)  *  W//  Selling  profit  of  V:  Profit_V  =  (30  -  13)  *  V//  So,  the  objective  function  is:  Maximize  (Profit_X  +  Profit_Y  +  Profit_Z  +  Profit_W  +  Profit_V)  /  (2  *  X  +  3  *  Y  +  4  *  Z  +  5  *  W  +  6  *  V)##  Generate  Constraint-1:The  company  has  $900  available  for  material  costs  next  week.//  5  *  X  +  7  *  Y  +  9  *  Z  +  11  *  W  +  13  *  V  <=  900##  Generate  Constraint-2:The  company  wants  to  produce  at  least  10  units  of  each  widget  next  week.//  X  >=  10;  Y  >=  10;  Z  >=  10;  W  >=  10;  V  >=  10##  Generate  Constraint-3:The  company  wants  to  spend  at  most  200  hours  on  production  next  week.//  2  *  X  +  3  *  Y  +  4  *  Z  +  5  *  W  +  6  *  V  <=  200##  Generate  Constraint-4:The  company  wants  to  ensure  that  the  total  production  of  Widget  W  does  not  exceed  the  combined  production  of  Widgets  X,  Y,  and  Z.//  W  <=  X  +  Y  +  Z‘‘‘‘‘‘questionA  company  produces  five  types  of  widgets:  X,  Y,  Z,  W,  and  V.  The  company  needs  to  determine  how  many  units  of  each  widget  to  produce  in  next  week.For  Widget  X,  the  selling  price  is  $10,  the  material  cost  is  $5,  and  the  production  time  is  2  hours.For  Widget  Y,  the  selling  price  is  $15,  the  material  cost  is  $7,  and  the  production  time  is  3  hours.For  Widget  Z,  the  selling  price  is  $20,  the  material  cost  is  $9,  and  the  production  time  is  4  hours.For  Widget  W,  the  selling  price  is  $25,  the  material  cost  is  $11,  and  the  production  time  is  5  hours.For  Widget  V,  the  selling  price  is  $30,  the  material  cost  is  $13,  and  the  production  time  is  6  hours.The  company  has  $900  available  for  material  costs  next  week.  The  company  wants  to  produce  at  least  10  units  of  each  widget  next  week.  The  company  wants  to  spend  at  most  200  hours  on  production  next  week.  The  company  wants  to  ensure  that  the  total  production  of  Widget  W  does  not  exceed  the  combined  production  of  Widgets  X,  Y,  and  Z.  The  company  has  only  one  production  line  and  can  only  produce  one  widget  at  a  time.Please  help  the  company  to  maximize  the  rate  at  which  it  earns  profits  (which  is  defined  as  the  sum  of  the  selling  profit  divided  by  the  sum  of  the  production  times).‘‘‘[Follow  the  Example  to  Convert  the  following  Scenario  to  a  Question]:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 一家公司生产五种类型的小部件：X、Y、Z、W 和 V。公司需要确定下周每种小部件的生产数量。对于小部件 X，售价为 $10，材料成本为 $5，生产时间为
    2 小时。对于小部件 Y，售价为 $15，材料成本为 $7，生产时间为 3 小时。对于小部件 Z，售价为 $20，材料成本为 $9，生产时间为 4 小时。对于小部件
    W，售价为 $25，材料成本为 $11，生产时间为 5 小时。对于小部件 V，售价为 $30，材料成本为 $13，生产时间为 6 小时。公司下周材料成本预算为
    $900。公司希望下周每种小部件至少生产 10 个。公司希望下周生产时间不超过 200 小时。公司希望确保小部件 W 的总生产量不超过小部件 X、Y 和 Z
    的总生产量。公司只有一条生产线，只能同时生产一种小部件。请帮助公司最大化其利润率（即销售利润总和除以生产时间总和）。
- en: Generating questions with table.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 生成带有表格的问题。
- en: '“user”:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: “用户”：
- en: '[Task  Description]:You  will  be  given  a  scenario  that  involves  optimization  problem.  The  scenario  is  organized  into  a  few  sections  start  with  "##".Each  section  contains  a  few  lines  of  text  that  describe  the  scenario.  The  mathematical  formal  solution  of  the  scenario  is  provided  in  the  comments  starting  with  "//".Your  job  is  to  convert  the  scenario  into  a  question  without  missing  any  information.  The  question  should  be  clear  and  concise,  and  do  not  expose  the  mathematical  formal  solution  of  the  scenario.[Example  of  converting  a  Scenario  to  a  Question  with  table]:‘‘‘scenario##  Define  Variables:A  company  produces  five  types  of  widgets:  X,  Y,  Z,  W,  and  V.  The  company  needs  to  determine  how  many  units  of  each  widget  to  produce  in  next  week.//  {"number  of  units  of  widget  X":  "X",  "range":  "X  >=  0",  "type":  "integer"}//  {"number  of  units  of  widget  Y":  "Y",  "range":  "Y  >=  0",  "type":  "integer"}//  {"number  of  units  of  widget  Z":  "Z",  "range":  "Z  >=  0",  "type":  "integer"}//  {"number  of  units  of  widget  W":  "W",  "range":  "W  >=  0",  "type":  "integer"}//  {"number  of  units  of  widget  V":  "V",  "range":  "V  >=  0",  "type":  "integer"}##  Define  Objective  Function:For  Widget  X,  the  selling  price  is  $10,  the  material  cost  is  $5,  and  the  production  time  is  2  hours.For  Widget  Y,  the  selling  price  is  $15,  the  material  cost  is  $7,  and  the  production  time  is  3  hours.For  Widget  Z,  the  selling  price  is  $20,  the  material  cost  is  $9,  and  the  production  time  is  4  hours.For  Widget  W,  the  selling  price  is  $25,  the  material  cost  is  $11,  and  the  production  time  is  5  hours.For  Widget  V,  the  selling  price  is  $30,  the  material  cost  is  $13,  and  the  production  time  is  6  hours.The  company  has  only  one  production  line  and  can  only  produce  one  widget  at  a  time.  The  company  aims  to  maximize  the  rate  at  which  it  earns  profits  (which  is  defined  as  the  sum  of  the  selling  profit  divided  by  the  sum  of  the  production  times).//  Selling  profit  of  X:  Profit_X  =  (10  -  5)  *  X//  Selling  profit  of  Y:  Profit_Y  =  (15  -  7)  *  Y//  Selling  profit  of  Z:  Profit_Z  =  (20  -  9)  *  Z//  Selling  profit  of  W:  Profit_W  =  (25  -  11)  *  W//  Selling  profit  of  V:  Profit_V  =  (30  -  13)  *  V//  So,  the  objective  function  is:  Maximize  (Profit_X  +  Profit_Y  +  Profit_Z  +  Profit_W  +  Profit_V)  /  (2  *  X  +  3  *  Y  +  4  *  Z  +  5  *  W  +  6  *  V)##  Generate  Constraint-1:The  company  has  $900  available  for  material  costs  next  week.//  5  *  X  +  7  *  Y  +  9  *  Z  +  11  *  W  +  13  *  V  <=  900##  Generate  Constraint-2:The  company  wants  to  produce  at  least  10  units  of  each  widget  next  week.//  X  >=  10;  Y  >=  10;  Z  >=  10;  W  >=  10;  V  >=  10##  Generate  Constraint-3:The  company  wants  to  spend  at  most  200  hours  on  production  next  week.//  2  *  X  +  3  *  Y  +  4  *  Z  +  5  *  W  +  6  *  V  <=  200##  Generate  Constraint-4:The  company  wants  to  ensure  that  the  total  production  of  Widget  W  does  not  exceed  the  combined  production  of  Widgets  X,  Y,  and  Z.//  W  <=  X  +  Y  +  Z‘‘‘‘‘‘questionA  company  produces  five  types  of  widgets:  X,  Y,  Z,  W,  and  V.  The  company  needs  to  determine  how  many  units  of  each  widget  to  produce  in  next  week.  The  selling  price,  material  cost,  and  production  time  for  each  widget  are  given  in  the  following  Table.|  Widget  |  Selling  Price  |  Material  Cost  |  Production  Time  ||--------|---------------|---------------|-----------------||  X  |  10$  |  5$  |  2  hours  ||  Y  |  15$  |  7$  |  3  hours  ||  Z  |  20$  |  9$  |  4  hours  ||  W  |  25$  |  11$  |  5  hours  ||  V  |  30$  |  13$  |  6  hours  |The  company  has  $900  available  for  material  costs  next  week.  The  company  wants  to  produce  at  least  10  units  of  each  widget  next  week.  The  company  wants  to  spend  at  most  200  hours  on  production  next  week.  The  company  wants  to  ensure  that  the  total  production  of  Widget  W  does  not  exceed  the  combined  production  of  Widgets  X,  Y,  and  Z.  The  company  has  only  one  production  line  and  can  only  produce  one  widget  at  a  time.Please  help  the  company  to  maximize  the  rate  at  which  it  earns  profits  (which  is  defined  as  the  sum  of  the  selling  profit  divided  by  the  sum  of  the  production  times).‘‘‘[Follow  the  Example  to  Convert  the  following  Scenario  to  a  Question  with  table]:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[任务描述]: 你将获得一个涉及优化问题的场景。场景被组织成几个以“##”开头的部分。每个部分包含几行文本，描述场景。场景的数学形式解决方案在以“//”开头的注释中提供。你的任务是将场景转换成一个问题，不遗漏任何信息。问题应该清晰简洁，并且不要暴露场景的数学形式解决方案。[将场景转换为问题的示例与表格]:
    ‘‘‘场景## 定义变量: 一家公司生产五种类型的小工具：X、Y、Z、W和V。公司需要确定下周每种小工具的生产数量。//  {"X的小工具数量":  "X",  "范围":  "X
    >= 0",  "类型":  "整数"}//  {"Y的小工具数量":  "Y",  "范围":  "Y >= 0",  "类型":  "整数"}//  {"Z的小工具数量":  "Z",  "范围":  "Z
    >= 0",  "类型":  "整数"}//  {"W的小工具数量":  "W",  "范围":  "W >= 0",  "类型":  "整数"}//  {"V的小工具数量":  "V",  "范围":  "V
    >= 0",  "类型":  "整数"}## 定义目标函数: 对于小工具X，售价为10美元，材料成本为5美元，生产时间为2小时。对于小工具Y，售价为15美元，材料成本为7美元，生产时间为3小时。对于小工具Z，售价为20美元，材料成本为9美元，生产时间为4小时。对于小工具W，售价为25美元，材料成本为11美元，生产时间为5小时。对于小工具V，售价为30美元，材料成本为13美元，生产时间为6小时。公司只有一条生产线，只能一次生产一个小工具。公司旨在最大化其盈利率（定义为销售利润之和除以生产时间之和）。//  X的销售利润:  Profit_X
    =  (10 - 5) * X//  Y的销售利润:  Profit_Y =  (15 - 7) * Y//  Z的销售利润:  Profit_Z =  (20
    - 9) * Z//  W的销售利润:  Profit_W =  (25 - 11) * W//  V的销售利润:  Profit_V =  (30 - 13)
    * V//  因此，目标函数是: 最大化 (Profit_X + Profit_Y + Profit_Z + Profit_W + Profit_V) /
    (2 * X + 3 * Y + 4 * Z + 5 * W + 6 * V)## 生成约束条件-1: 公司下周的材料成本预算为900美元。//  5 *
    X + 7 * Y + 9 * Z + 11 * W + 13 * V <= 900## 生成约束条件-2: 公司希望下周每种小工具至少生产10个单位。//  X
    >= 10;  Y >= 10;  Z >= 10;  W >= 10;  V >= 10## 生成约束条件-3: 公司希望下周生产总时间不超过200小时。//  2
    * X + 3 * Y + 4 * Z + 5 * W + 6 * V <= 200## 生成约束条件-4: 公司希望确保小工具W的总生产量不超过小工具X、Y和Z的总生产量。//  W
    <= X + Y + Z‘‘‘‘问题一家公司生产五种类型的小工具：X、Y、Z、W和V。公司需要确定下周每种小工具的生产数量。每种小工具的售价、材料成本和生产时间如下表所示。|  小工具  |  售价  |  材料成本  |  生产时间  ||--------|-------|-------|---------||  X  |  10美元  |  5美元  |  2小时  ||  Y  |  15美元  |  7美元  |  3小时  ||  Z  |  20美元  |  9美元  |  4小时  ||  W  |  25美元  |  11美元  |  5小时  ||  V  |  30美元  |  13美元  |  6小时  |公司下周的材料成本预算为900美元。公司希望下周每种小工具至少生产10个单位。公司希望下周生产总时间不超过200小时。公司希望确保小工具W的总生产量不超过小工具X、Y和Z的总生产量。公司只有一条生产线，只能一次生产一个小工具。请帮助公司最大化其盈利率（定义为销售利润之和除以生产时间之和）。'
- en: E.2.4 Code Generation
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2.4 代码生成
- en: '“system”:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: “系统”：
- en: 'You  are  a  mathematical  assistant.  Now,  you  will  be  provided  with  an  optimization  scenario  with  its  corresponding  question.  Please  follow  the  examples  to  solve  the  optimization  scenario  using  python  code  with  pyscipopt.  (Tips:  1.  Set  objective  as  a  variable  to  avoid  non-linear  objective.  2.  To  expedite  computation,  convert  division  to  multiplication.)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个数学助手。现在，你将获得一个优化场景及其相应的问题。请参考示例，使用带有 `pyscipopt` 的 Python 代码来解决优化场景。（提示：1.
    将目标设置为变量，以避免非线性目标。2. 为了加快计算速度，将除法转换为乘法。）
- en: '“user”:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: “用户”：
- en: '[Example-1]:‘‘‘scenario##  Define  Variables:Now  we  need  to  create  a  cylindrical  metal  jar  with  a  metal  shell.//  variables:  {"radius  of  the  cylindrical  jar":  "r",  "height  of  the  cylindrical  jar":  "h"},  where  r,  h  >=  0##  Define  Objective  Function:The  cost  of  the  metal  is  $10  per  square  meter.  Find  the  dimensions  that  will  minimize  the  cost  of  the  metal  to  manufacture  the  jar.//  The  surface  area  of  the  cylindrical  jar  is  the  sum  of  the  area  of  the  two  circular  ends  and  the  lateral  surface  area.  The  area  of  each  circular  end  is  \pi  *  r^2,  and  the  lateral  surface  area  is  2\pi*rh.//  So,  the  surface  area  of  the  cylindrical  jar  is  2\pi*r^2  +  2\pi*rh,  and  the  cost  of  the  metal  is  10  *  (2\pi*r^2  +  2\pi*rh).//  So,  the  objective  function  is:  Minimize  10  *  (2\pi*r^2  +  2\pi*rh)##  Generate  Constraint-1:The  volume  of  the  jar  must  be  at  least  1000  cubic  centimeters.//  \pi*r^2h  >=  1000‘‘‘‘‘‘pythonimport  mathimport  pyscipopt#  Create  a  new  modelmodel  =  pyscipopt.Model()#  Define  variables##  The  radius  and  height  of  the  cylindrical  jarr  =  model.addVar(vtype="CONTINUOUS",  name="r",  lb=0,  ub=100)  #  radius  of  the  cylindrical  jarh  =  model.addVar(vtype="CONTINUOUS",  name="h",  lb=0,  ub=100)  #  height  of  the  cylindrical  jar#  Define  objective  function##  set  objective  as  a  variable  (pyscipopt  does  not  support  non-linear  objective)obj  =  model.addVar(’obj’)model.setObjective(obj,  "minimize")##  the  objective  function  is:  Minimize  10  *  (2\pi*r^2  +  2\pi*rh)model.addCons(obj  ==  10  *  (2*math.pi*r**2  +  2*math.pi*r*h))#  Add  constraints##  The  volume  of  the  jar  must  be  at  least  1000  cubic  centimeters.model.addCons(math.pi*r**2*h  >=  1000)#  Solve  the  problemmodel.optimize()#  Print  the  optimal  solution  (value  of  the  variables  &  the  objective)print(’-’*10)if  model.getStatus()  ==  "optimal":print("Radius  of  the  cylindrical  jar:  ",  model.getVal(r))print("Height  of  the  cylindrical  jar:  ",  model.getVal(h))print("Minimized  Cost:  ",  model.getObjVal())else:print("The  problem  could  not  be  solved  to  optimality.")‘‘‘[Example-2]:‘‘‘scenario##  Define  Variables:You  are  designing  a  rectangular  poster  by  cutting  from  a  rectangular  piece  of  paper.//  variables:  {"width  of  the  poster":  "w",  "height  of  the  poster":  "h"},  where  w,  h  >=  0##  Define  Objective  Function:The  top  and  bottom  margins  are  2  inches,  and  the  side  margins  are  1  inch.  What  dimensions  of  the  poster  should  you  use  to  minimize  the  area  of  paper  used?//  The  width  of  the  used  paper  is  w  +  2*1,  and  the  height  of  the  used  paper  is  h  +  2*2.//  Therefore,  the  objective  function  is:  Minimize  (w  +  2)  *  (h  +  4)##  Generate  Constraint-1:The  poster  must  have  an  area  of  100  square  inches.//  The  area  of  the  poster  is  given  by  the  product  of  the  width  and  the  height,  and  it  is  given  that  the  area  is  100.  Therefore,  the  constraint  is  w  *  h  =  100‘‘‘‘‘‘pythonimport  mathimport  pyscipopt#  Create  a  new  modelmodel  =  pyscipopt.Model()#  Define  variables##  The  width  and  height  of  the  posterw  =  model.addVar(vtype="CONTINUOUS",  name="w",  lb=0,  ub=100)  #  width  of  the  posterh  =  model.addVar(vtype="CONTINUOUS",  name="h",  lb=0,  ub=100)  #  height  of  the  poster#  Define  objective  function##  set  objective  as  a  variable  (pyscipopt  does  not  support  non-linear  objective)obj  =  model.addVar(’obj’)model.setObjective(obj,  "minimize")##  the  objective  function  is:  Minimize  (w  +  2)  *  (h  +  4)model.addCons(obj  ==  (w  +  2)  *  (h  +  4))#  Add  constraints##  The  poster  must  have  an  area  of  100  square  inches.model.addCons(w  *  h  ==  100)#  Solve  the  problemmodel.optimize()#  Print  the  optimal  solution  (value  of  the  variables  &  the  objective)print(’-’*10)if  model.getStatus()  ==  "optimal":print("Width  of  the  poster:  ",  model.getVal(w))print("Height  of  the  poster:  ",  model.getVal(h))print("Minimized  Area  of  Paper  Used:  ",  model.getObjVal())else:print("The  problem  could  not  be  solved  to  optimality.")‘‘‘[Convert  the  following  Scenario  to  code]:‘‘‘scenario<...  Put  your  synthetic  scenario  here  ...>‘‘‘'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例-1]:‘‘‘场景##  定义变量: 现在我们需要创建一个带有金属外壳的圆柱形金属罐。// 变量:  {"圆柱形罐的半径":  "r",  "圆柱形罐的高度":  "h"},  其中
    r,  h  >=  0##  定义目标函数:  金属的成本是每平方米 $10。  找到能使制造罐子金属成本最小的尺寸。//  圆柱形罐的表面积是两个圆形底面的面积加上侧面积。  每个圆形底面的面积是
    \pi * r^2， 侧面积是 2\pi*rh。//  所以，圆柱形罐的表面积是 2\pi*r^2 + 2\pi*rh，  金属的成本是 10 * (2\pi*r^2
    + 2\pi*rh)。//  所以，目标函数是:  最小化 10 * (2\pi*r^2 + 2\pi*rh)##  生成约束条件-1:  罐子的体积必须至少为
    1000 立方厘米。//  \pi*r^2h >= 1000‘‘‘‘‘pythonimport  mathimport  pyscipopt#  创建一个新的模型model  =  pyscipopt.Model()#  定义变量##  圆柱形罐的半径和高度r  =  model.addVar(vtype="CONTINUOUS",  name="r",  lb=0,  ub=100)  #  圆柱形罐的半径h  =  model.addVar(vtype="CONTINUOUS",  name="h",  lb=0,  ub=100)  #  圆柱形罐的高度#  定义目标函数##  将目标设置为一个变量  (pyscipopt  不支持非线性目标)obj  =  model.addVar(’obj’)model.setObjective(obj,  "minimize")##  目标函数是:  最小化
    10 * (2\pi*r^2 + 2\pi*rh)model.addCons(obj  ==  10  *  (2*math.pi*r**2  +  2*math.pi*r*h))#  添加约束##  罐子的体积必须至少为
    1000 立方厘米.model.addCons(math.pi*r**2*h  >=  1000)#  求解问题model.optimize()#  打印最优解  (变量的值和目标)print(’-’*10)if  model.getStatus()  ==  "optimal":print("圆柱形罐的半径:  ",  model.getVal(r))print("圆柱形罐的高度:  ",  model.getVal(h))print("最小化成本:  ",  model.getObjVal())else:print("问题未能解决至最优。")‘‘‘[示例-2]:‘‘‘场景##  定义变量:
    你正在设计一个矩形海报，通过从一张矩形纸上裁剪得到。// 变量:  {"海报的宽度":  "w",  "海报的高度":  "h"},  其中 w,  h  >=  0##  定义目标函数:  上下边距是
    2 英寸，左右边距是 1 英寸。  你应该使用什么尺寸的海报以最小化使用的纸张面积？//  使用的纸张的宽度是 w + 2*1，高度是 h + 2*2。//  因此，目标函数是:  最小化
    (w + 2) * (h + 4)##  生成约束条件-1:  海报的面积必须为 100 平方英寸。//  海报的面积由宽度和高度的乘积给出，且已知面积为
    100。因此，约束条件是 w * h = 100‘‘‘‘‘pythonimport  mathimport  pyscipopt#  创建一个新的模型model  =  pyscipopt.Model()#  定义变量##  海报的宽度和高度w  =  model.addVar(vtype="CONTINUOUS",  name="w",  lb=0,  ub=100)  #  海报的宽度h  =  model.addVar(vtype="CONTINUOUS",  name="h",  lb=0,  ub=100)  #  海报的高度#  定义目标函数##  将目标设置为一个变量  (pyscipopt  不支持非线性目标)obj  =  model.addVar(’obj’)model.setObjective(obj,  "minimize")##  目标函数是:  最小化
    (w + 2) * (h + 4)model.addCons(obj  ==  (w  +  2)  *  (h  +  4))#  添加约束##  海报的面积必须为
    100 平方英寸.model.addCons(w  *  h  ==  100)#  求解问题model.optimize()#  打印最优解  (变量的值和目标)print(’-’*10)if  model.getStatus()  ==  "optimal":print("海报的宽度:  ",  model.getVal(w))print("海报的高度:  ",  model.getVal(h))print("最小化使用的纸张面积:  ",  model.getObjVal())else:print("问题未能解决至最优。")‘‘‘[将以下场景转换为代码]:‘‘‘场景<...  将你的合成场景放在这里  ...>‘‘‘'
