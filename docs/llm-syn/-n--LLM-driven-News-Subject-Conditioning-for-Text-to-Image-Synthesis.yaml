- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '\n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '\n: LLM驱动的新闻主题条件文本到图像合成'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10141](https://ar5iv.labs.arxiv.org/html/2404.10141)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.10141](https://ar5iv.labs.arxiv.org/html/2404.10141)
- en: Aashish Anantha Ramakrishnan, Sharon X. Huang & Dongwon Lee
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Aashish Anantha Ramakrishnan, Sharon X. Huang & Dongwon Lee
- en: College of Information Sciences and Technology
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 信息科学与技术学院
- en: The Pennsylvania State University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚州立大学
- en: University Park, PA
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: University Park, PA
- en: '{aza6352,suh972,dul13}@psu.edu'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{aza6352,suh972,dul13}@psu.edu'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Although Text-to-Image (T2I) Synthesis has made tremendous strides in enhancing
    synthesized image quality, current datasets evaluate model performance only on
    descriptive, instruction-based prompts. However, real-world news image captions
    take a more pragmatic approach, providing high-level situational and Named-Entity
    (NE) information along with limited physical object descriptions, making them
    abstractive in nature. To better evaluate the ability of T2I models to capture
    the intended subjects from news captions, we introduce the *Abstractive News Captions
    with High-level cOntext Representation* (\n) dataset, containing 70K+ samples
    sourced from 5 different news media organizations. With Large Language Models
    (LLM) achieving success in language and commonsense reasoning tasks, we explore
    the ability of different LLMs to identify and understand key subjects from abstractive
    captions. Our proposed method *Subject-Aware Fine-tuning* (SAFE), selects and
    enhances the representation of key subjects in synthesized images by leveraging
    LLM-generated subject weights. It also adapts to the domain distribution of news
    images and captions through Domain Fine-tuning, outperforming current T2I baselines
    on \n. By launching the \n dataset, we hope to motivate research in furthering
    the Natural Language Understanding (NLU) capabilities of T2I models. Dataset and
    evaluation code are available at [https://github.com/aashish2000/ANCHOR](https://github.com/aashish2000/ANCHOR).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文本到图像（T2I）合成在提升合成图像质量方面取得了巨大进展，但当前的数据集仅在描述性、基于指令的提示上评估模型性能。然而，现实世界中的新闻图像标题则采取更为务实的方法，提供高层次的情境和命名实体（NE）信息，同时对物理对象的描述有限，因此本质上是抽象的。为了更好地评估T2I模型从新闻标题中捕捉目标主题的能力，我们引入了*抽象新闻标题与高级上下文表示*
    (\n) 数据集，包含来自5个不同新闻媒体组织的70K+样本。随着大型语言模型（LLM）在语言和常识推理任务中的成功，我们探索了不同LLM从抽象标题中识别和理解关键主题的能力。我们提出的方法*主题感知微调*（SAFE），通过利用LLM生成的主题权重选择并增强合成图像中关键主题的表示。它还通过领域微调适应新闻图像和标题的领域分布，优于当前的T2I基线方法。通过发布\n数据集，我们希望激励进一步研究以提升T2I模型的自然语言理解（NLU）能力。数据集和评估代码可以在[https://github.com/aashish2000/ANCHOR](https://github.com/aashish2000/ANCHOR)获取。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'The capabilities of Generative AI in recent times have grown exponentially.
    In the field of Computer Vision, the launch of Large Vision Models (LVM) (Wang
    et al., [2023](#bib.bib41)), pre-trained on large-scale datasets has created a
    paradigm shift in terms of how we approach model architecture development. This
    has led to significant improvements in the visual fidelity of generated samples.
    One critical assumption that these LVM-powered T2I generators make is the style
    of caption used as input. Descriptive captions are most commonly used as they
    provide a simple and direct way to explain an image’s contents (Sharma et al.,
    [2018](#bib.bib33)), (Chen et al., [2015](#bib.bib3)). News media is one key domain
    where image captions follow sentence structures, differing from descriptive captions.
    Online news articles follow a common format: A headline followed by the article
    body, along with visual elements such as images or videos. These visual mediums
    help readers assimilate certain concepts discussed in the article. A good news
    image caption must not state obvious observations from the image, rather inform
    readers about the context behind the photo and support the topics/ideas discussed
    in the article (Federico, [2016](#bib.bib8)). Here we define context as the ”who,
    what, when, where and why” information conveyed while the image contents refer
    to the actual objects present in the image, their relative positions, the physical
    actions they perform, etc. This structure helps these captions to be more informative
    for readers while being easier to read. Since news image captions include high-level
    context information that doesn’t directly describe physical attributes of different
    image elements, we term them to have an Abstractive style of representation. The
    importance of understanding how the meaning of a sentence varies based on its
    structure is a key challenge in Natural Language Understanding (NLU).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来生成式 AI 的能力呈指数增长。在计算机视觉领域，大规模预训练的视觉大模型（LVM）（Wang 等，[2023](#bib.bib41)）的推出在模型架构开发方式上带来了范式转变。这导致了生成样本的视觉保真度显著提高。一个关键的假设是这些
    LVM 驱动的 T2I 生成器对输入的标题样式有要求。描述性标题最为常用，因为它们提供了简单直接的方式来解释图像内容（Sharma 等，[2018](#bib.bib33)），（Chen
    等，[2015](#bib.bib3)）。新闻媒体是一个关键领域，其中图像标题遵循句子结构，与描述性标题有所不同。在线新闻文章遵循常见格式：标题后跟正文，以及图像或视频等视觉元素。这些视觉媒介帮助读者理解文章中讨论的某些概念。好的新闻图像标题不应陈述图像中的显而易见的观察结果，而应向读者提供照片背后的背景信息，并支持文章中讨论的主题/观点（Federico，[2016](#bib.bib8)）。这里我们将背景定义为“谁、什么、何时、何地和为什么”的信息，而图像内容指的是图像中实际存在的物体、它们的相对位置、它们执行的物理动作等。这种结构使这些标题对读者更具信息量且更易读。由于新闻图像标题包括不直接描述不同图像元素物理属性的高级背景信息，我们将其称为抽象风格的表现形式。理解句子的意义如何根据其结构变化是自然语言理解（NLU）的一个关键挑战。
- en: '![Refer to caption](img/1b765a7ac2eca63ab0ec15fe188f20a7.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1b765a7ac2eca63ab0ec15fe188f20a7.png)'
- en: 'Figure 1: Example of descriptive captions from the COCO Captions dataset (Chen
    et al., [2015](#bib.bib3)) (Left) and abstractive captions from the \n (Right).
    Words highlighted in Blue directly translate to visual entities while words highlighted
    in Red influence the image indirectly, making them abstractive.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：来自 COCO Captions 数据集的描述性标题示例（Chen 等， [2015](#bib.bib3)）（左）和来自 \n 的抽象标题（右）。蓝色高亮的词直接翻译为视觉实体，而红色高亮的词间接影响图像，使其具有抽象性。
- en: Linguistic theories such as Pragmatics (Grice, [1975](#bib.bib11)) and Discourse
    Coherence (Alikhani et al., [2019](#bib.bib1)) help us define the role of context
    in image captions. The perceived meaning of an image may vary from the literal
    descriptions of image contents, even if they are semantically similar (Nie et al.,
    [2020](#bib.bib22)), (Vedantam et al., [2017](#bib.bib39)). Since the assumption
    that abstractive captions offer literal descriptions of image contents doesn’t
    hold true, the delineation between image content and context information is critical
    to the synthesis process. News image-caption pairs routinely contain multiple
    subjects and associated context information, which poses key challenges for caption
    comprehension. One significant challenge is estimating the relative importance
    of different subjects. As humans, we can rank the importance of each subject mentioned
    in a sentence and appropriately expect them to be represented in the generated
    image. Current Text-Image encoders such as CLIP struggle with such Visio-Linguistic
    Reasoning tasks (Thrush et al., [2022](#bib.bib35)), leading to catastrophic subject
    forgetting. We explore the use of LLMs as External Knowledge sources through salient
    subject extraction.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学理论，如语用学（Grice，[1975](#bib.bib11)）和话语连贯性（Alikhani 等，[2019](#bib.bib1)），帮助我们定义图像字幕中上下文的作用。图像的感知意义可能会与图像内容的字面描述有所不同，即使它们在语义上相似（Nie
    等，[2020](#bib.bib22)），（Vedantam 等，[2017](#bib.bib39)）。由于假设抽象字幕提供图像内容的字面描述并不成立，因此在合成过程中，图像内容与上下文信息之间的划分至关重要。新闻图像-字幕对通常包含多个主题和相关上下文信息，这对字幕理解提出了关键挑战。其中一个重要挑战是估计不同主题的相对重要性。作为人类，我们可以对句子中提到的每个主题进行排序，并合理地期望它们在生成的图像中得到体现。目前的文本-图像编码器，如CLIP，在这类视觉-语言推理任务中表现不佳（Thrush
    等，[2022](#bib.bib35)），导致严重的主题遗忘。我们探索通过显著主题提取将大型语言模型（LLMs）作为外部知识源的使用。
- en: 'To tackle the above-mentioned challenges, we propose a new dataset containing
    Abstractive News Captions with High-level cOntext Representation (\n), to help
    enhance the caption comprehension capabilities of T2I models. Popular open-source
    image-caption datasets such as COCO Captions (Chen et al., [2015](#bib.bib3))
    and Conceptual Captions (Sharma et al., [2018](#bib.bib33)) primarily contain
    descriptive captions with limited text subjects. The ANCHOR dataset is designed
    to be representative of real-world image captions included in news articles. Articles
    from top English News Media publishers are used for extracting relevant image-caption
    pairs. Compared to descriptive captions, news captions differ significantly as
    they contain variable sentence structures and a higher presence of Named-Entities
    (NE). To benchmark T2I models on both these aspects, we create 2 main subsets
    within our dataset: ANCHOR Non-Entity and ANCHOR Entity. The Non-entity subset
    consists 70K+ samples containing generic image concepts with the primary goal
    of evaluating sentence understanding. The Entity subset consists of image-caption
    pairs sorted per NE category. With PERSON entities being the most commonly observed
    in news media, we select the top 48 frequently mentioned entities from this class
    for constructing the subset. We also propose Subject-Aware FinE-tuning (SAFE)
    as a viable framework to improve subject understanding through explicit conditioning.
    We summarize our contributions as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述挑战，我们提出了一个新数据集，其中包含具有高级上下文表示的抽象新闻字幕（\n），以帮助增强T2I模型的字幕理解能力。流行的开源图像-字幕数据集，如COCO
    Captions（Chen 等，[2015](#bib.bib3)）和Conceptual Captions（Sharma 等，[2018](#bib.bib33)），主要包含描述性字幕，并且文本主题有限。ANCHOR数据集旨在代表实际新闻文章中的图像字幕。我们使用来自顶级英语新闻媒体出版商的文章来提取相关的图像-字幕对。与描述性字幕相比，新闻字幕有显著不同，因为它们包含多样的句子结构和更高频的命名实体（NE）。为了在这两个方面对T2I模型进行基准测试，我们在数据集中创建了两个主要子集：ANCHOR
    Non-Entity 和 ANCHOR Entity。Non-entity子集包含70K+样本，包含通用图像概念，主要用于评估句子理解。Entity子集由按命名实体类别排序的图像-字幕对组成。由于PERSON实体在新闻媒体中最常见，我们从这一类别中选择了48个最常被提及的实体来构建子集。我们还提出了基于主题感知的微调（SAFE）作为一种通过显式条件化提高主题理解的可行框架。我们总结了我们的贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce \n, the first large-scale, abstractive image-caption dataset for
    Text-to-Image synthesis focusing on both diverse concepts and Named Entities
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了\n，这是第一个大规模的、抽象的图像-字幕数据集，专注于多样化概念和命名实体的文本到图像合成。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present SAFE as a framework for improving subject representation using external
    knowledge
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了SAFE作为一个利用外部知识提高主题表示的框架。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform extensive experiments on captions from \n, demonstrating the effectiveness
    of SAFE in improving image-caption alignment
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对\n中的标题进行了广泛的实验，展示了SAFE在改善图像-标题对齐方面的有效性。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Text-to-Image Synthesis
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本到图像合成
- en: There have been significant improvements in the field of T2I generation since
    the launch of Generative Adversarial Networks (GAN) (Goodfellow et al., [2014](#bib.bib10)).
    Initial GAN-based T2I generators focused on learning dataset-specific feature
    correlations between images and text using Attention Networks (Zhang et al., [2017](#bib.bib48)),
    (Xu et al., [2018](#bib.bib46)), (Zhu et al., [2019](#bib.bib51)). As Transformer-based
    architectures (Vaswani et al., [2017](#bib.bib38)) and model pre-training was
    successful on Natural Language Processing tasks, multi-modal encoders such as
    CLIP (Radford et al., [2021](#bib.bib24)) significantly improved the of quality
    multi-modal embeddings and provided better input conditioning (Crowson et al.,
    [2022](#bib.bib4)), (Zhou et al., [2022](#bib.bib50)). Diffusion models (Sohl-Dickstein
    et al., [2015](#bib.bib34)) provided a breakthrough in training higher resolution
    models with greater expressivity by modeling generation as a reverse-Markov chain
    process (Nichol et al., [2022](#bib.bib21)), (Ramesh et al., [2021](#bib.bib25)),
    (Ding et al., [2021](#bib.bib7)). With the success of language model-based text-only
    encoders, Large Vision models adopt Large Language Model (LLM) based encoders
    for T2I generation, leveraging their language comprehension capabilities (Saharia
    et al., [2022](#bib.bib30)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自从生成对抗网络（GAN）（Goodfellow等，[2014](#bib.bib10)）推出以来，T2I生成领域取得了重大进展。最初基于GAN的T2I生成器专注于使用注意力网络（Zhang等，[2017](#bib.bib48)），（Xu等，[2018](#bib.bib46)），（Zhu等，[2019](#bib.bib51)）学习数据集特定的图像与文本特征相关性。随着基于Transformer的架构（Vaswani等，[2017](#bib.bib38)）和模型预训练在自然语言处理任务上的成功，多模态编码器如CLIP（Radford等，[2021](#bib.bib24)）显著提高了多模态嵌入的质量，并提供了更好的输入条件（Crowson等，[2022](#bib.bib4)），（Zhou等，[2022](#bib.bib50)）。扩散模型（Sohl-Dickstein等，[2015](#bib.bib34)）通过将生成建模为反向马尔可夫链过程（Nichol等，[2022](#bib.bib21)），（Ramesh等，[2021](#bib.bib25)），（Ding等，[2021](#bib.bib7)）在训练高分辨率模型方面取得了突破。随着基于语言模型的文本编码器的成功，大型视觉模型采用基于大型语言模型（LLM）的编码器进行T2I生成，利用其语言理解能力（Saharia等，[2022](#bib.bib30)）。
- en: Latent Diffusion Models
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 潜在扩散模型
- en: Latent Diffusion Models (LDM) differs from other Diffusion Probabilistic Models
    by splitting the training process into 2 separate phases. The first phase uses
    an auto-encoder to compress the latent space of the diffusion model into a perceptually
    equivalent lower dimensional representation space, which reduces overall complexity.
    This is achieved by training using a patch-based adversarial objective and perceptual
    loss. The second phase improves the conditioning mechanism of Diffusion models
    by augmenting the UNet with Cross Attention Layers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型（LDM）与其他扩散概率模型的不同之处在于将训练过程分为两个独立的阶段。第一个阶段使用自编码器将扩散模型的潜在空间压缩为感知等效的低维表示空间，从而降低整体复杂性。这是通过使用基于补丁的对抗目标和感知损失进行训练来实现的。第二个阶段通过用交叉注意力层增强UNet来改进扩散模型的条件机制。
- en: Datasets
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: With T2I models being pre-trained on larger and larger corpora, there has been
    a shift towards evaluation-only benchmarks with prompts to judge specific attributes
    of a generator’s performance. PartiPrompts (Saharia et al., [2022](#bib.bib30))
    and UniBench (Li et al., [2022](#bib.bib17)) provide diverse text prompts sorted
    based on style and difficulty. DiffusionDB (Wang et al., [2022b](#bib.bib42))
    is a large-scale collection of prompt-tuned caption-image pairs commonly used
    for sourcing captions for T2I evaluation. All these benchmarks contain captions
    that only provide sparse or detailed descriptions of physical entities within
    images. We aim to include captions containing situational context information
    and complex sentence structures as a part of \n.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着T2I模型在越来越大的语料库上进行预训练，评估性基准测试逐渐转向通过提示来判断生成器性能的特定属性。PartiPrompts（Saharia等，[2022](#bib.bib30)）和UniBench（Li等，[2022](#bib.bib17)）提供了根据风格和难度排序的多样化文本提示。DiffusionDB（Wang等，[2022b](#bib.bib42)）是一个大规模的提示调优的标题-图像对集合，通常用于为T2I评估提供标题。这些基准测试中的标题仅提供了对图像中物理实体的稀疏或详细描述。我们旨在将包含情境背景信息和复杂句子结构的标题作为\n的一部分。
- en: '3 \n: Dataset Overview'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3 \n: 数据集概述'
- en: 'The \n (Abstractive News Captions with High-level cOntext Representation) dataset
    is a large-scale image-caption pair dataset extracted from news articles. To construct
    this dataset, we use open-source news image captioning datasets: VisualNews (Liu
    et al., [2021](#bib.bib20)) and NYTimes800K (Tran et al., [2020](#bib.bib36)).
    To effectively test caption comprehension of T2I models, we need to isolate the
    impact of caption structures from other factors that influence the synthesized
    image quality. Named-Entity features such as faces of specific people pose a significant
    challenge to the generators (Rombach et al., [2022](#bib.bib28)), (Ramesh et al.,
    [2022](#bib.bib26)). This is likely due to the complexity of learning NE features
    compared to more generic visual concepts during the pre-training phase. To assess
    if artifacts generated by these models are due to poor subject understanding or
    entity features, we split our data into 2 distinct subsets: \n Non-Entity and
    \n Entity. From a combined 1.8M image-caption pairs, we remove 95% of low-quality
    image-caption pairs as a part of our pre-processing steps. We provide extensive
    dataset statistics and sample quality evaluation results in the supplementary
    section.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \n（具有高级上下文表示的抽象新闻标题）数据集是一个大规模的图像-标题对数据集，提取自新闻文章。为了构建这个数据集，我们使用了开源新闻图像标题数据集：VisualNews（Liu
    et al., [2021](#bib.bib20)）和 NYTimes800K（Tran et al., [2020](#bib.bib36)）。为了有效测试
    T2I 模型的标题理解能力，我们需要将标题结构的影响从其他影响合成图像质量的因素中隔离出来。特定人物的面部等命名实体特征对生成器（Rombach et al.,
    [2022](#bib.bib28)），（Ramesh et al., [2022](#bib.bib26)）构成了显著挑战。这可能是由于与更通用的视觉概念相比，学习
    NE 特征的复杂性较高。在评估这些模型生成的伪影是否由于对主题理解不佳或实体特征不清晰时，我们将数据分为两个不同的子集：\n 非实体和 \n 实体。在结合的
    180 万图像-标题对中，我们在预处理步骤中移除了 95% 的低质量图像-标题对。我们在附录部分提供了详细的数据集统计和样本质量评估结果。
- en: Image-based Filtering
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于图像的过滤
- en: We standardize the resolution of all images to 512x512\. By using Entropy-based
    cropping, we retain focus on points of interest within a frame. This helps keep
    the foreground object at the center of the image and limits information loss to
    only the background elements. To remove noisy and blurry images, we use CLIP-IQA
    (Wang et al., [2022a](#bib.bib40)) as a reference-free metric. To filter images
    based on noisiness and sharpness, we use a minimum threshold of 0.3.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有图像的分辨率标准化为 512x512。通过使用基于熵的裁剪方法，我们保留了图像中感兴趣点的焦点。这有助于保持前景对象在图像的中心，并将信息损失限制在背景元素上。为了去除噪声和模糊的图像，我们使用
    CLIP-IQA（Wang et al., [2022a](#bib.bib40)）作为无参考指标。为了根据噪声和清晰度过滤图像，我们使用了 0.3 的最低阈值。
- en: Caption Filtering and Entity Tagging
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标题过滤和实体标注
- en: In the first stage of filtering, we remove very short captions. We select captions
    with a minimum length of 6 words and above for our dataset. This is done to ensure
    that selected captions are informative enough for T2I synthesis. We use different
    approaches for tagging NEs based on the dataset the captions were extracted from.
    For captions extracted from the NYTimes800K news corpus, we use the provided NER
    annotations for filtering. The VisualNews corpus does not provide ground-truth
    annotations, so we identify mentions of names-entities using the Spacy library.
    We remove samples containing [’PERSON’, ’GPE’, ’LOC’, ’WORK_OF_ART’, ’ORG’] entity
    types due to their high presence in captions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤的第一阶段，我们移除非常短的标题。我们选择最少长度为 6 个词及以上的标题以用于数据集。这是为了确保所选标题对于 T2I 合成足够具有信息量。根据标题提取的数据集，我们使用不同的方法进行
    NE 标注。对于从 NYTimes800K 新闻语料库提取的标题，我们使用提供的 NER 注释进行过滤。VisualNews 语料库没有提供真实标签，因此我们使用
    Spacy 库来识别名称-实体的提及。我们移除了包含 [’PERSON’，’GPE’，’LOC’，’WORK_OF_ART’，’ORG’] 实体类型的样本，因为它们在标题中的出现频率较高。
- en: '![Refer to caption](img/053fc18c8471c167388433a2dd472f20.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/053fc18c8471c167388433a2dd472f20.png)'
- en: 'Figure 2: Overview of our dataset’s pre-processing and filtering steps'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们数据集的预处理和过滤步骤概述
- en: 3.1 \n Non-Entity Subset
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 \n 非实体子集
- en: This subset contains 72692 image-caption pairs selected from articles published
    by 5 different news media organizations. The train/validation/test split of the
    dataset is in the ratio 90%/5%/5% respectively. In this subset, we aim to understand
    how T2I models comprehend abstractive news caption structures. Since NEs are a
    confounding variable in the case of news captions, we remove samples with any
    NE mentions in this subset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个子集包含来自5个不同新闻媒体机构发布的72692个图像-标题对。数据集的训练/验证/测试拆分比例为90%/5%/5%。在这个子集中，我们旨在了解T2I模型如何理解抽象新闻标题结构。由于NE在新闻标题中的干扰变量，我们在这个子集中移除了任何NE提及的样本。
- en: Non-entity Sample Selection
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非实体样本选择
- en: Following caption-based filtering, we also ensure that the associated images
    do not contain representations of NEs which generators may struggle to replicate.
    Since T2I models struggle to generate specific faces of humans, we target the
    removal of all image-caption pairs with visible faces. Using a RetinaFace-based
    face detector, we flag and remove images containing identifiable faces.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于标题的过滤之后，我们还确保相关图像中不包含生成器可能难以复制的NE表示。由于T2I模型在生成特定的人脸时遇到困难，我们的目标是移除所有具有可见面孔的图像-标题对。使用基于RetinaFace的面部检测器，我们标记并移除包含可识别面孔的图像。
- en: 3.2 \n Entity Subset
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 \n 实体子集
- en: With NEs being a critical component of news image-caption pairs, this subset
    has been designed to evaluate the capabilities of T2I models to generate NEs with
    the provided context. This subset contains 7516 image-caption pairs with 48 different
    NEs. Each NE has a minimum of 43 image-caption pairs. The current version of our
    entity subset primarily includes PERSON entities. This is due to their frequency
    of mentions in news media, and consistency of physical features across images.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于NE是新闻图像-标题对中的一个关键组件，这个子集旨在评估T2I模型在提供的上下文中生成NE的能力。这个子集包含7516个图像-标题对，涉及48个不同的NE。每个NE至少有43个图像-标题对。我们当前版本的实体子集主要包括PERSON实体。这是由于它们在新闻媒体中提及的频率以及图像中身体特征的一致性。
- en: Multi-Modal NE Grounding
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多模态NE基础
- en: 'The challenge with NE mention detection is that entities can be referred to
    by different names according to the situation. Example: David Beckham can be referred
    to as: ”Beckham”, ”David”, ”David Robert Joseph Beckham”, etc. To avoid this ambiguity,
    we need to reliably link each mention to a real-world entity. We perform Multi-modal
    NE grounding to link each entity mention using Wikipedia as a real-world knowledge
    source. Using the REL Entity Linker (van Hulst et al., [2020](#bib.bib37)), we
    extract entity mentions from the previously selected samples and link them to
    their appropriate Wikipedia pages. We used a Wikipedia dump from 2019-07 as our
    knowledge source. Although this helped in removing erroneous mentions detected
    from text captions, we also need to ensure each image contains the mentioned entity.
    Using their linked Wikipedia pages, we download the main image and create a repository
    of reference images for each entity in our dataset. Since we are focusing on PERSON
    entities, we perform a sanity check to ensure that a face is detected in each
    of the downloaded reference images. To ground each image to an entity category,
    face recognition is performed using FaceNet (Schroff et al., [2015](#bib.bib31)).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NE提及检测的挑战在于实体可能根据情况以不同的名称出现。例如：大卫·贝克汉姆可能被称为：“贝克汉姆”、“大卫”、“大卫·罗伯特·约瑟夫·贝克汉姆”等。为了避免这种模糊性，我们需要可靠地将每个提及链接到现实世界中的实体。我们执行多模态NE基础，以使用维基百科作为现实世界知识源来链接每个实体提及。使用REL实体链接器（van
    Hulst等，[2020](#bib.bib37)），我们从先前选择的样本中提取实体提及，并将它们链接到其相应的维基百科页面。我们使用了2019-07的维基百科数据作为知识源。虽然这有助于去除从文本标题中检测到的错误提及，但我们还需要确保每张图片包含提到的实体。通过它们的维基百科页面，我们下载主要图像并为我们数据集中的每个实体创建参考图像库。由于我们专注于PERSON实体，我们执行了合理性检查，以确保在每张下载的参考图像中检测到面孔。为了将每张图片定位到实体类别，我们使用FaceNet（Schroff等，[2015](#bib.bib31)）进行面部识别。
- en: Face-Aware Cropping
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 面部感知裁剪
- en: The non-centered nature of foreground objects in news images poses a challenge
    for consistent image evaluation. Many photographs are taken as long shots with
    the entity’s face in different sections of the image. To standardize these images,
    we crop and resize the images taking into account the target entity position.
    By extracting the bounding boxes of our target entity face, we calculate its centroid
    as a reference coordinate for cropping. We then take a fixed window crop of the
    entity image such that the entity centroid is aligned closely to the center of
    the crop. This approach of Face-aware cropping helps maximize the image area occupied
    by an entity and further isolates its physical features. Through this cropping
    process, we can focus each entity category evaluation on the features of our target
    entity alone.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻图像中前景物体的非中心化特性对一致的图像评估构成了挑战。许多照片是长镜头拍摄的，实体的面孔位于图像的不同部分。为了标准化这些图像，我们根据目标实体位置裁剪和调整图像大小。通过提取目标实体面孔的边界框，我们计算其质心作为裁剪的参考坐标。然后，我们对实体图像进行固定窗口裁剪，使实体质心尽可能接近裁剪的中心。这种面部感知裁剪的方法有助于最大化实体所占的图像区域，并进一步隔离其物理特征。通过这一裁剪过程，我们可以将每个实体类别的评估集中在目标实体的特征上。
- en: 4 Improving Conditioning with External Knowledge
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用外部知识改进条件化
- en: Conditioning is the technique through which user inputs are incorporated into
    the generation process. For the task of T2I generation, we utilize conditioning
    to conform the generated image to certain criteria provided as a text input. Text
    conditioning in T2I models is accomplished through embeddings extracted by text
    encoders such as CLIP Radford et al. ([2021](#bib.bib24)). Encoders typically
    employ self-attention to analyze and assign importance scores to individual elements
    of the input sequence while retaining the global context information across the
    entire sequence. These encoders are trained on massively large image-caption pair
    datasets such as LAION Schuhmann et al. ([2022](#bib.bib32)) and scraped from
    various web sources. The objective of generic image captions is to explain the
    various components of an image in a statement-like format. This simplifies the
    task of word importance estimation during the generation process with every word
    contributing directly towards a visual concept. Let $S_{desc}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 条件化是将用户输入纳入生成过程的技术。对于 T2I 生成任务，我们利用条件化将生成图像调整为满足作为文本输入提供的特定标准。T2I 模型中的文本条件化是通过由文本编码器（如
    CLIP Radford et al. ([2021](#bib.bib24))）提取的嵌入来完成的。编码器通常使用自注意力来分析并为输入序列的各个元素分配重要性分数，同时保留整个序列的全局上下文信息。这些编码器是在大规模图像-标题对数据集（如
    LAION Schuhmann et al. ([2022](#bib.bib32))）上训练的，并从各种网络来源抓取。通用图像标题的目标是以声明式格式解释图像的各种组件。这简化了生成过程中文本重要性估计的任务，每个单词直接贡献于视觉概念。让
    $S_{desc}$。
- en: '![Refer to caption](img/a937ee2f5afb2c0f6a01e6edb416b2e2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a937ee2f5afb2c0f6a01e6edb416b2e2.png)'
- en: 'Figure 3: Overview of our Subject-Aware FinE-tuning Approach (SAFE)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们主题感知微调方法（SAFE）的概述
- en: '|  | $S_{desc}=\{T_{1},T_{2},\dots,T_{i},\dots,T_{m}\}$ |  | (1) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{desc}=\{T_{1},T_{2},\dots,T_{i},\dots,T_{m}\}$ |  | (1) |'
- en: Every subject is expected to either define or describe the properties of a visual
    concept present in the generated image. When generating $E_{desc}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主题都应定义或描述生成图像中存在的视觉概念的属性。在生成 $E_{desc}$ 时。
- en: '|  | $1$2 |  | (2) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: In the case of abstractive captions $S_{abstr}$. This scale multiplier helps
    align embeddings toward the intended meaning of a particular caption by acting
    as a prompt weight.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在抽象标题的情况下 $S_{abstr}$。这个尺度乘数通过作为提示权重来帮助对齐嵌入，以便朝着特定标题的预期含义对齐。
- en: '|  | $$W_{abstr}=\begin{cases}\text{$\beta$,}&amp;\quad\text{if $T_{i}$}\in
    T_{key}\\ \text{1,}&amp;\quad\text{otherwise.}\\'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$W_{abstr}=\begin{cases}\text{$\beta$,}&amp;\quad\text{如果 $T_{i}$}\in
    T_{key}\\ \text{1,}&amp;\quad\text{否则。}\\'
- en: \end{cases}$$ |  | (3) |
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}$$ |  | (3) |
- en: '|  | $E_{abstr}=TextEnc(S_{abstr})\cdot W_{abstr}$ |  | (4) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{abstr}=TextEnc(S_{abstr})\cdot W_{abstr}$ |  | (4) |'
- en: 4.1 Subject-Aware fine-tuning (SAFE)
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主题感知微调（SAFE）
- en: 'As illustrated in Figure [3](#S4.F3 "Figure 3 ‣ 4 Improving Conditioning with
    External Knowledge ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image
    Synthesis"), our method utilizes a Stable Diffusion-based backbone for news image
    generation. By taking advantage of prompt weighting and fine-tuning strategies,
    our approach aims to enhance both the image fidelity and prompt following capabilities
    over baseline models.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[3](#S4.F3 "Figure 3 ‣ 4 Improving Conditioning with External Knowledge ‣
    \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis")所示，我们的方法利用基于稳定扩散的骨干网络进行新闻图像生成。通过利用提示加权和微调策略，我们的方法旨在提高图像的保真度和对提示的遵循能力，相比于基线模型。'
- en: LLMs for Subject Conditioning
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs用于主题条件化
- en: The key challenge in implementing subject conditioning is identifying which
    subjects/phrases to weigh positively. To replace human guidance in the process
    of prompt weighting, we evaluate the use of LLMs in identifying salient subjects
    from sentences. Leveraging the commonsense reasoning abilities of LLMs, we utilize
    instruction-based prompting to extract salient subjects from each sentence. Subject
    identification is done in a zero-shot manner using only the prompt and the pre-trained
    world knowledge of LLMs. This process allows us to explicitly condition the input
    embeddings in an observable and explainable manner. Compared to other LLM-based
    grounding strategies such as (Lian et al., [2023](#bib.bib18)), (Feng et al.,
    [2023](#bib.bib9)) subject conditioning requires only single-stage prompting and
    also utilizes fewer tokens per generation. Additionally, we also compare different
    LLM architectures including both Commercial (GPT-4, GPT-3.5) and Open-source (Mixtral
    7x8B, Orca-13B) models on this task. This comparative study helps us ascertain
    the types of models capable of producing high-quality subject weights.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 实施主题条件化的关键挑战在于识别哪些主题/短语需要正向加权。为了替代人类在提示加权过程中的指导，我们评估了利用LLMs识别句子中的显著主题。通过利用LLMs的常识推理能力，我们使用基于指令的提示从每个句子中提取显著主题。主题识别采用零样本方式，仅使用提示和LLMs的预训练世界知识。这一过程使我们能够以可观察和可解释的方式显式地对输入嵌入进行条件化。与其他基于LLMs的基础策略相比，如（Lian
    et al., [2023](#bib.bib18)），（Feng et al., [2023](#bib.bib9)），主题条件化只需单阶段提示，并且每次生成使用的令牌也更少。此外，我们还比较了包括商业（GPT-4，GPT-3.5）和开源（Mixtral
    7x8B，Orca-13B）模型在内的不同LLM架构。这项比较研究帮助我们确定了能够产生高质量主题权重的模型类型。
- en: Handling Domain-Shift
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 处理领域转移
- en: News images and captions have specific characteristics that differ from the
    general outputs generated by T2I models. Specifically, real-life photographs with
    specific foreground and background objects are present in abundance compared to
    artistic or animated-style images. To tackle this domain shift, we develop our
    Domain fine-tuning (DFE) strategy on \n. Traditional Mean Squared Error-based
    losses used for Stable Diffusion fine-tuning tend to generate unrealistic images
    (Zhang et al., [2018](#bib.bib49)), (Lin & Yang, [2023](#bib.bib19)), making them
    unsuitable for our task. We adopt the Reward Feedback Learning (ReFL) Xu et al.
    ([2023](#bib.bib45)) strategy for directly optimizing Stable Diffusion on a reward
    model trained to score image-caption alignment. The selected reward model ImageReward
    Xu et al. ([2023](#bib.bib45)) has been trained on 137K human-annotated samples
    to predict alignment between image-caption pairs. Our proposed improvements in
    DFE over vanilla ReFL focus on improving both alignments with the ground truth
    image and caption instead of only caption alignment. In DFE, we initialize the
    latent vector of Stable Diffusion based on the ground truth images for each caption
    instead of random initialization as implemented in ReFL. This helps control the
    diffusion process in generating target distribution-aligned images. To increase
    training stability, we opt to finetune only the Attention Layers by inserting
    rank-decomposition matrices for selective weight updates Hu et al. ([2021](#bib.bib14)).
    Additionally, the noise scheduler of the original ReFL pipeline was limited to
    having 40 timesteps. The authors identified that latents sampled between 30-39
    timesteps produced distinguishable ImageReward scores. We extend this insight
    by setting the timesteps of our noise scheduler to 100 and sampling from timesteps
    40-99 for loss calculation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻图片和说明具有特定的特征，这些特征与 T2I 模型生成的一般输出不同。具体而言，现实生活中的照片中包含了大量特定的前景和背景对象，相较于艺术风格或动画风格的图片更多。为应对这一领域的变化，我们在
    \n 上开发了我们的领域微调（DFE）策略。用于稳定扩散微调的传统均方误差（MSE）损失往往生成不切实际的图像（Zhang 等，[2018](#bib.bib49)），（Lin
    & Yang，[2023](#bib.bib19)），因此不适用于我们的任务。我们采用了奖励反馈学习（ReFL）Xu 等（[2023](#bib.bib45)）策略，直接在经过训练的奖励模型上优化稳定扩散，以评分图像-说明对齐。所选的奖励模型
    ImageReward Xu 等（[2023](#bib.bib45)）已在 137K 人工标注样本上训练，以预测图像-说明对的对齐。我们提出的 DFE 改进相比于原始
    ReFL 更侧重于提高与真实图像和说明的对齐，而不仅仅是说明的对齐。在 DFE 中，我们基于每个说明的真实图像初始化稳定扩散的潜在向量，而不是像 ReFL
    中那样的随机初始化。这有助于控制生成目标分布对齐图像的扩散过程。为了提高训练稳定性，我们选择仅微调注意力层，通过插入秩分解矩阵来选择性地更新权重 Hu 等（[2021](#bib.bib14)）。此外，原始
    ReFL 流水线的噪声调度器的时间步长限制为 40。作者发现，在 30-39 个时间步之间采样的潜在变量生成了可区分的 ImageReward 分数。我们扩展了这一见解，将噪声调度器的时间步长设置为
    100，并在时间步 40-99 之间采样以计算损失。 |
- en: '| Original | SD 2.1 (Base) | SD 2.1 (CR) | SAFE (DFE + GPT-3.5) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | SD 2.1 (基础) | SD 2.1 (CR) | SAFE (DFE + GPT-3.5) |'
- en: '| ![Refer to caption](img/a95afdefdea18f2e6e99ff59e4f0e820.png) | ![Refer to
    caption](img/794734b605d3e52846082bb08afb1c11.png) | ![Refer to caption](img/a28392fd07b29b0a3c55a57e06602935.png)
    | ![Refer to caption](img/19a75f12bcbd57e4c10fb031d183b468.png) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/a95afdefdea18f2e6e99ff59e4f0e820.png) | ![参见说明](img/794734b605d3e52846082bb08afb1c11.png)
    | ![参见说明](img/a28392fd07b29b0a3c55a57e06602935.png) | ![参见说明](img/19a75f12bcbd57e4c10fb031d183b468.png)
    |'
- en: '| The school offers clothing, including shoes, to its students. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 学校为学生提供包括鞋子在内的服装。 |'
- en: '| ![Refer to caption](img/2649439bb1ed654c7fd9cf0d1a811af1.png) | ![Refer to
    caption](img/178e9de9a50a7f67c9ccb3d1a7c9285c.png) | ![Refer to caption](img/27700bae46e54e3c7b651bb6ac66d16a.png)
    | ![Refer to caption](img/9fc175ee058c49d09c56b163f0d047e5.png) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/2649439bb1ed654c7fd9cf0d1a811af1.png) | ![参见说明](img/178e9de9a50a7f67c9ccb3d1a7c9285c.png)
    | ![参见说明](img/27700bae46e54e3c7b651bb6ac66d16a.png) | ![参见说明](img/9fc175ee058c49d09c56b163f0d047e5.png)
    |'
- en: '| A bronze tiger shows assertiveness and a winning spirit. The books are all
    from colleagues. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 一只青铜虎展现了坚定的姿态和获胜的精神。这些书籍都是来自同事的。 |'
- en: '| ![Refer to caption](img/1ba3d6ffe385a20e2f85685bc2cc2144.png) | ![Refer to
    caption](img/c83fcb74c5c9d0457904b96872018262.png) | ![Refer to caption](img/698295ad11d078b139154f27f9227880.png)
    | ![Refer to caption](img/1d929f85dfae2195a43bc916b42c790f.png) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/1ba3d6ffe385a20e2f85685bc2cc2144.png) | ![参见说明](img/c83fcb74c5c9d0457904b96872018262.png)
    | ![参见说明](img/698295ad11d078b139154f27f9227880.png) | ![参见说明](img/1d929f85dfae2195a43bc916b42c790f.png)
    |'
- en: '| The Galaxy Note 5 can be used with a case that doubles as a physical Qwerty
    keyboard to aid typing. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Galaxy Note 5 可以使用一个既能作为物理 Qwerty 键盘又能帮助打字的保护壳。 |'
- en: 'Figure 4: Qualitative comparison of different T2I models on \n Non-Entity Subset.
    Words highlighted in Orange are used for subject conditioning'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不同T2I模型在非实体子集上的定性比较。用橙色突出显示的词语用于主题条件设置
- en: 5 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: Evaluation Metrics
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评价指标
- en: 'To holistically evaluate the samples generated by T2I models on \n Non-Entity,
    we report 3 different types of metrics: Frechet Inception Distance (FID) (Heusel
    et al., [2017](#bib.bib13)) and ImageReward (Xu et al., [2023](#bib.bib45)) and
    Human Preference Score V2 (Wu et al., [2023](#bib.bib43)). Frechet Inception Distance
    serves as an indicator to quantify the overall realism and diversity of generated
    samples compared to the ground truth images. With the distribution of datasets
    like \n diverging significantly from the Inception-V3 used in traditional FID
    calculations (Kynkäänniemi et al., [2022](#bib.bib16)), we adopt the more representative
    $FID_{CLIP}$ metric for our testing. To measure the relatedness of our generated
    images and ground truth captions, we utilize ImageReward. Compared to image-caption
    similarity metrics like CLIPScore (Hessel et al., [2021](#bib.bib12)), ImageReward
    is trained on real-world image-caption pairs annotated and ranked according to
    human preference. Similarly, Human Preference Score V2 also serves as an indicator
    of human preference alignment.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面评估T2I模型在非实体数据上生成的样本，我们报告3种不同类型的指标：Frechet Inception Distance (FID)（Heusel等，[2017](#bib.bib13)）和ImageReward（Xu等，[2023](#bib.bib45)）以及Human
    Preference Score V2（Wu等，[2023](#bib.bib43)）。Frechet Inception Distance作为一个指标，用于量化生成样本相对于真实图像的整体现实性和多样性。由于数据集的分布与传统FID计算中使用的Inception-V3（Kynkäänniemi等，[2022](#bib.bib16)）有显著差异，我们采用了更具代表性的$FID_{CLIP}$指标进行测试。为了衡量我们生成的图像与真实字幕的相关性，我们利用了ImageReward。与图像-字幕相似度指标如CLIPScore（Hessel等，[2021](#bib.bib12)）相比，ImageReward是基于根据人类偏好注释和排名的真实世界图像-字幕对进行训练的。同样，Human
    Preference Score V2也作为人类偏好一致性的指标。
- en: '| Model | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
- en: '| SAFE (DFE + GPT-3.5) | 7.2804 | 0.0664 | 0.2393 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE + GPT-3.5) | 7.2804 | 0.0664 | 0.2393 |'
- en: '| Stable Diffusion 2.1 (CR) | 10.6595 | -0.3388 | 0.2201 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Stable Diffusion 2.1 (CR) | 10.6595 | -0.3388 | 0.2201 |'
- en: '| Stable Diffusion 2.1 (Base) | 7.4780 | 0.02510 | 0.2385 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Stable Diffusion 2.1 (基础版) | 7.4780 | 0.02510 | 0.2385 |'
- en: '| Stable Diffusion 1.5 (Base) | 7.4742 | -0.0925 | 0.2288 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Stable Diffusion 1.5 (基础版) | 7.4742 | -0.0925 | 0.2288 |'
- en: 'Table 1: Results of Abstractive Text-to-Image synthesis on \n Non-Entity Subset'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在非实体子集上抽象文本到图像合成的结果
- en: Baseline Models
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准模型
- en: 'We select SOTA T2I models: Stable Diffusion V1.5 and V2.1 (Rombach et al.,
    [2022](#bib.bib28)) as baselines for the task of news image generation. These
    baseline candidates have shown strong performance in traditional Text-to-Image
    generation benchmarks such as COCO Captions (Chen et al., [2015](#bib.bib3)).
    Additionally, we also compare the performance of LLM-powered Caption Rewriting
    (CR) for translating abstractive captions into descriptive text. Specifically,
    we use an LLM to rewrite a caption into an instruction prompt of the format ”Generate
    an image …”. We feed the modified prompt as input to our T2I model.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了SOTA T2I模型：Stable Diffusion V1.5和V2.1（Rombach等，[2022](#bib.bib28)）作为新闻图像生成任务的基准。这些基准模型在传统的文本到图像生成基准测试（如COCO
    Captions（Chen等，[2015](#bib.bib3)））中表现强劲。此外，我们还比较了LLM驱动的字幕重写（CR）在将抽象字幕翻译为描述性文本方面的表现。具体来说，我们使用LLM将字幕重写为格式为“生成一张图片……”的指令提示。我们将修改后的提示作为输入提供给我们的T2I模型。
- en: Implementation Details
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节
- en: 'We set $guidance\_scale=7.5$ for all LLM extracted keywords in the original
    caption. Generated samples of Baseline and SAFE models using the same seed are
    presented in Figure [4](#S4.F4 "Figure 4 ‣ Handling Domain-Shift ‣ 4.1 Subject-Aware
    fine-tuning (SAFE) ‣ 4 Improving Conditioning with External Knowledge ‣ \n: LLM-driven
    News Subject Conditioning for Text-to-Image Synthesis"). We select GPT-3.5 as
    our default LLM model for collecting subject weights for all our fine-tuned models.
    On the \n Entity test-set, we assess the impact subject conditioning has in understanding
    abstractive captions containing NEs. We include our experimental results on the
    entity set as a part of our supplementary material.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '我们为原始字幕中的所有LLM提取的关键词设置了$guidance\_scale=7.5$。使用相同种子的基线和SAFE模型生成的样本展示在图[4](#S4.F4
    "Figure 4 ‣ Handling Domain-Shift ‣ 4.1 Subject-Aware fine-tuning (SAFE) ‣ 4 Improving
    Conditioning with External Knowledge ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis")中。我们选择GPT-3.5作为我们所有微调模型的默认LLM模型，用于收集主题权重。在实体测试集中，我们评估主题条件对理解包含NE的抽象字幕的影响。我们将实体集的实验结果作为补充材料的一部分。'
- en: 5.1 Result Evaluation
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 结果评估
- en: Quantitative Results
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定量结果
- en: 'On analysis of the scores presented in Table [1](#S5.T1 "Table 1 ‣ Evaluation
    Metrics ‣ 5 Experiments ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image
    Synthesis"), the SAFE models outperform both baseline Diffusion models on the
    \n Non-entity test set. The explicit guidance through both finetuning and subject
    conditioning contributes towards capturing the intended meaning of captions and
    also producing more news media-like images. Caption-Rewriting on the other hand
    performs significantly worse on all our benchmark metrics, signaling that the
    generations are not well aligned with abstractive prompts.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '对表[1](#S5.T1 "Table 1 ‣ Evaluation Metrics ‣ 5 Experiments ‣ \n: LLM-driven
    News Subject Conditioning for Text-to-Image Synthesis")中呈现的评分进行分析后发现，SAFE模型在非实体测试集上优于基线扩散模型。通过微调和主题条件的明确指导有助于捕捉字幕的预期含义，并生成更类似新闻媒体的图像。另一方面，Caption-Rewriting在所有基准指标上表现显著较差，表明生成的内容与抽象提示不够一致。'
- en: Qualitative Results
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定性结果
- en: 'From the examples presented in Figure [4](#S4.F4 "Figure 4 ‣ Handling Domain-Shift
    ‣ 4.1 Subject-Aware fine-tuning (SAFE) ‣ 4 Improving Conditioning with External
    Knowledge ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis"),
    we observe that SAFE models are capable of accurate prompt following compared
    to the baselines. In the first example, we see that the subjects: ”clothing”,
    ”shoes” and ”students” guide the SAFE model’s generated outputs to contain adequate
    representation. This translates to the visible children’s shoes and clothes in
    the generated sample. On the other hand, SD 2.1 (Base) fails to capture the presence
    of shoes in its result. Another key observation is that although SD 2.1 (CR) produces
    a well-grounded image, it mistakes the style of the image to be a drawing/sketch.
    Even without implied or stated mentions of the image style, caption re-writing
    tends to guide the model’s generations towards unrealistic samples. Similarly
    in the second example, we see that SD 2.1 (Base) forgets the presence of books
    in the caption which is well showcased by other models. These examples demonstrate
    the need for explicit subject conditioning over implicit methods such as caption-rewriting.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '从图[4](#S4.F4 "Figure 4 ‣ Handling Domain-Shift ‣ 4.1 Subject-Aware fine-tuning
    (SAFE) ‣ 4 Improving Conditioning with External Knowledge ‣ \n: LLM-driven News
    Subject Conditioning for Text-to-Image Synthesis")中呈现的示例可以观察到，与基线相比，SAFE模型能够准确跟随提示。在第一个示例中，我们看到“衣物”、“鞋子”和“学生”等主题指导SAFE模型生成的输出包含了足够的表现。这在生成的样本中表现为明显的儿童鞋子和衣物。另一方面，SD
    2.1（基线）未能捕捉到结果中的鞋子存在。另一个关键观察是，虽然SD 2.1（CR）生成了一幅扎实的图像，但它错误地将图像的风格误认为是绘画/素描。即使没有隐含或明确提到图像风格，字幕重写往往会将模型的生成引导到不现实的样本中。类似地，在第二个示例中，我们看到SD
    2.1（基线）忘记了字幕中提到的书籍，而其他模型则很好地展示了这一点。这些示例表明，相对于像字幕重写这样的隐性方法，明确的主题条件更为必要。'
- en: '| Model | Total Samples | Preferred Samples | Preference Score (%) (↑) |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 总样本数 | 优选样本数 | 偏好得分（%）（↑） |  |'
- en: '| SAFE (DFE + GPT-3.5) | 185 | 102 | 55.13 |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE + GPT-3.5) | 185 | 102 | 55.13 |  |'
- en: '| Stable Diffusion 2.1 | 185 | 83 | 44.86 |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 稳定扩散2.1 | 185 | 83 | 44.86 |  |'
- en: 'Table 2: Human Evaluation Results on \n'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：人工评估结果
- en: Human Evaluation
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工评估
- en: We perform human evaluation of SAFE models vs baseline Stable Diffusion on Amazon
    MTurk to understand perceived variations in subject understanding. From the \n
    Non-Entity test set, we randomly sample and filter 300 captions for our survey.
    The filtration procedure includes the removal of images where both models fail
    to represent the caption’s subjects adequately. We also remove images that contain
    Text Images (i.e., images containing generated text) as this is a documented problem
    for Stable Diffusion models (Rombach et al., [2022](#bib.bib28)). The questions
    in our survey require participants to pick the image that is most related to the
    provided caption and also rate the difficulty of choosing between the two images
    on a 5-point scale. The scale ranges from 1 - ”Very easy to distinguish” to 5
    - ”Very difficult to distinguish”. This measure is utilized to understand the
    rater’s confidence in assessing the image-caption pairs. We removed all samples
    rated as ”Very difficult to distinguish” from our analysis to ensure the quality
    of human ratings. Our analysis shows that raters consistently preferred images
    generated by SAFE over the baseline model, complementing our quantitative results.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Amazon MTurk上对SAFE模型与基线Stable Diffusion进行人工评估，以了解对主题理解的感知差异。从非实体测试集（Non-Entity
    test set）中，我们随机抽取并筛选了300个标题用于调查。筛选程序包括移除那些两个模型都未能充分表示标题主题的图像。我们还移除包含文本图像的图像（即包含生成文本的图像），因为这是Stable
    Diffusion模型的一个已知问题（Rombach et al., [2022](#bib.bib28)）。我们的调查问卷要求参与者选择与提供的标题最相关的图像，并在5分制的量表上评估选择这两张图像的难易程度。量表范围从1
    - “非常容易区分”到5 - “非常难以区分”。此措施用于了解评估者在评估图像-标题对时的信心。我们从分析中移除了所有被评为“非常难以区分”的样本，以确保人工评分的质量。我们的分析显示，评估者一致更喜欢SAFE生成的图像，相对于基线模型，这与我们的定量结果相辅相成。
- en: '| Model | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
- en: '| SAFE (DFE + GPT 3.5) | 7.2804 | 0.0664 | 0.2393 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE + GPT 3.5) | 7.2804 | 0.0664 | 0.2393 |'
- en: '| SAFE (DFE) | 7.4851 | 0.0249 | 0.2385 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE) | 7.4851 | 0.0249 | 0.2385 |'
- en: '| SAFE (GPT-3.5) | 7.2614 | 0.0624 | 0.2392 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (GPT-3.5) | 7.2614 | 0.0624 | 0.2392 |'
- en: '| SAFE (GPT-4) | 7.2482 | 0.0673 | 0.2391 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (GPT-4) | 7.2482 | 0.0673 | 0.2391 |'
- en: '| SAFE (Mixtral 7x8B) | 7.2649 | 0.0723 | 0.2394 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (Mixtral 7x8B) | 7.2649 | 0.0723 | 0.2394 |'
- en: '| SAFE (Orca-13B) | 7.3571 | 0.0298 | 0.2381 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (Orca-13B) | 7.3571 | 0.0298 | 0.2381 |'
- en: 'Table 3: Ablation study evaluating the effectiveness of different model components'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 消融研究评估不同模型组件的有效性'
- en: 5.2 Ablation Study
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 消融研究
- en: Impact of Subject Conditioning
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主题条件影响
- en: 'In this section, we investigate the impact of each of SAFE’s components on
    generation quality as shown in Table [3](#S5.T3 "Table 3 ‣ Human Evaluation ‣
    5.1 Result Evaluation ‣ 5 Experiments ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis"). We observe that Subject Conditioning provides a
    significant contribution towards the observed metric performance. The addition
    of DFE boosts the image-caption understanding without majorly impacting image
    fidelity, as reflected in all 3 metrics presented. The positive correlation between
    ImageReward and HPS V2 Scores even with the addition of DFE confirms that the
    finetuning process hasn’t overfit on the reward model’s predictions as well.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们探讨了每个SAFE组件对生成质量的影响，如表格[3](#S5.T3 "表 3 ‣ 人工评估 ‣ 5.1 结果评估 ‣ 5 实验 ‣ \n:
    基于LLM的新闻主题条件下的文本到图像合成")所示。我们观察到，主题条件对观察到的度量性能有显著贡献。DFE的添加提高了图像-标题理解，而未对图像保真度产生重大影响，如所呈现的所有3个度量所反映的那样。即使添加了DFE，ImageReward和HPS
    V2分数之间的正相关性也确认了微调过程没有过度拟合奖励模型的预测。'
- en: Subject Weight Quality Across LLM Models
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主题权重在不同LLM模型中的质量
- en: 'To assess the variation in commonsense reasoning and world knowledge of different
    LLM architectures, we collect subject weights from 4 different LLMs: GPT-3.5,
    GPT-4, Orca-13B and Mixtral 7x8B Mixture of Experts (MoE). For our ablation study,
    we replace only the provided subject weights from each model during inference
    using SD 2.1 (Base) as our T2I backbone. We can observe a clear correlation between
    LLM performance on other commonsense reasoning tasks and key subject delineation
    with Mixtral and GPT-4 outperforming other models. Subject weights generated by
    models lower number of parameters like Orca-13B still show improvements on 2 of
    the 3 metrics compared to our baselines. This demonstrates the potential for Open-source
    LLMs to be useful for boosting caption understanding in cross-modal generative
    tasks such as T2I.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估不同 LLM 架构在常识推理和世界知识上的变化，我们从 4 个不同的 LLM 中收集了主题权重：GPT-3.5、GPT-4、Orca-13B 和
    Mixtral 7x8B 专家混合模型（MoE）。在我们的消融研究中，我们仅在推理过程中使用 SD 2.1（基础版）作为我们的 T2I 基础，通过替换每个模型提供的主题权重。我们可以观察到
    LLM 在其他常识推理任务上的表现与 Mixtral 和 GPT-4 的关键主题划分之间存在明显的相关性，后者表现优于其他模型。尽管像 Orca-13B 这样参数较少的模型生成的主题权重仍在
    3 个指标中的 2 个上显示出相对基线的改进，但这展示了开源 LLM 在提升跨模态生成任务（如 T2I）中的标题理解潜力。
- en: 6 Conclusion and Limitation
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与限制
- en: We have presented \n, a novel news image-caption pair dataset for evaluating
    T2I generation performance on abstrative captions. We test popular baseline models
    on our dataset and analyze their performance concerning both image fidelity and
    image-caption alignment. We also propose SAFE for augmenting existing Diffusion-based
    generators to provide explicit subject weighting on abstractive captions. News
    domain-specific datasets \n motivate the development of journalism assistance
    tools. Our dataset can also help analyze the biases of T2I models since abstractive
    captions take advantage of the concept associations learned during their pre-training
    phase, rather than descriptively providing information on all physical attributes
    of an image.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 \n，一种用于评估抽象标题 T2I 生成性能的新型新闻图像-标题对数据集。我们在我们的数据集上测试了流行的基线模型，并分析了它们在图像真实性和图像-标题对齐方面的表现。我们还提出了
    SAFE，用于增强现有的基于扩散的生成器，以在抽象标题上提供明确的主题加权。新闻领域特定的数据集 \n 激励了新闻辅助工具的发展。我们的数据集还可以帮助分析
    T2I 模型的偏见，因为抽象标题利用了在预训练阶段学习到的概念关联，而不是描述性地提供图像的所有物理属性。
- en: Limitation
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制
- en: Since we build on top of open-source Large Foundational Models such as Stable
    Diffusion, GPT-3.5, GPT-4, Mixtral-7x8B and Orca, our approach inherits all their
    biases. The lack of task-specific finetuning to improve entity likeness generation
    is another limitation that our approach faces. Future research directions include
    evaluation of concept finetuning (Kumari et al., [2022](#bib.bib15)), (Ruiz et al.,
    [2023](#bib.bib29)) on abstractive captions and developing entity concept datasets
    based on \n.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们基于开源大型基础模型，如 Stable Diffusion、GPT-3.5、GPT-4、Mixtral-7x8B 和 Orca，我们的方法继承了所有这些模型的偏见。缺乏特定任务的微调以改进实体相似度生成是我们方法面临的另一个限制。未来的研究方向包括评估概念微调（Kumari
    等，[2022](#bib.bib15)），（Ruiz 等，[2023](#bib.bib29)）在抽象标题上的应用，并开发基于 \n 的实体概念数据集。
- en: 7 Acknowledgements
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: 'Special thanks to Aadarsh Anantha Ramakrishnan for his contributions through
    insightful research discussions and proofreading of the draft. This research has
    been partially supported by NSF Awards #1820609 and #2114824.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '特别感谢 Aadarsh Anantha Ramakrishnan 通过富有洞察力的研究讨论和草稿校对作出的贡献。本研究部分得到了 NSF 奖项 #1820609
    和 #2114824 的支持。'
- en: References
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Alikhani et al. (2019) Malihe Alikhani, Sreyasi Nag Chowdhury, Gerard de Melo,
    and Matthew Stone. CITE: A corpus of image-text discourse relations. *arXiv [cs.CL]*,
    April 2019.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alikhani 等（2019）玛丽赫·阿利赫尼、斯雷亚西·纳格·乔杜里、杰拉德·德·梅洛和马修·斯通。《CITE: 图像-文本话语关系语料库》。*arXiv
    [cs.CL]*，2019年4月。'
- en: Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, and Tim Brooks. Improving
    image generation with better captions, 2023.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Betker 等（2023）詹姆斯·贝特克、加布里埃尔·戈、李晶和蒂姆·布鲁克斯。《通过更好的标题改进图像生成》，2023年。
- en: 'Chen et al. (2015) Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam,
    Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO captions:
    Data collection and evaluation server. *arXiv preprint arXiv:1504.00325*, April
    2015.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2015）辛磊·陈、郝芳、林宗义、拉马克里希纳·维丹塔姆、索拉布·古普塔、皮奥特·美元和 C·劳伦斯·齐特尼克。微软 COCO 标题：数据收集和评估服务器。*arXiv
    预印本 arXiv:1504.00325*，2015年4月。
- en: 'Crowson et al. (2022) Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell
    Stander, Eric Hallahan, Louis Castricato, and Edward Raff. VQGAN-CLIP: Open domain
    image generation and editing with natural language guidance. *arXiv:2204.08583
    [cs]*, April 2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Crowson et al. (2022) Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell
    Stander, Eric Hallahan, Louis Castricato, 和 Edward Raff. VQGAN-CLIP: 使用自然语言指导的开放领域图像生成与编辑。*arXiv:2204.08583
    [cs]*, 2022年4月。'
- en: 'Deng et al. (2019) Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
    ArcFace: Additive angular margin loss for deep face recognition. In *2019 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*. IEEE, June 2019.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2019) Jiankang Deng, Jia Guo, Niannan Xue, 和 Stefanos Zafeiriou.
    ArcFace: 深度人脸识别的加性角度边距损失。发表于 *2019 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*。IEEE, 2019年6月。'
- en: 'Deng et al. (2020) Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia,
    and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation
    in the wild. In *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*, pp.  5203–5212\. IEEE, 2020.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2020) Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia,
    和 Stefanos Zafeiriou. Retinaface: 单次拍摄的多级人脸定位。发表于 *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*，第5203–5212页。IEEE, 2020年。'
- en: 'Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou,
    Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. CogView: Mastering
    Text-to-Image generation via transformers. May 2021.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou,
    Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, 和 Jie Tang. CogView: 通过变压器掌握文本到图像生成。2021年5月。'
- en: Federico (2016) Stephanie Federico. These are NPR’s photo caption guidelines.
    *NPR*, January 2016.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Federico (2016) Stephanie Federico. NPR的照片说明指南。*NPR*，2016年1月。
- en: 'Feng et al. (2023) Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun
    Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. LayoutGPT:
    Compositional visual planning and generation with large language models. *arXiv
    [cs.CV]*, May 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng et al. (2023) Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun
    Akula, Xuehai He, Sugato Basu, Xin Eric Wang, 和 William Yang Wang. LayoutGPT:
    使用大型语言模型进行组合视觉规划和生成。*arXiv [cs.CV]*，2023年5月。'
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. In *Advances in Neural Information Processing Systems*, volume 27\.
    Curran Associates, Inc., 2014.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 生成对抗网络。发表于
    *Advances in Neural Information Processing Systems*，第27卷。Curran Associates, Inc.,
    2014年。
- en: Grice (1975) Herbert P Grice. Logic and conversation. In *Speech acts*, pp. 
    41–58\. Brill, 1975.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grice (1975) Herbert P Grice. 逻辑与对话。发表于 *Speech Acts*，第41–58页。Brill, 1975年。
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. CLIPScore: A reference-free evaluation metric for image captioning.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pp.  7514–7528, Online and Punta Cana, Dominican Republic, November
    2021\. Association for Computational Linguistics.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    和 Yejin Choi. CLIPScore: 一种无参考的图像说明评估指标。发表于 *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*，第7514–7528页，在线及多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。'
- en: Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard
    Nessler, and Sepp Hochreiter. GANs trained by a two Time-Scale update rule converge
    to a local nash equilibrium. In *Advances in Neural Information Processing Systems*,
    volume 30\. Curran Associates, Inc., 2017.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard
    Nessler, 和 Sepp Hochreiter. 通过两阶段更新规则训练的GAN收敛到局部纳什均衡。发表于 *Advances in Neural Information
    Processing Systems*，第30卷。Curran Associates, Inc., 2017年。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models, 2021.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. LoRA: 大型语言模型的低秩适配，2021年。'
- en: Kumari et al. (2022) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman,
    and Jun-Yan Zhu. Multi-Concept customization of Text-to-Image diffusion. December
    2022.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumari et al. (2022) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman,
    和 Jun-Yan Zhu. 文本到图像扩散的多概念定制。2022年12月。
- en: Kynkäänniemi et al. (2022) Tuomas Kynkäänniemi, Tero Karras, Miika Aittala,
    Timo Aila, and Jaakko Lehtinen. The role of ImageNet classes in fréchet inception
    distance. March 2022.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kynkäänniemi et al. (2022) Tuomas Kynkäänniemi, Tero Karras, Miika Aittala,
    Timo Aila, 和 Jaakko Lehtinen. ImageNet类别在Fréchet Inception距离中的作用。2022年3月。
- en: 'Li et al. (2022) Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja
    Fidler, and Antonio Torralba. BigDatasetGAN: Synthesizing ImageNet with pixel-wise
    annotations. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pp.  21330–21340, 2022.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022）Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler
    和 Antonio Torralba. BigDatasetGAN：用像素级注释合成 ImageNet。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*
    上，页码 21330–21340，2022年。
- en: 'Lian et al. (2023) Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. LLM-grounded
    diffusion: Enhancing prompt understanding of text-to-image diffusion models with
    large language models. *ArXiv*, abs/2305.13655, May 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lian 等人（2023）Long Lian, Boyi Li, Adam Yala 和 Trevor Darrell. LLM 驱动的扩散：通过大型语言模型增强文本到图像扩散模型的提示理解。*ArXiv*，abs/2305.13655，2023年5月。
- en: Lin & Yang (2023) Shanchuan Lin and Xiao Yang. Diffusion model with perceptual
    loss. *arXiv [cs.CV]*, December 2023.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin & Yang（2023）Shanchuan Lin 和 Xiao Yang. 具有感知损失的扩散模型。*arXiv [cs.CV]*，2023年12月。
- en: 'Liu et al. (2021) Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez.
    Visual news: Benchmark and challenges in news image captioning. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pp. 
    6761–6771, Online and Punta Cana, Dominican Republic, November 2021\. Association
    for Computational Linguistics.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021）Fuxiao Liu, Yinghan Wang, Tianlu Wang 和 Vicente Ordonez. 视觉新闻：新闻图像描述的基准和挑战。在
    *2021 年自然语言处理实证方法会议论文集*，页码 6761–6771，线上和多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。
- en: 'Nichol et al. (2022) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
    Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards
    photorealistic image generation and editing with Text-Guided diffusion models.
    *arXiv:2112.10741 [cs]*, March 2022.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol 等人（2022）Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam,
    Pamela Mishkin, Bob McGrew, Ilya Sutskever 和 Mark Chen. GLIDE：朝着具有文本引导扩散模型的摄影真实图像生成和编辑方向迈进。*arXiv:2112.10741
    [cs]*，2022年3月。
- en: 'Nie et al. (2020) Allen Nie, Reuben Cohn-Gordon, and Christopher Potts. Pragmatic
    Issue-Sensitive image captioning. In *Findings of the Association for Computational
    Linguistics: EMNLP 2020*, pp.  1924–1938, Online, November 2020\. Association
    for Computational Linguistics.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等人（2020）Allen Nie, Reuben Cohn-Gordon 和 Christopher Potts. 实用性问题敏感图像描述。在
    *计算语言学协会发现：EMNLP 2020* 上，页码 1924–1938，线上，2020年11月。计算语言学协会。
- en: Otani et al. (2023) Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta
    Nakashima, Esa Rahtu, Janne Heikkilä, and Shin’ichi Satoh. Toward verifiable and
    reproducible human evaluation for text-to-image generation. In *2023 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  14277–14286\.
    IEEE, June 2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Otani 等人（2023）Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta Nakashima,
    Esa Rahtu, Janne Heikkilä 和 Shin’ichi Satoh. 朝着可验证和可重复的人类评估文本到图像生成的方向前进。在 *2023
    IEEE/CVF 计算机视觉与模式识别会议（CVPR）* 上，页码 14277–14286。IEEE，2023年6月。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models
    from natural language supervision. *arXiv:2103.00020 [cs]*, February 2021.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人（2021）Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel
    Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
    Gretchen Krueger 和 Ilya Sutskever. 从自然语言监督中学习可转移的视觉模型。*arXiv:2103.00020 [cs]*，2021年2月。
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image
    generation. February 2021.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh 等人（2021）Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea
    Voss, Alec Radford, Mark Chen 和 Ilya Sutskever. 零样本文本到图像生成。2021年2月。
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. Hierarchical Text-Conditional image generation with CLIP latents.
    *arXiv:2204.06125 [cs]*, April 2022.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh 等人（2022）Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu 和 Mark
    Chen. 基于 CLIP 潜变量的分层文本条件图像生成。*arXiv:2204.06125 [cs]*，2022年4月。
- en: '(27) Nils Reimers. sentence-transformers/all-MiniLM-L6-v2 · hugging face. [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).
    Accessed: 2024-4-5.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) Nils Reimers. sentence-transformers/all-MiniLM-L6-v2 · hugging face. [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)。访问日期：2024年4月5日。
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion
    models. In *2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*. IEEE, June 2022.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, 和 Bjorn Ommer. 使用潜在扩散模型进行高分辨率图像合成。见于 *2022 IEEE/CVF计算机视觉与模式识别会议 (CVPR)*。IEEE，2022年6月。
- en: 'Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
    Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion
    models for subject-driven generation. In *2023 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*. IEEE, June 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
    Michael Rubinstein, 和 Kfir Aberman. DreamBooth: 针对主题驱动生成的文本到图像扩散模型的微调。见于 *2023
    IEEE/CVF计算机视觉与模式识别会议 (CVPR)*。IEEE，2023年6月。'
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara
    Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad
    Norouzi. Photorealistic text-to-image diffusion models with deep language understanding.
    May 2022.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S
    Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, 和
    Mohammad Norouzi. 具有深度语言理解的逼真文本到图像扩散模型。2022年5月。
- en: 'Schroff et al. (2015) Florian Schroff, Dmitry Kalenichenko, and James Philbin.
    FaceNet: A unified embedding for face recognition and clustering. In *2015 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  815–823\.
    IEEE, June 2015.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schroff et al. (2015) Florian Schroff, Dmitry Kalenichenko, 和 James Philbin.
    FaceNet: 一个统一的面部识别和聚类嵌入。见于 *2015 IEEE计算机视觉与模式识别会议 (CVPR)*，第 815–823 页。IEEE，2015年6月。'
- en: 'Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu,
    Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton
    Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine
    Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open
    large-scale dataset for training next generation image-text models. October 2022.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu,
    Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton
    Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine
    Crowson, Ludwig Schmidt, Robert Kaczmarczyk, 和 Jenia Jitsev. LAION-5B: 一个开放的大规模数据集，用于训练下一代图像-文本模型。2022年10月。'
- en: 'Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
    Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic
    image captioning. In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  2556–2565, Melbourne,
    Australia, July 2018\. Association for Computational Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, 和 Radu Soricut.
    概念化描述：一个清理过的、具有超类的图像替代文本数据集，用于自动图像描述。见于 *第56届计算语言学协会年会论文集 (第1卷：长篇论文)*，第 2556–2565
    页，澳大利亚墨尔本，2018年7月。计算语言学协会。
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *Proceedings of the 32nd International Conference on Machine Learning*, pp. 
    2256–2265\. PMLR, June 2015.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    和 Surya Ganguli. 使用非平衡热力学的深度无监督学习。见于 *第32届国际机器学习大会论文集*，第 2256–2265 页。PMLR，2015年6月。
- en: 'Thrush et al. (2022) Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh,
    Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and
    language models for visio-linguistic compositionality. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  5238–5248,
    2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thrush et al. (2022) Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh,
    Adina Williams, Douwe Kiela, 和 Candace Ross. Winoground: 探索视觉和语言模型的视语组合性。见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，第
    5238–5248 页，2022年。'
- en: 'Tran et al. (2020) Alasdair Tran, Alexander Mathews, and Lexing Xie. Transform
    and tell: Entity-aware news image captioning. In *2020 IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*, pp.  13032–13042, Seattle, WA,
    USA, 2020\. IEEE.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran et al. (2020) Alasdair Tran, Alexander Mathews, 和 Lexing Xie. 转换与讲述：面向实体的新闻图像描述。见于
    *2020 IEEE/CVF计算机视觉与模式识别会议 (CVPR)*，第 13032–13042 页，美国华盛顿州西雅图，2020年。IEEE。
- en: 'van Hulst et al. (2020) Johannes M van Hulst, Faegheh Hasibi, Koen Dercksen,
    Krisztian Balog, and Arjen P de Vries. REL: An entity linker standing on the shoulders
    of giants. In *Proceedings of the 43rd International ACM SIGIR Conference on Research
    and Development in Information Retrieval*, SIGIR ’20, pp.  2197–2200, New York,
    NY, USA, July 2020\. Association for Computing Machinery.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'van Hulst et al. (2020) Johannes M van Hulst, Faegheh Hasibi, Koen Dercksen,
    Krisztian Balog, 和 Arjen P de Vries. REL: 站在巨人肩膀上的实体链接器。收录于 *Proceedings of the
    43rd International ACM SIGIR Conference on Research and Development in Information
    Retrieval*，SIGIR ’20，第2197–2200页，美国纽约，2020年7月。Association for Computing Machinery。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł Ukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan,
    and R Garnett (eds.), *Advances in Neural Information Processing Systems*, volume 30\.
    Curran Associates, Inc., 2017.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł Ukasz Kaiser, 和 Illia Polosukhin. Attention is all
    you need. 收录于 I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan,
    和 R Garnett (编辑)，*Advances in Neural Information Processing Systems*，第30卷。Curran
    Associates, Inc.，2017年。
- en: Vedantam et al. (2017) Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi
    Parikh, and Gal Chechik. Context-Aware captions from Context-Agnostic supervision.
    In *2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 
    1070–1079\. IEEE, July 2017.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vedantam et al. (2017) Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi
    Parikh, 和 Gal Chechik. Context-Aware captions from Context-Agnostic supervision.
    收录于 *2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*，第1070–1079页。IEEE，2017年7月。
- en: Wang et al. (2022a) Jianyi Wang, Kelvin C K Chan, and Chen Change Loy. Exploring
    CLIP for assessing the look and feel of images. *arXiv [cs.CV]*, July 2022a.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) Jianyi Wang, Kelvin C K Chan, 和 Chen Change Loy. 探索 CLIP
    评估图像的外观和感觉。*arXiv [cs.CV]*，2022年7月。
- en: Wang et al. (2023) Jiaqi Wang, Zhengliang Liu, Lin Zhao, Zihao Wu, Chong Ma,
    Sigang Yu, Haixing Dai, Qiushi Yang, Yiheng Liu, Songyao Zhang, Enze Shi, Yi Pan,
    Tuo Zhang, Dajiang Zhu, Xiang Li, Xi Jiang, Bao Ge, Yixuan Yuan, Dinggang Shen,
    Tianming Liu, and Shu Zhang. Review of large vision models and visual prompt engineering.
    July 2023.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Jiaqi Wang, Zhengliang Liu, Lin Zhao, Zihao Wu, Chong Ma,
    Sigang Yu, Haixing Dai, Qiushi Yang, Yiheng Liu, Songyao Zhang, Enze Shi, Yi Pan,
    Tuo Zhang, Dajiang Zhu, Xiang Li, Xi Jiang, Bao Ge, Yixuan Yuan, Dinggang Shen,
    Tianming Liu, 和 Shu Zhang. 大型视觉模型和视觉提示工程的综述。2023年7月。
- en: 'Wang et al. (2022b) Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang,
    Benjamin Hoover, and Duen Horng Chau. DiffusionDB: A large-scale prompt gallery
    dataset for text-to-image generative models. *arXiv:2210\. 14896 [cs]*, 2022b.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022b) Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang,
    Benjamin Hoover, 和 Duen Horng Chau. DiffusionDB: 一个大规模的提示库数据集用于文本到图像生成模型。*arXiv:2210.
    14896 [cs]*，2022b。'
- en: 'Wu et al. (2023) Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu,
    Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating
    human preferences of text-to-image synthesis. *arXiv [cs.CV]*, June 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2023) Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu,
    Rui Zhao, 和 Hongsheng Li. Human preference score v2: 一个用于评估文本到图像合成的人类偏好坚实基准。*arXiv
    [cs.CV]*，2023年6月。'
- en: 'Xiao et al. (2023) Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo Durand,
    and Song Han. FastComposer: Tuning-free multi-subject image generation with localized
    attention. *arXiv [cs.CV]*, May 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo Durand,
    和 Song Han. FastComposer: 无需调优的多主题图像生成与局部注意力。*arXiv [cs.CV]*，2023年5月。'
- en: 'Xu et al. (2023) Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li,
    Ming Ding, Jie Tang, and Yuxiao Dong. ImageReward: Learning and evaluating human
    preferences for Text-to-Image generation. April 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023) Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li,
    Ming Ding, Jie Tang, 和 Yuxiao Dong. ImageReward: 学习和评估文本到图像生成的人类偏好。2023年4月。'
- en: 'Xu et al. (2018) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-Grained text to image generation
    with attentional generative adversarial networks. In *2018 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  1316–1324, Salt Lake City, UT,
    USA, 2018\. IEEE.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2018) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, 和 Xiaodong He. AttnGAN: 使用注意力生成对抗网络的细粒度文本到图像生成。收录于 *2018 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*，第1316–1324页，美国盐湖城，2018年。IEEE。'
- en: Yuan et al. (2023) Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi,
    Xintao Wang, Ying Shan, and Huicheng Zheng. Inserting anybody in diffusion models
    via celeb basis. June 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2023) Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi,
    Xintao Wang, Ying Shan, 和 Huicheng Zheng. 通过名人基础将任何人插入扩散模型中。2023年6月。
- en: Zhang et al. (2017) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,
    and Oriol Vinyals. Understanding deep learning requires rethinking generalization.
    *arXiv:1611.03530 [cs]*, February 2017.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2017） Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, 和 Oriol Vinyals.
    理解深度学习需要重新思考泛化问题。*arXiv:1611.03530 [cs]*，2017 年 2 月。
- en: Zhang et al. (2018) Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
    and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual
    metric. In *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pp.  586–595, Salt Lake City, UT, 2018\. IEEE.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2018） Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, 和 Oliver
    Wang. 深度特征作为感知度量的非凡有效性。发表于 *2018 IEEE/CVF 计算机视觉与模式识别会议*，第 586–595 页，美国盐湖城，2018
    年。IEEE。
- en: Zhou et al. (2022) Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris
    Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards Language-Free
    training for Text-to-Image generation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  17907–17917\. openaccess.thecvf.com,
    2022.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2022） Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer,
    Tong Yu, Jiuxiang Gu, Jinhui Xu, 和 Tong Sun. 面向无语言训练的文本到图像生成。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第
    17907–17917 页\. openaccess.thecvf.com, 2022 年。
- en: 'Zhu et al. (2019) Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: Dynamic
    memory generative adversarial networks for Text-To-Image synthesis. In *2019 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.  5795–5803,
    Long Beach, CA, USA, 2019\. IEEE.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '朱等（2019） Minfeng Zhu, Pingbo Pan, Wei Chen, 和 Yi Yang. DM-GAN: 动态记忆生成对抗网络用于文本到图像的合成。发表于
    *2019 IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，第 5795–5803 页，美国加州长滩，2019 年。IEEE。'
- en: Appendix A Dataset Insights
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据集洞察
- en: A.1 Caption Statistics
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 标注统计
- en: 'In this section, we provide additional statistics on the \n dataset and analyze
    the distribution of image-caption pairs. In Table [4](#A1.T4 "Table 4 ‣ A.1 Caption
    Statistics ‣ Appendix A Dataset Insights ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis"), we provide caption statistics of \n compared to
    2 popular image-caption pair datasets: COCO Captions Chen et al. ([2015](#bib.bib3))
    and Conceptual Captions 3M (CC3M) Sharma et al. ([2018](#bib.bib33)). By tokenizing
    and lemmatizing each caption without case sensitivity, we compute the number of
    unique tokens present in a dataset. We utilize the NLTK library for both tokenization
    and lemmatization. We can observe that across different data splits of \n, the
    mean caption length is significantly higher with a greater variation in caption
    length compared to other datasets. In addition to the increased caption length,
    it also contains a significant amount of unique tokens considering the number
    of samples present. This highlights the diversity of captions in \n, showing greater
    expression in describing visual concepts. Table [4](#A1.T4 "Table 4 ‣ A.1 Caption
    Statistics ‣ Appendix A Dataset Insights ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis") also enables us to verify the consistency of image-caption
    pair properties present across the train, test, and validation splits.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们提供了 \n 数据集的附加统计信息，并分析了图像-标注对的分布。在表 [4](#A1.T4 "表 4 ‣ A.1 标注统计 ‣ 附录
    A 数据集洞察 ‣ \n: 基于 LLM 的新闻主题条件文本到图像合成") 中，我们提供了 \n 的标注统计数据，并与两个流行的图像-标注对数据集进行比较：COCO
    Captions Chen 等（[2015](#bib.bib3)）和 Conceptual Captions 3M (CC3M) Sharma 等（[2018](#bib.bib33)）。通过对每个标注进行标记化和词形还原（不区分大小写），我们计算了数据集中存在的唯一标记数量。我们使用
    NLTK 库进行标记化和词形还原。我们可以观察到，在不同的数据拆分中，\n 的平均标注长度明显较高，并且标注长度的变异性也大于其他数据集。除了标注长度增加外，考虑到样本数量，它还包含了大量的唯一标记。这突出了
    \n 标注的多样性，显示了描述视觉概念的更大表达能力。表 [4](#A1.T4 "表 4 ‣ A.1 标注统计 ‣ 附录 A 数据集洞察 ‣ \n: 基于
    LLM 的新闻主题条件文本到图像合成") 还使我们能够验证图像-标注对在训练、测试和验证拆分中的一致性。'
- en: '| Dataset | Unique Tokens | Caption Length |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 唯一标记 | 标注长度 |'
- en: '| Mean | StdDev |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 标准差 |'
- en: '| COCO Captions Train | 22767 | 10.42 | 0.88 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| COCO Captions Train | 22767 | 10.42 | 0.88 |'
- en: '| COCO Captions Val | 16647 | 10.42 | 0.87 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| COCO Captions 验证集 | 16647 | 10.42 | 0.87 |'
- en: '| CC3M Train | 45896 | 10.31 | 3.30 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| CC3M 训练集 | 45896 | 10.31 | 3.30 |'
- en: '| CC3M Val | 9289 | 10.40 | 3.35 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| CC3M 验证集 | 9289 | 10.40 | 3.35 |'
- en: '| \n Non-Entity Train | 51026 | 14.84 | 5.51 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| \n 非实体训练 | 51026 | 14.84 | 5.51 |'
- en: '| \n Non-Entity Val | 10619 | 14.93 | 5.38 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| \n 非实体验证集 | 10619 | 14.93 | 5.38 |'
- en: '| \n Non-Entity Test | 10485 | 14.67 | 5.36 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| \n 非实体测试集 | 10485 | 14.67 | 5.36 |'
- en: 'Table 4: Caption Statistics of \n'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: \n 标注统计'
- en: '| Name | Size | Related | Abstractive | Descriptive |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 大小 | 相关 | 抽象 | 描述 |'
- en: '| \n Full | 300 | 291 | 260 | 31 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| \n 全部 | 300 | 291 | 260 | 31 |'
- en: '| \n Entity | 100 | 99 | 93 | 6 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| \n 实体 | 100 | 99 | 93 | 6 |'
- en: '| \n Non-Entity | 200 | 192 | 167 | 25 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| \n 非实体 | 200 | 192 | 167 | 25 |'
- en: 'Table 5: Human Evaluation of \n Dataset abstractiveness'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：\n 数据集抽象性的人工评估
- en: A.2 Dataset Quality and Diversity
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 数据集质量与多样性
- en: 'For analyzing the categories of articles from which image-caption pairs have
    been selected for our dataset, we provide a unified category list in Figure [5](#A1.F5
    "Figure 5 ‣ A.2 Dataset Quality and Diversity ‣ Appendix A Dataset Insights ‣
    \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis"). With articles
    sourced from different news agencies, each source has its own article category
    taxonomy. To create a unified taxonomy, we fix the categories provided by articles
    from NYTimes as our template. To cluster similar article categories under one
    label, we utilize the lightweight sentence transformer all-MiniLM-L6-v2 [Reimers](#bib.bib27)
    . With a minimum similarity threshold of 0.5, we cluster every sample’s default
    topic description into NYTimes’s taxonomy labels. Here, we visualize our dataset’s
    top 30 article classes, showing the diverse spread of image-caption pairs present.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '为了分析我们数据集中从中选择了图像-标题对的文章类别，我们在图 [5](#A1.F5 "图 5 ‣ A.2 数据集质量与多样性 ‣ 附录 A 数据集洞察
    ‣ \n: 基于 LLM 的新闻主题条件生成文本到图像合成") 中提供了一个统一的类别列表。由于文章来源于不同的新闻机构，每个来源都有其自己的文章类别分类法。为了创建统一的分类法，我们以
    NYTimes 提供的类别作为模板。为了将相似的文章类别归为一个标签，我们使用了轻量级的句子转换器 all-MiniLM-L6-v2 [Reimers](#bib.bib27)。在最低相似性阈值为
    0.5 的情况下，我们将每个样本的默认主题描述聚类到 NYTimes 的分类标签中。在这里，我们可视化了数据集中前 30 个文章类别，展示了图像-标题对的多样分布。'
- en: 'To further assess the overall quality of the dataset and the number of image-caption
    pairs that are abstractive, we conducted a human evaluation study on Amazon MTurk.
    We consider a random sample of 300 samples extracted from the test split of \n.
    Out of the 300 selected images, 200 belong to the non-entity subset and 100 belong
    to the entity subset. Using this extracted sample, we perform a human evaluation
    of our dataset quality. The two questions we mainly aim to answer through this
    evaluation are: (1) Are the image-caption pairs closely related to each other
    from a human perspective? and (2) Are these captions Abstractive in nature? We
    launched our survey with 150 unique participants and each participant rated 10
    samples. The survey layout is presented in Figure [6](#A2.F6 "Figure 6 ‣ Appendix
    B Qualitative Evaluation of Generated Samples ‣ \n: LLM-driven News Subject Conditioning
    for Text-to-Image Synthesis"). Per Image-caption pair, we collect 5 responses
    amounting to a total of 1500 responses. We tabulate our results in [5](#A1.T5
    "Table 5 ‣ A.1 Caption Statistics ‣ Appendix A Dataset Insights ‣ \n: LLM-driven
    News Subject Conditioning for Text-to-Image Synthesis"). We observe that 97% of
    surveyed samples are related to each other. We also see that 89.3% of the related
    captions are rated as Abstractive in nature with the remaining 10.7% being descriptive.
    This supports our hypothesis and validates that our dataset pre-processing pipeline
    produces high-quality image-caption pairs.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步评估数据集的整体质量以及图像-标题对的抽象性数量，我们在 Amazon MTurk 上进行了人工评估研究。我们考虑从测试集中随机抽取的 300
    个样本。在这 300 张选择的图像中，200 张属于非实体子集，100 张属于实体子集。利用这个抽取的样本，我们对数据集的质量进行人工评估。我们通过这项评估主要旨在回答两个问题：（1）从人类的角度来看，图像-标题对彼此之间的相关性如何？以及（2）这些标题在性质上是否具备抽象性？我们启动了一个有
    150 名独立参与者的调查，每位参与者对 10 个样本进行了评分。调查布局见图 [6](#A2.F6 "图 6 ‣ 附录 B 生成样本的定性评估 ‣ \n:
    基于 LLM 的新闻主题条件生成文本到图像合成")。每个图像-标题对，我们收集了 5 个回应，总计 1500 个回应。我们的结果在 [5](#A1.T5 "表
    5 ‣ A.1 标题统计 ‣ 附录 A 数据集洞察 ‣ \n: 基于 LLM 的新闻主题条件生成文本到图像合成") 中汇总。我们观察到 97% 的调查样本彼此相关。我们还看到
    89.3% 的相关标题被评为抽象性，剩下的 10.7% 被评为描述性。这支持了我们的假设，并验证了我们的数据集预处理管道能够生成高质量的图像-标题对。'
- en: '![Refer to caption](img/9b8e8f7f72b6bfe910076cc05628a1df.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b8e8f7f72b6bfe910076cc05628a1df.png)'
- en: 'Figure 5: \n Distribution of Article Topics for samples in \n'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：\n 数据集中样本的文章主题分布
- en: Appendix B Qualitative Evaluation of Generated Samples
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 生成样本的定性评估
- en: 'For this project, all human evaluation surveys were created on Qualtrics and
    distributed through Amazon MTurk. All our studies have been conducted with Institutional
    Review Board (IRB) approval. We do not collect any personally identifiable data
    from participants in our study. Voluntary consent is obtained from each participant
    before taking part in all studies. We provide clear instructions for each evaluation
    task presented to participants with examples and test their understanding using
    a pre-survey questionnaire. This is done to ensure data quality and improve the
    consistency of task understanding across participants. Attention Check questions
    were also incorporated to prevent low-quality submissions from being accepted.
    The demographic for participants taking part in our survey was limited to people
    above 18 years of age. We provide screenshots of our survey user interface in
    Figure [7](#A2.F7 "Figure 7 ‣ Appendix B Qualitative Evaluation of Generated Samples
    ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis").'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '对于这个项目，所有人类评估调查均在 Qualtrics 上创建并通过 Amazon MTurk 分发。我们所有的研究都经过了机构审查委员会 (IRB)
    批准。我们不从参与者那里收集任何个人身份信息。在所有研究中，我们在参与之前获得了每位参与者的自愿同意。我们为参与者提供了每项评估任务的明确指示，并通过预调查问卷测试他们的理解。这是为了确保数据质量并提高参与者之间的任务理解一致性。还加入了注意力检查问题，以防止低质量提交被接受。参与我们调查的参与者年龄限制在
    18 岁以上。我们在图 [7](#A2.F7 "图 7 ‣ 附录 B 生成样本的定性评估 ‣ \n: 基于 LLM 的新闻主题条件化用于文本到图像合成")
    中提供了我们调查用户界面的屏幕截图。'
- en: Additionally, we compute the inter-annotator agreement scores for our MTurk
    participants to assess the significance of our results. Using Krippendorff’s $\alpha$
    being lower than the average scores reported on other rating tasks, we identify
    key reasons why this may be the case. In our case, each annotator does not rate
    every question present in our evaluation samples. So, the unanswered questions
    by a survey participant are treated as missing values. The high number of missing
    values when utilizing the typical formulation of this metric is one reason for
    the lower score observed. Additionally, other studies attempting to assess inter-annotator
    agreement of T2I generators (Otani et al., [2023](#bib.bib23)) on complex text-image
    datasets such as DrawBench (Saharia et al., [2022](#bib.bib30)) have reported
    similarly low scores, indicating the difficulty of this task.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们计算了我们 MTurk 参与者的标注者间一致性分数，以评估结果的显著性。使用 Krippendorff 的 $\alpha$ 低于其他评分任务报告的平均分数，我们识别出可能导致这种情况的关键原因。在我们的案例中，每个标注者并未对我们评估样本中的每个问题进行评分。因此，调查参与者未回答的问题被视为缺失值。在使用该度量的典型公式时，高缺失值数量是观察到低分数的一个原因。此外，其他研究尝试评估
    T2I 生成器（Otani 等人，[2023](#bib.bib23)）在复杂的文本图像数据集如 DrawBench（Saharia 等人，[2022](#bib.bib30)）上的标注者间一致性，也报告了类似的低分，表明了这项任务的困难性。
- en: '![Refer to caption](img/5d6639755dd1b7a84f6c9610c9727cd9.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5d6639755dd1b7a84f6c9610c9727cd9.png)'
- en: 'Figure 6: Survey UI for Data Quality Evaluation Study'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：数据质量评估研究的调查用户界面
- en: '![Refer to caption](img/fb02392a0c07c45f87003fe8a536229a.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fb02392a0c07c45f87003fe8a536229a.png)'
- en: 'Figure 7: Survey UI for Generated Image Evaluation Study'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：生成图像评估研究的调查用户界面
- en: Appendix C Salient Subject Selection
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 突出主题选择
- en: C.1 LLM Prompting Strategies
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 LLM 提示策略
- en: For extracting subject weights, we query multiple LLM architectures through
    instruction-based prompting. To reduce memory requirements and to speed up inference,
    we initialize in mixed precision mode and set $dtype=float16$. The system prompt
    is set as You are an AI assistant that follows instructions extremely well. Help
    as much as you can. In cases where the LLM returned no salient subject phrases,
    we default to using the standard text prompt as input for T2I generation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取主题权重，我们通过基于指令的提示查询多个 LLM 架构。为了减少内存需求并加快推理速度，我们在混合精度模式下初始化，并设置 $dtype=float16$。系统提示设置为您是一个非常擅长遵循指令的
    AI 助手。尽可能多地提供帮助。如果 LLM 未返回任何突出的主题短语，我们将默认使用标准文本提示作为 T2I 生成的输入。
- en: GPT-3.5/4 & Mixtral 8x7B
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT-3.5/4 & Mixtral 8x7B
- en: 'The user prompt for generating high-quality subject weights is set as: Use
    only the information provided in the prompt for answering the question. List the
    main topic word and additional topic words from the given image caption in the
    format: {”main_topic_word”: $<$'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '生成高质量主题权重的用户提示设置为：仅使用提示中提供的信息来回答问题。列出给定图像标题中的主要主题词和附加主题词，格式为：{”main_topic_word”:
    $<$'
- en: Orca Mini 13B
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Orca Mini 13B
- en: 'We modify the user prompt for Orca Mini to directly list salient subjects without
    delineation between ”main_topic_word” and ”additional_topic_words”. The user prompt
    is set as ”User: List only the main objects from the sentence: $<$”. From our
    testing, we found that this prompt pattern struck a balance between returning
    usable subject phrases and identifying only the salient subjects.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改了 Orca Mini 的用户提示，直接列出显著的主题，而不区分 ”main_topic_word” 和 ”additional_topic_words”。用户提示设置为
    ”用户：仅从句子中列出主要对象：$<$”。通过我们的测试，我们发现这种提示模式在返回可用的主题短语和仅识别显著主题之间达到了平衡。
- en: C.2 Impact of Subject Scale Multiplier
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 主题尺度倍增器的影响
- en: 'We perform the ablation study to identify the optimal value for the subject
    scale multiplier for implementing subject conditioning of text embeddings. We
    evaluate various candidate score multipliers as shown in Table [6](#A3.T6 "Table
    6 ‣ C.2 Impact of Subject Scale Multiplier ‣ Appendix C Salient Subject Selection
    ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis"). Here,
    x1 refers to a scale factor of $1.1$, and so forth. We selected a scale multiplier
    of x2 as it scores the highest in 2 out of 3 metrics tested. Increasing the scale
    multiplier beyond x2 does not provide any meaningful improvement in generation
    performance.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行剥离研究以确定用于实现文本嵌入的主题条件的最佳主题尺度倍增器值。我们评估了各种候选得分倍增器，如表 [6](#A3.T6 "表 6 ‣ C.2
    主题尺度倍增器的影响 ‣ 附录 C 显著主题选择 ‣ \n: 基于 LLM 的新闻主题条件用于文本到图像合成") 所示。这里，x1 指的是 $1.1$ 的尺度因子，以此类推。我们选择了
    x2 作为尺度倍增器，因为它在 3 个测试指标中的 2 个中得分最高。将尺度倍增器增加到 x2 以上并没有提供任何有意义的生成性能提升。'
- en: '| Model | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | $FID_{CLIP}$ (↓) | ImageReward (↑) | HPS V2 (↑) |'
- en: '| SAFE (DFE + GPT 3.5) (x1) | 7.3729 | 0.0564 | 0.2395 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE + GPT 3.5) (x1) | 7.3729 | 0.0564 | 0.2395 |'
- en: '| SAFE (DFE + GPT 3.5) (x2) | 7.2804 | 0.0664 | 0.2393 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE + GPT 3.5) (x2) | 7.2804 | 0.0664 | 0.2393 |'
- en: '| SAFE (DFE + GPT 3.5) (x3) | 7.3049 | 0.0040 | 0.2361 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE + GPT 3.5) (x3) | 7.3049 | 0.0040 | 0.2361 |'
- en: '| SAFE (DFE + GPT 3.5) (x4) | 7.8825 | -0.1835 | 0.2255 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE + GPT 3.5) (x4) | 7.8825 | -0.1835 | 0.2255 |'
- en: 'Table 6: Ablation study evaluating the effectiveness of different scale multiplier
    values'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 剥离研究评估不同规模倍增器值的有效性'
- en: Appendix D Performance on \n Entity Set
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 实体集上的性能
- en: Generating images of NEs, especially faces of people has been a challenging
    task for Text-to-Image generators due to the inability of these models to accurately
    encode facial features during their pre-training phase (Xiao et al., [2023](#bib.bib44)).
    We evaluate the ability of SAFE models to improve the quality of generated objects
    when prompted with news captions containing NEs. To accomplish this, we create
    a test-only set consisting of 50 images for each NE class present in \n Entity
    for evaluation. We extract subject weights for each caption in the test set and
    run inference of both SD 2.1 (Base) and SAFE (DFE + GPT-3.5) finetuned on the
    Non-entity set.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 生成 NEs（尤其是人脸）的图像对于文本到图像生成器来说一直是一个具有挑战性的任务，因为这些模型在预训练阶段无法准确编码面部特征（Xiao et al.,
    [2023](#bib.bib44)）。我们评估了 SAFE 模型在处理包含 NEs 的新闻标题时提高生成对象质量的能力。为此，我们创建了一个仅用于测试的集，其中包含每个
    NE 类的 50 张图像，以供评估。我们提取了测试集中每个标题的主题权重，并运行了对 SD 2.1 (Base) 和 SAFE (DFE + GPT-3.5)
    在非实体集上微调的推理。
- en: 'On \n Entity, we include Identity Preservation and Face Detection accuracy
    as additional metrics to quantify their entity image generation performance similar
    to (Yuan et al., [2023](#bib.bib47)). For detection accuracy, we utilize a RetinaFace
    (Deng et al., [2020](#bib.bib6)) based face detector and measure the average number
    of times a face is detected in a generated image. For Identity Preservation, we
    measure the degree of similarity between the reference image for an entity and
    the detected faces in an image. Out of the generated samples that have discernible
    faces, greedy matching is performed to select the most related face using Cosine
    Similarity distance. The ArcFace (Deng et al., [2019](#bib.bib5)) face recognition
    model is used for calculating Identity Preservation scores. We report the average
    metric scores across all entity classes in Table [7](#A4.T7 "Table 7 ‣ Appendix
    D Performance on \n Entity Set ‣ \n: LLM-driven News Subject Conditioning for
    Text-to-Image Synthesis") to visualize the overall quality of generated images.
    We also present Figure [8](#A4.F8 "Figure 8 ‣ Appendix D Performance on \n Entity
    Set ‣ \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis") with
    examples Ex1, Ex2, Ex3, Ex4 generated using the \n Entity Test Set.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '在\n实体中，我们将身份保留和面部检测准确性作为额外指标，以量化其实体图像生成性能，类似于（Yuan et al.，[2023](#bib.bib47)）。对于检测准确性，我们利用基于RetinaFace（Deng
    et al.，[2020](#bib.bib6)）的面部检测器，并测量在生成图像中检测到的面部的平均次数。对于身份保留，我们测量实体参考图像与图像中检测到的面部之间的相似度。在生成的样本中，对可辨别的面部进行贪婪匹配，以使用余弦相似度距离选择最相关的面部。使用ArcFace（Deng
    et al.，[2019](#bib.bib5)）面部识别模型计算身份保留分数。我们在表 [7](#A4.T7 "表 7 ‣ 附录 D 在\n实体集上的表现
    ‣ \n: 基于LLM的新闻主题条件文本到图像合成")中报告所有实体类别的平均指标分数，以可视化生成图像的整体质量。我们还展示了图 [8](#A4.F8 "图
    8 ‣ 附录 D 在\n实体集上的表现 ‣ \n: 基于LLM的新闻主题条件文本到图像合成")，其中包括使用\n实体测试集生成的示例Ex1、Ex2、Ex3、Ex4。'
- en: We observe that qualitatively, the generated images using SAFE cohesively include
    most objects presented in the caption while the images generated by SD 2.1 (Base)
    focus primarily on the NE’s present in the caption. This results in higher-quality
    faces generated at the cost of image artifacts such as repeated generations of
    the same entity. Quantitative metrics also struggle to capture image-caption similarity
    accurately when captions and images contain NEs as facial artifacts are penalized
    more compared to subject-forgetting errors. Similar inadequacies in metrics for
    assessing NE images have been observed by other works in the area of personalized
    T2I generation Yuan et al. ([2023](#bib.bib47)). Our experiments highlight the
    need for improved metrics to decouple the impact of generating NE features faithfully
    and representing all subjects mentioned in the caption accurately. Personalized
    T2I generation approaches may help alleviate the challenges in generating Entity
    images which we include as a part of our future work.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，从定性上看，使用SAFE生成的图像通常会包括大多数在描述中出现的对象，而SD 2.1 (Base)生成的图像主要集中在描述中出现的NE上。这导致生成的面部图像质量更高，但同时也出现了图像伪影，例如相同实体的重复生成。当描述和图像中包含NE时，定量指标也难以准确捕捉图像-描述的相似性，因为面部伪影相比于主体遗忘错误受到更多惩罚。其他研究在个性化T2I生成领域也观察到评估NE图像的指标存在类似不足（Yuan
    et al.，[2023](#bib.bib47)）。我们的实验突出了改进指标的必要性，以分离忠实生成NE特征和准确表示描述中提到的所有主体的影响。个性化T2I生成方法可能有助于缓解生成实体图像的挑战，这将作为我们未来工作的一个部分。
- en: '| Model | Identity (↑) | Detect (↑) | $FID_{CLIP}$ (↓) | ImageReward (↑) |
    HPS V2 (↑) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 身份保留 (↑) | 检测 (↑) | $FID_{CLIP}$ (↓) | 图像奖励 (↑) | HPS V2 (↑) |'
- en: '| SAFE (DFE + GPT-3.5) | 0.3323 | 0.9498 | 30.7756 | 0.6072 | 0.2564 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| SAFE (DFE + GPT-3.5) | 0.3323 | 0.9498 | 30.7756 | 0.6072 | 0.2564 |'
- en: '| Stable Diffusion 2.1 (Base) | 0.3391 | 0.9533 | 30.0651 | 0.6060 | 0.2565
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 稳定扩散 2.1 (Base) | 0.3391 | 0.9533 | 30.0651 | 0.6060 | 0.2565 |'
- en: 'Table 7: Results of Abstractive Text-to-Image synthesis on \n Entity Test-set
    averaged across all classes'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在\n实体测试集上，抽象文本到图像合成的结果，所有类别的平均值
- en: '| Orig | SD 2.1 (Base) | SAFE (DFE + GPT-3.5) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | SD 2.1 (Base) | SAFE (DFE + GPT-3.5) |'
- en: '| ![Refer to caption](img/a2afaed4050016eb6269501f9480ded5.png) | ![Refer to
    caption](img/5e6c8aaba1248affb02811912b746e71.png) | ![Refer to caption](img/940a1d302d4c19ea3149bfee2bc08da1.png)
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/a2afaed4050016eb6269501f9480ded5.png) | ![参见说明](img/5e6c8aaba1248affb02811912b746e71.png)
    | ![参见说明](img/940a1d302d4c19ea3149bfee2bc08da1.png) |'
- en: '| Obama awards Medal of Honor to member of SEAL Team 6. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 奥巴马向海豹突击队第6组成员颁发荣誉勋章。 |'
- en: '| ![Refer to caption](img/77879489b57a3ca350e2a60ca445665a.png) | ![Refer to
    caption](img/d3ba12bc872e50357c83b995acad453e.png) | ![Refer to caption](img/5f47211881019106be9296c6a7cb8e81.png)
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/77879489b57a3ca350e2a60ca445665a.png) | ![参见说明](img/d3ba12bc872e50357c83b995acad453e.png)
    | ![参见说明](img/5f47211881019106be9296c6a7cb8e81.png) |'
- en: '| London’s mayor Boris Johnson gives a big thumbs up to photographers during
    the unveiling of the 2012 Olympic rings on Tower Bridge. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 伦敦市长鲍里斯·约翰逊在塔桥揭幕2012年奥林匹克环时对摄影师竖起大拇指。 |'
- en: '| ![Refer to caption](img/75388dbe05a9ffa34de94ddc3ef12d38.png) | ![Refer to
    caption](img/132b694b75e77ec38ed4ba96f1e22200.png) | ![Refer to caption](img/0272297eefc80b9a3939f17c8c486573.png)
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/75388dbe05a9ffa34de94ddc3ef12d38.png) | ![参见说明](img/132b694b75e77ec38ed4ba96f1e22200.png)
    | ![参见说明](img/0272297eefc80b9a3939f17c8c486573.png) |'
- en: '| Donald Trump waves to the crowd during a campaign rally on June 18 2016 in
    Phoenix. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 唐纳德·特朗普在 2016 年 6 月 18 日在凤凰城的竞选集会上向人群挥手。 |'
- en: '| ![Refer to caption](img/7f44abf1a60c1a887672407021119155.png) | ![Refer to
    caption](img/c4f0014975761a5ba525947275c348bf.png) | ![Refer to caption](img/4ac3e9239d514f68ea30b927cc8f3a67.png)
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/7f44abf1a60c1a887672407021119155.png) | ![参见说明](img/c4f0014975761a5ba525947275c348bf.png)
    | ![参见说明](img/4ac3e9239d514f68ea30b927cc8f3a67.png) |'
- en: '| Hillary Clinton greets audience members following a campaign organizing event
    at Eagle Heights elementary in Clinton Iowa. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 希拉里·克林顿在爱荷华州克林顿市的鹰高小学举行的竞选组织活动后向观众致意。 |'
- en: 'Figure 8: Qualitative comparison of different T2I models on \n Entity Subset.
    Words highlighted in Orange are used for subject conditioning'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 不同 T2I 模型在实体子集上的定性比较。橙色高亮的词语用于主题调节。 |'
- en: Appendix E Additional Examples
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 额外示例 |
- en: 'We provide additional generated examples using both baseline and SAFE models
    for reference. Figure [9](#A5.F9 "Figure 9 ‣ Appendix E Additional Examples ‣
    \n: LLM-driven News Subject Conditioning for Text-to-Image Synthesis") with examples
    Ex5, Ex6, Ex7, Ex8, Ex9, Ex10, Ex11, Ex12 are generated from the test set of \n
    Non-Entity.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了使用基线和SAFE模型生成的附加示例供参考。图[9](#A5.F9 "图 9 ‣ 附录 E 额外示例 ‣ \n: LLM驱动的新闻主题调节用于文本到图像合成")中的示例
    Ex5, Ex6, Ex7, Ex8, Ex9, Ex10, Ex11, Ex12 是从测试集中生成的，属于非实体（Non-Entity）类别。 |'
- en: '| Orig | SD 2.1 (Base) | SD 2.1 (CR) | SAFE (DFE + GPT-3.5) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Orig | SD 2.1 (Base) | SD 2.1 (CR) | SAFE (DFE + GPT-3.5) |'
- en: '| ![[Uncaptioned image]](img/3cba8c3b033b6d631deac7f90edab736.png) | ![[Uncaptioned
    image]](img/1c01367052b0adc46b14cc65c564aa4d.png) | ![[Uncaptioned image]](img/9781c90aa65dc87254d07eba1b0e169f.png)
    | ![[Uncaptioned image]](img/d819cb9ea3563cb8c0c6f63e01d5baf7.png) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ![[无说明图像]](img/3cba8c3b033b6d631deac7f90edab736.png) | ![[无说明图像]](img/1c01367052b0adc46b14cc65c564aa4d.png)
    | ![[无说明图像]](img/9781c90aa65dc87254d07eba1b0e169f.png) | ![[无说明图像]](img/d819cb9ea3563cb8c0c6f63e01d5baf7.png)
    |'
- en: '| A Faraday bag, which blocks remote signals to devices such as cellphones
    and tablets. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 一个法拉第包，能够阻挡如手机和平板等设备的远程信号。 |'
- en: '| ![[Uncaptioned image]](img/bb80fadbf1b162ebc1ea025638e5c1d1.png) | ![[Uncaptioned
    image]](img/a2c8ab33b9410c3fe807c290e39e4983.png) | ![[Uncaptioned image]](img/3acfd46e5492720d2424ed5884971f6a.png)
    | ![[Uncaptioned image]](img/fdcd4514ea0d958785ecfcf72706929d.png) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| ![[无说明图像]](img/bb80fadbf1b162ebc1ea025638e5c1d1.png) | ![[无说明图像]](img/a2c8ab33b9410c3fe807c290e39e4983.png)
    | ![[无说明图像]](img/3acfd46e5492720d2424ed5884971f6a.png) | ![[无说明图像]](img/fdcd4514ea0d958785ecfcf72706929d.png)
    |'
- en: '| At first glance, pelota mixteca might resemble elements of baseball, volleyball
    and tennis, but a closer examination reveals a bit more nuance. Each jugada, as
    each individual game is called, involves approximately 10 players, and begins
    when one player initiates a serve from the cement slab. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 初看之下，pelota mixteca 可能会让人联想到棒球、排球和网球的元素，但深入观察会发现更多细微之处。每个叫做 jugada 的单独比赛涉及大约
    10 名玩家，比赛开始时由一名玩家从水泥板上发球。 |'
- en: '| ![[Uncaptioned image]](img/4fdc599e9f3d6237fdd184f972a99b1c.png) | ![[Uncaptioned
    image]](img/9bb90280f56f54af7b6605319f11cce3.png) | ![[Uncaptioned image]](img/ed6db097acb9545502e3257d58220125.png)
    | ![[Uncaptioned image]](img/67c9bdb7ca00a7ef48a5d09cf70e7644.png) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| ![[无说明图像]](img/4fdc599e9f3d6237fdd184f972a99b1c.png) | ![[无说明图像]](img/9bb90280f56f54af7b6605319f11cce3.png)
    | ![[无说明图像]](img/ed6db097acb9545502e3257d58220125.png) | ![[无说明图像]](img/67c9bdb7ca00a7ef48a5d09cf70e7644.png)
    |'
- en: '| The painted figure of a man is illuminated through a doorway to the dwelling.
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 通过住宅的门缝照亮了一个画着人的形象。 |'
- en: '| ![[Uncaptioned image]](img/83ad84646ab5ff380cf7fcfb10fef3e5.png) | ![[Uncaptioned
    image]](img/1c0a4d0d2d46bc88012e46fabd7b7a5c.png) | ![[Uncaptioned image]](img/75f8c2553bae2f7ec7748b92bddd9e19.png)
    | ![[Uncaptioned image]](img/5fe80e9516a99b2acab7bfc722b636cb.png) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| ![未说明的图像](img/83ad84646ab5ff380cf7fcfb10fef3e5.png) | ![未说明的图像](img/1c0a4d0d2d46bc88012e46fabd7b7a5c.png)
    | ![未说明的图像](img/75f8c2553bae2f7ec7748b92bddd9e19.png) | ![未说明的图像](img/5fe80e9516a99b2acab7bfc722b636cb.png)
    |'
- en: '| The detachable cable means the music doesn’t stop when the battery runs out
    but also makes it easy to connect to a computer or share your music with a friend.
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 可拆卸的电缆意味着当电池耗尽时音乐不会停止，而且还方便连接到计算机或与朋友分享音乐。 |'
- en: '| Orig | SD 2.1 (Base) | SD 2.1 (CR) | SAFE (DFE + GPT-3.5) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | SD 2.1（基础） | SD 2.1（CR） | SAFE（DFE + GPT-3.5） |'
- en: '| ![Refer to caption](img/afc36a52b00a1457e4d7769edc5971df.png) | ![Refer to
    caption](img/6cb689b80853d3505bd59bc17dd8c1c2.png) | ![Refer to caption](img/d31932947a5586a6b7e71211a8600292.png)
    | ![Refer to caption](img/5be1fbc3725f891f3eb712479824fd75.png) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/afc36a52b00a1457e4d7769edc5971df.png) | ![参见说明](img/6cb689b80853d3505bd59bc17dd8c1c2.png)
    | ![参见说明](img/d31932947a5586a6b7e71211a8600292.png) | ![参见说明](img/5be1fbc3725f891f3eb712479824fd75.png)
    |'
- en: '| Mediastreaming boxes can turn any TV smart or add features and channels to
    others for as little as 15. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 媒体流传输盒可以使任何电视变得智能，或为其他电视增加功能和频道，价格低至15美元。'
- en: '| ![Refer to caption](img/e2995c79f205aa5bdaefc23ad823801d.png) | ![Refer to
    caption](img/efcf717650edf5c648d871c37c25e73c.png) | ![Refer to caption](img/3b60389f8f748e36e63defdfa4c6917b.png)
    | ![Refer to caption](img/424dd770fc5e7747271c3ce1c16b3ab7.png) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/e2995c79f205aa5bdaefc23ad823801d.png) | ![参见说明](img/efcf717650edf5c648d871c37c25e73c.png)
    | ![参见说明](img/3b60389f8f748e36e63defdfa4c6917b.png) | ![参见说明](img/424dd770fc5e7747271c3ce1c16b3ab7.png)
    |'
- en: '| A sharp knife, one of a cook’s essential tools, is used to carefully cut
    onions, which are easier to brown (if they’re not bludgeoned) for a confit. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 一把锋利的刀子，作为厨师必备工具之一，用于仔细切割洋葱，使其更容易上色（如果没有被打碎的话），以用于做香煎菜肴。 |'
- en: '| ![Refer to caption](img/db251e647fb99ecdf3d2fc814dffa6eb.png) | ![Refer to
    caption](img/ef4b2efece3045105174bce9eb18bc88.png) | ![Refer to caption](img/ed551ae87cfd52dafc9fbf746fea0da7.png)
    | ![Refer to caption](img/6bded426f2c18555cb6ebb9919ecc5cd.png) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/db251e647fb99ecdf3d2fc814dffa6eb.png) | ![参见说明](img/ef4b2efece3045105174bce9eb18bc88.png)
    | ![参见说明](img/ed551ae87cfd52dafc9fbf746fea0da7.png) | ![参见说明](img/6bded426f2c18555cb6ebb9919ecc5cd.png)
    |'
- en: '| Although the home was built in 1871, the ground-floor commercial kitchen
    is contemporary. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 尽管这座房子建于1871年，但一楼的商业厨房却是现代的。 |'
- en: '| ![Refer to caption](img/7b89ab4e1293f8e417d84d8ebabce016.png) | ![Refer to
    caption](img/c6d784cf4401283db4e36473d2990b92.png) | ![Refer to caption](img/ffd390e7f00f3f6cba182adef51756d9.png)
    | ![Refer to caption](img/7bba7e55431d243c59809f7e16bcacc0.png) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/7b89ab4e1293f8e417d84d8ebabce016.png) | ![参见说明](img/c6d784cf4401283db4e36473d2990b92.png)
    | ![参见说明](img/ffd390e7f00f3f6cba182adef51756d9.png) | ![参见说明](img/7bba7e55431d243c59809f7e16bcacc0.png)
    |'
- en: '| About 200 firefighters were called to 401 Jewett Avenue. The front of the
    house was engulfed in flames when they arrived. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 大约200名消防员被召到401 Jewett Avenue。他们到达时，房子的前面已经被火焰吞噬。 |'
- en: 'Figure 9: Qualitative comparison of different T2I models on \n Non-Entity Subset.
    Words highlighted in Orange are used for subject conditioning'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：不同 T2I 模型在非实体子集上的定性比较。橙色高亮的词用于主题条件化
