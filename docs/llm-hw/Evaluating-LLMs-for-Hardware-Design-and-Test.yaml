- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Evaluating LLMs for Hardware Design and Test
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估用于硬件设计和测试的 LLMs
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.02326](https://ar5iv.labs.arxiv.org/html/2405.02326)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.02326](https://ar5iv.labs.arxiv.org/html/2405.02326)
- en: \lst@Key
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \lst@Key
- en: 'numbersnone\lstKV@SwitchCases#1none:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'numbersnone\lstKV@SwitchCases#1none:'
- en: 'left:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'left:'
- en: 'right:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'right:'
- en: Jason Blocklove New York University New York, NY USA
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Jason Blocklove 纽约大学 纽约，NY 美国
- en: jason.blocklove@nyu.edu    Siddharth Garg New York University New York, NY USA
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: jason.blocklove@nyu.edu    Siddharth Garg 纽约大学 纽约，NY 美国
- en: siddharth.garg@nyu.edu    Ramesh Karri New York University New York, NY USA
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: siddharth.garg@nyu.edu    Ramesh Karri 纽约大学 纽约，NY 美国
- en: rkarri@nyu.edu    Hammond Pearce University of New South Wales
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: rkarri@nyu.edu    Hammond Pearce 新南威尔士大学
- en: Sydney, Australia
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 悉尼，澳大利亚
- en: hammond.pearce@unsw.edu.au
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: hammond.pearce@unsw.edu.au
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have demonstrated capabilities for producing code
    in Hardware Description Languages (HDLs). However, most of the focus remains on
    their abilities to write functional code, not test code. The hardware design process
    consists of both design and test, and so eschewing validation and verification
    leaves considerable potential benefit unexplored, given that a design and test
    framework may allow for progress towards full automation of the digital design
    pipeline. In this work, we perform one of the first studies exploring how a LLM
    can both design and test hardware modules from provided specifications. Using
    a suite of 8 representative benchmarks, we examined the capabilities and limitations
    of the state-of-the-art conversational LLMs when producing Verilog for functional
    and verification purposes. We taped out the benchmarks on a Skywater 130nm shuttle
    and received the functional chip.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经展示了在硬件描述语言（HDLs）中生成代码的能力。然而，大多数关注点仍然集中在它们编写功能性代码的能力上，而不是测试代码。硬件设计过程包括设计和测试，因此忽略验证和确认会留下相当大的潜在好处未被探索，因为设计和测试框架可能允许实现数字设计管道的完全自动化。在这项工作中，我们进行了一项探索性研究，研究了
    LLM 如何根据提供的规格设计和测试硬件模块。使用一套8个具有代表性的基准测试，我们检查了最先进的对话式 LLM 在生成 Verilog 用于功能性和验证目的方面的能力和局限性。我们将基准测试投入
    Skywater 130nm 试产中，并获得了功能性芯片。
- en: 'Index Terms:'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Hardware Design and Verification, CAD, LLM
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件设计和验证，CAD，LLM
- en: I Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Digital hardware design has traditionally relied on a relatively niche skill
    set, requiring specifically trained engineers to both architect and implement
    new semiconductor hardware designs, and then to validate and verify those designs
    before they are taped out into integrated circuits. This testing process is time-consuming
    and difficult, requiring the development of appropriately comprehensive tests
    to ensure that every possible eventuality is accurately accounted for. Indeed,
    according to [[1](#bib.bib1)], 51% of development effort (cost) in both ASIC and
    FPGA-based systems are spent on verification. Any missed bugs will have increasingly
    expensive consequences, depending on how late in the product’s development and
    deployment lifecycle they are eventually discovered. In a push to simplify design
    and test, tools have been developed to support processes like high-level synthesis
    (HLS) [[2](#bib.bib2)], which make it easier for those with preexisting software
    development experience to create functioning hardware—though in order for an engineer
    to take full advantages of the domain, they will still often require a good level
    of hardware development knowledge.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数字硬件设计传统上依赖于相对小众的技能组合，需要专门培训的工程师来架构和实施新的半导体硬件设计，然后在这些设计被投入集成电路之前进行验证。这一测试过程耗时且困难，需要制定适当全面的测试以确保每种可能的情况都被准确考虑。事实上，根据[[1](#bib.bib1)]，在ASIC和FPGA系统中，51%的开发工作（成本）花费在验证上。任何遗漏的错误将根据其被发现的开发和部署生命周期的晚期阶段带来越来越昂贵的后果。为了简化设计和测试，已经开发出支持高层次综合（HLS）[[2](#bib.bib2)]的工具，这使得拥有预先存在的软件开发经验的人更容易创建功能性硬件——尽管为了让工程师充分利用这一领域，他们仍然通常需要具备良好的硬件开发知识。
- en: Other techniques have leveraged machine learning (ML) to enhance tooling for
    design and test. For design-focused examples [[3](#bib.bib3)] discusses ML adoption
    in logic synthesis, design space reduction, exploration, placement, and routing.
    These techniques focus on simplifying electronic design automation (EDA) algorithms
    rather than simplifying the tasks. Advances in natural language processing—in
    particular, Large Language Models (LLM)—have presented new avenues for hardware
    design. Prior work shows that LLMs can write code, including in verilog hardware
    description language (HDL)  [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)]. However, while this demonstrated how LLMs can aid design, research
    exploring LLMs for test are lacking.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其他技术利用机器学习 (ML) 来增强设计和测试工具。针对设计的示例 [[3](#bib.bib3)] 讨论了 ML 在逻辑综合、设计空间缩减、探索、布局和布线中的应用。这些技术专注于简化电子设计自动化
    (EDA) 算法，而不是简化任务。自然语言处理的进展——尤其是大语言模型 (LLM)——为硬件设计提供了新的途径。先前的工作表明，LLM 可以编写代码，包括
    Verilog 硬件描述语言 (HDL) [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)]。然而，尽管这展示了 LLM 如何帮助设计，但探讨 LLM 在测试中的应用的研究仍然不足。
- en: 'In this work we evaluate how LLMs may be used for both design and test together
    by starting from a plain language specification, instructing for Verilog implementation,
    and then creating the associated testbenches. To this end we leverage design and
    test tools in a feedback loop with a LLM such that minimal user debugging assistance
    is required. To this end, we provide a set of conversational LLMs with simple
    prompts to create the functional and verification HDL, which we then successfully
    taped out through Tiny Tapeout 3 [[9](#bib.bib9)]. Our contributions are: (i)
    Developing simple benchmarks to evaluate the capabilities of LLMs for functional
    hardware development and verification. (ii) Providing post-silicon results from
    a taped-out device. Open-source: Benchmarks, toolchain scripts, Verilog and LLM
    conversation logs are on Zenodo [[10](#bib.bib10)].'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们评估了 LLM 如何同时用于设计和测试，通过从自然语言规范开始，指导 Verilog 实现，然后创建相关的测试平台。为此，我们利用设计和测试工具与
    LLM 进行反馈循环，以便需要最少的用户调试协助。为此，我们提供了一组对话式 LLM，并使用简单的提示创建功能性和验证 HDL，然后通过 Tiny Tapeout
    3 [[9](#bib.bib9)] 成功完成。我们的贡献包括：（i）开发简单的基准测试，以评估 LLM 在功能性硬件开发和验证中的能力。（ii）提供来自已封装设备的后硅结果。开源：基准测试、工具链脚本、Verilog
    和 LLM 对话记录可在 Zenodo [[10](#bib.bib10)] 上获得。
- en: II Background and Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景和相关工作
- en: II-A Large Language Models (LLMs)
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 大语言模型 (LLMs)
- en: LLMs such as GPT-3 [[11](#bib.bib11)] and Codex [[12](#bib.bib12)] are a class
    of ML model which use a Transformer architecture [[13](#bib.bib13)] and are trained
    on a large corpus of plain-language data to generate a predicted output. LLMs
    receive an text-based input prompt, then produce the “most-likely” continuation
    of that prompt. By training over appropriate data this means LLMs can complete
    lexical sequences, including code. Further, LLMs have recently been made “conversational”
    using instruction-tuning. Rather than guessing the next most likely token in an
    “autocomplete” fashion, they ingest whole prompts and formulate complete responses
    to those prompts. Examples include ChatGPT [[14](#bib.bib14)] versions 3.5 and
    4, Google’s Bard [[15](#bib.bib15)], and HuggingFace’s HuggingChat [[16](#bib.bib16)].
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-3 [[11](#bib.bib11)] 和 Codex [[12](#bib.bib12)] 这样的 LLM 是一种使用 Transformer
    架构 [[13](#bib.bib13)] 的机器学习模型，它们在大量的自然语言数据上进行训练以生成预测输出。LLM 接收基于文本的输入提示，然后生成该提示的“最可能”继续部分。通过对适当数据的训练，这意味着
    LLM 可以完成词汇序列，包括代码。此外，LLM 最近通过指令调优变得“对话化”。它们不是以“自动完成”方式猜测下一个最可能的标记，而是处理整个提示并对这些提示形成完整的响应。例如，ChatGPT
    [[14](#bib.bib14)] 的 3.5 版和 4 版、谷歌的 Bard [[15](#bib.bib15)] 和 HuggingFace 的 HuggingChat
    [[16](#bib.bib16)]。
- en: II-B LLM Aided Design
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B LLM 辅助设计
- en: Several LLMs have been created with the intention of generating Verilog for
    hardware design, e.g. Pearce et al. [[4](#bib.bib4)] fine-tuned a GPT-2 model
    aimed at generating Verilog; and VeriGen [[5](#bib.bib5)] is a fine-tuned CodeGen
    model; another work just used GPT-4 directly to co-author a microprocessor [[7](#bib.bib7)]
    sent for tapeout. Commercial models also exist, such as RapidGPT [[17](#bib.bib17)],
    ChipNeMo [[18](#bib.bib18)], JedAI [[19](#bib.bib19)], and Synopsys.ai [[20](#bib.bib20)].
    Test-focused work is typically aimed at evaluating the models, e.g. VerilogEval [[8](#bib.bib8)]
    offers a set of standard benchmarks derived from HDLBits [[21](#bib.bib21)] for
    LLMs writing Verilog. LLM work focusing on design test is more nascent. [[22](#bib.bib22)]
    explored generating SystemVerilog assertions with some success, and [[23](#bib.bib23)]
    examined bug-finding in isolation without considering tool feedback.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 已经创建了几个 LLM，旨在生成用于硬件设计的 Verilog，例如 Pearce 等人 [[4](#bib.bib4)] 微调了一个目标生成 Verilog
    的 GPT-2 模型；VeriGen [[5](#bib.bib5)] 是一个微调过的 CodeGen 模型；还有其他工作直接使用 GPT-4 共同编写微处理器
    [[7](#bib.bib7)] 送往 tapeout。也存在商业模型，如 RapidGPT [[17](#bib.bib17)]、ChipNeMo [[18](#bib.bib18)]、JedAI
    [[19](#bib.bib19)] 和 Synopsys.ai [[20](#bib.bib20)]。测试重点的工作通常旨在评估模型，例如 VerilogEval
    [[8](#bib.bib8)] 提供了一组来自 HDLBits [[21](#bib.bib21)] 的标准基准，用于 LLM 编写 Verilog。聚焦于设计测试的
    LLM 工作则较为初期。 [[22](#bib.bib22)] 探索了生成 SystemVerilog 断言并取得了一些成功，[[23](#bib.bib23)]
    则在不考虑工具反馈的情况下研究了孤立的错误发现。
- en: III Prompting LLMs for Design and Test
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 设计和测试 LLM
- en: III-A Methodology
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 方法论
- en: '[Figure 1](#S3.F1 "In III-A Methodology ‣ III Prompting LLMs for Design and
    Test ‣ Evaluating LLMs for Hardware Design and Test") illustrates the main experimental
    structure for our design and test focused investigation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1](#S3.F1 "在 III-A 方法论 ‣ III 设计和测试 LLM ‣ 评估 LLM 在硬件设计和测试中的应用") 说明了我们设计和测试聚焦调查的主要实验结构。'
- en: '![Refer to caption](img/b9e272c7303e74afa785b90e679c8a7f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9e272c7303e74afa785b90e679c8a7f.png)'
- en: 'Figure 1: Simplified LLM conversation flowchart'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：简化的 LLM 对话流程图
- en: The prompts in [Figure 2](#S3.F2 "In III-A Methodology ‣ III Prompting LLMs
    for Design and Test ‣ Evaluating LLMs for Hardware Design and Test") (lines 2-8
    updated for each design) and [Figure 3](#S3.F3 "In III-A Methodology ‣ III Prompting
    LLMs for Design and Test ‣ Evaluating LLMs for Hardware Design and Test") (always
    constant) are given to the LLM. Then, the output is inspected to determine if
    it meets the I/O design specification. If not, it is regenerated with the same
    prompt up to five times, and is deemed failing if it cannot meet specifications.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2](#S3.F2 "在 III-A 方法论 ‣ III 设计和测试 LLM ‣ 评估 LLM 在硬件设计和测试中的应用")（第 2-8 行针对每个设计更新）和
    [图 3](#S3.F3 "在 III-A 方法论 ‣ III 设计和测试 LLM ‣ 评估 LLM 在硬件设计和测试中的应用")（始终不变）的提示被提供给
    LLM。然后，检查输出是否符合 I/O 设计规范。如果不符合，则使用相同的提示重新生成最多五次，如果仍然无法满足规范，则视为失败。'
- en: Once the design and testbench have been written, they are compiled with Icarus
    Verilog (iverilog) [[24](#bib.bib24)] and, if the compilation succeeds, simulated—closing
    the loop between design and test. If no errors are reported then the design passes
    with no feedback necessary (NFN). If instead either of those actions report errors
    they are fed back into the LLM and it is asked to “Please provide fixes.”, referred
    to as tool feedback (TF). However, LLMs cannot always complete designs. If we
    observe the same error or type of error appearing three times, then simple human
    feedback (SHF) is given by the user, usually by stating what type of problem in
    Verilog would cause this error (e.g. syntax error in declaring a signal). If the
    error continues, moderate human feedback (MHF) is given with more directed information
    being given to the tool to identify the specific error, and if the error persists
    then advanced human feedback (AHF) is given which relies on pointing out precisely
    where the error is and the method of fixing it. Once the design compiles and simulates
    with no failing test cases, it is considered a success. If advanced feedback does
    not fix the error or should the user need to write any Verilog to address the
    error, the test is considered a failure. The test is also considered a failure
    if the conversation exceeds 25 messages. Sometimes, an LLM’s response would be
    cut off due to excessive length. In those cases the model would be prompted with
    “Please continue”. The final code following this sequence would then usually need
    human editing for re-assembly, but no HDL was ever added during this process.
    On other occasions, responses included comments for the user to add their own
    code.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设计和测试台编写完成，它们将使用 Icarus Verilog (iverilog) [[24](#bib.bib24)] 进行编译，如果编译成功，则进行仿真——完成设计与测试之间的闭环。如果没有报告错误，则设计通过，无需反馈
    (NFN)。如果这些操作中的任何一个报告错误，这些错误会反馈给 LLM，并要求“请提供修复。”，这称为工具反馈 (TF)。然而，LLM 并不总是能够完成设计。如果我们观察到相同的错误或错误类型出现三次，则用户会提供简单的人类反馈
    (SHF)，通常是指出 Verilog 中导致此错误的问题类型（例如，声明信号时的语法错误）。如果错误持续存在，则提供适度的人类反馈 (MHF)，并向工具提供更多针对性的信息以识别特定错误，如果错误仍然存在，则提供高级人类反馈
    (AHF)，这依赖于准确指出错误的位置和修复方法。一旦设计编译并仿真时没有失败的测试用例，它被视为成功。如果高级反馈未能修复错误，或用户需要编写任何 Verilog
    来解决错误，则测试被视为失败。如果对话超过 25 条消息，测试也被视为失败。有时，LLM 的响应由于长度过长而被截断。在这些情况下，模型会被提示“请继续”。随后跟随的最终代码通常需要人工编辑以进行重新组装，但在此过程中没有添加
    HDL。在其他情况下，响应中包括了供用户添加自己代码的注释。
- en: 1I  am  trying  to  create  a  Verilog  model  for  a  shift  register.  It  must  meet  the  following  specifications:2-  Inputs:3  -  Clock4  -  Active-low  reset5  -  Data  (1  bit)6  -  Shift  enable7-  Outputs:8  -  Data  (8  bits)9How  would  I  write  a  design  that  meets  these  specifications?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在尝试创建一个移位寄存器的 Verilog 模型。它必须满足以下规格：- 输入：- 时钟- 低电平有效复位- 数据（1 位）- 移位使能- 输出：-
    数据（8 位）我该如何编写符合这些规格的设计？
- en: 'Figure 2: Design prompt with 8-bit shift register example. Lines 2-8 would
    be updated depending upon the desired spec.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：带有 8 位移位寄存器示例的设计提示。第 2-8 行将根据所需的规格进行更新。
- en: 1Can  you  create  a  Verilog  testbench  for  this  design?  It  should  be  self-checking  and  made  to  work  with  iverilog  for  simulation  and  validation.  If  test  cases  should  fail,  the  testbench  should  provide  enough  information  that  the  error  can  be  found  and  resolved.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你能为这个设计创建一个 Verilog 测试台吗？它应该是自检的，并且能够与 iverilog 一起进行仿真和验证。如果测试用例失败，测试台应提供足够的信息，以便找到并解决错误。
- en: 'Figure 3: Testbench prompt. This prompt remains constant.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：测试台提示。这一提示保持不变。
- en: III-B Real-world design constraints on benchmark design
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基准设计的实际设计约束
- en: 'Real-world hardware design has synthesis, budgetary, and tape-out constraints.
    We targeted the real-world platform Tiny Tapeout 3 [[9](#bib.bib9)], which sells
    small areas (1000 standard cells) of a Skywater 130nm shuttle. This adds constraints
    to the design: a limit on IO – each design was allowed 8 bits of input and 8 bits
    of output. We reserved 3 bits of input for a multiplexer to choose a benchmark,
    so we could have $2^{3}=8$ benchmarks. Each benchmark could only have 5 input
    bits, including clock and reset. The Tiny Tapeout toolflow relies on OpenLane [[25](#bib.bib25)]:
    we were restricted to Verilog-2001. Some benchmarks had requirements beyond the
    initial design, to examine how LLMs handled different constraints. The sequence
    generator and detector were given patterns to generate or detect respectively,
    ABRO was asked to use one-hot state encoding, and LFSR had an initial state and
    tap locations.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的硬件设计有合成、预算和布线约束。我们以现实世界平台Tiny Tapeout 3 [[9](#bib.bib9)]为目标，该平台销售Skywater
    130nm捷运的较小区域（1000个标准单元）。这为设计添加了约束：IO的限制——每个设计允许8位输入和8位输出。我们为多路复用器保留了3位输入，以选择基准，因此我们可以有$2^{3}=8$个基准。每个基准只能有5位输入，包括时钟和复位。Tiny
    Tapeout工具流程依赖于OpenLane [[25](#bib.bib25)]：我们被限制使用Verilog-2001。一些基准有超出初始设计的要求，以检查LLM如何处理不同的约束。序列生成器和检测器分别被给定了生成或检测的模式，ABRO被要求使用一热状态编码，而LFSR有初始状态和tap位置。
- en: III-C Challenge benchmarks
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 挑战基准
- en: The benchmarks given in [Table I](#S3.T1 "In III-C Challenge benchmarks ‣ III
    Prompting LLMs for Design and Test ‣ Evaluating LLMs for Hardware Design and Test")
    were designed to give insight in to hardware that the different LLMs could write.
    The functions are implemented in hardware, and taught at the level of an undergraduate
    digital logic curriculum.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 I](#S3.T1 "在 III-C 挑战基准 ‣ III 提示 LLM 进行设计与测试 ‣ 评估 LLM 硬件设计与测试")中给出的基准旨在提供不同LLM能够编写的硬件的洞察。这些功能已经在硬件中实现，并在本科数字逻辑课程的水平上进行讲授。'
- en: 'TABLE I: Benchmark Descriptions'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：基准描述
- en: '| Benchmark | Description |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 描述 |'
- en: '| --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 8-bit Shift Register | Shift register with enable |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 8位移位寄存器 | 带使能的移位寄存器 |'
- en: '| Sequence Generator | Generates a specific sequence of eight 8-bit values
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 序列生成器 | 生成特定的八个8位值序列 |'
- en: '| Sequence Detector | Detects if the correct 8 3-bit inputs were given consecutively
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 序列检测器 | 检测是否连续给出了正确的8个3位输入 |'
- en: '| ABRO FSM | One-hot state machine for detecting inputs A and B to emit O |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ABRO FSM | 用于检测输入A和B以发出O的一热状态机 |'
- en: '| Binary to BCD | Converts a 5-bit binary input into an 8-bit BCD output |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 二进制到BCD | 将5位二进制输入转换为8位BCD输出 |'
- en: '| LFSR | 8-bit Linear Feedback Shift Register |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| LFSR | 8位线性反馈移位寄存器 |'
- en: '| Traffic Light FSM | Cycle between 3 states based on a number of clock cycles
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 交通灯FSM | 根据时钟周期在3个状态之间循环 |'
- en: '| Dice Roller | Simulated rolling either a 4, 6, 8, or 20-sided die |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 掷骰器 | 模拟掷4面、6面、8面或20面骰子 |'
- en: 'III-D Model evaluation: Metrics'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 模型评估：指标
- en: 'TABLE II: Evaluated Conversational LLMs'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：评估的对话LLM
- en: '| Model | Release Date | Company | Open Access | Open Source |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 发布日期 | 公司 | 开放访问 | 开源 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ChatGPT-4 [[26](#bib.bib26)] | 14 Mar. 2023 | OpenAI | No | No |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-4 [[26](#bib.bib26)] | 2023年3月14日 | OpenAI | 否 | 否 |'
- en: '| ChatGPT-3.5 [[27](#bib.bib27)] | 30 Nov. 2022 | OpenAI | Yes | No |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT-3.5 [[27](#bib.bib27)] | 2022年11月30日 | OpenAI | 是 | 否 |'
- en: '| Bard [[15](#bib.bib15)] | 21 Mar. 2023 | Google | Yes | No |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Bard [[15](#bib.bib15)] | 2023年3月21日 | Google | 是 | 否 |'
- en: '| HuggingChat [[16](#bib.bib16)] | April 2023 | HuggingFace | Yes | Yes |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| HuggingChat [[16](#bib.bib16)] | 2023年4月 | HuggingFace | 是 | 是 |'
- en: 'The four conversational LLMs in [Table II](#S3.T2 "In III-D Model evaluation:
    Metrics ‣ III Prompting LLMs for Design and Test ‣ Evaluating LLMs for Hardware
    Design and Test") were evaluated in this work. As an initial qualifier, each model
    was prompted with the 8-bit shift register benchmark prompt with the intention
    of proceeding through the conversational flow in [Section III-A](#S3.SS1 "III-A
    Methodology ‣ III Prompting LLMs for Design and Test ‣ Evaluating LLMs for Hardware
    Design and Test"). The response from ChatGPT-4 is given in [Figure 4](#S3.F4 "In
    III-D Model evaluation: Metrics ‣ III Prompting LLMs for Design and Test ‣ Evaluating
    LLMs for Hardware Design and Test"), with the ChatGPT-3.5, Bard, and HuggingChat
    responses given in the Appendix. While ChatGPT models were able to meet the specifications
    and begin moving through the design flow, Bard and HuggingChat failed to meet
    the initial criteria of the specifications even when the initial prompts were
    regenerated five times. Bard failed to meet design specifications (Figure [2](#A0.F2
    "Figure 2 ‣ Evaluating LLMs for Hardware Design and Test")) and HuggingChat’s
    Verilog grew nonsensical past the module definition (Figure [3](#A0.F3 "Figure
    3 ‣ Evaluating LLMs for Hardware Design and Test")). Given the poor performance
    of Bard and HuggingChat on the qualifier benchmark prompt, we did the full set
    of tests only for ChatGPT models, both of which were able to continue with the
    conversational flow. For all benchmarks, we ran the conversations three times,
    as the LLMs are non-deterministic and respond differently to the same prompts.
    The repetition measures how consistently they can create benchmarks and testbenches,
    and how different runs with the same prompt differ in their implementation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作评估了[表 II](#S3.T2 "在 III-D 模型评估：指标 ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs 在硬件设计和测试中的表现")中的四个对话
    LLM。作为初步资格测试，每个模型都用 8 位移位寄存器基准提示进行了提示，目的是通过[第 III-A 节](#S3.SS1 "III-A 方法 ‣ III
    提示 LLMs 进行设计和测试 ‣ 评估 LLMs 在硬件设计和测试中的表现")的对话流程。ChatGPT-4 的响应如[图 4](#S3.F4 "在 III-D
    模型评估：指标 ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs 在硬件设计和测试中的表现")所示，ChatGPT-3.5、Bard 和 HuggingChat
    的响应见附录。虽然 ChatGPT 模型能够满足规格要求并开始进行设计流程，但即使重新生成了五次初始提示，Bard 和 HuggingChat 也未能满足初始规格标准。Bard
    未能满足设计规格（图[2](#A0.F2 "图 2 ‣ 评估 LLMs 在硬件设计和测试中的表现")），HuggingChat 的 Verilog 在模块定义之后变得荒谬（图[3](#A0.F3
    "图 3 ‣ 评估 LLMs 在硬件设计和测试中的表现")）。鉴于 Bard 和 HuggingChat 在资格基准提示上的表现不佳，我们仅对 ChatGPT
    模型进行了完整的测试，这两个模型均能够继续进行对话流程。对于所有基准，我们运行了三次对话，因为 LLMs 是非确定性的，并且对相同的提示会有不同的响应。重复测试衡量了它们在创建基准和测试平台时的一致性，以及相同提示的不同运行在实现上的差异。
- en: '1module  shift_register  (2  input  wire  clk,3  input  wire  reset_n,4  input  wire  data_in,5  input  wire  shift_enable,6  output  reg  [7:0]  data_out7);89always  @(posedge  clk  or  negedge  reset_n)  begin10  if  (!reset_n)  begin11  data_out  <=  8’b0;  //  Active-low  reset:  Clear  the  register  when  reset_n  is  low12  end  else  if  (shift_enable)  begin13  data_out  <=  {data_out[6:0],  data_in};  //  Shift  the  register  and  insert  the  new  data  bit14  end15end1617endmodule'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 1module  shift_register  (2  input  wire  clk,3  input  wire  reset_n,4  input  wire  data_in,5  input  wire  shift_enable,6  output  reg  [7:0]  data_out7);89always  @(posedge  clk  or  negedge  reset_n)  begin10  if  (!reset_n)  begin11  data_out  <=  8’b0;  //  低有效复位：当
    reset_n  为低时，清除寄存器12  end  else  if  (shift_enable)  begin13  data_out  <=  {data_out[6:0],  data_in};  //  移位寄存器并插入新数据位14  end15end1617endmodule
- en: 'Figure 4: 8-bit shift register attempt from ChatGPT-4.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: ChatGPT-4 的 8 位移位寄存器尝试。'
- en: 'Compliant vs. Non-compliant designs: Since the LLMs created both the functional
    code and testbenches, even if a design ‘passes’ the testbench it may still be
    ‘non-compliant’ with the original specification (e.g. an empty testbench which
    just returns ‘success!’). We therefore manually mark each passing result as ‘compliant’
    or ‘non-compliant’.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 合规设计与非合规设计：由于 LLMs 创建了功能代码和测试平台，即使一个设计通过了测试平台，它也可能仍然“非合规”于原始规格（例如，一个空测试平台只返回“成功！”）。因此，我们手动将每个通过的结果标记为“合规”或“非合规”。
- en: III-E Example conversation
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 示例对话
- en: '[Figure 5](#S3.F5 "In III-E Example conversation ‣ III Prompting LLMs for Design
    and Test ‣ Evaluating LLMs for Hardware Design and Test") provides the examples
    of the remainder of the conversation with ChatGPT-4 for the shift register T1—for
    succinctness, we remove the portions of the response that are irrelevant. This
    conversation flow follows the initial design prompt in [Figure 2](#S3.F2 "In III-A
    Methodology ‣ III Prompting LLMs for Design and Test ‣ Evaluating LLMs for Hardware
    Design and Test")), the design returned in [Figure 4](#S3.F4 "In III-D Model evaluation:
    Metrics ‣ III Prompting LLMs for Design and Test ‣ Evaluating LLMs for Hardware
    Design and Test"), and the testbench prompt in [Figure 3](#S3.F3 "In III-A Methodology
    ‣ III Prompting LLMs for Design and Test ‣ Evaluating LLMs for Hardware Design
    and Test")). Unfortunately, the testbench has erroneous traces (relevant portion
    presented in [Figure 5(a)](#S3.F5.sf1 "In Figure 5 ‣ III-E Example conversation
    ‣ III Prompting LLMs for Design and Test ‣ Evaluating LLMs for Hardware Design
    and Test")). When simulated, this will print error messages. These are returned
    to ChatGPT-4 using the message in [Figure 5(b)](#S3.F5.sf2 "In Figure 5 ‣ III-E
    Example conversation ‣ III Prompting LLMs for Design and Test ‣ Evaluating LLMs
    for Hardware Design and Test"). This prompts ChatGPT-4 to repair the testbench,
    giving the code in [Figure 5(c)](#S3.F5.sf3 "In Figure 5 ‣ III-E Example conversation
    ‣ III Prompting LLMs for Design and Test ‣ Evaluating LLMs for Hardware Design
    and Test"). The errors are resolved, and the design and testbench now compile
    and show passing simulation, meaning that the conversational design flow criteria
    is met.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5](#S3.F5 "在 III-E 示例对话 ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs 进行硬件设计和测试") 提供了与
    ChatGPT-4 关于移位寄存器 T1 的对话的其余部分的示例——为了简洁起见，我们删除了无关的回应部分。此对话流程遵循 [图2](#S3.F2 "在 III-A
    方法论 ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs 进行硬件设计和测试") 中的初始设计提示，[图4](#S3.F4 "在 III-D
    模型评估：指标 ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs 进行硬件设计和测试") 中返回的设计，以及 [图3](#S3.F3 "在 III-A
    方法论 ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs 进行硬件设计和测试") 中的测试平台提示。不幸的是，测试平台有错误的踪迹（相关部分见
    [图5(a)](#S3.F5.sf1 "在图5 ‣ III-E 示例对话 ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs 进行硬件设计和测试")）。当进行仿真时，这将打印错误消息。这些错误返回给
    ChatGPT-4，使用 [图5(b)](#S3.F5.sf2 "在图5 ‣ III-E 示例对话 ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs
    进行硬件设计和测试") 中的消息。这促使 ChatGPT-4 修复测试平台，给出 [图5(c)](#S3.F5.sf3 "在图5 ‣ III-E 示例对话
    ‣ III 提示 LLMs 进行设计和测试 ‣ 评估 LLMs 进行硬件设计和测试") 中的代码。错误已解决，设计和测试平台现在可以编译并显示通过仿真，这意味着对话设计流程标准已满足。'
- en: '1//  Test  case  1:  Simple  shift2data_in  =  1’b1;3shift_enable  =  1’b1;4repeat  (8)  @(posedge  clk);5if  (data_out  !==  8’b10000000)  $display("Error:  Test  case  1  failed.  Expected:  10000000,  Received:  %b",  data_out);67//  Test  case  2:  Shift  with  alternating  bits8//  ...  STIMULUS  TRUNCATED  FOR  SPACE9if  (data_out  !==  8’b10101010)  $display("Error:  Test  case  2  failed.  Expected:  10101010,  Received:  %b",  data_out);1011//  Test  case  3:  Shift  without  shift_enable12data_in  =  1’b0;13shift_enable  =  1’b0;14@(posedge  clk);15if  (data_out  !==  8’b10101010)  $display("Error:  Test  case  3  failed.  Expected:  10101010,  Received:  %b",  data_out);1617...1819//  Display  success  message  and  end  simulation20$display("All  test  cases  passed!");21$finish;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '1//  测试用例 1: 简单移位2data_in  =  1’b1;3shift_enable  =  1’b1;4repeat  (8)  @(posedge  clk);5if  (data_out  !==  8’b10000000)  $display("错误:
    测试用例 1 失败。预期: 10000000, 实际: %b", data_out);67//  测试用例 2: 交替位移8//  ...  刺激截断以节省空间9if  (data_out  !==  8’b10101010)  $display("错误:
    测试用例 2 失败。预期: 10101010, 实际: %b", data_out);1011//  测试用例 3: 无移位使能12data_in  =  1’b0;13shift_enable  =  1’b0;14@(posedge  clk);15if  (data_out  !==  8’b10101010)  $display("错误:
    测试用例 3 失败。预期: 10101010, 实际: %b", data_out);1617...1819//  显示成功消息并结束仿真20$display("所有测试用例通过!");21$finish;'
- en: (a) Portion of 8-bit shift register testbench with errors
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 带有错误的8位移位寄存器测试平台部分
- en: '1When  running  the  simulation  it  gives  the  following  output.  Please  provide  fixed  code.23Error:  Test  case  1  failed.  Expected:  10000000,  Received:  011111114Error:  Test  case  2  failed.  Expected:  10101010,  Received:  010101015Error:  Test  case  3  failed.  Expected:  10101010,  Received:  010101016All  test  cases  passed!'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '1在运行仿真时，它给出了以下输出。请提供修复后的代码。23错误: 测试用例 1 失败。预期: 10000000, 实际: 011111114错误: 测试用例
    2 失败。预期: 10101010, 实际: 010101015错误: 测试用例 3 失败。预期: 10101010, 实际: 010101016所有测试用例通过!'
- en: (b) Tool feedback prompt for 8-bit shift register
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 8位移位寄存器的工具反馈提示
- en: '1//  Test  case  1:  Simple  shift2data_in  =  1’b1;3shift_enable  =  1’b1;4repeat  (8)  @(posedge  clk);5if  (data_out  !==  8’b01111111)  $display("Error:  Test  case  1  failed.  Expected:  01111111,  Received:  %b",  data_out);67//  Test  case  2:  Shift  with  alternating  bits8//  ...  STIMULUS  TRUNCATED  FOR  SPACE9if  (data_out  !==  8’b01010101)  $display("Error:  Test  case  2  failed.  Expected:  01010101,  Received:  %b",  data_out);1011//  Test  case  3:  Shift  without  shift_enable12data_in  =  1’b0;13shift_enable  =  1’b0;14@(posedge  clk);15if  (data_out  !==  8’b01010101)  $display("Error:  Test  case  3  failed.  Expected:  01010101,  Received:  %b",  data_out);1617...1819//  Display  success  message  and  end  simulation20$display("All  test  cases  passed!");21$finish;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 1//  测试用例 1：简单的移位2data_in  =  1’b1;3shift_enable  =  1’b1;4repeat  (8)  @(posedge  clk);5if  (data_out  !==  8’b01111111)  $display("错误：测试用例
    1 失败。预期：01111111，实际：%b",  data_out);67//  测试用例 2：交替位的移位8//  ...  激励被截断以节省空间9if  (data_out  !==  8’b01010101)  $display("错误：测试用例
    2 失败。预期：01010101，实际：%b",  data_out);1011//  测试用例 3：无 shift_enable 的移位12data_in  =  1’b0;13shift_enable  =  1’b0;14@(posedge  clk);15if  (data_out  !==  8’b01010101)  $display("错误：测试用例
    3 失败。预期：01010101，实际：%b",  data_out);1617...1819//  显示成功消息并结束仿真20$display("所有测试用例通过！");21$finish;
- en: (c) Corrected portion of testbench code. Replaced values bold / highlighted.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 测试平台代码的更正部分。替换的值加粗/高亮。
- en: 'Figure 5: Remaining portions of the successful shift register T1 conversation
    with ChatGPT-4\. The design is compliant.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：成功移位寄存器 T1 的剩余部分与 ChatGPT-4 的对话。设计符合要求。
- en: IV Results
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 结果
- en: 'TABLE III: Benchmark challenge results'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：基准挑战结果
- en: '| Benchmark | Test Set | ChatGPT-4 | ChatGPT-3.5 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 测试集 | ChatGPT-4 | ChatGPT-3.5 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Outcome | Compliant | # Messages | Outcome | Compliant | # Messages |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 结果 | 符合 | 消息数量 | 结果 | 符合 | 消息数量 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Shift Register | T1 | TF | ✓ | 3 | SHF | ✓ | 13 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 移位寄存器 | T1 | TF | ✓ | 3 | SHF | ✓ | 13 |'
- en: '| T2 | TF | ✓ | 9 | FAIL | - | 25 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| T2 | TF | ✓ | 9 | 失败 | - | 25 |'
- en: '| T3 | AHF | ✓ | 15 | FAIL | - | 11 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| T3 | AHF | ✓ | 15 | 失败 | - | 11 |'
- en: '| Sequence Gen. | T1 | AHF | ✓ | 14 | FAIL | - | 25 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 序列生成 | T1 | AHF | ✓ | 14 | 失败 | - | 25 |'
- en: '| T2 | TF | ✓ | 4 | FAIL | - | 7 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| T2 | TF | ✓ | 4 | 失败 | - | 7 |'
- en: '| T3 | AHF | ✓ | 20 | FAIL | - | 25 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| T3 | AHF | ✓ | 20 | 失败 | - | 25 |'
- en: '| Sequence Det. | T1 | FAIL | - | 24 | FAIL | - | 21 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 序列检测 | T1 | 失败 | - | 24 | 失败 | - | 21 |'
- en: '| T2 | SHF | ✓ | 9 | SHF | ✗ | 8 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| T2 | SHF | ✓ | 9 | SHF | ✗ | 8 |'
- en: '| T3 | TF | ✓ | 13 | SHF | ✗ | 8 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| T3 | TF | ✓ | 13 | SHF | ✗ | 8 |'
- en: '| ABRO | T1 | FAIL | - | 16 | FAIL | - | 25 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ABRO | T1 | 失败 | - | 16 | 失败 | - | 25 |'
- en: '| T2 | AHF | ✓ | 20 | MHF | ✓ | 15 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| T2 | AHF | ✓ | 20 | MHF | ✓ | 15 |'
- en: '| T3 | TF | ✓ | 12 | NFN | ✗ | 3 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| T3 | TF | ✓ | 12 | NFN | ✗ | 3 |'
- en: '| LFSR | T1 | TF | ✓ | 12 | FAIL | - | 25 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| LFSR | T1 | TF | ✓ | 12 | 失败 | - | 25 |'
- en: '| T2 | SHF | ✓ | 7 | TF | ✓ | 4 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| T2 | SHF | ✓ | 7 | TF | ✓ | 4 |'
- en: '| T3 | SHF | ✓ | 9 | FAIL | - | 11 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| T3 | SHF | ✓ | 9 | 失败 | - | 11 |'
- en: '| Binary to BCD | T1 | TF | ✓ | 4 | SHF | ✗ | 8 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 二进制到 BCD | T1 | TF | ✓ | 4 | SHF | ✗ | 8 |'
- en: '| T2 | NFN | ✓ | 2 | FAIL | - | 12 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| T2 | NFN | ✓ | 2 | 失败 | - | 12 |'
- en: '| T3 | SHF | ✓ | 9 | TF | ✗ | 4 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| T3 | SHF | ✓ | 9 | TF | ✗ | 4 |'
- en: '| Traffic Light | T1 | TF | ✓ | 4 | FAIL | - | 25 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 红绿灯 | T1 | TF | ✓ | 4 | 失败 | - | 25 |'
- en: '| T2 | SHF | ✓ | 12 | FAIL | - | 13 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| T2 | SHF | ✓ | 12 | 失败 | - | 13 |'
- en: '| T3 | TF | ✓ | 5 | FAIL | - | 18 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| T3 | TF | ✓ | 5 | 失败 | - | 18 |'
- en: '| Dice Roller | T1 | SHF | ✗ | 8 | MHF | ✗ | 9 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 掷骰子 | T1 | SHF | ✗ | 8 | MHF | ✗ | 9 |'
- en: '| T2 | SHF | ✓ | 9 | FAIL | - | 25 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| T2 | SHF | ✓ | 9 | 失败 | - | 25 |'
- en: '| T3 | SHF | ✗ | 18 | NFN | ✗ | 3 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| T3 | SHF | ✗ | 18 | NFN | ✗ | 3 |'
- en: 'All chat logs are in [[10](#bib.bib10)]. [Table III](#S4.T3 "In IV Results
    ‣ Evaluating LLMs for Hardware Design and Test") shows the results of the three
    test-sets of benchmarks with ChatGPT-4 and -3.5. Experiment dates: The ChatGPT
    LLMs evolve over time. Our study was performed between 06 Apr and 12 May, 2023.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所有聊天记录在 [[10](#bib.bib10)] 中。[表 III](#S4.T3 "在 IV 结果 ‣ 评估用于硬件设计和测试的 LLM") 显示了
    ChatGPT-4 和 -3.5 的三个测试集基准结果。实验日期：ChatGPT LLM 随时间演变。我们的研究在 2023 年 4 月 6 日至 5 月
    12 日之间进行。
- en: IV-A Simulation Results
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 仿真结果
- en: ChatGPT-4 performed well. The majority of benchmarks passed, most of which required
    only tool feedback. ChatGPT-4 most frequently needed human feedback in testbench
    design. Several failure modes were consistent, with a common error being the addition
    of SystemVerilog-specific syntax in the design or testbench (not supported in
    iverilog). Testbenches produced by ChatGPT-4 were not comprehensive. Still, a
    majority of the designs that passed accompanying testbenches were deemed compliant.
    The two non-compliant ‘passes’ were Dice Rollers which did not produce pseudo-random
    outputs. The Dice Roller from test set T1 would output a 2 for one roll and then
    only 1 for all subsequent rolls, regardless of the die selected. Meanwhile, Dice
    Roller T3 would change values, but between a small set (depending on the chosen
    die) which is repeated. We synthesized test set T1 from ChatGPT-4 conversations
    for Tiny Tapeout 3, adding a wrapper module designed by ChatGPT-4\. The design
    took 85 combinational units, 4 diodes, 44 flip flops, 39 buffers, 300 taps.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT-4的表现良好。大多数基准测试通过，其中大部分仅需工具反馈。ChatGPT-4最常需要人类在测试基准设计中的反馈。几个失败模式具有一致性，常见错误是在设计或测试基准中添加了SystemVerilog特有的语法（在iverilog中不支持）。ChatGPT-4生成的测试基准并不全面，但大多数通过附带测试基准的设计被认为是合规的。两个不合规的‘通过’案例是骰子滚筒，这些骰子没有产生伪随机输出。测试集T1中的骰子滚筒会在一次投掷中输出2，而后续所有投掷中均仅输出1，无论选择了哪个骰子。与此同时，骰子滚筒T3会改变值，但仅在一个小范围内（取决于选择的骰子），且这种范围会重复。我们从ChatGPT-4的对话中合成了测试集T1用于Tiny
    Tapeout 3，添加了由ChatGPT-4设计的包装模块。该设计占用了85个组合单元，4个二极管，44个触发器，39个缓冲区，300个触点。
- en: ![Refer
    to caption](img/2a607ec60e08c0e0902acb3f9b49e058.png)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ![Refer
    to caption](img/2a607ec60e08c0e0902acb3f9b49e058.png)
- en: 'Figure 6: Tiny Tapeout 3 carrier board, with the chip in the green box. The
    board has and headers to interact with the chip to test the projects on it.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Tiny Tapeout 3载板，芯片在绿色框中。该板上有与芯片交互的连接器，以测试其上的项目。
- en: ChatGPT-3.5 performed worse than ChatGPT-4, with most conversations yielding
    a failed benchmark, and a majority of those that passed their testbenches being
    non-compliant. Modes of failure were less consistent with ChatGPT-3.5 than they
    were for ChatGPT-4, with many issues introduced between each conversation and
    benchmark. It required corrections to design and testbenches more often than ChatGPT-4.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT-3.5的表现不如ChatGPT-4，大多数对话都产生了失败的基准测试，且通过测试基准的多数情况不合规。失败模式在ChatGPT-3.5中比ChatGPT-4不够一致，许多问题在每次对话和基准测试之间引入。它比ChatGPT-4更频繁地需要对设计和测试基准进行修正。
- en: IV-B Silicon Results
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 硅结果
- en: Upon receiving the Tiny Tapeout 3 chip and carrier board, shown in [Figure 6](#S4.F6
    "In IV-A Simulation Results ‣ IV Results ‣ Evaluating LLMs for Hardware Design
    and Test"), the benchmarks were tested to ensure they matched the simulations.
    Each benchmark behaved in hardware as expected from the simulations ([Figure 4](#A0.F4
    "In Evaluating LLMs for Hardware Design and Test")), verifying that our design
    and verification can yield functional chips.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 收到Tiny Tapeout 3芯片和载板后，如[图6](#S4.F6 "在 IV-A 仿真结果 ‣ IV 结果 ‣ 硬件设计和测试的LLMs评估")所示，进行了基准测试以确保它们与仿真结果一致。每个基准在硬件中的表现与仿真结果相符（[图4](#A0.F4
    "在评估LLMs用于硬件设计和测试")），验证了我们的设计和验证能够产生功能正常的芯片。
- en: IV-C Evaluation
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 评估
- en: The ability to generate and test Verilog designs depends on the LLM. ChatGPT-4
    performed adequately, though it required human feedback for 12 of the 24 conversations
    to be successful and compliant with the specifications. When fixing errors, ChatGPT-4
    required several messages—even with minor errors, as it struggled to understand
    specific lines that caused error messages in iverilog. The errors it added tended
    to repeat between conversations. ChatGPT-4 struggled much more to create functioning
    testbenches than designs. The majority of benchmarks required little to no modification
    of the design, necessitating testbench repair. This is particularly true of FSMs,
    as the model seemed unable to create a testbench which properly checks the output
    without significant feedback regarding the state transitions and corresponding
    expected outputs. ChatGPT-3.5, on the other hand, struggled with both functional
    designs and testbenches. HuggingFace’s HuggingChat was the worst-performer, struggling
    at times to even write coherent Verilog. Google’s Bard was better at this, but
    was unable to follow instructions with enough detail that it could be evaluated.
    A major limitation with the performance of all models was in authorship of testbenches.
    We believe this reflects (non-) availability of open-source training data. The
    LLMs tested are non-deterministic, so the generated output was not consistent
    across tests, even for same input prompt. We performed a limited repeatability
    study by repeating experiments 3 times, but this does not reach statistical significance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 生成和测试Verilog设计的能力取决于LLM。虽然ChatGPT-4表现尚可，但在24次对话中需要人工反馈才能使12次对话成功并符合规格。在修复错误时，ChatGPT-4需要多条消息——即使是小错误，也因为难以理解特定行导致iverilog中的错误信息而遇到困难。它添加的错误在对话之间往往重复。ChatGPT-4在创建功能性测试平台方面的困难远大于设计。大多数基准测试几乎不需要对设计进行修改，但需要修复测试平台。特别是FSMs（有限状态机），因为模型似乎无法创建一个正确检查输出的测试平台，除非对状态转换和相应的期望输出进行大量反馈。另一方面，ChatGPT-3.5在功能设计和测试平台方面都存在困难。HuggingFace的HuggingChat表现最差，有时甚至难以编写连贯的Verilog。谷歌的Bard在这方面表现较好，但无法按照足够详细的说明进行操作。所有模型的一个主要限制是测试平台的创作。我们认为这反映了开源训练数据的（不）可用性。测试的LLMs是非确定性的，因此生成的输出在测试之间不一致，即使是相同的输入提示。我们通过重复实验三次进行了有限的重复性研究，但这并未达到统计显著性。
- en: V Conclusion
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: While current state of the art LLMs can be used for design tasks, they are still
    underperforming when it comes to test. Our investigation shows that ChatGPT-3.5
    and ChatGPT-4, the two premier models in this space, do not consistently understand
    or fix errors that they encounter during code authorship, nor can they reliably
    produce comprehensive testbenches in isolation. However, when an experienced hardware
    engineer is able to provide improved feedback, the most capable models were able
    to create simple and functioning designs with no direct Verilog written by the
    user. We believe that improvements in the LLM space, whether that be from new
    models or fine-tuning, could result in tools capable of simplifying hardware design
    and increasing designer productivity, especially considering common or simple
    hardware structures.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然目前最先进的LLM可以用于设计任务，但在测试方面仍表现不足。我们的调查显示，ChatGPT-3.5和ChatGPT-4这两个领域的顶级模型，在代码创作过程中遇到的错误并不能始终如一地理解或修复，也不能可靠地独立生成全面的测试平台。然而，当有经验的硬件工程师能够提供改进的反馈时，最有能力的模型能够创建简单且功能正常的设计，而无需用户直接编写Verilog。我们认为，LLM领域的改进，无论是通过新模型还是微调，都可能导致能够简化硬件设计并提高设计师生产力的工具，特别是考虑到常见或简单的硬件结构。
- en: References
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Foster, “Wilson Research Group Functional Verification Study 2020,”
    Nov. 2020, section: News. [Online]. Available: [https://blogs.sw.siemens.com/verificationhorizons/2020/11/05/part-1-the-2020-wilson-research-group-functional-verification-study/](https://blogs.sw.siemens.com/verificationhorizons/2020/11/05/part-1-the-2020-wilson-research-group-functional-verification-study/)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Foster，《Wilson Research Group功能验证研究2020》，2020年11月，第：新闻。[在线]。可用链接：[https://blogs.sw.siemens.com/verificationhorizons/2020/11/05/part-1-the-2020-wilson-research-group-functional-verification-study/](https://blogs.sw.siemens.com/verificationhorizons/2020/11/05/part-1-the-2020-wilson-research-group-functional-verification-study/)'
- en: '[2] P. Coussy and A. Morawiec, *High-level synthesis*.   Springer, 2010, vol. 1.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] P. Coussy 和 A. Morawiec，《*高级综合*》。Springer，2010，第1卷。'
- en: '[3] V. Hamolia and V. Melnyk, “A Survey of Machine Learning Methods and Applications
    in Electronic Design Automation,” in *2021 11th International Conference on Advanced
    Computer Information Technologies (ACIT)*, Sep. 2021, pp. 757–760\. [Online].
    Available: [https://ieeexplore.ieee.org/abstract/document/9548117](https://ieeexplore.ieee.org/abstract/document/9548117)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] V. Hamolia 和 V. Melnyk，"电子设计自动化中的机器学习方法与应用综述"，在 *2021 第11届国际先进计算机信息技术会议
    (ACIT)*，2021年9月，第757–760页。[在线]。可用: [https://ieeexplore.ieee.org/abstract/document/9548117](https://ieeexplore.ieee.org/abstract/document/9548117)'
- en: '[4] H. Pearce, B. Tan, and R. Karri, “DAVE: Deriving Automatically Verilog
    from English,” in *2020 ACM/IEEE 2nd Workshop on Machine Learning for CAD (MLCAD)*,
    Nov. 2020, pp. 27–32.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] H. Pearce, B. Tan, 和 R. Karri，"DAVE: 从英文自动推导 Verilog"，在 *2020 ACM/IEEE
    第二届机器学习与 CAD 研讨会 (MLCAD)*，2020年11月，第27–32页。'
- en: '[5] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-Gavitt,
    and S. Garg, “Benchmarking Large Language Models for Automated Verilog RTL Code
    Generation,” in *2023 Design, Automation & Test in Europe Conference & Exhibition
    (DATE)*, Apr. 2023, pp. 1–6, iSSN: 1558-1101\. [Online]. Available: [https://ieeexplore.ieee.org/abstract/document/10137086](https://ieeexplore.ieee.org/abstract/document/10137086)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-Gavitt,
    和 S. Garg，"大型语言模型在自动化 Verilog RTL 代码生成中的基准测试"，在 *2023 欧洲设计、自动化与测试会议及展览 (DATE)*，2023年4月，第1–6页，iSSN:
    1558-1101。[在线]。可用: [https://ieeexplore.ieee.org/abstract/document/10137086](https://ieeexplore.ieee.org/abstract/document/10137086)'
- en: '[6] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, and
    S. Garg, “VeriGen: A Large Language Model for Verilog Code Generation,” *ACM Transactions
    on Design Automation of Electronic Systems*, Feb. 2024, just Accepted. [Online].
    Available: [https://dl.acm.org/doi/10.1145/3643681](https://dl.acm.org/doi/10.1145/3643681)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, 和 S.
    Garg，"VeriGen: 用于 Verilog 代码生成的大型语言模型"，*ACM 电子系统设计自动化交易*，2024年2月，刚刚接受。[在线]。可用:
    [https://dl.acm.org/doi/10.1145/3643681](https://dl.acm.org/doi/10.1145/3643681)'
- en: '[7] J. Blocklove, S. Garg, R. Karri, and H. Pearce, “Chip-Chat: Challenges
    and Opportunities in Conversational Hardware Design,” in *2023 ACM/IEEE 5th Workshop
    on Machine Learning for CAD (MLCAD)*, Sep. 2023, pp. 1–6\. [Online]. Available:
    [https://ieeexplore.ieee.org/document/10299874](https://ieeexplore.ieee.org/document/10299874)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Blocklove, S. Garg, R. Karri, 和 H. Pearce，"Chip-Chat: 语音硬件设计中的挑战与机遇"，在
    *2023 ACM/IEEE 第五届机器学习与 CAD 研讨会 (MLCAD)*，2023年9月，第1–6页。[在线]。可用: [https://ieeexplore.ieee.org/document/10299874](https://ieeexplore.ieee.org/document/10299874)'
- en: '[8] M. Liu, N. Pinckney, B. Khailany, and H. Ren, “VerilogEval: Evaluating
    Large Language Models for Verilog Code Generation,” Sep. 2023, arXiv:2309.07544
    [cs]. [Online]. Available: [http://arxiv.org/abs/2309.07544](http://arxiv.org/abs/2309.07544)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. Liu, N. Pinckney, B. Khailany, 和 H. Ren，"VerilogEval: 评估大型语言模型在 Verilog
    代码生成中的表现"，2023年9月，arXiv:2309.07544 [cs]。[在线]。可用: [http://arxiv.org/abs/2309.07544](http://arxiv.org/abs/2309.07544)'
- en: '[9] “Tiny Tapeout,” May 2023\. [Online]. Available: [https://tinytapeout.com/](https://tinytapeout.com/)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] "Tiny Tapeout"，2023年5月。[在线]。可用: [https://tinytapeout.com/](https://tinytapeout.com/)'
- en: '[10] A. f. b. review, “Repository for ”Evaluating LLMs for Hardware Design
    and Test”,” Apr. 2024\. [Online]. Available: [https://zenodo.org/records/10947127](https://zenodo.org/records/10947127)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. f. b. review，"用于“评估硬件设计和测试的 LLMs” 的仓库"，2024年4月。[在线]。可用: [https://zenodo.org/records/10947127](https://zenodo.org/records/10947127)'
- en: '[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
    R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
    M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei, “Language Models are Few-Shot Learners,” in *Advances
    in Neural Information Processing Systems*, H. Larochelle, M. Ranzato, R. Hadsell,
    M. F. Balcan, and H. Lin, Eds., vol. 33.   Curran Associates, Inc., 2020, pp.
    1877–1901\. [Online]. Available: [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
    T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,
    E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A.
    Radford, I. Sutskever, 和 D. Amodei，"语言模型是少样本学习者"，在 *神经信息处理系统进展*，H. Larochelle,
    M. Ranzato, R. Hadsell, M. F. Balcan, 和 H. Lin，编辑，第33卷。Curran Associates, Inc.，2020年，第1877–1901页。[在线]。可用:
    [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)'
- en: '[12] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards,
    Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf,
    G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser,
    M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,
    E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang,
    I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike,
    J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati,
    K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and
    W. Zaremba, “Evaluating Large Language Models Trained on Code,” Jul. 2021, arXiv:2107.03374
    [cs]. [Online]. Available: [http://arxiv.org/abs/2107.03374](http://arxiv.org/abs/2107.03374)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H.
    Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    和 W. Zaremba，“Evaluating Large Language Models Trained on Code，”2021年7月，arXiv:2107.03374
    [cs]。[在线]. 可用：[http://arxiv.org/abs/2107.03374](http://arxiv.org/abs/2107.03374)'
- en: '[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is All you Need,” in *Advances in Neural
    Information Processing Systems*, vol. 30.   Curran Associates, Inc., 2017\. [Online].
    Available: [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“Attention is All you Need，”发表于*Advances in Neural
    Information Processing Systems*，第30卷。Curran Associates, Inc.，2017年。[在线]. 可用：[https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)'
- en: '[14] OpenAI, “ChatGPT: Optimizing Language Models for Dialogue,” Nov. 2022\.
    [Online]. Available: [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] OpenAI，“ChatGPT: Optimizing Language Models for Dialogue，”2022年11月。[在线].
    可用：[https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
- en: '[15] S. Pichai, “An important next step on our AI journey,” Feb. 2023\. [Online].
    Available: [https://blog.google/technology/ai/bard-google-ai-search-updates/](https://blog.google/technology/ai/bard-google-ai-search-updates/)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Pichai，“An important next step on our AI journey，”2023年2月。[在线]. 可用：[https://blog.google/technology/ai/bard-google-ai-search-updates/](https://blog.google/technology/ai/bard-google-ai-search-updates/)'
- en: '[16] HuggingFace, “HuggingChat,” May 2023\. [Online]. Available: [https://huggingface.co/chat](https://huggingface.co/chat)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] HuggingFace，“HuggingChat，”2023年5月。[在线]. 可用：[https://huggingface.co/chat](https://huggingface.co/chat)'
- en: '[17] “Welcome to RapidGPT | PrimisAI.” [Online]. Available: [https://primis.ai/docs](https://primis.ai/docs)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] “Welcome to RapidGPT | PrimisAI。”[在线]. 可用：[https://primis.ai/docs](https://primis.ai/docs)'
- en: '[18] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben,
    H. Anand, S. Banerjee, I. Bayraktaroglu, B. Bhaskaran, B. Catanzaro, A. Chaudhuri,
    S. Clay, B. Dally, L. Dang, P. Deshpande, S. Dhodhi, S. Halepete, E. Hill, J. Hu,
    S. Jain, B. Khailany, K. Kunal, X. Li, H. Liu, S. Oberman, S. Omar, S. Pratty,
    J. Raiman, A. Sarkar, Z. Shao, H. Sun, P. P. Suthar, V. Tej, K. Xu, and H. Ren,
    “ChipNeMo: Domain-Adapted LLMs for Chip Design,” Nov. 2023, arXiv:2311.00176 [cs].
    [Online]. Available: [http://arxiv.org/abs/2311.00176](http://arxiv.org/abs/2311.00176)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben,
    H. Anand, S. Banerjee, I. Bayraktaroglu, B. Bhaskaran, B. Catanzaro, A. Chaudhuri,
    S. Clay, B. Dally, L. Dang, P. Deshpande, S. Dhodhi, S. Halepete, E. Hill, J.
    Hu, S. Jain, B. Khailany, K. Kunal, X. Li, H. Liu, S. Oberman, S. Omar, S. Pratty,
    J. Raiman, A. Sarkar, Z. Shao, H. Sun, P. P. Suthar, V. Tej, K. Xu, 和 H. Ren，“ChipNeMo:
    Domain-Adapted LLMs for Chip Design，”2023年11月，arXiv:2311.00176 [cs]。[在线]. 可用：[http://arxiv.org/abs/2311.00176](http://arxiv.org/abs/2311.00176)'
- en: '[19] “Cadence JedAI Generative AI Solution for Chip, System, and Product Design.”
    [Online]. Available: [https://www.cadence.com/en_US/home/solutions/joint-enterprise-data-ai-platform.html](https://www.cadence.com/en_US/home/solutions/joint-enterprise-data-ai-platform.html)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] “Cadence JedAI Generative AI Solution for Chip, System, and Product Design。”[在线].
    可用：[https://www.cadence.com/en_US/home/solutions/joint-enterprise-data-ai-platform.html](https://www.cadence.com/en_US/home/solutions/joint-enterprise-data-ai-platform.html)'
- en: '[20] “Redefining Chip Design with AI-Powered EDA Tools | Synopsys.ai | Synopsys
    Blog.” [Online]. Available: [https://www.synopsys.com/blogs/chip-design/synopsys-ai-eda-tools.html](https://www.synopsys.com/blogs/chip-design/synopsys-ai-eda-tools.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] “通过 AI 驱动的 EDA 工具重新定义芯片设计 | Synopsys.ai | Synopsys 博客。” [在线]. 可用: [https://www.synopsys.com/blogs/chip-design/synopsys-ai-eda-tools.html](https://www.synopsys.com/blogs/chip-design/synopsys-ai-eda-tools.html)'
- en: '[21] “Problem sets - HDLBits.” [Online]. Available: [https://hdlbits.01xz.net/wiki/Problem_sets](https://hdlbits.01xz.net/wiki/Problem_sets)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] “问题集 - HDLBits。” [在线]. 可用: [https://hdlbits.01xz.net/wiki/Problem_sets](https://hdlbits.01xz.net/wiki/Problem_sets)'
- en: '[22] R. Kande, H. Pearce, B. Tan, B. Dolan-Gavitt, S. Thakur, R. Karri, and
    J. Rajendran, “(Security) Assertions by Large Language Models,” *IEEE Transactions
    on Information Forensics and Security*, pp. 1–1, 2024, conference Name: IEEE Transactions
    on Information Forensics and Security. [Online]. Available: [https://ieeexplore.ieee.org/abstract/document/10458667](https://ieeexplore.ieee.org/abstract/document/10458667)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] R. Kande, H. Pearce, B. Tan, B. Dolan-Gavitt, S. Thakur, R. Karri, 和 J.
    Rajendran，“（安全）大型语言模型中的断言，” *IEEE 信息取证与安全期刊*，第1–1页，2024年，会议名称: IEEE 信息取证与安全期刊。[在线].
    可用: [https://ieeexplore.ieee.org/abstract/document/10458667](https://ieeexplore.ieee.org/abstract/document/10458667)'
- en: '[23] B. Ahmad, B. Tan, R. Karri, and H. Pearce, “FLAG: Finding Line Anomalies
    (in code) with Generative AI,” Jun. 2023, arXiv:2306.12643 [cs]. [Online]. Available:
    [http://arxiv.org/abs/2306.12643](http://arxiv.org/abs/2306.12643)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Ahmad, B. Tan, R. Karri, 和 H. Pearce，“FLAG: 使用生成性 AI 查找代码中的行异常，”2023年6月，arXiv:2306.12643
    [cs]。[在线]. 可用: [http://arxiv.org/abs/2306.12643](http://arxiv.org/abs/2306.12643)'
- en: '[24] “Icarus Verilog.” [Online]. Available: [http://iverilog.icarus.com/home](http://iverilog.icarus.com/home)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] “Icarus Verilog。” [在线]. 可用: [http://iverilog.icarus.com/home](http://iverilog.icarus.com/home)'
- en: '[25] “OpenLane,” May 2023, original-date: 2020-07-20T19:35:02Z. [Online]. Available:
    [https://github.com/The-OpenROAD-Project/OpenLane](https://github.com/The-OpenROAD-Project/OpenLane)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] “OpenLane，”2023年5月，原始日期: 2020-07-20T19:35:02Z。[在线]. 可用: [https://github.com/The-OpenROAD-Project/OpenLane](https://github.com/The-OpenROAD-Project/OpenLane)'
- en: '[26] OpenAI, “GPT-4,” Mar. 2023\. [Online]. Available: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] OpenAI，“GPT-4，”2023年3月。[在线]. 可用: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
- en: '[27] ——, “Introducing ChatGPT,” Nov. 2022\. [Online]. Available: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] ——，“介绍 ChatGPT，”2022年11月。[在线]. 可用: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)'
- en: Figures A1, A2, and A3 give additional examples of LLMs responding to the initial
    prompt for an 8-bit shift register. Figure A2 highlights a repeated error made
    by Bard in generating a proper module definition.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A1、A2 和 A3 给出了 LLMs 对初始 8 位移位寄存器提示的额外示例。图 A2 突出了 Bard 在生成适当的模块定义时重复的错误。
- en: 1module  shift_register(2  input  clk,3  input  reset_n,4  input  data,5  input  shift_enable,6  output  reg  [7:0]  q7);89always  @(posedge  clk,  negedge  reset_n)  begin10  if  (!reset_n)  begin11  q  <=  8’b0;12  end  else  if  (shift_enable)  begin13  q  <=  {q[6:0],  data};14  end15end1617endmodule
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`module shift_register(input clk, input reset_n, input data, input shift_enable,
    output reg [7:0] q); always @(posedge clk, negedge reset_n) begin if (!reset_n)
    begin q <= 8''b0; end else if (shift_enable) begin q <= {q[6:0], data}; end end
    endmodule`'
- en: 'Figure 1: 8-bit shift register attempt from ChatGPT-3.5.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：ChatGPT-3.5 尝试的 8 位移位寄存器。
- en: 1module  shift_register  (2  input  clk,3  input  rst,4  input  [7:0]  data,5  input  shift_enable,6  output  reg  [7:0]  data_out7);89  always  @(posedge  clk  or  posedge  rst)  begin10  if  (rst)  begin11  data_out  <=  8’b0;12  end  else  begin13  if  (shift_enable)  begin14  data_out  <=  {data_out[6:0],  data};15  end16  end17  end1819endmodule
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`module shift_register (input clk, input rst, input [7:0] data, input shift_enable,
    output reg [7:0] data_out); always @(posedge clk or posedge rst) begin if (rst)
    begin data_out <= 8''b0; end else begin if (shift_enable) begin data_out <= {data_out[6:0],
    data}; end end end endmodule`'
- en: 'Figure 2: 8-bit shift register attempt by Bard. Input on line 4 is too wide.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Bard 尝试的 8 位移位寄存器。第4行的输入太宽。
- en: '1module  ShiftRegister(input  clk,  input  rst_n,  input  data_in,  input  shft_en,  output  reg  [7:0]  q);23...  //it  generates  a  number  of  parameters  here  which  are  not  referred  to45assign  data_out  =  ~data_in  &  shft_en  &  rst_n  ?  1’b0  :  (q[7:1]  <<  1)  |  {1’b0,  data_in};6always  @(*)  begin7  #  /*  x’FF  */  CLK_RQSTDCLK  *;8  #  /reset_n  R0  R0[(int*)&(_CTL--)]  <=  32’h8000_0000;9  if  (rst_n  &&  !shft_en)  begin10  $setuphold  (posedge  reset_n  ,  negedge  active  )  ;11  $display("***  RESET  ***\n");12  ‘#addr  ‘addr:‘addr[(int*)  &POS]‘pos  ?(int)  (  ((‘addr‘:addr[_ADDR]+  $_INTRTVALU)/2):1’b1;‘addr:(addr<>‘addr)‘pos  ,"Full  scan  done."));‘cout!="Division  by  zero!"  &  ’@(  !_LUT)  &&  (‘grandpa’)?(sprintf(‘snip‘,strlen(&_TI),($half*‘cout>&1)’hash  ()):  snprintf(!reserve2((const  char*)(type(*‘‘cout)strlen‘buf++->_GBIT]))))  oe=tmp2  &&  tmp3  ?  (__flash)((void*)realloc($(int*)(idx=(*(u*‘ptr‘)|(~(address>13  removecomments  Tr1ESran  diff  gred$\elleoformatfloat  format----------------------‘charTmpLim}  Len  rollover  Leng  len  Len  Non  Rollovr  OptimizationCenter}\n  resident  AsymptoticAspect}\np  ResetOpt  \All  %par  ID  ==  PCI0  SPC  ReconfigurationFeature  supported  }  IF  PROPRIETARY  ASPECT=‘port!\Plug14  ...‘‘‘'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '1module  ShiftRegister(input  clk,  input  rst_n,  input  data_in,  input  shft_en,  output  reg  [7:0]  q);23...  //它在这里生成了一些没有被引用的参数45assign  data_out  =  ~data_in  &  shft_en  &  rst_n  ?  1’b0  :  (q[7:1]  <<  1)  |  {1’b0,  data_in};6always  @(*)  begin7  #  /*  x’FF  */  CLK_RQSTDCLK  *;8  #  /reset_n  R0  R0[(int*)&(_CTL--)]  <=  32’h8000_0000;9  如果  (rst_n  &&  !shft_en)  开始10  $setuphold  (posedge  reset_n  ,  negedge  active  )  ;11  $display("***  RESET  ***\n");12  ‘#addr  ‘addr:‘addr[(int*)  &POS]‘pos  ?(int)  (  ((‘addr‘:addr[_ADDR]+  $_INTRTVALU)/2):1’b1;‘addr:(addr<>‘addr)‘pos  ,"完整扫描完成。"));‘cout!="除零错误!"  &  ’@(  !_LUT)  &&  (‘grandpa’)?(sprintf(‘snip‘,strlen(&_TI),($half*‘cout>&1)’hash  ()):  snprintf(!reserve2((const  char*)(type(*‘‘cout)strlen‘buf++->_GBIT]))))  oe=tmp2  &&  tmp3  ?  (__flash)((void*)realloc($(int*)(idx=(*(u*‘ptr‘)|(~(address>13  removecomments  Tr1ESran  diff  gred$\elleoformatfloat  format----------------------‘charTmpLim}  Len  rollover  Leng  len  Len  Non  Rollovr  OptimizationCenter}\n  resident  AsymptoticAspect}\np  ResetOpt  \All  %par  ID  ==  PCI0  SPC  ReconfigurationFeature  supported  }  IF  PROPRIETARY  ASPECT=‘port!\Plug14  ...‘‘‘'
- en: 'Figure 3: 8-bit ‘shift register’ by HuggingChat (truncated, formatted).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：8 位‘移位寄存器’由 HuggingChat 提供（已截断，已格式化）。
- en: Figure A4 shows the output of the sequence generator benchmark running on the
    Tiny Tapeout 3 board. This was captured using a DreamSourceLabs USB Logic Analyzer
    in the DSView software. The hexadecimal decoding at the top of the capture shows
    the correct generation of the first five elements of the output sequence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A4 显示了在 Tiny Tapeout 3 开发板上运行的序列生成器基准测试的输出。这是通过 DreamSourceLabs USB 逻辑分析仪和
    DSView 软件捕获的。捕获顶部的十六进制解码显示了输出序列前五个元素的正确生成。
- en: '![Refer to caption](img/fc79d8691d80f51f57ef6be46c49b2a0.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fc79d8691d80f51f57ef6be46c49b2a0.png)'
- en: 'Figure 4: The output of the sequence generator benchmark running on the Tiny
    Tapeout 3 board. This was captured using a DreamSourceLabs USB Logic Analyzer
    in the DSView software. The hexadecimal decoding at the top of the capture shows
    the correct generation of the first five elements of the output sequence.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在 Tiny Tapeout 3 开发板上运行的序列生成器基准测试的输出。这是通过 DreamSourceLabs USB 逻辑分析仪和 DSView
    软件捕获的。捕获顶部的十六进制解码显示了输出序列前五个元素的正确生成。
