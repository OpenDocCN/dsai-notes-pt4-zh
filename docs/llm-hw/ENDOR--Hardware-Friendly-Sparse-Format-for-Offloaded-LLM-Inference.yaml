- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:51:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:51:26'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ENDOR: 硬件友好的稀疏格式用于离线LLM推理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11674](https://ar5iv.labs.arxiv.org/html/2406.11674)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11674](https://ar5iv.labs.arxiv.org/html/2406.11674)
- en: Donghyeon Joo¹, Ramyad Hadidi², Soheil Feizi¹, Bahar Asgari¹ ¹Department of
    Computer Science, University of Maryland, ²Rain AI
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Donghyeon Joo¹, Ramyad Hadidi², Soheil Feizi¹, Bahar Asgari¹ ¹马里兰大学计算机科学系, ²Rain
    AI
- en: '{dhjoo98,bahar}@umd.edu, ramyad@rain.ai, sfeizi@cs.umd.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '{dhjoo98,bahar}@umd.edu, ramyad@rain.ai, sfeizi@cs.umd.edu'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The increasing size of large language models (LLMs) challenges their usage on
    resource-constrained platforms. For example, memory on modern GPUs is insufficient
    to hold LLMs that are hundreds of Gigabytes in size. Offloading is a popular method
    to escape this constraint by storing weights of an LLM model to host CPU memory
    and SSD, then loading each weight to GPU before every use. In our case study of
    offloaded inference, we found that due to the low bandwidth between storage devices
    and GPU, the latency of transferring large model weights from its offloaded location
    to GPU memory becomes the critical bottleneck with actual compute taking nearly
    0% of runtime. To effectively reduce the weight transfer latency, we propose a
    novel sparse format that compresses the unstructured sparse pattern of pruned
    LLM weights to non-zero values with high compression ratio and low decompression
    overhead. Endor achieves this by expressing the positions of non-zero elements
    with a bitmap. Compared to offloaded inference using the popular Huggingface Accelerate,
    applying Endor accelerates OPT-66B by 1.70$\times$ speedup on Llama2-70B.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的规模不断扩大，给资源受限的平台带来了挑战。例如，现代GPU的内存不足以容纳几百GB的大型语言模型。离线处理是一种流行的方法，通过将LLM模型的权重存储到主机CPU内存和SSD上，然后在每次使用前将每个权重加载到GPU上，从而避免了这一限制。在我们的离线推理案例研究中，我们发现由于存储设备与GPU之间的带宽较低，从离线位置传输大型模型权重到GPU内存的延迟成为了关键瓶颈，实际计算几乎占据了0%的运行时间。为了有效减少权重传输延迟，我们提出了一种新颖的稀疏格式，通过高压缩比和低解压开销将剪枝后的LLM权重的非结构化稀疏模式压缩为非零值。Endor通过用位图表示非零元素的位置来实现这一点。与使用流行的Huggingface
    Accelerate进行的离线推理相比，应用Endor使OPT-66B在Llama2-70B上加速了1.70$\times$。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have seen an exponential increase in size to achieve
    meta-human abilities such as complex reasoning and zero-shot tasks. However, the
    immense size of LLMs, such as 70-billion-parameter Llama [[21](#bib.bibx21)] and
    175-billion-parameter GPT-3 [[4](#bib.bibx4)], pose challenges to the computing
    platform. LLM inference is often limited by the GPU memory when it cannot hold
    the entire model. LLM practitioners often face a peculiar situation where the
    ‘GPU-must-hold-entire-model’ rule limits inference even with enough computing
    resources. This forces them to employ more GPUs, which is costly and illogical
    as already-sufficient compute resources are added. Model offloading presents a
    promising alternative. By partitioning a model into small sections and saving
    them to host CPU memory and storage device, computing platforms can overcome the
    limitations of GPU memory. Now comfortably fitting on GPU memory, these partitions
    are sequentially loaded during inference time. This approach, supported by prominent
    LLM frameworks such as Hugging Face Accelerate [[14](#bib.bibx14)] and Microsoft
    Deepspeed [[2](#bib.bibx2)], holds great potential for LLM inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的规模已经呈指数增长，以实现复杂推理和零-shot任务等超人类能力。然而，如70亿参数的Llama [[21](#bib.bibx21)]和1750亿参数的GPT-3
    [[4](#bib.bibx4)]等LLMs的巨大规模对计算平台提出了挑战。当GPU内存无法容纳整个模型时，LLM推理往往受到限制。LLM从业者常常面临一个特殊情况，即‘GPU必须容纳整个模型’的规则即使在计算资源充足的情况下也限制了推理。这迫使他们使用更多的GPU，这样做既昂贵又不合理，因为已经有足够的计算资源。模型离线处理提供了一种有前途的替代方案。通过将模型划分为小部分并将其保存到主机CPU内存和存储设备中，计算平台可以克服GPU内存的限制。现在，这些分区在GPU内存中舒适地适配，并在推理时顺序加载。这种方法得到Hugging
    Face Accelerate [[14](#bib.bibx14)]和Microsoft Deepspeed [[2](#bib.bibx2)]等著名LLM框架的支持，具有很大的潜力。
- en: 'In this paper, upon analyzing offloaded inference (Section [3](#S3 "3 Case
    Study: Offloading OPT-66B ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded
    LLM Inference")), we identify the critical bottleneck as the weight transfer latency
    of offloaded model weights. This overhead is significantly larger than the time
    spent in computation, resulting in a much longer end-to-end latency than the on-GPU
    inference. Model quantization has been a popular method in LLM deployment, decreasing
    the memory footprint by lowering the precision of each value. Alternatively, model
    pruning can achieve a similar level of model size reduction by removing relatively
    unimportant weight values with minor performance degradation. However, pruning
    methods such as SparseGPT [[11](#bib.bibx11)], which yield unstructured sparse
    pattern in weights do not reduce the size of stored weights due to the difficulty
    in expressing the unstructured distribution of non-zeros. When pruned, weights
    are stored as-is, it does not contribute to reducing weight transfer latency.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，通过分析卸载推理（第[3](#S3 "3 Case Study: Offloading OPT-66B ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")节），我们确定了关键瓶颈为卸载模型权重的传输延迟。这一开销显著大于计算时间，导致端到端的延迟远长于GPU上的推理。模型量化已经成为LLM部署中的一种流行方法，通过降低每个值的精度来减少内存占用。或者，模型剪枝可以通过去除相对不重要的权重值来实现类似的模型大小减少，尽管会有轻微的性能下降。然而，像SparseGPT[[11](#bib.bibx11)]这样的剪枝方法，会在权重中产生无结构的稀疏模式，但由于难以表达非零值的无结构分布，因此不会减少存储权重的大小。剪枝后，权重按原样存储，这对减少权重传输延迟没有贡献。'
- en: 'To address the bottleneck of offloaded LLM inference with minimal model performance
    degradation, we propose Endor (Section [4](#S4 "4 Endor Sparse Format and Pruning
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference")), a novel
    sparse format to efficiently compress the pruned unstructured sparse LLM weights.
    Endor efficiently compresses unstructured sparse weight matrices to reduce the
    weight transfer latency in transfer between storage device and host CPU memory,
    and between host CPU memory and GPU. To achieve high compression rate, Endor sparse
    format stores only the non-zero elements of the sparse LLM weights and use a bitmap
    to express the position of these elements. Decompression of Endor sparse format
    is highly parallelizable and shows minimal decompression overhead. Endor achieves
    up to 2.37$\times$ speedup compared to the dense offloaded inference using Huggingface
    Accelerate [[14](#bib.bibx14)] (Section [5](#S5 "5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")). As Endor preserves all non-zero
    values of pruned weights, it can be jointly applied with other methods that accelerate
    offloaded inference, such as quantization and activation sparsity for faster offloaded
    inference, and batch scheduling for higher throughput.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决卸载LLM推理的瓶颈，同时最小化模型性能下降，我们提出了Endor（第[4](#S4 "4 Endor Sparse Format and Pruning
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference")节），一种新颖的稀疏格式，用于高效压缩剪枝后的无结构稀疏LLM权重。Endor高效压缩无结构稀疏权重矩阵，以减少存储设备与主机CPU内存之间、主机CPU内存与GPU之间的权重传输延迟。为了实现高压缩率，Endor稀疏格式仅存储稀疏LLM权重中的非零元素，并使用位图表示这些元素的位置。Endor稀疏格式的解压缩高度并行化，且解压缩开销最小。与使用Huggingface
    Accelerate[[14](#bib.bibx14)]（第[5](#S5 "5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")节）的密集卸载推理相比，Endor实现了高达2.37$\times$的加速。由于Endor保留了剪枝后权重的所有非零值，它可以与其他加速卸载推理的方法联合应用，如量化和激活稀疏，以实现更快的卸载推理，以及批处理调度以提高吞吐量。'
- en: 2 Related Works
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: This section first reviews different weight size reduction methods we considered
    and then reviews previous studies that target offloaded inference.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先回顾了我们考虑的不同权重大小减少方法，然后回顾了针对卸载推理的先前研究。
- en: 2.1 Model Pruning, Quantization, and Compression Format
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 模型剪枝、量化和压缩格式
- en: Several studies have proposed model pruning [[13](#bib.bibx13), [10](#bib.bibx10),
    [3](#bib.bibx3), [15](#bib.bibx15), [22](#bib.bibx22)] that replaces insignificant
    elements in a weight tensor with zeros. Applying pruning to LLMs, SparseGPT [[11](#bib.bibx11)]
    employs an iterative pruning process with weight updates to decide the values
    to prune. Wanda [[20](#bib.bibx20)] uses a magnitude-based sorting of products
    between weight values and sample dataset inputs. In addition for both methods
    to prune in unstructured pattern, they can also enforce N:M structured pattern,
    where N elements remain in M consecutive elements, for computational speedup on
    supported GPUs [[18](#bib.bibx18)]. However, enforcing a structured pattern comes
    at the price of model accuracy degradation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究提出了模型剪枝[[13](#bib.bibx13)、[10](#bib.bibx10)、[3](#bib.bibx3)、[15](#bib.bibx15)、[22](#bib.bibx22)]，将权重张量中的不重要元素替换为零。对LLM应用剪枝，SparseGPT[[11](#bib.bibx11)]采用了带权重更新的迭代剪枝过程来决定需要剪枝的值。Wanda[[20](#bib.bibx20)]使用基于权重值与样本数据集输入之间乘积的幅度排序。此外，这两种方法不仅能以无结构模式进行剪枝，还能强制N:M结构模式，其中N个元素保留在M个连续元素中，以加速支持的GPU计算[[18](#bib.bibx18)]。然而，强制使用结构模式会导致模型精度下降。
- en: The same limitation of accuracy degradation apply when considering a structured
    sparse pattern to reduce the memory overhead of pruned weights. Alternatively,
    compressing the unstructured sparse pattern is a challenging task due to the random
    distribution of non-zero elements. If pruned weights are compressed using common
    sparsity formats such as compressed sparse row (CSR) fromat, the addition indexing
    data makes up for the reduction of elements. For example, the size of indexing
    data for a 50% sparse weight is equal to the size of zero elements, a compression
    rate of 0%. This forces pruned weights to be stored as whole dense matrices, with
    no weight size reduction despite the reduction in the number of weight elements.
    In our paper, we use weights pruned to 50% sparsity with the methods of Wanda [[20](#bib.bibx20)]
    and SparseGPT [[11](#bib.bibx11)], resulting in an unstructured distribution of
    non-zero values. Endor is applied on the pruned weights to reduce weight size
    during offloaded inference.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑使用结构化稀疏模式来减少剪枝权重的内存开销时，同样的精度下降限制适用。另一方面，由于非零元素的随机分布，压缩无结构稀疏模式是一项具有挑战性的任务。如果剪枝权重使用常见的稀疏格式如压缩稀疏行（CSR）格式进行压缩，附加的索引数据弥补了元素减少带来的减小。例如，50%稀疏权重的索引数据大小等于零元素的大小，压缩率为0%。这迫使剪枝权重被存储为完整的稠密矩阵，尽管权重元素数量减少，但权重大小没有减少。在我们的论文中，我们使用了Wanda[[20](#bib.bibx20)]和SparseGPT[[11](#bib.bibx11)]剪枝到50%稀疏度的方法，导致非零值的无结构分布。Endor应用于剪枝权重，以在离线推理过程中减少权重大小。
- en: 'Model quantization [[5](#bib.bibx5), [6](#bib.bibx6), [9](#bib.bibx9), [7](#bib.bibx7),
    [24](#bib.bibx24)] is an alternative solution that reduces both the model weight
    size and the amount of computation by reducing the precision of each value. For
    LLMs, SmoothQuant [[23](#bib.bibx23)] performs INT8 quantization of weights and
    activations to achieve memory reduction and speedup. GPTQ [[12](#bib.bibx12)]
    uses a layer-wise quantization to achieve more reduction with minimal accuracy
    degradation. SpQR [[8](#bib.bibx8)] retains higher precision outlier values in
    a mixed-precision scheme to minimize accuracy degradation. We design Endor sparse
    format to be compatible with LLM quantization, as evaluated in Section [5.4](#S5.SS4
    "5.4 Joint Application with Quantization ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '模型量化[[5](#bib.bibx5)、[6](#bib.bibx6)、[9](#bib.bibx9)、[7](#bib.bibx7)、[24](#bib.bibx24)]是一种替代方案，通过降低每个值的精度来减少模型权重大小和计算量。对于大型语言模型（LLMs），SmoothQuant[[23](#bib.bibx23)]对权重和激活进行INT8量化，以实现内存减少和加速。GPTQ[[12](#bib.bibx12)]使用逐层量化以在最小精度下降的情况下实现更多的减少。SpQR[[8](#bib.bibx8)]在混合精度方案中保留更高精度的异常值，以最小化精度下降。我们设计了Endor稀疏格式，以兼容LLM量化，如在第[5.4节](#S5.SS4
    "5.4 Joint Application with Quantization ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")所评估。'
- en: 'State of the art compression formats such as LZ4 [[16](#bib.bibx16)] and Meta
    Zstandard (ZSTD) [[26](#bib.bibx26)] encapsulate decades of effort in computing
    compression formats, which exploit the repetition of bit-wise patterns in raw
    data to achieve size reduction¹¹1We distinguish compression with compression format,
    which exploits bit-wise patterns, and compression with sparse format, such as
    CSR and Endor, that exploits the non-zeros of sparse matrices. Unless stated explicitly,
    ‘compression’ refers to ‘compression with sparse format.’. In Section [5.1](#S5.SS1
    "5.1 Compressed Offloaded Inference ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference"), we evaluate the usage of ZSTD to
    compress model weights in offloaded inference.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '最先进的压缩格式，如 LZ4 [[16](#bib.bibx16)] 和 Meta Zstandard (ZSTD) [[26](#bib.bibx26)]，凝聚了数十年在计算压缩格式方面的努力，它们利用原始数据中的位模式重复性来实现尺寸减少¹¹1我们将压缩分为利用位模式的压缩格式和利用稀疏矩阵非零的稀疏格式，如
    CSR 和 Endor。除非明确说明，“压缩”指的是“稀疏格式压缩”。在第[5.1](#S5.SS1 "5.1 Compressed Offloaded Inference
    ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference")节中，我们评估了使用
    ZSTD 压缩卸载推理中的模型权重。'
- en: 2.2 Offloaded Inference Optimization
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 卸载推理优化
- en: Alizadeh et al. [[1](#bib.bibx1)] exploits the activation sparsity of the ReLU
    function to load weights from storage selectively. They use a small predictor
    that accurately predicts the weight rows and columns that yield non-zero ReLU
    activation and load that subset of weights, leading to a 95% reduction of weight
    transfer. However, their target is limited to the fully connected layers of an
    LLM, which is, on average, 66% of the entire model weight transfer. Also, only
    a subset of prominent LLMs such as OPT and Falcon that employs ReLU activation
    benefit from this approach, excluding LLMs such as Llama and GPT4. Sheng et al.
    [[19](#bib.bibx19)] employs efficient computation scheduling, tensor placement,
    and KV cache compression to increase the throughput of offloaded inference. They
    achieve this by establishing a search space of possible configurations given a
    batch input of sequences. However, their work prioritizes increasing the throughput
    of batch inputs, which is suitable for a server-scale LLM serving. We aim to reduce
    the offloaded inference latency of a single batch input sequence, which is suitable
    for both server-scale and edge-scale LLM serving.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Alizadeh 等人 [[1](#bib.bibx1)] 利用 ReLU 函数的激活稀疏性来选择性地从存储中加载权重。他们使用一个小型预测器，准确预测产生非零
    ReLU 激活的权重行和列，并加载该子集的权重，从而减少了 95% 的权重传输。然而，他们的目标仅限于 LLM 的全连接层，这平均占整个模型权重传输的 66%。此外，只有如
    OPT 和 Falcon 这样的使用 ReLU 激活的显著 LLM 从这种方法中受益，排除了 Llama 和 GPT4 等 LLM。Sheng 等人 [[19](#bib.bibx19)]
    采用高效的计算调度、张量布置和 KV 缓存压缩来提高卸载推理的吞吐量。他们通过在给定的批次输入序列上建立可能配置的搜索空间来实现这一点。然而，他们的工作优先考虑增加批量输入的吞吐量，这适合服务器规模的
    LLM 服务。我们的目标是减少单批次输入序列的卸载推理延迟，这适用于服务器规模和边缘规模的 LLM 服务。
- en: '3 Case Study: Offloading OPT-66B'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 案例研究：卸载 OPT-66B
- en: 3.1 Workload Specification
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 工作负载规格
- en: 'Our workload is OPT-66B [[25](#bib.bibx25)] in float16 precision. OPT-66B consists
    of 64 OPT layers. Each OPT layer is divided into an attention sub-layer and a
    fully-connect sub-layer. Attention sub-layer contains four linear operations:
    key projection, query projection, value projection, and output projection with
    weight matrix of size $9,216\times 9,216$. Linear operations are the focus of
    our work, as their weight parameters are significantly larger than operations
    with smaller parameters (layer normalization) and operations with no weight parameter
    (matrix multiplication between attention score and value). With emphasis on offloaded
    inference latency, we use single batch text generation inference for all measurement
    and evaluation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作负载是 float16 精度的 OPT-66B [[25](#bib.bibx25)]。OPT-66B 由 64 层 OPT 组成。每一层 OPT
    分为注意力子层和全连接子层。注意力子层包含四个线性操作：键投影、查询投影、值投影和输出投影，权重矩阵的大小为 $9,216\times 9,216$。线性操作是我们工作的重点，因为它们的权重参数显著大于具有较小参数（层归一化）和没有权重参数（注意力分数与值之间的矩阵乘法）的操作。鉴于对卸载推理延迟的重视，我们对所有测量和评估使用单批次文本生成推理。
- en: 3.2 System Specification
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 系统规格
- en: 'Fitting our workload on GPU would require 132GB of VRAM, far surpassing the
    memory size of commercial GPUs. We use Hugging Face Accelerate to perform offloaded
    inference on a CPU-GPU heterogeneous platform, which consists of an RTX 4080 GPU
    with 16GB VRAM, 64GB host DRAM, and SK Hynix P31 NVMe 2TB SSD. Figure [1](#S3.F1
    "Figure 1 ‣ 3.2 System Specification ‣ 3 Case Study: Offloading OPT-66B ‣ ENDOR:
    Hardware-Friendly Sparse Format for Offloaded LLM Inference") illustrates the
    measured bandwidth between these devices. Bandwidth between storage device and
    host CPU DRAM is significantly smaller compared to the bandwidth between host
    CPU DRAM and the GPU.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的工作负载适配到 GPU 上需要 132GB 的 VRAM，这远远超过了商业 GPU 的内存大小。我们使用 Hugging Face Accelerate
    在一个 CPU-GPU 异构平台上进行卸载推理，该平台包括一个具有 16GB VRAM 的 RTX 4080 GPU、64GB 主机 DRAM 和 SK Hynix
    P31 NVMe 2TB SSD。图 [1](#S3.F1 "图 1 ‣ 3.2 系统规格 ‣ 3 案例研究：卸载 OPT-66B ‣ ENDOR：适合硬件的稀疏格式用于卸载
    LLM 推理") 说明了这些设备之间的带宽测量。存储设备和主机 CPU DRAM 之间的带宽显著小于主机 CPU DRAM 和 GPU 之间的带宽。
- en: '![Refer to caption](img/ed5ed691536790919efd95ee5fe01b6e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ed5ed691536790919efd95ee5fe01b6e.png)'
- en: 'Figure 1: Memory/SSD configuration including capacity and measured bandwidth.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：包括容量和测量带宽的内存/SSD 配置。
- en: 3.3 Offloading Setup
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 卸载设置
- en: Offloading involves mapping each OPT layer to GPU memory, host CPU memory, or
    storage device. OPT layer, which is the unit of offloading, refers to a group
    of operations comprised of attention, layer normalization, and fully-connected
    operations. GPU-mapped-layer weights are directly computed. CPU-mapped-layer weights
    are transferred to GPU memory before computation, while storage-mapped-layer weights
    are loaded to CPU and then to GPU before computation. To study offloaded inference,
    we map decoder layers 0 to 4 to GPU memory, layers 5 to 12 to CPU memory, and
    layers 13 to 63 to the SSD. Layer mapping prioritizes populating locations closer
    to GPU while leaving enough memory for computation and intermediate operands.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 卸载涉及将每个 OPT 层映射到 GPU 内存、主机 CPU 内存或存储设备。OPT 层，即卸载的单元，指的是由注意力、层归一化和全连接操作组成的一组操作。GPU
    映射层的权重直接计算。CPU 映射层的权重在计算之前转移到 GPU 内存，而存储映射层的权重在计算之前先加载到 CPU，然后再加载到 GPU。为了研究卸载推理，我们将解码器层
    0 到 4 映射到 GPU 内存，将层 5 到 12 映射到 CPU 内存，将层 13 到 63 映射到 SSD。层映射优先考虑填充靠近 GPU 的位置，同时为计算和中间操作数保留足够的内存。
- en: '![Refer to caption](img/1e49ca4ca287dd5f999a34ab397306ec.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1e49ca4ca287dd5f999a34ab397306ec.png)'
- en: 'Figure 2: Execution time comparison of offloaded OPT layers.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：卸载的 OPT 层的执行时间比较。
- en: 3.4 Offloaded Inference Analysis
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 卸载推理分析
- en: 'Out of a single pass through OPT-66B which took 54s, Figure [2](#S3.F2 "Figure
    2 ‣ 3.3 Offloading Setup ‣ 3 Case Study: Offloading OPT-66B ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference") compares the execution time of a single
    GPU-mapped, CPU-mapped, and SSD-mapped layer. The figure illustrates that the
    overhead of weight transfer increases significantly as the offloaded region is
    further from the GPU. The SSD-mapped layers have the slowest latency of 1 second,
    80% of which is spent loading weight from storage to CPU, and 20% is spent in
    DRAM to GPU loading, resulting in nearly 0% of time spent in actual computation.
    For the the CPU-mapped layer, weight only has to be transferred from CPU DRAM
    to GPU VRAM before computation, and thus does not include SSD to CPU transfer
    overhead. For the GPU-mapped layer, weight can be used immediately, thus does
    not include any weight transfer overhead.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过 OPT-66B 的一次处理时间为 54 秒的情况下，图 [2](#S3.F2 "图 2 ‣ 3.3 卸载设置 ‣ 3 案例研究：卸载 OPT-66B
    ‣ ENDOR：适合硬件的稀疏格式用于卸载 LLM 推理") 比较了单个 GPU 映射、CPU 映射和 SSD 映射层的执行时间。该图说明了随着卸载区域离
    GPU 越远，权重传输的开销显著增加。SSD 映射层的延迟最长，为 1 秒，其中 80% 的时间用于将权重从存储加载到 CPU，20% 的时间用于从 DRAM
    加载到 GPU，几乎没有时间用于实际计算。对于 CPU 映射层，权重只需从 CPU DRAM 传输到 GPU VRAM，然后进行计算，因此不包括 SSD 到
    CPU 的传输开销。对于 GPU 映射层，权重可以立即使用，因此不包括任何权重传输开销。
- en: 'The impact of SSD-CPU and CPU-GPU transfer times is in particular crucial in
    the LLM generation task, which involves sequential computations of all layers
    for every token that requires transferring weights from SSD. To explore this further,
    Figure [3](#S3.F3 "Figure 3 ‣ 3.4 Offloaded Inference Analysis ‣ 3 Case Study:
    Offloading OPT-66B ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM
    Inference") illustrates the timeline for an SSD-mapped OPT layer, showing how
    weight transfer dominates the execution time. Such a proportionate weight transfer
    latency is a result of the small CPU-SSD bandwidth shown in Figure [1](#S3.F1
    "Figure 1 ‣ 3.2 System Specification ‣ 3 Case Study: Offloading OPT-66B ‣ ENDOR:
    Hardware-Friendly Sparse Format for Offloaded LLM Inference"). Some offloading
    frameworks, such as Deepspeed [[2](#bib.bibx2)], overlap computation with loading.
    However, this has little effect as computation takes only a minute portion of
    execution time, rendering overlapping ineffective.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'SSD-CPU 和 CPU-GPU 传输时间在 LLM 生成任务中尤其关键，这涉及到每个令牌的所有层的顺序计算，这需要从 SSD 传输权重。为了进一步探讨这一点，图[3](#S3.F3
    "Figure 3 ‣ 3.4 Offloaded Inference Analysis ‣ 3 Case Study: Offloading OPT-66B
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference")展示了一个 SSD
    映射的 OPT 层的时间线，显示了权重传输如何主导执行时间。这样的权重传输延迟比例是图[1](#S3.F1 "Figure 1 ‣ 3.2 System Specification
    ‣ 3 Case Study: Offloading OPT-66B ‣ ENDOR: Hardware-Friendly Sparse Format for
    Offloaded LLM Inference")中显示的小 CPU-SSD 带宽的结果。一些卸载框架，如 Deepspeed [[2](#bib.bibx2)]，将计算与加载重叠。然而，由于计算只占执行时间的一小部分，重叠效果不显著。'
- en: '![Refer to caption](img/6e682e6cc8fc55890faef08def2777c1.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e682e6cc8fc55890faef08def2777c1.png)'
- en: 'Figure 3: Timeline of an SSD-mapped OPT layer.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：SSD 映射的 OPT 层的时间线。
- en: '3.5 Overcoming Bottleneck: Unstructured Sparsity Compression'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 克服瓶颈：无结构稀疏压缩
- en: 'Our two candidates for reducing the identified bottleneck weight transfer latency
    were quantization and pruning, which can reduce the size of weight parameters.
    In theory, reducing weight size would lead to a proportionate reduction of weight
    transfer latency. However, weight pruned in an unstructured pattern must be stored
    as a whole matrix, and the reduction of weight elements is not reflected in the
    weight transfer latency. For efficient LLM deployment, this constrains pruning
    pattern to make a structure, such as row-wise or 2:4 sparsity, which seriously
    deteriorates model accuracy compared to an unstructured pattern. Therefore, we
    propose a novel method to reflect the reduction of weight element to actual reduction
    in weight size by efficiently compressing the unstructured sparsity pattern. This
    retains the model accuracy while increasing the practicality and applicability
    of pruning techniques. We detail our sparse format in Section [4.1](#S4.SS1 "4.1
    Endor Sparse Format ‣ 4 Endor Sparse Format and Pruning ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference"). As the realms of quantization and
    pruning evolve, achieving good performance with even lower precision or with higher
    pruning ratio, our work aims to even the comparison ground by providing a way
    for LLM pruning to yield reduced memory footprint. We also design our method with
    the potential for joint application of pruning and quantization in mind. We explore
    the Endor’s effectiveness in jointly applying quantization and pruning in Section [5.4](#S5.SS4
    "5.4 Joint Application with Quantization ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '我们减少识别出的瓶颈权重传输延迟的两个候选方法是量化和剪枝，这可以减少权重参数的大小。理论上，减少权重大小将导致权重传输延迟的成比例减少。然而，无结构模式剪枝的权重必须作为整个矩阵存储，并且权重元素的减少没有反映在权重传输延迟中。为了高效部署
    LLM，这限制了剪枝模式必须具备结构，例如按行剪枝或 2:4 稀疏性，这与无结构模式相比会严重降低模型准确性。因此，我们提出了一种新方法，通过高效压缩无结构稀疏模式来反映权重元素减少对实际权重大小减少的影响。这保持了模型的准确性，同时提高了剪枝技术的实用性和适用性。我们在第[4.1节](#S4.SS1
    "4.1 Endor Sparse Format ‣ 4 Endor Sparse Format and Pruning ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")中详细描述了我们的稀疏格式。随着量化和剪枝领域的发展，实现更低精度或更高剪枝比的良好性能，我们的工作旨在通过提供一种使
    LLM 剪枝减少内存占用的方法来平衡比较基础。我们还设计了具有量化和剪枝联合应用潜力的方法。我们在第[5.4节](#S5.SS4 "5.4 Joint Application
    with Quantization ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly Sparse Format for
    Offloaded LLM Inference")中探讨了 Endor 在量化和剪枝联合应用中的有效性。'
- en: Algorithm 1 The compression algorithm. From a 50% pruned weight matrix $\mathbf{W}$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 压缩算法。来自 50% 剪枝的权重矩阵 $\mathbf{W}$。
- en: 1:Let $\mathbf{W}$ to $m$ to $n$8:              Append 1 to $\mathbf{b}_{i}$11:         end if12:     end for13:end for
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 设 $\mathbf{W}$ 为 $m$ 到 $n$8:                将 1 附加到 $\mathbf{b}_{i}$11:
          end if12:    end for13:end for'
- en: Algorithm 2 The decompression algorithm. To occupy a row of decompressed weight
    matrix $\mathbf{W}$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 解压缩算法。占据一行解压缩的权重矩阵 $\mathbf{W}$。
- en: 1:Let $\mathbf{W}$ is an array of non-zero elements3:Assume $\mathbf{b}$6:     for $j=1$9:              $k\leftarrow
    k+1$10:         end if11:     end for12:end for
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 设 $\mathbf{W}$ 为非零元素的数组3: 假设 $\mathbf{b}$6:       for $j=1$9:                $k\leftarrow
    k+1$10:       end if11:    end for12:end for'
- en: 4 Endor Sparse Format and Pruning
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 Endor 稀疏格式和剪枝
- en: 4.1 Endor Sparse Format
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 Endor 稀疏格式
- en: '![Refer to caption](img/13830eeba44e5fb7283081bec4c6aaa1.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/13830eeba44e5fb7283081bec4c6aaa1.png)'
- en: 'Figure 4: Sparse format for offloaded model weights. Bitmap is stored as a
    1-d vector.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 卸载模型权重的稀疏格式。位图存储为 1-d 向量。'
- en: 'Our proposed method reduces the size of each layer weight, previously unexplored
    due to the unstructured nature of sparse patterns. As Figure [4](#S4.F4 "Figure
    4 ‣ 4.1 Endor Sparse Format ‣ 4 Endor Sparse Format and Pruning ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference") shows, for every sparse weight matrix
    derived from pruning, we save only the non-zero elements and express the location
    of non-zero elements as a binary array, or a bitmap. In terms of compression ratio,
    the bitmap is a minimal addition to the weight size that is easily amortized by
    the much larger size reduction of removing zero-valued elements, reaching a compression
    ratio close to pruning ratio. We pruned our workload OPT-66B to 50% sparsity with
    pruning algorithm from Wanda [[20](#bib.bibx20)]. Each fully connected layer weight
    is a $9,216\times 36,864$ binary matrix of size 42MB, reducing the overall weight
    size to 56% of the original size. The reduction of weight size leads to a proportionate
    reduction of weight transfer latency.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出的方法减少了每一层权重的大小，这在之前由于稀疏模式的非结构化特性而未被探索。如图[4](#S4.F4 "图 4 ‣ 4.1 Endor 稀疏格式
    ‣ 4 Endor 稀疏格式和剪枝 ‣ ENDOR: 硬件友好的稀疏格式用于卸载 LLM 推理")所示，对于每个由剪枝得到的稀疏权重矩阵，我们仅保存非零元素，并将非零元素的位置表示为二进制数组或位图。在压缩比方面，位图是对权重大小的最小附加部分，通过去除零值元素的大幅减少来轻松摊销，达到接近剪枝比的压缩比。我们将我们的工作负载
    OPT-66B 剪枝到 50% 稀疏度，使用 Wanda 的剪枝算法[[20](#bib.bibx20)]。每个全连接层的权重是一个 $9,216\times
    36,864$ 的 42MB 二进制矩阵，将整体权重大小减少到原始大小的 56%。权重大小的减少导致了权重传输延迟的相应减少。'
- en: 'Compression. Algorithm [1](#alg1 "Algorithm 1 ‣ 3.5 Overcoming Bottleneck:
    Unstructured Sparsity Compression ‣ 3 Case Study: Offloading OPT-66B ‣ ENDOR:
    Hardware-Friendly Sparse Format for Offloaded LLM Inference") presents the compression
    of the pruned weight matrix into the proposed sparse format. Compression of weights
    and their placement into storage devices are performed before inference time;
    thus, it is irrelevant to inference overhead.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '压缩。算法 [1](#alg1 "算法 1 ‣ 3.5 克服瓶颈：非结构稀疏压缩 ‣ 3 案例研究：卸载 OPT-66B ‣ ENDOR: 硬件友好的稀疏格式用于卸载
    LLM 推理") 介绍了将剪枝后的权重矩阵压缩成提议的稀疏格式。权重的压缩及其在存储设备中的放置在推理时间之前完成；因此，与推理开销无关。'
- en: 'Decompression. Algorithm [2](#alg2 "Algorithm 2 ‣ 3.5 Overcoming Bottleneck:
    Unstructured Sparsity Compression ‣ 3 Case Study: Offloading OPT-66B ‣ ENDOR:
    Hardware-Friendly Sparse Format for Offloaded LLM Inference") presents the decompression
    of the sparse format. Decompression is performed on the GPU during inference time
    after the weight in sparse format is loaded from storage to host CPU memory and
    then from host CPU memory to GPU memory. We found GPU is more suitable to leverage
    the parallel nature of the decompression algorithm at a lower latency than decompression
    on CPU. We have studied its effect in detail in Section [5](#S5 "5 Evaluation
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '解压缩。算法 [2](#alg2 "算法 2 ‣ 3.5 克服瓶颈：非结构稀疏压缩 ‣ 3 案例研究：卸载 OPT-66B ‣ ENDOR: 硬件友好的稀疏格式用于卸载
    LLM 推理") 介绍了稀疏格式的解压缩。解压缩是在推理时在 GPU 上进行的，首先将稀疏格式的权重从存储加载到主机 CPU 内存，然后从主机 CPU 内存加载到
    GPU 内存。我们发现 GPU 更适合利用解压缩算法的并行特性，相较于在 CPU 上的解压缩具有更低的延迟。我们在第[5](#S5 "5 评估 ‣ ENDOR:
    硬件友好的稀疏格式用于卸载 LLM 推理")节中详细研究了其效果。'
- en: 4.2 Joint Application with Quantization and Activation Sparsity
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 与量化和激活稀疏性的联合应用
- en: 'Endor sparse format for pruned LLM is orthogonal to other optimization methods
    of offloaded inference, mainly quantization and activation sparsity. Acknowledging
    the potential of jointly applying pruning and quantization, Endor reduces the
    weight transfer latency of pruned LLM regardless of its bit-width. However, the
    compression rate will decrease. For example, when an 8-bit quantization is applied,
    a weight matrix of $9,216\times 36,864$ is naively of size 340MB. Applying Endor
    sparse format with 50% pruning would yield the isolated non-zero values to 170MB
    and the bitmap 42MB. Overall, the compression rate is now 62% as compared to 56%
    in a 16-bit weight matrix. We measure the speedup of quantization with Endor sparse
    format in Section [5.4](#S5.SS4 "5.4 Joint Application with Quantization ‣ 5 Evaluation
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference"). Leveraging
    activation sparsity involves a small network that predicts which neurons will
    be non-zero after the ReLU activation function. Using this prediction, the rows
    of up projection weight matrix and columns of down projection weight matrix is
    selectively loaded. Because Endor preserves the non-zero values of the pruned
    weight and expresses its position with a bitmap, a minimal bitmap processing can
    determine the non-zero values to selectively load.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '对于修剪的 LLM，Endor 稀疏格式与其他离线推理优化方法（主要是量化和激活稀疏性）是正交的。尽管认识到联合应用修剪和量化的潜力，Endor 无论位宽如何，都能减少修剪
    LLM 的权重传输延迟。然而，压缩率会下降。例如，当应用 8 位量化时，$9,216\times 36,864$ 的权重矩阵的大小为 340MB。应用 Endor
    稀疏格式并进行 50% 修剪，会将孤立的非零值压缩到 170MB，位图压缩到 42MB。总体而言，压缩率现在为 62%，而 16 位权重矩阵的压缩率为 56%。我们在第[5.4](#S5.SS4
    "5.4 Joint Application with Quantization ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")节中测量了 Endor 稀疏格式与量化的加速效果。利用激活稀疏性涉及一个小型网络，该网络预测
    ReLU 激活函数后的哪些神经元将为非零值。通过这个预测，选择性加载上投影权重矩阵的行和下投影权重矩阵的列。由于 Endor 保留了修剪权重的非零值，并用位图表示其位置，因此最小的位图处理可以确定非零值进行选择性加载。'
- en: 5 Evaluation
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: 'We used the LLM model and system configuration from Section [3.1](#S3.SS1 "3.1
    Workload Specification ‣ 3 Case Study: Offloading OPT-66B ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference") for evaluation. Additionally, we measure
    the effect of Endor on Llama2-70B on Appendix [C](#A3 "Appendix C Endor on Llama2-70B
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用了第[3.1](#S3.SS1 "3.1 Workload Specification ‣ 3 Case Study: Offloading
    OPT-66B ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference")节中的
    LLM 模型和系统配置进行评估。此外，我们在附录[C](#A3 "Appendix C Endor on Llama2-70B ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")中测量了 Endor 对 Llama2-70B 的效果。'
- en: 5.1 Compressed Offloaded Inference
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 压缩的离线推理
- en: To evaluate Endor, we first compare Endor’s performance with baseline, decompression
    on CPU, and ZSTD compression on a single linear operation of OPT. We then compare
    Endor’s effect with baseline on the inference pass of entire OPT-66B.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 Endor，我们首先将 Endor 的性能与基线、CPU 上的解压缩以及单个线性操作的 ZSTD 压缩进行比较。接着，我们在整个 OPT-66B
    的推理过程中将 Endor 的效果与基线进行比较。
- en: 'Per-Operation Speedup. Figure [5](#S5.F5 "Figure 5 ‣ 5.1 Compressed Offloaded
    Inference ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded
    LLM Inference") shows the offloaded execution of OPT layer’s fully-connected operation
    using our novel sparse format compression. Compared to the baseline offloaded
    inference using dense weight matrices, this compression method, which significantly
    reduces the weight size, leads to a 1.67$\times$ speedup of both storage to CPU
    and CPU to GPU weight transfer. Negligible latency added by decompression on GPU
    is far outweighed by the reduction of weight transfer overhead. Breakdown of other
    operations of OPT-66B for each methods are listed in Appendix [A](#A1 "Appendix
    A Endor Per-Operation Breakdown ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded
    LLM Inference").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '每操作加速。图[5](#S5.F5 "Figure 5 ‣ 5.1 Compressed Offloaded Inference ‣ 5 Evaluation
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference")展示了使用我们新颖的稀疏格式压缩的
    OPT 层全连接操作的离线执行。与使用密集权重矩阵的基线离线推理相比，这种显著减少权重大小的压缩方法导致存储到 CPU 和 CPU 到 GPU 权重传输的速度提高了
    1.67$\times$。GPU 上解压缩增加的微不足道的延迟远远被权重传输开销的减少所抵消。每种方法的 OPT-66B 的其他操作细节列在附录[A](#A1
    "Appendix A Endor Per-Operation Breakdown ‣ ENDOR: Hardware-Friendly Sparse Format
    for Offloaded LLM Inference")中。'
- en: '![Refer to caption](img/ca1097bad0b8a81a2cbf8109ca2a5732.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca1097bad0b8a81a2cbf8109ca2a5732.png)'
- en: 'Figure 5: Timeline of offloaded execution of fully-connected operation'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：全连接操作的卸载执行时间线
- en: $\bullet$  Decompression on CPU vs GPU – We compare the effect of decompression
    performed on CPU and GPU. On CPU, decompression is parallelized with multiprocessing
    on 12 CPU cores. Identical with Endor, reduced weight size reduces the dominant
    weight transfer overhead from storage to CPU. Even with multiprocessing, the decompression
    overhead overshadowed the weight transfer reduction. Also, because decompression
    is performed on CPU, there is no reduction in weight transfer overhead from CPU
    to GPU.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$  CPU与GPU上的解压缩 – 我们比较了在CPU和GPU上进行解压缩的效果。在CPU上，解压缩通过12个CPU核心的多进程进行并行处理。与Endor相同，减少的权重大小降低了从存储到CPU的主要权重传输开销。即使使用多进程，解压缩开销仍然超过了权重传输的减少。此外，由于解压缩在CPU上进行，因此没有减少从CPU到GPU的权重传输开销。
- en: $\bullet$  Comparison with ZSTD – To compare Endor latency with a compression
    format, we implemented an offloaded inference that uses offloaded weights compressed
    with ZSTD. During inference, zstandard executable is used to perform decompression
    on CPU. As shown on Figure 5, our sparse format compression outperforms ZSTD.
    While ZSTD achieves a similar compression ratio by exploring bit-level patterns
    of a pruned weight matrix (i.e., 58%), the decompression overhead itself exceeds
    the benefit gained from reduced weight transfer overhead. On the other hand, Endor
    sparse format can be decompressed much faster. Endor decompression overhead is
    smaller than just the reduction of CPU-GPU transfer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$  与ZSTD的比较 – 为了将Endor延迟与压缩格式进行比较，我们实现了一个使用ZSTD压缩的卸载权重的推理。在推理过程中，使用zstandard可执行文件在CPU上执行解压缩。如图5所示，我们的稀疏格式压缩优于ZSTD。虽然ZSTD通过探索修剪权重矩阵的位级模式（即58%）达到了类似的压缩比，但解压缩开销本身超过了减少的权重传输开销带来的好处。另一方面，Endor稀疏格式的解压缩速度要快得多。Endor解压缩的开销比CPU-GPU传输的减少还要小。
- en: 'End-to-end Speedup. We measure the execution time for single pass through OPT-66B
    during text generation. For accurate comparison with dense offloaded inference,
    we keep the same device mapping: Layers 0 to 4 on GPU, layers 5 to 12 on CPU,
    and layers 13 to 63 on storage. For a single pass through OPT-66B, dense offloaded
    inference takes 54s while Endor offloaded inference takes 35.8s, an overall 1.51$\times$
    speedup.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端加速。我们测量了OPT-66B在文本生成过程中的单次执行时间。为了与稠密卸载推理进行准确比较，我们保持相同的设备映射：GPU上的层0到4，CPU上的层5到12，以及存储上的层13到63。在一次OPT-66B的执行中，稠密卸载推理需要54秒，而Endor卸载推理仅需35.8秒，整体加速为1.51$\times$。
- en: SSD-mapped layers see even more speedup of 1.64$\times$ speedup to baseline
    offloaded inference.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: SSD映射层的加速比基线卸载推理快了1.64$\times$。
- en: '![Refer to caption](img/057131337305540a82cb1ef78729cfbf.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/057131337305540a82cb1ef78729cfbf.png)'
- en: 'Figure 6: Execution time comparison of dense and Endor offloaded OPT layers
    for SSD-mapped layers (on left) and CPU-mapped layers (on right).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：稠密和Endor卸载的OPT层在SSD映射层（左侧）和CPU映射层（右侧）的执行时间比较。
- en: 'In Figure [7](#S5.F7 "Figure 7 ‣ 5.1 Compressed Offloaded Inference ‣ 5 Evaluation
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference") a, we present
    a holistic comparison of each method for one OPT layer, which consists of the
    linear operations previously discussed. Using direct transfer, an SSD-offloaded
    OPT layer sees a 2.03$\times$ speedup from a dense OPT layer. Figure [7](#S5.F7
    "Figure 7 ‣ 5.1 Compressed Offloaded Inference ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference") b shows how the cumulative speedup
    is achieved in an offloaded Llama Layer.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[7](#S5.F7 "Figure 7 ‣ 5.1 Compressed Offloaded Inference ‣ 5 Evaluation
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference") a中，我们呈现了对一个OPT层的每种方法的全面比较，该层包括先前讨论的线性操作。使用直接传输，SSD卸载的OPT层相比于稠密OPT层有2.03$\times$的加速。图[7](#S5.F7
    "Figure 7 ‣ 5.1 Compressed Offloaded Inference ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference") b展示了在卸载的Llama层中如何实现累计加速。'
- en: '![Refer to caption](img/f876990a91cadcbaabf124e79bc61800.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f876990a91cadcbaabf124e79bc61800.png)'
- en: 'Figure 7: Timeline Comparison of SSD-mapped (a) OPT Layer and (b) Llama2 Layer.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：SSD映射的（a）OPT层和（b）Llama2层的时间线比较。
- en: 5.2 Direct SSD-GPU Transfer
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 直接SSD-GPU传输
- en: As decompression is done on GPU, SSD-offloaded weights can be read directly
    into GPU via Direct Memory Access for supported GPUs. We utilize NVIDIA GPUDirect
    Storage to implement direct SSD-GPU transfer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于解压缩在GPU上进行，SSD卸载的权重可以通过直接内存访问（Direct Memory Access）直接读入支持的GPU。我们利用NVIDIA GPUDirect
    Storage实现直接SSD-GPU传输。
- en: '![Refer to caption](img/2cfe90b5bfe6edaaba357c5012b7a2a0.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2cfe90b5bfe6edaaba357c5012b7a2a0.png)'
- en: 'Figure 8: Timeline comparison of a fully-connected operation with SSD-GPU direct
    transfer'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：完全连接操作与 SSD-GPU 直接传输的时间线比较
- en: 'As shown in Figure [8](#S5.F8 "Figure 8 ‣ 5.2 Direct SSD-GPU Transfer ‣ 5 Evaluation
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference"), weight
    transfer latency of an SSD-offloaded linear operation is further reduced. Instead
    of a two step transfer from SSD to CPU then from CPU to GPU, weight is loaded
    with a single transfer through the PCIe bus. This achieves a similar to slightly
    faster bandwidth compared to SSD-CPU transfer, effectively removing the CPU-GPU
    transfer latency. End-to-end, a single pass through OPT-66B took 24.2s. This is
    a 1.29$\times$ speedup from naive offloaded inference. We include additional measurements
    of direct SSD-GPU transfer in Appendix [B](#A2 "Appendix B Direct SSD-GPU Transfer
    ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [8](#S5.F8 "Figure 8 ‣ 5.2 直接 SSD-GPU 传输 ‣ 5 评估 ‣ ENDOR: 硬件友好的稀疏格式用于卸载 LLM
    推理") 所示，SSD 卸载线性操作的权重传输延迟进一步减少。权重通过 PCIe 总线进行单次传输，而不是从 SSD 到 CPU 再到 GPU 的两步传输。这达到了与
    SSD-CPU 传输相似或略快的带宽，有效地消除了 CPU-GPU 传输延迟。端到端，OPT-66B 单次通过需要 24.2 秒。这比原始卸载推理快 1.29$\times$。我们在附录 [B](#A2
    "Appendix B 直接 SSD-GPU 传输 ‣ ENDOR: 硬件友好的稀疏格式用于卸载 LLM 推理") 中包含了直接 SSD-GPU 传输的额外测量。'
- en: 5.3 Accuracy
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 准确性
- en: Because Endor sparse format retains the exact non-zero values and the positions
    of the pruned weight matrix, it preserves the performance of the utilized pruning
    method. For our workload OPT-66B pruned with SparseGPT [[11](#bib.bibx11)], we
    measured 9.34 perplexity on the WikiText [[17](#bib.bibx17)] validation set, which
    was very close to 9.33 perplexity measure with the dense model. When 2:4 structured
    sparsity is enforced, we measure the deteriorated perplexity of 10.07, highlighting
    the value of compressing unstructured sparsity. While we employed SparseGPT [[11](#bib.bibx11)]
    to prune OPT-66B, we use Wanda to prune Llama2-70B. Dense baseline achieved 3.12
    perplexity on WikiText dataset. Unstructured pruning achieved 3.97 perplexity,
    while 2:4 sparsity achieved 5.20 perplexity.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Endor 稀疏格式保留了确切的非零值和修剪后的权重矩阵的位置，它保留了所使用的修剪方法的性能。对于我们使用 SparseGPT [[11](#bib.bibx11)]
    修剪的 OPT-66B 工作负载，我们在 WikiText [[17](#bib.bibx17)] 验证集上测得 9.34 的困惑度，这与稠密模型的 9.33
    困惑度非常接近。当施加 2:4 结构稀疏时，我们测得困惑度降为 10.07，突显了压缩非结构稀疏的价值。虽然我们使用 SparseGPT [[11](#bib.bibx11)]
    修剪了 OPT-66B，但我们使用 Wanda 修剪了 Llama2-70B。稠密基线在 WikiText 数据集上达到了 3.12 的困惑度。非结构化修剪达到了
    3.97 的困惑度，而 2:4 稀疏达到了 5.20 的困惑度。
- en: 5.4 Joint Application with Quantization
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 量化的联合应用
- en: 'To validate Endor’s applicability, we jointly employ Endor on an offloaded
    inference with SmoothQuant [[23](#bib.bibx23)] 8-bit quantization, using the pruning
    of Wanda [[20](#bib.bibx20)]. Figure [9](#S5.F9 "Figure 9 ‣ 5.4 Joint Application
    with Quantization ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly Sparse Format for
    Offloaded LLM Inference") shows that Endor effectively reduces the weight transfer
    latency to achieve a 1.48$\times$ speedup. Note that the reduction of weight transfer
    latency is smaller compared to float16, reflecting the reduced compression ratio.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '为验证 Endor 的适用性，我们在与 SmoothQuant [[23](#bib.bibx23)] 8 位量化的卸载推理中联合使用 Endor，并使用
    Wanda [[20](#bib.bibx20)] 的修剪。图 [9](#S5.F9 "Figure 9 ‣ 5.4 量化的联合应用 ‣ 5 评估 ‣ ENDOR:
    硬件友好的稀疏格式用于卸载 LLM 推理") 显示，Endor 有效地减少了权重传输延迟，实现了 1.48$\times$ 的加速。请注意，与 float16
    相比，权重传输延迟的减少较小，反映了压缩比的降低。'
- en: '![Refer to caption](img/746ffcaa7dba62252eb411fa31401d41.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/746ffcaa7dba62252eb411fa31401d41.png)'
- en: 'Figure 9: Timeline comparison of a fully-connected operation with INT8 quantization'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：完全连接操作与 INT8 量化的时间线比较
- en: 5.5 Extent of Support
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 支持的范围
- en: The current implementation of Endor supports OPT-66B [[25](#bib.bibx25)] and
    Llama2-70 [[21](#bib.bibx21)]. While it offers pruning implementation of SparseGPT [[11](#bib.bibx11)]
    and  [[20](#bib.bibx20)], weight externally pruned with any pruning mechanism
    is also supported. It supports any sparsity pattern and sparsity ratio. It can
    be easily extended to support various LLMs available in the Huggingface Hub. We
    plan to open source Endor to stimulate a broader usage of LLM offloaded inference.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当前 Endor 的实现支持 OPT-66B [[25](#bib.bibx25)] 和 Llama2-70 [[21](#bib.bibx21)]。它提供了
    SparseGPT [[11](#bib.bibx11)] 和 [[20](#bib.bibx20)] 的修剪实现，同时也支持任何修剪机制的外部修剪。它支持任何稀疏模式和稀疏比率。它可以轻松扩展以支持
    Huggingface Hub 上的各种 LLM。我们计划开源 Endor 以促进更广泛的 LLM 离线推理使用。
- en: 6 Conclusions
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This work analyzed the offloaded inference of LLMs, a crucial solution to the
    democratization of LLMs that enables execution on constrained platforms. We identified
    the weight transfer between storage and compute elements as the bottleneck in
    end-to-end latency. We proposed a sparse format that compressed pruned LLM weights
    to reduce the memory footprint, effectively reducing the weight transfer overhead
    with minimal decompression overhead. We showed that our sparse format compression
    can be applied to existing optimization methods such as quantization and activation
    sparsity for maximum speedup. This solution can be applied on production-level
    constrained platforms such as phones, robots, and personal assistants. Additionally,
    this line of work reduces the weight transfer overhead to the scale of computation,
    therefore allowing computation and weight transfer to overlap. This paves the
    way for a complete overlap of computation with weight transfer that will result
    in the offloaded inference latency match a naive model-in-GPU inference latency.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作分析了 LLM 的离线推理，这是实现 LLM 普及化的关键解决方案，使其能够在受限平台上执行。我们确定了存储和计算元素之间的权重传输是端到端延迟的瓶颈。我们提出了一种稀疏格式，将修剪的
    LLM 权重压缩以减少内存占用，有效地减少了权重传输开销，同时仅有最小的解压缩开销。我们展示了我们的稀疏格式压缩可以应用于现有的优化方法，如量化和激活稀疏，以实现最大加速。此解决方案可应用于生产级受限平台，如手机、机器人和个人助理。此外，这项工作将权重传输开销降低到计算的规模，从而允许计算和权重传输重叠。这为计算与权重传输的完全重叠铺平了道路，从而使离线推理延迟与简单模型在
    GPU 上的推理延迟相匹配。
- en: 7 Discussion and Future Work
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论与未来工作
- en: Joint Application of Multiple Optimizations. We evaluated Endor’s compatibility
    with different optimization methods in terms of speedup. Further studies must
    be done to ensure that joint application of multiple optimizations retains the
    accuracy of dense LLMs. As Endor retains the practicality of unstructured sparse
    patterns for LLM pruning, we hope Endor stimulates robust research in efficient
    LLM deployment.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 多种优化方法的联合应用。我们评估了 Endor 与不同优化方法的兼容性，以加速效果为标准。需要进一步研究以确保多种优化方法的联合应用能够保持密集 LLM
    的准确性。由于 Endor 保留了无结构稀疏模式在 LLM 修剪中的实用性，我们希望 Endor 能促进高效 LLM 部署方面的深入研究。
- en: Offloading to Multiple SSDs. Weight transfer latency from SSD is constrained
    to the read bandwidth of the SSD. While the fastest SSDs provide read bandwidths
    up to 7000 MB/s, it is far below the achievable read bandwidth of both CPU and
    PCIe bus even after utilizing SSD-GPU direct transfer. Higher bandwidth is achievable
    by utilizing multiple SSDs. As Endor decompression is easily parallelizable, an
    efficient mapping of LLM weights to multiple SSDs will significantly reduce the
    weight transfer latency.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 向多个 SSD 转移。来自 SSD 的权重传输延迟受限于 SSD 的读取带宽。虽然最快的 SSD 提供高达 7000 MB/s 的读取带宽，但这仍远低于
    CPU 和 PCIe 总线的可实现读取带宽，即便在利用 SSD-GPU 直接传输之后也是如此。通过使用多个 SSD 可以实现更高的带宽。由于 Endor 解压缩易于并行化，将
    LLM 权重有效地映射到多个 SSD 上将显著减少权重传输延迟。
- en: Efficient Compute Unit. While our sparse format effectively reduces the amount
    of data transferred, its computation on GPU is done in a full dense matrix fashion,
    overlooking the possible reduced amount of computation. Nvidia GPUs support the
    computation of fine-grained sparsity but lack the support for a fully unstructured
    sparsity. When valid computations are correctly identified, the same computation
    throughput can be achieved with a smaller number of compute elements.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 高效计算单元。虽然我们的稀疏格式有效减少了数据传输量，但在 GPU 上的计算是以全密集矩阵方式进行的，忽略了可能减少的计算量。Nvidia GPU 支持细粒度稀疏计算，但缺乏对完全无结构稀疏的支持。当有效计算正确识别时，可以用较少的计算元素实现相同的计算吞吐量。
- en: References
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Keivan Alizadeh et al. “Llm in a flash: Efficient large language model
    inference with limited memory” In *arXiv preprint arXiv:2312.11514*, 2023'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Keivan Alizadeh 等. “闪电中的 LLM：有限内存下的大型语言模型高效推理” 在 *arXiv 预印本 arXiv:2312.11514*，2023'
- en: '[2] Reza Yazdani Aminabadi et al. “Deepspeed-Inference: Enabling Efficient
    Inference of Transformer Models at Unprecedented Scale” In *Proceedings of the
    International Conference on High Performance Computing, Networking, Storage and
    Analysis*, 2022'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Reza Yazdani Aminabadi 等. “Deepspeed-Inference：在前所未有的规模下实现变换器模型的高效推理” 在
    *国际高性能计算、网络、存储和分析会议论文集*，2022'
- en: '[3] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle and John Guttag.
    “What is the state of neural network pruning?” In *Proceedings of Machine Learning
    and Systems*, 2020'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle 和 John Guttag.
    “神经网络剪枝的现状如何？” 在 *机器学习与系统会议论文集*，2020'
- en: '[4] Tom Brown et al. “Language models are few-shot learners.” In *arXiv preprint
    arXiv:2307.09288*, 2020'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tom Brown 等. “语言模型是少样本学习者。” 在 *arXiv 预印本 arXiv:2307.09288*，2020'
- en: '[5] Jungwook Choi et al. “Pact: Parameterized clipping activation for quantized
    neural networks.” In *arXiv preprint arXiv:1805.06085*, 2018'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Jungwook Choi 等. “Pact：用于量化神经网络的参数化裁剪激活。” 在 *arXiv 预印本 arXiv:1805.06085*，2018'
- en: '[6] Matthieu Courbariaux, Yoshua Bengio and Jean-Pierre David “Binaryconnect:
    Training deep neural networks with binary weights during propagations.” In *Proceedings
    of the 28th International Conference on Neural Information Processing Systems*,
    2015'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Matthieu Courbariaux, Yoshua Bengio 和 Jean-Pierre David “Binaryconnect：在传播过程中使用二进制权重训练深度神经网络。”
    在 *第28届国际神经信息处理系统会议论文集*，2015'
- en: '[7] Tim Dettmers, Mike Lewis, Younes Belkada and Luke Zettlemoyer “LLM.int8():
    8-bit Matrix Multiplication for Transformers at Scale.” In *Proceedings of the
    36th International Conference on Neural Information Processing Systems*, 2022'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Tim Dettmers, Mike Lewis, Younes Belkada 和 Luke Zettlemoyer “LLM.int8():
    规模化变换器的8位矩阵乘法。” 在 *第36届国际神经信息处理系统会议论文集*，2022'
- en: '[8] Tim Dettmers et al. “SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression” In *Proceedings of the 12th International Conference on
    Learning Representations*, 2024'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Tim Dettmers 等. “SpQR：一种用于近乎无损的 LLM 权重压缩的稀疏量化表示” 在 *第12届国际学习表征会议论文集*，2024'
- en: '[9] Zhen Dong et al. “Hawq: Hessian aware quantization of neural networks with
    mixed-precision.” In *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Zhen Dong 等. “Hawq：具有混合精度的神经网络的赫西安感知量化。” 在 *IEEE/CVF 国际计算机视觉会议论文集*，2019'
- en: '[10] Jonathan Frankle and Carbin Michael. “The lottery ticket hypothesis: Finding
    sparse, trainable neural networks.” In *Proceedings of the 7th International Conference
    on Learning Representations*, 2019'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Jonathan Frankle 和 Carbin Michael. “彩票票假说：寻找稀疏、可训练的神经网络。” 在 *第七届国际学习表征会议论文集*，2019'
- en: '[11] Elias Frantar and Dan Alistarh “Sparsegpt: Massive language models can
    be accurately pruned in one-shot” In *Proceedings of the 40th International Conference
    on Machine Learning*, 2023'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Elias Frantar 和 Dan Alistarh “Sparsegpt：可以一次性准确剪枝的大型语言模型” 在 *第40届国际机器学习会议论文集*，2023'
- en: '[12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler and Dan Alistarh “GPTQ:
    Accurate Post-Training Quantization for Generative Pre-trained Transformers” In
    *Proceedings of the 11th International Conference on Learning Representations*,
    2023'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh “GPTQ：生成预训练变换器的准确后训练量化”
    在 *第11届国际学习表征会议论文集*，2023'
- en: '[13] Song Han, Jeff Pool, John Tran and William J Dally. “Learning both weights
    and connections for efficient neural networks.” In *Proceedings of the 28th International
    Conference on Neural Information Processing Systems*, 2015'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 宋汉、杰夫·普尔、约翰·特兰和威廉·J·达利 “同时学习权重和连接以提高神经网络效率。” 载于 *第28届国际神经信息处理系统会议论文集*，2015年'
- en: '[14] “Hugging Face Accelerate” [Accessed: April-28th-2024], [https://huggingface.co/docs/accelerate/en/index](https://huggingface.co/docs/accelerate/en/index)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] “Hugging Face Accelerate” [访问日期：2024年4月28日]，[https://huggingface.co/docs/accelerate/en/index](https://huggingface.co/docs/accelerate/en/index)'
- en: '[15] Namhoon Lee, Thalaiyasingam Ajanthan and Philip H. S. Torr. “Snip: Single-shot
    network pruning based on connection sensitivity.” In *Proceedings of the 6th International
    Conference on Learning Representations*, 2018'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 李南勋、塔莱亚辛·阿贾坦和菲利普·H·S·托尔 “Snip: 基于连接敏感度的单次网络剪枝。” 载于 *第6届国际学习表示会议论文集*，2018年'
- en: '[16] “LZ4” [Accessed: May-22th-2024], [https://lz4.org/](https://lz4.org/)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] “LZ4” [访问日期：2024年5月22日]，[https://lz4.org/](https://lz4.org/)'
- en: '[17] Stephen Merity, Caiming Xiong, James Bradbury and Richard Socher. “Pointer
    sentinel mixture models.” In *arXiv preprint arXiv:1609.07843*, 2016'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 斯蒂芬·梅里蒂、项才明、詹姆斯·布拉德伯里和理查德·索赫 “指针哨兵混合模型。” 载于 *arXiv预印本 arXiv:1609.07843*，2016年'
- en: '[18] Asit Mishra et al. “Accelerating Sparse Deep Neural Networks.” In *arXiv
    preprint arXiv:2104.08378*, 2021'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 阿西特·米什拉等 “加速稀疏深度神经网络。” 载于 *arXiv预印本 arXiv:2104.08378*，2021年'
- en: '[19] Ying Sheng et al. “FlexGen: high-throughput generative inference of large
    language models with a single GPU” In *Proceedings of the 40th International Conference
    on Machine Learning*, 2023'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 盛莹等 “FlexGen: 单个GPU高通量生成推断大型语言模型” 载于 *第40届国际机器学习会议论文集*，2023年'
- en: '[20] Mingjie Sun, Zhuang Liu, Anna Bair and J. Zico Kolter “A Simple and Effective
    Pruning Approach for Large Language Models” In *Proceedings of the 12th International
    Conference on Learning Representations*, 2024'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 孙名杰、刘壮、安娜·拜尔和J. Zico Kolter “一种简单有效的大型语言模型剪枝方法” 载于 *第12届国际学习表示会议论文集*，2024年'
- en: '[21] Hugo Touvron et al. “Llama 2: Open Foundation and Fine-Tuned Chat Models.”
    In *arXiv preprint arXiv:2307.09288*, 2023'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 雨果·图弗龙等 “Llama 2: 开放基础和微调对话模型。” 载于 *arXiv预印本 arXiv:2307.09288*，2023年'
- en: '[22] Hanrui Wang, Zhekai Zhang and Song Han. “SpAtten: Efficient Sparse Attention
    Architecture with Cascade Token and Head Pruning” In *Proceedings of the 27th
    IEEE International Symposium on High-Performance Computer Architecture*, 2021'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 王汉睿、张哲凯和宋汉 “SpAtten: 高效稀疏注意力架构与级联令牌和头部剪枝” 载于 *第27届IEEE国际高性能计算架构研讨会论文集*，2021年'
- en: '[23] Guangxuan Xiao et al. “SmoothQuant: Accurate and efficient post-training
    quantization for large language models” In *Proceedings of the 40th International
    Conference on Machine Learning*, 2023'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 肖光轩等 “SmoothQuant: 精确且高效的大型语言模型后训练量化” 载于 *第40届国际机器学习会议论文集*，2023年'
- en: '[24] Zhewei Yao et al. “ZeroQuant: Efficient and Affordable Post-Training Quantization
    for Large-Scale Transformers.” In *Proceedings of the 36th International Conference
    on Neural Information Processing Systems*, 2022'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 姚哲伟等 “ZeroQuant: 高效且经济的大规模变换器后训练量化。” 载于 *第36届国际神经信息处理系统会议论文集*，2022年'
- en: '[25] Susan Zhang et al. “OPT: Open Pre-trained Transformer Language Models.”
    In *arXiv preprint arXiv:2205.01068*, 2022'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 苏珊·张等 “OPT: 开放预训练变换器语言模型。” 载于 *arXiv预印本 arXiv:2205.01068*，2022年'
- en: '[26] “Zstandard” [Accessed: May-22th-2024], [http://facebook.github.io/zstd/](http://facebook.github.io/zstd/)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] “Zstandard” [访问日期：2024年5月22日]，[http://facebook.github.io/zstd/](http://facebook.github.io/zstd/)'
- en: Appendix A Endor Per-Operation Breakdown
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A Endor每操作分解
- en: 'Offloaded operations in OPT-66B can be categorized by the weight matrix shape.
    In Section [5.1](#S5.SS1 "5.1 Compressed Offloaded Inference ‣ 5 Evaluation ‣
    ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference"), we discussed
    the speedup on fully-connected operation with weight matrix $9,216\times 36,864$.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'OPT-66B中的离线操作可以按权重矩阵形状进行分类。在第[5.1](#S5.SS1 "5.1 压缩离线推断 ‣ 5 评估 ‣ ENDOR: 硬件友好稀疏格式的离线LLM推断")节中，我们讨论了权重矩阵$9,216\times
    36,864$的全连接操作加速。'
- en: 'Figure [10](#A1.F10 "Figure 10 ‣ Appendix A Endor Per-Operation Breakdown ‣
    ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference") shows the
    offloaded inference of the SSD-mapped linear operation from the attention sub-layer.
    Compressing the weight with Endor sparse format yields the reduction of weight
    transfer latency near proportionate to the compression ratio with minimal decompression
    overhead performed on GPU. Figure [11](#A1.F11 "Figure 11 ‣ Appendix A Endor Per-Operation
    Breakdown ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded LLM Inference")
    shows the offloaded inference of the SSD-mapped linear operation from fully-connected
    sub-layer. Both linear operations inside the fully-connected sub-layer, each with
    weight matrix sized $9,216\times 36,864$ shows the same latency.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10](#A1.F10 "图 10 ‣ 附录 A Endor 每操作细节 ‣ ENDOR：适合硬件的稀疏格式用于离线 LLM 推理") 显示了 SSD
    映射线性操作的离线推理，来自注意力子层。使用 Endor 稀疏格式压缩权重，减少了权重传输延迟，接近于压缩比的比例，且 GPU 上执行的解压缩开销最小。图
    [11](#A1.F11 "图 11 ‣ 附录 A Endor 每操作细节 ‣ ENDOR：适合硬件的稀疏格式用于离线 LLM 推理") 显示了 SSD 映射线性操作的离线推理，来自全连接子层。全连接子层内的两个线性操作，每个权重矩阵大小为
    $9,216\times 36,864$，显示了相同的延迟。
- en: '![Refer to caption](img/e2f9e938f481f7ebca840683afea5d84.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e2f9e938f481f7ebca840683afea5d84.png)'
- en: 'Figure 10: Timeline of SSD-mapped attention linear operation (weight size:
    $9216\times 9216$).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：SSD 映射注意力线性操作的时间线（权重大小：$9216\times 9216$）。
- en: '![Refer to caption](img/6efcd9c860234b30ceb9d2c7c28c7fbb.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6efcd9c860234b30ceb9d2c7c28c7fbb.png)'
- en: 'Figure 11: Timeline of SSD-mapped fully-connected linear operation (weight
    size: $36864\times 9216$).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：SSD 映射全连接线性操作的时间线（权重大小：$36864\times 9216$）。
- en: Appendix B Direct SSD-GPU Transfer
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 直接 SSD-GPU 转移
- en: 'To accompany the measurements of Section [5.2](#S5.SS2 "5.2 Direct SSD-GPU
    Transfer ‣ 5 Evaluation ‣ ENDOR: Hardware-Friendly Sparse Format for Offloaded
    LLM Inference"), we provide additional measurements of applying direct SSD-GPU
    transfer on offloaded operation from the attention sub-layer. Figure [12](#A2.F12
    "Figure 12 ‣ Appendix B Direct SSD-GPU Transfer ‣ ENDOR: Hardware-Friendly Sparse
    Format for Offloaded LLM Inference") shows that directly loading offloaded weight
    into GPU bypassing CPU removes the latency of CPU-GPU transfer.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配合第 [5.2](#S5.SS2 "5.2 直接 SSD-GPU 转移 ‣ 5 评估 ‣ ENDOR：适合硬件的稀疏格式用于离线 LLM 推理")
    节中的测量，我们提供了将直接 SSD-GPU 转移应用于注意力子层的离线操作的额外测量。图 [12](#A2.F12 "图 12 ‣ 附录 B 直接 SSD-GPU
    转移 ‣ ENDOR：适合硬件的稀疏格式用于离线 LLM 推理") 显示，直接将离线权重加载到 GPU，绕过 CPU，可以消除 CPU-GPU 转移的延迟。
- en: '![Refer to caption](img/9e82795e279b1501ff720bfb7cfe4657.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9e82795e279b1501ff720bfb7cfe4657.png)'
- en: 'Figure 12: Timeline of SSD-mapped attention linear operation using SSD-GPU
    Transfer.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：使用 SSD-GPU 转移的 SSD 映射注意力线性操作的时间线。
- en: Appendix C Endor on Llama2-70B
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C Endor 在 Llama2-70B 上
- en: C.1 Workload Specification
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 工作负载规格
- en: 'We validate Endor sparse format for Llama2-70B in float16 precision. Llama2-70B
    consists of 80 Llama layers. Each Llama layer is divided into an attention sub-layer
    and a fully-connected sub-layer. The attention sub-layer contains four linear
    operations: key projection, query projection, value projection, and output projection.
    However, one difference from OPT-66B is that Llama uses group query attention.
    While query projection and output projection uses weight matrix of size $8,192\times
    8,192$. For offloaded inference, we mapped 6 Llama layers on GPU, 10 layers on
    CPU, and 64 layers on SSD.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们验证了在 `float16` 精度下 Endor 稀疏格式在 Llama2-70B 上的表现。Llama2-70B 由 80 个 Llama 层组成。每个
    Llama 层被划分为一个注意力子层和一个全连接子层。注意力子层包含四个线性操作：键投影、查询投影、值投影和输出投影。然而，与 OPT-66B 的不同之处在于，Llama
    使用了组查询注意力。查询投影和输出投影使用的权重矩阵大小为 $8,192\times 8,192$。对于离线推理，我们将 6 个 Llama 层映射到 GPU，10
    个层映射到 CPU，64 个层映射到 SSD。
- en: C.2 Per-Operation Speedup
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 每操作加速
- en: 'We present Endor’s effect on the three categories of linear operations, of
    weight matrix size $1,024\times 8,192$. This time, we present the timeline of
    dense baseline, Endor, and Endor with direct SSD-GPU transfer. While the weight
    latency differs with the size of weight transferred, the effect of Endor is evident
    in all operation in both attention sub-layer (Figure [13](#A3.F13 "Figure 13 ‣
    C.2 Per-Operation Speedup ‣ Appendix C Endor on Llama2-70B ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference"), Figure [14](#A3.F14 "Figure 14 ‣
    C.2 Per-Operation Speedup ‣ Appendix C Endor on Llama2-70B ‣ ENDOR: Hardware-Friendly
    Sparse Format for Offloaded LLM Inference")) and fully-connected sub-layer (Figure [15](#A3.F15
    "Figure 15 ‣ C.2 Per-Operation Speedup ‣ Appendix C Endor on Llama2-70B ‣ ENDOR:
    Hardware-Friendly Sparse Format for Offloaded LLM Inference")). Endor effectively
    tackles the dominant weight transfer latency of offloaded inference by half with
    minimal decompression overhead. With GPU support, SSD-GPU direct transfer further
    reduces the latency by combining the SSD-CPU and CPU-GPU weight transfer into
    a single transaction.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了Endor对三种线性操作类别的影响，权重矩阵大小为 $1,024\times 8,192$。这次，我们展示了密集基线、Endor以及Endor与直接SSD-GPU传输的时间线。虽然权重延迟随着传输的权重大小而不同，但Endor在所有操作中的效果都很明显，无论是在注意力子层（图[13](#A3.F13
    "图 13 ‣ C.2 每操作加速 ‣ 附录 C Endor 在 Llama2-70B 上 ‣ ENDOR：硬件友好的稀疏格式用于卸载的LLM推理")、图[14](#A3.F14
    "图 14 ‣ C.2 每操作加速 ‣ 附录 C Endor 在 Llama2-70B 上 ‣ ENDOR：硬件友好的稀疏格式用于卸载的LLM推理")）还是全连接子层（图[15](#A3.F15
    "图 15 ‣ C.2 每操作加速 ‣ 附录 C Endor 在 Llama2-70B 上 ‣ ENDOR：硬件友好的稀疏格式用于卸载的LLM推理")）。Endor有效地将卸载推理的主要权重传输延迟减少了一半，且解压开销最小。通过GPU支持，SSD-GPU直接传输进一步减少了延迟，将SSD-CPU和CPU-GPU权重传输合并为一个事务。
- en: '![Refer to caption](img/4f30c8e0c3410b4b5b51c10018bc4f01.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4f30c8e0c3410b4b5b51c10018bc4f01.png)'
- en: 'Figure 13: Timeline of SSD-mapped attention linear operation (weight $1,024\times
    8,192$).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：SSD映射注意力线性操作的时间线（权重 $1,024\times 8,192$）。
- en: '![Refer to caption](img/8ac22cc748ad62c4f770de4f03c91d31.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8ac22cc748ad62c4f770de4f03c91d31.png)'
- en: 'Figure 14: Timeline of SSD-mapped attention linear operation (weight $8,192\times
    8,192$).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：SSD映射注意力线性操作的时间线（权重 $8,192\times 8,192$）。
- en: '![Refer to caption](img/074fc50172a0e8ea482c9b3be0cf8406.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/074fc50172a0e8ea482c9b3be0cf8406.png)'
- en: 'Figure 15: Timeline of SSD-mapped fully-connected linear operation (weight
    $28,672\times 8,192$).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：SSD映射全连接线性操作的时间线（权重 $28,672\times 8,192$）。
- en: C.3 End-to-End Speedup
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 端到端加速
- en: 'We measured the execution time for single pass through Llama2-70B during text
    generation. For accurate comparion with dense offloaded inference, we kept the
    same device mapping: Layers 0 to 5 on GPU, layers 6 to 15 on CPU, and layers 16
    to 79 on SSD.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测量了Llama2-70B在文本生成过程中单次传递的执行时间。为了与密集卸载推理进行准确比较，我们保持了相同的设备映射：0到5层在GPU上，6到15层在CPU上，16到79层在SSD上。
- en: For a single pass through Llama2-70B, dense offloaded inference took 57s while
    Endor offloaded inference took 35.2s, an overall 1.62$\times$ speedup.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一次Llama2-70B的传递，密集卸载推理需要57秒，而Endor卸载推理需要35.2秒，总体加速了1.62$\times$。
- en: Weight compression with enables more layer weights to reside on CPU. Under the
    same DRAM footprint, now 20 layers can be CPU-mapped. For a single pass through
    Llama2-70B, this took 32s without direct transfer and 24s with direct transfer,
    each an overall 1.78$\times$ speedup.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 权重压缩使更多的层权重能够驻留在CPU上。在相同的DRAM占用下，现在可以将20层映射到CPU。对于一次Llama2-70B的传递，没有直接传输需要32秒，而有直接传输需要24秒，每次总体加速了1.78$\times$。
- en: '![Refer to caption](img/919e755e1648b61dd8a1fdec58bacb5d.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/919e755e1648b61dd8a1fdec58bacb5d.png)'
- en: 'Figure 16: Execution time comparison of dense and Endor offloaded Llama layers
    for SSD-mapped layers (on left) and CPU-mapped layers (on right).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：SSD映射层（左侧）和CPU映射层（右侧）的密集和Endor卸载Llama层的执行时间比较。
