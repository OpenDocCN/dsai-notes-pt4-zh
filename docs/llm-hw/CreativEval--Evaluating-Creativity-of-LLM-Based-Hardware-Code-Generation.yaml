- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:51:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'CreativEval: 评估基于LLM的硬件代码生成的创造力'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.08806](https://ar5iv.labs.arxiv.org/html/2404.08806)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.08806](https://ar5iv.labs.arxiv.org/html/2404.08806)
- en: \lst@NormedDef\languageNormedDefd@verilog
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \lst@NormedDef\languageNormedDefd@verilog
- en: Verilog \lst@SaveOutputDef‘’\quotesngl@verilog\lst@SaveOutputDef“\backtick@verilog\lst@SaveOutputDef‘$\dollar@verilog
    \lst@Keyvlogconstantstyle \lst@Keyvlogdefinestyle \lst@Keyvlogsystemstyle
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Verilog \lst@SaveOutputDef‘’\quotesngl@verilog\lst@SaveOutputDef“\backtick@verilog\lst@SaveOutputDef‘$\dollar@verilog
    \lst@Keyvlogconstantstyle \lst@Keyvlogdefinestyle \lst@Keyvlogsystemstyle
- en: Matthew DeLorenzo, Vasudev Gohil, Jeyavijayan Rajendran
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Matthew DeLorenzo, Vasudev Gohil, Jeyavijayan Rajendran
- en: Texas A&M University, USA
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯农工大学，美国
- en: '{matthewdelorenzo, gohil.vasudev, jv.rajendran}@tamu.edu'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{matthewdelorenzo, gohil.vasudev, jv.rajendran}@tamu.edu'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have proved effective and efficient in generating
    code, leading to their utilization within the hardware design process. Prior works
    evaluating LLMs’ abilities for register transfer level code generation solely
    focus on functional correctness. However, the creativity associated with these
    LLMs, or the ability to generate novel and unique solutions, is a metric not as
    well understood, in part due to the challenge of quantifying this quality.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已证明在生成代码方面有效且高效，导致它们在硬件设计过程中得到了应用。先前评估LLMs在寄存器传输级代码生成能力的工作仅关注功能正确性。然而，与这些LLMs相关的创造力，即生成新颖独特解决方案的能力，是一个不那么容易理解的指标，部分原因在于量化这一特质的挑战。
- en: To address this research gap, we present CreativEval, a framework for evaluating
    the creativity of LLMs within the context of generating hardware designs. We quantify
    four creative sub-components, fluency, flexibility, originality, and elaboration,
    through various prompting and post-processing techniques. We then evaluate multiple
    popular LLMs (including GPT models, CodeLlama, and VeriGen) upon this creativity
    metric, with results indicating GPT-3.5 as the most creative model in generating
    hardware designs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填补这一研究空白，我们提出了CreativEval，一个在生成硬件设计的背景下评估LLMs创造力的框架。我们通过各种提示和后处理技术量化四个创造性子组件：流畅性、灵活性、原创性和扩展性。然后，我们在这一创造力指标上评估了多个流行的LLMs（包括GPT模型、CodeLlama和VeriGen），结果表明GPT-3.5是生成硬件设计中最具创造力的模型。
- en: 'Index Terms:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Hardware Design, LLM, Creativity
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件设计，LLM，创造力
- en: I Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Recent advancements within artificial intelligence, machine learning, and computing
    performance have resulted in the development of LLMs, which have quickly proven
    to be a widely applicable and successful solution when applied to a variety of
    text-based tasks [[1](#bib.bib1)]. After extensive training on large quantities
    of text data, these transformer-based models [[2](#bib.bib2)] have demonstrated
    the ability to not only successfully interpret the contextual nuances of a provided
    text (or prompt), but also generate effective responses to a near human-like degree [[3](#bib.bib3)].
    This can take the form of summarizing a document, answering and elaborating upon
    questions, and even generating code. The effectiveness and versatility of LLMs
    regarding textual understanding have resulted in their adoption within various
    applications, such as language translation [[4](#bib.bib4)], customer service
    chat-bots [[5](#bib.bib5)], and programming assistants [[1](#bib.bib1)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在人工智能、机器学习和计算性能方面的进展促成了LLMs的发展，这些模型在应用于各种基于文本的任务时，迅速证明了其广泛适用且成功的解决方案 [[1](#bib.bib1)]。经过大量文本数据的广泛训练，这些基于变换器的模型 [[2](#bib.bib2)]
    展现了不仅能够成功解读提供文本（或提示）的语境细微差别，还能生成接近人类水平的有效回应 [[3](#bib.bib3)]。这可以表现为总结文档、回答和扩展问题，甚至生成代码。LLMs在文本理解方面的有效性和多样性导致它们在语言翻译 [[4](#bib.bib4)]、客户服务聊天机器人 [[5](#bib.bib5)]
    和编程助手 [[1](#bib.bib1)] 等各种应用中得到了采用。
- en: Furthermore, the potential of LLM code generation has recently been explored
    within the integrated circuit (IC) design process [[6](#bib.bib6)], such as within
    the logic design stage. With chip designs continually growing in scale and complexity,
    efforts to increase the automation of this task through LLMs have been explored.
    This includes the evaluation of LLMs’ ability to generate hardware design codes
    from English prompts, leading to promising initial results within various frameworks [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最近已在集成电路（IC）设计过程中探讨了 LLM 代码生成的潜力[[6](#bib.bib6)]，例如在逻辑设计阶段。随着芯片设计规模和复杂性的不断增长，通过
    LLM 增加这一任务的自动化的努力已被探讨。这包括评估 LLM 从英文提示生成硬件设计代码的能力，并在各种框架内取得了初步的良好结果[[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]。
- en: With the goal of further optimizing these LLMs to the level of an experienced
    hardware designer, many research efforts have focused on improving performance
    within the metric of code functionality. This includes testing various LLM fine-tuning
    strategies and prompting methods for domain-optimized performance, such as register
    transfer level (RTL) code generation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步优化这些 LLM 使其达到经验丰富的硬件设计师的水平，许多研究工作集中在提升代码功能性指标的表现上。这包括测试各种 LLM 微调策略和领域优化性能的提示方法，如寄存器传输级（RTL）代码生成。
- en: However, another dimension to consider when evaluating the ability of a designer,
    absent from previous evaluations, is creativity. This term refers to the capacity
    to think innovatively—the ability to formulate new solutions or connections that
    are effective and unconventional [[11](#bib.bib11)]. When applied to hardware
    code generation, this can take the form of writing programs that are not only
    correct, but also novel, surprising, or valuable when compared to typical design
    approaches. This quality is essential to understanding the greater potential of
    LLMs as a tool for deriving new approaches to hardware design challenges, rather
    than simply a method to accelerate existing design practices. With a quantitative
    method of measuring this concept of creativity within LLM hardware generation,
    valuable insights could be derived, such as how performance could be further improved,
    or how LLMs can be best utilized within the hardware design process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在评估设计师能力时，另一个需要考虑的维度是创造力，这在以往的评估中未被涵盖。这个术语指的是创新思维的能力——能够提出新的解决方案或联系，这些解决方案是有效且非常规的[[11](#bib.bib11)]。当应用于硬件代码生成时，这可以表现为编写不仅正确而且与典型设计方法相比具有新颖性、惊奇性或价值的程序。这一质量对于理解
    LLM 作为工具在解决硬件设计挑战中所具有的更大潜力至关重要，而不仅仅是加速现有设计实践的方法。通过在 LLM 硬件生成中量化测量这种创造力概念，可以获得有价值的见解，例如如何进一步提升性能，或者如何在硬件设计过程中最佳利用
    LLM。
- en: 'To address this absence within the analysis of LLM-based RTL code generation,
    we propose a comparative evaluation framework in which the creativity of LLMs
    can be effectively measured. This assessment is composed of four cognitive subcategories
    of creativity (fluency, flexibility, originality, and elaboration), which are
    quantified and evaluated within the context of generating functional Verilog modules.
    Furthermore, this approach utilizes various prompting structures, generation strategies,
    and post-processing methods, from which the quality and variations of responses
    are utilized to generate a metric for creativity. This work presents the following
    contributions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 LLM 基于 RTL 代码生成分析中的这一空白，我们提出了一个比较评估框架，通过该框架可以有效地测量 LLM 的创造力。这一评估包括创造力的四个认知子类别（流畅性、灵活性、原创性和详细性），这些类别在生成功能性
    Verilog 模块的背景下被量化和评估。此外，这种方法利用了各种提示结构、生成策略和后处理方法，从中质量和响应的变化被用来生成创造力的度量标准。这项工作呈现了以下贡献：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, we propose the first framework from which a metric
    for creativity is defined for LLMs within the context of hardware design and code
    generation.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们提出了第一个框架，从中可以定义出针对硬件设计和代码生成的 LLM 创造力的度量标准。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comparative evaluation between state-of-the-art LLMs upon our creativity
    metric and its components, with GPT-3.5 achieving the highest result.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对最新 LLM 在我们的创造力度量标准及其组成部分上的比较评估，其中 GPT-3.5 获得了最高的结果。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'To enable future research, we will open-source our framework codebase and datasets
    here: [https://github.com/matthewdelorenzo/CreativEval/](https://github.com/matthewdelorenzo/CreativEval/)'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了促进未来的研究，我们将开源我们的框架代码库和数据集，地址为：[https://github.com/matthewdelorenzo/CreativEval/](https://github.com/matthewdelorenzo/CreativEval/)
- en: II Background and Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景和相关工作
- en: II-A LLMs for Code Generation and Hardware Design
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A LLM 在代码生成和硬件设计中的应用
- en: '![Refer to caption](img/806f0d1f98ef70deb68440ea27801ded.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/806f0d1f98ef70deb68440ea27801ded.png)'
- en: 'Figure 1: Experimental Framework - calculating creativity of LLMs in Verilog
    code generation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：实验框架 - 计算 LLM 在 Verilog 代码生成中的创造力。
- en: Many state-of-the-art LLMs have demonstrated remarkable success in generating
    code when provided only with a natural language description, such as GPT-3.5/4
    [[12](#bib.bib12)], BERT [[13](#bib.bib13)], and Claude [[14](#bib.bib14)], revolutionizing
    the software development process. These models demonstrate promising performance
    in code functionality, such as GPT-4 generating correct code for 67% of programming
    tasks in the HumanEval benchmark in a single response (pass@1) [[15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多最先进的 LLM 在仅提供自然语言描述时生成代码时表现出显著的成功，例如 GPT-3.5/4 [[12](#bib.bib12)], BERT [[13](#bib.bib13)],
    和 Claude [[14](#bib.bib14)]，这革新了软件开发过程。这些模型在代码功能性方面表现出色，例如 GPT-4 在 HumanEval 基准测试中在一次响应中正确生成
    67% 的编程任务代码 (pass@1) [[15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)]。
- en: Therefore, the applications of LLMs within hardware design through RTL code
    generation are explored within various studies, such as DAVE [[18](#bib.bib18)]
    which utilized GPT-2 for this task. VeriGen [[7](#bib.bib7)] then demonstrated
    that fine-tuning smaller models (CodeGen) upon a curated RTL dataset can outperform
    larger models in RTL tests. VerilogEval [[19](#bib.bib19)] presents enhanced LLM
    hardware generation through supervised fine-tuning, and provides an RTL benchmark
    for evaluating functionality in RTL generation. ChipNeMo [[9](#bib.bib9)] applied
    fine-tuning upon open-source models (Llama2 7B/13B) for various hardware design
    tasks. RTLCoder [[20](#bib.bib20)] presents an automated method for expanding
    the RTL dataset used for fine-tuning, resulting in a 7B-parameter model that outperforms
    GPT-3.5 on RTL benchmarks. Other works, including RTLLM [[21](#bib.bib21)] and
    Chip-Chat [[8](#bib.bib8)], explore prompt engineering strategies to enhance the
    quality and scale of LLM-generated designs. Although there is a plethora of work
    on LLM-based RTL generation, none of these prior works assess the creative component
    of LLMs in the hardware design process. We address this shortcoming in this work.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLM 在通过 RTL 代码生成进行硬件设计的应用在各种研究中得到了探讨，例如 DAVE [[18](#bib.bib18)] 利用 GPT-2
    进行此任务。VeriGen [[7](#bib.bib7)] 随后展示了在精心策划的 RTL 数据集上对较小模型 (CodeGen) 进行微调可以在 RTL
    测试中优于较大的模型。VerilogEval [[19](#bib.bib19)] 通过监督微调展示了增强的 LLM 硬件生成，并提供了一个 RTL 基准用于评估
    RTL 生成中的功能性。ChipNeMo [[9](#bib.bib9)] 对开源模型 (Llama2 7B/13B) 进行了微调，应用于各种硬件设计任务。RTLCoder [[20](#bib.bib20)]
    提出了扩展用于微调的 RTL 数据集的自动化方法，结果是一个 7B 参数模型在 RTL 基准测试中优于 GPT-3.5。其他工作，包括 RTLLM [[21](#bib.bib21)]
    和 Chip-Chat [[8](#bib.bib8)]，探讨了提示工程策略，以提高 LLM 生成设计的质量和规模。尽管在基于 LLM 的 RTL 生成方面有大量的研究，但这些先前的工作中没有评估
    LLM 在硬件设计过程中的创造性组件。我们在本工作中解决了这一不足。
- en: II-B Evaluating Creativity
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 创造力评估
- en: Prior cognitive science studies [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25)] have explored methods in which creative thinking can be effectively
    measured. A widely accepted creativity model [[24](#bib.bib24)] defines four primary
    cognitive dimensions from which divergent thinking, or the ability to generate
    creative ideas through exploring multiple possible solutions [[26](#bib.bib26)],
    can be measured—fluency, flexibility, originality, and elaboration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的认知科学研究 [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]
    探讨了有效衡量创造性思维的方法。一种被广泛接受的创造力模型 [[24](#bib.bib24)] 定义了四个主要的认知维度，通过这些维度可以衡量发散思维，或通过探索多种可能的解决方案生成创造性想法的能力 [[26](#bib.bib26)]——流畅性、灵活性、原创性和详细性。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fluency. The quantity of relevant and separate ideas able to be derived in response
    to a single given question.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流畅性。能够从单一给定问题中衍生出的相关且独立的想法的数量。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Flexibility. The ability to formulate alternative solutions to a given problem
    or example across a variety of categories.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灵活性。能够在各种类别中为给定问题或示例制定替代解决方案的能力。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Originality. A measure of how unique or novel a given idea is, differing from
    typical responses or solutions.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原创性。衡量给定创意的独特性或新颖性，区别于典型的响应或解决方案。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Elaboration. The ability to expand upon or refine a given idea. This can include
    the ability to construct complex solutions utilizing provided, basic concepts.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 详细阐述。扩展或完善给定创意的能力。这可以包括利用提供的基本概念构建复杂解决方案的能力。
- en: These subcategories have been widely in evaluating human creativity within educational
    research, including various studies of students [[27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29)] as a metric for effective learning. Furthermore, recent works
    explore the intersection between cognitive science and LLMs [[30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32)], in which the creativity of LLMs are evaluated
    within the context of natural language, demonstrating near-human like performance
    in many cases [[31](#bib.bib31)]. In particular, [[33](#bib.bib33)] utilizes the
    four creative subcategories to evaluate LLMs across multiple language-based cognitive
    tasks. However, this framework has not been adapted to LLMs within the context
    of generating hardware code. To this end, we devise our creativity evaluation
    framework for LLM-based hardware code generation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些子类别在教育研究中广泛用于评估人类创造力，包括对学生的各种研究[[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)]，作为有效学习的指标。此外，最近的研究探讨了认知科学与
    LLMs 之间的交集[[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)]，在自然语言背景下评估 LLM
    的创造力，展示了在许多情况下接近人类的表现[[31](#bib.bib31)]。特别地，[[33](#bib.bib33)] 利用四个创造性子类别评估 LLM
    在多个基于语言的认知任务中的表现。然而，该框架尚未适用于生成硬件代码的 LLM。因此，我们设计了用于 LLM 基于硬件代码生成的创造力评估框架。
- en: III CreativEval Framework
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III CreativEval 框架
- en: 'Given a target LLM, our CreativEval framework, as shown in Fig. [1](#S2.F1
    "Figure 1 ‣ II-A LLMs for Code Generation and Hardware Design ‣ II Background
    and Related Work ‣ CreativEval: Evaluating Creativity of LLM-Based Hardware Code
    Generation"), seeks to evaluate the creativity associated with LLMs in hardware
    code generation. CreativEval evaluates the previously defined subcategories of
    creativity—fluency, flexibility, originality, and elaboration. To this end, we
    query the target LLM with different Verilog-based prompts, and analyze the responses
    through various methods of post-processing to calculate the desired metrics, as
    explained below.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '针对目标 LLM，我们的 CreativEval 框架，如图 [1](#S2.F1 "图 1 ‣ II-A LLMs for Code Generation
    and Hardware Design ‣ II Background and Related Work ‣ CreativEval: Evaluating
    Creativity of LLM-Based Hardware Code Generation") 所示，旨在评估 LLM 在硬件代码生成中的创造力。CreativEval
    评估了先前定义的创造力子类别——流畅性、灵活性、原创性和详细阐述。为此，我们使用不同的基于 Verilog 的提示查询目标 LLM，并通过各种后处理方法分析响应，以计算所需的指标，具体如下所述。'
- en: III-A Fluency
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 流畅性
- en: 'To capture the quantity of relevant and separate ideas in our context, we define
    fluency as the average number of unique Verilog solutions generated by the target
    LLM in response to a given prompt. Our prompts contain a brief English description
    of the module and the module’s declaration, as shown in Listing [1](#LST1 "Listing
    1 ‣ III-A Fluency ‣ III CreativEval Framework ‣ CreativEval: Evaluating Creativity
    of LLM-Based Hardware Code Generation"). Each prompt is provided as input to the
    LLM, with the response intended to be the completed implementation of the module.
    As the inference process of LLMs contain variations in the generated responses,
    we generate $t$ responses for each prompt to estimate the average performance.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '为了捕捉我们背景下相关和独立的创意数量，我们将流畅性定义为目标 LLM 对给定提示生成的唯一 Verilog 解决方案的平均数量。我们的提示包含模块的简要英文描述和模块声明，如清单
    [1](#LST1 "清单 1 ‣ III-A Fluency ‣ III CreativEval Framework ‣ CreativEval: Evaluating
    Creativity of LLM-Based Hardware Code Generation") 所示。每个提示作为输入提供给 LLM，响应的目的是完成模块的实现。由于
    LLM 的推断过程包含生成响应的变化，我们为每个提示生成 $t$ 个响应以估计平均性能。'
- en: 1//Create  a  full  adder.2//A  full  adder  adds  three  bits  (including  carry-in)  and  produces  a  sum  and  carry-out.34module  top_module  (5  input  a,  b,  cin,6  output  cout,  sum  );
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 1//创建一个全加器。2//全加器加上三个位（包括进位）并生成一个和以及进位输出。34module top_module (5 input a, b,
    cin,6 output cout, sum );
- en: 'Listing 1: Fluency/Originality prompt example'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 1：流畅性/原创性提示示例
- en: Upon generating all responses, each response is then tested for functionality
    against the module’s associated testbench. If all test cases pass, the module
    is considered functional. Then, for each prompt, the functional responses (if
    any) are collected and compared to identify if they are unique implementations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成所有响应后，每个响应会在模块的相关测试平台上进行功能测试。如果所有测试用例都通过，则模块被视为功能正常。然后，对于每个提示，收集并比较功能响应（如果有的话），以确定它们是否为独特的实现。
- en: This is done through GNN4IP [[34](#bib.bib34)], a tool utilized to assess the
    similarities between circuits. By representing two Verilog modules as a data-flow
    graph (DFG), GNN4IP generates a similarity score within [-1,1], with larger values
    indicating a higher similarity. Each correct generated solution from the LLM is
    input into GNN4IP, and compared to its ideal solution, or “golden response”. Upon
    the generation of each similarity value for a given prompt, these results are
    then compared to determine how many unique values are in the response set, indicating
    the number of distinct solutions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过 GNN4IP [[34](#bib.bib34)] 完成的，这是一种用于评估电路之间相似性的工具。通过将两个 Verilog 模块表示为数据流图（DFG），GNN4IP
    生成一个[-1,1]范围内的相似度分数，较大的值表示更高的相似性。每个从 LLM 生成的正确解决方案会输入到 GNN4IP 中，并与其理想解决方案或“黄金响应”进行比较。在生成给定提示的每个相似度值后，这些结果将被比较以确定响应集合中有多少个独特值，从而表示不同解决方案的数量。
- en: 'Given that there are a set of $p$ prompts, there is a sub-total of the $t$,
    are defined as the set $R=\{{r_{1n},...,r_{mn}}\}$ responses. This process is
    repeated for all $n$ below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组$p$个提示，其中有一个总计为$t$的子集，定义为集合$R=\{{r_{1n},...,r_{mn}}\}$的响应。此过程对于所有$n$重复进行如下：
- en: '|  | $F=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{&#124;S(R_{i})&#124;}{t}\right)$
    |  | (1) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $F=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{&#124;S(R_{i})&#124;}{t}\right)$
    |  | (1) |'
- en: III-B Flexibility
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 灵活性
- en: 'Flexibility is quantified as the ability of the LLM to generate an alternative
    implementation of a Verilog module when provided with a solution. The prompts
    for this metric are constructed for a set of Verilog modules in which a correct
    solution (the golden response) is included (Listing [2](#LST2 "Listing 2 ‣ III-B
    Flexibility ‣ III CreativEval Framework ‣ CreativEval: Evaluating Creativity of
    LLM-Based Hardware Code Generation")). The LLM then rewrites the Verilog module,
    ideally resulting in a functional and unique implementation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '灵活性量化为 LLM 在提供解决方案时生成 Verilog 模块的替代实现的能力。此度量的提示是为一组 Verilog 模块构建的，其中包含一个正确的解决方案（黄金响应）（列表
    [2](#LST2 "Listing 2 ‣ III-B Flexibility ‣ III CreativEval Framework ‣ CreativEval:
    Evaluating Creativity of LLM-Based Hardware Code Generation")）。LLM 然后重写 Verilog
    模块，理想情况下生成一个功能性且独特的实现。'
- en: 1//  You  are  a  professional  hardware  designer  that  writes  correct,  fully  functional  Verilog  modules.2//  Given  the  fully  implemented  example  of  the  Verilog  module  below:34module  true_module(5  input  a,  b,  cin,6  output  cout,  sum  );7  assign  sum  =
    a  ^  b  ^  cin;8  assign  cout  = a  &  b  | a  &  cin  | b  &  cin;9endmodule1011//  Finish  writing  a  different  and  unique  implementation  of  the  provided  true_module  in  the  module  below,  top_module.12module  top_module  (13  input  a,  b,  cin,14  output  cout,  sum  );
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 1//  你是一个专业的硬件设计师，能够编写正确、完全功能的 Verilog 模块。2//  给定以下 Verilog 模块的完全实现示例：34module  true_module(5  input  a,  b,  cin,6  output  cout,  sum  );7  assign  sum  =
    a  ^  b  ^  cin;8  assign  cout  = a  &  b  | a  &  cin  | b  &  cin;9endmodule1011//  在下方模块
    top_module 中完成对所提供的 true_module 的不同且独特的实现。12module  top_module  (13  input  a,  b,  cin,14  output  cout,  sum  );
- en: 'Listing 2: Flexibility prompt example'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 2: 灵活性提示示例'
- en: 'As before, $t$ functional responses. These functional responses are compared
    directly with the golden response (through GNN4IP) to identify their similarity
    value. If the similarity value $s$ is then defined below:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，$t$ 个功能响应。这些功能响应会直接与黄金响应（通过 GNN4IP）进行比较，以确定它们的相似度值。如果相似度值$s$定义如下：
- en: '|  | $T(s)=\begin{cases}1&amp;\text{if }s<0\\ 0&amp;\text{if }s\geq 0\end{cases}$
    |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $T(s)=\begin{cases}1&\text{如果 }s<0\\ 0&\text{如果 }s\geq 0\end{cases}$ |  |
    (2) |'
- en: '|  | $X=\frac{1}{n}\sum_{i=1}^{n}\left(T[\min S(R_{i})]\right)$ |  | (3) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $X=\frac{1}{n}\sum_{i=1}^{n}\left(T[\min S(R_{i})]\right)$ |  | (3) |'
- en: III-C Originality
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 原创性
- en: The originality metric is defined as the variance (uniqueness) of an LLM-generated
    Verilog module in comparison to a typical, fully functional implementation. This
    metric is derived from the similarity value (generated through GNN4IP) between
    successful generations and their golden response.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 原创性度量定义为LLM生成的Verilog模块与典型的完全功能实现之间的方差（唯一性）。该度量值来源于成功生成的结果与其黄金响应之间的相似性值（通过GNN4IP生成）。
- en: 'The originality experiment follows the same prompt structure and procedure
    as described in [III-A](#S3.SS1 "III-A Fluency ‣ III CreativEval Framework ‣ CreativEval:
    Evaluating Creativity of LLM-Based Hardware Code Generation"). For each prompt,
    the response with the minimum similarity value is found. Then, the similarity
    values [-1, 1] are re-normalized to be on scale of [0, 1] with 1 indicating the
    least similarity (i.e. most original). These results are averaged over all $n$
    is described below:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '原创性实验遵循与[III-A](https://example.org/S3.SS1 "III-A Fluency ‣ III CreativEval
    Framework ‣ CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation")中描述的相同提示结构和程序。对于每个提示，找到相似性值最小的响应。然后，将相似性值[-1,
    1]重新归一化为[0, 1]的范围，其中1表示最少相似性（即最原创）。这些结果在所有$n$上取平均，如下所述：'
- en: '|  | $O=\frac{1}{n}\sum_{i=1}^{n}\frac{\left(-\min S(R_{i})+1\right)}{2}$ |  |
    (4) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $O=\frac{1}{n}\sum_{i=1}^{n}\frac{\left(-\min S(R_{i})+1\right)}{2}$ |  |
    (4) |'
- en: III-D Elaboration
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 详细说明
- en: 'To measure an LLM’s capacity for elaboration, the LLM is provided with multiple
    smaller Verilog modules in a prompt, and tasked with utilizing them to implement
    a larger, more complex module. As this metric requires multi-modular designs,
    a separate set of Verilog modules is utilized in constructing the prompts, as
    shown in Listing [3](#LST3 "Listing 3 ‣ III-D Elaboration ‣ III CreativEval Framework
    ‣ CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '为了衡量LLM的详细说明能力，LLM在提示中提供多个较小的Verilog模块，并任务是利用它们来实现一个更大、更复杂的模块。由于该度量要求多模块设计，因此在构建提示时使用了一组单独的Verilog模块，如列表[3](https://example.org/LST3
    "Listing 3 ‣ III-D Elaboration ‣ III CreativEval Framework ‣ CreativEval: Evaluating
    Creativity of LLM-Based Hardware Code Generation")所示。'
- en: 1//  You  are  given  a  module  add16  that  performs  a  16-bit  addition.2//Instantiate  two  of  them  to  create  a  32-bit  adder.34module  add16  (  input[15:0]  a,  input[15:0]  b,  input  cin,  output[15:0]  sum,  output  cout  );56module  top_module  (7  input  [31:0]  a,8  input  [31:0]  b,9  output  [31:0]  sum
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 1// 你被给定一个执行16位加法的模块add16。2//实例化两个这样的模块以创建一个32位加法器。34module  add16  (  input[15:0]  a,  input[15:0]  b,  input  cin,  output[15:0]  sum,  output  cout  );56module  top_module  (7  input  [31:0]  a,8  input  [31:0]  b,9  output  [31:0]  sum
- en: 'Listing 3: Elaboration prompt example'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 3: 详细说明提示示例'
- en: 'TABLE I: Comparison of different LLMs in terms of creativity and its subcategories'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 不同LLM在创造力及其子类别方面的比较'
- en: '| LLM | Functionality | Fluency | Flexibility | Originality | Elaboration |
    Creativity |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 功能性 | 流畅性 | 灵活性 | 原创性 | 详细说明 | 创造力 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| CodeLlama-7B [[35](#bib.bib35)] | 0.2417 | 0.1483 | 0.0000 | 0.2926 | 0.2222
    | 0.1658 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B [[35](https://example.org/bib.bib35)] | 0.2417 | 0.1483 | 0.0000
    | 0.2926 | 0.2222 | 0.1658 |'
- en: '| CodeLlama-13B [[36](#bib.bib36)] | 0.3167 | 0.1611 | 0.0260 | 0.3021 | 0.3333
    | 0.2056 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-13B [[36](https://example.org/bib.bib36)] | 0.3167 | 0.1611 | 0.0260
    | 0.3021 | 0.3333 | 0.2056 |'
- en: '| VeriGen-6B [[37](#bib.bib37)] | 0.3667 | 0.1244 | 0.1000 | 0.2527 | 0.3333
    | 0.2026 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| VeriGen-6B [[37](https://example.org/bib.bib37)] | 0.3667 | 0.1244 | 0.1000
    | 0.2527 | 0.3333 | 0.2026 |'
- en: '| VeriGen-16B [[38](#bib.bib38)] | 0.3250 | 0.1189 | 0.0556 | 0.2771 | 0.3333
    | 0.1962 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| VeriGen-16B [[38](https://example.org/bib.bib38)] | 0.3250 | 0.1189 | 0.0556
    | 0.2771 | 0.3333 | 0.1962 |'
- en: '| GPT-3.5 [[39](#bib.bib39)] | 0.3083 | 0.1343 | 0.1600 | 0.2526 | 0.3333 |
    0.2201 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 [[39](https://example.org/bib.bib39)] | 0.3083 | 0.1343 | 0.1600
    | 0.2526 | 0.3333 | 0.2201 |'
- en: '| GPT-4 [[40](#bib.bib40)] | 0.3750 | 0.1644 | 0.0795 | 0.2657 | 0.3333 | 0.2107
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 [[40](https://example.org/bib.bib40)] | 0.3750 | 0.1644 | 0.0795 |
    0.2657 | 0.3333 | 0.2107 |'
- en: 'Multiple LLM responses are generated for each module, which are all then checked
    for functionality. For all given functional solutions, the responses are checked
    to see if the solution utilizes the smaller modules (as opposed to a single modular
    solution). If any of the responses for a given prompt are both functional and
    utilize the smaller modules, it is considered a positive instance of elaboration.
    Given $p$ have at least one response that demonstrates elaboration, the metric
    is specified as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块生成多个LLM响应，然后检查其功能性。对于所有给定的功能性解决方案，响应被检查是否利用了较小的模块（而不是单个模块化解决方案）。如果任何响应对于给定提示既功能性良好又利用了较小的模块，则视为详细说明的积极实例。给定$p$至少有一个响应展示了详细说明，度量值定义为：
- en: '|  | $E=\left(\frac{n}{p}\right)$ |  | (5) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $E=\left(\frac{n}{p}\right)$ |  | (5) |'
- en: 'III-E Creativity: Putting It All Together'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 创造力：汇总
- en: Given each of the subcategories associated with creativity defined above, the
    metrics are then combined to define the overall creativity of a given LLM in Verilog
    hardware design.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述定义的与创造力相关的每个子类别，这些指标随后被组合以定义给定LLM在Verilog硬件设计中的整体创造力。
- en: '|  | $C=(0.25)F+(0.25)X+(0.25)O+(0.25)E$ |  | (6) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $C=(0.25)F+(0.25)X+(0.25)O+(0.25)E$ |  | (6) |'
- en: IV Experimental Evaluation
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验评估
- en: IV-A Experimental Setup
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 实验设置
- en: We evaluate multiple LLMs using the CreativEval framework, including CodeLlama
    7B [[35](#bib.bib35)] and 13B [[36](#bib.bib36)] parameter models, VeriGen 6B
    [[37](#bib.bib37)] and 16B [[38](#bib.bib38)] (16B model loaded in 8-bit quantization
    due to memory constraints), GPT-3.5 [[39](#bib.bib39)], and GPT-4 [[40](#bib.bib40)].
    The inference process of the VeriGen and CodeLlama models was performed locally
    on an NVIDIA A100 GPU with 80 GB RAM, while GPT-3.5/4 were queried through the
    OpenAI Python API. All scripts are written in Python 3.10, with Icarus Verilog
    10.3 as the simulator for evaluating functionality checks. The open-source GNN4IP
    repository was adapted to this framework to generate the similarity scores. The
    prompt dataset utilized for functionality, fluency, and originality consists of
    111 single-module HDLBits [[41](#bib.bib41)] prompts sourced through AutoChip [[42](#bib.bib42)],
    each containing a correctly implemented solution and testbench. The smaller prompt
    set used for elaboration contains 9 separate multi-module prompts from the same
    source. The base functionality metric (pass@10) is measured on all 120 prompts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用CreativEval框架评估了多个LLM，包括CodeLlama 7B [[35](#bib.bib35)] 和 13B [[36](#bib.bib36)]
    参数模型，VeriGen 6B [[37](#bib.bib37)] 和 16B [[38](#bib.bib38)]（由于内存限制，16B模型以8位量化加载），GPT-3.5
    [[39](#bib.bib39)] 和 GPT-4 [[40](#bib.bib40)]。VeriGen和CodeLlama模型的推理过程在具有80GB
    RAM的NVIDIA A100 GPU上本地执行，而GPT-3.5/4则通过OpenAI Python API查询。所有脚本均使用Python 3.10编写，Icarus
    Verilog 10.3作为功能检查的模拟器。开源GNN4IP库被调整以适应此框架以生成相似度分数。用于功能、流畅性和原创性的提示数据集包含111个来自AutoChip
    [[42](#bib.bib42)] 的单模块HDLBits [[41](#bib.bib41)] 提示，每个提示包含一个正确实现的解决方案和测试台。用于详细说明的较小提示集包含9个来自同一来源的单独多模块提示。基础功能指标（pass@10）在所有120个提示上进行测量。
- en: 'When generating LLM responses in all experiments, the LLMs were all set to
    the following inference hyperparameters: temperature=0.3; max_tokens=1024; top_k=10;
    top_p=0.95\. All responses were trimmed to the first generated instance of “endmodule”
    for effective functionality evaluation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有实验中生成LLM响应时，所有LLM都设置为以下推理超参数：temperature=0.3；max_tokens=1024；top_k=10；top_p=0.95。所有响应都被修剪到第一个生成的“endmodule”实例，以进行有效的功能评估。
- en: IV-B Results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 结果
- en: 'Table [I](#S3.T1 "TABLE I ‣ III-D Elaboration ‣ III CreativEval Framework ‣
    CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation") summarizes
    the results for all LLMs for all subcategories of creativity. In evaluating fluency,
    GPT-4 had the highest quantity of separate and correct Verilog solutions to a
    module (with respect to the modules that have at least one correct solution),
    with CodeLlama-13B achieving similar results. The VeriGen models comparatively
    struggled in this metric, partly due to repeated generations of similar implementations
    instead of different implementations.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [I](#S3.T1 "TABLE I ‣ III-D Elaboration ‣ III CreativEval Framework ‣ CreativEval:
    Evaluating Creativity of LLM-Based Hardware Code Generation")总结了所有LLM在所有创造力子类别中的结果。在流畅性评估中，GPT-4在模块的单独正确Verilog解决方案的数量上最高（相对于至少有一个正确解决方案的模块），CodeLlama-13B也达到了类似的结果。VeriGen模型在这一指标上表现相对较差，部分原因是生成了重复的类似实现而非不同实现。'
- en: Regarding flexibility, GPT-3.5 had the highest rate of generating alternative
    solutions to provided modules across most models. The models that struggled (e.g.,
    CodeLlama) produced results that were often direct copies of the provided module,
    indicating the ability to understand the prompt’s natural language description
    as an important factor that determined flexibility.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 关于灵活性，GPT-3.5在大多数模型中生成提供模块的替代解决方案的比率最高。那些表现不佳的模型（例如CodeLlama）产生的结果往往是提供模块的直接复制，这表明理解提示的自然语言描述是决定灵活性的一个重要因素。
- en: As for originality, the GPT models had slightly worse performance than the others,
    with CodeLlama performing best. This means that the successful solutions provided
    with the GPT models were, on average, closer to the ideal solution. This could
    be due to its large size and training dataset, resulting in a more direct retrieval
    of existing solutions or coding practices.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 就原创性而言，GPT 模型的表现略逊于其他模型，其中 CodeLlama 表现最佳。这意味着 GPT 模型提供的成功解决方案通常更接近理想解决方案。这可能是由于其庞大的规模和训练数据集，导致对现有解决方案或编码实践的更直接检索。
- en: Elaboration was largely similar for all modules, as the HDLBits dataset for
    this metric is comparatively small (9 modules). The models primarily excelled
    in correctly connecting the input and output parameters between separate modules,
    while struggling to generate the larger module solution.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有模块的阐述基本相似，因为该指标的 HDLBits 数据集相对较小（9 个模块）。这些模型主要在正确连接不同模块之间的输入和输出参数方面表现出色，但在生成较大模块解决方案时表现不佳。
- en: Overall, the GPT models were the most creative, with GPT-3.5 as the best, and
    CodeLlama-7B was the least creative. Creativity is shown to slightly drop for
    the larger model sizes of GPT and VeriGen.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，GPT 模型是最具创造力的，其中 GPT-3.5 最佳，而 CodeLlama-7B 最不具创造力。创造力在 GPT 和 VeriGen 的大型模型中略有下降。
- en: V Conclusion
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: Recent studies on LLMs regarding their applications to hardware design have
    effectively demonstrated their potential, applying many optimization strategies
    to increase the performance in terms of functional correctness. However, these
    studies do not investigate the creativity associated with LLMs in their ability
    to generate solutions, largely due to the lack of an effective metric. Within
    this work, we propose CreativEval, a framework to evaluate the creativity of LLMs
    in generating hardware code. By evaluating multiple popular LLMs within this framework,
    we perform a comparative analysis, concluding that GPT-3.5 had the greatest creativity.
    Future research in this direction can further evaluate more LLMs and on larger
    prompt sets.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于大型语言模型（LLMs）在硬件设计应用中的研究有效地展示了它们的潜力，通过应用许多优化策略来提高功能正确性。然而，这些研究并未探讨 LLMs 在生成解决方案方面的创造力，这主要由于缺乏有效的评估指标。在这项工作中，我们提出了
    CreativEval，一个评估 LLMs 生成硬件代码创造力的框架。通过在此框架内评估多个流行的 LLMs，我们进行了一项比较分析，得出结论 GPT-3.5
    具有最大的创造力。未来的研究可以进一步评估更多的 LLMs 和更大的提示集。
- en: Acknowledgment
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors acknowledge the support from the Purdue Center for Secure Microelectronics
    Ecosystem – CSME#210205\. This work was also partially supported by the National
    Science Foundation (NSF CNS–1822848 and NSF DGE–2039610).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢普渡大学安全微电子生态系统中心（CSME#210205）的支持。这项工作还部分得到了国家科学基金会（NSF CNS–1822848 和 NSF
    DGE–2039610）的资助。
- en: References
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Tim Keary, “12 Practical Large Language Model (LLM) Applications,” [https://www.techopedia.com/12-practical-large-language-model-llm-applications](https://www.techopedia.com/12-practical-large-language-model-llm-applications),
    2023, [Online; last accessed 21-Nov-2023].'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Tim Keary, “12 种实用的大型语言模型（LLM）应用，” [https://www.techopedia.com/12-practical-large-language-model-llm-applications](https://www.techopedia.com/12-practical-large-language-model-llm-applications)，2023年，[在线；最后访问时间
    2023年11月21日]。'
- en: '[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2023.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, 和 I. Polosukhin, “注意力机制是你所需要的一切，” 2023年。'
- en: '[3] Z. G. Cai, X. Duan, D. A. Haslett, S. Wang, and M. J. Pickering, “Do large
    language models resemble humans in language use?” 2024.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Z. G. Cai, X. Duan, D. A. Haslett, S. Wang, 和 M. J. Pickering, “大型语言模型在语言使用上是否类似于人类？”
    2024年。'
- en: '[4] T. Kocmi and C. Federmann, “Large language models are state-of-the-art
    evaluators of translation quality,” 2023.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Kocmi 和 C. Federmann, “大型语言模型是最先进的翻译质量评估工具，” 2023年。'
- en: '[5] K. Pandya and M. Holia, “Automating customer service using langchain: Building
    custom open-source gpt chatbot for organizations,” 2023.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] K. Pandya 和 M. Holia, “使用 langchain 自动化客户服务：为组织构建自定义开源 GPT 聊天机器人，” 2023年。'
- en: '[6] R. Zhong, X. Du, S. Kai, Z. Tang, S. Xu, H.-L. Zhen, J. Hao, Q. Xu, M. Yuan,
    and J. Yan, “Llm4eda: Emerging progress in large language models for electronic
    design automation,” *arXiv preprint arXiv:2401.12224*, 2023.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] R. Zhong, X. Du, S. Kai, Z. Tang, S. Xu, H.-L. Zhen, J. Hao, Q. Xu, M.
    Yuan, 和 J. Yan, “Llm4eda: 大型语言模型在电子设计自动化中的新进展，” *arXiv 预印本 arXiv:2401.12224*，2023年。'
- en: '[7] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, and
    S. Garg, “Verigen: A large language model for verilog code generation,” 2023.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, 和 S.
    Garg， “Verigen：一个用于 Verilog 代码生成的大型语言模型，” 2023年。'
- en: '[8] J. Blocklove, S. Garg, R. Karri, and H. Pearce, “Chip-chat: Challenges
    and opportunities in conversational hardware design,” in *2023 ACM/IEEE 5th Workshop
    on Machine Learning for CAD (MLCAD)*.   IEEE, Sep. 2023\. [Online]. Available:
    [http://dx.doi.org/10.1109/MLCAD58807.2023.10299874](http://dx.doi.org/10.1109/MLCAD58807.2023.10299874)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Blocklove, S. Garg, R. Karri, 和 H. Pearce， “Chip-chat：对话硬件设计中的挑战与机遇，”
    在 *2023 ACM/IEEE 第五届计算机辅助设计（MLCAD）机器学习研讨会* 中。 IEEE，2023年9月。[在线]. 可用： [http://dx.doi.org/10.1109/MLCAD58807.2023.10299874](http://dx.doi.org/10.1109/MLCAD58807.2023.10299874)'
- en: '[9] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang *et al.*,
    “Chipnemo: Domain-adapted llms for chip design,” 2024.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang *等*， “Chipnemo：适用于芯片设计的领域适应
    LLMs，” 2024年。'
- en: '[10] M. DeLorenzo, A. B. Chowdhury, V. Gohil, S. Thakur, R. Karri, S. Garg,
    and J. Rajendran, “Make every move count: Llm-based high-quality rtl code generation
    using mcts,” 2024.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. DeLorenzo, A. B. Chowdhury, V. Gohil, S. Thakur, R. Karri, S. Garg,
    和 J. Rajendran， “让每一步都值得：基于 LLM 的高质量 RTL 代码生成，使用 MCTS，” 2024年。'
- en: '[11] M. Runco and G. Jaeger, “The standard definition of creativity,” *Creativity
    Research Journal - CREATIVITY RES J*, vol. 24, pp. 92–96, 01 2012.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Runco 和 G. Jaeger， “创造力的标准定义，” *创造力研究杂志 - CREATIVITY RES J*，第24卷，第92–96页，2012年1月。'
- en: '[12] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman
    *et al.*, “Gpt-4 technical report,” 2024.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman
    *等*， “GPT-4 技术报告，” 2024年。'
- en: '[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding,” in *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*.   Minneapolis,
    Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171–4186\.
    [Online]. Available: [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova， “BERT：深度双向变换器的预训练用于语言理解，”
    在 *2019年北美计算语言学协会人类语言技术会议论文集：长篇和短篇论文* 中。 明尼阿波利斯，明尼苏达州：计算语言学协会，2019年6月，第4171–4186页。[在线].
    可用： [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423)'
- en: '[14] [Online]. Available: [https://www.anthropic.com/news/claude-3-haiku](https://www.anthropic.com/news/claude-3-haiku)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] [在线]. 可用： [https://www.anthropic.com/news/claude-3-haiku](https://www.anthropic.com/news/claude-3-haiku)'
- en: '[15] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,
    and D. Jiang, “Wizardcoder: Empowering code large language models with evol-instruct,”
    2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,
    和 D. Jiang， “Wizardcoder：通过 evol-instruct 赋能代码大型语言模型，” 2023年。'
- en: '[16] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan
    *et al.*, “Evaluating large language models trained on code,” 2021.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan
    *等*， “评估基于代码训练的大型语言模型，” 2021年。'
- en: '[17] Y. Wang, H. Le, A. D. Gotmare, N. D. Q. Bui, J. Li, and S. C. H. Hoi,
    “Codet5+: Open code large language models for code understanding and generation,”
    2023.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Wang, H. Le, A. D. Gotmare, N. D. Q. Bui, J. Li, 和 S. C. H. Hoi， “Codet5+：用于代码理解和生成的开放代码大型语言模型，”
    2023年。'
- en: '[18] H. Pearce, B. Tan, and R. Karri, “Dave: Deriving automatically verilog
    from english,” in *Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning
    for CAD*, ser. MLCAD ’20.   New York, NY, USA: Association for Computing Machinery,
    2020, p. 27–32\. [Online]. Available: [https://doi.org/10.1145/3380446.3430634](https://doi.org/10.1145/3380446.3430634)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] H. Pearce, B. Tan, 和 R. Karri， “Dave：自动从英文推导 Verilog，” 在 *2020 ACM/IEEE
    机器学习用于 CAD 研讨会论文集* 中，系列 MLCAD ’20。 纽约，美国：计算机协会，2020年，第27–32页。[在线]. 可用： [https://doi.org/10.1145/3380446.3430634](https://doi.org/10.1145/3380446.3430634)'
- en: '[19] M. Liu, N. Pinckney, B. Khailany, and H. Ren, “Verilogeval: Evaluating
    large language models for verilog code generation,” 2023.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. Liu, N. Pinckney, B. Khailany, 和 H. Ren， “Verilogeval：评估大型语言模型在 Verilog
    代码生成中的表现，” 2023年。'
- en: '[20] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and Z. Xie, “Rtlcoder: Outperforming
    gpt-3.5 in design rtl generation with our open-source dataset and lightweight
    solution,” 2024.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, 和 Z. Xie， “Rtlcoder：在设计 RTL
    生成中超越 GPT-3.5，借助我们的开源数据集和轻量级解决方案，” 2024年。'
- en: '[21] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, “Rtllm: An open-source benchmark
    for design rtl generation with large language model,” 2023.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Lu, S. Liu, Q. Zhang, 和 Z. Xie， “Rtllm：一个开源基准，用于大型语言模型的设计 RTL 生成，”
    2023年。'
- en: '[22] L. S. Almeida, L. P. Prieto, M. Ferrando, E. Oliveira, and C. Ferrándiz,
    “Torrance test of creative thinking: The question of its construct validity,”
    *Thinking Skills and Creativity*, vol. 3, no. 1, pp. 53–58, 2008\. [Online]. Available:
    [https://www.sciencedirect.com/science/article/pii/S1871187108000072](https://www.sciencedirect.com/science/article/pii/S1871187108000072)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] L. S. Almeida, L. P. Prieto, M. Ferrando, E. Oliveira, 和 C. Ferrándiz,
    “托伦斯创造性思维测试：其构建有效性的问题，” *思维技能与创造力*，第3卷，第1期，第53–58页，2008年。 [在线]. 可用： [https://www.sciencedirect.com/science/article/pii/S1871187108000072](https://www.sciencedirect.com/science/article/pii/S1871187108000072)'
- en: '[23] S. L. Doerr, “Conjugate lateral eye movement, cerebral dominance, and
    the figural creativity factors of fluency, flexibility, originality, and elaboration,”
    *Studies in Art Education*, vol. 21, no. 3, pp. 5–11, 1980\. [Online]. Available:
    [http://www.jstor.org/stable/1319788](http://www.jstor.org/stable/1319788)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. L. Doerr, “共轭侧向眼动、大脑主导性以及流畅性、灵活性、独创性和详细性这几个图形创造力因素，” *艺术教育研究*，第21卷，第3期，第5–11页，1980年。
    [在线]. 可用： [http://www.jstor.org/stable/1319788](http://www.jstor.org/stable/1319788)'
- en: '[24] J. P. Guilford, *The nature of human intelligence*.   McGraw-Hill, 1971.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. P. Guilford, *人类智力的本质*。 McGraw-Hill，1971年。'
- en: '[25] E. P. Torrance, “Torrance tests of creative thinking,” *Educational and
    psychological measurement*, 1966.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] E. P. Torrance, “托伦斯创造性思维测试，” *教育与心理测量*，1966年。'
- en: '[26] M. Arefi, “Comparation of creativity dimensions (fluency, flexibility,
    elaboration, originality) between bilingual elementary students (azari language-kurdish
    language) in urmia city iran - the iafor research archive,” Dec 2018\. [Online].
    Available: [https://papers.iafor.org/submission22045/](https://papers.iafor.org/submission22045/)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Arefi, “乌尔米亚市伊朗双语小学生（阿扎里语-库尔德语）创造力维度（流畅性、灵活性、详细性、独创性）的比较 - IAfor 研究档案，”
    2018年12月。 [在线]. 可用： [https://papers.iafor.org/submission22045/](https://papers.iafor.org/submission22045/)'
- en: '[27] S. A. Handayani, Y. S. Rahayu, and R. Agustini, “Students’ creative thinking
    skills in biology learning: fluency, flexibility, originality, and elaboration,”
    *Journal of Physics: Conference Series*, vol. 1747, no. 1, p. 012040, feb 2021\.
    [Online]. Available: [https://dx.doi.org/10.1088/1742-6596/1747/1/012040](https://dx.doi.org/10.1088/1742-6596/1747/1/012040)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. A. Handayani, Y. S. Rahayu, 和 R. Agustini, “学生在生物学习中的创造性思维技能：流畅性、灵活性、独创性和详细性，”
    *物理学杂志：会议系列*，第1747卷，第1期，第012040页，2021年2月。 [在线]. 可用： [https://dx.doi.org/10.1088/1742-6596/1747/1/012040](https://dx.doi.org/10.1088/1742-6596/1747/1/012040)'
- en: '[28] F. Alacapinar, “Grade level and creativity,” *Eurasian Journal of Educational
    Research (EJER)*, vol. 13, pp. 247–266, 01 2012.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] F. Alacapinar, “年级水平与创造力，” *欧亚教育研究期刊（EJER）*，第13卷，第247–266页，2012年1月。'
- en: '[29] M. Arefi and N. Jalali, “Comparation of creativity dimensions (fluency,
    flexibility, elaboration, originality) between bilingual elementary students (azari
    language-kurdish language) in urmia city–iran,” in *The IAFOR International Conference
    on Language Learning*, 2016.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Arefi 和 N. Jalali, “乌尔米亚市伊朗双语小学生（阿扎里语-库尔德语）创造力维度（流畅性、灵活性、详细性、独创性）的比较，”
    见 *IAFOR 国际语言学习会议*，2016年。'
- en: '[30] R. Shiffrin and M. Mitchell, Mar 2023\. [Online]. Available: [https://www.pnas.org/doi/abs/10.1073/pnas.2300963120](https://www.pnas.org/doi/abs/10.1073/pnas.2300963120)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] R. Shiffrin 和 M. Mitchell, 2023年3月。 [在线]. 可用： [https://www.pnas.org/doi/abs/10.1073/pnas.2300963120](https://www.pnas.org/doi/abs/10.1073/pnas.2300963120)'
- en: '[31] C. Stevenson, I. Smal, M. Baas, R. Grasman, and H. van der Maas, “Putting
    gpt-3’s creativity to the (alternative uses) test,” 2022.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Stevenson, I. Smal, M. Baas, R. Grasman, 和 H. van der Maas, “将 GPT-3
    的创造力进行（替代用途）测试，” 2022年。'
- en: '[32] M. Binz and E. Schulz, “Using cognitive psychology to understand gpt-3,”
    *Proceedings of the National Academy of Sciences*, vol. 120, no. 6, Feb. 2023\.
    [Online]. Available: [http://dx.doi.org/10.1073/pnas.2218523120](http://dx.doi.org/10.1073/pnas.2218523120)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Binz 和 E. Schulz, “利用认知心理学理解 GPT-3，” *国家科学院院刊*，第120卷，第6期，2023年2月。 [在线].
    可用： [http://dx.doi.org/10.1073/pnas.2218523120](http://dx.doi.org/10.1073/pnas.2218523120)'
- en: '[33] Y. Zhao, R. Zhang, W. Li, D. Huang, J. Guo, S. Peng, Y. Hao, Y. Wen, X. Hu,
    Z. Du, Q. Guo, L. Li, and Y. Chen, “Assessing and understanding creativity in
    large language models,” 2024.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Zhao, R. Zhang, W. Li, D. Huang, J. Guo, S. Peng, Y. Hao, Y. Wen, X.
    Hu, Z. Du, Q. Guo, L. Li, 和 Y. Chen, “评估和理解大型语言模型中的创造力”，2024。'
- en: '[34] R. Yasaei, S.-Y. Yu, E. K. Naeini, and M. A. A. Faruque, “Gnn4ip: Graph
    neural network for hardware intellectual property piracy detection,” 2021.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] R. Yasaei, S.-Y. Yu, E. K. Naeini, 和 M. A. A. Faruque, “GNN4IP：用于硬件知识产权盗版检测的图神经网络，”
    2021年。'
- en: '[35] “Hugging face.” [Online]. Available: [https://huggingface.co/codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] “Hugging face.” [在线]. 可用： [https://huggingface.co/codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)'
- en: '[36] “Hugging face.” [Online]. Available: [https://huggingface.co/codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] “Hugging face.” [在线]. 可用： [https://huggingface.co/codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf)'
- en: '[37] “Hugging face.” [Online]. Available: [https://huggingface.co/shailja/fine-tuned-codegen-6B-Verilog](https://huggingface.co/shailja/fine-tuned-codegen-6B-Verilog)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] “Hugging face.” [在线]. 可用： [https://huggingface.co/shailja/fine-tuned-codegen-6B-Verilog](https://huggingface.co/shailja/fine-tuned-codegen-6B-Verilog)'
- en: '[38] “Hugging face.” [Online]. Available: [https://huggingface.co/shailja/fine-tuned-codegen-16B-Verilog](https://huggingface.co/shailja/fine-tuned-codegen-16B-Verilog)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] “Hugging face.” [在线]. 可用： [https://huggingface.co/shailja/fine-tuned-codegen-16B-Verilog](https://huggingface.co/shailja/fine-tuned-codegen-16B-Verilog)'
- en: '[39] “fine-tuning and api updates.” [Online]. Available: [https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] “微调和 API 更新。” [在线]. 可用： [https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates)'
- en: '[40] “fine-tuning and api updates.” [Online]. Available: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] “微调和 API 更新。” [在线]. 可用： [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)'
- en: '[41] [Online]. Available: [https://hdlbits.01xz.net/wiki/Main_Page](https://hdlbits.01xz.net/wiki/Main_Page)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] [在线]. 可用： [https://hdlbits.01xz.net/wiki/Main_Page](https://hdlbits.01xz.net/wiki/Main_Page)'
- en: '[42] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, and R. Karri, “Autochip:
    Automating hdl generation using llm feedback,” 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. Thakur, J. Blocklove, H. Pearce, B. Tan, S. Garg, 和 R. Karri, “Autochip:
    利用 LLM 反馈自动生成 HDL,” 2023。'
