- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:51:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:35
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件感知并行提示解码用于内存高效加速LLM推理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18628](https://ar5iv.labs.arxiv.org/html/2405.18628)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18628](https://ar5iv.labs.arxiv.org/html/2405.18628)
- en: \newfloatcommand
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newfloatcommand
- en: capbtabboxtable[][\FBwidth]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: capbtabboxtable[][\FBwidth]
- en: Hao (Mark) Chen¹  Wayne Luk¹  Ka Fai Cedric Yiu²
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Hao (Mark) Chen¹  Wayne Luk¹  Ka Fai Cedric Yiu²
- en: Rui Li³  Konstantin Mishchenko³  Stylianos I. Venieris³  Hongxiang Fan^(1,3)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Rui Li³  Konstantin Mishchenko³  Stylianos I. Venieris³  Hongxiang Fan^(1,3)
- en: ¹Imperial College London, UK
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹帝国理工学院，英国
- en: ²Hong Kong Polytechnic University, Hong Kong
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ²香港理工大学，香港
- en: ³Samsung AI Center, Cambridge, UK
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ³三星AI中心，剑桥，英国
- en: '{hc1620,w.luk}@ic.ac.uk  {rui.li,s.venieris}@samsung.com'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '{hc1620,w.luk}@ic.ac.uk  {rui.li,s.venieris}@samsung.com'
- en: konsta.mish@gmail.com  cedric.yiu@polyu.edu.hk  hongxiangfan@ieee.org
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: konsta.mish@gmail.com  cedric.yiu@polyu.edu.hk  hongxiangfan@ieee.org
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The auto-regressive decoding of Large Language Models (LLMs) results in significant
    overheads in their hardware performance. While recent research has investigated
    various speculative decoding techniques for multi-token generation, these efforts
    have primarily focused on improving processing speed such as throughput. Crucially,
    they often neglect other metrics essential for real-life deployments, such as
    memory consumption and training cost. To overcome these limitations, we propose
    a novel parallel prompt decoding that requires only $0.0002$ further speed improvement.
    Our code is available at [https://github.com/hmarkc/parallel-prompt-decoding](https://github.com/hmarkc/parallel-prompt-decoding).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的自回归解码会导致硬件性能显著的开销。虽然近期研究调查了多标记生成的各种推测性解码技术，这些努力主要集中在提高处理速度，如吞吐量。然而，它们往往忽视了实际部署中其他重要的指标，如内存消耗和训练成本。为了克服这些局限性，我们提出了一种新颖的并行提示解码方法，只需额外$0.0002$的速度提升。我们的代码可在[https://github.com/hmarkc/parallel-prompt-decoding](https://github.com/hmarkc/parallel-prompt-decoding)找到。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/fb1c33fd97409e332a49f9513a4a43fb.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb1c33fd97409e332a49f9513a4a43fb.png)'
- en: 'Figure 1: Comparison of memory, speedup, and training cost on MT-Bench with
    Vicuna-7B. Circle diameter shows training GPU hours.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：MT-Bench上Vicuna-7B的内存、加速和训练成本比较。圆圈直径表示训练GPU小时数。
- en: The recent advances in large language models (LLMs) are increasingly gaining
    influence across various AI applications. However, autoregressive generation,
    the de facto approach employed in LLM inference, suffers from inadequate hardware
    performance due to its inherent sequential nature [[23](#bib.bib23)]. Speculative
    decoding [[13](#bib.bib13), [2](#bib.bib2), [11](#bib.bib11)], an emerging acceleration
    technique, employs a guess-and-verify framework for LLM inference, where a smaller
    draft model first predicts multiple tokens sequentially and then the original
    LLM verifies them in parallel. Despite its potential, the effectiveness of speculative
    decoding is limited by the complexity and cost of training a draft model capable
    of consistently achieving high acceptance rates across diverse base models and
    datasets. Additionally, the extra runtime memory overhead for executing draft
    models poses a significant barrier to the broader adoption of speculative decoding,
    particularly in edge and mobile environments where memory capacity is limited.
    Considering the growing need for user privacy and personalization, deploying LLMs
    on devices urges a more memory- and cost-efficient solution for accelerating LLM
    inference. Recent efforts have explored the possibility of generating multiple
    tokens in parallel without relying on a separate transformer draft model [[20](#bib.bib20)].
    Approaches such as inserting additional decoding heads [[1](#bib.bib1)] and retrieving
    frequently used tokens [[9](#bib.bib9)] are employed to enhance performance. However,
    these methods either aggressively assume conditional independence among the tokens
    generated in a single step [[1](#bib.bib1), [9](#bib.bib9)], or use placeholder
    tokens (e.g., [PAD] token) that do not convey enough contextual information [[20](#bib.bib20)].
    Therefore, they often suffer from low acceptance rates or degradation in output
    quality due to the lack of sufficient conditional information during inference.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）的进展在各种AI应用中正越来越有影响力。然而，作为LLM推理中实际采用的方法的自回归生成，由于其固有的顺序性质，受到了硬件性能不足的困扰[[23](#bib.bib23)]。一种新兴的加速技术——**推测解码**[[13](#bib.bib13),
    [2](#bib.bib2), [11](#bib.bib11)]，采用了一个猜测和验证的框架来进行LLM推理，其中一个较小的草稿模型首先顺序预测多个令牌，然后由原始LLM进行并行验证。尽管具有潜力，推测解码的有效性受到训练一个能够在多样的基础模型和数据集上始终保持高接受率的草稿模型的复杂性和成本的限制。此外，执行草稿模型所需的额外运行时内存开销也是推测解码更广泛采用的一个重大障碍，特别是在内存容量有限的边缘和移动环境中。考虑到对用户隐私和个性化需求的日益增长，将LLMs部署到设备上促使了对加速LLM推理的更高效内存和成本解决方案的需求。最近的努力探索了在不依赖于单独的变换器草稿模型的情况下并行生成多个令牌的可能性[[20](#bib.bib20)]。一些方法如插入额外的解码头[[1](#bib.bib1)]和检索常用令牌[[9](#bib.bib9)]被用来提高性能。然而，这些方法要么激进地假设在单步生成中令牌之间的条件独立[[1](#bib.bib1),
    [9](#bib.bib9)]，要么使用无法传达足够上下文信息的占位符令牌（例如，[PAD]令牌）[[20](#bib.bib20)]。因此，它们通常由于在推理过程中缺乏足够的条件信息而遭受低接受率或输出质量下降的问题。
- en: To alleviate the complexity and overhead associated with the use of draft models
    while maintaining a high acceptance rate, we propose Parallel Prompt Decoding
    (PPD), a novel architecture-agnostic and memory-efficient framework that adopts
    prompt tuning for non-autoregressive LLM inference. Inspired by the human natural
    language generation process where continuous words like common expressions and
    phrases are produced simultaneously, PPD introduces the use of prompt tokens,
    the meticulously trained embeddings, for multi-token prediction. Specifically,
    these trained prompt tokens are appended to the original input sequence in parallel,
    enabling the concurrent generation of multiple output tokens in a single forward
    pass. The key intuition of PPD lies in the observation that if trained properly,
    prompt tokens appended to the input can approximate tokens generated at future
    timesteps, thereby partially recovering the missing conditional dependency information
    for multi-token generation. By strategically positioning trained prompt tokens,
    PPD achieves up to a 28% higher acceptance rate when predicting long-range tokens.
    To further increase the token acceptance rate, we generate multiple candidate
    continuations with each prompt token and use them in combination with a customized
    tree attention mask to minimize the computation and memory overhead. The capability
    of PPD to use low-cost prompt tokens for accurate multi-token prediction forms
    the foundation for accelerating LLM inference. As shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference"), PPD achieves a comparable speedup to the state-of-the-art
    speculative decoding approaches with negligible memory overhead and reduced training
    cost. Moreover, to facilitate the optimized implementation of PPD across different
    hardware platforms, we propose a hardware-aware dynamic sparse tree technique
    that adaptively refines the prompt structure during runtime based on the computational
    resources available on the specific hardware.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻草稿模型使用中的复杂性和开销，同时保持高接受率，我们提出了并行提示解码（PPD），这是一种新颖的架构无关且内存高效的框架，采用提示调优进行非自回归LLM推理。受到人类自然语言生成过程的启发，其中连续的词汇，如常见表达和短语同时生成，PPD引入了精心训练的提示标记用于多标记预测。具体而言，这些训练的提示标记以并行的方式附加到原始输入序列中，实现了在一次前向传播中同时生成多个输出标记。PPD的关键直觉在于观察到，如果训练得当，附加到输入的提示标记可以近似未来时间步生成的标记，从而部分恢复多标记生成的缺失条件依赖信息。通过战略性地定位训练过的提示标记，PPD在预测长距离标记时可实现高达28%的接受率提升。为了进一步提高标记接受率，我们生成多个候选续集与每个提示标记，并结合定制的树注意力掩码来最小化计算和内存开销。PPD利用低成本提示标记进行准确的多标记预测，形成了加速LLM推理的基础。如图[1](#S1.F1
    "图1 ‣ 1 引言 ‣ 硬件感知并行提示解码用于内存高效加速LLM推理")所示，PPD在速度上与最先进的推测解码方法相当，同时内存开销微乎其微，训练成本降低。此外，为了促进PPD在不同硬件平台上的优化实现，我们提出了一种硬件感知的动态稀疏树技术，该技术在运行时根据特定硬件上的计算资源自适应地优化提示结构。
- en: '![Refer to caption](img/83cc2e570f9e9491521f830ba311fb6b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83cc2e570f9e9491521f830ba311fb6b.png)'
- en: 'Figure 2: Overview of PPD. The left section shows the location of trainable
    parameters and the middle section displays the combined guess-and-verify process
    during inference. The "prompt token" denotes the special token with separately
    trained embeddings to perform parallel prediction.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：PPD概述。左侧部分显示了可训练参数的位置，中间部分展示了推理过程中结合的猜测和验证过程。“prompt token”表示用于并行预测的特殊标记，其嵌入经过单独训练。
- en: To demonstrate the effectiveness of our approach, we evaluate PPD on MobileLLaMA
     [[6](#bib.bib6)], Vicuna-7b and Vicuna-13b [[5](#bib.bib5)]. Running on a single
    GPU using the A100-40GB and RTX 4090, our method achieves a speedup ratio for
    inference from 2.12$\times$ across a diverse range of popular datasets including
    MT-Bench, HumanEval, and GSM8K. Our experiments demonstrate that PPD not only
    achieves comparable throughput to the state-of-the-art speculative decoding method,
    but it also manages this with significantly fewer trainable parameters—specifically,
    0.0002% of trainable parameters—and incurs only a minimal memory overhead (0.0004%),
    showcasing that PPD is remarkably cost- and memory-efficient. The training of
    prompt tokens can be completed in 16 hours using one A100 GPU, 8 hours using four
    GeForce RTX 3090 GPUs, compared to the 1-2 days on four A100 GPUs required for
    Eagle [[16](#bib.bib16)]. Furthermore, since PPD does not require the modification
    of the original LLM or the addition of extra networks, it is highly adaptable
    and orthogonal to other decoding techniques. For instance, it can be effectively
    combined with a draft model to further reduce inference latency.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们方法的有效性，我们在MobileLLaMA  [[6](#bib.bib6)]、Vicuna-7b和Vicuna-13b [[5](#bib.bib5)]上评估了PPD。使用A100-40GB和RTX
    4090单GPU运行，我们的方法在包括MT-Bench、HumanEval和GSM8K在内的多种流行数据集上实现了推理速度提高2.12$\times$。我们的实验表明，PPD不仅在吞吐量上与最先进的推测解码方法相当，而且以显著更少的可训练参数（具体为0.0002%）实现，同时内存开销仅为0.0004%，展示了PPD在成本和内存效率方面的卓越表现。提示令牌的训练可以在使用一台A100
    GPU的情况下完成16小时，使用四台GeForce RTX 3090 GPU的情况下完成8小时，而Eagle [[16](#bib.bib16)]则需要四台A100
    GPU的1-2天。此外，由于PPD不需要修改原始LLM或添加额外的网络，它在适应性和与其他解码技术的兼容性上表现优异。例如，它可以与草稿模型有效结合，进一步减少推理延迟。
- en: 'Our contributions are summarized as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A novel Parallel Prompt Decoding (PPD) that adopts cost-effective prompt tokens
    for non-autoregressive LLM inference, achieving a high acceptance rate for long-distance
    token prediction with preserved output quality.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种新颖的并行提示解码（PPD）方法，采用具有成本效益的提示令牌用于非自回归LLM推理，实现了对长距离令牌预测的高接受率，同时保持了输出质量。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A hardware-aware dynamic sparse tree technique that adaptively optimizes the
    prompt structure of PPD at runtime based on the available compute and memory resources,
    facilitating its efficient deployment on various hardware platforms.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种硬件感知的动态稀疏树技术，它根据可用的计算和内存资源在运行时自适应地优化PPD的提示结构，从而促进其在各种硬件平台上的高效部署。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An open-source implementation of PPD, accompanied by comprehensive evaluations
    on various models and benchmarks. Our experiments demonstrate that PPD achieves
    significant speed improvements with negligible memory overhead and reduced training
    cost.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个开源的PPD实现，并对各种模型和基准进行了全面评估。我们的实验表明，PPD在几乎没有内存开销和减少训练成本的情况下，实现了显著的速度提升。
- en: 2 Background and Related Work
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: To enhance the inference speed of LLM, various approaches adopt an iterative
    guess-and-verify strategy to enable multi-token generation. In the guessing phase,
    potential future tokens are proposed at a faster speed than in traditional autoregressive
    implementations. Subsequently, a parallelized verification process assesses which
    guessed tokens should be accepted. Depending on how tokens are generated during
    the guess stage, these approaches can generally be categorized as i) speculative
    decoding and ii) parallel decoding.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高LLM的推理速度，各种方法采用迭代猜测和验证策略以实现多令牌生成。在猜测阶段，潜在的未来令牌以比传统自回归实现更快的速度提出。随后，平行化的验证过程评估哪些猜测的令牌应被接受。根据令牌在猜测阶段的生成方式，这些方法通常可以分为
    i) 推测解码和 ii) 并行解码。
- en: 2.1 Speculative Decoding
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1 推测解码
- en: The guessing phase of speculative decoding adopts a lightweight draft model
    to generate multiple tokens at an increased speed [[11](#bib.bib11)]. During the
    verification stage, the original LLM subsequently determines the acceptance of
    the guessed tokens. It is worth noting that both draft and original models still
    follow the auto-regressive inference scheme. The speedup comes from two factors: i) the
    draft model runs much faster than the original model and more tokens can be generated
    within the same time unit; and ii) token verification is executed concurrently,
    either by batching or by incorporating multiple candidates into a single input
    using customized sparse attention masks [[18](#bib.bib18)]. Therefore, the overall
    speedup depends on the acceptance rate and the inference latency of draft models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 推测解码的猜测阶段采用轻量级的草稿模型以更高的速度生成多个标记[[11](#bib.bib11)]。在验证阶段，原始LLM随后确定猜测标记的接受情况。值得注意的是，草稿模型和原始模型仍然遵循自回归推理方案。加速的因素有两个：i)
    草稿模型运行速度远快于原始模型，因此在相同的时间单位内可以生成更多标记；ii) 标记验证是并行执行的，要么通过批处理，要么通过使用自定义的稀疏注意力掩码[[18](#bib.bib18)]
    将多个候选标记纳入单一输入。因此，总体加速取决于草稿模型的接受率和推理延迟。
- en: Building on the speculative decoding scheme, various studies have been conducted
    to further optimize its inference speed. To improve the accuracy of the draft
    model and its token acceptance rate, Eagle [[16](#bib.bib16)] incorporates the
    hidden features into the draft model’s forward pass. SpecInfer [[18](#bib.bib18)]
    adopts a tree-based speculative inference and verification scheme, improving the
    diversity of speculation candidates. Sequoia [[4](#bib.bib4)] optimizes the sparse
    tree structure by considering the capability of the underlying hardware platforms.
    However, most of these methods require the storage and maintenance of a separate
    draft model. Moreover, there is extra complexity in designing an efficient draft
    model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于推测解码方案，已经进行了各种研究以进一步优化其推理速度。为了提高草稿模型的准确性及其标记接受率，**Eagle**[[16](#bib.bib16)]
    将隐藏特征纳入草稿模型的前向传递中。**SpecInfer**[[18](#bib.bib18)] 采用基于树的推测推理和验证方案，提高了推测候选的多样性。**Sequoia**[[4](#bib.bib4)]
    通过考虑底层硬件平台的能力来优化稀疏树结构。然而，这些方法大多数需要存储和维护一个单独的草稿模型。此外，设计高效的草稿模型也增加了额外的复杂性。
- en: 2.2 Parallel Decoding
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2 并行解码
- en: To overcome the inherent limitations of autoregressive inference and the memory
    overhead associated with using a separate draft model, several attempts have been
    made to integrate both guessing and verification using one unified model. Medusa¹¹1We
    categorize Medusa as parallel decoding because it only adopts LM heads instead
    of separate models. [[1](#bib.bib1)] introduces language model (LM) heads at the
    final layer of the original LLM, facilitating the generation of multiple tokens
    in a single forward pass. It also utilizes tree attention masks in its verification
    process to increase speed even further. To enhance token drafting with retrieval-augmented
    generation [[10](#bib.bib10)], Rest [[9](#bib.bib9)] introduce retrieval-based
    decoding tailored for specific scenarios. Inspired by Jacobi decoding [[20](#bib.bib20)]
    that adopts multiple special tokens to accelerate machine translation, Lookahead
    Decoding [[8](#bib.bib8)] improves upon this method by generating parallel n-grams
    and employing a caching memory pool. To capture more information while using multiple
    special tokens at distinct positions, PaSS [[19](#bib.bib19)] trains additional
    tokens with embedding layers for parallel decoding. Hierarchical parallel decoding [[17](#bib.bib17)]
    introduces the use of $[Fork]$ tokens, enabling parallel execution of multiple
    structural subroutines.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服自回归推理固有的限制以及使用单独草稿模型所带来的内存开销，已经有多次尝试将猜测和验证整合到一个统一模型中。**Medusa**¹¹1我们将Medusa归类为并行解码，因为它只采用了语言模型头，而不是单独的模型。[[1](#bib.bib1)]
    在原始大型语言模型（LLM）的最终层引入了语言模型（LM）头，从而在一次前向传递中生成多个标记。它还利用树状注意力掩码来进一步提高速度。为了增强基于检索的生成[[10](#bib.bib10)]，**Rest**[[9](#bib.bib9)]
    引入了针对特定场景的基于检索的解码。受**Jacobi解码**[[20](#bib.bib20)]的启发，该方法采用多个特殊标记来加速机器翻译，**Lookahead
    Decoding**[[8](#bib.bib8)] 通过生成并行的n-gram并使用缓存内存池改进了这一方法。为了在使用多个特殊标记于不同位置时捕获更多信息，**PaSS**[[19](#bib.bib19)]
    训练了具有嵌入层的附加标记以实现并行解码。**Hierarchical parallel decoding**[[17](#bib.bib17)] 引入了使用$[Fork]$标记，从而实现了多个结构化子例程的并行执行。
- en: Our approach can be categorized as parallel decoding, with three novel features
    to distinguish it from other approaches: 1) PPD trains the embeddings of parameterized
    ensemble prompt tokens, 2) it utilizes a dynamic sparse tree, adapting its structure
    at every inference step, and 3) we propose a hardware-aware algorithm for designing
    a dynamic sparse tree tailored to each hardware platform.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法可以归类为并行解码，具有三个新颖的特征以区别于其他方法：1) PPD 训练参数化集合提示令牌的嵌入，2) 它利用动态稀疏树，在每个推理步骤中调整其结构，以及
    3) 我们提出了一种硬件感知算法，用于设计适应每个硬件平台的动态稀疏树。
- en: 3 Parallel Prompt Decoding (PPD)
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 并行提示解码 (PPD)
- en: 'PPD trains embeddings for prompt tokens rather than developing a separate model.
    Our method integrates three substeps into a single decoding step, following the
    guess-and-verify strategy: (1) candidate generation, where multiple candidate
    continuations²²2A candidate token, also referred to as a ”guess token”, is a draft
    token generated from a prompt token. are predicted by strategically inserting
    the prompt tokens into the input sequence. Tree attention [[18](#bib.bib18)] merges
    the processing of multiple candidates into a single forward pass; (2) candidate
    verification, where two verification schemes, exact matching [[8](#bib.bib8)]
    and typical acceptance [[1](#bib.bib1)], are implemented; (3) candidate acceptance,
    where validated candidates are integrated into the input and KV cache is updated
    accordingly. The inference scheme in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference") illustrates the generation and verification combined in a single
    forward pass.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: PPD 训练提示令牌的嵌入，而不是开发单独的模型。我们的方法将三个子步骤整合到一个解码步骤中，遵循猜测和验证策略：(1) 候选生成，其中通过战略性地将提示令牌插入输入序列来预测多个候选续集。树注意力
    [[18](#bib.bib18)] 将多个候选的处理合并为一个前向传递；(2) 候选验证，其中实施了两种验证方案，精确匹配 [[8](#bib.bib8)]
    和典型接受 [[1](#bib.bib1)]；(3) 候选接受，其中经过验证的候选者被整合到输入中，并更新 KV 缓存。图 [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") 中的推理方案展示了生成和验证合并在一个前向传递中。
- en: 3.1 Prompt Tokens
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 提示令牌
- en: The prompt tokens are the key component of PPD to realize multi-token generation.
    Initially introduced in [[12](#bib.bib12)] to adapt LLMs for specific tasks, prompt
    tokens are typically prepended to the input, with outputs generated in an autoregressive
    manner. In this work, we propose a novel approach of utilizing prompt tokens by
    strategically positioning them at locations where tokens are anticipated to be
    generated in parallel. For conventional parallel decoding techniques [[23](#bib.bib23),
    [1](#bib.bib1)] that presume complete conditional independence among tokens decoded
    in a single step, the exact conditional probability $p(y_{i+k+1}|x,y_{1:i+k})$
    represents the generated outputs so far, $k> is the prompt token with token
    distance  $ 是具有令牌距离  and punctuation.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机插入提示令牌：在输入序列中随机插入提示令牌，可以减少仅在序列末尾附加它们所带来的上下文偏差。这种方法拓宽了提示令牌的预测能力，超越了如和标点符号等有限词汇。
- en: 'Knowledge Distillation: To align the predictive behavior of prompt tokens with
    the original LLM, we employ knowledge distillation. Instead of using hard labels,
    prompt tokens are trained against the logits produced by the original LLM. Following
    Medusa [[1](#bib.bib1)], The loss function is formulated as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏：为了使提示令牌的预测行为与原始LLM对齐，我们采用知识蒸馏。我们不使用硬标签，而是将提示令牌与原始LLM生成的logits进行训练。参照Medusa[[1](#bib.bib1)]，损失函数被制定为：
- en: '|  | $L_{PD}=\frac{1}{N}\sum_{i=1}^{N}D_{KL}(P_{i}\parallel Q_{i})\cdot\alpha^{i-1},$
    |  | (1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{PD}=\frac{1}{N}\sum_{i=1}^{N}D_{KL}(P_{i}\parallel Q_{i})\cdot\alpha^{i-1},$
    |  | (1) |'
- en: where $D_{KL}$ is the corresponding distribution from the original LLM, and
    $\alpha$ is the decay ratio.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{KL}$ 是来自原始LLM的对应分布，$\alpha$ 是衰减比率。
- en: 4 Dynamic Sparse Tree
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 动态稀疏树
- en: 4.1 Motivation
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1 动机
- en: To achieve higher speedup, PPD utilizes a specialized tree attention [[1](#bib.bib1),
    [18](#bib.bib18)] to process multiple candidates within a single decoding step
    without expanding the batch size. Notably, PPD employs a sparse tree [[1](#bib.bib1),
    [4](#bib.bib4)], designed to prioritize candidates with higher prediction accuracy.
    One key distinction from the sparse tree used in previous works is the appending
    of a sequence of prompt tokens to each tree node as shown in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Motivation ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference"). To optimize the
    amortized acceptance length across decoding steps, it is crucial to carefully
    balance the number of candidate tokens and prompt tokens. Instead of appending
    a uniform number of prompt tokens to every candidate token, we allocate them based
    on each candidate’s probability, causing the tree’s maximum depth and structure
    to vary at each decoding step.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更高的加速，PPD利用了专门的树形注意力[[1](#bib.bib1), [18](#bib.bib18)]，在单次解码步骤中处理多个候选项而无需扩大批量大小。值得注意的是，PPD采用了稀疏树[[1](#bib.bib1),
    [4](#bib.bib4)]，旨在优先处理预测准确度更高的候选项。与以往工作中使用的稀疏树的一个关键区别是，如图[3](#S4.F3 "图 3 ‣ 4.1
    动机 ‣ 4 动态稀疏树 ‣ 硬件感知的并行提示解码用于内存高效加速LLM推理")所示，每个树节点都附加了一系列提示令牌。为了优化跨解码步骤的摊销接受长度，必须仔细平衡候选令牌和提示令牌的数量。我们不是向每个候选令牌附加统一数量的提示令牌，而是根据每个候选项的概率分配这些令牌，从而使树的最大深度和结构在每次解码步骤中有所不同。
- en: '![Refer to caption](img/d7c00009a37d210065004fb074abdba1.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d7c00009a37d210065004fb074abdba1.png)'
- en: 'Figure 3: Dynamic sparse tree.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：动态稀疏树。
- en: 4.2 Construction Algorithm
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2 构造算法
- en: We aim to construct a dynamic sparse tree that maximizes the amortized number
    of tokens generated with limited candidate tokens and prompt tokens. We first
    define the tree construction algorithm as a constrained optimization problem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建一个最大化在有限候选代币和提示代币条件下生成的摊销代币数量的动态稀疏树。我们首先将树构建算法定义为一个约束优化问题。
- en: Definition 4.1.
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.1。
- en: Let $m$ corresponding to state $s_{k}$ composed solely of candidate tokens.
    The maximum depth of $\text{C}(T_{k})$.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $m$ 对应于仅由候选代币组成的状态 $s_{k}$。$\text{C}(T_{k})$ 的最大深度。
- en: Proposition 4.1.
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.1。
- en: For a dynamic sparse tree state $T_{k}$ at each path position $k$ represents
    the contribution of a token $v$ to the expected number of tokens.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动态稀疏树状态 $T_{k}$，在每个路径位置 $k$ 代表代币 $v$ 对期望代币数量的贡献。
- en: We then propose an approximation of the amortized number of tokens generated,
    by considering the tokens generated at the current and the next decoding step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后提出了一个代币摊销数量的近似值，通过考虑当前和下一解码步骤生成的代币。
- en: Proposition 4.2.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.2。
- en: The expected total number of tokens $F(T_{k})$ represents the state transition
    probability from state $s_{k}$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的代币总数 $F(T_{k})$ 代表从状态 $s_{k}$ 的状态转移概率。
- en: We are now ready to introduce Proposition [4.3](#S4.Thmproposition3 "Proposition
    4.3\. ‣ 4.2 Construction Algorithm ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel
    Prompt Decoding for Memory-Efficient Acceleration of LLM Inference"), which we
    use in the pruning algorithm.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备介绍命题 [4.3](#S4.Thmproposition3 "Proposition 4.3\. ‣ 4.2 Construction Algorithm
    ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference")，我们将在剪枝算法中使用该命题。
- en: Proposition 4.3.
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.3。
- en: For a dynamic sparse tree state $T_{k}$ is given by $\Delta F=p(c)\cdot(f(T_{i})-f(T_{i-1}))$
    denotes the number of prompt tokens prior to removal. We assume that .
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动态稀疏树状态 $T_{k}$，由 $\Delta F=p(c)\cdot(f(T_{i})-f(T_{i-1}))$ 表示，在移除之前的提示代币数量。我们假设
    。
- en: 'To construct an approximately optimal dynamic sparse tree with specified numbers
    of candidate and prompt tokens, the process includes: (1) Optimal Candidate Trees:
    Constructing trees using only candidate tokens at varying depths, employing the
    algorithm from Medusa [[1](#bib.bib1)] and Sequoia [[4](#bib.bib4)] to maximize
    $f(T_{k})$ (Proposition [4.3](#S4.Thmproposition3 "Proposition 4.3\. ‣ 4.2 Construction
    Algorithm ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel Prompt Decoding for
    Memory-Efficient Acceleration of LLM Inference")), continuing until the desired
    prompt token count is reached.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个具有指定候选代币和提示代币数量的近似最优动态稀疏树，过程包括：（1）最优候选树：使用仅包含候选代币的树在不同深度下构建，采用 Medusa [[1](#bib.bib1)]
    和 Sequoia [[4](#bib.bib4)] 的算法以最大化 $f(T_{k})$（命题 [4.3](#S4.Thmproposition3 "Proposition
    4.3\. ‣ 4.2 Construction Algorithm ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel
    Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")），直到达到所需的提示代币数量为止。
- en: We now introduce the formulation of the real amortized number of tokens generated.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在介绍生成的实际摊销代币数量的公式。
- en: Proposition 4.4.
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.4。
- en: The amortized number of tokens $R(T_{k})$ is the steady-state probability of
    state $s_{i}$ is the function defined in Proposition [4.1](#S4.Thmproposition1
    "Proposition 4.1\. ‣ 4.2 Construction Algorithm ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference").
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 代币的摊销数量 $R(T_{k})$ 是状态 $s_{i}$ 的稳态概率，这是在命题 [4.1](#S4.Thmproposition1 "Proposition
    4.1\. ‣ 4.2 Construction Algorithm ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel
    Prompt Decoding for Memory-Efficient Acceleration of LLM Inference") 中定义的函数。
- en: 'Hardware-awareness. All the probabilities used above can be approximated on
    a validation dataset. The dynamic sparse tree construction algorithm can now be
    formulated as finding the dynamic sparse tree $T$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知。上述使用的所有概率可以在验证数据集上进行近似。动态稀疏树构建算法现在可以被表述为寻找动态稀疏树 $T$：
- en: '|  | $c(n_{c},n_{p})=\max_{T,&#124;C(T)&#124;=n_{c},&#124;T&#124;=n_{c}+n_{p}}R(T).$
    |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $c(n_{c},n_{p})=\max_{T,|C(T)|=n_{c},|T|=n_{c}+n_{p}}R(T).$ |  |'
- en: For a fixed tree size $n$, to identify the dynamic sparse tree that maximizes
    $R(T_{k})$ (hardware-dependent). The speedup ratio, $\text{Speedup}(n)=\frac{\tau(n)}{L_{fp}(n)}$
    that maximizes $\text{Speedup}(n)$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于固定的树大小 $n$，以识别最大化 $R(T_{k})$ 的动态稀疏树（取决于硬件）。加速比 $\text{Speedup}(n)=\frac{\tau(n)}{L_{fp}(n)}$，其最大化
    $\text{Speedup}(n)$。
- en: 5 Experiments
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 实验
- en: Models and testbeds. We conducted all the experiments using MobileLLaMA-1.4B [[6](#bib.bib6)],
    Vicuna-7B and Vicuna-13B [[5](#bib.bib5)]. We used 3 prompt tokens and 1 EPT per
    prompt token for all inference experiments. The inference throughputs of the models
    are evaluated on a single NVIDIA A100 GPU with 40GB of memory and a GeForce RTX
    4090 using a batch size of 1 and FP16 precision. Further details about the experimental
    setup can be found in Appendix [C](#A3 "Appendix C Experiment Details ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference").
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和测试平台。我们使用 MobileLLaMA-1.4B [[6](#bib.bib6)]、Vicuna-7B 和 Vicuna-13B [[5](#bib.bib5)]
    进行了所有实验。所有推理实验中我们使用了 3 个提示符令牌和每个提示符令牌 1 个 EPT。模型的推理吞吐量在单个具有 40GB 内存的 NVIDIA A100
    GPU 和 GeForce RTX 4090 上进行评估，使用批量大小为 1 和 FP16 精度。有关实验设置的更多详细信息，请参见附录 [C](#A3 "附录
    C 实验细节 ‣ 硬件感知并行提示解码以提高 LLM 推理的内存效率")。
- en: Training. We froze all trainable parameters of the original LLM. Prompt token
    embeddings were trained using distillation logits generated from the ShareGPT
    dataset [[22](#bib.bib22)], with a maximum context length of 1024, a cosine learning
    rate scheduler starting at 0.01, and no warmup. Prompt token embeddings are initialized
    with normal text token embeddings.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练。我们冻结了原始 LLM 的所有可训练参数。提示符令牌嵌入使用从 ShareGPT 数据集 [[22](#bib.bib22)] 生成的蒸馏对数进行训练，最大上下文长度为
    1024，余弦学习率调度器从 0.01 开始，且没有预热。提示符令牌嵌入初始化为普通文本令牌嵌入。
- en: Datasets. We assess the throughput performance of PPD across various tasks and
    datasets. Specifically, we evaluated PPD using the MT-Bench dataset [[25](#bib.bib25)],
    which contains multi-turn questions with a range of topics, in both non-greedy
    (temperature follows the default configuration) and greedy settings (temperature=0).
    We used the GSM8K [[7](#bib.bib7)] and HumanEval [[3](#bib.bib3)] datasets only
    in the greedy setting. The GSM8K dataset consists of grade school math problems
    and we used the first 500 questions of the test split for our evaluations. HumanEval
    includes coding tasks, for which we set a maximum new token limit of 512 to control
    the length of the generated sequences. We used the Alpaca [[15](#bib.bib15)] dataset
    as the validation dataset to produce the latencies and acceptance lengths used
    for dynamic sparse tree construction.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们评估了 PPD 在各种任务和数据集上的吞吐量性能。具体来说，我们使用了 MT-Bench 数据集 [[25](#bib.bib25)]，该数据集包含多轮问题，涉及多种主题，评估了非贪婪（温度遵循默认配置）和贪婪设置（温度=0）。我们仅在贪婪设置下使用了
    GSM8K [[7](#bib.bib7)] 和 HumanEval [[3](#bib.bib3)] 数据集。GSM8K 数据集包含小学数学问题，我们使用了测试集中的前
    500 个问题进行评估。HumanEval 包括编码任务，我们设置了最大新令牌限制为 512，以控制生成序列的长度。我们使用 Alpaca [[15](#bib.bib15)]
    数据集作为验证数据集，以生成用于动态稀疏树构建的延迟和接受长度。
- en: 5.1 Speedup Comparison with Parallel Decoding Methods
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1 与并行解码方法的加速比较
- en: '![Refer to caption](img/ac2920a015e6501aa01dc2597efcf1d1.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ac2920a015e6501aa01dc2597efcf1d1.png)'
- en: 'Figure 4: Comparative evaluation of latency speedup between PPD and other parallel
    decoding methods. The experiments were conducted using the MT-Bench dataset, with
    the temperature set to MT-Bench’s default configuration for Medusa and PPD.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：PPD 与其他并行解码方法在延迟加速方面的比较评估。实验使用了 MT-Bench 数据集，温度设置为 Medusa 和 PPD 的默认配置。
- en: We compare the speedup ratios of PPD with state-of-the-art parallel decoding
    methods on MT-Bench in non-greedy settings in Figure [4](#S5.F4 "Figure 4 ‣ 5.1
    Speedup Comparison with Parallel Decoding Methods ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference").
    PPD achieves speedups up to 13.8% higher than Medusa and between 2 times and 3
    times higher than other parallel decoding methods. We examine the factors contributing
    to the enhanced speedup ratios and other performance metrics, as presented in
    Table [1](#S5.T1 "Table 1 ‣ 5.1 Speedup Comparison with Parallel Decoding Methods
    ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference"). The reasons for the increase in speedup ratios
    are two-fold. Firstly, PPD produces candidate tokens with a higher acceptance
    rate than Medusa when utilizing a sparse tree of the same size. Notably, PPD continues
    to achieve a comparable or slightly better acceptance rate even when employing
    a much smaller sparse tree – ranging from one-third to half the size. Secondly,
    PPD benefits from lower forward pass latency due to its ability to use smaller
    sparse tree sizes and hence shorter input lengths. PPD also eliminates the computational
    overhead associated with separate decoding heads. PPD maintains the same output
    quality, achieving about the same score on MT-Bench while using significantly
    fewer trainable parameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图表[4](#S5.F4 "图4 ‣ 5.1 与并行解码方法的加速比比较 ‣ 5 实验 ‣ 硬件感知并行提示解码以实现内存高效的LLM推理加速")中比较了PPD与最先进的并行解码方法在MT-Bench中的加速比，设置为非贪婪模式。PPD的加速比最高比Medusa高13.8%，并且比其他并行解码方法高2到3倍。我们检查了提升加速比和其他性能指标的因素，如表[1](#S5.T1
    "表1 ‣ 5.1 与并行解码方法的加速比比较 ‣ 5 实验 ‣ 硬件感知并行提示解码以实现内存高效的LLM推理加速")所示。加速比增加的原因有两个方面。首先，PPD在使用相同大小的稀疏树时，产生的候选令牌的接受率比Medusa更高。特别是，PPD即使在使用小得多的稀疏树——大小从三分之一到一半时，也能继续达到相当或稍好的接受率。其次，PPD由于能够使用较小的稀疏树尺寸，从而缩短了输入长度，受益于更低的前向传递延迟。PPD还消除了与单独解码头相关的计算开销。PPD保持相同的输出质量，在MT-Bench上的得分大致相同，同时使用的可训练参数显著减少。
- en: '| Model | Method | $T$ (%) | $S_{\text{tr}}$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | $T$ (%) | $S_{\text{tr}}$ |'
- en: '| M | Vanilla | 50.2 | 1.00 | 0.020 | - | NA | NA | 1 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| M | Vanilla | 50.2 | 1.00 | 0.020 | - | NA | NA | 1 |'
- en: '| PPD | 108.7 | 2.43 | 0.022 | Same | 4.50$e^{-4}$ | (10,84,89) | (40,285,285)
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| PPD | 108.7 | 2.43 | 0.022 | 相同 | 4.50$e^{-4}$ | (10,84,89) | (40,285,285)
    |'
- en: '| V-7B | Vanilla | 39.2 | 1.00 | 0.026 | 5.99 | NA | NA | 1 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| V-7B | Vanilla | 39.2 | 1.00 | 0.026 | 5.99 | NA | NA | 1 |'
- en: '| Medusa | 82.0 | 2.51 | 0.0307 | 5.98 | 8.07 | 63 | 63 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Medusa | 82.0 | 2.51 | 0.0307 | 5.98 | 8.07 | 63 | 63 |'
- en: '| PPD | 88.0 | 2.54 | 0.029 | 5.93 | 1.82$e^{-4}$ | (10,33,34) | (40,105,105)
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| PPD | 88.0 | 2.54 | 0.029 | 5.93 | 1.82$e^{-4}$ | (10,33,34) | (40,105,105)
    |'
- en: '| V-13B | Vanilla | 30.4 | 1.00 | 0.0330 | 6.38 | NA | NA | 1 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| V-13B | Vanilla | 30.4 | 1.00 | 0.0330 | 6.38 | NA | NA | 1 |'
- en: '| Medusa | 63.4 | 2.59 | 0.0408 | - | 5.52 | 63 | 63 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Medusa | 63.4 | 2.59 | 0.0408 | - | 5.52 | 63 | 63 |'
- en: '| PPD | 66.1 | 2.44 | 0.0379 | 6.32 | 7.87$e^{-5}$ | (10,20,20) | (40,60,60)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| PPD | 66.1 | 2.44 | 0.0379 | 6.32 | 7.87$e^{-5}$ | (10,20,20) | (40,60,60)
    |'
- en: 'Table 1: Comparative performance metrics of MobileLLaMA (M) for greedy setting,
    Vicuna-7B (V-7B) and Vicuna-13B (V-13B) for non-greedy setting using different
    decoding methods. The table details throughput ($T$) and input lengths ($S_{\text{input}}$),
    represented as tuples. Same means the output matches with that of the original
    LLM.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：使用不同解码方法的贪婪设置下MobileLLaMA (M)的比较性能指标，非贪婪设置下Vicuna-7B (V-7B)和Vicuna-13B (V-13B)。该表详细说明了吞吐量($T$)和输入长度($S_{\text{input}}$)，以元组形式表示。相同表示输出与原始LLM的输出匹配。
- en: Figure [5](#S5.F5 "Figure 5 ‣ 5.1 Speedup Comparison with Parallel Decoding
    Methods ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") displays the throughput of PPD on MT-Bench, HumanEval,
    and GSM8K with temperature equal to 0\. PPD achieves consistent walltime speedup
    ratios from 2.12$\times$. This can be attributed to the fact that both code and
    math equations often contain fixed patterns and repetitive symbols, which narrows
    the range of plausible candidates and simplifies the prediction. We also found
    that with typical acceptance, the speedup increases with temperature. Another
    notable trend is that smaller models, such as Vicuna-7B, generally achieve more
    significant speedup ratios as compared to larger models, like Vicuna-13B. PPD
    aims to generate more tokens per step, which comes with increased computational
    demands. For larger models that already require substantial computational resources,
    it is necessary to limit the size of the sparse tree to avoid exceeding the GPU’s
    utilization cap and causing increased latency. As a result, the number of tokens
    accepted per step is reduced, leading to lower speedups. However, this can be
    amortized when using more powerful GPUs than the NVIDIA A100 and the RTX 4090,
    such as NVIDIA H100.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S5.F5 "图 5 ‣ 5.1 与并行解码方法的加速比较 ‣ 5 实验 ‣ 硬件感知并行提示解码用于内存高效加速 LLM 推理") 显示了
    PPD 在 MT-Bench、HumanEval 和 GSM8K 上的吞吐量，温度为 0。PPD 实现了 2.12$\times$ 的一致性壁时间加速比。这可以归因于代码和数学方程通常包含固定模式和重复符号，从而缩小了合理候选的范围并简化了预测。我们还发现，随着典型接受度的增加，温度对加速有正面影响。另一个显著的趋势是，较小的模型（如
    Vicuna-7B）通常比较大的模型（如 Vicuna-13B）实现更显著的加速比。PPD 旨在每步生成更多的标记，这会增加计算需求。对于已经需要大量计算资源的大型模型，需要限制稀疏树的大小，以避免超出
    GPU 的利用上限并导致延迟增加。因此，每步接受的标记数减少，导致加速比降低。然而，当使用比 NVIDIA A100 和 RTX 4090 更强大的 GPU（如
    NVIDIA H100）时，这种影响可能会减轻。
- en: '![Refer to caption](img/2fafdcc4d5855bef9bf01c005cc32ac4.png)![Refer to caption](img/16dc144812532ff123d93bd87c250800.png)![Refer
    to caption](img/769e8600c769e6529b01321a7cf4911c.png)![Refer to caption](img/1b7a6b7e739841607e07ae652a257925.png)![Refer
    to caption](img/37f4802fa5bb2fd715de8b6320a69e13.png)![Refer to caption](img/f40587693ba432d81d6aea41fde4dfa8.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2fafdcc4d5855bef9bf01c005cc32ac4.png)![参见说明](img/16dc144812532ff123d93bd87c250800.png)![参见说明](img/769e8600c769e6529b01321a7cf4911c.png)![参见说明](img/1b7a6b7e739841607e07ae652a257925.png)![参见说明](img/37f4802fa5bb2fd715de8b6320a69e13.png)![参见说明](img/f40587693ba432d81d6aea41fde4dfa8.png)'
- en: 'Figure 5: Throughput of PPD and vanilla models across different tasks. The
    temperature for experiments are set to 0 and the generated output of PPD exactly
    matches that of the original LLM. We do not show results of Vicuna-13B on RTX
    4090 as it does not fit into the GPU memory.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同任务下 PPD 和原始模型的吞吐量。实验的温度设置为 0，PPD 生成的输出与原始 LLM 完全匹配。由于 Vicuna-13B 在 RTX
    4090 上无法适配 GPU 内存，因此未展示其结果。
- en: 5.2 Long-range Token Prediction
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2 长期标记预测
- en: '![Refer to caption](img/cffd636015b3728d435d80793d4912eb.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cffd636015b3728d435d80793d4912eb.png)'
- en: (a) PD vs. Medusa
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PD 与 Medusa
- en: '![Refer to caption](img/af587a71942c3786939aacd9cb4d87c1.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/af587a71942c3786939aacd9cb4d87c1.png)'
- en: (b) 100 EPT vs. 1 EPT
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 100 EPT 与 1 EPT
- en: '![Refer to caption](img/a4e13b56f0ff223a691b5a3fdde293b6.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a4e13b56f0ff223a691b5a3fdde293b6.png)'
- en: (c) 13b vs. 7b
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 13b 与 7b
- en: 'Figure 6: Accumulative accuracy comparisons across different model configurations
    and prediction distances. ‘V7’ for Vicuna-7B, and ‘V13’ for Vicuna-13B. The notation
    ‘@$i$. ‘100 EPT’ represents 100 EPTs per prompt token. Accumulative accuracy is
    defined as top-k accuracy (e.g., a prediction is correct if the top-k candidates
    contain the ground truth). These measurements were obtained from the Alpaca Eval
    dataset with a maximum of 20 steps.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同模型配置和预测距离下的累积准确度比较。‘V7’ 代表 Vicuna-7B，‘V13’ 代表 Vicuna-13B。符号‘@$i$’。‘100
    EPT’ 代表每个提示标记的 100 个 EPT。累积准确度定义为 top-k 准确度（例如，如果 top-k 候选中包含真实值，则预测正确）。这些测量来自
    Alpaca Eval 数据集，最大步数为 20。
- en: For a specific sparse tree, the accumulative accuracy provides a theoretical
    upper bound for the number of generated tokens per step and the maximum possible
    speedup ratio. Hence, maximizing accumulative accuracy is crucial for the effectiveness
    of PPD. Figure [6](#S5.F6 "Figure 6 ‣ 5.2 Long-range Token Prediction ‣ 5 Experiments
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference") demonstrates the accumulative accuracy of the tokens predicted
    at various positions. We summarize the following three key insights from the results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的稀疏树，累积准确性为每步生成的 tokens 数量和可能的最大加速比提供了理论上的上限。因此，最大化累积准确性对 PPD 的有效性至关重要。图
    [6](#S5.F6 "图 6 ‣ 5.2 长距 Token 预测 ‣ 5 实验 ‣ 硬件感知的并行提示解码以实现 LLM 推理的内存高效加速") 演示了在不同位置预测的
    tokens 的累积准确性。我们从结果中总结出以下三个关键见解。
- en: PPD excels at predicting more distant tokens. As depicted in Figure [6(a)](#S5.F6.sf1
    "In Figure 6 ‣ 5.2 Long-range Token Prediction ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference"),
    PPD consistently outperforms Medusa in accuracy across all token positions. The
    accuracy gap between PPD and Medusa widens with the increased token distance (e.g.,
    the top-10 accuracy difference is 0.03 for the ‘next next’ word versus 0.12 for
    the ‘next next next next’ word). This improvement can be attributed to PPD’s ability
    to partially recover conditional dependency information through causally connected
    prompt tokens.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: PPD 在预测更远的 tokens 方面表现优越。如图 [6(a)](#S5.F6.sf1 "图 6 ‣ 5.2 长距 Token 预测 ‣ 5 实验
    ‣ 硬件感知的并行提示解码以实现 LLM 推理的内存高效加速") 所示，PPD 在所有 token 位置的准确性上始终优于 Medusa。PPD 和 Medusa
    之间的准确性差距随着 token 距离的增加而扩大（例如，“下一个下一个”单词的 top-10 准确性差异为 0.03，而“下下下下一个”单词的差异为 0.12）。这种改进归因于
    PPD 通过因果连接的提示 token 部分恢复条件依赖信息的能力。
- en: PPD performs well at generating a broader array of plausible token candidates.
    For example, in predicting the token at a token distance of 3, the top-10 candidates
    exhibit an accuracy improvement of 0.1 over Medusa, compared to only 0.02 for
    the top-1 candidate. This demonstrates the value of using tree attention and the
    largest viable tree size during inference, as multiple candidate continuations
    further boost accuracy improvement.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: PPD 在生成更广泛的可行 token 候选项方面表现良好。例如，在预测距离为 3 的 token 时，top-10 候选项的准确性比 Medusa 提高了
    0.1，而 top-1 候选项仅提高了 0.02。这展示了在推理过程中使用树注意力和最大可行树大小的价值，因为多个候选续集进一步提升了准确性。
- en: Multiple EPTs per prompt token and larger model sizes yield modest improvements
    in prediction accuracy. Figure [6(b)](#S5.F6.sf2 "In Figure 6 ‣ 5.2 Long-range
    Token Prediction ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for
    Memory-Efficient Acceleration of LLM Inference") shows that using 100 EPTs per
    prompt token leads to accuracy improvement, ranging from 0.018 to 0.045. Figure [6(c)](#S5.F6.sf3
    "In Figure 6 ‣ 5.2 Long-range Token Prediction ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")
    displays that PPD with Vicuna-13B outperforms Vicuna-7B with an accuracy gain
    of 0.011$\thicksim$0.038\. This increase is due to Vicuna-13B’s greater embedding
    dimensions and deeper layers, which enhance the expressive power of prompt tokens.
    However, these gains are modest and can be offset by the increased computational
    burden of larger models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示 token 的多个 EPT 和更大的模型规模对预测准确性有适度的提升。如图 [6(b)](#S5.F6.sf2 "图 6 ‣ 5.2 长距 Token
    预测 ‣ 5 实验 ‣ 硬件感知的并行提示解码以实现 LLM 推理的内存高效加速") 所示，使用每个提示 token 的 100 个 EPT 会使准确性提高，范围从
    0.018 到 0.045。图 [6(c)](#S5.F6.sf3 "图 6 ‣ 5.2 长距 Token 预测 ‣ 5 实验 ‣ 硬件感知的并行提示解码以实现
    LLM 推理的内存高效加速") 显示，PPD 使用 Vicuna-13B 的准确性比 Vicuna-7B 提高了 0.011$\thicksim$0.038。这一增加归因于
    Vicuna-13B 更大的嵌入维度和更深的层次，增强了提示 tokens 的表达能力。然而，这些提升是适度的，并可能被更大模型的计算负担所抵消。
- en: 5.3 Memory Efficiency and Synergistic Integrations with Speculative Decoding
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3 内存效率与投机解码的协同集成
- en: '![Refer to caption](img/695694b18033060589255ac4174d4456.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/695694b18033060589255ac4174d4456.png)'
- en: 'Figure 7: Model memory usage.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：模型内存使用情况。
- en: Memory efficiency. As shown in Figure [7](#S5.F7 "Figure 7 ‣ 5.3 Memory Efficiency
    and Synergistic Integrations with Speculative Decoding ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference"),
    we compare the memory overhead of PPD with the leading parallel decoding (Medusa)
    and speculative decoding approaches (Eagle). The memory overhead of PPD is just
    0.004% of Medusa’s and 0.007% of Eagle’s. This efficiency stems from the efficient
    use of embeddings in PPD, which are significantly smaller than decoding heads
    and draft models, both of which scale with vocabulary size.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 内存效率。如图[7](#S5.F7 "图 7 ‣ 5.3 内存效率与预测解码的协同集成 ‣ 5 实验 ‣ 硬件感知并行提示解码以提高 LLM 推理的内存效率")所示，我们比较了
    PPD 与领先的并行解码（Medusa）和预测解码方法（Eagle）的内存开销。PPD 的内存开销仅为 Medusa 的 0.004% 和 Eagle 的
    0.007%。这种效率源于 PPD 中对嵌入的高效使用，这些嵌入显著小于解码头和草稿模型，后者随着词汇量的增加而增加。
- en: PPD + Speculative Decoding. As an orthogonal optimization in accelerating LLMs,
    PPD can be easily integrated with speculative decoding [[11](#bib.bib11)]. To
    demonstrate this, we applied PPD to Vicuna-68M [[24](#bib.bib24)] and used it
    as the draft model for Vicuna-7B. This combination resulted in a speedup of up
    to 1.22$\times$ for speculative decoding on Vicuna-7B compared to using speculative
    decoding alone.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PPD + 预测解码。作为加速大型语言模型（LLMs）的正交优化，PPD 可以轻松与预测解码[[11](#bib.bib11)]集成。为了证明这一点，我们将
    PPD 应用于 Vicuna-68M [[24](#bib.bib24)] 并将其用作 Vicuna-7B 的草稿模型。这种组合使得 Vicuna-7B 在预测解码中的速度提高了最多
    1.22$\times$，相比单独使用预测解码。
- en: 5.4 Ablation Study
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4 消融研究
- en: Dynamic Sparse Tree. Figure [8(a)](#S5.F8.sf1 "In Figure 8 ‣ 5.4 Ablation Study
    ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") shows that dynamic sparse trees consistently achieve
    longer acceptance lengths compared to static and random ones across varying sizes.
    The acceptance length for dynamic sparse trees shows a steady increase as the
    tree size extends, suggesting its good scalability. The convergence of dynamic
    and static sparse trees at larger sizes suggests a structural similarity emerging
    from constraints in tree depth and tree node count.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 动态稀疏树。图[8(a)](#S5.F8.sf1 "在图 8 ‣ 5.4 消融研究 ‣ 5 实验 ‣ 硬件感知并行提示解码以提高 LLM 推理的内存效率")显示，动态稀疏树在各种规模下始终比静态和随机树具有更长的接受长度。动态稀疏树的接受长度随着树的规模扩展而稳定增加，这表明其良好的可扩展性。动态和静态稀疏树在较大规模下的收敛表明了树深度和树节点数量约束下的结构相似性。
- en: Hardware-aware Tree Size. Figure [8(b)](#S5.F8.sf2 "In Figure 8 ‣ 5.4 Ablation
    Study ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") presents the theoretical speedup across different
    GPUs. Figure [8(c)](#S5.F8.sf3 "In Figure 8 ‣ 5.4 Ablation Study ‣ 5 Experiments
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference") validates that the optimal sparse tree size, derived from theoretical
    speedup models, indeed results in the greatest actual speedup observed.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知树大小。图[8(b)](#S5.F8.sf2 "在图 8 ‣ 5.4 消融研究 ‣ 5 实验 ‣ 硬件感知并行提示解码以提高 LLM 推理的内存效率")展示了不同
    GPU 下的理论加速。图[8(c)](#S5.F8.sf3 "在图 8 ‣ 5.4 消融研究 ‣ 5 实验 ‣ 硬件感知并行提示解码以提高 LLM 推理的内存效率")验证了从理论加速模型中得出的最佳稀疏树大小确实产生了观察到的最大实际加速。
- en: '![Refer to caption](img/7f2200db0837d718ac9529d52e25c608.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7f2200db0837d718ac9529d52e25c608.png)'
- en: (a)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/577f6b437e78d028817d5df937652e63.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/577f6b437e78d028817d5df937652e63.png)'
- en: (b)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/2f15873121e7c5b01e7190046c2bfed6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f15873121e7c5b01e7190046c2bfed6.png)'
- en: (c)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 8: Evaluation of Dynamic Sparse Tree Performance. The static sparse
    trees in (a) always use the largest possible prompt tokens for each candidate.
    The theoretical speedup in (b) is calculated as the ratio of acceptance lengths
    (hardware-independent) to latency overhead (hardware-dependent). The optimal tree
    size is obtained from the peak value of the theoretical speedup. The latencies
    in (b) are obtained from inference on the same prompt for 512 forward passes.
    (c) shows the actual speedup obtained by running inference on different GPUs with
    different tree lengths on Alpaca Eval dataset.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 动态稀疏树性能评估。图 (a) 中的静态稀疏树始终使用每个候选项的最大可能提示令牌。图 (b) 中的理论加速比计算为接受长度（硬件独立）与延迟开销（硬件依赖）的比率。最佳树大小由理论加速比的峰值获得。图
    (b) 中的延迟来自于对相同提示进行 512 次前向传递的推理。图 (c) 显示了在不同的 GPU 上对不同树长度的 Alpaca Eval 数据集进行推理时获得的实际加速比。'
- en: 6 Conclusion
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6 结论
- en: We introduced PPD, a memory-efficient, cost-effective, and powerful parallel
    decoding method that incorporates a hardware-aware dynamic sparse tree. Utilizing
    specially trained prompt tokens to predict long-range tokens accurately, PPD achieves
    a speedup of up to 2.49$\times$ in inference while employing only 0.0002% additional
    trainable parameters and without incorporating new models or architectural components.
    We believe that PPD offers a novel perspective on the capabilities of parallel
    decoding. In future work, it could be synergized with other speculative or parallel
    decoding techniques to expedite inference even further.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 PPD，一种内存高效、成本效益高且强大的并行解码方法，它包含一个硬件感知的动态稀疏树。利用特别训练的提示令牌来准确预测长程令牌，PPD 在推理时实现了高达
    2.49$\times$ 的加速，同时只使用了 0.0002% 的附加可训练参数，并且没有引入新的模型或架构组件。我们认为 PPD 提供了对并行解码能力的新视角。在未来的工作中，它可以与其他猜测性或并行解码技术协同使用，以进一步加速推理。
- en: References
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Cai et al. [2024] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D.
    Lee, Deming Chen, and Tri Dao. Medusa: Simple LLM Inference Acceleration Framework
    with Multiple Decoding Heads. *arXiv preprint arXiv:2401.10774*, 2024.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '蔡等人 [2024] 田乐·蔡、余宏·李、郑阳·耿、洪武·彭、杰森·D·李、德明·陈和三·道。Medusa: 一个简单的 LLM 推理加速框架，具有多个解码头。*arXiv
    预印本 arXiv:2401.10774*，2024。'
- en: Chen et al. [2023] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste
    Lespiau, Laurent Sifre, and John Jumper. Accelerating Large Language Model Decoding
    with Speculative Sampling. *arXiv preprint arXiv:2302.01318*, 2023.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2023] 查理·陈、塞巴斯蒂安·博尔戈、杰奥弗瑞·欧文、让-巴蒂斯特·莱皮奥、劳伦特·西弗和约翰·跳跃。通过猜测采样加速大型语言模型解码。*arXiv
    预印本 arXiv:2302.01318*，2023。
- en: Chen et al. [2021] Mark Chen et al. Evaluating Large Language Models Trained
    on Code. *arXiv preprint arXiv:2107.03374*, 2021.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2021] 马克·陈等人。评估在代码上训练的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021。
- en: 'Chen et al. [2024] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang,
    Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, Robust, and Hardware-aware
    Speculative Decoding. *arXiv preprint arXiv:2402.12374*, 2024.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2024] 朱梦陈、阿夫纳·梅、鲁斯兰·斯维尔谢夫斯基、余瞬黄、马克斯·里亚宾宁、志浩贾和贝迪·陈。Sequoia: 可扩展、稳健且硬件感知的猜测解码。*arXiv
    预印本 arXiv:2402.12374*，2024。'
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4
    with 90%* ChatGPT Quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '江等人 [2023] 魏林·江、卓翰·李、子林、英盛、张浩·吴、郝张、连敏·郑、思远·庄、永浩·庄、约瑟夫·E·冈萨雷斯、伊昂·斯托伊卡和埃里克·P·辛。Vicuna:
    一个开放源代码的聊天机器人，令人印象深刻的 GPT-4，质量达到 90%* ChatGPT，2023年3月。网址 [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: 'Chu et al. [2023] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang
    Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, and Chunhua Shen.
    MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices.
    *arXiv preprint arXiv:2312.16886*, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '楚等人 [2023] 向向楚、李梦、辛阳·林、双徐、杨阳、易名·胡、飞伟、辛宇·张、博·张、肖琳·魏和春华·申。MobileVLM: 一个快速、强大且开放的移动设备视觉语言助手。*arXiv
    预印本 arXiv:2312.16886*，2023。'
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, 和 John Schulman. 训练验证器解决数学文字问题。*arXiv 预印本 arXiv:2110.14168*，2021年。
- en: Fu et al. [2023] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking
    the Sequential Dependency of LLM Inference Using Lookahead Decoding, November
    2023. URL [https://lmsys.org/blog/2023-11-21-lookahead-decoding/](https://lmsys.org/blog/2023-11-21-lookahead-decoding/).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. [2023] Yichao Fu, Peter Bailis, Ion Stoica, 和 Hao Zhang. 通过前瞻解码打破LLM推理的序列依赖，2023年11月。网址
    [https://lmsys.org/blog/2023-11-21-lookahead-decoding/](https://lmsys.org/blog/2023-11-21-lookahead-decoding/)。
- en: 'He et al. [2023] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He.
    Rest: Retrieval-based Speculative Decoding. *arXiv preprint arXiv:2311.08252*,
    2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. [2023] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, 和 Di He.
    Rest: 基于检索的推测解码。*arXiv 预印本 arXiv:2311.08252*，2023年。'
- en: Karpukhin et al. [2020] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval
    for Open-Domain Question Answering. In *Proceedings of the 2020 Conference on
    Empirical Methods in Natural Language Processing (EMNLP)*, pages 6769–6781, 2020.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin et al. [2020] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, 和 Wen-tau Yih. 开放领域问答的密集段落检索。发表于
    *2020年自然语言处理经验方法会议 (EMNLP)*，第6769–6781页，2020年。
- en: Kim et al. [2024] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik,
    Michael W. Mahoney, Amir Gholami, and Kurt Keutzer. Speculative Decoding with
    Big Little Decoder. *Advances in Neural Information Processing Systems (NeurIPS)*,
    36, 2024.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2024] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik,
    Michael W. Mahoney, Amir Gholami, 和 Kurt Keutzer. 使用大小解码器的推测解码。*神经信息处理系统进展 (NeurIPS)*，36，2024年。
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power
    of Scale for Parameter-Efficient Prompt Tuning. In *Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, 2021.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. [2021] Brian Lester, Rami Al-Rfou, 和 Noah Constant. 参数高效提示调整的规模力量。发表于
    *自然语言处理的经验方法会议 (EMNLP)*，2021年。
- en: Leviathan et al. [2023] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    Inference from Transformers via Speculative Decoding. In *International Conference
    on Machine Learning (ICML)*, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan et al. [2023] Yaniv Leviathan, Matan Kalman, 和 Yossi Matias. 通过推测解码实现快速推理。发表于
    *国际机器学习会议 (ICML)*，2023年。
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 4582–4597\.
    Association for Computational Linguistics, 2021.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li and Liang [2021] Xiang Lisa Li 和 Percy Liang. 前缀调整: 优化生成的连续提示。发表于 *第59届计算语言学协会年会和第11届国际自然语言处理联合会议
    (第1卷: 长篇论文)*，第4582–4597页。计算语言学协会，2021年。'
- en: 'Li et al. [2023] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval:
    An Automatic Evaluator of Instruction-following Models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval),
    2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. AlpacaEval:
    指令跟随模型的自动评估器。 [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)，2023年。'
- en: 'Li et al. [2024] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE:
    Speculative Sampling Requires Rethinking Feature Uncertainty. *arXiv preprint
    arXiv:2401.15077*, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2024] Yuhui Li, Fangyun Wei, Chao Zhang, 和 Hongyang Zhang. EAGLE:
    推测采样需要重新思考特征不确定性。*arXiv 预印本 arXiv:2401.15077*，2024年。'
- en: 'Liu et al. [2024] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang,
    and Yuxiao Dong. APAR: LLMs can do auto-parallel auto-regressive decoding. *arXiv
    preprint arXiv:2401.06761*, 2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2024] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang,
    和 Yuxiao Dong. APAR: LLMs 能够进行自动并行自回归解码。*arXiv 预印本 arXiv:2401.06761*，2024年。'
- en: 'Miao et al. [2024] Xupeng Miao et al. SpecInfer: Accelerating Large Language
    Model Serving with Tree-based Speculative Inference and Verification. In *ACM
    International Conference on Architectural Support for Programming Languages and
    Operating Systems (ASPLOS)*, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miao 等人 [2024] Xupeng Miao 等人. SpecInfer: 基于树的推测推断与验证加速大型语言模型服务。在 *ACM 国际编程语言和操作系统体系结构支持会议
    (ASPLOS)*，2024年。'
- en: 'Monea et al. [2023] Giovanni Monea, Armand Joulin, and Edouard Grave. PaSS:
    Parallel Speculative Sampling. *arXiv preprint arXiv:2311.13581*, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Monea 等人 [2023] Giovanni Monea, Armand Joulin, 和 Edouard Grave. PaSS: Parallel
    Speculative Sampling. *arXiv 预印本 arXiv:2311.13581*, 2023年。'
- en: Santilli et al. [2023] Andrea Santilli, Silvio Severino, Emilian Postolache,
    Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. Accelerating
    Transformer Inference for Translation via Parallel Decoding. In *Annual Meeting
    of the Association for Computational Linguistics (ACL)*, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santilli 等人 [2023] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino
    Maiorca, Michele Mancusi, Riccardo Marin, 和 Emanuele Rodolà. 通过并行解码加速变换器推断用于翻译。在
    *计算语言学协会年会 (ACL)*，2023年。
- en: Saxena [2023] Apoorv Saxena. Prompt Lookup Decoding, November 2023. URL [https://github.com/apoorvumang/prompt-lookup-decoding/](https://github.com/apoorvumang/prompt-lookup-decoding/).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saxena [2023] Apoorv Saxena. Prompt Lookup Decoding, 2023年11月。网址 [https://github.com/apoorvumang/prompt-lookup-decoding/](https://github.com/apoorvumang/prompt-lookup-decoding/).
- en: ShareGPT [2023] ShareGPT. ShareGPT, 2023. URL [https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ShareGPT [2023] ShareGPT. ShareGPT, 2023年。网址 [https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered/).
- en: Stern et al. [2018] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise
    Parallel Decoding for Deep Autoregressive Models. In *Advances in Neural Information
    Processing Systems (NeurIPS)*, 2018.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stern 等人 [2018] Mitchell Stern, Noam Shazeer, 和 Jakob Uszkoreit. 深度自回归模型的块级并行解码。在
    *神经信息处理系统进展 (NeurIPS)*，2018年。
- en: Yang et al. [2024] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-Candidate
    Speculative Decoding. *arXiv preprint arXiv:2401.06706*, 2024.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2024] Sen Yang, Shujian Huang, Xinyu Dai, 和 Jiajun Chen. 多候选推测解码。*arXiv
    预印本 arXiv:2401.06706*，2024年。
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench
    and Chatbot Arena. In *Advances in Neural Information Processing Systems (NeurIPS)*,
    2023.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph
    E. Gonzalez, 和 Ion Stoica. 使用 MT-Bench 和 Chatbot Arena 评估 LLM 作为评判者。在 *神经信息处理系统进展
    (NeurIPS)*，2023年。
- en: Supplementary Material
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 附录材料
- en: Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知的并行提示解码，用于内存高效加速 LLM 推断
- en: \doparttoc\faketableofcontents\parttoc
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontents\parttoc
- en: Appendix A Training Loss
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A 训练损失
- en: '![Refer to caption](img/fb93c1feb73074a26b3a56fd516e6e9b.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb93c1feb73074a26b3a56fd516e6e9b.png)'
- en: (a) 3 prompt tokens, 1 EPTs
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 3 个提示令牌，1 个 EPT
- en: '![Refer to caption](img/5bcaa9f654594d66db214b02006803cd.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5bcaa9f654594d66db214b02006803cd.png)'
- en: (b) 3 prompt tokens, 100 EPTs
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 3 个提示令牌，100 个 EPT
- en: 'Figure 9: Training Loss'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 训练损失'
- en: We study the training loss of PPD with different EPTs. Figure [9(a)](#A1.F9.sf1
    "In Figure 9 ‣ Appendix A Training Loss ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference") shows that, with 3 prompt
    tokens and 1 EPT, the initial loss is quite high, starting above 5\. There is
    a sharp decrease in loss within the first epoch, dropping below 2\. After this
    initial drop, the loss stabilizes and oscillates around a value slightly below
    2 for the remainder of the training epochs (up to epoch 12). The loss oscillations
    remain within a narrow range, indicating consistent performance. The fluctuation
    can be attributed to the insertion of prompt tokens at random positions. On the
    other hand, Figure [9(b)](#A1.F9.sf2 "In Figure 9 ‣ Appendix A Training Loss ‣
    Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM
    Inference"), with 3 prompt tokens and 100 EPTs, shows the initial loss starting
    below 3, significantly lower than PPD with 1 EPT. Similarly, there is a sharp
    decrease within the first epoch, with the loss dropping to around 2.5\. However,
    unlike PPD with 1 EPT, the loss continues to decrease gradually over the epochs,
    showing a downward trend. This suggests that increasing the number of EPTs improves
    the model’s learning capacity and reduce training loss more effectively over time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了不同 EPT 的 PPD 训练损失。图 [9(a)](#A1.F9.sf1 "图 9 ‣ 附录 A 训练损失 ‣ 面向内存高效加速 LLM 推理的硬件感知并行提示解码")
    显示，使用 3 个提示标记和 1 个 EPT 时，初始损失相当高，超过 5\。在第一个 epoch 内损失急剧下降，降到 2\ 以下。之后，损失稳定并在剩余的训练
    epochs（最多到第 12 个 epoch）中围绕略低于 2 的值波动。损失波动保持在狭窄范围内，表明性能一致。波动可以归因于在随机位置插入提示标记。另一方面，图
    [9(b)](#A1.F9.sf2 "图 9 ‣ 附录 A 训练损失 ‣ 面向内存高效加速 LLM 推理的硬件感知并行提示解码") 显示，使用 3 个提示标记和
    100 个 EPT 时，初始损失低于 3，明显低于使用 1 个 EPT 的 PPD。同样，在第一个 epoch 内损失急剧下降，降到约 2.5\。然而，与
    1 个 EPT 的 PPD 不同，损失在 epochs 中逐渐减少，显示出下降趋势。这表明增加 EPT 数量可以提高模型的学习能力，并更有效地减少训练损失。
- en: Appendix B Extended Ablation Study
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 B 扩展消融研究
- en: B.1 Effect of EPTs on Prediction Accuracy
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1 EPT 对预测准确性的影响
- en: 'Table 2: Prediction Accuracy of PPD with different EPTs. ’@i’ denotes a token
    distance of i. ’Top-k’ denotes the top-k prediction accuracy. The results are
    obtained on Alpaca dataset with 20 steps.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同 EPT 的 PPD 预测准确性。‘@i’ 表示 i 的标记距离。‘Top-k’ 表示前 k 的预测准确性。结果是在 Alpaca 数据集上通过
    20 步获得的。
- en: '| EPT | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| EPT | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 100 | 0.506 | 0.794 | 0.276 | 0.602 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 0.506 | 0.794 | 0.276 | 0.602 |'
- en: '| 50 | 0.502 | 0.791 | 0.281 | 0.604 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 0.502 | 0.791 | 0.281 | 0.604 |'
- en: '| 20 | 0.501 | 0.791 | 0.276 | 0.607 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.501 | 0.791 | 0.276 | 0.607 |'
- en: '| 10 | 0.494 | 0.786 | 0.273 | 0.600 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.494 | 0.786 | 0.273 | 0.600 |'
- en: '| 5 | 0.499 | 0.787 | 0.265 | 0.596 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.499 | 0.787 | 0.265 | 0.596 |'
- en: '| 2 | 0.486 | 0.777 | 0.259 | 0.583 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.486 | 0.777 | 0.259 | 0.583 |'
- en: '| 1 | 0.472 | 0.771 | 0.248 | 0.576 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.472 | 0.771 | 0.248 | 0.576 |'
- en: Table [2](#A2.T2 "Table 2 ‣ B.1 Effect of EPTs on Prediction Accuracy ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") presents the prediction accuracy of PPD using
    different EPTs. The results indicate that increasing the number of EPTs generally
    enhances the prediction accuracy of PPD, particularly for long-range token predictions.
    Higher EPT numbers (e.g., 100 and 50) consistently produce better prediction accuracy
    compared to lower EPT numbers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#A2.T2 "表 2 ‣ B.1 EPT 对预测准确性的影响 ‣ 附录 B 扩展消融研究 ‣ 面向内存高效加速 LLM 推理的硬件感知并行提示解码")
    展示了使用不同 EPT 的 PPD 预测准确性。结果表明，增加 EPT 数量通常提高了 PPD 的预测准确性，尤其是对远程标记预测。更高的 EPT 数量（例如，100
    和 50）相比于较低的 EPT 数量一贯表现出更好的预测准确性。
- en: B.2 Impact of Knowledge Distillation (KD), Epochs, and Batch Size on Prediction
    Accuracy
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2 知识蒸馏（KD）、Epoch 和 Batch Size 对预测准确性的影响
- en: 'Table 3: Prediction Accuracy for PPD with and without knowledge distillation
    (KD) for different EPTs, epochs, and batch sizes.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同 EPT、epochs 和 batch sizes 的 PPD 在有无知识蒸馏（KD）情况下的预测准确性。
- en: '| EPT | KD | Epoch | Batch | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| EPT | KD | Epoch | Batch | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 100 | Yes | 1 | 4 | 0.504 | 0.793 | 0.273 | 0.598 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 1 | 4 | 0.504 | 0.793 | 0.273 | 0.598 |'
- en: '| 100 | Yes | 2 | 4 | 0.512 | 0.797 | 0.288 | 0.611 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 2 | 4 | 0.512 | 0.797 | 0.288 | 0.611 |'
- en: '| 100 | Yes | 6 | 4 | 0.520 | 0.802 | 0.302 | 0.620 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 6 | 4 | 0.520 | 0.802 | 0.302 | 0.620 |'
- en: '| 100 | Yes | 8 | 4 | 0.524 | 0.804 | 0.307 | 0.619 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 8 | 4 | 0.524 | 0.804 | 0.307 | 0.619 |'
- en: '| 100 | Yes | 10 | 4 | 0.523 | 0.804 | 0.305 | 0.623 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 10 | 4 | 0.523 | 0.804 | 0.305 | 0.623 |'
- en: '| 100 | Yes | 12 | 4 | 0.525 | 0.805 | 0.308 | 0.625 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 12 | 4 | 0.525 | 0.805 | 0.308 | 0.625 |'
- en: '| 100 | No | 12 | 4 | 0.506 | 0.794 | 0.276 | 0.602 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 否 | 12 | 4 | 0.506 | 0.794 | 0.276 | 0.602 |'
- en: '| 100 | Yes | 12 | 1 | 0.530 | 0.809 | 0.309 | 0.626 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 12 | 1 | 0.530 | 0.809 | 0.309 | 0.626 |'
- en: '| 1 | Yes | 12 | 1 | 0.484 | 0.775 | 0.259 | 0.581 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 12 | 1 | 0.484 | 0.775 | 0.259 | 0.581 |'
- en: '| 1 | Yes | 2 | 4 | 0.474 | 0.773 | 0.247 | 0.574 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 2 | 4 | 0.474 | 0.773 | 0.247 | 0.574 |'
- en: '| 1 | Yes | 6 | 4 | 0.480 | 0.773 | 0.250 | 0.580 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 6 | 4 | 0.480 | 0.773 | 0.250 | 0.580 |'
- en: '| 1 | Yes | 8 | 4 | 0.484 | 0.778 | 0.257 | 0.583 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 8 | 4 | 0.484 | 0.778 | 0.257 | 0.583 |'
- en: '| 1 | Yes | 10 | 4 | 0.482 | 0.777 | 0.257 | 0.584 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 10 | 4 | 0.482 | 0.777 | 0.257 | 0.584 |'
- en: '| 1 | Yes | 12 | 4 | 0.485 | 0.779 | 0.261 | 0.586 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 12 | 4 | 0.485 | 0.779 | 0.261 | 0.586 |'
- en: '| 1 | No | 12 | 4 | 0.472 | 0.771 | 0.248 | 0.576 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 否 | 12 | 4 | 0.472 | 0.771 | 0.248 | 0.576 |'
- en: Table [3](#A2.T3 "Table 3 ‣ B.2 Impact of Knowledge Distillation (KD), Epochs,
    and Batch Size on Prediction Accuracy ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")
    summarizes our results with different settings. We analyze the effect of each
    factor on the prediction accuracy in the following discussion.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#A2.T3 "表3 ‣ B.2 知识蒸馏 (KD)、周期和批量大小对预测准确性的影响 ‣ 附录B 扩展的消融研究 ‣ 硬件感知的并行提示解码以实现LLM推理的内存高效加速")
    总结了我们在不同设置下的结果。我们将在以下讨论中分析每个因素对预测准确性的影响。
- en: B.2.1 Training Epochs
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.2.1 训练周期
- en: We first investigate the effect of the number of training epochs on prediction
    accuracy. For models using 100 EPTs with KD enabled and a batch size of 4, we
    observe a steady improvement in prediction accuracy as the number of epochs increases.
    Specifically, the Top-1 accuracy at a 1-token distance increases from 0.504 at
    1 epoch to 0.525 at 12 epochs, while the Top-5 accuracy at a 1-token distance
    improves from 0.793 to 0.805\. Similarly, Top-1 accuracy at a 2-token distance
    increases from 0.273 to 0.308, and Top-5 accuracy at a 2-token distance improves
    from 0.598 to 0.625 over the same range of epochs. This trend demonstrates the
    positive impact of prolonged training on the performance of PPD when KD is applied.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调查了训练周期数对预测准确性的影响。对于使用100 EPTs且启用KD、批量大小为4的模型，我们观察到预测准确率随着周期数的增加而稳定提高。具体而言，1-token距离的Top-1准确率从1个epoch的0.504增加到12个epoch的0.525，而1-token距离的Top-5准确率从0.793提高到0.805。同样，2-token距离的Top-1准确率从0.273提高到0.308，2-token距离的Top-5准确率在相同的周期范围内从0.598提高到0.625。这一趋势展示了在应用KD时，延长训练对PPD性能的积极影响。
- en: B.2.2 Knowledge Distillation
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.2.2 知识蒸馏
- en: When KD is not applied, as shown for 100 EPTs at 12 epochs with a batch size
    of 4, the performance metrics are generally lower. The improvement in prediction
    accuracy with KD is up to 12%. This suggests that KD contributes significantly
    to prediction accuracy for PPD.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当未应用KD时，如在12个epoch的100 EPTs和批量大小为4的情况下，性能指标通常较低。KD带来的预测准确率提高最高可达12%。这表明KD对PPD的预测准确率有显著贡献。
- en: B.2.3 Effect of Batch Size
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.2.3 批量大小的影响
- en: We also examine the impact of batch size on the prediction accuracy. For the
    model trained with 100 EPTs, KD enabled, and 12 epochs, reducing the batch size
    from 4 to 1 results in a slight improvement in prediction accuracy up to 1%.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还考察了批量大小对预测准确性的影响。对于使用100 EPTs、启用KD和12个epoch的模型，将批量大小从4减少到1会使预测准确率略微提高，最多提高1%。
- en: B.3 Prefix Tuning + Prompt Token
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3 前缀调优 + 提示标记
- en: Prefix tuning [[14](#bib.bib14)], similar to prompt tuning, provides a parameter-efficient
    approach to fine-tune a pre-trained model. Unlike prompt tuning, it modifies the
    KV cache of every attention layer by prepending trained vectors. We hypothesize
    that the combination of prefix tuning and prompt tokens can lead to greater learning
    capacity and higher prediction accuracy. This hypothesis is based on the intuition
    that prompt tokens should see a different context than the input tokens when predicting
    long-range tokens. For example, if the input sequence is "Once upon a time", then
    enhancing the input with a prompt template might provide more suitable semantic
    context for long-range prediction. An enhanced input like "Predict the next-next
    token. Once upon a time" might empower the prompt token to predict the correct
    next-next token. Prefix tuning serves as the prompt template to enhance the hidden
    states visible to the prompt tokens.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调整[[14](#bib.bib14)]，类似于提示调整，提供了一种参数高效的方法来微调预训练模型。与提示调整不同的是，它通过在每个注意力层前添加训练过的向量来修改
    KV 缓存。我们假设前缀调整和提示标记的结合可以带来更大的学习能力和更高的预测准确率。这一假设基于这样一个直觉，即在预测长距离标记时，提示标记应该看到与输入标记不同的上下文。例如，如果输入序列是
    "Once upon a time"，那么使用提示模板来增强输入可能会为长距离预测提供更合适的语义上下文。像 "Predict the next-next
    token. Once upon a time" 这样的增强输入可能会使提示标记能够预测正确的下一个下一个标记。前缀调整作为提示模板来增强提示标记可见的隐藏状态。
- en: '![Refer to caption](img/0a56fc4b6e742fdfda0b368716c150a5.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0a56fc4b6e742fdfda0b368716c150a5.png)'
- en: 'Figure 10: ’P1’ is the prefix token for the prompt token ’S1’ and ’P2’ for
    ’S2’. ’C’ is the input token. The green tick means visibility during attention
    calculation. For instance, ’S1’ can see ’P1’ but cannot see ’P2’. ’C’ does not
    see any prefix tokens so the generated output corresponding to ’C’ is not altered
    by the use of prefix tuning.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：’P1’ 是提示标记 ’S1’ 的前缀标记，’P2’ 是 ’S2’ 的前缀标记。’C’ 是输入标记。绿色勾表示在注意力计算期间的可见性。例如，’S1’
    可以看到 ’P1’，但不能看到 ’P2’。’C’ 看不到任何前缀标记，因此 ’C’ 对应的生成输出不受前缀调整的影响。
- en: To retain the original model’s distribution, we modify the attention mask so
    that prefix tokens are only visible to prompt tokens. This ensures that we can
    generate outputs that preserve the original model’s distribution. We posit that
    prompt tokens at different positions should see different contexts so we allow
    a prompt token at a specific position to see a distinct set of prefix tokens,
    as shown in Figure [10](#A2.F10 "Figure 10 ‣ B.3 Prefix Tuning + Prompt Token
    ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference").
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持原始模型的分布，我们修改了注意力掩码，使得前缀标记仅对提示标记可见。这确保了我们可以生成保持原始模型分布的输出。我们假设不同位置的提示标记应该看到不同的上下文，因此我们允许特定位置的提示标记看到一组不同的前缀标记，如图[10](#A2.F10
    "Figure 10 ‣ B.3 Prefix Tuning + Prompt Token ‣ Appendix B Extended Ablation Study
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference")所示。
- en: 'Table 4: Prediction Accuracy of PPD with and without prefix tuning. 1 EPT is
    used for all models and 1 prefix token is used for prefix tuning.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：带有和不带有前缀调整的 PPD 预测准确率。所有模型使用 1 个 EPT，并且前缀调整使用 1 个前缀标记。
- en: '| Prefix Tuning | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 前缀调整 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| No | 0.485 | 0.779 | 0.261 | 0.586 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 序号 | 0.485 | 0.779 | 0.261 | 0.586 |'
- en: '| Yes | 0.412 | 0.738 | 0.204 | 0.541 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 0.412 | 0.738 | 0.204 | 0.541 |'
- en: Table [4](#A2.T4 "Table 4 ‣ B.3 Prefix Tuning + Prompt Token ‣ Appendix B Extended
    Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") compares the prediction accuracy of PPD with and
    without the use of prefix tuning. The results show that the models without prefix
    tuning outperform those with prefix tuning up to 28%, which suggests that, in
    this setup, prefix tuning does not enhance the prediction accuracy of PPD. Instead,
    it appears to degrade performance, potentially due to the complexity introduced
    by modifying the KV cache of attention layers with the prefix token. Unlike prompt
    tokens, prefix tokens do not interact with input tokens, meaning they do not change
    dynamically through the transformer layers based on the input context. This lack
    of interaction and dynamic adjustment could be a factor contributing to the decreased
    prediction accuracy observed with prefix tuning.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#A2.T4 "Table 4 ‣ B.3 Prefix Tuning + Prompt Token ‣ Appendix B Extended
    Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") 比较了使用和不使用前缀调优的 PPD 预测准确率。结果表明，没有前缀调优的模型在准确率上比有前缀调优的模型高达
    28%，这表明在这种设置下，前缀调优并未提升 PPD 的预测准确率。相反，它似乎会降低性能，可能是由于修改前缀令牌的注意力层 KV 缓存所引入的复杂性。与提示令牌不同，前缀令牌不会与输入令牌交互，即它们不会根据输入上下文通过变换器层动态调整。这种缺乏交互和动态调整可能是前缀调优导致预测准确率下降的一个因素。
- en: B.4 Custom Decoding Heads + Prompt Token
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.4 自定义解码头 + 提示令牌
- en: It has been demonstrated that a fine-tuned decoding head alone can effectively
    predict long-range tokens [[23](#bib.bib23), [1](#bib.bib1)]. Thus, we hypothesize
    that combining a separately fine-tuned decoding head with prompt tokens might
    further enhance the potential of PPD. As shown in Figure [11](#A2.F11 "Figure
    11 ‣ B.4 Custom Decoding Heads + Prompt Token ‣ Appendix B Extended Ablation Study
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference"), we trained a separate decoding head to transform only the hidden
    states of prompt tokens into logits. A key distinction from Medusa is that this
    decoding head is responsible for generating tokens at multiple positions, rather
    than just one.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，仅使用微调后的解码头就能有效预测长距离令牌 [[23](#bib.bib23), [1](#bib.bib1)]。因此，我们假设将单独微调的解码头与提示令牌结合，可能会进一步提升
    PPD 的潜力。如图 [11](#A2.F11 "Figure 11 ‣ B.4 Custom Decoding Heads + Prompt Token
    ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference") 所示，我们训练了一个单独的解码头，仅将提示令牌的隐藏状态转换为
    logits。与 Medusa 的关键区别在于，这个解码头负责在多个位置生成令牌，而不仅仅是一个位置。
- en: '![Refer to caption](img/9678fc8ddba45f8071566016b9982ee2.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9678fc8ddba45f8071566016b9982ee2.png)'
- en: 'Figure 11: Custom decoding head with PPD. The feature extractor refers to the
    LLMs without the decoding heads. ’H1’ is the generated hidden state for the input
    token ’C’. ’H2’ is the hidden state for the prompt token ’S1’ and ’H3’ for ’S2’.
    ’LM1’ is the original LLM’s decoding head and it takes in the hidden states of
    input tokens. ’LM2’ is the custom decoding heads for PPD and only takes in the
    hidden states of prompt tokens.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：带有 PPD 的自定义解码头。特征提取器指没有解码头的 LLM。‘H1’ 是输入令牌 ‘C’ 的生成隐藏状态。‘H2’ 是提示令牌 ‘S1’
    的隐藏状态，‘H3’ 是 ‘S2’ 的隐藏状态。‘LM1’ 是原始 LLM 的解码头，它接收输入令牌的隐藏状态。‘LM2’ 是 PPD 的自定义解码头，仅接收提示令牌的隐藏状态。
- en: We propose two training methods. In the first method, the custom decoding head
    and prompt tokens are trained together from scratch in a single stage. In the
    second method, the prompt tokens are initially trained for 2 epochs, followed
    by training both the prompt tokens and the decoding head with a smaller learning
    rate in a two-stage process.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了两种训练方法。在第一种方法中，自定义解码头和提示令牌一起从头开始训练，使用单阶段训练。在第二种方法中，提示令牌首先训练 2 个周期，然后以较小的学习率进行两阶段训练，训练提示令牌和解码头。
- en: 'Table 5: Prediction Accuracy of PPD with and without custom decoding head.
    1 EPT is used for all models. 1-stage and 2-stage refer to the training strategies
    of custom decoding head.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：PPD 在有无自定义解码头的预测准确率。所有模型均使用 1 EPT。1-stage 和 2-stage 指自定义解码头的训练策略。
- en: '| Method Name | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PPD without custom decoding head | 0.485 | 0.779 | 0.261 | 0.586 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 不带自定义解码头的 PPD | 0.485 | 0.779 | 0.261 | 0.586 |'
- en: '| PPD with custom decoding head (1-stage) | 0.385 | 0.614 | 0.229 | 0.482 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 带自定义解码头的 PPD（1-stage） | 0.385 | 0.614 | 0.229 | 0.482 |'
- en: '| PPD with custom decoding head (2-stage) | 0.506 | 0.795 | 0.276 | 0.602 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 带有自定义解码头的PPD（两阶段） | 0.506 | 0.795 | 0.276 | 0.602 |'
- en: Table [5](#A2.T5 "Table 5 ‣ B.4 Custom Decoding Heads + Prompt Token ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") presents the prediction accuracy of PPD with and
    without a custom decoding head. When trained using the single-stage method, PPD
    with the custom decoding head shows a 12%-21% decrease in prediction accuracy
    compared to the baseline PPD without the custom decoding head. This suggests that
    the single-stage approach does not result in stable or effective training.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 表[5](#A2.T5 "Table 5 ‣ B.4 Custom Decoding Heads + Prompt Token ‣ Appendix B
    Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference")展示了使用和不使用自定义解码头的PPD预测准确性。当采用单阶段方法训练时，使用自定义解码头的PPD相比于基线PPD（没有自定义解码头）显示出12%-21%的预测准确性下降。这表明单阶段方法不会导致稳定或有效的训练。
- en: In contrast, the two-stage training method results in a limited improvement
    of 2.1%-4.3% in prediction accuracy compared to the baseline. This suggests that
    adding a custom decoding head may not be necessary, given the additional trainable
    parameters and the limited improvement in prediction accuracy.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，两阶段训练方法在预测准确性上仅有2.1%-4.3%的有限改善。这表明，考虑到额外的可训练参数和有限的准确性提升，添加自定义解码头可能并非必要。
- en: B.5 Attention Masking for EPTs
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.5 EPTs的注意力掩蔽
- en: In this paper, we proposed a specialized attention mask for EPTs to achieve
    the effect of prompt ensemble. However, there are alternative masking strategies
    available. Here, we describe and compare three types of attention masks that we
    implemented and experimented with.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种针对EPTs的专用注意力掩蔽，以实现提示集合效果。然而，还有其他可用的掩蔽策略。在这里，我们描述并比较了我们实现和实验过的三种注意力掩蔽类型。
- en: '![Refer to caption](img/93523f27ca451486a148386453552cee.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/93523f27ca451486a148386453552cee.png)'
- en: (a) Ensemble Attention Mask
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 集合注意力掩蔽
- en: '![Refer to caption](img/25af6ca50580371f4c35376fa7d93be0.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25af6ca50580371f4c35376fa7d93be0.png)'
- en: (b) Decoder-like Attention Mask
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 解码器风格注意力掩蔽
- en: '![Refer to caption](img/367dba5a22a6a6901e63c805b6948cf4.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/367dba5a22a6a6901e63c805b6948cf4.png)'
- en: (c) Encoder-like Attention Mask
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 编码器风格注意力掩蔽
- en: 'Figure 12: Different Mask Strategies for EPTs. ’C’ is an input token. ’V1’
    and ’V2’ are the EPTs for prompt tokens ’S1’ and ’V3’ and ’V4’ for ’S2’.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：不同的EPT掩蔽策略。‘C’是输入标记。‘V1’和‘V2’是提示标记‘S1’的EPTs，‘V3’和‘V4’是‘S2’的EPTs。
- en: B.5.1 Ensemble Attention Masking
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.5.1 集合注意力掩蔽
- en: The ensemble attention masking is the masking strategy we previously described.
    In this approach, EPTs are divided into $n$ in group $i$. Since this masking strategy
    effectively averages the results of disjoint groups of EPTs, we refer to it as
    the "ensemble attention masking". Figure [12(a)](#A2.F12.sf1 "In Figure 12 ‣ B.5
    Attention Masking for EPTs ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")
    provides an example of the ensemble attention masking.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 集合注意力掩蔽是我们之前描述的掩蔽策略。在这种方法中，EPTs被划分为第$i$组中的$n$。由于这种掩蔽策略有效地平均了不相交的EPT组的结果，我们称之为“集合注意力掩蔽”。图[12(a)](#A2.F12.sf1
    "In Figure 12 ‣ B.5 Attention Masking for EPTs ‣ Appendix B Extended Ablation
    Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration
    of LLM Inference")提供了集合注意力掩蔽的示例。
- en: B.5.2 Decoder-like Attention Masking
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.5.2 解码器风格注意力掩蔽
- en: Decoder-like attention masking is a simple strategy where EPTs can only attend
    to EPTs with smaller position indices. This results in a triangular-shaped attention
    mask, similar to the one used in decoder layers, hence the name "decoder-like
    attention masking". Figure [12(b)](#A2.F12.sf2 "In Figure 12 ‣ B.5 Attention Masking
    for EPTs ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference") provides an example
    of this masking strategy.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器风格注意力掩蔽是一种简单策略，其中EPTs只能关注具有较小位置索引的EPTs。这会导致一个三角形形状的注意力掩蔽，类似于解码器层中使用的掩蔽，因此命名为“解码器风格注意力掩蔽”。图[12(b)](#A2.F12.sf2
    "In Figure 12 ‣ B.5 Attention Masking for EPTs ‣ Appendix B Extended Ablation
    Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration
    of LLM Inference")提供了这种掩蔽策略的示例。
- en: B.5.3 Encoder-like Attention Masking
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.5.3 编码器风格注意力掩蔽
- en: In encoder-like attention masking, an EPT corresponding to a prompt token $P$.
    This allows EPTs to see both preceding and succeeding EPTs, similar to the token
    visibility in an encoder layer, hence the name "encoder-like attention masking".
    Figure [12(c)](#A2.F12.sf3 "In Figure 12 ‣ B.5 Attention Masking for EPTs ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") illustrates this masking strategy.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似编码器的注意力掩码中，一个EPT对应于一个提示标记$P$。这使得EPT可以看到前面的和后面的EPT，类似于编码器层中的标记可见性，因此称为“类似编码器的注意力掩码”。图[12(c)](#A2.F12.sf3
    "在图12 ‣ B.5 注意力掩码用于EPT ‣ 附录B 扩展消融研究 ‣ 硬件感知并行提示解码以实现LLM推理的内存高效加速")展示了这种掩码策略。
- en: B.5.4 Results
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.5.4 结果
- en: 'Table 6: Prediction Accuracy of PPD with different attention masking strategies
    for EPTs. 100 EPT is used for all models.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：使用不同注意力掩码策略的PPD对EPT的预测准确率。所有模型使用100个EPT。
- en: '| Method Name | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PPD with ensemble attention mask | 0.506 | 0.794 | 0.276 | 0.602 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 使用集成注意力掩码的PPD | 0.506 | 0.794 | 0.276 | 0.602 |'
- en: '| PPD with decoder attention mask | 0.465 | 0.755 | 0.262 | 0.572 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 使用解码器注意力掩码的PPD | 0.465 | 0.755 | 0.262 | 0.572 |'
- en: '| PPD with encoder attention mask | 0.473 | 0.765 | 0.256 | 0.573 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 使用编码器注意力掩码的PPD | 0.473 | 0.765 | 0.256 | 0.573 |'
- en: The results in Table [6](#A2.T6 "Table 6 ‣ B.5.4 Results ‣ B.5 Attention Masking
    for EPTs ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference") indicate that the
    ensemble attention mask outperforms the other masking strategies. In comparison,
    the PPD with decoder attention mask shows 4.9%-8.0% lower prediction accuracy.
    The PPD with encoder attention mask also underperforms in prediction accuracy
    relative to the ensemble attention mask by 3.7%-7.2%.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表[6](#A2.T6 "表6 ‣ B.5.4 结果 ‣ B.5 注意力掩码用于EPT ‣ 附录B 扩展消融研究 ‣ 硬件感知并行提示解码以实现LLM推理的内存高效加速")中的结果表明，集成注意力掩码优于其他掩码策略。相比之下，使用解码器注意力掩码的PPD预测准确率低4.9%-8.0%。使用编码器注意力掩码的PPD的预测准确率也比集成注意力掩码低3.7%-7.2%。
- en: These results suggest that the ensemble attention mask is the most effective
    strategy among the three, likely due to its ability to effectively average the
    votes of disjoint groups of EPTs, thereby improving prediction accuracy. The decoder-like
    and encoder-like attention masks, while simpler, do not provide the same level
    of performance, indicating that the structure and specificity of the ensemble
    attention mask better facilitate accurate long-range token prediction. Additionally,
    ensemble attention masking is more sparse, which offers greater potential for
    optimization.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，集成注意力掩码在三者中最为有效，可能是由于它能有效地平均来自不同EPT组的投票，从而提高预测准确率。虽然类似解码器和类似编码器的注意力掩码更简单，但它们的性能不如集成注意力掩码，这表明集成注意力掩码的结构和特异性更好地促进了准确的长期标记预测。此外，集成注意力掩码更加稀疏，提供了更大的优化潜力。
- en: B.6 Aggregation Method for EPTs
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.6 EPT的聚合方法
- en: 'In addition to simply averaging the logits from EPTs, we explored more advanced
    aggregation methods. For instance, we applied learned weights to aggregate the
    logits. The final logit $p$ can be expressed as:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简单地平均EPT的logits外，我们还探索了更先进的聚合方法。例如，我们应用了学习权重来聚合logits。最终的logit $p$ 可以表示为：
- en: '|  | $p=\sum_{i=1}^{n}w_{i}\cdot p_{i},$ |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $p=\sum_{i=1}^{n}w_{i}\cdot p_{i},$ |  |'
- en: where $n$ EPT.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$n$为EPT。
- en: 'Table 7: Prediction Accuracy of PPD with different aggregation methods for
    EPTs. 100 EPT is used for all models.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：使用不同聚合方法的PPD对EPT的预测准确率。所有模型使用100个EPT。
- en: '| Aggregation Method | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 聚合方法 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Average | 0.506 | 0.794 | 0.276 | 0.602 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 0.506 | 0.794 | 0.276 | 0.602 |'
- en: '| Learned Weight | 0.503 | 0.779 | 0.250 | 0.576 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 学习权重 | 0.503 | 0.779 | 0.250 | 0.576 |'
- en: 'The results in Table [7](#A2.T7 "Table 7 ‣ B.6 Aggregation Method for EPTs
    ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference") show the prediction accuracy
    of PPD with two different aggregation methods for EPTs: simple averaging and learned
    weights. When using learned weights to aggregate logits, the model shows a slight
    decrease of 0.6%-9.4% in prediction accuracy.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](#A2.T7 "Table 7 ‣ B.6 Aggregation Method for EPTs ‣ Appendix B Extended
    Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference")中的结果显示了PPD在两种不同的EPT聚合方法下的预测准确度：简单平均和学习权重。使用学习权重进行logits聚合时，模型的预测准确度略微下降了0.6%-9.4%。
- en: These results suggest that while learned weights provide a more flexible aggregation
    method, they do not necessarily lead to improved prediction accuracy in this context.
    The simplicity and stability of the averaging method appear to offer better performance,
    possibly due to the additional complexity and potential overfitting introduced
    by learning the weights.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，尽管学习权重提供了更灵活的聚合方法，但在此上下文中并不一定会提高预测准确度。平均方法的简单性和稳定性似乎提供了更好的性能，这可能是由于学习权重引入的额外复杂性和潜在的过拟合。
- en: B.7 Multi-exit Ensemble
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.7 多出口集成
- en: While using EPTs for prompt ensemble improves prediction accuracy, it also increases
    input length, resulting in higher computational overhead and forward pass latency.
    To address this, we propose the use of a multi-exit ensemble method. In multi-exit
    ensemble, the hidden states of a prompt token from the last $k$ decoder layers
    are extracted and averaged to produce the final hidden state, which is then decoded
    by the decoding head into a guess token, as illustrated in Figure [13](#A2.F13
    "Figure 13 ‣ B.7 Multi-exit Ensemble ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference").
    This approach achieves prompt ensemble without the associated computational costs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用EPT进行提示集成可以提高预测准确度，但也会增加输入长度，从而导致更高的计算开销和前向传递延迟。为了解决这个问题，我们提出了多出口集成方法。在多出口集成中，从最后$k$个解码器层提取并平均一个提示令牌的隐藏状态，以生成最终的隐藏状态，然后由解码头解码为猜测令牌，如图[13](#A2.F13
    "Figure 13 ‣ B.7 Multi-exit Ensemble ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")所示。这种方法在没有相关计算成本的情况下实现了提示集成。
- en: '![Refer to caption](img/d5a14b7eaa1e26a1cacb1d6c442402bf.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d5a14b7eaa1e26a1cacb1d6c442402bf.png)'
- en: 'Figure 13: Mult-exit ensemble. ’D1’, ’D10’, ’D11’, and ’D12’ are the decoder
    layers in order. ’S1’ is a prompt token and ’H1’, ’H2’, ’H3’ are the corresponding
    hidden states from the last 3 decoder layers. ’H4’ is obtained from averaging
    these 3 hidden states. The decoding head ’LM’ translates ’H4’ into a token ’E’.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：多出口集成。’D1’，’D10’，’D11’，和’D12’是按顺序排列的解码器层。’S1’是提示令牌，’H1’，’H2’，’H3’是最后3个解码器层的对应隐藏状态。’H4’是通过平均这3个隐藏状态获得的。解码头’LM’将’H4’翻译为令牌’E’。
- en: The hypothesis is that taking the hidden states from the last few decoder layers
    for ensemble might work because these layers capture increasingly abstract and
    high-level representations of the input sequence. By averaging the hidden states
    from multiple layers, we can combine diverse but complementary information, leading
    to a more robust and accurate final hidden state. Additionally, since the final
    layers are closest to the output, they are more likely to contain refined and
    contextually relevant information, making the ensemble more effective.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 假设是，使用最后几个解码器层的隐藏状态进行集成可能有效，因为这些层捕捉到输入序列的越来越抽象和高级的表示。通过平均多个层的隐藏状态，我们可以结合多样但互补的信息，从而得到一个更强大和准确的最终隐藏状态。此外，由于最终层最接近输出，它们更可能包含经过精炼和上下文相关的信息，使集成更加有效。
- en: 'Table 8: Prediction Accuracy of PPD with and without multi-exit ensemble. 1
    EPT is used for all models. $k$ exits refer to the number of exits used.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：PPD在有无多出口集成下的预测准确度。所有模型使用1 EPT。$k$出口指使用的出口数量。
- en: '| Method Name | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PPD without multi-exit | 0.485 | 0.779 | 0.261 | 0.586 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| PPD without multi-exit | 0.485 | 0.779 | 0.261 | 0.586 |'
- en: '| PPD with 3 exits | 0.422 | 0.723 | 0.214 | 0.517 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| PPD with 3 exits | 0.422 | 0.723 | 0.214 | 0.517 |'
- en: '| PPD with 2 exits | 0.420 | 0.723 | 0.213 | 0.518 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| PPD with 2 exits | 0.420 | 0.723 | 0.213 | 0.518 |'
- en: Table [8](#A2.T8 "Table 8 ‣ B.7 Multi-exit Ensemble ‣ Appendix B Extended Ablation
    Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration
    of LLM Inference") shows the comparison of prediction accuracy of PPD with and
    without mult-exit ensemble. The results indicate that the introduction of multi-exit
    ensemble with both 2 and 3 exits results in a 7%-18% decrease in prediction accuracy
    compared to the baseline model without multi-exit.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表[8](#A2.T8 "Table 8 ‣ B.7 Multi-exit Ensemble ‣ Appendix B Extended Ablation
    Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration
    of LLM Inference")展示了PPD在使用和不使用多出口集成时预测准确性的比较。结果表明，引入2个和3个出口的多出口集成会导致预测准确性比基线模型降低7%-18%。
- en: These findings suggest that the multi-exit ensemble approach, as implemented,
    does not enhance prediction accuracy and instead leads to a notable decrease in
    performance. This may be due to the averaging of hidden states from multiple layers
    introducing noise or reducing the specificity of the representations needed for
    accurate prediction. Further refinement of the multi-exit ensemble may be necessary
    to achieve the desired improvements in accuracy.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现表明，所实现的多出口集成方法并没有提升预测准确性，反而导致了性能显著下降。这可能是由于从多个层的隐藏状态进行平均引入了噪声或减少了对准确预测所需的表示的特异性。可能需要进一步优化多出口集成以实现所需的准确性改进。
- en: Appendix C Experiment Details
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录C 实验细节
- en: For the throughput experiments, each result is obtained by averaging three separate
    runs. The standard deviations of these runs are reported as error bars in the
    bar charts. To ensure a fair comparison in our comparative experiments, we maintained
    consistent hardware settings and software versions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于吞吐量实验，每个结果是通过三次独立运行的平均值得出的。这些运行的标准差作为误差条显示在条形图中。为了确保比较实验的公平性，我们保持了一致的硬件设置和软件版本。
- en: We selected 3 prompt tokens because adding more would not further increase the
    expected acceptance length due to the tree size limit. The number of EPTs per
    prompt token was optimized to maximize throughput.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了3个提示符令牌，因为增加更多的提示符令牌不会进一步增加预期的接受长度，这是由于树的大小限制。每个提示符令牌的EPT数量经过优化，以最大化吞吐量。
- en: In Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference"), the temperature
    settings for PPD, Eagle [[16](#bib.bib16)], and Medusa [[1](#bib.bib1)] follow
    the default configuration, while the other models use a greedy setting (temperature=0).
    This choice is based on findings that retrieval-based methods perform significantly
    worse in non-greedy settings. Similarly, LOOKAHEAD DECODING [[8](#bib.bib8)],
    REST [[9](#bib.bib9)], and PLD [[21](#bib.bib21)] in Fig. [4](#S5.F4 "Figure 4
    ‣ 5.1 Speedup Comparison with Parallel Decoding Methods ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")
    also use a temperature setting of 0 for the same reasons.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference")中，PPD、Eagle [[16](#bib.bib16)]
    和Medusa [[1](#bib.bib1)]的温度设置遵循默认配置，而其他模型使用贪婪设置（温度=0）。这一选择基于检索方法在非贪婪设置下表现显著更差的发现。同样，LOOKAHEAD
    DECODING [[8](#bib.bib8)]、REST [[9](#bib.bib9)] 和PLD [[21](#bib.bib21)]在图[4](#S5.F4
    "Figure 4 ‣ 5.1 Speedup Comparison with Parallel Decoding Methods ‣ 5 Experiments
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference")中也使用了温度设置为0的原因相同。
- en: Appendix D Limitations
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录D 局限性
- en: 'Despite its efficiency, we have identified the following limitations of PPD:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PPD具有高效性，我们仍然发现了以下局限性：
- en: '1.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Low prediction accuracy for very small models. We found that for very small
    models like Vicuna-68M [[24](#bib.bib24)], which only has 2 decoder layers and
    an embedding dimension of less than 1000, PPD suffers from low prediction accuracy.
    This is because the embedding dimension determines the expressive power of a prompt
    token, and the transformer architecture’s depth is crucial for efficient information
    flow to the prompt tokens.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于非常小的模型，预测准确性较低。我们发现，对于像Vicuna-68M [[24](#bib.bib24)]这样只有2个解码器层和嵌入维度不足1000的非常小的模型，PPD的预测准确性较低。这是因为嵌入维度决定了提示符令牌的表达能力，而变换器架构的深度对信息高效传递至提示符令牌至关重要。
- en: '2.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: GPU compute resource constraint. Since PPD trades additional compute resources
    for increased throughput, its effectiveness depends on the availability of idle
    GPU compute resources. On a GPU with limited compute resources, the speedup ratios
    achieved by PPD are expected to decrease.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPU 计算资源限制。由于 PPD 通过增加吞吐量来交换额外的计算资源，因此其效果取决于空闲 GPU 计算资源的可用性。在计算资源有限的 GPU 上，PPD
    实现的加速比预计会下降。
- en: '3.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Extended input length. The improvement in acceptance length with PPD is not
    as significant as the gain in prediction accuracy compared to Medusa. This is
    because PPD must reserve a substantial portion of the input for prompt tokens,
    which limits the size of the sparse tree that can be used.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入长度的扩展。与 Medusa 相比，PPD 在接受长度上的改进不如在预测准确度上的提升显著。这是因为 PPD 必须保留大量的输入用于提示标记，这限制了可以使用的稀疏树的大小。
- en: Appendix E Societal Impact
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 E 社会影响
- en: In this paper, we proposed PPD to accelerate LLMs easily and cheaply. Since
    PPD reduces the time required for handling a single inference request, it could
    bring down the cost of deploying LLMs for both the companies and the public. This
    might lead to increased accessibility of LLM services. Moreover, latency-sensitive
    applications like chatbots will benefit greatly from the usage of PPD as it reduces
    the inference latency greatly, thereby enhancing the user experience.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 PPD 以便轻松且廉价地加速 LLM。由于 PPD 减少了处理单个推断请求所需的时间，这可能降低公司和公众部署 LLM 的成本。这可能导致
    LLM 服务的可及性提高。此外，像聊天机器人这样的延迟敏感型应用将从 PPD 的使用中大大受益，因为它大幅降低了推断延迟，从而提升了用户体验。
- en: While PPD aims to make AI more accessible, there may still be a digital divide
    where certain communities lack the necessary infrastructure, such as stable internet
    connections or modern hardware, to fully benefit from these advancements. This
    could further widen the gap between technology-privileged and underserved populations.
    On the other hand, PPD might be misused by malicious parties to manipulate the
    output of the original LLM, resulting in the generation of unreliable information
    and fake data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PPD 旨在使 AI 更加可及，但某些社区可能仍然存在数字鸿沟，例如缺乏稳定的互联网连接或现代硬件，无法充分利用这些进步。这可能进一步拉大技术优越与服务不足人群之间的差距。另一方面，PPD
    可能被恶意方滥用，操控原始 LLM 的输出，导致生成不可靠的信息和虚假数据。
