- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:43:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:43:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language we
    Prompt them in
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的伦理推理和道德价值对齐取决于我们提示它们的语言
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18460](https://ar5iv.labs.arxiv.org/html/2404.18460)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18460](https://ar5iv.labs.arxiv.org/html/2404.18460)
- en: Abstract
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Ethical reasoning is a crucial skill for Large Language Models (LLMs). However,
    moral values are not universal, but rather influenced by language and culture.
    This paper explores how three prominent LLMs – GPT-4, ChatGPT, and Llama2-70B-Chat
    – perform ethical reasoning in different languages and if their moral judgement
    depend on the language in which they are prompted. We extend the study of ethical
    reasoning of LLMs by Rao et al. ([2023](#bib.bib36)) to a multilingual setup following
    their framework of probing LLMs with ethical dilemmas and policies from three
    branches of normative ethics: deontology, virtue, and consequentialism. We experiment
    with six languages: English, Spanish, Russian, Chinese, Hindi, and Swahili. We
    find that GPT-4 is the most consistent and unbiased ethical reasoner across languages,
    while ChatGPT and Llama2-70B-Chat show significant moral value bias when we move
    to languages other than English. Interestingly, the nature of this bias significantly
    vary across languages for all LLMs, including GPT-4.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理推理是大型语言模型（LLMs）的一项关键技能。然而，道德价值观不是普遍存在的，而是受到语言和文化的影响。本文探讨了三种主要的LLMs——GPT-4、ChatGPT和Llama2-70B-Chat——在不同语言中的伦理推理表现，以及它们的道德判断是否依赖于提示它们的语言。我们在Rao等人（[2023](#bib.bib36)）的伦理推理研究的基础上，扩展到多语言环境，遵循他们通过三种规范伦理学分支（义务论、美德伦理学和结果论）中的伦理困境和政策对LLMs进行探测的框架。我们在六种语言中进行实验：英语、西班牙语、俄语、中文、印地语和斯瓦希里语。我们发现，GPT-4在所有语言中是最一致和最无偏见的伦理推理者，而ChatGPT和Llama2-70B-Chat在使用非英语语言时表现出显著的道德价值偏见。有趣的是，这种偏见在所有LLMs（包括GPT-4）中在不同语言间显著变化。
- en: \NAT@set@cites
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \NAT@set@cites
- en: Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language we
    Prompt them in
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的伦理推理和道德价值对齐取决于我们提示它们的语言
- en: '| Utkarsh Agarwal^(∗1)    Kumar Tanmay^(∗1)    Aditi Khandelwal^(∗1)   Monojit
    Choudhury^(†2) |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| Utkarsh Agarwal^(∗1)    Kumar Tanmay^(∗1)    Aditi Khandelwal^(∗1)   Monojit
    Choudhury^(†2) |'
- en: '| ^∗Microsoft Corporation |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| ^∗微软公司 |'
- en: '| ^†MBZUAI |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| ^†MBZUAI |'
- en: '| {t-utagarwal, t-ktanmay, t-aditikh}@microsoft.com, monojit.choudhury@mbzuai.ac.ae
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| {t-utagarwal, t-ktanmay, t-aditikh}@microsoft.com, monojit.choudhury@mbzuai.ac.ae
    |'
- en: Abstract content
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要内容
- en: '¹¹footnotetext: Equal contribution.²²footnotetext: Work done while at Microsoft.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹脚注：贡献相等。²²脚注：工作完成于微软。
- en: 1.   Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.   引言
- en: Large Language Models (LLMs) like ChatGPT have gained popularity all over the
    world for their ability to generate fluent and engaging natural language texts
    Schulman et al. ([2022](#bib.bib39)); OpenAI ([2023](#bib.bib33)). However, the
    widespread and rapid use of LLMs has brought about ethical concerns and potential
    problems, especially when we consider using them in different languages Blodgett
    et al. ([2021](#bib.bib6)); Choudhury and Deshpande ([2021](#bib.bib10)); Wang
    et al. ([2023](#bib.bib46)); Ahuja et al. ([2023](#bib.bib1)). As LLMs become
    more prevalent and find applications in everyday life, they must confront complex
    moral dilemmas rooted in the existence of multiple conflicting values, commonly
    dubbed as the problem of value pluralism James ([1891](#bib.bib25)); Dai and Dimond
    ([1998](#bib.bib15)); Ramesh et al. ([2023](#bib.bib35)). Several researchers
    (see for instance Rao et al. ([2023](#bib.bib36)) and Zhou et al. ([2023](#bib.bib51)))
    have argue that instead of being firmly aligned to a specific set of values, LLMs
    should be trained to function as generic ethical reasoners, adaptable to different
    contexts and languages. The final moral judgments, of course, should be made by
    the stakeholders at different stages of the application life-cycle. LLMs should
    be able to reason ethically in a generic way, given a situation and a moral stance,
    and should soundly resolve the dilemma when possible, or else ask for more clarity
    on the stance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 像 ChatGPT 这样的语言模型（LLMs）由于能够生成流畅且引人入胜的自然语言文本而在全球范围内获得了普及 Schulman 等人 ([2022](#bib.bib39));
    OpenAI ([2023](#bib.bib33))。然而，LLMs 的广泛和快速使用带来了伦理担忧和潜在问题，特别是当我们考虑在不同语言中使用它们时 Blodgett
    等人 ([2021](#bib.bib6)); Choudhury 和 Deshpande ([2021](#bib.bib10)); Wang 等人 ([2023](#bib.bib46));
    Ahuja 等人 ([2023](#bib.bib1))。随着 LLMs 的普及并在日常生活中找到应用，它们必须面对根植于多种冲突价值存在的复杂道德困境，通常被称为价值多元主义问题
    James ([1891](#bib.bib25)); Dai 和 Dimond ([1998](#bib.bib15)); Ramesh 等人 ([2023](#bib.bib35))。一些研究者（例如
    Rao 等人 ([2023](#bib.bib36)) 和 Zhou 等人 ([2023](#bib.bib51))) 认为，与其将 LLMs 固定为特定的价值观，不如训练它们作为通用的伦理推理者，适应不同的背景和语言。当然，最终的道德判断应由不同阶段的利益相关者做出。LLMs
    应该能够在给定情况和道德立场的情况下以通用的方式进行伦理推理，并在可能的情况下合理解决困境，否则应请求更多关于立场的明确性。
- en: In a recent study, Rao et al. ([2023](#bib.bib36)) has demonstrated that LLMs,
    especially GPT-4, are capable of carrying out sound ethical reasoning. They showed
    that when the LLMs are presented with a moral dilemma and a moral stance presented
    at different levels of abstraction and following different formalisms of normative
    ethics in the prompt, they are often capable of resolving the dilemma in a way
    that is consistent with the moral stance. They further argue that this is a promising
    direction towards solving the issues of value pluralism at a global scale, because
    the different stake-holders in the development and use of an AI system can specify
    their moral stance which can be meaningfully consumed in the prompt by the LLM
    to arrive at a sound moral judgment. The alternative approach of aligning LLMs
    to moral values is bound to fail due to the absence of a universal value hierarchy.
    However, this study was conducted only for English.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一项研究中，Rao 等人 ([2023](#bib.bib36)) 证明了大型语言模型（LLMs），尤其是 GPT-4，能够进行合理的伦理推理。他们展示了，当
    LLMs 面对道德困境时，无论道德立场以何种抽象层次和不同的规范伦理形式呈现，LLMs 往往能够以与道德立场一致的方式解决困境。他们进一步认为，这是一种有前景的方法，可以在全球范围内解决价值多元主义的问题，因为在开发和使用
    AI 系统的不同利益相关者可以指定他们的道德立场，并且 LLM 可以在提示中有意义地利用这些信息来得出合理的道德判断。将 LLM 调整为道德价值观的替代方法注定会失败，因为缺乏普遍的价值层次。然而，这项研究仅在英语中进行。
- en: It is a well established fact that the abilities of the LLMs in languages beyond
    English are often poor and unpredictable (see for example Ahuja et al. ([2023](#bib.bib1)),
    Zhao et al. ([2023](#bib.bib50)) and Wang et al. ([2023](#bib.bib46))). Moreover,
    an intriguing human phenomenon known as the Foreign Language effect Costa et al.
    ([2014a](#bib.bib13)) comes into play when people face moral dilemmas presented
    in a foreign language (L2). People often make different moral judgments in L2
    when compared to when they encounter the same dilemmas in their native language
    (L1). This suggests that language can significantly shape our emotional and cognitive
    responses to moral situations, influencing our choices and beliefs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，LLMs在英语以外语言中的能力往往较差且难以预测（例如，参见Ahuja等人（[2023](#bib.bib1)）、Zhao等人（[2023](#bib.bib50)）和Wang等人（[2023](#bib.bib46)））。此外，一种有趣的人类现象，即外语效应Costa等人（[2014a](#bib.bib13)），在面对外语（L2）中的道德困境时会发挥作用。人们在L2中作出的道德判断往往与在母语（L1）中面对相同困境时的判断不同。这表明，语言可以显著影响我们对道德情境的情感和认知反应，从而影响我们的选择和信念。
- en: 'Given this complex interplay of culture, language and values in making moral
    judgments, one might ask: Do LLMs also exhibit a Foreign Language effect, changing
    their behavior when confronted with moral dilemmas in different languages? In
    this work, we extend the study by Rao et al. ([2023](#bib.bib36)) to five languages
    other than English, namely Spanish, Russian, Chinese, Hindi, Arabic, and Swahili.
    We probe three popular LLMs: GPT-4 OpenAI ([2023](#bib.bib33)), ChatGPT (September
    2023) Schulman et al. ([2022](#bib.bib39)) and Llama2-70B-Chat Touvron et al.
    ([2023](#bib.bib45)) systematically to assess their ethical reasoning abilities
    in different languages by prompting them to resolve an ethical dilemma reflecting
    conflicts between interpersonal, professional, social and cultural values, and
    a set of ethical policies (i.e., moral stances) that can help arrive at a clear
    resolution of the dilemma. These policies are drawn from three branches of normative
    ethics: deontology Alexander and Moore ([2021](#bib.bib2)), virtue Hursthouse
    and Pettigrove ([2022](#bib.bib23)) and consequentialism Sinnott-Armstrong ([2022](#bib.bib40))
    and have three different levels of abstractions (similar to Rao et al. ([2023](#bib.bib36))).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于文化、语言和价值观在道德判断中的复杂相互作用，人们可能会问：大型语言模型（LLMs）是否也表现出外语效应，当面对不同语言中的道德困境时，其行为是否会发生变化？在这项工作中，我们将Rao等人（[2023](#bib.bib36)）的研究扩展到五种除英语之外的语言，即西班牙语、俄语、中文、印地语、阿拉伯语和斯瓦希里语。我们系统地探讨了三种流行的LLMs：GPT-4
    OpenAI（[2023](#bib.bib33)）、ChatGPT（2023年9月）Schulman等人（[2022](#bib.bib39)）以及Llama2-70B-Chat
    Touvron等人（[2023](#bib.bib45)），以评估它们在不同语言中的伦理推理能力，通过提示它们解决反映人际、职业、社会和文化价值冲突的伦理困境，以及一套可以帮助明确解决困境的伦理政策（即道德立场）。这些政策源自规范伦理学的三个分支：义务论Alexander和Moore（[2021](#bib.bib2)）、美德伦理学Hursthouse和Pettigrove（[2022](#bib.bib23)）和结果论Sinnott-Armstrong（[2022](#bib.bib40)），并具有三个不同的抽象层次（类似于Rao等人（[2023](#bib.bib36)））。
- en: 'Our study clearly demonstrates that indeed the LLMs exhibit different biases
    while resolving the moral dilemmas in different languages. This bias is minimal
    for English, in the sense that the resolution of the dilemma depends on the ethical
    policy rather than the model’s own judgment, and it is maximum for low-resource
    languages such as Hindi and Swahili. This observation can also be interpreted
    as a reduced ethical reasoning capability of the LLMs in languages beyond English.
    However, as we shall see in this paper, the ethical reasoning ability (or conversely
    the bias) is dilemma-specific, which makes us conclude that the LLMs have strong
    value alignment biases in languages beyond English. Other salient findings are:
    (1) Across all languages, GPT-4 has the highest ethical reasoning ability, while
    Llama2-70B-Chat has the poorest; (2) across all models, the reasoning is poorest
    for Hindi and Swahili, while best for unsurprisingly, English, and also Russian;
    and (3) across all models, English and Spanish, and Hindi and Chinese have similar
    bias patterns.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究清楚地表明，LLMs 确实在解决不同语言中的道德困境时表现出不同的偏见。这种偏见在英语中较小，因为困境的解决依赖于伦理政策而非模型自身的判断，而在如印地语和斯瓦希里语等低资源语言中则最大。这一观察也可以解读为
    LLMs 在英语之外的语言中伦理推理能力的减弱。然而，正如我们将在本文中看到的，伦理推理能力（或相反的偏见）是特定于困境的，这使我们得出结论，LLMs 在英语之外的语言中存在强烈的价值观对齐偏见。其他显著发现包括：（1）在所有语言中，GPT-4
    的伦理推理能力最高，而 Llama2-70B-Chat 的最差；（2）在所有模型中，印地语和斯瓦希里语的推理能力最差，而英语和俄语则最好；（3）在所有模型中，英语和西班牙语、印地语和中文具有相似的偏见模式。
- en: 2.   Background
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.   背景
- en: The topic of right and wrong has been a subject of ongoing discussion among
    philosophers, psychologists, and other social scientists. Each field brings its
    unique perspectives and worries into this debate. In this section, we take these
    concerns as a guide to offer an overview of this ongoing discussion and how it
    connects with the field of machine ethics. Our main focus is on how these conversations
    affect the Natural Language Processing (NLP) community, and we also explore how
    Large Language Models (LLMs) can advance the frontiers of machine ethics, particularly
    in multilingual contexts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正确与错误的话题一直是哲学家、心理学家以及其他社会科学家讨论的主题。每个领域都将其独特的视角和关切带入这一讨论中。在本节中，我们将这些关切作为指导，提供对这一持续讨论的概述，并探讨其与机器伦理领域的联系。我们的主要关注点是这些讨论如何影响自然语言处理（NLP）社区，同时我们也探讨了大型语言模型（LLMs）如何在多语言背景下推动机器伦理的前沿。
- en: 2.1.   Ethics and Moral Philosophy
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.   伦理学与道德哲学
- en: Ethics is the branch of philosophy concerned with determining what is morally
    good or bad and what is considered right or wrong. It encompasses systems or theories
    of moral values and principles Kant ([1977](#bib.bib28), [1996](#bib.bib29)).
    Within the realm of ethics, normative ethics plays a central role by seeking to
    establish standards of conduct for human actions, institutions, and ways of life.
    Normative ethics branches into deontology, which evaluates the inherent rightness
    or wrongness of actions based on moral rules or duties  Alexander and Moore ([2021](#bib.bib2));
    virtue ethics, which focuses on an individual’s character and virtues rather than
    specific rules or consequences  Hursthouse and Pettigrove ([2022](#bib.bib23));
    and consequentialism, which emphasizes the goodness or value of the outcomes or
    goals of actions Sinnott-Armstrong ([2022](#bib.bib40)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理学是哲学的一个分支，关注确定什么是道德上的善或恶，以及什么被认为是对或错。它涵盖了道德价值和原则的系统或理论  康德 ([1977](#bib.bib28),
    [1996](#bib.bib29))。在伦理学领域中，规范伦理学起着核心作用，它通过寻求建立人类行为、机构和生活方式的行为标准来发挥作用。规范伦理学分为三类：义务论，它根据道德规则或义务评估行为的固有正当性或错误性
     亚历山大和摩尔 ([2021](#bib.bib2))；美德伦理学，它关注个人的品格和美德，而不是具体的规则或后果  赫斯特豪斯和佩蒂格罗夫 ([2022](#bib.bib23))；以及结果主义，它强调行为结果或目标的善或价值
     辛诺特-阿姆斯特朗 ([2022](#bib.bib40))。
- en: Ethical dilemmas are situations characterized by conflicts between two or more
    moral values or principles, posing challenges for moral judgment and decision-making
     Slote ([1985](#bib.bib41)). The question of whether moral dilemmas can coexist
    with a consistent system of moral values is a subject of debate. Philosopher Williams
    argues that ethical consistency doesn’t eliminate the possibility of moral dilemmas,
    as some actions that ought to be done may be incompatible Williams ([1988](#bib.bib47)).
    Resolving these dilemmas often requires making new value judgments within the
    existing ethical framework.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理困境是指在两个或多个道德价值观或原则之间存在冲突的情况，这对道德判断和决策提出了挑战。斯洛特 ([1985](#bib.bib41))。关于道德困境是否可以与一致的道德价值体系共存的问题存在争议。哲学家威廉姆斯认为，道德一致性并不会消除道德困境的可能性，因为某些应该做的行为可能是不兼容的。威廉姆斯
    ([1988](#bib.bib47))。解决这些困境通常需要在现有伦理框架内做出新的价值判断。
- en: Value pluralism is a key component of ethical dilemmas, suggesting that there
    are multiple values that can be equally correct and yet in conflict with each
    other James ([1891](#bib.bib25)). Different individuals or cultures may prioritize
    these values differently, resulting in varying resolutions of ethical dilemmas,
    each of which is ethically sound and consistent. Within the realm of pluralism,
    there exist various sub-schools of thought, including Rossian Pluralism Ross and
    Stratton-Lake ([2002](#bib.bib37)) and Particularism Hare ([1965](#bib.bib18)),
    each offering distinct viewpoints. Rossian pluralists advocate for the evaluation
    of moral principles based on their merits and demerits. Conversely, particularists
    contend that the assessment of moral pros and cons should be context-dependent.
    Nevertheless, both schools of thought share a fundamental conviction that there
    is no one-size-fits-all principle capable of resolving all moral conflicts. They
    also reject the notion of a rigid hierarchy of moral principles that could facilitate
    such resolutions. This perspective implies that there is no universally applicable
    set of moral values or principles that can address all situations and apply uniformly
    to all individuals.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 价值多元主义是伦理困境的一个关键组成部分，它表明存在多种价值观可能都是正确的，但彼此之间却可能存在冲突。詹姆斯 ([1891](#bib.bib25))。不同的个人或文化可能会对这些价值观给予不同的优先级，从而导致伦理困境的不同解决方案，每种解决方案都在伦理上是合理且一致的。在多元主义领域内，存在各种子学派，包括罗斯式多元主义罗斯和斯特拉顿-莱克
    ([2002](#bib.bib37)) 以及特殊主义哈雷 ([1965](#bib.bib18))，每种学派提供了不同的观点。罗斯式多元主义者主张根据道德原则的优缺点进行评估。相反，特殊主义者认为道德利弊的评估应依赖于具体背景。然而，这两种学派都分享一个基本信念，即没有一种通用原则能够解决所有道德冲突。它们也拒绝存在一个严格的道德原则层级结构来促成这种解决。这种观点意味着没有一种普遍适用的道德价值观或原则能够应对所有情况并适用于所有个人。
- en: Inglehart and Welzel ([2010](#bib.bib24)) introduced a framework for mapping
    global cultures which employs a two-dimensional axis system, where the x-axis
    represents a spectrum that stretches from survival ethics on the left to self-expression
    on the right and y-axis covers a range from tradition-based or ethnocentric moral
    views at the bottom to democratic and rational principles at the top. This visual
    representation illustrates the tendency of societies to move diagonally from the
    lower-left corner to the upper-right corner as they progress through industrialization
    and development.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 英格尔哈特和韦尔泽 ([2010](#bib.bib24)) 提出了一个全球文化映射框架，该框架采用了一个二维轴系统，其中 x 轴代表从左侧的生存伦理到右侧的自我表达的光谱，而
    y 轴覆盖从底部的基于传统或民族中心的道德观到顶部的民主和理性原则的范围。这种视觉表示法展示了社会在工业化和发展过程中，往往会从左下角向右上角对角线移动的趋势。
- en: 2.2.   Foreign Language Effect on Morality
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 外语对道德的影响
- en: Recent studies such as Costa et al. ([2014b](#bib.bib14)); Hayakawa et al. ([2017](#bib.bib19));
    Corey et al. ([2017](#bib.bib12)) have found a fascinating link between moral
    judgment and what’s known as the "Foreign-Language Effect." This effect shows
    that people tend to make more practical choices when they face moral dilemmas
    in a foreign language (L2) compared to their native language (L1). This shift
    seems to be because using a foreign language makes people less emotionally connected
    to the situation, which, in turn, reduces the influence of emotions on their moral
    decisions. Čavar and Tytus ([2018](#bib.bib8)) also highlights that being better
    at and more comfortable with the foreign language (L2) can decrease this tendency
    to make practical decisions. This means that the language you use can significantly
    affect how you make moral choices, impacting many people. Furthermore, bilingual
    individuals’ moral decision-making process is quite complex, involving factors
    like the type of dilemma, emotional excitement, and the language they’re using
    Chan et al. ([2016](#bib.bib9)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究，如 Costa 等人（[2014b](#bib.bib14)）；Hayakawa 等人（[2017](#bib.bib19)）；Corey
    等人（[2017](#bib.bib12)）发现了道德判断与所谓的“外语效应”之间的有趣联系。这一效应表明，当人们在外语（L2）中面临道德困境时，比起使用母语（L1），他们往往会做出更实用的选择。这种变化似乎是因为使用外语使人们对情境的情感联系减少，从而降低了情感对道德决策的影响。Čavar
    和 Tytus（[2018](#bib.bib8)）也指出，掌握外语（L2）的能力和舒适度的提高可能会减少做出实用决策的倾向。这意味着你使用的语言可以显著影响你做出道德选择的方式，进而影响许多人。此外，双语者的道德决策过程相当复杂，涉及困境的类型、情感激动和他们所使用的语言（Chan
    等人，[2016](#bib.bib9)）。
- en: 2.3.   Ethics in LLMs
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.   LLMs 中的伦理
- en: In the field of Ethics in NLP, most approaches assume a deontological perspective,
    with developers setting moral rules, but these may not readily apply to various
    contexts or Large Language Models (LLMs)  Talat et al. ([2022](#bib.bib43)). Awad
    et al. ([2022](#bib.bib3)) introduce the Computational Reflective Equilibrium
    (CRE) framework for AI-based ethics, emphasizing moral intuitions and principles.
    Sambasivan et al. ([2021](#bib.bib38)), Bhatt et al. ([2022](#bib.bib5)) and Ramesh
    et al. ([2023](#bib.bib35)) have raised questions of value-pluralism in AI and
    the need for recontextualizing fairness and AI ethics, particularly in global
    contexts. Diddee et al. ([2022](#bib.bib16)) explore ethical concerns in Language
    Technologies for social good, emphasizing stakeholder interactions and strategies.
    Choudhury and Deshpande ([2021](#bib.bib10)) advocates the Rawlsian principle
    over utilitarianism in multilingual LLMs for linguistic fairness.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）伦理领域，大多数方法假设采用义务论视角，由开发者设定道德规则，但这些规则可能不适用于各种上下文或大型语言模型（LLMs）（Talat
    等，[2022](#bib.bib43)）。Awad 等人（[2022](#bib.bib3)）介绍了用于 AI 道德的计算反射平衡（CRE）框架，强调了道德直觉和原则。Sambasivan
    等人（[2021](#bib.bib38)），Bhatt 等人（[2022](#bib.bib5)）和 Ramesh 等人（[2023](#bib.bib35)）提出了
    AI 中价值多元主义的问题以及在全球背景下重新审视公平性和 AI 道德的必要性。Diddee 等人（[2022](#bib.bib16)）探讨了社会公益语言技术中的伦理问题，强调了利益相关者的互动和策略。Choudhury
    和 Deshpande（[2021](#bib.bib10)）主张在多语言 LLM 中采用罗尔斯原则，而不是功利主义原则，以实现语言公平。
- en: AI alignment seeks to ensure that AI systems conform to human goals and ethical
    standards, as highlighted by Piper ([Oct 15, 2020](#bib.bib34)). Various initiatives
    have put forth ethical frameworks, guidelines, and datasets to train and evaluate
    Language Models (LLMs) in terms of ethical considerations and societal norms Hendrycks
    et al. ([2020](#bib.bib20)); Zhou et al. ([2023](#bib.bib51)); Jiang et al. ([2021](#bib.bib26));
    Rao et al. ([2023](#bib.bib36)); Tanmay et al. ([2023](#bib.bib44)). Moreover,
    Tanmay et al. ([2023](#bib.bib44)) introduces an ethical framework utilizing the
    Defining Issues Test to assess the ethical reasoning abilities of LLMs. However,
    it’s worth noting that these efforts may be susceptible to biases based on the
    backgrounds of those providing annotations, as pointed out by Olteanu et al. ([2019](#bib.bib32)).
    Recent research has placed a growing emphasis on in-context learning and supervised
    tuning to align LLMs with ethical principles, as demonstrated by the studies conducted
    by Hendrycks et al. ([2020](#bib.bib20)); Zhou et al. ([2023](#bib.bib51)); Jiang
    et al. ([2021](#bib.bib26)); Rao et al. ([2023](#bib.bib36)); Sorensen et al.
    ([2023](#bib.bib42)). These methodologies aim to accommodate a range of ethical
    perspectives, recognizing the multifaceted nature of ethics. Rao et al. ([2023](#bib.bib36))
    posits that the generic ethical reasoning abilities can be infused into the LLMs
    so that they can handle value pluralism at a large scale. However, the authors
    have considered the ethical policies only in English.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: AI 对齐旨在确保 AI 系统符合人类目标和伦理标准，如 Piper ([Oct 15, 2020](#bib.bib34)) 所强调的。各种倡议提出了伦理框架、指南和数据集，以在伦理考量和社会规范方面训练和评估语言模型（LLMs），如
    Hendrycks 等人 ([2020](#bib.bib20))；Zhou 等人 ([2023](#bib.bib51))；Jiang 等人 ([2021](#bib.bib26))；Rao
    等人 ([2023](#bib.bib36))；Tanmay 等人 ([2023](#bib.bib44))。此外，Tanmay 等人 ([2023](#bib.bib44))
    引入了一种利用定义问题测试的伦理框架，以评估 LLMs 的伦理推理能力。然而，需要注意的是，这些努力可能会受到提供注释者背景的偏见影响，如 Olteanu
    等人 ([2019](#bib.bib32)) 指出的。最近的研究越来越重视上下文学习和监督调优，以将 LLMs 与伦理原则对齐，正如 Hendrycks
    等人 ([2020](#bib.bib20))；Zhou 等人 ([2023](#bib.bib51))；Jiang 等人 ([2021](#bib.bib26))；Rao
    等人 ([2023](#bib.bib36))；Sorensen 等人 ([2023](#bib.bib42)) 进行的研究所示。这些方法旨在容纳多种伦理视角，认识到伦理的多面性。Rao
    等人 ([2023](#bib.bib36)) 提出，可以将通用伦理推理能力注入到 LLMs 中，以便它们能够大规模处理价值多元化。然而，作者仅考虑了英语中的伦理政策。
- en: In this study, we will be extending the work of Rao et al. ([2023](#bib.bib36))
    with different languages to explore how LLMs behave when the multilingual ethical
    policies are infused into these LLMs . As far as we know, this study is the first
    of it’s kind dealing with the ethics of LLMs in multilingual settings.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们将扩展 Rao 等人 ([2023](#bib.bib36)) 的工作，使用不同语言来探索当将多语言伦理政策注入这些 LLMs 时，它们的表现如何。据我们所知，本研究是首个处理多语言环境下
    LLMs 伦理的类似研究。
- en: 2.4.   Multilingual Performance of LLMs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4. LLMs 的多语言表现
- en: Language Models (LLMs) exhibit remarkable multilingual capabilities in natural
    language processing tasks, although their proficiency varies among languages Zhao
    et al. ([2023](#bib.bib50)). Their primary training data is in English, but they
    also incorporate data from various other languages, contributing to their generalisability
    Brown et al. ([2020](#bib.bib7)); Chowdhery et al. ([2022](#bib.bib11)); Zhang
    et al. ([2022](#bib.bib49)); Zeng et al. ([2022](#bib.bib48)). Nevertheless, significant
    challenges arise when LLMs interact with non-English languages, especially in
    low-resource contexts Bang et al. ([2023](#bib.bib4)); Jiao et al. ([2023](#bib.bib27));
    Hendy et al. ([2023](#bib.bib21)); Zhu et al. ([2023](#bib.bib52)). Several studies
    suggest that enhancing their multilingual performance is possible through in-context
    learning and the strategic design of prompts Huang et al. ([2023](#bib.bib22));
    Nguyen et al. ([2023](#bib.bib31)). Experiments conducted by Ahuja et al. ([2023](#bib.bib1))
    and Wang et al. ([2023](#bib.bib46)) have brought to light an interesting aspect.
    They benchmarked LLMs across a range of Natural Language Processing (NLP) tasks,
    including Machine Translation, Natural Language Inference, Sentiment Analysis,
    Text Summarization, Named Entity Recognition, and Natural Language Generation.
    The results indicate that, while LLMs excel for a few well-resourced languages,
    they generally under-perform for most languages. Furthermore, Kovač et al. ([2023](#bib.bib30))
    have shown that LLMs exhibit context-dependent values and personality traits that
    can vary across different perspectives. This is in contrast to humans, who typically
    maintain more consistent values and traits across various contexts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（LLMs）在自然语言处理任务中表现出显著的多语言能力，尽管它们在不同语言中的能力有所不同Zhao等人（[2023](#bib.bib50)）。它们的主要训练数据是英语，但也融入了各种其他语言的数据，提升了它们的普遍性Brown等人（[2020](#bib.bib7)）；Chowdhery等人（[2022](#bib.bib11)）；Zhang等人（[2022](#bib.bib49)）；Zeng等人（[2022](#bib.bib48)）。然而，当LLMs与非英语语言互动时，尤其是在资源稀缺的环境中，面临重大挑战Bang等人（[2023](#bib.bib4)）；Jiao等人（[2023](#bib.bib27)）；Hendy等人（[2023](#bib.bib21)）；Zhu等人（[2023](#bib.bib52)）。一些研究表明，通过上下文学习和提示的战略设计，可以提高它们的多语言表现Huang等人（[2023](#bib.bib22)）；Nguyen等人（[2023](#bib.bib31)）。Ahuja等人（[2023](#bib.bib1)）和Wang等人（[2023](#bib.bib46)）进行的实验揭示了一个有趣的方面。他们对LLMs在一系列自然语言处理（NLP）任务中的表现进行了基准测试，包括机器翻译、自然语言推断、情感分析、文本摘要、命名实体识别和自然语言生成。结果表明，尽管LLMs在一些资源丰富的语言中表现出色，但对于大多数语言，它们的表现通常较差。此外，Kovač等人（[2023](#bib.bib30)）已经显示，LLMs表现出依赖上下文的价值观和个性特征，这些特征可能在不同视角中有所变化。这与人类通常在各种上下文中保持较为一致的价值观和特征形成对比。
- en: Importantly, the current body of research has primarily concentrated on the
    technical capabilities of multilingual LLMs. There has been a relative lack of
    exploration into their moral reasoning within diverse linguistic and cultural
    contexts. Recognizing the significant impact of LLMs on real-world applications
    and domains, there is a growing need to delve into the ethical dimensions surrounding
    these multilingual language models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，目前的研究主要集中在多语言LLMs的技术能力上。对它们在多样的语言和文化背景下的道德推理探索相对较少。鉴于LLMs对实际应用和领域的重大影响，越来越需要深入研究这些多语言语言模型的伦理维度。
- en: 3.   Methodology
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.   方法论
- en: 3.1.   Language-based Ethical Framework
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.   基于语言的伦理框架
- en: We extend the existing framework for defining ethical policies in the LLM prompt,
    as initially presented by Rao et al. ([2023](#bib.bib36)) and reformulate it to
    be used in multilingual settings. Let us consider an LLM, denoted as $\mathcal{L}$,
    denoted as
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们扩展了现有的框架，以定义LLM提示中的伦理政策，最初由Rao等人（[2023](#bib.bib36)）提出，并重新制定以用于多语言环境。让我们考虑一个LLM，记作$\mathcal{L}$。
- en: $p=P(\pi,x,\lambda)$.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $p=P(\pi,x,\lambda)$。
- en: 'Hence we extend the definition of ethical consistency to following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将伦理一致性的定义扩展为以下内容：
- en: 'Definition Ethical Consistency. The output produced by the model $\mathcal{L}$.
    We represent this as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理一致性定义。由模型$\mathcal{L}$生成的输出。我们表示为：
- en: $x\wedge\pi~{}\vdash_{e}\ y$
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $x\wedge\pi~{}\vdash_{e}\ y$
- en: where, similar to logical entailment, $\vdash_{e}$ represents ethical entailment.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，类似于逻辑蕴含，$\vdash_{e}$代表伦理蕴含。
- en: 'If a policy is ambiguous for the resolution of $x$ is not fully specified,
    the likelihood function:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个政策对$x$的解决方案不完全明确，似然函数为：
- en: $\mathcal{L}(P(\pi,x,\lambda))\rightarrow\phi$.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $\mathcal{L}(P(\pi,x,\lambda))\rightarrow\phi$。
- en: 3.2.   Ethical Policies
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.   伦理政策
- en: As explained in Rao et al. ([2023](#bib.bib36)), ethical policies are distinctly
    characterized as expressions of preference pertaining to either moral values or
    ethical principles. The absence of a universally agreed-upon set of ethical principles
    necessitates the flexibility to define policies based on various ethical formalisms
    or their combinations. In the context of a specific ethical formalism, denoted
    as $F$, there exists a set of fundamental moral principles, represented as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Rao 等人所解释的 ([2023](#bib.bib36))，伦理政策被明确地定义为与道德价值观或伦理原则相关的偏好表达。由于没有普遍认可的伦理原则集，必须灵活定义政策，基于各种伦理形式主义或其组合。在特定伦理形式主义的背景下，记作
    $F$，存在一组基本的道德原则，表示为
- en: $R^{F}={r^{F}_{1},r^{F}_{2},\dots r^{F}_{n_{F}}}$.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $R^{F}={r^{F}_{1},r^{F}_{2},\dots r^{F}_{n_{F}}}$。
- en: In our work, we extend these principles to the unique challenges posed by LLMs
    in multilingual settings.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们将这些原则扩展到多语言环境下 LLMs 所面临的独特挑战。
- en: It is important to underscore the definition of an ’Ethical Policy’ as described
    in this preceding work. An ethical policy, denoted as $\pi$ signifies a non-strict
    partial order relation governing the importance or priority of these ethical principles.
    This level of policy abstraction is herein referred to as a ’Level 2 policy,’
    and it serves as an illustrative example of how virtue ethics may manifest, for
    instance, as ’prioritizing loyalty over objective impartiality.’
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要强调前述工作中描述的“伦理政策”的定义。伦理政策，记作 $\pi$，表示一个非严格的偏序关系，规范这些伦理原则的重要性或优先级。这一层次的政策抽象在此称为“二级政策”，它作为一个示例，说明了美德伦理如何表现，例如“优先考虑忠诚而非客观公正。”
- en: Furthermore, the refinement of policies is also elaborated upon in this preceding
    work, wherein they are classified into ’Level 1’ and ’Level 0 policies.’ Level
    1 policies, such as ’favoring loyalty towards a friend over professional impartiality,’
    provide specificity by designating the variables to which ethical virtues apply.
    Meanwhile, Level 0 policies, like ’prioritizing loyalty towards her friend Aisha
    over objectivity towards scientific norms of publishing,’ delve into even finer
    details by specifying the values to which these virtues are to be applied.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，前述工作还详细阐述了政策的完善，其中将政策分为“一级政策”和“零级政策”。一级政策，例如“对朋友的忠诚优于职业公正”，通过指定伦理美德适用的变量来提供具体性。与此同时，零级政策，如“将对朋友
    Aisha 的忠诚优先于对科学出版规范的客观性”，*深入到更细微的细节*，通过指定这些美德应适用的价值观来探讨。
- en: The systematic approach to ethical policies described here underscores their
    pragmatic applicability, depending on the level of abstraction and specificity
    desired. It is important to note that these policies, as described in the previous
    work, are predominantly conveyed in natural language. Nevertheless, it is conceivable
    that future developments may explore alternative means of policy representation,
    including symbolic, neural, or hybrid models, drawing from various ethical formalisms.
    This nuanced understanding of ethical policies in different languages, as derived
    from the prior research, forms the basis for the discussion in this present paper
    .
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的伦理政策的系统方法强调了它们的实际应用，取决于所需的抽象层次和具体性。重要的是要注意，这些政策如前述工作所描述的，主要是以自然语言传达的。然而，可以想象未来的发展可能会探索包括符号、神经或混合模型在内的替代政策表示方法，汲取各种伦理形式主义。这种对不同语言中伦理政策的细致理解，正是本文讨论的基础。
- en: 4.   Evaluating Ethical Reasoning Across Languages
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.   跨语言评估伦理推理
- en: Here, we describe the design of our experiment to study the change in the ethical
    reasoning abilities of three popular Large Language Models with well known multilingual
    capabilities with prompts in different languages. In the experiment, the models
    were prompted with moral dilemmas (x’s) in language $L$).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们描述了我们实验的设计，旨在研究三种具有广泛多语言能力的流行大型语言模型在不同语言提示下伦理推理能力的变化。在实验中，模型被提示以语言 $L$
    中的道德困境（x’s）。
- en: We evaluate two OpenAI’s models, ChatGPT (GPT-3.5-turbo) Schulman et al. ([2022](#bib.bib39))
    and GPT-4 OpenAI ([2023](#bib.bib33)). ChatGPT is a finetuned version of the GPT-3.5
    model, optimized for dialog using RLHF. We use the September 2023 preview of ChatGPT
    for our experiments. GPT-4 is a larger and more recent model by OpenAI. We also
    evaluate Meta’s publicly available Llama2-70B-Chat-hf model Touvron et al. ([2023](#bib.bib45)).
    It is a 70B parameter model optimized for dialog use cases.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了两个OpenAI模型，ChatGPT（GPT-3.5-turbo）Schulman等人（[2022](#bib.bib39)）和GPT-4 OpenAI（[2023](#bib.bib33)）。ChatGPT是GPT-3.5模型的一个微调版本，使用RLHF优化了对话功能。我们使用了2023年9月的ChatGPT预览版进行实验。GPT-4是OpenAI推出的一个更大且更近期的模型。我们还评估了Meta公开的Llama2-70B-Chat-hf模型Touvron等人（[2023](#bib.bib45)）。它是一个70B参数模型，优化了对话使用场景。
- en: 'We set temperature equal to 0 for all the experiments. Others parameters are
    set as follows: top probability is 0.95 and the presence penalty is 1.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有实验的温度设置为0。其他参数设置如下：top probability为0.95，presence penalty为1。
- en: 4.1.   Dataset
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.   数据集
- en: 'We examine the moral dilemmas and value statements proposed in Rao et al. ([2023](#bib.bib36)),
    each of the four dilemmas contain nine pairs of contrasting policies with at three
    different levels of abstraction. These policies are related to three branches
    of normative ethics: Virtue, Deontology, and Consequentialism. The dilemmas highlights
    the clash between different ethical values. The three dilemmas created by Rao
    et al. ([2023](#bib.bib36)) emphasize conflicts of interpersonal versus professional
    and community versus personal values. Each of these dilemmas consists of nine
    policies, denoted as $\pi=(r^{F}_{i}\geq r^{F}_{j})$. This gives us a total of
    eighteen distinct policies for each dilemma.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了Rao等人（[2023](#bib.bib36)）提出的道德困境和价值陈述，这四个困境中的每一个都包含九对相对立的政策，具有三个不同的抽象层次。这些政策涉及三种规范伦理分支：美德伦理学、义务论和结果论。这些困境突显了不同伦理价值观之间的冲突。Rao等人（[2023](#bib.bib36)）创建的三个困境强调了人际价值观与职业及社区价值观与个人价值观之间的冲突。每个困境包含九项政策，表示为$\pi=(r^{F}_{i}\geq
    r^{F}_{j})$。这给我们每个困境提供了总共十八项不同的政策。
- en: To evaluate the ethical consistency of the Language Models’ outputs, we use
    the ideal resolutions for each dilemma under each policy, as annotated by Rao
    et al. ([2023](#bib.bib36)). None of these ideal resolutions equate to $\phi$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估语言模型输出的伦理一致性，我们使用Rao等人（[2023](#bib.bib36)）注释的每个困境在每个政策下的理想解决方案。这些理想解决方案都不等于$\phi$。
- en: Since all the proposed dilemmas are in english, we translate these dilemmas
    and the corresponding policies to six different languages (Spanish, Chinese, Russian,
    Hindi, Arabic and Swahili) using Google Translation API ³³3[https://translate.google.com/](https://translate.google.com/).
    We back-translated them into English to check the consistency of the meanings
    of the values by manual inspection. The prompt instruction is also translated
    similarly.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有提出的困境都是英文的，我们使用Google Translation API ³³3[https://translate.google.com/](https://translate.google.com/)将这些困境和相应的政策翻译成六种不同的语言（西班牙语、中文、俄语、印地语、阿拉伯语和斯瓦希里语）。我们将它们回译成英语，以通过人工检查确保价值观含义的一致性。提示说明也同样进行了翻译。
- en: 4.2.   Experiments
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.   实验
- en: 'First, we conduct a baseline experiment in which the models are prompted to
    respond to a moral dilemma without any given policy, and they provide their moral
    judgment or resolution from the three options: $y=$ 5) experiments are conducted,
    including four dilemmas and six permutations of options. We note the baseline
    resolution of the model for each of the dilemmas per language in Table [1](#S5.T1
    "Table 1 ‣ 5\. Results and Observations ‣ Ethical Reasoning and Moral Value Alignment
    of LLMs Depend on the Language we Prompt them in").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们进行基线实验，在此实验中，模型被提示在没有给定任何政策的情况下回应道德困境，并提供从三种选项中选择的道德判断或解决方案：$y=$ 5）进行的实验，包括四个困境和六种选项排列。我们在表[1](#S5.T1
    "Table 1 ‣ 5\. Results and Observations ‣ Ethical Reasoning and Moral Value Alignment
    of LLMs Depend on the Language we Prompt them in")中记录了模型在每种语言下对每个困境的基线解决方案。
- en: In the next part, there is a policy statement given along with the dilemma instructing
    the model to resolve the dilemma strictly based on the policy. This results in
    each model being probed a total of 432 times for each language (18 $\times$ 6).
    The prompt structure used here is the same as the one proposed with the dilemmas.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，提供了一个政策声明与困境一起，指示模型严格按照政策解决困境。这导致每种语言下每个模型被探测总共432次（18 $\times$ 6）。这里使用的提示结构与提出困境时的结构相同。
- en: 4.3.   Metrics
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.   评估指标
- en: The following metrics were used to study the models’ behaviors across the dilemmas
    in different languages.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下指标用于研究模型在不同语言中对困境的行为。
- en: Accuracy here is defined as the percentage of number of times the model correctly
    resolves the dilemma given the policy as per the proposed resolution.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的准确性定义为模型在根据建议的解决方案给定的政策下正确解决困境的次数百分比。
- en: 'Bias and Confusion are two key metrics calculated to assess model behavior.
    Bias is defined as the fraction of times the model sticks to its baseline stance,
    even when the provided policy dictates otherwise. Confusion is the fraction of
    times the model deviates from it’s baseline stance when the policy prompts it
    to stick with the same stance. We calculate both of these as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和混淆是用于评估模型行为的两个关键指标。偏差定义为模型即使在提供的政策另有指示时仍坚持其基线立场的次数比例。混淆是模型在政策提示下偏离其基线立场的次数比例。我们按如下方式计算这两个指标：
- en: '|  | $bias\ =\ \frac{\sum\nolimits_{i}(1\ &#124;\ x_{i}\neq A,\ y_{i}=A)}{\sum_{i}(1\
    &#124;\ x_{i}\neq A)}$ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $bias\ =\ \frac{\sum\nolimits_{i}(1\ &#124;\ x_{i}\neq A,\ y_{i}=A)}{\sum_{i}(1\
    &#124;\ x_{i}\neq A)}$ |  |'
- en: '|  | $confusion=\frac{\sum\nolimits_{i}(1\ &#124;\ \ x_{i}=A,\ y_{i}\neq A)}{\sum_{i}(1\
    &#124;\ x_{i}=A)}$ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $confusion=\frac{\sum\nolimits_{i}(1\ &#124;\ \ x_{i}=A,\ y_{i}\neq A)}{\sum_{i}(1\
    &#124;\ x_{i}=A)}$ |  |'
- en: Here, $x_{i}$ represents the model’s baseline stance.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$x_{i}$ 表示模型的基线立场。
- en: A higher bias value illustrates a strong alignment of the model to it’s preferred
    resolution which it still tries to reason for despite an opposing policy. A high
    confusion score illustrates the possibility that the model is perhaps not able
    to understand the dilemma and it’s associated values very well and thus deviates
    from expected resolution for no clear reason.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的偏差值说明模型在面对对立政策时，仍然试图依据其首选解决方案进行推理，从而与其偏好的解决方案高度一致。较高的混淆分数说明模型可能无法很好地理解困境及其相关值，因此无明显原因地偏离预期的解决方案。
- en: 5.   Results and Observations
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.   结果与观察
- en: Table [1](#S5.T1 "Table 1 ‣ 5\. Results and Observations ‣ Ethical Reasoning
    and Moral Value Alignment of LLMs Depend on the Language we Prompt them in") shows
    the baseline performance of all three models across various languages. GPT-4 resolves
    dilemmmas with a remarkably high agreement across all cases, with one notable
    exception observed in the Heinz dilemma when probed in Hindi, resulting in a 50%
    agreement rate. An intriguing observation emerges when analyzing the GPT-4 results
    for the Rajesh dilemma; a distinct pattern of opposite resolutions (highlighted
    in red) is observed for most languages when compared to the English context (highlighted
    in green). Furthermore, ChatGPT exhibits conflicting behaviors between English
    and Hindi for all dilemmas in its resolution process. Llama2-70B-Chat exhibits
    the least consistent behavior among the three models, primarily due to the lower
    degree of agreement among all possible permutations of choices. Notably, Llama2-70B-Chat
    demonstrates a remarkable departure from the behavior of ChatGPT and GPT-4 in
    most dilemmas and languages. Llama2-70B-Chat tends to opt for affirmative resolutions,
    such as "should share" and "should steal the drug," more frequently, showcasing
    a distinct and contrasting behavioral pattern when compared to its counterparts.
    These findings hint at the potential presence of bias within the models towards
    specific dilemmas (and consequently, while resolving certain kinds of value conflicts)
    in specific languages, warranting further investigation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S5.T1 "Table 1 ‣ 5\. Results and Observations ‣ Ethical Reasoning and
    Moral Value Alignment of LLMs Depend on the Language we Prompt them in") 显示了三种模型在各种语言中的基线表现。GPT-4
    在所有情况下对困境的解决达成了显著的一致性，但在用印地语探究 Heinz 困境时，观察到一个例外，达成率为 50%。在分析 GPT-4 结果时，对 Rajesh
    困境的观察显示，与英语环境（高亮为绿色）相比，大多数语言呈现出相反解决方案的独特模式（高亮为红色）。此外，ChatGPT 在解决过程中展现出英语和印地语之间的矛盾行为。Llama2-70B-Chat
    在三种模型中表现出最不一致的行为，这主要是由于所有可能选择排列之间的一致性较低。特别是，Llama2-70B-Chat 在大多数困境和语言中表现出明显不同于
    ChatGPT 和 GPT-4 的行为。Llama2-70B-Chat 更倾向于选择肯定的解决方案，如“应分享”和“应偷药”，与其对手相比，展现出一种明显对比的行为模式。这些发现暗示模型在特定语言中对特定困境（以及解决某些类型的价值冲突）可能存在潜在的偏见，值得进一步调查。
- en: Figure [2](#S6.F2 "Figure 2 ‣ 6\. Discussion and Conclusion ‣ Ethical Reasoning
    and Moral Value Alignment of LLMs Depend on the Language we Prompt them in") provides
    a comprehensive overview of the results obtained from policy-based resolution
    by the models across various languages, comparing them to the ground-truth resolutions.
    GPT-4 consistently demonstrates superior ethical reasoning abilities across most
    languages, with the notable exception of Hindi. In stark contrast, Llama2-70B-Chat
    exhibits the least ethical reasoning capability across the board.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S6.F2 "图 2 ‣ 6\. 讨论与结论 ‣ LLM的伦理推理与道德价值对齐取决于我们以何种语言提示它们")提供了模型在不同语言中基于政策的解决方案的综合概述，并将其与真实解决方案进行比较。GPT-4在大多数语言中
    consistently表现出优越的伦理推理能力，唯独印地语例外。相反，Llama2-70B-Chat在各个领域中表现出最差的伦理推理能力。
- en: Table [2](#S6.T2 "Table 2 ‣ 6\. Discussion and Conclusion ‣ Ethical Reasoning
    and Moral Value Alignment of LLMs Depend on the Language we Prompt them in") lists
    the accuracy of each model across the different levels of abstraction of the policies.
    We can see the trend that models tend to perform slightly better on average on
    lower abstraction levels.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S6.T2 "表 2 ‣ 6\. 讨论与结论 ‣ LLM的伦理推理与道德价值对齐取决于我们以何种语言提示它们")列出了每个模型在不同政策抽象层级中的准确性。我们可以看到，模型在较低抽象层级上的平均表现略好。
- en: When considering the different policies, it becomes evident from Figure [2](#S6.F2
    "Figure 2 ‣ 6\. Discussion and Conclusion ‣ Ethical Reasoning and Moral Value
    Alignment of LLMs Depend on the Language we Prompt them in") that Level 2 policies,
    aligned with the consequentialist framework, are where the models predominantly
    excel, except for ChatGPT for Russian. For Level 1 deontological policies, Llama2-70B-Chat
    and ChatGPT perform well. These models perform well for Level 1 policies in virtue
    ethics except for Russian and Spanish. This highlights the nuanced interaction
    between policy levels, ethical frameworks, and model performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑不同政策时，从图[2](#S6.F2 "图 2 ‣ 6\. 讨论与结论 ‣ LLM的伦理推理与道德价值对齐取决于我们以何种语言提示它们")可以明显看出，符合结果主义框架的第二级政策是模型表现突出的地方，除了ChatGPT在俄语中的表现。对于第一级义务论政策，Llama2-70B-Chat和ChatGPT表现良好。这些模型在美德伦理的第一级政策中表现出色，除了俄语和西班牙语。这突出了政策级别、伦理框架和模型表现之间的微妙互动。
- en: GPT-4 exhibits improved reasoning ability when Level 2 policies are applied
    in Arabic, Russian, and Spanish, as compared to English within the consequentialist
    framework. Deontological policies in Level 2 work better for Russian and Spanish
    than for English. However, for virtue ethics-based policies, GPT-4 shows a distinct
    advantage for English. Conversely, Llama2-70B-Chat demonstrates notably superior
    performance with all ethical policies when expressed in English, as compared to
    all other languages. Overall, a general trend emerges where all models tend to
    perform less effectively in Hindi and Swahili, while achieving their best results
    in English and Russian. These observations provide valuable insights into the
    models’ ethical reasoning abilities in various linguistic and ethical contexts.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用第二级政策时，GPT-4在阿拉伯语、俄语和西班牙语中的推理能力相比于英语得到了提升。第二级义务论政策在俄语和西班牙语中的效果优于英语。然而，对于基于美德伦理的政策，GPT-4在英语中表现出明显的优势。相反，Llama2-70B-Chat在所有伦理政策中用英语表达时表现出明显优于其他语言的表现。总体而言，所有模型在印地语和斯瓦希里语中的表现较差，而在英语和俄语中的表现最好。这些观察为模型在不同语言和伦理背景下的伦理推理能力提供了宝贵的见解。
- en: Figure [1](#S5.F1 "Figure 1 ‣ 5\. Results and Observations ‣ Ethical Reasoning
    and Moral Value Alignment of LLMs Depend on the Language we Prompt them in") provides
    a visual representation of our comparative analysis, illustrating the bias and
    confusion scores of each model across dilemmas and languages. From the figure,
    it becomes evident that GPT-4 exhibits the lowest levels of bias among the models,
    while ChatGPT demonstrates the highest bias levels. Interestingly, all models
    exhibit similar bias scores in both the English and Spanish, as well as in the
    case of Hindi and Chinese.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S5.F1 "图 1 ‣ 5\. 结果与观察 ‣ LLM的伦理推理与道德价值对齐取决于我们以何种语言提示它们")提供了我们比较分析的视觉表示，展示了每个模型在不同困境和语言中的偏见和混淆评分。从图中可以明显看出，GPT-4在模型中表现出最低的偏见水平，而ChatGPT表现出最高的偏见水平。有趣的是，所有模型在英语和西班牙语中的偏见评分相似，印地语和中文的情况也是如此。
- en: On comparing the confusion scores, Llama2-70B-Chat consistently shows the highest
    scores among all models, with GPT-4 displaying the lowest confusion scores. Notably,
    GPT-4’s behavior varies across languages in the Rajesh dilemma, highest confusion
    score being observed for Swahili. In the context of Hindi, all models exhibit
    significantly divergent behavior when compared to their performance in English.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 比较困惑分数时，Llama2-70B-Chat在所有模型中 consistently 显示出最高的分数，而GPT-4显示出最低的困惑分数。特别地，在Rajesh难题中，GPT-4的表现因语言而异，斯瓦希里语的困惑分数最高。在印地语的背景下，所有模型的表现与其在英语中的表现相比有显著的不同。
- en: '|  | English | Arabic | Chinese | Hindi | Russian | Spanish | Swahili |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 英语 | 阿拉伯语 | 中文 | 印地语 | 俄语 | 西班牙语 | 斯瓦希里语 |'
- en: '| ChatGPT |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT |'
- en: '| Heinz | 100% | 100% | 76.6% | 100% | 83.3% | 66.6% | 96.6% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Heinz | 100% | 100% | 76.6% | 100% | 83.3% | 66.6% | 96.6% |'
- en: '| Monica | 100% | 66.6% | 100% | 100% | 100% | 100% | 100% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Monica | 100% | 66.6% | 100% | 100% | 100% | 100% | 100% |'
- en: '| Rajesh | 100% | 66.6% | 100% | 100% | 100% | 66.6% | 66.6% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Rajesh | 100% | 66.6% | 100% | 100% | 100% | 66.6% | 66.6% |'
- en: '| Timmy | 100% | 83.3% | 100% | 50% | 96.6% | 83.3% | 83.3% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Timmy | 100% | 83.3% | 100% | 50% | 96.6% | 83.3% | 83.3% |'
- en: '| GPT-4 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 |'
- en: '| Heinz | 100% | 100% | 100% | 50% | 100% | 100% | 100% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Heinz | 100% | 100% | 100% | 50% | 100% | 100% | 100% |'
- en: '| Monica | 100% | 100% | 100% | 100% | 100% | 100% | 100% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Monica | 100% | 100% | 100% | 100% | 100% | 100% | 100% |'
- en: '| Rajesh | 100% | 100% | 100% | 66.6% | 63.3% | 100% | 56.6% |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Rajesh | 100% | 100% | 100% | 66.6% | 63.3% | 100% | 56.6% |'
- en: '| Timmy | 66.7% | 100% | 86.6% | 100% | 100% | 100% | 100% |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Timmy | 66.7% | 100% | 86.6% | 100% | 100% | 100% | 100% |'
- en: '| Llama2-70B-Chat |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70B-Chat |'
- en: '| Heinz | 100% | 66.7% | 83.3% | 66.6% | 66.6% | 100% | 50% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Heinz | 100% | 66.7% | 83.3% | 66.6% | 66.6% | 100% | 50% |'
- en: '| Monica | 100% | 66.7% | 50% | 83.3% | 66.7% | 100% | 50% |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Monica | 100% | 66.7% | 50% | 83.3% | 66.7% | 100% | 50% |'
- en: '| Rajesh | 83.3% | 66.7% | 66.7% | 66.7% | 100% | 66.7% | 100% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Rajesh | 83.3% | 66.7% | 66.7% | 66.7% | 100% | 66.7% | 100% |'
- en: '| Timmy | 66.7% | 66.7% | 66.7% | 66.7% | 83.3% | 83.3% | 57.1% |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Timmy | 66.7% | 66.7% | 66.7% | 66.7% | 83.3% | 83.3% | 57.1% |'
- en: 'Table 1: Baseline resolutions percentage of the times the majority resolution
    was chosen, Green - $y$ majority and yellow - equal'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：多数决策选择的基线分辨率百分比，绿色 - $y$ 多数，黄色 - 相等
- en: '![Refer to caption](img/7b2cff906523b064b049c39f3fc80d26.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7b2cff906523b064b049c39f3fc80d26.png)'
- en: (a) ChatGPT bias
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ChatGPT 偏差
- en: '![Refer to caption](img/dd6256870a784735a5c8856da2f2f987.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd6256870a784735a5c8856da2f2f987.png)'
- en: (b) ChatGPT confusion
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (b) ChatGPT 困惑
- en: '![Refer to caption](img/f50b9611153db52228ee51d04cc6e310.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f50b9611153db52228ee51d04cc6e310.png)'
- en: (c) GPT-4 bias
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (c) GPT-4 偏差
- en: '![Refer to caption](img/5c8d3832c80d5385f9aa31cd0ce84803.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5c8d3832c80d5385f9aa31cd0ce84803.png)'
- en: (d) GPT-4 confusion
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (d) GPT-4困惑
- en: '![Refer to caption](img/42fe7cdf48949f0416365283fd1465e4.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/42fe7cdf48949f0416365283fd1465e4.png)'
- en: (e) Llama2-70B-Chat bias
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Llama2-70B-Chat 偏差
- en: '![Refer to caption](img/5ca680a6b8f96562775e3b57ffda9328.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ca680a6b8f96562775e3b57ffda9328.png)'
- en: (f) Llama2-70B-Chat confusion
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Llama2-70B-Chat困惑
- en: 'Figure 1: Bias and Confusion scores for the three models for each language-dilemma
    pair'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：每种语言-难题配对的三种模型的偏差和困惑分数
- en: 6.   Discussion and Conclusion
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.   讨论与结论
- en: In this paper, we presented a study on the multilingual ethical reasoning capability
    of three popular LLMs, in the spirit of “ethical policy in prompt" over value
    alignment as originally suggested by  Rao et al. ([2023](#bib.bib36)). Our study
    shows that while for some languages, notably English and Russian, LLMs, especially
    GPT-4, has superior ethical reasoning abilities, for low-resource languages -
    Hindi and Swahili, all models fail to perform well. Thus, along the lines of many
    other studies on multilingual evaluation, our work provides further evidence in
    support of the performance gap across languages for LLMs, and brings out yet another
    dimension – that of ethical reasoning – where the gap is prominently evident.
    Why this gap exists, and how it can be bridged are two important problems we would
    like to consider for future studies.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们呈现了对三种流行LLM的多语言伦理推理能力的研究，体现了“提示中的伦理政策”而非最初由Rao等人建议的价值对齐。我们的研究表明，尽管对于一些语言，特别是英语和俄语，LLM，尤其是GPT-4，具有优越的伦理推理能力，但对于资源稀缺语言
    - 印地语和斯瓦希里语，所有模型的表现均不佳。因此，沿着许多多语言评估的研究方向，我们的工作进一步证实了LLM在语言间存在的性能差距，并突出了另一个维度 -
    伦理推理 - 这一差距表现得尤为明显。我们希望在未来的研究中考虑这个差距存在的原因及如何弥合它。
- en: Languages and values are strongly intertwined, as is language and culture. The
    World Value Survey Inglehart and Welzel ([2010](#bib.bib24)) shows how the values
    vary by countries, and therefore, in the languages spoken there. While our study
    brings out distinct biases of the LLMs across languages, it is not clear whether
    these biases are a reflection of cultural differences across the languages, or
    simply an artifact of poor performance. Similar bias patterns between English
    and Spanish, and Chinese and Hindi across models provide a hint that there might
    be more to this than just performance disparity. This is an interesting question
    that calls for further investigation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 语言和价值观密切相关，就像语言和文化一样。《世界价值观调查》Inglehart 和 Welzel ([2010](#bib.bib24)) 显示了不同国家的价值观如何变化，因此也体现在那里使用的语言中。虽然我们的研究揭示了大型语言模型在不同语言中的明显偏差，但这些偏差是否反映了语言间的文化差异，还是仅仅是性能不佳的结果，还不清楚。英语和西班牙语、中文和印地语之间的相似偏差模式提示，这可能不仅仅是性能差异。这是一个值得进一步调查的有趣问题。
- en: Recent studies in neuroscience and psychology has shown that humans, most of
    the time, arrive at a moral judgment akin to aesthetic judgments shaped by their
    past experiences and cultural biases, rather than by reasoning Haidt ([2001](#bib.bib17)).
    This
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的神经科学和心理学研究表明，人们大多数时候得出的道德判断类似于由其过去经历和文化偏见塑造的审美判断，而不是通过推理来做出的 Haidt ([2001](#bib.bib17))。这
- en: '| Model | Level | Arabic | Chinese | English | Hindi | Russian | Spanish |
    Swahili |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 级别 | 阿拉伯语 | 中文 | 英语 | 印地语 | 俄语 | 西班牙语 | 斯瓦希里语 |'
- en: '| ChatGPT | Level 0 | 66.0 | 54.2 | 60.4 | 55.9 | 68.1 | 50.0 | 50.0 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 级别 0 | 66.0 | 54.2 | 60.4 | 55.9 | 68.1 | 50.0 | 50.0 |'
- en: '| Level 1 | 58.3 | 52.1 | 59.0 | 49.3 | 55.6 | 50.7 | 50.7 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 级别 1 | 58.3 | 52.1 | 59.0 | 49.3 | 55.6 | 50.7 | 50.7 |'
- en: '| Level 2 | 54.2 | 53.5 | 56.9 | 50.2 | 56.3 | 48.6 | 48.6 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 级别 2 | 54.2 | 53.5 | 56.9 | 50.2 | 56.3 | 48.6 | 48.6 |'
- en: '| Average | 59.5 | 53.3 | 58.8 | 51.8 | 60.0 | 49.8 | 49.8 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 59.5 | 53.3 | 58.8 | 51.8 | 60.0 | 49.8 | 49.8 |'
- en: '| GPT-4 | Level 0 | 81.3 | 61.8 | 95.8 | 61.1 | 85.6 | 90.9 | 66.7 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 级别 0 | 81.3 | 61.8 | 95.8 | 61.1 | 85.6 | 90.9 | 66.7 |'
- en: '| Level 1 | 84.0 | 79.9 | 95.8 | 68.8 | 95.5 | 91.7 | 75.0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 级别 1 | 84.0 | 79.9 | 95.8 | 68.8 | 95.5 | 91.7 | 75.0 |'
- en: '| Level 2 | 72.9 | 68.1 | 88.2 | 58.3 | 80.6 | 82.6 | 72.9 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 级别 2 | 72.9 | 68.1 | 88.2 | 58.3 | 80.6 | 82.6 | 72.9 |'
- en: '| Average | 79.4 | 69.9 | 93.3 | 62.7 | 87.2 | 88.4 | 71.5 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 79.4 | 69.9 | 93.3 | 62.7 | 87.2 | 88.4 | 71.5 |'
- en: '| Llama2 | Level 0 | 47.2 | 61.8 | 81.9 | 51.4 | 73.6 | 63.9 | 40.5 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 | 级别 0 | 47.2 | 61.8 | 81.9 | 51.4 | 73.6 | 63.9 | 40.5 |'
- en: '| Level 1 | 48.9 | 60.4 | 79.9 | 50.0 | 73.6 | 68.8 | 42.5 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 级别 1 | 48.9 | 60.4 | 79.9 | 50.0 | 73.6 | 68.8 | 42.5 |'
- en: '| Level 2 | 45.8 | 59.7 | 72.2 | 50.7 | 63.9 | 54.9 | 40.6 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 级别 2 | 45.8 | 59.7 | 72.2 | 50.7 | 63.9 | 54.9 | 40.6 |'
- en: '| Average | 47.3 | 60.6 | 78.0 | 50.7 | 70.4 | 62.5 | 41.2 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 47.3 | 60.6 | 78.0 | 50.7 | 70.4 | 62.5 | 41.2 |'
- en: 'Table 2: Accuracy (%) (wrt ground truth) of resolution for policies averaged
    over types of ethics and abstraction levels.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同伦理类型和抽象水平的政策解决准确率（%）（相对于真实情况）的平均值。
- en: '![Refer to caption](img/251e702e49cf08e19bf78bfb1a81e595.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/251e702e49cf08e19bf78bfb1a81e595.png)'
- en: (a) ChatGPT
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ChatGPT
- en: '![Refer to caption](img/c58f21c684e2584e9639f881c2a31fcc.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c58f21c684e2584e9639f881c2a31fcc.png)'
- en: (b) GPT-4
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GPT-4
- en: '![Refer to caption](img/d8381148f57451fb1eb53a18d5e92a21.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d8381148f57451fb1eb53a18d5e92a21.png)'
- en: (c) Llama2-70B-Chat
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Llama2-70B-Chat
- en: 'Figure 2: Accuracy(%) (wrt ground truth) of resolution for policies of different
    types and levels of abstraction across different languages'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同语言中不同类型和抽象水平政策的解决准确率（%）（相对于真实情况）
- en: is also known to be the reason behind implementation of unfair policies by governments
    and organizations, even if the people involved in making these decisions had the
    right intentions. In this light, use of LLMs for ethical reasoning support across
    cultures, values and languages can be a very promising use-case with significant
    large scale positive impact.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 也被认为是政府和组织实施不公平政策的原因，即使参与这些决策的人有着正确的意图。在这种情况下，使用大型语言模型（LLMs）支持跨文化、价值观和语言的伦理推理可能是一个非常有前景的用例，具有显著的大规模积极影响。
- en: Broader Impact Statement
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响声明
- en: Our framework is subject to some key limitations. Firstly, it relies on the
    latest models, such as ChatGPT, GPT-4, and Llama2-70B-Chat, for ethical reasoning
    and the results cannot be generalized to all current models and primarily supports
    an ’in context’ ethical policy approach. However, we anticipate that forthcoming
    language models will enhance this capability. Another limitation pertains to the
    construction of dilemmas, moral policies, and ideal resolutions which is designed
    by Rao et al. ([2023](#bib.bib36)) who mention that these may include some bias
    due to their ethnically homogenous background, potentially limiting the diversity
    of representation. Our study focuses on a limited set of languages, primarily
    emphasizing linguistic diversity, which may restrict the generalizability of our
    findings to languages not included. Additionally, using Google Translator for
    multilingual dilemma translation introduces the potential for translation errors.
    Despite these constraints, our research provides valuable insights into the cross-cultural
    ethical decision-making of Large Language Models (LLMs) across diverse languages,
    underscoring the importance of addressing these limitations in future investigations
    to enhance the strength and robustness of our findings. An ethical concern stemming
    from our research is the potential misinterpretation that GPT-4’s superior ethical
    reasoning capabilities could imply its readiness for real-life ethical decision-making.
    This assumption can be perilous, as the model’s testing is confined to just seven
    languages, and caution should be exercised against generalizing its performance
    to untested languages. It’s important to emphasize that our current study doesn’t
    offer a robust foundation for employing LLMs in moral judgment processes, and
    further research and considerations are warranted.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架存在一些关键的局限性。首先，它依赖于最新的模型，如ChatGPT、GPT-4和Llama2-70B-Chat进行伦理推理，结果不能推广到所有当前模型，并且主要支持“在上下文中”的伦理政策方法。然而，我们预计即将推出的语言模型将增强这一能力。另一个局限性涉及困境、道德政策和理想解决方案的构建，这些是由拉奥等人设计的（[2023](#bib.bib36)），他们提到这些可能由于其民族背景的同质性而包含一些偏见，从而可能限制了表现的多样性。我们的研究集中于有限的语言集，主要强调语言多样性，这可能限制了我们的发现对未包含语言的推广性。此外，使用Google翻译进行多语言困境翻译可能引入翻译错误。尽管存在这些限制，我们的研究提供了关于大型语言模型（LLMs）在多种语言下跨文化伦理决策的宝贵见解，强调了在未来研究中解决这些限制的重要性，以增强我们发现的力度和稳健性。我们研究的一个伦理问题是，GPT-4的优越伦理推理能力可能被误解为其准备好进行实际伦理决策。这一假设可能是危险的，因为模型的测试仅限于七种语言，应谨慎将其性能推广到未测试的语言。重要的是要强调，我们目前的研究并没有为在道德判断过程中使用LLMs提供坚实的基础，需要进一步的研究和考虑。
- en: Acknowledgements
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'We would like to thank the following people and their corresponding native
    languages for their help in validation of the translations: Abdelrahman Atef Mohamed
    Ali Sadallah (Arabic, MBZUAI), Hongyi Zhang (Chinese, Microsoft Corporation),
    Roman Kazakov (Russian, MBZUAI), Emilio Cueva (Spanish, MBZUAI), Jesus Ortiz Barajas
    (Spanish MBZUAI), Millicent Ochieng (Swahili, Microsoft Corporation)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢以下人员及其对应的母语，他们在翻译验证过程中提供了帮助：阿卜杜勒拉赫曼·阿特夫·穆罕默德·阿里·萨达拉赫（阿拉伯语，MBZUAI），张鸿义（中文，微软公司），罗曼·卡扎科夫（俄语，MBZUAI），埃米利奥·库埃瓦（西班牙语，MBZUAI），耶稣·奥尔蒂斯·巴哈哈斯（西班牙语，MBZUAI），米利森特·奥钦（斯瓦希里语，微软公司）
- en: \c@NAT@ctr
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: \c@NAT@ctr
- en: ''
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ahuja et al. (2023) Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain,
    Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika
    Bali, et al. 2023. Mega: Multilingual evaluation of generative ai. *arXiv preprint
    arXiv:2303.12528*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿胡贾等（2023）卡比尔·阿胡贾，瑞沙夫·哈达，米利森特·奥钦，普拉奇·贾因，哈希塔·迪德，塞缪尔·迈纳，塔努贾·甘努，萨米尔·赛戈尔，马克麦德·艾赫迈德，卡利卡·巴利，等。2023年。《Mega：生成式人工智能的多语言评估》。*arXiv预印本
    arXiv:2303.12528*。
- en: Alexander and Moore (2021) Larry Alexander and Michael Moore. 2021. Deontological
    Ethics. In Edward N. Zalta, editor, *The Stanford Encyclopedia of Philosophy*,
    Winter 2021 edition. Metaphysics Research Lab, Stanford University.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚历山大和摩尔（2021）拉里·亚历山大和迈克尔·摩尔。2021年。《义务论伦理学》。在爱德华·N·扎尔塔（编辑），*斯坦福哲学百科全书*，2021年冬季版。斯坦福大学形而上学研究实验室。
- en: Awad et al. (2022) Edmond Awad, Sydney Levine, Michael Anderson, Susan Leigh
    Anderson, Vincent Conitzer, M.J. Crockett, Jim A.C. Everett, Theodoros Evgeniou,
    Alison Gopnik, Julian C. Jamison, Tae Wan Kim, S. Matthew Liao, Michelle N. Meyer,
    John Mikhail, Kweku Opoku-Agyemang, Jana Schaich Borg, Juliana Schroeder, Walter
    Sinnott-Armstrong, Marija Slavkovik, and Josh B. Tenenbaum. 2022. [Computational
    ethics](https://doi.org/https://doi.org/10.1016/j.tics.2022.02.009). *Trends in
    Cognitive Sciences*, 26(5):388–405.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Awad等（2022）Edmond Awad, Sydney Levine, Michael Anderson, Susan Leigh Anderson,
    Vincent Conitzer, M.J. Crockett, Jim A.C. Everett, Theodoros Evgeniou, Alison
    Gopnik, Julian C. Jamison, Tae Wan Kim, S. Matthew Liao, Michelle N. Meyer, John
    Mikhail, Kweku Opoku-Agyemang, Jana Schaich Borg, Juliana Schroeder, Walter Sinnott-Armstrong,
    Marija Slavkovik, 和 Josh B. Tenenbaum。2022年。[计算伦理](https://doi.org/https://doi.org/10.1016/j.tics.2022.02.009)。*认知科学趋势*，26(5):388–405。
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al.
    2023. A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning,
    hallucination, and interactivity. *arXiv preprint arXiv:2302.04023*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bang等（2023）Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su,
    Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung等人。2023年。对ChatGPT在推理、幻觉和互动性的多任务、多语言、多模态评估。*arXiv预印本
    arXiv:2302.04023*。
- en: 'Bhatt et al. (2022) Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave,
    and Vinodkumar Prabhakaran. 2022. [Re-contextualizing Fairness in NLP: The Case
    of India](https://aclanthology.org/2022.aacl-main.55). In *Proceedings of the
    2nd Conference of the Asia-Pacific Chapter of the Association for Computational
    Linguistics and the 12th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 727–740, Online only. Association for Computational
    Linguistics.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatt等（2022）Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, 和 Vinodkumar
    Prabhakaran。2022年。[在NLP中重新定位公平性：以印度为例](https://aclanthology.org/2022.aacl-main.55)。在*第2届亚太地区计算语言学协会会议暨第12届国际联合自然语言处理会议（第1卷：长篇论文）*，第727–740页，仅在线。计算语言学协会。
- en: 'Blodgett et al. (2021) Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,
    Robert Sim, and Hanna Wallach. 2021. Stereotyping Norwegian salmon: An inventory
    of pitfalls in fairness benchmark datasets. In *Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages
    1004–1015.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blodgett等（2021）Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim,
    和 Hanna Wallach。2021年。刻板印象化挪威鲑鱼：公平性基准数据集中的陷阱清单。在*第59届计算语言学协会年会暨第11届国际联合自然语言处理会议（第1卷：长篇论文）*，第1004–1015页。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell等人。2020年。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: 'Čavar and Tytus (2018) Franziska Čavar and Agnieszka Ewa Tytus. 2018. Moral
    judgement and foreign language effect: when the foreign language becomes the second
    language. *Journal of Multilingual and Multicultural Development*, 39(1):17–28.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Čavar和Tytus（2018）Franziska Čavar 和 Agnieszka Ewa Tytus。2018年。道德判断和外语效应：当外语成为第二语言时。*多语言和多文化发展杂志*，39(1):17–28。
- en: Chan et al. (2016) Yuen-Lai Chan, Xuan Gu, Jacky Chi-Kit Ng, and Chi-Shing Tse.
    2016. Effects of dilemma type, language, and emotion arousal on utilitarian vs
    deontological choice to moral dilemmas in Chinese-English bilinguals. *Asian Journal
    of Social Psychology*, 19(1):55–65.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan等（2016）Yuen-Lai Chan, Xuan Gu, Jacky Chi-Kit Ng, 和 Chi-Shing Tse。2016年。困境类型、语言和情感唤醒对中英双语者道德困境中功利主义与义务论选择的影响。*亚洲社会心理学杂志*，19(1):55–65。
- en: Choudhury and Deshpande (2021) Monojit Choudhury and Amit Deshpande. 2021. How
    Linguistically Fair Are Multilingual Pre-Trained Language Models? In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 35, pages 12710–12718.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choudhury和Deshpande（2021）Monojit Choudhury 和 Amit Deshpande。2021年。多语言预训练语言模型在语言公平性方面有多公平？在*AAAI人工智能会议论文集*，第35卷，第12710–12718页。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等 (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, 等. 2022. Palm: 通过路径扩展语言建模。*arXiv preprint arXiv:2204.02311*。'
- en: 'Corey et al. (2017) Joanna D Corey, Sayuri Hayakawa, Alice Foucart, Melina
    Aparici, Juan Botella, Albert Costa, and Boaz Keysar. 2017. Our moral choices
    are foreign to us. *Journal of experimental psychology: Learning, Memory, and
    Cognition*, 43(7):1109.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Corey 等 (2017) Joanna D Corey, Sayuri Hayakawa, Alice Foucart, Melina Aparici,
    Juan Botella, Albert Costa, 和 Boaz Keysar. 2017. 我们的道德选择对我们来说是外来的。*Journal of
    experimental psychology: Learning, Memory, and Cognition*, 43(7):1109。'
- en: Costa et al. (2014a) Albert Costa, Alice Foucart, Sayuri Hayakawa, Melina Aparici,
    Jose Apesteguia, Joy Heafner, and Boaz Keysar. 2014a. [Your morals depend on language](https://doi.org/10.1371/journal.pone.0094842).
    *PLOS ONE*, 9(4):1–7.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa 等 (2014a) Albert Costa, Alice Foucart, Sayuri Hayakawa, Melina Aparici,
    Jose Apesteguia, Joy Heafner, 和 Boaz Keysar. 2014a. [你的道德观依赖于语言](https://doi.org/10.1371/journal.pone.0094842)。*PLOS
    ONE*, 9(4):1–7。
- en: Costa et al. (2014b) Albert Costa, Alice Foucart, Sayuri Hayakawa, Melina Aparici,
    Jose Apesteguia, Joy Heafner, and Boaz Keysar. 2014b. Your morals depend on language.
    *PloS one*, 9(4):e94842.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa 等 (2014b) Albert Costa, Alice Foucart, Sayuri Hayakawa, Melina Aparici,
    Jose Apesteguia, Joy Heafner, 和 Boaz Keysar. 2014b. 你的道德观依赖于语言。*PloS one*, 9(4):e94842。
- en: Dai and Dimond (1998) Y T Dai and M F Dimond. 1998. [Filial piety. A cross-cultural
    comparison and its implications for the well-being of older parents](https://journals.healio.com/doi/10.3928/0098-9134-19980301-05).
    *Journal of Gerontological Nursing*, 24.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 和 Dimond (1998) Y T Dai 和 M F Dimond. 1998. [孝顺。跨文化比较及其对年长父母幸福的影响](https://journals.healio.com/doi/10.3928/0098-9134-19980301-05)。*Journal
    of Gerontological Nursing*, 24。
- en: Diddee et al. (2022) Harshita Diddee, Kalika Bali, Monojit Choudhury, and Namrata
    Mukhija. 2022. [The Six Conundrums of Building and Deploying Language Technologies
    for Social Good](https://doi.org/10.1145/3530190.3534792). In *ACM SIGCAS/SIGCHI
    Conference on Computing and Sustainable Societies (COMPASS)*, COMPASS ’22, page
    12–19, New York, NY, USA. Association for Computing Machinery.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diddee 等 (2022) Harshita Diddee, Kalika Bali, Monojit Choudhury, 和 Namrata Mukhija.
    2022. [构建和部署社会公益语言技术的六个难题](https://doi.org/10.1145/3530190.3534792)。在 *ACM SIGCAS/SIGCHI
    计算与可持续社会会议 (COMPASS)*, COMPASS ’22, 页 12–19, 纽约, NY, USA. 计算机协会。
- en: 'Haidt (2001) Jonathan Haidt. 2001. The emotional dog and its rational tail:
    A social intuitionist approach to moral judgment. *Psychological review*, 108(4):814.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haidt (2001) Jonathan Haidt. 2001. 情感的狗与其理性的尾巴：一种社会直觉主义的道德判断方法。*Psychological
    review*, 108(4):814。
- en: Hare (1965) R. M. Hare. 1965. [*Freedom and Reason*](https://doi.org/10.1093/019881092X.001.0001).
    Oxford University Press.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hare (1965) R. M. Hare. 1965. [*自由与理性*](https://doi.org/10.1093/019881092X.001.0001)。牛津大学出版社。
- en: Hayakawa et al. (2017) Sayuri Hayakawa, David Tannenbaum, Albert Costa, Joanna D
    Corey, and Boaz Keysar. 2017. Thinking more or feeling less? explaining the foreign-language
    effect on moral judgment. *Psychological science*, 28(10):1387–1397.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hayakawa 等 (2017) Sayuri Hayakawa, David Tannenbaum, Albert Costa, Joanna D
    Corey, 和 Boaz Keysar. 2017. 多思考还是少感觉？解释外语效应对道德判断的影响。*Psychological science*, 28(10):1387–1397。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch,
    Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. Aligning AI with Shared Human
    Values. *arXiv preprint arXiv:2008.02275*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch,
    Jerry Li, Dawn Song, 和 Jacob Steinhardt. 2020. 使 AI 与共享人类价值观一致。*arXiv preprint
    arXiv:2008.02275*。
- en: Hendy et al. (2023) Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak,
    Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan
    Awadalla. 2023. How good are GPT models at machine translation? a comprehensive
    evaluation. *arXiv preprint arXiv:2302.09210*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendy 等 (2023) Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed
    Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, 和 Hany Hassan Awadalla.
    2023. GPT 模型在机器翻译中的表现如何？综合评估。*arXiv preprint arXiv:2302.09210*。
- en: 'Huang et al. (2023) Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao,
    Ting Song, Yan Xia, and Furu Wei. 2023. Not All Languages Are Created Equal in
    LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. *arXiv
    preprint arXiv:2305.07004*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2023) Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting
    Song, Yan Xia, 和 Furu Wei. 2023. 在 LLM 中并非所有语言都是平等的：通过跨语言思维提示提升多语言能力。*arXiv preprint
    arXiv:2305.07004*。
- en: Hursthouse and Pettigrove (2022) Rosalind Hursthouse and Glen Pettigrove. 2022.
    Virtue Ethics. In Edward N. Zalta and Uri Nodelman, editors, *The Stanford Encyclopedia
    of Philosophy*, Winter 2022 edition. Metaphysics Research Lab, Stanford University.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hursthouse 和 Pettigrove（2022）Rosalind Hursthouse 和 Glen Pettigrove。2022年。美德伦理学。在
    Edward N. Zalta 和 Uri Nodelman 编辑的 *斯坦福哲学百科全书*，2022年冬季版。斯坦福大学形而上学研究实验室。
- en: Inglehart and Welzel (2010) Ronald Inglehart and Chris Welzel. 2010. The WVS
    cultural map of the world. *World Values Survey*.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inglehart 和 Welzel（2010）Ronald Inglehart 和 Chris Welzel。2010年。WVS 世界文化地图。*世界价值调查*。
- en: James (1891) William James. 1891. [The Moral Philosopher and the Moral Life](https://doi.org/10.1086/intejethi.1.3.2375309).
    *The International Journal of Ethics*, 1(3):330.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: James（1891）William James。1891年。[道德哲学家与道德生活](https://doi.org/10.1086/intejethi.1.3.2375309)。*国际伦理学杂志*，1(3):330。
- en: Jiang et al. (2021) Liwei Jiang, Jena D Hwang, Chandra Bhagavatula, Ronan Le
    Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt,
    Saadia Gabriel, et al. 2021. Can machines learn morality? The Delphi Experiment.
    *arXiv preprint arXiv:2110.07574*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人（2021）Liwei Jiang、Jena D Hwang、Chandra Bhagavatula、Ronan Le Bras、Jenny
    Liang、Jesse Dodge、Keisuke Sakaguchi、Maxwell Forbes、Jon Borchardt、Saadia Gabriel
    等。2021年。机器能学习道德吗？德尔福实验。*arXiv 预印本 arXiv:2110.07574*。
- en: Jiao et al. (2023) Wenxiang Jiao, Wenxuan Wang, JT Huang, Xing Wang, and ZP Tu.
    2023. Is chatgpt a good translator? yes with gpt-4 as the engine. *arXiv preprint
    arXiv:2301.08745*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao 等人（2023）Wenxiang Jiao、Wenxuan Wang、JT Huang、Xing Wang 和 ZP Tu。2023年。ChatGPT
    是个好翻译器吗？是的，使用 GPT-4 作为引擎。*arXiv 预印本 arXiv:2301.08745*。
- en: 'Kant (1977) Immanuel Kant. 1977. *Kant: Lectures on Ethics*. Hackett Publishing
    Company.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kant（1977）Immanuel Kant。1977年。*康德：伦理学讲座*。Hackett Publishing Company。
- en: Kant (1996) Immanuel Kant. 1996. The metaphysics of morals.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kant（1996）Immanuel Kant。1996年。道德形而上学。
- en: Kovač et al. (2023) Grgur Kovač, Masataka Sawayama, Rémy Portelas, Cédric Colas,
    Peter Ford Dominey, and Pierre-Yves Oudeyer. 2023. Large language models as superpositions
    of cultural perspectives. *arXiv preprint arXiv:2307.07870*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovač 等人（2023）Grgur Kovač、Masataka Sawayama、Rémy Portelas、Cédric Colas、Peter
    Ford Dominey 和 Pierre-Yves Oudeyer。2023年。大语言模型作为文化视角的叠加。*arXiv 预印本 arXiv:2307.07870*。
- en: Nguyen et al. (2023) Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty,
    and Lidong Bing. 2023. Democratizing llms for low-resource languages by leveraging
    their english dominant abilities with linguistically-diverse prompts. *arXiv preprint
    arXiv:2306.11372*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人（2023）Xuan-Phi Nguyen、Sharifah Mahani Aljunied、Shafiq Joty 和 Lidong
    Bing。2023年。通过利用其以英语为主的能力与语言多样化的提示，民主化低资源语言的 llms。*arXiv 预印本 arXiv:2306.11372*。
- en: 'Olteanu et al. (2019) Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and
    Emre Kıcıman. 2019. Social data: Biases, methodological pitfalls, and ethical
    boundaries. *Frontiers in big data*, 2:13.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olteanu 等人（2019）Alexandra Olteanu、Carlos Castillo、Fernando Diaz 和 Emre Kıcıman。2019年。社会数据：偏见、方法论陷阱与伦理界限。*大数据前沿*，2:13。
- en: OpenAI (2023) OpenAI. 2023. [GPT-4 Technical Report](http://arxiv.org/abs/2303.08774).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023年。[GPT-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: Piper (Oct 15, 2020) Kelsey Piper. Oct 15, 2020. [The case for taking AI seriously
    as a threat to humanity](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piper（2020年10月15日）Kelsey Piper。2020年10月15日。[将人工智能作为对人类的威胁严肃对待的理由](https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment)。
- en: 'Ramesh et al. (2023) Krithika Ramesh, Sunayana Sitaram, and Monojit Choudhury.
    2023. [Fairness in language models beyond English: Gaps and challenges](https://aclanthology.org/2023.findings-eacl.157).
    In *Findings of the Association for Computational Linguistics: EACL 2023*, pages
    2106–2119, Dubrovnik, Croatia. Association for Computational Linguistics.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh 等人（2023）Krithika Ramesh、Sunayana Sitaram 和 Monojit Choudhury。2023年。[超越英语的语言模型公平性：差距与挑战](https://aclanthology.org/2023.findings-eacl.157)。发表于
    *计算语言学协会会议：EACL 2023*，第2106–2119页，克罗地亚杜布罗夫尼克。计算语言学协会。
- en: 'Rao et al. (2023) Abhinav Rao, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal,
    and Monojit Choudhury. 2023. [Ethical reasoning over moral alignment: A case and
    framework for in-context ethical policies in llms](http://arxiv.org/abs/2310.07251).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rao 等人（2023）Abhinav Rao、Aditi Khandelwal、Kumar Tanmay、Utkarsh Agarwal 和 Monojit
    Choudhury。2023年。[道德对齐的伦理推理：在上下文中为 llms 制定伦理政策的案例与框架](http://arxiv.org/abs/2310.07251)。
- en: Ross and Stratton-Lake (2002) David Ross and Philip Stratton-Lake. 2002. [*The
    Right and the Good*](https://doi.org/10.1093/0199252653.001.0001). Oxford University
    Press.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross 和 Stratton-Lake（2002）David Ross 和 Philip Stratton-Lake。2002年。[*正确与善良*](https://doi.org/10.1093/0199252653.001.0001)。牛津大学出版社。
- en: Sambasivan et al. (2021) Nithya Sambasivan, Erin Arnesen, Ben Hutchinson, Tulsee
    Doshi, and Vinodkumar Prabhakaran. 2021. Re-imagining algorithmic fairness in
    India and beyond. In *Proceedings of the 2021 ACM conference on fairness, accountability,
    and transparency*, pages 315–328.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sambasivan et al. (2021) Nithya Sambasivan, Erin Arnesen, Ben Hutchinson, Tulsee
    Doshi, 和 Vinodkumar Prabhakaran. 2021. 在印度及其他地区重新构想算法公平性。收录于*2021年ACM公平性、问责制与透明度会议论文集*，页码
    315–328。
- en: 'Schulman et al. (2022) John Schulman, Barret Zoph, Christina Kim, Jacob Hilton,
    Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael
    Pokorny, Rapha Gontijo Lopes, Shengjia Zhao, Arun Vijayvergiya, Eric Sigler, Adam
    Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings, Rajeev Nayak,
    Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah
    Deutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long
    Ouyang, Leonard Bogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan
    Lowe, Dan Mossing, Joost Huizinga, Roger Jiang, Carroll Wainwright, Diogo Almeida,
    Steph Lin, Marvin Zhang, Kai Xiao, Katarina Slama, Steven Bills, Alex Gray, Jan
    Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg Brockman, and Nick Ryder.
    2022. [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt).
    *OpenAI*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2022) John Schulman, Barret Zoph, Christina Kim, Jacob Hilton,
    Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael
    Pokorny, Rapha Gontijo Lopes, Shengjia Zhao, Arun Vijayvergiya, Eric Sigler, Adam
    Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings, Rajeev Nayak,
    Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah
    Deutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long
    Ouyang, Leonard Bogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan
    Lowe, Dan Mossing, Joost Huizinga, Roger Jiang, Carroll Wainwright, Diogo Almeida,
    Steph Lin, Marvin Zhang, Kai Xiao, Katarina Slama, Steven Bills, Alex Gray, Jan
    Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg Brockman, 和 Nick Ryder.
    2022. [ChatGPT：优化对话的语言模型](https://openai.com/blog/chatgpt)。*OpenAI*。
- en: Sinnott-Armstrong (2022) Walter Sinnott-Armstrong. 2022. Consequentialism. In
    Edward N. Zalta and Uri Nodelman, editors, *The Stanford Encyclopedia of Philosophy*,
    Winter 2022 edition. Metaphysics Research Lab, Stanford University.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinnott-Armstrong (2022) Walter Sinnott-Armstrong. 2022. **结果主义**。收录于 Edward
    N. Zalta 和 Uri Nodelman 编纂的*斯坦福哲学百科全书*，2022年冬季版。斯坦福大学形而上学研究实验室。
- en: Slote (1985) Michael Slote. 1985. [Utilitarianism, Moral Dilemmas, and Moral
    Cost](http://www.jstor.org/stable/20014092). *American Philosophical Quarterly*,
    22(2):161–168.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slote (1985) Michael Slote. 1985. [功利主义、道德困境和道德成本](http://www.jstor.org/stable/20014092)。*美国哲学季刊*，22(2)：161–168。
- en: 'Sorensen et al. (2023) Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine,
    Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula,
    et al. 2023. Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights,
    and Duties. *arXiv preprint arXiv:2309.00779*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorensen et al. (2023) Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine,
    Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula,
    等。2023. 价值万花筒：与多元化的人类价值观、权利和责任互动的人工智能。*arXiv 预印本 arXiv:2309.00779*。
- en: 'Talat et al. (2022) Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh,
    Ryan Cotterell, and Adina Williams. 2022. [On the Machine Learning of Ethical
    Judgments from Natural Language](https://doi.org/10.18653/v1/2022.naacl-main.56).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 769–779, Seattle,
    United States. Association for Computational Linguistics.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talat et al. (2022) Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh,
    Ryan Cotterell, 和 Adina Williams. 2022. [从自然语言中学习伦理判断的机器学习](https://doi.org/10.18653/v1/2022.naacl-main.56)。收录于*2022年北美计算语言学协会年会：人类语言技术会议论文集*，页码
    769–779，华盛顿州西雅图。计算语言学协会。
- en: Tanmay et al. (2023) Kumar Tanmay, Aditi Khandelwal, Utkarsh Agarwal, and Monojit
    Choudhury. 2023. Exploring Large Language Models’ Cognitive Moral Development
    through Defining Issues Test. *arXiv preprint arXiv:2309.13356*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanmay et al. (2023) Kumar Tanmay, Aditi Khandelwal, Utkarsh Agarwal, 和 Monojit
    Choudhury. 2023. 通过定义问题测试探索大型语言模型的认知道德发展。*arXiv 预印本 arXiv:2309.13356*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等。2023. Llama 2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: 'Wang et al. (2023) Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding,
    Ai Ti Aw, and Nancy F Chen. 2023. SeaEval for Multilingual Foundation Models:
    From Cross-Lingual Alignment to Cultural Reasoning. *arXiv preprint arXiv:2309.04766*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023）Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, Ai
    Ti Aw, 和 Nancy F Chen。2023。《面向多语言基础模型的 SeaEval：从跨语言对齐到文化推理》。*arXiv 预印本 arXiv:2309.04766*。
- en: Williams (1988) Bernard Williams. 1988. Ethical consistency. *Essays on moral
    realism*, pages 41–58.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams（1988）Bernard Williams。1988。《伦理一致性》。*《道德现实主义论文集》*，第41–58页。
- en: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
    An open bilingual pre-trained model. *arXiv preprint arXiv:2210.02414*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等人（2022）Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming
    Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia 等人。2022。《Glm-130b：一个开放的双语预训练模型》。*arXiv
    预印本 arXiv:2210.02414*。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2022）Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等人。2022。《Opt：开放的预训练变换器语言模型》。*arXiv
    预印本 arXiv:2205.01068*。
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2023）Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang,
    Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong 等人。2023。《大语言模型调查》。*arXiv
    预印本 arXiv:2303.18223*。
- en: Zhou et al. (2023) Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu,
    Irwin King, and Helen Meng. 2023. Rethinking Machine Ethics–Can LLMs Perform Moral
    Reasoning through the Lens of Moral Theories? *arXiv preprint arXiv:2308.15399*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2023）Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu, Irwin
    King, 和 Helen Meng。2023。《重新思考机器伦理——大语言模型能否通过伦理理论的视角进行道德推理？》。*arXiv 预印本 arXiv:2308.15399*。
- en: 'Zhu et al. (2023) Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng
    Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation
    with large language models: Empirical results and analysis. *arXiv preprint arXiv:2304.04675*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2023）Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong,
    Jiajun Chen, Lei Li, 和 Shujian Huang。2023。《使用大语言模型的多语言机器翻译：实证结果与分析》。*arXiv 预印本
    arXiv:2304.04675*。
