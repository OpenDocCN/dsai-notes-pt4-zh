- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:51'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
    Models for Bangla Sentiment Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.10783](https://ar5iv.labs.arxiv.org/html/2308.10783)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Md. Arid Hasan ¹, Shudipta Das ², Afiyat Anjum ², Firoj Alam ³,
  prefs: []
  type: TYPE_NORMAL
- en: Anika Anjum ², Avijit Sarker ², Sheak Rashed Haider Noori ²
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rapid expansion of the digital world has propelled sentiment analysis into
    a critical tool across diverse sectors such as marketing, politics, customer service,
    and healthcare. While there have been significant advancements in sentiment analysis
    for widely spoken languages, low-resource languages, such as Bangla, remain largely
    under-researched due to resource constraints. Furthermore, the recent unprecedented
    performance of Large Language Models (LLMs) in various applications highlights
    the need to evaluate them in the context of low-resource languages. In this study,
    we present a sizeable manually annotated dataset encompassing 33,605 Bangla news
    tweets and Facebook comments. We also investigate zero- and few-shot in-context
    learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering
    a comparative analysis against fine-tuned models. Our findings suggest that monolingual
    transformer-based models consistently outperform other models, even in zero and
    few-shot scenarios. To foster continued exploration, we intend to make this dataset
    and our research tools publicly available to the broader research community. In
    the spirit of further research, we plan to make this dataset and our experimental
    resources publicly accessible to the wider research community.¹¹1https://anonymous.com/
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sentiment analysis is an influential sub-area of NLP that deals with sentiment,
    emotions, affect and stylistic analysis in language. There has been significant
    research effort for sentiment analysis due to its need in various fields, such
    as business, finance, politics, education, and services (Cui et al. [2023](#bib.bib20)).
    The analysis typically has been done different types of content – domains (news,
    blog post, customer reviews, social media posts), modalities (textual and multimodal)
    (Hussein [2018](#bib.bib26); Dashtipour et al. [2016](#bib.bib21)). The surge
    in user-generated content on social media platforms has become a significant phenomenon,
    as individuals increasingly voice their opinions on a wide array of topics through
    comments and tweets. As a result, these platforms have garnered considerable research
    attention as valuable sources of data for sentiment analysis (Yue et al. [2019](#bib.bib50)).
    Leveraging such data resources (Dashtipour et al. [2016](#bib.bib21)), substantial
    progress has been achieved for the sentiment analysis in English. The advancements
    range from quantifying sentiment polarity to tackling more complex challenges
    like identifying aspects (Chen et al. [2022](#bib.bib15)), multimodal sentiment
    detection (Liang et al. [2022](#bib.bib33)), explainability (Cambria et al. [2022](#bib.bib13)),
    and multilingual sentiment analysis (Barbieri, Espinosa Anke, and Camacho-Collados
    [2022](#bib.bib8); Galeshchuk, Qiu, and Jourdan [2019](#bib.bib23)).
  prefs: []
  type: TYPE_NORMAL
- en: There has been a growing research interest over time in sentiment analysis for
    low-resource languages (Batanović, Nikolić, and Milosavljević [2016](#bib.bib9);
    Nabil, Aly, and Atiya [2015](#bib.bib37); Muhammad et al. [2023](#bib.bib35)).
    Similar to other low resources languages, the research for the sentiment analysis
    for Bangla has been limited (Islam et al. [2021](#bib.bib27), [2023](#bib.bib28)).
    A study conducted by Alam et al. ([2021a](#bib.bib2)) emphasized the primary challenges
    associated with Bangla sentiment analysis, specifically issues of duplicate instances
    in the data, inadequate reporting of annotation agreement, and generalization.
    These challenges were also highlighted in (Islam et al. [2021](#bib.bib27)), further
    emphasizing the need to address them for effective sentiment analysis in Bangla.
    To further facilitate sentiment analysis research in Bangla, we have created a
    multi-platform sentiment analysis dataset in this study. The dataset has undergone
    multiple rounds of pre-processing and validation to ensure its suitability for
    both sentiment analysis tasks and qualitative investigations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide a comparative analysis using various pre-trained language models,
    including zero and few-shot settings with Flan-T5, GPT-4 and Bloomz as presented
    in [1](#Sx1.F1 "Figure 1 ‣ Introduction ‣ Zero- and Few-Shot Prompting with LLMs:
    A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis"). The
    performance clearly shows that LLMs outperforms random and majority baselines.
    More details are discussed in the Results section.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/511a7072da4286b757af7ce919ba923e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Performance comparisons with baselines (random and majority), fine-tuned
    models and LLMs (GPT and Bloomz).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development of one of the largest manually annotated datasets for sentiment
    analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigation into zero-shot and few-shot learning using various LLMs. We are
    the first to provide such a comprehensive evaluation for Bangla sentiment analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparative analysis of performance differences between in-context learning
    and fine-tuned models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigation of how different prompting variations affect performance in in-context
    learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Based on our extensive experiments our findings are summarized below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuned models yield better results compared to both zero- and few-shot in-context
    learning setups.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuned models using monolingual text (BanglaBERT) demonstrate superior performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is little to no performance difference between zero- and few-shot learning
    with the GPT-4 model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the majority of zero- and few-shot runs, Bloomz yielded better performance
    than GPT-4.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While Bloomz failed to predict the neutral class, GPT-4 struggled with positive
    class prediction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of the paper is structured as follows: Section Related Work provides
    an overview of relevant literature. The Dataset section delves into the dataset
    used, along with an analysis of its contents. In Section Methodology, we detail
    the models and experiments. The Results and Discussion section presents and discusses
    our findings. Lastly, Section Conclusion wraps up the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the realm of sentiment classification for Bangla, the current state-of-the-art
    research focuses on two key aspects: resource development and tackling model development
    challenges. Earlier work in this area has encompassed rule-based methodologies
    as well as classical machine learning approaches and recently the use of pre-trained
    models has received a wider attention.'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Over time there has been several resources developed including manual and semi-supervised
    labeling approach (Chowdhury and Chowdhury [2014](#bib.bib17); Alam et al. [2021a](#bib.bib2);
    Islam et al. [2021](#bib.bib27), [2023](#bib.bib28); Kabir et al. [2023](#bib.bib30)).
    Chowdhury and Chowdhury ([2014](#bib.bib17)) developed a dataset using semi-supervised
    approaches and trained models SVM and Maximum Entropy. The study of Kabir et al.
    ([2023](#bib.bib30)) proposed an annotated sentiment corpus comprising 158,065
    reviews collected from online bookstores. The annotations were primarily based
    on the rating of the reviews, with the majority (89.6%) being in the positive
    class. The study also evaluated classical and BERT-based models for training and
    performance assessment. The skewness of this dataset makes it particularly challenging.
    SentiGold (Islam et al. [2023](#bib.bib28))²²2Note that this dataset is not publicly
    available. is a well-balanced sentiment dataset containing 70K entries from 30
    domains. It was gathered from various sources, including YouTube, Facebook, newspapers,
    blogs, etc., and labeled into five classes. The reported inter-annotator agreement
    stands at 0.88.
  prefs: []
  type: TYPE_NORMAL
- en: Rahman and Kumar Dey ([2018](#bib.bib41)) labeled 5,700 instances as positive,
    negative, or neutral for two aspect-based sentiment analysis (ABSA) tasks, specifically
    extracting aspect categories and polarity. The authors curated two new datasets
    from cricket and restaurants domains. Islam et al. ([2021](#bib.bib27)) developed
    the SentNoB dataset, comprising 15,000 manually annotated comments collected from
    the comments section of Bangla news articles and videos across 13 diverse domains.
    The experimental findings using this dataset indicate that lexical feature combinations
    outperform neural models.
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Various classical algorithms have been employed in different studies for sentiment
    classification in Bangla. These include Bernoulli Naive Bayes (BNB), Decision
    Tree, Support Vector Machine (SVM), Maximum Entropy (ME), and Multinomial Naive
    Bayes (MNB) (Rahman and Hossen [2019](#bib.bib40); Banik and Rahman [2018](#bib.bib7);
    Chowdhury et al. [2019](#bib.bib16)). Islam et al. ([2016](#bib.bib29)) developed
    a sentiment classification system for textual movie reviews in Bangla. The authors
    utilized two machine learning algorithms, Naive Bayes (NB) and SVM, and provided
    comparative results. Additionally, Islam et al. ([2016](#bib.bib29)) employed
    NB with rules for sentiment detection in Bengali Facebook statuses.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms have been extensively explored in the context of Bangla
    sentiment analysis (Hassan et al. [2016](#bib.bib25); Aziz Sharfuddin, Nafis Tihami,
    and Saiful Islam [2018](#bib.bib5); Tripto and Ali [2018](#bib.bib47); Ashik,
    Shovon, and Haque [2019](#bib.bib4); Karim et al. [2020](#bib.bib31); Sazzed [2021](#bib.bib44);
    Sharmin and Chakma [2021](#bib.bib46)). In the study conducted by Tripto and Ali
    ([2018](#bib.bib47)), the authors utilized Long Short-Term Memory (LSTM) networks
    and Convolutional Neural Networks (CNNs) with an embedding layer to identify both
    sentiment and emotion in YouTube comments. Ashik, Shovon, and Haque ([2019](#bib.bib4))
    conducted a comparative analysis of classical algorithms, such as Support Vector
    Machines (SVM), alongside deep learning algorithms, including LSTM and CNN, for
    sentiment classification of Bangla news comments. Karim et al. ([2020](#bib.bib31))
    integrated word embeddings into a Multichannel Convolutional-LSTM (MConv-LSTM)
    network, enabling the prediction of various types of hate speech, document classification,
    and sentiment analysis in the Bangla language. Another aspect explored in sentiment
    analysis is the utilization of LSTM models due to the prevalence of romanized
    Bangla texts in social media. Hassan et al. ([2016](#bib.bib25)); Aziz Sharfuddin,
    Nafis Tihami, and Saiful Islam ([2018](#bib.bib5)) employed LSTM models to design
    and evaluate their sentiment analysis models, taking into account the unique characteristics
    of romanized Bangla texts.
  prefs: []
  type: TYPE_NORMAL
- en: In the study conducted by Hasan et al. ([2020](#bib.bib24)), a comprehensive
    comparison was performed on various annotated sentiment datasets consisting of
    Bangla content from social media sources. The research investigated the effectiveness
    of both classical algorithms, such as SVM, and deep learning algorithms, including
    CNN and transformer models. Notably, the deep learning algorithm XLMRoBERTa exhibited
    superior performance with an accuracy of 0.671, surpassing the classical algorithm
    SVM, which achieved an accuracy of 0.581.
  prefs: []
  type: TYPE_NORMAL
- en: In a review article by Alam et al. ([2021a](#bib.bib2)), the authors investigated
    nine NLP tasks, including sentiment analysis. They reported that transformer-based
    models, particularly XLM-RoBERTa-large, are more suitable for Bangla text categorization
    problems than other machine learning techniques such as LSTM, BERT, and CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Study:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We developed a largest MUltiplatform BAngla SEntiment (MUBASE) social media
    dataset, consisting of Facebook posts and tweets. Following the recommendations
    outlined in Alam et al. ([2021a](#bib.bib2)), we ensured that the dataset is clean,
    free of duplicates, and possesses high annotation quality with an annotation agreement
    score of 0.84\. We have made the dataset publicly available for the community.
    We conducted experiments that goes beyond traditional approaches and smaller transformer-based
    models. Specifically, we investigated the effectiveness of advanced models such
    as Flan-T5, GPT-4 and Bloomz in both zero- and few-shot settings.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data Collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We collected tweets and comments from both Facebook posts and Twitter. To collect
    tweets, we focused on user accounts associated with the following news media sources:
    BBC Bangla, Prothom Alo, and BD24Live. For the comments from the Facebook posts,
    we selected public pages belonging to several news media outlets. Our selection
    of news media was based on the availability of a substantial number of comments.
    In total, we collected approximately 35,000 posts/comments associated with various
    Bangla news portals. Then we removed all the posts, which contains only emojis
    and URLs as well as duplicate data and filtered tweets while collecting through
    API. We also removed all the Banglish comments from our initial dataset. These
    filtering and duplicate-removal steps resulted in $33,606$ entries. In the rest
    of the paper, we will use the term post to refer to post and comments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#Sx3.T1 "Table 1 ‣ Data Collection ‣ Dataset ‣ Zero- and Few-Shot
    Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment
    Analysis") presents the distribution of the number of posts and comments associated
    with each social media source. Our preliminary study reveals that Twitter users
    post both positive and negative sentiments, while showing fewer neutral expressions.
    On the other hand, Facebook users post more negative sentiments. Overall, the
    distribution of posts with negative sentiment is higher in the dataset. We further
    analyzed the distribution of sentences by the number of words associated with
    each class label, as shown in Figure [2](#Sx3.F2 "Figure 2 ‣ Data Collection ‣
    Dataset ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
    Models for Bangla Sentiment Analysis"). We created different ranges of sentence
    length buckets in order to understand and define the sequence length while training
    the transformer based models. It appears that more than 80% of the posts lie within
    twenty words, which is expected with social media posts, as observed in previous
    studies (Alam et al. [2021b](#bib.bib3)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Facebook | Twitter | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 2,245 | 8,315 | 10,560 |'
  prefs: []
  type: TYPE_TB
- en: '| Neutral | 4,866 | 1,331 | 6,197 |'
  prefs: []
  type: TYPE_TB
- en: '| Negative | 9,078 | 7,771 | 16,849 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 16,189 | 17,417 | 33,606 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Class label distribution across different sources of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb270332b82a20638b5fda123d82b290.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The distribution sentence length (number of words) associated with
    each sentiment label.'
  prefs: []
  type: TYPE_NORMAL
- en: Annotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To perform the annotation, we developed an annotation guideline based on previous
    studies (Mukta et al. [2021](#bib.bib36)). For the Bangla sentiment polarity annotation,
    Mukta et al. ([2021](#bib.bib36)) proposed a classification with five labels:
    strongly negative, weakly negative, neutral, strongly positive, and weakly positive.
    However, due to the difficulty in distinguishing between strong and weak labels,
    we opted for a simplified approach with three labels: negative, neutral, and positive.'
  prefs: []
  type: TYPE_NORMAL
- en: Each post was independently annotated by three annotators, all of whom are native
    speakers of Bangla. The annotators consisted of both male and female undergraduate
    students studying computer science. The final label for each post was determined
    based on the majority agreement among the annotators. However, in cases where
    there was disagreement among the annotators, a consensus meeting was organized
    to resolve any discrepancies and reach a final decision. Note that annotators
    are also the authors of the paper, hence, there has not been any payment for the
    annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Inter-Annotation Agreement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The quality of the annotations was assessed by calculating the inter-annotator
    agreement. As mentioned previously, three annotators independently annotated each
    post, adhering to the provided annotation instructions. We calculated the Fleiss
    Kappa ($\kappa$, indicating a perfect agreement among the annotators.³³3Note that
    values of Kappa of and 0.81–1.0 correspond perfect agreement as reported in (Landis
    and Koch [1977](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: Data Split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For our experiments, we divided the dataset into training, development, and
    test sets, comprising 70%, 10%, and 20% of the data, respectively. To ensure a
    balanced class label distribution across the sets, we employed stratified sampling
    (Sechidis, Tsoumakas, and Vlahavas [2011](#bib.bib45)). The distribution of the
    data split is provided in Table [2](#Sx3.T2 "Table 2 ‣ Data Split ‣ Dataset ‣
    Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models
    for Bangla Sentiment Analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Train | Dev | Test | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | 7,342 | 1,126 | 2,092 | 10,560 |'
  prefs: []
  type: TYPE_TB
- en: '| Neutral | 4,319 | 601 | 1,277 | 6,197 |'
  prefs: []
  type: TYPE_TB
- en: '| Negative | 11,811 | 1,700 | 3,338 | 16,849 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 23,472 | 3,427 | 6,707 | 33,606 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Class label distribution of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data Pre-processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The content shared on social media is mostly noisy and includes emoticons, usernames,
    hashtags, URLs, invisible characters, and symbols. To clean the data, we removed
    the noisy portion (emoticons, usernames, hashtags, URLs, invisible characters,
    etc.) of the data. Then we applied tokenization and removed the stopwords from
    the data. Identifying usernames in Facebook posts is more challenging than in
    tweets. While tweets precede usernames with an ‘@’ symbol, Facebook posts have
    no such distinguishing pattern. To address this, we removed English text from
    Facebook posts since most usernames are in English. However, for usernames in
    Bangla text, removal was challenging due to the absence of a consistent pattern
    or a comprehensive Bangla name dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the performance measure for all different experimental settings we compute
    accuracy, and weighted preision, recall and F[1] score. We choose to use weighted
    version of the metric as it takes into account class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Evaluation Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For all experiments, except for LLMs (as detailed below), we trained the models
    using the training set, fine-tuned the parameters with the development set, and
    assessed the model’s performance on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conducted our experiments using classical models, small and large language
    models. Note that we use the definition of small and large models discussed in
    (Zhao et al. [2023](#bib.bib51)). LLMs is used to refer to the models containing
    tens or hundreds of billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We conducted our experiments using classical models as well as both small and
    large language models. It is worth noting that we follow the definitions of ‘small’
    and ‘large’ models discussed in (Zhao et al. [2023](#bib.bib51)). The term ‘LLMs’
    refers to models encompassing tens or hundreds of billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As baselines, we used both a majority (i.e., the class with the highest frequency)
    and a random approach. These methods have been widely used as baseline techniques
    in numerous studies, for example, (Rosenthal, Farra, and Nakov [2019](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: Classical Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While classical models such as SVM (Platt [1998](#bib.bib39)) and Random Forest (Breiman
    [2001](#bib.bib11)) have been widely used in prior studies and remain in use in
    many low-resource production settings, we also wanted to assess their performance.
    To prepare the data for these models, we transformed the text into a tf-idf representation.
    During our experiments with SVM and RF, we used standard parameter settings.
  prefs: []
  type: TYPE_NORMAL
- en: Small Language Models (SLMs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Large-scale pre-trained transformer models (PLMs) have achieved state-of-the-art
    performance across numerous NLP tasks. In our study, we fine-tuned several of
    these models. These included the monolingual transformer model BanglaBERT (Bhattacharjee
    et al. [2022](#bib.bib10)) and multilingual transformers such as multilingual
    BERT (mBERT) (Devlin et al. [2019](#bib.bib22)), XLM-RoBERTa (XLM-r) (Conneau
    et al. [2020](#bib.bib19)), Bloomz (560m and 1.7B parameters models) (Muennighoff
    et al. [2022](#bib.bib34)). We used the Transformer toolkit (Wolf et al. [2020](#bib.bib48))
    for the experiment. Following the guidelines outlined in (Devlin et al. [2019](#bib.bib22)),
    we fine-tuned each model using the default settings over three epochs. Due to
    instability, we performed ten reruns for each experiment using different random
    seeds, and we picked the model that performed best on the development set. We
    provided the details of the parameters settings in Appendix [A](#A1 "Appendix
    A Details of the experiments ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative
    Study with Fine-tuned Models for Bangla Sentiment Analysis").'
  prefs: []
  type: TYPE_NORMAL
- en: GPT Embedding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For many downstram NLP tasks, embedding extracted from pre-trained models followed
    by fine-tuning a feed-forward network provided a reasonable results and also reasonable
    setup for a low-resource production setting. Hence, we wanted to see the performance
    of this setting. We first extract the embeddings using OpenAI’s text-embedding-ada-002
    model for each data split. We then fine-tune a feed-forward network on the embeddings
    extracted from the training set to train our model. Our feed-forward model utilizes
    the Rectified Linear Unit (ReLU) activation function. We have set the learning
    rate to 0.001 and the hidden layer size to 500\. We validate our model using the
    validation set and finally, we evaluate the model using the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the LLMs, we investigate their performance with in-context zero- and few-shot
    learning setting without any specific training. It involve prompting and post-processing
    of output to extract the expected content. Therefore, for each task, we experimented
    with a number of prompts, guided by the same instruction and format as recommended
    in the OpenAI Chat playground, and PromptSource (Bach et al. [2022](#bib.bib6)).
    We used following models: Flan-T5 (large and XL) (Chung et al. [2022](#bib.bib18)),
    Bloomz (1.7B, 3B, 7.1B, 176B-8bit) (Muennighoff et al. [2022](#bib.bib34)) and
    GPT-4 (OpenAI [2023](#bib.bib38)). To ensure a deterministic predictions, we set
    the temperatures to zero of all these models.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompting Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs produces varied responses depending of the prompt design, which is a complex
    and iterative process that present challenges due to the unknown representation
    of information within and different LLMs. The instructions expressed in our prompts
    include both native (Bangla) and English languages with the content in Bangla.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We employ zero-shot prompting, providing natural language instructions that
    describe the task and specify the expected output. This approach enables the LLMs
    to construct a context that refines the inference space, yielding a more accurate
    output. In Figure [3](#Sx4.F3 "Figure 3 ‣ Zero-shot ‣ Prompting Strategy ‣ Methodology
    ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
    Models for Bangla Sentiment Analysis"), we provide an example of a zero-shot prompt,
    emphasizing the instructions and placeholders for both input and label. Given
    that GPT-4 has the capability to play a role, threefore, we also provide a role
    for it as an “expert annotator” Along with the instruction we provide the labels
    to guide the LLMs. Within the instruction, we we provide information of how the
    LLMs should present their output, aiming to eliminate the need for post-processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our initial set of experiments with Bloomz, we observed that it did not
    respond as effectively to the same instructions as GPT-4\. Therefore, we used
    more straightforward instructions for Bloomz, as illustrated in Figure [4](#Sx4.F4
    "Figure 4 ‣ Zero-shot ‣ Prompting Strategy ‣ Methodology ‣ Zero- and Few-Shot
    Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment
    Analysis"). For the other versions of Bloomz and Flan-T5, we used same promopt
    as Bloomz.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4e64812e597d3b7ce92d117bb68fc11a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Zero-shot prompt example for GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cd04b4e898784e88a2af88a66a0d4e80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Zero-shot prompt example for Bloomz.'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The seminal work by Brown et al. ([2020](#bib.bib12)) demonstrated that few-shot
    learning offers superior performance when compared to the zero-shot learning setup.
    This has also proven by numerous benchmarking studies (e.g., (Ahuja et al. [2023](#bib.bib1))).
    In our study, we conducted few-shot experiments using GPT-4 and Bloomz. For few-shot
    learning, we selected examples from the available training data. We used maximal
    marginal relevance-based (MMR) selection to construct example sets that are deemed
    relevant and diverse (Carbonell and Goldstein [1998](#bib.bib14)). This approach
    has been demonstrated as a successful method for selecting few-shot examples by
    Ye et al. ([2022](#bib.bib49)). The MMR technique calculates the similarity between
    a test example and the training dataset, subsequently selecting $m$ setting will
    be explored in out future study. Note that our experiments of few-shot with Bloomz
    was worse than zero-shot, which might require further investigation. Therefore,
    in this study, we do not further discuss the Bloomz experiments with few-shot.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [5](#Sx4.F5 "Figure 5 ‣ Few-shot ‣ Prompting Strategy ‣ Methodology
    ‣ Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned
    Models for Bangla Sentiment Analysis"), we present an example of a few-shot prompt
    for GPT-4\. The few-shot prompt distinguishes itself from the zero-shot in several
    ways: (i) We provided additional information for the role, (ii) We simplified
    the instructions, and (iii) We included $m$-shot examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Our choices of prompts was based on our extensive experiments on similar tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df0a73a48df7021ca672e4ca0279cf00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Few-shot prompt example for GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: Result and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Exp | Acc | P | R | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 33.56 | 38.31 | 33.56 | 33.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Majority | 49.77 | 24.77 | 49.77 | 49.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Classic Models |'
  prefs: []
  type: TYPE_TB
- en: '| SVM | 55.81 | 53.33 | 55.81 | 52.39 |'
  prefs: []
  type: TYPE_TB
- en: '| RF | 56.75 | 54.61 | 56.75 | 52.62 |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding (GPT) | 57.79 | 57.30 | 57.79 | 57.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Bloomz-560m | 61.71 | 63.08 | 61.97 | 63.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Bloomz-1.7B | 61.16 | 59.76 | 61.16 | 59.95 |'
  prefs: []
  type: TYPE_TB
- en: '| BERT-m | 64.95 | 64.92 | 64.95 | 64.90 |'
  prefs: []
  type: TYPE_TB
- en: '| XLM-r (base) | 66.63 | 66.24 | 66.63 | 66.28 |'
  prefs: []
  type: TYPE_TB
- en: '| XLM-r (large) | 66.33 | 65.63 | 66.33 | 65.79 |'
  prefs: []
  type: TYPE_TB
- en: '| BanglaBERT | 69.08 | 67.61 | 69.08 | 67.98 |'
  prefs: []
  type: TYPE_TB
- en: '| BanglaBERT* | 70.33 | 69.13 | 70.33 | 69.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero- and Few-shot on LLMs |'
  prefs: []
  type: TYPE_TB
- en: '| Open Models - 0-shot |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-T5 (large) | 41.28 | 20.23 | 13.77 | 20.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-T5 (xl) | 49.42 | 29.46 | 18.18 | 29.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Bloomz-1.7B | 58.33 | 49.38 | 58.33 | 50.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Bloomz-3B | 59.73 | 50.98 | 59.73 | 51.53 |'
  prefs: []
  type: TYPE_TB
- en: '| Bloomz-7.1B | 62.83 | 50.92 | 62.83 | 56.24 |'
  prefs: []
  type: TYPE_TB
- en: '| Bloomz 176B (8bit) | 61.84 | 51.16 | 61.84 | 55.54 |'
  prefs: []
  type: TYPE_TB
- en: '| Bloomz Majority | 61.97 | 51.32 | 61.97 | 61.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Closed Models - $m$-shot |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4: 0-Shot | 60.21 | 61.65 | 60.21 | 59.99 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4: 0-Shot (BN inst.) | 60.70 | 61.71 | 60.70 | 59.96 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4: 3-Shot | 59.14 | 64.80 | 59.14 | 59.68 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4: 5-Shot | 59.25 | 63.94 | 59.25 | 59.67 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 Majority | 59.74 | 63.26 | 59.74 | 59.74 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Performance of different sets of experiments. * indicates trained
    on combined MUBASE, SentiNoB(Islam et al. [2021](#bib.bib27)), and Alam et al.
    ([2021a](#bib.bib2)). BN Ins. refers that instruction is provided in native Bangla
    language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [3](#Sx5.T3 "Table 3 ‣ Result and Discussion ‣ Zero- and Few-Shot
    Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment
    Analysis"), we reported the results of our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison with Baselines:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All experimental setup outperformed random and majority baselines except Flan-T5.
  prefs: []
  type: TYPE_NORMAL
- en: Performance of Classic Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The performance of the SVM and Random Forest better than baseline, however,
    worse than other except Flan-T5.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuned models consistently outperform across various settings. Results using
    GPT embeddings are superior to classical models, though not as effective as some
    other approaches. Although multilingual models such as BERT-m, XLM-r, and Bloomz
    show promising direction, however, models trained on monolingual text ultimately
    achieve superior performance.
  prefs: []
  type: TYPE_NORMAL
- en: Given the superior performance of monolingual models across various settings,
    we chose to augment our training data. By integrating the SentiNoB training set
    with the MUBASE training set and fine-tuning with BanglaBERT, we managed to boost
    performance by an additional 1.41% of F1.
  prefs: []
  type: TYPE_NORMAL
- en: When comparing the smaller Bloomz model (560m) to the larger one (1.7B), the
    smaller model performs better. This suggests that more training data might be
    required to effectively train such a large model. A similar pattern is observed
    with the XLM-r model when comparing its base and large versions.
  prefs: []
  type: TYPE_NORMAL
- en: Zero- and Few-shot Prompt-Based Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bloomz:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As can be seen in Table [3](#Sx5.T3 "Table 3 ‣ Result and Discussion ‣ Zero-
    and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for
    Bangla Sentiment Analysis"), the performance of zero- and few-shot approaches
    is promising, though there is a significant difference compared to the best monolingual
    fine-tuned transformer-based model. When comparing different parameter sizes of
    Bloomz, we observe that performance increases from 1.7B to 7.1B. However, we see
    a lower performance with Bloomz 176B compared to 7.1B, which might be due to the
    8-bit precision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensemble:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We hypothesized that predictions from different models might vary, and an ensemble
    of their outputs might provide better results. Therefore, we opted to use a majority-based
    ensemble method, resulting in a 5.73% improvement in weighted F1.
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT4:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The performance of GPT-4 is higher than that of other LLMs. Our experiments
    with different types of prompting did not yield a clear improvement, as can be
    seen in Table [3](#Sx5.T3 "Table 3 ‣ Result and Discussion ‣ Zero- and Few-Shot
    Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment
    Analysis"). While prior studies on other tasks and languages showed a clear performance
    gain with a few-shot setup, in our study, we did not find such a gain, only slight
    differences in precision. Therefore, our future studies will include further investigation
    of few-shot learning setups.'
  prefs: []
  type: TYPE_NORMAL
- en: Our experiments revealed that native language instructions achieved performance
    comparable to that of English instructions. This indicates the potential for using
    native language prompts for Bangla sentiment analysys.
  prefs: []
  type: TYPE_NORMAL
- en: While the ensemble of different Bloomz settings improved performance, it did
    not help for GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Error Analysis on the Output of Prompts:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Further analysis the results of the LLMs outputs we observed that (i) Flan-T5
    (xl) labeled only 5 posts as negative, and Flan-T5 (large) labeled only 45 posts
    as negative, (ii) Bloomz completely failed to label posts as neutral, and (i)
    GPT-4 struggled to predict positive class.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we present our evaluation of LLMs using zero and few-shot prompting.
    We offer a detailed comparison with fine-tuned models. Our experiments were conducted
    on a newly developed dataset named ”MUBASE”, for which we provide an in-depth
    analysis. Our results indicate that while LLMs represent a promising research
    direction, the smaller versions of fine-tuned pre-trained models outperform them.
    The performance of LLMs suggests that sentiment analysis in a new domain is feasible
    with reasonable accuracy without the need to develop a new dataset or train a
    new model. Future research directions include using other recently released datasets
    and providing a comparative analysis with LLMs. Additionally, further study on
    few-shot learning represents another promising avenue.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ahuja et al. (2023) Ahuja, K.; Hada, R.; Ochieng, M.; Jain, P.; Diddee, H.;
    Maina, S.; Ganu, T.; Segal, S.; Axmed, M.; Bali, K.; et al. 2023. MEGA: Multilingual
    Evaluation of Generative AI. *arXiv preprint arXiv:2303.12528*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alam et al. (2021a) Alam, F.; Hasan, M. A.; Alam, T.; Khan, A.; Tajrin, J.;
    Khan, N.; and Chowdhury, S. A. 2021a. A review of bangla natural language processing
    tasks and the utility of transformer models. *arXiv preprint arXiv:2107.03844*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alam et al. (2021b) Alam, F.; Qazi, U.; Imran, M.; and Ofli, F. 2021b. Humaid:
    Human-annotated disaster incidents data from twitter with deep learning benchmarks.
    In *Proceedings of the International AAAI Conference on Web and social media*,
    volume 15, 933–942.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashik, Shovon, and Haque (2019) Ashik, M. A.-U.-Z.; Shovon, S.; and Haque, S.
    2019. Data Set For Sentiment Analysis On Bengali News Comments And Its Baseline
    Evaluation. In *Proc. of ICBSLP*, 1–5\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aziz Sharfuddin, Nafis Tihami, and Saiful Islam (2018) Aziz Sharfuddin, A.;
    Nafis Tihami, M.; and Saiful Islam, M. 2018. A Deep Recurrent Neural Network with
    BiLSTM model for Sentiment Classification. In *2018 International Conference on
    Bangla Speech and Language Processing (ICBSLP)*, 1–4\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bach et al. (2022) Bach, S.; Sanh, V.; Yong, Z. X.; Webson, A.; Raffel, C.;
    Nayak, N. V.; Sharma, A.; Kim, T.; Bari, M. S.; Févry, T.; et al. 2022. PromptSource:
    An Integrated Development Environment and Repository for Natural Language Prompts.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*, 93–104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banik and Rahman (2018) Banik, N.; and Rahman, M. H. H. 2018. Evaluation of
    Naïve Bayes and Support Vector Machines on Bangla Textual Movie Reviews. In *Proc.
    of ICBSLP*, 1–6\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barbieri, Espinosa Anke, and Camacho-Collados (2022) Barbieri, F.; Espinosa Anke,
    L.; and Camacho-Collados, J. 2022. XLM-T: Multilingual Language Models in Twitter
    for Sentiment Analysis and Beyond. In *Proceedings of the Thirteenth Language
    Resources and Evaluation Conference*, 258–266\. Marseille, France: European Language
    Resources Association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batanović, Nikolić, and Milosavljević (2016) Batanović, V.; Nikolić, B.; and
    Milosavljević, M. 2016. Reliable baselines for sentiment analysis in resource-limited
    languages: The serbian movie review dataset. In *Proceedings of the Tenth International
    Conference on Language Resources and Evaluation (LREC’16)*, 2688–2696.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhattacharjee et al. (2022) Bhattacharjee, A.; Hasan, T.; Ahmad, W.; Mubasshir,
    K. S.; Islam, M. S.; Iqbal, A.; Rahman, M. S.; and Shahriyar, R. 2022. BanglaBERT:
    Language Model Pretraining and Benchmarks for Low-Resource Language Understanding
    Evaluation in Bangla. In *Findings of the Association for Computational Linguistics:
    NAACL 2022*, 1318–1327\. Seattle, United States: Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Breiman (2001) Breiman, L. 2001. Random forests. *Machine learning*, 45(1):
    5–32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan,
    J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal,
    S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler,
    D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
    S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever,
    I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. *Advances in Neural
    Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cambria et al. (2022) Cambria, E.; Liu, Q.; Decherchi, S.; Xing, F.; and Kwok,
    K. 2022. SenticNet 7: A commonsense-based neurosymbolic AI framework for explainable
    sentiment analysis. In *Proceedings of the Thirteenth Language Resources and Evaluation
    Conference*, 3829–3839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carbonell and Goldstein (1998) Carbonell, J.; and Goldstein, J. 1998. The use
    of MMR, diversity-based reranking for reordering documents and producing summaries.
    In *Proceedings of the 21st annual international ACM SIGIR conference on Research
    and development in information retrieval*, 335–336.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Chen, C.; Teng, Z.; Wang, Z.; and Zhang, Y. 2022. Discrete
    opinion tree induction for aspect-based sentiment analysis. In *Proceedings of
    the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, 2051–2064.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chowdhury et al. (2019) Chowdhury, R. R.; Hossain, M. S.; Hossain, S.; and Andersson,
    K. 2019. Analyzing sentiment of movie reviews in Bangla by applying machine learning
    techniques. In *Proc. of (ICBSLP)*, 1–6\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chowdhury and Chowdhury (2014) Chowdhury, S.; and Chowdhury, W. 2014. Performing
    sentiment analysis in Bangla microblog posts. In *2014 International Conference
    on Informatics, Electronics Vision (ICIEV)*, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus,
    W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2022. Scaling instruction-finetuned
    language models. *arXiv preprint arXiv:2210.11416*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conneau et al. (2020) Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.;
    Wenzek, G.; Guzmán, F.; Grave, E.; Ott, M.; Zettlemoyer, L.; and Stoyanov, V.
    2020. Unsupervised Cross-lingual Representation Learning at Scale. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    ACL ’20, 8440–8451\. Online: Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cui et al. (2023) Cui, J.; Wang, Z.; Ho, S.-B.; and Cambria, E. 2023. Survey
    on sentiment analysis: evolution of research methods and topics. *Artificial Intelligence
    Review*, 1–42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dashtipour et al. (2016) Dashtipour, K.; Poria, S.; Hussain, A.; Cambria, E.;
    Hawalah, A. Y.; Gelbukh, A.; and Zhou, Q. 2016. Multilingual sentiment analysis:
    state of the art and independent comparison of techniques. *Cognitive computation*,
    8: 757–771.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, NAACL-HLT ’19, 4171–4186\.
    Minneapolis, Minnesota, USA: Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Galeshchuk, Qiu, and Jourdan (2019) Galeshchuk, S.; Qiu, J.; and Jourdan, J.
    2019. Sentiment Analysis for Multilingual Corpora. In *Proceedings of the 7th
    Workshop on Balto-Slavic Natural Language Processing*, 120–125\. Florence, Italy:
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hasan et al. (2020) Hasan, M. A.; Tajrin, J.; Chowdhury, S. A.; and Alam, F.
    2020. Sentiment classification in Bangla textual content: a comparative study.
    In *2020 23rd International Conference on Computer and Information Technology
    (ICCIT)*, 1–6\. IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassan et al. (2016) Hassan, A.; Amin, M. R.; Al Azad, A. K.; and Mohammed,
    N. 2016. Sentiment Analysis on Bangla and Romanized Bangla Text using Deep Recurrent
    Models. In *2016 International Workshop on Computational Intelligence (IWCI)*,
    51–56\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hussein (2018) Hussein, D. M. E.-D. M. 2018. A survey on sentiment analysis
    challenges. *Journal of King Saud University-Engineering Sciences*, 30(4): 330–338.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam et al. (2021) Islam, K. I.; Kar, S.; Islam, M. S.; and Amin, M. R. 2021.
    SentNoB: A dataset for analysing sentiment on noisy Bangla texts. In *Findings
    of the Association for Computational Linguistics: EMNLP 2021*, 3265–3271.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Islam et al. (2023) Islam, M. E.; Chowdhury, L.; Khan, F. A.; Hossain, S.;
    Hossain, S.; Rashid, M. M. O.; Mohammed, N.; and Amin, M. R. 2023. SentiGOLD:
    A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation.
    *arXiv preprint arXiv:2306.06147*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Islam et al. (2016) Islam, M. S.; Islam, M. A.; Hossain, M. A.; and Dey, J. J.
    2016. Supervised approach of sentimentality extraction from Bengali facebook status.
    In *2016 19th International Conference on Computer and Information Technology
    (ICCIT)*, 383–387\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kabir et al. (2023) Kabir, M.; Mahfuz, O. B.; Raiyan, S. R.; Mahmud, H.; and
    Hasan, M. K. 2023. BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis
    from Book Reviews. *arXiv preprint arXiv:2305.06595*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karim et al. (2020) Karim, M. R.; Chakravarthi, B. R.; McCrae, J. P.; and Cochez,
    M. 2020. Classification Benchmarks for Under-resourced Bengali Language based
    on Multichannel Convolutional-LSTM Network. *CoRR*, abs / 2004.07807.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Landis and Koch (1977) Landis, J. R.; and Koch, G. G. 1977. The measurement
    of observer agreement for categorical data. *biometrics*, 159–174.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2022) Liang, Y.; Meng, F.; Xu, J.; Chen, Y.; and Zhou, J. 2022.
    MSCTD: A Multimodal Sentiment Chat Translation Dataset. In *Proceedings of the
    60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, 2601–2613.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muennighoff et al. (2022) Muennighoff, N.; Wang, T.; Sutawika, L.; Roberts,
    A.; Biderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z.-X.; Schoelkopf,
    H.; et al. 2022. Crosslingual generalization through multitask finetuning. *arXiv
    preprint arXiv:2211.01786*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Muhammad et al. (2023) Muhammad, S. H.; Abdulmumin, I.; Ayele, A. A.; Ousidhoum,
    N.; Adelani, D. I.; Yimam, S. M.; Ahmad, I. S.; Beloucif, M.; Mohammad, S.; Ruder,
    S.; et al. 2023. Afrisenti: A twitter sentiment analysis benchmark for african
    languages. *arXiv preprint arXiv:2302.08956*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukta et al. (2021) Mukta, M. S. H.; Islam, M. A.; Khan, F. A.; Hossain, A.;
    Razik, S.; Hossain, S.; and Mahmud, J. 2021. A comprehensive guideline for Bengali
    sentiment annotation. *Transactions on Asian and Low-Resource Language Information
    Processing*, 21(2): 1–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nabil, Aly, and Atiya (2015) Nabil, M.; Aly, M.; and Atiya, A. 2015. Astd:
    Arabic sentiment tweets dataset. In *Proceedings of the 2015 conference on empirical
    methods in natural language processing*, 2515–2519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI, R. 2023. GPT-4 technical report. *arXiv*, 2303–08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platt (1998) Platt, J. 1998. *Fast Training of Support Vector Machines using
    Sequential Minimal Optimization*. MIT Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahman and Hossen (2019) Rahman, A.; and Hossen, M. S. 2019. Sentiment Analysis
    on Movie Review Data Using Machine Learning Approach. In *2019 International Conference
    on Bangla Speech and Language Processing (ICBSLP)*, 1–4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahman and Kumar Dey (2018) Rahman, M. A.; and Kumar Dey, E. 2018. Datasets
    for Aspect-Based Sentiment Analysis in Bangla and Its Baseline Evaluation. *Data*,
    3(2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Reimers, N.; and Gurevych, I. 2019. Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, 3982–3992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosenthal, Farra, and Nakov (2019) Rosenthal, S.; Farra, N.; and Nakov, P.
    2019. SemEval-2017 task 4: Sentiment analysis in Twitter. *arXiv preprint arXiv:1912.00741*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sazzed (2021) Sazzed, S. 2021. Improving sentiment classification in low-resource
    bengali language utilizing cross-lingual self-supervised learning. In *International
    Conference on Applications of Natural Language to Information Systems*, 218–230\.
    Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sechidis, Tsoumakas, and Vlahavas (2011) Sechidis, K.; Tsoumakas, G.; and Vlahavas,
    I. 2011. On the Stratification of Multi-label Data. In Gunopulos, D.; Hofmann,
    T.; Malerba, D.; and Vazirgiannis, M., eds., *Machine Learning and Knowledge Discovery
    in Databases*, ECML-PKDD ’11, 145–158\. Berlin, Heidelberg: Springer Berlin Heidelberg.
    ISBN 978-3-642-23808-6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharmin and Chakma (2021) Sharmin, S.; and Chakma, D. 2021. Attention-based
    convolutional neural network for Bangla sentiment analysis. *Ai & Society*, 36(1):
    381–396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tripto and Ali (2018) Tripto, N. I.; and Ali, M. E. 2018. Detecting multilabel
    sentiment and emotions from Bangla youtube comments. In *2018 International Conference
    on Bangla Speech and Language Processing (ICBSLP)*, 1–6\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;
    Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer,
    S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Le Scao, T.; Gugger,
    S.; Drame, M.; Lhoest, Q.; and Rush, A. 2020. Transformers: State-of-the-Art Natural
    Language Processing. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing: System Demonstrations*, EMNLP ’20, 38–45. Online:
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2022) Ye, X.; Iyer, S.; Celikyilmaz, A.; Stoyanov, V.; Durrett, G.;
    and Pasunuru, R. 2022. Complementary Explanations for Effective In-Context Learning.
    *arXiv preprint arXiv:2211.13892*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yue et al. (2019) Yue, L.; Chen, W.; Li, X.; Zuo, W.; and Yin, M. 2019. A survey
    of sentiment analysis in social media. *Knowledge and Information Systems*, 60:
    617–663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.;
    Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A survey of large language
    models. *arXiv preprint arXiv:2303.18223*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Details of the experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the experiments with transformer models, we adhered to the following hyper-parameters
    during the fine-tuning process. Additionally, we have released all our scripts
    for the reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch size: 8;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning rate (Adam): 2e-5;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of epochs: 10;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Max seq length: 256.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Models and Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BanglaBERT (csebuetnlp/banglabert):L=12, H=768, A=12, total parameters: 110M;
    where L is the number of layers (i.e., Transformer blocks), H is the hidden size,
    and A is the number of self-attention heads; (110M);'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XLM-RoBERTa (xlm-roberta-base): L=24, H=1027, A=16; the total number of parameters
    is 355M.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bloomz (bigscience/bloom-560m): L=24, H=1024, A=16; the total number of parameters
    is 560M.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bloomz (bigscience/bloom-1b7): L=24, H=2048, A=16; the total number of parameters
    is 1.7B.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
