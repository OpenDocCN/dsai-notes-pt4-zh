- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:59'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinforcement Learning in the Era of LLMs: What is Essential? What is needed?
    An RL Perspective on RLHF, Prompting, and Beyond.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06147](https://ar5iv.labs.arxiv.org/html/2310.06147)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hao Sun
  prefs: []
  type: TYPE_NORMAL
- en: Department of Applied Mathematics and Theoretical Physics
  prefs: []
  type: TYPE_NORMAL
- en: University of Cambridge
  prefs: []
  type: TYPE_NORMAL
- en: hs789@cam.ac.uk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent advancements in Large Language Models (LLMs) have garnered wide attention
    and led to successful products such as ChatGPT and GPT-4\. Their proficiency in
    adhering to instructions and delivering harmless, helpful, and honest (3H) responses
    can largely be attributed to the technique of Reinforcement Learning from Human
    Feedback (RLHF). In this paper, we aim to link the research in conventional RL
    to RL techniques used in LLM research. Demystify this technique by discussing
    why, when, and how RL excels. Furthermore, we explore potential future avenues
    that could either benefit from or contribute to RLHF research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Highlighted Takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RLHF is Online Inverse RL with Offline Demonstration Data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RLHF  Behavior
    Cloning (BC) by alleviating the problem of compounding error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The RM step in RLHF generates a proxy of the expensive human feedback, such
    an insight can be generalized to other LLM tasks such as prompting evaluation
    and optimization where feedback is also expensive.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The policy learning in RLHF is more challenging than conventional problems studied
    in IRL due to their high action dimensionality and feedback sparsity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The main superiority of PPO over off-policy value-based methods is its stability
    gained from (almost) on-policy data and conservative policy updates.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1 A Crash Introduction to RL: Online RL, Offline RL, and Inverse RL'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will briefly introduce some basic concepts needed in our
    discussion later. We begin by highlighting the important intuitions behind the
    technique of Reinforcement Learning (RL), followed by a more technical formalism.
    Our goal is to ensure everyone, regardless of their background, can grasp the
    intricacies of RL and its impact on Large Language Models.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Essential Concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In RL, an agent learns through interacting with an environment and receiving
    feedback in the form of rewards. The fundamental objective of RL is to find a
    policy, which is a mapping from states to actions, that maximizes the expected
    cumulative reward over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are several useful concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Environment ![[Uncaptioned image]](img/6213dd5bb95fe172786ed56c8e42ebf1.png)=![[Uncaptioned
    image]](img/eb9127c8b2a0cb066cfe1d5e109882ce.png)+![[Uncaptioned image]](img/608e69ce3b8d442f54ade01a2ab92016.png).)
    When we are talking about an Environment ![[Uncaptioned image]](img/6213dd5bb95fe172786ed56c8e42ebf1.png)
    we are talking about the dynamics model ![[Uncaptioned image]](img/eb9127c8b2a0cb066cfe1d5e109882ce.png)
    and the reward function ![[Uncaptioned image]](img/608e69ce3b8d442f54ade01a2ab92016.png).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Agent ![[Uncaptioned image]](img/1242af32d620ecb6bf08a5b756e2df39.png)) An
    Agent is the subject of a policy that interacts with the environment. In a sequential
    decision-making problem, there can be multiple decision steps, and a smart policy
    will make its decision at every step by considering every piece of information
    it can collect till then. e.g., using recurrent networks to record histories in
    [[1](#bib.bib1), [2](#bib.bib2)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Difficulties ![[Uncaptioned image]](img/bcdd9a5700d14ccebf5fe49961b24f6d.png))
    Why is it hard to learn? 1\. the learning objective is non-differentiable, it
    engages the unknown environment. 2\. the policy needs to trade off between exploring
    random novel behaviors that potentially can be better than the current, yet as
    those are random behaviors, they are usually worse than the current — you may
    imagine how hard it would be for the LLM generation tasks when there are  10k
    tokens (as action) to choose from …
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Learning ![[Uncaptioned image]](img/45bb66abd5f040e921aa68079ee4f293.png))
    The key insight behind the learning step in RL is to increase the probability
    of executing the good actions (which leads to a high cumulative future reward)
    and decrease the probability of executing bad actions (which have a low cumulative
    future reward). An easy-to-follow approach can be performing supervised learning
    on a collected set of good actions. e.g., Using Supervised Learning to mimic successful
    trajectories as an alternative approach to RL [[3](#bib.bib3), [4](#bib.bib4)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.2 Technical Formumation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RL can be formally represented using the Markov Decision Processes (MDPs), where
    decisions are made in discrete time steps, and each decision affects the state
    of the environment in the subsequent step.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 Markov Decision Processes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Formally, we denote the MDP as $\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{R},\rho_{0},\gamma\}$
    is the discount factor that trades off between short-term and long-term returns.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Online RL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86e143223e2a5b32e7837de312c5856b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A pictorial illustration of RL: an agent interacts with the environment
    and learns from trial and error.'
  prefs: []
  type: TYPE_NORMAL
- en: In the Online RL setting, an agent with policy $\pi\in\Pi:\mathcal{S}\mapsto\Delta(\mathcal{A})$.
  prefs: []
  type: TYPE_NORMAL
- en: At each time step $t$. The agent’s objective is to maximize its expected return.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: We can alternatively denote the trajectory generated by a policy $\pi$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{\pi}(\tau)=\rho_{0}\Pi_{t=0}^{T}\pi(a_{t}&#124;s_{t})\mathcal{T}(s_{t+1}&#124;s_{t},a_{t}),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $T$ denotes the length of decision sequences. The learning objective can
    be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\tau\sim p_{\pi}(\tau)}\left[\sum_{t=0}^{T}\gamma^{t}\mathcal{R}(s_{t},a_{t})\right].$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 1.2.3 Offline RL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the Offline RL setting, interactions with the environment are strictly forbidden.
    The learning problem is no longer online learning but learning from a static dataset
    of decision logs $\mathcal{D}_{\mathrm{Off-RL}}=\{(s^{i}_{t},a^{i}_{t},s^{i}_{t+1},r^{i}_{t})\}$.
  prefs: []
  type: TYPE_NORMAL
- en: The most obvious difficulty in the offline RL setting is such a setting prohibits
    exploration — hence it hinders the improvement of policy learning to be improved
    over the demonstration data (though sometimes the learned policy can be better
    than the demonstration).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another fundamental challenge is the distributional shift: although offline
    RL learns from a static dataset, its evaluation is actually based on rolling out
    a policy in an environment — this is different from the ordinary supervised learning
    settings where the training set and test set are sampled from the same distribution.
    In offline RL training, the state distribution is sampled from rolling out the
    behavior policy $\pi_{\beta}$.'
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, assuming the decision dataset is collected from an optimal
    behavior policy $\pi_{\beta}^{*}$ based on such an expert decision dataset can
    be denoted as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\ell(\pi)=\mathbb{E}_{p_{\pi}(\tau)}\left[\sum_{t=0}^{T}\mathbbm{1}(\pi(s_{t})\neq
    a^{*}_{t})\right]$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'Then we have the following theorems:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1.1  (Behavior Clone Error Bound. Ross et al. [[7](#bib.bib7)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If $\pi$ is the best possible bound on the expected error of the learned policy.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 1.2  (Compounding Error.).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An intuitive interpretation of this quadratic relationship between the error
    bound and the generalization error is that those errors aggregate along the trajectory.
    i.e., whenever the learned policy makes a mistake, it tends to make more mistakes
    from then on as that action is not optimal and will lead to other out-of-distribution
    states, which will lead to further mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 1.3  (Behavior Clone).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can always set up a supervised learning objective in offline RL to minimize
    the difference between decision demonstration pairs. i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi=\arg\min_{\pi}\mathbb{E}_{(s^{i}_{t},a^{i}_{t})\sim\mathcal{D}}&#124;&#124;a^{i}_{t}-\pi(s^{i}_{t})&#124;&#124;^{2}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/179e2ab6ef895bcdcf2f83f0f6b98ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: In Offline RL, a behavior policy interacts with the environment and
    generates a decision dataset. Then such a decision dataset is used to learn a
    policy without access to the environment (offline).'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 Imitation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to alleviate the challenge of compounding error we discussed above,
    Imitation Learning (IL) considers the setting where a dynamics model is available
    during learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another Motivation of IL: Reward Design is Hard'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The setup of IL is especially common for problems where reward engineering is
    hard. This is because although the “reward hypothesis” tells us whenever we can
    define a reward function for a task, it can be solved by RL, it does not consider
    whether this task can be efficiently solved. For instance, in playing Go or StarCraft,
    it’s easy to define a reward function that returns $+1$ when losing. However,
    it will not be hard to imagine that such a reward function is extremely sparse
    to provide helpful information during learning. In another example of teaching
    robots to finish complex tasks, imitation can also circumvent the difficulty of
    describing a motion sequence with a reward function [[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: A Method for Reward Engineering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In a previous paper [[9](#bib.bib9)], we show and illustrate why using a $0$.
    A simple reward shifting with a few lines of code added to the RL reward function
    can be used to improve exploration (for Online RL) or enhance conservative exploitation
    (for Offline RL).
  prefs: []
  type: TYPE_NORMAL
- en: 'To alleviate the challenge of reward engineering in RL tasks, IL is introduced
    to learn to use the dynamics model but without a pre-defined reward model. Consider
    those examples: (1) in learning humanoid robotics locomotion skills, it is hard
    to define an objective to let the robot “walk as a human” — however, providing
    demonstration data to show how humans walk is much easier. (2) in autonomous driving,
    it is hard to define the objective of “driving safe and well” — however, we should
    be able to provide human driving videos or control sequences as demonstrations
    of good and safe driving behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: The objective of IL is to learn from a (decision) demonstration dataset, with
    access to a dynamics model — such that the current policy can be rolled out in
    the real environment. Intuitively, with such a dynamics model, the optimization
    objective will no longer be $s_{t}\sim p_{\pi_{\beta}}(\tau)$ — the distributional
    shift problem can be alleviated.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5bcf35a675d8f56b6f01420322d784b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: In Imitation Learning (IL), the agent learns from feedback from the
    decision dataset, but the observations are from a real dynamics model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many practical methods for implementing such a learning process,
    and the most famous work in the Deep-RL era is the GAIL [[10](#bib.bib10)], which
    conducts IL through adversarial learning: the policy is a generator of behaviors,
    while a discriminator then tries to identify whether a trajectory is generated
    by the behavior policy $\pi_{\beta}$ or by the generator (the policy learned).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the theory results, we have the following theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1.4  (DAgger Error Bound, Ross et al. [[7](#bib.bib7)]).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If $\pi$ is the best possible bound on the expected error of the learned policy.
  prefs: []
  type: TYPE_NORMAL
- en: Remark 1.5.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This requires the additional assumption of being able to access the behavior
    (expert) policy $\pi_{\beta}$ .
  prefs: []
  type: TYPE_NORMAL
- en: 'Takeaway: Comparing Theorem [1.1](#S1.Thmtheorem1 "Theorem 1.1 (Behavior Clone
    Error Bound. Ross et al. [7]). ‣ 1.2.3 Offline RL ‣ 1.2 Technical Formumation
    ‣ 1 A Crash Introduction to RL: Online RL, Offline RL, and Inverse RL ‣ Reinforcement
    Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective
    on RLHF, Prompting, and Beyond.") and Theorem [1.4](#S1.Thmtheorem4 "Theorem 1.4
    (DAgger Error Bound, Ross et al. [7]). ‣ A Method for Reward Engineering ‣ 1.2.4
    Imitation Learning ‣ 1.2 Technical Formumation ‣ 1 A Crash Introduction to RL:
    Online RL, Offline RL, and Inverse RL ‣ Reinforcement Learning in the Era of LLMs:
    What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond."),
    we see that having access to a dynamics model ![[Uncaptioned image]](img/eb9127c8b2a0cb066cfe1d5e109882ce.png)
    is essential in controlling the error bound.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.5 Inverse Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inverse reinforcement learning (IRL) is actually just one of the many solutions
    to IL problems, with an emphasis on reward model learning. It first learns a reward
    model, and then uses such a reward model — combined with the dynamics model —
    to perform online RL.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9112bfccf5cfc88246b0b2b891675069.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Inverse Reinforcement Learning (IRL) solves the IL tasks in two steps:
    (1). reward modeling that distills the knowledge of underlying learning objectives
    that the behavior policy seems to optimize from the offline decision demonstration
    dataset. (2). combining such a learned reward model and the accessible dynamics
    model, everything needed for an online RL algorithm is right there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Offline IL and Offline IRL: What if both the reward model and dynamics model
    are not available? This situation is clearly more challenging. The demonstration
    dataset in such settings will be in the form of $\mathcal{D}_{\mathrm{OIL}}=\{(s^{i}_{t},a^{i}_{t},s^{i}_{t+1})\}$.
    Besides the behavior cloning method, there are several alternative approaches
    like the energy-based method SBIL [[11](#bib.bib11)], and the latent space decomposition
    method ABC [[12](#bib.bib12)]. ABC can be regarded as an accountable counterpart
    of BC, therefore, it works in all settings where BC can be applied.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.6 Learning from Demonstrations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another related but different topic is Learning from Demonstrations (LfD) [[13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15)], which leverages the demonstration dataset
    as a warm-start for RL. For instance, in the aforementioned tasks of Go or StarCraft,
    we can first use the demonstration dataset to perform behavior cloning (BC) and
    then use the learned BC policy as a warm start for RL. LfD also benefits the exploration
    of robotics control tasks where the reward can be extremely sparse, and defined
    as “whether the goal is achieved”. In a nutshell, LfD uses demonstrations to improve
    exploration in reward sparse tasks, and those demonstrations may not be optimal
    (e.g., non-expert players’ replay of StarCraft [[16](#bib.bib16)]), LfD then returns
    to RL and a sparse reward function to further refine the policy learned from demonstration
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.7 Comparison Between Different Settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The table below summarizes the differences between RL, Offline-RL, IL, IRL,
    Offline-IRL, and LfD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summarization of the differences between RL, Offline-RL, IL, IRL,
    Offline-IRL, and LfD.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Problem | External | External | Learned | (Near)-Expert | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Settings | Dynamics | Reward | Reward | Demonstration | Solvers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Model | Model | Model |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RL | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) | ![[Uncaptioned
    image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) | ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png)
    | ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png) | PPO [[17](#bib.bib17)],
    TD3 [[18](#bib.bib18)],SAC [[19](#bib.bib19)] |'
  prefs: []
  type: TYPE_TB
- en: '| Offline-RL | ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png)
    | ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png) | ![[Uncaptioned
    image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) or ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png)
    | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) | BC, ABC [[12](#bib.bib12)],
    CQL [[20](#bib.bib20)], WGCSL [[21](#bib.bib21)] |'
  prefs: []
  type: TYPE_TB
- en: '| IL | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) | ![[Uncaptioned
    image]](img/41b910777a04d213fa87210ff75f5b4d.png) | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png)
    or ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png) | ![[Uncaptioned
    image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) | BC, ABC [[12](#bib.bib12)],
    GAIL [[10](#bib.bib10)] |'
  prefs: []
  type: TYPE_TB
- en: '| IRL | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) |
    ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png) | ![[Uncaptioned
    image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png)
    | BC, ABC [[12](#bib.bib12)], T-REX [[22](#bib.bib22)] |'
  prefs: []
  type: TYPE_TB
- en: '| Offline-IRL | ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png)
    | ![[Uncaptioned image]](img/41b910777a04d213fa87210ff75f5b4d.png) | ![[Uncaptioned
    image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png)
    | BC, ABC [[12](#bib.bib12)], SBIL [[11](#bib.bib11)] |'
  prefs: []
  type: TYPE_TB
- en: '| LfD | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) |
    ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png) | ![[Uncaptioned
    image]](img/41b910777a04d213fa87210ff75f5b4d.png) | ![[Uncaptioned image]](img/0065e940a8fc3eac94f4ff4300b1c13a.png)
    | DQNfD [[14](#bib.bib14)], DDPGfD [[15](#bib.bib15)], AlphaStar [[16](#bib.bib16)]
    |'
  prefs: []
  type: TYPE_TB
- en: '2 RLHF: Solving the Problem of Offline RL with Online Inverse RL'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 LLM Alignment from Human Feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the task of LLM alignment from human feedback, LLMs are fine-tuned to better
    follow user instructions. In the seminal paper of OpenAI [[23](#bib.bib23)], such
    an alignment includes two general parts: supervised fine-tuning (SFT) and reinforcement
    learning from human feedback (RLHF). Figure [5](#S2.F5 "Figure 5 ‣ 2.1 LLM Alignment
    from Human Feedback ‣ 2 RLHF: Solving the Problem of Offline RL with Online Inverse
    RL ‣ Reinforcement Learning in the Era of LLMs: What is Essential? What is needed?
    An RL Perspective on RLHF, Prompting, and Beyond.") below illustrates those concrete
    steps. The first part of SFT is relatively easy to follow and implement, yet the
    secret and insight behind RLHF are more intricate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4bd46713c72471b91ee7af2e8c9dfa59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: (From Ouyang et al. [[23](#bib.bib23)]) There are 3 steps to align
    LLMs to human preference. Step 1: supervised fine-tuning of pre-trained LLM to
    follow instructions (generated by human demonstration data). Step 2: sample multiple
    responses for every query, and rank those responses according to human preference.
    Then a reward model can be learned to mimic the human preference. Step 3: Optimize
    the language model through RL to maximize the feedback from the reward model'
  prefs: []
  type: TYPE_NORMAL
- en: '2.2 Aligning with Human Preference: the Online Nature and Offline Practice'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ideally, the RLHF phase can be conducted with human-in-the-loop, as shown in
    Figure [6](#S2.F6 "Figure 6 ‣ 2.2 Aligning with Human Preference: the Online Nature
    and Offline Practice ‣ 2 RLHF: Solving the Problem of Offline RL with Online Inverse
    RL ‣ Reinforcement Learning in the Era of LLMs: What is Essential? What is needed?
    An RL Perspective on RLHF, Prompting, and Beyond."). In such an online setting,
    Human provides feedback to every response of LLMs, and LLMs learn from the external
    reward model of human preference. In fact, OpenAI now should be able to conduct
    such a process by collecting user’s feedback on ChatGPT’s response. But usually,
    such an online setting is infeasible due to its high cost of keeping humans in
    the loop.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5758046a18bdb5131637b0b7727acd40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: RLHF as an online RLproblem: Human preference is the underlying reward
    model, however, querying humans to provide feedback on every response is usually
    infeasible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Practically, RLHF addresses such a difficulty by generating an offline alignment
    dataset that contains different queries (i.e., states $s$ in RL). From such a
    perspective, the RLHF may seem to be a natural online RL problem but adjusted
    into an offline RL problem due to cost considerations. Figure [7](#S2.F7 "Figure
    7 ‣ 2.2 Aligning with Human Preference: the Online Nature and Offline Practice
    ‣ 2 RLHF: Solving the Problem of Offline RL with Online Inverse RL ‣ Reinforcement
    Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective
    on RLHF, Prompting, and Beyond.") illustrates such a generation process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d8aca8d5770e5d947f7f861dbc70055b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Because of the high cost of keeping humans in the loop, the practice
    of RLHF considers learning with an offline dataset generated by interactions between
    (the SFT) LLMs and Human annotators. The generated offline dataset is then used
    for LLM alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall the problems of distributional shift and compounding error we discussed
    above in Offline RL, it seems RLHF must suffer from such problems. However, we
    show in the next section that RLHF can actually be solved as an Online IL problem,
    rather than an offline RL problem.
  prefs: []
  type: TYPE_NORMAL
- en: '2.3 RLHF: From Offline-RL to Online Imitation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The essential observation we would highlight in RLHF is that the dynamics model
    in response generation is known. Specifically, harking back to Figure 6, the actions
    are tokens generated by LLMs, and the responses (trajectories) are concatenations
    of those generated tokens. Given the auto-regression nature of LLMs’ generation
    process, given an initial query denoted as $s_{0}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$s_{0}\sim p(s_{0})$: (Interpretation:) sample from query distribution | (RL
    Language:) sample from initial state distribution'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $a_{0}\sim\ell(s_{0})$ | (RL Language:) sample action from the policy
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$s_{1}=\mathcal{T}(s_{0},a_{0})=\mathrm{Concat}(s_{0},a_{0})$: (Interpretation:)
    concatenate the generated token and the query as input for LLM for the next token
    generation | (RL Language:) the transition dynamics gives the next state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$a_{1}\sim\ell(s_{1})$: …'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure [8](#S2.F8 "Figure 8 ‣ 2.3 RLHF: From Offline-RL to Online Imitation
    ‣ 2 RLHF: Solving the Problem of Offline RL with Online Inverse RL ‣ Reinforcement
    Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective
    on RLHF, Prompting, and Beyond.") showcases why the LLM alignment task can be
    solved as an online IL in practice (c.f. Figure [3](#S1.F3 "Figure 3 ‣ A Method
    for Reward Engineering ‣ 1.2.4 Imitation Learning ‣ 1.2 Technical Formumation
    ‣ 1 A Crash Introduction to RL: Online RL, Offline RL, and Inverse RL ‣ Reinforcement
    Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective
    on RLHF, Prompting, and Beyond."): pictorial illustration of IL)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a249cae778406d61ed693fe49ef5bf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: When aligning LLMs using an offline dataset, the dynamics model is
    a concatenation of the generated token and existing tokens, therefore, the offline
    RL problem can be solved by online IL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Practically, RLHF chooses to use the Inverse RL approach for the IL problem
    — with the first step explicitly learning a reward model, and the second step
    conducting RL using such a reward model. Figure [9](#S2.F9 "Figure 9 ‣ 2.3 RLHF:
    From Offline-RL to Online Imitation ‣ 2 RLHF: Solving the Problem of Offline RL
    with Online Inverse RL ‣ Reinforcement Learning in the Era of LLMs: What is Essential?
    What is needed? An RL Perspective on RLHF, Prompting, and Beyond.") illustrates
    the learning procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2b31fec28f76754cb139e7ade338b2c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: RLHF is online IRL. The reward modeling step learns a reward function
    from the alignment dataset, and given such a reward model and the known transition
    dynamics (concatenation), online RL algorithms like PPO can then be applied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Takeaway: When aligning LLMs with an offline human-preference alignment dataset,
    RLHF uses an online IRL approach. This is because the transition dynamics model
    is known. Leveraging such a property, the compounding error and distributional
    shift problems of offline RL can be alleviated.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Challenges and Open Questions from an RL Perspective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.4.1 Why is RLHF better than SFT?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the discussions above, the reason RLHF can be better than SFT — from an
    RL perspective — is that RLHF leverages the fact that the dynamics model is known,
    and uses IL to solve the alignment problem. On the other hand, SFT corresponds
    to the behavior clone approach, which suffers from the problem of compounding
    error. Therefore, as long as IL > BC, we have RLHF > SFT.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Is PPO the Only Solution?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, several works have proposed alternatives to RLHF, including DPO [[24](#bib.bib24)]
    that directly optimizes the LLM using human-preference data without reward modeling,
    RRHF [[25](#bib.bib25)] and RAFT [[26](#bib.bib26)] propose ranking-based sampling
    methods as alternatives to PPO to address the computational instability issue
    and high GPU memory demand of PPO.
  prefs: []
  type: TYPE_NORMAL
- en: 'So clearly, there is still a lot of space for future improvement over PPO.
    We would like to mention the following pros and cons (mostly based on empirical
    observations) of PPO:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PPO works well in large-scale discrete tasks [[16](#bib.bib16)]. The action
    space of LLM is far larger than normal RL problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'PPO has a faster wall-clock training time compared to off-policy methods like
    DQN [[27](#bib.bib27)]. PPO can be highly environment-parallelized. In fact, this
    is normally an implementation problem: in DQN a higher Update-to-Data (UTD) ratio [[28](#bib.bib28)]
    is used: updates of networks in DQN are conducted every time step, but in PPO
    the updates of networks only happen at the end of an episode. e.g., after the
    entire response is generated.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aligning to human preference is a sparse reward problem. In the sense that only
    at the end of an episode will the agent receive a reward signal (provided by human
    feedback or the learned reward function). Such a setting is relevant to the multi-goal
    robotics control tasks [[29](#bib.bib29)] where the idea of Hindsight learning
    shines with the value-based methods [[30](#bib.bib30), [3](#bib.bib3)] — rather
    than policy-based methods like TRPO [[31](#bib.bib31)] and PPO. There are several
    attempts using the Hindsight relabeling trick for LLM fine-tuning [[32](#bib.bib32),
    [33](#bib.bib33)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A fun fact is that policy-gradient and value-based methods are almost equivalent [[34](#bib.bib34)].
    But in practice, the studies on LLM finetuning now mainly focus on the on-policy
    policy-based methods. The performance differences between policy-based methods
    and value-based methods can be mainly attributed to (1). on-policy/ off-policy
    data — the staleness of the data they used for value and policy learning; and
    (2). whether using an aggressive and explicit or conservative and implicit policy
    learning — while the policy-gradient methods like PPO and TRPO use a value function
    implicitly for policy learning (i.e., use them as critics to calculate policy
    gradient values that improve policy quality), the value-based methods like TD3
    and SAC explicitly turns the learned value function into policies (i.e., through
    either deterministic policy gradient DPG [[35](#bib.bib35)] in TD3 or the Boltzmann
    policy [[36](#bib.bib36)] as in SAC/soft Q-learning [[37](#bib.bib37)].)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.4.3 What to Improve?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Credit Assignment: The preference provided by humans is on a trajectory level.
    Hence the learned reward model can only compare responses on an entire level.
    Is there a way to assign credit to different tokens or part of tokens? A known
    fact in RL is dense reward problems are much easier to learn, though they do not
    necessarily outperform the sparse reward settings. [[29](#bib.bib29)] (because
    of local minima, again, a reward engineering problem)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Algorithmic Design: RL algorithms are seldom designed in a way that assumes
    knowing the dynamics model. But in LLM alignment, the actions are actually generated
    in an auto-regressive manner. Is there a more efficient and stable RL algorithm
    that works better than PPO in such a series generation setting? This is a sort
    of Auto-Regressive MDP.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prompting: Is the prompting strategy optimized? Maybe the prompting strategy
    is not correct in getting the desired answer. Prompt optimization can definitely
    help improve the performance of LLMs. To address such a point, we introduce recent
    work on query-dependent prompt optimization [[38](#bib.bib38)] in the next section,
    which also links RL and LLM.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3 Prompting with Offline IRL: Prompt Optimization is RL from AI Feedback'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7566bf84dbd85b33faea5a2462629d17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: (From Sun [[38](#bib.bib38)].) A motivating example ([left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179),
    [right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999)).
    No prompt is perfect that works for all queries. The optimal prompt is query-dependent.
    Yet the seeking of such prompts is hindered by 2 challenges. Prompt-OIRL [[38](#bib.bib38)]
    optimizes prompt during inference on a query-dependent level effectively and cost-efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The Query-Dependent Prompting Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Out of the many attempts, prompting — a natural language prefix or instruction
    that explains how to complete the task — stands out as a lightweight promising
    solution for eliciting the capabilities of LLMs without model parameter tuning.
    While the advances in zero-shot prompting strategies highlight the potential for
    finding effective query-independent solutions, its reliance on manual crafting
    efforts and the vast search space over natural language intensifies the difficulty
    in discovering effective prompts. Moreover, as demonstrated in Figure [10](#S3.F10
    "Figure 10 ‣ 3 Prompting with Offline IRL: Prompt Optimization is RL from AI Feedback
    ‣ Reinforcement Learning in the Era of LLMs: What is Essential? What is needed?
    An RL Perspective on RLHF, Prompting, and Beyond."), the optimal prompt is query
    dependent — there is no perfect prompt that works for all queries.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Prompt-OIRL: Prompt Evaluation and Optimization with Offline Inverse RL'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prompt-OIRL is a novel approach grounded in offline inverse reinforcement learning,
    designed to reconcile effective and cost-efficient query-dependent prompt evaluation
    and optimization. This method leverages offline datasets from existing evaluations,
    utilizing Inverse-RL to craft a reward model tailored for offline, query-specific
    prompt evaluations. Prompt-OIRL offers several benefits: it forecasts prompt efficacy,
    minimizes costs, and explores the prompt space more effectively — all at a query-dependent
    level. We validate our approach across various LLMs and arithmetic reasoning datasets,
    underscoring its viability as a formidable solution for query-dependent offline
    prompt evaluation and optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Potential Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While Prompt-OIRL primarily centers on arithmetic reasoning tasks, we wish
    to underscore the versatility of Prompt-OIRL’s insights for broader applications,
    especially where there exists a prompting demonstration dataset accompanied by
    ratings of the prompted responses. As a hypothetical approach to dataset construction
    with human annotators incorporated into the process, consider this: human annotators
    could employ LLMs to accomplish specific tasks. They might offer multiple prompts
    as instructions for the task, and the ensuing LLM responses can then be graded
    based on proficiency in executing the given task. In fact, these annotators could
    be everyday LLM users keen on evaluating diverse responses. We earmark this intriguing
    concept for subsequent exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sun et al. [2021] Hao Sun, Ziping Xu, Meng Fang, Zhenghao Peng, Jiadong Guo,
    Bo Dai, and Bolei Zhou. Safe exploration by solving early terminated mdp. *arXiv
    preprint arXiv:2107.04200*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ni et al. [2021] Tianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent
    model-free rl can be a strong baseline for many pomdps. *arXiv preprint arXiv:2110.05038*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2019] Hao Sun, Zhizhong Li, Xiaotong Liu, Bolei Zhou, and Dahua
    Lin. Policy continuation with hindsight inverse dynamics. *Advances in Neural
    Information Processing Systems*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2020a] Hao Sun, Ziping Xu, Yuhang Song, Meng Fang, Jiechao Xiong,
    Bo Dai, and Bolei Zhou. Zeroth-order supervised policy improvement. *arXiv preprint
    arXiv:2006.06600*, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2020b] Hao Sun, Zhenghao Peng, Bo Dai, Jian Guo, Dahua Lin, and
    Bolei Zhou. Novel policy seeking with constrained optimization. *arXiv preprint
    arXiv:2005.10696*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2022a] Hao Sun, Ziping Xu, Zhenghao Peng, Meng Fang, Taiyi Wang,
    Bo Dai, and Bolei Zhou. Constrained mdps can be solved by eearly-termination with
    recurrent models. In *NeurIPS 2022 Foundation Models for Decision Making Workshop*,
    2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ross et al. [2011] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction
    of imitation learning and structured prediction to no-regret online learning.
    In *Proceedings of the fourteenth international conference on artificial intelligence
    and statistics*, pages 627–635\. JMLR Workshop and Conference Proceedings, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. [2018] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel
    Van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based
    character skills. *ACM Transactions On Graphics (TOG)*, 37(4):1–14, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2022b] Hao Sun, Lei Han, Rui Yang, Xiaoteng Ma, Jian Guo, and Bolei
    Zhou. Exploit reward shifting in value-based deep-rl: Optimistic curiosity-based
    exploration and conservative exploitation via linear reward shaping. *Advances
    in Neural Information Processing Systems*, 35:37719–37734, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho and Ermon [2016] Jonathan Ho and Stefano Ermon. Generative adversarial imitation
    learning. *Advances in neural information processing systems*, 29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jarrett et al. [2020] Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar.
    Strictly batch imitation learning by energy-based distribution matching. *Advances
    in Neural Information Processing Systems*, 33:7354–7365, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2023] Hao Sun, Alihan Hüyük, Daniel Jarrett, and Mihaela van der
    Schaar. Accountable batched control with decision corpus. *Advances in Neural
    Information Processing Systems*, 36, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaal [1996] Stefan Schaal. Learning from demonstration. *Advances in neural
    information processing systems*, 9, 1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hester et al. [2018] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot,
    Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband,
    et al. Deep q-learning from demonstrations. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 32, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nair et al. [2018] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba,
    and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations.
    In *2018 IEEE international conference on robotics and automation (ICRA)*, pages
    6292–6299\. IEEE, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vinyals et al. [2019] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,
    Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell,
    Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent
    reinforcement learning. *Nature*, 575(7782):350–354, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2017a] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujimoto et al. [2018] Scott Fujimoto, Herke Hoof, and David Meger. Addressing
    function approximation error in actor-critic methods. In *International conference
    on machine learning*, pages 1587–1596\. PMLR, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George
    Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel,
    et al. Soft actor-critic algorithms and applications. *arXiv preprint arXiv:1812.05905*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. [2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
    Conservative q-learning for offline reinforcement learning. *Advances in Neural
    Information Processing Systems*, 33:1179–1191, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2022] Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali
    Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised
    learning and its connection to offline rl. *arXiv preprint arXiv:2202.04478*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2019] Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott
    Niekum. Extrapolating beyond suboptimal demonstrations via inverse reinforcement
    learning from observations. In *International conference on machine learning*,
    pages 783–792\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. [2023] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model. *arXiv preprint arXiv:2305.18290*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2023] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang
    Huang, and Fei Huang. Rrhf: Rank responses to align language models with human
    feedback without tears. *arXiv preprint arXiv:2304.05302*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. [2023] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe
    Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning
    for generative foundation model alignment. *arXiv preprint arXiv:2304.06767*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep
    reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Janner et al. [2019] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
    When to trust your model: Model-based policy optimization. *Advances in neural
    information processing systems*, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plappert et al. [2018] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob
    McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej,
    Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics
    environments and request for research. *arXiv preprint arXiv:1802.09464*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrychowicz et al. [2017] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas
    Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel,
    and Wojciech Zaremba. Hindsight experience replay. *Advances in neural information
    processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael
    Jordan, and Philipp Moritz. Trust region policy optimization. In *International
    conference on machine learning*, pages 1889–1897\. PMLR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Languages
    are rewards: Hindsight finetuning using human feedback. *arXiv preprint arXiv:2302.02676*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023] Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel,
    and Joseph E Gonzalez. The wisdom of hindsight makes language models better instruction
    followers. *arXiv preprint arXiv:2302.05206*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2017b] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence
    between policy gradients and soft q-learning. *arXiv preprint arXiv:1704.06440*,
    2017b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. [2014] David Silver, Guy Lever, Nicolas Heess, Thomas Degris,
    Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms.
    In *International conference on machine learning*, pages 387–395\. Pmlr, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: O’Donoghue et al. [2016] Brendan O’Donoghue, Remi Munos, Koray Kavukcuoglu,
    and Volodymyr Mnih. Combining policy gradient and q-learning. *arXiv preprint
    arXiv:1611.01626*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haarnoja et al. [2017] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey
    Levine. Reinforcement learning with deep energy-based policies. In *International
    conference on machine learning*, pages 1352–1361\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun [2023] Hao Sun. Offline prompt evaluation and optimization with inverse
    reinforcement learning. *arXiv preprint arXiv:2309.06553*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
