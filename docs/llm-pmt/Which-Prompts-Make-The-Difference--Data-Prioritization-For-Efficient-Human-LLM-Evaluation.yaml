- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM
    Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.14424](https://ar5iv.labs.arxiv.org/html/2310.14424)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: name=Meriem Boubdir    affiliation=Cohere for AI    email=meri.boubdir@gmail.com
       name=Edward Kim    affiliation=Cohere    email=edward@cohere.com    name=Beyza
    Ermis    affiliation=Cohere for AI    email=beyza@cohere.com    name=Marzieh Fadaee
       affiliation=Cohere for AI    email=marzieh@cohere.com    name=Sara Hooker   
    affiliation=Cohere for AI    email=sarahooker@cohere.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Human evaluation is increasingly critical for assessing large language models,
    capturing linguistic nuances, and reflecting user preferences more accurately
    than traditional automated metrics. However, the resource-intensive nature of
    this type of annotation process poses significant challenges. The key question
    driving our work: is it feasible to minimize human-in-the-loop feedback by prioritizing
    data instances which most effectively distinguish between models? We evaluate
    several metric-based methods and find that these metrics enhance the efficiency
    of human evaluations by minimizing the number of required annotations, thus saving
    time and cost, while ensuring a robust performance evaluation. We show that our
    method is effective across widely used model families, reducing instances of indecisive
    (or “tie”) outcomes by up to 54% compared to a random sample when focusing on
    the top-20 percentile of prioritized instances. This potential reduction in required
    human effort positions our approach as a valuable strategy in future large language
    model evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: \pdfcolInitStack
  prefs: []
  type: TYPE_NORMAL
- en: tcb@breakable
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have produced notable breakthroughs in downstream
    performance [[61](#bib.bib61); [11](#bib.bib11); [19](#bib.bib19); [62](#bib.bib62);
    [91](#bib.bib91); [49](#bib.bib49); [8](#bib.bib8); [78](#bib.bib78)], but have
    also introduced new challenges in model evaluation. The success of LLMs has initiated
    a fundamental paradigm shift away from small specialized models designed for single
    tasks to universal models expected to perform well across a wide range of tasks.
    This shift has also posed an existential challenge for evaluation, with a need
    to move away from solely task-specific automatic metrics of evaluation and increasing
    reliance on human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: While automatic metrics offer a degree of objectivity and reproducibility, alongside
    the benefits of speed and cost-effectiveness, they often fall short in fully capturing
    the complexities and nuances of natural language [[48](#bib.bib48); [68](#bib.bib68)].
    Moreover, automatic metrics often rely on auxiliary models which introduce potential
    points of failure and unexpected challenges over time [[58](#bib.bib58)]. For
    example, reference-based metrics such as BLEU [[54](#bib.bib54)] and ROUGE [[45](#bib.bib45)]
    are usually poor indicators of human judgment, as they emphasize lexical overlap
    and struggle to account for the diverse expressions inherent in semantic representation
     [[34](#bib.bib34); [84](#bib.bib84); [9](#bib.bib9)]. This limitation is particularly
    prominent in open-ended conversations [[47](#bib.bib47)], open-response question
    answering [[41](#bib.bib41)], summarization [[66](#bib.bib66)], and code generation
    [[30](#bib.bib30)], where there isn’t a single reference ground truth for comparison,
    but rather diverse yet equivalent responses.
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluation has emerged as an indispensable method in assessing LLMs’ performance.
    It provides direct feedback on understandability, usability, correctness, and
    safety [[79](#bib.bib79)] from the end-user’s perspective. Furthermore, human
    feedback plays an essential role in aligning model behavior through either supervised
    finetuning on human annotated completions [[3](#bib.bib3); [60](#bib.bib60); [89](#bib.bib89)]
    or reinforcement learning with reward models learned from human preferences  [[92](#bib.bib92);
    [70](#bib.bib70); [2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: However, this growing dependence on human evaluation has introduced certain
    trade-offs. The evaluation process itself is resource-intensive requiring substantial
    investments in human resources and domain-specific expertise [[22](#bib.bib22);
    [6](#bib.bib6)]. Notably, in the context of safety, human annotation can expose
    evaluators to highly toxic content, which carries a significant risk to their
    mental health and leaves them vulnerable to lasting psychological harm [[69](#bib.bib69)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/63930935506fe0de49407d2e4ec5d134.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Evaluation Methodology: Human evaluators assess N prompts, each with
    completions from two different models, A and B (left). Completions are ranked
    by dissimilarity, from the most distinct to potential ties (right). The dashed
    line marks a threshold at ‘k’ evaluations beyond which the likelihood of a tied
    outcome increases, making further evaluations less informative at assessing models
    performance discrepancies.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we aim to reduce the number of required human annotations while
    mitigating tie outcomes in a comparison setting, thereby diminishing both time
    and resource expenditures. We ask Which prompts should be prioritized to most
    efficiently differentiate and rank relative model quality amongst a pool of models?
    To achieve this, we introduce prompt prioritization as a strategy to reduce the
    need for extensive human feedback and effectively decrease the time and cost complexities
    associated with the annotation collection process. By informatively mitigating
    tie outcomes in a comparison setting, our aim is to maintain robust performance
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We benchmark several metrics, such as Cross-Entropy and KL divergence, to assess
    the dissimilarity between model completions. These metrics, efficient to compute
    as derivatives of a model’s generation probabilities, guide our prompt ranking
    strategy. Across several popular and widely used model families such as Flan-T5
    [[20](#bib.bib20)], Dolly-V2 [[23](#bib.bib23)] families, as well as MPT-7B-instruct
    [[74](#bib.bib74)] and Falcon-7B-instruct [[56](#bib.bib56)], we arrive at consistent
    results. Our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Systematic Offline Ranking: We introduce a systematic, offline ranking method
    to expedite the evaluation process. This approach leverages KL divergence and
    Cross-Entropy to prioritize prompts and completion pairs based on their predicted
    potential to yield decisive preference outcomes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduction in Ties: We show that our use of KL divergence for ranking completion
    pairs results in a remarkable 54% reduction in ties within the top 20% of prioritized
    prompts when compared to random selection. This finding highlights the potential
    of our approach in improving the efficiency of human-in-the-loop evaluations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robustness of Elo Scores: We highlight that our ranking strategy enhances the
    stability of Elo scores, a widely used evaluation metric for players in zero-sum
    games, across various model comparisons. This improvement allows us to reduce
    our dependence on extensive human annotations while maintaining reliable and conclusive
    models rankings.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many human evaluation pipelines use a pairwise comparison format where model
    A is compared to model B. Pairwise evaluation is often considered to be less subjective
    and more consistent compared to other evaluation methods because it focuses on
    relative judgments rather than absolute judgments. This involves comparing the
    responses of two models to a single prompt, a strategy referred to as a head-to-head
    pairwise block comparison, dating back to the work of  Thurstone [[77](#bib.bib77)].
    When a comparison for a given prompt favors one completion over the other, we
    gain insight into the relative performance of the two models (e.g., Sample 1 in
    Figure [2](#S2.F2 "Figure 2 ‣ 2 Methodology ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation")). If both models yield
    high-quality and equally valid responses for a prompt, discerning the superiority
    of one over the other becomes inherently challenging, as demonstrated in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Methodology ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation") (Sample 2). In this work, our objective involves
    strategically selecting prompts that amplify the informativeness of each comparison,
    thereby streamlining and optimizing the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample
    1: Comparison breaks tie – effective for ranking models Clear preference for completion
    A. This comparison has low likelihood of ties. Prompt Put the concepts
    together to form a sentence: kitchen, sit, table.
    Completion
    A A man sits at a table in a kitchen.
    Completion
    B The kitchen sits at the table.Sample 2: Comparison results
    in a tie – not effective for ranking models Both completions present equivalently
    suitable coherent and rational responses. Prompt Put the concepts
    together to form a sentence: jump, pool, swimsuit.
    Completion
    A A girl in a swimsuit jumps into a pool.
    Completion
    B Jumping into a swimming pool without a swimsuit
    is a bad idea.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Comparison of a prompt, used to rank completions from models A and
    B, Left: which is strong at distinguishing which model is better (low likelihood
    of ties in human eval), Right: which is weak at distinguishing which model is
    better (high likelihood of ties in human eval). Our goal is to automatically identify
    and rank prompts which are similar to sample 1 higher than the ones similar to
    sample 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, let us consider an evaluation set $P=\{p_{i}\}_{i=1}^{N}$, defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{cases}Score_{i}^{A}=1\text{ and }Score_{i}^{B}=0&amp;\text{if
    $c_{i}^{A}$ is preferred}\\ Score_{i}^{A}=0\text{ and }Score_{i}^{B}=1&amp;\text{if
    $c_{i}^{B}$ is preferred}\\'
  prefs: []
  type: TYPE_NORMAL
- en: Score_{i}^{A}=Score_{i}^{B}=1&amp;\text{if both are of similar quality}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}$$ |  | (1) |
  prefs: []
  type: TYPE_NORMAL
- en: We propose an offline approach that automatically ranks prompts in $P$ using
    model completions. While we assess responses from two models, our primary focus
    is on highlighting their dissimilarity. Although widely used, conventional string
    matching techniques such as BLEU and ROUGE are not well-suited for this problem.
    These metrics may indicate a high lexical overlap between two completions, but
    they might differ significantly in meaning or quality. Coversely, completions
    may exhibit minimal lexical overlap while delivering equally valid answers to
    the same prompt (see Figure [2](#S2.F2 "Figure 2 ‣ 2 Methodology ‣ Which Prompts
    Make The Difference? Data Prioritization For Efficient Human LLM Evaluation")).
    Such a focus is instrumental in minimizing tie outcomes in human evaluations,
    which can arise when both completions are viewed by annotators as similarly good
    or bad.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to rearrange the prompts and present them to the annotators in an
    order that starts with a low likelihood of a tie outcome and progresses to a higher
    likelihood. To achieve this, we reorder the prompt and completion pairs within
    an evaluation set $P$ (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Which
    Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation")).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Quantifying the "A vs. B" dissimilarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To enhance our evaluation process effectively, it is essential to understand
    how two model outputs diverge when given the same prompt. This section outlines
    our strategy to systematically prioritize prompts that tend to diminish tie outcomes
    in the top percentile, while simultaneously maximizing them in the bottom percentile.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, we quantify the variation between the responses of different
    models $\{c_{i}^{M_{1}},...,c_{i}^{M_{n}}\}$ is the number of models within our
    evaluation pool.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we investigate scoring metrics that utilize the log probabilities
    or probabilities of the completions, as computed by their respective generating
    models. This serves as an efficient proxy metric for quality and has been demonstrated
    to be more calibrated as an indicator of model certainty at larger model sizes
    [[16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: Let $S^{A}=(s_{1}^{A},s_{2}^{A},\ldots,s_{t}^{A})$. To accomplish our objective,
    we employ both KL Divergence (Equation [2](#S2.E2 "Equation 2 ‣ 2.1 Quantifying
    the "A vs. B" dissimilarity ‣ 2 Methodology ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation")) and Cross-Entropy (Equation [3](#S2.E3
    "Equation 3 ‣ 2.1 Quantifying the "A vs. B" dissimilarity ‣ 2 Methodology ‣ Which
    Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation"))
    as thesemetrics quantify the difference between two probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $KL=\sum_{i}\mathbf{p}_{i}^{A}\log\frac{\mathbf{p}_{i}^{A}}{\mathbf{p}_{i}^{B}}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $CE=-\sum_{i}\mathbf{p}_{i}^{A}\log\mathbf{p}_{i}^{B}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: KL Divergence measures how one probability distribution diverges from a second,
    expected distribution, making it useful for capturing differences between model
    completions, $S^{A}$. We normalize each sequence by the sum of its probabilities
    before calculating the divergence. Cross-Entropy indicates the dissimilarity between
    the two distributions, serving as an indicator of the model’s relative performance.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have model completions for each model in $\mathbb{M}$ respectively.
    The prompts in each ordered set span from pairs with the most dissimilar completions
    to those with the least dissimilar completions according to the metric. We anticipate
    that this prompt ranking will lead to a more efficient differentiation of model
    performance during human evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Applying our ranking in Elo rating computation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Originally introduced by  Elo [[28](#bib.bib28)] for ranking chess players,
    the Elo rating system has emerged as a pivotal tool in evaluating and comparing
    models performance, particularly in pairwise preference-based setups  [[70](#bib.bib70);
    [1](#bib.bib1); [50](#bib.bib50); [2](#bib.bib2); [51](#bib.bib51)]. By computing
    Elo ratings for each model within a pool, we can establish a global ranking tier
    based solely on pairwise comparisons. In this section, we explore how our proposed
    evaluation methodology can benefit the convergence and robustness of Elo-based
    evaluations, especially when dealing with limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: The appeal of the Elo rating system in evaluating LLMs lies in its ability to
    effectively capture the performance dynamics within a constantly changing pool
    of models. Furthermore, it provides an intuitive interpretation of the ratings
    as a measure of expected performance in a direct head-to-head comparison of any
    two models.
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition, the expected Elo scores for each model are calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle E_{\mathsf{A}}$ |  | (4a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle E_{\mathsf{B}}$ |  | (4b) |'
  prefs: []
  type: TYPE_TB
- en: 'Here $R_{\mathsf{A}}$ is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R_{\mathsf{A}}^{\prime}=R_{\mathsf{A}}+K\cdot(S_{\mathsf{A}}-E_{\mathsf{A}})$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: In this equation, $R_{\mathsf{A}}$.
  prefs: []
  type: TYPE_NORMAL
- en: After $N$ evaluations, the final Elo ratings, referred to as Elo scores are
    used to rank models. Higher scores signify stronger models, and the scores difference
    between models serves as a valuable indicator of performance disparities. As indicated
    in Equation [5](#S2.E5 "Equation 5 ‣ 2.2 Applying our ranking in Elo rating computation
    ‣ 2 Methodology ‣ Which Prompts Make The Difference? Data Prioritization For Efficient
    Human LLM Evaluation"), Elo ratings are updated through pairwise comparisons.
    The arrangement of evaluation prompts and, subsequently, the outcome of each comparison,
    can influence the behavior of the Elo ratings. We use this evaluation framework
    to investigate how our prompt rankings impact the robustness of Elo scores and
    consequently model performance rankings.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Pool of prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our objective is to evaluate model quality across a range of tasks, spanning
    from unstructured to more structured formats. To achieve this, we sample prompts
    from several different sources. The social *chit-chat* dataset, Soda [[39](#bib.bib39)],
    is utilized to assess the models’ performance in handling informal conversations.
    To evaluate the models’ proficiency in structured and goal-oriented settings,
    we incorporate prompts from the Public Pool of Prompts (P3) dataset [[64](#bib.bib64)].
    Prompts from the datasets CommonsenseQA [[72](#bib.bib72)] and CommonGen [[44](#bib.bib44)]
    serve to assess the models’ capability of reasoning and coherent narratives generation
    given some context. We also include AdversarialQA, which consists of complex prompts
    specifically designed to mislead the models. We randomly sample 100 examples from
    each dataset to create our target pool of prompts. In the case of open-domain
    dialog corpora, we process the dialogue into a sequence of utterances and randomly
    sample a turn index. The prompt is then constructed up to that turn, and the model’s
    task is to generate the next turn(s).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Details of the models used for evaluation in this work.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Base model | Architecture | Size | Finetuning data |  |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-t5 | T5 | encoder-decoder | [3B, 11B] | formal instruct |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dolly-v2 | pythia | decoder-only | [7B, 12B] | colloquial instruct |  |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-instruct | falcon | decoder-only | [7B] | instruct/chat |  |'
  prefs: []
  type: TYPE_TB
- en: '| MPT-instruct | mpt | decoder-only | [7B] | colloquial instruct/preference
    |  |'
  prefs: []
  type: TYPE_TB
- en: We assemble an evaluation pool comprised of several state-of-the-art and widely
    used language models including Flan-T5 [[20](#bib.bib20)] and Dolly-V2 [[23](#bib.bib23)]
    families, as well as MPT-7b-instruct [[74](#bib.bib74)] and falcon-7b-instruct
    [[56](#bib.bib56)]. Each of these models, ranging from 3 billion to 12 billion
    parameters, has been selected for their demonstrated capabilities across various
    NLP tasks and the diversity they bring to our pool regarding architectural differences
    and training methodologies. While Flan-T5 is an encoder-decoder model, all other
    models are decoder only. The base models also differ between models, ranging from
    Pythia [[8](#bib.bib8)] to T5 [[21](#bib.bib21)]. Table [1](#S3.T1 "Table 1 ‣
    3.2 Models ‣ 3 Experimental Setup ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation") provides additional information about each
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Completion generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To ensure fairness in comparing models, it is essential to generate high-quality
    completions for each model in our evaluation pool. We briefly describe below the
    parameters we used during inference time for each model, chosen in accordance
    with the guidelines specified by the respective model repositories or HuggingFace
    model card.
  prefs: []
  type: TYPE_NORMAL
- en: For all our models, we generate completions in a zero-shot manner. For flan-t5-xxl
    and flan-t5-xl models, inference was run exclusively on a CPU device using the
    JAX framework [[10](#bib.bib10)], configured for deterministic outputs via $\tau=0$.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the absence of a specific padding token for these models, we used the
    "" token as a substitute. Completions were generated in variable batches,
    ranging from 8 to 50, depending on the model sizes, on an Nvidia A100 GPU with
    40GB memory, to maximize computational efficiency. To reduce memory usage, we
    run inference in a Bfloat16 setting  [[37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: It is important to highlight that due to the differences in training and fine-tuning
    data among models and the diverse nature of our evaluation tasks, certain models
    faced challenges when dealing with specific datasets. In particular MPT-7b-instruct
    and falcon-7b-instruct struggled with generating completions for prompts from
    the AdversarialQA. To address this, we appended the phrase "Answer:" to the end
    of these prompts, enabling models to generate reasonable completions. However,
    the quality of completions from MPT-7b-instruct on both AdversarialQA and Soda
    remained subpar, necessitating the omission of these datasets in experiments involving
    these models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Human annotation collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Annotator Task We incorporate an outcome-level relative assessment (ORA) for
    human evaluations. This comparative method, as described by [[29](#bib.bib29)],
    operates on assumptions about the rationality of human judgment, commonly employed
    in LLM evaluations [[2](#bib.bib2)]: Generate pairs with outputs from two systems,
    have annotators select their preferred choice from each pair, and calculate a
    preference-based score to evaluate system performance.'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we create random pairs containing completions from models $\mathsf{A}$
    in the interface. This random assignment prevents any potential bias that might
    arise if annotators consistently favor one position over the other.
  prefs: []
  type: TYPE_NORMAL
- en: Random Baseline The procedure we follow to establish a baseline for comparison
    between two models involves presenting prompts $P=\{p_{i}\}_{i=1}^{N}$, against
    rankings based on both KL divergence (KL) and Cross-Entropy (CE) at varying top
    k percentiles. An effective metric that aptly measures the dissimilarity between
    two completions should manifest as reduced tie rates, particularly in the earliest,
    highest-ranked annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation of Preferences For each unique “A vs. B” model comparison experiment,
    we collect $N_{evals}$ when comparing models from different families.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An efficient ranking will aim to minimize tie outcomes among the highest-ranked
    instances. By prioritizing instances with a lower likelihood of inducing a tie,
    we can obtain early signals to determine the superior model. Ideally, by employing
    a robust ranking system, one can draw the same conclusions about model performance
    using a specific subset of prompts as they would have by annotating the entire
    dataset. This is particularly valuable since human annotations can be costly,
    and budget constraints may make it unfeasible to manually annotate all instances
    within a given prompt pool. Our goal is to arrive at a ranking where annotating
    only a subset of the top k% of evaluation instances, where $k sat, ball —> football, swim —> swimmer • A completion should include at
    least one sentence with all concept words. • A completion may start with “The
    sentence is” • A completion may also include a description of the example sentence.
    • A completion with concept words split between two or more sentences is not Ok.
    CommonsenseQA — Explain why one choice among others is the correct answer: • Completions
    may include the prompt itself or part of it. This is Ok. • The completion should
    have a decent level of common sense, logical or moral reasoning, depending on
    the situation. • Some completions will start hallucinating scenarios as a possible
    explanation, these are per-case acceptable. If you are unsure, please share the
    example on the slack channel!'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Pairwise Comparison Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We detail the outcome rates of each pairwise comparison discussed in Section
    [4](#S4 "4 Results and Analysis ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation"). The summarized results are depicted in Figure [6](#A2.F6
    "Figure 6 ‣ Appendix B Pairwise Comparison Results ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation"). We observe that models
    within the same family, particularly those of proximate number or parameters,
    such as dolly-v2-12b vs. dolly-v2-7b, display analogous responses attributed to
    their shared architectures and overlapping training data, resulting in higher
    tie rates. On the other hand, comparisons between models from different families
    yield lower tie rates. Such models exhibit varied writing styles, stemming from
    their unique training backgrounds, making evaluations more decisive. Our evaluation
    prompts set, curated to encompass diverse tasks, accentuates these output disparities,
    with certain tasks proving more intuitive for some models and challenging for
    others.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b3d1763e5ef63965739ca884a02dba0.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. flan-t5-xl
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eff9fc58987fbfa8b2287378633d87a7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) dolly-v2-12b vs. dolly-v2-7b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc6dda92a488467662532be185245f6c.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) flan-t5-xxl vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e521a3fdbe5089dfec743c4646484de8.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) MPT-7b-instruct vs. falcon-7b-instruct
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Barplots representing the win rates per model for each of our four
    experiments, alongside the rate of tie outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Elo Scores Analysis Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building upon section [2.2](#S2.SS2 "2.2 Applying our ranking in Elo rating
    computation ‣ 2 Methodology ‣ Which Prompts Make The Difference? Data Prioritization
    For Efficient Human LLM Evaluation"), where we examine the impact of our ranking
    metrics on the Robustness of Elo Scores used to relatively rank models, we provide
    a direct comparison between inter- and intra-family results (see Fig. [7](#A3.F7
    "Figure 7 ‣ Appendix C Elo Scores Analysis Results ‣ Which Prompts Make The Difference?
    Data Prioritization For Efficient Human LLM Evaluation")). Additionally, we chart
    the progression of Elo ratings throughout the evaluation process. Compared to
    a random sequence (refer to Fig. [8](#A3.F8 "Figure 8 ‣ Appendix C Elo Scores
    Analysis Results ‣ Which Prompts Make The Difference? Data Prioritization For
    Efficient Human LLM Evaluation")), our ranking metrics amplify the divergence
    of Elo scores, especially within the early top k% of evaluation instances, up
    to top 40 percentile.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0e8918eea21beeb197d9ad9a500188c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. flan-t5-xl
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a23f26dec8d2cefed0fb877a129557b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) dolly-v2-7b vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f7b423ab742b82ba1dc44295c73d6622.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) flan-t5-xxl vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49364e1c114c72948aec1714dd5c75d0.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) MPT-7b-instruct vs. falcon-7b-instruct
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Elo Scores for inter- and intra-family experiments. Dashed lines
    indicate the ‘Gold Standard’ values for each model; refer to Table [3](#S4.T3
    "Table 3 ‣ 4.4 Elo Score Robustness ‣ 4 Results and Analysis ‣ Which Prompts Make
    The Difference? Data Prioritization For Efficient Human LLM Evaluation") for details.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3d5e8dd5bf333fc74dd05e2308e63de.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) flan-t5-xxl vs. flan-t5-xl
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f8925b747b509641345dbdb05a056ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) dolly-v2-7b vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95e9e989bd9921e5bddc1e67217abff9.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) flan-t5-xxl vs. dolly-v2-12b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/606df771a42a19cc0535661ab6a95dec.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) MPT-7b-instruct vs. falcon-7b-instruct
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Elo Ratings Analysis for Inter- and Intra-Family Model Comparisons.
    We contrast the ranking methods, KL Divergence and Cross-Entropy, against a Random
    Sequence strategy to demonstrate their impact on ratings update per evaluation
    instance.'
  prefs: []
  type: TYPE_NORMAL
