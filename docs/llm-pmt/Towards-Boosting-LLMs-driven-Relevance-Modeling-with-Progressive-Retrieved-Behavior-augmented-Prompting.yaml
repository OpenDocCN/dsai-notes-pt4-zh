- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:53'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented
    Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.09439](https://ar5iv.labs.arxiv.org/html/2408.09439)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zeyuan Chen Ant Group  [chenzeyuan.czy@antgroup.com](mailto:chenzeyuan.czy@antgroup.com)
    ,  Haiyan Wu Alibaba Group  [wuhaiyan.why@taobao.com](mailto:wuhaiyan.why@taobao.com)
    ,  Kaixin Wu Ant Group  [daniel.wkx@antgroup.com](mailto:daniel.wkx@antgroup.com)
    ,  Wei Chen Ant Group  [qianmu.cw@antgroup.com](mailto:qianmu.cw@antgroup.com)
    ,  Mingjie Zhong Ant Group  [mingjie.zmj@antgroup.com](mailto:mingjie.zmj@antgroup.com)
    ,  Jia Xu Ant Group  [steve.xuj@antgroup.com](mailto:steve.xuj@antgroup.com) , 
    Zhongyi Liu Ant Group  [zhongyi.lzy@antgroup.com](mailto:zhongyi.lzy@antgroup.com)
     and  Wei Zhang East China Normal University  [zhangwei.thu2011@gmail.com](mailto:zhangwei.thu2011@gmail.com)(2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Relevance modeling is a critical component for enhancing user experience in
    search engines, with the primary objective of identifying items that align with
    users’ queries. Traditional models only rely on the semantic congruence between
    queries and items to ascertain relevance. However, this approach represents merely
    one aspect of the relevance judgement, and is insufficient in isolation. Even
    powerful Large Language Models (LLMs) still cannot accurately judge the relevance
    of a query and an item from a semantic perspective. To augment LLMs-driven relevance
    modeling, this study proposes leveraging user interactions recorded in search
    logs to yield insights into users’ implicit search intentions. The challenge lies
    in the effective prompting of LLMs to capture dynamic search intentions, which
    poses several obstacles in real-world relevance scenarios, i.e., the absence of
    domain-specific knowledge, the inadequacy of an isolated prompt, and the prohibitive
    costs associated with deploying LLMs. In response, we propose $\mathit{ProRBP}$rompting
    framework for integrating search scenario-oriented knowledge with LLMs effectively.
    Specifically, we perform the user-driven behavior neighbors retrieval from the
    daily search logs to obtain domain-specific knowledge in time, retrieving candidates
    that users consider to meet their expectations. Then, we guide LLMs for relevance
    modeling by employing advanced prompting techniques that progressively improve
    the outputs of the LLMs, followed by a progressive aggregation with comprehensive
    consideration of diverse aspects. For online serving, we have developed an industrial
    application framework tailored for the deployment of LLMs in relevance modeling.
    Experiments on real-world industry data and online A/B testing demonstrate our
    proposal achieves promising performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'search engine, relevance modeling, large language models, user behavior^†^†copyright:
    acmlicensed^†^†journalyear: 2024^†^†doi: XXXXXXX.XXXXXXX^†^†conference: Proceedings
    of the 33rd ACM International Conference on Information and Knowledge Management;
    October 21–25, 2024; Boise, Idaho, USA^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Information systems Information retrieval^†^†ccs: Information systems Data mining'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In today’s landscape of excessive information, search engines have become critical
    for online content platforms, allowing users to swiftly find preferred content
    that match their search queries. To ensure a user-friendly experience, relevance
    modeling is crucial to preserve a satisfactory connection between a query and
    the displayed outcomes, forming a core element of search engine functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the relevant literature, foundational studies (Robertson et al., [2009](#bib.bib19);
    Shah and Pomerantz, [2010](#bib.bib21); Svore and Burges, [2009](#bib.bib24))
    have engaged in feature engineering to accomplish text matching, yet they lacked
    sufficient generalization and accuracy. Subsequently, deep learning-based approaches
    have risen as a new paradigm, with two primary categories: representation-based
    approaches (Shen et al., [2014](#bib.bib22); Palangi et al., [2014](#bib.bib11);
    Rao et al., [2019](#bib.bib17)) and interaction-based approaches (Parikh et al.,
    [2016](#bib.bib14); Chen et al., [2016](#bib.bib2); Hu et al., [2014](#bib.bib7);
    Pang et al., [2016](#bib.bib13)). Lately, pre-trained architectures such as BERT (Devlin
    et al., [2018](#bib.bib4)) have achieved significant progress in Natural Language
    Understanding (NLU) tasks. As a result, several studies (Yao et al., [2022](#bib.bib32);
    Lu et al., [2020](#bib.bib10); Reimers and Gurevych, [2019](#bib.bib18); Jin et al.,
    [2023](#bib.bib8)) are introduced that aim to capture the semantic relationships
    between queries and items. Most recently, Large Language Models (LLMs) have showcased
    their exceptional capabilities across a wide range of Natural Language Processing
    (NLP) applications. These models, such as GPT (Radford et al., [2019](#bib.bib15)),
    LLaMA (Touvron et al., [2023](#bib.bib26)), and GLM (Du et al., [2022](#bib.bib5)),
    are trained on massive corpora of texts, which enables them to maintain an exhaustive
    world knowledge. Nonetheless, identifying user search intentions accurately remains
    challenging when relying solely on semantic understanding, due to the absence
    of specialized domain knowledge required for complex industrial search scenarios.
    The texts of queries and items in Alipay search scenario are quite short and ambiguous,
    making it hard to convey effective information contained in their identity. For
    example, given a query “Zhe Yi”, the abbreviation of a hospital, it is hard to
    comprehend the actual semantics. But its historical clicked items include “the
    first affiliated hospital of Zhejiang University”, indicating strong correlations
    between them to help search intention identifying. As such, leveraging behavior
    data to assist relevance modeling is a natural strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing studies (Chen et al., [2023](#bib.bib3); Zeng et al., [2022](#bib.bib33);
    Zhu et al., [2021](#bib.bib35); Li et al., [2021](#bib.bib9); Pang et al., [2022](#bib.bib12))
    have primarily conducted the use of user behavior data. But they all consider
    constructing the pre-training dataset or the topology structure based on click
    behaviors without integrating semantics fully and effectively. Despite the success
    of LLMs, it’s still uncertain how well they can integrate world knowledge and
    specialized domain knowledge represented by user behavior data to master relevance
    modeling. To this end, this paper aims to investigate the potential of LLMs in
    relevance modeling with user search behavior. By utilizing strategic prompting
    techniques, specialized domain knowledge could be easily injected into LLMs for
    relevance modeling, whereas the performance varies due to the following unresolved
    issues: (i) The acquirement of domain-specific knowledge. Though domain-specific
    knowledge is vital in improving search scenario-oriented relevance modeling capabilities
    of LLMs, not all knowledge is beneficial. The noisy user behavior data may mislead
    LLMs to undesired judgements. Moreover, specialized domain knowledge of search
    scenarios undergoes rapid changes on a daily basis. The limited capacity of LLMs
    to adapt swiftly to these changes presents a significant obstacle to their ability
    to render accurate relevance judgments. (ii) The inadequacy of an isolated prompt.
    Despite LLMs could derive the relevance degree exploiting an isolated prompt,
    LLMs exhibit insensitivity to input, meaning they lack awareness of the aspects
    from which to infer relevance. In addition, the isolated prompts place greater
    demands on the quality of the prompts itself. Except for the aforementioned issues,
    deploying LLMs affordably in industrial scenarios is also a consideration worth
    addressing.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the above problems, we propose a novel $\underline{Pro}$). To acquire
    domain-specific knowledge in time, we perform a user-driven behavior neighbor
    retrieval from the daily updated search logs, retrieving candidates that users
    consider to meet their expectations currently. Then we anticipate employing advanced
    prompting techniques that progressively improve the outputs of the LLMs, followed
    by a progressive aggregation with comprehensive consideration of diverse aspects
    to form a holistic relevance model. As for the online serving of LLMs, we design
    an industrial implementation framework enabling LLMs to fully handle search relevance
    scenarios with the affordable cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we make the following contributions of this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, we are the first to successfully investigate the
    potential of LLMs with user behavior data to master relevance modeling.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose $\mathit{ProRBP}$ with two novel plug-in modules. Firstly, a user-driven
    behavior neighbors retrieval is developed to acquire domain-specific knowledge
    in time. Secondly, the proposal of progressive prompting and aggregation can strengthen
    the judgement of relevance for LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explore an industrial implementation enabling LLMs to fully handle search
    relevance scenarios in Alipay search with the affordable cost and latency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We demonstrate $\mathit{ProRBP}$ framework achieves superior performance through
    experiments on real-world industry data and online A/B testing. It has been deployed
    online and outperforms prior approaches (Chen et al., [2023](#bib.bib3)) on core
    metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Relevance Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The section concretely discusses the most recent advances in relevance modeling
    studies. Relevance modeling in search can be viewed as a text matching problem
    as the sub-domain of information retrieval (IR). The majority of work focuses
    on distinctions at the semantic level, while a minority of methods judge the relevance
    from a behavioral perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. Semantics-driven Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Current semantics-driven approaches can be classified into two aspects: feature-based
    approaches and deep learning-based approaches. The first category is centered
    on manual-crafted features such as TF-IDF similarity and BM25 (Svore and Burges,
    [2009](#bib.bib24)). Despite their usefulness, these feature-based approaches
    have limited generalization ability due to their domain-specific features and
    require significant labor resources.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to address the limitations of the above approaches, deep learning-based
    approaches emerge as the new paradigm, which can be broadly classified into representation-based
    approaches and interaction-based approaches. The former focuses on learning a
    low-dimensional representation of data while the latter emphasizes capturing the
    interaction between inputs. For instance, DSSM (Shen et al., [2014](#bib.bib22))
    is a classical two-tower representation-based model that encodes the query and
    the document separately. In this paradigm, recurrent (Palangi et al., [2014](#bib.bib11);
    Tai et al., [2015](#bib.bib25)) and convolutional (Hu et al., [2014](#bib.bib7);
    Shen et al., [2014](#bib.bib22)) networks are adopted to extract low-dimensional
    semantic representations. For these methods, the encoding of each input is carried
    out independently of the others. Consequently, these models face challenges in
    modeling complex relationships. To overcome this limitation, interaction-based
    models are proposed. DecompAtt (Parikh et al., [2016](#bib.bib14)) leverages attention
    network to align and aggregate representations. In parallel, recurrent (Chen et al.,
    [2016](#bib.bib2)) and convolutional (Hu et al., [2014](#bib.bib7); Pang et al.,
    [2016](#bib.bib13)) networks are employed for modeling complex interactions.
  prefs: []
  type: TYPE_NORMAL
- en: In recent times, pre-trained models like BERT (Devlin et al., [2018](#bib.bib4))
    have made remarkable strides in Natural Language Understanding (NLU). As a result,
    representation-based (Yao et al., [2022](#bib.bib32); Lu et al., [2020](#bib.bib10);
    Reimers and Gurevych, [2019](#bib.bib18); Jin et al., [2023](#bib.bib8)) and interaction-based
    architectures (Wang et al., [2019](#bib.bib28)) are proposed to leverage the capabilities
    of these models to encode semantic correlations between queries and items. Most
    recently, Large Languages Models (LLMs) like GPT (Radford et al., [2019](#bib.bib15)),
    LLaMA (Touvron et al., [2023](#bib.bib26)), BLOOM (Workshop et al., [2022](#bib.bib30))
    and GLM (Du et al., [2022](#bib.bib5)) trained on massive corpora of texts have
    shown their superior ability in language understanding, generation, interaction,
    and reasoning tasks.  (Sun et al., [2023](#bib.bib23)) investigates the potential
    of utilizing LLMs for searching and demonstrates that appropriately instructs
    ChatGPT and GPT-4 can produce competitive and even superior results to supervised
    methods widely used information retrieval benchmarks.  (Chen et al., [2023](#bib.bib3))
    tries to deal with long-tail query-item matching through LLMs efficiently and
    effectively. In these research work, they tend to exploit world knowledge stored
    in parameters of LLMs to judge the relevance between the query and item. However,
    general LLMs can not adapt to the industrial scenario due to the lack of domain-specific
    knowledge and insensitivity to the relevance judgement. In this work, we try to
    explore the LLMs-driven relevance modeling comprehensively in Alipay search engine
    to meet the above requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2\. Behavior-driven Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to textual information, there are a few related works that aim to
    integrate user behavior data into their models. The utilization of user behavior
    data can provide valuable insights into search intention, which can then be used
    to enhance the relevance of the search engines. MASM (Yao et al., [2021](#bib.bib31))
    leverages the historical behavior data to complete model pre-training as a weak-supervision
    signal with a newly proposed training objective.  (Zhu et al., [2021](#bib.bib35);
    Li et al., [2021](#bib.bib9); Pang et al., [2022](#bib.bib12)) try the incorporation
    of click graphs to enhance the effectiveness of search systems.  (Chen et al.,
    [2023](#bib.bib3)) endeavors to exploit behavior neighbors while considering interaction
    granularity and topology structure. However, no work has fully integrated LLMs
    with user behavior comprehensively in relevance modeling. We target to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Problem Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assume we have the target query $q$. The most naive prompt in the relevance
    modeling could be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\tau(q,i)=Is\,[q]\,and\,[i]\,related?\,[mask]\,,$ |  |'
  prefs: []
  type: TYPE_TB
- en: The relevance label $y_{qi}\in\{0,1\}$. Then the relevance degree could be given
    from LLMs for subsequent applications in Alipay search scenario. It worth noting
    that the above formulation is the basic explanation of relevance modeling using
    LLMs. We will further explore novel ways below based on this basic.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The architecture of the proposed $\mathit{ProRBP}$ framework is depicted in
    Figure [2](#S4.F2 "Figure 2 ‣ 4\. Methodology ‣ Towards Boosting LLMs-driven Relevance
    Modeling with Progressive Retrieved Behavior-augmented Prompting"). It contains
    two novel plug-in modules: (1) user-driven behavior neighbor retrieval for obtaining
    domain-specific knowledge in time to meet users’ expectation; (2) progressive
    prompting and aggregation for improving the sensitivity of LLMs to relevance judgement
    and stable prediction results. Through the two novel plug-in modules, relevance
    degree generation can be performed. In what follows, we elaborate on the two modules.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a7ff93e616d2f0c762c0e479a3b303c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The pipeline of user-driven behavior neighbor retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/32723d8181941901d044cd460f7692c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. The proposed framework $\mathit{ProRBP}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. User-driven Behavior Neighbor Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally, LLMs possess an exhaustive world knowledge with the benefit of massive
    corpora of texts pre-training. However, LLMs still struggle to understand user
    search intentions for short and ambiguous queries and items from Alipay search
    due to the lack of specialized and rapidly evolving domain knowledge. There are
    a few approaches could deal with the above problem, e.g., continual pre-training
    and retrieval-augmented generation. Considering efficiency and cost issues, the
    promising approach is retrieval-augmented language modeling (Gao et al., [2023](#bib.bib6)),
    grounding the LLMs during generation by conditioning on relevant candidates retrieved
    from an external knowledge source. Drawing inspiration from this, we devise a
    user-driven behavior neighbor retrieval module. This module could retrieve the
    daily search logs of users to obtain daily changing behavior neighbors that users
    consider relevant. The pipeline of user-driven behavior neighbor retrieval is
    depicted as Figure [1](#S4.F1 "Figure 1 ‣ 4\. Methodology ‣ Towards Boosting LLMs-driven
    Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limited number of items displayed for a query in Alipay search scenario,
    almost all results can be seen by users. With this in mind, we could analyze that
    a higher click-through rate reflects that users believe the corresponding query-item
    pairs can better meet their search intents and needs when the exposure PV (i.e.,
    page view) reaches a certain quantity. Thus, we filter out the query-item pairs
    with less than 100 exposure PV and formulate the remaining search logs as high-confidence
    logs. Then, we utilize the high-confidence logs from the past month to calculate
    the click-through rate for the exposed query-item pairs. To mitigate the effect
    of noises, query-item pairs are selected above a click-through threshold (e.g.,
    0.2) and the neighbors are arranged in descending order based on click-through
    rate from the query and item perspective, respectively. And we only choose top-K
    (e.g., 20) neighbors for corresponding query and item. Through this approach,
    it is possible to select dual behavior neighbors with high confidence, which in
    turn can assist LLMs in the process of in-context learning (Ram et al., [2023](#bib.bib16);
    Wang et al., [2023](#bib.bib27)). To ensure the timeliness of knowledge, we utilize
    the daily updated search logs to repeat the above operation every day and construct
    daily separate indexes for the dual behavioral neighbors of queries and items.
    The neighbors could be retrieved from indexes for the corresponding query $q$
    based on the above information. The daily prompt will be send to LLMs for further
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Progressive Prompting and Aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the fact that LLMs can determine a relevance score using an isolated
    prompt, they exhibit insensitivity to the input. It indicates that they lack the
    awareness required to decide the aspects from which to infer relevance. Furthermore,
    relying on isolated prompts imposes higher demands on the quality of the prompts
    itself, as the diversity of domain knowledge and the sensitivity to slight modifications
    of prompts may lead to unexpected results. As a result, it’s expected that using
    progressive prompting and aggregation process to gradually guide and consider
    diverse aspects will eventually lead to the creation of a unified model that assesses
    relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Progressive Prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is designed for making LLMs sensitive to the diverse aspects for the relevance
    judgement and improving the robustness of the model performance. Specifically,
    we firstly decompose the mentioned prompt above and construct least-to-most prompts
    step by step (Zhou et al., [2022](#bib.bib34); Wei et al., [2022](#bib.bib29))
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The prompts are documented as shown in Figure [2](#S4.F2 "Figure 2 ‣ 4\. Methodology
    ‣ Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented
    Prompting"). Besides, different prompting documents could obtain the consistent
    improvement in our local experiments exploiting this module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then LLMs could reason the likelihood of the verbalizer exploiting the least-to-most
    prompts step by step for the sensitivity to relevance judgement and stable prediction
    results, which could be dubbed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Simply, all least-to-most prompts share the same relevance label $y_{qi}$-th
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Progressive Aggregation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once we obtain the $L$-th prompt defined by us. The overall relevance score
    could be acquired through aggregating the probabilities from the least-to-most
    sub-tasks progressively as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\mathcal{P}(v&#124;\cdot)=\sum_{l=1}^{L}\mathcal{K}(\Delta_{l})\times\mathcal{P}(v&#124;\tau_{l})\,,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'This relevance score could be treated as the production of overall task, deserving
    to supervise mainly. The loss function is given by $\mathcal{L}_{main}$. The hybrid
    objective function can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\mathcal{L}=\mathcal{L}_{main}+\alpha\mathcal{L}_{auxi}\,,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ is a hyper-parameter to control the strength of the least-to-most
    sub-tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/187df4f5358b0d434ec67055c1029939.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. The illustration of online and offline collaborative service.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Industrial Implementation for LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real-world search scenarios (e.g., Alipay search), it is hard to deploy LLMs
    to handle all the search traffics with acceptable cost and latency. However, the
    judgment of relevance is objective and non-personalized. This implies that the
    relevance scores for the same query-item pair should remain consistent for all
    users, unlike recommendation algorithms that are personalized for each individual.
    As long as we can obtain the relevance scores for the query-item pairs, we can
    perform online services for all users, which greatly reduces the volume of online
    requests. And for the certain query or item, their semantic information is relatively
    stable and does not vary much. Inspired by this, we come to a efficient and effective
    solution (i.e, online and offline collaborative service) with the affordable cost
    and latency. Figure [3](#S4.F3 "Figure 3 ‣ 4.2.2\. Progressive Aggregation ‣ 4.2\.
    Progressive Prompting and Aggregation ‣ 4\. Methodology ‣ Towards Boosting LLMs-driven
    Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting") concretely
    illustrates online serving process of our proposed online and offline collaborative
    service when a certain query-item pair appears.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we utilize LLMs (e.g., 10B parameters) to perform offline inference
    on daily search logs exploiting the daily prompts and update relevance scores
    stored in the database. The cost of offline inference is minimal, so we can choose
    LLMs with a larger number of parameters to enhance the accuracy of relevance score
    calculations. Meanwhile, we would deploy distilled smaller LLMs (e.g., 2B parameters)
    for online serving. Since the method of distillation is not the focus of this
    paper, we have omitted the introduction of the method here. With this model, it
    is possible to make online predictions for query-item pairs that do not have stored
    scores in the database. When a user enter a query in Alipay search, the service
    will firstly look up the database from the offline service and obtain relevance
    scores based query and candidate items. Once the relevance scores of the query-item
    pair can not be obtained from the database, the online service will be requested
    for predicting the relevance score timely. Note that approximately 95% of the
    traffic will be handled by the offline service, while the remaining traffic will
    be handled by the online service. Through this method, we have achieved affordable
    efficiency and resource requirements for exploiting LLMs to handle relevance judgements
    of industrial scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we conduct extensive experiments to validate the effectiveness
    of our proposed $\mathit{ProRBP}$ framework by answering the following pivotal
    research questions:'
  prefs: []
  type: TYPE_NORMAL
- en: RQ1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are the results of $\mathit{ProRBP}$ framework compared with other competitive
    relevance modeling models?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RQ2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do the main components of $\mathit{ProRBP}$ framework affect its relevance
    modeling performance?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Besides, we investigate the effect of different hyper-parameter settings (e.g.,
    the number of behavior neighbors, the type of the kernel functions, and the strength
    of the least-to-most sub-tasks). Finally, we conduct online A/B testing to show
    the performance of the proposed method compared to previous deployed model.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.1.1\. Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to evaluate the performance of all the models with reliability, we
    select the real-world industry data used in mini apps search scenario of Alipay
    search engine and present its statistics in Table [1](#S6.T1 "Table 1 ‣ 6.1.2\.
    Baseline Models ‣ 6.1\. Experimental Setup ‣ 6\. Experiments ‣ Towards Boosting
    LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting").
    The dataset is labeled by human annotators, where Good and Bad annotations denote
    label 1 and 0 respectively. User historical behavior data is sampled from the
    search logs of the search engine. Although datasets such as WANDS¹¹1[https://github.com/wayfair/WANDS/tree/main](https://github.com/wayfair/WANDS/tree/main)
    and MSLR²²2[https://www.microsoft.com/en-us/research/project/mslr/](https://www.microsoft.com/en-us/research/project/mslr/)
    are publicly available, they do not contain the requisite user historical behavior
    data. Hence, we select this in-house data to evaluate the proposed framework.
    Other related work (Yao et al., [2022](#bib.bib32), [2021](#bib.bib31); Zhu et al.,
    [2021](#bib.bib35); Li et al., [2021](#bib.bib9); Chen et al., [2023](#bib.bib3))
    also selects one in-house data to evaluate the proposed approaches. The partial
    data of this paper was released before³³3[https://github.com/alipay/BehaviorAugmentedRelevanceModel](https://github.com/alipay/BehaviorAugmentedRelevanceModel).
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2\. Baseline Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We select a set of popular NLU (Natural Language Understanding)-based, behavior-based
    and NLG (Natural Language Generation)-based relevance models as the baselines.
    For the NLU-based models, we choose three common models including two-tower and
    single-tower architectures: DSSM (Shen et al., [2014](#bib.bib22)), ReprBert (Yao
    et al., [2022](#bib.bib32)), Bert (Devlin et al., [2018](#bib.bib4)). The behavior-based
    models all consider constructing the pre-training dataset (MASM (Yao et al., [2021](#bib.bib31)))
    or the topology structure based on click behaviors (TextGNN (Zhu et al., [2021](#bib.bib35)),
    AdsGNN (Li et al., [2021](#bib.bib9)), BARL-ASe (Chen et al., [2023](#bib.bib3))).
    For the NLG-based models, we choose two foundation models including two set of
    architectures: the causal decoder (BLOOM (Workshop et al., [2022](#bib.bib30)))
    and the prefix decoder (GLM (Du et al., [2022](#bib.bib5))).'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DSSM is a two-tower representation-based model. It encodes the embedding of
    a given query and item independently and computes the relevance score accordingly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReprBert is a representation-based Bert model that utilizes novel interaction
    strategies to achieve a balance between representation interactions and model
    latency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bert has achieved significant progress on NLP tasks as an interaction-based
    model. Here we concatenate the query and item as the input of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MASM leverages the historical behavior data to complete model pre-training as
    a weak-supervision signal with a newly proposed training objective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TextGNN extends the two-tower model with the complementary graph information
    from user historical behaviors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdsGNN further proposes three aggregation methods for the user behavior graph
    from different perspectives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BARL-ASe proposes dual behavior-neighbors augmented relevance model with self-supervised
    learning. And it exploits LLMs to deal with long-tailed query-item pairs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BLOOM is a language model trained on 46 natural languages and 13 programming
    languages with the causal decoder architecture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GLM is a language model with both powerful natural language understanding and
    generation capabilities based on the prefix decoder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Table 1\. Statistics of the human-annotated dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | # Sample | # Query | # Item | # Good | # Bad |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Train | 773,744 | 87,499 | 88,724 | 460,610 | 313,134 |'
  prefs: []
  type: TYPE_TB
- en: '| Valid | 97,032 | 40,754 | 25,192 | 57,914 | 39,118 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | 96,437 | 40,618 | 25,004 | 57,323 | 39,114 |'
  prefs: []
  type: TYPE_TB
- en: 6.1.3\. Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use Area Under Curve (AUC), F1-score (F1), and False Negative Rate (FNR)
    to measure the multidimensional performance of all models. AUC and F1 are commonly
    used in the studied area, of which higher metric values represent better model
    performance. Conversely, lower False Negative Rate (FNR) values are preferable,
    as they indicate a lower false filtering rate of models. Note that AUC often serves
    as the most significant metric in our task while the others provide auxiliary
    supports for our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4\. Model Implementations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We tune our model for 5 epochs with batch size 64 and learning rate 3e-05 in
    the supervised fine-tuning stage of LLMs. For the foundation models, we select
    the 1.1B BLOOM⁴⁴4https://huggingface.co/bigscience/bloom-1b1 and GLM with different
    magnitude of parameters (e.g., 0.3B, 2B and 10B) pre-trained by Alipay. The number
    of retrieved behavior neighbors is set to 20\. We tune the parameter $\alpha$
    within the ranges of {0, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0}. We conduct the experiments
    on NVIDIA Tesla A100 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Main results on real-world industry data. (-) denotes the lower value
    corresponds to better performance. Improvements over variants are statistically
    significant with p ¡ 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | AUC | F1 | FNR (-) |'
  prefs: []
  type: TYPE_TB
- en: '| DSSM | 0.8356 | 0.8210 | 0.1389 |'
  prefs: []
  type: TYPE_TB
- en: '| ReprBert | 0.8388 | 0.8376 | 0.1280 |'
  prefs: []
  type: TYPE_TB
- en: '| Bert | 0.8540 | 0.8534 | 0.1150 |'
  prefs: []
  type: TYPE_TB
- en: '| MASM | 0.8547 | 0.8318 | 0.1289 |'
  prefs: []
  type: TYPE_TB
- en: '| TextGNN | 0.8847 | 0.8489 | 0.1290 |'
  prefs: []
  type: TYPE_TB
- en: '| AdsGNN | 0.8878 | 0.8458 | 0.1454 |'
  prefs: []
  type: TYPE_TB
- en: '| BARL-ASe | 0.9078 | 0.8658 | 0.1054 |'
  prefs: []
  type: TYPE_TB
- en: '| GLM-0.3B | 0.8608 | 0.8585 | 0.1106 |'
  prefs: []
  type: TYPE_TB
- en: '|   +$\mathit{ProRBP}$ | 0.9105 | 0.8778 | 0.1035 |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM-1.1B | 0.8543 | 0.8511 | 0.1174 |'
  prefs: []
  type: TYPE_TB
- en: '|   +$\mathit{ProRBP}$ | 0.8991 | 0.8675 | 0.1121 |'
  prefs: []
  type: TYPE_TB
- en: '| GLM-2B | 0.8619 | 0.8598 | 0.1065 |'
  prefs: []
  type: TYPE_TB
- en: '|   +$\mathit{ProRBP}$ | 0.9120 | 0.8776 | 0.1007 |'
  prefs: []
  type: TYPE_TB
- en: '| GLM-10B | 0.8751 | 0.8656 | 0.1006 |'
  prefs: []
  type: TYPE_TB
- en: '|   +$\mathit{ProRBP}$ | 0.9143 | 0.8801 | 0.0973 |'
  prefs: []
  type: TYPE_TB
- en: 6.2\. Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.2.1\. Performance Comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table [2](#S6.T2 "Table 2 ‣ 6.1.4\. Model Implementations ‣ 6.1\. Experimental
    Setup ‣ 6\. Experiments ‣ Towards Boosting LLMs-driven Relevance Modeling with
    Progressive Retrieved Behavior-augmented Prompting") shows the overall comparison
    of our proposed framework $\mathit{ProRBP}$ with different baselines. Our findings
    indicate that DSSM performs poorly since it merely encodes the embeddings of the
    query and item independently as a basic two-tower model. By further comparing
    DSSM with ReprBert, we find the performance is improved to a certain degree. This
    demonstrates the pre-trained models utilizing the large corpus can bring additional
    gains. Compared to ReprBert, Bert achieves better performance, as the interaction-based
    models possess more advanced text relevance modeling abilities than representation-based
    models.
  prefs: []
  type: TYPE_NORMAL
- en: For the models of MASM, TextGNN, and AdsGNN, they exploit historical behavior
    data to build pre-training dataset or click graph to enhance the effectiveness
    of search systems. And they achieve significantly better results than the above-mentioned
    methods in the metric of AUC rather than F1 and FNR. This can be attributed to
    the introduction of auxiliary signals, which may bring improvement for the ranking
    ability of models but introduce some noises leading to the loss of F1 and FNR.
    The performance differences among them depend on the utilization of behavior data
    and modeling granularity. The newly proposed model BARL-ASe achieves the best
    performance in behavior-based relevance models, as it effectively leverages dual
    behavior neighbors for corresponding queries and items to enable the adaptable
    combination of fine-grained interactions and topology information.
  prefs: []
  type: TYPE_NORMAL
- en: GLM-0.3B, BLOOM-1.1B, GLM-2B and GLM-10B denote that we use the corresponding
    language models with the amount of parameters to perform relevance judgements
    exploiting the naive prompts as stated in Section [3](#S3 "3\. Problem Formulation
    ‣ Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented
    Prompting"). They exhibit relatively good performance, demonstrating the positive
    effect of massive training datasets and large-scale parameters. Performance variances
    among them with different parameters appear minimal, possibly because the simplicity
    of the relevance task doesn’t fully tap into the advantages of LLMs. Their performance
    has a certain gap compared to behavior-based models especially on the core metric
    AUC. This may be attributed to the lack of domain-specific knowledge and the limitations
    of an isolated prompt. Thus, $\mathit{ProRBP}$ framework is proposed to address
    the mentioned problem and significantly improve the foundation models’ performance.
    The performance differences between BLOOM and GLM may stem from differences in
    architecture and training corpora.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, exploiting our framework $\mathit{ProRBP}$ to proceed online prediction
    considering affordable cost and efficiency. Besides, they both obtain better gains
    under all the metrics on the evaluation dataset compared to the second-best performed
    model BARL-ASe that was deployed in our scenario before.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Ablation study of GLM-2B+$\mathit{ProRBP}$
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | AUC | F1 | FNR (-) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathit{ProRBP}$ | 0.9120 | 0.8776 | 0.1007 |'
  prefs: []
  type: TYPE_TB
- en: '|   -BNR | 0.8943 | 0.8656 | 0.1037 |'
  prefs: []
  type: TYPE_TB
- en: '|   -PPA | 0.8872 | 0.8645 | 0.1045 |'
  prefs: []
  type: TYPE_TB
- en: '|   -Both | 0.8619 | 0.8598 | 0.1065 |'
  prefs: []
  type: TYPE_TB
- en: 6.2.2\. Ablation Study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To investigate the contributions of key modules adopted by $\mathit{ProRBP}$
    framework, we provide the following variants of our complete framework:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “-BNR” denotes discarding user-driven behavior neighbor retrieval module.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “-PPA” denotes discarding progressive prompting and aggregation module.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “-Both” represents removing two modules mentioned above simultaneously.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Throughout the ablation study shown in Table [3](#S6.T3 "Table 3 ‣ 6.2.1\.
    Performance Comparison ‣ 6.2\. Experimental Results ‣ 6\. Experiments ‣ Towards
    Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented
    Prompting"), we observe that:'
  prefs: []
  type: TYPE_NORMAL
- en: $\diamond$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By analyzing the results of “-BNR” and “PPA”, we could conclude that both “BNR”
    and “PPA” modules yield significantly positive results. This suggests that the
    importance of the domain-specific knowledge and advanced prompting techniques
    in LLM-driven relevance modeling.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\diamond$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And “PPA” demonstrates greater significance than “BNR”. It is likely that the
    naive prompts with domain-specific knowledge can not release the potentials of
    LLMs totally for relevance modeling. It proves that the improvement of the sensitivity
    of LLMs to relevance judgement and prediction stability brings more gains than
    domain-specific knowledge in our task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\diamond$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of “-Both” demonstrates removing two modules could significantly
    degrade the model’s performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6.3\. Parameter Sensitivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e29ab7ea7f8d481ede93faed8071f2e2.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/de416f6765e05f860c36fd332ac7d9e2.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49795015d10c04af1b59e65280febe95.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. Result variation with different settings.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we further analyze the impact of some key hyper-parameter settings
    of our model, including the number of behavior neighbors, the type of the kernel
    functions, and the strength of the least-to-most sub-tasks. We report their results
    in terms of AUC for simplicity and the trend for the other metrics is similar.
    It should be noted that for each hyper-parameter, a tuning process is conducted
    while ensuring that the remaining hyper-parameters are set to their optimal values.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1\. Effect of the Number of Behavior Neighbors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Figure [4a](#S6.F4.sf1 "In Figure 4 ‣ 6.3\. Parameter Sensitivity ‣ 6\. Experiments
    ‣ Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented
    Prompting") presents the change of our method’s performance w.r.t. different numbers
    of behavior neighbors, which is searched in $\{1,2,3,5,7,10,15,20\}$. As expected,
    the results tend to be better when the number gets more. Such improvement might
    come from more domain-specific knowledge used for modeling. After reaching the
    peak, the performance remains stable.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2\. Effect of Different Kernel Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We investigate the contributions of different types of kernel functions used
    by $\mathit{ProRBP}$ framework as shown in Figure [4b](#S6.F4.sf2 "In Figure 4
    ‣ 6.3\. Parameter Sensitivity ‣ 6\. Experiments ‣ Towards Boosting LLMs-driven
    Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting").
    To realize this, we select different kernels and incorporate them into our framework
    to show the detailed performance. For ease of illustration, the Gaussian kernel,
    exponential kernel, logarithmic-decay kernel and mean pooling operation are abbreviated
    as norm, exp, log, and mean, respectively. As shown in Table [4b](#S6.F4.sf2 "In
    Figure 4 ‣ 6.3\. Parameter Sensitivity ‣ 6\. Experiments ‣ Towards Boosting LLMs-driven
    Relevance Modeling with Progressive Retrieved Behavior-augmented Prompting"),
    the exponential kernel achieves the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3\. Effect of the Strength of Sub-tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Figure [4c](#S6.F4.sf3 "In Figure 4 ‣ 6.3\. Parameter Sensitivity ‣ 6\. Experiments
    ‣ Towards Boosting LLMs-driven Relevance Modeling with Progressive Retrieved Behavior-augmented
    Prompting") depicts the performance variation w.r.t. the change of our method’s
    strength of the sub-tasks in $\{0,0.05,0.1,0.2,0.3,0.5,1.0\}$. We can see that
    there is a clear trend that when the weight is increased from 0, the results are
    continuously boost. Moreover, when the weight reaches a certain extent, the results
    tend to be stable or become even slightly worse. The reason is that increasing
    the strength of the sub-tasks too much might affect the study of the main task.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4\. Online A/B Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The proposed method has been deployed in the relevance stage of Alipay search
    platform providing search service of mini apps and demonstrated its significant
    performance gains in online A/B testing compared with the previous model BARL-ASe.
    The each experiment takes about 3% proportion of Alipay search traffic for two
    weeks. Compared with the previously deployed model, the proposed method improves
    the valid PV-CTR⁵⁵5the number of valid clicks divided by the number of searches
    by 0.33% on average without causing an increase in latency. And the results of
    human annotations show the model can reduce the rate of irrelevant results by
    1.07% points on average. The results demonstrate that our proposal can improve
    the experience of users.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper studies the relevance modeling problem by integrating world knowledge
    stored in the parameters of LLMs with specialized domain knowledge represented
    by user behavior data for achieving promising performance. The novel framework
    $\mathit{ProRBP}$ is proposed, which innovatively develops user-driven behavior
    neighbor retrieval module to learn domain-specific knowledge in time and introduces
    progressive prompting and aggregation module for considering diverse aspects of
    the relevance and prediction stability. We explore an industrial implementation
    (i.e., online and offline collaborative service) to deploy LLMs to handle full-scale
    search traffics of Alipay with acceptable cost and latency. The comprehensive
    experiments on real-world industry data and online A/B testing validate the superiority
    of our proposal and the effectiveness of its main modules.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2016) Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang,
    and Diana Inkpen. 2016. Enhanced LSTM for natural language inference. *arXiv preprint
    arXiv:1609.06038* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Zeyuan Chen, Wei Chen, Jia Xu, Zhongyi Liu, and Wei Zhang.
    2023. Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised
    Learning. In *Proceedings of the 32nd ACM International Conference on Information
    and Knowledge Management*. 4516–4522.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. GLM: General language model pretraining with
    autoregressive blank infilling. In *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*. 320–335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan,
    Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation
    for large language models: A survey. *arXiv preprint arXiv:2312.10997* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2014) Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014.
    Convolutional neural network architectures for matching natural language sentences.
    *Advances in neural information processing systems* 27 (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2023) Hanqi Jin, Jiwei Tan, Lixin Liu, Lisong Qiu, Shaowei Yao,
    Xi Chen, and Xiaoyi Zeng. 2023. MSRA: A Multi-Aspect Semantic Relevance Approach
    for E-Commerce via Multimodal Pre-Training. In *Proceedings of the 32nd ACM International
    Conference on Information and Knowledge Management*. 3988–3992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu,
    Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, and Qi Zhang. 2021. Adsgnn:
    Behavior-graph augmented relevance modeling in sponsored search. In *Proceedings
    of the 44th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 223–232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2020) Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twinbert: Distilling
    knowledge to twin-structured compressed bert models for large-scale retrieval.
    In *Proceedings of the 29th ACM International Conference on Information & Knowledge
    Management*. 2645–2652.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Palangi et al. (2014) Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong
    He, Jianshu Chen, Xinying Song, and R Ward. 2014. Semantic modelling with long-short-term
    memory for information retrieval. *arXiv preprint arXiv:1412.6629* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al. (2022) Bochen Pang, Chaozhuo Li, Yuming Liu, Jianxun Lian, Jianan
    Zhao, Hao Sun, Weiwei Deng, Xing Xie, and Qi Zhang. 2022. Improving Relevance
    Modeling via Heterogeneous Behavior Graph Learning in Bing Ads. In *Proceedings
    of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. 3713–3721.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al. (2016) Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan,
    and Xueqi Cheng. 2016. Text matching as image recognition. In *Proceedings of
    the AAAI Conference on Artificial Intelligence*, Vol. 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parikh et al. (2016) Ankur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob
    Uszkoreit. 2016. A decomposable attention model for natural language inference.
    *arXiv preprint arXiv:1606.01933* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog* 1, 8 (2019), 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented
    language models. *arXiv preprint arXiv:2302.00083* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rao et al. (2019) Jinfeng Rao, Linqing Liu, Yi Tay, Wei Yang, Peng Shi, and
    Jimmy Lin. 2019. Bridging the gap between relevance matching and semantic matching
    for short text similarity modeling. In *Proceedings of the 2019 Conference on
    Empirical Methods in Natural Language Processing and the 9th International Joint
    Conference on Natural Language Processing (EMNLP-IJCNLP)*. 5370–5381.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
    Sentence embeddings using siamese bert-networks. *arXiv preprint arXiv:1908.10084*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. 2009. The
    probabilistic relevance framework: BM25 and beyond. *Foundations and Trends® in
    Information Retrieval* 3, 4 (2009), 333–389.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schick and Schütze (2020) Timo Schick and Hinrich Schütze. 2020. Exploiting
    cloze questions for few shot text classification and natural language inference.
    *arXiv preprint arXiv:2001.07676* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shah and Pomerantz (2010) Chirag Shah and Jefferey Pomerantz. 2010. Evaluating
    and predicting answer quality in community QA. In *Proceedings of the 33rd international
    ACM SIGIR conference on Research and development in information retrieval*. 411–418.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2014) Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire
    Mesnil. 2014. Learning semantic representations using convolutional neural networks
    for web search. In *Proceedings of the 23rd international conference on world
    wide web*. 373–374.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin,
    and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language
    Models as Re-Ranking Agent. *arXiv preprint arXiv:2304.09542* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Svore and Burges (2009) Krysta M Svore and Christopher JC Burges. 2009. A machine
    learning approach for improved BM25 retrieval. In *Proceedings of the 18th ACM
    conference on Information and knowledge management*. 1811–1814.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tai et al. (2015) Kai Sheng Tai, Richard Socher, and Christopher D Manning.
    2015. Improved semantic representations from tree-structured long short-term memory
    networks. *arXiv preprint arXiv:1503.00075* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Jianing Wang, Chengyu Wang, Chuanqi Tan, Jun Huang, and Ming
    Gao. 2023. Boosting In-Context Learning with Factual Knowledge. *arXiv preprint
    arXiv:2309.14771* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan
    Xia, Liwei Peng, and Luo Si. 2019. Structbert: Incorporating language structures
    into pre-training for deep language understanding. *arXiv preprint arXiv:1908.04577*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems* 35 (2022), 24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workshop et al. (2022) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher
    Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha
    Luccioni, François Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2021) Shaowei Yao, Jiwei Tan, Xi Chen, Keping Yang, Rong Xiao, Hongbo
    Deng, and Xiaojun Wan. 2021. Learning a product relevance model from click-through
    data in e-commerce. In *Proceedings of the Web Conference 2021*. 2890–2899.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shaowei Yao, Jiwei Tan, Xi Chen, Juhao Zhang, Xiaoyi Zeng,
    and Keping Yang. 2022. ReprBERT: Distilling BERT to an Efficient Representation-Based
    Relevance Model for E-Commerce. In *Proceedings of the 28th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining*. 4363–4371.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2022) Zhiyuan Zeng, Yuzhi Huang, Tianshu Wu, Hongbo Deng, Jian
    Xu, and Bo Zheng. 2022. Graph-based Weakly Supervised Framework for Semantic Relevance
    Learning in E-commerce. In *Proceedings of the 31st ACM International Conference
    on Information & Knowledge Management*. 3634–3643.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    2022. Least-to-most prompting enables complex reasoning in large language models.
    *arXiv preprint arXiv:2205.10625* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2021) Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus
    Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, and Huasha Zhao. 2021. Textgnn:
    Improving text encoder via graph neural network in sponsored search. In *Proceedings
    of the Web Conference 2021*. 2848–2857.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
