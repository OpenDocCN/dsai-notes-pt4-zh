- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:41:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:41:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact
    of Prompt Engineering and Knowledge Retrieval'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在注册营养师考试中的准确性与一致性：提示工程和知识检索的影响
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02964](https://ar5iv.labs.arxiv.org/html/2408.02964)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02964](https://ar5iv.labs.arxiv.org/html/2408.02964)
- en: Iman Azimi^(1,*), Mohan Qi¹, Li Wang², Amir M. Rahmani³, and Youlin Li¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Iman Azimi^(1,*), Mohan Qi¹, Li Wang², Amir M. Rahmani³, 和 Youlin Li¹
- en: ¹Department of Engineering, iHealth Labs
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹工程系，iHealth Labs
- en: ²Department of Clinical Research, iHealth Labs
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²临床研究部门，iHealth Labs
- en: ³School of Nursing and Department of Computer Science, University of California,
    Irvine
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³加州大学欧文分校护理学院和计算机科学系
- en: ^*Corresponding author, iman.azimi@ihealthlabs.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^*通讯作者，iman.azimi@ihealthlabs.com
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) are fundamentally transforming human-facing applications
    in the health and well-being domains: boosting patient engagement, accelerating
    clinical decision-making, and facilitating medical education. Although state-of-the-art
    LLMs have shown superior performance in several conversational applications, evaluations
    within nutrition and diet applications are still insufficient. In this paper,
    we propose to employ the Registered Dietitian (RD) exam to conduct a standard
    and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet,
    and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries.
    Our evaluation includes 1050 RD exam questions encompassing several nutrition
    topics and proficiency levels. In addition, for the first time, we examine the
    impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency
    (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency
    of the responses. Our findings revealed that while these LLMs obtained acceptable
    overall performance, their results varied considerably with different prompts
    and question domains. GPT-4o with CoT-SC prompting outperformed the other approaches,
    whereas Gemini 1.5 Pro with ZS recorded the highest consistency. For GPT-4o and
    Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency.
    RAP was particularly effective for GPT-4o to answer Expert level questions. Consequently,
    choosing the appropriate LLM and prompting technique, tailored to the proficiency
    level and specific domain, can mitigate errors and potential risks in diet and
    nutrition chatbots.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）正在从根本上改变健康和福祉领域的面向人类应用：提升患者参与度，加速临床决策，促进医学教育。尽管最先进的LLMs在多个对话应用中表现优异，但在营养和饮食应用中的评估仍然不足。本文提出利用注册营养师（RD）考试对最先进的LLMs进行标准化和全面评估，评估GPT-4o、Claude
    3.5 Sonnet和Gemini 1.5 Pro在营养查询中的准确性和一致性。我们的评估包括1050道涵盖多个营养主题和水平的RD考试题目。此外，我们首次考察了零样本（ZS）、思维链（CoT）、带自我一致性的思维链（CoT-SC）和检索增强提示（RAP）对回答的准确性和一致性的影响。我们的发现揭示，尽管这些LLMs整体表现可接受，但它们的结果在不同提示和问题领域之间差异很大。使用CoT-SC提示的GPT-4o优于其他方法，而使用ZS的Gemini
    1.5 Pro记录了最高的一致性。对于GPT-4o和Claude 3.5，CoT提高了准确性，而CoT-SC提高了准确性和一致性。RAP对GPT-4o回答专家级别的问题特别有效。因此，选择适当的LLM和提示技术，根据能力水平和具体领域量身定制，可以减少饮食和营养聊天机器人中的错误和潜在风险。
- en: '^†^†journal:  \newcites'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†期刊： \newcites
- en: a
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: a
- en: Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: There is growing interest in leveraging conversational models, commonly known
    as chatbots, in healthcare, particularly in the areas of diet and nutrition [[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]. The rise of large language models (LLMs) is significantly
    transforming human-machine interactions in this context, creating new opportunities
    for nutrition management applications and lifestyle enhancement that involve natural
    language understanding and generation [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)].
    These chatbots can serve as assistants to health providers (e.g., dietitian or
    nurses) or as ubiquitous companions for patients, providing preventive care, personalized
    meal planning, and chronic disease management [[7](#bib.bib7)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健中，特别是在饮食和营养领域，利用对话模型（通常称为聊天机器人）的兴趣正在增加[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)]。大型语言模型（LLM）的崛起正显著改变人机互动的方式，为涉及自然语言理解和生成的营养管理应用和生活方式改善创造了新的机会[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)]。这些聊天机器人可以作为健康提供者（例如，营养师或护士）的助手，也可以作为患者的无处不在的伴侣，提供预防护理、个性化的餐饮计划和慢性疾病管理[[7](#bib.bib7)]。
- en: Since the release of ChatGPT [[8](#bib.bib8)] in November 2022, numerous nutrition
    management studies have developed or employed LLM-based chatbots to target different
    health conditions, such as type 2 diabetes, obesity, liver diseases, kidney diseases,
    and cardiovascular diseases, to mention a few [[1](#bib.bib1), [9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11), [7](#bib.bib7), [12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]. These studies highlight
    the potential of chatbots interventions to enhance diet and promote lifestyle
    behavior changes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自从**ChatGPT**于2022年11月发布以来，许多营养管理研究已开发或使用基于LLM的聊天机器人，以针对不同的健康状况，如2型糖尿病、肥胖、肝病、肾病和心血管疾病等[[1](#bib.bib1),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [7](#bib.bib7), [12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]。这些研究突出了聊天机器人干预提升饮食和促进生活方式行为改变的潜力。
- en: Due to the life-critical nature of these applications, they must provide high
    quality attributes, such as accuracy, consistency, safety, and fairness, before
    being deployed in real-world settings for end-users [[17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)]. Recent studies have evaluated the LLM-based chatbots within
    nutritional and dietary contexts. For example, Sun et al. [[20](#bib.bib20)] and
    Barlas et al. [[21](#bib.bib21)] assessed the performance of ChatGPT in providing
    nutritional management support for diabetic patients. Other investigations focused
    on chatbots’ reliability in delivering accurate calorie and macronutrient information
    [[22](#bib.bib22), [23](#bib.bib23)]. For non-communicable diseases, the accuracy
    of dietary advice generated by ChatGPT’s were assessed [[24](#bib.bib24), [10](#bib.bib10)].
    Other studies also examined ChatGPT’s ability to address common nutrition-related
    inquiries, highlighting its strength and weakness in offering personalized and
    accurate nutritional information [[25](#bib.bib25), [26](#bib.bib26)]. However,
    the existing evaluation studies on nutrition-related chatbots face three major
    challenges.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些应用的生命关键性质，它们必须在实际应用于最终用户之前提供高质量的属性，如准确性、一致性、安全性和公平性[[17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)]。最近的研究评估了基于LLM的聊天机器人在营养和饮食背景下的表现。例如，Sun等[[20](#bib.bib20)]和Barlas等[[21](#bib.bib21)]评估了**ChatGPT**在为糖尿病患者提供营养管理支持方面的表现。其他研究则关注于聊天机器人在提供准确的卡路里和宏量营养素信息方面的可靠性[[22](#bib.bib22),
    [23](#bib.bib23)]。对于非传染性疾病，**ChatGPT**生成的饮食建议的准确性也得到了评估[[24](#bib.bib24), [10](#bib.bib10)]。其他研究还考察了**ChatGPT**处理常见营养相关问题的能力，突出了其在提供个性化和准确营养信息方面的优势和不足[[25](#bib.bib25),
    [26](#bib.bib26)]。然而，现有的关于营养相关聊天机器人的评估研究面临三个主要挑战。
- en: First, prior research on the LLMs application in nutrition has relied solely
    on ad-hoc or subjective evaluations. In these studies, domain experts designed
    a set of questions focused on specific diseases or nutrition topics. Subsequently,
    human evaluators were instructed to grade the responses in terms of accuracy,
    comprehensiveness, or attractiveness [[27](#bib.bib27), [21](#bib.bib21), [20](#bib.bib20)].
    Human-in-the-loop evaluation is widely recognized as a popular and well-established
    strategy for assessing chatbots in the literature [[18](#bib.bib18), [19](#bib.bib19)].
    However, these evaluations are not comprehensive regarding nutrition problems
    and are prone to human errors or biases, as they depend on the opinion of an individual
    expert, especially when no standard guidelines are followed in the evaluation
    process. Additionally, they are time-consuming and costly. This limitation can
    be observed in the current nutrition chatbots evaluation, as their assessments
    are restricted to a few hundred interactions (i.e., prompts) at most.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，关于LLMs在营养中的应用的先前研究仅依赖于临时或主观的评估。在这些研究中，领域专家设计了一套问题，集中在特定疾病或营养主题上。随后，人类评估员被指示根据准确性、全面性或吸引力对回答进行评分[[27](#bib.bib27),
    [21](#bib.bib21), [20](#bib.bib20)]。人类在环评估被广泛认为是评估聊天机器人的一种流行且成熟的策略[[18](#bib.bib18),
    [19](#bib.bib19)]。然而，这些评估在营养问题方面不够全面，并且容易出现人为错误或偏见，因为它们依赖于个别专家的意见，尤其是在评估过程中没有遵循标准指导方针。此外，它们耗时且成本高。这一限制可以在当前的营养聊天机器人评估中观察到，因为它们的评估通常限制在最多几百次互动（即提示）之内。
- en: Second, most of the nutrition and diet studies have focused only on ChatGPT-3.5
    or ChatGPT-4\. The landscape of LLMs is rapidly evolving. New models and techniques
    are being released frequently, within weeks or months [[28](#bib.bib28)]. This
    rapid advancement requires the evaluation of a wide range of models to ensure
    the best possible solutions for diet and nutrition management applications. In
    addition, existing research on nutrition evaluation has ignored the impact of
    prompt engineering techniques. They have been limited to zero-shot prompting methods
    with either no instructions or fixed instructions. Prompt engineering is an important
    technique for enhancing the capabilities, adaptability, and applicability of LLMs
    [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，大多数营养和饮食研究仅关注了ChatGPT-3.5或ChatGPT-4。大语言模型（LLMs）的发展迅速，新模型和技术频繁发布，通常在几周或几个月内[[28](#bib.bib28)]。这种快速的进步要求评估广泛的模型，以确保饮食和营养管理应用的最佳解决方案。此外，现有的营养评估研究忽略了提示工程技术的影响。这些研究仅限于零-shot提示方法，没有说明或固定说明。提示工程是提升LLMs能力、适应性和适用性的一个重要技术[[29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)]。
- en: Third, previous work merely focused on the overall accuracy of LLMs responses.
    Their findings indicated that the models were generally accurate, but they still
    had errors [[10](#bib.bib10), [27](#bib.bib27), [21](#bib.bib21), [24](#bib.bib24)].
    These studies did not examine the errors, along with the strategies to enhance
    the LLMs’ responses. Wang et al. [[33](#bib.bib33)] highlights this issue in the
    context of clinical medicine. Moreover, the non-deterministic behavior of LLMs
    was ignored [[34](#bib.bib34)]. Within the healthcare and medical sectors, there
    is a strong demand for deterministic outcomes, ensuring that identical inputs
    generate identical outputs. The consistency and reliability of LLMs in answering
    nutrition-related questions must be evaluated to determine if their performance
    varies with identical or different prompts. In the nutrition context, to the best
    of our knowledge, only one study [[22](#bib.bib22)] has explored the consistency
    of ChatGPT-3.5 and ChatGPT-4 responses, using a zero-shot prompt for 222 food
    items across five repeated measurements.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，先前的工作仅关注LLMs回答的整体准确性。他们的发现表明，这些模型通常准确，但仍存在错误[[10](#bib.bib10), [27](#bib.bib27),
    [21](#bib.bib21), [24](#bib.bib24)]。这些研究没有检查错误及提高LLMs回答的策略。Wang等人[[33](#bib.bib33)]在临床医学的背景下强调了这一问题。此外，LLMs的非确定性行为被忽略了[[34](#bib.bib34)]。在医疗和医学领域，强烈需求确定性的结果，以确保相同的输入产生相同的输出。必须评估LLMs在回答营养相关问题时的一致性和可靠性，以确定它们的表现是否会因相同或不同的提示而有所变化。在营养方面，据我们所知，只有一项研究[[22](#bib.bib22)]探讨了ChatGPT-3.5和ChatGPT-4的回答一致性，该研究使用了零-shot提示，对222种食品进行五次重复测量。
- en: '![Refer to caption](img/815ad9c69f7fded1e357e922df97e44a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/815ad9c69f7fded1e357e922df97e44a.png)'
- en: 'Figure 1: Percentage Scores of the approaches on the RD exam. GPT-4o, Claude
    3.5 Sonnet, and Gemini 1.5 Pro are indicated with blue, orange, and green markers,
    respectively. The Zero Shot (ZS), Chain of Thought (CoT), Chain of Thought with
    Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) techniques
    are indicated with circle, square, triangle, and star markers, respectively.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同方法在 RD 考试中的百分比得分。GPT-4o、Claude 3.5 Sonnet 和 Gemini 1.5 Pro 分别用蓝色、橙色和绿色标记。零次试验（ZS）、思维链（CoT）、带自我一致性的思维链（CoT-SC）和检索增强提示（RAP）技术分别用圆形、方形、三角形和星形标记。
- en: 'In this paper, we thoroughly evaluate the accuracy and consistency of GPT-4o
    [[35](#bib.bib35)], Claude 3.5 Sonnet [[36](#bib.bib36)], and Gemini 1.5 Pro [[37](#bib.bib37)]
    in addressing nutrition-related inquiries. To achieve this, we leverage the Registered
    Dietitian (RD) exam [[38](#bib.bib38)] for the first time, as a standard certification
    examination that serves to assess whether dietitians meet the qualifications required
    to practice in the dietetics and nutrition field. Our evaluation includes 1050
    multiple-choice questions with different proficiency levels, covering four nutrition
    domains: i.e., principles of dietetics, nutrition care, food service systems,
    and food and nutrition management. To investigate the impact of prompts, the questions
    are presented to the LLMs using four different prompting techniques: 1) Zero Shot
    prompting (ZS), 2) Chain of Thought (CoT), 3) Chain of Thought with Self Consistency
    (CoT-SC), and 4) Retrieval Augmented Prompting (RAP) enabled by external nutrition
    knowledge. We then compare the responses with the ground truth answers, enabling
    an objective assessment of the model’s performance. To examine the consistency
    of the responses, we perform repeated measurements by asking each model the same
    set of questions multiple times using each prompting technique. The responses
    for each technique and model are compared within and across groups.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文彻底评估了 GPT-4o [[35](#bib.bib35)]、Claude 3.5 Sonnet [[36](#bib.bib36)] 和 Gemini
    1.5 Pro [[37](#bib.bib37)] 在处理营养相关询问时的准确性和一致性。为此，我们首次利用注册营养师（RD）考试 [[38](#bib.bib38)]
    作为标准认证考试，以评估营养师是否符合营养学和饮食学领域的实践资格要求。我们的评估包括 1050 道不同水平的选择题，涵盖四个营养领域：即营养学原则、营养护理、食品服务系统和食品与营养管理。为了研究提示的影响，问题使用四种不同的提示技术呈现给
    LLMs：1）零次提示（ZS）、2）思维链（CoT）、3）带自我一致性的思维链（CoT-SC）和 4）通过外部营养知识增强的检索提示（RAP）。然后我们将回答与真实答案进行比较，从而客观评估模型的表现。为了检查回答的一致性，我们通过多次向每个模型提出相同问题来进行重复测量。每种技术和模型的响应在组内和组间进行比较。
- en: 'Table 1: The percentage scores (mean and standard deviation) of the LLMs’ responses
    on the RD exam questions.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：LLMs 在 RD 考试问题上的百分比得分（均值和标准差）。
- en: '| Benchmark | Prompt | GPT-4o | Claude 3.5 S. | Gemini 1.5 P. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 提示 | GPT-4o | Claude 3.5 S. | Gemini 1.5 P. |'
- en: '| RD Exam | Zero Shot | 91.92% (0.28) | 90.04% (0.10) | 90.78% (0.11) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| RD 考试 | 零次试验 | 91.92% (0.28) | 90.04% (0.10) | 90.78% (0.11) |'
- en: '|  | Chain of Thought | 94.32% (0.18) | 92.32% (0.27) | 88.82% (0.63) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | 思维链 | 94.32% (0.18) | 92.32% (0.27) | 88.82% (0.63) |'
- en: '|  | Chain of Thought w. Self Consistency | 94.48% (0.22) | 92.67% (0.16) |
    90.02% (0.39) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | 思维链与自我一致性 | 94.48% (0.22) | 92.67% (0.16) | 90.02% (0.39) |'
- en: '|  | Retrieval Augmented Prompting | 92.78% (0.27) | 89.22% (0.18) | 89.66%
    (0.11) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | 检索增强提示 | 92.78% (0.27) | 89.22% (0.18) | 89.66% (0.11) |'
- en: 'Table 2: The performance of the LLMs on the MMLU [[39](#bib.bib39)], GPQA [[40](#bib.bib40)],
    and DROP [[41](#bib.bib41)] benchmarks, collected from [[36](#bib.bib36), [35](#bib.bib35),
    [42](#bib.bib42)].'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：LLMs 在 MMLU [[39](#bib.bib39)]、GPQA [[40](#bib.bib40)] 和 DROP [[41](#bib.bib41)]
    基准测试中的表现，数据收集自 [[36](#bib.bib36)、[35](#bib.bib35)、[42](#bib.bib42)]。
- en: '| Benchmark | Prompt | GPT-4o | Claude 3.5 S. | Gemini 1.5 P. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 提示 | GPT-4o | Claude 3.5 S. | Gemini 1.5 P. |'
- en: '| MMLU (Undergraduate Level Knowledge) | Zero Shot | 88.70% | 88.30% | - |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| MMLU（本科水平知识） | 零次试验 | 88.70% | 88.30% | - |'
- en: '|  | Five Shot | - | 88.70% | 85.90% |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | 五次试验 | - | 88.70% | 85.90% |'
- en: '| GPQA (Graduate Level Reasoning) | Chain of Thought | 53.60% | 59.40% | 46.20%
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| GPQA（研究生水平推理） | 思维链 | 53.60% | 59.40% | 46.20% |'
- en: '| DROP (Reasoning) | Three Shot | 83.40% | 87.10% | 74.90% |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| DROP（推理） | 三次试验 | 83.40% | 87.10% | 74.90% |'
- en: Results
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Accuracy
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确性
- en: Overall Performance
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总体表现
- en: 'The results show that all the approaches obtained a score of over 88% in selecting
    the correct option for the 1050 RD exam questions, as indicated in Figure [1](#Sx1.F1
    "Figure 1 ‣ Introduction ‣ Accuracy and Consistency of LLMs in the Registered
    Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval") and
    Table [1](#Sx1.F1 "Figure 1 ‣ Introduction ‣ Accuracy and Consistency of LLMs
    in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge
    Retrieval"). Overall, GPT-4o achieved the highest score (the blue markers in the
    figure) ranging between 91% and 95%, with the best score for CoT-SC. On the other
    hand, Gemini 1.5 Pro (the green markers) had the lowest scores.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '结果显示，所有方法在 1050 道 RD 考试问题中选择正确选项的得分均超过 88%，如图 [1](#Sx1.F1 "Figure 1 ‣ Introduction
    ‣ Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact
    of Prompt Engineering and Knowledge Retrieval") 和表格 [1](#Sx1.F1 "Figure 1 ‣ Introduction
    ‣ Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact
    of Prompt Engineering and Knowledge Retrieval") 所示。总体而言，GPT-4o 获得了最高的得分（图中的蓝色标记），得分范围为
    91% 到 95%，其中 CoT-SC 取得了最佳得分。另一方面，Gemini 1.5 Pro（绿色标记）的得分最低。'
- en: In both GPT-4o and Claude 3.5 Sonnet, the CoT and CoT-SC prompting techniques
    resulted in similar percentage scores, which were approximately 2.5 percent higher
    than the ZS prompting’s scores. However, the combination of Gemini with CoT or
    CoT-SC did not improve the accuracy but produced wider percentage scores across
    repeated measurements, with ranges of 1.9 and 1.2\. Moreover, RAP obtained better
    scores, compared to ZS, in GPT-4o but slightly decreased the performance of Claude
    and Gemini models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPT-4o 和 Claude 3.5 Sonnet 中，CoT 和 CoT-SC 提示技术的效果相似，得分大约比 ZS 提示高出 2.5%。然而，将
    Gemini 与 CoT 或 CoT-SC 结合使用并没有提高准确率，反而在重复测量中产生了更广泛的百分比得分，范围为 1.9 和 1.2。值得注意的是，RAP
    在 GPT-4o 中取得了比 ZS 更好的得分，但略微降低了 Claude 和 Gemini 模型的表现。
- en: 'In addition to our findings, an overview of the models’ performance on existing
    knowledge and reasoning benchmarks are indicated in Table [2](#Sx1.T2 "Table 2
    ‣ Introduction ‣ Accuracy and Consistency of LLMs in the Registered Dietitian
    Exam: The Impact of Prompt Engineering and Knowledge Retrieval"). The performance
    scores of these three benchmarks were collected from [[36](#bib.bib36), [35](#bib.bib35),
    [42](#bib.bib42)]. The GPQA benchmark [[40](#bib.bib40)] includes 448 multiple-choice
    questions on biology, physics, and chemistry. The MMLU benchmark [[39](#bib.bib39)]
    contains multiple-choice questions from 57 topics, such as elementary mathematics,
    US history, computer science, and law; and the DROP benchmark [[41](#bib.bib41)]
    consists of 96,567 questions focusing on discrete reasoning over the content of
    paragraphs, including addition, counting, and sorting. Claude 3.5 Sonnet outperformed
    the other LLMs in all scenarios, except for MMLU using the ZS prompting. These
    findings do not fully align with our findings presented in Table [1](#Sx1.T1 "Table
    1 ‣ Introduction ‣ Accuracy and Consistency of LLMs in the Registered Dietitian
    Exam: The Impact of Prompt Engineering and Knowledge Retrieval").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '除了我们的发现外，表格 [2](#Sx1.T2 "Table 2 ‣ Introduction ‣ Accuracy and Consistency
    of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and
    Knowledge Retrieval") 中列出了模型在现有知识和推理基准上的表现概况。这三个基准的表现分数分别来自 [[36](#bib.bib36),
    [35](#bib.bib35), [42](#bib.bib42)]。GPQA 基准 [[40](#bib.bib40)] 包含 448 道关于生物学、物理学和化学的多项选择题。MMLU
    基准 [[39](#bib.bib39)] 包含 57 个主题的多项选择题，如基础数学、美国历史、计算机科学和法律；DROP 基准 [[41](#bib.bib41)]
    包含 96,567 道题目，专注于对段落内容的离散推理，包括加法、计数和排序。除了在使用 ZS 提示的 MMLU 中，Claude 3.5 Sonnet 在所有场景中都超越了其他
    LLM。这些发现与表格 [1](#Sx1.T1 "Table 1 ‣ Introduction ‣ Accuracy and Consistency of
    LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge
    Retrieval") 中展示的结果不完全一致。'
- en: '![Refer to caption](img/e00db67d15c00e135ce981282839bb51.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e00db67d15c00e135ce981282839bb51.png)'
- en: (a) Average errors per approach by proficiency level. The exam includes 149
    Easy, 352 Moderate, 392 Difficult, and 157 Expert levels questions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 每种方法在各熟练程度的平均错误。考试包括 149 道简单题、352 道中等题、392 道困难题和 157 道专家级题目。
- en: '![Refer to caption](img/a156849f674d58cac25c0035a2812e5b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a156849f674d58cac25c0035a2812e5b.png)'
- en: (b) Average errors per approach by domain. The exam includes 237 principles
    of dietetics, 392 nutrition care for individuals and groups, 185 food service
    systems, and 236 management of food and nutrition programs and services questions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 每种方法在各领域的平均错误。考试内容包括 237 个饮食学原则、392 个个体和团体的营养护理、185 个食品服务系统以及 236 个食品和营养计划与服务的管理问题。
- en: 'Figure 2: The LLMs’ inaccurate responses based on the RD exam questions’ proficiency
    levels and domains.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLMs在RD考试问题的熟练度等级和领域下的不准确回答。
- en: Subgroup Error Analysis
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 子组错误分析
- en: We categorize the RD exam questions into different subgroups, within which the
    LLMs’ inaccurate responses are assessed. To achieve this, we analyze the errors
    obtained in terms of proficiency levels and four nutrition domains (i.e., topics).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将RD考试问题分为不同的子组，在这些子组内评估LLMs的不准确回答。为此，我们分析了根据熟练度等级和四个营养领域（即主题）获得的错误。
- en: 'Proficiency Levels: The approaches are evaluated based on the questions’ proficiency
    levels, provided by the Academy of Nutrition and Dietetics, eatrightPREP for the
    RDN Exam [[43](#bib.bib43)]. The exam consists of 149 Easy, 352 Moderate, 392
    Difficult, and 149 Expert levels questions. Figure [2(a)](#Sx2.F2.sf1 "In Figure
    2 ‣ Overall Performance ‣ Accuracy ‣ Results ‣ Accuracy and Consistency of LLMs
    in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge
    Retrieval") shows the average errors for each approach.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 熟练度等级：这些方法根据由营养与饮食学学会提供的RDN考试问题的熟练度等级进行评估[[43](#bib.bib43)]。该考试包含149个Easy、352个Moderate、392个Difficult和149个Expert等级的问题。图[2(a)](#Sx2.F2.sf1
    "图2 ‣ 整体表现 ‣ 准确性 ‣ 结果 ‣ 注册营养师考试中LLMs的准确性和一致性：提示工程和知识检索的影响")显示了每种方法的平均错误。
- en: GPT-4o obtained the lowest overall average error counts. The model with CoT-SC
    resulted in the fewest errors across the proficiency levels, with the average
    errors of 0.6, 10.6, 22.4, and 24.4 for Easy, Moderate, Difficult, and Expert
    levels questions, respectively. Compared to ZS prompting, CoT and CoT-SC improved
    the model’s performance at all levels, but RAP only enhanced the responses of
    the Difficult and Expert level questions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o获得了最低的总体平均错误数量。使用CoT-SC的模型在各熟练度等级下错误最少，Easy、Moderate、Difficult和Expert等级的问题的平均错误分别为0.6、10.6、22.4和24.4。与ZS提示相比，CoT和CoT-SC在所有级别上改善了模型的表现，但RAP仅在Difficult和Expert级别的问题上有所提升。
- en: Similar to the GPT-4o approaches, Claude 3.5 Sonnet performance was enhanced
    by CoT and CoT-SC. Claude 3.5 Sonnet with CoT and CoT-SC achieved similar average
    error rates. Conversely, using Claude 3.5 Sonnet, RAP recorded the highest error
    counts, particularly with 5 more errors (on average) for Expert questions, compared
    to the ZS prompting technique.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GPT-4o的方法，Claude 3.5 Sonnet的性能通过CoT和CoT-SC得到了提升。Claude 3.5 Sonnet在使用CoT和CoT-SC后，达到了相似的平均错误率。相反，使用Claude
    3.5 Sonnet时，RAP记录了最高的错误数量，尤其是在Expert问题上，比ZS提示技术多出5个错误（平均）。
- en: Gemini 1.5 Pro had the highest number of errors overall. The ZS prompting recorded
    the lowest average errors with Gemini. Compared to ZS, CoT and CoT-SC improved
    the responses of the Moderate questions but obtained higher average errors for
    the Difficult and Expert level questions. RAP obtained higher error rates for
    Moderate and Difficult questions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini 1.5 Pro的总体错误数量最高。ZS提示记录了Gemini的最低平均错误。与ZS相比，CoT和CoT-SC改善了Moderate问题的回答，但在Difficult和Expert级别的问题上获得了更高的平均错误。RAP在Moderate和Difficult问题上获得了更高的错误率。
- en: '![Refer to caption](img/4a06ae3a6d46437f5b4b0b5003bb5b35.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4a06ae3a6d46437f5b4b0b5003bb5b35.png)'
- en: 'Figure 3: The Cohen’s Kappa coefficients measured for each of the 12 pairwise
    comparisons using the RD exam. The dark blue indicates high levels of agreement,
    while the light blue represents lower agreement levels.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于RD考试对每一对12个比较测量的Cohen's Kappa系数。深蓝色表示高一致性，而浅蓝色表示较低的一致性。
- en: 'Domains: The inaccurate responses collected by each approach is evaluated based
    on four domains: D1) Principles of Dietetics, D2) Nutrition Care for Individuals
    and Groups, D3) Food Service System, and D4) Management of Food and Nutrition
    Programs and Service. The exam consists of 237, 392, 185, and 236 questions for
    D1, D2, D3, and D4, respectively. As illustrated in Figure [2(b)](#Sx2.F2.sf2
    "In Figure 2 ‣ Overall Performance ‣ Accuracy ‣ Results ‣ Accuracy and Consistency
    of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and
    Knowledge Retrieval"), the impact of prompt engineering techniques varied across
    the domains for the three LLMs.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 领域：每种方法收集的不准确响应根据四个领域进行评估：D1) 营养学原则，D2) 个体和群体营养护理，D3) 食品服务系统，以及 D4) 食品和营养项目及服务管理。考试包括
    D1、D2、D3 和 D4 的 237、392、185 和 236 道问题。如图 [2(b)](#Sx2.F2.sf2 "图 2 ‣ 总体表现 ‣ 准确性
    ‣ 结果 ‣ 注册营养师考试中 LLM 的准确性和一致性：提示工程和知识检索的影响") 所示，三种 LLM 的提示工程技术对各领域的影响有所不同。
- en: GPT-4o with CoT-SC reduced the average error counts in D3 from 27.4 to 12 and
    in D4 from 28.4 to 18.2, compared to GPT-4o with ZS. CoT and RAP also showed similar
    improvements in error rates although RAP recorded more errors for D2\. Using GPT-4o,
    different prompting techniques resulted in small changes in the error rates observed
    in D1.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GPT-4o 的 ZS 相比，GPT-4o 使用 CoT-SC 将 D3 中的平均错误数从 27.4 减少到 12，将 D4 中的错误数从 28.4
    减少到 18.2。CoT 和 RAP 也显示出类似的错误率改进，尽管 RAP 在 D2 中记录了更多的错误。使用 GPT-4o 时，不同的提示技术对 D1
    的错误率变化较小。
- en: Claude 3.5 Sonnet showed that transitioning from ZS prompting to CoT-SC or CoT
    reduced the average errors across the four domains. On the other hand, RAP slightly
    improved D1 and D2 but obtained more errors in D3 and D4.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 3.5 Sonnet 显示，从 ZS 提示转变为 CoT-SC 或 CoT 可以减少四个领域中的平均错误数。另一方面，RAP 对 D1 和
    D2 的表现稍有改善，但在 D3 和 D4 中出现了更多的错误。
- en: With Gemini 1.5 Pro, different prompts led to small variations in error counts,
    with changes of fewer than 4 errors on average in D1, D3, and D4\. However, ZS
    prompting obtained the lowest error count in D2, with an average of 26.2 errors.
    Nevertheless, this outcome shows approximately 6 errors higher than the performance
    achieved by GPT-4o. In D2, Gemini and CoT obtained the highest error rates.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Gemini 1.5 Pro 时，不同的提示导致错误计数的变化较小，D1、D3 和 D4 的平均错误数变化不到 4 个。然而，ZS 提示在 D2
    中获得了最低的错误计数，平均为 26.2 个错误。尽管如此，这一结果显示比 GPT-4o 的表现高出大约 6 个错误。在 D2 中，Gemini 和 CoT
    的错误率最高。
- en: Consistency
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一致性
- en: Inter-rater Analysis
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评审员间分析
- en: 'The inter-rater reliability of the responses from the approaches was analyzed
    to investigate their agreement. To achieve this, Cohen’s Kappa coefficient was
    calculated for each pair of approaches to determine if they selected the same
    choices, whether accurate or inaccurate. Our study includes 12 distinct approaches
    (3 LLMs multiplied by 4 prompting techniques), so Cohen’s Kappa was measured for
    each of the 12 pairwise comparisons. Since each approach is repeated five times,
    one set of measurements per approach is randomly selected to assess the inter-rater
    reliability. Figure [3](#Sx2.F3 "Figure 3 ‣ Subgroup Error Analysis ‣ Accuracy
    ‣ Results ‣ Accuracy and Consistency of LLMs in the Registered Dietitian Exam:
    The Impact of Prompt Engineering and Knowledge Retrieval") presents the Cohen’s
    Kappa coefficients, where dark blue indicates high levels of agreement, and light
    blue represents lower agreement levels. Additionally, the detailed statistical
    data are presented in Supplementary Table S.1.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查方法之间的一致性，对响应的评审员间可靠性进行了分析。为此，计算了每对方法的 Cohen’s Kappa 系数，以确定它们是否选择了相同的选项，无论是准确的还是不准确的。我们的研究包括
    12 种不同的方法（3 种 LLM 乘以 4 种提示技术），因此 Cohen’s Kappa 为 12 对方法间比较中的每一对进行了测量。由于每种方法重复五次，因此随机选择每种方法的一组测量数据来评估评审员间可靠性。图
    [3](#Sx2.F3 "图 3 ‣ 子组错误分析 ‣ 准确性 ‣ 结果 ‣ 注册营养师考试中 LLM 的准确性和一致性：提示工程和知识检索的影响") 展示了
    Cohen’s Kappa 系数，其中深蓝色表示高水平的一致性，浅蓝色表示较低的一致性。此外，详细的统计数据在补充表 S.1 中呈现。
- en: The approaches based on GPT-4o showed a high degree of agreement, indicated
    by a Cohen’s Kappa coefficient of 0.98 between CoT and CoT-SC and a coefficient
    of 0.93 between RAP and the other three prompting techniques. This confirms that
    altering these prompting techniques did not result in a substantial change in
    the GPT-4o’s behavior. Similarly, Claude 3.5-based approaches indicated comparable
    levels of agreement. In contrast, the Gemini 1.5 Pro’s approaches recorded relatively
    lower Cohen’s Kappa coefficients, despite maintaining high overall agreement.
    The Cohen’s Kappa coefficients of the Gemini-based approaches were from 0.84 to
    0.93\. The agreement level between CoT and CoT-SC was 0.92, and the agreement
    between ZS and RAP was 0.93\. Interestingly, among the prompting techniques, the
    approaches (even with different LLMs) using CoT and CoT-SC obtained higher levels
    of agreement.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GPT-4o 的方法显示了高程度的一致性，CoT 和 CoT-SC 之间的 Cohen’s Kappa 系数为 0.98，而 RAP 与其他三种提示技术之间的系数为
    0.93。这确认了改变这些提示技术并未对 GPT-4o 的行为产生实质性变化。同样，Claude 3.5 基于的方法显示了相当的相似一致性。相比之下，Gemini
    1.5 Pro 的方法记录了相对较低的 Cohen’s Kappa 系数，尽管总体一致性较高。Gemini 基于的方法的 Cohen’s Kappa 系数从
    0.84 到 0.93。CoT 和 CoT-SC 之间的一致性为 0.92，ZS 和 RAP 之间的一致性为 0.93。有趣的是，在提示技术中，使用 CoT
    和 CoT-SC 的方法（即使使用不同的 LLM）获得了更高的一致性水平。
- en: Intra-rater Analysis
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评分者内分析
- en: 'In this study, each approach was repeated five times, resulting in five sets
    of responses. The intra-rater reliability of the responses was evaluated by measuring
    the repeatability of the approaches, determining how consistently they agreed
    with themselves when receiving the same questions. For this purpose, Fleiss Kappa
    was employed to assess the intra-rater agreements. Table [3](#Sx2.T3 "Table 3
    ‣ Intra-rater Analysis ‣ Consistency ‣ Results ‣ Accuracy and Consistency of LLMs
    in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge
    Retrieval") indicates the Fleiss Kappa coefficients, and Supplementary Table S.2
    includes the detailed statistical data.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '在本研究中，每种方法重复进行了五次，得出了五组响应。通过测量方法的重复性来评估响应的一致性，确定它们在接收相同问题时的自我一致性。为此，使用了 Fleiss
    Kappa 来评估评分者内的一致性。表 [3](#Sx2.T3 "Table 3 ‣ Intra-rater Analysis ‣ Consistency
    ‣ Results ‣ Accuracy and Consistency of LLMs in the Registered Dietitian Exam:
    The Impact of Prompt Engineering and Knowledge Retrieval") 显示了 Fleiss Kappa 系数，附录表
    S.2 包含了详细的统计数据。'
- en: Gemini 1.5 Pro combined with the ZS prompting achieved the highest agreement
    among all combinations, whereas the Gemini with CoT produced the lowest agreement.
    The approaches based on Claude 3.5 Sonnet demonstrated the highest overall agreement.
    For the three LLMs, the ZS prompting technique consistently resulted in the highest
    agreement, as indicated by Fleiss’s Kappa coefficients of 0.996, 0.987, and 0.980
    for Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o, respectively. Similarly, the
    coefficients of the LLMs with RAP were high. The CoT-SC recorded the third highest
    agreement, while the CoT obtained the lowest.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini 1.5 Pro 结合 ZS 提示在所有组合中实现了最高的一致性，而 Gemini 与 CoT 的组合则表现出最低的一致性。基于 Claude
    3.5 Sonnet 的方法表现出了最高的整体一致性。对于三种 LLM，ZS 提示技术始终导致了最高的一致性，Fleiss 的 Kappa 系数分别为 Gemini
    1.5 Pro、Claude 3.5 Sonnet 和 GPT-4o 的 0.996、0.987 和 0.980。同样，使用 RAP 的 LLM 的系数也很高。CoT-SC
    记录了第三高的一致性，而 CoT 取得了最低的一致性。
- en: 'Table 3: The Fleiss Kappa coefficients of the 12 approaches. Each approach
    was repeated 5 times.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：12 种方法的 Fleiss Kappa 系数。每种方法重复进行了 5 次。
- en: '|  |  | Fleiss’ Kappa | 95% CI |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Fleiss’ Kappa | 95% CI |'
- en: '| GPT-4o | ZS | 0.980 | 0.973 – 0.987 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | ZS | 0.980 | 0.973 – 0.987 |'
- en: '| CoT | 0.969 | 0.960 – 0.977 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.969 | 0.960 – 0.977 |'
- en: '| CoT-SC | 0.977 | 0.970 – 0.985 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| CoT-SC | 0.977 | 0.970 – 0.985 |'
- en: '| RAP | 0.985 | 0.978 – 0.991 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| RAP | 0.985 | 0.978 – 0.991 |'
- en: '| Claude 3.5 S. | ZS | 0.987 | 0.981 – 0.992 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3.5 S. | ZS | 0.987 | 0.981 – 0.992 |'
- en: '| CoT | 0.975 | 0.967 – 0.983 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.975 | 0.967 – 0.983 |'
- en: '| CoT-SC | 0.982 | 0.975 – 0.988 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| CoT-SC | 0.982 | 0.975 – 0.988 |'
- en: '| RAP | 0.977 | 0.970 – 0.985 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| RAP | 0.977 | 0.970 – 0.985 |'
- en: '| Gemini 1.5 P. | ZS | 0.996 | 0.993 – 0.999 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Gemini 1.5 P. | ZS | 0.996 | 0.993 – 0.999 |'
- en: '| CoT | 0.902 | 0.887 – 0.917 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 0.902 | 0.887 – 0.917 |'
- en: '| CoT-SC | 0.938 | 0.926 – 0.951 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| CoT-SC | 0.938 | 0.926 – 0.951 |'
- en: '| RAP | 0.991 | 0.987 – 0.996 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| RAP | 0.991 | 0.987 – 0.996 |'
- en: Discussion
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Our findings indicated that all the approaches, combining three LLMs with four
    prompt engineering techniques, successfully passed the RD exam. However, the three
    leading LLMs had different performance levels in terms of the number of inaccurate
    responses and consistency. In addition, the prompting techniques had considerable
    impacts on the results. Such prompting impacts were also explored in other evaluation
    studies, for example, in clinical medicine [[33](#bib.bib33)], mental health [[44](#bib.bib44)]
    and radiology [[45](#bib.bib45)].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究结果表明，所有将三种LLM与四种提示工程技术相结合的方法均成功通过了RD考试。然而，这三种领先的LLM在不准确响应的数量和一致性方面的表现水平不同。此外，提示技术对结果有显著影响。这些提示影响在其他评估研究中也被探索过，例如临床医学[[33](#bib.bib33)]、心理健康[[44](#bib.bib44)]和放射学[[45](#bib.bib45)]。
- en: 'The combination of GPT-4o with CoT-SC prompting outperformed the other approaches
    in terms of accuracy, while Gemini 1.5 Pro with ZS prompting showed the highest
    consistency. On the other hand, the lowest average percentage score was 89.22%
    for Gemini 1.5 Pro with CoT, which also showed the lowest agreement in repeated
    measurements, with a coefficient of 0.902\. GPT-4o recorded the highest accuracy
    overall (see Table [1](#Sx1.T1 "Table 1 ‣ Introduction ‣ Accuracy and Consistency
    of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and
    Knowledge Retrieval")).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT-4o与CoT-SC提示的组合在准确性方面优于其他方法，而Gemini 1.5 Pro与ZS提示显示出最高的一致性。另一方面，Gemini 1.5
    Pro与CoT的最低平均得分为89.22%，这也表现出在重复测量中最低的一致性，系数为0.902。GPT-4o总体上记录了最高的准确性（见表[1](#Sx1.T1
    "Table 1 ‣ Introduction ‣ Accuracy and Consistency of LLMs in the Registered Dietitian
    Exam: The Impact of Prompt Engineering and Knowledge Retrieval")）。'
- en: 'This outcome contrasts with previous non-nutrition research, except in MMLU
    [[39](#bib.bib39)] with ZS prompting (see Table [2](#Sx1.T2 "Table 2 ‣ Introduction
    ‣ Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact
    of Prompt Engineering and Knowledge Retrieval")). Claude 3.5 with CoT obtained
    a 59.4% score on GPQA [[40](#bib.bib40)]. However, the three LLMs using CoT on
    the RD exam achieved scores above 90%. This difference might be due to the different
    difficulty levels of the exams. Particularly, 14.9% of the questions in the RD
    Exam are at the Expert level. However, as reported by Rein et al. [[40](#bib.bib40)],
    the GPQA questions are “extremely difficult,” from which PhD students achieved
    a 65% score while non-expert individuals achieved a 34% score. Moreover, DROP
    [[41](#bib.bib41)] demonstrated that Claude 3.5 with Three Shot prompting outperformed
    in reasoning over text. Conversely, our results indicated that GPT-4o performed
    better using the reasoning process of CoT prompting.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '这一结果与之前的非营养研究形成对比，但MMLU[[39](#bib.bib39)]中的ZS提示（见表[2](#Sx1.T2 "Table 2 ‣ Introduction
    ‣ Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact
    of Prompt Engineering and Knowledge Retrieval")）除外。Claude 3.5在GPQA[[40](#bib.bib40)]上获得了59.4%的得分。然而，三种LLM在RD考试中使用CoT取得了90%以上的得分。这种差异可能是由于考试难度的不同。特别是，RD考试中有14.9%的问题达到专家级别。然而，Rein等人[[40](#bib.bib40)]报告称，GPQA问题“极其困难”，博士生的得分为65%，而非专家的得分为34%。此外，DROP[[41](#bib.bib41)]表明Claude
    3.5使用Three Shot提示在文本推理中表现优异。相反，我们的结果表明GPT-4o在使用CoT提示的推理过程时表现更好。'
- en: Prior nutrition-focused research indicated that ChatGPT was accurate in most
    nutrition instances, but the chatbot also recorded errors that could potentially
    harm and negatively impact the end-users. Therefore, achieving general accuracy
    is insufficient for practical real-world applications. For example, Sun et al. [[20](#bib.bib20)]
    indicated that ChatGPT-3.5 and ChatGPT-4 passed the Chinese RD exam (included
    200 questions) and the food recommendations were acceptable despite the presence
    of mistakes for specific foods, such as root vegetables and dry beans. Mishra
    et al. [[46](#bib.bib46)] tested ChatGPT in eight medical nutritional therapy
    scenarios and discussed that ChatGPT should be avoided for complex scenarios.
    Similarly, other studies [[24](#bib.bib24), [10](#bib.bib10)] discussed that ChatGPT
    has great potentials for nutritional management focusing on non-communicable diseases,
    but the model might be potentially harmful by providing inaccurate responses,
    particularly in complex situations. Another study [[22](#bib.bib22)] leveraged
    ChatGPT-3.5 and ChatGPT-4 to provide nutritional information for eight menus.
    Their results indicated that responses had no significant differences compared
    to nutritionists’ recommendations in terms of energy, carbohydrate, and fat contents,
    but the difference was statistically significant for protein. The potential of
    ChatGPT to generate dietary advice for patients with allergic to food allergens
    were also investigated [[27](#bib.bib27)]. It was shown that although the model
    was generally accurate, it produced harmful diets. These studies highlight the
    need for further investigation into LLM responses within the context of food and
    nutrition.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 之前以营养为重点的研究表明，ChatGPT在大多数营养实例中是准确的，但该聊天机器人也记录了可能对最终用户造成伤害和负面影响的错误。因此，实现一般准确性对于实际应用是不够的。例如，Sun等人[[20](#bib.bib20)]表明，ChatGPT-3.5和ChatGPT-4通过了中国注册营养师考试（包括200道问题），尽管存在对某些食物（如根菜和干豆）的错误，食物推荐仍被接受。Mishra等人[[46](#bib.bib46)]在八种医学营养治疗场景中测试了ChatGPT，并讨论了在复杂场景中应避免使用ChatGPT。类似地，其他研究[[24](#bib.bib24),
    [10](#bib.bib10)]讨论了ChatGPT在针对非传染性疾病的营养管理中具有巨大潜力，但在复杂情况下提供不准确的回答可能会潜在地造成伤害。另一项研究[[22](#bib.bib22)]利用ChatGPT-3.5和ChatGPT-4为八种菜单提供营养信息。他们的结果表明，在能量、碳水化合物和脂肪含量方面，与营养师的建议没有显著差异，但在蛋白质方面差异具有统计学意义。还研究了ChatGPT生成针对食物过敏患者的饮食建议的潜力[[27](#bib.bib27)]。结果表明，尽管模型总体上准确，但它生成了有害的饮食。这些研究强调了在食品和营养背景下进一步调查LLM回应的必要性。
- en: Our results confirmed previous findings about the overall accuracy of ChatGPT
    and the instances of inaccurate responses. However, unlike the existing work,
    our study is not merely restricted to ChatGPT or the ZS prompting technique. We
    focused on examining errors across various subcategories and mitigate them by
    employing prompting techniques (reasoning and ensemble) and external knowledge
    retrieval.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果确认了之前关于ChatGPT整体准确性和不准确回答实例的发现。然而，与现有研究不同，我们的研究不仅仅限于ChatGPT或ZS提示技术。我们专注于检查各种子类别中的错误，并通过采用提示技术（推理和集成）以及外部知识检索来减轻这些错误。
- en: 'CoT guided LLMs to perform a reasoning process when answering a question. Our
    findings showed that CoT, compared to ZS prompting, enhanced the accuracy of GPT-4o
    and Claude 3.5 Sonnet but led to diminished consistency. The LLMs with CoT do
    not consistently generate the same reasoning paths, even with identical prompts
    (see Table [3](#Sx2.T3 "Table 3 ‣ Intra-rater Analysis ‣ Consistency ‣ Results
    ‣ Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact
    of Prompt Engineering and Knowledge Retrieval")). This variability indicates randomness
    in the selection of reasoning paths.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'CoT指导LLMs在回答问题时进行推理过程。我们的发现表明，与ZS提示相比，CoT提高了GPT-4o和Claude 3.5 Sonnet的准确性，但导致了一致性的下降。使用CoT的LLMs即使在相同的提示下也不会一致生成相同的推理路径（见表[3](#Sx2.T3
    "Table 3 ‣ Intra-rater Analysis ‣ Consistency ‣ Results ‣ Accuracy and Consistency
    of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and
    Knowledge Retrieval")）。这种变异性表明推理路径的选择存在随机性。'
- en: We observed that the reasoning steps of CoT considerably reduced the LLMs’ mistakes
    for the questions with Easy, Moderate, and Difficult proficiency levels, but this
    improvement was less for Expert-level questions, where only a few errors were
    corrected. Additionally, CoT notably improved the questions about D3) food service
    systems, which involved calculations for food cost and portion estimation/forecasting.
    CoT also enhanced the accuracy of D4) food and nutrition management, which included
    theoretical and conceptual questions requiring an understanding of implicitly
    stated relationships. These improvements by CoT are consistent with existing literature,
    indicating CoT enhances LLMs’ performance in arithmetic and commonsense tasks
    by establishing logical connections [[47](#bib.bib47)]. Conversely, the combination
    of Gemini 1.5 Pro with CoT showed different patterns, where both accuracy and
    consistency decreased. Gemini with CoT was unable to select a choice from the
    given multiple-choice options for 20 out of 1050 questions (on average). Although
    the errors on Easy and Moderate levels questions slightly decreased, the errors
    on Difficult and Expert levels questions notably increased.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到 CoT 的推理步骤显著减少了 LLMs 在简单、中等和困难水平问题上的错误，但对于专家级问题的改进较少，仅纠正了一些错误。此外，CoT 显著改善了关于
    D3) 食品服务系统的问题，这些问题涉及食品成本和份量估算/预测的计算。CoT 还提高了 D4) 食品和营养管理的准确性，这包括需要理解隐含关系的理论和概念性问题。这些
    CoT 的改进与现有文献一致，表明 CoT 通过建立逻辑联系增强了 LLMs 在算术和常识任务中的表现 [[47](#bib.bib47)]。相反，将 Gemini
    1.5 Pro 与 CoT 结合显示出不同的模式，准确性和一致性均有所下降。Gemini 与 CoT 的组合在 1050 个问题中的 20 个（平均）未能从给定的多项选择选项中选择一个选项。虽然简单和中等水平问题的错误略有减少，但困难和专家级问题的错误显著增加。
- en: It should be noted that while CoT reduced errors in questions requiring calculations,
    our observations indicate that CoT responses still include miscalculations and
    rounding errors. This issue may arise due to the inherent characteristics of Transformer
    models, designed to generate text based on tokens rather than numerical values.
    Potential solutions to address these issues include agentic approaches [[48](#bib.bib48),
    [49](#bib.bib49)], which integrate LLMs with calculator tools or symbolic computing
    systems.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，虽然 CoT 在需要计算的问题中减少了错误，我们的观察表明，CoT 的响应仍然包含计算错误和四舍五入误差。这一问题可能由于 Transformer
    模型的固有特性，即根据标记生成文本而不是数值。解决这些问题的潜在方案包括代理方法 [[48](#bib.bib48), [49](#bib.bib49)]，这些方法将
    LLMs 与计算器工具或符号计算系统集成。
- en: 'CoT-SC guided LLMs to perform multiple independent reasoning processes, then
    the responses were merged using a majority voting method. Our findings revealed
    that CoT-SC (compared to CoT) improved accuracy, particularly in Gemini 1.5 Pro.
    However, in GPT-4o and Claude 3.5, this improvement was small, as it only led
    to the correction of a few errors. This small difference can also be observed
    in their high inter-rater coefficient agreement, as illustrated in Figure [3](#Sx2.F3
    "Figure 3 ‣ Subgroup Error Analysis ‣ Accuracy ‣ Results ‣ Accuracy and Consistency
    of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and
    Knowledge Retrieval"). This finding does not support the literature suggesting
    that CoT-SC considerably enhances the accuracy of CoT [[50](#bib.bib50)].'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'CoT-SC 引导 LLMs 执行多个独立的推理过程，然后使用多数投票方法合并响应。我们的发现表明，与 CoT 相比，CoT-SC 提高了准确性，特别是在
    Gemini 1.5 Pro 中。然而，在 GPT-4o 和 Claude 3.5 中，这种改进较小，因为它仅纠正了一些错误。这种小差异也可以从它们的高评估者间一致性系数中观察到，如图
    [3](#Sx2.F3 "Figure 3 ‣ Subgroup Error Analysis ‣ Accuracy ‣ Results ‣ Accuracy
    and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt
    Engineering and Knowledge Retrieval") 所示。这一发现不支持文献中提出的 CoT-SC 显著提高 CoT 准确性的观点
    [[50](#bib.bib50)]。'
- en: On the other hand, CoT-SC achieved notably higher consistency (intra-rater agreement)
    compared to CoT. The ensemble process enabled by CoT-SC mitigates the randomness
    in the selection of reasoning paths. For GPT-4o and Claude 3.5 Sonnet, the Fleiss’
    Kappa agreements of CoT-SC were as robust as the agreements of ZS prompting. The
    Gemini’s inability to select a choice from the given multiple-choice options also
    improved, reducing them from 20 in CoT to 6 in CoT-SC. This highlights the importance
    of employing such ensemble techniques to enhance the consistency of LLM’s reasoning
    process by combining multiple reasoning paths rather than relying on a single
    path.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，CoT-SC 在一致性（同一评分者的一致性）方面明显优于 CoT。CoT-SC 通过集成过程减少了推理路径选择中的随机性。对于 GPT-4o
    和 Claude 3.5 Sonnet，CoT-SC 的 Fleiss’ Kappa 一致性与 ZS 提示的一致性同样稳健。Gemini 在给定多个选择选项中选择答案的能力也有所改善，从
    CoT 的 20 个减少到 CoT-SC 的 6 个。这突显了采用集成技术的重要性，通过结合多个推理路径而不是依赖单一路径来提高 LLM 推理过程的一致性。
- en: RAP integrated external relevant information from multiple references into the
    input prompts. However, our findings showed that RAP did not consistently improve
    accuracy across the three models. GPT-4o effectively leveraged the retrieved information
    to reduce error rates, particularly for Difficult and Expert questions that required
    more comprehensive understanding. Similar to CoT and CoT-SC, RAP improved D3)
    food service systems and D4) food and nutrition management questions. Although
    relevant information was provided in our knowledge base, RAP (compared to ZS)
    has recorded higher error rates for D2) nutrition care. D2 questions are mostly
    related to medical nutrition therapy, dietary guidelines, counseling skills, and
    nutrition care process. This higher error rates might arise from irrelevant retrieval,
    where the retrieval model fetches extraneous information [[51](#bib.bib51)]. Additionally,
    the complexity or ambiguity of the queries might contribute to this problem making
    it challenging for the retrieval model to find the most relevant chunks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: RAP 将来自多个参考文献的外部相关信息集成到输入提示中。然而，我们的发现表明，RAP 并未在三种模型中一致地提高准确性。GPT-4o 有效利用检索到的信息来降低错误率，尤其是在需要更全面理解的困难和专家问题中。类似于
    CoT 和 CoT-SC，RAP 改进了 D3) 食品服务系统和 D4) 食品及营养管理问题。尽管我们的知识库中提供了相关信息，RAP（与 ZS 相比）在
    D2) 营养护理中的错误率更高。D2 问题主要涉及医疗营养治疗、饮食指南、咨询技巧和营养护理过程。这些较高的错误率可能来源于无关的检索，其中检索模型获取了多余的信息
    [[51](#bib.bib51)]。此外，查询的复杂性或模糊性可能也会导致检索模型难以找到最相关的信息块。
- en: In contrast to GPT-4o, Gemini 1.5 Pro with RAP showed opposite behavior, as
    the accuracy for the Difficult and Expert questions reduced. We noticed that,
    in some cases, Gemini was prioritizing external information over its own internal
    knowledge, even when that external information was irrelevant to the question.
    This resulted in incorrect interpretations and answers. For example, for two questions,
    the model generated “The provided text does not contain the answer to the question
    as it pertains to dietary restrictions for patients on Linezolid.” and “The provided
    text focuses on Body Mass Index (BMI) but does not contain information about when
    weight and BMI peak.” This issue was particularly observed in D2, where error
    rates increased from 26.2 (ZS) to 35.6 (RAP).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GPT-4o 相比，配备 RAP 的 Gemini 1.5 Pro 展现了相反的行为，因为在困难和专家问题的准确性下降。我们注意到，在某些情况下，Gemini
    将外部信息优先于其自身内部知识，即使这些外部信息与问题无关。这导致了错误的解释和答案。例如，对于两个问题，模型生成了“提供的文本不包含关于 Linezolid
    患者饮食限制的问题的答案。”和“提供的文本关注体重指数 (BMI)，但未包含关于体重和 BMI 峰值的时间的信息。”这个问题在 D2 中尤其明显，错误率从
    26.2 (ZS) 增加到 35.6 (RAP)。
- en: It is worth noting that the prompting techniques had less impact, whether positive
    or negative, on D1) Principles of Dietetics questions compared to the other domains.
    D1 questions primarily focus on general food science, nutrients, biochemistry,
    and related research (e.g. which fruit has the highest fructose?), compared to
    the other domains that are more specialized in dietetics or involve more domain
    knowledge. For D1, GPT-4o achieved the best accuracy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，相较于其他领域，提示技术对 D1) 营养学原理问题的影响，无论是正面还是负面，都较小。D1 问题主要关注一般食品科学、营养、化学、生物化学和相关研究（例如哪种水果含有最高的果糖？），而其他领域则更专业化于营养学或涉及更多领域知识。对于
    D1，GPT-4o 实现了最佳准确性。
- en: This study is limited to the leading proprietary LLM models. These models are
    user-friendly and highly powerful. Our results also confirm their significant
    potential in food and nutrition applications. Yet, growing concerns are being
    raised about their lack of openness and limited access. In contrast, open-source
    LLMs are emerging rapidly, offering benefits, such as improved data security and
    privacy, decreased reliance on vendors, and the ability to customize models. Examples
    of the state-of-the-art open-source LLMs are Llama 3 [[52](#bib.bib52)], Falcon
    2 [[53](#bib.bib53)], and Yi-34B [[54](#bib.bib54)]. Given their advantages, future
    research should evaluate the performance of open-source LLMs in the diet and nutrition
    field.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究限于领先的专有LLM模型。这些模型用户友好且功能强大。我们的结果还确认了它们在食品和营养应用中的显著潜力。然而，关于它们缺乏开放性和访问限制的担忧日益增加。相比之下，开源LLM迅速涌现，提供了诸如改进的数据安全和隐私、减少对供应商的依赖以及定制模型的能力等好处。最先进的开源LLM示例包括Llama
    3 [[52](#bib.bib52)]、Falcon 2 [[53](#bib.bib53)] 和 Yi-34B [[54](#bib.bib54)]。鉴于它们的优势，未来的研究应评估开源LLM在饮食和营养领域的表现。
- en: Our evaluation has primarily concentrated on the accuracy and consistency of
    the models. Given the sensitivity of health and nutrition applications, ensuring
    high accuracy and consistency is essential. However, it is important to assess
    LLMs from other perspectives, such as safety, bias, privacy, and emotional support,
    to mention a few [[18](#bib.bib18), [19](#bib.bib19), [55](#bib.bib55)]. Future
    work in this direction will involve evaluating LLMs according to these trustworthiness
    metrics by leveraging patient-centric questions, answers, and conversations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估主要集中在模型的准确性和一致性上。鉴于健康和营养应用的敏感性，确保高准确性和一致性至关重要。然而，从其他角度评估LLM，如安全性、偏见、隐私和情感支持等也是重要的
    [[18](#bib.bib18)、[19](#bib.bib19)、[55](#bib.bib55)]。未来的工作将包括根据这些可信度指标评估LLM，利用以患者为中心的问题、答案和对话。
- en: Additionally, we examined the impacts of prompt engineering methods on LLM answers
    to diet and nutrition questions. Various studies have explored the role of fine-tuning
    [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58)] and agentic methods [[23](#bib.bib23),
    [12](#bib.bib12), [59](#bib.bib59)]. Future research should evaluate their impact
    on nutrition management applications.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还考察了提示工程方法对LLM回答饮食和营养问题的影响。各种研究探讨了微调 [[56](#bib.bib56)、[57](#bib.bib57)、[58](#bib.bib58)]
    和主动方法 [[23](#bib.bib23)、[12](#bib.bib12)、[59](#bib.bib59)] 的作用。未来的研究应评估这些方法在营养管理应用中的影响。
- en: In conclusion, this study assessed the accuracy and consistency of the GPT-4o,
    Claude 3.5 Sonnet, and Gemini 1.5 Pro in responding to diet and nutrition questions
    of the RD exam. In contrast to the previous LLM evaluation studies focusing on
    nutritional management, our experiments were not restricted to ChatGPT or ZS prompting.
    We evaluated the models using the RD exam and analyzed their errors across various
    questions complexities and nutrition domains. Our findings highlighted the strengths
    and weaknesses of the three LLMs, showing the influence of different prompting
    techniques on their responses to the RD exam questions. GPT-4o with CoT-SC prompting
    outperformed other approaches, while Gemini 1.5 Pro with ZS indicated the highest
    consistency. For GPT-4o and Claude 3.5, the application of CoT improved accuracy,
    while CoT-SC enhanced both accuracy and consistency. RAP particularly improved
    GPT-4o performance in addressing difficult- expert-level questions. Consequently,
    selecting the appropriate LLM and prompt engineering, tailored to the proficiency
    level and specific domain, can considerably reduce errors and mitigate potential
    risks in diet and nutrition chatbot applications.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本研究评估了GPT-4o、Claude 3.5 Sonnet 和 Gemini 1.5 Pro 在回答RD考试饮食和营养问题时的准确性和一致性。与之前关注营养管理的LLM评估研究不同，我们的实验不限于ChatGPT或ZS提示。我们使用RD考试评估模型，并分析了其在不同问题复杂性和营养领域中的错误。我们的发现突出了三种LLM的优缺点，展示了不同提示技术对RD考试问题回答的影响。GPT-4o采用CoT-SC提示优于其他方法，而Gemini
    1.5 Pro使用ZS提示显示出最高的一致性。对于GPT-4o和Claude 3.5，应用CoT提高了准确性，而CoT-SC提升了准确性和一致性。RAP特别改善了GPT-4o在处理困难专家级问题上的表现。因此，选择适当的LLM和提示工程，针对熟练程度和具体领域进行定制，可以显著减少错误并降低饮食和营养聊天机器人应用中的潜在风险。
- en: Methods
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: '![Refer to caption](img/b93dd94e8128a10ed5d1868c9b0eb035.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b93dd94e8128a10ed5d1868c9b0eb035.png)'
- en: (a) Zero Shot (ZS) prompting
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Zero Shot (ZS) 提示
- en: '![Refer to caption](img/a65c18c01bcbc65fe3afae6657506dda.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a65c18c01bcbc65fe3afae6657506dda.png)'
- en: (b) Chain of Thought (CoT) prompting
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 思维链（CoT）提示
- en: '![Refer to caption](img/d05863bb3c796e7484343483cd5aaf3f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d05863bb3c796e7484343483cd5aaf3f.png)'
- en: (c) Chain of Thought with Self Consistency (CoT-SC) prompting
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 自我一致性的思维链（CoT-SC）提示
- en: '![Refer to caption](img/c5f3512e5c7db370df3243e3190d0686.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c5f3512e5c7db370df3243e3190d0686.png)'
- en: (d) Retrieval Augmented Prompting (RAP)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 检索增强提示（RAP）
- en: 'Figure 4: Schematic illustrations of the four prompting techniques used in
    the evaluation. The inputs include multiple-choice questions along with task description,
    and the generated output includes the selected choice.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：评估中使用的四种提示技术的示意图。输入包括多项选择题及任务描述，生成的输出包括所选的选项。
- en: Registered Dietitian Exam
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注册营养师考试
- en: 'The Registration Examination for Dietitians is a required exam for individuals
    seeking to obtain the registered dietitian credential. To take the exam, candidates
    must successfully complete the eligibility requirements provided by the Commission
    on Dietetic Registration (CDR) [[60](#bib.bib60)]. The examination is computer-based
    and consists of 125 to 145 four-choice questions [[61](#bib.bib61)]. The exam
    includes multiple-choice questions from four major domains: D1) Principles of
    Dietetics (21%), D2) Nutrition Care for Individuals and Groups (45%), D3) Food
    Service Systems (13%), and D4) Management of Food and Nutrition Programs and Services
    (21%) [[61](#bib.bib61)]. The exam is scored from 1 to 50, and the minimum score
    to pass is 25\. The score is calculated based on the candidate’s performance as
    well as the difficulty levels of the questions [[61](#bib.bib61)].'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注册营养师考试是获得注册营养师资格证书的必考考试。要参加考试，考生必须成功完成由营养注册委员会（CDR）提供的资格要求[[60](#bib.bib60)]。该考试是计算机化的，由125至145道四选一题组成[[61](#bib.bib61)]。考试包括来自四个主要领域的多项选择题：D1)
    营养学原理（21%），D2) 个体和群体的营养护理（45%），D3) 食品服务系统（13%），和D4) 食品和营养项目及服务的管理（21%）[[61](#bib.bib61)]。考试评分范围为1到50，及格分数为25。分数根据考生的表现以及问题的难度水平进行计算[[61](#bib.bib61)]。
- en: Within the four domains, D1 covers topics related to i) food, nutrition, and
    supporting sciences, ii) education, communication and technology, and iii) research
    applications. D2 consists of the topics related to i) screening and assessment,
    ii) diagnosis, iii) planning and intervention, and iv) monitoring and evaluation.
    D3 includes topics related to i) menu development, ii) procurement, production,
    distribution, and service, iii) sanitation and safety, and iv) equipment and facility
    planning. D4 includes topics related to i) functions of management, ii) human
    resource management, iii) financial management, iv) marketing and public relations;
    and v) quality management and regulatory compliance [[61](#bib.bib61)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在四个领域中，D1涵盖了与i) 食物、营养及支持科学，ii) 教育、沟通和技术，iii) 研究应用相关的话题。D2包括与i) 筛查和评估，ii) 诊断，iii)
    规划和干预，以及iv) 监测和评估相关的话题。D3涵盖了与i) 菜单开发，ii) 采购、生产、分配和服务，iii) 卫生和安全，以及iv) 设备和设施规划相关的话题。D4包括与i)
    管理职能，ii) 人力资源管理，iii) 财务管理，iv) 市场营销和公共关系，以及v) 质量管理和法规遵从相关的话题[[61](#bib.bib61)]。
- en: Large Language Models
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: 'GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, as the leading LLMs chatbots
    [[62](#bib.bib62), [63](#bib.bib63)], are employed in this study for evaluation.
    OpenAI released GPT-4o, their new flagship model, on May 13, 2024 [[35](#bib.bib35)],
    Claude 3.5 Sonnet was launched, by Anthropic, as their strongest vision model
    yet, on Jun 20, 2024 [[36](#bib.bib36)], and Google announced Gemini 1.5 Pro as
    their next-generation model on February 15, 2024 [[37](#bib.bib37)]. An overview
    of the models’ performance on other benchmarks are indicated in Table [2](#Sx1.T2
    "Table 2 ‣ Introduction ‣ Accuracy and Consistency of LLMs in the Registered Dietitian
    Exam: The Impact of Prompt Engineering and Knowledge Retrieval"). Find more details
    in [[36](#bib.bib36), [35](#bib.bib35), [42](#bib.bib42)].'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o、Claude 3.5 Sonnet 和 Gemini 1.5 Pro，作为领先的LLM聊天机器人[[62](#bib.bib62), [63](#bib.bib63)]，在本研究中用于评估。OpenAI于2024年5月13日发布了GPT-4o，新旗舰模型[[35](#bib.bib35)]，Claude
    3.5 Sonnet由Anthropic推出，是其最强的视觉模型，于2024年6月20日发布[[36](#bib.bib36)]，Google于2024年2月15日宣布了Gemini
    1.5 Pro作为其下一代模型[[37](#bib.bib37)]。模型在其他基准测试中的表现概述见表[2](#Sx1.T2 "表2 ‣ 介绍 ‣ 注册营养师考试中的LLMs的准确性和一致性：提示工程和知识检索的影响")。更多详细信息见[[36](#bib.bib36),
    [35](#bib.bib35), [42](#bib.bib42)]。
- en: In this study, we set the temperature setting to 0 for all the models to better
    evaluate the LLMs’ knowledge and decision-making in nutrition and diet applications,
    minimizing the effect of external variables on consistency. The temperature parameter,
    ranging from 0 to 2, regulates the uncertainty or randomness in the output [[64](#bib.bib64)].
    With a temperature setting of 0, the model generates responses by selecting the
    next words with the highest probability, making the model “more deterministic.”
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们将所有模型的温度设置为0，以更好地评估LLMs在营养和饮食应用中的知识和决策，尽量减少外部变量对一致性的影响。温度参数范围为0到2，调节输出中的不确定性或随机性
    [[64](#bib.bib64)]。温度设置为0时，模型通过选择下一个具有最高概率的词生成响应，使模型“更具确定性”。
- en: Prompt Engineering
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示工程
- en: 'Four prompting techniques are utilized in this study for the models evaluation.
    Schematic illustrations of the four techniques are shown in Figure [4](#Sx4.F4
    "Figure 4 ‣ Methods ‣ Accuracy and Consistency of LLMs in the Registered Dietitian
    Exam: The Impact of Prompt Engineering and Knowledge Retrieval"). Additionally,
    the instructions used for the prompting techniques are presented in Supplementary
    Table S.3.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用了四种提示技术来评估模型。四种技术的示意图见图 [4](#Sx4.F4 "图4 ‣ 方法 ‣ LLMs在注册营养师考试中的准确性和一致性：提示工程和知识检索的影响")。此外，用于提示技术的说明见补充表S.3。
- en: 1) Zero Shot (ZS) prompting generates the simplest type of prompt, including
    a question and a fixed task description. The model leverages its internal knowledge
    to generate responses [[29](#bib.bib29)]. To the best of our knowledge, existing
    evaluations of LLM chatbots focusing on nutrition and diet have utilized ZS prompting
    for their assessments. 2) Chain of Thought (CoT) prompting consists of a question
    and a description to the model to answer the question through intermediate reasoning
    steps [[47](#bib.bib47)]. CoT has been widely used in medical studies [[65](#bib.bib65),
    [33](#bib.bib33)]. 3) Chain of Thought with Self Consistency (CoT-SC) prompting
    creates several independent reasoning paths using CoT. Subsequently, the outcomes
    are aggregated [[50](#bib.bib50)]. In our experiments, we selected three independent
    reasoning paths and used a majority voting method for the aggregation. 4) Retrieval
    Augmented Prompting (RAP) fetches relevant information from a knowledge base in
    real-time and integrates it into the input prompt [[51](#bib.bib51), [66](#bib.bib66)].
    In contrast to the other prompting techniques, using RAP, the model generates
    responses by relying not only on its internal knowledge but also on external information.
    In our study, the knowledge base includes 125 documents (such as articles, books,
    and guidelines) recommended by the Academy of Nutrition and Dietetics [[43](#bib.bib43)],
    as references for the RD exam. The full list of the references used for RAP is
    provided in Supplementary Table S.4\. For the implementation, we leveraged a conventional
    Retrieval Augmented Generation (RAG) framework [[51](#bib.bib51)]. To achieve
    this, the references were divided into 512-token chunks, using the Amazon Titan
    Text Embeddings v2 model [[67](#bib.bib67)] for text embeddings. Then, the Cosine
    Similarity method [[68](#bib.bib68)] was utilized to identify the most similar
    chunks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 1) Zero Shot (ZS) 提示生成最简单的提示类型，包括一个问题和一个固定的任务描述。模型利用其内部知识生成响应 [[29](#bib.bib29)]。据我们所知，现有的针对营养和饮食的LLM聊天机器人评估都使用了ZS提示。
    2) Chain of Thought (CoT) 提示包含一个问题和一个描述，模型通过中间推理步骤回答问题 [[47](#bib.bib47)]。CoT在医学研究中被广泛使用
    [[65](#bib.bib65), [33](#bib.bib33)]。 3) Chain of Thought with Self Consistency
    (CoT-SC) 提示创建几个独立的推理路径，使用CoT。随后，结果被汇总 [[50](#bib.bib50)]。在我们的实验中，我们选择了三个独立的推理路径，并使用多数投票方法进行汇总。
    4) Retrieval Augmented Prompting (RAP) 实时从知识库中获取相关信息并将其集成到输入提示中 [[51](#bib.bib51),
    [66](#bib.bib66)]。与其他提示技术相比，使用RAP时，模型不仅依赖于其内部知识，还依赖于外部信息来生成响应。在我们的研究中，知识库包括125份由营养与饮食学会推荐的文档（如文章、书籍和指南）
    [[43](#bib.bib43)]，作为RD考试的参考。RAP使用的参考文献的完整列表见补充表S.4。为了实现这一点，我们利用了传统的Retrieval
    Augmented Generation (RAG)框架 [[51](#bib.bib51)]。为此，参考文献被划分为512-token的块，使用Amazon
    Titan Text Embeddings v2模型 [[67](#bib.bib67)] 进行文本嵌入。然后，使用Cosine Similarity方法
    [[68](#bib.bib68)] 识别最相似的块。
- en: Data Collection
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: The 1050 RD exam questions were delivered to the three models using the four
    prompting techniques. Each question was asked five times. Consequently, we collected
    60 (i.e., $3\times 4\times 5$) sets of 1050 responses. As previously mentioned,
    the questions include four choices. We observed that sometimes the LLMs were unable
    to select an option from the multiple choices and provided responses such as,
    “None of the above,” “Since no option is correct, we cannot provide a final answer
    within the requested tags,” or “Cannot be determined with the given information.”
    In summary, this issue occurred once for GPT-4o with CoT, once for GPT-4o with
    CoT-SC, 15 times for Claude 3.5 with RAP, 100 times for Gemini 1.5 with CoT, 30
    times for Gemini 1.5 with CoT-SC, and 63 times for Gemini 1.5 with RAP. For these
    responses, we added another option, labeled “Others.”
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 1050 道 RD 考试题目使用四种提示技术交付给三种模型。每道题目询问五次。因此，我们收集了 60 组 (即 $3\times 4\times 5$)
    的 1050 个响应。如前所述，问题包括四个选择。我们观察到，有时 LLMs 无法从多个选择中选择一个选项，并提供了如“以上都不是”、“由于没有选项是正确的，我们无法在要求的标签内提供最终答案”或“无法根据提供的信息确定”的响应。总之，这种情况发生在
    GPT-4o 使用 CoT 一次，GPT-4o 使用 CoT-SC 一次，Claude 3.5 使用 RAP 15 次，Gemini 1.5 使用 CoT
    100 次，Gemini 1.5 使用 CoT-SC 30 次，以及 Gemini 1.5 使用 RAP 63 次。对于这些响应，我们添加了另一个选项，标记为“其他”。
- en: The collected responses were compared with the ground truth answers provided
    by the Academy of Nutrition and Dietetics, eatrightPREP [[43](#bib.bib43)]. It
    should be noted that we used a new chat session for each query to minimize bias
    in the evaluation caused by information leakage from other questions. The data
    collection was performed in Python using OpenAI [[69](#bib.bib69)], google-generativeai
    [[70](#bib.bib70)], Boto3 [[71](#bib.bib71)], and lxml.etree [[72](#bib.bib72)]
    libraries.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 收集到的响应与由营养与饮食学学会提供的真实答案进行了比较，使用了 eatrightPREP [[43](#bib.bib43)]。需要注意的是，我们为每个查询使用了新的聊天会话，以最小化由于其他问题的信息泄漏而导致的评估偏差。数据收集是在
    Python 环境中使用 OpenAI [[69](#bib.bib69)]、google-generativeai [[70](#bib.bib70)]、Boto3
    [[71](#bib.bib71)] 和 lxml.etree [[72](#bib.bib72)] 库进行的。
- en: Statistical Analysis
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统计分析
- en: The responses were evaluated in terms of accuracy and consistency. Accuracy
    measures how close a set of responses are to the ground truth answers. To this
    end, we calculate the percentage score, which is the ratio of correct responses
    to all responses multiplied by 100\. The percentage score indicates how well the
    LLMs can detect the correct option. As previously mentioned, each measurement
    is repeated five times. The five repeated measurements in each test are grouped,
    and the mean and standard deviation of the scores are calculated.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 响应在准确性和一致性方面进行了评估。准确性衡量一组响应接近真实答案的程度。为此，我们计算了百分比分数，即正确响应与所有响应的比例乘以 100\. 百分比分数表明
    LLMs 能够多好地检测正确选项。如前所述，每次测量重复五次。在每个测试中的五次重复测量被分组，并计算分数的均值和标准差。
- en: Consistency refers to the degree to which responses produce the same results.
    To assess consistency, we perform inter-rater and intra-rater analysis approaches.
    [[73](#bib.bib73)]. For the former, the agreement between the responses obtained
    from different models / prompting techniques are evaluated. To this end, Cohen’s
    Kappa [[74](#bib.bib74)] was utilized to measure the degree of agreement between
    two sets of responses. For example, the agreement between responses obtained from
    GPT-4o with ZS prompting and GPT-4o with CoT prompting are calculated. Furthermore,
    for the intra-rater analysis, Fleiss Kappa test [[75](#bib.bib75)] was used to
    indicate the degree of overall agreement between the repeated measurements under
    fixed conditions. For instance, we assess whether GPT-4o with ZS prompting provides
    the same choices in repeated measurements. It should be noted that the statistical
    analysis was conducted in R Programming using irr [[76](#bib.bib76)] and boot
    [[77](#bib.bib77)] libraries.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性是指响应结果产生相同结果的程度。为了评估一致性，我们进行评估者间分析和评估者内分析方法。[[73](#bib.bib73)]。对于前者，我们评估来自不同模型/提示技术的响应之间的协议。为此，采用了
    Cohen’s Kappa [[74](#bib.bib74)] 来衡量两组响应之间的协议程度。例如，计算使用 ZS 提示的 GPT-4o 和使用 CoT
    提示的 GPT-4o 之间响应的协议。此外，对于评估者内分析，使用了 Fleiss Kappa test [[75](#bib.bib75)] 来指示在固定条件下重复测量之间的总体协议程度。例如，我们评估
    GPT-4o 使用 ZS 提示是否在重复测量中提供相同的选择。需要注意的是，统计分析是在 R 编程环境中使用 irr [[76](#bib.bib76)]
    和 boot [[77](#bib.bib77)] 库进行的。
- en: Data Availability
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可用性
- en: The RD exam questions used in this study are not publicly available and can
    be accessed via [https://www.eatrightprep.org](https://www.eatrightprep.org).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用的RD考试问题未公开，可以通过[https://www.eatrightprep.org](https://www.eatrightprep.org)访问。
- en: Code Availability
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码可用性
- en: The codes for data collection, API calls, and statistical analysis are available
    at [https://github.com/iHealthLab/DietitianExamEval](https://github.com/iHealthLab/DietitianExamEval).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集、API调用和统计分析的代码可在 [https://github.com/iHealthLab/DietitianExamEval](https://github.com/iHealthLab/DietitianExamEval)
    上获取。
- en: Competing Interests
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 竞争利益
- en: The authors declare no competing interests. Moreover, the funders of the study
    had no role in study design, data collection and analysis, or interpretation of
    results and preparation of the manuscript.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 作者声明无竞争利益。此外，研究资助者未参与研究设计、数据收集和分析、结果解释及手稿准备。
- en: References
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Singh, B. *et al.* Systematic review and meta-analysis of the effectiveness
    of chatbots on lifestyle behaviours. *npj Digital Medicine*  6, 118 (2023).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Singh, B. *等* 聊天机器人对生活方式行为影响的系统评价和荟萃分析。*npj数字医学* 6, 118 (2023)。'
- en: '[2] Webster, P. Six ways large language models are changing healthcare. *Nature
    Medicine*  29, 2969–2971 (2023).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Webster, P. 大型语言模型改变医疗保健的六种方式。*自然医学* 29, 2969–2971 (2023)。'
- en: '[3] Ma, P. *et al.* Large language models in food science: Innovations, applications,
    and future. *Trends in Food Science & Technology* 104488 (2024).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Ma, P. *等* 大型语言模型在食品科学中的应用：创新、应用及未来。*食品科学与技术趋势* 104488 (2024)。'
- en: '[4] Clusmann, J. *et al.* The future landscape of large language models in
    medicine. *Communications medicine*  3, 141 (2023).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Clusmann, J. *等* 大型语言模型在医学中的未来景象。*通讯医学* 3, 141 (2023)。'
- en: '[5] Meskó, B. The impact of multimodal large language models on health care’s
    future. *Journal of medical Internet research*  25, e52865 (2023).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Meskó, B. 多模态大型语言模型对未来医疗保健的影响。*医学互联网研究杂志* 25, e52865 (2023)。'
- en: '[6] Bond, A., Mccay, K. & Lal, S. Artificial intelligence & clinical nutrition:
    What the future might have in store. *Clinical nutrition ESPEN* (2023).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Bond, A., Mccay, K. & Lal, S. 人工智能与临床营养：未来可能的展望。*临床营养ESPEN* (2023)。'
- en: '[7] Dao, D., Teo, J. Y. C., Wang, W. & Nguyen, H. D. LLM-Powered Multimodal
    AI Conversations for Diabetes Prevention. In *Proceedings of the 1st ACM Workshop
    on AI-Powered Q&A Systems for Multimedia*, 1–6 (2024).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Dao, D., Teo, J. Y. C., Wang, W. & Nguyen, H. D. LLM驱动的多模态AI对话用于糖尿病预防。见
    *第1届ACM多媒体AI问答系统研讨会论文集*，1–6 (2024)。'
- en: '[8] OpenAI. Introducing ChatGPT. [https://openai.com/index/chatgpt/](https://openai.com/index/chatgpt/)
    (2022). Accessed: August 2024.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] OpenAI. 介绍ChatGPT。 [https://openai.com/index/chatgpt/](https://openai.com/index/chatgpt/)
    (2022)。访问时间：2024年8月。'
- en: '[9] Liu, Y. *et al.* Exploring the usability of a chatbot-based conversational
    dietary assessment tool among cardiovascular patients. *European Journal of Preventive
    Cardiology*  30, zwad125–281 (2023).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Liu, Y. *等* 探索基于聊天机器人的对话式饮食评估工具在心血管患者中的可用性。*欧洲预防心脏病学杂志* 30, zwad125–281
    (2023)。'
- en: '[10] Pugliese, N. *et al.* Accuracy, Reliability, and Comprehensibility of
    ChatGPT-generated Medical Responses for Patients with Nonalcoholic Fatty Liver
    Disease. *Clinical Gastroenterology and Hepatology*  22, 886–889 (2024).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Pugliese, N. *等* ChatGPT生成的非酒精性脂肪肝疾病患者医学回应的准确性、可靠性和可理解性。*临床胃肠病学与肝病学* 22,
    886–889 (2024)。'
- en: '[11] Kim, D. W. *et al.* Qualitative evaluation of artificial intelligence-generated
    weight management diet plans. *Frontiers in Nutrition*  11, 1374834 (2024).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Kim, D. W. *等* 人工智能生成的体重管理饮食计划的定性评估。*营养学前沿* 11, 1374834 (2024)。'
- en: '[12] Abbasian, M. *et al.* Knowledge-Infused LLM-Powered Conversational Health
    Agent: A Case Study for Diabetes Patients. In *the 46th Annual International Conference
    of the IEEE Engineering in Medicine and Biology Society* (IEEE, 2024).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Abbasian, M. *等* 知识注入的LLM驱动对话健康代理：糖尿病患者的案例研究。见 *第46届IEEE医学与生物工程年会* (IEEE,
    2024)。'
- en: '[13] Haman, M., Školník, M. & Lošták, M. AI dietitian: Unveiling the accuracy
    of ChatGPT’s nutritional estimations. *Nutrition*  119, 112325 (2024).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Haman, M., Školník, M. & Lošták, M. AI营养师：揭示ChatGPT营养估计的准确性。*营养* 119,
    112325 (2024)。'
- en: '[14] Qarajeh, A. *et al.* AI-Powered Renal Diet Support: Performance of ChatGPT,
    Bard AI, and Bing Chat. *Clinics and Practice*  13, 1160–1172 (2023).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Qarajeh, A. *等* AI驱动的肾脏饮食支持：ChatGPT、Bard AI和Bing Chat的表现。*临床与实践* 13, 1160–1172
    (2023)。'
- en: '[15] Tsai, C.-H. *et al.* Generating Personalized Pregnancy Nutrition Recommendations
    with GPT-Powered AI Chatbot. In *20th International Conference on Information
    Systems for Crisis Response and Management (ISCRAM)*, vol. 2023, 263 (2023).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Tsai, C.-H. *等*。利用GPT驱动的AI聊天机器人生成个性化孕期营养建议。在*第20届危机响应与管理信息系统国际会议（ISCRAM）*，第2023卷，263
    (2023)。'
- en: '[16] Zhou, P. *et al.* FoodSky: A Food-oriented Large Language Model that Passes
    the Chef and Dietetic Examination. *arXiv preprint arXiv:2406.10261* (2024).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Zhou, P. *等*。FoodSky：一个通过厨师和营养考试的食品导向大型语言模型。*arXiv预印本 arXiv:2406.10261*
    (2024)。'
- en: '[17] Thirunavukarasu, A. J. *et al.* Large language models in medicine. *Nature
    medicine*  29, 1930–1940 (2023).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Thirunavukarasu, A. J. *等*。医学中的大型语言模型。*自然医学* 29, 1930–1940 (2023)。'
- en: '[18] Abbasian, M. *et al.* Foundation metrics for evaluating effectiveness
    of healthcare conversations powered by generative AI. *NPJ Digital Medicine*  7,
    82 (2024).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Abbasian, M. *等*。用于评估生成型人工智能驱动的医疗对话效果的基础指标。*NPJ数字医学* 7, 82 (2024)。'
- en: '[19] Liang, P. *et al.* Holistic evaluation of language models. *arXiv preprint
    arXiv:2211.09110* (2022).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Liang, P. *等*。语言模型的全面评估。*arXiv预印本 arXiv:2211.09110* (2022)。'
- en: '[20] Sun, H. *et al.* An AI dietitian for type 2 diabetes mellitus management
    based on large language and image recognition models: preclinical concept validation
    study. *Journal of Medical Internet Research*  25, e51300 (2023).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Sun, H. *等*。基于大型语言和图像识别模型的二型糖尿病管理AI营养师：临床前概念验证研究。*医学互联网研究杂志* 25, e51300
    (2023)。'
- en: '[21] Barlas, T., Altinova, A. E., Akturk, M. & Toruner, F. B. Credibility of
    ChatGPT in the assessment of obesity in type 2 diabetes according to the guidelines.
    *International Journal of Obesity*  48, 271–275 (2024).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Barlas, T., Altinova, A. E., Akturk, M. & Toruner, F. B. ChatGPT在依据指南评估二型糖尿病肥胖方面的可信度。*国际肥胖杂志*
    48, 271–275 (2024)。'
- en: '[22] Hoang, Y. N. *et al.* Consistency and accuracy of artificial intelligence
    for providing nutritional information. *JAMA network open*  6, e2350367–e2350367
    (2023).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Hoang, Y. N. *等*。人工智能在提供营养信息方面的一致性和准确性。*JAMA网络开放* 6, e2350367–e2350367
    (2023)。'
- en: '[23] Yang, Z. *et al.* ChatDiet: Empowering personalized nutrition-oriented
    food recommender chatbots through an LLM-augmented framework. *Smart Health*  32,
    100465 (2024).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yang, Z. *等*。ChatDiet：通过LLM增强框架赋能个性化营养导向的食品推荐聊天机器人。*智能健康* 32, 100465 (2024)。'
- en: '[24] Ponzo, V. *et al.* Is ChatGPT an Effective Tool for Providing Dietary
    Advice? *Nutrients*  16, 469 (2024).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Ponzo, V. *等*。ChatGPT是提供饮食建议的有效工具吗？*营养* 16, 469 (2024)。'
- en: '[25] Kirk, D., van Eijnatten, E. & Camps, G. Comparison of answers between
    ChatGPT and human dieticians to common nutrition questions. *Journal of Nutrition
    and Metabolism*  2023, 5548684 (2023).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Kirk, D., van Eijnatten, E. & Camps, G. ChatGPT与人工营养师在常见营养问题上的回答比较。*营养与代谢杂志*
    2023, 5548684 (2023)。'
- en: '[26] Szymanski, A., Wimer, B. L., Anuyah, O., Eicher-Miller, H. A. & Metoyer,
    R. A. Integrating Expertise in LLMs: Crafting a Customized Nutrition Assistant
    with Refined Template Instructions. In *Proceedings of the CHI Conference on Human
    Factors in Computing Systems*, 1–22 (2024).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Szymanski, A., Wimer, B. L., Anuyah, O., Eicher-Miller, H. A. & Metoyer,
    R. A. 在LLMs中整合专业知识：利用精炼模板指令打造定制营养助手。在*CHI计算机系统人因会议论文集*，1–22 (2024)。'
- en: '[27] Niszczota, P. & Rybicka, I. The credibility of dietary advice formulated
    by ChatGPT: robo-diets for people with food allergies. *Nutrition*  112, 112076
    (2023).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Niszczota, P. & Rybicka, I. ChatGPT制定的饮食建议的可信度：针对食物过敏者的机器人饮食。*营养* 112,
    112076 (2023)。'
- en: '[28] Minaee, S. *et al.* Large language models: A survey. *arXiv preprint arXiv:2402.06196*
    (2024).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Minaee, S. *等*。大型语言模型：一项调查。*arXiv预印本 arXiv:2402.06196* (2024)。'
- en: '[29] Sahoo, P. *et al.* A systematic survey of prompt engineering in large
    language models: Techniques and applications. *arXiv preprint arXiv:2402.07927*
    (2024).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Sahoo, P. *等*。大型语言模型中的提示工程系统调查：技术与应用。*arXiv预印本 arXiv:2402.07927* (2024)。'
- en: '[30] Chen, B., Zhang, Z., Langrené, N. & Zhu, S. Unleashing the potential of
    prompt engineering in large language models: a comprehensive review. *arXiv preprint
    arXiv:2310.14735* (2023).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Chen, B., Zhang, Z., Langrené, N. & Zhu, S. 大型语言模型中提示工程的潜力释放：一项全面的回顾。*arXiv预印本
    arXiv:2310.14735* (2023)。'
- en: '[31] Wang, J. *et al.* Prompt engineering for healthcare: Methodologies and
    applications. *arXiv preprint arXiv:2304.14670* (2023).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Wang, J. *等*。医疗保健中的提示工程：方法与应用。*arXiv预印本 arXiv:2304.14670* (2023)。'
- en: '[32] Maharjan, J. *et al.* OpenMedLM: prompt engineering can out-perform fine-tuning
    in medical question-answering with open-source large language models. *Scientific
    Reports*  14, 14156 (2024).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Maharjan, J. *et al.* OpenMedLM：在医学问答中，提示工程可以超越微调的表现。*Scientific Reports*
    14, 14156 (2024)。'
- en: '[33] Wang, L. *et al.* Prompt engineering in consistency and reliability with
    the evidence-based guideline for LLMs. *npj Digital Medicine*  7, 41 (2024).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Wang, L. *et al.* 提示工程在一致性和可靠性方面与 LLM 的基于证据的指南。*npj Digital Medicine*
    7, 41 (2024)。'
- en: '[34] Ouyang, S., Zhang, J. M., Harman, M. & Wang, M. LLM is Like a Box of Chocolates:
    the Non-determinism of ChatGPT in Code Generation. *arXiv preprint arXiv:2308.02828*
    (2023).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Ouyang, S., Zhang, J. M., Harman, M. & Wang, M. LLM 就像一盒巧克力：ChatGPT 在代码生成中的非确定性。*arXiv
    预印本 arXiv:2308.02828* (2023)。'
- en: '[35] OpenAI. Hello GPT-4o. [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)
    (2024). Accessed: August 2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] OpenAI. Hello GPT-4o。 [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)
    (2024)。访问时间：2024年8月。'
- en: '[36] Anthropic. Introducing Claude 3.5 Sonnet. [https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet)
    (2024). Accessed: August 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Anthropic. 介绍 Claude 3.5 Sonnet。 [https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet)
    (2024)。访问时间：2024年8月。'
- en: '[37] Google. Introducing Gemini 1.5, Google’s next-generation AI model. [https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)
    (2024). Accessed: August 2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Google. 介绍 Gemini 1.5，谷歌的下一代 AI 模型。 [https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)
    (2024)。访问时间：2024年8月。'
- en: '[38] Commission on Dietetic Registration. Registered Dietitian Nutritionist.
    [https://www.cdrnet.org/RDN](https://www.cdrnet.org/RDN). Accessed: August 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Commission on Dietetic Registration. 注册营养师。 [https://www.cdrnet.org/RDN](https://www.cdrnet.org/RDN)。访问时间：2024年8月。'
- en: '[39] Hendrycks, D. *et al.* Measuring massive multitask language understanding.
    *arXiv preprint arXiv:2009.03300* (2020).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Hendrycks, D. *et al.* 测量大规模多任务语言理解。*arXiv 预印本 arXiv:2009.03300* (2020)。'
- en: '[40] Rein, D. *et al.* GPQA: A Graduate-level Google-proof Q&A Benchmark. *arXiv
    preprint arXiv:2311.12022* (2023).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Rein, D. *et al.* GPQA：一个研究生水平的 Google 防范问答基准。*arXiv 预印本 arXiv:2311.12022*
    (2023)。'
- en: '[41] Dua, D. *et al.* DROP: A reading comprehension benchmark requiring discrete
    reasoning over paragraphs. *arXiv preprint arXiv:1903.00161* (2019).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Dua, D. *et al.* DROP：一个需要对段落进行离散推理的阅读理解基准。*arXiv 预印本 arXiv:1903.00161*
    (2019)。'
- en: '[42] Reid, M. *et al.* Gemini 1.5: Unlocking multimodal understanding across
    millions of tokens of context. *arXiv preprint arXiv:2403.05530* (2024).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Reid, M. *et al.* Gemini 1.5：解锁跨越数百万标记上下文的多模态理解。*arXiv 预印本 arXiv:2403.05530*
    (2024)。'
- en: '[43] Academy of Nutrition and Dietetics . [https://www.eatrightprep.org/](https://www.eatrightprep.org/).
    Accessed: August 2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Academy of Nutrition and Dietetics。 [https://www.eatrightprep.org/](https://www.eatrightprep.org/)。访问时间：2024年8月。'
- en: '[44] Grabb, D. The impact of prompt engineering in large language model performance:
    a psychiatric example. *Journal of Medical Artificial Intelligence*  6 (2023).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Grabb, D. 提示工程对大型语言模型表现的影响：一个精神病学的例子。*Journal of Medical Artificial Intelligence*
    6 (2023)。'
- en: '[45] Russe, M. F., Reisert, M., Bamberg, F. & Rau, A. Improving the use of
    LLMs in radiology through prompt engineering: from precision prompts to zero-shot
    learning. In *RöFo-Fortschritte auf dem Gebiet der Röntgenstrahlen und der bildgebenden
    Verfahren* (Georg Thieme Verlag KG, 2024).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Russe, M. F., Reisert, M., Bamberg, F. & Rau, A. 通过提示工程改善 LLM 在放射学中的应用：从精准提示到零样本学习。见
    *RöFo-Fortschritte auf dem Gebiet der Röntgenstrahlen und der bildgebenden Verfahren*
    (Georg Thieme Verlag KG, 2024)。'
- en: '[46] Mishra, V., Jafri, F., Abdul Kareem, N., Aboobacker, R. & Noora, F. Evaluation
    of accuracy and potential harm of ChatGPT in medical nutrition therapy-a case-based
    approach. *F1000Research*  13, 137 (2024).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Mishra, V., Jafri, F., Abdul Kareem, N., Aboobacker, R. & Noora, F. 评估
    ChatGPT 在医学营养治疗中的准确性和潜在危害——基于案例的方法。*F1000Research* 13, 137 (2024)。'
- en: '[47] Wei, J. *et al.* Chain-of-Thought Prompting Elicits Reasoning in Large
    Language Models. *Advances in neural information processing systems*  35, 24824–24837
    (2022).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Wei, J. *et al.* Chain-of-Thought 提示激发大型语言模型的推理能力。*Advances in neural
    information processing systems* 35, 24824–24837 (2022)。'
- en: '[48] Gou, Z. *et al.* ToRA: A Tool-Integrated Reasoning Agent for Mathematical
    Problem Solving. *arXiv preprint arXiv:2309.17452* (2023).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Gou, Z. *et al.* ToRA：一个工具集成的数学问题求解推理代理。*arXiv 预印本 arXiv:2309.17452* (2023)。'
- en: '[49] Abbasian, M., Azimi, I., Rahmani, A. M. & Jain, R. Conversational health
    agents: A personalized LLM-powered agent framework. *arXiv preprint arXiv:2310.02374*
    (2023).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Abbasian, M., Azimi, I., Rahmani, A. M. & Jain, R. 对话健康代理：一个个性化的LLM驱动代理框架。
    *arXiv预印本 arXiv:2310.02374* (2023)。'
- en: '[50] Wang, X. *et al.* Self-Consistency Improves Chain of Thought Reasoning
    in Language Models. *arXiv preprint arXiv:2203.11171* (2022).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Wang, X. *等* 自一致性提升语言模型中的思维链推理。 *arXiv预印本 arXiv:2203.11171* (2022)。'
- en: '[51] Gao, Y. *et al.* Retrieval-augmented generation for large language models:
    A survey. *arXiv preprint arXiv:2312.10997* (2023).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Gao, Y. *等* 用于大型语言模型的检索增强生成：综述。 *arXiv预印本 arXiv:2312.10997* (2023)。'
- en: '[52] Meta AI. Introducing Meta Llama 3: The most capable openly available LLM
    to date. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)
    (2024). Accessed: August 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Meta AI. 推出Meta Llama 3：迄今为止最强大的开放式LLM。 [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)
    (2024)。访问时间：2024年8月。'
- en: '[53] Technology Innovation Institute. Falcon LLM. [https://falconllm.tii.ae/](https://falconllm.tii.ae/)
    (2024). Accessed: August 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Technology Innovation Institute. Falcon LLM。 [https://falconllm.tii.ae/](https://falconllm.tii.ae/)
    (2024)。访问时间：2024年8月。'
- en: '[54] 01.AI. Yi-34B. [https://huggingface.co/01-ai/Yi-34B](https://huggingface.co/01-ai/Yi-34B)
    (2024). Accessed: August 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 01.AI. Yi-34B。 [https://huggingface.co/01-ai/Yi-34B](https://huggingface.co/01-ai/Yi-34B)
    (2024)。访问时间：2024年8月。'
- en: '[55] Sun, L. *et al.* TrustLLM: Trustworthiness in large language models. *arXiv
    preprint arXiv:2401.05561* (2024).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Sun, L. *等* TrustLLM：大型语言模型中的可信度。 *arXiv预印本 arXiv:2401.05561* (2024)。'
- en: '[56] Xu, L., Xie, H., Qin, S.-Z. J., Tao, X. & Wang, F. L. Parameter-efficient
    fine-tuning methods for pretrained language models: A critical review and assessment.
    *arXiv preprint arXiv:2312.12148* (2023).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Xu, L., Xie, H., Qin, S.-Z. J., Tao, X. & Wang, F. L. 预训练语言模型的参数高效微调方法：批判性综述与评估。
    *arXiv预印本 arXiv:2312.12148* (2023)。'
- en: '[57] Singhal, K. *et al.* Large language models encode clinical knowledge.
    *Nature*  620, 172–180 (2023).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Singhal, K. *等* 大型语言模型编码临床知识。 *Nature* 620, 172–180 (2023)。'
- en: '[58] Zhang, X. *et al.* Comparison of prompt engineering and fine-tuning strategies
    in large language models in the classification of clinical notes. *AMIA Summits
    on Translational Science Proceedings*  2024, 478 (2024).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Zhang, X. *等* 大型语言模型在临床笔记分类中的提示工程与微调策略比较。 *AMIA Summits on Translational
    Science Proceedings* 2024, 478 (2024)。'
- en: '[59] Li, Y. *et al.* Personal LLM agents: Insights and survey about the capability,
    efficiency and security. *arXiv preprint arXiv:2401.05459* (2024).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Li, Y. *等* 个人LLM代理：关于能力、效率和安全性的洞察与调查。 *arXiv预印本 arXiv:2401.05459* (2024)。'
- en: '[60] Commission on Dietetic Registration, Registered Dietitian (Rd) Or Registered
    Dietitian Nutritionist (Rdn) Certification. [https://www.cdrnet.org/RDN](https://www.cdrnet.org/RDN).
    Accessed: August 2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Commission on Dietetic Registration, 注册营养师（RD）或注册营养师营养师（RDN）认证。 [https://www.cdrnet.org/RDN](https://www.cdrnet.org/RDN)。访问时间：2024年8月。'
- en: '[61] Commission on Dietetic Registration. *Candidate Handbook, RD Exam* (Academy
    of Nutrition and Dietetics, 2024). URL [https://admin.cdrnet.org/vault/2459/web//RD%20Handbook%20for%20Candidates%20-%206-2024.pdf](https://admin.cdrnet.org/vault/2459/web//RD%20Handbook%20for%20Candidates%20-%206-2024.pdf).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Commission on Dietetic Registration. *考生手册，RD考试*（营养与饮食学学会，2024年）。网址 [https://admin.cdrnet.org/vault/2459/web//RD%20Handbook%20for%20Candidates%20-%206-2024.pdf](https://admin.cdrnet.org/vault/2459/web//RD%20Handbook%20for%20Candidates%20-%206-2024.pdf)。'
- en: '[62] Chatbot Arena. Chat with Open Large Language Models. [https://chat.lmsys.org/?leaderboard](https://chat.lmsys.org/?leaderboard).
    Accessed: August 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Chatbot Arena. 与开放的大型语言模型聊天。 [https://chat.lmsys.org/?leaderboard](https://chat.lmsys.org/?leaderboard)。访问时间：2024年8月。'
- en: '[63] Artificial Analysis. Model & API Providers Analysis. [https://artificialanalysis.ai/](https://artificialanalysis.ai/).
    Accessed: August 2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Artificial Analysis. 模型与API提供商分析。 [https://artificialanalysis.ai/](https://artificialanalysis.ai/)。访问时间：2024年8月。'
- en: '[64] Peeperkorn, M., Kouwenhoven, T., Brown, D. & Jordanous, A. Is temperature
    the creativity parameter of large language models? *arXiv preprint arXiv:2405.00492*
    (2024).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Peeperkorn, M., Kouwenhoven, T., Brown, D. & Jordanous, A. 温度是否是大型语言模型的创造力参数？
    *arXiv预印本 arXiv:2405.00492* (2024)。'
- en: '[65] Holmes, J. *et al.* Evaluating large language models on a highly-specialized
    topic, radiation oncology physics. *Frontiers in Oncology*  13, 1219326 (2023).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Holmes, J. *等* 在高度专业化的领域——放射肿瘤物理学中评估大型语言模型。 *Frontiers in Oncology* 13,
    1219326 (2023)。'
- en: '[66] Li, Y. *et al.* ChatDoctor: A Medical Chat Model Fine-Tuned on a Large
    Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. *Cureus*  15 (2023).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Li, Y. *等人* ChatDoctor：基于医疗领域知识对大型语言模型Meta-AI (LLaMA)的微调医学聊天模型。*Cureus*
    15 (2023)。'
- en: '[67] Sebastien Stormacq. Amazon Titan Text Embeddings V2 now available in Amazon
    Bedrock, optimized for improving RAG. [https://aws.amazon.com/blogs/aws/amazon-titan-text-v2-now-available-in-amazon-bedrock-optimized-for-improving-rag/](https://aws.amazon.com/blogs/aws/amazon-titan-text-v2-now-available-in-amazon-bedrock-optimized-for-improving-rag/)
    (2024). Accessed: August 2024.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Sebastien Stormacq. Amazon Titan Text Embeddings V2 现已在 Amazon Bedrock
    中推出，优化以提升RAG性能。 [https://aws.amazon.com/blogs/aws/amazon-titan-text-v2-now-available-in-amazon-bedrock-optimized-for-improving-rag/](https://aws.amazon.com/blogs/aws/amazon-titan-text-v2-now-available-in-amazon-bedrock-optimized-for-improving-rag/)
    (2024)。访问时间：2024年8月。'
- en: '[68] Manning, C. D., Raghavan, P. & Schütze, H. Scoring, term weighting & the
    vector space model. In *Introduction to Information Retrieval* (Cambridge University
    Press, 2008).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Manning, C. D., Raghavan, P. & Schütze, H. 打分、术语加权与向量空间模型。在*信息检索导论*（剑桥大学出版社，2008）。'
- en: '[69] Libraries, OpenAI API. [https://platform.openai.com/docs/libraries/python-library](https://platform.openai.com/docs/libraries/python-library).
    Accessed: August 2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] 图书馆，OpenAI API. [https://platform.openai.com/docs/libraries/python-library](https://platform.openai.com/docs/libraries/python-library)。访问时间：2024年8月。'
- en: '[70] google-generativeai, Gemini API. [https://pypi.org/project/google-generativeai/](https://pypi.org/project/google-generativeai/).
    Accessed: August 2024.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] google-generativeai, Gemini API. [https://pypi.org/project/google-generativeai/](https://pypi.org/project/google-generativeai/)。访问时间：2024年8月。'
- en: '[71] AWS SDK for Python (Boto3) Documentation. [https://docs.aws.amazon.com/pythonsdk/](https://docs.aws.amazon.com/pythonsdk/).
    Accessed: August 2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] AWS SDK for Python (Boto3) 文档。 [https://docs.aws.amazon.com/pythonsdk/](https://docs.aws.amazon.com/pythonsdk/)。访问时间：2024年8月。'
- en: '[72] Stefan Behnel. The lxml.etree Tutorial. [https://lxml.de/tutorial.html](https://lxml.de/tutorial.html).
    Accessed: August 2024.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Stefan Behnel. lxml.etree 教程。 [https://lxml.de/tutorial.html](https://lxml.de/tutorial.html)。访问时间：2024年8月。'
- en: '[73] Hallgren, K. A. Computing inter-rater reliability for observational data:
    an overview and tutorial. *Tutorials in quantitative methods for psychology*  8,
    23 (2012).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Hallgren, K. A. 计算观察数据的评估者间一致性：概述与教程。*心理学定量方法教程* 8, 23 (2012)。'
- en: '[74] Cohen, J. A coefficient of agreement for nominal scales. *Educational
    and psychological measurement*  20, 37–46 (1960).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Cohen, J. 名义尺度的一致性系数。*教育与心理测量* 20, 37–46 (1960)。'
- en: '[75] Fleiss, J. L. Measuring nominal scale agreement among many raters. *Psychological
    bulletin*  76, 378 (1971).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Fleiss, J. L. 测量多评估者间名义尺度的一致性。*心理学通报* 76, 378 (1971)。'
- en: '[76] Gamer, M., Lemon, J. & Singh, I. F. P. *irr: Various Coefficients of Interrater
    Reliability and Agreement* (2019). URL [https://CRAN.R-project.org/package=irr](https://CRAN.R-project.org/package=irr).
    R package version 0.84.1.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Gamer, M., Lemon, J. & Singh, I. F. P. *irr: 各种评估者间一致性和协议系数*（2019）。网址
    [https://CRAN.R-project.org/package=irr](https://CRAN.R-project.org/package=irr)。R包版本
    0.84.1。'
- en: '[77] Canty, A., Ripley, B. & Brazzale, A. R. *boot: Bootstrap Functions* (2024).
    URL [https://CRAN.R-project.org/package=boot](https://CRAN.R-project.org/package=boot).
    R package version 1.3-30.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Canty, A., Ripley, B. & Brazzale, A. R. *boot: Bootstrap 函数*（2024）。网址
    [https://CRAN.R-project.org/package=boot](https://CRAN.R-project.org/package=boot)。R包版本
    1.3-30。'
- en: See pages - of [supplementary.pdf](supplementary.pdf)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[补充材料.pdf](supplementary.pdf)的页面
