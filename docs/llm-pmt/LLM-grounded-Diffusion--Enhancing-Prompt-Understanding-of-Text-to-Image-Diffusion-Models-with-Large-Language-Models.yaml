- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:54'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
    Models with Large Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.13655](https://ar5iv.labs.arxiv.org/html/2305.13655)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Long Lian¹   Boyi Li¹   Adam Yala^(1,2)   Trevor Darrell¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹UC Berkeley   ²UCSF
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Recent advancements in text-to-image diffusion models have yielded impressive
    results in generating realistic and diverse images. However, these models still
    struggle with complex prompts, such as those that involve numeracy and spatial
    reasoning. This work proposes to enhance prompt understanding capabilities in
    diffusion models. Our method leverages a pretrained large language model (LLM)
    for grounded generation in a novel two-stage process. In the first stage, the
    LLM generates a scene layout that comprises captioned bounding boxes from a given
    prompt describing the desired image. In the second stage, a novel controller guides
    an off-the-shelf diffusion model for layout-grounded image generation. Both stages
    utilize existing pretrained models without additional model parameter optimization.
    Our method significantly outperforms the base diffusion model and several strong
    baselines in accurately generating images according to prompts that require various
    capabilities, doubling the generation accuracy across four tasks on average. Furthermore,
    our method enables instruction-based multi-round scene specification and can handle
    prompts in languages not supported by the underlying diffusion model. We anticipate
    that our method will unleash users’ creativity by accurately following more complex
    prompts.¹¹1Project page: [https://llm-grounded-diffusion.github.io](https://llm-grounded-diffusion.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d89d108e1af9b04062fd9dbb1bf7dec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (a) Stable Diffusion Rombach et al. ([2022](#bib.bib29)) often struggles
    to accurately follow prompts that involve negation, numeracy, attribute binding,
    and spatial relationships. (b) Our method LMD achieves enhanced prompt understanding
    capabilities and accurately follows these types of prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eba90321fdcde5d2667c29dded175194.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Our proposed LMD enhances prompt understanding in text-to-image diffusion
    models through a novel two-stage generation process: 1) An LLM layout generator
    takes a prompt from the user and outputs an image layout in the form of captioned
    bounding boxes. 2) A stable diffusion model guided by our layout-grounded controller
    generates the final image. Both stages utilize frozen pretrained models, which
    makes our method applicable to off-the-shelf LLMs and other diffusion models without
    grounding in their training objective.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a44d4e0f394a3c46c48588a8ef21415.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: LMD is naturally able to (a) perform instruction-based multi-round
    scene specification and (b) generate images from prompts in languages not supported
    by the underlying diffusion model.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The field of text-to-image generation has witnessed significant advancements,
    particularly with the emergence of diffusion models. These models have showcased
    remarkable capabilities in generating realistic and diverse images in response
    to textual prompts. However, despite the impressive results, diffusion models
    often struggle to accurately follow complex prompts that require specific capabilities
    to understand. [Fig. 1](#S0.F1 "In LLM-grounded Diffusion: Enhancing Prompt Understanding
    of Text-to-Image Diffusion Models with Large Language Models") shows that Stable
    Diffusion Rombach et al. ([2022](#bib.bib29)) could not generate a certain number
    of objects or understand negation in the prompt. It also struggles with spatial
    reasoning or associating attributes correctly with objects.'
  prefs: []
  type: TYPE_NORMAL
- en: One potential solution to address this issue is of course to gather a comprehensive
    multi-modal dataset comprising intricate captions and train a text-to-image diffusion
    model for enhanced prompt understanding. Nonetheless, this approach presents notable
    drawbacks. It requires considerable time and resources to curate a diverse and
    high-quality multi-modal dataset, not to mention the challenges associated with
    training or fine-tuning a diffusion model on such extensive data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, we propose a novel training-free method that equips the diffusion
    model with an LLM that provides grounding for enhanced prompt understanding. Our
    method LLM-grounded Diffusion (LMD) consists of a two-stage generation process
    as shown in [Fig. 2](#S0.F2 "In LLM-grounded Diffusion: Enhancing Prompt Understanding
    of Text-to-Image Diffusion Models with Large Language Models").'
  prefs: []
  type: TYPE_NORMAL
- en: In the first stage of our method, we adapt an LLM to be a text-grounded layout
    generator through in-context learning. Given a prompt describing the desired image,
    the LLM generates scene layouts in the form of captioned bounding boxes, along
    with a background caption.
  prefs: []
  type: TYPE_NORMAL
- en: In the second stage, we introduce a novel controller that guides an existing
    diffusion model without grounding in its training objective (e.g., Stable Diffusion)
    to follow the layout grounding generated in the first stage. In contrast to previous
    and concurrent works on region control (e.g., Bar-Tal et al. ([2023](#bib.bib3));
    Chen et al. ([2023](#bib.bib6)); Xie et al. ([2023](#bib.bib37))) that apply semantic
    control to certain spatial regions, our approach allows precise control over object
    instances in designated regions.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, both stages utilize frozen pretrained models off-the-shelf, making
    our method applicable to LLMs and diffusion models trained independently without
    any LLM or diffusion model parameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to enhanced prompt understanding, our method also enables instruction-based
    multi-round scene specification ([Fig. 3](#S0.F3 "In LLM-grounded Diffusion: Enhancing
    Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models")(a))
    and image generation from prompts in languages not supported by the base diffusion
    model ([Fig. 3](#S0.F3 "In LLM-grounded Diffusion: Enhancing Prompt Understanding
    of Text-to-Image Diffusion Models with Large Language Models")(b)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Fig. 1](#S0.F1 "In LLM-grounded Diffusion: Enhancing Prompt Understanding
    of Text-to-Image Diffusion Models with Large Language Models"), our LMD provides
    a unified solution to several caveats in prompt understanding at once and enables
    accurate and high-quality image generation from complex prompts. We demonstrate
    that a diffusion model, grounded with an LLM-generated layout using LMD, outperforms
    its base diffusion model and several strong baselines, doubling the average generation
    accuracy across four benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our work makes four main contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We improve prompt understanding in text-to-image diffusion models by leveraging
    LLMs in a training-free two-stage generation process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We introduce a novel controller that steers a diffusion model to generate images
    grounded on bounding box layouts from the LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LMD enables instruction-based scene specification and allows broader language
    support for the input prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We propose a benchmark to assess the prompt-following ability of a text-to-image
    model and demonstrate the superior performance of LMD.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We expect LMD to empower users with more precise control of text-to-image diffusion
    models. [Our code and benchmark have been released](https://llm-grounded-diffusion.github.io/).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-image diffusion models. High-quality image generation from textual descriptions
    has been popular Ramesh et al. ([2022](#bib.bib28)); Saharia et al. ([2022](#bib.bib32));
    Rombach et al. ([2022](#bib.bib29)), especially with Latent/Stable Diffusion Rombach
    et al. ([2022](#bib.bib29)) that proposed denoising in the latent space and decoding
    the denoised latents to high-resolution pixel space. However, these models tends
    to exhibit subpar performance when it comes to complex prompts that require skills
    such as binding attributes to objects and spatial reasoning Ramesh et al. ([2022](#bib.bib28)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to our instruction-based scene specification, Brooks et al. ([2023](#bib.bib4))
    recently proposed instruction-based image editing with diffusion models. Wu et al.
    ([2023](#bib.bib36)) and Gupta and Kembhavi ([2023](#bib.bib11)) also allow using
    external image editing models in an LLM-driven dialog. Different from these methods,
    we aim to edit the scene layout rather than the image pixels, which easily allows
    a greater set of instructions such as swapping/moving objects. We refer readers
    to [Appendix A](#A1 "Appendix A Preliminary introduction on diffusion models ‣
    LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
    Models with Large Language Models") for a brief introduction to diffusion models.'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs for visual grounding. Many multimodal models benefit from integrating LLMs
    for grounding vision models. BLIP Li et al. ([2023a](#bib.bib17)) bootstraps vision-language
    pre-training from frozen image encoders and LLMs. Flamingo Alayrac et al. ([2022](#bib.bib1))
    tackles tasks such as few-shot visual question-answering and captioning tasks.
    Beyond, Rozanova et al. ([2021](#bib.bib31)) finds that document-based models
    can learn a reasonable amount of spatially relevant features that make them transferable
    to the UI grounding task. Ghanimifard and Dobnik ([2019](#bib.bib9)) reveals that
    the language model possesses the capability to differentiate between the functional
    and geometric biases of spatial relations through encoding, despite lacking access
    to visual features of the scene. Gupta et al. ([2021](#bib.bib10)) uses Transformer Vaswani
    et al. ([2017](#bib.bib35)) for layout prediction, but it focuses on generating
    layouts for a limited closed set of object classes in the annotated training set.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b6721ea6fca7e5a90d0e1978122ea80b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: In stage 1, to generate an image layout from a user prompt, we first
    embed the user prompt into a template with instructions and in-context examples.
    We then query a frozen LLM for completion. Finally, we parse the LLM completion
    to obtain a set of captioned bounding boxes and a background caption.'
  prefs: []
  type: TYPE_NORMAL
- en: Spatially-conditioned image generation creates new content based on given prior
    visual knowledge such as pose, segmentation map, stroke, and layout. Prior to
    the popularity of diffusion models, SPADE Park et al. ([2019](#bib.bib27)), BlobGAN Epstein
    et al. ([2022](#bib.bib8)), and Layout2Im Zhao et al. ([2019](#bib.bib41)) synthesize
    photorealistic images by a given layout. Xu et al. ([2017](#bib.bib38)); Johnson
    et al. ([2018](#bib.bib15)); Herzig et al. ([2020](#bib.bib12)) generate images
    with scene graphs. ControlNet Zhang and Agrawala ([2023](#bib.bib40)), GLIGEN Li
    et al. ([2023b](#bib.bib18)) and ReCo Yang et al. ([2023](#bib.bib39)) propose
    training-based adaptation on the diffusion models for spatially-conditioned image
    generation. However, these methods rely on annotated external datasets such as
    COCO Lin et al. ([2014](#bib.bib19)) to supply images with annotations such as
    boxes and captions. Furthermore, training-based adaptation not only makes the
    model incompatible to add-ons such as pretrained LoRA Hu et al. ([2021](#bib.bib14))
    weights but also renders it difficult to train a new LoRA model, since the adapted
    model requires additional spatial annotations for LoRA training set.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, we propose a training-free generation controller that steers existing
    text-to-image diffusion models that are not specifically trained for layout-grounded
    image generation and does not require external datasets. Furthermore, our method
    can also integrate with training-based methods for further improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Very recently, Bar-Tal et al. ([2023](#bib.bib3)); Avrahami et al. ([2023](#bib.bib2));
    Chen et al. ([2023](#bib.bib6)) allow training-free region control in image generation
    and share a similar task formulation with our layout-to-image stage. However,
    both works ground the image generation on the semantics of the region and pose
    little control over the number of object instances inside one semantic region,
    whereas our method focuses on grounding generation on object instances.
  prefs: []
  type: TYPE_NORMAL
- en: 3 LLM-grounded Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce our method LLM-grounded Diffusion (LMD). LMD
    focuses on the standard text-to-image generation setting: generate image $\mathbf{x}_{0}$
    and decoding to pixel space. Our method generates an image in two stages: text-grounded
    layout generation ([Section 3.1](#S3.SS1 "3.1 LLM-based Layout Generation ‣ 3
    LLM-grounded Diffusion ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding
    of Text-to-Image Diffusion Models with Large Language Models")) and layout-grounded
    image generation ([Section 3.2](#S3.SS2 "3.2 Layout-grounded Stable Diffusion
    ‣ 3 LLM-grounded Diffusion ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding
    of Text-to-Image Diffusion Models with Large Language Models")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f1c1850491d172df2f6b969a6e90d512.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: In stage 2, we introduce a novel layout-grounded controller that
    guides stable diffusion to generate images based on the layout obtained from the
    previous stage. Our layout-grounded image generation process consists of two steps: (a)
    generating masked latents for each box specified in the layout, with attention
    control ensuring that the object is placed in the designated box; and (b) composing
    the masked latents coherently to generate images that adhere to the specified
    foreground boxes and background caption.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 LLM-based Layout Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To generate the layout of an image, our method embed the input text prompt
    $\mathbf{y}$ into a template and query the LLM for completion ([Fig. 4](#S2.F4
    "In 2 Related Work ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of
    Text-to-Image Diffusion Models with Large Language Models")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layout representation of LMD comprises two components: 1) a captioned bounding
    box for each foreground object, with coordinates specified in the (x, y, width,
    height) format, and 2) a simple and concise caption describing the image background
    along with a negative prompt indicating what should not appear in a generated
    image. The negative prompt is empty when the layout does not impose restrictions
    on what should not appear.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompting. Our prompt to LLM has two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Task specification:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Your task is to generate the
    bounding boxes for the objects mentioned in the caption, along with a background
    prompt describing the scene.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Supporting details:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The images are of size 512x512…
    Each bounding box should be in the format of … If needed, you can make reasonable
    guesses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In-context learning. Similar to Brooks et al. ([2023](#bib.bib4)), we provide
    the LLM with manually curated examples after the task description. Through these
    examples, we clarify the layout representation and provide preferences to disperse
    ambiguity.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Caption: A watercolor painting
    of a wooden table in the living room with an apple on it'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objects: [(‘a wooden table’, [65, 243, 344, 206]), (‘an apple’, [206, 306,
    81, 69])]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Background prompt: A watercolor painting of a living room'
  prefs: []
  type: TYPE_NORMAL
- en: Negative prompt:
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure precise layout control, we adhere to two key principles in our example
    design: 1) Each object instance is represented by a single bounding box. For instance,
    if the prompt mentions four apples, we include four boxes with the word "apple"
    included in the caption. 2) We leave no foreground objects specified in the boxes
    to the background caption to ensure all foreground objects are controlled by our
    layout-grounded image generator ([Section 3.2](#S3.SS2 "3.2 Layout-grounded Stable
    Diffusion ‣ 3 LLM-grounded Diffusion ‣ LLM-grounded Diffusion: Enhancing Prompt
    Understanding of Text-to-Image Diffusion Models with Large Language Models")).
    These principles allow for accurate and controlled layout generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM completion. After providing examples, we prompt the LLM for completion²²2We
    use [Text Completion](https://platform.openai.com/docs/guides/gpt/completions-api)/[Chat
    Completion](https://platform.openai.com/docs/api-reference/chat) API to query
    the LLM for completion.:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Caption: [input prompt from
    the user]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objects: [start of LLM completion]'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting layout from the LLM completion is then parsed and used for the
    subsequent image generation process. We refer readers to the [Appendix F](#A6
    "Appendix F Our LLM prompt ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding
    of Text-to-Image Diffusion Models with Large Language Models") for our complete
    prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Layout-grounded Stable Diffusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this stage, we introduce a controller to ground the image generation on the
    LLM-generated layout. While previous training-free region control methods Bar-Tal
    et al. ([2023](#bib.bib3)); Chen et al. ([2023](#bib.bib6)) apply semantic guidance
    through regional latent denoising or attention manipulation, these methods lack
    the ability to control the number of objects within a semantic region. This limitation
    arises as the different instances are often indistinguishable in either the latent
    space or the attention map, hindering the control over object instances.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, LMD enables instance-level grounding by first generating masked
    latents for each individual bounding box and then composing the masked latents
    for overall image generation. This allows for precise placement and attribute
    binding for each object instance.
  prefs: []
  type: TYPE_NORMAL
- en: Per-box masked latents. While diffusion models lack inherent instance-level
    distinction in their latent space or attention maps for fine-grained control,
    we observe that they can generate images with one specified instance. Hence, we
    process one foreground box at a time for instance-level grounding.
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in [Fig. 5](#S3.F5 "In 3 LLM-grounded Diffusion ‣ LLM-grounded
    Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with
    Large Language Models")(a), for each foreground object $i$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure the object aligns with the bounding box, we manipulate the cross-attention
    maps $\mathbf{A}^{(i)}$ of the noise-prediction network.³³3The cross-attention
    layer index is omitted for simplicity. We sum the energy values for all layers
    during optimization. Each map describes the affinity from pixels to text tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{A}^{(i)}_{uv}=\texttt{Softmax}(\mathbf{q}_{u}^{T}\mathbf{k}_{v})$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{q}_{u}$ in the prompt, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following Chen et al. ([2023](#bib.bib6)); Xie et al. ([2023](#bib.bib37)),
    we strengthen the cross-attention from pixels inside the box to tokens associated
    with the box caption while attenuating the cross-attention from pixels outside
    the box. To achieve this, we define a simple energy function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle E(\mathbf{A}^{(i)},i,v)=-$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\omega\texttt{Topk}_{u}(\mathbf{A}_{uv}\cdot(1-\mathbf{m}_{i}))$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\cdot$. The energy function is minimized by updating the latent before
    each denoising step:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{z}^{(i)}_{t}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{z}^{(i)}_{t-1}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\eta$ contains the token indices for the box caption in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'After generation, we obtain the cross-attention map that corresponds to the
    box caption, which serves as a saliency mask for the object. We optionally use
    SAM Kirillov et al. ([2023](#bib.bib16)) to refine the quality of the mask. This
    can be done by querying either with the pixel location that has the highest saliency
    value or with the layout box.⁴⁴4The precision of instance masks could be further
    improved by using an open vocabulary instance segmentation model with box captions
    as queries. We leave this to future research. With mask $m^{(i)}$ for exactly
    one foreground instance, we perform element-wise multiplication between the mask
    and the latent at each denoising step:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{\hat{z}}^{(i)}_{t}=\mathbf{z}^{(i)}_{t}\otimes m^{(i)}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'Composed image generation. As illustrated in [Fig. 5](#S3.F5 "In 3 LLM-grounded
    Diffusion ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image
    Diffusion Models with Large Language Models")(b), at each denoising time step
    $t$, replacing the original content in the masked region:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{z}^{(\text{comp})}_{t}\leftarrow\texttt{Compose}(\mathbf{z}^{(\text{comp})}_{t},\mathbf{\hat{z}}^{(i)}_{t})\quad\forall
    i$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{z}^{(\text{comp})}_{T}$ for foreground generation for consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'We further transfer the cross-attention maps from per-box generation to the
    corresponding regions in the composed generation to enhance instance-level guidance
    through adapting the energy function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle E^{(\text{comp})}(\mathbf{A}^{(\text{comp})},\mathbf{A}^{(i)},i,v)=$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\quad E(\mathbf{A}^{(i)},i,v)+\lambda\sum_{u\in B_{i}}\Big{&#124;}\mathbf{A}^{(\text{comp})}_{uv}-\mathbf{A}^{(i)}_{uv}\Big{&#124;}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda=2.0$ is summed up for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models tend to generate object placement in the initial denoising
    steps and then refine the details in later steps Bar-Tal et al. ([2023](#bib.bib3)).
    Taking this into consideration, we only compose the latents from timestep $T$
    balances instance control and image coherency. By primarily intervening during
    the steps for object placement, our method allows the diffusion model to optimize
    for more coherent generation (e.g., making adjustments to match the interactions)
    while still adhering to the instance specification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we decode latents $\mathbf{z}^{(\text{comp})}_{0}$ via the diffusion
    image decoder. Please refer to [Appendix B](#A2 "Appendix B Pseudo-code for layout-grounded
    image generation ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image
    Diffusion Models with Large Language Models") for the pseudo-code for layout-grounding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c53f1ec7ce2b6c1bfc5d3a8bbd1f03db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: LMD supports instruction-based scene specification, empowering the
    users to add/move/remove objects and modify object attributes in multiple rounds
    of dialog. (a): the initial prompt for the scene; (b)-(i): eight subsequent instructions
    that sequentially modify the scene. By separating the generation of each foreground
    object as well as the background, LMD ensures consistent image generation when
    the same seed is used for image generation throughout the dialog.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/122f85e8b0d2fd83b045b629a3a3b9bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: LMD outperforms its base text-to-image diffusion model Rombach et al.
    ([2022](#bib.bib29)) in accurately following the prompts that require spatial
    and language reasoning. Best viewed when zoomed in.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Integration with training-based methods. Our training-free guidance method
    can also be integrated with training-based methods such as GLIGEN Li et al. ([2023b](#bib.bib18))
    to leverage instance-annotated external datasets when available. GLIGEN inserts
    and trains adapter layers taking box inputs. The integration with GLIGEN, denoted
    as LMD+, involves adopting its adapter weights and passing the layout guidance
    to the adapter layers. Note that LMD+ uses adapters along with the cross-attention
    guidance introduced above, which greatly surpasses using only the GLIGEN adapters,
    as shown in [Table 2](#S4.T2 "In 4.1 Qualitative Comparison ‣ 4 Evaluation ‣ LLM-grounded
    Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with
    Large Language Models"). We achieve further enhanced instance and attribute control
    without additional training through this integration.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Additional Capabilities of LMD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our LLM-grounded generation pipeline allows for two additional capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruction-based scene specification. Leveraging an LLM that supports multi-round
    dialog (e.g., GPT-3.5/4), LMD empowers the users to specify the desired image
    with multiple instructions following an initial prompt ([Fig. 3](#S0.F3 "In LLM-grounded
    Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with
    Large Language Models")(a)). Specifically, after the initial image generation,
    users can simply instruct the LLM to update the layout and then generate images
    with the updated layout. Since we edit the layout rather than the raw image, our
    generation remains consistent after multiple rounds of requests and can handle
    requests, such as swapping objects, that were unachievable through previous instruction-based
    image editing method Brooks et al. ([2023](#bib.bib4)), as demonstrated in [Fig. 6](#S3.F6
    "In 3.2 Layout-grounded Stable Diffusion ‣ 3 LLM-grounded Diffusion ‣ LLM-grounded
    Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with
    Large Language Models"). This capability applies to both LMD and LMD+.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supporting more languages. By giving an in-content example of a non-English
    user prompt and English captions⁵⁵5We simply translate the input prompt of our
    last in-context example, while keeping the layout captions in English., the LLM
    layout generator accepts non-English user prompts and outputs layouts with English
    captions. This allows generation from prompts in languages unsupported by the
    underlying diffusion model without re-training ([Fig. 3](#S0.F3 "In LLM-grounded
    Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with
    Large Language Models")(b)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Qualitative Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We qualitatively compare our approach with Stable Diffusion (SD, Rombach et al.
    ([2022](#bib.bib29))), our base model for layout-grounded image generation. SD
    is chosen as our base model given its strong capabilities and widespread adoption
    in text-to-image generation research. We use gpt-4 model OpenAI ([2023](#bib.bib26))
    for layout generation for qualitative comparison, unless stated otherwise. In
    [Fig. 7](#S3.F7 "In 3.2 Layout-grounded Stable Diffusion ‣ 3 LLM-grounded Diffusion
    ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
    Models with Large Language Models"), we observe that our two-stage text-to-image
    generation approach greatly enhances the prompt understanding compared to our
    base model by generating images that align with the LLM-generated layouts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also compare with semantic-controlled image generation methods in [Appendix C](#A3
    "Appendix C Qualitative comparison with semantic-grounded image generation methods
    ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
    Models with Large Language Models"). Our method shows superior instance-level
    control while still being training-free.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Tasks | SD | LMD | LMD+ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Negation | $28\%$) |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Numeracy | $39\%$) |'
  prefs: []
  type: TYPE_TB
- en: '| Attribute Binding | $52\%$) |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial Relationships | $28\%$) |'
  prefs: []
  type: TYPE_TB
- en: '| Average | $37\%$) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: With guidance from the LLM-based layout generator and our novel layout-grounded
    controller, our LMD significantly outperforms the Stable Diffusion model (SD,
    Rombach et al. ([2022](#bib.bib29))) that we use under the hood in four tasks
    that benchmark prompt understanding. LMD represents our method directly applied
    on SD. LMD+ denotes additionally integrating GLIGEN Li et al. ([2023b](#bib.bib18))
    into our controller without additional training.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Stage 1/Stage 2 | Negation | Numeracy | Attribute | Spatial | Average |  |'
  prefs: []
  type: TYPE_TB
- en: '| Training-free methods: |  |'
  prefs: []
  type: TYPE_TB
- en: '| LMD/MultiDiffusion (Bar-Tal et al., [2023](#bib.bib3)) | 100% | 30% | 42%
    | 36% | 52.0% |  |'
  prefs: []
  type: TYPE_TB
- en: '| LMD/Backward Guidance (Chen et al., [2023](#bib.bib6)) | 100% | 42% | 36%
    | 61% | 59.8% |  |'
  prefs: []
  type: TYPE_TB
- en: '| LMD/BoxDiff (Xie et al., [2023](#bib.bib37)) | 100% | 32% | 55% | 62% | 62.3%
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| LMD/LMD  (Ours) | 100% | 62% | 65% | 79% | 76.5% | (+ 14.2) |'
  prefs: []
  type: TYPE_TB
- en: '| Training-based methods: |  |'
  prefs: []
  type: TYPE_TB
- en: '| LMD/GLIGEN (Li et al., [2023b](#bib.bib18)) | 100% | 57% | 57% | 45% | 64.8%
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| LMD/LMD+  (Ours) | 100% | 86% | 69% | 67% | 80.5% | (+ 15.7) |'
  prefs: []
  type: TYPE_TB
- en: '| LMD/LMD+  (Ours, GPT-4) | 100% | 84% | 79% | 82% | 86.3% | (+ 21.5) |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluating generated layouts only (upper bound for image generation): |  |'
  prefs: []
  type: TYPE_TB
- en: '| LMD/- | 100% | 97% | 100% | 99% | 99.0% |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Our LLM layout generator (stage 1) is compatible with other layout-to-image
    methods as stage 2, although our proposed layout-grounded controller performs
    the best among them. Our controller could also be integrated with training-based
    Li et al. ([2023b](#bib.bib18)), denoted as LMD+, for additional improvements.
    Finally, the LLM-generated layouts almost always align with the prompt, highlighting
    that the bottleneck is the layout-grounded image generation. The scores for negation
    task are high because we pass the negative prompt generated by ther LLM to the
    underlying diffusion model, which does not depend on the stage 2 implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Image (Layout) Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Stage 1 Model | Average (4 tasks) |'
  prefs: []
  type: TYPE_TB
- en: '| StableBeluga2 |       67.0% | (96.0%) |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |       80.5% | (99.0%) |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4 |       86.3% | (100.0%) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Ablations on different LLMs in stage 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Quantitative evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Benchmark. We propose a text-to-image evaluation benchmark that includes four
    tasks: negation, generative numeracy, attribute binding, and spatial reasoning.
    Negation and generative numeracy involve generating a specific number of objects.
    Attribute binding involves assigning the right attribute to the right object.
    Spatial reasoning involves understanding words that describe the relative locations
    of objects. For each task, we programmatically compose 100 prompts and query each
    model for text-to-image generation. gpt-3.5-turbo Brown et al. ([2020](#bib.bib5))
    is used in LMD for the benchmarks. We also implemented a LMD variant that integrate
    GLIGEN Li et al. ([2023b](#bib.bib18)) into our controller without further training
    and denote it as LMD+. We refer readers to the appendix for implementation details.'
  prefs: []
  type: TYPE_NORMAL
- en: Detection-based evaluation. We use an open-vocabulary object detector, OWL-ViT Minderer
    et al. ([2022](#bib.bib23)), to obtain bounding boxes for the objects of interest.
    We then check whether each generated image satisfies the requirements in the prompt.
    The accuracy of each task is computed by calculating the proportion of the image
    generations that match the prompt over all generations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Results. As presented in [Table 1](#S4.T1 "In 4.1 Qualitative Comparison ‣
    4 Evaluation ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image
    Diffusion Models with Large Language Models"), our model shows significant improvements
    in generation accuracy, ranging from $1.3\times$ compared to SD across four tasks
    and doubling the accuracy on average. Notably, LMD achieves image generation accuracy
    that is more than twice of the SD accuracy for the spatial relationships and the
    negation task. This highlights the utility of the grounding image generation on
    the LLM layout generator. Furthermore, when adding GLIGEN to our pipeline with
    attention-based control and leveraging in-domain instance-annotated data, denoted
    as LMD+, our method achieves additional improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Layout-to-image stage. As shown in [Table 2](#S4.T2 "In 4.1 Qualitative Comparison
    ‣ 4 Evaluation ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image
    Diffusion Models with Large Language Models"), compared with training-free semantic
    control methods, our proposed layout-grounded controller provides much better
    instance-level grounding. Notably, our controller even surpasses training-based
    method GLIGEN Li et al. ([2023b](#bib.bib18)) in attribute binding and spatial
    reasoning task. When integrated with GLIGEN to leverage instance-annotated datasets,
    our integration, denoted as LMD+, allows for further improvements without the
    need for additional training. We also provide an ablation on the base diffusion
    model in [Appendix E](#A5 "Appendix E Ablations on different base diffusion models
    ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
    Models with Large Language Models"): Thanks to our training-free nature, LMD maintain
    the gains to the base model (2$\bm{\times}$ performance boost) when we switch
    the base diffusion model from SDv1.5 to SDv2.1 without hyperparam tuning. This
    showcases the potential of integrating LMD with future diffusion models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text-to-layout stage. We explored LLM selections in [Table 3](#S4.T3 "In 4.1
    Qualitative Comparison ‣ 4 Evaluation ‣ LLM-grounded Diffusion: Enhancing Prompt
    Understanding of Text-to-Image Diffusion Models with Large Language Models").
    All LLMs generate layouts that almost perfectly follow the requirements in the
    prompts, indicating the bottleneck to be the layout-to-image stage. gpt-4 shows
    improved results in layout and the subsequent image generation, compared to gpt-3.5-turbo.
    The capability to generate high-quality layouts are not limited to proprietary
    LLMs, with Llama2-based StableBeluga2 ([Mahan et al.,](#bib.bib22) ; Touvron et al.,
    [2023](#bib.bib34); Mukherjee et al., [2023](#bib.bib25)) also able to perform
    text-to-layout generation in the stage 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we aim to enhance the prompt understanding capabilities of text-to-image
    diffusion models. We present a novel training-free two-stage generation process
    that incorporates LLM-based text-grounded layout generation and layout-grounded
    image generation. Our method also enables instruction-based scene specification
    and generation from prompt in languages unsupported by the base diffusion model.
    Our method outperforms strong baselines in accurately following the prompts in
    text-to-image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we use off-the-shelf models without joint fine-tuning, the LLM may generate
    a layout with ambiguity and may be confusing to the diffusion model. The LLM may
    also output a layout that the diffusion model is not good at generating. For example,
    the generated layout in [Fig. 8](#S6.F8 "In 6 Limitations ‣ LLM-grounded Diffusion:
    Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language
    Models") is feasible for a close-up image, but the diffusion model generates an
    image viewing from far away, resulting in disproportionate objects and background.
    A better format for expressing the layout (e.g., scene graph) and using layouts
    obtained from bounding box detections as in-context examples may alleviate the
    problem. Furthermore, partially occluded instances in the single-instance generation
    stage may lead to incomplete foreground objects in the composed generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although our method did not generate coherent images in cases like [Fig. 8](#S6.F8
    "In 6 Limitations ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of
    Text-to-Image Diffusion Models with Large Language Models"), our method still
    offers more interpretability for diagnosis compared to our base model. The user
    can inspect the layout generated in the first stage to determine whether the unsatisfying
    generation is due to the text-to-layout stage or the layout-to-image stage and
    selectively re-run with another random seed or manually tweak the boxes or the
    captions in the layout for desired generation. In contrast, stable diffusion baseline
    does not offer insights on why it generates two red apples when the prompt specifies
    both apples to be green, not to mention providing measures for fine-grained manipulation
    and control to achieve desired image generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Our method also inherits biases from the Stable Diffusion model. These biases
    have been analyzed in prior works Luccioni et al. ([2023](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1572525fb0242ce33e792515c03d23a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A failure case occurs when our method generates objects that are
    disproportionate to the background due to layout ambiguity. The LLM-generated
    layout is suitable for a close-up image, but the layout-to-image model interprets
    it as viewed from far away.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,
    Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot
    learning. *Advances in Neural Information Processing Systems*, 35:23716–23736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Avrahami et al. (2023) Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,
    Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. 2023. Spatext:
    Spatio-textual representation for controllable image generation. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    18370–18380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bar-Tal et al. (2023) Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
    2023. Multidiffusion: Fusing diffusion paths for controlled image generation.
    *arXiv preprint arXiv:2302.08113*, 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brooks et al. (2023) Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2023.
    Instructpix2pix: Learning to follow image editing instructions. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    18392–18402.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Minghao Chen, Iro Laina, and Andrea Vedaldi. 2023. Training-free
    layout control with cross-attention guidance. *arXiv preprint arXiv:2304.03373*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion
    models beat gans on image synthesis. *Advances in Neural Information Processing
    Systems*, 34:8780–8794.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Epstein et al. (2022) Dave Epstein, Taesung Park, Richard Zhang, Eli Shechtman,
    and Alexei A Efros. 2022. Blobgan: Spatially disentangled scene representations.
    In *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
    23–27, 2022, Proceedings, Part XV*, pages 616–635. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghanimifard and Dobnik (2019) Mehdi Ghanimifard and Simon Dobnik. 2019. What
    a neural language model tells us about spatial relations. In *Proceedings of the
    Combined Workshop on Spatial Language Understanding (SpLU) and Grounded Communication
    for Robotics (RoboNLP)*, pages 71–81.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2021) Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry S
    Davis, Vijay Mahadevan, and Abhinav Shrivastava. 2021. Layouttransformer: Layout
    generation and completion with self-attention. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, pages 1004–1014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta and Kembhavi (2023) Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual
    programming: Compositional visual reasoning without training. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    14953–14962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Herzig et al. (2020) Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor
    Darrell, and Amir Globerson. 2020. Learning canonical representations for scene
    graph to image generation. In *Computer Vision–ECCV 2020: 16th European Conference,
    Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVI 16*, pages 210–227. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    diffusion probabilistic models. *Advances in Neural Information Processing Systems*,
    33:6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2018) Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018. Image
    generation from scene graphs. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, pages 1219–1228.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
    Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C.
    Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. 2023. Segment anything. *arXiv:2304.02643*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a.
    Blip-2: Bootstrapping language-image pre-training with frozen image encoders and
    large language models. *arXiv preprint arXiv:2301.12597*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei
    Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2023b. Gligen: Open-set grounded
    text-to-image generation. *arXiv preprint arXiv:2301.07093*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft
    coco: Common objects in context. In *Computer Vision–ECCV 2014: 13th European
    Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13*,
    pages 740–755\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luccioni et al. (2023) Alexandra Sasha Luccioni, Christopher Akiki, Margaret
    Mitchell, and Yacine Jernite. 2023. Stable bias: Analyzing societal representations
    in diffusion models. *arXiv preprint arXiv:2303.11408*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo (2022) Calvin Luo. 2022. Understanding diffusion models: A unified perspective.
    *arXiv preprint arXiv:2208.11970*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (22) Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian
    Laforte. [Stable beluga models](%5Bhttps://huggingface.co/stabilityai/StableBeluga2%5D(https://huggingface.co/stabilityai/StableBeluga2)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minderer et al. (2022) Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim
    Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab,
    Mostafa Dehghani, Zhuoran Shen, et al. 2022. Simple open-vocabulary object detection
    with vision transformers. *arXiv preprint arXiv:2205.06230*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mokady et al. (2022) Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
    Daniel Cohen-Or. 2022. Null-text inversion for editing real images using guided
    diffusion models. *arXiv preprint arXiv:2211.09794*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar,
    Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. [Orca: Progressive learning
    from complex explanation traces of gpt-4](http://arxiv.org/abs/2306.02707).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2019) Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.
    2019. Semantic image synthesis with spatially-adaptive normalization. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    2337–2346.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. 2022. Hierarchical text-conditional image generation with clip
    latents. *arXiv preprint arXiv:2204.06125*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion
    models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 10684–10695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-net: Convolutional networks for biomedical image segmentation. In *Medical
    Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International
    Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18*, pages
    234–241\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozanova et al. (2021) Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei
    Cheng, Dell Zhang, and Andre Freitas. 2021. Grounding natural language instructions:
    Can large language models capture spatial information? *arXiv preprint arXiv:2109.08634*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,
    Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with
    deep language understanding. *Advances in Neural Information Processing Systems*,
    35:36479–36494.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020) Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising
    diffusion implicit models. *arXiv preprint arXiv:2010.02502*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](http://arxiv.org/abs/2307.09288).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. 2023. Visual chatgpt: Talking, drawing and editing with visual
    foundation models. *arXiv preprint arXiv:2303.04671*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2023) Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian
    Zhang, Yefeng Zheng, and Mike Zheng Shou. 2023. Boxdiff: Text-to-image synthesis
    with training-free box-constrained diffusion. *arXiv preprint arXiv:2307.10816*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2017) Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. 2017.
    Scene graph generation by iterative message passing. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 5410–5419.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin
    Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. 2023. Reco:
    Region-controlled text-to-image generation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 14246–14255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Agrawala (2023) Lvmin Zhang and Maneesh Agrawala. 2023. Adding conditional
    control to text-to-image diffusion models. *arXiv preprint arXiv:2302.05543*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. 2019.
    Image generation from layout. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 8584–8593.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Preliminary introduction on diffusion models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Diffusion models are a class of powerful generative models that learn the data
    distribution of complex datasets. During the forward process, noise is added to
    an image to the input data $\mathbf{x}_{0}$ to transform it into a sample that
    resembles the real data in the training dataset. The reverse process is often
    referred to as “denoising”. We refer readers to Luo ([2022](#bib.bib21)) for a
    more in-depth introduction to diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: 'DDPM. Ho et al. ([2020](#bib.bib13)). The denoising process of denoising diffusion
    probabilistic models starts with the initial noise vector sampled from a standard
    Gaussian noise vector $\mathbf{x}_{T}\sim\mathcal{N}(0,\mathbf{I})$ learns to
    predict the added noise for the forward process by minimizing the training objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=&#124;&#124;\bm{\epsilon}-\bm{\epsilon}_{\theta(\mathbf{x}_{t},t)}&#124;&#124;^{2}$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'At inference time, for each of the $T$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{z}\sim\mathcal{N}(0,\mathbf{I})$ that controls the size of the
    denoising step.
  prefs: []
  type: TYPE_NORMAL
- en: 'DDIM Song et al. ([2020](#bib.bib33)). Denoising diffusion implicit models
    are a generalization to DDPM which allows sampling with fewer iterations. DDIM
    applies the following update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{x}_{t-1}=\sqrt{\alpha_{t-1}}\Big{(}\frac{\mathbf{x}_{t}-\sqrt{1-\alpha_{t}}\bm{\epsilon}_{\theta}(\mathbf{x}_{t},t)}{\sqrt{\alpha_{t}}}\Big{)}+\sigma_{t}\bm{\epsilon}_{t}$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: Note that DDIM shares the same training procedure with DDIM, which means we
    can take a model trained with DDPM objective and perform faster sampling using
    DDIM. When $\sigma_{t}$ is called DDIM inversion Mokady et al. ([2022](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: Latent Diffusion and Stable Diffusion Rombach et al. ([2022](#bib.bib29)). While
    DDIM and DDPM are denoising images from raw pixel space, latent diffusion proposes
    to denoise from initial latent noise $\mathbf{z}_{T}$. Latent/Stable Diffusion
    lowers the cost of training high resolution diffusion models and is widely used
    in text-to-image generation. Our method improves the prompt understanding of Stable
    Diffusion without adapting the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Latent diffusion also proposes a conditioning scheme that allows generating
    samples with other modalities (e.g., text) as the condition. The condition is
    realized through cross-attention layers Vaswani et al. ([2017](#bib.bib35)) that
    attend from latent locations in U-Net Ronneberger et al. ([2015](#bib.bib30))
    feature maps to the encoded condition (e.g., text features from a CLIP text encoder).
  prefs: []
  type: TYPE_NORMAL
- en: Stable diffusion models are large text-to-image generation models trained on
    large multi-modal datasets using the techniques proposed for latent diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Layout-grounded image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 1:A set of captioned bounding boxes $\{(\mathbf{b}^{(i)},\mathbf{y}^{(i)})\}_{i=1}^{N}$18:for $t\leftarrow
    T$
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Pseudo-code for layout-grounded image generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We present the pseudo-code for our layout-grounding stage (stage 2) in [Algorithm 1](#alg1
    "In Appendix A Preliminary introduction on diffusion models ‣ LLM-grounded Diffusion:
    Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language
    Models"). We explain the functionality of the functions used in the pseudo-code:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SampleGaussian samples i.i.d standard Gaussian as the initial noise for the
    latent tensor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PromptForBox simply sets “[background prompt] with [box caption]” (e.g., “a
    realistic image of an indoor scene with a gray cat”) as the denoising prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AttnControl performs backward guidance to minimize the energy function described
    in the main text to encourage the attention to the area within the box and discourage
    the attention on area outside the box.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Denoise denotes one denoising step by the diffusion model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TemporalAverage averages the cross-attention map across the timestep dimension
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SAMRefine refines the attention map by internally decoding the latent and refining
    with SAM. If SAM is not enabled, we perform a thresholding instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ComposedPrompt composes the prompt for overall generation. We offer two options
    for the overall prompt: using the original input prompt or composing the prompt
    as “[background prompt] with [box caption 1], [box caption 2], …”. The former
    one allows capturing the object as well as forground-background interactions that
    are not captured in the layout. The latter allows captions in languages unsupported
    by the diffusion model and stays robust when the caption is misleading (e.g.,
    “neither of the apples is red"). We use the latter by default but also allow the
    former for fine-grained adjustments.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LatentCompose spatially composes the latent with respect to the mask, replacing
    the content of the destination latent on the masked locations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AttnTransfer performs backward guidance to minimize the energy function described
    in the main text to encourage the attention in overall generation within the box
    to be similar to the attention in per-box generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/faccd9526cae8fa7d99ed9efd321b032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Our layout-grounded image generator has better instance-level control
    compared to MultiDiffusion Bar-Tal et al. ([2023](#bib.bib3)) and Chen et al.
    ([2023](#bib.bib6)) (backward guidance). While MultiDiffusion and Chen et al.
    ([2023](#bib.bib6)) only specify the semantic regions, our layout-guided image
    generator specifies one instance at the location of each box. Our method correctly
    generates exactly one ball for a box in three out of four attempts.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Qualitative comparison with semantic-grounded image generation methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We compare with MultiDiffusion Bar-Tal et al. ([2023](#bib.bib3)) and Chen
    et al. ([2023](#bib.bib6)) which allow semantic-grounded image generation. Since
    MultiDiffusion and Chen et al. ([2023](#bib.bib6)) are proposed to leverage semantic
    masks as the guidance without a text-to-layout generation stage, we reuse the
    layout generated in stage 1 of LMD ([Fig. 9](#A2.F9 "In Appendix B Pseudo-code
    for layout-grounded image generation ‣ LLM-grounded Diffusion: Enhancing Prompt
    Understanding of Text-to-Image Diffusion Models with Large Language Models")(b)).
    We present the first four generated images with no random seed selection. While
    Stable Diffusion does not adhere to the number of balls in the prompt ([Fig. 9](#A2.F9
    "In Appendix B Pseudo-code for layout-grounded image generation ‣ LLM-grounded
    Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with
    Large Language Models")(c)), MultiDiffusion and Chen et al. ([2023](#bib.bib6))
    generate images with semantics that match the layout ([Fig. 9](#A2.F9 "In Appendix
    B Pseudo-code for layout-grounded image generation ‣ LLM-grounded Diffusion: Enhancing
    Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models")(d)
    and (e)) without fine-grained control over the instances. As shown in [Fig. 9](#A2.F9
    "In Appendix B Pseudo-code for layout-grounded image generation ‣ LLM-grounded
    Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with
    Large Language Models")(f), our method correctly generates three plastic balls
    in three out of four images, showing better instance-level control.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Image Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Average (4 tasks) |'
  prefs: []
  type: TYPE_TB
- en: '| SD v1.5 (Default setting) |           37% |'
  prefs: []
  type: TYPE_TB
- en: '| LMD on SD v1.5 (Ours, default setting) |           77% ($\textbf{2.1}\bm{\times}$)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SD v2.1 |           38% |'
  prefs: []
  type: TYPE_TB
- en: '| LMD on SD v2.1 (Ours) |           77% ($\textbf{2.0}\bm{\times}$) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Our method achieves significant gains when adapted to Stable Diffusion
    v2.1 as the base model without any hyperparam tuning or model training. The performance
    of our method could be improved by additional hyperparam tuning. This shows a
    promising signal that our method could also improve along with the enhancement
    of diffusion models in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Details for text-to-image benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We pick 10 common object types from the COCO dataset Lin et al. ([2014](#bib.bib19))
    for generation⁶⁶6Backpack, book, bottle, bowl, car, cat, chair, cup, dog, and
    laptop..
  prefs: []
  type: TYPE_NORMAL
- en: For negation and generative numeracy task, each prompt requires the model to
    generate a layout of a scene with some number of a certain object or without a
    certain object. Then we count the number of objects and consider the layout to
    be correct if the number of the object of that particular type matches the one
    in the prompt, with the number ranging from 1 to 5.
  prefs: []
  type: TYPE_NORMAL
- en: The objective for each prompt in the attribute binding task is to generate a
    object of a color and another object of another color, for which the evaluation
    is similar to other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For the spatial relationship task, we generate an object at a certain location
    and another object at an opposite location (left/right and top/bottom). We then
    check the spatial coordinates of the boxes to ensure the layout exactly matches
    the prompt. In each task, we generate 100 text prompts, with 400 text prompts
    in total.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts. For the negation benchmark, we use the prompt A realistic photo of
    a scene without [object name].
  prefs: []
  type: TYPE_NORMAL
- en: For generative numeracy, we use the prompt A realistic photo of a scene with
    [number] [object name].
  prefs: []
  type: TYPE_NORMAL
- en: For attribute assignment, we use the prompt A realistic photo of a scene with
    [modifier 1] [object name 1] and [modifier 2] [object name2], where the two modifiers
    are randomly chosen from colors (red, orange, yellow, green, blue, purple, pink,
    brown, black, white, gray).
  prefs: []
  type: TYPE_NORMAL
- en: For the spatial relationship benchmark, we use the prompt A realistic photo
    of a scene with [object name 1] on the [location] and [modifier 2] [object name2]
    on the [opposite location], where the location is chosen from left, right, top,
    and bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation details. For LMD, we use Stable Diffusion v1.5 by default. For
    LMD+, we use GLIGEN Li et al. ([2023b](#bib.bib18)) model without additional training
    or adaptation. We selected the GLIGEN Li et al. ([2023b](#bib.bib18)) model trained
    based on Stable Diffusion v1.4, which is the latest at the time of writing. We
    use $\eta=5$. We decouple the scheduler for latent composition and energy minimization,
    and we only conduct energy minimization during the first 10 iterations of denoising
    to accelerate generation. The visualizations are generated by LMD by default unless
    stated otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Ablations on different base diffusion models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike training-based methods, our method is applicable to different base diffusion
    models. As shown in [Table 4](#A3.T4 "In Appendix C Qualitative comparison with
    semantic-grounded image generation methods ‣ LLM-grounded Diffusion: Enhancing
    Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models"),
    our method maintains considerable performance gains when the base diffusion model
    is switched. Note that we do not perform hyperparam tuning on SDv2.1 and directly
    use the settings from SDv1.5\. Additional hyperparam tuning may further improve
    the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Our LLM prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our LLM prompt is listed in [Table 5](#A6.T5 "In Appendix F Our LLM prompt
    ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion
    Models with Large Language Models"). Our in-context examples are listed in [Table 6](#A6.T6
    "In Appendix F Our LLM prompt ‣ LLM-grounded Diffusion: Enhancing Prompt Understanding
    of Text-to-Image Diffusion Models with Large Language Models").'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1You  are  an  intelligent  bounding  box  generator.  I  will  provide  you  with  a  caption  for  a  photo,  image,  or  painting.  Your  task  is  to  generate  the  bounding  boxes  for  the  objects  mentioned  in  the  caption,  along  with  a  background  prompt  describing  the  scene.  The  images  are  of  size  512x512.  The  top-left  corner  has  coordinate  [0,  0].  The  bottom-right  corner  has  coordinnate  [512,  512].  The  bounding  boxes  should  not  overlap  or  go  beyond  the  image  boundaries.  Each  bounding  box  should  be  in  the  format  of  (object  name,  [top-left  x  coordinate,  top-left  y  coordinate,  box  width,  box  height])  and  should  not  include  more  than  one  object.  Do  not  put  objects  that  are  already  provided  in  the  bounding  boxes  into  the  background  prompt.  Do  not  include  non-existing  or  excluded  objects  in  the  background  prompt.  Use  "A  realistic  scene"  as  the  background  prompt  if  no  background  is  given  in  the  prompt.  If  needed,  you  can  make  reasonable  guesses.  Please  refer  to  the  example  below  for  the  desired  format.23[In-context  Examples]45Caption:  [User  Prompt]6Objects:
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Our full prompt to the LLM for layout generation. LLM starts completion
    from “Objects:”.'
  prefs: []
  type: TYPE_NORMAL
- en: '| 1Caption:  A  realistic  image  of  landscape  scene  depicting  a  green  car  parking  on  the  left  of  a  blue  truck,  with  a  red  air  balloon  and  a  bird  in  the  sky2Objects:  [(’a  green  car’,  [21,  281,  211,  159]),  (’a  blue  truck’,  [269,  283,  209,  160]),  (’a  red  air  balloon’,  [66,  8,  145,  135]),  (’a  bird’,  [296,  42,  143,  100])]3Background  prompt:  A  realistic  landscape  scene4Negative  prompt:56Caption:  A  realistic  top-down  view  of  a  wooden  table  with  two  apples  on  it7Objects:  [(’a  wooden  table’,  [20,  148,  472,  216]),  (’an  apple’,  [150,  226,  100,  100]),  (’an  apple’,  [280,  226,  100,  100])]8Background  prompt:  A  realistic  top-down  view9Negative  prompt:1011Caption:  A  realistic  scene  of  three  skiers  standing  in  a  line  on  the  snow  near  a  palm  tree12Objects:  [(’a  skier’,  [5,  152,  139,  168]),  (’a  skier’,  [278,  192,  121,  158]),  (’a  skier’,  [148,  173,  124,  155]),  (’a  palm  tree’,  [404,  105,  103,  251])]13Background  prompt:  A  realistic  outdoor  scene  with  snow14Negative  prompt:1516Caption:  An  oil  painting  of  a  pink  dolphin  jumping  on  the  left  of  a  steam  boat  on  the  sea17Objects:  [(’a  steam  boat’,  [232,  225,  257,  149]),  (’a  jumping  pink  dolphin’,  [21,  249,  189,  123])]18Background  prompt:  An  oil  painting  of  the  sea19Negative  prompt:2021Caption:  A  cute  cat  and  an  angry  dog  without  birds22Objects:  [(’a  cute  cat’,  [51,  67,  271,  324]),  (’an  angry  dog’,  [302,  119,  211,  228])]23Background  prompt:  A  realistic  scene24Negative  prompt:  birds2526Caption:  Two  pandas  in  a  forest  without  flowers27Objects:  [(’a  panda’,  [30,  171,  212,  226]),  (’a  panda’,  [264,  173,  222,  221])]28Background  prompt:  A  forest29Negative  prompt:  flowers3031Caption:  An  oil  painting  of  a  living  room  scene  without  chairs  with  a  painting  mounted  on  the  wall,  a  cabinet  below  the  painting,  and  two  flower  vases  on  the  cabinet32Objects:  [(’a  painting’,  [88,  85,  335,  203]),  (’a  cabinet’,  [57,  308,  404,  201]),  (’a  flower  vase’,  [166,  222,  92,  108]),  (’a  flower  vase’,  [328,  222,  92,  108])]33Background  prompt:  An  oil  painting  of  a  living  room  scene34Negative  prompt:  chairs
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Our in-context examples.'
  prefs: []
  type: TYPE_NORMAL
