- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:02'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'HowToCaption: Prompting LLMs to Transform Video Annotations at Scale'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.04900](https://ar5iv.labs.arxiv.org/html/2310.04900)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nina Shvetsova^(  1,2,3), Anna Kukleva^(∗2), Xudong Hong^(2,6),
  prefs: []
  type: TYPE_NORMAL
- en: Christian Rupprecht⁴, Bernt Schiele², Hilde Kuehne^(1,3,5)
  prefs: []
  type: TYPE_NORMAL
- en: ¹ Goethe University Frankfurt ² MPI for Informatics, Saarland Informatics Campus,
    ³ University of Bonn,
  prefs: []
  type: TYPE_NORMAL
- en: ⁴ University of Oxford, ⁵ MIT-IBM Watson AI Lab, ⁶ Saarland University
  prefs: []
  type: TYPE_NORMAL
- en: '{nshvetso,akukleva}@mpi-inf.mpg.de equal contribution'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instructional videos are an excellent source for learning multimodal representations
    by leveraging video-subtitle pairs extracted with automatic speech recognition
    systems (ASR) from the audio signal in the videos. However, in contrast to human-annotated
    captions, both speech and subtitles naturally differ from the visual content of
    the videos and thus provide only noisy supervision for multimodal learning. As
    a result, large-scale annotation-free web video training data remains sub-optimal
    for training text-video models. In this work, we propose to leverage the capability
    of large language models (LLMs) to obtain fine-grained video descriptions aligned
    with videos. Specifically, we prompt an LLM to create plausible video descriptions
    based on ASR narrations of the video for a large-scale instructional video dataset.
    To this end, we introduce a prompting method that is able to take into account
    a longer text of subtitles, allowing us to capture context beyond a single sentence.
    To align the captions to the video temporally, we prompt the LLM to generate timestamps
    for each produced caption based on the subtitles. In this way, we obtain human-style
    video captions at scale without human supervision. We apply our method to the
    subtitles of the HowTo100M dataset, creating a new large-scale dataset, HowToCaption.
    Our evaluation shows that the resulting captions not only significantly improve
    the performance over many different benchmark datasets for text-video retrieval
    but also lead to a disentangling of textual narration from the audio, boosting
    performance in text-video-audio tasks. ¹¹1[github.com/ninatu/howtocaption](https://github.com/ninatu/howtocaption)
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Textual descriptions of visual information allow for navigating large amounts
    of visual data. Improving the alignment between visual and textual modalities
    is crucial for many applications, *e.g*., in the context of text-video retrieval
    to identify videos based on the described content. Recently, image-text cross-modal
    learning has achieved remarkable performance (Radford et al., [2021](#bib.bib34))
    in many downstream tasks by pre-training on large-scale web datasets consisting
    of text-image pairs. To collect video data on a similar scale, media platforms
    such as YouTube can be used as a great source of freely available videos (Abu-El-Haija
    et al., [2016](#bib.bib1); Zhou et al., [2018](#bib.bib53)). Most of these videos
    include some narrations, *e.g*., in instructional videos (Miech et al., [2019](#bib.bib26)),
    people explain and show how to accomplish one or another task. To transform spoken
    language from the videos into subtitles, current automatic speech recognition
    (ASR) systems (Radford et al., [2023](#bib.bib35)) can be used, providing aligned
    text-video annotated pairs for free. This automatic supervisory signal can easily
    scale to large video datasets. However, such video web data poses additional challenges (Han
    et al., [2022](#bib.bib14); Miech et al., [2019](#bib.bib26)): (1) spoken and
    visual information in the video can deviate from each other, *e.g*., when speakers
    provide information beyond what is visible or when spoken instructions do not
    temporally align with the actions shown, (2) speech contains filler words and
    phrases, such as “I’m going to”, and can be incomplete and sometimes contains
    grammatical errors, and (3) ASR transcripts usually do not have punctuation and
    may contain errors. Therefore, a simple matching of videos and corresponding ASR
    subtitles provides only weak, noisy information, and the learned representations
    are not as generalizable as similar image-text representations learned on web
    data (Miech et al., [2019](#bib.bib26)).'
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, we propose a new framework, HowToCaption, that leverages
    large language models (LLMs) (Chiang et al., [2023](#bib.bib8)) to generate human-style
    captions on a large scale for web-video instructional datasets based on corresponding
    ASR subtitles. By carefully designing prompts, we show that the LLM can effectively
    map long, noisy subtitles into concise and descriptive human-style video captions.
    This approach allows us to create a large-scale dataset of video captions without
    any human supervision. Moreover, we can obtain a temporal alignment between the
    generated captions and specific moments in the given video sequences by tasking
    the LLM to predict timestamps for each caption. Our method can generate aligned
    text-video pairs on a large scale without human intervention. For additional quality
    improvement, we apply filtering and realignment within short temporal windows
    with respect to the generated timestamp. Beyond providing better annotation, the
    new captions provide the advantage that they are no longer a direct output of
    the speech signal, thus effectively decoupling audio and text. Current methods
    usually avoid using audio (Miech et al., [2020](#bib.bib27); Han et al., [2022](#bib.bib14)),
    as the ASR subtitle is directly derived from speech, thus leading to the problem
    that any text-to-audio+video retrieval would mainly retrieve the closest speech
    signal while disregarding the video. Being able to generate captions that deviate
    from the speech thus allows to extend retrieval to audio+video without the need
    for fine-tuned regularization.
  prefs: []
  type: TYPE_NORMAL
- en: To verify the effectiveness of the proposed HowToCaption method, we generate
    new captions for the large-scale HowTo100M dataset (Miech et al., [2019](#bib.bib26)).
    We evaluate the quality of the improved narrations on various challenging zero-shot
    downstream tasks over four different datasets, namely YouCook2 (Zhou et al., [2018](#bib.bib53)),
    MSR-VTT (Xu et al., [2016](#bib.bib48)), MSVD (Chen & Dolan, [2011](#bib.bib7)),
    and LSMDC (Rohrbach et al., [2015](#bib.bib37)). It shows that the generated captions
    not only provide a better training signal but also allow for a decoupling of speech
    and caption annotation, allowing a retrieval based on audio, vision, and subtitles
    at scale. We release a new HowToCaption dataset with high-quality textual descriptions
    to show the potential of generated captions for web text-video pairs. We also
    make code publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: 'We summarize the contributions of the paper as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a HowToCaption method to efficiently convert noisy ASR subtitles
    of instructional videos into fine-grained video captions, which leverages recent
    advances in LLMs and generates high-quality video captions at scale without any
    human supervision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We create a new HowToCaption dataset with high-quality human-style textual descriptions
    with our proposed HowToCaption method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing the HowToCaption dataset for training text-video models allows us
    to significantly improve the performance over many benchmark datasets for text-to-video
    retrieval. Moreover, since new textual annotation allows us to disentangle audio
    and language modalities in instructional videos, where ASR subtitles were highly
    correlated to audio, we show a boost in text-video+audio retrieval performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As this work introduces a method to improve ASR-based Video-Language datasets,
    which is centered around LLM, we organize the related work into three categories:
    related datasets, learning from ASR data, and LLM in vision-language tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Large-Scale Video-Language Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Manual annotation of video captioning datasets is even more time-consuming than
    image captioning since it involves video trimming and localization of caption
    boundaries. Currently, manually annotated video captioning datasets, e.g., MSR-VTT (Xu
    et al., [2016](#bib.bib48)), and YouCook2 (Zhou et al., [2018](#bib.bib53)), are
    limited in size. Therefore, different methods of mining video with weak supervision
    from the Internet were considered. Such datasets as YouTube-8M (Abu-El-Haija et al.,
    [2016](#bib.bib1)) and IG-Kinetics-65M (Ghadiyaram et al., [2019](#bib.bib13))
    provided multiple class labels based on query click signals and metadata (Abu-El-Haija
    et al., [2016](#bib.bib1)) or hashtags (Ghadiyaram et al., [2019](#bib.bib13)).
    However, short class labels are a suboptimal supervision compared to textual descriptions (Desai
    & Johnson, [2021](#bib.bib10)). Therefore, Bain et al. ([2021](#bib.bib3)) considered
    scrapping from the web videos with associated alt-text, similarly to image-based
    Conceptual Captions dataset (Sharma et al., [2018](#bib.bib40)), obtaining the
    WebVid2M dataset (Bain et al., [2021](#bib.bib3)) that contains 2.5M videos-text
    pairs. Stroud et al. ([2020](#bib.bib42)) proposed to use meta information, such
    as titles, video descriptions, and tags from YouTube, as a textual annotation
    and created the WTS-70M dataset. And Nagrani et al. ([2022](#bib.bib28)) proposed
    to transfer image captions from an image-text dataset to videos by searching videos
    with similar frames to the image and, therefore, collected the VideoCC3M dataset.
    However, most videos in the WebVid2M dataset do not have audio, which is an essential
    part of video analysis, and captions in the VideoCC3M dataset are derived from
    images and, therefore, tend to describe more static scenes rather than actions.
    At the same time, the title and tags of WTS-70M provide only high-level video
    descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to this, Miech et al. ([2019](#bib.bib26)) proposed the HowTo100M
    dataset, where instructional videos are naturally accompanied by dense textual
    supervision in the form of subtitles obtained from ASR (Automatic Speech Recognition)
    systems. The HowTo100M dataset with 137M clips sourced from 1.2M YouTube videos
    was proven to be effective for pre-training video-audio-language representations (Rouditchenko
    et al., [2021](#bib.bib38); Chen et al., [2021](#bib.bib6); Shvetsova et al.,
    [2022](#bib.bib41)). The followed-up YT-Temporal-180M (Zellers et al., [2021](#bib.bib51))
    and HD-VILA-100M (Xue et al., [2022](#bib.bib49)) datasets are created by using
    the same idea, but expand the HowTo100M with a larger number of videos, higher
    diversity, and higher video resolution. While ASR supervision can provide a scalable
    way to create a large video dataset with dense annotation, the quality of ASR
    subtitles is still not on par with human-annotated captions. In this work, we
    propose a method to create high-quality captions for videos at scale by leveraging
    LLM and subtitles.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Learning with Noisy ASR Subtitles of Instructional Videos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem of misalignment and noisiness of ASR supervision in instructional
    videos, such as in the HowTo100M dataset, were addressed in multiple works. MIL-NCE
    loss (Miech et al., [2020](#bib.bib27)) and soft max-margin ranking loss (Amrani
    et al., [2021](#bib.bib2)) were proposed to adapt contrastive loss to misalignment
    in text-video pairs. Zellers et al. ([2021](#bib.bib51)) proposed to use LLM to
    add punctuation and capitalization to ASR subtitles and remove mistranscription
    errors. Han et al. ([2022](#bib.bib14)) proposed to train temporal alignment networks
    to filter out subtitles that are not alignable to the video and determine alignment
    for the others. However, to the best of our knowledge, (Lin et al., [2022](#bib.bib22)) is
    the only work that goes beyond just removing mistranscription errors and ASR re-alignment,
    where Lin et al. ([2022](#bib.bib22)) proposed to match the sentences from ASR
    subtitles to a large base of descriptions of the steps from wikiHow dataset (Koupaee
    & Wang, [2018](#bib.bib16)) (distant supervision). In our work, we propose to
    use LLM to create video captions given ASR subtitles, which allows us to create
    a detailed description that is specific for every video and has proper sentence
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Large Pre-trained Language Models in Vision-Language Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, there has been a remarkable success of LLMs in many language-related
    tasks (Devlin et al., [2018](#bib.bib11); Radford et al., [2019](#bib.bib33);
    Raffel et al., [2020](#bib.bib36)). Latest large language models such as GPT-3.5 (Neelakantan
    et al., [2022](#bib.bib29)), Alpaca (Taori et al., [2023](#bib.bib46)) or Vicuna (Chiang
    et al., [2023](#bib.bib8)) have demonstrated excellent zero-shot capabilities
    on common sense inference (Chang & Bergen, [2023](#bib.bib4)). This success has
    prompted research into integrating common-sense knowledge into vision-language
    tasks to enhance their performance. In this regard, some methods (Sun et al.,
    [2019](#bib.bib44); Su et al., [2019](#bib.bib43); Lu et al., [2019](#bib.bib24);
    Tan & Bansal, [2019](#bib.bib45)) initialize the language part of vision-language
    models from pre-trained LLM. Another line of work (Cho et al., [2021](#bib.bib9);
    Li et al., [2023](#bib.bib19); Zhao et al., [2023](#bib.bib52)) uses LLM as a
    decoder to enable vision-to-language generation. For example, the MiniGPT-4 (Zhu
    et al., [2023](#bib.bib54)) model enhances a frozen Vicuna model by aligning visual
    encoder tokens with Vicuna’s input token space, enabling visual reasoning capabilities,
    *e.g*., image question answering or image captioning. In this regard, some works (Lialin
    et al., [2023](#bib.bib20); Zhao et al., [2023](#bib.bib52)) adapted visually
    conditioned LLM for visual captioning and created captioning pseudo labels for
    large-scale video data that later used for vision-language tasks. However, these
    methods require human-annotated datasets to train a captioning model , while our
    method does not require any label data and aims to transform free available annotation
    (ASR subtitles) into textual descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/66d42edf2d24184bc1ef0fb10582ad16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: . Schematic visualization the proposed HowToCaption method. Obtained
    from Automatic Speech Recognition System (ASR) subtitles divided into blocks that
    contain longer contextual information. A large pre-trained language model is then
    used to generate plausible video captions based on ASR subtitles, along with timestamps
    for each caption. These generated captions and timestamps are further additionally
    post-processed to enhance their alignment to the video and filter out captions
    with low similarity to the corresponding video by leveraging a pre-trained text-video
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Problem Statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a dataset of $N$ and ends at $\tau^{e}_{n,i}$.
  prefs: []
  type: TYPE_NORMAL
- en: The generated captions aim to serve for vision-language or vision-language-{other
    modalities (such as audio)} tasks, providing language supervision in the form
    of “human-written-like” captions rather than scrambled noisy ASR subtitles. That
    enables the potential of collecting large-scale datasets with long-term videos
    and their fine-grained textual descriptions for free, without human supervision.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Video-Language Retrieval Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we will describe our method for generating the HowToCaption dataset,
    we will briefly recap the training of video-language retrieval models (V-L model),
    as it is one of the main use cases for this dataset. Furthermore, we also use
    a V-L model to improve the temporal alignment in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We base our video-language retrieval model (V-L model) on the pre-trained BLIP
    image-language dual encoder model (Li et al., [2022](#bib.bib18)). We maintain
    the architecture of the text encoder $f(c)\in\mathbb{R}^{d}$, resulting in the
    following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $B$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 HowToCaption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To generate fine-grained captions for the instructional videos, we propose to
    leverage recent large language models that demonstrate great zero-shot performance
    in many different tasks formulated with natural language. Namely, we prompt the
    LLM to read the ASR subtitles of the video and create a plausible video description
    based on this. Since one subtitle only covers a small part of the video and lacks
    a global context, we propose to aggregate multiple subtitles together with their
    timestamp information. Then, we task the LLM to create detailed descriptions based
    on the entire input and estimate timestamps for each generated sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overview of our approach is shown in [Figure 1](#S3.F1 "In 3 Method ‣ HowToCaption:
    Prompting LLMs to Transform Video Annotations at Scale"). For each video, first,
    we slice a given sequence of subtitles into blocks that contain long context information
    about the video. Then, the ASR subtitles of each block are summarised into a video
    caption using the LLM that we prompt with our task description. The LLM also predicts
    timestamps for each sentence in the video caption, which we further refine in
    our post-processing step based on similarities of a caption sentence to video
    clips in the neighboring area of predicted timestamps.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 LLM Prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For our language prompt (shown in [Figure 1](#S3.F1 "In 3 Method ‣ HowToCaption:
    Prompting LLMs to Transform Video Annotations at Scale")), we leverage the same
    “main” prompt for LLM, as in the Vicuna-13B model (Chiang et al., [2023](#bib.bib8)):
    “A chat between a curious human and…” that defines the requirement from LLM to
    give a helpful answer to our questions. Then, we describe our request, what data
    we need to process and how it should be processed: “I will give you an automatically
    recognized speech…”. We found structuring the prompt in the way that the task
    description given at the beginning of the prompt and the long ASR input $S_{n}$
    is a constant video clip length parameter (number of seconds). Please see [Section 4.3](#S4.SS3
    "4.3 Ablation Studies ‣ 4 Experimental Results ‣ HowToCaption: Prompting LLMs
    to Transform Video Annotations at Scale") and [Section A.2](#A1.SS2 "A.2 Additional
    Results in Prompt Engineering ‣ Appendix A Appendix ‣ HowToCaption: Prompting
    LLMs to Transform Video Annotations at Scale") for a detailed evaluation of these
    choices.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.3.2 Post-processing: Filtering & Alignment'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'ASR subtitles suffer from bad temporal alignment (Han et al., [2022](#bib.bib14);
    Miech et al., [2019](#bib.bib26)). Although the LLM prompted to produce video
    captions can filter some noise in the ASR subtitles, some generated captions are
    still misaligned with the video. Therefore, inspired by the TAN method (Han et al.,
    [2022](#bib.bib14)) that automatically predicts the alignability of subtitles
    and matching timestamps, we further improve our obtained captions with a filtering
    & alignment post-processing step ([Figure 1](#S3.F1 "In 3 Method ‣ HowToCaption:
    Prompting LLMs to Transform Video Annotations at Scale")).'
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we utilize the video-language encoder model $(f,g)$ is a similarity
    score threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further improve the alignment of captions, we perform multiple rounds of
    filtering & alignment. In practice, we found that the improvement after two rounds
    is marginal. For subsequent rounds, we finetune (c.f. [Section 3.2](#S3.SS2 "3.2
    Video-Language Retrieval Model ‣ 3 Method ‣ HowToCaption: Prompting LLMs to Transform
    Video Annotations at Scale")) the V-L model on the filtered & aligned video-captions
    pairs $\{(v_{i},c_{i})\}$. This regularization prevents the model from forgetting (Hou
    et al., [2019](#bib.bib15)). Then, during filtering & alignment, we use the average
    of the similarities of the finetuned and original model. We show an impact of
    these changes in [Section A.3](#A1.SS3 "A.3 Ablations of Filtering & Alignment
    Post-processing ‣ Appendix A Appendix ‣ HowToCaption: Prompting LLMs to Transform
    Video Annotations at Scale").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 HowToCaption Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18614888faeeda56b7fc66092f60722c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Caption: Matt Swanson gives a tip to use buckets to direct the path of the
    ball'
  prefs: []
  type: TYPE_NORMAL
- en: 'ASR: move them around to help direct the path'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86a74c698ef658c3e6dafcb46b20f9ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Caption: Dog wants to hang out near dirt or other dogs with bones to acquire
    more bones'
  prefs: []
  type: TYPE_NORMAL
- en: 'ASR: she might need it for later so this is stage one of hiding the bone burying
    the bone…'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9bdda878d02ad56b9e5695b1e9f99653.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Caption: Making a bow with two colors'
  prefs: []
  type: TYPE_NORMAL
- en: 'ASR: so it’s not going to really show'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ddd4861bc7bb44a54b038de75d70cc4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Caption: Make sure the bottle stays together'
  prefs: []
  type: TYPE_NORMAL
- en: 'ASR: but this yeah and it just stays or it won’t get off it’s busy here'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Examples video-captions pairs from our HowToCaption dataset. ASR
    subtitles with only noisy supervision for the video are converted from spoken
    to written-language-style captions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the proposed HowToCaption  approach to 1.2M long-term instructional
    videos and ASR subtitles of the HowTo100M dataset and obtain the HowToCaption dataset.
    By prompting the Vicuna-13B model, we obtain $\sim 70$M initial captions. After
    filtering & alignment (details in [Section 4.2](#S4.SS2 "4.2 Implementation Details
    ‣ 4 Experimental Results ‣ HowToCaption: Prompting LLMs to Transform Video Annotations
    at Scale")) we obtain 25M high-quality video-caption pairs. We show examples from
    our HowToCaption dataset in [Figure 2](#S3.F2 "In 3.4 HowToCaption Dataset ‣ 3
    Method ‣ HowToCaption: Prompting LLMs to Transform Video Annotations at Scale").
    We note that generated captions follow different text styles, *e.g*., the first
    and the second examples contain a long description of an object and its actions,
    the third describes the process, and the last one is instruction. The average
    length of the generated captions is 9.3 words.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate the proposed HowToCaption dataset as a video-text dataset for large-scale
    pre-training of vision-language models, we train our T-V model on HowToCaption and
    assess its zero-shot video-text retrieval performance on four widely recognized
    video-text benchmarks: YouCook2 (Zhou et al., [2018](#bib.bib53)), MSR-VTT (Xu
    et al., [2016](#bib.bib48)), MSVD (Chen & Dolan, [2011](#bib.bib7)), and LSMDC (Rohrbach
    et al., [2015](#bib.bib37)). While the YouCook2 dataset consists of instructional
    cooking videos and might be considered as an in-domain benchmark for the HowToCaption,
    the other datasets encompass a broader range of topics and video types, including
    non-instructional YouTube videos and movies. To evaluate the properties of HowToCaption dataset
    in comparison with other large-scale pre-training datasets, we also train our
    T-V model on HowTo100M (Miech et al., [2019](#bib.bib26)), on HowTo100M with step
    labels (Lin et al., [2022](#bib.bib22)), HTM-AA (Han et al., [2022](#bib.bib14)),
    VideoCC3M (Nagrani et al., [2022](#bib.bib28)), and WebVid2M (Bain et al., [2021](#bib.bib3))
    datasets and compare zero-shot text-video retrieval performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Datasets and Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pre-training Datasets. HowTo100M is a dataset of 1.2M instructional videos
    with ASR subtitles collected by querying YouTube with 23k different “how to” tasks
    from WikiHow articles. We consider three versions of annotations of this dataset:
    Sentencified HowTo100M, with pre-processed ASR subtitles by structuring them into
    full sentences by Han et al. ([2022](#bib.bib14)); HowTo100M with Distant Supervision,
    where ASR subtitles were linked to WikiHow (Koupaee & Wang, [2018](#bib.bib16))
    step descriptions via distant supervision by Lin et al. ([2022](#bib.bib22));
    and HTM-AA (Han et al., [2022](#bib.bib14)), an auto-aligned (AA) version of HowTo100M,
    where subtitle timestamps were adjusted to improve alignment to videos, discarding
    non-alignable subtitles. WebVid2M (Bain et al., [2021](#bib.bib3)) is a large
    open-domain dataset of 2.5M of short videos scrapped from the internet with their
    alt-text. VideoCC3M (Nagrani et al., [2022](#bib.bib28)) is a dataset of 10M video-text
    pairs collected by transferring captions from image-text CC3M dataset (Changpinyo
    et al., [2021](#bib.bib5)) to videos with similar visual content.'
  prefs: []
  type: TYPE_NORMAL
- en: Downstream Datasets. YouCook2 (Zhou et al., [2018](#bib.bib53)) is a dataset
    of instructional cooking videos, where each video clip is annotated with a recipe
    step. We used 3.5k test set for evaluation. MSR-VTT (Xu et al., [2016](#bib.bib48))
    contains 10k YouTube videos on various topics and human descriptions. Following
    previous works (Bain et al., [2021](#bib.bib3); Nagrani et al., [2022](#bib.bib28)),
    we use the 1k test set for evaluation. MSVD (Chen & Dolan, [2011](#bib.bib7))
    is a dataset of video snippets with their textual summary. The evaluation set
    consists of 670 videos. LSMDC (Rohrbach et al., [2015](#bib.bib37)) is a collection
    of movies sliced into video clips with human-written descriptions. The test set
    consists of 1k video-caption pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics. To evaluate zero-shot text-video retrieval, we used standard Recall@$K$
    (R1, R5, R10) and Median Rank (MedR).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As LLM, we utilize Vicuna-13B (Chiang et al., [2023](#bib.bib8)), which is
    LLAMA (Touvron et al., [2023](#bib.bib47)) model fine-tuned to follow natural
    language instructions. In [Section A.1](#A1.SS1 "A.1 Grounding Captions to Video
    Content with MiniGPT-4 ‣ Appendix A Appendix ‣ HowToCaption: Prompting LLMs to
    Transform Video Annotations at Scale") we additionally experiment with the MiniGPT-4
    model (Zhu et al., [2023](#bib.bib54)) to generate captions from subtitles grounded
    on visual content. For our T-V model, we follow the dual encoder of the BLIP architecture.
    We uniformly sample 4 frames from a video clip during training and 12 frames during
    evaluation. For HowToCaption  we use $T=10$. More details are in [Section A.4](#A1.SS4
    "A.4 Additional Implementation Details ‣ Appendix A Appendix ‣ HowToCaption: Prompting
    LLMs to Transform Video Annotations at Scale").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Ablation Studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 1: Ablation of LLM prompts. We step by step construct a prompt for LLM
    that concisely and in detail describes the caption generation task. To emphasize
    our incremental adjustments, we label the sentences as x$n$) and new sentences
    introduced in the current prompt (e.g., x4: Write only …). With each prompt, we
    obtain 2M video-text pairs from 100k HowTo100M videos that we later use for T-V
    model training (lower-resource setup). Downstream zero-shot text-video retrieval
    performance is reported.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | YouCook2 | MSR-VTT | MSVD | LSMDC | Average |'
  prefs: []
  type: TYPE_TB
- en: '| R10$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| x1: Here is an automatically recognized speech from a video: $<$. x2: Write
    a synopsis for this video. x3: Begin each sentence with an estimated timestamp.
    | 37.5 | 22.5 | 71.0 | 3 | 80.5 | 2 | 37.3 | 30 | 56.6 | 14.4 |'
  prefs: []
  type: TYPE_TB
- en: '| $<$ | 39.3 | 20.5 | 71.4 | 3 | 81.0 | 2 | 36.5 | 32.5 | 57.1 | 14.5 |'
  prefs: []
  type: TYPE_TB
- en: '| $<$ | 39.8 | 20 | 71.0 | 3 | 80.9 | 2 | 37.2 | 30.5 | 57.2 | 13.9 |'
  prefs: []
  type: TYPE_TB
- en: '| $<$ | 39.5 | 19.5 | 71.6 | 3 | 81.2 | 2 | 37.9 | 29 | 57.6 | 13.4 |'
  prefs: []
  type: TYPE_TB
- en: '| $<$ | 40.4 | 19 | 71.4 | 3 | 81.4 | 2 | 37.1 | 30 | 57.6 | 13.5 |'
  prefs: []
  type: TYPE_TB
- en: '| x1’: Here is an automatically recognized speech from a video segment that
    is cut from a long video: $<$ | 40.0 | 20 | 72.0 | 3 | 81.1 | 2 | 37.8 | 29 |
    57.7 | 13.5 |'
  prefs: []
  type: TYPE_TB
- en: '| I will give you an automatically recognized speech with timestamps from a
    video segment that is cut from a long video. $<$ | 40.6 | 19 | 72.0 | 3 | 81.6
    | 2 | 37.7 | 30 | 58.0 | 13.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Effect of a longer context. For the “no context” option, we predict
    captions from individual ASR subtitles. With our “long context” option, we input
    multiple ASR subtitles with timestamps and the model generated captions based
    on longer context. This ablation is done in lower-resource setup.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | YouCook2 | MSR-VTT | MSVD | LSMDC | Average |'
  prefs: []
  type: TYPE_TB
- en: '| R1$\uparrow$ | R1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| No context: single ASR subtitle | 11.1 | 27.9 | 38.4 | 21 | 37.7 | 62.4 |
    72.6 | 3 | 43.3 | 71.7 | 80.2 | 2 | 16.5 | 30.4 | 38.4 | 30 | 27.1 | 48.1 | 57.4
    | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| Long context: multiple ASR+timestamps | 12.1 | 30.0 | 40.6 | 19 | 37.9 |
    61.6 | 72 | 3 | 43.9 | 72.7 | 81.6 | 2 | 16.8 | 31.4 | 37.7 | 30 | 27.7 | 48.9
    | 58.0 | 13.5 |'
  prefs: []
  type: TYPE_TB
- en: Prompt Engineering. Since prompting LLM with subtitles from 1.2M videos is resource
    extensive, we perform prompt engineering ablations in a lower-resource setup,
    where we use a 100k subset of HowTo100M videos ($\sim$pp in R10, and particularly
    advantageous for the YouCook2 and MSVD datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Effect of filtering&alignment. With each post-processing variant,
    we obtain 25M video-text pairs that we later use for T-V model training. Downstream
    zero-shot text-video retrieval performance is reported.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Caption Post-processing | YouCook2 | MSR-VTT | MSVD | LSMDC | Average |'
  prefs: []
  type: TYPE_TB
- en: '| R10$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Lower bound: original ASR as supervision | 39.3 | 20 | 61.7 | 5 | 77.1 |
    2 | 31.5 | 56 | 52.4 | 20.8 |'
  prefs: []
  type: TYPE_TB
- en: '| No post-processing | 40.2 | 18 | 65.9 | 4 | 79.8 | 2 | 34.4 | 40 | 55.1 |
    16.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering (using BLIP) | 42.5 | 16 | 71.2 | 3 | 81.7 | 2 | 37.4 | 30 | 58.2
    | 12.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering&alignment (using BLIP) | 42.4 | 17 | 71.7 | 3 | 82.2 | 2 | 38.5
    | 29.5 | 58.7 | 12.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering&alignment (with ours) | 44.1 | 15 | 73.3 | 3 | 82.1 | 2 | 38.6
    | 29 | 59.5 | 12.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Filtering & Alignment. Further, we assess the impact of the proposed filtering&alignment
    procedure on the quality of captions of the acquired dataset in [Table 3](#S4.T3
    "In 4.3 Ablation Studies ‣ 4 Experimental Results ‣ HowToCaption: Prompting LLMs
    to Transform Video Annotations at Scale"). We examine the performance of the T-V
    model when trained on differently post-processed versions of the dataset. Remarkably,
    we discovered that the obtained video-caption pairs, even without any post-processing,
    significantly outperform the original ASR-based supervision. Subsequently, by
    employing the filtering and alignment procedure to leave only 25M pairs based
    on video-caption similarities derived from BLIP pre-trained weights, we achieve
    a notable performance enhancement of 3.6 p.p.in R10\. Furthermore, filtering &
    alignment with our proposed fine-tuning without forgetting yields an additional
    0.8pp boost in R10 performance. More ablations can be found in [Section A.3](#A1.SS3
    "A.3 Ablations of Filtering & Alignment Post-processing ‣ Appendix A Appendix
    ‣ HowToCaption: Prompting LLMs to Transform Video Annotations at Scale").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: Zero-shot text-to-video retrieval performance of model trained on
    different video-text datasets. For each dataset, we train our T-V model and report
    downstream zero-shot text-video retrieval performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Video-Text Training Data | YouCook2 | MSR-VTT | MSVD | LSMDC | Average |'
  prefs: []
  type: TYPE_TB
- en: '| R1$\uparrow$ | R1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| - (zero-shot, with BLIP initialization) | 6.1 | 16.2 | 23.6 | 69 | 34.3 |
    59.8 | 70.6 | 3 | 38.5 | 65.0 | 74.0 | 2 | 14.7 | 29.5 | 36.5 | 31 | 23.4 | 42.6
    | 51.2 | 26.3 |'
  prefs: []
  type: TYPE_TB
- en: '| HowTo100M with ASRs | 12.2 | 29.1 | 39.3 | 20 | 30.8 | 52.6 | 61.7 | 5 |
    39.2 | 68.3 | 77.1 | 2 | 12.9 | 24.7 | 31.5 | 56 | 23.8 | 43.7 | 52.4 | 20.8 |'
  prefs: []
  type: TYPE_TB
- en: '| HowTo100M with distant supervision | 8.3 | 21.5 | 30.3 | 34 | 28.6 | 54.0
    | 66.3 | 5 | 38.5 | 68.6 | 79.4 | 2 | 12.1 | 24.7 | 32.4 | 42.5 | 21.9 | 42.2
    | 52.1 | 20.9 |'
  prefs: []
  type: TYPE_TB
- en: '| HTM-AA | 13.4 | 32.2 | 43.5 | 15 | 29.8 | 54.1 | 64.3 | 4 | 38.7 | 68.6 |
    78.7 | 2 | 11.9 | 23.9 | 30.5 | 46 | 23.5 | 44.7 | 54.3 | 16.8 |'
  prefs: []
  type: TYPE_TB
- en: '| HowToCaption (ours) | 13.4 | 33.1 | 44.1 | 15 | 37.6 | 62.0 | 73.3 | 3 |
    44.5 | 73.3 | 82.1 | 2 | 17.3 | 31.7 | 38.6 | 29 | 28.2 | 50.0 | 59.5 | 12.3 |'
  prefs: []
  type: TYPE_TB
- en: '| VideoCC3M | 5.3 | 15.1 | 21.7 | 84 | 33.9 | 57.9 | 67.1 | 4 | 39.6 | 66.7
    | 76.8 | 2 | 14.8 | 29.4 | 35.8 | 33 | 23.4 | 42.3 | 50.4 | 30.8 |'
  prefs: []
  type: TYPE_TB
- en: '| WebVid2M | 7.3 | 20.7 | 29.0 | 46 | 38.5 | 61.7 | 71.9 | 3 | 44.5 | 73.4
    | 82.1 | 2 | 17.8 | 31.2 | 39.8 | 25 | 27.0 | 46.8 | 55.7 | 19.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Comparision With Other Web Datasets. In [Table 4](#S4.T4 "In 4.4 Main Results
    ‣ 4 Experimental Results ‣ HowToCaption: Prompting LLMs to Transform Video Annotations
    at Scale"), we assess the pre-training effectiveness of our proposed HowToCaption dataset
    compared to other web video-language datasets. Specifically, we evaluate different
    textual annotations of HowTo100M videos: sentencified ASR subtitles (Han et al.,
    [2022](#bib.bib14)), task steps from distant supervision (Lin et al., [2022](#bib.bib22)),
    and auto-aligned ASR subtitles (Han et al., [2022](#bib.bib14)). Additionally,
    we conduct evaluations on WebVid2M (Bain et al., [2021](#bib.bib3)) and VideoCC3M (Nagrani
    et al., [2022](#bib.bib28)) datasets. Our findings indicate that the model pre-trained
    on our HowToCaption dataset significantly outperforms models pre-trained on other
    versions of HowTo100M annotations, with an average improvement of 5.2pp in R10\.
    This improvement is most pronounced for the MSR-VTT, MSVD, and LSMDC datasets,
    which feature full-sentence captions. Interestingly, for the YouCook2 dataset
    with captions in the form of step descriptions like “cut tomato”, HTM-AA already
    exhibits a high baseline performance, but our HowToCaption still provides a performance
    boost. We also observe that the VideoCC3M dataset does not enhance initial BLIP
    performance on any datasets except for the MSVD. We attribute it to the fact that
    the VideoCC3M dataset adopts captions from the CC3M dataset (Changpinyo et al.,
    [2021](#bib.bib5)) and transfers them to videos, potentially not introducing significantly
    new knowledge for the BLIP-initialised model since BLIP was pre-trained on multiple
    datasets including CC3M. On the other hand, WebVid2M demonstrated performance
    improvements across all datasets, but our HowToCaption dataset notably outperforms
    WebVid2M on YouCook2 and MSR-VTT, only underperforming on LSMDC.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparison in zero-shot text-to-video retrieval with baseline methods:
    Nagrani et al. ([2022](#bib.bib28)), Frozen-in-Time (Bain et al., [2021](#bib.bib3)),
    CLIP-straight (Portillo-Quintero et al., [2021](#bib.bib32)), CLIP4CLIP (Luo et al.,
    [2022](#bib.bib25)), VideoCoCa (Yan et al., [2022](#bib.bib50)), BLIP (Li et al.,
    [2022](#bib.bib18)). “+ fusion b.” denotes a usage of a fusion bottleneck., “+
    temp” denotes of usage of temporal attention. $\ddagger$CC (Changpinyo et al.,
    [2021](#bib.bib5))+COCO (Lin et al., [2014](#bib.bib21))+VG (Krishna et al., [2017](#bib.bib17))+SBU (Ordonez
    et al., [2011](#bib.bib31)) +LAION (Schuhmann et al., [2021](#bib.bib39)).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Vision Encoder | Image-Text Data | Video-Text Data | YouCook2 |
    MSR-VTT | MSVD | LSMDC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| R1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Nagrani et al. ([2022](#bib.bib28)) | ViT-B + fusion. b. | - | VideoCC3M
    | - | - | - | - | 18.9 | 37.5 | 47.1 | - | - | - | - | - | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| Frozen-in-Time | ViT-B/16 + temp. | CC+COCO | WebVid-2M | - | - | - | - |
    24.7 | 46.9 | 57.2 | 7 |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP-straight | ViT-B/32 | WIT | - | - | - | - | - | 31.2 | 53.7 | 64.2 |
    4 | 37.0 | 64.1 | 73.8 | 2 | 11.3 | 22.7 | 29.2 | 56.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP4CLIP | ViT-B/32 | WIT | HTM100M | - | - | - | - | 32.0 | 57.0 | 66.9
    | 4 | 38.5 | 66.9 | 76.8 | 2 | 15.1 | 28.5 | 36.4 | 28 |'
  prefs: []
  type: TYPE_TB
- en: '| VideoCoCa | $\sim$ViT-B/18 + temp. | JFT-3B | VideoCC3M | 16.5 | - | - |
    - | 31.2 | - | - | - | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| BLIP | ViT-B/16 | 5 datasets$\ddagger$ | - | 6.1 | 16.2 | 23.6 | 69 | 34.3
    | 59.8 | 70.6 | 3 | 38.5 | 65.0 | 74.0 | 2 | 14.7 | 29.5 | 36.5 | 30.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | ViT-B/16 | 5 datasets$\ddagger$ | HTM-Captions | 13.4 | 33.1 | 44.1
    | 15 | 37.6 | 62 | 73.3 | 3 | 44.5 | 73.3 | 82.1 | 2 | 17.3 | 31.7 | 38.6 | 29
    |'
  prefs: []
  type: TYPE_TB
- en: 'Comparison with SOTA in Zero-shot Text-Video Retrieval. In [Table 5](#S4.T5
    "In 4.4 Main Results ‣ 4 Experimental Results ‣ HowToCaption: Prompting LLMs to
    Transform Video Annotations at Scale"), we also conduct a comparison with zero-shot
    retrieval baselines. It is important to acknowledge that comparing state-of-the-art
    methods can be challenging due to variations in backbone capacity, training objectives,
    and other factors. Nevertheless, it is worth highlighting that our approach consistently
    outperforms the baseline methods in zero-shot text-video retrieval across all
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Zero-shot text-video+audio retrieval. MIL-NCE (Miech et al., [2020](#bib.bib27)),
    TAN (Han et al., [2022](#bib.bib14)), MMT (Gabeur et al., [2020](#bib.bib12)),
    AVLNet (Rouditchenko et al., [2021](#bib.bib38)), MCN (Chen et al., [2021](#bib.bib6)),
    EAO (Shvetsova et al., [2022](#bib.bib41)). $\ddagger$ denote text-video only
    retrieval models. R152+RX101 denotes ResNet-152+ResNeXt101.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Vision Enc | YouCook2 | MSR-VTT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| R1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MIL-NCE$\ddagger$ | S3D | 15.1 | 38.0 | 51.2 | 10 | 9.9 | 24.0 | 32.4 | 29.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| TAN$\ddagger$ | S3D | 20.1 | 45.5 | 59.5 | 7.0 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MMT | Transformer | - | - | - | - | - | 14.4 | - | 66 |'
  prefs: []
  type: TYPE_TB
- en: '| AVLNet | R152+RX101 | 19.9 | 36.1 | 44.3 | 16 | 8.3 | 19.2 | 27.4 | 47 |'
  prefs: []
  type: TYPE_TB
- en: '| MCN | R152+RX101 | 18.1 | 35.5 | 45.2 | - | 10.5 | 25.2 | 33.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| EAO | S3D | 24.6 | 48.3 | 60.4 | 6 | 9.3 | 22.9 | 31.2 | 35 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | S3D | 25.5 | 51.1 | 63.6 | 5 | 13.2 | 30.3 | 41.5 | 17 |'
  prefs: []
  type: TYPE_TB
- en: 'Text-Video+Audio Retrieval. It is known that instructional video datasets,
    e.g., HowTo100M or HD-VILA, suffer from a high correlation of audio modality to
    a textual description, therefore hindering building a text-video+audio retrieval
    system where the video is extended with audio. The usage of ASR narrations as
    supervisory textual description leads retrieval models to primarily perform speech
    recognition on the audio, hindering true language-audio connections. Therefore,
    training text-video+audio systems on these datasets usually requires additional
    regularization, such as shifting audio timestamps or assigning lower weights to
    the audio loss (Shvetsova et al., [2022](#bib.bib41)). Our HowToCaption dataset
    resolves this issue by providing richer textual descriptions, allowing us to train
    a text-video+audio retrieval system without regularization. To evaluate this,
    we train a multimodal Everything-At-Once (EAO) (Shvetsova et al., [2022](#bib.bib41))
    model that learns to fuse any combinations of text, video, and audio modalities
    on our proposed HowToCaption without any additional tricks and evaluate zero-shot
    text-video+audio retrieval performance. [Table 6](#S4.T6 "In 4.4 Main Results
    ‣ 4 Experimental Results ‣ HowToCaption: Prompting LLMs to Transform Video Annotations
    at Scale") shows the proposed model significantly outperforms all baselines and
    over directly comparable EAO model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Freely available web videos serve as a rich source of multimodal text-video
    data. Nevertheless, training on such data presents challenges, primarily due to
    weak supervision offered by video subtitles for text-visual learning. In this
    method, we address this problem by leveraging the capabilities of large-language
    models (LLMs). We propose a novel approach, HowToCaption, that involves prompting
    an LLM to create detailed video captions based on ASR subtitles. Simultaneously,
    we temporally align the generated captions to videos by predicting timestamps
    with LLM that is further followed by the filtering & alignment step, which additionally
    ensures synchronization with the video content. To validate the efficacy of the
    proposed HowToCaption method, we curate a new large-scale HowToCaption dataset,
    featuring high-quality human-style textual video descriptions derived from the
    videos and ASR subtitles of the HowTo100M dataset. Our HowToCaption dataset helps
    to improve performance across multiple text-video retrieval benchmarks and also
    separates textual subtitles from the audio modality, enhancing text-to-video-audio
    tasks. This work demonstrates the potential of LLMs for creating annotation-free,
    large-scale text-video datasets.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abu-El-Haija et al. (2016) Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee,
    Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan.
    Youtube-8m: A large-scale video classification benchmark. *arXiv preprint arXiv:1609.08675*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amrani et al. (2021) Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein.
    Noise estimation using density estimation for self-supervised multimodal learning.
    *AAAI*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bain et al. (2021) Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.
    Frozen in time: A joint video and image encoder for end-to-end retrieval. In *ICCV*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang & Bergen (2023) Tyler A Chang and Benjamin K Bergen. Language model behavior:
    A comprehensive survey. *arXiv preprint arXiv:2303.11504*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Changpinyo et al. (2021) Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
    Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize
    long-tail visual concepts. In *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde Kuehne,
    Samuel Thomas, Angie Boggust, Rameswar Panda, Brian Kingsbury, Rogerio Feris,
    David Harwath, et al. Multimodal clustering networks for self-supervised learning
    from unlabeled videos. In *ICCV*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen & Dolan (2011) David Chen and William B Dolan. Collecting highly parallel
    data for paraphrase evaluation. In *ACL*, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2021) Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language
    tasks via text generation. In *ICML*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Desai & Johnson (2021) Karan Desai and Justin Johnson. Virtex: Learning visual
    representations from textual annotations. In *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gabeur et al. (2020) Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia
    Schmid. Multi-modal transformer for video retrieval. In *ECCV*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghadiyaram et al. (2019) Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-scale
    weakly-supervised pre-training for video action recognition. In *CVPR*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2022) Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal alignment
    networks for long-term video. In *CVPR*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. (2019) Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua
    Lin. Learning a unified classifier incrementally via rebalancing. In *CVPR*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koupaee & Wang (2018) Mahnaz Koupaee and William Yang Wang. Wikihow: A large
    scale text summarization dataset. *arXiv preprint arXiv:1810.09305*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
    Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A
    Shamma, et al. Visual genome: Connecting language and vision using crowdsourced
    dense image annotations. *IJCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
    Bootstrapping language-image pre-training for unified vision-language understanding
    and generation. In *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models. *arXiv preprint arXiv:2301.12597*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lialin et al. (2023) Vladislav Lialin, Stephen Rawls, David Chan, Shalini Ghosh,
    Anna Rumshisky, and Wael Hamza. Scalable and accurate self-supervised multimodal
    representation learning without aligned video and text data. In *WACV*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *ECCV*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022) Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach,
    Shih-Fu Chang, and Lorenzo Torresani. Learning to recognize procedural activities
    with distant supervision. In *CVPR*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *ICLR*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2019) Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
    Pretraining task-agnostic visiolinguistic representations for vision-and-language
    tasks. In *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2022) Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan
    Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video
    clip retrieval and captioning. *Neurocomputing*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand
    Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding
    by watching hundred million narrated video clips. In *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miech et al. (2020) Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan
    Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations
    from uncurated instructional videos. In *CVPR*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagrani et al. (2022) Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja
    Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid. Learning audio-video modalities
    from image captions. In *ECCV*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neelakantan et al. (2022) Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford,
    Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris
    Hallacy, et al. Text and code embeddings by contrastive pre-training. *arXiv preprint
    arXiv:2201.10005*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation
    learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ordonez et al. (2011) Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text:
    Describing images using 1 million captioned photographs. In *NeurIPS*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Portillo-Quintero et al. (2021) Jesús Andrés Portillo-Quintero, José Carlos
    Ortiz-Bayliss, and Hugo Terashima-Marín. A straightforward framework for video
    retrieval using clip. In *Pattern Recognition: 13th Mexican Conference*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models
    from natural language supervision. In *ICML*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2023) Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
    McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.
    In *ICML*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rohrbach et al. (2015) Anna Rohrbach, Marcus Rohrbach, and Bernt Schiele. The
    long-short story of movie description. In *GCPR*. Springer, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rouditchenko et al. (2021) Andrew Rouditchenko, Angie Boggust, David Harwath,
    Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar
    Panda, Rogerio Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, and
    James Glass. Avlnet: Learning audio-visual language representations from instructional
    videos. In *Interspeech*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont,
    Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev,
    and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text
    pairs. *arXiv preprint arXiv:2111.02114*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
    Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic
    image captioning. In *ACL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shvetsova et al. (2022) Nina Shvetsova, Brian Chen, Andrew Rouditchenko, Samuel
    Thomas, Brian Kingsbury, Rogerio S Feris, David Harwath, James Glass, and Hilde
    Kuehne. Everything at once-multi-modal fusion transformer for video retrieval.
    In *CVPR*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stroud et al. (2020) Jonathan C Stroud, Zhichao Lu, Chen Sun, Jia Deng, Rahul
    Sukthankar, Cordelia Schmid, and David A Ross. Learning video representations
    from textual web supervision. *arXiv preprint arXiv:2007.14937*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2019) Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei,
    and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations.
    *arXiv preprint arXiv:1908.08530*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019) Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and
    Cordelia Schmid. Videobert: A joint model for video and language representation
    learning. In *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan & Bansal (2019) Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality
    encoder representations from transformers. *arXiv preprint arXiv:1908.07490*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A
    strong, replicable instruction-following model. *Stanford Center for Research
    on Foundation Models*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2016) Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
    video description dataset for bridging video and language. In *CVPR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2022) Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei
    Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language
    representation with large-scale video transcriptions. In *CVPR*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2022) Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh,
    Yonghui Wu, and Jiahui Yu. Video-text modeling with zero-shot transfer from contrastive
    captioners. *arXiv preprint arXiv:2212.04979*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zellers et al. (2021) Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung
    Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script
    knowledge models. *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Rohit Girdhar.
    Learning video representations from large language models. In *CVPR*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2018) Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic
    learning of procedures from web instructional videos. In *AAAI*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the appendix, we provide additional experimental evaluations and additional
    implementation details. First, we conduct experiments with MiniGPT-4 (Zhu et al.,
    [2023](#bib.bib54)) to generate captions that are grounded on visual content in [Section A.1](#A1.SS1
    "A.1 Grounding Captions to Video Content with MiniGPT-4 ‣ Appendix A Appendix
    ‣ HowToCaption: Prompting LLMs to Transform Video Annotations at Scale"). Then,
    we provide additional results of prompt engineering in [Section A.2](#A1.SS2 "A.2
    Additional Results in Prompt Engineering ‣ Appendix A Appendix ‣ HowToCaption:
    Prompting LLMs to Transform Video Annotations at Scale"). Finally, we perform
    ablation of our filtering & alignment method in [Section A.3](#A1.SS3 "A.3 Ablations
    of Filtering & Alignment Post-processing ‣ Appendix A Appendix ‣ HowToCaption:
    Prompting LLMs to Transform Video Annotations at Scale") and provide additional
    implementation details in [Section A.4](#A1.SS4 "A.4 Additional Implementation
    Details ‣ Appendix A Appendix ‣ HowToCaption: Prompting LLMs to Transform Video
    Annotations at Scale").'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Grounding Captions to Video Content with MiniGPT-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our generated captions with Vicuna-13B are based solely on ASR subtitles. To
    additionally ground the produced captions on visual content, we experiment with
    the recent miniGPT-4 model (Zhu et al., [2023](#bib.bib54)). The MiniGPT-4 consists
    of the frozen Vicuna-13B model and a visual encoder with a Q-Former Li et al.
    ([2022](#bib.bib18)) that projects visual features from an image into tokens in
    a language model embedding space that are later treated as word tokens in the
    Vicuna-13B model. To ground generated captions in the visual modality, we create
    a grid image from 4 uniformly sampled frames from a video clip and slightly adapt
    the prompt to enforce the LLM to leverage the given image into generated captions
    ([Table 7](#A1.T7 "In A.4 Additional Implementation Details ‣ Appendix A Appendix
    ‣ HowToCaption: Prompting LLMs to Transform Video Annotations at Scale")). We
    applied our approach to obtain visually grounded captions with the MiniGPT-4 model
    and obtain HowToCaption-grounded. For this dataset, we follow exactly the same
    hyperparameters that we use for HowToCaption. In [Table 8](#A1.T8 "In A.4 Additional
    Implementation Details ‣ Appendix A Appendix ‣ HowToCaption: Prompting LLMs to
    Transform Video Annotations at Scale"), we evaluate the downstream retrieval performance
    of the T-V model trained on HowToCaption-grounded. The dataset shows mixed results
    compared to the HowToCaption; while it is beneficial for the MSR-VTT and the MSVD
    dataset, performance on the YouCook2 dataset drops. To facilitate further analysis,
    we will release both captions sets: the ASR-based only HowToCaption, produced
    by the Vicuna-13B, and HowToCaption-grounded, produced by the MiniGPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Additional Results in Prompt Engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [Table 9](#A1.T9 "In A.4 Additional Implementation Details ‣ Appendix A
    Appendix ‣ HowToCaption: Prompting LLMs to Transform Video Annotations at Scale"),
    we provide an additional evaluation of language prompts. First, we experiment
    with phrases such as “write a likely summary$\ldots$”. While the keyword “likely”
    almost does not change downstream performance, the keyword “creative” is not beneficial
    for 3 out of 4 datasets. We also experiment with utilizing another timestamp format
    in the LLM prompt. Namely, instead of using “n”s (such as 0s, 65s), we use “minutes”:“seconds”
    format (such as 00:00, 01:05). We found that simple timestamp format “n”s results
    in higher performance.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Ablations of Filtering & Alignment Post-processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [Table 10](#A1.T10 "In A.4 Additional Implementation Details ‣ Appendix
    A Appendix ‣ HowToCaption: Prompting LLMs to Transform Video Annotations at Scale"),
    we ablate two modifications of the fine-tuning and alignment processes for the
    second round of filtering & alignment. We observe that the dataset obtained after
    the second round of filtering & alignment without these modifications shows lower
    performance than the dataset obtained with the first round (using the BLIP model).
    We attribute this to forgetting during fine-tuning. However, we note that both
    proposed modifications boost performance, as well as their combination. In [Table 11](#A1.T11
    "In A.4 Additional Implementation Details ‣ Appendix A Appendix ‣ HowToCaption:
    Prompting LLMs to Transform Video Annotations at Scale"), we also analyze if more
    rounds of filtering & alignment lead to a better quality dataset. We employ 20k
    iterations of fine-tuning of the T-V model on the obtained dataset after each
    filtering & alignment round. We do not observe any performance boost with more
    filtering & alignment rounds.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Additional Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our T-V model, we follow BLIP’s (Li et al., [2022](#bib.bib18)) dual encoder
    architecture with a ViT-B/16 visual encoder and a $\textrm{BERT}_{\textrm{base}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Prompts for the Vicuna-13B and MiniGPT-4 models. Difference is highlighted
    with bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Vicuna-13B | MiniGPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| I will give you an automatically recognized speech with timestamps from a
    video segment that is cut from a long video. Write a summary for this video segment.
    Write only short sentences. Describe only one action per sentence. Keep only actions
    that happen in the present time. Begin each sentence with an estimated timestamp.
    Here is this automatically recognized speech: $<$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Comparison of HowToCaption and HowToCaption-grounded datasets obtained
    with Vicuna-13b and MiniGPT-4 large language models, respectively. For each dataset,
    we train a T-V model and report downstream zero-shot text-video retrieval performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | YouCook2 | MSR-VTT | MSVD | LSMDC |'
  prefs: []
  type: TYPE_TB
- en: '| R1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| HowToCaption (Vicuna-13B) | 13.4 | 33.1 | 44.1 | 15 | 37.6 | 62.0 | 73.3
    | 3 | 44.5 | 73.3 | 82.1 | 2 | 17.3 | 31.7 | 38.6 | 29 |'
  prefs: []
  type: TYPE_TB
- en: '| HowToCaption-grounded (MiniGPT-4) | 12.4 | 29.8 | 39.9 | 20.5 | 38.3 | 62.5
    | 73.2 | 2 | 46.2 | 73.9 | 82.5 | 2 | 16.8 | 31.0 | 38.7 | 27 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Additional experiments with LLM prompts. We report modifications that
    we have done compared to our default prompt, which is highlighted. With each prompt,
    we obtain 2M video-text pairs from 100k HowTo100M videos that we later use for
    T-V model training (low-recourse setup). Downstream zero-shot text-video retrieval
    performance is reported.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | YouCook2 | MSR-VTT | MSVD | LSMDC | Average |'
  prefs: []
  type: TYPE_TB
- en: '| R10$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| I will give you an automatically recognized speech with timestamps from a
    video segment that is cut from a long video. Write a summary for this video segment.
    Write only short sentences. Describe only one action per sentence. Keep only actions
    that happen in the present time. Begin each sentence with an estimated timestamp.
    Here is this automatically recognized speech: $<$ (ours) | 40.6 | 19 | 72.0 |
    3 | 81.6 | 2 | 37.7 | 30 | 58.0 | 13.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Modification: Write a summary for this video segment. $\longrightarrow$ Write
    a likely summary for this video segment. | 40.8 | 18.5 | 71.4 | 3 | 81.5 | 2 |
    37.7 | 30 | 57.9 | 13.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Modification: Write a summary for this video segment. $\longrightarrow$ Write
    a creative summary for this video segment. | 40.0 | 19 | 71.6 | 3 | 81.2 | 2 |
    37.8 | 27 | 57.7 | 12.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Modification: $<$ | 40.8 | 18.5 | 71.5 | 3 | 81.2 | 2 | 37.2 | 29 | 57.7
    | 13.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Ablation of our filtering & alignment method. With each post-processing
    variant, we obtain 25M video-text pairs that we later use for T-V model training.
    Downstream zero-shot text-video retrieval performance is reported.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Caption Post-processing | YouCook2 | MSR-VTT | MSVD | LSMDC | Average |'
  prefs: []
  type: TYPE_TB
- en: '| R10$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering&alignment (using the BLIP) | 42.4 | 17 | 71.7 | 3 | 82.2 | 2 |
    38.5 | 29.5 | 58.7 | 12.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering&alignment after second round | 42.4 | 17 | 69.4 | 3 | 81.2 | 2
    | 38.1 | 33 | 57.8 | 13.8 |'
  prefs: []
  type: TYPE_TB
- en: '| + regularization $L_{align}$ | 44.3 | 15 | 71.9 | 3 | 81.9 | 2 | 39 | 28
    | 59.3 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| + averaging similarities of the finetuned and original model | 43.7 | 15
    | 72.8 | 3 | 82 | 2 | 39.6 | 27 | 59.5 | 11.8 |'
  prefs: []
  type: TYPE_TB
- en: '| +regularization $L_{align}$ + averaging similarities of the finetuned and
    original model (ours) | 44.1 | 15 | 73.3 | 3 | 82.1 | 2 | 38.6 | 29 | 59.5 | 12.3
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Ablation of the filtering & alignment method with more rounds of
    filtering & alignment with fine-tuning of the T-V model after each round. With
    each post-processing variant, we obtain 25M video-text pairs that we later use
    for T-V model training. Downstream zero-shot text-video retrieval performance
    is reported.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Caption Post-processing | YouCook2 | MSR-VTT | MSVD | LSMDC | Average |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| R10$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering&alignment (using the BLIP) = 1 round | 42.4 | 17 | 71.7 | 3 | 82.2
    | 2 | 38.5 | 29.5 | 58.7 | 12.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering&alignment after 2’nd round (ours) | 44.1 | 15 | 73.3 | 3 | 82.1
    | 2 | 38.6 | 29 | 59.5 | 12.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Filtering&alignment after 4’th round | 44.5 | 15 | 72.2 | 3 | 81.8 | 2 |
    38.6 | 29 | 59.3 | 12.3 |'
  prefs: []
  type: TYPE_TB
