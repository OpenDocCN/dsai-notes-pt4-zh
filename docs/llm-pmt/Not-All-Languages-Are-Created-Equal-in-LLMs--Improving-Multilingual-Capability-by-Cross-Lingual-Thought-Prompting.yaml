- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability
    by Cross-Lingual-Thought Prompting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.07004](https://ar5iv.labs.arxiv.org/html/2305.07004)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Haoyang Huang¹,  Tianyi Tang²¹¹footnotemark: 1,  Dongdong Zhang¹^†,  Wayne
    Xin Zhao²'
  prefs: []
  type: TYPE_NORMAL
- en: Ting Song¹, Yan Xia¹, Furu Wei¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹Microsoft Research Asia, China
  prefs: []
  type: TYPE_NORMAL
- en: ²Gaoling School of Artificial Intelligence, Renmin University of China
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)   Equal
    contribution. $\dagger$ Corresponding author.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) demonstrate impressive multilingual capability,
    but their performance varies substantially across different languages. In this
    work, we introduce a simple yet effective method, called cross-lingual-thought
    prompting (XLT), to systematically improve the multilingual capability of LLMs.
    Specifically, XLT is a generic template prompt that stimulates cross-lingual and
    logical reasoning skills to enhance task performance across languages. We conduct
    comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding,
    and generation tasks, covering both high-resource and low-resource languages.
    Experimental results show that XLT not only remarkably enhances the performance
    of various multilingual tasks but also significantly reduces the gap between the
    average performance and the best performance of each task in different languages.
    Notably, XLT brings over 10 points of average improvement in arithmetic reasoning
    and open-domain question-answering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/42e1045a71ef2ab4f415d4d1ed3ece6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of our method. Given a request, its associated meta information
    is filled into the placeholders of the XLT template to form the language-independent
    prompt, which is fed to the LLM to enhance the generation of responses in the
    desired format.'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) demonstrate impressive multilingual capability
    in a wide range of natural language processing tasks, including language generation,
    knowledge utilization, and complex reasoning (Zhao et al., [2023](#bib.bib43)).
    Their performance in downstream tasks has been shown to reach or even surpass
    human-level performance (Brown et al., [2020](#bib.bib3); Chowdhery et al., [2022](#bib.bib7);
    Scao et al., [2022](#bib.bib33)). The capabilities of LLMs stem from the extensive
    volume of training data they leveraged (Kaplan et al., [2020](#bib.bib17)). The
    training data for current models is primarily dominated by the English language
    corpus, but it also encompasses data from other languages, as described in GPT-3 (Brown
    et al., [2020](#bib.bib3)), PaLM (Chowdhery et al., [2022](#bib.bib7)), and BLOOM (Scao
    et al., [2022](#bib.bib33)), etc.
  prefs: []
  type: TYPE_NORMAL
- en: There are over 7,000 languages worldwide, with the vast majority being low-resource
    or extremely low-resource languages (Forkel et al., [2022](#bib.bib12)). Despite
    the latest GPT-4 model (OpenAI, [2023](#bib.bib25)) demonstrating some generalization
    capabilities in multilingual tasks as evaluated on the MMLU benchmark (Hendrycks
    et al., [2021](#bib.bib14)), it is still the case that LLMs do not have equal
    capability to handle all languages, leading to imbalanced capability across different
    languages. Furthermore, several evaluation results (Bang et al., [2023](#bib.bib2);
    Jiao et al., [2023](#bib.bib16); Hendy et al., [2023](#bib.bib15); Zhu et al.,
    [2023](#bib.bib45)) indicate that large models struggle with understanding and
    generating non-English languages, particularly in low-resource or extremely low-resource
    languages. Therefore, to democratize language intelligence and minimize performance
    gaps in different language, it is essential and meaningful to stimulate and enhance
    the multilingual capability of models in non-English and low-resource languages.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, LLMs can improve multilingual capability by augmenting data  (Lin
    et al., [2022](#bib.bib21)) or fine-tuning models (Chen et al., [2021](#bib.bib4),
    [2022](#bib.bib5)), but both are computationally expensive. Alternatively, in-context
    learning with prompts can also boost performance  (Brown et al., [2020](#bib.bib3);
    Ahuja et al., [2023](#bib.bib1); Wei et al., [2022c](#bib.bib40)) but is limited
    to monolingual tasks (Sanh et al., [2022](#bib.bib32)).
  prefs: []
  type: TYPE_NORMAL
- en: This work explores a universal in-context learning approach to enhance the multilingual
    capability of LLMs. We introduce a simple yet effective method, called cross-lingual-thought
    prompting (XLT), to enable models to handle various natural language processing
    tasks across different target languages. Our method employs a generic and language-independent
    prompt, which eliminates the need to update model parameters. Depending on the
    task input type, cross-lingual-thought prompting guides the large language model
    to assume the role of an expert in a specific language for a particular task.
    Given its predefined meta information, XLT directs LLMs to respond logically through
    a process involving problem understanding, cross-lingual thinking, task analysis,
    task execution, and output formatting. During this process, our method is designed
    to stimulate models’ cross-lingual and logical reasoning skills, enabling them
    to respond to input requests regardless of the language. For enhanced performance,
    few-shot learning can also be employed with our method by providing an LLM-generated
    response output as a demonstration using cross-lingual-thought prompting zero-shot
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{\textbf{task\_name}}|  expert  for  |\TaskLanguage{\textbf{task\_language}}|.|\TaskInput{\textbf{task\_input}}|You  should  retell/repeat  the  |\TaskElement{\textbf{input\_tag}}|  in  English.You  should  |\TaskGoal{\textbf{task\_goal}}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{\textbf{output\_type}}|  (|\OutputConstraint{\textbf{output\_constraint}}|)  in  this  format  ’|\OutputFormat{\textbf{output\_type}}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Illustration of XLT template. Referring to Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual
    Capability by Cross-Lingual-Thought Prompting") and Appendix for instantiated
    examples.'
  prefs: []
  type: TYPE_NORMAL
- en: We conduct a comprehensive evaluation to verify the effectiveness of XLT across
    seven representative multilingual benchmarks of natural language reasoning, understanding,
    and generation tasks. Each benchmark includes multilingual data covering both
    high-resource and low-resource languages. The experimental results demonstrate
    that our method can significantly improve the performance of all benchmarks across
    languages under both zero-shot and few-shot learning settings. Notably, XLT achieves
    an average gain of over 10 points on the MGSM and MKQA benchmarks. Furthermore,
    we observe that our prompting method significantly reduces the gap between the
    average performance and the best performance of each task in different languages,
    indicating its potential to democratize language intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Cross-Lingual-Thought Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although LLMs are capable of accepting any input and generating responses, users
    typically structure their requests in the form of prompts to elicit the desired
    output. The design of these prompts is crucial for achieving optimal performance
    on downstream tasks, as LLMs are sensitive to the format of the prompts chosen (Zhao
    et al., [2021](#bib.bib44)). Through a process called instruction tuning (Wei
    et al., [2022a](#bib.bib38)), models can develop the ability to follow natural
    language instructions (Wei et al., [2022b](#bib.bib39)), which can reduce their
    sensitivity to prompt engineering (Wei et al., [2022a](#bib.bib38)). In accordance
    with the guidelines of the OpenAI cookbook¹¹1[https://github.com/openai/openai-cookbook](https://github.com/openai/openai-cookbook),
    we propose a cross-lingual thought prompting template, denoted as the XLT template.
    This generic template allows LLMs to respond to requests with cross-lingual thought
    and supports a wide range of multilingual tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Not All Languages Are Created
    Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting")
    displays the XLT template, with the colored sections representing placeholders.
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Not All Languages Are Created Equal
    in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting")
    showcases an example of instantiated prompt for the Chinese request. The following
    section will explain the details of constructing XLT.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Construction of XLT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The XLT template is designed to emulate the process humans employ when handling
    multilingual tasks. Our template is written in English, as English is the dominant
    language during LLM pre-training, and existing research indicates that English
    prompting is more effective for multilingual tasks (Shi et al., [2023](#bib.bib34)).
    In contrast to the vanilla prompt that only includes a task description, our XLT
    template aims to elicit multilingual capability through cross-lingual thoughts.
    This template comprises six logical instructions in sequence. To complete the
    template, only seven placeholders need to be filled in based on intrinsic knowledge
    of the task and the request, as depicted in igure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability
    by Cross-Lingual-Thought Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: Role Assigning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: . First, the model receives a role definition that helps establish the model’s
    behavior. This concept is akin to the system role of ChatGPT²²2[https://platform.openai.com/docs/guides/chat/introduction](https://platform.openai.com/docs/guides/chat/introduction).
    To achieve this, we simply need to fulfill the task name with a known category
    (such as commonsense reasoning or paraphrase identification), along with the language
    of the task in the task language field.
  prefs: []
  type: TYPE_NORMAL
- en: Task Inputting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: . Second, we explicitly append the request as the task input. The request is
    basically structured in terms of the task type so as to make sure the model can
    comprehend it. For example, in the natural language inference task, the two sentence
    inputs are specified with “premise” and “hypothesis”, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-lingual Thinking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: . We encourage the model to engage in cross-lingual thought by rephrasing the
    requested content in English, which is the dominant language used as a pivot language
    by Shi et al. ([2023](#bib.bib34)) and Ahuja et al. ([2023](#bib.bib1)). Rephrasing
    the requested content enclosed in the input tag helps the model better understand
    the request in its native language and knowledge. Our observations suggest that
    using keywords such as "retell" or "repeat" while rephrasing the content may result
    in better performance in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Task Analyzing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: . After rephrasing the task input, we need to complete the task in task goal.
    This step is comparable to the task description used in conventional prompting
    methods. In practice, we can get the task information from the literature or seek
    assistance from ChatGPT to generate effective prompts for solving the task (Jiao
    et al., [2023](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: CoT Task Solving
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: . We then ask the model to follow the instructions and complete the task step
    by step. Since LLMs exhibit a strong ability to maintain a chain-of-thought (Wei
    et al., [2022c](#bib.bib40)), we carefully design instructions to guide the model,
    with the hope that it will respond to our instructions in a step-by-step manner
    and utilize the intermediate outputs to aid in solving the task.
  prefs: []
  type: TYPE_NORMAL
- en: Output Formatting
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: . Finally, we should regularize the output format of the model to obtain the
    exact answer. LLMs are utilized in a zero- or few-shot manner, and they tend to
    generate texts that may not conform to the format of the target answer. Fortunately,
    LLMs possess a strong ability to follow instructions, and we can define the output
    format in terms of output type and output constraint. The output type can be a
    number, index, or text, while the output constraint is optional and determined
    based on the task requirements. Output constraint may include length limitations,
    language specifications, and other relevant factors.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 XLT for Few-shot Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The above construction of XLT can be directly fed to LLMs to yield outputs,
    which is performed in the zero-shot learning setting. In addition, we also explore
    incorporating demonstrations into XLT to enable few-shot learning. Different from
    previous work that just appends model outputs to the corresponding request (Shi
    et al., [2023](#bib.bib34)) or utilizes a verbalizer to format the output, our
    method constructs the demonstrations with better formatted model outputs from
    a step-by-step processing-based XLT. As illustrated in Figure [3](#S2.F3 "Figure
    3 ‣ 2.2 XLT for Few-shot Learning ‣ 2 Cross-Lingual-Thought Prompting ‣ Not All
    Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting"), we first sample a few examples from the development set and incorporate
    the requested parts into XLT. The zero-shot learning is performed over LLM to
    collect responses that are further aligned with those of the samples. Only response-aligned
    requests are assembled with the corresponding model responses to form final demonstrations
    for few-shot learning. In this way, the demonstrations are constructed with rich
    logical knowledge via XLT, which will cater to the XLT-based generation of new
    requests. In practice, we can also correct or design the demonstrations for better
    alignment with the instruction logic.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f5cd7fec343864e20ed74e555647f5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Construction process for few-shot learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To comprehensively verify the effectiveness of our method on language-independent
    generality, we evaluate our XLT template on different LLMs covering various natural
    language processing tasks in multiple languages.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Experimental Setups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 Tasks and Benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conduct evaluations on seven typical benchmarks related to reasoning, understanding,
    and generation tasks that can represent different capabilities of LLMs, encompassing
    both high-resource and low-resource languages. These benchmarks cover 27 different
    languages, including English (en), German (de), Russian (ru), French (fr), Chinese
    Simplified (zh), Spanish (es), Japanese (ja), Italian (it), Vietnamese (vi), Turkish
    (tr), Indonesian (id), Swahili (sw), Arabic (ar), Korean (ko), Greek (el), Thai
    (th), Bulgarian (bg), Hindi (hi), Estonian (et), Bengali (bn), Tamil (ta), Galician
    (gl), Urdu (ur), Telugu (te), Javanese (jv), Haitian Creole (ht), and Southern
    Quechua (qu). In terms of the language distribution statistics in the Common Crawl
    Monthly Archives³³3[https://commoncrawl.github.io/cc-crawl-statistics/plots/languages](https://commoncrawl.github.io/cc-crawl-statistics/plots/languages)
    and the language performance of LLMs (Shi et al., [2023](#bib.bib34); Ahuja et al.,
    [2023](#bib.bib1)), we have arranged them in the order of language frequency from
    high-resource to low-resource. In particular, the frequency of some underrepresented
    languages is even less than 0.1% (e.g., bn, ta, gl, ur, te, jv, ht, qu).
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasoning tasks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Arithmetic Reasoning. The MGSM (Shi et al., [2023](#bib.bib34)) benchmark contains
    grade school mathematical problems and asks the model to calculate the correct
    answer. It covers 11 languages, and we utilize the accuracy score for evaluation.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Commonsense Reasoning. The XCOPA (Ponti et al., [2020](#bib.bib29)) benchmark
    contains one premise and two choices. It asks the model to choose which one is
    the result or cause of the premise. It covers 11 languages from 11 diverse families,
    and we utilize the accuracy score for evaluation.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding tasks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural Language Inference. The XNLI (Conneau et al., [2018](#bib.bib9)) benchmark
    contains one premise and one hypothesis and requires the model to determine whether
    the hypothesis is entailed, contradicted, or neutral conditioned on the premise.
    It covers 15 languages, and we utilize the accuracy score for evaluation.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Paraphrase Identification. The PAWS-X (Yang et al., [2019](#bib.bib41)) benchmark
    contains two sentences and requires the model to judge whether they paraphrase
    each other or not. It covers 7 languages, and we utilize the accuracy score for
    evaluation.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation tasks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Question Answering. The MKQA (Longpre et al., [2021](#bib.bib23)) benchmark
    contains an open-domain question and asks the model to predict a short answer.
    Since it has unanswerable questions or long questions that do not have precise
    answers, we remove these questions during evaluation. It covers 25 languages,
    and we choose a subset of 10 languages, including de, en, es, fr, ja, ru, th,
    tr, vi, and zh. We utilize the token overlap F1 score for evaluation.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarization. The XL-Sum* (Hasan et al., [2021](#bib.bib13)) (250 test samples
    randomly sampled from XL-Sum per language) benchmark contains a long news article
    and wants the model to summarize it into a short text. It covers 44 languages,
    and we choose a subset of 6 languages, including en, es, fr, tr, vi, and zh. We
    utilize the ROUGE-1 score (Lin, [2004](#bib.bib20)) for evaluation.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine Translation. The FLORES* (Costa-jussà et al., [2022](#bib.bib10)) (200
    test samples randomly sampled from FLORES-200 per language) benchmark contains
    parallel text from Wikimedia projects for 204 languages, yielding over 40,000
    translation directions. We choose a subset of 12 directions, including high resource
    to high resource translation (i.e., zh$\leftrightarrow$th). We utilize the SacreBLEU
    score (Papineni et al., [2002](#bib.bib28); Post, [2018](#bib.bib30)) for evaluation.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Among these benchmarks, MGSM, XCOPA, XNLI, PAWS-X, and MKQA are parallel, i.e.,
    the instances are semantics-equivalent across each language. For all benchmarks,
    we report the results on the test sets using all instances (Table [5](#A0.T5 "Table
    5 ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability
    by Cross-Lingual-Thought Prompting")), except for XL-Sum and FLORES-200, where
    we only sample 250 and 200 examples respectively to show the trend of generation
    performance. In the few-shot setting, we randomly choose examples from the development
    set if they have, otherwise, we translate the English training set into corresponding
    languages to construct several examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Basic Prompt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'are the vanilla in our experiments that were proposed and suggested in previous
    work. After determining the prompt, we format each monolingual instance using
    the English basic prompt. This setting is similar to the monolingual prompting
    in MEGA (Ahuja et al., [2023](#bib.bib1)). The basic prompts used for the evaluation
    of each benchmark are listed in Table [5](#A0.T5 "Table 5 ‣ Not All Languages
    Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting"). Note that, we dismiss the baseline using native-language, since MEGA (Ahuja
    et al., [2023](#bib.bib1)) reveals monolingual prompting is superior to cross-lingual
    prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-Thought
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: (CoT) prompting invokes LLMs to generate a series of intermediate results to
    solve reasoning tasks Wei et al. ([2022c](#bib.bib40)), which is still effective
    under multilingual scenarios Shi et al. ([2023](#bib.bib34)). In experiments,
    we append the instruction “Let’s think step-by-step and tell me the answer in
    the end” after the input to prompt LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Translate-English
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: leverages the robust capabilities of LLMs in English to tackle multilingual
    tasks, as suggested by both Shi et al. ([2023](#bib.bib34)) and Ahuja et al. ([2023](#bib.bib1)).
    This approach translates instances from other languages into English beforehand.
    In practice, we utilize the Google Translate API to translate examples into English
    and apply the basic prompt to format them. Note that, we do not apply this method
    to generation tasks since they require the output in respective language rather
    English.
  prefs: []
  type: TYPE_NORMAL
- en: XLT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'utilizes the proposed template consisting of multiple instructions introduced
    in Section [2](#S2 "2 Cross-Lingual-Thought Prompting ‣ Not All Languages Are
    Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting"). The instantiated XLT templates for each benchmark are listed in Table [6](#A0.T6
    "Table 6 ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual
    Capability by Cross-Lingual-Thought Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In few-shot learning scenarios, for basic prompt, we use the same template
    as an additional input to the model. For XLT, we provide the exemplars with XLT
    template inputs and anticipate desirable step-by-step outputs as outlined in Figure [3](#S2.F3
    "Figure 3 ‣ 2.2 XLT for Few-shot Learning ‣ 2 Cross-Lingual-Thought Prompting
    ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability
    by Cross-Lingual-Thought Prompting"). In the subsequent evaluation, we apply the
    5-shot setting, except for the XL-Sum* experiments, which use the 3-shot setting
    due to input length constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We mainly evaluate two LLMs from the GPT-3.5 series models:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: text-davinci-003⁴⁴4[https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)
    is trained using instruction tuning and reinforcement learning from human feedback (Ouyang
    et al., [2022](#bib.bib26)). It can perform a wide range of natural language tasks
    with satisfactory results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo[4](#footnote4 "footnote 4 ‣ 1st item ‣ 3.1.3 LLMs ‣ 3.1 Experimental
    Setups ‣ 3 Experiments ‣ Not All Languages Are Created Equal in LLMs: Improving
    Multilingual Capability by Cross-Lingual-Thought Prompting") is optimized for
    chat based on text-davinci-003 and suitable for traditional NLP tasks. It is the
    most capable GPT-3.5 model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To verify the compatibility of our XLT template, we further incorporate LLaMA-2-Chat Touvron
    et al. ([2023](#bib.bib36)) (Llama-2-70b-chat-hf) as our base models. It is an
    open-source model that has been trained through supervised fine-tuning and reinforcement
    learning from human feedback on the base LLaMA 2 model. In addition, we also refer
    to the existing results from other LLMs, such as code-davinci-002[4](#footnote4
    "footnote 4 ‣ 1st item ‣ 3.1.3 LLMs ‣ 3.1 Experimental Setups ‣ 3 Experiments
    ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability
    by Cross-Lingual-Thought Prompting"), when the evaluation is comparable. During
    inference, we employ greedy search (i.e., temperature=0) to generate the LLM responses.
    We find LLMs have excellent instruction-following abilities to respond to our
    instructions in the given format. Therefore, we just extract the part after “Answer
    format:” as labels.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings | Reasoning | Understanding | Generation |'
  prefs: []
  type: TYPE_TB
- en: '| MGSM | XCOPA | XNLI | PAWS-X | MKQA | XL-Sum* | FLORES* |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 12.5 | 70.1 | 53.3 | 52.0 | 29.0 | 23.7 | 15.4 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 25.7 | 70.9 | 53.0 | 57.8 | 30.9 | 23.8 | 15.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Translate-En | 15.7 | 68.0 | 54.8 | 55.0 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 23.9 | 73.3 | 62.4 | 57.1 | 40.2 | 25.2 | 17.7 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 23.3 | 76.9 | 52.6 | 65.5 | 31.6 | 24.7 | 19.1 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 45.5 | 78.3 | 54.8 | 61.0 | 14.8 | 25.4 | 19.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Translate-En | 27.1 | 75.7 | 52.2 | 66.8 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 70.0 | 80.3 | 65.5 | 63.6 | 42.7 | 26.1 | 21.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 45.5 | 75.6 | 59.1 | 68.7 | 39.1 | 26.8 | – |'
  prefs: []
  type: TYPE_TB
- en: '| Translate-En | 46.5 | 77.4 | 56.9 | 68.5 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 55.4 | 81.3 | 67.5 | 72.2 | 49.6 | 27.3 | – |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 63.0 | 80.1 | 61.4 | 66.4 | 43.7 | 25.5 | – |'
  prefs: []
  type: TYPE_TB
- en: '| Translate-En | 65.1 | 81.9 | 58.3 | 63.7 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 72.5 | 85.9 | 65.0 | 69.1 | 52.5 | 27.9 | – |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The average scores in different languages for the seven benchmarks
    in zero-shot and few-shot settings. We omit the results (denoted as “–”) of Translate-En
    since it is not applicable for generation tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings | Reasoning | Understanding | Generation |'
  prefs: []
  type: TYPE_TB
- en: '| MGSM | XCOPA | XNLI | PAWS-X | MKQA |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot setting |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 65.2 | 77.8 | 83.8 | 97.1 | 60.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 65.4 | 80.1 | 83.5 | 89.5 | 61.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Translate-En | 77.2 | 78.7 | 86.0 | 95.3 | 51.6 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 68.5 | 82.1 | 80.7 | 88.4 | 78.7 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 73.0 | 83.6 | 80.5 | 89.0 | 61.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 66.7 | 85.7 | 80.7 | 88.9 | 46.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Translate-En | 80.4 | 84.6 | 79.8 | 90.7 | 54.1 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 84.1 | 89.1 | 88.0 | 96.2 | 75.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot setting |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 75.4 | 82.0 | 82.5 | 88.2 | 74.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Translate-En | 77.1 | 82.6 | 79.5 | 87.8 | 68.5 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 84.5 | 85.6 | 85.3 | 91.6 | 82.7 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 76.1 | 84.1 | 83.6 | 94.4 | 82.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Translate-En | 78.6 | 86.4 | 79.2 | 95.4 | 71.3 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 86.2 | 89.7 | 84.3 | 94.1 | 83.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The democratization degree of tasks against languages.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multilingual Capability.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We comprehensively evaluate XLT’s performance over seven tasks. The average
    score of text-davinci-003 is summarized in Figure LABEL:fig:radar(a) and Table [1](#S3.T1
    "Table 1 ‣ 3.1.3 LLMs ‣ 3.1 Experimental Setups ‣ 3 Experiments ‣ Not All Languages
    Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting"), and more details are listed in Appendix [A](#A1 "Appendix A Additional
    Experiments ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual
    Capability by Cross-Lingual-Thought Prompting"). As for the CoT prompting, it
    can enhance reasoning tasks while becomes less effective on understanding and
    generation tasks. In terms of the Translate-En prompting, it can boost the performance
    in the zero-shot settings while may not work well in the few-shot settings. Overall,
    compared to the three baseline methods, XLT achieves significant improvements
    over two LLMs for all tasks on both zero-shot and few-shot settings regardless
    of the language difference, except for a slight drop on the PAWS-X benchmark in
    the zero-shot setting. It is noted that XLT achieves remarkable gains of nearly
    20 points on average in the MGSM benchmark for the arithmetic reasoning task and
    around 10 points on average in the MKQA benchmark for the open-domain question
    answering task. The experiments demonstrates the effectiveness of XLT for empowering
    LLM with multilingual capability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the compatibility test, we list the results of LLaMA-2-Chat on the MGSM
    benchmark in Table [7](#A1.T7 "Table 7 ‣ A.1 Results on Reasoning Tasks ‣ Appendix
    A Additional Experiments ‣ Not All Languages Are Created Equal in LLMs: Improving
    Multilingual Capability by Cross-Lingual-Thought Prompting"). It is notable that
    LLaMA 2 can also benefit from our cross-lingual-thought, which further demonstrates
    the generality of our XLT template. However, the gains of LLaMA-2-Chat is not
    as good as GPT-based models. Our analysis reveals this gap can primarily be attributed
    to LLaMA 2’s poorer multi-step instruction-following ability.'
  prefs: []
  type: TYPE_NORMAL
- en: Language Democratization.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Furthermore, we try to assess the democratization degree of tasks between languages
    by defining a “democratization score”, which calculates the average percentage
    of performance attained by different languages relative to the best performance
    among all languages. Given the evaluation scores of $s_{1},s_{2},\dots,s_{l}$
    language on a task, the democratization score is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{\sum_{i=1}^{l}s_{i}}{l}/\max\{s_{i}\}_{i=1}^{l}.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Table [2](#S3.T2 "Table 2 ‣ 3.1.3 LLMs ‣ 3.1 Experimental Setups ‣ 3 Experiments
    ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability
    by Cross-Lingual-Thought Prompting") presents the degree of democratization for
    tasks across languages under both zero-shot learning and few-shot learning, and
    we further summarize it in Figure LABEL:fig:radar(b) by averaging all scores per
    task regardless of the setting and model differences. We can observe that XLT
    leads to higher democratization scores in general, particularly for XCOPA, and
    MKQA. As for MGSM, XNLI, and PAWS-X, our XLT can improve performance in multiple
    languages, where the overall performance of the baseline is consistently lower
    but the gap between languages is smaller as shown in Tables [7](#A1.T7 "Table
    7 ‣ A.1 Results on Reasoning Tasks ‣ Appendix A Additional Experiments ‣ Not All
    Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting"), [9](#A1.T9 "Table 9 ‣ A.2 Results on Understanding Tasks ‣ Appendix
    A Additional Experiments ‣ Not All Languages Are Created Equal in LLMs: Improving
    Multilingual Capability by Cross-Lingual-Thought Prompting"), and [10](#A1.T10
    "Table 10 ‣ A.2 Results on Understanding Tasks ‣ Appendix A Additional Experiments
    ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability
    by Cross-Lingual-Thought Prompting"). In conclusion, our method can reduce the
    performance gap between languages and improve the language democratization of
    LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings | MGSM | XNLI | FLORES* |'
  prefs: []
  type: TYPE_TB
- en: '| de | zh | hi | vi | jv$\rightarrow$jv |'
  prefs: []
  type: TYPE_TB
- en: '|  | XLT | 79.8 | 72.6 | 61.3 | 64.8 | 19.0 | 10.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Instruction Ablation |     w/o Role Assigning | 76.6 | 69.2 | 57.8 | 63.9
    | 16.2 | 8.8 |'
  prefs: []
  type: TYPE_TB
- en: '|     w/o Cross-lingual Thinking | 75.6 | 62.0 | 56.1 | 62.2 | 13.2 | 8.2 |'
  prefs: []
  type: TYPE_TB
- en: '|     w/o CoT Task Solving | 77.0 | 68.0 | 62.9 | 65.2 | 16.8 | 9.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Instruction Order |     Swap Role Assigning and Task Inputting | 77.2 | 71.8
    | 54.2 | 61.5 | 19.6 | 11.2 |'
  prefs: []
  type: TYPE_TB
- en: '|     Swap Role Assigning and Task Analyzing | 76.8 | 70.8 | 61.0 | 64.0 |
    15.8 | 8.8 |'
  prefs: []
  type: TYPE_TB
- en: '|     Swap Cross-lingual Thinking and Task Analyzing | 79.0 | 71.2 | 59.5 |
    63.4 | 16.5 | 9.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Rephrasing Word |     w/ retell | 79.8 | 72.6 | 61.3 | 64.8 | 18.2 | 10.3
    |'
  prefs: []
  type: TYPE_TB
- en: '|     w/ repeat | 77.6 | 68.0 | 60.7 | 64.6 | 19.0 | 10.5 |'
  prefs: []
  type: TYPE_TB
- en: '|     w/ translate | 76.4 | 70.0 | 60.1 | 64.5 | 17.5 | 10.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Performance comparison across different variants of XLT. All the experiments
    are conducted using gpt-3.5-turbo under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: '|    Demonstration format | en | de | ru | fr | zh | es | ja | sw | th | bn
    | te | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Basic input + Basic output | 84.0 | 79.2 | 78.8 | 78.8 | 70.8 | 81.2 | 68.8
    | 70.8 | 68.8 | 65.2 | 44.8 | 71.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic input + XLT output | 82.4 | 72.4 | 71.2 | 75.2 | 64.4 | 78.8 | 63.2
    | 66.8 | 53.6 | 54.8 | 32.4 | 65.0 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT input + XLT output | 84.8 | 81.4 | 80.2 | 79.2 | 71.8 | 81.6 | 72.8 |
    71.2 | 69.8 | 64.4 | 40.8 | 72.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance comparison across different few-shot variants on the MGSM
    benchmark. All the experiments are conducted with 5 demonstrations using gpt-3.5-turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Further Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we further investigate the factors that affect the performance
    of XLT and how they affect various multilingual benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Ablation of XLT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the XLT variants, we mainly conduct experiments to compare the following
    strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ablating the instructions. Since our XLT consists of six logical instructions,
    we disable the Role Assigning, Cross-lingual Thinking, and CoT Task Solving instructions
    separately to analyze the contribution per instruction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reordering the instructions. Considering the logicality of our instructions,
    we further change the order of the instructions in XLT to explore whether LLMs
    will handle tasks differently and lead to different results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the content word. As prompts are usually sensitive to the word choice,
    we verify the robustness of XLT when alternating the rephrasing keyword with “retell”,
    “repeat”, and “translate” in the cross-lingual thinking instruction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The outcomes are presented in Table [3](#S3.T3 "Table 3 ‣ Language Democratization.
    ‣ 3.2 Experimental Results ‣ 3 Experiments ‣ Not All Languages Are Created Equal
    in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting"),
    indicating that XLT surpasses almost all the variants, thereby validating the
    effectiveness and reasonableness of our proposed XLT method.'
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of each instruction.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The results from the “Instruction Ablation" row indicate that: (1) Cross-lingual
    Thinking yields more significant gains compared to other instructions. This suggests
    that the LLM’s ability of cross-lingual thinking is activated, allowing it to
    utilize its knowledge in English to solve tasks effectively; (2) Removing Role
    Assigning from XLT impedes the model’s understanding of the ultimate goal for
    diverse multilingual tasks, highlighting the task transferability of XLT; and
    (3) the better performance of XLT can also be attributed to CoT Task Solving,
    which requires the model to respond to complex instructions in a step-by-step
    manner.'
  prefs: []
  type: TYPE_NORMAL
- en: The order of logical instructions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The performance drop is evident when the order of our designed logical instructions
    is switched. When designing XLT, we have taken into account the process by which
    humans solve multilingual problems, and this experiment further confirms the optimum
    order of our XLT template. Placing the Role Assigning instruction later may confuse
    the model initially. Additionally, conducting Cross-lingual Thinking before Task
    Analyzing is crucial since we rely on the English task-solving abilities of LLMs
    to handle multilingual tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The robustness of word choice for rephrasing keywords.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can find that different words indeed affect the performance of XLT, but it
    is less sensitive to the other variants. Through experimentation, we have determined
    that “repeat" yields better results for text summarization and machine translation,
    while “retell" is more suitable for the remaining five tasks. Our aim is to provide
    XLT with a more unified template, while still allowing users to fine-tune specific
    keywords for optimal performance in their tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Effectiveness of XLT Few-shot Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned in Section [2.2](#S2.SS2 "2.2 XLT for Few-shot Learning ‣ 2 Cross-Lingual-Thought
    Prompting ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual
    Capability by Cross-Lingual-Thought Prompting"), the construction of demonstrations
    for XLT few-shot learning differs from the previous method. We have compared XLT
    and basic prompt. Here, we focus on the construction of the demonstration input-output
    pairs and compare various demonstrations that may be used to perform XLT few-shot
    learning. The illustrations can be found in Figure [4](#A1.F4 "Figure 4 ‣ A.3
    Results on Generation Tasks ‣ Appendix A Additional Experiments ‣ Not All Languages
    Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basic prompt input + Basic prompt output: This is the normal demonstration
    format used in most of the previous work.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basic prompt input + XLT output: This ablation is to separate the effect of
    input and output formats in the demonstration.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'XLT input + XLT output: This is the method that we used in this work.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Observing the experimental results presented in Table [4](#S3.T4 "Table 4 ‣
    Language Democratization. ‣ 3.2 Experimental Results ‣ 3 Experiments ‣ Not All
    Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting"), we can conclude that: (1) Our XLT few-shot learning outperforms all
    other variants, thus confirming its effectiveness. (2) The use of normal demonstrations
    for XLT few-shot learning leads to a decrease in performance. (3) Merely incorporating
    XLT as a demonstration input without its output does not result in any improvements.
    (4) Consistency in the demonstration for few-shot learning is crucial, implying
    that the demonstration input-output format should align better with its zero-shot
    learning input-output format.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 LLM Capability Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the impressive capabilities of LLMs, it is crucial to determine their
    impact on natural language processing tasks. Liang et al. ([2022](#bib.bib19))
    conduct a comprehensive evaluation of LLMs from various perspectives, such as
    accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency. Bang
    et al. ([2023](#bib.bib2)) extensively evaluate the ChatGPT model on multiple
    natural language processing tasks and find that the model performs well in high-resource
    languages but exhibits certain limitations in low-resource and non-Latin script
    languages. Additionally, studies by Jiao et al. ([2023](#bib.bib16)) and Hendy
    et al. ([2023](#bib.bib15)) compare different GPT models with supervised models
    for machine translation tasks and find that GPT models have competitive translation
    abilities in high-resource languages but perform less effectively in low-resource
    languages. It is worth noting that achieving multilingual generative AI capability
    necessitates cross-lingual knowledge to further improve the model’s performance.
    In this context, Ahuja et al. ([2023](#bib.bib1)) evaluate the multilingual task
    understanding ability of GPT models and attempt to enhance their task processing
    abilities in other languages using English knowledge. Our work also focuses on
    evaluating the multilingual capabilities of LLMs, including reasoning, understanding,
    and generative capabilities. Our evaluations indicate that LLMs exhibit differences
    in high-resource and low-resource abilities, which necessitates additional efforts
    to enhance their multilingual capability.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Multilingual Task Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multilingual knowledge has been shown to be exploitable and transferable between
    languages to improve model performance (Devlin et al., [2019](#bib.bib11); Conneau
    et al., [2020](#bib.bib8); Raffel et al., [2020](#bib.bib31); Ouyang et al., [2021](#bib.bib27);
    Chi et al., [2021](#bib.bib6)). While much research has been devoted to multilingual
    understanding tasks, multilingual generation tasks are more challenging, particularly
    when the target language is low-resource or non-English (Ma et al., [2021](#bib.bib24);
    Liu et al., [2020](#bib.bib22)). Two methods can enable models to support multilingual
    task processing: one is training a supervised model that covers multiple languages
    for multilingual processing (Costa-jussà et al., [2022](#bib.bib10)), and the
    other is training a pre-trained model and using fine-tuning to transfer knowledge
    among languages to achieve multilingual capability (Chen et al., [2021](#bib.bib4),
    [2022](#bib.bib5)). However, the emergence of LLMs has made it possible to directly
    process multilingual tasks via in-context learning (Brown et al., [2020](#bib.bib3);
    Ahuja et al., [2023](#bib.bib1)). These LLMs, with hundreds of billions or even
    trillions of parameters, require a significant amount of computation resources
    for training, making traditional fine-tuning methods less feasible. To improve
    the generative ability of LLMs, researchers explore in-context learning methods
    that do not require updating model parameters, such as few-shot prompting (Vilar
    et al., [2022](#bib.bib37)), automatic prompt learning (Shin et al., [2020](#bib.bib35)),
    task-instruction prompting (Ye et al., [2023](#bib.bib42)), chain-of-thought prompting (Wei
    et al., [2022c](#bib.bib40)), etc. Our work builds upon these methods and proposes
    an optimized, generic, and language-independent prompt to enhance the multilingual
    capability of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work investigates the language processing capabilities of large language
    models in multilingual settings and expects to develop a universal framework for
    handling diverse multilingual tasks. To accomplish this goal, we propose a generic
    prompt, referred to as XLT, to enhance the multilingual capability and reduce
    the performance gaps among languages in tasks related to language understanding,
    reasoning, and generation in non-English and low-resource languages. Although
    our method is generally applicable across tasks and languages, we discovered that
    prompting design factors such as instruction logic and word choice have explicit
    impacts on its effectiveness. Cross-language thinking in XLT is particularly effective.
    Finally, we hope this work can inspire further research to prioritize the development
    of generic prompting. By doing so, large language models can encompass a wider
    range of modalities and languages.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tianyi Tang and Xin Zhao are supported by National Natural Science Foundation
    of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant
    No. 4222027 and L233008.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to limitations imposed by the evaluation benchmarks and OpenAI API cost,
    we conducted tests on 27 languages, which merely scratch the surface of the vast
    array of languages in the world. Besides, our XLT template is based on English.
    It deserves to explore whether the template written in task language can lead
    to better performance and how to better construct the instruction in each language.
    Furthermore, we only verify the effectiveness of our method on two GPT-based models
    (i.e., text-davinci-003 and gpt-3.5-turbo) and LLaMA-2-Chat. It is worthwhile
    to investigate the generality of our template on more models, such as BLOOM and
    PaLM.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ahuja et al. (2023) Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain,
    Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika
    Bali, et al. 2023. Mega: Multilingual evaluation of generative ai. *arXiv preprint
    arXiv:2303.12528*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al.
    2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning,
    hallucination, and interactivity. *arXiv preprint arXiv:2302.04023*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language models are few-shot learners. In *Advances in Neural Information
    Processing Systems*, volume 33, pages 1877–1901\. Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Guanhua Chen, Shuming Ma, Yun Chen, Li Dong, Dongdong Zhang,
    Jia Pan, Wenping Wang, and Furu Wei. 2021. Zero-shot cross-lingual transfer of
    neural machine translation with multilingual pretrained encoders. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    15–26, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Guanhua Chen, Shuming Ma, Yun Chen, Dongdong Zhang, Jia
    Pan, Wenping Wang, and Furu Wei. 2022. Towards making the most of cross-lingual
    transfer for zero-shot neural machine translation. In *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, pages 142–157, Dublin, Ireland. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chi et al. (2021) Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal,
    Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021. InfoXLM:
    An information-theoretic framework for cross-lingual language model pre-training.
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3576–3588,
    Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
    and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning
    at scale. In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8440–8451, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conneau et al. (2018) Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina
    Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating
    cross-lingual sentence representations. In *Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing*, pages 2475–2485, Brussels,
    Belgium. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Costa-jussà et al. (2022) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
    Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel
    Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered
    machine translation. *arXiv preprint arXiv:2207.04672*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*, pages 4171–4186, Minneapolis, Minnesota. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Forkel et al. (2022) Robert Forkel et al. 2022. Glottocodes: Identifiers linking
    families, languages and dialects to comprehensive reference information. *Semantic
    Web*, 13(6):917–924.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hasan et al. (2021) Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi
    Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar.
    2021. XL-sum: Large-scale multilingual abstractive summarization for 44 languages.
    In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 4693–4703, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask
    language understanding. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendy et al. (2023) Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak,
    Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan
    Awadalla. 2023. How good are gpt models at machine translation? a comprehensive
    evaluation. *arXiv preprint arXiv:2302.09210*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiao et al. (2023) Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and
    Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine.
    *arXiv preprint arXiv:2301.08745*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    2020. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2023) Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu
    Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond
    english: Towards a comprehensive evaluation of large language models in multilingual
    learning. *arXiv preprint arXiv:2304.05613*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of
    summaries. In *Text Summarization Branches Out*, pages 74–81, Barcelona, Spain.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang,
    Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du,
    Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian
    O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov,
    and Xian Li. 2022. Few-shot learning with multilingual generative language models.
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 9019–9052, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,
    Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising
    pre-training for neural machine translation. *Transactions of the Association
    for Computational Linguistics*, 8:726–742.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Longpre et al. (2021) Shayne Longpre, Yi Lu, and Joachim Daiber. 2021. MKQA:
    A linguistically diverse benchmark for multilingual open domain question answering.
    *Transactions of the Association for Computational Linguistics*, 9:1389–1406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2021) Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Alexandre
    Muzio, Saksham Singhal, Hany Hassan Awadalla, Xia Song, and Furu Wei. 2021. Deltalm:
    Encoder-decoder pre-training for language generation and translation by augmenting
    pretrained multilingual encoders. *arXiv preprint arXiv:2106.13736*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *arXiv*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    language models to follow instructions with human feedback. In *Advances in Neural
    Information Processing Systems*, volume 35, pages 27730–27744\. Curran Associates,
    Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2021) Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian,
    Hua Wu, and Haifeng Wang. 2021. ERNIE-M: Enhanced multilingual representation
    by aligning cross-lingual semantics with monolingual corpora. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    27–38, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics*,
    pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ponti et al. (2020) Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu
    Liu, Ivan Vulić, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal
    commonsense reasoning. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, pages 2362–2376, Online. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post (2018) Matt Post. 2018. A call for clarity in reporting BLEU scores. In
    *Proceedings of the Third Conference on Machine Translation: Research Papers*,
    pages 186–191, Belgium, Brussels. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21(140):1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanh et al. (2022) Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang
    Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
    M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla,
    Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike
    Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit
    Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,
    Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,
    Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted
    training enables zero-shot task generalization. In *International Conference on
    Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou,
    Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought
    reasoners. In *The Eleventh International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace,
    and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with
    Automatically Generated Prompts. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, pages 4222–4235, Online. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vilar et al. (2022) David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
    Viresh Ratnakar, and George Foster. 2022. Prompting palm for translation: Assessing
    strategies and performance. *arXiv preprint arXiv:2211.09102*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language
    models are zero-shot learners. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. 2022b. Emergent abilities of large language models. *Transactions on Machine
    Learning Research*. Survey Certification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022c) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022c. Chain-of-thought prompting
    elicits reasoning in large language models. In *Advances in Neural Information
    Processing Systems*, volume 35, pages 24824–24837\. Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge.
    2019. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3687–3692, Hong Kong, China. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2023) Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun
    Kim, and Minjoon Seo. 2023. In-context instruction learning. *arXiv preprint arXiv:2302.14691*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. 2021. Calibrate before use: Improving few-shot performance of language
    models. In *Proceedings of the 38th International Conference on Machine Learning*,
    volume 139 of *Proceedings of Machine Learning Research*, pages 12697–12706\.
    PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng
    Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation
    with large language models: Empirical results and analysis. *arXiv preprint arXiv:2304.04675*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Table 5: The basic prompt of each benchmark. #Test denotes the number of instances
    in the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | #Test | Basic Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| MGSM | 250 | Request: {problem} |'
  prefs: []
  type: TYPE_TB
- en: '| XCOPA | 500 | Here is a premise: {premise}. What is the {question}? Help
    me pick the more plausible option: -choice1: {choice1}, -choice2: {choice2} |'
  prefs: []
  type: TYPE_TB
- en: '| XNLI | 5,010 | {premise} Based on previous passage, is it true that {hypothesis}?
    Yes, No, or Maybe? |'
  prefs: []
  type: TYPE_TB
- en: '| PAWS-X | 2,000 | Sentence 1: {sentence1} Sentence 2: {sentence2} Question:
    Does Sentence 1 paraphrase Sentence 2? Yes or No? |'
  prefs: []
  type: TYPE_TB
- en: '| MKQA | 6,758 | Answer the question in one or a few words in {target_language}:
    {question}? |'
  prefs: []
  type: TYPE_TB
- en: '| XL-Sum* | 250 | Summarize this article: {article} |'
  prefs: []
  type: TYPE_TB
- en: '| FLORES* | 200 | {source} Translate from {source_language} to {target_language}:
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Task meta data consisting of task name, input tag, task goal, output
    type, and output constraint per benchmark. Detailed examples of the input for
    each benchmark are listed in the following part.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | Task name | Input tag | Task goal | Output type | Output constraint
    |'
  prefs: []
  type: TYPE_TB
- en: '| MGSM | arithmetic reasoning | request | do step-by-step answer to obtain
    a number answer | answer | – |'
  prefs: []
  type: TYPE_TB
- en: '| XCOPA | commonsense reasoning | premise and the options | do step-by-step
    answer to pick a choice | choice number | – |'
  prefs: []
  type: TYPE_TB
- en: '| XNLI | natural language inference | hypothesis and the premise | judge whether
    the hypothesis is true, false, or undetermined given the premise. The relationship
    can be chosen from entailment, contradiction, and neutral | relationship | – |'
  prefs: []
  type: TYPE_TB
- en: '| PAWS-X | paraphrase identification | sentence 1 and sentence 2 | provide
    a yes or no answer to the question: Does Sentence 1 paraphrase Sentence 2? | answer
    | choosing either yes or no |'
  prefs: []
  type: TYPE_TB
- en: '| MKQA | question answering | question | answer the question in English in
    one or a few words | answer | in one or a few words in {target_language} |'
  prefs: []
  type: TYPE_TB
- en: '| XL-Sum | multilingual summarization | entire text | think step-by-step to
    summarize the entire text in a maximum of two sentences | summary | into one sentence
    in {target_language} |'
  prefs: []
  type: TYPE_TB
- en: '| FLORES | machine translation | source sentence | provide the {target_language}
    translation for the English source sentence | target translation | – |'
  prefs: []
  type: TYPE_TB
- en: Appendix A Additional Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Results on Reasoning Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [7](#A1.T7 "Table 7 ‣ A.1 Results on Reasoning Tasks ‣ Appendix A Additional
    Experiments ‣ Not All Languages Are Created Equal in LLMs: Improving Multilingual
    Capability by Cross-Lingual-Thought Prompting") presents the results of the MGSM
    benchmark. XLT significantly improves the arithmetic reasoning capabilities of
    both models, particularly for gpt-3.5-turbo in the zero-shot setting. We hypothesize
    that gpt-3.5-turbo may have undergone supervised fine-tuning (Ouyang et al., [2022](#bib.bib26))
    with arithmetic reasoning samples in the chain-of-thought format, which enables
    XLT to activate its arithmetic reasoning ability directly. For both low-resource
    languages (e.g., sw, th, bn, and te) and high-resource languages, XLT can further
    enhance the performance. Even under the few-shot setting, XLT can still significantly
    improve the reasoning performance of both models and reduce the performance gap
    for all languages. Notably, for some high-resource languages, such as de, ru,
    fr, and es, the performance is comparable to English.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings (high$\rightarrow$low) | en | de | ru | fr | zh | es | ja | sw |
    th | bn | te | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 19.2 | 12.8 | 15.6 | 16.4 | 15.2 | 13.6 | 12.8 | 7.2 | 8.8
    | 11.6 | 4.4 | 12.5 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 30.0 | 32.4 | 23.6 | 34.8 | 29.2 | 26.8 | 26.0 | 13.6 | 18.4 | 14.8
    | 12.8 | 23.9 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 32.0 | 24.8 | 28.0 | 31.6 | 22.0 | 29.2 | 22.4 | 24.4 | 16.8
    | 18.0 | 7.6 | 23.3 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 84.4 | 79.8 | 77.6 | 75.2 | 72.6 | 76.8 | 71.0 | 70.8 | 63.8 | 56.8
    | 42.0 | 70.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-70b-chat-hf |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 58.8 | 48.0 | 47.2 | 45.6 | 39.6 | 50.4 | 39.2 | 10.0 | 13.6
    | 17.2 | 5.2 | 34.1 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 60.0 | 52.8 | 52.8 | 48.8 | 42.4 | 52.0 | 39.2 | 16.4 | 18.0 | 17.6
    | 10.4 | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | code-davinci-002 |'
  prefs: []
  type: TYPE_TB
- en: '| (Shi et al., [2023](#bib.bib34))^* | 53.6 | 46.4 | 48.8 | 46.4 | 47.2 | 51.6
    | 44.8 | 37.6 | 41.2 | 41.2 | 42.8 | 45.6 |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 60.4 | 45.6 | 51.6 | 45.6 | 38.8 | 51.6 | 37.6 | 48.8 | 30.4
    | 43.6 | 46.8 | 45.5 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 65.6 | 58.0 | 57.6 | 56.8 | 53.2 | 58.0 | 54.4 | 58.8 | 42.4 | 53.2
    | 51.8 | 55.4 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 82.8 | 69.2 | 71.6 | 72.4 | 46.8 | 71.2 | 56.0 | 60.0 | 44.0
    | 62.4 | 56.6 | 63.0 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 84.8 | 81.4 | 80.2 | 79.2 | 71.8 | 81.6 | 72.8 | 71.2 | 69.8 | 64.4
    | 40.8 | 72.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Accuracy scores on the MGSM benchmark. Shi et al. ([2023](#bib.bib34))^*
    utilize 6-shot learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings (high$\rightarrow$low) | zh | it | vi | tr | id | sw | th | et |
    ta | ht | qu | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 85.4 | 90.0 | 69.2 | 80.6 | 83.8 | 56.4 | 66.6 | 73.0 | 53.4
    | 61.6 | 50.4 | 70.1 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 85.8 | 89.2 | 76.0 | 81.0 | 86.4 | 59.2 | 67.2 | 83.4 | 55.2 | 72.2
    | 50.2 | 73.3 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 90.4 | 92.0 | 83.6 | 86.6 | 88.2 | 77.0 | 70.2 | 84.0 | 57.2
    | 65.2 | 51.2 | 76.9 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 87.8 | 89.8 | 87.5 | 90.2 | 89.5 | 82.0 | 78.0 | 88.4 | 64.0 | 74.6
    | 51.8 | 80.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | code-davinci-002 |'
  prefs: []
  type: TYPE_TB
- en: '| (Shi et al., [2023](#bib.bib34))^* | 93.4 | 96.6 | 86.6 | 91.2 | 91.4 | 67.4
    | 84.2 | 88.8 | 55.8 | 79.6 | 52.2 | 80.7 |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| (Ahuja et al., [2023](#bib.bib1))^† | – | 94.6 | – | 89.8 | 93.0 | 82.8 |
    84.8 | 89.6 | 87.0 | 82.8 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 90.8 | 92.2 | 80.2 | 85.2 | 90.8 | 63.6 | 69.2 | 81.8 | 53.6
    | 73.2 | 51.0 | 75.6 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 94.0 | 95.0 | 87.0 | 94.0 | 92.8 | 68.4 | 79.4 | 90.4 | 59.4 | 80.8
    | 53.0 | 81.3 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 91.0 | 95.2 | 86.2 | 89.0 | 88.6 | 79.2 | 73.6 | 92.0 | 58.6
    | 74.2 | 53.0 | 80.1 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 92.8 | 95.8 | 90.6 | 92.2 | 90.2 | 92.6 | 85.2 | 93.0 | 70.8 | 86.0
    | 56.2 | 85.9 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Accuracy scores on the XCOPA benchmark. Shi et al. ([2023](#bib.bib34))^*
    utilize 6-shot learning. Ahuja et al. ([2023](#bib.bib1))^† utilize 8-shot learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The XCOPA benchmark results are presented in Table [8](#A1.T8 "Table 8 ‣ A.1
    Results on Reasoning Tasks ‣ Appendix A Additional Experiments ‣ Not All Languages
    Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting"). Our XLT approach significantly enhances the performance of both models
    in both settings, as compared to basic prompting. In the zero-shot setting, XLT
    demonstrates significant improvements for relatively low-resource languages (e.g.,
    sw, th, et, ta, and ht), but it underperforms the baseline for some high-resource
    languages such as zh and it. In the few-shot setting, XLT brings enhancements
    for both high- and low-resource languages. Our findings suggest that XLT is more
    effective for low-resource languages, particularly for gpt-3.5-turbo on sw, th,
    ta, and ht, where it yields improvements of over 10 accuracy points.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Results on Understanding Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [9](#A1.T9 "Table 9 ‣ A.2 Results on Understanding Tasks ‣ Appendix A
    Additional Experiments ‣ Not All Languages Are Created Equal in LLMs: Improving
    Multilingual Capability by Cross-Lingual-Thought Prompting") presents the results
    of the XNLI benchmark. In the zero-shot setting, our XLT significantly outperforms
    the basic prompt in all languages. Additionally, when using few-shot setups on
    high- and low-resource languages, both text-davinci-003 and gpt-3.5-turbo show
    significant improvements compared to the basic prompt. Specifically, for low-resource
    languages such as th, bg, hi, and ur, XLT achieves an average improvement of 9.4
    accuracy scores for text-davinci-003 and 5.3 accuracy scores for gpt-3.5-turbo.
    This demonstrates that XLT is effective for both models, but text-davinci-003
    has better natural language inference capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings (high$\rightarrow$low) | en | de | ru | fr | zh | es | vi | tr |
    sw | ar | el | th | bg | hi | ur | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 63.6 | 59.4 | 55.9 | 60.9 | 51.6 | 59.7 | 49.5 | 53.9 | 40.8
    | 51.9 | 53.2 | 49.7 | 54.4 | 49.8 | 45.3 | 53.3 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 77.4 | 67.7 | 64.2 | 68.3 | 64.8 | 69.4 | 62.0 | 61.5 | 54.3 | 58.7
    | 61.1 | 56.3 | 62.6 | 55.1 | 53.0 | 62.4 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 65.4 | 55.5 | 50.6 | 53.2 | 48.8 | 59.8 | 52.1 | 54.4 | 49.6
    | 50.9 | 54.9 | 44.8 | 55.7 | 49.2 | 44.8 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 74.4 | 68.5 | 66.0 | 69.8 | 64.9 | 69.4 | 64.8 | 65.0 | 60.1 | 62.8
    | 68.3 | 62.1 | 67.7 | 61.3 | 57.3 | 65.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| (Ahuja et al., [2023](#bib.bib1))^† | 79.5 | 71.7 | 67.3 | 71.8 | 65.8 |
    72.2 | 66.9 | 67.6 | 57.3 | 65.1 | 69.3 | 62.0 | 70.8 | 63.3 | 55.1 | 67.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 71.6 | 65.8 | 62.5 | 63.4 | 56.7 | 64.6 | 59.4 | 56.9 | 48.2
    | 57.3 | 62.0 | 55.0 | 62.6 | 52.4 | 48.0 | 59.1 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 79.1 | 70.8 | 70.0 | 69.5 | 69.2 | 71.0 | 67.3 | 66.9 | 59.5 | 65.7
    | 67.8 | 63.7 | 70.4 | 63.5 | 58.1 | 67.5 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 73.4 | 66.3 | 60.9 | 67.9 | 60.2 | 68.1 | 60.2 | 62.6 | 55.7
    | 58.8 | 64.7 | 52.7 | 64.6 | 53.8 | 50.8 | 61.4 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 77.1 | 69.3 | 64.4 | 69.6 | 62.9 | 70.6 | 63.2 | 64.4 | 60.2 | 63.4
    | 66.6 | 59.8 | 66.9 | 60.0 | 56.5 | 65.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Accuracy scores on the XNLI benchmark. Ahuja et al. ([2023](#bib.bib1))^†
    utilize 8-shot learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings (high$\rightarrow$low) | en | de | fr | zh | es | ja | ko | Avg.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 53.3 | 52.9 | 50.8 | 53.6 | 53.0 | 50.3 | 50.3 | 52.0 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 64.6 | 56.0 | 55.3 | 57.4 | 56.0 | 54.6 | 55.9 | 57.1 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 73.6 | 68.3 | 68.4 | 63.4 | 69.6 | 59.7 | 55.7 | 65.5 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 65.3 | 66.1 | 64.8 | 65.5 | 63.3 | 62.4 | 57.6 | 63.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| (Ahuja et al., [2023](#bib.bib1))^† | 72.5 | 69.8 | 71.3 | 65.2 | 70.1 |
    65.4 | 65.8 | 68.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 77.8 | 70.6 | 72.5 | 65.0 | 71.7 | 62.5 | 60.5 | 68.7 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 76.5 | 78.8 | 77.4 | 61.1 | 78.0 | 65.0 | 68.7 | 72.2 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 65.9 | 70.3 | 66.6 | 64.1 | 68.2 | 65.6 | 64.0 | 66.4 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 73.4 | 69.8 | 68.5 | 70.9 | 67.8 | 66.4 | 66.7 | 69.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Accuracy scores on the PAWS-X benchmark. Ahuja et al. ([2023](#bib.bib1))^†
    utilize 8-shot learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [10](#A1.T10 "Table 10 ‣ A.2 Results on Understanding Tasks ‣ Appendix
    A Additional Experiments ‣ Not All Languages Are Created Equal in LLMs: Improving
    Multilingual Capability by Cross-Lingual-Thought Prompting") displays the comparisons
    on the PAWS-X task, where XLT outperforms basic prompt in all languages, particularly
    for low-resource languages under the few-shot setting. We observe a slight performance
    drop on average in zero-shot learning compared to gpt-3.5-turbo for some high-resource
    languages (e.g., en, de, and fr). Based on our analysis of intermediate outputs,
    we infer that the drop in performance may be due to cross-lingual thinking that
    alters the original meaning of the two sentences, leading to difficulties in judgment.
    Additionally, a comparable pattern is evident in a previous study (Ahuja et al.,
    [2023](#bib.bib1)), where non-Latin script languages (ja, zh, and ko) exhibit
    significantly poorer performance than English or German in the few-shot setting.
    Nevertheless, by demonstrating the construction of XLT, we can guide the model
    on how to think across different languages and effectively address the aforementioned
    issues.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Results on Generation Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The MKQA benchmark outcomes are listed in Table [11](#A1.T11 "Table 11 ‣ A.3
    Results on Generation Tasks ‣ Appendix A Additional Experiments ‣ Not All Languages
    Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought
    Prompting"). Across all languages in the zero-shot and few-shot settings, the
    XLT template shows a significant improvement over the basic prompt. It is worth
    noting that text-davinci-003 performs worse than gpt-3.5-turbo in this task, and
    we speculate that the latter is optimized for open question answering, which is
    common in daily chat. Additionally, our findings indicate that XLT can notably
    enhance the performance of under-resourced languages. XLT brings over 10 points
    of improvement for these languages. (e.g., zh, ja, vi, and tr) This aligns with
    previous benchmarking studies and is particularly noteworthy in this evaluation.
    We suspect that high-resource and low-resource languages share the same cross-lingual
    thinking as English to greatly leverage the LLM’s ability to solve English open-domain
    QA.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings (high$\rightarrow$low) | en | de | ru | fr | zh | es | ja | vi |
    tr | th | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 48.1 | 33.8 | 15.9 | 34.8 | 18.2 | 34.1 | 27.7 | 23.6 | 24.0
    | 29.6 | 29.0 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 51.1 | 42.3 | 27.3 | 43.0 | 36.7 | 43.3 | 46.8 | 35.8 | 37.8 | 38.1
    | 40.2 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 51.1 | 40.6 | 28.4 | 40.1 | 16.5 | 39.3 | 25.9 | 23.3 | 26.9
    | 23.7 | 31.6 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 56.7 | 46.0 | 33.9 | 47.6 | 33.0 | 47.9 | 47.5 | 36.5 | 39.1 | 38.6
    | 42.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 52.6 | 42.3 | 21.8 | 42.9 | 33.1 | 42.8 | 45.5 | 35.5 | 37.5
    | 36.6 | 39.1 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 57.6 | 49.4 | 42.7 | 50.9 | 51.0 | 50.0 | 60.0 | 46.9 | 46.9 | 40.5
    | 49.6 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 53.2 | 48.6 | 31.0 | 46.1 | 40.9 | 47.9 | 51.4 | 38.5 | 40.0
    | 39.3 | 43.7 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 59.6 | 52.5 | 43.8 | 53.9 | 51.9 | 54.0 | 63.2 | 49.4 | 52.1 | 44.7
    | 52.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: F1 scores on the MKQA benchmark. The average score is the macro average
    F1 score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the XL-Sum* benchmark are presented in Table [12](#A1.T12 "Table
    12 ‣ A.3 Results on Generation Tasks ‣ Appendix A Additional Experiments ‣ Not
    All Languages Are Created Equal in LLMs: Improving Multilingual Capability by
    Cross-Lingual-Thought Prompting"). It can be observed that XLT outperforms the
    basic prompt in both zero- and few-shot settings across all languages. Additionally,
    the LLM model exhibits a significant improvement in generating summaries under
    the few-shot setting compared to the zero-shot setting. This suggests that providing
    fewer examples can effectively guide the model in summarizing multilingual texts.
    Furthermore, the few-shot results revealed an interesting finding that text-davinci-003
    performed better when gpt-3.5-turbo and text-davinci-003 use basic prompt. However,
    once XLT is enabled, gpt-3.5-turbo outperforms text-davinci-003, highlighting
    the effectiveness of our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings (high$\rightarrow$low) | en | fr | zh | es | vi | tr | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 22.2 | 26.2 | 30.8 | 25.1 | 22.0 | 15.9 | 23.7 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 24.4 | 28.2 | 32.2 | 26.0 | 22.3 | 17.9 | 25.2 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| (Lai et al., [2023](#bib.bib18)) | 19.7 | 20.8 | 21.1 | 17.8 | – | 14.5 |
    – |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 25.3 | 26.2 | 30.2 | 26.3 | 21.1 | 19.2 | 24.7 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 26.8 | 28.1 | 33.3 | 26.4 | 21.3 | 20.5 | 26.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | text-davinci-003 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 29.2 | 29.6 | 33.2 | 28.3 | 22.5 | 18.1 | 26.8 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 28.2 | 30.3 | 34.4 | 29.4 | 22.7 | 18.6 | 27.3 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 25.7 | 27.2 | 30.8 | 27.8 | 21.5 | 19.7 | 25.5 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 28.5 | 29.2 | 35.0 | 28.6 | 23.7 | 22.3 | 27.9 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: ROUGE-1 scores on the XL-Sum* benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine translation is a special generation task where the source and target
    are two different languages. The experiment in this part is to verify how XLT
    boosts machine translation tasks. Since English has been specified as the pivot
    language in the cross-lingual thinking in XLT, we exclude English-centric tasks
    to avoid language redundancy and focus on 12 non-English translation directions
    in the FLORES* benchmark, which includes both high-resource and low-resource languages.
    As shown in Table [13](#A1.T13 "Table 13 ‣ A.3 Results on Generation Tasks ‣ Appendix
    A Additional Experiments ‣ Not All Languages Are Created Equal in LLMs: Improving
    Multilingual Capability by Cross-Lingual-Thought Prompting"), XLT achieves impressive
    zero-shot results for all languages compared with basic prompt. For example, it
    significantly improves translation quality in Chinese-to-X or X-to-Chinese. The
    result emphasizes that XLT will potentially transfer the knowledge of a high-resource
    pivot language like English to the target language. While the benefit of XLT may
    not be as obvious for high-to-high translations, it becomes more significant for
    high-to-low, low-to-high, and low-to-low translations. For instance, XLT improves
    the translation performance of gpt-3.5-turbo by nearly 4.0, 2.8, and 3.3 BLEU
    points for th$\rightarrow$th translations, respectively, demonstrating its effectiveness
    regardless of whether the source language is high-resource or low-resource. Noticing
    that Hendy et al. ([2023](#bib.bib15)) have shown that few-shot configurations
    do not yield significant improvements over the zero-shot setup for translation
    tasks, we do not evaluate the few-shot paradigm on FLORES* in this work and leave
    it for future exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Settings | High-High | High-Low | Low-Low |'
  prefs: []
  type: TYPE_TB
- en: '| zh-ru | de-vi | zh-th | zh-jv | th-gl | jv-th |'
  prefs: []
  type: TYPE_TB
- en: '| $\rightarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | text-davinci-003 |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 19.8 | 24.2 | 26.5 | 24.5 | 10.2 | 11.8 | 8.1 | 14.0 | 17.9
    | 12.0 | 10.0 | 6.2 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 21.6 | 24.8 | 27.4 | 24.8 | 12.6 | 16.4 | 11.1 | 18.2 | 20.7 | 14.2
    | 11.7 | 9.0 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Basic Prompt | 23.3 | 25.4 | 34.1 | 29.6 | 16.6 | 18.6 | 9.1 | 16.2 | 18.1
    | 18.5 | 13.0 | 7.2 |'
  prefs: []
  type: TYPE_TB
- en: '| XLT | 25.3 | 25.6 | 33.3 | 31.3 | 19.9 | 19.3 | 10.5 | 19.0 | 22.1 | 21.9
    | 15.9 | 10.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: BLEU scores on the FLORES* benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e7300067f08b34f67221c67deaec64d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustrations of different demonstration input-output pairs in the
    few-shot learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|\TaskInput{Request:  制作一件袍子需要  2  匹蓝色纤维布料和这个数量一半的白色纤维布料。它一共}||\TaskInput{需要用掉多少匹布料？}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:  |\Answer{3}|匹布料'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|\Answer{3}|匹布料。'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: A Chinese example of the MGSM benchmark using basic prompt and the
    corresponding outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  an  |\TaskName{arithmetic  reasoning}|  expert  for  |\TaskLanguage{Chinese}|.|\TaskInput{Request:  制作一件袍子需要  2  匹蓝色纤维布料和这个数量一半的白色纤维布料。它一共}||\TaskInput{需要用掉多少匹布料？}|You  should  retell  the  |\TaskElement{request}|  in  English.You  should  |\TaskGoal{do  step-by-step  answer  to  obtain  a  number  answer}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  in  this  format  ’|\OutputFormat{Answer}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The  request  is:  To  make  a  robe,  you  need  2  pieces  of  blue  fiber  fabric  and  half  of  that  amount  of  white  fiber  fabric.  How  many  pieces  of  fabric  are  needed  intotal?Step-by-step  answer:1.  Start  with  the  amount  of  blue  fiber  fabric:  2  pieces2.  Half  of  that  amount  is  1  piece3.  Add  the  1  piece  of  white  fiber  fabric  to  the  2  pieces  of  blue  fiber  fabric4.  Answer:  |\Answer{3}|  pieces  of  fabric  are  needed  in  total.'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The  request  is  asking  how  many  pieces  of  fabric  are  needed  to  make  a  robe,  given  that  it  requires  2  pieces  of  blue  fiber  fabric  and  half  that  amount  of  whitefiber  fabric.To  solve  this  problem,  we  need  to  first  find  out  how  many  pieces  of  white  fiber  fabric  are  needed.  We  can  do  this  by  dividing  the  number  of  blue  fiber  fabricpieces  by  2:2  ÷  2  =  1So  we  need  1  piece  of  white  fiber  fabric.To  find  the  total  number  of  fabric  pieces  needed,  we  can  add  the  number  of  blue  and  white  pieces  together:2  +  1  =  3Therefore,  Answer:  |\Answer{3}|  pieces  of  fabric  are  needed  to  make  the  robe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: A Chinese example of the MGSM benchmark using XLT and the corresponding
    outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  an  |\TaskName{arithmetic  reasoning}|  expert  for  |\TaskLanguage{Chinese}|.|\TaskInput{Request:  罗杰有5个网球。他又买了2罐网球。每罐有3个网球。他现在有多少个网球？}|You  should  retell  the  |\TaskElement{request}|  in  English.You  should  |\TaskGoal{do  step-by-step  answer  to  obtain  a  number  answer}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  in  this  format  ’|\OutputFormat{Answer}|:’.|\Newline||\Newline||\TaskExample{Retelling  the  request  in  English:  Roger  currently  has  5  tennis  balls…  }||\TaskExample{Step-by-step  answer:}||\TaskExample{1.  Roger  has  5  tennis  balls.}||\TaskExample{2.  He  buys  2  cans  of  tennis  balls.}||\TaskExample{3.  Each  can  has  3  tennis  balls,  so  2  cans  will  have  a  total  of  6  tennis  …}||\TaskExample{4.To  find  out  how  many  tennis  balls  Roger  has  in  total,  we  need  to  add  the  …}||\TaskExample{5.  Adding  5  (initial  tennis  balls)  and  6  (purchased  tennis  balls)  gives  …}||\TaskExample{Answer:  11.}||\Newline||\Newline|…  (the  remaining  4  examples)|\Newline||\Newline|I  want  you  to  act  as  an  |\TaskName{arithmetic  reasoning}|  expert  for  |\TaskLanguage{Chinese}|.|\TaskInput{Request:  制作一件袍子需要  2  匹蓝色纤维布料和这个数量一半的白色纤维布料。它一共}||\TaskInput{需要用掉多少匹布料？}|You  should  retell  the  |\TaskElement{request}|  in  English.You  should  |\TaskGoal{do  step-by-step  answer  to  obtain  a  number  answer}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  in  this  format  ’|\OutputFormat{Answer}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Request:  To  make  a  robe,  two  pieces  of  blue  fabric  and  half  the  amount  of  white  fabric  are  needed.  How  many  pieces  of  fabric  are  needed  in  total?Step-by-step  answer:1.  Two  pieces  of  blue  fabric  are  needed.2.  Half  the  amount  of  blue  fabric  is  needed  for  white  fabric,  so  that  is  1  piece  of  white  fabric  (2  x  0.5  =  1).3.  To  find  out  how  many  pieces  of  fabric  are  needed  in  total,  we  need  to  add  the  number  of  blue  fabric  and  white  fabric.4.  Adding  2  (blue  fabric)  and  1  (white  fabric)  gives  us  a  total  of  3  pieces  of  fabric.Answer:  |\Answer{3}|  pieces  of  fabric.'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Request:  Making  one  robe  requires  2  pieces  of  blue  fiber  fabric  and  half  that  amount  of  white  fiber  fabric.  How  many  pieces  of  fabric  are  needed  in  total?Step-by-step  answer:1.  To  make  one  robe,  we  need  2  pieces  of  blue  fiber  fabric.2.  We  also  need  half  that  amount  of  white  fiber  fabric,  which  is  1  piece  (2/2  =  1).3.  Therefore,  to  make  one  robe,  we  need  a  total  of  2  +  1  =  3  pieces  of  fiber  fabric.Answer:  We  need  |\Answer{3}|  pieces  of  fiber  fabric  in  total  to  make  one  robe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: A Chinese example of the MGSM benchmark using XLT and the corresponding
    outputs under the few-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|\TaskInput{Here  is  a  premise:  Ese  oli  mullikilesse  mässitud.  What  is  the  cause?  Help  me}||\TaskInput{pick  the  more  plausible  option:  -choice1:  See  oli  õrn,  -choice2:  See  oli  väike.}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choice|\Answer{2}|:  See  oli  väike.'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As  an  AI  language  model,  I  cannot  determine  the  cause  of  the  premise  withoutadditional  context.  The  options  provided  do  not  seem  to  be  directly  relatedto  the  premise.  Can  you  please  provide  more  information  or  context?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: An Estonian example of the XCOPA benchmark using basic prompt and
    the corresponding outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{commonsense  reasoning}|  expert  for  |\TaskLanguage{Estonian}|.|\TaskInput{Here  is  a  premise:  Ese  oli  mullikilesse  mässitud.  What  is  the  cause?  Help  me}||\TaskInput{pick  the  more  plausible  option:  -choice1:  See  oli  õrn,  -choice2:  See  oli  väike.}|You  should  retell  the  |\TaskElement{premise  and  the  options}|  in  English.You  should  |\TaskGoal{do  step-by-step  answer  to  pick  a  choice}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{choice  number}|  in  this  format  ’|\OutputFormat{Choice  number}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise:  Ese  oli  mullikilesse  mässitud.Options:  Choice  1:  See  oli  õrn,  Choice  2:  See  oli  väike.Step  1:  Consider  the  context  of  the  premise.  What  kind  of  object  was  mässitud  (inserted)  into  the  pocket?Step  2:  Consider  the  two  options.  Choice  1:  See  oli  õrn  (It  was  soft)  and  Choice  2:  See  oli  väike  (It  was  small).Step  3:  Based  on  the  context  and  the  two  options,  the  more  plausible  choice  is  Choice  2:  See  oli  väike  (It  was  small).Choice  number:  |\Answer{2}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise:  "Ese  oli  mullikilesse  mässitud"  (The  object  was  wrapped  in  bubble  wrap).Options:1.  See  oli  õrn  (It  was  fragile).2.  See  oli  väike  (It  was  small).Step  1:  Based  on  the  premise,  we  know  that  the  object  was  wrapped  in  bubble  wrap.  It  does  not  provide  any  information  about  the  size  or  fragility  of  the  item.Step  2:  However,  common  sense  would  suggest  that  fragile  items  are  more  likely  to  be  wrapped  in  bubble  wrap  for  protection  during  transportation  or  storage.Therefore,  the  more  plausible  option  is  choice  1:  "See  oli  õrn"  (It  was  fragile).Choice  number:  |\Answer{1}|.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: An Estonian example of the XCOPA benchmark using XLT and the corresponding
    outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{commonsense  reasoning}|  expert  for  |\TaskLanguage{Estonian}|.|\TaskInput{Here  is  a  premise:  Mees  jäi  vanaks.  What  is  the  effect?  Help  me  pick  the  more  plausible  option:}||\TaskInput{-choice1:  Ta  juuksed  muutusid  halliks,  -choice2:  Ta  müüs  oma  asjad  maha.}|You  should  retell  the  |\TaskElement{premise  and  the  options}|  in  English.You  should  |\TaskGoal{do  step-by-step  answer  to  pick  a  choice}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{choice  number}|  in  this  format  ’|\OutputFormat{Choice  number}|:’.|\Newline||\Newline||\TaskExample{Premise:  A  man  turned  on  the  tap.}||\TaskExample{Options:  1.  The  toilet  filled  with  water.  2.  Water  flowed  from  the  tap.}||\TaskExample{Step-by-step  answer:}||\TaskExample{1.  The  premise  states  that  a  man  turned  on  the  tap,  which  implies  that  water  started  flowing.}||\TaskExample{2.  The  first  option  suggests  that  the  toilet  filled  with  water,  which  is  possible  if  the  tap  …}||\TaskExample{3.  The  second  option  suggests  that  water  flowed  from  the  tap,  which  is  the  more  plausible  …}||\TaskExample{Choice  Number:  2.  Water  flowed  from  the  tap.  The  answer  is  choice2."}||\Newline||\Newline|…  (the  remaining  4  examples)|\Newline||\Newline|I  want  you  to  act  as  a  |\TaskName{commonsense  reasoning}|  expert  for  |\TaskLanguage{Estonian}|.|\TaskInput{Here  is  a  premise:  Ese  oli  mullikilesse  mässitud.  What  is  the  cause?  Help  me  pick  the  more}||\TaskInput{plausible  option:  -choice1:  See  oli  õrn,  -choice2:  See  oli  väike.}|You  should  retell  the  |\TaskElement{premise  and  the  options}|  in  English.You  should  |\TaskGoal{do  step-by-step  answer  to  pick  a  choice}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{choice  number}|  in  this  format  ’|\OutputFormat{Choice  Number}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise:  The  object  was  wedged  into  a  crack.Options:1.  It  was  delicate.2.  It  was  small.Step-by-step  answer:1.  The  premise  states  that  the  object  was  wedged  into  a  crack.2.  Option  1  suggests  that  the  object  was  delicate,  which  is  not  necessarily  related  to  the  object  being  wedged  into  a  crack.3.  Option  2  suggests  that  the  object  was  small,  which  is  more  plausible  as  a  small  object  is  more  likely  to  fit  into  a  crack.Choice  Number:  |\Answer{2}|.  It  was  small.The  answer  is  choice2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise:  "The  object  was  wrapped  in  bubble  wrap."Options:1.  It  was  fragile.2.  It  was  small.Step-by-step  answer:1.  The  premise  states  that  the  object  was  wrapped  in  bubble  wrap,  which  is  often  used  to  protect  fragile  items  during  transport.2.  Bubble  wrap  is  not  typically  used  to  protect  small  items,  so  option  1  is  more  plausible.3.  Therefore,  the  more  plausible  option  is  Choice  1:  It  was  fragile.Choice  Number:  |\Answer{1}|.  It  was  fragile.  The  answer  is  choice1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: An Estonian example of the XCOPA benchmark using XLT and the corresponding
    outputs under the few-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|\TaskInput{Les  options  ne  sont  pas  si  attrayantes.  Based  on  previous  passage,  is  it  true}||\TaskInput{that  Les  choix  ne  sont  pas  trop  attrayants?  Yes,  No,  or  Maybe?}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'maybe'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'entailment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: A French example of the XNLI benchmark using basic prompt and the
    corresponding outputs under the zero-shot setting. We need a verbalizer to map
    entailment to yes, contradiction to no, and neutral to maybe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{natural  language  inference}|  expert  for  |\TaskLanguage{French}|.|\TaskInput{Premise:  Les  options  ne  sont  pas  si  attrayantes.}||\TaskInput{Hypothesis:  Les  choix  ne  sont  pas  trop  attrayants.}|You  should  retell  the  |\TaskElement{premise  and  hypothesis}|  in  English.You  should  |\TaskGoal{judge  whether  the  hypothesis  is  true  (entailment),  false}||\TaskGoal{(contradiction),  or  undetermined  (neutral)  given  the  premise.  The  relationship}||\TaskGoal{can  be  chosen  from  entailment,  contradiction,  and  neutral}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{relationship}|  in  this  format  ’|\OutputFormat{Relationship}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise  (in  English):  The  options  are  not  very  attractive.Hypothesis  (in  English):  The  choices  are  not  too  attractive.Relationship:  |\Answer{Contradiction}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Premise:  The  options  are  not  so  attractive.Hypothesis:  The  choices  are  not  too  attractive.Relationship:  |\Answer{Entailment}|.  The  hypothesis  is  true  given  the  premise  as  the  concepts  of  options  andchoices  are  interchangeable  and  both  refer  to  the  same  thing.  The  degree  of  attractiveness  is  alsoconveyed  in  a  similar  manner  in  both  statements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'entailment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: A French example of the XNLI benchmark using XLT and the corresponding
    outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{natural  language  inference}|  expert  for  |\TaskLanguage{French}|.|\TaskInput{Premise:  savons-nous  ce  que  nous  allons  dire  ?}||\TaskInput{Hypothesis:  Je  sais  que  nous  n’avons  aucune  idée  de  ce  que  nous  allons  dire.}|You  should  retell  the  |\TaskElement{premise  and  hypothesis}|  in  English.You  should  |\TaskGoal{judge  whether  the  hypothesis  is  true  (entailment),  false}||\TaskGoal{(contradiction),  or  undetermined  (neutral)  given  the  premise.  The  relationship}||\TaskGoal{can  be  chosen  from  entailment,  contradiction,  and  neutral}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{relationship}|  in  this  format  ’|\OutputFormat{Relationship}|:’.|\Newline||\Newline||\TaskExample{Retelled  premise:  do  we  know  what  we’re  going  to  say.}||\TaskExample{Retelled  hypothesis:  I  know  we  don’t  have  a  clue  what  we’ll  say.}||\TaskExample{Relationship:  contradiction}||\Newline||\Newline|…  (the  remaining  4  examples)|\Newline||\Newline|I  want  you  to  act  as  a  |\TaskName{natural  language  inference}|  expert  for  |\TaskLanguage{French}|.|\TaskInput{Premise:  Les  options  ne  sont  pas  si  attrayantes.}||\TaskInput{Hypothesis:  Les  choix  ne  sont  pas  trop  attrayants.}|You  should  retell  the  |\TaskElement{premise  and  hypothesis}|  in  English.You  should  |\TaskGoal{judge  whether  the  hypothesis  is  true  (entailment),  false}||\TaskGoal{(contradiction),  or  undetermined  (neutral)  given  the  premise.  The  relationship}||\TaskGoal{can  be  chosen  from  entailment,  contradiction,  and  neutral}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{relationship}|  in  this  format  ’|\OutputFormat{Relationship}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'retelled  premise:  the  options  are  not  so  attractive.retelled  hypothesis:  the  choices  are  not  too  attractive.relationship:  |\Answer{entailment}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'retelled  premise:  the  options  are  not  very  attractive.retelled  hypothesis:  the  choices  are  not  too  attractive.relationship:  |\Answer{entailment}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'entailment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13: A French example of the XNLI benchmark using XLT and the corresponding
    outputs under the few-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic prompt input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|\TaskInput{Sentence  1:  Lito  spielte  für  den  Zingone  Club  Football.  Sentence  2:  Zingone  spielte  für  den}||\TaskInput{Club  Lito  Fußball.  Does  Sentence  1  paraphrase  Sentence  2?  Yes  or  No?}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'No'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'No'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14: A Germany example of the PAWS-X benchmark using basic prompt and
    the corresponding outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{paraphrase  identification}|  expert  for  |\TaskLanguage{Germany}|.|\TaskInput{Sentence  1:  Lito  spielte  für  den  Zingone  Club  Football}||\TaskInput{Sentence  2:  Zingone  spielte  für  den  Club}||\TaskInput{Question:  Does  Sentence  1  paraphrase  Sentence  2?  Yes  or  No?}|You  should  retell  the  |\TaskElement{sentence  1  and  sentence  2}|  in  English.You  should  |\TaskGoal{provide  a  yes  or  no  answer  to  the  question:  Does  Sentence  1  paraphrase  Sentence  2?}|You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  |\OutputConstraint{choosing  either  yes  or  no}|  in  this  format  ’|\OutputFormat{Answer}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence  1:  Lito  played  football  for  the  Zingone  Club.Sentence  2:  Zingone  played  football  for  the  Lito  Club.Answer:  |\Answer{Yes}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence  1:  Lito  played  football  for  the  Zingone  Club.Sentence  2:  Zingone  played  football  for  the  Club  Lito.Answer:  |\Answer{No}|,  Sentence  1  does  not  paraphrase  Sentence  2.Step-by-step  answer:1.  Retold  Sentence  1  and  Sentence  2  in  English.2.  Provided  a  yes  or  no  answer  to  the  question:  Does  Sentence  1  paraphrase  Sentence  2?3.  Answered  the  request  in  the  required  format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'No'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15: A Germany example of the PAWS-X benchmark using XLT and the corresponding
    outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{paraphrase  identification}|  expert  for  |\TaskLanguage{Germany}|.|\TaskInput{Sentence  1:  Der  Ponoru  ist  eine  Nebenfluss  des  Horezu  in  Rumänien}|.|\TaskInput{Sentence  2:  Der  Ponoru  ist  ein  Nebenfluss  des  Horezu  in  Rumänien}|.|\TaskInput{Question:  Does  Sentence  1  paraphrase  Sentence  2?  Yes  or  No?}|You  should  retell  the  |\TaskElement{sentence  1  and  sentence  2}|  in  English.You  should  |\TaskGoal{provide  a  yes  or  no  answer  to  the  question:  Does  Sentence  1  paraphrase  Sentence  2?}|You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  |\OutputConstraint{choosing  either  yes  or  no}|  in  this  format  ’|\OutputFormat{Answer}|:’.|\Newline||\Newline||\TaskExample{Retelled  Sentence  1:  The  Ponoru  River  is  a  tributary  of  the  Horezu  River  in  Romania  .}||\TaskExample{Retelled  Sentence  2:  The  Ponoru  River  is  a  tributary  of  the  Horezu  in  Romania  .}||\TaskExample{Answer:  Yes}||\Newline||\Newline|…  (the  remaining  4  examples)|\Newline||\Newline|I  want  you  to  act  as  a  |\TaskName{paraphrase  identification}|  expert  for  |\TaskLanguage{Germany}|.|\TaskInput{Sentence  1:  Lito  spielte  für  den  Zingone  Club  Football}||\TaskInput{Sentence  2:  Zingone  spielte  für  den  Club}||\TaskInput{Question:  Does  Sentence  1  paraphrase  Sentence  2?  Yes  or  No?}|You  should  retell  the  |\TaskElement{sentence  1  and  sentence  2}|  in  English.You  should  |\TaskGoal{provide  a  yes  or  no  answer  to  the  question:  Does  Sentence  1  paraphrase  Sentence  2?}|You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  |\OutputConstraint{choosing  either  yes  or  no}|  in  this  format  ’|\OutputFormat{Answer}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retelled  Sentence  1:  Lito  played  football  for  the  Zingone  Club.Retelled  Sentence  2:  Zingone  played  football  for  the  Lito  Club.Answer:  |\Answer{Yes}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retelled  Sentence  1:  Lito  played  football  for  the  Zingone  Club.Retelled  Sentence  2:  Zingone  played  football  for  the  Lito  Club.Answer:  |\Answer{No}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'No'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 16: A Germany example of the PAWS-X benchmark using XLT and the corresponding
    outputs under the few-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|\TaskInput{Answer  the  question  in  one  or  a  few  words  in}|  |\TaskLanguage{Japanese}|  |\TaskInput{:  世界で一番背の高い人類は誰で}||\TaskInput{すか？}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: '最高身長者。'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: '誰？'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ロバート・ワドロー'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 17: A Japanese example of the MKQA benchmark using basic prompt and
    the corresponding outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{question  answering}|  expert  for  |\TaskLanguage{Japanese}|.|\TaskInput{Question:  世界で一番背の高い人類は誰ですか？}|You  should  retell  the  |\TaskElement{question}|  in  English.You  should  |\TaskGoal{answer  the  question  in  English  in  one  or  a  few  words.}|You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  |\OutputConstraint{in  one  or  a  few  words  in  Japanese}|  in  this  format  ’|\OutputFormat{Answer}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question  in  English:  Who  is  the  tallest  human  being  in  the  world?Answer:  Robert  Wadlow  (2.72  m).Answer  in  Japanese:  答え：|\Answer{ロバート・ワドロウ（2.72  m）。}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retelling:  Who  is  the  tallest  person  in  the  world?Answer:  Robert  WadlowStep-by-step  response:1.  The  question  is  asking  for  the  tallest  person  in  the  world.2.  The  answer  is  Robert  Wadlow,  who  was  8  feet  11  inches  (2.72  meters)  tall.Answer  in  Japanese:  答え：|\Answer{ロバート・ワドロー}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ロバート・ワドロー'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18: A Japanese example of the MKQA benchmark using XLT and the corresponding
    outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{question  answering}|  expert  for  |\TaskLanguage{Japanese}|.|\TaskInput{Question:  フルハウスでスティーブが出てきたのはどのシーズンですか？}|You  should  retell  the  |\TaskElement{question}|  in  English.You  should  |\TaskGoal{answer  the  question  in  English  in  one  or  a  few  words.}|You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  |\OutputConstraint{in  one  or  a  few  words  in  Japanese}|  in  thisformat  ’|\OutputFormat{Answer}|:’.|\Newline||\Newline||\TaskExample{Question:  what  season  does  steve  come  in  full  house?}||\TaskExample{English  Answer:  5.0  seasons.}||\TaskExample{Japanese  Answer:  5.0  時節}||\Newline||\Newline|…  (the  remaining  4  examples)|\Newline||\Newline|I  want  you  to  act  as  a  |\TaskName{question  answering}|  expert  for  |\TaskLanguage{Japanese}|.|\TaskInput{Question:  世界で一番背の高い人類は誰ですか？}|You  should  retell  the  |\TaskElement{question}|  in  English.You  should  |\TaskGoal{answer  the  question  in  English  in  one  or  a  few  words.}|You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{answer}|  |\OutputConstraint{in  one  or  a  few  words  in  Japanese}|  in  thisformat  ’|\OutputFormat{Answer}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'question:  who  is  the  tallest  human  being  in  the  worldenglish  answer:  sultan  kösenjapanese  answer:  |\Answer{スルタン・ケーセン}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'question:  who  is  the  tallest  person  in  the  worldnenglish  answer:  robert  wadlowjapanese  answer:  |\Answer{ロバート・ワドロー}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ロバート・ワドロー'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 19: A Japanese example of the MKQA benchmark using XLT and the corresponding
    outputs under the few-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|\TaskInput{Summarize  this  article:  Huang  Ming  es  tan  apasionado  por  la  energía  solar  que  construyó  el  llamado  "Solar  Valley"}||\TaskInput{(o  Ciudad  Solar):  un  conjunto  de  edificios  que  funcionan  íntegramente  a  partir  del  sol.  En  su  diseño  también  se}||\TaskInput{integran  los  paneles  solares  que  los  alimentan.  Pero  aunque  Huang  soñaba  con  que  todo  el  mundo  imitaría  sus}||\TaskInput{esfuerzos,  todavía  sigue  siendo  la  única  Ciudad  Solar  en  China  (y  del  mundo).  Video  producido  por  Álvaro  Ricciardelli.}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Huang  Ming  es  un  innovador  apasionado  por  la  energía  solar  que  construyó  el  primer  conjunto  de  edificiosalimentados  íntegramente  por  el  sol,  conocido  como  "Solar  Valley"  o  Ciudad  Solar.  A  pesar  de  que  Huang  esperabaque  su  proyecto  inspirara  a  otros,  todavía  es  el  único  de  su  tipo  en  China  y  el  mundo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Huang  Ming  built  the  "Solar  Valley"  in  China,  a  collection  of  buildings  that  run  entirely  on  solar  power.  Despitehis  hopes  that  others  would  follow  his  lead,  it  remains  the  only  solar  city  in  China  and  the  world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lo  llaman  "El  rey  sol",  aunque  a  él  no  le  gusta.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 20: A Spanish example of the XL-Sum* benchmark using basic prompt and
    the corresponding outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{multilingual  summarization}|  expert  for  |\TaskLanguage{Spanish}|.|\TaskInput{Text:  Huang  Ming  es  tan  apasionado  por  la  energía  solar  que  construyó  el  llamado  "Solar  Valley"  (o  Ciudad  Solar):}||\TaskInput{un  conjunto  de  edificios  que  funcionan  íntegramente  a  partir  del  sol.  En  su  diseño  también  se  integran  los}||\TaskInput{paneles  solares  que  los  alimentan.  Pero  aunque  Huang  soñaba  con  que  todo  el  mundo  imitaría  sus  esfuerzos,}||\TaskInput{todavía  sigue  siendo  la  única  Ciudad  Solar  en  China  (ydel  mundo).  Video  producido  por  Álvaro  Ricciardelli.}|You  should  repeat  the  |\TaskElement{entire  text}|  in  English.You  should  |\TaskGoal{think  step-by-step  to  summarize  the  entire  text  in  a  maximum  of  two  sentences}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{summary}|  |\OutputConstraint{into  one  sentence  in  Spanish}|  in  this  format  ’|\OutputFormat{Summary}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary:  |\Answer{Huang  Ming  es  un  apasionado  de  la  energía  solar  que  construyó  la  única  Ciudad  Solar  en  China  y  del}||\Answer{mundo,  llamada  "Solar  Valley",  que  funciona  íntegramente  con  paneles  solares.}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The  text  is  in  Spanish  and  talks  about  Huang  Ming,  who  built  the  "Solar  Valley"  in  China,  a  group  ofbuildings  that  run  entirely  on  solar  energy.  Despite  his  hopes  that  others  would  follow  his  lead,  it  remains  theonly  solar  city  in  China  and  the  world.To  summarize  in  two  sentences:  Huang  Ming  built  the  "Solar  Valley"  in  China,  a  group  of  buildings  that  runentirely  on  solar  energy,  but  it  remains  the  only  solar  city  in  China  and  the  world.Summary:  |\Answer{Huang  Ming  construyó  la  "Ciudad  Solar"  en  China,  un  conjunto  de  edificios  que  funcionan  íntegramente}||\Answer{a  partir  del  sol,  pero  sigue  siendo  la  única  ciudad  solar  en  China  y  en  el  mundo.}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lo  llaman  "El  rey  sol",  aunque  a  él  no  le  gusta.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 21: A Spanish example of the XL-Sum* benchmark using XLT and the corresponding
    outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{multilingual  summarization}|  expert  for  |\TaskLanguage{Spanish}|.|\TaskInput{Text:  Testigos  del  tiroteo  explicaron  que  el  atacante  asaltó  la  mezquita  a  los  pocos  minutos  de  haber  empezado}||\TaskInput{la  oración.  La  policía  de  Nueza  Zelanda  declaró  que  hay  cuatro  personas  detenidas  y  que  no  puede  descartarse}||\TaskInput{que  haya  más  implicados  en  los  ataque.  Ahora  puedes  recibir  notificaciones  de  BBC  Mundo.  Descarga  la  nueva}||\TaskInput{versión  de  nuestra  app  y  actívalas  para  no  perderte  nuestro  mejor  contenido.}|You  should  repeat  the  |\TaskElement{entire  text}|  in  English.You  should  |\TaskGoal{think  step-by-step  to  summarize  the  entire  text  in  a  maximum  of  two  sentences}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{summary}|  |\OutputConstraint{into  one  sentence  in  Spanish}|  in  this  format  ’|\OutputFormat{Summary}|:’.|\Newline||\Newline||\TaskExample{The  text  states  that  a  shooter  attacked  a  mosque  during  prayer  time  in  New  Zealand,  with  four  people  currently}||\TaskExample{in  custody  and  the  possibility  of  more  involved.  The  article  also  promotes  the  BBC  Mundo  app  for  notifications.}||\TaskExample{To  summarize,  a  mosque  in  New  Zealand  was  attacked  during  prayer  time  with  four  people  currently  detained  and}||\TaskExample{the  potential  for  more  involvement,  while  also  promoting  the  BBC  Mundo  app  for  notifications.}||\TaskExample{Summary:  Al  menos  49  personas  murieron  en  dos  tiroteos  en  mezquitas  en  Nueva  Zelanda.}||\Newline||\Newline|…  (the  remaining  2  examples)|\Newline||\Newline|I  want  you  to  act  as  a  |\TaskName{multilingual  summarization}|  expert  for  |\TaskLanguage{Spanish}|.|\TaskInput{Text:  Huang  Ming  es  tan  apasionado  por  la  energía  solar  que  construyó  el  llamado  "Solar  Valley"  (o  Ciudad  Solar):}||\TaskInput{un  conjunto  de  edificios  que  funcionan  íntegramente  a  partir  del  sol.  En  su  diseño  también  se  integran  los}||\TaskInput{paneles  solares  que  los  alimentan.  Pero  aunque  Huang  soñaba  con  que  todo  el  mundo  imitaría  sus  esfuerzos,  }||\TaskInput{todavía  sigue  siendo  la  única  Ciudad  Solar  en  China  (y  del  mundo).  Video  producido  por  Álvaro  Ricciardelli.}|You  should  repeat  the  |\TaskElement{entire  text}|  in  English.You  should  |\TaskGoal{think  step-by-step  to  summarize  the  entire  text  in  a  maximum  of  two  sentences}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{summary}|  |\OutputConstraint{into  one  sentence  in  Spanish}|  in  this  format  ’|\OutputFormat{Summary}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary:  |\Answer{Huang  Ming  construyó  la  única  Ciudad  Solar  del  mundo,  que  funciona  íntegramente  con  energía  solar.}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The  text  highlights  Huang  Ming’s  passion  for  solar  energy  and  his  creation  of  the  "Solar  Valley",  a  group  ofbuildings  that  run  entirely  on  solar  power,  but  despite  his  hopes  for  others  to  follow  suit,  it  remains  theonly  solar  city  in  China  and  the  world.Summary:  |\Answer{Huang  Ming  construyó  la  única  Ciudad  Solar  en  China  y  del  mundo,  un  conjunto  de  edificios  que}||\Answer{funcionan  íntegramente  a  partir  del  sol.}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lo  llaman  "El  rey  sol",  aunque  a  él  no  le  gusta.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 22: A Spanish example of the XL-Sum* benchmark using XLT and the corresponding
    outputs under the few-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|\TaskInput{Nalika  ana  siji  vaksin  pacoban  katon  bisa  nyuda  kematian  Ebola,  nganti  saiki,  ora  ana  obat  }||\TaskInput{sing  kabukten  jelas  kanggo  ngobati  infeksi  sing  ana.  Translate  from  Javanese  to  Chinese:}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: '目前尚无明确的治疗方法可以治愈埃博拉病毒感染，但是研究人员已经开发出一种疫苗来预防埃博拉病毒引起的死亡。'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: '目前为止，尚无明确的治疗方案可用于治疗埃博拉病毒感染，但如果接种一种疫苗可能会有所帮助，直至现在为止。'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '虽然有一种实验性疫苗看似能够降低埃博拉病毒的死亡率，但迄今为止，还没明确证明任何药物适合治疗现有的感染。'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 23: A jv$\rightarrow$zh example of the FLORES* benchmark using basic
    prompt and the corresponding outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'I  want  you  to  act  as  a  |\TaskName{machine  translation}|  expert  for  |\TaskLanguage{Javanese  to  Chinese}|.|\TaskInput{Source  sentence:  Nalika  ana  siji  vaksin  pacoban  katon  bisa  nyuda  kematian  Ebola,  nganti  saiki,}||\TaskInput{ora  ana  obat  sing  kabukten  jelas  kanggo  ngobati  infeksi  sing  ana.}|You  should  repeat  the  |\TaskElement{source  sentence}|  in  English.You  should  |\TaskGoal{provide  the  Chinese  translation  for  the  English  source  sentence}|.You  should  step-by-step  answer  the  request.You  should  tell  me  the  |\OutputFormat{target  translation}|  in  this  format  ’|\OutputFormat{Target  translation}|:’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'text-davinci-003 output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source  sentence:  When  there  was  a  vaccine  available  to  prevent  death  from  Ebola,  until  now,  there  is  no  clearevidence  of  a  treatment  to  cure  the  infection.Target  translation:  |\Answer{当有一种疫苗可以预防埃博拉死亡时，直到现在，还没有清楚的证据表明有治疗感染的方法}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'gpt-3.5-turbo output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source  sentence:  Nalika  ana  siji  vaksin  pacoban  katon  bisa  nyuda  kematian  Ebola,  nganti  saiki,  ora  ana  obat  singkabukten  jelas  kanggo  ngobati  infeksi  sing  ana.English  source  sentence:  When  there  is  a  vaccine  trial  that  can  help  prevent  Ebola  deaths,  until  now,  there  isno  clear  proven  medicine  to  treat  existing  infections.Chinese  translation:  当有一种疫苗试验可以帮助预防埃博拉死亡时，到目前为止，没有明确的已证明的药物来治疗现有的感染。Target  translation:  |\Answer{当有一种可以帮助预防埃博拉死亡的疫苗试验时，到目前为止，没有明确证明可以治疗现有感染的药}||\Answer{物。}|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '虽然有一种实验性疫苗看似能够降低埃博拉病毒的死亡率，但迄今为止，还没明确证明任何药物适合治疗现有的感染。'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 24: A jv$\rightarrow$zh example of the FLORES* benchmark using XLT and
    the corresponding outputs under the zero-shot setting.'
  prefs: []
  type: TYPE_NORMAL
