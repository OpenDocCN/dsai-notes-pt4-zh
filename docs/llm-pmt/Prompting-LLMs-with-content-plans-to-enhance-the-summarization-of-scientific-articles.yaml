- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:37'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Prompting LLMs with content plans to enhance the summarization of scientific
    articles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08282](https://ar5iv.labs.arxiv.org/html/2312.08282)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![[Uncaptioned image]](img/3aa628847abe739a7af93c79bc4b612a.png) Aldan Creo](https://orcid.org/0000-0002-7401-5198)'
  prefs: []
  type: TYPE_NORMAL
- en: Singular Research Center on Intelligent Technologies (CiTIUS)
  prefs: []
  type: TYPE_NORMAL
- en: University of Santiago de Compostela
  prefs: []
  type: TYPE_NORMAL
- en: Santiago de Compostela, ES
  prefs: []
  type: TYPE_NORMAL
- en: aldan.creo@rai.usc.es
  prefs: []
  type: TYPE_NORMAL
- en: '&[![[Uncaptioned image]](img/3aa628847abe739a7af93c79bc4b612a.png) Manuel Lama](https://orcid.org/0000-0001-7195-6155)'
  prefs: []
  type: TYPE_NORMAL
- en: Singular Research Center on Intelligent Technologies (CiTIUS)
  prefs: []
  type: TYPE_NORMAL
- en: University of Santiago de Compostela
  prefs: []
  type: TYPE_NORMAL
- en: Santiago de Compostela, ES
  prefs: []
  type: TYPE_NORMAL
- en: manuel.lama@usc.es
  prefs: []
  type: TYPE_NORMAL
- en: \AND[![[Uncaptioned image]](img/3aa628847abe739a7af93c79bc4b612a.png) Juan C. Vidal](https://orcid.org/0000-0002-8682-6772)
  prefs: []
  type: TYPE_NORMAL
- en: Singular Research Center on Intelligent Technologies (CiTIUS)
  prefs: []
  type: TYPE_NORMAL
- en: University of Santiago de Compostela
  prefs: []
  type: TYPE_NORMAL
- en: Santiago de Compostela, ES
  prefs: []
  type: TYPE_NORMAL
- en: juan.vidal@usc.es
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This paper presents novel prompting techniques to improve the performance of
    automatic summarization systems for scientific articles. Scientific article summarization
    is highly challenging due to the length and complexity of these documents. We
    conceive, implement, and evaluate prompting techniques that provide additional
    contextual information to guide summarization systems. Specifically, we feed summarizers
    with lists of key terms extracted from articles, such as author keywords or automatically
    generated keywords. Our techniques are tested with various summarization models
    and input texts. Results show performance gains, especially for smaller models
    summarizing sections separately. This evidences that prompting is a promising
    approach to overcoming the limitations of less powerful systems. Our findings
    introduce a new research direction of using prompts to aid smaller models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Keywords* Natural Language Processing  $\cdot$ Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automatic text summarization is an active area of research dedicated to producing
    shortened versions of documents while retaining the most relevant information.
    Initial approaches to automatic summarization heavily leaned on extractive methods;
    however, most current state-of-the-art systems are based on abstractive summarization
    models, such as transformer architectures [[34](#bib.bib34)], which have demonstrated
    state-of-the-art results. These models are commonly implemented as encoder-decoder
    architectures, where the encoder builds representations of the input text, and
    the decoder generates the target summary. Extensive pretraining on large datasets
    equips these models with strong language modeling capabilities to generate fluent
    abstractive summaries.
  prefs: []
  type: TYPE_NORMAL
- en: While automatic summarization has applications across many domains involving
    large volumes of text, one notably challenging genre is scientific articles. Summarizing
    scientific articles poses difficulties beyond summarizing other document types
    due to these papers’ great length and linguistic complexity. Scientific articles
    exhibit high variability in length. The highly technical vocabulary and complex
    discourse structures make summarization of scientific papers challenging even
    for state-of-the-art natural language processing systems [[13](#bib.bib13), [33](#bib.bib33)].
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, scientific articles follow irregular organizational structures,
    unlike genres such as news with predictable templated content. In the biomedical
    domain focused on in this paper, a commonly followed scheme is the IMRAD structure
    (Introduction, Methods, Results, and Discussion) [[21](#bib.bib21)]. However,
    specifics may vary between subfields and journals, with sections further divided
    or additional sections present. This variability in structure poses difficulties
    for summarization systems to adapt. Consequently, abstracting scientific articles
    is acknowledged as a remarkably challenging domain within the field of automatic
    text summarization [[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: We introduce and investigate novel prompting techniques to improve the performance
    of state-of-the-art scientific article summarizers based on transformer architectures.
    In this paper, ‘prompting’ refers to feeding summarizers with lists of key terms
    extracted from the input articles. Intuitively, supplying key terms may help focus
    summarizers on salient concepts to include in outputs. Our central hypothesis
    is that decoder prompting techniques will lead to gains in standard automatic
    evaluation metrics of summarization quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct experiments applying our proposed prompting techniques to various
    state-of-the-art transformer-based summarization models. Our techniques are conceived
    to be easily obtainable for any input text, not relying on future knowledge as
    in some prior work [[22](#bib.bib22)]. Specifically, we test five models covering
    a diversity of architectures: [LongT5](#glo.main.LongT5) small and large variants
    employing different attention mechanisms, [LED](#glo.main.LED) [[1](#bib.bib1)],
    which uses Longformer attention [[9](#bib.bib9)], and [BigBirdPegasus](#glo.main.BigBirdPegasus)
    [[2](#bib.bib2)] combining BigBird [[36](#bib.bib36)] and Pegasus [[37](#bib.bib37)]
    architectures. We also experiment with different text inputs, either the concatenation
    of introduction and discussion text or sections separately, to determine whether
    prompts provide greater gains when summarizing sections in isolation.'
  prefs: []
  type: TYPE_NORMAL
- en: Our results demonstrate consistent performance improvements from prompting techniques
    on smaller models, especially when summarizing sections independently. These models
    obtain ROUGE-1 score increases around $0.1$ when summarizing sections aided by
    prompts. In confusion testing, smaller models exhibit performance degradation
    when fed unrelated prompts, indicating reliance on prompt information. Taken together,
    these findings reveal that prompting is an effective approach to overcoming the
    fundamental limitations of smaller, less capable summarization systems. Rather
    than large models, lightweight models supplemented with prompts may be preferable
    in resource-constrained contexts like mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the core contributions of this work are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose novel prompting techniques to provide key term context and enhance
    scientific literature summarizers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our techniques obtain prompts through simple extraction methods not requiring
    future knowledge, in contrast to prior work [[22](#bib.bib22)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We perform extensive experiments on combinations of various state-of-the-art
    models and input text types, analyzing factors impacting prompt effectiveness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our results show enhanced performance when employing prompts for smaller summarization
    models, particularly in the context of section-level summarization. This underscores
    the potential of prompting as a promising technique to assist smaller models,
    particularly when computational resources are constrained.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The remainder of this paper is structured as follows. Section [2](#S2 "2 Related
    Work ‣ Prompting LLMs with content plans to enhance the summarization of scientific
    articles") reviews related prior work on which we build. Section [3](#S3 "3 Methods
    ‣ Prompting LLMs with content plans to enhance the summarization of scientific
    articles") presents our proposed methods for conceiving and evaluating prompting
    techniques. Section [4](#S4 "4 Results ‣ Prompting LLMs with content plans to
    enhance the summarization of scientific articles") details our experimental setup
    and results. Section [5](#S5 "5 Discussion ‣ Prompting LLMs with content plans
    to enhance the summarization of scientific articles") provides a discussion analyzing
    our findings. Section [6](#S6 "6 Future Work ‣ Prompting LLMs with content plans
    to enhance the summarization of scientific articles") proposes opportunities for
    future research. Finally, Section [7](#S7 "7 Conclusion ‣ Prompting LLMs with
    content plans to enhance the summarization of scientific articles") provides concluding
    remarks.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automatically summarizing scientific articles is a long-studied challenge within
    automatic text summarization. Earlier conventional approaches relied heavily on
    extractive methods, where summarization involves identifying and extracting salient
    snippets, typically sentences, from the original document. For instance, one common
    family of techniques scored sentences based on statistical metrics like word frequencies
    to determine the importance of extraction [[35](#bib.bib35), [30](#bib.bib30),
    [6](#bib.bib6)]. There have also been attempts to produce scientific article summaries
    by analyzing their citation contexts [[26](#bib.bib26), [12](#bib.bib12), [28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: However, the current dominant paradigm has shifted decisively toward abstractive
    methods using neural network architectures. Our work focuses on techniques to
    enhance state-of-the-art abstractive scientific summarizers based on transformer
    models, which have become ubiquitous in natural language generation applications.
    Before discussing our proposed contributions, we first contextualize our work
    by summarizing prior studies on which we seek to build.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Planning with Learned Entity Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An especially relevant prior technique our work aims to build on is planning
    with learned entity prompts, proposed by [[22](#bib.bib22)]. Their method trains
    scientific summarizers by providing an instruction composed of the named entities
    present in the reference summary, ordered by appearance. For example, as illustrated
    in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Planning with Learned Entity Prompts ‣ 2
    Related Work ‣ Prompting LLMs with content plans to enhance the summarization
    of scientific articles"), entities like ‘Frozen’, ‘Princess Anna’, or ‘Snow Queen
    Elsa’ are fed to the decoder to control summary content.
  prefs: []
  type: TYPE_NORMAL
- en: [EntityChain] Frozen $|$ Snow Queen Elsa [Summary]
    "Frozen", the latest Disney musical, preaches the importance of embracing your
    true nature. It depicts fearless Princess Anna who joins forces with mountaineer
    Kristoff and his reindeer sidekick to find estranged sister, Snow Queen Elsa,
    and break her icy spell. Reference summary
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Prompt example from [[22](#bib.bib22)]. Prompt in magenta, summary
    in red, entities in blue.'
  prefs: []
  type: TYPE_NORMAL
- en: Their training procedure involves first generating the prompt by extracting
    entities from the reference summary. The prompt is concatenated to the article
    text as the decoder input. At inference time, the model takes only the article
    as input and must generate both prompt and summary from scratch. According to
    their experiments, this approach improves faithfulness to included entities and
    reduces hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: A clear limitation, however, is that accurately predicting key entities in advance
    to generate high-quality prompts is highly challenging, especially for complex
    scientific documents. The authors address this by further pretraining their model
    to generate entity chains from abstracts but do so only using scientific news
    summaries, which are far more straightforward than scientific articles. Directly
    producing entity prompts for long technical papers thus remains an unsolved difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: Our work explores alternative prompting techniques that do not rely on advanced
    knowledge of key entities; instead, we extract prompts directly from input texts
    using simple, unsupervised methods. Models can thereby leverage prompts without
    strictly following specified entities. Our techniques are designed to be easily
    obtainable without needing additional complex models for prompt generation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Faceted Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another relevant line of work that inspired our methods is scientific summarization
    by individual paper sections rather than full text. Two prominent papers in this
    area are [[32](#bib.bib32)] and [[25](#bib.bib25)]. The fundamental concept involves
    partitioning scientific articles into distinct sections and subsequently generating
    independent summaries for each section. These individual summaries are then amalgamated
    to form a comprehensive summary encapsulating the entire text. The key advantage
    is simplifying the summarization task by restricting focus to one section, which
    reduces complexity. Models can concentrate on concepts specific to individual
    sections rather than needing to capture the entire document.
  prefs: []
  type: TYPE_NORMAL
- en: In [[32](#bib.bib32)], the decoder input includes an instruction denoting which
    section type is being summarized. They train a single model on all sections while
    providing the section identifier. In contrast, [[25](#bib.bib25)] trains separate
    models for each section type. We similarly study section-level summarization but
    adopt the former approach of training singular models, as in our preliminary experiments,
    we did not observe clear benefits from specialized models.
  prefs: []
  type: TYPE_NORMAL
- en: A limitation acknowledged by both papers is that summarizing sections in isolation
    loses important global context, as sections relate closely to each other. Our
    prompting techniques aimed at providing contextual information may be particularly
    beneficial for section-focused summarization, which we analyze in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, prior work motivates the investigation of scientific article summarization
    using individual sections. Nevertheless, the absence of access to the complete
    text may result in the loss of contextual information that is not inherently evident
    in isolated sections. This information deficit may hinder the generation of a
    comprehensive summary. Our prompts containing key terms from the entire article
    could assist in recovering lost global context when summarizing sections individually.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section details our proposed methods, which conceive and evaluate a set
    of novel prompting techniques to enhance the performance of state-of-the-art transformer
    models on scientific summarization across various settings. We describe our three
    key evaluation dimensions below.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Prompting Technique Dimension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our core dimension involves comparing various approaches for generating prompts
    to provide scientific summarizers with useful contextual information. As highlighted
    in Section [2](#S2 "2 Related Work ‣ Prompting LLMs with content plans to enhance
    the summarization of scientific articles"), a prior method by [[22](#bib.bib22)]
    instructed models with chains of entities extracted from reference summaries.
    However, accurately predicting key entities in advance is highly challenging for
    complex documents like scientific articles.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, our techniques provide lists of salient terms obtained through unsupervised
    extraction from input texts. The key hypothesis is that supplying relevant terms
    will help focus summarizers on important concepts to include in generated outputs.
    Crucially, our prompts are conceived to be easily obtainable without needing to
    involve additional models or knowledge of the structure of the summary to be generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In total, we conceive and evaluate five distinct prompting techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Keywords](#glo.main.Keywords), which consists of author-curated terms, not
    necessarily covering all topics.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MeSH](#glo.main.MeSH), providing a taxonomy of terms related to the article.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KeyBERT](#glo.main.KeyBERT), leveraging pretrained language representations
    for identifying salient terms.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TF](#glo.main.TF), taking the most frequent terms in the whole text.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TF-IDF](#glo.main.TF-IDF), selecting the terms that have a higher frequency
    with respect to the other sections in the text.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'All our proposed techniques structure prompts similarly, starting with a delimiter
    token indicating prompt start, e.g., [CONTENT], followed by the extracted key
    terms for that article section, ending with a [SUMMARY] token to denote summary
    start: [CONTENT] term1 $|$ ... termN [SUMMARY].'
  prefs: []
  type: TYPE_NORMAL
- en: The core hypothesis is that training summarization models with these informative
    term prompts will allow scientific summarizers to better identify and focus on
    salient concepts to include in generated outputs. Models can learn to leverage
    supplied terms as useful guidance without having to rigidly follow exactly the
    provided keywords, which may not cover all necessary concepts. During training,
    reference summaries are concatenated after prompts to optimize models as usual
    through teacher forcing, while only prompts are given at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our experiments aim to determine whether these easily obtained general
    prompts listing salient terms can enhance summarizers’ focus on core concepts
    to improve output quality without requiring strict instruction following or complex
    generative models for prompt production. We evaluate their impact when integrated
    with various state-of-the-art summarizers. By comparing a diversity of extraction
    techniques to generate prompts, we aim to determine what styles of key terms best
    provide useful context. We expect that effectiveness may vary with respect to
    the technique used.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Model Dimension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second dimension we investigate involves studying our proposed prompting
    techniques integrated with a range of current state-of-the-art transformer models
    for scientific summarization. As highlighted in Section [2](#S2 "2 Related Work
    ‣ Prompting LLMs with content plans to enhance the summarization of scientific
    articles"), transformers have become dominant for abstractive summarization. We
    select a diversity of architectures to analyze prompting effects more robustly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core set of models we evaluate are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LT5-Base-Local](#glo.main.LT5-Base-Local) [[3](#bib.bib3)], [LT5-Base-ETC](#glo.main.LT5-Base-ETC)
    [[4](#bib.bib4)] and [LT5-Large-ETC](#glo.main.LT5-Large-ETC) [[5](#bib.bib5)],
    three variants in size and attention architecture of the [LongT5](#glo.main.LongT5)
    [[17](#bib.bib17)] model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LED](#glo.main.LED), Longformer Encoder-Decoder, combining the architecture
    of BART [[18](#bib.bib18)] with the Longformer [[9](#bib.bib9)] attention model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[BigBirdPegasus](#glo.main.BigBirdPegasus), applying the BigBird [[36](#bib.bib36)]
    sparse attention mechanism to Google’s Pegasus [[37](#bib.bib37)] model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These choices cover a range of attention mechanisms, model sizes, and base architectures.
    [LongT5](#glo.main.LongT5) allows comparing local against [ETC](#glo.main.ETC)
    attention, and base against large model size scaling; [LED](#glo.main.LED) provides
    an alternate architecture through Longformer attention; and [BigBirdPegasus](#glo.main.BigBirdPegasus)
    employs the most complex attention. All models are fine-tuned on our scientific
    summarization dataset with and without prompts.
  prefs: []
  type: TYPE_NORMAL
- en: By integrating prompting techniques with multiple state-of-the-art summarizers,
    we aim to determine whether effectiveness generalizes across models or varies
    by architecture. We hypothesize that smaller models may benefit more from prompts
    providing helpful context compared to large models with greater representation
    capacity. Similarly, global attention may use prompts more effectively than models
    using sparse local attention only. Analyzing interactions between prompts and
    models is a key goal.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Input Text Dimension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The third dimension explored concerns the text inputs fed to the encoders.
    We study three main conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[I+D](#glo.main.I+D), where we take the concatenation of the introduction and
    discussion sections.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[S-n/a](#glo.main.S-n/a), taking the texts of the different sections, summarising
    them separately, and later integrating the different summaries into a general
    summary, as described in Section [2.2](#S2.SS2 "2.2 Faceted Summarization ‣ 2
    Related Work ‣ Prompting LLMs with content plans to enhance the summarization
    of scientific articles"). The prompt for the summarization of each section is
    not annotated with the type of the section.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[S-w/a](#glo.main.S-w/a), adopting the same approach as [S-n/a](#glo.main.S-n/a),
    but prepending the prompt corresponding to each section with the annotation of
    the type of the section (e.g., ‘Introduction’, ‘Methods’, …).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We hypothesize that in the case of [I+D](#glo.main.I+D), prompts may provide
    lesser gains since this text already expresses a cohesive overview. On the other
    hand, when summarizing sections in isolation ([S-n/a](#glo.main.S-n/a) and [S-w/a](#glo.main.S-w/a)),
    prompts may be more helpful in recovering lost global context no longer expressed
    within single sections. We expect prompts may help more when summarizing sections
    independently.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, when generating section-level summaries, we compare either providing
    just the target section text alone ([S-n/a](#glo.main.S-n/a)), or prepending the
    prompt with an explicit token denoting the section type, e.g., [INTRODUCTION]
    ([S-w/a](#glo.main.S-w/a)). We hypothesize that explicitly indicating the section
    type could further improve results by focusing models on that section’s expected
    content.
  prefs: []
  type: TYPE_NORMAL
- en: By evaluating the three settings, we aim to understand when contextual information
    from prompts provides the most significant gains. We expect faceted summarization
    may benefit most from supplementary global context through prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes our experiments evaluating the conceived prompting techniques
    integrated with various state-of-the-art summarization models and input texts.
    We first detail the dataset used, preprocessing performed, training methodology,
    and evaluation protocol. We then present the results.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train and evaluate scientific summarization systems, we use a dataset of
    open-access biomedical papers from PubMed Central [[15](#bib.bib15)]. We focus
    on the biomedical domain due to the large volume of openly available papers with
    structured abstracts summarizing key sections.
  prefs: []
  type: TYPE_NORMAL
- en: We extract a subset of 11,614 articles satisfying filtering criteria designed
    to obtain high-quality training examples from the complete PubMed Central corpus.
    For inclusion, papers must contain full text, author keywords, and structured
    abstracts with identifiable Introduction, Methods, Results, and Discussion ([IMRAD](#glo.main.IMRAD))
    sections modeling standard scientific manuscript organization [[21](#bib.bib21)].
    Articles deviating over two standard deviations from the mean length are excluded
    as outliers. This exclusion ensures that adequately sized input texts remain available
    for training summarization models.
  prefs: []
  type: TYPE_NORMAL
- en: In the preprocessing stage, segments are extracted from aggregated metadata
    or through the application of heuristics that identify common header phrases such
    as “Introduction" or “Discussion". Input texts are truncated at 2048 tokens for
    [I+D](#glo.main.I+D) and 512 tokens for [S-n/a](#glo.main.S-n/a) and [S-w/a](#glo.main.S-w/a),
    leaving 98.6% and 66% of texts unchanged, respectively. Reference abstracts are
    truncated at 512 tokens, a length surpassing that of 98% of examples. The dataset
    is randomly partitioned into training (70%), validation (15%), and test (15%)
    sets. Final metric performances are reported on the test set using models selected
    based on validation results.
  prefs: []
  type: TYPE_NORMAL
- en: This scientific summarization dataset provides a challenging and realistic benchmark
    to rigorously evaluate our proposed prompting techniques when integrated with
    modern transformer architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We perform additional Fine-Tuning on our dataset to integrate prompting capabilities
    within summarization models. As model capabilities vary, we tune hyperparameters,
    including learning rates and gradient accumulation steps, on validation results
    using Ray Tune [[19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: The core training procedure is as follows. In each batch, prompts are formulated
    per the specified technique outlined in Section [3.1](#S3.SS1 "3.1 Prompting Technique
    Dimension ‣ 3 Methods ‣ Prompting LLMs with content plans to enhance the summarization
    of scientific articles"). For each model pertaining to Section [3.2](#S3.SS2 "3.2
    Model Dimension ‣ 3 Methods ‣ Prompting LLMs with content plans to enhance the
    summarization of scientific articles"), training is executed using a target output
    corresponding to the prompt’s concatenation with the target summary derived from
    the training example. This concatenation is contingent upon the input text dimension,
    as described in Section [3.3](#S3.SS3 "3.3 Input Text Dimension ‣ 3 Methods ‣
    Prompting LLMs with content plans to enhance the summarization of scientific articles").
    Training is conducted utilizing teacher forcing, and validation ROUGE is monitored
    to implement early stopping, preventing overfitting or inefficient resource utilization.
    During inference, only prompts and input texts are supplied as input.
  prefs: []
  type: TYPE_NORMAL
- en: We compare models trained on our dataset with versus without prompts to quantify
    the impact of prompting techniques. As a strong baseline, all models are first
    fine-tuned on the dataset without prompts, referred to as Fine-Tuning. This measures
    their out-of-the-box summarization capabilities. We then compare Fine-Tuning with
    our prompting techniques, e.g., [KeyBERT](#glo.main.KeyBERT).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We present the primary summarization results for the test split in Table [1](#S4.T1
    "Table 1 ‣ 4.3 Metrics ‣ 4 Results ‣ Prompting LLMs with content plans to enhance
    the summarization of scientific articles"). Table [2](#S4.T2 "Table 2 ‣ 4.3 Metrics
    ‣ 4 Results ‣ Prompting LLMs with content plans to enhance the summarization of
    scientific articles") illustrates the relative improvement of the techniques compared
    to Fine-Tuning, computed according to Equation [1](#S4.E1 "In 4.3 Metrics ‣ 4
    Results ‣ Prompting LLMs with content plans to enhance the summarization of scientific
    articles"), where the scores of $\text{V}_{\text{Technique}}$ are calculated using
    ROUGE metrics [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Improvement}=\frac{\text{V}_{\text{Technique}}-\text{V}_{\text{Fine-Tuning}}}{\text{V}_{\text{Fine-Tuning}}}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: In addition to overall performance, we conduct targeted confusion testing by
    feeding models with prompts extracted from unrelated articles. If prompts improve
    focusing, the provision of irrelevant prompts should decrease quality compared
    to proper in-domain prompts. More significant degradation implies better exploitation
    of prompt information. We analyze confusion testing results to isolate the benefits
    of prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '[BigBirdPegasus](#glo.main.BigBirdPegasus) [LED](#glo.main.LED) [LT5-Base-Local](#glo.main.LT5-Base-Local)
    [LT5-Base-ETC](#glo.main.LT5-Base-ETC) [LT5-Large-ETC](#glo.main.LT5-Large-ETC)
    [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D)
    [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a)
    [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a)
    [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) ROUGE-1
    Original 0.273 0.356 $\mathbf{0.347}$ 0.395 $\mathbf{0.392}$ $0.296$ $0.110$ $\mathbf{0.103}$
    0.207 0.214 $\mathbf{0.173}$ $0.219$ $\mathbf{0.377}$ 0.266 0.373 $\mathbf{0.403}$
    0.304'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Test results.'
  prefs: []
  type: TYPE_NORMAL
- en: '[BigBirdPegasus](#glo.main.BigBirdPegasus) [LED](#glo.main.LED) [LT5-Base-Local](#glo.main.LT5-Base-Local)
    [LT5-Base-ETC](#glo.main.LT5-Base-ETC) [LT5-Large-ETC](#glo.main.LT5-Large-ETC)
    [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D)
    [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a)
    [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a)
    [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) ROUGE-1
    [Keywords](#glo.main.Keywords) $-0.126$ $-0.052$ $-0.144$ $0.044$ $0.088$ $0.192$
    $0.096$ $0.065$ $0.085$ $0.058$ $0.153$ $0.251$ $-0.175$ $-0.034$ $-0.071$'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Improvements of the techniques presented, in relation to Fine-Tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[BigBirdPegasus](#glo.main.BigBirdPegasus) [LED](#glo.main.LED) [LT5-Base-Local](#glo.main.LT5-Base-Local)
    [LT5-Base-ETC](#glo.main.LT5-Base-ETC) [LT5-Large-ETC](#glo.main.LT5-Large-ETC)
    [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D)
    [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a)
    [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a)
    [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) ROUGE-1
    [Keywords](#glo.main.Keywords) $0.255$ $0.346$ $\mathbf{0.336}$ $\mathbf{0.143}$
    $\mathbf{0.159}$ $0.116$ $\mathbf{0.304}$ $\mathbf{0.304}$ $\mathbf{0.313}$'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Confusion tests results.'
  prefs: []
  type: TYPE_NORMAL
- en: '[BigBirdPegasus](#glo.main.BigBirdPegasus) [LED](#glo.main.LED) [LT5-Base-Local](#glo.main.LT5-Base-Local)
    [LT5-Base-ETC](#glo.main.LT5-Base-ETC) [LT5-Large-ETC](#glo.main.LT5-Large-ETC)
    [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D)
    [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a)
    [S-w/a](#glo.main.S-w/a) [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a)
    [I+D](#glo.main.I+D) [S-n/a](#glo.main.S-n/a) [S-w/a](#glo.main.S-w/a) ROUGE-1
    [Keywords](#glo.main.Keywords) $0.060$ $-0.033$ $-0.001$ $-0.375$ $-0.436$ $-0.743$
    $-0.227$ $-0.277$ $-0.238$ $-0.179$ $-0.192$ $-0.270$ $0.043$ $-0.046$ $-0.055$'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison of the results of confusion tests with respect to Table
    [1](#S4.T1 "Table 1 ‣ 4.3 Metrics ‣ 4 Results ‣ Prompting LLMs with content plans
    to enhance the summarization of scientific articles").'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The general trend indicates that prompting yields modest yet consistent ROUGE
    improvements across all smaller models ([LED](#glo.main.LED), [LT5-Base-Local](#glo.main.LT5-Base-Local),
    and [LT5-Base-ETC](#glo.main.LT5-Base-ETC)) for [I+D](#glo.main.I+D), suggesting
    that supplied terms offer valuable global context. However, more substantial gains
    are evident in the case of section-level summarization for these models. For example,
    [LT5-Base-ETC](#glo.main.LT5-Base-ETC) demonstrates an increase of up to 0.241
    in ROUGE-1 and 0.462 in ROUGE-2 when prompts are utilized for section summarization.
    While other techniques do not exhibit such significant enhancements, they consistently
    manifest improvements.
  prefs: []
  type: TYPE_NORMAL
- en: As previously stated, prompting results in significantly more significant improvements
    for smaller models compared to larger ones ([BigBirdPegasus](#glo.main.BigBirdPegasus)
    and [LT5-Large-ETC](#glo.main.LT5-Large-ETC)). The substantial enhancements observed
    in smaller models imply that lightweight architectures, characterized by reduced
    representational power, exhibit a heightened dependence on supplementary external
    information derived from prompts that concentrate on content.
  prefs: []
  type: TYPE_NORMAL
- en: In general, no single technique consistently outperforms across all settings,
    implying that the optimal selection depends on specific architectures and tasks.
    Nevertheless, [KeyBERT](#glo.main.KeyBERT) generally exhibits superior performance,
    albeit by a slight margin. This observation suggests that prompting techniques
    provide additional information that aids in contextualizing the summarization
    task. This information proves beneficial across various cases without a discernible
    dependence on any particular prompting technique becoming apparent.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the ETC attention mechanism ([LT5-Base-ETC](#glo.main.LT5-Base-ETC))
    appears to outperform the sliding window attention mechanism ([LT5-Base-Local](#glo.main.LT5-Base-Local)).
    This superiority is likely attributed to the positioning of the instruction at
    the beginning of the text. The sliding window attention mechanism exhibits limitations
    as the summary generation progresses. In contrast, utilizing global and random
    attention, ETC mitigates this limitation, thereby accounting for the superior
    results obtained. Similarly, [LED](#glo.main.LED) also incorporates a global attention
    mechanism, underscoring the importance of adopting an attention architecture that
    ensures continuous access to the instruction throughout the summarization process.
    Notably, although [BigBirdPegasus](#glo.main.BigBirdPegasus) employs the most
    complex attention mechanism among the five, this complexity does not necessarily
    translate into enhanced performance in exploiting the studied techniques. This
    discrepancy may be attributed to the reduced reliance on contextual information
    for generating high-quality summaries, as previously discussed.
  prefs: []
  type: TYPE_NORMAL
- en: To further analyze models’ reliance on informative prompts, Tables [3](#S4.T3
    "Table 3 ‣ 4.3 Metrics ‣ 4 Results ‣ Prompting LLMs with content plans to enhance
    the summarization of scientific articles") and [4](#S4.T4 "Table 4 ‣ 4.3 Metrics
    ‣ 4 Results ‣ Prompting LLMs with content plans to enhance the summarization of
    scientific articles") present the confusion test results for section summarization,
    as detailed in Section [4.3](#S4.SS3 "4.3 Metrics ‣ 4 Results ‣ Prompting LLMs
    with content plans to enhance the summarization of scientific articles"). As depicted
    in the tables, smaller models demonstrate significant declines in quality when
    exposed to shuffled prompts, while [BigBirdPegasus](#glo.main.BigBirdPegasus)
    and [LT5-Large-ETC](#glo.main.LT5-Large-ETC) exhibit a mixture of improvements
    and deteriorations, showing a tendency to disregard prompts in generating predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall trend suggests that supplying additional terms through concise
    prompts allows smaller models to better identify and concentrate on salient concepts
    to include in generated outputs. Our findings demonstrate the efficacy of prompting
    techniques for scientific summarizers, with implications emerging:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting provides consistent gains across smaller models when summarizing sections.
    This confirms that prompts can focus systems on key concepts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion testing reveals diminished performance when prompts are unrelated
    for smaller models. In this case, prompting is actively exploited rather than
    ignored, unlike in larger models, which benefit less from prompts, instead relying
    on internal learned representations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among prompt generation techniques, [KeyBERT](#glo.main.KeyBERT) performs marginally
    better. performs marginally better. Nonetheless, optimal techniques vary depending
    on the setting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [ETC](#glo.main.ETC) attention mechanism presents advancements compared
    to sliding window attention. This improvement is likely attributable to the capability
    to integrate contextual information from the prompt through global and random
    attention mechanisms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For section summarization, supplying section types further improves results,
    confirming this extra explicit context helps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The central implication derived from our findings is that employing decoder
    prompting offers a means to ameliorate the inherent limitations of smaller summarization
    models within suitable contexts. Rather than exclusively focusing on developing
    larger architectures, compact models enhanced with instructive prompts may present
    a practical alternative for environments with resource constraints, such as mobile
    devices. By addressing their inherent shortcomings through external guidance,
    achieving robust performance from lightweight summarizers seems attainable.
  prefs: []
  type: TYPE_NORMAL
- en: Our work introduces prompting as a general technique to meaningfully enhance
    small neural network summarizers. Prompting could potentially aid simple extractive
    systems by focusing selections and representing an alternative research direction.
    Our techniques provide an easily adoptable means of upgrading summarizers’ capacities
    without requiring extensive re-engineering. Numerous promising opportunities exist
    for further exploration of decoder prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While results indicate consistent summarization improvements from prompting
    techniques, especially for smaller models run on sections, there remain promising
    opportunities for future work. We highlight some high-level directions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our results showed some variability in performance between different prompting
    techniques. Further techniques could be explored, such as RAKE [[29](#bib.bib29)],
    TextRank [[20](#bib.bib20)], and YAKE [[10](#bib.bib10)], to automatically extract
    salient concepts from texts to form prompts. Comparing these approaches could
    reveal even better-performing options.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approach in [[22](#bib.bib22)] proposes instruction based on predicted named
    entities in summaries. While highly challenging for long scientific papers, future
    work could investigate automatically generating quality entity prompt chains.
    This could produce more optimized prompts than our current term extraction methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results indicate that sliding window attention does not perform on par with
    ETC for prompting. Therefore, an idea is to adapt global attention to directly
    focus on prompt token positions, thus avoiding the lossy aggregation into chunk
    representations in ETC. This adaptation could enhance prompt utilization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In summary, there exist opportunities for future research to build upon our
    introduced techniques. Continued analysis of optimal methods and architectural
    integration merits exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper introduces and assesses a collection of innovative prompting techniques
    aimed at enhancing scientific summarization systems by offering contextual guidance
    through informative key-term prompts. We propose and examine various prompting
    methods based on provided terms ([Keywords](#glo.main.Keywords) and [MeSH](#glo.main.MeSH)),
    salient items from [KeyBERT](#glo.main.KeyBERT), and extractive statistics ([TF](#glo.main.TF)
    and [TF-IDF](#glo.main.TF-IDF)). Our techniques are designed to be seamlessly
    integrated with any summarizer without requiring complex additional generative
    models or knowledge of the text to be generated, unlike in prior research.
  prefs: []
  type: TYPE_NORMAL
- en: We conducted experiments to assess the impact of our proposed prompting approaches
    on several state-of-the-art transformer-based scientific article summarizers,
    employing various model sizes and attention mechanisms. Our analysis covered both
    [I+D](#glo.main.I+D) and section-level summarization. Our findings indicate that
    focused local contexts derive the greatest benefit from global information provided
    through prompts. Additionally, we observed a more significant improvement in smaller
    models, which lack representational capacity, compared to larger models with fewer
    intrinsic limitations when subjected to prompting.
  prefs: []
  type: TYPE_NORMAL
- en: The findings of our study reveal consistent enhancements in standard quality
    metrics such as ROUGE when incorporating prompting in smaller models, particularly
    when executed on individual sections. These configurations yield more precise
    and comprehensive summaries that encompass a greater number of key concepts from
    gold references than unaltered models. Additionally, confusion testing provides
    further evidence that these models actively leverage supplied informative terms.
  prefs: []
  type: TYPE_NORMAL
- en: This work makes multiple key contributions. We introduce a new direction of
    decoder prompting for enhancing summarization, analyze efficacy across models
    and tasks, and demonstrate particular utility for improving fundamental deficiencies
    of smaller models in appropriate contexts. Rather than solely bigger architectures,
    smaller prompted models may suffice in some real-world resource-limited applications.
  prefs: []
  type: TYPE_NORMAL
- en: Our findings reveal prompting as an easily adoptable technique to upgrade base
    summarizer implementations designed for long-form documents. There remain open
    questions for future investigation, including tailoring prompts to models, developing
    augmented prompting mechanisms, studying sentence-level prompting, and integrating
    automatic evaluation of contextual relevance. There are broad opportunities for
    future advancements building on this work on decoder prompting to enhance the
    summarization of scientific articles.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work has received financial support from the Consellería de Educación,
    Universidade e Formación Profesional (accreditation 2019-2022 ED431G-2019/04),
    the European Regional Development Fund (ERDF), which acknowledges the CiTIUS -
    Centro Singular de Investigación en Tecnoloxías Intelixentes da Universidade de
    Santiago de Compostela as a Research Center of the Galician University System,
    and the Spanish Ministry of Science and Innovation (grants PDC2021-121072-C21
    and PID2020-112623GB-I00). Aldan Creo is supported by the Spanish Ministerio de
    Universidades under the Collaboration Fellowships in University Departments scheme
    (BDNS-633225). Furthermore, the authors also wish to thank the supercomputing
    facilities provided by CESGA.
  prefs: []
  type: TYPE_NORMAL
- en: Glossary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BigBirdPegasus
  prefs: []
  type: TYPE_NORMAL
- en: '[BigBirdPegasus](#glo.main.BigBirdPegasus) [[2](#bib.bib2)] combines Google’s
    Pegasus [[37](#bib.bib37)] model with the BigBird [[36](#bib.bib36)] sparse attention
    mechanism, allowing it to handle extremely long sequences. BigBird uses a combination
    of random, sliding window, and global attention to reduce the standard quadratic
    self-attention complexity. We incorporate [BigBirdPegasus](#glo.main.BigBirdPegasus)
    as it employs the most sophisticated attention scheme of models tested.'
  prefs: []
  type: TYPE_NORMAL
- en: ETC
  prefs: []
  type: TYPE_NORMAL
- en: Extended Transformer Construction (ETC) is an attention mechanism that utilizes
    both local attention and attention towards a set of global tokens [[7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: I+D
  prefs: []
  type: TYPE_NORMAL
- en: Concatenation of the introduction and discussion texts, employed as a method
    to approximate the full text without incorporating the complexity of processing
    it entirely. This approach is motivated by the findings of [[31](#bib.bib31)],
    which demonstrates that the performance of summarization models utilizing both
    the introduction and discussion is comparable to those using the complete text.
    Given the cohesive overview expressed in the integration of introduction and discussion,
    our hypothesis is that prompts may yield lesser gains.
  prefs: []
  type: TYPE_NORMAL
- en: IMRAD
  prefs: []
  type: TYPE_NORMAL
- en: A commonly followed scheme for the structure of the abstracts of scientific
    articles, most prominently used in medical literature, featuring separate texts
    for the Introduction, Methods, Results and Discussion sections [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: KeyBERT
  prefs: []
  type: TYPE_NORMAL
- en: An unsupervised keyword extraction model based on distilled BERT [[14](#bib.bib14)]
    embeddings. We generate keywords from full input texts to form prompts, letting
    the model determine important terms rather than authors [[16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: Keywords
  prefs: []
  type: TYPE_NORMAL
- en: The technique constructs prompts from author-provided keywords for each article.
    Author keywords offer an intuitive indicator of salient topical content. Prompts
    contain keywords separated by a delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: LED
  prefs: []
  type: TYPE_NORMAL
- en: A Longformer Encoder-Decoder is based on BART [[18](#bib.bib18)] while utilizing
    Longformer self-attention [[9](#bib.bib9)] to process long texts. Longformer combines
    sliding local attention windows with global attention from selected tokens. We
    integrate the base model to provide a comparison using an alternate attention
    approach from BigBird.
  prefs: []
  type: TYPE_NORMAL
- en: LongT5
  prefs: []
  type: TYPE_NORMAL
- en: LongT5 [[17](#bib.bib17)] is an extension of T5 [[27](#bib.bib27)] incorporating
    optimized sparse attention mechanisms to handle longer sequences beyond T5’s 512
    token limit. It allows input length up to 4096 tokens with either sliding window
    ([LT5-Base-Local](#glo.main.LT5-Base-Local)) or attention ([LT5-Base-ETC](#glo.main.LT5-Base-ETC)
    and [LT5-Large-ETC](#glo.main.LT5-Large-ETC)).
  prefs: []
  type: TYPE_NORMAL
- en: LT5-Base-ETC
  prefs: []
  type: TYPE_NORMAL
- en: Variant of in base size employing attention [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: LT5-Base-Local
  prefs: []
  type: TYPE_NORMAL
- en: Variant of in base size employing sliding window attention only [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: LT5-Large-ETC
  prefs: []
  type: TYPE_NORMAL
- en: Variant of in large size employing attention [[5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: MeSH
  prefs: []
  type: TYPE_NORMAL
- en: In biomedical scientific articles, manuscripts are indexed with MeSH (Medical
    Subject Headings) terms indicating key topics [[24](#bib.bib24)]. They provide
    a comprehensive controlled vocabulary for medical concepts.
  prefs: []
  type: TYPE_NORMAL
- en: S-n/a
  prefs: []
  type: TYPE_NORMAL
- en: Concatenation of the introduction and discussion texts. When summarizing sections
    in isolation, we hypothesize that prompts may be more useful for recovering lost
    global context no longer expressed within single sections. We expect prompts may
    help more when summarizing sections independently.
  prefs: []
  type: TYPE_NORMAL
- en: S-w/a
  prefs: []
  type: TYPE_NORMAL
- en: Same as [S-n/a](#glo.main.S-n/a), with the addition that the identifier of the
    section type is prepended to the prompt, our hypothesis being that the additional
    context given may entail an increase in performance.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs: []
  type: TYPE_NORMAL
- en: A standard unsupervised statistic measuring how important words are to a document
    within a corpus. Words with high term frequency locally and low document frequency
    globally receive the highest scores. We extract the top [TF-IDF](#glo.main.TF-IDF)
    terms for each section as prompts [[38](#bib.bib38)].
  prefs: []
  type: TYPE_NORMAL
- en: TF
  prefs: []
  type: TYPE_NORMAL
- en: This unsupervised technique constructs prompts from the most frequently occurring
    non-stopwords in each article. TF provides a basic measure of term importance.
    Stopwords are filtered out using SpaCy [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] allenai/led-base-16384. URL [https://huggingface.co/allenai/led-base-16384](https://huggingface.co/allenai/led-base-16384).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: goo [a] google/bigbird-pegasus-large-pubmed. a. URL [https://huggingface.co/google/bigbird-pegasus-large-pubmed](https://huggingface.co/google/bigbird-pegasus-large-pubmed).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: goo [b] google/long-t5-local-base. b. URL [https://huggingface.co/google/long-t5-local-base](https://huggingface.co/google/long-t5-local-base).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: goo [c] google/long-t5-tglobal-base. c. URL [https://huggingface.co/google/long-t5-tglobal-base](https://huggingface.co/google/long-t5-tglobal-base).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: goo [d] google/long-t5-tglobal-large. d. URL [https://huggingface.co/google/long-t5-tglobal-large](https://huggingface.co/google/long-t5-tglobal-large).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agrawal et al. [2019] Kritika Agrawal, Aakash Mittal, and Vikram Pudi. Scalable,
    semi-supervised extraction of structured information from scientific literature.
    pages 11–20\. Association for Computational Linguistics, 2019. doi:[10.18653/v1/W19-2602](https://doi.org/10.18653/v1/W19-2602).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ainslie et al. [2020] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav
    Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang,
    and Li Yang. Etc: Encoding long and structured inputs in transformers. 4 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Altmami and Menai [2022] Nouf Ibrahim Altmami and Mohamed El Bachir Menai.
    Automatic summarization of scientific articles: A survey. *Journal of King Saud
    University - Computer and Information Sciences*, 34:1011–1028, 4 2022. ISSN 13191578.
    doi:[10.1016/j.jksuci.2020.04.020](https://doi.org/10.1016/j.jksuci.2020.04.020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beltagy et al. [2020] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer:
    The long-document transformer. 4 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Campos et al. [2020] Ricardo Campos, Vítor Mangaravite, Arian Pasquali, Alípio
    Jorge, Célia Nunes, and Adam Jatowt. Yake! keyword extraction from single documents
    using multiple local features. *Information Sciences*, 509:257–289, 1 2020. ISSN
    00200255. doi:[10.1016/j.ins.2019.09.013](https://doi.org/10.1016/j.ins.2019.09.013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chin-Yew [2004] Lin Chin-Yew. Rouge: A package for automatic evaluation of
    summaries. *Text Summarization Branches Out: Proceedings of the ACL-04 Workshop*,
    pages 74–81, 2004. URL [https://aclanthology.org/W04-1013/](https://aclanthology.org/W04-1013/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohan and Goharian [2017] Arman Cohan and Nazli Goharian. Contextualizing citations
    for scientific summarization using word embeddings and domain knowledge. 5 2017.
    doi:[10.1145/3077136.3080740](https://doi.org/10.1145/3077136.3080740).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohan et al. [2018] Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui,
    Seokhwan Kim, Walter Chang, and Nazli Goharian. A discourse-aware attention model
    for abstractive summarization of long documents. 4 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.
    doi:[10.18653/v1/N19-1423](https://doi.org/10.18653/v1/N19-1423). URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] National Center for Biotechnology Information. Pubmed annual baseline.
    URL [https://www.nlm.nih.gov/databases/download/pubmed_medline.html](https://www.nlm.nih.gov/databases/download/pubmed_medline.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grootendorst [2020] Maarten Grootendorst. Keybert: Minimal keyword extraction
    with bert., 2020. URL [https://doi.org/10.5281/zenodo.4461265](https://doi.org/10.5281/zenodo.4461265).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. [2021] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon,
    Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer
    for long sequences. 12 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. [2019] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising
    sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. 10 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liaw et al. [2018] Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz,
    Joseph E. Gonzalez, and Ion Stoica. Tune: A research platform for distributed
    model selection and training. 7 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mihalcea and Tarau [2004] Rada Mihalcea and Paul Tarau. Textrank: Bringing
    order into text. pages 404–411\. Association for Computational Linguistics, 7
    2004. URL [https://aclanthology.org/W04-3252](https://aclanthology.org/W04-3252).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakayama et al. [2005] Takeo Nakayama, Nobuko Hirai, Shigeaki Yamazaki, and
    Mariko Naito. Adoption of structured abstracts by general medical journals and
    format for a structured abstract. *Journal of the Medical Library Association
    : JMLA*, 93:237–42, 4 2005. ISSN 1536-5050.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narayan et al. [2021] Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo Simões,
    Vitaly Nikolaev, and Ryan McDonald. Planning with learned entity prompts for abstractive
    summarization. *Transactions of the Association for Computational Linguistics*,
    9:1475–1492, 12 2021. ISSN 2307-387X. doi:[10.1162/tacl_a_00438](https://doi.org/10.1162/tacl_a_00438).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neumann et al. [2019] Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar.
    Scispacy: Fast and robust models for biomedical natural language processing. 2
    2019. doi:[10.18653/v1/W19-5034](https://doi.org/10.18653/v1/W19-5034).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] National Library of Medicine. Medical subject headings. URL [https://www.nlm.nih.gov/mesh/meshhome.html](https://www.nlm.nih.gov/mesh/meshhome.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oh et al. [2023] Hanseok Oh, Seojin Nam, and Yongjun Zhu. Structured abstract
    summarization of scientific articles: Summarization using full-text section information.
    *Journal of the Association for Information Science and Technology*, 74:234–248,
    2 2023. ISSN 2330-1635. doi:[10.1002/asi.24727](https://doi.org/10.1002/asi.24727).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qazvinian et al. [2010] Vahed Qazvinian, Dragomir R Radev, and Arzucan Özgür.
    Citation summarization through keyphrase extraction. pages 895–903\. Coling 2010
    Organizing Committee, 8 2010. URL [https://aclanthology.org/C10-1101](https://aclanthology.org/C10-1101).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2019] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. 10 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ronzano and Saggion [2016] Francesco Ronzano and Horacio Saggion. An empirical
    assessment of citation information in scientific summarization, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rose et al. [2010] Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. Automatic
    keyword extraction from individual documents, 3 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saggion [2011] Horacio Saggion. Learning predicate insertion rules for document
    abstracting, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. [2019] Eva Sharma, Chen Li, and Lu Wang. Bigpatent: A large-scale
    dataset for abstractive and coherent summarization. 6 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soleimani et al. [2022] Amir Soleimani, Vassilina Nikoulina, Benoit Favre, and
    Salah Ait Mokhtar. Zero-shot aspect-based scientific document summarization using
    self-supervised pre-training. pages 49–62\. Association for Computational Linguistics,
    2022. doi:[10.18653/v1/2022.bionlp-1.5](https://doi.org/10.18653/v1/2022.bionlp-1.5).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teufel and Moens [2002] Simone Teufel and Marc Moens. Summarizing scientific
    articles: Experiments with relevance and rhetorical status. *Computational Linguistics*,
    28:409–445, 12 2002. ISSN 0891-2017. doi:[10.1162/089120102762671936](https://doi.org/10.1162/089120102762671936).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. volume 2017-December, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2016] Shansong Yang, Weiming Lu, Zhanjiang Zhang, Baogang Wei,
    and Wenjia An. Amplifying scientific paper’s abstract by leveraging data-weighted
    reconstruction. *Information Processing & Management*, 52:698–719, 7 2016. ISSN
    03064573. doi:[10.1016/j.ipm.2015.12.014](https://doi.org/10.1016/j.ipm.2015.12.014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. [2020] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. 7 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J.
    Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.
    12 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: zhi Liu et al. [2018] Cai zhi Liu, Yan xiu Sheng, Zhi qiang Wei, and Yong-Quan
    Yang. Research of text classification based on improved tf-idf algorithm. pages
    218–222\. IEEE, 8 2018. ISBN 978-1-5386-7416-1. doi:[10.1109/IRCE.2018.8492945](https://doi.org/10.1109/IRCE.2018.8492945).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
