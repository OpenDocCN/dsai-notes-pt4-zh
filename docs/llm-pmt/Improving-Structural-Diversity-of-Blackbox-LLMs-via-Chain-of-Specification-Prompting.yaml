- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:40:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.06186](https://ar5iv.labs.arxiv.org/html/2408.06186)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Halley Young
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Research
  prefs: []
  type: TYPE_NORMAL
- en: halleyyoung@microsoft.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Yimeng Zeng'
  prefs: []
  type: TYPE_NORMAL
- en: University of Pennsylvania
  prefs: []
  type: TYPE_NORMAL
- en: yimengz@seas.upenn.edu
  prefs: []
  type: TYPE_NORMAL
- en: \ANDJacob Gardner
  prefs: []
  type: TYPE_NORMAL
- en: University of Pennsylvania
  prefs: []
  type: TYPE_NORMAL
- en: jacobrg@seas.upenn.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Osbert Bastani'
  prefs: []
  type: TYPE_NORMAL
- en: University of Pennsylvania
  prefs: []
  type: TYPE_NORMAL
- en: obastani@seas.upenn.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The capability to generate diverse text is a key challenge facing large language
    models (LLMs). Thus far, diversity has been studied via metrics such as $n$-gram
    diversity or diversity of BERT embeddings. However, for these kinds of diversity,
    the user has little control over the dimensions along which diversity is considered.
    For example, in the poetry domain, one might desire diversity in terms of rhyme
    and meter, whereas in the code domain, one might desire diversity in terms of
    the kinds of expressions used to solve a problem. We propose a diversity metric
    called *structural diversity*, where the user provides a mapping from generated
    text to features capturing the kinds of diversity that they care about. In addition,
    we propose a novel strategy called *chain-of-specification (CoS) prompting* for
    improving diversity by first having the LLM generate a specification encoding
    one instance of structural features, and then prompting the LLM to generate text
    that satisfies these features; notably, our strategy works with blackbox LLMs.
    In our experiments, we show that for structural diversity in the poetry and code
    domains, CoS significantly improves diversity compared to several baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting
  prefs: []
  type: TYPE_NORMAL
- en: Halley Young Microsoft Research halleyyoung@microsoft.com                       
    Yimeng Zeng University of Pennsylvania yimengz@seas.upenn.edu
  prefs: []
  type: TYPE_NORMAL
- en: Jacob Gardner University of Pennsylvania jacobrg@seas.upenn.edu                       
    Osbert Bastani University of Pennsylvania obastani@seas.upenn.edu
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent advances in large language models (LLMs), such as ChatGPT OpenAI ([2022](#bib.bib6)),
    have led to significant improvements in the quality and coherence of machine-generated
    text. However, the diversity of the generated text remains limited, particularly
    in terms of capturing high-level semantic properties and stylistic variations.
    As a consequence, there has been a great deal of interest in techniques for improving
    the diversity of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Much of the existing work on diversity has focused on metrics based on $n$-grams
    or semantic representations such as BERT embeddings. However, in many applications,
    users may desire diversity along specific dimensions. For instance, users might
    want generated poems to be diverse in terms of the structure and content of the
    poem, such as imagery and language, rhyming scheme, meter, etc. Alternatively,
    in code generation, users may want to generate code in using a range of different
    paradigms (e.g., for Python, list comprehension vs. loop vs. recursion) so they
    can choose the fastest.
  prefs: []
  type: TYPE_NORMAL
- en: To account for these forms of diversity, we assume the user has provided a feature
    mapping $\phi:\mathcal{X}\to\mathcal{S}$ that encode structural properties of
    text, such as the example structures of poems and programs given above; thus,
    we refer to this notion of diversity as *structural diversity*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb0ead38628f45a3bb7165aac7d6d169.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Chain-of-Specification prompting'
  prefs: []
  type: TYPE_NORMAL
- en: The key challenge is how to design techniques capable of diverse generation
    according to a user-defined feature mapping. Inspired by chain-of-thought prompting Wei
    et al. ([2022](#bib.bib11)), we propose a two-step prompting strategy, which we
    call *single specification (SS) prompting* (summarized in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification
    Prompting")). First, we use the LLM to generate an example of structure $s\in\mathcal{S}$
    with diverse structure.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy can also be chained, where we first generate a high-level specification,
    then generate a low-level specification that satisfies it, and finally generate
    text that satisfies the low-level specification; more levels can also be included.
    For example, in the poetry domain, the prompts might ask for different kinds of
    structure, such as style, theme, imagery, etc. This process resembles chain-of-thought
    prompting Wei et al. ([2022](#bib.bib11)) since it asks the LLM to derive the
    final generated text using multiple steps, which we refer to as *chain-of-specification
    (CoS) prompting*.
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate our approach on domains including poetry generation, code generation,
    and generating coding challenge problems. Our results demonstrate that our approach
    is significantly more effective at improving structural diversity compared to
    existing approaches for diverse generation; one exception is for models that are
    not instruct tuned, since our approach relies on instruction following to work
    well. Finally, we also provide evidence that structural diversity captures qualitatively
    different aspects of diversity compared with existing metrics such as $n$-gram
    and BERT embedding diversity, demonstrating the value of structural diversity.
  prefs: []
  type: TYPE_NORMAL
- en: Example. We’ve provided an example of three poems generated using each our CoS
    algorithm and a standard random sampling strategy in Appendix [A](#A1 "Appendix
    A Examples ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification
    Prompting"). In the examples generated by random sampling, even though the words
    are different from one poem to another, the content and structure appear very
    consistent across all three samples. In constrast, the poems sampled using SoC
    exhibit significantly different structure and content. These kinds of differences
    occur in all of our domains.
  prefs: []
  type: TYPE_NORMAL
- en: Contributions. We propose a novel framework for studying structural diversity
    in text generation, where diversity is defined as the entropy of a user-defined
    mapping into a feature space. In addition, we propose chain-of-specification (CoS)
    prompting, an effective algorithm for improving structural diversity. Our experiments
    demonstrate that our approach can significantly improve structural diversity compared
    to several baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Related work. Several recent studies have investigated methods for quantifying
    and improving the diversity of text generated by LLMs. For example, Holtzman et al.
    ([2020](#bib.bib3)) proposes a set of metrics for evaluating the diversity of
    generated text, including self-BLEU, distinct $n$-grams, and entropy. They also
    introduced a new decoding method called nucleus sampling, which aims to improve
    diversity by sampling from the top-p portion of the probability distribution at
    each step. This approach works with any model but is white box, whereas our algorithm
    is black box.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. ([2018](#bib.bib13)) studies the trade-off between diversity and
    quality in text generation, using a combination of automatic metrics and human
    evaluations. They found that increasing the diversity of generated text often
    comes at the cost of reduced coherence and relevance. To address this issue, they
    proposed a new approach called DiversityGAN, which uses a generative adversarial
    network to generate diverse and high-quality text. This approach requires modifying
    the training process. To the best of our knowledge, the only black-box diversity
    improvement algorithm was developed and discussed in Ippolito et al. ([2019](#bib.bib4)),
    who suggested oversampling, clustering the samples into much fewer clusters using
    an approach such as K-Means, and then only taking the centroid from each cluster
    - we compare to this baseline in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Wang et al. ([2023](#bib.bib10)) investigates the diversity of text
    generated by GPT-4 OpenAI ([2022](#bib.bib6)) using metrics such as $n$-gram)
    diversity, it tends to exhibit lower “global” (i.e., semantic) diversity compared
    to human-written text. However, they do not study how to bridge this gap.
  prefs: []
  type: TYPE_NORMAL
- en: More broadly, there has been work studying diversity for reasoning (Naik et al.,
    [2023](#bib.bib5); Zhang et al., [2024](#bib.bib12)), and improving diversity
    of recommender systems Carraro and Bridge ([2024](#bib.bib1)).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Chain-of-Specification Prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem formulation. We assume given a mapping $\phi:\mathcal{X}\to\mathcal{S}$
    is binary and define metrics accordingly, but more general spaces can be used
    if the metrics are correspondingly modified. For example, in the poetry domain,
    a latent variable could be the sentiment of the poem (idyllic, melancholic, etc.),
    the rhyme scheme (regular, irregular, etc.), or the meter (iambic pentameter,
    free verse, etc.), encoded as one-hot variables.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to generate outputs $x\in\mathcal{X}$); then, we define diversity
    to be
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle D=\mathbb{E}_{x_{1},...,x_{k}\sim p}[F(\phi(x_{1}),...,\phi(x_{k}))].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Specification prompting. The idea in specification prompting is to first prompt
    the LLM to generate a random specification $s\in\mathcal{S}$. We provide examples
    of prompts in Appendix [C](#A3 "Appendix C Prompts ‣ Improving Structural Diversity
    of Blackbox LLMs via Chain-of-Specification Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-specification prompting. We can straightforwardly extend specification
    prompting by first generating high-level specifications, then generating low-level
    specifications, and then generating the text. In this case, we assume the user
    provides mappings $\phi_{j}:\mathcal{S}_{j}\to\mathcal{S}_{j-1}$ is empty).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we first draw samples $s_{m,i}\sim q_{m}$.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is particularly effective for domains where the desired output
    can be naturally decomposed into a hierarchy of specifications—e.g., for poetry
    generation, the high-level specifications could include poetry styles and themes,
    whereas the mid-level and low-level specifications could include more specific
    attributes such as emotional tone, imagery, or rhyme schemes.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets. We conducted experiments on three domains: poetry, coding challenge
    problem descriptions, and code solutions. The real human poetry dataset was taken
    from the Poetry Foundation’s collection of curated material Divy ([2021](#bib.bib2)).
    The coding challenge problem descriptions were non-overlapping problem descriptions
    from Project CodeNet Puri et al. ([2021](#bib.bib7)). The code solutions were
    also taken from CodeNet, with the requirement that only one solution be sampled
    per individual coding challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: In the poetry domain, features include spacing, rhyme, and meter; in the coding
    challenge problems domain, features include whether the problem uses matrix manipulation
    or whether it specifies memory constraints; and in the code solutions domain,
    features include whether the program uses recursion or whether it has input validation.
    We constructed 300 features for the poetry domain, 90 for the educational coding
    challenge domain, and 185 for the Python code domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d432c6e614d0d7fb9cf17d881a373778.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Results of Diversity Metrics for Poetry, Code, and Coding Problem
    Domains, respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Approaches. We compare the diversity of our approach to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random sampling: Sample (with positive temperature) from the LLM based on a
    default prompt that describes the target domain.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Human: Human-written text in the domain.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post-decoding clustering (PDC): A method proposed in Ippolito et al. ([2019](#bib.bib4))
    to improve diversity by generating a magnitude more outputs than needed and then
    taking the ones corresponding to the centroids of $K$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nucleus (top P) sampling: A method proposed by (Holtzman et al., [2020](#bib.bib3))
    that samples text from the most likely tokens that collectively have probability
    at least $p$; this strategy allows for diversity while truncating the less reliable
    tail of the distribution.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Metrics. To measure structural diversity, we use metrics that capture the coverage
    of the feature space $\mathcal{S}$. This assumption holds in our experiments since
    structural features tend to be difficult to satisfy. Then, we define *coverage*
    to be
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle C_{n}(S)=\frac{\sum_{c\in\mathcal{C}_{n}}\log(&#124;\{s\in
    S:c\subseteq s\}&#124;+1)}{\log(&#124;S&#124;+1)},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{C}_{n}$ of samples, most feature combinations are rare, meaning
    structural diversity is required to achieve high coverage.
  prefs: []
  type: TYPE_NORMAL
- en: We provide results for variations of our coverage metric, as well as standard
    $n$-gram and BERT embedding diversity metrics, in Appendix [B](#A2 "Appendix B
    Additional Experiments ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification
    Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: 'Language models. We evaluate four LLMs: ChatGPT-3.5-turbo at a temperature
    of 1.0, Llama3-70B-Instruct (Touvron et al., [2023](#bib.bib9)) at a temperature
    of 1.0, Llama3-70B-Instruct at a temperature of 0.7, and vanilla Llama3-70B (i.e.,
    not instruct tuned) at a temperature of 1.0\. We provide prompts in Appendix [C](#A3
    "Appendix C Prompts ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification
    Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: Samples. For each LLM, we took $k=300$ samples when bootstrapping with sub-sampling
    50 samples per iteration. Similarly, the human datasets were evaluated using 300
    random samples.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Improving
    Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting") shows
    results for each domain (column), LLM configuration (row), and approach (bar).
  prefs: []
  type: TYPE_NORMAL
- en: Comparison to baselines. CoS almost always outperforms all our baselines. The
    main exception is Llama without instruction tuning, which is expected sicne our
    approach relies on instruction following to be effective. In many cases, SS also
    performs well in both the poetry and problem domains, though it performs worse
    in the Python domain, likely because the specifications are more difficult to
    follow (e.g., using loops vs. recursion). In some cases, incorporating PDC and
    CoS (i.e., CoS+) produces a small additional benefit, though it can also sometimes
    reduce performance, indicating that its effectiveness is domain specific.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison to human. For the poetry and problem domains, CoS matches or even
    slightly exceeds the human dataset in terms of diversity, highlighting the effectiveness
    of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison on existing metrics. We show results on $n$-gram and BERT embedding
    diversity in Appendix [B](#A2 "Appendix B Additional Experiments ‣ Improving Structural
    Diversity of Blackbox LLMs via Chain-of-Specification Prompting"). The diversity
    of our approach is still high according to these metrics, though the baselines
    often perform similarly well; for instance, random sampling is competitive with
    our approach in many instances. One important observation is that the human dataset
    is sometimes significantly less diverse than the LLMs according to these metrics;
    in addition, for BERT embedding diversity, GPT-4 tends to be less diverse despite
    being a stronger model. These trends suggest that these diversity metrics represent
    qualitatively different forms of diversity compared to structural diversity. The
    specific kind of diversity may be domain dependent, but structural diversity has
    the key advantage that the user can tailored it to their domain.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison across models. In general, instruction tuning tends to improve diversity.
    The relationship between temperature and diversity is more complicated; generally,
    temperature increases diversity at the token level, but it can make it harder
    to satisfy structures leading to lower structural diversity. Finally, while GPT-4
    generally exhibits more diversity, especially when using CoS prompting, except
    in the poetry domain.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have proposed a novel framework for improving the structural diversity of
    black box LLMs where the user provides features encoding desired structural diversity
    properties, and then we use chain-of-specification prompting to automatically
    generate diverse outputs. Our experiments demonstrate that our framework is effective
    at improving structural diversity, Wwe also find evidence that structural diversity
    is qualitatively different from more traditional metrics such as $n$-gram diversity
    and diversity of BERT embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One limitation of our approach is that it requires the user to design the feature
    mapping from text to structures. While this mapping gives the user significant
    control over the kind of diversity they care about, it requires additional effort
    for each new domain where our technique is applied. For many domains, it may be
    possible to automate parts of this effort, for instance, by asking a strong model
    such as GPT-4 to identify reasonable structural features in new domains. In addition,
    generating chains of specifications requires sampling significantly more tokens
    compared to random sampling. The benefit is that our work can be generally applicable
    to the black box setting. In the white box LLM setting, finetuning techniques
    might enhance diversity without the need to sample additional tokens.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Carraro and Bridge (2024) Diego Carraro and Derek Bridge. 2024. Enhancing recommendation
    diversity by re-ranking with large language models. *arXiv preprint arXiv:2401.11506*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divy (2021) TG Divy. 2021. [Poetry foundation poems](https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems).
    Kaggle dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
    Choi. 2020. The curious case of neural text degeneration. In *International Conference
    on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ippolito et al. (2019) Daphne Ippolito, Reno Kriz, João Sedoc, Maria Kustikova,
    and Chris Callison-Burch. 2019. [Comparison of diverse decoding methods from conditional
    language models](https://doi.org/10.18653/v1/P19-1365). In *Proceedings of the
    57th Annual Meeting of the Association for Computational Linguistics*, pages 3752–3762,
    Florence, Italy. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naik et al. (2023) Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid
    Palangi, and Besmira Nushi. 2023. Diversity of thought improves reasoning abilities
    of large language models. *arXiv preprint arXiv:2310.07088*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2022) OpenAI. 2022. [Chatgpt: A large language model for conversational
    ai](https://openai.com/blog/chatgpt). *OpenAI Blog*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puri et al. (2021) Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo
    Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey
    Decker, et al. 2021. Codenet: A large-scale ai for code dataset for learning a
    diversity of coding tasks. *arXiv preprint arXiv:2105.12655*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tevet and Berant (2021) Guy Tevet and Jonathan Berant. 2021. [Evaluating the
    evaluation of diversity in natural language generation](https://doi.org/10.18653/v1/2021.eacl-main.25).
    In *Proceedings of the 16th Conference of the European Chapter of the Association
    for Computational Linguistics: Main Volume*, pages 326–346, Online. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Yijun Wang, Jingxuan Wang, Jiahong Liu, He Lin, Jiawei Li,
    Jipeng Zhang, Fei Wei, Bing Qin, Ting Liu, Tak-Lam Lam, et al. 2023. Is chatgpt
    a good translator? a preliminary study. *arXiv preprint arXiv:2301.08745*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2024) Tianhui Zhang, Bei Peng, and Danushka Bollegala. 2024. Improving
    diversity of commonsense generation by large language models via in-context learning.
    *arXiv preprint arXiv:2404.16807*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao,
    Dinghan Shen, and Lawrence Carin. 2018. Diversitygan: Diversity-aware text generation.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 3360–3369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are examples generated by random sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/b68eff9fbc42fb55bb332708badbfd10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are examples generated by CoS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/c19ee2539abd2165a2bd06459f2e509e.png)'
  prefs: []
  type: TYPE_IMG
- en: Appendix B Additional Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Additional Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We consider the following metrics for measuring diversity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coverage. The sum of logarithmically weighted feature combination counts, normalized
    by the maximum possible weighted count:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C_{n}(S)=\frac{\sum_{c\in\mathcal{C}_{n}}\log(&#124;\{s\in S:c\subseteq
    s\}&#124;+1)}{\log(&#124;S&#124;+1)}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{C}_{n}$ features. This metric measures the extent to which the
    samples cover the range of possible improbable structural properties, ensuring
    that the generated text exhibits a diverse set of rare features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weighted surprisal. The average surprisal of each feature combination, weighted
    by its probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This metric quantifies the unexpectedness of the observed combinations of improbable
    structural properties, giving higher weight to rare feature combinations. It ensures
    that the generated text contains surprising and informative structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosted Jaccard diversity. The sum of Jaccard distances between all pairs of
    feature combinations, weighted by their sizes and normalized by the total number
    of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This metric measures the dissimilarity between pairs of feature combinations,
    giving higher weight to larger combinations. It ensures that the generated samples
    have distinct sets of improbable structural properties, promoting diversity in
    the text’s rare features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dice Diversity. The average Dice distance between all pairs of feature combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This metric quantifies the dissimilarity between pairs of feature combinations
    using the Dice coefficient, which emphasizes the presence of rare features in
    both combinations. It ensures that the generated text has a diverse set of improbable
    structural properties that are not frequently shared between samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'One-way inclusion diversity. The average one-way inclusion coefficient between
    all pairs of feature combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This metric measures the dissimilarity between pairs of feature combinations
    using the one-way inclusion coefficient, which quantifies the proportion of rare
    features in one combination that are not present in the other. It ensures that
    the generated text has a diverse set of improbable structural properties that
    are not subsumed by other samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weighted overlap diversity: The average overlap coefficient between all pairs
    of feature combinations, weighted by their sizes and normalized by the total number
    of combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: This metric quantifies the similarity between pairs of feature combinations
    using the overlap coefficient, which measures the proportion of shared rare features.
    By subtracting this metric from 1, we obtain a diversity measure that ensures
    the generated text has a diverse set of improbable structural properties with
    minimal overlap between samples.
  prefs: []
  type: TYPE_NORMAL
- en: $n$, it is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D=-\sum_{i=1}^{M}p_{i}\log p_{i}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $p_{i}$ is the total number of unique 4-grams in the documents. This metric
    quantifies the unpredictability of the text based on the variety of its 4-gram
    constructs, with higher values indicating more diverse generations.
  prefs: []
  type: TYPE_NORMAL
- en: BERT Embedding Diversity. We measure the diversity of text documents based on
    the variability in their BERT embeddings, as per Tevet and Berant ([2021](#bib.bib8)).
    This approach utilizes the pre-trained BERT model to convert textual data into
    high-dimensional vectors, where each vector represents the semantic content of
    a text. In particular, the diversity is the pairwise cosine distances between
    the BERT embeddings of all generated texts. First, each text is transformed into
    an embedding by averaging the output vectors (i.e., BERT’s last hidden layer)
    of all tokens in it. Then, we compute the cosine distances between every pair
    of embeddings to form a distance matrix. Finally, the BERT diversity is the mean
    of all of these pairwise distances.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Additional Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We show results for each of the additional metrics in Figures [3](#A2.F3 "Figure
    3 ‣ B.2 Additional Results ‣ Appendix B Additional Experiments ‣ Improving Structural
    Diversity of Blackbox LLMs via Chain-of-Specification Prompting"), [4](#A2.F4
    "Figure 4 ‣ B.2 Additional Results ‣ Appendix B Additional Experiments ‣ Improving
    Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting"),
    [5](#A2.F5 "Figure 5 ‣ B.2 Additional Results ‣ Appendix B Additional Experiments
    ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting"),
    [6](#A2.F6 "Figure 6 ‣ B.2 Additional Results ‣ Appendix B Additional Experiments
    ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting"),
    [7](#A2.F7 "Figure 7 ‣ B.2 Additional Results ‣ Appendix B Additional Experiments
    ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting"),
    [8](#A2.F8 "Figure 8 ‣ B.2 Additional Results ‣ Appendix B Additional Experiments
    ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting"),
    [9](#A2.F9 "Figure 9 ‣ B.2 Additional Results ‣ Appendix B Additional Experiments
    ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting"),
    & [10](#A2.F10 "Figure 10 ‣ B.2 Additional Results ‣ Appendix B Additional Experiments
    ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/075e37a30e3f268130e7d9ed654c9b80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Results of coverage diversity for Poetry, Code, and Coding Problem
    Domains, respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ae49c1b76b665521a63c9fc888ef019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Results of weighted surprisal diversity for Poetry, Code, and Coding
    Problem Domains, respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee923dd6dffb47481fe22257435263d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Results of boosted Jaccard diversity for Poetry, Code, and Coding
    Problem Domains, respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0aa4d8c9790229191a4bd37cc7b9e7db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Results of Dice diversity for Poetry, Code, and Coding Problem Domains,
    respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23c4dfe615558bf67484aac9e1e3e3af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Results of one way inclusion diversity for Poetry, Code, and Coding
    Problem Domains, respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/823f5ea89efe09922025ec05439b8090.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Results of weighted overlap diversity for Poetry, Code, and Coding
    Problem Domains, respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/273a7cf26b077b243d5a9a39c2223291.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Results of $n$-gram diversity for Poetry, Code, and Coding Problem
    Domains, respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a35113145f8b0be632ffb9d3bc6563c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Results of BERT diversity for Poetry, Code, and Coding Problem Domains,
    respectively. Higher = Better.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We show the prompts used for each of our three domains in Figures [11](#A3.F11
    "Figure 11 ‣ Appendix C Prompts ‣ Improving Structural Diversity of Blackbox LLMs
    via Chain-of-Specification Prompting"), [13](#A3.F13 "Figure 13 ‣ Appendix C Prompts
    ‣ Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting"),
    & [12](#A3.F12 "Figure 12 ‣ Appendix C Prompts ‣ Improving Structural Diversity
    of Blackbox LLMs via Chain-of-Specification Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Prompt: Come up with an interesting and original prompt for a poem. Return
    the prompt only; no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Style: Suggest a poetic style that fits and does not at all contradict,
    but rather complements and is integrated into this prompt prompt. Describe what
    the poem does or doesn’t do relative to ’traditional’ poetry, and how this relates
    to the prompt. Don’t limit yourself in any way - consider styles from Shakespeare
    to contemporary eco-concrete-poetry and modernist whitespace, concrete, and fragmentation
    poetry, and from all over the world. Consider influences as varied as John Dunne
    and Charles Bernstein or ee. Cummings. Or, interpolate between existing styles,
    or invent a new poetic style and describe it. Note that you should prioritize
    finding a way to satisfy the prompt prompt over all other constraints, and attempt
    to strictly enhance that prompt. Just return the description of the poetic style
    - no commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Theme: Given the poetic style ’style’ (which you may or may not be familiar
    with) and this prompt: prompt, suggest a suitable theme, and elaborate how it
    will be presented in relation to the form in detail. Note that you should prioritize
    finding a way to satisfy the prompt prompt over all other constraint, and attempt
    to strictly enhance that prompt. Return only the theme, no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Mood: Given the theme ’theme’ and the style style and the prompt prompt,
    suggest a congruent (yet not necessarily the most obvious) emotional arc to the
    poem. Note that you should prioritize finding a way to satisfy the prompt prompt
    over all other constraint. Return only the emotional arc and how it fits with
    the prompt as a paragraph, no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Imagery: Given the theme ’theme’ and the style ’style’ and the emotional
    arc ’mood’ and the prompt prompt, provide one possible type of imagery to include
    in the poem. Note that you should prioritize finding a way to satisfy the prompt
    prompt over all other constraint. Return only the imagery, no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Poem: Compose a poem with this prompt: prompt in the form of form, exploring
    the theme ’theme’, conveying a ’mood’ emotional arc, and incorporating this imagery:
    ’imagery’. Note that you should prioritize finding a way to satisfy the prompt
    prompt over all other constraint. Return only the poem, no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Prompts used for CoS+PDC sampling of poems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Types: Write an example input-output type pair for a python programming
    challenge. Return only the input type and output type; no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Goal: Write an educational goal for a python programming challenge. You
    are constrained in one way: The input-output types must be types.Some examples
    might be teaching a particular lesson about recursion, or teaching about the importance
    of programming efficiently, but any educational goal within computer science could
    work. Return the educational goal description as a paragraph only; no prefix or
    commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM CoS Program: Write a python program which satisfies the following educational
    goal: goal and has the following input-output-types: types. Return the python
    program only; no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM SS Program: Write a python program which satisfies the following input-output
    type: types. Return the python code alone; no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Random Program: Write a 100-line python program. Return the code only;
    no prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: Prompts used for CoS+PDC/SS/Random sampling of Python programs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Goal 1: Write an educational goal for a programming challenge. Some examples
    might be teaching a particular lesson about recursion, or teaching about the importance
    of programming efficiently, but any educational goal within computer science could
    work. Return the educational goal description as a paragraph only; no prefix or
    commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Goal 2: Conditioned on wanting to teach about Goal 1\. Write a secondary
    educational goal you might have for a coding challenge. Return a paragraph-long
    description of what you’re trying to achieve pedagogically, in addition to: Goal
    1\. Return it as a paragraph without prefix or commentary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM CoS problem description: Write an example coding challenge problem which
    could work for a programming teacher who wants to teach primarily about the following:
    Goal 1 and secondarily about the following: Goal 2\. Make it as descriptive as
    possible, including a description of the problem, example input-output, and any
    additional information that may be needed. Note that it should be programming-language
    agnostic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM SS Problem Description: Write an example coding challenge problem which
    could work for a programming teacher who wants to teach about the following: Goal
    1\. Make it as descriptive as possible, including a description of the problem,
    example input-output, and any additional information that may be needed. Note
    that it should be programming-language agnostic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Random Problem Description: Write an example educational coding challenge
    problem. Make it as descriptive as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13: Prompts used for CoS+PDC/SS/Random sampling of coding challenge
    problem descriptions.'
  prefs: []
  type: TYPE_NORMAL
