- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:34'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized Imitation Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11359](https://ar5iv.labs.arxiv.org/html/2309.11359)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{textblock*}'
  prefs: []
  type: TYPE_NORMAL
- en: (2cm,0.5cm)This work has been submitted to the IEEE for possible publication.
    Copyright may be transferred without notice, after which this version may no longer
    be accessible.
  prefs: []
  type: TYPE_NORMAL
- en: Jingkai Sun^(1,∗), Qiang Zhang^(1,∗), Yiqun Duan², Xiaoyang Jiang¹, Chong Cheng¹
    and Renjing Xu^(1,†) ^∗ are equal contributors, ^† is the corresponding author¹The
    authors are with The Hong Kong University of Science and Technology (Guangzhou),
    China. {jsun444, qzhang749, ccheng735}@connect.hkust-gz.edu.cn, jxxxxxyGOAT@gmail.com,
    renjingxu@ust.hk²The author is with Human $|$ Centric AI Centre, Australia Artificial
    Intelligence Institute University of Technology Sydney 2007 Ultimo Australia.
    yiqun.duan@student.uts.edu.au
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, reinforcement learning and imitation learning have shown great
    potential for controlling humanoid robots’ motion. However, these methods typically
    create simulation environments and rewards for specific tasks, resulting in the
    requirements of multiple policies and limited capabilities for tackling complex
    and unknown tasks. To overcome these issues, we present a novel approach that
    combines adversarial imitation learning with large language models (LLMs). This
    innovative method enables the agent to learn reusable skills with a single policy
    and solve zero-shot tasks under the guidance of LLMs. In particular, we utilize
    the LLM as a strategic planner for applying previously learned skills to novel
    tasks through the comprehension of task-specific prompts. This empowers the robot
    to perform the specified actions in a sequence. To improve our model, we incorporate
    codebook-based vector quantization, allowing the agent to generate suitable actions
    in response to unseen textual commands from LLMs. Furthermore, we design general
    reward functions that consider the distinct motion features of humanoid robots,
    ensuring the agent imitates the motion data while maintaining goal orientation
    without additional guiding direction approaches or policies. To the best of our
    knowledge, this is the first framework that controls humanoid robots using a single
    learning policy network and LLM as a planner. Extensive experiments demonstrate
    that our method exhibits efficient and adaptive ability in complicated motion
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Advances in humanoid robotics require the development of complex, adaptable,
    and efficient control policies. These policies enable humanoid robots to operate
    effectively in human-centered environments, thereby aiding in the successful execution
    of a diverse range of tasks. Despite significant advancements in the domain of
    humanoid robotics, a substantial disparity exists between the theoretical potential
    of these systems and their practical efficacy in executing complex tasks. In this
    paper, we propose a novel framework to combine the capabilities of Generative
    Adversarial Imitation Learning (GAIL)[[1](#bib.bib1)] action data imitation with
    the planning capabilities of the large language models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7cae113fb9e479ee0302faa184784805.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Our framework enables robots to combine the skills obtained from
    imitation learning with the planning capabilities of LLMs to accomplish complex
    tasks. For example, with known obstacles as well as its own coordinates, the robot
    accomplishes the task of hitting a target after avoiding obstacles by scheduling
    reusable skills.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The GAIL framework consists of two main components: a policy and a discriminator
    [[2](#bib.bib2)] and trains policy to perform tasks by imitating expert behavior.
    The policy interacts with the environment to generate action based on the current
    state. The discriminator aims to distinguish between the trajectories generated
    by the expert and those produced by the learning policy. However, merely imitating
    unlabeled action data limits the learned policy to generating specific actions
    in particular situations, failing to integrate effectively with high-level policies
    such as Adversarial Skill Embeddings (ASE)[[3](#bib.bib3)]. ASE can only accomplish
    a specific task by training the high-level policy to match the skills of the low-level
    network in the process of training. To address this limitation, we employ motion
    data in conjunction with its associated textual labels to generate latent vectors[[4](#bib.bib4)].
    These latent vectors are subsequently integrated into the imitation learning network
    as conditions and skills. These enable the policy to produce contextually appropriate
    actions when fed with specific skills as input. Leveraging the planning capabilities
    of LLMs, our method enables the execution of a diverse array of tasks without
    requiring the development of task-specific high-level networks. This is achieved
    through the utilization of reusable skills, thereby enhancing the system’s adaptability
    and efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing the LLM as a skills planner, our approach enables the humanoid
    to complete tasks autonomously based on problem descriptions. To guarantee that
    the robot receives actionable and concise instructions, we refine the complexity
    of the LLM’s outputs by using Contrastive Language-Image Pre-Training (CLIP)[[5](#bib.bib5)]
    text encoder and an integrated codebook-based vector quantization. This enhances
    the robustness of the system concerning the randomness of LLMs-generated outputs.
    The codebook consists of skills generated from the text labels encoded during
    training. It is used to correspond similar commands output by the larger model
    to specific skills. Additionally, we design a general reward consisting of root-oriented
    and hip-oriented rewards to ensure that the policy trained by imitation learning
    can execute specific actions along the given directions rather than local coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our approach focuses on enhancing the integration of GAIL’s dynamic control
    abilities with the cognitive capabilities of LLMs. In summary, the planning results
    are shown in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Prompt, Plan, Perform:
    LLM-based Humanoid Control via Quantized Imitation Learning") and our contributions
    include: (I) To the best of our knowledge, we propose a framework that employs
    a single policy network with LLMs for planning to control humanoid robots to accomplish
    tasks for the first time. (II) By introducing codebook-based vector quantization,
    our framework is equipped to handle a variety of similar but unseen instruction
    sets, enhancing robustness to LLMs or human commands. (III) By designing a general
    reward for the hip joints, we implemented a single policy to control the direction
    of robots and perform most of the motion.'
  prefs: []
  type: TYPE_NORMAL
- en: II Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Large Language Model for Robotics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, the rise of large-scale language models like ChatGPT[[6](#bib.bib6)]
    and Llama2[[7](#bib.bib7)] has opened new doors for natural language understanding
    and generation. Thus, there are various prior works have explored using the large
    language model for task planning [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]
    or robot manipulation[[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15)]. SayCan[[16](#bib.bib16)] combines value functions associated
    with low-level skills with the large language models, thereby establishing a bridge
    between rich linguistic understanding and real-world physical interactions. Inner
    Monologue[[8](#bib.bib8)] studies the impact of human feedback on LLMs, including
    scenario descriptions and human-computer interactions. Their findings indicate
    that detailed examples and closed-loop feedback can improve the performance of
    LLMs. RT-2[[17](#bib.bib17)] employs Internet-scale data to train vision-language
    models, enabling their application as end-to-end robotic control systems. This
    model is not merely confined to generating actions based on observations; it also
    assimilates a comprehensive understanding of the human world derived from online
    sources. DoReMi[[18](#bib.bib18)]facilitates real-time error identification and
    recovery mechanisms during the execution of plans generated by LLMs on humanoid
    robots.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Adversarial Imitation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imitation learning (IL) aims to solve complex tasks where learning a policy
    from scratch is rather challenging, by learning useful skills or behaviors from
    expert demonstrations in auto-driving[[19](#bib.bib19), [20](#bib.bib20)] or robotics[[21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)]. GAIL[[1](#bib.bib1)] first introduces a discriminator
    network to provide additional reward signals to the policy. It makes the policy
    more consistent with the distribution of expert data. Thus, GAIL addresses some
    of the limitations of behavior cloning [[24](#bib.bib24)] (such as compounding
    error caused by covariate shift) by learning an objective function to measure
    the similarity between the policy and the expert data. Adversarial Motion Prior
    (AMP)[[25](#bib.bib25)] learns to imitate different skills from the reference
    data in order to fulfill high-level tasks. But in AMP, a single policy is typically
    specialized for a limited set of tasks, because the performance of the agent is
    strongly correlated with the reward function designed during training. [[26](#bib.bib26)]
    realizes a single policy with controllable skill sets from unlabeled datasets
    containing diverse motion by using a skill discriminator. Based on AMP, ASE[[3](#bib.bib3)]
    utilize hierarchical model[[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30)] to make low-level policy learn reusable skills form motion data
    in latent space. Then it further trains many high-level policies to solve downstream
    tasks by invoking a low-level policy and designing different reward functions.
    However, this method still requires many reward functions for specific missions
    and falls short in its capacity to handle tasks that are not encountered during
    training. [[31](#bib.bib31)] and [[32](#bib.bib32)] employ textual labels and
    motion data to convert the skills learned through their respective policies into
    explicit variables. These variables can then be manipulated by humans through
    syntax trees and finite state machines to facilitate task completion. However,
    these methods rely on multiple policies and involve considerable manual labor.
    Based on the above shortcomings, we introduce a framework that incorporates large
    language models and general directional rewards. This enables the execution of
    unseen tasks using a single policy, significantly reducing the manual work required.
  prefs: []
  type: TYPE_NORMAL
- en: III Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In contrast to GAIL, our framework is built upon Conditional Adversarial Latent
    Models (CALM)[[32](#bib.bib32)], using a conditional discriminator[[33](#bib.bib33)]
    to enable it to match a specific latent to an action. Both GAIL and CALM are typically
    defined in the context of a discrete-time Markov Decision Process (MDP). MDP is
    defined as a tuple $(\mathcal{S},\mathcal{A},\mathcal{R},p,\gamma)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textnormal{arg}\max_{\theta}\mathbb{E}_{(s_{t},a_{t})\sim p_{\theta}(s_{t},a_{t})}\left[\sum_{t=0}^{T-1}\gamma^{t}r_{t}\right]$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where T denotes the time horizon of MDP.
  prefs: []
  type: TYPE_NORMAL
- en: In the traditional reinforcement learning[[34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36)], the reward functions need to be manually designed specifically
    for different tasks, but this is difficult for learning motor skills from expert
    demonstrations. Imitation learning emerges as a viable alternative for situations
    where defining a deterministic reward function is impractical, yet the task can
    be accomplished by imitating expert demonstrations. GAIL tackles some limitations
    of normal imitation learning such as behavior cloning by introducing a discriminator
    $\mathcal{D}(s_{t},a_{t})$ in order to optimize the learning objective which requires
    the state-action pairs of agents. To extend the learning objective to datasets
    that only offer the state of the agent, we follow [[37](#bib.bib37)] to transfer
    it to the state and the next state pair instead of the original state-action pair.
    We introduce the Jensen-Shannon divergence[[38](#bib.bib38)] as the objective
    of the discriminator,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textnormal{arg}\min_{\mathcal{D}}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\mathbb{E}_{d^{\pi}(s_{t},s_{t+1})}\left[\textnormal{log}(1-\mathcal{D}(s_{t},s_{t+1}))\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $d^{\mathcal{M}}(s_{t},a_{t})$ denote the state-action pair distribution
    of reference motion dataset and policy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39af57ad1b175dcc1b45546ec730e006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overview of our proposed system. Motion captions with the same semantics
    are first clustered together by fine-tuning the CLIP Text encoder. Subsequently,
    the output text features are fed into the policy training by codebook-based vector
    quantization. Our pre-training system feeds a reference dataset defining the desired
    underlying motion and its corresponding text labels (marked in red in the figure)
    into the training discriminator to provide discriminator rewards for policy training.
    The discriminator reward is then combined with the task reward for controlling
    orientation to train a policy that allows the robot to execute the demonstrated
    motion in the specified orientation. These two processes are not trained at the
    same time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d7502e0edf2bd9c4d4502df9dd90290.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Evaluation system overview. The Human definitions are fed into the
    LLMs as prompts. The LLMs output the sequence of actions and the target orientation
    of each action. The text of the action sequence is input to the CLIP Text encoder,
    and the target orientation is input to the policy as an observation concatenated
    with the observation given by the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: IV APPROACH
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As illustrated in Figure [2](#S3.F2 "Figure 2 ‣ III Background ‣ Prompt, Plan,
    Perform: LLM-based Humanoid Control via Quantized Imitation Learning"), the architecture
    of our proposed method is designed to execute tasks via a single policy framework.
    Our architecture is composed of three primary components: an adaptive language-based
    skill motion policy, a CLIP-based adaptive language discrete encoder, and the
    large language model planner.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Adaptive Language-based Skill Motion Policy Pre-train
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our motion policy pre-train method consists of three main parts: a skill encoder,
    and a discriminator, a reinforcement learning policy. The skill encoder is used
    to reduce the dimension of the text features output by CLIP-based adaptive language
    discrete encoder to generate latent vectors. For generating close and uniform
    skills of the same kinds of actions on the latent space, we introduce alignment
    loss and uniformity loss to train the skill encoder like [[32](#bib.bib32)]. The
    discriminator is consistent with the description in the previous section. In our
    framework, each label of the motion dataset is encoded as a latent vector $z$
    to the discriminator and the motion policy as conditions and skills. The discriminator
    outputs the degree of similarity between the state transitions generated by policy
    and the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textnormal{arg}\min_{\mathcal{D}}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\omega_{gp}\mathbb{E}_{d^{\mathcal{M}}({s}_{t},{s}_{t+1})}\left[\&#124;\mathbf{\triangledown_{\alpha}\mathcal{D}(\alpha)}\&#124;^{2}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\omega_{gp}$. During each iteration, a randomly selected type of motion
    is input into the discriminator. In this setup, the agent is rewarded more favorably
    only if its executed actions closely align with the selected motion type.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike CALM, our motion policy is trained further by the combination of two
    rewards, the discriminator reward and the task reward. This is because the policy
    trained through imitation learning performs motions in their local coordinates.
    However, real-world task solutions require the humanoid robot to move in a specified
    direction, and even upper body action (e.g., sword swinging) needs to determine
    the orientation. Therefore, the high-level policy is required to specify directions
    for the low-level policy in the CALM framework. This leads to additional network
    training and still does not represent the direction of upper body action direction
    since its network rewards whole body movement direction. Thus, we design a general
    reward that incorporates the physical mechanism characteristics of humanoid robots.
    This is aimed at ensuring that the policy trained through imitation learning can
    move or perform specific actions along specific directions. The general reward
    consists of three items, root direction (also called pelvis), left hip direction,
    and right hip direction. The reward is formulated as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\omega_{1}$ are the direction vectors of the root, left hip, and right
    hip or humanoid robot. The total reward is,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\omega_{task}$ is the the weight of discriminator reward. We find the
    direction of the root infers the upper body action direction, and the average
    of two hips direction infers the movements of the whole body. Thus, we design
    the general reward to ensure that the whole body moves forward (except for movements
    to the left or right in the local coordinates) and the upper body’s action in
    a specific direction.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B CLIP-based Adaptive Language Discrete Encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To encode natural language commands(also called captions), our system employs
    a pre-trained text encoder from the CLIP model. This encoder transforms the input
    commands into a 512-dimensional vector space. Leveraging an extensive dataset
    comprising action-related text, we fine-tune this encoder to optimize its performance
    specifically for our target application. In the fine-tuning phase, we utilize
    Mean Squared Error (MSE) as a loss function to make the encoded latent vectors
    obtained from different labels corresponding to the same motion, closer together
    in the latent space. The loss function is formulated as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $Enc_{T}$ as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle k$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle f_{d}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle e_{k}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $f=Enc_{T}(text)$ to the label feature generated by fine-tuning the CLIP
    text encoder. As such, fine-tuning the CLIP text encoder and vector quantization
    of the text features allows the agent to handle captions that are unseen and reduces
    the memory of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Large Language Model Motion Planner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we explore the possibility of extracting action planning knowledge
    from pre-trained language models without further training, simply by using the
    prompt. We include descriptions of the robot itself, the task scenario, the action
    skills it can perform, and some information about the output formatting examples
    and constraints as prompts. The pipeline of the evaluation system with LLMs is
    shown in Figure [3](#S3.F3 "Figure 3 ‣ III Background ‣ Prompt, Plan, Perform:
    LLM-based Humanoid Control via Quantized Imitation Learning"). Furthermore, the
    most important part of the prompt is an example of accomplishing a simple task
    (e.g., moving in a specified direction). Otherwise, LLMs need to interact with
    the user through subsequent interactions in order to output the action sequences
    correctly. The output commands of LLMs consist of two parts, the textual commands
    and coordinate orientation. Textual commands are sequences of reusable action
    skills that a robot follows to perform an action to accomplish a specific task.
    The coordinate orientation indicates the direction of the target when the robot
    performs the action. The former is fed into the policy via a fine-tuned CLIP-based
    text encoder, while the latter is added to the observation vector to control the
    robot. In the actual completion of the task, we take the goal, obstacles, and
    global coordinates of the robot as known information. Under this assumption, actions
    related to the robot’s movement are judged to be completed or not in terms of
    the distance to the goal position. For actions unrelated to movement, the time
    required to execute the motion serves as the determining factor for action completion.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b283c596ce5b234ed5b5c4b28c35c18a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Initialization of an obstacle avoidance attack task. The gray rectangle
    represents the attack target, the blue markers are the middle path point of the
    LLMs plan, the red is the obstacle, and the green line points to the current target
    orientation of the robot.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D System Architecture Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our robot is a simplified model consisting of spheres, boxes, and cylinders.
    The whole robot is made up of three joints in the arms, three joints in the legs,
    a joint in the waist, and a joint in the neck. All of these joints except the
    knee and elbow have three degrees of freedom. The action space of policy is the
    target position of each joint. The state space consists of the height of the root
    from the ground and positions and velocities, rotation, and angular velocities
    of the other bodies in the robot’s local coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: V Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we conduct experiments involving humanoid robot tasks. These
    tasks include controlling the robot to perform diverse actions through natural
    language, incorporating large language models for navigation, and knocking down
    objects. Specifically, we show that our CLIP-based adaptive language discrete
    encoder can handle diverse natural language commands. Additionally, the general
    reward structure we introduce proves effective in directing the robot’s movement
    along a specified direction, without influencing the quality of imitation from
    the motion dataset. We collect data and train on a single A100 GPU on 4096 Isaac
    Gym[[39](#bib.bib39)] environments in parallel. And our reinforcement learning
    algorithm is Proximal Policy Optimization (PPO)[[40](#bib.bib40)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d7da87f5018d39f756e9836a7470446e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The movements and their target orientations for each step in completing
    the obstacle avoidance attack task. The example shows the initial position of
    the robot (0,0), obstacle position (3,0), and target position (6,0). The obstacle
    is a rectangle with a length of 1.2m in the x-axis direction and a length of 1.8m
    in the y-axis direction. The blue markers are the waypoints of the LLMs plan,
    the red is the obstacle, and the green line points to the current target orientation
    of the robot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Comparison of the abilities of different methods'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Abilities |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; unseen &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; task &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; single &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; input diverse &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; language &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; no label &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; request &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| AMP[[25](#bib.bib25)] |  | ✓ |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ASE[[3](#bib.bib3)] |  |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| PADL[[31](#bib.bib31)] | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CALM[[32](#bib.bib32)] | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | ✓ | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: V-A Solve Downstream Tasks with LLMs Planner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the process of imitation learning, we feed the position, velocity, height
    of root, rotation relative to the root of each body, the velocity of the joints,
    and the position of the key bodies (right hand, left hand, right foot, left foot,
    sword, shield) into discriminator. This input strategy enables the policy to produce
    actions that utilize fewer dimensions of motion data. In order to demonstrate
    the motion policy uses reusable skills to accomplish the zero-shot tasks through
    the LLMs planner, we design a scenario where the robot must navigate around obstacles
    to reach and knock down a target. As shown in Figure [4](#S4.F4 "Figure 4 ‣ IV-C
    Large Language Model Motion Planner ‣ IV APPROACH ‣ Prompt, Plan, Perform: LLM-based
    Humanoid Control via Quantized Imitation Learning"), the robot is initially oriented
    squarely towards the target and needs to bypass the red obstacle in the middle
    and eventually knock down the target once it reaches the vicinity of the target.
    The LLMs planner, prompted by the pre-input position of the obstacle and other
    constraints, outputs the robot’s actions and corresponding goal orientations.
    The action type output from the LLMs is fed into the encoder, and the orientation
    data is incorporated into the observation vector. As the results are shown in
    Figure [5](#S5.F5 "Figure 5 ‣ V Experiments ‣ Prompt, Plan, Perform: LLM-based
    Humanoid Control via Quantized Imitation Learning"), the complex unseen task can
    be solved step by step through our proposed method by combining a single reinforcement
    learning policy with the large language model planner. Table [I](#S5.T1 "TABLE
    I ‣ V Experiments ‣ Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized
    Imitation Learning") shows the comparison of our approach with other methods.
    In contrast to ASE, our method does not need to train high-level policies to accomplish
    specific tasks. Compared to CALM, for scenarios with randomly generated obstacle
    locations and sizes, our approach does not need to manually design the path points
    during obstacle avoidance and can automate more complex tasks without the requirement
    of designing finite state machines.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f938ef9c0c1d14d24288b2ab1479423.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparisons of Robot Position and Orientation Error (calculated by
    root direction) during running forward and dodging backward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Comparison of the accuracy of different CLIP Text Encoders'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Motion class |'
  prefs: []
  type: TYPE_TB
- en: '| slash | run | walk | turn | dodge | shield |'
  prefs: []
  type: TYPE_TB
- en: '| finetune(our) | 82.45% | 90.21% | 87.93% | 91.93% | 88.13% | 79.43% |'
  prefs: []
  type: TYPE_TB
- en: '| w/o finetune | 46.23% | 53.78% | 54.34% | 61.93% | 51.20% | 45.21% |'
  prefs: []
  type: TYPE_TB
- en: V-B Evaluate Adaptive Language Encoder by Unseen Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the second experiment, we investigate the ability of our method to generalize
    to unseen commands by using pre-trained language text encoder and codebook-based
    vector quantization. Specifically, for each representative motion, we test our
    method with rephrased commands, which are unseen during training. Our test captions
    consist of three parts: the verbs (e.g., ”walk”), adverbs describing the direction
    (e.g., ”forward”), and adverbs describing the features (e.g., ”carefully”). Then
    we generate a large number of related synonyms and combine them randomly as our
    train dataset and test dataset for fine-tuning the CLIP text encoder. In Table [II](#S5.T2
    "TABLE II ‣ V-A Solve Downstream Tasks with LLMs Planner ‣ V Experiments ‣ Prompt,
    Plan, Perform: LLM-based Humanoid Control via Quantized Imitation Learning"),
    we show the accuracy of generating appropriate actions in response to unseen commands
    and also compare the results with the model without fine-tuning. To illustrate,
    during the policy training phase, a familiar text command such as ”run forward”
    might be used, while an equivalent but previously unseen command could be ”rush
    ahead rapidly”. In this experiment, all outputs from the text encoder are fed
    into codebook-based vector quantization. This step is essential as the policy
    is designed to manage only those captions that have been encountered during its
    training phase. To evaluate the system’s robustness against novel input, we introduce
    unseen synonymous captions as test queries. The accuracy rate is calculated based
    on the network’s ability to effectively map previously unseen commands to captions
    that were encountered during the policy training phase. According to our experiments,
    our approach can handle unseen commands. This helps to enhance the robustness
    of our approach, as the output of the large language model is stochastic to some
    extent, and thus the output of natural language commands needs to be limited to
    what is acceptable for the model. Moreover, this is also important for future
    applications of such an approach to direct human use of language control, where
    user-input commands are much more uncontrollable.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Comparison of General Task Rewards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we evaluate the efficacy of three distinct reward structures
    for controlling the robot’s orientation. The first reward structure only focuses
    on the orientation of the robot’s root, the second emphasizes the orientation
    of the robot’s direction of movement, and the third incorporates the orientation
    of both the robot’s root and its two hips. We set the desired direction of the
    robot along the x-axis. Figure [6](#S5.F6 "Figure 6 ‣ V-A Solve Downstream Tasks
    with LLMs Planner ‣ V Experiments ‣ Prompt, Plan, Perform: LLM-based Humanoid
    Control via Quantized Imitation Learning") illustrates the average error of the
    robot’s movement concerning the direction of the target during ”dodge backward”
    and “run forward”. Our experiments suggest that rewarding solely based on root
    orientation does not offer effective control over the robot’s forward movement.
    Utilizing a reward structure that considers both the robot’s direction of movement
    and root orientation results in a conflict that inhibits the successful execution
    of the ”backward ducking” action. Furthermore, for upper-body motions that do
    not involve overall robot movement, a movement-based reward causes the robot to
    persist in moving forward. In contrast, our general reward scheme allows the robot
    to maintain its initial position during upper-body actions. Therefore, we propose
    a general reward structure that includes both the root and hip orientations. This
    structure effectively maintains the desired orientation of the robot’s upper body
    while also facilitating precise control over its direction of movement.'
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion and Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose a framework that combines a single adversarial imitation
    learning policy and LLMs to perform complex tasks by scheduling the skills. Moreover,
    we design the general reward to ensure that a single control policy is capable
    of addressing a majority of requirements. Finally, our experiments confirm that
    the framework we proposed efficiently solves complex tasks and adapts to uncertain
    semantic outputs of LLMs by introducing codebook-based vector quantization. However,
    the robot models we used with reference motion data are relatively ideal. Our
    future work is to generate more practical data and apply the framework to real
    humanoid robots.
  prefs: []
  type: TYPE_NORMAL
- en: VII ACKNOWLEDGMENT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to extend their sincere gratitude to Xue Bin (Jason)
    Peng for his invaluable suggestions and insights that have significantly contributed
    to the improvement of this project. His expertise and timely advice have been
    a cornerstone in the successful completion of our work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Ho and S. Ermon, “Generative adversarial imitation learning,” *Advances
    in neural information processing systems*, vol. 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Jolicoeur-Martineau, “The relativistic discriminator: a key element
    missing from standard gan,” *arXiv preprint arXiv:1807.00734*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] X. B. Peng, Y. Guo, L. Halper, S. Levine, and S. Fidler, “Ase: Large-scale
    reusable adversarial skill embeddings for physically simulated characters,” *ACM
    Transactions On Graphics (TOG)*, vol. 41, no. 4, pp. 1–17, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y. Chebotar, K. Hausman, Y. Lu, T. Xiao, D. Kalashnikov, J. Varley, A. Irpan,
    B. Eysenbach, R. Julian, C. Finn *et al.*, “Actionable models: Unsupervised offline
    reinforcement learning of robotic skills,” *arXiv preprint arXiv:2104.07749*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *International conference on machine learning*.   PMLR,
    2021, pp. 8748–8763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray *et al.*, “Training language models to follow instructions
    with human feedback,” *Advances in Neural Information Processing Systems*, vol. 35,
    pp. 27 730–27 744, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson,
    I. Mordatch, Y. Chebotar *et al.*, “Inner monologue: Embodied reasoning through
    planning with language models,” *arXiv preprint arXiv:2207.05608*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot
    planners: Extracting actionable knowledge for embodied agents,” in *International
    Conference on Machine Learning*.   PMLR, 2022, pp. 9118–9147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] N. Di Palo, A. Byravan, L. Hasenclever, M. Wulfmeier, N. Heess, and M. Riedmiller,
    “Towards a unified agent with foundation models,” in *Workshop on Reincarnating
    Reinforcement Learning at ICLR 2023*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, “Calvin: A benchmark
    for language-conditioned policy learning for long-horizon robot manipulation tasks,”
    *IEEE Robotics and Automation Letters*, vol. 7, no. 3, pp. 7327–7334, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] O. Mees, L. Hermann, and W. Burgard, “What matters in language conditioned
    robotic imitation learning over unstructured data,” *IEEE Robotics and Automation
    Letters*, vol. 7, no. 4, pp. 11 205–11 212, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] O. Mees, J. Borja-Diaz, and W. Burgard, “Grounding language with visual
    affordances over unstructured data,” in *2023 IEEE International Conference on
    Robotics and Automation (ICRA)*.   IEEE, 2023, pp. 11 576–11 582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where pathways
    for robotic manipulation,” in *Conference on Robot Learning*.   PMLR, 2022, pp.
    894–906.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    and A. Zeng, “Code as policies: Language model programs for embodied control,”
    in *2023 IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE,
    2023, pp. 9493–9500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn,
    C. Fu, K. Gopalakrishnan, K. Hausman *et al.*, “Do as i can, not as i say: Grounding
    language in robotic affordances,” *arXiv preprint arXiv:2204.01691*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski,
    T. Ding, D. Driess, A. Dubey, C. Finn *et al.*, “Rt-2: Vision-language-action
    models transfer web knowledge to robotic control,” *arXiv preprint arXiv:2307.15818*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y. Guo, Y.-J. Wang, L. Zha, Z. Jiang, and J. Chen, “Doremi: Grounding
    language model by detecting and recovering from plan-execution misalignment,”
    *arXiv preprint arXiv:2307.00329*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. Hu, G. Corrado, N. Griffiths, Z. Murez, C. Gurau, H. Yeo, A. Kendall,
    R. Cipolla, and J. Shotton, “Model-based imitation learning for urban driving,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 20 703–20 716,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] L. Le Mero, D. Yi, M. Dianati, and A. Mouzakitis, “A survey on imitation
    learning techniques for end-to-end autonomous vehicles,” *IEEE Transactions on
    Intelligent Transportation Systems*, vol. 23, no. 9, pp. 14 128–14 147, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Hua, L. Zeng, G. Li, and Z. Ju, “Learning for a robot: Deep reinforcement
    learning, imitation learning, transfer learning,” *Sensors*, vol. 21, no. 4, p.
    1278, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] E. Johns, “Coarse-to-fine imitation learning: Robot manipulation from
    a single demonstration,” in *2021 IEEE international conference on robotics and
    automation (ICRA)*.   IEEE, 2021, pp. 4613–4619.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, “Viola: Object-centric imitation
    learning for vision-based robot manipulation,” in *Conference on Robot Learning*.   PMLR,
    2023, pp. 1199–1210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] F. Torabi, G. Warnell, and P. Stone, “Behavioral cloning from observation,”
    *arXiv preprint arXiv:1805.01954*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] X. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa, “Amp: Adversarial
    motion priors for stylized physics-based character control,” *ACM Transactions
    on Graphics (ToG)*, vol. 40, no. 4, pp. 1–20, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] C. Li, S. Blaes, P. Kolev, M. Vlastelica, J. Frey, and G. Martius, “Versatile
    skill control via self-supervised adversarial imitation of unlabeled mixed motions,”
    in *2023 IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE,
    2023, pp. 2944–2950.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek, “Hierarchical reinforcement
    learning: A comprehensive survey,” *ACM Computing Surveys (CSUR)*, vol. 54, no. 5,
    pp. 1–35, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Ji, Z. Li, Y. Sun, X. B. Peng, S. Levine, G. Berseth, and K. Sreenath,
    “Hierarchical reinforcement learning for precise soccer shooting skills using
    a quadrupedal robot,” in *2022 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*.   IEEE, 2022, pp. 1479–1486.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Huang, Z. Wang, J. Zhou, and J. Lu, “Planning irregular object packing
    via hierarchical reinforcement learning,” *IEEE Robotics and Automation Letters*,
    vol. 8, no. 1, pp. 81–88, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] M. Eppe, C. Gumbsch, M. Kerzel, P. D. Nguyen, M. V. Butz, and S. Wermter,
    “Intelligent problem-solving as integrated hierarchical reinforcement learning,”
    *Nature Machine Intelligence*, vol. 4, no. 1, pp. 11–20, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Juravsky, Y. Guo, S. Fidler, and X. B. Peng, “Padl: Language-directed
    physics-based character control,” in *SIGGRAPH Asia 2022 Conference Papers*, 2022,
    pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] C. Tessler, Y. Kasten, Y. Guo, S. Mannor, G. Chechik, and X. B. Peng,
    “Calm: Conditional adversarial latent models for directable virtual characters,”
    in *ACM SIGGRAPH 2023 Conference Proceedings*, 2023, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] J. Ma, H. Xu, J. Jiang, X. Mei, and X.-P. Zhang, “Ddcgan: A dual-discriminator
    conditional generative adversarial network for multi-resolution image fusion,”
    *IEEE Transactions on Image Processing*, vol. 29, pp. 4980–4995, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. S. Polydoros and L. Nalpantidis, “Survey of model-based reinforcement
    learning: Applications on robotics,” *Journal of Intelligent & Robotic Systems*,
    vol. 86, no. 2, pp. 153–173, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. R. Mahmood, D. Korenkevych, G. Vasan, W. Ma, and J. Bergstra, “Benchmarking
    reinforcement learning algorithms on real-world robots,” in *Conference on robot
    learning*.   PMLR, 2018, pp. 561–591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] P. Kormushev, S. Calinon, and D. G. Caldwell, “Reinforcement learning
    in robotics: Applications and real-world challenges,” *Robotics*, vol. 2, no. 3,
    pp. 122–148, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] F. Torabi, G. Warnell, and P. Stone, “Generative adversarial imitation
    from observation,” *arXiv preprint arXiv:1807.06158*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Menéndez, J. Pardo, L. Pardo, and M. Pardo, “The jensen-shannon divergence,”
    *Journal of the Franklin Institute*, vol. 334, no. 2, pp. 307–318, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller,
    N. Rudin, A. Allshire, A. Handa *et al.*, “Isaac gym: High performance gpu-based
    physics simulation for robot learning,” *arXiv preprint arXiv:2108.10470*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
