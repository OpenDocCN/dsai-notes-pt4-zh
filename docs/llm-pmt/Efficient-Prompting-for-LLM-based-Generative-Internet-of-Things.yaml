- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:42:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:42:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Efficient Prompting for LLM-based Generative Internet of Things
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效提示用于基于LLM的生成型物联网
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10382](https://ar5iv.labs.arxiv.org/html/2406.10382)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10382](https://ar5iv.labs.arxiv.org/html/2406.10382)
- en: Bin Xiao, Burak Kantarci, , Jiawen Kang,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Bin Xiao、Burak Kantarci、Jiawen Kang，
- en: Dusit Niyato, , Mohsen Guizani B. Xiao an B. Kantarci are with the School of
    Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON,
    Canada. Emails:{bxiao103, burak.kantarci}@uottawa.ca
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Dusit Niyato、Mohsen Guizani、B. Xiao 和 B. Kantarci 现为加拿大渥太华大学电气工程与计算机科学学院的成员。电子邮件：{bxiao103,
    burak.kantarci}@uottawa.ca
- en: 'J. Kang is with Guandong University of Technology, China, Email: kavinkang@gdut.edu.cn'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: J. Kang 现为中国广东工业大学的成员，邮箱：kavinkang@gdut.edu.cn
- en: 'D. Niyato is with Nanyang Technological University, Singapore, Email: dniyato@ntu.edu.sg'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: D. Niyato 现为新加坡南洋理工大学的成员，邮箱：dniyato@ntu.edu.sg
- en: M. Guizani is with Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),
    Abu Dhabi, UAE, mohsen.guizani@mbzuai.ac.ae
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: M. Guizani 现为阿布扎比穆罕默德·本·扎耶德人工智能大学（MBZUAI）的成员，邮箱：mohsen.guizani@mbzuai.ac.ae
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have demonstrated remarkable capacities on various
    tasks, and integrating the capacities of LLMs into the Internet of Things (IoT)
    applications has drawn much research attention recently. Due to security concerns,
    many institutions avoid accessing state-of-the-art commercial LLM services, requiring
    the deployment and utilization of open-source LLMs in a local network setting.
    However, open-source LLMs usually have more limitations regarding their performance,
    such as their arithmetic calculation and reasoning capacities, and practical systems
    of applying LLMs to IoT have yet to be well-explored. Therefore, we propose a
    text-based generative IoT (GIoT) system deployed in the local network setting
    in this study. To alleviate the limitations of LLMs and provide service with competitive
    performance, we apply prompt engineering methods to enhance the capacities of
    the open-source LLMs, design a Prompt Management Module and a Post-processing
    Module to manage the tailored prompts for different tasks and process the results
    generated by the LLMs. To demonstrate the effectiveness of the proposed system,
    we discuss a challenging Table Question Answering (Table-QA) task as a case study
    of the proposed system, as tabular data is usually more challenging than plain
    text because of their complex structures, heterogeneous data types and sometimes
    huge sizes. We conduct comprehensive experiments on two popular Table-QA datasets,
    and the results show that our proposal can achieve competitive performance compared
    with state-of-the-art LLMs, demonstrating that the proposed LLM-based GIoT system
    can provide competitive performance with tailored prompting methods and is easily
    extensible to new tasks without training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种任务中展示了卓越的能力，将LLMs的能力整合到物联网（IoT）应用中近年来吸引了大量研究关注。由于安全问题，许多机构避免使用最先进的商业LLM服务，因此需要在本地网络环境中部署和利用开源LLM。然而，开源LLM通常在其性能方面存在更多限制，例如其算术计算和推理能力，且应用LLM于IoT的实际系统尚未得到充分探索。因此，本研究提出了一种基于文本的生成型物联网（GIoT）系统，在本地网络环境中进行部署。为了缓解LLM的限制并提供具有竞争力的性能，我们应用提示工程方法来增强开源LLM的能力，设计了一个提示管理模块和一个后处理模块，以管理不同任务的定制提示并处理LLM生成的结果。为了展示所提系统的有效性，我们将一个具有挑战性的表格问答（Table-QA）任务作为案例研究，因为表格数据由于其复杂结构、异质数据类型和有时巨大的规模，通常比纯文本更具挑战性。我们在两个流行的Table-QA数据集上进行了全面的实验，结果表明我们提出的方案与最先进的LLM相比能够实现竞争力的性能，证明了所提出的基于LLM的GIoT系统能够通过定制提示方法提供具有竞争力的性能，并且可以轻松扩展到新的任务，而无需重新训练。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Generative Internet of Things, Table Question Answering, Prompt Engineering,
    Large Language Model
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成型物联网、表格问答、提示工程、大型语言模型
- en: I Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Artificial Intelligence of Things (AIoT), integrating Artificial Intelligence
    (AI) and Internet of Things (IoT) for efficient data analysis and intelligent
    decision making [[1](#bib.bib1)], have been widely discussed in many studies and
    applied in many scenarios, such as Healthcare [[2](#bib.bib2)], Smart Cities [[3](#bib.bib3)]
    and Industries [[4](#bib.bib4)]. Currently, task-specific machine learning and
    Deep Neural Network (DNN) models are the mainstream choices for AIoT providing
    services to IoT devices, which are often deployed on the cloud and edge servers [[1](#bib.bib1)].
    With the development of Generative Artificial Intelligence (GAI), especially large
    language models (LLMs), leveraging its remarkable general capacities to the IoT
    applications, termed as Generative Internet of Things (GIoT) [[5](#bib.bib5)],
    becomes a promising research direction [[6](#bib.bib6), [7](#bib.bib7)]. Different
    from task-specific models, LLMs, such as GPT-4 [[8](#bib.bib8)], have demonstrated
    their general capacities on a wide range of tasks, such as data analysis, code
    generation, reasoning, planning and many others, making it possible to provide
    various services with a single LLM model, maintaining competitive performance
    with tailored task-specific models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能物联网（AIoT），即将人工智能（AI）和物联网（IoT）整合用于高效的数据分析和智能决策[[1](#bib.bib1)]，已在许多研究中广泛讨论并应用于各种场景，例如医疗保健[[2](#bib.bib2)]、智能城市[[3](#bib.bib3)]和工业[[4](#bib.bib4)]。目前，任务特定的机器学习和深度神经网络（DNN）模型是AIoT为IoT设备提供服务的主流选择，这些模型通常部署在云端和边缘服务器上[[1](#bib.bib1)]。随着生成式人工智能（GAI），特别是大型语言模型（LLMs）的发展，将其卓越的通用能力应用于IoT，称为生成式物联网（GIoT）[[5](#bib.bib5)]，成为一个有前景的研究方向[[6](#bib.bib6),
    [7](#bib.bib7)]。与任务特定模型不同，LLMs，如GPT-4[[8](#bib.bib8)]，已在数据分析、代码生成、推理、规划等广泛任务上展示了其通用能力，使得利用单一LLM模型提供多种服务成为可能，并且其性能与定制的任务特定模型相当。
- en: Despite the remarkable capacities of LLMs, many issues need to be considered
    when integrating LLMs into IoT systems for an LLM-based GIoT system. First, following
    the Scaling Law [[9](#bib.bib9)], the capacities of an LLM are growing with its
    number of parameters and the scale of the training dataset, making LLMs often
    have a tremendous number of parameters, which lead to high hardware requirements
    for the training and inference. For example, popular open-source LLMs providing
    competitive performance on some public benchmarks often have around 70 billion
    parameters, such as Mixtral-8*7B [[10](#bib.bib10)] and Llama-3-70B [[11](#bib.bib11)].
    These numbers of parameters make it not practical to deploy LLMs on edge IoT devices.
    Even though deploying LLMs on the edge and cloud servers can be an option, the
    efficiency of an LLM-based GIoT system still needs to be carefully considered.
    Besides the hardware requirements and efficiency issues, data privacy and security
    issues hinder many institutions from accessing commercial state-of-the-art LLMs.
    For example, for a healthcare IoT application collecting private data from patients,
    it is necessary to avoid uploading collected data to public, commercial LLM services
    for data analysis because of privacy issues. Therefore, a safe, transparent, and
    controllable open-source LLM deployed in a local network is more suitable for
    many institutions, even though commercial LLMs can provide better services. At
    last, the performance and the scalability of a LLM-based GIoT system are another
    two critical considerations because both commercial and open-source LLMs have
    some inherent limitations [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)],
    such as their hallucination issues, limited reasoning capacities for complex tasks.
    Typically, prompting methods and fine-tuning are two directions that can alleviate
    these limitations. Specifically, prompting methods improve the performance of
    an LLM by designing tailored prompts for different tasks to elicit the capacities
    of an LLM. For example, Chain of Thought (CoT) [[14](#bib.bib14)] is a popular
    prompting method to improve the LLM’s reasoning performance by providing reasoning
    rationales. Program of Thoughts (PoT) [[13](#bib.bib13)] is another prompting
    method generating Python code to offload the reasoning and calculation tasks to
    the Python interpreter. Since these prompting methods focus on tailoring task-specific
    prompts for LLMs, they often lead to longer inference time because of their larger
    number of prompting tokens. By contrast, fine-tuning an LLM for a task can also
    improve the performance but requires label datasets and computation resources
    for the model training. As discussed in some studies [[15](#bib.bib15), [16](#bib.bib16)],
    fine-tuning an LLM for unseen tasks can increase the model’s bias and degrade
    its general capacities.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs具有显著的能力，但在将LLMs集成到物联网系统中以实现基于LLM的GIoT系统时，仍需考虑许多问题。首先，根据Scaling Law [[9](#bib.bib9)]，LLM的能力随着其参数数量和训练数据集规模的增长而增加，这使得LLMs通常拥有大量的参数，从而对训练和推理的硬件要求很高。例如，一些在公共基准上表现出色的流行开源LLMs通常拥有约70亿个参数，如Mixtral-8*7B [[10](#bib.bib10)]和Llama-3-70B [[11](#bib.bib11)]。这些参数数量使得在边缘物联网设备上部署LLMs不切实际。尽管在边缘和云服务器上部署LLMs可以是一种选择，但LLM基于GIoT系统的效率仍需仔细考虑。除了硬件要求和效率问题外，数据隐私和安全问题阻碍了许多机构访问商业最先进的LLMs。例如，对于收集患者私人数据的医疗保健物联网应用，出于隐私问题，必须避免将收集的数据上传到公共的商业LLM服务进行数据分析。因此，即使商业LLMs能提供更好的服务，部署在本地网络中的安全、透明和可控的开源LLM更适合许多机构。最后，基于LLM的GIoT系统的性能和可扩展性是另两个关键考虑因素，因为商业和开源LLMs都有一些固有的限制 [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]，例如它们的幻觉问题、对复杂任务的推理能力有限。通常，提示方法和微调是缓解这些限制的两个方向。具体来说，提示方法通过设计针对不同任务的量身定制的提示来提升LLM的性能，从而引发LLM的能力。例如，Chain
    of Thought (CoT) [[14](#bib.bib14)]是一种流行的提示方法，通过提供推理理由来提高LLM的推理性能。Program of Thoughts
    (PoT) [[13](#bib.bib13)]是另一种提示方法，通过生成Python代码将推理和计算任务交给Python解释器。由于这些提示方法专注于为LLMs量身定制任务特定的提示，因此它们通常会导致较长的推理时间，因为提示令牌的数量较多。相比之下，为特定任务微调LLM也可以提高性能，但需要标签数据集和计算资源进行模型训练。正如一些研究所讨论的那样 [[15](#bib.bib15),
    [16](#bib.bib16)]，为未见任务微调LLM可能会增加模型的偏见并降低其通用能力。
- en: Since practical solutions applying LLMs to the IoT setting have yet to be well-explored,
    even though there have been some studies [[6](#bib.bib6), [7](#bib.bib7)] discussing
    the potential and possible frameworks of such applications, we propose a practical
    LLM-based GIoT system in this study. Considering the discussed possible issues
    for an LLM-based GIoT system, we propose to deploy the open-source LLM on the
    edge server in a local network setting to address the and employ prompting methods
    to enhance the capacities of the proposed system for different tasks. Specifically,
    a Prompt Management Module and a Post-processing Module are proposed to be deployed
    in the edge server, in which the former is responsible for the selection, management
    and creation of prompts and demonstrations for the requests from IoT devices,
    and the latter is responsible for post-processing the results generated by the
    LLMs, as shown in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Efficient Prompting
    for LLM-based Generative Internet of Things"). With the proposed Prompt Management
    Module and Post-processing Module, the GIoT system can be easily extended to new
    tasks by adding tailored task-specific prompts in the Task-specific Prompts Database,
    providing competitive performance for a wide range of tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将大型语言模型（LLMs）应用于物联网（IoT）环境的实际解决方案尚未得到充分探索，尽管已有一些研究[[6](#bib.bib6), [7](#bib.bib7)]讨论了此类应用的潜力和可能的框架，我们在本研究中提出了一种基于LLM的实际GIoT系统。考虑到针对基于LLM的GIoT系统讨论的潜在问题，我们建议在本地网络环境中的边缘服务器上部署开源LLM，并采用提示方法来提升所提出系统在不同任务中的能力。具体来说，建议在边缘服务器上部署一个**提示管理模块**和一个**后处理模块**，其中前者负责选择、管理和创建针对物联网设备请求的提示和演示，后者则负责对LLM生成的结果进行后处理，如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")所示。通过所提议的提示管理模块和后处理模块，GIoT系统可以通过在任务特定提示数据库中添加量身定制的任务特定提示，轻松扩展到新任务，为广泛的任务提供有竞争力的性能。
- en: 'To demonstrate the effectiveness of the proposed LLM-based GIoT system, we
    implement a Semi-structured Table Question Answering (Table-QA) service in the
    proposed system, which is useful and challenging. Specifically, the Table-QA service
    aims to answer the query question based on the given table information. Since
    tables are widely used to summarize critical information in many data sources,
    such as visually rich documents and web pages [[17](#bib.bib17)], Table-QA [[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)] can be
    a useful service to provide analysis to the tabular data, and has also drawn much
    research attention recently. Typically, tables can be easily categorized into
    two groups: structured and semi-structured tables. Structured tables are usually
    from relational database systems with explicit schema describing their structures
    and data types, meaning the programming languages, such as SQL, can naturally
    process them. By contrast, tables from other sources, such as web pages and visually
    rich documents, are usually semi-structured without schema requirements, resulting
    in complex structures, heterogeneous data types and sometimes huge sizes, making
    the Table-QA task on these semi-structured tables more challenging. Therefore,
    we use the Table-QA problem on the semi-structured tables to verify the proposed
    LLM-based system, which is a more challenging setting. To provide competitive
    service as commercial LLMs, we propose a three-stage prompting method, including
    task-planning, task-conducting and task-correction stages, leveraging Python code
    to conduct reasoning steps. The proposed prompting method can improve the performance
    of open-source LLMs, which can also demonstrate that the proposed system can be
    easily extended to new tasks with prompting methods.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示所提出的基于LLM的GIoT系统的有效性，我们在该系统中实现了一个半结构化表格问答（Table-QA）服务，这一服务既有用又具有挑战性。具体来说，表格问答服务旨在根据给定的表格信息回答查询问题。由于表格广泛用于总结许多数据源中的关键信息，如视觉丰富的文档和网页 [[17](#bib.bib17)]，表格问答 [[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)] 可以作为提供表格数据分析的有用服务，并且最近也引起了大量研究关注。通常，表格可以简单地分为两类：结构化表格和半结构化表格。结构化表格通常来自关系数据库系统，具有明确的模式来描述其结构和数据类型，这意味着编程语言，如SQL，能够自然地处理它们。相比之下，来自其他来源的表格，如网页和视觉丰富的文档，通常是半结构化的，没有模式要求，导致结构复杂、数据类型异构，有时表格规模巨大，使得这些半结构化表格上的表格问答任务更加具有挑战性。因此，我们使用半结构化表格上的表格问答问题来验证所提出的基于LLM的系统，这是一种更具挑战性的设置。为了提供与商业LLMs相竞争的服务，我们提出了一种三阶段提示方法，包括任务规划、任务执行和任务修正阶段，利用Python代码进行推理步骤。所提出的提示方法可以提高开源LLMs的性能，这也可以证明所提出的系统可以轻松扩展到新的任务中，通过提示方法。
- en: '![Refer to caption](img/dc8d45bab6967e36906a692a1e50e8dc.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dc8d45bab6967e36906a692a1e50e8dc.png)'
- en: 'Figure 1: Overall architecture of the proposed LLM-based GIoT system.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：所提出的基于LLM的GIoT系统的整体架构。
- en: 'To sum up, the contributions of this study can be 3-fold:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本研究的贡献可以归纳为三方面：
- en: '1.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose an LLM-based GIoT system with open-source LLMs deployed in the local
    network setting, which includes a Prompt Management Module, a Post-processing
    Module and a Task-specific Prompts Database to address the considerations in data
    privacy and security, system scalability, and enhance the capacities of the LLM
    by integrating prompting methods.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个基于LLM的GIoT系统，使用开源LLMs在本地网络环境中部署，该系统包括一个提示管理模块、一个后处理模块和一个任务特定提示数据库，以解决数据隐私和安全、系统可扩展性方面的考虑，并通过集成提示方法提升LLM的能力。
- en: '2.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We discuss a challenging Table-QA problem to demonstrate the proposed LLM-based
    GIoT system and propose a three-stage prompting solution, including task-planning,
    task-conducting and task-correction stages, to alleviate the issues caused by
    the complex structures, heterogeneous data types, huge tables and the limitations
    of LLMs and also reduce the inference cost. A series of atomic operations is proposed
    to describe and measure the similarities of QA tasks and select proper demonstrations
    for prompt creation.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了一个具有挑战性的表格问答问题，以展示所提出的基于LLM的GIoT系统，并提出了一种三阶段提示解决方案，包括任务规划、任务执行和任务修正阶段，以缓解复杂结构、异构数据类型、大规模表格以及LLMs的限制所带来的问题，并降低推理成本。我们提出了一系列原子操作来描述和测量问答任务的相似性，并选择适当的示例来创建提示。
- en: '3.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We conduct extensive experiments on the WikiTableQA and TabFact datasets with
    open-source LLMs to verify the proposed LLM-based GIoT system and the proposed
    prompting method. The experimental results demonstrate that our proposed prompting
    method can outperform the baseline methods by a large margin, achieving state-of-the-art
    performance. We also conduct comprehensive analyses that consider the performance
    of different prompting methods, the inference costs, and the behaviour of different
    open-source LLMs, which can be a guide for selecting LLMs and prompting methods
    for an LLM-based GIoT system.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在WikiTableQA和TabFact数据集上使用开源LLM进行了广泛实验，以验证提出的基于LLM的GIoT系统和提示方法。实验结果表明，我们提出的提示方法能显著优于基线方法，达到最先进的性能。我们还进行了全面分析，考虑了不同提示方法的性能、推理成本以及不同开源LLM的行为，这些分析可以为选择LLM和提示方法用于基于LLM的GIoT系统提供指导。
- en: 'The rest of this paper is organized as follows: Section [II](#S2 "II Related
    Work ‣ Efficient Prompting for LLM-based Generative Internet of Things") discusses
    related studies, including recent studies applying LLMs to the IoT systems, Table-QA
    solutions and prompting methods. Section [IV](#S4 "IV Case Study of Table-QA for
    the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things") describes our proposed LLM-based GIoT system and prompting solution.
    Section [V](#S5 "V Experiments and Analysis ‣ Efficient Prompting for LLM-based
    Generative Internet of Things") shows the experimental results and discusses the
    design aspects of the proposed prompting method. At last, we draw our conclusion
    and possible future directions in Section [VI](#S6 "VI Conclusion and Future Work
    ‣ Efficient Prompting for LLM-based Generative Internet of Things").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本文其余部分组织如下：第[II](#S2 "II Related Work ‣ Efficient Prompting for LLM-based Generative
    Internet of Things")节讨论了相关研究，包括最近将LLM应用于物联网系统、Table-QA解决方案和提示方法的研究。第[IV](#S4 "IV
    Case Study of Table-QA for the LLM-based GIoT System ‣ Efficient Prompting for
    LLM-based Generative Internet of Things")节描述了我们提出的基于LLM的GIoT系统和提示解决方案。第[V](#S5
    "V Experiments and Analysis ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")节展示了实验结果，并讨论了所提提示方法的设计方面。最后，我们在第[VI](#S6 "VI Conclusion and Future
    Work ‣ Efficient Prompting for LLM-based Generative Internet of Things")节中得出结论并探讨未来的可能方向。
- en: II Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: II-A Generative Models in IoT
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 物联网中的生成模型
- en: As generative models, especially text-based LLMs, have demonstrated their remarkable
    capacities in a wide range of tasks, integrating their capacities into IoT applications
    has attracted the research community’s attention. There have been some studies [[5](#bib.bib5),
    [6](#bib.bib6), [23](#bib.bib23)] summarizing the critical components of Generative
    Models and discussing the potential of applying Generative Models to IoT systems.
    Authors in study [[5](#bib.bib5)] firstly term the combination of generative models
    and IoT applications as Generative IoT (GIoT), discuss the foundations of generative
    models and the potential of GIoT applications, including Vision-based, Audio-based,
    Text-based and other GIoT applications. Similarly, authors in study [[6](#bib.bib6)]
    also point out various potential GIoT applications in many fields, such as Mobile
    Networks, Autonomous Vehicles, and many others. Besides discussing the insights
    and potentials of GIoT, some studies examine various aspects of applying LLMs
    in IoT settings. CASIT [[7](#bib.bib7)] is an LLM-agent-based IoT framework proposing
    a Sensor Interface to convert sensor data into natural language and design multiple
    types of LLM-based Agent to cooperate and analyze the sensor data and user requests.
    LLMind [[24](#bib.bib24)] introduces a framework that integrates various domain-specific
    AI modules and enables IoT device cooperation to enhance the LLM’s capabilities
    for conducting complex tasks. Study [[25](#bib.bib25)] proposes an intelligent
    control framework integrating Integrated Terrestrial Non-terrestrial Networks,
    IoT and language models, identifying key components, potential applications and
    challenges. Overall, current studies regarding GIoT applications mainly focus
    on proposing abstractions of the systems, and practical cases need to be explored
    in more depth. Therefore, this study proposes an extensible LLM-based GIoT system
    using prompting methods and uses a challenging Table-QA problem as a case study
    to illustrate the proposed system.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成模型，尤其是基于文本的语言模型（LLMs），在广泛任务中展示了其卓越的能力，将这些能力整合到物联网（IoT）应用中已经引起了研究界的关注。一些研究[[5](#bib.bib5),
    [6](#bib.bib6), [23](#bib.bib23)]总结了生成模型的关键组成部分，并讨论了将生成模型应用于物联网系统的潜力。研究[[5](#bib.bib5)]中的作者首先将生成模型与物联网应用的结合称为生成物联网（GIoT），讨论了生成模型的基础以及GIoT应用的潜力，包括基于视觉、基于音频、基于文本及其他GIoT应用。类似地，研究[[6](#bib.bib6)]中的作者也指出了GIoT在许多领域中的各种潜在应用，如移动网络、自动驾驶车辆等。除了讨论GIoT的见解和潜力外，一些研究还考察了在物联网环境中应用语言模型的各个方面。CASIT[[7](#bib.bib7)]是一个基于LLM代理的物联网框架，提出了一种传感器接口，将传感器数据转换为自然语言，并设计了多种类型的基于LLM的代理来协作和分析传感器数据和用户请求。LLMind[[24](#bib.bib24)]引入了一个框架，集成了各种领域特定的AI模块，并使物联网设备协作以增强LLM执行复杂任务的能力。研究[[25](#bib.bib25)]提出了一个智能控制框架，整合了综合地面与非地面网络、物联网和语言模型，识别关键组件、潜在应用和挑战。总体而言，当前关于GIoT应用的研究主要集中在提出系统的抽象，实际案例需要更深入的探索。因此，本研究提出了一个可扩展的基于LLM的GIoT系统，使用提示方法，并以具有挑战性的表格问答问题作为案例研究来说明所提系统。
- en: II-B Table Question Answering
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 表格问答
- en: 'Since this study uses the Table-QA task as the case study of the proposed LLM-based
    GIoT system, we include recent studies using LLM to solve the Table-QA problem
    in this section. Specifically, most of the studies applying LLMs to the Table-QA
    task can be categorized into instruction tuning based and prompt engineering based
    approaches. Instruction-tuning approaches usually need to collect large-scale
    datasets and then further fine-tuned LLMs with parameter-efficient fine-tuning
    methods, such as LORA [[26](#bib.bib26)] and LongLORA [[27](#bib.bib27)]. TableLLAMA [[28](#bib.bib28)]
    and TAT-LLM [[20](#bib.bib20)] are typical examples of applying instruction tuning
    for the table processing. TableLLAMA is a generalist model for tables fine-tuned
    on Llama2 [[29](#bib.bib29)] with LongLORA. For fine-tuning TableLLAMA, a dataset
    collection named TableInstruct is proposed by collecting table-based samples from
    14 datasets for 11 tasks. TAT-LLM is another fine-tuned LLAMA2 model specifically
    for the discrete reasoning over tables, which decomposes the Table-QA into three
    steps: Extractor, Reasoner and Executor. To construct the dataset for the model
    fine-tuning, TAT-LLM proposes a template to guide LLM in generating data following
    the proposed step-wise pipeline.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本研究将 Table-QA 任务作为提出的基于 LLM 的 GIoT 系统的案例研究，我们在本节中包含了使用 LLM 解决 Table-QA 问题的最新研究。具体而言，大多数将
    LLM 应用于 Table-QA 任务的研究可以分为基于指令调优和基于提示工程的方法。指令调优方法通常需要收集大规模数据集，然后进一步使用参数高效的调优方法，如
    LORA [[26](#bib.bib26)] 和 LongLORA [[27](#bib.bib27)]，对 LLM 进行调优。TableLLAMA [[28](#bib.bib28)]
    和 TAT-LLM [[20](#bib.bib20)] 是应用指令调优进行表格处理的典型例子。TableLLAMA 是一个通用的表格模型，通过 LongLORA
    对 Llama2 [[29](#bib.bib29)] 进行调优。为了对 TableLLAMA 进行调优，提出了一个名为 TableInstruct 的数据集，通过从
    14 个数据集中收集用于 11 个任务的表格样本。TAT-LLM 是另一个专门针对表格上的离散推理进行调优的 LLAMA2 模型，它将 Table-QA 分解为三个步骤：提取器、推理器和执行器。为了构建模型调优的数据集，TAT-LLM
    提出了一个模板，以指导 LLM 按照提出的逐步管道生成数据。
- en: 'On the other hand, prompting engineering based solutions focus on proposing
    proper prompts to the language model to elicit LLMs’ capacities. Since Table-QA
    needs to extract relevant information and evidence from tables and perform reasoning,
    decomposing the Table-QA task into multiple steps is also widely adopted in prompting
    engineering-based solutions. Besides, as LLM often fails to process large tables
    and complex reasoning, many studies [[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [20](#bib.bib20)] propose to decompose large tables into small tables in the extraction
    step and divide the complex reasoning task into more straightforward reasoning
    questions. For example, StructGPT [[33](#bib.bib33)] defines specialized interfaces
    for information extraction and linearizes the extracted sub-tables as inputs to
    the LLM for question answering. EEDP [[34](#bib.bib34)] is another example proposing
    a prompting method containing four steps: Elicit, Extract, Decompose and Predict.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于提示工程的解决方案侧重于提出适当的提示，以引出语言模型的能力。由于 Table-QA 需要从表格中提取相关信息和证据并进行推理，将 Table-QA
    任务分解为多个步骤在基于提示工程的解决方案中也被广泛采用。此外，由于 LLM 经常无法处理大型表格和复杂推理，许多研究 [[30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [20](#bib.bib20)] 提出了在提取步骤中将大型表格分解为小表格，并将复杂的推理任务分解为更简单的推理问题。例如，StructGPT
    [[33](#bib.bib33)] 定义了用于信息提取的专用接口，并将提取的子表线性化为 LLM 的问题回答输入。EEDP [[34](#bib.bib34)]
    是另一个例子，提出了一种包含四个步骤的提示方法：引导、提取、分解和预测。
- en: Besides these studies decomposing the Table-QA into extraction and reasoning
    steps and dividing difficult extraction and reasoning tasks into simpler ones,
    programming languages, such as SQL and Python, are also widely leveraged in these
    solutions to overcome the limition of LLMs in arithmetic calculation and reasoning.
    Dater [[31](#bib.bib31)] is a typical solution following the multi-step design
    and leveraging SQL to conduct reasoning. More specifically, Dater decomposes both
    large evidence and complex questions into relevant and simpler ones, applies SQL
    queries to produce numerical and logical reasoning results to the decomposed sub-questions,
    and generates the final results based on the sub-results and extracted evidence
    with ICL. Binding [[35](#bib.bib35)] is another example of applying Python and
    SQL for the table processing, which proposes a unified API to map tasks into executable
    programs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将表格问答分解为提取和推理步骤以及将困难的提取和推理任务分解为更简单的任务之外，编程语言如SQL和Python也被广泛应用于这些解决方案中，以克服LLMs在算术计算和推理方面的局限性。Dater
    [[31](#bib.bib31)] 是一个典型的多步骤设计解决方案，利用SQL进行推理。具体而言，Dater 将大量证据和复杂问题分解为相关的更简单问题，应用SQL查询生成对分解后子问题的数值和逻辑推理结果，并基于子结果和提取的证据通过ICL生成最终结果。Binding
    [[35](#bib.bib35)] 是另一个应用Python和SQL进行表格处理的例子，提出了一个统一的API，将任务映射到可执行的程序中。
- en: All in all, as the complexities of Table-QA in the information extraction and
    reasoning, breaking Table-QA into multiple steps and decomposing difficult extraction
    and reasoning tasks into simpler ones have been the dominant solution, and external
    tools such as SQL and Python, can compensate the limitations of LLMs in arithmetic
    calculation and reasoning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，鉴于表格问答（Table-QA）在信息提取和推理中的复杂性，将表格问答分解为多个步骤，并将困难的提取和推理任务简化为更简单的任务一直是主要的解决方案，并且外部工具如SQL和Python可以弥补大型语言模型（LLMs）在算术计算和推理方面的局限性。
- en: II-C Prompting Methods
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 提示方法
- en: This section focuses on recent prompting methods for LLMs because our proposed
    LLM-based GIoT system applies prompting methods to enhance the LLMs’ capacities
    for different tasks. As some of these methods have also been applied to the Table-QA
    problem, some methods included in this section can be overlapped with the discussed
    studies in Section [II-B](#S2.SS2 "II-B Table Question Answering ‣ II Related
    Work ‣ Efficient Prompting for LLM-based Generative Internet of Things"). There
    have been many studies demonstrated that prompting LLMs with a few demonstrations
    and intermediate reasoning steps can elicit the reasoning capacities of LLMs,
    which often refer to In Context Learning (ICL) [[36](#bib.bib36)] and CoT [[14](#bib.bib14)].
    Although ICL and CoT are useful, writing proper prompting demonstrations is non-trivial
    and time-consuming. Therefore, some studies [[37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39)] propose to use LLMs to generate demonstrations. Auto-CoT [[37](#bib.bib37)]
    proposes to prompt LLMs with Zero-shot CoT to generate reasoning rationales but
    finds that the generated rationales often contain mistakes. To mitigate the wrong
    demonstration issue, Auto-CoT proposes clustering and sampling the questions first
    and then applying simple heuristics to sample simpler questions and rationales
    to construct demonstrations. Synthetic Prompting [[38](#bib.bib38)] is another
    typical solution for constructing demonstrations with LLMs, containing forward
    and backward processes. In the backward process of Synthetic Prompting, a topic
    word, a target complexity and a self-generated reasoning chain are used as conditions
    to generate the synthetic question, and the synthetic question is used in the
    forward process to generate the precise synthetic reasoning chain. Along with
    these studies focusing on demonstration generation with LLMs, ensemble methods
    are also effective for reasoning. For example, self-consistency decoding [[40](#bib.bib40)]
    first generates a set of candidate outputs from the LLM with different sampling
    methods, such as temperature sampling, and then aggregates the sampled outputs
    and uses the most consistent output as the final result. Multi-Chain Reasoning [[41](#bib.bib41)]
    is another ensemble method mixing information from multiple reasoning chains.
    Different from studies [[40](#bib.bib40)] to ensemble results of multi-reasoning
    chains, Multi-Chain Reasoning collects evidence from multiple reasoning chains
    and prompts the LLM to give the final answer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍了最近针对大语言模型（LLM）的提示方法，因为我们提出的基于LLM的GIoT系统应用了提示方法来增强LLM在不同任务中的能力。由于这些方法中的一些也已应用于Table-QA问题，本节中的某些方法可能与第[II-B](#S2.SS2
    "II-B Table Question Answering ‣ II Related Work ‣ Efficient Prompting for LLM-based
    Generative Internet of Things")节中讨论的研究有重叠。许多研究已证明，通过少量示例和中间推理步骤对LLM进行提示，可以激发LLM的推理能力，这通常被称为上下文学习（ICL）[[36](#bib.bib36)]和链式推理（CoT）[[14](#bib.bib14)]。尽管ICL和CoT很有用，但编写合适的提示示例并非易事，且耗时。因此，一些研究[[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39)]建议使用LLM生成示例。Auto-CoT[[37](#bib.bib37)]建议用零-shot
    CoT对LLM进行提示以生成推理理由，但发现生成的理由通常包含错误。为了减轻错误示例的问题，Auto-CoT建议先对问题进行聚类和采样，然后应用简单的启发式方法来采样更简单的问题和理由，从而构建示例。Synthetic
    Prompting[[38](#bib.bib38)]是另一种构建LLM示例的典型解决方案，包含前向和后向过程。在Synthetic Prompting的后向过程中，使用主题词、目标复杂度和自生成的推理链作为条件生成合成问题，而合成问题在前向过程中用于生成精确的合成推理链。除了这些关注于LLM示例生成的研究外，集成方法在推理中也有效。例如，自一致性解码[[40](#bib.bib40)]首先使用不同的采样方法（如温度采样）从LLM生成一组候选输出，然后聚合采样的输出，并使用最一致的输出作为最终结果。多链推理[[41](#bib.bib41)]是另一种混合多个推理链信息的集成方法。与研究[[40](#bib.bib40)]中多推理链结果集成不同，多链推理从多个推理链中收集证据，并提示LLM给出最终答案。
- en: In addition to these prompting methods, the integration of programming languages
    and other external tools holds great potential for overcoming the limitations
    of LLMs. For instance, LLMs often struggle with precise arithmetic calculations,
    especially division operations, and staying updated with the latest information.
    To address these issues, PoT [[13](#bib.bib13)], PAL [[12](#bib.bib12)], and many
    other studies [[42](#bib.bib42)] leverage Programming Language to assist the reasoning
    steps. Some studies [[43](#bib.bib43)] introduce the concept of Agent and call
    external tools by parsing the LLM outputs to enhance the LLM’s capabilities.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些提示方法之外，编程语言和其他外部工具的整合具有克服LLMs局限性的巨大潜力。例如，LLMs常常在精确的算术计算，特别是除法操作和保持最新信息方面遇到困难。为了解决这些问题，PoT [[13](#bib.bib13)]、PAL [[12](#bib.bib12)]以及许多其他研究 [[42](#bib.bib42)]利用编程语言来辅助推理步骤。一些研究 [[43](#bib.bib43)]引入了Agent的概念，通过解析LLM输出调用外部工具，以增强LLM的能力。
- en: III System Model
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 系统模型
- en: '![Refer to caption](img/609b9fea8b14994479d198e3db42ded4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/609b9fea8b14994479d198e3db42ded4.png)'
- en: 'Figure 2: Detailed workflow of the proposed LLM-based GIoT system.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：提出的基于LLM的GIoT系统的详细工作流程。
- en: As discussed in Section [I](#S1 "I Introduction ‣ Efficient Prompting for LLM-based
    Generative Internet of Things"), in this study, we consider the scenarios in which
    IoT devices cannot access commercial LLMs because of data privacy and security
    considerations, which is a practical setting in many institutions, such as hospitals.
    Alternatively, as shown in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Efficient
    Prompting for LLM-based Generative Internet of Things"), an open-source LLM can
    be deployed in a local edge server to process the requests from IoT devices. Because
    fine-tuning an LLM for a specific task requires substantial datasets and computational
    resources, limiting the scalability of extending to multiple tasks, limiting the
    scalability of extending to multiple tasks, we propose to use prompting methods
    to enhance the capacities for different tasks with a Prompt Management Module,
    a Post-processing Module and a Task-specific Prompts Database, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Efficient Prompting for LLM-based Generative Internet
    of Things"). For a text-based request from IoT devices, such as a translation
    task to a sentence, the proposed Prompt Management Module is responsible for parsing
    the request and searching the task-specific prompt template and demonstrations
    regarding the task, as steps 1, 2 and 3 in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Efficient Prompting for LLM-based Generative Internet of Things"). After obtaining
    the task-specific prompt template, the Prompt Management Module constructs the
    final prompt and sends the request to the LLM. In our design, the capacities to
    deal with new tasks can be easily extended to the system by adding new prompt
    templates and demonstrations in the Task-specific Prompt Database without training
    or fine-tuning to the LLM. Since the results generated by the LLM are sometimes
    not ideal, we propose a Post-processing Module to process the results further.
    For example, some methods must prompt the LLM model multiple times to obtain the
    final results, whose intermediate results and Optional Task Request should be
    processed by the Post-processing Module. Besides, programming language-aided prompting
    methods, such as PAL [[12](#bib.bib12)] and PoT [[13](#bib.bib13)], generate the
    Python code instead of the final results, which means that the Post-processing
    Module should also be responsible for executing the generated Python code to obtain
    the final results. It is worth mentioning that, in the proposed solution, the
    IoT devices and the edge server can be easily connected with Wi-Fi and other wireless
    network protocols, and the communication between them can be easily implemented
    with HTTP protocol.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[I](#S1 "I Introduction ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")节中讨论了，在本研究中，我们考虑了由于数据隐私和安全性问题，物联网设备无法访问商业LLM的场景，这在许多机构（如医院）中是一个实际情况。或者，如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")所示，可以在本地边缘服务器上部署一个开源LLM来处理来自物联网设备的请求。由于为特定任务微调LLM需要大量的数据集和计算资源，这限制了扩展到多个任务的可扩展性，我们建议使用提示方法通过一个提示管理模块、一个后处理模块和一个任务特定提示数据库来增强不同任务的能力，如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")所示。对于来自物联网设备的基于文本的请求，如句子的翻译任务，提议的提示管理模块负责解析请求并搜索与任务相关的提示模板和演示，如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")中的步骤1、2和3所示。获得任务特定的提示模板后，提示管理模块构建最终提示并将请求发送给LLM。在我们的设计中，通过在任务特定提示数据库中添加新的提示模板和演示，可以轻松扩展处理新任务的能力，而无需对LLM进行训练或微调。由于LLM生成的结果有时并不理想，我们提出了一个后处理模块来进一步处理结果。例如，一些方法必须多次提示LLM模型以获得最终结果，其间的结果和可选任务请求应由后处理模块处理。此外，编程语言辅助的提示方法，如PAL [[12](#bib.bib12)]和PoT [[13](#bib.bib13)]，生成的是Python代码而不是最终结果，这意味着后处理模块还需负责执行生成的Python代码以获得最终结果。值得一提的是，在提议的解决方案中，物联网设备和边缘服务器可以通过Wi-Fi和其他无线网络协议轻松连接，它们之间的通信可以通过HTTP协议轻松实现。
- en: 'Following these discussed steps, Figure [2](#S3.F2 "Figure 2 ‣ III System Model
    ‣ Efficient Prompting for LLM-based Generative Internet of Things") shows the
    detailed components of the proposed two modules and the detailed workflow of the
    proposed GIoT system. Specifically, the Prompt Management Module consists of three
    components: Request Parsing, Prompt Search, and Prompt Generation. The Request
    Parsing component receives and parses the requests from the IoT devices and outputs
    Task ID, Task Step and parsed Data, in which Task ID and Task Step would be used
    in the Prompt Search component to search corresponding Prompt Instructions and
    Prompt Demonstrations from the Task Specific Prompts Database. The parsed Data
    generated by the Request Parsing component is the information that needs to be
    further processed by the LLM. For example, for a service of translating English
    to Chinese, the request from the IoT devices can be ”Task Name: Translation. Data:
    This is a sample translation service.” Then the Data parsed by the Request Parsing
    component should be ”This is a sample translation service.”, which would be used
    by the Prompt Generation component to generate the final Task Prompt together
    with the Prompt Instruction and Prompt Demonstrations. It is worth mentioning
    that Prompt Generation can implement customize demonstration selection methods
    to optimize the performance of applying In Context Learning (ICL) [[36](#bib.bib36)].
    For the Post-processing Module, since the output of the LLM cannot always follow
    the instructions, a Result Parsing function is needed to refine the outputs. For
    single-stage prompting methods, the result generated by the Result Parsing function
    can be the final result returned to the IoT device, as the path 1 shown in Figure [2](#S3.F2
    "Figure 2 ‣ III System Model ‣ Efficient Prompting for LLM-based Generative Internet
    of Things"). By contrast, prompting methods, such as PoT [[13](#bib.bib13)] and
    PAL [[12](#bib.bib12)], generate Python code and conduct the reasoning steps by
    an external Python Interpreter. Therefore, a Code Execution component should be
    included in the Post-processing Module. At last, a Optional Request Management
    component need to be implemented for the prompting methods with multiple-stages.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '按照这些讨论的步骤，图[2](#S3.F2 "Figure 2 ‣ III System Model ‣ Efficient Prompting for
    LLM-based Generative Internet of Things")展示了提议的两个模块的详细组件和GIoT系统的详细工作流程。具体来说，Prompt
    Management Module（提示管理模块）包括三个组件：Request Parsing（请求解析）、Prompt Search（提示搜索）和Prompt
    Generation（提示生成）。Request Parsing组件接收并解析来自IoT设备的请求，输出Task ID（任务ID）、Task Step（任务步骤）和解析后的Data（数据），其中Task
    ID和Task Step将用于Prompt Search组件，从Task Specific Prompts Database（任务特定提示数据库）中搜索相应的Prompt
    Instructions（提示说明）和Prompt Demonstrations（提示演示）。Request Parsing组件生成的解析数据是LLM需要进一步处理的信息。例如，对于一个英语到中文的翻译服务，来自IoT设备的请求可能是“Task
    Name: Translation. Data: This is a sample translation service.” 然后Request Parsing组件解析出的数据应为“This
    is a sample translation service.”，该数据将与Prompt Instruction和Prompt Demonstrations一起用于Prompt
    Generation组件生成最终的Task Prompt。值得一提的是，Prompt Generation可以实现自定义演示选择方法，以优化应用In Context
    Learning（ICL）的性能[[36](#bib.bib36)]。对于Post-processing Module（后处理模块），由于LLM的输出无法总是遵循指令，因此需要Result
    Parsing功能来精炼输出。对于单阶段提示方法，Result Parsing功能生成的结果可以是返回给IoT设备的最终结果，如图[2](#S3.F2 "Figure
    2 ‣ III System Model ‣ Efficient Prompting for LLM-based Generative Internet of
    Things")中路径1所示。相比之下，如PoT[[13](#bib.bib13)]和PAL[[12](#bib.bib12)]等提示方法生成Python代码并通过外部Python解释器进行推理步骤。因此，Post-processing
    Module中应包含一个Code Execution组件。最后，对于多阶段提示方法，需要实现一个Optional Request Management组件。'
- en: IV Case Study of Table-QA for the LLM-based GIoT System
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 案例研究：基于LLM的GIoT系统中的表格问答
- en: In this section, we use the Table-QA task as a case study to demonstrate the
    proposed LLM-based GIoT System. We first provide the problem formulation and examine
    the challenging aspects of the Table-QA task in Section [IV-A](#S4.SS1 "IV-A Problem
    Formulation and Analysis ‣ IV Case Study of Table-QA for the LLM-based GIoT System
    ‣ Efficient Prompting for LLM-based Generative Internet of Things"). Then, we
    describe our proposed three-stage prompting method in detail in Section [IV-B](#S4.SS2
    "IV-B Proposed Prompting Method for Table-QA ‣ IV Case Study of Table-QA for the
    LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things").
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用表格问答任务作为案例研究来展示所提议的基于LLM的GIoT系统。我们首先在第[IV-A](#S4.SS1 "IV-A Problem
    Formulation and Analysis ‣ IV Case Study of Table-QA for the LLM-based GIoT System
    ‣ Efficient Prompting for LLM-based Generative Internet of Things")节中提供问题的公式化，并检视表格问答任务的挑战性方面。然后，我们在第[IV-B](#S4.SS2
    "IV-B Proposed Prompting Method for Table-QA ‣ IV Case Study of Table-QA for the
    LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")节中详细描述了我们提议的三阶段提示方法。
- en: IV-A Problem Formulation and Analysis
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 问题定义与分析
- en: '![Refer to caption](img/2b7602aa42a8e679b244941214eaff68.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2b7602aa42a8e679b244941214eaff68.png)'
- en: 'Figure 3: Comparison of CoT, PoT and the proposed method. It is worth mentioning
    that many details are omitted due to space limitations. The proposed method contains
    task-planning, task-conducting, and task-correction stages, and it uses a statistical
    table and sub-tables in these stages to avoid the original huge tables.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：CoT、PoT和提议方法的比较。值得一提的是，由于空间限制，许多细节被省略。提议的方法包含任务规划、任务执行和任务修正阶段，并且在这些阶段中使用统计表和子表来避免原始的庞大表格。
- en: 'For a Table-QA service deployed in the proposed LLM-based GIoT system, it contains
    two key components: a parametric a LLM with parameters $\theta$.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在提议的基于LLM的GIoT系统中部署的Table-QA服务包含两个关键组件：一个带有参数$\theta$的参数化LLM。
- en: As mentioned in Section [I](#S1 "I Introduction ‣ Efficient Prompting for LLM-based
    Generative Internet of Things"), many prompting methods have been proposed to
    enhance LLMs’ capacities. For example, Chain of Thought (CoT) [[14](#bib.bib14)]
    is a popular prompting method providing reasoning rationales to elicit LLMs’ reasoning
    capacities. PAL [[12](#bib.bib12)] and PoT [[13](#bib.bib13)] propose to offload
    the reasoning steps to Python codes. However, these typical prompting methods
    cannot perform well in the semi-structured Table-QA problem because of the complex
    structures, heterogeneous data types, and sometimes huge tables with numerous
    columns and rows. More specifically, prompting methods without applying programming
    languages must first extract the correct information from semi-structured tables
    before reasoning, which is challenging for LLMs, especially when the table is
    huge [[30](#bib.bib30)]. Figure [3](#S4.F3 "Figure 3 ‣ IV-A Problem Formulation
    and Analysis ‣ IV Case Study of Table-QA for the LLM-based GIoT System ‣ Efficient
    Prompting for LLM-based Generative Internet of Things") contains a failure example
    of CoT, which interprets 1936/37 as two years and fails to extract the correct
    Year 1953/54. Similarly, programming-aided solutions, such as PAL and PoT, can
    also suffer from this information extraction issue if we define the relevant information
    as Python variables. Applying Pandas Library [[44](#bib.bib44), [45](#bib.bib45)]
    can alleviate this information extraction issue [[46](#bib.bib46)] by providing
    proper selection criteria, but introducing extra difficulties caused by the heterogeneous
    data types. Figure [3](#S4.F3 "Figure 3 ‣ IV-A Problem Formulation and Analysis
    ‣ IV Case Study of Table-QA for the LLM-based GIoT System ‣ Efficient Prompting
    for LLM-based Generative Internet of Things") shows a PoT example using Pandas
    Library, which fails to run because the values in column Year cannot be directly
    compared with 1936. Besides, complex structures of semi-structured tables can
    often lead to wrong results, especially for program-aided solutions. Figure [4(a)](#S4.F4.sf1
    "In Figure 4 ‣ IV-A Problem Formulation and Analysis ‣ IV Case Study of Table-QA
    for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things") shows an example from WikiTableQA [[47](#bib.bib47)] dataset, which
    contains several spanning cells across multiple columns and rows, and inconsistent
    data types, such as the column Season. Even though some methods [[48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50)] can transform this table into a standard table
    by repeating table spanning cell into multiple single table cells so that SQL
    or Python Pandas library can process it, its structure still can lead to wrong
    results. For example, when an LLM is prompted to generate Python code to answer
    the question ”What is the maximum League Apps after 2004?”. Two Totals in the
    column Season will be compared with correct Seasons, which leads to wrong results.
    Besides, the generated code needs to compare the values from the column Season,
    which can lead to an error of execution because  operation cannot be applied to the string ”2011-12” and
    the integer 2004, as highlighted in Figure [4(b)](#S4.F4.sf2 "In Figure 4 ‣ IV-A
    Problem Formulation and Analysis ‣ IV Case Study of Table-QA for the LLM-based
    GIoT System ‣ Efficient Prompting for LLM-based Generative Internet of Things"),
    which is another failure example caused by the heterogeneous data types. Along
    with applying Python to enhance the LLMs, some studies [[35](#bib.bib35), [31](#bib.bib31)]
    apply SQL to the Table-QA problem. However, SQL is a programming language designed
    for structured tables, meaning that semi-structured tables need to be transformed
    into structured format first so that tables can be imported into the relational
    databases to execute SQL queries, which is another challenging task for the semi-structured
    Table-QA problem discussed in this study. Besides, as pointed out by some studies [[51](#bib.bib51),
    [35](#bib.bib35)], some questions are not answerable by merely using SQL. Therefore,
    considering the flexibility of Python and the limitations of applying SQL to the
    semi-structured Table-QA task, we apply Python instead of SQL in our solution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[I](#S1 "I Introduction ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")节所述，已经提出了许多提示方法来增强大型语言模型（LLMs）的能力。例如，链式思维（Chain of Thought, CoT）[[14](#bib.bib14)]是一种流行的提示方法，通过提供推理理由来激发LLMs的推理能力。PAL
    [[12](#bib.bib12)]和PoT [[13](#bib.bib13)]提议将推理步骤转移到Python代码中。然而，这些典型的提示方法在半结构化的表格问答问题中表现不佳，因为这些表格具有复杂的结构、多样的数据类型，并且有时包含大量的列和行。更具体来说，没有应用编程语言的提示方法必须首先从半结构化表格中提取正确的信息再进行推理，这对于LLMs来说具有挑战性，特别是当表格非常庞大时[[30](#bib.bib30)]。图[3](#S4.F3
    "Figure 3 ‣ IV-A Problem Formulation and Analysis ‣ IV Case Study of Table-QA
    for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")展示了一个CoT的失败示例，该示例将1936/37解释为两个年份，未能提取正确的年份1953/54。同样，像PAL和PoT这样的编程辅助解决方案，如果我们将相关信息定义为Python变量，也可能遭遇此信息提取问题。应用Pandas库[[44](#bib.bib44),
    [45](#bib.bib45)]可以通过提供适当的选择标准来缓解此信息提取问题[[46](#bib.bib46)]，但会引入由于数据类型异构性带来的额外困难。图[3](#S4.F3
    "Figure 3 ‣ IV-A Problem Formulation and Analysis ‣ IV Case Study of Table-QA
    for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")展示了一个使用Pandas库的PoT示例，该示例由于列“Year”中的值无法与1936直接比较而无法运行。此外，半结构化表格的复杂结构经常导致错误结果，尤其是对于程序辅助的解决方案。图[4(a)](#S4.F4.sf1
    "In Figure 4 ‣ IV-A Problem Formulation and Analysis ‣ IV Case Study of Table-QA
    for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")展示了一个来自WikiTableQA [[47](#bib.bib47)]数据集的示例，该示例包含了跨越多个列和行的跨单元格以及不一致的数据类型，例如列“Season”。即使一些方法[[48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50)]可以通过将表格跨单元格重复成多个单一表格单元格来将此表格转换为标准表格，以便SQL或Python
    Pandas库可以处理，但其结构仍然可能导致错误结果。例如，当LLM被提示生成Python代码以回答“2004年后最大的League Apps是什么？”的问题时，列“Season”中的两个“Total”将与正确的“Seasons”进行比较，这将导致错误结果。此外，生成的代码需要比较“Season”列中的值，这可能导致执行错误，因为操作无法应用于字符串“2011-12”和整数2004，如图[4(b)](#S4.F4.sf2
    "In Figure 4 ‣ IV-A Problem Formulation and Analysis ‣ IV Case Study of Table-QA
    for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")所示，这是另一个由于数据类型异构性导致的失败示例。除了应用Python来增强LLMs外，一些研究[[35](#bib.bib35),
    [31](#bib.bib31)]将SQL应用于表格问答问题。然而，SQL是一种针对结构化表格设计的编程语言，这意味着半结构化表格需要首先转换为结构化格式，以便将表格导入关系数据库执行SQL查询，这对半结构化的表格问答问题来说是另一项具有挑战性的任务。此外，正如一些研究[[51](#bib.bib51),
    [35](#bib.bib35)]指出的那样，一些问题仅使用SQL无法回答。因此，考虑到Python的灵活性和SQL在半结构化表格问答任务中的局限性，我们在解决方案中选择应用Python而不是SQL。
- en: '![Refer to caption](img/67f7510098271f4aedbde0690f84e4a2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/67f7510098271f4aedbde0690f84e4a2.png)'
- en: (a) A sample table with a complex structure from the WikiTableQA dataset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 来自 WikiTableQA 数据集的一个具有复杂结构的示例表格。
- en: '![Refer to caption](img/43691a35a30b08cd96d7ac2e98fee3ca.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/43691a35a30b08cd96d7ac2e98fee3ca.png)'
- en: (b) A sample of generated Python code to answer the question What is the maximum
    League Apps after 2004?.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 一个生成的 Python 代码示例，用于回答问题：2004 年之后的最大 League Apps 是什么？
- en: 'Figure 4: An example of a semi-structured table and its failed Python code
    because of the heterogeneous data types and table’s complex structure. The Python
    code is generated by Mixtral-8x7B.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：一个半结构化表格的示例及其失败的 Python 代码，原因是数据类型异质性和表格复杂结构。Python 代码由 Mixtral-8x7B 生成。
- en: Besides the issues of applying prompting methods to the semi-structured Table-QA
    problem, current studies only focus on optimizing the prediction accuracy without
    considering their inference cost, which is critical for our proposed GIoT system.
    As mentioned, some tables can be huge, which can lead to long prompts and inference
    time. Even though some solutions [[52](#bib.bib52), [31](#bib.bib31)] propose
    decomposing huge tables into sub-tables for further reasoning, they need to prompt
    the full huge table into the LLM first. Some studies [[30](#bib.bib30)] truncate
    the huge tables, which can drastically reduce the inference cost but introduce
    the risk of losing critical information in the tables and sometimes make it impossible
    to give the correct answer. Besides, ensemble methods, such as self-consistency
    decoding and majority voting, are often employed to improve the performance further [[40](#bib.bib40),
    [31](#bib.bib31), [52](#bib.bib52)], but also drastically increasing the inference
    cost simultaneously.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将提示方法应用于半结构化 Table-QA 问题的挑战外，目前的研究仅关注优化预测准确性，而没有考虑推理成本，这对我们提出的 GIoT 系统至关重要。如前所述，有些表格可能非常庞大，这可能导致长提示和推理时间。即便有些解决方案 [[52](#bib.bib52),
    [31](#bib.bib31)] 提出将庞大表格分解成子表格以进行进一步推理，但仍需将整个庞大表格输入 LLM。一些研究 [[30](#bib.bib30)]
    切断庞大表格，这可以大幅降低推理成本，但会引入丢失表格关键信息的风险，有时使得正确答案无法提供。此外，集成方法，如自一致解码和多数投票，通常被用于进一步提高性能 [[40](#bib.bib40),
    [31](#bib.bib31), [52](#bib.bib52)]，但也会同时大幅增加推理成本。
- en: Lastly, most of these studies are based on In Context Learning (ICL), using
    demonstrations to guide the LLM in conducting target tasks, while crafting and
    selecting proper demonstrations is still an open issue. Many studies [[53](#bib.bib53),
    [54](#bib.bib54)] pointed out that the content, number and order of demonstration
    can all influence the results. Therefore, we define a series of atomic operations
    to measure the complexity of a query question to the given table and describe
    the steps with the defined atomic operations, which can be a metric for the demonstration
    selection when crafting prompts. To mitigate the issues limiting the performance
    of LLMs, including the complex table structure, heterogeneous data types, huge
    tables, and the inherent limitations of LLMs in reasoning capacities, we propose
    a three-stage prompting solution containing task-planning, task-conducting and
    task-correction stages, as shown in Figure [3](#S4.F3 "Figure 3 ‣ IV-A Problem
    Formulation and Analysis ‣ IV Case Study of Table-QA for the LLM-based GIoT System
    ‣ Efficient Prompting for LLM-based Generative Internet of Things"). The task-planning
    stage prompts the LLM to analyze the statistical information of the given table
    and provide programming steps, data requirements, and relevant columns to solve
    the query question and generate a plan. Then, the task-conducting stage first
    generates a default answer as the final answer when the generated Python code
    fails to execute and then generates the Python code based on the plan from the
    first stage. When a task-conducting stage fails to execute, the heterogeneous
    data types are usually the reason. Therefore, we also include a task-correction
    stage, which can generate normalization functions to normalize the data and correct
    the error in the task-conducting stage. It is worth mentioning that our proposed
    method can avoid huge tables as a part of the prompt, which can drastically reduce
    the number of prompting tokens when the tables are huge.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些研究大多基于上下文学习（ICL），通过示例引导大型语言模型（LLM）执行目标任务，但制定和选择合适的示例仍然是一个未解决的问题。许多研究 [[53](#bib.bib53),
    [54](#bib.bib54)] 指出，示例的内容、数量和顺序都能影响结果。因此，我们定义了一系列原子操作来测量查询问题对给定表格的复杂性，并用定义的原子操作描述这些步骤，这可以作为制定提示时示例选择的一个指标。为了缓解限制LLMs性能的问题，包括复杂的表格结构、异构数据类型、大型表格以及LLMs在推理能力上的固有限制，我们提出了一个包含任务规划、任务执行和任务修正三个阶段的三阶段提示解决方案，如图[3](#S4.F3
    "Figure 3 ‣ IV-A Problem Formulation and Analysis ‣ IV Case Study of Table-QA
    for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")所示。任务规划阶段提示LLM分析给定表格的统计信息，并提供编程步骤、数据需求和相关列以解决查询问题并生成计划。然后，任务执行阶段在生成的Python代码无法执行时首先生成一个默认答案作为最终答案，然后根据第一阶段的计划生成Python代码。当任务执行阶段无法执行时，异构数据类型通常是原因。因此，我们还包括一个任务修正阶段，可以生成规范化函数来规范数据并纠正任务执行阶段的错误。值得一提的是，我们提出的方法可以避免将大型表格作为提示的一部分，这可以在表格很大时显著减少提示标记的数量。
- en: IV-B Proposed Prompting Method for Table-QA
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 提出的表格问答提示方法
- en: IV-B1 Overall Workflow
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 整体工作流程
- en: '![Refer to caption](img/eb828aaa801893bee27d52cf3fd94085.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb828aaa801893bee27d52cf3fd94085.png)'
- en: 'Figure 5: The workflow of the proposed prompting solution. Notably, the question
    and the table are from the request of an IoT device. The Python interpreter is
    in the Post-processing Module, and the stages of selecting demonstrations and
    creating prompts are in the Prompt Management Module.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：提出的提示解决方案的工作流程。值得注意的是，问题和表格来自于物联网设备的请求。Python 解释器位于后处理模块，而选择示例和创建提示的阶段则在提示管理模块中。
- en: As discussed in Sections [I](#S1 "I Introduction ‣ Efficient Prompting for LLM-based
    Generative Internet of Things") and  [IV-A](#S4.SS1 "IV-A Problem Formulation
    and Analysis ‣ IV Case Study of Table-QA for the LLM-based GIoT System ‣ Efficient
    Prompting for LLM-based Generative Internet of Things"), we use the challenging
    Table-QA task as the case study of the proposed LLM-based GIoT system and propose
    to prompt open-source LLMs to generate Python code for the semi-structured Table-QA
    problem to mitigate the issues caused by complex table structures, heterogeneous
    data types, huge tables, and limitations of LLMs. Specifically, we propose a three-stage
    prompting method, including task-planning, task-conducting and task-correction
    stages. The proposed workflow is shown in Figure [5](#S4.F5 "Figure 5 ‣ IV-B1
    Overall Workflow ‣ IV-B Proposed Prompting Method for Table-QA ‣ IV Case Study
    of Table-QA for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based
    Generative Internet of Things"), in which the question is the total number of
    points scored by the tide in the last 3 games combined. To answer this question,
    the statistics table of the given table contains information regarding the column
    names, data types, and the first and last entries, which are first generated as
    a part of the task-planning prompt, together with instructions, demonstrations,
    and questions, as shown in Figure [6](#S4.F6 "Figure 6 ‣ IV-B2 Demonstration Crafting
    and Selection ‣ IV-B Proposed Prompting Method for Table-QA ‣ IV Case Study of
    Table-QA for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative
    Internet of Things"). Then, the task-planning prompt is fed into the open-source
    LLM to make the reasoning plan, including Relevant Columns, Operations, and Programming
    Steps. It is worth mentioning that the statistics table can be far more compact
    than the original table when the original table is huge, which can reduce the
    number of prompt tokens. With the Relevant Columns, Operations and Programming
    Steps from the results of the task-planning step, the Relevant Columns are used
    to extract the contents of these columns, the Operations are used to select the
    demonstrations, and the Programming Steps are parts of the instructions to guide
    the reasoning function generation. With these processed results, the task-conducting
    prompt is constructed and fed into the open-source LLM to generate the reasoning
    Python function and a default answer, in which the default is treated as the final
    answer if the code fails to run. Notably, since we use the Pandas library in our
    solution, the original table needs to be represented as a dictionary of the list
    and fed into the generated function as a parameter. As running the generated Python
    code is almost cost-free compared with LLM inference, and some tables do not need
    to be normalized, we try to execute the reasoning function first. If there is
    no error from the execution, then the result of the execution should be the final
    result. However, if the execution fails, we construct the task-correction prompt
    containing the content of Relevant columns and reasoning code to generate the
    normalization function, apply the normalization to the original table, and then
    feed the normalized original table as the parameter to the reasoning function
    to run it again. For the example in Figure [5](#S4.F5 "Figure 5 ‣ IV-B1 Overall
    Workflow ‣ IV-B Proposed Prompting Method for Table-QA ‣ IV Case Study of Table-QA
    for the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things"), we need to extract the string ”W 21-14”, ”L 23-24” and ”W 24-17”
    first and then extract the scores ”21”, ”23”, ”24” and convert them into integers,
    which beyond the capacities of the LLM. Therefore, the first run of the reasoning
    function would fail, even though its reasoning logic is correct. While the normalization
    function can correctly extract and convert the points from the string to integers,
    the second run of the reasoning function should be successful.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[I](#S1 "I 引言 ‣ 针对基于LLM的生成型物联网的高效提示")节和第[IV-A](#S4.SS1 "IV-A 问题定义与分析 ‣ IV
    基于LLM的GIoT系统的Table-QA案例研究 ‣ 针对基于LLM的生成型物联网的高效提示")节中讨论了，我们使用具有挑战性的Table-QA任务作为所提议的基于LLM的GIoT系统的案例研究，并建议通过提示开源LLM生成Python代码，以解决复杂表结构、异构数据类型、巨大表格以及LLM的局限性所造成的问题。具体来说，我们提出了一种三阶段的提示方法，包括任务规划、任务执行和任务纠正阶段。所提出的工作流程如图[5](#S4.F5
    "图5 ‣ IV-B1 总体工作流程 ‣ IV-B 提出的Table-QA提示方法 ‣ IV 基于LLM的GIoT系统的Table-QA案例研究 ‣ 针对基于LLM的生成型物联网的高效提示")所示，其中问题是潮汐在最后三场比赛中总共得分的数量。为了回答这个问题，给定表格的统计表包含有关列名、数据类型以及首尾条目的信息，这些信息首先生成作为任务规划提示的一部分，连同说明、示例和问题，如图[6](#S4.F6
    "图6 ‣ IV-B2 示例制作与选择 ‣ IV-B 提出的Table-QA提示方法 ‣ IV 基于LLM的GIoT系统的Table-QA案例研究 ‣ 针对基于LLM的生成型物联网的高效提示")所示。然后，任务规划提示被输入到开源LLM中，以制定推理计划，包括相关列、操作和编程步骤。值得一提的是，当原始表格非常庞大时，统计表可以比原始表格更为紧凑，这可以减少提示令牌的数量。利用任务规划步骤的相关列、操作和编程步骤，相关列用于提取这些列的内容，操作用于选择示例，而编程步骤是指导推理函数生成的说明的一部分。利用这些处理结果，构建任务执行提示并输入到开源LLM中，以生成推理Python函数和默认答案，其中如果代码运行失败，则默认答案被视为最终答案。值得注意的是，由于我们在解决方案中使用了Pandas库，因此原始表格需要表示为列表字典，并作为参数传递到生成的函数中。由于运行生成的Python代码几乎没有成本，而一些表格无需归一化，我们尝试首先执行推理函数。如果执行没有错误，则执行结果应该是最终结果。然而，如果执行失败，我们构建包含相关列内容和推理代码的任务纠正提示，以生成归一化函数，应用归一化到原始表格，然后将归一化后的原始表格作为参数传递给推理函数再次运行。以图[5](#S4.F5
    "图5 ‣ IV-B1 总体工作流程 ‣ IV-B 提出的Table-QA提示方法 ‣ IV 基于LLM的GIoT系统的Table-QA案例研究 ‣ 针对基于LLM的生成型物联网的高效提示")中的示例为例，我们首先需要提取字符串”W
    21-14”、”L 23-24”和”W 24-17”，然后提取分数”21”、“23”、“24”并将其转换为整数，这超出了LLM的能力。因此，推理函数的第一次运行将会失败，即使其推理逻辑是正确的。而归一化函数可以正确地从字符串中提取和转换分数为整数，第二次运行推理函数应该会成功。
- en: IV-B2 Demonstration Crafting and Selection
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 示例制作与选择
- en: '![Refer to caption](img/8cd11925f6dc994e64716a4166d839cd.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8cd11925f6dc994e64716a4166d839cd.png)'
- en: 'Figure 6: The task-planning prompt. The defined operations and the statistics
    table are highlighted with green and yellow. $<$ are from the table to be analyzed.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：任务规划提示。定义的操作和统计表用绿色和黄色突出显示。$<$来自待分析的表。
- en: As shown in Figure [5](#S4.F5 "Figure 5 ‣ IV-B1 Overall Workflow ‣ IV-B Proposed
    Prompting Method for Table-QA ‣ IV Case Study of Table-QA for the LLM-based GIoT
    System ‣ Efficient Prompting for LLM-based Generative Internet of Things"), three
    prompts need to be constructed to answer a question; the demonstrations used in
    these three prompts are critical for the LLM’s performance, especially for the
    task-conducting stage to generate the reasoning function. Therefore, we defined
    a series of atomic operations to describe the reasoning logic to answer questions,
    as shown in Table [I](#S4.T1 "TABLE I ‣ IV-B2 Demonstration Crafting and Selection
    ‣ IV-B Proposed Prompting Method for Table-QA ‣ IV Case Study of Table-QA for
    the LLM-based GIoT System ‣ Efficient Prompting for LLM-based Generative Internet
    of Things"). For the task-planning step, we craft a question-and-answer pair for
    each type of operation and instruct the LLM to generate the Relevant Columns,
    Operations and Programming Steps, as shown in Figure [6](#S4.F6 "Figure 6 ‣ IV-B2
    Demonstration Crafting and Selection ‣ IV-B Proposed Prompting Method for Table-QA
    ‣ IV Case Study of Table-QA for the LLM-based GIoT System ‣ Efficient Prompting
    for LLM-based Generative Internet of Things"). For the task-conducting prompts,
    we construct two question and reasoning function pairs for each defined operation
    and use operation as the metric to select these pairs. Even though we prompt the
    LLM to select one defined operation as output, the LLM can generate results without
    following the instructions. Therefore, when the Operations cannot be matched,
    the default setting contains all the question and function pairs. Figure [7](#S4.F7
    "Figure 7 ‣ IV-B2 Demonstration Crafting and Selection ‣ IV-B Proposed Prompting
    Method for Table-QA ‣ IV Case Study of Table-QA for the LLM-based GIoT System
    ‣ Efficient Prompting for LLM-based Generative Internet of Things") shows an example
    of COUNT operation, which includes a Meta Information table, Column Details and
    two questions with their Python solutions and default answers. It is worth mentioning
    that the demonstrations and prompts discussed in this section are stored in the
    Task-specific Prompts Database, and using operations as metrics to select proper
    demonstrations is the function of the proposed Prompt Management Module, as shown
    in Figures [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Efficient Prompting for LLM-based
    Generative Internet of Things") and  [2](#S3.F2 "Figure 2 ‣ III System Model ‣
    Efficient Prompting for LLM-based Generative Internet of Things").
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[5](#S4.F5 "Figure 5 ‣ IV-B1 Overall Workflow ‣ IV-B Proposed Prompting Method
    for Table-QA ‣ IV Case Study of Table-QA for the LLM-based GIoT System ‣ Efficient
    Prompting for LLM-based Generative Internet of Things")所示，回答一个问题需要构建三个提示；这三个提示中使用的示例对LLM的表现至关重要，尤其是在任务执行阶段生成推理功能。因此，我们定义了一系列原子操作来描述回答问题的推理逻辑，如表[I](#S4.T1
    "TABLE I ‣ IV-B2 Demonstration Crafting and Selection ‣ IV-B Proposed Prompting
    Method for Table-QA ‣ IV Case Study of Table-QA for the LLM-based GIoT System
    ‣ Efficient Prompting for LLM-based Generative Internet of Things")所示。对于任务规划步骤，我们为每种操作类型制定一个问答对，并指示LLM生成相关列、操作和编程步骤，如图[6](#S4.F6
    "Figure 6 ‣ IV-B2 Demonstration Crafting and Selection ‣ IV-B Proposed Prompting
    Method for Table-QA ‣ IV Case Study of Table-QA for the LLM-based GIoT System
    ‣ Efficient Prompting for LLM-based Generative Internet of Things")所示。对于任务执行提示，我们为每个定义的操作构建两个问题和推理功能对，并使用操作作为指标来选择这些对。尽管我们提示LLM选择一个定义的操作作为输出，LLM仍然可以在不遵循指令的情况下生成结果。因此，当操作无法匹配时，默认设置包含所有问题和功能对。图[7](#S4.F7
    "Figure 7 ‣ IV-B2 Demonstration Crafting and Selection ‣ IV-B Proposed Prompting
    Method for Table-QA ‣ IV Case Study of Table-QA for the LLM-based GIoT System
    ‣ Efficient Prompting for LLM-based Generative Internet of Things")展示了COUNT操作的示例，包括一个元信息表、列详细信息以及两个问题及其Python解决方案和默认答案。值得一提的是，本节讨论的示例和提示存储在任务特定的提示数据库中，使用操作作为指标来选择合适的示例是建议的提示管理模块的功能，如图[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Efficient Prompting for LLM-based Generative Internet
    of Things")和[2](#S3.F2 "Figure 2 ‣ III System Model ‣ Efficient Prompting for
    LLM-based Generative Internet of Things")所示。
- en: '![Refer to caption](img/081ce61354654153046240514b539dec.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/081ce61354654153046240514b539dec.png)'
- en: 'Figure 7: The task-conducting prompt. The Meta Information and Column Details
    are highlighted with yellow, and the default answers are highlighted with blue.
    $<$ are from the table to be analyzed.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 任务执行提示。元信息和列细节用黄色高亮显示，默认答案用蓝色高亮显示。 $<$ 来自待分析的表格。'
- en: 'TABLE I: Defined operations for Demonstration selection for code generation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 用于代码生成的演示选择定义的操作。'
- en: '| Operation Type | Operation Name | Description |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 操作类型 | 操作名称 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Reasoning | SelectTable | select a cell from the table based on a criteria
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | SelectTable | 根据标准从表格中选择一个单元格 |'
- en: '| ADDITION/DIFF | addition or subtraction |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| ADDITION/DIFF | 加法或减法 |'
- en: '| TIMES/DIVISION | production or quotient of two numbers |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| TIMES/DIVISION | 两个数字的乘积或商 |'
- en: '| AVG | average of several numbers |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| AVG | 多个数字的平均值 |'
- en: '| COUNT | count the number based on a criteria |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| COUNT | 根据标准计数 |'
- en: '| MAX/MIN | select the maximum/minimum one from given numbers |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| MAX/MIN | 从给定数字中选择最大/最小值 |'
- en: V Experiments and Analysis
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 实验与分析
- en: V-A Datasets and Experimental Settings
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 数据集和实验设置
- en: We evaluate our proposed prompting solution on WikiTableQA [[47](#bib.bib47)]
    and TabFact [[55](#bib.bib55)] datasets, which are two datasets created by Wiki-tables
    without text context. WikiTableQA mainly contains compositional questions, such
    as questions requiring counting and ranking table contents. TabFact is a Fact
    Verification dataset, which can be treated as a special setting of a typical Table-QA
    problem whose answer set is $\{True,False\}$. Since the proposed solution of this
    study is based on ICL, which is a few-shot learning setting without any training
    stage, we only use the test set of these two datasets to evaluate the performance,
    which contains 4344 and 12828 QA pairs, respectively. TabFact dataset further
    categorizes the test set into simple and complex groups, which include 4219 and
    8609 QA pairs, respectively. Considering the large size of TabFact dataset, some
    studies [[31](#bib.bib31), [35](#bib.bib35)] conducted experiments one a small
    subset of TabFact, which contains 1,005 simple and 1,019 complex QA pairs. To
    compare with these studies, we also report the results on this small test subset
    of TabFact.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 WikiTableQA [[47](#bib.bib47)] 和 TabFact [[55](#bib.bib55)] 数据集上评估了我们提出的提示解决方案，这两个数据集由
    Wiki-tables 创建，且没有文本上下文。 WikiTableQA 主要包含组合性问题，如要求计数和排序表格内容的问题。 TabFact 是一个事实验证数据集，可以看作是一个典型表格问答问题的特殊设置，其答案集为
    $\{True,False\}$。由于本研究提出的解决方案基于 ICL，这是一种无需训练阶段的少量学习设置，我们仅使用这两个数据集的测试集来评估性能，这些测试集分别包含
    4344 和 12828 对问答。 TabFact 数据集进一步将测试集分类为简单和复杂两组，分别包括 4219 和 8609 对问答。考虑到 TabFact
    数据集的规模较大，一些研究 [[31](#bib.bib31), [35](#bib.bib35)] 在 TabFact 的一个小子集中进行了实验，该子集包含
    1005 对简单问答和 1019 对复杂问答。为了与这些研究进行比较，我们也报告了 TabFact 小测试子集上的结果。
- en: 'TABLE II: Experimental results on WikiTableQA dataset with Exact Match Accuracy
    as metric.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 使用精确匹配准确率作为度量的 WikiTableQA 数据集的实验结果。'
- en: '| LLM | Method | EM Acc |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 方法 | EM 准确率 |'
- en: '| Codex | Binder | 61.90 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Codex | Binder | 61.90 |'
- en: '| Dater | 65.90 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Dater | 65.90 |'
- en: '| Mixtral-8x7B | Direct | 53.08 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 直接 | 53.08 |'
- en: '| CoT | 53.48 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 53.48 |'
- en: '| PoT | 40.40 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 40.40 |'
- en: '| Tab-PoT | 63.33 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 63.33 |'
- en: '| Mistral-7B | Direct | 27.19 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 直接 | 27.19 |'
- en: '| CoT | 30.46 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 30.46 |'
- en: '| PoT | 27.66 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 27.66 |'
- en: '| Tab-PoT | 52.12 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 52.12 |'
- en: '| DeepSeek-67B | Direct | 54.72 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-67B | 直接 | 54.72 |'
- en: '| CoT | 55.57 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 55.57 |'
- en: '| PoT | 43.92 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 43.92 |'
- en: '| Tab-PoT | 66.78 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 66.78 |'
- en: '| DeepSeek-7B | Direct | 33.86 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-7B | 直接 | 33.86 |'
- en: '| CoT | 34.65 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 34.65 |'
- en: '| PoT | 19.61 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 19.61 |'
- en: '| Tab-PoT | 40.03 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 40.03 |'
- en: As the proposed LLM-based GIoT system is designed to be deployed in a local
    network, we use open-source LLMs, including Mixtral-8x7B ¹¹1https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1,
    Mistral-7B ²²2https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2, DeepSeek-67B ³³3https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat
    and DeepSeek-7B ⁴⁴4https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat to
    conduct our experiments. Since the proposed solution in this study is a prompt
    engineering method, we include Direct Prompting, CoT [[14](#bib.bib14)], PoT [[13](#bib.bib13)],
    Binder [[35](#bib.bib35)] and Dater [[31](#bib.bib31)] as benchmarks, in which
    Binder and Dater are two solutions leveraging SQL. For the implementation of benchmark
    methods, we use the implementation of TableCoT⁵⁵5https://github.com/wenhuchen/TableCoT [[30](#bib.bib30)]
    for the Direct Prompting and CoT. We re-implemented the PoT method following the
    example prompts reported in PoT [[13](#bib.bib13)]. The results of Dater and Binder
    are directly from study [[31](#bib.bib31)]. We use beam search decoding for the
    experiments. At last, even though we employ ICL to provide demonstrations to guide
    the LLM output of the final results in a ”{},” sometimes LLMs can fail to follow
    this output format. Therefore, we use a simple answer alignment step to post-process
    the results with incorrect formats. More specifically, we employ a direct prompting
    method by providing a few demonstrations containing the question, answer and formatted
    answer following TableCoT [[30](#bib.bib30)]. It is worth mentioning that we use
    the default precision of parameters, namely bfloat16, for the LLMs in this section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提出的基于LLM的GIoT系统设计为在本地网络中部署，我们使用了开源LLM，包括Mixtral-8x7B ¹¹1https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1、Mistral-7B ²²2https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2、DeepSeek-67B ³³3https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat
    和DeepSeek-7B ⁴⁴4https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat进行实验。由于本研究提出的解决方案是提示工程方法，我们包含了Direct
    Prompting、CoT [[14](#bib.bib14)]、PoT [[13](#bib.bib13)]、Binder [[35](#bib.bib35)]
    和Dater [[31](#bib.bib31)]作为基准，其中Binder和Dater是两种利用SQL的解决方案。对于基准方法的实现，我们使用TableCoT⁵⁵5https://github.com/wenhuchen/TableCoT [[30](#bib.bib30)]的实现来进行Direct
    Prompting和CoT。我们根据PoT [[13](#bib.bib13)]中报告的示例提示重新实现了PoT方法。Dater和Binder的结果直接来自研究 [[31](#bib.bib31)]。我们在实验中使用了beam
    search解码。最后，尽管我们使用ICL提供演示以指导LLM输出最终结果的“{}”格式，但有时LLM可能无法遵循该输出格式。因此，我们使用一个简单的答案对齐步骤来后处理格式不正确的结果。更具体地说，我们通过提供几个包含问题、答案和格式化答案的演示来采用直接提示方法，按照TableCoT [[30](#bib.bib30)]。值得一提的是，我们在这一部分使用了LLM的默认参数精度，即bfloat16。
- en: We term our proposed prompting solution as Tab-PoT, and the experimental results
    are shown in Table [II](#S5.T2 "TABLE II ‣ V-A Datasets and Experimental Settings
    ‣ V Experiments and Analysis ‣ Efficient Prompting for LLM-based Generative Internet
    of Things") and Table [III](#S5.T3 "TABLE III ‣ V-A Datasets and Experimental
    Settings ‣ V Experiments and Analysis ‣ Efficient Prompting for LLM-based Generative
    Internet of Things"). The experimental results show that our proposed prompting
    solution can perform competitively compared with state-of-the-art methods. The
    Tab-PoT with DeepSeek-67B can achieve state-of-the-art performance, and the Tab-PoT
    with both Mixtral-8x7B and DeepSeek-67B can improve the original PoT method by
    at least 22% on the WikiTableQA dataset. Besides, applying LLMs with a larger
    number of parameters can significantly improve performance. The CoT does not show
    many benefits in improving performance compared with direct prompting on the WikiTableQA
    dataset while consistently improving the performance on the TabFact dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提出的提示解决方案称为Tab-PoT，实验结果见表 [II](#S5.T2 "TABLE II ‣ V-A Datasets and Experimental
    Settings ‣ V Experiments and Analysis ‣ Efficient Prompting for LLM-based Generative
    Internet of Things") 和表 [III](#S5.T3 "TABLE III ‣ V-A Datasets and Experimental
    Settings ‣ V Experiments and Analysis ‣ Efficient Prompting for LLM-based Generative
    Internet of Things")。实验结果表明，我们提出的提示解决方案在与最先进的方法相比时表现竞争力。Tab-PoT与DeepSeek-67B可以实现最先进的性能，而Tab-PoT与Mixtral-8x7B和DeepSeek-67B的结合可以使原始PoT方法在WikiTableQA数据集上的性能提高至少22%。此外，使用参数更多的LLM可以显著提升性能。CoT在WikiTableQA数据集上相比于直接提示并未显示出显著的性能提升，但在TabFact数据集上性能持续提升。
- en: 'TABLE III: Experimental results on TabFact dataset with Exact Match Accuracy
    as metric. $full$ mean the full and small versions of TabFact dataset.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：TabFact数据集上基于准确匹配度的实验结果。$full$表示TabFact数据集的完整版本和小版本。
- en: '| LLM | Method | Simple[full] | Complex[full] | All[full] | Simple[small] |
    Complex[small] | All[small] |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLM | Method | Simple[full] | Complex[full] | All[full] | Simple[small] |
    Complex[small] | All[small] |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Codex | Binder | - | - | - | - | - | 85.10 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Codex | Binder | - | - | - | - | - | 85.10 |'
- en: '| Dater | - | - | - | 91.20 | 80.00 | 85.60 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Dater | - | - | - | 91.20 | 80.00 | 85.60 |'
- en: '| Mixtral-8x7B | Direct | 80.59 | 69.98 | 73.47 | 81.29 | 70.56 | 75.89 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | Direct | 80.59 | 69.98 | 73.47 | 81.29 | 70.56 | 75.89 |'
- en: '| CoT | 83.53 | 74.94 | 77.77 | 86.07 | 74.19 | 80.09 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 83.53 | 74.94 | 77.77 | 86.07 | 74.19 | 80.09 |'
- en: '| PoT | 73.33 | 69.07 | 70.47 | 76.02 | 70.66 | 73.32 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 73.33 | 69.07 | 70.47 | 76.02 | 70.66 | 73.32 |'
- en: '| Tab-PoT | 86.49 | 76.06 | 79.49 | 88.36 | 75.07 | 81.67 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 86.49 | 76.06 | 79.49 | 88.36 | 75.07 | 81.67 |'
- en: '| Mixtral-7B | Direct | 73.57 | 64.83 | 67.70 | 73.13 | 62.71 | 67.89 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-7B | Direct | 73.57 | 64.83 | 67.70 | 73.13 | 62.71 | 67.89 |'
- en: '| CoT | 73.31 | 67.52 | 69.43 | 73.73 | 67.12 | 70.41 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 73.31 | 67.52 | 69.43 | 73.73 | 67.12 | 70.41 |'
- en: '| PoT | 66.11 | 63.58 | 64.41 | 65.97 | 66.54 | 66.25 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 66.11 | 63.58 | 64.41 | 65.97 | 66.54 | 66.25 |'
- en: '| Tab-PoT | 77.48 | 66.83 | 69.75 | 76.82 | 66.93 | 71.84 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 77.48 | 66.83 | 69.75 | 76.82 | 66.93 | 71.84 |'
- en: '| DeepSeek-67B | Direct | 84.36 | 74.19 | 77.53 | 84.68 | 72.62 | 78.61 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-67B | Direct | 84.36 | 74.19 | 77.53 | 84.68 | 72.62 | 78.61 |'
- en: '| CoT | 87.44 | 78.00 | 81.10 | 88.46 | 76.84 | 82.61 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 87.44 | 78.00 | 81.10 | 88.46 | 76.84 | 82.61 |'
- en: '| PoT | 74.43 | 71.79 | 72.65 | 76.32 | 72.72 | 74.51 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 74.43 | 71.79 | 72.65 | 76.32 | 72.72 | 74.51 |'
- en: '| Tab-PoT | 90.09 | 78.91 | 82.58 | 91.34 | 80.27 | 85.77 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 90.09 | 78.91 | 82.58 | 91.34 | 80.27 | 85.77 |'
- en: '| DeepSeek-7B | Direct | 59.16 | 55.42 | 56.65 | 59.50 | 55.94 | 57.71 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-7B | Direct | 59.16 | 55.42 | 56.65 | 59.50 | 55.94 | 57.71 |'
- en: '| CoT | 69.31 | 61.18 | 63.85 | 70.65 | 59.76 | 65.17 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 69.31 | 61.18 | 63.85 | 70.65 | 59.76 | 65.17 |'
- en: '| PoT | 63.71 | 58.26 | 60.06 | 64.88 | 58.98 | 61.91 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 63.71 | 58.26 | 60.06 | 64.88 | 58.98 | 61.91 |'
- en: '| Tab-PoT | 70.42 | 62.13 | 64.86 | 70.75 | 61.04 | 65.86 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 70.42 | 62.13 | 64.86 | 70.75 | 61.04 | 65.86 |'
- en: V-B Discussion and Analysis
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 讨论与分析
- en: V-B1 Ablation Study
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 消融研究
- en: 'As discussed in Section [IV](#S4 "IV Case Study of Table-QA for the LLM-based
    GIoT System ‣ Efficient Prompting for LLM-based Generative Internet of Things"),
    our proposed prompting solution consists of three stages: task-planning, task-conducting
    and task-correction. The task-planning stage can output the relevant columns and
    reasoning steps to answer the query question, which can be treated as a step of
    table decomposition and question decomposition, which can also be applied to the
    conventional PoT. In the task-conducting stage, we prompt the LLM to generate
    the Python code and a default answer. The default answer is the final answer when
    the Python code fails to run even after the task-correction stage. Since the default
    answer is generated after the Python code, it can be treated as an implicit CoT
    where the Python code is the reasoning rationales in the CoT. Finally, the third
    stage generates normalization functions to correct the errors in the Python code
    caused by the heterogeneous data types, which rely on the relevant columns generated
    by the first stage. Therefore, in this section, we conduct four ablation experiments
    by applying task-planning, default answer in task-conducting, task-planning and
    task-correction, and task-planning and default answer in task-conducting to the
    conventional PoT. The experimental results are shown in Table [IV](#S5.T4 "TABLE
    IV ‣ V-B1 Ablation Study ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis
    ‣ Efficient Prompting for LLM-based Generative Internet of Things"), where Plan,
    Correction and Default represent applying task-planning, task-correction and the
    default answer in task-conducting stages. We use Mixtral-8x7B as the LLM, and
    the experimental results demonstrate that each of the proposed three components
    can significantly improve the PoT baseline.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第[IV](#S4 "IV Table-QA案例研究针对LLM基础的GIoT系统 ‣ 高效提示LLM基础的生成型物联网")节中讨论的，我们提出的提示解决方案包括三个阶段：任务规划、任务执行和任务纠正。任务规划阶段可以输出相关的列和推理步骤以回答查询问题，这可以视为表格分解和问题分解的步骤，也可以应用于传统的PoT。在任务执行阶段，我们提示LLM生成Python代码和默认答案。默认答案是在Python代码无法运行并经过任务纠正阶段之后得到的最终答案。由于默认答案是在Python代码之后生成的，它可以被视为隐式CoT，其中Python代码是CoT中的推理依据。最后，第三阶段生成规范化函数来纠正由于异构数据类型导致的Python代码中的错误，这依赖于第一阶段生成的相关列。因此，在本节中，我们通过将任务规划、任务执行中的默认答案、任务规划和任务纠正、以及任务规划和任务执行中的默认答案应用于传统PoT，进行四个消融实验。实验结果如表[IV](#S5.T4
    "TABLE IV ‣ V-B1 消融研究 ‣ V-B 讨论与分析 ‣ V 实验与分析 ‣ 高效提示LLM基础的生成型物联网")所示，其中Plan、Correction和Default分别表示应用任务规划、任务纠正和任务执行阶段中的默认答案。我们使用Mixtral-8x7B作为LLM，实验结果表明，每个提出的三个组件均能显著改善PoT基线。
- en: 'TABLE IV: Ablation study results on the WikiTableQA dataset with Exact Match
    Accuracy as metric.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表IV：基于精确匹配准确率作为度量的WikiTableQA数据集上的消融研究结果。
- en: '| Model | Plan | Correction | Default | EM Acc |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 计划 | 纠正 | 默认 | EM准确率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PoT |  |  |  | 40.40 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| PoT |  |  |  | 40.40 |'
- en: '| Ablation 1 | ✓ |  |  | 46.52 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 消融实验 1 | ✓ |  |  | 46.52 |'
- en: '| Ablation 2 |  |  | ✓ | 53.66 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 消融实验 2 |  |  | ✓ | 53.66 |'
- en: '| Ablation 3 | ✓ | ✓ |  | 53.31 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 消融实验 3 | ✓ | ✓ |  | 53.31 |'
- en: '| Ablation 4 | ✓ |  | ✓ | 58.43 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 消融实验 4 | ✓ |  | ✓ | 58.43 |'
- en: '| Tab-PoT | ✓ | ✓ | ✓ | 63.33 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | ✓ | ✓ | ✓ | 63.33 |'
- en: V-B2 The impact of quantization
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 量化的影响
- en: Since the LLMs usually have very high hardware requirements for inference, quantization
    methods are widely used to compact LLMs using lower precision parameters. In this
    section, we conduct experiments to compare the performance of quantization versions
    of LLMs. Specifically, similar to the previous section, we also use Mixtral-8x7B
    to conduct experiments on the WikiTableQA dataset and compare its 16-bit, 8-bit
    and 4-bit versions of applying the proposed Tab-PoT solution and the experimental
    results are shown in Table [V](#S5.T5 "TABLE V ‣ V-B2 The impact of quantization
    ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis ‣ Efficient Prompting
    for LLM-based Generative Internet of Things"). For our proposed Tab-Pot, even
    though applying quantization methods can lead to worse performance, the performance
    of 8-bit and 4-bit versions is still competitive. It is worth mentioning that
    the 4-bit version shares a similar RAM footprint with the Mistral-7B model but
    achieves much higher performance, as shown in Table [V](#S5.T5 "TABLE V ‣ V-B2
    The impact of quantization ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis
    ‣ Efficient Prompting for LLM-based Generative Internet of Things") and Table [II](#S5.T2
    "TABLE II ‣ V-A Datasets and Experimental Settings ‣ V Experiments and Analysis
    ‣ Efficient Prompting for LLM-based Generative Internet of Things").
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs通常对推理有非常高的硬件要求，因此量化方法被广泛用于使用较低精度参数来压缩LLMs。在本节中，我们进行实验以比较量化版本LLMs的性能。具体来说，类似于前一节，我们也使用Mixtral-8x7B在WikiTableQA数据集上进行实验，并比较其16-bit、8-bit和4-bit版本应用所提出的Tab-PoT解决方案的实验结果，如表[V](#S5.T5
    "TABLE V ‣ V-B2 The impact of quantization ‣ V-B Discussion and Analysis ‣ V Experiments
    and Analysis ‣ Efficient Prompting for LLM-based Generative Internet of Things")所示。对于我们提出的Tab-PoT，即使应用量化方法可能导致性能下降，8-bit和4-bit版本的性能仍然具有竞争力。值得一提的是，4-bit版本与Mistral-7B模型的RAM占用相似，但性能却高得多，如表[V](#S5.T5
    "TABLE V ‣ V-B2 The impact of quantization ‣ V-B Discussion and Analysis ‣ V Experiments
    and Analysis ‣ Efficient Prompting for LLM-based Generative Internet of Things")和表[II](#S5.T2
    "TABLE II ‣ V-A Datasets and Experimental Settings ‣ V Experiments and Analysis
    ‣ Efficient Prompting for LLM-based Generative Internet of Things")所示。
- en: 'TABLE V: The impact of LLM quantization methods.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表V：LLM量化方法的影响。
- en: '| LLM | Parameter Precision | EM Acc |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 参数精度 | EM准确率 |'
- en: '| --- | --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Mixtral-8x7B | 16-bit | 63.33 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 16-bit | 63.33 |'
- en: '| 8-bit | 62.20 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 8-bit | 62.20 |'
- en: '| 4-bit | 60.52 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 4-bit | 60.52 |'
- en: V-B3 Analysis on different implementations of PoT
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3 对不同PoT实现的分析
- en: Since Python is a flexible programming language that can implement a function
    with multiple implementations. In this section, we discuss the differences among
    these different types of implementations. More specifically, one straightforward
    implementation is using the Python Standard Library and following the extraction
    and reasoning steps, as shown in Figure [8](#S5.F8 "Figure 8 ‣ V-B3 Analysis on
    different implementations of PoT ‣ V-B Discussion and Analysis ‣ V Experiments
    and Analysis ‣ Efficient Prompting for LLM-based Generative Internet of Things").
    This method selects relevant data from the table, defines the data with a List
    of Tuples, and then conducts reasoning over the defined List of Tuples. One obvious
    drawback of this implementation method is that it needs to repeat the relevant
    columns in the Python code, which can be very large when the table contains a
    large number of rows, leading to more inference time. Therefore, a refined method
    can use the table as the parameter of the solution function, then as shown in
    Figure [9](#S5.F9 "Figure 9 ‣ V-B3 Analysis on different implementations of PoT
    ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis ‣ Efficient Prompting
    for LLM-based Generative Internet of Things"). At last, since Pandas is a widely
    used Python library to process tabular data, we can also use Pandas to finish
    the reasoning tasks with a table dictionary as the input, as shown in Figure [10](#S5.F10
    "Figure 10 ‣ V-B3 Analysis on different implementations of PoT ‣ V-B Discussion
    and Analysis ‣ V Experiments and Analysis ‣ Efficient Prompting for LLM-based
    Generative Internet of Things"). We conduct experiments on the WikiTableQA dataset
    to compare the performance of these three types of implementations. Even though
    the implementation of applying Python Standard Library can show some benefits
    regarding the EM Accuracy, it requires more Prompt Tokens and Completion Tokens,
    as shown in Table [VI](#S5.T6 "TABLE VI ‣ V-B3 Analysis on different implementations
    of PoT ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis ‣ Efficient
    Prompting for LLM-based Generative Internet of Things"), because this implementation
    needs to extract relevant from the table directly and define them as a Python
    dictionary, List or variables. On the other hand, both solutions introducing function
    parameters can reduce the number of prompting tokens and completion tokens, and
    applying Pandas Library can achieve better performance than using the standard
    library.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Python 是一种灵活的编程语言，可以实现多种不同的功能。在本节中，我们讨论这些不同实现方式之间的差异。更具体地说，一种简单的实现方法是使用 Python
    标准库，并按照提取和推理步骤进行操作，如图 [8](#S5.F8 "Figure 8 ‣ V-B3 Analysis on different implementations
    of PoT ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis ‣ Efficient
    Prompting for LLM-based Generative Internet of Things") 所示。这种方法从表格中选择相关数据，用元组列表定义数据，然后对定义的元组列表进行推理。这种实现方法的一个明显缺点是需要在
    Python 代码中重复相关列，当表格包含大量行时，这可能会非常庞大，导致推理时间增加。因此，一种改进的方法是将表格作为解决函数的参数，如图 [9](#S5.F9
    "Figure 9 ‣ V-B3 Analysis on different implementations of PoT ‣ V-B Discussion
    and Analysis ‣ V Experiments and Analysis ‣ Efficient Prompting for LLM-based
    Generative Internet of Things") 所示。最后，由于 Pandas 是一个广泛使用的 Python 库来处理表格数据，我们还可以使用
    Pandas 通过将表格字典作为输入来完成推理任务，如图 [10](#S5.F10 "Figure 10 ‣ V-B3 Analysis on different
    implementations of PoT ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis
    ‣ Efficient Prompting for LLM-based Generative Internet of Things") 所示。我们在 WikiTableQA
    数据集上进行实验，以比较这三种实现方法的性能。尽管应用 Python 标准库的实现可以在 EM 准确性方面显示一些优势，但它需要更多的提示令牌和完成令牌，如表
    [VI](#S5.T6 "TABLE VI ‣ V-B3 Analysis on different implementations of PoT ‣ V-B
    Discussion and Analysis ‣ V Experiments and Analysis ‣ Efficient Prompting for
    LLM-based Generative Internet of Things") 所示，因为这种实现需要直接从表格中提取相关内容并将其定义为 Python
    字典、列表或变量。另一方面，引入函数参数的两种解决方案可以减少提示令牌和完成令牌的数量，并且应用 Pandas 库可以比使用标准库获得更好的性能。
- en: 'TABLE VI: Ablation study results on the WikiTableQA dataset with Exact Match
    Accuracy as metric.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：在 WikiTableQA 数据集上的消融研究结果，以准确匹配度为度量标准。
- en: '| Method | EM Acc | #AVG Prompt | #AVG Completion |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | EM 准确率 | 平均提示令牌 | 平均完成令牌 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | Tokens | Tokens |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 令牌 | 令牌 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| STDLib | 44.96 | 2365 | 732 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 标准库 | 44.96 | 2365 | 732 |'
- en: '| STDLib-Para | 31.17 | 2157 | 114 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 标准库-参数 | 31.17 | 2157 | 114 |'
- en: '| Pandas | 40.40 | 2241 | 88 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Pandas | 40.40 | 2241 | 88 |'
- en: '![Refer to caption](img/29a51a0a2e55f026da8ac4d6648deb7b.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/29a51a0a2e55f026da8ac4d6648deb7b.png)'
- en: 'Figure 8: PoT implementation of applying Python Standard Library. Some lines
    are omitted due to the limited page.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：应用 Python 标准库的 PoT 实现。由于页面限制，部分行已省略。
- en: '![Refer to caption](img/a72d3977e5faa5e08d4c55331369bf01.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a72d3977e5faa5e08d4c55331369bf01.png)'
- en: 'Figure 9: PoT implementation of applying Python Standard Library with parameters.
    Some lines are omitted due to the limited page.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：应用 Python 标准库并带参数的 PoT 实现。由于页面限制，部分行已省略。
- en: '![Refer to caption](img/7ab2d1afa26d1df754d019ccb5b3a065.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7ab2d1afa26d1df754d019ccb5b3a065.png)'
- en: 'Figure 10: PoT implementation of applying Python Pandas Library. Some lines
    are omitted due to the limited page.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：应用 Python Pandas 库的 PoT 实现。由于页面限制，部分行已省略。
- en: V-B4 Analysis on inference cost
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B4 推理成本分析
- en: Since the inference cost is highly correlated with the number of tokens, we
    use the prompt tokens and generated tokens as the metrics to measure the inference
    cost in this section. Therefore, we calculate the Average Prompt Tokens and Completion
    Tokens on the WikiTableQA dataset. As shown in Table [VII](#S5.T7 "TABLE VII ‣
    V-B4 Analysis on inference cost ‣ V-B Discussion and Analysis ‣ V Experiments
    and Analysis ‣ Efficient Prompting for LLM-based Generative Internet of Things"),
    the proposed Tab-PoT can introduce some overhead compared with other methods regarding
    the average prompt tokens and average completion tokens on the WikiTableQA dataset.
    Since the proposed Tab-Pot contains three stages at most, each including instructions
    and demonstrations, it can reduce the number of Prompt Tokens only when the input
    table is huge. We group the number of table tokens into 15 bins and plot the relation
    between the number of table tokens and the prompt tokens on the WikiTableQA dataset.
    When the number of a table’s tokens is larger than around 1867, our proposed Tab_PoT
    can use fewer prompt tokens than PoT, which means fewer computation operations
    and less inference time, as shown in Figure [11](#S5.F11 "Figure 11 ‣ V-B4 Analysis
    on inference cost ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis ‣
    Efficient Prompting for LLM-based Generative Internet of Things"). Since the WikitTableQA
    dataset contains a large portion of tables whose number of tokens is smaller than
    1158, the average prompt tokens of the proposed Tab-PoT is still larger than the
    one of PoT overall, as shown in Table [VII](#S5.T7 "TABLE VII ‣ V-B4 Analysis
    on inference cost ‣ V-B Discussion and Analysis ‣ V Experiments and Analysis ‣
    Efficient Prompting for LLM-based Generative Internet of Things"). As pointed
    out by some studies [[30](#bib.bib30)], the LLM can perform well on small tables,
    meaning that we can easily extend the proposed Tab-PoT with other methods, such
    as CoT, by applying a threshold regarding the size of the input table, to reduce
    the number of prompt tokens and maintain the performance simultaneously. It is
    worth mentioning that the price of prompt tokens and completion tokens are different
    when using commercial LLMs, such as GPT-4 [[8](#bib.bib8)].
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于推理成本与令牌的数量高度相关，我们在本节中使用提示令牌和生成令牌作为衡量推理成本的指标。因此，我们计算了 WikiTableQA 数据集上的平均提示令牌和完成令牌。如表 [VII](#S5.T7
    "TABLE VII ‣ V-B4 Analysis on inference cost ‣ V-B Discussion and Analysis ‣ V
    Experiments and Analysis ‣ Efficient Prompting for LLM-based Generative Internet
    of Things") 所示，所提出的 Tab-PoT 在 WikiTableQA 数据集上的平均提示令牌和平均完成令牌方面相较于其他方法可能会引入一些开销。由于所提出的
    Tab-PoT 最多包含三个阶段，每个阶段包括指令和演示，它仅在输入表格非常大时才会减少提示令牌的数量。我们将表格令牌的数量分为 15 个区间，并绘制 WikiTableQA
    数据集上表格令牌数量与提示令牌数量之间的关系。当表格的令牌数量大于约 1867 时，我们提出的 Tab-PoT 能够使用比 PoT 更少的提示令牌，这意味着计算操作更少，推理时间更短，如图 [11](#S5.F11
    "Figure 11 ‣ V-B4 Analysis on inference cost ‣ V-B Discussion and Analysis ‣ V
    Experiments and Analysis ‣ Efficient Prompting for LLM-based Generative Internet
    of Things") 所示。由于 WikiTableQA 数据集包含大量令牌数量小于 1158 的表格，所提出的 Tab-PoT 的平均提示令牌数量仍然大于
    PoT 的总体数量，如表 [VII](#S5.T7 "TABLE VII ‣ V-B4 Analysis on inference cost ‣ V-B Discussion
    and Analysis ‣ V Experiments and Analysis ‣ Efficient Prompting for LLM-based
    Generative Internet of Things") 所示。正如一些研究 [[30](#bib.bib30)] 所指出，LLM 在小型表格上表现良好，这意味着我们可以通过应用关于输入表格大小的阈值，将所提出的
    Tab-PoT 与其他方法（例如 CoT）结合，来减少提示令牌的数量并同时保持性能。值得一提的是，使用商业 LLM（例如 GPT-4）时，提示令牌和完成令牌的费用不同 [[8](#bib.bib8)]。
- en: '![Refer to caption](img/8684d5c5e9486e795fb4094635aa7f45.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8684d5c5e9486e795fb4094635aa7f45.png)'
- en: 'Figure 11: Comparison of Prompting Tokens between PoT and Tab_PoT.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：PoT 与 Tab_PoT 之间提示令牌的比较。
- en: 'TABLE VII: Comparisons of Average Prompt Tokens and Completion Tokens.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：平均提示令牌和完成令牌的比较。
- en: '| Method | #AVG Prompt Tokens | #AVG Completion Tokens |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | #AVG 提示令牌 | #AVG 完成令牌 |'
- en: '| --- | --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Direct | 1405 | 10 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 直接 | 1405 | 10 |'
- en: '| CoT | 1599 | 44 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 1599 | 44 |'
- en: '| PoT | 2241 | 88 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| PoT | 2241 | 88 |'
- en: '| Tab-PoT | 2685 | 192 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Tab-PoT | 2685 | 192 |'
- en: VI Conclusion and Future Work
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论与未来工作
- en: In this study, we propose a LLM-based GIoT system, which can be deployed in
    a local network setting to address the security concerns of many scenarios. To
    demonstrate the proposed LLM-based GIoT system, we use a challenging semi-structured
    Table-QA problem as a case study and propose a three-stage prompting solution
    to alleviate the issues caused by complex structures, heterogeneous data types,
    huge tables, and LLMs’ limitations. The proposed prompting solution uses a statistics
    table in the first stage and sub-tables in the following stages, which can reduce
    the inference cost and improve the performance when the original table is huge.
    We define a series of atomic operations to guide the demonstration crafting and
    selection, which can reduce reasoning errors. Besides, we use the task-correction
    stage to correct the failure code caused by the heterogeneous data types and use
    a default answer as the final answer when the generated Python code fails to run
    even after the task-correction step, which can be caused by the complex structure
    or the limitations of the LLM. As demonstrated in Section [V](#S5 "V Experiments
    and Analysis ‣ Efficient Prompting for LLM-based Generative Internet of Things"),
    designing tailored prompting methods can improve the performance of open-source
    LLMs, achieving state-of-the-art performance, and the proposed LLM-based GIoT
    system can be easily extended by adding task-specific prompt instructions and
    demonstrations to the system. Besides, as the proposed GIoT system is designed
    to deploy in a edge server, applying quantization to the LLM can be a good option
    to reduce the hardware requirements, as discussed in Section [V-B2](#S5.SS2.SSS2
    "V-B2 The impact of quantization ‣ V-B Discussion and Analysis ‣ V Experiments
    and Analysis ‣ Efficient Prompting for LLM-based Generative Internet of Things").
    In this study, we focus on text data, while IoT devices can generate data in multiple
    data types, such as time series and images. Therefore, extending the current system
    to handle data in various modalities can be a further direction.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了一种基于LLM的GIoT系统，该系统可以部署在本地网络环境中，以解决多种场景下的安全问题。为了展示所提议的基于LLM的GIoT系统，我们使用了一个具有挑战性的半结构化Table-QA问题作为案例，并提出了一个三阶段的提示解决方案，以缓解复杂结构、异构数据类型、大表格和LLM的限制所造成的问题。所提出的提示解决方案在第一阶段使用统计表，在后续阶段使用子表，这可以减少推理成本，并在原始表格巨大时提高性能。我们定义了一系列原子操作来指导演示制作和选择，这可以减少推理错误。此外，我们使用任务修正阶段来纠正由异构数据类型引起的失败代码，并在生成的Python代码在任务修正步骤后仍无法运行时，使用默认答案作为最终答案，这可能是由于复杂结构或LLM的限制造成的。如第[V](#S5
    "V 实验与分析 ‣ 基于LLM的生成物联网的高效提示")节所示，设计量身定制的提示方法可以提高开源LLM的性能，达到最先进的表现，所提议的基于LLM的GIoT系统可以通过向系统中添加特定任务的提示指令和演示轻松扩展。此外，由于所提议的GIoT系统旨在部署在边缘服务器上，应用量化到LLM可以是减少硬件需求的一个不错选择，如第[V-B2](#S5.SS2.SSS2
    "V-B 量化的影响 ‣ V-B 讨论与分析 ‣ V 实验与分析 ‣ 基于LLM的生成物联网的高效提示")节所讨论的那样。在本研究中，我们专注于文本数据，而物联网设备可以生成多种数据类型的数据，如时间序列和图像。因此，将当前系统扩展到处理各种模态的数据可以是进一步的方向。
- en: Acknowledgement
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported in part by the Natural Sciences and Engineering Research
    Council of Canada (NSERC) under the CREATE TRAVERSAL and DISCOVERY programs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分由加拿大自然科学与工程研究委员会（NSERC）在CREATE TRAVERSAL和DISCOVERY项目下资助。
- en: References
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Hu, M. Li, J. Gao, C. Zhou, and X. S. Shen, “Adaptive device-edge collaboration
    on dnn inference in aiot: A digital twin-assisted approach,” *IEEE Internet of
    Things Journal*, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Hu, M. Li, J. Gao, C. Zhou, 和 X. S. Shen，“在aiot中对dnn推理的自适应设备-边缘协作：一种数字双胞胎辅助的方法，”
    *IEEE物联网期刊*，2023年。'
- en: '[2] M. Adil, M. K. Khan, N. Kumar, M. Attique, A. Farouk, M. Guizani, and Z. Jin,
    “Healthcare internet of things: Security threats, challenges and future research
    directions,” *IEEE Internet of Things Journal*, 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. Adil, M. K. Khan, N. Kumar, M. Attique, A. Farouk, M. Guizani, 和 Z.
    Jin，“医疗物联网：安全威胁、挑战与未来研究方向，” *IEEE物联网期刊*，2024年。'
- en: '[3] J. Fan, W. Yang, Z. Liu, J. Kang, D. Niyato, K.-Y. Lam, and H. Du, “Understanding
    security in smart city domains from the ant-centric perspective,” *IEEE Internet
    of Things Journal*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Fan, W. Yang, Z. Liu, J. Kang, D. Niyato, K.-Y. Lam, 和 H. Du，“从以蚂蚁为中心的视角理解智能城市领域的安全性”，*IEEE
    Internet of Things Journal*，2023年。'
- en: '[4] J. Franco, A. Aris, B. Canberk, and A. S. Uluagac, “A survey of honeypots
    and honeynets for internet of things, industrial internet of things, and cyber-physical
    systems,” *IEEE Communications Surveys & Tutorials*, vol. 23, no. 4, pp. 2351–2383,
    2021.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Franco, A. Aris, B. Canberk, 和 A. S. Uluagac，“针对物联网、工业物联网和网络物理系统的蜜罐和蜜网调查”，*IEEE通讯调查与教程*，卷.
    23，第4期，第2351–2383页，2021年。'
- en: '[5] J. Wen, J. Nie, J. Kang, D. Niyato, H. Du, Y. Zhang, and M. Guizani, “From
    generative ai to generative internet of things: Fundamentals, framework, and outlooks,”
    *IEEE Internet of Things Magazine*, vol. 7, no. 3, pp. 30–37, 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Wen, J. Nie, J. Kang, D. Niyato, H. Du, Y. Zhang, 和 M. Guizani，“从生成式AI到生成式物联网：基础、框架和展望”，*IEEE
    Internet of Things Magazine*，卷. 7，第3期，第30–37页，2024年。'
- en: '[6] X. Wang, Z. Wan, A. Hekmati, M. Zong, S. Alam, M. Zhang, and B. Krishnamachari,
    “Iot in the era of generative ai: Vision and challenges,” *arXiv preprint arXiv:2401.01923*,
    2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] X. Wang, Z. Wan, A. Hekmati, M. Zong, S. Alam, M. Zhang, 和 B. Krishnamachari，“在生成式AI时代的物联网：愿景与挑战”，*arXiv预印本
    arXiv:2401.01923*，2024年。'
- en: '[7] N. Zhong, Y. Wang, R. Xiong, Y. Zheng, Y. Li, M. Ouyang, D. Shen, and X. Zhu,
    “Casit: Collective intelligent agent system for internet of things,” *IEEE Internet
    of Things Journal*, 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] N. Zhong, Y. Wang, R. Xiong, Y. Zheng, Y. Li, M. Ouyang, D. Shen, 和 X.
    Zhu，“CASIT: 面向物联网的集体智能体系统”，*IEEE Internet of Things Journal*，2024年。'
- en: '[8] OpenAI, “Gpt-4 technical report,” *ArXiv*, vol. abs/2303.08774, 2023\.
    [Online]. Available: https://api.semanticscholar.org/CorpusID:257532815'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] OpenAI，“GPT-4技术报告”，*ArXiv*，卷. abs/2303.08774，2023年。[在线]. 可用: https://api.semanticscholar.org/CorpusID:257532815'
- en: '[9] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
    S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural language models,”
    *arXiv preprint arXiv:2001.08361*, 2020.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
    S. Gray, A. Radford, J. Wu, 和 D. Amodei，“神经语言模型的规模法则”，*arXiv预印本 arXiv:2001.08361*，2020年。'
- en: '[10] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand *et al.*, “Mixtral of
    experts,” *arXiv preprint arXiv:2401.04088*, 2024.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand *等*，“专家混合模型”，*arXiv预印本
    arXiv:2401.04088*，2024年。'
- en: '[11] Meta AI, “Meta ai blog: Llama 3,” 2024, accessed: 2024-06-10\. [Online].
    Available: https://ai.meta.com/blog/meta-llama-3/'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Meta AI，“Meta AI博客: Llama 3”，2024年，访问时间: 2024-06-10。[在线]. 可用: https://ai.meta.com/blog/meta-llama-3/'
- en: '[12] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig,
    “Pal: Program-aided language models,” in *International Conference on Machine
    Learning*.   PMLR, 2023, pp. 10 764–10 799.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, 和 G.
    Neubig，“Pal: 程序辅助语言模型”，在*国际机器学习会议*中。PMLR，2023年，第10 764–10 799页。'
- en: '[13] W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program of thoughts prompting:
    Disentangling computation from reasoning for numerical reasoning tasks,” *Transactions
    on Machine Learning Research*, 2023\. [Online]. Available: https://openreview.net/forum?id=YfZ4ZPt8zd'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] W. Chen, X. Ma, X. Wang, 和 W. W. Cohen，“思维提示程序：为数字推理任务解耦计算与推理”，*机器学习研究交易*，2023年。[在线].
    可用: https://openreview.net/forum?id=YfZ4ZPt8zd'
- en: '[14] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
    *et al.*, “Chain-of-thought prompting elicits reasoning in large language models,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 24 824–24 837,
    2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D.
    Zhou *等*，“链式思考提示引发大型语言模型的推理”，*神经信息处理系统进展*，卷. 35，第24 824–24 837页，2022年。'
- en: '[15] T. Tu, Z. He, Z. Zheng, Z. Zheng, J. Jiang, Y. Gong, C. Hu, and D. Cheng,
    “Towards lifelong unseen task processing with a lightweight unlabeled data schema
    for aiot,” *IEEE Internet of Things Journal*, 2024.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T. Tu, Z. He, Z. Zheng, Z. Zheng, J. Jiang, Y. Gong, C. Hu, 和 D. Cheng，“通过轻量级无标签数据模式处理终身未见任务以适应AIoT”，*IEEE
    Internet of Things Journal*，2024年。'
- en: '[16] J. Yin, J. Dong, Y. Wang, C. De Sa, and V. Kuleshov, “Modulora: Finetuning
    3-bit llms on consumer gpus by integrating with modular quantizers,” *arXiv preprint
    arXiv:2309.16119*, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Yin, J. Dong, Y. Wang, C. De Sa, 和 V. Kuleshov，“Modulora: 通过与模块化量化器集成，在消费级GPU上微调3位LLMs”，*arXiv预印本
    arXiv:2309.16119*，2023年。'
- en: '[17] B. Xiao, M. Simsek, B. Kantarci, and A. A. Alkheir, “Table detection for
    visually rich document images,” *Knowledge-Based Systems*, vol. 282, p. 111080,
    2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] B. Xiao, M. Simsek, B. Kantarci, 和 A. A. Alkheir, “视觉丰富文档图像中的表格检测，”*知识基础系统*，第282卷，第111080页，2023年。'
- en: '[18] J. Herzig, P. K. Nowak, T. Müller, F. Piccinno, and J. M. Eisenschlos,
    “Tapas: Weakly supervised table parsing via pre-training,” *arXiv preprint arXiv:2004.02349*,
    2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Herzig, P. K. Nowak, T. Müller, F. Piccinno, 和 J. M. Eisenschlos, “Tapas：通过预训练进行弱监督表格解析，”*arXiv预印本
    arXiv:2004.02349*，2020年。'
- en: '[19] Z. Jiang, Y. Mao, P. He, G. Neubig, and W. Chen, “Omnitab: Pretraining
    with natural and synthetic data for few-shot table-based question answering,”
    *arXiv preprint arXiv:2207.03637*, 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Z. Jiang, Y. Mao, P. He, G. Neubig, 和 W. Chen, “Omnitab：利用自然和合成数据进行少量样本表格问答的预训练，”*arXiv预印本
    arXiv:2207.03637*，2022年。'
- en: '[20] F. Zhu, Z. Liu, F. Feng, C. Wang, M. Li, and T.-S. Chua, “Tat-llm: A specialized
    language model for discrete reasoning over tabular and textual data,” *arXiv preprint
    arXiv:2401.13223*, 2024.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] F. Zhu, Z. Liu, F. Feng, C. Wang, M. Li, 和 T.-S. Chua, “Tat-llm：一种专门用于表格和文本数据离散推理的语言模型，”*arXiv预印本
    arXiv:2401.13223*，2024年。'
- en: '[21] Z. Yu, L. He, Z. Wu, X. Dai, and J. Chen, “Towards better chain-of-thought
    prompting strategies: A survey,” *arXiv preprint arXiv:2310.04959*, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Z. Yu, L. He, Z. Wu, X. Dai, 和 J. Chen, “朝着更好的链式思维提示策略：一项调查，”*arXiv预印本
    arXiv:2310.04959*，2023年。'
- en: '[22] B. Zhao, C. Ji, Y. Zhang, W. He, Y. Wang, Q. Wang, R. Feng, and X. Zhang,
    “Large language models are complex table parsers,” in *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing*, 2023, pp. 14 786–14 802.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] B. Zhao, C. Ji, Y. Zhang, W. He, Y. Wang, Q. Wang, R. Feng, 和 X. Zhang,
    “大型语言模型是复杂的表格解析器，”发表于*2023年自然语言处理实证方法会议论文集*，2023年，第14,786–14,802页。'
- en: '[23] S. De, M. Bermudez-Edo, H. Xu, and Z. Cai, “Deep generative models in
    the industrial internet of things: a survey,” *IEEE Transactions on Industrial
    Informatics*, vol. 18, no. 9, pp. 5728–5737, 2022.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. De, M. Bermudez-Edo, H. Xu, 和 Z. Cai, “工业物联网中的深度生成模型：一项调查，”*IEEE工业信息学学报*，第18卷，第9期，第5728–5737页，2022年。'
- en: '[24] H. Cui, Y. Du, Q. Yang, Y. Shao, and S. C. Liew, “Llmind: Orchestrating
    ai and iot with llms for complex task execution,” *arXiv preprint arXiv:2312.09007*,
    2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] H. Cui, Y. Du, Q. Yang, Y. Shao, 和 S. C. Liew, “Llmind：通过大型语言模型协调AI和物联网以执行复杂任务，”*arXiv预印本
    arXiv:2312.09007*，2023年。'
- en: '[25] B. Rong and H. Rutagemwa, “Leveraging large language models for intelligent
    control of 6g integrated tn-ntn with iot service,” *IEEE Network*, 2024.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] B. Rong 和 H. Rutagemwa, “利用大型语言模型进行6g集成tn-ntn智能控制与物联网服务，”*IEEE网络*，2024年。'
- en: '[26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” *arXiv preprint
    arXiv:2106.09685*, 2021.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen, “Lora：大型语言模型的低秩适配，”*arXiv预印本 arXiv:2106.09685*，2021年。'
- en: '[27] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia, “Longlora:
    Efficient fine-tuning of long-context large language models,” *arXiv preprint
    arXiv:2309.12307*, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, 和 J. Jia, “Longlora：高效微调长上下文大型语言模型，”*arXiv预印本
    arXiv:2309.12307*，2023年。'
- en: '[28] T. Zhang, X. Yue, Y. Li, and H. Sun, “Tablellama: Towards open large generalist
    models for tables,” *arXiv preprint arXiv:2311.09206*, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] T. Zhang, X. Yue, Y. Li, 和 H. Sun, “Tablellama：朝着开放的大型通用表格模型，”*arXiv预印本
    arXiv:2311.09206*，2023年。'
- en: '[29] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等*，“Llama 2：开放基础和微调聊天模型，”*arXiv预印本
    arXiv:2307.09288*，2023年。'
- en: '[30] W. Chen, “Large language models are few (1)-shot table reasoners,” in
    *Findings of the Association for Computational Linguistics: EACL 2023*, 2023,
    pp. 1120–1130.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] W. Chen, “大型语言模型是少量（1）-shot 表格推理器，”发表于*计算语言学协会发现：EACL 2023*，2023年，第1120–1130页。'
- en: '[31] Y. Ye, B. Hui, M. Yang, B. Li, F. Huang, and Y. Li, “Large language models
    are versatile decomposers: Decomposing evidence and questions for table-based
    reasoning,” in *Proceedings of the 46th International ACM SIGIR Conference on
    Research and Development in Information Retrieval*, 2023, pp. 174–184.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Ye, B. Hui, M. Yang, B. Li, F. Huang, 和 Y. Li, “大型语言模型是多功能的解构者：为基于表格的推理分解证据和问题，”发表于*第46届国际ACM
    SIGIR信息检索研究与发展会议论文集*，2023年，第174–184页。'
- en: '[32] W. Lin, R. Blloshmi, B. Byrne, A. de Gispert, and G. Iglesias, “An inner
    table retriever for robust table question answering,” in *Proceedings of the 61st
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, 2023, pp. 9909–9926.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] W. Lin, R. Blloshmi, B. Byrne, A. de Gispert, 和 G. Iglesias, “一个内嵌表检索器，用于稳健的表格问答，”
    见于 *第61届计算语言学协会年会（第1卷：长篇论文）*，2023年，第9909–9926页。'
- en: '[33] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J. rong Wen, “Structgpt:
    A general framework for large language model to reason over structured data,”
    in *Conference on Empirical Methods in Natural Language Processing*, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, 和 J. rong Wen, “Structgpt：一个通用框架，用于大语言模型在结构化数据上进行推理，”
    见于 *自然语言处理实证方法会议*，2023年。'
- en: '[34] P. Srivastava, M. Malik, and T. Ganu, “Assessing llms’ mathematical reasoning
    in financial document question answering,” *arXiv preprint arXiv:2402.11194*,
    2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] P. Srivastava, M. Malik, 和 T. Ganu, “评估LLMs在金融文档问答中的数学推理，” *arXiv 预印本
    arXiv:2402.11194*，2024年。'
- en: '[35] Z. Cheng, T. Xie, P. Shi, C. Li, R. Nadkarni, Y. Hu, C. Xiong, D. Radev,
    M. Ostendorf, L. Zettlemoyer, N. A. Smith, and T. Yu, “Binding language models
    in symbolic languages,” in *The Eleventh International Conference on Learning
    Representations*, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. Cheng, T. Xie, P. Shi, C. Li, R. Nadkarni, Y. Hu, C. Xiong, D. Radev,
    M. Ostendorf, L. Zettlemoyer, N. A. Smith, 和 T. Yu, “将语言模型绑定到符号语言中，” 见于 *第十一届国际学习表征会议*，2023年。'
- en: '[36] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *等*，“语言模型是少样本学习者，” *神经信息处理系统进展*，第33卷，第1877–1901页，2020年。'
- en: '[37] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought prompting
    in large language models,” in *The Eleventh International Conference on Learning
    Representations (ICLR 2023)*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Z. Zhang, A. Zhang, M. Li, 和 A. Smola, “大型语言模型中的自动思维链提示，” 见于 *第十一届国际学习表征会议（ICLR
    2023）*，2023年。'
- en: '[38] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen, “Synthetic
    prompting: Generating chain-of-thought demonstrations for large language models,”
    *arXiv preprint arXiv:2302.00618*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, 和 W. Chen, “合成提示：为大型语言模型生成思维链演示，”
    *arXiv 预印本 arXiv:2302.00618*，2023年。'
- en: '[39] K. Shum, S. Diao, and T. Zhang, “Automatic prompt augmentation and selection
    with chain-of-thought from labeled data,” in *Findings of the Association for
    Computational Linguistics: EMNLP 2023*, H. Bouamor, J. Pino, and K. Bali, Eds.   Singapore:
    Association for Computational Linguistics, Dec. 2023, pp. 12 113–12 139\. [Online].
    Available: https://aclanthology.org/2023.findings-emnlp.811'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. Shum, S. Diao, 和 T. Zhang, “基于标记数据的自动提示增强和选择带有思维链，” 见于 *计算语言学协会年会论文集：EMNLP
    2023*，H. Bouamor, J. Pino, 和 K. Bali 编辑。新加坡：计算语言学协会，2023年12月，第12,113–12,139页。[在线].
    可用链接: https://aclanthology.org/2023.findings-emnlp.811'
- en: '[40] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery,
    and D. Zhou, “Self-consistency improves chain of thought reasoning in language
    models,” *arXiv preprint arXiv:2203.11171*, 2022.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery,
    和 D. Zhou, “自洽性改善了语言模型中的思维链推理，” *arXiv 预印本 arXiv:2203.11171*，2022年。'
- en: '[41] O. Yoran, T. Wolfson, B. Bogin, U. Katz, D. Deutch, and J. Berant, “Answering
    questions by meta-reasoning over multiple chains of thought,” in *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, H. Bouamor,
    J. Pino, and K. Bali, Eds.   Singapore: Association for Computational Linguistics,
    Dec. 2023, pp. 5942–5966\. [Online]. Available: https://aclanthology.org/2023.emnlp-main.364'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] O. Yoran, T. Wolfson, B. Bogin, U. Katz, D. Deutch, 和 J. Berant, “通过在多个思维链上进行元推理回答问题，”
    见于 *2023年自然语言处理实证方法会议论文集*，H. Bouamor, J. Pino, 和 K. Bali 编辑。新加坡：计算语言学协会，2023年12月，第5942–5966页。[在线].
    可用链接: https://aclanthology.org/2023.emnlp-main.364'
- en: '[42] Y. Cao, S. Chen, R. Liu, Z. Wang, and D. Fried, “Api-assisted code generation
    for question answering on varied table structures,” in *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing*, 2023, pp. 14 536–14 548.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Cao, S. Chen, R. Liu, Z. Wang, 和 D. Fried, “API辅助的代码生成，用于处理各种表结构中的问答，”
    见于 *2023年自然语言处理实证方法会议论文集*，2023年，第14,536–14,548页。'
- en: '[43] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro,
    L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can
    teach themselves to use tools,” in *Advances in Neural Information Processing
    Systems*, A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,
    Eds., vol. 36.   Curran Associates, Inc., 2023, pp. 68 539–68 551.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, E. Hambro,
    L. Zettlemoyer, N. Cancedda, 和 T. Scialom，“Toolformer：语言模型可以自我学习使用工具，”在 *神经信息处理系统进展*，A.
    Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, 和 S. Levine 编辑，第36卷。Curran
    Associates, Inc.，2023年，第68,539–68,551页。'
- en: '[44] T. pandas development team, “pandas-dev/pandas: Pandas,” Feb. 2020\. [Online].
    Available: https://doi.org/10.5281/zenodo.3509134'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] T. pandas 开发团队，“pandas-dev/pandas: Pandas，”2020年2月。 [在线]。可用链接：https://doi.org/10.5281/zenodo.3509134'
- en: '[45] Wes McKinney, “Data Structures for Statistical Computing in Python,” in
    *Proceedings of the 9th Python in Science Conference*, Stéfan van der Walt and
    Jarrod Millman, Eds., 2010, pp. 56 – 61.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Wes McKinney，“Python中的统计计算数据结构，”在 *第9届Python科学会议论文集*，Stéfan van der Walt
    和 Jarrod Millman 编辑，2010年，第56–61页。'
- en: '[46] Y. Zhao, Y. Li, C. Li, and R. Zhang, “MultiHiertt: Numerical reasoning
    over multi hierarchical tabular and textual data,” in *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*.   Dublin, Ireland: Association for Computational Linguistics, May 2022,
    pp. 6588–6600\. [Online]. Available: https://aclanthology.org/2022.acl-long.454'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Zhao, Y. Li, C. Li, 和 R. Zhang，“MultiHiertt：多层次表格和文本数据的数值推理，”在 *第60届计算语言学协会年会论文集（第1卷：长论文）*。爱尔兰都柏林：计算语言学协会，2022年5月，第6588–6600页。
    [在线]。可用链接：https://aclanthology.org/2022.acl-long.454'
- en: '[47] P. Pasupat and P. Liang, “Compositional semantic parsing on semi-structured
    tables,” in *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, 2015, pp. 1470–1480.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] P. Pasupat 和 P. Liang，“半结构化表格上的组成语义解析，”在 *第53届计算语言学协会年会及第7届国际自然语言处理联合会议（第1卷：长论文）*，2015年，第1470–1480页。'
- en: '[48] B. Xiao, M. Simsek, B. Kantarci, and A. A. Alkheir, “Rethinking detection
    based table structure recognition for visually rich documents,” *arXiv preprint
    arXiv:2312.00699*, 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] B. Xiao, M. Simsek, B. Kantarci, 和 A. A. Alkheir，“重新思考基于检测的表结构识别用于视觉丰富文档，”
    *arXiv预印本 arXiv:2312.00699*，2023年。'
- en: '[49] J. Fernandes, B. Xiao, M. Simsek, B. Kantarci, S. Khan, and A. A. Alkheir,
    “Tablestrrec: framework for table structure recognition in data sheet images,”
    *International Journal on Document Analysis and Recognition (IJDAR)*, pp. 1–19,
    2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Fernandes, B. Xiao, M. Simsek, B. Kantarci, S. Khan, 和 A. A. Alkheir，“Tablestrrec：数据表图像中的表格结构识别框架，”
    *国际文档分析与识别期刊（IJDAR）*，第1–19页，2023年。'
- en: '[50] B. Smock, R. Pesala, and R. Abraham, “PubTables-1M: Towards comprehensive
    table extraction from unstructured documents,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp.
    4634–4642.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] B. Smock, R. Pesala, 和 R. Abraham，“PubTables-1M：迈向从非结构化文档中全面提取表格，”在 *IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2022年6月，第4634–4642页。'
- en: '[51] T. Shi, C. Zhao, J. Boyd-Graber, H. Daumé III, and L. Lee, “On the potential
    of lexico-logical alignments for semantic parsing to sql queries,” *arXiv preprint
    arXiv:2010.11246*, 2020.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] T. Shi, C. Zhao, J. Boyd-Graber, H. Daumé III, 和 L. Lee，“词汇逻辑对齐在语义解析到SQL查询中的潜力，”
    *arXiv预印本 arXiv:2010.11246*，2020年。'
- en: '[52] Y. Zhang, J. Henkel, A. Floratou, J. Cahoon, S. Deep, and J. M. Patel,
    “Reactable: Enhancing react for table question answering,” *arXiv preprint arXiv:2310.00815*,
    2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Zhang, J. Henkel, A. Floratou, J. Cahoon, S. Deep, 和 J. M. Patel，“Reactable：增强React以进行表格问答，”
    *arXiv预印本 arXiv:2310.00815*，2023年。'
- en: '[53] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot, “Complexity-based
    prompting for multi-step reasoning,” *arXiv preprint arXiv:2210.00720*, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Fu, H. Peng, A. Sabharwal, P. Clark, 和 T. Khot，“基于复杂性的多步骤推理提示，” *arXiv预印本
    arXiv:2210.00720*，2022年。'
- en: '[54] B. Wang, S. Min, X. Deng, J. Shen, Y. Wu, L. Zettlemoyer, and H. Sun,
    “Towards understanding chain-of-thought prompting: An empirical study of what
    matters,” in *The 61st Annual Meeting of the Association for Computational Linguistics*,
    2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] B. Wang, S. Min, X. Deng, J. Shen, Y. Wu, L. Zettlemoyer, 和 H. Sun，“迈向理解思路链提示：一项关于重要因素的实证研究，”在
    *第61届计算语言学协会年会*，2023年。'
- en: '[55] W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y.
    Wang, “Tabfact: A large-scale dataset for table-based fact verification,” *arXiv
    preprint arXiv:1909.02164*, 2019.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, 和 W. Y.
    Wang，“Tabfact: 一个大规模的表格事实验证数据集，” *arXiv 预印本 arXiv:1909.02164*，2019。'
