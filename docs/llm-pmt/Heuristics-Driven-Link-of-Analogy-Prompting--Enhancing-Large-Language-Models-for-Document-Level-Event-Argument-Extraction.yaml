- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:48:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:48:15'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启发式驱动的类比提示：提升大型语言模型在文档级事件论据提取中的表现
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.06555](https://ar5iv.labs.arxiv.org/html/2311.06555)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.06555](https://ar5iv.labs.arxiv.org/html/2311.06555)
- en: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1'
- en: ¹Nanyang Technological University, Singapore
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹南洋理工大学，新加坡
- en: ²Future Resilient Systems Programme, Singapore-ETH Centre, Singapore
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²未来韧性系统计划，新加坡-ETH 中心，新加坡
- en: '{hanzhang001, junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{hanzhang001, junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg'
- en: ekzmao@ntu.edu.sg
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ekzmao@ntu.edu.sg
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this study, we investigate in-context learning (ICL) in document-level event
    argument extraction (EAE). The paper identifies key challenges in this problem,
    including example selection, context length limitation, abundance of event types,
    and the limitation of Chain-of-Thought (CoT) prompting in non-reasoning tasks.
    To address these challenges, we introduce the Heuristic-Driven Link-of-Analogy
    (HD-LoA) prompting method. Specifically, we hypothesize and validate that LLMs
    learn task-specific heuristics from demonstrations via ICL. Building upon this
    hypothesis, we introduce an explicit heuristic-driven demonstration construction
    approach, which transforms the haphazard example selection process into a methodical
    method that emphasizes task heuristics. Additionally, inspired by the analogical
    reasoning of human, we propose the link-of-analogy prompting, which enables LLMs
    to process new situations by drawing analogies to known situations, enhancing
    their adaptability. Extensive experiments show that our method outperforms the
    existing prompting methods and few-shot supervised learning methods, exhibiting
    F1 score improvements of $4.53\%$, indicating its effectiveness across different
    tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们探讨了文档级事件论据提取（EAE）中的上下文学习（ICL）。论文识别了这个问题中的关键挑战，包括示例选择、上下文长度限制、事件类型丰富性以及
    Chain-of-Thought（CoT）提示在非推理任务中的局限性。为了解决这些挑战，我们引入了启发式驱动的类比（HD-LoA）提示方法。具体而言，我们假设并验证了
    LLMs 通过 ICL 从示例中学习任务特定的启发式规则。基于这一假设，我们提出了一种明确的启发式驱动的示例构建方法，将随意的示例选择过程转变为强调任务启发式规则的系统方法。此外，受到类比推理的启发，我们提出了类比提示，这使得
    LLMs 通过类比已知情况来处理新情况，从而增强其适应性。大量实验表明，我们的方法在不同任务中表现优于现有提示方法和少量监督学习方法，F1 分数提高了 $4.53\%$，显示了其在不同任务中的有效性。
- en: 'Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式驱动的类比提示：提升大型语言模型在文档级事件论据提取中的表现
- en: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1 ¹Nanyang Technological University, Singapore
    ²Future Resilient Systems Programme, Singapore-ETH Centre, Singapore {hanzhang001,
    junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg ekzmao@ntu.edu.sg'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hanzhang Zhou^(1,2), Junlang Qian¹, Zijian Feng^(1,2), Hui Lu¹, Zixiao Zhu^(1,2),
    Kezhi Mao^(1,2)¹¹footnotemark: 1 ¹南洋理工大学，新加坡 ²未来韧性系统计划，新加坡-ETH 中心，新加坡 {hanzhang001,
    junlang001, feng0119, hui007, zixiao001}@e.ntu.edu.sg ekzmao@ntu.edu.sg'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Document-level Event Argument Extraction (EAE) aims to transform unstructured
    event information from documents into structured formats encapsulating event arguments,
    facilitating their interpretation and application in various domains (Grishman,
    [2019](#bib.bib16)). The prevalent approach for this task relies on the collection
    of labeled data and the subsequent model training via supervised learning (Liu
    et al., [2023a](#bib.bib20); Pouran Ben Veyseh et al., [2022](#bib.bib28); Zhou
    and Mao, [2022](#bib.bib45); Du and Cardie, [2020a](#bib.bib7)). While effective,
    this approach comes with the significant drawback: it necessitates a substantial
    amount of training data, which is particularly burdensome and costly given the
    complexity inherent to document-level EAE.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文档级事件论证提取（EAE）旨在将文档中的非结构化事件信息转化为封装事件论证的结构化格式，从而促进其在各个领域的解释和应用 (Grishman, [2019](#bib.bib16))。这一任务的主流方法依赖于标注数据的收集以及通过监督学习进行的模型训练
    (Liu et al., [2023a](#bib.bib20); Pouran Ben Veyseh et al., [2022](#bib.bib28);
    Zhou and Mao, [2022](#bib.bib45); Du and Cardie, [2020a](#bib.bib7))。虽然有效，但这一方法有一个显著的缺点：它需要大量的训练数据，而考虑到文档级EAE的复杂性，这既繁琐又昂贵。
- en: In this context, in-context learning (ICL) (Brown et al., [2020](#bib.bib4);
    Liu et al., [2022](#bib.bib19); Zhou et al., [2022](#bib.bib44)), an emergent
    ability of large language models (LLMs), offers a promising alternative to supervised
    learning. ICL alleviates the need for large-scale data as it only uses a few examples
    as input-output pairs of the prompt to guide LLMs in performing the task on an
    unseen example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，内部学习（ICL） (Brown et al., [2020](#bib.bib4); Liu et al., [2022](#bib.bib19);
    Zhou et al., [2022](#bib.bib44))，一种大型语言模型（LLM）的新兴能力，为监督学习提供了一个有前景的替代方案。ICL减少了对大规模数据的需求，因为它只使用少量的示例作为输入-输出对，指导LLM在未见示例上执行任务。
- en: '![Refer to caption](img/01d00cc7c409fef85d91b3a113a0dcd1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/01d00cc7c409fef85d91b3a113a0dcd1.png)'
- en: 'Figure 1: CoT’s step-by-step reasoning degrades to a mere single step for non-reasoning
    tasks. Reasoning steps of two reasoning tasks (in orange) and two non-reasoning
    tasks (in blue) are compared. Different colors indicate distinct reasoning steps.
    Prompts are adopted from (Shum et al., [2023](#bib.bib31))'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：CoT的逐步推理在非推理任务中退化为单一步骤。比较了两个推理任务（橙色）和两个非推理任务（蓝色）的推理步骤。不同的颜色表示不同的推理步骤。提示词采自
    (Shum et al., [2023](#bib.bib31))
- en: .
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'However, applying ICL to document-level EAE presents numerous challenges. The
    design of in-context demonstrations is crucial to the effectiveness of ICL, with
    the performance of ICL being significantly influenced by factors such as the selection
    of examples and the formatting of reasoning steps (Zhang et al., [2023](#bib.bib43);
    Liu et al., [2022](#bib.bib19); Zhang et al., [2022](#bib.bib42); Fu et al., [2022](#bib.bib10)).
    Consequently, several crucial challenges emerge concerning the prompting strategy:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将ICL应用于文档级EAE面临众多挑战。上下文示例的设计对ICL的有效性至关重要，ICL的性能受示例选择和推理步骤格式等因素的显著影响 (Zhang
    et al., [2023](#bib.bib43); Liu et al., [2022](#bib.bib19); Zhang et al., [2022](#bib.bib42);
    Fu et al., [2022](#bib.bib10))。因此，关于提示策略出现了几个关键挑战：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Example selection. Gaining clarity on how to select demonstration examples requires
    an in-depth understanding of what LLMs learn from these demonstrations in ICL.
    At present, however, the understanding of this aspect remains limited (Dong et al.,
    [2022](#bib.bib6)). Therefore, what are the guiding principles for example selection,
    and what characterizes a ’good’ example for document-level EAE?
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例选择。对如何选择演示示例的清晰理解需要深入了解LLM在ICL中的学习方式。然而，目前对这一方面的理解仍然有限 (Dong et al., [2022](#bib.bib6))。因此，选择示例的指导原则是什么，什么样的示例被认为是文档级EAE的“好”示例？
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Context length limit. Following the experimental set-up of chain-of-thought
    (CoT) prompting (Wei et al., [2022](#bib.bib38)), demonstrations typically incorporate
    six or eight examples. However, in the context of document-level EAE, selecting
    eight documents as examples might significantly extend the context length, potentially
    surpassing the token limit of LLMs.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上下文长度限制。根据链式推理（CoT）提示的实验设置 (Wei et al., [2022](#bib.bib38))，示例通常包含六或八个。然而，在文档级EAE的背景下，选择八个文档作为示例可能会显著延长上下文长度，可能超出LLM的令牌限制。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Abundance of Event Types. The EAE task can feature more than a hundred distinct
    event types and argument roles. Yet, in-context examples can only capture a narrow
    subset of these, leaving the majority of argument roles unseen. Creating individual
    prompts for each unique event type and argument role is both labor-intensive and
    impractical. This leads to the question: How to design a universal prompting strategy
    that effectively addresses a wide range of unseen event types and argument roles?'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事件类型的丰富性。EAE任务可能包含超过一百种不同的事件类型和论证角色。然而，上下文示例只能捕捉到这些中的一小部分，导致大多数论证角色未被看到。为每种独特的事件类型和论证角色创建单独的提示既劳动密集又不切实际。这就引出了一个问题：如何设计一种通用的提示策略，以有效应对广泛的未见事件类型和论证角色？
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompting strategy for non-reasoning task. While the CoT prompting strategy
    is extensively used across a variety of tasks, both reasoning and non-reasoning,
    it was originally designed only for reasoning tasks such as arithmetic reasoning.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Heuristics-Driven Link-of-Analogy
    Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction"),
    applying CoT to non-reasoning tasks will degrade its step-by-step reasoning into
    a single-step process, compromising its effectiveness. Consequently, there is
    a need for a reasoning strategy tailored for non-reasoning tasks.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非推理任务的提示策略。虽然链式思维（CoT）提示策略在各种任务中得到了广泛使用，包括推理任务和非推理任务，但它最初仅设计用于推理任务，如算术推理。如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 启发式驱动的类比提示：增强大型语言模型在文档级事件论证提取中的能力")所示，将CoT应用于非推理任务将其逐步推理降级为单步过程，从而影响其效果。因此，需要为非推理任务量身定制的推理策略。
- en: '![Refer to caption](img/c9b0d55ffed6814941a8ee80315fb18a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c9b0d55ffed6814941a8ee80315fb18a.png)'
- en: 'Figure 2: The implicit heuristics behind in-context examples.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：上下文示例背后的隐含启发式。
- en: 'In this work, we put forward a novel hypothesis that LLMs learn task-specific
    heuristics from examples and validate the hypothesis through experiments. Building
    upon this hypothesis, we propose heuristic-driven link-of-analogy prompting to
    address the aforementioned questions. To elaborate:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一个新颖的假设，即大型语言模型（LLMs）从示例中学习任务特定的启发式，并通过实验验证了这一假设。在此假设的基础上，我们提出了启发式驱动的类比提示方法，以解决上述问题。具体说明如下：
- en: 'We propose and empirically validate the hypothesis that LLMs learn task specific
    heuristics from examples in ICL. The hypothesis is derived by drawing an analogy
    from the training of deep learning models and the fine-tuning of pretrained language
    models (PLMs). Both these models achieve enhanced performance by learning task-specific
    patterns, either through training or fine-tuning on task data (Zhou and Srikumar,
    [2022](#bib.bib46); Shachaf et al., [2021](#bib.bib30); Kim et al., [2018](#bib.bib17);
    Najafabadi et al., [2015](#bib.bib24)). Drawing a parallel, we hypothesize that,
    although LLMs’ parameters are frozen in ICL, providing them with in-context examples
    allows LLMs to learn task-specific patterns during inference. These patterns encompasses
    both lower-level patterns like label space and input text distribution, and higher-level
    heuristics from demonstrations. We notice the learning of lower-level patterns
    has been validated Min et al. ([2022](#bib.bib23)). Therefore, we focus on substantiating
    the learning of task-specific heuristics in ICL. An illustration of the implicit
    heuristics behind in-context examples can be found in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large
    Language Models for Document-Level Event Argument Extraction"), while quantitative
    validations of our hypothesis are detailed in Section [2](#S2 "2 What LLMs learn
    from the demonstration? ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出并实证验证了LLMs从ICL示例中学习任务特定启发式的假设。该假设通过类比深度学习模型的训练和预训练语言模型（PLMs）的微调得出。这些模型通过在任务数据上训练或微调学习任务特定模式，从而提高性能（Zhou
    和 Srikumar，[2022](#bib.bib46)；Shachaf 等，[2021](#bib.bib30)；Kim 等，[2018](#bib.bib17)；Najafabadi
    等，[2015](#bib.bib24)）。类比推理，我们假设尽管LLMs的参数在ICL中是冻结的，但通过提供上下文示例，LLMs可以在推理过程中学习任务特定模式。这些模式包括标签空间和输入文本分布等较低级的模式，以及来自示例的较高级启发式。我们注意到较低级模式的学习已在Min等人（[2022](#bib.bib23)）的研究中得到验证。因此，我们重点验证了ICL中任务特定启发式的学习。上下文示例中隐含启发式的说明见图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing
    Large Language Models for Document-Level Event Argument Extraction")，而我们假设的定量验证详见第[2](#S2
    "2 What LLMs learn from the demonstration? ‣ Heuristics-Driven Link-of-Analogy
    Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction")节。'
- en: We propose a heuristic-driven demonstration construction method. Based on our
    hypothesis, task heuristics are crucial for the ICL performance of LLMs. Yet,
    these heuristics are implicitly conveyed through examples, leading to complexity
    for ultilizing these heuristics and uncertainties about whether LLMs have recognized
    these heuristics. To address this, we propose to explicitly provide task heuristics
    in demonstrations. This approach simplify the LLM’s two-step process of implicit
    heuristic recognition and subsequent inference into a more straightforward, singular
    step of heuristic-based inference.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于启发式驱动的示例构建方法。基于我们的假设，任务启发式对LLMs的ICL性能至关重要。然而，这些启发式通常通过示例隐式传达，导致利用这些启发式的复杂性，以及不确定LLMs是否识别了这些启发式。为了解决这一问题，我们建议在示例中明确提供任务启发式。这种方法将LLMs隐式启发式识别和后续推理的两步过程简化为更直接的单步启发式推理。
- en: We propose the link-of-analogy prompting method for document-level EAE that
    suitable for non-reasoning tasks. To address the aforementioned challenges of
    abundance of event types in EAE and the limitations of CoT prompting on non-reasoning
    tasks, we present the link-of-analogy prompting. Inspired by the analogical reasoning––a
    core mechanism of human cognition–this approach enables LLMs to extrapolate knowledge
    from known event types to unseen ones. By enabling LLMs to process new situations
    through drawing an analogy to known situations, the link-of-analogy prompting
    significantly enhances the performance of ICL.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了适用于文档级EAE的类比链接提示方法，适合非推理任务。为了应对EAE中事件类型丰富性和CoT提示在非推理任务上的局限性，我们提出了类比链接提示。受到类比推理——人类认知的核心机制——的启发，这种方法使LLMs能够将已知事件类型的知识外推到未见事件。通过将新情况与已知情况类比处理，这种方法显著提升了ICL的性能。
- en: We implement our heuristic-driven link-of-analogy prompting method across three
    LLM models on two document-level EAE datasets and two datasets on sentiment analysis
    and natural language inference tasks. Extensive experiments demonstrate that our
    method outperforms the state-of-the-art prompting methods and few-shot supervised
    learning methods, as evidenced by absolute F1 score increases of $4.53\%$, respectively.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个LLM模型上实施了基于启发式驱动的类比提示方法，应用于两个文档级EAE数据集和两个情感分析与自然语言推理任务的数据集。广泛的实验表明，我们的方法优于最先进的提示方法和少量监督学习方法，F1分数绝对提高了$4.53\%$。
- en: 2 What LLMs learn from the demonstration?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 LLM从演示中学到了什么？
- en: '![Refer to caption](img/31af91b41129c98ec37954fe8b83359f.png)![Refer to caption](img/f6cffaf318ad31e980cb7858913b2c64.png)![Refer
    to caption](img/c734e8830f06ffae2c529dd6ed71aae5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/31af91b41129c98ec37954fe8b83359f.png)![参考说明](img/f6cffaf318ad31e980cb7858913b2c64.png)![参考说明](img/c734e8830f06ffae2c529dd6ed71aae5.png)'
- en: 'Figure 3: An illustration of the correlation between example quantity and heuristic
    diversity in well-designed prompts. # Examples: the number of examples used in
    each prompt of the corresponding paper. # Heuristics: the number of heuristics
    identified in each prompt of the corresponding paper. # Heuristics in Rand.: the
    average number of heuristics in the prompt composed of random selected examples.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：展示了设计良好的提示中，示例数量与启发式多样性之间的相关性。 # 示例：每个提示中使用的示例数量。 # 启发式：每个提示中识别的启发式方法数量。
    # 随机启发式：由随机选择的示例组成的提示中启发式方法的平均数量。'
- en: There is minimal understanding of what LLMs learn from the demonstration of
    ICL currently available. In this work, we hypothesize that LLMs learn task-specific
    heuristics from examples of ICL based on an analogy from the training of deep
    learning models. We confirm this hypothesis with carefully designed experiments
    in two aspects.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 目前对LLM从ICL演示中学习到的内容理解甚微。在这项工作中，我们假设LLM基于深度学习模型训练中的类比，从ICL示例中学习任务特定的启发式方法。我们通过精心设计的实验从两个方面确认了这一假设。
- en: 2.1 Correlation between Example Quantity and Heuristic Diversity in Well-Designed
    Prompts
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 例子数量与设计良好的提示中的启发式多样性之间的相关性
- en: If our hypothesis holds, it implies that well-constructed prompts should incorporate
    a diverse range of heuristics. To validate this implication, we assess both the
    quantity of examples and the quantity of different heuristics present in prompts
    from published studies.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的假设成立，这意味着精心构建的提示应包含多样化的启发式方法。为了验证这一含义，我们评估了已发布研究中的示例数量和提示中不同启发式方法的数量。
- en: 'However, manually estimating the number of heuristics can introduce subjectivity,
    leading to potential biases. Thus, the challenge is to objectively identify the
    implicit heuristics inherent in demonstration examples. In this work, we employ
    GPT-4 to recognize the implicit heuristics for each example and to determine if
    there are shared heuristics across multiple examples. An detailed example of the
    prompt we used, along with the heuristics identified by GPT-4, can be found in
    Appendix [B](#A2 "Appendix B Recognize Implicit Heuristics of In-Context Examples
    by GPT-4 ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language
    Models for Document-Level Event Argument Extraction").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，手动估计启发式方法的数量可能引入主观性，导致潜在的偏差。因此，挑战在于客观识别示例中的隐含启发式方法。在这项工作中，我们使用GPT-4来识别每个示例中的隐含启发式方法，并确定是否存在跨多个示例的共享启发式方法。我们使用的提示及GPT-4识别的启发式方法的详细示例见附录[B](#A2
    "附录 B 通过GPT-4识别上下文示例的隐含启发式方法 ‣ 启发式驱动的类比提示：提升大型语言模型在文档级事件论证提取中的表现")。
- en: 'We investigate the relationship between the number of examples used in the
    prompt and the number of heuristics inherent in the corresponding prompt, across
    six cutting-edge prompting papers on three datasets. Specifically, SOTA prompting
    methods including CoT (Wei et al., [2022](#bib.bib38)), Automate-CoT (Shum et al.,
    [2023](#bib.bib31)), Auto-CoT (Zhang et al., [2023](#bib.bib43)), Iter-CoT (Sun
    et al., [2023](#bib.bib34)), Boosted (Pitis et al., [2023](#bib.bib27)), Active-CoT
    (Diao et al., [2023](#bib.bib5)) are investigated and datasets of commonsense
    reasoning and arithmetic reasoning are evaluated. The experimental results are
    demonstrated in Figure [3](#S2.F3 "Figure 3 ‣ 2 What LLMs learn from the demonstration?
    ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction"). It is observable that in meticulously
    designed prompts, the number of heuristics closely matches the number of examples.
    Furthermore, the number of heuristics in these carefully constructed prompts significantly
    exceeds that in prompts composed of randomly selected examples. This observation
    substantiates our hypothesis that well-constructed prompts indeed encapsulate
    diverse heuristics in examples.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了提示中使用的示例数量与对应提示中固有启发式数量之间的关系，涵盖了三种数据集上的六篇前沿提示研究论文。具体来说，我们研究了包括CoT（Wei等人，[2022](#bib.bib38)）、Automate-CoT（Shum等人，[2023](#bib.bib31)）、Auto-CoT（Zhang等人，[2023](#bib.bib43)）、Iter-CoT（Sun等人，[2023](#bib.bib34)）、Boosted（Pitis等人，[2023](#bib.bib27)）、Active-CoT（Diao等人，[2023](#bib.bib5)）在内的SOTA（State-of-the-Art）提示方法，并评估了常识推理和算术推理的数据集。实验结果展示在图[3](#S2.F3
    "图 3 ‣ 2 大型语言模型从演示中学到了什么？ ‣ 启发式驱动的类比提示：增强大型语言模型的文档级事件论证提取")中。可以观察到，在精心设计的提示中，启发式的数量与示例数量密切匹配。此外，这些精心构建的提示中的启发式数量显著高于由随机选择的示例组成的提示。这一观察结果证实了我们的假设，即精心构建的提示确实包含了多样化的启发式方法。
- en: '![Refer to caption](img/cb5008944067c92086af0d94b1520a95.png)![Refer to caption](img/bc6dc26f467b60c07264ce7cfa4ef01a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cb5008944067c92086af0d94b1520a95.png)![参考说明](img/bc6dc26f467b60c07264ce7cfa4ef01a.png)'
- en: 'Figure 4: Comparison of ICL performance using single-heuristic strategy versus
    diverse-heuristics strategy across different number of example on the StrategyQA
    and SST-2 Dataset.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用单一启发式策略与多样化启发式策略在不同示例数量下的ICL性能比较，基于StrategyQA和SST-2数据集。
- en: '![Refer to caption](img/d35914d1238e191fa862ed7102a55051.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d35914d1238e191fa862ed7102a55051.png)'
- en: 'Figure 5: An illustration of HD-LoA prompting.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：HD-LoA提示的示意图。
- en: 2.2 Comparing Diverse-Heuristics and Single-Heuristic Strategies
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 比较多样化启发式与单一启发式策略
- en: If our hypothesis holds true, then incorporating a broader range of heuristics
    in demonstrations should enable LLMs to identify and leverage more heuristics,
    leading to enhanced performance. With this in mind, we examine the impact of heuristic
    diversity in in-context examples on the performance of ICL. Specifically, we juxtapose
    two in-context example selection strategies on the Strategy QA dataset. The single-heuristic
    strategy formulates prompts where all rationales of in-context examples follow
    a same heuristic. Conversely, the diverse-heuristic strategy constructs prompts
    where all rationales of in-context examples exhibit different heuristics. We construct
    prompts that follow the two strategies based on the original prompts from Diao
    et al. ([2023](#bib.bib5)); Shum et al. ([2023](#bib.bib31)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的假设成立，那么在演示中融入更多的启发式方法应能使大型语言模型（LLMs）识别和利用更多的启发式方法，从而提升性能。考虑到这一点，我们研究了上下文示例中的启发式多样性对ICL（In-Context
    Learning）性能的影响。具体来说，我们对比了在Strategy QA数据集上两种上下文示例选择策略。单一启发式策略制定的提示中，上下文示例的所有理由都遵循相同的启发式方法。相对而言，多样化启发式策略构建的提示中，上下文示例的所有理由展示了不同的启发式方法。我们根据Diao等人的原始提示（[2023](#bib.bib5)）和Shum等人的原始提示（[2023](#bib.bib31)）构建了遵循这两种策略的提示。
- en: 'The performance comparison of prompts constructed by the diverse-heuristic
    strategy and the single-heuristic strategy on the StrategyQA (Geva et al., [2021](#bib.bib14))
    and SST-2 (Socher et al., [2013](#bib.bib32)) datasets is illustrated in Figure
    [4](#S2.F4 "Figure 4 ‣ 2.1 Correlation between Example Quantity and Heuristic
    Diversity in Well-Designed Prompts ‣ 2 What LLMs learn from the demonstration?
    ‣ Heuristics-Driven Link-of-Analogy Prompting: Enhancing Large Language Models
    for Document-Level Event Argument Extraction"). From the results, it’s evident
    that with the same number of examples, the diverse-heuristics strategy significantly
    outperforms the single-heuristic approach. This finding also supports our hypothesis
    that LLMs indeed learn heuristics from in-context examples.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S2.F4 "图 4 ‣ 2.1 示例数量与设计良好的提示中的启发式多样性之间的相关性 ‣ 2 LLM 从演示中学到什么？ ‣ 启发式驱动的类比链接提示：增强大语言模型以进行文档级事件论证提取")展示了多启发式策略与单一启发式策略在StrategyQA
    (Geva et al., [2021](#bib.bib14))和SST-2 (Socher et al., [2013](#bib.bib32))数据集上的提示性能比较。从结果来看，同样数量的示例中，多启发式策略显著优于单一启发式方法。这一发现也支持了我们的假设，即LLM确实从上下文示例中学习启发式。
- en: 3 Heuristic-Driven Demonstration Construction
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 启发式驱动的示例构建
- en: 'Building on our understanding of heuristic learning during ICL, we aim to address
    the challenge of example selection for ICL. From our validation experiments in
    the previous section, we observed that: (1) LLMs can recognize the heuristics
    underlying in-context examples and leverage them to enhance performance. (2) These
    heuristics are implicitly embedded within the explanations of in-context examples.
    As a result, LLMs’ utilization of heuristics involves a two-step process: first
    recognizing the implicit heuristics and then making inferences based on these
    heuristics.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在ICL期间对启发式学习的理解，我们旨在解决ICL示例选择的挑战。通过前一部分的验证实验，我们观察到： (1) LLM可以识别上下文示例中隐含的启发式，并利用这些启发式提升性能。
    (2) 这些启发式隐含在上下文示例的解释中。因此，LLM对启发式的利用涉及两个步骤：首先识别隐含的启发式，然后基于这些启发式进行推断。
- en: 'The above observation leads to our idea: Can we simplify the two-step process
    by explicitly providing LLMs with task-specific heuristics? Specifically, rather
    than providing examples of input-output pairs with heuristics implicitly embedded
    in the output, our approach replaces most examples in the prompt with distinct
    task-specific heuristics, as demonstrated by the heuristics in Figure [5](#S2.F5
    "Figure 5 ‣ 2.1 Correlation between Example Quantity and Heuristic Diversity in
    Well-Designed Prompts ‣ 2 What LLMs learn from the demonstration? ‣ Heuristics-Driven
    Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level
    Event Argument Extraction").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上述观察引发了我们的想法：我们是否可以通过明确提供特定任务的启发式来简化这两个步骤？具体而言，与其提供带有启发式的输入-输出对示例，我们的方法用不同的任务特定启发式替换了提示中的大部分示例，如图[5](#S2.F5
    "图 5 ‣ 2.1 示例数量与设计良好的提示中的启发式多样性之间的相关性 ‣ 2 LLM 从演示中学到什么？ ‣ 启发式驱动的类比链接提示：增强大语言模型以进行文档级事件论证提取")所示。
- en: 'It is worth noting that our method still retains a minimum number of examples
    for two reasons: First, the formatting of reasoning steps, such as the link-of-analogy
    prompting proposed in the following section, requires clear illustration through
    the output of an example. Second, a minimum number of examples are necessary to
    mitigate bias in the answers. For instance, in sentiment analysis tasks, it is
    essential to provide at least two examples, one positive and one negative, to
    prevent bias towards a particular sentiment. In the context of EAE task, we only
    maintain one example to demonstrate the formatting of reasoning steps.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们的方法仍然保留了最少数量的示例，原因有两个：首先，如下部分提出的类比链接提示等推理步骤的格式化，需要通过示例的输出进行清晰的说明。其次，最少数量的示例是为了减轻答案中的偏见。例如，在情感分析任务中，必须提供至少两个示例，一个正面和一个负面，以防对特定情感产生偏见。在EAE任务的背景下，我们只保留一个示例来演示推理步骤的格式化。
- en: 'There are three advantages of our approach. Firstly, it provides a guidance
    on the example selection process. The example selection process of ICL is usually
    an indiscriminate, manual process (Liu et al., [2023b](#bib.bib21); Wei et al.,
    [2022](#bib.bib38); Zhou et al., [2022](#bib.bib44)). However, our method addresses
    the prevailing uncertainty in ICL example selection, converting the directionless
    and indiscriminate process into a methodical approach that emphasizes task-specific
    heuristics. In addition, the two-step recognize and inference process of utilizing
    heuristics is simplified to only one step, thus simplifying the reasoning complexity
    for LLMs. Finally, it condenses lengthy examples that consists of input-output
    pairs into compact heuristics, reducing the context length of prompts. Specifically,
    we propose the heuristic-driven demonstration construction method that contains
    two parts: low-level pattern demonstration and heuristics generation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法有三个优势。首先，它提供了关于示例选择过程的指导。ICL 的示例选择过程通常是一个随意的人工过程（Liu et al., [2023b](#bib.bib21);
    Wei et al., [2022](#bib.bib38); Zhou et al., [2022](#bib.bib44)）。然而，我们的方法解决了 ICL
    示例选择中普遍存在的不确定性，将无方向和随意的过程转变为一种有条理的方法，强调任务特定的启发式规则。此外，利用启发式规则的两步识别和推理过程被简化为仅一步，从而简化了
    LLMs 的推理复杂性。最后，它将包含输入输出对的冗长示例压缩为紧凑的启发式规则，从而减少了提示的上下文长度。具体来说，我们提出了一种启发式驱动的演示构建方法，包括两个部分：低级模式演示和启发式生成。
- en: To provide a clearer and more precise understanding of our heuristic-driven
    demonstration construction method, we also describe it using mathematical expressions
    as follows.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰、准确地理解我们的启发式驱动演示构建方法，我们还使用数学表达式进行了描述。
- en: 'In standard demonstration construction, given the question $\mathbf{Q}$ of
    the LLM’s output is generated as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准演示构建中，给定问题 $\mathbf{Q}$ 的 LLM 输出生成如下：
- en: '|  | $\displaystyle o_{j}=f_{LLM}(\mathbf{C}_{8},\mathbf{Q},o_{"  and  "<\t>"  in  the  document,  serves  to  indicate  the  presence  of  the  event.  Only  the  event  explicitly  linked  to  the  trigger  word  should  be  considered.Specifically,  you  will  use  the  pattern  in  the  pattern  list  below  to  identify  event  arguments,  and  re-evaluate  the  identified  argument  candidates  to  get  the  final  answer.pattern  list:[Semantic  Pattern:  [giver]  is  the  person,  group,  or  organization  in  the  document  that  gives  the  grant  or  gift.  For  example  "[John]  is  the  donor".Syntactic.SubjectVerbObject  Pattern:  [giver]  may  be  the  subject  of  a  verb  that  indicates  the  act  of  giving,  or  granting.  For  example,  "[John]  gave  Mary  a  book"Syntactic.PassiveVoiceSyntax  Pattern:  In  sentences  using  passive  voice,  the  [giver]  might  be  introduced  by  prepositions  such  as  ’by’  to  describe  the  act  of  giving  or  granting.  For  example,  "The  book  was  given  to  Mary  by  [John]".Syntactic.PrepositionalPhrases  Pattern:  [giver]  may  be  indicated  by  prepositions  such  as  ’from’,  and  ’of’,  which  typically  precede  the  giver  in  the  context  of  a  giving  or  granting  event.  For  example,  "She  received  a  gift  from  [John]".Syntactic.PossessiveNoun  Pattern:  [giver]  may  be  the  possessor  in  noun  phrases  that  describe  the  object  being  given  or  granted.  For  example,  "[John]’s  donation".Multi-Sentence  Pattern:  the  giver  is  not  explicitly  understood  from  only  one  sentence,  but  it  can  be  understood  by  combining  the  information  of  multiple  sentences.  For  example,  consider  "The  charity  received  a  generous  donation.  This  was  from  an  [anonymous  man]."  The  ’anonymous  man’  as  the  giver  is  not  explicit  in  the  second  sentence  alone.  However,  if  we  link  ’this’  in  the  second  sentence  to  the  ’donation’  in  the  first,  we  understand  that  the  ’anonymous  man’  is  the  giver  of  the  donation.]Example  task:Question:  What  are  the  event  arguments  of  giver,  beneficiary,  and  recipient  in  the  "transaction.transaction.giftgrantprovideaid"  event  in  the  provided  news  document?  The  trigger  word  is  "granted"  located  between  ""  and  "".  If  an  event  argument  is  explicitly  stated  in  the  document,  ensure  that  the  identified  argument  is  quoted  exactly  as  the  entity  appears  in  the  given  document.  Note,  if  an  event  argument  is  not  explicitly  indicated  in  the  document,  or  is  not  directly  linked  to  the  event  signaled  by  the  trigger  word,  use  "not  specified"  as  the  answer.’Document:  a  news  documenttrigger  sentence:  "The  access  to  the  research  center  in  the  city  was  granted  by  the  administrator.  The  man,  Ripley  Johnson,  earned  it."Answer:Elaborate  the  meaning  of  event  type  and  its  argument  roles:"transaction.transaction.giftgrantprovideaid":  The  event  involves  a  transfer  of  money  or  resources  in  the  form  of  a  gift,  grant,  or  provision  of  aid,  signaled  by  the  action  of  granting.[giver]:  the  giver  is  the  person,  group,  or  organization  that  provides  or  grants  money,  resources,  or  access  in  the  event.[beneficiary]:  the  beneficiary  is  the  party  who  ultimately  benefits  from  the  transaction.[recipient]:  the  recipient  is  the  entity  that  receives  the  money,  resources,  or  access  granted  in  the  event.Recognizing  [giver]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [giver]  in  the  news  document:  Semantic  Pattern  and  Syntactic.PassiveVoiceSyntax  Pattern  because  "was  granted"  indicates  Syntactic.PassiveVoiceSyntax.Step  2:  Apply  selected  patterns  to  identify  [giver]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [giver]  based  on  Semantic  Pattern  f1:  "[giver]  is  the  person,  group,  or  organization  that  gives  the  grant  or  gift  in  the  document".  Applying  pattern  f1  to  the  document,  "administrator"  is  the  entity  that  gives  access  of  the  research  center.Step  2.2  Identify  the  [giver]  based  on  Syntactic.PassiveVoiceSyntax  Pattern  f2:  "In  sentences  using  passive  voice,  the  [giver]  might  be  the  object  of  a  verb  describe  the  act  of  giving  or  granting."  Applying  pattern  f2  to  the  document,  "research  center"  is  the  object  of  "granted"  which  describes  the  act  of  giving  or  granting.Step  3:  reevaluate_argument_candidates(["administrator",  "research  center"]):Is  argument  "administrator"  alignment  with  the  argument  role  [giver]?  yes,  because  "administrator"  granted  access  to  the  research  center.Is  argument  "research  center"  alignment  with  the  argument  role  [giver]?  no,  because  "research  center"  is  the  place  that  access  has  been  granted  to,  but  it  doesn’t  give  access.[giver]:  "administrator"Recognizing  [beneficiary]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [beneficiary]  in  the  news  document:  Semantic  PatternStep  2:  Apply  selected  patterns  to  identify  [beneficiary]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [beneficiary]  based  on  semantic  pattern  f1:  "[beneficiary]  is  the  entity  that  ultimately  benefits  from  the  gift  or  grant".  Applying  pattern  f1  to  the  given  document,  the  entity  that  ultimately  benefits  from  the  grant  is  "not  specified".Step  3:  reevaluate_argument_candidates("not  specified"):Is  argument  "not  specified"  alignment  with  argument  role  [beneficiary]?  Yes,  the  [beneficiary]  is  not  explicitly  mentioned  so  "not  specified"  is  correct.[beneficiary]:  "not  specified"Recognizing  [recipient]  in  the  given  document:Step  1  Select  one  or  two  patterns  in  the  pattern  list  that  are  best  suited  to  identify  the  [recipient]  in  the  news  document:  Semantic  Pattern  and  Multi-Sentence  Pattern  because  recognizing  [recipient]  requires  an  understanding  of  "it"  in  "earned  it"  across  sentencesStep  2:  Apply  selected  patterns  to  identify  [recipient]  independently.  Each  pattern  should  be  applied  in  isolation,  without  considering  the  other  pattern.Step  2.1  Identify  the  [recipient]  based  on  Semantic  Pattern  f1:  "[recipient]  is  the  entity  that  receives  the  gift  or  grant".  Applying  pattern  f1  to  the  given  document,  the  entity  that  receives  the  gift  or  grant  is  "Ripley  Johnson".Step  2.2  Identify  the  [recipient]  based  on  Multi-Sentence  Pattern  f2:  in  the  phrase  "The  man,  Ripley  Johnson,  earned  it",  "it"  refers  to  the  access.  Therefore,  "The  man"  earned  the  granted  access.  [recipient]  is  "The  man".Step  3:  reevaluate_argument_candidates(["Ripley  Johnson",  "The  man"]):Is  argument  "Ripley  Johnson"  alignment  with  the  argument  role  [recipient]?  yes,  because  "Ripley  Johnson"  earned  the  access.Is  argument  "Ripley  Johnson"  alignment  with  the  argument  role  [recipient]?  yes,  because  "The  man"  earned  the  access."The  man"  is  preferred  because  it  is  closer  to  the  trigger  word.[recipient]:  "The  man"Target  task:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 指示：你的任务是事件论元提取。在此任务中，你将获得一个描述事件的文档。你的工作是识别并提取与事件相关的每个论元角色相对应的关键组成部分（事件论元）。触发词位于文档中的特殊标记
    "" 和 "<\t>" 之间，用于指示事件的存在。只有与触发词明确关联的事件应被考虑。具体而言，你将使用以下模式列表中的模式来识别事件论元，并重新评估识别出的论元候选以获得最终答案。模式列表：[语义模式：
    [giver] 是文档中给予赠款或礼物的个人、团体或组织。例如 "[John] 是捐赠者"。句法.主谓宾模式：[giver] 可能是指示给予或授予行为的动词的主语。例如，"[John]
    给了 Mary 一本书"。句法.被动语态模式：在使用被动语态的句子中，[giver] 可能通过诸如 ’by’ 的介词引入，以描述给予或授予的行为。例如，“这本书由
    [John] 赠送给 Mary”。句法.介词短语模式：[giver] 可能通过介词如 ’from’ 和 ’of’ 指出，这些介词通常在给予或授予事件的上下文中位于给予者之前。例如，“她从
    [John] 那里收到了一份礼物”。句法.所有格名词模式：[giver] 可能是描述被给予或授予物体的名词短语中的所有者。例如，"[John] 的捐赠"。多句模式：给予者在单个句子中没有明确提及，但可以通过结合多个句子中的信息来理解。例如，“慈善机构收到了一笔慷慨的捐赠。这来自一位
    [匿名人士]。”在第二个句子中，‘匿名人士’ 作为给予者并不明确。然而，如果我们将第二个句子中的‘这’与第一个句子中的‘捐赠’关联起来，我们可以理解‘匿名人士’
    是捐赠的给予者。] 示例任务：问题：在提供的新闻文档中，“transaction.transaction.giftgrantprovideaid”事件的 giver、beneficiary
    和 recipient 论元是什么？触发词是 "granted"。如果事件论元在文档中明确陈述，请确保所识别的论元与文档中出现的实体完全一致。注意，如果事件论元未在文档中明确指出，或未直接与触发词所示事件关联，请使用
    "未指定" 作为答案。文档：新闻文档 触发句子：“城市的研究中心的访问权限由管理员 授予。这个人，Ripley Johnson，赚到了它。”答案：阐述事件类型及其论元角色的含义：“transaction.transaction.giftgrantprovideaid”：该事件涉及以赠礼、资助或援助的形式转移金钱或资源，由授予的行为信号表示。[giver]：给予者是指在事件中提供或授予金钱、资源或访问权限的个人、团体或组织。[beneficiary]：受益者是最终从交易中获益的一方。[recipient]：接受者是事件中获得授予的金钱、资源或访问权限的实体。识别文档中的
    [giver]：第1步：选择一个或两个最适合识别新闻文档中 [giver] 的模式：语义模式和句法.被动语态模式，因为“was granted” 指示句法.被动语态模式。第2步：独立应用所选模式识别
    [giver]。每个模式应单独应用，不考虑其他模式。第2.1步：基于语义模式 f1 识别 [giver]：“[giver] 是在文档中给予资助或礼物的个人、团体或组织”。应用模式
    f1 到文档中，“管理员” 是给予研究中心访问权限的实体。第2.2步：基于句法.被动语态模式 f2 识别 [giver]：“在使用被动语态的句子中，[giver]
    可能是描述给予或授予行为的动词的宾语。”应用模式 f2 到文档中，“研究中心” 是“授予”的宾语，描述了给予或授予的行为。第3步：重新评估论元候选（["管理员",
    "研究中心"]）：论元 “管理员” 是否与 [giver] 的论元角色一致？是的，因为“管理员” 授予了研究中心的访问权限。论元 “研究中心” 是否与 [giver]
    的论元角色一致？不，因为“研究中心” 是获得访问权限的地方，但它并不授予访问权限。[giver]：“管理员” 识别文档中的 [beneficiary]：第1步：选择一个或两个最适合识别新闻文档中
    [beneficiary] 的模式：语义模式。第2步：独立应用所选模式识别 [beneficiary]。每个模式应单独应用，不考虑其他模式。第2.1步：基于语义模式
    f1 识别 [beneficiary]：“[beneficiary] 是最终从礼物或赠款中获益的实体。”应用模式 f1 到给定文档中，最终从赠款中获益的实体是“未指定”。第3步：重新评估论元候选（“未指定”）：论元
    “未指定” 是否与 [beneficiary] 的论元角色一致？是的，[beneficiary] 没有明确提到，所以“未指定” 是正确的。[beneficiary]：“未指定”
    识别文档中的 [recipient]：第1步：选择一个或两个最适合识别新闻文档中 [recipient] 的模式：语义模式和多句模式，因为识别 [recipient]
    需要理解“earned it” 中的“it” 跨句子。第2步：独立应用所选模式识别 [recipient]。每个模式应单独应用，不考虑其他模式。第2.1步：基于语义模式
    f1 识别 [recipient]：“[recipient] 是接受礼物或赠款的实体。”应用模式 f1 到给定文档中，接受礼物或赠款的实体是“Ripley
    Johnson”。第2.2步：基于多句模式 f2 识别 [recipient]：在短语 “这个人，Ripley Johnson，赚到了它” 中，“它” 指的是访问权限。因此，“这个人”
    赚到了授予的访问权限。[recipient] 是“这个人”。第3步：重新评估论元候选（["Ripley Johnson", "这个人"]）：论元 “Ripley
    Johnson” 是否与 [recipient] 的论元角色一致？是的，因为“Ripley Johnson” 赚到了访问权限。论元 “Ripley Johnson”
    是否与 [recipient] 的论元角色一致？是的，因为“这个人” 赚到了访问权限。“这个人” 更合适，因为它离触发词更近。[recipient]：“这个人”
- en: C.4 The demonstration for DocEE Dataset
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4 DocEE 数据集的演示
- en: 'Instruction:  Your  task  is  Event  Argument  Extraction.  In  this  task,  you  will  be  provided  with  a  document  that  describes  an  event.  Your  job  is  to  extract  and  quote  the  entities  (event  arguments)  from  the  document  that  correspond  to  each  argument  role  associated  with  the  event.Specifically,  you  will  use  the  patterns  in  the  "Pattern  List"  below  to  identify  event  arguments.  The  answers  should  also  satisfy  the  "Argument  Format  Principle"  and  "Argument  Number  Principle"  below.Pattern  List:["Semantic  Pattern":  [giver]  is  the  person,  group,  or  organization  in  the  document  that  gives  the  grant  or  gift.  For  example,  "[John]  is  the  donor"."Syntactic.SubjectVerbObject  Pattern":  [giver]  may  be  the  subject  of  a  verb  that  indicates  the  act  of  giving,  or  granting.  For  example,  "[John]  gave  Mary  a  book"."Syntactic.PrepositionalPhrases  Pattern":  [giver]  may  be  indicated  by  prepositions  such  as  ’from’,  and  ’of’,  which  typically  precede  the  giver  in  the  context  of  a  giving  or  granting  event.  For  example,  "She  received  a  gift  from  [John]".]Argument  Extracting  Principles:["Argument  Format  Principle":  articles  and  prepositions  are  not  included  in  the  identified  event  argument.  For  example,  select  "damaged  car"  among  "damaged  car",  "damaged  car  belonging  to  the  victim"  and  "the  damaged  car";  select  "the  officer"  rather  than  "the  officer  under  investigation"."Argument  Number  Principle":  Most  event  arguments  only  have  one  unique  mention.  However,  only  in  situations  where  you  are  strongly  sure  that  the  mentions  associated  with  an  argument  role  are  distinctly  different,  you  are  permitted  to  extract  <=  3  mentions.]Example  Task:Question:  What  are  the  event  arguments  of  ’Date’,  ’Magnitude’,  ’Number  of  Destroyed  Building’,  ’Casualties  and  Losses’  in  the  ’Earthquakes’  event  in  the  provided  news  document  below?  Ensure  that  each  identified  argument  is  quoted  exactly  as  the  entity  appears  in  the  given  document.  If  an  event  argument  is  not  explicitly  mentioned,  please  answer  "not  specified".Document:  a  given  newsAnswer:Elaborate  the  meaning  of  event  type  and  its  argument  roles:’Earthquakes’:  The  event  involves  the  shaking  of  the  surface  of  the  Earth  resulting  from  a  sudden  release  of  energy  in  the  Earth’s  lithosphere.[Date]:  when  the  earthquake  occurred.[Magnitude]:  the  measure  of  the  size  of  the  earthquake.[Number  of  Destroyed  Building]:  the  number  of  buildings  or  structures  that  were  damaged  or  destroyed  due  to  the  earthquake.[Casualties  and  Losses]:  the  number  of  people  killed  or  injured,  and  the  amount  of  economic  losses  caused  by  the  earthquake.Recognizing  [Date]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Date]  in  the  news  document:  Semantic  Pattern.Step  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Date]  is  when  the  earthquake  occurred".  The  [Date]  is  "not  specified"  because  the  document  didn’t  mention  when  the  earthquake  occurred.Step  3:  reevaluate_argument_candidates(["not  specified"]):Is  "not  specified"  alignment  with  the  argument  role  [Date]?  Yes,  because  [Date]  is  not  explicitly  mentioned  in  the  document,  so  "not  specified"  is  correct.[Date]:  "not  specified"Recognizing  [Magnitude]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Magnitude]  in  the  news  document:  Semantic  Pattern.Step  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Magnitude]  is  the  measure  of  the  size  of  the  earthquake".  The  [Magnitude]  is  "6.6"  because  the  document  mentions  "The  magnitude  6.6  shock".Step  3:  reevaluate_argument_candidates(["6.6"]):Is  "6.6"  alignment  with  the  argument  role  [Magnitude]?  Yes,  because  it  quantifies  the  size  of  the  earthquake.[Magnitude]:  "6.6"Recognizing  [Number  of  Destroyed  Building]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Number  of  Destroyed  Building]  in  the  news  document:  Semantic  PatternStep  2.1  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Number  of  Destroyed  Building]  is  the  people  killed,  injured,  or  properties  affected  by  the  drought.".  The  [Number  of  Destroyed  Building]  is  "more  than  80%  of  the  houses"  because  the  document  mentions  this  specific  proportion  related  to  houses  destroyed.Step  3:  reevaluate_argument_candidates(["more  than  80%  of  the  houses"]):Is  "more  than  80%  of  the  houses"  alignment  with  the  argument  role  [Number  of  Destroyed  Building]?  No,  because  it  describes  the  percentage  not  the  specific  number  of  buildings  destroyed.[Number  of  Destroyed  Building]:  "not  specified"Recognizing  [Casualties  and  Losses]  in  the  given  document:Step  1  Select  a  pattern  in  the  pattern  list  that  are  best  suited  to  identify  the  [Casualties  and  Losses]  in  the  news  document:  Semantic  PatternStep  2  Identify  the  argument  based  on  Semantic  Pattern  f1:  "[Casualties  and  Losses]  is  the  number  of  people  killed  or  injured,  and  the  amount  of  economic  losses  caused  by  the  earthquake".  The  [Casualties  and  Losses]  is  "claimed  142  deaths"  and  "800  houses  were  damaged"  because  the  document  provides  this  explicit  numbers  about  people  killed  due  to  the  earthquake  and  mentions  this  number  of  houses  destroyed,  representing  economic  losses.Step  3:  reevaluate_argument_candidates(["claimed  142  deaths",  "800  houses  were  damaged"]):Is  "claimed  142  deaths"  alignment  with  the  argument  role  [Casualties  and  Losses]?  Yes,  because  it  provides  details  about  the  number  of  people  killed  or  injured  due  to  the  earthquake.Is  "800  houses  were  damaged"  alignment  with  the  argument  role  [Casualties  and  Losses]?  Yes,  because  it  describes  the  extent  of  property  damage  caused  by  the  earthquake.[Casualties  and  Losses]:  "claimed  142  deaths",  "800  houses  were  damaged"End  of  answerTarget  task:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 指令：你的任务是事件参数提取。在这个任务中，你将获得一个描述事件的文档。你的工作是从文档中提取并引用与事件相关的每个参数角色相对应的实体（事件参数）。具体来说，你将使用下面的“模式列表”中的模式来识别事件参数。答案还应满足下面的“参数格式原则”和“参数数量原则”。模式列表：[语义模式]：[giver]
    是文档中给予资助或礼物的个人、团体或组织。例如，“[John] 是捐赠者”。[句法.主语动词宾语模式]：[giver] 可能是表示给予或授予行为的动词的主语。例如，“[John]
    给 Mary 一本书”。[句法.介词短语模式]：[giver] 可能通过如“from”和“of”等介词来表示，这些介词通常在给予或授予事件的上下文中位于 giver
    前面。例如，“她从 [John] 那里收到了一份礼物”。]参数提取原则：[参数格式原则]：在识别的事件参数中不包括冠词和介词。例如，从“损坏的车”，“属于受害者的损坏的车”和“损坏的车”中选择“损坏的车”；选择“警官”而不是“正在调查中的警官”。[参数数量原则]：大多数事件参数只有一个唯一的提及。然而，只有在你非常确定与参数角色相关的提及明显不同的情况下，你才允许提取
    <= 3 个提及。]示例任务：问题：在提供的新闻文档中，“地震”事件的“日期”，“震级”，“受损建筑数量”，“伤亡和损失”事件参数是什么？确保每个识别出的参数都与文档中的实体完全一致。如果事件参数没有明确提及，请回答“未指定”。文档：某新闻。答案：阐述事件类型及其参数角色的含义：“地震”：该事件涉及由于地球地壳突然释放能量而引起的地球表面的震动。[日期]：地震发生的时间。[震级]：地震大小的衡量标准。[受损建筑数量]：因地震造成损坏或毁坏的建筑物或结构的数量。[伤亡和损失]：因地震造成的人员死亡或受伤的数量，以及经济损失的金额。识别给定文档中的
    [日期]：步骤1 选择最适合识别新闻文档中 [日期] 的模式列表中的模式：语义模式。步骤2.1 根据语义模式 f1 识别参数：“[日期] 是地震发生的时间”。[日期]
    是“未指定”，因为文档中没有提及地震发生的时间。步骤3：重新评估参数候选项（[“未指定”]）：“未指定”是否符合 [日期] 的参数角色？是，因为文档中没有明确提及
    [日期]，所以“未指定”是正确的。[日期]：“未指定”识别给定文档中的 [震级]：步骤1 选择最适合识别新闻文档中 [震级] 的模式列表中的模式：语义模式。步骤2.1
    根据语义模式 f1 识别参数：“[震级] 是地震大小的衡量标准”。[震级] 是“6.6”，因为文档提到“震中震级为 6.6”。步骤3：重新评估参数候选项（[“6.6”]）：“6.6”是否符合
    [震级] 的参数角色？是，因为它量化了地震的大小。[震级]：“6.6”识别给定文档中的 [受损建筑数量]：步骤1 选择最适合识别新闻文档中 [受损建筑数量]
    的模式列表中的模式：语义模式。步骤2.1 根据语义模式 f1 识别参数：“[受损建筑数量] 是因干旱造成的人员死亡、受伤或财产受损的数量。”[受损建筑数量]
    是“超过 80% 的房屋”，因为文档提到与受损房屋相关的具体比例。步骤3：重新评估参数候选项（[“超过 80% 的房屋”]）：“超过 80% 的房屋”是否符合
    [受损建筑数量] 的参数角色？不，因为它描述了百分比而不是具体的受损建筑数量。[受损建筑数量]：“未指定”识别给定文档中的 [伤亡和损失]：步骤1 选择最适合识别新闻文档中
    [伤亡和损失] 的模式列表中的模式：语义模式。步骤2 根据语义模式 f1 识别参数：“[伤亡和损失] 是因地震造成的人员死亡或受伤的数量，以及经济损失的金额”。[伤亡和损失]
    是“声称 142 人死亡”和“800 所房屋受损”，因为文档提供了关于地震造成的人员死亡的明确数字，并提到了这一数量的房屋受损，代表了经济损失。步骤3：重新评估参数候选项（[“声称
    142 人死亡”，“800 所房屋受损”]）：“声称 142 人死亡”是否符合 [伤亡和损失] 的参数角色？是，因为它提供了有关因地震造成的人员死亡或受伤的详细信息。“800
    所房屋受损”是否符合 [伤亡和损失] 的参数角色？是，因为它描述了地震造成的财产损害的程度。[伤亡和损失]：“声称 142 人死亡”，“800 所房屋受损”
