- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by
    Auto-regressive LLMs’ Prompting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10474](https://ar5iv.labs.arxiv.org/html/2405.10474)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xinzhe Li School of IT, Deakin University, Australia
  prefs: []
  type: TYPE_NORMAL
- en: '{lixinzhe, m.liu}@deakin.edu.au Ming Liu School of IT, Deakin University, Australia'
  prefs: []
  type: TYPE_NORMAL
- en: '{lixinzhe, m.liu}@deakin.edu.au'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Over the last decade, a wide range of training and deployment strategies for
    Large Language Models (LLMs) have emerged. Among these, the prompting paradigms
    of Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in Artificial
    Intelligence (AI). This paper aims to emphasize *the significance of utilizing
    free-form ¹¹1“Free-form” describes a stream of meaningful symbols, created by
    humans or through auto-regressive methods. modalities (forms of input and output)
    and verbal free-form contexts as user-directed channels (methods for transforming
    modalities) for downstream deployment*. Specifically, we analyze the structure
    of modalities within both two types of LLMs and six task-specific channels during
    deployment. From the perspective of users, our analysis introduces and applies
    the analytical metrics of task customizability, transparency, and complexity to
    gauge their usability, highlighting the superior nature of AR-LLMs’ prompting
    paradigms. Moreover, we examine the stimulation of diverse cognitive behaviors
    in LLMs through the adoption of free-form text and verbal contexts, mirroring
    human linguistic expressions of such behaviors. We then detail four common cognitive
    behaviors to underscore how AR-LLMs’ prompting successfully imitate human-like
    behaviors using this free-form modality and channel. Lastly, the potential for
    improving LLM deployment, both as autonomous agents and within multi-agent systems,
    is identified via cognitive behavior concepts and principles.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatGPT has emerged as the most popular AI application, with a vast user base.
    The success of GPT models can be attributed to the scaling of transformer-based
    neural networks and the extensive pre-training data, as explored in previous studies
    (Radford et al., [2019](#bib.bib25); Brown et al., [2020](#bib.bib3)). The scope
    of this paper is directed towards Large Language Models (LLMs) that are sufficiently
    large to acquire world knowledge, commonsense, and the linguistic capabilities
    required to attain high performance on benchmarks such as GLUE (Wang et al., [2019](#bib.bib34)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although LLMs are commonly perceived as general-purpose language intelligence
    models, the practice often diverges from employing a singular, all-encompassing
    model for every task. Instead, the deployment frequently entails developing a
    suite of specialized models tailored to specific tasks. This specialization is
    facilitated through the introduction of task-specific channels, modifying the
    model’s structure or its pre-trained parameters to better suit the nuances of
    individual tasks. This highlights a departure from the ideal of a universal, one-size-fits-all
    model, while the broad capabilities of LLMs suggest they could serve as jack-of-all-trades
    in language processing. This trend towards creating task-specific models may stem
    from the tradition of evaluating linguistic intelligence through a variety of
    distinct tasks and benchmarks (Wang et al., [2019](#bib.bib34)), with researchers
    striving to excel in these tasks independently to set new benchmarks. In this
    paper, we delve into the mechanisms behind prevalent deployment paradigms including
    AR-LLMs’ prompting, which underpins ChatGPT’s operation, and highlight several
    critical observations: 1) Models tailored with optimized task-specific channels
    often suffer from issues related to task customizability, transparency, and user-level
    complexity during deployment, affecting their overall usability; 2) Anticipated
    to mimic human-like intelligence, they often exhibit slow thinking through shortcuts
    (Kahneman, [2011](#bib.bib12)); 3) They frequently fall short in showcasing advanced
    cognitive behaviors, which we contend are vital for convincing users of the models’
    intelligence. Conversely, AR-LLMs’ prompting paradigms introduce a more natural,
    human-like channel (verbal free-form context) for representing a wide array of
    real-life tasks and employ form-form output modalities to showcase cognitive behaviors
    in complex scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in this paper, we commence by examining the foundational principles
    of language modeling, revisiting the notable split in language modeling approaches
    that emerged in the late 2010s: auto-encoding LMs (AE-LMs) exemplified by BERT
    (Jin et al., [2020](#bib.bib10)) and auto-regressive LMs (AR-LMs) exemplified
    by the GPT series (Radford et al., [2018](#bib.bib24); Brown et al., [2020](#bib.bib3)).
    Rather than delve into an extensive array of deployment paradigms, we introduce
    and discuss the concepts of modalities and channels to investigate how LLMs are
    deployed (§[2](#S2 "2 Deploying Large Language Models ‣ Rethinking ChatGPT’s Success:
    Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting")).
    Upon evaluating different deployment paradigms for LLMs, it becomes clear that
    aside from the AR-LLMs’ prompting approach, other paradigms struggle to demonstrate
    advanced human-like cognitive behaviors. This shortfall is attributed to the constraints
    within modalities and channels, coupled with a tendency towards superficial learning,
    i.e., slow thinking (§[3](#S3 "3 Evaluation of Modalities and Channels ‣ Rethinking
    ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive
    LLMs’ Prompting") and §[4.1](#S4.SS1 "4.1 Thinking, Fast And Slow ‣ 4 Cognitive
    Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability
    and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting")). In contrast,
    via specified context in the free-from text, the AR-LLMs’ prompting strategy imitate
    human-like cognitive behaviors, such as reasoning, planning, and feedback learning,
    which are elucidated in Table [2](#S4.T2 "Table 2 ‣ Mechanism for Feedback Learning
    ‣ 4.4 Feedback Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm
    ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive
    LLMs’ Prompting") (§[4](#S4 "4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm
    ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive
    LLMs’ Prompting")). Finally, we explore how understanding cognitive behaviors
    can help overcome the tuning and deployment obstacles encountered by LLMs functioning
    as autonomous entities and within multi-agent frameworks (§[5](#S5 "5 Bridging
    LLM Deployment Gaps with Insights from Cognitive Behaviors ‣ Rethinking ChatGPT’s
    Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deploying Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section elucidates the dual objectives underlying language models, which
    both aim to model the joint probability distribution of text sequences through
    self-supervised learning techniques and generate text that is relevant to the
    given context. After this introduction, we present a novel framework that facilitate
    the characterization of various deployment paradigms through two types of data
    modalities, which support language comprehension, coupled with six unique channels
    for processing these modalities.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 The Fundamental Dichotomy in Language Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Objective of Language Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of language modeling is to estimate the joint probability distribution
    of sequences of text (Bengio et al., [2003](#bib.bib2)). This involves developing
    two distinct yet relaxed formulations for constructing LLMs that leverage self-supervised
    learning from vast quantities of unlabeled text data. The self-supervised approach
    enables the training of LLMs on extensive text corpora, a practice that has been
    thoroughly investigated in various studies (Liu et al., [2019](#bib.bib18); Wei
    et al., [2022a](#bib.bib39)). This paper focuses on how the intrinsic design of
    language models impacts their usability and potential to express cognitive behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-Regressive (Left-to-Right) Language Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typically, language modeling is approached by predicting the subsequent token
    in a sequence based on the preceding tokens. This prediction is quantified as
    the product of conditional probabilities for each subsequent token, considering
    its previous tokens, in accordance with the chain rule (Bengio et al., [2003](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}P\left(w_{1},\ldots,w_{N}\right)=\prod_{t=1}^{N}P\left(w_{t}\mid
    w_{0},\ldots,w_{t-1}\right)\end{split}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Here, $w_{0}$ serves as a marker for the beginning of text.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-Encoding (Denoising) Language Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the context of auto-encoding language modeling, noise is intentionally introduced
    to an input sequence $w_{1},w_{2},...w_{N}$. The primary aim is to optimize
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max\prod_{t=1}^{N}P\left(w_{t}\mid\hat{w}_{1},\ldots,\hat{w}_{N}\right)$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\hat{w}_{1},\hat{w}_{2},...\hat{w}_{N}$ represents the altered, noise-added
    version of the input sequence. The approach of masking specific tokens in the
    text at random, known as token-level masked language modeling (Devlin et al.,
    [2019](#bib.bib4)), is a widely adopted strategy. This involves substituting original
    tokens with a special token, such as “[MASK]”, and training the model to predict
    these original tokens based on the context of the surrounding, unmasked tokens.
    The discrepancy between the original and reconstructed sequences is quantified
    through a reconstruction loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: This denoising methodology also includes other variants such as span-level masked
    language modeling (Joshi et al., [2020](#bib.bib11)), text infilling (Lewis et
    al., [2020](#bib.bib14)), among others.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Exploring the Modalities within Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section delves into the concept of “modalities” within LLMs, a term often
    implicitly associated with research on multimodal systems to describe diverse,
    human-like channels of communication, such as text, speech, gestures, and visual
    inputs (Bartneck et al., [2020](#bib.bib1)). Here, “modalities” specifically refer
    to the various forms of input and output data utilized in LLM deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the operation of both AR-LLMs and AE-LLMs, we identify three primary modalities:
    a unique textual modality for both the input and output in AR-LLMs (unrestricted
    text), a distinct textual modality for AE-LLMs (masked text or contextualized
    n-grams), and a shared modality of intermediate dense representations applicable
    to both models: 1) Intermediate Dense Representations: Fundamentally, LLMs convert
    each word (or subword) in a sequence into dense vector embeddings. These embeddings
    are generated through a series of mathematical operations, such as the self-attention
    mechanism, at every layer of the neural network, and are represented as $\left\{h_{i}^{l}\right\}$
    represents the complete count of layers within the model. 2) Textual Modalities:
    AE-LLMs feature an input modality of masked text, with the output modality being
    contextualized n-grams designed to reconstruct the masked sections. Conversely,
    due to their auto-regressive design, AR-LLMs are capable of encoding any text
    as context and generating free-form text outputs, thereby employing unrestricted
    text for both input and output. These modalities are inherently linked to their
    respective language modeling strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Channels | Relevant Paradigms | Customizability | Transparency | Complexity
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Adapter | Adapter tuning | ✗ | ✗ | $T$ |'
  prefs: []
  type: TYPE_TB
- en: '| Output layers | LLM fine-tuning; Adapter tuning | ✗ | ✗ | $T$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLMs | LLM fine-tuning; PET | ✗ | ✗ | $T$ |'
  prefs: []
  type: TYPE_TB
- en: '| Activation prefixes | Prefix tuning | ✗ | ✗ | $T$ |'
  prefs: []
  type: TYPE_TB
- en: '| Verbal free-form context | AR-LLMs’ prompting | ✓ | ✓ | $0$ |'
  prefs: []
  type: TYPE_TB
- en: '| Contextual text patterns | PET; Auto-prompt | ✗ | ✓(PET); ✗(Auto-prompt)
    | $N\times T$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Evaluation of deployment channels for language models: A comparative
    analysis of task customizability, transparency and complexity from the users’
    perspective. PET: Pattern exploitation training; $T$: the number of patterns per
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Task-specific Channels for Deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To tailor the core capabilities of LLMs for specific downstream tasks, both
    input and intermediate modalities can be altered directly (for instance, by appending
    prefixes or incorporating verbal context) or indirectly through the use of parametric
    modules such as neural networks, including adapters and output layers as described
    subsequently. It’s worth noting that direct modifications, such as prefixes, can
    also be achieved using parametric modules. These parametric modules undergo optimization
    via task-specific supervised learning. In this context, we describe the means
    for modality transformation aimed at specific tasks as task-specific channels.
    For clarify, modalities are the types of data or the form in which data is processed,
    while channels are the pathways or methods through which these data modalities
    are adapted or transformed for specific tasks. Task-specific channels encompass:
    1) Adapter: Adapters are compact neural networks that can be embedded between
    an LLM’s layers. A well-known approach, adapter tuning (Houlsby et al., [2019](#bib.bib8)),
    involves optimizing the adapter’s parameters while leaving the original LLM parameters
    intact. These adapters are designed to adjust the intermediate layer representations
    to better align with task-specific needs. 2) LLMs Themselves: An alternative strategy
    involves modifying the LLM directly to produce task-specific representations by
    fine-tuning the model’s weights across all or selected layers. This method of
    fine-tuning is prevalent for AE-LLMs (Jin et al., [2020](#bib.bib10)) and has
    also been applied to AR-LLMs in early use of GPT-like models (Radford et al.,
    [2018](#bib.bib24)). 3) Output Layers: Once task-specific representations are
    produced by either adapters or the LLM directly, the function of the output layers
    is to translate these representations into a designated output space. These layers
    typically consist of one or several linear layers. For example, linear functions
    are frequently used for tasks involving classification, while tasks that involve
    extractive question answering often necessitate the use of two linear functions
    to determine the beginning and concluding positions of the answer within a text
    passage. 4) Activation Prefixes: Within the scope of deploying LLMs via task-specific
    supervised learning, where training neural networks is common, prefix tuning (Li
    and Liang, [2021](#bib.bib15)) presents an innovative method that employs prefixes
    to directly modify intermediate representations. These prefixes are essentially
    embeddings that are added at various layers, with dimensions identical to those
    of token embeddings, functioning as virtual tokens. Introducing these prefixes
    at earlier stages in the model allows for the infusion of task-specific information
    into more advanced layers, thereby improving the model’s alignment with the desired
    task objectives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the four channels previously outlined, verbal channels offer a unique
    approach for articulating the task context in which LLMs can identify and execute
    the intended tasks. These channels include: 5) Verbal Free-form Context: In this
    approach, a context is articulated using free-form text, such as task instructions
    and few-shot demonstrations, which can activate complex cognitive functions. By
    merely incorporating task instructions within the context, AR-LLMs are enabled
    to undertake a multitude of tasks through zero-shot prompts. Another widely adopted
    method is few-shot prompting (Radford et al., [2019](#bib.bib25); Brown et al.,
    [2020](#bib.bib3)), which involves learning from a limited number of examples
    for in-context learning without the need for gradient updates, showcasing a human-like
    efficiency in acquiring new tasks. This method is particularly effective in eliciting
    cognitive behaviors akin to those observed with few-shot demonstrations, with
    further details discussed in Section [4](#S4 "4 Cognitive Behaviors Under AR-LLMs’
    Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors
    Enabled by Auto-regressive LLMs’ Prompting"). It’s important to recognize that,
    in contrast to channels that are easily differentiated by input-side modalities
    (such as task-specific examples), this channel (e.g., task instructions) can intertwine
    with model inputs, e.g., task-specific examples. This allows for the seamless
    integration of the models’ world knowledge into tasks, for instance, “summarize
    deep learning technology”. 6) Contextual Text Patterns: Given their training on
    a denoising language model objective, AE-LLMs excel in completing texts by filling
    in missing words, a trait that can be leveraged for downstream tasks. Task-specific
    patterns, in this regard, serve as a mechanism to alter given task-specific examples.
    Typically, this involves appending the examples with a cloze-style phrase or sentence
    (text with missing words) tailored to the task, allowing the model to predict
    the intended task outcomes based on the placeholders filled within the text. Pattern
    Exploitation Training (PET) (Schick and Schütze, [2021](#bib.bib28)) involves
    the creative design of task-specific patterns and the fine-tuning of LLMs to these
    patterns. Conversely, auto-prompt methods (Shin et al., [2020](#bib.bib29)) seek
    to optimize task-specific patterns to better fit the models, enhancing their ability
    to interpret and respond to the given tasks effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Evaluation of Modalities and Channels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Evaluating Usability of Deployment Channels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces a framework for assessing the usability of language
    model deployment channels, focusing on their customizability, transparency, and
    complexity, as summarized in Table [1](#S2.T1 "Table 1 ‣ 2.2 Exploring the Modalities
    within Large Language Models ‣ 2 Deploying Large Language Models ‣ Rethinking
    ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive
    LLMs’ Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Customizability of User-level Tasks: Extent of User Control over Channels'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Essentially, any task can be articulated in human languages, such as English,
    using free-form context. This adaptability is a testament to the evolution of
    human language over thousands of years, which has been refined to describe a vast
    array of everyday and complex scientific problems. Typically, in a zero-shot learning
    context, the channel consists solely of task instructions within the prompts,
    capable of encompassing a wide range of tasks. For instance, Wang et al., Wang
    et al. ([2022](#bib.bib35)) have converted standard NLP datasets designed for
    optimized channels into instruction-based formats for 76 different tasks. Moreover,
    free-form task instructions allow for nuanced control mechanisms, including explicit
    directives (such as specifying output formats or initiating reasoning processes)
    and subtle cues (such as inducing cognitive behaviors through few-shot examples).
    These aspects will be further explored in Section [4](#S4 "4 Cognitive Behaviors
    Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and
    Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting") and summarized
    in Table [2](#S4.T2 "Table 2 ‣ Mechanism for Feedback Learning ‣ 4.4 Feedback
    Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking
    ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive
    LLMs’ Prompting"). In contrast, since other channels are set during the optimization
    process for specific tasks, they lack the flexibility for user-directed modifications.
    Channels that require adjustments, such as fine-tuning the LLM, adapter tuning,
    or prefix tuning, rely on supervised learning methods for configuration. Although
    prompting in AE-LLMs could, in theory, facilitate task adjustments at inference
    time without prior task-specific fine-tuning—akin to AR-LLMs’ prompting approach—it
    often requires task-specific optimization to achieve effective channel performance.
    For example, techniques like Pattern Exploitation Training (PET) (Schick and Schütze,
    [2021](#bib.bib28)) utilize mathematical optimization to adapt models to specific
    patterns, whereas Auto-prompt (Shin et al., [2020](#bib.bib29)) optimizes text
    patterns for language models. The question of whether this need for optimization
    arises from the inherent complexities of auto-encoding language models invites
    further research.'
  prefs: []
  type: TYPE_NORMAL
- en: 'User-level Transparency: Can Channel Formulation Be Easily Understood by Users?'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The focus here is on the understandability of the channels themselves to lay
    users, rather than their functional effectiveness, as this greatly influences
    the user experience. For example, the objective of an output layer is clear —
    transforming LLM representations into a specific output format. However, the process
    involving dense representations through matrix multiplication is not intuitively
    understandable to the non-specialist. Moreover, text patterns refined through
    AE-LLMs’ Auto-prompting often lack the straightforwardness found in manually created
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'User-level Complexity: Assessing the Number of Conceptual Components'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This analysis evaluates the conceptual load required to deploy $T$.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Evaluating Expressiveness of Modalities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During LLM fine-tuning and adapter tuning, the task-specific output layers
    strictly limit the range of possible outputs, hindering the potential for detailed
    expressiveness and, by extension, advanced cognitive behaviors. The output space
    is tightly defined, with actions or labels being pre-determined and given specific
    meanings through task-specific supervised learning. Nonetheless, certain probing
    techniques allow us to uncover the thought processes behind their predictions,
    a topic we will explore further in Section [4.1](#S4.SS1 "4.1 Thinking, Fast And
    Slow ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s
    Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting").
    When it comes to AE-LLMs prompted with text patterns, these models are limited
    to generating only specific tokens or words, constrained by the patterns set in
    advance. These constraints, such as token positions and quantities dictated by
    the input patterns, along with the need for grammatical and coherent text completion,
    restrict the models’ ability to articulate complex ideas, plans, and actions.
    On the other hand, AR-LLMs’ prompting capitalizes on their auto-regressive nature
    to produce unbounded, free-form text, influenced solely by the given input context.
    This capability is further demonstrated in Section [4](#S4 "4 Cognitive Behaviors
    Under AR-LLMs’ Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and
    Cognitive Behaviors Enabled by Auto-regressive LLMs’ Prompting") and summarized
    in Table [2](#S4.T2 "Table 2 ‣ Mechanism for Feedback Learning ‣ 4.4 Feedback
    Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm ‣ Rethinking
    ChatGPT’s Success: Usability and Cognitive Behaviors Enabled by Auto-regressive
    LLMs’ Prompting"), showcasing the open-ended expressiveness unique to the AR-LLM
    prompting paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Cognitive Behaviors Under AR-LLMs’ Prompting Paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section elucidates the capability of AR-LLM prompting paradigms to exhibit
    cognitive behaviors expressed by the free-form modalities by mainpulating the
    free-form channels. It’s important to clarify that not every AR-LLM demonstrates
    cognitive behaviors—smaller models like GPT-2 (Radford et al., [2019](#bib.bib25))
    may not. Specifically, we analyze four cognitive behaviors: thinking, reasoning,
    planning, and feedback learning, leaving the examination of their interrelationships
    for future research.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Thinking, Fast And Slow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the core of cognitive behavior lies thinking. The Kahneman’s framework (Kahneman,
    [2011](#bib.bib12)) divides thinking into two distinct systems: the fast system
    operates through intuitive shortcuts for quick navigation of daily situations
    without extensive analysis. Conversely, the slow system, or System 2, involves
    conscious, detailed and methodical examination of information, necessitating logical
    deliberation to arrive at decisions and address challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: Fast Thinking via Task-specific Channels
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using channels trained through task-specific supervised learning can achieve
    performances that rival or exceed human performance. Nonetheless, they often struggle
    with generalizing to data from natural domain shifts, adversarial perturbations
    and debiased data, as summarized by Li et al., Li et al. ([2023](#bib.bib17)).
    This limitation is consistently attributed to shortcut learning, such as classifying
    sentences containing the word “No” as “contradiction” in text entailment tasks
    (Wallace et al., [2019](#bib.bib33); Du et al., [2021](#bib.bib5)). The intriguing
    question arises whether task-specific channels can also develop System 2 — the
    fast system. While the limited expressiveness of task-specific outputs does not
    offer straightforward evidence, Li et al., Li and Liu ([2023](#bib.bib16)) employ
    a technical probe (Sundararajan et al., [2017](#bib.bib32)) to reveal that indulgence
    in shortcut learning during task-specific training impedes the development of
    the slow system. While the mentioned research primarily examines the LLM fine-tuning
    paradigm, it’s our contention that shortcut learning and the fast thinking are
    likely prevalent across all the parametric channels, including prefixes and adapters,
    trained on supervised datasets to some degree. This is attributed to the inherent
    characteristics of gradient descent optimization, as demonstrated by empirical
    findings in Li et al., Li and Liu ([2023](#bib.bib16)). Another empirical evidence
    shows that methods like prefix and adapter tuning, although more resilient, still
    notably falter under distribution shifts and adversarial attacks (Han et al.,
    [2021](#bib.bib6); Yang and Liu, [2022](#bib.bib41)). The mitigated impact observed
    in prefix and adapter tuning is attributed to the fact that the underlying LLMs
    are not directly engaged as task-specific channels, as explored by (Han et al.,
    [2021](#bib.bib6)). While we draw parallels between reliance on shortcuts and
    fast thinking within human cognition, some research within the NLP field argues
    that such dependency on shortcuts (dataset biases) detracts from the models’ relevance
    to human-level cognition (Zhong et al., [2023](#bib.bib46)). This perspective
    arises from the view that the shortcuts might not reflect genuine human cognitive
    activities within the field of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Minimal Fast Thinking Evident with AR-LLMs Prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Research findings (Si et al., [2023](#bib.bib31); Zhang et al., [2022](#bib.bib44))
    consistently indicate the difficulty of inducing fast thinking in AR-LLMs through
    prompting techniques. These models typically remain unfazed by various distributional
    shifts, such as domain shift and adversarial perturbations. Min et al., Min et
    al. ([2022](#bib.bib21)) demonstrate that, even with few-shot demonstrations for
    in-context learning, the models tend to leverage the structure of these demonstrations
    to organize the generation rather than relying on simplistic input-to-label mappings
    for predictions. Additionally, Raman et al., Raman et al. ([2023](#bib.bib26))
    show that PET prompting improve the AE-LLMs’ ability to withstand adversarial
    attacks. Nonetheless, this enhanced robustness is somewhat restricted. The constrained
    effectiveness could be attributed to the dependency on task-specific channels
    inherent during the deployment of the PET prompting.
  prefs: []
  type: TYPE_NORMAL
- en: Slow Thinking in Prompting Paradigms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The remainder of this section will illustrate the capacity of AR-LLMs’ prompting
    to replicate the human slow thinking process through the exhibition of effortful
    mental activities, as encapsulated in Table [2](#S4.T2 "Table 2 ‣ Mechanism for
    Feedback Learning ‣ 4.4 Feedback Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’
    Prompting Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors
    Enabled by Auto-regressive LLMs’ Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Reasoning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reasoning is a thinking process to conclusions or decisions with the sequential
    and interconnected nature, i.e., chain-of-thoughts (CoTs) (Wei et al., [2022b](#bib.bib40)).
    This is the most common definition in the NLP/LLM are to investigate the LLMs’
    reasoning ability. With a reasoning path in free-form modality, models can better
    solve complicated tasks requiring multi-step reasoning compared to the conclusion
    without CoTs. As an illustration, Wei et al., Wei et al. ([2022b](#bib.bib40))
    substantially boosts model efficacy in solving mathematical reasoning bechmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning is defined as the process of arriving at conclusions or decisions
    through a sequential and interconnected series of thoughts, often referred to
    as a chain-of-thoughts (CoTs) (Wei et al., [2022b](#bib.bib40)). This definition
    is widely accepted in the field of Natural Language Processing (NLP) for exploring
    the reasoning capabilities of LLMs. By employing a reasoning path via the modality
    of free-form text, models are more adept at tackling complex tasks that necessitate
    multi-step reasoning, as opposed to reaching conclusions without the aid of CoTs.
    Technically, the auto-regressive nature employs the thoughts or intermediate steps
    generated as the prior for generating subsequent thoughts and, ultimately, the
    final predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Context for Eliciting Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two primary contexts are employed to facilitate the creation of intermediate
    reasoning steps: incorporating a Chain of Thought (CoT) triggers in task instructions
    (zero-shot CoTs), such as “Let’s think step-by-step” (Kojima et al., [2022](#bib.bib13)),
    within prompts, or integrating manually crafted reasoning steps in a few-shot
    learning context (few-shot CoTs) (Wei et al., [2022b](#bib.bib40)). To circumvent
    the manual compilation of few-shot demonstrations with reasoning sequences, Zhang
    et al., Zhang et al. ([2023](#bib.bib45)) developed a method to automatically
    generate few-shot demonstrations by choosing several queries and utilizing zero-shot
    CoTs to craft reasoning sequences for each query (Auto CoTs). Given that simple
    greedy decoding (producing a single chain) is prone to error accumulation in intermediate
    steps, Wang et al., Wang et al. ([2023b](#bib.bib37)) propose generating multiple
    chains and consolidating them through majority voting, thereby enhancing model
    accuracy in both scenarios (CoTs-SC).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Planning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Planning involves the forethought and organization of actions or steps to achieve
    a predetermined objective. This process fundamentally requires a comprehension
    or representation of the environment and involves breaking down tasks into smaller,
    manageable subgoals. It represents a key cognitive behavior modeled within the
    fields of AI. Typical planning methods break down tasks into subgoals through
    explicit symbolic representation (Russell and Norvig, [2010](#bib.bib27)). For
    instance, partial-order planning ensures the logical sequencing of actions by
    modeling actions, preconditions, effects, and the relations among actions in such
    a way that actions are logically sequenced to meet the goal’s preconditions. Differing
    from traditional approaches that rely on explicitly modeled knowledge and reasoning
    mechanisms, LLMs leverage their inherent knowledge and inferential capabilities
    to mimic planning. They do this by producing text sequences that suggest a logical
    progression of steps or actions directed towards an objective (Hao et al., [2023](#bib.bib7);
    Wang et al., [2023a](#bib.bib36); Huang et al., [2022](#bib.bib9)). This skill
    stems from the models’ proficiency in forecasting the subsequent most likely word
    sequence based on a context indicative of planning or reasoning processes.
  prefs: []
  type: TYPE_NORMAL
- en: Context to Elicit Plans
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the activation of reasoning processes, the process of planning can
    be prompted through the inclusion of specific planning cues in zero-shot scenarios,
    such as the prompt “let’s carry out the plan” (Wang et al., [2023a](#bib.bib36)),
    or through the demonstration of planning steps in few-shot examples (Huang et
    al., [2022](#bib.bib9)). Experimental findings indicate that instructions tailored
    to tasks significantly enhance the performance of LLMs on various tasks. For instance,
    directives like “pay attention to calculation” (Hao et al., [2023](#bib.bib7))
    or “identify key variables and their corresponding figures to formulate a plan”
    (Wang et al., [2023a](#bib.bib36)) have been shown to improve outcomes in tasks
    requiring numerical reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Planning for Sequential Decision-making
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This ability is essential for addressing problems requiring a series of decisions,
    especially when deploying LLMs in open-world scenarios like robotics. In such
    environments, tasks typically need physical actions (grounded), involve translating
    broad objectives into actionable steps (high-level), and present a vast range
    of possible actions (open-ended). Research has demonstrated the effectiveness
    of LLMs in deconstructing complex goals into actionable sequences within such
    dynamic environments, as seen in projects like ALFWorld (Yao et al., [2023b](#bib.bib43)),
    VirtualHome (Huang et al., [2022](#bib.bib9)), and Minecraft (Wang et al., [2023c](#bib.bib38)).
    An example from ALFWorld illustrates this: achieving the objective of “examining
    paper under desklamp” necessitates LLMs to devise practical plans (e.g., initially
    approaching the coffee table, then acquiring the paper and utilizing the desklamp)
    and subsequently generate textual instructions for execution in real-world settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Feedback Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As Kahneman et al., Kahneman ([2011](#bib.bib12)) elucidates, although System
    1 may rush to judgments that are biased or erroneous, System 2 has the capacity
    to identify and rectify these mistakes through introspection on the rapid decisions
    made by System 1\. Similarly, LLMs have shown the ability to mimic this aspect
    of human cognition.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback Generation and Contextual Basis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LLMs are adept at generating feedback by reflecting on their previously given
    responses or observations from interactions with the external environment. There
    are two primary scenarios for feedback generation: 1) Feedback based on previous
    actions and external feedback: In the work by Yao et al., Yao et al. ([2023b](#bib.bib43)),
    LLMs engaging with a Wikipedia API to search for entities that do not exist, such
    as “Search[goddess frigg]”, may encounter a 404 error, delivered in JSON format.
    In response, LLMs can articulate feedback about the error related to their action,
    such as stating, “Could not find goddess frigg.”. 2) Feedback based solely on
    prior responses: This approach is relevant in various situations where external
    environmental feedback is absent (Shinn et al., [2023](#bib.bib30); Hao et al.,
    [2023](#bib.bib7)). In such cases, LLMs can give feedback on previous answers
    by applying certain evaluation metrics, such as determining the relevance of a
    sub-question to a broader question requiring intricate, multi-step reasoning (Hao
    et al., [2023](#bib.bib7)). This process involves using a prompt that asks the
    LLM to judge the utility of a sub-question in addressing the main question (Prompt:
    “Given a question, assess if the subquestion aids in solving the original question.
    Answer ’Yes’ or ’No’. Question: {goal}; Subquestion: {action}. Is the subquestion
    useful?”). Furthermore, feedback may be presented as numerical scores, such as
    the confidence scores (normalized logits) for ’Yes’ or ’No’ answers, instead of
    in verbal form. The decision to use numerical rather than verbal feedback is contingent
    on the specific requirements of the feedback mechanism, as explored in subsequent
    discussions.'
  prefs: []
  type: TYPE_NORMAL
- en: Mechanism for Feedback Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After generating feedback, Shinn et al., Shinn et al. ([2023](#bib.bib30)) directly
    include verbal feedback LLMs have produced to enhance the accuracy of their responses
    or decisions. Meanwhile, Hao et al., Hao et al. ([2023](#bib.bib7)) detail a methodology
    where numerical feedback serves as a reward system for guiding LLMs for action
    selection. Following this, the LLMs function as world models to predict the subsequent
    state of state-action pairs during the planning phase, utilizing Monte-Carlo Tree
    Search (MCTS) to achieve this aim. Instead of allowing LLMs to directly process
    the feedback, an implicit feedback learning strategy is employed where a feedback
    loop is deliberately established to influence the sequence of actions undertaken
    by the LLMs via the update of state values during the propagation phase of MCTS.
  prefs: []
  type: TYPE_NORMAL
- en: '| Behaviors | Context | Relevant Works |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reasoning | CoT triggers, e.g., “Let’s think step by step.” | Zero-shot CoTs
    (Kojima et al., [2022](#bib.bib13)), Auto-CoTs (Zhang et al., [2023](#bib.bib45))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot demos with CoTs | Few-shot CoTs (Wei et al., [2022b](#bib.bib40)),
    CoTs-SC (Wang et al., [2023b](#bib.bib37)),'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-prompt (Zhang et al., [2023](#bib.bib45)),
  prefs: []
  type: TYPE_NORMAL
- en: ToT (Yao et al., [2023a](#bib.bib42)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| Planning | Zero-shot instruction | Wang et al., Wang et al. ([2023a](#bib.bib36))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot demos with planning steps | Huang et al., Huang et al. ([2022](#bib.bib9))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Feedback Learning | Observations from external environments | Reflexion (Shinn
    et al., [2023](#bib.bib30)) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Previous answers/decisions | Self-refine (Madaan et al., [2023](#bib.bib20)),
    Reflexion (Shinn et al., [2023](#bib.bib30)),'
  prefs: []
  type: TYPE_NORMAL
- en: RAP (Hao et al., [2023](#bib.bib7)) |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Cognitive behaviors enabled by free-form context. For the “Feedback
    Learning” sections, we illustrate the contexts utilized to produce feedback. It’s
    worth noting that the methods for feedback adaptation might not always employ
    free-form context; for instance, they may involve advanced search techniques as
    outlined in our study. The final column presents examples of tasks for demonstration
    purposes, though the list is not comprehensive.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Bridging LLM Deployment Gaps with Insights from Cognitive Behaviors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section investigates how insights into cognitive behaviors can aid in addressing
    the tuning and deployment challenges faced by LLMs operating as autonomous agents
    and within multi-agent systems.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous Cognitive Behaviors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Instead of relying on explicit contextual cues to trigger advanced cognitive
    functions, an intelligent system is expected to independently engage in reasoning,
    planning, and decision-making as it interacts with the external world—for instance,
    by seeking input from humans or utilizing available tools. To foster such autonomous
    behaviors, various algorithms aim to tune LLMs for independently exhibiting behaviors
    that align with human cognitive processes. For instance, Liu et al., Liu et al.
    ([2023](#bib.bib19)) have developed techniques for instruction tuning that facilitates
    autonomous reasoning. Yet, the challenge remains in creating instructional data
    that encapsulates higher-order cognitive functions. A pivotal question emerges:
    *How can various cognitive behaviors be encapsulated within free-form text (instruction
    data)?* Addressing this question is crucial for ensuring that the data used for
    tuning mirrors human cognitive processes, thereby making the resulting model actions
    more human-like. Unraveling this issue might necessitate insights from both cognitive
    psychology and linguistics. Another approach to tuning involves the use of reliable
    reward models, such as reinforcement learning from human feedback (RLHF) (Ouyang
    et al., [2022](#bib.bib23)) and behavior cloning (Nakano et al., [2021](#bib.bib22)).
    Many studies (Ouyang et al., [2022](#bib.bib23); Nakano et al., [2021](#bib.bib22))
    develop reward models based on comparisons of model-generated responses, with
    human evaluators ranking these responses. An unresolved inquiry remains: *How
    can reward models be devised to truly reflect human cognitive preferences?*'
  prefs: []
  type: TYPE_NORMAL
- en: Navigating Free-form Contexts in Multi-turn Interactions Within Multi-agent
    Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In exploring planning and feedback learning strategies, it becomes evident
    that multi-agent systems are designed to facilitate interactions between LLMs
    and the external world, as well as among LLMs themselves. Examples include LLMs
    acting as both evaluators and actors in feedback learning environments (Shinn
    et al., [2023](#bib.bib30)) or taking on roles as evaluators, actors, and environmental
    simulators in planning scenarios (Hao et al., [2023](#bib.bib7)). Such setups
    necessitate more than a one-time interaction between LLMs and users, requiring
    instead sustained, multi-turn dialogues. Challenges emerge within these complex
    interactions: *How do agents process and integrate information from other agents
    and their own previous dialogues? How can environmental data be stringified into
    a format understandable by LLMs? What strategies can simplify the management of
    extended sequences of interaction trajectories?*'
  prefs: []
  type: TYPE_NORMAL
- en: Advancing Towards A Unified Inference Framework for Multi-agent Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Despite the recent development of various LLM-based multi-agent systems, there
    remains an absence of a unified framework across these models. Exploring such
    framework serves as a key drive for this research on a cognitive framework. Organizing
    LLM-based agents by their cognitive behaviors offers a pathway to this unification.
    For instance, as indicated in Table [2](#S4.T2 "Table 2 ‣ Mechanism for Feedback
    Learning ‣ 4.4 Feedback Learning ‣ 4 Cognitive Behaviors Under AR-LLMs’ Prompting
    Paradigm ‣ Rethinking ChatGPT’s Success: Usability and Cognitive Behaviors Enabled
    by Auto-regressive LLMs’ Prompting"), the ReAct framework (Yao et al., [2023b](#bib.bib43))
    employs a reasoning agent for decision-making, whereas the RAP framework (Hao
    et al., [2023](#bib.bib7)) utilizes a reasoning agent for decision-making alongside
    a feedback learning agent for evaluation. Delving deeper into the relationships
    between cognitive behaviors might benefit from insights in cognitive psychology.
    Taking the concept of Self-Regulated Learning (SRL) as defined by Zimmerman ([2000](#bib.bib47)),
    planning and feedback learning are intertwined, enhancing the learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: “Self-regulated learning refers to self-generated thoughts, feelings, and actions
    that are planned and cyclically adapted to the attainment of personal goals”
  prefs: []
  type: TYPE_NORMAL
- en: 'This rationale lays the groundwork for future efforts to combine planning frameworks
    (like RAP) with feedback learning frameworks (such as Reflexion). Furthermore,
    the structure of certain inference models showcases their interconnections and
    the presence of common detailed components. For instance, both feedback learning
    frameworks, exemplified by Reflexion Shinn et al. ([2023](#bib.bib30)), and planning
    frameworks, such as RAP Hao et al. ([2023](#bib.bib7)), incorporate a common detailed
    module: a feedback learning agent. This agent plays a pivotal role in decision-making
    by facilitating the selection of appropriate actions from an assortment of tools
    and environments. It is evident that the interconnection within the intricate
    inference framework can be examined, especially at the level of cognitive behavior.
    Our comprehensive analysis aims to pave the way for the systematic creation of
    multi-agent LLM-based frameworks, or inversely, to stimulate research in cognitive
    psychology.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In summary, our analysis seeks to inspire further research in AI, within the
    domain of language intelligence and beyond, to move away from heavily optimized
    task-specific channels. Instead, we advocate for the adoption of natural and free-form
    modalities throughout the pretraining phase via self-supervised learning, followed
    by straightforward inference-time deployment that eschews the necessity for mathematically
    optimizing task-specific channels. We developed an analytical framework to examine
    the deployment of LLMs to reach the conclusion. Besides, the auto-regressive nature
    of free-form modalities, leveraged during pretraining, enhances the capacity for
    exhibiting a range of human-like cognitive behaviors by utilizing the free-form
    channel. It is important to clarify that we do not advocate that LLMs possess
    conscious thought. Rather, our findings illustrate how LLMs, such as ChatGPT,
    can imitate the outcomes of human cognitive activities via the free-form modality
    given suitable verbal context. Lastly, we highlight the opportunity to address
    challenges in LLM deployment through the integration of cognitive behavior concepts.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bartneck et al. [2020] Christoph Bartneck, Tony Belpaeme, Friederike Eyssel,
    Takayuki Kanda, Merel Keijsers, and Selma Šabanović. Human-robot interaction:
    An introduction. Cambridge University Press, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. [2003] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian
    Janvin. A neural probabilistic language model. JMLR, 3:1137–1155, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In Advances in NIPS, volume 33,
    pages 1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In NAACL-HLT, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. [2021] Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande,
    Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu. Towards interpreting and
    mitigating shortcut learning behavior of NLU models. In NAACL-HLT, pages 915–929,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. [2021] Wenjuan Han, Bo Pang, and Ying Nian Wu. Robust transfer learning
    with pretrained language models through adapters. In ACL-IJCNLP, pages 854–861,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy
    Wang, and Zhiting Hu. Reasoning with language model is planning with world model.
    In EMNLP, pages 8154–8173, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In ICML, pages 2790–2799,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    Language models as zero-shot planners: Extracting actionable knowledge for embodied
    agents. In ICML, pages 9118–9147, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. [2020] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.
    Is bert really robust? a strong baseline for natural language attack on text classification
    and entailment. In AAAI, volume 34, pages 8018–8025, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. [2020] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke
    Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and
    predicting spans. TACL, 8:64–77, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kahneman [2011] D. Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux,
    2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In
    Advances in NIPS, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. [2020] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. In ACL, pages 7871–7880, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and
    Roberto Navigli, editors, ACL-IJCNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Liu [2023] Xinzhe Li and Ming Liu. Make text unlearnable: Exploiting
    effective patterns to protect personal data. In TrustNLP, pages 249–259, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023] Xinzhe Li, Ming Liu, Shang Gao, and Wray Buntine. A survey
    on out-of-distribution evaluation of neural nlp models. In IJCAI-23, 7 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji
    Zhou, and Yue Zhang. Logicot: Logical chain-of-thought instruction tuning. In
    EMNLP, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. [2023] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck,
    Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback.
    In Advances in NIPS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? In EMNLP, pages 11048–11064, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakano et al. [2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. Webgpt: Browser-assisted question-answering with human
    feedback. arXiv preprint arXiv:2112.09332, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training
    language models to follow instructions with human feedback. In Advances in NIPS,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raman et al. [2023] Mrigank Raman, Pratyush Maini, J Kolter, Zachary Lipton,
    and Danish Pruthi. Model-tuning via prompts makes NLP models adversarially robust.
    In EMNLP, pages 9266–9286, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Russell and Norvig [2010] Stuart J Russell and Peter Norvig. Artificial intelligence
    a modern approach. London, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schick and Schütze [2021] Timo Schick and Hinrich Schütze. Exploiting cloze-questions
    for few-shot text classification and natural language inference. In EACL, pages
    255–269, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. [2020] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace,
    and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically
    Generated Prompts. In EMNLP, pages 4222–4235, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement
    learning. In Advances in NIPS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Si et al. [2023] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng
    Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable.
    In ICLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sundararajan et al. [2017] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic
    attribution for deep networks. In ICML, pages 3319–3328, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wallace et al. [2019] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,
    and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp.
    In EMNLP-IJCNLP, pages 2153–2162, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2019] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform
    for natural language understanding. In ICLR. OpenReview.net, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh
    Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran,
    Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai,
    Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar
    Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney,
    Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,
    Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay
    Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative
    instructions on 1600+ NLP tasks. In EMNLP, pages 5085–5109, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models. In ACL, pages 2609–2634,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023b] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. In ICLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023c] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning
    with LLMs enables open-world multi-task agents. In Advances in NIPS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022a] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. Emergent abilities of large language models. TMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting
    elicits reasoning in large language models. In Advances in NIPS, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Liu [2022] Zonghan Yang and Yang Liu. On robust prefix-tuning for text
    classification. In ICLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. In Advances in NIPS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. [2023b] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in
    language models. In ICLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2022] Hongxin Zhang, Yanzhe Zhang, Ruiyi Zhang, and Diyi Yang.
    Robustness of demonstration-based learning under limited data scenario. In EMNLP,
    pages 1769–1782, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic
    chain of thought prompting in large language models. In ICLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. [2023] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric
    benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zimmerman [2000] Barry J Zimmerman. Attaining self-regulation: A social cognitive
    perspective. In Handbook of self-regulation, pages 13–39\. 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
