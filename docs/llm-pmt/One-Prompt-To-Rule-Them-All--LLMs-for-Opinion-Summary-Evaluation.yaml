- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:40'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11683](https://ar5iv.labs.arxiv.org/html/2402.11683)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: First Author
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 1
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 2
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 3
  prefs: []
  type: TYPE_NORMAL
- en: email@domain
  prefs: []
  type: TYPE_NORMAL
- en: '&Second Author'
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 1
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 2
  prefs: []
  type: TYPE_NORMAL
- en: Affiliation / Address line 3
  prefs: []
  type: TYPE_NORMAL
- en: email@domain    Tejpalsingh Siledar^∗^♣, Swaroop Nath^∗^♣, Sri Raghava^∗^♣,
    Rupasai Rangaraju^∗^♣,
  prefs: []
  type: TYPE_NORMAL
- en: Swaprava Nath^♣, Pushpak Bhattacharyya^♣,
  prefs: []
  type: TYPE_NORMAL
- en: Suman Banerjee^♠, Amey Patil^♠, Sudhanshu Shekhar Singh^♠,
  prefs: []
  type: TYPE_NORMAL
- en: Muthusamy Chelliah^♠, Nikesh Garera^♠
  prefs: []
  type: TYPE_NORMAL
- en: ^♣Computer Science and Engineering, IIT Bombay, India,
  prefs: []
  type: TYPE_NORMAL
- en: ^♠Flipkart, India
  prefs: []
  type: TYPE_NORMAL
- en: '{tejpalsingh, swaroopnath, sriraghava, rupasai, swaprava, pb}@cse.iitb.ac.in'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evaluation of opinion summaries using conventional reference-based metrics rarely
    provides a holistic evaluation and has been shown to have a relatively low correlation
    with human judgments. Recent studies suggest using Large Language Models (LLMs)
    as reference-free metrics for NLG evaluation, however, they remain unexplored
    for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets
    inhibit progress. To address this, we release the SummEval-Op dataset covering
    $7$ with humans, outperforming all previous approaches. To the best of our knowledge,
    we are the first to investigate LLMs as evaluators on both closed-source and open-source
    models in the opinion summarization domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: Tejpalsingh Siledar^∗^♣, Swaroop Nath^∗^♣, Sri Raghava^∗^♣, Rupasai Rangaraju^∗^♣,
    Swaprava Nath^♣, Pushpak Bhattacharyya^♣, Suman Banerjee^♠, Amey Patil^♠, Sudhanshu
    Shekhar Singh^♠, Muthusamy Chelliah^♠, Nikesh Garera^♠ ^♣Computer Science and
    Engineering, IIT Bombay, India, ^♠Flipkart, India {tejpalsingh, swaroopnath, sriraghava,
    rupasai, swaprava, pb}@cse.iitb.ac.in
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: Equal contribution'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/946a269c9efd0257e08b37c71c3148db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: G-Eval vs. Op-I-Prompt. On closed-source model (ChatGPT-$3.5$ dimensions:
    fluency (FA), coherence (CO), relevance (RE), faithfulness (FA), aspect coverage (AC),
    sentiment consistency (SC), and specificity (SP). Check Figure [4](#S5.F4 "Figure
    4 ‣ 5.1 Summarization Models ‣ 5 Experiments ‣ One Prompt To Rule Them All: LLMs
    for Opinion Summary Evaluation") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Opinion summarization systems predominantly use traditional metrics such as
    Rouge (Lin, [2004](#bib.bib20)) and BertScore (Zhang et al., [2019](#bib.bib35))
    for automatic evaluation, however, they have been shown to have poor correlations
    with human judgments (Shen and Wan, [2023](#bib.bib26)). Moreover, these metrics
    fall short of comprehensively evaluating opinion summaries. Additionally, obtaining
    reference-based datasets at a large scale is an expensive process.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Large Language Models (LLMs) have been utilized as reference-free
    evaluators for Natural Language Generation (NLG) outputs (Fu et al., [2023](#bib.bib10);
    Chiang and Lee, [2023a](#bib.bib5), [b](#bib.bib6); Wang et al., [2023](#bib.bib30);
    Liu et al., [2023](#bib.bib21)). The idea is to prompt a powerful LLM such as
    ChatGPT-$3.5$) primarily because of the limitations of the open-source models
    in following instructions and producing the desired output (Chiang and Lee, [2023b](#bib.bib6)).
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we first create SummEval-OP, a reference-free opinion summarization
    dataset covering $7$ dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SummEval-Op^*^**[https://github.com/tjsiledar/SummEval-OP](https://github.com/tjsiledar/SummEval-OP),
    an opinion summarization benchmark dataset, consisting of a total of $2,912$ dimensions-
    fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency,
    and specificity related to the evaluation of opinion summaries (Section [4](#S4
    "4 SummEval-Op Benchmark Dataset ‣ One Prompt To Rule Them All: LLMs for Opinion
    Summary Evaluation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Op-I-Prompt, a dimension-independent prompt and Op-Prompts, a dimension-dependent
    set of prompts, enabling opinion summary evaluation for all the $7$ on average
    in correlation with human judgments (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation"), Section
    [3](#S3 "3 Methodology ‣ One Prompt To Rule Them All: LLMs for Opinion Summary
    Evaluation")). To the best of our knowledge we are the first to test the applicability
    of different prompt approaches on open-source LLMs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Benchmarking of recent LLMs (closed and open-source) on the aforementioned
    $7$ dimensions for the task of opinion summarization, which to the best of our
    knowledge is first of its kind (Section [6](#S6 "6 Results and Analysis ‣ One
    Prompt To Rule Them All: LLMs for Opinion Summary Evaluation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Detailed analysis, comparing an open-source LLM against a closed-source LLM
    acting as evaluators for automatic evaluation of opinion summaries on $7$ dimensions.
    Analysis indicates that Op-I-Prompt emerges as a good alternative for evaluating
    opinion summaries showing a high correlation with humans when compared with alternatives
    (Section [6](#S6 "6 Results and Analysis ‣ One Prompt To Rule Them All: LLMs for
    Opinion Summary Evaluation")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLM-based Evaluators  Fu et al. ([2023](#bib.bib10)) introduced GPTScore that
    operates on the premise that a generative pre-training model (e.g. GPT-3) is likely
    to assign a higher probability to the generation of high-quality text in line
    with provided instructions and context. Chiang and Lee ([2023a](#bib.bib5)) were
    the first to explore LLMs for evaluation. Chiang and Lee ([2023b](#bib.bib6))
    provide concrete guidelines that improve ChatGPT’s correlation with humans. Wang
    et al. ([2023](#bib.bib30)) conducted an initial survey exploring the utilization
    of ChatGPT as an NLG evaluator. Kocmi and Federmann ([2023](#bib.bib16)) used
    GPT models for evaluating machine learning tasks. Liu et al. ([2023](#bib.bib21))
    introduced G-Eval, a framework for evaluation of NLG outputs using Chain of Thought
    (CoT) (Wei et al., [2023](#bib.bib32)) and assigning weights to a predetermined
    set of integer scores based on their generation probabilities from GPT-3/4\. Chen
    et al. ([2023](#bib.bib4)) were the first to investigate approaches to reference-free
    NLG evaluation using LLMs, finding that an explicit score generated by ChatGPT
    is the most effective and stable approach. Zheng et al. ([2023](#bib.bib36)) show
    that strong LLMs such as GPT-4 achieve a similar level of agreement to that of
    humans and hence can be used to approximate human preferences. Our work investigates
    two prompt strategies and tests the applicability of different prompt approaches
    on closed-source and open-source LLMs for opinion summary evaluation for $7$ dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Opinion Summary Evaluation Benchmark Shen and Wan ([2023](#bib.bib26)) created
    the OpinSummEval dataset, utilizing the Yelp test set (Chu and Liu, [2019](#bib.bib8)),
    annotating for $4$ dimensions on the recent LLM summaries, subsequently establishing
    benchmarks for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/61a6ba7b173e4df8d0dfd719659d13d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparison of Prompt Approaches. G-Eval Prompts first generates the
    Evaluation Steps using Task Description and Evaluation Criteria in Chain-of-Thought
    fashion. Finally the full prompt is used to evaluate the opinion summaries. In
    contrast, our Op-I-Prompt is simpler and has Task Description, Evaluation Criteria,
    and Evaluation Steps fixed for a dimension/metric independent evaluation. Here,
    only the Metric part needs to be changed for evaluating any dimension/metric.
    Finally Op-Prompts are dimension/metric dependent prompts that needs to be specifically
    crafted for each dimension/metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We describe our dimension independent and dependent prompts and the model scoring
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Prompt Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2 Related Work ‣ One Prompt To Rule Them All:
    LLMs for Opinion Summary Evaluation") shows the different prompt approaches for
    evaluating opinion summaries. In general, the prompts include the following $3$
    components-'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task Description: Defines the task that the LLM will be performing. In our
    case, the task is to evaluate a summary corresponding to a set of reviews on a
    given metric/dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Criteria: Defines the criteria that will be used to perform the
    task. In our case, the task being opinion summary evaluation, the criteria is
    to assign a score ($1-5$) for a certain metric/dimension depending on the extent
    to which the summary adheres to it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Steps: This comprises the steps that the LLM must take to correctly
    perform the described task. In our case, it contains the steps that the LLM should
    follow to evaluate a certain metric/dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose two prompt approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Op-I-Prompt  is a metric-independent opinion summary evaluation prompt. Here
    we split the Evaluation Criteria to create a new component Metric consisting only
    the evaluation dimension. All the remaining components i.e. Task Description,
    Evaluation Criteria, and Evaluation Steps are crafted in such a way that they
    are applicable in general to any opinion summary evaluation dimension. This benefits
    us in the following way: (a) we have a metric independent prompt that can now
    evaluate any metric/dimension just by replacing with the desired definition of
    the dimension within the Metric block (b) the remaining components, crafted specifically
    keeping the task in mind, ensures that the evaluation by LLM takes place as defined
    by us.'
  prefs: []
  type: TYPE_NORMAL
- en: Op-Prompts is a set of metric-dependent prompts. We specifically handcrafted
    these prompts for each of the $7$ evaluation dimensions. Although this ensures
    that the evaluation happens exactly in the way we define, this requires a certain
    level of expertise in the evaluation domain and prompting. This could be seen
    as a much stricter version of the prompt compared to Op-I-Prompt where the prompt
    is suited to any evaluation dimension which is not the case here. A prompt defined
    for a certain dimension could not be utilized for any other dimension.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, G-Eval (Liu et al., [2023](#bib.bib21)) used auto chain-of-thoughts
    Wei et al. ([2022](#bib.bib31)) by using Task Description and Evaluation Criteria
    to automatically generate the Evaluation Steps. Finally, all the components together
    constitute the G-Eval prompt that is used by an LLM to evaluate summaries. Our
    work investigates the applicability of all these prompts to both closed-source
    and open-source models for evaluating opinion summaries.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Scoring Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Liu et al. ([2023](#bib.bib21)) pointed out the limitation of LLM outputting
    an integer score and proposed using a weighted average of the scores as the LLMs
    output, where the weights are the probabilities of the corresponding score. Formally,
    say, the scoring is scheme is from $\{s_{1},...,s_{j}\}$ is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle o=\sum_{k=1}^{j}p(s_{k})\times s_{k}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: $p(s_{k})$) to get a reliable estimate of the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 4 SummEval-Op Benchmark Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created the SummEval-Op benchmark dataset for evaluating the opinion summaries
    on $7$ dimensions. In this section, we discuss the dataset used, opinion summary
    evaluation metrics, annotation details, and its analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We utilized the Amazon test set (He and McAuley, [2016](#bib.bib12); Bražinskas
    et al., [2020](#bib.bib3)), comprising of reviews from $4$ for each product. We
    do not directly consider only one of the human summaries as this would bias the
    summaries to a single person.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Opinion Summarization Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The evaluation of opinion summaries focused on the following $7$ dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: fluency (FL)- The quality of summary in terms of grammar, spelling, punctuation,
    capitalization, word choice, and sentence structure and should contain no errors.
    The summary should be easy to read, follow, comprehend and should contain no errors.
    Annotators received specific guidelines on how to penalize summaries based on
    fluency levels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: coherence (CO)- The collective quality of all sentences. The summary should
    be well-structured and well-organized. The summary should not just be a heap of
    related information, but should build from sentence to a coherent body of information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: relevance (RE)- The summary should not contain opinions that are either not
    consensus or important. The summary should include only important opinions from
    the reviews. Annotators were instructed to penalize summaries if they contained
    redundancies and excess/unimportant information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: faithfulness (FA)- Every piece of information mentioned in the summary should
    be verifiable/supported/inferred from the reviews only. Summaries should be penalized
    if any piece of information is not verifiable/supported/inferred from the reviews
    or if the summary overgeneralizes something.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: aspect coverage (AC)- The summary should cover all the aspects that are majorly
    being discussed in the reviews. Summaries should be penalized if they miss out
    on an aspect that was majorly being discussed in the reviews and awarded if it
    covers all.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: sentiment consistency (SC)- All the aspects being discussed in the summary should
    accurately reflect the consensus sentiment of the corresponding aspects from the
    reviews. Summaries should be penalized if they do not cover accurately the sentiment
    regarding any aspect within the summary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: specificity (SP)- The summary should avoid containing generic opinions. All
    the opinions within the summary should contain detailed and specific information
    about the consensus opinions. Summaries should be penalized for missing out details
    and should be awarded if they are specific.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Annotation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For creating the SummEval-Op dataset, annotations were collected for a total
    of $13$ (# of summaries) x $7$ ratings.
  prefs: []
  type: TYPE_NORMAL
- en: 'We chose to hire $3$. We asked the raters to be critical and discuss the ratings
    during re-evaluation. Check Appendix [B](#A2 "Appendix B Rater Agreement ‣ One
    Prompt To Rule Them All: LLMs for Opinion Summary Evaluation")'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Annotation Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | Round-I $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| fluency | 0.55 | 0.84 |'
  prefs: []
  type: TYPE_TB
- en: '| coherence | 0.43 | 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| relevance | 0.50 | 0.79 |'
  prefs: []
  type: TYPE_TB
- en: '| faithfulness | 0.63 | 0.86 |'
  prefs: []
  type: TYPE_TB
- en: '| aspect coverage | 0.64 | 0.82 |'
  prefs: []
  type: TYPE_TB
- en: '| sentiment consistency | 0.41 | 0.78 |'
  prefs: []
  type: TYPE_TB
- en: '| specificity | 0.34 | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '| AVG | 0.50 | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Krippendorff’s alpha coefficient (${\alpha}$ dimensions. As expected,
    we see an improvement in Round-II coefficient scores.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afe574687dad8b6ba7a097dec55dbc95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Ratings Distribution. We plot the average frequency of scores obtained
    by human raters across $7$ is mostly preferred.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluated the inter-rater agreement for the $3$). We report the dimension-wise
    agreement scores for both rounds in Table [1](#S4.T1 "Table 1 ‣ 4.4 Annotation
    Analysis ‣ 4 SummEval-Op Benchmark Dataset ‣ One Prompt To Rule Them All: LLMs
    for Opinion Summary Evaluation"). We observe that for both Round-I and Round-II,
    faithfulness and aspect coverage score higher than others. This is mostly because
    faithfulness and aspect coverage could be identified by cross-examining with the
    reviews. After Round-II, coherence and specificity are the most disagreed upon
    between raters. This could be attributed to their subjective nature (Kryściński
    et al., [2018](#bib.bib18)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4.4 Annotation Analysis ‣ 4 SummEval-Op Benchmark
    Dataset ‣ One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation") shows
    the average frequency of assigning a particular score by human raters for $7$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discuss the available benchmark dataset for opinion summary evaluation, the
    summarization models used for opinion summary generation, baseline metrics, and
    the implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | FL $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\rho$ |'
  prefs: []
  type: TYPE_TB
- en: '| SummEval-Op (Ours) | Humans | 0.80 | 0.77 | 0.81 | 0.76 | 0.91 | 0.86 | 0.89
    | 0.85 | 0.93 | 0.87 | 0.91 | 0.85 | 0.92 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Rouge-1 | -0.36 | -0.28 | -0.30 | -0.24 | -0.31 | -0.23 | -0.35 | -0.26 |
    -0.44 | -0.32 | -0.38 | -0.29 | -0.30 | -0.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Rouge-2 | -0.23 | -0.18 | -0.14 | -0.10 | -0.17 | -0.12 | -0.21 | -0.16 |
    -0.26 | -0.19 | -0.24 | -0.18 | -0.14 | -0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| Rouge-L | -0.39 | -0.32 | -0.30 | -0.23 | -0.34 | -0.25 | -0.40 | -0.30 |
    -0.51 | -0.37 | -0.45 | -0.33 | -0.38 | -0.27 |'
  prefs: []
  type: TYPE_TB
- en: '| BERTScore | -0.32 | -0.27 | -0.28 | -0.22 | -0.29 | -0.22 | -0.34 | -0.26
    | -0.51 | -0.43 | -0.41 | -0.33 | -0.37 | -0.28 |'
  prefs: []
  type: TYPE_TB
- en: '| BARTScore | -0.19 | -0.15 | -0.19 | -0.14 | -0.29 | -0.22 | -0.33 | -0.25
    | -0.45 | -0.35 | -0.37 | -0.28 | -0.36 | -0.27 |'
  prefs: []
  type: TYPE_TB
- en: '| SummaC | 0.23 | 0.20 | 0.18 | 0.14 | 0.30 | 0.25 | 0.25 | 0.21 | 0.24 | 0.19
    | 0.25 | 0.20 | 0.26 | 0.21 |'
  prefs: []
  type: TYPE_TB
- en: '| UniEval | 0.36 | 0.28 | 0.52 | 0.42 | 0.33 | 0.25 | 0.17 | 0.14 | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| G-Eval-3.5 | 0.63 | 0.55 | 0.59 | 0.49 | 0.68 | 0.56 | 0.70 | 0.58 | 0.79
    | 0.67 | 0.73 | 0.61 | 0.75 | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: '| OP-I-GPT-3.5 | 0.60 | 0.51 | 0.61 | 0.51 | 0.69 | 0.56 | 0.71 | 0.59 | 0.80
    | 0.68 | 0.73 | 0.61 | 0.74 | 0.61 |'
  prefs: []
  type: TYPE_TB
- en: '| G-Eval-Mistral | 0.50 | 0.43 | 0.54 | 0.45 | 0.52 | 0.42 | 0.54 | 0.44 |
    0.61 | 0.49 | 0.55 | 0.46 | 0.62 | 0.50 |'
  prefs: []
  type: TYPE_TB
- en: '| OP-Mistral | 0.38 | 0.32 | 0.58 | 0.47 | 0.56 | 0.45 | 0.57 | 0.46 | 0.80
    | 0.67 | 0.60 | 0.49 | 0.75 | 0.62 |'
  prefs: []
  type: TYPE_TB
- en: '| OP-I-Mistral | 0.54 | 0.45 | 0.58 | 0.47 | 0.59 | 0.47 | 0.63^∗ | 0.51^∗
    | 0.82^∗ | 0.70^∗ | 0.73^∗ | 0.61^∗ | 0.71^∗ | 0.58^∗ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Spearman ($\rho$) to G-Eval-Mistral computed using Mann-Whitney U
    Test. Humans- averaged correlation of each annotator with the overall averaged
    ratings.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Summarization Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pre-LLMs: For the Pre-LLMs, we obtain the publicly available summaries for
    the Amazon test set of these models. These models were trained in a self-supervised
    manner using only reviews data. (1) PlanSum (Amplayo and Lapata, [2020](#bib.bib1))
    uses content plans to create relevant review-summary pairs. The content plans
    take the form of aspect and sentiment distributions which are used along with
    input reviews for generating summaries. (2) MultimodalSum (Im et al., [2021](#bib.bib13))
    uses non-text data such as image and metadata along with reviews to generate opinion
    summaries. It uses a separate encoder for each modality and uses synthetic datasets
    to train the model in an end-to-end fashion. (3) Siledar et al. ([2023](#bib.bib27))
    uses lexical and semantic similarities to create a highly relevant synthetic dataset
    of review-summary pairs. This is then used to fine-tune any pre-trained language
    model for generating opinion summaries. (hereby referred to as LS-Sum-G).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4beaaccfe6bbcbf707abc4fc29fa016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Spearman correlation scores at different number of output generations
    (n) for the $7$B as their LLM. Generally, Op-I-Prompt shows better relative performance
    on both closed-source and open-source models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs: For the LLMs, we use simple prompts ^*^**Check Appendix [D.4](#A4.SS4
    "D.4 Summarization Prompt ‣ Appendix D Prompts ‣ One Prompt To Rule Them All:
    LLMs for Opinion Summary Evaluation") for the prompt to generate opinion summaries.
    These models were not specifically fine-tuned for opinion summarization. We use
    the HuggingFace library (Wolf et al., [2020](#bib.bib33)) to access these models.
    (1) ChatGPT-$\mathbf{3.5}$K user-shared conversations collected from ShareGPT
    [ShareGPT](#bib.bib25) . We use the: lmsys/vicuna-7b-v1.5 model and lmsys/vicuna-13b-v1.5
    model. (5) Solar-$\mathbf{10.7}$B (Tunstall et al., [2023](#bib.bib29)) is an
    open-sourced fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained
    on a mix of publicly available, synthetic datasets using Direct Preference Optimization
    (DPO) (Rafailov et al., [2023](#bib.bib24)). We use the beta version: HuggingFaceH4/zephyr-7b-beta
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following baseline metrics are used: Rouge-{1,2,L} score (Lin, [2004](#bib.bib20)),
    BERTScore (Zhang et al., [2019](#bib.bib35)), BARTScore (Yuan et al., [2021](#bib.bib34)),
    SummaC (Laban et al., [2022](#bib.bib19)), UniEval (Zhong et al., [2022](#bib.bib37)).
    We include G-Eval (Liu et al., [2023](#bib.bib21)) as our prompt-based baseline.
    G-Eval-3.5 and G-Eval-Mistral use ChatGPT-$3.5$B as their LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For evaluation, we used Mistral-$7$B size ensures easy replication. We set
    the hyperparameters to `n=100, temperature=0.7` to sample multiple generations.
    Example prompts are in Appendix [D](#A4 "Appendix D Prompts ‣ One Prompt To Rule
    Them All: LLMs for Opinion Summary Evaluation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | FL $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Human Summaries | 4.39 | 4.41 | 3.78 | 3.98 | 3.54 | 3.71 | 3.66 |'
  prefs: []
  type: TYPE_TB
- en: '| Pre-LLMs |'
  prefs: []
  type: TYPE_TB
- en: '| PlanSum | 1.86 | 1.94 | 1.60 | 1.38 | 1.52 | 1.59 | 1.56 |'
  prefs: []
  type: TYPE_TB
- en: '| MultimodalSum | 4.62 | 4.09 | 2.63 | 2.27 | 2.18 | 2.76 | 2.43 |'
  prefs: []
  type: TYPE_TB
- en: '| LS-Sum-G | 4.76 | 4.40 | 2.87 | 2.74 | 2.32 | 3.03 | 2.69 |'
  prefs: []
  type: TYPE_TB
- en: '| LLMs |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-$3.5$ | 4.89 | 4.58 | 4.25 | 4.71 | 4.22 | 4.16 | 3.96 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-$4$ | 5.00 | 4.91 | 3.52 | 4.96 | 4.93 | 4.83 | 4.57 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA$2$B | 4.79 | 4.34 | 3.77 | 4.49 | 3.67 | 3.79 | 3.46 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA$2$B | 4.87 | 4.49 | 4.25 | 4.62 | 4.02 | 4.00 | 3.94 |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral-$7$B | 4.86 | 4.60 | 4.33 | 4.66 | 4.56 | 4.35 | 4.25 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-$7$B | 4.83 | 4.23 | 3.92 | 4.35 | 3.96 | 3.92 | 3.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-$13$B | 4.87 | 4.41 | 4.09 | 4.43 | 4.03 | 4.00 | 3.77 |'
  prefs: []
  type: TYPE_TB
- en: '| Solar-$10.7$B | 4.89 | 4.73 | 4.20 | 4.72 | 4.50 | 4.56 | 4.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Zephyr-$7$B | 4.89 | 4.36 | 4.08 | 4.54 | 4.18 | 3.95 | 3.83 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Model-wise averaged annotator ratings of opinion summaries along $7$
    dimensions for the Amazon test set. Best scores are in bold, second-best are underlined.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'G-Eval vs. Op-I-Prompt vs. Op-Prompts. Table [2](#S5.T2 "Table 2 ‣ 5 Experiments
    ‣ One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation") and Table
    [5](#A1.T5 "Table 5 ‣ Appendix A Available Benchmark Dataset ‣ One Prompt To Rule
    Them All: LLMs for Opinion Summary Evaluation") report the summary-level ^*^**Check
    Appendix [C](#A3 "Appendix C Opinion Summary Evaluation ‣ One Prompt To Rule Them
    All: LLMs for Opinion Summary Evaluation") for definition. correlation scores
    on the SummEval-OP and OpinSummEval dataset. In the case of closed-source models,
    we observe that our Op-I-GPT-3.5 outperforms or performs comparably to G-Eval-3.5
    across all dimensions on both datasets. Specifically, our Op-I-GPT-3.5 outperforms
    G-Eval-3.5 on all $4$ dimensions for the OpinSummEval dataset, whereas for the
    SummEval-Op dataset, outperforms on coherence, faithfulness, and aspect coverage,
    performs comparably on relevance and sentiment consistency, underperforms slightly
    on fluency  and specificity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For open-source models, overall, we observe that Op-I-Mistral performs the
    best, followed by Op-Mistral and then G-Eval-Mistral. Figure [4](#S5.F4 "Figure
    4 ‣ 5.1 Summarization Models ‣ 5 Experiments ‣ One Prompt To Rule Them All: LLMs
    for Opinion Summary Evaluation") shows the performance of different prompt approaches
    over n=100 generations for $7$ dimensions and by a large margin specifically for
    aspect coverage, sentiment consistency, and specificity.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | AVG-S | MW $\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FL | G-Eval-Mistral^∗ | 0.48 | $\mathbf{2.9\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral | 0.38 |'
  prefs: []
  type: TYPE_TB
- en: '| CO | G-Eval-Mistral^∗ | 0.52 | $\mathbf{2.1\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral | 0.47 |'
  prefs: []
  type: TYPE_TB
- en: '| RE | G-Eval-Mistral | 0.51 | $6.7\times 10^{-2}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral | 0.49 |'
  prefs: []
  type: TYPE_TB
- en: '| FA | G-Eval-Mistral | 0.53 | $\mathbf{1.9\times 10^{-1}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral | 0.54 |'
  prefs: []
  type: TYPE_TB
- en: '| AC | G-Eval-Mistral | 0.58 | $\mathbf{2.1\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral^∗ | 0.74 |'
  prefs: []
  type: TYPE_TB
- en: '| SC | G-Eval-Mistral | 0.54 | $\mathbf{2.1\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral^∗ | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: '| SP | G-Eval-Mistral | 0.59 | $\mathbf{7.4\times 10^{-4}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Op-I-Mistral^∗ | 0.63 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Significance Test. P-values computed using Mann-Whitney U Test (MW)
    and T-Test (TT) between the average Spearman correlation scores (AVG-S) taken
    over $10$ represents significant performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Significance Testing. We perform significance testing using the Mann-Whitney
    U Test (McKnight and Najab, [2010](#bib.bib22)) for comparison between Op-I-Mistral
    and G-Eval-Mistral. Table [2](#S5.T2 "Table 2 ‣ 5 Experiments ‣ One Prompt To
    Rule Them All: LLMs for Opinion Summary Evaluation") report results for Spearman
    and Kendall Tau scores computed by using the scoring function with n=100. Op-I-Mistral
    significantly (p-value $1 - The metric is not followed at all while generating the summary
    from the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 2 - The metric is followed only to a limited extent while generating
    the summary from the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 3 - The metric is followed to a good extent while generating
    the summary from the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 4 - The metric is followed mostly while generating the summary
    from the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: '5 - The metric is followed completely while generating the summary
    from the reviews. Metric:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aspect Coverage - The summary should cover all the aspects that are majorly
    being discussed in the reviews. Summaries should be penalized if they miss out
    on an aspect that was majorly being discussed in the reviews and awarded if it
    covers all. Reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Evaluation Steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the following steps strictly while giving the response:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.First write down the steps that are needed to evaluate the summary as per
    the metric. Reiterate what metric you will be using to evaluate the summary.
  prefs: []
  type: TYPE_NORMAL
- en: 2.Give a step-by-step explanation if the summary adheres to the metric considering
    the reviews as the input. Stick to the metric only for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.Next, evaluate the extent to which the metric is followed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.Use the previous information to rate the summary using the evaluation criteria
    and assign a score within the  tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Strictly give the score within  tags only e.g Score- 5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First give a detailed explanation and then finally give a single score following
    the format: Score- 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'THE EVALUATION AND SCORE MUST BE ASSIGNED STRICTLY ACCORDING TO THE METRIC
    ONLY AND NOTHING ELSE! Response:'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Op-Prompts for Aspect Coverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Task Description:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be given a set of reviews. You will then be given one summary written
    for the set of reviews. Your task is to rate the summary on one metric. Make sure
    you understand the following evaluation metric very clearly. Your task is to rate
    the summary corresponding to the given reviews on the evaluation criteria. Evaluation
    Criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Aspect Coverage - The summary should cover all the aspects that are majorly
    being discussed in the reviews. Summaries should be penalized if they miss out
    on an aspect that was majorly being discussed in the reviews and awarded if it
    covers all.
  prefs: []
  type: TYPE_NORMAL
- en: 1 - Summary does not cover any important aspects present in the
    reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 2 - Summary does not cover most of the important aspects present
    in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 3 - Summary covers around half of the important aspects present
    in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 4 - Summary covers most of the important aspects present in reviews.
  prefs: []
  type: TYPE_NORMAL
- en: '5 - Summary covers all the important aspects discussed in reviews.
    Metric:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aspect Coverage - The summary should cover all the aspects that are majorly
    being discussed in the reviews. Summaries should be penalized if they miss out
    on an aspect that was majorly being discussed in the reviews and awarded if it
    covers all. Reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Evaluation Steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go step-by-step. Follow the following steps strictly while giving the
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.Identify the important aspects present in the reviews and list them with numbering
  prefs: []
  type: TYPE_NORMAL
- en: 2.Identify the important aspects present in the summary and list them with numbering
  prefs: []
  type: TYPE_NORMAL
- en: 3.Identify the important aspects covered by the summary that are present in
    the reviews and list them with numbering
  prefs: []
  type: TYPE_NORMAL
- en: 4.Calculate the total number of important aspects covered by the summary that
    are present in the reviews
  prefs: []
  type: TYPE_NORMAL
- en: 5.Calculate the total number of important aspects present in the reviews
  prefs: []
  type: TYPE_NORMAL
- en: 6.Finally use the evaluation criteria to output only a single score within 
    tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Strictly give the score within  tags only e.g Score- 5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First give a detailed explanation of how much is the coverage and then finally
    give a single score following the format: Score- 5 Response:'
  prefs: []
  type: TYPE_NORMAL
- en: D.3 G-Eval for Aspect Coverage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Task Description:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be given a set of reviews and a corresponding summary. Make sure you
    understand the following evaluation metric very clearly. Your task is to rate
    the summary corresponding to the given reviews on the evaluation criteria. Evaluation
    Criteria: Aspect Coverage (1-5) - The summary should cover all the aspects that
    are majorly being discussed in the reviews. Summaries should be penalized if they
    miss out on an aspect that was majorly being discussed in the reviews and awarded
    if it covers all. Reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Evaluation Steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.Read through the given set of reviews carefully.
  prefs: []
  type: TYPE_NORMAL
- en: 2.Compare the content of the reviews to the provided summary.
  prefs: []
  type: TYPE_NORMAL
- en: 3.Evaluate whether the summary covers all the major aspects that are being discussed
    in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 4.Rate the summary on a scale of 1-5 based on how well it covers the aspects
    discussed in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 5.Provide a brief explanation for your rating, citing specific examples from
    the reviews and summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Strictly give the score within  tags only e.g Score: 5.
    Response:'
  prefs: []
  type: TYPE_NORMAL
- en: D.4 Summarization Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generate a summary for the following set of reviews. Generate the summary in
    a paragraph format. No bulletpoints or explanations needed. Just output the summary
    text. Reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: '{} Summary:'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Dimension Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For ablation, we try out three different definition variations of aspect coverage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition 1: The summary should cover all the aspects that are majorly being
    discussed in the reviews. Summaries should be penalized if they miss out on an
    aspect that was majorly being discussed in the reviews and awarded if it covers
    all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition 2: This refers to the comprehensiveness of a summary in capturing
    all significant aspects discussed in reviews. A summary is evaluated based on
    its ability to include major topics of discussion; it is deemed deficient if it
    overlooks any crucial aspect and commendable if it encompasses them all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition 3: Aspect coverage pertains to the extent to which a summary encapsulates
    the key facets discussed in reviews. Summaries are evaluated based on their ability
    to incorporate major discussion points. They are considered deficient if they
    omit any critical aspect and commendable if they address them all comprehensively.'
  prefs: []
  type: TYPE_NORMAL
