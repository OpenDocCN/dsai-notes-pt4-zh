- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:42:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:42:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing
    and Optimization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个更好的文本生成LLM评估器：提示输出排序和优化的影响
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.09972](https://ar5iv.labs.arxiv.org/html/2406.09972)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.09972](https://ar5iv.labs.arxiv.org/html/2406.09972)
- en: KuanChao Chu^*    Yi-Pei Chen^*    Hideki Nakayama
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: KuanChao Chu^*    Yi-Pei Chen^*    Hideki Nakayama
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This research investigates prompt designs of evaluating generated texts using
    large language models (LLMs). While LLMs are increasingly used for scoring various
    inputs, creating effective prompts for open-ended text evaluation remains challenging
    due to model sensitivity and subjectivity in evaluation of text generation. Our
    study experimented with different prompt structures, altering the sequence of
    output instructions and including explanatory reasons. We found that the order
    of presenting reasons and scores significantly influences LLMs’ scoring, with
    a different level of rule understanding in the prompt. An additional optimization
    may enhance scoring alignment if sufficient data is available. This insight is
    crucial for improving the accuracy and consistency of LLM-based evaluations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究探讨了使用大型语言模型（LLMs）评估生成文本的提示设计。虽然LLMs在对各种输入评分中的使用越来越广泛，但由于模型的敏感性和文本生成评估的主观性，为开放式文本评估创建有效的提示仍然具有挑战性。我们的研究试验了不同的提示结构，改变了输出指令的顺序并包含了解释性理由。我们发现，呈现理由和分数的顺序显著影响LLMs的评分，并且在提示中对规则的理解程度有所不同。如果数据足够充足，额外的优化可能会增强评分的一致性。这一见解对提高基于LLM的评估的准确性和一致性至关重要。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: '^†^†footnotetext: Equal contribution'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†脚注：等贡献
- en: Using large language models (LLMs) [[1](#bib.bib1), [2](#bib.bib2)] as evaluators
    to assign scores to the given inputs have become prevalent. Leblond et al. [[3](#bib.bib3)]
    outputs a score between 0 and 1 to estimate the correctness of generated code,
    thereby automatically ranking its quality. Similarly, Park et al. [[4](#bib.bib4)]
    assign poignancy score to the generated text for the retrieval task. Other research
    explores using LLMs to assess generated texts, finding the LLM scores correlates
    higher with human evaluators than existing automatic metrics [[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLMs）[[1](#bib.bib1), [2](#bib.bib2)]作为评估者来对给定输入分配分数已变得普遍。Leblond等人[[3](#bib.bib3)]输出0到1之间的分数以估计生成代码的正确性，从而自动排名其质量。同样，Park等人[[4](#bib.bib4)]为检索任务分配生成文本的感人分数。其他研究探讨了使用LLMs评估生成文本，发现LLM的分数与人类评估者的相关性高于现有的自动指标[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]。
- en: However, designing evaluation prompt for LLMs is not a trivial task, especially
    for dialogue evaluation. Different models exhibit varied sensitivity to the nuances
    of input prompts. Even slight linguistic variations can lead to significant fluctuations
    in task performance [[9](#bib.bib9)]. Moreover, the inherent subjectivity in dialogue
    evaluation adds on the difficulty and versatility in LLMs’ evaluation results.
    While prompt optimization techniques [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)] have been developed to assist in designing more effective prompts,
    these methods require paired input-output samples for objective value calculation.
    Unfortunately, the lack of available dialogue-score pairing data hampers the application
    of prompt optimization in dialogue evaluation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，设计用于LLMs的评估提示并非一件简单的任务，尤其是对话评估。不同的模型对输入提示的细微差别表现出不同的敏感性。即使是微小的语言变化也可能导致任务性能的显著波动[[9](#bib.bib9)]。此外，对话评估中的固有主观性增加了LLMs评估结果的难度和多样性。虽然已开发出提示优化技术[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]以帮助设计更有效的提示，但这些方法需要成对的输入-输出样本来进行客观值计算。不幸的是，缺乏可用的对话评分配对数据阻碍了提示优化在对话评估中的应用。
- en: In this study, we aim to investigate the influence of prompt design on dialogue
    evaluation, specifically focusing on how the output instructions affects the resulting
    scores. We have developed multiple prompt variations to assess the quality of
    a series of dialogues. These variations involve altering the sequence order of
    the outputs and examining whether including explanatory reasons along with the
    scores impacts the evaluation. Our analysis compares the influence of different
    prompts on the scoring outcomes across various versions of GPT models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们旨在调查提示设计对对话评价的影响，特别关注输出指令如何影响结果分数。我们开发了多个提示变体来评估一系列对话的质量。这些变体涉及改变输出的顺序，并检查是否包括解释理由与分数一起会影响评价。我们的分析比较了不同提示对各种
    GPT 模型版本的评分结果的影响。
- en: '![Refer to caption](img/5fe537009bb9d0f631e0b541f7fef2a5.png)![Refer to caption](img/f4ff2eaa12602c928e006c0ce916daf4.png)![Refer
    to caption](img/1be18a936e792c4677c978185b6ff20d.png)![Refer to caption](img/362490a2bf0f801ae05c747d857088a7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5fe537009bb9d0f631e0b541f7fef2a5.png)![参见标题](img/f4ff2eaa12602c928e006c0ce916daf4.png)![参见标题](img/1be18a936e792c4677c978185b6ff20d.png)![参见标题](img/362490a2bf0f801ae05c747d857088a7.png)'
- en: 'Figure 1: Score distribution across 50 trials for each model and output instruction
    configuration for a dialogue set.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：每个模型和输出指令配置在对话集中的 50 次试验中的分数分布。
- en: We observed that the different order of output instructions can result in different
    scoring distributions by certain LLMs, even when the corresponding output reasons
    are similar. Considering the sequential generation nature of auto-regressive models,
    placing the score after the reasons allows it to reference both the reasons and
    the input prompt, a dynamic not possible when this order is reversed. The finding
    suggests that a “reason-first” output instruction might lead to a more comprehensive
    understanding and adherence to the specific requirements of the task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，不同的输出指令顺序会导致某些 LLM 在分数分布上有所不同，即使对应的输出理由类似。考虑到自回归模型的顺序生成特性，将分数放在理由之后可以参考理由和输入提示，而这种动态在顺序颠倒时不可实现。研究结果表明，“理由优先”输出指令可能会导致对任务具体要求的更全面理解和遵守。
- en: Finally, we further investigate the effect of prompt optimization on the “reason-first”
    output instruction. This is to observe the most effective form of instruction
    in a hypothetical setting where paired data is available.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进一步研究提示优化对“理由优先”输出指令的影响。这是为了观察在配对数据可用的假设环境下，指令的最有效形式。
- en: '![Refer to caption](img/e4ec590fb6aa5c2526db9ae1a0740ea0.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e4ec590fb6aa5c2526db9ae1a0740ea0.png)'
- en: 'Figure 2: The form of prompt for LLM scorer in conversation evaluation. The
    special rules section is simplified for better readability.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：对话评价中 LLM 评分员的提示形式。特殊规则部分已简化以提高可读性。
- en: 2\. Output Instruction in Dialogue Evaluation
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 对话评价中的输出指令
- en: '| Config | Output Instruction in the Prompt |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Config | 输出提示中的指令 |'
- en: '| ex (s) | Example JSON output: |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ex (s) | 示例 JSON 输出： |'
- en: '|  | {"score": 5} |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | {"score": 5} |'
- en: '| ex (sr) | Example JSON output: |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ex (sr) | 示例 JSON 输出： |'
- en: '|  | {"score": 5, "reasons": "$<$"} |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | {"score": 5, "reasons": "$<$"} |'
- en: '| ex (rs) | swap the order of “score” and “reasons” in ex (sr) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ex (rs) | 在 ex (sr) 中交换“分数”和“理由”的顺序 |'
- en: '| json (s) | Output a json of the following format: |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| json (s) | 输出以下格式的 JSON： |'
- en: '|  | {"score": "$<$"} |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | {"score": "$<$"} |'
- en: '| json (sr) | Output a json of the following format: |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| json (sr) | 输出以下格式的 JSON： |'
- en: '|  | {"score": "$<$", "reasons": "point out the issues and your reasons for
    the rating"} |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | {"score": "$<$", "reasons": "指出问题及评分理由"} |'
- en: '| json (rs) | swap the order of “score” and “reasons” in json (sr) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| json (rs) | 在 json (sr) 中交换“分数”和“理由”的顺序 |'
- en: 'Table 1: The variations of output instruction.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：输出指令的变体。
- en: Approach
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方法
- en: In this section, the task assigned to the LLM is to rate a given set of dialogues
    on a scale from 1 to 10, where 1 indicates no issues in the set of dialogues,
    and 10 signifies severe problems. Additionally, if specified in the prompt, the
    LLM is required to provide a rationale for the rating. The dialogues are presented
    in chronological order, and the output score is determined based on a comprehensive
    evaluation of the entire set, focusing on key aspects such as factual accuracy,
    repetitiveness, and coherence.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，任务是对给定的对话集进行 1 到 10 的评分，其中 1 表示对话集没有问题，10 表示严重问题。此外，如果提示中指定了，LLM 需要提供评分的理由。对话按时间顺序呈现，输出得分基于对整个对话集的全面评估，重点关注关键方面，如事实准确性、重复性和连贯性。
- en: Along with the task description, we have integrated five customized rules into
    the prompt, derived from observations in previous experiments without these rules.
    The special rules include instructions for the LLM to prioritize the number of
    issues over their impact and to assign more weight to aspects exhibiting significant
    issues, rather than averaging out the score across all aspects.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了任务描述，我们还将五条定制规则整合进了提示中，这些规则源自之前实验中的观察，这些实验没有这些规则。特殊规则包括指导LLM优先考虑问题数量而非其影响，并且对表现出显著问题的方面赋予更多权重，而不是对所有方面的得分进行平均。
- en: 'The final evaluation prompt is organized as follows: a set of dialogues, task
    description, special rules, and output instruction (see Table [1](#S2.T1 "Table
    1 ‣ 2\. Output Instruction in Dialogue Evaluation ‣ A Better LLM Evaluator for
    Text Generation: The Impact of Prompt Output Sequencing and Optimization")). For
    each set of dialogues, we conducted $N$ different models.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的评估提示组织如下：一组对话、任务描述、特殊规则和输出指令（见表 [1](#S2.T1 "表 1 ‣ 2\. 对话评估中的输出指令 ‣ 更好的 LLM
    评估器：提示输出排序和优化的影响")）。对于每组对话，我们进行了 $N$ 种不同的模型。
- en: '![Refer to caption](img/e8ca657be4ff8a10d5773ea9e3d95c45.png)![Refer to caption](img/6f92bba209bf283ea74c1dc641fc7e07.png)![Refer
    to caption](img/ea93325a8b17d6a490939aa975d4c563.png)![Refer to caption](img/6cc66ee3e178149ba8cd6488f7f0ed31.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e8ca657be4ff8a10d5773ea9e3d95c45.png)![参见标题](img/6f92bba209bf283ea74c1dc641fc7e07.png)![参见标题](img/ea93325a8b17d6a490939aa975d4c563.png)![参见标题](img/6cc66ee3e178149ba8cd6488f7f0ed31.png)'
- en: 'Figure 3: Score distribution across 50 trials for each model and output instruction
    configuration for a dialogue set, with the ‘special rules’ omitted from the prompt.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：每个模型和输出指令配置在 50 次试验中的得分分布，‘特殊规则’被省略在提示中。
- en: Data
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据
- en: To assess the capability of LLMs in identifying issues within dialogues, we
    collected LLM-generated dialogues from Park et al. [[4](#bib.bib4)] and manually
    grouped them into 25 sets. Each set contains four to six dialogues and exhibits
    one or more problems, such as repetition or contradictions between dialogues.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM识别对话中问题的能力，我们从 Park 等人处收集了LLM生成的对话 [[4](#bib.bib4)] 并手动将其分组为 25 组。每组包含四到六个对话，表现出一个或多个问题，例如对话之间的重复或矛盾。
- en: Model
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型
- en: 'We selected four recent LLMs to serve as scorers: gpt-3.5-turbo-0613, gpt-3.5-turbo-1106,
    gpt-4-0613, and gpt-4-1106-preview (gpt-4-1106). Note that our aim is to analyze
    the evaluation scores across various models when altering output instructions,
    and not to compare them with human judgements for this task.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了四个最近的LLM作为评分者：gpt-3.5-turbo-0613、gpt-3.5-turbo-1106、gpt-4-0613 和 gpt-4-1106-preview
    (gpt-4-1106)。请注意，我们的目标是分析在更改输出指令时各种模型的评估得分，而不是与人为判断进行比较。
- en: 2.1 Result and Analysis
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 结果与分析
- en: '| Config | GPT-3.5-turbo | GPT-4 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | GPT-3.5-turbo | GPT-4 |'
- en: '| -0613 | -1106 | -0613 | -1106 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| -0613 | -1106 | -0613 | -1106 |'
- en: '| ex (s) | 3.68 $\pm$1.05 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ex (s) | 3.68 $\pm$1.05 |'
- en: '| ex (sr) | 4.20 $\pm$1.12 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ex (sr) | 4.20 $\pm$1.12 |'
- en: '| ex (rs) | 6.09 $\pm$0.90 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ex (rs) | 6.09 $\pm$0.90 |'
- en: '| json (s) | 4.03 $\pm$1.24 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| json (s) | 4.03 $\pm$1.24 |'
- en: '| json (sr) | 4.66 $\pm$1.06 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| json (sr) | 4.66 $\pm$1.06 |'
- en: '| json (rs) | 5.78 $\pm$0.95 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| json (rs) | 5.78 $\pm$0.95 |'
- en: 'Table 2: Mean scores and std for 25 dialogue sets, evaluated across different
    models and output instruction configurations.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：25 组对话的平均分和标准差，不同模型和输出指令配置下的评估结果。
- en: The Importance of Output Instruction Order
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输出指令顺序的重要性
- en: 'Table [2](#S2.T2 "Table 2 ‣ 2.1 Result and Analysis ‣ 2\. Output Instruction
    in Dialogue Evaluation ‣ A Better LLM Evaluator for Text Generation: The Impact
    of Prompt Output Sequencing and Optimization") presents the mean scores and standard
    deviations (std) of 10 trials for all 25 dialogue sets across all configs and
    models. In both ex ($\cdot$) formats, the mean scores for the rs settings (output
    reasons before the score) are generally higher than their sr (output score before
    reasons) counterparts. ¹¹1The exception is observed with the gpt-4-1106 model.For
    instance, in the json (rs) config using gpt-4-0613, the mean score is 5.34, while
    it drops to 3.26 in json (sr), despite providing similar reasons. We conjecture
    that in the rs setting, the autoregressive nature of the model allows the score
    to be influenced by the previously outputted reasons.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[2](#S2.T2 "Table 2 ‣ 2.1 Result and Analysis ‣ 2\. Output Instruction in
    Dialogue Evaluation ‣ A Better LLM Evaluator for Text Generation: The Impact of
    Prompt Output Sequencing and Optimization")展示了所有25个对话集在所有配置和模型下10次试验的平均分和标准差（std）。在两种ex
    ($\cdot$)格式中，rs设置（在分数之前输出理由）的平均分通常高于其对应的sr设置（在理由之前输出分数）。¹¹1例外的是gpt-4-1106模型。例如，在使用gpt-4-0613的json
    (rs)配置中，平均分为5.34，而在json (sr)中则降至3.26，尽管提供了类似的理由。我们推测，在rs设置中，模型的自回归特性使得分数会受到之前输出的理由的影响。'
- en: Different Levels of Rule Understanding
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同层次的规则理解
- en: 'In a focused study on a single set with additional 40 trials, as depicted in
    Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Better LLM Evaluator for Text
    Generation: The Impact of Prompt Output Sequencing and Optimization"), we observed
    a trend consistent with the findings presented in Table [2](#S2.T2 "Table 2 ‣
    2.1 Result and Analysis ‣ 2\. Output Instruction in Dialogue Evaluation ‣ A Better
    LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and
    Optimization"). However, as shown in Fig.[3](#S2.F3 "Figure 3 ‣ Approach ‣ 2\.
    Output Instruction in Dialogue Evaluation ‣ A Better LLM Evaluator for Text Generation:
    The Impact of Prompt Output Sequencing and Optimization"), when we removed the
    ‘special rules’ from the prompt, we found that most scores were lower and the
    distinctions between different settings became less pronounced. This highlights
    the models’ sensitivity to the changes of the prompt.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在对单一数据集进行的额外40次试验的集中研究中，如图[1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Better
    LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and
    Optimization")所示，我们观察到的趋势与表格[2](#S2.T2 "Table 2 ‣ 2.1 Result and Analysis ‣ 2\.
    Output Instruction in Dialogue Evaluation ‣ A Better LLM Evaluator for Text Generation:
    The Impact of Prompt Output Sequencing and Optimization")中的发现一致。然而，如图[3](#S2.F3
    "Figure 3 ‣ Approach ‣ 2\. Output Instruction in Dialogue Evaluation ‣ A Better
    LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and
    Optimization")所示，当我们从提示中移除‘特殊规则’时，我们发现大多数分数较低，不同设置之间的区别变得不那么明显。这突显了模型对提示变化的敏感性。'
- en: '| Method | Iteration | Output Instruction | score on $\mathcal{S}$) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 迭代 | 输出指令 | 在$\mathcal{S}$上的评分 |'
- en: '| init(json(rs)) | 0 | Output a json of the following format: {”reasons”: ”point
    out your reasons for the rating on coherence”, ”score”: ”the rating”} | -1.271
    | 51.9 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| init(json(rs)) | 0 | 输出如下格式的json: {”reasons”: ”指出你对一致性的评分理由”, ”score”: ”评分”}
    | -1.271 | 51.9 |'
- en: '| +GRIPS | 1 | Output a json of   following format: {”reasons”: ”point out
    your reasons for   rating on coherence”, ”score”: ”the rating”} | -1.267 |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| +GRIPS | 1 | 输出如下格式的json: {”reasons”: ”指出你对一致性的评分理由”, ”score”: ”评分”} | -1.267
    |  |'
- en: '|  | 4 | Output json of following format: {”reasons”: ”point out your reasons
    for rating on coherence”, a ”score”: ”of rating”} | -1.234 |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | 4 | 输出如下格式的json: {”reasons”: ”指出你对一致性的评分理由”, ”score”: ”评分”} | -1.234 |  |'
- en: '|  | 7 | Output json of following format ”{”reasons” ””point out your reasons
    for rating on coherence”, ”score” ””of rating”} | -1.100 |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | 7 | 输出如下格式的json: {”reasons”: ”指出你对一致性的评分理由”, ”score”: ”评分”} | -1.100 |  |'
- en: '| +OPRO | 25 | Assess the provided summary’s coherence in relation to the source
    text, grading it on a scale of 1 to 5. Coherence refers to the seamless connection
    of ideas and the logical structuring of information from the source into a unified
    summary. The summary should not just be a cluster of related information, but
    a well-knit narrative reflecting the source’s key points. Ensure you provide concise
    reasoning for your rating, discussing the summary’s strengths or weaknesses in
    its coherence. Consistently refer back to these guidelines during your assessment.
    |  | 46.0 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| +OPRO | 25 | 评估所提供摘要与源文本的一致性，并按照1到5的等级进行评分。一致性指的是思想的无缝衔接和将信息从源文本逻辑性地整合成一个统一的摘要。摘要不仅仅是相关信息的集合，而是反映源文本关键点的连贯叙述。确保为你的评分提供简明的理由，讨论摘要在一致性方面的优点或缺点。在评估过程中，始终参考这些指南。
    |  | 46.0 |'
- en: '|  | 43 | Evaluate the summary provided, grading its coherence with the original
    news article on a scale of 1-5. Consider how the summary reflects the logical
    progression, structure, and key details from the source text. A superior summary
    should interpret related details into a clear, comprehensive story, not merely
    gathering related facts. For an effective assessment, underpin your score with
    explicit examples from the text. Concentrate on the flow and organization of the
    summary throughout your evaluation. |  | 62.2 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | 43 | 评估所提供的摘要，按照1-5的等级评分其与原新闻文章的一致性。考虑摘要如何反映源文本的逻辑进展、结构和关键细节。一个优秀的摘要应将相关细节解读成一个清晰、全面的故事，而不仅仅是收集相关事实。为了有效评估，用明确的文本示例支撑你的评分。在评估过程中集中关注摘要的流畅性和组织结构。
    |  | 62.2 |'
- en: 'Table 3: Examples of output instructions with optimization methods GRIPS and
    OPRO. We pick the one with highest scores on $\mathcal{S}$ as the optimized instructions.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：带有优化方法GRIPS和OPRO的输出指令示例。我们选择在$\mathcal{S}$上得分最高的作为优化后的指令。
- en: 3\. Prompt optimization
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 提示优化
- en: Approach
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方法
- en: 'We assume there are sufficient paired input and output data samples for the
    scoring task. Hence, in this section, we examine how output instructions can be
    enhanced through additional prompt optimization. We apply two recent optimization
    methods to the existing prompt: GRIPS [[13](#bib.bib13)] and OPRO [[11](#bib.bib11)].
    For a given score set $\mathcal{S}$. Meanwhile, OPRO directly employs a LLM as
    an optimizer to generate new prompts, with previous score trajectories included
    as part of the reference material for the optimizer. Specifically, we utilize
    the json (rs) style for the output instruction in the initial prompt, optimizing
    only the output instruction section while keeping the other sections unchanged.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设有足够的配对输入和输出数据样本用于评分任务。因此，在本节中，我们检查如何通过额外的提示优化来增强输出指令。我们对现有提示应用了两种最新的优化方法：GRIPS
    [[13](#bib.bib13)]和OPRO [[11](#bib.bib11)]。对于给定的评分集$\mathcal{S}$。同时，OPRO直接使用LLM作为优化器生成新提示，并将以前的评分轨迹作为优化器参考材料的一部分。具体来说，我们在初始提示中使用json
    (rs)样式的输出指令，仅优化输出指令部分，同时保持其他部分不变。
- en: Data and Implementation Details
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据和实现细节
- en: For this experiment, we utilize SummEval [[14](#bib.bib14)], which comprises
    1,600 text summarization samples derived from 100 unique document pieces and 16
    summarization models. Each sample includes human expert ratings ranging from 1
    to 5 across various aspects. We focus on the ”coherence” aspect for the expected
    output. To ensure rating diversity in $\mathcal{S}$, we uniformly sample 10% of
    the documents after ranking them by their average coherence score to create the
    score set for GRIPS, and use a halved subset for OPRO. The remaining 90% of the
    data is reserved for testing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们利用SummEval [[14](#bib.bib14)]，它包含1,600个文本摘要样本，来自100个独特的文档片段和16个摘要模型。每个样本包括从1到5的人工专家评分，涉及各个方面。我们关注“**一致性**”方面作为期望的输出。为了确保$\mathcal{S}$中的评分多样性，我们在按平均一致性评分排序后，均匀抽取10%的文档来创建GRIPS的评分集，并使用一半的子集用于OPRO。其余90%的数据保留用于测试。
- en: For GRIPS, we configure the editing operations at the word level and conduct
    10 search iterations, with 5 candidates in each iteration. For OPRO, we employ
    gpt-4-0613 as the optimizer, running 50 iterations with 2 generations each. We
    include two randomly selected data exemplars from $\mathcal{S}$ when generating
    the output instructions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GRIPS，我们在单词级别配置编辑操作并进行10次搜索迭代，每次迭代5个候选项。对于OPRO，我们使用gpt-4-0613作为优化器，运行50次迭代，每次生成2个结果。在生成输出指令时，我们包括了从$\mathcal{S}$中随机选择的两个数据样本。
- en: We calculate the mean absolute error (MAE) between the human ratings and LLM
    ratings to evaluate the overall score for $\mathcal{S}$, with the number of trials
    increased to 20.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了人类评分和LLM评分之间的平均绝对误差（MAE），以评估$\mathcal{S}$的整体得分，同时将试验次数增加到20次。
- en: 3.1 Result and Analysis
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 结果与分析
- en: The Changes in Output Instructions and Generalization on Test Data
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输出指令的变化和测试数据上的泛化
- en: 'In the prompt examples shown in Table [3](#S2.T3 "Table 3 ‣ Different Levels
    of Rule Understanding ‣ 2.1 Result and Analysis ‣ 2\. Output Instruction in Dialogue
    Evaluation ‣ A Better LLM Evaluator for Text Generation: The Impact of Prompt
    Output Sequencing and Optimization"), OPRO demonstrates a preference for developing
    a diverse and detailed output instruction section. Some of the content either
    repeats or adds new details to the grading policy and may explicitly request the
    provision of reasons. The changes made using GRIPS, however, are comparatively
    limited due to the smaller search space. For example, the candidate with the best
    score on $\mathcal{S}$ underwent only five editing commands: “del-del-swap-swap-del.”'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [3](#S2.T3 "表 3 ‣ 不同级别的规则理解 ‣ 2.1 结果与分析 ‣ 2. 输出指令在对话评估中的作用 ‣ 更好的LLM评估器：提示输出顺序和优化的影响")中显示的提示示例中，OPRO显示出对开发多样化且详细的输出指令部分的偏好。部分内容重复或添加了对评分政策的新细节，可能会明确要求提供理由。然而，使用GRIPS进行的更改相对有限，因为搜索空间较小。例如，在$\mathcal{S}$上得分最高的候选项只经历了五个编辑命令：“del-del-swap-swap-del。”
- en: 'Table [4](#S3.T4 "Table 4 ‣ The Changes in Output Instructions and Generalization
    on Test Data ‣ 3.1 Result and Analysis ‣ 3\. Prompt optimization ‣ A Better LLM
    Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization")
    lists the performance of the LLM scorer on the test set. Surprisingly, the output
    instruction modified with GRIPS yield improved scores, whereas OPRO shows the
    opposite. However, the number of iterations set for OPRO is fewer than what was
    used in the original paper, due to cost constraints. Therefore, its performance
    might be far from saturation. Following [[15](#bib.bib15)], we apply William’s
    test to Pearson’s r, and we found that the improvement with GRIPS over the initial
    instruction is statistically significant.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#S3.T4 "表 4 ‣ 输出指令的变化和测试数据上的泛化 ‣ 3.1 结果与分析 ‣ 3. 提示优化 ‣ 更好的LLM评估器：提示输出顺序和优化的影响")列出了LLM评分器在测试集上的表现。令人惊讶的是，使用GRIPS修改的输出指令产生了更好的评分，而OPRO则相反。然而，由于成本限制，OPRO设置的迭代次数少于原文中使用的次数。因此，其性能可能远未饱和。根据[[15](#bib.bib15)]，我们对皮尔逊r应用了William’s检验，我们发现GRIPS相对于初始指令的改进在统计上是显著的。
- en: '|  | MAE($\downarrow$) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | MAE($\downarrow$) |'
- en: '| init(json(rs)) | 0.739 | 0.599 | 0.439 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| init(json(rs)) | 0.739 | 0.599 | 0.439 |'
- en: '| +GRIPS | 0.696 | 0.614 | 0.449 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| +GRIPS | 0.696 | 0.614 | 0.449 |'
- en: '| +OPRO | 0.780 | 0.466 | 0.328 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| +OPRO | 0.780 | 0.466 | 0.328 |'
- en: 'Table 4: The MAE and dataset-level correlation coefficients between LLM scorer
    and human experts ratings on the test set, using inital and post-optimization
    output instructions.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：LLM评分器与人类专家在测试集上的MAE和数据集级相关系数，使用初始和优化后的输出指令。
- en: The Score Difference for Swapped Orders Exists
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 存在交换顺序的得分差异
- en: We also compare the json (rs) and json (sr) types of initial instructions. The
    averaged score on the test set is 3.656 for json (rs) and 3.270 for json (sr).
    This reflects a similar distribution difference to what we observed in the previous
    section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还比较了初始指令的json (rs)和json (sr)类型。json (rs)在测试集上的平均分为3.656，而json (sr)为3.270。这反映出与我们在上一节观察到的类似的分布差异。
- en: Challenges
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 挑战
- en: First, although we intentionally divide a score set to study the effect of optimization,
    collecting related samples for a specific scoring task can be challenging. Second,
    these optimizations are applied globally to all data samples, while the optimized
    output instructions might vary for individual cases. Finally, the optimized instructions
    could break the structured output format and thus requires careful post-generation
    processing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，尽管我们有意将评分集分开以研究优化的效果，但为特定评分任务收集相关样本可能是具有挑战性的。其次，这些优化在所有数据样本上是全局应用的，而优化后的输出指令可能因个别情况而有所不同。最后，优化后的指令可能会打破结构化的输出格式，因此需要在生成后进行仔细处理。
- en: 4\. Conclusion
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 结论
- en: Our study highlights the scoring sensitivity of LLMs to the order of output
    instructions, which can be amplified by task-specific rules. Users should verify
    that their intentions align with the outputs using crafted prompts. Additionally,
    prompt optimization has proven beneficial in aligning LLM outputs with ground-truth
    scores, emphasizing the need for sufficient data samples for arbitrary scoring
    tasks. These findings offer insights for designing prompts in subjective tasks
    like dialogue evaluation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究突出了LLMs对输出指令顺序的评分敏感性，这种敏感性可以通过特定任务的规则得到放大。用户应通过精心设计的提示来验证他们的意图是否与输出结果一致。此外，提示优化已被证明有助于将LLM输出与真实评分对齐，强调了为任意评分任务提供足够数据样本的必要性。这些发现为设计主观任务中的提示（如对话评估）提供了见解。
- en: References
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: OpenAI [2023] OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI。GPT-4技术报告。*ArXiv*，abs/2303.08774，2023年。
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等[2023] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。Llama
    2：开放基础和微调的聊天模型。*arXiv预印本 arXiv:2307.09288*，2023年。
- en: Leblond et al. [2023] placeholder Leblond et al. Alphacode 2 technical report,
    2023. URL https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leblond等[2023] 占位符 Leblond等。Alphacode 2技术报告，2023年。网址 https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf。
- en: '[4] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. *UIST ’23*.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Joon Sung Park、Joseph C O’Brien、Carrie J Cai、Meredith Ringel Morris、Percy
    Liang 和 Michael S Bernstein。生成代理：人类行为的互动仿真。*UIST ’23*。'
- en: Gao et al. [2023] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang,
    and Xiaojun Wan. Human-like summarization evaluation with chatgpt. *arXiv preprint
    arXiv:2304.02554*, 2023.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等[2023] 高铭琪 Mingqi Gao、阮杰 Jie Ruan、孙仁良 Renliang Sun、尹勋健 Xunjian Yin、杨士平 Shiping
    Yang 和万小军 Xiaojun Wan。使用ChatGPT进行类人总结评估。*arXiv预印本 arXiv:2304.02554*，2023年。
- en: 'Shen et al. [2023] Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and
    Lidong Bing. Large language models are not yet human-level evaluators for abstractive
    summarization. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, pages 4215–4233, 2023.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等[2023] 陈辉 Shen、程丽英 Cheng、阮玄霖 Xuan-Phi Nguyen、游洋 Yang You 和邴立东 Lidong Bing。大型语言模型尚未达到人类水平的抽象总结评估能力。在*计算语言学协会会议：EMNLP
    2023*中，页码4215–4233，2023年。
- en: 'Liu et al. [2023] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment.
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, 2023.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等[2023] 刘洋 Yang Liu、伊特 Dan Iter、徐一琼 Yichong Xu、王硕航 Shuohang Wang、徐若晨 Ruochen
    Xu 和朱成光 Chenguang Zhu。G-eval：使用GPT-4进行更好的人类对齐的NLG评估。在*2023年自然语言处理实证方法会议论文集*中，2023年。
- en: Luo et al. [2023] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. Chatgpt as
    a factual inconsistency evaluator for abstractive text summarization. *arXiv preprint
    arXiv:2303.15621*, 2023.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo等[2023] 罗哲恒 Zheheng Luo、谢倩倩 Qianqian Xie 和安娜尼亚杜 Sophia Ananiadou。将ChatGPT作为抽象文本总结的事实不一致性评估工具。*arXiv预印本
    arXiv:2303.15621*，2023年。
- en: 'Leidinger et al. [2023] Alina Leidinger, Robert Van Rooij, and Ekaterina Shutova.
    The language of prompting: What linguistic properties make a prompt successful?
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*,
    2023.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leidinger等[2023] 阿丽娜·莱丁格 Alina Leidinger、罗伯特·范·鲁伊 Robert Van Rooij 和叶卡特琳娜·舒托娃
    Ekaterina Shutova。提示的语言：什么语言属性使提示成功？在*2023年自然语言处理实证方法会议*中，2023年。
- en: 'Chen et al. [2023] Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and
    Tianyi Zhou. Instructzero: Efficient instruction optimization for black-box large
    language models. *arXiv preprint arXiv:2306.03082*, 2023.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等[2023] 陈立昌 Lichang Chen、陈久海 Jiuhai Chen、汤姆·戈尔德斯坦 Tom Goldstein、黄恒 Heng
    Huang 和周天翼 Tianyi Zhou。Instructzero：黑箱大型语言模型的高效指令优化。*arXiv预印本 arXiv:2306.03082*，2023年。
- en: Yang et al. [2023] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V
    Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. *arXiv preprint
    arXiv:2309.03409*, 2023.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等[2023] 杨成润 Chengrun Yang、王雪之 Xuezhi Wang、陆一峰 Yifeng Lu、刘瀚霄 Hanxiao Liu、李国伟
    Quoc V Le、周丹尼 Denny Zhou 和陈新云 Xinyun Chen。大型语言模型作为优化器。*arXiv预印本 arXiv:2309.03409*，2023年。
- en: 'Zhang et al. [2023] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans,
    and Joseph E. Gonzalez. TEMPERA: Test-time prompt editing via reinforcement learning.
    In *The Eleventh International Conference on Learning Representations*, 2023.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 [2023] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans 和 Joseph
    E. Gonzalez. TEMPERA: 通过强化学习进行测试时的提示编辑。在 *第十一届学习表征国际会议* 中，2023。'
- en: 'Prasad et al. [2023] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal.
    Grips: Gradient-free, edit-based instruction search for prompting large language
    models. In *Proceedings of the 17th Conference of the European Chapter of the
    Association for Computational Linguistics*, pages 3827–3846, 2023.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prasad 等 [2023] Archiki Prasad, Peter Hase, Xiang Zhou 和 Mohit Bansal. Grips:
    无梯度、基于编辑的指令搜索以提示大型语言模型。在 *第 17 届计算语言学协会欧洲分会会议录* 中，第 3827–3846 页，2023。'
- en: 'Fabbri et al. [2021] Alexander R Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. Summeval: Re-evaluating summarization
    evaluation. *Transactions of the Association for Computational Linguistics*, 9:391–409,
    2021.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri 等 [2021] Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming
    Xiong, Richard Socher 和 Dragomir Radev. Summeval: 重新评估摘要评价。*计算语言学协会会刊*，9:391–409,
    2021。'
- en: '[15] Cheng-Han Chiang and Hung-yi Lee. A closer look into using large language
    models for automatic evaluation. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Cheng-Han Chiang 和 Hung-yi Lee. 详细探讨了使用大型语言模型进行自动评估。在 *计算语言学协会发现：EMNLP
    2023* 中。'
- en: Appendix
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Full Special Rules
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 完整特殊规则
- en: 'Table [5](#A1.T5 "Table 5 ‣ Appendix A Full Special Rules ‣ A Better LLM Evaluator
    for Text Generation: The Impact of Prompt Output Sequencing and Optimization")
    presents the full prompt of the special rules section in Fig. [2](#S1.F2 "Figure
    2 ‣ 1\. Introduction ‣ A Better LLM Evaluator for Text Generation: The Impact
    of Prompt Output Sequencing and Optimization"), which was trimmed for better readability.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#A1.T5 "表 5 ‣ 附录 A 完整特殊规则 ‣ 更好的 LLM 评估器用于文本生成：提示输出顺序和优化的影响") 展示了图 [2](#S1.F2
    "图 2 ‣ 1\. 引言 ‣ 更好的 LLM 评估器用于文本生成：提示输出顺序和优化的影响") 中特殊规则部分的完整提示，已为提高可读性而进行了剪裁。
- en: '| ”Unfinished conversation” need not be viewed as an issue. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| “未完成的对话”不需要视为问题。 |'
- en: '| Do note that the scoring should not be an average of the performance in each
    of the three aspects. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 请注意，评分不应是三个方面表现的平均值。 |'
- en: '| Even if the conversations are generally coherent but contain a issue in one
    aspect, the score should lean towards the higher end of the scale. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 即使对话通常连贯但在某一方面存在问题，评分也应倾向于量表的高端。 |'
- en: '| Do not give a small score just because the issue is not very impactful. Consider
    the number of issues rather than its impact. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 不要仅仅因为问题影响不大就给出较低的分数。应考虑问题的数量，而不是其影响。 |'
- en: '| Your overall score should reflect the utmost concern observed in any of the
    aspects. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 你的总体评分应反映出在任何方面观察到的最大关注度。 |'
- en: 'Table 5: Special rules for conversation evaluation.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 对话评估的特殊规则。'
- en: Appendix B Example Dialogue Set
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 示例对话集
- en: 'Table LABEL:tab:dialoguesample lists an example dialogue set we have studied
    in Section [2.](#S2 "2\. Output Instruction in Dialogue Evaluation ‣ A Better
    LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and
    Optimization") The results shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing
    and Optimization") and Fig. [3](#S2.F3 "Figure 3 ‣ Approach ‣ 2\. Output Instruction
    in Dialogue Evaluation ‣ A Better LLM Evaluator for Text Generation: The Impact
    of Prompt Output Sequencing and Optimization") are produced from this set. The
    original data can be found in [[4](#bib.bib4)]. ²²2https://reverie.herokuapp.com/arXiv_Demo/'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:tab:dialoguesample 列出了我们在第 [2](#S2 "2\. 对话评估中的输出指令 ‣ 更好的 LLM 评估器用于文本生成：提示输出顺序和优化的影响")
    节中研究的示例对话集。图 [1](#S1.F1 "图 1 ‣ 1\. 引言 ‣ 更好的 LLM 评估器用于文本生成：提示输出顺序和优化的影响") 和图 [3](#S2.F3
    "图 3 ‣ 方法 ‣ 2\. 对话评估中的输出指令 ‣ 更好的 LLM 评估器用于文本生成：提示输出顺序和优化的影响") 显示的结果均来自该数据集。原始数据可在
    [[4](#bib.bib4)] 中找到。²²2https://reverie.herokuapp.com/arXiv_Demo/
- en: 'Table 6: Example dialogue set for conversation evaluation.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 对话评估的示例对话集。'
- en: '| Time: 07:08 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 时间: 07:08 |'
- en: '| Mei Lin: “Good morning, John. How did you sleep?” |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Mei Lin: “早上好，John。你睡得好吗？” |'
- en: '| John Lin: “Not bad, although I had a lot on my mind. I’m curious about who
    will be running for the local mayor election next month.” |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “还不错，虽然我有很多事情要考虑。我对下个月的地方市长选举感到好奇。” |'
- en: '| Mei Lin: “Oh, that’s interesting. Have you asked anyone about it?” |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Mei Lin: “哦，这很有趣。你问过别人了吗？” |'
- en: '| John Lin: “Yes, I’ve been asking everyone I meet. I’m also taking online
    classes to stay up to date on new medications and treatments.” |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “是的，我问了我遇到的每个人。我还在上网课，以跟上新的药物和治疗方法。” |'
- en: '| Mei Lin: “That’s great. By the way, have you noticed anything strange with
    Eddy lately? He seems a bit rebellious.” |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Mei Lin: “那很好。顺便问一下，你最近有没有注意到艾迪有什么奇怪的行为？他似乎有点叛逆。” |'
- en: '| John Lin: “Yes, I have noticed that. I think we need to keep an eye on him
    and make sure he’s okay.” |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “是的，我注意到了。我认为我们需要留意他，确保他没问题。” |'
- en: '| Mei Lin: “Agreed. And speaking of neighbors, do you know Tamara Taylor who
    lives on the next block?” |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Mei Lin: “同意。说到邻居，你知道住在下一个街区的塔玛拉·泰勒吗？” |'
- en: '| John Lin: “I’ve heard of her, but I haven’t had a conversation with her yet.”
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “我听说过她，但还没有和她交谈过。” |'
- en: '| Mei Lin: “Okay. And what about our elderly neighbors, the Moores? You’ve
    been acquainted with them for a few years, right?” |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Mei Lin: “好的。那我们的年长邻居摩尔一家怎么样？你已经认识他们几年了，对吧？” |'
- en: '| John Lin: “Yes, I really like Jennifer Moore’s art. She’s very talented.”
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “是的，我非常喜欢詹妮弗·摩尔的艺术。她非常有才华。” |'
- en: '| Mei Lin: “That’s nice. Alright then, I’ll see you later. Have a good day
    at work.” |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Mei Lin: “那很好。好吧，我稍后见。祝你工作愉快。” |'
- en: '| John Lin: “Thanks. You too, Mei Lin.” |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “谢谢。你也是，美琳。” |'
- en: '| Time: 07:53 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Time: 07:53 |'
- en: '| John Lin: “Hey Tom, how’s it going?” |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “嗨，汤姆，怎么样？” |'
- en: '| Tom Moreno: “Good morning John, just checking the news before starting my
    day.” |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Tom Moreno: “早安约翰，只是在开始一天前查看新闻。” |'
- en: '| John Lin: “Yeah, I hear you. Have you heard anything about who might be running
    for the local mayor election next month?” |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “是的，我听到了。你有没有听说过下个月谁可能竞选地方市长？” |'
- en: '| Tom Moreno: “Actually, I was just reading an article about that. There are
    a few candidates being considered but I haven’t made up my mind yet.” |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Tom Moreno: “实际上，我刚刚读了一篇关于此事的文章。目前有几个候选人正在考虑中，但我还没有做出决定。” |'
- en: '| John Lin: “Well, if you hear anything else, let me know. By the way, did
    you see the new medications and treatments I’ve been learning about in my online
    classes? There’s some really exciting stuff out there.” |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “好吧，如果你有其他消息，请告诉我。顺便问一下，你看到我在网上课程中学习的新药物和治疗方法了吗？那里有一些非常令人兴奋的东西。”
    |'
- en: '| Tom Moreno: “No, I haven’t had a chance to look into that yet. It sounds
    interesting though. By the way, have you noticed anything strange with your neighbor’s
    rebellious son lately?” |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Tom Moreno: “不，我还没有机会调查这个。听起来很有趣。顺便问一下，你最近有没有注意到你邻居的叛逆儿子有什么奇怪的行为？” |'
- en: '| John Lin: “Actually, I have. I’m planning on keeping an eye on him and making
    sure everything is okay. It’s good to know that we are both keeping an eye out
    for the well being of our community. Oh, and thanks for the extra stock you gave
    me the other day. It really helped out.” |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “实际上，我有。我打算留意他，确保一切正常。知道我们都在关注社区的福祉真好。哦，还有谢谢你前几天给我的额外库存，真的很有帮助。”
    |'
- en: '| Tom Moreno: “No problem, John. I always try to help out where I can. By the
    way, have you seen any of Jennifer Moore’s new artwork? She’s really been stepping
    up her game lately.” |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Tom Moreno: “没问题，约翰。我总是尽力提供帮助。顺便问一下，你看到詹妮弗·摩尔的新艺术作品了吗？她最近真的提高了水平。” |'
- en: '| John Lin: “Yeah, I actually saw her new piece at the gallery the other day.
    It was really impressive.” |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “是的，我实际上前几天在画廊看到她的新作品了。真的很令人印象深刻。” |'
- en: '| Time: 08:07 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Time: 08:07 |'
- en: '| John Lin: “Hi there, Jennifer! I couldn’t help but notice your beautiful
    artwork. I’m really looking forward to seeing your upcoming exhibition. Have you
    finalized the date yet?” |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| John Lin: “嗨，詹妮弗！我不禁注意到你的美丽艺术作品。我非常期待看到你的即将到来的展览。你确定了日期吗？” |'
- en: '| Jennifer Moore: “Thank you, John! I appreciate your kind words. And yes,
    the exhibition is scheduled for next month. I’ve been quite busy preparing for
    it.” |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Jennifer Moore: “谢谢你，约翰！我很感激你的好意。是的，展览定在下个月。我一直忙于准备。” |'
- en: '| John Lin: “That’s great to hear. I remember Tom Moreno mentioning how much
    he enjoyed your last exhibition. By the way, I heard you’ve been mentoring young
    artists. That’s really inspiring. Can you tell me more about it?” |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 约翰·林：“很高兴听到这个。我记得汤姆·莫雷诺提到他非常喜欢你的上次展览。顺便问一下，我听说你一直在指导年轻艺术家。这真的很鼓舞人心。你能多讲讲吗？”
    |'
- en: '| Jennifer Moore: “Yes, I love mentoring younger artists. It’s such a joy to
    see them find their own creative voice. I have a few mentees right now who are
    working on some really interesting projects.” |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 詹妮弗·摩尔：“是的，我喜欢指导年轻艺术家。看到他们找到自己的创造性声音真是一种喜悦。我现在有几个徒弟，他们正在进行一些非常有趣的项目。” |'
- en: '| John Lin: “That’s wonderful. I’m sure they’re lucky to have you as their
    mentor. On another note, have you been following the local politics?” |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 约翰·林：“那太棒了。我相信他们很幸运有你作为他们的导师。另一方面，你有关注本地政治吗？” |'
- en: '| Jennifer Moore: “Actually, I’m not really interested in politics. I prefer
    to focus on my art and my relationships with the people I care about.” |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 詹妮弗·摩尔：“事实上，我对政治并不太感兴趣。我更喜欢专注于我的艺术和我关心的人际关系。” |'
- en: '| John Lin: “I see. That makes sense. Well, it was really nice chatting with
    you, Jennifer. I’ll be sure to mark the date of your exhibition on my calendar.”
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 约翰·林：“我明白了。这很有道理。嗯，很高兴和你聊天，詹妮弗。我会在日历上标记你的展览日期。” |'
- en: '| Jennifer Moore: “Likewise, John. Thank you for stopping by. And please do
    come to my exhibition. I’d love to see you there.” |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 詹妮弗·摩尔：“同样，约翰。感谢你的光临。请务必来我的展览。我很期待见到你。” |'
- en: '| Time: 09:16 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 时间：09:16 |'
- en: '| Giorgio Rossi: “Excuse me, are you John Lin?” |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 乔治·罗西：“对不起，你是约翰·林吗？” |'
- en: '| John Lin: “Yes, that’s me. How may I help you?” |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 约翰·林：“是的，正是我。我可以帮你什么？” |'
- en: '| Giorgio Rossi: “I noticed that you’re very knowledgeable about medications
    and treatments. I’ve been curious about some of the latest developments in that
    field. Would you mind sharing some of your insights with me?” |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 乔治·罗西：“我注意到你对药物和治疗非常了解。我对这个领域的一些最新进展感到好奇。你能分享一些你的见解吗？” |'
- en: '| John Lin: “Of course, I’m happy to share what I know. What specifically are
    you interested in?” |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 约翰·林：“当然，我很高兴分享我所知道的。你具体对哪方面感兴趣？” |'
- en: '| Giorgio Rossi: “Well, I’m particularly intrigued by how some medications
    are being developed based on mathematical patterns found in nature. Do you have
    any knowledge about that?” |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 乔治·罗西：“嗯，我特别对一些药物是如何基于自然界中的数学模式进行开发感兴趣。你对此有什么了解吗？” |'
- en: '| John Lin: “Hmm, that’s not really my area of expertise. But I do know that
    Jennifer Moore, the artist who’s running for local mayor, is also a scientist
    who incorporates mathematical patterns into her work. Maybe she would have more
    information on that.” |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 约翰·林：“嗯，这确实不是我的专长领域。但我知道，正在竞选地方市长的艺术家**詹妮弗·摩尔**也是一位将数学模式融入她作品中的科学家。也许她会有更多信息。”
    |'
