- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.07840](https://ar5iv.labs.arxiv.org/html/2405.07840)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xiaoyu Chen [0000-0001-8945-3150](https://orcid.org/0000-0001-8945-3150 "ORCID
    identifier") Laboratory of Brain Atlas and Brain-Inspired Intelligence, State
    Key Laboratory of Multimodal Artificial Intelligence Systems, CASIASchool of Artificial
    Intelligence, University of Chinese Academy of ScienceBeijingChina [chenxiaoyu2022@ia.ac.cn](mailto:chenxiaoyu2022@ia.ac.cn)
    ,  Changde Du [0000-0002-0084-433X](https://orcid.org/0000-0002-0084-433X "ORCID
    identifier") Laboratory of Brain Atlas and Brain-Inspired Intelligence, State
    Key Laboratory of Multimodal Artificial Intelligence Systems, CASIABeijingChina
    [changde.du@ia.ac.cn](mailto:changde.du@ia.ac.cn) ,  Liu Che Laboratory of Brain
    Atlas and Brain-Inspired Intelligence, State Key Laboratory of Multimodal Artificial
    Intelligence Systems, CASIAUniversity of Chinese Academy of ScienceBeijingChina
    [liuche2022@ia.ac.cn](mailto:liuche2022@ia.ac.cn) ,  Yizhe Wang CASIABeijingChina
    [yizhe.wang@ia.ac.cn](mailto:yizhe.wang@ia.ac.cn)  and  Huiguang He [0000-0002-0684-1711](https://orcid.org/0000-0002-0684-1711
    "ORCID identifier") Laboratory of Brain Atlas and Brain-Inspired Intelligence,
    State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIASchool
    of Artificial Intelligence, University of Chinese Academy of ScienceBeijingChina
    [huiguang.he@ia.ac.cn](mailto:huiguang.he@ia.ac.cn)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Decoding language information from brain signals represents a vital research
    area within brain-computer interfaces, particularly in the context of deciphering
    the semantic information from the fMRI signal. However, many existing efforts
    concentrate on decoding small vocabulary sets, leaving space for the exploration
    of open vocabulary continuous text decoding. In this paper, we introduce a novel
    method, the Brain Prompt GPT (BP-GPT). By using the brain representation that
    is extracted from the fMRI as a prompt, our method can utilize GPT-2 to decode
    fMRI signals into stimulus text. Further, we introduce a text-to-text baseline
    and align the fMRI prompt to the text prompt. By introducing the text-to-text
    baseline, our BP-GPT can extract a more robust brain prompt and promote the decoding
    of pre-trained LLM. We evaluate our BP-GPT on the open-source auditory semantic
    decoding dataset and achieve a significant improvement up to $4.61\%$ on BERTScore
    across all the subjects compared to the state-of-the-art method. The experimental
    results demonstrate that using brain representation as a prompt to further drive
    LLM for auditory neural decoding is feasible and effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do, Not, Us, This, Code, Put, the, Correct, Terms, for, Your, Paper^†^†ccs:
    Do Not Use This Code Generate the Correct Terms for Your Paper^†^†ccs: Do Not
    Use This Code Generate the Correct Terms for Your Paper^†^†ccs: Do Not Use This
    Code Generate the Correct Terms for Your Paper^†^†ccs: Do Not Use This Code Generate
    the Correct Terms for Your Paper'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “The limits of my language mean the limits of my world” - Ludwig Wittgenstein.
    Wittgenstein’s statement refers to the standpoint that a person’s entire understanding
    of the world is reflected in the things they can describe in language. So it is
    important for human-centric artificial intelligence, for example, the brain-computer
    interface, to understand the language information from the human brain.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/32b77cf82613a81eb42f19843fbe9268.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. We focus on decoding semantic information from fMRI in the auditory
    neural decoding scenario and use fMRI signals as prompts to guide a pre-trained
    GPT-2 to achieve decoding.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, due to the rapid development of deep learning and its widespread
    application in brain-computer interfaces, especially in neural encoding and decoding
    (Scotti et al., [2024](#bib.bib27); Lin et al., [2022](#bib.bib13); Du et al.,
    [2023](#bib.bib8); Li et al., [2022](#bib.bib12); Wang et al., [2022](#bib.bib31)),
    significant progress has been made in the research field of extracting language
    information from brain signals. For example, reconstructing audio of auditory
    stimuli from brain signals (Yang et al., [2015](#bib.bib34); Pasley et al., [2012](#bib.bib19);
    Santoro et al., [2017](#bib.bib26); Défossez et al., [2023](#bib.bib5)), or reconstructing
    corresponding text (Affolter et al., [2020](#bib.bib3); Défossez et al., [2023](#bib.bib5);
    Pereira et al., [2018](#bib.bib21); Xi et al., [2023](#bib.bib33); Tang et al.,
    [2023](#bib.bib29)). In this article, we focus on reconstructing language information
    by decoding the original text (see Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM") for an illustration).
    Specifically, we focus on text decoding in auditory neural decoding scenarios.
    For the types of brain-computer interfaces, we focus on decoding text from functional
    magnetic resonance imaging (fMRI), a widely used non-invasive brain-computer interface
    in both research and practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/480a0d1e05aca11dc7f033d2dc7e158a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2\. The training stages of our method. The upper part: we use the BERT
    and GPT-2 for the encoder and decoder of our text-to-text baseline. In this baseline,
    the BERT representation will be mapped into a text prompt which is used for reconstructing
    the original text using GPT-2\. The lower part: we use a transformer fMRI encoder
    to extract the fMRI prompt and add a contrastive loss to align the fMRI prompt
    to the text prompt. Then, the GPT-2 will decode the text according to the fMRI
    prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this goal, two primary challenges must be addressed. Firstly,
    the temporal resolution of fMRI signals presents a significant obstacle. Despite
    its commendable spatial resolution and non-invasive characteristics, fMRI signals
    exhibit significantly low temporal resolution. For instance, in the context of
    commonly spoken English, where the average speaking speed exceeds 2 words per
    second (Tang et al., [2023](#bib.bib29)), the BOLD response to neural activity
    rises and falls over approximately 10 seconds which is extremely slower than the
    speech stimuli (Logothetis, [2003](#bib.bib15)). Secondly, another essential challenge
    is the significant difference between fMRI modality and text modality. In fact,
    in auditory information decoding scenarios, the text does not present as the stimulus
    signal received by the subject. Rather, it contains the semantics of the stimulus
    signal. Therefore, the quality of modal alignment significantly influences the
    effectiveness of the decoding model.
  prefs: []
  type: TYPE_NORMAL
- en: The low temporal resolution in fMRI necessitates our model to decode multiple
    words from a single fMRI signal during the decoding process. To tackle this ill-posed
    inverse problem, we propose a solution by incorporating a language prior through
    a pre-trained Large Language Model (LLM) - Generative Pre-trained Transformer
    2 (GPT-2) (Radford et al., [2019](#bib.bib23)). As illustrated in the bottom part
    of Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ Open-vocabulary Auditory Neural
    Decoding Using fMRI-prompted LLM"), we establish a mapping from fMRI to prompts
    and train GPT-2 to employ autoregressive methods, generating corresponding text
    based on the provided fMRI prompts. By applying the cross-entropy loss to the
    output logit of GPT-2, the fMRI encoder can learn the suitable prompts for the
    target text.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate the substantial modal disparity between the fMRI signal and the
    text data, we introduced another text-to-text baseline during the training stage.
    As depicted in the upper section of Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction
    ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM"), we utilized
    Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al.,
    [2018](#bib.bib7)) as the text encoder to extract the text prompt. Subsequently,
    we reconstructed the text using GPT-2 in a manner similar to the brain-to-text.
    Since the input and output of the text-to-text baseline are identical, eliminating
    modal differences, we designate the text prompt as the optimal prompt for the
    ground-truth text. Consequently, we incorporated a contrastive loss to align the
    fMRI prompt with the text prompt, enhancing decoding performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our main contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose Brain Prompt-GPT (BP-GPT), which is a novel structure that can use
    the fMRI prompt to decode the text of speech stimuli in an end-to-end structure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduced a language prior by a pre-trained LLM (GPT-2) to compensate for
    the low temporal resolution of fMRI; through contrastive learning, we encourage
    fMRI prompts to align with text prompts, thereby reducing the impact of modal
    differences on decoding performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We evaluated our BP-GPT model on an open-source auditory semantic decoding dataset
    and achieved a significant improvement of up to $4.61\%$ on BERTScore across all
    subjects compared to the state-of-the-art method. These results demonstrate the
    feasibility and advantages of our approach.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Large language models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the introduction of the transformer architecture (Vaswani et al., [2017](#bib.bib30)),
    numerous transformer-based Large Language Models (LLMs) have emerged. While all
    these models utilize stacked attention layers to construct their networks, each
    exhibits unique characteristics. Broadly speaking, existing LLMs can be categorized
    into three groups: encoder-only, decoder-only, and encoder-decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder-only LLMs are mostly used to extract features of input text and
    are used for different downstream tasks. The most representative one is the BERT
    (Devlin et al., [2018](#bib.bib7)) model. BERT adopts a bidirectional architecture
    and trains using a masked language model and next-sense prediction task, allowing
    it to extract representations of text based on the previous and following text.
    A popular optimized version of BERT is RoBERTa (Liu et al., [2019](#bib.bib14)),
    which builds upon BERT’s architecture and pre-training objectives, refining the
    training process to achieve improved performance. It removes the next sentence
    prediction objective and trains on more data and for longer epochs, leading to
    better representations.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-only LLMs stem from the decoder component of the transformer architecture.
    Their key feature lies in masking future positions to ensure that predictions
    of the current token are based solely on preceding tokens. The most prominent
    example of decoder-only LLMs is the GPT (Generative Pre-trained Transformer) series
    (Radford et al., [2018](#bib.bib22), [2019](#bib.bib23); Brown et al., [2020](#bib.bib4);
    Achiam et al., [2023](#bib.bib2)), which introduces a large-scale autoregressive
    language model based on the Transformer architecture. These models have demonstrated
    the effectiveness of unsupervised pre-training followed by fine-tuning across
    various natural language processing tasks. Beyond their technical aspects, they
    have significantly advanced natural language understanding and generation tasks,
    including translation, question answering, code generation, and even creative
    writing. Notably, with the emergence of ChatGPT, the GPT series has become the
    most popular choice among LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the aforementioned architectures, there are models that utilize
    the encoder-decoder architecture to leverage the strengths of both components.
    Examples include BART (Lewis et al., [2019](#bib.bib11)) and T5 (Raffel et al.,
    [2020](#bib.bib24)). While both models are based on the encoder-decoder architecture,
    each adopts a unique approach and possesses distinct capabilities for natural
    language processing and generation tasks. T5 emphasizes a text-to-text framework
    for unified processing of various tasks, whereas BART focuses on bidirectional
    and auto-regressive training for sequence-to-sequence tasks such as summarization
    and translation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Decoding the Brain Signals into Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the early stages of text decoding, model outputs often comprised a single
    vocabulary and only supported a limited vocabulary set. For instance, in the Brain2word
    (Affolter et al., [2020](#bib.bib3)), researchers employed a classification model
    to decode participants’ brain signals when they read words. In the study by Defossez
    et al. (Défossez et al., [2023](#bib.bib5)), contrastive learning was applied
    to decode words or phrases from auditory brain signals. Additionally, Pereira
    et al. (Pereira et al., [2018](#bib.bib21)) and Sun et al. (Sun et al., [2019](#bib.bib28))
    explored the decoding of sentences from brain signals in their respective works.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, due to the success of LLMs, some work has begun to attempt
    to use LLMs to decode complete texts from various types of brain data. For example,
    Wang et al. (Wang and Ji, [2022](#bib.bib32)) proposed using BART (Lewis et al.,
    [2019](#bib.bib11)) to decode text from the EEG and eye tracking signals of the
    subjects during reading, achieving the decoding of open vocabularies. In DeWave
    (Duan et al., [2023](#bib.bib9)), Duan et al. proposed using a quantized variant
    encoder and BART to improve the decoding of EEG2text, achieving decoding without
    external event markers like handling or eye tracking. Unlike decoding using EEG,
    in UniCoRN (Xi et al., [2023](#bib.bib33)), Xi et al. also used BART to decode
    fMRI signals into text. Tang et al (Tang et al., [2023](#bib.bib29)). used linear
    regression and GPT, based on the neural encoding architecture, to complete text
    decoding using similarity measurement. Our work, along with UniCoRN and Tang et
    al.’s most relevant, is based on fMRI decoding. Different from UniCoRN which treats
    fMRI as a foreign language and uses machine translation structure for text decoding,
    we adopted a prompt approach. Compared to Tang et al., which adopts a neural encoding
    architecture, our method adopts a more direct neural decoding architecture, which
    is an end-to-end approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a6d5503cde826cf927b6aef4db3c9b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. An illustration of the inference stage is provided here. During this
    stage, the fMRI prompt is considered as the preceding text for the target text
    generation. Subsequently, GPT-2 generates the text in an autoregressive manner,
    relying on both the fMRI prompt and the generated text. For deciding the length
    of decoding text, we compared two strategies in this work. The first one is to
    use the word rate model to predict the length of text; The second one is to use
    special tokens and fine-tune the GPT-2, the decoding process will end when GPT-2
    generates enough special tokens ($ in our implementation).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. A Text to Text Baseline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fundamental concept of our method draws inspiration from the encoding and
    decoding processes applied to text and its corresponding modalities. In the field
    of Natural Language Processing (NLP) or Computer Vision (CV), various tasks demand
    the encoding and decoding of text, such as text-to-image generation (Ramesh et al.,
    [2021](#bib.bib25)) image captioning (Zhou et al., [2020](#bib.bib37); Mokady
    et al., [2021](#bib.bib17)). In this part, we align both the encoding input and
    decoding target with the text corresponding to auditory stimuli. Due to the remarkable
    progress made by LLMs recently (Devlin et al., [2018](#bib.bib7); Radford et al.,
    [2019](#bib.bib23); Brown et al., [2020](#bib.bib4)) and its continued evolution
    at an astonishing speed (Achiam et al., [2023](#bib.bib2)), we have established
    the encoder and decoder of our text-to-text model entirely based on the pre-trained
    LLMs, minimizing the volume of parameters that need to be trained. This design
    facilitates the seamless integration of new LLMs, enabling easy upgrades for our
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We chose BERT as the text encoder and GPT-2 as the text decoder in this work
    and drew inspiration from the recent image caption works (Zhou et al., [2020](#bib.bib37);
    Mokady et al., [2021](#bib.bib17)) which use encoded representation as the prompt
    of target text in the decoding. Specifically, we first encode the text into the
    representation space using the last hidden state of BERT. Then a mapping network
    is applied to map the BERT representation into the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\mathit{P}_{i}^{T}=\mathbf{M}_{\theta}(\mathbf{BERT}(x_{i})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the $\mathbf{M}(\cdot)$ is the extracted prompt sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reconstruct the text, we feed the prompt into GPT-2 to generate the original
    text. The text was generated using the next-token-prediction paradigm and the
    prompt is considered as the preceding text of the target text. In the training,
    the parameters of BERT are fixed. For the GPT-2, fine-tuning is not a mandatory
    option here due to the presence of the mapping network. So only the parameters
    of the mapping network have to be optimized. We use a cross-entropy loss between
    the output of GPT-2 and the tokens of the target text for the optimization of
    the mapping network:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\displaystyle\mathcal{L}_{text}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (3) |  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where the $\mathit{W}=(w_{1},\ldots,w_{j})$ is the length of prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. fMRI to Text Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two main challenges in decoding text from the fMRI signals: (1) the
    low temporal resolution of fMRI and (2) the significant modal difference between
    fMRI and text. In this part, we will introduce our method which could address
    the two challenges above. Our method consists of two essential components. The
    first is the fMRI-prompted text decoding method, which introduces the prompt paradigm
    into fMRI decoding and addresses the problem of low temporal resolution of fMRI.
    The second is aligning the fMRI prompt to the text prompt and therefore reduce
    the impact of modal differences and address the last challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. fMRI-prompted text decoding.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Due to the low temporal resolution of fMRI, we need to decode multiple words
    from each fMRI sample point. We adopted a similar structure as the text-to-text
    baseline to solve this problem. Specifically, we use a fMRI encoder model to encode
    the fMRI into the representation space and use this representation as the prompt
    of the text generation process of GPT-2:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\mathit{P}_{i}^{B}=\mathbf{E}_{\eta}(x_{i}^{B}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the $\mathbf{E}_{\eta}$ denote the fMRI prompt which is extracted by the
    fMRI encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we did not introduce a pre-trained fMRI encoder here, the fMRI-to-text
    part did not require a mapping network. Furthermore, the fMRI encoder can be trained
    using the cross-entropy loss on the output of the GPT-2\. Here, fine-tuning the
    GPT-2 is not a mandatory option either. When we choose not to fine-tune the GPT-2,
    the loss is completely dependent on the parameters of the fMRI encoder $\eta$
    and has the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\displaystyle\mathcal{L}_{brain}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (6) |  |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: However, fine-tuning the GPT-2 may bring performance improvements in downstream
    tasks. Meanwhile, in the text decoding task of this work, the fine-tuning of the
    GPT-2 can bring more specific benefits. We will introduce this specific benefits
    in Section [3.4](#S3.SS4 "3.4\. Inference ‣ 3\. Method ‣ Open-vocabulary Auditory
    Neural Decoding Using fMRI-prompted LLM") and compare these two options in the
    later experimental section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Align with the Optimal Prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the considerable significant modal differences between fMRI and text,
    extracting effective fMRI prompts through the fMRI encoder poses challenges. Conversely,
    the text baseline inherently circumvents this modal difference. So we argue that
    the prompt derived from the text baseline can be deemed as the optimal prompt
    for the text to be generated. To utilize the optimal prompt, we employ contrastive
    learning to align the fMRI prompt with the text prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we set the fMRI prompt and text prompt of the same text as the
    positive pair and calculate the similarity between the positive pair using the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\mathit{S}_{p}=\exp(cos(\mathit{P}^{i}_{B}\cdot\mathit{P}^{i}_{T})/\tau),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the $\tau$ refers to the temperature hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the negative pair, we use the fMRI prompt and text prompt from different
    texts and the fMRI prompt of different texts. The similarity between the negative
    pairs can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'Based on the definition above, the contrastive loss has the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $L_{\mathcal{C}}=-\mathbb{E}\left[\log\frac{S_{p}}{S_{n}}\right].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 3.3\. Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will divide the training process into two stages. In the initial stage,
    the text-to-text baseline is trained for text encoding and decoding. This process
    enables the model to learn to extract the optimal prompt for the target text,
    which is then utilized as the target of contrastive learning in the subsequent
    stage. Then, we train our decoding model in the second stage using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\displaystyle L=L_{brain}+\alpha L_{C},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the $\alpha$ is a hyperparameter for the contrastive loss.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As is shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.2\. Decoding the Brain Signals
    into Text ‣ 2\. Related Work ‣ Open-vocabulary Auditory Neural Decoding Using
    fMRI-prompted LLM"), in the inference stage, we extract the fMRI prompt first
    and generate the text using GPT-2 depending on the fMRI prompt. The text will
    be generated word by word in a next-token-prediction paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: As our focus is on text decoding within an auditory neural decoding scenario,
    the auditory stimuli received by the subjects exclusively consist of words without
    punctuation. This will cause trouble for the model during the inference phase,
    as we cannot determine the end of generation through the stop token (usually,
    periods are used). Although punctuation can be manually added during text annotation
    for audio stimuli, this introduces two concerns. Firstly, the speaker might have
    delivered a spontaneous speech without adhering to the ground-truth speech draft.
    Secondly, since the decoding often uses the fMRI signals within a fixed-length
    window, and the end of this window does not always match the end of the sentence,
    even if we add the punctuation manually, they are very likely unable to indicate
    the end of the generation.
  prefs: []
  type: TYPE_NORMAL
- en: For the reasons outlined above, we chose not to include punctuation in the model
    training process. To find the end of the generation in the inference stage, we
    offer two strategies here, an illustration of these two strategies is shown in
    Figure [3](#S2.F3 "Figure 3 ‣ 2.2\. Decoding the Brain Signals into Text ‣ 2\.
    Related Work ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM").
  prefs: []
  type: TYPE_NORMAL
- en: In the first one, we adopt an approach from recent work (Tang et al., [2023](#bib.bib29)),
    utilizing a word rate model to predict the number of words perceived by participants.
    The text generation process will be stopped when the length of the generated text
    meets the word count predicted by the word rate model.
  prefs: []
  type: TYPE_NORMAL
- en: The second strategy is to use the special token to segment text based on the
    repetition time (TR) of fMRI. In this work, we add $ to the gound-truth text during
    the training, and stop the generation when we meet enough $ in the inference text.
    We also add an equal mark to point out the beginning of the gourd-truth text.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Mapping Network and fMRI Encoder Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core of our model lies in mapping the original fMRI signal or representation
    of the text encoder into the prompt. Given the complexities in these transformations,
    especially for the fMRI-to-text part, we employed the transformer structures (Vaswani
    et al., [2017](#bib.bib30)) to accomplish the mapping of original features to
    the prompt. The transformer’s inherent capability for global attention enables
    the extraction of semantic relationships from the input fMRI sequence, yielding
    a fMRI prompt better suited for the target text.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BLEU-1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| UTS01 | UTS02 | UTS03 | UTS01 | UTS02 | UTS03 | UTS01 | UTS02 | UTS03 |'
  prefs: []
  type: TYPE_TB
- en: '| T2T+WR | 0.1968 | 0.2085 | 0.1862 | 0.1414 | 0.1466 | 0.1266 | 0.8192 | 0.8205
    | 0.8163 |'
  prefs: []
  type: TYPE_TB
- en: '| T2T+WR+fine-tune | 0.2296 | 0.2421 | 0.241 | 0.1692 | 0.1699 | 0.176 | 0.8242
    | 0.8278 | 0.8259 |'
  prefs: []
  type: TYPE_TB
- en: '| T2T+Spe | 0.2189 | 0.2044 | 0.2187 | 0.2032 | 0.204 | 0.2082 | 0.8325 | 0.8328
    | 0.8343 |'
  prefs: []
  type: TYPE_TB
- en: '| T2T+Spe+fine-tune | 0.2621 | 0.2613 | 0.2554 | 0.2627 | 0.2597 | 0.2549 |
    0.8432 | 0.8451 | 0.8417 |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. Text-to-text performance. ’WR’ refers to use word rate model in the
    inference, and ’Spe’ refers to the special tokens. In the results without annotation
    ’fine-tune’, the parameter of GPT-2 is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e0cbc4fb7a4872361e6a238ee7a03d78.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) UTS01
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c50f459da09faefaf5ca9b97d29991d2.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) UTS02
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/498b36185a70a14703c757553ee5d2d3.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) UTS03
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. The cortical flatmaps for the auditory cortex (in red color) of the
    different subjects we used.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1\. Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate our method on an fMRI dataset obtained during a passive natural
    language listening task (LeBel et al., [2023](#bib.bib10)) along with its extended
    data (Tang et al., [2023](#bib.bib29)). This dataset comprises fMRI data from
    8 subjects recorded while they passively listened to naturally spoken English
    stories. The stories were sourced from The Month and New York Times Modern Love
    podcasts. Specifically, the first 3 subjects (UTS01 to UTS03) in the dataset had
    access to stories from both The Month and New York Times Modern Love, expanding
    the total number of stories to 84 for these subjects. For the remaining 5 subjects
    (UTS04 to UTS08), all 27 stories are from The Month. To maintain consistency with
    existing works (LeBel et al., [2023](#bib.bib10); Tang et al., [2023](#bib.bib29)),
    we selected the story ”Where There’s Smoke”, shared by all subjects, as the test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: In the vanilla section of the dataset, the stories were divided into 5 sections,
    each containing 5 stories and an additional test story (”Where There’s Smoke”).
    This configuration resulted in a total duration of stimulating audio exceeding
    6 hours for all subjects. In the extended section of the dataset, the first three
    subjects underwent an additional 10 sessions, increasing the overall dataset duration
    to 81 hours across all subjects. All stories feature a single speaker narrating
    an autobiographical tale without a prepared script. The texts were manually transcribed
    by one listener and automatically aligned to the audio using the Penn Phonetics
    Lab Forced Aligner (P2FA) (Yuan et al., [2008](#bib.bib35)). Consequently, the
    transcriptions lack punctuation marks, and some words may be repeated as the speaker
    is thinking and organizing their language. Additionally, certain sounds, such
    as ”cough,” ”laugh,” ”lip smack,” ”misc noise,” and ”silence,” were annotated.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Implementing Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our work, we choose the first three subjects that have the extended data
    for all of our experiments. For the region of interest (ROI), we use the voxel
    in the auditory cortex for our experiment (see Figure [4](#S3.F4 "Figure 4 ‣ 3.5\.
    Mapping Network and fMRI Encoder Model ‣ 3\. Method ‣ Open-vocabulary Auditory
    Neural Decoding Using fMRI-prompted LLM") for the cortical flat maps). The temperature
    of contrastive loss is set to $\tau=0.1$.
  prefs: []
  type: TYPE_NORMAL
- en: For the mapping network, the BERT representation will be first mapped to a 512-dimensional
    vector before passing forward to the transformer. We use an 8-layer transformer
    here, with 8 attention heads in each layer. The fMRI encoder has the same architecture
    as the mapping network, except for a linear layer for the input.
  prefs: []
  type: TYPE_NORMAL
- en: All the codes are implemented using PyTorch (Paszke et al., [2019](#bib.bib20)),
    and training on an Nvidia A-100 GPU with AdamW optimizer (Loshchilov and Hutter,
    [2017](#bib.bib16)). The batch size is set to 32.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Baseline and Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We compare our method to Tang et al. (Tang et al., [2023](#bib.bib29)), which
    is the state-of-the-art method in the dataset we used. In their method, they use
    a neuron encoding structure, which uses the GPT to generate proposal words and
    encode the proposal words to fMRI to find the most matching word. For a fair comparison,
    we use the same story (”Where There’s Smoke”) in the dataset as the test set and
    divide the entire story into 20-second windows, calculate evaluation metrics within
    each window, and take the average of all windows as the metric score for the entire
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Tang et al. (Tang et al., [2023](#bib.bib29)), we use identical language
    similarity metrics to evaluate our method in several aspects. BLEU (Papineni et al.,
    [2002](#bib.bib18)) indicates the number of individual translated segments that
    appear in the ground-truth text. METEOR (Denkowski and Lavie, [2014](#bib.bib6))
    computes the harmonic mean of unigram precision and recall. And BERTScore (Zhang
    et al., [2019](#bib.bib36)) computes a similarity score for each token in the
    candidate sentence with each token in the reference sentence using contextual
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BLEU-1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| UTS01 | UTS02 | UTS03 | UTS01 | UTS02 | UTS03 | UTS01 | UTS02 | UTS03 |'
  prefs: []
  type: TYPE_TB
- en: '| Tang et al. | 0.2331 | 0.2426 | 0.2470 | 0.1621 | 0.1677 | 0.1703 | 0.8077
    | 0.8104 | 0.8116 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT | 0.2159 | 0.2111 | 0.2113 | 0.2082 | 0.1976 | 0.2034 | 0.832 | 0.8322
    | 0.8332 |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. Compare our method with the existing work. The BP-GPT refers to using
    the special tokens for the inference and fine-tuning the GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Evaluation the Text-to-text Baseline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we treat the text prompt as the optimal prompt for decoding in our method,
    we first evaluate the performance of the text-to-text baseline in our work. We
    consider 4 settings in this part. For the first two settings, we use the word
    rate model in the inference and fine-tune or fix the parameters of GPT-2 in the
    training. For the last two settings, we use the special tokens and also compare
    two options for fine-tuning or fixing the parameters of GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: The experiment results are listed in Table [1](#S3.T1 "Table 1 ‣ 3.5\. Mapping
    Network and fMRI Encoder Model ‣ 3\. Method ‣ Open-vocabulary Auditory Neural
    Decoding Using fMRI-prompted LLM"). From the experimental results, we can find
    that our text-to-text baseline can effectively encode and decode text under various
    experimental settings. This result supports our further application of this prompt
    paradigm in fMRI-to-text decoding. Also, we find that fine-tuning the GPT-2 in
    the training can always bring improvements in performance, regardless of the inference
    strategy we used. For the inference strategies, rather than using a word rate
    model to infer the text length, we find that adding special tokens in the ground-truth
    text in the training stage can effectively improve the decoding performance. Moreover,
    we find that fine-tuning the GPT-2 for the special tokens can bring a significant
    improvement in the results. Specifically, this setting can bring up to $5.69\%$
    on BERTScore among all the subjects. Based on these experiment results, we choose
    the setting that uses special tokens and fine-tune the GPT-2 as the text-to-text
    baseline for all the following experiments which use the special tokens as inference
    strategies. Also, the text-to-text baseline for the word rate model strategies
    in the following experiments is also fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Evaluation of fMRI to Text Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part, we evaluate the decoding performance of our model. To align with
    existing work (Tang et al., [2023](#bib.bib29)), we report the performance of
    the first three subjects: UTS01, UTS02, and UTS03, who have experienced all 84
    stories in the experiment. For the setting of our method, we add special tokens
    in the ground-truth text for the training. The GPT-2 is fine-tuned in the training,
    and the inference is stopped when the GPT-2 generates enough $. We refer to this
    setting as the BP-GPT in all the experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: As is shown in Table [2](#S4.T2 "Table 2 ‣ 4.3\. Baseline and Evaluation Metrics
    ‣ 4\. Experiment ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted
    LLM"), our method achieves comparable or even better performance. Specifically,
    on the METEOR, our method can achieve an improvement ranging from $2.99\%$ on
    all the subjects.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6\. Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this part, we make ablation studies that evaluate the contributions of contrastive
    learning and different inference strategies for the performance. We report these
    experiment results in Table [3](#S4.T3 "Table 3 ‣ 4.6\. Ablation Study ‣ 4\. Experiment
    ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM") and Table
    [4](#S4.T4 "Table 4 ‣ 4.6\. Ablation Study ‣ 4\. Experiment ‣ Open-vocabulary
    Auditory Neural Decoding Using fMRI-prompted LLM"). For the inference strategies,
    we mark ’WR’ for the results that use the word rate model to distinguish it from
    the results that use the special tokens in the inference.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BLEU-1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| UTS01 | UTS02 | UTS03 | UTS01 | UTS02 | UTS03 | UTS01 | UTS02 | UTS03 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT-wo-contras (WR) | 0.1855 | 0.1851 | 0.1979 | 0.1372 | 0.1358 | 0.1442
    | 0.8134 | 0.8131 | 0.8172 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT (WR) | 0.2052 | 0.1944 | 0.2017 | 0.1451 | 0.1476 | 0.1497 | 0.8192
    | 0.8164 | 0.8198 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT-wo-contras | 0.2027 | 0.2041 | 0.2043 | 0.1943 | 0.1958 | 0.1946 |
    0.829 | 0.8278 | 0.8281 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT | 0.2159 | 0.2111 | 0.2113 | 0.2082 | 0.1976 | 0.2034 | 0.832 | 0.8322
    | 0.8332 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3\. Abalation study: contrastive learning. Experiments that use the word
    rate model in the inference are marked with ’WR’.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BLEU-1$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| UTS01 | UTS02 | UTS03 | UTS01 | UTS02 | UTS03 | UTS01 | UTS02 | UTS03 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT-wo-fine-tune (WR) | 0.198 | 0.1936 | 0.1997 | 0.1343 | 0.1409 | 0.139
    | 0.8157 | 0.8145 | 0.8129 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT (WR) | 0.2052 | 0.1944 | 0.2017 | 0.1451 | 0.1476 | 0.1497 | 0.8192
    | 0.8164 | 0.8198 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT-wo-fine-tune | 0.2083 | 0.1949 | 0.2065 | 0.206 | 0.1924 | 0.2022
    | 0.8318 | 0.8295 | 0.8298 |'
  prefs: []
  type: TYPE_TB
- en: '| BP-GPT | 0.2159 | 0.2111 | 0.2113 | 0.2082 | 0.1976 | 0.2034 | 0.832 | 0.8322
    | 0.8332 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4\. Abalation study: inference strategy. Experiments that use the word
    rate model in the inference are marked with ’WR’.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ed99062ec201dfe1a70af6c87843c44.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) BLEU-1 and METEOR
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0ce407f96eeab5a6e091ca713e5ec36.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) BERTScore
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5\. The performance under different prompt length.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1\. Contrastive Learning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To demonstrate the effectiveness of contrastive learning, we compare the performance
    of our method with or without contrastive learning. We include the experiment
    results on both inference strategies since the challenge of the significant modal
    difference between the fMRI and text exists both in these settings. We want to
    explore through these experiments whether aligning the fMRI prompt with the text
    prompt can bring performance improvements in various inference strategies. Here,
    different inference strategies will lead to different text-to-text baselines.
    As the target of the conservative learning, the text prompt is extracted using
    the text-to-text baseline that has the same inference strategy as the fMRI-to-text
    model. We would like to refer the Section [4.4](#S4.SS4 "4.4\. Evaluation the
    Text-to-text Baseline ‣ 4\. Experiment ‣ Open-vocabulary Auditory Neural Decoding
    Using fMRI-prompted LLM") for more details.
  prefs: []
  type: TYPE_NORMAL
- en: We report the experiment results in Table [3](#S4.T3 "Table 3 ‣ 4.6\. Ablation
    Study ‣ 4\. Experiment ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted
    LLM"). By comparing Table [3](#S4.T3 "Table 3 ‣ 4.6\. Ablation Study ‣ 4\. Experiment
    ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM") with Table
    [1](#S3.T1 "Table 1 ‣ 3.5\. Mapping Network and fMRI Encoder Model ‣ 3\. Method
    ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM"), we can find
    that there is a big gap between the performance of fMRI-to-text decoding and the
    performance of the text-to-text decoding, indicating the modal differences between
    text and fMRI impact the decoding performance seriously. Also, through the results,
    we find that aligning the fMRI prompt with the text prompt always brings performance
    improvements, no matter what inference strategies have been chosen. Specifically,
    when using a word rate model at the inference stage, aligning the fMRI prompt
    to the text prompt can bring an improvement up to $1.97\%$ on BERTScore. This
    result proves that our approach of aligning fMRI prompts with text prompts is
    feasible and effective.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2\. Inference Strategy.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the characteristics of decoding tasks in auditory decoding scenarios,
    the choice inference strategy has become particularly important. We further take
    the ablation study on it. Specifically, we compared four experimental settings,
    including the performance of two inference schemes with a fine-tuned and not fine-tuned
    GPT-2\. The result is reported in Table [4](#S4.T4 "Table 4 ‣ 4.6\. Ablation Study
    ‣ 4\. Experiment ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted
    LLM"). Also, same as in Section [4.6.1](#S4.SS6.SSS1 "4.6.1\. Contrastive Learning.
    ‣ 4.6\. Ablation Study ‣ 4\. Experiment ‣ Open-vocabulary Auditory Neural Decoding
    Using fMRI-prompted LLM"), the aligning target is the corresponding text prompt
    under the same inference strategy.
  prefs: []
  type: TYPE_NORMAL
- en: As is shown in Table [4](#S4.T4 "Table 4 ‣ 4.6\. Ablation Study ‣ 4\. Experiment
    ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted LLM"), we find
    that using special tokens to indicate the end of decoding can always bring a performance
    improvement. Also, fine-tuning the GPT-2 can bring more improvement with the special
    tokens. We believe that fine-tuning the GPT-2 in the training can make the parameters
    of both the fMRI encoder and GPT-2 adapt to the special token. However, if fine-tuning
    is not performed, only the fMRI encoder will learn to adjust the fMRI prompt to
    enable GPT-2 to output $ at the end of each text fragment that corresponds to
    a fMRI TR, bringing a negative influence on the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.7\. Prompt Length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our method, fMRI drives GPT-2 to generate decoding targets through the prompt.
    Therefore, the length of the prompt is crucial for the decoding performance. In
    this part, we investigated the relationship between prompt length and decoding
    performance and reported the result in Figure [5](#S4.F5 "Figure 5 ‣ 4.6\. Ablation
    Study ‣ 4\. Experiment ‣ Open-vocabulary Auditory Neural Decoding Using fMRI-prompted
    LLM"). We use our BP-GPT setting, that is use special tokens in inference with
    a fine-tuned GPT-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: As is shown in the figure, the decoding performance increases with the length
    of the prompt. Although a longer prompt length can bring better results, it also
    incurs greater hardware costs, whether it is for the mapping network or the fMRI
    encoder. Due to GPU memory limitations, the maximum prompt limit during our experiment
    was 30\. However, based on the experimental results, we expect a better performance
    with a longer prompt length.
  prefs: []
  type: TYPE_NORMAL
- en: 4.8\. Conclution and Future Works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this work, we propose a decoding method capable of extracting text from fMRI
    signals within the auditory neural decoding scenario. The central concept of our
    method involves employing an fMRI-prompted Large Language Model (LLM) for decoding.
    Specifically, we utilize an fMRI encoder to extract fMRI representations, which
    serve as prompts for the pre-trained GPT-2 model. Through the application of cross-entropy
    loss, our fMRI encoder learns the appropriate prompt for GPT-2 to generate the
    target text. In order to reduce modal differences between the text and fMRI, we
    introduce a contrastive loss to align the fMRI prompt with the text prompt. Experimental
    results demonstrate the effectiveness and advantage of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our prompt-based LLM decoding method offers ease of implementation
    and extends to various text-based neural decoding tasks. Furthermore, with the
    ongoing advancement of LLMs, our method remains readily compatible with updated
    and superior LLMs, facilitating performance improvements effortlessly. Moving
    forward, our focus will be on applying our prompted LLM decoding paradigm to a
    broader range of neural decoding fields and integrating it with additional LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Affolter et al. (2020) Nicolas Affolter, Beni Egressy, Damian Pascual, and
    Roger Wattenhofer. 2020. Brain2word: decoding brain activity for language generation.
    *arXiv preprint arXiv:2009.04765* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Défossez et al. (2023) Alexandre Défossez, Charlotte Caucheteux, Jérémy Rapin,
    Ori Kabeli, and Jean-Rémi King. 2023. Decoding speech perception from non-invasive
    brain recordings. *Nature Machine Intelligence* 5, 10 (2023), 1097–1107.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denkowski and Lavie (2014) Michael Denkowski and Alon Lavie. 2014. Meteor universal:
    Language specific translation evaluation for any target language. In *Proceedings
    of the ninth workshop on statistical machine translation*. 376–380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2023) Changde Du, Kaicheng Fu, Jinpeng Li, and Huiguang He. 2023.
    Decoding visual neural representations by multimodal learning of brain-visual-linguistic
    features. *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. (2023) Yiqun Duan, Jinzhao Zhou, Zhen Wang, Yu-Kai Wang, and Chin-Teng
    Lin. 2023. Dewave: Discrete eeg waves encoding for brain dynamics to text translation.
    *arXiv preprint arXiv:2309.14030* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeBel et al. (2023) Amanda LeBel, Lauren Wagner, Shailee Jain, Aneesh Adhikari-Desai,
    Bhavin Gupta, Allyson Morgenthal, Jerry Tang, Lixiang Xu, and Alexander G Huth.
    2023. A natural language fMRI dataset for voxelwise encoding models. *Scientific
    Data* 10, 1 (2023), 555.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Rui Li, Yiting Wang, Wei-Long Zheng, and Bao-Liang Lu. 2022.
    A multi-view spectral-spatial-temporal masked autoencoder for decoding emotions
    with self-supervised learning. In *Proceedings of the 30th ACM International Conference
    on Multimedia*. 6–14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2022) Sikun Lin, Thomas Sprague, and Ambuj K Singh. 2022. Mind
    reader: Reconstructing complex images from brain activities. *Advances in Neural
    Information Processing Systems* 35 (2022), 29624–29636.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logothetis (2003) Nikos K Logothetis. 2003. The underpinnings of the BOLD functional
    magnetic resonance imaging signal. *Journal of Neuroscience* 23, 10 (2003), 3963–3971.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mokady et al. (2021) Ron Mokady, Amir Hertz, and Amit H Bermano. 2021. Clipcap:
    Clip prefix for image captioning. *arXiv preprint arXiv:2111.09734* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*.
    311–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pasley et al. (2012) Brian N Pasley, Stephen V David, Nima Mesgarani, Adeen
    Flinker, Shihab A Shamma, Nathan E Crone, Robert T Knight, and Edward F Chang.
    2012. Reconstructing speech from human auditory cortex. *PLoS biology* 10, 1 (2012),
    e1001251.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *Advances in neural information processing systems* 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pereira et al. (2018) Francisco Pereira, Bin Lou, Brianna Pritchett, Samuel
    Ritter, Samuel J Gershman, Nancy Kanwisher, Matthew Botvinick, and Evelina Fedorenko.
    2018. Toward a universal decoder of linguistic meaning from brain activation.
    *Nature communications* 9, 1 (2018), 963.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training. (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog* 1, 8 (2019), 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research* 21, 140 (2020), 1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image
    generation. In *International conference on machine learning*. Pmlr, 8821–8831.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santoro et al. (2017) Roberta Santoro, Michelle Moerel, Federico De Martino,
    Giancarlo Valente, Kamil Ugurbil, Essa Yacoub, and Elia Formisano. 2017. Reconstructing
    the spectrotemporal modulations of real-life sounds from fMRI response patterns.
    *Proceedings of the National Academy of Sciences* 114, 18 (2017), 4799–4804.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scotti et al. (2024) Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin,
    Alex Nguyen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg,
    Kenneth Norman, et al. 2024. Reconstructing the mind’s eye: fMRI-to-image with
    contrastive learning and diffusion priors. *Advances in Neural Information Processing
    Systems* 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Jingyuan Sun, Shaonan Wang, Jiajun Zhang, and Chengqing Zong.
    2019. Towards sentence-level brain decoding with distributed representations.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 33. 7047–7054.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2023) Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander G Huth.
    2023. Semantic reconstruction of continuous language from non-invasive brain recordings.
    *Nature Neuroscience* (2023), 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Yixin Wang, Shuang Qiu, Dan Li, Changde Du, Bao-Liang Lu,
    and Huiguang He. 2022. Multi-modal domain adaptation variational autoencoder for
    EEG-based emotion recognition. *IEEE/CAA Journal of Automatica Sinica* 9, 9 (2022),
    1612–1626.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Ji (2022) Zhenhailong Wang and Heng Ji. 2022. Open vocabulary electroencephalography-to-text
    decoding and zero-shot sentiment classification. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 36\. 5350–5358.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. (2023) Nuwa Xi, Sendong Zhao, Haochun Wang, Chi Liu, Bing Qin, and
    Ting Liu. 2023. UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive
    signals and human language. *arXiv preprint arXiv:2307.05355* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2015) Minda Yang, Sameer A Sheth, Catherine A Schevon, Guy M Mckhann
    Ii, and Nima Mesgarani. 2015. Speech reconstruction from human auditory cortex
    with deep neural networks. In *Sixteenth Annual Conference of the International
    Speech Communication Association*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2008) Jiahong Yuan, Mark Liberman, et al. 2008. Speaker identification
    on the SCOTUS corpus. *Journal of the Acoustical Society of America* 123, 5 (2008),
    3878.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. *arXiv
    preprint arXiv:1904.09675* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020) Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason
    Corso, and Jianfeng Gao. 2020. Unified vision-language pre-training for image
    captioning and vqa. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 34. 13041–13049.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
