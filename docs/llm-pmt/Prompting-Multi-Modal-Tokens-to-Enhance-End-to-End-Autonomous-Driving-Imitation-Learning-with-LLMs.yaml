- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:03'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation
    Learning with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04869](https://ar5iv.labs.arxiv.org/html/2404.04869)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yiqun Duan¹^,^∗, Qiang Zhang²^,^∗, Renjing Xu²^,^† ¹Yiqun Duan is with the HAI
    Centre, Australia Artificial Intelligence Institute, School of Computer Science,
    University of Technology Sydney, 2007, Ultimo, Australia yiqun.duan@student.uts.edu.au²
    Qiang Zhang and Renjing Xu are with the School of Computer Science at The Hong
    Kong University of Science and Technology (Guangzhou) qzhang749@connect.hkust-gz.edu.cn,
    renjingxu@hkust-gz.edu.cn^∗ are equal contributors, ${\dagger}$ is the corresponding
    author
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The utilization of Large Language Models (LLMs) within the realm of reinforcement
    learning, particularly as planners, has garnered a significant degree of attention
    in recent scholarly literature. However, a substantial proportion of existing
    research predominantly focuses on planning models for robotics that transmute
    the outputs derived from perception models into linguistic forms, thus adopting
    a ‘pure-language’ strategy. In this research, we propose a hybrid End-to-End learning
    framework for autonomous driving by combining basic driving imitation learning
    with LLMs based on multi-modality prompt tokens. Instead of simply converting
    perception results from the separated train model into pure language input, our
    novelty lies in two aspects. 1) The end-to-end integration of visual and LiDAR
    sensory input into learnable multi-modality tokens, thereby intrinsically alleviating
    description bias by separated pre-trained perception models. 2) Instead of directly
    letting LLMs drive, this paper explores a hybrid setting of letting LLMs help
    the driving model correct mistakes and complicated scenarios. The results of our
    experiments suggest that the proposed methodology can attain driving scores of
    $49.21\%$ in the offline evaluation conducted via CARLA. These performance metrics
    are comparable to the most advanced driving models.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recent advancements in AI systems for autonomous driving can be classified
    into two main categories: Pipeline Formation and End-to-End (E2E) Formation. Pipeline
    formation [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)], involves manually decomposing
    the drive into sequential modules. These modules consist of tasks such as localization
    [[9](#bib.bib9), [10](#bib.bib10)], scene reconstruction [[11](#bib.bib11), [12](#bib.bib12)],
    planning [[13](#bib.bib13), [14](#bib.bib14)], and control [[15](#bib.bib15),
    [16](#bib.bib16)] according to certain rules. However, these separately learned
    rules struggle to cover long-tail scenarios, which are crucial in driving situations.
    Thus, researchers are aiming to make the system learn using an end-to-end formation
    through reinforcement or imitation learning, to better mimic human-like decision-making
    and bring it a step closer to handling these important scenarios effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/214c3637d3837952218e22cdb290fe21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of utilizing language model for autonomous driving. The
    system takes the camera and LiDAR input and extracts shallow feature maps with
    two branches. Then a joint Swin transformer encodes perception input into joint
    token representation. The prompt is constructed by sequentially concatenating
    multi-modality joint tokens, status repeat tokens, and driving task tokens. The
    task prompts consist of two kinds 1) directly predicting the driving actions and
    2) driving action correction given the driving output. The controller model also
    predicts an uncertainty score by an MLP layer, which decides whether to ask GPT
    to correction or drive by itself. The model is trained by auto-regressively predicting
    perception description and the driving action. Driving actions are executed by
    a final controller.'
  prefs: []
  type: TYPE_NORMAL
- en: E2E driving approaches [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21)] employ state-to-action imitation or reinforcement
    learning strategies on intermediate feature representation states. This enables
    agents to behave appropriately in various driving contexts. The introduction of
    multi-modal sensory fusion techniques [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25)] have significantly improved the perception state representation.
    These techniques make use of both visual and LiDAR data [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [24](#bib.bib24), [25](#bib.bib25), [29](#bib.bib29)], presenting
    it in a Bird’s Eye View (BEV) format or as a flat tensor for the decoding process.
    ST-P3 [[17](#bib.bib17)] approach and Transfuser [[22](#bib.bib22)] reach SOTA
    performance with the multi-modality imitation learning.
  prefs: []
  type: TYPE_NORMAL
- en: However, these methods emphasize on state representation and pay less attention
    to the decoder. Generally, the learning system predicts waypoints for the next
    n time steps based on the encoded feature, with an additional controller translating
    waypoints into actual controls. However, merely predicting waypoints falls short
    in the face of the complex demands of autonomous driving. This limitation hinders
    the practical application of end-to-end driving systems. Interfuser [[30](#bib.bib30)]
    and TCP [[31](#bib.bib31)] respectively improve the decoder by introducing safety
    constraints on the controller and directly adding a branch to predict safety control
    signals. UniAD [[32](#bib.bib32)] try to unify pipelines with an end-to-end training
    framework and reach SOTA performance by introducing more immediate supervision
    such as occupancy and lanes. Especially, ThinkTwice [[33](#bib.bib33)] proposes
    to involve the decoder in an anthropomorphized approach that enables interaction
    between the encoder and decoder following ‘driving after thinking twice’.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s hard to manually list all possible driving rules or components
    comprehensively. In that case, letting the autonomous system learn semantically
    could benefit the end-to-end driving tasks. ADAPT [[34](#bib.bib34)] has given
    a first exploration of modeling the driving task as an image captioning model,
    which simultaneously predicts driving actions and driving explanation as a caption
    using visual transformers. More recent works have investigated using Large Language
    Models (LLMs) for wider autonomous/robotic systems. PaLM-E [[35](#bib.bib35)]
    utilized pre-trained LLM to complete multiple embodied tasks, including sequential
    robotic manipulation planning, visual question answering, and captioning. Mini-GPT4 [[36](#bib.bib36)]
    and LLaVA [[37](#bib.bib37)] introduce practical approaches for visual context
    tuning for LLMs. However, end-to-end autonomous driving has not yet been adequately
    explored. Moreover, these methods concentrate on the language model in an auto-regressive
    way. How to incorporate imitation learning and reinforcement learning in autonomous
    driving scenarios remain uninvestigated.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose to model end-to-end autonomous driving as a multi-modality
    language model. For state encoding, we propose a two-branch framework to encode
    both visual and LiDAR features into joint feature token representations. We prompt
    the multi-modality visual tokens into driving languages, which contain driving
    descriptions followed by driving actions, not only waypoints but also control
    signals. The driving-like language formation could help the model learn comprehensive
    driving skills not merely waypoints, not merely control signals, but also the
    reasoning before taking the action. Despite the auto-regression loss, we design
    a subtle mechanism to incorporate a language model with driving metrics reward-guided
    reinforcement tuning. Considering the randomness of the large language generation
    model, 1) the generated results may exceed the normal safety threshold or do not
    contain waypoints or driving actions. 2) the generated waypoints and the control
    signal may conflict with each other. To enhance driving safety, we design a mechanism
    of re-quering the driving decision with enhanced prompt to enable model to rethink
    the driving decision.
  prefs: []
  type: TYPE_NORMAL
- en: We have performed comprehensive experiments using the off-board driving simulator
    CARLA [[38](#bib.bib38)], a platform capable of providing the requisite driving
    information necessary for generating prompted language supervision. The findings
    of our experiments suggest that LLMs can attain a level of driving performance
    comparable to the current state-of-the-art models in this domain. The incorporation
    of language models into driving systems with reinforcement learning enhancement
    offers clear advantages. When driving actions are prompted through language, it
    enables model learning that extends beyond the mere prediction of waypoints. It
    also allows for the extraction of the underlying driving logic that is inherently
    contained within the language itself. This methodology serves as a step towards
    the development of a more personalized driving model, thereby bringing us closer
    to the goal of a more human-like autonomous driving system. The contribution lies
    threefold.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A novel framework incorporates multi-modality perception as joint token representations
    into language prompts for end-to-end autonomous driving.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple yet effective prompting approach on unified observation, current states,
    trajectory, and control actions in continuous prompts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating reward-guided reinforcement learning supervision on language prompts
    in autonomous driving scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imitation Learning for End-to-End Driving: The evolution of End-to-End autonomous
    driving has typically followed a destructured, then structured trajectory. Early
    explorations, such as ALVINN [[39](#bib.bib39)], and DAVE [[40](#bib.bib40)],
    modeled a simple projection relationship between the input from a single-view
    camera and the steering wheel or accelerator angle. The prevalent approach at
    this stage was to eliminate human prior knowledge, thereby offering the network
    a more direct learning target to improve the fitting capability for fully supervised
    behavior cloning. However, sensory input was limited in this early era, leading
    to the introduction of multiple imitation learning techniques [[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)] to enhance the data quality for behavior cloning.
    Subsequent developments saw the expansion of camera sensor usage [[44](#bib.bib44)]
    from a single view to multiple views to enhance planning stability. In more recent
    works, the reintroduction of prior knowledge has been explored to augment driving
    performance. WOR [[45](#bib.bib45)] and MARL[[46](#bib.bib46)] suggest that incorporating
    prior trained knowledge from related vision tasks such as detection [[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49)] could enhance model performance. NEAT [[20](#bib.bib20)]
    introduced an attention module between different camera views to boost feature
    quality. ST-P3 [[17](#bib.bib17)], VAD [[50](#bib.bib50)], and UniAD [[32](#bib.bib32)]
    amalgamate human prior module design from pipeline methods to increase driving
    rationality.'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning for LLMs: For language models, relying solely on supervised
    learning to generate accurate responses is inadequate as it’s difficult to determine
    whether the model truly understands the answer to a question. The RL framework
    motivates a model to produce correct answers in a natural manner without resorting
    to fabrication because the model receives poor scores for incorrect answers. Reinforcement
    Learning from Human Feedback (RLHF) [[51](#bib.bib51)] is a critical technology
    in the current wave of large language models (LLMs). Three of OpenAI’s most prominent
    LLMs, namely InstructGPT [[52](#bib.bib52)], ChatGPT, and GPT4, rely on RLHF as
    their core technology. Although PaLM-E integrated the 540 billion-parameter PaLM
    [[53](#bib.bib53)] LLM and the 22 billion-parameter Vision Transformer [[54](#bib.bib54)]
    to create an embodied multimodal language model, its approach on the autonomous
    driving task lacked sufficient detail. Haomo’s DriveGPT ¹¹1[https://drivegpt.haomoai.com/](https://drivegpt.haomoai.com/)
    for the autonomous driving scenario proposes to convert BEV information into drive
    language and learning language tokens. However, it is not solely end-to-end as
    it does not learn directly with raw tokens from sensors such as cameras and LiDAR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Modality Fusion For Driving Tasks: The limited capabilities of a monotonous
    camera sensor have led to the exploration of other sensory modalities, such as
    LiDAR, Radar, and Openmap, to enhance driving stability [[55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58)]. Most previous approaches to modality fusion
    have been specifically designed for certain perception tasks such as 3D object
    detection [[24](#bib.bib24), [25](#bib.bib25)], depth estimation [[29](#bib.bib29),
    [59](#bib.bib59)], and instance motion forecasting [[60](#bib.bib60), [61](#bib.bib61)].
    BEVFusion [[25](#bib.bib25)], for instance, fuses LiDAR with image features by
    using completely independent encoders with geometric projection into the same
    Bird’s-eye-view (BEV) feature space at a late stage. However, Transfuser [[22](#bib.bib22)]
    observed that pure geometrical fusion representation impairs the performance of
    comprehensive and complex urban E2E autonomous driving.'
  prefs: []
  type: TYPE_NORMAL
- en: III Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Multi-Modality Joint Token Encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to provide a more suitable driving context for GPT-based models, we
    propose to fuse Visual-LiDAR perception input into a joint token representation.
    The architecture is described in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣
    Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation
    Learning with LLMs"), which contains two stages, early fusion and late fusion.
    This two-stage network is designed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Early Fusion: In the first stage, two distinct CNN branches are utilized to
    extract shallow features from a monotonic image and LiDAR inputs, respectively.
    The image branch concatenates three front view camera inputs, each with a field
    of view of 60 degrees, into a single monotonic view, subsequently reshaped to
    the shape $3\times 160\times 704$. As these lower-level features retain strong
    geometric relations, the independent encoder can extract tight local feature representation
    with fewer distractions. During this stage, we apply cross-modality self-attention
    to enhance geometrical feature fusion.'
  prefs: []
  type: TYPE_NORMAL
- en: The feature map $\mathbf{F}_{im}$ dimension as defined in Eq. [1](#S3.E1 "In
    III-A Multi-Modality Joint Token Encoder ‣ III Methodology ‣ Prompting Multi-Modal
    Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{Q}_{F}=$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{F}_{Li}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{A}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\rm{m_{1}}$ denote two different MLP layers that project the joint attention
    into proper shape and are directly added to the original feature map after normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Late Fusion: After low-level feature extraction with attention, we intend to
    align features from both modalities into a unified semantic token space, where
    we treat each $16\times 16\times C$ feature segment to be a semantic word as shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Prompting Multi-Modal Tokens to
    Enhance End-to-End Autonomous Driving Imitation Learning with LLMs"). To maintain
    the spatial relationships across these tokens, we use the cross-attention mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/16e5ef22e5f51013d294cd96c52e0746.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Illustration of joint token representation, the perception token
    is aligned with segmentation embedding and position embedding to distinguish from
    normal word token.'
  prefs: []
  type: TYPE_NORMAL
- en: A segment embedding is added with positional embedding $\mathbf{PE}+\mathbf{SE}$.
    As the late fusion is performed by share transformer encoding, this could introduce
    a better joint semantic feature representation between especially for the language
    model.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Prompt Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the endeavor to integrate Large Language Models (LLMs) within the domain
    of autonomous driving, a salient challenge lies in the effective design of language
    prompts and the translation of supervisory signals into language constructs. A
    key factor in this process entails the creation of prompts that adeptly incorporate
    multimodal tokens, while also considering the vehicle’s instantaneous state, the
    ambient environment, and short-term as well as long-term objectives. Instead of
    letting the LLMs directly drive, the basic design logic is to let LLMs help the
    basic driving model correct driving actions. This setting alleviates the LLMs
    generating actions or goals not exist in real scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d4a9a023f73c1071910b60ea98aec39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of the prompt construction. Both traditional perception
    supervision and driving actions are converted into language descriptions through
    a descriptor.'
  prefs: []
  type: TYPE_NORMAL
- en: The given information during driving is first contacted sequentially, 1) multi-modality
    tokens 2) self-status of the car 3) driving task command. Here, the self-status
    of the car denotes the current speed, throttle, brake, and current position and
    driving task commands denote the instruction (perform driving/solve drive conflicts).
    After that, supervisions are appended sequentially as language guidance considering
    two aspects 1) perception description and 2) driving action as shown in Fig. [3](#S3.F3
    "Figure 3 ‣ III-B Prompt Construction ‣ III Methodology ‣ Prompting Multi-Modal
    Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs").
    Perception description guides the language model to predict perception results
    which are mostly used as auxiliary heads supervision in previous works [[22](#bib.bib22),
    [63](#bib.bib63), [50](#bib.bib50), [32](#bib.bib32)], such as segmentation, depth
    prediction, BEV prediction, and detection. Driving action predicts both the waypoints
    in the next 4 time steps and control actions. A notable advantage of auto-regressive
    models over their traditional counterparts is their inherent ability to learn
    driving logic, facilitated by their intrinsic capacity to assimilate the complex,
    underlying linguistic logic. Towards this end, we have proposed a comprehensive
    prompt design strategy, which is depicted above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tasks: The task prompts are designed to have three modes. 1) Given the multi-modality
    tokens directly generate perception observation output, and driving actions. Normally
    the driving language output is not used for driving considering the driving cost,
    yet the normal learning parallel with the driving model is crucial for establishing
    driving logic. 2) Re-query the LLM if the driving modal’s language output contradicts
    to the safety controller. 3) Given the multi-modality tokens and driving output,
    correct the driving error by LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Re-Query Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A remarkable problem of auto-regressive models hiders the performance is the
    uncertainty of the language prediction. Since in Section [III-B](#S3.SS2 "III-B
    Prompt Construction ‣ III Methodology ‣ Prompting Multi-Modal Tokens to Enhance
    End-to-End Autonomous Driving Imitation Learning with LLMs"), we propose to predict
    comprehensive driving actions and waypoints together in sequential order, it is
    observed in our experiment that the waypoints prediction might be conflicted with
    the control actions as illustrated in Fig. [4](#S3.F4 "Figure 4 ‣ III-C Re-Query
    Mechanism ‣ III Methodology ‣ Prompting Multi-Modal Tokens to Enhance End-to-End
    Autonomous Driving Imitation Learning with LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b6f0bd6108227935248a15c140d3d887.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of the re-query mechanism when there is disagreement.'
  prefs: []
  type: TYPE_NORMAL
- en: Here the driving actions and waypoints are extracted through predefined regular
    expressions. The waypoints are converted to the control signal by the traditional
    pid controller used in previous works [[22](#bib.bib22), [63](#bib.bib63)]. If
    conflicts between the predicted action and the exceed the pre-defined threshold
    the system will trigger a re-query mechanism. At this stage, the task commands
    part in the prompt (Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Prompting Multi-Modal
    Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs"))
    will be changed from the primary driving commands to query with the previous disagreement.
    If without an agreement, we will select the waypoints leaded action for final
    driving control. This mechanism could help the language model to ‘think twice’
    when dealing with unclear or hard situations.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Reinforcement Guided Tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pure auto-regressive learning is not sufficient to deal with the regression
    trajectories. Yet, under our design, the waypoints are purely learned in auto-regressive
    with MLE loss. In that case, we add a reinforcement-guided regression loss to
    boost the model with the prediction accuracy. Different from InstructGPT [[52](#bib.bib52)]
    that need human feedback to apply reinforcement fine-tuning, our scenarios have
    automated generated guidance. As the goal is to imitate the expert driving trajectories
    which are accessible in our task setting, we can assume the description of the
    expert trajectories as $y_{w}$ is presented as a reward in policy gradient as
    defined below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{rl}=LLM(\mathbf{x}^{T_{i}}&#124;,\mathbf{x}^{1:T_{i}-1})R(\mathbf{x}^{1:T_{i}},mask),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where mask denotes whether this is a parameter token. If the token is not a
    parameter token the reward will be always 0\. $\mathbf{x}^{T_{i}}$ is the total
    length).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{x}_{n}^{1:|x|}$
  prefs: []
  type: TYPE_NORMAL
- en: IV Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IV-A Experimental Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Simulation Environment: This paper performs the end-to-end driving task under
    the simulator. We use the CARLA Simulator 0.9.14 ²²2[https://carla.org/](https://carla.org/),
    a high-performance driving simulator under urban scenario as our simulation environment.
    The driving is performed in a variety of areas under various weather and lighting
    conditions. The agent is expected to perform driving given predefined routes under
    complicated traffic and weather conditions, where the routes are a sequence of
    sparse goal locations with GPS coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Collection: We use the simulator to collect 1000 routes in 8 official
    town maps with an average length of 400m by using CARLA rule-based expert auto-agent [[65](#bib.bib65)]
    with 228k training pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Benchmark: This paper is evaluated by both online benchmark and
    offline benchmarks. Although the CARLA simulator provides an official evaluation
    leaderboard, the use time is restricted to only 200 hours per month yet K80 GPUs
    are not feasible to deploy large LLMs, which makes the official leaderboard unsuitable
    for ablation studies or obtaining detailed statistics involving We conduct our
    ablation comparison based on the Longeset6 Benchmark proposed by Transfuser [[22](#bib.bib22)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Metrics: For both online evaluation and offline evaluation, we follow
    the official evaluation metrics to calculate three main metrics, Route Completion
    (RC), Infraction Score (IS), and Driving Score (DS). The RC score is the percentage
    of route distance completed. Given $R_{i}$ incurred by the agent during completing
    the routes.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Architectual: The multi-modality hybrid fusion network uses cameras and LiDAR
    as the sensory inputs, where image input is monotonic with FOV 120 degree and
    reshaped into shape $(160,704)$ after two times downsampling. InterFuser [[30](#bib.bib30)]
    and TCP [[31](#bib.bib31)]. For controller and safety loss, we directly follow
    structures from InterFuser [[30](#bib.bib30)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training of the LLMs: The driving data are gathered by autonomous agents as
    detailed in [[30](#bib.bib30)], ensuring comprehensive logging of all safety metrics.
    These metrics are then transformed into ”driving languages” through a set of predefined
    prompts, which utilize task token 1 for representation. Subsequently, the driving
    model is trained employing the loss functions delineated in InterFuser [[30](#bib.bib30)].
    Error data are compiled by assessing the training set, capturing all outputs from
    both the driving model and the ground truth (GT). Notably, these driving languages
    incorporate task tokens 2 and 3 at sampling rates of $20\%$, respectively. The
    uncertainty prediction layer undergoes training by categorizing this duo of data
    into binary labels and then leveraging the cross-entropy loss for optimization.
    As for the language model, we employ Vicuna 33B ³³3[https://huggingface.co/lmsys/vicuna-33b-v1.3](https://huggingface.co/lmsys/vicuna-33b-v1.3)
    as the pre-trained checkpoints and perform finetuning'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Local Driving Evaluation on LongSet6 on Carla 0.9.14'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method | DS $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| NEAT [[20](#bib.bib20)] | 20.63±2.34 | 45.31±3.25 | 0.54±0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| WOR [[67](#bib.bib67)] | 21.48±2.09 | 52.28±4.07 | 0.51±0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| GRIAD [[68](#bib.bib68)] | 26.98±4.23 | 71.43±3.57 | 0.56±0.04 |'
  prefs: []
  type: TYPE_TB
- en: '| LAV [[69](#bib.bib69)] | 37.62±1.77 | 83.94±2.69 | 0.59±0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| GF [[22](#bib.bib22)] | 27.44±2.11 | 80.43±3.98 | 0.32±0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Late TF [[22](#bib.bib22)] | 35.42±4.07 | 83.13±1.01 | 0.39±0.04 |'
  prefs: []
  type: TYPE_TB
- en: '| TransFuser [[22](#bib.bib22)] | 46.95±5.81 | 89.64±2.08 | 0.52±0.08 |'
  prefs: []
  type: TYPE_TB
- en: '| TCP [[31](#bib.bib31)] | 49.95±3.78 | 92.12±2.08 | 0.59±0.08 |'
  prefs: []
  type: TYPE_TB
- en: '| InterFuser [[30](#bib.bib30)] | 52.68±6.23 | 92.08±2.78 | 0.62±0.09 |'
  prefs: []
  type: TYPE_TB
- en: '| PromptGPT | 52.34±5.11 | 92.37±1.09 | 0.60±0.09 |'
  prefs: []
  type: TYPE_TB
- en: '|   Expert | 75.83±2.45 | 89.82±0.59 | 0.85±0.03 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: IV-C Driving Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Comparison Baseline: We compare this method with state-of-the-art (SOTA) E2E
    driving baselines below: 1) NEAT [[20](#bib.bib20)] the attention field fusion
    on image inputs. 2) WOR [[67](#bib.bib67)], multi-stage Q-function based learning
    framework; 3) GRIAD [[68](#bib.bib68)] general reinforced imitation baseline;
    4) LAV [[69](#bib.bib69)], the extension of WOR learning from multiple observed
    agents; 5) Transfuser [[22](#bib.bib22)], the solid SOTA baseline which introduce
    transformer fusion into E2E driving. Since MMFN [[23](#bib.bib23)] are Transfuser
    baseline if only with image and Lidar, we do not list the evaluation results of
    MMFN. Since the target of this paper is to enhance the modality fusion, we also
    do not list InterFuser [[30](#bib.bib30)] as it introduces an additional controller
    and decision module. We also include Late-Transfuser (Late TF) and geometrical
    fusion (GF) in [[22](#bib.bib22)] to report the early-late fusion comparison.
    TCP [[31](#bib.bib31)] and InterFuser [[30](#bib.bib30)] are selected as the representative
    of adding additional safety guidance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics Analysis: The performance is reported in the offline benchmark LongSet6 [[22](#bib.bib22)]
    in Table [I](#S4.T1 "TABLE I ‣ IV-B Implementation Details ‣ IV Experiment ‣ Prompting
    Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning
    with LLMs"). The mean and variance metric values are calculated by evaluating
    each agent three times. PromptGPT reached a mean driving score (DS) of $49.08$.
    Although the general performance is lower than current SOTA baselines InterFuser [[30](#bib.bib30)]
    and TCP [[31](#bib.bib31)], the PromptGPT-based driving could still reach competitive
    performance compared to other previous methods. Meanwhile, the safety score (IS)
    is with the same level of TCP while route completion (RC) is lower, which is rational
    since comprehensive language-based driving could better consider the corner cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Driving Visualization To give more intuitive explanations, we visualize the
    driving mediate state for both traditional waypoints prediction networks (TransFuser [[22](#bib.bib22)]
    and LLM-based E2E driving methods in Fig. [5](#S4.F5 "Figure 5 ‣ IV-C Driving
    Performance ‣ IV Experiment ‣ Prompting Multi-Modal Tokens to Enhance End-to-End
    Autonomous Driving Imitation Learning with LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a88e68b9ea950444b3ba94c2c02298ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Visualization of driving states between traditional methods and language
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: Given two typical scenarios, we could observe that language prompts could describe
    most of the information in the driving scenario. Yet, it still suggests that LLMs
    are still not very good at recognizing small and partially visible vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To assess the impact of LLM-facilitated driving correction, we conduct driving
    experiments across three distinct tasks: 1) Driving solely with LLMs, 2) Driving
    exclusively with the driving model, and 3) Employing both LLMs and the driving
    model for driving. Furthermore, we introduce various scales of LLMs to evaluate
    the influence of model size on performance. The results are presented in Table [II](#S4.T2
    "TABLE II ‣ IV-D Ablation Study ‣ IV Experiment ‣ Prompting Multi-Modal Tokens
    to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs")'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Ablation study on different settings for DriveLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Component | Method | DS $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| LLMs | Vicuna 7B | 16.54±2.34 | 39.28±3.42 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna 13B | 34.55±2.21 | 79.43±4.05 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna 33B | 43.08±5.28 | 91.87±1.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Task-Type | Drive (Vicuna 33B) | 43.08±5.28 | 87.43±3.98 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Drive + w/o re-query | 39.62±4.30 | 83.44±2.03 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Drive + Correction | 52.34±5.11 | 92.37±1.09 |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: Our observations indicate that when relying solely on language model-based driving,
    the performance attains a driving score of $43.08$. However, when employing LLMs
    as correction advisers, the performance elevates to state-of-the-art levels. These
    results underscore the effectiveness of our approach. Additionally, concerning
    the influence of model scales, there’s a distinct correlation between model size
    and performance. As we scale the LLM from 7B to 33B, there’s a notable performance
    boost. This trend aligns with our expectations, as intricate driving logic necessitates
    substantial model capacity.
  prefs: []
  type: TYPE_NORMAL
- en: V Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While language models exhibit competitive performance in driving simulators,
    they are not without limitations: The inference speed, combined with the re-query
    mechanism, results in sluggish response times, rendering it unsuitable for real-time
    driving. Additionally, the complexity of the driving framework poses challenges
    for evaluation on online benchmarks. The unpredictability inherent in language
    prompts is also a concern. On occasion, the LLM may not generate the necessary
    driving actions, prematurely halting and solely predicting perception parameters.
    Also, in order to train LLMs for perform correction, the current framework need
    the driving model pre-trained and evaluated in a ‘off-shelf’ formation to collect
    language training corpus, which is not yet End-to-End as well as heavy to train.'
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we delve into the innovative approach of utilizing LLMs with multi-modality
    tokens for end-to-end (E2E) autonomous driving. Rather than letting LLMs take
    the wheel directly, we suggest harnessing them to aid driving models in rectifying
    erroneous behaviors, especially in complex scenarios. Our experiments indicate
    that this corrective approach can prevent LLMs from generating non-existent actions.
    While the current language model hasn’t yet surpassed state-of-the-art performance,
    it demonstrates a commendable level of competitiveness. This implies the significant
    potential for integrating LLMs within the realm of autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] I. Gog, S. Kalra, P. Schafhalter, M. A. Wright, J. E. Gonzalez, and I. Stoica,
    “Pylot: A modular platform for exploring latency-accuracy tradeoffs in autonomous
    vehicles,” in 2021 IEEE International Conference on Robotics and Automation (ICRA),
    pp. 8806–8813, IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] T. Liu, Q. Liao, L. Gan, F. Ma, J. Cheng, X. Xie, Z. Wang, Y. Chen, Y. Zhu,
    S. Zhang, et al., “Hercules: An autonomous logistic vehicle for contact-less goods
    transportation during the covid-19 outbreak,” arXiv preprint arXiv:2004.07480,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] J. Jiao, Y. Zhu, H. Ye, H. Huang, P. Yun, L. Jiang, L. Wang, and M. Liu,
    “Greedy-based feature selection for efficient lidar slam,” in 2021 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 5222–5228, IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y. Liu, Y. Yixuan, and M. Liu, “Ground-aware monocular 3d object detection
    for autonomous driving,” IEEE Robotics and Automation Letters, vol. 6, no. 2,
    pp. 919–926, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Y. Li and J. Ibanez-Guzman, “Lidar for autonomous driving: The principles,
    challenges, and trends for automotive lidar and perception systems,” IEEE Signal
    Processing Magazine, vol. 37, no. 4, pp. 50–61, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] H. Song, W. Ding, Y. Chen, S. Shen, M. Y. Wang, and Q. Chen, “Pip: Planning-informed
    trajectory prediction for autonomous driving,” in European Conference on Computer
    Vision, pp. 598–614, Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Wen, J. Park, and K. Cho, “A scenario generation pipeline for autonomous
    vehicle simulators,” Human-centric Computing and Information Sciences, vol. 10,
    no. 1, pp. 1–15, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] L. Claussmann, M. Revilloud, D. Gruyer, and S. Glaser, “A review of motion
    planning for highway autonomous driving,” IEEE Transactions on Intelligent Transportation
    Systems, vol. 21, no. 5, pp. 1826–1848, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] T. Qin, Y. Zheng, T. Chen, Y. Chen, and Q. Su, “A light-weight semantic
    map for visual localization towards autonomous driving,” in 2021 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 11248–11254, IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Woo, B. Fidan, and W. W. Melek, “Localization for autonomous driving,”
    Handbook of Position Location: Theory, Practice, and Advances, Second Edition,
    pp. 1051–1087, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] C. R. Dyer, “Volumetric scene reconstruction from multiple views,” in
    Foundations of Image Understanding, pp. 469–489, Springer, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Bozic, P. Palafox, J. Thies, A. Dai, and M. Nießner, “Transformerfusion:
    Monocular rgb scene reconstruction using transformers,” Advances in Neural Information
    Processing Systems, vol. 34, pp. 1403–1414, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Letchner, J. Krumm, and E. Horvitz, “Trip router with individualized
    preferences (trip): Incorporating personalization into route planning,” in AAAI,
    pp. 1795–1800, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] N. Abu, W. Bukhari, M. Adli, S. Omar, and S. Sohaimeh, “A comprehensive
    overview of classical and modern route planning algorithms for self-driving mobile
    robots,” Journal of Robotics and Control (JRC), vol. 3, no. 5, pp. 666–678, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Tsugawa, “Vision-based vehicles in japan: Machine vision systems and
    driving control systems,” IEEE Transactions on industrial electronics, vol. 41,
    no. 4, pp. 398–405, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Kong, M. Pfeiffer, G. Schildbach, and F. Borrelli, “Kinematic and dynamic
    vehicle models for autonomous driving control design,” in 2015 IEEE intelligent
    vehicles symposium (IV), pp. 1094–1099, IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Hu, L. Chen, P. Wu, H. Li, J. Yan, and D. Tao, “St-p3: End-to-end vision-based
    autonomous driving via spatial-temporal feature learning,” in European Conference
    on Computer Vision, pp. 533–549, Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. Tampuu, T. Matiisen, M. Semikin, D. Fishman, and N. Muhammad, “A survey
    of end-to-end driving: Architectures and training methods,” IEEE Transactions
    on Neural Networks and Learning Systems, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al., “End to end learning for
    self-driving cars,” arXiv preprint arXiv:1604.07316, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] K. Chitta, A. Prakash, and A. Geiger, “Neat: Neural attention fields for
    end-to-end autonomous driving,” in Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp. 15793–15803, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. Toromanoff, E. Wirbel, and F. Moutarde, “End-to-end model-free reinforcement
    learning for urban driving using implicit affordances,” in Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7153–7162,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Prakash, K. Chitta, and A. Geiger, “Multi-modal fusion transformer
    for end-to-end autonomous driving,” in Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 7077–7087, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Q. Zhang, M. Tang, R. Geng, F. Chen, R. Xin, and L. Wang, “Mmfn: Multi-modal-fusion-net
    for end-to-end driving,” arXiv preprint arXiv:2207.00186, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Huang, G. Huang, Z. Zhu, and D. Du, “Bevdet: High-performance multi-camera
    3d object detection in bird-eye-view,” arXiv preprint arXiv:2112.11790, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and S. Han, “Bevfusion:
    Multi-task multi-sensor fusion with unified bird’s-eye view representation,” arXiv
    preprint arXiv:2205.13542, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Fadadu, S. Pandey, D. Hegde, Y. Shi, F.-C. Chou, N. Djuric, and C. Vallespi-Gonzalez,
    “Multi-view fusion of sensor data for improved perception and prediction in autonomous
    driving,” in Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision, pp. 2349–2357, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object detection
    network for autonomous driving,” in Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 1907–1915, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Zhou, P. Sun, Y. Zhang, D. Anguelov, J. Gao, T. Ouyang, J. Guo, J. Ngiam,
    and V. Vasudevan, “End-to-end multi-view fusion for 3d object detection in lidar
    point clouds,” in Conference on Robot Learning, pp. 923–932, PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and Z. Li, “Bevdepth:
    Acquisition of reliable depth for multi-view 3d object detection,” arXiv preprint
    arXiv:2206.10092, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. Shao, L. Wang, R. Chen, H. Li, and Y. Liu, “Safety-enhanced autonomous
    driving using interpretable sensor fusion transformer,” arXiv preprint arXiv:2207.14024,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] P. Wu, X. Jia, L. Chen, J. Yan, H. Li, and Y. Qiao, “Trajectory-guided
    control prediction for end-to-end autonomous driving: A simple yet strong baseline,”
    in NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin,
    W. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao, and H. Li, “Planning-oriented
    autonomous driving,” in Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] X. Jia, P. Wu, L. Chen, J. Xie, C. He, J. Yan, and H. Li, “Think twice
    before driving: Towards scalable decoders for end-to-end autonomous driving,”
    in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] B. Jin, X. Liu, Y. Zheng, P. Li, H. Zhao, T. Zhang, Y. Zheng, G. Zhou,
    and J. Liu, “Adapt: Action-aware driving caption transformer,” arXiv preprint
    arXiv:2302.00673, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
    A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth,
    S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch,
    and P. Florence, “Palm-e: An embodied multimodal language model,” in arXiv preprint
    arXiv:2303.03378, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing
    vision-language understanding with advanced large language models,” arXiv preprint
    arXiv:2304.10592, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla:
    An open urban driving simulator,” in Conference on robot learning, pp. 1–16, PMLR,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    Advances in Neural Information Processing Systems, vol. 1, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al., “End to end learning for
    self-driving cars,” arXiv preprint arXiv:1604.07316, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] F. Codevilla, M. Müller, A. López, V. Koltun, and A. Dosovitskiy, “End-to-end
    driving via conditional imitation learning,” in IEEE International Conference
    on Robotics and Automation, pp. 4693–4700, IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to drive
    by imitating the best and synthesizing the worst,” arXiv preprint arXiv:1812.03079,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam,
    A. Bewley, and A. Shah, “Learning to drive in a day,” in 2019 International Conference
    on Robotics and Automation (ICRA), pp. 8248–8254, IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Hecker, D. Dai, and L. Van Gool, “End-to-end learning of driving models
    with surround-view cameras and route planners,” in Proceedings of the European
    Conference on Computer Vision, pp. 435–453, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] D. Chen, V. Koltun, and P. Krähenbühl, “Learning to drive from a world
    on rails,” in Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp. 15590–15599, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M. Toromanoff, E. Wirbel, and F. Moutarde, “End-to-end model-free reinforcement
    learning for urban driving using implicit affordances,” in Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7153–7162,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 770–778, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    Neural computation, vol. 1, no. 4, pp. 541–551, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” arXiv,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] B. Jiang, S. Chen, Q. Xu, B. Liao, J. Chen, H. Zhou, Q. Zhang, W. Liu,
    C. Huang, and X. Wang, “Vad: Vectorized scene representation for efficient autonomous
    driving,” arXiv preprint arXiv:2303.12077, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei,
    “Deep reinforcement learning from human preferences,” Advances in neural information
    processing systems, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray, et al., “Training language models to follow instructions
    with human feedback,” Advances in Neural Information Processing Systems, vol. 35,
    pp. 27730–27744, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham,
    H. W. Chung, C. Sutton, S. Gehrmann, et al., “Palm: Scaling language modeling
    with pathways,” arXiv preprint arXiv:2204.02311, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer,
    A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al., “Scaling vision transformers
    to 22 billion parameters,” arXiv preprint arXiv:2302.05442, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, “Pointpillars:
    Fast encoders for object detection from point clouds,” in Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 12697–12705, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] N. Rhinehart, R. McAllister, K. Kitani, and S. Levine, “Precog: Predictions
    conditioned on goals in visual multi-agent scenarios,” in Proceedings of International
    Conference on Computer Vision, vol. 2, p. 4, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C. Schmid, “Vectornet:
    Encoding hd maps and agent dynamics from vectorized representation,” in Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11525–11533,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] D. Chen, B. Zhou, V. Koltun, and P. Krähenbühl, “Learning by cheating,”
    in Conference on Robot Learning, pp. 66–75, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. Zhao, Q. Sun, C. Zhang, Y. Tang, and F. Qian, “Monocular depth estimation
    based on deep learning: An overview,” Science China Technological Sciences, vol. 63,
    no. 9, pp. 1612–1627, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] A. Hu, Z. Murez, N. Mohan, S. Dudas, J. Hawke, V. Badrinarayanan, R. Cipolla,
    and A. Kendall, “Fiery: Future instance prediction in bird’s-eye view from surround
    monocular cameras,” in Proceedings of the IEEE/CVF International Conference on
    Computer Vision, pp. 15273–15282, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time end-to-end
    3d detection, tracking and motion forecasting with a single convolutional net,”
    in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 3569–3577, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., “An image is worth 16x16
    words: Transformers for image recognition at scale,” arXiv preprint arXiv:2010.11929,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] W. Nie, Q. Liang, Y. Wang, X. Wei, and Y. Su, “Mmfn: Multimodal information
    fusion networks for 3d model classification and retrieval,” ACM Transactions on
    Multimedia Computing, Communications, and Applications (TOMM), vol. 16, no. 4,
    pp. 1–22, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” arXiv preprint arXiv:1707.06347, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. Shao, L. Wang, R. Chen, H. Li, and Y. Liu, “Safety-enhanced autonomous
    driving using interpretable sensor fusion transformer,” in Conference on Robot
    Learning, pp. 726–737, PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. Saha, O. Mendez, C. Russell, and R. Bowden, “Translating images into
    maps,” in 2022 International Conference on Robotics and Automation (ICRA), pp. 9200–9206,
    IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] D. Chen, V. Koltun, and P. Krähenbühl, “Learning to drive from a world
    on rails,” in Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pp. 15590–15599, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R. Chekroun, M. Toromanoff, S. Hornauer, and F. Moutarde, “Gri: General
    reinforced imitation and its application to vision-based autonomous driving,”
    arXiv preprint arXiv:2111.08575, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] D. Chen and P. Krähenbühl, “Learning from all vehicles,” in Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17222–17231,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
