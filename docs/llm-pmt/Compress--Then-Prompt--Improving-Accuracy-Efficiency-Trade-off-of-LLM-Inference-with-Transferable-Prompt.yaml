- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:20
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩，再提示：通过可转移的提示改善 LLM 推理的准确性-效率权衡
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11186](https://ar5iv.labs.arxiv.org/html/2305.11186)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11186](https://ar5iv.labs.arxiv.org/html/2305.11186)
- en: Zhaozhuo Xu Equal contribution. The order of authors is determined by flipping
    a coin. Department of Computer Science, Rice University Zirui Liu^* Department
    of Computer Science, Rice University Beidi Chen Department of Electrical and Computer
    Engineering, Carnegie Mellon University Yuxin Tang Department of Computer Science,
    Rice University Jue Wang ETH Zürich, Switzerland Kaixiong Zhou Department of Computer
    Science, Rice University Xia Hu Department of Computer Science, Rice University
    Anshumali Shrivastava Department of Computer Science, Rice University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 赵卓曙 同等贡献。作者顺序由掷硬币决定。计算机科学系，莱斯大学 Zirui Liu^* 计算机科学系，莱斯大学 Beidi Chen 电气与计算机工程系，卡内基梅隆大学
    Yuxin Tang 计算机科学系，莱斯大学 Jue Wang ETH 苏黎世，瑞士 Kaixiong Zhou 计算机科学系，莱斯大学 Xia Hu 计算机科学系，莱斯大学
    Anshumali Shrivastava 计算机科学系，莱斯大学
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While the numerous parameters in Large Language Models (LLMs) contribute to
    their superior performance, this massive scale makes them inefficient and memory-hungry.
    Thus, they are hard to deploy on commodity hardware, such as one single GPU. Given
    the memory and power constraints of such devices, model compression methods are
    widely employed to reduce both the model size and inference latency, which essentially
    trades off model quality in return for improved efficiency. Thus, optimizing this
    accuracy-efficiency trade-off is crucial for the LLM deployment on commodity hardware.
    In this paper, we introduce a new perspective to optimize this trade-off by prompting
    compressed models. Specifically, we first observe that for certain questions,
    the generation quality of a compressed LLM can be significantly improved by adding
    carefully designed hard prompts, though this isn’t the case for all questions.
    Based on this observation, we propose a soft prompt learning method where we expose
    the compressed model to the prompt learning process, aiming to enhance the performance
    of prompts. Our experimental analysis suggests our soft prompt strategy greatly
    improves the performance of the $8\times$ compressed LLaMA-7B model (with a joint
    4-bit quantization and 50% weight pruning compression), allowing them to match
    their uncompressed counterparts on popular benchmarks. Also, we demonstrate that
    these learned prompts can be transferred across various datasets, tasks, and compression
    levels. Hence with this transferability, we can stitch the soft prompt to a newly
    compressed model to improve the test-time accuracy in an “in-situ” way.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型语言模型（LLMs）中的大量参数有助于它们的卓越性能，但这一大规模也使得它们效率低下且占用大量内存。因此，它们难以在普通硬件上部署，例如单个 GPU。鉴于这些设备的内存和功耗限制，模型压缩方法被广泛采用，以减少模型大小和推理延迟，这本质上是以模型质量换取更高的效率。因此，优化这种准确性-效率权衡对于
    LLM 在普通硬件上的部署至关重要。本文介绍了一种通过提示压缩模型来优化这种权衡的新视角。具体来说，我们首先观察到，对于某些问题，通过添加精心设计的硬提示，可以显著提高压缩
    LLM 的生成质量，尽管并非所有问题都如此。基于这一观察，我们提出了一种软提示学习方法，将压缩模型暴露于提示学习过程中，旨在提升提示的性能。我们的实验证明，我们的软提示策略极大地提高了
    $8\times$ 压缩 LLaMA-7B 模型（经过联合 4-bit 量化和 50% 权重剪枝压缩）的性能，使其在流行基准上与未压缩的模型相匹配。此外，我们还展示了这些学习到的提示可以在各种数据集、任务和压缩级别之间转移。因此，凭借这种可转移性，我们可以将软提示拼接到新压缩的模型中，以“原位”方式提高测试时的准确性。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) [[27](#bib.bib27), [28](#bib.bib28), [2](#bib.bib2),
    [40](#bib.bib40), [33](#bib.bib33)] has revolutionized the field of Natural Language
    Processing (NLP). Notably, LLMs are known for their in-context learning ability,
    allowing them to generalize to unseen tasks without additional fine-tuning [[2](#bib.bib2)].
    Specifically, LLMs are controlled through user-provided natural language specifications
    of the task, or *prompts*, which illustrate how to complete a task. Equipped with
    the in-context learning ability, we only need to serve a single large model to
    efficiently handle different tasks. Despite of their remarkable adaptability,
    LLMs are very expensive to deploy [[3](#bib.bib3), [35](#bib.bib35)]. The inference
    process of LLMs, such as LLaMA 2 [[34](#bib.bib34)], may require multiple powerful
    GPUs, which is prohibitively expensive for the general community. Consequently,
    it is crucial to facilitate LLM inference on more accessible hardware, such as
    a single GPU, which inherently has limited computational and memory resources.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）[[27](#bib.bib27), [28](#bib.bib28), [2](#bib.bib2), [40](#bib.bib40),
    [33](#bib.bib33)]已经彻底改变了自然语言处理（NLP）领域。值得注意的是，LLMs因其上下文学习能力而闻名，使其能够在无需额外微调的情况下泛化到未见过的任务[[2](#bib.bib2)]。具体来说，LLMs通过用户提供的自然语言任务规范，或称*提示*，来控制任务的完成方式。具备上下文学习能力后，我们只需要提供一个大型模型就可以高效处理不同任务。尽管它们的适应性极强，但LLMs的部署成本非常高[[3](#bib.bib3),
    [35](#bib.bib35)]。LLMs的推理过程，例如LLaMA 2[[34](#bib.bib34)]，可能需要多个强大的GPU，这对普通社区来说成本过于高昂。因此，必须促进在更可访问的硬件上进行LLM推理，例如单个GPU，而这些硬件本身的计算和内存资源有限。
- en: 'To address this problem, model compression methods are widely employed to reduce
    the model size and inference latency, such as quantization [[26](#bib.bib26),
    [4](#bib.bib4), [36](#bib.bib36), [7](#bib.bib7)] and pruning [[6](#bib.bib6)].
    These methods essentially trade off model quality in return for reduced latency
    and model size. Thus, there is an inevitable trade-off between accuracy and efficiency,
    resulting in a noticeable reduction in the model’s accuracy and, consequently,
    the overall performance benefits of LLMs. To get a sense, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), the full model (LLaMA-7B)
    is able to provide accurate answers to all three questions. However, the pruned
    model generates unrelated and off-topic answers to the same questions.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这个问题，广泛采用了模型压缩方法来减少模型的大小和推理延迟，例如量化[[26](#bib.bib26), [4](#bib.bib4), [36](#bib.bib36),
    [7](#bib.bib7)]和剪枝[[6](#bib.bib6)]。这些方法本质上是以牺牲模型质量为代价来减少延迟和模型大小。因此，在准确性和效率之间存在不可避免的权衡，导致模型的准确性显著下降，从而影响了LLMs的整体性能。举例来说，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")所示，完整模型（LLaMA-7B）能够准确回答所有三个问题。然而，剪枝后的模型对相同问题生成了无关且离题的答案。'
- en: Both model compression and prompts can influence the generation quality of LLMs.
    Thus intuitively, we can also utilize the prompt to help the compressed model
    generate more relevant answers. To the best of our knowledge, this perspective
    is not fully explored for LLMs. Thus one natural question is, *for a compressed
    model, can we design a prompt that helps it correct its predictions accordingly?*
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩和提示都能影响LLMs的生成质量。因此，直观地，我们还可以利用提示来帮助压缩后的模型生成更相关的答案。据我们了解，这一观点在LLMs中尚未被完全探讨。因此，一个自然的问题是，*对于一个压缩后的模型，我们能否设计一个提示来帮助它相应地纠正预测？*
- en: 'In this paper, we provide the first affirmative answer to the above question.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    we manually attach the prompt “*Please carefully examine the weight matrix within
    the model, as it may contain errors. It is crucial to verify its accuracy and
    make any necessary adjustments to ensure optimal performance*” to the original
    question. The prompted pruned model, i.e., “LLaMA-7B (62.5% sparsity) w./ Hard
    Prompt” in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    shows a significant improvement in its responses, although not all of them are
    accurate or complete. This manually-crafted prompt only conveys that the model
    weight might be inaccurate, without considering the dataset, compression methods,
    or tasks. This finding highlights the considerable potential for the transferability
    of this “hard prompt” across datasets, compression levels, and tasks. Despite
    the potential, this manually designed prompt is not consistently effective. Inspired
    by previous learnable prompt works [[19](#bib.bib19), [18](#bib.bib18)], we hypothesize
    that by involving the compressed weight in the prompt learning process, a learnable
    prompt could potentially surpass the performance of the manually-designed prompt,
    while maintaining the transferability. Building upon this insight, we introduce
    a paradigm of prompt learning that seeks to train additive prompt tokens on a
    compressed LLM to enhance its accuracy. We underscore that the primary distinction
    between our prompt learning approach and previous prompt tuning frameworks [[19](#bib.bib19),
    [18](#bib.bib18), [32](#bib.bib32)] is that earlier methods mainly utilized the
    prompt to adapt the model for specific downstream tasks. In contrast, the learned
    prompt in this paper resembles the hard prompt in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), as it can be transferred between
    various datasets, compression methods, and tasks.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们首次对上述问题给出了肯定的答案。如图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ 压缩，然后提示：通过可转移提示改进 LLM 推理的准确性-效率权衡")所示，我们手动将提示“*请仔细检查模型中的权重矩阵，因为它可能包含错误。验证其准确性并进行必要的调整以确保最佳性能*”附加到原始问题中。提示化的剪枝模型，即图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 压缩，然后提示：通过可转移提示改进 LLM 推理的准确性-效率权衡")中的“LLaMA-7B（62.5% 稀疏性）带硬提示”，在其回应中表现出显著的改进，尽管并非所有回答都是准确或完整的。这种手工设计的提示仅传达了模型权重可能不准确，而没有考虑数据集、压缩方法或任务。这一发现突显了这种“硬提示”在数据集、压缩级别和任务之间的转移潜力。尽管具有潜力，这种手工设计的提示并不是始终有效。受以前可学习提示工作的启发[[19](#bib.bib19),
    [18](#bib.bib18)]，我们假设通过将压缩权重纳入提示学习过程中，一个可学习的提示可能会超越手工设计的提示，同时保持转移性。基于这一见解，我们引入了一种提示学习范式，旨在训练压缩
    LLM 上的附加提示令牌以提高其准确性。我们强调，我们的提示学习方法与以前的提示调优框架[[19](#bib.bib19), [18](#bib.bib18),
    [32](#bib.bib32)]的主要区别在于，早期方法主要利用提示来适应模型以执行特定的下游任务。相比之下，本文中的学习提示类似于图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 压缩，然后提示：通过可转移提示改进 LLM 推理的准确性-效率权衡")中的硬提示，因为它可以在不同的数据集、压缩方法和任务之间转移。
- en: '![Refer to caption](img/feb7c0645cd18f5e1a431e7305ba3a6c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/feb7c0645cd18f5e1a431e7305ba3a6c.png)'
- en: 'Figure 1: The hard prompt enables compressed LLMs to regain commonsense. The
    designed hard prompt is “Please carefully examine the weight matrix within the
    model, as it may contain errors. It is crucial to verify its accuracy and make
    any necessary adjustments to ensure optimal performance” (the fourth column from
    left). We highlight the improved answers with green color.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：硬提示使压缩的 LLM 能够恢复常识。设计的硬提示是“请仔细检查模型中的权重矩阵，因为它可能包含错误。验证其准确性并进行必要的调整以确保最佳性能是至关重要的”（从左数第四列）。我们用绿色突出显示改进的答案。
- en: Our experimental analysis suggests our method greatly improves the performance
    of the $8\times$ compressed LLaMA-7B model (with a joint 4-bit quantization and
    50% weight pruning compression), allowing them to match their uncompressed counterparts
    on several standard benchmarks. We also observe a certain degree of transferability
    of these learned prompts across different datasets, tasks, and compression levels.
    Hence with this transferability, we can stitch the soft prompt to a newly compressed
    model to improve the test-time accuracy in an “in-situ” way.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验分析表明，我们的方法大大提高了$8\times$压缩的LLaMA-7B模型（结合4位量化和50%权重剪枝压缩）的性能，使其在多个标准基准上与未压缩的模型匹配。我们还观察到这些学习到的提示词在不同数据集、任务和压缩级别之间具有一定的迁移性。因此，利用这种迁移性，我们可以将软提示词嵌入到新的压缩模型中，以“原地”方式提高测试时的准确性。
- en: 2 Problem Statement and Related Work
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题陈述和相关工作
- en: In this section, we will begin by introducing the efficiency bottleneck of LLM
    inference. Then we will introduce current approximation approaches that are designed
    to reduce the computation and memory overhead and improve LLM inference latency.
    Finally, we will provide a review of recent progress that has been made in the
    development of prompts for LLMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将首先介绍LLM推理的效率瓶颈。接着，我们将介绍当前旨在减少计算和内存开销、提高LLM推理延迟的近似方法。最后，我们将回顾最近在LLM提示词开发方面取得的进展。
- en: 2.1 Efficiency Bottleneck of LLM Inference
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM推理的效率瓶颈
- en: LLMs adopt a decoder-only, autoregressive approach where token generation is
    carried out step by step, with each token’s generation dependent on the previously
    generated results. For instance, models such as GPT [[27](#bib.bib27), [28](#bib.bib28),
    [2](#bib.bib2)] follow this paradigm. A recent study by [[20](#bib.bib20)] investigates
    the inference process of OPT-175B models and finds that (1) token generation is
    the dominant factor contributing to the inference latency, and (2) Multilayer
    Perceptron (MLP) incurs higher I/O and computation latency compared to attention
    blocks during token generation. While system-level optimizations [[30](#bib.bib30),
    [9](#bib.bib9), [10](#bib.bib10)] can enhance the inference time of LLMs, they
    do not directly mitigate the computation and memory I/Os involved in the LLM inference
    process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs采用仅解码器的自回归方法，其中标记生成是逐步进行的，每个标记的生成都依赖于之前生成的结果。例如，像GPT [[27](#bib.bib27),
    [28](#bib.bib28), [2](#bib.bib2)]这样的模型遵循这一范式。最近[[20](#bib.bib20)]的一项研究调查了OPT-175B模型的推理过程，并发现（1）标记生成是导致推理延迟的主要因素，以及（2）多层感知器（MLP）在标记生成过程中比注意力块引入了更高的I/O和计算延迟。虽然系统级优化[[30](#bib.bib30),
    [9](#bib.bib9), [10](#bib.bib10)]可以提高LLM的推理时间，但它们并未直接缓解LLM推理过程中的计算和内存I/O。
- en: 2.2 Approximation in LLM Inference
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM推理中的近似方法
- en: 'In addition to optimizing at the system level, there are two primary approaches
    for reducing both computation and memory I/O to minimize the latency in inference.
    (1) Sparse modeling: the general idea is to choose a particular set of weights
    in certain layers to minimize both computation and memory I/O [[6](#bib.bib6),
    [20](#bib.bib20)]. These techniques are also closely related to pruning [[12](#bib.bib12),
    [15](#bib.bib15), [17](#bib.bib17), [14](#bib.bib14)] in the literature. Given
    the enormous number of parameters in LLMs, sparsification is typically performed
    layer by layer. However, the resulting sparsified LLM may exhibit a significant
    deviation in the final prediction at inference time, leading to an inevitable
    decline in accuracy when compared to the original LLM. (2) Quantization: it refers
    to the process of compressing trained weight values in LLMs into lower bits [[26](#bib.bib26),
    [4](#bib.bib4), [36](#bib.bib36), [7](#bib.bib7)]. Empirical evaluations have
    shown that int8 quantization can provide a great approximation of the predictive
    performance of the original LLMs [[4](#bib.bib4)]. However, there is a significant
    decline in accuracy when attempting to reduce the number of bits even further.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在系统级别进行优化外，还有两种主要方法可以减少计算和内存 I/O，以最小化推理中的延迟。(1) 稀疏建模：一般的想法是选择某些层中的一组特定权重，以最小化计算和内存
    I/O[[6](#bib.bib6), [20](#bib.bib20)]。这些技术也与文献中的剪枝[[12](#bib.bib12), [15](#bib.bib15),
    [17](#bib.bib17), [14](#bib.bib14)] 密切相关。鉴于 LLMs 中参数的数量庞大，稀疏化通常是逐层进行的。然而，结果的稀疏化
    LLM 可能在推理时表现出显著的预测偏差，与原始 LLM 相比，准确性不可避免地下降。(2) 量化：指的是将训练后的权重值压缩为更低位数的过程[[26](#bib.bib26),
    [4](#bib.bib4), [36](#bib.bib36), [7](#bib.bib7)]。经验评估表明，int8 量化可以提供原始 LLM 的预测性能的良好近似[[4](#bib.bib4)]。然而，当尝试进一步减少位数时，准确性会显著下降。
- en: 2.3 Prompt for LLMs
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLMs 的提示
- en: LLMs are known for their in-context learning ability, allowing them to generalize
    to unseen tasks without additional fine-tuning [[2](#bib.bib2)]. Specifically,
    LLMs are controlled through user-provided natural language specifications of the
    task, or *prompts*, which illustrate how to complete a task. In this paradigm,
    we do not enforce modifications on the LLMs themselves. Instead, we focus on adapting
    the inputs to the LLMs for better predictive performance in downstream tasks.
    A typical strategy is to insert tokens before the input sequence to affect the
    attention mechanism. It has been shown in [[2](#bib.bib2)] that prompt engineering
    enables LLMs to match the performance of fine-tuned language models on a variety
    of language understanding tasks. Moreover, [[18](#bib.bib18)] empirically indicate
    that there is an equivalence between modifying the input and fine-tuning the model.
    Furthermore, [[31](#bib.bib31)] studies the transferability of prompts across
    similar datasets or even tasks. Since then, we have witnessed the growth of prompt
    tuning infrastructure [[5](#bib.bib5)]. However, we would like to emphasize that
    most of the current demonstrations of prompt tuning are task-specific [[19](#bib.bib19),
    [18](#bib.bib18)]. When considering efficiency, it is desirable for a prompt to
    exhibit transferability across various settings.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 以其在上下文学习能力而闻名，这使得它们能够在不需要额外微调的情况下对未见过的任务进行泛化[[2](#bib.bib2)]。具体而言，LLMs
    通过用户提供的自然语言任务说明或*提示*来进行控制，这些提示说明了如何完成任务。在这种范式中，我们不对 LLMs 本身进行修改。而是专注于调整输入以提高下游任务的预测性能。一个典型的策略是在输入序列前插入标记，以影响注意力机制。在[[2](#bib.bib2)]中已经证明，提示工程使得
    LLMs 在各种语言理解任务上的表现可以匹配经过微调的语言模型。此外，[[18](#bib.bib18)] 经验性地表明，修改输入与微调模型之间存在等价性。此外，[[31](#bib.bib31)]
    研究了提示在相似数据集甚至任务之间的可转移性。从那时起，我们见证了提示调整基础设施的增长[[5](#bib.bib5)]。然而，我们要强调的是，目前大多数提示调整的演示都是针对特定任务的[[19](#bib.bib19),
    [18](#bib.bib18)]。在考虑效率时，希望提示能够在各种设置中表现出可转移性。
- en: 3 Motivation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动机
- en: The compression methods reduce the computational complexity at the cost of giving
    less accurate outputs. Thus, there naturally exists an accuracy-efficiency trade-off.
    In this section, we first empirically evaluate the trade-off of compressed LLMs.
    Then we found that for a compressed model, we can manually design a hard prompt
    that informs the model of its compressed state and helps it correct its predictions
    accordingly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩方法在降低计算复杂度的同时，会导致输出的准确性降低。因此，自然存在一个准确性与效率的权衡。在这一部分，我们首先对压缩 LLM 的权衡进行了经验评估。然后我们发现，对于一个压缩模型，我们可以手动设计一个硬提示，告知模型其压缩状态，并帮助其相应地纠正预测。
- en: 3.1 Performance of the Existing Approaches
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 现有方法的性能
- en: '![Refer to caption](img/21e61511b80faecdf4405077c3ea76c5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/21e61511b80faecdf4405077c3ea76c5.png)'
- en: (a) Quantization
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 量化
- en: '![Refer to caption](img/e4ba1d43b04e23c730c5bf2b5084fd25.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e4ba1d43b04e23c730c5bf2b5084fd25.png)'
- en: (b) Pruning
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 剪枝
- en: 'Figure 2: The validation perplexity of LLaMA-7B on C4 dataset at different
    compression level. The green line is the PPL of the original model.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLaMA-7B 在 C4 数据集上的验证困惑度（PPL）在不同压缩等级下的表现。绿色线为原始模型的 PPL。
- en: Experimental Setup. We assess the trade-off using LLaMA [[33](#bib.bib33)] on
    C4 dataset [[29](#bib.bib29)]. Here we adopt two representative post-training
    compression methods, i.e., GPTQ [[7](#bib.bib7)] and SparseGPT [[6](#bib.bib6)],
    to analyze the trade-off across various compression levels. We note that we choose
    post-training compression methods primarily for their ease of deployment. For
    the quantization method, we apply GPTQ to compress the model weights into 2, 3,
    and 4 bits integer numbers. As for the pruning method, we employ SparseGPT to
    eliminate 50%, 62.5%, and 75% of the model parameters. We would like to note that
    the post-training compression is conducted using the training set of C4, and subsequently,
    we evaluate the performance of the compression with the validation set of C4.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。我们使用 LLaMA [[33](#bib.bib33)] 在 C4 数据集 [[29](#bib.bib29)] 上评估了权衡。在这里，我们采用两种代表性的后训练压缩方法，即
    GPTQ [[7](#bib.bib7)] 和 SparseGPT [[6](#bib.bib6)]，来分析不同压缩等级下的权衡。我们注意到，我们选择后训练压缩方法主要是由于其易于部署。对于量化方法，我们应用
    GPTQ 将模型权重压缩为 2 位、3 位和 4 位整数。至于剪枝方法，我们使用 SparseGPT 删除 50%、62.5% 和 75% 的模型参数。我们还要指出，后训练压缩是使用
    C4 的训练集进行的，随后我们使用 C4 的验证集来评估压缩的性能。
- en: 'Quantitative Results. As shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Performance
    of the Existing Approaches ‣ 3 Motivation ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), we visualize the evaluation
    perplexity (PPL) [[16](#bib.bib16)] versus the compression level. When we prune
    50% of the parameters or quantize the parameters to 4 bits, the PPL remains closer
    to that of the full LLaMA model. The PPL consistently increases as we decrease
    the allocated resource (e.g., bit-width/sparsity). Notably, the PPL will explode
    when the resource is below a certain threshold. For instance, the PPL shifts from
    14 to 53 as sparsity increases from 62.5% to 75%. Moreover, the PPL grows significantly
    from around 11 to around 691 when we lower the quantization bits from 3-bit to
    2-bit.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '定量结果。如图 [2](#S3.F2 "Figure 2 ‣ 3.1 Performance of the Existing Approaches ‣
    3 Motivation ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt") 所示，我们可视化了评估困惑度（PPL）[[16](#bib.bib16)]
    与压缩等级之间的关系。当我们剪枝 50% 的参数或将参数量化为 4 位时，PPL 保持接近完整 LLaMA 模型的 PPL。随着分配资源（例如，位宽/稀疏性）的减少，PPL
    一直在增加。值得注意的是，当资源低于某个阈值时，PPL 将急剧增加。例如，当稀疏性从 62.5% 增加到 75% 时，PPL 从 14 变为 53。此外，当我们将量化位数从
    3 位降低到 2 位时，PPL 从大约 11 显著增长到大约 691。'
- en: 'Qualitative Results. As shown in the left part of Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), besides PPL, we also do a case study
    to understand how compression affects model generation results. In this example,
    the full model is able to provide accurate and relevant answers to all three simple
    questions. Specifically, it correctly identifies Long Beach as a city in Los Angeles
    County, California, pinpoints Tulsa in northeastern Oklahoma, and describes asparagus
    as a spring vegetable belonging to the lily family. However, the pruned model
    with 62.5% weight sparsity struggles to generate meaningful responses. Instead
    of providing the requested information, its answers seem unrelated and tangential.
    For example, the pruned model responds with a statement about seeking a job when
    asked about Long Beach, mentions being a student at the University of Tulsa when
    asked about Tulsa’s location, and admits uncertainty about Asparagus. This case
    study demonstrates that aggressive model compression, such as the 62.5% weight
    sparsity applied to the pruned model, can lead to a significant degradation in
    the quality of generated responses.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '质量结果。如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")左侧所示，除了PPL外，我们还进行了案例研究以了解压缩如何影响模型生成结果。在这个例子中，完整的模型能够对所有三个简单问题提供准确且相关的答案。具体而言，它正确识别了洛杉矶县，加州的长滩，定位了位于俄克拉荷马州东北部的塔尔萨，并描述了芦笋作为春季蔬菜，属于百合科。然而，具有62.5%权重稀疏性的剪枝模型在生成有意义的回应方面表现不佳。它的回答似乎与请求的信息无关且旁岔。例如，当询问长滩时，剪枝模型回应的是寻找工作，询问塔尔萨的地点时提到自己是塔尔萨大学的学生，而对芦笋则表示不确定。这个案例研究表明，激进的模型压缩，例如应用于剪枝模型的62.5%权重稀疏性，会导致生成的响应质量显著下降。'
- en: 3.2 Prompt Compressed Models
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 提示压缩模型
- en: 'In-context learning refers to the ability of adapting to the context provided
    within the input data through user-provided natural language specifications [[37](#bib.bib37),
    [25](#bib.bib25)], often referred to as *prompts*. Prompts serve to guide LLMs
    toward generating desired predictions by offering useful contextual information.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    the compressed model generates answers that are unrelated and off-topic when responding
    to these simple questions. Thus one natural question is, *for a compressed model,
    can we design a specific prompt that helps it correct its predictions accordingly?*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '上下文学习指的是通过用户提供的自然语言规格[[37](#bib.bib37), [25](#bib.bib25)]，适应输入数据中提供的上下文的能力，通常被称为*提示*。提示用于指导LLMs生成期望的预测，提供有用的上下文信息。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")所示，压缩模型在回答这些简单问题时生成的答案无关且跑题。因此，一个自然的问题是，*对于一个压缩模型，我们能否设计一个特定的提示来帮助其相应地修正预测？*'
- en: 'Following the question, we manually design the hard prompt as “*Please carefully
    examine the weight matrix within the model, as it may contain errors. It is crucial
    to verify its accuracy and make any necessary adjustments to ensure optimal performance*”.
    The results are shown in the fourth column of Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"). The observations are summarized as
    follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '在提问之后，我们手动设计了一个硬提示：“*请仔细检查模型中的权重矩阵，因为它可能包含错误。核实其准确性并进行必要的调整，以确保最佳性能*”。结果显示在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")的第四列中。观察结果总结如下：'
- en: 'The prompted pruned model, i.e., “LLaMA-7B (62.5% sparsity) w./ Hard Prompt”
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"), shows
    a significant improvement in its responses, although not all of them are accurate
    or complete. Specifically, (1) when explicitly told about its compressed state,
    the prompted pruned model correctly identifies that Long Beach is located in the
    United States. However, it does not provide further information about the city,
    such as its presence in Los Angeles County, California. (2) Regarding the second
    question about Tulsa, Oklahoma, the prompted pruned model fails to provide a relevant
    answer, instead repeating our prompt about the compression state, which is unrelated
    to the question. (3) When asked about asparagus, the prompted pruned model correctly
    identifies it as a plant used for cooking.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '提示的剪枝模型，即图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")中的“LLaMA-7B（62.5%
    稀疏性）w./硬提示”，在其响应中表现出显著的改进，尽管并非所有响应都准确或完整。具体而言，（1）当明确告知其压缩状态时，提示的剪枝模型正确识别出长滩位于美国。然而，它没有提供关于该城市的进一步信息，例如其位于加利福尼亚州洛杉矶县。（2）对于关于塔尔萨，俄克拉荷马州的第二个问题，提示的剪枝模型未能提供相关答案，而是重复了我们关于压缩状态的提示，与问题无关。（3）当询问关于芦笋的问题时，提示的剪枝模型正确识别出芦笋是一种用于烹饪的植物。'
- en: 'Insights. By explicitly informing the model of its compressed state, LLMs can
    generate more relevant responses for certain questions. The success of the designed
    prompt implies three great potentials:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 见解。通过明确告知模型其压缩状态，LLMs可以针对某些问题生成更相关的响应。设计的提示的成功暗示了三个重要的潜力：
- en: '1.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Cross-Dataset Transferability. This human-designed prompt only provides the
    information that model weight is inaccurate. So intuitively, irrespective of the
    specific dataset being used, we hypothesize that the LLMs can generate more relevant
    responses with the same prompt.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨数据集迁移性。这个人工设计的提示仅提供模型权重不准确的信息。因此，直观地说，无论使用特定的数据集，我们假设LLMs可以使用相同的提示生成更相关的响应。
- en: '2.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Cross-Compression Transferability. Similarly, the human-designed prompt only
    mentions that the weight is inaccurate, without specifying the exact compression
    level or method. We hypothesize that LLMs can generate more relevant responses
    with the same prompt across different compression levels and methods.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨压缩迁移性。同样，人工设计的提示只提到权重不准确，而没有具体说明压缩级别或方法。我们假设LLMs可以在不同的压缩级别和方法下使用相同的提示生成更相关的响应。
- en: '3.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Cross-Task Transferability. If LLMs can understand their compressed state and
    adjust accordingly, this adaptability is not limited to specific tasks or problem
    domains. Instead, it can be extended to a wide range of tasks.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨任务迁移性。如果LLMs能够理解其压缩状态并进行相应调整，这种适应性不限于特定任务或问题领域。相反，它可以扩展到广泛的任务。
- en: However, despite the potential, as we analyzed at the beginning of this section,
    the manually designed prompt is not consistently effective. In other words, it
    only works for some problems, and not all answers generated are accurate or complete.
    Inspired by previous learnable prompt work [[19](#bib.bib19), [18](#bib.bib18)],
    we hypothesize that by involving the compressed weight in the prompt learning
    process, a learnable prompt could potentially surpass the performance of the hard
    prompt while still retaining the transferability aspects of the hard prompt.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管有潜力，正如我们在本节开始时分析的那样，手动设计的提示并不总是有效。换句话说，它只对某些问题有效，并不是所有生成的答案都准确或完整。受到之前可学习提示工作的启发
    [[19](#bib.bib19), [18](#bib.bib18)]，我们假设通过在提示学习过程中涉及压缩权重，可学习的提示可能在保留硬提示的迁移性方面的同时，超越硬提示的性能。
- en: 4 Learning Prompt for Efficient LLM Inference
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 高效LLM推理的学习提示
- en: In this section, we will begin by introducing the formulation of the prompt
    learning paradigm. Then, we will shift our focus to the maximum likelihood objective
    of learning the prompt. Finally, we will delve into the transferability of the
    learned prompts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍提示学习范式的公式。然后，我们将把重点转向学习提示的最大似然目标。最后，我们将深入探讨所学提示的迁移性。
- en: 4.1 Formulation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 公式化
- en: 'Section [3.2](#S3.SS2 "3.2 Prompt Compressed Models ‣ 3 Motivation ‣ Compress,
    Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt") has shown that incorporating prompts can enhance the predictive performance
    of compressed LLMs. However, discovering effective language-based prompts through
    trial and error is a cumbersome and inefficient process that requires exploring
    a vast vocabulary space. Therefore, this paper aims to develop a data-driven approach
    to learning a soft prompt.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 部分 [3.2](#S3.SS2 "3.2 提示压缩模型 ‣ 3 动机 ‣ 压缩，再提示：通过可迁移提示提高LLM推理的准确性-效率权衡")已经表明，结合提示可以增强压缩LLM的预测性能。然而，通过试错法发现有效的基于语言的提示是一个繁琐且低效的过程，需要探索广泛的词汇空间。因此，本文旨在开发一种数据驱动的方法来学习软提示。
- en: Typically an LLM would have a tokenizer that maps each input sentence into a
    sequence of integers $[x_{0},x_{1},\cdots,x_{n}]$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，LLM会有一个分词器，将每个输入句子映射为整数序列 $[x_{0},x_{1},\cdots,x_{n}]$。
- en: 4.2 Learning Objectives
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 学习目标
- en: In this study, we present a prompt learning strategy that can be utilized as
    a post-training process for compressed LLMs. Given an LLM model with parameters
    denoted as $\theta$ before it. Next, we optimize the following objective.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了一种提示学习策略，可作为压缩LLM的后训练过程。给定一个参数为$\theta$的LLM模型。接下来，我们优化以下目标。
- en: '|  | $1$2 |  | (1) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'We note that the model parameter $\widetilde{\theta}$. Specifically, the Eq ([1](#S4.E1
    "In 4.2 Learning Objectives ‣ 4 Learning Prompt for Efficient LLM Inference ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt")) aims to maximize the likelihood of correctly predicting
    the next token in the sequence, given the preceding tokens. In this way, the learned
    prompt is aware of the compressed weights, as the gradient flows through these
    compressed weights during the optimization process. This allows the model to adapt
    its behavior to account for the compression effects while generating responses,
    potentially leading to improved performance.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到模型参数$\widetilde{\theta}$。具体来说，公式 ([1](#S4.E1 "在 4.2 学习目标 ‣ 4 高效LLM推理的学习提示
    ‣ 压缩，再提示：通过可迁移提示提高LLM推理的准确性-效率权衡"))旨在最大化在给定前面的标记的情况下正确预测序列中下一个标记的可能性。通过这种方式，学习到的提示能够了解压缩权重，因为在优化过程中梯度会流经这些压缩权重。这使得模型能够在生成响应时调整其行为以考虑压缩效应，从而可能提高性能。
- en: 4.3 Transferability of Learned Prompt
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 学习到的提示的可迁移性
- en: 'The findings derived from Section [3.2](#S3.SS2 "3.2 Prompt Compressed Models
    ‣ 3 Motivation ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt") have provided us with a compelling
    impetus to delve into the exploration of the transferability of prompt tokens
    acquired through Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt
    for Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")). The representation of
    these prompt tokens, as well as their acquisition through one dataset, could have
    a significant impact on other NLP applications. Specifically, we have chosen to
    concentrate on the scenarios below.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从部分 [3.2](#S3.SS2 "3.2 提示压缩模型 ‣ 3 动机 ‣ 压缩，再提示：通过可迁移提示提高LLM推理的准确性-效率权衡")得出的发现为我们提供了深入探讨通过公式 ([1](#S4.E1
    "在 4.2 学习目标 ‣ 4 高效LLM推理的学习提示 ‣ 压缩，再提示：通过可迁移提示提高LLM推理的准确性-效率权衡"))获得的提示标记的可迁移性的强大动力。这些提示标记的表示以及通过一个数据集获得它们可能对其他NLP应用产生重要影响。具体来说，我们选择专注于以下场景。
- en: Cross-Dataset Transferability. We aim to investigate whether prompt tokens trained
    from one dataset are applicable to other datasets. Prompt learning, while more
    efficient than fine-tuning, necessitates significant computational power and memory.
    With a single Nvidia-A100 possessing 40GB of memory, only the prompt learning
    of the LLaMA-7B model using a batch size of 1, sequence length of 1024, and 100
    prompt tokens can be supported. If we perform a single round of prompt learning
    for a compressed LLM and achieve favorable outcomes across various datasets, we
    can substantially enhance the accuracy-efficiency trade-offs of the LLM during
    inference.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 跨数据集传递性。我们旨在调查从一个数据集训练得到的提示令牌是否适用于其他数据集。提示学习虽然比微调更高效，但仍需要大量的计算能力和内存。使用一台拥有 40GB
    内存的 Nvidia-A100 显卡，最多只能支持对 LLaMA-7B 模型的提示学习，该配置下的批量大小为 1，序列长度为 1024，提示令牌为 100
    个。如果我们对一个压缩 LLM 进行一次提示学习，并在不同数据集上取得良好结果，我们可以显著提高 LLM 在推理过程中的准确性与效率的权衡。
- en: Cross-Compression Transferability. We aim to investigate the feasibility of
    utilizing learned prompts trained from a compressed LLM to another compressed
    LLM with different compression levels. For instance, we assess whether a prompt
    trained on a sparse LLM with a 75% sparsity can effectively boost the performance
    of an LLM with a 50% weight sparsity. Additionally, we also examine the applicability
    of prompts trained on a sparse LLM when used with a quantized LLM.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 跨压缩传递性。我们旨在调查将从一个压缩的 LLM 学习到的提示用于另一个具有不同压缩级别的压缩 LLM 的可行性。例如，我们评估在一个稀疏 LLM（稀疏度为
    75%）上训练的提示是否能够有效提升一个稀疏度为 50% 的 LLM 的性能。此外，我们还考察在量化 LLM 上使用从稀疏 LLM 训练得到的提示的适用性。
- en: 'Cross-Task Transferability. We aim to investigate whether the learned prompt
    trained from Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for
    Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")) on token generation tasks
    can be applied to other NLP tasks. This exploration will prove the effectiveness
    of prompts in improving the accuracy-efficiency trade-offs in the zero-shot generalization
    of LLMs in downstream tasks such as question answering.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 跨任务传递性。我们旨在调查从 Eq ([1](#S4.E1 "在 4.2 学习目标 ‣ 4 高效 LLM 推理的学习提示 ‣ 压缩，然后提示：通过可传递提示提升
    LLM 推理的准确性与效率权衡")) 的令牌生成任务中学习到的提示是否可以应用于其他 NLP 任务。这一探索将证明提示在提高 LLM 在下游任务（如问答）的零样本泛化中准确性与效率权衡方面的有效性。
- en: 5 Experiment
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'In this section, we assess the effectiveness of our prompt strategy in enhancing
    the trade-off between accuracy and efficiency during LLM inference. We commence
    by outlining the experimental setup, followed by presenting the results of token
    generation. Furthermore, we investigate the transferability of prompts across
    different datasets and compression levels. For additional experiments related
    to transferability and efficiency, please refer to Appendix [A](#A1 "Appendix
    A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), where we have included further details.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们的提示策略在提升 LLM 推理过程中的准确性与效率权衡方面的有效性。我们首先概述了实验设置，然后展示了令牌生成的结果。此外，我们还研究了提示在不同数据集和压缩级别之间的传递性。有关传递性和效率的其他实验，请参见附录 [A](#A1
    "附录 A 更多实验 ‣ 压缩，然后提示：通过可传递提示提升 LLM 推理的准确性与效率权衡")，其中包含了更多详细信息。
- en: 5.1 Experiment Setting
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'In our experimental framework, we incorporated the use of an Nvidia V100 GPU
    to conduct inference and prompt learning in LLMs. The datasets we utilized for
    token generation were comprehensive, including the Common Crawl’s web corpus (C4) [[29](#bib.bib29)],
    Wikitext-2 [[23](#bib.bib23)], and the Penn Treebank (PTB) [[22](#bib.bib22)]
    databases. We set the sequence length for these datasets to 1024\. For the token
    generation task, we use perplexity (PPL) [[16](#bib.bib16)] as the evaluation
    metric. We also introduce some downstream tasks to evaluate the cross-task transferability
    of the learned prompt. We will introduce the task information in the specific
    section. At the core of our modeling approach, we adopted the Open Pre-trained
    Transformer (OPT) Language Models [[40](#bib.bib40)] and Large Language Model
    Architecture (LLaMA) [[33](#bib.bib33)]. To compress the OPT and LLaMA model,
    we employed techniques from both SparseGPT [[6](#bib.bib6)] and GPTQ [[7](#bib.bib7)]
    methodologies. We refer the readers to Appendix [A.1](#A1.SS1 "A.1 Experiment
    Details ‣ Appendix A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") for more experimental details.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的实验框架中，我们采用了 Nvidia V100 GPU 来进行推断和 LLM 的提示学习。我们用于 token 生成的数据集非常全面，包括 Common
    Crawl 的网页语料库 (C4) [[29](#bib.bib29)]、Wikitext-2 [[23](#bib.bib23)] 和 Penn Treebank
    (PTB) [[22](#bib.bib22)] 数据库。我们将这些数据集的序列长度设置为 1024。对于 token 生成任务，我们使用困惑度 (PPL)
    [[16](#bib.bib16)] 作为评估指标。我们还引入了一些下游任务，以评估学习到的提示的跨任务可迁移性。我们将在具体部分介绍任务信息。在我们的建模方法核心中，我们采用了
    Open Pre-trained Transformer (OPT) Language Models [[40](#bib.bib40)] 和 Large
    Language Model Architecture (LLaMA) [[33](#bib.bib33)]。为了压缩 OPT 和 LLaMA 模型，我们结合使用了
    SparseGPT [[6](#bib.bib6)] 和 GPTQ [[7](#bib.bib7)] 的技术。更多实验细节请参考附录 [A.1](#A1.SS1
    "A.1 Experiment Details ‣ Appendix A More Experiments ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")。'
- en: 5.2 Token Generation Results
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 Token 生成结果
- en: 'On the C4 training set, we compress the OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B
    using SparseGPT [[6](#bib.bib6)]. We utilize sparsity levels of 50%, 62.5%, and
    75% for compression. Additionally, we employ GPTQ [[7](#bib.bib7)] for 2-bit,
    3-bit, and 4-bit quantization. Furthermore, prompt learning is applied to each
    compressed model using the methodology introduced in Eq ([1](#S4.E1 "In 4.2 Learning
    Objectives ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")).
    We set $k$ in Eq. [1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for
    Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") to 100, i.e., incorporating
    100 learnable prompt tokens. In Table [1](#S5.T1 "Table 1 ‣ 5.2 Token Generation
    Results ‣ 5 Experiment ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), we also conduct the ablation
    study on the impact of the number of soft tokens using 3-bit quantized LLaMA-7B
    on PTB dataset. We observe that there is still a significant improvement with
    25 prompt tokens, and we can improve the performance by increasing the prompt
    size.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '在 C4 训练集上，我们使用 SparseGPT [[6](#bib.bib6)] 压缩了 OPT-1.3B、OPT-2.7B、OPT-6.7B 和
    LLaMA-7B。我们采用了 50%、62.5% 和 75% 的稀疏级别进行压缩。此外，我们使用 GPTQ [[7](#bib.bib7)] 进行 2-bit、3-bit
    和 4-bit 量化。进一步地，我们对每个压缩模型应用了基于 Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4
    Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")) 引入的方法进行提示学习。我们将
    Eq. [1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for Efficient LLM
    Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt") 中的 $k$ 设置为 100，即包含 100 个可学习的提示 token。在表格
    [1](#S5.T1 "Table 1 ‣ 5.2 Token Generation Results ‣ 5 Experiment ‣ Compress,
    Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt") 中，我们还进行了关于 3-bit 量化 LLaMA-7B 在 PTB 数据集上软 token 数量影响的消融研究。我们观察到，即使使用 25
    个提示 token，仍然有显著的改进，并且通过增加提示大小可以进一步提高性能。'
- en: 'Table 1: Ablation study on the impact of the number of soft tokens using 3-bit
    quantized LLama-7B on PTB dataset.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：关于 3-bit 量化 LLaMA-7B 在 PTB 数据集上软 token 数量影响的消融研究。
- en: '| # tokens | Perplexity |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| # tokens | 困惑度 |'
- en: '| Baseline (0 tokens) | 15.74 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 基线（0 tokens） | 15.74 |'
- en: '| 25 tokens | 9.26 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 25 tokens | 9.26 |'
- en: '| 50 tokens | 8.61 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 50 tokens | 8.61 |'
- en: '| 75 tokens | 8.17 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 75 tokens | 8.17 |'
- en: '| 100 tokens | 7.76 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 100 tokens | 7.76 |'
- en: 'Figure [3](#S5.F3 "Figure 3 ‣ 5.2 Token Generation Results ‣ 5 Experiment ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") demonstrates the impact of our approach on the validation
    set of C4\. We observe a significant improvement in PPL across all compression
    levels. Firstly, by employing soft prompt tokens, the compressed LLMs using SparseGPT
    with 50% sparsity even outperform the full model counterparts, exhibiting lower
    PPL. This trend is also observed in the 4-bit quantization of LLMs using GPTQ.
    Secondly, even with further enhanced compression, the compressed LLMs with soft
    prompt tokens learned from Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning
    Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")) still maintain comparable
    PPL to their original counterparts. Notably, prompts learned from each of the
    four 3-bit quantized models aid in surpassing the performance of their respective
    full model counterparts. We also observe a similar effect in sparse models with
    62.5% sparsity for OPT-1.3B and OPT-2.7B. Conversely, prompts learned from both
    OPT-6.7B and LLaMA-7B assist in achieving the same PPL as their full model counterparts.
    Lastly, our approach significantly enhances the predictive performance of extreme
    scale compression. In both SparseGPT with 75% sparsity and GPTQ with 2-bit quantization,
    we find that the prompt learning strategy substantially improves the PPL across
    all four models. For example, prompts learned over the 2-bit GPTQ compression
    of OPT-1.3B reduce the PPL from 2337.8 to 59.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S5.F3 "Figure 3 ‣ 5.2 Token Generation Results ‣ 5 Experiment ‣ Compress,
    Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt") 展示了我们的方法对 C4 验证集的影响。我们观察到在所有压缩水平下 PPL 有显著改善。首先，通过使用软提示符，采用 50% 稀疏度的 SparseGPT
    压缩 LLM 甚至优于完整模型，表现出较低的 PPL。这一趋势在使用 GPTQ 的 4 位量化 LLM 中也得到了体现。其次，即使在进一步增强的压缩情况下，从
    Eq [1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for Efficient LLM
    Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt") 中学习到的软提示符压缩 LLM 仍保持与其原始对应模型相当的 PPL。值得注意的是，从四种
    3 位量化模型中学习到的提示符有助于超越其各自完整模型的性能。我们还在 OPT-1.3B 和 OPT-2.7B 的 62.5% 稀疏模型中观察到了类似的效果。相反，从
    OPT-6.7B 和 LLaMA-7B 学到的提示符则帮助实现了与其完整模型相同的 PPL。最后，我们的方法显著提高了极端规模压缩的预测性能。在 SparseGPT
    75% 稀疏度和 GPTQ 2 位量化中，我们发现提示符学习策略在所有四个模型中都显著改善了 PPL。例如，经过 2 位 GPTQ 压缩的 OPT-1.3B
    提示符将 PPL 从 2337.8 降低到 59。'
- en: '![Refer to caption](img/54845cbb8b0a35cbc11663b46318c277.png)![Refer to caption](img/d815b17e952b49fbe10ffd4039c6acce.png)![Refer
    to caption](img/871112434c354959a0083005090d1f3e.png)![Refer to caption](img/2f2ed94e12ee95703b7f16d9eefe9ace.png)![Refer
    to caption](img/f637f09b1c9071616dadc3471093ab54.png)![Refer to caption](img/8814daa37b68cc20a1d781156d8feb7f.png)![Refer
    to caption](img/486d311142e2839cd798913cd4d6d21e.png)![Refer to caption](img/218fa6e146804b23e912a8a362222c6c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54845cbb8b0a35cbc11663b46318c277.png)![参见说明](img/d815b17e952b49fbe10ffd4039c6acce.png)![参见说明](img/871112434c354959a0083005090d1f3e.png)![参见说明](img/2f2ed94e12ee95703b7f16d9eefe9ace.png)![参见说明](img/f637f09b1c9071616dadc3471093ab54.png)![参见说明](img/8814daa37b68cc20a1d781156d8feb7f.png)![参见说明](img/486d311142e2839cd798913cd4d6d21e.png)![参见说明](img/218fa6e146804b23e912a8a362222c6c.png)'
- en: 'Figure 3: OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on C4 dataset, validation
    set at different bit-width and sparsity. Here the “Baseline” (green line) represents
    the uncompressed model.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：OPT-1.3B、OPT-2.7B、OPT-6.7B 和 LLaMA-7B 在 C4 数据集上的验证集，不同位宽和稀疏度下的表现。这里的“Baseline”（绿色线）表示未压缩模型。
- en: 5.3 Cross-Dataset Transferability
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 跨数据集的可迁移性
- en: Intuitively, a model compressed using one dataset should achieve decent predictive
    performance when transferred to other datasets [[7](#bib.bib7), [6](#bib.bib6)].
    Here we assess whether the prompt tokens learned from one dataset exhibit similar
    transferability across different datasets. Specifically, we first compress a model
    with SparseGPT or GPTQ using C4 training set. We then learn the prompt with the
    compressed model on C4 training set. Finally, we evaluate the performance of this
    compressed model with and without the learned prompts on other datasets, e.g.,
    Wikitext-2 and PTB dataset. We emphasize the entire process does not involve any
    task-specific data, and our results thus remain “zero-shot”.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，使用一个数据集压缩的模型在迁移到其他数据集时应该能取得不错的预测性能[[7](#bib.bib7), [6](#bib.bib6)]。在这里，我们评估了从一个数据集学习到的提示令牌是否在不同数据集之间表现出类似的迁移性。具体而言，我们首先使用C4训练集通过SparseGPT或GPTQ压缩一个模型。然后，我们在C4训练集上使用压缩模型学习提示。最后，我们在其他数据集（例如Wikitext-2和PTB数据集）上评估该压缩模型在有无学习到的提示下的表现。我们强调，整个过程不涉及任何特定任务的数据，因此我们的结果仍然是“零样本”。
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ 5.3 Cross-Dataset Transferability ‣ 5 Experiment
    ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") presents the performance of OPT-1.3B, OPT-2.7B, OPT-6.7B,
    and LLaMA-7B on the test set of Wikitext-2 and the PTB dataset. For each LLM model,
    we also include the performance of its compressed versions with 50%, 62.5%, and
    75% sparsity using SparseGPT. Additionally, we include the performance of each
    model’s compressed version with 2-bit, 3-bit, and 4-bit quantization using GPTQ.
    The figures demonstrate the consistent advantages of prompt tokens across the
    two datasets. For every model with 50% sparsity or 4-bit quantization, learning
    prompts from the C4 dataset result in a lower PPL compared to the full model counterpart.
    Moreover, we observe a substantial improvement in PPL when using learned prompt
    tokens as the model becomes more compressed. This phenomenon validates that the
    prompts learned on top of compressed models can be effectively transferred across
    datasets.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S5.F4 "图 4 ‣ 5.3 跨数据集迁移性 ‣ 5 实验 ‣ 压缩后提示：改善 LLM 推理的准确性-效率权衡")展示了OPT-1.3B、OPT-2.7B、OPT-6.7B和LLaMA-7B在Wikitext-2和PTB数据集测试集上的表现。对于每个LLM模型，我们还包括了其压缩版本在50%、62.5%和75%稀疏度下的表现，这些压缩版本使用了SparseGPT。此外，我们还包括了每个模型在2-bit、3-bit和4-bit量化下的压缩版本的表现，这些量化版本使用了GPTQ。图示显示了在这两个数据集上提示令牌的持续优势。对于每个具有50%稀疏度或4-bit量化的模型，来自C4数据集的学习提示相较于全模型对应版本的PPL较低。此外，当使用学习到的提示令牌时，随着模型的压缩程度增加，我们观察到PPL的显著改善。这一现象验证了在压缩模型上学习到的提示可以有效地在数据集之间迁移。
- en: 'Due to the page limits, we also conduct the ablation experiments on the transferability
    in Appendix [A.2](#A1.SS2 "A.2 Ablation on the Transferability ‣ Appendix A More
    Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt"). Specifically, we compare the transferred
    soft prompts against the soft prompts that are trained on the downstream dataset,
    which serve as the top-line counterpart. We also observe that with learned soft
    prompt, the gap between the full model and quantized model is greatly reduced'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于页面限制，我们还在附录[A.2](#A1.SS2 "A.2 迁移性的消融实验 ‣ 附录 A 更多实验 ‣ 压缩后提示：改善 LLM 推理的准确性-效率权衡")中进行了迁移性消融实验。具体来说，我们将迁移的软提示与在下游数据集上训练的软提示进行比较，后者作为基线对照。我们还观察到，使用学习到的软提示时，全模型与量化模型之间的差距大大缩小。
- en: '![Refer to caption](img/a37604363d72cb4bc3fbbeb90764fa9c.png)![Refer to caption](img/30023b13bfe0a86e02d25d0092d07b20.png)![Refer
    to caption](img/b5b5a9ce09dd5957c6a421b8f4a125a3.png)![Refer to caption](img/cb4c8b3203502c36765bc9216a37e56d.png)![Refer
    to caption](img/56f48705ea099928db36e3d197216e7b.png)![Refer to caption](img/9421fa41c53c6eff38f7f0e63e96a090.png)![Refer
    to caption](img/663ee1912cdcc3644de0ac38a007ad29.png)![Refer to caption](img/edccb2bf54b8098e9c6046b2cdeb7c3c.png)![Refer
    to caption](img/ab41f3e7dab7a5b42403dded1a512534.png)![Refer to caption](img/7c246064d76beedaaef68d81d33a0761.png)![Refer
    to caption](img/7709a039188a9d86af4fbbb34b10eade.png)![Refer to caption](img/fc9fd246230c507502b395392011f495.png)![Refer
    to caption](img/0bd89a4fd86659df94e2b534694d2f20.png)![Refer to caption](img/8dde0f51f17b6fb10e098cfe71a26c01.png)![Refer
    to caption](img/3de97c8cab5600ab2e4f896763bbe81d.png)![Refer to caption](img/13646aa56e39f660c3e7750e58e5bb41.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a37604363d72cb4bc3fbbeb90764fa9c.png)![参考标题](img/30023b13bfe0a86e02d25d0092d07b20.png)![参考标题](img/b5b5a9ce09dd5957c6a421b8f4a125a3.png)![参考标题](img/cb4c8b3203502c36765bc9216a37e56d.png)![参考标题](img/56f48705ea099928db36e3d197216e7b.png)![参考标题](img/9421fa41c53c6eff38f7f0e63e96a090.png)![参考标题](img/663ee1912cdcc3644de0ac38a007ad29.png)![参考标题](img/edccb2bf54b8098e9c6046b2cdeb7c3c.png)![参考标题](img/ab41f3e7dab7a5b42403dded1a512534.png)![参考标题](img/7c246064d76beedaaef68d81d33a0761.png)![参考标题](img/7709a039188a9d86af4fbbb34b10eade.png)![参考标题](img/fc9fd246230c507502b395392011f495.png)![参考标题](img/0bd89a4fd86659df94e2b534694d2f20.png)![参考标题](img/8dde0f51f17b6fb10e098cfe71a26c01.png)![参考标题](img/3de97c8cab5600ab2e4f896763bbe81d.png)![参考标题](img/13646aa56e39f660c3e7750e58e5bb41.png)'
- en: 'Figure 4: OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on Wikitext-2 and PTB
    test set at different bit-width and sparsity. Here the “Baseline” (green line)
    represents the uncompressed model.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：OPT-1.3B、OPT-2.7B、OPT-6.7B和LLaMA-7B在Wikitext-2和PTB测试集上的不同比特宽度和稀疏度下的表现。这里的“基线”（绿色线）表示未压缩模型。
- en: 5.4 Cross-Compression Transferability
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 跨压缩可转移性
- en: 'In this section, we assess the transferability of learned prompts across various
    compression levels. Specifically, we aim to address the following questions: (1)
    Can the prompt learned from an LLM compressed through sparsification at a specific
    sparsity level be applied to other sparse LLMs with different sparsities? (2)
    Can the prompt learned from an LLM quantized to a particular bit level be applied
    to other quantized LLMs with different bits? (3) Is it possible to transfer prompts
    learned from sparse LLMs to quantized LLMs, or vice versa, in order to enhance
    predictive accuracy?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了在不同压缩水平下学到的提示的可转移性。具体来说，我们旨在解决以下问题：（1）从在特定稀疏水平下通过稀疏化压缩的LLM中学到的提示能否应用于其他具有不同稀疏度的稀疏LLM？（2）从量化到特定比特水平的LLM中学到的提示能否应用于其他具有不同比特的量化LLM？（3）是否可以将从稀疏LLM中学到的提示转移到量化LLM中，或反之，以提高预测准确性？
- en: 'In Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Cross-Compression Transferability ‣ 5
    Experiment ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt"), we assess the performance of employing
    prompts derived from a compressed LLM on other compressed LLMs, employing various
    compression approaches and levels. As an example, we utilize LLaMA-7B and present
    the PPL results on the validation set of C4, as well as the test sets of Wikitext-2
    and PTB. In this context, the “target” refers to the compression type and level
    for the compressed model, while the “source” represents the type and level of
    the compressed model from which the prompt is learned. For example, “source 4-bit”
    indicates that the prompt is learned from a compressed model with 4-bit quantization.
    Based on the figures, we address the raised questions from three perspectives:
    (1) Regarding sparse LLMs, prompts learned from higher sparsity can be effectively
    transferred to models with lower sparsity. For instance, prompts learned from
    62.5% and 75% sparsity can be applied to a sparse LLaMA-7B model with 50% sparsity,
    resulting in a better PPL compared to the original LLaMA-7B model. (2) For quantized
    LLMs, prompts learned from lower bit quantization levels can be successfully applied
    to models with higher bit quantization, while achieving comparable performance.
    (3) There is a certain degree of transferability of prompts learned between different
    compression types, especially when the compression level is less. For instance,
    a prompt learned from a LLaMA-7B model with 4-bit quantization can be transferred
    to a LLaMA-7B model with 50% sparsity.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](#S5.F5 "图 5 ‣ 5.4 跨压缩迁移性 ‣ 5 实验 ‣ 压缩，然后提示：通过可迁移提示改进LLM推理的准确性-效率权衡")中，我们评估了在其他压缩LLM上使用从压缩LLM中得到的提示的性能，采用了各种压缩方法和级别。例如，我们使用了LLaMA-7B，并展示了C4验证集、Wikitext-2测试集和PTB测试集上的PPL结果。在此背景下，“目标”指的是压缩模型的压缩类型和级别，而“源”表示的是从中学习提示的压缩模型的类型和级别。例如，“源
    4-bit”表示提示是从一个4位量化的压缩模型中学习得来的。根据这些图表，我们从三个方面回答了提出的问题：（1）对于稀疏LLM，从较高稀疏度中学习的提示可以有效地转移到较低稀疏度的模型中。例如，从62.5%和75%稀疏度中学习的提示可以应用于50%稀疏度的稀疏LLaMA-7B模型，相比于原始LLaMA-7B模型，结果具有更好的PPL。（2）对于量化LLM，从较低位量化级别中学习的提示可以成功地应用于较高位量化的模型，同时实现相当的性能。（3）在不同压缩类型之间的提示具有一定的迁移性，尤其是在压缩级别较低时。例如，从4位量化的LLaMA-7B模型中学习的提示可以迁移到50%稀疏度的LLaMA-7B模型。
- en: '![Refer to caption](img/a1e4f2d7b5197ce74a92f8de6b1edaac.png)![Refer to caption](img/80e76cf3ed521ae780fda6e4e4474bea.png)![Refer
    to caption](img/e44448a5f498f1f639be3c384ff5fd55.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a1e4f2d7b5197ce74a92f8de6b1edaac.png)![参见说明](img/80e76cf3ed521ae780fda6e4e4474bea.png)![参见说明](img/e44448a5f498f1f639be3c384ff5fd55.png)'
- en: 'Figure 5: LLaMA-7B transfer between different sparsity and bit-width. The “target”
    refers to the compression type and level for the compressed model, while the“source”
    represents the type and level of the compressed model from which the prompt is
    learned. For example, “4-bit” in source indicates that the prompt is learned from
    a compressed model with 4-bit quantization.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：LLaMA-7B在不同稀疏度和位宽之间的迁移。 “目标”指的是压缩模型的压缩类型和级别，而“源”表示的是从中学习提示的压缩模型的类型和级别。例如，源中的“4-bit”表示提示是从一个4位量化的压缩模型中学习得来的。
- en: 5.5 Combination of Sparsification and Quantization
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 稀疏化与量化的组合
- en: 'Table 2: The PPL of joint 50% sparsity + 4-bit quantization with learned prompts
    on the validation set of C4 and a test set of Wikitext-2 and PTB. The prompt is
    learned on C4 training set.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：联合50%稀疏度 + 4位量化的提示在C4验证集和Wikitext-2及PTB测试集上的PPL。提示在C4训练集上学习。
- en: '| Models | C4 | Wikitext-2 | PTB |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | C4 | Wikitext-2 | PTB |'
- en: '| Full | 7.59 | 6.34 | 11.02 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | 7.59 | 6.34 | 11.02 |'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 50% + 4-bit &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 50% + 4-bit &#124;'
- en: '&#124; (w./o. prompt) &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (无提示) &#124;'
- en: '| 10.94 | 9.67 | 17.39 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 10.94 | 9.67 | 17.39 |'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 50% + 4-bit &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 50% + 4-bit &#124;'
- en: '&#124; (w./ prompt) &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (有提示) &#124;'
- en: '| 7.38 | 7.31 | 10.64 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 7.38 | 7.31 | 10.64 |'
- en: 'In this section, we explore the effectiveness of the prompt strategy in the
    combination of sparsification and quantization for compressing LLM. Since sparsification
    and quantization target different aspects of compression, it is natural to combine
    them to achieve better efficiency. Table [2](#S5.T2 "Table 2 ‣ 5.5 Combination
    of Sparsification and Quantization ‣ 5 Experiment ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt") presents
    the PPL before and with, and without the learned prompt on the validation set
    of C4, as well as the test sets of Wikitext-2 and PTB. We choose the LLaMA-7B
    model compressed using 50% sparsity and 4-bit quantization from the training set
    of C4\. We should note that the prompt learning process also takes place on the
    training set of C4\. Our results demonstrate that the prompt learning strategy
    remains effective when combining sparsification and quantization. Additionally,
    with the prompt, the 50% sparse and 4-bit compressed model still performs comparably
    to the original LLaMA-7B.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们探讨了在稀疏化和量化的组合中，提示策略在压缩LLM中的有效性。由于稀疏化和量化针对压缩的不同方面，自然将它们结合起来以实现更好的效率。表[2](#S5.T2
    "Table 2 ‣ 5.5 Combination of Sparsification and Quantization ‣ 5 Experiment ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt")展示了在C4验证集上，包含和不包含学习到的提示前后的PPL，以及Wikitext-2和PTB测试集上的PPL。我们选择了使用50%稀疏化和4位量化的LLaMA-7B模型，来自C4的训练集。我们应该注意到，提示学习过程也发生在C4的训练集上。我们的结果表明，提示学习策略在结合稀疏化和量化时仍然有效。此外，使用提示时，50%稀疏和4位压缩模型的表现仍然与原始LLaMA-7B相当。'
- en: 6 Conclusion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This research showcases an innovative approach to optimize the trade-off between
    computational efficiency and accuracy in Large Language Models (LLMs). The study
    demonstrates that utilizing a distinct input format and strategically chosen prompts
    can significantly improve the performance of compressed LLMs. The introduction
    of a prompt learning paradigm, which emphasizes the addition of precise prompts
    over a compressed LLM, has shown to enhance their accuracy, often matching and
    even surpassing that of the original models. The research also highlights the
    transferability of these learned prompts across different datasets, tasks, and
    compression levels, revealing promising avenues for further advancements in scaling
    LLMs on common hardware. The results underline the significance of prudent input
    editing to a compressed large model, potentially revolutionizing the way we approach
    LLM inference on standard hardware platforms.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究展示了一种创新的方法来优化大型语言模型（LLMs）在计算效率和准确性之间的权衡。研究表明，利用一种独特的输入格式和策略性选择的提示可以显著提高压缩LLMs的性能。引入的提示学习范式强调在压缩LLM上添加精确提示，已被证明可以提高其准确性，通常达到甚至超过原始模型的水平。研究还突出了这些学习到的提示在不同数据集、任务和压缩级别之间的可迁移性，揭示了在通用硬件上扩展LLMs的进一步进展的有前景的途径。结果强调了对压缩大模型进行谨慎输入编辑的重要性，这可能会彻底改变我们在标准硬件平台上处理LLM推理的方式。
- en: References
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    and Yejin Choi. PIQA: reasoning about physical commonsense in natural language.
    In *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 7432–7439\. AAAI Press,
    2020. URL [https://ojs.aaai.org/index.php/AAAI/article/view/6239](https://ojs.aaai.org/index.php/AAAI/article/view/6239).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    和 Yejin Choi. PIQA：关于自然语言中物理常识的推理。在*第34届AAAI人工智能大会，AAAI 2020，第32届人工智能创新应用大会，IAAI
    2020，第10届人工智能教育进展AAAI研讨会，EAAI 2020，纽约，NY，美国，2020年2月7-12日*，页7432–7439。AAAI出版社，2020。网址
    [https://ojs.aaai.org/index.php/AAAI/article/view/6239](https://ojs.aaai.org/index.php/AAAI/article/view/6239)。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等人. 语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: 'Chen et al. [2023] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt:
    How to use large language models while reducing cost and improving performance.
    *arXiv preprint arXiv:2305.05176*, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2023] Lingjiao Chen, Matei Zaharia, 和 James Zou. Frugalgpt: 如何在减少成本和提高性能的同时使用大型语言模型。*arXiv
    预印本 arXiv:2305.05176*，2023。'
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    Llm. int8 (): 适用于大规模变换器的 8 位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*，2022。'
- en: 'Ding et al. [2022] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan
    Liu, Haitao Zheng, and Maosong Sun. Openprompt: An open-source framework for prompt-learning.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*, pages 105–113, 2022.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. [2022] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan
    Liu, Haitao Zheng, 和 Maosong Sun. Openprompt: 一个开源的提示学习框架。在*第60届计算语言学协会年会：系统演示*中，第105–113页，2022。'
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot, 2023.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar and Alistarh [2023] Elias Frantar 和 Dan Alistarh. Sparsegpt: 大型语言模型可以在一次训练中准确修剪，2023。'
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. Gptq: 生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022。'
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
    URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, 和 Andy
    Zou. 少样本语言模型评估框架，2021年9月。网址 [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)。
- en: GitHub [2023a] GitHub. [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm),
    2023a.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub [2023a] GitHub. [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)，2023a。
- en: GitHub [2023b] GitHub. [https://github.com/mlc-ai/web-llm](https://github.com/mlc-ai/web-llm),
    2023b.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub [2023b] GitHub. [https://github.com/mlc-ai/web-llm](https://github.com/mlc-ai/web-llm)，2023b。
- en: 'Gugger et al. [2022] S Gugger, L Debut, T Wolf, P Schmid, Z Mueller, and S Mangrulkar.
    Accelerate: Training and inference at scale made simple, efficient and adaptable.
    [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate),
    2022.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gugger et al. [2022] S Gugger, L Debut, T Wolf, P Schmid, Z Mueller, 和 S Mangrulkar.
    Accelerate: 简化、高效和可适应的大规模训练和推理。 [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate)，2022。'
- en: 'He et al. [2018] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. Amc: Automl for model compression and acceleration on mobile devices.
    In *Proceedings of the European conference on computer vision (ECCV)*, pages 784–800,
    2018.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. [2018] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, 和 Song
    Han. Amc: 移动设备上的自动化模型压缩和加速。在*欧洲计算机视觉会议 (ECCV) 论文集*中，第784–800页，2018。'
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, 和 Jacob Steinhardt. 测量大规模多任务语言理解。*arXiv 预印本 arXiv:2009.03300*，2020。
- en: 'Hubara et al. [2021a] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner,
    Joseph Naor, and Daniel Soudry. Accelerated sparse neural training: A provable
    and efficient method to find n: m transposable masks. *Advances in Neural Information
    Processing Systems*, 34:21099–21111, 2021a.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hubara et al. [2021a] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner,
    Joseph Naor, 和 Daniel Soudry. 加速稀疏神经训练：一种可证明且高效的方法来寻找 n: m 可转置掩码。*神经信息处理系统进展*，34:21099–21111，2021a。'
- en: Hubara et al. [2021b] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Accurate post training quantization with small calibration sets.
    In *International Conference on Machine Learning*, pages 4466–4475\. PMLR, 2021b.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. [2021b] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, 和
    Daniel Soudry. 使用小型校准集的准确后训练量化。在*国际机器学习会议*上，第4466–4475页，PMLR，2021b。
- en: Jelinek et al. [1977] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K
    Baker. Perplexity—a measure of the difficulty of speech recognition tasks. *The
    Journal of the Acoustical Society of America*, 62(S1):S63–S63, 1977.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jelinek 等 [1977] Fred Jelinek, Robert L Mercer, Lalit R Bahl, 和 James K Baker.
    困惑度——语音识别任务难度的量度。*美国声学学会期刊*，62(S1):S63–S63，1977年。
- en: Kwon et al. [2022] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun,
    Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers.
    *arXiv preprint arXiv:2204.09656*, 2022.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等 [2022] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt
    Keutzer, 和 Amir Gholami. 一种快速的变换器后训练剪枝框架。*arXiv 预印本 arXiv:2204.09656*，2022年。
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pages 3045–3059, 2021.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等 [2021] Brian Lester, Rami Al-Rfou, 和 Noah Constant. 参数高效提示调优的规模效应。见于
    *2021年自然语言处理经验方法会议论文集*，第3045–3059页，2021年。
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang [2021] Xiang Lisa Li 和 Percy Liang. 前缀调优: 优化生成的连续提示。*arXiv 预印本 arXiv:2101.00190*，2021年。'
- en: 'Liu et al. [2023] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,
    Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, and
    Beidi Chen. Deja vu: Contextual sparsity for efficient llms at inference time.
    In *International Conference on Machine Learning*. PMLR, 2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2023] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, 和 Beidi
    Chen. Déjà vu: 在推理时高效大语言模型的上下文稀疏性。见于 *国际机器学习会议*。PMLR, 2023年。'
- en: Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net, 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter [2019] Ilya Loshchilov 和 Frank Hutter. 解耦权重衰减正则化。见于 *第七届国际学习表征会议,
    ICLR 2019, 新奥尔良, LA, USA, 2019年5月6-9日*。OpenReview.net, 2019。网址 [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)。
- en: 'Marcus et al. [1994] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert
    MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn
    treebank: Annotating predicate argument structure. In *Human Language Technology:
    Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994*, 1994.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marcus 等 [1994] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre,
    Ann Bies, Mark Ferguson, Karen Katz, 和 Britta Schasberger. Penn Treebank: 注释谓词论元结构。见于
    *人类语言技术: 1994年3月8-11日在新泽西州Plainsboro举行的研讨会论文集*，1994年。'
- en: Merity et al. [2017] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings*. OpenReview.net, 2017. URL [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 [2017] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard Socher.
    指针哨兵混合模型。见于 *第五届国际学习表征会议, ICLR 2017, 法国图卢兹, 2017年4月24-26日, 会议论文集*。OpenReview.net,
    2017。网址 [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe)。
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. In *Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing*, pages 2381–2391, 2018.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等 [2018] Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal.
    一套盔甲能导电吗？一个用于开放书籍问答的新数据集。见于 *2018年自然语言处理经验方法会议论文集*，第2381–2391页，2018年。
- en: 'Min et al. [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? *arXiv preprint arXiv:2202.12837*, 2022.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Min 等 [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, 和 Luke Zettlemoyer. 重新思考示范的角色: 什么让上下文学习有效？*arXiv 预印本 arXiv:2202.12837*，2022年。'
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR, 2020.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等 [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos,
    和 Tijmen Blankevoort. 向上还是向下？后训练量化的自适应舍入。见于 *国际机器学习会议*，第7197–7206页。PMLR, 2020年。
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福德等人 [2018] 亚历克·拉德福德、卡尔提克·纳拉西曼、蒂姆·萨利曼斯、伊利亚·苏茨克维尔等。通过生成预训练提高语言理解能力。2018。
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福德等人 [2019] 亚历克·拉德福德、杰弗里·吴、瑞温·查德、大卫·鲁安、达里奥·阿莫代、伊利亚·苏茨克维尔等。语言模型是无监督的多任务学习者。*OpenAI
    博客*，1(8)：9，2019。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉费尔等人 [2020] 科林·拉费尔、诺姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙兰·纳朗、迈克尔·马特纳、阎琦·周、韦·李、彼得·J·刘。通过统一的文本到文本转换器探索迁移学习的极限。*机器学习研究期刊*，21(1)：5485–5551，2020。
- en: Sheng et al. [2023] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez,
    and othersi. High-throughput generative inference of large language models with
    a single gpu. In *International Conference on Machine Learning*. PMLR, 2023.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盛等人 [2023] 盛莹、郑连敏、袁彬航、李卓焕、马克斯·瑞亚宾、丹尼尔·Y·傅、谢智强、陈北笛、克拉克·巴雷特、约瑟夫·E·冈萨雷斯等。利用单个 GPU
    高通量生成推理大语言模型。在*国际机器学习大会*。PMLR，2023。
- en: 'Su et al. [2022] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai
    Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability
    of prompt tuning for natural language processing. In *Proceedings of the 2022
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 3949–3969, 2022.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苏等人 [2022] 苏宇生、王小智、秦宇佳、陈智敏、林彦凯、王华东、温凯越、刘志远、李鹏、李娟子等。关于自然语言处理的提示调优的迁移能力。在*2022年北美计算语言学协会：人类语言技术会议论文集*，第3949–3969页，2022。
- en: 'Tang [2023] Yuxin Tang. Chain-of-thought prompting under streaming batch: A
    case study. *arXiv preprint arXiv:2306.00550*, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唐 [2023] 唐宇欣。流式批处理下的链式思维提示：案例研究。*arXiv 预印本 arXiv:2306.00550*，2023。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图夫隆等人 [2023a] 休戈·图夫隆、蒂博·拉夫里尔、戈特耶·伊扎卡德、谢维耶·马尔蒂内、玛丽-安·拉绍、蒂莫泰·拉克鲁瓦、巴蒂斯特·罗济埃尔、纳曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔等。Llama：开放和高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023a。
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图夫隆等人 [2023b] 休戈·图夫隆、路易斯·马丁、凯文·斯通、彼得·阿尔贝特、阿姆贾德·阿尔马海里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、苏姆雅·巴特拉、普拉贾瓦尔·巴尔加瓦、施鲁提·博萨尔等。Llama
    2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。
- en: Wu et al. [2023] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe
    Liu, and Xin Jin. Fast distributed inference serving for large language models.
    *arXiv preprint arXiv:2305.05920*, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023] 吴秉阳、钟寅敏、张紫力、黄刚、刘轩哲、金欣。大语言模型的快速分布式推理服务。*arXiv 预印本 arXiv:2305.05920*，2023。
- en: 'Xiao et al. [2022] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. *arXiv preprint arXiv:2211.10438*, 2022.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等人 [2022] 肖光轩、林杰、米卡埃尔·塞兹内克、朱利安·德穆斯、宋寒。SmoothQuant：用于大语言模型的准确且高效的后训练量化。*arXiv
    预印本 arXiv:2211.10438*，2022。
- en: Xie et al. [2022] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu
    Ma. An explanation of in-context learning as implicit bayesian inference. In *The
    Tenth International Conference on Learning Representations, ICLR 2022, Virtual
    Event, April 25-29, 2022*. OpenReview.net, 2022. URL [https://openreview.net/forum?id=RdJVFCHjUMI](https://openreview.net/forum?id=RdJVFCHjUMI).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等人 [2022] 桑·迈克尔·谢、阿迪提·拉古纳坦、佩西·梁、滕昱马。将上下文学习解释为隐式贝叶斯推理。在*第十届国际学习表征会议，ICLR 2022，虚拟事件，2022年4月25-29日*。OpenReview.net，2022。网址
    [https://openreview.net/forum?id=RdJVFCHjUMI](https://openreview.net/forum?id=RdJVFCHjUMI)。
- en: Yuan et al. [2022] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri
    Dao, Beidi Chen, Percy S Liang, Christopher Re, and Ce Zhang. Decentralized training
    of foundation models in heterogeneous environments. *Advances in Neural Information
    Processing Systems*, 35:25464–25477, 2022.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袁等人 [2022] 冯航袁、永军何、贾里德·戴维斯、天一张、三道、贝迪·陈、彭西·S·梁、克里斯托弗·雷和泽·张。异质环境下基础模型的去中心化训练。*神经信息处理系统进展*，35:25464–25477，2022年。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800, 2019.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '泽勒斯等人 [2019] 罗温·泽勒斯、阿里·霍尔茨曼、约纳坦·比斯克、阿里·法赫迪和叶锦·崔。Hellaswag: 机器真的能完成你的句子吗？ 在
    *第57届计算语言学协会年会论文集*，第 4791–4800 页，2019年。'
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等人 [2022] 苏珊·张、斯蒂芬·罗勒、纳曼·戈亚尔、米克尔·阿尔特克斯、莫雅·陈、朔辉·陈、克里斯托弗·德万、莫娜·迪亚布、谢安·李、希·维多利亚·林等。Opt:
    开放预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022年。'
- en: Appendix
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A More Experiments
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 更多实验
- en: A.1 Experiment Details
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 实验细节
- en: In the experiment, we employed the AdamW [[21](#bib.bib21)] optimizer as our
    chosen optimizer. We conducted iterative prompt updates using a batch size of
    4, a weight decay of $10^{-5}$. We set the total optimization steps as 30,000
    and use the model corresponding to the best validation perplexity as the final
    model. To facilitate mix-precision training and system-level optimization, we
    leveraged the accelerate library [[11](#bib.bib11)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们使用了 AdamW [[21](#bib.bib21)] 优化器作为我们选择的优化器。我们进行了迭代提示更新，批量大小为 4，权重衰减为
    $10^{-5}$。我们将总优化步骤设置为 30,000，并使用最佳验证困惑度对应的模型作为最终模型。为了促进混合精度训练和系统级优化，我们利用了 accelerate
    库 [[11](#bib.bib11)]。
- en: 'All experiments are conducted on a server with eight Nvidia V100 (32GB) GPUs,
    1.5T main memory, and two Intel Xeon CPU E5-2699A. The software and package version
    is specified below:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验均在配备八块 Nvidia V100 (32GB) GPU、1.5T 主内存和两颗 Intel Xeon CPU E5-2699A 的服务器上进行。软件和包版本如下所示：
- en: 'Table 3: Package configurations of our experiments.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 我们实验的包配置。'
- en: '| Package | Version |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 包 | 版本 |'
- en: '| CUDA | 11.6 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 11.6 |'
- en: '| pytorch | 2.0.1 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| pytorch | 2.0.1 |'
- en: '| transformers | 4.30.0.dev0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| transformers | 4.30.0.dev0 |'
- en: '| accelerate | 0.18.0 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| accelerate | 0.18.0 |'
- en: A.2 Ablation on the Transferability
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 转移性消融
- en: 'In Table [4](#A1.T4 "Table 4 ‣ A.2 Ablation on the Transferability ‣ Appendix
    A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), we conduct the ablation study on
    the transferability of the learned soft prompts using quantized LLaMA-7B on Wikitext2
    and PTB dataset. Specifically, we compare the transferred soft prompts against
    the soft prompts that are trained on the downstream dataset, which serve as the
    top-line counterpart. We observe that directly trained prompts perform better
    than our transferred prompts. However, we note that models with our transferred
    prompts are much closer to the top-line compared to the compressed model without
    prompts, especially for extremely compressed models. This suggests the effectiveness
    of our transferable prompts. We also observe that with learned soft prompt, the
    gap between the full model and quantized model is greatly reduced. For example,
    without learned prompts, the gaps between the full model and 3bit model are 4.72
    (PTB, 11.02 versus 15.74) and 3.12 (Wikitext2, 6.33 versus 9.45). However, after
    adding the learned prompt, the gap was reduced to 0.9 (PTB, 6.86 versus 7.76)
    and 0.75 (Wikitext-2, 5.58 versus 6.33). Also, after adding learned prompts, 4-bit
    quantized can almost match the full model with negligible perplexity drop, which
    highlights the importance of learned prompts.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[4](#A1.T4 "Table 4 ‣ A.2 Ablation on the Transferability ‣ Appendix A More
    Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt")中，我们对使用量化LLaMA-7B在Wikitext2和PTB数据集上的学习软提示的可迁移性进行了消融研究。具体而言，我们将迁移的软提示与在下游数据集上训练的软提示进行比较，后者作为基准。我们观察到，直接训练的提示比我们的迁移提示效果更好。然而，我们注意到，与没有提示的压缩模型相比，具有我们迁移提示的模型更接近基准模型，尤其是对于极度压缩的模型。这表明我们的可迁移提示的有效性。我们还观察到，使用学习的软提示后，完整模型和量化模型之间的差距大大减少。例如，在没有学习提示的情况下，完整模型与3位模型之间的差距分别为4.72（PTB，11.02对15.74）和3.12（Wikitext2，6.33对9.45）。然而，添加学习提示后，这一差距减少到0.9（PTB，6.86对7.76）和0.75（Wikitext-2，5.58对6.33）。此外，添加学习提示后，4位量化模型几乎可以与完整模型匹敌，并且困惑度下降可以忽略不计，这突显了学习提示的重要性。'
- en: 'Table 4: Perplexity comparison between full model and quantized models with
    different prompts, where we report test perplexity on PTB and Wikitext-2 dataset.
    “w./o. prompt” refers to the quantized model without soft prompts.“w./ direct
    prompt” means the soft prompts are directly trained on the target dataset.“w./
    transferred prompt” means the prompt is trained on C4 dataset and then transferred
    to the target dataset.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：完整模型与不同提示的量化模型之间的困惑度比较，我们报告了在PTB和Wikitext-2数据集上的测试困惑度。“w./o. prompt”指的是没有软提示的量化模型。“w./
    direct prompt”指的是软提示直接在目标数据集上训练。“w./ transferred prompt”指的是在C4数据集上训练的提示，然后迁移到目标数据集。
- en: '| Model | PTB | Wikitext2 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | PTB | Wikitext2 |'
- en: '| Full Model | 11.02 | 6.33 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 | 11.02 | 6.33 |'
- en: '| Full Model w./ direct prompt | 6.86 | 5.57 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 w./ direct prompt | 6.86 | 5.57 |'
- en: '| 4-bit |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 4-bit |'
- en: '&#124; w./o. &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./o. &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; prompt &#124;'
- en: '| 11.65 | 6.92 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 11.65 | 6.92 |'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ direct &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ direct &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; prompt &#124;'
- en: '| 7.04 | 5.88 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 7.04 | 5.88 |'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ transferred &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ transferred &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; prompt &#124;'
- en: '| 9.25 | 6.26 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 9.25 | 6.26 |'
- en: '| 3-bit |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit |'
- en: '&#124; w./o. &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./o. &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; prompt &#124;'
- en: '| 15.74 | 9.45 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 15.74 | 9.45 |'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ &#124;'
- en: '&#124; direct prompt &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; direct prompt &#124;'
- en: '| 7.76 | 6.33 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 7.76 | 6.33 |'
- en: '|'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./  transferred &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ transferred &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; prompt &#124;'
- en: '| 10.81 | 6.90 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 10.81 | 6.90 |'
- en: '| 2-bit |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit |'
- en: '&#124; w./o. &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./o. &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; prompt &#124;'
- en: '| 5883.13 | 2692.81 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 5883.13 | 2692.81 |'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ direct &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ direct &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; prompt &#124;'
- en: '| 14.98 | 16.67 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 14.98 | 16.67 |'
- en: '|'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ transferred &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ transferred &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; prompt &#124;'
- en: '| 29.82 | 20.56 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 29.82 | 20.56 |'
- en: A.3 Cross-Task Transferability
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 跨任务迁移性
- en: 'In this section, we explore the transferability of learned prompts across different
    tasks. Specifically, we aim to assess the effectiveness of prompts learned from
    token generation tasks, as indicated by Eq ([1](#S4.E1 "In 4.2 Learning Objectives
    ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")), in
    downstream tasks of LLM. As an illustrative example, we consider the zero-shot
    generalization tasks of LLaMA-7B [[33](#bib.bib33)]. For evaluation purposes,
    we have chosen OpenbookQA [[24](#bib.bib24)], Hellaswag [[39](#bib.bib39)], PIQA [[1](#bib.bib1)],
    and the high school European history task from [[13](#bib.bib13)]. The European
    history task is particularly interesting due to its inclusion of a lengthy context
    sentence for each question. We employ the lm-evaluation-hardness framework [[8](#bib.bib8)],
    incorporating adapters from [[38](#bib.bib38)], for the purpose of conducting
    the experiment.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了学习到的提示在不同任务之间的迁移性。具体而言，我们旨在评估从令牌生成任务中学到的提示的有效性，如公式 ([1](#S4.E1 "在
    4.2 学习目标 ‣ 4 提高 LLM 推理效率的学习提示 ‣ 压缩再提示：通过可迁移提示提升 LLM 推理的准确性-效率权衡")) 所示，在 LLM 的下游任务中的表现。作为一个示例，我们考虑了
    LLaMA-7B 的零-shot 泛化任务 [[33](#bib.bib33)]。为了评估，我们选择了 OpenbookQA [[24](#bib.bib24)]、Hellaswag [[39](#bib.bib39)]、PIQA [[1](#bib.bib1)]
    和来自 [[13](#bib.bib13)] 的高中欧洲历史任务。欧洲历史任务特别有趣，因为每个问题都包含了较长的上下文句子。我们使用了 lm-evaluation-hardness
    框架 [[8](#bib.bib8)]，并结合了来自 [[38](#bib.bib38)] 的适配器，以进行实验。
- en: 'Table [5](#A1.T5 "Table 5 ‣ A.4 Efficiency Profiling ‣ Appendix A More Experiments
    ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") presents the results in terms of normalized accuracy,
    and we also include the standard deviation, as indicated by [[8](#bib.bib8)].
    The table clearly demonstrates that the learned prompt significantly enhances
    the accuracy of these tasks. These findings imply that prompts acquired through
    token generation tasks can effectively enhance the accuracy-efficiency trade-off
    of compressed LLMs.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#A1.T5 "表 5 ‣ A.4 效率分析 ‣ 附录 A 更多实验 ‣ 压缩再提示：通过可迁移提示提升 LLM 推理的准确性-效率权衡")
    显示了归一化准确率的结果，我们还包括了标准差，如 [[8](#bib.bib8)] 所示。表中清楚地展示了学习到的提示显著提高了这些任务的准确性。这些发现表明，通过令牌生成任务获得的提示可以有效提升压缩
    LLM 的准确性-效率权衡。
- en: A.4 Efficiency Profiling
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 效率分析
- en: '![Refer to caption](img/40dad02bab150b86fe9e9b3d94e8ff32.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/40dad02bab150b86fe9e9b3d94e8ff32.png)'
- en: 'Figure 6: Caption'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：标题
- en: 'In this section, we analyze how the inclusion of prompt tokens impacts the
    latency of LLM inference. Figure [6](#A1.F6 "Figure 6 ‣ A.4 Efficiency Profiling
    ‣ Appendix A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") illustrates the latency
    of three OPT models and the LLaMA-7B model utilized in this paper, considering
    the insertion of additional prompt tokens with varying lengths. For token generation,
    we set the sequence length to 1024\. The figure demonstrates that the addition
    of prompt tokens does not significantly increase the latency of LLM inference,
    particularly when the inserted tokens account for less than 10% of the original
    sequence length. Furthermore, our observations indicate that the latency does
    not exhibit a linear correlation with the length of the inserted tokens, highlighting
    the effectiveness of the prompt in facilitating efficient LLM inference.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了提示令牌的引入如何影响 LLM 推理的延迟。图 [6](#A1.F6 "图 6 ‣ A.4 效率分析 ‣ 附录 A 更多实验 ‣ 压缩再提示：通过可迁移提示提升
    LLM 推理的准确性-效率权衡") 展示了本文中使用的三种 OPT 模型和 LLaMA-7B 模型的延迟情况，考虑了插入不同长度的额外提示令牌。对于令牌生成，我们将序列长度设置为
    1024。图中展示了提示令牌的增加并未显著增加 LLM 推理的延迟，特别是当插入的令牌占原始序列长度的比例低于 10% 时。此外，我们的观察表明，延迟与插入令牌的长度之间没有线性相关性，这突显了提示在促进高效
    LLM 推理中的有效性。
- en: 'Table 5: The zero-shot results on transforming the learned prompt to OpenBookQA,
    Hellaswag, PIQA, and High School European History dataset.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：将学习到的提示转化为 OpenBookQA、Hellaswag、PIQA 和高中欧洲历史数据集的零-shot 结果。
- en: '| Models |  | OpenbookQA | Hellaswag | PIQA | High School European History
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |  | OpenbookQA | Hellaswag | PIQA | 高中欧洲历史 |'
- en: '| Full |  | 0.410±0.022 | 0.497±0.005 | 0.702±0.011 | 0.364±0.038 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 全部 |  | 0.410±0.022 | 0.497±0.005 | 0.702±0.011 | 0.364±0.038 |'
- en: '| 50% | w./o. Prompt | 0.412±0.022 | 0.449±0.005 | 0.682±0.011 | 0.364±0.038
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 无提示 | 0.412±0.022 | 0.449±0.005 | 0.682±0.011 | 0.364±0.038 |'
- en: '| + Learned Prompt | 0.400±0.022 | 0.469±0.005 | 0.689±0.011 | 0.358±0.037
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| + 学习提示 | 0.400±0.022 | 0.469±0.005 | 0.689±0.011 | 0.358±0.037 |'
- en: '| 62.5% | w./o. Prompt | 0.396±0.022 | 0.380±0.005 | 0.638±0.011 | 0.345±0.037
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 62.5% | 无提示 | 0.396±0.022 | 0.380±0.005 | 0.638±0.011 | 0.345±0.037 |'
- en: '| + Learned Prompt | 0.402±0.022 | 0.433±0.005 | 0.668±0.011 | 0.345±0.037
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| + 学习提示 | 0.402±0.022 | 0.433±0.005 | 0.668±0.011 | 0.345±0.037 |'
- en: '| 75% | w./o. Prompt | 0.366±0.022 | 0.280±0.004 | 0.549±0.012 | 0.315±0.036
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 75% | 无提示 | 0.366±0.022 | 0.280±0.004 | 0.549±0.012 | 0.315±0.036 |'
- en: '| + Learned Prompt | 0.358±0.021 | 0.344±0.005 | 0.614±0.011 | 0.358±0.037
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| + 学习提示 | 0.358±0.021 | 0.344±0.005 | 0.614±0.011 | 0.358±0.037 |'
- en: '| 4-bit | w./o. Prompt | 0.410±0.022 | 0.487±0.005 | 0.690±0.011 | 0.358±0.037
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 4-bit | 无提示 | 0.410±0.022 | 0.487±0.005 | 0.690±0.011 | 0.358±0.037 |'
- en: '| + Learned Prompt | 0.418±0.022 | 0.487±0.005 | 0.692±0.011 | 0.352±0.037
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| + 学习提示 | 0.418±0.022 | 0.487±0.005 | 0.692±0.011 | 0.352±0.037 |'
- en: '| 3-bit | w./o. Prompt | 0.378±0.022 | 0.446±0.005 | 0.674±0.011 | 0.358±0.037
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | 无提示 | 0.378±0.022 | 0.446±0.005 | 0.674±0.011 | 0.358±0.037 |'
- en: '| + Learned Prompt | 0.404±0.022 | 0.459±0.005 | 0.688±0.011 | 0.358±0.037
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| + 学习提示 | 0.404±0.022 | 0.459±0.005 | 0.688±0.011 | 0.358±0.037 |'
- en: '| 2-bit | w./o. Prompt | 0.354±0.021 | 0.240±0.004 | 0.491±0.012 | 0.315±0.036
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | 无提示 | 0.354±0.021 | 0.240±0.004 | 0.491±0.012 | 0.315±0.036 |'
- en: '| + Learned Prompt | 0.350±0.021 | 0.294±0.005 | 0.563±0.012 | 0.333±0.037
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| + 学习提示 | 0.350±0.021 | 0.294±0.005 | 0.563±0.012 | 0.333±0.037 |'
- en: Appendix B More Visualization
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 更多可视化
- en: 'In this section, we present further visualizations of compression-aware prompts,
    as demonstrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then
    Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt") in Section [1](#S1 "1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"). The
    results unveil a significant improvement achieved by utilizing a hard, task-independent
    prompt on compressed LLMs. Additionally, we showcase the visualization of responses
    generated using our prompt derived from the C4 training set. It is worth noting
    that, in certain instances, the task-independent and learned prompt outperforms
    the hard prompt.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了压缩感知提示的进一步可视化，如图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ 压缩，然后提示：通过可转移提示提高LLM推理的准确性-效率权衡")中所示，位于第[1](#S1
    "1 引言 ‣ 压缩，然后提示：通过可转移提示提高LLM推理的准确性-效率权衡")节。结果揭示了利用硬提示和任务无关提示在压缩LLM上实现的显著改进。此外，我们展示了使用从C4训练集衍生出的提示生成的响应的可视化。值得注意的是，在某些情况下，任务无关提示和学习提示优于硬提示。
- en: '![Refer to caption](img/ced5f317593922f7b67fb5e72a6fd603.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ced5f317593922f7b67fb5e72a6fd603.png)'
- en: 'Figure 7: Case study for the effect of prompts on a pruned LLaMA-7B with a
    62.5% weight sparsity.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：关于提示对剪枝LLaMA-7B（62.5%权重稀疏）的影响的案例研究。
- en: '![Refer to caption](img/79f7ade8f36b17ce587ad6d3370f13d3.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/79f7ade8f36b17ce587ad6d3370f13d3.png)'
- en: 'Figure 8: Case study for the effect of prompts on a pruned LLaMA-7B with a
    4-bit quantization.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：关于提示对剪枝LLaMA-7B的影响的案例研究，量化位数为4位。
