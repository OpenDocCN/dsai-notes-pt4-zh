- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:35'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'VPGTrans: Transfer Visual Prompt Generator across LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.01278](https://ar5iv.labs.arxiv.org/html/2305.01278)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ao Zhang ¹  Hao Fei ¹  Yuan Yao ^(2∗)  Wei Ji ¹  Li Li ¹  Zhiyuan Liu ²  Tat-Seng
    Chua ¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹ NExT++ Lab, School of Computing, National University of Singapore
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Computer Science and Technology, Tsinghua University
  prefs: []
  type: TYPE_NORMAL
- en: 'zhanga6@outlook.com  haofei37@nus.edu.sg  yaoyuanthu@163.com Corresponding
    Author: Hao Fei, Yuan Yao'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since developing a new multimodal LLM (MLLM) by pre-training on a tremendous
    amount of image-text pairs from scratch is exceedingly resource-consuming, connecting
    an existing LLM with a comparatively lightweight visual prompt generator (VPG)
    becomes a feasible paradigm. However, further tuning the VPG component of the
    MLLM still incurs significant computational costs, such as thousands of GPU hours
    and millions of training data points. An alternative solution is to transfer an
    existing VPG from one MLLM to the target MLLM. In this work, we investigate VPG
    transferability across LLMs for the first time, aiming to reduce the cost of VPG
    transfer. Specifically, we explore VPG transfer across different LLM sizes (e.g.,
    small-to-large) and types. We identify key factors to maximize the transfer efficiency,
    based on which we develop a simple yet highly effective two-stage transfer framework,
    called VPGTrans. Notably, it enables VPG transfer from BLIP-2 OPT${}_{\text{2.7B}}$
    from scratch. Furthermore, we provide a series of intriguing findings and discuss
    potential explanations behind them. Finally, we showcase the practical value of
    our VPGTrans approach, by customizing two novel MLLMs, including VL-LLaMA and
    VL-Vicuna, with the recently released LLaMA and Vicuna LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project: [https://vpgtrans.github.io](https://vpgtrans.github.io)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e10d22c7bbd569e204825411dcd9f18f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (a) The general architecture of MLLMs, e.g., BLIP-2 [[30](#bib.bib30)]
    and PaLM-E [[16](#bib.bib16)], including a visual prompt generator (VPG), a linear
    projector and a backbone LLM. Typically, to tune the MLLM, only the VPG and the
    projector are updated, while the LLM is kept frozen. (b) This work investigates
    the VPG transferability across LLMs, including different LLM sizes and LLM types.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Background.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recent years have witnessed a great rise in large-scale language models (LLMs)
    in ushering the human-like artificial intelligence. Text-based LLMs [[42](#bib.bib42),
    [7](#bib.bib7), [44](#bib.bib44)] are further enhanced by associating with other
    modalities such as vision, leading to the multimodal LLMs (MLLMs), such as BLIP-2 [[30](#bib.bib30)],
    Flamingo [[2](#bib.bib2)], GPT-4 [[8](#bib.bib8)] for multimodal dialog system,
    and PaLM-E [[16](#bib.bib16)] for embodied AI system. To construct a MLLM, a visual
    prompt generator (VPG) module (cf. Fig. [1](#S0.F1 "Figure 1 ‣ VPGTrans: Transfer
    Visual Prompt Generator across LLMs")(a)) that produces soft prompts for the input
    images/videos is added¹¹1Also including a linear projector for dimension matching.
    for bridging the gap between vision and language modalities. Currently, such architecture
    has been frequently adopted by many popular MLLMs  [[30](#bib.bib30), [27](#bib.bib27)].
    For example, BLIP-2 pre-trains a CLIP-ViT [[43](#bib.bib43)] combined with a Q-Former
    as VPG. To obtain the final MLLM, the VPG needs to be tuned. Ideally, the vast
    LLM backbone can remain untouched, leaving only the relatively lightweight VPG
    module to be fully or partially updated.²²2 Note that Flamingo also inserts some
    tunable parameters into the LLM part, but recent works [[30](#bib.bib30), [16](#bib.bib16)]
    found that freezing LLM can be more efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: Motivation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: However, building a MLLM is inevitably computation-expensive, due to the huge
    overhead brought by the LLM. For example, training a BLIP-2 FlanT5${}_{\text{XXL}}$
    needs over 600 A100-GPU hours on over 100 million image-text pairs. Hopefully,
    transferring a pre-trained VPG (which is the main body of trainable parts) from
    an existing MLLM to a novel LLM instead of training from scratch,³³3It is not
    a rigorous expression, because the VPG is typically a pre-trained model, like
    CLIP [[43](#bib.bib43)]. We use it for simplicity in this paper. offers a promising
    solution. Intuitively, all the MLLMs literally can share the same VPG infrastructure
    and utility,⁴⁴4while the projector can not be shared due to the dimension mismatch.
    which makes the VPG transfer theoretically feasible. In this work, we thus investigate
    the potential of transferring VPG across LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Proposal.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Specifically, this paper examines the transferability of VPG across LLMs: 1)
    with different sizes (the same type), i.e. , *transfer across LLM sizes*, and
    2) across different LLM types, i.e. , *transfer across LLM type*, as illustrated
    in Fig. [1](#S0.F1 "Figure 1 ‣ VPGTrans: Transfer Visual Prompt Generator across
    LLMs")(b).'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer across LLM Sizes (TaS)]. It has been a typical practice for LLM-related
    research [[8](#bib.bib8)] to validate the training strategy and the hyperparameter
    on smaller models (e.g., OPT${}_{\text{2.7B}}$). It is thus worth exploring whether
    a VPG trained on a smaller LLM can be transferred to a larger LLM, resulting in
    reduced computational costs & data, and maintaining comparable performance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer across LLM Types (TaT)]. With a well-tuned VPG for a type of LLM,
    it is interesting to see if VPG can be transferred to other types of LLMs even
    with different architectures (e.g., decoder v.s. encoder-decoder). If the transfer
    can be achieved, how to make it more efficient?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We conduct a series of exploratory analyses (cf. $\S$). For stage-2, there is
    a vanilla fine-tuning of both the VPG and projector. Despite its simplicity, VPGTrans
    is able to significantly speed up the VPG-transfer process without harming the
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Results and Findings.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Via extensive experiments on the transfer across LLM sizes and types (cf. $\S$[5](#S5
    "5 Exp-II: Transfer across Different Model Types ‣ VPGTrans: Transfer Visual Prompt
    Generator across LLMs")), we gain the following key observations:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VPGTrans helps to avoid the performance drop caused by directly inheriting the
    VPG and achieves at most 10 times acceleration for the small-to-large transfer
    across LLMs in the same type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VPGTrans can also achieve comparable or better performance than training from
    scratch and achieve at most 5 times acceleration for the transfers between different
    model types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notably, our VPGTrans helps to achieve a BLIP-2 ViT-G OPT${}_{\text{2.7B}\rightarrow\text{6.7B}}$
    transfer with less than 10% of GPU hours and 10.7% of training data required for
    the original model training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, our framework can even outperform the original BLIP-2 OPT${}_{\text{6.7B}}$
    on most of the evaluated datasets, with a +2.9 improvement on VQAv2 and a +3.4
    improvement on OKVQA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our investigation further reveals some intriguing findings, for which we provide
    possible explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When conducting TaS from LLM${}_{\text{src}}$[4.2](#S4.SS2.SSS0.Px1 "∙ The
    smaller size of LLM_"src", the easier the transfer. ‣ 4.2 VPGTrans Enabling Faster
    Convergence without Performance Drop under TaS ‣ 4 Exp-I: Transfer across Different
    Model Sizes ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When conducting TaT, efficient VPG transfer can not be achieved between two
    small LLMs with our VPGTrans, due to the large gap between small LLMs’ embedding
    space (cf. $\S$[5.2](#S5.SS2.SSS0.Px1 "∙ There is no speed-up of TaT between two
    small LLMs. ‣ 5.2 VPGTrans Enabling Faster Convergence only on Large LLMs under
    TaT ‣ 5 Exp-II: Transfer across Different Model Types ‣ VPGTrans: Transfer Visual
    Prompt Generator across LLMs")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c98825bbf1a9b4f9d573450188ebffc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparing the cost between *training VPG from scratch* vs. *transferring
    VPG via our VPGTrans strategy*. Note the LLM via VPGTrans is FlanT5${}_{\text{XL}\rightarrow\text{XXL}}$,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Contributions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this study, we show for the first time that effective VPG transfer across
    LLMs can be achieved under most conditions, suggesting that it is possible to
    build a new MLLM with considerably lower computational cost, as seen in Fig. [2](#S1.F2
    "Figure 2 ‣ Results and Findings. ‣ 1 Introduction ‣ VPGTrans: Transfer Visual
    Prompt Generator across LLMs"). To summarize, we make the following key contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective approach. We investigate the key factors for VPG-transfer efficiency
    and propose a two-stage transfer framework VPGTrans. The approach helps to achieve
    a highly-efficient VPG transfer across LLMs with less training data and even task
    improvements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intriguing findings. By exploring the VPG transfer across LLMs, we reveal several
    intriguing findings and provide potential explanations that will shed light on
    further research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open source. We showcase how to customize a novel GPT-4-like MLLM with our
    VPGTrans (cf. $\S$[6](#S6 "6 Customizing New MLLMs with Any LLMs ‣ VPGTrans: Transfer
    Visual Prompt Generator across LLMs")), and release two multimodal-version MLLMs:
    VL-LLaMA and VL-Vicuna. All codes and models is released at [https://github.com/VPGTrans/VPGTrans](https://github.com/VPGTrans/VPGTrans).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will outlines the existing prevailing MLLMs, and elaborates on
    the settings of the exploratory analyses of these MLLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 MLLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Architecture.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [1](#S0.F1 "Figure 1 ‣ VPGTrans: Transfer Visual Prompt
    Generator across LLMs")(a), current MLLMs mostly adopt a common architecture,
    including a visual prompt generator (VPG), a projector, and a backbone LLM. Typically,
    VPG takes images/videos as inputs, and encodes the visual input into a fixed length
    of soft prompts. Then, a linear projector is employed to align the soft prompt’s
    dimension to LLM’s word embedding dimension. Finally, the LLM will generate sentences
    based on the information from the soft prompt. We list some of the recent representative
    MLLMs in Table [1](#S2.T1 "Table 1 ‣ Architecture. ‣ 2.1 MLLM ‣ 2 Preliminary
    ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: MLLMs architectures and pre-training paradigm. ${\dagger}$: it is
    a GPT-2-like LLM with relative position embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: '| MLLMs | VPG | VPG Trainable | LLM | LLM Trainable |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| KOSMOS-1 [[19](#bib.bib19)] | CLIP [[43](#bib.bib43)] | All | Rand. Init.
    LM | All |'
  prefs: []
  type: TYPE_TB
- en: '| Frozen [[54](#bib.bib54)] | NF-ResNet-50 [[6](#bib.bib6)] | NF-ResNet-50
    | GPT-2-like${\dagger}$ [[42](#bib.bib42)] | No |'
  prefs: []
  type: TYPE_TB
- en: '| Flamingo [[2](#bib.bib2)] | NFNet-F6 [[6](#bib.bib6)]+Resampler [[21](#bib.bib21)]
    | Resampler | Chinchilla [[18](#bib.bib18)] | Xattn-Dense |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM-E [[16](#bib.bib16)] | ViT [[15](#bib.bib15)] / OSRT [[49](#bib.bib49)]
    | All | PaLM [[11](#bib.bib11)] | No |'
  prefs: []
  type: TYPE_TB
- en: '| BLIP-2 [[30](#bib.bib30)] | EVA-CLIP [[52](#bib.bib52)] + Q-Former [[30](#bib.bib30)]
    | Q-Former | OPT [[60](#bib.bib60)] / Flan-T5 [[12](#bib.bib12)] | No |'
  prefs: []
  type: TYPE_TB
- en: Training Paradigm.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given a MLLM, typically the VPG and linear projector will be trained, fully
    or partially. For example, PaLM-E updates all of the parameters of VPG in the
    pre-training stage, while BLIP-2 and Flamingo freeze the ViTs and tune their Q-Former
    and Resampler, respectively. As the main part of the whole architecture, the LLM
    is usually frozen during the training or tuned only a small portion (e.g., 10B
    for Flamingo-80B). KOSMOS-1 is an exception, which does not use a pre-trained
    LLM but trains the LLM from scratch. Such a training paradigm typically results
    in much longer training time and data (both multimodal and pure text corpus).
    Recent works [[30](#bib.bib30), [16](#bib.bib16)] show that adopting an existing
    LLM and freezing all of its parameters can also achieve excellent performance
    with significantly reduced computational cost, which leads to the trend of adapting
    frozen pre-trained LLM. For example, BLIP-2 FlanT5${}_{\text{XXL}}$ (12.1B) can
    achieve better zero-shot VQAv2 performance (65.0% in Acc.) compared with KOSMOS-1
    (51.0% in Acc.) and Flamingo-80B (56.3% in Acc.). Thus, in this paper, we mainly
    focus on VPG transfer across frozen LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Experiment Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Architecture.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We adopt BLIP-2’s architecture and training paradigm. In our exploration experiments,
    we consider using the VPG that consists of a CLIP ViT-L/14 [[43](#bib.bib43)],
    and a Q-Former that has already undergone a BLIP-like pre-training (the 1st stage
    pre-training in BLIP-2’s paper [[30](#bib.bib30)]).
  prefs: []
  type: TYPE_NORMAL
- en: Training Data.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For all of the exploration experiments, we adopt human-annotated COCO caption
    dataset [[34](#bib.bib34)] and web image-text pairs SBU dataset [[40](#bib.bib40)],
    which results in 1.4 million image-text pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Direction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the small-to-large model transfer among the same type of LLMs, we investigate:
    1) OPT [[60](#bib.bib60)] (decoder-only) series including 125M, 350M, 1.3B, and
    2.7B, and 2) FlanT5 [[12](#bib.bib12)] (encoder-decoder) ranging *base, large*,
    and *XL*. For the transfer across different types of LLMs, we consider the ones
    of OPT and FlanT5 with similar sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To evaluate the performance of MLLMs, we choose two caption datasets: (1) COCO
    caption [[34](#bib.bib34)] (2) NoCaps [[1](#bib.bib1)], and three VQA datasets:
    (3) VQAv2 [[4](#bib.bib4)] (4) GQA [[20](#bib.bib20)] (5) OKVQA [[37](#bib.bib37)].
    We make evaluations after the pre-training without task-specific fine-tuning and
    report the CIDEr [[55](#bib.bib55)] for all caption tasks and accuracy for all
    VQA tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We follow the same implementation details of BLIP-2, via the open code.⁵⁵5[https://github.com/salesforce/lavis](https://github.com/salesforce/lavis)
    Concretely, we use FP16 and BFloat16 for OPT and FlanT5 respectively in the model
    training. For the learning rate, we first conduct a linear warm-up from 1e-6 to
    1e-4, and then use a cosine learning rate schedule with the minimal $lr$=1e-5
    for 10 epochs. Due to the limited data amount, we slightly decrease the batch
    size, which we find beneficial for the final performance. Specifically, we set
    the batch size of 1,728 and 1,152 for OPT and FlanT5-based models, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Maximizing the Transfer Efficiency with a Two-stage Transfer Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first identify the key factors for maximizing transfer efficiency,
    based on which we then motivate our solution for better transfer.
  prefs: []
  type: TYPE_NORMAL
- en: '3.1 Exploratory Analysis: Identifying Key Factors for VPG Transfer'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Via selected experiments of small-to-large transfer among OPT models, we can
    obtain the following key observations. More systematical comparisons are conducted
    in the later section (cf. $\S$[4](#S4 "4 Exp-I: Transfer across Different Model
    Sizes ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Inheriting the trained VPG can accelerate training.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To demonstrate this, we compare the convergence rates of VPG training on OPT${}_{\text{350M}}$
    accelerates convergence, particularly for two caption tasks. However, for datasets
    that require fine-grained visual perception such as VQAv2 and GQA, directly conduct
    continue training with an inherited VPG will harm the performance. We hypothesize
    that tuning VPG with a randomly initialized projector will compromise the existing
    fine-grained visual perception ability of VPG. The possible reason can be that,
    the VPG is typically a pre-trained model with powerful visual perception ability,
    and thus updating based on the gradient passed through a random projector will
    mislead the VPG at the initial steps [[2](#bib.bib2), [33](#bib.bib33), [23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c531c9ebd46aca43b8811d8449e9e69d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Comparisons between i) inheriting VPG from OPT${}_{\text{125M}}$
    from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c848fcf118a144da640ca85e2ef7243e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: First warming-up then transferring can avoid performance drop on
    VQAv2 and accelerate convergence for COCO caption.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Warming up the linear projector can prevent performance drop and expedite
    VPG training.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To verify this, we first conduct a warm-up training of the linear projector
    for 3 epochs, during which both VPG and LLM are frozen. Subsequently, we jointly
    train VPG and the projector and plot the performance curve in Fig. [4](#S3.F4
    "Figure 4 ‣ ∙ Inheriting the trained VPG can accelerate training. ‣ 3.1 Exploratory
    Analysis: Identifying Key Factors for VPG Transfer ‣ 3 Maximizing the Transfer
    Efficiency with a Two-stage Transfer Strategy ‣ VPGTrans: Transfer Visual Prompt
    Generator across LLMs") (the warm-up process is not included in this figure).
    The results show that the performance drop observed in Fig.[3](#S3.F3 "Figure
    3 ‣ ∙ Inheriting the trained VPG can accelerate training. ‣ 3.1 Exploratory Analysis:
    Identifying Key Factors for VPG Transfer ‣ 3 Maximizing the Transfer Efficiency
    with a Two-stage Transfer Strategy ‣ VPGTrans: Transfer Visual Prompt Generator
    across LLMs") can be avoided in Fig. [4](#S3.F4 "Figure 4 ‣ ∙ Inheriting the trained
    VPG can accelerate training. ‣ 3.1 Exploratory Analysis: Identifying Key Factors
    for VPG Transfer ‣ 3 Maximizing the Transfer Efficiency with a Two-stage Transfer
    Strategy ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs"). Additionally,
    we observe that the warm-up training leads to fewer training steps required for
    VPG and projector joint training. However, we must emphasize that warming up is
    a costly step. In the case of a large LLM, such as 6.7B, the trainable parameters
    of BLIP-2’s VPG will account for less than 10% of the total parameters, where
    freezing VPG can only lead to a reduction of 5.4% of A100 hours (36.9 out of 684.0
    A100 hours). We will elaborate on how to accelerate the linear projector warm-up
    in our later discussion (cf. [3.1](#S3.SS1.SSS0.Px3 "∙ Initializing LLM_"tgt"’s
    projector with the help of the word converter can accelerate the linear projector
    warm-up. ‣ 3.1 Exploratory Analysis: Identifying Key Factors for VPG Transfer
    ‣ 3 Maximizing the Transfer Efficiency with a Two-stage Transfer Strategy ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$’s projector with the help of the word converter can accelerate the
    linear projector warm-up.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In fact, the VPG and projector trained on LLM${}_{\text{src}}$’s soft prompt,
    we can directly get a VPG suitable for LLM[tgt]. One natural idea is to leverage
    the word embeddings of both models as a proxy for the soft prompt [[25](#bib.bib25)].
    The intuition behind the scene is that, the soft prompt works in the same format
    as normal words.
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate our hypothesis, we conduct an experiment on the transfer from OPT${}_{\text{125M}}$
    and converter. As shown in Table [2](#S3.T2 "Table 2 ‣ ∙ Initializing LLM_"tgt"’s
    projector with the help of the word converter can accelerate the linear projector
    warm-up. ‣ 3.1 Exploratory Analysis: Identifying Key Factors for VPG Transfer
    ‣ 3 Maximizing the Transfer Efficiency with a Two-stage Transfer Strategy ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs"), we observe that the initialization
    can reduce the 3 epochs’ warm-up to 2 epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Comparison between linear projector warm-up with/without word embedding
    initialization. The metric is COCO caption’s CIDEr.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Epoch | w/ init. | w/o init. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 130.2 | 126.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 132.7 | 131.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 133.4 | 132.8 |'
  prefs: []
  type: TYPE_TB
- en: $\bullet$ Linear projector warm-up enables faster convergence with an extremely
    large learning rate.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To determine the most efficient transfer practice, we experiment with training
    the projector using different learning rates. Surprisingly, we find that the linear
    projector enables fast and stable convergence with an extremely large learning
    rate. Specifically, by setting the learning rate to 5 times of the original value,
    the COCO caption’s CIDEr score can reach 133.1 with 1 epoch training, which is
    higher than the 3 epochs results of w/o init. as shown in Table [2](#S3.T2 "Table
    2 ‣ ∙ Initializing LLM_"tgt"’s projector with the help of the word converter can
    accelerate the linear projector warm-up. ‣ 3.1 Exploratory Analysis: Identifying
    Key Factors for VPG Transfer ‣ 3 Maximizing the Transfer Efficiency with a Two-stage
    Transfer Strategy ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 A Two-stage VPG Transfer Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By connecting all the dots as discussed above in $\S$[4](#S4 "4 Exp-I: Transfer
    across Different Model Sizes ‣ VPGTrans: Transfer Visual Prompt Generator across
    LLMs") & [5](#S5 "5 Exp-II: Transfer across Different Model Types ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e441b4a5b04217990bd4fbda0fbe49a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Our two-stage VPGTrans framework. Stage-1 is to first (a) inherit
    the VPG of LLM${}_{\text{src}}$ epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\blacktriangleright$ Stage-1: Projector Warm-up.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (a) Inherit VPG. We first initialize the VPG for LLM${}_{\text{tgt}}$.
  prefs: []
  type: TYPE_NORMAL
- en: (b) Projector Initialization. Then, we initialize the projector for LLM${}_{\text{tgt}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The word converter is a linear layer trained with text-only caption data to
    convert the LLM${}_{\text{src}}$. Then, we minimize the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=1-sim(g_{c}(x_{s}),x_{t})\,.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Once we obtain the word converter $g_{c}(\cdot)$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{t}(x)=f_{s}(g_{c}(x))=W_{s}(W_{c}x+b_{c})+b_{s}\,,$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: resulting in $f_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: (c) Warm-up Training. Then, we only train the projector in this stage with a
    frozen VPG and LLM. Specifically, we train the projector for 1 epoch with 5 times
    of the normal learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '$\blacktriangleright$ Stage-2: Vanilla Fine-tuning.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (d) Vanilla Fine-tuning. In the final step, we conduct a joint training of VPG
    and projector for $n$ epochs with a normal learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '4 Exp-I: Transfer across Different Model Sizes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we conduct experiments to systematically illustrate the effectiveness
    of our VPGTrans and analyze the relationship between transfer efficiency and model
    size. For simplicity, we use TaS to represent the transfer across different model
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part, we introduce baselines and transfer variants. For details about
    training data and implementation details, please refer to the experiment settings
    in the Preliminary (cf. [2.2](#S2.SS2 "2.2 Experiment Settings ‣ 2 Preliminary
    ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We mainly compare our VPGTrans with *training from scratch* (TFS) and *VPG inheritance*
    (VPG Inherit), where we report their performance on the aforementioned 5 tasks
    without further task-specific fine-tuning. For our VPGTrans, the word converter
    training only requires updating a linear layer on tokenized text data and typically
    takes less than 10 minutes on 1 A100 GPU with less than 15G GPU memory. Meanwhile,
    freezing the VPG can lead to at least 14 A100 minutes speed-up per epoch. Therefore,
    we consider the whole stage-1 training as the 1st epoch for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Variants.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We conducted experiments on transfer learning using 1) the OPT model across
    four different sizes: 125M, 350M, 1.3B, and 2.7B, and 2) the FlanT5 model across
    three sizes: *base, large*, and *XL*. However, we encountered significant instability
    during training with FlanT5${}_{\text{large}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fdf954f63a32053152df847fabbad374.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparison between different methods across 3 TaS variants on 5 tasks.
    Note that the model is directly evaluated after pre-training without further fine-tuning.
    Please refer to Appendix$\S$[C](#A3 "Appendix C Extended TaS Experiments ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs") for other transfer variants.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 VPGTrans Enabling Faster Convergence without Performance Drop under TaS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First of all, as shown in Fig. [6](#S4.F6 "Figure 6 ‣ Transfer Variants. ‣
    4.1 Experimental Settings ‣ 4 Exp-I: Transfer across Different Model Sizes ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs"), our VPGTrans can consistently
    accelerate the model convergence. For COCO caption and NoCaps that require more
    training steps to converge, our VPGTrans (green line) can be higher than the other
    two lines (blue and orange lines). To give a quantitative evaluation of the speed-up
    rate, we show the speed-up rate in Table [3](#S4.T3 "Table 3 ‣ 4.2 VPGTrans Enabling
    Faster Convergence without Performance Drop under TaS ‣ 4 Exp-I: Transfer across
    Different Model Sizes ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs").
    The speed-up rate is calculated by considering the number of epochs reduced to
    achieve the best TFS performance on a particular dataset. Formally, given a dataset
    $D$ delivers a 10 times speed-up.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, when compared with the VPG inherit in Fig. [6](#S4.F6 "Figure 6 ‣
    Transfer Variants. ‣ 4.1 Experimental Settings ‣ 4 Exp-I: Transfer across Different
    Model Sizes ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs"), our VPGTrans
    can achieve a higher speed-up rate on all of the variants on caption tasks, and
    achieve better performance on most variants except for OPT${}_{\text{1.3B}\rightarrow\text{2.7B}}$[C.3](#A3.SS3
    "C.3 Comparison between VPGTrans and VPG Inherit ‣ Appendix C Extended TaS Experiments
    ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs") for more comparisons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: The speed-up rate of our VPGTrans compared with training from scratch
    (TFS). The symbol "-" means VPGTrans can not achieve better performance than TFS.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transfer | COCO Caption | NoCaps | VQAv2 | GQA | OKVQA |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{125M}\rightarrow\text{350M}}$ | 1.7 | 3.0 | 1.0 | 5.0 | 5.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{125M}\rightarrow\text{1.3B}}$ | 9.0 | 10.0 | 9.0 | - | 2.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{350M}\rightarrow\text{1.3B}}$ | 4.5 | 5.0 | 9.0 | 2.0 | 2.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{125M}\rightarrow\text{2.7B}}$ | 10.0 | 10.0 | 2.0 | 2.0 | 3.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{350M}\rightarrow\text{2.7B}}$ | 10.0 | 10.0 | 2.0 | - | 3.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{1.3B}\rightarrow\text{2.7B}}$ | 3.3 | 3.3 | 2.0 | - | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5${}_{\text{base}\rightarrow\text{XL}}$ | 1.0 | 1.1 | 3.0 | 4.0 | 2.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5${}_{\text{XL}}$ | 5.0 | 5.0 | 2.0 | 2.0 | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{2.7B}}$ | 1.7 | 2.0 | - | 2.0 | - |'
  prefs: []
  type: TYPE_TB
- en: We provide interesting findings with respect to the efficiency transfer by VPGTrans
    in the following.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$, the easier the transfer.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our OPT based experiments, we notice an interesting phenomenon: when transferring
    to a given LLM${}_{\text{tgt}}$ on 3 VQA tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e5bb8da2892fc0f6f6b1d100b87ab3fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The confusion matrix. Only linear layers are trained for VPG evaluation.
    Models are tested on COCO caption with SPICE metric to compare the VPGs trained
    on different LLM[src].'
  prefs: []
  type: TYPE_NORMAL
- en: 'We hypothesize that training VPG on larger OPT will have a worse influence
    on VPG’s existing fine-grained perception ability, which might be caused by the
    enlarging embedding dimensions. To validate our hypothesis, we fix the VPG weight
    and only tune linear projectors to test VPGs trained on different LLM[src] through
    cross-size transfer. The SPICE [[3](#bib.bib3)] metric on COCO caption is used
    to evaluate the VPG’s visual perception ability, where SPICE is specifically designed
    for visual concept perception in captions. As shown in Fig. [7](#S4.F7 "Figure
    7 ‣ ∙ The smaller size of LLM_"src", the easier the transfer. ‣ 4.2 VPGTrans Enabling
    Faster Convergence without Performance Drop under TaS ‣ 4 Exp-I: Transfer across
    Different Model Sizes ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs"),
    for each row, given the LLM[tar], the performance of VPG trained on smaller LLM[src]
    can outperform the larger ones in most conditions, which indicates a better visual
    perception ability of VPG trained on smaller LLM[src]. Therefore, adapting a VPG
    from a smaller OPT model which is less affected, is helpful to take fewer steps
    to reach the TFS’s best performance and achieve even better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Scale-up Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To validate the effectiveness of our VPGTrans on the real-world application
    level, we experiment on transferring from BLIP-2 ViT-G OPT${}_{\text{2.7B}}$[C.4](#A3.SS4
    "C.4 Scale-up Experiment Implementation Details ‣ Appendix C Extended TaS Experiments
    ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs") for implementation
    details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison between models built with our VPGTrans and the original
    BLIP-2 ViT-G OPT${}_{\text{6.7B}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | VQAv2 | GQA | OKVQA | GPU hours | training data |'
  prefs: []
  type: TYPE_TB
- en: '| val | test-dev | test |'
  prefs: []
  type: TYPE_TB
- en: '| BLIP-2 ViT-G OPT${}_{\text{6.7B}}$ | 54.3 | 36.4 | 36.4 | 631.5 | 129M |'
  prefs: []
  type: TYPE_TB
- en: '| BLIP-2 ViT-G OPT${}_{\text{2.7B}\rightarrow\text{6.7B}}$ (ours) | 57.2 |
    36.2 | 39.8 | 59.0 | 13.8M |'
  prefs: []
  type: TYPE_TB
- en: '| VL-LLaMA${}_{\text{7B}}$ (ours) | 58.1 | 37.5 | 37.4 | 67.1 | 13.8M |'
  prefs: []
  type: TYPE_TB
- en: '| BLIP-2 ViT-G FlanT5${}_{\text{XXL}}$ | 65.2 | 44.7 | 45.9 | 684.0 | 121.6M
    |'
  prefs: []
  type: TYPE_TB
- en: '| BLIP-2 ViT-G FlanT5${}_{\text{XL}\rightarrow\text{XXL}}$ (ours) | 65.2 |
    45.0 | 45.0 | 32.4 | 5.3M |'
  prefs: []
  type: TYPE_TB
- en: Speed-up with non-degenerated performances.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Table [4](#S4.T4 "Table 4 ‣ 4.3 Scale-up Experiments ‣ 4 Exp-I:
    Transfer across Different Model Sizes ‣ VPGTrans: Transfer Visual Prompt Generator
    across LLMs"), we can see that (1) OPT${}_{\textbf{2.7B}\rightarrow\textbf{6.7B}}$
    only shows improvement on VQAv2 and GQA. Thus, we do not show a checkpoint with
    more training steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b292bdab4c22437880240a21084779be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Comparison between different methods across 2 TaT variants on 5 tasks.
    Note that the model is directly evaluated after pre-training without further fine-tuning.
    Please refer to Appendix$\S$[D](#A4 "Appendix D Extended TaT Experiments ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs") for other transfer variants.'
  prefs: []
  type: TYPE_NORMAL
- en: '5 Exp-II: Transfer across Different Model Types'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we further investigate the transfer across different model
    types. For simplicity, we mark this type of transfer as TaT.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experimental Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part, we introduce baselines and transfer variants. For details about
    training data and implementation details, please refer to the experiment settings
    in the Preliminary (cf. [2.2](#S2.SS2 "2.2 Experiment Settings ‣ 2 Preliminary
    ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We mainly compare our VPGTrans with *training from scratch* (TFS), and report
    the performance on the aforementioned 5 tasks. Other details (cf. [4.1](#S4.SS1
    "4.1 Experimental Settings ‣ 4 Exp-I: Transfer across Different Model Sizes ‣
    VPGTrans: Transfer Visual Prompt Generator across LLMs")) are totally the same
    with TaS experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Variants.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conducted experiments on transfer learning between 1) OPT${}_{\text{350M}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 VPGTrans Enabling Faster Convergence only on Large LLMs under TaT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ There is no speed-up of TaT between two small LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A finding is that on TaT our VPGTrans does not show speed-up for small models,
    and even shows a degeneration of training speed in the initial several epochs.
    As shown in Fig. [8](#S4.F8 "Figure 8 ‣ Speed-up with non-degenerated performances.
    ‣ 4.3 Scale-up Experiments ‣ 4 Exp-I: Transfer across Different Model Sizes ‣
    VPGTrans: Transfer Visual Prompt Generator across LLMs"), when transferring from
    OPT${}_{\text{350M}}$, the convergence speed of VPGTrans is even slower than TFS
    in the initial several epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Speed-up of VPGTrans happens in large LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: However, when moving to the large LLMs like OPT${}_{\text{2.7B}}$ with our VPGTrans
    are obviously higher than TFS. We hypothesize that larger LLM typically learned
    more generalizable text embeddings and share more similarity among relative word
    distances, which enables an easier VPG transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Customizing New MLLMs with Any LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Above, we thoroughly certify the efficacy of our proposed VPGTrans approach
    for higher efficient transfer of VPG. In this section, we illustrate how to apply
    the VPGTrans framework for VPG transfer to customize new MLLMs with any LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: VL-LLaMA.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'By applying our VPGTrans, we can equip the recently released LLaMA [[53](#bib.bib53)]
    model with a VPG trained on BLIP-2 OPT${}_{\text{6.7B}}$ to perceive the visual
    information. As shown in Table [4](#S4.T4 "Table 4 ‣ 4.3 Scale-up Experiments
    ‣ 4 Exp-I: Transfer across Different Model Sizes ‣ VPGTrans: Transfer Visual Prompt
    Generator across LLMs"), we can see that our VL-LLaMA can outperform the original
    BLIP-2 OPT[6.7B] on all datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21e7e7741dd928af087976692210656a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Comparison between MiniGPT-4 and our VL-Vicuna.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparison between our VL-Vicuna and SOTA MLLMs for multimodal conversation.
    The evaluation is done by Multimodality Chatbot Arena platform via user voting.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | Model | Elo Rating |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | LLaMA-Adapter v2 [[17](#bib.bib17)] | 1023.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | LLaVA [[35](#bib.bib35)] | 1019.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | VL-Vicuna (ours) | 1012.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | MiniGPT-4 [[62](#bib.bib62)] | 1011.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | InstructBLIP [[13](#bib.bib13)] | 999.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | mPLUG-Owl [[59](#bib.bib59)] | 996.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Otter [[26](#bib.bib26)] | 981.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | BLIP-2 [[30](#bib.bib30)] | 955.8 |'
  prefs: []
  type: TYPE_TB
- en: VL-Vicuna.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An exciting application of our VPGTrans is to build a GPT-4 [[8](#bib.bib8)]
    style multimodal conversation chatbot. To achieve our goal, we employ Vicuna [[10](#bib.bib10)]
    as our base LLM. Similarly, we transfer the VPG from BLIP-2 OPT${}_{\text{6.7B}}$[E](#A5
    "Appendix E Extended Results for MLLM Customization ‣ VPGTrans: Transfer Visual
    Prompt Generator across LLMs") for more cases. In addition, we also report the
    ranking and ELO ratings for our VL-Vicuna compared with the other 7 SOTA multimodal
    chatbots in Table [5](#S6.T5 "Table 5 ‣ VL-LLaMA. ‣ 6 Customizing New MLLMs with
    Any LLMs ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs") (The evaluation
    is done by Multimodality Chatbot Arena platform⁶⁶6[http://vlarena.opengvlab.com](http://vlarena.opengvlab.com)).
    The results show the effectiveness of our VL-Vicuna compared with existing SOTA
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we conduct a comprehensive investigation to the problem of VPG
    transferability across LLMs. We first explore the key factors for maximizing the
    transfer efficiency under the VPG transfer across different LLM sizes and types.
    Based on the key findings, we propose a novel two-stage transfer framework, namely
    VPGTrans, which can help to achieve comparable or better performance while significantly
    reducing the training costs. Moreover, a list of important findings and possible
    reasons behind them are shown and discussed. Finally, we demonstrate the practical
    value of our VPGTrans, by customizing new MLLMs via VPG transfer from existing
    MLLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research is supported by NExT++ Lab, Singapore Ministry of Education Academic
    Research Fund Tier 2 under MOE’s official grant number T2EP20221-0023, and CCF-Baidu
    Open Fund.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agrawal et al. [2019] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,
    Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.
    Nocaps: Novel object captioning at scale. In *Proceedings of the IEEE/CVF international
    conference on computer vision*, pages 8948–8957, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alayrac et al. [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,
    Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.
    *Advances in Neural Information Processing Systems*, 35:23716–23736, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anderson et al. [2016] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen
    Gould. Spice: Semantic propositional image caption evaluation. In *Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part V 14*, pages 382–398\. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Antol et al. [2015] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question
    answering. In *Proceedings of the IEEE international conference on computer vision*,
    pages 2425–2433, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2022] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brock et al. [2021] Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan.
    High-performance large-scale image recognition without normalization. In *International
    Conference on Machine Learning*, pages 1059–1071\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. [2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2020] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal
    Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation
    learning. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
    August 23–28, 2020, Proceedings, Part XXX*, pages 104–120. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://vicuna.lmsys.org](https://vicuna.lmsys.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. [2022] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. [2022] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang,
    Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing
    discrete text prompts with reinforcement learning. *arXiv preprint arXiv:2205.12548*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
    Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias
    Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
    Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Driess et al. [2023] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. *arXiv preprint
    arXiv:2303.03378*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2023] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng,
    Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao.
    Llama-adapter v2: Parameter-efficient visual instruction model. *arXiv preprint
    arXiv:2304.15010*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.
    *arXiv preprint arXiv:2203.15556*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2023] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham
    Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al.
    Language is not all you need: Aligning perception with language models. *arXiv
    preprint arXiv:2302.14045*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hudson and Manning [2019] Drew A Hudson and Christopher D Manning. Gqa: A new
    dataset for real-world visual reasoning and compositional question answering.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 6700–6709, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaegle et al. [2021] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
    Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative
    attention. In *International conference on machine learning*, pages 4651–4664\.
    PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. [2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge
    Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In *Computer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXXIII*, pages 709–727\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. [2022] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma,
    and Percy Liang. Fine-tuning can distort pretrained features and underperform
    out-of-distribution. *arXiv preprint arXiv:2202.10054*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. [2022] Brian Lester, Joshua Yurtsever, Siamak Shakeri, and Noah
    Constant. Reducing retraining by recycling parameter-efficient prompts. *arXiv
    preprint arXiv:2208.05577*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023a] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang
    Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning.
    *arXiv preprint arXiv:2305.03726*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023b] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang,
    Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, and Yueting Zhuang. Fine-tuning
    multimodal llms to follow zero-shot demonstrative instructions. *arXiv preprint
    arXiv:2308.04152*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq
    Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language
    representation learning with momentum distillation. *Advances in neural information
    processing systems*, 34:9694–9705, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
    Bootstrapping language-image pre-training for unified vision-language understanding
    and generation. In *International Conference on Machine Learning*, pages 12888–12900\.
    PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023c] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models. *arXiv preprint arXiv:2301.12597*, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023d] Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang,
    Yun Zheng, Deli Zhao, and Yongdong Zhang. Momentdiff: Generative video moment
    retrieval from random to real. In *Advances in neural information processing systems*,
    2023d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023e] Pandeng Li, Chen-Wei Xie, Liming Zhao, Hongtao Xie, Jiannan
    Ge, Yun Zheng, Deli Zhao, and Yongdong Zhang. Progressive spatio-temporal prototype
    matching for text-video retrieval. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 4100–4110, 2023e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lian et al. [2022] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang.
    Scaling & shifting your features: A new baseline for efficient model tuning. *arXiv
    preprint arXiv:2210.08823*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *Computer Vision–ECCV 2014: 13th European Conference,
    Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13*, pages 740–755\.
    Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual
    instruction tuning, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. [2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
    Pretraining task-agnostic visiolinguistic representations for vision-and-language
    tasks. *Advances in neural information processing systems*, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marino et al. [2019] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh
    Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge.
    In *Proceedings of the IEEE/cvf conference on computer vision and pattern recognition*,
    pages 3195–3204, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menick et al. [2022] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides,
    Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
    Geoffrey Irving, et al. Teaching language models to support answers with verified
    quotes. *arXiv preprint arXiv:2203.11147*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mokady et al. [2021] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip
    prefix for image captioning. *arXiv preprint arXiv:2111.09734*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ordonez et al. [2011] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text:
    Describing images using 1 million captioned photographs. *Advances in neural information
    processing systems*, 24, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image
    generation. In *International Conference on Machine Learning*, pages 8821–8831\.
    PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. [2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. Hierarchical text-conditional image generation with clip latents.
    *arXiv preprint arXiv:2204.06125*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2015] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
    r-cnn: Towards real-time object detection with region proposal networks. *Advances
    in neural information processing systems*, 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,
    Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language
    understanding. *Advances in Neural Information Processing Systems*, 35:36479–36494,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sajjadi et al. [2022] Mehdi SM Sajjadi, Daniel Duckworth, Aravindh Mahendran,
    Sjoerd van Steenkiste, Filip Pavetic, Mario Lucic, Leonidas J Guibas, Klaus Greff,
    and Thomas Kipf. Object scene representation transformer. *Advances in Neural
    Information Processing Systems*, 35:9512–9524, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. [2019] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei,
    and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations.
    *arXiv preprint arXiv:1908.08530*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. [2022] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai
    Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability
    of prompt tuning for natural language processing. In *Proceedings of the 2022
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 3949–3969, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2023] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
    Eva-clip: Improved training techniques for clip at scale. *arXiv preprint arXiv:2303.15389*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsimpoukelli et al. [2021] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
    SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen
    language models. *Advances in Neural Information Processing Systems*, 34:200–212,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vedantam et al. [2015] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    CIDEr: Consensus-based image description evaluation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 4566–4575, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang
    Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit
    Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language
    tasks. *arXiv preprint arXiv:2208.10442*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022a] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*,
    2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large
    language models. *arXiv preprint arXiv:2201.11903*, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. [2023] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang
    Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang
    Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.
    mplug-owl: Modularization empowers large language models with multimodality, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2022] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu.
    Learning to prompt for vision-language models. *International Journal of Computer
    Vision*, 130(9):2337–2348, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Vision and Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vision and language models (VLMs) [[50](#bib.bib50), [29](#bib.bib29), [32](#bib.bib32),
    [31](#bib.bib31)] aim at understanding visual and textual information with a single
    model. Previously, the VLM mainly employs a pre-trained object detector as its
    feature extractor and conducts unsupervised training on a huge amount of image-text
    pairs. For example, VL-Bert [[50](#bib.bib50)], ViL-Bert [[36](#bib.bib36)] and
    Uniter [[9](#bib.bib9)] adopt Faster-RCNN [[47](#bib.bib47)] to extract image
    information into object features and take advantage of the masked language model
    as their pre-training task. Later, due to the prevalence of vision transformer
    (ViT), the VLM paradigm is turned into end-to-end training with a ViT as the visual
    encoder. The representative works include ALBEF [[28](#bib.bib28)], BLIP [[29](#bib.bib29)],
    and BEIT-v3 [[56](#bib.bib56)], which show the state-of-the-arts supervised training
    performance on a wide range of downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, LLMs have shown their remarkable capability as zero/few-shot learners [[7](#bib.bib7)]
    and a series of emergent abilities [[57](#bib.bib57)] like in-context learning [[7](#bib.bib7)],
    and chain-of-thoughts reasoning [[58](#bib.bib58)]. A new paradigm, i.e., MLLMs
    is created by associating the VLM or pure vision encoders with LLMs. As we illustrated
    before, the VLM or visual encoders are typically able to convert the input vision
    signals into LLM-understandable soft prompts, and thus we call them VPG. The MLLMs
    advance in inheriting the great potentials of the backbone LLMs, and thus are
    capable of achieving excellent zero/few-shot performances [[2](#bib.bib2), [30](#bib.bib30)]
    on downstream tasks or be equipped with visual planning ability [[16](#bib.bib16)].
    However, connecting the VPG to the existing LLMs with further tuning is costly.
    Even the BLIP-2 [[30](#bib.bib30)], targeted at efficient training, will take
    over 600 A100 GPU hours on over 100M image-text pairs for its largest model. With
    this regard, our proposed VPGTrans can effectively reduce the cost of building
    new MLLMs with the help of existing ones.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompt Transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this paper, we investigate the VPG transfer, where the soft prompt is to
    represent the content of specific inputs like images and videos. In addition to
    the content prompt, the more explored soft prompt is the task prompt [[24](#bib.bib24),
    [61](#bib.bib61), [22](#bib.bib22)], where a sequence of soft prompts are tuned
    to assist the pre-trained models to achieve better performance on specific tasks.
    There have already been some works exploring the transferability of task prompts.
    For example, Su et al. [[51](#bib.bib51)] conducts a series of experiments to
    illustrate the transferability across tasks and models. Specifically, Su et al.
    [[51](#bib.bib51)] find that the transfer between similar tasks is beneficial
    for training speed-up and better performance. Lester et al. [[25](#bib.bib25)]
    proposes to recycle soft prompts across models with vocab-to-vocab transformations,
    or linear-combination transformations. Note that our word converter initialization
    is similar to the idea of vocab-to-vocab transformations. However, we do not observe
    a zero-shot transfer in our experiments like them, which indicates a potential
    difference between the content prompts and task prompts. Another way of soft prompt
    transfer [[14](#bib.bib14)] is to conduct prompt tuning on discrete prompts, and
    thus the discrete prompts can be directly shared across models. Different from
    these task prompts transfer works, our VPG transfer scenario actually suffers
    from fewer limitations. For example, the task soft prompts transfer suffers from
    the dimension change problem, where the main body of the trainable parameters
    should be processed. However, our VPG (the main trainable parameters) can naturally
    be shared among LLMs with different embedding dimensions and leave the dimension
    change problem to a simple projector with ignorable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Extended Findings in Exploratory Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we show extended findings of exploratory analysis (cf. $\S$[3.1](#S3.SS1
    "3.1 Exploratory Analysis: Identifying Key Factors for VPG Transfer ‣ 3 Maximizing
    the Transfer Efficiency with a Two-stage Transfer Strategy ‣ VPGTrans: Transfer
    Visual Prompt Generator across LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ 1\. Merely tuning the projector can not achieve the best performance.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We want to clarify that merely tuning the projector is insufficient for achieving
    the best performance. Notably, as shown in Fig. [10](#A2.F10 "Figure 10 ‣ ∙ 1\.
    Merely tuning the projector can not achieve the best performance. ‣ Appendix B
    Extended Findings in Exploratory Analysis ‣ VPGTrans: Transfer Visual Prompt Generator
    across LLMs"), significant performance gaps are observed between the “*only linear*”
    (green curve) and “*train from scratch*” (orange curve) approaches for COCO caption
    and NoCaps. Therefore, if the goal is to build a multimodal conversation robot
    using carefully collected dialog data, training only the linear projector is insufficient
    to align with the provided data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/654e3a84cef2d31b78761981c5a53f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Comparisons between i) inheriting VPG from OPT${}_{\text{125M}}$
    from scratch. iii) training only the projector.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf258a17c7ad2f738a994055b0ae9317.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Interpreting generated soft prompts for OPT${}_{\text{125M}}$ with
    nearest words.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ 2\. Word embedding converter can not replace a trained linear projector.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As demonstrated in Fig. [11](#A2.F11 "Figure 11 ‣ ∙ 1\. Merely tuning the projector
    can not achieve the best performance. ‣ Appendix B Extended Findings in Exploratory
    Analysis ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs"), we observe
    a common pattern: the last token of soft prompts is closest to EOS, while the
    middle tokens represent the image content. Such a phenomenon indicates a similarity
    between soft prompts and word embeddings. However, they are not identical. For
    instance, the norm of soft prompts is typically around 10 times the average norm
    of word embeddings. It is important to note that the linear projector initialization
    cannot replace the warm-up training. Using only the linear projector initialization
    even yields a random performance. We believe that a better understanding of how
    prompt works will further benefit the VPG’s transfer learning.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ 3\. The projector warm-up is robust to a larger learning rate, while
    VPG can not.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first thing we want to clarify is that the 5 times normal learning rate
    will result in a training crash for VPG. Additionally, we find that although increasing
    the learning rate to 10 times in the projector warm-up does not yield any additional
    acceleration, the projector can converge without crashing during training.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Extended TaS Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we first illustrate extending findings of TaS experiments
    (cf. $\S$[4](#S4 "4 Exp-I: Transfer across Different Model Sizes ‣ VPGTrans: Transfer
    Visual Prompt Generator across LLMs")). Then, we introduce the implementation
    details of scale-up experiments. We also plot a more complete version of Fig. [6](#S4.F6
    "Figure 6 ‣ Transfer Variants. ‣ 4.1 Experimental Settings ‣ 4 Exp-I: Transfer
    across Different Model Sizes ‣ VPGTrans: Transfer Visual Prompt Generator across
    LLMs") in Fig. [13](#A3.F13 "Figure 13 ‣ Appendix C Extended TaS Experiments ‣
    VPGTrans: Transfer Visual Prompt Generator across LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c66f90135717129d8066c3fc6c804b49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Comparison of training FlanT5${}_{\text{large}}$ using different
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc3413229c4cf75281582487c6aaa1a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Comparison between different methods across 7 TaS variants on 5
    tasks. Note that the model is directly evaluated after pre-training without further
    fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: C.1 VPGTrans Enabling Stable Training under TaS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we illustrated before, FlanT5${}_{\text{large}}$. We plot the performance
    curve of the COCO caption in Fig. [12](#A3.F12 "Figure 12 ‣ Appendix C Extended
    TaS Experiments ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: C.2 VPGTrans Enabling Training with Less Data under TaS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We empirically find that TaS can reduce the requirement for the amount of training
    data. By reducing the training data to only COCO, we find no obvious performance
    drop. However, we want to stress that the retained data should be of high quality,
    which means that if the same number of SBU data is retained, the performance especially
    for captioning will drop. The conclusion can also be found in Table [4](#S4.T4
    "Table 4 ‣ 4.3 Scale-up Experiments ‣ 4 Exp-I: Transfer across Different Model
    Sizes ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs"). We refer the
    readers to the next subsection for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: C.3 Comparison between VPGTrans and VPG Inherit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Fig. [13](#A3.F13 "Figure 13 ‣ Appendix C Extended TaS Experiments
    ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs"), the green line (our
    VPGTrans) can be higher than the orange line (VPG inherit) for the majority of
    various conditions. Especially when considering the best performance of different
    tasks, our VPGTrans can achieve better performance than VPG inherit (non “-” in
    Table [6](#A3.T6 "Table 6 ‣ C.3 Comparison between VPGTrans and VPG Inherit ‣
    Appendix C Extended TaS Experiments ‣ VPGTrans: Transfer Visual Prompt Generator
    across LLMs")) for over 74% Transfer-Task variants. Moreover, among the variants
    that our VPGTrans can achieve better performance, our VPGTrans can also achieve
    a speed-up on 69.2% conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The speed-up rate of our VPGTrans compared with VPG inherit. The symbol
    "-" means VPGTrans can not achieve better performance than VPG inherit.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transfer | COCO Caption | NoCaps | VQAv2 | GQA | OKVQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{125M}\rightarrow\text{350M}}$ | 1.1 | 2.7 | 10.0 | 6.0 | 6.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{125M}\rightarrow\text{1.3B}}$ | - | 2.7 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{350M}\rightarrow\text{1.3B}}$ | - | 1.7 | 1.0 | 2.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{125M}\rightarrow\text{2.7B}}$ | 3.0 | 3.0 | 3.0 | 1.0 | 3.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{350M}\rightarrow\text{2.7B}}$ | 3.3 | 4.5 | 1.0 | 1.0 | 1.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{1.3B}\rightarrow\text{2.7B}}$ | 2.3 | 3.0 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5${}_{\text{base}\rightarrow\text{XL}}$ | - | 1.0 | 1.8 | 9.0 | 0.4
    |'
  prefs: []
  type: TYPE_TB
- en: C.4 Scale-up Experiment Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The scale-up experiment refers to the results in Table [4](#S4.T4 "Table 4
    ‣ 4.3 Scale-up Experiments ‣ 4 Exp-I: Transfer across Different Model Sizes ‣
    VPGTrans: Transfer Visual Prompt Generator across LLMs"). We try to imitate BLIP-2’s
    pre-training data composition. First of all, two human-annotated datasets COCO
    and VG are used. SBU is also used. Then, BLIP-2 uses BLIP to generate captions
    for the 115M web images and rank them with CLIP ViT-L/14. We also adopt similar
    synthetic data from Laion-COCO.⁷⁷7[https://laion.ai/blog/laion-coco](https://laion.ai/blog/laion-coco)
    We report the concrete number of data we use in Table [4](#S4.T4 "Table 4 ‣ 4.3
    Scale-up Experiments ‣ 4 Exp-I: Transfer across Different Model Sizes ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs"). For the stage-1 training, we keep
    the same as the previous validation experiments where COCO and SBU are used for
    warm-up with a 5 times the learning rate. Then, we use COCO, VG, and Laion-COCO
    for the stage-2 training. Note that we have tried to include Laion-COCO and VG
    for the stage-1 training, but found no obvious difference and thus use COCO and
    SBU for simplicity. For VL-Vicuna, to align with the conversation scenario, we
    further fine-tune our VL-Vicuna with MiniGPT-4’s self-instruct data, which is
    3,439 image-text pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Extended TaT Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we mainly illustrate extending findings of TaT experiments
    (cf. $\S$[5](#S5 "5 Exp-II: Transfer across Different Model Types ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs")). We plot a more complete version
    of Fig. [8](#S4.F8 "Figure 8 ‣ Speed-up with non-degenerated performances. ‣ 4.3
    Scale-up Experiments ‣ 4 Exp-I: Transfer across Different Model Sizes ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs") in Fig. [14](#A4.F14 "Figure 14
    ‣ D.1 Linear Transfer Gap between Different LLM’s Visual Prompts ‣ Appendix D
    Extended TaT Experiments ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Comparison between tuning only the projector (i.e. linear transfer)
    and the best results the VPGTrans achieve.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transfer | COCO Caption | VQAv2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| linear | best | linear | best |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5${}_{\text{base}}$ | 110.8 | 136.5 | 40.4 | 44.2 |'
  prefs: []
  type: TYPE_TB
- en: '| FlanT5${}_{\text{XL}}$ | 132.1 | 139.3 | 50.4 | 50.3 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{350M}}$ | 1.1 | 122.1 | 34.0 | 49.9 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT${}_{\text{2.7B}}$ | 106.5 | 133.2 | 51.3 | 53.5 |'
  prefs: []
  type: TYPE_TB
- en: D.1 Linear Transfer Gap between Different LLM’s Visual Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As illustrated in Section [5.2](#S5.SS2.SSS0.Px1 "∙ There is no speed-up of
    TaT between two small LLMs. ‣ 5.2 VPGTrans Enabling Faster Convergence only on
    Large LLMs under TaT ‣ 5 Exp-II: Transfer across Different Model Types ‣ VPGTrans:
    Transfer Visual Prompt Generator across LLMs"), it is more difficult to transfer
    between two small LLMs with our VPGTrans due to the weaker linear transferability
    between two small LLMs’ visual prompts. To better support our results, we compare
    the results of tuning only the projector (i.e. linear transfer) and the best results
    the VPGTrans can achieve. As shown in Table [7](#A4.T7 "Table 7 ‣ Appendix D Extended
    TaT Experiments ‣ VPGTrans: Transfer Visual Prompt Generator across LLMs"), we
    can see that when conducting transfers between two large models (OPT${}_{\text{350M}}$’s
    linear and best. If considering the transfer between the small to large LLMs under
    TaT, both the VPG’s visual perception ability and transfer gap should be considered.
    We leave the systematical exploration for future works.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e88ba335b5916cfd657ff191b57fe72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Comparison between different methods across 4 TaS variants on 5
    tasks. Note that the model is directly evaluated after pre-training without further
    fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Extended Results for MLLM Customization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We show more comparisons between VL-Vicuna and MiniGPT-4 in Fig. [15](#A5.F15
    "Figure 15 ‣ Appendix E Extended Results for MLLM Customization ‣ VPGTrans: Transfer
    Visual Prompt Generator across LLMs"). We can see that our VL-Vicuna has better
    visual perception ability. For example, when MiniGPT-4 falsely recognizes the
    three people in the image as two in the first example, our VL-Vicuna can not only
    recognize the number of people but also tell their roles. Moreover, our VL-Vicuna
    can successfully link the vision content with external knowledge. In the third
    example, our VL-Vicuna can recognize Leonardo and link the content with his films
    like Titanic.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df847c3a5a5c29e49b02fc693cd9b0ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Comparison between MiniGPT-4 and our VL-Vicuna.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Potential Impact and Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our VPGTrans is designed for building new MLLMs with lower computational cost,
    i.e., shorter training time and less training data. With an already pre-trained
    MLLM, VPGTrans enables fast VPG transfer to build either a larger MLLM or a MLLM
    with a different type of LLM. We hope VPGTrans can facilitate teams in LLM communicty
    to customize their MLLMs with reduced cost. There are also possible limitations
    of the current version of VPGTrans. The first one is that our VPGTrans should
    rely on some already-aligned VPGs. The second potential limitation is that the
    VPGTrans-built MLLMs still suffer from the common problems of content generation
    AI systems [[45](#bib.bib45), [46](#bib.bib46), [48](#bib.bib48), [7](#bib.bib7)].
    For example, the VL-Vicuna may make up some sentences with falsely recognized
    visual facts, like what is shown in Fig. [16](#A6.F16 "Figure 16 ‣ Appendix F
    Potential Impact and Limitations ‣ VPGTrans: Transfer Visual Prompt Generator
    across LLMs"). It is worth exploring associating our VPGTrans with training safer
    models [[5](#bib.bib5), [41](#bib.bib41), [38](#bib.bib38)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26166cc2c916f5ed2b3874576ec44c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Failure cases of our VL-Vicuna.'
  prefs: []
  type: TYPE_NORMAL
