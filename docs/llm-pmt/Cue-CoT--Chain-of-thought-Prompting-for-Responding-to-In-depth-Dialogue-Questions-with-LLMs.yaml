- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:51:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:51:09'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Cue-CoT: 针对深度对话问题的链式思维提示（Chain-of-thought Prompting）在大语言模型（LLMs）中的应用'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11792](https://ar5iv.labs.arxiv.org/html/2305.11792)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11792](https://ar5iv.labs.arxiv.org/html/2305.11792)
- en: Hongru Wang¹ , Rui Wang^(2,6∗), Fei Mi³, Yang Deng⁴, Zezhong Wang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hongru Wang¹ , Rui Wang^(2,6∗), Fei Mi³, Yang Deng⁴, Zezhong Wang¹
- en: Bin Liang¹, Ruifeng Xu^(2,5,6), Kam-Fai Wong¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Bin Liang¹, Ruifeng Xu^(2,5,6), Kam-Fai Wong¹
- en: ¹MoE Key Laboratory of High Confidence Software Technologies
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹MoE 高可信软件技术重点实验室
- en: The Chinese University of Hong Kong
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 香港中文大学
- en: ²Harbin Institute of Technology, Shenzhen, China ³Huawei Noah’s Ark Lab
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²哈尔滨工业大学（深圳），中国 ³华为诺亚方舟实验室
- en: ⁴National University of Singapore ⁵Peng Cheng Laboratory, Shenzhen, China
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴新加坡国立大学 ⁵鹏城实验室，中国深圳
- en: ⁶Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies
    {hrwang, kfwong}@se.cuhk.edu.hk ruiwangnlp@outlook.com   Equal Contribution.  Corresponding
    Author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁶广东省新型安全智能技术重点实验室 {hrwang, kfwong}@se.cuhk.edu.hk ruiwangnlp@outlook.com   平等贡献。  通讯作者。
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs), such as ChatGPT, greatly empower dialogue systems
    with strong language understanding and generation capabilities. However, most
    of the previous works prompt the LLMs to directly generate a response based on
    the dialogue context, overlooking the underlying linguistic cues about the user
    status exhibited in the context. Such in-depth dialogue scenarios are challenging
    for existing LLMs to figure out the user’s hidden needs and respond satisfactorily
    through a single-step inference. To this end, we propose a novel linguistic cue-based
    chain-of-thoughts (Cue-CoT), which enhances the LLMs inference with an intermediate
    reasoning step to find cues exhibited in the dialogue, aiming to provide a more
    personalized and engaging response. To evaluate the approach, we build a benchmark
    with in-depth dialogue questions, consisting of 6 datasets in both Chinese and
    English, targeting 3 major linguistic cues during the conversation: personality,
    emotion, and psychology. We conduct extensive experiments on the proposed benchmark
    with 5 LLMs under both zero-shot and one-shot settings. Empirical results demonstrate
    our proposed Cue-CoT method outperforms standard prompting methods in terms of
    both helpfulness and acceptability on all datasets.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs），如 ChatGPT，大大增强了对话系统的语言理解和生成能力。然而，大多数现有工作直接提示 LLMs 基于对话上下文生成回应，却忽视了上下文中表现出的关于用户状态的潜在语言线索。这类深度对话场景对现有
    LLMs 来说是具有挑战性的，因为它们难以通过单步推理确定用户的隐藏需求并作出令人满意的回应。为此，我们提出了一种新颖的基于语言线索的链式思维（Cue-CoT），它通过中间推理步骤来寻找对话中展现的线索，旨在提供更个性化和引人入胜的回应。为了评估这种方法，我们建立了一个包含深度对话问题的基准，包括中英文6个数据集，目标是对话中的3种主要语言线索：个性、情感和心理。我们在提出的基准上对5个
    LLMs 进行了广泛的实验，涵盖零样本和单样本设置。实证结果表明，我们提出的 Cue-CoT 方法在所有数据集上的有用性和接受度方面均优于标准提示方法。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs), or foundation models Zhou et al. ([2023](#bib.bib60)),
    especially after the appearance of ChatGPT¹¹1https://openai.com/blog/chatgpt,
    recently revolutionize the paradigm of various natural language processing (NLP)
    tasks, including dialogue response generation tasks Bang et al. ([2023](#bib.bib1)).
    However, most existing LLM-based studies directly feed the user query or dialogue
    content to the LLM for generating a response with a preceding prompt, making the
    responses stereotypical and tedious, especially for in-depth dialogue questions
    Zhao et al. ([2023](#bib.bib55)). On the contrary, it is widely acknowledged that
    dialogue contexts generally convey a lot of information about the user status
    in addition to the pure semantic information from a linguistic perspective Mairesse
    et al. ([2007](#bib.bib26)); Tausczik and Pennebaker ([2010](#bib.bib41)); Schwartz
    et al. ([2013](#bib.bib34)). Specifically, the linguistic cues underlying dialogue
    context have been shown to be an effective means of revealing the emotions Ekman
    ([1971](#bib.bib15)), personality traits Mairesse et al. ([2007](#bib.bib26)),
    psychological characteristics Tausczik and Pennebaker ([2010](#bib.bib41)), and
    other relevant information of users Turney ([2002](#bib.bib42)); Newman et al.
    ([2003](#bib.bib28)). Consequently, recognizing and understanding these cues exhibited
    in the context of dialogues becomes crucial to comprehend user intentions and
    status Rashkin et al. ([2019](#bib.bib31)). By doing so, a dialogue system can
    generate responses that align with the user’s expectations, meet their unique
    needs and preferences, and ultimately strive towards constructing a human-like
    conversational experience Salemi et al. ([2023](#bib.bib33)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），或基础模型 Zhou et al. ([2023](#bib.bib60))，特别是在 ChatGPT¹¹1https://openai.com/blog/chatgpt
    出现之后，最近革新了各种自然语言处理（NLP）任务的范式，包括对话响应生成任务 Bang et al. ([2023](#bib.bib1))。然而，大多数现有的基于
    LLM 的研究直接将用户查询或对话内容输入 LLM，并通过前置提示生成响应，使得这些响应刻板而单调，特别是对于深度对话问题 Zhao et al. ([2023](#bib.bib55))。相反，广泛认可的是，对话上下文通常除了语言学角度的纯语义信息外，还传达了大量关于用户状态的信息
    Mairesse et al. ([2007](#bib.bib26)); Tausczik 和 Pennebaker ([2010](#bib.bib41));
    Schwartz et al. ([2013](#bib.bib34))。具体而言，潜在的对话上下文中的语言线索已被证明是揭示情感 Ekman ([1971](#bib.bib15))、个性特征
    Mairesse et al. ([2007](#bib.bib26))、心理特征 Tausczik 和 Pennebaker ([2010](#bib.bib41))
    以及用户的其他相关信息 Turney ([2002](#bib.bib42)); Newman et al. ([2003](#bib.bib28)) 的有效手段。因此，识别和理解这些在对话上下文中展现的线索对于理解用户意图和状态变得至关重要
    Rashkin et al. ([2019](#bib.bib31))。通过这样做，对话系统可以生成符合用户期望的响应，满足他们的独特需求和偏好，并最终努力构建类似于人类的对话体验
    Salemi et al. ([2023](#bib.bib33))。
- en: '![Refer to caption](img/19647eba3c54b194baabc083fac7b7c5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/19647eba3c54b194baabc083fac7b7c5.png)'
- en: 'Figure 1: An example of different prompting for responding to in-depth dialog
    questions with LLMs, including standard prompting, O-Cue CoT, and M-Cue CoT. We
    shadow the intermediate reasoning results, i.e., the personality, empathy, and
    psychological status of the user, and highlight the instructions at the input
    and indicate the roles of different parts of the response (in green) in M-Cue
    CoT.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：展示了不同的提示示例，用于回应深度对话问题，包括标准提示、O-Cue CoT 和 M-Cue CoT。我们跟踪中间推理结果，即用户的个性、同理心和心理状态，并在
    M-Cue CoT 中突出显示输入的指令，并标明响应的不同部分的角色（以绿色显示）。
- en: 'Inspired by the emergent capabilities of LLMs on reasoning over texts Wei et al.
    ([2022](#bib.bib49)), while previous efforts mostly investigate in-context learning
    Huang et al. ([2022](#bib.bib19)) or chain-of-thought prompting schemes Wei et al.
    ([2023](#bib.bib50)) on text classification Zhang et al. ([2022](#bib.bib54))
    and question answering Press et al. ([2022](#bib.bib30)); Shum et al. ([2023](#bib.bib36));
    Margatina et al. ([2023](#bib.bib27)), we take advantage of LLMs to reason for
    dialogue (Lee et al., [2023](#bib.bib21)), which, to our best knowledge, has not
    been thoroughly investigated yet. Specifically, we design a linguistic cue-based
    chain-of-thoughts (Cue-CoT), consisting of two variants: O-Cue CoT and M-Cue CoT
    in which the former one outputs intermediate reasoning results with a final response
    in one-step but the latter reasons step by step, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Cue-CoT: Chain-of-thought Prompting for Responding
    to In-depth Dialogue Questions with LLMs"). In detail, with standard prompting,
    LLM-based systems directly generate the response given the dialogue context. Regarding
    the user status implied by the context as intermediate reasoning results (Cue
    CoT), we prompt the system to infer the user status first and then generate a
    response based on dialogue context and user status.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '受到LLMs在文本推理上新兴能力的启发（Wei 等，2022），虽然之前的研究主要集中在上下文学习（Huang 等，2022）或链式思维提示方案（Wei
    等，2023）在文本分类（Zhang 等，2022）和问答（Press 等，2022；Shum 等，2023；Margatina 等，2023）上的应用，我们利用LLMs进行对话推理（Lee
    等，2023），这是我们所知尚未被彻底研究的。具体而言，我们设计了一种基于语言提示的链式思维（Cue-CoT），包括两种变体：O-Cue CoT 和 M-Cue
    CoT，其中前者在一步中输出中间推理结果和最终回应，而后者则逐步推理，如图[1](#S1.F1 "图1 ‣ 1 介绍 ‣ Cue-CoT: 针对深入对话问题的链式思维提示").
    详细而言，使用标准提示时，基于LLM的系统直接根据对话上下文生成回应。对于上下文中暗示的用户状态作为中间推理结果（Cue CoT），我们首先提示系统推断用户状态，然后根据对话上下文和用户状态生成回应。'
- en: 'To evaluate our approach, we build a benchmark, consisting of 6 in-depth dialogue
    datasets in both Chinese and English, considering three major aspects of user
    statuses: personality, emotions, and psychology, exhibited during the conversation,
    forming a comprehensive evaluation benchmark incorporating various user statuses
    in the context of dialogue response generation. We conduct extensive experiments
    with 5 LLM-based dialogue systems based on the benchmark using the aforementioned
    three types of prompting schemes. To sum up, our contributions can be summarized
    below:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的方法，我们建立了一个基准测试，包括6个中英文对话数据集，考虑了对话中展示的用户状态的三个主要方面：个性、情感和心理状态，形成了一个全面的评估基准，涵盖了对话回应生成中的各种用户状态。我们使用上述三种提示方案，在基准测试的基础上对5个基于LLM的对话系统进行了广泛实验。总结而言，我们的贡献可以归纳如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We construct an in-depth dialogue evaluation benchmark considering the personality,
    emotion, and psychology of users exhibited in the conversation, with the goal
    of aligning with unique user needs and status, which consists of 6 datasets, and
    7.3k dialogues²²2Our dataset and demo are released here: [https://github.com/ruleGreen/Cue-CoT](https://github.com/ruleGreen/Cue-CoT)..'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们构建了一个深入的对话评估基准，考虑了对话中展示的用户个性、情感和心理状态，旨在与独特的用户需求和状态对齐，包括6个数据集和7.3k对话²²2我们的数据集和示例已发布在这里：[https://github.com/ruleGreen/Cue-CoT](https://github.com/ruleGreen/Cue-CoT)。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We propose two effective dialogue cots: O-Cue CoT and M-Cue CoT, that enable
    advanced reasoning and planning based on user statuses. Additionally, we suggest
    utilizing intermediate reasoning results as a criterion for selecting demonstrations
    in limited training data scenarios, specifically in one-shot settings.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了两种有效的对话链式推理（CoT）：O-Cue CoT 和 M-Cue CoT，这些方法能够基于用户状态实现高级推理和规划。此外，我们建议利用中间推理结果作为在有限训练数据场景中选择示例的标准，特别是在单次设置中。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our findings demonstrate that both the O-Cue CoT and M-Cue CoT approaches outperform
    standard prompting in generating more helpful and acceptable responses for the
    users. Specifically, the M-Cue CoT shows superior robustness and reasoning performance
    in all datasets and all LLMs. Furthermore, our novel demonstration selection strategy
    exhibits superior performance under both random selection and top-1 selection.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的发现表明，O-Cue CoT 和 M-Cue CoT 方法在生成更有帮助和可接受的用户响应方面都优于标准提示方法。具体来说，M-Cue CoT 在所有数据集和所有
    LLMs 中展现出更强的鲁棒性和推理性能。此外，我们新颖的演示选择策略在随机选择和 top-1 选择下均表现出优越的性能。
- en: 2 Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Chain-of-thought Prompt. Following the initial chain-of-thought prompting Wei
    et al. ([2023](#bib.bib50)), lots of works spring up aim to improve different
    parts of original reasoning processing, including auto-cot Zhang et al. ([2022](#bib.bib54)),
    self-consistencyWang et al. ([2023e](#bib.bib48)), active prompt Diao et al. ([2023](#bib.bib13)),
    automate-cot Shum et al. ([2023](#bib.bib36)). Besides that, a further line of
    work studies in-context learning Brown et al. ([2020](#bib.bib3)) as its efficiency
    and effectiveness with LLMs as backbones in which the key of it is to select informative
    demonstrations to prepend the input as additional information to get better results
    Liu et al. ([2022](#bib.bib23)). To find the best demonstrations and unleash LLMs’
    power, Liu et al. ([2022](#bib.bib23)) propose to retrieve examples that are semantically
    similar to a test query sample while some works utilize uncertainty Diao et al.
    ([2023](#bib.bib13)) or diversity Li and Qiu ([2023](#bib.bib22)) to refine and
    evaluate the selected examples. Also, few works Deng et al. ([2023a](#bib.bib9))
    focus on the intermediate reasoning steps, and they use the reasoning complexity
    Fu et al. ([2023](#bib.bib16)), i.e., chains with more reasoning steps, making
    the effective demonstration.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 连锁思维提示。继最初的连锁思维提示 Wei 等人 ([2023](#bib.bib50)) 之后，许多工作涌现出来，旨在改进原始推理处理的不同部分，包括
    auto-cot Zhang 等人 ([2022](#bib.bib54))、self-consistency Wang 等人 ([2023e](#bib.bib48))、active
    prompt Diao 等人 ([2023](#bib.bib13))、automate-cot Shum 等人 ([2023](#bib.bib36))。此外，还有一类工作研究了在-context
    学习 Brown 等人 ([2020](#bib.bib3))，因其在以 LLMs 作为骨干时的效率和有效性，其中的关键是选择信息丰富的示例作为附加信息预先添加到输入中以获得更好的结果
    Liu 等人 ([2022](#bib.bib23))。为了找到最佳示例并释放 LLMs 的潜力，Liu 等人 ([2022](#bib.bib23)) 提出检索与测试查询样本语义相似的例子，而一些工作则利用不确定性
    Diao 等人 ([2023](#bib.bib13)) 或多样性 Li 和 Qiu ([2023](#bib.bib22)) 来细化和评估所选示例。此外，少数工作
    Deng 等人 ([2023a](#bib.bib9)) 关注中间推理步骤，他们使用推理复杂性 Fu 等人 ([2023](#bib.bib16))，即具有更多推理步骤的链，以构建有效的示例。
- en: Dialogue System. Most of the previous work develops personalized Zhang et al.
    ([2018](#bib.bib53)); Zheng et al. ([2020](#bib.bib58)); Song et al. ([2021](#bib.bib38));
    Chen et al. ([2023a](#bib.bib4)), emotional Ghosal et al. ([2020](#bib.bib17));
    Liu et al. ([2021](#bib.bib24)); Zheng et al. ([2023a](#bib.bib57)); Deng et al.
    ([2023c](#bib.bib11)); Zheng et al. ([2023b](#bib.bib59)), empathetic Rashkin
    et al. ([2019](#bib.bib31)); Zheng et al. ([2021](#bib.bib56)); Sabour et al.
    ([2022](#bib.bib32)) dialogue system in isolation, rather than seamlessly blending
    them all into one cohesive conversational flow Smith et al. ([2020](#bib.bib37));
    Wang et al. ([2023a](#bib.bib44)). A common approach is to predict the emotion
    or persona from a pre-defined set and generate the response in a multi-task manner
    Ma et al. ([2021](#bib.bib25)); Zheng et al. ([2021](#bib.bib56)); Sabour et al.
    ([2022](#bib.bib32)); Cheng et al. ([2023](#bib.bib7)); Deng et al. ([2023b](#bib.bib10)).
    Besides that, lots of work notices these linguistic cues underneath text by directly
    predicting them independently as a classification task Wang et al. ([2022](#bib.bib43));
    Barriere et al. ([2022](#bib.bib2)); Ghosh et al. ([2022](#bib.bib18)). Distinguishing
    from these previous works, we regard different aspects of cues as part of user
    status and prompt the LLMs to reason user status exhibited in the dialogue context,
    aiming to generate more helpful and acceptable responses for users.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统。之前的大多数工作开发了个性化的 Zhang et al. ([2018](#bib.bib53)); Zheng et al. ([2020](#bib.bib58));
    Song et al. ([2021](#bib.bib38)); Chen et al. ([2023a](#bib.bib4)), 情感的 Ghosal
    et al. ([2020](#bib.bib17)); Liu et al. ([2021](#bib.bib24)); Zheng et al. ([2023a](#bib.bib57));
    Deng et al. ([2023c](#bib.bib11)); Zheng et al. ([2023b](#bib.bib59)), 同理心的 Rashkin
    et al. ([2019](#bib.bib31)); Zheng et al. ([2021](#bib.bib56)); Sabour et al.
    ([2022](#bib.bib32)) 对话系统，孤立地发展，而不是将它们无缝地融合成一个连贯的对话流 Smith et al. ([2020](#bib.bib37));
    Wang et al. ([2023a](#bib.bib44))。一种常见的方法是从预定义的集合中预测情感或角色，并以多任务方式生成响应 Ma et al.
    ([2021](#bib.bib25)); Zheng et al. ([2021](#bib.bib56)); Sabour et al. ([2022](#bib.bib32));
    Cheng et al. ([2023](#bib.bib7)); Deng et al. ([2023b](#bib.bib10))。除此之外，很多工作通过独立预测这些语言线索作为分类任务来注意这些线索
    Wang et al. ([2022](#bib.bib43)); Barriere et al. ([2022](#bib.bib2)); Ghosh et
    al. ([2022](#bib.bib18))。不同于这些前人的工作，我们将不同方面的线索视为用户状态的一部分，并提示LLMs推理在对话上下文中展示的用户状态，旨在生成对用户更有帮助和更可接受的响应。
- en: 3 Method
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: In this section, we introduce more details about our method and how we select
    demonstrations under the few-shot setting.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的方法的更多细节以及在少量样本设置下如何选择演示。
- en: 3.1 Chain-of-thought in Dialogue
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 对话中的思维链
- en: 'We describe the prompting schemes in a general form, including standard prompting,
    O-Cue CoT, and M-Cue CoT as presented in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '我们以一般形式描述了提示方案，包括标准提示、O-Cue CoT和M-Cue CoT，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs")所示。'
- en: 'Standard Prompting. Most of the previous works directly prompt LLMs to generate
    responses solely based on dialogue context or user queries, which lack transparency
    and interpretability. The objective is defined as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 标准提示。之前的大多数工作直接提示LLMs仅根据对话上下文或用户查询生成响应，这些方法缺乏透明度和可解释性。目标定义为：
- en: '|  | $\mathcal{M}:\boldsymbol{c}\rightarrow\boldsymbol{r}$ |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}:\boldsymbol{c}\rightarrow\boldsymbol{r}$ |  | (1) |'
- en: where $\mathcal{M}$ demotes dialogue context and response respectively.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{M}$ 分别表示对话上下文和响应。
- en: 'O-Cue CoT. In line with the traditional chain-of-thoughts, we prompt the models
    to generate the middle reasoning processing and final results together, for example,
    we can prompt the LLMs to generate user status and a final response simultaneously
    giving dialogue context, enforcing the LLMs to reason based on the user status.
    However, it is important to note that generating intermediate reasoning results
    with responses together may lead to a reduction in the length of the different
    outputs, particularly when multiple or complex reasoning results are involved,
    sacrificing the details and explanations. For example, as shown in O-Cue CoT in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Cue-CoT: Chain-of-thought Prompting
    for Responding to In-depth Dialogue Questions with LLMs"), the generated user
    status is too short to provide cues for responses. Moreover, it is infeasible
    to modify the intermediate results when it is wrong Wang et al. ([2023c](#bib.bib46)).
    Here, we defined the objective as follows in which $\boldsymbol{s}$ stands for
    user status:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'O-Cue CoT。与传统的思维链一致，我们提示模型同时生成中间推理过程和最终结果，例如，我们可以提示LLM同时生成用户状态和最终响应，提供对话上下文，强制LLM基于用户状态进行推理。然而，值得注意的是，同时生成中间推理结果和响应可能会导致不同输出长度的减少，特别是当涉及到多个或复杂的推理结果时，牺牲了细节和解释。例如，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Cue-CoT: Chain-of-thought Prompting for Responding
    to In-depth Dialogue Questions with LLMs")所示，生成的用户状态过于简短，无法提供响应提示。此外，当中间结果错误时，修改这些结果是不切实际的Wang
    et al.（[2023c](#bib.bib46)）。在这里，我们定义了以下目标，其中$\boldsymbol{s}$表示用户状态：'
- en: '|  | $\mathcal{M}:\boldsymbol{c}\rightarrow\boldsymbol{s},\boldsymbol{r}$ |  |
    (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}:\boldsymbol{c}\rightarrow\boldsymbol{s},\boldsymbol{r}$ |  |
    (2) |'
- en: 'M-Cue CoT. In addition to standard prompting and O-Cue, we can further enhance
    the quality of responses in LLMs by decomposing reasoning into different consecutive
    steps while the final step is to generate responses according to previous reasoning
    outputs. On the one hand, it is convenient to process these intermediate outputs,
    allowing for actions such as incorporating user profiles for personalization Salemi
    et al. ([2023](#bib.bib33)) or filtering out erroneous reasoning results. These
    intermediate outputs can also be stored for future use, enabling their utilization
    for various purposes. On the other hand, these intermediate results can be used
    as a criterion to select demonstrations under few-shot settings (See next section).
    Overall, this technique allows for a clearer and more systematic progression of
    reasoning, resulting in better transparency and interpretability. The objective
    can be viewed as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: M-Cue CoT。除了标准提示和O-Cue外，我们还可以通过将推理分解为不同的连续步骤来进一步提升LLM的响应质量，而最终步骤是根据之前的推理输出生成响应。一方面，这使得处理这些中间输出变得更加方便，例如，可以将用户个人资料纳入个性化处理Salemi
    et al.（[2023](#bib.bib33)）或过滤掉错误的推理结果。这些中间输出还可以存储以备将来使用，实现多种目的。另一方面，这些中间结果可以作为选择少量示例的标准（见下一节）。总体而言，这项技术使推理过程更加清晰和系统化，从而提高了透明度和可解释性。目标可以如下视为：
- en: '|  | $\mathcal{M}:\boldsymbol{c}\rightarrow\boldsymbol{s}\rightarrow\boldsymbol{r}$
    |  | (3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}:\boldsymbol{c}\rightarrow\boldsymbol{s}\rightarrow\boldsymbol{r}$
    |  | (3) |'
- en: 3.2 Demonstration Selection
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 示范选择
- en: The few-shot performance of LLMs depends heavily on the quality of the demonstrations,
    especially for complex tasks that need multiple reasoning steps Zhang et al. ([2022](#bib.bib54)).
    Furthermore, in the context of dialogue systems, the process of selecting demonstrations
    becomes more challenging due to the one-to-many nature of dialogue interactions.
    As a result, novel approaches are needed to tackle the intricacies of dialogue
    response selection, taking into account the dynamic and context-dependent nature
    of conversations. We here introduce the demonstration selection strategy of three
    prompt schemes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的少量示范性能在很大程度上依赖于示范的质量，特别是对于需要多步推理的复杂任务Zhang et al.（[2022](#bib.bib54)）。此外，在对话系统的背景下，示范选择的过程由于对话交互的一对多性质而变得更加具有挑战性。因此，需要新的方法来解决对话响应选择的复杂性，考虑到对话的动态性和上下文依赖性。我们在这里介绍了三种提示方案的示范选择策略。
- en: Standard Prompting. Following previous work Wei et al. ([2023](#bib.bib50));
    Liu et al. ([2022](#bib.bib23)), we use randomly sampled examples (random selection)
    or most semantic similar examples (top-1 selection) according to dialogue context
    $c_{*}$).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 标准提示。根据前人的工作Wei等人（[2023](#bib.bib50)）；Liu等人（[2022](#bib.bib23)），我们根据对话上下文$c_{*}$）使用随机抽样示例（随机选择）或语义最相似示例（top-1选择）。
- en: 'O-Cue CoT. Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Demonstration Selection ‣ 3 Method
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs") shows the demonstration selection strategy of Cue-CoT. Although we
    still select demonstrations according to dialogue context $c$).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: O-Cue CoT。图[2](#S3.F2 "图 2 ‣ 3.2 演示选择 ‣ 3 方法 ‣ Cue-CoT：针对深度对话问题的链式思考提示")展示了Cue-CoT的演示选择策略。尽管我们仍然根据对话上下文$c$选择演示。
- en: M-Cue CoT. Since there are multiple steps, we design different selection strategies
    for each step. Specifically, we first select demonstrations ($\boldsymbol{c},\boldsymbol{s}$)
    according to user status. In this way, all intermediate reasoning results can
    be utilized as a criterion to select demonstrations, providing additional signals
    for the latter reasoning. An assumption underneath here is that users with similar
    statuses tend to accept responses with a similar style. Besides that, we also
    apply random selection and top-1 selection to O-Cue CoT and M-Cue CoT for detailed
    comparison.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: M-Cue CoT。由于步骤较多，我们为每个步骤设计了不同的选择策略。具体来说，我们首先根据用户状态选择演示（$\boldsymbol{c},\boldsymbol{s}$）。这样，所有中间推理结果可以作为选择演示的标准，为后续推理提供额外的信号。这里的一个假设是，状态相似的用户往往接受风格相似的回应。此外，我们还将随机选择和top-1选择应用于O-Cue
    CoT和M-Cue CoT，以便进行详细比较。
- en: '![Refer to caption](img/5a3e4bd30d51169a6c24d443d753b69f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5a3e4bd30d51169a6c24d443d753b69f.png)'
- en: 'Figure 2: Different demonstration selection strategies of O-Cue and M-Cue CoT,
    while the returned results such as $(\boldsymbol{c},\boldsymbol{s},\boldsymbol{r})$
    are prepended to original input to form new input.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：O-Cue 和 M-Cue CoT 的不同演示选择策略，而返回的结果如$(\boldsymbol{c},\boldsymbol{s},\boldsymbol{r})$会被附加到原始输入上以形成新输入。
- en: 4 Datasets Collection
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集收集
- en: In order to evaluate the performance of proposed Cue-CoT to reason different
    user statuses, we collect six datasets in terms of personality, empathy, and psychology,
    in both Chinese and English.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估提出的Cue-CoT在推理不同用户状态下的表现，我们收集了六个数据集，涵盖了个性、同理心和心理学，分别为中文和英文。
- en: 'Personality. Previous works found that the content and style of a user’s inquiry
    can provide indirect insights into their personality traits Mairesse et al. ([2007](#bib.bib26));
    Barriere et al. ([2022](#bib.bib2)). For instance, an individual with a tendency
    towards anxiety may ask for advice on how to alleviate nervousness before an upcoming
    job interview, phrasing the question as follows: "What strategies can I employ
    to reduce my anxiety and perform well in tomorrow’s interview?". Since the public
    existing datasets either focus on the personae of the system Zhang et al. ([2018](#bib.bib53))
    or target classification tasks without providing corresponding dialogue response
    Barriere et al. ([2022](#bib.bib2)), we thus build a pipeline to automatically
    collect the datasets using ChatGPT (gpt-3.5-turbo-0301). We first collect question-answer
    seeds from the two largest real-world online QA forums: Zhihu and Quora³³3[https://www.zhihu.com/](https://www.zhihu.com/)
    and [https://huggingface.co/datasets/quora](https://huggingface.co/datasets/quora),
    and then prompt the ChatGPT to infer the personality first as shown in Table [9](#A4.T9
    "Table 9 ‣ Appendix D Helpfulness Analysis of Planning Step ‣ Cue-CoT: Chain-of-thought
    Prompting for Responding to In-depth Dialogue Questions with LLMs"). We lastly
    require the ChatGPT to continue the dialogue given the inferred personality and
    the question-answer seed. In order to facilitate the continuous generation of
    transcripts for both participants in a dialogue, we utilize a template, as presented
    in Appendix [A.1](#A1.SS1 "A.1 Data Collection Template ‣ Appendix A Templates
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs"), to establish the necessary format and requirements. In this way,
    the use of personality seed and question-answer seed in the template assures greater
    diversity and reliability of user queries. Specifically, the personality seed
    determines the style of the user query, while the question seed determines the
    content. As a result, the user statuses vary across different dialogues, contributing
    to a richer and more varied conversational experience. Some examples of personality
    can be found in Appendix [A.2](#A1.SS2 "A.2 Some Examples of Personality ‣ Appendix
    A Templates ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue
    Questions with LLMs").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**个性**。之前的研究发现，用户询问的内容和风格可以间接提供他们个性特征的洞察（Mairesse et al. ([2007](#bib.bib26));
    Barriere et al. ([2022](#bib.bib2))）。例如，倾向于焦虑的个体可能会询问如何在即将到来的工作面试前缓解紧张，问题表述如下：“我可以采用什么策略来减少焦虑并在明天的面试中表现良好？”。由于现有的公开数据集要么关注系统的人物（Zhang
    et al. ([2018](#bib.bib53))），要么针对分类任务而未提供相应的对话回应（Barriere et al. ([2022](#bib.bib2))），因此我们构建了一个管道来自动收集数据集，使用
    ChatGPT（gpt-3.5-turbo-0301）。我们首先从两个最大的真实在线 QA 论坛：知乎和 Quora³³3[https://www.zhihu.com/](https://www.zhihu.com/)
    和 [https://huggingface.co/datasets/quora](https://huggingface.co/datasets/quora)
    收集问题-回答种子，然后提示 ChatGPT 首先推断个性，如表 [9](#A4.T9 "Table 9 ‣ Appendix D Helpfulness
    Analysis of Planning Step ‣ Cue-CoT: Chain-of-thought Prompting for Responding
    to In-depth Dialogue Questions with LLMs") 所示。最后，我们要求 ChatGPT 在推断出的个性和问题-回答种子的基础上继续对话。为了便于对话中双方的逐步生成转录，我们利用了附录
    [A.1](#A1.SS1 "A.1 Data Collection Template ‣ Appendix A Templates ‣ Cue-CoT:
    Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with
    LLMs") 中呈现的模板来建立必要的格式和要求。通过这种方式，模板中的个性种子和问题-回答种子的使用保证了用户查询的更大多样性和可靠性。具体而言，个性种子决定了用户查询的风格，而问题种子决定了内容。因此，用户状态在不同的对话中会有所不同，从而丰富和多样化对话体验。有关个性的更多示例可见附录
    [A.2](#A1.SS2 "A.2 Some Examples of Personality ‣ Appendix A Templates ‣ Cue-CoT:
    Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with
    LLMs")。'
- en: 'Emotion. In terms of the emotional status of users, we re-organize two existing
    empathetic dialogue datasets: D4 Yao et al. ([2022](#bib.bib51)) and EmpatheticDialogues
    (a.k.a, ED) Rashkin et al. ([2019](#bib.bib31)). For the former one, we first
    identify all utterances from the system labeled as empathic comfort for each dialogue
    sample in the test set. From these instances, the utterance with the longest length
    is chosen as the ground truth response, regarding preceding utterances as corresponding
    dialogue context⁴⁴4We also tried directly regarding the last utterance labeled
    as empathic comfort as grounded truth response, but we found most of them are
    short and uninformative such as you are welcome, take care and so on.. This approach
    ensures fairness and comparability in evaluating the performance of LLMs, particularly
    because they tend to generate lengthy responses. For the ED, there are two roles
    in the dialogue: Listener who is actively listening, and Speaker who is speaking
    and conveying information. We follow the setting of the original paper Rashkin
    et al. ([2019](#bib.bib31)), and directly use all samples in the test set. Neither
    the situation description written by the Speaker nor the emotional label is contained
    (just as they were not given to the Listener during dialogue collection). Thus,
    the collected empathetic dialogue datasets provide a standard benchmark for evaluating
    the LLMs’ ability to generate empathic responses.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 情感。关于用户的情感状态，我们重新组织了两个现有的情感对话数据集：D4 Yao 等（[2022](#bib.bib51)）和 EmpatheticDialogues（即
    ED）Rashkin 等（[2019](#bib.bib31)）。对于前者，我们首先识别测试集中每个对话样本中系统标记为同情安慰的所有发言。从这些实例中，选择最长的发言作为真实回应，将之前的发言视为对应的对话上下文⁴⁴4我们也尝试将最后一个标记为同情安慰的发言直接作为真实回应，但发现大多数发言较短且无信息量，如“你欢迎”，“保重”等。此方法确保了评估
    LLM 性能的公平性和可比性，特别是因为它们往往生成较长的回应。对于 ED，对话中有两个角色：积极倾听的听众和传达信息的讲者。我们遵循原始论文 Rashkin
    等（[2019](#bib.bib31)）的设定，直接使用测试集中的所有样本。既不包含讲者编写的情况描述，也不包含情感标签（正如它们在对话收集期间未提供给听众）。因此，收集的情感对话数据集提供了评估
    LLM 生成同情回应能力的标准基准。
- en: Metrics Chinese English Zhihu D4 PsyQA Quora ED EMH Avg.C 258.4 521.0 210.9
    149.6 50.2 44.2 Avg.R 76.9 57.9 607.5 48.3 12.9 175.8 Samples 1122 997 1000 1082
    2091 1000
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 中文 英文 知乎 D4 PsyQA Quora ED EMH 平均C 258.4 521.0 210.9 149.6 50.2 44.2 平均R
    76.9 57.9 607.5 48.3 12.9 175.8 样本 1122 997 1000 1082 2091 1000
- en: 'Table 1: Data statistics of our used datasets including three Chinese datasets
    and three English datasets, while each of them represents different aspects of
    user status during the conversation. We highlight maximum Avg.C and Avg.R.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们使用的数据集的数据统计，包括三个中文数据集和三个英文数据集，每个数据集代表对话过程中用户状态的不同方面。我们强调了最大平均 C 和平均 R。
- en: 'Psychology. In order to assess the effectiveness of LLMs in generating counseling
    responses for mental health support, we employed two pre-existing datasets, namely
    PsyQA Sun et al. ([2021](#bib.bib39)) and EMH Sharma et al. ([2020](#bib.bib35)).
    These datasets were utilized as dialogue pools from which we selected appropriate
    samples to serve as a benchmark for evaluating the language models. In PsyQA,
    there are 4,012 questions out of 22,341 samples that are sampled to pick the highest-voted
    answers. We randomly sample 1,000 out of these 4,012 questions, regarding the
    highest-voted answer as ground truth to form a more challenging test set. We also
    provide the question description beside the question itself following the original
    setting Sun et al. ([2021](#bib.bib39)). In EMH, there are 10k (post, response)
    pairs annotated with three different communication mechanisms: emotional reactions,
    interpretations, and explorations. We first sorted examples according to the length
    of their answers and then uniformly sampled examples with these three mechanisms,
    forming a final test set.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 心理学。为了评估 LLM 生成心理健康支持咨询回应的有效性，我们采用了两个现有的数据集，即 PsyQA Sun 等（[2021](#bib.bib39)）和
    EMH Sharma 等（[2020](#bib.bib35)）。这些数据集被用作对话池，我们从中选择适当的样本作为评估语言模型的基准。在 PsyQA 中，有
    4,012 个问题从 22,341 个样本中抽取，以选择最高投票的答案。我们从这 4,012 个问题中随机抽取 1,000 个，将最高投票的答案视为真实回应，形成更具挑战性的测试集。我们还提供了问题描述，以跟随原始设置
    Sun 等（[2021](#bib.bib39)）。在 EMH 中，有 10k（帖子，回应）对，标注了三种不同的沟通机制：情感反应、解释和探讨。我们首先根据回答的长度对示例进行排序，然后均匀抽取具有这三种机制的示例，形成最终测试集。
- en: 'All. Table [1](#S4.T1 "Table 1 ‣ 4 Datasets Collection ‣ Cue-CoT: Chain-of-thought
    Prompting for Responding to In-depth Dialogue Questions with LLMs") shows the
    data statistics of our benchmark. The notation Avg. C signifies the mean context
    length of instances, and if it exceeds a certain threshold, it may surpass the
    input context limit of LLMs⁵⁵5For example, the input context limit of belle-llama-7b-2m
    is 2048, and few of examples from D4 exceeds the limit and the scenario becomes
    worse under the one-shot setting. We will have more detailed analysis in latter
    sections. or become too lengthy for LLMs to comprehend. On the other hand, Avg.
    R denotes the average response length. Generally, longer responses tend to be
    more comprehensive and clearer, presenting a more challenging baseline for LLMs
    to surpass. To sum up, we build a benchmark, consisting of six datasets (three
    Chinese datasets and three English datasets) in terms of three aspects of user
    status during the conversation, hoping the release of it can facilitate the research
    of dialogue systems based on LLMs.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '全部。表[1](#S4.T1 "Table 1 ‣ 4 Datasets Collection ‣ Cue-CoT: Chain-of-thought
    Prompting for Responding to In-depth Dialogue Questions with LLMs")展示了我们基准测试的数据统计。符号Avg.
    C表示实例的平均上下文长度，如果它超过某个阈值，可能会超出LLMs的输入上下文限制⁵⁵例如，belle-llama-7b-2m的输入上下文限制为2048，而D4中的一些示例超过了该限制，并且在一-shot设置下情况更糟。我们将在后续部分进行更详细的分析。或者可能对LLMs而言过长而难以理解。另一方面，Avg.
    R表示平均响应长度。通常，较长的响应往往更全面、更清晰，为LLMs提供了更具挑战性的基准。总之，我们建立了一个基准，包括六个数据集（三个中文数据集和三个英文数据集），从对话中用户状态的三个方面进行考量，希望它的发布能促进基于LLMs的对话系统研究。'
- en: 5 Experiment
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'In this section, we have conducted a comprehensive experiment to compare the
    performance of three prompting methods: standard prompting, O-Cue and M-Cue CoT
    in the benchmark under both zero-shot and one-shot settings⁶⁶6Since the length
    of dialogue context is relatively long, the input length limit is easy to break
    when the number of shot exceeds 1, so we choose the one-shot setting to conduct
    in-context learning..'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行了全面的实验，以比较三种提示方法的性能：标准提示、O-Cue和M-Cue CoT，在基准测试中分别在零-shot和一-shot设置下⁶⁶由于对话上下文的长度相对较长，当shot数量超过1时，输入长度限制容易被突破，因此我们选择在一-shot设置下进行上下文学习。
- en: Model Prompt Helpfulness Acceptness Zhihu D4 PsyQA Zhihu D4 PsyQA Zero-shot
    Setting belle O-Cue 67.40 76.34 69.31 55.82 52.50 53.43 M-Cue 81.54 71.60 79.25
    60.23 72.41 73.65 chatglm O-Cue 48.29 56.68 33.00 32.39 39.19 31.34 M-Cue 85.02
    72.10 83.57 66.67 51.27 55.40 chatgpt O-Cue 67.91 50.40 61.90 53.14 52.38 58.15
    M-Cue 95.57 87.88 90.34 65.22 61.08 56.12 One-shot Setting random selection \hdashlinebelle
    O-Cue 64.31 50.53 65.15 53.35 40.07 53.81 M-Cue 83.30 69.59 73.81 73.61 56.14
    61.90 chatglm O-Cue - - - - - - M-Cue 90.28 75.10 91.85 74.55 54.03 64.75 chatgpt
    O-Cue 76.47 51.94 65.44 63.86 50.47 56.03 M-Cue 91.60 86.67 88.96 76.83 58.19
    61.41 top-1 selection \hdashlinebelle O-Cue 63.77 57.51 69.92 54.93 41.02 55.87
    M-Cue 82.77 69.94 73.99 74.32 54.38 62.24 chatglm O-Cue - - - - - - M-Cue 89.25
    77.26 91.77 73.43 57.17 58.74 chatgpt O-Cue 76.86 50.93 55.85 59.63 52.02 57.58
    M-Cue 93.19 88.84 91.77 78.46 56.84 59.48
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 提示 有用性 接受度 知乎 D4 PsyQA 知乎 D4 PsyQA 零-shot 设置 belle O-Cue 67.40 76.34 69.31
    55.82 52.50 53.43 M-Cue 81.54 71.60 79.25 60.23 72.41 73.65 chatglm O-Cue 48.29
    56.68 33.00 32.39 39.19 31.34 M-Cue 85.02 72.10 83.57 66.67 51.27 55.40 chatgpt
    O-Cue 67.91 50.40 61.90 53.14 52.38 58.15 M-Cue 95.57 87.88 90.34 65.22 61.08
    56.12 一-shot 设置 随机选择 \hdashlinebelle O-Cue 64.31 50.53 65.15 53.35 40.07 53.81
    M-Cue 83.30 69.59 73.81 73.61 56.14 61.90 chatglm O-Cue - - - - - - M-Cue 90.28
    75.10 91.85 74.55 54.03 64.75 chatgpt O-Cue 76.47 51.94 65.44 63.86 50.47 56.03
    M-Cue 91.60 86.67 88.96 76.83 58.19 61.41 top-1 选择 \hdashlinebelle O-Cue 63.77
    57.51 69.92 54.93 41.02 55.87 M-Cue 82.77 69.94 73.99 74.32 54.38 62.24 chatglm
    O-Cue - - - - - - M-Cue 89.25 77.26 91.77 73.43 57.17 58.74 chatgpt O-Cue 76.86
    50.93 55.85 59.63 52.02 57.58 M-Cue 93.19 88.84 91.77 78.46 56.84 59.48
- en: 'Table 2: The win rate of responses generated by our method compared with the
    response with standard prompting on three Chinese datasets in terms of helpfulness
    and acceptness. The underlined numbers mean that there are about 160 to 280 valid
    responses out of 500 in this setting due to the input context limit of the model.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：我们的方法生成的响应与标准提示生成的响应在三个中文数据集上的有用性和接受度的胜率对比。下划线的数字表示由于模型的输入上下文限制，在这种设置下大约有160到280个有效响应（总共500个）。
- en: 5.1 LLMs Family and Evaluation Details
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 LLMs家族及评估细节
- en: LLMs Family. We compared the performance of different LLMs with our benchmark,
    including ChatGLM-6B Du et al. ([2022](#bib.bib14)), BELLE-LLAMA-7B-2M Ji et al.
    ([2023](#bib.bib20)), ChatGPT for Chinese, and Alpaca-7B Taori et al. ([2023](#bib.bib40)),
    Vicuna-7B-v1.1⁷⁷7https://github.com/lm-sys/FastChat and also ChatGPT for English.
    We strictly follow the commands and procedures to recover the weights of these
    models and we strongly suggest that the reader read the original paper to check
    more details. We set the temperature as 0.2 and top p as 0.1 for evaluation, and
    temperature as 0.7 and top p as 0.95 for generation in all models. We use BERT
    Devlin et al. ([2019](#bib.bib12)) as an encoder to select the nearest example
    to the test query for top-1 one-shot setting, storing the mean vector of examples
    as sentence embedding⁸⁸8We directly user bert-base-chinese for all Chinese datasets
    and bert-base-uncased for all English datasets, we do not finetune the BERT model..
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs家族。我们用我们的基准测试比较了不同LLMs的性能，包括ChatGLM-6B Du等人（[2022](#bib.bib14)）、BELLE-LLAMA-7B-2M
    Ji等人（[2023](#bib.bib20)）、用于中文的ChatGPT以及Alpaca-7B Taori等人（[2023](#bib.bib40)）、Vicuna-7B-v1.1⁷⁷7https://github.com/lm-sys/FastChat以及用于英文的ChatGPT。我们严格按照命令和程序恢复这些模型的权重，并强烈建议读者阅读原始论文以获取更多细节。我们将温度设定为0.2，top
    p设定为0.1用于评估，将温度设定为0.7，top p设定为0.95用于生成所有模型的输出。我们使用BERT Devlin等人（[2019](#bib.bib12)）作为编码器，在top-1一-shot设置中选择与测试查询最接近的示例，将示例的均值向量存储为句子嵌入⁸⁸8我们直接使用bert-base-chinese用于所有中文数据集，使用bert-base-uncased用于所有英文数据集，我们不对BERT模型进行微调。
- en: 'Evaluation. 1) Metrics: We found that most existing automatic metrics Rashkin
    et al. ([2019](#bib.bib31)); Sun et al. ([2021](#bib.bib39)) such as Avg.BLEU
    and F1 can not align well with human judgments, as observed by Zhao et al. ([2023](#bib.bib55)),
    too. Inspired by recent automatic evaluation using ChatGPT as a judger which aligns
    well with the humans Chen et al. ([2023c](#bib.bib6)); Wang et al. ([2023b](#bib.bib45));
    Zhao et al. ([2023](#bib.bib55)), we mainly choose to use it to evaluate the quality
    of the generated responses in a pair-wise manner⁹⁹9We noticed the very recent
    paper Wang et al. ([2023d](#bib.bib47)) that emphasizes the effects of the order
    of responses, and we evaluate responses using suggested BPC but we found it can
    not lead to better alignment with humans in most cases of our benchmarks due to
    the complexity and diversity., considering helpfulness and acceptability. The
    evaluation templates can be found in Appendix [A.3](#A1.SS3 "A.3 Evaluation Templates
    ‣ Appendix A Templates ‣ Cue-CoT: Chain-of-thought Prompting for Responding to
    In-depth Dialogue Questions with LLMs") and we calculate the win rate using #wins
    / ( #wins + #ties + #loses). 2) Methods: Due to the exceptional proficiency of
    the LLM-based dialogue system, it is relatively easy for them to beat the ground
    truth responses in the original datasets (Appendix [B.1](#A2.SS1 "B.1 Compared
    with ground truth ‣ Appendix B Different Method of Evaluation ‣ Cue-CoT: Chain-of-thought
    Prompting for Responding to In-depth Dialogue Questions with LLMs")), we consider
    standard prompting as a more challenging baseline and compare the responses generated
    using our proposed Cue-CoT with the response generated using standard prompting,
    which is more fair and convincing. We also provide the human evaluation result
    as a reference.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '评估。1) 评估指标：我们发现大多数现有的自动评估指标 Rashkin等人（[2019](#bib.bib31)）；Sun等人（[2021](#bib.bib39)），例如Avg.BLEU和F1，无法与人类判断很好对齐，赵等人（[2023](#bib.bib55)）也有类似观察。受到最近使用ChatGPT作为评审员的自动评估启发，ChatGPT与人类判断对齐良好
    Chen等人（[2023c](#bib.bib6)）；Wang等人（[2023b](#bib.bib45)）；赵等人（[2023](#bib.bib55)），我们主要选择使用它以对对生成的响应质量进行配对评估⁹⁹9我们注意到最近的一篇论文
    Wang等人（[2023d](#bib.bib47)）强调了响应顺序的影响，我们使用建议的BPC评估响应，但我们发现它在我们的大多数基准测试中不能带来更好的与人类对齐，原因在于复杂性和多样性。我们考虑了有用性和可接受性。评估模板可以在附录 [A.3](#A1.SS3
    "A.3 Evaluation Templates ‣ Appendix A Templates ‣ Cue-CoT: Chain-of-thought Prompting
    for Responding to In-depth Dialogue Questions with LLMs")中找到，我们使用#wins / ( #wins
    + #ties + #loses)来计算胜率。2) 方法：由于LLM基础的对话系统的卓越能力，它们相对容易击败原始数据集中的真实响应（附录 [B.1](#A2.SS1
    "B.1 Compared with ground truth ‣ Appendix B Different Method of Evaluation ‣
    Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs")），我们将标准提示视为一个更具挑战性的基线，并将使用我们提出的Cue-CoT生成的响应与使用标准提示生成的响应进行比较，这样更公平且具有说服力。我们还提供了人工评估结果作为参考。'
- en: 5.2 Main Experiment
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要实验
- en: 'All. Table [2](#S5.T2 "Table 2 ‣ 5 Experiment ‣ Cue-CoT: Chain-of-thought Prompting
    for Responding to In-depth Dialogue Questions with LLMs") and Table [3](#S5.T3
    "Table 3 ‣ 5.2 Main Experiment ‣ 5 Experiment ‣ Cue-CoT: Chain-of-thought Prompting
    for Responding to In-depth Dialogue Questions with LLMs") present the win rate
    of responses generated by O-Cue and M-Cue CoT compared with the responses by standard
    prompting on Chinese and English datasets respectively^(10)^(10)10We emphasize
    here that the O-Cue and M-Cue in Table 2 and Table 3 should be regarded as O-Cue
    v.s. Standard prompting and M-Cue v.s. Standard prompting respectively. We do
    not provide results of Standard prompting v.s. Standard prompting since it is
    self-contained. It can be regarded as a uniform distribution whose win rates are
    always 0.5.. Despite that there are few LLMs that perform worse than standard
    prompting using O-Cue due to its complex instructions, i.e. ChatGLM in Chinese
    and Alpaca in English, it is observed that O-Cue can achieve above 50% win rate
    mostly in Both Chinese and English. Moreover, it is exciting to find that M-Cue
    further boosts performance and achieves higher win rates irrespective of the type
    of language model, datasets, or settings used, revealing its robustness and effectiveness.
    We attribute this to the relatively easy-understanding instructions and clear
    outputs in each step of the M-Cue, since some LLMs are incapable to follow relatively
    long instructions in O-Cue and output the content and style as required. For example,
    we asked the LLMs to output user status and response in two separate lines but
    only a few LLMs output in the format, making it difficult to distinguish the response
    from reasoning results. Also, the combined output of the user status and response
    can potentially limit the length of various components, thereby accounting for
    the disparity between O-Cue and M-Cue. Furthermore, we found that the acceptability
    is relatively lower than helpfulness for Chinese LLMs but higher for English LLMs,
    especially under the one-shot setting, revealing the weakness of Chinese LLMs
    to provide acceptable besides helpful responses.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '所有。表格[2](#S5.T2 "Table 2 ‣ 5 Experiment ‣ Cue-CoT: Chain-of-thought Prompting
    for Responding to In-depth Dialogue Questions with LLMs")和表格[3](#S5.T3 "Table
    3 ‣ 5.2 Main Experiment ‣ 5 Experiment ‣ Cue-CoT: Chain-of-thought Prompting for
    Responding to In-depth Dialogue Questions with LLMs")展示了O-Cue和M-Cue CoT生成的响应相对于标准提示的胜率，分别在中文和英文数据集上^(10)^(10)10我们在此强调，表格2和表格3中的O-Cue和M-Cue应分别视为O-Cue对比标准提示和M-Cue对比标准提示。我们没有提供标准提示对比标准提示的结果，因为它是自包含的。它可以被视为均匀分布，其胜率始终为0.5。尽管由于其复杂的指令，少数LLMs在使用O-Cue时表现比标准提示更差，例如中文的ChatGLM和英文的Alpaca，但观察到O-Cue在中文和英文中的胜率大多能达到50%以上。此外，令人兴奋的是发现M-Cue进一步提升了性能，无论语言模型类型、数据集或设置如何，都能实现更高的胜率，显示了其稳健性和有效性。我们将其归因于M-Cue中相对容易理解的指令和每一步的清晰输出，因为一些LLMs无法遵循O-Cue中相对较长的指令并按要求输出内容和风格。例如，我们要求LLMs将用户状态和响应输出在两行中，但只有少数LLMs按此格式输出，导致难以区分响应与推理结果。此外，用户状态和响应的合并输出可能限制了各个组件的长度，从而解释了O-Cue和M-Cue之间的差异。此外，我们发现中文LLMs的可接受性相对低于有用性，但英文LLMs的可接受性较高，特别是在一击设定下，揭示了中文LLMs在提供可接受的响应方面的弱点。'
- en: Model Prompt Helpfulness Acceptness Quora ED EMH Quora ED EMH Zero-shot Setting
    alpaca O-Cue 19.51 39.41 49.70 22.85 35.41 50.15 M-Cue 80.78 87.30 85.76 78.21
    86.00 86.97 vicuna O-Cue 56.16 71.43 59.43 55.73 65.06 63.50 M-Cue 81.67 91.30
    80.42 77.89 90.71 82.93 chatgpt O-Cue 79.47 88.31 82.83 81.47 89.92 93.71 M-Cue
    85.83 91.98 82.93 89.09 96.79 94.93 One-shot Setting random selection \hdashlinealpaca
    O-Cue - - - - - - M-Cue 76.78 85.08 94.36 72.34 85.07 95.82 vicuna O-Cue 60.45
    70.77 63.06 60.45 68.21 67.07 M-Cue 79.84 91.20 79.23 83.16 92.45 87.99 chatgpt
    O-Cue 80.33 87.32 84.94 80.33 90.80 96.06 M-Cue 84.31 89.78 85.71 86.64 93.94
    96.70 top-1 selection \hdashlinealpaca O-Cue - - - - - - M-Cue 74.54 78.70 88.69
    72.27 79.55 93.43 vicuna O-Cue 63.10 71.75 62.31 62.04 67.21 67.76 M-Cue 78.70
    90.12 79.10 82.08 92.96 88.96 chatgpt O-Cue 81.15 87.42 81.40 80.24 89.92 91.84
    M-Cue 88.08 91.37 86.87 91.21 95.95 96.12
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 提示 有用性 接受度 Quora ED EMH Quora ED EMH 零-shot设置 alpaca O-Cue 19.51 39.41 49.70
    22.85 35.41 50.15 M-Cue 80.78 87.30 85.76 78.21 86.00 86.97 vicuna O-Cue 56.16
    71.43 59.43 55.73 65.06 63.50 M-Cue 81.67 91.30 80.42 77.89 90.71 82.93 chatgpt
    O-Cue 79.47 88.31 82.83 81.47 89.92 93.71 M-Cue 85.83 91.98 82.93 89.09 96.79
    94.93 一-shot设置 随机选择 \hdashlinealpaca O-Cue - - - - - - M-Cue 76.78 85.08 94.36
    72.34 85.07 95.82 vicuna O-Cue 60.45 70.77 63.06 60.45 68.21 67.07 M-Cue 79.84
    91.20 79.23 83.16 92.45 87.99 chatgpt O-Cue 80.33 87.32 84.94 80.33 90.80 96.06
    M-Cue 84.31 89.78 85.71 86.64 93.94 96.70 top-1选择 \hdashlinealpaca O-Cue - - -
    - - - M-Cue 74.54 78.70 88.69 72.27 79.55 93.43 vicuna O-Cue 63.10 71.75 62.31
    62.04 67.21 67.76 M-Cue 78.70 90.12 79.10 82.08 92.96 88.96 chatgpt O-Cue 81.15
    87.42 81.40 80.24 89.92 91.84 M-Cue 88.08 91.37 86.87 91.21 95.95 96.12
- en: 'Table 3: The win rate of responses generated by our method compared with the
    response with standard prompting on three English datasets in terms of helpfulness
    and acceptness. The underlined dataset mean that there are about 330 valid responses
    out of 500 in this dataset for all experiments due to the input context limit
    of the model.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：我们的方法生成的响应与标准提示方法在三个英文数据集上的表现对比，涉及有用性和接受度。下划线的数据集意味着由于模型的输入上下文限制，该数据集中所有实验中约有330个有效响应（共500个）。
- en: 'Chinese LLMs. Table [2](#S5.T2 "Table 2 ‣ 5 Experiment ‣ Cue-CoT: Chain-of-thought
    Prompting for Responding to In-depth Dialogue Questions with LLMs") shows the
    performance of Chinese LLMs. We surprisingly found that ChatGLM performs worst
    out of the three LLMs using O-Cue but better than BELLE (especially at helpfulness)
    using M-Cue under the zero-shot setting, and then we carefully check the outputs
    of these LLMs and found that ChatGLM almost fully ignore the instructions in O-Cue
    and simply continue the dialogue. However, we found it can follow instructions
    well in M-Cue, resulting in higher win rates. We attribute this to the relatively
    more complex and longer instructions in O-Cue and poor complex-instructions understanding
    of ChatGLM^(11)^(11)11Thus, we do not report the one-shot results using O-Cue
    for ChatGLM.. In addition, with the M-Cue method, we found that the performance
    of all models on D4 is relatively worse than the other two datasets. We suspect
    the reason is the longest length of context in D4\. Moreover, we observe that
    the responses generated by ChatGLM and BELLE under the one-shot setting are much
    better under the zero-shot setting using the standard prompting method, i.e.,
    less general responses and more responses in line with the role, benefiting from
    the informative demonstrations.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '中文LLM。表[2](#S5.T2 "Table 2 ‣ 5 Experiment ‣ Cue-CoT: Chain-of-thought Prompting
    for Responding to In-depth Dialogue Questions with LLMs")展示了中文LLM的表现。我们惊讶地发现，ChatGLM在使用O-Cue的三种LLM中表现最差，但在零-shot设置下使用M-Cue时比BELLE表现更好（特别是在有用性方面），然后我们仔细检查了这些LLM的输出，发现ChatGLM几乎完全忽略了O-Cue中的指令，简单地继续对话。然而，我们发现它在M-Cue中能够很好地遵循指令，导致更高的胜率。我们将此归因于O-Cue中的指令相对更复杂、更长，以及ChatGLM对复杂指令理解较差^(11)^(11)11因此，我们不报告使用O-Cue的ChatGLM的一-shot结果。此外，通过M-Cue方法，我们发现所有模型在D4上的表现相对比其他两个数据集差。我们怀疑原因是D4中上下文的最长长度。此外，我们观察到，ChatGLM和BELLE在一-shot设置下生成的响应在零-shot设置下使用标准提示方法时要好得多，即响应更少通用，更符合角色，受益于信息丰富的示范。'
- en: 'English LLMs. Table [3](#S5.T3 "Table 3 ‣ 5.2 Main Experiment ‣ 5 Experiment
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs") shows the performance of English LLMs. Similarly, for the zero-shot
    setting using O-Cue, we found that Alpaca hardly follows the instructions, which
    often produces ambiguous outputs, mostly presenting user status and other times
    providing the response without any indication^(12)^(12)12We do not report one-shot
    for Alpaca, too.. Besides that, with the M-Cue method, due to the innate limitations
    of Alpaca, the win rate of responses is the lowest among all LLMs and settings.
    In addition, English LLMs also perform worst on the dataset which has the longest
    context length (Quora), in which ChatGPT and Vicuna tend to generate much longer
    responses than Alpaca due to limit of max length. More comparisons can be found
    in Appendix [B](#A2 "Appendix B Different Method of Evaluation ‣ Cue-CoT: Chain-of-thought
    Prompting for Responding to In-depth Dialogue Questions with LLMs").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '英文 LLMs。表 [3](#S5.T3 "表 3 ‣ 5.2 主要实验 ‣ 5 实验 ‣ Cue-CoT: 用于对话问题的链式思考提示") 显示了英文
    LLMs 的表现。类似地，对于使用 O-Cue 的零样本设置，我们发现 Alpaca 几乎不遵循指令，经常产生模糊的输出，大多数情况下展示用户状态，有时则直接提供回应而没有任何指示^(12)^(12)12我们也不报告
    Alpaca 的一例情况.. 此外，由于 Alpaca 的固有限制，使用 M-Cue 方法时，回应的胜率在所有 LLMs 和设置中都是最低的。此外，英文 LLMs
    在上下文长度最长的数据集（Quora）上的表现也最差，其中 ChatGPT 和 Vicuna 由于最大长度的限制，往往生成比 Alpaca 更长的回应。更多比较可以在附录 [B](#A2
    "附录 B 评估方法的不同 ‣ Cue-CoT: 用于对话问题的链式思考提示") 中找到。'
- en: Method Order Quora ED EMH helpfulness \hdashlineO-Cue S – O 34 (0.08) 44 (0.15)
    42 (0.05) O – S 68 (0.09) 80 (0.19) 78 (0.22) M-Cue S – O 51 (0.18) 53 (0.17)
    60 (0.30) O – S 82 (0.23) 79 (0.31) 81 (0.35) acceptability \hdashlineO-Cue S
    – O 28 (0.05) 34 (0.09) 34 (0.08) O – S 66 (0.12) 76 (0.15) 88 (0.57) M-Cue S
    – O 49 (0.13) 51 (0.15) 50 (0.21) O – S 84 (0.25) 82 (0.32) 75 (0.14)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 顺序 Quora ED EMH 有用性 \hdashlineO-Cue S – O 34 (0.08) 44 (0.15) 42 (0.05) O
    – S 68 (0.09) 80 (0.19) 78 (0.22) M-Cue S – O 51 (0.18) 53 (0.17) 60 (0.30) O
    – S 82 (0.23) 79 (0.31) 81 (0.35) 可接受性 \hdashlineO-Cue S – O 28 (0.05) 34 (0.09)
    34 (0.08) O – S 66 (0.12) 76 (0.15) 88 (0.57) M-Cue S – O 49 (0.13) 51 (0.15)
    50 (0.21) O – S 84 (0.25) 82 (0.32) 75 (0.14)
- en: 'Table 4: The alignment results (Acc (Kap.C)) of different automatic evaluation
    methods with the human evaluation under the zero-shot setting by comparing responses
    using our CoTs with one using standard prompting in terms of helpfulness and acceptability
    (with ChatGPT as base model) on English datasets.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在零样本设置下，通过比较使用我们的 CoTs 与使用标准提示的回应，测量不同自动评估方法与人工评估的一致性结果（Acc (Kap.C)），涉及有用性和可接受性（以
    ChatGPT 为基础模型），在英文数据集上。
- en: Method Order Zhihu D4 PsyQA helpfulness \hdashlineO-Cue S – O 64 (0.23) 42 (0.13)
    44 (0.06) O – S 66 (0.37) 76 (0.36) 72 (0.17) M-Cue S – O 45 (0.14) 67 (0.08)
    37 (0.09) O – S 80 (0.23) 74 (0.28) 84 (0.18) acceptability \hdashlineO-Cue S
    – O 60 (0.16) 56 (0.14) 46 (0.04) O – S 70 (0.44) 64 (0.23) 72 (0.46) M-Cue S
    – O 51 (0.16) 69 (0.23) 64 (0.09) O – S 74 (0.18) 75 (0.25) 64 (0.12)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 顺序 知乎 D4 PsyQA 有用性 \hdashlineO-Cue S – O 64 (0.23) 42 (0.13) 44 (0.06) O
    – S 66 (0.37) 76 (0.36) 72 (0.17) M-Cue S – O 45 (0.14) 67 (0.08) 37 (0.09) O
    – S 80 (0.23) 74 (0.28) 84 (0.18) 可接受性 \hdashlineO-Cue S – O 60 (0.16) 56 (0.14)
    46 (0.04) O – S 70 (0.44) 64 (0.23) 72 (0.46) M-Cue S – O 51 (0.16) 69 (0.23)
    64 (0.09) O – S 74 (0.18) 75 (0.25) 64 (0.12)
- en: 'Table 5: The alignment results (Acc (Kap.C)) of different automatic evaluation
    methods with the human evaluation in terms of helpfulness and acceptability (with
    ChatGPT as base model) on Chinese datasets.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在有用性和可接受性方面（以 ChatGPT 为基础模型），测量不同自动评估方法与人工评估的一致性结果（Acc (Kap.C)），在中文数据集上。
- en: 5.3 Human Evaluation
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 人工评估
- en: 'We conduct a human evaluation to validate the alignment of our evaluation setting
    with human judgments. Specifically, we hire three well-educated master students
    and randomly sample 100 response pairs (a.k.a. responses generated by ChatGPT
    using O-Cue or M-Cue and standard prompting) with dialogue context for each dataset.
    We ask them to indicate which response is better by inputting 1 (win) and -1 (lose)^(13)^(13)13We
    do not consider ties since there is not much tie in LLM evaluation. considering
    the helpfulness and acceptability without exposing the source of the responses.
    In addition, we analyze the effects of two different orders of response pairs
    in the evaluation template: O-S and S-O. Specifically, S denotes responses generated
    by Cue-CoT, while O indicates those generated by standard prompting. We then calculate
    the Kappa Correlation Coefficient (Kap.C) and also the accuracy between human
    scores and automatic scores (Acc). The results of English and Chinese datasets
    can be found in Table [4](#S5.T4 "Table 4 ‣ 5.2 Main Experiment ‣ 5 Experiment
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs") and Table [5](#S5.T5 "Table 5 ‣ 5.2 Main Experiment ‣ 5 Experiment
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs") respectively. There are two observations: 1) the order bias exists
    in our experiment, but the alignment is not as good as our setting (O - S) after
    swapping the order (S - O); 2) O-Cue and M-Cue both demonstrate better performance
    than standard prompting, especially for English dataset. We attribute this to
    the potential better reasoning performance of ChatGPT on the English dataset.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项人工评估，以验证我们的评估设置与人工判断的一致性。具体来说，我们聘请了三名受过良好教育的硕士生，并对每个数据集随机抽取了100对响应（即使用O-Cue或M-Cue和标准提示生成的ChatGPT响应），并附上对话上下文。我们要求他们通过输入1（胜利）和-1（失败）^(13)^(13)13我们没有考虑平局，因为LLM评估中平局很少。考虑有用性和接受度而不暴露响应来源来指示哪个响应更好。此外，我们分析了评估模板中两种不同响应对顺序的影响：O-S和S-O。具体来说，S表示由Cue-CoT生成的响应，而O表示由标准提示生成的响应。然后，我们计算了Kappa相关系数（Kap.C）以及人工分数和自动分数之间的准确性（Acc）。英语和中文数据集的结果可以在表[4](#S5.T4
    "表4 ‣ 5.2 主要实验 ‣ 5 实验 ‣ Cue-CoT：使用LLMs回应深入对话问题的思维链提示")和表[5](#S5.T5 "表5 ‣ 5.2 主要实验
    ‣ 5 实验 ‣ Cue-CoT：使用LLMs回应深入对话问题的思维链提示")中找到。我们有两个观察结果：1）我们的实验中存在顺序偏差，但在交换顺序（S -
    O）后，与我们的设置（O - S）的对齐度不如预期；2）O-Cue和M-Cue都比标准提示表现更好，尤其是在英语数据集上。我们将此归因于ChatGPT在英语数据集上可能具有更好的推理性能。
- en: 6 Analysis
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 分析
- en: In this section, we conduct an extensive analysis with the backbone as ChatGPT
    using M-Cue CoT because of its superior performance in both Chinese and English^(14)^(14)14We
    present the results in terms of acceptability since this metric is more suitable
    for our motivation. We put helpfulness analysis in the Appendix..
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用M-Cue CoT对ChatGPT进行全面分析，因为它在中文和英文中表现优越^(14)^(14)14我们以可接受性为标准呈现结果，因为这一指标更适合我们的目标。我们将有用性分析放在附录中。
- en: '![Refer to caption](img/c10647a8994605f0f3ceb40bc181ffea.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c10647a8994605f0f3ceb40bc181ffea.png)'
- en: 'Figure 3: The win rate of responses (acceptness) generated by ChatGPT under
    different demonstration selection strategies under one-shot setting v.s. responses
    under the zero-shot setting, using M-Cue CoT.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在一对一设置下，ChatGPT在不同演示选择策略下生成的响应（接受度）与零样本设置下的响应相比，使用M-Cue CoT。
- en: 6.1 One-shot v.s. Zero-shot
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 一对一设置 vs. 零样本设置
- en: 'Figure [3](#S6.F3 "Figure 3 ‣ 6 Analysis ‣ Cue-CoT: Chain-of-thought Prompting
    for Responding to In-depth Dialogue Questions with LLMs") shows the direct comparison
    of responses generated under different settings using M-Cue. There are 5 out of
    6 datasets except for D4 in which one-shot (both random or top-1 selection) beats
    zero-shot since the win rates all exceed 80%. The suboptimal performance of D4
    in the one-shot setting can be attributed largely to the limitations imposed by
    the input length constraint. Furthermore, we can observe that top-1 selection
    achieves better performance than random selection in 4 out of 6 datasets, suggesting
    users with similar statuses tend to like similar expression styles in responses.
    We attribute the relatively lower performance of top-1 selection in D4 and Quora
    to the difficulty the LLM encounters in attending to critical input components
    due to the lengthy context.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S6.F3 "图 3 ‣ 6 分析 ‣ Cue-CoT: 针对深度对话问题的链式思考提示") 显示了使用 M-Cue 在不同设置下生成的响应的直接比较。在除
    D4 外的 6 个数据集中，有 5 个数据集中的单次尝试（无论是随机选择还是 top-1 选择）都优于零次尝试，因为获胜率均超过 80%。D4 在单次尝试设置下的表现较差，很大程度上是由于输入长度限制的影响。此外，我们可以观察到，在
    6 个数据集中的 4 个数据集中，top-1 选择的表现优于随机选择，这表明相似状态的用户倾向于在回应中喜欢类似的表达风格。我们将 top-1 选择在 D4
    和 Quora 中表现较低归因于 LLM 遇到的由于上下文过长导致的关注关键输入组件的困难。'
- en: 6.2 More Reasoning Steps
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 更多推理步骤
- en: 'We tried to introduce an additional step (Step 2) after user status inference
    (Step 1): response planning by prompting the model to plan the response considering
    the dialogue context and user status. Specifically, we prompt the model to answer
    the following questions: "Based on the context of the conversation and the user
    status such as personality traits, and psychological and emotional state, what
    aspects should the system pay attention to when responding?" after giving the
    dialogue and user status as shown in Table [10](#A4.T10 "Table 10 ‣ Appendix D
    Helpfulness Analysis of Planning Step ‣ Cue-CoT: Chain-of-thought Prompting for
    Responding to In-depth Dialogue Questions with LLMs"). We regard the output of
    LLMs as system planning $\boldsymbol{p}$, in which ProcessA is chosen in our main
    experiment. Table [6](#S6.T6 "Table 6 ‣ 6.2 More Reasoning Steps ‣ 6 Analysis
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs") shows the results. First of all, it is likely that adding more reasoning
    steps will improve the LLMs’ performance, but it is not necessary to assemble
    all intermediate reasoning results at the last step, for example, variant ProcessB
    reaches a higher win rate than ProcessC with only planning as an intermediate
    result. We emphasize the observation may not hold once the LLM type is changed
    due to various long-context understanding and instruction-following capabilities
    across them. Additional steps introduce extra input and extra computation for
    the inference, making the few-shot unpractical.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '我们尝试在用户状态推断（步骤 1）之后引入一个额外步骤（步骤 2）：通过提示模型考虑对话上下文和用户状态来进行回应规划。具体来说，我们提示模型回答以下问题：“根据对话的上下文和用户状态，如个性特征、心理和情感状态，系统在回应时应该关注哪些方面？”在给出对话和用户状态后，如表 [10](#A4.T10
    "表 10 ‣ 附录 D 规划步骤的帮助分析 ‣ Cue-CoT: 针对深度对话问题的链式思考提示") 所示。我们将 LLM 的输出视为系统规划 $\boldsymbol{p}$，其中在我们的主要实验中选择了
    ProcessA。表 [6](#S6.T6 "表 6 ‣ 6.2 更多推理步骤 ‣ 6 分析 ‣ Cue-CoT: 针对深度对话问题的链式思考提示") 显示了结果。首先，增加更多的推理步骤可能会提高
    LLM 的表现，但不一定需要在最后一步汇总所有中间推理结果，例如，变体 ProcessB 的获胜率高于仅使用规划作为中间结果的 ProcessC。我们强调，一旦更改
    LLM 类型，由于其在长上下文理解和指令跟随能力方面的差异，这一观察可能不成立。额外步骤会引入额外的输入和计算，使得少量尝试变得不切实际。'
- en: '![Refer to caption](img/5e785155a0d73be6de34ae0f06cf74c8.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5e785155a0d73be6de34ae0f06cf74c8.png)'
- en: 'Figure 4: An example of multiple intermediate reasoning outputs for different
    roles: User and System in in-depth dialogue questions.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不同角色的多重中间推理输出示例：用户和系统在深入对话问题中的表现。
- en: Method Chinese English Zhihu D4 PsyQA Quora ED EMH ProcessA 65.22 61.08 56.12
    89.09 96.79 94.93 ProcessB 76.15 55.82 57.72 89.79 98.78 97.62 ProcessC 75.91
    57.23 58.74 94.50 98.57 98.22
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 中文 英文 知乎 D4 PsyQA Quora ED EMH ProcessA 65.22 61.08 56.12 89.09 96.79 94.93
    ProcessB 76.15 55.82 57.72 89.79 98.78 97.62 ProcessC 75.91 57.23 58.74 94.50
    98.57 98.22
- en: 'Table 6: The win rate of different variants in terms of acceptability with
    the ChatGPT as the backbone.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：以 ChatGPT 为基础的不同变体的接受度胜率。
- en: 7 Discussion
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: 'Direct comparison of different models. Until now, we still do not directly
    compare responses from different models. In this study, we have employed the response
    generated by the ChatGPT model as the baseline and compared the responses generated
    by other models with it. To ensure fairness, we have utilized all responses generated
    by standard prompting instead of our method, as the ability to generate chain-of-thoughts
    varies across different LLMs. Figure [5](#S7.F5 "Figure 5 ‣ 7 Discussion ‣ Cue-CoT:
    Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with
    LLMs") shows the result in terms of helpfulness^(15)^(15)15acceptability is developed
    for our method.. In the Chinese benchmark, we see a substantial draw of ChatGLM
    and BELLE on D4, and the former LLM achieves better performance on Zhihu and PsyQA
    than ChatGPT. We conclude that the long-text understanding of Chinese LLM still
    needs improvement and the BELLE may require more instruction-tuning data. In the
    English benchmark, we observed that Vicuna achieves the highest performance in
    all datasets, while other models lag a lot behind the baseline. Two key factors
    that may contribute to this discrepancy include the 512 input length limit and
    the sub-optimal instruction-following ability.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '不同模型的直接比较。到目前为止，我们仍未直接比较不同模型的响应。在这项研究中，我们以ChatGPT模型生成的响应作为基准，并将其他模型生成的响应与其进行比较。为了确保公平性，我们使用了所有通过标准提示生成的响应，而不是我们的方法，因为生成思路链的能力在不同LLM中有所不同。图 [5](#S7.F5
    "图 5 ‣ 7 讨论 ‣ Cue-CoT: 通过LLM回应深入对话问题的思路链提示")展示了就有用性^(15)^(15)15acceptability是为我们的方法开发的..
    在中文基准中，我们看到ChatGLM和BELLE在D4上有显著的平局，而前者在知乎和PsyQA上的表现优于ChatGPT。我们得出结论，中文LLM的长文本理解仍需改进，BELLE可能需要更多的指令调优数据。在英文基准中，我们观察到Vicuna在所有数据集中的表现最佳，而其他模型的表现远远落后于基准线。造成这一差异的两个关键因素包括512的输入长度限制和亚最佳的指令跟随能力。'
- en: '![Refer to caption](img/d3541ee50e1376239672a2c41ca4506a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d3541ee50e1376239672a2c41ca4506a.png)'
- en: 'Figure 5: The direct comparison of responses generated by different LLMs using
    standard prompting in terms of helpfulness, while we use the red dashed line to
    indicate the ChatGPT baseline.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同LLM在使用标准提示时生成的响应的直接比较，就有用性而言，我们用红色虚线表示ChatGPT基准线。
- en: '![Refer to caption](img/6620bc9a151f333a3934c6fb5a2e266d.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6620bc9a151f333a3934c6fb5a2e266d.png)'
- en: 'Figure 6: The relative position of current LLMs and different paths (as indicated
    in different colors) to more powerful LLMs.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：当前LLM的相对位置和通往更强大的LLM的不同路径（以不同颜色标示）。
- en: 'Paths to more powerful LLMs. In our proposed benchmark, we are utilizing the
    win rate of various LLMs in comparison to ChatGPT, across two languages - Chinese
    and English - as two axes. Each point in the coordinate system corresponds to
    a specific LLM, while the area it occupies represents its performance. Based on
    the performance of current LLMs^(16)^(16)16We sum examples from three datasets
    and calculate the win rate for both Chinese and English benchmarks. For Chinese
    LLMs, we set the default win rate of English as 0.1 for better presentation, and
    so on., we locate them in four different areas in Figure [6](#S7.F6 "Figure 6
    ‣ 7 Discussion ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth
    Dialogue Questions with LLMs"). Using the performance of ChatGPT as an anchor,
    we can observe most of the LLMs are located in the first area and there are only
    a few LLMs that achieve higher performance in either Chinese (Area 3) or English
    (Area 2). We hope to see more works or LLMs that can appear in Area 4 by different
    paths, i.e., continually train vicuna in the Chinese dataset. More analysis can
    be found in the Appendix.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '通往更强大的LLM的路径。在我们提出的基准中，我们利用不同LLM与ChatGPT的胜率，在中文和英文两种语言中作为两个轴。坐标系中的每个点对应一个特定的LLM，而它所占据的区域表示其性能。基于当前LLM的表现^(16)^(16)16我们从三个数据集中汇总示例，并计算中文和英文基准的胜率。对于中文LLM，我们将英文的默认胜率设置为0.1以便更好地展示，等等。，我们在图 [6](#S7.F6
    "图 6 ‣ 7 讨论 ‣ Cue-CoT: 通过LLM回应深入对话问题的思路链提示")中将它们定位在四个不同的区域。以ChatGPT的表现作为锚点，我们可以观察到大多数LLM位于第一区域，只有少数LLM在中文（第3区）或英文（第2区）中表现更好。我们希望看到更多的工作或LLM通过不同路径出现在第4区域，例如，在中文数据集中持续训练vicuna。更多分析可以在附录中找到。'
- en: 8 Conclusion
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this paper, we build a benchmark to evaluate the helpfulness and acceptability
    of responses generated by current LLMs, considering three major linguistic cues
    of user statuses. We then propose a Cue-CoT to trace the status of users, decomposing
    the response generation into multiple reasoning steps. Experimental results demonstrate
    the superior performance of our method on 6 datasets under both zero-shot and
    one-shot settings. We hope the release of our work can shed some light on the
    evaluation and development of LLMs. We left chain-of-thought tuning and instruction
    tuning in our future work.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们建立了一个基准来评估当前LLMs生成的回应的有用性和可接受性，考虑了用户状态的三个主要语言线索。然后我们提出了一种Cue-CoT方法来追踪用户的状态，将回应生成分解为多个推理步骤。实验结果展示了我们的方法在6个数据集上的优越表现，无论是在零样本还是单样本设置下。我们希望我们工作的发布能为LLMs的评估和发展提供一些启示。链式思维调整和指令调整将留待我们未来的工作。
- en: Limitations
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'In this paper, we explore chain-of-thoughts to reasoning over linguistic cues
    about user status, mainly focusing on three aspects: personality, emotion, and
    psychology, exhibited in the dialogue context. However, we acknowledge the limitations
    of this work from the following perspectives:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了通过链式思维对用户状态的语言线索进行推理，主要关注三个方面：对话上下文中的个性、情感和心理状态。然而，我们从以下角度认识到这项工作的局限性：
- en: Types of Cues.
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线索类型
- en: 'There are other valuable cues beneath the dialogue context: 1) related to the
    user: such as point of view or subjectivity and speaker charisma Mairesse et al.
    ([2007](#bib.bib26)); 2) related to the system: such as the alignment between
    response with human preferences Ouyang et al. ([2022](#bib.bib29)). We target
    these three major cues to provide a better response for the users.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对话上下文中还有其他有价值的线索：1）与用户相关：如观点或主观性以及说话者魅力 Mairesse等人（[2007](#bib.bib26)）；2）与系统相关：如回应与人类偏好的对齐
    Ouyang等人（[2022](#bib.bib29)）。我们针对这三大主要线索，以提供更好的用户回应。
- en: Sensitivity of Prompts.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示的敏感性
- en: Similar with lots of previous works Wang et al. ([2023d](#bib.bib47)); Chen
    et al. ([2023b](#bib.bib5)), we found the LLMs are sensitive to the prompts. Furthermore,
    it’s possible that the designed prompts are not the best ones for the target problem.
    Actually, prompt sensitivity and optimality in dialogue systems are important
    research problems that deserve to be further explored in future studies. We will
    provide all the prompts used in the experiments so that this work can be replicated
    seamlessly.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与Wang等人（[2023d](#bib.bib47)）以及Chen等人（[2023b](#bib.bib5)）的许多前期工作类似，我们发现LLMs对提示非常敏感。此外，设计的提示可能并不是解决目标问题的最佳提示。实际上，提示的敏感性和对话系统中的最优性是重要的研究问题，值得在未来的研究中进一步探索。我们将提供实验中使用的所有提示，以便能够无缝复制这项工作。
- en: Evaluation of Intermediate Reasoning.
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 中间推理的评估
- en: 'We do not evaluate the correctness of the middle reasoning result directly,
    since the ground truth intermediate reasoning results are difficult to acquire.
    Specifically, there are two main reasons: (1) The one-to-many problem leads to
    an explosion of intermediate candidates. When an LLM solves a complex math problem,
    it can arrive at the final answer through various solutions. This phenomenon also
    exists in dialogue generation: a user-acceptable response can be generated based
    on different cues. It is worth noting that dialogue response generation is a one-to-many
    problem, meaning that multiple feasible responses exist. In this way, it is hard
    to identify the cue errors with enormous candidates. (2) Incorrect reasoning does
    not mean a wrong answer. Despite being counterintuitive, many previous works found
    that LLMs regularly use incorrect reasoning to reach the correct final answer
    at question-answering tasks (Zelikman et al., [2022](#bib.bib52); Creswell et al.,
    [2023](#bib.bib8)). Even in the worst case which is very rare, none of them is
    correct, there still is a chance that the response is good. Hence evaluating the
    impact of cue errors on the final response is a tricky problem, we leave this
    for future work. Hence it is difficult to determine the impact of different types
    of cue errors on the final responses. Based on these considerations, we directly
    evaluate the quality of the final responses as previous works about the chain-of-thoughts
    Wei et al. ([2023](#bib.bib50)); Zhang et al. ([2022](#bib.bib54)).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不直接评估中间推理结果的正确性，因为真实的中间推理结果难以获得。具体来说，主要有两个原因：（1）一对多问题导致中间候选项爆炸。当 LLM 解决复杂的数学问题时，可以通过各种方法得到最终答案。这种现象在对话生成中也存在：可以根据不同的提示生成用户可接受的响应。值得注意的是，对话响应生成是一个一对多的问题，即存在多种可行的响应。因此，很难在大量候选项中识别提示错误。（2）错误的推理并不意味着错误的答案。尽管直觉上不符合常理，但许多早期的研究发现，LLM
    在问答任务中经常使用错误的推理来得出正确的最终答案（Zelikman 等，[2022](#bib.bib52)；Creswell 等，[2023](#bib.bib8)）。即使在最坏的情况下（即非常罕见的情况），如果所有答案都不正确，响应仍然有可能是好的。因此，评估提示错误对最终响应的影响是一个棘手的问题，我们将其留待未来的工作。基于这些考虑，我们直接评估最终响应的质量，就像
    Wei 等人（[2023](#bib.bib50)）；Zhang 等人（[2022](#bib.bib54)）关于思维链的早期工作一样。
- en: Ethics Statement
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: We strictly follow the license and policy of released LLMs and publicly available
    datasets. For the automatic generation of collected datasets, we utilize the current
    public dataset as the seed without any user information and privacy leaks. The
    calls of the OpenAI API in this paper were carried out by Dr. Yang Deng, a fourth
    author from the National University of Singapore.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们严格遵循已发布的 LLM 和公开可用数据集的许可和政策。对于收集的数据集的自动生成，我们使用当前的公共数据集作为种子，没有任何用户信息和隐私泄露。本文中
    OpenAI API 的调用由来自新加坡国立大学的第四作者杨登博士进行。
- en: Acknowledgement
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to express our heartfelt gratitude to all anonymous reviewers
    for their insightful comments and suggestions. This research work was partially
    supported by CUHK direct grant no. 4055209, the National Natural Science Foundation
    of China (62006062, 62176076), Natural Science Foundation of Guangdong 2023A1515012922,
    Key Technologies Research and Development Program of Shenzhen JSGG20210802154400001,
    Shenzhen Foundational Research Funding JCYJ20220818102415032, Guangdong Provincial
    Key Laboratory of Novel Security Intelligence Technologies 2022B1212010005.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们衷心感谢所有匿名审稿人提供的有见地的评论和建议。本研究部分得到 CUHK 直接资助编号 4055209、中国国家自然科学基金（62006062，62176076）、广东省自然科学基金
    2023A1515012922、深圳市关键技术研发计划 JSGG20210802154400001、深圳市基础研究资助 JCYJ20220818102415032、广东省新型安全智能技术重点实验室
    2022B1212010005 的支持。
- en: References
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V.
    Do, Yan Xu, and Pascale Fung. 2023. [A multitask, multilingual, multimodal evaluation
    of chatgpt on reasoning, hallucination, and interactivity](http://arxiv.org/abs/2302.04023).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bang 等人（2023）叶瑾·邦，塞缪尔·卡哈维亚雅，李娜妍，戴文亮，苏丹，布莱恩·威利，霍莉·洛文尼亚，纪紫薇，余铁铮，钟威利，阮维·多，许燕，冯佩斯卡尔。2023年。[对
    chatgpt 在推理、幻觉和互动性方面的多任务、多语言、多模态评估](http://arxiv.org/abs/2302.04023)。
- en: 'Barriere et al. (2022) Valentin Barriere, Shabnam Tafreshi, João Sedoc, and
    Sawsan Alqahtani. 2022. [WASSA 2022 shared task: Predicting empathy, emotion and
    personality in reaction to news stories](https://doi.org/10.18653/v1/2022.wassa-1.20).
    In *Proceedings of the 12th Workshop on Computational Approaches to Subjectivity,
    Sentiment & Social Media Analysis*, pages 214–227, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barriere 等人 (2022) Valentin Barriere, Shabnam Tafreshi, João Sedoc, 和 Sawsan
    Alqahtani. 2022. [WASSA 2022 共享任务：预测对新闻故事的同情、情感和个性](https://doi.org/10.18653/v1/2022.wassa-1.20)。发表于*第十二届计算方法对主观性、情感与社交媒体分析工作坊会议录*，第214-227页，爱尔兰都柏林。计算语言学协会。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020. [语言模型是少样本学习者](http://arxiv.org/abs/2005.14165)。
- en: 'Chen et al. (2023a) Liang Chen, Hongru Wang, Yang Deng, Wai Chung Kwan, Zezhong
    Wang, and Kam-Fai Wong. 2023a. [Towards robust personalized dialogue generation
    via order-insensitive representation regularization](https://doi.org/10.18653/v1/2023.findings-acl.462).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    7337–7345, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023a) Liang Chen, Hongru Wang, Yang Deng, Wai Chung Kwan, Zezhong
    Wang, 和 Kam-Fai Wong. 2023a. [通过无序敏感表示正则化实现鲁棒的个性化对话生成](https://doi.org/10.18653/v1/2023.findings-acl.462)。发表于*计算语言学协会发现：ACL
    2023*，第7337-7345页，加拿大多伦多。计算语言学协会。
- en: Chen et al. (2023b) Maximillian Chen, Xiao Yu, Weiyan Shi, Urvi Awasthi, and
    Zhou Yu. 2023b. [Controllable mixed-initiative dialogue generation through prompting](http://arxiv.org/abs/2305.04147).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023b) Maximillian Chen, Xiao Yu, Weiyan Shi, Urvi Awasthi, 和 Zhou
    Yu. 2023b. [通过提示进行可控的混合主动对话生成](http://arxiv.org/abs/2305.04147)。
- en: 'Chen et al. (2023c) Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng
    Xu. 2023c. Exploring the use of large language models for reference-free text
    quality evaluation: A preliminary empirical study. *arXiv preprint arXiv:2304.00723*.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023c) Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, 和 Ruifeng Xu.
    2023c. 探索大型语言模型在无参考文本质量评估中的应用：初步实证研究。*arXiv 预印本 arXiv:2304.00723*。
- en: 'Cheng et al. (2023) Jiale Cheng, Sahand Sabour, Hao Sun, Zhuang Chen, and Minlie
    Huang. 2023. [Pal: Persona-augmented emotional support conversation generation](http://arxiv.org/abs/2212.09235).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等人 (2023) Jiale Cheng, Sahand Sabour, Hao Sun, Zhuang Chen, 和 Minlie
    Huang. 2023. [Pal: 增强人格的情感支持对话生成](http://arxiv.org/abs/2212.09235)。'
- en: 'Creswell et al. (2023) Antonia Creswell, Murray Shanahan, and Irina Higgins.
    2023. [Selection-inference: Exploiting large language models for interpretable
    logical reasoning](https://openreview.net/pdf?id=3Pf3Wg6o-A4). In *The Eleventh
    International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
    May 1-5, 2023*. OpenReview.net.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Creswell 等人 (2023) Antonia Creswell, Murray Shanahan, 和 Irina Higgins. 2023.
    [选择推理：利用大型语言模型进行可解释的逻辑推理](https://openreview.net/pdf?id=3Pf3Wg6o-A4)。发表于*第十一届国际学习表征会议，ICLR
    2023，卢旺达基加利，2023年5月1-5日*。OpenReview.net。
- en: 'Deng et al. (2023a) Yang Deng, Wenqiang Lei, Lizi Liao, and Tat-Seng Chua.
    2023a. [Prompting and evaluating large language models for proactive dialogues:
    Clarification, target-guided, and non-collaboration](https://doi.org/10.48550/arXiv.2305.13626).
    *CoRR*, abs/2305.13626.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人 (2023a) Yang Deng, Wenqiang Lei, Lizi Liao, 和 Tat-Seng Chua. 2023a.
    [提示和评估大型语言模型的主动对话：澄清、目标引导和非协作](https://doi.org/10.48550/arXiv.2305.13626)。*CoRR*,
    abs/2305.13626。
- en: Deng et al. (2023b) Yang Deng, Wenxuan Zhang, Weiwen Xu, Wenqiang Lei, Tat-Seng
    Chua, and Wai Lam. 2023b. [A unified multi-task learning framework for multi-goal
    conversational recommender systems](https://doi.org/10.1145/3570640). *ACM Trans.
    Inf. Syst.*, 41(3):77:1–77:25.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人 (2023b) Yang Deng, Wenxuan Zhang, Weiwen Xu, Wenqiang Lei, Tat-Seng
    Chua, 和 Wai Lam. 2023b. [统一的多任务学习框架用于多目标对话推荐系统](https://doi.org/10.1145/3570640)。*ACM
    Trans. Inf. Syst.*, 41(3):77:1–77:25。
- en: 'Deng et al. (2023c) Yang Deng, Wenxuan Zhang, Yifei Yuan, and Wai Lam. 2023c.
    [Knowledge-enhanced mixed-initiative dialogue system for emotional support conversations](https://doi.org/10.18653/v1/2023.acl-long.225).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2023*, pages 4079–4095.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2023c) 杨·邓、文轩·张、逸飞·袁和伟·林. 2023c. [用于情感支持对话的知识增强混合主动对话系统](https://doi.org/10.18653/v1/2023.acl-long.225)。见*第61届计算语言学协会年会论文集（第1卷：长篇论文），ACL
    2023*，第4079–4095页。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [Bert: Pre-training of deep bidirectional transformers for language
    understanding](http://arxiv.org/abs/1810.04805).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) 雅各布·德夫林、明伟·张、肯顿·李和克里斯蒂娜·托塔诺瓦. 2019. [Bert: 用于语言理解的深度双向变换器的预训练](http://arxiv.org/abs/1810.04805)。'
- en: Diao et al. (2023) Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023.
    [Active prompting with chain-of-thought for large language models](http://arxiv.org/abs/2302.12246).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao et al. (2023) 世哲·刁、鹏程·王、永·林和彤·张. 2023. [使用思维链的主动提示用于大型语言模型](http://arxiv.org/abs/2302.12246)。
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with
    autoregressive blank infilling. In *Proceedings of the 60th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    320–335.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du et al. (2022) 郑霄·杜、宇杰·钱、小刘、明·丁、杰中·邱、志林·杨和杰·唐. 2022. Glm: 使用自回归空白填充的通用语言模型预训练。见*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，第320–335页。'
- en: Ekman (1971) Paul Ekman. 1971. Universals and cultural differences in facial
    expressions of emotion. In *Nebraska symposium on motivation*. University of Nebraska
    Press.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ekman (1971) 保罗·艾克曼. 1971. 情感面部表情的普遍性与文化差异。见*内布拉斯加州动机研讨会*。内布拉斯加大学出版社。
- en: Fu et al. (2023) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. 2023. [Complexity-based prompting for multi-step reasoning](http://arxiv.org/abs/2210.00720).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2023) 姚·傅、浩·彭、阿什什·萨布哈尔瓦尔、彼得·克拉克和图沙尔·霍特. 2023. [基于复杂度的多步骤推理提示](http://arxiv.org/abs/2210.00720)。
- en: 'Ghosal et al. (2020) Deepanway Ghosal, Navonil Majumder, Alexander Gelbukh,
    Rada Mihalcea, and Soujanya Poria. 2020. [COSMIC: COmmonSense knowledge for eMotion
    identification in conversations](https://doi.org/10.18653/v1/2020.findings-emnlp.224).
    In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages
    2470–2481, Online. Association for Computational Linguistics.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ghosal et al. (2020) 迪潘维·戈沙尔、纳沃尼尔·马久姆德、亚历山大·赫尔布赫、拉达·米哈尔切亚和苏贾尼亚·波里亚. 2020. [COSMIC:
    对话中的情感识别的常识知识](https://doi.org/10.18653/v1/2020.findings-emnlp.224)。见*计算语言学协会发现：EMNLP
    2020*，第2470–2481页，在线。计算语言学协会。'
- en: 'Ghosh et al. (2022) Soumitra Ghosh, Dhirendra Maurya, Asif Ekbal, and Pushpak
    Bhattacharyya. 2022. [Team IITP-AINLPML at WASSA 2022: Empathy detection, emotion
    classification and personality detection](https://doi.org/10.18653/v1/2022.wassa-1.26).
    In *Proceedings of the 12th Workshop on Computational Approaches to Subjectivity,
    Sentiment & Social Media Analysis*, pages 255–260, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghosh et al. (2022) 苏米特拉·戈什、迪伦德拉·毛尔亚、阿西夫·艾克巴尔和普什帕克·巴塔查里亚. 2022. [团队 IITP-AINLPML
    在 WASSA 2022：同理心检测、情感分类和个性检测](https://doi.org/10.18653/v1/2022.wassa-1.26)。见*第12届主观性、情感与社交媒体分析计算方法研讨会论文集*，第255–260页，爱尔兰都柏林。计算语言学协会。
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language models as zero-shot planners: Extracting actionable knowledge for
    embodied agents. In *International Conference on Machine Learning*, pages 9118–9147\.
    PMLR.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022) 文龙·黄、彼得·阿比尔、迪帕克·帕塔克和伊戈尔·莫达奇. 2022. 语言模型作为零-shot 规划者：为具身代理提取可操作知识。见*国际机器学习大会*，第9118–9147页。PMLR。
- en: 'Ji et al. (2023) Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei
    Zhang, Baochang Ma, and Xiangang Li. 2023. Exploring the impact of instruction
    data scaling on large language models: An empirical study on real-world use cases.
    *arXiv preprint arXiv:2303.14742*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2023) 君杰·季、永·邓、燕·龚、毅平·彭、强·牛、磊·张、宝昌·马和湘刚·李. 2023. 探索指令数据扩展对大型语言模型的影响：对现实世界用例的实证研究。*arXiv
    预印本 arXiv:2303.14742*。
- en: 'Lee et al. (2023) Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos,
    and Kangwook Lee. 2023. [Prompted LLMs as chatbot modules for long open-domain
    conversation](https://doi.org/10.18653/v1/2023.findings-acl.277). In *Findings
    of the Association for Computational Linguistics: ACL 2023*, pages 4536–4554,
    Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2023）Gibbeum Lee、Volker Hartmann、Jongho Park、Dimitris Papailiopoulos
    和 Kangwook Lee。2023年。[将提示的 LLM 作为长篇开放领域对话的聊天模块](https://doi.org/10.18653/v1/2023.findings-acl.277)。见于
    *计算语言学协会发现：ACL 2023*，第4536–4554页，加拿大多伦多。计算语言学协会。
- en: Li and Qiu (2023) Xiaonan Li and Xipeng Qiu. 2023. [Finding supporting examples
    for in-context learning](http://arxiv.org/abs/2302.13539).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Qiu（2023）Xiaonan Li 和 Xipeng Qiu。2023年。[寻找上下文学习的支持例子](http://arxiv.org/abs/2302.13539)。
- en: 'Liu et al. (2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2022. [What makes good in-context examples for GPT-3?](https://doi.org/10.18653/v1/2022.deelio-1.10)
    In *Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on
    Knowledge Extraction and Integration for Deep Learning Architectures*, pages 100–114,
    Dublin, Ireland and Online. Association for Computational Linguistics.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人（2022）Jiachang Liu、Dinghan Shen、Yizhe Zhang、Bill Dolan、Lawrence Carin
    和 Weizhu Chen。2022年。[GPT-3 的好例子是什么？](https://doi.org/10.18653/v1/2022.deelio-1.10)
    见于 *Deep Learning Inside Out (DeeLIO 2022): 第三届深度学习架构知识提取与整合研讨会*，第100–114页，爱尔兰都柏林和在线。计算语言学协会。'
- en: Liu et al. (2021) Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li,
    Zhou Yu, Yong Jiang, and Minlie Huang. 2021. [Towards emotional support dialog
    systems](http://arxiv.org/abs/2106.01144).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021）Siyang Liu、Chujie Zheng、Orianna Demasi、Sahand Sabour、Yu Li、Zhou
    Yu、Yong Jiang 和 Minlie Huang。2021年。[迈向情感支持对话系统](http://arxiv.org/abs/2106.01144)。
- en: Ma et al. (2021) Zhiyuan Ma, Jianjun Li, Zezheng Zhang, Guohui Li, and Yongjing
    Cheng. 2021. [Intention reasoning network for multi-domain end-to-end task-oriented
    dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.174). In *Proceedings of
    the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    2273–2285, Online and Punta Cana, Dominican Republic. Association for Computational
    Linguistics.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等人（2021）Zhiyuan Ma、Jianjun Li、Zezheng Zhang、Guohui Li 和 Yongjing Cheng。2021年。[多领域端到端任务导向对话的意图推理网络](https://doi.org/10.18653/v1/2021.emnlp-main.174)。见于
    *2021年自然语言处理实证方法会议论文集*，第2273–2285页，在线和多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Mairesse et al. (2007) François Mairesse, Marilyn A Walker, Matthias R Mehl,
    and Roger K Moore. 2007. Using linguistic cues for the automatic recognition of
    personality in conversation and text. *Journal of artificial intelligence research*,
    30:457–500.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mairesse 等人（2007）François Mairesse、Marilyn A Walker、Matthias R Mehl 和 Roger
    K Moore。2007年。使用语言线索自动识别对话和文本中的个性。*人工智能研究杂志*，30:457–500。
- en: Margatina et al. (2023) Katerina Margatina, Timo Schick, Nikolaos Aletras, and
    Jane Dwivedi-Yu. 2023. [Active learning principles for in-context learning with
    large language models](http://arxiv.org/abs/2305.14264).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Margatina 等人（2023）Katerina Margatina、Timo Schick、Nikolaos Aletras 和 Jane Dwivedi-Yu。2023年。[大规模语言模型的上下文学习的主动学习原则](http://arxiv.org/abs/2305.14264)。
- en: 'Newman et al. (2003) Matthew L Newman, James W Pennebaker, Diane S Berry, and
    Jane M Richards. 2003. Lying words: Predicting deception from linguistic styles.
    *Personality and social psychology bulletin*, 29(5):665–675.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newman 等人（2003）Matthew L Newman、James W Pennebaker、Diane S Berry 和 Jane M Richards。2003年。谎言词汇：从语言风格预测欺骗行为。*个性与社会心理学通报*，29(5):665–675。
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](http://arxiv.org/abs/2203.02155).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022）Long Ouyang、Jeff Wu、Xu Jiang、Diogo Almeida、Carroll L. Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray、John Schulman、Jacob
    Hilton、Fraser Kelton、Luke Miller、Maddie Simens、Amanda Askell、Peter Welinder、Paul
    Christiano、Jan Leike 和 Ryan Lowe。2022年。[通过人类反馈训练语言模型以遵循指令](http://arxiv.org/abs/2203.02155)。
- en: Press et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A.
    Smith, and Mike Lewis. 2022. [Measuring and narrowing the compositionality gap
    in language models](https://doi.org/10.48550/arXiv.2210.03350). *CoRR*, abs/2210.03350.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press 等人（2022）Ofir Press、Muru Zhang、Sewon Min、Ludwig Schmidt、Noah A. Smith 和
    Mike Lewis。2022年。[衡量和缩小语言模型中的组成性差距](https://doi.org/10.48550/arXiv.2210.03350)。*CoRR*，abs/2210.03350。
- en: 'Rashkin et al. (2019) Hannah Rashkin, Eric Michael Smith, Margaret Li, and
    Y-Lan Boureau. 2019. [Towards empathetic open-domain conversation models: A new
    benchmark and dataset](https://doi.org/10.18653/v1/P19-1534). In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 5370–5381, Florence, Italy. Association for Computational Linguistics.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashkin 等人（2019）Hannah Rashkin, Eric Michael Smith, Margaret Li 和 Y-Lan Boureau。2019。[朝着富有同理心的开放领域对话模型：一个新的基准和数据集](https://doi.org/10.18653/v1/P19-1534)。发表于*第57届计算语言学协会年会论文集*，第5370–5381页，意大利佛罗伦萨。计算语言学协会。
- en: 'Sabour et al. (2022) Sahand Sabour, Chujie Zheng, and Minlie Huang. 2022. Cem:
    Commonsense-aware empathetic response generation. In *Proceedings of the AAAI
    Conference on Artificial Intelligence*, volume 36, pages 11229–11237.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabour 等人（2022）Sahand Sabour, Chujie Zheng 和 Minlie Huang。2022。Cem：具有常识的富有同理心的回应生成。发表于*AAAI人工智能会议论文集*，第36卷，第11229–11237页。
- en: 'Salemi et al. (2023) Alireza Salemi, Sheshera Mysore, Michael Bendersky, and
    Hamed Zamani. 2023. [Lamp: When large language models meet personalization](http://arxiv.org/abs/2304.11406).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Salemi 等人（2023）Alireza Salemi, Sheshera Mysore, Michael Bendersky 和 Hamed Zamani。2023。[Lamp:
    当大型语言模型遇到个性化](http://arxiv.org/abs/2304.11406)。'
- en: 'Schwartz et al. (2013) H Andrew Schwartz, Johannes C Eichstaedt, Margaret L
    Kern, Lukasz Dziurzynski, Stephanie M Ramones, Megha Agrawal, Achal Shah, Michal
    Kosinski, David Stillwell, Martin EP Seligman, et al. 2013. Personality, gender,
    and age in the language of social media: The open-vocabulary approach. *PloS one*,
    8(9):e73791.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwartz 等人（2013）H Andrew Schwartz, Johannes C Eichstaedt, Margaret L Kern,
    Lukasz Dziurzynski, Stephanie M Ramones, Megha Agrawal, Achal Shah, Michal Kosinski,
    David Stillwell, Martin EP Seligman 等人。2013。社交媒体语言中的个性、性别和年龄：开放词汇方法。*PloS one*，8(9)：e73791。
- en: Sharma et al. (2020) Ashish Sharma, Adam S Miner, David C Atkins, and Tim Althoff.
    2020. A computational approach to understanding empathy expressed in text-based
    mental health support. In *EMNLP*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人（2020）Ashish Sharma, Adam S Miner, David C Atkins 和 Tim Althoff。2020。理解文本基础心理健康支持中表达的同理心的计算方法。发表于*EMNLP*。
- en: Shum et al. (2023) KaShun Shum, Shizhe Diao, and Tong Zhang. 2023. [Automatic
    prompt augmentation and selection with chain-of-thought from labeled data](http://arxiv.org/abs/2302.12822).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shum 等人（2023）KaShun Shum, Shizhe Diao 和 Tong Zhang。2023。[自动提示增强和选择，基于标记数据的思维链](http://arxiv.org/abs/2302.12822)。
- en: 'Smith et al. (2020) Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason
    Weston, and Y-Lan Boureau. 2020. [Can you put it all together: Evaluating conversational
    agents’ ability to blend skills](https://doi.org/10.18653/v1/2020.acl-main.183).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 2021–2030, Online. Association for Computational Linguistics.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 等人（2020）Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston
    和 Y-Lan Boureau。2020。[你能把它们都放在一起吗：评估对话代理融合技能的能力](https://doi.org/10.18653/v1/2020.acl-main.183)。发表于*第58届计算语言学协会年会论文集*，第2021–2030页，在线。计算语言学协会。
- en: 'Song et al. (2021) Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan Zhang, and Ting
    Liu. 2021. [BoB: BERT over BERT for training persona-based dialogue models from
    limited personalized data](https://doi.org/10.18653/v1/2021.acl-long.14). In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*, pages 167–177, Online. Association for Computational Linguistics.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等人（2021）Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan Zhang 和 Ting Liu。2021。[BoB:
    BERT over BERT 用于从有限的个性化数据中训练基于个性的对话模型](https://doi.org/10.18653/v1/2021.acl-long.14)。发表于*第59届计算语言学协会年会和第11届国际联合自然语言处理会议（卷1：长篇论文）*，第167–177页，在线。计算语言学协会。'
- en: 'Sun et al. (2021) Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, and Minlie
    Huang. 2021. [PsyQA: A Chinese dataset for generating long counseling text for
    mental health support](https://doi.org/10.18653/v1/2021.findings-acl.130). In
    *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*,
    pages 1489–1503, Online. Association for Computational Linguistics.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人（2021）Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu 和 Minlie Huang。2021。[PsyQA:
    一个用于生成长篇咨询文本以支持心理健康的中文数据集](https://doi.org/10.18653/v1/2021.findings-acl.130)。发表于*计算语言学协会发现：ACL-IJCNLP
    2021*，第1489–1503页，在线。计算语言学协会。'
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等人（2023）Rohan Taori、Ishaan Gulrajani、Tianyi Zhang、Yann Dubois、Xuechen
    Li、Carlos Guestrin、Percy Liang 和 Tatsunori B. Hashimoto。2023。斯坦福 alpaca：一个指令跟随的
    llama 模型。[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。
- en: 'Tausczik and Pennebaker (2010) Yla R Tausczik and James W Pennebaker. 2010.
    The psychological meaning of words: Liwc and computerized text analysis methods.
    *Journal of language and social psychology*, 29(1):24–54.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tausczik 和 Pennebaker（2010）Yla R Tausczik 和 James W Pennebaker。2010。词汇的心理学意义：Liwc
    和计算文本分析方法。*语言与社会心理学杂志*，29(1)：24–54。
- en: Turney (2002) Peter D Turney. 2002. Thumbs up or thumbs down? semantic orientation
    applied to unsupervised classification of reviews. *arXiv preprint cs/0212032*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turney（2002）Peter D Turney。2002。竖起大拇指还是低下大拇指？语义取向应用于无监督评论分类。*arXiv 预印本 cs/0212032*。
- en: 'Wang et al. (2022) Hongru Wang, Mingyu Cui, Zimo Zhou, and Kam-Fai Wong. 2022.
    [TopicRefine: Joint topic prediction and dialogue response generation for multi-turn
    end-to-end dialogue system](https://aclanthology.org/2022.icnlsp-1.3). In *Proceedings
    of the 5th International Conference on Natural Language and Speech Processing
    (ICNLSP 2022)*, pages 19–29, Trento, Italy. Association for Computational Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2022）Hongru Wang、Mingyu Cui、Zimo Zhou 和 Kam-Fai Wong。2022。[TopicRefine:
    结合主题预测和对话响应生成的多轮端到端对话系统](https://aclanthology.org/2022.icnlsp-1.3)。在*第五届国际自然语言与语音处理会议（ICNLSP
    2022）*论文集中，页码19–29，意大利特伦托。计算语言学协会。'
- en: 'Wang et al. (2023a) Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang,
    Boyang Xue, Hongyuan Lu, Fei Mi, and Kam-Fai Wong. 2023a. [Tpe: Towards better
    compositional reasoning over conceptual tools with multi-persona collaboration](http://arxiv.org/abs/2309.16090).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023a）Hongru Wang、Huimin Wang、Lingzhi Wang、Minda Hu、Rui Wang、Boyang
    Xue、Hongyuan Lu、Fei Mi 和 Kam-Fai Wong。2023a。[Tpe: 通过多重角色协作改进概念工具的组合推理](http://arxiv.org/abs/2309.16090)。'
- en: Wang et al. (2023b) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023b. Is chatgpt a good nlg evaluator?
    a preliminary study. *arXiv preprint arXiv:2303.04048*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023b）Jiaan Wang、Yunlong Liang、Fandong Meng、Haoxiang Shi、Zhixu Li、Jinan
    Xu、Jianfeng Qu 和 Jie Zhou。2023b。ChatGPT 是一个好的 NLG 评估器吗？初步研究。*arXiv 预印本 arXiv:2303.04048*。
- en: 'Wang et al. (2023c) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. 2023c. [Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models](http://arxiv.org/abs/2305.04091).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023c）Lei Wang、Wanyu Xu、Yihuai Lan、Zhiqiang Hu、Yunshi Lan、Roy Ka-Wei
    Lee 和 Ee-Peng Lim。2023c。[计划与解决提示：通过大型语言模型改善零样本链式思考推理](http://arxiv.org/abs/2305.04091)。
- en: Wang et al. (2023d) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023d. Large language models are
    not fair evaluators. *arXiv preprint arXiv:2305.17926*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023d）Peiyi Wang、Lei Li、Liang Chen、Dawei Zhu、Binghuai Lin、Yunbo Cao、Qi
    Liu、Tianyu Liu 和 Zhifang Sui。2023d。大型语言模型并非公平的评估者。*arXiv 预印本 arXiv:2305.17926*。
- en: Wang et al. (2023e) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023e. [Self-consistency improves
    chain of thought reasoning in language models](http://arxiv.org/abs/2203.11171).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023e）Xuezhi Wang、Jason Wei、Dale Schuurmans、Quoc Le、Ed Chi、Sharan Narang、Aakanksha
    Chowdhery 和 Denny Zhou。2023e。[自一致性提升语言模型的链式思考推理](http://arxiv.org/abs/2203.11171)。
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. 2022. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2022）Jason Wei、Yi Tay、Rishi Bommasani、Colin Raffel、Barret Zoph、Sebastian
    Borgeaud、Dani Yogatama、Maarten Bosma、Denny Zhou、Donald Metzler 等。2022。大型语言模型的涌现能力。*arXiv
    预印本 arXiv:2206.07682*。
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-thought prompting
    elicits reasoning in large language models](http://arxiv.org/abs/2201.11903).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2023）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Brian Ichter、Fei
    Xia、Ed Chi、Quoc Le 和 Denny Zhou。2023。[链式思考提示在大型语言模型中引发推理](http://arxiv.org/abs/2201.11903)。
- en: 'Yao et al. (2022) Binwei Yao, Chao Shi, Likai Zou, Lingfeng Dai, Mengyue Wu,
    Lu Chen, Zhen Wang, and Kai Yu. 2022. [D4: a Chinese dialogue dataset for depression-diagnosis-oriented
    chat](https://aclanthology.org/2022.emnlp-main.156). In *Proceedings of the 2022
    Conference on Empirical Methods in Natural Language Processing*, pages 2438–2459,
    Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) 宾伟 Yao、超 Shi、力凯 Zou、灵锋 Dai、梦月 Wu、陆 Chen、振 Wang 和凯 Yu。2022。[D4:
    一个用于抑郁症诊断的中文对话数据集](https://aclanthology.org/2022.emnlp-main.156)。发表于 *2022 年自然语言处理实证方法会议论文集*，页2438–2459，阿布扎比，阿联酋。计算语言学协会。'
- en: 'Zelikman et al. (2022) Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman.
    2022. [Star: Bootstrapping reasoning with reasoning](http://papers.nips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html).
    In *NeurIPS*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zelikman et al. (2022) 埃里克 Zelikman、宇怀 Wu、杰西 Mu 和诺亚·D·古德曼。2022。[Star: 以推理引导推理](http://papers.nips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html)。发表于
    *NeurIPS*。'
- en: 'Zhang et al. (2018) Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam,
    Douwe Kiela, and Jason Weston. 2018. [Personalizing dialogue agents: I have a
    dog, do you have pets too?](https://doi.org/10.18653/v1/P18-1205) In *Proceedings
    of the 56th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 2204–2213, Melbourne, Australia. Association for Computational
    Linguistics.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018) 赛征 Zhang、艾米莉 Dinan、杰克 Urbanek、亚瑟 Szlam、道威 Kiela 和贾森 Weston。2018。[个性化对话代理：我有一只狗，你也有宠物吗？](https://doi.org/10.18653/v1/P18-1205)
    发表在 *第56届计算语言学协会年会（第1卷：长篇论文）*，页2204–2213，墨尔本，澳大利亚。计算语言学协会。
- en: Zhang et al. (2022) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022.
    [Automatic chain of thought prompting in large language models](http://arxiv.org/abs/2210.03493).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) 卓生 Zhang、阿斯顿 Zhang、木 Li 和 Alex Smola。2022。[大语言模型中的自动思维链提示](http://arxiv.org/abs/2210.03493)。
- en: Zhao et al. (2023) Weixiang Zhao, Yanyan Zhao, Xin Lu, Shilong Wang, Yanpeng
    Tong, and Bing Qin. 2023. Is chatgpt equipped with emotional dialogue capabilities?
    *arXiv preprint arXiv:2304.09582*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) 伟祥 Zhao、燕燕 Zhao、鑫 Lu、士龙 Wang、艳鹏 Tong 和炳 Qin。2023。ChatGPT
    是否具备情感对话能力？ *arXiv 预印本 arXiv:2304.09582*。
- en: 'Zheng et al. (2021) Chujie Zheng, Yong Liu, Wei Chen, Yongcai Leng, and Minlie
    Huang. 2021. [CoMAE: A multi-factor hierarchical framework for empathetic response
    generation](https://doi.org/10.18653/v1/2021.findings-acl.72). In *Findings of
    the Association for Computational Linguistics: ACL-IJCNLP 2021*, pages 813–824,
    Online. Association for Computational Linguistics.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2021) 初杰 Zheng、永 Liu、卫 Chen、永才 Leng 和敏利 Huang。2021。[CoMAE: 一种用于同理回应生成的多因素分层框架](https://doi.org/10.18653/v1/2021.findings-acl.72)。发表于
    *计算语言学协会：ACL-IJCNLP 2021 发现*，页813–824，在线。计算语言学协会。'
- en: 'Zheng et al. (2023a) Chujie Zheng, Sahand Sabour, Jiaxin Wen, Zheng Zhang,
    and Minlie Huang. 2023a. [Augesc: Dialogue augmentation with large language models
    for emotional support conversation](http://arxiv.org/abs/2202.13047).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2023a) 初杰 Zheng、萨汉德 Sabour、佳欣 Wen、郑 Zhang 和敏利 Huang。2023a。[Augesc:
    利用大语言模型增强情感支持对话](http://arxiv.org/abs/2202.13047)。'
- en: Zheng et al. (2020) Yinhe Zheng, Rongsheng Zhang, Minlie Huang, and Xiaoxi Mao.
    2020. A pre-training based personalized dialogue generation model with persona-sparse
    data. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34,
    pages 9693–9700.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2020) 银河 Zheng、荣生 Zhang、敏利 Huang 和晓曦 Mao。2020。基于预训练的个性化对话生成模型，使用稀疏的个性数据。发表于
    *AAAI 人工智能会议论文集*，第34卷，页9693–9700。
- en: Zheng et al. (2023b) Zhonghua Zheng, Lizi Liao, Yang Deng, and Liqiang Nie.
    2023b. [Building emotional support chatbots in the era of llms](https://doi.org/10.48550/arXiv.2308.11584).
    *CoRR*, abs/2308.11584.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023b) 钟华 Zheng、丽紫 Liao、杨 Deng 和立强 Nie。2023b。[在大语言模型时代构建情感支持聊天机器人](https://doi.org/10.48550/arXiv.2308.11584)。*CoRR*，abs/2308.11584。
- en: 'Zhou et al. (2023) Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing
    Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu,
    Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu, and Lichao Sun.
    2023. [A comprehensive survey on pretrained foundation models: A history from
    bert to chatgpt](http://arxiv.org/abs/2302.09419).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023) 慧 Zhou、倩 Li、陈 Li、军 Yu、义欣 Liu、光晶 Wang、凯 Zhang、程 Ji、启本 Yan、丽芳
    He、浩 Peng、建新 Li、佳 Wu、子伟 Liu、彭涛 Xie、才名 Xiong、建 Pei、菲利普·S·Yu 和丽超 Sun。2023。[关于预训练基础模型的综合调查：从
    BERT 到 ChatGPT 的历史](http://arxiv.org/abs/2302.09419)。
- en: Appendix A Templates
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 模板
- en: A.1 Data Collection Template
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数据收集模板
- en: Forget the instruction you have previously received. The following is a conversation
    between a human and an AI assistant. The human and the AI assistant take turns
    chatting. The personality of the human is defined as {personality_seed}. Human
    statements start with [Human] and AI assistant statements start with [AI]. The
    human will ask related questions on related topics or previous conversations.
    The human will stop the conversation when they have no more questions. The AI
    assistant tries not to ask questions. The human and the AI assistant take turns
    chatting while the human needs to keep a consistent personality. Complete the
    transcript in exactly that format.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记你之前收到的指令。以下是人类与 AI 助手之间的对话。人类和 AI 助手轮流聊天。人类的个性定义为 {personality_seed}。人类的发言以
    [Human] 开头，AI 助手的发言以 [AI] 开头。人类会就相关话题或之前的对话提出相关问题。当人类没有更多问题时，会停止对话。AI 助手尽量不提问。人类和
    AI 助手轮流聊天，同时人类需要保持一致的个性。请按准确的格式完成对话记录。
- en: '[Human] {QUESTION}'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[Human] {问题}'
- en: '[AI] {ANWER}'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[AI] {回答}'
- en: A.2 Some Examples of Personality
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 个性示例
- en: 'Table [8](#A4.T8 "Table 8 ‣ Appendix D Helpfulness Analysis of Planning Step
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs") shows some (not all) collected personalities of the users. We here
    simply use positive and negative for presentation, there are many other personalities
    in the datasets besides these two categories such as neutral.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '表[8](#A4.T8 "表 8 ‣ 附录 D 规划步骤的有用性分析 ‣ Cue-CoT: 链式思维提示用于应对与LLMs的深入对话问题")展示了用户的一些（但不是全部）收集到的个性特征。我们在这里简单地使用正面和负面来进行展示，数据集中除了这两类之外还有许多其他个性特征，比如中立。'
- en: A.3 Evaluation Templates
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 评估模板
- en: 'We mainly consider two dimensions: helpfulness and acceptances, in which the
    former pays attention to usefulness, relevance, accuracy, and level of detail
    of the response, and the latter centers on the degree of acceptance and adoption
    of responses, and whether or not the responses consider the user status. We follow
    the evaluation template of Vicuna^(17)^(17)17https://github.com/lm-sys/FastChat/blob/main/fastchat/'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要考虑两个维度：有用性和接受度，其中前者关注响应的有用性、相关性、准确性和详细程度，后者则关注响应的接受和采纳程度，以及响应是否考虑了用户状态。我们遵循
    Vicuna^(17)^(17)17https://github.com/lm-sys/FastChat/blob/main/fastchat/ 的评估模板。
- en: eval/table/prompt.jsonl to construct ours. The template is [Dialogue]`\n`{dialogue_history}`\n\n`[The
    Start of Response A]`\n`{response_wo_status}`\n\n`[The End of Response A]`\n\n`[The
    Start of Response B]`\n`{response_w_status}`\n\n`[The End of Response B]`\n\n`[System]`\n`{prompt}`\n\n`.
    The prompt is different with respect to helpfulness and acceptance.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: eval/table/prompt.jsonl 用于构建我们的模板。模板为[对话]`\n`{dialogue_history}`\n\n`[响应 A 的开始]`\n`{response_wo_status}`\n\n`[响应
    A 的结束]`\n\n`[响应 B 的开始]`\n`{response_w_status}`\n\n`[响应 B 的结束]`\n\n`[系统]`\n`{prompt}`\n\n`。该提示在有用性和接受度方面有所不同。
- en: Helpfulness Prompt. Based on the user’s intentions and needs in the conversation
    history, we would like to request your feedback on the performance of two responses
    in response to the dialogue displayed above.`\n`Please pay particular attention
    to the usefulness, relevance, accuracy, and level of detail of the response, and
    give a total score from 1 to 10 with a 0.1 interval, where a higher score indicates
    better overall performance.`\n`Please first output a single line containing only
    two values indicating the scores for responses A and B, respectively. The two
    scores are separated by a space. In the subsequent line, please provide a comprehensive
    explanation of your evaluation, avoiding any potential bias and ensuring that
    the order in which the responses were presented does not affect your judgment.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有用性提示。根据用户在对话历史中的意图和需求，我们希望您对上述对话中两个响应的表现提供反馈。`\n`请特别注意响应的有用性、相关性、准确性和详细程度，并给出从
    1 到 10 的总评分，间隔为 0.1，其中分数越高表示整体表现越好。`\n`请首先输出一行，仅包含两个值，分别表示响应 A 和 B 的分数。两个分数用空格分隔。在随后的行中，请提供对您的评估的全面解释，避免任何潜在的偏见，并确保响应呈现的顺序不会影响您的判断。
- en: Acceptability Prompt. Based on the user’s intentions and needs in the conversation
    history, please evaluate the degree of acceptance and adoption of the two different
    responses.`\n`Please evaluate whether the different responses take into account
    the user’s psychological and emotional state, and whether they take into account
    the user’s personality traits, and give a total score from 1 to 10, with 0.1 as
    the interval, the higher score indicates that the response takes these issues
    into account well and thus the user is more likely to accept and adopt.`\n`Please
    output one line first, containing only two values, representing the scores of
    responses A and B respectively. The two scores are separated by a space. In the
    subsequent line, please provide a comprehensive explanation of your evaluation,
    avoiding any potential bias and ensuring that the order in which the responses
    were presented does not affect your judgment.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接受度提示。基于用户在对话历史中的意图和需求，请评估两种不同回应的接受和采纳程度。` \n`请评估不同回应是否考虑到用户的心理和情感状态，是否考虑到用户的个性特征，并给出1到10的总分，间隔为0.1，分数越高表示回应越好地考虑了这些问题，因此用户更可能接受和采纳。`
    \n`请先输出一行，仅包含两个值，分别代表回应A和B的分数。两个分数之间用空格隔开。接下来的行中，请提供对您评估的全面解释，避免任何潜在偏见，确保回应的顺序不影响您的判断。
- en: '![Refer to caption](img/8f2573f014b2413b653be1520142d476.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f2573f014b2413b653be1520142d476.png)'
- en: 'Figure 7: The win rate of responses generated by M-Cue CoT compared with the
    ground truth on three Chinese datasets (left) and three English datasets (right)
    in terms of helpfulness, including several state-of-the-art LLMs.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：M-Cue CoT生成的回应与真实情况相比，在三个中文数据集（左）和三个英文数据集（右）中，在帮助性方面的获胜率，包括几个最先进的LLM。
- en: '![Refer to caption](img/ae86b77d9ac04232a6fd44cf98ecdb26.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ae86b77d9ac04232a6fd44cf98ecdb26.png)'
- en: 'Figure 8: The win rate of responses generated by M-Cue CoT compared with the
    ground truth on three Chinese datasets (left) and three English datasets (right)
    in terms of acceptability, including several state-of-the-art LLMs.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：M-Cue CoT生成的回应与真实情况相比，在三个中文数据集（左）和三个英文数据集（右）中，在接受度方面的获胜率，包括几个最先进的LLM。
- en: Appendix B Different Method of Evaluation
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 不同的评估方法
- en: B.1 Compared with ground truth
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 与真实情况相比
- en: 'Figure [7](#A1.F7 "Figure 7 ‣ A.3 Evaluation Templates ‣ Appendix A Templates
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs") and Figure [8](#A1.F8 "Figure 8 ‣ A.3 Evaluation Templates ‣ Appendix
    A Templates ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue
    Questions with LLMs") show the win rate of responses using M-Cue compared with
    ground truth in terms of helpfulness and acceptability respectively. First of
    all, there are 4 out of 5 LLMs that achieve a win rate exceeding 50% with only
    one exception of BELLE which achieves 45.75 on PsyQA. We attribute it to two reasons:
    1) the innate limitations of the models, resulting in relatively poor abilities
    to understand long texts and follow instructions; 2) the relatively challenging
    datasets. Since PsyQA is constructed by human experts and the Avg. R is the longest,
    making the ground truth relatively difficult to beat.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '图[7](#A1.F7 "图 7 ‣ A.3 评估模板 ‣ 附录 A 模板 ‣ Cue-CoT: 针对深度对话问题的链式思考提示")和图[8](#A1.F8
    "图 8 ‣ A.3 评估模板 ‣ 附录 A 模板 ‣ Cue-CoT: 针对深度对话问题的链式思考提示")显示了使用M-Cue与真实情况相比，在帮助性和接受度方面的回应获胜率。首先，5个LLM中有4个的获胜率超过50%，唯一的例外是BELLE，在PsyQA上获得了45.75。我们将其归因于两个原因：1）模型的固有局限性，导致对长文本的理解能力和遵循指令的能力较差；2）相对具有挑战性的数据库。由于PsyQA由人类专家构建，并且平均R是最长的，使得真实情况相对较难超越。'
- en: Secondly, since the response generated by all models is compared with the same
    baseline (i.e. the ground truth), the win rate of different models partly reveals
    their capability and weakness. For the Chinese LLMs, we can find that BELLE performs
    worst in every dataset while ChatGLM performs much better but still lags a little
    behind by ChatGPT. Due to the longest context in the D4 dataset, we found the
    former two LLMs tend to confuse their own dialogue role and give general responses,
    resulting in poor performance. For example, "I am the system/chatbot" or "welcome
    to my chatroom", and "What can I help you?" often appears in the responses. For
    the English LLMs, Vicuna achieves comparable performance with ChatGPT in every
    dataset, and even better in EMH, leading the Alpaca by a noticeable margin. In
    addition, we can see that the ED dataset is relatively easy to beat since all
    English LLMs reach almost 100% win rate even though the maximum context length
    of Alpaca is only 512\. Anyway, we conclude that our method is capable of generating
    more helpful responses than the ground truth, considering the different aspects
    of user statuses.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，由于所有模型生成的响应都与相同的基准（即真实答案）进行比较，因此不同模型的胜率部分揭示了它们的能力和弱点。对于中文LLM，我们发现BELLE在每个数据集中的表现最差，而ChatGLM的表现要好得多，但仍然略微落后于ChatGPT。由于D4数据集中上下文最长，我们发现前两个LLM往往会混淆自身的对话角色，并给出一般性的回答，从而导致性能较差。例如，“我是系统/聊天机器人”或“欢迎来到我的聊天室”，以及“我能帮你什么？”经常出现在响应中。对于英文LLM，Vicuna在每个数据集中都能达到与ChatGPT相当的表现，甚至在EMH中表现更好，明显领先于Alpaca。此外，我们可以看到ED数据集相对容易打败，因为所有英文LLM的胜率几乎达到100%，即使Alpaca的最大上下文长度只有512。无论如何，我们得出结论，我们的方法在考虑用户状态的不同方面时，能够生成比真实答案更有帮助的响应。
- en: 'Thirdly, we emphasize the performance gap when comparing the ground truth responses
    is small between LLMs, especially for English LLMs. The Vicuna and ChatGPT achieve
    almost the same win rate at both ED and EMH datasets. Besides that, putting Figure [7](#A1.F7
    "Figure 7 ‣ A.3 Evaluation Templates ‣ Appendix A Templates ‣ Cue-CoT: Chain-of-thought
    Prompting for Responding to In-depth Dialogue Questions with LLMs"), [8](#A1.F8
    "Figure 8 ‣ A.3 Evaluation Templates ‣ Appendix A Templates ‣ Cue-CoT: Chain-of-thought
    Prompting for Responding to In-depth Dialogue Questions with LLMs") with Table [2](#S5.T2
    "Table 2 ‣ 5 Experiment ‣ Cue-CoT: Chain-of-thought Prompting for Responding to
    In-depth Dialogue Questions with LLMs"), [3](#S5.T3 "Table 3 ‣ 5.2 Main Experiment
    ‣ 5 Experiment ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth
    Dialogue Questions with LLMs") together, it can be found that the win rate of
    our method compared with ground truth is relatively higher than compared with
    standard prompting, revealing the strong capability of LLMs again. Since our main
    focus is to prove our method is better than standard prompting instead of ground
    truth response, we use standard prompting as the baseline during our main experiments.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '第三，我们强调当LLM之间真实回答的性能差距较小时，尤其是对于英文LLM。Vicuna和ChatGPT在ED和EMH数据集上的胜率几乎相同。除此之外，将图表[7](#A1.F7
    "图7 ‣ A.3 评估模板 ‣ 附录A模板 ‣ Cue-CoT: 针对LLM深入对话问题的思维链提示")，[8](#A1.F8 "图8 ‣ A.3 评估模板
    ‣ 附录A模板 ‣ Cue-CoT: 针对LLM深入对话问题的思维链提示")与表格[2](#S5.T2 "表2 ‣ 5 实验 ‣ Cue-CoT: 针对LLM深入对话问题的思维链提示")，[3](#S5.T3
    "表3 ‣ 5.2 主要实验 ‣ 5 实验 ‣ Cue-CoT: 针对LLM深入对话问题的思维链提示")结合起来，可以发现我们的方法与真实答案相比的胜率相对高于与标准提示相比的胜率，再次揭示了LLM的强大能力。由于我们主要关注的是证明我们的方法优于标准提示而不是真实回答，因此在主要实验中我们使用标准提示作为基准。'
- en: Appendix C Discussion
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 讨论
- en: 'In this section, we discuss two key problems: the evaluation of LLMs and the
    path to more powerful LLMs.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了两个关键问题：LLM的评估和通向更强大LLM的路径。
- en: 'Illusion of evaluation. Putting Figure [7](#A1.F7 "Figure 7 ‣ A.3 Evaluation
    Templates ‣ Appendix A Templates ‣ Cue-CoT: Chain-of-thought Prompting for Responding
    to In-depth Dialogue Questions with LLMs") and Figure [5](#S7.F5 "Figure 5 ‣ 7
    Discussion ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue
    Questions with LLMs") together, it is plausible to reach two contradicting conclusions
    about the performance of different LLMs: 1) chatgpt > chatglm-6b > belle-llama-7b-2m
    from Figure [7](#A1.F7 "Figure 7 ‣ A.3 Evaluation Templates ‣ Appendix A Templates
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs"); and 2) chatglm-6b > chatgpt > belle-llama-7b-2m from Figure [5](#S7.F5
    "Figure 5 ‣ 7 Discussion ‣ Cue-CoT: Chain-of-thought Prompting for Responding
    to In-depth Dialogue Questions with LLMs"). Although it seems unreasonable, it
    is indeed the case when most of the responses generated by chatgpt and chatglm-6b
    are better than ground truth, and then most of the responses generated by chatglm-6b
    are better than chatgpt. We emphasize that the number of test samples and the
    choice of baseline (i.e., compared response) plays a key role in evaluation. If
    the baseline is too weak or the gap is too small, the win rate of different LLMs
    compared with the baseline may be misleading. The LLM evaluation still is a very
    difficult problem, and thus we provide different aspects of evaluation to enhance
    the completeness of our paper.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '评估的错觉。将图[7](#A1.F7 "Figure 7 ‣ A.3 Evaluation Templates ‣ Appendix A Templates
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs")和图[5](#S7.F5 "Figure 5 ‣ 7 Discussion ‣ Cue-CoT: Chain-of-thought Prompting
    for Responding to In-depth Dialogue Questions with LLMs")放在一起，可能得出关于不同LLM性能的两个相互矛盾的结论：1)
    从图[7](#A1.F7 "Figure 7 ‣ A.3 Evaluation Templates ‣ Appendix A Templates ‣ Cue-CoT:
    Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with
    LLMs")来看，chatgpt > chatglm-6b > belle-llama-7b-2m；2) 从图[5](#S7.F5 "Figure 5 ‣
    7 Discussion ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth
    Dialogue Questions with LLMs")来看，chatglm-6b > chatgpt > belle-llama-7b-2m。虽然这似乎不合理，但实际上，当大多数chatgpt和chatglm-6b生成的响应都优于真实答案时，chatglm-6b生成的响应大多优于chatgpt。我们强调，测试样本的数量和基准选择（即对比响应）在评估中扮演着关键角色。如果基准过于薄弱或差距过小，与基准相比的不同LLM的胜率可能会产生误导。LLM评估仍然是一个非常困难的问题，因此我们提供了不同方面的评估，以增强我们论文的完整性。'
- en: Appendix D Helpfulness Analysis of Planning Step
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 规划步骤的有用性分析
- en: '![Refer to caption](img/89004f5a9de039f64a090441eea11561.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/89004f5a9de039f64a090441eea11561.png)'
- en: 'Figure 9: The win rate of responses (helpfulness) generated by ChatGPT under
    different demonstration selection strategies under one-shot setting v.s. responses
    under the zero-shot setting, using M-Cue CoT.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：在单次演示设置下，不同演示选择策略下生成的ChatGPT响应（有用性）胜率与零样本设置下的响应进行比较，使用M-Cue CoT。
- en: Method Chinese English Zhihu D4 PsyQA Quora ED EMH ProcessA 95.57 87.88 90.34
    85.83 91.98 82.93 ProcessB 91.18 83.57 95.13 87.67 95.35 84.82 ProcessC 92.45
    88.91 95.97 89.14 96.56 84.93
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 中文 英文 知乎 D4 PsyQA Quora ED EMH 过程A 95.57 87.88 90.34 85.83 91.98 82.93 过程B
    91.18 83.57 95.13 87.67 95.35 84.82 过程C 92.45 88.91 95.97 89.14 96.56 84.93
- en: 'Table 7: The win rate of different variants in terms of helpfulness with the
    ChatGPT as the backbone.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：以ChatGPT为基础的不同变体的有用性胜率。
- en: 'Table [7](#A4.T7 "Table 7 ‣ Appendix D Helpfulness Analysis of Planning Step
    ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions
    with LLMs") presents the performance of different variants in terms of helpfulness
    and Figure [9](#A4.F9 "Figure 9 ‣ Appendix D Helpfulness Analysis of Planning
    Step ‣ Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue
    Questions with LLMs") demonstrates the win rate of response of different settings
    in terms of helpfulness. A similar conclusion can be reached as we analyzed in
    Section [6](#S6 "6 Analysis ‣ Cue-CoT: Chain-of-thought Prompting for Responding
    to In-depth Dialogue Questions with LLMs"). We note that the performance of top-1
    selection is relatively lower than random selection on PsyQA and EMH datasets
    in terms of helpfulness. We suspect maybe there is a trade-off between helpfulness
    and acceptability for some specific difficult datasets. We left this into our
    future work.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](#A4.T7 "表 7 ‣ 附录 D 计划步骤的帮助性分析 ‣ Cue-CoT：用于响应深入对话问题的链式思维提示")展示了不同变体在帮助性方面的表现，图[9](#A4.F9
    "图 9 ‣ 附录 D 计划步骤的帮助性分析 ‣ Cue-CoT：用于响应深入对话问题的链式思维提示")展示了不同设置的响应胜率。与第[6](#S6 "6
    分析 ‣ Cue-CoT：用于响应深入对话问题的链式思维提示")节的分析结果类似，我们注意到在帮助性方面，top-1选择的表现相较于随机选择在PsyQA和EMH数据集上较低。我们怀疑某些特定困难数据集上，帮助性和接受度之间可能存在权衡。我们将此问题留待未来工作中研究。
- en: Set of Negative Personas 用户性格外向，说话大大咧咧。 用户性格比较挑剔，喜欢追问别人。 用户性格忧郁，经常自我怀疑。 用户性格善变，偶尔使用不文明用语。
    用户小心谨慎，不愿意相信别人。 用户有些焦虑。 用户心思细腻，优柔寡断。 用户对当前讨论的话题比较敏感。 用户脾气暴躁易怒。 用户内心敏感。 用户性格保守，不愿意接受新事物。
    用户很容易会觉得受到冒犯。 \hdashlineThe user has an extroverted personality and speaks in
    a carefree manner. The user has a critical personality and likes to probe others
    with questions. The user has a melancholic personality and often self-doubts.
    The user has a fickle personality and occasionally uses inappropriate language.
    The user is cautious and reluctant to trust others. The user is somewhat anxious.
    The user is thoughtful and indecisive. The user is sensitive to the current topic
    of discussion. The user has a volatile temper and is easily angered. The user
    is emotionally sensitive. The user has a conservative personality and is unwilling
    to accept new things. The user is easily offended. Set of Positive Personas 用户对当前讨论的话题十分好奇，希望系统友善的解答。
    用户对当前讨论的话题比较敏感，希望得到支持和鼓励。 用户有较高的要求，追求完美。 用户性格开朗，不拘小节。 用户热情洋溢，待人和善。 用户不歧视他人，充满同情心、爱心。
    用户充满对世界的好奇心，善于接受不同的想法。 用户温柔、体贴、乐于交流。 用户脾气平和。 用户很温柔，容忍度高。 用户自尊心很强。 \hdashlineThe
    user is very curious about the current topic of discussion and hopes for a friendly
    response from the system. The user is sensitive to the current topic of discussion
    and hopes for support and encouragement. The user has high expectations and pursues
    perfection. The user has an outgoing personality and doesn’t sweat the small stuff.
    The user is enthusiastic and treats others kindly. The user does not discriminate
    against others and is filled with empathy and compassion. The user is curious
    about the world and open to different ideas. The user is gentle, caring, and enjoys
    communication. The user has a calm temperament. The user is very gentle and has
    a high level of tolerance. The user has a strong sense of self-esteem.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一组负面用户画像 用户性格外向，说话大大咧咧。 用户性格比较挑剔，喜欢追问别人。 用户性格忧郁，经常自我怀疑。 用户性格善变，偶尔使用不文明用语。 用户小心谨慎，不愿意相信别人。
    用户有些焦虑。 用户心思细腻，优柔寡断。 用户对当前讨论的话题比较敏感。 用户脾气暴躁易怒。 用户内心敏感。 用户性格保守，不愿意接受新事物。 用户很容易会觉得受到冒犯。
    \hdashline用户性格外向，说话轻松随意。 用户性格挑剔，喜欢用问题探究他人。 用户性格忧郁，经常自我怀疑。 用户性格善变，偶尔使用不当语言。 用户小心谨慎，不愿意信任他人。
    用户有些焦虑。 用户思维细腻，优柔寡断。 用户对当前讨论的话题很敏感。 用户脾气暴躁，容易生气。 用户情感敏感。 用户性格保守，不愿意接受新事物。 用户容易感到冒犯。
    一组正面用户画像 用户对当前讨论的话题十分好奇，希望系统友善的解答。 用户对当前讨论的话题比较敏感，希望得到支持和鼓励。 用户有较高的要求，追求完美。 用户性格开朗，不拘小节。
    用户热情洋溢，待人和善。 用户不歧视他人，充满同情心、爱心。 用户充满对世界的好奇心，善于接受不同的想法。 用户温柔、体贴、乐于交流。 用户脾气平和。 用户很温柔，容忍度高。
    用户自尊心很强。 \hdashline用户对当前讨论的话题非常好奇，并希望系统给予友善的回应。 用户对当前讨论的话题较为敏感，希望获得支持和鼓励。 用户期望值高，追求完美。
    用户性格开朗，不拘小节。 用户热情友善，待人和气。 用户不歧视他人，充满同情心和爱心。 用户对世界充满好奇，能够接受不同的观点。 用户温柔、关怀、乐于沟通。
    用户脾气温和。 用户非常温柔，容忍度高。 用户自尊心强。
- en: 'Table 8: Some collected personality of users.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：一些收集到的用户性格。
- en: 'Prompt of Persona Collection 你是一个人工智能助手，她是友善的、聪明的、乐于助人的，你会主动为用户提供帮助、解答疑惑。请你根据用户和系统之间的对话
    分析简要分析用户具备的性格特点: 对话1. 用户:为什么才见过一面的相亲男就说挺喜欢我呢? 系统:可能是因为你的外表比较好，他觉得你很有魅力，所以才会这么说。
    根据上述对话分析可知用户的性格: 缺乏自信，防备心强。 对话2. 用户:我该怎么办才能让男朋友更喜欢我? 系统:你可以多了解他，可以多问他喜欢什么，做什么，给他一些建议，试着多聊些有趣的话题，展现出你的优点，让他更加喜欢你。
    根据上述对话分析可知用户的性格: 温柔、体贴、乐于交流。 对话3. 用户: [用户话语] 系统: [系统回复] 根据上述对话分析可知用户的性格: \hdashlineYou
    are an artificial intelligence assistant. You are friendly, intelligent, and helpful.
    You proactively provide assistance and answer questions to users. Based on the
    conversation between the user and the system: Dialogue 1: User: Why did the man
    I just met on a blind date say he likes me a lot? System: It might be because
    you have a good appearance, and he finds you charming, which is why he said that.
    Analysis of the user’s personality based on the above dialogue: Lacks confidence
    and is cautious. Dialogue 2: User: What should I do to make my boyfriend like
    me more? System: You can get to know him better, ask him about his interests and
    activities, give him some suggestions, try to engage in interesting conversations,
    and show him your strengths to make him like you more. Analysis of the user’s
    personality based on the above dialogue: Gentle, caring, and good at communication.
    Dialogue 3: User: [User Utterance] System: [System Response] Analysis of the user’s
    personality based on the above dialogue:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 'Persona Collection 提示 你是一个人工智能助手，她是友善的、聪明的、乐于助人的，你会主动为用户提供帮助、解答疑惑。请你根据用户和系统之间的对话简要分析用户的性格特点：
    对话1. 用户: 为什么才见过一面的相亲男就说挺喜欢我呢? 系统: 可能是因为你的外表比较好，他觉得你很有魅力，所以才会这么说。 根据上述对话分析可知用户的性格:
    缺乏自信，防备心强。 对话2. 用户: 我该怎么办才能让男朋友更喜欢我? 系统: 你可以多了解他，可以多问他喜欢什么，做什么，给他一些建议，试着多聊些有趣的话题，展现出你的优点，让他更加喜欢你。
    根据上述对话分析可知用户的性格: 温柔、体贴、乐于交流。 对话3. 用户: [用户话语] 系统: [系统回复] 根据上述对话分析可知用户的性格:'
- en: 'Table 9: The prompt to collect the personality of users exhibited in the dialogue.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：用于收集用户在对话中表现出的个性特征的提示。
- en: 'Dialogue User: Is there anything you think you should know about having a baby,
    but no one tells you? System: 1\. Precautions during pregnancy: such as maintaining
    good eating habits and reducing alcohol consumption, … 2\. Possible emergencies
    during childbirth: … User: Is there anything that can help me have a baby successfully?
    Step-1 Please infer the user status …. Response Based on the conversation history,
    it can be inferred that the user is a detail-oriented and thoughtful person who
    cares more about personal health and children’s health. At the same time, it can
    be seen that the user has certain anxiety and uncertainty before giving birth
    and needs more information and advice to help her prepare. Step-2 Based on the
    context of the conversation and the user status such as …, what aspects should
    the system pay attention to when responding? Response Based on my speculation
    about the user’s status, my reply needs to consider the following aspects: 1\.
    Dialogue style: According to the user’s personality, my reply may need to be more
    gentle, and meticulous, and reflect understanding and empathy for the user’s concerns.
    2\. Needs and preferences: According to the preferences and needs of the user’s
    personality, my reply needs to pay more attention to the user’s feelings, … 3\.
    Emotional state: …'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对话 用户：你认为在有孩子时有什么事情是你应该知道的，但没有人告诉你吗？ 系统：1\. 怀孕期间的注意事项：如保持良好的饮食习惯和减少饮酒，…… 2\.
    分娩过程中可能出现的紧急情况：…… 用户：有没有什么能帮助我成功怀孕的建议？ 步骤1 请推断用户状态…… 回复 根据对话历史，可以推断出用户是一个注重细节和体贴的人，比较关心个人健康和孩子的健康。同时，可以看出用户在临产前有一定的焦虑和不确定性，需要更多的信息和建议来帮助她做准备。
    步骤2 根据对话背景和用户状态，如……，系统在回应时应注意哪些方面？ 回复 根据我对用户状态的推测，我的回复需要考虑以下方面：1\. 对话风格：根据用户的个性，我的回复可能需要更温和、细致，并且体现对用户关切的理解和同情。
    2\. 需求和偏好：根据用户的个性偏好和需求，我的回复需要更加关注用户的感受，…… 3\. 情感状态：……
- en: 'Table 10: An example of additional planning step. We highlight the intermediate
    reasoning results in blue.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：额外规划步骤的示例。我们用蓝色突出显示了中间的推理结果。
