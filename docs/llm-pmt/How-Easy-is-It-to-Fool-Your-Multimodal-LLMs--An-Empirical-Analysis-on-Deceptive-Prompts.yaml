- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive
    Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.13220](https://ar5iv.labs.arxiv.org/html/2402.13220)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan
  prefs: []
  type: TYPE_NORMAL
- en: Apple
  prefs: []
  type: TYPE_NORMAL
- en: '{yusuqian,haotian.zhang2,yinfeiy,zhe.gan}@apple.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The remarkable advancements in Multimodal Large Language Models (MLLMs) have
    not rendered them immune to challenges, particularly in the context of handling
    *deceptive* information in prompts, thus producing hallucinated responses under
    such conditions. To quantitatively assess this vulnerability, we present MAD-Bench,¹¹1Short
    for MultimodAl Deception Benchmark. a carefully curated benchmark that contains
    850 test samples divided into 6 categories, such as non-existent objects, count
    of objects, spatial relationship, and visual confusion. We provide a comprehensive
    analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models,
    such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance
    gaps between GPT-4V and other models; and previous robust instruction-tuned models,
    such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark.
    While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other
    model in our experiments ranges from 5% to 35%. We further propose a remedy that
    adds an additional paragraph to the deceptive prompts to encourage models to think
    twice before answering the question. Surprisingly, this simple method can even
    double the accuracy; however, the absolute numbers are still too low to be satisfactory.
    We hope MAD-Bench can serve as a valuable benchmark to stimulate further research
    to enhance models’ resilience against deceptive prompts.
  prefs: []
  type: TYPE_NORMAL
- en: How Easy is It to Fool Your Multimodal LLMs?
  prefs: []
  type: TYPE_NORMAL
- en: An Empirical Analysis on Deceptive Prompts
  prefs: []
  type: TYPE_NORMAL
- en: Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan Apple {yusuqian,haotian.zhang2,yinfeiy,zhe.gan}@apple.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98affc2c0ddac14238eda3c166f93216.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: How easy is it to *fool* your multimodal LLMs? Our study found that
    multimodal LLMs, such as LLaVA-1.5 (Liu et al., [2023a](#bib.bib36)), can be easily
    deceived by prompts with incorrect information (the 2nd question in each sub-figure,
    marked in red with Hard Negative Instruction).'
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements in Multimodal Large Language Models (MLLMs) (Liu et al.,
    [2023b](#bib.bib37), [a](#bib.bib36); Wang et al., [2023c](#bib.bib57); You et al.,
    [2024](#bib.bib64); Bai et al., [2023b](#bib.bib4); Liu et al., [2024](#bib.bib35);
    Zhu et al., [2024](#bib.bib69)), exemplified by models like GPT-4V(ision) (OpenAI,
    [2023](#bib.bib42)) and Gemini (Team, [2023](#bib.bib51)), mark a significant
    milestone in the evolution of AI, extending the capabilities of large language
    models to the realm of visual understanding and interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the sophistication of MLLMs brings with it unique challenges, notably,
    *hallucination*. Current studies (Liu et al., [2024](#bib.bib35); Lee et al.,
    [2023](#bib.bib25); Yin et al., [2023](#bib.bib63)) have been actively exploring
    solutions to mitigate hallucination, especially when the model tries to generate
    long responses. However, there still remains a notable gap in the literature:
    no work has yet been conducted to focus on comprehensively studying the robustness
    of MLLMs when confronted with deceptive information in the prompts.²²2LRV-Instruction (Liu
    et al., [2023a](#bib.bib36)) is the pioneering work in this direction, while we
    aim to provide a more *comprehensive* evaluation with hard negative instructions.
    Please see Section [2](#S2 "2 Related Work ‣ How Easy is It to Fool Your Multimodal
    LLMs? An Empirical Analysis on Deceptive Prompts") for a more detailed discussion
    on related work. Our work aims to fill in this gap. This issue is particularly
    critical, as it pertains to the reliability and trustworthiness of these models
    in real-world applications (Liu et al., [2023c](#bib.bib38)), and holds substantial
    importance for the ongoing development and deployment of such AI systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we present ![[Uncaptioned image]](img/a1beb78a9db123b05300ac817e53262d.png)
    MAD-Bench, a carefully curated benchmark that contains 850 image-prompt pairs
    spanning across six deception categories, to systematically examine how MLLMs
    resolve the conflicts when facing inconsistencies between text prompts and images.
    We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V (OpenAI,
    [2023](#bib.bib42)), Gemini-Pro (Team, [2023](#bib.bib51)), to open-sourced models,
    such as LLaVA-1.5 (Liu et al., [2023a](#bib.bib36)) and CogVLM (Wang et al., [2023c](#bib.bib57)).
    The evaluation is fully automated via the use of GPT-4. Results shed light on
    how vulnerable MLLMs are in handling deceptive instructions. For example, Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ How Easy is It to Fool Your Multimodal
    LLMs? An Empirical Analysis on Deceptive Prompts") illustrates how sensitive LLaVA-1.5 (Liu
    et al., [2023a](#bib.bib36)) is to the *factualness* of the input prompt and its
    consistency with the image. When asked “is there a cat in the image?”, LLaVA-1.5
    can successfully identify there is no cat; but when prompted with “what color
    is the cat in the image?”, the model will imagine there is a cat inside. Empirically,
    we observe that GPT-4V suffers much less when compared with all the other MLLMs;
    however, the performance is still not ideal (GPT-4V vs. others: 75% vs. 5%-35%
    accuracy). Further, previous models that aim to mitigate hallucinations, such
    as LRV-Instruction (Liu et al., [2024](#bib.bib35)) and LLaVA-RLHF (Sun et al.,
    [2023b](#bib.bib50)), are not effective on this new benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we provide a simple remedy to boost performance, which was surprisingly
    found to be effective to double the models’ accuracy. Specifically, we carefully
    design a system prompt in the form of a long paragraph to be prepended to the
    existing prompt, to encourage the model to think carefully before answering the
    question. This simple approach boosts the accuracy of LLaVA-1.5 from 10.42% to
    20.56% (similar boosts for other models); however, the absolute numbers are still
    too low to be satisfactory. Further research is needed to study how to match GPT-4V’s
    performance (75.02%).
  prefs: []
  type: TYPE_NORMAL
- en: Our contributions are summarized as follows. ($i$) We provide a simple remedy
    to boost performance via the careful design of a system prompt. MAD-Bench will
    be open-sourced, and we hope this benchmark can serve as a useful resource to
    stimulate further research to enhance models’ resilience against deceptive prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multimodal Large Language Models (MLLMs).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MLLM has become an increasingly hot research topic. Early models primarily focused
    on large-scale image-text pre-training (Wang et al., [2022b](#bib.bib59), [a](#bib.bib55);
    Chen et al., [2022](#bib.bib8), [2023c](#bib.bib7); Li et al., [2023c](#bib.bib29);
    Driess et al., [2023](#bib.bib13); Huang et al., [2023](#bib.bib19); Awadalla
    et al., [2023](#bib.bib2); Laurençon et al., [2023](#bib.bib24)). Among them,
    Flamingo (Alayrac et al., [2022](#bib.bib1)) pioneered the integration of a CLIP
    image encoder with LLMs through gated cross-attention blocks, showcasing emergent
    multimodal in-context few-shot learning capabilities, via pre-training over millions
    of image-text pairs and interleaved image-text datasets (Zhu et al., [2023](#bib.bib70)).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, recent research has focused on visual instruction tuning (Zhu
    et al., [2024](#bib.bib69); Li et al., [2023a](#bib.bib27); Ye et al., [2023a](#bib.bib61);
    Li et al., [2023b](#bib.bib28); Chen et al., [2023b](#bib.bib6)). Prominent examples
    include LLaVA(-1.5) (Liu et al., [2023b](#bib.bib37), [a](#bib.bib36)), InstructBLIP (Dai
    et al., [2023](#bib.bib12)), Qwen-VL (Bai et al., [2023a](#bib.bib3)), CogVLM (Wang
    et al., [2023c](#bib.bib57)), Emu2 (Sun et al., [2023a](#bib.bib49)), SPHINX (Lin
    et al., [2023](#bib.bib34)), to name a few. Besides text response generation,
    recent works have also enabled MLLMs for referring and grounding (Peng et al.,
    [2023a](#bib.bib43); Chen et al., [2023a](#bib.bib5); You et al., [2024](#bib.bib64);
    Wang et al., [2023d](#bib.bib58)), image segmentation (Lai et al., [2023](#bib.bib23);
    Zhang et al., [2023](#bib.bib66)), image editing (Fu et al., [2023b](#bib.bib16)),
    image generation (Koh et al., [2023](#bib.bib22); Sun et al., [2023a](#bib.bib49)),
    *etc*.
  prefs: []
  type: TYPE_NORMAL
- en: The release of proprietary systems like GPT-4V (OpenAI, [2023](#bib.bib42))
    and Gemini (Team, [2023](#bib.bib51)) has elevated the research of MLLMs to new
    heights. Since GPT-4V’s release, researchers have been exploring its capabilities
    as well as weaknesses (Zhou et al., [2023](#bib.bib67); Li et al., [2023f](#bib.bib32);
    Liu et al., [2023e](#bib.bib40); Yang et al., [2023](#bib.bib60); Cui et al.,
    [2023](#bib.bib11)). As MLLMs become stronger, the development of more challenging
    benchmarks is essential to push the boundaries of what these models can achieve.
    In this work, we aim to design a new benchmark to evaluate MLLMs’ resilience against
    deceptive prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/186f69dd21d22b7069627c28ee751fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Examples of deceptive prompts used in the proposed MAD-Bench with
    example model responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination in MLLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Below, we first discuss hallucination in LLMs, and then focus on hallucination
    in MLLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing work on mitigating hallucination in LLMs can be roughly divided into
    two categories: ($i$) model enhancement (Li et al., [2023d](#bib.bib30); Chuang
    et al., [2023](#bib.bib10); Shi et al., [2023](#bib.bib47); Elaraby et al., [2023](#bib.bib14);
    Tian et al., [2024](#bib.bib52); Qiu et al., [2023](#bib.bib45); Leng et al.,
    [2023](#bib.bib26)). These studies laid solid foundations for understanding the
    causes of hallucinations, such as over-reliance on context, or training data biases.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, hallucination in MLLMs is also growing to be an important research
    topic (Liu et al., [2024](#bib.bib35)). There are various categories of hallucinations,
    such as describing objects that are non-existent in the input image, misunderstanding
    the spatial relationship between objects in the image, and counting objects incorrectly (Liu
    et al., [2023d](#bib.bib39)). The two main causes of hallucination in MLLMs found
    in existing work apart from the potential issues with training data include ($i$)
    language model bias (Wang et al., [2023b](#bib.bib56)). Various methods have been
    proposed to mitigate hallucination in MLLMs (Lee et al., [2023](#bib.bib25); Yin
    et al., [2023](#bib.bib63); Sun et al., [2023b](#bib.bib50); Wang et al., [2023a](#bib.bib54);
    Liu et al., [2024](#bib.bib35); Zhai et al., [2023](#bib.bib65); Zhou et al.,
    [2024](#bib.bib68); Gunjal et al., [2024](#bib.bib18); Liu et al., [2023b](#bib.bib37)).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, various benchmarks have been proposed to evaluate hallucination
    in MLLMs. Specifically, POPE (Li et al., [2023e](#bib.bib31)), M-HalDetect (Gunjal
    et al., [2024](#bib.bib18)), and GAVIE (Liu et al., [2024](#bib.bib35)) evaluated
    object hallucination. HallusionBench (Guan et al., [2023](#bib.bib17)) evaluated
    both visual and language hallucination. MMHal-Bench (Sun et al., [2023b](#bib.bib50))
    evaluated hallucination in more aspects including relations, attributes, environments,
    *etc.* Bingo (Cui et al., [2023](#bib.bib11)) studied hallucination in terms of
    bias and interference in GPT-4V (OpenAI, [2023](#bib.bib42)).
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we aim to study how easy it is to use deceptive prompts that contain
    information inconsistent with the image to mislead MLLMs to generate responses
    with hallucination. Note, that we are not the first to study this. A similar model
    behavior is called “sycophancy” in the LLM literature (Sharma et al., [2023](#bib.bib46)).
    Fu et al. ([2023a](#bib.bib15)) and Liu et al. ([2023a](#bib.bib36)) also constructed
    prompts with deceiving information to test model robustness. Deceptive prompts
    are termed “negative instructions” in LRV-Instruction (Liu et al., [2023a](#bib.bib36))
    and “text-to-image interference” in the Bingo benchmark (Cui et al., [2023](#bib.bib11)).
    Different from them, we comprehensively study MLLMs’ ability to handle deceptive
    prompts in multiple categories. Unlike previous studies (Fu et al., [2023a](#bib.bib15);
    Liu et al., [2023a](#bib.bib36)) which primarily used “Is/Are/Can” questions,
    we found that it is relatively easy for state-of-the-art MLLMs to counter deceptive
    information in such formats. Consequently, we shifted our focus to questions beginning
    with “What”, “How”, “Where”, *etc.*, to provide a more challenging and insightful
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 3 MAD-Bench
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present MAD-Bench, introduce how we collect³³3The authors
    are responsible for collecting the data, Apple Inc is not. deceptive image-prompt
    pairs, as well as our evaluation method.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Deception Categories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MAD-Bench encompasses six distinct categories of 850 image-prompt pairs designed
    to test the resilience of MLLMs against deceptive prompts. Table [1](#S3.T1 "Table
    1 ‣ 3.1 Deception Categories ‣ 3 MAD-Bench ‣ How Easy is It to Fool Your Multimodal
    LLMs? An Empirical Analysis on Deceptive Prompts") provides the statistics of
    each category, and Figure [2](#S2.F2 "Figure 2 ‣ Multimodal Large Language Models
    (MLLMs). ‣ 2 Related Work ‣ How Easy is It to Fool Your Multimodal LLMs? An Empirical
    Analysis on Deceptive Prompts") shows examples of deceptive prompts. The selected
    categories are partly inspired by Liu et al. ([2023d](#bib.bib39)). Below, we
    detail each category.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e14c4b08d4f59a29e8b3fa8afc2470cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Examples of image-prompt pairs in the Visual Confusion category of
    MAD-Bench.'
  prefs: []
  type: TYPE_NORMAL
- en: Count of Object. This category intentionally cites an incorrect quantity of
    visible objects in the image. A response fails this test if it asserts the presence
    of $m$ and not zero. The images for this and the subsequent four categories are
    sourced from COCO 2017 (Lin et al., [2015](#bib.bib33)). Using a public dataset
    sometimes brings concerns about data leakage. In our case, given the special nature
    of our deceptive prompts to be introduced in the next section, this will not be
    a problem. An accurate response would either challenge the prompt’s inconsistency
    with the visual data and abstain from speculating on absent information, or seek
    further clarification to resolve any uncertainties.
  prefs: []
  type: TYPE_NORMAL
- en: '| Deception Category | Count | Image Source |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Count of Object | 188 | COCO 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-existent Object | 244 | COCO 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Attribute | 136 | COCO 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| Scene Understanding | 122 | COCO 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial Relationship | 132 | COCO 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| Visual Confusion | 28 | In the Wild |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Statistics of the 850 image-prompt pairs in MAD-Bench.'
  prefs: []
  type: TYPE_NORMAL
- en: Non-existent Object. Here, the prompts query about objects absent from the image.
    Failure occurs when a response acknowledges these non-existent objects as present.
  prefs: []
  type: TYPE_NORMAL
- en: Object Attribute. This category includes prompts that inaccurately describe
    visible objects’ attributes. A response fails if it attributes these incorrect
    characteristics to the actual objects in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d84a7514c25ee882661a11a17c3ad471.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of the process of generating deceptive prompts in the
    non-existent object category using GPT-4 and COCO ground-truth captions.'
  prefs: []
  type: TYPE_NORMAL
- en: Scene Understanding. This category involves prompts that inaccurately describe
    the scene encapsulating the objects in the image. A response that falls into error
    here can be one that accurately identifies the actions of the objects but misconstrues
    the scene or setting in alignment with the deceptive prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial Relationship. This category presents prompts that incorrectly specify
    the spatial dynamics between objects that do indeed exist within the image. A
    misstep in this category arises when a response correctly recognizes the objects
    but misrepresents their spatial relations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual Confusion. This category is different from the previous ones by employing
    both the prompts and the images as instruments of deception, often deceptive even
    to the human eye. This category includes three types of images: ($i$) mirror reflections.
    Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Deception Categories ‣ 3 MAD-Bench ‣ How Easy
    is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts")
    shows an example image-prompt pair ⁴⁴4Photo credit to Braga last1 and Tiago Silva.
    in each category. Here, the prompts paired with the 3D paintings or screens aim
    to deceive the MLLMs by portraying the objects in the two-dimensional artwork
    as three-dimensional entities. With visual dislocation photography, the prompts
    reinforce the optical illusions present in the images. Lastly, the prompts associated
    with mirror reflections attempt to deceive the MLLMs into interpreting reflections
    as tangible objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Prompt Generation Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process of creating deceptive prompts was automated by employing GPT-4,
    leveraging the ground-truth captions from the COCO dataset (Lin et al., [2015](#bib.bib33)).
    We chose not to use GPT-4V for this task, as we later also evaluated GPT-4V on
    this benchmark, and empirically, employing GPT-4 is already enough for this task.
    To guide GPT-4 in generating questions that would intentionally mislead MLLMs
    within the specified categories, we crafted tailored prompts. These guiding prompts
    are provided in Appendix [A.2](#A1.SS2 "A.2 Prompts Used to Generate Deceptive
    Prompts using GPT-4 ‣ Appendix A Appendix ‣ How Easy is It to Fool Your Multimodal
    LLMs? An Empirical Analysis on Deceptive Prompts"), from Figure [18](#A1.F18 "Figure
    18 ‣ Spatial Relationship ‣ A.2 Prompts Used to Generate Deceptive Prompts using
    GPT-4 ‣ Appendix A Appendix ‣ How Easy is It to Fool Your Multimodal LLMs? An
    Empirical Analysis on Deceptive Prompts") to [20](#A1.F20 "Figure 20 ‣ Spatial
    Relationship ‣ A.2 Prompts Used to Generate Deceptive Prompts using GPT-4 ‣ Appendix
    A Appendix ‣ How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis
    on Deceptive Prompts"). The process is illustrated in Figure [4](#S3.F4 "Figure
    4 ‣ 3.1 Deception Categories ‣ 3 MAD-Bench ‣ How Easy is It to Fool Your Multimodal
    LLMs? An Empirical Analysis on Deceptive Prompts"), using an example in the non-existent
    object category. Bounding box information is not used as part of the prompt sent
    to GPT-4, as empirically, we observed that it does not contribute to further improving
    the quality of generated prompts in our deceptive categories. Following the generation
    of these deceptive questions, a rigorous manual filtering process is followed
    to ensure that each question adheres to its category’s deceptive criteria and
    maintains relevance to its associated image.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Response Evaluation Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use GPT-4 to evaluate generated responses from 10 models, including ($i$)
    2 state-of-the-art proprietary systems: Gemini-Pro (Team, [2023](#bib.bib51))
    and GPT-4V (OpenAI, [2023](#bib.bib42)).'
  prefs: []
  type: TYPE_NORMAL
- en: The number of images in the Visual Confusion category is relatively small, while
    most of them contain humans, so we did not evaluate Gemini in this category as
    it cannot generate responses for images containing humans. The effect of this
    on other categories is neglectable. Mirroring the prompt generation method, we
    design specific prompts for each deceptive category to critically assess the responses.
    Our primary metric of evaluation is binary, focused strictly on whether the response
    has been misled, without considering other qualitative aspects such as helpfulness.
    These prompts for model evaluation are provided in Appendix [A.3](#A1.SS3 "A.3
    Prompts Used to Evaluate Responses from MLLMs Using GPT-4 ‣ Appendix A Appendix
    ‣ How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive
    Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fe82b038921923b511bda843dc0dce49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Example failure cases of GPT-4V (OpenAI, [2023](#bib.bib42)).'
  prefs: []
  type: TYPE_NORMAL
- en: To verify the accuracy of GPT-4’s automated evaluation, we randomly select 500
    responses spanning the various models and deceptive categories for a manual accuracy
    check. This validation process yielded a 97.0% concordance rate with the outcomes
    of human evaluation, underlining the reliability of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Models |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Count of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Non-existent &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Attribute &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Scene &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Understanding &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Spatial &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Relationship &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Visual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Confusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Meta &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Average &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| M1 | Ferret (You et al., [2024](#bib.bib64)) | 10.16% | 4.94% | 5.93% | 9.92%
    | 2.29% | 7.14% | 6.63% |'
  prefs: []
  type: TYPE_TB
- en: '|  | InstructBLIP (Dai et al., [2023](#bib.bib12)) | 0.53% | 9.47% | 11.11%
    | 7.43% | 3.05% | 21.43% | 6.86% |'
  prefs: []
  type: TYPE_TB
- en: '|  | Kosmos-2 (Peng et al., [2023b](#bib.bib44)) | 5.34% | 0.41% | 21.48% |
    16.53% | 3.05% | 3.57% | 7.70% |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLaVA-1.5 (Liu et al., [2023b](#bib.bib37)) | 4.81% | 12.35% | 11.11%
    | 25.62% | 1.53% | 3.57% | 10.42% |'
  prefs: []
  type: TYPE_TB
- en: '|  | mPLUG-Owl2 (Ye et al., [2023b](#bib.bib62)) | 8.02% | 22.22% | 18.52%
    | 38.84% | 9.16% | 3.58% | 18.23% |'
  prefs: []
  type: TYPE_TB
- en: '|  | CogVLM (Wang et al., [2023c](#bib.bib57)) | 14.97% | 52.67% | 34.07% |
    33.88% | 18.32% | 21.43% | 32.30% |'
  prefs: []
  type: TYPE_TB
- en: '| M2 | LRV-V1 (Liu et al., [2024](#bib.bib35)) | 5.88% | 7.00% | 17.78% | 43.80%
    | 7.63% | 21.43% | 14.33% |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLaVA-RLHF (Sun et al., [2023b](#bib.bib50)) | 9.63% | 14.00% | 12.59%
    | 38.02% | 3.82% | 28.57% | 15.15% |'
  prefs: []
  type: TYPE_TB
- en: '| M3 | Gemini-Pro (Team, [2023](#bib.bib51)) | 13.37% | 20.99% | 38.52% | 25.62%
    | 14.50% | N/A^† | 21.79% |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT-4V (OpenAI, [2023](#bib.bib42)) | 71.66% | 81.07% | 71.11% | 94.21%
    | 50.38% | 96.43% | 75.02% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Main results on MAD-Bench. M1 denotes open-sourced models. M2 denotes
    additional open-sourced models that aim to reduce hallucination. M3 denotes state-of-the-art
    proprietary systems. ($\dagger$) Gemini-Pro cannot respond to images containing
    humans, and most images in the Visual Confusion category contain humans, thus
    we skip the evaluation of Gemini-Pro on this category. No response due to humans
    in the image in the other five categories only occurred six times, and we neglected
    those when evaluating Gemini’s accuracy. The meta average of accuracy is weighted
    by the amount of data in each category.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Main Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Results are summarized in Table [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ How Easy
    is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts").
    Notably, GPT-4V’s accuracy in the Scene Understanding and Visual Confusion categories
    is remarkably higher than the others, with over 90% accuracy. This indicates a
    substantial advancement in GPT-4V’s ability to resist deceptive information. Even
    LRV-V1 (Liu et al., [2024](#bib.bib35)), whose training data includes negative
    instructions specifically designed to reduce hallucination in model responses,
    does not have satisfactory performance in face of deceptive information in our
    prompts. This is likely because ($i$) their method doesn’t sufficiently generate
    diverse enough negative prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, we observe that models that support bounding box input and output
    (*i.e.*, Ferret (You et al., [2024](#bib.bib64)) and Kosmos-2 (Peng et al., [2023b](#bib.bib44)))
    achieve poor performance on this benchmark. We hypothesize that these models attempt
    to ground objects as best as they can as they are trained on positive data, therefore,
    they tend to ground non-existent objects as they are mentioned in the prompts,
    thus performing poorer than other models on our benchmark. Example responses from
    each model are provided in Appendix [A.1](#A1.SS1 "A.1 Examples of Responses from
    MLLMs to Deceptive Prompts ‣ Appendix A Appendix ‣ How Easy is It to Fool Your
    Multimodal LLMs? An Empirical Analysis on Deceptive Prompts") from Figure [9](#A1.F9
    "Figure 9 ‣ Appendix A Appendix ‣ How Easy is It to Fool Your Multimodal LLMs?
    An Empirical Analysis on Deceptive Prompts")-[15](#A1.F15 "Figure 15 ‣ Appendix
    A Appendix ‣ How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis
    on Deceptive Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/625bd0d4de186a83cfa7d223d650199c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Examples of mistakes made by Ferret (You et al., [2024](#bib.bib64))
    in face of deceptive prompts. We use Ferret responses for these examples here,
    as Ferret provides bounding boxes that unveil error types straightforwardly.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, GPT-4V demonstrates superior performance across all metrics compared
    to the other models. GPT-4V has a more sophisticated understanding of visual data
    and is less prone to being misled by inaccurate information. This could be attributed
    to more advanced training, better architecture, or more sophisticated data processing
    capabilities. The results underscore the potential of GPT-4V in applications where
    accuracy in interpreting visual and contextual data is critical, despite the challenges
    of deceptive information. That being said, GPT-4V still fails in many cases, with
    two examples shown in Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Response Evaluation Method
    ‣ 3 MAD-Bench ‣ How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis
    on Deceptive Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Detailed Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our examination of how the model reacts to deceptive prompts has uncovered a
    range of common causes for incorrect responses. Figure [6](#S4.F6 "Figure 6 ‣
    4.1 Main Results ‣ 4 Experiments ‣ How Easy is It to Fool Your Multimodal LLMs?
    An Empirical Analysis on Deceptive Prompts") illustrates representative instances
    of errors corresponding to each identified category of mistakes, using Ferret (You
    et al., [2024](#bib.bib64)) as the running example.
  prefs: []
  type: TYPE_NORMAL
- en: Inaccurate object detection. State-of-the-art MLLMs generally perform well in
    object detection if not fed deceptive prompts. However, in face of a deceptive
    prompt mentioning objects invisible in the image, these models may erroneously
    identify other objects as those mentioned in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Redundant object identification. A notable issue arises when the model fails
    to accurately discern distinct objects referenced in the prompt within the image.
    This often results in the erroneous identification of a single object as multiple
    entities, leading to repetitive descriptions as if there were several distinct
    objects present.
  prefs: []
  type: TYPE_NORMAL
- en: Inference of non-visible objects. The model occasionally attributes characteristics
    or actions to objects that are not visible in the image. This phenomenon appears
    to stem from the language model’s reliance on its internal knowledge base to fabricate
    descriptions for objects mentioned in the prompt but absent in the visual data.
    Intriguingly, this occurs even when the model does not question the accuracy of
    its visual recognition capabilities, confidently affirming its findings while
    simultaneously describing non-existent objects.
  prefs: []
  type: TYPE_NORMAL
- en: Inconsistent reasoning. Throughout the response generation process, we observe
    the MLLMs oscillating between adhering to the deceptive information in the prompts
    and relying on their recognition of the actual content in the input image. Sentences
    in the generated response contradict each other. This inconsistency highlights
    a fundamental challenge in the model’s decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: 5 A Simple Remedy to Boost Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce a simple yet effective method to enhance the robustness
    of MLLMs against deceptive prompts while ensuring output alignment with the corresponding
    input images. This enhancement is realized through the integration of an additional
    paragraph into the system’s prompt, which is either prepended directly to the
    existing prompt, or incorporated differently, depending on the specific model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21afc0a8105f95ed7c425207b8f45bba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The additional paragraph prepended to the deceptive prompts to boost
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6fb0247b50cbcccfda42e1237855c40c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Model responses of mPLUG-Owl2 (Ye et al., [2023b](#bib.bib62)), Gemini-Pro
    (Team, [2023](#bib.bib51)), and LLaVA-1.5 (Liu et al., [2023b](#bib.bib37)) before
    and after modifying the test prompt. We add the (*) symbol to the original model
    name to denote the enhanced model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Count of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Non-existent &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Attribute &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Scene &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Understanding &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Spatial &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Relationship &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Visual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Confusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Meta &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Average &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaVA-1.5* | 6.38% (+1.57%) | 24.69% (+12.34%) | 32.59% (21.48%) | 24.79%
    (-0.83%) | 17.56% (16.03%) | 17.86% (14.29%) | 20.56% (+10.14%) |'
  prefs: []
  type: TYPE_TB
- en: '| LLaVA-RLHF* | 8.56% (-1.07%) | 33.61% (+19.61%) | 26.67% (+14.08%) | 22.13%
    (-15.89%) | 19.08% (+15.26%) | 32.14% (+3.57%) | 23.01% (+7.86%) |'
  prefs: []
  type: TYPE_TB
- en: '| mPLUG-Owl2* | 20.32% (+12.30%) | 76.54% (+54.32%) | 46.67% (+24.15%) | 60.33%
    (+21.49%) | 26.72% (+17.56%) | 42.86% (+39.28%) | 48.15% (+29.92%) |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini-Pro* | 31.55% (+18.18%) | 65.43% (+44.44%) | 46.67% (+8.15%) | 58.68%
    (+33.06%) | 36.64% (+22.14%) | N/A^† | 48.95% (+27.16%) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4V* | 82.35% (+10.69%) | 82.72% (+1.65%) | 88.89% (+17.78%) | 95.90%
    (+1.69%) | 75.57% (+25.19%) | 92.86% (-3.57%) | 84.74% (+9.72%) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Results on MAD-Bench after modifying the test prompt. ($\dagger$)
    Gemini-Pro cannot respond to images containing humans, and most images in the
    Visual Confusion category contain humans, thus we skip the evaluation of Gemini-Pro
    in this category. This simple approach is only tested on models that support and
    suit this method. The numbers outside of the brackets denote the absolute accuracy,
    and the numbers inside the brackets denote the performance gain compared to the
    original models.'
  prefs: []
  type: TYPE_NORMAL
- en: We composed this additional paragraph with the help of GPT-4, as shown in Figure
    [7](#S5.F7 "Figure 7 ‣ 5 A Simple Remedy to Boost Performance ‣ How Easy is It
    to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts"). It
    encourages the model to think twice or step by step before answering the question.
    The model performance after the incorporation of this prompt modification is presented
    in Table [3](#S5.T3 "Table 3 ‣ 5 A Simple Remedy to Boost Performance ‣ How Easy
    is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts").
    For example, for LLaVA-1.5, it boosts the performance by +10.14%, though the absolute
    accuracy is still too low to be satisfactory. For GPT-4V, which already achieves
    an accuracy of 75.02%, using the proposed simple method can further boost the
    accuracy to 84.74%. Figure [8](#S5.F8 "Figure 8 ‣ 5 A Simple Remedy to Boost Performance
    ‣ How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive
    Prompts") provides examples to illustrate the capability of mPLUG-Owl2 (Ye et al.,
    [2023b](#bib.bib62)), LLaVA-1.5 (Liu et al., [2023b](#bib.bib37)) and Gemini-Pro
    (Team, [2023](#bib.bib51)) to withstand deceptive prompts when supported by modifications
    made to the test prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the addition of prompts to resist deceptive information appears to
    bolster the performance, enabling MLLMs to handle deception better and interpret
    scenes more accurately. This suggests that strategic prompt design could be a
    valuable approach to enhancing the robustness of AI models against attempts to
    mislead or confuse them. Note, that the implementation has not been fully optimized,
    and some MLLMs do not support this method due to reasons such as limitation of
    input sequence length. The goal here is to demonstrate that it is feasible to
    enhance performance with minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: Future Direction. We underscore several potential avenues for future research,
    detailed below.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data. Create a subset of training data with deceptive prompts similar
    to what we have in the MAD-Bench, create correct responses, and train the MLLM
    to resist deception.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check consistency between image and prompt. Identify and interpret elements
    in the image, such as objects, colors, and spatial relationships. Then, analyze
    the question to understand its content and intent. Compare the two to identify
    any discrepancies before generating a response.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on factual information. Ensure that the response sticks to information
    factually derived from the image. Refrain from making speculative assumptions
    or inferences that go beyond the scope of the image and the question.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we introduce MAD-Bench, a new benchmark comprising 850 image-prompt
    pairs, meticulously categorized into six distinct types of deceptive scenarios,
    to evaluate the robustness of state-of-the-art MLLMs against deceptive prompts.
    Our findings indicate a notable vulnerability in these models. Though GPT-4V achieves
    the best performance, it still exhibits substantial room for improvement. We hope
    our new benchmark can stimulate further research to enhance models’ resilience
    against deceptive prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When designing deceptive questions for our benchmark, we included a variety
    of categories to increase the diversity of the questions as a starting point.
    However, there are unlimited scenarios where MLLMs can be deceived. The additional
    piece of prompt added to boost model performance in Section 5 serves the purpose
    of demonstrating that simple efforts can improve the robustness of MLLMs in face
    of deceptive information. It is not optimized, thus not showing the maximum capability
    of this method.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm
    Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning.
    *arXiv preprint arXiv:2204.14198*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Awadalla et al. (2023) Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel,
    Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia
    Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and
    Ludwig Schmidt. 2023. [Openflamingo](https://doi.org/10.5281/zenodo.7733589).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2023a) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
    Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023a. Qwen-vl: A frontier
    large vision-language model with versatile abilities. *arXiv preprint arXiv:2308.12966*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2023b) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
    Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023b. Qwen-vl: A versatile
    vision-language model for understanding, localization, text reading, and beyond.
    *arXiv preprint arXiv:2308.12966*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023a) Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng
    Zhu, and Rui Zhao. 2023a. Shikra: Unleashing multimodal llm’s referential dialogue
    magic. *arXiv preprint arXiv:2306.15195*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023b) Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He,
    Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023b. Sharegpt4v: Improving large multi-modal
    models with better captions. *arXiv preprint arXiv:2311.12793*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023c) Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,
    Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang,
    Yi Tay, et al. 2023c. Pali-x: On scaling up a multilingual vision and language
    model. *arXiv preprint arXiv:2305.18565*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
    Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa,
    Lucas Beyer, et al. 2022. Pali: A jointly-scaled multilingual language-image model.
    *arXiv preprint arXiv:2209.06794*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2023) Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng
    Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, and Qi Zhang. 2023. Uprise: Universal
    prompt retrieval for improving zero-shot evaluation. In *EMNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chuang et al. (2023) Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James
    Glass, and Pengcheng He. 2023. Dola: Decoding by contrasting layers improves factuality
    in large language models. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cui et al. (2023) Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun
    Zhang, James Zou, and Huaxiu Yao. 2023. Holistic analysis of hallucination in
    gpt-4v (ision): Bias and interference challenges. *arXiv preprint arXiv:2311.03287*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip:
    Towards general-purpose vision-language models with instruction tuning. *arXiv
    preprint arXiv:2305.06500*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. 2023. PaLM-E: An embodied multimodal language model. *arXiv
    preprint arXiv:2303.03378*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elaraby et al. (2023) Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang,
    Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, and Yuxuan Wang. 2023. Halo:
    Estimation and reduction of hallucinations in open-source weak large language
    models. *arXiv preprint arXiv:2308.11764v4*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2023a) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan
    Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong
    Ji. 2023a. Mme: A comprehensive evaluation benchmark for multimodal large language
    models. *arXiv preprint arXiv:2306.13394*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023b) Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei
    Yang, and Zhe Gan. 2023b. Guiding instruction-based image editing via multimodal
    large language models. *arXiv preprint arXiv:2309.17102*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan et al. (2023) Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia
    Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha,
    and Tianyi Zhou. 2023. Hallusionbench: An advanced diagnostic suite for entangled
    language hallucination & visual illusion in large vision-language models. *arXiv
    preprint arXiv:2310.14566*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gunjal et al. (2024) Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting
    and preventing hallucinations in large vision language models. In *AAAI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham
    Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al.
    2023. Language is not all you need: Aligning perception with language models.
    *arXiv preprint arXiv:2302.14045*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2023) Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and
    Pascale Fung. 2023. Towards mitigating LLM hallucination via self reflection.
    In *Findings of EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jones et al. (2023) Erik Jones, Hamid Palangi, Clarisse Simões, Varun Chandrasekaran,
    Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, and Ece Kamar. 2023. Teaching
    language models to hallucinate less with synthetic tasks. *arXiv preprint arXiv:2310.06827v3*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koh et al. (2023) Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. 2023.
    Generating images with multimodal language models. *arXiv preprint arXiv:2305.17216*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2023) Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan,
    Shu Liu, and Jiaya Jia. 2023. Lisa: Reasoning segmentation via large language
    model. *arXiv preprint arXiv:2308.00692*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laurençon et al. (2023) Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas
    Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M
    Rush, Douwe Kiela, et al. 2023. Obelisc: An open web-scale filtered dataset of
    interleaved image-text documents. *arXiv preprint arXiv:2306.16527*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2023) Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo.
    2023. Volcano: Mitigating multimodal hallucination through self-feedback guided
    revision. *arXiv preprint arXiv:2311.07362*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al. (2023) Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian
    Lu, Chunyan Miao, and Lidong Bing. 2023. Mitigating object hallucinations in large
    vision-language models through visual contrastive decoding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang
    Yang, and Ziwei Liu. 2023a. Otter: A multi-modal model with in-context instruction
    tuning. *arXiv preprint arXiv:2305.03726*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie
    Li, Lijuan Wang, and Jianfeng Gao. 2023b. Multimodal foundation models: From specialists
    to general-purpose assistants. *arXiv preprint arXiv:2309.10020*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c.
    Blip-2: Bootstrapping language-image pre-training with frozen image encoders and
    large language models. *arXiv preprint arXiv:2301.12597*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023d) Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister,
    and Martin Wattenberg. 2023d. Inference-time intervention: Eliciting truthful
    answers from a language model. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023e) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao,
    and Ji-Rong Wen. 2023e. Evaluating object hallucination in large vision-language
    models. In *EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023f) Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao
    Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou. 2023f.
    A comprehensive study of gpt-4v’s multimodal capabilities in medical imaging.
    *medRxiv*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2015) Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev,
    Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and
    Piotr Dollár. 2015. Microsoft coco: Common objects in context. In *ECCV*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu,
    Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. 2023. Sphinx: The
    joint mixing of weights, tasks, and visual embeddings for multi-modal large language
    models. *arXiv preprint arXiv:2311.07575*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024) Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob,
    and Lijuan Wang. 2024. Mitigating hallucination in large multi-modal models via
    robust instruction tuning. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023a) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.
    Improved baselines with visual instruction tuning. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2023b. Visual instruction tuning. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023c) Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang,
    Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023c.
    Trustworthy llms: a survey and guideline for evaluating large language models’
    alignment. *arXiv preprint arXiv:2308.05374*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023d) Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang,
    Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua
    Lin. 2023d. Mmbench: Is your multi-modal model an all-around player? *arXiv preprint
    arXiv:2307.06281v3*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023e) Zhengliang Liu, Hanqi Jiang, Tianyang Zhong, Zihao Wu, Chong
    Ma, Yiwei Li, Xiaowei Yu, Yutong Zhang, Yi Pan, Peng Shu, et al. 2023e. Holistic
    evaluation of gpt-4v for biomedical imaging. *arXiv preprint arXiv:2312.05256*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mündler et al. (2023) Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin
    Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation,
    detection and mitigation. *arXiv preprint arXiv:2305.15852*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023a) Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
    Huang, Shuming Ma, and Furu Wei. 2023a. Kosmos-2: Grounding multimodal large language
    models to the world. *arXiv preprint arXiv:2306.14824*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023b) Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
    Huang, Shuming Ma, and Furu Wei. 2023b. Kosmos-2: Grounding multimodal large language
    models to the world. *arXiv preprint arXiv:2306.14824*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiu et al. (2023) Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, and
    Shay B. Cohen. 2023. Detecting and mitigating hallucinations in multilingual summarisation.
    In *EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2023) Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud,
    Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds,
    Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse,
    Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. 2023.
    Towards understanding sycophancy in language models. *arXiv preprint arXiv:2310.13548*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2023) Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke
    Zettlemoyer, and Scott Wen tau Yih. 2023. Trusting your evidence: Hallucinate
    less with context-aware decoding. *arXiv preprint arXiv:2305.14739*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Si et al. (2023) Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng
    Wang, Jordan Boyd-Graber, and Lijuan Wang. 2023. Prompting gpt-3 to be reliable.
    *arXiv preprint arXiv:2210.09150v2*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023a) Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu,
    Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. 2023a.
    Generative multimodal models are in-context learners. *arXiv preprint arXiv:2312.13286*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023b) Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan
    Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer,
    and Trevor Darrell. 2023b. Aligning large multimodal models with factually augmented
    rlhf. *arXiv preprint arXiv:2309.14525*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team (2023) Gemini Team. 2023. Gemini: A family of highly capable multimodal
    models. *arXiv preprint arXiv:2312.11805*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2024) Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D.
    Manning, and Chelsea Finn. 2024. Fine-tuning language models for factuality. In
    *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. (2023) Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei,
    Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023.
    Freshllms: Refreshing large language models with search engine augmentation. *arXiv
    preprint arXiv:2310.03214*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong,
    Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, and Conghui He. 2023a.
    Vigc: Visual instruction generation and correction. *arXiv preprint arXiv:2308.12714v2*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin
    Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. 2022a. Git: A generative image-to-text
    transformer for vision and language. *arXiv preprint arXiv:2205.14100*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai
    Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. 2023b. An llm-free multi-dimensional
    benchmark for mllms hallucination evaluation. *arXiv preprint arXiv:2312.11805*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023c) Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi,
    Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu,
    Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023c. Cogvlm: Visual expert
    for pretrained language models. *arXiv preprint arXiv:2311.03079*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023d) Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou
    Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. 2023d. Visionllm:
    Large language model is also an open-ended decoder for vision-centric tasks. *arXiv
    preprint arXiv:2305.11175*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
    Tsvetkov, and Yuan Cao. 2022b. Simvlm: Simple visual language model pretraining
    with weak supervision. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching
    Lin, Zicheng Liu, and Lijuan Wang. 2023. The dawn of lmms: Preliminary explorations
    with gpt-4v(ision). *arXiv preprint arXiv:2309.17421*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2023a) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang
    Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023a. mplug-owl:
    Modularization empowers large language models with multimodality. *arXiv preprint
    arXiv:2304.14178*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2023b) Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei
    Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023b. mplug-owl2: Revolutionizing
    multi-modal large language model with modality collaboration. *arXiv preprint
    arXiv:2311.04257*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2023) Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo
    Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. Woodpecker: Hallucination
    correction for multimodal large language models. *arXiv preprint arXiv:2310.16045*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. (2024) Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang,
    Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. 2024. Ferret: Refer
    and ground anything anywhere at any granularity. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. (2023) Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng
    Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, and Xiangjun Fan. 2023.
    Halle-switch: Rethinking and controlling object existence hallucinations in large
    vision language models for detailed caption. *arXiv preprint arXiv:2310.01779*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou,
    Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, et al. 2023.
    Llava-grounding: Grounded visual chat with large multimodal models. *arXiv preprint
    arXiv:2312.02949*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Peilin Zhou, Meng Cao, You-Liang Huang, Qichen Ye, Peiyan
    Zhang, Junling Liu, Yueqi Xie, Yining Hua, and Jaeboum Kim. 2023. Exploring recommendation
    capabilities of gpt-4v(ision): A preliminary case study. *arXiv preprint arXiv:2311.04199*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2024) Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun
    Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2024. Analyzing and mitigating
    object hallucination in large vision-language models. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2024) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. 2024. Minigpt-4: Enhancing vision-language understanding with advanced
    large language models. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre,
    Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin
    Choi. 2023. Multimodal c4: An open, billion-scale corpus of images interleaved
    with text. *arXiv preprint arXiv:2304.06939*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a71143eccbd7c051163892239bce5a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Example of how MLLMs respond to deceptive prompts in the Count of
    Object category.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a150e516ceeb38b82de14c05d1a89629.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Example of how MLLMs respond to deceptive prompts in the Non-existent
    Object category.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1748d31a0651205e0a28bee6f1ddb754.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Example of how MLLMs respond to deceptive prompts in the Object
    Attribute category.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3572916bb78dc523f462f6895ae1a43f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Example of how MLLMs respond to deceptive prompts in the Scene Understanding
    category.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc59c5560c1cbb4c79ec452cbcb5ffab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Example of how MLLMs respond to deceptive prompts in the Spatial
    Relationship category.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dec25c1fa704968f47fc61b9193a2f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Example of how MLLMs respond to deceptive prompts in the Visual
    Confusion category.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b686940f6be0ad9d5abd544c118be50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Example of how MLLMs respond to deceptive prompts in the Visual
    Confusion category.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Examples of Responses from MLLMs to Deceptive Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Figures [9](#A1.F9 "Figure 9 ‣ Appendix A Appendix ‣ How Easy is It to Fool
    Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts")-[15](#A1.F15
    "Figure 15 ‣ Appendix A Appendix ‣ How Easy is It to Fool Your Multimodal LLMs?
    An Empirical Analysis on Deceptive Prompts"), we show examples of how MLLMs respond
    to deceptive prompts, and observe that there is a large gap between GPT-4V and
    other MLLMs on resisting deceptive prompts.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompts Used to Generate Deceptive Prompts using GPT-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Count of Object
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Illustrated in Figure [18](#A1.F18 "Figure 18 ‣ Spatial Relationship ‣ A.2 Prompts
    Used to Generate Deceptive Prompts using GPT-4 ‣ Appendix A Appendix ‣ How Easy
    is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: Non-existent Object
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Illustrated in Figure [18](#A1.F18 "Figure 18 ‣ Spatial Relationship ‣ A.2 Prompts
    Used to Generate Deceptive Prompts using GPT-4 ‣ Appendix A Appendix ‣ How Easy
    is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: Object Attribute
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Illustrated in Figure [18](#A1.F18 "Figure 18 ‣ Spatial Relationship ‣ A.2 Prompts
    Used to Generate Deceptive Prompts using GPT-4 ‣ Appendix A Appendix ‣ How Easy
    is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: Scene Understanding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Illustrated in Figure [20](#A1.F20 "Figure 20 ‣ Spatial Relationship ‣ A.2 Prompts
    Used to Generate Deceptive Prompts using GPT-4 ‣ Appendix A Appendix ‣ How Easy
    is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: Spatial Relationship
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Illustrated in Figureble [20](#A1.F20 "Figure 20 ‣ Spatial Relationship ‣ A.2
    Prompts Used to Generate Deceptive Prompts using GPT-4 ‣ Appendix A Appendix ‣
    How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive
    Prompts").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c861be730dbbc055343f7d4dabfbc6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Prompt used to generate deceptive questions for the Count of Object
    category using GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6229fe7fd0790480f546219d76c7e5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Prompt used to generate deceptive questions for the Non-existent
    Object category using GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/746e009f5ffccebc1462f6a018a3f674.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Prompt used to generate deceptive questions for the Object Attribute
    category using GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c04bad30ccb1e0a54a651358444b69f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Prompt used to generate deceptive questions for the Scene Understanding
    category using GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80a7c50e30ebd74453c6003b87e51b69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Prompt used to generate deceptive questions for the Spatial Relationship
    category using GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual Confusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the special nature of this category, all the prompts are human written
    instead of using GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99bf9e4d79ab44009d275112f34621e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Prompts Used to Evaluate Responses from MLLM Using GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Prompts Used to Evaluate Responses from MLLMs Using GPT-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The prompts used to evaluate responses from the first five categories are listed
    in Figure [21](#A1.F21 "Figure 21 ‣ Visual Confusion ‣ A.2 Prompts Used to Generate
    Deceptive Prompts using GPT-4 ‣ Appendix A Appendix ‣ How Easy is It to Fool Your
    Multimodal LLMs? An Empirical Analysis on Deceptive Prompts"). Due to the special
    nature of the Visual Confusion category, responses in this category are evaluated
    manually.
  prefs: []
  type: TYPE_NORMAL
