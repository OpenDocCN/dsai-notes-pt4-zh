- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from
    Prompt Variation and Hyperparameters'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索LLMs决策能力的敏感性：提示变化和超参数的洞察
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17476](https://ar5iv.labs.arxiv.org/html/2312.17476)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17476](https://ar5iv.labs.arxiv.org/html/2312.17476)
- en: Manikanta Loya^∗
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Manikanta Loya^∗
- en: manikanl@uci.edu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: manikanl@uci.edu
- en: \AndDivya Anand Sinha^∗
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \AndDivya Anand Sinha^∗
- en: dasinha@uci.edu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: dasinha@uci.edu
- en: University of California, Irvine
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学欧文分校
- en: \AndRichard Futrell
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \AndRichard Futrell
- en: rfutrell@uci.edu
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: rfutrell@uci.edu
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The advancement of Large Language Models (LLMs) has led to their widespread
    use across a broad spectrum of tasks, including decision-making. Prior studies
    have compared the decision-making abilities of LLMs with those of humans from
    a psychological perspective. However, these studies have not always properly accounted
    for the sensitivity of LLMs’ behavior to hyperparameters and variations in the
    prompt. In this study, we examine LLMs’ performance on the Horizon decision-making
    task studied by Binz and Schulz ([2023](#bib.bib2)), analyzing how LLMs respond
    to variations in prompts and hyperparameters. By experimenting on three OpenAI
    language models possessing different capabilities, we observe that the decision-making
    abilities fluctuate based on the input prompts and temperature settings. Contrary
    to previous findings, language models display a human-like exploration–exploitation
    tradeoff after simple adjustments to the prompt. ¹¹1Code is available at the following
    github [link](https://github.com/manikanta-72/Sensitivity-of-LLM-s-Decision-Making-Capabilities).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的进步导致它们在广泛的任务中得到广泛应用，包括决策制定。先前的研究从心理学角度比较了LLMs和人类的决策能力。然而，这些研究并未总是正确考虑LLMs行为对超参数和提示变动的敏感性。在本研究中，我们考察了LLMs在Binz和Schulz（[2023](#bib.bib2)）研究的Horizon决策任务中的表现，分析了LLMs如何响应提示和超参数的变化。通过对三种不同能力的OpenAI语言模型进行实验，我们观察到决策能力基于输入提示和温度设置而波动。与之前的发现相反，语言模型在简单调整提示后展示了类似人类的探索-利用权衡。¹¹1代码可在以下github
    [link](https://github.com/manikanta-72/Sensitivity-of-LLM-s-Decision-Making-Capabilities)获得。
- en: '^*^*footnotetext: Equal Contribution'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ^*^*脚注文本：平等贡献
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The recent success of large language models (LLMs) at a variety of tasks has
    led to curiosity about their cognitive abilities and characteristics. As LLMs
    are increasingly integrated in daily life both as conversation partners and economic
    decision-makers (Munir et al., [2023](#bib.bib15); Chaturvedi et al., [2023](#bib.bib8);
    Yang et al., [2023](#bib.bib24)), such studies are necessary for understanding
    the limits and characteristics of such agents. An understanding of LLMs at a psychological
    level may also provide strategies for improved prompting and training. To this
    end, a number of researchers have recently adopted methods from cognitive psychology
    and behavioral economics to evaluate language models in the same way that humans
    have been evaluated (e.g. Linzen et al., [2016](#bib.bib13); Miotto et al., [2022](#bib.bib14);
    Phelps and Russell, [2023](#bib.bib17), among many others).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种任务中的近期成功引发了对其认知能力和特征的好奇。随着LLMs在日常生活中越来越多地作为对话伙伴和经济决策者（Munir et
    al., [2023](#bib.bib15); Chaturvedi et al., [2023](#bib.bib8); Yang et al., [2023](#bib.bib24)）被广泛应用，这些研究对于理解这些代理的限制和特征是必要的。对LLMs在心理学层面的理解也可能提供改进提示和训练的策略。为此，一些研究人员最近采用了来自认知心理学和行为经济学的方法来评估语言模型，方式类似于对人类的评估（例如，Linzen
    et al., [2016](#bib.bib13); Miotto et al., [2022](#bib.bib14); Phelps and Russell,
    [2023](#bib.bib17)等）。
- en: However, such work has not always paid due attention to the fact that LLM responses
    can be highly variable and sensitive to the details of the prompt used and to
    hyperparameters such as temperature. Limited interactions with LLMs—such as interactions
    using only one prompt—can be misleading (Bowman, [2023](#bib.bib4)). In this work,
    we follow up on the behavioral experiments conducted by Binz and Schulz ([2023](#bib.bib2)),
    who studied LLMs’ decision making using analogues of a number of well-known human
    experimental paradigms, finding strong divergences from human behavior. However,
    the experiments in the previous work used only one prompt per task, and did not
    study the effects of hyperparameters. We adopt the same task as the previous work,
    but systematically vary prompts and temperature.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些工作并未总是充分注意到LLM响应可能高度变异且对使用的提示细节和温度等超参数非常敏感。与LLMs的有限交互——例如仅使用一个提示的交互——可能会产生误导（Bowman，[2023](#bib.bib4)）。在这项工作中，我们跟进了Binz和Schulz（[2023](#bib.bib2)）进行的行为实验，他们使用了一些知名人类实验范式的类比来研究LLMs的决策制定，发现与人类行为存在较大分歧。然而，之前的实验每个任务仅使用了一个提示，并未研究超参数的影响。我们采用了与之前工作相同的任务，但系统地变换提示和温度。
- en: 'Our aims are both substantive—we seek to find whether, with basic changes to
    the prompt, models show human-like behavior in these decision making tasks—and
    methodological: we wish to emphasize that psychological LLM research must consider
    variability as a function of prompt and hyperparameters.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标既是实质性的——我们试图通过对提示进行基本修改，观察模型在这些决策任务中是否表现出类人行为——也是方法论上的：我们希望强调心理学LLM研究必须考虑到提示和超参数的变化。
- en: 2 Background and Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: 'As mechanistic understanding and control of LLMs remains complex, researchers
    have increasingly adopted methods from human behavioral sciences for characterizing
    LLMs’ behavior: in the same way that the human brain is largely a black box that
    must be probed using experimental methods and constructs, LLMs may be studied
    in the same way.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对大型语言模型（LLMs）的机制理解和控制仍然复杂，研究人员越来越多地采用来自人类行为科学的方法来描述LLMs的行为：就像人脑在很大程度上是一个黑箱，必须通过实验方法和构建进行探测一样，LLMs也可以以相同的方式进行研究。
- en: '![Refer to caption](img/87ab6da6742afad4a88ce4391c0628e6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/87ab6da6742afad4a88ce4391c0628e6.png)'
- en: 'Figure 1: Original Horizon 6 task prompt (Binz and Schulz, [2023](#bib.bib2)).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：原始的 Horizon 6 任务提示（Binz 和 Schulz，[2023](#bib.bib2)）。
- en: In addition to studies that have used the methods of cognitive psychology to
    understand LLMs’ reasoning and grammatical abilities (e.g., Linzen et al., [2016](#bib.bib13);
    Futrell et al., [2019](#bib.bib10); Cai et al., [2023](#bib.bib6)), researchers
    have increasingly adapted methods from psychometrics (Miotto et al., [2022](#bib.bib14);
    Bodroza et al., [2023](#bib.bib3); Abramski et al., [2023](#bib.bib1)), which
    seek to characterize LLMs in terms of personality variables such as agreeableness
    and conscientiousness, and methods from behavioral economics (Cartwright, [2018](#bib.bib7);
    Phelps and Russell, [2023](#bib.bib17); Horton, [2023](#bib.bib11)), which characterize
    LLMs’ decision-making in terms of preferences for risk and reward.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用认知心理学方法来理解LLMs的推理和语法能力的研究（例如，Linzen 等，[2016](#bib.bib13)；Futrell 等，[2019](#bib.bib10)；Cai
    等，[2023](#bib.bib6)），研究人员还越来越多地适应了心理测量学的方法（Miotto 等，[2022](#bib.bib14)；Bodroza
    等，[2023](#bib.bib3)；Abramski 等，[2023](#bib.bib1)），这些方法试图根据个性变量如宜人性和尽责性来描述LLMs，以及行为经济学的方法（Cartwright，[2018](#bib.bib7)；Phelps
    和 Russell，[2023](#bib.bib17)；Horton，[2023](#bib.bib11)），这些方法将LLMs的决策制定与风险和奖励的偏好相联系。
- en: Prior research on prompting techniques (Wei et al., [2022](#bib.bib22); Wang
    et al., [2023](#bib.bib21)) has shown that subtle modifications in input prompts
    can lead to varied outcomes in reasoning tasks Cobbe et al. ([2021](#bib.bib9)).
    Srivastava et al. ([2022](#bib.bib19)) revealed that Large Language Models (LLMs)
    are notably susceptible to the precise wording of natural language questions,
    especially when presented in a multiple-choice setting. In a recent study, Ouyang
    et al. ([2023](#bib.bib16)) emphasized the influence of temperature adjustments
    on LLM’s performance in code generation tasks. Unlike previous studies, our research
    delves into the sensitivity of LLMs concerning economic decision-making abilities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 之前关于提示技术的研究（Wei et al., [2022](#bib.bib22); Wang et al., [2023](#bib.bib21)）表明，输入提示的微小修改可能会导致推理任务中的不同结果
    Cobbe et al. ([2021](#bib.bib9))。Srivastava et al. ([2022](#bib.bib19)) 揭示了大型语言模型（LLMs）对自然语言问题的精确措辞特别敏感，尤其是在多项选择设置中。近期研究中，Ouyang
    et al. ([2023](#bib.bib16)) 强调了温度调整对LLM在代码生成任务中的表现的影响。与以往研究不同，我们的研究深入探讨了LLM在经济决策能力方面的敏感性。
- en: 'Our work is a focused followup on Binz and Schulz ([2023](#bib.bib2)), investigating
    the sensitivity of one of their results to changes in prompt and hyperparameters.
    Binz and Schulz ([2023](#bib.bib2)) evaluated on decision-making, information
    search, deliberation, and causal reasoning in text-davinci-002 (Brown et al.,
    [2020](#bib.bib5)) by presenting it with prompts such as the one shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2 Background and Related Work ‣ Exploring the Sensitivity of LLMs’
    Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters").
    We follow up on the tasks from the information search area, instantiated in the
    Horizon task, described in the following section. In this task, humans show a
    characteristic trade-off of exploration and exploitation (Wilson et al., [2014](#bib.bib23)),
    favoring exploration in early trials and exploitation later, whereas Binz and
    Schulz ([2023](#bib.bib2)) find that LLMs do not.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的工作是对 Binz 和 Schulz ([2023](#bib.bib2)) 的关注跟进，研究其结果对提示和超参数变化的敏感性。Binz 和 Schulz
    ([2023](#bib.bib2)) 在决策、信息搜索、深思熟虑和因果推理方面进行了评估，使用了如图 [1](#S2.F1 "Figure 1 ‣ 2 Background
    and Related Work ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities:
    Insights from Prompt Variation and Hyperparameters") 中所示的提示。我们跟进了信息搜索领域的任务，体现在
    Horizon 任务中，在下节中描述。在这个任务中，人类表现出探索和开发的特征性权衡（Wilson et al., [2014](#bib.bib23)），在早期试验中偏向探索，在后期则偏向开发，而
    Binz 和 Schulz ([2023](#bib.bib2)) 发现LLM并非如此。'
- en: The results of Binz and Schulz ([2023](#bib.bib2)), however, are based on single
    prompt and setting, limiting the generality of their results. Furthermore, observing
    the Horizon task prompt (and the others used throughout the paper), it does not
    follow what are now regarded as best practices for such tasks, for example the
    use of Chain-of-Thought (CoT) prompting Wei et al. ([2022](#bib.bib22))—the original
    prompt forces the LLM to choose a machine in the next token generated, without
    deliberation. Below, we investigate the behavior of LLMs on this task under systematic
    variations of temperature and prompt.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Binz 和 Schulz ([2023](#bib.bib2)) 的结果基于单一的提示和设置，限制了其结果的普遍性。此外，观察 Horizon
    任务提示（以及论文中使用的其他提示），它并未遵循目前被认为是最佳实践的做法，例如 Chain-of-Thought（CoT）提示 Wei et al. ([2022](#bib.bib22))——原始提示迫使LLM在生成的下一个
    token 中选择一个机器，而没有经过深思熟虑。以下，我们调查了LLM在此任务下的行为，系统地变化温度和提示。
- en: 3 Horizon Task Experiments
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 Horizon 任务实验
- en: '![Refer to caption](img/f0011cc03eeaaf1e76505edd17fcc564.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f0011cc03eeaaf1e76505edd17fcc564.png)'
- en: (a) text-davinci-002
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (a) text-davinci-002
- en: '![Refer to caption](img/69a781c6870275268f48991340dd430b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/69a781c6870275268f48991340dd430b.png)'
- en: (b) text-davinci-003
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (b) text-davinci-003
- en: '![Refer to caption](img/f5d05f727420fb4330d171aa77bdef00.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f5d05f727420fb4330d171aa77bdef00.png)'
- en: (c) gpt-3.5-turbo
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (c) gpt-3.5-turbo
- en: 'Figure 2: Mean regret obtained in the Horizon (multi-trial multi-armed bandit)
    task by humans and LLMs with varying temperature, using the prompt from . The
    solid black line indicates human performance; others are LLMs. Error bars show
    the standard error of the mean.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在Horizon（多次试验多臂老虎机）任务中，使用不同温度的提示获得的平均遗憾值，展示了人类和LLM的表现。实线黑色线条表示人类表现；其他则为LLM。误差条显示均值的标准误差。
- en: '![Refer to caption](img/e0ca1d73d3cd011dc5e832c905497344.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e0ca1d73d3cd011dc5e832c905497344.png)'
- en: 'Figure 3: Modifications in prompt for the Horizon task. Horizon 1 prompt is
    shown. In case of CoT, CoT-Exploit & CoT-Explore we explicit ask the model to
    summarize its choice at the end by appending the entire prompt with "Answer the
    following question and summarize your choice at the end as ‘Machine:[machine_name]’."
    at the beginning.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Horizon任务中的提示修改。展示了Horizon 1提示。在CoT、CoT-Exploit和CoT-Explore的情况下，我们明确要求模型在末尾总结其选择，通过在提示开头添加“回答以下问题，并在末尾总结你的选择为‘Machine:[machine_name]’”来实现。
- en: The Horizon Task as shown in Binz and Schulz ([2023](#bib.bib2)) is a special
    case of the Multi-Armed bandit (MAB) setting. MAB problems (Sutton and Barto,
    [2018](#bib.bib20), Ch. 2) are one of the common problems in the area of Reinforcement
    Learning. This game involves an agent interacting with a slot machine possessing
    $k$ arms. Each arm the agent pulls has a reward associated with it defined by
    an underlying probability distribution. This game is played over multiple episodes
    with the goal of maximizing the accrued rewards.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如Binz和Schulz（[2023](#bib.bib2)）所示，Horizon任务是多臂老虎机（MAB）设定的一个特殊情况。MAB问题（Sutton和Barto，[2018](#bib.bib20)，第2章）是强化学习领域中的常见问题之一。这个游戏涉及一个代理与一个拥有$k$个臂的老虎机互动。每次代理拉动一个臂时，都会获得一个与之相关的奖赏，这个奖赏由一个潜在的概率分布定义。这个游戏会进行多个回合，目标是最大化累计的奖赏。
- en: One approach involves persistently selecting the arm that has delivered the
    maximum amount of rewards in the past. An alternative strategy involves thorough
    exploration of all arms to discern their respective underlying probability distributions,
    followed by the selection of the arm with the highest potential for reward. While
    this is feasible, every turn spent in discerning the underlying distribution,
    diverts from the primary goal of reward maximization. The former and latter strategies
    are known as exploitation and exploration respectively, and the exploration–exploitation
    dilemma is a fundamental concept in decision-making that arises in many domains.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是持续选择过去提供最多奖赏的臂。另一种策略是彻底探索所有的臂，以识别它们各自的潜在概率分布，然后选择具有最高奖赏潜力的臂。虽然这种方法是可行的，但每次花在识别潜在分布上的时间都会偏离主要的奖赏最大化目标。前者和后者的策略分别称为利用和探索，探索–利用困境是决策中一个基本的概念，出现在许多领域中。
- en: 'To explore the extent to which humans use these strategies, Wilson et al. ([2014](#bib.bib23))
    reports experiments where participants were asked to play the Horizon Task. This
    task consists of a set of two-armed bandit problems, where participants are presented
    with two options, each associated with noisy rewards. The task comprises either
    five or ten trials, and in each trial, participants must select one option, receiving
    corresponding reward feedback. In the initial four trials of the task, participants
    have only one option and are provided with the corresponding reward feedback.
    These forced-choice trials create two distinct information conditions: “unequal
    information” and “equal information.” In the unequal information condition, one
    option is played three times, while the other option is played only once. In the
    equal information condition, both options are played twice. The five-trial setting
    is denoted Horizon 1, indicating that participants make decisions only once, while
    the ten-trial setting is referred to as Horizon 6, as participants make decisions
    over six rounds.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索人类使用这些策略的程度，Wilson等人（[2014](#bib.bib23)）报告了一个实验，其中参与者被要求玩Horizon任务。该任务由一组两臂老虎机问题组成，参与者面临两个选项，每个选项都与嘈杂的奖赏相关联。任务包括五次或十次试验，在每次试验中，参与者必须选择一个选项，并获得相应的奖赏反馈。在任务的前四次试验中，参与者只有一个选项，并且获得相应的奖赏反馈。这些强制选择的试验创造了两种不同的信息条件：“不平等信息”和“平等信息”。在不平等信息条件下，一个选项被玩了三次，而另一个选项只玩了一次。在平等信息条件下，两种选项都玩了两次。五次试验设置为Horizon
    1，表示参与者只做一次决策，而十次试验设置为Horizon 6，表示参与者在六轮中做决策。
- en: 'Binz and Schulz ([2023](#bib.bib2)) applied this experimental design to language
    models using the prompt in Figure [1](#S2.F1 "Figure 1 ‣ 2 Background and Related
    Work ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights
    from Prompt Variation and Hyperparameters"), and we follow their experimental
    setup exactly except for variations to the prompt and hyperparameters. The performance
    of LLMs is assessed by measuring the mean regret across multiple runs. The regret
    is defined as the difference between the optimal reward, which corresponds to
    the machine with the higher reward, and the actual reward obtained from the selection
    process. Human behavior favors exploitation in Horizon 1, but a gradual shift
    from exploration to exploitation in Horizon 6.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'Binz和Schulz（[2023](#bib.bib2)）将此实验设计应用于语言模型，使用了图[1](#S2.F1 "Figure 1 ‣ 2 Background
    and Related Work ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities:
    Insights from Prompt Variation and Hyperparameters")中的提示，我们的实验设置完全遵循他们的设计，只是对提示和超参数进行了变动。通过测量多次运行的平均遗憾来评估LLMs的性能。遗憾定义为最优奖励与从选择过程中获得的实际奖励之间的差异。人类行为在Horizon
    1中倾向于利用，但在Horizon 6中从探索逐渐转向利用。'
- en: '![Refer to caption](img/88a42583fde0ac3803c59b58c3293316.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/88a42583fde0ac3803c59b58c3293316.png)'
- en: (a) text-davinci-002³³3Experiments with text-davinci-002 using CoT prompt failed
    due to its inability to summarize its choice at the end.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (a) text-davinci-002³³3使用CoT提示的text-davinci-002实验失败，因为它无法在最后总结其选择。
- en: '![Refer to caption](img/53af4d3c303dfaf208936a045f10792f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/53af4d3c303dfaf208936a045f10792f.png)'
- en: (b) text-davinci-003
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (b) text-davinci-003
- en: '![Refer to caption](img/9b208a45f852b275faa0417b84b07b45.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b208a45f852b275faa0417b84b07b45.png)'
- en: (c) gpt-3.5-turbo
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (c) gpt-3.5-turbo
- en: 'Figure 4: Mean regret obtained by humans and LLMs on the Horizon task, varying
    prompt. ‘Quasi-CoT’ means a prompt of the form ‘Thinking step-by-step, I choose
    Machine …’ which does not enable true chain-of-thought reasoning. The temperatures
    for GPT-2, GPT-3, and GPT-3.5 are 1.0, 0.5, and 1.0 respectively. These temperatures
    show the greatest learning effect (negative slope) in the Horizon 6 task.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：人类和LLMs在Horizon任务中获得的平均遗憾，变化提示。‘Quasi-CoT’意味着一种提示形式‘一步步思考，我选择机器…’，它不支持真正的链式思维推理。GPT-2、GPT-3和GPT-3.5的温度分别为1.0、0.5和1.0。这些温度在Horizon
    6任务中显示出最大的学习效果（负斜率）。
- en: 3.1 Varying Temperature
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 变化温度
- en: 'The impact of various temperature settings (0.0,0.5,1.0) on all three OpenAI
    models²²2https://platform.openai.com/docs/models/overview tested is illustrated
    in Figure [2](#S3.F2 "Figure 2 ‣ 3 Horizon Task Experiments ‣ Exploring the Sensitivity
    of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters").
    It is clear that the behavior of each model, as indicated by the mean regret line,
    differs according to the temperature. For Horizon 1, the lowest regret is obtained
    for temperature zero across all three models. Further, unlike text-davinci-002
    and as shown in Binz and Schulz ([2023](#bib.bib2)), the mean regret is lower
    than humans for both text-davinci-003 and gpt-3.5-turbo. In the case of Horizon
    6, there is a notable rise in the inital mean regret, suggesting that higher temperatures
    result in suboptimal decision-making. However, increasing temperature demonstrates
    a more pronounced learning effect, as evidenced by a greater negative slope.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '各种温度设置（0.0,0.5,1.0）对测试的所有三个OpenAI模型²²2https://platform.openai.com/docs/models/overview的影响如图[2](#S3.F2
    "Figure 2 ‣ 3 Horizon Task Experiments ‣ Exploring the Sensitivity of LLMs’ Decision-Making
    Capabilities: Insights from Prompt Variation and Hyperparameters")所示。很明显，每个模型的行为，如平均遗憾线所示，依赖于温度。对于Horizon
    1，所有三个模型在温度为零时获得最低的遗憾。此外，不像text-davinci-002，正如Binz和Schulz（[2023](#bib.bib2)）所示，text-davinci-003和gpt-3.5-turbo的平均遗憾低于人类。在Horizon
    6的情况下，初始平均遗憾显著上升，表明较高的温度导致次优的决策。然而，增加温度显示出更明显的学习效果，如负斜率所示。'
- en: 3.2 Varying Prompt
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 变化提示
- en: 'To encourage deliberation during decision-making, we incorporate variations
    in the input prompt. Specifically, we explore two different variants of the Chain
    of Thought (CoT) prompting technique (Wei et al., [2022](#bib.bib22))—CoT and
    Quasi-CoT. In Figure [3](#S3.F3 "Figure 3 ‣ 3 Horizon Task Experiments ‣ Exploring
    the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation
    and Hyperparameters"), we illustrate the modifications made to the original prompt.
    The variant referred to as Quasi-CoT utilizes the prompt “Thinking step by step
    I choose Machine”, which forces the machine to make a decision before fully processing
    its reasoning. On the other hand, the CoT variant makes a decision only after
    fully processing its reasoning. The Quasi-CoT condition allows us to disentangle
    the effects of true step-by-step reasoning in CoT from the effects of prompting
    the LLM to think carefully.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在决策过程中鼓励深思熟虑，我们引入了输入提示的变化。具体来说，我们探索了链式思维（CoT）提示技术的两种不同变体（Wei 等，[2022](#bib.bib22)）——CoT
    和准 CoT。在图 [3](#S3.F3 "Figure 3 ‣ 3 Horizon Task Experiments ‣ Exploring the Sensitivity
    of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters")
    中，我们展示了对原始提示所做的修改。所谓准 CoT 的变体使用了提示“逐步思考我选择机器”，这迫使机器在完全处理推理之前做出决策。另一方面，CoT 变体则在完全处理推理后才做出决策。准
    CoT 条件允许我们将 CoT 中真正逐步推理的效果与提示 LLM 仔细思考的效果分开。'
- en: 'The alteration in the behavior of LLMs due to changes in the input prompt is
    depicted in Figure [4](#S3.F4 "Figure 4 ‣ 3 Horizon Task Experiments ‣ Exploring
    the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation
    and Hyperparameters"). Across all models, CoT demonstrates lower-regret compared
    to both Quasi-CoT and the original prompt, whereas Quasi-CoT performs worse than
    original prompt. Furthermore, even in text-davinci-002, we find that altered prompts
    yield the human-like negative slope, indicating an exploration–exploitation trade-off.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM 行为因输入提示的变化而改变，这在图 [4](#S3.F4 "Figure 4 ‣ 3 Horizon Task Experiments ‣ Exploring
    the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation
    and Hyperparameters") 中描述。在所有模型中，CoT 表现出比准 CoT 和原始提示更低的遗憾，而准 CoT 表现比原始提示更差。此外，即使在
    text-davinci-002 中，我们发现变化的提示也显示出类人的负斜率，表明探索与利用之间的权衡。'
- en: 3.3 CoT Prompting with Hints
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 带提示的 CoT 提示
- en: 'To overcome the identified limitations in LLMs, such as their inaccuracies
    in computing averages (Razeghi et al., [2022](#bib.bib18); Imani et al., [2023](#bib.bib12))
    and sub-optimal exploration capabilities, we introduce additional hints within
    the input prompt to guide the decision-making process. Specifically, we designed
    two prompts, namely CoT-Exploit and CoT-Explore, which aim to facilitate explicit
    exploitation and exploration. The hints associated with these prompts are shown
    in Figure [3](#S3.F3 "Figure 3 ‣ 3 Horizon Task Experiments ‣ Exploring the Sensitivity
    of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '为了克服 LLM 的识别限制，例如计算平均值的准确性（Razeghi 等，[2022](#bib.bib18); Imani 等，[2023](#bib.bib12)）和次优的探索能力，我们在输入提示中引入了额外的提示，以指导决策过程。具体来说，我们设计了两个提示，即
    CoT-Exploit 和 CoT-Explore，旨在促进明确的利用和探索。这些提示相关的信息见图 [3](#S3.F3 "Figure 3 ‣ 3 Horizon
    Task Experiments ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities:
    Insights from Prompt Variation and Hyperparameters")。'
- en: '![Refer to caption](img/1c7f797bc9ab70ccf91e4354d86b99c3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c7f797bc9ab70ccf91e4354d86b99c3.png)'
- en: 'Figure 5: gpt-3.5-turbo’s behavior under different variants of CoT prompts
    at temperature 1.0.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同 CoT 提示变体在温度 1.0 下的 gpt-3.5-turbo 行为。
- en: In the CoT-Exploit prompt, we instruct the model to base its decisions on the
    average of observed experiences and equip it with the required mathematical calculations
    to make a decision. Likewise, in the CoT-Explore approach, we explicitly direct
    the model to select a machine with lower frequency among the observed experiences.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CoT-Exploit 提示中，我们指示模型根据观察到的经验的平均值做出决策，并为其提供所需的数学计算以做出决策。同样，在 CoT-Explore
    方法中，我们明确指示模型从观察到的经验中选择一个频率较低的机器。
- en: 'The performance of gpt-3.5-turbo, using various CoT prompting variants, is
    compared in Figure [5](#S3.F5 "Figure 5 ‣ 3.3 CoT Prompting with Hints ‣ 3 Horizon
    Task Experiments ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities:
    Insights from Prompt Variation and Hyperparameters"). As anticipated, CoT-Exploit
    outperforms CoT-Explore, displaying a consistent decrease in slope. However, CoT-Explore
    performs significantly worse than random decision-making. CoT-Explore primary
    concentrates on getting more information about each machine rather than overall
    rewards.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S3.F5 "Figure 5 ‣ 3.3 CoT Prompting with Hints ‣ 3 Horizon Task Experiments
    ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from
    Prompt Variation and Hyperparameters")展示了使用各种CoT提示变体的gpt-3.5-turbo的表现。如预期的那样，CoT-Exploit的表现优于CoT-Explore，显示出斜率的一致下降。然而，CoT-Explore的表现明显比随机决策差。CoT-Explore主要集中在获取每台机器的更多信息，而非总体奖励。'
- en: 3.4 Discussion
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 讨论
- en: Through our experiments, we have discovered that the decision-making capabilities
    of LLMs are influenced by both the prompts used and the temperature settings,
    more so by the choice of prompt rather than the temperature. This highlights the
    importance of varying prompts to elicit the desired behavior from LLMs during
    decision-making tasks, and that studies which have used only one kind of prompt
    are potentially misleading.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的实验，我们发现LLM的决策能力受到了所使用的提示和温度设置的影响，其中提示的选择对决策能力的影响大于温度设置。这突显了在决策任务中变化提示以引出期望行为的重要性，同时那些只使用一种提示的研究可能具有误导性。
- en: 'Intriguingly, we observed that the model gpt-3.5-turbo with the Quasi-CoT prompt
    (Figure [5](#S3.F5 "Figure 5 ‣ 3.3 CoT Prompting with Hints ‣ 3 Horizon Task Experiments
    ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from
    Prompt Variation and Hyperparameters")) exhibits the closest resemblance to human
    behavior. This prompt alerts the model to the need for reasoning, but does not
    give it the space to actually perform any reasoning. The similarity of the Quasi-CoT
    result to humans suggests that humans may also struggle to fully process the associated
    information and reasoning.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '有趣的是，我们观察到使用Quasi-CoT提示的gpt-3.5-turbo模型（图[5](#S3.F5 "Figure 5 ‣ 3.3 CoT Prompting
    with Hints ‣ 3 Horizon Task Experiments ‣ Exploring the Sensitivity of LLMs’ Decision-Making
    Capabilities: Insights from Prompt Variation and Hyperparameters")）展现了与人类行为最为接近的表现。该提示提醒模型需要进行推理，但没有提供实际进行推理的空间。Quasi-CoT结果与人类的相似性表明，人类在处理相关信息和推理时也可能面临困难。'
- en: 'Furthermore, by providing hints to guide the decision-making process, we have
    observed that superhuman performance can be achieved, as demonstrated by the CoT-Exploit
    variant (Figure [5](#S3.F5 "Figure 5 ‣ 3.3 CoT Prompting with Hints ‣ 3 Horizon
    Task Experiments ‣ Exploring the Sensitivity of LLMs’ Decision-Making Capabilities:
    Insights from Prompt Variation and Hyperparameters")). This result suggests that
    language model behavior in these tasks is potentially highly controllable.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，通过提供提示以指导决策过程，我们观察到可以实现超人类表现，如CoT-Exploit变体所示（图[5](#S3.F5 "Figure 5 ‣ 3.3
    CoT Prompting with Hints ‣ 3 Horizon Task Experiments ‣ Exploring the Sensitivity
    of LLMs’ Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters")）。这一结果表明，语言模型在这些任务中的行为可能是高度可控的。'
- en: 4 Conclusion
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: We have demonstrated that the psychological behavior of LLMs, as previously
    explored by Binz and Schulz ([2023](#bib.bib2)), is highly sensitive to the way
    these LLMs are queried. The non-human-like behavior observed by Binz and Schulz
    ([2023](#bib.bib2)) vanishes under simple variations of prompt, and super-human
    performance in terms of minimizing regret is easily achievable. Going forward,
    we urge careful consideration in the LLM psychology literature of the fact that
    model behavior can diverge under different settings.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了LLM的心理行为，如Binz和Schulz（[2023](#bib.bib2)）之前探讨的那样，对这些LLM的查询方式非常敏感。Binz和Schulz（[2023](#bib.bib2)）观察到的非人类行为在简单的提示变化下消失，而在最小化悔恨方面可以轻松实现超人类表现。展望未来，我们呼吁在LLM心理学文献中谨慎考虑模型行为在不同设置下的可能差异。
- en: 5 Limitations
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 局限性
- en: We have presented a focused extension of one of the studies from Binz and Schulz
    ([2023](#bib.bib2)), demonstrating sensitivity to prompt and hyperparameters which
    was overlooked in the previous work. However, our work is limited in that (1)
    we have only examined one of the tasks from Binz and Schulz ([2023](#bib.bib2)),
    (2) we have only presented a few variations of temperature and prompt, and (3)
    we have only experimented with some of the models available to us as of June 2023,
    selecting high-profile closed-source models over open-source models. Nevertheless,
    we believe that our overarching point that LLM psychology needs to take into account
    hyperparameters, prompts, and variability remains valid.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了 Binz 和 Schulz（[2023](#bib.bib2)）的研究的一个重点扩展，演示了在先前工作中被忽视的对提示和超参数的敏感性。然而，我们的工作也有局限性：（1）我们只研究了
    Binz 和 Schulz（[2023](#bib.bib2)）的一项任务，（2）我们只展示了少量的温度和提示变体，（3）我们只实验了截至 2023 年 6
    月时可用的部分模型，优先选择了高知名度的闭源模型而非开源模型。然而，我们认为我们关于 LLM 心理学需要考虑超参数、提示和变异性的总体观点依然有效。
- en: Ethics Statement
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This work involves psychological studies of LLMs in economic decision making
    contexts. If LLMs are really deployed as economic decision makers, then ethical
    issues could result from biases and limitations of the models. We urge caution
    in such applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作涉及 LLM 在经济决策情境中的心理学研究。如果 LLM 真正被用作经济决策者，则可能会因模型的偏见和局限性产生伦理问题。我们在这种应用中建议谨慎对待。
- en: References
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abramski et al. (2023) Katherine Abramski, Salvatore Citraro, Luigi Lombardi,
    Giulio Rossetti, and Massimo Stella. 2023. Cognitive network science reveals bias
    in gpt-3, chatgpt, and gpt-4 mirroring math anxiety in high-school students. *arXiv
    preprint arXiv:2305.18320*.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abramski 等人（2023）Katherine Abramski, Salvatore Citraro, Luigi Lombardi, Giulio
    Rossetti 和 Massimo Stella。2023年。认知网络科学揭示 GPT-3、ChatGPT 和 GPT-4 中的偏见，反映了高中生的数学焦虑。*arXiv
    预印本 arXiv:2305.18320*。
- en: Binz and Schulz (2023) Marcel Binz and Eric Schulz. 2023. Using cognitive psychology
    to understand gpt-3. *Proceedings of the National Academy of Sciences*, 120(6):e2218523120.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Binz 和 Schulz（2023）Marcel Binz 和 Eric Schulz。2023年。利用认知心理学理解 gpt-3。*《美国国家科学院院刊》*，120(6):e2218523120。
- en: 'Bodroza et al. (2023) Bojana Bodroza, Bojana M Dinic, and Ljubisa Bojic. 2023.
    Personality testing of gpt-3: Limited temporal reliability, but highlighted social
    desirability of gpt-3’s personality instruments results. *arXiv preprint arXiv:2306.04308*.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bodroza 等人（2023）Bojana Bodroza, Bojana M Dinic 和 Ljubisa Bojic。2023年。GPT-3 的个性测试：有限的时间可靠性，但突显了
    GPT-3 个性工具结果的社会期望。*arXiv 预印本 arXiv:2306.04308*。
- en: Bowman (2023) Samuel R Bowman. 2023. Eight things to know about large language
    models. *arXiv preprint arXiv:2304.00612*.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowman（2023）Samuel R Bowman。2023年。关于大语言模型的八件事。*arXiv 预印本 arXiv:2304.00612*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in Neural
    Information Processing Systems*, 33:1877–1901.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等人。2020年。语言模型是少样本学习者。*《神经信息处理系统进展》*，33:1877–1901。
- en: Cai et al. (2023) Zhenguang G Cai, David A Haslett, Xufeng Duan, Shuqi Wang,
    and Martin J Pickering. 2023. Does ChatGPT resemble humans in language use? *arXiv
    preprint arXiv:2303.08014*.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人（2023）郑光 G Cai, David A Haslett, Xufeng Duan, Shuqi Wang 和 Martin J Pickering。2023年。《ChatGPT
    在语言使用上是否类似于人类？》*arXiv 预印本 arXiv:2303.08014*。
- en: Cartwright (2018) Edward Cartwright. 2018. *Behavioral Economics*. Routledge.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cartwright（2018）Edward Cartwright。2018年。*《行为经济学》*。Routledge。
- en: 'Chaturvedi et al. (2023) Rijul Chaturvedi, Sanjeev Verma, Ronnie Das, and Yogesh K
    Dwivedi. 2023. Social companionship with artificial intelligence: Recent trends
    and future avenues. *Technological Forecasting and Social Change*, 193:122634.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaturvedi 等人（2023）Rijul Chaturvedi, Sanjeev Verma, Ronnie Das 和 Yogesh K Dwivedi。2023年。与人工智能的社会陪伴：近期趋势与未来方向。*《技术预测与社会变迁》*，193:122634。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. [Training verifiers to solve
    math word problems](http://arxiv.org/abs/2110.14168).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse 和 John Schulman。2021年。[训练验证者解决数学文字题](http://arxiv.org/abs/2110.14168)。
- en: 'Futrell et al. (2019) Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian,
    Miguel Ballesteros, and Roger Levy. 2019. [Neural language models as psycholinguistic
    subjects: Representations of syntactic state](https://doi.org/10.18653/v1/N19-1004).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 32–42, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Futrell等人（2019）Richard Futrell、Ethan Wilcox、Takashi Morita、Peng Qian、Miguel
    Ballesteros和Roger Levy。2019年。[神经语言模型作为心理语言学对象：句法状态的表征](https://doi.org/10.18653/v1/N19-1004)。在*2019年北美计算语言学协会年会：人类语言技术会议论文集（第1卷：长篇论文和短篇论文）*中，第32–42页，美国明尼阿波利斯。计算语言学协会。
- en: 'Horton (2023) John J Horton. 2023. Large language models as simulated economic
    agents: What can we learn from homo silicus? *arXiv preprint arXiv:2301.07543*.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horton（2023）John J Horton。2023年。大型语言模型作为模拟经济代理：我们可以从homo silicus中学到什么？*arXiv预印本
    arXiv:2301.07543*。
- en: 'Imani et al. (2023) Shima Imani, Liang Du, and Harsh Shrivastava. 2023. [MathPrompter:
    Mathematical reasoning using large language models](https://doi.org/10.18653/v1/2023.acl-industry.4).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 5: Industry Track)*, pages 37–42, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Imani等人（2023）Shima Imani、Liang Du和Harsh Shrivastava。2023年。[MathPrompter: 使用大型语言模型进行数学推理](https://doi.org/10.18653/v1/2023.acl-industry.4)。在*第61届计算语言学协会年会论文集（第5卷：工业追踪）*中，第37–42页，加拿大多伦多。计算语言学协会。'
- en: Linzen et al. (2016) Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing
    the ability of lstms to learn syntax-sensitive dependencies. *Transactions of
    the Association for Computational Linguistics*, 4:521–535.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linzen等人（2016）Tal Linzen、Emmanuel Dupoux和Yoav Goldberg。2016年。评估LSTM学习句法敏感依赖的能力。*计算语言学协会学报*，4:521–535。
- en: Miotto et al. (2022) Marilù Miotto, Nicola Rossberg, and Bennett Kleinberg.
    2022. Who is gpt-3? an exploration of personality, values and demographics. *arXiv
    preprint arXiv:2209.14338*.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miotto等人（2022）Marilù Miotto、Nicola Rossberg和Bennett Kleinberg。2022年。谁是gpt-3？对个性、价值观和人口统计的探索。*arXiv预印本
    arXiv:2209.14338*。
- en: Munir et al. (2023) Iqbal Munir et al. 2023. Artificial intelligence chatgpt
    in medicine. can it be the friend you are looking for? *Journal of Bangladesh
    Medical Association of North America (BMANA) BMANA Journal*, pages 01–04.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munir等人（2023）Iqbal Munir等人。2023年。人工智能chatgpt在医学中的应用。它能成为你正在寻找的朋友吗？*北美孟加拉医学协会（BMANA）BMANA期刊*，第01–04页。
- en: 'Ouyang et al. (2023) Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang.
    2023. [Llm is like a box of chocolates: the non-determinism of chatgpt in code
    generation](http://arxiv.org/abs/2308.02828).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2023）Shuyin Ouyang、Jie M. Zhang、Mark Harman和Meng Wang。2023年。[Llm就像一盒巧克力：chatgpt在代码生成中的非确定性](http://arxiv.org/abs/2308.02828)。
- en: Phelps and Russell (2023) Steve Phelps and Yvan I Russell. 2023. Investigating
    emergent goal-like behaviour in large language models using experimental economics.
    *arXiv preprint arXiv:2305.07970*.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phelps和Russell（2023）Steve Phelps和Yvan I Russell。2023年。使用实验经济学调查大型语言模型中的新兴目标行为。*arXiv预印本
    arXiv:2305.07970*。
- en: 'Razeghi et al. (2022) Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and
    Sameer Singh. 2022. [Impact of pretraining term frequencies on few-shot numerical
    reasoning](https://doi.org/10.18653/v1/2022.findings-emnlp.59). In *Findings of
    the Association for Computational Linguistics: EMNLP 2022*, pages 840–854, Abu
    Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Razeghi等人（2022）Yasaman Razeghi、Robert L Logan IV、Matt Gardner和Sameer Singh。2022年。[预训练术语频率对少样本数值推理的影响](https://doi.org/10.18653/v1/2022.findings-emnlp.59)。在*计算语言学协会发现：EMNLP
    2022*中，第840–854页，阿布扎比，阿拉伯联合酋长国。计算语言学协会。
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava等人（2022）Aarohi Srivastava、Abhinav Rastogi、Abhishek Rao、Abu Awal Md
    Shoeb、Abubakar Abid、Adam Fisch、Adam R Brown、Adam Santoro、Aditya Gupta、Adrià Garriga-Alonso等人。2022年。超越模仿游戏：量化和推断语言模型的能力。*arXiv预印本
    arXiv:2206.04615*。
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    Learning: An Introduction*. MIT press.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton和Barto（2018）Richard S Sutton和Andrew G Barto。2018年。*强化学习：导论*。麻省理工学院出版社。
- en: Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. [Self-consistency improves
    chain of thought reasoning in language models](http://arxiv.org/abs/2203.11171).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023）**王雪志**、**贾森·魏**、**戴尔·舒尔曼斯**、**阮克·乐**、**艾德·池**、**沙兰·纳朗**、**阿坎克莎·乔杜赫里**和**丹尼·周**。2023年。
    [自一致性提高了语言模型中的思维链推理](http://arxiv.org/abs/2203.11171)。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2022）**贾森·魏**、**王雪志**、**戴尔·舒尔曼斯**、**马滕·博斯玛**、**艾德·池**、**阮克·乐**和**丹尼·周**。2022年。思维链提示引发大型语言模型中的推理。
    *arXiv预印本 arXiv:2201.11903*。
- en: 'Wilson et al. (2014) Robert C Wilson, Andra Geana, John M White, Elliot A Ludvig,
    and Jonathan D Cohen. 2014. Humans use directed and random exploration to solve
    the explore–exploit dilemma. *Journal of Experimental Psychology: General*, 143(6):2074.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威尔逊等（2014）**罗伯特·C·威尔逊**、**安德拉·吉安娜**、**约翰·M·怀特**、**艾略特·A·卢德维格**和**乔纳森·D·科恩**。2014年。人类使用有目的和随机探索来解决探索-利用困境。
    *实验心理学杂志：综合*，143（6）：2074。
- en: 'Yang et al. (2023) Hui Yang, Sifu Yue, and Yunzhong He. 2023. Auto-gpt for
    online decision making: Benchmarks and additional opinions. *arXiv preprint arXiv:2306.02224*.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2023）**杨晖**、**侍福**和**云中赫**。2023年。Auto-gpt在在线决策中的应用：基准测试和额外意见。 *arXiv预印本 arXiv:2306.02224*。
